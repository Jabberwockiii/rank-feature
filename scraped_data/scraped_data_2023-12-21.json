[
  {
    "title": "StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation",
    "link": "https://arxiv.org/pdf/2312.12491.pdf",
    "upvote": "59",
    "text": "StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation\nAkio Kodaira1,\u2217 Chenfeng Xu1,\u2217 Toshiki Hazama1,\u2217 Takanori Yoshimoto2 Kohei Ohno3\nShogo Mitsuhori4 Soichi Sugano5 Hanying Cho6 Zhijian Liu7 Kurt Keutzer1\n1UC Berkeley\n2University of Tsukuba\n3International Christian University\n4Toyo University\n5Tokyo Institute of Technology\n6Tohoku University\n7MIT\n{akio.kodaira, xuchenfeng}@berkeley.edu\nFigure 1. Real-time image-to-image results from camera and screen captures. The first and second columns show examples of AI-assisted\ndrawing in real-time, and the third column displays the real-time rendering of 2D illustrations from 3D avatars. The fourth and fifth\ncolumns demonstrate the real-time camera filter.\nAbstract\nWe introduce StreamDiffusion, a real-time diffusion\npipeline designed for interactive image generation. Exist-\n\u2217 denotes equal contribution\nThis work was done when Toshiki was a remote intern at UC Berkeley\ning diffusion models are adept at creating images from text\nor image prompts, yet they often fall short in real-time in-\nteraction. This limitation becomes particularly evident in\nscenarios involving continuous input, such as Metaverse,\nvideo game graphics rendering, live video streaming, and\nbroadcasting, where high throughput is imperative. To ad-\narXiv:2312.12491v1  [cs.CV]  19 Dec 2023\ndress this, we present a novel and simple approach that\ntransforms the original sequential denoising into the batch-\ning denoising process. Stream Batch eliminates the con-\nventional wait-and-interact approach and enables fluid and\nhigh throughput streams. To handle the disparity in fre-\nquencies between data input and model throughput, we de-\nsign a novel input-output queue for parallelizing the stream-\ning process. Moreover, the existing diffusion pipeline uses\nclassifier-free guidance to enforce the generated results\nconditioned on the prompts. We point out that current im-\nplementations are inefficient due to the inherent redundancy\nof negative conditional denoising. To mitigate the redun-\ndant computations, we propose a novel residual classifier-\nfree guidance (RCFG) algorithm that reduces the number\nof negative conditional denoising steps to only one or even\nzero.\nBesides, to optimize power consumption, we use\na simple and effective stochastic similarity filtering strat-\negy, which significantly reduces GPU activation frequency,\nthereby improving GPU utilization efficiency. Our batch-\ning denoising strategy achieves around 1.5x speedup com-\npared to the sequential denoising method at different de-\nnoising levels. The proposed (RCFG) leads to up-to 2.05x\nspeedups compared to the previous conventional classifier-\nfree guidance. The combination of the proposed strategies\nand existing mature acceleration tools makes the image-to-\nimage generation achieve up-to 91.07fps on one RTX 4090\nGPU, improving the throughputs of AutoPipline developed\nby Diffusers over 59.56x. Beyond the pipeline efficiency,\nour proposed StreamDiffusion also significantly reduces the\nenergy consumption by 2.39x on one RTX 3060 GPU and\n1.99x on one RTX 4090 GPU for a static scene input, re-\nspectively. The code is available at https://github.\ncom/cumulo-autumn/StreamDiffusion.\n1. Introduction\nRecently, there has been a growing trend in the commer-\ncialization of diffusion models [3, 21, 23] for applications\nwithin the Metaverse, as well as in the realms of online\nvideo streaming and broadcasting. These areas require dif-\nfusion pipelines that offer high throughput and low latency\nto ensure efficient human interaction. A pertinent example\nis the use of diffusion models to create virtual YouTubers.\nThese digital personas should be capable of reacting in a\nfluid and responsive manner to user input.\nTo advance high throughput and real-time interactive ca-\npabilities, current efforts primarily focus on reducing the\nnumber of denoising iterations, such as decreasing from 50\niterations to just a few [16, 17] or even one [13, 28]. The\ncommon strategy is to distill the multi-step diffusion models\ninto a few steps or re-frame the diffusion process with neu-\nral Ordinary Differential Equations (ODE) [14, 26]. Quan-\ntization has also been applied to diffusion models [9, 12] to\nimprove efficiency.\nIn this paper, we start from an orthogonal direction and\nintroduce StreamDiffusion, a pipeline-level solution that\nenables real-time interactive image generation with high\nthroughput.\nWe highlight that existing model design ef-\nforts can still integrate with our pipeline. Our approach en-\nables the use of N-step denoising diffusion models while\nstill keeping high throughput and offers users more flexibil-\nity in choosing their preferred models.\nWe leverage a simple strategy in StreamDiffusion: in-\nstead of the original sequential denoising mode, we batch\nthe denoising steps. We are inspired by the asynchronous\nprocessing in the computer architecture pipeline, and point\nout that we do not need to wait for the previous denoising\nprocesses to finish then to start the current denoising step,\nas shown in Fig. 2. In order to deal with the synchronous\nissue of input frequency and U-Net processing frequency,\nwe leverage a queue strategy to cache the inputs and out-\nput. Our pipeline is different from merely asynchronous\nprocessing. We make use of the advantage of GPU paral-\nlelism; thus, we can use one U-Net to denoise a batched\nnoise latent feature. More importantly, the existing diffu-\nsion pipeline incorporated classifier-free guidance to em-\nphasize the given prompts in the generated images. Never-\ntheless, traditional implementations of classifier-free guid-\nance are plagued by excessive and redundant computational\noverheads. We introduce an innovative approach termed as\nresidual classifier-free guidance (RCFG). The novel tech-\nnique approximates the negative condition with a virtual\nresidual noise, which allows us to calculate the negative\ncondition noise only during the initial step of the process.\nFurther, we point out that maintaining the diffusion mod-\nels always in an interactive mode is energy-consuming as it\nkeeps hitting GPU. To reduce the energy, we further ap-\nply a stochastic similarity filtering strategy. In the pipeline,\nwe compute the similarities between continuous inputs and\ndetermine whether the diffusion model should process the\nimages based on the similarity.\nExperiments demonstrate that our proposed StreamDif-\nfusion can achieve up to 91.07fps for image generation on a\none RTX4090 GPU, surpassing the diffusion Autopipeline\nfrom Diffusers team by up to 59.6x. Besides, our stochastic\nsimilarity filtering strategy significantly reduces the GPU\npower usage by 2.39x on one RTX 3090GPU and by 1.99x\non one RTX 4090GPU. Our proposed StreamDiffusion is\na new diffusion pipeline that is not only efficient but also\nenergy-saving.\n2. Related works\n2.1. Diffusion Models\nDiffusion models, as introduced in [8, 25], have sparked\nconsiderable interest in the commercial sector due to their\nFigure 2. The concept of Stream Batch. In our approach, instead of waiting for a single image to be fully denoised before processing the\nnext input image, we accept the next input image after each denoising step. This creates a denoising batch where the denoising steps are\nstaggered for each image. By concatenating these staggered denoising steps into a batch, we can efficiently process continuous inputs using\na U-Net for batch processing. The input image encoded at timestep t is generated and decoded at timestep t + n, where n is the number of\nthe denoising steps.\nhigh-quality image generation capabilities and the level of\ncontrol they offer over the generated images. These models\nhave been progressively adapted for various applications,\nincluding text-to-image generation [2, 19, 20], image edi-\ntion [1, 22], and video generation [4, 5].\nA notable advancement in this area is the development of\nconsistent models [26], which have demonstrated the poten-\ntial to improve the efficiency of the sampling process with\nrarely compromising on image quality. Building on this,\nrecent studies have focused on enhancing the efficiency of\ndiffusion models by reducing the number of sampling steps\nrequired, further expanding their practical applicability and\nefficiency.\n2.2. Accelerating Diffusion Model\nDiffusion models are currently limited by their slow speed\nin generating outputs.\nTo address this drawback, sev-\neral strategies have been developed. These include meth-\nods that don\u2019t require additional training, like using Ordi-\nnary Differential Equation (ODE) solvers, as explored by\n[14, 15, 17, 26]. Other techniques involve adaptive step size\nsolvers as discussed by [10] and predictor-corrector meth-\nods noted by [25]. The training-based methods are also\nproposed, such as optimized discretization techniques [27],\nneural operator design [29], and model distillation [24]. Our\nproposed method distinguishes itself significantly from the\napproaches mentioned previously. While earlier methods\nprimarily focus on the low latency of their individual model\ndesigns, our approach takes a different route. We introduce\na comprehensive pipeline-level solution specifically tailored\nfor high throughputs, which enables efficient interactive dif-\nfusion.\nOur pipeline is designed to seamlessly integrate\nmost of the low-latency diffusion models discussed above.\nBy doing so, we achieve a notable increase in speed within\nthe interactive generation process.\n3. StreamDiffusion Pipeline\nStreamDiffusion is a new diffusion pipeline aiming for high\nthroughput.\nIt comprises several key components:\nthe\nStream Batch strategy, Residual Classifier-Free Guidance\n(RCFG), an input-output queue, Stochastic Similarity Fil-\nter, a pre-computation procedure, and model acceleration\ntools with a tiny-autoencoder. We elaborate on the details\nbelow.\nFigure 3. Virtual residual noise vectors: The orange vectors depict\nthe virtual residual noise that starts from the PF ODE trajectory\nand points to the original input latent x0\n3.1. Batching the denoise step\nIn diffusion models, denoising steps are performed sequen-\ntially, resulting in a proportional increase in the processing\ntime of U-Net relative to the number of steps. However,\nto generate high-fidelity images, it is necessary to increase\nthe number of steps. To resolve the high-latency generation\nin interactive diffusion, we propose a method called Stream\nBatch.\nThe Stream Batch technique restructures sequential de-\nnoising operations into batched processes, wherein each\nbatch corresponds to a predetermined number of denoising\nsteps, as depicted in Fig. 2. The size of each batch is de-\ntermined by the number of these denoising steps. This ap-\nproach allows for each batch element to advance one step\nfurther in the denoising sequence via a single pass through\nU-Net. By iteratively applying this method, it is possible\nto effectively transform input images encoded at timestep t\ninto their corresponding image-to-image results at timestep\nt+n, thereby streamlining the denoising procedure.\nStream Batch significantly reduces the need for multiple\nU-Net inferences. The processing time does not escalate\nlinearly with the number of steps. This technique effectively\nshifts the trade-off from balancing processing time and gen-\neration quality to balancing VRAM capacity and generation\nquality. With adequate VRAM scaling, this method enables\nthe production of high-quality images within the span of a\nsingle U-Net processing cycle, effectively overcoming the\nconstraints imposed by increasing denoising steps.\n3.2. Residual Classifier-Free Guidance\nClassifier-free guidance (CFG) [7] is an algorithm that en-\nhances the effect of the original conditioning by performing\nvector calculations between an unconditioning or a nega-\ntive conditioning term [6] and the original conditioning term\n(Eq. 1).\n\u03f5\u03c4i,cfg = \u03f5\u03c4i,\u00afc + \u03b3(\u03f5\u03c4i,c \u2212 \u03f5\u03c4i,\u00afc),\n(1)\nwhere \u03f5\u03c4i,c and \u03f5\u03c4i,\u00afc are predicted residual noise at the de-\nnoising step \u03c4i with conditioning embedding c and negative\nconditioning embedding \u00afc, respectively, and \u03b3 is a guidance\nscale.\nThis allows for benefits such as strengthening the effect\nof the prompt. However, to compute the negative condition-\ning residual noise, it is necessary to pair each input latent\nvariable with the negative conditioning embedding and pass\nit through the U-Net at every inference time. To address this\ndrawback, we introduce Residual Classifier-Free Guidance\n(RCFG), which significantly reduces the computational cost\nof the additional U-Net inference for the negative condition-\ning embedding.\nFirstly, the encoded input latent x0 can be transferred to\nthe noise distribution x\u03c40 as follows,\nx\u03c40 = \u221a\u03b1\u03c40x0 +\np\n\u03b2\u03c40\u03f50,\n(2)\nwhere \u03b1\u03c40 and \u03b2\u03c40 are values determined by a noise sched-\nuler and \u03f50 is a sampled noise from a Gaussian N(0, I).\nFollowing the implementation of latent consistency model\n[26], we can predict the data distribution \u02c6x0,\u03c4i\u22121,cfg and\ngenerate a next step noise distribution x\u03c4i,cfg using the CFG\nresidual noise \u03f5\u03c4i\u22121,cfg as follows,\n\u02c6x0,\u03c4i\u22121,cfg = x\u03c4i\u22121,cfg \u2212 p\u03b2\u03c4i\u22121\u03f5\u03c4i\u22121,cfg\n\u221a\u03b1\u03c4i\u22121\n(3)\nx\u03c4i,cfg = \u221a\u03b1\u03c4i \u02c6x0,\u03c4i\u22121,cfg +\np\n\u03b2\u03c4i\u03f5i,\n(4)\nWhere \u03f5i \u223c N(0, I).\nFor the next step CFG calculation, instead of comput-\ning the next step negative conditioning residual noise \u03f5\u03c4i,\u00afc,\nwhich requires an additional U-Net computation cost, we\nhypothesize the existence of virtual residual noise \u03f5\u03c4i,\u00afc\u2032, and\na virtual negative condition embedding \u00afc\u2032 that predicts the\noriginal input image\u2019s latent x0 from x\u03c4i,cfg, as shown in\nFig. 3. Similarly to Eq. 3, this virtual residual noise satis-\nfies Eq. 5 to predict the input latent x0.\nx0 \u2248 \u02c6x0,\u03c4i,\u00afc\u2032 = x\u03c4i,cfg \u2212\np\n\u03b2\u03c4i\u03f5\u03c4i,\u00afc\u2032\n\u221a\u03b1\u03c4i\n(5)\nGiven the initial value x0, and the subsequent values\nof x\u03c4i,cfg obtained sequentially through Eq. 4, the virtual\nresidual noise \u03f5\u03c4i,\u00afc\u2032 can be analytically determined by em-\nploying these values with the Eq. 5:\n\u03f5\u03c4i,\u00afc\u2032 = x\u03c4i,cfg \u2212 \u221a\u03b1\u03c4ix0\np\n\u03b2\u03c4i\n(6)\nFigure 4. Input-Output Queue: The process of converting input images into a tensor data format manageable by the pipeline, and conversely,\nconverting decoded tensors back into output images requires a non-negligible amount of additional processing time. To avoid adding these\nimage processing times to the bottleneck process, the neural network inference process, we have segregated image pre-processing and post-\nprocessing into separate threads, allowing for parallel processing. Moreover, by utilizing an Input Tensor Queue, we can accommodate\ntemporary lapses in input images due to device malfunctions or communication errors, enabling smooth streaming.\nWith the virtual residual noise \u03f5\u03c4i,\u00afc\u2032 obtained from Eq. 6,\nwe formulate RCFG as follows,\n\u03f5\u03c4i,cfg = \u03b4\u03f5\u03c4i,\u00afc\u2032 + \u03b3(\u03f5\u03c4i,c \u2212 \u03b4\u03f5\u03c4i,\u00afc\u2032)\n(7)\nwhere \u03b4 is a magnitude moderation coefficient for the vir-\ntual residual noise that softens the effect and the approxi-\nmation error of the virtual residual noise.\nRCFG that uses the original input image latent x0 as the\nresidual term can effectively generate results that diverge\nfrom the original input image according to the magnitude of\nthe guidance scale \u03b3, thereby enhancing the effect of condi-\ntioning without the need for additional U-Net computations.\nWe call this method Self-Negative RCFG.\nNot only to deviate from the original input image x0, but\nalso to diverge from any negative condition, we can com-\npute the negative conditioning residual noise \u03f5\u03c40,\u00afc using U-\nNet only onetime for the first denoising step and use this to\npredict the negative conditioning denoised data as follows,\n\u02c6x0,\u03c40,\u00afc = x\u03c40 \u2212\np\n\u03b2\u03c40\u03f5\u03c40,\u00afc\n\u221a\u03b1\u03c40\n(8)\nIn Eq. 6, instead of x0, using \u02c6x0,\u03c40,\u00afc, we can obtain the vir-\ntual residual noise \u03f5\u03c4i+1,\u00afc\u2032 that can effectively diverge the\ngeneration results from the controllable negative condition-\ning embedding \u00afc. We call this Onetime-Negative RCFG.\nBy combining Eq.\n2, Eq.\n6, and Eq.\n8, both Self-\nNegative RCFG and Onetime-Negative RCFG can be uni-\nformly represented by the following equation,\n\u03f5\u03c4i,\u00afc\u2032 = x\u03c4i,cfg \u2212 \u221a\u03b1\u03c4ix0\np\n\u03b2\u03c4i\n+\ns\n\u03b1\u03c4i\u03b2\u03c40\n\u03b2\u03c4i\u03b1\u03c40\n(\u03f5\u03c40,\u00afc \u2212 \u03f50)\n(9)\nWhen \u03f5\u03c40,\u00afc = \u03f50, Eq. 9 is identical to Eq. 6 and returns\nSelf-Negative RCFG result.\nIn contrast to the conventional CFG, which requires\n2n computations of U-Net, the Self-Negative RCFG and\nOnetime-Negative RCFG necessitate only n and n+1 com-\nputations of U-Net, respectively, where n is the number of\nthe denoising steps.\n3.3. Input-Output Queue\nThe current bottleneck in high-speed image generation sys-\ntems lies in the neural network modules, including VAE\nand U-Net. To maximize the overall system speed, pro-\ncesses such as pre-processing and post-processing of im-\nages, which do not require handling by the neural network\nmodules, are moved outside of the pipeline and processed\nin parallel.\nIn the context of input image handling, specific opera-\ntions, including resizing of input images, conversion to ten-\nsor format, and normalization, are meticulously executed.\nTo address the disparity in processing frequencies between\nthe human inputs and the model throughput, we design an\ninput-output queuing system to enable efficient paralleliza-\ntion, as shown in Fig. 4. This system operates as follows:\nprocessed input tensors are methodically queued for Diffu-\nsion Models. During each frame, Diffusion Model retrieves\nthe most recent tensor from the input queue and forwards\nit to the VAE Encoder, thereby triggering the image gener-\nation sequence. Correspondingly, tensor outputs from the\nVAE Decoder are fed into an output queue. In the subse-\nquent output image handling phase, these tensors are sub-\nject to a series of post-processing steps and conversion into\nthe appropriate output format. Finally, the fully processed\nimage data is transmitted from the output handling system\nFigure 5. Inference pipeline overview: The core diffusion inference pipeline, including VAE and U-Net. By incorporating a denoising\nbatch and pre-computed prompt embedding cache, sampled noise cache, and scheduler values cache, the inference pipeline is enhanced\nin terms of speed and enables real-time image generation. Stochastic Similarity Filter (SSF) is designed to save GPU power usage. It\ndynamically gates the pass of the diffusion model. This framework realizes fast and energy-efficient real-time inference.\nto the rendering client.\n3.4. Stochastic Similarity Filter\nWhen images remain unchanged or show minimal changes,\nparticularly in scenarios without active user interaction or\nstatic environment, nearly identical input images are of-\nten repeatedly fed into the VAE and U-Net. This leads to\nthe generation of identical or nearly identical images and\nunnecessary consumption of GPU resources. In contexts\ninvolving continuous inputs, such instances of unmodified\ninput images can occasionally occur. To tackle this issue\nand minimize unnecessary computational load, we propose\na strategy termed stochastic similarity filter (SSF), as shown\nin Fig. 5.\nWe calculate the cosine similarity between the current\ninput Image It and the past reference frame Image Iref.\nSC(It, Iref) =\nIt \u00b7 Iref\n\u2225It\u2225\u2225Iref\u2225\n(10)\nBased on this cosine similarity, we calculate the proba-\nbility of skipping the subsequent VAE and U-Net processes.\nIt is given by\nP(skip|It, Iref) = max\n\u001a\n0, SC(It, Iref) \u2212 \u03b7\n1 \u2212 \u03b7\n\u001b\n,\n(11)\nwhere \u03b7 is the similarity threshold. This probability decides\nwhether subsequent processes like VAE Encoding, U-Net,\nand VAE Decoding should be skipped or not. If not skipped,\nthe input image at that time is saved and updated as the ref-\nerence image Iref for future use. This probabilistic skip-\nping mechanism allows the network to operate fully in dy-\nnamic scenes with low inter-frame similarity, while in static\nscenes with high inter-frame similarity, the network\u2019s oper-\national rate decreases, conserving computational resources.\nThe GPU usage is modulated seamlessly based on the sim-\nilarity of the input images, enabling smooth adaptation to\nscenes with varying dynamics.\nNote:\nWe emphasize that compared to determine\nwhether we skip the compute via a hard threshold, the pro-\nposed probability-sampling-based similarity filtering strat-\negy leads to a more smooth video generation. Because the\nhard threshold is prone to making the video stuck which\nhurts the impression of watching video streaming, while the\nsampling-based method significantly improves the smooth-\nness.\n3.5. Pre-computation\nThe U-Net architecture requires both input latent variables\nand conditioning embeddings. Typically, the conditioning\nembedding is derived from a prompt embedding, which re-\nmains constant across different frames. To optimize this,\nwe pre-compute the prompt embedding and store it in a\ncache. In interactive or streaming mode, this pre-computed\nprompt embedding cache is recalled. Within U-Net, the Key\nand Value are computed based on this pre-computed prompt\nembedding for each frame. We have modified the U-Net to\nstore these Key and Value pairs, allowing them to be reused.\nWhenever the input prompt is updated, we recompute and\nupdate these Key and Value pairs inside U-Net.\nFor consistent input frames across different timesteps\nand to improve computational efficiency, we pre-sample\nGaussian noise for each denoising step and store it in the\ncache. This ensures that while each denoising step has dis-\ntinct noise, every timestep retains the same noise (xt,\u03c4 \u0338=\nxt,\u03c4+1 and xt+1,\u03c4 = xt,\u03c4). This approach is particularly\nrelevant for image-to-image tasks.\nWe also precompute \u03b1\u03c4 and \u03b2\u03c4, the noise strength coef-\nficients for each denoising step \u03c4, defined as:\nxt = \u221a\u03b1\u03c4x0 +\np\n\u03b2\u03c4\u03f5\n(12)\nThis is a minor point in low throughput scenarios, but at\nframe rates higher than 60 FPS, the overhead of recomput-\ning these static values becomes noticeable. If we achieve 60\nFPS (i.e. 16.666 ms) with recomputing these static values in\nthe loop with pre-computed cache, we can achieve 62 FPS\n(i.e. 16.066 ms).\nWe note that we have a specific design of key-value\ncache for Latent Consistency Models (LCM). As per the\noriginal paper, we need to compute cskip(\u03c4) and cout(\u03c4) to\nsatisfy the following equation:\nf\u03b8(x, \u03c4) = cskip(\u03c4)x + cout(\u03c4)F\u03b8(x, \u03c4).\n(13)\nThe functions cskip(\u03c4) and cout(\u03c4) in original LCM [17]\nis constructed as follows:\ncskip(\u03c4) =\n\u03c32\ndata\n(s\u03c4)2 + \u03c32\ndata\n,\ncout(\u03c4) =\n\u03c3datas\u03c4\np\n\u03c32\ndata + (s\u03c4)2 ,\n(14)\nwhere \u03c3data = 0.5, and the timestep scaling factor s =\n10. We note that with s = 10, cskip(\u03c4) and cout(\u03c4) approx-\nimate delta functions that enforce the boundary condition\nto the consistency models. (i.e., at denoising step \u03c4 = 0,\ncskip(0) = 1, cout(0) = 0; and at \u03c4 \u0338= 0, cskip(\u03c4) = 0,\ncout(\u03c4) = 1). At inference time, there\u2019s no need to recom-\npute these functions repeatedly. We can either pre-compute\ncskip(\u03c4) and cout(\u03c4) for all denoising steps \u03c4 in advance\nor simply use constant values cskip = 0, cout = 1 for any\narbitrary denoising step \u03c4.\n3.6. Model Acceleration and Tiny AutoEncoder\nWe employ TensorRT to construct the U-Net and VAE en-\ngines, further accelerating the inference speed. TensorRT\nis an optimization toolkit from NVIDIA that facilitates\nhigh-performance deep learning inference. It achieves this\nby performing several optimizations on neural networks,\nincluding layer fusion, precision calibration, kernel auto-\ntuning, dynamic tensor memory, and more. These optimiza-\ntions are designed to increase throughput and efficiency for\ndeep learning applications.\nTo optimize speed, we configured the system to use static\nbatch sizes and fixed input dimensions (height and width).\nThis approach ensures that the computational graph and\nmemory allocation are optimized for a specific input size,\nleading to faster processing times. However, this means\nthat if there is a requirement to process images with dif-\nferent shapes (i.e., varying heights and widths) or to use\ndifferent batch sizes (including those for denoising steps),\na new engine tailored to these specific dimensions must be\nbuilt. This is because the optimizations and configurations\napplied in TensorRT are specific to the initially defined di-\nmensions and batch size, and changing these parameters\nwould necessitate a reconfiguration and re-optimization of\nthe network within TensorRT.\nBesides, we employ a tiny AutoEncoder, which has been\nengineered as a streamlined and efficient counterpart to the\ntraditional Stable Diffusion AutoEncoder [11, 20]. TAESD\nexcels in rapidly converting latents into full-size images\nand accomplishing decoding processes with significantly\nreduced computational demands.\nFigure 6.\nGPU Usage comparison under static scene.\n(GPU:\nRTX3060, Number of frames: 20) The blue line represents the\nGPU usage with SSF, the orange line indicates GPU usage with-\nout SSF, and the red line denotes the Skip probability calculated\nbased on the cosine similarity between input frames. Additionally,\nthe top of the plot displays input images corresponding to the same\ntimestamps. In this case, the character in the input images is only\nblinking. Thus, this analysis compares GPU usage in a static sce-\nnario.\n4. Experiments\nWe implement StreamDiffusion pipeline upon LCM, LCM-\nLoRA [17, 18] and SD-turbo [21]. As a model acceler-\nator, we use TensorRT and for the light weight efficient\nVAE we use TAESD [11].\nOur pipeline is compatible\nto the consumer GPU. We test our pipeline on NVIDIA\nRTX4090 GPU, Intel Core i9-13900K CPU, Ubuntu22.04.3\nLTS, and NVIDIA RTX3060 GPU, Intel Core i7-12700K,\nWindows11 for image generation. We note that we evalu-\nate the throughput mainly via the average inference time per\nimage through processing 100 images.\n4.1. Quantitative Evaluation of Denoising Batch\nThe efficiency comparison between the denoising batch and\nthe original sequential U-Net loop is shown in Fig. 8. When\nimplementing a denoising batch strategy, we observe a sig-\nnificant improvement in processing time. It achieves a re-\nduction by half when compared to a conventional U-Net\nTable 1. Comparison of Average Inference Time at Different Denoising Steps with Speedup Factors. (GPU: NVIDIA RTX4090, CPU:\nCore i9-13900K, OS: Ubuntu22.04.3 LTS)\nDenoising Step\nStreamDiffusion (ms)\nStreamDiffusion w/o TRT (ms)\nAutoPipeline Image2Image (ms)\n1\n10.65 (59.6x)\n21.34 (29.7x)\n634.40 (1x)\n2\n16.74 (39.3x)\n30.61 (21.3x)\n652.66 (1x)\n4\n26.93 (25.8x)\n48.15 (14.4x)\n695.20 (1x)\n10\n62.00 (13.0x)\n96.94 (8.3x)\n803.23 (1x)\nTable 2. Comparison of Average Inference Time at Different Denoising Steps among Different CFG Methods\nDenoising Step\nSelf-Negative RCFG (ms)\nOnetime-Negative RCFG (ms)\nNormal CFG (ms)\n1\n11.04 (1.52x)\n16.55 (1.01x)\n16.74 (1x)\n2\n16.61 (1.64x)\n20.64 (1.32x)\n27.18 (1x)\n3\n20.64 (1.74x)\n27.25 (1.32x)\n35.91 (1x)\n4\n26.19 (1.90x)\n31.65 (1.57x)\n49.71 (1x)\n5\n31.47 (2.05x)\n36.04 (1.79x)\n64.64 (1x)\nFigure 7. GPU Usage comparison under dynamic scene. (GPU:\nRTX4090, Number of frames: 1000) The blue line represents the\nGPU usage with SSF, the orange line indicates GPU usage without\nSSF, and the red line denotes the Skip probability calculated based\non the cosine similarity between input frames. Additionally, the\ntop of the plot displays input images corresponding to the same\ntimestamps. In this case, the character in the input images keeps\nmoving dynamically. Thus, this analysis compares GPU usage in\na dynamic scenario.\nFigure 8.\nAverage inference time comparison between Stream\nBatch and normal sequential denoising without TensorRT.\nFigure 9.\nAverage inference time comparison between Stream\nBatch and normal sequential denoising with using TensorRT\nloop at sequential denoising steps. Even though applying\nTensorRT, the accelerate tool for neural modules, our pro-\nposed Stream Batch still boosts the efficiency of original\nsequential diffusion pipeline by a large margin at different\ndenoising steps.\nBesides, we compare our method with the AutoPipeline-\nForImage2Image, which is a pipeline developed by Hug-\ngingface diffusers 1.\nThe average inference time com-\nparison is presented in Table.\n1.\nOur pipeline demon-\nstrates a substantial speed increase.\nWhen we use Ten-\nsorRT, StreamDiffusion achieves a minimum speed-up 13.0\ntimes when running the 10 denoising steps, and reaching up\nto 59.6 times in scenarios when involving a single denois-\ning step. Even though without TensorRT, StreamDiffusion\nachieves 29.7 times speed up compared to AutoPipeline\nwhen using one step denoising, and 8.3 times speedup at\n10 step denoising.\nTable. 2 presents a comparison of the inference times\n1https://github.com/huggingface/diffusers\nFigure 10. Results using no CFG, standard CFG, and RCFG with Self-Negative and Onetime-Negative approaches. When compared to\ncases where CFG is not utilized, the cases with CFG utilized can intensify the impact of prompts. In the proposed method RCFG, a more\npronounced influence of prompts was observed. Both CFG and RCFG use guidance scale \u03b3 = 1.4. For RCFG, the first two rows use\nmagnitude modelation coefficient \u03b4 = 1.0, and the third row uses \u03b4 = 0.5.\nfor StreamDiffusion pipelines with RCFG and conven-\ntional CFG. The additional computations required to apply\nSelf-Negative RCFG are merely lightweight vector opera-\ntions, resulting in negligible changes in inference time com-\npared to when Self-Negative is not used. When employ-\ning Onetime-Negative RCFG, additional UNet computa-\ntions are necessary for the first step of the denoising process.\nTherefore, One-time-negative RCFG and conventional CFG\nhave almost identical inference times for a single denoising\nstep case. However, as the number of denoising steps in-\ncreases, the difference in inference time from conventional\nCFG to both Self-Negative and Onetime-Negative RCFG\nbecomes more pronounced. At denoising step 5, a speed im-\nprovement of 2.05x is observed with Self-Negative RCFG\nand 1.79x with Onetime-Negative RCFG, compared to con-\nventional CFG.\n4.2. Energy Consumption\nWe then conduct a comprehensive evaluation of the energy\nconsumption associated with our proposed stochastic simi-\nlarity filter (SSF), as depicted in Figure. 6 and Figure. 7.\nThese figures provide the GPU utilization patterns when\nSSF (Threshold \u03b7 set at 0.98) is applied to input videos\ncontaining scenes with periodic static characteristics. The\ncomparative analysis reveals that the incorporation of SSF\nsignificantly mitigates GPU usage in instances where the in-\nput images are predominantly static and demonstrate a high\ndegree of similarity.\nFigure. 6 delineates the results derived from a metic-\nulously executed two-denoise-step img2img experiment.\nThis experiment was conducted on a 20-frame video se-\nquence, employing NVIDIA RTX3060 graphics processing\nunits with or without the integration of SSF. The experiment\nFigure 11. Text-to-Image generation results. We use four step denoising for LCM-LoRA, and one step denoising for sd-turbo. Our\nStreamDiffusion enables the real-time generation of images with quality comparable to those produced using Diffusers AutoPipeline\nText2Image.\nresults indicate a substantial decrease in average power con-\nsumption from 85.96w to 35.91w on one RTX3060 GPU.\nUsing the same static scene input video with one NVIDIA\nRTX4090GPU, the power consumption was reduced from\n238.68w to 119.77w.\nFurthermore, Figure. 7 expounds on the findings from\na similar two-denoise-step img2img experiment using one\nRTX4090GPU. This time the evaluation of energy con-\nsumption is performed on a 1000-frame video featuring dy-\nnamic scenes. Remarkably, even under drastically dynamic\nconditions, the SSF efficiently extracted several frames ex-\nhibiting similarity from the dynamic sequence. This pro-\ncess results in a noteworthy reduction in average power con-\nsumption, from 236.13w to 199.38w. These findings under-\nscore the efficacy of the Stochastic Similarity Filter in en-\nhancing energy efficiency, particularly in scenarios involv-\ning static or minimally varying visual content.\n4.3. Ablation study\nIn our ablation study, as summarized in Table 3, we evalu-\nate the average inference time of our proposed method un-\nder various configurations to understand the contribution of\neach component. Our proposed StreamDiffusion achieves\nan average inference time of 10.98/9.42 ms and 26.93/26.30\nms for denoising steps 1 and 4 on image-to-image/text-to-\nimage generation, respectively. When the stream batch pro-\ncessing is removed (\u2019w/o stream batch\u2019), we observe a large\ntime consumption increase, especially at 4 denoising steps.\nThe absence of TensorRT (\u2019w/o TRT\u2019) leads to a more in-\ncrease in time cost. The removal of pre-computation also\nresults in increased time cost but not much. We attribute\nthe reason to the limited number of key-value computations\nin StableDiffusion. Besides, the exclusion of input-output\nqueue (\u2019w/o IO queue\u2019) also demonstrates an impact on av-\nerage inference time, which mainly aims to optimize the\nTable 3. The effect of different modules on average inference time\nat different denoising steps. The different module ablations are\nvalidated on image-to-image generation.\nMethod\nDenoising Step\nLatency [ms]\nOurs (txt2img)\n1\n9.42\n4\n26.30\nOurs (img2img)\n1\n10.65\n4\n26.93\nw/o stream batch\n1\n10.70\n4\n36.54\nw/o TRT\n1\n21.34\n4\n48.15\nw/o Pre-computation\n1\n13.63\n4\n31.16\nw/o TinyAutoEncoder\n1\n49.72\n4\n65.48\nw/o IO queue\n1\n15.13\n4\n29.72\nw/o any optimization\n1\n76.78\n4\n134.37\nwith SSF\n1\n10.66\n4\n27.02\nparallelization issue results from pre- and post-processing.\nIn the AutoPipelineImage2Image\u2019s adding noise function,\nthe precision of tensors is converted from fp32 to fp16 for\neach request, leading to a decrease in speed. In contrast, the\nStreamDiffusion pipeline standardizes the precision of vari-\nables and computational devices beforehand. It does not\nperform tensor precision conversion or computation device\ntransfers during inference. Consequently, even without any\noptimization (\u2019w/o any optimization\u2019), our pipeline signifi-\ncantly outperforms the AutoPipelineImage2Image in terms\nof speed. We also evaluate the impact on the inference time\nof our pipeline with additional modules such as SSF and\nSelf-Negative RCFG. As shown in Table. 2 and Table. 3,\nthese additional modules, which only perform lightweight\nmatrix and vector operations, do not have a noticeable ef-\nfect on inference time.\n4.4. Qualitative Results\nThe generation results using the StreamDiffusion pipeline\nfor real-time image-to-image transformation are presented\nin Fig.\n1.\nThis pipeline enables image generation with\nvery low throughput from input images received in real time\nfrom cameras or screen capture devices. At the same time, it\nis capable of producing high quality images that effectively\nalign to the specified prompt conditions. These capabilities\ndemonstrate the applicability of our pipeline in various real\ntime applications, such as real time game graphic rendering,\ngenerative camera effect filters, real time face conversion,\nand AI-assisted drawing.\nThe alignment of generated images to prompt condition-\ning using Residual Classifier-Free Guidance (RCFG) is de-\npicted in Fig. 10. The generated images, without using any\nform of CFG, exhibit weak alignment to the prompt, partic-\nularly in aspects like color changes or the addition of non-\nexistent elements, which are not effectively implemented.\nIn contrast, the use of CFG or RCFG enhances the abil-\nity to modify original images, such as changing hair color,\nadding body patterns, and even incorporating objects like\nglasses. Notably, the use of RCFG results in a stronger in-\nfluence of the prompt compared to standard CFG. RCFG,\nalthough limited to image-to-image applications, can com-\npute the vector for negative conditioning while continuously\nreferencing the latent value of the input image and the ini-\ntially sampled noise. This approach yields more consistent\ndirections for the negative conditioning vector compared\nto the standard CFG, which uses UNet at every denoising\nstep to calculate the negative conditioning vector. Conse-\nquently, this leads to more pronounced changes from the\noriginal image. However, there is a trade-off in terms of\nthe stability of the generated results. While Self-Negative\nRCFG enhances the prompt\u2019s effectiveness, it also has the\ndrawback of increasing the contrast of the generated im-\nages. To address this, adjusting the delta in Eq. 7 can\nmodulate the magnitude of the virtual residual noise vec-\ntor, thereby mitigating the rise in contrast. Additionally,\nusing Onetime-Negative RCFG with appropriately chosen\nnegative prompts can effectively mitigate contrast increases\nwhile improving prompt adherence, as observed in Fig. 10.\nThis approach allows the generated images to blend more\nnaturally with the original image.\nFinally, the quality of standard text-to-image generation\nresults is demonstrated in Fig.\n11.\nUsing the sd-turbo\nmodel, high-quality images like those shown in Fig. 11\ncan be generated in just one step. When images are pro-\nduced using our proposed StreamDiffusion pipeline and sd-\nturbo model in an environment with GPU: RTX 4090, CPU:\nCore i9-13900K, and OS: Ubuntu 22.04.3 LTS, it\u2019s feasi-\nble to generate such high-quality images at a rate exceeding\n100fps. Furthermore, by increasing the batch size of im-\nages generated at once to 12, our pipeline can continuously\nproduce approximately 150 images per second. The images\nenclosed in red frames shown Fig. 11 are generated in four\nsteps using community models merged with LCM-LoRA.\nWhile these LCM models require more than 1 step for high\nquality image generation, resulting in a reduction of speed\nto around 40fps, these LCM-LoRA based models offer the\nflexibility of utilizing any base model, enabling the genera-\ntion of images with diverse expressions.\n5. Conclusion\nWe propose StreamDiffusion, a pipeline-level solution for\ninteractive diffusion generation. StreamDiffusion consists\nof several optimization strategies for both throughput and\nGPU usage, including stream batch, residual classifier-free\ngudiance (RCFG), IO-queue for parallelization, stochastic\nsimilarity filter, pre-computation, Tiny AutoEncoder and\nthe use of model acceleration tool. The synergistic com-\nbination of these elements results in a marked improvement\nin efficiency. Specifically, StreamDiffusion achieves up to\n91.07 frames per second (fps) on a standard consumer-grade\nGPU for image generation tasks. This performance level is\nparticularly beneficial for a variety of applications, includ-\ning but not limited to the Metaverse, online video stream-\ning, and broadcasting sectors. Furthermore, StreamDiffu-\nsion demonstrates a significant reduction in GPU power\nconsumption, achieving at least a 1.99x decrease. This no-\ntable efficiency gain underscores StreamDiffusion\u2019s poten-\ntial for commercial application, offering a compelling so-\nlution for energy-conscious, high-performance computing\nenvironments.\n6. Acknowledgments\nWe sincerely thank Taku Fujimoto and Huggingface team\nfor their invaluable feedback, courteous support, and in-\nsightful discussions.\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 18208\u201318218, 2022.\n3\n[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM Trans. Graph., 42(4), 2023. 3\n[3] James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng\nWang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee,\nYufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu,\nYunxinJiao, and Aditya Ramesh. Improving image gener-\nation with better captions. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023. 3\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 3\n[6] Yilun Du, Shuang Li, and Igor Mordatch. Compositional\nvisual generation with energy based models. In Advances in\nNeural Information Processing Systems, pages 6637\u20136647.\nCurran Associates, Inc., 2020. 4\n[7] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[9] Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen,\nand Xianglong Liu.\nTfmq-dm: Temporal feature main-\ntenance quantization for diffusion models.\narXiv preprint\narXiv:2311.16503, 2023. 2\n[10] Alexia Jolicoeur-Martineau, Ke Li, R\u00b4emi Pich\u00b4e-Taillefer,\nTal Kachman, and Ioannis Mitliagkas. Gotta go fast when\ngenerating data with score-based models.\narXiv preprint\narXiv:2105.14080, 2021. 3\n[11] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes, 2022. 7\n[12] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen\nDong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.\nQ-diffusion: Quantizing diffusion models. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 17535\u201317545, 2023. 2\n[13] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and\nQiang Liu. Instaflow: One step is enough for high-quality\ndiffusion-based text-to-image generation.\narXiv preprint\narXiv:2309.06380, 2023. 2\n[14] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. Advances\nin Neural Information Processing Systems, 35:5775\u20135787,\n2022. 2, 3\n[15] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 3\n[16] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao.\nLatent consistency models:\nSynthesizing high-\nresolution images with few-step inference, 2023. 2\n[17] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von\nPlaten, Apolin\u00b4ario Passos, Longbo Huang, Jian Li, and Hang\nZhao. Lcm-lora: A universal stable-diffusion acceleration\nmodule. arXiv preprint arXiv:2311.05556, 2023. 2, 3, 7\n[18] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von\nPlaten, Apolin\u00b4ario Passos, Longbo Huang, Jian Li, and Hang\nZhao. Lcm-lora: A universal stable-diffusion acceleration\nmodule, 2023. 7\n[19] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 3\n[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 3, 7\n[21] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 7\n[22] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. 2022. 3\n[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho\u2020, David\nFleet, and Mohammad Norouzi.\nImagen: unprecedented\nphotorealism \u00d7 deep level of language understanding. 2022.\n2\n[24] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 3\n[25] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2, 3\n[26] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever.\nConsistency\nmodels.\narXiv\npreprint\narXiv:2303.01469, 2023. 2, 3, 4\n[27] Daniel Watson, Jonathan Ho, Mohammad Norouzi, and\nWilliam Chan. Learning to efficiently sample from diffu-\nsion probabilistic models. arXiv preprint arXiv:2106.03802,\n2021. 3\n[28] Tianwei Yin, Micha\u00a8el Gharbi, Richard Zhang, Eli Shecht-\nman, Fr\u00b4edo Durand, William T. Freeman, and Taesung Park.\nOne-step diffusion with distribution matching distillation.\narXiv, 2023. 2\n[29] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Aziz-\nzadenesheli, and Anima Anandkumar. Fast sampling of dif-\nfusion models via operator learning. In International Con-\nference on Machine Learning, pages 42390\u201342402. PMLR,\n2023. 3\n"
  },
  {
    "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU",
    "link": "https://arxiv.org/pdf/2312.12456.pdf",
    "upvote": "39",
    "text": "arXiv:2312.12456v1  [cs.LG]  16 Dec 2023\nPowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU\nYixin Song, Zeyu Mi\u2217, Haotong Xie and Haibo Chen\nInstitute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University\nAbstract\nThis paper introduces PowerInfer,a high-speed Large Lan-\nguage Model (LLM) inference engine on a personal com-\nputer (PC) equipped with a single consumer-grade GPU. The\nkey underlying the design of PowerInfer is exploiting the\nhigh locality inherent in LLM inference, characterized by a\npower-law distribution in neuron activation. This distribution\nindicates that a small subset of neurons, termed hot neurons,\nare consistently activated across inputs, while the majority,\ncold neurons, vary based on speci\ufb01c inputs. PowerInfer ex-\nploits such an insight to design a GPU-CPU hybrid inference\nengine: hot-activated neurons are preloaded onto the GPU\nfor fast access, while cold-activated neurons are computed on\nthe CPU, thus signi\ufb01cantly reducing GPU memory demands\nand CPU-GPU data transfers. PowerInfer further integrates\nadaptive predictors and neuron-aware sparse operators, opti-\nmizing the ef\ufb01ciency of neuron activation and computational\nsparsity. Evaluation shows that PowerInfer attains an average\ntoken generation rate of 13.20 tokens/s, with a peak of 29.08\ntokens/s, across various LLMs (including OPT-175B) on a\nsingle NVIDIA RTX 4090 GPU, only 18% lower than that\nachieved by a top-tier server-grade A100 GPU. This signi\ufb01-\ncantly outperforms llama.cpp by up to 11.69\u00d7 while retain-\ning model accuracy.\n1\nIntroduction\nGenerative large language models (LLMs) have garnered\nattention for their remarkable capabilities in creative writ-\ning, advanced code generation, and sophisticated natural lan-\nguage processing tasks [5,42,49]. These models, widely de-\nployed in data centers equipped with high-end and expensive\nserver-grade GPUs, have signi\ufb01cantly in\ufb02uenced our daily\nlives and work practices. Meanwhile, there is an emerging\ntrend of running LLMs on more accessible local platforms,\nparticularly personal computers (PCs) with consumer-grade\nGPUs. This evolution is driven by the need for enhanced\ndata privacy [25], model customization [22], and reduced in-\nference costs [42]. In contrast to data-center deployments,\nwhich prioritize high throughput [18, 37, 47], local deploy-\nments focus on low latency in processing small batches.\nNonetheless, deploying LLMs on consumer-grade GPUs\npresents signi\ufb01cant challenges due to their substantial mem-\n\u2217Zeyu Mi (yzmizeyu@sjtu.edu.cn) is the corresponding author.\nory requirements. LLMs, typically functioning as autoregres-\nsive Transformers,sequentially generate text token-by-token,\neach needing to access the entire model consisting of hun-\ndreds of billions of parameters. Therefore, the inference pro-\ncess is fundamentally constrained by the GPU\u2019s memory ca-\npacity. This limitation is particularly acute in local deploy-\nments where the processing of individual requests (often just\none at a time) [6] leaves minimal opportunity for parallel pro-\ncessing.\nExisting approaches to such memory issues include model\ncompression and of\ufb02oading. Compression techniques like\nquantization [12, 46], distillation [48], and pruning [23] re-\nduce the model size. However, even deeply compressed\nmodels remain too large for consumer-grade GPUs. For in-\nstance, an OPT-66B model with 4-bit precision demands\napproximately 40GB of memory just to load its parame-\nters [20], exceeding the capacity of even high-end GPUs like\nthe NVIDIA RTX 4090. Model of\ufb02oading, which partitions\nthe model between GPU and CPU at the Transformer layer\nlevel [3,14,37]. State-of-the-art systems like llama.cpp [14]\ndistribute layers between CPU and GPU memories, leverag-\ning both for inference, thus reducing the GPU resources re-\nquired. However, this method is hindered by the slow PCIe\ninterconnect and the CPUs\u2019 limited computational capabili-\nties, resulting in high inference latency.\nIn this paper, we argue that the key reason for memory\nissues in LLM inference is the locality mismatch between\nhardware architecture and the characteristics of LLM in-\nference. Current hardware architectures are designed with\na memory hierarchy optimized for data locality. Ideally, a\nsmall, frequently accessed working set should be stored in\nthe GPU, which offers higher memory bandwidth but limited\ncapacity. In contrast, larger, less frequently accessed data are\nbetter suited for CPUs, which provide more extensive mem-\nory capacity but lower bandwidth. Nevertheless, the vast vol-\nume of parameters required for each LLM inference iteration\nleads to a working set that is too large for a single GPU, thus\nimpeding ef\ufb01cient locality exploitation.\nWe have observed that LLM inference inherently exhibits\nhigh locality. Speci\ufb01cally, during each inference iteration, a\nlimited number of neurons1 are activated, signi\ufb01cantly in\ufb02u-\n1This paper de\ufb01nes a neuron as a speci\ufb01c row/column in a weight matrix.\n1\nencing the outcome of token inference. These activations,\nwhich are input-speci\ufb01c, can be accurately predicted dur-\ning runtime. For example, in the OPT model, less than 10%\nof the elements in the activation map are non-zero, and\nthese can be predicted with more than 93% accuracy at run-\ntime [21]. Notably, neuron activation in an LLM follows a\nskewed power-law distribution: a small subset of neurons\nconsistently contributes to the majority of activations (over\n80%) across various inputs (hot-activated), while the major-\nity are involved in the remaining activations, which are deter-\nmined based on the inputs at runtime (cold-activated).\nBuilding on the locality insights, we introduce PowerIn-\nfer, an ef\ufb01cient LLM inference system optimized for local\ndeployments using a single consumer-grade GPU. The key\nidea of PowerInfer is to exploit the locality in LLM infer-\nence by assigning the minor hot neurons to the GPU, while\ncold neurons, which constitute the majority, are managed by\nthe CPU. PowerInfer preselects and preloads hot-activated\nneurons onto the GPU of\ufb02ine and leverages online predic-\ntors during runtime to identify activated neurons. This ap-\nproach allows the GPU and CPU to independently process\ntheir respective sets of neurons, thereby minimizing the need\nfor costly PCIe data transfers.\nHowever, there are signi\ufb01cant challenges that complicate\nthe design of PowerInfer. First, the online predictors, which\nare essential for identifying active neurons in LLM layers\nand are typically situated on the GPU, occupy a considerable\namount of GPU memory. This memory could otherwise be\nused for the LLM. To address this, PowerInfer introduces an\nadaptive method for constructing smaller predictors for lay-\ners with higher activation sparsity and skewness. This itera-\ntive process reduces the size of the predictors while maintain-\ning their accuracy, thus freeing up GPU memory for LLM\ninferences.\nSecond, leveraging LLM sparsity requires the use of\nsparse operators. Conventional libraries like cuSPARSE [30]\nare not optimal due to their general-purpose design, which in-\ncludes tracking each non-zero element and converting dense\nmatrices into sparse formats [45,51]. In contrast, PowerInfer\ndesigns neuron-aware sparse operators that directly interact\nwith individual neurons, thereby bypassing operations on en-\ntire matrices. This approach enables ef\ufb01cient matrix-vector\nmultiplication at the neuron level and removes the need for\nspeci\ufb01c sparse format conversions.\nLastly, the optimal placement of activated neurons be-\ntween the GPU and CPU in PowerInfer is a complex task. It\ninvolves evaluating each neuron\u2019s activation rate, intra-layer\ncommunication, and available hardware resources like GPU\nmemory sizes. To effectively manage this, PowerInfer uti-\nlizes an of\ufb02ine phase to generate a neuron placement pol-\nicy. This policy uses a metric that measures each neuron\u2019s\nimpact on LLM inference outcomes and is framed as an in-\nteger linear programming problem. The policy formulation\nconsiders factors such as neuron activation frequencies and\nthe bandwidth hierarchy of CPU and GPU architectures.\nThe online inference engine of PowerInfer was imple-\nmented by extending llama.cpp with an additional 4,200\nlines of C++ and CUDA code. Its of\ufb02ine component, com-\nprising a pro\ufb01ler and a solver, builds upon the transformers\nframework [44] with approximately 400 lines of Python code.\nPowerInferis compatible with various popular LLM families,\nincluding OPT (7B-175B), LLaMA (7B-70B), and Falcon-\n40B, and supports consumer-grade GPUs like the NVIDIA\nRTX 4090 and NVIDIA RTX 2080Ti.\nPerformance evaluation reveals that PowerInfer, when de-\nployed on a PC equipped with a single NVIDIA RTX\n4090 GPU, delivers an average generation speed of 13.20\ntokens/s for quantized models and 8.32 tokens/s for non-\nquantized models, maintaining model accuracy. These re-\nsults signi\ufb01cantly surpass llama.cpp\u2019s performance, exhibit-\ning up to 8.00\u00d7 and 11.69\u00d7 improvements for quantized and\nnon-quantized models, respectively. Signi\ufb01cantly, the infer-\nence speed achieved on an NVIDIA RTX 4090 GPU (priced\nat approximately $2,000) is only 18% slower compared to\nthe performance on a top-tier A100 GPU (costing around\n$20,000) that can fully accommodate the model. PowerIn-\nfer\u2019s source code is publicly available at https://github.\ncom/SJTU-IPADS/PowerInfer.\n2\nBackground and Motivation\nEmbedding\nDecoding\nTransformer\nTransformer\nEmbedding\nDecoding\nTransformer\nTransformer\nI love reading\nOSDI\nOSDI\nPapers\n\u2026\n\u2026\nEmbedding\nDecoding\nTransformer\nTransformer\npapers\n<EOS>\n\u2026\nPrompt Phase\nGeneration Phase\nFigure 1: The inference procedure of an LLM.\n2.1\nLLM Inference & Architecture\nLLM inference, an autoregressive model, generates each to-\nken based on previous ones. The process, illustrated in Fig-\nure 1, starts with a prompt (e.g., \"I love reading\") and un-\nfolds in two phases: \ufb01rst, the prompt phase outputs an initial\ntoken (\"OSDI\"), then the generation phase sequentially pro-\nduces tokens until a maximum limit or an end-of-sequence\n(<EOS>) token is reached. Each token generation, an infer-\nence iteration, requires running the full LLM model.\nThe LLM architecture includes multiple Transformer lay-\ners, each comprising a self-attention and an MLP (Multi-\nLayer Perceptron) block (see Figure 2, left). The self-\nattention block generates embedding vectors by capturing\nthe relationships among input tokens. In this process, dif-\nferent heads focus on extracting distinct feature information.\n2\nFC1 Weights\nSelf-Attention Block\nFC1 Layer\nReLU\nFC2 Layer\n\u2026\n\u00d7\n=\nReLU\nFC2 Weights\n=\n\u00d7\n\u2026\nMLP Block\nFC1 Input\nFC1 Output\nFC2 input\nFC2 Output\nFigure 2: The architecture of a Transformer layer and how neurons\nare sparsely activated in FC1 and FC2 layers due to the ReLU func-\ntion. The neurons that are activated are represented as green rows or\ncolumns encircled by red lines. The output vector from FC1 is then\nsupplied to FC2 as its input vector.\nThe computation results from these different heads are ag-\ngregated and then utilized as the input for the MLP block.\nThe MLP block applies non-linear transformations via fully\nconnected layers and activation functions to re\ufb01ne the input\nsequence representation. The output either advances to sub-\nsequent layers or forms the LLM\u2019s \ufb01nal output.\nIn Figure 2 (right), the MLP block\u2019s layers, FC1 and FC2,\ngenerate vectors through matrix multiplication. Each output\nelement comes from the dot product of an input vector and a\nneuron (a row/column in a weight matrix). Activation func-\ntions like ReLU [1] act as gates to selectively retain or dis-\ncard values in a vector, in\ufb02uencing neuron activations in FC1\nand FC2. For example, ReLU in this \ufb01gure \ufb01lters out nega-\ntive values, allowing only positively valued neurons in FC1\nto in\ufb02uence the output. These neurons, which contribute to\nthe output, are considered activated in this paper. Similarly,\nthese values also affect which neurons in FC2 are activated\nand involved in the computation of its output vector.\nActivation Sparsity. Recent studies have revealed that\nLLM inference shows a notable sparsity in neuron activa-\ntion [19,21,50]. For example, we observe that approximately\n80% of neurons in the OPT-30B model remain inactivated\nduring the inference. This phenomenon of activation spar-\nsity exists in both self-attention and MLP blocks. In self-\nattention blocks, nearly half of the attention heads (neurons)\nmake minimal contributions, leading to their high sparsity.\nThe sparsity observed within the MLP blocks is primarily\nattributed to the characteristics of the activation functions.\nCrucially, the activation sparsity is input-speci\ufb01c,meaning\nthat the activation of speci\ufb01c neurons is directly in\ufb02uenced\nby the current input and cannot be predetermined before the\nmodel\u2019s inference iteration begins. While it is not feasible to\nknow which neurons will be activated before the entire model\nruns, it is possible to predict neuron activations a few layers\nin advance within the ongoing model iteration. DejaVu [21],\nfor instance, utilizes MLP-based predictors during inference,\nachieving a remarkable accuracy rate of at least 93% in pre-\ndicting neuron activation.\nL4\nL5\nL6\n\u2026\nLn\nCPU Mem\nGPU Mem\nCPU\nGPU\nL3\nL2\nL1\nPCIe\nL4\n1\n2\nCPU \nResults\nL1\nL2\nL3\n\u2026\nLn-3\nCPU Mem\nGPU Mem\nCPU\nGPU\nLn-2\nLn-1\nLn\nPCIe\nCPU \nResults\n2\n3\n(a) GPU-Centric O\ufb04oading\n(b) GPU-CPU Hybrid O\ufb04oading\n1\n3\nFigure 3: Typical existing of\ufb02oading solutions. (a) shows a GPU-\ncentric approach, while (b) is the CPU-GPU hybrid of\ufb02oading ap-\nproach.\n2.2\nOf\ufb02oading-based LLM Serving\n 0\n 800\n 1600\n 2400\n 3200\n 4000\n1\n4\n8\n16\nLatency per iteration (ms)\nFlexGen\nDejaVu(UM)\nllama.cpp\n(a)\n90\n100\n~\n~\nFlexGen-PCIe\nFlexGen-GPU\nDejaVu(UM)-PCIe\nDejaVu(UM)-GPU\nllama.cpp-CPU\nllama.cpp-GPU\n1\n4\n8\n16\n0\n4\n8\nBreakdown (%)\n0.5\n0.5\n0.5\n0.5\n0.4\n0.4\n0.4\n0.4\n2.4\n1.7\n1.1\n0.6\n~\n~\n(b)\nFigure 4: Performance comparison and analysis for serving OPT-\n30B on NVIDIA RTX 4090 GPU. The yellow blocks refer to Flex-\nGen, the gray blocks refer to DejaVu (UM) and the blue blocks refer\nto llama.cpp. (a) The Y-axis indicates execution time for one itera-\ntion and the X-axis represents batch sizes for input. (b) The Y-axis\nindicates the proportion of execution time, and the X-axis indicates\nbatch sizes for input.\nCurrent model compression techniques are inadequate\nfor \ufb01tting large language models (LLMs) within resource-\nlimited consumer-grade GPUs. In contrast, the of\ufb02oading\ntechnique, which leverages the CPU\u2019s additional computa-\ntional and memory resources, presents a more viable solu-\ntion for accommodating LLMs on such hardware. Figure 3\nillustrates two main of\ufb02oading approaches:\nGPU-centric of\ufb02oading utilizes CPU memory to store\nportions of the model parameters that exceed the GPU\u2019s ca-\npacity. During each iteration, as depicted in Figure 3a), it pro-\ncesses the parameters located in the GPU memory, transfer-\nring more from the CPU as needed. This strategy enables\nthe inference of LLMs of varying sizes, provided that suf\ufb01-\ncient combined CPU memory and hard disk storage are avail-\nable. FlexGen [37] is a typical example that adopts a zig-zag\nscheduling approach to prioritize throughput over latency,\nprocessing batches sequentially for each layer. Nonetheless,\nthis method leads to substantial per-token latency in latency-\nsensitive scenarios (Figure 4a), mainly due to frequent data\ntransfers between GPU and CPU, especially with batch sizes\nof one. Over 99.5% of processing time is consumed by trans-\nferring LLM weights from CPU to GPU, signi\ufb01cantly im-\npacting overall latency, as illustrated in Figure 4b.\nDejaVu [21] accelerates LLM inference by using activa-\ntion sparsity. It selectively processes only those neurons that\nare predicted to be activated (called predicted neurons for\nbrevity), while bypassing the inactivated ones. However, this\napproach, initially designed for data center inference, strug-\ngles on consumer-grade GPUs that cannot accommodate full-\n3\nscale LLMs. The key challenge with DejaVu in such contexts\nstems from the need to frequently transfer activated neurons\nfrom the CPU to the GPU during runtime. For LLMs like\nOPT-30B that exceed GPU memory limits, DejaVu2, albeit\nreducing the computational load on the GPU, is constrained\nby the data transfer procedure (Figure 4a). Consequently, as\nshown in Figure 4a, DejaVu experiences signi\ufb01cant inference\nlatency, comparable to that of FlexGen.\nHybrid of\ufb02oading distributes model parameters between\nGPU and CPU, splitting them at the Transformer layer level\nas shown in llama.cpp [14] (Figure 3b). The CPU processes\nits layers \ufb01rst, then sends intermediate results to the GPU\nfor token generation. This of\ufb02oading method reduces infer-\nence latency to around 600ms (Figure 4a) by minimizing\ndata transfer and mitigating slow PCIe bandwidth.\nHowever, hybrid of\ufb02oading still faces the locality mis-\nmatch issue, leading to suboptimal latency. Each inference\niteration accesses the entire model, resulting in poor locality\nfor hierarchical GPU-CPU memory structures. GPUs, while\ncomputationally powerful,are constrained by memory capac-\nity. For instance, a 30B-parametermodel on a 24GB NVIDIA\nRTX 4090 GPU means only 37% of the model is on the GPU,\nshifting most computational tasks to the CPU. The CPU,\nwith higher memory but lower computational power, ends up\nhandling 98% of the total computational load (Figure 4b).\n3\nInsights into Locality in LLM Inference\nThis section introduces our insights into locality in the LLM\ninference procedure, highlighting two distinctive features.\n3.1\nInsight-1: Power-law Activation\n 0\n 20\n 40\n 60\n 80\n 100\n 0\n 20\n 40\n 60\n 80\n 100\nCDF(%)\nNeuron proportion (%)\nLLaMA(ReGLU)-70B\nOPT-30B\n26\n43\n(a)\n 0\n 20\n 40\n 60\n 80\n 100\nNeuron proportion (%)\nLLaMA(ReGLU)-70B\nOPT-30B\n26\n17\n(b)\nFigure 5: Cumulative distribution function (CDF) of neuron acti-\nvation in OPT-30B and LLaMA(ReGLU)-70B. (a) CDF in a single\nMLP layer. (b) CDF across the entire model. The X-axis shows neu-\nron proportion. The Y-axis represents the CDF of neuron activation.\nLLM inference exhibits a high degree of locality, indi-\ncating that a consistent group of neurons is frequently ac-\ntivated. Notwithstanding the input dependence of LLM ac-\ntivation sparsity, a power-law distribution is evident among\nactivated neurons. Figure 5a reveals that in the MLP layers\nof OPT-30B and LLaMA (ReGLU)-70B, 26% and 43% of\nneurons respectively are responsible for 80% of total activa-\ntions. These are termed hot-activated neurons. Conversely,\nthe activation of the remaining 74% and 57% of neurons is\ninput-dependent, classifying them as cold-activated neurons.\n2Since DejaVu only works for GPU, we modi\ufb01ed it by using NVIDIA\nUni\ufb01ed Memory (UM) [29] to fetch parameters from CPU memory.\nThis high locality is not con\ufb01ned to a single layer but\nextends throughout the model. As illustrated in Figure 5b,\napproximately 17% of neurons in OPT-30B and 26% in\nLLaMA (ReGLU)-70B are responsible for 80% of the total\nactivations across all layers.\n3.2\nInsight-2: Fast In-CPU Computation\n 0\n 1\n 2\n1\n2\n4\n8\n16\n32\nTime(ms)\nBatch size\nLoad-And-Execute\nDirect-Execute\n(a) MLP layer\n 0\n 2\n 4\n1\n2\n4\n8\n16\n32\nBatch size\nLoad-And-Execute\nDirect-Execute\n(b) Attention layer\nFigure 6: Comparison of execution time for load-then-execute ver-\nsus direct-execute methods when 10% and 60% neuron weights of\none MLP and attention layer in OPT-30B are CPU-resident. The\nX-axis shows input batch sizes, and the Y-axis measures execution\ntime (ms). Load-then-execute involves transferring these neuron\nweights to GPU memory for computation, whereas direct-execute\ncomputes them directly on the CPU.\nIf activated neurons reside in CPU memory, computing\nthem on the CPU is faster than transferring them to the GPU,\nespecially with the small numberof activated neurons and the\nsmall batch sizes typical in local deployments. Modern CPUs\nwith vector extensions can ef\ufb01ciently handle such smaller\nmatrix computations.\nWe compared the time to load and compute 10%3 of the\nMLP layer and 60% of attention layer\u2019s CPU-side neurons on\nthe GPU versus direct CPU execution in OPT-30B. Results\nin Figure 6 indicate that for batch sizes under 32, the time\ntaken to transfer the weights of these neurons and compute\nthem on the GPU (NVIDIA RTX 4090) exceeds the time\nrequired for calculation directly on the CPU using the AVX2\nvector extension.\n4\nPowerInfer Overview\nThis paper introduces PowerInfer, a low-latency LLM in-\nference system deployed in a PC equipped with a single\nconsumer-grade GPU. PowerInfer proposes a neuron-aware\nof\ufb02oading strategy and an inference engine by fully leverag-\ning the high locality insights described in \u00a73. It utilizes both\nGPU and CPU for weight storage, accommodating LLMs of\nvarious sizes. This of\ufb02oading approach, based on Insight-\n1, effectively exploits the power-law distribution of LLM\ninference. Speci\ufb01cally, PowerInfer preloads the GPU with\nweights for neurons that activate frequently, while less active\nneurons\u2019 weights are kept on the CPU.\nTo reduce inference latency, the inference engine com-\nputes only neurons predicted as active by online predictors,\nskipping most inactive ones. Moreover, the preloading strat-\negy enables PowerInfer to allocate the bulk of inference tasks\n3While Insight-1 indicates that 43% of neurons account for 80% of the\ntotal activations in a single MLP layer, it is typically found that only about\n10% of its neurons are activated during an individual inference iteration.\n4\n\u2026\nPro\ufb01ler & Solver\nLLM\nGPU Mem\nGPU\n1\nCPU Mem\n2\n3\n3\n4\n4\nO\ufb04ine\nOnline\nPCIe\nGPU\nExecutor\nCPU\nExecutor\nCPU\nCold\nHot\nCPU\nCore\nGPU\nSM\nPredicated\nActivation\nCalculation\nLoading\nFigure 7: The architecture overview and inference work\ufb02ow of Pow-\nerInfer.\nto the GPU, given that hot-activated neurons that have been\nloaded on the GPU constitute a major fraction of activations.\nFor cold-activated neurons not in GPU memory, PowerIn-\nfer executes their computations on the CPU, eliminating the\nneed for weight transfers to the GPU (Insight-2).\n4.1\nArchitecture and Work\ufb02ow\nFigure 7 presents an architectural overview of PowerInfer,\ncomprising both of\ufb02ine and online components. Due to the\nvariation in locality properties among different LLMs, the\nof\ufb02ine component should pro\ufb01le LLMs\u2019 activation sparsity,\ndifferentiating between hot and cold neurons. In the online\nphase, the inference engine loads two types of neurons into\nboth GPU and CPU, serving LLM requests with low latency\nduring runtime.\nLLM Pro\ufb01ler and Policy Solver (Of\ufb02ine): This compo-\nnent includes an LLM pro\ufb01ler that collects activation data\nfrom inference processes using requests derived from gen-\neral datasets (e.g., C4 [32]). It monitors neuron activation\nacross all layers (Step \u2460), followed by a policy solver catego-\nrizing neurons as hot or cold. The solver aims to allocate fre-\nquently activated neurons to the GPU and others to the CPU.\nIt uses a neuron impact metric and hardware speci\ufb01cations\nto balance the workload, using integer linear programming\nto maximize the GPU\u2019s impact metric for neurons (Step \u2461).\nNeuron-aware LLM Inference Engine (Online): Before\nprocessing user requests, the online engine assigns the two\ntypes of neurons to their respective processing units (Step \u2462),\nas per the of\ufb02ine solver\u2019s output. During runtime, the engine\ncreates GPU and CPU executors, which are threads running\non the CPU side, to manage concurrent CPU-GPU compu-\ntations (Step \u2463). The engine also predicts neuron activation\nand skips non-activated ones. Activated neurons preloaded in\nGPU memory are processed there, while the CPU calculates\nand transfers results for its neurons to the GPU for integra-\ntion. The engine uses sparse-neuron-aware operators on both\nCPU and GPU, focusing on individual neuron rows/columns\nwithin matrices.\n1\n2\n4\n6\n3\n5\n7\nOnline Predictor\nCPU Mem\nLayer Output\nGPU Mem\nLayer Input\n1\n2\n6\n4\n5\n3\n7\nLayer i\nO\ufb04ine\nSplit\nFigure 8: An illustrative example shows how PowerInfer calculates\ndifferent neurons for one LLM layer.\n4.2\nSingle Layer Example\nFigure 8 illustrates how PowerInfer coordinates GPU and\nCPU in processing a layer\u2019s neurons. It classi\ufb01es neurons\nbased on of\ufb02ine data, assigning hot-activated ones (e.g., in-\ndices 3, 5, 7) to GPU memory and others to CPU memory.\nUpon receiving an input, a predictor identi\ufb01es which neurons\nin the current layer are likely to be activated. For instance,\nit predicts activation for neurons 3, 4, and 5. It is crucial to\nnote that hot-activated neurons, identi\ufb01ed through of\ufb02ine sta-\ntistical analysis, may not consistently match the runtime ac-\ntivation behaviors. For example, neuron 7, though labeled as\nhot-activated, is forecasted to be inactive in this case.\nBoth CPU and GPU then process predicted active neurons,\nignoring inactive ones. The GPU computes neurons 3 and 5,\nwhile the CPU handles neuron 4. Once neuron 4\u2019s compu-\ntation is complete, its output is sent to the GPU for result\nintegration.\n5\nNeuron-aware Inference Engine\nThis section presents a detailed introduction to the neuron-\naware inference engine in PowerInfer. We \ufb01rst elaborate on\nthe design of activation predictors leveraged by PowerInfer\nin \u00a75.1. Then, we elucidate the process of dividing and man-\naging neurons between the CPU and GPU in \u00a75.2. Following\nthis, the design of the hybrid execution model within Power-\nInfer is described in \u00a75.3. Lastly, we explain the details of\nneuron-aware operators used in PowerInfer in \u00a75.4.\n5.1\nAdaptive Sparsity Predictors\nThe online inference engine in PowerInfer reduces computa-\ntional loads by only processing those neurons that are pre-\ndicted to be activated. This method was also used in De-\njaVu [21], which advocates for training a set of \ufb01xed-size\nMLP predictors. Within each Transformer layer, DejaVu uti-\nlizes two separate predictors to forecast the activation of neu-\nrons in the self-attention and MLP blocks. Consequently, the\ninference computation is con\ufb01ned to neurons anticipated to\nbe active.\nHowever, designing effective predictors for local deploy-\nments with limited resources is challenging, balancing pre-\ndiction accuracy and model size. These predictors,frequently\ninvoked for neuron activation prediction, should be stored in\nGPU memory for fast access. Yet, the considerable memory\n5\n 0\n 50\n 100\n 150\n 200\n0.97\n0.975\n0.98\n0.985\n0.99\n0.995\nLow skewness\nHigh skewness\nParameters (MB)\nSparsity\nFigure 9: Correlation between predictor parameter size and layer\nsparsity at a guaranteed 95% accuracy level for OPT-175B. The\nX-axis represents sparsity, and the Y-axis represents the predictor\nparameter size. The bar indicates the average parameter size for the\nmodel in the corresponding sparsity, while the error bar re\ufb02ects \ufb02uc-\ntuations in the predictor parameter size due to skewness within the\nlayer.\nrequirements of numerous \ufb01xed-size predictors can encroach\nupon the space needed for storing LLM parameters. For ex-\nample, predictors for the OPT-175B model require around\n27GB of GPU memory, surpassing an NVIDIA RTX 4090\nGPU\u2019s capacity. On the other hand, naively reducing pre-\ndictor size may impair accuracy; a decrease from 480MB\nto 320MB in predictor size dropped its accuracy from 92%\nto 84%, further adversely affecting the overall LLM accu-\nracy (e.g., winogrande [35] task accuracy from 72.77% to\n67.96%).\nWe have observed that the size of predictors is in\ufb02uenced\nby two main factors: the sparsity of LLM layers and their\ninternal skewness. As shown in Figure 9, layers with higher\nactivation sparsity simplify the task of identifying activated\nneurons, allowing for smaller predictor models. In contrast,\nlayers with lower activation sparsity necessitate larger mod-\nels with more parameters, as accurately pinpointing activated\nneurons becomes increasingly challenging. Additionally, in\ncases of high skewness, where activations are heavily concen-\ntrated in a few neurons, even a compact predictor can achieve\nhigh accuracy.\nTo optimize for these factors, PowerInfer designs an iter-\native training method for non-\ufb01xed-size predictors for each\nTransformer layer. The process begins by establishing a base-\nline model size based on the layer\u2019s sparsity pro\ufb01le (Fig-\nure 9). Subsequently, the model size is iteratively adjusted,\ntaking into account the internal activation skewness to main-\ntain accuracy. An MLP predictor typically comprises input,\nhidden, and output layers. Since the dimensions of the in-\nput and output layers are determined by the Transformer\nlayer\u2019s structure, modi\ufb01cations primarily target the hidden\nlayer. During the iterative adjustments, the hidden layer\u2019s di-\nmension is modi\ufb01ed according to the observed skewness. For\nlayers exhibiting signi\ufb01cant skewness, the hidden layer size\nis reduced progressively until accuracy falls below 95%. Con-\nversely, for layers with minimal skewness, the dimension is\nincreased to improve accuracy. Through this approach, Pow-\nerInfer effectively limits predictor parameters to a mere 10%\nof the total LLM parameters.\n5.2\nNeuron Placement and Management\nWhen the of\ufb02ine solver determines a neuron placement pol-\nicy, the online inference engine of PowerInfer loads the\nmodel into the CPU and GPU memory as per the policy. For\neach layer, which may consist of multiple weight matrices,\nPowerInfer assigns each neuron to either the GPU or CPU\nbased on whether the neuron is hot-activated.\nEnsuring the accurate computation of these segmented\nneurons in their proper sequence is vital for precise results.\nTo this end, PowerInfer creates two neuron tables, one lo-\ncated in the CPU and the other in the GPU memory. These\ntables correlate each neuron to its original position in the\nmatrix. During the process of multiplying with an input\ntensor, each neuron interacts with its corresponding tensor\nvalue, guided by the mappings in the neuron tables. The ad-\nditional memory required for these neuron tables is relatively\ninsigni\ufb01cant, totaling only about 9MB for an LLM like OPT-\n175B, which needs 350GB of storage.\n5.3\nGPU-CPU Hybrid Execution\nGiven that PowerInfer processes only a limited number of\nneurons predicted to be active, such as less than 10% in an\nMLP layer, a potential method for GPU and CPU collab-\noration involves transferring cold-activated neuron weights\nfrom the CPU to the GPU for computation. However, as per\nInsight-2, the time spent transferring activated neurons to the\nGPU surpasses the time needed for direct computation on the\nCPU. Therefore, PowerInfer implements a GPU-CPU hybrid\nexecution model, wherein both units independently compute\ntheir respective activated neurons and then combine the re-\nsults on the GPU. This method effectively balances the com-\nputational workload, leveraging the strengths of each unit\nwhile reducing transfer time inef\ufb01ciencies.\nBefore inference,PowerInfer constructs a computationally\ndirected acyclic graph (DAG) with each node representing\na computational LLM inference operator and stores it in a\nglobal queue in the CPU memory. Each operator in the queue\nis tagged with its prerequisite operators. During inference,\ntwo types of executors, pthreads created by the host OS, man-\nage calculations on both CPU and GPU. They pull operators\nfrom the global queue, check dependencies, and assign them\nto the appropriate processing unit. The GPU and CPU use\ntheir neuron-aware operators, with the GPU executor launch-\ning GPU operators using APIs like cudaLaunchKernel, and\nthe CPU executor coordinating unoccupied CPU cores for\ncalculations. Before executing an operator, the CPU executor\nalso determines the necessary thread count for parallel com-\nputation. To manage operator dependencies, especially when\na parent node of a CPU operator is processed on the GPU, a\nbarrier ensures GPU computations are complete before the\nCPU starts its operator.\nIn scenarios where activated neurons are split between\nGPU and CPU, synchronization between these processing\nunits also becomes crucial. After one unit \ufb01nishes its neuron\n6\ncalculations, it waits for the other to merge results. As GPU\nneurons are activated more frequently, PowerInfer assigns\nmerging operations to the GPU. To optimize synchroniza-\ntion overhead, a selective synchronization strategy is used,\nbypassing result synchronization when the CPU executor has\nno activated neurons, allowing it to proceed to subsequent\nblocks, thereby enhancing overall ef\ufb01ciency.\n5.4\nNeuron-aware Operator\nConsidering the activation sparsity in LLMs, matrix multi-\nplication operations can bypass inactive neurons and their\nweights, necessitating the use of sparse operators. However,\ncurrent sparse matrix multiplication tools, including state-of-\nthe-art sparse-aware compilers like SparTA [52] and Flash-\nLLM [45], as well as libraries like cuSPARSE [30] and\nSpunik [33], fall short in this regard. They either support\nonly static compilation of sparse-aware kernels or require dy-\nnamic conversion of sparse matrices into dense formats, lead-\ning to signi\ufb01cant performance overhead, especially with the\ndynamic sparsity in our scenario. Additionally, the dynamic\nJIT compiler PIT [51], though ef\ufb01cient for general sparse\nmatrix multiplication on GPUs, is not suited for CPU-GPU\nhybrid execution where CPU computational capabilities are\nlimited.\nTo overcome these limitations, PowerInfer introduces\nneuron-aware operators that directly compute activated neu-\nrons and their weights on both GPU and CPU without the\nneed for runtime conversion to dense format. These operators\ndiffer from traditional ones as they focus on individual row/-\ncolumn vectors within a matrix rather than the entire matrix.\nThey \ufb01rst determine a neuron\u2019s activation status and then pro-\ncess it if predicted to be active, alongside the corresponding\nrow or column of the parameter matrix.\nNeuron-aware Operators for GPU: Despite vector-\nvector calculations being less ef\ufb01cient than matrix-vectorcal-\nculations on GPU, neuron-aware operators based on vector-\nvector computation are advantageous when the batch size is\nsmall. They avoid unnecessary computations and memory\noperations associated with inactive neurons and do not need\ncostly matrix conversions. Furthermore, these operators al-\nlow all thread blocks to concurrently check neuron activa-\ntions and compute corresponding vectors if activated.\nNeuron-aware Operators for CPU: Neuron-aware oper-\nators are particularly bene\ufb01cial for CPUs, which generally\nhave lower parallelism and matrix computation ef\ufb01ciency.\nThe CPU executor assigns a neuron-aware operator to mul-\ntiple cores, dividing neurons into smaller batches for concur-\nrent activation checking. Each core processes only the ac-\ntivated neurons in its batch, optimizing vector-vector calcu-\nlations with hardware vector extensions like AVX2, widely\nsupported in modern CPUs.\n6\nNeuron Placement Policy\nTo fully unleash the computationalcapability of the GPU and\nCPU, PowerInfer\u2019s of\ufb02ine component provides a placement\npolicy to guide the allocation of each neuron to either the\nGPU or CPU. This policy, output by a solver, controls neu-\nron placement within each layer, thereby de\ufb01ning the run-\ntime computational workload for the respective processing\nunits. The solver considers a range of factors, including each\nneuron\u2019s activation frequency, communication overhead, and\nthe computational capacities of the processing units, such as\ntheir memory sizes and bandwidths.\nThe solver de\ufb01nes an impact metric for each neuron to\nmodel its activation information. By integrating the neuron\nimpacts with the capabilities of different computing units,\nthe solver constructs an integer linear programming model\nto generate the optimal neuron placement.\n6.1\nOf\ufb02ine Pro\ufb01ling\nBefore determining the placement of each neuron, the of\ufb02ine\npro\ufb01ler of PowerInfer needs to gather runtime inference data\nfor each neuron. To achieve this, it deploys the LLM to han-\ndle requests generated from multiple general datasets, such\nas C4 [32] and Wikipedia [10]. To accurately measure acti-\nvation information, the pro\ufb01ler inserts a monitoring kernel\nafter each block within a Transformer layer. Additionally, it\nbuilds a neuron information table on the GPU, designed to\ntrack the activation count of each neuron.\nThis kernel checks whether each neuron in the layer gets\nactivated during the inference process and, if so, increments\nthe corresponding count in the neuron table. Once all re-\nquests have been processed, the pro\ufb01ler retrieves the activa-\ntion data from this table and passes it to the solver.\n6.2\nNeuron Impact Metric\nThe neuron impact metric measures each neuron\u2019s contribu-\ntion to the LLM\u2019s overall inference outcome,crucial for GPU\nneuron allocation. We calculate this metric effectively by\nleveraging the fact that pro\ufb01led activation frequency mirrors\nruntime behavior accurately, provided the pro\ufb01ling involves\na substantial amount of input data. As Equation 1 shows, this\nmetric for a neuron is de\ufb01ned by its activation frequency ob-\ntained during pro\ufb01ling.\nvi = fi\n\u2200i \u2208 N\n(1)\n6.3\nModeling of Neuron Placement\nBased on the neuron impact metric, PowerInfer utilizes a\nsolver to optimize the total impacts of all neurons in the GPU.\nThis cumulative impact is formulated as the objective func-\ntion, as de\ufb01ned in Equation 2. This function is then input into\nan integer linear programming framework to identify a spe-\nci\ufb01c solution that maximizes the function. The binary vari-\nable ain, de\ufb01ned in Equation 3 indicates whether the neuron\nn is placed on processing unit i.\nMaximize\nti = \u2211\ne\u2208N\naie \u2217ve\u2200i \u2208 {GPU}\n(2)\n\u2211\ni\u2208U\nain = 1\n\u2200n \u2208 N\n(3)\n7\nSymbol\nType\nDescription\nL\nPar\nAll layers\nN\nPar\nAll neurons\nU\nPar\nCPU and GPU\nfi\nPar\nActivation frequency of neuron j\nNi\nPar\nNeuron in layer i\nvi\nPar\nNeuron impact for neuron j\nMi\nPar\nThe memory size for neuron i\nMCapj\nPar\nThe memory size for processing unit j\nBandwidthj\nPar\nThe memory bandwidth for processing unit j\nTsync\nPar\nThe time required for one synchronization between\nthe CPU and GPU\nK\nPar\nA large positive number\nain\nVar\nWhether neuron n is placed on processing unit U\nT\nj\nl\nVar\nThe time for computing one neuron in layer l on\nprocessing j\nCl\nVar\nThe minimum number of neurons required to be al-\nlocated on the GPU when the solver opts to split\nneurons in layer l\nyl\nVar\nBinary auxliary variable for layer l to facilitate the\nmodeling of conditional constraints\nTable 1: Terminology for ILP formulation. The Par represents the\nparameters gathered from the pro\ufb01ler or the expressions used to de-\n\ufb01ne constraints, none of which need to be solved by the solver. The\nVar refers to the constraint and objective variables that emerge from\nthe modeling process, which need to be solved by the solver.\nWhen maximizing the objective function, the solver also\nneeds to consider two sets of constraints associated with the\ncommunication overhead between processing units and their\nhardware capabilities.\n6.3.1\nCommunication Constraint\nThe number of neurons preloaded onto the GPU is limited\nby the communication overhead within a layer, a constraint\ndictated by hardware PCIe bandwidth limitations. If too few\nneurons are preloaded, this overhead negates the computa-\ntional bene\ufb01ts offered by the GPU. As a result, the solver\nmust identify a minimum number of neurons to allocate to\nthe GPU for processing. This ensures that neuron process-\ning on the GPU, including synchronization time, is more ef-\n\ufb01cient than CPU processing, as speci\ufb01ed in Inequality 4. In\nthis inequality,Cl is the minimum count of neurons that must\nbe assigned to the GPU for layer l.\nWhen solving Inequality 4, it is essential to de\ufb01ne both\nthe computation time for an individual neuron in layer l and\nthe intra-layer communication overhead, Tsync. In LLM infer-\nence, especially with smaller batch sizes, the process is pri-\nmarily limited by memory bandwidth. Therefore, the com-\nputation time for a neuron approximately equals the time\nneeded to access all of its weights once, as indicated in Equa-\ntion 5. With smaller batch sizes, the extent of intra-layer data\ntransfer tends to be consistent across layers, leading to a uni-\nform synchronization cost. Consequently, we describe Tsync\nas the pro\ufb01led overhead for a single instance of intra-layer\ncommunication.\nCl \u22c5T GPU\nl\n+Tsync \u2264 Cl \u22c5TCPU\nl\n\u2200l \u2208 L\n(4)\nT j\ni = Mi/Bandwidth j\n\u2200j \u2208 D,\u2200i \u2208 L\n(5)\n6.3.2\nMemory Constraint\nNeuron placement is further constrained by the memory ca-\npacities of the processing units, as de\ufb01ned in Inequality 6.\nMoreover, the solver ensures that when allocating neurons\nof a layer to the GPU, it either assigns at least the minimum\nnumber of neurons speci\ufb01ed in Inequality 4 to offset com-\nmunication costs or opts not to allocate any neurons from\nthat layer to the GPU. Speci\ufb01cally, the number of neurons\nfor layer l on the GPU must either exceed Cl or be equal to\nzero.\nTo model this, we introduce an auxiliary binary variable,\nyl, which can be either 1 or 0. This variable determines\nwhether any neurons are assigned to the GPU for layer l.\nFor computational convenience, a suf\ufb01ciently large number\nK is also introduced. Inequalities 7 and 8 are formulated\nto model this constraint. When yl is 1, indicating neuron\nplacement on the GPU for this layer, and given that K is\nadequately large, these two inequalities effectively become\nyl \u2264 \u2211e\u2208Nl aie \u2264 K. Conversely, if yl is set to 0, signifying\nno neuron placement on the GPU for layer l, the inequalities\nreduce to \u2211e\u2208Nl aie = 0.\n\u2211\nn\u2208N\na jn \u22c5Mn < MCap j\n\u2200j \u2208 U\n(6)\n\u2211\ne\u2208Nl\naie \u2265 Cl \u22c5yl\n\u2200l \u2208 L,\u2200i \u2208 {GPU}\n(7)\n\u2211\ne\u2208Nl\naie \u2264 K \u22c5yl\n\u2200l \u2208 L,\u2200i \u2208 {GPU}\n(8)\n6.3.3\nILP Optimization\nSubsequently, thesolver utilizes IntegerLinear Programming\n(ILP) to optimize the objective function, conforming to all\nthe constraints from Equation/Inequality 3 to 8. Given that\nILP problems are inherently NP-complete, directly solving\nthem for an LLM with hundreds of billions of parameters\nposes a considerable computational challenge. To expedite\nthe process and achieve an approximate solution, the primary\nstrategy involves aggregating neurons within each layer into\nbatches for collective placement analysis. Speci\ufb01cally, the\nsolver groups 64 neurons with similar impacts from a layer\ninto a single batch. This batching strategy dramatically re-\nduces the total neuron count, N, from several millions to\nroughly tens of thousands, thereby signi\ufb01cantly decreasing\nthe time to solve the ILP problem to approximately 10 sec-\nonds.\n8\n7\nImplementation\nThe online inference engine of PowerInfer has been imple-\nmented by incorporating an additional 4,200 lines of C++\nand CUDA code into llama.cpp [14], a state-of-the-art open-\nsource LLM inference framework designed for PCs. The ex-\ntensions made by PowerInfer include modi\ufb01cations to the\nmodel loader for distributing an LLM across GPU and CPU,\nfollowing the guidance from the of\ufb02ine solver\u2019s outputs. We\nhave also optimized the inference engine for GPU-CPU hy-\nbrid execution and introduced 10 neuron-aware operators for\nboth processing units. All other components and functional-\nities of llama.cpp remains unchanged. For instance, the KV\ncache continues to reside in CPU memory, allowing more\nGPU memory for hot-activated neurons, as its access has\nminimal impact on inference latency, particularly in small\nbatch sizes. Furthermore, around 400 lines of Python code\nwere added to the transformers framework [44], enabling it\nto function as an of\ufb02ine pro\ufb01ler and solver for PowerInfer.\nThe current implementation of PowerInfer supports a\nrange of mainstream LLM families with varying parameter\nsizes, including the OPT [49] family (from 7B to 175B pa-\nrameters), the LLaMA [42] family (7B to 70B), and Falcon-\n40B [2]. For these models, PowerInfer utilizes DejaVu [21]\nto train online activation predictors, which has been en-\nhanced with an adaptive training method. While training an\nLLM is a lengthy process, often taking several hours, it is\na one-time task. The duration of this process can be signi\ufb01-\ncantly reduced by utilizing multiple high-end GPUs.\n8\nEvaluation\n8.1\nExperimental Setup\nHardware. To demonstrate the generalization of PowerIn-\nfer across various hardware setups, experiments were con-\nducted on two distinct PC con\ufb01gurations, representing both\nhigh-end and low-end hardware scenarios:\n\u2022 PC-High: Equipped with an Intel i9-13900K processor\n(eight physical cores at 5.4GHz) and 192GB host mem-\nory (memory bandwidth of 67.2 GB/s). This con\ufb01gura-\ntion includes an NVIDIA RTX 4090 GPU (24G) with a\nmemory bandwidth of 1TB/s and operates with a PCIe\n4.0 interface (64GB/s bandwidth).\n\u2022 PC-Low: Features an Intel i7-12700K processor (eight\nphysical cores at 4.9GHz), coupled with 64GB of host\nmemory (memory bandwidth 38.4 GB/s). It also in-\ncludes an NVIDIA RTX 2080Ti GPU (11G) with a\nmemory bandwidth of 616GB/s and utilizes PCIe 3.0\ninterface (32GB/s bandwidth).\nModels. We use a range of OPT [49] models with parameters\nfrom 6.7B to 175B, as well as Falcon(ReLU)-40B [38] and\nLLaMA(ReGLU)-70B [39] models. Notably, the 175B pa-\nrameter model is comparable in size to the GPT-3 model [5].\nFor our experiments, all models in our experiments use FP16\nand INT4 quantized parameters, with intermediate activa-\ntions in FP32, consistent with recent LLM research prac-\ntices [12,47].\nWorkloads. The workloads for our experiments are derived\nfrom ChatGPT prompts [28] and Alpaca [41] datasets, cover-\ning a wide spectrum of language model uses. These datasets\nconsist of input and output texts typical of real LLM ser-\nvices. ChatGPT prompts include user interactions with Chat-\nGPT [31], and Alpaca features instruction sets generated by\nGPT3.5 through self-instruction.\nBaseline\nSystem.\nWe\ncompare\nPowerInfer\nwith\nllama.cpp [14], a state-of-the-art local LLM inference\nframework. To facilitate this comparison, we extended\nllama.cpp to support the OPT model, as it lacks native\ncompatibility. While other alternatives like FlexGen [37]\nand DejaVu [21] exist, they exhibit higher latency in the\nlatency-sensitive scenarios discussed in\nthis paper, as\nanalyzed in \u00a72.2. Therefore, llama.cpp serves as the more\nrelevant benchmark for our evaluation.\nKey Metrics. As we focus on low latency setting, our pri-\nmary evaluation metric is end-to-end generation speed, quan-\nti\ufb01ed as the average number of tokens generated per second\n(tokens/s). It is calculated by dividing the total count of gen-\nerated tokens by the end-to-end response time, offering a pre-\ncise measure of the response generation process\u2019s ef\ufb01ciency.\n8.2\nEnd-to-End Performance\n0\n4\n8\n12\n16\nSpeedup\n1.0\n2.7\n1.6\n13\n1.6\n16\n0.3\n1.2\n0.6\n2.2\n0.6\n2.3\n0.8\n4.4\n1.1\n11\n1.1\n13\n0.4\n0.6\n0.6\n1.7\n0.6\n2.0\nOPT 30B\nOPT 66B\nFalcon 40B\nLLaMA 70B\nInput length=64\nllama.cpp\nPowerInfer\n8\n128\n512\n8\n128\n512\n8\n128\n512\n8\n128\n512\nOutput length\n0\n4\n8\n12\nSpeedup\n0.9\n2.6\n1.6\n13\n1.6\n16\n0.2\n0.8\n0.5\n2.1\n0.6\n2.3\n0.8\n1.7\n1.1\n8.8\n1.1\n11\n0.4\n0.7\n0.6\n1.8\n0.6\n1.9\nInput length=128\nFigure 10: Speedup of various models on PC-High in FP16 format.\nThe X axis indicates the output length. The Y axis represents the\nspeedup compared with llama.cpp. The number above each bar in-\ndicates the end-to-end generation speed (tokens/s). The \ufb01rst row of\nthe \ufb01gure is con\ufb01gured with an input length of around 64, and the\nsecond row with an input length of approximately 128.\nWe \ufb01rst compare the end-to-end inference performance of\nPowerInferand llama.cpp with a batch size of one,the typical\nsetting for local deployments [6]. Given real-world dialog in-\nput/output length variability [18], we sample prompts from\nAlpaca and ChatGPT datasets, ranging from 8 to 128 char-\nacters. Both PowerInfer and llama.cpp generated 8, 128, and\n512 tokens in response to each prompt.\nFigure 10 illustrates the generation speeds for vari-\nous models and input-output con\ufb01gurations on a PC-High\nequipped with an NVIDIA RTX 4090. On average, Power-\nInfer achieves a generation speed of 8.32 tokens/s, reaching\n9\n0\n2\n4\n6\n8\n10\nSpeedup\n4.8\n8.2\n7.3\n32\n7.4\n37\n1.3\n3.1\n1.9\n12\n1.9\n13\n0.3\n0.7\n0.6\n1.8\n0.6\n2.0\n1.6\n2.5\n2.0\n8.6\n2.0\n9.5\nOPT 7B\nOPT 13B\nOPT 30B\nLLaMA 13B\nInput length=64\nllama.cpp\nPowerInfer\n8\n128\n512\n8\n128\n512\n8\n128\n512\n8\n128\n512\nOutput length\n0\n2\n4\n6\n8\nSpeedup\n3.6\n5.4\n6.8\n27\n6.9\n33\n1.2\n2.1\n1.8\n9.3\n1.9\n12\n0.3\n0.5\n0.6\n1.6\n0.6\n1.9\n1.5\n3.3\n2.0\n9.4\n2.0\n9.3\nInput length=128\nFigure 11: Speedup of various models on PC-Low in FP16 format.\nThe X axis indicates the output length. The Y axis represents the\nspeedup compared with llama.cpp. The number above each bar in-\ndicates the end-to-end generation speed (tokens/s). The \ufb01rst row of\nthe \ufb01gure is con\ufb01gured with an input length of around 64, and the\nsecond row with an input length of approximately 128.\nup to 16.06 tokens/s, signi\ufb01cantly outperforming llama.cpp\nwith average speedups of 7.23\u00d7, and for Falcon-40B, up to\n11.69\u00d7. The performance superiority of PowerInfer becomes\nmore pronounced as the number of output tokens increases\nsince the generation phase plays a more signi\ufb01cant role in the\noverall inference time. In this phase, a small number of neu-\nrons are activated on both CPU and GPU, leading to fewer\nunnecessary computations compared to llama.cpp. For exam-\nple, in the case of OPT-30B, only around 20% of neurons\nare activated for each token generated, with the majority pro-\ncessed on the GPU, a bene\ufb01t of PowerInfer\u2019s neuron-aware\ninference engine.\nFigure 11 shows that on a lower-end PC (PC-Low), Pow-\nerInfer still attains considerable performance enhancement\nover llama.cpp, averaging a speedup of 5.01\u00d7 and peaking at\n7.06\u00d7. However, these improvements are smaller compared\nto those on a higher-end PC (PC-High), primarily due to the\n11GB GPU memory limitation of PC-Low. This limitation af-\nfects the number of neurons that can be allocated to the GPU,\nparticularly for models with around 30B parameters or more,\nleading to a greater dependence on the CPU for processing a\nlarger number of activated neurons.\nFigure 12 presents the distribution of neuron loads be-\ntween the CPU and GPU for both PowerInfer and llama.cpp.\nNeuron loads refer to the proportion of activated neuron com-\nputations carried out by each processing unit. Notably, on\nPC-High, PowerInfer signi\ufb01cantly increases the GPU\u2019s share\nof neuron load, from an average of 20% to 70%. This indi-\ncates that the GPU processes 70% of activated neurons. How-\never, in cases where the model\u2019s memory requirements far\nexceed the GPU\u2019s capacity, such as running a 60GB model\non an 11GB 2080Ti GPU, the GPU\u2019s neuron load is reduced\nto 42%. This decrease is due to the GPU\u2019s limited memory,\nwhich is insuf\ufb01cient to host all hot-activated neurons, neces-\nsitating that the CPU compute a portion of these neurons.\nIn scenarios involving long input prompts with relatively\nshort output lengths, which are less common [28], PowerIn-\nfer demonstrates only limited performance gains. In such sit-\n 0\n 20\n 40\n 60\n 80\n 100\nOPT-30B\nOPT-66B\nOPT-175B(INT4)\nNeuron Load Breakdown (%)\nllama.cpp-CPU\nllama.cpp-GPU\n(a) PC-High\nOPT-13B\nOPT-66B(INT4)\nOPT-30B\nPowerInfer-CPU\nPowerInfer-GPU\n(b) PC-Low\nFigure 12: Neuron load distribution on CPU and GPU during in-\nference. The yellow block refers to llama.cpp, and blue block refers\nto PowerInfer.\nuations, the prompt phase, where a substantial number of to-\nkens are processed simultaneously, becomes a crucial factor\nin determining inference speed. This results in each token\nactivating a unique set of neurons, substantially diminishing\nactivation sparsity. As a consequence, the CPU becomes the\nprimary bottleneck in the inference process, tasked with pro-\ncessing a considerable number of cold-activated neurons but\nconstrained by its computational capabilities.\nInference with Quantization. Figure 13 illustrates that Pow-\nerInfer effectively supports LLMs that are compressed using\nINT4 quantization. On a high-end PC (PC-High), PowerIn-\nfer delivers responses at an average speed of 13.20 token-\ns/s, reaching a peak of 29.08 tokens/s. The average speedup\nachieved compared with llama.cpp is 2.89\u00d7, with a maxi-\nmum of 4.28\u00d7. On a lower-end setup (PC-Low), the aver-\nage speedup is 5.01\u00d7, peaking at 8.00\u00d7. The reduction in\nmemory requirements due to quantization enables PowerIn-\nfer to more ef\ufb01ciently manage larger models. For instance,\nin our experiment with the OPT-175B model on PC-High,\nPowerInfer nearly reaches two tokens per second, surpassing\nllama.cpp by a factor of 2.66\u00d7.\nBatching Inference. We also evaluate the end-to-end infer-\nence performance of PowerInfer with different batch sizes, as\nshown in Figure 14. PowerInfer demonstrates a signi\ufb01cant\nadvantage when the batch size is smaller than 32, achiev-\ning an average 6.08\u00d7 improvement in performance com-\npared with llama.cpp. As the batch size increases, the speed-\nup ratio offered by PowerInfer decreases. This reduction is\nattributed to the diminished sparsity of model joint activa-\ntions. However, even with the batch size set to 32, PowerIn-\nfer still maintains a considerable speedup, achieving a 4.38\u00d7\nspeedup.\n8.3\nAblation Studies\n8.3.1\nPerformance Breakdown\nFigure 15 breaks down the contributions of each PowerIn-\nfer component to the overall performance speedup. Using\na step-by-step integration method, we progressively incorpo-\nrate PowerInfer features into llama.cpp. First, we add Power-\nInfer\u2019s predictors and neuron-aware operators into llama.cpp\n(labeled \"+PO\"), enabling computation of only activated neu-\nrons on both GPU and CPU. Yet, +PO still adheres to layer-\nwise computation, where each layer is processed entirely by\neither GPU or CPU.\nBuilding on +PO, we introduce PowerInfer\u2019s hybrid in-\n10\n8\n128\n512\n8\n128\n512\n8\n128\n512\n8\n128\n512\nOutput length\n0\n2\n4\n6\nSpeedup\n1.9\n7.6\n2.9\n12\n2.9\n12\n0.5\n1.2\n0.7\n1.9\n0.7\n1.8\n15\n22\n17\n29\n17\n27\n2.4\n6.4\n3.2\n9.5\n3.2\n10\nOPT 66B\nOPT 175B\nFalcon 40B\nLLaMA 70B\nPC-High\nllama.cpp\nPowerInfer\n8\n128\n512\n8\n128\n512\n8\n128\n512\n8\n128\n512\nOutput length\n0\n2\n4\n6\n8\n10\nSpeedup\n2.9\n7.6\n3.9\n13\n3.8\n12\n0.8\n3.3\n1.1\n5.3\n1.1\n5.0\n1.7\n11\n2.0\n16\n2.0\n15\n1.0\n2.9\n1.1\n4.2\n1.1\n4.2\nOPT 30B\nOPT 66B\nFalcon 40B\nLLaMA 70B\nPC-Low\nFigure 13: Speedup of different models on PC-High and PC-Low in\nINT4 format. The X axis indicates the output length. The Y axis rep-\nresents the speedup compared with llama.cpp. The number above\neach bar indicates the end-to-end generation speed (tokens/s). The\nupper row of the \ufb01gure presents performance on PC-High, while the\nlower row details those on PC-Low.\n 0\n 8\n 16\n 24\n 32\n 40\n1\n4\n8\n16\n32\nTokens/s\nBatch size\nllama.cpp\nPowerInfer\n10.73X\n5.73X\n4.85X\n4.69X\n4.38X\nFigure 14: Batch inference speedup of Falcon-40B on PC-High.\nThe X axis indicates the request batch size, the Y axis represents the\nend-to-end token generation speed (tokens/s). The number above\neach bar shows the speedup compared with llama.cpp.\nference engine (denoted \"+Engine\"), which allows neuron-\naware operators to process neurons within the same layer si-\nmultaneously on both GPU and CPU. +Engine uses a naive\nneuron partitioning policy that assigns frequently activated\nneurons to the GPU. The \ufb01nal step involves integrating\nour optimized policy (\"+Policy\"), formulated by the of\ufb02ine\nsolver as described in \u00a76, into the +Engine setup, showcas-\ning the full capabilities of PowerInfer.\nThe initial integration of +PO into llama.cpp yields per-\nformance boosts of 1.98\u00d7 and 2.00\u00d7 for OPT-30B and\nOPT-66B, respectively, primarily by reducing unnecessary\ninactive neurons. +Engine further escalates these gains to\n9.97\u00d7 and 3.43\u00d7, thanks to precise neuron placement and\nintra-layer calculations that signi\ufb01cantly increase the GPU\u2019s\ncomputational share. Finally, incorporating +Policy results\nin improvements of 10.47\u00d7 and 3.67\u00d7. The enhancement\nachieved by our policy lies in its ability to \ufb01nely balance the\nintra-layer communication overhead. The naive partitioning\npolicy in +Engine overlooks the GPU-CPU intra-layer com-\nmunication, often offsetting the bene\ufb01ts of assigning high-\nfrequency activation neurons to the GPU. Conversely, our\npolicy in PowerInfer more adeptly balances processing loads\nand communication costs between the CPU and GPU.\n 0\n 2\n 4\n 6\n 8\n 10\n 12\nOPT-30B\nOPT-66B\nSpeedup\nllama.cpp\n+PO\n+Engine\n+Policy\nFigure 15: Performance breakdown for each component of Power-\nInfer on PC-High.\n 0\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nExecution Time (ms)\nSparse operator on CPU\ndense\nPyTorch sparse\nPowerInfer\n 0\n 0.1\n 0.2\n 0.3\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nSparse operator on GPU\ndense\nPIT\nPowerInfer\nFigure 16: Comparing Neuron-aware operator with different\nsparse operators on PC-Low. The X axis indicates the sparsity level,\nthe Y axis represents the execution time(ms).\n8.3.2\nNeuron-aware Operator Performance\nThis section evaluates the performance of PowerInfer\u2019s\nsparse operators on both CPU and GPU across various spar-\nsity levels. We benchmark PowerInfer against leading sparse\nlibraries: for CPU benchmarks, we use PyTorch sparse, the\nstate-of-the-art sparse kernels within PyTorch, as our base-\nline. In GPU, PowerInfer is compared with PIT [51]. Given\nthat the sparsity in LLMs is typically based on neuron gran-\nularity, our experiments are speci\ufb01cally designed to evaluate\nsparse matrices of this nature. We focus on sparse matrix-\nvector multiplication using a [4096, 4096] \u00d7 [4096, 1] con-\n\ufb01guration, a common setup in local LLM inference [6]. To\nadjust sparsity, we introduce zero values to matrix rows.\nFigure 16 shows that PowerInfer\u2019s operator achieves\nnearly linear acceleration with increasing sparsity levels, a\nstark contrast to dense matrix computations. On the CPU,\ntraditional sparse operators do not outperform dense com-\nputation until sparsity surpasses 87%. However, PowerIn-\nfer\u2019s CPU operator outperforms dense matrix multiplication\neven at sparsity levels below 10%. For the GPU, PowerIn-\nfer matches PIT in performance. Its primary advantage, how-\never, is its uni\ufb01ed CPU-GPU framework. This design allows\nfor \ufb02exible execution of sparse operators on both process-\ning units, unlike PIT, which is optimized solely for GPU-\nbased sparse matrix multiplication and does not support hy-\nbrid CPU-GPU environments.\n8.3.3\nPredictor Overhead\nThe execution time of the online predictors for different mod-\nels is also measured, as depicted in Figure 17. On average,\nthe execution of predictors constitutes less than 10% of the\ntotal inference time in PowerInfer. This ef\ufb01ciency is primar-\nily due to the adaptive methods used in constructing sparsity\npredictors, which minimizes computational load. Moreover,\nthese dense-model predictors are incorporated into Power-\nInfer\u2019s solver for neuron placement decisions, with a prefer-\nence for allocating them to the GPU. This strategy effectively\n11\n 0\n 20\n 40\n 60\n 80\n 100\nOPT-7B\nOPT-13B\nOPT-30B(INT4)\nOPT-30B\nBreakdown (%)\nInference\nPrediction\nFigure 17: End-to-end prediction overhead of PowerInfer on PC-\nLow. The X axis represents various models, while the Y-axis dis-\nplays the percentage breakdown between predictor overhead and\nLLM inference time.\n 0\n 10\n 20\n 30\nOPT-30B\nFalcon-40B\nAverage tokens/s\n(a) Input length = 1\nllama.cpp(4090)\nPowerInfer(4090)\nvLLM(A100)\n18%\n93%\n23%\n92%\n \n \n \n \nOPT-30B\nFalcon-40B\n(b) Input length = 64\n28%\n93%\n29%\n94%\nFigure 18: Generation speed of NVIDIA RTX 4090 compared with\nsingle A100. The X axis represents various models, while the Y-axis\nrepresents end-to-end generation speed (tokens/s) under various in-\nference framework. The percentages within the arrows represent the\nslowdown relative to vLLM on the A100.\nleverages the parallel processing capabilities of GPUs, fur-\nther minimizing the overhead associated with the predictors.\n8.3.4\nPerformance Comparison with A100\nIn our study, we analyze the extent to which PowerInfer re-\nduces the performance gap between a consumer-grade GPU\nand its top-tier server-grade counterpart. Therefore, we eval-\nuate the generation speed of PowerInfer, deployed on PC-\nHigh, in comparison to the performance of llama.cpp and\nvLLM [18] executed on a single 80GB NVIDIA A100 GPU.\nWe chose the OPT-30B and Falcon-40B models for compari-\nson, considering their exact memory requirements matching\nprecisely with the capacity of the A100 GPU. Our evaluation\nused input lengths of 1 and 64 to measure pure generation\nspeed and conversational interactions, respectively.\nFigure 18a demonstrates that PowerInfer signi\ufb01cantly nar-\nrows the performance gap between the NVIDIA 4090 and\nA100 in generation tasks with input length 1. On PC-High,\nllama.cpp lags behind vLLM on the A100 by 93% and 92%\nfor OPT-30B and Falcon-40B, respectively, but PowerInfer\nreduces this to 18% and 23%. Figure 18b shows that despite\nreduced cumulative sparsity in the prompt phase, PowerInfer\nstill reduces the performance gap to 28% and 29%. The re-\nmaining disparity mainly stems from the CPU\u2019s considerable\ncomputational load, which has become a bottleneck.\n8.4\nLLM Accuracy\nSince PowerInfer selectively omits neurons predicted to be\ninactive, we investigated whether this approach affects the\ninference accuracy of LLMs. Table 2 compares the accu-\nracy of models from the OPT, Falcon (ReLU), and LLaMA\n(ReGLU) families, both with and without differentiating ac-\ntivated/inactivated neurons, across a variety of downstream\ntasks. The results show that PowerInfer causes negligible\nTable 2: Comparison of LLM accuracy between PowerInfer-\noptimized models (termed as \"model-sparse\") and their original\ncounterparts. COPA [34] serves as a QA dataset focused on eval-\nuating causal reasoning skills in language models. PIQA [4] and\nWinogrande [35] are designed for probing common sense reason-\ning abilities and the understanding of physical world interactions in\nLLMs. RTE [15] is used to assess natural language understanding\nvia textual entailment.\nPIQA\nWinogrande\nRTE\nCOPA\nOPT-7B\n75.78%\n65.19%\n55.23%\n81.00%\nOPT-7B-sparse\n75.67%\n65.51%\n55.96%\n81.00%\nOPT-13B\n76.01%\n64.96%\n58.12%\n85.00%\nOPT-13B-sparse\n76.28%\n65.98%\n56.32%\n84.00%\nOPT-30B\n77.58%\n68.43%\n58.40%\n82.00%\nOPT-30B-sparse\n77.48%\n67.56%\n59.93%\n86.00%\nOPT-66B\n78.62%\n68.82%\n60.29%\n86.00%\nOPT-66B-sparse\n79.16%\n67.80%\n61.01%\n87.00%\nOPT-175B\n79.65%\n72.77%\n59.93%\n88.00%\nOPT-175B-sparse\n79.26%\n72.38%\n61.98%\n87.00%\nFalcon(ReLU)-40B\n81.23%\n75.45%\n66.43%\n92.00%\nFalcon(ReLU)-40B-sparse\n81.01%\n75.92%\n64.62%\n93.00%\nLLaMA(ReGLU)-70B\n82.01%\n75.93%\n75.81%\n89.00%\nLLAMA(ReGLU)-70B-sparse\n82.05%\n75.53%\n77.62%\n90.00%\nloss in inference accuracy, regardless of the model size or\ntype of task, consistent with previous research \ufb01ndings [21].\nAlthough the predictors in each Transformer layer maintain\nan accuracy rate above 95%, they may occasionally miss\nsome active neurons. As a result, there are minor \ufb02uctuations\nin LLM accuracy, leading to slight decreases or sometimes\neven increases in performanceon speci\ufb01c downstream tasks.\n9\nRelated Work\nLLM Activation Sparsity: Recent advancements like De-\njaVu [21], PIT [51], and brainstorm [8] are crucial to opti-\nmizing LLM inference, akin to PowerInfer. DejaVu [21] pro-\nposes enhancing inference speed through activation sparsity\nprediction, while PowerInfer leverages a power-law distribu-\ntion in neuron activations, focusing on GPU computation of\nfrequently activated neurons. PIT [51] accelerates GPU tasks\nby converting sparse to dense matrices. However, these meth-\nods, primarily exploiting GPU sparsity, face limitations in\nresource-constrained local environments.\nLLM Weight Sparsity: Model pruning [16, 17, 24], re-\nducing parameter count by setting some weights to zero,\nis exempli\ufb01ed by SparseGPT [11] and Wanda [40], achiev-\ning nearly 50% unstructured sparsity. SparTA [52] leverages\nboth sparse tensor and SIMT cores by dividing sparse ma-\ntrices. Flash-LLM [45] introduces a \"Load-as-Sparse and\nCompute-as-Dense\" approach for tensor core SpMM. How-\never, these methods, orthogonal to LLMs\u2019 intrinsic sparse ac-\ntivations, usually incur accuracy losses and wall-clock model\nacceleration challenges [27]. This is in contrast to the natu-\nral sparse activations utilized by PowerInfer, which maintain\nperformance and enhance computational ef\ufb01ciency.\nSpeculative LLM Inference: speculative inference [6,\n12\n13, 43] can also be leveraged to serve models exceeding\nGPU memory. Speculative decoding [7] uses a smaller, faster\nmodel to pre-decode tokens, later validated by the main\nmodel in a batch, reducing steps and CPU-GPU communica-\ntion. SpecInfer [26], as another example, effectively reduces\nthe number of LLM decoding steps and the overall communi-\ncation between CPU DRAM and GPU HBM. While separate\nfrom our focus, integrating speculative inference into Power-\nInfer could further boost LLM inference speed.\nLLM-Speci\ufb01c Serving Optimizations: The prominence\nof Transformers has led to specialized serving systems [9,\n36, 53]. Orca [47] introduces iteration-level scheduling.\nvLLM [18] implements Paged Attention for token storage\nin varied GPU memory addresses, overcoming KV cache\u2019s\ncontinuous storage limit. While vLLM effectively mitigates\nthe issue of severe GPU memory fragmentation, it does not\naddress the challenge of deploying models on PCs where the\nentire model cannot \ufb01t within the available GPU memory.\n10\nConclusion\nPowerInfer is a fast inference system optimized for LLMs\nthat exploits the locality property in LLM inference. It uti-\nlizes adaptive predictors and neuron-aware operators for\nneuron activation and computational sparsity. PowerInfer\nachieves up to 11.69\u00d7 faster LLM inference compared to sys-\ntems like llama.cpp, without compromising accuracy.\nReferences\n[1] Abien Fred Agarap. Deep learning using recti\ufb01ed linear units\n(relu). arXiv preprint arXiv:1803.08375, 2018.\n[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,\nAlessandro Cappelli, Ruxandra Cojocaru, Merouane Deb-\nbah, Etienne Gof\ufb01net, Daniel Heslow, Julien Launay, Quentin\nMalartic, Badreddine Noune, Baptiste Pannier, and Guilherme\nPenedo. Falcon-40B: an open large language model with state-\nof-the-art performance. 2023.\n[3] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ah-\nmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase,\nShaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-\ninference: enabling ef\ufb01cient inference of transformer models\nat unprecedented scale. In SC22: International Conference\nfor High Performance Computing, Networking, Storage and\nAnalysis, pages 1\u201315. IEEE, 2022.\n[4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.\nPiqa: Reasoning about physical commonsense in natural lan-\nguage. In Proceedings of the AAAI conference on arti\ufb01cial\nintelligence, volume 34, pages 7432\u20137439, 2020.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020.\n[6] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and\nTri Dao. Medusa: Simple framework for accelerating llm gen-\neration with multiple decoding heads. https://github.com/\nFasterDecoding/Medusa, 2023.\n[7] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-\nBaptiste Lespiau, Laurent Sifre, and John Jumper. Accelerat-\ning large language model decoding with speculative sampling,\n2023.\n[8] Weihao Cui, Zhenhua Han, Lingji Ouyang, Yichuan Wang,\nNingxin Zheng, Lingxiao Ma, Yuqing Yang, Fan Yang, Jilong\nXue, Lili Qiu, Lidong Zhou, Quan Chen, Haisheng Tan, and\nMinyi Guo. Optimizing dynamic neural networks with brain-\nstorm. In 17th USENIX Symposium on Operating Systems De-\nsign and Implementation (OSDI 23), pages 797\u2013815, Boston,\nMA, July 2023. USENIX Association.\n[9] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbo-\ntransformers: an ef\ufb01cient gpu serving system for transformer\nmodels.\nIn Proceedings of the 26th ACM SIGPLAN Sym-\nposium on Principles and Practice of Parallel Programming,\npages 389\u2013402, 2021.\n[10] Wikimedia Foundation. Wikimedia downloads.\n[11] Elias Frantar and Dan Alistarh.\nSparseGPT: Massive lan-\nguage models can be accurately pruned in one-shot.\narXiv\npreprint arXiv:2301.00774, 2023.\n[12] Elias Frantar, Saleh Ashkboos, Torsten Hoe\ufb02er, and Dan\nAlistarh.\nGPTQ:\nAccurate post-training compression\nfor generative pretrained transformers.\narXiv preprint\narXiv:2210.17323, 2022.\n[13] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking\nthe sequential dependency of llm inference using lookahead\ndecoding, November 2023.\n[14] Georgi Gerganov. ggerganov/llama.cpp: Port of facebook\u2019s\nllama model in c/c++.\nhttps://github.com/ggerganov/\nllama.cpp, 2023.\n[15] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nWilliam B Dolan.\nThe third pascal recognizing textual en-\ntailment challenge.\nIn Proceedings of the ACL-PASCAL\nworkshop on textual entailment and paraphrasing, pages 1\u20139,\n2007.\n[16] Song Han, Huizi Mao, and William J Dally.\nDeep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding.\narXiv preprint\narXiv:1510.00149, 2015.\n[17] Song Han, Jeff Pool, John Tran, and William J. Dally. Learn-\ning both weights and connections for ef\ufb01cient neural networks.\nIn Proceedings of the 28th International Conference on Neu-\nral Information Processing Systems - Volume 1, NIPS\u201915,\npage 1135\u20131143, Cambridge, MA, USA, 2015. MIT Press.\n[18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng,\nLianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang,\nand Ion Stoica. Ef\ufb01cient memory management for large lan-\nguage model serving with pagedattention. In Proceedings of\nthe 29th Symposium on Operating Systems Principles, SOSP\n\u201923, page 611\u2013626, New York, NY, USA, 2023. Association\nfor Computing Machinery.\n[19] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li,\nAnkit Singh Rawat, Sashank J Reddi, Ke Ye, Felix Chern,\nFelix Yu, Ruiqi Guo, et al.\nThe lazy neuron phenomenon:\nOn emergence of activation sparsity in transformers. In The\n13\nEleventh International Conference on Learning Representa-\ntions, 2022.\n[20] Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin\nZhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. Do emergent\nabilities exist in quantized large language models: An empiri-\ncal study, 2023.\n[21] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,\nZhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong\nTian, Christopher Re, and Beidi Chen. Deja vu: Contextual\nsparsity for ef\ufb01cient LLMs at inference time.\nIn Andreas\nKrause, Emma Brunskill, Kyunghyun Cho, Barbara Engel-\nhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceed-\nings of the 40th International Conference on Machine Learn-\ning, volume 202 of Proceedings of Machine Learning Re-\nsearch, pages 22137\u201322176. PMLR, 23\u201329 Jul 2023.\n[22] Hanjia\nLyu, Song\nJiang, Hanqing\nZeng, Qifan\nWang,\nSi Zhang, Ren Chen, Chris Leung, Jiajie Tang, Yinglong Xia,\nand Jiebo Luo. Llm-rec: Personalized recommendation via\nprompting large language models, 2023.\n[23] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner:\nOn the structural pruning of large language models.\narXiv\npreprint arXiv:2305.11627, 2023.\n[24] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner:\nOn the structural pruning of large language models. In Ad-\nvances in Neural Information Processing Systems, 2023.\n[25] Iv\u00e1n Mart\u00ednez Toro, Daniel Gallego Vico, and Pablo Orgaz.\nPrivateGPT, May 2023.\n[26] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,\nZeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xi-\naoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen,\nReyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating\ngenerative large language model serving with speculative in-\nference and token tree veri\ufb01cation, 2023.\n[27] Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del\nMundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari,\nand Mehrdad Farajtabar. Relu strikes back: Exploiting activa-\ntion sparsity in large language models, 2023.\n[28] MohamedRashad.\nhttps://huggingface.co/datasets/\nMohamedRashad/ChatGPT-prompts, 2023.\n[29] NVIDIA. Uni\ufb01ed memory programming.\nhttps://docs.\nnvidia.com/cuda/cuda-c-programming-guide/index.\nhtml#um-unified-memory-programming-hd, 2021.\n[30] NVIDIA. cuSPARSE: Basic Linear Algebra for Sparse Ma-\ntrices on NVIDIA GPUs. https://developer.nvidia.com/\ncusparse, 2023.\n[31] OpenAI. https://openai.com/blog/chatgpt, 2023.\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Pe-\nter J Liu. Exploring the limits of transfer learning with a uni-\n\ufb01ed text-to-text transformer. The Journal of Machine Learn-\ning Research, 21(1):5485\u20135551, 2020.\n[33] Google Research. Sputnik: a library of sparse linear algebra\nkernels and utilities for deep learning. https://github.com/\ngoogle-research/sputnik, 2023.\n[34] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gor-\ndon. Choice of plausible alternatives: An evaluation of com-\nmonsense causal reasoning. In 2011 AAAI Spring Symposium\nSeries, 2011.\n[35] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,\nand Yejin Choi.\nWinogrande: An adversarial winograd\nschema challenge at scale.\nCommunications of the ACM,\n64(9):99\u2013106, 2021.\n[36] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper,\nNicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu,\nLianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, and Ion\nStoica. S-lora: Serving thousands of concurrent lora adapters,\n2023.\n[37] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max\nRyabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Sto-\nica, and Ce Zhang. Flexgen: High-throughput generative in-\nference of large language models with a single gpu. 2023.\n[38] SparseLLM.\nRelufalcon-40b.\nhttps://huggingface.co/\nSparseLLM/ReluFalcon-40B.\n[39] SparseLLM.\nRelullama-70b.\nhttps://huggingface.co/\nSparseLLM/ReluLLaMA-70B.\n[40] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A\nsimple and effective pruning approach for large language mod-\nels. arXiv preprint arXiv:2306.11695, 2023.\n[41] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\nHashimoto.\nStanford alpaca: An instruction-following\nllama model. https://github.com/tatsu-lab/stanford_\nalpaca, 2023.\n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\ntinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Roz-\ni\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama:\nOpen and ef\ufb01cient foundation language models.\narXiv\npreprint arXiv:2302.13971, 2023.\n[43] Yiding Wang, Kai Chen, Haisheng Tan, and Kun Guo. Tabi:\nAn ef\ufb01cient multi-level inference system for large language\nmodels. In Proceedings of the Eighteenth European Confer-\nence on Computer Systems, EuroSys \u201923, page 233\u2013248, New\nYork, NY, USA, 2023. Association for Computing Machinery.\n[44] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam\nShleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien\nPlu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush. Transform-\ners: State-of-the-art natural language processing. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations, pages 38\u2013\n45, Online, October 2020. Association for Computational Lin-\nguistics.\n[45] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang,\nZhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuai-\nwen Leon Song.\nFlash-llm: Enabling cost-effective and\nhighly-ef\ufb01cient large generative model inference with unstruc-\ntured sparsity, 2023.\n14\n[46] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien De-\nmouth, and Song Han. Smoothquant: Accurate and ef\ufb01cient\npost-training quantization for large language models. In In-\nternational Conference on Machine Learning, pages 38087\u2013\n38099. PMLR, 2023.\n[47] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong\nKim, and Byung-Gon Chun.\nOrca: A distributed serving\nsystem for Transformer-Based generative models.\nIn 16th\nUSENIX Symposium on Operating Systems Design and Im-\nplementation (OSDI 22), pages 521\u2013538, Carlsbad, CA, July\n2022. USENIX Association.\n[48] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham\nShah, Charles Robert Jankowski, Deqing Yang, and Yanghua\nXiao.\nDistilling script knowledge from large language\nmodels for constrained language planning.\narXiv preprint\narXiv:2305.05252, 2023.\n[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022.\n[50] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong\nSun, and Jie Zhou. MoE\ufb01cation: Transformer feed-forward\nlayers are mixtures of experts. In Findings of ACL 2022, 2022.\n[51] Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han,\nLingxiao Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang,\nLili Qiu, Mao Yang, and Lidong Zhou. Pit: Optimization of\ndynamic sparse deep learning models via permutation invari-\nant transformation. In Proceedings of the 29th Symposium on\nOperating Systems Principles, SOSP \u201923, page 331\u2013347, New\nYork, NY, USA, 2023. Association for Computing Machinery.\n[52] Ningxin Zheng, Bin Lin, Quanlu Zhang, Lingxiao Ma, Yuqing\nYang, Fan Yang, Yang Wang, Mao Yang, and Lidong Zhou.\nSparTA: Deep-Learning model sparsity via Tensor-with-\nSparsity-Attribute. In 16th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 22), pages 213\u2013\n232, Carlsbad, CA, July 2022. USENIX Association.\n[53] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun.\nPetS: A uni\ufb01ed framework for Parameter-Ef\ufb01cient transform-\ners serving. In 2022 USENIX Annual Technical Conference\n(USENIX ATC 22), pages 489\u2013504, Carlsbad, CA, July 2022.\nUSENIX Association.\n15\n"
  },
  {
    "title": "Generative Multimodal Models are In-Context Learners",
    "link": "https://arxiv.org/pdf/2312.13286.pdf",
    "upvote": "31",
    "text": "Generative Multimodal Models are In-Context Learners\nQuan Sun1\u2217\nYufeng Cui1\u2217\nXiaosong Zhang1\u2217\nFan Zhang1\u2217\nQiying Yu2,1\u2217\nZhengxiong Luo1\nYueze Wang1\nYongming Rao1\nJingjing Liu2\nTiejun Huang1,3\nXinlong Wang1\u2020\n1 Beijing Academy of Artificial Intelligence\n2 Tsinghua University\n3 Peking University\n\u2217equal contribution\n\u2020project lead\ncode & models: https://github.com/baaivision/Emu\nAbstract\nThe human ability to easily solve multimodal tasks in\ncontext (i.e., with only a few demonstrations or simple in-\nstructions), is what current multimodal systems have largely\nstruggled to imitate. In this work, we demonstrate that the\ntask-agnostic in-context learning capabilities of large mul-\ntimodal models can be significantly enhanced by effective\nscaling-up. We introduce Emu2, a generative multimodal\nmodel with 37 billion parameters, trained on large-scale\nmultimodal sequences with a unified autoregressive objec-\ntive. Emu2 exhibits strong multimodal in-context learning\nabilities, even emerging to solve tasks that require on-the-fly\nreasoning, such as visual prompting and object-grounded\ngeneration. The model sets a new record on multiple mul-\ntimodal understanding tasks in few-shot settings.\nWhen\ninstruction-tuned to follow specific instructions, Emu2 fur-\nther achieves new state-of-the-art on challenging tasks such\nas question answering benchmarks for large multimodal\nmodels and open-ended subject-driven generation. These\nachievements demonstrate that Emu2 can serve as a base\nmodel and general-purpose interface for a wide range of\nmultimodal tasks. Code and models are publicly available\nto facilitate future research.\n1. Introduction\nMultimodal tasks [26, 42] encompass anything involving\nunderstanding and generation in single or multiple modal-\nities [5, 20, 59], which can be highly diverse and long-\ntail. Previous multimodal systems largely rely on design-\ning task-specific architecture and collecting a sizable super-\nvised training set, both of which are difficult to scale, partic-\nularly when this process needs to be repeated for each new\ntask encountered. By contrast, humans can solve a new task\nin context, i.e., with only a few demonstrations or simple\n\u2020Correspondence to wangxinlong@baai.ac.cn\ninstructions \u2013 a capability that current multimodal models\nhave yet to learn.\nRecently, generative pretrained language models have\ndemonstrated strong in-context learning abilities [12, 22,\n74]. By training a 37-billion-parameter model Emu2 and\nthoroughly evaluating it on diverse multimodal tasks, we\ndemonstrate that a scaled-up multimodal generative pre-\ntrained model can harness similar in-context learning abil-\nities and effectively generalize to unseen multimodal tasks.\nEmu2 is trained with a unified autoregressive objective:\npredict-the-next-multimodal-element (either visual embed-\ndings or textual tokens). In this unified generative pretrain-\ning process, large-scale multimodal sequences (e.g., text,\nimage-text pairs, and interleaved image-text-video) are used\nfor model training.\nWe measure Emu2\u2019s capabilities of learning from a few\nexamples or instructions on standard multimodal datasets,\nas well as new tasks unseen in the training set. Specifically,\nEmu2 is evaluated under two scenarios: (a) few-shot set-\nting, where we allow as many examples as possible to fit\nthe context window of the model; and (b) instruction tun-\ning, where the model is tuned to follow specific instructions.\nEmu2 achieves promising results in the few-shot setting\non a wide range of vision-language tasks. For example, it\ndemonstrates state-of-the-art few-shot performance on mul-\ntiple visual question-answering datasets. We observe a per-\nformance improvement when the number of examples in\ncontext increases. Figure 1 illustrates Emu2\u2019s strong mul-\ntimodal reasoning capabilities for tasks in the wild, e.g.,\nrecognition and counting in a specific format. Emu2 also\nlearns to follow visual prompting in context (e.g., the circles\nlaid on the images in Figure 1), even although it struggles\nat a smaller scale or at zero shot.\nAs Emu2 is inherently equipped to handle interleaved\ntext-image-video at both input and output, it serves as\na powerful and versatile base model for diverse multi-\nmodal tasks, by following specific task instructions. For\nexample, after instruct tuning with conversational data,\n1\narXiv:2312.13286v1  [cs.CV]  20 Dec 2023\nInput Prompt\nIn-context Completion\n[dog: 1, frisbee: 1].\n[cat: 3].\n[beer: 3, banana: 2].\nThe text in the red \ncircle: 'Rights'.\nThe text in the red \ncircle: 'Ave'.\nThe text in the red \ncircle: 'Do Not'.\nThe text in the red circle: \u2018Lynn\u2019.\nmotorcycle's wheel.\nwoman's feet.\ncar's license plate.\nmotorcycle's headlight.\nCompletion\nGenerate from any prompt sequence\n<a man> is riding <a big  horse> on the <water> in front of <mountain>\nimpressionist painting of an \nastronaut in a jungle\nAn image of <emu> wearing a big sunglasses on the beach\n[burger: 1, glass: 1, bottle: 1].\n                    \nand <a dog> \nAn anime draw of   <a baby emu>\n \n, in the style of \na photo of \na yellow \nbackpack:\na photo of \na blue\nbackpack:\na photo of \na red\nbackpack:\na photo of \na brown\nbackpack:\na photo of \na blue and red \nbackpack:\nThe subject A \nwith a city in the \nbackground:\nThe subject A wearing a \nsanta hat:\nThe subject A in a \npurple wizard outfit:\nThe subject A \nwearing a \nrainbow hat:\n <a dog>\n <a dog>\nand\nrunning \non the \ncity \nstreet\nMultimodal understanding\nLook at this sequence of three \nshapes. What shape should come \nas the fourth shape? Explain your \nreasoning with detailed \ndescriptions of the first shapes.\nThe first shape is a triangle, the \nsecond shape is a square, and \nthe third shape is a pentagon. \nTo find the next shape in the \nsequence, we need to identify a \npattern. In this case, the pattern \nis that each shape has one more \nside than the previous shape. \nTherefore, the next shape in the \nsequence should have six sides, \nwhich is a hexagon.\nWhat is funny about this image?\n\u3002\nThe funny aspect of this image is that \na squirrel is holding a camera and \nappears to be taking a picture of a \nbird. This is amusing because \nanimals, such as squirrels and birds, \ndo not typically use cameras or \nengage in human-like activities like \nphotography. The scene is set up in a \nway that makes it look like the \nsquirrel is intentionally capturing the \nbird's image, which adds a humorous \ntwist to the scene. Additionally, the \nbird is perched on a mushroom, \nfurther enhancing the whimsical and \nplayful nature of the image.\nFigure 1. Emu2 is a large generative multimodal model that serves as a foundation and a general-purpose interface for a broad range of\nmultimodal tasks across understanding and generation, with remarkable in-context learning abilities.\n2\nEncoder\nGenerative Multimodal Model\n\u2026\nClassification\n[/IMG] An\n[IMG]\negg that will\n</s>\n<s>\nemu\nhatch into a\nbaby emu\n[/IMG]\n[IMG]\n\u2026\nemu egg that will\nhatch into a\nbaby emu\nAn\n</s>\n[IMG]\n[IMG]\nRegression\n\u2026\n\u2026\nDecoder\nEncoder\nDecoder\n\u2026\nFigure 2. Overview of Emu2 architecture. Emu2 learns with a predict-the-next-element objective in multimodality. Each image in the\nmultimodal sequence is tokenized into embeddings via a visual encoder, and then interleaved with text tokens for autoregressive modeling.\nThe regressed visual embeddings will be decoded into an image or a video by a visual decoder.\nEmu2 achieves state-of-the-art results on visual question-\nanswering tasks, and surpasses previous models of more\ncomplex designs. In addition, Emu2 can be fine-tuned to\nfunction as a controllable visual generation model of high\nquality. It is capable of accepting a mixture of text, loca-\ntions and images as conditions, and generating images that\nare grounded as specified.\nGiven the broad spectrum of capabilities displayed by\nEmu2, we conduct a thorough analysis of its potential so-\ncietal implications and discuss in detail potential concerns\nover misuse. By identifying further tasks where Emu2\u2019s\nin-context learning can further improve, we highlight the\nnecessity for continuous enhancement of the model and the\nimportance of deploying Emu2 responsibly.\n2. Approach\n2.1. Model Architecture\nEmu2 is a generative multimodal model that learns with\na predict-the-next-element objective in multimodal context.\nAs illustrated in 2, the architecture of Emu2 consists of\nthree components:\nVisual Encoder, Multimodal Model-\ning, and Visual Decoder. Each image in the input multi-\nmodal sequence is tokenized into continuous embeddings\nvia the Visual Encoder and then interleaved with text tokens\nfor autoregressive Multimodal Modeling.\nThe regressed\nvisual embeddings are then decoded into an image or a\nvideo by the Visual Decoder. Specifically, we leverage pre-\ntrained EVA-02-CLIP-E-plus [71], LLaMA-33B [74] and\nSDXL [59] to initialize the Visual Encoder, Multimodal\nModeling, and Visual Decoder, respectively. Compared to\nEmu [72], Emu2 embraces a simpler framework which con-\nnects the Visual Encoder and Multimodal Modeling through\nmean pooling each image to 8 \u00d7 8 image patches, followed\nby a linear projection, instead of using an additional C-\nFormer [72].\n2.2. Pretraining\n2.2.1\nData\nThe pretraining data for Emu2 comprises several pub-\nlicly accessible datasets, including image-text pairs from\nLAION-2B [66] and CapsFusion-120M [88], video-text\npairs from WebVid-10M [8], interleaved image-text data\nfrom Multimodal-C4 (MMC4) [96], interleaved video-\ntext data from YT-Storyboard-1B [72], grounded image-\ntext pairs from GRIT-20M introduced by Kosmos-2 [58]\nand CapsFusion-grounded-100M curated by CapsFusion-\n120M. Additionally, language-only data from Pile [27] is\nincluded to retain textual reasoning capability.\n2.2.2\nTraining\nSimilar to Emu [72], Emu2 learns with the predict-the-next-\nelement objective within a multimodal sequence. Each im-\nage is encoded into N = 64 dimension-fixed visual embed-\ndings and then interleaved with text tokens to construct a\nmultimodal sequence. The interleaved sequence is then fed\ninto a Transformer decoder for autoregressive modeling.\nEmu2 is first pretrained on image-text and video-text\npair data with only captioning loss on the text tokens. The\ninput images are resized to 224\u00d7224. We adopt the AdamW\noptimizer [51] with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 1\u00d710\u22126. The\nmaximum learning rate is 1\u00d7 10\u22124 for the linear projection\nlayer, 3\u00d710\u22125 for Multimodel Modeling, and 5\u00d710\u22125 for\nVisual Encoder. We pretrain Emu2 on 162 million image-\ntext samples and 7 million video-text samples for 35,200\niterations. The global batch size is 6,144 for the image-text\n3\npairs and 768 for video-text pairs. The training process is\nthen restarted at a higher 448-pixel resolution for an addi-\ntional 4,000 iterations.\nThen, we freeze the Visual Encoder and only opti-\nmize the linear projection layer and Multimodel Modeling\nwith both text classification loss and image regression loss.\nAdditional datasets including image-text interleaved data,\nvideo-text interleaved data, grounded image-text pair data,\nand language-only data are used in the training. All im-\nages are resized to 448 \u00d7 448, and the maximum learning\nrate is 1 \u00d7 10\u22125. We use a global batch size of 12,800 for\nimage-text pair data, 6,400 for video-text pair data, 3,200\nfor image-text and video-text interleaved data, and 800 for\nlanguage-only data. The training process spans 20,350 iter-\nations and consumes about 160 million samples of image-\ntext data and 3.8B tokens of language-only data.\n2.2.3\nVisual Decoding\nWe train the Visual Decoder to directly decode visual em-\nbeddings generated by the Visual Encoder into image. We\nuse SDXL-base[59] as the initialization of our Visual De-\ncoder, which is fully trained to solve the new task of au-\ntoencoding. Specifically, we use N visual embeddings as\nthe condition input to the Visual Decoder and adjust the di-\nmension of the projection layers in cross-attention modules\nto match the dimension of visual embeddings.\nUnlike Emu [72] where each optimization step of its Vi-\nsual Decoder requires an autoregressive inference of the\nlanguage model, Emu2\u2019s visual decoding can be considered\nas training a detokenizer, which can be trained off-the-shelf\nwithout the language model. Once trained, the Visual De-\ncoder together with the Visual Encoder works as an image\nautoencoder that can tokenize an image into embeddings\nand detokenize back. During Emu2 inference, it generates\nN image embeddings and decodes to an image on the fly.\nFor the decoding of video data, we train a diffusion-\nbased decoder [68]. Similar to [47, 75], we adapt a 2D\ndenoising U-Net to 3D style by inserting a 1D temporal\nconvolution following each 2D spatial convolutional layer\nand extending the spatial attention to spatial-temporal at-\ntention. This video decoder is initialized via Stable Diffu-\nsion 2.1 [63] and fully trained to generate video clips con-\nditioned on visual embeddings from Emu2.\nTraining Setup. We use the images in LAION-COCO [2]\nand LAION-Aesthetics [1] to train the Visual Decoder un-\nder the task of image autoencoding. The Visual Encoder\nand VAE in SDXL are frozen, and only the U-Net is up-\ndated during training. We adopt AdamW optimizer [51]\nwith \u03b21 = 0.9, \u03b22 = 0.999 and the weight decay of 0.01.\nWe use log learning rate warm-up and linear learning rate\ndecay with a peak learning rate of 1 \u00d7 10\u22124 for 2,000 and\n6,000 steps, respectively. We filter out images whose res-\nreal image\nSEED\nEmu\nEmu2\nFigure 3. Comparison of autoencoding results among different\nmethods [28, 72]. Emu2\u2019s Visual Encoder and Visual Decoder in\nthe architecture of CLIP-Diffusers form a strong autoencoder.\nolution is lower than 512 \u00d7 512. The input to the Visual\nEncoder is set to 448 \u00d7 448, while the output of the Vi-\nsual Decoder is set to 1024 \u00d7 1024. We also employ the\nclassifier-free guidance [31], which randomly discards im-\nage embeddings with the probability of 10%. The batch size\nis set to 2,048 in total.\n2.3. Instruction Tuning\nEmu2 can be efficiently aligned to follow specific task in-\nstructions. We fine-tune the base model with conversational\ndata to yield Emu2-Chat, which is capable of following\nmultimodal questions and making responses in dialogue.\nSimilarly, we derive a controllable visual generation model\nEmu2-Gen, which is capable of accepting a mix of text,\nlocations, and images as conditions, and generating images\nthat are grounded in the specified text or subject.\n2.3.1\nInstruction-Following Chat\nTraining Data.\nWe adopt a uniform approach to train\non both academic-task-oriented datasets and multimodal\nchat data to empower Emu2-Chat with the instruction-\nfollowing ability while retaining rich visual knowledge. As\nacademic-task-oriented datasets have brief annotations that\nlimit the model\u2019s capacity to provide more comprehensive\nand helpful responses, we distinguish between these two\ndata categories by employing different system messages\nand including instructions with output-format control in-\nformation as used in [49].\nA summary of data used is\nas follows: (a) Academic-task-oriented data: image cap-\ntioning [19, 67], visual question answering [29, 33, 69],\nknowledgeable question answering [52, 54], multimodal\nclassification [46], and referring expression comprehen-\n4\nsion [35, 53]. (b) Multimodal chat data: GPT-assisted vi-\nsual instruction [50, 94], language instruction [4, 73], clock\nreading [84], and video chat [45].\nTraining Objective.\nIn instruction tuning of Emu2-\nChat, two special tokens, [USER] and [ASSISTANT],\nare incorporated into the model to denote roles.\nThese\ntokens help organize different data types in the follow-\ning format:\n\u201c<Sys.Msg.> [USER]: <Instruction>\n[ASSISTANT]: <Answer>\u201d.\nHere <Sys.Msg.> repre-\nsents system message and varies between the two major task\ncategories (academic-task-oriented and multimodal chat).\nThe <Instruction> section comprises multimodal to-\nkens, including images, videos, and text. Only tokens in\nthe <Answer> section will be supervised by cross-entropy\nloss during training.\nTraining Setup. We use a global batch size of 768 and\ntrain for 8k steps. The learning rate linearly warms up to\n1\u00d710\u22125 in the first 100 steps, then decays to zero with a co-\nsine schedule. The model is trained using the AdamW opti-\nmizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03f5 = 1\u00d710\u22126, and a gradi-\nent clipping of 5.0. The sequence length during training is\nlimited to 2048, and any excess beyond that is truncated di-\nrectly. We consistently employed an input image/video res-\nolution of 448 \u00d7 448. For video data, we uniformly sample\nframes in time as input to the model. The number of sam-\npled frames for each video is randomly chosen from 8, 12,\nand 16. To capture more intricate spatial details, following\nthe visual encoder stage, we apply mean-pooling to each\nstatic image, dividing it into 16 \u00d7 16 tokens during instruc-\ntion fine-tuning. This differs from the pre-training phase,\nwhere 8 \u00d7 8 tokens were utilized.\n2.3.2\nControllable Visual Generation\nTraining Data.\nWe leverage a mix of high-quality\ndatasets to unleash the potential of controllable generation\nin context.\nWe use a grounded image-text pair dataset\nCapsFusion-grounded-100M and GRIT [58] for grounded\ntext-to-image generation. To mitigate the impact of image\nbackgrounds on the effectiveness of multi-entity subject-\ndriven generation, we employ SAM [37] to preprocess the\ngrounding data, yielding a subset of approximately 5 mil-\nlion samples with segmentation results. Additionally, we\nleverage InstructPix2Pix constructed by [11] for image edit-\ning tasks. For the text-to-image task, we use a filtered sub-\nset of CapsFusion [88], LAION-Aesthetics [1], SA-1B [37],\nand LAION-High-Resolution [3].\nWe also collect data from premium sources (e.g., Un-\nsplash [21]) and outputs from advanced text-to-image sys-\ntems (e.g., Midjourney-V5 [55] and DALL-E-3 [10]) for\nquality fine-tuning. This diverse dataset includes around\n500k high-quality image-text pairs. For all the data above,\nduring the training, only samples with image resolutions\nhigher than 448 \u00d7 448 were retained to ensure generation\nquality. More details can be found in the supplementary.\nModel\nShot VQAv2 OKVQA VizWiz TextVQA Hateful\nMemes\nKosmos-1 (1.6B)\n0\n51.0\n-\n29.2\n-\n-\n4\n51.8\n-\n35.3\n-\n-\n8\n51.4\n-\n39.0\n-\n-\nFlamingo (9B)\n0\u2217\n51.8\n44.7\n28.8\n31.8\n57.0\n4\n56.3\n49.3\n34.9\n33.6\n62.7\n8\n58.0\n50.0\n39.4\n33.6\n63.9\n16\n59.4\n50.8\n43.0\n33.5\n64.5\nFlamingo (80B)\n0\u2217\n56.3\n50.6\n31.6\n35.0\n46.4\n4\n63.1\n57.4\n39.6\n36.5\n68.6\n8\n65.6\n57.5\n44.8\n37.3\n70.0\n16\n66.8\n57.8\n48.4\n37.6\n70.0\nIDEFICS (80B)\n0\u2217\n60.0\n45.2\n36.0\n30.9\n60.6\n4\n63.6\n52.4\n40.4\n34.4\n57.8\n8\n64.8\n55.1\n46.1\n35.7\n58.2\n16\n65.4\n56.8\n48.3\n36.3\n57.8\nEmu (14B)\n0\u2217\n52.9\n42.8\n34.4\n-\n-\n4\n58.4\n-\n41.3\n-\n-\n8\n59.0\n-\n43.9\n-\n-\n16\n-\n-\n-\n-\n-\nEmu2 (37B)\n0\n33.3\n26.7\n40.4\n26.2\n52.2\n4\n67.0\n53.2\n54.6\n48.2\n62.4\n8\n67.8\n54.1\n54.7\n49.3\n65.8\n16\n68.8\n57.1\n57.0\n50.3\n66.0\nTable 1. Zero-shot and few-shot evaluations of Emu2. 0\u2217 denotes\ntext two-shot and image zero-shot results following Flamingo [5].\nThe best results are in bold and the second best are underlined.\nTraining Objective.\nWe use the same unified gen-\nerative pretraining objective to adapt to diverse gen-\neration tasks in context.\nSpecifically, a training sam-\nple for generation is formulated as:\n\u201c<s>A photo\nof <p>a man</p><coor>image embedding of\nobject localization image</coor>[IMG]image\nembedding of man[/IMG]sitting next to\n<p>a dog</p><coor>image embedding of\nobject localization image</coor>[IMG]image\nembedding of dog[/IMG][IMG]image embedding\nof the whole image[/IMG]</s>\u201d.\nWe\nrepresent\nthe coordinates of each object directly in image form by\ndrawing the bounding box of each object at its specified\nlocation on a black image.\nOur Emu2-Gen conducts\nunified multimodal modeling of the text, object image, and\ncorresponding object localization image.\nThe regression\nloss only applies to the visual embeddings of the last\nimage. We freeze the Visual Encoder during fine-tuning.\nWe randomly drop tokens of entities and object localization\nimage to enhance model adaptability and robustness. Addi-\ntionally, we apply data augmentation to each object image,\nincorporating random background variations and random\ncrop, aiming to reduce the reliance on image backgrounds.\nTraining Setup. We use a global batch size of 4,096 and\n5\nModel\nVisual Question Answer\nLMM Benchmarks\nVQAv2\nOKVQA\nGQA\nVizWiz\nTextVQA\nMSVD\nMSRVTT\nSEED\nMM-Vet\nTS\nMMMU\nFlamingo-9B [5]\n51.8\n44.7\n-\n28.8\n-\n30.2\n13.7\n-\n-\n-\n-\nFlamingo-80B [5]\n56.3\n50.6\n-\n31.6\n-\n35.6\n17.4\n-\n-\n-\n-\nKosmos-1 [32]\n51.0\n-\n-\n29.2\n-\n-\n-\n-\n-\n-\n-\nKosmos-2 [58]\n51.1\n-\n-\n-\n-\n-\n-\n50.0\n-\n-\n26.6\nBLIP-2-13B [43]\n-\n-\n41.0\n19.6\n42.5\n20.3\n10.3\n46.4\n22.4\n-\n-\nInstructBLIP-13B [23]\n-\n-\n49.5\n33.4\n50.7\n41.2\n24.8\n-\n25.6\n552.4\n-\nIDEFICS-9B [39]\n50.9\n38.4\n-\n35.5\n25.9\n-\n-\n-\n-\n-\n-\nIDEFICS-80B [39]\n60.0\n45.2\n-\n36.0\n30.9\n-\n-\n-\n-\n-\n-\nShikra-13B [16]\n77.4*\n47.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\nQwen-VL-13B-Chat [6]\n78.2*\n56.6*\n57.5*\n38.9\n61.5*\n-\n-\n58.2\n-\n645.2\n-\nLLaVA-1.5-13B [49]\n80.0*\n-\n63.3*\n53.6\n61.3\n-\n-\n61.6\n35.4\n-\n33.6\nCogVLM [78]\n83.4*\n58.9*\n-\n-\n68.1*\n-\n-\n-\n-\n662.6\n30.1\nEmu-I [72]\n62.0\n49.2\n46.0\n38.3\n-\n37.0\n21.2\n-\n36.3\n-\n-\nEmu2-Chat\n84.9*\n64.8*\n65.1*\n54.9\n66.6*\n49.0\n31.4\n62.8\n48.5\n703.8\n34.1\nTable 2. Results on visual question answering and LMM benchmarks. * indicates that samples from this task\u2019s training set have been\ntrained. SEED and TS respectively represent SEED-Bench [40] and TouchStone [7]. For MM-Vet, we present the average result of five\nscoring runs.\ntrain for 3k steps. The learning rate linearly warms up to\n5 \u00d7 10\u22125 in the first 100 steps, then decays to zero with a\ncosine schedule. We further fine-tune for 900 steps using\nthe 500k high-quality pairs with a batch size of 2048.\n3. Evaluation\n3.1. Pretrained Base Model\nWe evaluate zero-shot and few-shot abilities of Emu2 on\nOKVQA [54], VQAv2 [29], VizWiz [30], TextVQA [69],\nand HatefulMemes [36] tasks. Details of the datasets and\nprompts can be found in supplementary materials.\nThe\nresults are presented in Table 1. Emu2 demonstrates re-\nmarkable in-context ability, showcasing improved perfor-\nmance with more in-context samples seen.\nSpecifically,\non VQAv2, VizWiz and TextVQA datasets, Emu2 outper-\nforms Flamingo-80B and IDEFICS-80B under all few-shot\nsettings with a much smaller model scale (37B).\nFigure 1 demonstrates Emu2\u2019s few-shot capabilities in\nthe wild. For example, the model learns to classify and\ncount simultaneously in a specific format via a few exam-\nples (row 1). Additionally, Emu2 is capable of following\nvisual prompts in context, e.g., the red circles laid on the\nimages (row 2 and 3).\n3.2. Instruction-Following Chat\nOur Emu2-Chat is evaluated on academic-task-oriented\nbenchmarks including image question-answering datasets\n(VQAv2 [29], OKVQA [54], GQA [33], VizWiz [30],\nTextVQA [69]) and video question-answering datasets\n(MSVD [83] and MSRVTT [83]). The evaluation also en-\ncompassed recent benchmarks for large multimodal models,\nincluding SEED-Bench [40], MM-Vet [89], TouchStone [7]\nand MMMU [90]. When evaluated on SEED-Bench, we\nfollowed the setup of LLaVa-1.5 [49] by presenting options\nto the model for completing multiple-choice tasks.\nAs shown in Table 2, Emu2-Chat consistently outper-\nforms other models in image question-answering tasks, en-\ncompassing well-established benchmarks like VQAv2 and\nGQA. Notably, it shows a noticeable improvement in the\nOKVQA task, which requires the utilization of external\nknowledge, showcasing the advantage of our model for\nmastering real-world knowledge. When it comes to video\nquestion-answering, Emu2-Chat demonstrated advantages\neven though it did not use video question-answering data\nfor training. It achieved an accuracy of 49.0 and 31.4 on the\nMSVD-QA and MSRVTT-QA tasks, respectively, surpass-\ning InstructBLIP, Emu, and the larger Flamingo-80B. More\nimportantly, our model has also achieved better results on\nLMM benchmarks.\nLMM benchmarks such as MM-Vet\nprovide a more comprehensive evaluation of model abil-\nities, including solving complicated tasks.\nEmu2-Chat\nachieves a score of 48.5 in MM-Vet and 703.8 in Touch-\nStone, confirming its superior capability in understand-\ning and solving multimodal problems compared to existing\nmodels.\n3.3. Controllable Visual Generation\nQualitative Results. Figure 3 presents a visualization of\nEmu2\u2019s autoencoding results.\nWith Emu2\u2019s Visual En-\ncoder and Visual Decoder, we can tokenize an image into\nvisual embeddings and detokenize them back. Compared\nwith SEED [28] and Emu [72], Emu2 shows significantly\nsuperior results. We also evaluate our image autoencod-\ning results on MS-COCO [48] and achieve a strong 0.907\nCLIP-I [60] score. More results are in the supplementary.\n6\nA photo of <the first dog>, \n<the second dog>,  <the third \ndog> on the grass\nA photo of <the first dog>, \n<the second dog>,  <the third \ndog> on the beach\n a bear and  a sunflower\nAn oil painting of <the first \ndog>, <the second dog>,  <the \nthird dog>\n<a bear>            and  <a sunflower>\n<a bear>                     and <a sunflower>\nas an oil painting \nby Monet\n in a dark forest in 3D\n<a dog>                           in\n<A  clock>\nis on the table\nFigure 4. Visualization of Emu2-Gen\u2019s controllable generation capability. The model is capable of accepting a mix of text, locations and\nimages as input, and generating images in context. The presented examples include text- and subject-grounded generation, stylization,\nmulti-entity composition, subject-driven editing, and text-to-image generation.\nAs depicted in Figure 4, Emu2-Gen is capable of ac-\ncepting a mixture of text, locations and images as input,\nand generating images in context.\nThe model skillfully\nengages in various controllable visual generation tasks in\na zero-shot setting, capitalizing on the in-context learn-\ning capabilities in multimodality.\nExamples in Figure 4\nshow generated images of three dogs conditioned on differ-\nent subjects, locations and scenarios. The presented visual\nsamples demonstrate the model\u2019s proficiency in tasks such\nas re-contextualization, stylization, modification, region-\ncontrollable generation, and multi-entity composition.\nZero-shot Text-to-image Generation.\nWe evaluate the\nzero-shot text-to-image generation capability on 30k ran-\ndomly sampled data from the MS-COCO [48] validation\nset. We employ CLIP-ViT-B [61], following the approach\nin DALL-E 3[10], to calculate the CLIP-T score to assess\nprompt-following ability. Additionally, we utilize CLIP-\nViT-L, as in GILL[38], to compute the CLIP-I score for\nmeasuring image similarity. A higher score means the gen-\nerated image is more similar to the prompt or the real image.\nTable 3 shows that Emu2-Gen achieves the state-of-the-art\nperformance in terms of both CLIP-I and CLIP-T scores\ncompared to various unimodal generation models and mul-\ntimodal models. More text-to-image generation cases can\nbe found in supplementary.\nZero-shot\nSubject-driven\nGeneration.\nFollowing\nModels\nCLIP-I \u2191\nCLIP-T \u2191\nunimodal generation models\nMUSE [14]\n-\n0.320\nImagen [65]\n-\n0.270\nDALL-E 2 \u2020 [62]\n-\n0.314\nDALL-E 3 \u2020 [10]\n-\n0.320\nSDv1.5 [63]\n0.667\n0.302\nSDXL [59]\n0.674\n0.310\nmultimodal generation models\nGILL [38]\n0.684\n-\nSEED [28]\n0.682\n-\nEmu [72]\n0.656\n0.286\nEmu2-Gen\n0.686\n0.297\nTable 3. Quantitative comparison of zero-shot text-to-image gen-\neration on MS-COCO [48] validation set. 30k samples are ran-\ndomly sampled. \u2020CLIP-T score is calculated on 4,096 samples.\nWe also evaluate our image autoencoding results on MS-COCO\nwhich achieves a strong 0.907 CLIP-I score.\nKosmos-G [57], we also evaluate our model\u2019s subject-\ndriven image editing ability on DreamBench [64].\nWe\ngenerate four images for each prompt, resulting in a total of\n3,000 images for a comprehensive evaluation. We employ\nDINO [13] and CLIP-I [60] to evaluate subject fidelity,\nand CLIP-T [60] to evaluate text fidelity, aligning with the\n7\nMethods\nDINO \u2191 CLIP-I \u2191 CLIP-T \u2191\nReal Images (Oracle)\n0.774\n0.885\n-\nFine-Tuning\nTextual Inversion [25]\n0.569\n0.780\n0.255\nDreamBooth [64]\n0.668\n0.803\n0.305\nBLIP-Diffusion [43]\n0.670\n0.805\n0.302\nTest Time Tuning Free\nRe-Imagen* [17]\n0.600\n0.740\n0.270\nSuTI [18]\n0.741\n0.819\n0.304\nBLIP-Diffusion* [43]\n0.594\n0.779\n0.300\nKosmos-G* (single image input)\n0.694\n0.847\n0.287\nEmu2-Gen * (single image input)\n0.766\n0.850\n0.287\nTable 4.\nQuantitative comparison of zero-shot single-entity\nsubject-driven generation on DreamBench. * denotes zero-shot\nmethods.\nmethodology established by DreamBooth. Notably, Emu2-\nGen excels in subject fidelity, as evidenced by its superior\nperformance on DINO and CLIP-I metrics compared to\nmethods like BLIP-Diffusion and Kosmos-G. Emu2-Gen\nimpressively reconstructs subjects with just one image\ninput in zero-shot setting, demonstrating superior subject\nfidelity through powerful visual decoding. Further illustra-\ntive cases are provided in the supplementary, showcasing\nEmu2-Gen\u2019s proficiency in multi-entity generation.\n4. Related Work\nLarge Multimodal Models. Recent years have witnessed\nthe rapid growth of large multimodal models [5, 20, 32, 72].\nCLIP [60] has pioneered the learning of large multimodal\nmodels with a contrastive learning objective on massive\nimage-text pair data, yielding impressive zero-shot per-\nformance on image classification tasks.\nThe BEiT se-\nries [9, 76] re-imagines visual signals as discrete tokens,\nallowing for language-model-like pretraining. Flamingo [5]\nand Kosmos series [32, 58] exhibit promising zero-shot and\nfew-shot multi-modal understanding performance by pre-\ntraining on large-scale image-text interleaved data. With\nthe remarkable progress in LLM and its open sourcing,\nconnecting pre-trained vision backbones with LLMs with\nimage-text pairs [43] or visual instruction tuning data [50]\nbecomes a popular solution to efficiently learning large mul-\ntimodal models.\nBLIP series [43, 44], LLaVA [50] and\nMiniGPT4 [95] show promising results by connecting vi-\nsion encoders and LLMs with a small intermediate model.\nA school of successive efforts [70, 77, 85, 91, 92] further\nimproves visual instruction tuning with better overall train-\ning pipelines [6, 41], grounding annotations [15, 16, 86, 93],\nand extra tasks [6]. There are early studies on training more\nunified large multimodal models [24, 28, 72, 87] that are\ncapable of performing multimodal understanding and gen-\neration simultaneously. In this paper, we further explore the\ndistinct solution proposed in Emu [72]: learning large mul-\ntimodal models with generative objectives on both texts and\nimages. By further scaling up generative multimodal mod-\nels, we demonstrate promising in-context learning abilities\nlike those of LLMs on both text and image generation tasks.\nIn-Context Learning. Recent advancements in large lan-\nguage models [12, 22] have underscored their capacity for\nin-context learning [12], where models leverage a few con-\ntextual examples to adapt to new tasks. This phenomenon,\nparticularly evident as LLMs scale up in size and data,\nhas been exploited for complex challenges such as math-\nematical reasoning [82], signaling new emergent ability in\nmodel behavior [81]. There are a few early attempts in in-\ncontext learning in the realm of vision and multimodality.\nFlamingo [5] integrates visual inputs to LLMs, enabling the\nin-context learning of visual-linguistic tasks such as image\ncaptioning and OCR through language-based interfacing.\nPainter [79] and SegGPT [80] conduct an early study of vi-\nsual in-context learning. Inspired by the emerging abilities\nof large language models, in this work we study the prob-\nlem of multimodal in-context learning by scaling up gener-\native multimodal models and demonstrating strong results\nin broad understanding and generation tasks.\n5. Broader Impact and Limitations\nLarge multimodal models offer a wide range of benefits to\nsociety, from enhancing visual navigation and medical diag-\nnostics to increasing accessibility for individuals with visual\nimpairment. The in-context learning capabilities of Emu2\nallow it to quickly adapt to new tasks or environments, even\nwith limited data, ushering in numerous potential applica-\ntions. The generative capabilities of Emu2 can be highly\nvaluable to the creative industries.\nHowever, there are potential downsides in more power-\nful multimodal models to be considered. The hallucination\nissue of multimodal models may cause incorrect and unrea-\nsonable predictions in certain cases. Emu2 may also gen-\nerate harmful or biased content like other generative mod-\nels [56] since the training data may be biased or contain\nunsuitable content. We are actively working to enhance the\nrobustness of multimodal models, reduce model hallucina-\ntions, improve the fairness of training data, and reduce toxic\ndata. We also call on the wider community to pay attention\nto the potential social impact of multimodal models as they\nare growing larger and stronger.\nOne of the limitations of Emu2 is that its in-context\nlearning capability could fail in some complex scenes or\ntasks, e.g., counting in a crowd. Additionally, there is still\na gap between Emu2\u2019s question-answering capability and\nthat of closed multimodal systems. For example, GPT-4V\nachieves 67.7 MM-Vet score vs. Emu2\u2019s 48.5, although al-\n8\nready being state-of-the-art among public models. We be-\nlieve there is much room to improve as the quality and quan-\ntity of training data improve and as model scale continues\nto grow.\n6. Conclusion\nWe present a 37 billion-parameter generative multimodal\nmodel Emu2 that shows strong performance and versatil-\nity on many multimodal tasks in the in-context settings.\nEmu2 serves as a base model and a general-purpose in-\nterface for a variety of multimodal tasks. We demonstrate\nstate-of-the-art results on a broad range of benchmarks of\nmultimodal understanding and generation. Specifically, our\nmodel largely surpasses prior work on the lately proposed\nLMM benchmarks that require more advanced capability\ncompared to classic academic benchmarks.\nEmu2 also\nshows remarkable capability of controllable visual gener-\nation in multimodal context, e.g., subject-/text-grounded\ngeneration.\nAdditionally, we review the limitations and\nbroader social impact of Emu2. Despite discussed weak-\nnesses, these results suggest that generative multimodal\nmodel at scale may be an important step towards the de-\nvelopment of adaptable, general multimodal systems.\nReferences\n[1] Laion-aesthetics. https://laion.ai/blog/laion-\naesthetics/, . 4, 5, 1, 2\n[2] Laion coco:\n600m synthetic captions from laion2b-en.\nhttps://laion.ai/blog/laion-coco/, . 4, 1\n[3] Laion-high-resolution.\nhttps://huggingface.co/\ndatasets/laion/laion-high-resolution, . 5,\n2\n[4] Sharegpt. https://sharegpt.com/. 5, 2\n[5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022. 1, 5, 6, 8, 3\n[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 6,\n8\n[7] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan\nZhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jin-\ngren Zhou. Touchstone: Evaluating vision-language mod-\nels by language models. arXiv preprint arXiv:2308.16890,\n2023. 6\n[8] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 3, 1\n[9] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021. 8\n[10] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,\nYunxin Jiao, and Aditya Ramesh. Improving image genera-\ntion with better captions. 2023. 5, 7, 2\n[11] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n5, 2\n[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 1, 8\n[13] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 7\n[14] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 7\n[15] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li,\nMaosong Sun, and Yang Liu.\nPosition-enhanced visual\ninstruction tuning for multimodal large language models.\narXiv preprint arXiv:2308.13437, 2023. 8\n[16] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 6, 8\n[17] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W\nCohen. Re-imagen: Retrieval-augmented text-to-image gen-\nerator. arXiv preprint arXiv:2209.14491, 2022. 8\n[18] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 8\n[19] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 4, 2\n[20] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian\nGoodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al.\nPali-3 vision language models: Smaller, faster, stronger.\narXiv preprint arXiv:2310.09199, 2023. 1, 8\n[21] Luke Chesser and Timothy Carbone. Unsplash. https:\n//github.com/unsplash/datasets. 5, 2\n[22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\n9\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 1, 8\n[23] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li,\nPascale Fung, and Steven C. H. Hoi.\nInstructblip: To-\nwards general-purpose vision-language models with instruc-\ntion tuning. arXiv preprint arXiv:2305.06500, 2023. 6\n[24] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng\nMa, and Li Yi.\nDreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023. 8\n[25] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 8\n[26] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu,\nJianfeng Gao, et al. Vision-language pre-training: Basics, re-\ncent advances, and future trends. Foundations and Trends\u00ae\nin Computer Graphics and Vision, 14(3\u20134):163\u2013352, 2022.\n1\n[27] Leo Gao, Stella Biderman, Sid Black, Laurence Golding,\nTravis Hoppe, Charles Foster, Jason Phang, Horace He,\nAnish Thite, Noa Nabeshima, Shawn Presser, and Connor\nLeahy. The Pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027, 2020. 3,\n1\n[28] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023. 4, 6, 7, 8\n[29] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904\u20136913, 2017. 4,\n6, 2\n[30] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018. 6\n[31] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[32] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv preprint arXiv:2302.14045, 2023. 6, 8\n[33] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6700\u20136709, 2019. 4, 6, 2\n[34] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:26565\u201326577, 2022. 3\n[35] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 con-\nference on empirical methods in natural language processing\n(EMNLP), pages 787\u2013798, 2014. 5, 2\n[36] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj\nGoswami, Amanpreet Singh, Pratik Ringshia, and Davide\nTestuggine. The hateful memes challenge: Detecting hate\nspeech in multimodal memes. Advances in neural informa-\ntion processing systems, 33:2611\u20132624, 2020. 6\n[37] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 5, 2\n[38] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models.\narXiv\npreprint arXiv:2305.17216, 2023. 7\n[39] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\nObelics: An open web-scale filtered dataset of interleaved\nimage-text documents.\nIn Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Bench-\nmarks Track, 2023. 6\n[40] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 6\n[41] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 8\n[42] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,\nLinjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foun-\ndation models: From specialists to general-purpose assis-\ntants. arXiv preprint arXiv:2309.10020, 1(2):2, 2023. 1\n[43] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 6, 8\n[44] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888\u2013\n12900. PMLR, 2022. 8\n[45] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint\narXiv:2305.06355, 2023. 5, 2\n[46] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,\nShuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu\nSun, et al.\nM3it:\nA large-scale dataset towards multi-\nmodal multilingual instruction tuning.\narXiv preprint\narXiv:2306.04387, 2023. 4, 2\n[47] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu,\nQi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong\n10\nWang. Videogen: A reference-guided latent diffusion ap-\nproach for high definition text-to-video generation.\narXiv\npreprint arXiv:2309.00398, 2023. 4\n[48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 6, 7\n[49] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 4, 6, 3\n[50] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 5, 8, 2\n[51] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 3, 4\n[52] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering.\nAdvances\nin Neural Information Processing Systems, 35:2507\u20132521,\n2022. 4\n[53] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11\u201320, 2016. 5, 2\n[54] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge.\nIn Proceedings\nof the IEEE/cvf conference on computer vision and pattern\nrecognition, pages 3195\u20133204, 2019. 4, 6, 2\n[55] Midjourney. Midjourney. https://www.midjourney.\ncom. 5, 2\n[56] Leonardo Nicoletti and Dina Bass. Humans are biased: Gen-\nerative ai is even worse. Bloomberg Technology+ Equality.\nAccessed June, 23:2023, 2023. 8\n[57] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-g: Generating images\nin context with multimodal large language models. arXiv\npreprint arXiv:2310.02992, 2023. 7, 3\n[58] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 3, 5, 6, 8, 1, 2\n[59] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion models\nfor high-resolution image synthesis, 2023. 1, 3, 4, 7\n[60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 6, 7, 8\n[61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 7\n[62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 7\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 4, 7\n[64] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 7, 8, 3\n[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 7\n[66] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5b:\nAn open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022. 3, 1\n[67] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u2013\n28, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer,\n2020. 4, 2\n[68] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 4\n[69] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8317\u20138326, 2019. 4, 6, 2\n[70] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and\nDeng Cai. Pandagpt: One model to instruction-follow them\nall. arXiv preprint arXiv:2305.16355, 2023. 8\n[71] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 3\n[72] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023. 3, 4, 6, 7,\n8, 1\n11\n[73] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B\nHashimoto. Stanford alpaca: An instruction-following llama\nmodel, 2023. 5, 2\n[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-\nlien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023. 1,\n3\n[75] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 4\n[76] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as\na foreign language: Beit pretraining for vision and vision-\nlanguage tasks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 19175\u2013\n19186, 2023. 8\n[77] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, et al.\nVisionllm: Large language model is also an\nopen-ended decoder for vision-centric tasks. arXiv preprint\narXiv:2305.11175, 2023. 8\n[78] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, et al. Cogvlm: Visual expert for pretrained language\nmodels. arXiv preprint arXiv:2311.03079, 2023. 6\n[79] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and\nTiejun Huang.\nImages speak in images:\nA generalist\npainter for in-context visual learning.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6830\u20136839, 2023. 8\n[80] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,\nChunhua Shen, and Tiejun Huang. Seggpt: Segmenting ev-\nerything in context. arXiv preprint arXiv:2304.03284, 2023.\n8\n[81] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret\nZoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, et al. Emergent abilities of\nlarge language models.\narXiv preprint arXiv:2206.07682,\n2022. 8\n[82] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824\u201324837, 2022. 8\n[83] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video question answer-\ning via gradually refined attention over appearance and mo-\ntion. In Proceedings of the 25th ACM international confer-\nence on Multimedia, pages 1645\u20131653, 2017. 6\n[84] Charig Yang, Weidi Xie, and Andrew Zisserman. It\u2019s about\ntime: Analog clock reading in the wild. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2508\u20132517, 2022. 5, 2\n[85] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 8\n[86] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen\nZhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and\nYinfei Yang. Ferret: Refer and ground anything anywhere\nat any granularity. arXiv preprint arXiv:2310.07704, 2023.\n8\n[87] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al. Scaling autoregressive multi-\nmodal models: Pretraining and instruction tuning.\narXiv\npreprint arXiv:2309.02591, 2023. 8\n[88] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui,\nFan Zhang, Xinlong Wang, and Jingjing Liu.\nCapsfu-\nsion: Rethinking image-text data at scale.\narXiv preprint\narXiv:2310.20550, 2023. 3, 5, 1, 2\n[89] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 6\n[90] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for ex-\npert agi. arXiv preprint arXiv:2311.16502, 2023. 6\n[91] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu,\nand Tat-Seng Chua. Transfer visual prompt generator across\nllms. arXiv preprint arXiv:2305.01278, 2023. 8\n[92] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al.\nInternlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. arXiv preprint\narXiv:2309.15112, 2023. 8\n[93] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 8\n[94] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced\nvisual instruction tuning for text-rich image understanding.\narXiv preprint arXiv:2306.17107, 2023. 5, 2\n[95] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 8\n[96] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak\nGadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig\nSchmidt, William Yang Wang, and Yejin Choi. Multimodal\nc4: An open, billion-scale corpus of images interleaved with\ntext. arXiv preprint arXiv:2304.06939, 2023. 3, 1\n12\nGenerative Multimodal Models are In-Context Learners\nSupplementary Material\nA. More Pretraining Details\nA.1. Dataset Details\nIn pretraining, we exclusively leverage image-text pairs and\nvideo-text pairs for stage 1 training. We additionally lever-\nage interleaved and language-only data altogether for stage\n2. The integration of visual embeddings with text tokens\ngenerates unified multimodal sequences. These sequences\nare then structured by appending the tokens <s> and </s>\nto denote the beginning and end of each sequence.\nImage/Video-text Pairs.\nIn the pretraining stage, we uti-\nlize image-text pairs from LAION-2B [66] and CapsFusion-\n120M [88], along with video-text pairs from WebVid-\n10M [8]. During pretraining stage 2, each image or video\nis randomly placed before or after its corresponding text\nwith a probability of 0.5, respectively. For each video, we\nrandomly sample 8 frames. To structure the visual embed-\ndings, we append two special tokens, [IMG] and [/IMG],\nto signify the start and end of the visual embeddings. In the\ncase of videos, where there are T frames, each frame is en-\ncoded into a set of visual embeddings, and a special token,\n[VIDEO], is prepended to the start of the frame embedding\nsequence. This design helps distinguish between multiple\nimages and video frames within the multimodal sequences.\nInterleaved Image/Video-text Data.\nWe harness the\nMultimodal-C4\n(MMC4)\ndataset\n[96]\nand\nthe\nYT-\nStoryboard-1B dataset [72] as expansive sources of image\nand video-text interleaved data. This approach aims to un-\nlock the in-context learning capability of multimodal mod-\nels. For each MMC4 document, we randomly sample N = 8\nimages, accompanied by their corresponding sentences, to\nconstruct a subsequence of L = 1024. During pretraining\nstage 2, each image or frame is randomly positioned before\nor after its corresponding text with a probability of 0.5. The\nspecial tokens used in this interleaved data are consistent\nwith those employed in the image-text pair data.\nGrounded Image-text Pairs.\nWe curated a dataset of\ngrounded image-text pairs named CapsFusion-grounded-\n100M, employing data from CapsFusion [88] processed\nthrough the dataset construction pipeline proposed by\nKosmos-2 [58]. Additionally, we utilized the 20M GRIT\ndataset introduced by Kosmos-2 [58]. To enhance the di-\nversity and context of the dataset, we randomly positioned\neach phrase before or after its corresponding coordinates\nwith a probability of 0.7. The bounding box can be rep-\nresented using its top-left point (x1, y1) and bottom-right\npoint (x2, y2). We transform continuous coordinates into\n224 discrete tokens [58], the coordinates of a sample box\ncan be formulated as <loc000><loc000><loc224><loc224>.\nWe added these tokens to the word vocabulary to facilitate\nunified modeling with text. To distinguish grounding text\nfrom regular text strings, we introduced two special tokens,\n<coor> and </coor>, marking the beginning and end of\nthe bounding box coordinates. Moreover, to establish the\ncorrect association between bounding boxes and their cor-\nresponding descriptive phrases, an additional set of special\ntokens, <p> and </p>, was appended. To guide the model\nin grounding text output to the provided image, we utilized\nthe special token <grounding>. This comprehensive set\nof tokens and instructions enriches the training data for ef-\nfective multimodal modeling and understanding.\nLanguage-only Data.\nTo maintain text reasoning capa-\nbilities, we engage in joint training with the language mod-\neling dataset Pile [27]. The entire text corpus from Pile is\npreprocessed offline, and each training sample is tokenized\ninto 2048 tokens using the LLaMA tokenizer. We randomly\nsample a total of 3.6 billion tokens for pretraining purposes.\nA.2. Training Hyperparameters\nWe report the detailed training hyperparameter settings of\nEmu2 during the pretraining in Table 5.\nA.3. Visual Decoding\nA.3.1\nDataset Details\nWe utilize images in LAION-COCO [2] and LAION-\nAesthetics [1] to train the Visual Decoder. Images whose\nresolution is smaller than 512 \u00d7 512 are filtered to prevent\ngenerating low-quality results. We employ ratio-preserving\nrandom scaling followed by random cropping of a square\nportion from the scaled image to keep all training images\nunstretched. The original image size and crop coordinates\nare used as additional conditions following SDXL [59].\nA.3.2\nTraining Hyperparameters\nThe detailed hyperparameters of visual decoding training\nare summarized in Table 6.\n1\nConfiguration\nEmu2 Stage 1\nEmu2 Stage 2\nVisual Encoder init.\nEVA-02-CLIP-E-plus\nEmu2 stage 1\nMultimodel Modeling init.\nLLaMA-33B\nEmu2 stage 1\nLinear projection layer init.\nrandom\nEmu2 stage 1\nInput image resolution\n2242 4482\n4482\nOptimizer\nAdamW\nOptimizer hyper-parameters\n\u03b21 = 0.9, \u03b22 = 0.95, eps = 10\u22126\nPeak learning rate\n1 \u00d7 10\u22124, 3 \u00d7 10\u22125, 5 \u00d7 10\u22125\n1 \u00d7 10\u22125\nLearning rate schedule\ncosine decay\nGradient clip\n5.0\nTraining steps\n35.2k 4.0k\n20.35k\nWarmup ratio\n0.02\n0.1\nGlobal batch size*\n6144, 768\n12800, 6400, 3200, 800\nNumerical precision\nbfloat16\nTable 5. Summary of pretraining hyperparameters of Emu2 in\npretraining stages. Peaking leaning rates are 1 \u00d7 10\u22124 for the\nlinear projection layer, 3 \u00d7 10\u22125 for Multimodel Modeling, and\n5 \u00d7 10\u22125 for Visual Encoder. *Global batch size: 1) 6144 for\nimage-text pairs and 768 for video-text pairs in stage 1. 2) 12800\nfor image-text pairs, 6400 for video-text pairs, 3200 for image-\ntext/video-text interleaved data, and 800 for language-only data in\nstage 2.\nConfiguration\nVisual Decoding\nVisual Encoder init.\nEmu2 stage 1\nVisual Decoder init.\nSDXL-base\nEncoder input image resolution\n448 \u00d7 448\nDecoder output image resolution\n1024 \u00d7 1024\nOptimizer\nAdamW\nOptimizer hyper-parameters\n\u03b21 = 0.9, \u03b22 = 0.999, eps = 10\u22128\nPeak learning rate\n1 \u00d7 10\u22124\nLearning rate schedule\nlog warm-up, linear decay\nGradient clip\n1.0\nTotal training steps\n8,000\nWarmup steps\n2,500\nbatch size\n2,048\nNumerical precision\nbfloat16\nClassifier-free guidance\n10%\nNoise offset\n0.1\nTable 6. Summary of training hyperparameters of Emu2 Visual\nDecoder. The Visual Encoder is frozen during training.\nB. Instruction-Following Chat\nB.1. Dataset Details\nWe used two types of training data, academic-task-oriented\ndata and multi-modal chat data, in instruction fine-tuning\nof\nEmu2-Chat\nThe\nacademic-task-oriented\ndatasets\nwe utilized comprise image captioning datasets such\nas COCO Caption [19], and TextCaps [67], as well as\nvisual question-answering datasets like VQAv2 [29],\nOKVQA [54], GQA [33], TextVQA [69], and multi-\nmodal classification data constructed in M3IT [46].\nRefCOCO [35], RefCOCO+ [53] and RefCOCOg [53]\ndatasets are also used. The public multi-modal chat data\nwe use includes GPT-assisted visual instruction data\nLLaVa [50] and LLaVaR [94], language instruction data\nfrom ShareGPT [4] and Alpaca [73], and video instruction\ndata from VideoChat [45]. Beyond these, we constructed\ninstruction fine-tuning data from an analog clock reading\ndataset [84]. For academic-task-oriented datasets, we use\nthe system message \u201cYou are a helpful assistant,\ndedicated to provide concise and efficient\nanswers.\u201d, and for the multi-modal chat data, the sys-\ntem\nmessage\nis\n\u201cYou are a helpful assistant,\ndedicated to delivering comprehensive and\nmeticulous responses.\u201d.\nB.2. Training Hyperparameters\nThe detailed training hyper-parameters of Emu2-Chat are\nsummarized in Table 7.\nConfiguration\nEmu2-Chat\ninit.\nEmu2\nInput image resolution\n448 \u00d7 448\nOptimizer\nAdamW\nOptimizer hyper-parameters\n\u03b21 = 0.9, \u03b22 = 0.98, eps = 10\u22126\nPeak learning rate\n1 \u00d7 10\u22125\nLearning rate schedule\ncosine decay\nGradient clip\n5.0\nTraining steps\n8,000\nWarmup steps\n100\nGlobal Batch size\n768\nNumerical precision\nbfloat16\nTable 7. Summary of training hyperparameters of Emu2-Chat.\nC. Controllable Visual Generation\nC.1. Dataset Details\nWe use the grounded image-text pairs dataset,\ni.e.,\nCapsFusion-grounded-100M\nand\nGRIT\n[58]\nfor\ngrounded text-to-image generation.\nWe use SAM [37]\nto\nobtain\nsegmentation\nresults\nfor\nthe\ncorrespond-\ning grounding boxes.\nWe leverage InstructPix2Pix\nconstructed by [11] for image editing tasks.\nThe\nsample will be formulated as \u201c<s>[IMG]embedding\nof origin image[/IMG]instruct editing\nprompt[IMG]embedding of edited\nimage[/IMG]</s>\u201d. For the text-to-image task, we use a\nfiltered subset the CapsFusion [88], LAION-Aesthetics [1],\nSA-1B [37], and LAION-High-Resolution [3].\nFor high-quality fine-tuning, our datasets were meticu-\nlously sourced from premium sources, e.g., Unsplash [21],\nand outputs from advanced text-to-image systems, e.g.,\nMidjourney-V5 [55] and DALL-E-3 [10]. This comprehen-\nsive approach ensured a diverse and rich dataset, comprising\napproximately 500,000 instances of high-quality image-text\n2\npairs, instrumental in refining and enhancing the aesthetic\nquality of our Emu2-Gen model\u2019s generated images.\nC.2. Training Hyperparameters\nWe report the detailed training hyperparameter settings of\nEmu2-Gen during the instruction-tuning in Table 8.\nConfiguration\nEmu2-Gen stage1\nEmu2-Gen QFT\ninit.\nEmu2\nEmu2-Gen stage1\nInput image resolution\n448 \u00d7 448\nOptimizer\nAdamW\nOptimizer hyper-parameters\n\u03b21 = 0.9, \u03b22 = 0.95, eps = 10\u22126\nPeak learning rate\n5 \u00d7 10\u22125\n1 \u00d7 10\u22125\nLearning rate schedule\ncosine decay\nGradient clip\n1.0\nTraining steps\n3k\n0.9k\nWarmup ratio\n0.0\nGlobal Batch size*\n4096, 3584, 2048\n2048, 1024, 2048\nNumerical precision\nbfloat16\nTable 8. Summary of training hyperparameters of Emu2-Gen.\n*Dataset types are text-to-image pairs, grounded text-to-image and\nimage editing pairs.\nD. Evaluation Details\nPretrained Base Model.\nFor few-shot evaluation of\nEmu2, we adopt the Retrieval In-Context Example Selec-\ntion (RICES) approach for choosing few-shot examples,\nfollowing Flamingo [5] and Emu [72]. The chosen few-shot\nexamples will be separated by \u201c.\n\u201d and then placed ahead\nof the test sample. We use the prompt \u201d[image] based\non the picture, [question] short answer:\u201d.\nFor zero-shot evaluation, as no example is given, we find the\nabove simple prompt cannot effectively control the model\nbehavior and the model tends to output a sentence rather\nthan a word or phrase.\nThus, we modify the prompt to\n\u201d[image] based on the picture, answer in one\nword or phrase.\n[question] short answer:\u201d.\nThis adjustment aligns the model\u2019s output more closely\nwith the distribution of the tested datasets, where responses\ntypically consist of a succinct word or phrase. The splits\nand metrics for each benchmark are detailed in Table 9.\nInstruction-Following Chat.\nThe evaluation of Emu2-\nChat follows the assessment method of Emu-I [72],\nutilizing generation hyper-parameters with a beam size of\n5. For video input, 16 frames are uniformly sampled as\nvisual conditions.\nIn the question-answering benchmark\nthat requires short answers, we employ the system message\n\u201cYou are a helpful assistant, dedicated to\nprovide concise and efficient answers.\u201d along\nwith the output format control information used in [49].\nIn the benchmark for scoring with GPT-4, we use the\nsystem\nmessage\n\u201cYou are a helpful assistant,\ndedicated to delivering comprehensive and\nmeticulous responses.\u201d. We provide an overview of\nthe evaluation benchmarks in Table 9.\nBenchmark\nTask\nSplit\nMetric\nVQAv2\nScene understanding VQA\nTest-dev\nVQA score(\u2191)\nVizWiz\nScene understanding VQA\nTest-dev\nVQA score(\u2191)\nGQA\nUnderstanding & reasoning VQA Test-dev\nEM(\u2191)\nOKVQA\nExternal knowledge VQA\nVal\nVQA score(\u2191)\nTextVQA\nText-oriented VQA\nVal\nVQA score(\u2191)\nHateful Memes Meme classification\nSeen Test ROC AUC(\u2191)\nMSVD-QA\nEvent understanding VQA\nTest\nEM(\u2191)\nMSRVTT-QA\nEvent understanding VQA\nTest\nEM(\u2191)\nMMMU\nMassive multi-discipline QA\nTest\nAccuracy(\u2191)\nSEED-Bench\nImage/Video multi-choice QA\n-\nAccuracy(\u2191)\nMM-Vet\nOpen-ended generation\n-\nGPT-4 score(\u2191)\nTouchStone\nOpen-ended generation\n-\nGPT-4 score(\u2191)\nTable 9. Summary of the evaluation benchmarks.\nControllable Visual Generation.\nFor all evaluation of\nvisual generation tasks, we use EulerDiscreteScheduler [34]\nwith 50 diffusion steps. The classifier-free guidance scale is\nset to 3.0. To evaluate on DreamBench [64], we select ex-\nactly the same image for each object as chosen in Kosmos-\nG [57]. Similarly to Kosmos-G, we also slightly modified\nthe original prompt for the the original prompt with the pre-\nfix \u201da\u201d , for example, \u201da red {}\u201d is modified to \u201d{}Make\nit red\u201d\nE. Qualitative Results\nWe present qualitative cases for Emu2-Gen in Figure 5-11\nand for Emu2-Chat in Figure 12-14, respectively.\nA photo of <the \nfirst dog>, <the \nsecond dog>,  \n<the third dog> \non the grass\nA oil painting of \n<the first dog>, \n<the second \ndog>, <the third \ndog>\nA photo of <the first \ndog>, <the second \ndog>, <the third \ndog> swimming \nunder the water\nFigure 5. Illustration of controllable visual generation of subject-\ndriven generation across multiple images with layout guidance.\n3\nreal image\nautoencode result\nreal image\nautoencode result\nFigure 6. Qualitative cases of image autoencoding.\n4\nIn this scene, a cyclist pedals through a sun-\ndappled forest trail, with rays of light filtering \nthrough the dense foliage. The vibrant colors of \nthe rider's gear harmonize with the natural hues \nof the surrounding flora.\nA woman with natural beauty, her hair gently \nflowing, and a soft, genuine smile, standing in a \nreal-world cherry blossom park.\ncyber punk city street,all building made from \ntranslucent hologram glass material,pastel neon \nlight,night scene with pastel purple and pink sky\nThree minuscule explorers dwarfed by Ethereal \nmonumental gigantic cliffs, minimalistic \nlandscape, beige and gray snowscene , in the \nstyle of Andy Fairhurst.\nHand-painted castle, surrounded by trees, \nautumn, leaves flying\nA panoramic view of the Grand Canyon at \nsunset, with the vast, layered rock formations \ndramatically lit by the fading golden sun, \ncasting deep shadows and highlighting the rich, \nred hues of the canyon walls.\nA dog that has been meditating all the time.\nMajestic waterfall in a lush, green tropical forest.\nA Blue bird\nFigure 7. Illustration of text-to-image generation.\n5\ncute baby panda\n<a panda>                                        on the beach      \nMake it wearing \na very cool \nsunglassess\n<a panda>                                       and <a dog>                                     on the beach      \nAn oil painting of two animals                      , in the style of Van Gogh\nfloats above the \nforest\nis capturing the \nbustling cityscape \nfrom a high rooftop, \nas the sun sets \nbehind skyscrapers.\n<A cartoon character> \n walking in the street. It looks like \nhyper-realistic cityscape at night, with \nrain-soaked streets reflecting neon \nlights.\nReplace the text \nbackground color \nwith yellow\nwearing a big hat \non the beach in \nthe evening\nFigure 8. Illustration of zero-shot controllable visual generation with interleaved vision-language prompt.\n6\n<A man>                                      with <a dog>                                     , and <a car>                                           in GTA-V\n<a bear>                                wearing <a crown>                                           and <a bow tie>                       \nin the \nsnow kingdom           \nwearing <a hat> \n<a cat>\nand eating <a cake>\nA photo of \na birthday \nparty in the \ngarden, \nAn image draw of <a dog>\nand\n<an emu>\nin the style of \nin the cover of \nA photo of <a man>\nholding the\non the stage\n<words on neon lamp>\nAn anime draw of <a cat>\nand <a dog>\nin the style of \n<A singer>               in <a black cloth>               is singing into <microphone>    \n<a pekingese> \n is on the beach with an <umbrella>\nnear the\n<sea>\n<a dog>\n<a book>\non the table\nFigure 9. Illustration of zero-shot controllable visual generation with interleaved vision-language prompt.\n7\nwearing a big sunglasses \non the beach\nfloats above the \nforest\nA person is riding a big  \nhorse on the water in \nfront of mountain\nis capturing the \nbustling cityscape \nfrom a high rooftop, \nas the sun sets behind \nskyscrapers.\n<a dog>                                    \nin a birthday \nparty in the \ngarden\nA blue bird\n<a bear>                      and <a tree>\nin the Christmas party\nFigure 10. Illustration of zero-shot video generation with interleaved vision-language prompt.\n8\nS* in the jungle\nS* with a city in the \nbackground\nS* on the beach\nS* in a chef outfit\nSubject (S*)\nS* on top of the \nsidewalk in a crowded street\nS* in the snow\nS* Make it purple\nS* on a \ncobblestone street\nSubject (S*)\nFigure 11. Zero-shot subject-driven generation results on DreamBench.\n9\nEmu2-Chat\nInput Image\nInput Text\nLook at this sequence of three shapes. What shape should come \nas the fourth shape? Explain your reasoning with detailed \ndescriptions of the first shapes.\nModel Output\nThe first shape is a triangle, the second shape is a square, and the third shape is a \npentagon. To find the next shape in the sequence, we need to identify a pattern. In this \ncase, the pattern is that each shape has one more side than the previous shape. Therefore, \nthe next shape in the sequence should have six sides, which is a hexagon.\nGemini Ultra\nThe fourth shape should be a hexagon. The first shape is a triangle, the second shape is a \nsquare, and the third shape is a pentagon. The number of sides in each shape is increasing \nby one \nCogVLM-Chat\nThe sequence of shapes could be interpreted in various ways, but one possible \ncontinuation is a square, as it complements the triangle and pentagon shapes by forming \na 2D shape that can be thought of as a 'flat' version of the other three. The triangle \nrepresents a 3D shape, the pentagon a 5D shape, and the square a 2D shape.\nFigure 12. Qualitative case of Emu2-Chat.\n10\nEmu2-Chat\nInput Image\nInput Text\nCompare the average kinetic energies of the particles in each sample. The diagrams \nbelow show two pure samples of gas in identical closed, rigid containers. Each colored \nball represents one gas particle. Both samples have the same number of particles. \nWhich sample has the higher temperature? Sample A or sample B?\nModel Output\nThe sample with the higher temperature is Sample A, where the mass of each particle is \n32 u and the average particle speed is 600 m/s. In Sample B, the mass of each particle is \n32 u, but the average particle speed is 540 m/s. Since the temperature is a measure of the \naverage kinetic energy of the particles, and kinetic energy is proportional to the square of \nthe velocity, the higher the velocity, the higher the temperature. In Sample A, the higher \naverage particle speed results in a higher temperature compared to Sample B.\nGPT-4V\nBecause the average particle speed in sample A is higher than in sample B (600 m/s vs. \n540 m/s), the particles in sample A have a higher average kinetic energy. Therefore, \nsample A has a higher temperature than sample B. \nCogVLM-Chat\nThe kinetic energy of a particle is given by the formula (1/2)mv^2, where m is the mass \nand v is the velocity of the particle. Since the velocities of the particles in both samples \nare the same, the sample with the higher mass particles will have the higher kinetic \nenergy. Therefore, Sample A, with particles having a mass of 32 u, will have a higher \naverage kinetic energy than Sample B, which has particles with a mass of 32 u.\nFigure 13. Qualitative case of Emu2-Chat.\n11\nEmu2-Chat\nInput Image\nInput Text\nImagine you are a guiding robot. \nHere is a photo I took. Please tell me how to get to the restroom.\nModel Output\nTo get to the restrooms, you should follow the \"Restrooms\" sign in the picture. The sign \nis pointing to the left, so you should head in that direction to find the restrooms. If you \nneed further assistance, please let me know, and I'll do my best to help you navigate to \nthe restrooms.\nCogVLM-Chat\nTo get to the restroom, follow the direction indicated by the sign. It's located to the right \nof the sign and is labeled 'Restrooms'.\nQwen-VL-Chat\nTo reach the restrooms, follow the directional arrow on the sign pointing downwards and \ntowards your right side. The restroom should be located in that direction based on the \ninformation provided by the sign. Keep an eye out for any additional signs or markers \nindicating where the restrooms might be within this area of the building.\nFigure 14. Qualitative case of Emu2-Chat.\n12\n"
  },
  {
    "title": "Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model",
    "link": "https://arxiv.org/pdf/2312.13252.pdf",
    "upvote": "25",
    "text": "Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model\nSaurabh Saxena\u2020, Junhwa Hur\u2021, Charles Herrmann\u2021, Deqing Sun\u2021, David J. Fleet\u2020*\n\u2020Google DeepMind\n\u2021Google Research\n{srbs,junhwahur,irwinherrmann,deqingsun,davidfleet}@google.com\nAbstract\nWhile methods for monocular depth estimation have\nmade significant strides on standard benchmarks, zero-shot\nmetric depth estimation remains unsolved. Challenges in-\nclude the joint modeling of indoor and outdoor scenes,\nwhich often exhibit significantly different distributions of\nRGB and depth, and the depth-scale ambiguity due to un-\nknown camera intrinsics. Recent work [5] proposed a spe-\ncialized multi-head architecture for jointly modeling indoor\nand outdoor scenes. In contrast, we advocate a generic,\ntask-agnostic diffusion model, with several advancements\nsuch as log-scale depth parameterization to enable joint\nmodeling of indoor and outdoor scenes, conditioning on the\nfield-of-view (FOV) to handle scale ambiguity and syntheti-\ncally augmenting FOV during training to generalize beyond\nthe limited camera intrinsics in training datasets. Further-\nmore, by employing a more diverse training mixture than\nis common, and an efficient diffusion parameterization, our\nmethod, Diffusion for Metric Depth (DMD) achieves a 25%\nreduction in relative error (REL) on zero-shot indoor and\n33% reduction on zero-shot outdoor datasets over the cur-\nrent SOTA [5] using only a small number of denoising steps.\nFor an overview see diffusion-vision.github.io/dmd\n1. Introduction\nMonocular estimation of metric depth in general environ-\nments, while useful for applications such as mobile robotics\nand autonomous driving, has proven elusive. The two main\nbarriers stem from (1) the large differences in RGB and\ndepth distributions one finds in indoor and outdoor datasets,\nand (2) intrinsic scale ambiguity in images when one lacks\nknowledge of camera intrinsics. Not surprisingly, most cur-\nrent models for monocular depth are either specific to in-\ndoor or outdoor scenes, or, if trained for both, they estimate\nscale-invariant depth.\nCurrent models for metric depth are often trained solely\non indoor or outdoor scenes, primarily on a single dataset\ncaptured with fixed camera intrinsics (e.g., with an RGBD\n*DF is also affiliated with the University of Toronto and the Vector\nInstitute.\n0.0\n0.2\n0.4\n0.6\n0.8\nRelative depth error (REL\n)\nNYU*\nSunRGBD\nibims-1\nDIODE\nIndoor\nHypersim\n-6.5%\n-26.0%\n-36.6%\n-12.1%\n-24.1%\nIndoors\nDMD (ours)\nZoeDepth\n0.0\n0.2\n0.4\n0.6\n0.8\nRelative depth error (REL\n)\nKITTI*\nVKITTI 2\nDDAD\nDIML\nOutdoor\nDIODE\nOutdoor\n-7.0%\n-12.4%\n-21.7%\n-70.4%\n-26.9%\nOudoors\nDMD (ours)\nZoeDepth\nFigure 1.\nRelative depth error for DMD (ours) compared to\nZoeDepth (SOTA) on eight zero-shot and two in-distribution (\u2217)\nbenchmarks. DMD outperforms ZoeDepth by a substantial mar-\ngin on all benchmarks.\ncamera, or, with RGB+LIDAR for outdoor scenes). Such\nmodels avoid challenges caused by different indoor and out-\ndoor depth distributions, but at the cost of generality. They\nalso overfit to the camera intrinsics of the training dataset\nand do not generalize well to out of distribution data.\nThe predominant approach to jointly modeling indoor\nand outdoor data is to estimate scale- and shift-invariant\ndepth, rather than metric depth (e.g., MiDaS [34]). By nor-\nmalizing the depth distributions, one brings indoor and out-\ndoor depth distributions closer, and also avoids the prob-\nlems of scale ambiguities in the presence of variable cam-\nera intrinsics. Recently there has been growing interest in\nbridging these different approaches, training joint indoor-\noutdoor models that estimate metric depth. To cope with\nboth indoor and outdoor domains, ZoeDepth [5] adds two\nheads to MiDaS [34], one for each domain, to convert from\nscale-invariant depth to metric depth.\nIn this paper we advocate denoising diffusion models for\nzero-shot metric depth estimation, with the aid of several\nkey innovations to obtain state of the art performance. In\nparticular, field-of-view (FOV) augmentation is used dur-\ning training to improve generalization to different camera\nintrinsics, and FOV conditioning in training and inference\nhelps to resolve intrinsic scale ambiguities, providing a fur-\n1\narXiv:2312.13252v1  [cs.CV]  20 Dec 2023\nther boost in performance. Representing depth in the log\ndomain allocates model capacity in a more balanced way for\nindoor and outdoor scenes, improving indoor performance.\nFinally, we find that v-parameterization in neural network\ndenoising greatly increases inference speed. The resulting\nmodel, dubbed DMD (Diffusion for Metric Depth), outper-\nforms recently proposed metric depth model, ZoeDepth [5].\nIn particular, DMD yields much lower relative depth er-\nror than ZoeDepth on all eight out-of-distribution datasets\nin [5] when fine-tuned on the same data. Expanding the\ntraining dataset leads to further improvement in perfor-\nmance (Fig. 1).\nTo summarize, we make the following contributions:\n\u2022 We introduce DMD, a simple yet effective method for\nzero-shot metric depth estimation on general scenes.\n\u2022 We propose synthetically augmenting the FOV for im-\nproved generalization, FOV conditioning to resolve\ndepth-scale ambiguity and representing depth in log-scale\nto better utilize the model\u2019s representation capacity.\n\u2022 DMD establishes a SOTA on zero-shot metric depth,\nachieving 25% and 33% lower relative error than\nZoeDepth on indoor and outdoor datasets,\nrespec-\ntively, while being efficient, owing to the use of v-\nparameterization for diffusion.\n2. Related Work\nMonocular depth (fine-tuned and evaluated in-domain).\nGiven the challenges of learning a joint indoor-outdoor\nmodel, most approaches have restricted models to target a\nsingle dataset (either indoor or outdoor) with fixed intrin-\nsics. In this setting, great progress has been made with ad-\nvancements in specialized architectures [15, 16] such as the\nuse of binning [1, 3, 8, 18, 30] or loss functions [15, 28]\nthat are suited for this task. [2] proposed combining multi-\nple training datasets with variable intrinsics by normalizing\nthe images to the same camera intrinsic.\nJoint indoor-outdoor models.\nTo train joint indoor-\noutdoor models, one can mitigate the difficulty of learn-\ning diverse scene statistics by estimating scale- and shift-\ninvariant depth instead.\nMiDaS [34] trains their model\non diverse indoor-outdoor datasets and demonstrates good\ngeneralization to various unseen datasets. However, they\ndo not provide metric depth.\nDPT [35] leverages this\nfor pre-training and further fine-tunes separately for metric\ndepth on NYU and KITTI. ZoeDepth [5] proposes adding\na mixture-of-experts head, supervised by scene type, on top\nof a similarly pre-trained model, thereby handling indoor\nand outdoor scenes. In contrast, our model, DMD, uses a\nrelatively generic framework, without domain-specific ar-\nchitectural components.\nIntrinsics-conditioned monocular depth. Incorporating\ncamera intrinsics for depth estimation has been briefly ex-\nplored in previous work [17, 22]. They argue that intrinsic-\nconditioning allows one to train on multiple datasets with\nvarying intrinsics, but this is only demonstrated with small-\nscale experiments. Similar to our method, ZeroDepth [21]\nintroduces an intrinsic-conditioned metric-scale depth esti-\nmator that is trained on large-scale training datasets.\nWhile conditioning on the input field-of-view, we intro-\nduce a novel field-of-view augmentation scheme that aug-\nments training data by cropping or uncropping to simulate\ndiverse FOVs and provide a large scale study on zero-shot\ngeneralization to both indoor and outdoor domains as well\nas robustness to diverse camera intrinsics.\nDiffusion for vision. Denoising diffusion models [23, 43]\nhave recently emerged as a powerful class of generative\nmodels. Although initially proposed for natural image gen-\neration [13, 24, 32, 38], they have recently been shown to be\neffective for several computer vision tasks such as semantic\nsegmentation [26], panoptic segmentation [9], optical flow\n[40] and monocular depth estimation [14, 26, 40]. Ours is\nthe first demonstration that diffusion models can also sup-\nport state-of-the-art zero-shot metric depth estimation for\ngeneral indoor or outdoor scenes.\n3. Diffusion for Metric Depth (DMD)\nIn what follows we describe DMD (Diffusion for Metric\nDepth) and the design decisions to solve these issues. In\nparticular, we cast monocular depth estimation as a gener-\native RGB-to-depth translation task using denoising diffu-\nsion. To this end we introduce several technical innovations\nto conventional diffusion models and training procedures to\naccommodate zero-shot, metric depth.\n3.1. Diffusion models\nDiffusion models are probabilistic models that assume a for-\nward process that gradually transforms a target distribution\ninto a tractable noise distribution. A learned neural denoiser\nis trained to reverse this process, iteratively converting a\nnoise sample to a sample from the target distribution. They\nhave been shown to be remarkably effective with images\nand video, and they have recently begun to see use for dense\nvision tasks like segmentation, tracking, optical flow, and\ndepth estimation. They are attractive as they exhibit strong\nperformance on regression tasks, capturing posterior uncer-\ntainty, without task specific architectures, loss functions and\ntraining procedures.\nFor DMD we build on the task-agnostic Efficient U-Net\narchitecture from DDVM [40].\nWhile DDVM used the\n\u03f5-parameterization for training the neural depth denoiser,\nhere instead we use the v-parameterization [39]. We find\nthat the v-parameterization yields remarkably efficient in-\nference, using as few as one or two refinement steps, with-\nout requiring progressive distillation [39].\n2\nUnder the v-parameterization, the denoising network is\ngiven a noisy target image (in our case a depth map), zt =\n\u03b1tx+\u03c3t\u03f5, where x is the noiseless target input (depth map),\n\u03f5 \u223c N(0, I), t \u223c U(0, 1), \u03c32\nt = 1\u2212\u03b12\nt , and \u03b1t > 0 is com-\nputed with a pre-determined noise schedule, and the denois-\ning network predicts v \u2261 \u03b1t\u03f5\u2212\u03c3tx. From the output of the\ndenoising network, i.e., v\u03b8(zt, y, t), where y is an optional\nconditioning signal (RGB image in this case), one obtains\nan estimate of x at step t, i.e., \u02c6xt = \u03b1tzt \u2212 \u03c3tv\u03b8(zt, y, t),\nand the corresponding estimate of the noise, denoted \u02c6\u03f5t.\nUnder this parameterization, with a conventional L2 norm,\nthe training objective is based on the expected \u2018truncated\nSNR weighting\u2019 loss, i.e., max(\u2225x \u2212 \u02c6xt\u22252\n2, \u2225\u03f5 \u2212 \u02c6\u03f5t\u22252\n2) [39].\nMotivated by the superior performance of the L1 loss in\ntraining DDVM [40] compared to the L2, we similarly em-\nploy a L1 loss for DMD as well, yielding the objective\nEx,y,t,\u03f5 [max(\u2225x \u2212 \u02c6xt\u22251, \u2225\u03f5 \u2212 \u02c6\u03f5t\u22251)] .\n(1)\n3.2. Joint indoor-outdoor modelling\nTraining a joint indoor-outdoor model can be difficult be-\ncause of the large differences in depth distributions one\nfinds in indoor and outdoor scenes. Much of the available\nindoor training data have depths up to 10m, while outdoor\nscenes include ground truth depths up to 80m.\nFurther,\ntraining data is often lacking the variation in camera intrin-\nsics needed for robustness to images from different cam-\neras. Rather, many datasets are captured with a fixed cam-\nera. To mitigate these issues we propose three innovations,\nnamely, the use of log depth, field of view augmentation,\nand field of view conditioning.\nLog depth. Diffusion models usually model data distribu-\ntion in [-1, 1]. One might convert metric depth to this range\nwith linear scaling, i.e.,\ndlin = normalize(dr/dmax) ,\n(2)\nwhere dr is the raw depth in meters, dmax is often taken to\nbe 80m, to accommodate the usual outdoor depth range, and\nnormalize(d) = clip(2\u2217d\u22121, \u22121, 1). This, however, allo-\ncates little representation capacity to indoor scenes (where\ndepths are usually less than 10m).\nInstead, we can allocate more representation capacity to\nindoor scenes with log-scaled depth (dlog) as the target for\ninference, i.e.,\ndlog = normalize\n\u0012 log(dr/dmin)\nlog(dmax/dmin)\n\u0013\n,\n(3)\nwhere dmin and dmax denote the minimum and maximum\nsupported depths (e.g., 0.5m and 80m). Empirically, we\nfind log scaling to be remarkably beneficial.\nField-of-view augmentation.\nBecause datasets for depth\nestimation often have little or no variation in the field of\nview, it is easy for models to over-fit and thus generalize\npoorly to images with different camera intrinsics. To en-\ncourage models to generalize well to different fields of view,\nwe propose to augment training data by cropping or uncrop-\nping to simulate diverse FOVs. While cropping is straight-\nforward, for uncropping it is unclear how best to pad the\nenlarged image. Our preliminary experiments used gener-\native uncropping with Palette [37], however, we found that\npadding the RGB image with Gaussian noise (mean-zero,\nvariance 1) works as well, is simpler and more efficient.\nFor missing ground truth depth with uncropping aug-\nmentation, we adopt the approach in [40], using a combi-\nnation of near-neighbor in-filling and step-unrolled denois-\ning during training. It is shown in [40] that this technique\nis effective in coping with the inherent distribution shift be-\ntween training and testing when ground truth data are noisy\nor incomplete.\nField-of-view conditioning. Metric depth estimation from\na single image is ill-posed in the presence of unknown cam-\nera intrinsics, as depth scales inversely with the field-of-\nview. While one might hope that diversifying camera in-\ntrinsics through FOV augmentation will help generalization\nto different cameras, we and others [48] observe that it is\nnot sufficient in itself. FOV augmentations do help simu-\nlate some variation in camera intrinsics, but variations in\nother factors, like focal length, are hard to simulate.\nAs a conditioning signal, to help disambiguate depth\nscale, we use tan(\u03b8/2), where \u03b8 is the vertical FOV. We\nexplored conditioning on the horizontal FOV as well, but\nthat did not improve results substantially.\n4. Experiments\n4.1. Training data\nDDVM [40] showed that using large amounts of diverse\ntraining data is important for generic models with no task-\nspecific inductive biases. Here we follow the training strat-\negy proposed in [40], initializing the model with unsuper-\nvised pre-training on ImageNet [12] and Places365 [52],\nwith tasks proposed in [37]. This is followed by supervised\npre-training on ScanNet [11], SceneNet-RGBD [31], and\nWaymo [45]. Unlike [40], we also include the DIML In-\ndoor [10] dataset for more diversity. No FOV augmentation\nor conditioning is used for this pre-training stage.\nFor the final training stage we train on a mixture of NYU\n[42], Taskonomy [51], KITTI [19] and nuScenes [7]. At\nthis stage we apply FOV augmentation to NYU, KITTI and\nnuScenes, but not Taskonomy as it is large and has sub-\nstantial FOV diversity and also add the FOV conditioning.\nFinally, to enable fair comparisons with the current SOTA\nZoeDepth [5] model, we also train a model solely from\nNYU and KITTI, like ZoeDepth.\n3\nImage\nGround Truth\nZoeDepth [5]\nDMD (ours)\nDIODE\nIndoor\nHypersim\nibims-1\nNYU\nSunRGBD\nFigure 2. Qualitative comparison between our method and ZoeDepth [5] on indoor scenes. Unlike ZoeDepth, our method estimates depths\nat more accurate scale over diverse datasets.\nSUN RGB-D\niBims-1 Benchmark\nDIODE Indoor\nHyperSim\nMethod\n\u03b41 \u2191\nREL \u2193 RMSE \u2193\n\u03b41 \u2191\nREL \u2193 RMSE \u2193\n\u03b41 \u2191\nREL \u2193 RMSE \u2193\n\u03b41 \u2191\nREL \u2193 RMSE \u2193\nBTS [29]\n0.740\n0.172\n0.515\n0.538\n0.231\n0.919\n0.210\n0.418\n1.905\n0.225\n0.476\n6.404\nAdaBins [3]\n0.771\n0.159\n0.476\n0.555\n0.212\n0.901\n0.174\n0.443\n1.963\n0.221\n0.483\n6.546\nLocalBins [4]\n0.777\n0.156\n0.470\n0.558\n0.211\n0.880\n0.229\n0.412\n1.853\n0.234\n0.468\n6.362\nNeWCRFs [50]\n0.798\n0.151\n0.424\n0.548\n0.206\n0.861\n0.187\n0.404\n1.867\n0.255\n0.442\n6.017\nZoeD-M12-NK [5]\n0.856\n0.123\n0.356\n0.615\n0.186\n0.777\n0.386\n0.331\n1.598\n0.274\n0.419\n5.830\nDMD-NK\n0.914\n0.109\n0.306\n0.801\n0.130\n0.563\n0.402\n0.298\n1.407\n0.356\n0.382\n5.527\nDMD-MIX\n0.930\n0.091\n0.275\n0.859\n0.118\n0.447\n0.380\n0.291\n1.292\n0.497\n0.318\n4.394\nTable 1. Zero-shot results on unseen indoor datasets (evaluated at pixels with GT depth less 8m for SUN RGB-D, 10m for iBims-1 and\nDIODE Indoor, and 80m for Hypersim).\nBest and second-best results are highlighted. Trained on the same data (NYU and KITTI),\nDMD-NK outperforms ZoeD-M12-NK. With more data (Taskonomy and nuScenes), DMD-MIX outperforms by a much larger margin.\n4\nImage\nGround Truth\nZoeDepth [5]\nDMD (ours)\nDDAD\nDIML\nOutdoor\nDIODE\nOutdoor\nKITTI\nVirtual\nKITTI 2\nFigure 3. Qualitative comparison between DMD and ZoeDepth [5] on outdoor scenes. Compared with ZoeDepth [5], our method is able\nto estimate a more accurate depth scale.\nVirtual KITTI 2\nDDAD\nDIML Outdoor\nDIODE Outdoor\nMethod\n\u03b41 \u2191\nREL \u2193 RMSE \u2193\n\u03b41 \u2191\nREL \u2193 RMSE \u2193\n\u03b41 \u2191\nREL \u2193 RMSE \u2193\n\u03b41 \u2191\nREL \u2193 RMSE \u2193\nBTS [29]\n0.831\n0.115\n5.368\n0.805\n0.147\n7.550\n0.016\n1.785\n5.908\n0.171\n0.837\n10.48\nAdaBins [3]\n0.826\n0.122\n5.420\n0.766\n0.154\n8.560\n0.013\n1.941\n6.272\n0.161\n0.863\n10.35\nLocalBins [4]\n0.810\n0.127\n5.981\n0.777\n0.151\n8.139\n0.016\n1.820\n6.706\n0.170\n0.821\n10.27\nNeWCRFs [50]\n0.829\n0.117\n5.691\n0.874\n0.119\n6.183\n0.010\n1.918\n6.283\n0.176\n0.854\n9.228\nZoeD-M12-NK [5]\n0.850\n0.105\n5.095\n0.824\n0.138\n7.225\n0.292\n0.641\n3.610\n0.208\n0.757\n7.569\nDMD-NK\n0.872\n0.093\n4.828\n0.842\n0.122\n6.740\n0.544\n0.300\n2.522\n0.162\n0.627\n9.577\nDMD-MIX\n0.890\n0.092\n4.387\n0.907\n0.108\n5.365\n0.602\n0.190\n2.089\n0.187\n0.553\n8.943\nTable 2. Zero-shot results on four unseen outdoor datasets.\nBest and second-best results are highlighted. Following pre-training,\nDMD-NK is trained on NYUv2 and KITTI, while DMD-MIX adds Taxonomy and NuScenes for training. Using the same fine-tuning data,\nDMD-NK outperforms ZoeD-M12-NK on all benchmarks except DIODE Outdoor where it outperforms on REL but is behind on other\nmetrics. DMD-MIX shows further performance improvements over DMD-NK with the expanded training mixture.\nIndoors\nOutdoors\nExperiment\nNYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD\nREL\nLinear scaling\n0.082\n0.108\n0.324\n0.146\n0.398\n0.056\n0.467\n0.630\n0.092\n0.122\nLog scaling\n0.076\n0.109\n0.298\n0.130\n0.382\n0.055\n0.300\n0.628\n0.093\n0.122\nRMS Linear scaling\n0.340\n0.314\n1.526\n0.612\n5.693\n2.516\n3.126\n10.129\n4.788\n6.288\nLog scaling\n0.313\n0.306\n1.407\n0.563\n5.527\n2.527\n2.522\n9.577\n4.828\n6.740\nTable 3. Ablation showing that log depth improves quantitative performance on indoor datasets which is understandable since log-scaling\nincreases the share of representation capacity allocated to shallow depths.\n5\nImage\nGround Truth\nPrediction with linear-scaling\nPrediction with log-scaling\nFigure 4. Linearly scaling depth leads to noisy predictions for images with shallow depth. See Section 3.2 for more details. Predicting\ndepth in a log-scale fixes this. Note that here we use a max depth of 5 meters for better visualization.\nImage\nGround Truth\nDMD-NK\nDMD-MIX\nibims-1\nDDAD\nFigure 5. Qualitative comparison between DMD-NK (fine-tuned on NYU and KITTI) and DMD-MIX (fine-tuned on KITTI, NYU,\nnuScenes, and Taskonomy). DMD-MIX further improves depth scale estimation as well as fine details on depth boundaries.\nIndoors\nOutdoors\nIn-distribution\nOOD\nFigure 6. Plots showing the effect of perturbing the FOV during\ninference. Optimal performance is at or near the true FOV. Perfor-\nmance degrades with larger perturbation.\n4.2. Design choices\nDenoiser Architecture. We adopt the modifications of the\nEfficient U-Net [38] proposed in DDVM [40], with one fur-\nther modification to support FOV conditioning. The FOV\nembedding, like the timestep embedding, is constructed by\nfirst building a sin-cos positional embedding [47] followed\nby linear projection. The sum of these two embeddings is\nused to modulate different layers of the denoiser backbone\nusing FiLM [33] layers. Following [5] we train at a resolu-\ntion of 384\u00d7512. The predicted depth maps are resized to\nthe ground-truth resolution for evaluation, following prior\nwork [5]. Other training hyper-parameters such as the batch\nsize and optimizer details are like those in [40].\nAugmentations. In addition to the FOV augmentation (Sec.\n3.2) we use random horizontal flip augmentation, like many\nprior works.\nSampler. We use the DDPM [23] sampler with eight de-\nnoising steps for indoor datasets. For outdoor datasets we\nfind that two denoising steps suffice. We report results using\na mean of eight samples, following [40]; as shown in Table\n6, sample averaging leads to improved performance.\nEvaluation. We adopt the evaluation protocol of ZoeDepth\n[5]. We report in-distribution performance on the NYU [42]\nand KITTI [19] datasets, and generalization performance on\neight unseen datasets [5], namely, SunRGBD [44], iBims-\n1 [44], DIODE Indoor [46], Hypersim [36] for indoors,\nand Virtual KITTI 2 [6], DDAD [20], DIML Outdoor [10],\n6\nMethod\nNYU\nKITTI\n\u03b41\u2191\n\u03b42\u2191\n\u03b43 \u2191\nREL \u2193\nRMS \u2193\nlog10 \u2193\n\u03b41\u2191\n\u03b42\u2191\n\u03b43\u2191\nREL \u2193\nSq-rel \u2193\nRMS \u2193\nRMS log \u2193\nDomain-specific models:\nBTS [29]\n0.885\n0.978\n0.994\n0.110\n0.392\n0.047\n0.956\n0.993\n0.998\n0.059\n0.245\n2.756\n0.096\nDPT [35]\n0.904\n0.988\n0.998\n0.110\n0.357\n0.045\n0.959\n0.995\n0.999\n0.062\n\u2013\n2.573\n0.092\nAdaBins [3]\n0.903\n0.984\n0.997\n0.103\n0.364\n0.044\n0.964\n0.995\n0.999\n0.058\n0.190\n2.360\n0.088\nNeWCRFs [50]\n0.922\n0.992\n0.998\n0.095\n0.334\n0.041\n0.974\n0.997\n0.999\n0.052\n0.155\n2.129\n0.079\nBinsFormer [30]\n0.925\n0.989\n0.997\n0.094\n0.330\n0.040\n0.974\n0.997\n0.999\n0.052\n0.151\n2.098\n0.079\nPixelFormer [1]\n0.929\n0.991\n0.998\n0.090\n0.322\n0.039\n0.976\n0.997\n0.999\n0.051\n0.149\n2.081\n0.077\nIEBins [41]\n0.936\n0.992\n0.998\n0.087\n0.314\n0.038\n0.978\n0.998\n0.999\n0.050\n0.142\n2.011\n0.075\nMIM [49]\n0.949\n0.994\n0.999\n0.083\n0.287\n0.035\n0.977\n0.998\n1.000\n0.050\n0.139\n1.966\n0.075\nDDVM [40]\n0.946\n0.987\n0.996\n0.074\n0.315\n0.032\n0.965\n0.994\n0.998\n0.055\n0.292\n2.613\n0.089\nJoint indoor-outdoor models:\nZoeD-M12-NK\n0.953\n0.995\n0.999\n0.077\n0.277\n0.033\n0.966\n0.993\n0.996\n0.057\n0.204\n2.362\n0.087\nDMD-NK\n0.944\n0.986\n0.996\n0.076\n0.313\n0.033\n0.964\n0.994\n0.999\n0.055\n0.219\n2.527\n0.087\nDMD-MIX\n0.953\n0.989\n0.996\n0.072\n0.296\n0.031\n0.967\n0.995\n0.999\n0.053\n0.203\n2.411\n0.084\nTable 4. In-domain results showing better relative error than ZoeDepth for our models on both the NYU and KITTI datasets. Best results\n(amongst indoor-outdoor models only) are bolded. For reference, we provide results for models trained separately for the indoor and\noutdoor domains showing that our results are competitive despite being a more general model.\nIndoors\nOutdoors\nMetric Experiment\nNYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD\nREL\nNo FOV cond\n0.081\n0.116\n0.316\n0.18\n0.400\n0.057\n1.257\n0.613\n0.100\n0.121\nWith FOV cond\n0.076\n0.109\n0.298\n0.130\n0.382\n0.055\n0.300\n0.628\n0.093\n0.122\nRMS\nNo FOV cond\n0.319\n0.325\n1.474\n0.712\n5.196\n2.574\n5.382\n8.582\n5.021\n6.826\nWith FOV cond\n0.313\n0.306\n1.407\n0.563\n5.527\n2.527\n2.522\n9.577\n4.828\n6.740\nTable 5. Depth errors for models trained with and without field-of-view conditioning. Results shows that FOV conditioning provides a\nsubstantial boost in performance. DIML Outdoor benefits the most, which is understandable given its large FOV, for which generalization\nis a major challenge for simple FOV augmentation.\nDIODE Outdoor [46] for outdoors. We closely follow the\nevaluation protocol, including depth ranges and cropping,\nused in [5] and report results using the standard error and\naccuracy metrics that are used in literature.\n4.3. Results\nZero-shot. Tables 1 and 2 report zero-shot performance\non eight OOD datasets. DMD-NK permits fair compari-\nson with ZoeDepth, as both are fine-tuned on NYU and\nKITTI. With these datasets, DMD outperforms ZoeDepth\nacross all but DIODE Outdoor, where DMD-NK outper-\nforms ZoeDepth on relative error but not RMSE and \u03b41.\nImportantly, but perhaps not surprisingly, further perfor-\nmance improvements are obtained by fine-tuning on larger\namounts of data.\nOur variant trained on a mixture with\nKITTI, NYU, nuScenes, and Taskonomy, dubbed DMD-\nMIX, generalizes much better on OOD test data, establish-\ning an even stronger new state of the art.\nFigures 2 and 3 visualize qualitative comparison be-\ntween DMD and ZoeDepth on indoor and outdoor datasets\nrespectively. Our method captures more accurate metric-\nscale depth with both indoor and outdoor scenes. Fig. 5\nillustrates the qualitative differences in depth maps from\nDMD-NK and DMD-MIX. By virtue of training on a larger\ndataset, DMD-MIX significantly improves the depth scale\nand fine-grained depth details near object boundaries.\nIn-distribution.\nTable 4 reports results on the KITTI\nand NYU datasets.\nOn KITTI, DMD-MIX outperforms\nZoeDepth on all metrics except RMSE where it is com-\npetitive.\nOn NYU DMD-MIX substantially outperforms\nZoeDepth on relative error and is competitive on other met-\nrics. Interestingly, DMD-MIX outperforms DMD-NK on\nin-distribution data.\n4.4. Ablations\nWe next consider several ablations to test different compo-\nnents of the model, all with DMD-NK for expedience.\nLog vs linearly scaled depth. Table 3 shows that parame-\nterizing depth in log scale (Sec. 3.2) improves quantitative\nperformance. As expected, this is beneficial for datasets of\nindoor scenes and also for datasets of outdoor scenes with\nshallower depths, like DIML Outdoor and DIODE Outdoor.\nFurthermore, as shown in Fig. 4, using linear-scaling leads\nto noise artifacts in the depth estimates for indoor scenes\n7\nIndoors\nOutdoors\nMetric # samples NYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD\nREL\n1\n0.077\n0.110\n0.304\n0.135\n0.383\n0.055\n0.311\n0.634\n0.093\n0.122\n8\n0.076\n0.109\n0.298\n0.130\n0.382\n0.055\n0.300\n0.628\n0.093\n0.122\nRMS\n1\n0.321\n0.311\n1.426\n0.579\n5.521\n2.544\n2.572\n9.602\n4.842\n6.755\n8\n0.313\n0.306\n1.407\n0.563\n5.527\n2.527\n2.522\n9.577\n4.828\n6.740\nTable 6. Averaging multiple samples leads to small but consistent improvement on both REL and RMS.\nIndoors\nOutdoors\nMetric Experiment\nNYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD\nREL\nNo FOV aug or cond\n0.074\n0.124\n0.337\n0.180\n0.479\n0.055\n1.399\n0.615\n0.095\n0.116\nW/ FOV aug and cond 0.076\n0.109\n0.298\n0.130\n0.382\n0.055\n0.300\n0.628\n0.093\n0.122\nRMS\nNo FOV aug or cond\n0.310\n0.348\n1.535\n0.722\n5.247\n2.597\n5.919\n8.529\n4.874\n6.476\nW/ FOV aug and cond 0.313\n0.306\n1.407\n0.563\n5.527\n2.527\n2.522\n9.577\n4.828\n6.740\nTable 7. Ablation showing that training without FOV augmentations and conditioning hurts generalization to out-of-domain data due to\noverfitting on the training data intrinsics.\nREL \u2193\nRMSE \u2193\nNYU KITTI\nNYU KITTI\nZoeDepth Auto Router 0.102\n0.075\n0.377\n2.584\nOurs no fov aug / cond\n0.074\n0.055\n0.310\n2.597\nTable 8. Comparison against ZoeDepth without scene type su-\npervision. ZoeDepth performance degrades significantly when the\nscene type (indoor or outdoor) is not provided. DMD learns well\nwithout such supervision.\nwhich is resolved with log-depth parameterization.\nField-of-view conditioning. Table 5 shows that FOV con-\nditioning achieves the best performance. Fig. 6 perturbs the\nconditioning FOV signal during inference, showing that op-\ntimal performance occurs at or close to the true FOV.\nNo FOV augmentation or conditioning. ZoeDepth found\nthat without scene-type supervision for the experts (i.e.,\nAuto Router), ZoeDepth\u2019s performance degrades, even for\nin-domain data. To compare against ZoeDepth in this set-\nting, we fine-tune a model on NYU and KITTI without FOV\naugmentations or conditioning.\nInterestingly, DMD per-\nforms relatively well in this setting for in-domain data (Ta-\nble 8). Nevertheless, as shown in Table 7, OOD perfor-\nmance is better with FOV augmentation and conditioning.\n\u03f5 vs v diffusion parameterization. Inference latency is\na concern with diffusion models for vision. DDVM [40],\nfor example, uses 128 denoising steps for depth estima-\ntion which can be prohibitive. We find that using the v-\nparameterization dramatically reduces the number of de-\nnoising steps required for good performance. As shown in\nTable 9, \u03f5-parameterization requires 64 denoising steps to\nmatch the performance of a model with v-parameterization\nusing only 1 denoising step. Intuitively, v-parameterization\nNYU\nKITTI\nNum denoising steps\n\u03f5\nv\n\u03f5\nv\n1\n2.374 0.077 0.596 0.056\n4\n1.484 0.075 0.406 0.055\n16\n0.409 0.074 0.141 0.055\n64\n0.077 0.074 0.056 0.055\nTable 9. Comparison of relative error on NYU and KITTI for mod-\nels trained with \u03f5- and v- parameterization. Both are fine-tuned on\nNYU and KITTI without FOV augmentation or conditioning.\nensures that the model accurately recovers the signal at both\nends of the noise schedule, unlike \u03f5-parameterization where\nestimating the noise is easy for low SNR inputs.\n5. Conclusion\nWe propose a generic diffusion-based monocular metric\ndepth generator with no task-specific inductive biases and\nno specialized architectures for handling diverse indoor and\noutdoor scenes. Our log-scale depth parameterization ade-\nquately allocates representation capacity to different depth\nranges. We advocate augmenting the FOV of training data\nthrough simple cropping/uncropping to enable generaliza-\ntion to fields-of-view beyond those in the training datasets\nand show that simply uncropping with noise padding is ef-\nfective for simulating a larger FOV. We find that condition-\ning on the FOV is essential for disambiguating depth-scale.\nWe further propose a new fine-tuning dataset mixture that\ndramatically improves performance. With these innovations\ncombined, we establish a new state-of-the-art outperform-\ning the prior work of ZoeDepth across all zero-shot and in-\ndomain datasets by a substantial margin.\n8\nAcknowledgements\nWe thank Jon Shlens, Kevin Swersky, Shekoofeh Azizi and\nCristina Vasconcelos for their detailed feedback and help\nwith figures. We also thank Ting Chen, Daniel Watson,\nForrester Cole and others in Google DeepMind and Google\nResearch for helpful discussions.\nReferences\n[1] Ashutosh Agarwal and Chetan Arora. Attention Attention\nEverywhere: Monocular depth prediction with skip atten-\ntion. In WACV, 2023. 2, 7\n[2] Manuel L\u00b4opez Antequera, Pau Gargallo, Markus Hofinger,\nSamuel Rota Bul`o, Yubin Kuang, and Peter Kontschieder.\nMapillary planet-scale depth dataset. In ECCV, pages 589\u2013\n604, 2020. 2\n[3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\nAdaBins: Depth estimation using adaptive bins. In CVPR,\npages 4009\u20134018, 2021. 2, 4, 5, 7\n[4] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\nLocalBins: Improving depth estimation by learning local dis-\ntributions. In ECCV, pages 480\u2013496, 2022. 4, 5\n[5] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter\nWonka, and Matthias M\u00a8uller. ZoeDepth: Zero-shot trans-\nfer by combining relative and metric depth. arXiv preprint\narXiv:2302.12288, 2023. 1, 2, 3, 4, 5, 6, 7, 11, 12\n[6] Yohann Cabon, Naila Murray, and Martin Humenberger. Vir-\ntual KITTI 2, 2020. 6\n[7] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuScenes: A multi-\nmodal dataset for autonomous driving. In CVPR, 2020. 3\n[8] Yuanzhouhan Cao, Zifeng Wu, and Chunhua Shen.\nEsti-\nmating depth from monocular images as classification using\ndeep fully convolutional residual networks. IEEE T-CSVT,\n28(11):3174\u20133182, 2017. 2\n[9] Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton, and\nDavid J. Fleet. A generalist framework for panoptic segmen-\ntation of images and videos. In ICCV, 2023. 2\n[10] Jaehoon\nCho,\nDongbo\nMin,\nYoungjung\nKim,\nand\nKwanghoon Sohn.\nDIML/CVL RGB-D dataset:\n2M\nRGB-D images of natural indoor and outdoor scenes. arXiv\npreprint arXiv:2110.11590, 2021. 3, 6\n[11] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet:\nRichly-annotated 3D reconstructions of indoor scenes.\nIn\nCVPR, 2017. 3\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248\u2013255, 2009. 3\n[13] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\nGANs on image synthesis. In NeurIPS, 2022. 2\n[14] Yiqun Duan, Xianda Guo, and Zheng Zhu. DiffusionDepth:\nDiffusion denoising approach for monocular depth estima-\ntion. arXiv preprint arXiv:2303.05021, 2023. 2\n[15] David Eigen and Rob Fergus. Predicting depth, surface nor-\nmals and semantic labels with a common multi-scale convo-\nlutional architecture. In ICCV, pages 2650\u20132658, 2015. 2\n[16] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. In NIPS, 2014. 2\n[17] Jose M. Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis\nMontesano, Thomas Brox, and Javier Civera. CAM-Convs:\nCamera-Aware Multi-Scale Convolutions for Single-View\nDepth. In CVPR, 2019. 2\n[18] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\nmanghelich, and Dacheng Tao. Deep ordinal regression net-\nwork for monocular depth estimation. In CVPR, pages 2002\u2013\n2011, 2018. 2\n[19] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets Robotics: The KITTI dataset. The\nInternational Journal of Robotics Research, 32(11):1231\u2013\n1237, 2013. 3, 6\n[20] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven-\ntos, and Adrien Gaidon.\n3D packing for self-supervised\nmonocular depth estimation. In CVPR, 2020. 6\n[21] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus,,\nand Adrien Gaidon. Towards zero-shot scale-aware monoc-\nular depth estimation. In ICCV, pages 9233\u20139243, 2023. 2\n[22] Lei He, Guanghui Wang, and Zhanyi Hu. Learning depth\nfrom single images with deep neural network embedding fo-\ncal length. IEEE Transactions on Image Processing, 27(9):\n4676\u20134689, 2018. 2\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-\nfusion Probabilistic Models. NeurIPS, 2020. 2, 6\n[24] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. JMLR, 2022. 2\n[25] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Jonathan Eisen-\nmann, Matt Fisher, Emiliano Gambaretto, Sunil Hadap, and\nJean-Francois Lalonde. A perceptual measure for deep sin-\ngle image camera calibration. In CVPR. IEEE, 2018. 12\n[26] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu,\nZhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. DDP:\nDiffusion model for dense visual prediction. In ICCV, 2023.\n2\n[27] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver\nWang, Kevin Blackburn-Matzen, Matthew Sticha, and\nDavid F. Fouhey. Perspective fields for single image cam-\nera calibration. In CVPR, 2023. 12\n[28] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-\nerico Tombari, and Nassir Navab. Deeper depth prediction\nwith fully convolutional residual networks. In 3DV, pages\n239\u2013248, 2016. 2\n[29] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong\nSuh. From big to small: Multi-scale local planar guidance\nfor monocular depth estimation. arXiv:1907.10326, 2019. 4,\n5, 7\n[30] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.\nBinsFormer: Revisiting adaptive bins for monocular depth\nestimation. arxiv.2204.00987, 2022. 2, 7\n9\n[31] John McCormac, Ankur Handa, Stefan Leutenegger, and\nAndrew J. Davison. SceneNet RGB-D: Can 5M synthetic\nimages beat generic imagenet pre-training on indoor seg-\nmentation? In ICCV, 2017. 3\n[32] Alex Nichol and Prafulla Dhariwal. Improved denoising dif-\nfusion probabilistic models. In ICML, 2021. 2\n[33] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\nmoulin, and Aaron Courville. FiLM: Visual Reasoning with\na General Conditioning Layer . In AAAI, 2018. 6\n[34] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE TPAMI, 44(3):1623\u20131637, 2020. 1, 2\n[35] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction.\nIn ICCV, pages\n12179\u201312188, 2021. 2, 7\n[36] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit\nKumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,\nand Joshua M. Susskind. Hypersim: A photorealistic syn-\nthetic dataset for holistic indoor scene understanding.\nIn\nICCV, 2021. 6\n[37] Chitwan Saharia, William Chan, Huiwen Chang, Chris A.\nLee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mo-\nhammad Norouzi. Palette: Image-to-Image Diffusion Mod-\nels. In SIGGRAPH, 2022. 3, 11\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J.\nFleet, and Mohammad Norouzi.\nPhotorealistic Text-to-\nImage Diffusion Models with Deep Language Understand-\ning. In NeurIPS, 2022. 2, 6\n[39] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR, 2022. 2, 3\n[40] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek\nKar, Mohammad Norouzi, Deqing Sun, and David J. Fleet.\nThe surprising effectiveness of diffusion models for optical\nflow and monocular depth estimation. In NeurIPS, 2023. 2,\n3, 6, 7, 8\n[41] Shuwei Shao, Zhongcai Pei, Xingming Wu, Zhong Liu, Wei-\nhai Chen, and Zhengguo Li. IEBins: Iterative elastic bins for\nmonocular depth estimation. In NeurIPS, 2023. 7\n[42] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus.\nIndoor segmentation and support inference from\nRGBD images. In ECCV, pages 746\u2013760, 2012. 3, 6\n[43] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn ICML, pages 2256\u2013\n2265, 2015. 2\n[44] Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao.\nSun RGB-D: A RGB-D scene understanding benchmark\nsuite. In CVPR, pages 567\u2013576, 2015. 6\n[45] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han,\nJiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger,\nMaxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao,\nShuyang Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen,\nand Dragomir Anguelov. Scalability in perception for au-\ntonomous driving: Waymo open dataset. In CVPR, 2020.\n3\n[46] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,\nHaochen Wang, Falcon Z. Dai, Andrea F. Daniele, Moham-\nmadreza Mostajabi, Steven Basart, Matthew R. Walter, and\nGregory Shakhnarovich. DIODE: A Dense Indoor and Out-\ndoor DEpth Dataset. CoRR, abs/1908.00463, 2019. 6, 7\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NIPS, 2017. 6\n[48] Chengrui Wei, Meng Yang, Lei He, and Nanning Zheng. FS-\nDepth: Focal-and-scale depth estimation from a single image\nin unseen indoor scene. arXiv preprint arXiv:2307.14624,\n2023. 3\n[49] Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han\nHu, and Yue Cao. Revealing the dark secrets of masked im-\nage modeling. In CVPR, 2023. 7\n[50] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and\nPing Tan. Neural window fully-connected CRFs for monoc-\nular depth estimation. In CVPR, pages 3916\u20133925, 2022. 4,\n5, 7\n[51] Amir Zamir, Alexander Sax, William Shen, Leonidas\nGuibas, Jitendra Malik, and Silvio Savarese. Taskonomy:\nDisentangling task transfer learning. In CVPR, 2018. 3\n[52] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. IEEE transactions on pattern analysis\nand machine intelligence, 40(6):1452\u20131464, 2017. 3\n10\nImage\nGround Truth\nZoeDepth [5]\nDMD (ours)\nDIODE\nIndoor\nHypersim\nibims-1\nNYU\nSunRGBD\nFigure 7. Qualitative comparison between our method and ZoeDepth [5] on indoor scenes. Compared with ZoeDepth, our method estimates\ndepths at more accurate scale over diverse datasets.\nA. Additional qualitative examples\nIn Fig. 7 and Fig. 8, we provide additional qualitative com-\nparison between DMD and ZoeDepth on indoor and out-\ndoor datasets respectively. Our method consistently cap-\ntures more accurate metric-scale depth in various indoor and\noutdoor scenes.\nB. Handling unknown field-of-view\nWhile RGB camera intrinsics are available for most prac-\ntical uses of monocular depth estimators (e.g. cell phones,\nrobot platforms or self-driving cars), they may sometimes\nbe unknown (e.g. internet images or generative imagery).\nOne solution to handle the unknown FOV would be to esti-\nmate the camera intrinsics from the RGB image.\nTo test this, we train a simple model for predicting the\nvertical field-of-view (FOV). The model consists of the en-\ncoder of a pre-trained Palette [37] model followed by a spa-\ntial average pooling layer and a linear head that predicts a\nscalar. It is trained to regress to tan(\u03b8/2), where \u03b8 is the\nvertical FOV, using a L1 loss. We train on a mix of the NYU\nand KITTI datasets employing the same FOV augmentation\nstrategy as used for DMD-NK.\nTable 10 compares the depth estimation performance of\nour models when using the estimated FOV versus the true\nFOV. Despite the simplistic design of our FOV estimator,\nthe depth estimation performance when using the estimated\nFOV is competitive to that when using the true FOV, except\nfor the DIML outdoor dataset, where the large error in FOV\nestimation leads to a much worse relative error for depth.\n11\nImage\nGround Truth\nZoeDepth [5]\nDMD (ours)\nDDAD\nDIML\nOutdoor\nDIODE\nOutdoor\nKITTI\nVirtual\nKITTI 2\nFigure 8. Qualitative comparison between DMD and ZoeDepth [5] on outdoor scenes. Compared with ZoeDepth [5], our method is able\nto estimate a more accurate depth scale.\nIndoors\nOutdoors\nExperiment\nNYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD\nDMD-NK true FOV\n0.076\n0.109\n0.298\n0.130\n0.382\n0.055\n0.300\n0.628\n0.093\n0.122\nDMD-NK est. FOV\n0.077\n0.124\n0.302\n0.140\n0.407\n0.062\n1.490\n0.639\n0.114\n0.132\nDMD-MIX true FOV\n0.072\n0.091\n0.291\n0.118\n0.318\n0.053\n0.190\n0.553\n0.092\n0.108\nDMD-MIX est. FOV\n0.072\n0.136\n0.291\n0.114\n0.361\n0.061\n1.049\n0.560\n0.131\n0.119\nFOV est. error (degrees) 1.520\n5.114\n0.735\n2.339\n4.224\n0.842\n27.998\n3.459\n4.142\n4.384\nTable 10. Comparison of relative error (REL) using an estimated (est.) FOV versus the true FOV. Even with a simple FOV estimator the\ndepth estimation results using an estimated FOV are competitive to those with the true FOV, except for DIML Outdoor where our FOV\nestimator struggles to predict correct FOV as shown by the large error in FOV estimation.\nThis might be due to the significantly larger FOV of DIML\noutdoor, compared to other outdoor datasets, which could\nbe challenging for our simple FOV regressor to generalize\nto. We hypothesize that incorporating more sophisticated\ncamera intrinsic prediction models, such as those proposed\nby [25, 27], could lead to further improvements in FOV esti-\nmation accuracy, and thereby better metric depth estimates.\nHowever, we defer a thorough investigation of this approach\nto future work.\nC. Additional training details\nWe perform the first stage of supervised training for 1.5M\nsteps with a learning rate of 1 \u00d7 10\u22124 and the second (final)\nstage for 50k steps with a learning rate of 3 \u00d7 10\u22125. For the\nFOV augmentation in the second stage we randomly uni-\nformly sample a scale in [0.8, 1.5] to crop/uncrop the image\nand depth maps.\n12\n"
  },
  {
    "title": "InstructVideo: Instructing Video Diffusion Models with Human Feedback",
    "link": "https://arxiv.org/pdf/2312.12490.pdf",
    "upvote": "14",
    "text": "InstructVideo: Instructing Video Diffusion Models with Human Feedback\nHangjie Yuan1* Shiwei Zhang2\nXiang Wang2\nYujie Wei2\nTao Feng3\nYining Pan4\nYingya Zhang2\nZiwei Liu5\nSamuel Albanie6\nDong Ni1\u2020\n1Zhejiang University\n2Alibaba Group\n3Tsinghua University\n4Singapore University of Technology and Design\n5S-Lab, Nanyang Technological University\n6CAML Lab, University of Cambridge\nProject page:\nhttps://instructvideo.github.io/\nAbstract\nDiffusion models have emerged as the de facto paradigm\nfor video generation. However, their reliance on web-scale\ndata of varied quality often yields results that are visually\nunappealing and misaligned with the textual prompts. To\ntackle this problem, we propose InstructVideo to in-\nstruct text-to-video diffusion models with human feedback\nby reward fine-tuning. InstructVideo has two key in-\ngredients: 1) To ameliorate the cost of reward fine-tuning\ninduced by generating through the full DDIM sampling\nchain, we recast reward fine-tuning as editing. By lever-\naging the diffusion process to corrupt a sampled video,\nInstructVideo requires only partial inference of the\nDDIM sampling chain, reducing fine-tuning cost while im-\nproving fine-tuning efficiency. 2) To mitigate the absence of\na dedicated video reward model for human preferences, we\nrepurpose established image reward models, e.g., HPSv2.\nTo this end, we propose Segmental Video Reward, a mech-\nanism to provide reward signals based on segmental sparse\nsampling, and Temporally Attenuated Reward, a method\nthat mitigates temporal modeling degradation during fine-\ntuning. Extensive experiments, both qualitative and quanti-\ntative, validate the practicality and efficacy of using image\nreward models in InstructVideo, significantly enhanc-\ning the visual quality of generated videos without compro-\nmising generalization capabilities. Code and models will\nbe made publicly available.\n1. Introduction\nThe emergence of diffusion models [33, 69, 71, 99] has sig-\nnificantly boosted generation quality across a wide range of\nmedia content [35, 41, 63, 78]. This generation paradigm\nhas shown promise for video generation [5, 34, 35, 78, 81],\n*Work conducted during his research internships at DAMO Academy.\n\u2020Corresponding author.\nVersion 1\nReward \nModel\n\u2746\nGradi\nLoR\nPre-tr\nUNe\nFroz\nInstructVideo\nCaption\nOriginal\nGeneration\n\u2746\n\u2746\nGradient\nLoRA\nPre-trained\nUNet\n\u2746\nFrozen\nImage \nReward \nModel\nFine-tuning\nCaption\nNoised \nVideo\n\u2746\n\u2746\nCaption\nGeneration\n\u2746\n\u2746\n\u2746\n\u2746\nTrainable\nFigure 1. Overview of the InstructVideo framework. Our\nmethod performs efficient fine-tuning on sampled video-text pairs,\ninstructed by human preferences in image reward models.\ndespite the challenges of working with high-dimensional\ndata.\nWhile diffusion models are one factor driving\nprogress, the scaling of training datasets has also played\na key role [67, 83]. However, despite recent progress, the\nvisual quality of generated videos still leaves room for im-\nprovement [78, 87]. A significant contributing factor to this\nissue is the varying quality of web-scale data employed dur-\ning pre-training [3, 66], which can yield models capable of\ngenerating content that is visually unappealing, toxic and\nmisaligned with the prompt.\nWhile aligning model outputs with human preferences\nhas proven highly effective for control [13], text genera-\ntion [2, 44, 56, 57, 74] and image generation [43, 89, 93],\nit remains a notion unexplored in video diffusion models.\nThe most widely-adopted methods for aligning models with\nhuman preferences include off-line reinforcement learning\n(RL) [4, 45, 57] and direct reward back-propagation [16,\n59]. Typically, this entails training a reward model on man-\nually annotated datasets that is then subsequently used to\nfine-tune the pre-trained generative model.\nTwo major challenges arise when seeking to align video\narXiv:2312.12490v1  [cs.CV]  19 Dec 2023\ngeneration models with human preferences: 1) The opti-\nmization process for optimizing human preferences is com-\nputationally demanding, often requiring video generation\nfrom textual inputs. While video generation pre-training us-\ning DDPM [33] requires only a single-step inference for ev-\nery iteration, reward optimization requires a 50-step DDIM\ninference [70]. 2) The curation of a large annotated dataset\nto capture human preferences of videos is labor-intensive,\nwhile the computation- and memory-intensive demands of\nutilizing ViT-H [88] or ViT-L [93]-based computational al-\nternatives to evaluate the entire video are high.\nTo surmount these mentioned challenges, we propose\nInstructVideo, a model that efficiently instructs text-\nto-video diffusion models to follow human feedback, as il-\nlustrated in Fig. 1. Regarding the first challenge of the de-\nmanding reward fine-tuning process caused by generating\nthrough the full DDIM sampling chain, we recast the prob-\nlem of reward fine-tuning as an editing procedure. This\nreformulation requires only partial inference of the DDIM\nsampling chain, thereby reducing computational demands\nwhile improving fine-tuning efficiency. Drawing inspira-\ntion from established editing workflows in diffusion mod-\nels [6, 7, 17, 27, 50, 53, 55, 60, 102], where primary vi-\nsual content is initially corrupted with noise and then re-\nshaped by a target prompt, our method focuses on refining\ncoarse and structural videos into more detailed and nuanced\noutputs. This contrasts with previous methods [4, 16, 43]\nthat generate results directly from text. Such blurry and\nstructural videos, serving as the starting point for reward\nfine-tuning, are procured by a simple diffusion process with\nnegligible cost. During generation, the optimized model re-\ntains the capability to produce videos directly from textual\ninputs. In conjunction with back-propagation truncation of\nthe sampling chain, we make reward fine-tuning on text-to-\nvideo diffusion models computationally attainable and ef-\nfective.\nRegarding the second challenge (the lack of a reward\nmodel tailored for video generation), we postulate that the\nvisual excellence of a video is tied to both the quality of\nits individual frames and the fluidity of motion across con-\nsecutive frames. To this end, we resort to off-the-shelf im-\nage reward models, e.g., HPSv2 [88], to ascertain frame\nquality. Drawing inspiration from temporal segment net-\nworks [79], we propose Segmental Video Reward (SegVR),\nwhich strategically evaluates video quality based on a sub-\nset of sparely sampled frames. By providing sparse reward\nsignals, SegVR offers dual benefits: it not only ameliorates\ncomputational burden but also mitigates temporal model-\ning collapse. On the other hand, although LoRA [37] is\nadopted by default to retain the capability to generate tem-\nporally smooth videos, SegVR still leads to videos with vi-\nsual artifacts, such as structure twitching and color jitter-\ning. To mitigate this, we propose Temporally Attenuated\nReward (TAR), which operates under the hypothesis that\ncentral frames should be assigned paramount importance,\nwith emphasis tapering off towards peripheral frames. This\nstrategic allocation of importance across frames ensures a\nmore stable and visually coherent video generation process.\nAs part of our pioneering effort to align video diffusion\nmodels with human preferences, we conduct extensive ex-\nperiments to assess the practicality and efficacy of integrat-\ning image reward models within InstructVideo. Our\nfindings reveal that InstructVideo markedly enhances\nthe visual quality of generated videos without sacrificing the\nmodel\u2019s generalization capabilities, setting a new precedent\nfor future research in video generation.\n2. Related Work\nVideo generation via diffusion models. Early efforts at\nvideo generation focused on GANs [25, 36, 48, 58, 68,\n76, 86, 96] and VAEs [48, 54, 94]. However, due to the\ncomplexity of jointly modeling spatio-temporal dynamics,\ngenerating videos from texts remains an unresolved chal-\nlenge. Recent methods for video generation aim to miti-\ngate this by utilizing the de facto generation method, i.e.,\ndiffusion models [33, 69, 70, 99], for generating videos\nwith diversity and fidelity [1, 5, 8\u201310, 12, 20, 26, 29\u2013\n31, 34, 35, 38, 49, 51, 52, 61, 65, 73, 80, 82\u201384, 90\u2013\n92, 95, 97, 98, 100, 103\u2013105] and scaling up the pre-\ntraining data or model architecture [34, 36, 67, 83, 85].\nVDM [35] represents a pioneering work that extended im-\nage diffusion models to video generation. Owing to the\ncomputation-intensive nature of diffusion models, follow-\nup research sought to reduce overhead by leveraging the la-\ntent space [63], e.g., ModelScopeT2V [78], Video LDM [5],\nMagicVideo [104] and SimDA [92], etc.. To enable more\ncontrollable generation, further efforts introduce spatio-\ntemporal conditions [11, 20, 42, 81, 90, 95], e.g., Video-\nComposer [81], Gen-1 [20], DragNUWA [95], etc.. How-\never, generating videos that adhere to human preferences\nremains a challenge.\nHuman preference model. Understanding human prefer-\nence in visual content generation remains challenging [40,\n43, 46, 88, 89, 93]. Some pioneering works target solv-\ning this problem by annotating a dataset with human\npreferences, e.g., Pick-a-pic [40], ImageReward [93] and\nHPD [88, 89]. Language-image models, e.g. CLIP [62]\nand BLIP [47], are then fine-tuned on the resulting anno-\ntated data. As such, the fine-tuned models represent a data-\ndriven approach to modelling human preferences. However,\nthe annotation process for capturing human preferences is\nhighly labor- and cost-intensive. Thus, in this paper, we\nadopt off-the-shelf image preference models to improve the\nquality of generated videos.\nLearning from human feedback.\nLearning from hu-\nman feedback was first studied in the context of reinforce-\n1\nFine-tuning phase\nDiffusion \nDDIM Sampling\n\u00d7(\ud835\udf49 ' \ud835\udc6b)\nVideo-text pair\nDogwood blossoms are \nblowing in the wind.\nv\nReward\nmodel\nv\nTAR\n\ud835\udc45\n\ud835\udc67!\n\ud835\udc67#(( ) *)\n\ud835\udc67\nBack-propagation\n\u2212\ud835\udc45\n\ud835\udc67#(%)\nSegmental sampling and decoding\n\ud835\udc65!\n'\n\ud835\udc50\n\ud835\udc67\"$%&\nFigure 2. The reward fine-tuning framework of InstructVideo. During fine-tuning, we sample video-text pairs and apply a diffusion\nprocess to corrupt the videos to a noise level \u03c4. Subsequently, we perform partial inference of the DDIM sampling chain to obtain the\nhuman preference edited videos. By utilizing SegVR and TAR, we can leverage image reward models to perform reward fine-tuning for\nvideo generation. The VQGAN encoder and decoder are omitted for clarity. In this example, the blurry video z is edited guided by human\npreferences, producing a result that highlights the vibrancy and structure of the dogwood blossoms.\nment learning and agent alignment [14, 44] and later in\nlarge language models [57, 72], enabling them to gener-\nate helpful, honest and harmless textual outputs. This goal\nof learning from human feedback is also desirable in vi-\nsual content generation.\nIn image generation and edit-\ning, a key objective is to align generated images with\nthe prompt [4, 16, 22, 59, 77, 101], preventing surprising\nand toxic results [43, 89, 93]. Lee et al. utilizes reward-\nweighted regression on a manually collected dataset, aim-\ning to mitigate misalignment with respect to factors such as\ncount, color and background. DDPO [4] and DPOK [22]\npropose to use policy gradients on a multi-step diffu-\nsion model [21], demonstrating improved rewards of aes-\nthetic quality, image-text alignment, compressibility, etc..\nDRaFT [16] and AlignProp [59] achieve feedback opti-\nmization by back-propagating the gradients of a differen-\ntiable reward function through the sampling procedure via\ngradient checkpointing [28]. However, learning from hu-\nman feedback for video diffusion models remains under-\nexplored owing to its prohibitive cost. InstructVideo\naims to fill this gap, providing a solution for more efficient\nreward fine-tuning.\n3. Methodology\nIn this section, we commence with preliminaries. Next, we\ndelve into the details of InstructVideo, which encom-\npasses: 1) A reformulation of reward fine-tuning as editing\nthat ensures computational efficiency and efficacy. 2) Seg-\nmental Video Reward (SegVR) and Temporally Attenuated\nReward (TAR) that enable efficient reward fine-tuning with\nimage reward models.\n3.1. Preliminaries\nText-to-video diffusion models. Text-to-video diffusion\nmodels aim to map textual input into a distribution repre-\nsenting video data via a reverse diffusion process [33, 69].\nThese models typically operate in a latent space to handle\ncomplex video data [63, 78, 81].\nDuring pre-training, a\nsampled video x is processed by a fixed encoder [19] to\nderive its latent representation z \u2208 RF \u00d7h\u00d7w\u00d73, where the\nvideo\u2019s spatial dimensions are compressed by a factor of\n8. Next, random noise is injected into the sampled video\nby the forward diffusion process according to a predeter-\nmined schedule {\u03b2t}T\nt=1. This process can be described as\nzt = \u221a\u00af\u03b1tz + \u221a1 \u2212 \u00af\u03b1t\u03f5, where \u03f5 \u2208 N(0, 1) is random\nnoise with identical dimensions to z, \u00af\u03b1t = Qt\ns=1 \u03b1s and\n\u03b1t = 1 \u2212 \u03b2t. A UNet [15, 64] \u03f5\u03b8 is adopted to perform\ndenoising, enabling the generation of videos through a re-\nverse diffusion process, conditioned on the video caption c.\nOptimisation employs the following reweighted variational\nbound [33]:\nL(\u03b8) = Ez,\u03f5,c,t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(\u221a\u00af\u03b1tz +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, c, t)\u22252\n2\n\u0003\n(1)\nDuring inference, we adopt the DDIM sampling [70]\nmethod for realistic video generation.\nReward fine-tuning. Reward fine-tuning aims to optimize\na pre-trained model to enhance the expected rewards of a\nreward function r(\u00b7, \u00b7). In our case, we target optimizing the\nparameters \u03b8 of a text-to-video diffusion model to enhance\nthe expected reward of the generated videos given the text\ndistribution:\nLr(\u03b8) = EP(c)EP\u03b8(x0|c)[\u2212r(x0, c)]\n(2)\nwhere x0 is the video generated from the sampled text c\nvia the diffusion model through the DDIM sampling chain.\nThe reward function r(\u00b7, \u00b7) is typically a pre-trained model\nto assess the quality of the model output.\n3.2. InstructVideo\nIn Fig. 2, we illustrate InstructVideo\u2019s fine-tuning\npipeline and elaborate on the technical contributions below.\n3.2.1\nReward Fine-tuning as Editing\nReward fine-tuning with diffusion models is costly due to\nthe iterative refinement process during generation using\nDDIM [70].\nDuring generation, initial steps are crucial\nfor shaping coarse, structural aspects of videos, with subse-\nquent steps refining the coarse videos. Understanding that\nthe essence of reward fine-tuning is not to drastically alter\nthe model\u2019s output but to subtly adjust it in line with human\npreferences, we propose to reinterpret reward fine-tuning as\na form of editing [17, 53, 87]. This perspective shift allows\nus to perform partial inference of the DDIM sampling chain,\nreducing computational demands and easing optimization.\nTo implement this idea, we first curate a small amount\nof fine-tuning data from pre-training data for reward fine-\ntuning.\nFor each video-text pair (x, c), we acquire the\nvideo\u2019s latent embedding z as stated in Sec. 3.1. We aim\nto smooth out the video to eliminate undesirable artifacts\nand distortions [53]. To achieve this, we leverage the diffu-\nsion process rather than DDIM inversion [17, 53] to enable\nefficient editing. If we denote the number of DDIM steps\nas D and the number of pre-training DDPM steps as T, we\ndefine a mapping d : {1, . . . , D} \u2192 {1, . . . , T} that maps\nDDIM step index to the DDPM step index1, formulated as:\nd(i) = T\nD \u00b7 (i \u2212 1) + 1\n(3)\nGiven the noise level \u03c4, the targeted diffusion step tnoi for\ninjecting noise is formulated as:\ntnoi = d(\u03c4 \u00b7 D)\n(4)\nThis allows us to obtain the starting point for reward fine-\ntuning via the diffusion process:\nztnoi =\np\n\u00af\u03b1tnoiz +\np\n1 \u2212 \u00af\u03b1tnoi\u03f5\n(5)\nBased on ztnoi, we can perform \u03c4 \u00d7 D steps of DDIM\nsampling [70] along the DDIM sub-sequence to obtain the\nedited result z0, which consumes \u03c4 of the computation of\nthe full sampling chain. Utilizing the decoder [19], we de-\ncode z0 in the latent space to x0 in the RGB space.\n3.2.2\nReward Fine-tuning with Image Reward Models\nSince curating large datasets to capture human preferences\nfor training video reward models is prohibitively expen-\nsive, we resort to off-the-shelf image reward models r(\u00b7, \u00b7),\ne.g., HPSv2 [88]. HPSv2 is trained on 430k pairs of im-\nages, which are annotated by humans for text-image align-\nment and image quality. Given that videos are natural ex-\ntensions of images, we posit that these human preferences\nare also applicable to videos. However, initial experiments\nwith applying dense reward fine-tuning produced degraded\n1For example, if D = 20 and T = 1000, then the DDIM step sub-\nsequence is {1, 51, . . . , 901, 951}, i.e., d(2) = 51.\nmotion continuity. Taking inspiration from temporal seg-\nment networks [79], given a video x0 \u2208 RF \u00d7H\u00d7W \u00d73\ngenerated from its caption c, we evenly divide it into\nS segments.\nWithin each segment, we perform random\nframe sampling, obtaining a sparse set capturing the essence\nof the video xg\n0 = {xg(1)\n0\n, . . . , xg(S)\n0\n}.\nHere, g(i) =\nUniform\n\u0000(i \u2212 1) \u00b7 F\nS , i \u00b7 F\nS \u2212 1\n\u0001\ndenotes a uniform sam-\npling of index within ith segment. Utilizing r(\u00b7, \u00b7), we com-\npute the reward score R with respect to x0 as follows:\nR = Aggi[r(xg(i)\n0\n, c)],\ni = 1, . . . , S\n(6)\nwhere Aggi denotes the aggregation function along index\ni. To consider the impact of all frames in xg\n0, an intuitive\nimplementation of Aggi is the mean function.\nHowever, the simple aggregation function leads to no-\nticeable visual artifacts in the generated videos, such as\nstructural twitching and color jittering. This issue arises be-\ncause the mean function places equal weight on all frames,\ndisregarding the inherent dynamic nature of videos where\nthe reward scores of frames can vary throughout the se-\nquence. To address this, we introduce TAR that strategically\nemphasizes central frames, with the emphasis tapering off\ntowards the peripheral frames, thereby avoiding uniformly\noptimizing all frames\u2019 reward scores to be equally high. We\ndefine the temporally attenuated coefficient as:\nfi = e\u2212\u03bbtar|g(i)\u2212 F\n2 |\n(7)\nwhere \u03bbtar controls the degree of the attenuating rate. We\nset \u03bbtar = 1 by default. Incorporating this coefficient, we\nrewrite the reward score R:\nR = 1\nS\nXS\ni=1 fi \u00b7 r(xg(i)\n0\n, c)\n(8)\nThe optimization objective in Eq. (2) can be rewritten as:\nLr(\u03b8) = EP(c)EP\u03b8(x0|c)[\u2212R]\n(9)\n3.3. Reward Fine-tuning and Inference\nData preparation and evaluation metric.\nWe follow\nDDPO [4] to experiment on prompts describing 45 animal\nspecies. In contrast to DDPO, since InstructVideo re-\nlies on video-text data, we select video-text pairs as the fine-\ntuning data from the base model\u2019s pre-training dataset, i.e.,\nWebVid10M, ensuring that no extra data is introduced. It\nis worth noting that we do not apply any quality filtering\nmethod to ensure that the selected videos are of high qual-\nity. Specifically, we select about 20 video-text pairs for each\nanimal species. To evaluate the model\u2019s ability to optimize\nthe reward scores, we also collect evaluation data compris-\ning about 6 prompts for each animal. We use HPSv2 score\nto measure the optimization performance of reward fine-\ntuning on the first frames of all segments.\nModelScopeT2V (D=20)\nInstructVideo (D=20)\nModelScopeT2V (D=50)\nCat walking \nto the camera.\nMountain goat\ngrazing on a cliff\nby the sea.\nBee collecting\nsunflower pollen\ncloseup footage.\nParrot fish in sea\nunderwater eating\nstone coral.\nSmall bird during\nrain in india.\nFigure 3. Comparion of InstructVideo with the base model ModelScopeT2V. ModelScopeT2V utilizes 20 and 50 DDIM steps.\nReward fine-tuning. We adopt the publicly available text-\nto-video diffusion model ModelScopeT2V [78] as our base\nmodel. ModelScopeT2V is trained on WebVid10M [3] with\nT = 1000 and is able to generate videos of 16 \u00d7 256 \u00d7 256\nresolution, which we divide into S = 4 segments. By de-\nfault, we adopt the differentiable HPSv2 [88] as the reward\nmodel and perform 20-step DDIM inference, i.e., D = 20.\nClassifier-free guidance [32] is adopted by default. Directly\nback-propagating the reward loss to the diffusion models\ncan be computationally intensive and risks catastrophic for-\ngetting [23, 24, 39]. To circumvent these issues, we incor-\nporate LoRA [37] by default. To further accelerate fine-\ntuning, we truncate the gradient to only back-propagate the\nlast DDIM sampling step following [16]. Experiments are\nconducted on 4 NVIDIA A100s, with the batch size set to\n8 and the learning rate set to 1 \u00d7 10\u22125. To strike a cost-\nperformance balance, we fine-tune InstructVideo with\ndefault parameters for 20k steps if not otherwise stated.\nInference.\nAfter reward fine-tuning, we merge the\nLoRA weights into the ModelScopeT2V parameters to en-\nsure that InstructVideo\u2019s inference cost is identical\nto ModelScopeT2V [37].\nFor text-to-video generation,\nInstructVideo uses 20-step DDIM inference.\n4. Experiments\n4.1. Effectiveness of InstructVideo\nComparison with the base model ModelScopeT2V. To\nverify the efficacy of InstructVideo, we compare it\nwith ModelScopeT2V [78] utilizing 20 and even 50 DDIM\nsteps in Fig. 3. Examining the examples, we observe that\nthe quality of videos generated by InstructVideo con-\nsistently outperforms the base model by a margin. Specif-\nically, notable enhancements include 1) clearer and more\ncoherent structures and scenes even if the animal is mov-\ning, exemplified by the walking cat and the swimming fish;\n2) more appealing coloration, exemplified by the sunflower,\nthe bee and the mountain goat; 3) an enhanced delineation\nof scene details, exemplified by the rock and grass on the\ncliff, and the texture of all the animals; and 4) improved\nvideo-text alignment, exemplified by the distinct portrayal\nof sunflowers and the bird\u2019s reflections on the water. Re-\nmarkably, these advancements are achieved without com-\npromising motion fluidity and the resultant videos can of-\nten surpass the video quality of the WebVid10M dataset.\nNotably, InstructVideo even attenuates watermarks\npresent in WebVid10M. These qualitative leaps, consis-\ntently favored by human annotators, are attributed to the\nreward fine-tuning process, which effectively refines the\nvideo diffusion model.\nComparison with other reward fine-tuning methods.\nThis aims to validate the efficacy of reward fine-tuning con-\nceptualized as an editing process. We compare with other\nrepresentative reward fine-tuning methods, including policy\ngradient algorithm, DDPO [4], reward-weighted regression,\nRWR [43] and direct reward back-propagation method,\nDRaFT [16]. For DRaFT, we adopt DRaFT-1 for efficient\nfine-tuning. SegVR and TAR are employed for all com-\npared methods to standardize reward signals. The compara-\ntive analysis on the evaluation set, presented in Fig. 5(a),\nWhite butterfly on violet flower\ncloseup macro view.\nDuck walks \nby the lake.\nMule deer in Nebraska \nrunning across landscape.\nRWR\nDDPO\nDRaFT\nInstructVideo\n\u8fd9\u91cc\u4e3b\n\u975e\u52a8\u7269\nFigure 4. Comparison of InstructVideo with other reward fine-tuning methods. We set D = 20 for all methods.\n(b)\n(c)\n(a)\n0\n10\n20\n30\n40\n50\nTime (h)\n0.250\n0.255\n0.260\n0.265\n0.270\nReward Score\nComparison with Reward Fine-tuning Methods\nInstructVideo\nDRaFT\nRWR\nDDPO\n0\n10\n20\n30\n40\nTime (h)\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n0.28\nReward Score\nThe Effect of Varying \ntar\n0.5\n1.0\n2.0\n0\n10\n20\n30\n40\n50\nTime (h)\n0.250\n0.255\n0.260\n0.265\n0.270\nReward Score\nThe Effect of Varying Noise Level\n0.1\n0.3\n0.5\n0.6\n0.7\n0.9\nFigure 5. (a) Comparison with other reward fine-tuning methods. (b) The effect of varying noise level \u03c4. The vertical dashed line\nindicates 20k steps of optimization for \u03c4 = 0.6. (c) The effect of varying \u03bbtar.\nleads to two findings: 1) Both RWR and DDPO exhibit\na performance plateau after about 11 hours of fine-tuning,\nwith further optimization failing to enhance or even dete-\nriorating performance. 2) Direct reward back-propagation\nmethods, including InstructVideo and DRaFT, ini-\ntially lag during the first 11 hours but subsequently demon-\nstrate fine-tuning efficiency, especially InstructVideo.\nTo further validate the efficacy of our method, we provide\nvisual comparisons in Fig. 4, where we adopt the optimal\nfine-tuned checkpoint for each method in Fig. 5(a). The\nexamples reflect InstructVideo \u2019s superiority, evident\nin: 1) the clarity and coherence of structural and scenic ele-\nments, 2) the vibrancy of colors, 3) the precision in depict-\ning intricacies, and 4) enhanced video-text alignment.\nGeneralization to unseen text prompts.\nWe assessed\nthe model\u2019s generalization capabilities using two distinct\nsets of prompts: 1) those describing new animals and 2)\nthose related to non-animals, none of which are present in\nthe fine-tuning data. For this purpose, we curate about 4\nprompts each for 6 new animal species following [59] and\n46 prompts for non-animals. Our comparative analysis of\nInstructVideo, the base model ModelScopeT2V and\nother reward fine-tuning methods is presented in Tab. 1. It\nis worth noting that during fine-tuning, SegVR and TAR\nare adopted by default to provide reward signals. Our ob-\nservations are threefold: 1) Increasing the number of DDIM\nsteps enhances the video quality. 2) Compared to the base\nmodel ModelScopeT2V in the second row, all methods im-\nprove reward scores for these unseen prompts. 3) Among\nall methods, InstructVideo outperforms other alterna-\ntives, affirming its superior generalization capabilities. To\nfurther demonstrate this, we provide visual comparisons\nin Fig. 6, featuring a set of unseen animal species, vari-\nous sceneries, and human figures. The presented examples\ndisplay an enhanced quality and exhibit an improvement in\nvideo-text alignment.\nUser study. To further qualitatively compare the videos\ngenerated by InstructVideo and other methods, we\nconduct a user study comparing our methods with the base\nmodel ModelScopeT2V and the reward fine-tuned model\nLand snail in southern of\nThailand moving forward.\nSide view of elderly woman \npetting cat in window.\nFly over teen on rocks \nrevealing frosted mountain.\nRWR\nDDPO\nDRaFT\nInstructVideo\nFigure 6. Comparison of InstructVideo\u2019s generalization capabilities with other methods. We set D = 20 for all methods.\nMethod\nIn-domain\nNew Animals Non-animals\nModelScopeT2V\u2020\n0.2542 \u00b1 0.0122\n0.2541 \u00b1 0.0109\n0.2610 \u00b1 0.0158\nModelScopeT2V\n0.2506 \u00b1 0.0155\n0.2502 \u00b1 0.0138\n0.2557 \u00b1 0.0177\nDDPO [4]\n0.2511 \u00b1 0.0114\n0.2524 \u00b1 0.0112\n0.2564 \u00b1 0.0171\nRWR [43]\n0.2550 \u00b1 0.0166\n0.2517 \u00b1 0.0101\n0.2625 \u00b1 0.0146\nDRaFT [16]\n0.2584 \u00b1 0.0123\n0.2561 \u00b1 0.0098\n0.2644 \u00b1 0.0174\nInstructVideo 0.2717 \u00b1 0.0137\n0.2645 \u00b1 0.0125\n0.2682 \u00b1 0.0202\nTable 1. Generalization to unseen text prompts. \u2020 denotes the\nmodel utilizes D = 50 while others adopt D = 20. \u2018In-domain\u2019\ndenotes in-domain animal prompts from the evaluation data.\nInstructVideo\nQuality\nAlignment\nvs\nOurs\nTie\nOther\nOurs\nTie\nOther\nModelScopeT2V\n75.5% 15.5% 9.0% 28.5% 57.0% 14.5%\nDRaFT\n76.0% 10.5% 13.5% 30.0% 51.5% 18.5%\nTable 2. User study. \u2018Tie\u2019 indicates instances where annotators\nthink two videos are of comparable quality. \u2018Quality\u2019 and \u2018Align-\nment\u2019 represent video quality and video-text alignment.\nDRaFT in Tab. 2. We recruited two participants who have\nrelated research experience in generative models to assess\nthe quality of videos in terms of video quality and video-text\nalignment. To simplify the annotation process, participants\nwere presented with pairs of videos and asked to identify\nwhich video was superior or if both were of equal qual-\nity. To ensure a comprehensive comparison, we chose 60\nprompts from the 45 fine-tuning animal species, 20 prompts\nfrom the 6 new animal species and 20 prompts describing\nnon-animals. More details are presented in the Appendix.\nWe observe that our method consistently outperforms other\nmethods.\nSpecifically, improvements in video quality, a\nnoted shortcoming of the base model, are more pronounced\nthan improvements in video-text alignment.\n4.2. Ablation Study\nThe effect of varying noise level \u03c4. To determine the opti-\nmal choice for noise level \u03c4, we vary its value and evaluate\nits impact on reward scores using the evaluation data. We il-\nlustrate the results in Fig. 5(b). An increase in \u03c4 from 0.1 to\n0.5 correlates with a progressive enhancement in the highest\nreward scores achieved by InstructVideo. However,\nexcessively prolonged fine-tuning precipitates a sharp de-\ncline in generative performance. This phenomenon can be\nattributed to the limited edited space available to the model\nat lower noise levels, which constricts its ability to find\nthe optimal output space as directed by the reward scores.\nWhen we further increase \u03c4 from 0.6 to 0.9, we observe\nthat the reward score enhancement per hour becomes mi-\nnor, suggesting challenges associated with generating from\nan extended sampling chain. Optimally, a noise level of\n\u03c4 = 0.6 strikes a balance, providing a feasible starting point\nfor editing that still allows for a substantial exploration of\nthe edited space. After 20k steps, more optimization leads\nto over-optimization [59], meaning that further steps can de-\ngrade the visual quality of the output despite potential in-\ncreases in the reward score. Thus, we finalize on \u03c4 = 0.6\nwith 20k steps of fine-tuning.\nThe effect of varying \u03bbtar.\nTo determine the optimal\nchoice for \u03bbtar, we vary its value and evaluate its impact.\nWe illustrate the results in Fig. 5(c).\nA relatively high\nvalue \u03bbtar, such as 2.0, results in fi decaying exponentially\nfaster towards those border frames, thus providing dimin-\nished reward signals. A relatively low value \u03bbtar, such as\n0.5, leads to fi decaying more gently towards those border\nframes, thus strengthening the reward signals. This equal-\nized weighting across frames can destabilize fine-tuning,\nleading to a precipitous decline in reward scores. Subse-\nInstructVideo\nw/o TAR\nw/o SegVR\nFigure 7. Ablation study on SegVR and TAR. This video shows\na white dog walking in the park in slow motion.\nquent increases in scores do not necessarily indicate im-\nproved video quality but indicate quality degradation, as\nrevealed by the rising variance. Thus, an appropriate co-\nefficient to ensure stable fine-tuning is imperative and we\nfinalize on \u03bbtar = 1.0.\nAblation on SegVR and TAR. To qualitatively verify the\nefficacy of SegVR and TAR, we present illustrative results\nin Fig. 7. Removing either SegVR or TAR results in a no-\nticeable reduction in temporal modeling capabilities. This\nsuggests that overly dense or excessively strong reward sig-\nnals can lead to generation collapse. The degradation of\nmodeling temporal dynamics often leads to the degraded\nquality of the individual frames due to the intertwined na-\nture of spatial and temporal parameters. These observations\nunderscore the critical roles of SegVR and TAR in main-\ntaining fine-tuning stability.\n4.3. Further Analysis\nThe evolution of the generated videos during reward\nfine-tuning. To elucidate how the reward fine-tuning works,\nwe present a visual progression in Fig. 8. The top row de-\npicts a video generated without fine-tuning. All the frames\nin this video exhibit a lack of the dog\u2019s fur texture. More-\nover, a notable blurriness characterizes the third frame due\nto sudden and unanticipated motion, while the fourth frame\nsuffers from a loss of facial clarity. As reward fine-tuning\nproceeds, we observe a noticeable enhancement in terms\nof all aspects mentioned above. Surprisingly, watermarks,\nwhich are consistently present across the dataset, also grad-\nually fade. The resultant video is full of clear details and\naesthetically pleasing coloration.\nImpact of fine-tuning data quality on the fine-tuning re-\nsults. To investigate this, we self-collect a dataset compris-\ning an equivalent number of video-caption pairs for 45 an-\nimal species, which are employed for fine-tuning. The re-\nsults are illustrated in Fig. 9. We employ horizontal dashed\nlines to indicate the quality of different data, inferred from\nreward scores. While the variance of the generated videos\nHow does the\nvideo evolve\uff1f\nModelScopeT2V\nInstructVideo\n(5k steps)\nInstructVideo\n(10k steps)\nInstructVideo\n(15k steps)\nInstructVideo\n(20k steps)\nFigure 8. The evolution of generated videos during fine-tuning.\nThe video shows a bobtail dog walking.\n0\n10\n20\n30\n40\nTime (h)\n0.24\n0.25\n0.26\n0.27\n0.28\nReward Score\nThe Effect of Utilizing Different Fine-tuning Data\nWebVid data\nCollected data\nFigure 9. The effect of utilizing different fine-tuning data. The\ncolored horizontal dashed lines denote the reward scores of differ-\nent fine-tuning data, matched by the color of the respective curve.\nare comparable for two kinds of fine-tuning data, the em-\nployment of higher-quality data, i.e., WebVid10M, yields\nsuperior average reward scores compared to that obtained\nusing the lower-quality counterpart. This suggests that su-\nperior fine-tuning data can facilitate reward fine-tuning.\nConstraints of fine-tuning data on resultant video qual-\nity. Fig. 9 showcases that InstructVideo is capable of\ngenerating videos that achieve reward scores significantly\nexceeding those of the fine-tuning data itself, as denoted by\nthe horizontal dashed lines. This observation leads us to\nconclude that the quality of the fine-tuning data does not\nimpose a ceiling on the potential quality of the fine-tuned\nresults. Our fine-tuning pipeline has the propensity to sur-\npass the initial data quality, thus facilitating the generation\nof videos with substantially enhanced reward scores.\n5. Conslusion\nIn this paper, we introduce InstructVideo, a method\nthat pioneers instructing video diffusion models with hu-\nman feedback by reward fine-tuning.\nWe recast reward\nfine-tuning as an editing process that mitigates computa-\ntional burden and enhances fine-tuning efficiency. We re-\nsort to image reward models to provide human feedback on\ngenerated videos and propose SegVR and TAR to ensure\neffective fine-tuning. Extensive experiments validate that\nInstructVideo not only elevates visual quality but also\nmaintains robust generalization capabilities.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-\nBin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent\ndiffusion with temporal shift for efficient text-to-video gen-\neration. arXiv preprint arXiv:2304.08477, 2023. 2\n[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, et al. A general lan-\nguage assistant as a laboratory for alignment. arXiv preprint\narXiv:2112.00861, 2021. 1\n[3] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In IEEE International Conference on\nComputer Vision, 2021. 1, 5\n[4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and\nSergey Levine. Training diffusion models with reinforce-\nment learning. arXiv preprint arXiv:2305.13301, 2023. 1,\n2, 3, 4, 5, 7\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim\nDockhorn, Seung Wook Kim, Sanja Fidler, and Karsten\nKreis. Align your latents: High-resolution video synthe-\nsis with latent diffusion models.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 22563\u201322575, 2023. 1, 2\n[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 23206\u201323217, 2023. 2\n[7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Sta-\nblevideo: Text-driven consistency-aware diffusion video\nediting.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 23040\u201323050,\n2023. 2\n[8] Hong Chen, Xin Wang, Guanning Zeng, Yipeng Zhang,\nYuwei Zhou, Feilin Han, and Wenwu Zhu. Videodreamer:\nCustomized multi-subject text-to-video generation with\ndisen-mix finetuning.\narXiv preprint arXiv:2311.00990,\n2023. 2\n[9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter1:\nOpen diffusion models for high-quality\nvideo generation, 2023.\n[10] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng,\nTsung-Yi Lin, and Ming-Hsuan Yang. Motion-conditioned\ndiffusion model for controllable video synthesis.\narXiv\npreprint arXiv:2304.14404, 2023. 2\n[11] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video:\nControllable text-to-video generation with diffusion mod-\nels, 2023. 2\n[12] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin\nZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu\nQiao, and Ziwei Liu.\nSeine: Short-to-long video diffu-\nsion model for generative transition and prediction. arXiv\npreprint arXiv:2310.20700, 2023. 2\n[13] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,\nShane Legg, and Dario Amodei. Deep reinforcement learn-\ning from human preferences. Advances in neural informa-\ntion processing systems, 30, 2017. 1\n[14] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,\nShane Legg, and Dario Amodei. Deep reinforcement learn-\ning from human preferences. Advances in neural informa-\ntion processing systems, 30, 2017. 3\n[15] \u00a8Ozg\u00a8un C\u00b8 ic\u00b8ek, Ahmed Abdulkadir, Soeren S Lienkamp,\nThomas Brox, and Olaf Ronneberger.\n3d u-net: learn-\ning dense volumetric segmentation from sparse annota-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2016:\n19th International Confer-\nence, Athens, Greece, October 17-21, 2016, Proceedings,\nPart II 19, pages 424\u2013432. Springer, 2016. 3\n[16] Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet.\nDirectly fine-tuning diffusion models on differentiable re-\nwards. arXiv preprint arXiv:2309.17400, 2023. 1, 2, 3, 5,\n7\n[17] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord.\nDiffedit:\nDiffusion-based seman-\ntic image editing with mask guidance.\narXiv preprint\narXiv:2210.11427, 2022. 2, 4\n[18] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan,\nShizhe Diao, Jipeng Zhang, Kashun Shum, and Tong\nZhang. Raft: Reward ranked finetuning for generative foun-\ndation model alignment. arXiv preprint arXiv:2304.06767,\n2023. 1\n[19] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming\ntransformers for high-resolution image synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2021. 3, 4\n[20] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis. Structure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 2\n[21] Ying Fan and Kangwook Lee.\nOptimizing ddpm\nsampling with shortcut fine-tuning.\narXiv preprint\narXiv:2301.13362, 2023. 3\n[22] Ying\nFan,\nOlivia\nWatkins,\nYuqing\nDu,\nHao\nLiu,\nMoonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-\nmad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok:\nReinforcement learning for fine-tuning text-to-image diffu-\nsion models. arXiv preprint arXiv:2305.16381, 2023. 3,\n1\n[23] Tao Feng, Mang Wang, and Hangjie Yuan.\nOvercom-\ning catastrophic forgetting in incremental object detection\nvia elastic response distillation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9427\u20139436, 2022. 5\n[24] Tao Feng, Hangjie Yuan, Mang Wang, Ziyuan Huang, Ang\nBian, and Jianzhou Zhang. Progressive learning without\nforgetting. arXiv preprint arXiv:2211.15215, 2022. 5\n[25] Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu,\nJong-Chyi Su, William Yang Wang, and Sean Bell. Tell\nme what happened: Unifying text-guided video completion\nvia multimodal masked video generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10681\u201310692, 2023. 2\n[26] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, An-\ndrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang,\nMing-Yu Liu, and Yogesh Balaji. Preserve your own cor-\nrelation: A noise prior for video diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 22930\u201322941, 2023. 2\n[27] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow:\nConsistent diffusion features for consistent\nvideo editing. arXiv preprint arXiv:2307.10373, 2023. 2\n[28] Audrunas Gruslys, R\u00b4emi Munos, Ivo Danihelka, Marc\nLanctot, and Alex Graves. Memory-efficient backpropa-\ngation through time. Advances in neural information pro-\ncessing systems, 29, 2016. 3\n[29] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing\nZhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang\nJiang, and Hang Xu.\nReuse and diffuse:\nIterative\ndenoising for text-to-video generation.\narXiv preprint\narXiv:2309.03549, 2023. 2\n[30] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen.\nLatent video diffusion models for high-\nfidelity long video generation. 2022.\n[31] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,\nYuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao\nWeng, Ying Shan, et al.\nAnimate-a-story: Storytelling\nwith retrieval-augmented video generation. arXiv preprint\narXiv:2307.06940, 2023. 2\n[32] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in Neural Informa-\ntion Processing Systems, 33:6840\u20136851, 2020. 1, 2, 3\n[34] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al.\nIma-\ngen video: High definition video generation with diffusion\nmodels. arXiv preprint arXiv:2210.02303, 2022. 1, 2\n[35] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv preprint arXiv:2204.03458, 2022. 1,\n2\n[36] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers. arXiv preprint\narXiv:2205.15868, 2022. 2\n[37] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2, 5, 1\n[38] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto Henschel,\nZhangyang Wang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators.\narXiv preprint arXiv:2303.13439, 2023. 2\n[39] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\nVeness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-\nBarwinska, et al.\nOvercoming catastrophic forgetting in\nneural networks. Proceedings of the national academy of\nsciences, 114(13):3521\u20133526, 2017. 5\n[40] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland\nMatiana, Joe Penna, and Omer Levy. Pick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023. 2\n[41] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. Diffwave: A versatile diffusion model\nfor audio synthesis. In International Conference on Learn-\ning Representations, 2021. 1\n[42] Ariel Lapid, Idan Achituve, Lior Bracha, and Ethan Fetaya.\nGd-vdm: Generated depth for better diffusion-based video\ngeneration. arXiv preprint arXiv:2306.11173, 2023. 2\n[43] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,\nYuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad\nGhavamzadeh, and Shixiang Shane Gu.\nAligning text-\nto-image models using human feedback.\narXiv preprint\narXiv:2302.12192, 2023. 1, 2, 3, 5, 7\n[44] Jan Leike, David Krueger, Tom Everitt, Miljan Martic,\nVishal Maini, and Shane Legg. Scalable agent alignment\nvia reward modeling: a research direction. arXiv preprint\narXiv:1811.07871, 2018. 1, 3\n[45] Sergey Levine, Aviral Kumar, George Tucker, and Justin\nFu.\nOffline reinforcement learning:\nTutorial, review,\nand perspectives on open problems.\narXiv preprint\narXiv:2005.01643, 2020. 1\n[46] Chunyi Li, Zicheng Zhang, Haoning Wu, Wei Sun,\nXiongkuo Min, Xiaohong Liu, Guangtao Zhai, and Weisi\nLin.\nAgiqa-3k: An open database for ai-generated im-\nage quality assessment. arXiv preprint arXiv:2306.04717,\n2023. 2\n[47] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation.\nIn\nICML, pages 12888\u201312900. PMLR, 2022. 2\n[48] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceed-\nings of the AAAI conference on artificial intelligence, 2018.\n2\n[49] Binhui Liu, Xin Liu, Anbo Dai, Zhiyong Zeng, Zhen Cui,\nand Jian Yang. Dual-stream diffusion net for text-to-video\ngeneration. arXiv preprint arXiv:2308.08316, 2023. 2\n[50] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Ji-\naya Jia. Video-p2p: Video editing with cross-attention con-\ntrol. arXiv preprint arXiv:2303.04761, 2023. 2\n[51] Shijie Ma, Huayi Xu, Mengjian Li, Weidong Geng,\nMeng Wang, and Yaxiong Wang.\nOptimal noise pursuit\nfor augmenting text-to-video generation.\narXiv preprint\narXiv:2311.00949, 2023. 2\n[52] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-\nguided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023. 2\n[53] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential\nequations. arXiv preprint arXiv:2108.01073, 2021. 2, 4\n[54] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubra-\nmanian.\nSync-draw: Automatic video generation using\ndeep recurrent attentive architectures. In Proceedings of the\n25th ACM international conference on Multimedia, pages\n1096\u20131104, 2017. 2\n[55] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and\nYedid Hoshen. Dreamix: Video diffusion models are gen-\neral video editors. arXiv preprint arXiv:2302.01329, 2023.\n2\n[56] OpenAI. GPT-4 technical report, 2023. 1\n[57] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training lan-\nguage models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 1, 3\n[58] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and\nTao Mei. To create what you tell: Generating videos from\ncaptions.\nIn Proceedings of the 25th ACM international\nconference on Multimedia, pages 1789\u20131798, 2017. 2\n[59] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and\nKaterina Fragkiadaki.\nAligning text-to-image diffusion\nmodels with reward backpropagation.\narXiv preprint\narXiv:2310.03739, 2023. 1, 3, 6, 7\n[60] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 2\n[61] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yu-\njie Wei, Yingya Zhang, Changxin Gao, and Nong Sang.\nHierarchical spatio-temporal decoupling for text-to-video\ngeneration. arXiv preprint arXiv:2312.04483, 2023. 2\n[62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 2\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 1, 2, 3\n[64] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image seg-\nmentation. In Medical Image Computing and Computer-\nAssisted Intervention\u2013MICCAI 2015: 18th International\nConference, Munich, Germany, October 5-9, 2015, Pro-\nceedings, Part III 18, pages 234\u2013241. Springer, 2015. 3\n[65] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu,\nJianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining\nGuo. Mm-diffusion: Learning multi-modal diffusion mod-\nels for joint audio and video generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10219\u201310228, 2023. 2\n[66] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki.\nLaion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. arXiv preprint arXiv:2111.02114, 2021. 1\n[67] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 1, 2\n[68] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3626\u20133636, 2022. 2\n[69] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Computer Vision, pages 2256\u20132265. PMLR, 2015.\n1, 2, 3\n[70] Jiaming Song,\nChenlin Meng,\nand Stefano Ermon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2, 3, 4\n[71] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole. Score-\nbased generative modeling through stochastic differential\nequations. arXiv preprint arXiv:2011.13456, 2020. 1\n[72] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,\nRyan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,\nand Paul F Christiano. Learning to summarize with human\nfeedback. Advances in Neural Information Processing Sys-\ntems, 33:3008\u20133021, 2020. 3\n[73] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng,\nand Mohit Bansal. Any-to-any generation via composable\ndiffusion. arXiv preprint arXiv:2305.11846, 2023. 2\n[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\nAurelien Rodriguez, Armand Joulin, Edouard Grave, and\nGuillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023. 1\n[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS,\npages 5998\u20136008, 2017. 1\n[76] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open do-\nmain textual description. arXiv preprint arXiv:2210.02399,\n2022. 2\n[77] Dimitri von R\u00a8utte, Elisabetta Fedele, Jonathan Thomm, and\nLukas Wolf. Fabric: Personalizing diffusion models with\niterative feedback. arXiv preprint arXiv:2307.10159, 2023.\n3\n[78] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023.\n1, 2, 3, 5\n[79] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks: Towards good practices for deep action recogni-\ntion. In ECCV, pages 20\u201336. Springer, 2016. 2, 4\n[80] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu.\nVideofactory: Swap\nattention in spatiotemporal diffusions for text-to-video gen-\neration. arXiv preprint arXiv:2305.10874, 2023. 2\n[81] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Videocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 1, 2, 3\n[82] Xiang Wang,\nShiwei Zhang,\nHan Zhang,\nYu Liu,\nYingya Zhang, Changxin Gao, and Nong Sang.\nVide-\nolcm: Video latent consistency model.\narXiv preprint\narXiv:2312.09109, 2023. 2\n[83] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video genera-\ntion with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 1, 2\n[84] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan,\nZhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and\nHongming Shan.\nDreamvideo: Composing your dream\nvideos with customized subject and motion. arXiv preprint\narXiv:2312.04433, 2023. 2\n[85] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei\nJi, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva:\nGenerating open-domain videos from natural descriptions.\narXiv preprint arXiv:2104.14806, 2021. 2\n[86] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation.\nIn European\nConference on Computer Vision, pages 720\u2013736. Springer,\n2022. 2\n[87] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 1, 4\n[88] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen,\nFeng Zhu, Rui Zhao, and Hongsheng Li.\nHuman pref-\nerence score v2: A solid benchmark for evaluating hu-\nman preferences of text-to-image synthesis. arXiv preprint\narXiv:2306.09341, 2023. 2, 4, 5\n[89] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hong-\nsheng Li. Human preference score: Better aligning text-to-\nimage models with human preference. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 2096\u20132105, 2023. 1, 2, 3\n[90] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang,\nYong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xi-\naodong Cun, Xintao Wang, et al. Make-your-video: Cus-\ntomized video generation using textual and structural guid-\nance. arXiv preprint arXiv:2306.00943, 2023. 2\n[91] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-\ntao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:\nAnimating open-domain images with video diffusion pri-\nors. 2023.\n[92] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang\nJiang. Simda: Simple diffusion adapter for efficient video\ngeneration. arXiv preprint arXiv:2308.09710, 2023. 2\n[93] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward:\nLearning and evaluating human preferences for text-to-\nimage generation. arXiv preprint arXiv:2304.05977, 2023.\n1, 2, 3\n[94] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas.\nVideogpt: Video generation using vq-vae and\ntransformers. arXiv preprint arXiv:2104.10157, 2021. 2\n[95] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023. 2\n[96] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho\nKim, Jung-Woo Ha, and Jinwoo Shin. Generating videos\nwith dynamics-aware implicit generative adversarial net-\nworks. arXiv preprint arXiv:2202.10571, 2022. 2\n[97] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 18456\u2013\n18466, 2023. 2\n[98] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation, 2023. 2\n[99] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim:\nGeneralized denoising diffusion implicit models.\narXiv\npreprint arXiv:2206.05564, 2022. 1, 2\n[100] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 2\n[101] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-\nChih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio\nSavarese, Stefano Ermon, et al. Hive: Harnessing human\nfeedback for instructional visual editing.\narXiv preprint\narXiv:2303.09618, 2023. 3\n[102] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li,\nand Jun Zhu.\nControlvideo:\nAdding conditional con-\ntrol for one shot text-to-video editing.\narXiv preprint\narXiv:2305.17098, 2023. 2\n[103] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Jun-\nhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and\nMike Zheng Shou.\nMotiondirector: Motion customiza-\ntion of text-to-video diffusion models.\narXiv preprint\narXiv:2310.08465, 2023. 2\n[104] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2\n[105] Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi\nTuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, and\nJianlong Fu. Moviefactory: Automatic movie creation from\ntext using large generative models for language and images.\narXiv preprint arXiv:2306.07257, 2023. 2\nInstructVideo: Instructing Video Diffusion Models with Human Feedback\nSupplementary Material\nIn this Appendix, we first detail on the potential so-\ncietal impact (Appendix A), and limitations and poten-\ntial future work (Appendix B). Subsequently, we pro-\nvide additional details about implementing LoRA (Ap-\npendix C) and user study (Appendix D). Moreover, we\nprovide more visualization results to demonstrate the ef-\nficacy of InstructVideo (Appendix E). Finally, we\nprovide experiments to validate the efficacy of generation\nwith 50-step DDIM inference (Appendix F), reward fine-\ntuning with 50-step DDIM inference (Appendix G) and\nInstructVideo\u2019s adaptation to other reward functions\n(Appendix H).\nA. Potential Societal Impact\nInstructVideo, as the pioneering effort in instructing\nvideo diffusion models with human feedback, prioritizes\nusers\u2019 preferences for AI-generated content. We conducted\nthis research, motivated by the varied quality of generated\nvideos induced by the varied quality of the curated web-\nscale datasets. Pre-training models on such unfiltered data\ncan lead to outputs that deviate from human preferences. In\nthe context of the broader research community, we advocate\nthat video generation systems, akin to other generative mod-\nels like language models [56, 57], should prioritize ethical\nconsiderations and human values.\nMoreover, conventional video generation systems might\nnot always resonate with all users in terms of aesthetic\nstyle and often struggle with accurately reflecting textual\nprompts. InstructVideo steps in as a human-centered\ntechnology, efficiently addressing these issues in a data- and\ncomputation-efficient way and opening up possibilities for\ncommercial applications, particularly in sectors like educa-\ntion and entertainment.\nHowever, as InstructVideo primarily targets re-\nsearch, aiming at investigating the practicality of aligning\nvideo diffusion models with human preferences, its deploy-\nment to any circumstance beyond research should be ap-\nproached with thorough oversight and evaluation to ensure\nresponsible and ethical use.\nB. Limitation and Future work\nWe recognize that InstructVideo, as an initial en-\ndeavor in this area, comes with its limitations. Although\nwe validate the efficacy of image reward models, we antici-\npate that specialized video reward models capturing human\npreferences might be even more superior since they evaluate\none generated video as a whole. Additionally, as a common\nissue mentioned in previous works [4, 18, 22, 59], reward\nfine-tuning carries a risk of over-optimization, meaning that\nexcessive optimization steps will result in the degradation\nof the video quality despite potential increases in the reward\nscore. Addressing these aspects presents avenues for future\nresearch, including the development of a more advanced\nvideo reward model and the design of strategic mechanisms\nto identify and ameliorate over-optimization.\nC. More Details about Implementing LoRA\nTo instantiate LoRA [37] for efficient tuning, we adopt the\nimplementation used in Diffusers2. Specifically, we config-\nure the intrinsic rank within LoRA to 4 to ensure fast pro-\ncessing. LoRA modifications are applied to every Trans-\nformer [75] layer within our model, targeting the linear\nlayers responsible for query, key, value, and output pro-\njections.\nModelScopeT2V [78] contains 1,347.44M pa-\nrameters, whereas the additional parameters introduced by\nadding LoRA amount to only 1.58M \u2013 approximately 0.1%\nof the total ModelScopeT2V parameters.\nD. More Details about User Study\nIn the main paper, we present a user study to demon-\nstrate the effectiveness of InstructVideo. This study\ninvolves a comparative analysis of videos generated by\nInstructVideo and other methods, focusing on two key\naspects: video quality and video-text alignment. For video\nquality, we asked annotators to evaluate: 1) The overall vi-\nsual quality of the videos, 2) Alignment with general human\naesthetic preferences, such as pleasing visuals, texture and\ndetails, and 3) The smoothness and consistency in terms of\nstructural and color transitions within the video. Regarding\nvideo-text alignment, annotators are tasked with determin-\ning the extent to which the generated videos accurately and\nclearly represent the content of the provided text prompts.\nThis assessment included evaluating the depiction of enti-\nties, attributes, relationships, and motions as described in\nthe prompts. To simplify the evaluation process, annotators\nare asked to perform pairwise comparisons between videos,\nthereby streamlining their task to direct contrasts rather than\nisolated assessments.\nE. More Visualization Results\nWe provide more visualization results to exemplify the con-\nclusions we draw in the main paper, including: 1) More\nresults demonstrating how the generated videos evolve as\n2https://github.com/huggingface/diffusers/blob/\nmain/src/diffusers/models/lora.py\nMethod\nIn-domain New Animals Non-animals\nModelScopeT2V\n0.2506 \u00b1 0.0155\n0.2502 \u00b1 0.0138\n0.2557 \u00b1 0.0177\nModelScopeT2V\u2020\n0.2542 \u00b1 0.0122\n0.2541 \u00b1 0.0109\n0.2610 \u00b1 0.0158\nInstructVideo 0.2717 \u00b1 0.0137 0.2645 \u00b1 0.0125\n0.2682 \u00b1 0.0202\nInstructVideo\u2020 0.2736 \u00b1 0.0125 0.2664 \u00b1 0.0131\n0.2739 \u00b1 0.0210\nTable A.1. Generation with 50-step DDIM inference after fine-\ntuning with 20-step DDIM inference. \u2020 denotes the model utilizes\nD = 50 while others adopt D = 20. \u2018In-domain\u2019 denotes in-\ndomain animal prompts from the evaluation data.\nthe fine-tuning process proceeds as shown in Fig. A.2;\n2) More results showcasing the comparison between\nInstructVideo and the base model ModelScopeT2V\nas illustrated in Fig. A.3; 3) More results exemplifying the\ncomparison between InstructVideo and other reward\nfine-tuning methods as shown in Fig. A.4; 4) More results\nshowing the InstructVideo\u2019s generalization capabili-\nties to unseen text prompts as shown in Fig. A.5.\nF. 50-Step Generation with InstructVideo\nTo\nshowcase\nthe\nadaptability\nand\neffectiveness\nof\nInstructVideo,\nwe conduct experiments using a\n50-step DDIM inference for generation after initial fine-\ntuning with 20-step DDIM inference.\nThe results are\nshown in Tab. A.1. We observe that InstructVideo,\ndespite being fine-tuned with a 20-step protocol, remains\neffective under a longer 50-step DDIM inference protocol,\nas demonstrated by the boosted reward scores. We present\nseveral cases to further illustrate InstructVideo\u2019s\nefficacy as shown in Fig. A.6.\nWe observe that both\ninference schemes can significantly improve over the base\nmodel and adopting more inference steps can occasionally\nlead to better results.\nG. 50-Step Reward Fine-tuning\nTo assess the adaptation of InstructVideo to differ-\nent DDIM steps, we experiment on reward fine-tuning with\nthe commonly-used 50-step DDIM inference and evaluate\nits 20-step generation quality for a fair comparison. We\npresent the results in Fig. A.1. The results demonstrate that\nInstructVideo could be optimized towards higher re-\nward scores in both settings. However, utilizing 50 steps de-\ngrades the fine-tuning efficiency, likely due to the increased\ncomputation brought by longer sampling chains.\nH. Adaptation to Other Reward Functions\nIn the main paper,\nwe focus on the utilization of\nHPSv2 [88]. To further validate the generalization of our\nmethod to other reward functions, we explore the applica-\ntion of ImageReward [93] as our reward model. ImageRe-\nward is a general-purpose text-to-image human preference\nreward model, fine-tuned on BLIP [47]. We perform reward\n0\n10\n20\n30\n40\n50\nTime (h)\n0.24\n0.25\n0.26\n0.27\n0.28\nReward Score\nReward Fine-tuning with 50 DDIM Steps\n20 steps\n50 steps\nFigure A.1. Reward finetuning with 50-step DDIM inference.\nfine-tuning as HPSv2 and present results in Fig. A.7. We\nobserve that the quality of the videos are generally boosted\nin terms of structures, color vibrancy and details, despite\nthat the stylistic aspects of the videos differ from those fine-\ntuned with HPSv2.\ne\n\uff1f\nModelScopeT2V\nInstructVideo\n(5k steps)\nInstructVideo\n(10k steps)\nInstructVideo\n(15k steps)\nInstructVideo\n(20k steps)\nFigure A.2. More examples showing the evolution of generated videos during fine-tuning.\nModelScopeT2V (D=20)\nInstructVideo (D=20)\nModelScopeT2V (D=50)\nBee collects honey in \nflower at morning.\nBighorn sheep ovis\ncanadensisram\nbetween snowcovered\nsage bushes.\nBantam or chicken \non the garden.\nGreat tit in bird \nfeeder.\nwith\nModel\nFigure A.3. More examples showing the comparion of InstructVideo with the base model ModelScopeT2V. ModelScopeT2V\nutilizes 20 and 50 DDIM steps.\nHorse grazing on pasture and \neating green grass.\nBrush turkey head \ncomes toward bower.\nClose up grey rabbit \neating corn.\nRWR\nDDPO\nDRaFT\nInstructVideo\n\u8fd9\u91cc\u4e3b\u8981\n\u975e\u52a8\u7269\u4e5f\nwith\nrd\nFigure A.4. More examples showing the comparison of InstructVideo with other reward fine-tuning methods. We set D = 20 for\nall methods.\nPortrait of cheetah acinonyx\njubatus.\nLobster moth caterpillar is \neating leaf of host plant.\nDrone fly up near old historical \nwhite Christianity church.\nRWR\nDDPO\nDRaFT\nInstructVideo\nto\ns\nFigure A.5. More examples showing the comparison of InstructVideo\u2019s generalization capabilities with other methods. We set\nD = 20 for all methods.\nModelScopeT2V (D=50)\nInstructVideo (D=50)\nInstructVideo (D=20)\nClose up of llama \nin autumn.\nWater bird on the \nlake in spring \npodiceps cristatus.\nGoat in a green \nsummer.\nFish swimming by \nin kelp.\nM \nFigure A.6. Generation with 50-step DDIM inference after fine-tuning with 20-step DDIM inference.\nModelScopeT2V\nInstructVideo\nA zebra eating \ngrass on nature.\nClose up of \nblacknose sheep \nhead.\nAn beetle on the \nbranch in close up.\nRussian blue cat \nplayfully biting \nand scratching \nowners hand.\nFigure A.7. Comparison of InstructVideo fine-tuned using ImageReward with the base model ModelScopeT2V. We set D = 20\nfor two methods.\n"
  },
  {
    "title": "Splatter Image: Ultra-Fast Single-View 3D Reconstruction",
    "link": "https://arxiv.org/pdf/2312.13150.pdf",
    "upvote": "13",
    "text": "Splatter Image: Ultra-Fast Single-View 3D Reconstruction\nStanislaw Szymanowicz\nChristian Rupprecht\nAndrea Vedaldi\nVisual Geometry Group \u2014 University of Oxford\n{stan,chrisr,vedaldi}@robots.ox.ac.uk\n360  Novel Views\n\u2218\nInput\n3D Reconstruction: \nSplatter Image\nInput\nImage-to-\nImage Network\nRendering: 588FPS\nxyz\n\u03a3\nRGB\nopacity\nU-Net: 38FPS\nGaussian \nSplatting\n360  Novel \nViews\n\u2218\n360  Novel Views\n\u2218\nInput\nFigure 1. The Splatter Image is an ultra-fast method for single- and few-view 3D reconstruction. It works by applying an image-to-image\nneural network to the input view and obtain, as output, another image that holds the parameters of one coloured 3D Gaussian per pixel.\nThe resulting Gaussian mixture can be rendered very quickly into an arbitrary view of the object by using Gaussian Splatting.\nAbstract\nWe introduce the Splatter Image1, an ultra-fast approach\nfor monocular 3D object reconstruction which operates at\n38 FPS. Splatter Image is based on Gaussian Splatting,\nwhich has recently brought real-time rendering, fast train-\ning, and excellent scaling to multi-view reconstruction. For\nthe first time, we apply Gaussian Splatting in a monocu-\nlar reconstruction setting. Our approach is learning-based,\nand, at test time, reconstruction only requires the feed-\nforward evaluation of a neural network. The main inno-\nvation of Splatter Image is the surprisingly straightforward\ndesign: it uses a 2D image-to-image network to map the\ninput image to one 3D Gaussian per pixel. The resulting\nGaussians thus have the form of an image, the Splatter Im-\nage. We further extend the method to incorporate more than\none image as input, which we do by adding cross-view at-\ntention. Owning to the speed of the renderer (588 FPS),\nwe can use a single GPU for training while generating en-\ntire images at each iteration in order to optimize perceptual\nmetrics like LPIPS. On standard benchmarks, we demon-\nstrate not only fast reconstruction but also better results\nthan recent and much more expensive baselines in terms of\nPSNR, LPIPS, and other metrics.\n1Website: szymanowiczs.github.io/splatter-image\n1. Introduction\nSingle-view 3D reconstruction poses a fundamental chal-\nlenge in computer vision. In this paper, we contribute Splat-\nter Image, a method that achieves ultra-fast single-view re-\nconstruction of the 3D shape and appearance of objects.\nThis method uses Gaussian Splatting [14] as the underlying\n3D representation, taking advantage of its rendering quality\nand speed. It works by predicting a 3D Gaussian for each\nof the input image pixels, using an image-to-image neural\nnetwork. Remarkably, the 3D Gaussians in the resulting\n\u2018Splatter Image\u2019 provide 360\u25e6 reconstructions (Fig. 1) of\nquality matching or outperforming much slower methods.\nThe key challenge in using 3D Gaussians for monocular\nreconstruction is to design a network that takes an image of\nan object as input and produces as output a corresponding\nGaussian mixture that represents all sides of it. We note\nthat, while a Gaussian mixture is a set, i.e., an unordered\ncollection, it can still be stored in an ordered data structure.\nSplatter Image takes advantage of this fact by using a 2D\nimage as a container for the 3D Gaussians, so that each pixel\ncontains in turn the parameters of one Gaussian, including\nits opacity, shape, and colour.\nThe advantage of storing sets of 3D Gaussians in an im-\nage is that it reduces the reconstruction problem to learning\nan image-to-image neural network. In this manner, the re-\nconstructor can be implemented utilizing only efficient 2D\narXiv:2312.13150v1  [cs.CV]  20 Dec 2023\noperators (e.g., 2D convolution instead of 3D convolution).\nWe use in particular a U-Net [32] as those have demon-\nstrated excellent performance in image generation [31]. In\nour case, their ability to capture small image details [44]\nhelps to obtain higher-quality reconstructions.\nSince the 3D representation in Splatter Image is a mix-\nture of 3D Gaussians, it enjoys the rendering speed and\nmemory efficiency of Gaussian Splatting, which is advan-\ntageous both in inference and training. In particular, ren-\ndering stops being a training bottleneck and we can afford\nto generate complete views of the object to optimize percep-\ntual metrics like LPIPS [45]. Possibly even more remark-\nably, the efficiency is such that our model can be trained on a\nsingle GPU on standard benchmarks of 3D objects, whereas\nalternative methods typically require distributed training on\nseveral GPUs. We also extend Splatter Image to take sev-\neral views as input. This is achieved by taking the union\nof the Gaussian mixtures predicted from individual views,\nafter registering them to a common reference. Furthermore,\nwe allow different views to communicate during prediction\nby injecting lightweight cross-view attention layers in the\narchitecture.\nEmpirically, we study several properties of Splatter Im-\nage. First, we note that, while the network only sees one\nside of the object, it can still produce a 360\u25e6 reconstruction\nof it by using the prior acquired during training. The 360\u25e6\ninformation is coded in the 2D image by allocating different\nGaussians in a given 2D neighbourhood to different parts\nof the 3D object. We also show that many Gaussians are\nin practice inactivated by setting their opacity to zero, and\ncan thus be culled in post-processing. We further validate\nSplatter Image by comparing it to alternative, slower recon-\nstructors on standard benchmark datasets like ShapeNet [4]\nand CO3D [28]. Compared to these slower baselines, Splat-\nter Image is not only competitive in terms of quality, but in\nfact state-of-the-art in several cases, improving both recon-\nstruction PSNR and LPIPS. We argue that this is because\nthe very efficient design allows training the model more ef-\nfectively, including using image-level losses like LPIPS.\nTo summarise, our contributions are: (1) to port Gaus-\nsian Splatting to learning-based monocular reconstruction;\n(2) to do so with the Splatter Image, a straightforward, effi-\ncient and performant 3D reconstruction approach that op-\nerates at 38 FPS on a standard GPU; (3) to also extend\nthe method to multi-view reconstruction; (4) and to ob-\ntain state-of-the-art reconstruction performance in standard\nbenchmarks in terms of reconstruction quality and speed.\n2. Related work\nRepresentations for single-view 3D reconstruction. In\nrecent years, implicit representations like NeRF [24] have\ndominated learning-based few-view reconstruction. Some\nworks have approached this problem by parameterising the\nMLP in NeRF using global [12, 29], local [44] or both\nglobal and latent codes [16]. However, implicit represen-\ntations, particularly MLP-based ones, are notoriously slow\nto render, up to 2s for a single 128 \u00d7 128 image.\nSome follow-up works [8, 38] have used faster implicit\nrepresentations based on voxel grids that encode opacities\nand colours directly [8, 38] \u2014 similar to DVGO [37], they\ncan thus achieve significant speed-ups. However, due to\ntheir voxel-based representation, they scale poorly with res-\nolution. They also assume the knowledge of the absolute\nviewpoint of each object image.\nA hybrid implicit-explicit triplane representation [2] has\nbeen proposed as a compromise between rendering speed\nand memory consumption. Triplanes can be predicted by\nnetworks in the camera view space, instead of a global\nreference frame, thus allowing reconstruction in the view-\nspace [7]. While they are not as fast to render as explicit\nrepresentations, they are fast enough to be effectively used\nfor single-view reconstruction [1, 7].\nFinally, several works predict multi-view images di-\nrectly [3, 19, 39, 42].\n3D models can then be obtained\nwith test-time multi-view optimisation. The main disadvan-\ntage of image-to-image novel view generators is that they\nexhibit noticeable flicker and are 3D inconsistencies, thus\nlimiting the quality of obtained reconstructions. In addi-\ntion, test-time optimisation is an additional overhead, limit-\ning the overall reconstruction speed.\nIn contrast to these works, our method predicts a mixture\nof 3D Gaussians in a feed-forward manner. As a result, our\nmethod is fast at inference and achieves real-time rendering\nspeeds while achieving state-of-the-art image quality across\nmultiple metrics on the standard single-view reconstruction\nbenchmark ShapeNet-SRN [35].\nWhen more than one view is available at the input, one\ncan learn to interpolate between available views in the 3D\nspace to estimate the scene geometry [5, 21], learn a view\ninterpolation function [41] or optimize a 3D representation\nof a scene using semantic priors [11]. Our method is primar-\nily a single-view reconstruction network, but we do show\nhow Splatter Image can be extended to fuse multiple views.\nHowever, we focus our work on object-centric reconstruc-\ntion rather than on generalising to unseen scenes.\n3D Reconstruction with Point Clouds. PointOutNet [9]\nadapted PointNet [27] to take image encoding as input\nand trained point cloud prediction networks using 3D point\ncloud supervision. PVD [46] and PC2 [23] extended this ap-\nproach using Diffusion Models [10] by conditioning the de-\nnoising process on partial point clouds and RGB images, re-\nspectively. These approaches require ground truth 3D point\nclouds, limiting their applicability. Other works [17, 30, 43]\ntrain networks for Novel Views Synthesis purely from\nvideos and use point clouds as intermediate 3D represen-\ntations for conditioning 2D inpainting or generation net-\nworks. However, these point clouds are assumed to cor-\nrespond to only visible object points. In contrast, our Gaus-\nsians can model any part of the object, and thus afford 360\u25e6\nreconstruction.\nPoint cloud-based representations have also been used\nfor high-quality reconstruction from multi-view images.\nADOP [33] used points with a fixed radius and also used\n2D inpainting networks for hole-filling. Gaussian Splat-\nting [14] used non-isotropic 3D Gaussians with variable\nscale, thus removing the need for 2D inpainting networks.\nWhile showing high-quality results, Gaussian Splatting re-\nquires many images per scene and has not yet been used in\na learning-based reconstruction framework as we do here.\nOur method also uses 3D Gaussians as an underlying\nrepresentation but predicts them from as few as a single\nimage. Moreover, it outputs a full 360\u25e6 3D reconstruction\nwithout using 2D or 3D inpainting networks.\nProbabilistic 3D Reconstruction. Single-view 3D recon-\nstruction is an ambiguous problem, so several authors argue\nthat it should be tackled as a conditional generative prob-\nlem. Diffusion Models have been employed for conditional\nNovel View Synthesis [3, 18, 19, 42]. Due to generating im-\nages without underlying geometries, the output images ex-\nhibit noticeable flicker. This can be mitigated by simultane-\nously generating multi-view images [20, 34], or guaranteed\nby reconstructing a geometry at every step of the denoising\nprocess [38, 40]. Other works build and use a 3D [6, 25] or\n2D [7, 22] prior which can be used in an image-conditioned\nauto-decoding framework.\nHere, we focus on deterministic reconstruction. How-\never, few-view reconstruction is required to output 3D ge-\nometries from feed-forward methods [20, 34, 38, 40]. Our\nmethod is capable of few-view 3D reconstruction, thus it is\ncomplimentary to these generative methods and could lead\nto improvements in generation speed and quality.\n3. Method\nWe provide background information on Gaussian Splatting\nin Sec. 3.1, and then describe the Splatter Image in Secs. 3.2\nto 3.6.\n3.1. Overview of Gaussian Splatting\nA radiance field [24] is given by the opacity function\n\u03c3(x) \u2208 R+ and the colour function c(x, \u03bd) \u2208 R3, where\n\u03bd \u2208 S2 is the viewing direction of the 3D point x \u2208 R3.\nThe field is rendered onto an image I(u) by integrating\nthe colors observed along the ray x\u03c4 = x0 \u2212 \u03c4\u03bd, \u03c4 \u2208 R+\nthat passes through pixel u:\nI(u) =\nZ \u221e\n0\nc(x\u03c4, \u03bd)\u03c3(x\u03c4)e\u2212\nR \u03c4\n0 \u03c3(x\u00b5) d\u00b5 d\u03c4.\n(1)\nGaussian Splatting [48] represents these two functions as a\nmixture \u03b8 of G colored Gaussians\ngi(x) = exp\n\u0012\n\u22121\n2(x \u2212 \u00b5i)\u22a4\u03a3\u22121\ni (x \u2212 \u00b5i)\n\u0013\n,\nwhere 1 \u2264 i \u2264 G, \u00b5i \u2208 R3 is the Gaussian mean or center\nand \u03a3i \u2208 R3\u00d73 is its covariance, specifying its shape and\nsize. Each Gaussian has also an opacity \u03c3i \u2208 R+ and a\nview-dependent colour ci(v) \u2208 R3. Together, they define a\nradiance field as follows:\n\u03c3(x) =\nG\nX\ni=1\n\u03c3igi(x),\nc(x, \u03bd) =\nPG\ni=1 ci(\u03bd)\u03c3igi(x)\nPG\nj=1 \u03c3igi(x)\n.\n(2)\nThe mixture of Gaussians is thus given by the set\n\u03b8 = {(\u03c3i, \u00b5i, \u03a3i, ci), i = 1, . . . , G}.\nGaussian Splatting [14, 48] provides a very fast differen-\ntiable renderer I = R(\u03b8, \u03c0) that approximates Eq. (1), map-\nping the mixture \u03b8 to a corresponding image I given a view-\npoint \u03c0.\n3.2. The Splatter Image\nThe renderer R maps the set of 3D Gaussians \u03b8 to an image\nI. We now seek for an inverse function \u03b8 = S(I) which\nreconstructs the mixture of 3D Gaussians \u03b8 from an image\nI, thereby performing single-view 3D reconstruction.\nOur key innovation is to propose an extremely simple\nand yet effective design for such a function. Specifically, we\npredict a Gaussian for each pixel of the input image I, using\na standard image-to-image neural network architecture that\noutputs an image M, the Splatter Image.\nIn more detail, Let u = (u1, u2, 1) denote one of the\nH \u00d7 W image pixels. This corresponds to ray x = ud in\ncamera space, where d is the depth of the ray point. Our\nnetwork f takes as input the H \u00d7 W \u00d7 3 RGB image, and\noutputs directly a H \u00d7 W \u00d7 K tensor, where each pixel is\nassociated to the K-dimensional feature vector packing the\nparameters Mu = (\u03c3, \u00b5, \u03a3, c) of a corresponding Gaussian.\nWe assume that Gaussians are expressed in the same ref-\nerence frame of the camera. As illustrated in Fig. 2, The\nnetwork predicts the depth d and offset (\u2206x, \u2206y, \u2206y), set-\nting\n\u00b5 =\n\uf8ee\n\uf8f0\nu1d + \u2206x\nu2d + \u2206y\nd + \u2206z\n\uf8f9\n\uf8fb .\n(3)\nThe network also predicts the opacity \u03c3, the shape \u03a3 and\nthe colour c. For now, we assume that the colour is Lam-\nbertian, i.e., c(\u03bd) = c \u2208 R3, and relax this assumption in\nSec. 3.5. Section 3.6 explains in detail how these quantities\nare predicted.\nd\nx, y, z\n(d, x, y, z)\nFigure 2. Predicting locations. The location of each Gaussian\nis parameterised by depth d and a 3D offset \u2206 = (\u2206x, \u2206y, \u2206z).\nThe 3D Gaussians are projected to depth d (blue) along camera\nrays (green) and moved by the 3D offset \u2206 (red).\nDiscussion. One may wonder how this design can predict a\nfull 360\u25e6 reconstruction of the object when the reconstruc-\ntion is aligned to only one of its views. In practice, the\nnetwork learns to use some of the Gaussians to reconstruct\nthe given view, and some to reconstruct unseen portions of\nthe scene, automatically. Note also that the network can\nalso decide to switch off any Gaussian by simply predicting\n\u03c3 = 0, if needed. These points are then not rendered and\ncan be culled in post-processing.\nOur design can also be seen as an extension of depth\nprediction networks, where the network is only tasked with\npredicting the depth of each pixel. Here, we also predict\ninvisible parts of the geometry, as well as the appearance.\n3.3. Learning formulation\nLearning to predict the Splatter Image is simple and effi-\ncient \u2014 we carry it out on a single GPU using at most\n20GB of memory at training time in all our single-view re-\nconstruction experiments. For training, we assume a multi-\nview dataset, either real or synthetic. At a minimum, this\ndataset D consists of triplets (I, J, \u03c0), where I is a source\nimage, J a target image, and \u03c0 the viewpoint change be-\ntween the source and the target camera. Then we simply\nfeed the source I as input to Splatter Image, and minimize\nthe average reconstruction loss of target view J:\nL(S) =\n1\n|D|\nX\n(I,J,\u03c0)\u2208D\n\u2225J \u2212 R(S(I), \u03c0)\u22252.\n(4)\nImage-level losses. A main advantage of the speed and ef-\nficiency of our method is that it allows for rendering entire\nimages at each training iteration, even for relatively large\nbatches (this differs from NeRF [24], which only generates\na certain number of pixels in a batch). In particular, this\nmeans that, in addition to decomposable losses like the L2\nloss above, we can use image-level losses like LPIPS [45],\nwhich do not decompose into per-pixel losses. In practice,\nwe experiment with a combination of such losses.\nScale normalization. Estimating the scale of an object\nfrom a single view is ambiguous, and this ambiguity will be\nchallenging to resolve for a network trained with a loss like\none in Eq. (4). In synthetic datasets this is not an issue be-\ncause all objects are at a fixed distance from the camera and\nrendered with the same camera intrinsics, thus removing the\nambiguity. However, in real datasets like CO3D [28], this\nambiguity is present. We apply pre-processing following\nthe protocol of [38], thus approximately fixing the scale of\nall objects.\nRegularisations. We also add generic regularisers to pre-\nvent parameters from taking on unreasonable values (e.g.,\nGaussians which are larger than the reconstructed objects,\nor vanishingly small). Please see the sup. mat. for details.\n3.4. Extension to multiple input viewpoints\nIf two or more input views Ij, j \u2208 {1, . . . , N} are provided,\nwe can apply network S multiple times to obtain multiple\nSplatter Images Mj, one per view.\nWarping 3D Gaussians. If (R, T) is the relative camera\npose change from an additional view to the reference view,\nwe can take the mixture of 3D Gaussians \u03b8 defined in the\nadditional view\u2019s coordinates and warp it to the reference\nview. Specifically, a Gaussian g of parameters (\u03c3, \u00b5, \u03a3, c)\nmaps to Gaussian \u02dcg of parameters (\u03c3, \u02dc\u00b5, \u02dc\u03a3, \u02dcc) where\n\u02dc\u00b5 = R\u00b5 + T,\n\u02dc\u03a3 = R\u03a3R\u22a4,\n\u02dcc = c.\nWe use the symbol \u03d5[\u03b8] to denote the Gaussian Splat ob-\ntained by warping each Gaussian in \u03b8. Here we have also\nassumed a Lambertian colour model and will discuss in\nSec. 3.5 how more complex models transform.\nPredicting Composite 3D Gaussian Mixtures. Given N\ndifferent views Ij and corresponding warps \u03d5, we can ob-\ntain a composite mixture of 3D Gaussians simply by taking\ntheir union:\n\u0398 =\nN\n[\nj=1\n\u03d5j[S(Ij)].\nNote that this set of 3D Gaussians is defined in the coordi-\nnate system of the reference camera.\n3.5. View-dependent colour\nGeneralising beyond the Lambertian colour model, we\nuse spherical harmonics [14] to represent view-dependent\ncolours. For a particular Gaussian (\u03c3, \u00b5, \u03a3, c), we then de-\nfine [c(\u03bd; \u03b1)]i = PL\nl=0\nPL\nm=\u2212L \u03b1ilmY m\nl (\u03bd) where \u03b1ilm\nare coefficients predicted by the network and Y m\nl\nare spher-\nical harmonics, L is the order of the expansion, and \u03bd \u2208 S2\nis the viewing direction.\nWarping\nthe\ncolour\nmodel. The\nviewpoint\nchange\nof Sec. 3.4 transforms a viewing direction \u03bd in the source\ncamera to the corresponding viewing direction in the ref-\nerence frame as \u02dc\u03bd = R\u03bd.\nWe can then find the trans-\nformed colour function by finding the coefficients \u02dc\u03b1 such\nthat c(\u03bd; \u03b1) = c(\u02dc\u03bd; \u02dc\u03b1). This is possible because spherical\nharmonics are closed under rotation; in fact, each order is\nclosed. However, the general case requires the computa-\ntion of Wigner matrices. For simplicity, we only consider\norders L = 0 (Lambertian) and L = 1. Hence, the first\nlevel has one constant component Y 0\n0 and the second level\nhas three components which we can write collectively as\nY1 = [Y \u22121\n1\n, Y 0\n1 , Y 1\n1 ] such that\nY1(\u03bd) =\nr\n3\n4\u03c0 \u03a0\u03bd,\n\u03a0 =\n\uf8ee\n\uf8f0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n\uf8f9\n\uf8fb .\nWe can then conveniently rewrite [c(\u03bd; \u03b1)]i\n= \u03b1i0 +\n\u03b1\u22a4\ni1Y1(\u03bd). From this and c(\u03bd; \u03b10, \u03b11) = c(\u02dc\u03bd; \u02dc\u03b10, \u02dc\u03b11) we\nconclude that \u02dc\u03b1i0 = \u02dc\u03b1i0, and \u02dc\u03b1i1 = \u03a0\u22121R\u03a0\u03b1i1.\n3.6. Neural network architecture\nThe bulk of the predictor S mapping the input image I\nto the mixture of Gaussians \u03b8 is architecturally identical\nto the SongUNet of [36]. The last layer is replaced with\na 1 \u00d7 1 convolutional layer with 12 + kc output chan-\nnels, where kc \u2208 {3, 12} depending on the colour model.\nGiven I \u2208 R3\u00d7H\u00d7W as input, the network thus produces\na (12 + kc) \u00d7 H \u00d7 W tensor as output, coding, for each\npixel u channels, the parameters (\u02c6\u03c3, \u2206, \u02c6d, \u02c6s, \u02c6q, \u03b1) which\nare then transformed to opacity, offset, depth, scale, rota-\ntion and colour, respectively. These are activated by non-\nlinear functions to obtain the Gaussian parameters. Specif-\nically, the opacity is obtained using the sigmoid opera-\ntor as \u03c3 = sigmoid(\u02c6\u03c3). The depth is obtained as d =\n(zfar \u2212 znear) sigmoid( \u02c6d) + znear. The mean \u00b5 is then ob-\ntained using Eq. (3). Following [14], the covariance is ob-\ntained as \u03a3 = R(q) diag(exp \u02c6s)2R(q)\u22a4 where R(q) is the\nrotation matrix with quaternion q = \u02c6q/\u2225\u02c6q\u2225 and \u02c6q \u2208 R4.\nFor multi-view reconstruction, we apply the same net-\nwork to each input view and then use the approach\nof Sec. 3.4 to fuse the individual reconstructions. In order to\nallow the network to coordinate and exchange information\nbetween views, we consider two modifications to it.\nFirst, we condition the network with the corresponding\ncamera pose (R, T) (we only assume access to the relative\ncamera pose to a common but otherwise arbitrary reference\nframe). In fact, since we consider cameras in a turn-table-\nlike configuration, we only pass vectors (Re3, T) where\ne3 = (0, 0, 1). We do so by encoding each entry via a si-\nnusoidal positional embedding of order 9, resulting in 60\ndimensions in total. Finally, these are applied to the U-Net\nblocks via FiLM [26] embeddings.\nSecond, we add cross-attention layers to allow commu-\nnication between the features of different views. We do so\nin a manner similar to [34], but only at the lowest UNet res-\nolution, which maintains the computational cost very low.\n4. Experiments\nWe begin our experiments with an evaluation of the speed of\nthe method, followed by an evaluation of single-view recon-\nstruction on two synthetic and two real object categories,\nand finish with an assessment of multi-view reconstruction.\nDatasets. The standard benchmark for evaluating single-\nview 3D reconstruction is ShapeNet-SRN [35]. We train\nour method in the single-class setting and report results on\nthe \u201cCar\u201d and \u201cChair\u201d classes, following prior work. We\nuse the images, camera intrinsics, camera poses and data\nsplits as provided by the dataset and train our method using\nrelative camera poses: the reconstruction is done in the view\nspace of the conditioning camera.\nIn addition, we challenge our method with two classes\nfrom the CO3D dataset: Hydrants and Teddybears. Simi-\nlarly to recent methods [3, 40] we take the largest center-\ncrop in the original images and resize to 128 \u00d7 128 resolu-\ntion with Lanczos interpolation. Similarly to many single-\nand few-view reconstruction methods [44, 47? ] we also re-\nmove backgrounds. To reduce the scale ambiguity in single-\nview reconstruction, we follow the pre-processing protocol\nof Viewset Diffusion [38] and adjust bounds zfar and znear.\nSee supplement for details.\nFinally, we use the ShapeNet-SRN Cars dataset for the\nevaluation of the two-view reconstruction quality.\nBaselines. For ShapeNet, we compare against implicit [12,\n16, 35, 44], hydrid implicit-explicit [7] and explicit meth-\nods [8, 38] methods.\nWhile Viewset Diffusion [38] and\nNeRFDiff [7] were introduced as diffusion-based generative\nmethods, one can use their deterministic variants by using\ntheir reconstruction network in a single forward pass. Since\nwe are proposing a deterministic reconstruction method,\nwe do not compare to methods that employ Score Distil-\nlation [47] or feed-forward diffusion models [3, 42]. For\nCO3D we compare against PixelNeRF which we train for\n400,000 iterations with their officially released code on data\npreprocessed with the aforementioned protocol.\nImplementation details can be found in the supp. mat.\n4.1. Assessing quality\nWe measure the quality of our reconstructions by evaluat-\ning the quality of Novel View Synthesis, in line with related\nworks [16, 44]. We perform reconstruction from a given\nsource view and render the 3D shape to unseen target views.\nWe measure the Peak Signal-to-Noise Ratio (PSNR), Struc-\ntural Similarity (SSIM) and perceptual quality (LPIPS). We\nfollow standard protocol in the ShapeNet-SRN datasets: in\nMethod\nRC\n1-view Cars\n1-view Chairs\nPSNR \u2191SSIM \u2191LPIPS \u2193PSNR \u2191SSIM \u2191LPIPS \u2193\nSRN\n\u2717\n22.25\n0.88\n0.129\n22.89\n0.89\n0.104\nCodeNeRF\n\u2717\n23.80\n0.91\n0.128\n23.66\n0.90\n0.166\nFE-NVS\n\u2717\n22.83\n0.91\n0.099\n23.21\n0.92\n0.077\nViewsetDiff w/o D \u2717\n23.21\n0.90\n0.116\n24.16\n0.91\n0.088\nPixelNeRF\n\u2713\n23.17\n0.89\n0.146\n23.72\n0.90\n0.128\nVisionNeRF\n\u2713\n22.88\n0.90\n0.084\n24.48\n0.92\n0.077\nNeRFDiff w/o NGD\u2713\n23.95\n0.92\n0.092\n24.80\n0.93\n0.070\nOurs\n\u2713\n24.00\n0.92\n0.078\n24.43\n0.93\n0.067\nTable 1.\nShapeNet-SRN: Single-View Reconstruction.\nOur\nmethod achieves State-of-the-Art reconstruction quality on all\nmetrics on the Car dataset and on two metrics in the Chair dataset,\nwhile performing reconstruction in the camera view-space. \u2018RC\u2019\nindicates if a method can operate using only relative camera poses.\nsingle-view reconstruction, we use view 64 as the condi-\ntioning view and in two-view reconstruction we use views\n64 and 128 as conditioning. All unseen views are used for\nthe computation of metrics. We also evaluate single-view\nreconstruction quality in CO3D. We use the first frame as\nconditioning and all other frames as target frames. We use\nall testing sequences in the Hydrant and Teddybear classes\nwhere the first conditioning frame has a valid foreground\nmask (with probability p > 0.8). In practice, this means\nevaluating on 49 \u2018Hydrant\u2019 and 93 \u2018Teddybear\u2019 sequences.\n4.1.1\nSingle-view 3D reconstruction\nShapeNet-SRN Cars and Chairs. In Tab. 1 we compare\nthe single-view reconstruction quality on the ShapeNet-\nSRN benchmark. Our method outperforms all deterministic\nreconstruction methods in Structural Similarity (SSIM) and\nPerceptual Quality (LPIPS), indicating it provides sharper\nreconstructions. Moreover, it also outperforms all determin-\nistic baselines on the car dataset on Peak Signal-to-Noise\nRatio (PSNR), suggesting that the reconstructions are also\nmore accurate. In addition to strong quantitative perfor-\nmance, our method only needs relative camera poses (does\nnot require canonical camera poses) and is more efficient in\nboth training and testing (see Sec. 4.2).\nIn Fig. 3 we qualitatively show that our method is able\nto capture interesting and thin geometries and capture the\ndetails of the conditioning views well. Fig. 4 shows that our\nmethod produces sharper and more accurate reconstructions\nthan prior works in challenging situations with limited visi-\nbility and thin structures.\nCO3D. Qualitative results and a comparison to PixelNeRF\nare shown in Fig. 5. Splatter Image predicts sharper images\nwhile also being 1000\u00d7 faster. Quantitatively (Tab. 2) our\nmodel outperforms PixelNeRF for both categories on SSIM\nand LPIPS and results in PSNR on par with PixelNeRF.\nInput\nNovel View Renders\nFigure 3. ShapeNet-SRN. Our method is able to represent difficult\ngeometries (e.g., windshield in yellow car), and preserve details\nof the conditioning image (front of red car), occluded parts (left\nchair), thin structures (middle chair) and complex shapes (right\nchair).\nInput\nPixelNeRF\nOurs\nGT\nVisionNeRF\nFigure 4.\nShapeNet-SRN Comparison.\nOur method outputs\nmore accurate reconstructions (cars\u2019 backs, top chair) and better\nrepresents thin regions (bottom chair).\nObject\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nHydrant\nPixelNeRF\n21.76\n0.78\n0.207\nHydrant\nOurs\n22.10\n0.81\n0.148\nTeddybear\nPixelNeRF\n19.57\n0.67\n0.297\nTeddybear\nOurs\n19.51\n0.73\n0.236\nTable 2. CO3D: Single-View. Our method outperforms Pixel-\nNeRF on this challenging benchmark across most metrics.\nInput\nPixelNeRF\nOurs\nGT\nFigure 5. CO3D Hydrants and Teddybears. Our method outputs\nsharper reconstructions than PixelNeRF while being 100x faster in\ninference.\nMethod\nRelative\n2-view Cars\nPose\nPSNR \u2191\nSSIM \u2191\nSRN\n\u2717\n24.84\n0.92\nCodeNeRF\n\u2717\n25.71\n0.91\nFE-NVS\n\u2717\n24.64\n0.93\nPixelNeRF\n\u2713\n25.66\n0.94\nOurs\n\u2713\n26.01\n0.94\nTable 3. Two-view reconstruction on ShapeNet-SRN Cars.\n4.1.2\nTwo-view 3D reconstruction\nWe compare our multi-view reconstruction model on\nShapeNet-SRN Cars by training a model for two-view pre-\ndictions (see Tab. 3). Prior work often relies on absolute\ncamera pose conditioning, meaning that the model learns to\nrely on the canonical orientation of the object in the dataset.\nThis limits the applicability of these models, as in practice\nfor a new image of an object, the absolute camera pose is\nof course unknown. Here, only ours and PixelNeRF can\ndeal with relative camera poses as input. Interestingly, our\nmethod shows not only better performance than PixelNeRF\nbut also improves over SRN, CodeNeRF, and FE-NVS that\nrely on absolute camera poses.\n4.1.3\nAblations\nWe evaluate the influence of individual components of our\nmethod on the final performance. Due to computational\ncost, we train these models at a shorter training schedule for\n100k iterations with L2 and further 25k with L2 and LLPIPS.\nSingle-View Model. We show the results of our ablation\nstudy for the single-view model in Tab. 4.\nWe test the\nimpact of all components. We train a model (w/o image)\nthat uses a fully connected, unstructured output instead of a\nInput\nOpacity\nLocation\nNovel View\nFigure 6. Analysis. Splatter Images represent full 360\u25e6 of objects\nby allocating background pixels to appropriate 3D locations (third\nrow) to predict occluded elements like wheels (left) or chair legs\n(middle). Alternatively, it predicts offsets in the foreground pixels\nto represent occluded chair parts (right).\nSplatter Image. This model cannot transfer image informa-\ntion directly to their corresponding Gaussians and does not\nachieve good performance. We also ablate predicting the\ndepth along the ray by simply predicting 3D coordinates for\neach Gaussian. This version also suffers from its inability\nto easily align the input image with the output. Remov-\ning the 3D offset prediction mainly harms the backside of\nthe object while leaving the front faces the same. This re-\nsults in a lower impact on the overall performance of this\ncomponent. Changing the degrees of freedom of the pre-\ndicted covariance matrix to be isotropic or removing view-\ndependent appearance (by removing Spherical Harmonics\nprediction) also reduced the image fidelity. Finally, remov-\ning perceptual loss (w/o LLPIPS) results in a very small drop\nin PSNR but a significant worsening of LPIPS, indicating\nthis loss is important for perceptual sharpness of reconstruc-\ntions. Being able to use LPIPS in optimisation is a direct\nconsequence of employing a fast-to-render representation\nand being able to render full images at training time.\nMulti-View Model. Table 5 ablates the multi-view model.\nWe individually remove the multi-view attention blocks,\nthe camera embedding and the warping component of the\nmulti-view model and find that they all are important to\nachieve the final performance.\nAnalysis. In Fig. 6, we analyse how 3D information is\nstored inside a Splatter Image. Since all information is ar-\nranged in an image format, we can visualise each of the\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFull model\n22.25\n0.90\n0.115\nw/o image\n20.60\n0.87\n0.152\nw/o depth\n21.21\n0.88\n0.145\nw/o view dir.\n21.77\n0.89\n0.121\nisotropic\n22.01\n0.89\n0.118\nw/o offset\n22.06\n0.90\n0.119\nw/o LLPIPS\n22.22\n0.89\n0.141\nTable 4. Ablations: Single-View Reconstruction.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFull model\n24.11\n0.92\n0.087\nw/o cross-view attn\n23.68\n0.92\n0.091\nw/o cam embed\n23.91\n0.92\n0.088\nw/o warping\n23.84\n0.92\n0.088\nTable 5. Ablations: Multi-View Reconstruction.\nRP\nE \u2193\nR \u2193\nForward \u2193\nTest \u2193\nNeRFDiff\n\u2713\n(0.031)\n(0.0180)\n(0.103)\n(4.531)\nFE-NVS\n\u2717\n(0.015)\n(0.0032)\n(0.028)\n(0.815)\nVisionNeRF\n\u2713\n0.008\n2.4312\n9.733\n607.8\nPixelNeRF\n\u2713\n0.003\n1.8572\n7.432\n463.3\nViewsetDiff\n\u2717\n0.025\n0.0064\n0.051\n1.625\nOurs 2-view\n\u2713\n0.030\n0.0017\n0.037\n0.455\nOurs 1-view\n\u2713\n0.026\n0.0017\n0.033\n0.451\nTable 6. Speed. Time required for image encoding (E), render-\ning (R), the \u2018Forward\u2019 time, indicative of train-time efficiency and\nthe \u2018Test\u2019 time, indicative of test-time efficiency. Our method is\nthe most efficient in both train and test time across open-source\navailable methods and only requires relative camera poses. \u2018RP\u2019\nindicates if a method can operate using only relative camera poses.\nmodalities: opacity, depth, and location offset. Pixels of the\ninput image that belong to the object tend to describe their\ncorresponding 3D structure, while pixels outside of the ob-\nject wrap around to close the object on the back.\n4.2. Assessing efficiency\nOne advantage of the Splatter Image is its training and test\ntime efficiency, which we assess below.\nTest-time efficiency. First, we assess the \u2018Test\u2019 time speed,\ni.e., the time it takes for the trained model to reconstruct an\nobject and generate a certain number of images. We refer-\nence the evaluation protocol of the standard ShapeNet-SRN\nbenchmark [35] and render 250 images at 1282 resolution.\nAssessing wall-clock time fairly is challenging as it de-\npends on many factors. All measurements reported here are\ndone on a single NVIDIA V100 GPU. We use officially re-\nleased code of Viewset Diffusion [38], PixelNeRF [44] and\nVisionNeRF [16] and rerun those on our hardware. NeRFD-\niff [7] and FE-NVS [8] do not have code available, so we\nuse their self-reported metrics. FE-NVS was evaluated os-\ntensibly on the same type of GPU, while NeRFDiff does\nnot include information about the hardware used and we\nwere unable to obtain more information. Since we could\nnot perfectly control these experiments, the comparisons to\nNeRFDiff and FE-NVS are only indicative. For Viewset\nDiffusion and NeRFDiff we report the time for a single pass\nthrough the reconstruction network.\nTab. 6 reports the \u2018Encoding\u2019 (E) time, spent by the net-\nwork to compute the object\u2019s 3D representation from an im-\nage, and the \u2018Rendering\u2019 (R) time, spent by the network to\nrender new images from the 3D representation. From those,\nwe calculate the \u2018Test\u2019 time, equal to the \u2018Encoding\u2019 time\nplus 250 \u2018Rendering\u2019 time. As shown in the last column\nof Tab. 6, our method is more than 1000\u00d7 faster in testing\nthan PixelNeRF and VisionNeRF (while achieving equal or\nsuperior quality of reconstruction in Tab. 1). Our method is\nalso faster than voxel-based Viewset Diffusion even though\ndoes not require knowing the absolute camera pose. The\nefficiency of our method is very useful to iterate quickly\nin research; for instance, evaluating our method on the full\nShapeNet-Car validation set takes less than 10 minutes on\na single GPU. In contrast, PixelNeRF takes 45 GPU-hours.\nTrain-time efficiency. Next, we assess the efficiency of the\nmethod during training. Here, the encoding time becomes\nmore significant because one typically renders only a few\nimages to compute the reconstruction loss and obtain a gra-\ndient (e.g., because there are only so many views available\nin the training dataset, or because generating more views\nprovides diminishing returns in terms of supervision). As\ntypical values (and as used by us in this work), we assume\nthat the method is tasked with generating 4 new views at\neach iteration instead of 250 as before. We call this the \u2018For-\nward\u2019 time and measure it the same way. As shown in the\n\u2018Forward\u2019 column of Tab. 6, our method is 246\u00d7 faster at\ntraining time than implicit methods and 1.5\u00d7 than Viewset\nDiffusion, which uses an explicit representation. With this,\nwe can train models achieving state-of-the-art quality on a\nsingle A6000 GPU in 7 days, while VisionNeRF requires\n16 A100 GPUs for 5 days.\n5. Conclusion\nWe have presented Splatter Image, a simple and fast method\nfor single- or few-view 3D reconstruction. The method pro-\ncesses images efficiently using an off-the-shelf 2D CNN ar-\nchitecture and predicts a pseudo-image containing one col-\nored 3D Gaussian per pixel. By combining fast inference\nwith fast rendering via Gaussian Splatting, Splatter Image\ncan be trained and evaluated quickly on synthetic and real\nbenchmarks. Splatter Image achieves state-of-the-art per-\nformance, does not require canonical camera poses, is sim-\nple to implement and offers significant computational sav-\nings in both training and inference.\nEthics. We use various datasets in a manner compatible\nwith their terms. There is no processing of personal data.\nFor further details on ethics, data protection, and copy-\nright please see https://www.robots.ox.ac.uk/\n\u02dcvedaldi/research/union/ethics.html.\nAcknowledgements. S. Szymanowicz is supported by an\nEPSRC Doctoral Training Partnerships Scholarship (DTP)\nEP/R513295/1 and the Oxford-Ashton Scholarship.\nA.\nVedaldi is supported by ERC-CoG UNION 101001212.\nReferences\n[1] Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Hen-\nderson, Hakan Bilen, Niloy J. Mitra, and Paul Guerrero.\nRenderDiffusion: Image diffusion for 3D reconstruction, in-\npainting and generation. In Proc. CVPR, 2023. 2\n[2] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In Proc. CVPR, 2022. 2\n[3] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-\nder W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait-\ntala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.\nGeNVS: Generative novel view synthesis with 3D-aware dif-\nfusion models. In Proc. ICCV, 2023. 2, 3, 5, 1\n[4] Angel X. Chang, Thomas A. Funkhouser, Leonidas J.\nGuibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio\nSavarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong\nXiao, Li Yi, and Fisher Yu. ShapeNet an information-rich 3d\nmodel repository. arXiv.cs, abs/1512.03012, 2015. 2\n[5] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su.\nMVSNeRF: Fast\ngeneralizable radiance field reconstruction from multi-view\nstereo. In Proc. ICCV, 2021. 2\n[6] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:\nA unified approach to 3d generation and reconstruction. In\nICCV, 2023. 3\n[7] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNerfdiff: Single-image view synthesis with nerf-guided dis-\ntillation from 3d-aware diffusion.\nIn Proc. ICML, pages\n11808\u201311826. PMLR, 2023. 2, 3, 5, 8\n[8] Pengsheng Guo, Miguel Angel Bautista, Alex Colburn,\nLiang Yang, Daniel Ulbricht, Joshua M. Susskind, and Qi\nShan.\nFast and explicit neural view synthesis.\nIn Proc.\nWACV, 2022. 2, 5, 8\n[9] Leonidas Guibas Haoqiang Fan, Hao Su. A point set gen-\neration network for 3d object reconstruction from a single\nimage. In ICCV, 2017. 2\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Proc. NeurIPS, 2020. 2\n[11] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\non a diet: Semantically consistent few-shot view synthesis.\nIn Proc. ICCV, pages 5885\u20135894, 2021. 2\n[12] Wonbong Jang and Lourdes Agapito. CodeNeRF: Disentan-\ngled neural radiance fields for object categories. In Proc.\nICCV, 2021. 2, 5\n[13] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. In Proc. NeurIPS, 2022. 1\n[14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis. 3D Gaussian Splatting for Real-Time\nRadiance Field Rendering. Proc. SIGGRAPH, 42(4), 2023.\n1, 3, 4, 5\n[15] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 1\n[16] Kai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin,\nYi-Chang Shih, and Ravi Ramamoorthi. Vision transformer\nfor nerf-based view synthesis from a single input image. In\nProc. WACV, 2023. 2, 5, 8\n[17] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh\nMakadia, Noah Snavely, and Angjoo Kanazawa.\nInfinite\nnature: Perpetual view generation of natural scenes from a\nsingle image. In Proc. ICCV, 2021. 2\n[18] Minghua Liu,\nChao Xu,\nHaian Jin,\nLinghao Chen,\nMukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:\nAny single image to 3D mesh in 45 seconds without per-\nshape optimization. arXiv.cs, abs/2306.16928, 2023. 3\n[19] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In Proc. ICCV, 2023. 2, 3\n[20] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n3\n[21] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\nWenping Wang. SparseNeuS: Fast generalizable neural sur-\nface reconstruction from sparse views. In Proc. ECCV, 2022.\n2\n[22] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi. Realfusion: 360\u00b0 reconstruction of any ob-\nject from a single image. In Proc. CVPR, 2023. 3\n[23] Luke Melas-Kyriazi, Christian Rupprecht, and Andrea\nVedaldi. PC2: Projection-conditioned point cloud diffusion\nfor single-image 3d reconstruction. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 2\n[24] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proc. ECCV, 2020. 2, 3, 4\n[25] Norman\nM\u00a8uller,\nYawar\nSiddiqui,\nLorenzo\nPorzi,\nSamuel Rota Bul`o,\nPeter Kontschieder,\nand Matthias\nNie\u00dfner.\nDiffRF: Rendering-guided 3D radiance field\ndiffusion. In Proc. CVPR, 2023. 3\n[26] Ethan Perez, Florian Strub, Harm de Vries, Vincent Du-\nmoulin, and Aaron C. Courville. FiLM: Visual reasoning\nwith a general conditioning layer. In AAAI, 2018. 5\n[27] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.\nPointNet: Deep learning on point sets for 3D classification\nand segmentation. arXiv preprint arXiv:1612.00593, 2016.\n2\n[28] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon Objects in 3D: Large-scale learning and evaluation of\nreal-life 3D category reconstruction. In Proc. CVPR, 2021.\n2, 4\n[29] Konstantinos Rematas, Ricardo Martin-Brualla, and Vittorio\nFerrari. ShaRF: Shape-conditioned radiance fields from a\nsingle view. In Proc. ICML, 2021. 2\n[30] Chris Rockwell, David F. Fouhey, and Justin Johnson. Pix-\nelsynth: Generating a 3d-consistent experience from a single\nimage. In Proc. ICCV, 2021. 2\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Proc. CVPR, 2022. 2\n[32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn Proc. MICCAI, 2015. 2\n[33] Darius R\u00a8uckert, Linus Franke, and Marc Stamminger. Adop:\nApproximate differentiable one-pixel point rendering. 2022.\n3\n[34] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv:2308.16512, 2023. 3, 5\n[35] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks:\nContinuous 3d-\nstructure-aware neural scene representations.\nIn Proc.\nNeurIPS, 2019. 2, 5, 8\n[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. Proc. ICLR, 2021. 5, 1\n[37] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proc. CVPR, 2022. 2\n[38] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion: (0-)image-conditioned 3D gen-\nerative models from 2D data. In Proceedings of the Interna-\ntional Conference on Computer Vision (ICCV), 2023. 2, 3,\n4, 5, 8, 1\n[39] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.\nMulti-view 3d models from single images with a convolu-\ntional network. In ECCV, 2016. 2\n[40] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon\nRezchikov, Joshua B. Tenenbaum, Fr\u00b4edo Durand, William T.\nFreeman, and Vincent Sitzmann.\nDiffusion with forward\nmodels: Solving stochastic inverse problems without direct\nsupervision. arXiv.cs, abs/2306.11719, 2023. 3, 5, 1\n[41] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P.\nSrinivasan, Howard Zhou, Jonathan T. Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas A. Funkhouser.\nIbrnet: Learning multi-view image-based rendering. In Proc.\nCVPR, 2021. 2\n[42] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. In\nProc. ICLR, 2023. 2, 3, 5\n[43] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\nJohnson. Synsin: End-to-end view synthesis from a single\nimage, 2020. 2\n[44] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\nPixelNeRF: Neural radiance fields from one or few images.\nIn Proc. CVPR, 2021. 2, 5, 8, 1\n[45] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In Proc. CVPR, 2018. 2, 4\n[46] Linqi Zhou, Yilun Du, and Jiajun Wu.\n3d shape genera-\ntion and completion through point-voxel diffusion. In Proc.\nICCV, 2021. 2\n[47] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nProc. CVPR, 2023. 5\n[48] Matthias Zwicker, Hanspeter Pfister, Jeroen van Baar, and\nMarkus H. Gross. EWA volume splatting. In Proc. IEEE\nVisualization Conference,, 2001. 3\nSplatter Image: Ultra-Fast Single-View 3D Reconstruction\nSupplementary Material\nA. Additional results\nThe project website szymanowiczs.github.io/\nsplatter- image.html includes additional results\nfrom our method on 4 object classes, comparisons of our\nmethod to baselines, a demonstration of running our method\non a video frame-by-frame and a recording with a short\nsummary of Splatter Image.\nCode is available via the\nproject website. Moreover, we present static comparisons\nof our method to PixelNeRF [44] and VisionNeRF on\nShapeNet-SRN Cars and Chairs in Fig. 7, as well as static\ncomparisons of our method to PixelNeRF on CO3D Hy-\ndrants and Teddybears in Fig. 8.\nB. CO3D Data preprocessing\nImage center-cropping. The renderer we use assumes the\nimage\u2019s principal point to be at the image centre.\nWe\npre-process image data to satisfy this constraint, similarly\nto [3, 40]. We take the largest square crop in the original\nimages centred on the principal point. We then reshape the\nimages to 128 \u00d7 128 resolution. We adjust the focal length\naccordingly with the resulting transformations.\nAddressing scale/depth ambiguity. Single-view recon-\nstruction is an ambiguous problem. In particular, given a\nsingle image, the scale of the object and its distance from\nthe camera are ambiguous. First, we normalise the scale of\nall scenes in the dataset following the protocol from [38]\nwhich normalises each scene using the ground truth point\ncloud so that the objects have the same scale. However,\neven after this transformation estimating the distance be-\ntween the object and the camera from visual information\nalone remains an ambiguous and challenging problem in\nthis dataset: focal lengths vary between and within se-\nquences, objects are partially cropped, and global scene pa-\nrameters such as distance to the object, camera trajectory\nand the angle at which objects are viewed all vary, posing a\nchallenge to both our and baseline methods. Thus, for both\nPixelNeRF and our method we set the center of prediction\nto the center of the object.\nIn our method we achieve this by setting znear = zgt \u2212 w\nand zfar = zgt + w, where zgt is the ground truth distance\nfrom the object to the source camera and w is a scalar bound\nthat is larger than the post-normalization object size. In\nPixelNeRF, we follow the default setting for turntable-style\ndatasets and provide the network with x = xv \u2212 zgt where\nx is the sample location at which we query the network and\nxv is the sample location in camera view space.\nC. Implementation details.\nWe train our model (based on SongUNet [36]) with L2 re-\nconstruction loss (Eq.4 main paper) on 3 unseen views and\nthe conditioning view for 800,000 iterations. We use the\nnetwork implementation from [13]. We use the Adam opti-\nmizer [15] with learning rate 2\u00d710\u22125 and batch size 8. For\nrasterization, we use the Gaussian Splatting implementation\nof [14]. After 800,000 iterations we decrease the learning\nrate by a factor of 10 and train for a further 100,000 (Cars,\nHydrants, Teddybears) or 200,000 (Chairs) iterations with\nthe loss L = (1 \u2212 \u03b1)L2 + \u03b1LLPIPS and \u03b1 = 0.01. Training\ndone is on a single NVIDIA A6000 GPU and takes around\n7 days.\nIn addition, for CO3D we use regularisation losses to\nprevent exceedingly large or vanishingly small Gaussians\nfor numerical stability. We regularize large Gaussians with\nthe mean of their activated scale s = exp \u02c6s when it is bigger\nthan a threshold scale sbig = 20.\nLbig = (P\ni si1(si > sbig))/(P\ni 1(si > sbig)).\nSmall Gaussians are regularized with a mean of their\nnegative deactivated scale \u02c6s when it is smaller than a\nthreshold \u02c6ssmall\n=\n\u22125:\nLsmall\n=\n(P\ni \u2212\u02c6si1(\u02c6si\n<\n\u02c6ssmal l))/(P\ni 1(\u02c6si < \u02c6ssmall)).\nD. Implementation details \u2014 PixelNeRF.\nWe use the official PixelNeRF implementation [44]. We use\nthe same preprocessed data as for our method. We modify\nthe activation function of opacity from ReLU to Softplus\nwith the \u03b2 parameter \u03b2 = 3.0 for improved training stabil-\nity. Parametrization of the sampling points to be centered\nabout the ground truth distance to the camera zgt as dis-\ncussed in Appendix B is available as default in the official\nimplementation.\nE. Training resource estimate\nWe compare the compute resources needed at training time\nby noting the GPU used, its capacity, the number of GPUs\nand the number of days needed for training in Tab. 7. We\nreport the compute resources reported in original works,\nwhere available.\nNeRFDiff only reports the resources\nneeded to train their \u2018Base\u2019 models and we were unable to\nobtain more information about their \u2018Large\u2019 models which\nwe compare against in the main paper. We thus report an es-\ntimate of such resources which we obtained by multiplying\nthe number of GPUs used in the \u2018Base\u2019 models by a factor\nof 2. Our method is significantly cheaper than VisionNeRF\nand NeRFDiff. The resources required are similar to those\nInput\nPixelNeRF \n0.54FPS\nOurs \n588FPS\nGT\nVisionNeRF \n0.41FPS\nFigure 7. ShapeNet-SRN. Our method (fourth column) outputs reconstructions which are better than PixelNeRF (second column) and\nmore or equally accurate than VisionNeRF (third column) while rendering 3 orders of magnitude faster (rendering speed in Frames Per\nSecond denoted underneath method name).\nInput\nPixelNeRF \n0.54FPS\nOurs \n588FPS\nGT\nFigure 8. CO3D. Our method (third column) outputs reconstructions which are sharper than PixelNeRF (second column) while rendering\n3 orders of magnitude faster (rendering speed in Frames Per Second denoted underneath method name).\nMethod\nGPU\nMemory\n# GPUs\nDays\nGPU \u00d7 Days\nVisionNeRF\nA100\n80G\n16\n5\n80\nNeRFDiff\nA100\n80G\n16*\n3\n48\nViewDiff\nA40\n48G\n2\n3\n6\nPixelNeRF\nTiRTX\n24G\n1\n6\n6\nOurs\nA6000\n48G\n1\n7\n7\nTable 7. Training resources. Ours, Viewset Diffusion and Pix-\nelNeRF have significantly lower compute costs than VisionNeRF\nand NeRFDiff. Memory denotes the memory capacity of the GPU.\n* denotes estimates.\nof Viewset Diffusion and PixelNeRF, while we achieve bet-\nter performance and do not require absolute camera poses.\nF. Covariance warping implementation\nAs described in Sec. 3.4 in the main paper, the 3D Gaus-\nsians are warped from one view\u2019s reference frame to another\nwith \u02dc\u03a3 = R\u03a3R\u22a4 where R is the relative rotation matrix of\nthe reference frame transformation. The covariance is pre-\ndicted using a 3-dimensional scale and quaternion rotation\nso that \u03a3 = RqSR\u22a4\nq where S = diag (exp(\u02c6s))2. Thus the\nwarping is applied by applying rotation matrix R to the ori-\nentation of the Gaussian \u02dcRq = RRq. In practice this is im-\nplemented in the quaternion space with the composition of\nthe predicted quaternion q and the quaternion representation\nof the relative rotation p = m2q(R) where m2q denotes the\nmatrix-to-quaternion transformation, resulting in \u02dcq = pq.\n"
  },
  {
    "title": "Cached Transformers: Improving Transformers with Differentiable Memory Cache",
    "link": "https://arxiv.org/pdf/2312.12742.pdf",
    "upvote": "11",
    "text": "Cached Transformers: Improving Transformers with Differentiable Memory\nCache\nZhaoyang Zhang1, Wenqi Shao1, Yixiao Ge3, Xiaogang Wang1, Jinwei Gu1, Ping Luo2\n1The Chinese University of Hong Kong 2The University of Hong Kong 3Tencent Inc\nAbstract\nThis work introduces a new Transformer model called\nCached Transformer, which uses Gated Recurrent Cached\n(GRC) attention to extend the self-attention mechanism with\na differentiable memory cache of tokens. GRC attention en-\nables attending to both past and current tokens, increasing the\nreceptive field of attention and allowing for exploring long-\nrange dependencies. By utilizing a recurrent gating unit to\ncontinuously update the cache, our model achieves significant\nadvancements in six language and vision tasks, including lan-\nguage modeling, machine translation, ListOPs, image classi-\nfication, object detection, and instance segmentation. Further-\nmore, our approach surpasses previous memory-based tech-\nniques in tasks such as language modeling and displays the\nability to be applied to a broader range of situations.\nIntroduction\nThe design of Transformer (Vaswani et al. 2017), a deep\nmodel stacking self-attention and feed-forward layers, has\nachieved remarkable progress in various tasks. Compared\nto the traditional deep models, a key characteristic of\nTransformer is the self-attention mechanism, which enables\nglobal receptive field and allows each token to access all the\nother tokens in a data batch, providing a flexible scheme\nto capture contextual representation (Vaswani et al. 2017;\nDosovitskiy et al. 2021; Carion et al. 2020) . Such paradigm\nis however in a complexity square to sequence length, thus\nnot suitable to model long-term dependencies. In this work,\nwe aim to extend the conventional transformer models using\nattention with a long-term token representation in a mem-\nory cache, which enables larger and longer receptive field at\nminimal additional computations.\nCapturing long-range relationships between tokens and\nsamples is crucial for various tasks due to several reasons.\n(i) In sequential data such as language sentences, there can\nexist dependencies between tokens that are far away from\neach other. For example, an event or character can be re-\nferred to from time to time across multiple paragraphs in an\narticle. Failing to capture such dependencies can result in\npoor performance in natural language processing tasks. (ii)\nModeling cross-sample relationships can also be useful for\nnon-sequential data like images. For example, incorporating\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n5\n10\n15\n20\n25\nModel Capacity (G FLOPs)\n76\n78\n80\n82\n84\n Accuracy \nViT\nPVT\nCached PVT (ours)\nPVTv2\nCached PVTv2 (ours)\nSwin\nCvT\nFigure\n1:\nPerformance-Complexity\nCurve:\nTop-1\naccu-\nracy(%) comparisons on ImageNet with respect to model ca-\npacity(G FLOPs) using vision transformers(Parameter-efficiency\ncurves).Curves of our cached models are consistently on top of\ntheir corresponding baselines (PVT and PVTv2), indicating the ef-\nfectiveness of GRC-cached models considering both complexity\nand accuracy.\na memory module that stores prototypical feature represen-\ntations can enable instance-invariant feature learning, lead-\ning to improved performance in vision tasks (Long et al.\n2022; Deng et al. 2022). Furthermore, other studies (Wang\net al. 2020b; Zhong et al. 2019) have demonstrated that us-\ning cross-batch memory to store previous embeddings can\nbe beneficial for visual representation learning. (iii) Longer-\nrange attention has also been shown to enhance the represen-\ntation learning ability of models, as demonstrated in works\nlike (Dai et al. 2019; Wu et al. 2022; Tay et al. 2021b).\nHowever, longer dependency modeling makes computa-\ntions more expensive. For example, the vanilla Transformer\nhas O(T 2) computational complexity in each attention mod-\nule when handling a token sequence of length T. Although\nsome works apply efficient alternatives, such as low-rank de-\ncomposition (Wang et al. 2020a; Zhu et al. 2021), block-\nbased sparsification (Zaheer et al. 2020), and local sensi-\ntive hashing (Kitaev, Kaiser, and Levskaya 2020), they still\nhave complexity linear to the token length (O(T)) and thus\nunable to efficiently capture sparse long-range dependency.\nAnother line of research (Wu et al. 2022) reduces the com-\nplexity of attention module by selecting top-k token pairs\narXiv:2312.12742v1  [cs.CV]  20 Dec 2023\nfrom a memory cache for the current tokens, but the cost\nof maintaining a huge cache of tokens for all layers is still\nsignificant. Hence, developing efficient and effective mech-\nanisms for capturing long-range dependencies remains an\nactive area of research.\nTo address these issues, we propose a novel family of\nTransformer models called Cached Transformer, which has\na Gated Recurrent Cache (GRC) that enables Transform-\ners to access historical knowledge, as ilustrated in Fig. 2.\nThe GRC is implemented as a meta-learner that compresses\nthe historical representation into embedding vectors and up-\ndates them adaptively with a gating mechanism, avoiding\nthe need for a large memory cache. The GRC updates the\npast representation with a reset gate that suppresses histor-\nical caches and an update gate that further updates the sup-\npressed caches using the current token sequences. This de-\nsign allows the GRC to access previously seen knowledge\nin a computationally efficient way. Based on the GRC, we\nimplement a semi-cached attention mechanism that attends\nto both the latent and current tokens.\nWe propose Cached Transformer with Gated Recurrent\nCache (GRC) and make the following contributions, which\nmake it more appealing than prior arts in several aspects.\n\u2022 GRC is built on a general differentiable formulation and\nis compatible with various attention schemes, Trans-\nformer networks, and tasks. We demonstrate that GRC\ncan be easily plugged into diverse Transformer-variants\nsuch as Transformer-XL (Dai et al. 2019), ViT (Doso-\nvitskiy et al. 2021), PVT (Wang et al. 2021, 2022),\nSwin (Liu et al. 2021) Bigbird (Zaheer et al. 2020), and\nReformer (Kitaev, Kaiser, and Levskaya 2020).\n\u2022 GRC can cache all representations of arbitrary length re-\ncurrently, independent of sequence length, while exist-\ning cache-based methods can only capture recent tokens\n(Rae et al. 2019; Dai et al. 2019) or require KNN search-\ning at each step (Wu et al. 2022).\n\u2022 Besides efficiency, GRC surpasses previous memory-\nbased methods (Dai et al. 2019; Burtsev et al. 2020; Bu-\nlatov, Kuratov, and Burtsev 2022) by a large margin on\nboth vision (Table 2) and language tasks (Table 5).\n\u2022 GRC yields consistent improvements not only in sequen-\ntial data such as texts but also in spatial context such as\nimage classification (Table 1) and object detection (Ta-\nble 3). To our knowledge, existing works of Vision Trans-\nformers mainly focused on learning intra-sample tokens,\nwhile GRC is the first attempt to model cross-sample re-\nlationships by attending over inter-sample tokens, such\nas tokens from different independent images.\n\u2022 We observe that models with GRC may attend more\nover the cache than the regular self-attention. We inves-\ntigate this behavior in image classification and find that\nGRC can separate features into two parts, attending over\ncaches yielding instance-invariant features, as well as\nattending over self, yielding instance-specific features\n(See in Fig. 4). This behavior is similar to that of a vec-\ntor prototype (Caron et al. 2020), which enables cross-\nsample regularization to avoid overfitting.\nExtensive experiments show that the Cached Transformer\nwith GRC achieves promising results on various vision\nand language Transformer backbones. (i) Language: In\nthe IWSLT14 De-En benchmark for machine translation,\nPreNormed Transformer+GRC yields 36.0 BLEU, outper-\nforming the baselines by 0.5. In the challenging long-range-\narena benchmark (Tay et al. 2021a), GRC improves state-of-\nthe-art methods with different attention types including Re-\nformer (Kitaev, Kaiser, and Levskaya 2020), Bigbird (Za-\nheer et al. 2020), and regular Transformer (Vaswani et al.\n2017) consistently by up to 1.2% accuracy. (ii) Vision: For\nimage classification on ImageNet (Krizhevsky, Sutskever,\nand Hinton 2012), we plug GRC into the recent vision trans-\nformers of different scales, such as ViT (Dosovitskiy et al.\n2021), PVT (Wang et al. 2021), PVTv2 (Wang et al. 2022),\nSwin (Liu et al. 2021), and obtain up to 3.3% accuracy gain.\nAs shown in Fig. 1, our cached model with PVTv2 backbone\nachieves superior performance considering both the model\ncomplexity and accuracy. We further evaluate GRC on the\nCOCO (Lin et al. 2014) dataset for object detection and in-\nstance segmentation, where PVT+GRC can yield more than\n4.0 box AP improvement.\nRelated works\nCached Language Models.\nCache models are effective in\nlong-range modeling , and are firstly introduced by (Kupiec\n1989; Kuhn and De Mori 1990) for speech recognition. In\ngeneral, a cache model stores representations of the past,\nwhich are usually unigrams or key-value pairs for future\ncomputation. Transformer-XL (Dai et al. 2019) further ap-\nplies this technique to transformers, where the cache stores\nprevious key-value pairs in attentions from previous training\nsteps. Many memory-based methods are explored following\nTransformer-XL: For instance, MT (Burtsev et al. 2020) and\nRMT (Bulatov, Kuratov, and Burtsev 2022) use extra mem-\nory tokens to store local and global information for different\nsegments of inputs. (Rae et al. 2019) compress the tokens\nbefore they\u2019re saved in the cache to reduce memories and\ncomputations. However, these methods often use cache in a\nfixed-length and first-in-first-out (FIFO) manner, which lim-\nits the amount of tokens that can be memorized in sequence.\nIn contrast, our proposed GRC-based Cached Transformers\nlearn to build the cache adaptively with a complexity that is\nindependent of the attention range.\nVision Transformers.\nVision transformers and their vari-\nants have recently achieved remarkable success in various\nvision tasks. The original Vision Transformer (ViT) model\n(Dosovitskiy et al. 2021) was the first to split images into\npatch sequences and feed them into transformer encoders.\nDespite producing competitive results compared to convolu-\ntional neural networks (CNNs), ViTs require costly pretrain-\ning on large-scale datasets like JFT-300M (Sun et al. 2017).\nTo address this issue, several works (Shao et al. 2022) at-\ntribute it to the lack of inductive bias in ViTs and propose\nintroducing convolutional priors to encode inductive bias\nsuch as local context. For example, DeiT (Touvron et al.\n2021b) uses a convolutional teacher to distill knowledge for\nthe transformers, Swin-Transformer (Liu et al. 2021) con-\nducts attention in sliding windows, and ConViT (d\u2019Ascoli\nAttention\nGRC Updates\nCached Attention\nGRC Updates\n......\n......\nAttention\nSelf Attention\n......\nFigure 2: Comparisons of vanilla self-attention and cached atten-\ntions at training stage. The self-attention only attends to the token\nitself (Xt). While in cached attention, outputs at training step t\n(denoted by Yt) are derived by attending over a Gated Recurrent\nCache (GRC, i.e., Ct derived from historical tokens X0 to Xt),\nand the current token (Xt).\net al. 2021) uses a \u201dsoft\u201d convolutional module to encode\nlocality. However, existing methods focus mainly on intra-\nsample tokens, whereas our proposed GRC enhances vision\ntransformers by learning instance-invariant features via at-\ntending over inter-sample tokens. This allows GRC-based\ntransformers to capture richer contextual information and\nachieve even better performance on vision tasks.\nMethodology\nIn this section, we first revisit the vanilla language and vi-\nsion transformer models, then introduce implementation of\nCached Transformers with Gated Recurrent Cache(GRC).\nVanilla Transformer\nWe begin with a brief review of the standard transformer\narchitecture. The transformer model (Vaswani et al. 2017)\nis constructed by stacking multi-head self-attention blocks\nand feed-forward layers which is usually a two-layer lin-\near transformation with activation. Each transformer block\nis fed with T \u00d7 D input tokens, where T is the number of\ntokens and D represents the size of token embedding.\nSelf-attention mechanism. As shown in Fig.2, the self-\nattention module first projects each input X into Q (query),\nK (key), and V (value) using linear transformations. Typi-\ncally, the self-attention is performed in a multi-head manner\nwhere the input will be divided into multiple heads for par-\nallel computation. The output of the attention head h can be\nwritten as :\noh\nself = softmax(QhKT\nh /\np\nD/H)Vh,\n(1)\nwhere oh\nself is the output of head h of the self-attention and\nH is the number of heads. The output from heads will be\nconcatenated and then fed into another linear transforma-\ntions with normalization and residual connections.\nLimitations. As shown in Eqn.(1), the vanilla self-\nattention mechanism used in Transformers is highly sen-\nsitive to sequence length, with a computational complex-\nity of O(T 2) with respect to the sequence length T. This\nmeans that the computational cost grows rapidly as the se-\nquence length increases, which limits the model\u2019s ability\nto capture long-term relationships in the data. As a result,\nvanilla Transformers can only model relatively short se-\nquences of tokens in language tasks, and it also makes it\nchallenging to develop cross-task memory modules (Wang\net al. 2020b; Zhong et al. 2019) in a attention-based way\nfor vision tasks. Towards this issue, we introduce the pro-\nposed Cached Transformers, which provides a more flexible\nparadigm for capturing long-term dependencies, leading to\nconsistent improvements for both vision and language tasks.\nCached Transformer\nTo extend receptive fields of both language and vision trans-\nformers, in this section we will introduce our implementa-\ntions of Cached Transformers, which maintains a continu-\nous cache termed Gated Recurrent Cache (GRC) to support\nefficient long-term representation learning. The core idea\nis to hold token embedding as caches which can dynam-\nically record historical samples according to their signifi-\ncance. The Cached Transformer will then gain additional ca-\npabilities to encode both the current and accumulated infor-\nmation by attending to the gathering of caches C and inputs\nX. Such an attention scheme is described as GRC-Attention,\nand the following parts present more details.\nGeneral implementations. The proposed Cached Trans-\nformers enable attending over caches on arbitrary multi-\nlayers architectures accepting sequential inputs. Typically,\nthe Cached Transformer models can be derived by replac-\ning their self-attention blocks with the proposed GRC-\nAttention. Fig. 3 (b) gives overall illustrations of how the\nGRC-Attention is conducted.\nConsidering input sequence Xt \u2208 RB\u00d7T \u00d7D, where B is\nthe batch size and t denotes training steps, GRC-attention\nattends to both the memory cache and the current tokens.\nWe formulate GRC-attention by\nOh = \u03c3(\u03bbh) \u2217 oh\nmem + (1 \u2212 \u03c3(\u03bbh)) \u2217 oh\nself,\n(2)\nwhere Oh and oh\nmem are the outputs of the GRC-attention\nand Cached attention (i.e., attention over memory cache)\nin the head h, respectively. oh\nself is the output of the self-\nattention in Eqn.(1). Moreover, in Eqn.(2), \u03c3(\u00b7) is the sig-\nmoid function and \u03bbh is a head-wise learnable ratio trading\noff self-attention and Cached attention 1.\nTo construct the triplet key, query and value for Cached\nattention, we choose a portion of Xt as input\n\u00afXt\n\u2208\nRB\u00d7T \u00d7Dm, which is derived by slicing Xt on channel di-\nmension. Note that Dm = rD2 indicates channels used\nfor memorizing the past tokens embedding, where r is the\ncaching ratio. With \u00afXt, the accumulated cache Ct\u22121 will\nthen be updated to Ct according to the GRC update rules\nas shown in Fig. 3. We describe the construction of GRC in\nSec in detail. The Cached attention can be then conducted\n1All of the \u03bbh is initialized to be 0.\n2At most cases we adopt Dm =\nD\n2 to reduce the complexity\nof Cached attention , which means we choose half of the inputs to\nupdate caches\nTokens \nCaches  \nFC\nUpdated caches\nSelf-Attention\nGRC\nUpdates\nCached Attention\n+\nOutputs\n(a) GRC Updates\n(b) GRC-Attention\nReset caches\n: reset gates \n: update gates \nFigure 3: The illustration of proposed GRC-Attention in Cached Transformers. (a) Details of the updating process of Gated Recurrent Cache.\nThe updated cache Ct is derived based on current tokens \u00af\nXt and cache of last step Ct\u22121. The reset gates gr reset the previous cache Ct\u22121\nto reset cache \u02dcCt, and the update gates gu controls the update intensity. (b) Overall pipeline of GRC-Attention. Inputs will attend over cache\nand themselves respectively, and the outputs are formulated as interpolation of the two attention results.\nby using \u00afXt as queries and Ct as keys and values, written\nas:\noh\nmem = softmax( \u00afQh \u00afKT\nh /\np\nDm/H) \u00afVh,\n(3)\nwhere \u00afQh, \u00afKh and \u00afVh are obtained by linear projections of\nh-th head of \u00afXt, Ct and Ct respectively.\nGeneralizations. Note that while we typically formulate\nCached Transformer as a self-attention based model, it can\nalso be an arbitrary transformer variant. In other words,\nthe attention mechanism used to acquire oh\nself and oh\nmem in\nEqn.(2) can be substituted by any other attention-like func-\ntions, such as sparse attentions (Zaheer et al. 2020) or local\nhashing (Kitaev, Kaiser, and Levskaya 2020). Further ex-\nperiments will provide validations of Cached Transformers\non several transformer variants.\nGated Recurrent Cache Update\nThis section describes the formulation and updating of pro-\nposed Gated Recurrent Cache (GRC).\nCache Initialization. The GRC is characterized to be\nfixed-length vectors Ct \u2208 RTm\u00d7Dm. Unlike previous works\nthat formulate cache to be tokens or words directly (Tu et al.\n2018; Dai et al. 2019), GRC embeds historical tokens im-\nplicitly. By learning to embed arbitrary length samples into\nCt, GRC allows traversing caches in constant time that is\nindependent of the number of memorized tokens. The cache\nC0 will be initialized to be Tm-length zero vectors before\ntraining, and then updated as depicted in Fig. 3(a).\nGating Mechanism. Inspired by gated RNNs (Cho et al.\n2014), we adopt the gating mechanism to enable GRC to\ndynamically capture dependencies at different time scales.\nSpecifically, the updating process of Ct is filtered by update\ngates gu and reset gates gr. Considering updating GRC at\ntime step t, we first calculate the gates gu and gr:\ngu = \u03c3(Wu[ \u00afXt, Ct\u22121]) and gr = \u03c3(Wr[ \u00afXt, Ct\u22121]), (4)\nwhere \u03c3 denotes sigmoid function and [\u00b7, \u00b7] concatenates to-\nkens in channel dimension. For valid concatenation, \u00afXt is\ninterpolated into a Tm-by-Dm token. The updated cache Ct\nis formulated by a linear interpolation as given by:\nCt = (1 \u2212 gu)Ct\u22121 + gu \u02dcCt and \u02dcCt = Wc[ \u00afXt, gr \u2299 Ct\u22121]\n(5)\nwhere \u2299 is element-wise multiplication. In above process,\nthe update gates gu decides how much current sample \u00afXt\nupdates the cache and the reset gates gr suppress the accu-\nmulated cache to forget unimportant components. Note that\nshape of the derived Ct is B \u00d7 Tm \u00d7 Dm as Xt is involved,\nand we therefore average across the batch dimension to fit\nthe cache size.\nExperiments\nThis section extensively evaluates the effectiveness of the\nproposed Cached Transformer and Gated Recurrent Cache\n(GRC) in both vision and language tasks, including language\nmodeling on WikiText-103, Long Listops of Long Range\nArena (Tay et al. 2021a), machine translation on IWSLT14\n(Cettolo et al. 2014) / IWSLT15 (Cettolo et al. 2015), im-\nage classification on ImageNet (Krizhevsky, Sutskever, and\nHinton 2012), and object detection and instance segmenta-\ntion on COCO2017 (Lin et al. 2014). In addition, as the\ncached models are newly introduced to vision transformers,\nwe also perform thorough discussions on the role of the pro-\nposed caches and their significance. All of the experiments\nare conducted on Tesla V100 GPUs.\nImage Classification\nExperiments Setup.\nWe first evaluate our methods on\nImagenet-1k for image classification. We implement our\nGRC-Attention as a general pytorch module which main-\ntains fixed-length buffers as cache. In image classification\ntask, we set the cache ratio r to be 0.5 and keep cache length\nTm equal to the length of image patches T. For fair com-\nparisons, we directly replace the self-attention layers in cor-\nresponding transformers with our GRC-Attention module\nwithout varying the architecture and hyperparameters. To\nmaintain spatial token structures, we add positional encod-\nings to our proposed GRC-Attention like other vision trans-\nformers. Both the baselines and their cached counterparts are\nInputs\nSelf-Attention \nCached Attention \nFigure 4: Visualizations of averaged features output from self-attention and cached attention, which is obtained by feeding images of\nImageNet validation sets to trained cached ViT-S. The results are obtained by averaging features over channel(and head) dimension. Both\n\u00afoself and \u00afomem are unflattened to 14 \u00d7 14 for better comparisons. Dark pixels mean small values.\nTable 1: Performance of various Cached Transformers evaluated\non ImageNet. \u201d(Cached)\u201d indicates models implemented with the\nproposed GRC-Attention. Top-1 / Top-5 / \u2206 Top-1 denotes top-1\naccuracy / top-5 accuracy / top-1 accuracy difference respectively.\nThe cached models outperform their corresponding baselines con-\nsistently.\nArchitecture\nTop-1 (%)\nTop-5 (%)\n\u2206 Top-1 (%)\nViT-S\n79.9\n95.0\n-\nViT-S (Cached)\n81.3\n95.5\n+ 1.4\nPVT-Tiny\n75.1\n92.3\n-\nPVT-Tiny (Cached)\n78.4\n94.2\n+ 3.3\nPVT-Small\n79.9\n95.0\n-\nPVT-Small (Cached)\n81.8\n95.9\n+ 1.9\nPVT-Medium\n81.2\n95.7\n-\nPVT-Medium (Cached)\n83.0\n96.4\n+ 1.8\nSwin-T\n81.2\n95.5\n-\nSwin-T (Cached)\n82.1\n95.9\n+ 0.9\nPVTv2-B2\n82.0\n95.9\n-\nPVTv2-B2 (Cached)\n82.6\n96.2\n+ 0.6\nPVTv2-B\n83.2\n96.3\n-\nPVTv2-B3 (Cached)\n83.7\n96.4\n+ 0.5\nPVTv2-B4\n83.6\n96.3\n-\nPVTv2-B4 (Cached )\n84.1\n96.6\n+ 0.5\ntrained with 224 \u00d7 224 size inputs using 16 GPUs. To fully\nvalidate the proposed cache mechanism, we evaluate GRC-\nAttention on four recent vision transformers including: ViTs\n(Dosovitskiy et al. 2021), PVT (Wang et al. 2021), Swin-\nTransformer (Liu et al. 2021) and PVT-v2 (Wang et al.\n2022). Without bells and whistles, all of the training settings\nfor cached models are kept consistent with the original base-\nlines including data augmentation, optimizer type, learning\nrates and training epochs.\nClassification Results.\nTable 1 reports overall perfor-\nmance of cached transformers on corresponding baselines.\nAs shown, transformers implemented with GRC-Attention\nconsistently outperform their no-cache counterparts by\nyielding significantly higher accuracy, demonstrating the ef-\nfectiveness of our proposed caching mechanism. For in-\nstance, by enabling cache, PVT-Tiny can achieve 78.4% top-\n1 accuracy and 94.2% top-5 accuracy, surpassing the orig-\ninal PVT-Tiny by 3.3% and 1.9% respectively. Moreover,\neven for the recent stronger backbone PVTv2, our proposed\ncached mechanism can still keep > 0.5 top-1 improvements.\nComplexity Analysis.\nIn current settings where cache ra-\ntio r = 0.5, replacing all the attention layers with GRC-\nAttention will cost approximately an extra 10% \u2212 15%\nFLOPs and Params. Considering the performance improve-\nments, the extra computations are acceptable (See in Fig. 1)\nand more efficient than increasing the depth and width of\nmodels.\nSignificance of Cached Attention.\nTo verify that the\nabove performance gains mainly come from attending over\ncaches, we analyze the contribution of omem by visualizing\nthe learnable attention ratio \u03c3(\u03bbh). Please be reminded that\nin Eq 2, outputs of GRC-Attention is derived by interpolat-\ning outputs of cached attention oh\nmem and self-attention oh\nself\naccording to \u03c3(\u03bbh). Hence, \u03c3(\u03bbh) can be used to represent\nthe relative significance of oh\nmem and oh\nself. Fig. 5 depicts\nthe learned \u03c3(\u03bbh) for each head respect to layers in ViT-S,\nPVT-Tiny and PVT-Small. As we can see, for more than half\nof the layers, \u03c3(\u03bbh) is larger than 0.5, denoting that outputs\nof those layers are highly dependent on the cached attention.\nBesides, we also notice an interesting fact that the models al-\nways prefer more cached attention except for the last several\nlayers. This makes us curious about the roles of cached at-\ntention: what is the feature that models actually learn by at-\ntending over caches? The following paragraph answers this\nquestion.\nRoles of Cached Attention.\nWe investigate the function\nof GRC-Attention by visualizing their interior feature maps.\nWe choose the middle layers of cached ViT-S, averaging\nthe outputs from self-attention oself and cached attention\n(omem) across the head and channel dimension, and then\nnormalizing them into [0, 1]. The corresponding results are\ndenoting as \u00afoself and \u00afomem, respectively. Fig. 4 provides\nvisualizations of \u00afoself and \u00afomem obtained by feedings im-\nages of ImageNet validation sets to trained cached ViT-S.\nAs \u00afoself and \u00afomem are sequences of patches, they are unflat-\ntened to 14\u00d714 shape for better comparison. From Fig. 4 we\ncan see, features derived by the above two attentions are vi-\nsually complementary. In GRC-Attention, omem is derived\nby attending over the proposed cache (GRC) containing\ncompressive representations of historical samples, and thus\nFigure 5: Visualizations of learned \u03c3(\u03bbh) for each head with respect to layer number (from shallow to deep) in different models: ViT-S,\nPVT-Tiny and PVT-Small. Note that the ViT-S has 6 heads for all the layers, while PVT-Tiny and PVT-Small adopt a progressive head\nstrategy where head numbers increase from 1 to 8 gradually. Circles with different colors denote those different heads. \u03c3(\u03bbh) controls the\ninterpolation ratio of cached attention outputs omem which reflects head-wise contributions of cached attention to the final outputs. Note that\n\u03c3(\u03bbh) > 0.5 means cached attention contributes more than self-attention. As shown, in all of the three models, \u03c3(\u03bbh) > 0.5 holds for more\nthan half of the GRC-Attention layers, deducing that the model outputs are significantly dependent on the cache.\nTable 2: Performance(Top-1 Accuracy) comparisons of cached\nmodels using GRC and attention-based\nModel\nNo cache\nAttention-based cache\nGRC\nViT-S\n79.9\n80.0\n81.3\nPVT-Tiny\n75.1\n74.8\n78.4\nPVT-Small\n79.9\n79.6\n81.8\nbeing adept in recognizing public and frequently showing-\nup patches of this class. While for oself from self-attention\nbranch, it can focus on finding out more private and charac-\nteristic features of current instance.\nWith above postulates, we can attempt to explain the reg-\nularity of \u03c3(\u03bbh) in Fig. 5: employing more omem (larger\n\u03c3(\u03bbh) ) in former layers can help the network to distinguish\nthis instance coarsely, and employing more oself (smaller\n\u03c3(\u03bbh)) enable the model to make fine-grained decision.\nCross-sample regularization.\nThe above paragraph also\nshows that our proposed cache performs similarly to vector\nprototypes (Caron et al. 2020), storing public features of\nthe same class implicitly and allowing models to classify in-\nputs with both the public and characteristic representations.\nIn such a way, the predictions are not only dependent on the\ncurrent inputs but also on related cached samples, thus pro-\nviding a cross-sample regularization to avoid overfitting.\nGRC v.s. other memory-based methods.\nWe perform\nfurther ablations to compare GRC and attention-based\nmemory for image classification in ImageNet-1k. We de-\nploy Transformer-XL-style caches to Vision Transform-\ners(including ViT-S, PVT-Tiny and PVT-Small) and com-\npare them to corresponding GRC-cached models. As shown\nin Table 2, GRC-cached models consistently outperform\ntheir attention-based cache and no-cache counterparts. Be-\nsides, it can be noted that the attention-based cache can\nhardly improve the model performance.\nObject Detection and Instance Segmentation.\nExperiments Setup.\nWe further assess the generalization\nof our GRC-Attention on object detection / instance segmen-\ntation track using COCO2017 dataset (Lin et al. 2014). The\nmodels are trained on the COCO train2017 (118k images)\nand evaluated on val2017 (5k images). We use the cached\nTable 3: Object detection and instance segmentation performance\non COCO val2017 following Mask R-CNN 1\u00d7 settings.\nArchitecture\nAPb\nAPb\n50\nAPb\n75\nAPm\nAPm\n50\nAPm\n75\nPVT-Tiny\n36.7\n59.2\n39.3\n35.1\n56.7\n37.3\n+ Cached\n41.0 (+ 4.6)\n63.4\n44.8\n38.3 (+ 3.2)\n60.4\n41.1\nPVT-Small\n40.4\n62.9\n43.8\n36.3\n60.1\n40.3\n+ Cached\n44.5 (+ 4.1)\n67.1\n48.6\n41.0 (+ 4.7)\n64.0\n44.1\nPVT-Medium\n42.0\n64.4\n45.6\n39.0\n61.6\n42.1\n+ Cached\n46.6 (+ 4.6)\n68.2\n51.0\n42.3 (+ 3.3)\n65.3\n45.5\nPVT as backbone and adopt the Mask R-CNN detector (He\net al. 2017) to verify the effectiveness of GRC-Attention.\nThe standard COCO metrics of Average Precision (AP) for\nbounding box detection (APbb) and instance segmentation\n(APm) are used to evaluate our methods. All of the train-\ning settings and hyperparameters are kept the same as PVT\noriginal implementation (Wang et al. 2021), and all of the\ninvolved models are trained for 12 epochs using 8 GPUs.\nFor both the cached PVT and baselines, backbones are firstly\npretrained on ImageNet and then fine-tuned for detection.\nResuts.\nAs shown in Table 3, when using Mask R-CNN\nfor object detection, the cached PVTs significantly outper-\nform their baselines. For example, the AP of cached PVT-\nMedium is 4.6 (46.6 vs. 42.0) points better than its no-cache\ncounterparts. Similar results can also be found in instance\nsegmentation results, where cached PVT-Medium achieves\n3.3 higher APm (39.0 vs. 42.3). These results demonstrate\nthe generalizability of the proposed caching mechanism.\nLanguage Modeling\nExperimental Setup\nIn this work, we conduct exper-\niments to compare the performance of Gated Recurrent\nCache (GRC) with Transformer-XL (Dai et al. 2019) on a\nlanguage modeling task using the WikiText-103 benchmark.\nTo implement GRC-cached language models, we use the\npublicly available fairseq framework and follow the default\nmemory-based Transformer-XL configurations as our base-\nlines, including model architecture and training settings. To\nensure a fair comparison, we compare GRC-cached mod-\nels with two other memory-based methods, Memory Trans-\nfomer (MT)\n(Burtsev et al. 2020) and Recurrent Mem-\nory Transformer (RMT)\n(Bulatov, Kuratov, and Burtsev\n2022). We implement GRC-cached models by replacing the\nTable 4: Neural machine translation results using Pre-Norm Transformers in terms of BLEU scores.\nArchitecture\nIWSLT14\nIWSLT15\nDe-En\nEs-En\nEn-Fr\nDe-En\nEn-Vi\nCs-En\nTransformer\n35.5\n41.4\n41.5\n36.1\n29.8\n28.8\nTransformer (GRC-cached)\n36.0(+ 0.5)\n41.8(+ 0.4)\n41.7(+ 0.2)\n36.3(+ 0.2)\n30.2(+ 0.4)\n29.4(+ 0.6)\nTable 5: Comparison of performance(Test PPL) for GRC and other\nMemory-based methods (Burtsev et al. 2020; Bulatov, Kuratov, and\nBurtsev 2022) on WikiText-103. The smaller is better. GRC out-\nperform Transformer-XL and previous memory-based methods for\nlanguage modeling by a large margin of 1.1 PPL.\nArchitecture\nbaseline\nMT-cached\nRMT-cached\nGRC-cached\nTransformer-XLbase\n24.0\n23.99\n23.95\n22.9\nTransformer-XLlarge\n18.3\n-\n-\n17.9\ncaching scheme with the GRC approach while keeping all\ndata augmentation and hyper-parameters unchanged for a\nmore fair comparison.\nComparison to Other Memory-Based Methods\nWe\npresent the performance of GRC-cached models com-\npared to Transformer-XL baselines and other memory-based\nmethods in Table 5. The results show that GRC-cached mod-\nels outperform Transformer-XL and other memory-based\nmethods in terms of perplexity on both base and large-scale\nmodels. For instance, GRC-cached Transformer-XLbase\nachieves up to 1.1 lower PPL compared to the baseline\nTransformer-XL and 1.05 lower PPL to the RMT, demon-\nstrating the superiority of GRC over previous memory-based\nTransformer methods.\nLong Range Arena\nExperiments Setup. We extensively conduct experiments\non recently proposed Long Range Arena (LRA) benchmarks\n(Tay et al. 2021a) to validate our proposed methods under\nthe long-context scenario. To demonstrate the long-range se-\nquence modeling capability of GRC-Attention and the cor-\nresponding cache mechanism, we choose the challenging\nLong ListOps task in LRA, which is a longer variation of\nListOps task\n(Nangia and Bowman 2018) with up to 2k\nlength sequences and considerablely difficult. In this task,\nwe also extend GRC-Attention to efficient attention variants\nby replacing the self-attention function (See section ). Con-\ncretely, we compare GRC-Attention to their no-cache coun-\nterparts on baselines including Transformer (Vaswani et al.\n2017), BigBird (Zaheer et al. 2020) and Reformer (Kitaev,\nKaiser, and Levskaya 2020). For those efficient attentions\nlike BigBird and Reformer, we only import gated recurrent\ncache and maintain their inner attention function unchanged.\nAll of the experiments are under default settings in (Tay\net al. 2021a).\nResults.\nTable 6 reports Long ListOps results. As shown,\ncached models consistently outperform their baselines (in-\ncluding the SOTA methods Reformer) significantly. For in-\nstance, by employing GRC, BigBird model can achieve 1.39\nhigher accuracy. These results show the long-range sequence\nmodeling ability of GRC as well as its generalizability to\nother attention variants.\nTable 6: Results on Long ListOPs task in LRA in terms of ac-\ncuracy. The \u201dcached\u201d column indicates cached models whose at-\ntention layers are implemented as generalized GRC-Attention. \u2206\ndenotes the difference between proposed cached models and base-\nlines.\nArchitecture\nbaseline\nGRC-cached\n\u2206\nTransformer\n36.23\n37.40\n+ 1.17\nBigBird\n36.06\n37.45\n+ 1.39\nReformer\n37.27\n37.85\n+ 0.58\nNeural Machine Translation\nExperiments Setups.\nWe experiment our methods on\nwidely used public datasets IWSLT14 and IWSLT15. Mul-\ntiple language sources3are included to fully verify effective-\nness of the proposed GRC, and models are trained for each\ntrack individually. We adopt the Pre-Norm Transformer set-\ntings in (Wang et al. 2019) and implement the models using\nfairseq-py (Ott et al. 2019) framework. Following (Wang\net al. 2019; Ott et al. 2019), we generally increase the learn-\ning rates by 2 and average the last 10 checkpoints for in-\nference. We employ the proposed GRC-cached models by\nreplacing all attention modules of transformer encoder lay-\ners with GRC-Attention. The cache length Tm is set to be\n64 for all cached models. All the transformers in this task\nare using six encoder layers and six decoder layers. For a\nfair comparison, both the baselines and cached models are\ntrained under identical settings.\nResults.\nWe use BLEU (Papineni et al. 2002) as evalua-\ntion metrics and compare GRC cached transformers to their\nbaselines in Table 4. It can be seen that consistent improve-\nments can be reached by applying GRC-Attention to base-\nlines. For tracks like IWSLT14 De-En and IWSLT15 Cs-En,\nthe increments can achieve 0.5/0.6 points, which is actually\nsignificant for these tasks.\nDiscussion\nWe introduce Cached Transformer with Gated Recurrent\nCache (GRC), a simple extension to Transformer-based\nmodels that significantly increases the length of attention\ncontext by allowing access to historical states through a gat-\ning mechanism. GRC embeds previous tokens, whether they\nare close or distant, as fixed-length vectors, without com-\nplexity dependence on the number of cached tokens. Con-\nsequently, GRC model token dependencies over a broader\nrange of input, resulting in improved accuracy and perfor-\nmance across diverse Transformers-variants with different\narchitectures and attention functions, on a variety of vision\nand language tasks.\n3IWSLT14: German-English(De-En), Spanish-English(Es-En)\nand English-French(En-Fr), IWSLT15: German-English(De-En),\nEnglish-Vietnamese(En-Vi) and Czech-English(Cs-En)\nReferences\nAinslie, J.; Ontanon, S.; Alberti, C.; Cvicek, V.; Fisher, Z.; Pham,\nP.; Ravula, A.; Sanghai, S.; Wang, Q.; and Yang, L. 2020. ETC: En-\ncoding long and structured inputs in transformers. arXiv preprint\narXiv:2004.08483.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The\nlong-document transformer. arXiv preprint arXiv:2004.05150.\nBrahma, S. 2018. Improved language modeling by decoding the\npast. arXiv preprint arXiv:1808.05908.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhari-\nwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al.\n2020. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33: 1877\u20131901.\nBulatov, A.; Kuratov, Y.; and Burtsev, M. 2022. Recurrent memory\ntransformer. Advances in Neural Information Processing Systems,\n35: 11079\u201311091.\nBurtsev, M. S.; Kuratov, Y.; Peganov, A.; and Sapunov, G. V. 2020.\nMemory transformer. arXiv preprint arXiv:2006.11527.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.;\nand Zagoruyko, S. 2020. End-to-end object detection with trans-\nformers. In European conference on computer vision, 213\u2013229.\nSpringer.\nCaron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.; and\nJoulin, A. 2020. Unsupervised learning of visual features by con-\ntrasting cluster assignments. Advances in Neural Information Pro-\ncessing Systems, 33: 9912\u20139924.\nCettolo, M.; Niehues, J.; St\u00a8uker, S.; Bentivogli, L.; Cattoni, R.; and\nFederico, M. 2015. The IWSLT 2015 Evaluation Campaign. In\nProceedings of the 12th International Workshop on Spoken Lan-\nguage Translation: Evaluation Campaign, 2\u201314. Da Nang, Viet-\nnam.\nCettolo, M.; Niehues, J.; St\u00a8uker, S.; Bentivogli, L.; and Federico,\nM. 2014. Report on the 11th IWSLT evaluation campaign. In Pro-\nceedings of the 11th International Workshop on Spoken Language\nTranslation: Evaluation Campaign, 2\u201317. Lake Tahoe, California.\nCho, K.; Van Merri\u00a8enboer, B.; Bahdanau, D.; and Bengio, Y. 2014.\nOn the properties of neural machine translation: Encoder-decoder\napproaches. arXiv preprint arXiv:1409.1259.\nChoromanski, K.; Likhosherstov, V.; Dohan, D.; Song, X.; Gane,\nA.; Sarlos, T.; Hawkins, P.; Davis, J.; Mohiuddin, A.; Kaiser, L.;\net al. 2020. Rethinking attention with performers. arXiv preprint\narXiv:2009.14794.\nDai, Z.; Yang, Z.; Yang, Y.; Carbonell, J.; Le, Q. V.; and Salakhut-\ndinov, R. 2019. Transformer-xl: Attentive language models beyond\na fixed-length context. arXiv preprint arXiv:1901.02860.\nDeng, W.; Marsh, J.; Gould, S.; and Zheng, L. 2022. Fine-Grained\nClassification via Categorical Memory Networks. IEEE Transac-\ntions on Image Processing, 31: 4186\u20134196.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert:\nPre-training of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai,\nX.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.;\nGelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth\n16x16 Words: Transformers for Image Recognition at Scale. In\nInternational Conference on Learning Representations.\nd\u2019Ascoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.; Biroli, G.;\nand Sagun, L. 2021. Convit: Improving vision transformers with\nsoft convolutional inductive biases. In International Conference on\nMachine Learning, 2286\u20132296. PMLR.\nGrave, E.; Joulin, A.; and Usunier, N. 2016.\nImproving neu-\nral language models with a continuous cache.\narXiv preprint\narXiv:1612.04426.\nHe, K.; Gkioxari, G.; Doll\u00b4ar, P.; and Girshick, R. 2017. Mask r-cnn.\nIn Proceedings of the IEEE international conference on computer\nvision, 2961\u20132969.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 770\u2013778.\nKhandelwal, U.; Levy, O.; Jurafsky, D.; Zettlemoyer, L.; and\nLewis, M. 2019. Generalization through memorization: Nearest\nneighbor language models. arXiv preprint arXiv:1911.00172.\nKitaev, N.; Kaiser, \u0141.; and Levskaya, A. 2020. Reformer: The effi-\ncient transformer. arXiv preprint arXiv:2001.04451.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet\nclassification with deep convolutional neural networks. Advances\nin neural information processing systems, 25.\nKuhn, R.; and De Mori, R. 1990. A cache-based natural language\nmodel for speech recognition. IEEE transactions on pattern anal-\nysis and machine intelligence, 12(6): 570\u2013583.\nKupiec, J. 1989. Probabilistic models of short and long distance\nword dependencies in running text. In Speech and Natural Lan-\nguage: Proceedings of a Workshop Held at Philadelphia, Pennsyl-\nvania, February 21-23, 1989.\nLin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Doll\u00b4ar, P. 2017.\nFocal loss for dense object detection. In Proceedings of the IEEE\ninternational conference on computer vision, 2980\u20132988.\nLin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan,\nD.; Doll\u00b4ar, P.; and Zitnick, C. L. 2014. Microsoft coco: Common\nobjects in context. In European conference on computer vision,\n740\u2013755. Springer.\nLiu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and\nGuo, B. 2021. Swin transformer: Hierarchical vision transformer\nusing shifted windows. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 10012\u201310022.\nLong, A.; Yin, W.; Ajanthan, T.; Nguyen, V.; Purkait, P.; Garg, R.;\nBlair, A.; Shen, C.; and van den Hengel, A. 2022. Retrieval aug-\nmented classification for long-tail visual recognition. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 6959\u20136969.\nLoshchilov, I.; and Hutter, F. 2018. Decoupled Weight Decay Reg-\nularization. In International Conference on Learning Representa-\ntions.\nNangia, N.; and Bowman, S. R. 2018. Listops: A diagnostic dataset\nfor latent tree learning. arXiv preprint arXiv:1804.06028.\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grang-\nier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible Toolkit for\nSequence Modeling. In NAACL-HLT (Demonstrations).\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a\nMethod for Automatic Evaluation of Machine Translation. In ACL.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018.\nImproving language understanding by generative pre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.;\net al. 2019. Language models are unsupervised multitask learners.\nOpenAI blog, 1(8): 9.\nRae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap, T. P.\n2019.\nCompressive transformers for long-range sequence mod-\nelling. arXiv preprint arXiv:1911.05507.\nRamachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Levskaya,\nA.; and Shlens, J. 2019. Stand-alone self-attention in vision mod-\nels. Advances in Neural Information Processing Systems, 32.\nRoy, A.; Saffar, M.; Vaswani, A.; and Grangier, D. 2021. Efficient\ncontent-based sparse attention with routing transformers. Transac-\ntions of the Association for Computational Linguistics, 9: 53\u201368.\nShao, W.; Ge, Y.; Zhang, Z.; XU, X.; Wang, X.; Shan, Y.; and Luo,\nP. 2022. Dynamic Token Normalization improves Vision Trans-\nformers. In International Conference on Learning Representations.\nSukhbaatar, S.; Ju, D.; Poff, S.; Roller, S.; Szlam, A.; Weston, J.;\nand Fan, A. 2021. Not all memories are created equal: Learning to\nforget by expiring. In International Conference on Machine Learn-\ning, 9902\u20139912. PMLR.\nSun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Revisiting\nunreasonable effectiveness of data in deep learning era. In Pro-\nceedings of the IEEE international conference on computer vision,\n843\u2013852.\nTay, Y.; Dehghani, M.; Abnar, S.; Shen, Y.; Bahri, D.; Pham, P.;\nRao, J.; Yang, L.; Ruder, S.; and Metzler, D. 2021a. Long Range\nArena : A Benchmark for Efficient Transformers. In International\nConference on Learning Representations.\nTay, Y.; Dehghani, M.; Aribandi, V.; Gupta, J.; Pham, P. M.; Qin,\nZ.; Bahri, D.; Juan, D.-C.; and Metzler, D. 2021b. Omninet: Om-\nnidirectional representations from transformers. In International\nConference on Machine Learning, 10193\u201310202. PMLR.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.;\nand J\u00b4egou, H. 2021a. Training data-efficient image transformers\n& distillation through attention. In International Conference on\nMachine Learning, 10347\u201310357. PMLR.\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and J\u00b4egou,\nH. 2021b. Going deeper with image transformers. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision,\n32\u201342.\nTu, Z.; Liu, Y.; Shi, S.; and Zhang, T. 2018. Learning to remember\ntranslation history with a continuous cache. Transactions of the\nAssociation for Computational Linguistics, 6: 407\u2013420.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is\nall you need. Advances in neural information processing systems,\n30.\nWang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; and Chao,\nL. S. 2019. Learning deep transformer models for machine trans-\nlation. arXiv preprint arXiv:1906.01787.\nWang, S.; Li, B. Z.; Khabsa, M.; Fang, H.; and Ma, H. 2020a.\nLinformer: Self-attention with linear complexity. arXiv preprint\narXiv:2006.04768.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.; Lu, T.;\nLuo, P.; and Shao, L. 2021. Pyramid vision transformer: A versa-\ntile backbone for dense prediction without convolutions. In Pro-\nceedings of the IEEE/CVF International Conference on Computer\nVision, 568\u2013578.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.; Lu,\nT.; Luo, P.; and Shao, L. 2022. PVT v2: Improved baselines with\nPyramid Vision Transformer. Computational Visual Media, 1\u201310.\nWang, X.; Zhang, H.; Huang, W.; and Scott, M. R. 2020b. Cross-\nbatch memory for embedding learning.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 6388\u20136397.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.; and\nZhang, L. 2021. Cvt: Introducing convolutions to vision transform-\ners. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 22\u201331.\nWu, Y.; Rabe, M. N.; Hutchins, D.; and Szegedy, C. 2022. Memo-\nrizing transformers. arXiv preprint arXiv:2203.08913.\nYuan, L.; Chen, Y.; Wang, T.; Yu, W.; Shi, Y.; Jiang, Z.-H.; Tay,\nF. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Training\nvision transformers from scratch on imagenet. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 558\u2013\n567.\nZaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Alberti, C.;\nOntanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang, L.; et al. 2020.\nBig bird: Transformers for longer sequences. Advances in Neural\nInformation Processing Systems, 33: 17283\u201317297.\nZhong, Z.; Zheng, L.; Luo, Z.; Li, S.; and Yang, Y. 2019.\nIn-\nvariance matters: Exemplar memory for domain adaptive person\nre-identification. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 598\u2013607.\nZhu, C.; Ping, W.; Xiao, C.; Shoeybi, M.; Goldstein, T.; Anandku-\nmar, A.; and Catanzaro, B. 2021. Long-short transformer: Efficient\ntransformers for language and vision. Advances in Neural Infor-\nmation Processing Systems, 34.\nFull Literature Reviews\nLanguage Transformers. Transformer is firstly introduced\nin language processing by ((Vaswani et al. 2017)), and a\ngreat deal of work has been done to improve it. The most\ninfluential works of language transformers are GPT (Rad-\nford et al. 2018, 2019; Brown et al. 2020) and BERT (Devlin\net al. 2018). The GPT/BERT family works in a \u2018pretraining-\nfinetuning\u2019 fashion, achieving state-of-art performance on\nvarious language benchmarks. But they are also expensive\non both hardware and energy. Another series of methods im-\nprove the vanilla transformers and also seek for trade-off\nbetween performance and efficiency. To cover long-range\ninformation in language, several efficient attention tech-\nniques are proposed such as kernel approximation (Wang\net al. 2020a; Choromanski et al. 2020), sparsification (Za-\nheer et al. 2020; Roy et al. 2021) and local hashing (Kitaev,\nKaiser, and Levskaya 2020) ). Rather than these lightweight\nattentions, other works attempt to apply attention selectively\nwith predefined rules like sliding windows (Beltagy, Peters,\nand Cohan 2020), hierarchical architecture (Ainslie et al.\n2020) and token pruning (Sukhbaatar et al. 2021).\nCache Language Models and Memory-based Meth-\nods. Cache models are effective in long-range modeling\n, and are firstly introduced by\n(Kupiec 1989; Kuhn and\nDe Mori 1990) for speech recognition. In general, a cache\nmodel stores representations of the past, which are usually\nunigrams or key-value pairs for future computation. (Grave,\nJoulin, and Usunier 2016) extend such kinds of methods\nto neural language models(RNNs), where cache stores the\nmost-recent pairs of inputs and hidden states, and (Brahma\n2018) then improves neural cache models by decoding the\npast tokens as a regularization. Transformer-XL (Dai et al.\n2019) further applies this technique to transformers, where\nthe cache stores previous key-value pairs in attentions from\nprevious training steps.\nMany memory-based methods are explored following\nTransformer-XL: For instance, MT (Burtsev et al. 2020) and\nRMT (Bulatov, Kuratov, and Burtsev 2022) use extra mem-\nory tokens to store local and global information for different\nsegments of inputs. (Rae et al. 2019) compress the tokens\nbefore they\u2019re saved in the cache to reduce memories and\ncomputations. In addition to general representations, some\nworks also store task-specific information in cache to im-\nprove performance. For instance, (Tu et al. 2018) proposes\nto enhance neural machine translation models by remem-\nbering translation history. However, these methods often use\ncache in a fixed-length and first-in-first-out (FIFO) manner,\nwhich limits the amount of tokens that can be memorized in\nsequence.\nTo address this issue, recent memory-based work (Khan-\ndelwal et al. 2019; Wu et al. 2022) proposes to store key-\nvalue pairs in a large cache without compression and per-\nform K-nearest neighbor (KNN) lookup to search over them.\nWhile this approach yields competitive results in language\nmodeling, it still requires a large memory footprint and sig-\nnificant time for searching, particularly for longer attention\nranges. In contrast, our proposed GRC-based Cached Trans-\nformers learn to build the cache adaptively with a complex-\nity that is independent of the attention range.\nVision Transformers. Vision transformers (and variants)\nhave achieved great success in various vision tasks recently.\nViTs (Dosovitskiy et al. 2021) firstly propose to split im-\nages into patch sequences and feed them into transformer\nencoders. Although yielding competitive results to CNNs,\nViTs have an issue requiring costly pretraining on large-\nscale datasets like JFT-300M (Sun et al. 2017). Many works\n(Shao et al. 2022) ascribe this to the lack of inductive bias\nand propose to introduce convolutional priors to ViTs to en-\ncode inductive bias like local context. For example, DeiT\n(Touvron et al. 2021b) use a convolution teachers to dis-\ntill knowledge for the transformers, Swin-Transformer (Liu\net al. 2021) conducts attention in sliding windows, and Con-\nViT (d\u2019Ascoli et al. 2021) use a \u201dsoft\u201d convolutional mod-\nule to encode locality. Moreover, other methods like PVT\n(Wang et al. 2022), T2T (Yuan et al. 2021), and CVT (Wu\net al. 2021) further improve vision transformers by import-\ning convolutional priors in CNNs (He et al. 2016). Differ-\nent from existing methods that focus on intra-sample tokens,\nthe proposed GRC further enhances vision transformers by\nmodeling dependencies of inter-sample tokens.\nImplementation Details\nTraining and Inference Algorithms\nAlgorithm 1 provides detailed produces of the proposed\nGRC-Attention in a forward pass. During training, each\nGRC-Attention module maintains a continuous cache Ct,\nwhich will be updated at each iteration. Note that all the\ncomputations involved in GRC-Attention are differentiable\nand corresponding parameters can thus be optimized us-\ning gradient-based methods. The accumulated caches Ct are\nstored with network parameters after training, and will be\nused directly for inference without any further updating.\nOther implementation details.\nGRC supports end-to-end training with other network pa-\nrameters using gradient-based methods. The cache Ct will\nbe saved as buffers together with trained models and used\nfor evaluation. Similar to training statistics in batch normal-\nization, Ct will be freezed at inference time and thus require\nno updating. We also notice that some tasks accept varying\nlength inputs (varying T, like object detection or machine\ntranslation). In such cases, we will interpolate valid input\nsequences(without zero-padding) to the fixed cache length\nTm and then continue cache updating.\nExperimental details\nImage classification on ImageNet. We evaluate the per-\nformance of GRC-Attention using various vision transform-\ners((including ViTs, PVT, Swin, and PVTv2)) on ImageNet-\n1k(Krizhevsky, Sutskever, and Hinton 2012), which consists\nof 1.28M training images and 50K validation validation im-\nages from 1K classes. For each baseline, we implement their\ncached variants by replacing all of their self-attention layers\nwith GRC-Attention directly, keeping their architectures un-\nchanged. By default, the cache ratio is set as 0.5 and cache\nlength equals to the patch numbers Tm = T. As suggested\nby (Ramachandran et al. 2019), positional encodings are\nTable 7: Object detection performance on COCO val2017 following RetinaNet 1\u00d7 settings.\nArchitecture\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nPVT-Tiny\n36.7\n56.9\n38.9\n22.6\n38.8\n50.0\nPVT-Tiny (Cached)\n40.2 (+ 3.5)\n61.1\n43.1\n25.0\n43.7\n53.4\nPVT-Small\n40.4\n61.3\n43.0\n25.0\n42.9\n55.7\nPVT-Small (Cached)\n44.0 (+ 3.6)\n65.4\n47.4\n29.7\n47.7\n57.5\nPVT-Medium\n41.9\n63.1\n44.3\n25.0\n44.9\n57.6\nPVT-Medium (Cached)\n45.7 (+ 3.8)\n67.1\n49.1\n29.0\n49.3\n60.2\nAlgorithm 1: Forward pass of GRC-Attention at training\nstage.\nRequire: training step t (t\n>\n0) , mini batch inputs\nX \u2208 RB\u00d7T \u00d7D, learnable parameters \u03bbh for head h\n\u2208\n{0, 1, ..., H\u22121}, accumulate cache Ct\u22121 \u2208 RTm\u00d7Dm, where\nDm = rD and r is caching ratio.\nEnsure: initialize C0 to be zero vectors, \u03bbh = 0 for all heads,\ncaching ratio r = 0.5, and let Tm = T (for image classifi-\ncation / Long ListOps / Object Detection) or Tm = 64 (for\nMachine Translation).\nOutput: the attention outputs Oh over both caches and inputs.\n1: calculate \u00af\nXt \u2208 RB\u00d7T \u00d7Dm by slicing inputs Xt with ratio r.\n2: interpolating \u00af\nXt to length Tm if T \u0338= Tm.\n3: calculate update gates gu and reset gates gr following Eqn.(4).\n4: calculate Ct following Eqn.(5) and averaging Ct on batch di-\nmension.\n5: update Ct\u22121 \u2190\u2212 Ct and store it.\n6: calculate self-attention outputs oh\nself following Eqn.(1).\n7: calculate cached attention outputs oh\nmem following Eqn.(3).\n8: calculate Oh following Eqn.(2).\nadded to GRC-Attentions. To fairly compare cached models\nto their baselines, we adopt their original training settings\nincluding data augmentations, optimizers and other hyper-\nparameters. Specifically, we use Adam optimizer with a mo-\nmentum of 0.9 and a weight decay of 0.05. All of the models\nare trained in 224 \u00d7 224 images for 300 epochs, with co-\nsine learning rate scheduler. Both the baselines and cached\nmodels use standard timm augmentation like (Touvron et al.\n2021a), including normalization, random cropping, horizon-\ntal flipping and color jittering. Global average poolings are\nused in PVT and Swin, where pooling sizes for the first two\nblocks are 4 and 2, respectively. All of the models are trained\non 16 Nvidia Tesla V100 GPUs, with 32 GB memory.\nObject detection and instance segmenation on COCO\n2017.\nThe models are trained on the COCO train2017\n(118K images) and evaluated on val2017 (5K images). We\nuse the cached PVT as backbone and adopt the Mask R-\nCNN detector (He et al. 2017) to verify the effectiveness\nof GRC-Attention. Before training, we use the weight pre-\ntrained on ImageNet(from prior experiments) to initialize\nthe backbone except for the cache C, which will be ini-\ntialized to zeros. As input length (T) varies in object de-\ntection, at the training stage \u00afX will be interpolated to be of\nlength Tm to update the cache. The standard COCO met-\nrics of Average Precision (AP) for bounding box detection\n(APbb) and instance segmentation (APm) are used to eval-\nuate our methods. All of the training settings and hyperpa-\nrameters are kept the same as PVT original implementation\n(Wang et al. 2021), and all of the involved models are trained\nfor 12 epochs (1\u00d7 training schedule) using 8 V100 GPUs.\nAdamW(Loshchilov and Hutter 2018) optimizer is adopted\nwith 1 \u00d7 10\u22124 initial learning rates. The training images are\nresized to 800 \u00d7 1333, which means the shorter side is 800\npixels and the longer side does not exceed 1333 pixels. At\nthe testing stage, the shorter side of the input images is fixed\nto 800. For both the cached PVT and baselines, backbones\nare firstly pretrained on ImageNet and then fine-tuned for\nobject detection.\nTable 8: Performance of GRC on ImageNet-22k.\nModel\nTop-1 Acc\nTop-5 Acc\nSwin-T\n80.9\n96.0\nSwin-T (Cached)\n81.7 (+0.8)\n96.4 (+0.4)\nLong ListOps on LRA.\nFor all experiments on the LRA\nbenchmark, we follow the released codes of (Tay et al.\n2021a), implement GRC-Attention using Flax and keep all\nthe other training settings unchanged. Specifically, all eval-\nuated models are constructed with 512 embedding dimen-\nsion, 1024 mlp dimension, 8 heads and 6 layers, with only\nattention functions are replaced by different attention vari-\nants and their cached versions. Like practice in image clas-\nsification, GRC modules are initialized with r = 0.5. Each\nmodel is trained for 5K steps(with 1K steps for warmups) on\n2K length sequences individually with batch size 32. Adam\noptimizer is adopted with initial learning rates of 0.05 and\nweight decay of 0.1.\nMachine Translation on IWSLT14 and IWSLT15.\nWe\nexperiment our methods on widely used public datasets\nIWSLT14(Cettolo et al. 2014) and IWSLT15(Cettolo\net al. 2015). For each dataset, we choose three tracks to\nvalidate the proposed GRC-Attention, including German-\nEnglish(De-En),\nSpanish-English(Es-En)\nand\nEnglish-\nFrench(En-Fr) in IWSLT14 and German-English(De-En),\nEnglish-Vietnamese(En-Vi)\nand\nCzech-English(Cs-En)\nin IWSLT15. The Pre-Norm Transformer in (Wang et al.\n2019) is used as baselines and the models are implemented\nusing fairseq-py (Ott et al. 2019) framework. Following\n(Wang et al. 2019; Ott et al. 2019), we generally increase\nthe learning rates by 2 and average the last 10 checkpoints\nfor inference. The GRC-cached models are derived by\nreplacing their attention functions in Transformer encoders\nTable 9: Training/inference time for GRC-cached models on ImageNet.\nModel\nTraining throughput\nTesting throughput\nFLOPs\nTop-1 Accuracy\nPVT-Tiny\n313\n930\n1.90G\n75.1\nPVT-Tiny(Cached)\n257\n768\n2.15G\n78.4\nPVT-Small\n181\n689\n3.80G\n79.9\nPVT-Small(Cached)\n146\n561\n4.29G\n81.8\nPVT-Medium\n101\n393\n6.70G\n81.2\nPVT-Medium(Cached)\n84\n319\n7.61G\n83.0\nTable 10: Performance of GRC with different caching ratios.\nModel\nRatio\nFLOPs\nAcc\nPVT-Tiny\n0.000\n1.90G\n75.1\nPVT-Tiny\n0.125\n1.93G\n75.7\nPVT-Tiny\n0.250\n1.96G\n76.8\nPVT-Tiny\n0.500\n2.15G\n78.4\nPVT-Tiny\n1.000\n2.97G\n78.5\nwith GRC-Attention modules, which is initialized with\ncaching ratio r = 0.5 and cache length Tm = 64. All the\ntransformers in this task consist of 6 encoder layers and\n6 decoder layers, trained with max length 512 and Adam\noptimizer. The learning rates is initially 0.0015 and then\ndecreased by inverse square root scheduler(Ott et al. 2019).\nExtensive Results and Ablations\nExtensive Results on ImageNet-22k\nWe also follow the Swin-Transformer implementation to\npre-train our cached Swin-T model on ImageNet-22k. As\nshown in Tab. 8, GRC effectively enhances the performance\nof the Swin-T model pre-trained on a larger dataset. In our\nfinal version, we\u2019ll include additional results of other ViT /\nSwin-Transformer variants on ImageNet22k.\nExtensive Results on Object Detection\nWe extensively apply GRC-Attention to RetinaNet(Lin et al.\n2017), another representative dense prediction network for\nobject detection. We choose PVTs(Wang et al. 2021) with\nvarying sizes as the backbones, including PVT-Tiny, PVT-\nSmall, and PVT-Medium. Like the practice for Mask R-\nCNN, we use pre-trained PVTs cached by GRC-Attention\nto initialize the backbone of RetinaNet and train the models\nfor 12 epochs (RetinaNet 1\u00d7 schedule) with batch size 16 on\n8 GPUs. Following practice in (Wang et al. 2021), we adopt\nAdamW(Loshchilov and Hutter 2018) optimizer with initial\nlearning rate 1\u00d710\u22124 to update the parameters. The standard\nCOCO metric Average Precision(AP) is used to evaluate\nthe models. Tab. 10 shows the detection results using Reti-\nnaNet. Consistent to Mask R-CNN, cached PVTs markedly\nimprove their baselines in terms of precision. For example,\nthe cached PVT-Medium can achieve 3.8 AP higher than the\nvanilla PVT-Medium, which is quite significant for this task.\nTo sum up, these experiments on downstream tasks (object\ndetection and instance segmentation) demonstrate the gen-\neralization capability of the proposed GRC-Attention mech-\nanism in dense vision tasks.\nSelection of Caching Ratio\nFor main hyper-parameters like caching ratio and mem-\nory length, we conduct a series of preliminary experiments\nand choose the proper ones to achieve better complexity-\nperformance trade-off. Tab. 10 provides ablations towards\ncaching ratios on ImageNet. As shown, we can observe that\nthe performance improvements from larger caching ratio(r)\nbecome marginal when r > 0.5.\nTraining and Inference Throughput\nWe compare the throughput(images/sec, per GPU) of GRC-\ncached models and baselines on ImageNet and the results\nare shown in Tab. 9. The GPU model is Tesla V100. We\ncan see that GRC improves the performance of PVT mod-\nels of different sizes while introducing a marginal computa-\ntional cost increase. Specifically, GRC-cached models sur-\npass their corresponding no-cache baselines by 1.8%-3.3%\ntop-1 accuracy with about 15%-20% drops in throughput.\nPlease also kindly note that although GRC improves model\nperformances with slightly reduced speed, it\u2019s still signifi-\ncantly more efficient than improving models by increasing\nmodel depth/width. For example, GRC-cached PVT-Small\nachieves 81.8% training accuracy with 146 training through-\nput, even outperforming the no-cache PVT-Medium which\nyields 81.2% accuracy with 101 training throughput.\n"
  },
  {
    "title": "MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers",
    "link": "https://arxiv.org/pdf/2312.12468.pdf",
    "upvote": "7",
    "text": "MaskINT: Video Editing via Interpolative Non-autoregressive Masked\nTransformers\nHaoyu Ma1*, Shahin Mahdizadehaghdam2, Bichen Wu2, Zhipeng Fan2, Yuchao Gu3,\nWenliang Zhao2, Lior Shapira2, Xiaohui Xie1\n1University of California, Irvine, 2GenAI, Meta, 3National University of Singapore\nhttps://maskint.github.io\nAbstract\nRecent advances in generative AI have significantly en-\nhanced image and video editing, particularly in the con-\ntext of text prompt control. State-of-the-art approaches pre-\ndominantly rely on diffusion models to accomplish these\ntasks. However, the computational demands of diffusion-\nbased methods are substantial, often necessitating large-\nscale paired datasets for training, and therefore challeng-\ning the deployment in practical applications. This study\naddresses this challenge by breaking down the text-based\nvideo editing process into two separate stages. In the first\nstage, we leverage an existing text-to-image diffusion model\nto simultaneously edit a few keyframes without additional\nfine-tuning. In the second stage, we introduce an efficient\nmodel called MaskINT, which is built on non-autoregressive\nmasked generative transformers and specializes in frame\ninterpolation between the keyframes, benefiting from struc-\ntural guidance provided by intermediate frames. Our com-\nprehensive set of experiments illustrates the efficacy and\nefficiency of MaskINT when compared to other diffusion-\nbased methodologies. This research offers a practical solu-\ntion for text-based video editing and showcases the poten-\ntial of non-autoregressive masked generative transformers\nin this domain.\n1. Introduction\nText-based video editing, which aims to modify a video\u2019s\ncontent or style in accordance with a provided text descrip-\ntion while preserving the motion and semantic layout, plays\nan important role in a wide range of applications, including\nadvertisement, live streaming, and the movie industry, etc.\nThis challenging task requires that edited video frames not\nonly match the given text prompt but also ensure the con-\nsistency across all video frames.\n*Work done during an internship at GenAI, Meta\nRecently, numerous studies have showcased the impres-\nsive capabilities of diffusion models [18] in the domain of\ntext-to-image and text-to-video generation [3, 37, 38, 40].\nLater on, built upon Stable Diffusion (SD) [37], several\nworks have achieved remarkable success in the realm of\ntext-based image editing [4, 43, 52]. When extending to\ntext-based video editing, existing works can be mainly di-\nvided into two ways: One is to train diffusion models with\ntemporal modules on paired text-video datasets [13, 19].\nHowever, due to the lack of extensive text-to-video datasets,\nthese works typically struggle to achieve the same level of\nediting expertise seen in the realm of image editing. The\nother involves leveraging a pre-trained text-to-image diffu-\nsion models in a training-free techniques [15, 25, 34, 53].\nThese works usually extend the self-attention across all\nframes to achieve an overall temporal consistency. How-\never, this attention-based temporal constraint remains im-\nplicit and suboptimal.\nMoreover, while diffusion-based\ntechniques are capable of producing high-fidelity videos,\nthe use of diffusion models to produce all video frames\nproves to be a highly time-consuming process. The integra-\ntion of the global attention across all frames in these video\nediting methods further extends the processing time, ren-\ndering them less practical for real-world applications.\nMeanwhile, studies indicate that non-autoregressive\nmasked generative transformers [7, 8, 16, 51] can at-\ntain similar levels of performance in generating images or\nvideos compared to diffusion-based methods, while bring\nsignificant efficiency [8]. These works first tokenize im-\nage or videos into a sequence of discrete tokens [12], and\nthen train transformers [10, 44] with masked token mod-\neling to predict these tokens. During the inference time,\nthey employ non-autoregressive decoding, which generates\nall tokens in parallel and iteratively refine predictions in a\nfew steps. Nonetheless, extending these techniques to per-\nform global editing tasks, such as stylization, presents a\nformidable challenge.\nThis type of task requires the re-\nplacement of nearly all tokens with new ones, as opposed\nto merely modifying tokens within a localized region.\n1\narXiv:2312.12468v1  [cs.CV]  19 Dec 2023\n\"corgi gnaws bone on the snow\"\n\"a wolf\"\nInput\nEdit\n\"a burning car drives on asphalt road in ghost town\"\na car drives on asphalt road, cartoon style\nFigure 1. Examples of video editing with MaskINT.\nIn this paper, we disentangle text-based video editing\ninto two separate stages. In the first stage, we utilize exist-\ning text-based image editing models to jointly edit only two\nkeyframes (i.e., the initial and last frames) from the video,\nguided by the provided text prompt. In the second stage,\nwe propose a novel Interpolative Non-autoregressive gener-\native Transformers (MaskINT), which performs structure-\naware frame interpolation by leveraging both the color in-\nformation of the initial and final frames, as well as structural\ncues like edge or depth maps from intermediate frames.\nThrough disentanglement of keyframe editing and frame\ninterpolation into separate stages, our pipeline eliminates\nthe requirement for paired video datasets during training,\nthereby enabling us to train the MaskINT using video-only\ndatasets. Furthermore, thanks to its non-autoregressive de-\ncoding, MaskINT significantly accelerates the generation of\nintermediate frames compared to using diffusion models for\nthis purpose. We show that our method balances the trade\noff between quality and efficiency, offering comparable per-\nformance with existing diffusion methods yet taking much\nless time in generation.\nOur major contributions are summarized as follows:\n\u2022 We propose to disentangle the text-based video edit-\ning into a two stage pipeline, that involves keyframes\njoint editing using existing image diffusion model and\nstructure-aware frame interpolation with masked gener-\native transformers trained on video only datasets.\n\u2022 We propose MaskINT to perform structure-aware frame\ninterpolation, which is the pioneer work that explicitly in-\ntroduces structure control into non-autoregressive gener-\native transformers.\n\u2022 Experimental\nresults\ndemonstrate\nthat\nour\nmethod\nachieves comparable performance with diffusion methods\nin terms of temporal consistency and alignment with text\nprompts, while providing 5-7 times faster inference times.\n2. Related Work\nGenerative Transformers.\nFollowing GPT [5], many\npioneer works [12, 14, 20, 26, 50] tokenize images/videos\ninto discrete tokens, and train Autoregressive Generative\nTransformers to perform image/video generation, where\ntokens are generated sequentially based on previous out-\nput. However, these autoregressive methods become ex-\nceedingly time-consuming when the length of the token se-\nquence increases. Recently, Non-autoregressive Generative\nTransformers, capable of simultaneously generating all to-\nkens in parallel, have emerged as efficient solutions [7, 51].\nSpecifically, MaskGiT [7] first shows the capability and ef-\nficiency of this technique in image generation. It can be\nseamlessly extended to tasks like inpainting and extrapo-\nlation by applying various initial mask constraints. Muse\n[8] achieves state-of-the-art performance in text-to-image\ngeneration by training on large-scale text-image datasets\nand brings significantly efficiency improvement. StyleDrop\n[41] further finetunes Muse with human feedback to per-\nform text-to-image generation guided with a reference style\nimage. Furthermore, MaskSketch [1] introduces implicit\nstructural guidance into MaskGiT by calculating the simi-\nlarity of attention maps in the sampling step. Nevertheless,\nthis implicit structure condition is suboptimal.\nIn video generation, MaskViT [16] employ 2D tokenizer\nand trains a bidirectional window transformer to perform\nframe prediction. Phenaki [45] trains a masked transformer\nto generate short video clips condition on text prompt and\nextends it to arbitrary long video with different prompts in\nan autoregressive way. MAGVIT [51] utilizes 3D tokenizer\nto quantize videos and trains a single model to perform mul-\ntiple video generation tasks such as inpainting, outpainting,\nframe interpolation, etc. However, to the best of our knowl-\nedge, there is currently no existing literature in the field of\ntext-based video editing utilizing masked generative trans-\nformers. Besides, there is a notable absence of research that\ndelves into explicit structural control within this area.\nDiffusion Models in Image Editing.\nLeveraging the ad-\nvancements of SD [37], numerous studies have achieved\nsignificant success in the field of text-based image editing\n2\n[9, 17, 24, 29, 30, 32, 43, 52, 54]. For example, Control-\nNet [52], T2I-Adapter [30], and Composer [21] finetune\nSD with spatial condition such as depth maps and edge\nmaps, enabling text-to-image synthesis with the same struc-\nture as the input image. The PNP [43] incorporates DDIM\ninversion features [42] from the input image into the text-\nto-image generation process alongside SD, enabling image\nediting without the necessity of additional training or fine-\ntuning.\nInstructpix2pix [4] trains a conditional diffusion\nmodel for text-guided image editing using synthetic paired\nexamples, which avoid the tedious inversion.\nNeverthe-\nless, employing these methods on each video frame inde-\npendently often leads to inconsistencies and flickering.\nDiffusion Models in Video Editing.\nRecently, diffu-\nsion models also dominate the field of video generation\nand video editing [19, 40]. For example, Gen-1 [13] trains\na video diffusion models on paired text-video datasets to\ngenerate videos with both depth map and text prompts.\nMeanwhile, several works utilize pre-trained image diffu-\nsion models to achieve video editing in a training-free way\n[6, 25, 34, 46, 47, 49, 53]. To enable a cohesive global\nappearance among edited frames, a common approach in\nthese studies involves extending the attention module of\nSD to encompass multiple frames and conducting cross-\nframe attention. In detail, Text2Video-Zero [25] performs\ncross-frame attention of each frame on the first frame to\npreserve appearance consistency.\nControlVideo [53] ex-\ntends ControlNet with fully cross-frame attention to joint\nedit all frames and further improves the performance with\ninterleaved-frame smoother. TokenFlow [15] enhance PNP\n[43] with extended-attention to jointly edit a few keyframes\nat each denoising step and propagate them throughout the\nvideo based on nearest-neighbor field.\nOn the contrary,\nwe only utilize existing image diffusion models to edit two\nkeyframes, rather than all video frames.\nVideo Frame Interpolation (VFI).\nVFI aims to generate\nintermediate images between a pair of frames, which can\nbe applied to creating slow-motion videos and enhancing\nrefresh rate. Advanced methods typically entail estimating\ndense motions between frames, like optical flow, and sub-\nsequently warping the provided frames to generate interme-\ndiate ones [22, 23, 28, 31, 36, 39]. However, these meth-\nods are most effective with simple or monotonous motion.\nThus, they cannot be directly applied to the second stage. In\nour work, we perform frame interpolation by incorporating\nadditional structural signals.\n3. Preliminaries\n3.1. Masked Generative Transformers\nMasked generative transformers [7] follow a two-stage\npipeline. In the first stage, an image is quantized into a\nsequence of discrete tokens via a Vector-Quantized (VQ)\nauto-encoder [12]. In detail, given an image I \u2208 RH\u00d7W \u00d73,\nan encoder E encodes it into a series of latent vectors and\ndiscretize them through a nearest neighbour look up in a\ncodebook of quantized embeddings with size M. To this\nend, an image can be represented with a sequence of code-\nbook\u2019s indices Z = [zi]h\u00d7w\ni=1 , zi \u2208 {1, 2, ..., M}, where h\nand w is the resolution of latent features. A decoder D\ncan reconstruct the indices back to image D(Z) \u2248 I. In\nthe second stage, a bidirectional transformer model [44] is\nlearned with Masked Token Modeling (MTM). Specifically,\nduring training, a random mask ratio r \u2208 (0, 1) is selected\nand \u230a\u03b3(r) \u00b7 h \u00d7 w\u230b tokens in Z are replaced with a special\n[MASK] token, where \u03b3(r) is a mask scheduling function\n[7]. We denote the corrupted sequence with masked tokens\nas \u00afZ and conditions such as class labels or text prompt as\nc. Given the training dataset D, a BERT [10] parameterized\nby \u03a6 is learned to minimize the cross-entropy loss between\nthe predicted and the ground truth token at each masked po-\nsition:\nLMT M = E\nZ\u2208D\n\"\nX\n\u00afzi=[MASK]\n\u2212 log p\u03a6(zi|\u00afZ, c)\n#\n.\n(1)\nDuring inference time, non-autoregressive decoding is ap-\nplied to generate images. Specifically, given the conditions\nc, all tokens are initialized as [MASK] tokens. At step k, all\ntokens are predicted in parallel while only tokens with the\nhighest prediction scores are kept. The remaining tokens\nwith least prediction scores are masked out and regenerated\nin the next iteration. The mask ratio is determined by \u03b3( k\nat step k, where K is the total number of iteration steps.K )\n3.2. ControlNet\nLatent Diffusion Models\nDenoising Diffusion Proba-\nbilistic Models (DDPM) [18] generate images through a\nprogressive noise removal process applied to an initial\nGaussian noisy image, carried out over a span of T(>> K)\ntime steps. To enable efficient high-resolution image gen-\neration, Latent Diffusion models [37] operates the diffusion\nprocess in the latent space of an autoencoder. First, an en-\ncoder E\u2032 compresses an image I to a low-resolution latent\ncode x = E\u2032(I) \u2208 Rh\u00d7w\u00d7c. Second, a U-Net \u03f5\u03b8 with atten-\ntion modules [44] is trained to remove the noise with loss\nfunction:\nLLDM = Ex0,\u03f5\u223cN(0,I),t\u223c\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t, \u03c4)\u22252\n2,\n(2)\nwhere \u03c4 is the text prompt and xt is the noisy latent sample\nat timestep t. Stable Diffusion is trained on datasets with\nbillion scale text-image pairs, which serves as the founda-\ntion model of many generation tasks.\nControlNet\nIn practice, it\u2019s challenging to use text\nprompt to describe the layout of generated image. Further-\nmore, ControlNet [52] is proposed to provide spatial lay-\nout conditions such as edge map, depth map, and human\n3\nposes. In detail, ControlNet train the same U-Net archi-\ntecture as SD and finetune it with specific conditions. We\ndenote ControlNet as \u03f5\u2032\n\u03b8(xt, t, \u03c4, s), where s is the spatial\nlayout condition.\n4. Methodology\n4.1. Overview\nFig.\n2 shows an overview of our framework.\nWe dis-\nentangle the video editing task into keyframe joint edit-\ning stage and structure-aware frame interpolation stage.\nSpecifically, given a video clip with N frames {Ii}N\ni=1,\nin the first stage, with the input text prompt \u03c4, we si-\nmultaneously edit two keyframes, i.e., the initial frame\nI1 and last frame IN, using existing image-editing model\ng(\u00b7) that requires no additional tuning. This frame based\njoint editing module provides high-quality coherent edited\nframes \u2217I1, \u2217IN = g(I1, IN, \u03c4) and is highly efficient on\na pair of frame. In the second stage, we propose Mask-\nINT to perform structure-aware frame interpolation via non-\nautoregressive transformers. The entire edited video frames\nare generated by {\u2217Ii}N\ni=1 = f\u03a6(\u2217I1, \u2217IN, {Si}N\ni=1), where\nSi \u2208 [0, 1]H\u00d7W \u00d71 is the structural condition (i.e., the HED\nedge map [48]). Our MaskINT is trained with masked to-\nken modeling (MTM) on video only datasets, conditioning\non the structural signal {Si}N\ni=1 as well as the initial frame\nI1 and last frame IN.\n4.2. Keyframes Joint Editing\nTo maintain the structure layout of selected keyframes,\nwe take ControlNet [52] to edit I1 and IN based on text\nprompt \u03c4 as well as their edge maps S1 and SN. How-\never, even with the identical noise, applying ControlNet\nto each keyframes individually, i.e., \u03f5\u2032\n\u03b8(x1\nt, t, \u03c4, S1) and\n\u03f5\u2032\n\u03b8(xN\nt , t, \u03c4, SN), cannot guarantee the appearance consis-\ntency.\nTo address this issue, following previous work\n[15, 47, 53], we extend the self-attention blocks to simulta-\nneously process two keyframes. In detail, the self-attention\nblock projects the noisy feature map xj\nt of jth frame at time\nstep t into query Qj, key Kj, value Vj in the original U-\nNet of SD. We extend the self-attention block to perform at-\ntention across all selected keyframes by concatenating their\nkeys and values and calculate the attention by\nSoftmax(Qj[K1, KN]T\n\u221ac\n)[V1, VN]\n(3)\nNote that although only two frames were used in Eq.3, this\njoint editing can seamlessly generalize to any number of\nframes, but at the cost of longer processing time and huge\ndemand in resources.\n4.3. MaskINT\nStructural-aware embeddings.\nPrevious works [16,\n51] demonstrate that non-autoregressive masked generative\ntransformers can effectively perform frame prediction and\ninterpolation tasks. However, these architectures lack ex-\nplicit control of structure, making it difficult to follow the\nmotion of original videos. In our work, we explicitly in-\ntroduce structural condition of each frame into the gener-\nation process. Specifically, we tokenize both RGB frames\n{Ii}N\ni=1 and structure maps {Si}N\ni=1 with an off-the-shelf\n2D VQ tokenizer [12]. We utilize 2D VQ rather than 3D\nVQ [51] to accommodate varying numbers of frames and\nframe rate without constraints. We denote the tokens from\nRGB frames as cZ = {czi}N\u00d7h\u00d7w\ni=1\n(color token) and to-\nkens from edge maps as sZ = {szi}N\u00d7h\u00d7w\ni=1\n(structure\ntoken), where szi, czi \u2208 {1, 2, ..., M}, where M is the\ncodebook size. Subsequently, two distinct embedding lay-\ners eC(\u00b7) and eS(\u00b7) are learned to map token indices cZ\nand sZ into their respective embedding spaces.\nLearn-\nable 2D spatial positional encoding PS and temporal po-\nsitional encoding PT are also added [2]. Thus, the input\nto the following transformer layers can be formulated by\nX = ec(cZ) + es(sZ) + PS + PT \u2208 RN\u00d7h\u00d7w\u00d7c.\nTransformer with Window-Restricted Attention.\nPre-\nvious masked generative transformers [7, 51] employ a pure\ntransformer with global attention [44]. However, Given that\nthere is no substantial motion between consecutive frames,\nwe adopt self-attention within a restricted window to fur-\nther mitigate computational overhead, following MaskViT\n[16]. In detail, our approach involves two distinct stages\nof attention. Initially, we employ spatial window attention,\nconfining attention to tokens within a frame of dimensions\n1\u00d7h\u00d7w. Subsequently, we extend this to spatial-temporal\nwindow attention, which confines attention to tokens within\na tube of dimensions N \u00d7hw \u00d7ww, where hw and ww is the\nwindow size, where hw < h and ww < w, which greatly\nreduce the complexity. Besides, to further reduce computa-\ntion of transformer, we also add a shallow convolution lay-\ners to downsample X before the transformer encoder layers\nand an upsample layer at the end.\nTraining.\nBy fully disentangling keyframes editing and\nframe interpolation into distinct stages, our model no longer\nnecessitates paired videos for training. Consequently, we\ncan train MaskINT using video only datasets. Denote the\ncolor token of ith RGB frame as cZi = {czi}h\u00d7w\ni=1 . Dur-\ning the training time, we keep color tokens of the ini-\ntial frame cZ1 and last frame cZN, and randomly replace\n[\u03b3(r) \u00b7 (N \u2212 2) \u00b7 N] color tokens of intermediate frames\nwith the [MASK] tokens. We denote this corrupted video\ncolor tokens as \u00af\ncZ = {cZ1, \u00af\ncZ\n2, ..., \u00af\ncZ\nN\u22121, cZN}. The\nstructure-aware window-restricted transformer with param-\neters \u0398 is trained by\nLMT M =\nE\ncZ,sZ\u2208D[\nX\n\u00af\nczi=[MASK]\n\u2212 log p\u0398(czi| \u00af\ncZ, sZ)]\n(4)\n4\nStructure Detector\nBidirectional \nWindow \nTransformer\nVQ Tokenizer\n(a) Training\nStructural token szi\nColor token czi\ncZ\nZs\n{Ii}N\ni=1\n{Si}N\ni=1\nLMTM\n\u00af\ncZ\nrandom  \nmask\n[MASK] token\nStructure Detector\nkey frames\n(b) Inference\n: \"a male lion on ice in snowy day\"\n\u03c4\nStage 1: Key frames joint editing\nStage 2: Structure-aware Frame Interpolation\n*IN\n*I1\nT2I model \nJoint Editing\nVQ Tokenizer\nBidirectional \nWindow \nTransformer\ncZ\nsZ\nVQ Decoder\n  iterations\nK\nI1\nIN\nS1\nSN\n{Ii}N\ni=1\n*I1\n*IN\n{Si}N\ni=1\n{*Ii}N\ni=1\nOptional\nFigure 2. Overview of MaskINT. MaskINT disentangle the video editing task into two separate stages, i.e., keyframes joint editing and\nstructure-aware frame interpolation.\nInference.\nDuring the inference time, our MaskINT can\nseamlessly generalize to perform frame interpolation be-\ntween the jointly edited frames, although it is only trained\nwith regular videos. Specifically, we tokenize the the initial\nand last edited frames \u2217I1 and \u2217IN from Stage 1 into color\ntokens c\n\u2217Z1 and c\n\u2217ZN, and initialize color tokens of all in-\ntermediate frames { \u00af\ncZ\n2, ..., \u00af\ncZ\nN\u22121} with [MASK] tokens.\nWe follow the iterative decoding in MaskGiT [7] with a total\nnumber of K steps. At step k, we predict all color tokens in\nparallel and keep tokens with the highest confidence score.\n5. Experiments\n5.1. Settings\nImplementation Details.\nWe train our model with 100k\nvideos from ShutterStock website1. During training time,\nwe random select a T = 16 video clip with frame interval\n1, 2, 4 from each video and resize it to 384\u00d7672. We utilize\nRetina-VQ [11] with 8 downsample ratio, i.e., each frame\nhas 48 \u00d7 84 tokens. We employ Transformer-Base as our\nMaskINT and optimized it from scratch with the AdamW\noptimizer [27] for a duration of 100 epochs. The initial\nlearning rate is set to 1e\u22124 and decayed with cosine sched-\nule. During the inference time, we set the number of decod-\n1https://www.shutterstock.com/video\ning step K to 32 and the temperature t to 4.5.\nEvaluation.\nFollowing [47], we use the selected 40\nobject-centric videos of the DAVIS dataset [33], covering\nhumans, animals, vehicles, etc.\nBesides, we also select\n30 unseen videos from the ShutterStock dataset. For each\nvideo, we manually design 5 edited prompts, including ob-\nject editing, background changes and style transfers. Fol-\nlowing previous works [34, 53], we assess the quality of the\ngenerated videos using CLIP [35]. In detail, we evaluate\n1) Temporal consistency, which calculates the average co-\nsine similarity of all pairs of consecutive frames. 2) Prompt\nconsistency, which calculates the average cosine similarity\nbetween given text prompt and all video frames. To evaluate\nthe efficiency, we report the duration required for generating\na 16-frame video clip on a single NVIDIA A6000 GPU.\n5.2. Results\nWe select methods that built upon text-to-image diffusion\nmodels for comparison, including TokenFlow [15], Text-to-\nvideo zero [25], and ControlVideo [53]. We also consider\napply ControlNet to each frame individually with the same\ninitial noise as baseline. Besides, we also compare with\nframe interpolation with FILM [36] with the same edited\nkeyframes.\n5\n\"a marble sculpture of woman\"\n\"black poodle dog runs\"\n\"a male lion on ice in snowy day\"\n\"cute plastic pig Money Bank\"\n\"a woman is running in autumn\"\n\"a car drives on ice road in snowy day\"\n\"a dog looking through car window, van gogh style\"\n\"two men play kite surf, van gogh style\"\nFigure 3. Examples of video editing with MaskINT. Frames with red bounding box are jointly edited keyeframes.\nMethod\nDAVIS\nShutterStock\nTime\nT.C.\nP.C.\nT.C.\nP.C\nControlNet per frame [52]\n0.9137\n0.3136\n0.9419\n0.3040\n50s\nText2Video-zero [25]\n0.9642\n0.3124\n0.9811\n0.3036\n60s\nTokenFlow [15]\n0.9774\n0.3169\n0.9869\n0.3133\n150s\nControlVideo-edge [53]\n0.9746\n0.3143\n0.9864\n0.3032\n120s\nMaskINT (ours)\n0.9519\n0.3112\n0.9714\n0.3038\n22s\nTable 1. Quantitative comparisons on the quality of the gener-\nated videos. \u201cT.C.\u201d stands for \u201ctemporal consistency\u201d, and \u201cP.C.\u201d\nstands for \u201cprompt consistency\u201d.\nQuantitative Comparisons.\nTable 1 summarize the per-\nformance of these methods on both DAVIS and Shutter-\nStock datasets.\nNotably, our method achieves compara-\nble performance with diffusion methods, in terms of both\ntemporal consistency and prompt consistency, while brings\na significant acceleration in processing speed.\nIn detail,\nMaskINT is almost 5.5 times faster than ControlVideo [53],\nwhose fully cross-frame attention is computationally exten-\nsive. Moreover, MaskINT is nearly 7 times faster than To-\nkenFlow [15], whose DDIM inversion is time-consuming.\nOn the contrary, our acceleration is derived from a combina-\ntion of a lightweight network design and a reduced number\nof decoding steps in masked generative transformers.\nQualitative Comparisons.\nFig. 1 and Fig. 3 show sev-\neral samples of edited videos with MaskINT. Our method\nis capable of generating temporally consistent videos that\nadhere to the provided text prompts. This extends to a wide\nrange of applications with text prompts, encompassing tasks\nsuch as stylization, background editing, foreground editing,\nand more. It also works well on challenging videos with\nsubstantial motion, such as jumping and running. More-\nover, Fig. 4 provides a qualitative comparison of Mask-\nINT to other baselines.\nRemarkably, diffusion methods\n[15, 25, 53] can ensure the consistency of overall appear-\nance, but sometimes cannot maintain the consistency of\ndetailed regions. For example, both TokenFlow [15] and\nText2Video-Zero[25] exhibit noticeable artifacts in the leg\nregion of the human subjects. ControlVideo [53] produces\n6\n\"a man performs freestyle dance outdoors, van gogh style\"\nInput\nControlVideo\nControlNet\nTokenFlow\nText2video-0\nFILM\n\"a man with black clothes on snowboard on sand in desert\"\nOurs\nFigure 4. Qualitative comparisons with diffusion-based methods. More examples are shown in Supplementary.\ninconsistent hats. The potential explanation lies in the fact\nthat these methods offer control over temporal consistency\nimplicitly. Furthermore, FILM [36] produces videos that\ndeviate from the original motions with the same edited\nkeyframes. Our MaskINT consistently interpolates the in-\ntermediate frames based on the structure condition and even\nmaintain better consistency in local regions.\nExtension on Long Video Editing.\nSince the non-\nautoregressive pipeline generates all video frames simulta-\nneously, it\u2019s challenging for it to edit an entire long video\ndue to GPU memory limitation. Nevertheless, our frame-\nwork can still be extended to generate long videos by di-\nviding the long video into short clips and progressively\nperforming frame interpolation within each clip. For in-\nstance, given a video with 60 frames, we select the 1st, 16th,\n31st, 46th, and 60th frames as keyframes. We jointly edit\nthese selected 5 frames together and then perform structure-\naware frame interpolation within each pair of consecutive\nkeyframes.\nAs shown in Fig.\n5, with this design, our\nmethod can still generate consistent long videos. Besides,\nin this proposed extension, the generation of later frames is\ndecoupled from the generated early frames, which differs\nfrom the autoregressive long-video generation pipeline in\nPhenaki [45]. Consequently, even if some early frames en-\ncounter difficulties, the generation of later frames can still\nproceed successfully.\ntwo men play kite surf, van gogh style\n1\n7\n13\n19\n25\n31\n37\n43\n49\n55\n1\n7\n13\n19\n25\n31\n37\n43\n49\n55\nFigure 5. Examples of long video generation. The number indi-\ncates the index of frame. More examples are in Supplementary.\n5.3. Ablation Studies\nVideo Frame Interpolations\nWe conduct a quantitative\nevaluation on the performance of our structure-aware in-\nterpolation module.\nIn this ablation study, we perform\nframe interpolation using the original keyframes to recon-\nstruct the original intermediate frames, and compare the\ninterpolated frames with the original video frames at the\n7\npixel level. We use same testing samples from DAVIS and\nShutterstock datasets, employing peak signal-to-noise ratio\n(PSNR), learned perceptual image patch similarity (LPIPS),\nand structured similarity (SSIM) as the evaluation metrics.\nWe benchmark our method against two state-of-the-art VFI\nmethods, namely FILM [36] and RIFE [22]. We also show-\ncase the performance of applying VQ-GAN [12] to all video\nframes, serving as an upper bound for our method.\nAs in Table 2, our method significantly outperforms\nVFI methods on all evaluation metrics, with the benefit of\nthe structure guidance from the intermediate frames. Fur-\nthermore, Fig.\n6 shows qualitative comparison between\nvideo frame interpolation methods FILM [36] and Mask-\nINT. Even when confronted with significant motion be-\ntween two frames, our approach successfully reconstructs\nthe original video, maintaining consistent motion through\nthe aid of structural guidance. In contrast, FILM introduces\nundesirable artifacts, including disorted background, dupli-\ncated cat hands, and the absence of a camel\u2019s head, etc. The\nmajor reason is that current VFI models mainly focus on\ngenerating slow-motion effects and enhancing frame rate,\nmaking them less effective in handling frames with large\nmotions, which usually requires a better semantic under-\nstanding. Additionally, the absence of structural guidance\nposes a challenge for these VFI methods in accurately align-\ning generated videos with the original motion.\nFILM\nMaskINT\nGround \nTruth\nFILM\nMaskINT\nGround \nTruth\nFigure 6. Qualitative comparisons on video reconstruction with\noriginal RGB frames. Frames with red bounding box are given.\nNumber of of keyframes\nAlthough our model is trained\nwith frame interpolation by default, MaskINT can seam-\nlessly generalize to an arbitrary number of keyframes with-\nout finetuning. We assess the impact of varying the quan-\ntity of keyframes on the generation performance. As shown\nin the left part of Table 3, with an increase in the number\nMethod\nDAVIS\nShutterStock\nPSNR\u2191\nSSIM \u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nRIFE [22]\n17.31\n0.5195\n0.2512\n20.44\n0.7210\n0.1533\nFILM [36]\n17.00\n0.5011\n0.2363\n20.90\n0.7453\n0.1246\nMaskINT\n22.15\n0.6332\n0.1483\n24.19\n0.7616\n0.1097\nVQGAN (ground truth)\n25.66\n0.7429\n0.0784\n27.81\n0.8327\n0.0561\nTable 2. Quantitative comparisons on video frame interpolation\nwith original keyframes.\nof keyframes, the model exhibits an improvement in per-\nformance. Generally, with more information, performing\nframe interpolation is easier. However, simultaneously edit-\ning more keyframes requires longer time due to the global\nattention among them.\nDecoding steps\nWe also explore the number of decoding\nsteps of the masked generative transformers in the second\nstage. The right part of Table 3 shows that more decoding\nsteps can bring slight improvement on the temporal consis-\ntency, but requires more time. Considering the trade-off be-\ntween performance and efficiency, we chose K = 32 steps\nby default in all experiments.\n# keyframes\nT.C.\nP.C.\nTime\n1\n0.9690\n0.2984\n19s\n2\n0.9714\n0.3038\n22s\n3\n0.9721\n0.3051\n26s\n4\n0.9728\n0.3069\n29s\n6\n0.9737\n0.3035\n35s\n#decoding step K\nT.C.\nP.C.\nTime\n16\n0.9691\n0.3038\n15s\n32\n0.9714\n0.3038\n22s\n64\n0.9719\n0.3040\n33s\n128\n0.9720\n0.3041\n62s\nTable 3. Ablation study on ShutterStock dataset of the number\nof keyframes and the number of decoding steps K. \u201cT.C.\u201d stands\nfor \u201ctemporal consistency\u201d and \u201cP.C.\u201d stands for \u201cprompt consis-\ntency\u201d.\n6. Limitation and Future Work\nOne potential limitation of our approach is that it still ne-\ncessitates the use of diffusion-based methods for keyframe\nediting in the initial stage, which can still be somewhat\ntime-consuming. As for the future work, given that our ap-\nproach disentangles video editing into two distinct stages,\nwe intend to explore the integration of token-based meth-\nods, like Muse [8], for image editing. This endeavor aims\nto further enhance the efficiency of the initial stage.\n7. Conclusion\nWe propose MaskINT towards consistent and efficient\nvideo editing with text prompt.\nMaskINT disentangle\nthis task into keyframes joint editing with diffusion meth-\nods and structure-aware frame interpolation with non-\nautoregressive masked transformers. Experimental results\ndemonstrate that MaskINT achieves comparable perfor-\nmance with pure diffusion-based methods while signifi-\ncantly reduces the inference time.\nMoreover, our work\ndemonstrates the substantial promise of non-autoregressive\ngenerative transformers within the realm of video editing.\n8\nReferences\n[1] Dina Bashkirova, Jos\u00b4e Lezama, Kihyuk Sohn, Kate Saenko,\nand Irfan Essa.\nMasksketch: Unpaired structure-guided\nmasked image generation.\nIn CVPR, pages 1879\u20131889,\n2023. 2\n[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time attention all you need for video understanding?\nIn ICML, 2021. 4\n[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, pages 22563\u201322575, 2023.\n1\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, pages 18392\u201318402, 2023. 1, 3\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. NeurIPS, 33:1877\u2013\n1901, 2020. 2\n[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In ICCV,\npages 23206\u201323217, 2023. 3\n[7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer. In\nCVPR, pages 11315\u201311325, 2022. 1, 2, 3, 4, 5\n[8] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. ICML, 2023. 1, 2, 8\n[9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord.\nDiffedit:\nDiffusion-based seman-\ntic image editing with mask guidance.\narXiv preprint\narXiv:2210.11427, 2022. 3\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. NAACL, 2019. 1, 3\n[11] Abhimanyu Dubey, Filip Radenovic, Dhruv Mahajan, and\nVignesh Ramanathan. Retina vq. 2023. 5\n[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\npages 12873\u201312883, 2021. 1, 2, 3, 4, 8\n[13] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nICCV, 2023. 1, 3\n[14] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan\nPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.\nLong video generation with time-agnostic vqgan and time-\nsensitive transformer. In ECCV, pages 102\u2013118. Springer,\n2022. 2\n[15] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 1, 3, 4, 5, 6\n[16] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu,\nRoberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, and Li Fei-Fei. Maskvit: Masked\nvisual pre-training for video prediction. In ICLR, 2023. 1, 2,\n4\n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33:6840\u20136851, 2020. 1,\n3\n[19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 3\n[20] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and\nJie Tang. Cogvideo: Large-scale pretraining for text-to-video\ngeneration via transformers. ICLR, 2023. 2\n[21] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 3\n[22] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi,\nand Shuchang Zhou.\nReal-time intermediate flow estima-\ntion for video frame interpolation. In ECCV, pages 624\u2013642.\nSpringer, 2022. 3, 8\n[23] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan\nYang, Erik Learned-Miller, and Jan Kautz.\nSuper slomo:\nHigh quality estimation of multiple intermediate frames for\nvideo interpolation. In CVPR, pages 9000\u20139008, 2018. 3\n[24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, pages 6007\u20136017, 2023. 3\n[25] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators.\nICCV, 2023. 1, 3, 5, 6\n[26] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid.\nCcvs: context-aware controllable video synthesis. NeurIPS,\n34:14042\u201314055, 2021. 2\n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 5\n[28] Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya\nJia. Video frame interpolation with transformer. In CVPR,\npages 3532\u20133542, 2022. 3\n[29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. ICLR, 2022. 3\n[30] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 3\n9\n[31] Simon Niklaus, Long Mai, and Feng Liu. Video frame inter-\npolation via adaptive separable convolution. In ICCV, pages\n261\u2013270, 2017. 3\n[32] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, pages 1\u201311, 2023. 3\n[33] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alexander Sorkine-Hornung, and Luc Van Gool.\nThe 2017 davis challenge on video object segmentation.\narXiv:1704.00675, 2017. 5\n[34] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. ICCV,\n2023. 1, 3, 5\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763, 2021. 5\n[36] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun,\nCaroline Pantofaru, and Brian Curless. Film: Frame interpo-\nlation for large motion. In ECCV, 2022. 3, 5, 7, 8\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 1, 2, 3\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 35:36479\u201336494, 2022. 1\n[39] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvfi: ex-\ntreme video frame interpolation.\nIn ICCV, pages 14489\u2013\n14498, 2021. 3\n[40] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. ICLR, 2023. 1, 3\n[41] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 2\n[42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. ICLR, 2021. 3\n[43] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation.\nIn CVPR, pages 1921\u20131930,\n2023. 1, 3\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 30, 2017. 1,\n3, 4\n[45] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In ICLR, 2023. 2, 7\n[46] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 3\n[47] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. ICCV, 2023.\n3, 4, 5\n[48] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In ICCV, pages 1395\u20131403, 2015. 4\n[49] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. ACM SIGGRAPH Asia Conference Proceedings,\n2023. 3\n[50] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu.\nVector-quantized image modeling with\nimproved VQGAN. In ICLR, 2022. 2\n[51] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked\ngenerative video transformer. In CVPR, pages 10459\u201310469,\n2023. 1, 2, 4\n[52] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models. ICCV, 2023. 1, 3,\n4, 6\n[53] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 1, 3, 4, 5, 6\n[54] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N\nMetaxas, and Jian Ren.\nSine: Single image editing with\ntext-to-image diffusion models. In CVPR, pages 6027\u20136037,\n2023. 3\n10\nMaskINT: Video Editing via Interpolative Non-autoregressive Masked\nTransformers\nSupplementary Material\n8. Diverse Structural Conditions\nAlthough we by default utilize HED edge map as our struc-\nture condition for both stages, our method can also employ\nother structural controls in both stages due to the disentan-\nglement. In this study, we explore utilizing ControlNet with\ndepth map to perform key frame editing, and using the depth\nmap as the guidance to perform structure-aware frame inter-\npolation. Additionally, we explore various combinations of\nthese approaches. As summarized in Table 4, all of these\ncombinations achieve the same level of performance. The\nperformance of prompt consistency (P.C.) is determined by\nthe specific key frame editing methods employed. For the\nsecond stage, depth control typically offers greater flexibil-\nity than HED edge control for frame interpolation. This\ncould be the potential reason for the slightly worse perfor-\nmance in temporal consistency with edge-based key frame\nediting.\nStage1\nStage2\nT.C.\nP.C\nHED edge\nHED edge\n0.9714\n0.3038\ndepth map\nHED edge\n0.9713\n0.3159\nHED edge\ndepth map\n0.9683\n0.3035\ndepth map\ndepth map\n0.9719\n0.3171\nTable 4. Quantitative comparisons of the combination of varied\nstructural conditions in each stage on ShutterStock. \u201cT.C.\u201d stands\nfor \u201ctemporal consistency\u201d, and \u201cP.C.\u201d stands for \u201cprompt consis-\ntency\u201d.\n9. Additional Examples for Comparisons\nWe add more comparisons with diffusion methods in Fig. 8.\nOur methods can maintain the temporal consistency among\nvariant examples like other pure-diffusion methods.\n10. Additional Editing Examples\nWe present more video editing examples in Figure 9 to show\nthe generalization of our method.\n11. Example of Failure Cases\nSince we disentangle the video editing tasks into two sepa-\nrate stage, the final performance of the generated video de-\npends on the key frame editing in the first stage. In certain\nchallenging scenarios, the attention-based key frame edit-\ning stage struggles to produce consistent frames, primarily\ndue to the complexity of the scene or the presence of ex-\nceptionally large motion. In this case, our MaskINT can\nstill interpolate the intermediate frames, albeit with the po-\ntential for introducing artifacts. Figure 7 show some failure\ncases when the first stage fails.\n\"two cats, cartoon style\"\n\"teenagers play skateboard, van gogh style\"\nFigure 7. Examples of failure cases.\n1\n\"a a rhino walks on ice in snowy day\"\nInput\nControlVideo\nControlNet\nTokenFlow\nText2video-0\nFILM\nOurs\n\"a car drives on asphalt road, van gogh style\"\nFigure 8. Additional Qualitative comparisons with diffusion-based methods. Frames with red bounding box are jointly edited keyeframes.\n\"a blue airplane flies away in the dark night\"\n\"a panda\"\n\"a man hikes in autumn\"\n\"frozen fish in water\"\nFigure 9. Additional Editing examples with MaskINT. Frames with red bounding box are jointly edited keyeframes.\n2\n"
  },
  {
    "title": "Mini-GPTs: Efficient Large Language Models through Contextual Pruning",
    "link": "https://arxiv.org/pdf/2312.12682.pdf",
    "upvote": "7",
    "text": "Mini-GPTs: Efficient Large Language Models through\nContextual Pruning\nTim Valicenti\u2217\nMassachusetts Institute of Technology\nCambridge, MA 02142\ntvalicen@mit.edu\nJustice Vidal\nMassachusetts Institute of Technology\nCambridge, MA 02142\njmvidal@mit.edu\nRitik Patnaik\nMassachusetts Institute of Technology\nCambridge, MA 02142\nrik01@mit.edu\nAbstract\nIn AI research, the optimization of Large Language Models (LLMs) remains a\nsignificant challenge, crucial for advancing the field\u2019s practical applications and\nsustainability. Building upon the foundational work of Professor Song Han\u2019s\nlab at MIT, this paper introduces a novel approach in developing Mini-GPTs\nvia contextual pruning. Our methodology strategically prunes the computational\narchitecture of traditional LLMs, like Phi-1.5, focusing on retaining core function-\nalities while drastically reducing model sizes. We employ the technique across\ndiverse and complex datasets, including US law, Medical Q&A, Skyrim dialogue,\nEnglish-Taiwanese translation, and Economics articles. The results underscore\nthe efficiency and effectiveness of contextual pruning, not merely as a theoretical\nconcept but as a practical tool in developing domain-specific, resource-efficient\nLLMs. Contextual pruning is a promising method for building domain-specific\nLLMs, and this research is a building block towards future development with more\nhardware compute, refined fine-tuning, and quantization.\n1\nIntroduction & Literature Review\nThe advent of Large Language Models (LLMs) like GPT-4 has marked a paradigm shift in artificial in-\ntelligence, offering unparalleled capabilities in natural language processing. However, their extensive\ncomputational demands pose significant challenges, particularly in terms of cost, latency, emission\nconcerns, and cloud dependence. This has spurred interest in model optimization techniques, notably\nmodel pruning, to address these challenges.\nModel pruning, as explored by Han et al., 2015 in \u201cDeep Compression: Compressing Deep Neural\nNetworks with Pruning, Trained Quantization and Huffman Coding\u201d, has emerged as a promising\navenue for reducing neural network sizes without substantially compromising their performance. This\ntechnique involves systematically removing non-critical weights from a network, thereby reducing its\ncomplexity, size, cost, and latency. Further advancements by Frankle et al., 2018 in \u201cThe Lottery\nTicket Hypothesis: Finding Sparse, Trainable Neural Networks\u201d introduced the concept of identifying\nand training sparse subnetworks within larger models, suggesting that these \u2019lottery tickets\u2019 can\nachieve similar accuracy to their dense counterparts.\n\u2217https://github.com/tval2/contextual-pruning\nPreprint. Under review.\narXiv:2312.12682v1  [cs.CL]  20 Dec 2023\nThis paper examines the application of contextual pruning in creating Mini-GPTs, smaller yet efficient\nversions of existing LLMs. By analyzing and removing less critical weights specific to different\ndomains, such as law, healthcare, and finance, we aim to maintain or enhance model performance\nwhile significantly reducing size and resource usage. This approach stacks with those designed by\nHan et al., 2015 as synapse pruning (or connection pruning), quantization, and neural architecture\nsearch may done separately to our approach.\nThe initial motivation for pruning on context came from the realization that modern open-source\nLLMs are trained on broad datasets (e.g. Wikipedia, commercial-free books, and Reddit) but B2B\nusers are only leveraging a small fraction of the information latent in the network that\u2019s relevant to\ntheir use case. By analogy, an LLM used at a hospital doesn\u2019t need to know options trading and\nShakespeare - it just needs common sense, logical reasoning skills, and healthcare domain knowledge.\n2\nMethodology\nOur methodology for developing Mini-GPTs through contextual pruning primarily focused on linear\nlayers, activation layers, and embedding layers. We also considered various datasets and models.\nThis section highlights these choices.\n2.1\nData\nCategory\nSize (text entries)\nSource\nGeneral (used for testing only)\n4k\nwikitext-2-raw-v1\nUS Law\n10k\nlexlms\nMedical Q&A\n15k\nLaurent1/MedQuad-MedicalQnADataset\nEnglish-Taiwanese Translation\n311k\nzetavg/coct-en-zh-tw-translations-twp-300k\nSkyrim Full Transcript\n35k\nsentiment-lexicon-skyrim\nEconomics Textbook\n6k\ntinymlFP (economics_text)\nTable 1: Overview of datasets used\nOur data collection focused on diverse domains to ensure a comprehensive evaluation of our pruning\nmethodology - they are listed in Table 1. The belief is that the more dissimilar two datasets are, the\nmore differences in neuron importance we\u2019ll find (and then therefor be able to prune).\n2.2\nInitial Model Selection\nModel\nHuggingFace\nSize\nParams\nPhi-1.5\nmicrosoft/phi-1_5\n5437 MiB\n1.4B\nOpt-1.3\nfacebook/opt-1.3b\n5019 MiB\n1.3B\nLlama-1.3\nprinceton-nlp/Sheared-LLaMA-1.3B\n5144 MiB\n1.3B\nTable 2: Model selection\nWe selected GPT-like architectures due to their robustness and popularity in various NLP tasks,\nincluding machine translation and multiple choice question answering. Our base models, highlighted\nin Table 2, are pre-trained transformers built by Microsoft (Phi-1.5) or Meta (Llama-1.3 and Opt-1.3),\nand they each came with a customized Byte-Pair Encoding (BPE) tokenizer in HuggingFace.\n2.3\nContextual Analysis for Pruning\nWe conducted a detailed analysis of neuron outputs across linear layers, activation functions, and\nembeddings. This analysis helped us identify the weights that were less crucial for maintaining\nperformance in specific domains.\nContextual Analysis for Pruning:\nThis crucial step involved three types of pruning, each\ntargeting different model components:\n2\nFigure 1: Linear Layer Pruning\n2.3.1\nLinear Layer Pruning\nmj = 1\nn\nn\nX\nb=1\n||aj,b||1 < \u03f5t\n(1)\nTo contextual prune the linear layers of an LLM, we tracked the neuron outputs and calculated, for\neach dataset, the normalized L1-norm of each neuron. Equation 1 shows this where aj,b is the j-th\nneuron of batch b, mj is the j-th activation\u2019s average magnitude across batches and \u03f5t is our pruning\nthreshold.\nFigure 1 conceptually shows how this impacts pruning by looking at a basic linear layer computation.\nWhen normalized across input batches, if the L1-norm is close to the pruning threshold then we prune\nthe corresponding unused rows in the transpose weight matrix (red). Similarly, when normalized\nacross output batches we identify which columns in the transpose weight matrix to prune (since they\nare not being utilities due to neuron-synapses interaction.\nFigure 2: comparison between magnitudes of neurons between skyrim and healthcare domains\nIn Figure 2 we show example output of L1-norms for each neuron normalized for 2 datasets. Blue\nscatter points that fall below the orange line mean the neurons were activated more heavily in the first\ndataset as compared to the second dataset - and perhaps could be pruned from dataset 2.\n2.3.2\nActivation Layer Pruning\nThis pruning targeted the activation layers, where non-essential activation neurons are identified\nand removed. As shown in Figure 3, the approach is very similar to that of linear layers. One main\ndifference is that we only look at the outputs of the layer, not the inputs. The other difference is\nthat we must look to the previous layer to prune the weight from. If the normalized L1-norm of the\nactivation neuron is below the pruning threshold then we prune the corresponding column in the\ntranspose weight matrix of the prior layer. In the 3 models we looked at this was primarily done to\nGeLU and ReLU layers.\n3\nFigure 3: Activation Layer Pruning\n2.3.3\nEmbedding Layer Pruning\nFigure 4: Embedding Layer Pruning\nLastly, we created functionality to prune embeddings layers (and the corresponding LM Head layer).\nThis was done by measuring the token frequency of a particular dataset. While this approach works,\nwe found that in order to use effectively very large calibration sets are needed to provide confidence\nthat a token is truly not needed. One way to do this is to compare the token frequency curves of\ndifferent domains.\n3\nEvaluation and Results\nIn this section, we present the evaluation methodology and results of our Mini-GPTs post contextual\npruning. We used two primary metrics for evaluation: perplexity and multiple-choice question (MCQ)\ntesting.\n3.1\nPerplexity Evaluation\nPerplexity measures how well a large language model can predict the next word given a string of\ncontext and is a standard metric in determining a language model\u2019s performance. Generally, a lower\nperplexity indicates a better model. From Table 3, we generally observe a reduction or no change in\n4\nPhi-1.5\nBase\nPost prune\nFine-tune\nRecovery epochs\nRelative Size (%)\nMedical\n4.640\n4.579\n2.722\n1\n90.134\nSkyrim\n30.989\n29.728\n12.687\n1\n89.805\nEconomics\n15.165\n15.132\n6.728\n1\n97.064\nTranslation\n20.292\n20.198\n10.429\n1\n97.765\nLegal\n20.029\n19.904\n8.490\n1\n94.490\nOpt-1.3\nMedical\n3.829\n4.615\n3.203\n1\n88.369\nSkyrim\n19.777\n26.836\n8.373\n1\n89.820\nEconomics\n13.283\n16.916\n8.639\n1\n91.225\nTranslation\n17.187\n26.630\n11.994\n2\n90.619\nLegal\n14.251\n17.260\n11.444\n1\n90.427\nLlama-1.3\nMedical\n3.177\n3.177\n1.799\n1\n99.789\nSkyrim\n15.712\n15.705\n4.612\n1\n99.717\nEconomics\n8.514\n8.513\n3.535\n1\n99.760\nTranslation\n14.607\n14.606\n5.065\n1\n99.841\nLegal\n8.312\n8.312\n3.613\n1\n99.765\nTable 3: Perplexity results of pruning models with linear and activation threshold of 10\u22123 and pruning\nembeddings <= 0; Models fine-tuned until perplexity recovered, with max training epochs of 200.\nperplexity across all datasets post-pruning and fine-tuning, indicating that the models were able to\nretain much of their ability in their respective domains despite the reduction in usable parameters.\n3.2\nMultiple-Choice Question Testing\nWe further evaluated our models on 100 domain-specific MCQs to further ensure that the model\nretained its ability prior to pruning. Since only phi-1.5 could generate a string containing the correct\nanswer choice, to remain consistent across each model, a model\u2019s answer to a MCQ was selected by\npicking the question + answer string concatenation that resulted in the lowest perplexity, effectively\nusing the model\u2019s best guess. The results shown in Table 4 that the pruned models performed\ncomparably and, in some cases, better than their un-pruned versions, demonstrating the effectiveness\nof our pruning methodology.\nPhi-1.5\nBase (%)\nPost prune (%)\nFine-tune (%)\nRecovery epochs\nRelative Size (%)\nMedical\n33.000\n27.000\n25.000\n1\n90.134\nSkyrim\n62.000\n63.000\n63.000\n1\n89.805\nEconomics\n68.421\n67.368\n68.421\n1\n97.064\nTranslation\n36.000\n37.000\n38.000\n1\n97.765\nOpt-1.3\nMedical\n32.000\n25.000\n24.000\n1\n88.369\nSkyrim\n73.000\n58.000\n67.000\n1\n89.820\nEconomics\n46.316\n47.368\n51.579\n1\n91.225\nTranslation\n38.000\n35.000\n32.000\n2\n90.619\nLlama-1.3\nMedical\n30.000\n30.000\n31.000\n1\n99.789\nSkyrim\n65.000\n65.000\n63.000\n1\n99.717\nEconomics\n48.421\n49.474\n46.316\n1\n99.760\nTranslation\n46.000\n46.000\n53.000\n1\n99.841\nTable 4: MCQ accuracy results of pruning models with linear and activation threshold of 10\u22123 and\npruning embeddings <= 0; Models fine-tuned until perplexity recovered, with max training epochs of\n200.\n5\n3.3\nLarge Pruning Threshold\nTo test the limits of our pruning methodology, we also tested a linear and activation threshold of\n10\u22121.\nPhi-1.5\nBase\nPost prune\nFine-tune\nRecovery epochs\nRelative Size (%)\nMedical\n4.640\n35417.938\n4.312\n25\n58.116\nSkyrim\n30.989\n20174.240\n27.963\n21\n59.808\nEconomics\n15.165\n25619.248\n11.178\n13\n66.972\nTranslation\n20.292\n129.540\n13.671\n5\n69.069\nLegal\n20.029\n18902.793\n18.519\n11\n64.410\nOpt-1.3\nMedical\n3.829\n9559.019\n22.407\n200\n64.703\nSkyrim\n19.777\n1830.905\n19.774\n71\n64.412\nEconomics\n13.283\n7515.678\n37.525\n200\n64.957\nTranslation\n17.187\n5248.911\n36.943\n200\n63.334\nLegal\n14.251\n7545.842\n45.976\n200\n65.091\nLlama-1.3\nMedical\n3.177\n69290.547\n3.342\n200\n69.126\nSkyrim\n15.712\n3364.670\n13.635\n33\n68.098\nEconomics\n8.514\n71864.391\n8.403\n85\n68.868\nTranslation\n14.607\n53817.781\n14.074\n78\n69.451\nLegal\n8.312\n16954.877\n8.204\n45\n69.513\nTable 5: Perplexity results of pruning models with linear and activation threshold of 10\u22121 and pruning\nembeddings <= 0; Models fine-tuned until perplexity recovered, with max training epochs of 200\nFrom Table 5, we find a potential size reduction of up to 41.884% with the Phi model while recovering\nperplexity prior to pruning. Generally, however, the results indicate we are approaching the limit of\npruning for these models as Opt struggles heavily to recover perplexity prior to pruning, and Phi\nand Llama take 10s of epochs to recover where only 1 was necessary in the 10\u22123 case. Furthermore,\nlooking at the MCQ results[6] for each model, overall, we find that accuracy decreases again after\nfine-tuning while the perplexity on the fine-tuning set decreases, indicating overfitting. Further testing\nis required to determine if this can be mitigated with a larger, more representative dataset for each\ncategory or if this level of size reduction is too great entirely. The results on the much larger English to\nTaiwanese dataset suggest the former, as MCQ accuracy increased across all models after fine-tuning.\n4\nConclusion and Future Work\nOur research on Mini-GPTs through contextual pruning has shown promising results in balancing\nefficiency with performance. The significant reduction in model sizes, coupled with maintained or\nimproved accuracy in domain-specific tasks, validates our approach. For future work, we plan to\nfocus on several key areas:\n\u2022 Pruning off Max Neuron Magnitude: We aim to explore pruning based on maximum\nneuron magnitude, which might be more robust against outliers.\n\u2022 Fine Tune and Evaluate on Larger Datasets: To enhance representativeness and general-\nizability, we will fine tune our models on larger datasets and more compute power to prevent\noverfitting.\n\u2022 Combining with Other Optimization Techniques: We plan to integrate our pruning\nmethod with techniques like quantization for higher performing models.\n\u2022 Exploring Other Models: Our methodology will be applied to more up-to-date models,\nsuch as Phi-2 by Microsoft.\n6\nPhi-1.5\nBase (%)\nPost prune (%)\nFine-tune (%)\nRecovery epochs\nRelative Size (%)\nMedical\n33.000\n25.000\n25.000\n25\n58.116\nSkyrim\n62.000\n28.000\n32.000\n21\n59.808\nEconomics\n68.421\n35.789\n29.474\n13\n66.972\nTranslation\n36.000\n30.000\n33.000\n5\n69.069\nOpt-1.3\nMedical\n32.000\n32.000\n28.000\n200\n64.703\nSkyrim\n73.000\n27.000\n23.000\n71\n64.412\nEconomics\n46.316\n29.474\n21.053\n200\n64.957\nTranslation\n38.000\n30.000\n31.000\n200\n63.334\nLlama-1.3\nMedical\n30.000\n25.000\n24.000\n200\n69.126\nSkyrim\n65.000\n27.000\n30.000\n33\n68.098\nEconomics\n48.421\n21.053\n17.895\n85\n68.868\nTranslation\n46.000\n26.000\n28.000\n78\n69.451\nTable 6: MCQ accuracy results of pruning models with linear and activation threshold of 10\u22121 and\npruning embeddings <= 0; Models fine-tuned until perplexity recovered, with max training epochs of\n200\nOur research opens new avenues in domain-specific model optimization, promising wider applications\nfor LLMs in the world. This especially allows for more on-prem usage in industries such as gaming,\nhealthcare, defense, and consumer use.\nReferences\nFrankle, Jonathan and Michael Carbin (2018). \u201cThe Lottery Ticket Hypothesis: Finding Sparse,\nTrainable Neural Networks\u201d. In: eprint: arXiv:1803.03635.\nHan, Song, Huizi Mao, and William J. Dally (2015). Deep Compression: Compressing Deep Neural\nNetworks with Pruning, Trained Quantization and Huffman Coding. eprint: arXiv:1510.00149.\n7\n"
  },
  {
    "title": "Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.12487.pdf",
    "upvote": "6",
    "text": "Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models\nAngela Castillo\u22171\nJonas Kohler\u22172\nJuan C. P\u00b4erez\u22172,3\nJuan Pablo P\u00b4erez1\nAlbert Pumarola2\nBernard Ghanem3\nPablo Arbel\u00b4aez1\nAli Thabet2\n1Center for Research and Formation in Artificial Intelligence, Universidad de los Andes\n2GenAI, Meta\n3King Abdullah University of Science and Technology (KAUST)\nFigure 1.\nAccelerating Guided Diffusion Models with Adaptive Guidance: By casting diffusion guidance as a Neural Architecture Search\nproblem, we present Adaptive Guidance (AG), an efficient variant of Classifier-Free Guidance that saves 25% of total NFEs without compromising image\nquality. AG constitutes a training-free, plug-and-play alternative to Guidance Distillation that achieves 50% of its speed-ups while offering the ability to\nhandle dynamic negative prompts. As depicted above, our approach (left) replicates the baseline one-to-one and furthermore outperforms a na\u00a8\u0131ve reduction\nof diffusion steps (right).\nAbstract\nThis paper presents a comprehensive study on the role of\nClassifier-Free Guidance (CFG) in text-conditioned diffu-\nsion models from the perspective of inference efficiency. In\nparticular, we relax the default choice of applying CFG in\nall diffusion steps and instead search for efficient guidance\npolicies. We formulate the discovery of such policies in the\ndifferentiable Neural Architecture Search framework. Our\nfindings suggest that the denoising steps proposed by CFG\nbecome increasingly aligned with simple conditional steps,\nwhich renders the extra neural network evaluation of CFG\nredundant, especially in the second half of the denoising\nprocess. Building upon this insight, we propose \u201cAdaptive\nGuidance\u201d (AG), an efficient variant of CFG, that adap-\ntively omits network evaluations when the denoising pro-\n* Equal contributions.\ncess displays convergence. Our experiments demonstrate\nthat AG preserves CFG\u2019s image quality while reducing com-\nputation by 25%.\nThus, AG constitutes a plug-and-play\nalternative to Guidance Distillation, achieving 50% of the\nspeed-ups of the latter while being training-free and retain-\ning the capacity to handle negative prompts. Finally, we\nuncover further redundancies of CFG in the first half of the\ndiffusion process, showing that entire neural function eval-\nuations can be replaced by simple affine transformations of\npast score estimates. This method, termed LINEARAG, of-\nfers even cheaper inference at the cost of deviating from the\nbaseline model. Our findings provide insights into the effi-\nciency of the conditional denoising process that contribute\nto more practical and swift deployment of text-conditioned\ndiffusion models.\n1\narXiv:2312.12487v1  [cs.LG]  19 Dec 2023\n1. Introduction\nDiffusion Models (DMs) [14] exhibit outstanding gen-\nerative capacities across domains such as images [45],\nvideo [15], audio [20], human pose estimation [5], and even\ncosmological simulations [48]. DMs generate data by sam-\npling a noise instance and iteratively denoising the instance\nwith a neural network. The sequential nature of this denois-\ning operation makes sampling from DMs a slow and expen-\nsive process. In particular, the time required to sample from\na given DM is a function of (i) the latency of each denoising\niteration, and (ii) the total number of denoising steps.\nMany practical applications entail \u201cconditional generation\u201d,\nwhere DMs create samples conditioned on specific criteria\nsuch as a class, a text, or an image [37]. DMs achieve con-\nditional generation by replacing regular (i.e., unconditional)\ndenoising steps with conditional ones, in which the neural\nnetwork processes both the input and the condition. While\nconditional denoising steps provide competitive results, Ho\net al. proposed the technique of Classifier-Free Guidance\n(CFG) [13] to enhance sample quality. CFG enriches the\nconditional denoising process by leveraging implicit priors\nof the diffusion model itself. Despite its simplicity, CFG\nsignificantly improves sample quality in tasks such as text-\nto-image [8, 37, 40], image editing [4, 35, 50], and text-\nto-3D [26, 43]. Yet, the benefits of CFG come at the cost\nof duplicating the Number of Function Evaluations (NFEs),\nsince each denoising iteration requires evaluating the neural\nnetwork both conditionally and unconditionally. Adding to\nthe problem, neural networks used in practice for DMs max\nout the parallelization capacity of production-grade GPUs1,\npreventing simultaneous computation of the conditional and\nunconditional function evaluations.\nIn this paper, we improve the efficiency of text-to-image\ndiffusion models that use Classifier-Free Guidance (CFG).\nOur analysis reveals that not all denoising steps contribute\nequally to image quality, suggesting that the traditional pol-\nicy of applying CFG in all steps is sub-optimal. Instead,\nwe search for policies offering more desirable trade-offs\nbetween quality and NFEs by employing techniques from\ndifferentiable Neural Architecture Search (NAS) [29]. Our\nNAS-based search suggests unnecessary computations take\nplace in the latter part of the denoising process. We draw\nupon this finding, and propose an adaptive version of CFG\nthat we call \u201cAdaptive Guidance\u201d (AG). Our AG policy is\nan efficient variant of CFG that enjoys the image quality\nof CFG despite requiring 25% fewer NFEs. Please refer to\nFig. 1 for an illustration of the generation quality of AG.\n1By saturating memory bandwidth and/or CUDA cores. For example,\nusing bfloat16 and batch size of 1, an EMU-768 model requires 1\u2032553\nms on an A100 GPU without CFG. With CFG, latency almost doubles to\n2\u2032865 ms.\nCompared to efficiency-oriented techniques like guidance\ndistillation [36], AG is easy to implement, is training-free,\nand preserves the capacity to handle negative prompts. Fi-\nnally, we propose LINEARAG, a fast version of AG that\nestimates updates required by AG as a linear combination\nof past iterates. LINEARAG provides further reductions in\ncomputation at the cost of imperceptible losses in sample\nquality.\nIn summary, our contributions are threefold:\n\u2022 We show that techniques from gradient-based Neural Ar-\nchitecture Search (NAS) can be leveraged in the context\nof sampling from denoising diffusion models to discover\nefficient guidance policies.\n\u2022 We propose an efficient and general plug-and-play alter-\nnative to Guidance Distillation that achieves 50% of the\nspeed-ups while offering the ability to handle dynamic\nnegative prompts and image editing.\n\u2022 We discover that regularities across diffusion paths en-\nable the replacement of certain NFEs in CFG with affine\ntransformations of past iterates. This observation enables\nfurther runtime reductions and constitutes an interesting\nstarting point for future research.\nFind our full project and more resources on bcv-\nuniandes.github.io/adaptiveguidance-wp/.\n2. Related Work\n2.1. Fast Inference with Diffusion Models\nDiffusion models [14, 38, 52] achieve density estimation\nand sampling by modeling a reversible transport map T that\npushes forward a base distribution pb that is tractable (usu-\nally a standard Gaussian) to a target distribution p\u2217(x), i.e.,\nT#pb = p\u2217(x). In contrast to traditional measure trans-\nport approaches (e.g., [6, 10, 19]), diffusion models do not\nparameterize T explicitly but rather learn it implicitly from\nthe reverse direction of a gradual noising process. This ap-\nproach has the benefit of the transport T being learnable\nwithout the need for simulation. However, it also suffers\nfrom having higher inference costs due to the iterative na-\nture of the sampling process.\nThus, a large body of work has focused on producing faster\nand more efficient ways of sampling from diffusion mod-\nels. One angle of attack is the solver employed for integrat-\ning the differential equations that underlie the diffusion pro-\ncess. For example, methods based on exponential integrator\n[32, 33], higher order solvers [18, 58, 59] or model-specific\n2\nbespoke solvers [49, 60] have been proposed. Orthogonal to\nthese efforts, [51] proposes parallelizing sampling via fixed-\npoint iterations. Another common goal of exploration is re-\nducing the size of the neural network that performs denois-\ning [25, 40, 56]. For example, [56] explores ways of dis-\ntilling a large teacher network into a smaller, more efficient,\nstudent. Yet, another set of papers explores ways of reduc-\ning the size of the diffusion\u2019s latent space [12, 16, 44, 45].\nRecently, a line of research explored reformulations of the\ndiffusion process in order to reduce curvature in both the\nforward (noising) [1, 27] and backward (de-noising) trajec-\ntories [18, 21, 31, 42], which allows for larger solver steps\neven when employing lower-order solvers.\nAlong these\nlines, [47] proposes to progressively reduce the number of\ndiffusion steps by distillation.\nWithin the field of accelerating diffusion models, AutoDif-\nfusion [24] is conceptually similar to our study in the sense\nthat they employ a neural architecture search-inspired al-\ngorithm to improve the runtime of a pre-trained diffusion\nmodel. In contrast to AutoDiffusion, our method employs\na more efficient gradient-based search instead of an evolu-\ntionary one. Furthermore, we optimize per-step guidance\noptions, while AutoDiffusion focuses on time schedule and\nnetwork architecture.\n2.2. Conditioning Diffusion Paths\nFor both image generation and editing, the most challeng-\ning and practical cases involve some form of condition-\ning. Inspired by the success of class-conditioning in GANs\n(e.g., [39]), [9] proposes to enhance the estimates of the\ndiffusion probability path pt(x|c) with the gradient of an\nimage classifier p\u03b8(c|x). Similarly, [37] proposes to use\nCLIP guidance for text-to-image generation with diffusion\nmodels. Yet, both approaches are prone to adversarial out-\ncomes (i.e., degenerate solutions) and struggle with the do-\nmain shift between the noisy images of the diffusion sam-\npling process and the clean images on which the guidance\nmodels are trained.\nIn their seminal work, Ho et al. [13] show that the diffu-\nsion process can be successfully conditioned in a \u201cclassifier-\nfree\u201d manner by leveraging implicit priors of the diffusion\nmodel itself. Toward this end, Ho et al. jointly train a net-\nwork to predict both unconditional and conditional scores.\nDuring generation, the two scores are combined, giving rise\nto the technique known as CFG, to pinpoint samples with\nhigh conditional probability, as given by the inverted diffu-\nsion model as implicit classifier2. Unfortunately, by defini-\n2While implicit classifiers are generally imperfect, especially when the\nmodel does not perfectly capture the data distribution (see [11], for in-\ntion, the CFG scheme requires two, instead of one, NFEs\nper step, which doubles the sampling latency of the diffu-\nsion process on state-of-the-art models that max out GPU\nparallelization on a single sample.\nGuidance Distillation (GD) [36] elegantly mitigates the\nneed for an additional unconditional forward pass. How-\never, GD requires re-training as well as re-evaluation, both\nof which are resource-intensive.3 Moreover, this technique\ncannot handle dynamic negative prompts, which are an im-\nportant asset for responsible AI. It also does not work with\ncompositional guidance [30], which is, for instance, used\nin text-to-3D generation [43]. Finally, it is unclear how to\ngeneralize GD to multimodal conditioning employed, for\nexample, in image editing [4, 50].\nIn this work, we propose plug-and-play alternatives to\nGuidance Distillation that achieve 50% of the speed-ups at\nequal sample quality while conceptually omitting the afore-\nmentioned problems. For example, AG accommodates neg-\native prompts (in Sec. 5), image editing (in Appendix B),\nis training-free, and exactly replicates the outputs of a given\nbaseline such that no re-evaluation is needed.\n2.3. Neural Architecture Search\nNeural Architecture Search (NAS) aims at automating the\ndesign of neural network architectures by conceptualizing\nthe network as a Directed Acyclic Graph (DAG) and explor-\ning different layers as its nodes [3, 28, 41, 61, 62]. We focus\nthis review on differentiable NAS methods [22, 23, 29, 55].\nThe DARTS framework [29] is particularly relevant to our\nwork, as it introduces a continuous relaxation of the layer\nrepresentation, allowing architecture search to be differen-\ntiable and, hence, more efficient. Here, we leverage analo-\ngies between neural network design and the diffusion pro-\ncess by unrolling the diffusion process\u2019 graph in the time\ndimension, and thus considering each step as a distinct node\nin the DAG. This allows us to directly apply DARTS to\nsearch for an optimal guidance option at each node.\n3. Background on Diffusion Models\nAs introduced in Sec. 2, diffusion models generate images\nby reversing a pre-defined noising process. In particular,\nwhen the noising process is an Ornstein-Uhlenbeck pro-\ncess, the continuous time limit of the forward SDE reads\nstance), the efficacy of CFG remains unambiguously evident in practice.\n3To achieve comparable performance to CFG, Guidance Distillation on\nEMU-768 requires around 10k iterations with a batch size of 32, which\namounts to roughly four GPU days on A100.\n3\nas dx = f(x, t) dt + g(t) dw, where f(x, t) : Rd \u2192 Rd\nis a vector-valued drift coefficient, g(t) : R \u2192 R is the\ndiffusion coefficient of x(t) and w is standard Brownian\nmotion. Anderson\u2019s Theorem [2] states that, under mild as-\nsumptions, this SDE satisfies a reverse-time process:\ndx =\n\u0002\nf(x, t) \u2212 g(t)2\u2207x log pt(x)\n\u0003\ndt + g(t) d\u00afw,\n(1)\nwhere \u00afw is the reverse-time Brownian motion. As shown\nin [17, 54], the marginal transport map can be learned (in\nexpectation) by maximum likelihood estimation of the\nscores of individually diffused data samples \u2207x log pt(x)\nin a simulation-free manner. This map is commonly learnt\nby optimizing the parameters \u03b8 of a time-conditioned neural\nnetwork that produces score estimates \u03f5\u03b8(xt, t).4\nAs shown in [54], the SDE in Eq. (1) has a deterministic\ncounterpart (i.e., an ODE) that enjoys equivalent marginal\nprobability densities:\ndx =\n\u0014\nf(x, t) \u2212 1\n2g(t)2\u2207x log pt(x)\n\u0015\ndt.\n(2)\nSolving Eq. (2) generally yields better results when fewer\ndiscretization steps are taken [18].\nConditional generation with diffusion models.\nThe dif-\nfusion framework can be extended to allow for conditional\ngeneration by learning the score log pt(x|c), where c is, for\nexample, a class- or text-condition. Current state-of-the-art\nmodels for conditional generation employ \u201cClassifier-Free\nGuidance\u201d (CFG) [13], a technique in which both the con-\nditional and unconditional scores are linearly combined to\ndenoise the sample. In particular, CFG proposes to follow\nthe score estimate given by\n\u03f5cfg(xt, c, s) = \u03f5\u03b8(xt, \u2205) + s \u00b7 (\u03f5\u03b8(xt, c) \u2212 \u03f5\u03b8(xt, \u2205)), (3)\nwhere \u2205 is the unconditional prompt token, and s > 1\nindicates the guidance strength.\nWhile this new score\nmay not directly reflect the gradient of a classifier\u2019s\nlog-likelihood, it is inspired by the gradient of an im-\nplicit classifier p\u2032(c|x)\n\u221d\np(x|c)/p(x).\nAs a result,\n\u2207x log p(c|x) \u221d \u2207x log p(x|c) \u2212 \u2207x log p(x) and hence\n\u03f5\u03bb(xt, c) \u221d \u03f5(xt, \u2205) + s \u00b7 \u2207x log p(x|c). In that sense, CFG\nshifts probability mass toward data where an implicit clas-\nsifier p\u2032(c|x) assigns a high likelihood to the condition c.\nNotably, evaluating Eq. (3) introduces an extra NFE com-\npared to unguided sampling, which may up to double the\nlatency. Next, we search for efficient ways of guiding the\ndenoising process, aiming at reducing NFEs while retaining\nthe benefits of CFG. In the following sections, we discuss\nthese approaches along with their respective results.\n4For brevity\u2019s sake, we omit the conditioning of \u03f5 on t going forward.\n4. Gradient Search along Diffusion Dynamics\nDesign space for guided diffusion steps.\nWe assume ac-\ncess to a pre-trained diffusion model \u03a6 : X \u00d7 C \u2192 X\nworking in latent space X = RH\u00d7W \u00d7C and condition\nspace C, where c \u2208 C is a condition, e.g., a text prompt.\nInitializing xT \u223c pb, where pb represents a Gaussian dis-\ntribution, setting a condition c \u2208 C and a time-schedule\n\u03c4 = {T, T \u22121, ..., 0}, the diffusion model builds a sequence\nof latent codes\n{xt}T\nt=0 s.t. xT \u223c pb, xt\u22121 = \u03a6(solver(\u00afxt)),\n(4)\nwhere solver represents an ODE solver for Eq. (2). The\nmodel \u03a6 operates under classifier-free guidance as given\nin Eq. (3), i.e., \u00afxt = \u03f5cfg(xt, c, s) and s is constant over\ntime. While this setup is the default in most popular dif-\nfusion models [8, 37, 45, 46], we highlight that multiple\nalternatives exist for \u00afxt at any given t, each associated with\ndifferent computational costs:\n\u2022 Unconditional score:\n\u03f5\u03b8(xt, \u2205)\n(1 NFE)\n\u2022 Conditional score:\n\u03f5\u03b8(xt, c)\n(1 NFE)\n\u2022 CFG score:\n\u03f5cfg(xt, c, st)\n(2 NFEs)\nHere, \u03f5\u03b8 represents a neural network parameterized by\nfrozen weights \u03b8, and st is no longer constant in time.\nDenote by ft the particular step choice at time t with\nft \u2208 Ft = {\u03f5\u03b8(xt, \u2205), \u03f5\u03b8(xt, c), \u03f5cfg(xt, c, st)} . Then, the\nsearch space for the complete diffusion process is given by:\nS = QT\nt=0 Ft, where the product symbol denotes the Carte-\nsian product over sets.\nAs a result, S is the set of all possible sequences of choices\n\u03b6 = (f0, f1, . . . , fT ), which we henceforth refer to as poli-\ncies.\nClearly, S is unbounded as long as st \u2208 R.\nAl-\nthough this fact is not problematic in itself for gradient-\nbased search, we constrain st to be in a bounded and finite\nset S = {s1, ..., sk} in order to obtain simpler and more\ngeneralizable policies. As a result, the search spaces con-\ntain a total of |S| = | QT\nt=0 Ft| = (2 + k)T +1 different\npolicies.\nEnabling backpropagation with soft alphas.\nSearching\nS for policies with a good performance-latency trade-off\nconstitutes a large-scale combinatorial problem, especially\nsince T is usually in the range of 20 to 50. Thus, inspired\nby the literature on NAS, we relax the discrete search into\na continuous one. This decision allows for effectively using\ngradients to navigate the high-dimensional search space. In\nparticular, for each set of choices Ft, we introduce a train-\nable vector \u03b1t \u2208 Rk+2 and obtain the solver input as a\n4\nAG (ours)\n\u00af\u03b3 = \u221e (40NFEs)\n\u00af\u03b3 = 0.993 (32NFEs)\n\u00af\u03b3 = 0.991 (30NFEs)\nZoom of image to the left\nCFG\nsteps = 20 (40NFEs)\nsteps = 16 (32NFEs)\nsteps = 15 (30NFEs)\nZoom of image to the left\nFigure 2. Adaptive Guidance (AG) vs. Classifier-Free Guidance (CFG) for multiple Number of Function Evaluations (NFEs). For\nAG we keep the number of denoising iterations constant but reduce the number of steps using CFG by increasing the threshold \u00af\u03b3 (top). CFG simply reduces\nthe total number of diffusion steps (bottom). Vertically aligned samples require the exact same number of NFEs. As can be seen, AG replicates the baseline\nvery closely while CFG with less steps introduces artifacts.\nsoftmax weighting of the individual options\n\u00afxt := softmax (\u03b1)\u22ba Ft\n(5)\nOnce trained, the score matrix \u03b1 := [\u03b1\u22ba\nT , ..., \u03b1\u22ba\n0] repre-\nsents a multinominal distribution over the per-iteration op-\ntions (FT , ..., F0) from which we can sample concrete poli-\ncies \u03b6. In the following, we define a differentiable objective\nto guide our search for efficient and effective guidance poli-\ncies.\nSearch objective.\nWe seek a policy \u03b6 that gives rise to a\ndiffusion model that replicates \u03a6 as closely as possible, as\nquantified by a differentiable metric d : X \u00d7 X \u2192 [0, \u221e)\nthat measures the distance between the endpoints of the two\ndiffusion paths (x0\u2032 and x0, respectively). Our goal is to\nachieve replication with fewer NFEs than the reference pol-\nicy ft = \u03f5cfg(xt, c, s), \u2200t. Towards this end, we optimize:\n\u03b1\u2217 = argmin\u03b1 [d(x0, x\u2032\n0(\u03b6(\u03b1)) + \u03bbg(\u03b6(\u03b1))] ,\n(6)\nwhere \u03bb\n>\n0 and g(\u03b6(\u03b1)) regularizes the sum of\nthe scores obtained by passing \u03b1 through a Gumbel-\nsoftmax [34] weighted by the per-choice costs (1 for uncon-\nditional/conditional steps and 2 for CFG steps with st > 1).\nThus, g represents a (differentiable) proxy for the total\nNFEs of the policy \u03b6(\u03b1). We employ a ReLU offset to a\ntarget cost limit \u00afc under which no penalty is employed.\nFor the policy search, we initialize \u03b1 as i.i.d. uniform ran-\ndom variables. Subsequently, in each training iteration, we\nsample xT \u223c N(0, I) and use our baseline model \u03a6 to\ngenerate a target image x0. The same starting noise tensor\nis then being fed through a student model \u03a6\u2032 that mimics \u03a6\nbut employs a soft alpha-weighted forward pass according\nto Eq. (5) to obtain x\u2032\n0(\u03b6(\u03b1)). Given these two images, we\ncompute the differentiable loss in Eq. (6) and backpropagate\nthrough \u03a6\u2032 w.r.t. \u03b1.5\n4.1. Experimental Setup\nWe perform our guidance search in the context of text-to-\nimage generation using the popular Stable Diffusion archi-\ntecture [45], which we refer to as LDM-512.6 This model\nhas 900M parameters and generates images at a 512 \u00d7 512\nresolution via a latent space of shape 4\u00d764\u00d764. To show-\ncase that our findings generalize beyond the model they\nwere searched on, we validate the found policies on a state-\nof-the-art EMU model [8], which we refer to as EMU-768.\nThis model has 2.7B parameters, produces photorealistic\nimages at a resolution of 768 \u00d7 768, and uses a latent space\n5To cope with limited memory resources, we re-run certain forward-\npass segments during backward (\u201cactivation checkpointing\u201d).\n6We train LDM-512 from scratch on a commissioned dataset of images.\n5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nStep Index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScore\n--------\n-------\n-------------\nFigure 3. Search results. Average scores and standard deviations over\nsteps in the diffusion process for the different guidance options. The 30\nbest searches are represented. As can be seen, CFG is most important in\nthe beginning, and the score decreases over time.\nof shape 16 \u00d7 96 \u00d7 96.\nFor training, we generate 10, 000 noise-image pairs ran-\ndomly selected from the CC3M dataset [53] using our\nLDM-512 with T = 20 DPM++ [32] solver steps and a\nfixed guidance strength of s = 7.5. In our search space S\nwe include k = 3 guidance strengths, which gives a total of\nfive discrete choices: unconditional \u03f5\u03b8(xt, \u2205), conditional\n\u03f5\u03b8(xt, c), as well as \u03f5cfg(xt, c, a\u00b77.5) for a \u2208 { 1\n2, 1, 2}. We\noptimize Eq. (6) with the Lion optimizer [7] for 5 epochs.\nAll evaluation metrics are computed on a subset of 1, 000\nprompts from the Open User Input (OUI) dataset [8].\n4.2. Search Results\nUpon completion of our search, we find that the best-\nperforming policies focused essentially on three guidance\nchoices: conditional, unconditional, and CFG with s =\n7.5.7 The score distribution of these policies is summarized\nin Figure 3. Notably, a distinct pattern emerges in the search\nresults: namely, the importance assigned to CFG is high\nin the first half of the denoising process but drops signifi-\ncantly in the second half. This fact follows intuition: text-\nconditioning is particularly important for determining the\noverall semantic structure of the image, and this semantic\nstructure is set up early on in the diffusion process, while the\nlater steps focus more on generating local information and\nhigh-frequency details (see e.g., Fig. 17 in the Appendix).\nInterestingly, this generative structure is mirrored in the in-\nner workings of the diffusion process.\nNamely, the co-\nsine similarity \u03b3t between the conditional (\u03f5(xt, c)) and un-\n7In hindsight, this is not surprising as replicating a baseline model re-\nquires following the entire diffusion trajectory. Yet, by the design of the\nmeasured transport T, paths cannot cross, and there is no way of returning\nto the baseline once stepped off using a different guidance scale.\n0\n5\n10\n15\n20\nStep index\n0.84\n0.88\n0.92\n0.96\n1.00\nCosine similarity\nLDM-512\nEMU-768\n0\n5\n10\n15\n20\nStep index\n0.96\n0.97\n0.98\n0.99\n1.00\nCosine similarity\nthres = 0.991\nFigure 4. Cosine similarities over time. Left: Average cosine sim-\nilarity \u03b3 and 99% confidence interval over 1, 000 IOU prompts for EMU\nand LDM. Right: Zoom to y-values in [0.955, 1.0] for 21 EMU samples.\nconditional (\u03f5(xt, \u2205)) network predictions increases almost\nmonotonically over time. As shown in Fig. 4, \u03b3t achieves\nalmost perfect alignment towards the end of the diffusion\nprocess. That is, we empirically observe\nlim\nt\u21920\n\u0014\n\u03b3t :=\n\u03f5\u03b8(xt, c) \u00b7 \u03f5\u03b8(xt, \u2205)\n\u2225\u03f5\u03b8(xt, c)\u2225\u2225\u03f5\u03b8(xt, \u2205)\u2225\n\u0015\n= 1.\n(7)\nIn light of this finding, AG works because it stops guid-\ning precisely when the conditional and unconditional up-\ndate steps have converged, and guiding hence no longer in-\ntroduces shifts in direction.\n5. Adaptive Guidance\nDefinition.\nSection 4.2 found that the conditional and un-\nconditional updates become increasingly correlated over\ntime. This fact suggests an intuitive way to save NFEs by\nstopping CFG computation when this correlation is high.\nWe thus expand on this intuition to propose \u201cAdaptive Guid-\nance\u201d (AG), a principled technique to decrease sampling\ncost while maintaining high image quality. In particular,\nAG adaptively switches from CFG updates to (cheaper) con-\nditional updates when \u03b3t (Eq. (7)) exceeds a threshold \u00af\u03b3,\nwhere \u00af\u03b3 \u2208 [0, 1] is the only hyper-parameter of AG. As a\nresult, AG results in uncomplicated policies such as\n\u03b6AG = [\u03f5cfg(xT , c), ..., \u03f5cfg(xt, c), \u03f5\u03b8(xt\u22121, c), ..., \u03f5\u03b8(x0, c)],\nwhere the truncation point t is a function of \u00af\u03b3, the starting\nseed xT and the conditioning c.\nWe highlight that \u03b6AG is independent of the particular time\nschedule \u03c4 (Eq. (4)) and solver used for the sampling pro-\ncess.\nHence, it can be within a wide class of diffusion\nmodels and for arbitrary numbers of diffusion steps. Im-\nportantly, as shown in Fig. 4, the cosine similarity trend\nfound on LDM-512 generalizes to the much larger EMU-\n768 model.\n6\n24\n26\n28\n30\n32\n34\n36\n38\n40\nNFEs\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSSIM\nSearched policies\nCFG\nAG = 0.9996\nAG = 0.9994\nAG = 0.9990\nFigure 5. Search results: SSIM of different searched policies (dots)\ncompared to the 20 step CFG baseline on LDM-512. Also depicted are\nresults of AG for different truncation threshold \u00af\u03b3 (dashed lines) as well\nCFG with na\u00a8\u0131ve step reduction (solid lines). The total number of steps\nreduces from right to left. As can be seen, AG is strictly better at replicating\nthe baseline than a na\u00a8\u0131ve step reduction. Furthermore, it matches most\nindividual searched policies, while being simpler and scalable.\nEMU-756\nSSIM \u2191\nWin \u2191\nLose \u2193\nNFEs \u2193\nCFG\n0.91 \u00b1 0.03\n502\n498\n40\nAG \u00af\u03b3 = 0.991\n498\n502\n29.6 \u00b1 1.3\nTable 1. Evaluation results. Comparison of AG (\u00af\u03b3 = 0.991, ap-\nproximately 30 NFE) and the 20 step (i.e. 40 NFE) CFG baseline. Avg.\nSSIM and majority voting of five trained human evaluators, each voting\non 1, 000 images generated from OUI prompts. AG achieves equal visual\nquality despite using 25% fewer NFEs.\nQuantitative evaluation.\nWe compare AG and CFG w.r.t.\ntheir capacity to reconstruct a set of 1, 000 target images\n(computed from a baseline model with 20 CFG steps, i.e.,\n40 NFEs). We run this experiment for LDM-512 and report\nresults in Figure 5 for various values of \u00af\u03b3. We find that AG\ncan replicate the target images to higher accuracy than CFG\nfor the entire regime we considered (from 22 to 40 NFEs).\nAgain, as detailed in Appendix A, these findings generalize\nto the much larger EMU-768 model.\nQualitative evaluation.\nFigure 2 depicts samples gener-\nated with AG for different \u00af\u03b3 values. Our results suggest that\nup to 50% of the diffusion steps can be performed without\nCFG at no cost to image quality. Moreover, Figures 1 and\n2 showcase samples where AG outperforms the na\u00a8\u0131ve alter-\nnative of reducing the total number of diffusion steps.\nHuman evaluation.\nWe further validate AG\u2019s capacity to\ngenerate images of CFG-level quality via a thorough human\nevaluation. Our assessment involved five trained human an-\nnotators, who were tasked with voting for the most visually\nappealing instance from a pair of images. One image was\n\u201cA whale breaching near a mountain\u201d (win)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (30NFEs)\n\u201cA red laptop with earbuds sitting on a table\u201d (lose)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (29NFEs)\nFigure 6. Human evaluation examples. An exemplary sample for\nwin (top) and lose (bottom) of AG with \u00af\u03b3 = 0.991 vs. CFG. The baseline\nCFG tends to produce higher frequencies, which can be for better (bottom)\nor worse (top). We report further examples in Appendix A.\nsampled from CFG (with 40 NFEs), and the other from AG\nwith \u00af\u03b3 = 0.991, inducing approximately a 25% reductionin\nNFEs. We ran this test on 1, 000 prompts from the OUI\ndataset and report results in Table 1. Statistical analysis re-\nvealed a mean difference in votes of \u22120.047 (SD = 2.543),\nindicating no significant overall preference. The distribu-\ntion of votes was nearly even, with AG favored in 498 cases\nand CFG in 502 cases (majority voting). We further con-\nducted a two-sided Wilcoxon Signed-Rank Test, yielding a\np-value of 0.603, with a test statistic of 244, 590, indicating\nno significant difference in visual appeal between the two\nmodels (p > 0.05). These findings suggest that, despite\nthe efficiency of AG, the generated images are of compara-\nble quality to CFG, as judged by human annotators search-\ning for visual aesthetics. Figure 13 depicts some exemplary\nsamples of this evaluation.\n5.1. Replacing NFEs with Affine Transformations\nIn the previous sections, we found that, in the latter stages\nof denoising, CFG updates can be replaced with conditional\nsteps. Yet, for the first half of the denoising steps, guidance\nis of particular importance. Indeed, as shown in the first\ncolumn of Fig. 8, reducing the number of guidance steps\n7\n\u201cA delicious chocolate cake with fruits\u201d\nCFG\n(40NFEs)\n-\u201craspberry\u201d CFG\n(40NFEs)\n-\u201craspberry\u201d AG\n(30NFEs)\nFigure 7. Negative prompts. As can be seen, AG produces similar\nresults to CFG when using non-empty negative prompts, again highlighting\nthe importance of only the first T\n2 diffusion steps for semantic structure.\nMore such examples can be found in Figure 11 in the Appendix.\nto as few as five (followed by 15 conditional steps) signifi-\ncantly degrades image quality. At the same time, the smooth\nalignment of conditional and unconditional steps over time\nand the high concentration around the mean of the cosine\nsimilarities depicted in Fig. 4 suggest a high regularity in\ndiffusion paths.\nIntrigued by this observation, we probe\nfor linear patterns in the diffusion path. Indeed, we find\nthat unconditional network evaluations \u03f5(xt, \u2205) can often\nbe estimated with high accuracy via affine transformations\nof network evaluations at previous iterations. To compute\nthe parameters of these affine transformations, we generate\na small dataset of 200 images from EMU-768 and store the\nintermediate iterations. Subsequently, we model a given un-\nconditional step at any t < T as a linear combination of the\nprevious iterations in the diffusion chain as\n\u02c6\u03f5(xt, \u2205) =\nt\nX\ni=T\n\u03b2c\ni \u03f5\u03b8(xi, c) +\nt+1\nX\ni=T\n\u03b2\u2205\ni \u03f5\u03b8(xi, \u2205),\n(8)\nwhere \u03b2c\ni and \u03b2\u2205\ni are scalars. We learn these Linear Regres-\nsion (LR) coefficients for each step by solving a simple Or-\ndinary Least Squares problem on the set of 200 trajectories.\nTogether with the time required for generating the dataset,\nwe obtain LR coefficients for all steps in under 20 minutes.\nDuring sampling, computing \u02c6\u03f5(xt, \u2205) is essentially for free.\nPerhaps surprisingly, we find that this estimator displays\nremarkable capacity to predict unconditional steps.\nOf\ncourse, for any unconditional score replaced by an LR pre-\ndictor, the following denoising step will no longer have\nground truth past information and errors accumulate auto-\nregressively.\nYet, interleaving CFG steps with \u201capproxi-\nmated\u201d CFG steps (where the \u03f5\u03b8(xt, \u2205) is replaced by its\nlinear estimator \u02c6\u03f5(xt, \u2205)), reduces the rate of error accumu-\nlation. We term this strategy LINEARAG. When T = 20,\nLINEARAG performs ten steps, alternating between CFG (2\nNFEs) and LR-based CFG (1 NFE), followed by ten LR-\nbased CFG steps. More details can be found in App. C.\nAs depicted in Fig. 8, LINEARAG drastically improves im-\n\u201cA happy cow in the Swiss alps\u201d\n\u201cA traditional tea house in a garden with cherry blossom trees\u201d\nAG \u00af\u03b3 = 0.975\n(25NFEs)\nna\u00a8\u0131ve interleaving CFG\n(25NFEs)\n... LINEARAG ...\n(25NFEs)\nFigure 8. Replacing CFG in the first half of diffusion steps.\nThree different approaches to reduce the number of NFEs in the first 50%\nof diffusion steps. As can be seen, LINEARAG samples show increased\nsharpness, dynamic lightning with higher contrast, and more vivid colors.\nage quality over AG with very low \u00af\u03b3. Furthermore, it shows\nthat the LR successfully recognizes patterns along the path\nsince it compares favorably to the na\u00a8\u0131ve alternative of sim-\nply alternating between CFG and conditional steps for the\nfirst half (followed by T\n2 conditional steps). Finally, as re-\nported in Figures 7 and 11, LINEARAG can even handle\nnegative prompts to a certain extent.\n6. Conclusions\nIn this paper, we have leveraged the gradient-based NAS\nframework to bring about principled insights into the de-\nnoising process of conditional diffusion models.\nIn par-\nticular, we found that Classifier-Free Guidance involves\ncomputational redundancies of different sorts in both the\nfirst and second parts of the diffusion process. Leveraging\nthese insights, we first proposed Adaptive Guidance, a very\ngeneral and efficient plug-and-play variant of CFG that is\nable to closely replicate a baseline model while reducing\nthe number of NFEs needed for guidance by up to 50%.\nCompared to Guidance Distillation, AG is training-free, ex-\ntremely easy to implement and it offers considerable flex-\nibility, for example when it comes to negative prompts or\nimage editing.\nSecond, we proposed an even faster variant of CFG, termed\nLINEARAG, that increases the guidance NFE savings of AG\nto 75% by replacing entire network evaluations with sur-\nprisingly simple linear transformations of past information.\n8\nHowever, these extra runtime savings come at the price of\nLINEARAG no longer replicating the baseline one-to-one,\nwhich entails the need for extensive evaluations. As such,\nLINEARAG is to be considered more as a proof of concept\nas well as an interesting starting point for future research\non finding ways to effectively leverage smoothness in and\nregularity across diffusion paths for efficient inference.\nReferences\n[1] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-\nEijnden.\nStochastic interpolants: A unifying framework\nfor flows and diffusions. arXiv preprint arXiv:2303.08797,\n2023. 3\n[2] Brian DO Anderson. Reverse-time diffusion equation mod-\nels. Stochastic Processes and their Applications, 12(3):313\u2013\n326, 1982. 4\n[3] Andrew Brock, Theodore Lim, James M Ritchie, and Nick\nWeston. Smash: one-shot model architecture search through\nhypernetworks. arXiv preprint arXiv:1708.05344, 2017. 3\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n2, 3, 1, 4\n[5] Angela Castillo, Maria Escobar, Guillaume Jeanneret, Al-\nbert Pumarola, Pablo Arbel\u00b4aez, Ali Thabet, and Artsiom\nSanakoyeu.\nBodiffusion:\nDiffusing sparse observations\nfor full-body human motion synthesis.\narXiv preprint\narXiv:2304.11118, 2023. 2\n[6] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and\nDavid K Duvenaud.\nNeural ordinary differential equa-\ntions. Advances in neural information processing systems,\n31, 2018. 2\n[7] Xiangning Chen, Chen Liang, Da Huang, Esteban Real,\nKaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang\nLuong, Cho-Jui Hsieh, et al.\nSymbolic discovery of op-\ntimization algorithms.\narXiv preprint arXiv:2302.06675,\n2023. 6\n[8] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807, 2023. 2, 4, 5, 6\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 3\n[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-\ngio.\nDensity estimation using real nvp.\narXiv preprint\narXiv:1605.08803, 2016. 2\n[11] Yves Grandvalet and Yoshua Bengio.\nSemi-supervised\nlearning by entropy minimization. Advances in neural in-\nformation processing systems, 17, 2004. 3\n[12] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 10696\u201310706, 2022. 3\n[13] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2, 3, 4\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[16] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. The Journal of\nMachine Learning Research, 23(1):2249\u20132281, 2022. 3\n[17] Aapo Hyv\u00a8arinen and Peter Dayan.\nEstimation of non-\nnormalized statistical models by score matching.\nJournal\nof Machine Learning Research, 6(4), 2005. 4\n[18] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:26565\u201326577, 2022. 2, 3, 4\n[19] Durk P Kingma and Prafulla Dhariwal. Glow: Generative\nflow with invertible 1x1 convolutions. Advances in neural\ninformation processing systems, 31, 2018. 2\n[20] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. Diffwave: A versatile diffusion model for\naudio synthesis. arXiv preprint arXiv:2009.09761, 2020. 2\n[21] Sangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing\ntrajectory curvature of ode-based generative models. arXiv\npreprint arXiv:2301.12003, 2023. 3\n[22] Guohao Li, Guocheng Qian, Itzel C Delgadillo, Matthias\nMuller, Ali Thabet, and Bernard Ghanem. Sgas: Sequential\ngreedy architecture search. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1620\u20131630, 2020. 3\n[23] Guohao Li, Mengmeng Xu, Silvio Giancola, Ali Thabet, and\nBernard Ghanem. Lc-nas: Latency constrained neural archi-\ntecture search for point cloud networks. In 2022 Interna-\ntional Conference on 3D Vision (3DV), pages 1\u201311. IEEE,\n2022. 3\n[24] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao,\nRui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji.\nAutodiffusion: Training-free optimization of time steps and\narchitectures for automated diffusion model acceleration. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7105\u20137114, 2023. 3\n[25] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,\nYun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-\nfusion: Text-to-image diffusion model on mobile devices\nwithin two seconds. arXiv preprint arXiv:2306.00980, 2023.\n3\n[26] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\n9\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2\n[27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matt Le. Flow matching for generative mod-\neling. arXiv preprint arXiv:2210.02747, 2022. 3\n[28] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon\nShlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan\nHuang, and Kevin Murphy. Progressive neural architecture\nsearch. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 19\u201334, 2018. 3\n[29] Hanxiao\nLiu,\nKaren\nSimonyan,\nand\nYiming\nYang.\nDarts: Differentiable architecture search.\narXiv preprint\narXiv:1806.09055, 2018. 2, 3\n[30] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and\nJoshua B Tenenbaum. Compositional visual generation with\ncomposable diffusion models. In European Conference on\nComputer Vision, pages 423\u2013439. Springer, 2022. 3\n[31] Xingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow\nstraight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003, 2022. 3\n[32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. Advances\nin Neural Information Processing Systems, 35:5775\u20135787,\n2022. 2, 6\n[33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 2\n[34] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The\nconcrete distribution: A continuous relaxation of discrete\nrandom variables. arXiv preprint arXiv:1611.00712, 2016.\n5\n[35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 2, 1\n[36] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 14297\u201314306, 2023. 2, 3\n[37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2, 3, 4\n[38] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 2\n[39] Augustus Odena, Christopher Olah, and Jonathon Shlens.\nConditional image synthesis with auxiliary classifier gans. In\nInternational conference on machine learning, pages 2642\u2013\n2651. PMLR, 2017. 3\n[40] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195\u20134205,\n2023. 2, 3\n[41] Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and\nJeff Dean. Efficient neural architecture search via parameter\nsharing. arXiv preprint arXiv:1802.03268, 2018. 3\n[42] Aram-Alexandre\nPooladian,\nHeli\nBen-Hamu,\nCarles\nDomingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky\nChen. Multisample flow matching: Straightening flows with\nminibatch couplings.\narXiv preprint arXiv:2304.14772,\n2023. 3\n[43] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2, 3\n[44] Dominic\nRampas,\nPablo\nPernias,\nElea\nZhong,\nand\nMarc Aubreville.\nFast text-conditional discrete denois-\ning on vector-quantized latent spaces.\narXiv preprint\narXiv:2211.07292, 2022. 3\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 4, 5\n[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 4\n[47] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 3\n[48] Andreas Schanz, Florian List, and Oliver Hahn. Stochastic\nsuper-resolution of cosmological simulations with denoising\ndiffusion models. arXiv preprint arXiv:2310.06929, 2023. 2\n[49] Neta Shaul, Juan Perez, Ricky TQ Chen, Ali Thabet, Albert\nPumarola, and Yaron Lipman. Bespoke solvers for genera-\ntive flow models. arXiv preprint arXiv:2310.19075, 2023.\n3\n[50] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain,\nAmit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman.\nEmu edit: Precise image editing via recognition and genera-\ntion tasks. arXiv preprint arXiv:2311.10089, 2023. 2, 3, 1,\n4\n[51] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh,\nand Nima Anari.\nParallel sampling of diffusion models.\narXiv preprint arXiv:2305.16317, 2023. 3\n[52] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n2\n[53] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.\nMass: Masked sequence to sequence pre-training for lan-\nguage generation. arXiv preprint arXiv:1905.02450, 2019.\n6\n[54] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\n10\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 4\n[55] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\nJia, and Kurt Keutzer. Fbnet: Hardware-aware efficient con-\nvnet design via differentiable neural architecture search. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 10734\u201310742, 2019. 3\n[56] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang.\nDiffusion probabilistic model made slim. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 22552\u201322562, 2023. 3\n[57] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicbrush: A manually annotated dataset for instruction-\nguided image editing.\narXiv preprint arXiv:2306.10012,\n2023. 1\n[58] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffu-\nsion models with exponential integrator. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n2\n[59] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and\nJiwen Lu.\nUnipc:\nA unified predictor-corrector frame-\nwork for fast sampling of diffusion models. arXiv preprint\narXiv:2302.04867, 2023. 2\n[60] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-\nsolver-v3: Improved diffusion ode solver with empirical\nmodel statistics. In Thirty-seventh Conference on Neural In-\nformation Processing Systems, 2023. 3\n[61] Barret Zoph and Quoc Le. Neural architecture search with\nreinforcement learning.\nIn International Conference on\nLearning Representations, 2016. 3\n[62] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8697\u20138710,\n2018. 3\n11\nAdaptive Guidance: Training-free Acceleration of Conditional Diffusion Models\nSupplementary Material\n24\n26\n28\n30\n32\n34\n36\n38\n40\nNFEs\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSSIM\nAG = 0.995\nAG = 0.992\nAG = 0.990\nCFG\nFigure 9. AG vs CFG: SSIM (mean and 95% CI) compared to\nthe 20 step CFG baseline on EMU-768. Depicted are results of\nAG for different truncation threshold \u00af\u03b3 (dashed lines) as well CFG\nwith na\u00a8\u0131ve step reduction (solid lines). The total number of steps\nreduces from right to left. As can be seen, AG is strictly better at\nreplicating the baseline than a na\u00a8\u0131ve step reduction. Similar results\nfor LDM-512 can be found in Fig. 5\nA. Evaluations on EMU-768\nAs stated in the main paper, our policy search was per-\nformed on the LDM-512 model. Importantly, we find that\nthe resulting adaptive guidance policies generalize to the\nmuch bigger and more powerful EMU-768 model. For ex-\nample 9, similar to Fig. 5 on LDM, shows that AG scales\nmore favorably than CFG for different numbers of NFEs on\nEMU.\nFor our human evaluation results, we generated images us-\ning 20 CFG steps as well as 20 AG steps with \u00af\u03b3 = 0.991,\nwhich gave rise to an average of 29.6 NFEs (that is, the av-\nerage sample was generated with around 10 guided steps,\nfollowed by 10 unguided (conditional) steps). We used the\nsame seed sequence for both models on a subset of 1000\nprompts from OUI.\nAfter generation, for each prompt, the images of both mod-\nels were shown side-by-side to a random subset of 5 out of\na pool of 42 trained human evaluators. The order of the im-\nages was also random. Annotators had to vote for higher\nvisual appeal. There was no tie option to incentivize active\nengagement. The vote distribution was symmetric around\nzero (see Fig. 10. Hence, no significant difference in the\nmodel performance can be found by paired difference tests.\n4\n2\n0\n2\n4\nDifferences (AG votes - CFG votes)\n0\n50\n100\n150\n200\n250\nFrequency\nFigure 10. Voting distribution. Distribution of voting difference\nof five annotators for AG vs.\nCFG for 1000 samples. As can\nbe seen, the distribution is very symmetric around zero. Hence,\npaired difference tests can find no significant difference in the\nmodel performance.\nB. Image editing\nA large body of works proposes to use text-to-image models\nnot only for generation of novel images but also for instruc-\ntion based editing of existing ones (e.g., [4, 35, 50, 57]).\nOne particularly successful approach within this realm,\ntermed InstructPix2Pix [4], achieve successful image edit-\ning by augmenting the CFG paradigm to not only text but\nimage and text conditioning, giving rise to the modified\nscore estimate\n\u03f5pix2pix(xt, c, I) =\u03f5\u03b8(xt, \u2205, \u2205)\n+ sc \u00b7 (\u03f5\u03b8(xt, c, I) \u2212 \u03f5\u03b8(xt, \u2205, I))\n+ sT \u00b7 (\u03f5\u03b8(xt, \u2205, I) \u2212 \u03f5\u03b8(xt, \u2205, \u2205)).\n(9)\nThis has two important implications. First, a single step\nin the diffusion process now requires 3 instead of 2 NFEs.\nSecond, Guidance Distillation can no longer be applied as\npart of the \u201cunconditional\u201d update step is now dynamic\n(i.e., I changes across samples, akin to the case of negative\nprompts).\nBoth effects are unfortunate as fast generation is particularly\nrelevant in the image editing context, where users may want\nto try various instructions in sequence. Interestingly, we\nfind that \u2013 similar to the case of simple text conditioning\n\u2013 the terms in Eq. 9 converge over time. Hence, as shown\n1\n\u201cA Tuscany villa\u201d (success)\nCFG (40NFEs)\n-\u201cswimmingpool\u201d CFG (40NFEs)\n-\u201cswimmingpool\u201d AG (30NFEs)\n-\u201csw..pool\u201d LINEARAG (25NFEs)\n\u201cA healthy bowl of salad\u201d (success)\nCFG (40NFEs)\n-\u201ctomatoes\u201d CFG (40NFEs)\n-\u201ctomatoes\u201d AG (30NFEs)\n-\u201ctomatoes\u201d LINEARAG (25NFEs)\n\u201cAn Italian pizza\u201d (failure)\nCFG (40NFEs)\n-\u201craspberry\u201d CFG (40NFEs)\n-\u201craspberry\u201d AG (30NFEs)\n-\u201craspberry\u201d LINEARAG (25NFEs)\nFigure 11. More negative prompts. More examples of using negative prompts with adaptive and linear adaptive guidance. The top and middle rows\nshow successful examples. The bottom row shows a failure case. Importantly, standard CFG also fails in the latter case.\nin Figure 14, AG can again be employed to reduce NFEs\nwithout noticeable loss of quality. In the depicted images,\nAG employs only ten (instead of 20) \u03f5pix2pix(xt, c, I) steps,\nthereby saving 33.3% of the total number of NFEs.\nC. OLS\nIn section 5.1, we have shown that entire neural network\ncalls can be replaced by a simple linear regression in the\npast. This is particularly relevant for the first half of the\ndiffusion steps, where we found guidance most important.\nTowards this end, we generated as little as 200 paths from\na 20-step CFG model and trained 20 linear regression mod-\nels, one for each timestep, always taking the past uncondi-\ntional as well as the past- and current conditional steps as\nregressors and the current unconditional as target. Impor-\ntantly, we learned a single (scalar) regression coefficient for\neach high-dimensional regressor.8 The per-step errors of the\nlearned LR models are depicted in Fig. 15.\nAfter training the LR models, one can replace the uncondi-\ntional network call \u03f5\u03b8(xt, \u2205) in CFG with the simple linear\n8Simple extensions like doing one OLS per channel did not show any\nsignificant improvement\n2\n\u201cTwo violins standing up with their bows on the ground\u201d (win)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (30NFEs)\n\u201cA toucan close up, midnight, lake, dark, moon light\u201d (win)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (30NFEs)\n\u201cthree wolf moon but with cats instead of wolves\u201d (win)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (31NFEs)\n\u201ca realistic medieval castle built for bees in a sunflower field\u201d (win)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (29NFEs)\nFigure 12. Human evaluation examples (win). More samples from\nthe human evaluation trials. The figure depicts a subset biased towards\ngreater visual difference. We emphasize that images drawn uniformly from\nthe dataset almost always look alike. This explains the draw situation de-\npicted in Table 1.\n\u201cFast commuter train moving past an outdoor platform.\u201d (lose)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (31NFEs)\n\u201cThree bears standing in a field outside.\u201d (lose)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (29NFEs)\n\u201cbee farm, The beatles, bees, honey, honey farm\u201d (win)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (30NFEs)\n\u201ctwo cats patting a magical crystal ball\u201d (win)\nCFG (40NFEs)\nAG, \u00af\u03b3 = 0.991 (30NFEs)\nFigure 13. Human evaluation examples (lose). More samples from\nthe human evaluation trials. The figure depicts a subset biased towards\ngreater visual difference. We emphasize that images drawn uniformly from\nthe dataset almost always look alike. This explains the draw situation de-\npicted in Table 1.\n3\n\u201cTurn the horse into a cow\u201d\n\u201cMake it winter\u201d\n\u201cTurn the hat into a wreath of flowers\u201d\nwhit Input image for\nthe win\nCFG-based editing\n(60NFEs)\nAG-based editing\n(40NFEs)\nFigure 14. Image editing. Instruction based editing with EMU Edit\n[50], which builds upon InstructPix2Pix [4]. Depicted are the original im-\nage (left), classic CFG editing (Eq. 9) and AG editing, which gives equal\nquality results while reducing NFEs by 33.3%. Importantly, Guidance\nDistillation is not directly applicable for this task as the update steps are\nconditioned on the input image.\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nStep Index\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\nMSE\nformance of Linear Regression Models as a Function of Number of Initial Steps\nTrain\nTest\nFigure 15. Per-step OLS errors. MSE of \u02c6\u03f5(xt, \u2205) and \u03f5\u03b8(xt, \u2205)\nafter learning the regression coefficients of Eq. 8. The results de-\npict 200 training and 100 test samples.\ncombination \u02c6\u03f5(xt, \u2205) from Eq. 8, giving rise to\n\u02c6\u03f5cfg(xt, c, s) = \u02c6\u03f5(xt, \u2205) + s \u00b7 (\u03f5\u03b8(xt, c) \u2212 \u02c6\u03f5(xt, \u2205)). (10)\nImportantly, this \u02c6\u03f5cfg update now only costs 1 NFE com-\npared to the 2 NFEs for \u03f5cfg from Eq. 3.\nWe found that the LR estimators \u02c6\u03f5(xt, \u2205) can replace all un-\nconditional network calls \u03f5(xt, \u2205) when given hypothetical\nground truth past information. Of course, such informa-\ntion is no longer available once an upstream CFG step has\nbeen replaced with \u02c6\u03f5cfg. Having observed that the regression\nweights \u03b2i are highest for the most recent past, we found the\nbest policy to be one that alternates between true CFG steps\nand LR-based CFG steps. For example, for the twenty-step\nbaseline we are using the following policy\n\u03b6LINEARAG =[\u03f5cfg,T , \u02c6\u03f5cfg,T \u22121, \u03f5cfg,T \u22122, \u02c6\u03f5cfg,T \u22123...\u03f5cfg,T/2,\n\u02c6\u03f5cfg,T/2\u22121, \u02c6\u03f5cfg,T/2\u22122, ..., \u02c6\u03f5cfg,0].\n(11)\nAll LINEARAG results depicted in Fig. 8, 16 and 11 used\n\u03b6LINEARAG.\n4\n\u201cA painting of a gondola in the canals of 16th century Venice\u201d\n\u201cA group of porcelain tucans painted in Inka style\u201d\n\u201cAn ancient castle on a cliff overlooking a vast, mist-covered valley\u201d\n\u201cA giraffe eating a green plant\u201d\nAG \u00af\u03b3 = 0.975\n(25NFEs)\nna\u00a8\u0131ve interleaving CFG\n(25NFEs)\n... LINEARAG ...\n(25NFEs)\nFigure 16. Replacing CFG in the first half of diffusion steps.\nThree different approaches to reduce the number of NFEs in the first 50%\nof diffusion steps. As can be seen, LINEARAG samples show increased\nsharpness, dynamic lightning with higher contrast, and more vivid colors.\n(Best viewed in zoom.)\n5\nIndividual denoising iterates.\nPoint-wise difference between iterates.\nFigure 17. Denoising process displays scene organization even in early iterations. The (post-decoder) individual denoising iterates (top)\nsuggest little information is known in the early iterations of the process. However, when computing the point-wise differences between the\ndecoded iterates (bottom) shows that even the earliest iterations of the denoising process already display scene organization.\n6\n"
  },
  {
    "title": "Model-Based Control with Sparse Neural Dynamics",
    "link": "https://arxiv.org/pdf/2312.12791.pdf",
    "upvote": "5",
    "text": "Model-Based Control with Sparse Neural Dynamics\nZiang Liu1,2\nGenggeng Zhou2\u2217\nJeff He2\u2217\nTobia Marcucci3\nLi Fei-Fei2\nJiajun Wu2\nYunzhu Li2,4\n1Cornell University\n2Stanford University\n3Massachusetts Institute of Technology\n4University of Illinois Urbana-Champaign\nziangliu@cs.cornell.edu\n{g9zhou,jeff2024}@stanford.edu\ntobiam@mit.edu\n{feifeili,jiajunwu}@cs.stanford.edu\nyunzhuli@illinois.edu\nAbstract\nLearning\npredictive\nmodels\nfrom\nobservations\nusing\ndeep\nneural\nnet-\nworks (DNNs) is a promising new approach to many real-world planning and\ncontrol problems. However, common DNNs are too unstructured for effective\nplanning, and current control methods typically rely on extensive sampling or\nlocal gradient descent. In this paper, we propose a new framework for integrated\nmodel learning and predictive control that is amenable to efficient optimization al-\ngorithms. Specifically, we start with a ReLU neural model of the system dynam-\nics and, with minimal losses in prediction accuracy, we gradually sparsify it by\nremoving redundant neurons. This discrete sparsification process is approximated\nas a continuous problem, enabling an end-to-end optimization of both the model\narchitecture and the weight parameters. The sparsified model is subsequently\nused by a mixed-integer predictive controller, which represents the neuron acti-\nvations as binary variables and employs efficient branch-and-bound algorithms.\nOur framework is applicable to a wide variety of DNNs, from simple multilayer\nperceptrons to complex graph neural dynamics. It can efficiently handle tasks\ninvolving complicated contact dynamics, such as object pushing, compositional\nobject sorting, and manipulation of deformable objects. Numerical and hardware\nexperiments show that, despite the aggressive sparsification, our framework can\ndeliver better closed-loop performance than existing state-of-the-art methods. \u2020\n1\nIntroduction\nOur mental model of the physical environment enables us to easily carry out a broad spectrum of\ncomplex control tasks, many of which lie far beyond the capabilities of present-day robots [35].\nIt is, therefore, desirable to build predictive models of the environment from observations and de-\nvelop optimization algorithms to help the robots understand the impact of their actions and make\neffective plans to achieve a given goal. Physics-based models [27, 79] have excellent generalization\nability but typically require full-state information of the environment, which is hard and sometimes\nimpossible to obtain in complicated robotic (manipulation) tasks. Learning-based dynamics mod-\neling circumvents the problem by learning a predictive model directly from raw sensory observa-\ntions, and recent successes are rooted in the use of deep neural networks (DNNs) as the functional\nclass [15, 22, 60, 51]. Despite their improved prediction accuracy, DNNs are highly nonlinear,\n\u2217denotes equal contribution\n\u2020Please see our website at robopil.github.io/Sparse-Dynamics/ for additional visualizations.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.12791v1  [cs.RO]  20 Dec 2023\nPiecewise Affine Systems\nObject Pushing\nObject Sorting\nRope Manipulation\n(b) Planning with sparsified neural dynamics\n(c) Benchmark tasks\nReLU\nReLU\nReLU\nReLU\nReLU\nReLU\nReLU\nReLU\nReLU\nReLU\nObservation\nAction\nNew Observation\nSparsify\nID\nReLU\nReLU\nID\nID\nID\nReLU\nID\nObservation\nAction\nNew Observation\nWorld\nMIP Planner\nNeural Dynamics\nObservation\nObservation, Action\nNew Observation\nAction\n(a) Sparsify the learned neural dynamics\nFigure 1: Model-based control with sparse neural dynamics. (a) Our framework sparsifies the neural dy-\nnamics models by either removing neurons or replacing ReLU activation functions with identity mappings (ID).\n(b) The sparsified models enable the use of efficient MIP methods for planning, which can achieve better closed-\nloop performance than sampling-based alternatives commonly used in model-based RL. (c) We evaluate our\nframework on various dynamical systems that involve complex contact dynamics, including tasks like object\npushing and sorting, and manipulating a deformable rope.\nmaking model-based planning with neural dynamics models very challenging. Existing methods\noften rely on extensive sampling or local gradient descent to compute control signals, and can be\nineffective for complicated and long-horizon planning tasks.\nCompared to DNNs, simpler models like linear models are amenable to optimization tools with bet-\nter guarantees, but often struggle to accurately fit observation data. An important question arises:\nhow precise do these models need to be when employed within a feedback control loop? The cogni-\ntive science community offers substantial evidence suggesting that humans do not maintain highly\naccurate mental models; nevertheless, these less precise models can be effectively used with environ-\nmental feedback [31, 11]. This notion is also key in control-oriented system identification [26, 48]\nand model order reduction [59, 66]. The framework from Li et al. [41] trades model expressive-\nness and precision for more efficient and effective optimization-based planning through the learning\nof compositional Koopman operators. However, their approach is limited by the linearity of the\nrepresentation in the Koopman embedding space and struggles with more complex dynamics.\nIn this paper, we propose a framework for integrated model learning and control that trades off\nprediction accuracy for the use of principled optimization tools. Drawing inspiration from the neural\nnetwork pruning and neural architecture search communities [23, 58, 17, 6, 44], we start from a\nneural network with ReLU activation functions and gradually reduce the nonlinearity of the model\nby removing ReLU units or replacing them with identity mappings (Figure 1a). This yields a highly\nsparsified neural dynamics model, that is amenable to model-based control using state-of-the-art\nsolvers for mixed-integer programming (MIP) (Figure 1b).\nWe present examples where the proposed sparsification pipeline can determine region partition and\nuncover the underlying system for simple piecewise affine systems. Moreover, it can maintain high\nprediction accuracy for more complex manipulation tasks, using a considerably smaller portion of\nthe original nonlinearities. Importantly, our approach allows the joint optimization of the network\narchitecture and weight parameters. This yields a spectrum of models with varying degrees of\nsparsification. Within this spectrum, we can identify the simplest model that is adequate to meet the\nrequirements of the downstream closed-loop control task.\nOur contributions can be summarized as follows: (i) We propose a novel formulation for identifying\nthe dynamics model from observation data. For this step, we introduce a continuous approximation\nof the sparsification problem, enabling end-to-end gradient-based optimization for both the model\nclass and the model parameters (Figure 1a). (ii) By having significantly fewer ReLU units than\n2\nthe full model, the sparsified dynamics model allows us to solve the predictive-control problems\nusing efficient MIP solvers (Figure 1b). This can lead to better closed-loop performance compared\nto both model-free and model-based reinforcement learning (RL) baselines. (iii) Our framework\ncan be applied to many types of neural dynamics, from vanilla multilayer perceptrons (MLPs) to\ncomplex graph neural networks (GNNs). We show its effectiveness in a variety of simulated and\nreal-world manipulation tasks with complex contact dynamics, such as object pushing and sorting,\nand manipulation of deformable objects (Figure 1c).\n2\nRelated Work\nModel learning for planning and control. Model-based RL agents learn predictive models of their\nenvironment from observations, which are subsequently used to plan their actions [10, 57]. Re-\ncent successes in this domain often heavily rely on DNNs, exhibiting remarkable planning and\ncontrol results in challenging simulated tasks [63], as well as complex real-world locomotion\nand manipulation tasks [37, 60]. Many of these studies draw inspiration from advancements in\ncomputer vision, learning dynamics models directly in pixel-space [16, 12, 13, 77, 67], keypoint\nrepresentation [34, 51, 42], particle/mesh representation [39, 65, 28], or low-dimensional latent\nspace [70, 1, 22, 21, 63, 75]. While previous works typically assume that the model class is given\nand fixed during the optimization process, our work puts emphasis on finding the desired model\nclass via an aggressive network sparsification, to support optimization tools with better guarantees.\nWe are willing to sacrifice the prediction accuracy for better closed-loop performance using more\nprincipled optimization techniques.\nNetwork sparsification. The concept of neural network sparsification is not new and traces back\nto the 1990s [36]. Since then, extensive research has been conducted, falling broadly into two\ncategories: network pruning [24, 23, 72, 58, 38, 25, 3, 47, 17, 46, 6, 78] and neural architecture\nsearch [80, 9, 43, 14, 69]. Many of these studies have demonstrated that fitting an overparameterized\nmodel before pruning yields better results than directly fitting a smaller model. Our formulation is\nclosely related to DARTS [44] and FBNet [74], which both seek a continuous approximation of the\ndiscrete search process. However, unlike typical structured network compression methods, which try\nto remove as many units as possible, our goal here is to minimize the model nonlinearity. To this end,\nour method also permits the substitution of ReLU activations with identity mappings. This leaves\nthe number of units unchanged but makes the downstream optimization problem much simpler.\nMixed-integer modeling of neural networks. The input-output map of a neural network with\nReLU activations is a piecewise affine function that can be modeled exactly through a set of mixed-\ninteger linear inequalities. This allows us to use highly-effective MIP solvers for the solution of\nthe model-based control problem. The same observation has been leveraged before for robustness\nanalysis of DNNs [68, 76] and for providing safety guarantees for control with neural dynamics\nmodels [71], while the efficiency of these mixed-integer models has been thoroughly studied in [2].\n3\nMethod\nIn this section, we describe our methods for learning a dynamics model using environmental ob-\nservations and for sparsifying DNNs through a continuous approximation of the discrete pruning\nprocess. Then we discuss how the sparsified model can be used by an MIP solver for trajectory\noptimization and closed-loop control.\n3.1\nLearning a dynamics model over the observation space\nAssume we have a dataset D = {(ym\nt , um\nt ) | t = 1, . . . , T, m = 1, . . . , M} collected via interac-\ntions with the environment, where ym\nt and um\nt denote the observation and action obtained at time t\nin trajectory m. Our goal is to learn an autoregressive model \u02c6f\u03b8, parameterized by \u03b8, as a proxy of\nthe real dynamics that takes a small sequence of observations and actions from time t\u2032 to the current\ntime t, and predicts the next observation at time t + 1:\n\u02c6ym\nt+1 = \u02c6f\u03b8(ym\nt\u2032:t, um\nt\u2032:t).\n(1)\n3\nWe optimize the parameter \u03b8 to minimize the simulation error that describes the long-term discrep-\nancy between the prediction and the actual observation:\nL(\u03b8) =\nM\nX\nm=1\nX\nt\n\u2225ym\nt+1 \u2212 \u02c6f\u03b8(\u02c6ym\nt\u2032:t, um\nt\u2032:t)\u22252\n2.\n(2)\n3.2\nNeural network sparsification by removing or replacing ReLU activations\nWe instantiate the transition function \u02c6f\u03b8 as a ReLU neural network with N hidden layers. Let us\ndenote the number of neurons in the ith layer as Ni. When given an input x = (ym\nt\u2032:t, um\nt\u2032:t), we\ndenote the value of the jth neuron in layer i before the ReLU activation as xij. Regular ReLU\nneural networks apply the rectifier function to every xij and obtain the activation value using x+\nij =\nhij(xij) \u2261 ReLU(xij) \u225c max(0, xij). The nonlinearity introduced by the ReLU function allows\nthe neural networks to fit the dataset but makes the downstream planning and control tasks more\nchallenging. As suggested by many prior works in the field of neural network compression [23, 17],\na lot of these ReLUs are redundant and can be removed with minimal effects on the prediction\naccuracy. In this work, we reduce the number of ReLU functions by replacing the function hij with\neither an identity mapping ID(xij) \u225c xij or a zero function Zero(xij) \u225c 0, where the latter is\nequivalent to removing the neuron (Figure 1a).\nWe divide the parameters in \u02c6f\u03b8 into two vectors, \u03b8 = (\u03c9, \u03b1). The vector \u03c9 collects the weight\nmatrices and the bias terms. The vector \u03b1 consists of a set of integer variables that parameterize the\narchitecture of the neural network: \u03b1 = {\u03b1ij \u2208 {1, 2, 3} | i = 1, . . . , N, j = 1, . . . , Ni}, such that\nhij(xij) =\n\uf8f1\n\uf8f2\n\uf8f3\nReLU(xij)\nif \u03b1ij = 1\nID(xij)\nif \u03b1ij = 2\nZero(xij)\nif \u03b1ij = 3\n.\n(3)\nThe sparsification problem can then be formulated as the following MIP:\nmin\n\u03b8=(\u03c9,\u03b1)\nL(\u03b8)\ns.t.\nN\nX\ni=1\nNi\nX\nj=1\n1(\u03b1ij = 1) \u2264 \u03b5,\n(4)\nwhere 1 is the indicator function, and the value of \u03b5 decides the number of regular ReLU functions\nthat are allowed to remain in the neural network.\n3.3\nReparameterizing the categorical distribution using Gumbel-Softmax\nSolving the optimization problem in Equation 4 is hard, as the number of integer variables in \u03b1\nequals the number of ReLU neurons in the neural network, which is typically very large. Therefore,\nwe relax the problem by introducing a random variable \u03c0ij indicating the categorical distribution of\n\u03b1ij assigning to one of the three categories, where \u03c0k\nij \u225c p(\u03b1ij = k) for k = 1, 2, 3. We can then\nreformulate the problem as:\nmin\n\u03c9,\u03c0\nE[L(\u03b8)]\ns.t.\nN\nX\ni=1\nNi\nX\nj=1\n\u03c01\nij \u2264 \u03b5,\n\u03b1ij \u223c \u03c0ij,\n(5)\nwhere \u03c0 \u225c {\u03c0ij | i = 1, . . . , N, j = 1, . . . , Ni}.\nIn Equation 5, the sampling procedure \u03b1ij \u223c \u03c0ij is not differentiable. In order to make end-to-end\ngradient-based optimization possible, we employ the Gumbel-Softmax [29, 50] technique to obtain\na continuous approximation of the discrete distribution.\nSpecifically, for a 3-class categorical distribution \u03c0ij, where the class probabilities are denoted as\n\u03c01\nij, \u03c02\nij, \u03c03\nij, Gumbel-Max [18] allows us to draw 3-dimensional one-hot categorical samples \u02c6zij\nfrom the distribution via:\n\u02c6zij = OneHot(arg max\nk\n(log \u03c0k\nij + gk)),\n(6)\n4\nwhere gk are i.i.d. samples drawn from Gumbel(0, 1), which is obtained by sampling uk \u223c\nUniform(0, 1) and computing gk = \u2212 log(\u2212 log(uk)). We can then use the softmax function as\na continuous, differentiable approximation of the arg max function:\nzk\nij =\nexp ((log \u03c0k\nij + gk)/\u03c4)\nP\nk\u2032 exp ((log \u03c0k\u2032\nij + gk\u2032)/\u03c4).\n(7)\nWe denote this operation as zij \u223c Concrete(\u03c0ij, \u03c4) [50], where \u03c4 is a temperature parameter con-\ntrolling how close the softmax approximation is to the discrete distribution. As the temperature \u03c4\napproaches zero, samples from the Gumbel-Softmax distribution become one-hot and identical to\nthe original categorical distribution.\nAfter obtaining zij, we can calculate the activation value x+\nij as a weighted sum of different func-\ntional choices:\nx+\nij = \u02c6hij(xij) \u225c z1\nij \u00b7 ReLU(xij) + z2\nij \u00b7 ID(xij) + z3\nij \u00b7 Zero(xij),\n(8)\nand then use gradient descent to optimize both the weight parameters \u03c9 and the architecture distri-\nbution parameters \u03c0.\nDuring training, we can also constrain zij to be one-hot vectors by using arg max, but use the\ncontinuous approximation in the backward pass by approximating \u2207\u03b8\u02c6zij \u2248 \u2207\u03b8zij. This is denoted\nas \u201cStraight-Through\u201d Gumbel Estimator in [29].\n3.4\nOptimization algorithm\nInstead of limiting the number of regular ReLUs from the very beginning of the training process,\nwe start with a randomly initialized neural network and use gradient descent to optimize \u03c9 and \u03c0 by\nminimizing the following objective function until convergence:\nE[L(\u03b8)] + \u03bbR(\u03c0),\n(9)\nwhere the regularization term R(\u03c0) \u225c PN\ni=1\nPNi\nj=1 \u03c01\nij aims to explicitly reduce the use of the\nregular ReLU function. One could also consider adjusting it to R(\u03c0) \u225c PN\ni=1\nPNi\nj=1(\u03c01\nij + \u03bbID\u03c02\nij)\nwith a small \u03bbID to discourage the use of identity mappings at the same time.\nWe then take an iterative approach by starting with a relatively large \u03b51 and gradually decrease its\nvalue for K iterations with \u03b51 > \u03b52 > \u00b7 \u00b7 \u00b7 > \u03b5K = \u03b5. Within each optimization iteration using \u03b5k,\nwe first rank the neurons according to max(\u03c02\nij, \u03c03\nij) in descending order, and assign the activation\nfunction for the top-ranked neurons as ID if \u03c02\nij \u2265 \u03c03\nij, or Zero otherwise, while keeping the bottom\n\u03b5k neurons intact using Gumbel-Softmax as described in Section 3.3. Subsequently, we continue\noptimizing \u03c9 and \u03c0 using gradient descent to minimize Equation 9. The sparsification process\ngenerates a range of models at various sparsification levels for subsequent investigations.\n3.5\nClosed-loop feedback control using the sparsified models\nAfter we have obtained the sparsified dynamics models, we fix the model architecture and formulate\nthe model-based planning task as the following trajectory optimization problem:\nmin\nu\nX\nt\nc(yt, ut)\ns.t.\nyt+1 = \u02c6f\u03b8(yt\u2032:t, ut\u2032:t),\n(10)\nwhere c is the cost function. When the transition function \u02c6f\u03b8 is a highly nonlinear neural network,\nsolving the optimization problem is not easy. Previous methods [77, 12, 60, 15, 51] typically regard\nthe transition function as a black box and rely on sampling-based algorithms like the cross-entropy\nmethod (CEM) [61] and model-predictive path integral (MPPI) [73] for online planning. Others\nhave also tried applying gradient descent to derive the action signals [39, 40]. However, the number\nof required samples grows exponentially with the number of inputs and trajectory length. Gradient\ndescent can also be stuck in local optima, and it is also hard to assess the optimality or robustness of\nthe derived action sequence using these methods.\n5\nFull (576 ReLUs)\nSparsified (2 ReLUs)\nGround Truth\nFull (576 ReLUs)\nSparsified (2 ReLUs)\nGround Truth\n(a) Piecewise affine function #1\n(b) Piecewise affine function #2\nFigure 2: Recover the ground truth piecewise affine functions from data. We evaluate our sparsification\npipeline on two hand-designed piecewise affine functions composed of four linear pieces. Our pipeline suc-\ncessfully generates sparsified models with 2 ReLUs that accurately fit the data, determine the region partition,\nand recover the underlying ground truth system.\n3.5.1\nMixed-integer formulation of ReLU neural dynamics\nThe sparsified neural dynamics models open up the possibility of dissecting the model and solving\nthe problem using more principled optimization tools. Specifically, given that a ReLU neural net-\nwork is a piecewise affine function, we can formulate Equation 10 as MIP. We assign to each ReLU\na binary variable a = 1(x \u2265 0) to indicate whether the pre-activation value is larger or smaller than\nzero. Given lower and upper bounds on the input l \u2264 x \u2264 u (which we calculate by passing the\noffline dataset through the sparsified neural networks), the equality x+ = ReLU(x) \u225c max(0, x)\ncan be modeled through the following set of mixed-integer linear constraints:\nx+ \u2264 x \u2212 l(1 \u2212 a),\nx+ \u2265 x,\nx+ \u2264 ua,\nx+ \u2265 0,\na \u2208 {0, 1}.\n(11)\nIf only a few ReLUs are left in the model, Equation 10 can be efficiently solved to global optimality.\nThe formulation in Equation 11 is the simplest mixed-integer encoding of a ReLU network, and a\nvariety of strategies are available in the literature to accelerate the solution of our MIPs. For large-\nscale models, it is possible to warm start the optimization process using sampling-based methods or\ngradient descent, and subsequently refine the solution using MIP solvers [53]. There also exist more\nadvanced techniques to formulate the MIP [2, 52, 54], these can lead to tighter convex relaxations of\nour problem and allow us to identify high-quality solutions of Equation 10 earlier in the branch-and-\nbound process. The ability to find globally-optimal solutions is attractive but requires the model to\nexhibit a reasonable global performance. The sparsification step helps us also in this direction, since\nwe typically expect a smaller simulation error from a sparsified (simpler) model than its unsparsified\n(very complicated) counterpart when moving away from the training distribution. In addition, we\ncould also explicitly counteract this issue with the addition of trust-region constraints that prevent\nthe optimizer from exploiting model inaccuracies in the areas of the input space that are not well-\nsupported by the training data [56].\n3.5.2\nTradeoff between model accuracy and closed-loop control performance\nModels with fewer ReLUs are generally less accurate but permit the use of more advanced opti-\nmization tools, like efficient branch-and-bound algorithms implemented in state-of-the-art solvers.\nWithin a model-predictive control (MPC) framework, the controller can leverage the environmental\nfeedback to counteract prediction errors via online modifications of the action sequence. The itera-\ntive optimization procedure in Section 3.4 yields a series of models at different sparsification levels.\nBy comparing their performances and investigating the trade-off between prediction accuracy and\nclosed-loop control performance, we can select the model with the most desirable capacity.\n4\nExperiments\nIn our experiments, we seek to address three central questions: (1) How does the varying number\nof ReLUs affect the prediction accuracy? (2) How does the varying number of ReLUs affect open-\nloop planning? (3) Can the sparsified model, when combined with more principled optimization\nmethods, deliver better closed-loop control results?\nEnvironments, tasks, and model classes. We evaluate our framework on four environments spec-\nified in different observation spaces, including state, keypoints, and object-centric representations.\n6\n(a) Prediction Accuracy\n(b) Trajectory Optimization\nFigure 3: Quantitative analysis of the sparsified models for open-loop prediction and planning. (a) Long-\nterm future prediction error, with the shaded area representing the region between the 25th and 75th percentiles.\nThe significant overlap between the curves suggests that reducing the number of ReLUs only leads to a minimal\ndecrease in prediction accuracy. (b) Results of the trajectory optimization problem from Equation 10. We\ncompare two optimization formulations: mixed-integer programming (MIP) and model-predictive path integral\n(MPPI), using models with varying levels of sparsification. The figure clearly indicates that MIP consistently\noutperforms its sampling-based counterpart, MPPI.\nThese evaluation environments make use of different modeling classes, including vanilla MLPs and\ncomplex GNNs. For closed-loop control evaluation, we additionally present the performance of our\nframework on two standardized benchmark environments from OpenAI Gym [8], CartPole-v1\nand Reacher-v4.\n\u2022 Piecewise affine function. We consider manually designed two-dimensional piecewise affine\nfunctions consisting of four linear pieces (Figure 2), and the goal is to recover the ground-truth\nsystem from data through the sparsification process starting from an overparameterized MLP.\nTo train the model, we collect 1,600 transition pairs from the ground truth functions uniformly\ndistributed over the 2D input space.\n\u2022 Object pushing. A fully-actuated pusher interacts with an object moving on a 2D plane, as\ndepicted in Figure 4a. The goal is to manipulate the object to reach a randomly generated target\npose. We generated 50,000 transition pairs using the Pymunk simulator [7]. The observation yt is\ndefined by the position of four selected keypoints on the object, and the dynamics model is also\ninstantiated as an MLP.\n\u2022 Object sorting. In Figure 4c, a pusher is used to sort a cluster of objects that lie on a table into\ncorresponding target regions. In this environment, we generate a dataset consisting of 150,000\ntransition pairs with two objects using Pymunk. Following the success of previous graph-based\ndynamics models [4, 40, 39], we use GNNs as the model class. The model takes the object\npositions as input and allows compositional generalization to extrapolate settings containing more\nobjects, supporting up to 8 objects as tested in our benchmark.\n\u2022 Rope manipulation. Figure 4b shows the task of manipulating a deformable rope into a target\nshape. We generate a dataset of 60,000 transition pairs through random interactions using Nvidia\nFleX [49]. We use an MLP to model the dynamics, and the observation yt is the position of four\nselected keypoints on the rope.\n4.1\nHow does the varying number of ReLUs affect the prediction accuracy?\nRecover the ground truth piecewise affine system from data. The sparsification procedure starts\nwith the full model with four hidden layers and 576 ReLU units. It then undergoes seven iterations of\nsparsification, with the number of remaining ReLUs, represented as \u03b5k, diminishing from 25 down\nto 2. As illustrated in Figure 2, the sparsified model, which retains only two ReLUs, accurately\nidentifies the region partition and achieves a nearly zero distance from the ground truth. This enables\n7\nTrial #1\n(a) Object Pushing\n(c) Object Sorting\nTime\nTrial #2\n(b) Rope Manipulation\nTime\nTrial #1\nTrial #2\nTime\nInitial\nTarget\nInitial\nTarget\nInitial\nTarget\nInitial\nTarget\nFigure 4: Qualitative results on closed-loop feedback control. (a) In object pushing, the objective is to\nmanipulate the object to reach a randomly generated target pose, depicted as transparent in the first column.\nThe second column illustrates how the planner, using the sparsified model, can leverage feedback from the\nenvironment to compensate for the modeling errors and accurately achieve the target. (b) The framework is\nalso applicable to rope manipulation. Our sparsified model, in conjunction with the MIP formulation, facili-\ntates closed-loop feedback control to manipulate the rope into desired configurations. (c) Our framework also\nconsistently succeeds in object sorting tasks that involve complex contact events. Using the same model with\nthe MIP formulation, the system can manipulate up to eight objects, sorting them into their respective regions.\nthe model to recover the underlying ground truth system and demonstrates the effectiveness of the\nsparsification procedure.\nFuture prediction using sparsified models at different sparsification levels. Existing literature\nprovides comprehensive studies indicating that neural networks are overparameterized [23, 24, 17].\nStill, we are interested in understanding how the proposed sparsification process affects the model\nprediction accuracy. We evaluate our framework on three benchmark environments, object pushing,\nsorting, and rope manipulation. Starting with the full model, we gradually sparsify it using decreas-\ning values of \u03b5k. During training, we focus solely on the accuracy of one-step prediction but evaluate\nthe models for their long-horizon predictive capability.\nFigure 3a illustrates the prediction accuracy for models with varying numbers of ReLUs. \u201cObject\nSorting-2\u201d denotes the task of sorting objects into two piles, while \u201cObject Sorting-3\u201d represents\nsorting into three piles. The blue curve shows the full-model performance, and the shaded area\ndenotes the region between the 25th and 75th percentiles over 100 trials. The figure suggests that,\neven with significantly fewer ReLUs, the model still achieves a reasonable future prediction perfor-\nmance, with the confidence region significantly overlapping with that of the full model. It is worth\n8\n12 15\n12 15\n12 18 24\nFigure 5: Quantitative analysis of model sparsification vs. closed-loop control performance. The horizon-\ntal axis represents the number of ReLUs remaining in the model, and the vertical axis indicates the closed-loop\ncontrol performance. As shown in the figure, there exists a nice trade-off between the levels of model sparsifi-\ncation and the performance of closed-loop control. Models with fewer ReLUs are typically less accurate than\nthe full model but make the MIP formulation tractable to solve. Across the spectrum of models, there exists a\nsweet spot, where a model, although only reasonably accurate, benefits from more powerful optimization tools\nand can lead to superior closed-loop control results. Moreover, our method consistently outperforms commonly\nused RL techniques such as PPO and SAC.\u2021\nnoting that our framework is adaptable to both vanilla MLPs (utilized in object pushing and rope\nmanipulation) and GNNs (employed for object sorting), thereby showcasing the broad applicability\nof our proposed method. Later in Section 4.2 and 4.3, we will demonstrate that the sparsified mod-\nels, although slightly less accurate than the full model, can yield superior open-loop and closed-loop\noptimization results when paired with more effective optimization tools.\n4.2\nHow does the varying number of ReLUs affect open-loop planning?\nHaving obtained the sparsified models and examined their prediction accuracy, we next assess how\nthese models can be applied to solve the trajectory optimization problem in Equation 10. The spar-\nsified model contains significantly fewer ReLUs, making it feasible to use formulations with bet-\nter optimality guarantees, as discussed in Section 3.5. Specifically, we formulate the optimization\nproblem using MIP (Equation 11) and solve the problem using a commercial optimization solver,\nGurobi [19]. We compare our method with MPPI, a commonly-used sampling-based alternative\nfrom the model-based RL community. As illustrated in Figure 3b, the MIP formulation permits the\nuse of advanced branch-and-bound optimization procedures. With a sufficiently small number of\nReLU units remaining in the neural dynamics models, we can solve the problem optimally. This\nconsistently outperforms MPPI by a significant margin.\n4.3\nCan the sparsified model deliver better closed-loop control results?\nThe results in Section 4.2 only tell us how good different optimization procedures are as measured by\nthe learned dynamics model. However, what we really care about is the performance when executing\noptimized plans in the original simulator or the real world. Therefore, it is crucial to evaluate the\neffectiveness of these models within a closed-loop control framework. Here we employ an MPC\n\u2021We omit the result of PPO on rope manipulation due to compute limitations, because our rope simulator\ndoes not support accelerated-time simulation and takes excessively long before PPO gains reasonable perfor-\nmance. We omit the result of SAC on CartPole-v1 because the Stable Baselines 3 SAC implementation does\nnot support a discrete action space.\n9\nframework that, taking into account the feedback from the environment, allows the agent to make\nonline adjustments to the action sequence.\nFigure 4 visualizes multiple execution trials of object pushing, sorting, and rope manipulation in the\nreal world using our method. Our framework reliably pushes the object to its target pose, deforms\nthe rope into the desired shape, and sorts the many objects into the corresponding piles. We then\npresent the quantitative results for object pushing, sorting, and rope manipulation, along with two\ntasks from OpenAI Gym [8], CartPole-v1 and Reacher-v4, measured in simulation, in Figure 5.\nAcross various tasks, we observe a similar trade-off between the levels of model sparsification and\nclosed-loop control performance. As the number of ReLUs decreases, there is typically a slight de-\ncrease in prediction accuracy, but as illustrated in Figure 5, this allows us to formulate the trajectory\noptimization problem as an MIP and solve it using efficient branch-and-bound algorithms. Conse-\nquently, within the spectrum of sparsified models, there exists an optimal point where a model, albeit\nonly reasonably accurate, benefits from the more effective optimization tools and can result in better\nclosed-loop control performance. Our iterative sparsification process, discussed in Section 3.4, en-\nables us to easily identify such model. Furthermore, our method consistently outperforms commonly\nused RL techniques such as PPO [64] and SAC [20] when using the same number of interactions\nwith the underlying environments.\n5\nDiscussion\nConclusion. In this work, we propose to sparsify neural dynamics models for more effective closed-\nloop, model-based planning and control. Our formulation allows an end-to-end optimization of\nboth the model class and the weight parameters. The sparsified models enable the use of efficient\nbranch-and-bound algorithms and can deliver better performance in closed-loop control. Our frame-\nwork applies to various dynamical systems and multiple neural network architectures, including\nvanilla MLPs and complicated GNNs. We also demonstrate the effectiveness and applicability of\nour method through its application to simple piecewise affine systems and manipulation tasks in-\nvolving complex contact dynamics and deformable objects.\nOur work draws inspiration and merges techniques from both the learning and control communities,\nwhich we hope can spur future investigations in this interdisciplinary direction to take advantage\nand make novel use of the powerful tools from both communities.\nLimitations and future work. Our method relies on sparsifying neural dynamics models to fewer\nReLU units to make the control optimization process solvable in a reasonable time due to the worst-\ncase exponential run time of MIP solvers. Although our experiments showed that this already en-\nabled us to complete a wide variety of tasks, our approach may struggle when facing a much larger\nneural dynamics model.\nOur experiments also demonstrated superior closed-loop control performance using sparsified dy-\nnamics models with only reasonably good prediction accuracy as a result of benefiting from stronger\noptimization tools, but our approach may suffer if the sparsified dynamics model becomes signifi-\ncantly worse and incapable of providing useful forward predictions.\nAcknowledgments.\nThis work is in part supported by ONR MURI N00014-22-1-2740. Ziang Liu\nis supported by the Siebel Scholars program.\nReferences\n[1] Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to\npoke by poking: Experiential learning of intuitive physics. arXiv preprint arXiv:1606.07419,\n2016.\n[2] Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma.\nStrong mixed-integer programming formulations for trained neural networks. Mathematical\nProgramming, pages 1\u201337, 2020.\n[3] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional\nneural networks. ACM Journal on Emerging Technologies in Computing Systems (JETC), 13\n(3):1\u201318, 2017.\n10\n[4] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction\nnetworks for learning about objects, relations and physics. Advances in neural information\nprocessing systems, 29, 2016.\n[5] Paul J. Besl and Neil D. McKay. A method for registration of 3-d shapes. IEEE Trans. Pattern\nAnal. Mach. Intell., 14:239\u2013256, 1992.\n[6] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the\nstate of neural network pruning? arXiv preprint arXiv:2003.03033, 2020.\n[7] Victor Blomqvist. Pymunk. https://pymunk.org, November 2022. Version 6.4.0.\n[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.\norg/abs/1606.01540.\n[9] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target\ntask and hardware. arXiv preprint arXiv:1812.00332, 2018.\n[10] Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach\nto policy search. In Proceedings of the 28th International Conference on machine learning\n(ICML-11), pages 465\u2013472. Citeseer, 2011.\n[11] James K Doyle and David N Ford. Mental models concepts for system dynamics research.\nSystem dynamics review: the journal of the System Dynamics Society, 14(1):3\u201329, 1998.\n[12] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning\nwith temporal skip connections. In CoRL, pages 344\u2013356, 2017.\n[13] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual\nforesight: Model-based deep reinforcement learning for vision-based robotic control. arXiv\npreprint arXiv:1812.00568, 2018.\n[14] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al. Neural architecture search: A survey.\nJ. Mach. Learn. Res., 20(55):1\u201321, 2019.\n[15] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017\nIEEE International Conference on Robotics and Automation (ICRA), pages 2786\u20132793. IEEE,\n2017.\n[16] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical inter-\naction through video prediction. arXiv preprint arXiv:1605.07157, 2016.\n[17] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable\nneural networks. arXiv preprint arXiv:1803.03635, 2018.\n[18] Emil Julius Gumbel. Statistical theory of extreme values and some practical applications: a\nseries of lectures, volume 33. US Government Printing Office, 1954.\n[19] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. URL https://www.\ngurobi.com.\n[20] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. In International\nconference on machine learning, pages 1861\u20131870. PMLR, 2018.\n[21] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:\nLearning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.\n[22] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and\nJames Davidson. Learning latent dynamics for planning from pixels. In International Confer-\nence on Machine Learning, pages 2555\u20132565. PMLR, 2019.\n11\n[23] Song Han, Huizi Mao, and William J Dally.\nDeep compression:\nCompressing deep\nneural networks with pruning, trained quantization and huffman coding.\narXiv preprint\narXiv:1510.00149, 2015.\n[24] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections\nfor efficient neural network. Advances in neural information processing systems, 28, 2015.\n[25] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural\nnetworks. In Proceedings of the IEEE international conference on computer vision, pages\n1389\u20131397, 2017.\n[26] Arthur J Helmicki, Clas A Jacobson, and Carl N Nett. Control oriented system identification: a\nworst-case/deterministic approach in h/sub infinity. IEEE Transactions on Automatic control,\n36(10):1163\u20131176, 1991.\n[27] Fran\u00e7ois Robert Hogan and Alberto Rodriguez. Feedback control of the pusher-slider system:\nA story of hybrid and underactuated contact dynamics.\narXiv preprint arXiv:1611.08268,\n2016.\n[28] Zixuan Huang, Xingyu Lin, and David Held. Mesh-based dynamics model with occlusion\nreasoning for cloth manipulation. In Robotics: Science and Systems (RSS), 2022.\n[29] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.\narXiv preprint arXiv:1611.01144, 2016.\n[30] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:\nModel-based policy optimization. CoRR, abs/1906.08253, 2019. URL http://arxiv.org/\nabs/1906.08253.\n[31] Natalie A Jones, Helen Ross, Timothy Lynam, Pascal Perez, and Anne Leitch. Mental models:\nan interdisciplinary synthesis of theory and methods. Ecology and Society, 16(1), 2011.\n[32] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\nhttp://arxiv.org/abs/1412.6980.\n[33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Gir-\nshick. Segment anything. arXiv:2304.02643, 2023.\n[34] Tejas D Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds,\nAndrew Zisserman, and Volodymyr Mnih. Unsupervised learning of object keypoints for per-\nception and control. Advances in neural information processing systems, 32:10724\u201310734,\n2019.\n[35] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\nmachines that learn and think like people. Behavioral and brain sciences, 40, 2017.\n[36] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural\ninformation processing systems, pages 598\u2013605, 1990.\n[37] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learn-\ning quadrupedal locomotion over challenging terrain. Science robotics, 5(47), 2020.\n[38] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\nefficient convnets. arXiv preprint arXiv:1608.08710, 2016.\n[39] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenenbaum, and Antonio Torralba. Learning\nparticle dynamics for manipulating rigid bodies, deformable objects, and fluids. arXiv preprint\narXiv:1810.01566, 2018.\n[40] Yunzhu Li, Jiajun Wu, Jun-Yan Zhu, Joshua B Tenenbaum, Antonio Torralba, and Russ\nTedrake. Propagation networks for model-based control under partial observation. In 2019\nInternational Conference on Robotics and Automation (ICRA), pages 1205\u20131211. IEEE, 2019.\n12\n[41] Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba. Learning compositional\nkoopman operators for model-based control. In International Conference on Learning Repre-\nsentations, 2020. URL https://openreview.net/forum?id=H1ldzA4tPr.\n[42] Yunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, and Animesh Garg. Causal dis-\ncovery in physical systems from videos. Advances in Neural Information Processing Systems,\n33, 2020.\n[43] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei,\nAlan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In\nProceedings of the European conference on computer vision (ECCV), pages 19\u201334, 2018.\n[44] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search.\narXiv preprint arXiv:1806.09055, 2018.\n[45] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jian-\nwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection, 2023.\n[46] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and\nJian Sun. Metapruning: Meta learning for automatic neural network channel pruning. In\nProceedings of the IEEE/CVF international conference on computer vision, pages 3296\u20133305,\n2019.\n[47] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.\nLearning efficient convolutional networks through network slimming. In Proceedings of the\nIEEE international conference on computer vision, pages 2736\u20132744, 2017.\n[48] Lennart Ljung. System identification. Springer, 1998.\n[49] Miles Macklin, Matthias M\u00fcller, Nuttapong Chentanez, and Tae-Yong Kim. Unified particle\nphysics for real-time applications. ACM Transactions on Graphics (TOG), 33(4):1\u201312, 2014.\n[50] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\n[51] Lucas Manuelli, Yunzhu Li, Pete Florence, and Russ Tedrake.\nKeypoints into the fu-\nture: Self-supervised correspondence in model-based reinforcement learning. arXiv preprint\narXiv:2009.05085, 2020.\n[52] Tobia Marcucci and Russ Tedrake.\nMixed-integer formulations for optimal control of\npiecewise-affine systems. In Proceedings of the 22nd ACM International Conference on Hy-\nbrid Systems: Computation and Control, pages 230\u2013239, 2019.\n[53] Tobia Marcucci and Russ Tedrake. Warm start of mixed-integer programs for model predictive\ncontrol of hybrid systems. IEEE Transactions on Automatic Control, 2020.\n[54] Tobia Marcucci, Jack Umenberger, Pablo A Parrilo, and Russ Tedrake. Shortest paths in graphs\nof convex sets. arXiv preprint arXiv:2101.11565, 2021.\n[55] Microsoft. Neural Network Intelligence, 1 2021. URL https://github.com/microsoft/\nnni.\n[56] Peter Mitrano, Dale McConachie, and Dmitry Berenson. Learning where to trust unreliable\nmodels in an unstructured world for deformable object manipulation. Science Robotics, 6(54):\neabd8170, 2021.\n[57] Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-based reinforcement\nlearning: A survey. arXiv preprint arXiv:2006.16712, 2020.\n[58] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convo-\nlutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440,\n2016.\n13\n[59] Bruce Moore. Principal component analysis in linear systems: Controllability, observability,\nand model reduction. IEEE transactions on automatic control, 26(1):17\u201332, 1981.\n[60] Anusha Nagabandi, Kurt Konolige, Sergey Levine, and Vikash Kumar. Deep dynamics models\nfor learning dexterous manipulation. In Conference on Robot Learning, pages 1101\u20131112.\nPMLR, 2020.\n[61] Reuven Y Rubinstein and Dirk P Kroese. The cross-entropy method: a unified approach to\ncombinatorial optimization, Monte-Carlo simulation and machine learning. Springer Science\n& Business Media, 2013.\n[62] Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin A.\nRiedmiller, Raia Hadsell, and Peter W. Battaglia. Graph networks as learnable physics engines\nfor inference and control. CoRR, abs/1806.01242, 2018. URL http://arxiv.org/abs/\n1806.01242.\n[63] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Si-\nmon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering\natari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.\n[64] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[65] Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and Jiajun Wu.\nRobocraft: Learn-\ning to see, simulate, and shape elasto-plastic objects with graph networks. arXiv preprint\narXiv:2205.02909, 2022.\n[66] Kin Cheong Sou, Alexandre Megretski, and Luca Daniel. A quasi-convex optimization ap-\nproach to parameterized model order reduction. In Proceedings of the 42nd annual Design\nAutomation Conference, pages 933\u2013938, 2005.\n[67] HJ Suh and Russ Tedrake. The surprising effectiveness of linear models for visual foresight in\nobject pile manipulation. arXiv preprint arXiv:2002.09093, 2020.\n[68] Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with\nmixed integer programming. arXiv preprint arXiv:1711.07356, 2017.\n[69] Tianzhe Wang, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, and Song\nHan. Apq: Joint search for network architecture, pruning and quantization policy. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n2078\u20132087, 2020.\n[70] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed\nto control: A locally linear latent dynamics model for control from raw images. arXiv preprint\narXiv:1506.07365, 2015.\n[71] Tianhao Wei and Changliu Liu. Safe control with neural network dynamic models. In Roya\nFiroozi, Negar Mehr, Esen Yel, Rika Antonova, Jeannette Bohg, Mac Schwager, and Mykel\nKochenderfer, editors, Proceedings of The 4th Annual Learning for Dynamics and Control\nConference, volume 168 of Proceedings of Machine Learning Research, pages 739\u2013750.\nPMLR, 23\u201324 Jun 2022. URL https://proceedings.mlr.press/v168/wei22a.html.\n[72] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity\nin deep neural networks. arXiv preprint arXiv:1608.03665, 2016.\n[73] Grady Williams, Andrew Aldrich, and Evangelos A Theodorou. Model predictive path integral\ncontrol: From theory to parallel computation. Journal of Guidance, Control, and Dynamics,\n40(2):344\u2013357, 2017.\n[74] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong\nTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet\ndesign via differentiable neural architecture search. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages 10734\u201310742, 2019.\n14\n[75] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Day-\ndreamer: World models for physical robot learning. In Conference on Robot Learning, pages\n2226\u20132240. PMLR, 2023.\n[76] Kai Y Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, and Aleksander Madry.\nTrain-\ning for faster adversarial robustness verification via inducing relu stability. arXiv preprint\narXiv:1809.03008, 2018.\n[77] Lin Yen-Chen, Maria Bauza, and Phillip Isola. Experience-embedded visual foresight. In\nConference on Robot Learning, pages 1015\u20131024. PMLR, 2020.\n[78] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and\nHongsheng Li. Learning n: M fine-grained structured sparse neural networks from scratch.\narXiv preprint arXiv:2102.04010, 2021.\n[79] Jiaji Zhou, Yifan Hou, and Matthew T Mason. Pushing revisited: Differential flatness, trajec-\ntory planning, and stabilization. The International Journal of Robotics Research, 38(12-13):\n1477\u20131489, 2019.\n[80] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv\npreprint arXiv:1611.01578, 2016.\n15\nA\nHow does our method compare to prior works in model-based RL?\nIn this experiment, we aim to examine how the closed-loop control performance of our method com-\npare to prior works in model-based reinforcement learning, evaluated on standard benchmark envi-\nronments. We conduct experiments on two additional tasks from OpenAI Gym [8], CartPole-v1\nand Reacher-v4, following the same procedures as described in Section 4.3. On top of a sampling-\nbased planner (MPPI) and a model-free RL method (PPO), we employ two additional model-based\nRL methods, (1) using PPO to learn a control policy from our learned full neural dynamics model,\nand (2) MBPO [30] learning a model and a policy from scratch. The model-based RL methods\nrequire additional time to train a policy using the learned dynamics model, whereas our approach\ndirectly optimizes a task objective over the dynamics model without needing additional training.\nThe experiment results shown in Figure 6 further demonstrate the superior performance of our ap-\nproach compared to prior methods on the two standard benchmark tasks. Notably, our approach\nachieves better performance with highly sparsified neural dynamics models with fewer ReLUs com-\npared to prior works.\nFigure 6: Closed-loop control performance of our method (MIP) compared against prior methods on two new\nenvironments. Our method with fewer ReLUs outperforms prior methods using models with more ReLUs, and\nwe similarly observe a sweet spot that balances between model prediction accuracy and control performance.\nFigure 7: We tested the closed-loop control performance of dynamics models trained and simplified using our\nmethod by incorporating them as the forward model in a model-based RL framework optimized with PPO. Our\nfindings indicate that even when the dynamics models are substantially simplified, they continue to allow for\nsatisfactory control performance.\nB\nDo models trained using our approach generalize to prior model-based\nRL methods?\nThe neural dynamics model learned in our method is generic and not limited to only working\nwith our planning framework. We take the learned full and sparsified dynamics models on the\nCartPole-v1 environment and train a control policy with PPO interacting only with the learned\nmodel, and provide the experiment results in Figure 7.\n16\nOurs (2 ReLUs)\nLi et al. (2 ReLUs)\nGround Truth\nOur (2 ReLUs)\nLi et al. (2 ReLUs)\nGround Truth\n(a) Piecewise affine function #1\n(b) Piecewise affine function #2\nFigure 8: Comparison with the channel pruning baseline on recovering the PWA functions. We evaluate\nboth our sparsification pipeline and the channel pruning baseline using two hand-designed piecewise affine\nfunctions, each composed of four linear pieces. Our pipeline successfully recovers the underlying ground truth\nsystem, whereas the baseline does not provide an accurate match.\nThe results show that the neural dynamics models trained in our method can generalize and combine\nwith another model-based control framework. As the model becomes progressively sparsified, the\nclosed-loop control performance gracefully degrades.\nC\nHow does our sparsification technique compare to prior neural network\npruning methods?\nThe aim of this study is to discuss and compare our sparsification technique with the pruning meth-\nods commonly employed in the field. Most pruning strategies in existing literature primarily focus\non eliminating as many neurons as possible to reduce computation and enhance efficiency. In con-\ntrast, our method aims to eliminate as many nonlinearities from the neural networks as possible.\nThis differs from channel pruning, which only zeroes out values. Our approach permits the replace-\nment of ReLU activations with identity mappings, the inclusion of which allows a more accurate\nmodel to be achieved at an equivalent level of sparsification. This offers a considerable advantage\nduring the planning stage.\nTo illustrate our point more concretely, we provide, in this section, experimental results comparing\nour method against Level 1 Channel Pruning as referenced in [38].\nC.1\nEvaluation on Piecewise Affine (PWA) Functions\nThe two pruning methods are tasked to recover the ground truth PWA functions, as detailed in the\nexperiment section of the main paper. Figure 8 illustrates the results after sparsifying the neural\nnetworks to two rectified linear units (ReLU) using both methods. Our method successfully identi-\nfies the region partition and the correct equations for describing values of each region, whereas the\nbaseline [38] exhibits noticeable deviations.\nC.2\nEvaluation on Dynamics Prediction\nIn this section, we extend the comparison to three other tasks: object pushing, object sorting, and\nrope manipulation. For the object pushing and rope manipulation tasks, we train the neural dynamics\nmodel for a defined number of epochs before pruning is carried out by masking particular channels.\nPost-pruning, model speedup is performed using Neural Network Intelligence Library [55] to alter\nthe model\u2019s architecture and remove masked neurons. This process is repeated as further channels\nare pruned and the models are finetuned for additional epochs.\nFor the object sorting task involving graph neural networks, we perform a similar procedure to\nconstruct the baseline. During the initial model training phase, the mask resulting from the L1\nnorm pruning operation is used to nullify specific weights, and the corresponding gradients are also\nmasked during the finetuning phase.\nTo ensure fairness and reliability in the comparison, we maintain identical settings for both our\nsparsification technique and the pruning baseline. Therefore, for every round of compression, both\nmodels are subjected to the same number of training epochs, using the same dataset, and are reduced\nto the same number of ReLU units.\n17\nFigure 9: Dynamics prediction error of sparsified models using our method vs. baseline. We compare the\ndynamics prediction error of models sparsified using our method against models sparsified using the channel\npruning method proposed by Li et al. [38]. The x-axis represents the number of remaining ReLU units in\nthe model. The y-axis represents the prediction error measured by the root mean squared error between the\nprediction and the ground truth next state. Because our sparsification method only targets non-linear units\nwhile allowing linear units, models sparsified using our method constantly exhibit lower prediction error across\nall task settings.\nWe provide quantitative comparisons between our sparsification method and the baseline in Fig-\nure 9. Throughout the sparsification process, because our sparsification objective allows replacing\nnon-linearities with identity mappings, our method consistently achieves a superior performance\nmeasured by prediction error, across all tasks.\nC.3\nEvaluation on Closed-Loop Control\nIn this experiment, we aim to further examine whether our method also boosts the closed-loop\ncontrol performance that is critical for executing the optimized plans in the real world. We choose\nthe object pushing task, and prune the learned dynamics model down to 36, 24, 18, and 15 ReLU\nunits using our proposed sparsification method and the channel pruning method proposed by Li et\nal. [38] respectively. As shown in Figure 10, models pruned using our method consistently exhibit\nsuperior closed-loop control performance across all sparsification levels.\nFigure 10: An ablation of our sparsification method compared with a prior network pruning method, evaluated\nby closed-loop control performance, demonstrating superior performance in closed-loop control.\nD\nExperiment Details\nD.1\nPerception\nWe use a single top-down RealSense D435i camera to capture color and depth images of the\nworkspace (Figure 11). The color image is segmented using state-of-the-art detection and segmen-\ntation models, Grounding DINO [45] and Segment Anything Model (SAM) [33]. Given a prompt,\nGrounding DINO generates a bounding box for the corresponding objects in the image. To mini-\nmize detection errors, we implement specific thresholds for confidence and bounding box area for\ndifferent prompts. The resulting bounding box coordinates are then utilized by SAM to produce an\ninstance mask for the corresponding object. The mask, along with the depth image, camera intrinsics\nand extrinsics, are used to calculate the position of all segmented points in the global frame.\n18\nRealSense Camera\nObject Pusher\nWorkspace\nFigure 11: Robot hardware experiments setup. Robot experiments setup with a Franka Emika 7-DOF Panda\nrobot arm and a top-down RealSense D435i RGB-D camera.\n(a) Object Pushing\n(c) Rope Manipulation\n(b) Object Sorting\nFigure 12: Object and keypoint detection examples from our perception pipeline. (a) Bounding box\nand detected keypoints positions. (b) Instance bounding boxes and detected object color. (c) Evenly spaced\nkeypoints detected on the rope segment.\nD.2\nPWA Functions\nWe test our sparsification algorithm on recovering simple PWA functions, with two arguments and\nfour affine pieces. The training data contains 1.6 \u00d7 103 transition pairs uniformly sampled within\nthe input space with the ground truth function.\nOur sparsification method starts with a Multi-Layer Perceptron (MLP) with [96, 192, 192, 96] units\nin each layer and aggressively sparsifies into a compact model with only 2 ReLU units. We train\nusing an Adam optimizer [32] with a learning rate of 1 \u00d7 10\u22124 until convergence, then finetune for\n6 \u00d7 103 epochs in each sparsification round to obtain the final model.\nD.3\nObject Pushing\nWe provide a task-specific prompt \u201cletter t\u201d to the perception module along with confidence and\nbounding box thresholds to retrieve the bounding box around the T-shaped object (Figure 12a). After\nobtaining the segmentation mask of the object within the bounding box from SAM, we apply the\n19\n(a) Object Pushing\n(b) Object Sorting\n(c) Rope Manipulation\nFigure 13: Simulation environments used for data collection.\n(a) The T-shaped object (pink) and the\npusher (black) implemented in Pymunk for the object pushing task. (b) Objects of different colors interacting\nwith the pusher (black) for the object sorting task. (c) The rope (pink) and the pusher (yellow) for the rope\nmanipulation task, implemented with PyFleX [39].\nmask to the depth image captured by the top-down camera to select only the subset of the pointcloud\non the object. The position and orientation of the T-shaped object is determined by registering the\ncaptured pointcloud against a reference pointcloud sampled from a mesh model, using the Iterative\nClosest Points method [5].\nThe goal of the object pushing task is to position and orient a T-shaped object to align with a ran-\ndomly sampled configuration within the workspace. Each action is a straight push from a start\nposition to an end position. The state of the object at each time step is described with an ordered list\nof the coordinates of four selected keypoints. The dataset contains 5 \u00d7 104 transition pairs collected\nusing the Pymunk simulator [7] (Figure 13a). To train our dynamics model, we concatenate the\ncurrent keypoint state with the action, and pass through a three-layer MLP with 256 hidden units in\neach layer. We supervise the training with the mean squared error between the predicted next state\n\u02c6y and the ground truth state y from executing the given action in the current state in the simulator,\naccompanied with an L1 and L2 weight regularization term and both ReLU and ID regularization\nterms as described in Section 3.4 of the main paper.\nThe full loss is defined as\nL = MSE(y, \u02c6y) + \u03bbReLU\u03c01 + \u03bbID\u03c02 + \u03bbreg(RL1 + RL2),\n(12)\nwhere \u03bbReLU = 2 \u00d7 10\u22123, \u03bbID = 1 \u00d7 10\u22124, and \u03bbreg = 3 \u00d7 10\u22124 are constants balancing between\nthe loss terms, selected for each task based on empirical performance on dynamics prediction. The\nsparsification process starts with the full model with 768 ReLUs and gradually reduces to 0 ReLUs\n(linear model), trained with an Adam optimizer [32] with a learning rate of 5 \u00d7 10\u22124. We train the\nfull model until convergence, then train each of the sparsified models for 400 epochs.\nWe formulate the closed-loop feedback control problem with the sparsified models following Section\n3.5, and optimize for the objective to minimize the squared L2 distance between the predicted state\n\u02c6y by the dynamics model and the goal state y\u2217.\nOur dynamics models are trained with one NVIDIA GeForce RTX 2080 Ti or one NVIDIA TITAN\nRTX GPU, followed by closed-loop control optimization using Gurobi [19] on an Intel i7-7700K\n4.2 GHz CPU.\nD.4\nObject Sorting\nIn the object sorting task, we initialize the environment with two or more randomly placed objects of\ntwo to three different colors. The goal is to collect objects of the same color near a distinctive goal\nregion for each color. We experiment with six task variations, with two colors, one to four objects\neach color, or three colors, one or two objects each. The actions are specified by the start and end\nlocations, similar to the object pushing task.\nFor perception, we prompt Grounding DINO with \u201cblocks\u201d, then use the returned bounding boxes\nto obtain instance segmentation masks from SAM (Figure 12b). The position of each object is\n20\ncalculated as the centroid of the positions of all points captured by the segmentation mask. The\ncolor of each object is determined based on the average color of all pixels captured by the mask.\nTo demonstrate the applicability of our method on more complex neural dynamics models, and due\nto the permutation-invariant nature of the task, we choose graph neural networks as the function\nclass. We adapt a model architecture similar to what was employed by Sanchez-Gonzalez et al.\n[62] with 64 hidden units per layer, and train with an Adam optimizer [32] using a learning rate\nof 1 \u00d7 10\u22123. The loss function is the mean squared error between the predicted next state for all\nobjects and the ground truth state, coupled with regularization terms as described in Equation 12,\nwith \u03bbReLU = 3 \u00d7 10\u22124, \u03bbID = 3 \u00d7 10\u22125, and \u03bbreg = 1 \u00d7 10\u22124. The full model with 512 ReLUs is\ntrained until convergence on a dataset with 1.5 \u00d7 105 transitions involving random interactions with\nonly two objects collected in Pymunk [7] (Figure 13b), and each sparsified model is trained for 100\nepochs.\nThe closed-loop control optimization is formulated using the same sparsified model for all task vari-\nations. We calculated the squared L2 distance between each object and the goal of the corresponding\ncolor, and use the summation of all individual object-goal distances as the optimization objective.\nD.5\nRope Manipulation\nWe use a prompt of \u201crope\u201d for the rope manipulation task. Detecting the pose of keypoints on the\ndeformable rope requires an additional step in the perception module. After generating a segmenta-\ntion mask using SAM, we use Singular Value Decomposition to find the vector that approximately\ndivides the rope mask into two segments of equal length, then recursively bisects each segment until\nwe obtain eight segments of similar lengths. The keypoints are calculated as the mean of all points\non the segmentation mask in each rope segment (Figure 12c).\nThe rope manipulation task requires deforming a rope segment to match a desired goal shape spec-\nified by eight keypoints evenly spaced on the rope. Leveraging the PyFleX simulator [39], we\ncollected 6 \u00d7 104 transition pairs generated through random interaction as the dataset (Figure 13c).\nWe adopt a similar formulation as in the object pushing task, using a three-layer MLP with 256\nhidden units in each layer for the dynamics model, trained with the mean squared error loss between\nthe predicted state and the ground truth state at the next time step. For the regularization terms, we\nfollow Equation 12 with \u03bbReLU = 2 \u00d7 10\u22123, \u03bbID = 5 \u00d7 10\u22125, and \u03bbreg = 1 \u00d7 10\u22124. We optimize\nthe full model with an Adam optimizer [32] with a learning rate of 5 \u00d7 10\u22124 until convergence, then\ntrain each subsequent sparsified model for 500 epochs.\n21\n"
  },
  {
    "title": "UniSDF: Unifying Neural Representations for High-Fidelity 3D Reconstruction of Complex Scenes with Reflections",
    "link": "https://arxiv.org/pdf/2312.13285.pdf",
    "upvote": "5",
    "text": "UniSDF: Unifying Neural Representations\nfor High-Fidelity 3D Reconstruction of Complex Scenes with Reflections\nFangjinhua Wang1,2*\nMarie-Julie Rakotosaona2\nMichael Niemeyer2\nRichard Szeliski2\nMarc Pollefeys1\nFederico Tombari2\n1ETH Z\u00a8urich\n2Google\nFigure 1. Comparison of surface normals (top) and RGB renderings (bottom) on \u201cgarden spheres\u201d [37]. While the state-of-the-art methods\nRef-NeRF [37], ENVIDR [18], and Neuralangelo [17] struggle to reconstruct reflective elements or fine geometric details, our method\naccurately models both, leading to high-quality mesh reconstructions of all parts of the scene. Best viewed when zoomed in.\nAbstract\nNeural 3D scene representations have shown great po-\ntential for 3D reconstruction from 2D images. However,\nreconstructing real-world captures of complex scenes still\nremains a challenge. Existing generic 3D reconstruction\nmethods often struggle to represent fine geometric details\nand do not adequately model reflective surfaces of large-\nscale scenes.\nTechniques that explicitly focus on reflec-\ntive surfaces can model complex and detailed reflections\nby exploiting better reflection parameterizations. However,\nwe observe that these methods are often not robust in real\nunbounded scenarios where non-reflective as well as re-\nflective components are present.\nIn this work, we pro-\npose UniSDF, a general purpose 3D reconstruction method\nthat can reconstruct large complex scenes with reflections.\nWe investigate both view-based as well as reflection-based\ncolor prediction parameterization techniques and find that\n*This work was conducted during an internship at Google.\nexplicitly blending these representations in 3D space en-\nables reconstruction of surfaces that are more geometri-\ncally accurate, especially for reflective surfaces. We fur-\nther combine this representation with a multi-resolution\ngrid backbone that is trained in a coarse-to-fine manner,\nenabling faster reconstructions than prior methods.\nEx-\ntensive experiments on object-level datasets DTU, Shiny\nBlender as well as unbounded datasets Mip-NeRF 360 and\nRef-NeRF real demonstrate that our method is able to ro-\nbustly reconstruct complex large-scale scenes with fine de-\ntails and reflective surfaces. Please see our project page at\nhttps://fangjinhuawang.github.io/UniSDF.\n1. Introduction\nGiven multiple images of a scene, accurately reconstruct-\ning a 3D scene is an open problem in 3D computer vi-\nsion. 3D meshes from reconstruction methods can be used\nin many downstream applications, e.g. scene understanding,\n1\narXiv:2312.13285v1  [cs.CV]  20 Dec 2023\nrobotics, and creating 3D experiences for augmented/virtual\nreality [31, 44]. Typical aspects of real-world scenes such as\nuniformly colored areas or non-Lambertian surfaces remain\nchallenging.\nAs a traditional line of research, multi-view stereo meth-\nods [13, 33, 38, 42] usually estimate depth maps with pho-\ntometric consistency and then reconstruct the surface as a\npost-processing step, e.g. point cloud fusion with screened\nPoisson surface reconstruction [15] or TSDF fusion [9].\nHowever, they are unable to reconstruct reflective surfaces\nsince their appearances are not multi-view consistent.\nRecently, Neural Radiance Fields (NeRF) [22] ren-\nder compelling photo-realistic images by parameterizing\na scene as a continuous function of radiance and volume\ndensity using a multi-layer perceptron (MLP). More re-\ncent works [4, 7, 24, 35] replace or augment MLPs with\ngrid based data structures to accelerate training. For ex-\nample, Instant-NGP (iNGP) [24] uses a pyramid of grids\nand hashes to encode features and a tiny MLP to process\nthem. Motivated by NeRF, neural implicit reconstruction\nmethods [40, 43] combine signed distance functions (SDF)\nwith volume rendering, and produce smooth and complete\nsurfaces. For acceleration, recent works [17, 32] rely on\nhash grid representations and reconstruct surfaces with finer\ndetails. However, these NeRF-based methods cannot accu-\nrately reconstruct reflective surfaces.\nTo better capture the appearance of reflective surfaces,\nRef-NeRF [37] computes the color with separate diffuse\nand specular components and parameterizes the appear-\nance using reflected view that exploits the surface normals.\nBakedSDF [44] adopts the same Ref-NeRF components\nwith a VolSDF [43] backbone to reconstruct large-scale\nscenes with shiny surfaces. However, BakedSDF is slow\nto train and struggles with reconstructing fine details.\nWe observe that while reflected view radiance fields\ncan effectively reconstruct highly specular reflections, they\nstruggle to represent more diffuse or ambiguous reflection\ntypes that can be found in real scenes. In contrast, we find\nthat direct camera view radiance fields are more robust to\ndifficult surfaces in real settings, although the reconstruc-\ntions still present artifacts for reflective scenes. In this pa-\nper, we seamlessly bring together geometry based reflected\nview radiance fields and camera view-based radiance fields\ninto a novel unified radiance field for representing 3D real\nscenes accurately in the presence of reflections. Our method\nis robust for reconstructing both real challenging scenes and\nhighly reflective surfaces.\nThe proposed method, named UniSDF, achieves state-\nof-the-art performance on DTU [1], Shiny Blender [37],\nMip-NeRF 360 dataset [3] and Ref-NeRF real dataset [37].\nIt demonstrates the capability to accurately reconstruct\ncomplex scenes with large scale, fine details and reflective\nsurfaces as we see in Fig. 1. Our contributions are summa-\nrized as follows:\n\u2022 We propose a novel algorithm that learns to seamlessly\ncombine two radiance fields in 3D while exploiting the\nadvantages or each representation. Our method produces\nhigh quality object surfaces in both reflective and non-\nreflective regions.\n\u2022 Ours method relies on a hash grid backbone that enables\nfast training while maintaining high reconstruction qual-\nity. Moreover, our pipeline is robust and does not require\nlarge amounts of parameter tuning.\n2. Related Works\nMulti-view stereo (MVS).\nMany traditional [33, 41] and\nlearning-based [13, 38, 39, 42] MVS methods first estimate\nmulti-view depth maps and then reconstruct the surface by\nfusing depth maps in a post-processing step. As the core\nstep, depth estimation is mainly based on the photometric\nconsistency assumption across multiple views. However,\nthis assumption fails for glossy surfaces with reflections,\nand thus MVS methods cannot reconstruct them accurately.\nNeural radiance fields (NeRF).\nAs a seminal method in\nview synthesis, NeRF [22] represents a scene as a continu-\nous volumetric field with an MLP, with position and cam-\nera view direction as inputs, and renders an image using\nvolumetric ray-tracing. Since NeRF is slow to train, some\nmethods [7, 24, 35] use voxel-grid-like data structures to\naccelerate training. Many follow-up works apply NeRFs to\ndifferent tasks, e.g. sparse-view synthesis [26, 36, 46], real-\ntime rendering [8, 14, 29, 45], 3D generation [6, 20, 28] and\npose estimation [19, 34, 49]. For the 3D reconstruction task,\nthere are many methods [11, 17, 21, 27, 30, 32, 40, 43, 47]\nintegrating NeRF with signed distance functions, a common\nimplicit function for geometry. Specifically, they transform\nSDFs back to volume density for volume rendering. How-\never, we observe that they are unable to reconstruct shiny /\nreflective surfaces since NeRF\u2019s camera view direction pa-\nrameterization for the color prediction does not accurately\nmodel reflective parts of the scene.\nNeRFs for reflections.\nRecently, Ref-NeRF [37] repa-\nrameterizes the appearance prediction with separate diffuse\nand reflective components by using the reflected view di-\nrection, which improves the rendering of specular surfaces.\nAs a result, recent works such as BakedSDF [44] or EN-\nVIDR [18] adopt this representation to reconstruct glossy\nsurfaces of unbounded scenes and with material decompo-\nsition, respectively. While leading to strong view-synthesis\nresults for reflective areas, we find that reflective radiance\nfield approaches often lead to overly smooth reconstructions\nwith missing details and that their optimization is not stable\non real-world scenes. In contrast to existing methods with a\n2\nsingle radiance field, we propose to seamlessly combine re-\nflected view radiance fields and camera view based radiance\nfields into a novel unified radiance field, which is robust for\nreconstruction in challenging scenes with highly reflective\nsurfaces.\n3. Method\nIn this section, we first review the basic elements of NeRF.\nWe then describe the architecture and training strategy of\nour method.\n3.1. NeRF Preliminaries\nIn NeRF [22], a 3D scene is represented by mapping a po-\nsition x and ray direction d to a volumetric density \u03c3 and\ncolor c using MLP. For a pixel in the target viewpoint and\nits corresponding ray r = o+td, distance values ti are sam-\npled along the ray. The density \u03c3i is predicted by a spatial\nMLP that receives the position x as input, while the direc-\ntional MLP that predicts the color ci uses the bottleneck\nvector b(x) from the density MLP and the view direction d\nas input. The final color C is rendered as:\nC =\nX\ni\nwici, wi = Ti\u03b1i,\n(1)\nwhere \u03b1i = 1\u2212exp(\u2212\u03c3i\u03b4i) is opacity, \u03b4i = ti \u2212ti\u22121 is the\ndistance between adjacent samples, and Ti = Qi\u22121\nj=1(1\u2212\u03b1j)\nis the accumulated transmittance. The model is trained by\nminimizing the loss between the predicted and ground truth\ncolor:\nLcolor = E[(||C \u2212 Cgt||2].\n(2)\nNote that Mildenhall et al. [22] uses a single-layer di-\nrectional MLP and thus often describes the combination\nof NeRF\u2019s spatial and view dependence MLPs as a single\nMLP.\n3.2. UniSDF\nGiven a set of known images of a scene that potentially con-\ntains reflective surfaces, our goal is to optimize a neural im-\nplicit field and reconstruct the scene with high fidelity and\ngeometric accuracy. We propose UniSDF, a method that en-\nables us to seamlessly combine camera view radiance fields\nand reflected view radiance fields to reconstruct both (a)\nnon-reflective surfaces, diffuse reflective surfaces and com-\nplex surfaces with both reflective and non-reflective areas as\nwell as (b) highly specular surfaces with a well defined and\ndetailed reflected environment. Our pipeline is shown in\nFig. 2. We generate two radiance fields that are parameter-\nized by camera view directions or reflected directions and\ncombine them at the pixel level using a learned rendered\nweight.\nSDF\nMLP\n\ud835\udc53\n\ud835\udc31\niNGP\n\ud835\udc1d\n\ud835\udec4\nReflection\n\ud835\udc1b\n\ud835\udc27\n\ud835\udc02\ud835\udc84\ud835\udc82\ud835\udc8e\nWeight MLP\n\ud835\udc02\ud835\udc93\ud835\udc86\ud835\udc87\n\ud835\udc16\n\ud835\udc02\nEq.8\n\ud835\udf4e\ud835\udc93\n\ud835\udc31\n\ud835\udc31\n\ud835\udc31\n\ud835\udc1d\n\ud835\udc53\u0be5\u0bd8\u0bd9\n\ud835\udc53\u0bd6\u0bd4\u0be0\n\ud835\udc53\u0bea\nRadiance MLP\nRadiance MLP\n\ud835\udc51\n\ud835\udc27\u2032\nFigure 2. Pipeline of UniSDF. We combine the camera radiance\nfield and the reflected radiance field in 3D. Given a position x,\nwe extract iNGP features \u03b3 and input them to an MLP f that esti-\nmates a signed distance value d used to compute the NeRF density.\nWe parametrize the camera view and reflected view radiance fields\nwith two different MLPs fcam and fref respectively. Finally, we\nlearn a continuous weight field that is used to compute the final\ncolor as a weighted composite W of the radiance fields colors\nCcam and Cref after volume rendering, Eq. 8.\nVolume rendering the SDF.\nWe represent the scene ge-\nometry using a signed distance field (SDF), which defines\nthe surface S as the zero level set of the SDF d:\nS = {x : d(x) = 0}\n(3)\nTo better reconstruct large-scale scenes, we follow Mip-\nNeRF 360 [3] and transform x into a contracted space with\nthe following contraction:\ncontract(x) =\n(\nx\n||x|| \u2264 1\n\u0010\n2 \u2212\n1\n||x||\n\u0011 \u0010\nx\n||x||\n\u0011\n||x|| > 1\n(4)\nFor volume rendering, we compute the volume density \u03c3(x)\nfrom the signed distance d(x) as: \u03c3(x) = \u03b1\u03a8\u03b2 (d(x)),\nwhere \u03a8\u03b2 is the cumulative distribution function of a zero-\nmean Laplace distribution with learnable scale parameter\n\u03b2 > 0. The surface normal at x can be computed as the gra-\ndient of the signed distance field: n = \u2207d(x)/||\u2207d(x)||.\nHash Encoding with iNGP.\nTo accelerate training and\nimprove reconstruction of high-frequency details, we use\niNGP [24] to map each position x to a higher-dimensional\nfeature space. Specifically, the features {\u03b3l(x)} from the\npyramid levels of iNGP are extracted with trilinear inter-\npolation and then concatenated to form one single feature\nvector \u03b3(x), which is passed to the SDF MLP.\nCamera View & Reflected View Radiance Fields.\nIn\ncontrast to most existing NeRF methods [2, 22, 37] that use\na single radiance field, we propose to combine a camera\nview radiance field and a reflected view radiance field to\nbetter represent reflective and non reflective surfaces.\n3\nWe follow NeRF [22] for representing our camera view\nradiance field ccam, which is computed from features de-\nfined at each position and the camera view direction:\nccam = fcam(x, d, n, b),\n(5)\nwhere b is the bottleneck feature vector from SDF MLP, n\nis the normal at x and d is the view direction. Similarly to\nrecent works [40, 43], we notice that using surface normals\nas input leads to better quality.\nWe represent the reflected radiance field cref with an\nMLP fref as:\ncref = fref(x, \u03c9r, n, b),\n(6)\nwhere \u03c9r is the reflected view direction around the normal\nn. Unlike Ref-NeRF [37], which uses separate diffuse and\nspecular components, we only use the specular component,\nleading to a simpler architecture. Additionally, we observe\nthat using separate diffuse and specular components can\nlead to optimization instabilities resulting in geometry ar-\ntifacts (see also Fig. 11).\nThe main difference between the two radiance fields is\nthe view directional input of the MLP. We visualize the ren-\ndered color for two radiance fields and surface normal in\nFig. 3. Observe that our method mainly uses the reflected\nradiance field to represent highly specular reflections such\nas the tree reflections in the garden spheres or the environ-\nment reflection on the sedan car. The camera view radiance\nfield is used to represent more diffuse reflections. Fig. 4 il-\nlustrates these observations, where we show the weight for\nreflected view radiance field. We can observe that more re-\nflective parts of the scene tend to be represented with re-\nflected radiance (red). Moreover, the normal maps of our\nmethod show that our model accurately reconstructs reflec-\ntive and non-reflective surface geometry.\nLearned composition.\nWe compose the two radiance\nfields using a learned weight field in 3D. We use an MLP\nfw to learn the weight values w:\nw = sigmoid (fw(x, n, b)) .\n(7)\nWe compose the signals at the pixel level. We first vol-\nume render W, Cref, Ccam following Eq. 1. We then com-\npose the colors for each pixel as follows:\nC = W \u00b7 Cref + (1 \u2212 W) \u00b7 Ccam.\n(8)\nMotivation of dual radiance fields.\nDisambiguating the\ninfluence of geometry, color and reflections is an ill posed\nproblem in 3D reconstruction from 2D images. We observe\nin Fig. 8 that when using a single camera view radiance field\n(CamV) or a single reflected view radiance field with a dif-\nfuse and specular decomposition similar to Ref-NeRF [37]\nFigure 3. Visualization of our color of reflected view radiance\nfield, color of camera view radiance field and normal on \u201csedan\u201d,\n\u201ctoycar\u201d and \u201cgarden spheres\u201d scenes [37].\nFigure 4. Visualization of our learned weight W for reflected view\nradiance field on \u201csedan\u201d and \u201cgarden spheres\u201d scenes [37]. Our\nmethod assigns high weight for reflective surfaces, e.g. window\nand hood of car, spheres.\n(RefV) with our backbone, appearance effects such as view\ndependencies or reflections are sometimes wrongly repre-\nsented by inacurrate geometry. These limitations are also\ncommon in recent methods such as BakedSDF [44], as we\ncan see in Fig. 7.\nMoreover, since each type of radiance field is specialized\nfor different levels of reflection strength and complexity,\nwe observe that the reconstructed geometries while using\nthe two types of radiance are often complementary (Fig. 8).\nIn our method, we explicitly intertwine the radiance fields\nin 3D to continuously determine and use the most adapted\nparametrization for each surface area.\n3.3. Training and Regularization\nCoarse-to-fine training.\nWe observe that directly opti-\nmizing all the features in our multi-resolution hash grid\n4\nleads to overfitting, in particular to specular appearance de-\ntails, which in turn results in incorrect geometry as we show\nin Fig. 10. We observe that this model tends to fake specular\neffects by embedding emitters inside the surface exploiting\nthe numerous learnable features in the hash grid.\nTherefore, we propose to instead optimize the hash grid\nfeatures in a coarse-to-fine fashion, similarly to [17, 32],\nto avoid overfitting and promote smoother and more realis-\ntic surfaces. Specifically, we start with Linit coarse pyramid\nlevels in the beginning of training, and introduce a new level\nwith higher resolution every T0 training fraction (see imple-\nmentation details).\nRegularization.\nFollowing prior works [40, 43], we use\nan eikonal loss [12] to encourage d(x) to approximate a\nvalid SDF:\nLeik = Ex[(||\u2207d(x)|| \u2212 1)2].\n(9)\nTo promote normal smoothness, we constrain the com-\nputed surface normal n to be close to a predicted normal\nvector n\u2032. n\u2032 is predicted by the SDF MLP and normalized.\nWe use the normal smoothness loss Lp [37] as:\nLp =\nX\ni\nwi||n \u2212 n\u2032||2.\n(10)\nWe also use the orientation loss Lo from Ref-NeRF [37]\nto penalize normals that are \u201cback-facing\u201d, using:\nLo =\nX\ni\nwi max(0, n \u00b7 d)2.\n(11)\nFull loss function.\nThe full loss function L includes the\ncolor loss Lcolor of composed color C and the regulariza-\ntions, which is written as follows:\nL = Lcolor + \u03bb1Leik + \u03bb2Lp + \u03bb3Lo.\n(12)\n4. Experiments\n4.1. Experimental Settings\nDatasets.\nWe evaluate our method on four different types\nof datasets. The DTU dataset [1] is an indoor object-centric\nmulti-view stereo dataset with ground truth point clouds.\nFollowing prior works [40, 43], we use the same 15 scenes\nfor evaluation.\nThe Shiny Blender dataset is proposed\nin [37] and contains six different shiny objects that are\nrendered in Blender under conditions similar to the NeRF\ndataset. The Mip-NeRF 360 dataset is proposed in [3] and\ncontains complex unbounded indoor and outdoor scenes\ncaptured from many viewing angles. We further evaluate\non the three real world scenes with reflections that are in-\ntroduced in Ref-NeRF [37], which consists of the scenes\n\u201csedan\u201d, \u201cgarden shperes\u201d and \u201ctoycar\u201d. For simplicity, we\nname these 3 scenes the \u201cRef-NeRF real dataset\u201d.\nImplementation details.\nBased on the Mip-NeRF 360\ncodebase [23], we implement our method in Jax [5] with the\nre-implementation of VolSDF [43] and iNGP [24]. In our\niNGP hierarchy of grids and hashes, we use 15 levels from\n32 to 4096, where each level has 4 channels. For coarse to\nfine training, we set Linit = 4 and T0 = 2%. Similar to mip-\nNeRF 360 [3], we use two rounds of proposal sampling and\nthen a final NeRF sampling round. Following Zip-NeRF,\nwe penalize the sum of the mean of squared grid/hash val-\nues at each pyramid level with a loss multiplier as 0.1. Our\nmodels are all trained on 8 NVIDIA Tesla V100-SXM2-\n16GB GPUs with a batch size of 214. We train 25k steps\non DTU / Shiny Blender and 100k steps on 360 / Ref-NeRF\nreal datasets, which takes 0.75h and 3.50h respectively. See\nthe supplement for more details.\nBaselines.\nWe compare our method to state-of-the-art\nmethods in view synthesis [3, 4, 24, 37] and implicit re-\nconstruction [17, 18, 32, 44].\nZip-NeRF [4] and Neu-\nralangelo [17] are hash grid-based top performing meth-\nods for view synthesis and reconstruction, respectively.\nBakedSDF [44] integrates VolSDF, Ref-NeRF and Mip-\nNeRF 360 to reconstruct high quality mesh for unbounded\nscenes with reflective surfaces. ENVIDR [18] is a top per-\nforming method for reconstructing and rendering specular\nsurfaces that integrates VolSDF, Ref-NeRF, iNGP and an\nenvironment MLP for lighting. Note that this method relies\non explicitly supervising the renderer training with ground\ntruth materials, which we do not rely on.\nWe train Neuralangelo with batch size as 211 for 800k\niterations on the Ref-NeRF real dataset, which takes about\n58 hours on an NVIDIA 3090 GPU for each scene. We also\ntrain BakedSDF [44] on the Shiny Blender and Ref-NeRF\nreal datasets with a batch size of 214 for 250k iterations. As\nwe find that BakedSDF\u2019s optimization is unstable on these\ncomplex scenes and results degrade with default parame-\nters, we carefully tune its hyperparameters for each scene\nand report best results (see supp. mat. for details).\nTo further evaluate the effectiveness of our method,\nwe propose two custom baselines, named \u201cCamV\u201d and\n\u201cRefV\u201d. Using the same backbone as our method, \u201cCamV\u201d\nuses only the camera view radiance field, while \u201cRefV\u201d\nuses only the reflected view radiance field following Ref-\nNeRF [37]. Note that for both baselines, we also use our\ncoarse-to-fine training strategy to improve performance.\n4.2. Evaluation Results\nDTU.\nWe evaluate the accuracy of 3D reconstruction on\nthe DTU dataset [1]. Similar to NeuS [40], we use an ad-\nditional NeRF network to model the background with the\ncoordinate contraction from Mip-NeRF 360 [3]. Follow-\ning prior works, we extract the mesh at 512 resolution. We\nsummarize the quantitative results in Table 1. For Neuralan-\n5\nMethods\nChamfer Distance (mm) \u2193\nVolSDF [43]\n0.86\nNeuS [40]\n0.87\nNeuralWarp [10]\n0.68\nNeuralangelo [17]\n1.07\nPermutoSDF [32]\n0.68\nOurs\n0.64\nTable 1. Quantitative reconstruction results on DTU dataset [1].\nBest score is in bold.\nFigure 5. Visualization and Chamfer Distance error (CD) of re-\nconstructions on DTU scan 110. Our method produces smooth\nsurface with more details, while Neualangelo [17] outputs an over-\nsmoothed surface. Best viewed when zoomed in.\nMethods\nPSNR \u2191 SSIM \u2191 LPIPS \u2193 MAE\u25e6 \u2193\nNVDiffRec [25]\n28.70\n0.945\n0.119\n17.05\nRef-NeRF [37]\n35.96\n0.967\n0.058\n18.38\nENVIDR [18]\n35.85\n0.983\n0.036\n4.61\nOurs\n36.82\n0.976\n0.043\n4.76\nTable 2. Quantitative results on Shiny Blender [37]. Red, orange,\nand yellow indicate the first, second, and third best performing\nalgorithms for each metric.\ngelo [17], we report the reproduced results from their offi-\ncial implementation. Our method achieves state-of-the-art\nperformance when compared with other methods. We vi-\nsualize the qualitative results on DTU scan 110 in Fig. 5.\nOur method reconstructs the surface with more details and\nfewer artifacts than Neuralangelo [17].\nShiny Blender.\nWe summarize the rendering metrics\n(PSNR, SSIM, LPIPS) and mean angular error (MAE)\nin Table 2.\nNote that although running intensive hyper-\nparameter searches, we could not find stable settings for\nBakedSDF [44] on the \u201ccar\u201d and \u201cteapot\u201d scenes (please see\nsupp. mat. for details). Our method performs best in PSNR\nand performs on par with ENVIDR [18] in SSIM, LPIPS\nand MAE. Note that ENVIDR additionally uses an environ-\nment MLP and relies on extra supervision with ground truth\nmaterials, which we do not require, to improve rendering\nand reconstruction. As shown in Fig. 6, BakedSDF [44] re-\nconstructs an erroneous hole in the front and incorrect sur-\nFigure 6. Qualitative comparison on \u201ctoaster\u201d scene [37]. Our\nmethod reconstructs more accurate surface for the shiny object,\nwhile BakedSDF [44] fakes specular effects by embedding emit-\nters inside the reflective surface (highlighted with red box) and\nreconstructs extra geometry below the object.\nface elements under the object, while our method recon-\nstructs the reflective surface more accurately.\nMip-NeRF 360 dataset.\nAs shown in Table 3, our method\nranks second on outdoor scenes and third on indoor scenes.\nNote that state-of-the-art Zip-NeRF [4] focuses on image\nrendering while we focus on high-quality geometric recon-\nstruction. Compared with BakedSDF [44], our performance\nis significantly better in all metrics. As shown in Fig. 7, our\nmethod reconstructs more complete surfaces and better de-\ntails, while BakedSDF shows hole artifacts in some regions\nand struggles to reconstruct fine geometric details.\nRef-NeRF real dataset.\nWe summarize the rendering\nmetrics in Table 4. Our method outperforms Ref-NeRF [37]\nand Neuralangelo [17] in both SSIM and LPIPS, and\nBakedSDF [44] in LPIPS. As shown in Fig. 1, though EN-\nVIDR [18] performs well on Shiny Blender [37], it has\nsimilar artifacts on the spheres as Neuralangelo [17]. Our\nmethod accurately reconstructs the smooth surface of the\nreflective spheres and the fine details on the statue. We visu-\nalize the weight W for our reflected radiance Cref in Fig. 4.\nOur method automatically assigns high weights for the re-\nflective view radiance field on the shiny surfaces, which en-\ncourages it to model reflections, as shown in Fig. 3.\n6\nMethods\nOutdoor Scenes\nIndoor Scenes\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nNeRF [22]\n21.46\n0.458\n0.515\n26.84\n0.790\n0.370\nNeRF++ [48]\n22.76\n0.548\n0.427\n28.05\n0.836\n0.309\nMip-NeRF 360 [3]\n24.47\n0.691\n0.283\n31.72\n0.917\n0.180\nInstant-NGP [24]\n22.90\n0.566\n0.371\n29.15\n0.880\n0.216\nMERF [29]\n23.19\n0.616\n0.343\n27.80\n0.855\n0.271\nZip-NeRF [4]\n25.56\n0.750\n0.207\n32.25\n0.926\n0.168\nBakedSDF [44]\n23.40\n0.619\n0.379\n30.21\n0.888\n0.243\nOurs\n24.77\n0.723\n0.241\n31.28\n0.915\n0.180\nTable 3. Quantitative results on the \u201coutdoor\u201d and \u201cindoor\u201d scenes of Mip-NeRF 360 dataset [3]. Red, orange, and yellow indicate the first,\nsecond, and third best performing algorithms for each metric.\nFigure 7. Qualitative comparison with BakedSDF [44] on \u201cbi-\ncycle\u201d and \u201cofficebonsai\u201d scene of Mip-NeRF 360 dataset [3].\nBakedSDF produces hole structures in many regions (highlighted\nwith dotted orange boxes) and less details of fine structures (high-\nlighted with red boxes), while our method reconstructs more com-\nplete surfaces and better details. Best viewed when zoomed in.\nMethods\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMip-NeRF 360 [3]\n24.27\n0.650\n0.276\nRef-NeRF [37]\n24.06\n0.589\n0.355\nZip-NeRF [4]\n23.68\n0.635\n0.247\nNeuralangelo [17]\n23.70\n0.608\n0.330\nBakedSDF [44]\n24.43\n0.636\n0.325\nOurs\n23.70\n0.636\n0.265\nTable 4. Quantitative results on the Ref-NeRF real dataset [37].\nRed, orange, and yellow indicate the first, second, and third best\nperforming algorithms for each metric.\nCustom baselines comparison.\nWe compare our method\nwith our two custom baselines on the DTU [1], Mip-NeRF\n360 [3], and Ref-NeRF real [37] datasets. As shown in Ta-\nble 5, our method outperforms the two baselines in all met-\nrics on all three datasets. CamV mostly outperforms RefV,\nwhile RefV fails on one scene in DTU. This shows that the\ncamera view radiance field is usually more robust than the\nFigure 8. Qualitative comparison with two baselines, RefV and\nCamV on \u201csedan\u201d and \u201ctoycar\u201d scene [37]. . Best viewed when\nzoomed in.\nreflected view radiance field, although this method does not\nreconstruct the geometry of reflective regions well.\nFig. 8 shows a qualitative comparison, where RefV re-\nconstructs smooth surface for the reflective back window\nbut has artifacts on the side for \u201csedan\u201d, while CamV fails\nto reconstruct accurate surfaces because of the reflections.\nOn the \u201ctoycar\u201d scene, RefV fails to reconstruct the cor-\nrect geometry, while CamV reconstructs shiny surfaces bet-\nter while showing artifacts on the hood.\nFor RefV, we\nsometimes observe optimization issues with separate dif-\nfuse and specular components, where the specular com-\nponent may be blank throughout training and the diffuse\ncomponent (w./o. directional input) wrongly represents the\nview-dependent appearance with incorrect geometry (see\nsup. mat. for details). By coupling two difference radiance\nfields continuously in 3D, our method represents the appear-\nance and geometry better than the baselines that only use a\nsingle radiance field, leading to higher-quality reconstructed\nsurfaces.\nMesh visualization.\nWe visualize our reconstructed\nmeshes of unbounded outdoor and indoor scenes from [3]\nin Fig. 9. Our method is able to accurately reconstruct the\nscenes, including fine geometric details.\n7\nMethods\nDTU\n360 Outdoor Scenes\n360 Indoor Scenes\nRef-NeRF real dataset\nCD (mm) \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nCamV\n0.85\n24.42\n0.712\n0.257\n30.81\n0.909\n0.184\n23.30\n0.622\n0.283\nRefV\n0.89*\n24.01\n0.709\n0.249\n30.15\n0.901\n0.190\n23.02\n0.615\n0.301\nOurs\n0.64\n24.77\n0.723\n0.241\n31.28\n0.915\n0.180\n23.70\n0.636\n0.265\nTable 5. Quantitative comparison with two custom baselines, CamV and RefV, on DTU [1], Mip-NeRF 360 dataset [3] and Ref-NeRF real\ndataset [37]. *: RefV fails on scan 110 of DTU, the reported chamfer distance (CD) is the average of other 14 scans. Best scores are in\nbold.\nFigure 9. Visualization of our reconstructed mesh on \u201cbicycle\u201d\nand \u201ckitchenlego\u201d scene from [3]. Best viewed when zoomed in.\n4.3. Ablation Study\nCoarse-to-fine training.\nAs shown in Fig. 10, the re-\nconstructions contain artifacts on the specular window and\nhood without training in a coarse-to-fine manner. With all\nfeature pyramid grids activated in the beginning, the hash\ngrid backbone can easily overfit to the specular effects with\nwrong geometry.\nDiffuse component in Reflected View Radiance Field.\nRecall that we only use the specular component with re-\nflected view direction as input. We find that adding an addi-\ntional diffuse component that depends on the position only\nas proposed in [37] leads to artifacts in the reconstructions\n(see e.g. the side of the car in Fig. 11), while our method\nproduces a more complete and accurate surface.\nFigure 10.\nAblation of coarse-to-fine training on \u201csedan\u201d\nscene [37]. Without coarse-to-fine training, the model overfits to\nthe specular appearance of the car with incorrect geometry, while\nour method reconstruct the shiny surface more accurately.\nFigure 11. Ablation of diffuse component in reflected view radi-\nance field on \u201csedan\u201d scene [37]. An additional diffuse component\nin the reflected view radiance field as in [37] leads to artifacts on\nthe surface of car, while our method outputs more accurate sur-\nfaces.\n5. Conclusion\nIn this paper, we have presented UniSDF, a novel algorithm\nthat learns to seamlessly combine radiance fields for robust\nand accurate reconstruction of complex scenes with reflec-\ntions. We find that camera view radiance fields, e.g. NeRF,\nare robust to complex real settings but cannot reconstruct\nreflective surfaces well, while reflective radiance fields, e.g.\nRef-NeRF, can effectively reconstruct highly specular sur-\nfaces but struggle in real-world settings and to represent\nother types of surfaces. By adaptively combining camera\nview radiance fields and reflective radiance fields in 3D, our\nmethod significantly outperforms the baselines with either\nsingle radiance field. Together with a hash grid backbone to\naccelerate training and improve reconstruction details, our\nmethod achieves state-of-the-art performance in reconstruc-\ntion and rendering on object-level and unbounded scenes.\n8\nAcknowledgement.\nWe would like to thank Dor Verbin,\nPeter Hedman, Ben Mildenhall and Pratul P. Srinivasan for\nfeedback and comments.\nReferences\n[1] Henrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis,\nEngin Tola, and Anders Bjorholm Dahl. Large-scale data for\nmultiple-view stereopsis. IJCV, 2016. 2, 5, 6, 7, 8, 1, 3\n[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-NeRF: A multiscale representation for anti-aliasing\nneural radiance fields.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5855\u2013\n5864, 2021. 3\n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 2, 3, 5, 7, 8, 1\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan,\nand\nPeter\nHedman.\nZip-NeRF:\nAnti-\naliased grid-based neural radiance fields.\narXiv preprint\narXiv:2304.06706, 2023. 2, 5, 6, 7, 1\n[5] James Bradbury, Roy Frostig, Peter Hawkins, Matthew J.\nJohnson, Chris Leary, Dougal Maclaurin, George Necula,\nAdam Paszke, Jake VanderPlas, Skye Wanderman-Milne,\nand Qiao Zhang.\nJAX: composable transformations of\nPython+NumPy programs, 2018. 5\n[6] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 2\n[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. TensorRF: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 2\n[8] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. MobileNeRF: Exploiting the polygon ras-\nterization pipeline for efficient neural field rendering on mo-\nbile architectures.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n16569\u201316578, 2023. 2\n[9] Brian Curless and Marc Levoy. A volumetric method for\nbuilding complex models from range images. In Proceedings\nof the 23rd annual conference on Computer graphics and\ninteractive techniques, pages 303\u2013312, 1996. 2\n[10] Franc\u00b8ois Darmon, B\u00b4en\u00b4edicte Bascle, Jean-Cl\u00b4ement Devaux,\nPascal Monasse, and Mathieu Aubry. Improving neural im-\nplicit surfaces geometry with patch warping. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6260\u20136269, 2022. 6, 3\n[11] Qiancheng Fu, Qingshan Xu, Yew Soon Ong, and Wen-\nbing Tao. Geo-Neus: Geometry-consistent neural implicit\nsurfaces learning for multi-view reconstruction. Advances\nin Neural Information Processing Systems, 35:3403\u20133416,\n2022. 2\n[12] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and\nYaron Lipman. Implicit geometric regularization for learning\nshapes. arXiv preprint arXiv:2002.10099, 2020. 5\n[13] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong\nTan, and Ping Tan. Cascade cost volume for high-resolution\nmulti-view stereo and stereo matching. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 2495\u20132504, 2020. 2\n[14] Peter Hedman,\nPratul P Srinivasan,\nBen Mildenhall,\nJonathan T Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 5875\u20135884, 2021. 2\n[15] Michael Kazhdan and Hugues Hoppe. Screened Poisson sur-\nface reconstruction. ACM Transactions on Graphics (ToG),\n32(3):1\u201313, 2013. 2\n[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. ICLR, 2015. 1\n[17] Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H Tay-\nlor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.\nNeuralangelo: High-fidelity neural surface reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 8456\u20138465, 2023. 1, 2,\n5, 6, 7, 3\n[18] Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Sel-\nvakumar Panneer, and Nandita Vijaykumar. ENVIDR: Im-\nplicit differentiable renderer with neural environment light-\ning. arXiv preprint arXiv:2303.13022, 2023. 1, 2, 5, 6, 4\n[19] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-\nmon Lucey. BARF: Bundle-adjusting neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5741\u20135751, 2021. 2\n[20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2\n[21] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\nWenping Wang. SparseNeuS: Fast generalizable neural sur-\nface reconstruction from sparse views. In European Confer-\nence on Computer Vision, pages 210\u2013227. Springer, 2022.\n2\n[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 2,\n3, 4, 7, 5\n[23] Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter\nHedman, Ricardo Martin-Brualla, and Jonathan T. Barron.\nMultiNeRF: A Code Release for Mip-NeRF 360, Ref-NeRF,\nand RawNeRF, 2022. 5\n[24] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. SIGGRAPH, 2022. 2, 3, 5, 7, 1\n9\n[25] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3D models, materials, and lighting\nfrom images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8280\u2013\n8290, 2022. 6, 4\n[26] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nNeRF: Regularizing neural radiance fields for view synthesis\nfrom sparse inputs. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5480\u20135490, 2022. 2\n[27] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUNISURF: Unifying neural implicit surfaces and radiance\nfields for multi-view reconstruction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 5589\u20135599, 2021. 2\n[28] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3d using 2d diffusion. arXiv\npreprint arXiv:2209.14988, 2022. 2\n[29] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srini-\nvasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Pe-\nter Hedman. MERF: Memory-efficient radiance fields for\nreal-time view synthesis in unbounded scenes. ACM Trans-\nactions on Graphics (TOG), 42(4):1\u201312, 2023. 2, 7\n[30] Yufan Ren, Fangjinhua Wang, Tong Zhang, Marc Pollefeys,\nand Sabine S\u00a8usstrunk. Volrecon: Volume rendering of signed\nray distance functions for generalizable multi-view recon-\nstruction. arXiv preprint arXiv:2212.08067, 2022. 2\n[31] Gernot Riegler and Vladlen Koltun. Stable view synthesis.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2021. 2\n[32] Radu Alexandru Rosu and Sven Behnke. PermutoSDF: Fast\nmulti-view reconstruction with implicit surfaces using per-\nmutohedral lattices. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8466\u20138475, 2023. 2, 5, 6, 3\n[33] Johannes L Sch\u00a8onberger,\nEnliang Zheng,\nJan-Michael\nFrahm, and Marc Pollefeys.\nPixelwise view selection for\nunstructured multi-view stereo. In Computer Vision\u2013ECCV\n2016: 14th European Conference, Amsterdam, The Nether-\nlands, October 11-14, 2016, Proceedings, Part III 14, pages\n501\u2013518. Springer, 2016. 2\n[34] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davi-\nson. iMAP: Implicit mapping and positioning in real-time.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 6229\u20136238, 2021. 2\n[35] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5459\u2013\n5469, 2022. 2\n[36] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,\nand Federico Tombari. SPARF: Neural radiance fields from\nsparse and noisy poses. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 4190\u20134200, 2023. 2\n[37] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T Barron, and Pratul P Srinivasan.\nRef-NeRF:\nStructured view-dependent appearance for neural radiance\nfields. In 2022 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5481\u20135490. IEEE,\n2022. 1, 2, 3, 4, 5, 6, 7, 8\n[38] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo\nSpeciale, and Marc Pollefeys.\nPatchmatchNet: Learned\nmulti-view patchmatch stereo.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 14194\u201314203, 2021. 2\n[39] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, and\nMarc Pollefeys. IterMVS: Iterative probability estimation for\nefficient multi-view stereo. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 8606\u20138615, 2022. 2\n[40] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. NeuS: Learning neural im-\nplicit surfaces by volume rendering for multi-view recon-\nstruction. arXiv preprint arXiv:2106.10689, 2021. 2, 4, 5, 6,\n3\n[41] Qingshan Xu and Wenbing Tao. Multi-scale geometric con-\nsistency guided multi-view stereo. Computer Vision and Pat-\ntern Recognition (CVPR), 2019. 2\n[42] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long\nQuan.\nMVSNet: Depth inference for unstructured multi-\nview stereo. In Proceedings of the European conference on\ncomputer vision (ECCV), pages 767\u2013783, 2018. 2\n[43] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. Advances in Neu-\nral Information Processing Systems, 34:4805\u20134815, 2021. 2,\n4, 5, 6, 3\n[44] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,\nPratul P Srinivasan, Richard Szeliski, Jonathan T Barron,\nand Ben Mildenhall. BakedSDF: Meshing neural sdfs for\nreal-time view synthesis. arXiv preprint arXiv:2302.14859,\n2023. 2, 4, 5, 6, 7, 1, 3\n[45] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. PlenOctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[46] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 2\n[47] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-\ntler, and Andreas Geiger.\nMonoSDF: Exploring monocu-\nlar geometric cues for neural implicit surface reconstruc-\ntion.\nAdvances in neural information processing systems,\n35:25018\u201325032, 2022. 2\n[48] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. NeRF++: Analyzing and improving neural radiance\nfields. arXiv preprint arXiv:2010.07492, 2020. 7, 5\n[49] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hu-\njun Bao, Zhaopeng Cui, Martin R Oswald, and Marc Polle-\nfeys. NICE-SLAM: Neural implicit scalable encoding for\n10\nSLAM.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 12786\u2013\n12796, 2022. 2\n11\nUniSDF: Unifying Neural Representations\nfor High-Fidelity 3D Reconstruction of Complex Scenes with Reflections\nSupplementary Material\nIn the supplementary document, we first discuss more\nimplementation details of our method, including network\narchitecture and training settings. Second, we discuss the\nbaseline BakedSDF [44] in more detail.\nWe show that\nBakedSDF is often not stable and we discuss how we fine-\ntune the method to improve performance on several scenes.\nThird, we summarize the detailed evaluation results for in-\ndividual scenes on DTU [1], Shiny Blender [37], Mip-NeRF\n360 [3] and Ref-NeRF real [37] datasets. We also qualita-\ntively compare with the finetuned BakedSDF. Finally, we\ndiscuss the comparison with two custom baselines, RefV\nand CamV, in more details.\n6. Implementation Details\n6.1. Network Architecture\nIn addition to the iNGP [24] structure that we have intro-\nduced in the main paper, we further discuss the details of the\nMLP architectures. Specifically, the SDF MLP f has 2 lay-\ners with 256 hidden units and outputs the bottleneck feature\nvector b with size 256. The two radiance MLP fcam, fref\nhave 4 layers with 256 hidden units. Besides, the weight\nMLP fw has a single layer with 256 hidden units.\nRecall that following Mip-NeRF 360 [3], we use two\nrounds of proposal sampling and then a final NeRF sam-\npling round.\nThe proposal sampling is used to bound\nthe scene geometry and recursively generate more detailed\nsample intervals, while the final NeRF sampling is used to\nrender the final set of intervals into an image. We set the\nnumber of samples for these 3 sampling rounds as 64, 32,\n32 for the object-level DTU [1] and Shiny Blender [37], and\n64, 64, 32 for unbounded Mip-NeRF 360 [3] and Ref-NeRF\nreal [37] datasets.\nIn the Sec. 3 of the main paper, we mainly introduce\nmodel details of the final NeRF sampling round, where the\ncolor is rendered, for simplicity. Thus, we introduce the de-\ntails for proposal sampling rounds here. Specifically, the\nproposal sampling rounds only have a SDF MLP, i.e. no ra-\ndiance MLP and weight MLP, since color is not rendered in\nthese rounds. Moreover, the two proposal sampling rounds\nshare a SDF MLP, which is different from the SDF MLP in\nthe NeRF sampling round. Contrary to Zip-NeRF [4] that\nuses a distinct iNGP for each sampling round, we use a sin-\ngle iNGP that is shared by all sampling rounds. We find that\nthis produces similar performance as using multiple iNGPs\nbut explicitly simplifies the model.\n6.2. Training\nIn the loss function (Eq. 12 in the main paper, which is\nfor the final NeRF sampling round), we set \u03bb1 = 10\u22124\nand \u03bb3 = 10\u22123. Moreover, we set \u03bb2 = 10\u22124 for Shiny\nBlender [37], and \u03bb2 = 10\u22123 for DTU [1], Mip-NeRF\n360 [3] and Ref-NeRF real [37] datasets. For proposal sam-\npling rounds, we replace Lcolor with Lprop, the proposal loss\ndescribed in Mip-NeRF 360 [3].\nFor training, we use the Adam [16] optimizer with \u03b21 =\n0.9, \u03b22 = 0.999, \u03f5 = 10\u22126. We warm up the learning rate in\nthe first 2% iterations and then decay the it logarithmically\nfrom 5 \u00d7 10\u22123 to 5 \u00d7 10\u22124.\n7. Baselines\nAs we discuss in the main paper, the optimization of\nBakedSDF [44] is sensitive and often fails completely on\nShiny Blender [37] and Ref-NeRF real dataset [37], as\nshown in Fig. 12.\nBakedSDF only uses eikonal loss, Leik, for regulariza-\ntion, where the corresponding loss weight is set to 0.1 by\ndefault. We experimentally find that decreasing the eikonal\nloss weight can stabilize the training and thus carefully tune\nit for each scene. For Shiny Blender [37], we set the eikonal\nloss weight as 10\u22122 for \u201ctoaster\u201d, \u201chelmet\u201d and \u201ccoffee\u201d,\nand 10\u22121 for \u201cball\u201d. Unfortunately, we could not find the\nbest eikonal loss weight for \u201ccar\u201d and \u201cteapot\u201d scenes and\nthe training keeps failing. For Ref-NeRF real dataset [37],\nwe set the eikonal loss weight as 10\u22122 for \u201csedan\u201d and \u201ctoy-\ncar\u201d, and 10\u22125 for \u201cgarden spheres\u201d.\nFigure 12.\nFinal image rendering and normal of original\nBakedSDF [44] on \u201cgarden spheres\u201d scene [37]. The training fails\ncompletely.\n8. Detailed Evaluation Results\nTables 6, 7, 8 and 9 contain the detailed metrics for each in-\ndividual scene on DTU [1], Shiny Blender [37], Mip-NeRF\n1\n360 [3] and Ref-NeRF real [37] datasets respectively.\nOn Ref-NeRF real dataset, we qualitatively compare\nwith our finetuned BakedSDF on \u201cgarden spheres\u201d in\nFig. 13. The finetuned BakedSDF has small artifacts on\nthe spheres and fails to reconstruct fine geometric details\nof the statue as well as the ground, while our method re-\nconstructs the reflective spheres and the fine geometric de-\ntails better. Additionally, we compare with different meth-\nods [17, 37, 44] on \u201ctoycar\u201d scene, as shown in Fig. 14. The\nfinetuned BakedSDF successfully reconstructs the smooth\nsurface, but contains artifacts on the plate as highlighted.\nNeuralangelo [17] reconstructs a large floater around the\nsteering wheel.\nBakedSDF\nOurs\nFigure\n13.\nQualitative\ncomparison\nwith\nthe\nfinetuned\nBakedSDF [44] on \u201cgarden spheres\u201d scene [37]. BakedSDF has\nsmall artifacts on the spheres and oversmooths the geometry de-\ntails of statue and ground. Best viewed when zoomed in.\n9. Custom Baselines Comparison\nIn the main paper, we have compared our method with two\ncustom baselines, CamV and RefV, both quantitatively (Ta-\nble 5) and qualitatively (Fig. 8).\nIn Fig. 15, we further visualize the qualitative results on\nscan 37 of DTU [1]. On the one hand, RefV reconstructs\nholes on the objects with or without reflections, which is\nsimilar to the artifacts that BakedSDF [44] shows in Fig. 7\nof the main paper. On the other hand, though the surface\nis a little noisy, CamV reconstructs shiny objects relatively\nwell. Note that in Fig. 8 of the main paper, CamV also re-\nconstructs the shiny surfaces of \u201ctoycar\u201d well, despite hav-\ning some small artifacts. Since scan 37 of DTU and \u201ctoy-\ncar\u201d of Ref-NeRF real dataset mainly contain reflective sur-\nfaces that are less specular, we can infer that camera view\nradiance field can handle less specular reflections to some\nextent.\nAs shown in Fig. 16, we sometimes observe that RefV\nhas optimization issues with separate diffuse and specu-\nlar components. Specifically, the specular component may\nbe empty throughout training, while the diffuse component\nrepresents both view-dependent and non view-dependent\nappearance. Since diffuse component depends only on the\n3D position x, the view-dependence is represented with in-\ncorrect geometry, as shown in the results of \u201ctoycar\u201d in\nFig. 16. We believe that this issue may be related to the\nhigh frequency signals that iNGP [24] can encode. In RefV,\nthe diffuse component is parameterized by the feature vec-\ntor \u03b3 from iNGP, which is capable of representing very high\nfrequency signal. Therefore, the diffuse component may\ntake advantage of the high capacity of the iNGP represen-\ntation to model the view-dependent appearance with geom-\netry only, leading to an incorrect reconstruction. This is\nespecially true for small-scale scenes with relatively simple\nview-dependent appearance, e.g. \u201ctoycar\u201d.\n2\nMethods\n24\n37\n40\n55\n63\n65\n69\n83\n97\n105\n106\n110\n114\n118\n122\nVolSDF [43]\n1.14\n1.26\n0.81\n0.49\n1.25\n0.70\n0.72\n1.29\n1.18\n0.70\n0.66\n1.08\n0.42\n0.61\n0.55\nNeuS [40]\n1.37\n1.21\n0.73\n0.40\n1.20\n0.70\n0.72\n1.01\n1.16\n0.82\n0.66\n1.69\n0.39\n0.49\n0.51\nNeuralWarp [10]\n0.49\n0.71\n0.38\n0.38\n0.79\n0.81\n0.82\n1.20\n1.06\n0.68\n0.66\n0.74\n0.41\n0.63\n0.51\nNeuralangelo [17]\n0.49\n1.05\n0.95\n0.38\n1.22\n1.10\n2.16\n1.68\n1.78\n0.93\n0.44\n1.46\n0.41\n1.13\n0.97\nPermutoSDF [32]\n0.52\n0.75\n0.41\n0.37\n0.90\n0.66\n0.59\n1.37\n1.07\n0.85\n0.46\n0.98\n0.33\n0.39\n0.50\nOurs\n0.54\n0.84\n0.66\n0.51\n0.76\n0.64\n0.71\n0.70\n0.86\n0.57\n0.69\n0.65\n0.45\n0.56\n0.50\nTable 6. Quantitative Chamfer distance results of individual scenes on DTU dataset [1]. Red and orange indicate the first and second best\nperforming algorithms for each scene.\nFigure 14. Qualitative comparison with Ref-NeRF [37], BakedSDF [44] and Neuralangelo [17] on \u201ctoycar\u201d scene [37]. The finetuned\nBakedSDF has artifacts on the plate, while Neuralangelo has a large floater around the steering wheel (both are highlighted with red\nboxes). Best viewed when zoomed in.\n  Reference Image                         CamV (CD: 0.88)                         RefV (CD: 1.12)                              Ours (CD: 0.84)\nFigure 15. Comparison with two baselines, CamV and RefV, on scan 37 of DTU [1] (CD is Chamfer distance error). CamV reconstructs\nmore noisy surface on the red handle with reflections (highlighted with red box and zoomed in), while RefV generates holes on the shiny\nobjects and even the brick without any reflections. Best viewed when zoomed in.\n3\nMethods\ncar\nball\nhelmet teapot toaster coffee\nPSNR \u2191\nNVDiffRec [25] 27.98 21.77\n26.97\n40.44\n24.31\n30.74\nRef-NeRF [37]\n30.82 47.46\n29.68\n47.90\n25.70\n34.21\nBakedSDF [44]\n-\n31.35\n35.50\n-\n23.84\n35.06\nENVIDR [18]\n29.88 41.03\n36.98\n46.14\n26.63\n34.45\nOurs\n29.86 44.10\n38.84\n48.76\n26.18\n33.17\nSSIM \u2191\nNVDiffRec [25] 0.963 0.858\n0.951\n0.996\n0.928\n0.973\nRef-NeRF [37]\n0.955 0.995\n0.958\n0.998\n0.922\n0.974\nBakedSDF [44]\n-\n0.979\n0.990\n-\n0.939\n0.978\nENVIDR [18]\n0.972 0.997\n0.993\n0.999\n0.955\n0.984\nOurs\n0.954 0.993\n0.990\n0.998\n0.945\n0.973\nLPIPS \u2193\nNVDiffRec [25] 0.045 0.297\n0.118\n0.011\n0.169\n0.076\nRef-NeRF [37]\n0.041 0.059\n0.075\n0.004\n0.095\n0.078\nBakedSDF [44]\n-\n0.094\n0.019\n-\n0.079\n0.072\nENVIDR [18]\n0.031 0.020\n0.022\n0.003\n0.097\n0.044\nOurs\n0.047 0.039\n0.021\n0.004\n0.072\n0.078\nMAE\u25e6 \u2193\nNVDiffRec [25] 11.78 32.67\n21.19\n5.55\n16.04\n15.05\nRef-NeRF [37]\n14.93\n1.55\n29.48\n9.23\n42.87\n12.24\nBakedSDF [44]\n-\n0.44\n1.74\n-\n12.24\n3.31\nENVIDR [18]\n7.10\n0.74\n1.66\n2.47\n6.45\n9.23\nOurs\n6.88\n0.45\n1.72\n2.80\n8.71\n8.00\nTable 7.\nQuantitative results of individual scenes on Shiny\nBlender [37]. Note that BakedSDF [44] fails on \u201ccar\u201d and \u201cteapot\u201d\nscenes. Red, orange, and yellow indicate the first, second, and\nthird best performing algorithms for each scene.\nFigure 16.\nVisualization of diffuse color component, specular\ncolor component and normal for RefV on \u201csedan\u201d and \u201ctoycar\u201d\nscenes [37]. RefV successfully decomposes two color components\nfor \u201csedan\u201d, while it fails on \u201ctoycar\u201d with blank specular compo-\nnent. Best viewed when zoomed in.\n4\nPSNR\nOutdoor\nIndoor\nbicycle\nflowers\ngarden\nstump\ntreehill\nroom\ncounter\nkitchen\nbonsai\nNeRF [22]\n21.76\n19.40\n23.11\n21.73\n21.28\n28.56\n25.67\n26.31\n26.81\nNeRF++ [48]\n22.64\n20.31\n24.32\n24.34\n22.20\n28.87\n26.38\n27.80\n29.15\nMip-NeRF 360 [3]\n24.40\n21.64\n26.94\n26.36\n22.81\n31.40\n29.44\n32.02\n33.11\nInstant NGP [24]\n22.79\n19.19\n25.26\n24.80\n22.46\n30.31\n26.21\n29.00\n31.08\nZip-NeRF [4]\n25.80\n22.40\n28.20\n27.55\n23.89\n32.65\n29.38\n32.50\n34.46\nBakedSDF [44]\n23.05\n20.55\n26.44\n24.39\n22.55\n30.68\n27.99\n30.91\n31.26\nOurs\n24.67\n21.83\n27.46\n26.39\n23.51\n31.25\n29.26\n31.73\n32.86\nSSIM\nOutdoor\nIndoor\nbicycle\nflowers\ngarden\nstump\ntreehill\nroom\ncounter\nkitchen\nbonsai\nNeRF [22]\n0.455\n0.376\n0.546\n0.453\n0.459\n0.843\n0.775\n0.749\n0.792\nNeRF++ [48]\n0.526\n0.453\n0.635\n0.594\n0.530\n0.852\n0.802\n0.816\n0.876\nMip-NeRF 360 [3]\n0.693\n0.583\n0.816\n0.746\n0.632\n0.913\n0.895\n0.920\n0.939\nInstant NGP [24]\n0.540\n0.378\n0.709\n0.654\n0.547\n0.893\n0.845\n0.857\n0.924\nZip-NeRF [4]\n0.769\n0.642\n0.860\n0.800\n0.681\n0.925\n0.902\n0.928\n0.949\nBakedSDF [44]\n0.588\n0.504\n0.793\n0.662\n0.543\n0.892\n0.845\n0.903\n0.911\nOurs\n0.737\n0.606\n0.844\n0.759\n0.670\n0.914\n0.888\n0.919\n0.939\nLPIPS\nOutdoor\nIndoor\nbicycle\nflowers\ngarden\nstump\ntreehill\nroom\ncounter\nkitchen\nbonsai\nNeRF [22]\n0.536\n0.529\n0.415\n0.551\n0.546\n0.353\n0.394\n0.335\n0.398\nNeRF++ [48]\n0.455\n0.466\n0.331\n0.416\n0.466\n0.335\n0.351\n0.260\n0.291\nMip-NeRF 360 [3]\n0.289\n0.345\n0.164\n0.254\n0.338\n0.211\n0.203\n0.126\n0.177\nInstant NGP [24]\n0.398\n0.441\n0.255\n0.339\n0.420\n0.242\n0.255\n0.170\n0.198\nZip-NeRF [4]\n0.208\n0.273\n0.118\n0.193\n0.242\n0.196\n0.185\n0.116\n0.173\nBakedSDF [44]\n0.400\n0.437\n0.204\n0.343\n0.471\n0.270\n0.293\n0.165\n0.244\nOurs\n0.243\n0.320\n0.136\n0.242\n0.265\n0.206\n0.206\n0.124\n0.184\nTable 8. Quantitative results of individual scenes on Mip-NeRF 360 dataset [3]. Red, orange, and yellow indicate the first, second, and\nthird best performing algorithms for each scene.\nMethods\nSedan\nToycar\nGarden Spheres\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMip-NeRF 360 [3]\n25.56\n0.708\n0.304\n24.32\n0.654\n0.256\n22.94\n0.587\n0.268\nRef-NeRF [37]\n25.20\n0.639\n0.406\n24.40\n0.627\n0.292\n22.57\n0.502\n0.366\nZip-NeRF [4]\n25.85\n0.733\n0.260\n23.41\n0.626\n0.243\n21.77\n0.545\n0.238\nNeuralangelo [17]\n24.82\n0.656\n0.384\n24.28\n0.638\n0.293\n22.03\n0.529\n0.313\nBakedSDF [44]\n25.70\n0.700\n0.332\n24.51\n0.655\n0.280\n23.08\n0.553\n0.363\nENVIDR [18]\n-\n-\n-\n-\n-\n-\n22.67\n0.550\n0.312\nOurs\n24.68\n0.700\n0.309\n24.15\n0.639\n0.245\n22.27\n0.567\n0.243\nTable 9. Quantitative results of individual scenes on the Ref-NeRF real dataset [37]. Since ENVIDR [18] is unable to handle unbounded\nscenes, we report the results of \u201cgarden spheres\u201d from their paper. Red, orange, and yellow indicate the first, second, and third best\nperforming algorithms for each metric.\n5\n"
  },
  {
    "title": "SpecNeRF: Gaussian Directional Encoding for Specular Reflections",
    "link": "https://arxiv.org/pdf/2312.13102.pdf",
    "upvote": "5",
    "text": "SpecNeRF: Gaussian Directional Encoding for Specular Reflections\nLi Ma1,2\nVasu Agrawal2\nHaithem Turki2,3\nChangil Kim2\nChen Gao2\nPedro Sander1\nMichael Zollh\u00a8ofer2\nChristian Richardt2\n1The Hong Kong University of Science and Technology\n2Meta Reality Labs\n3Carnegie Mellon University\nAbstract\nNeural radiance fields have achieved remarkable perfor-\nmance in modeling the appearance of 3D scenes. However,\nexisting approaches still struggle with the view-dependent\nappearance of glossy surfaces, especially under complex\nlighting of indoor environments. Unlike existing methods,\nwhich typically assume distant lighting like an environment\nmap, we propose a learnable Gaussian directional encoding\nto better model the view-dependent effects under near-field\nlighting conditions. Importantly, our new directional en-\ncoding captures the spatially-varying nature of near-field\nlighting and emulates the behavior of prefiltered environ-\nment maps. As a result, it enables the efficient evaluation of\npreconvolved specular color at any 3D location with varying\nroughness coefficients. We further introduce a data-driven\ngeometry prior that helps alleviate the shape radiance am-\nbiguity in reflection modeling. We show that our Gaussian\ndirectional encoding and geometry prior significantly im-\nprove the modeling of challenging specular reflections in\nneural radiance fields, which helps decompose appearance\ninto more physically meaningful components.\n1. Introduction\nNeural radiance fields (NeRFs) have emerged as a popular\nscene representation for novel-view synthesis [29, 39, 46].\nBy training a neural network based on sparse observations\nof a 3D scene, NeRF-like representations are able to syn-\nthesize novel views with photorealistic visual quality. In\nparticular, with a scalable model design, such as InstantNGP\n[31], NeRFs are able to model room-scale 3D scenes with\nextraordinary detail [48]. However, existing approaches typ-\nically only manage to model mild view-dependent effects\nlike those seen on nearly diffuse surfaces. When encounter-\ning highly view-dependent glossy surfaces, NeRFs struggle\nto model the high-frequency changes when the viewpoint\nchanges. Instead, they tend to \u201cfake\u201d specular reflections by\nplacing them underneath surfaces, which may result in poor\nGround Truth\nNormal\nView \ndirection\nReflection \ndirection\nNeRF\nFourier dir. \nencoding\nRef-NeRF\nIntegrated dir. \nencoding\nOurs\nGaussian dir. \nencoding\nFigure 1. We propose a Gaussian directional encoding that leads\nto better modeling of specular reflections under near-field lighting\nconditions. In contrast, the integrated directional encoding utilized\nin Ref-NeRF [41] and fourier directional encoding in NeRF [29]\nexhibits suboptimal performance under similar conditions.\nview interpolation and \u201cfoggy\u201d geometry [41]. Moreover,\nfake reflections are not viable if one can look behind the\nsurface, as NeRF can no longer hide the reflections there.\nAccurately modeling and reconstructing specular reflec-\ntions presents notable challenges, especially for room-scale\nscenes. Physically correct reflection modeling involves path-\ntracing many rays for every single pixel, which is impractical\nfor NeRF-like volumetric scene representations, primarily\ndue to the large computational requirements to shade one\npixel. Consequently, an efficient approximation of the reflec-\ntion shading is needed for a feasible modeling of reflections.\nExisting works [13, 41] address this challenge by incorpo-\nrating heuristic modules inspired by real-time image-based\nlighting (IBL) [30] techniques, such as explicit ray bounce\ncomputations to enhance NeRF\u2019s capability to simulate re-\nflections, and integrated directional encoding to simulate\nappearance change under varying surface roughness.\nWhile these improvements have shown to be effective in\nmodeling specular reflections for NeRFs, they are limited\nto object-level reconstruction under distant lighting, which\nassumes the object is lit by a 2D environment map. They\nwork poorly for modeling near-field lighting, where the cor-\nresponding environment map varies spatially. The issue is\n1\narXiv:2312.13102v1  [cs.CV]  20 Dec 2023\nthat existing methods rely on directional encodings to em-\nbed ray directions for generating view-dependent reflections.\nThese encodings, such as Fourier encoding or spherical har-\nmonics, are spatially invariant. Figure 1 demonstrates one\nexample of NeRF [29] and Ref-NeRF [41] reconstructions\nof an indoor scene with spatially-varying lighting. NeRF\nproduces extremely noisy geometry, resulting in artifacts in\nthe rendering result. Ref-NeRF offers a slight improvement,\nbut still struggles with noisy geometry and view interpola-\ntion. This indicates the spatial invariance in the directional\nencodings of existing methods presents challenges under\nspatially-varying lighting conditions.\nIn this work, we propose a novel Gaussian directional\nencoding that is tailored for spatially varying lighting condi-\ntions. Instead of only encoding a 2D ray direction, we use a\nset of learnable 3D Gaussians as the basis to embed a 5D ray\nspace including both ray origin and ray direction. We show\nthat, with appropriately optimized Gaussian parameters, this\nencoding introduces an important inductive bias towards\nnear-field lighting, which enhances the model\u2019s ability to\ncapture the characteristics of specular surfaces, leading to\nphotorealistic reconstructions of shiny reflections. We further\ndemonstrate that by changing the scale of the 3D Gaussians,\nwe can edit the apparent roughness of a surface.\nWhile our proposed Gaussian directional encoding im-\nproves the reflection modeling of NeRF, high-quality reflec-\ntion reconstruction also requires an accurate surface geome-\ntry and normal in order to compute accurate reflection rays.\nHowever, the geometry within NeRFs is often noisy in the\nearly phases of training, which presents challenges in si-\nmultaneously optimizing for good geometry and reflections.\nTo better address this challenge, we introduce a data-driven\nprior to direct the NeRF model towards the desired solu-\ntion. We deploy a monocular normal estimation network to\nsupervise the normal of the geometry at the beginning of\nthe training stage, and show that this bootstrapping strategy\nimproves the reconstruction of normals, and further leads\nto successful modeling of specular reflections. We conduct\nexperiments on several public datasets and show that the\nproposed method outperforms existing methods, achieving\nhigher-quality photorealistic rendering of reflective scenes\nwhile also providing more meaningful and accurate color\ncomponent decomposition. Our contributions can be sum-\nmarized as follows:\n\u2022 We propose a novel Gaussian directional encoding that is\nmore effective in modeling view-dependent effects under\nnear-field lighting conditions.\n\u2022 We propose to use monocular normal estimation to resolve\nshape-radiance ambiguity in the early training stages.\n\u2022 Our full NeRF pipeline achieves state-of-the-art novel\nview synthesis performance for specular reflections.\n2. Related Work\nReflection-aware NeRFs.\nSuccessfully modeling view-\ndependent effects, such as specular reflections, can greatly\nenhance the photorealism of the reconstructed NeRF. NeRF\nmodels view-dependency by conditioning the radiance on\nthe positional encoding [37] of the input ray direction, which\nis only capable of mild view-dependent effects. Ref-NeRF\n[41] improves NeRF\u2019s capability for modeling reflections by\nconditioning the view-dependent appearance on the reflec-\ntion ray direction instead of incident ray direction, and by\nmodulating the directional encoding based on surface rough-\nness. This reparameterization of outgoing radiance makes\nthe underlying scene function simpler, leading to a better\ngeometry and view interpolation quality for glossy objects.\nRef-NeuS [13] further extends these concepts to a surface-\nbased representation. However, these are primarily designed\nfor object-level reconstruction under environment map light-\ning conditions. Modeling large-scale scenes with near-field\nlighting remains a problem. Clean-NeRF [25] decomposes\nthe radiance into diffuse and specular colors, and supervises\nthe two components by least-square estimations of multiple\ninput rays. This alleviates the ambiguity of highly specular\nregions; yet, it does not change the view-dependent structure\nof the NeRF model, thus limiting its ability to model reflec-\ntions. NeRF-DS [49] models specular in dynamic scenes and\nconsiders the variations in reflections caused by dynamic\ngeometry through the use of a dynamic normal field, but\nrequires additional object masks for accurate specular recon-\nstruction.\nNeRF-based Inverse Rendering.\nInverse rendering goes\nbeyond simple reflection modeling and aims to jointly re-\ncover one or more of scene geometry, material appearance\nand the lighting condition. In practice, the material appear-\nance is typically modeled using physically-based rendering\nassets such as albedo, roughness and glossiness. Mesh-based\ninverse rendering methods [2, 33, 44, 63] try to recover ma-\nterials using differentiable path tracing [22]. However, they\ntypically assume a given geometry, since optimizing mesh\ngeometry is challenging. On the contrary, NeRF-based in-\nverse rendering approaches [5, 36, 60, 61] make it easier\nto optimize geometry jointly by modeling material proper-\nties and density continuously in a volumetric 3D space. The\nlighting is usually represented as point or directional lights\n[5, 20, 55], an environment texture map [26\u201328, 36, 60],\nor an implicit texture map modeled by spherical Gaussians\n[6, 11, 58, 61, 62] or MLPs [7, 24]. Most methods are lim-\nited to object-level reconstruction and assume the lighting\nis spatially invariant (i.e. distant). NeILF [51] and NeILF++\n[57] model lighting as a 5D light field using another MLP,\nbut still focus mainly on small-scale reconstruction. Several\nworks apply inverse rendering for relighting outdoor scenes\n[21, 35, 42, 53]. However, they focus more on diffuse ma-\n2\nInput ray\n3D Gaussian\nDirectional\nEncoding\nTint\ns\u2019\nReflection\n(Eq. 3)\n3D position\nx\nview dir\nd\nview dir\nd\nHash \nEncoding\nSH \nEncoding\nDiffuse\ncd\u2019\nDensity\n\u03c4\u2019\nRoughness\n\u03c1\u2019\nNormal\nn\u2019\nTint\ns\nDiffuse\ncd\nDepth\nt0\nRoughness\n\u03c1\nNormal\nn\nReflection \ndirection\ndr\nSpecular\ncs\nOutput\ncolor\nc\nScreen space\n\u03b3(cd + cs \u2299 s) (Eq. 1)\nMLP2\nMLP1\nMLP3\nor = o + t0d\no + td\nVolumetric \nRendering\nNormal from \nmonocular  \nestimation\nnmono\nsupervision\nFigure 2. An overview of our model. The key enabler for specular reflections is our novel 3D Gaussian directional encoding module that\nconverts the reflected ray into a spatially-varying embedding, which is further decoded into specular color.\nterials with correct shadow modeling instead of reflections.\nIn this work, we generally have a different goal compared\nto inverse rendering, focusing only on correctly modeling\nreflections for better novel-view synthesis, rather than trying\nto discern material properties for standalone use.\nNeRF with mirror reflections.\nOne special case of re-\nflection is mirror reflection. One approach represents the\nreflected scene as a separate NeRF [14], and composites\nthe two NeRF results in image space. This is also deployed\nin image-based rendering [47] and large-scale NeRF recon-\nstruction [45]. Given a multi-mirror scene, the idea can be\nfurther extended to multi-space NeRFs [52]. An alternate\napproach is to explicitly model the mirror geometry, and\nto render the mirrored scene by path tracing [15, 56]. How-\never, since estimating the mirror geometry is highly ill-posed,\nmanual annotation is usually needed. Curved reflectors need\neven more careful handling [19, 40].\n3. Preliminaries\nWe first review Ref-NeRF [41] for decomposing view-\ndependent appearance. Similar to NeRF, Ref-NeRF models\nthe scene as a function that maps the position x and view di-\nrection d to the final color c and density \u03c4. The difference is\nthat Ref-NeRF predicts the color as a combination of diffuse\ncolor cd and specular color cs:\nc = \u03b3(cd + cs \u2299 s),\n(1)\nwhere s is the specular tint, \u2018\u2299\u2019 the element-wise product,\nand \u03b3(\u00b7) a tone-mapping function. To predict the specular\ncolor cs, Ref-NeRF first predicts the surface normal n, rough-\nness \u03c1, and features \u03c6 at location x using an MLP. Then,\nthe specular color cs is parameterized as a function of the\nreflection direction dr:\ncs = F\u03b8(\u03bbIDE(dr, \u03c1), \u03c6),\n(2)\nwhere \u03bbIDE(\u00b7) is the integrated directional encoding intro-\nduced by Ref-NeRF, F\u03b8(\u00b7) represents an MLP with parame-\nters \u03b8, and the reflection direction dr is the input direction d\nreflected at the predicted surface normal n:\ndr = d \u2212 2(d \u00b7 n)n.\n(3)\nBy conditioning the specular color on reflection direction\nand roughness, the function F\u03b8 needs to fit is much simpler.\n4. Method\nOur goal is to enhance NeRF\u2019s capabilities for modeling\nspecular reflections under near-field lighting conditions. Fig-\nure 2 presents an overview of our pipeline. A key contribu-\ntion is the 3D Gaussian directional encoding that maps a ray\nand surface roughness to a ray embedding.\nTo render a pixel, we sample points along an input ray\no+td, and predict volume density \u03c4 \u2032, diffuse color c\u2032\nd, tint s\u2032,\nroughness \u03c1\u2032, and normal direction n\u2032 at each sample point\n(we denote per-sample properties using a prime). Given that\nreflections occur only at the surface, we evaluate the specular\ncomponent once per ray on the surface obtained from the\nNeRF depth. This also results in less computation than per-\nsample-point specular shading. Consequently, we calculate\nvolumetric depth t0 by rendering the ray marching distance\nat each sample point. We also volumetrically render all at-\ntributes to synthesize screen-space attributes (cd, s, \u03c1, n).\nNote that the rendered normal must be normalized to yield\nthe final screen-space normal n. We then evaluate the spec-\nular component by first computing the reflected ray using\norigin or = o + t0d, and the reflection direction dr derived\nusing Equation 3. The reflected ray or + tdr and surface\nroughness \u03c1 are then encoded using our novel 3D Gaus-\nsian directional encoding. After a tiny MLP, we compute\nthe specular color cs, and the final rendering result using\nEquation 1.\nFrom a physically based rendering perspective, Equa-\ntion 1 is analogous to the Cook\u2013Torrance approximation\n[10] of the rendering equation [16]. The term cs\u2299s can be\ninterpreted as the split-sum approximation of the specular\npart of the Cook-Torrance model, with the specular color cs\ncorresponding to the preconvolved incident light, and the tint\ns to the pre-integrated bidirectional reflectance distribution\nfunction (BRDF).\n4.1. Gaussian Directional Encoding\nExisting works parameterize view-dependent appearance by\nfirst encoding view or reflection direction into Fourier or\nspherical harmonics (SH) features, which results in a spa-\ntially invariant encoding of the view direction. Therefore, it\nbecomes challenging for the NeRF to model spatially vary-\ning view-dependent effects, such as near-field lighting. We\nillustrate this via a toy example in Figure 3, where we place\n3\ncoefficients\nposition\nOurs\nSH based\n1\n2\n3\n4\n1\n2\n3\n4\nFigure 3. Toy example of 3D Gaussian encoding. Left: A hemi-\nsphere probe translates underneath 4 lights along positions num-\nbered 1 to 4. Note that we dilate the lights for better visualization.\nRight: Representation of the probe\u2019s specular components using\nspherical harmonics and our 3D Gaussian directional encoding.\nThe SH encoding shows a more complex pattern under position\nchange, while ours has spatially largely invariant coefficients. This\nsuggests a simpler function for the specular prediction MLP to fit\nusing Gaussian directional encoding.\na hemispherical specular probe in a simple scene with four\nlights of different shapes and colors. Then we represent the\nspecular component of the toy example by linearly combin-\ning the directional encoding features. We find the optimal\ncoefficients for each encoding type that best fit the ground-\ntruth specular component using stochastic gradient descent,\nand visualize them in Figure 3. We can see that even for\nthis simple toy setup, the SH-based encoding requires com-\nplex, spatially varying coefficients, which complicates the\nunderlying function for the NeRF to fit and interpolate.\nWe propose to spatially vary the encoding function by\ndefining the basis functions via several learnable 3D Gaus-\nsians. Specifically, we parameterize 3D Gaussians using their\nposition \u00b5i \u2208 R3, scale \u03c3i \u2208 R3, and quaternion rotation\nqi \u2208H:\nGi(x) = exp\n\u0010\n\u2212\n\r\rQ(x \u2212 \u00b5i; qi) \u2299 \u03c3\u22121\ni\n\r\r2\n2\n\u0011\n,\n(4)\nwhere Q(v; qi) represents applying quaternion rotation qi\nto the vector v. To compute the i-th dimension of the en-\ncoding for a ray o + td, we need to define a basis function\nPi(o, d) \u2208 R that maps the ray to a scalar value given the\nGaussian parameters. While there are many ways to define\nthe mapping, we find one that is efficient and has a closed-\nform solution by defining the projection as the maximum\nvalue of the Gaussian along the ray:\nPi(o, d) = max\nt\u22650 Gi(o + td).\n(5)\nIn the supplement, we derive a closed-form solution:\nPi(o, d) =\n(\nexp\n\u0010\n(o\u22a4\ni di)2\nd\u22a4\ni di\n\u2212 o\u22a4\ni oi\n\u0011\no\u22a4\ni di > 0\nGi(o)\notherwise,\n(6)\nwhere oi and di are the ray origin and direction transformed\ninto Gaussian local space:\noi = Q(o \u2212 \u00b5i; qi) \u2299 \u03c3\u22121\ni ,\n(7)\ndi = Q(d; qi) \u2299 \u03c3\u22121\ni .\n(8)\nGround \nTruth\nSH based\nOurs\nIncrease \nroughness\nPosition\nRoughness\n4\n3\n2\n1\nFigure 4. Stereographic projections of the specular fitting results for\nthe toy example in Figure 3. Both encodings produce 25 coefficients\nfor each color channel, which are then summed to produce the\nfinal color. Note that the GT shows soft boundaries because it is\na pre-convolved incident light. The 3D Gaussian-based encoding\ndemonstrates superior performance in representing the specular\nchange with positional changes, and is also capable of smoothly\nvarying roughness.\nBy applying Equation 6 for every 3D Gaussian, we obtain\na vector of projected values {Pi}, which forms our final\nencoding features. Similar to existing NeRF-based represen-\ntations [9, 29, 31], we rely on a small MLP to convert the\nencoding to a specular color cs.\nAs illustrated by the toy example in Figure 3, our Gaus-\nsian directional encoding exhibits more constant coefficients\nin response to the position changes, suggesting a smoother\nmapping from the embedding features to the specular color.\nThis smoothness is due to the Gaussian basis function pro-\nducing spatially varying features that mimic the behavior of\nhow the specular component would change under near-field\nlighting conditions. As a result, the underlying functions that\nmodel the specular reflections are easier to learn.\nWe also visualize the fitted specular color of both ap-\nproaches in Figure 4. Our 3D Gaussian directional encoding\nmore accurately captures the spatial variations of the specu-\nlar components.\nSimilar to Ref-NeRF, we use an additional \u201croughness\u201d\nvalue \u03c1 to control the maximum frequency of the specu-\nlar color. We achieve this in our Gaussian embedding by\nmultiplying each Gaussian\u2019s scale \u03c3i with the roughness \u03c1.\nIntuitively, a larger Gaussian results in a smoother function\nwith varying direction d. Substituting the \u03c3i with \u03c1\u03c3i in\nEquation 6 leads to the complete equation of our 3D Gaus-\nsian encoding. Figure 4 demonstrates the ability of our 3D\n4\nGaussian-based encoding to modify roughness on the fly.\n4.2. Optimizing the Gaussian Directional Encoding\nIt is worth noting that our proposed Gaussian encoding cor-\nrectly models spatially varying specular reflections only\nwhen the Gaussians are positioned properly in 3D space.\nWe thus jointly optimize the Gaussian parameters together\nwith the NeRF during training, to ensure the Gaussians are\nin the optimal state for modeling reflections. However, there\nis no direct supervision for the Gaussian parameters.\nOur experiments show that without proper initial Gaus-\nsian parameters, the optimization may lead to suboptimal\nlocal minima, resulting in inconsistent quality of specular\nreconstruction. To address this, we propose an initialization\nstage for the Gaussian parameters and to bootstrap the specu-\nlar color prediction. As mentioned earlier, the specular color\nis essentially the preconvolved incident light, which can be\ndirectly deduced from input images.\nMotivated by this observation, we train the 3D Gaussians\nand the specular decoder (MLP3 in Figure 2) in the initializa-\ntion stage using the input images. We train an incident light\nfield that accommodates a diversity of rays and roughness\nvalues. Therefore, we apply a range of Gaussian blurs to all\ninput images using a series of standard deviations, generating\nGaussian pyramids. These pyramids of input images provide\na pseudo target for incident light under different degrees of\nsurface roughness. In each iteration of the training, we sam-\nple pixels from the pyramids and trace rays to these pixels.\nThe traced rays are also associated with a roughness value\nthat is equivalent to the blur\u2019s standard deviation. We encode\neach ray with roughness using our Gaussian directional en-\ncoding, and predict the specular color cs using the decoder.\nBy minimizing the errors between cs and the pseudo ground\ntruth, we refine the Gaussian parameters and the specular\ndecoder, which then serve as initialization for subsequent\njoint optimization stage.\n4.3. Resolving the Shape\u2013Radiance Ambiguity\nRegardless of view-dependent parameterization, there re-\nmains a fundamental ambiguity between shape and radiance\nin NeRFs. For example, consider a perfect mirror reflection.\nWithout any prior knowledge, it is nearly impossible for the\nNeRF model to tell whether the mirror is a flat surface with\nperfect reflection, or a window to a (virtual) scene behind\nthe surface. Therefore, prior information is needed to guide\nthe model to learn the correct geometry. Inspired by recent\nprogress in monocular geometry estimation [4, 12, 34, 54],\nwe propose to supervise the predicted normal n using monoc-\nular normal estimation nmono [12]:\nLmono =\nX\nj\n\r\rnj \u2212 Rj nj\nmono\n\r\r2\n2 ,\n(9)\nTarget\nFull\nw/o Lmono\nw/o early stop\nFigure 5. The specular component reconstruction (first row, except\nthe first image), novel-view synthesis results (second row) and nor-\nmal visualizations (third row) under varying monocular normal\nsupervision. The target normal visualizes the monocular normal\nprediction. Without Lmono, the predicted normal exhibits enormous\nerror, leading to poor specular reconstruction. Without early stop-\nping Lmono, minor errors in the predicted normals lead to a slight\ndegradation in the reflection quality compared to our full model.\nwhere the superscript j is a ray index, and Rj is the corre-\nsponding camera rotation matrix that converts normals from\nview space to world space.\nHowever, monocular normals are prone to error. We there-\nfore use them primarily as initialization and apply Lmono\nonly at the beginning of the training, so that the errors in\nthe normals do not overwhelm the geometry of the NeRF.\nFigure 5 and Table 2 show results with different configura-\ntions of Lmono. We can see that without monocular normal\nas supervision (\u2018w/o Lmono\u2019), the predicted normals have\ncatastrophic errors, such as those pointing inwards (orange)\nor lying parallel (violet) to the surface. Consequently, the\nlearned specular component is less accurate due to the in-\ncorrect normals. Despite this, a somewhat plausible specular\nreflection can still be learned as the Gaussian encoding can\n\u201ccheat\u201d the reflections even with erroneous normals. On the\nother hand, without early stopping of the loss (\u2018w/o early\nstop\u2019), minor inaccuracies from the monocular normals per-\nmeate into predicted normals, leading to a degradation of the\nreflection quality.\n4.4. Losses\nTo jointly optimize all parameters within our proposed\npipeline, we use a combination of multiple loss terms:\nL = Lc+Lprop+\u03bbdistLdist+\u03bbmonoLmono+\u03bbnormLnorm. (10)\nIn this equation, Lc is the L1 reconstruction loss between the\npredicted and ground-truth colors. The terms Lprop and Ldist\nare adopted from mip-NeRF 360 [3], where Lprop supervises\nthe density proposal networks, and Ldist is the distortion loss\nencouraging density sparsity. To tie predicted normals to\nthe density field, we use Ref-NeRF\u2019s normal prediction loss\nLnorm [41], which guides the predicted normal n with the\n5\nTable 1. Quantitative comparisons of novel-view synthesis on three datasets. We highlight the best numbers in bold.\nMethods\nEyeful Tower dataset [48]\nNISR [45] + Inria [33] dataset\nShiny dataset [41]\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nOurs\n32.583\n0.9328\n0.1445\n30.771\n0.8909\n0.1655\n26.564\n0.7277\n0.2776\nNeRF [29]\n31.854\n0.9254\n0.1626\n30.748\n0.8873\n0.1728\n26.469\n0.7235\n0.2852\nRef-NeRF [41]\n31.652\n0.9258\n0.1570\n30.654\n0.8903\n0.1669\n26.502\n0.7242\n0.2827\nMS-NeRF [52]\n31.715\n0.9311\n0.1561\n30.224\n0.8840\n0.1816\n26.466\n0.7070\n0.3225\nEyeful Workshop\nGT Test Image\nGT & SfM Normal\nOurs\nRef-NeRF [41]\nMS-NeRF [52]\nNeRF [29]\nNISR LivingRoom2\nEyeful Office2\nFigure 6. Comparisons of novel-view synthesis quality and normal map visualizations. Our method consistently reconstructs reflections\nwhile other methods either produce \u2018faked\u2019 reflections, resulting in incorrect normals, or fail to model reflections entirely.\ndensity gradient direction. Further elaboration on these loss\ncomponents can be found in the supplementary material.\nIn our experiments, we set \u03bbdist = 0.002, aligning with\nthe settings of the \u201cnerfacto\u201d model in Nerfstudio [38]. For\n\u03bbmono, we choose a value of 1 in the first 4K iterations, and\nreduce to 0 thereafter to cease its effect, as described earlier.\nWe also assign \u03bbnorm = 10\u22123, which is slightly higher than\nthe weight in Ref-NeRF [41], as we find that this produces\nslightly smoother normals without substantially compromis-\ning the rendering quality.\n5. Experiments\nImplementation.\nTo model room-scale scenes, we employ\na network architecture similar to the \u201cnerfacto\u201d model pre-\nsented in Nerfstudio [38]. We use two small density networks\nas proposal networks, supervised via Lprop. We sample 256\nand 96 points for each proposal network, and 48 points for\nthe final NeRF model. These three networks all use hash-\nbased positional encodings. When querying the hash features\nin the final NeRF model, we incorporate the LOD-aware\nscheme proposed in VR-NeRF [48]. We train our model\nfor 100,000 iterations and randomly sample 12,800 rays in\neach iteration. This process takes around 8 GB of GPU mem-\nory and approximately 3.5 hours to train a model using an\nNVIDIA A100 GPU. Further details regarding the model\u2019s\nstructure can be found in the supplementary materials.\nDatasets.\nWe evaluate our method on several datasets with\na focus on indoor scenes characterized by near-field lighting\nconditions. First, we evaluate on the Eyeful Tower dataset\n[48], which provides high-quality HDR captures of 11 indoor\nscenes. Each scene is coupled with calibrated camera param-\neters and a mesh reconstructed via Agisoft Metashape [1].\nWe select 9 scenes that feature notable reflective properties.\nWe downsample the images of each scene to a resolution\nof 854\u00d71280 pixels. We curated around 50\u201370 views per\n6\nTest Image\nOurs\nFinal\nDiffuse\nSpecular\nTint\nRoughness\nNormal\nRef-NeRF\nTest Image\nOurs\nRef-NeRF\nFigure 7. Intermediate components of our approach compared to Ref-NeRF [41]. Our approach produces a more meaningful decomposition\nunder room-scale lighting settings.\nscene that contain glossy surfaces for evaluation, leaving\nthe remaining views for training. We also evaluate our ap-\nproach on public indoor datasets NISR [45] and Inria [33]\n(NISR+Inria). Moreover, to assess the performance under\nfar-field lighting, we evaluate the real shiny dataset in Ref-\nNeRF [41]. We report the average PSNR, SSIM, and LPIPS\n[59] metrics for evaluating rendering quality.\n5.1. Comparisons\nWe compare our method with several baselines: NeRF [29],\nRef-NeRF [41], and MS-NeRF [52], which specializes in\nmirror-like reflections by decomposing NeRF into multiple\nspaces. For a fair comparison, we re-implement these base-\nlines, such that we share the same NeRF backbone and ren-\ndering configurations, with the only difference being the way\ndifferent methods decompose and parameterize the output\ncolor. We report the numerical results across three datasets in\nTable 1. Our method demonstrates superior performance on\nthe Eyeful Tower dataset, indicating the effectiveness of our\nmethod. On the NISR+Inria datasets, our method marginally\noutperforms the baselines, likely due to the dataset con-\ntaining few reflection surfaces. Notably, while our method\nis tailored for near-field lighting conditions, it also shows\npromising results on the Shiny dataset, which comprises\nfar-field lighting scenarios. This is because our Gaussian\ndirectional encoding can simulate a spatially invariant en-\ncoding by positioning Gaussians at a significant distance.\nQualitative results on the Eyeful Tower and NISR+Inria\ndatasets are provided in Figure 6. We can see that while other\nbaselines occasionally synthesize plausible reflections, they\nresort to approximations that fake the reflections by placing\nemitters underneath the surface. As a result, they either pro-\nduce incorrect geometry, or fail to model the reflections. Our\nmethod, in contrast, successfully models specular highlights\nFigure 8. We evaluate the novel-view synthesis quality with respect\nto the number of Gaussians across five scenes. The green dashed\nline is the setting we use in our experiments.\non the surface. We provide additional video results in the\nsupplementary material.\nWe also visualize and compare the decomposition pro-\nduced by our method and Ref-NeRF in Figure 7. We can see\nthat Ref-NeRF fails to obtain a meaningful decomposition\nunder near-field lighting, and produces holes in the geom-\netry, whereas our method consistently achieves a realistic\nseparation of specular and diffuse components.\n5.2. Ablation Studies\nNumber of Gaussians.\nOne important hyperparameter in\nthe Gaussian directional encoding is the number of Gaus-\nsians, as it directly influences the model\u2019s capacity to repre-\nsent specular colors. We conduct experiments to evaluate the\nimpact of varying the number of Gaussians on five scenes\nfrom the Eyeful Tower dataset, and show the relationship\nbetween the number of Gaussians and the rendering qual-\nity in Figure 8. The rendering quality improves when using\nmore Gaussians, but the improvement saturates as the num-\nber increases beyond 400. Note that using a larger number\nof Gaussians also entails greater computation costs and GPU\n7\nTable 2. Ablations of our method on the Eyeful Tower dataset [48].\nThe \u201ce.s.\u201d indicates early stopping the Lmono after 4K iterations.\nGaussians Mono.\nPrior\nE. S.\nLmono\nMethod\nInit. Opt.\nPSNR\u2191 SSIM\u2191 LPIPS\u2193\nFull\n\u2713\n\u2713\n\u2713\n\u2713\n32.58\n0.9328 0.1445\nw/o init\n\u2717\n\u2713\n\u2713\n\u2713\n32.52\n0.9304 0.1429\nw/o opt.\n\u2713\n\u2717\n\u2713\n\u2713\n32.06\n0.9265 0.1581\nw/o Lmono\n\u2713\n\u2713\n\u2717\n\u2014\n32.31\n0.9288 0.1503\nw/o e.s.\n\u2713\n\u2713\n\u2713\n\u2717\n32.46\n0.9292 0.1502\nFull\nw/o init\nw/o opt.\nSpecular\nTint\nFinal\nError map\nFigure 9. Example results under different Gaussian optimization\nsettings. Without initializing the Gaussian parameters (\u2018w/o init\u2019)\nor optimizing Gaussians jointly with the NeRF (\u2018w/o opt.\u2019), the\nGaussian embedding struggles to model specularities accurately.\nmemory requirements for every rendered pixel. Therefore,\nwe use 256 Gaussians for all experiments, to strike a good\nbalance between rendering quality and computational effi-\nciency.\nOptimizing Gaussians.\nTo optimize the Gaussian direc-\ntional encoding effectively, we first initialize them by train-\ning an incident light field, and then jointly finetune the Gaus-\nsian encoding together with the NeRF model. We demon-\nstrate the significance of initialization (\u2018w/o init\u2019) and fine-\ntuning (\u2018w/o opt.\u2019) by omitting each process individually.\nWe show quantitative results in Table 2 and a qualitative\nexample in Figure 9. Without initialization, the model can\nstill reconstruct reflections to some extent, resulting in a\nslightly better average LPIPS score, yet it fails to model\nsome specular details, such as the light blobs. Neglecting\nthe joint optimization of Gaussians leads to complete failure\nin modeling specular reflections. As illustrated in Figure 9,\nwith the inaccurate specular modeling, the tints suppress the\nspecular reflections, which ultimately leads to the inability\nto represent reflections in the final rendered image.\nIncreasing roughness\nSpecular\nFinal\nFigure 10. We can control the roughness of the scene by adding an\noffset to the input roughness.\nOurs\nGT\nFigure 11. Our method cannot reconstruct mirror-like perfect re-\nflections due to the limited capacity of the 3D Gaussian encoding.\n6. Discussion and Conclusion\nApplications.\nOur primary goal was to improve the quality\nof novel-view synthesis with specular reflective surfaces. We\nachieve this via our proposed Gaussian directional encoding\nthat enables a meaningful decomposition of specular and\ndiffuse components in a scene. Moreover, this also enables\napplications other than novel-view synthesis, such as reflec-\ntion removal, and surface roughness editing. For instance,\nFigure 7 shows that we can easily remove reflection using\nthe diffuse component. Furthermore, Figure 10 demonstrates\nan example of editing roughness. By adding an offset to the\npredicted roughness during rendering, we can effectively\nmanipulate the glossiness of the real surface.\nLimitations.\nWhile our method improves on existing base-\nlines, it has some limitations. As we parameterize the spec-\nular color via only several hundreds of Gaussians, the en-\ncoding is limited to relatively low frequency compared with\nperfect mirror-like reflections. We show such a failure case\nin Figure 11. We can see that our method is only able to\nlearn a blurry version of the reflection. This could be allevi-\nated by using many more Gaussians, as demonstrated in 3D\nGaussian splatting [17]. However, the computational cost\nof traversing all Gaussians for every pixel quickly becomes\nprohibitive. More efficient traversal, such as by rasterization,\ncould be interesting future work.\nConclusion.\nIn this paper, we proposed a pipeline to im-\nprove the existing approach in modeling and reconstructing\nview-dependent effects in a NeRF representation. Central\nto our approach is a new Gaussian directional encoding to\nenhance the capability of neural radiance fields to model\n8\nspecular reflections under near-field lighting. We also uti-\nlize monocular normal supervision to help resolve shape\u2013\nradiance ambiguity. Experiments have demonstrated the ef-\nfectiveness of each of our contributions. We believe this work\nproposes a practical and effective solution for reconstruct-\ning NeRFs in room-scale scenes, specifically addressing the\nchallenges of accurately capturing specular reflections.\nAcknowledgments.\nThe authors from HKUST were partly\nsupported by the Hong Kong Research Grants Council\n(RGC). We would like to thank Linning Xu and Zhao Dong\nfor helpful discussions.\nReferences\n[1] Agisoft LLC. Agisoft Metashape Professional. Computer\nsoftware, 2022. 6\n[2] Dejan Azinovi\u00b4c, Tzu-Mao Li, Anton Kaplanyan, and Matthias\nNie\u00dfner. Inverse path tracing for joint material and lighting\nestimation. In CVPR, 2019. 2\n[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded\nanti-aliased neural radiance fields. In CVPR, 2022. 5, 4\n[4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,\nand Matthias M\u00a8uller. ZoeDepth: Zero-shot transfer by com-\nbining relative and metric depth. arXiv:2302.12288, 2023.\n5\n[5] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall,\nKalyan Sunkavalli, Milo\u02c7s Ha\u02c7san, Yannick Hold-Geoffroy,\nDavid Kriegman, and Ravi Ramamoorthi. Neural reflectance\nfields for appearance acquisition. arXiv:2008.03824, 2020. 2\n[6] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar-\nron, Ce Liu, and Hendrik P.A. Lensch. NeRD: Neural re-\nflectance decomposition from image collections. In ICCV,\n2021. 2\n[7] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu,\nJonathan T. Barron, and Hendrik P.A. Lensch. Neural-PIL:\nNeural pre-integrated lighting for reflectance decomposition.\nIn NeurIPS, 2021. 2\n[8] G. Bradski. The OpenCV Library. Dr. Dobb\u2019s Journal of\nSoftware Tools, 2000. 3\n[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. TensoRF: Tensorial radiance fields. In ECCV, 2022.\n4\n[10] Robert L Cook and Kenneth E Torrance. A reflectance model\nfor computer graphics. ACM Trans. Graph., 1(1):7\u201324, 1982.\n3\n[11] Youming Deng, Xueting Li, Sifei Liu, and Ming-Hsuan Yang.\nDIP: Differentiable interreflection-aware physics-based in-\nverse rendering. In 3DV, 2024. 2\n[12] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir\nZamir. Omnidata: A scalable pipeline for making multi-task\nmid-level vision datasets from 3D scans. In ICCV, 2021. 5\n[13] Wenhang Ge, Tao Hu, Haoyu Zhao, Shu Liu, and Ying-Cong\nChen. Ref-NeuS: Ambiguity-reduced neural implicit surface\nlearning for multi-view reconstruction with reflection. In\nICCV, 2023. 1, 2\n[14] Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and Song-\nHai Zhang. NeRFReN: Neural radiance fields with reflections.\nIn CVPR, 2022. 3\n[15] Leif Van Holland, Ruben Bliersbach, Jan U. M\u00a8uller, Patrick\nStotko, and Reinhard Klein. TraM-NeRF: Tracing mirror\nand near-perfect specular reflections through neural radiance\nfields. arXiv:2310.10650, 2023. 3\n[16] James T. Kajiya. The rendering equation. Computer Graphics\n(Proceedings of SIGGRAPH), 20(4):143\u2013150, 1986. 3\n[17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. 3D Gaussian splatting for real-time radi-\nance field rendering. ACM Trans. Graph., 42(4):139:1\u201314,\n2023. 8\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 1, 3\n[19] Georgios Kopanas, Thomas Leimk\u00a8uhler, Gilles Rainer,\nCl\u00b4ement Jambon, and George Drettakis. Neural point cat-\nacaustics for novel-view synthesis of reflections. ACM Trans.\nGraph., 41(6):201:1\u201315, 2022. 3\n[20] Junxuan Li and Hongdong Li. Neural reflectance for shape\nrecovery with shadow handling. In CVPR, 2022. 2\n[21] Quewei Li, Jie Guo, Yang Fei, Feichao Li, and Yanwen\nGuo. NeuLighting: Neural lighting for free viewpoint out-\ndoor scene relighting with unconstrained photo collections.\nIn SIGGRAPH Asia, pages 13:1\u20139, 2022. 2\n[22] Tzu-Mao Li, Miika Aittala, Fr\u00b4edo Durand, and Jaakko Lehti-\nnen. Differentiable Monte Carlo ray tracing through edge\nsampling. ACM Trans. Graph., 37(6):222:1\u201311, 2018. 2\n[23] Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H Tay-\nlor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.\nNeuralangelo: High-fidelity neural surface reconstruction. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 4\n[24] Ruofan Liang, Jiahao Zhang, Haoda Li, Chen Yang,\nYushi Guan, and Nandita Vijaykumar.\nSPIDR: SDF-\nbased neural point fields for illumination and deformation.\narXiv:2210.08398, 2022. 2\n[25] Xinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. Clean-\nNeRF: Reformulating NeRF to account for view-dependent\nobservations. arXiv:2303.14707, 2023. 2\n[26] Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng\nWang, Lingjie Liu, Taku Komura, and Wenping Wang. NeRO:\nNeural geometry and BRDF reconstruction of reflective ob-\njects from multiview images. ACM Trans. Graph., pages\n114:1\u201322, 2023. 2\n[27] Linjie Lyu, Ayush Tewari, Thomas Leimkuehler, Marc Haber-\nmann, and Christian Theobalt. Neural radiance transfer fields\nfor relightable novel-view synthesis with global illumination.\nIn ECCV, 2022.\n[28] Alexander Mai, Dor Verbin, Falko Kuester, and Sara\nFridovich-Keil. Neural microfacet fields for inverse rendering.\nIn ICCV, 2023. 2\n[29] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. In ECCV, 2020. 1, 2, 4, 6, 7, 5\n9\n[30] Gene S Miller and CR Hoffman. Illumination and reflection\nmaps. In ACM SIGGRAPH, 1984. 1\n[31] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander\nKeller. Instant neural graphics primitives with a multireso-\nlution hash encoding. ACM Trans. Graph., 41(4):102:1\u201315,\n2022. 1, 4\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\nimperative style, high-performance deep learning library. In\nAdvances in Neural Information Processing Systems 32, pages\n8024\u20138035. Curran Associates, Inc., 2019. 1, 4\n[33] Julien Philip, S\u00b4ebastien Morgenthaler, Micha\u00a8el Gharbi, and\nGeorge Drettakis. Free-viewpoint indoor neural relighting\nfrom multi-view stereo. ACM Trans. Graph., 40(5):194:1\u201318,\n2021. 2, 6, 7\n[34] Ren\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. TPAMI, 44(3):1623\u20131637, 2022. 5\n[35] Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie\nLiu, Vladislav Golyanik, and Christian Theobalt. NeRF for\noutdoor scene relighting. In ECCV, 2022. 2\n[36] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew\nTancik, Ben Mildenhall, and Jonathan T. Barron. NeRV:\nNeural reflectance and visibility fields for relighting and view\nsynthesis. In CVPR, 2021. 2\n[37] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. In NeurIPS, 2020. 2\n[38] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent\nYi, Terrance Wang, Alexander Kristoffersen, Jake Austin,\nKamyar Salahi, Abhik Ahuja, David Mcallister, Justin Kerr,\nand Angjoo Kanazawa. Nerfstudio: A modular framework\nfor neural radiance field development. In SIGGRAPH, pages\n72:1\u201312, 2023. 6, 2\n[39] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srini-\nvasan, Edgar Tretschk, Yifan Wang, Christoph Lassner,\nVincent Sitzmann, Ricardo Martin-Brualla, Stephen Lom-\nbardi, Tomas Simon, Christian Theobalt, Matthias Niessner,\nJonathan T. Barron, Gordon Wetzstein, Michael Zollh\u00a8ofer,\nand Vladislav Golyanik. Advances in neural rendering. Com-\nput. Graph. Forum, 41(2):703\u2013735, 2022. 1\n[40] Kushagra Tiwary, Akshat Dave, Nikhil Behari, Tzofi\nKlinghoffer, Ashok Veeraraghavan, and Ramesh Raskar.\nORCa: Glossy objects as radiance field cameras. In CVPR,\n2023. 3\n[41] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF:\nStructured view-dependent appearance for neural radiance\nfields. In CVPR, 2022. 1, 2, 3, 5, 6, 7, 4\n[42] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob\nMunkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, and\nSanja Fidler. Neural fields meet explicit geometric represen-\ntations for inverse rendering of urban scenes. In CVPR, 2023.\n2\n[43] Liwen Wu, Rui Zhu, Mustafa B. Yaldiz, Yinhao Zhu, Hong\nCai, Janarbek Matai, Fatih Porikli, Tzu-Mao Li, Manmohan\nChandraker, and Ravi Ramamoorthi. Factorized inverse path\ntracing for efficient and accurate material-lighting estimation.\nIn ICCV, 2023. 4, 5\n[44] Liwen Wu, Rui Zhu, Mustafa B. Yaldiz, Yinhao Zhu, Hong\nCai, Janarbek Matai, Fatih Porikli, Tzu-Mao Li, Manmohan\nChandraker, and Ravi Ramamoorthi. Factorized inverse path\ntracing for efficient and accurate material-lighting estimation.\nIn ICCV, 2023. 2\n[45] Xiuchao Wu, Jiamin Xu, Zihan Zhu, Hujun Bao, Qixing\nHuang, James Tompkin, and Weiwei Xu. Scalable neural\nindoor scene rendering. ACM Trans. Graph., 41(4):98:1\u201316,\n2022. 3, 6, 7\n[46] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\nShiqin Yan, Numair Khan, Federico Tombari, James Tompkin,\nVincent Sitzmann, and Srinath Sridhar. Neural fields in visual\ncomputing and beyond. Comput. Graph. Forum, 2022. 1\n[47] Jiamin Xu, Xiuchao Wu, Zihan Zhu, Qixing Huang, Yin Yang,\nHujun Bao, and Weiwei Xu. Scalable image-based indoor\nscene rendering with reflections. ACM Trans. Graph., 40(4):\n60:1\u201314, 2021. 3\n[48] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia,\nAayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo\nPorzi, Peter Kontschieder, Alja\u02c7z Bo\u02c7zi\u02c7c, Dahua Lin, Michael\nZollh\u00a8ofer, and Christian Richardt. VR-NeRF: High-fidelity\nvirtualized walkable spaces. In SIGGRAPH Asia, 2023. 1, 6,\n8, 4, 7\n[49] Zhiwen Yan, Chen Li, and Gim Hee Lee. NeRF-DS: Neural\nradiance fields for dynamic specular objects. In CVPR, 2023.\n2\n[50] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-\nproving few-shot neural rendering with free frequency reg-\nularization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n8254\u20138263, 2023. 4\n[51] Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang,\nDavid McKinnon, Yanghai Tsin, and Long Quan. NeILF:\nNeural incident light field for physically-based material esti-\nmation. In ECCV, 2022. 2\n[52] Ze-Xin Yin, Jiaxiong Qiu, Ming-Ming Cheng, and Bo Ren.\nMulti-space neural radiance fields. In CVPR, 2023. 3, 6, 7, 5\n[53] Bohan Yu, Siqi Yang, Xuanning Cui, Siyan Dong, Baoquan\nChen, and Boxin Shi. MILO: Multi-bounce inverse rendering\nfor indoor scene with light-emitting objects. IEEE TPAMI,\n45(8):10129\u201310142, 2023. 2\n[54] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-\ntler, and Andreas Geiger. MonoSDF: Exploring monocular\ngeometric cues for neural implicit surface reconstruction. In\nNeurIPS, 2022. 5\n[55] Chong Zeng, Guojun Chen, Yue Dong, Pieter Peers, Hongzhi\nWu, and Xin Tong. Relighting neural radiance fields with\nshadow and highlight hints. In SIGGRAPH, 2023. 2\n10\n[56] Junyi Zeng, Chong Bao, Rui Chen, Zilong Dong, Guofeng\nZhang, Hujun Bao, and Zhaopeng Cui. Mirror-NeRF: Learn-\ning neural radiance fields for mirrors with Whitted-style ray\ntracing. In ACM Multimedia, 2023. 3\n[57] Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang,\nDavid McKinnon, Yanghai Tsin, and Long Quan. NeILF++:\nInter-reflectable light fields for geometry and material estima-\ntion. In ICCV, 2023. 2\n[58] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and\nNoah Snavely. PhySG: Inverse rendering with spherical gaus-\nsians for physics-based material editing and relighting. In\nCVPR, 2021. 2\n[59] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 7\n[60] Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul\nDebevec, William T. Freeman, and Jonathan T. Barron. NeR-\nFactor: Neural factorization of shape and reflectance under an\nunknown illumination. ACM Trans. Graph., 40(6):237:1\u201318,\n2021. 2\n[61] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei\nJia, and Xiaowei Zhou. Modeling indirect illumination for\ninverse rendering. In CVPR, 2022. 2\n[62] Youjia Zhang, Teng Xu, Junqing Yu, Yuteng Ye, Junle\nWang, Yanqing Jing, Jingyi Yu, and Wei Yang.\nNeMF:\nInverse volume rendering with neural microflake field.\narXiv:2304.00782, 2023. 2\n[63] Yiyu Zhuang, Qi Zhang, Xuan Wang, Hao Zhu, Ying Feng,\nXiaoyu Li, Ying Shan, and Xun Cao. NeAI: A pre-convoluted\nrepresentation for plug-and-play neural ambient illumination.\narXiv:2304.08757, 2023. 2\n11\nSpecNeRF: Gaussian Directional Encoding for Specular Reflections\nSupplementary Material\nA. Supplementary Video\nWe provide a supplementary video for visual comparisons\nunder a moving camera trajectory. We highly encourage\nreaders to watch our video, where our method produces\nresults with better specular reflection reconstruction.\nB. Gaussian Directional Encoding Proofs\nRecall that we define each Gaussian as:\nG(x) = exp\n\u0010\n\u2212\n\r\rQ(x \u2212 \u00b5; q) \u2299 \u03c3\u22121\r\r2\n2\n\u0011\n,\n(11)\nwhere \u00b5 is the position and \u03c3 the scale of the Gaussian.\nQ(x; q) applies the quaternion rotation q to a 3D vector x.\nFor ease of notation, we omit the subscript i (compared to\nthe main paper) as the same equation is applied to every\nGaussian. In practice, we optimize the inverse scale \u03c8 =\n\u03c3\u22121 instead of directly using \u03c3 to improve the numerical\nstability.\nWe further define the basis function P(o, d) over a given\nray o+td with the Gaussian parameters (\u00b5, \u03c3, q) as:\nP(o, d) = max\nt\u22650 G(o + td).\n(12)\nWe start by applying the following variable substitution that\nconverts the ray origin o and direction d from world-space\ninto the space of the Gaussian:\no = Q(o \u2212 \u00b5; q) \u2299 \u03c3\u22121,\n(13)\nd = Q(d; q) \u2299 \u03c3\u22121.\n(14)\nIt follows that\nG(o + td) = exp\n\u0010\n\u2212\n\r\rQ(o + td \u2212 \u00b5; q) \u2299 \u03c3\u22121\r\r2\n2\n\u0011\n(15)\n= exp\n\u0010\n\u2212\n\r\ro + td\n\r\r2\n2\n\u0011\n(16)\n= exp\n\u0010\n\u2212o\u22a4o \u2212 2o\u22a4dt \u2212 d\n\u22a4dt2\u0011\n.\n(17)\nSince the exponential function is monotonic, G is maximized\nwhen the quadratic function\nf(t) = \u2212o\u22a4o \u2212 2o\u22a4dt \u2212 d\n\u22a4dt2\n(18)\nreaches its maximum. Since the quadratic coefficient, \u2212d\n\u22a4d,\nis negative for any non-zero vector d, f(t) reaches its maxi-\nmum when f \u2032(t) = 0, i.e. for t = t0 = \u2212 o\u22a4d\nd\n\u22a4d. Furthermore,\nG(o + td) monotonically decreases for t \u2265 t0. Given that\nt \u2265 0, when t0 \u2264 0, the G(o+td) reaches maximum always\nat t = 0. To sum up, the maximum value of G(o + td) falls\ninto the following two cases:\nmax\nt\u22650 G(o + td) =\n\uf8f1\n\uf8f2\n\uf8f3\nexp\n\u0010\n\u2212\n\r\ro + t0d\n\r\r2\n2\n\u0011\nt0 > 0\nexp\n\u0010\n\u2212 \u2225o\u22252\n2\n\u0011\notherwise,\n(19)\nBy substituting \u2212 o\u22a4d\nd\n\u22a4d for t0, this equation is the same as\nEquation 6 in the main paper.\nC. Implementation details\nFigure 12 zooms into our model\u2019s network architecture and\nclarifies the role of each used MLP.\nC.1. Model Structure\nWe list the model structure parameters in Table 3. We use\nseparate MLP heads to predict each property at each sample\nlocation. Note that we use a lower resolution configuration\nfor normal hash encoding, because we find that constraining\nthe smoothness of the normal stabilizes the optimization\nprocess and leads to better specular reflection reconstruction.\nNormal parameterization.\nTo predict normals, we first\noutput a 3-element vector n\u2032\nraw using the normal MLP with-\nout any output activation and normalize it to get the predicted\nnormal n\u2032 = n\u2032\nraw/ \u2225n\u2032\nraw\u22252. However, in practice, we find\nthat this will occasionally lead to a normal flipping issue\nwhen n\u2032\nraw is numerically small and n\u2032 will flip its direction\nwith only a very small deviation of n\u2032\nraw during training. Fig-\nure 13 visualizes this issue. The flip of the predicted normal\nwill further lead to a suboptimal normals derived from the\ndensity gradient due to the normal prediction loss Lnorm. To\nalleviate this normal flip issue, we correct the direction of\nthe predicted normal by forcing the angle between the final\nnormal n\u2032 and the view direction to be smaller than 90\u00b0:\nn\u2032 = \u2212 sign(d \u00b7 n\u2032\nraw)\nn\u2032\nraw\n\u2225n\u2032raw\u22252\n,\n(20)\nwhere d is the ray direction. We can see from Figure 13 that\nthis normal correction operation helps us prevent the normal\nflip, and yields a better normal prediction.\nC.2. Training and Rendering Configuration\nWe use the Adam optimizer [18] to train our NeRF model\nusing the default parameter configurations in PyTorch [32]\nfor the optimizer, except that we set the learning rate to\n0.005. When rendering a pixel, we first shoot rays from\nthe camera origin to the pixel locations, and then sample\n1\nNormal\nn\u2019\nTint\ns\u2019\n3D position\nx\nview dir\nd\nHash \nEncoding\nSH \nEncoding\nDiffuse\ncd\u2019\nNormal \nMLP\nNormal\nHash \nEncoding\nDensity \nMLP\nTint \nMLP\nDensity \nfeature\n\ud835\udf11\u2019\nDensity\n\u03c4\u2019\nDiffuse \nMLP\nRoughness \nMLP\nRoughness\n\u03c1\u2019\nSpecular \nMLP\n3D Gaussian\nDirectional \nEncoding\nSpecular\ncs\nReflection \norigin\nor\nReflection \ndirection\ndr\nRoughness\n\u03c1\nFigure 12. We zoom in the MLPs and some important modules as in Figure 2. The detailed module configurations are shown in Table 3.\nTable 3. The value for each parameter. The module names are\nconsistent with those shown in Figure 12. For all MLPs, we use\nReLU activations in hidden layers.\nModule\nConfiguration\nValue\nSH Encoding\nOrder\n3\nTint MLP\n# of hidden layer\n2\n# of neuron per layer\n64\nOutput activation\nSigmoid\nHash\nEncoding\n# of levels\n16\nHash table size\n222\n# of feature dim. per entry\n2\nCoarse resolution\n128\nScale factor per level\n1.4\nDensity\nMLP\n# of hidden layer\n1\n# of neuron per layer\n64\nOutput activation\nExp\nDensity feature dim.\n16\nDiffuse\nMLP\n# of hidden layer\n2\n# of neuron per layer\n64\nOutput activation\nSigmoid\nRoughness\nMLP\n# of hidden layer\n2\n# of neuron per layer\n64\nOutput activation\nSoftplus\nNormal\nHash\nEncoding\n# of levels\n4\nHash table size\n219\n# of feature dim. per entry\n4\nCoarse resolution\n16\nScale factor per level\n1.5\nNormal\nMLP\n# of hidden layer\n1\n# of neuron per layer\n64\nOutput activation\nNone\nSpecular\nMLP\n# of hidden layer\n2\n# of neuron per layer\n64\nOutput activation\nSigmoid\npoints along each ray. Similar to Nerfstudio [38], we use two\nlevels of proposal sampling, guided by two density fields.\nSpecifically, in the first round, we sample 256 points using\nGround Truth\nw/o normal corr.\nw/ normal corr.\nImage\nPredicted normal\nDensity gradient\nFigure 13. An example of the normal flip issue. As indicated by\nthe green arrow, the predicted normal is occasionally flipped due\nto small perturbation during training, which leads to artifacts in\nrendering images and the density gradient. Our normal correction\n(normal corr.) prevents the flip issue by optionally reversing the\nnormal direction based on the view direction.\nexponential distance. We set the far distance to a constant\nvalue of 800 meters, and we determine the near distance\nfor each scene using the minimum distance between all the\nstructure-from-motion points and the viewing cameras. Then,\nin each iteration of the proposal sampling process, we feed\nthe samples into the proposal network sampler and generate\nnew samples based on the integration weights of the input\nsamples. We sample 96 samples in the first iteration of the\nproposal process, followed by 48 samples in the second. The\nmodel structures of the proposal networks follow those in\nthe \u201cnerfactor\u201d model in Nerfstudio [38].\n2\nBlur kernel size = 1\n9\n17\n33\n65\n129\nBlurred Input\nPrediction\nFigure 14. One example of the Gaussian initialization input and predictions for different scales.\nC.3. Gaussian Parameter Optimization\nTo obtain optimal Gaussian parameters for the Gaussian di-\nrectional encoding, we use an initialization stage to seed the\nGaussian parameters and specular MLP weights. The pro-\ncess is illustrated in Figure 15. We optimize a pre-convolved\nincident light field composed of our 3D Gaussian directional\nencoding and the Specular MLP. We first apply a range of\nGaussian blurs to all input images using a series of standard\ndeviations, generating pyramids of blurry input images. In\nour experiments, we first scale the input images to have 360\npixels along the longest axis. Then, we apply OpenCV\u2019s\nGaussianBlur [8] with kernel sizes (1, 3, 5, 9, 17, 33, 65,\n129). Regions that involve the image border during blurring\nare marked as invalid, resulting in a wider invalid border\nwith larger kernel size. Figure 14 showcases one example\nview with some of the blur kernels used.\nAll valid blurred pixels compose our ray dataset for the\ninitialization stage. In each iteration, we sample 25,600 pix-\nels from the ray dataset, and generate the corresponding ray\norigin o, direction d, and the blur kernel size k. We train the\nGaussian parameters and Specular MLP using Adam [18]\noptimizer with a learning rate of 0.001, and leave other pa-\nrameters as default. We supervise the output color using the\ncorresponding blurry color in the Gaussian pyramid using L1\nerrors. We find this small network converges quickly, thus\nwe only train for 8,000 iterations, which takes around half\nan hour to finish on one NVIDIA A100 GPU. We visualize\nthe fitted pre-convolved incident light field in Figure 14. The\nreconstructed pre-convolved light field well-represents the\ninput with multiple blur levels. We also visualize the fitted\nGaussian blobs of two scenes in Figure 16. We can see that\nsome Gaussian blobs are aligned with the underlying objects\n(e.g. the lamp on the ceiling).\nSpecular \nMLP\n3D Gaussian\nDirectional \nEncoding\nOutput \nColor\nc\nRay origin\no\nRay direction\nd\nBlur kernel \nsize\nk\nSpecular \nMLP\n3D Gaussian\nDirectional \nEncoding\nSpecular\ncs\nReflection \norigin\nor\nReflection \ndirection\ndr\nRoughness\n\u03c1\nBlurry \nColor\nck\nsupervision\nInitialization Stage\nInitialize\nInitialize\nNeRF Optimization Stage\nFigure 15. Illustration of the initialization stage. We optimize the\nGaussian parameters and the Specular MLP using the Gaussian-\nblurred input images, and then use them as initialization for the\nNeRF optimization stage.\nSfM mesh\nGaussian blobs\nCombined view\nFigure 16. Visualization of the learned Gaussian blobs of two\nscenes. We assign a random color for each Gaussian blob for better\nvisibility.\nC.4. Losses\nRecall that in our experiments, the final loss is a combination\nof several terms:\nL = Lc+Lprop+\u03bbdistLdist+\u03bbmonoLmono+\u03bbnormLnorm. (21)\n3\nIn this section, we follow the notation that i = 1, ..., N is\nthe sample point index along a ray. We omit the ray index as\neach loss term has the same form for all rays. The loss term\nis averaged over all rays within a training batch.\nReconstruction Loss.\nThe reconstruction loss Lc is the\nL1 norm between the predicted color c and the ground-truth\ncolor cgt:\nLc = \u2225c \u2212 cgt\u22251 .\n(22)\nFor the Eyeful Tower dataset, we compute the reconstruction\nloss in the Perceptual Quantizer (PQ) color space, as in VR-\nNeRF [48]. For other public datasets, we use the standard\nsRGB color space.\nProposal Loss and Distortion Loss.\nThe proposal loss\nLprop supervises the density field of the proposal network to\nbe consistent with that of the main NeRF. The distortion loss\nLdist is a regularization term for the density field of the main\nNeRF. It consolidates the volumetric blending weights into\nas small a region as possible. Please refer to Barron et al. [3]\nfor the detailed definitions and explanations of both losses.\nNormal Prediction Loss.\nWe encourage the predicted nor-\nmals from the normal MLP to be consistent with the underly-\ning geometry of NeRF. For this, we use a normal prediction\nloss Lnorm that supervises the normal n\u2032\ni predicted for ev-\nery sample point using the normal MLP and NeRF density\ngradient gi:\nLnorm = 1\nN\nX\ni\n\r\r\r\rn\u2032\ni \u2212 \u2212gi\n\u2225gi\u2225\n\r\r\r\r .\n(23)\nTo compute the gradient of the density \u03c4 \u2032 with respect to\nthe input world coordinate x = (x, y, z), we could use the\nanalytical gradient, which is natively supported by PyTorch\n[32]. However, we model the density field using a hash-\ngrid based representation, which is prone to noisy gradients\nand has poor optimization performance [23]. Therefore, we\nadopt a modified version of the numerical gradient from\nNeuralangelo [23]. To compute the gradient along the x-axis,\nwe use\n\u2207x\u03c4 \u2032 = \u03c4 \u2032(x + \u03f5x) \u2212 \u03c4 \u2032(x \u2212 \u03f5x)\n2\u03f5\n,\n(24)\nwhere \u03f5x = (\u03f5, 0, 0). The equations for computing the gradi-\nent along the y- and z-axes can be derived analogously. Over-\nall, \u2207x\u03c4 involves sampling six additional points to query\nthe density value. Instead of predefining the schedule of the\n\u03f5 value during training, we compute a per-sample \u03f5 value\nthat is consistent with the cone tracing radius at the sample\nlocation: \u03f5 = t\u00b7r. Here, t is the ray-marching distance of\nthe sample point, and r is the base radius of a pixel at unit\ndistance along the ray.\nAdditional Losses.\nFor the Eyeful Tower dataset, we also\ndeploy a depth supervision loss and an \u201cempty around cam-\nera\u201d loss, following VR-NeRF [48]. For the depth loss, we\nsupervise the NeRF depth with the depth from structure-\nfrom-motion mesh using L1 distance in the first 500 iter-\nations. For the \u201cempty around camera\u201d loss, we randomly\nsample 128 points in the unit sphere around training cameras,\nand regularize the density value to be zero. This reduces the\nnear-plane ambiguity as shown in FreeNeRF [50]. We set the\nweights of the depth loss and \u201cempty around camera\u201d loss to\n0.1 and 10, respectively.\nD. Additional Experiments\nD.1. Number of Gaussians\nWe test the GPU memory usage of our Gaussian directional\nencoding and the specular MLP under a series of Gaussians,\nand visualize the results in Figure 17. We can see that our\nreflection model adds very little GPU memory overhead\ncompared to the approximately 8 GB of overall memory\nused for training the whole pipeline.\nFigure 17. The GPU memory consumption of the Gaussian direc-\ntional encoding and the Specular MLP with various number of\nGaussians. We test GPU memory with a batch size of 12,800 rays.\nThe green dashed line is the configuration used in our experiments.\nD.2. Synthetic Dataset\nWe compare our method with several baselines on the FIPT\nsynthetic dataset [43]. In addition to the baselines described\nin the main paper, we also compare with FIPT [43], a state-\nof-the-art path-tracing-based inverse rendering approach. We\nreport the average PSNR, SSIM and LPIPS metrics for novel-\nview synthesis. Since we have the ground-truth mesh for\nthe synthetic dataset, we also report the mean angular error\n(MAE) used in Ref-NeRF [41] for evaluating the estimated\nnormal accuracy. The results in Table 4 show that our method\nachieves the best novel-view synthesis quality and geometry\naccuracy. Interestingly, despite the use of ground-truth geom-\netry for the physically based inverse rendering approach, the\nnovel-view synthesis is worse than any NeRF-based base-\nline by a large margin. This suggests that introducing a fully\nphysically based rendering model may be a disadvantage\nwhen it comes to novel-view synthesis quality, at least com-\npared to NeRF-like approaches that are tailored for the view\nsynthesis task.\n4\nTable 4. Quantitative comparisons of novel-view synthesis and ge-\nometry quality on the FIPT synthetic dataset. Our method achieves\nthe best view synthesis quality, and is most accurate in terms of\ngeometry. We highlight the best numbers in bold.\nMethods\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nMAE\u25e6\u2193\nOurs\n32.043\n0.8657\n0.1266\n16.09\nNeRF [29]\n31.621\n0.8586\n0.1325\n34.16\nRef-NeRF [41]\n31.952\n0.8650\n0.1250\n18.76\nMS-NeRF [52]\n31.441\n0.8534\n0.1345\n42.19\nFIPT [43]\n28.322\n0.6922\n0.1379\n0\u2020\n\u2020Note that FIPT uses the ground-truth geometry.\nD.3. Additional Results\nWe show additional comparisons and decomposition results\nin Figure 18 and Figure 19. Our method achieves the best\nvisual quality as well as the predicted normal quality for\nspecular reflections.\n5\nShiny1\nGT Test Image\nGT & SfM Normal\nOurs\nRef-NeRF [41]\nMS-NeRF [52]\nNeRF [29]\nEyeful Tower Apartment\nEyeful Tower Office2\nEyeful Tower Office2\nEyeful Tower Workshop\nNISR LivingRoom2\nFigure 18. Comparisons of novel-view synthesis quality and normal map visualizations on the Eyeful Tower [48] and NISR datasets [45].\n6\nTest Image\nOurs\nFinal\nDiffuse\nSpecular\nTint\nRoughness\nNormal\nRef-NeRF\nTest Image\nOurs\nRef-NeRF\nTest Image\nOurs\nRef-NeRF\nTest Image\nOurs\nRef-NeRF\nTest Image\nOurs\nRef-NeRF\nFigure 19. Additional results for intermediate component visualizations of our approach compared to Ref-NeRF [41] on the Eyeful Tower\n[48] and NISR datasets [45]. Our approach produces more accurate decompositions and normal maps.\n7\n"
  },
  {
    "title": "Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting",
    "link": "https://arxiv.org/pdf/2312.13271.pdf",
    "upvote": "4",
    "text": "Repaint123: Fast and High-quality One Image to 3D Generation with\nProgressive Controllable 2D Repainting\nJunwu Zhang1*\nZhenyu Tang1*\nYatian Pang1,3\nXinhua Cheng1\nPeng Jin1\nYida Wei4\nMunan Ning1,2\nLi Yuan1,2\u2020\n1Peking University 2 Pengcheng Laboratory 3 National University of Singapore\n4 Wuhan University\nReference\nNovel views\nNormal\nReference\nNovel views\nNormal\nFigure 1. Repaint123 generates high-quality 3D content with detailed texture in only 2 minutes from a single image. Repaint123 adopts\nGaussian Splatting in the coarse stage, and then utilize a 2D controllable diffusion model with repainting stategy to generate view-consistent\nhigh-quality images. This allows for fast and high-quality refinement of the extracted mesh texture through simple MSE loss.\nAbstract\nRecent one image to 3D generation methods com-\nmonly adopt Score Distillation Sampling (SDS). Despite\nthe impressive results, there are multiple deficiencies in-\ncluding multi-view inconsistency, over-saturated and over-\nsmoothed textures, as well as the slow generation speed. To\naddress these deficiencies, we present Repaint123 to allevi-\nate multi-view bias as well as texture degradation and speed\nup the generation process. The core idea is to combine the\npowerful image generation capability of the 2D diffusion\nmodel and the texture alignment ability of the repainting\nstrategy for generating high-quality multi-view images with\nconsistency. We further propose visibility-aware adaptive\nrepainting strength for overlap regions to enhance the gen-\nerated image quality in the repainting process. The gener-\n*Equal contribution.\n\u2020Corresponding author.\nated high-quality and multi-view consistent images enable\nthe use of simple Mean Square Error (MSE) loss for fast\n3D content generation. We conduct extensive experiments\nand show that our method has a superior ability to gener-\nate high-quality 3D content with multi-view consistency and\nfine textures in 2 minutes from scratch. Our project page\nis available at https://pku-yuangroup.github.\nio/repaint123/.\n1. Introduction\nGenerating 3D content from one given reference image\nplays a key role at the intersection of computer vision\nand computer graphics [11, 17, 24, 25, 33, 35], serving as\na pivotal conduit for innovative applications across fields\nincluding robotics, virtual reality, and augmented reality.\nNonetheless, this task is quite challenging since it is ex-\n1\narXiv:2312.13271v3  [cs.CV]  27 Dec 2023\nCoarse novel view\nMagic123\nControllable 2D Repaint\nBack-projection\nOcclusion region\nRefine(SDS)\nMSE\nRef\nDepth\nRefine\nConsistent & Fine texture\nInconsistent & Poor texture\nRefine\n(MSE)\nRefine\n(SDS)\nNeRF\nNeRF\nGaussian \nSplatting\n30min\n1min\n1min\nCoarse novel view\nRepaint123(Ours)\n30min\n30min\n1min\nReference view\nCoarse novel view\nCoarse novel view\nFigure 2. Motivation of our proposed pipeline. Current methods\nadopt SDS loss, resulting in inconsistent and poor texture. Our\nidea is to combine the powerful image generation capability of the\ncontrollable 2D diffusion model and the texture alignment ability\nof the repainting strategy for generating high-quality multi-view\nconsistent images. The repainted images enable simple MSE loss\nfor fast 3D content generation.\npected to generate high-quality 3D content with multi-view\nconsistency and fine textures in a short period of time.\nRecent studies [22, 24, 28, 50] utilize diffusion mod-\nels [10, 39], which have notably advanced image genera-\ntion techniques, to guide the 3D generation process given\none reference image. Generally, the learnable 3D repre-\nsentation such as NeRF is rendered into multi-view images,\nwhich then are distilled by rich prior knowledge from dif-\nfusion models via Score Distillation Sampling (SDS) [34].\nHowever, SDS may have conflicts with 3D representation\noptimization [16], leading to multi-view bias and texture\ndegradation. Despite their impressive results, multiple defi-\nciencies including multi-view inconsistency as well as over-\nsaturated color and over-smoothed textures are widely ac-\nknowledged. Moreover, SDS is very time-consuming as a\nlarge number of optimization steps are required.\nTo address these deficiencies mentioned above, we pro-\npose a novel method called Repaint123 to alleviate multi-\nview bias as well as texture degradation and speed up the\ngeneration process. Our core idea is shown in Figure 2 re-\nfine stage. We combine the powerful image generation ca-\npability of the 2D diffusion model and the alignment ability\nof the repainting strategy for generating high-quality multi-\nview images with consistency, which enables using simple\nMean Square Error (MSE) loss for fast 3D representation\noptimization. Specifically, our method adopts a two-stage\noptimization strategy. The first stage follows DreamGaus-\nsian [49] to obtain a coarse 3D model in 1 minute. In the\nrefining stage, we first aim to generate multi-view consistent\nimages. For proximal multi-view consistency, we utilize the\ndiffusion model to repaint the texture of occlusion (unob-\nserved) regions by referencing neighboring visible textures.\nTo mitigate accumulated view bias and ensure long-term\nview consistency, we adopt a mutual self-attention strategy\nto query correlated textures from the reference view. As\nfor enhancing the generated image quality, we use a pre-\ntrained 2D diffusion model with the reference image as an\nimage prompt to perform classifier-free guidance. We fur-\nther improve the generated image quality by applying adap-\ntive repainting strengths for the overlap region, based on\nthe visibility from previous views. As a result, with high-\nquality and multi-view consistent images, we can generate\n3D content from these sparse views extremely fast using\nsimple MSE loss.\nWe conduct extensive one image to 3D generation ex-\nperiments on multiple datasets and show that our method\nis able to generate a high-quality 3D object with multi-\nview consistency and fine textures in about 2 minutes from\nscratch. Compared to state-of-the-art techniques, we repre-\nsent a major step towards high-quality 3D generation, sig-\nnificantly improving multi-view consistency, texture qual-\nity, and generation speed.\nOur contributions can be summarized as follows:\n\u2022 Repaint123 comprehensively considers the controllable\nrepainting process for image-to-3d generation, preserving\nboth the proximal view and long-term view consistency.\n\u2022 We also propose to enhance the generated view quality\nby adopting visibility-aware adaptive repainting strengths\nfor the overlap regions.\n\u2022 Through a comprehensive series of experiments, we show\nthat our method consistently demonstrates high-quality\n3D content generation ability in 2 minutes from scratch.\n2. Related Works\n2.1. Diffusion Models for 3D Generation\nThe recent notable achievements in 2D diffusion mod-\nels [10, 39] have brought about exciting prospects for gen-\nerating 3D objects. Pioneering studies [34, 52] have intro-\nduced the concept of distilling a 2D text-to-image genera-\ntion model for the purpose of generating 3D shapes. Subse-\nquent works[1, 5, 6, 9, 16, 22, 28, 35, 37, 42\u201344, 50, 51, 54\u2013\n56, 59, 60, 63] have adopted a similar per-shape optimiza-\ntion approach, building upon these initial works. Never-\ntheless, the majority of these techniques consistently expe-\nrience low efficiency and multi-face issues. In contrast to\na previous study HiFi-123 [60] that employed similar in-\nversion and attention injection techniques for image-to-3D\ngeneration, our approach differs in the selection of diffu-\nsion model and incorporation of depth prior. We utilize sta-\nble diffusion with ControlNet, introducing depth prior as\nan additional condition for simplicity and flexibility across\n2\nvarious other conditions. In comparison, HiFi-123 employs\na depth-based diffusion model (stable-diffusion-2-depth)\nconcatenating depth latent with original latent for more pre-\ncise geometry control. Meanwhile, we also differ in many\nother aspects, like the use of repainting strategy, optimiza-\ntion with MSE loss, and Gaussian Splatting representation.\nRecently, some works [23, 25, 26, 47] extend 2D diffu-\nsion models from single-view images to multi-view images\nto generate multi-view images for reconstruction, while\nthese methods usually suffer from low-quality textures as\nthe multi-view diffusion models are trained on limited and\nsynthesized data.\n2.2. Controllable Image Synthesis\nOne of the most significant challenges in the field of image\ngeneration has been controllability. Many works have been\ndone recently to increase the controllability of generated\nimages. ControlNet [61] and T2I-adapter [31] attempt to\ncontrol the creation of images by utilizing data from differ-\nent modalities. Some optimization-based methods [30, 40]\nlearn new parameters or fine-tune the diffusion model in or-\nder to control the generation process. Other methods [3, 57]\nleverage the attention layer to introduce information from\nother images for gaining better control.\n2.3. 3D Representations\nNeural Radiance Fields (NeRF) [29], as a volumetric ren-\ndering method, has gained popularity for its ability to en-\nable 3D optimization [2, 4, 7, 14, 21] under 2D supervision,\nwhile NeRF optimization can be time-consuming. Numer-\nous efforts [32, 41] for spatial pruning have been dedicated\nto accelerating the training process of NeRF on the recon-\nstruction setting. however, they fail in the generation setting\nof Nerf. Recently, 3D Gaussian splatting [8, 18, 49, 58] has\nemerged as an alternative 3D representation to NeRF and\nhas shown remarkable advancements in terms of both qual-\nity and speed, offering a promising avenue.\n3. Preliminary\n3.1. DDIM Inversion\nDDIM [46] transforms random noise xT into clean data\nx0 over a series of time steps, by using the determinis-\ntic DDIM sampling in the reverse process, i.e., xt\u22121 =\n(\u03b1t\u22121/\u03b1t)(xt \u2212 \u03c3t\u03f5\u03d5) + \u03c3t\u22121\u03f5\u03d5. On the contrary, DDIM\ninversion progressively converts clean data to a noisy state\nxT , i.e., xt = (\u03b1t/\u03b1t\u22121)(xt\u22121 \u2212 \u03c3t\u22121\u03f5\u03d5) + \u03c3t\u03f5\u03d5, here \u03f5\u03d5\nis the predicted noise by the UNet. This method retains the\nquality of the data being rebuilt while greatly speeding up\nthe process by skipping many intermediate diffusion steps.\n3.2. 3D Gaussian Splatting\nGaussian Splatting [18] presents a novel method for synthe-\nsizing new views and reconstructing 3D scenes, achieving\nreal-time speed. Unlike NeRF, Gaussian Splatting uses a\nset of anisotropic 3D Gaussians defined by their locations,\ncovariances, colors, and opacities to represent the scene. To\ncompute the color of each pixel p in the image, it utilizes a\ntypical neural point-based rendering [19, 20], The rendering\nprocess is as follows:\nC(p) =\nX\ni\u2208N\nci\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j) ,\nwhere, \u03b1i = oie\u2212 1\n2 (p\u2212\u00b5i)T \u03a3\u22121\ni\n(p\u2212\u00b5i),\n(1)\nwhere ci, oi, \u00b5i, and \u03a3i represent the color, opacity, posi-\ntion, and covariance of the i-th Gaussian respectively, and\nN denotes the number of the related Gaussians.\n4. Method\nIn this section, we introduce our two-stage framework for\nfast and high-quality 3D generation from one image, as il-\nlustrated in Figure 4.\nIn the coarse stage, we adopt 3D\nGaussian Splatting as the representation following Dream-\nGaussian [49] to learn a coarse geometry and texture opti-\nmized by SDS loss. In the refining stage, we convert the\ncoarse model to mesh representation and propose a pro-\ngressive, controllable repainting scheme for texture refine-\nment.\nFirst, we obtain the view-consistency images for\nnovel views by progressively repainting the invisible re-\ngions relative to previously optimized views with geome-\ntry control and the guidance from reference image(see Sec-\ntion 4.1). Then, we employ image prompts for classifier-\nfree guidance and design an adaptive repainting strategy for\nfurther enhancing the generation quality in the overlap re-\ngions (see Section 4.2). Finally, with the generated view-\nconsistent high-quality images, we utilize simple MSE loss\nfor fast 3D content generation.(see Section 4.3).\n4.1. Multi-view Consistent Images Generation\nAchieving high-quality image-to-3D generation is a chal-\nlenging task because it necessitates pixel-level alignment\nin overlap regions while maintaining semantic-level and\ntexture-level consistency between reference view and novel\nviews. To achieve this, our key insight is to progressively\nrepaint the occlusions with the reference textures. Specifi-\ncally, we first delineate the overlaps and occlusions between\nthe reference-view image and a neighboring novel-view im-\nage. Inspired by HiFi-123 [60], we invert the coarse novel-\nview image to deterministic intermediate noised latents by\nDDIM Inversion [46] and then transfer reference textures\nthrough reference attention feature injection [3]. The in-\nversion preserves coarse 3D consistent color information\n3\n\u00d7 N\nK\nDDIM Inversion\n\ud835\udc59\ud835\udc59 steps\n(\ud835\udc59\ud835\udc59 \u2212 1) steps\n\ud835\udc59\ud835\udc59 steps\nControlled Denoising\nNovel-view \ncoarse image \nReference image\nRepainting\n1 \u2212 \ud835\udc40\ud835\udc40\ud835\udc61\ud835\udc61\n\ud835\udc65\ud835\udc65\ud835\udc61\ud835\udc61\u22121\n\ud835\udc5f\ud835\udc5f\ud835\udc5f\ud835\udc5f\ud835\udc5f\ud835\udc5f\n\ud835\udc65\ud835\udc65\ud835\udc61\ud835\udc61\u22121\n\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc5f\ud835\udc5f\n\ud835\udc65\ud835\udc65\ud835\udc61\ud835\udc61\n\ud835\udc65\ud835\udc65\ud835\udc61\ud835\udc61\u22121\n\ud835\udc65\ud835\udc65\ud835\udc61\ud835\udc61\n\ud835\udc5f\ud835\udc5f\ud835\udc5f\ud835\udc5f\ud835\udc5f\ud835\udc5f\nRepainting mask \ud835\udc40\ud835\udc40\ud835\udc61\ud835\udc61\n.\n+\n*\nImage \nRef\nDepth\nV\nRefined image\nIm\ncon\nVisibility \nmap\nControlNet\nCLIP\nTimestep-aware\nbinarization\nFigure 3. Controllable repainting scheme. Our scheme employs DDIM Inversion [46] to generate deterministic noisy latent from coarse\nimages, which are then refined via a diffusion model controlled by depth-guided geometry, reference image semantics, and attention-driven\nreference texture. We binarize the visibility map into an overlap mask by the timestep-aware binarization operation. Overlap regions are\nselectively repainted during each denoising step, leading to the high-quality refined novel-view image.\nin occlusions while the attention injection replenishes con-\nsistent high-frequency details. Subsequently, we iteratively\ndenoise and blend the noised latent using inverted latents\nfor neighbor harmony and pixel-level alignment in overlaps.\nFinally, we bidirectionally rotate the camera and progres-\nsively apply this repainting process from the reference view\nto all views. By doing this, we can seamlessly repaint occlu-\nsions with both short-term consistency (overlaps alignment\nand neighbor harmony) and long-term consistency (back-\nview consistency of semantics and textures).\nObtaining Occlusion Mask. To get the occlusion mask\nMn in the novel view with the rendered image In and depth\nmap Dn, given a repainted reference view with Ir and Dr,\nwe first back-project the 2D pixels in the view Vr into 3D\npoints Pr by scaling camera rays of Vr with depth values\nDr. Then, we render a depth map D\u2032\nn from Pr and the\ntarget perspective Vn. Regions with dissimilar depth values\nbetween the two novel-view depth maps (Dn and D\u2032\nn) are\nocclusion regions in occlusion mask Mn.\nPerforming DDIM Inversion. As shown in the red part\nof Figure 3, to utilize the 3D-consistent coarse color and\nmaintain the textures in overlap regions, we perform DDIM\ninversion on the novel-view image I to get the intermediate\ndeterministic latents xinv. With the inverted latents, we can\ndenoise reversely to reconstruct the input image faithfully.\nRepainting the Occlusions with Depth Prior.\nAs\nshown in Figure 3, with the inverted latents, we can replace\nthe overlap parts in the denoised latents during each denois-\ning step to enforce the overlapped regions unchanged while\nharmonizing the occlusion regions:\nxt\u22121 = xinv\nt\u22121 \u2299 (1 \u2212 M) + xrev\nt\u22121 \u2299 M,\n(2)\nwhere xrev\nt\u22121 \u223c N (\u00b5\u03d5 (xt, t) , \u03a3\u03d5 (xt, t)) is the denoised la-\ntent of timestep t. Besides, We employ ControlNet [61] to\nimpose additional geometric constraints from coarse depth\nmaps to ensure the geometric consistency of images.\nInjecting Reference Textures. To mitigate the cumu-\nlative texture bias at the back view, we incorporate a mu-\ntual self-attention mechanism [3] that injects reference at-\ntention features into the novel-view repainting process dur-\ning each denoising step. By replacing the novel-view con-\ntent features (Key features Kt and Value features Vt) with\nreference-view attention features (Kr and Vr), the novel-\nview image features can directly query the high-quality ref-\nerence features by:\nAttention(Qt, Kr, Vr) = Softmax\n\u0012QtKT\nr\n\u221a\nd\n\u0013\nVr,\n(3)\nwhere Qt is the novel-view query features projected from\nthe spatial features. This enhances texture details transfer\nand improves the consistency of the novel-view image.\nRepainting 360\u25e6Progressively. As shown in Figure 4,\nwe progressively sample new camera views that alternate\nbetween clockwise and counterclockwise increments. To\nensure consistency in the junction of the two directions, our\napproach selects the nearest camera views from each of the\ntwo directions (as shown in the red cameras in Figure 4)\nto compute occlusion masks and repaint the texture in the\nnovel view.\nM = Mr \u2229 Mr\u2032,\n(4)\n4\nDepth\nNovel view\n+ Noise\nZero-123\nRef. img\nCoarse stage\n\ud835\udc73\ud835\udc73\ud835\udc7a\ud835\udc7a\ud835\udc7a\ud835\udc7a\ud835\udc7a\ud835\udc7a\nMesh \nExtraction\nRefine stage\nRef\n\ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc74\ud835\udc7a\ud835\udc7a\ud835\udc74\ud835\udc74\n\ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc74\ud835\udc7a\ud835\udc7a\ud835\udc74\ud835\udc74\nRef. view\n\ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc74\ud835\udc7a\ud835\udc7a\ud835\udc74\ud835\udc74\nRef. view\nRef. img\n\ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc74\ud835\udc7a\ud835\udc7a\ud835\udc74\ud835\udc74\nRepainting\nVisibility map\nNovel img\n\ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc74\ud835\udc7a\ud835\udc7a\ud835\udc74\ud835\udc74\nNovel view\nProgressive sampling\n1\n3\n5\n6\n4\n2\n1\n5\n3\nRefined\nFigure 4. Image-to-3D generation pipeline. In the coarse stage, we adopt Gaussian Splatting representation optimized by SDS loss at the\nnovel view. In the fine stage, we export Mesh representation and bidirectionally and progressively sample novel views for controllable\nprogressive repainting. The novel-view refined images will compute MSE loss with the input novel-view image for efficient generation.\nCameras in red are bidirectional neighbor cameras for obtaining the visibility map.\nstreng\n1\n0\ncos \ud835\udf03\ud835\udf03\n1 \u2212 cos \ud835\udf03\ud835\udf03\n\ud835\udc8f\ud835\udc8f\nResolution \n= R cos \ud835\udf03\ud835\udf031\nOcclusion m\nRefinement\nstrength\nCapacity\n\ud835\udc8f\ud835\udc8f\nPrevious \noblique view\nfront view\nCurrent view\n\ud835\udf03\ud835\udf032\n1\n0\n\ud835\udc79\ud835\udc79 \ud835\udc1c\ud835\udc1c\ud835\udc1c\ud835\udc1c\ud835\udc1c\ud835\udc1c \ud835\udf3d\ud835\udf3d\ud835\udfcf\ud835\udfcf(\ud835\udc8d\ud835\udc8d\ud835\udc8d\ud835\udc8d\ud835\udc8d\ud835\udc8d) \ud835\udc79\ud835\udc79 \ud835\udc1c\ud835\udc1c\ud835\udc1c\ud835\udc1c\ud835\udc1c\ud835\udc1c \ud835\udf3d\ud835\udf3d\ud835\udfd0\ud835\udfd0(\ud835\udc89\ud835\udc89\ud835\udc89\ud835\udc89\ud835\udc89\ud835\udc89\ud835\udc89\ud835\udc89)\n1 \u2212 cos \ud835\udf03\ud835\udf031\nInformation \nreplenishment\n1 \u2212 cos \ud835\udf03\ud835\udf032\nRefinement\nResolution \n= R cos \ud835\udf03\ud835\udf032\nResolution \nViewing\nLarger information capacity\nResolution \nCapacity \nLess Details\nMore details\nLess Details\nSame area on \nthe object \n\ud835\udf3d\ud835\udf3d\ud835\udfcf\ud835\udfcf > \ud835\udf3d\ud835\udf3d\ud835\udfd0\ud835\udfd0\n\ud835\udf03\ud835\udf031\nFigure 5. Relation between camera view and refinement strength.\nThe areas in the red box are the same regions from different views.\nwhere Mr and Mr\u2032 are the occlusion mask obtained from\ntwo reference views, respectively and M is the final mask.\n4.2. Image Quality Enhancement\nDespite the progressive repainting process sustaining both\nshort-term and long-term consistency, the accumulated tex-\nture degradation over the incremented angles can result in\ndeteriorated multi-view image quality. We discover that the\ndegradation is from both overlap and occlusion regions. For\noverlap regions, as shown in Figure 5, when the previous\nview is an oblique view, it leads to a low-resolution up-\ndate on the texture maps, resulting in high distortion when\nrendering from the front view. Therefore, we propose a\nvisibility-aware adaptive repainting process to refine the\noverlap regions with different strengths based on the pre-\nvious best viewing angle on these regions. For occlusion\nregions, they achieve limited quality due to the absence of\ntext prompts to perform classifier-free guidance [15], which\nis essential to diffusion models for high-quality image gen-\neration. To improve overall quality, we adopt a CLIP [36]\nencoder (as shown in Figure 3) to encode and project the\nreference image to image prompts for guidance.\nVisibility-aware Adaptive Repainting. Optimal refine-\nment strength for the overlap regions is crucial, as exces-\nsive strength produces unfaithful results while insufficient\nstrength limits quality improvement. To select the proper re-\nfinement strength, we associate the denoising strength with\nthe visibility map V (similar to the concept of trimap [38]).\nAs explained in detail in Appendix 8, the visibility map\nis obtained based on the normal maps (i.e., the cos\u03b8 be-\ntween the normal vectors of the viewed fragments and the\ncamera view directions) in the current view and previous\nviews. For occlusion regions, we set the values in V to\n0. For overlap regions where the current camera view pro-\nvides a worse rendering angle compared to previous camera\nviews, we set the values in V to 1, indicating these regions\ndo not require refinement. For the remaining areas in V ,\nwe set the values to cos\u03b8\u2217, which is the largest cos\u03b8 among\nall previous views and indicates the best visibility during\nthe previous optimization process. In contrast to prior ap-\nproaches [38, 53] employing fixed denoising strength for all\nrefined fragments, our work introduces a timestep-aware bi-\nnarization strategy to adaptively repaint the overlap regions\nbased on the visibility map for the faithfulness-realism\ntrade-off. Specifically, as shown in Figure 5, we view re-\npainting as a process similar to super-resolution that replen-\nishes detailed information. According to the Orthographic\nProjection Theorem, which asserts that the projected reso-\nlution of a fragment is directly proportional to cos\u03b8, we can\nassume that the repainting strength is equal to (1 \u2212 cos\u03b8\u2217).\n5\nTherefore, we can binarize the soft visibility map to the hard\nrepainting mask based on the current timestep during each\ndenoising step, denoted by the green box \u201cTimestep-aware\nbinarization\u201c in Figure 3 and visualized in Figure 10:\nM i,j\nt\n=\n(\n1,\nif V i,j > 1 \u2212 t/T\n0,\nelse,\n(5)\nwhere Mt is the adaptive repainting mask, i, and j are the\n2D position of the fragment in visibility map V , and T is\nthe total number of timesteps of the diffusion model.\nProjecting Reference Image to Prompts. For image\nconditioning, previous image-to-3D methods usually uti-\nlize textual inversion [13], which is extremely slow (several\nhours) for optimization and provides limited texture infor-\nmation due to limited number of learned tokens. Other tun-\ning techniques, such as Dreambooth [40], require prolonged\noptimization and tend to overfit the reference view. Besides,\nvision-language ambiguity is a common issue when extract-\ning text from the caption model. To tackle these issues, as\nshown in Figure 3, we adopt IP-Adapter [57] to encode and\nproject the reference image into the image prompt of 16 to-\nkens that are fed into an effective and lightweight cross-\nattention adapter in the pre-trained text-to-image diffusion\nmodels. This provides visual conditions for diffusion mod-\nels to perform classifier-free guidance for enhanced quality.\n4.3. Fast and High-quality 3D Generation\nIn the coarse stage, we adopt 3D Gaussian Splatting [18]\nwith SDS optimization for fast generation. In the fine stage,\nwith the controllable progressive repainting process above,\nwe can generate view-consistent high-quality images for ef-\nficient high-quality 3D generation. The refined images are\nthen used to directly optimize the texture through a pixel-\nwise MSE loss:\nLMSE = ||Ifine \u2212 I||2\n2,\n(6)\nwhere Ifine represents the refined images obtained from\ncontrollable repainting and I represent the rendered images\nfrom 3D. The MSE loss is fast to compute and deterministic\nto optimize, resulting in fast refinement.\n5. Experiment\n5.1. Implementation Details\nIn our experiment, we follow DreamGaussian [49] to adopt\n3D Gaussian Splatting [18] representation at the coarse\nstage. We also explore NeRF as an alternative to Gaussian\nSplatting for the coarse stage, with results detailed in the\nAppendix. For all results of our method, we use the same\nhyperparameters.\nWe progressively increment the view-\npoints by 40 degrees, and opt to invert rendered images\nover 30 steps. Stable diffusion 1.5 is adopted for all ex-\nperimented methods.\n5.2. Baselines\nWe adopt RealFusion [28], Make-It-3D [50], and Zero123-\nXL [24], Magic123 [35] as our NeRF-based baselines and\nDreamGaussian [49] as our Gaussian-Splatting-based base-\nline. RealFusion presents a single-stage algorithm for NeRF\ngeneration leveraging an MSE loss for the reference view\nalong with a 2D SDS loss for novel views. Make-It-3D is a\ntwo-stage approach that shares similar objectives with Re-\nalFusion but employs a point cloud representation for re-\nfinement at the second stage. Zero123 enables the synthe-\nsis of novel views conditioned on images without the need\nfor training data, achieving remarkable quality in generating\n3D content when combined with SDS. Integrating Zero123\nand RealFusion, Magic123 incorporates a 2D SDS loss with\nZero123 for consistent geometry and adopts DMTet [45]\nrepresentation at the second stage. DreamGaussian inte-\ngrates 3D Gaussian Splatting into 3D generation and greatly\nimproves the speed. For Zero123-XL, we adopt the imple-\nmentation [48], For other works, we use their officially re-\nleased code for evaluation.\n5.3. Evaluation Protocol\nDatasets. Based on previous research, we utilized the Re-\nalfusion15 dataset [28] and test-alpha dataset collected by\nMake-It-3D [50], which comprises many common things.\nEvaluation metrics.\nAn effective 3D generation ap-\nproach should closely resemble the reference view, and\nmaintain consistency of semantics and textures with the\nreference when observed from new views. Therefore, to\nevaluate the overall quality of the generated 3D object, we\nchoose the following metrics from two aspects: 1) PSNR\nand LPIPS [62], which measure pixel-level and perceptual\ngeneration quality respectively at the reference view; 2)\nCLIP similarity [36] and contextual distance [27], which\nassess the similarity of semantics and textures respectively\nbetween the novel perspective and the reference view.\n5.4. Comparisons\nQuantitative Comparisons.\nAs shown in Table 1, we\nevaluate the quality of generated 3D objects across various\nmethods. Our method achieves superior 3D consistency in\ngenerating 3D objects, as evidenced by best performance\nfrom CLIP-similarity and contextual distance metrics. Al-\nthough our method achieves better reference-view recon-\nstruction results than DreamGaussian, there is a gap com-\npared with Nerf-based approaches, which we attribute to\nthe immaturity of current Gaussian-Splatting-based meth-\nods. Compared with Nerf-based methods for the optimiza-\ntion time, our approach reaches a significant acceleration of\nover 10 times and simultaneously achieves high quality.\nQualitative Comparisons. Figure 6 displays the quali-\ntative comparison results between our method and the base-\nline, while Figure 1 shows multiple novel-view images gen-\n6\nFigure 6. Qualitative comparisons on image-to-3D generation. Zoom in for texture details.\n7\nDataset\nMetrics \\ Methods\nNeRF-based\nGaussian-Splatting-based\nRealFusion\nMake-it-3D\nZero-123-XL*\nMagic123\nDreamGaussian\nRepaint123\nRealFusion15\nCLIP-Similarity\u2191\n0.71\n0.81\n0.83\n0.82\n0.77\n0.85\nContext-Dis\u2193\n2.20\n1.82\n1.59\n1.64\n1.61\n1.55\nPSNR\u2191\n19.24\n16.56\n19.56\n19.68\n18.94\n19.00\nLPIPS\u2193\n0.194\n0.177\n0.108\n0.107\n0.111\n0.101\nTest-alpha\nCLIP-Similarity\u2191\n0.68\n0.76\n0.84\n0.84\n0.79\n0.88\nContext-Dis\u2193\n2.20\n1.73\n1.52\n1.57\n1.62\n1.50\nPSNR\u2191\n22.91\n17.21\n24.39\n24.69\n22.33\n22.38\nLPIPS\u2193\n0.105\n0.237\n0.050\n0.046\n0.057\n0.048\nOptimization time\n20min\n1h\n30min\n1h (+2h)\n2min\n2 min\nTable 1. We show quantitative results in terms of CLIP-Similarity\u2191 / Contextual-Distance\u2193 / PSNR\u2191 / LPIPS\u2193. The results are shown on\nthe RealFusion15 and test-alpha datasets, while bold reflects the best for all methods and the underline represents the best for Gaussian-\nSplatting-based methods. * indicates that Zero123-XL adds a mesh fine-tuning stage to further improve quality. The time required by\ntextual inversion is indicated in parentheses.\nMethod\\ Metric\nCLIP\u2191\nContextual\u2193\nPSNR\u2191\nLPIPS\u2193\nCoarse\n0.71\n1.78\n21.17\n0.133\nrepaint\n0.71\n1.62\n22.41\n0.049\n+mutual attention\n0.78\n1.56\n22.42\n0.048\n+image prompt\n0.84\n1.52\n22.40\n0.048\n+adaptive (Full)\n0.88\n1.50\n22.38\n0.048\nTable 2. Quantitative ablation study on Test-alpha dataset.\nerated by our methods. Repaint123 achieves the best vi-\nsual results in terms of texture consistency and generation\nquality as opposed to other methods. From the visual com-\nparison in Figure 6, we discover that DreamGaussian and\nZero123-XL usually result in over-smooth textures, lower-\ning the quality of the 3D object generation. Magic123 often\nproduces inconsistent oversaturated colors in invisible ar-\neas. Realfusion and Make-It-3D fail to generate full geome-\ntry and consistent textures. This demonstrates Repaint123\u2019s\nsuperiority over the current state of the art and its capacity\nto generate high-quality 3D objects in about 2 minutes.\n5.5. Ablation and Analysis\nIn this section, we further conduct both qualitative and\nquantitative ablation studies (as shown in Figure 7 and Ta-\nble 2) to demonstrate the effectiveness of our designs. Fur-\nthermore, we analyze the angular interval during repainting.\nEffectiveness of Progressive Repainting. As shown in\nFigure 7, there are noticeable multi-face issues (anya) and\ninconsistencies in the overlap regions (ice cream). These in-\nconsistencies are from the absence of constraints on overlap\nregions and can introduce conflicts and quality degradation\nof the final 3D generation.\nEffectiveness of Mutual Attention.\nIn Table 2, we\ncan see that mutual attention can significantly improve the\nmulti-view consistency and fine-grained texture consistency\nOurs\nw/o repaint\nw/o mutual\nw/o ip\nw/o soft\n280 \u00b0\n120 \u00b0\n40\u00b0\n80 \u00b0\nInput\nView\nFigure 7. Qualitative ablation study. Red boxes show artifacts.\ncompared to vanilla repainting. As shown in Figure 7, the\nsynthesized images without mutual attention strategy main-\ntain the semantics but fails to transfer detailed textures from\nthe reference image.\nEffectiveness of Image Prompt. As shown in Table 2,\nimage prompt can further improve the multi-view consis-\ntency. As shown in Figure 7 and Table 2, without an image\nprompt for classifier-free guidance, the multi-view images\nfail to generate detailed realistic and consistent textures.\nEffectiveness of Adaptive Repainting. As shown in\nFigure 7, without the adaptive repainting mask, the oblique\nregions in the previous view will lead to artifacts when fac-\ning these regions due to previous low-resolution updates.\nTable 2 also demonstrates the effectiveness as both CLIP\nsimilarity and Contextual distance improve significantly.\nAnalysis of Angle Interval. We study the effect of using\n8\nAngle\\ Metric\nCLIP\u2191\nContextual\u2193\nPSNR\u2191\nLPIPS\u2193\n20\u25e6\n0.873\n1.504\n22.35\n0.051\n40\u25e6\n0.881\n1.506\n22.38\n0.048\n60\u25e6\n0.888\n1.497\n22.27\n0.050\n80\u25e6\n0.885\n1.487\n22.26\n0.051\nTable 3. Effects of the chosen angle of neighboring viewpoints in\nRepaint123 on Test-alpha dataset.\n(1)   Angle \ud835\udfd2\ud835\udfce\u00b0\n(2)   Angle \ud835\udfd4\ud835\udfce\u00b0\nFigure 8. Visual comparison when choosing 40\u25e6and 60\u25e6as the\nangle interval. The red box shows the resulting multi-face issues.\ndifferent angle intervals on the performance of Repaint123\nin Table 3. The table demonstrates that the metrics achieve\ntheir peak performance when the angle is set to 60 degrees.\nNonetheless, Figure 8 illustrates that there is a reduced over-\nlapping area when choosing 60 degrees as the angle interval,\nwhich consequently increases the likelihood of encounter-\ning a multi-head problem during the optimization process.\nThus, we ultimately choose 40 degrees as the ideal angle\ninterval for the optimization process.\n6. Discussion\nWhile Gaussian Splatting is fast, due to the lack of techno-\nlogical maturity for generation tasks and mesh extraction, it\nmay exhibit geometry artifacts, such as holes, and achieve\ninferior results compared to NeRF-based methods. These\nissues are expected to be resolved with its development.\n7. Conclusion\nThis work presents Repaint123 for generating high-quality\n3D content from a single image in about 2 minutes. By\nleveraging progressive controllable repaint, our approach\novercomes the limitations of existing studies and achieves\nstate-of-the-art results in terms of both texture quality and\nmulti-view consistency, paving the way for future progress\nin one image 3D content generation. Furthermore, we val-\nidate the effectiveness of our proposed method through a\ncomprehensive set of experiments.\nReferences\n[1] Mohammadreza\nArmandpour,\nHuangjie\nZheng,\nAli\nSadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-\nimagine the negative prompt algorithm:\nTransform 2d\ndiffusion into 3d, alleviate janus problem and beyond. arXiv\npreprint arXiv:2304.04968, 2023. 2\n[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. CVPR, 2022. 3\n[3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 22560\u201322570,\n2023. 3, 4\n[4] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In CVPR, 2022. 3\n[5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2\n[6] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-\nto-3d generation with explicit view synthesis. arXiv preprint\narXiv:2308.11473, 2023. 2\n[7] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-\nterization pipeline for efficient neural field rendering on mo-\nbile architectures. arXiv preprint arXiv:2208.00277, 2022.\n3\n[8] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using\ngaussian splatting. arXiv preprint arXiv:2309.16585, 2023.\n3\n[9] Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei\nZhang, Jian Zhang, and Li Yuan. Progressive3d: Progres-\nsively local editing for text-to-3d content creation with com-\nplex semantic prompts. arXiv preprint arXiv:2310.11784,\n2023. 2\n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[11] Zhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao,\nQiangqiang Wu, Weilin Wan, Taku Komura, and Wenping\nWang. Tore: Token reduction for efficient human mesh re-\ncovery with transformer. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 15143\u2013\n15155, 2023. 1\n[12] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items. In 2022 In-\nternational Conference on Robotics and Automation (ICRA),\npages 2553\u20132560. IEEE, 2022. 12\n9\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 6\n[14] Peter Hedman,\nPratul P. Srinivasan,\nBen Mildenhall,\nJonathan T. Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. ICCV, 2021. 3\n[15] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[16] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023. 2\n[17] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 1\n[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 3, 6\n[19] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis.\nPoint-based neural rendering with per-\nview optimization. In Computer Graphics Forum, pages 29\u2013\n43. Wiley Online Library, 2021. 3\n[20] Georgios Kopanas, Thomas Leimk\u00a8uhler, Gilles Rainer,\nCl\u00b4ement Jambon, and George Drettakis. Neural point cata-\ncaustics for novel-view synthesis of reflections. ACM Trans-\nactions on Graphics (TOG), 41(6):1\u201315, 2022. 3\n[21] Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H Tay-\nlor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.\nNeuralangelo: High-fidelity neural surface reconstruction. In\nCVPR, 2023. 3\n[22] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, 2023. 2\n[23] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023. 3\n[24] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298\u20139309, 2023. 1, 2, 6\n[25] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 1, 3\n[26] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 3\n[27] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The\ncontextual loss for image transformation with non-aligned\ndata. In Proceedings of the European conference on com-\nputer vision (ECCV), pages 768\u2013783, 2018. 6\n[28] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In CVPR, 2023. 2, 6\n[29] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 3\n[30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 3\n[31] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 3\n[32] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM TOG, 2022. 3\n[33] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 1\n[34] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2\n[35] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843,\n2023. 1, 2, 6\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 5, 6\n[37] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nbooth3d:\nSubject-driven text-to-3d generation.\narXiv\npreprint arXiv:2303.13508, 2023. 2\n[38] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. arXiv preprint arXiv:2302.01721, 2023. 5\n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2\n[40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\n10\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 3, 6\n[41] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In CVPR, 2022. 3\n[42] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young\nChun.\nDitto-nerf: Diffusion-based iterative text to omni-\ndirectional 3d model.\narXiv preprint arXiv:2304.02827,\n2023. 2\n[43] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023.\n[44] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-\n3d: Towards single-view anything reconstruction in the wild.\narXiv preprint arXiv:2304.10261, 2023. 2\n[45] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021. 6\n[46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. arXiv:2010.02502, 2020. 3,\n4\n[47] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion:(0-) image-conditioned 3d gener-\native models from 2d data. arXiv preprint arXiv:2306.07881,\n2023. 3\n[48] Jiaxiang Tang.\nStable-dreamfusion:\nText-to-3d with\nstable-diffusion, 2022. https://github.com/ashawkey/stable-\ndreamfusion. 6\n[49] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 2, 3, 6\n[50] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d\ncreation from a single image with diffusion prior. In ICCV,\n2023. 2, 6\n[51] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 2\n[52] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 2\n[53] Tianfu Wang, Menelaos Kanakis, Konrad Schindler, Luc\nVan Gool, and Anton Obukhov.\nBreathing new life\ninto 3d assets with generative repainting.\narXiv preprint\narXiv:2309.08523, 2023. 5\n[54] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2\n[55] Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen\nZhao, Haocheng Feng, Jingtuo Liu, and Errui Ding.\nHd-\nfusion: Detailed text-to-3d generation leveraging multiple\nnoise estimation. arXiv preprint arXiv:2307.16183, 2023.\n[56] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360 views. arXiv e-prints, pages\narXiv\u20132211, 2022. 2\n[57] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 3, 6\n[58] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng\nZhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-\ndreamer: Fast generation from text to 3d gaussian splatting\nwith point cloud priors. arXiv preprint arXiv:2310.08529,\n2023. 3\n[59] Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin\nWang, and Fan Wang. Points-to-3d: Bridging the gap be-\ntween sparse points and shape-controllable text-to-3d gener-\nation. arXiv preprint arXiv:2307.13908, 2023. 2\n[60] Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu\nLi, Long Quan, Ying Shan, and Yonghong Tian. Hifi-123:\nTowards high-fidelity one image to 3d content generation.\narXiv preprint arXiv:2310.06744, 2023. 2, 3\n[61] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 3, 4\n[62] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 6\n[63] Joseph Zhu and Peiye Zhuang.\nHifa: High-fidelity text-\nto-3d with advanced diffusion guidance.\narXiv preprint\narXiv:2305.18766, 2023. 2\n11\nRepaint123: Fast and High-quality One Image to 3D Generation with\nProgressive Controllable 2D Repainting\nSupplementary Material\n8. Visibility Map and Repainting Strength\nThis section delineates our proposed visibility map and its\nrelation to the repainting strength in detail and visualization.\nObtaining Visibility Map. Figure 9 shows the process\nof transforming the novel-view normal map to the visibility\nmap based on the previous neighbor-view normal map. We\nfirst conduct a back-projection of the preceding normal map\ninto 3D points, subsequently rendering a normal map from\nthe novel view based on these 3D points, i.e., the normal\nmap in the projected view as shown in Figure 9. Comparing\nnovel-view normal maps with the projected novel-view nor-\nmal maps yields a high-resolution visibility map, assigning\nprojected normal map values to areas with improved visi-\nbility in the novel view (non-white parts of visibility map in\nFigure 9) for further refinement and a value of 1 to other re-\ngions (white parts of visibility map in Figure 9) for preser-\nvation. The final visibility map is obtained by downsam-\npling from 512x512 to 64x64 resolution, facilitating subse-\nquent repainting mask generation in the latent space.\nTimestep-aware Binarization. As shown in Figure 10,\nwe visualize our proposed timestep-aware binarization pro-\ncess to transform the visibility map into the timestep-\ndependent repainting mask. Based on the proportional re-\nlation between visibility and repainting strength elucidated\nin Figure 3 in the main paper, the repainting region (black\nareas in Figure 10) can be obtained by selecting areas in the\nvisibility map with a visibility value not exceeding 1\u2212t/T,\nwhere T represents the maximum timestep during train-\ning (typically 1000), and t denotes the current repainting\ntimestep. As illustrated in Figure 10, decreasing denoising\ntimesteps enlarges repainting regions, indicating a progres-\nsive refinement according to prior visibility.\n9. Evaluation on Multi-view Dataset\nWe adopt the Google Scanned Object (GSO) dataset [12]\nand use 10 objects for multi-view evaluation of the gener-\nated 3D objects with 3D ground truth. As shown in Table 4,\nthe results indicate that our method is capable of generat-\ning high-quality 3D contents with multi-view consistency\ncompared with the strong baselines.\n10. Evaluation of NeRF-based Repaint123\nAs our repainting approach is plug-and-play for the refine-\nment stage, we can change the representation in the coarse\nstage from Gaussian Splatting to NeRF. As presented in Ta-\nOcclusion mask\nCurrent-view depth map\nBackground mask\nCurrent-view depth map\nPrevious \nview\nProjected \nview\nCurrent \nview\nVisibility Map\nNormal \nmap\nVisibility \nMap*\nImage\n512\n64\nMean pooling\nCompare\nFigure 9. Visibility map creation process. The value in the nor-\nmal map represents the visibility. White parts of the visibility map\nare less visible regions in the current view compared to the previ-\nous views, while non-white parts are more visible regions with the\nvalue of previous visibility.\nBinarize\n1000 (T)\nBackground mask\nBlack means generate, white means keep\nt\nVisibility Map\nRepainting mask\n800\n600\n400\n200\n1\n> 1 - t/T\nFigure 10. Timestep-aware binarization. Black areas represent\nrepainting regions, and white areas denote preservation regions.\nMethod\\ Metric\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nCLIP\u2191\nContextual\u2193\nSyncdreamer\n13.201\n0.784\n0.322\n0.612\n1.686\nMagic123\n14.985\n0.803\n0.244\n0.767\n1.376\nZero-123-XL\n15.118\n0.813\n0.229\n0.761\n1.334\nDreamGaussian\n15.391\n0.814\n0.237\n0.736\n1.407\nRepaint123\n15.393\n0.814\n0.214\n0.812\n1.319\nTable 4. Multi-view quantitative comparison with image-to-3D\ngeneration baselines on GSO dataset.\nble 5 and Figure 11, the generated 3D objects can be signif-\nicantly improved by using our repainting method.\n11. Ablation Study on Prompt\nIn this section, we conduct ablations on various prompts,\nincluding image prompt, text prompt, textual inversion, and\nempty prompt. As shown in Table 6, prompts significantly\nenhance both multi-view consistency and quality in com-\nparison to results obtained without prompts. The efficacy\nstems from the classifier-free guidance.\nAmong various\nprompts, image prompts demonstrate superior performance,\nshowcasing the superior accuracy and efficiency of visual\n12\nDataset\nMetrics \\ Methods\nNeRF-based\nGaussian-Splatting-based\nMagic123\nOurs*\nDreamGaussian\nOurs\nRealFusion15\nCLIP-Similarity\u2191\n0.82\n0.85\n0.77\n0.85\nContext-Dis\u2193\n1.64\n1.57\n1.61\n1.55\nPSNR\u2191\n19.68\n20.27\n18.94\n19.00\nLPIPS\u2193\n0.107\n0.096\n0.111\n0.101\nTest-alpha\nCLIP-Similarity\u2191\n0.84\n0.88\n0.79\n0.88\nContext-Dis\u2193\n1.57\n1.46\n1.62\n1.50\nPSNR\u2191\n24.69\n24.91\n22.33\n22.38\nLPIPS\u2193\n0.046\n0.036\n0.057\n0.048\nTable 5. We show quantitative results in terms of CLIP-Similarity\u2191 / Contextual-Distance\u2193 / PSNR\u2191 / LPIPS\u2193. The results are shown on\nthe RealFusion15 and test-alpha datasets, while bold reflects the best for Nerf-based and Gaussian-Splatting-based methods respectively.\n* indicates that we adopt NeRF representation for the coarse stage.\nReference\nReference\nMagic123\nOurs\nMagic123\nOurs\nFigure 11. Visual comparison between our NeRF-based method and Magic123.\nPrompt\\ Metric\nPSNR\u2191\nLPIPS\u2193\nCLIP\u2191\nContextual\u2193\nNone\n19.02\n0.102\n0.79\n1.60\nText\n19.00\n0.102\n0.83\n1.58\nTextual Inversion\n19.01\n0.101\n0.84\n1.57\nImage\n19.00\n0.101\n0.85\n1.55\nTable 6. Ablation on RealFusion15 dataset under various prompt\nconditions. Image prompt achieves superior performance.\nprompts over text prompts, including time-consuming opti-\nmized textual prompts.\n12. More Results\nThe videos in the supplementary material show more\nimage-to-3D generation results of our method, demonstrat-\ning our method can produce high-quality 3D contents with\nconsistent appearances.\n13\n"
  },
  {
    "title": "RadEdit: stress-testing biomedical vision models via diffusion image editing",
    "link": "https://arxiv.org/pdf/2312.12865.pdf",
    "upvote": "3",
    "text": "RadEdit: stress-testing biomedical vision models via diffusion image editing\nFernando P\u00b4erez-Garc\u00b4\u0131a * 1 Sam Bond-Taylor * 1 Pedro P. Sanchez + 2 Boris van Breugel + 3 Daniel C. Castro 1\nHarshita Sharma 1 Valentina Salvatelli 1 Maria T. A. Wetscherek 1 Hannah Richardson 1\nMatthew P. Lungren 4 5 1 Aditya Nori 1 Javier Alvarez-Valle 1 Ozan Oktay \u2020 1 Maximilian Ilse \u2020 1\nAbstract\nBiomedical imaging datasets are often small and\nbiased, meaning that real-world performance of\npredictive models can be substantially lower than\nexpected from internal testing. This work pro-\nposes using generative image editing to simu-\nlate dataset shifts and diagnose failure modes of\nbiomedical vision models; this can be used in\nadvance of deployment to assess readiness, po-\ntentially reducing cost and patient harm.\nEx-\nisting editing methods can produce undesirable\nchanges, with spurious correlations learned due\nto the co-occurrence of disease and treatment in-\nterventions, limiting practical applicability. To\naddress this, we train a text-to-image diffusion\nmodel on multiple chest X-ray datasets and in-\ntroduce a new editing method RadEdit that uses\nmultiple masks, if present, to constrain changes\nand ensure consistency in the edited images. We\nconsider three types of dataset shifts: acquisition\nshift, manifestation shift, and population shift,\nand demonstrate that our approach can diagnose\nfailures and quantify model robustness without\nadditional data collection, complementing more\nqualitative tools for explainable AI.\n1. Introduction\nDeveloping accurate and robust models for biomedical im-\nage analysis requires large and diverse datasets that are of-\nten difficult to obtain due to ethical, legal, geographical,\nand financial constraints [44]. This leads to biased train-\ning datasets that affect the performance of trained models\nand generalisation to real-world scenarios [43, 64]. Specif-\nically, such data mismatch may arise from genuine differ-\nences in upstream data acquisition as well as from the se-\n*Equal contribution, +Work done at Microsoft Health Futures,\n\u2020Equal contribution 1Microsoft Health Futures 2University of\nEdinburgh 3University of Cambridge 4University of California\n5Stanford University. Correspondence to: Maximilian Ilse <max-\nilse(at)microsoft.com>.\nPreprint.\nBiased training datasets\nEdited stress-testing set\nCOVID-19 positive\nCOVID-19 negative\nPneumothorax\nNo pneumothorax\nHealthy patient\nLung segmentation\nAcquisition\nshift\nManifestation\nshift\nPopulation\nshift\n-COVID-19\n-Pneumothorax\n+Lung abnormality\nFalse\npositive\nFalse\npositive\nFigure 1: Stress-testing models by simulating dataset shifts\nvia image editing. Here, a COVID-19 classifier instead de-\ntects acquisition differences (top); a pneumothorax classi-\nfier instead detects chest tubes (middle); and a lung seg-\nmentation model mislabels abnormal lungs (bottom).\nlection criteria for dataset creation, which materialise as\nvarious forms of dataset shifts (population, acquisition, an-\nnotation, prevalence, manifestation) [9].\nBiomedical vision models, when put into real-world use,\ncan be unhelpful or potentially even harmful to patients if\nthey are affected by dataset shifts\u2014leading to missed diag-\nnoses [24, 60, 80, 82]. For example, the COVID-19 pan-\ndemic led to hundreds of detection tools being developed,\nwith some put into use in hospitals; yet Roberts et al. [60]\nfound that \u201cnone of the models identified are of potential\nclinical use due to methodological flaws and/or underlying\nbiases.\u201d It is therefore crucial to be able to properly assess\nsuch models for biases, prior to real-world use.\nRecent deep generative models have made remarkable im-\nprovements in terms of sample quality, diversity, and steer-\nability [30, 38, 52, 61]. These models have been shown\nto generalise to out-of-distribution domains [6, 20, 35, 45],\nopening up avenues for new applications. One such ap-\nplication is generating synthetic data for stress-testing dis-\n1\narXiv:2312.12865v2  [cs.CV]  21 Dec 2023\nRadEdit: stress-testing biomedical vision models via diffusion image editing\ncriminative models [45, 55, 78]. This involves creating data\nthat is realistic, yet can represent settings, domains, or pop-\nulations that do not appear (enough) in the real training and\ntest data.\nIn this work, we investigate how deep generative models\ncan be used for stress-testing biomedical imaging models.\nWe consider three dataset shift scenarios:\n1. classifying COVID-19 cases when the positive and\nnegative cases were acquired at different hospitals\n(Section 5.2);\n2. detecting the presence of pneumothorax when chest\ndrains are absent (Section 5.3); and\n3. segmenting lungs in the presence of pathologies rarely\nor not seen in the training dataset (Section 5.4).\nFor each of these scenarios, we simulate dataset shifts. This\nproduces stress-test sets which can occur in the real world\nbut do not appear or are underrepresented in the original\ntraining and test sets. Following prior work, these test sets\nare synthesised using generative image editing. This ap-\nproach, as opposed to generating images from scratch, only\nminimally modifies the images. Hence, it better retains fi-\ndelity and diversity [45, 55]. Moreover, editing offers the\nunique flexibility of controlled counter-factual generation.\nFine-grained control over conditional generation requires\nmeta-data to describe each component of interest, much of\nwhich may not exist within the training dataset. In accor-\ndance with each of the above scenarios, we use generative\nediting to 1. remove only COVID-19 while keeping visual\nindicators of the different hospitals; 2. remove only pneu-\nmothorax while keeping the chest drain; and 3. add abnor-\nmalities that occlude lung structures in the image.\nWe train a generative diffusion model [29, 61] on a large\nnumber of chest X-rays from a variety of biomedical imag-\ning datasets (Section 5.1).\nDespite the diversity within\nthese datasets, substantial bias is still present, some of\nwhich are learned by the generative model. As a result,\nwhen using diffusion models for image editing, correlated\nfeatures may also be modified. For example, in Scenario 2,\nremoving the pneumothorax might also remove the chest\ndrains as both features typically co-occur in datasets [63].\nWe observe other artefacts at the border of the editing\nmasks and artefacts that occur when editing images out-\nside of the training dataset domain of the diffusion model\nused for editing. To overcome these challenges, we pro-\npose using multiple masks to break existing correlations.\nThis involves defining which regions should change, and\nexplicitly forcing correlated regions to remain unchanged.\nIn summary, our contribution is two-fold: first, we intro-\nduce a novel editing approach that reduces the presence\nof artefacts in the edited images compared to prior work\n[12, 55]. Second, we demonstrate that our editing approach\nallows us to construct synthetic datasets with specific data\nshifts. We conduct a broad set of experiments using these\nsynthetic datasets to stress-test and thereby expose biases\nin biomedical classification and segmentation models.\n2. Related work\nIn this section, we discuss the extensive recent develop-\nments in diffusion-based image editing, stress-testing vi-\nsion models, and counterfactuals in biomedical imaging.\n2.1. Generative image editing\nSince the development of modern deep generative models,\nseveral approaches to image editing have emerged. Many\nof these early approaches use compressed latent manipula-\ntion [15, 56, 69, 77] where fine-grained edits are difficult to\nachieve and can result in unwanted changes. More recently,\nthe unparalleled flexibility of diffusion models, together\nwith advances in plain text conditioning, have opened up\nnew avenues for editing techniques.\nHere, we describe some notable diffusion editing methods.\nSDEdit [50] shows that diffusion models trained solely on\nreal images can be used to generate images from sketches\nby perturbing sketches with noise, then running the reverse\ndiffusion process from that time step. Palette [65] is an\nimage-to-image diffusion model that can be used for in-\npainting by filling a masked region with noise and learning\nto denoise that region. Blended diffusion [2, 3] uses masks\nwith CLIP [57] conditioning to guide local edits. Multi-\nple works show that injecting U-Net activations obtained\nby encoding the original image into the generation process\nmakes the global structure of the source and edited images\nclosely match [26, 76]. DiffEdit [12] uses text prompts to\ndetermine the appropriate region to edit. Mokady et al. [51]\nimprove the quality of diffusion inversions by optimising\nthe diffusion trajectory.\nCrucially, in the works which use masks for editing, a sin-\ngle type of mask is always used to define the region of in-\nterest. In this work, we argue that a second type of mask\nis required to avoid the loss of features caused by spurious\ncorrelations. As better editing approaches are developed,\nthis requirement should be kept in mind.\n2.2. Stress-testing\nSeveral approaches have used non-deep-generative-model\nmethods to stress-test networks. Hendrycks & Dietterich\n[25] evaluate classification models\u2019 robustness to corrup-\ntions such as blurring, Gaussian noise, and JPEG artefacts.\nSakaridis et al. [66] stress-test a segmentation model for\nroads by using an optical model to add synthetic fog to\nscenes. Koh et al. [41] collate a dataset presenting various\n2\nRadEdit: stress-testing biomedical vision models via diffusion image editing\ndistribution shifts.\nMore recent models have made use of conditional genera-\ntive models to simulate shifts. Prabhu et al. [55] propose\nLANCE, which stress-tests ImageNet [14] classification\nmodels by using diffusion-based image editing to modify\nthe subject in images via caption editing with a large lan-\nguage model (LLM); Kattakinda et al. [39] do similar, but\nmodify the background rather than the subject. Li et al. [45]\nuse diffusion models with a single subject mask to sepa-\nrately edit backgrounds and subjects. Van Breugel et al.\n[78] use generative adversarial networks (GANs) to simu-\nlate distribution shifts on tabular data. This line of research\nis partially related to adversarial attacks [21] where the fo-\ncus is on minimally modifying images such that they are vi-\nsually indistinguishable to a human, but the attacked model\nfails.\n2.3. Biomedical imaging counterfactuals\nGenerative models have also previously been applied to\ngenerate biomedical counterfactuals. Reinhold et al. [59]\nmanipulate causes of multiple sclerosis in brain MRI with\ndeep structural causal models [53]. Sanchez et al. [67] use\ngenerative editing to remove pathologies and thereby de-\ntect abnormalities. Ktena et al. [42] use counterfactuals to\ngenerate out-of-distribution samples for improving classi-\nfier performance. Gu et al. [22] train a diffusion model to\nmodel disease progression by conditioning on a prior X-\nray and text progression description. Unlike our approach,\nthese methods do not use masks to enforce which regions\nmay or may not be edited, meaning that spurious correla-\ntions might affect edits.\n3. Preliminaries\nIn this section, we introduce the background context for\nstress-testing biomedical imaging models: failure modes\nof existing biomedical imaging models caused by different\ntypes of dataset shifts; diffusion models as versatile genera-\ntive models; and generative image editing via text prompts.\n3.1. Dataset shifts\nDataset shift refers to a circumstance where there is a dis-\ncrepancy between the distributions of training and test data\ndue to external factors [9, 37]. Such shifts are regularly\nobserved in machine learning for biomedical imaging, of-\nten due to data scarcity. For example, collected training\ndatasets might consist primarily of healthy patients. How-\never, when the model is used in practice after training, there\ncould be a shift towards unhealthy patients. A taxonomy of\ndifferent types of dataset shifts in the context of biomedical\nimaging was developed by Castro et al. [9]. In this paper,\nwe consider three dataset shifts of particular interest.\nAcquisition shift results from the use of different scanners\n(manufacturer, hardware, and software) or imaging proto-\ncols as often encountered when using data from multiple\ncohorts. These changes affect factors such as image reso-\nlution, contrast, patient positioning, and image markings.\nManifestation shift results from the way the prediction tar-\ngets physically manifest in anatomy changes between do-\nmains. For example, training datasets could consist of more\nsevere pathological cases than observed in practice, or a\npathology may present with different visual features (e.g.,\nin the majority of cases, support devices co-occur).\nPopulation shift results from differences in intrinsic char-\nacteristics of the populations under study, changing the\nanatomical appearance distribution. This definition encom-\npasses examples such as age, sex, ethnicity, and comor-\nbidities, but also abnormalities such as pleural effusion and\nsupport devices. In contrast to manifestation shift, the shift\nin anatomical appearance is not affected by prediction tar-\ngets.\n3.2. Diffusion models\nDenoising diffusion probabilistic models (DDPMs) [29,\n72] are a versatile and effective class of generative models\nthat enable sampling from the data distribution by learning\nto denoise samples corrupted with Gaussian noise. DDPMs\nare formed by defining a forward time process that gradu-\nally adds noise to data points x0 through the recursion\nxt =\np\n1 \u2212 \u03b2txt\u22121 +\np\n\u03b2t\u03f5t,\nt = 1, . . . , T\n(1)\ns.t. xt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u00af\u03f5t ,\n(2)\nwhere \u03f51:T , \u00af\u03f51:T \u223c N(0, I), \u03b21:T is a predefined noise\nschedule that determines how quickly to corrupt the data\nand ensures that xT contains little to no information about\nx0, and \u00af\u03b1t = Qt\ns=1(1 \u2212 \u03b2s). To transform this corruption\nprocess into a generative model, the process is reversed in\ntime, gradually transforming white noise into data points.\nWhile the exact reversal is intractable, a variational approx-\nimation can be defined by the following process [73]:\nxt\u22121 = \u02c6\u00b5t(xt, f\u03b8(xt, t, c)) + \u03c3tzt,\nt = 1, . . . , T\n(3)\n\u02c6\u00b5t(xt, \u03f5t)=\u221a\u00af\u03b1t\u22121\nxt\u2212\u221a1\u2212\u00af\u03b1t\u03f5t\n\u221a\u00af\u03b1t\n+\nq\n1\u2212 \u00af\u03b1t\u22121\u2212\u03c32\nt \u03f5t, (4)\nwhere c is a conditioning signal such as a text descrip-\ntion of the image, f\u03b8(xt, t, c) is a learned approxima-\ntion of the noise \u00af\u03f5t that corrupted the image x0 to ob-\ntain xt, z1:T\n\u223c N(0, I) and \u03c31:T controls how much\nnoise is introduced in the generative process. When \u03c3t =\np\n(1 \u2212 \u00af\u03b1t\u22121)/(1 \u2212 \u00af\u03b1t)\np\n1 \u2212 \u00af\u03b1t/\u00af\u03b1t\u22121 the generative pro-\ncess is Markovian and known as a DDPM [29], while for\n\u03c3t =0 the generative process becomes deterministic and is\ncalled a denoising diffusion implicit model (DDIM) [73].\n3\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n3.3. Image editing\nThe deterministic nature of the DDIM formulation leads to\nsamples x0 having a one-to-one correspondence with latent\nvectors xT . As a result, we can \u2018encode\u2019 data points to la-\ntent vectors deterministically by running the DDIM genera-\ntive process in reverse [73]. This approach is called DDIM\ninversion. Several approaches [12, 50] have shown that it\nis possible to edit images, e.g., changing components such\nas the subject while maintaining the style of the original\nimage, by running the reverse diffusion process augmented\nby the latent vectors and a modified text prompt c.\nHowever, editing with DDIM inversion can lead to unde-\nsired artefacts in the edited images. For example, structures\nunrelated to the desired edit may also change shape, size,\nor location. To address this, Huberman-Spiegelglas et al.\n[31] propose DDPM inversion, which better retains struc-\nture when editing. They achieve this by adapting the orig-\ninal forward process defined in Equation (2), replacing the\ncorrelated vectors \u00af\u03f51:T with statistically independent vec-\ntors \u02dc\u03f51:T (see Algorithm 1). These noise vectors are then\nused in the generative process, retaining the structure of\nthe original image better than DDIM inversion.\nAlgorithm 1 DDPM inversion [31]\nRequire: original image x0, inversion prompt cinv\nfor t \u2190 1 to T do \u25b7 Sample statistically independent \u02dc\u03f5t\n\u02dc\u03f5t \u223c N(0, I)\n\u02c6xt \u2190 \u221a\u00af\u03b1tx0 + \u221a1 \u2212 \u00af\u03b1t\u02dc\u03f5t\nfor t \u2190 T to 1 do\n\u25b7 Isolate zt from series \u02c6x1:T\n\u03f5t \u2190 f\u03b8(\u02c6xt, t, cinv)\nzt \u2190 (\u02c6xt\u22121 \u2212 \u02c6\u00b5t(\u02c6xt, \u03f5t))/\u03c3t\n\u02c6xt\u22121 \u2190 \u02c6\u00b5t(\u02c6xt, \u03f5t) + \u03c3tzt\n\u25b7 Avoid error accum.\nreturn (\u02c6x1:T , z1:T )\n4. Method\nOur objective is to create synthetic test data through image\nediting, to rigorously evaluate biomedical imaging models\nby simulating specific data shifts. This synthetic data is\nthen used to predict model robustness, eliminating the need\nfor additional real-world test data.\n4.1. Limitations of existing editing methods\nRecent advancements in diffusion modelling have enabled\nthe editing of images in a highly controlled manner. For\ninstance, if we aim to test for an underrepresented pathol-\nogy, we can control for the location and severity while in-\ncorporating the pathology into an image using descriptive\nprompts. However, as we describe in the following para-\ngraphs, we find that two prevalent approaches for editing\nnatural images result in undesirable artefacts in the biomed-\nical domain, rendering them unsuitable for stress-testing\nbiomedical vision models.\nThe first approach, LANCE [55], which uses DDIM in-\nversion for editing [51, 73], does not limit the area that\nis edited within the image. Instead, it uses only a global\nprompt to modify aspects of the original image. While such\nan approach works well in the natural image domain, we\nfind that it leads to artefacts in the biomedical image do-\nmain. For example, in Figure 2, we take an image contain-\ning a pneumothorax and chest drain and attempt to remove\nonly the pneumothorax using the prompt \u2018No acute car-\ndiopulmonary process\u20191, while keeping the rest of the im-\nage, including the chest drain, intact. However, we observe\nthat not only is the region containing the pneumothorax al-\ntered, but the chest drain is also removed (Figure 2c)2.\n(a) Original Image\n(b) RadEdit (ours)\n(c) LANCE2 [55]\nFigure 2: Removing pneumothorax (red) from X-rays us-\ning LANCE2 (c) results in the spuriously correlated chest\ndrain (blue) also being removed. In contrast to LANCE2,\nRadEdit (b, ours) uses the pneumothorax (red) and the\nchest drain mask (blue) to remove the pneumothorax while\npreserving the chest drain.\nFor both edits, we use the\nprompt \u2018No acute cardiopulmonary process\u2019.\nLANCE2\nalso results in a decrease in image contrast and less well-\ndefined anatomical structures, which are preserved with\nRadEdit.\nThis makes LANCE unsuitable for, e.g., our manifestation\nshift evaluation (Section 5.3), which requires that devices\nsuch as chest drains are preserved. We hypothesise that\nthis artefact indicates that the underlying diffusion model\n(described in Section 5.1) has learned correlations between\ncertain pathologies and support devices.\nThe second commonly used editing approach addresses\nthis issue by introducing a mask medit into the editing pro-\ncess, allowing editing only within the given region.\nA\nwidely used method for masked image editing is DiffEdit\n[12], which is outlined in Algorithm 2 for classifier-free\nguidance (CFG) weight w [28], and using DDPM inversion\n[31], as defined in Algorithm 1.\n1This is a common radiological description of a \u2018normal\u2019\nchest X-ray. The more direct editing prompt \u2018No pneumothorax\u2019\nleads to even more severe artefacts; see discussion in Appendix E.\n2For LANCE, we perform the text perturbation manually.\n4\nRadEdit: stress-testing biomedical vision models via diffusion image editing\nAlgorithm 2 DiffEdit3 [12] with DDPM inversion\nRequire: original image x0, inversion prompt cinv, editing\nprompt c, exclusion mask medit\n(\u02c6x1:T , z1:T ) \u2190 DDPMINVERSION(x0, cinv)\nxT \u2190 \u02c6xT\nfor t \u2190 T to 1 do\n\u03f5cond,t \u2190 f\u03b8(xt, t, c)\n\u03f5uncond,t \u2190 f\u03b8(xt, t, c = \u2205)\n\u03f5t \u2190 \u03f5uncond,t + w(\u03f5cond,t \u2212 \u03f5uncond,t)\n\u25b7 CFG\nxt\u22121 \u2190 \u02c6\u00b5t(xt, \u03f5t) + \u03c3tzt\nxt\u22121 \u2190 medit \u2299 xt\u22121 + (1 \u2212 medit) \u2299 \u02c6xt\u22121\nreturn edited version of x0\nWhile DiffEdit guarantees that parts of the image outside of\nthe mask remain unaltered, it can lead to a different class\nof artefacts: sharp discrepancies are often seen at the mask\nboundaries. Figure 3 illustrates a use-case of image editing\nwhere after editing the are inside of the mask is inconsistent\nwith the area outside of the mask.3 Adding consolidation\nresults in a large change that cannot be completely con-\nstrained to within the masked lung region and should lead\nto a partial occlusion of the lung border. As such, DiffEdit\nresults in unrealistic artefacts in the area where the lung\nborder was previously visible (Figure 3c). Moreover, sim-\nilar artefacts emerge when the editing mask is noisy, for\ninstance, when parts of the area we intend to edit are in-\ncorrectly excluded from the mask. Such artefacts render\nDiffEdit unsuitable for the experiments in Section 5.4.\n(a) Original Image\n(b) RadEdit (ours)\n(c) DiffEdit3 [12]\nFigure 3:\nAdding consolidation to the left lung using\nDiffEdit3 (c) results in a dark border along the original lung\nmask (red) since editing can only occur within the masked\nregion. RadEdit (b; ours) allows the region outside of the\nmask (red) to change to ensure consistency, resulting in\nmore realistic edits. For both edits we use the prompt \u2018New\nleft upper lobe consolidation\u2019 and a mask of the lung (red).\n4.2. Improved editing with RadEdit\nTo address the issues outlined in the previous section, we\npropose RadEdit: by introducing inclusion and exclusion\n3For DiffEdit, we use the ground-truth mask rather than a self-\npredicted one, since the latter could result in a mask including\nspurious features.\nmasks into the editing process, RadEdit provides additional\ncontrol, explicitly specifying which areas must remain un-\nchanged (inclusion) and which should be actively modified\nbased on the conditioning signal (exclusion).\nCrucially,\nthese masks do not need to be mutually exclusive, meaning\nthat changes are permitted in the unmasked regions to en-\nsure global consistency. To use masks to correct for model\nbiases, we assume for the most part, that spurious correla-\ntions are non-overlapping [47].\nThe steps of RadEdit are detailed in Algorithm 3: for\neach time step t in the generation process of the edited\nimage, we use the output of the previous step xt to pre-\ndict the conditional noise \u03f5cond,t using the editing prompt c\nand to predict the unconditional noise \u03f5uncond,t. We then\ncombine the predicted noises using CFG [28], resulting\nin the combination \u03f5t. Since we aim to edit only within\nthe exclusion mask medit, we combine \u03f5t and \u03f5uncond,t via\nmedit \u2299 \u03f5t + (1 \u2212 medit) \u2299 \u03f5uncond,t. Using \u03f5t only inside\nthe mask medit allows us to use high guidance scale val-\nues (following [31], we use a value of 15). This ensures\nthat, e.g., a pathology is completely removed without dras-\ntically changing the rest of the image, i.e., 1 \u2212 medit. See\nAppendix E for an in-depth discussion about observed arte-\nfacts when no masks are used for editing.\nAfter obtaining the edited image xt\u22121, we undo possible\nchanges in the region of the image described by the inclu-\nsion mask mkeep via mkeep\u2299 \u02c6xt+(1\u2212mkeep)\u2299xt\u22121, where\n\u02c6xt\u22121 is the output of the DDPM inversion at time step t\u22121.\nIt is important to note that instead of initiating our generat-\ning process from pure noise we set xT = \u02c6xT , where \u02c6xT is\nthe last output of the DDPM inversion.\nAlgorithm 3 RadEdit (ours) uses multiple masks for edit-\ning to decouple spurious correlations\nRequire: original image x0, inversion prompt cinv, editing\nprompt c, exclusion mask medit, inclusion mask\nmkeep\n(\u02c6x1:T , z1:T ) \u2190 DDPMINVERSION(x0, cinv)\nxT \u2190 \u02c6xT\nfor t \u2190 T to 1 do\n\u03f5cond,t \u2190 f\u03b8(xt, t, c)\n\u03f5uncond,t \u2190 f\u03b8(xt, t, c = \u2205)\n\u03f5t \u2190 \u03f5uncond,t + w(\u03f5cond,t \u2212 \u03f5uncond,t)\n\u25b7 CFG\n\u03f5t \u2190 medit \u2299 \u03f5t + (1 \u2212 medit) \u2299 \u03f5uncond,t\nxt\u22121 \u2190 \u02c6\u00b5t(xt, \u03f5t) + \u03c3tzt\nxt\u22121 \u2190 mkeep \u2299 \u02c6xt\u22121 + (1 \u2212 mkeep) \u2299 xt\u22121\nreturn edited version of x0\nIn Figures 2b and 3b, we show that RadEdit enables us to\nperform artefact-free editing while preserving all structures\nof interest.\nBecause the anatomical layout of the origi-\nnal image remains intact, the masks still correspond to the\nsame structures in the edited images. In Section 5.4, we\n5\nRadEdit: stress-testing biomedical vision models via diffusion image editing\nshow that the same masks used for editing can be reused to\nstress-test lung segmentation models.\nIn practice, we use a latent diffusion model [61]. There-\nfore, all operations in Algorithm 3 are performed in the\nlatent space of a variational autoencoder (VAE) [61]. How-\never, this does not limit the generality of the approach. For\ndetails about our diffusion model, see Section 5.1.\n4.3. Use synthetic images for uncovering bias\nDespite significant advancements in biomedical computer\nvision, recent studies have shown that bias in training and\ntest data can lead to unrealistically high performance of\nmachine learning models on the test set [13, 64]. In our\nexperiments, we create synthetic test datasets with RadEdit\nto quantify the robustness of models to specific types of\ndataset shifts. By using masks, we can precisely edit the\noriginal training data to represent either acquisition shift,\npopulation shift, or manifestation shift [9] (Sections 5.2\nto 5.4).\nThese synthetic test sets are used to stress-test (potentially\nbiased) biomedical vision models by comparing perfor-\nmance to the real (biased) test set; a significant drop in\nperformance indicates that the vision model is not robust\nto the synthetic dataset shift.\nThe use of image editing in this manner can serve as a com-\nplementary tool to visual explainable AI tools like Grad-\nCAM [68] and saliency maps [1, 71], which offer only\nqualitative insight into the robustness of vision models.\n4.4. BioViL-T editing score\nThe stochastic nature of image editing with generative\nmodels means that some edits will be of higher quality\nthan others.\nTo filter out poor-quality generations, an\nimage\u2013text editing score can be used to quantitatively as-\nsess how closely related an image\u2013text pair is, as a pre-\ntrained model is expected to embed similar images and\ntext to nearby vectors [4, 17, 57, 58].\nFor image edit-\ning, we instead assess how similar the change in text and\nimage embeddings are after editing: for a real image\u2013\ntext pair (Ireal, Treal), edited image\u2013text pair (Iedit, Tedit),\nimage embedding model EI, and text embedding model\nET , we calculate \u2206I = EI(Ireal) \u2212 EI(Iedit) and \u2206T =\nET (Treal) \u2212 ET (Tedit), then the editing score is defined\nbased on directional similarity [19]:\nSBioViL-T = 1 \u2212\n\u2206I \u00b7 \u2206T\n\u2225\u2206I\u2225\u2225\u2206T\u2225 .\n(5)\nGiven the focus on biomedical data, we do not use\ngeneral-purpose image and text encoders such as CLIP\n[57]. Instead, we use the image and text encoders from\nBioViL-T [5].\nBioViL-T is a domain-specific vision\u2013\nlanguage model trained to analyse chest X-rays and radiol-\nogy reports, therefore it is well suited to measure changes\nin the edited image, such as removed pathologies. Follow-\ning Prabhu et al. [55], we discard images with SBioViL-T <\n0.2. This is not only effective for filtering out poor quality\nedits but also able to detect whether the original image Ireal\ndoes not match the original text description Treal well.\n5. Experiments\n5.1. Diffusion model\nOur editing method is heavily dependent on a diffusion\nmodel that can generate realistic chest X-rays. We use the\nVAE [27, 40] of SDXL [54] since it can adequately recon-\nstruct chest X-rays [10]. During training, only the weights\nof the denoising U-Net are updated, i.e., the VAE is frozen\n[10].\nWe use three datasets for training: MIMIC-CXR [36],\nChestX-ray8 [81], and CheXpert [32]. In total, we used\n487 680 images downsampled to 512 \u00d7 512 pixels for\ntraining.\nFor the MIMIC-CXR dataset, we only include frontal view\nchest X-rays with a corresponding impression section4 in\nthe radiology report, which we use to condition the de-\nnoising U-Net. We employ the tokeniser and frozen text\nencoder from Bannur et al. [5]. For the ChestX-ray8 and\nCheXpert datasets, we condition with a list of all abnor-\nmalities present in an image as indicated by the labels, e.g.,\n\u2018Cardiomegaly. Pneumothorax.\u2019. If the list of abnormalities\nis empty, we use the string \u2018No findings\u2019.\nAn overview of the labels for each dataset can be found in\nAppendix A, alongside more details about the training of\nthe diffusion model.\n5.2. Acquisition shift\nIn this section, we show how our editing method can be\nused to quantify the robustness of a model to acquisition\nshift.\nWe closely follow the experimental setup of De-\nGrave et al. [13]. They show that deep learning systems\nbuilt to detect COVID-19 from chest radiographs rely on\nconfounding factors rather than pathology features. The\nproblem arises when COVID-19-positive and -negative im-\nages in the training dataset come from disparate sources.\nIn our setup, all COVID-19-positive cases come from the\nBIMCV dataset [79] (we call this subset BIMCV+), and\nall COVID-19-negative cases come from the MIMIC-CXR\ndataset [36]. A model trained with the above two datasets\nwill rely on spurious features indicative of the data\u2019s origin,\nsuch as laterality markers or black space at the image bor-\n4The impression is a short, clinically actionable summary of\nthe main findings in the radiology report.\n6\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n(a) Example image from the\nMIMIC-CXR dataset [36].\n(b) Example image from the\nBIMCV+ dataset [79].\nFigure 4: Comparison of the visual appearance between\nthe MIMIC-CXR and BIMCV+ dataset. As shown by [13]\nthere are distinct differences in the laterality markings (top\nleft corner) as well as the amount of black space in the top\nand bottom of the images. The bounding boxes in (b) indi-\ncate the presence of abnormalities caused by COVID-19.\nders, to predict whether an image is COVID-19-positive,\ninstead of learning visual features caused by the pathology.\nSee Figure 4 for an example of the differences in appear-\nance between the two datasets.\nFor all edits, we start with an image from the BIMCV+\ntraining set, which includes manually annotated bound-\ning boxes of abnormalities caused by COVID-19. We use\nthe bounding boxes as the exclusion mask medit.\nSince\nonly a single type of mask is available we set the inclu-\nsion mask mkeep = 1 \u2212 medit.\nThe editing prompt is\n\u2018No acute cardiopulmonary process\u2019 for all images. The\nresult of our editing procedure is a synthetic dataset of\nCOVID-19-negative images containing the same spurious\nfeatures as the BIMCV+ dataset, such as laterality markers\nor black space at the image borders. Note that our diffusion\nmodel in Section 5.1 was trained using neither the BIMCV\ndatasets nor any labels or prompts containing COVID-19,\ni.e., we perform zero-shot edits.\nAfter filtering the edits using the BioViL-T editing score\nfrom Section 4.4, we are left with 2774 images, all of which\nare COVID-19-negative while containing the same spuri-\nous features as the BIMCV+ dataset.\nIn Table 1, we show the performance of a COVID-19 clas-\nsifier trained on BIMCV+ and MIMIC-CXR. In accordance\nwith DeGrave et al. [13], we find that the classifier per-\nforms exceptionally well on the real test set (comprised of\ntest splits of both datasets) since the model learned to dis-\ntinguish the two data sources instead of learning visual ab-\nnormalities related to COVID-19. However, in the second\nrow of Table 1, we see a drop of 95% in accuracy meaning\nthat the model fails to classify the images of the synthetic\ndataset as COVID-19-negative. The model is not robust to\na shift in acquisition.\nTo show that the decreased performance of the classifier is\nnot caused by artefacts in the edited images, we train a sec-\nond, more robust COVID-19 classifier, using the BIMCV+\nand BIMCV- datasets, as seen in [13], where the BIMCV-\ndataset consist of only COVID-19-negative cases from the\nsame cohort as the BIMCV+ dataset. We test the robust\nmodel on the same two test datasets. If we compare rows\none and three of Table 1, we find that the robust classifier\nperforms worse on the test set comprising samples from\nBIMCV+ and MIMIC-CXR than the previous model (row\none). This is expected as the robust model relies on actual\npathology features to predict COVID-19. Last, rows three\nand four of Table 1 show that the robust model performs\nsimilarly on the real and synthetic test sets, attesting the\nquality of our edits. For additional details of the experi-\nmental setup see Appendix C.\nTable 1: Quantifying robustness of COVID-19 detectors\nto acquisition shift. The \u2018Biased\u2019 dataset is a combination\nof BIMCV+ and MIMIC-CXR; the \u2018Unbiased\u2019 dataset is\na combination of BIMCV+ and BIMCV-; the \u2018Synthetic\u2019\ntest set consists of COVID-19-negative images which con-\ntain the same spurious features as the BIMCV+ datasets,\nsuch as laterality markers or black space at image borders.\nWe report mean accuracy and standard deviation across five\nruns.\nTrain data\nTest data\nAccuracy\nBiased\nBiased\n99.1 \u00b1 0.2\nBiased\nSynthetic\n5.5 \u00b1 2.1\nUnbiased\nBiased\n74.4 \u00b1 3.0\nUnbiased\nSynthetic\n76.0 \u00b1 7.7\n5.3. Manifestation shift\nIn the following section, we show how RadEdit can be used\nto quantify the robustness of a computer vision model to\nmanifestation shift. We closely follow the experimental\nsetup of Rueckel et al. [64], who demonstrate that the clas-\nsification results of models trained to predict pneumotho-\nrax are strongly biased towards predicting pneumothorax\nwhen chest drains are present. This is due to chest drains\nbeing a common treatment for pneumothorax, resulting in\nthe majority of images in public datasets like CANDID-\nPTX [16] containing a chest drain only if there is pneu-\nmothorax. As a result, only 1% of the images (170 images)\nin the CANDID-PTX dataset contain a chest drain but no\npneumothorax. Rueckel et al. [64] show that, while the av-\nerage performance of a pneumothorax classifier trained on\nthe CANDID-PTX dataset is high, the performance on the\nsubset of images with no pneumothorax and a chest drain is\n7\nRadEdit: stress-testing biomedical vision models via diffusion image editing\nsignificantly lower. The strong correlation between pneu-\nmothorax and chest drain makes the above experimental\nsetup highly suitable to show how RadEdit can test the\nrobustness of biomedical vision models to manifestation\nshift.\nWe use RadEdit to create a synthetic dataset consisting\nof images with no pneumothorax and a chest drain. We\nfilter all images that contain pneumothorax and a chest\ndrain from the training set for editing. In the case of the\nCANDID-PTX dataset, segmentation masks for pneumoth-\norax and chest drains are available. We use the pneumoth-\norax segmentation mask as the exclusion mask, medit, and\nthe chest tube segmentation mask as the inclusion mask\nmkeep. For editing, we use the prompt \u2018No acute cardiopul-\nmonary process.\u2019. Using an inclusion mask we ensure that\nthe chest drain will be still present after editing. Further-\nmore, we allow the rest of the image to change, to prevent\nartefacts at the border of the masks, e.g., if the masks do\nnot fully capture the pneumothorax or chest drain. The dif-\nfusion model from Section 5.1 is used for all edits. Note\nthat the diffusion model has not seen the CANDID-PTX\ndataset during training, i.e., we perform zero-shot edits.\nAfter filtering the edits using the BioViL-T editing score\nfrom Section 4.4, we are left with 628 images, all of which\ncontain a chest drain but no pneumothorax. In comparison,\nthe real test set contains only 16 of those cases.\nIn Table 2, we show the performance of a pneumothorax\nclassifier trained on the CANDID-PTX dataset. In accor-\ndance with [64] we find that the classifier performs excep-\ntionally well on the test split of CANDID-PTX. However,\nthe test split contains only 16 cases of images with no pneu-\nmothorax and a chest drain. In row two of Table 2, we show\na drastic drop in performance on the synthetic test set, i.e.,\nwe show that the model is not robust to manifestation shift.\nAnalogous to Section 5.2, we use a more robust model to\nshow that the drop in performance on the synthetic dataset\ndoes not come from editing artefacts. We follow [64] and\ntrain a second model on the SIIM-ACR dataset [83] dataset.\nFurthermore, the model is trained to detect the presence of\npneumothorax, as well as to segment pneumothorax and\nchest drains.\nWe test the more robust model on the same two test\ndatasets. Comparing rows one and three of Table 2, we\nfind that the robust model performs on par with the clas-\nsifier in row one. Last, comparing the performance of the\nrobust model in rows three and four of Table 2 we find that\nthe model performs similarly on the real test set and the\nsynthetic one. Attesting the quality of our edits. In accor-\ndance with the findings of [64] there is still a performance\ngap that indicates that the robust model is still suffering\nfrom mild manifestation shift. For additional details of the\nexperimental setup see Appendix C.\nTable 2: Quantifying robustness of pneumothorax de-\ntectors to manifestation shift. For training the \u2018Biased\u2019\nmodel we use the CANDID-PTX dataset.\nThe training\ntask is classifying the presence or absence of pneumoth-\norax. For training the \u2018Unbiased\u2019 model we use the SIIM-\nACR dataset, the training task is to classify and segment\nthe pneumothorax. There are two test sets: the \u2018Biased\u2019\ntest data comes from the CANDID-PTX dataset which ex-\nhibits strong confounding between the pneumothorax and\nthe chest tubes; the \u2018Synthetic\u2019 test data consists solely of\nedited images that contain chest drains but no pneumoth-\norax.\nWe report mean accuracy and standard deviation\nacross five runs.\nModel\nTest data\nAccuracy\nBiased\nBiased\n93.3 \u00b1 0.6\nBiased\nSynthetic\n17.9 \u00b1 3.7\nUnbiased\nBiased\n93.7 \u00b1 1.3\nUnbiased\nSynthetic\n81.7 \u00b1 7.1\n5.4. Population shift\nIn this section, we demonstrate how our editing method\ncan be used to quantify the robustness of pre-trained lung\nsegmentation models to population shifts. In general, the\nlabour-intensive nature of manually segmenting X-ray im-\nages, paired with the high skill level demanded of the la-\nbeller, typically results in manually labelled biomedical\nimaging datasets that are small and focus on single patholo-\ngies or healthy patients [11, 34, 70]. The lack of variation\nwithin these training datasets means that often there are few\nto zero examples of some abnormalities, namely medical\nequipment and pathologies which appear as white regions\non X-rays; as such, lung segmentation models trained on\nthese datasets are sensitive to features which obscure the\nlung [46]. To assess the robustness of segmentation models\nto such cases, images would have to be collected and man-\nually segmented for each occlusion to be tested, which is\ntime-consuming and costly.\nRadEdit allows us to stress-test segmentation models while\nbypassing the need to collect and label more data. To con-\nstruct stress-testing sets we take healthy X-rays with cor-\nresponding ground-truth lung segmentation maps, then add\nabnormalities to the lungs. Because editing is constrained\nwithin the ground-truth segmentations, the lung boundaries\nshould remain unchanged after the edit\u2014meaning that we\ncan estimate the robustness of segmentation models to ab-\nnormalities without additional manual labelling; when edit-\ning a single lung, we use RadEdit, with the exclusion mask\nmedit corresponding to the lung to be edited and the inclu-\nsion mask mkeep to the lung which stays the same. When\nediting both lungs we set mkeep = 0. We allow the region\n8\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n0\n20\n40\n\u2206 AHD \u2193\nHealthy\nedit\n\u2192\nOedema\nHealthy\nedit\n\u2192\nPacemaker\nHealthy\nedit\n\u2192\nConsolidation\nWeak Predictor\nStrong Predictor\n(a) Across edited datasets, a smaller increase in AHD is seen for\nthe strong predictor model, demonstrating that it is less biased.\n1\n0.95\n0.9\n0.85\n\u2206 Dice score \u2191\nHealthy\nedit\n\u2192\nOedema\nHealthy\nedit\n\u2192\nPacemaker\nHealthy\nedit\n\u2192\nConsolidation\nWeak Predictor\nStrong Predictor\n(b) Across edited datasets, a smaller decrease in Dice score is seen\nfor the strong predictor model, demonstrating that it is less biased.\nFigure 5: Quantifying robustness of lung segmentation\nmodels to population shift.\nWe evaluate a \u2018weak pre-\ndictor\u2019 trained on MIMIC-Seg (a small set of predomi-\nnantly healthy patients), shown in blue, a \u2018strong predictor\u2019\ntrained on CheXmask (a large mixed set of patients with\nmore abnormalities), shown in red; reporting the change\nin Dice score and average Hausdorff distance (AHD) after\nediting in oedema, pacemakers, and consolidation, with re-\nspect to the models evaluated on the ground truth test set\n(baseline values shown as horizontal lines).\noutside of the lungs to potentially change to allow opacity\nadjustments, or for necessary elements to be added outside\nof the lungs.\nWe generate stress sets for three abnormalities: pulmonary\noedema, pacemaker, and upper lobe consolidation.\nFor\npulmonary oedema, the prompt we use is \u2018Moderate pul-\nmonary oedema. The heart size is normal.\u2019, for pacemaker,\nit is \u2018Left pectoral pacemaker in place. The position of the\nleads is as expected. Otherwise unremarkable chest ra-\ndiographic examination.\u2019, and for consolidation it is \u2018New\n[left/right] upper lobe consolidation.\u2019. These prompts are\nphrased to match similar impressions in the training data.\nWe generate a single edit per image in the MIMIC-Seg [11]\ntraining set, then filter out lower quality edits using the edit-\ning score (Section 4.4).\n(a) Original\n(b) Edited\n(c) Weak\nPredictor\n(d) Strong\nPredictor\nFigure 6: Adding pulmonary oedema (top), pacemakers\n(middle), and consolidation (bottom) with RadEdit, we ob-\nserve that the \u2018strong predictor\u2019 (d), a segmentation model\ntrained on CheXmask [18] (a large mixed set of patients\nwith many containing abnormalities) is more robust to\nthese abnormalities the \u2018weak predictor\u2019 (c), a segmenta-\ntion model trained on MIMIC-Seg [11] (a small set of pre-\ndominantly healthy patients), with the weak predictor trac-\ning around the pacemaker and poorly annotating the con-\nsolidated lung. Blue: ground-truth annotation; red: pre-\ndicted segmentation.\nWe test the performance of two segmentation models on\nour edits: a \u2018weak predictor\u2019 which is a U-Net segmen-\ntation model [8, 33, 62, 75] trained on the MIMIC-Seg\ndataset [11] which contains 1141 image\u2013mask pairs man-\nually verified by radiologists, the majority of which are\nhealthy patients; and a \u2018strong predictor\u2019, another U-Net\nmodel on the CheXmask dataset [18] which is substan-\ntially larger, containing 676 803 image\u2013mask pairs, many\nof which will be unhealthy patients with lung abnormali-\nties (more details can be found in Appendix D). As such,\nwe would expect that the strong predictor would be less\nbiased than the weak predictor which should result in a\ngreater level of robustness to abnormalities and therefore\nprovide more accurate segmentations. The quality of the\nsegmentations is evaluated with Dice score, the harmonic\nmean of the precision and recall, and 95th percentile AHD,\na measure of the distance between two sets [49].\nIn Figure 5, we evaluate these segmentation models on the\nsynthetic edited stress sets (which do not contain any of\nthe original real data), and show how the metrics change\nrelative to performance on the real data.\nWe find that\nboth models are relatively robust to mild-to-moderate pul-\nmonary oedema. For the more substantial occlusions ob-\n9\nRadEdit: stress-testing biomedical vision models via diffusion image editing\ntained by adding pacemakers or consolidation, we observe\nlarger drops in quantitative performance in both Dice score\nand Hausdorff distance.\nHowever, we observe that the\nstrong predictor is more robust to these occlusions than the\nweak predictor, confirming the hypothesis that observing\nmore abnormal cases during training results in a greater\nlevel of robustness. A visual comparison can also be found\nin Figure 6: for pulmonary oedema, both segmentation\nmodels can accurately segment, despite the abnormality;\nfor pacemakers, the weak predictor incorrectly segments\naround the pacemakers, while the strong predictor more\naccurately segments the lungs; and for consolidation, both\nmodels are less able to segment the lungs accurately, how-\never, the strong predictor\u2019s predictions are considerably\ncloser to the ground truth. More visual examples can be\nfound in Appendix D.\n6. Limitations and future work\nDespite the encouraging results presented in the paper,\nRadEdit has its limitations and more work is needed to\nextend it to more applications.\nCurrently, we manually\nanalyse training datasets and models and predict potential\nfailure cases, simulate these failures to test the hypothesis,\nand finally quantitatively evaluate the model; future work\ncould automate such failure mode discovery to simplify this\npipeline. Another limitation is that current editing tech-\nniques do not enable all manners of stress testing; for exam-\nple, with current approaches, we are unable to test segmen-\ntation models\u2019 behaviour to cardiomegaly since this would\nrequire adjusting the segmentation map after the edit. How-\never, this could potentially be enabled by enlarging heart\nsegmentations to simulate cardiomegaly and adjusting the\nground-truth lung segmentation accordingly.\nWhile we found that introducing masks to DDPM inversion\nto constrain the editing process is effective at preventing\nunwanted edits from spurious correlations occurring out-\nside of the masks, this does not guarantee that changes\nthat occur within the mask are always as expected. For\nexample, when adding abnormalities to the lungs, addi-\ntional changes may occur, such as cardiomegaly being in-\ntroduced, or lungs shrinking in size. We observe that this\noccurs infrequently, only for additions (not for subtrac-\ntions) and larger edits. While careful prompt engineering\nis effective in preventing such changes, future work on im-\nproving structure maintenance when editing is needed.\nWhen producing simulated stress test sets, several factors\naffect edit quality. For example, hyperparameters includ-\ning CFG weight, number of inference steps, and what time\nstep to start the reverse diffusion process from. Further-\nmore, effective prompting requires knowledge of the re-\nports, e.g., PPM vs pacemaker. LLM prompt rephrasing\nmay improve this [23]. Moreover, the capability of the text\nencoder model to provide informative features for the dif-\nfusion model across pathologies, together with the capabil-\nity of the generative model to capture fine details and well\ncover the underlying data distribution places a fundamental\nrestriction on what edits are possible.\nFinally, more research is required to develop better ap-\nproaches for quantifying edit quality for downstream tasks.\nIn particular, observing a change in downstream perfor-\nmance is not indicative of effectiveness for downstream\nevaluation as image quality may be poor. While the in-\ntroduced BioViL-T [5] editing score goes some way to\nquantify edit quality, this introduces reliance on an external\nmodel which may also be biased. In addition, the BioViL-T\nediting score is not suited to detect the artefacts introduced\nby LANCE2 and DiffEdit3.\n7. Conclusion\nIn this study, we illustrate the efficacy of generative im-\nage editing as a robust tool for stress-testing biomedical\nvision models.\nOur focus is on assessing their robust-\nness against three types of dataset shifts commonly encoun-\ntered in biomedical imaging: acquisition shift, manifesta-\ntion shift, and population shift. We highlight that one of\nthe significant challenges in biomedical image editing is\nthe correlations learned by the generative model, which can\nresult in artefacts during the editing process. To mitigate\nthese artefacts RadEdit relies on various types of masks\nto restrict the effects of the editing to certain areas while\nensuring the consistency of the edited images. This ap-\nproach enables us to generate synthetic test sets of high fi-\ndelity that exhibit common dataset shifts. We then utilize\nthese synthetic test sets to identify and quantify the failure\nmodes of biomedical classification and segmentation mod-\nels. This provides a valuable supplement to explainable\nAI approaches such as Grad-CAM [68] and saliency maps\n[1, 71]\n10\nRadEdit: stress-testing biomedical vision models via diffusion image editing\nReferences\n[1] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I.,\nHardt, M., and Kim, B. Sanity checks for saliency\nmaps. Advances in neural information processing sys-\ntems, 31, 2018. 6, 10\n[2] Avrahami, O., Lischinski, D., and Fried, O. Blended\ndiffusion for text-driven editing of natural images. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 18208\u2013\n18218, 2022. 2\n[3] Avrahami, O., Fried, O., and Lischinski, D. Blended\nlatent diffusion.\nACM Transactions on Graphics\n(TOG), 42(4):1\u201311, 2023. 2\n[4] Azadi, S., Olsson, C., Darrell, T., Goodfellow, I., and\nOdena, A. Discriminator rejection sampling. arXiv\npreprint arXiv:1810.06758, 2018. 6\n[5] Bannur, S., Hyland, S., Liu, Q., Perez-Garcia, F., Ilse,\nM., Castro, D. C., Boecking, B., Sharma, H., Bouzid,\nK., Thieme, A., Schwaighofer, A., Wetscherek, M.,\nLungren, M. P., Nori, A., Alvarez-Valle, J., and\nOktay, O.\nLearning to Exploit Temporal Structure\nfor Biomedical Vision-Language Processing. In The\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 1 2023.\ndoi: 10.48550/\narxiv.2301.04558. 6, 10, 15\n[6] Barbano, R., Denker, A., Chung, H., Roh, T. H., Ar-\nrdige, S., Maass, P., Jin, B., and Ye, J. C. Steerable\nconditional diffusion for out-of-distribution adapta-\ntion in imaging inverse problems.\narXiv preprint\narXiv:2308.14409, 2023. 1\n[7] Buslaev, A., Iglovikov, V. I., Khvedchenya, E., Pari-\nnov, A., Druzhinin, M., and Kalinin, A. A. Albumen-\ntations: Fast and flexible image augmentations. Infor-\nmation, 11(2):125, February 2020. ISSN 2078-2489.\ndoi: 10.3390/info11020125. 16\n[8] Cardoso, M. J., Li, W., Brown, R., Ma, N., Ker-\nfoot, E., Wang, Y., Murrey, B., Myronenko, A., Zhao,\nC., Yang, D., et al. Monai: An open-source frame-\nwork for deep learning in healthcare. arXiv preprint\narXiv:2211.02701, 2022. 9\n[9] Castro, D. C., Walker, I., and Glocker, B. Causality\nmatters in medical imaging. Nature Communications,\n11(1):3673, 2020. 1, 3, 6\n[10] Chambon,\nP.,\nBluethgen,\nC.,\nDelbrouck,\nJ.-B.,\nVan der Sluijs, R., Po\u0142acin, M., Chaves, J. M. Z.,\nAbraham, T. M., Purohit, S., Langlotz, C. P., and\nChaudhari, A.\nRoentgen: vision-language founda-\ntion model for chest x-ray generation. arXiv preprint\narXiv:2211.12737, 2022. 6\n[11] Chen, L.-C., Kuo, P.-C., Wang, R., Gichoya, J., and\nCeli, L. A. Chest X-ray segmentation images based\non MIMIC-CXR (version 1.0.0). PhysioNet, 2022. 8,\n9, 17, 18\n[12] Couairon, G., Verbeek, J., Schwenk, H., and Cord,\nM. DiffEdit: Diffusion-based semantic image editing\nwith mask guidance, 2022. 2, 4, 5\n[13] DeGrave, A. J., Janizek, J. D., and Lee, S.-I.\nAI\nfor radiographic COVID-19 detection selects short-\ncuts over signal. Nature Machine Intelligence, 3(7):\n610\u2013619, 2021.\nISSN 2522-5839.\ndoi: 10.1038/\ns42256-021-00338-7. 6, 7\n[14] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and\nFei-Fei, L. ImageNet: A large-scale hierarchical im-\nage database. In 2009 IEEE Conference on Computer\nVision and Pattern Recognition, pp. 248\u2013255, 2009. 3\n[15] Dosovitskiy, A., Tobias Springenberg, J., and Brox, T.\nLearning to generate chairs with convolutional neural\nnetworks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 1538\u2013\n1546, 2015. 2\n[16] Feng, S., Azzollini, D., Kim, J. S., Jin, C.-K., Gor-\ndon, S. P., Yeoh, J., Kim, E., Han, M., Lee, A., Pa-\ntel, A., Wu, J., Urschler, M., Fong, A., Simmers, C.,\nTarr, G. P., Barnard, S., and Wilson, B. Curation of\nthe CANDID-PTX dataset with free-text reports. Ra-\ndiology: Artificial Intelligence, 3(6):e210136, 2021.\nISSN 2638-6100. doi: 10.1148/ryai.2021210136. 7\n[17] Fernandez, V., Sanchez, P., Pinaya, W. H. L., Ja-\ncenk\u00b4ow, G., Tsaftaris, S. A., and Cardoso, J.\nPri-\nvacy distillation:\nReducing re-identification risk\nof multimodal diffusion models.\narXiv preprint\narXiv:2306.01322, 2023. 6\n[18] Gaggion, N., Mosquera, C., Mansilla, L., Aineseder,\nM., Milone, D. H., and Ferrante, E. CheXmask: a\nlarge-scale dataset of anatomical segmentation masks\nfor multi-center chest X-ray images. arXiv preprint\narXiv:2307.03293, 2023. 9, 17, 18\n[19] Gal, R., Patashnik, O., Maron, H., Bermano, A. H.,\nChechik, G., and Cohen-Or, D. StyleGAN-NADA:\nCLIP-guided domain adaptation of image generators.\nACM Transactions on Graphics, 41(4), 2022.\ndoi:\n10.1145/3528223.3530164. 6\n[20] Garipov, T., De Peuter, S., Yang, G., Garg, V.,\nKaski, S., and Jaakkola, T.\nCompositional sculpt-\ning of iterative generative processes. arXiv preprint\narXiv:2309.16115, 2023. 1\n11\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n[21] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\ning and harnessing adversarial examples, 2015. 3\n[22] Gu, Y., Yang, J., Usuyama, N., Li, C., Zhang, S.,\nLungren, M. P., Gao, J., and Poon, H.\nBiomed-\njourney: Counterfactual biomedical image generation\nby instruction-learning from multimodal patient jour-\nneys. arXiv preprint arXiv:2310.10765, 2023. 3\n[23] Hao, Y., Chi, Z., Dong, L., and Wei, F.\nOptimiz-\ning prompts for text-to-image generation. In Thirty-\nseventh Conference on Neural Information Process-\ning Systems, 2023. 10\n[24] Heaven, W. D. Hundreds of ai tools have been built\nto catch covid. none of them helped. MIT Technology\nReview. Retrieved December 2023, 2021. 1\n[25] Hendrycks, D. and Dietterich, T. Benchmarking neu-\nral network robustness to common corruptions and\nperturbations. In International Conference on Learn-\ning Representations, 2018. 2\n[26] Hertz, A., Mokady, R., Tenenbaum, J., Aberman,\nK., Pritch, Y., and Cohen-or, D. Prompt-to-prompt\nimage editing with cross-attention control.\nIn The\nEleventh International Conference on Learning Rep-\nresentations, 2022. 2\n[27] Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot,\nX., Botvinick, M., Mohamed, S., and Lerchner, A.\n\u03b2-VAE: Learning basic visual concepts with a con-\nstrained variational framework. In International Con-\nference on Learning Representations, 2016. 6\n[28] Ho, J. and Salimans, T. Classifier-free diffusion guid-\nance, 2022. 4, 5, 15\n[29] Ho, J., Jain, A., and Abbeel, P. Denoising diffusion\nprobabilistic models, 2020. 2, 3, 15\n[30] Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi,\nM., and Salimans, T. Cascaded diffusion models for\nhigh fidelity image generation. The Journal of Ma-\nchine Learning Research, 23(1):2249\u20132281, 2022. 1\n[31] Huberman-Spiegelglas, I., Kulikov, V., and Michaeli,\nT. An edit friendly DDPM noise space: Inversion and\nmanipulations, 2023. 4, 5\n[32] Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus,\nS., Chute, C., Marklund, H., Haghgoo, B., Ball, R.,\nShpanskaya, K., Seekins, J., Mong, D. A., Halabi,\nS. S., Sandberg, J. K., Jones, R., Larson, D. B., Lan-\nglotz, C. P., Patel, B. N., Lungren, M. P., and Ng, A. Y.\nCheXpert: A large chest radiograph dataset with un-\ncertainty labels and expert comparison, 2019. 6\n[33] Isensee, F., J\u00a8ager, P. F., Kohl, S. A., Petersen, J., and\nMaier-Hein, K. H. Automated design of deep learning\nmethods for biomedical image segmentation. arXiv\npreprint arXiv:1904.08128, 2019. 9\n[34] Jaeger, S., Candemir, S., Antani, S., W\u00b4ang, Y.-X. J.,\nLu, P.-X., and Thoma, G.\nTwo public chest x-\nray datasets for computer-aided screening of pul-\nmonary diseases. Quantitative imaging in medicine\nand surgery, 4(6):475, 2014. 8\n[35] Jaini, P., Clark, K., and Geirhos, R.\nIntriguing\nproperties of generative classifiers.\narXiv preprint\narXiv:2309.16779, 2023. 1\n[36] Johnson, A. E. W., Pollard, T. J., Berkowitz, S. J.,\nGreenbaum, N. R., Lungren, M. P., Deng, C.-y.,\nMark, R. G., and Horng, S.\nMIMIC-CXR, a de-\nidentified publicly available database of chest radio-\ngraphs with free-text reports.\nScientific Data, 6\n(1):317, 2019.\nISSN 2052-4463.\ndoi: 10.1038/\ns41597-019-0322-0. 6, 7\n[37] Jones, C., Castro, D. C., Ribeiro, F. D. S., Ok-\ntay, O., McCradden, M., and Glocker, B.\nNo fair\nlunch: A causal perspective on dataset bias in ma-\nchine learning for medical imaging. arXiv preprint\narXiv:2307.16526, 2023. 3\n[38] Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman,\nE., Paris, S., and Park, T. Scaling up gans for text-\nto-image synthesis. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pp. 10124\u201310134, 2023. 1\n[39] Kattakinda, P., Levine, A., and Feizi, S.\nInvari-\nant learning via diffusion dreamed distribution shifts.\narXiv preprint arXiv:2211.10370, 2022. 3\n[40] Kingma, D. P. and Welling, M. Auto-encoding varia-\ntional bayes, 2022. 6\n[41] Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M.,\nZhang, M., Balsubramani, A., Hu, W., Yasunaga,\nM., Phillips, R. L., Gao, I., et al. Wilds: A bench-\nmark of in-the-wild distribution shifts.\nIn Interna-\ntional Conference on Machine Learning, pp. 5637\u2013\n5664. PMLR, 2021. 2\n[42] Ktena, I., Wiles, O., Albuquerque, I., Rebuffi, S.-A.,\nTanno, R., Roy, A. G., Azizi, S., Belgrave, D., Kohli,\nP., Karthikesalingam, A., et al.\nGenerative models\nimprove fairness of medical classifiers under distribu-\ntion shifts. arXiv preprint arXiv:2304.09218, 2023.\n3\n12\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n[43] Larrazabal, A. J., Nieto, N., Peterson, V., Milone,\nD. H., and Ferrante, E. Gender imbalance in med-\nical imaging datasets produces biased classifiers for\ncomputer-aided diagnosis.\nProceedings of the Na-\ntional Academy of Sciences, 117(23):12592\u201312594,\n2020. 1\n[44] Lee, C. H. and Yoon, H.-J. Medical big data: promise\nand challenges. Kidney research and clinical practice,\n36(1):3, 2017. 1\n[45] Li, X., Chen, Y., Zhu, Y., Wang, S., Zhang, R., and\nXue, H. ImageNet-e: Benchmarking neural network\nrobustness via attribute editing, 2023. 1, 2, 3\n[46] Liu, W., Luo, J., Yang, Y., Wang, W., Deng, J., and\nYu, L. Automatic lung segmentation in chest x-ray\nimages using improved u-net. Scientific Reports, 12\n(1):8649, 2022. 8\n[47] Locatello, F., Weissenborn, D., Unterthiner, T., Ma-\nhendran, A., Heigold, G., Uszkoreit, J., Dosovitskiy,\nA., and Kipf, T. Object-centric learning with slot at-\ntention. Advances in Neural Information Processing\nSystems, 33:11525\u201311538, 2020. 5\n[48] Loshchilov, I. and Hutter, F. Decoupled weight decay\nregularization. In International Conference on Learn-\ning Representations, 2018. 15\n[49] Maier-Hein, L., Reinke, A., Godau, P., Tizabi, M. D.,\nBuettner, F., Christodoulou, E., Glocker, B., Isensee,\nF., Kleesiek, J., Kozubek, M., Reyes, M., Riegler,\nM. A., Wiesenfarth, M., Kavur, A. E., Sudre, C. H.,\nBaumgartner, M., Eisenmann, M., Heckmann-N\u00a8otzel,\nD., R\u00a8adsch, A. T., Acion, L., Antonelli, M., Arbel, T.,\nBakas, S., Benis, A., Blaschko, M., Cardoso, M. J.,\nCheplygina, V., Cimini, B. A., Collins, G. S., Fara-\nhani, K., Ferrer, L., Galdran, A., van Ginneken, B.,\nHaase, R., Hashimoto, D. A., Hoffman, M. M., Huis-\nman, M., Jannin, P., Kahn, C. E., Kainmueller, D.,\nKainz, B., Karargyris, A., Karthikesalingam, A., Ken-\nngott, H., Kofler, F., Kopp-Schneider, A., Kreshuk,\nA., Kurc, T., Landman, B. A., Litjens, G., Madani, A.,\nMaier-Hein, K., Martel, A. L., Mattson, P., Meijering,\nE., Menze, B., Moons, K. G. M., M\u00a8uller, H., Nichy-\nporuk, B., Nickel, F., Petersen, J., Rajpoot, N., Rieke,\nN., Saez-Rodriguez, J., S\u00b4anchez, C. I., Shetty, S., van\nSmeden, M., Summers, R. M., Taha, A. A., Tiulpin,\nA., Tsaftaris, S. A., Calster, B. V., Varoquaux, G., and\nJ\u00a8ager, P. F. Metrics reloaded: Recommendations for\nimage analysis validation, 2023. 9\n[50] Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu,\nJ.-Y., and Ermon, S. SDEdit: Guided image synthe-\nsis and editing with stochastic differential equations,\n2022. 2, 4\n[51] Mokady, R., Hertz, A., Aberman, K., Pritch, Y., and\nCohen-Or, D. Null-text inversion for editing real im-\nages using guided diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 6038\u20136047, 2023. 2, 4\n[52] M\u00a8uller-Franzes, G., Niehues, J. M., Khader, F.,\nArasteh, S. T., Haarburger, C., Kuhl, C., Wang, T.,\nHan, T., Nebelung, S., Kather, J. N., et al.\nDiffu-\nsion probabilistic models beat gans on medical im-\nages. arXiv preprint arXiv:2212.07501, 2022. 1\n[53] Pawlowski, N., Castro, D. C., and Glocker, B. Deep\nstructural causal models for tractable counterfactual\ninference. In Advances in Neural Information Pro-\ncessing Systems, volume 33, pp. 857\u2013869, 2020. 3\n[54] Podell, D., English, Z., Lacey, K., Blattmann, A.,\nDockhorn, T., M\u00a8uller, J., Penna, J., and Rombach, R.\nSDXL: Improving latent diffusion models for high-\nresolution image synthesis, 2023. 6\n[55] Prabhu, V., Yenamandra, S., Chattopadhyay, P., and\nHoffman, J.\nLANCE: Stress-testing visual mod-\nels by generating language-guided counterfactual im-\nages, 2023. 2, 3, 4, 6, 19\n[56] Radford, A., Metz, L., and Chintala, S.\nUnsu-\npervised representation learning with deep convolu-\ntional generative adversarial networks. arXiv preprint\narXiv:1511.06434, 2015. 2\n[57] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A.,\nGoh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin,\nP., Clark, J., et al. Learning transferable visual mod-\nels from natural language supervision.\nIn Interna-\ntional conference on machine learning, pp. 8748\u2013\n8763. PMLR, 2021. 2, 6\n[58] Razavi, A., Van den Oord, A., and Vinyals, O. Gener-\nating diverse high-fidelity images with vq-vae-2. Ad-\nvances in neural information processing systems, 32,\n2019. 6\n[59] Reinhold, J. C., Carass, A., and Prince, J. L.\nA\nstructural causal model for MR images of multiple\nsclerosis.\nIn Medical Image Computing and Com-\nputer Assisted Intervention \u2013 MICCAI 2021, volume\n12905 of LNCS, pp. 782\u2013792, 2021. doi: 10.1007/\n978-3-030-87240-3 75. 3\n[60] Roberts, M., Driggs, D., Thorpe, M., Gilbey, J., Ye-\nung, M., Ursprung, S., Aviles-Rivero, A. I., Etmann,\nC., McCague, C., Beer, L., et al. Common pitfalls\nand recommendations for using machine learning to\ndetect and prognosticate for covid-19 using chest ra-\ndiographs and ct scans. Nature Machine Intelligence,\n3(3):199\u2013217, 2021. 1\n13\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n[61] Rombach, R., Blattmann, A., Lorenz, D., Esser, P.,\nand Ommer, B. High-resolution image synthesis with\nlatent diffusion models, 2022. 1, 2, 6, 15\n[62] Ronneberger, O., Fischer, P., and Brox, T.\nU-\nnet: Convolutional networks for biomedical image\nsegmentation.\nIn Medical Image Computing and\nComputer-Assisted Intervention\u2013MICCAI 2015: 18th\nInternational Conference, Munich, Germany, Octo-\nber 5-9, 2015, Proceedings, Part III 18, pp. 234\u2013241.\nSpringer, 2015. 9\n[63] Rueckel, J., Trappmann, L., Schachtner, B., Wesp,\nP., Hoppe, B. F., Fink, N., Ricke, J., Dinkel, J.,\nIngrisch, M., and Sabel, B. O.\nImpact of con-\nfounding thoracic tubes and pleural dehiscence ex-\ntent on artificial intelligence pneumothorax detection\nin chest radiographs.\nInvestigative Radiology, 55\n(12):792\u2013798, July 2020.\nISSN 0020-9996.\ndoi:\n10.1097/rli.0000000000000707. 2\n[64] Rueckel, J., Huemmer, C., Fieselmann, A., Ghesu,\nF.-C., Mansoor, A., Schachtner, B., Wesp, P., Trapp-\nmann, L., Munawwar, B., Ricke, J., Ingrisch, M., and\nSabel, B. O. Pneumothorax detection in chest radio-\ngraphs: optimizing artificial intelligence system for\naccuracy and confounding bias reduction using in-\nimage annotations in algorithm training. European\nRadiology, 31(10):7888\u20137900, 2021. doi: 10.1007/\ns00330-021-07833-w. 1, 6, 7, 8\n[65] Saharia, C., Chan, W., Chang, H., Lee, C. A., Ho, J.,\nSalimans, T., Fleet, D. J., and Norouzi, M. Palette:\nImage-to-image diffusion models, 2022. 2\n[66] Sakaridis, C., Dai, D., and Van Gool, L. Semantic\nfoggy scene understanding with synthetic data. Inter-\nnational Journal of Computer Vision, 126:973\u2013992,\n2018. 2\n[67] Sanchez, P., Kascenas, A., Liu, X., O\u2019Neil, A. Q., and\nTsaftaris, S. A. What is healthy? generative counter-\nfactual diffusion for lesion localization. In MICCAI\nWorkshop on Deep Generative Models, pp. 34\u201344.\nSpringer, 2022. 3\n[68] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam,\nR., Parikh, D., and Batra, D. Grad-cam: Visual ex-\nplanations from deep networks via gradient-based lo-\ncalization. In Proceedings of the IEEE international\nconference on computer vision, pp. 618\u2013626, 2017. 6,\n10\n[69] Shen, Y., Gu, J., Tang, X., and Zhou, B. Interpret-\ning the latent space of gans for semantic face editing.\nIn Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pp. 9243\u20139252,\n2020. 2\n[70] Shiraishi, J., Katsuragawa, S., Ikezoe, J., Matsumoto,\nT., Kobayashi, T., Komatsu, K.-i., Matsui, M., Fu-\njita, H., Kodera, Y., and Doi, K. Development of a\ndigital image database for chest radiographs with and\nwithout a lung nodule: receiver operating character-\nistic analysis of radiologists\u2019 detection of pulmonary\nnodules. American Journal of Roentgenology, 174(1):\n71\u201374, 2000. 8\n[71] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep\ninside convolutional networks:\nVisualising image\nclassification models and saliency maps.\narXiv\npreprint arXiv:1312.6034, 2013. 6, 10\n[72] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N.,\nand Ganguli, S.\nDeep unsupervised learning us-\ning nonequilibrium thermodynamics.\nIn Interna-\ntional conference on machine learning, pp. 2256\u2013\n2265. PMLR, 2015. 3\n[73] Song, J., Meng, C., and Ermon, S. Denoising diffu-\nsion implicit models, 2022. 3, 4\n[74] Su, X., Song, J., Meng, C., and Ermon, S. Dual diffu-\nsion implicit bridges for image-to-image translation,\n2023. 17\n[75] Tan, M. and Le, Q. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In Inter-\nnational conference on machine learning, pp. 6105\u2013\n6114. PMLR, 2019. 9, 16\n[76] Tumanyan, N., Geyer, M., Bagon, S., and Dekel,\nT.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pp. 1921\u20131930, 2023. 2\n[77] Upchurch, P., Gardner, J., Pleiss, G., Pless, R.,\nSnavely, N., Bala, K., and Weinberger, K. Deep fea-\nture interpolation for image content changes. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 7064\u20137073, 2017. 2\n[78] Van Breugel, B., Seedat, N., Imrie, F., and van der\nSchaar, M. Can you rely on your model evaluation?\nimproving model evaluation with synthetic test data,\n2023. 2, 3\n[79] Vay\u00b4a, M. d. l. I., Saborit, J. M., Montell, J. A., Pertusa,\nA., Bustos, A., Cazorla, M., Galant, J., Barber, X.,\nOrozco-Beltr\u00b4an, D., Garc\u00b4\u0131a-Garc\u00b4\u0131a, F., Caparr\u00b4os, M.,\nGonz\u00b4alez, G., and Salinas, J. M. BIMCV COVID-\n19+: a large annotated dataset of RX and CT images\nfrom COVID-19 patients, 2020. version: 3. 6, 7\n[80] von Borzyskowski, I., Mazumder, A., Mateen, B., and\nWooldridge, M.\nData science and ai in the age of\ncovid-19, 2021. 1\n14\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n[81] Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., and\nSummers, R. M. ChestX-ray8: Hospital-scale chest\nx-ray database and benchmarks on weakly-supervised\nclassification and localization of common thorax dis-\neases. In 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 3462\u20133471,\n2017. doi: 10.1109/CVPR.2017.369. 6\n[82] Wynants, L., Van Calster, B., Collins, G. S., Riley,\nR. D., Heinze, G., Schuit, E., Albu, E., Arshi, B., Bel-\nlou, V., Bonten, M. M., et al. Prediction models for\ndiagnosis and prognosis of covid-19: systematic re-\nview and critical appraisal. bmj, 369, 2020. 1\n[83] Zawacki, A., Wu, C., Shih, G., Elliott, J., Fomitchev,\nM., Hussain, M. P., Culliton, P., and Bao, S. Siim-acr\npneumothorax segmentation, 2019. 8\nA. Experimental details for Section 5.1:\ndiffusion model\nIn this section, we provide additional details on how the\ndiffusion model used for all experiments in Section 5 was\ntrained. The VAE downsamples the input images by a fac-\ntor of eight, meaning that the latent space has spatial di-\nmensions 64 \u00d7 64. For the diffusion model, we use the\nlinear beta schedule and \u03f5-prediction proposed by Ho et al.\n[29]. The U-Net architecture is as used by Rombach et al.\n[61], which we instantiate with base channels 128, chan-\nnel multipliers (1, 2, 4, 6, 8), and self-attention at feature\nresolutions 32 \u00d7 32 and below, with each attention head\nbeing 32-dimensions. The BioViL-T text encoder [5] has\na maximum token length of 128, so sentences within the\nimpression are shuffled and then clipped to this length. An\nexponential moving average is used on model parameters,\nwith a decay factor of 0.999. Conditioning dropout is used\nduring training to allow CFG when sampling [28]. Train-\ning was performed using 48 V100 GPUs for 300 epochs\nusing automatic mixed precision. The AdamW [48] opti-\nmiser was used, with a fixed learning rate of 10\u22124.\nThe preprocessing steps are:\n1. resize such that the short side of the image has size\n512, using bilinear interpolation;\n2. centre-crop to 512 \u00d7 512 pixels;\n3. map minimum and maximum intensity values to\n[\u22121, 1].\nWe use the following label categories for the CheXpert\ndataset:\n1. Atelectasis\n2. Cardiomegaly\n3. Consolidation\n4. Oedema\n5. Enlarged\ncardiomediastinum\n6. Fracture\n7. Lung lesion\n8. Lung opacity\n9. No finding\n10. Pleural effusion\n11. Pleural other\n12. Pneumonia\n13. Pneumothorax\n14. Support devices\nFor ChestX-ray8, we use:\n1. Atelectasis\n2. Cardiomegaly\n3. Consolidation\n4. Oedema\n5. Effusion\n6. Emphysema\n7. Fibrosis\n8. Hernia\n9. Infiltration\n10. Mass\n11. No Finding\n12. Nodule\n13. Pleural thickening\n14. Pneumonia\n15. Pneumothorax\n15\nRadEdit: stress-testing biomedical vision models via diffusion image editing\nB. Experimental details for Section 5.2:\nacquisition shift\nThe datasets used and their respective train/validation/test\nsplits are as follows:\n1. BIMCV+: 3008/344/384\n2. BIMCV-: 1721/193/never used for testing\n3. MIMIC-CXR: 5000/500/500 (randomly sampled)\n4. Synthetic: never used for training or validation/2774\n(after filtering)\nAll splits were made ensuring non-overlapping subject IDs.\nThe filtering of the synthetic test dataset was done using\nthe prompts: \u2018Opacities\u2019 and \u2018No acute cardiopulmonary\nprocess\u2019.\nFor training, we converted the original labels of the\nBIMCV datasets as follows: if an image has the label \u2018Neg-\native for Pneumonia\u2019 or \u2018Atypical Appearance\u2019 we assign\nlabel 0; while if it has the label \u2018Typical Appearance\u2019 or\n\u2018Indeterminate Appearance\u2019 we assign label 1.\nThe classifier is trained using a ResNet50 architecture with\nbatch size 32, 100 epochs and learning rate 10\u22125. The\nmodel was evaluated at the point of best validation area un-\nder the receiver operating characteristic curve (AUROC).\nThe preprocessing steps are:\n1. Resize such that the short side of the image has size\n512, using bilinear interpolation;\n2. centre-crop to 512 \u00d7 512 pixel;\n3. map minimum and maximum intensity values to [0, 1].\nThe following augmentations were used:\n1. Random horizontal flip with probability 0.5\n2. Random affine transformations with rotation \u03b8\n\u223c\nU(\u221230, 30) degrees and shear \u03d5 \u223c U(\u221215, 15) de-\ngrees\n3. Random colour jittering with brightness jb\n\u223c\nU(0.8, 1.2) and contrast jc \u223c U(0.8, 1.2)\n4. Random cropping with scale s \u223c U(0.8, 1)\n5. Addition of Gaussian noise with zero mean and stan-\ndard deviation \u03c3 = 0.05\nC. Experimental details for Section 5.3:\nmanifestation shift\nThe datasets used and their respective train/validation/test\nsplits are as follows:\n1. CANDID-PTX: 13 836/1539/1865\n2. SIIM-ACR: 10 712/1625/never used for testing\n3. Synthetic: never used for training or validation/629\n(after filtering)\nAll splits were made ensuring non-overlapping subject IDs.\nThe filtering of the synthetic test dataset was done using the\nprompts: \u2018Pneumothorax\u2019 and \u2018No acute cardiopulmonary\nprocess\u2019.\nAfter observing that the contours of the pneumothorax and\nchest drain masks often do not include the borders of the\npneumothorax or chest drain we apply isotropic dilation\nwith a radius of 5. Examples of such dilated masks can\nbe seen in Figure 11 (a).\nFor the \u2018Biased\u2019 classifier the same model architecture,\ntraining hyperparameters and data augmentation as de-\nscribed in Appendix B\nIn the case of the \u2018Unbiased\u2019 model, a segmentation model\nis trained using the EfficientNet U-Net [75] architecture.\nWe add a single classification layer to the lowest resolution\nof the U-Net. The segmentation model is trained to seg-\nment pneumothorax, and the classifier is used to detect the\npresence of pneumothorax.\nThe combined model is trained for 100 epochs with batch\nsize 16, learning rate 5 \u00d7 10\u22124 , and a cosine scheduler with\nwarm-up during the first 6% of steps. The model was eval-\nuated at the point of best validation AUROC for the pneu-\nmothorax classifier.\nData preprocessing and augmentation were as described in\nAppendix B, with s \u223c U(0.9, 1.1). Additionally, a ran-\ndom elastic transform with scale 0.15 (as implemented in\nAlbumentations [7]) was used.\nD. Experimental details for Section 5.4:\npopulation shift\nThe datasets used and their respective train/validation/test\nsplits are as follows:\n1. MIMIC-Seg: 911/114/115\n2. CheXmask: 169206/36580/36407\n3. Synthetic Oedema: never used for training or valida-\ntion/787 (after filtering)\n4. Synthetic Pacemaker: never used for training or vali-\ndation/744 (after filtering)\n5. Synthetic Consolidation: never used for training or\nvalidation/1577 (after filtering)\nAll splits were made ensuring non-overlapping subject IDs.\nThe same segmentation model architecture, training hyper-\nparameters, and data augmentation/preprocessing steps are\nused as described above in Appendix C.\n16\nRadEdit: stress-testing biomedical vision models via diffusion image editing\nIn Figures 7 to 9 we show more examples of edits produced\nby RadEdit to stress test the segmentation models. Here it\ncan be observed that RadEdit edits are high-quality, with\nboth general anatomy maintained after the edit, as well as\nimage markings.\nE. Artefacts of zero-shot editing without\nmasks\nDuring the development of RadEdit, we encountered nu-\nmerous artefacts when editing images from the BIMCV+\nor CANDID-PTX dataset without using masks.\nIn Fig-\nure 10, we compare RadEdit with LANCE when editing\nimages from the BIMCV+ dataset.\nWhile RadEdit pre-\nserves the laterality markers in the top left corner of the\nimage, LANCE either alters the laterality markers or com-\npletely removes them. In both cases, we use the prompt\n\u2018No acute cardiopulmonary process\u2019 to edit the image.\nFurthermore, in Figure 11, we compare RadEdit with\nLANCE when editing images from the CANDID-PTX\ndataset using the prompt \u2018No pneumothorax\u2019. In contrast to\nFigure 2 (c), where we use the \u2018No acute cardiopulmonary\nprocess\u2019, LANCE does seem to preserve the chest drains.\nHowever, in Figure 11, we find that LANCE produces a\nvariety of artefacts as explained is the caption of Figure 11.\nBoth artefacts can be explained by recent advances in us-\ning diffusion models for image-to-image translation. In Su\net al. [74], the authors show that image-to-image transla-\ntion can be performed with two independently trained dif-\nfusion models. They first obtain a latent representation \u02c6xt\nfrom the source images with the source diffusion model,\nand then decode the latent using the target model to con-\nstruct the target image.\nWe argue that since the diffu-\nsion model in Section 5.1 was not trained on data from\nBIMCV+ or CANDID-PTX in those cases we perform\nimage-to-image translation along with the image editing.\nI.e., editing images outside of the training distribution of\nthe diffusion model leads to images that look more simi-\nlar to images from within the training distribution. In the\ncase of RadEdit, where we heavily rely on masks to control\nthe editing, we only observe minor artefacts. However, in\nthe case of LANCE we observe major artefacts that make\nLANCE unsuitable for stress testing of biomedical imag-\ning models. We tried varying the hyperparameters (e.g.,\nthe guidance scale) of LANCE but it did not remove the\nartefacts.\n(a) Original\n(b) Edited\n(c) Weak\nPredictor\n(d) Strong\nPredictor\nFigure 7: Additional edits simulated by RadEdit for stress-\ntesting two segmentation models, the \u2018weak predictor\u2019 is\ntrained on MIMIC-Seg [11] (c) and the \u2018strong predic-\ntor\u2019 on CheXmask [18] (d) respectively, by adding pul-\nmonary oedema, which appears as an increased opacity in\nthe airspace, via the prompt \u2018Moderate pulmonary oedema.\nThe heart size is normal.\u2019 Ground truth mask: blue; pre-\ndicted: red. Similar to the example in Figure 6, both seg-\nmentation models predict relatively accurate segmentation\nmaps, indicating a high level of robustness this pathology.\nEdits are visually high quality, with anatomy well main-\ntained, and the oedema clearly identifiable.\n17\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n(a) Original\n(b) Edited\n(c) Weak\nPredictor\n(d) Strong\nPredictor\nFigure 8: Additional edits simulated by RadEdit for stress-\ntesting two segmentation models, the \u2018weak predictor\u2019 is\ntrained on MIMIC-Seg [11] (c) and the \u2018strong predictor\u2019\non CheXmask [18] (d) respectively, by adding pacemak-\ners, which can be seen in the top left of images, via the\nprompt \u2018Left pectoral pacemaker in place. The position\nof the leads is as expected. Otherwise unremarkable chest\nradiographic examination.\u2019 Ground truth mask: blue; pre-\ndicted: red. Similar to the example in Figure 6, the segmen-\ntation model trained on MIMIC-Seg (which contains pre-\ndominantly healthy patients) incorrectly segments around\nthe pacemakers, while the model trained on CheXmask\n(which is larger and contains more abnormal cases), seg-\nments more accurately.\n(a) Original\n(b) Edited\n(c) Weak\nPredictor\n(d) Strong\nPredictor\nFigure 9: Additional edits simulated by RadEdit for stress-\ntesting two segmentation models, the \u2018weak predictor\u2019 is\ntrained on MIMIC-Seg [11] (c) and the \u2018strong predictor\u2019\non CheXmask [18] (d) respectively, by adding upper-lobe\nconsolidation, which can be seen as white regions in the\nupper parts of the lungs, via the prompt \u2018New [left/right]\nupper lobe consolidation.\u2019 Ground truth mask: blue; pre-\ndicted: red. Similar to the example in Figure 6, both mod-\nels are less able to segment the lungs accurately, however,\nsegmentations by the model trained on MIMIC-Seg are no-\ntably worse, often segmenting around the consolidated re-\ngion.\n18\nRadEdit: stress-testing biomedical vision models via diffusion image editing\n(a) Original Image\n(b) RadEdit (ours)\n(c) LANCE2 [55]\nFigure 10: Using LANCE (c) to remove COVID-19, not\nonly are the laterality markers missing but the contrast is\ndecreased. In addition, a small part of a laterality marker\nwas added in the upper right corner. In contrast, RadEdit\n(b; ours) uses masks to preserve laterality markers, which\nalso preserves anatomical structures in the process, and re-\ntains the original contrast.\n(a) Original Image\n(b) RadEdit (ours)\n(c) LANCE2 [55]\nFigure 11: Removing pneumothorax from X-rays using\nRadEdit (b; ours) results in a minimally modified X-ray,\nwith the pneumothorax successfully removed and chest\ndrain still present. In contrast, LANCE (c) while keeping\nmost of the chest drain in place, fails to properly remove the\npneumothorax, instead modifying the appearance to look\nmore like a wire; moreover, there are extensive artefacts\nbilaterally, with abdomen, face, and arms added, modified\ngas pattern and heart, as well as the lung apexes no longer\nbeing asymmetrical, making it unclear whether the X-rays\nare of the same patient.\n19\n"
  }
]