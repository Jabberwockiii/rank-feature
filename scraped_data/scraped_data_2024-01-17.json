[
  {
    "title": "InstantID: Zero-shot Identity-Preserving Generation in Seconds",
    "link": "https://arxiv.org/pdf/2401.07519.pdf",
    "upvote": "48",
    "text": ": Zero-shot Identity-Preserving\nGeneration in Seconds\nQixun Wang12, Xu Bai12, Haofan Wang12\u22c6, Zekui Qin12, Anthony Chen123,\nHuaxia Li2, Xu Tang2, and Yao Hu2\nInstantX Team1, Xiaohongshu Inc2, Peking University3\n{haofanwang.ai@gmail.com}\nhttps://instantid.github.io\nAbstract. There has been significant progress in personalized image\nsynthesis with methods such as Textual Inversion, DreamBooth, and\nLoRA. Yet, their real-world applicability is hindered by high storage de-\nmands, lengthy fine-tuning processes, and the need for multiple reference\nimages. Conversely, existing ID embedding-based methods, while requir-\ning only a single forward inference, face challenges: they either necessitate\nextensive fine-tuning across numerous model parameters, lack compati-\nbility with community pre-trained models, or fail to maintain high face\nfidelity. Addressing these limitations, we introduce InstantID, a pow-\nerful diffusion model-based solution. Our plug-and-play module adeptly\nhandles image personalization in various styles using just a single facial\nimage, while ensuring high fidelity. To achieve this, we design a novel\nIdentityNet by imposing strong semantic and weak spatial conditions,\nintegrating facial and landmark images with textual prompts to steer the\nimage generation. InstantID demonstrates exceptional performance and\nefficiency, proving highly beneficial in real-world applications where iden-\ntity preservation is paramount. Moreover, our work seamlessly integrates\nwith popular pre-trained text-to-image diffusion models like SD1.5 and\nSDXL, serving as an adaptable plugin. Our codes and pre-trained check-\npoints will be available at https://github.com/InstantID/InstantID.\nKeywords: Image Synthesis, Image Customization, ID Preservation\n1\nIntroduction\nImage generation technology has seen remarkable advancements with the emer-\ngence of large text-to-image diffusion models such as GLIDE[12], DALL\u00b7E 2[15],\nImagen[19], and Stable Diffusion (SD)[16]. A focal point of these developments\nis personalized and customized generation, which seeks to create images consis-\ntent in style, subject, or character ID based on one or more reference images.\nThis approach has garnered significant interest for its broad potential in appli-\ncations like E-commerce advertising, AI portraits, image animation, and virtual\n\u22c6 Corresponding author\narXiv:2401.07519v2  [cs.CV]  2 Feb 2024\n2\nWang et al.\nFig. 1. The awesome team of InstantID in author order, generated with InstantID.\ntry-ons. However, a notable challenge remains: generating customized images\nthat accurately preserve the intricate identity details of human subjects. This\ntask is particularly demanding, as human facial identity (ID) involves more nu-\nanced semantics and requires a higher standard of detail and fidelity compared\nto general styles or objects, which primarily focus on coarse-grained textures\nand colors. Existing vanilla text-to-image models, dependent on detailed textual\ndescriptions, fall short in achieving strong semantic relevance in customized gen-\neration. To enhance controllability, recent works such as ControlNet[25] and T2I-\nadapter[11] have been introduced. These advancements add spatial conditioning\ncontrols to large pre-trained text-to-image models, facilitating fine-grained struc-\ntural control through elements like user-drawn sketches, depth maps, body poses,\nand semantic segmentation maps. Additionally, adaptations for style or content\nreferencing, such as the T2I-adapter\u2019s style adapter and Uni-ControlNet\u2019s[26]\nglobal controller, have made strides in this area. Despite these advancements,\nthe fidelity of the generated images to the reference remains only partial.\nTo enhance image generation with reference images, current personalized\ngeneration methods can be classified into two types, based on the need for\nfine-tuning during testing. The first type includes methods necessitating fine-\ntuning, with leading examples like DreamBooth[18], Textual Inversion[4], and\nLow-Rank Adaptation[6] (LoRA). These methods aim to fine-tune pre-trained\ntext-to-image models to better reflect new aspects in reference images. While\neffective in achieving high accuracy, the fine-tuning process is resource-intensive\nand time-consuming, limiting their practicality. Additionally, they often require\nmultiple references in various settings and struggle in limited-data scenarios,\nsuch as with identical character IDs.\nInstantID: Zero-shot Identity-Preserving Generation in Seconds\n3\nThe second type bypasses fine-tuning during inference. It involves construct-\ning a large amount of domain-specific data and building a lightweight adapter\nfor feature extraction from the reference image. This adapter then integrates\nthese features into the diffusion generation process using cross-attention. IP-\nAdapter[24] is one of the representative works, which employs a unique cross-\nattention mechanism to separate text and image features, allowing for the injec-\ntion of the reference image as a visual prompt. However, this approach, which pri-\nmarily relies on CLIP[14]\u2019s image encoder, tends to produce only weakly aligned\nsignals, falling short in creating high-fidelity, customized images.\nIn light of these constraints, we introduce a novel approach (InstantID)\nfocusing on instant identity-preserving image synthesis. In this study, we bridge\nthe gap between high fidelity and efficiency by introducing a simple plug-and-\nplay module, enabling it to adeptly handle image personalization in any style\nusing only one facial image while maintaining high fidelity. To preserve the facial\nidentity from the reference image, we design a novel face encoder to retain the\nintricate details by adding strong semantic and weak spatial conditions, which in-\ncorporate facial image, landmark image and textual prompts to guide the image\ngeneration process. We distinguish our work from previous ones in the follow-\ning aspects: (1) Plugability and compatibility: we focus on training a lightweight\nadapter instead of the full parameters of UNet[17], making our module pluggable\nand compatible with pre-trained models in the community; (2) Tuning-free: our\nmethod requires only a single forward propagation for inference, eliminating the\nneed for fine-tuning. This feature makes InstantID highly economical and practi-\ncal for real-world applications; (3) Superior performance: with just one reference\nimage, InstantID achieves state-of-the-art results, demonstrating high fidelity\nand flexibility. Remarkably, it can match or even surpass the performance of\ntraining-based methods like LoRAs, which rely on multiple reference images.\nIn summary, our contributions are threefold:\n\u2022 We present InstantID, an innovative ID-preserving adaptation method for\npre-trained text-to-image diffusion models to decently bridge the gap between\nfidelity and efficiency. Experimental results demonstrate the excellent perfor-\nmance of our proposed method as compared with other state-of-the-art methods\nin this domain.\n\u2022 InstantID is pluggable and compatible with other custom models fine-tuned\nfrom the same base diffusion model, enabling ID preservation in pre-trained\nmodels at no additional cost. Furthermore, InstantID maintains considerable\ncontrol over text editing as observed in the original Stable Diffusion model,\nenabling the smooth integration of IDs into various styles.\n\u2022 The excellent performance and efficiency of InstantID ignite its huge po-\ntential for a range of real-world applications, such as novel view synthesis, ID\ninterpolation, multi-ID and multi-style synthesis.\n4\nWang et al.\n2\nRelated Work\n2.1\nText-to-image Diffusion Models\nText-to-image diffusion models [15,19,16,12,1,7] achieve state-of-the-art image\ngeneration results and have received unprecedented interest from the community\nin recent years. A common practice is to encode the text prompt into latent\nthrough a pre-trained language encoder like CLIP [14] and use the latent to guide\nthe diffusion process. For instance, GLIDE [12] adopts a cascaded text-guided\ndiffusion architecture to support both image generation and editing. DisCo [21]\nuses the text encoder from CLIP [14] to process text inputs and introduces CLIP\nguidance in the diffusion process. Stable Diffusion [16] is one of the highlighted\nworks of latent diffusion models (LDM) where the diffusion process is performed\nin the latent image space instead of the original pixel space, which significantly\nreduces the computation cost. As a follow-up, Stable Diffusion XL [13] (SDXL)\nis a powerful text-to-image generation model that introduced a larger UNet and\na second text encoder to gain stronger textual control over generated images.\n2.2\nSubject-driven Image Generation\nSubject-driven text-to-image generation, which uses a limited set of images\nof a particular subject to generate customized images based on text descrip-\ntions, has seen notable advancements. Previous subject-driven approaches like\nDreamBooth [18], Textual Inversion [4], ELITE [22], E4T [5], and ProFusion\n[28] fine-tune a special prompt token S\u2217 to describe the target concepts dur-\ning the fine-tuning process. In contrast, other recent methods strive to enable\nsubject-driven text-to-image generation without the need for additional fine-\ntuning. These methods typically involve training additional modules while keep-\ning the core pre-trained text-to-image models frozen. A leading example of this\napproach is IP-Adapter [24], which aims to decouple the cross-attention mech-\nanism by separating the cross-attention layers for text features and image fea-\ntures. A concurrent work, Anydoor [2], complements the commonly used identity\nfeature with detail features that are designed to maintain texture details while\nallowing versatile local variations.\n2.3\nID Preserving Image Generation\nID-preserving image generation is a special case of subject-driven generation,\nbut it focuses on face attributes with strong semantics and finds broad appli-\ncation in real-world scenarios. Existing works can be mainly divided into two\ncategories based on their reliance on test-time fine-tuning. Low-Rank Adapta-\ntion [6] (LoRA) is a popular lightweight training technique that inserts a minimal\nnumber of new weights into the model before training on customized datasets.\nHowever, LoRA necessitates individual training for each new character, limiting\nits flexibility. In contrast, recent developments have introduced optimization-\nfree methods, bypassing additional fine-tuning or inversion processes. Face0 [20]\nInstantID: Zero-shot Identity-Preserving Generation in Seconds\n5\noverrides the last three text tokens with the projected face embedding in CLIP\nspace and uses the joint embedding as a condition to guide the diffusion pro-\ncess. PhotoMaker [10] adopts a similar approach, but enhances its ability to\nextract ID-centric embeddings by fine-tuning part of the Transformer [3] layers\nin the image encoder and merging the class and image embeddings. FaceStudio\n[23] presents a hybrid-guidance identity-preserving image synthesis framework,\nwhere the face embedding is integrated into both CLIP vision embeddings and\nCLIP text embeddings via linear projection, then the merged guidance embed-\nding is fused into the UNet with cross-attentions. IP-Adapter-FaceID [24] uses\nface ID embedding from a face recognition model instead of CLIP image em-\nbedding to retain ID consistency. Nevertheless, these methods either necessitate\ntraining the full parameters of UNet, sacrificing compatibility with existing pre-\ntrained community models, or fall short in ensuring high face fidelity. To ad-\ndress these limitations, we introduce a pluggable module that adeptly bridges\nthe divide between training-free and training-intensive methods. Our method,\nrequiring no fine-tuning during inference, seamlessly aligns with readily available\npre-trained diffusion models like SD1.5 and SDXL, achieving exceptional fidelity\nin face preservation.\n3\nMethods\nInstantID is an efficient lightweight adapter, effortlessly endowing pre-trained\ntext-to-image diffusion models with the capability of ID preservation, akin to a\n\u2019free lunch\u2019. In this part, we first discuss preliminaries about the text-to-image\ndiffusion model and custom image generation in Section 3.1, and then introduce\nthe detailed design of our InstantID in Section 3.2. Finally, the specifics of our\ntraining and inference process are detailed in Section 3.3.\n3.1\nPreliminaries\nStable Diffusion. Our method is built on top of Stable Diffusion [16], which\nperforms the diffusion process computationally efficiently in low-dimensional la-\ntent space instead of pixel space with an auto-encoder [8]. Specifically, given an\ninput image xi \u2208 RH\u00d7W \u00d73, the encoder first maps it to a latent representation:\nz0 = \u03be(xi), where z0 \u2208 Rh\u00d7w\u00d7c, f = H/h = W/w is the downsampling factor\nand c is the latent dimension. The diffusion process adopts a denoising UNet [17]\n\u03f5\u03b8 to denoise a normally-distributed noise \u03f5 with noisy latent zt, current timestep\nt and condition C, where C represents the embedding of textual prompts gen-\nerated via a pre-trained CLIP [14] text encoder. The overall training objective\nis defined as\nL = Ezt,t,C,\u03f5\u223cN (0,1)[||\u03f5 \u2212 \u03f5\u03b8(zt, t, C)||2\n2].\n(1)\nControllable Diffusion Models. We utilize ControlNet[25] as an exemplar,\nwhich is capable of adding spatial control to a pre-trained diffusion model as con-\nditions, extending beyond the capabilities of basic textual prompts. ControlNet\n6\nWang et al.\nintegrates the UNet architecture from Stable Diffusion with a trainable replica\nof UNet. This replica features zero convolution layers within the encoder blocks\nand the middle block. The full process of ControlNet is executed as follows,\nyc = F(x, \u03b8) + Z(F(x + Z(c, \u03b8z1), \u03b8c), \u03b8z2).\n(2)\nControlNet distinguishes itself from the original Stable Diffusion in the lat-\nter residual item, where F is the UNet architecture, x is the latent, \u03b8 is the\nfrozen weight of the pre-trained model, Z corresponds to zero convolutions with\nweights \u03b8z1, \u03b8z2, and \u03b8c is the trainable weight of ControlNet. To put it simply,\nControlNet encodes spatial condition information (sketches, poses, depth maps,\netc.) by adding residuals to UNet Block and then embeds it into the original\nnetwork.\nImage Prompt Adapter. IP-Adapter[24] introduces a novel approach to achiev-\ning image prompt capabilities in parallel with text prompts without modifying\nthe original text-to-image models. It employs a unique decoupled cross-attention\nstrategy, embedding image features through additional cross-attention layers\nwhile leaving other parameters unchanged. The decoupled cross-attention can\nbe illustrated as\nZnew = Attention(Q, Kt, V t) + \u03bb \u00b7 Attention(Q, Ki, V i),\n(3)\nwhere Q, Kt, V t are the query, key, and values matrices of the attention\noperation for text cross-attention, Ki and V i are for image cross-attention. Given\nthe query features Z and the image features ci, Q = ZWq Ki = ciW i\nk, V i =\nciW i\nv. Note that only W i\nk and W i\nk are trainable weights.\n3.2\nMethodology\nOverview. Given only one reference ID image, InstantID aims to generate cus-\ntomized images with various poses or styles from a single reference ID image\nwhile ensuring high fidelity. Figure 2 provides an overview of our method. It in-\ncorporates three crucial components: (1) An ID embedding that captures robust\nsemantic face information; (2) A lightweight adapted module with decoupled\ncross-attention, facilitating the use of an image as a visual prompt; (3) An Iden-\ntityNet that encodes the detailed features from the reference facial image with\nadditional spatial control.\nID Embedding. Contrary to prior approaches like IP-Adapter [24], FaceStudio\n[23], and PhotoMaker [10], which rely on a pre-trained CLIP image encoder for\nvisual prompt extraction, our work targets the need for stronger semantic details\nand enhanced fidelity in the ID preservation task. CLIP\u2019s inherent limitation lies\nin its training on weakly aligned data, which means its encoded features predom-\ninantly capture broad, ambiguous semantic information like composition, style,\nand colors. Such features can act as general supplements to text embeddings, but\nInstantID: Zero-shot Identity-Preserving Generation in Seconds\n7\nFig. 2. The overall pipeline of our proposed InstantID. Our model consists of\nthree parts to maintain high facial fidelity. First, we adopt a face encoder instead of\nCLIP to extract semantic face features and use a trainable projection layer to project\nthem to the space of text features. We take the projected feature as face embedding.\nThen, a lightweight adaptive module with decoupled cross-attention is introduced to\nsupport images as prompts. Finally, we propose IdentityNet to encode complex features\nin reference facial images with additional weak spatial control. In IdentityNet, the\ngeneration process is fully guided by face embedding without any textual information.\nOnly newly added modules are updated, while the pre-trained text-to-image model\nremains frozen to ensure flexibility. After training, users can generate ID-preserving\nimages of any style in high fidelity for free.\nthey fall short for tasks requiring precise ID preservation, where stronger seman-\ntics and higher fidelity are paramount. Over recent years, extensive research in\nface representation, particularly in face recognition and ReID, has demonstrated\nthe efficacy of face representation in complex tasks like face recognition and re-\nconstruction. However, its direct application in image generation with diffusion\nmodels is underexplored. In our research, we leverage a pre-trained face model\nto detect and extract face ID embedding from the reference facial image, provid-\ning us with strong identity features to guide the image generation process. This\nleads us to a critical question: how do we effectively inject the identity\nfeatures into the diffusion models? We explore and answer this question in\nthe subsequent sections of our work.\n8\nWang et al.\nImage Adapter. The capability of image prompting in pre-trained text-to-\nimage diffusion models significantly enhances textual prompts, particularly for\ncontent that is challenging to describe with text. In our approach, we adopt\na strategy similar to IP-Adapter for image prompting, as shown in Figure 2\nupper branch, a lightweight adaptive module with decoupled cross-attention is\nintroduced to support images as prompts. However, we diverge by employing\nID embedding as our image prompt, as opposed to the coarse-aligned CLIP\nembedding. This choice is aimed at achieving a more nuanced and semantically\nrich prompt integration.\nIdentityNet. Despite the availability of methods to integrate image prompts\nwith text prompts (e.g. IP-Adapter), we argue that these techniques offer only\ncoarse-grained improvements, and such a level of integration is insufficient for ID-\npreserving image generation. We attribute this limitation to the inherent training\nmechanism and properties of the pre-trained diffusion model. For instance, when\nimage and text prompts are concatenated before the attention layers, the model\nstruggles with fine-grained control over extended token sequences. Yet, directly\nadding the text and image tokens in cross-attention tends to weaken the control\nexerted by text tokens. Moreover, attempts to enhance the strength of image\ntokens for improved fidelity can inadvertently impair the editing abilities of text\ntokens. This presents a significant challenge in our work, which we address by\nadopting an alternative feature embedding method, ControlNet. This method\ntypically utilizes spatial information as input for the controllable module, main-\ntaining consistency with UNet settings in the diffusion model and including text\nas a conditional element in cross-attention layers.\nIn our adaptation of ControlNet, there are mainly two modifications: 1) In-\nstead of fine-grained OpenPose facial keypoints, we use only five facial keypoints\n(two for the eyes, one for the nose, and two for the mouth) for conditional in-\nput. 2) We eliminate the text prompts and use ID embedding as conditions for\ncross-attention layers in the ControlNet. We discuss our motivation below.\nFirst, we recognize that while spatial control is essential in ControlNet, it\nmust be sensitively applied, especially for facial regions. We opt for facial land-\nmarks as spatial control signals but restrict ourselves to five key points (two\nfor the eyes, one for the nose, and two for the mouth) for a more generalized\nconstraint than detailed key points. This approach serves two purposes: a) In-\nstantID is trained on real-world human images, not manually cropped face im-\nages, where faces are often a minor part of the scene, making precise key point\ndetection challenging. b) We aim to reduce the impact of spatial constraints\nand prevent overemphasis on redundant facial information, like face shape or\nmouth closure, to maintain editability. Nonetheless, we find that without any\nspatial constraints, the human face\u2019s degree of freedom may be excessively large,\ncomplicating the generation of satisfactory results. A map of coarse key points,\ntherefore, offers a balanced solution.\nSecond, the reason why we eliminate the text prompts and use ID embedding\nas condition for cross-attention layers instead is because this approach enables\nInstantID: Zero-shot Identity-Preserving Generation in Seconds\n9\nthe network to focus exclusively on ID-related representations, thereby not being\ninfluenced by generalized descriptions of faces and backgrounds.\n3.3\nTraining and Inference Strategies\nDuring training, we only optimize the parameters of the Image Adapter and\nthe IdentityNet while keeping the parameters of the pre-trained diffusion model\nfrozen. We train the entire InstantID pipeline on image-text pairs featuring hu-\nman subjects, employing a training objective akin to that used in the original\nstable diffusion work:\nL = Ezt,t,C,Ci,\u03f5\u223cN (0,1)[||\u03f5 \u2212 \u03f5\u03b8(zt, t, C, Ci)||2\n2],\n(4)\nwhere Ci is the task-specific image condition on IdentityNet. Note that in\nthe training process, we do not randomly drop text or image conditions, as we\nhave removed the text prompts condition in IdentityNet.\nA key feature of our work is the separation between text cross-attention and\nimage cross-attention within the image prompt adapter, as well as the decoupling\nof the image prompt adapter and IdentityNet. This design choice allows us to\nindependently and flexibly adjust the weight of these image conditions, ensuring\na more controlled and targeted training and inference process.\n4\nExperiments\nWe implement with Stable Diffusion and train it on the large-scale open-source\ndataset LAION-Face [27], which consists of 50 million image-text pairs to en-\nsure diversity. In addition, we collect 10 million high-quality human images from\nthe Internet with annotations automatically generated by BLIP2 [9] to further\nimprove the generation quality. We focus on single-person images and utilize a\npre-trained face model, antelopev21, for detecting and extracting face ID em-\nbeddings from human images. We train on the original human images instead of\ncropped face datasets. While training, only the parameters of the Image Adapter\nand IdentityNet are updated, while the pre-trained text-to-image model remains\nfrozen. Our experiments are based on the SDXL-1.0 model and are conducted\non 48 NVIDIA H800 GPUs (80GB) with a batch size of 2 per GPU.\n4.1\nQualitative Results\nWe present qualitative results in various settings to demonstrate the robustness,\nprompt editability, and compatibility of our method.\nImage Only. We use an empty prompt to guide the generation process entirely\nby the reference image. Our results without prompts, as seen in Figure 3 \u2018empty\n1 https://github.com/deepinsight/insightface\n10\nWang et al.\nFig. 3. Demonstration of the robustness, editability, and compatibility of\nInstantID. Column 1 shows the result of Image Only results where the prompt is\nset to empty during inference. Columns 2-4 show the editability through text prompt.\nColumns 5-9 show the compatibility with existing ControlNets (canny & depth).\nFig. 4. Effect of the number of reference images. For multiple reference images,\nwe take the average mean of ID embeddings as image prompt. InstantID is able to\nachieve good results even with only one single reference image.\nprompt\u2019, demonstrate InstantID\u2019s ability to robustly maintain rich semantic fa-\ncial content like expression, age, and identity. However, certain semantics, such\nas gender, may not be perfectly accurate (see second row).\nImage + Prompt. In several prompt settings (Figure 3, columns 2-4), we\nobserve no degradation in text control capabilities while ensuring identity con-\nsistency, effectively changing gender, clothes, and hair color.\nImage + Prompt + Spatial Control. Our method is compatible with pre-\ntrained spatial control models like ControlNet. Figure 3, columns 5-9, illustrates\nInstantID: Zero-shot Identity-Preserving Generation in Seconds\n11\nthis compatibility, showing that we can introduce flexible spatial control using\npre-trained ControlNet (canny, depth) models.\n4.2\nAblative Study\nWe assess the effectiveness of each internal module during inference and its\nimpact on the generated results. Figure 9 in Appendix demonstrates that Iden-\ntityNet alone achieves good ID retention, and the addition of the Image Adapter\nfurther enhances facial detail restoration.\nWe also examine the influence of the number of reference images on genera-\ntion quality in Figure 4. More reference images generally improve our generation\nquality, but even with a single image, our method achieves remarkable fidelity.\nNote that in training-based methods, the number of images of the same person\nusually directly affects the quality of generation. For example, LoRAs training\nusually requires dozens of images, and if the image similarity is too high, it will\nbe troubled by over-fitting. Yet in our case, we use the average embedding of all\nreference images, and this further helps improve our generation quality.\n4.3\nComparison to Previous Methods\nWith IP-Adapters. To demonstrate the effectiveness of our method, we com-\npare our work with existing state-of-the-art methods on personalized generation\nwith one single reference image. Specifically, we compare the results of pre-\ntrained IP-Adapter, IP-Adapter-FaceID and IP-Adapter-FaceID-Plus with our\nInstantID. From Figure 5, there are two substantial observations: 1) ID embed-\nding is fundamental to the task of face preservation. Unlike the CLIP embedding\nused in IPA, which offers only a coarse representation of images, ID embedding\ninherently carries rich semantic information, such as identity, age, and gender.\nThis high-level semantic content is crucial for accurate and detailed face preser-\nvation. 2) Introducing ID embedding solely at the cross-attention level, as done in\nIPA-FaceID and IPA-FaceID-Plus, certainly enhances face fidelity. However, this\napproach can inadvertently impact text control. A notable example is the inabil-\nity to seamlessly integrate the facial area with the background style. This limita-\ntion highlights a trade-off between face fidelity and text control. In contrast, our\nInstantID method demonstrates robust handling of different reference images,\nachieving higher fidelity in results. More importantly, it effectively maintains\nthe integrity of the ID, blending it seamlessly into various styles. This compari-\nson underscores InstantID\u2019s superiority in preserving identity while maintaining\nstylistic flexibility and control.\nWith LoRAs. To further demonstrate the superiority of InstantID, we con-\nducted a comparative analysis with LoRA models that were trained using multi-\nple reference images. For this purpose, we selected several pre-trained character\nLoRA models, such as those resembling Jackie Chan2 and Emma Watson3, from\n2 https://civitai.com/models/78839/a-man-who-resembles-jackie-chan\n3 https://civitai.com/models/182550/essenz-emma-watson-celebrity-lora-for-sdxl-10\n12\nWang et al.\nFig. 5. Comparison of InstantID with other methods conditioned on dif-\nferent characters and styles. From left to right are IP-Adapter-SDXL, IP-\nAdapter-SDXL-FaceID (* indicates experimental version), IP-Adapter-SD1.5-FaceID,\nIP-Adapter-SD1.5-FaceID-Plus. As shown in the figure, we found that the IP-Adapter\nthat relies on CLIP embedding cannot achieve facial fidelity, and also leads to the\ndegradation of prompt control to generate styles. IP-Adapter-FaceID introduces face\nembedding, which improves face fidelity, but it still cannot achieve high fidelity. IP-\nAdapter-FaceID-Plus combines the embedding of face and CLIP, which can achieve\ngood facial fidelity, but there is a style degradation problem, resulting in the face be-\ning unable to blend into the background style. In contrast, our proposed InstantID is\ncompatible with various styles while maintaining high fidelity.\nFig. 6. Comparison of InstantID with pre-trained character LoRAs. We can\nachieve competitive results as LoRAs without any training.\nInstantID: Zero-shot Identity-Preserving Generation in Seconds\n13\nCivitai. The comparison yielded a pleasantly surprising result: InstantID is ca-\npable of achieving competitive results using just a single image, even without\nthe necessity of test-time fine-tuning. This finding is significant when contrasted\nwith the LoRA models. LoRA models, on the one hand, have specific require-\nments regarding data quality and quantity, which can limit their applicability in\nreal-world scenarios. On the other hand, they necessitate the additional training\nof customized models. In stark contrast, InstantID enables efficient single-step\ninference with a solitary image, showcasing remarkable efficiency that can greatly\nbenefit various downstream applications. This efficiency, coupled with the lack\nof need for extensive training or multiple reference images, positions InstantID\nas a highly practical and versatile tool in the realm of image generation.\nFig. 7. Comparison of InstantID with InsightFace Swapper. However, in non-\nrealistic style, our work is more flexible on the integration of face and background.\nIn addition, we compare InstantID with InsightFace Swapper4, a widely used\nblack-box face-swapping model with a non-diffusion model structure, known for\nits integration into community projects like ROOP5 and ReActor6. Our compari-\nson, as depicted in Figure 7, reveals that while InsightFace Swapper is effective in\nmost standard scenarios, InstantID demonstrates greater flexibility in blending\nfaces with backgrounds, especially in non-realistic styles like anime. This adapt-\nability showcases the advanced capabilities of InstantID in handling a diverse\nrange of artistic expressions.\n4.4\nApplications\nFurthermore, InstantID\u2019s low-cost, plug-and-play character ID preservation opens\nthe door to a multitude of downstream applications. To spark ideas and show-\ncase the potential, we explore several intriguing use cases. These include novel\nview synthesis, where InstantID enables the creation of new perspectives and\nangles while maintaining character consistency. We also delve into identity in-\nterpolation between different characters, demonstrating InstantID\u2019s ability to\n4 https://github.com/haofanwang/inswapper\n5 https://github.com/s0md3v/roop\n6 https://github.com/Gourieff/sd-webui-reactor\n14\nWang et al.\nseamlessly blend features from multiple identities. Additionally, we highlight the\nmulti-identity synthesis, illustrating how InstantID can manage complex sce-\nnarios involving multiple characters. Each of these applications underscores In-\nstantID\u2019s versatility and effectiveness in various creative and practical contexts.\nResults are in Appendix (see Figure 10, Figure 11 and Figure 12, respectively).\n5\nConclusions and Future Work\nIn this paper, we propose InstantID as a solution for zero-shot identity-preserving\ngeneration with a simple plug-and-play module, enabling it to adeptly handle\nimage personalization in any style using only one facial image while maintaining\nhigh fidelity. There are two core designs in our InstantID. An Image Adapter\nthat enhances facial detail fidelity and an IdentityNet that ensures strong ID\ncontrol to preserve complex facial features. Our experimental results affirm the\nrobustness and compatibility of InstantID, alongside its ability to preserve the\ntext editing capabilities of the original model. Its pluggable nature further al-\nlows seamless integration with other models such as LoRAs and ControlNet,\nbroadening its applicability without incurring additional costs. The exceptional\nperformance and efficiency of InstantID pave the way for its application in diverse\nreal-world scenarios where identity preservation is paramount. However, despite\nthe effectiveness of InstantID, certain challenges remain to be addressed. The\nID embedding in our model, while rich in semantic information like gender and\nage, has highly coupled facial attributes, which poses a challenge for face editing.\nFuture developments might involve decoupling these facial attribute features to\nenhance flexibility. Moreover, we may observe some limitations in InstantID as-\nsociated with the biases inherent in the face model we used. Furthermore, ethical\nconsiderations surrounding the maintenance of human faces in our model, par-\nticularly the potential creation of offensive or culturally inappropriate imagery,\nare crucial concerns that warrant careful consideration in future work.\nInstantID: Zero-shot Identity-Preserving Generation in Seconds\n15\nReferences\n1. Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila,\nT., Laine, S., Catanzaro, B., et al.: ediffi: Text-to-image diffusion models with an\nensemble of expert denoisers. arXiv preprint arXiv:2211.01324 (2022)\n2. Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., Zhao, H.: Anydoor: Zero-shot\nobject-level image customization. arXiv preprint arXiv:2307.09481 (2023)\n3. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby,\nN.: An image is worth 16x16 words: Transformers for image recognition at scale.\nIn: 9th International Conference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net (2021), https://openreview.net/\nforum?id=YicbFdNTTy\n4. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,\nCohen-Or, D.: An image is worth one word: Personalizing text-to-image genera-\ntion using textual inversion (2022). https://doi.org/10.48550/ARXIV.2208.01618,\nhttps://arxiv.org/abs/2208.01618\n5. Gal, R., Arar, M., Atzmon, Y., Bermano, A.H., Chechik, G., Cohen-Or, D.: De-\nsigning an encoder for fast personalization of text-to-image models. arXiv preprint\narXiv:2302.12228 (2023)\n6. Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.:\nLora: Low-rank adaptation of large language models. In: ICLR (2021)\n7. Huang, L., Chen, D., Liu, Y., Shen, Y., Zhao, D., Zhou, J.: Composer: Creative and\ncontrollable image synthesis with composable conditions. In: International Confer-\nence on Machine Learning (2023), https://api.semanticscholar.org/CorpusID:\n257038979\n8. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. In: Bengio, Y., Le-\nCun, Y. (eds.) 2nd International Conference on Learning Representations, ICLR\n2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings (2014),\nhttp://arxiv.org/abs/1312.6114\n9. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597 (2023)\n10. Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M.M., Shan, Y.: Photomaker: Customiz-\ning realistic human photos via stacked id embedding (2023)\n11. Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter:\nLearning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453 (2023)\n12. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,\nSutskever, I., Chen, M.: Glide: Towards photorealistic image generation and edit-\ning with text-guided diffusion models. In: International Conference on Machine\nLearning (2021), https://api.semanticscholar.org/CorpusID:245335086\n13. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M\u00a8uller, J., Penna,\nJ., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis (2023)\n14. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from\nnatural language supervision. In: International conference on machine learning. pp.\n8748\u20138763. PMLR (2021)\n16\nWang et al.\n15. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125\n1(2), 3 (2022)\n16. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. pp. 10684\u201310695 (2022)\n17. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, Oc-\ntober 5-9, 2015, Proceedings, Part III 18. pp. 234\u2013241. Springer (2015)\n18. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2023)\n19. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,\nK., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-\nto-image diffusion models with deep language understanding. Advances in Neural\nInformation Processing Systems 35, 36479\u201336494 (2022)\n20. Valevski, D., Wasserman, D., Matias, Y., Leviathan, Y.: Face0: Instantaneously\nconditioning a text-to-image model on a face (2023)\n21. Wang, T., Li, L., Lin, K., Lin, C.C., Yang, Z., Zhang, H., Liu, Z., Wang, L.: Disco:\nDisentangled control for referring human dance generation in real world. arXiv\npreprint arXiv:2307.00040 (2023)\n22. Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., Zuo, W.: Elite: Encoding visual\nconcepts into textual embeddings for customized text-to-image generation. arXiv\npreprint arXiv:2302.13848 (2023)\n23. Yan, Y., Zhang, C., Wang, R., Zhou, Y., Zhang, G., Cheng, P., Yu, G., Fu, B.:\nFacestudio: Put your face everywhere in seconds (2023)\n24. Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compati-\nble image prompt adapter for text-to-image diffusion models. arXiv preprint\narXiv:2308.06721 (2023)\n25. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 3836\u20133847 (2023)\n26. Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-\ncontrolnet: All-in-one control to text-to-image diffusion models. Advances in Neural\nInformation Processing Systems (2023)\n27. Zheng, Y., Yang, H., Zhang, T., Bao, J., Chen, D., Huang, Y., Yuan, L., Chen,\nD., Zeng, M., Wen, F.: General facial representation learning in a visual-linguistic\nmanner. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 18697\u201318709 (2022)\n28. Zhou, Y., Zhang, R., Sun, T., Xu, J.: Enhancing detail preservation for cus-\ntomized text-to-image generation: A regularization-free approach. arXiv preprint\narXiv:2305.13579 (2023)\nInstantID: Zero-shot Identity-Preserving Generation in Seconds\n17\nA\nSupplementary Details\nA.1\nImplementation Detail\nIn Figure 3, the spatial control, canny image (b) and depth image (c) are ex-\ntracted from a same reference pose image Figure 8 (a).\nFig. 8. The spatial controls extracted from a reference image.\nA.2\nSupplementary Results\nFig. 9. Effect of Image Adapter and IdentityNet. The x-axis and y-axis corre-\nspond to the weight strength of Image Adapter and IdentityNet respectively.\n18\nWang et al.\nFig. 10. Novel View Synthesis under any given pose.\nFig. 11. Identity interpolation between different characters.\nFig. 12. Multi-identity synthesis with regional control.\n"
  },
  {
    "title": "Scalable Pre-training of Large Autoregressive Image Models",
    "link": "https://arxiv.org/pdf/2401.08541.pdf",
    "upvote": "33",
    "text": "Scalable Pre-training of Large Autoregressive Image Models\nAlaaeldin El-Nouby\nMichal Klein\nShuangfei Zhai\nMiguel Angel Bautista\nAlexander Toshev\nVaishaal Shankar\nJoshua M Susskind\nArmand Joulin\u2217\nApple\nhttps://github.com/apple/ml-aim\n0.3119\n0.3084\n0.3039\n0.2989\n76\n77\n78\nAIM-0.6B\nAIM-1B\nAIM-3B\nAIM-7B\nPre-training validation loss (IN-1k)\nTop-1 accuracy\n(15 benchmarks)\n1M\n100M\n2B\n73\n73.5\n74\n74.5\nNumber of unique images seen (log scale)\nTop-1 accuracy\n(15 benchmarks)\nAIM-0.6B\nFigure 1. AIM scaling behavior (Left) As we scale the capacity of AIM, we observe improved performance for the pre-training objective\nwhich directly correlates with stronger downstream performance. (Right) AIM exhibits stronger downstream performance when trained\nusing larger sets of uncurated web data [32, 33]. The downstream performance is the average attentive probe top-1 accuracy over a diverse\nset of 15 image recognition benchmarks. All models are trained for the same number of updates.\nAbstract\nThis paper introduces AIM, a collection of vision models\npre-trained with an autoregressive objective. These models\nare inspired by their textual counterparts, i.e., Large Lan-\nguage Models (LLMs), and exhibit similar scaling proper-\nties. Specifically, we highlight two key findings: (1) the per-\nformance of the visual features scale with both the model\ncapacity and the quantity of data, (2) the value of the objec-\ntive function correlates with the performance of the model\non downstream tasks. We illustrate the practical implica-\ntion of these findings by pre-training a 7 billion parameter\nAIM on 2 billion images that achieves 84.0% on ImageNet-\n1k with a frozen trunk. Interestingly, even at this scale, we\nobserve no sign of saturation in performance, suggesting\nthat AIM potentially represents a new frontier for training\nlarge-scale vision models. The pre-training of AIM is sim-\nilar to the pre-training of LLMs, and does not require any\nimage-specific strategy to stabilize the training at scale.\n\u2217Work done while at Apple. Now at Google DeepMind.\n1. Introduction\nPre-training task agnostic models has become the standard\nin Natural Language Processing with the recent revolution\nof large language models (LLMs) [13, 64, 75]. These mod-\nels can solve complex reasoning tasks from a few exam-\nples [13], follow instructions [59], and now serve as the en-\ngine of widely used AI assistants such as ChatGPT. A key\nfactor contributing to their success is the ability to consis-\ntently improve as the capacity (i.e., number of parameters)\nor the amount of pre-training data [64] increases.\nThe scaling behavior of these models is remarkable for\ntwo key reasons. First, even though these models are trained\nwith a simple objective \u2013 predicting the next word in a sen-\ntence given its past \u2013 they are able to learn intricate patterns\nover long contexts. Second, the scalability of this autore-\ngressive objective is mostly observed when used in conjunc-\ntion with certain architectures, and in particular Transform-\ners [79], highlighting the potential synergy between the au-\ntoregressive pre-training and this architecture.\nThese observations naturally raise the follow-up ques-\ntion of whether the success of scaling Transformers with\nan autoregressive objective is exclusive to text. This is par-\nticularly significant considering that none of the aforemen-\n1\narXiv:2401.08541v1  [cs.CV]  16 Jan 2024\ntioned elements are inherently specific to language model-\ning. Autoregressive objectives take their roots in the data\ncompression literature [69], and similar approaches have\nbeen investigated in audio [57] and images [18, 76]. The\nTransformer architecture has also been successfully used in\nother domains, in particular, computer vision with the suc-\ncess of the Vision Transformers (ViT) [29]. Therefore, as\na first step towards generalizing the findings of LLMs, we\nexplore if training ViT models with an autoregressive objec-\ntive leads to competitive performance, in terms of learning\nrepresentations, with the same scaling ability as LLMs.\nIn this paper, we introduce Autoregressive Image Mod-\nels (AIM), an autoregressive approach for large-scale pre-\ntraining for visual features. We revisit prior work in au-\ntoregressive representation learning such as iGPT [18] us-\ning a modern toolset that includes vision transformers, col-\nlections of large-scale web data [32, 33] and recent ad-\nvances in LLM pre-training [43, 75]. Additionally, we in-\ntroduce two architectural modifications to adapt autoregres-\nsive pre-training to visual features. First, instead of restrict-\ning the self-attention to be fully causal as is typically the\ncase for LLMs, we adopt a prefix attention, as in T5 [66].\nThis choice enables moving to a fully bidirectional atten-\ntion during downstream tasks.\nSecond, we use a heav-\nily parameterized token-level prediction head, inspired by\nthe heads used in contrastive learning [19]. We observe\nthat this modification significantly improves the quality of\nthe subsequent features with little overhead during training.\nOverall, the training of AIM is similar to the training of\nrecent LLMs and does not rely on any stability-inducing\ntechniques [24, 45, 74] that supervised [24, 74] or self-\nsupervised [5, 58] methods need.\nWe provide a study of a series of models, ranging from\n600M to 7B parameters pre-trained using 2B uncurated im-\nages with permissive licenses.\nOur AIM models exhibit\nstrong scaling behavior w.r.t. the model size as shown in\nFigure 1 where higher capacity models achieve better down-\nstream performance, measured as the average accuracy over\n15 image recognition benchmarks. More importantly, there\nis a correlation between the value of our objective function\non a validation set and the quality of the subsequent frozen\nfeatures.\nThis observation confirms that the autoregres-\nsive objective is adequate for the training of visual features.\nFurthermore, we observe consistent improvement in down-\nstream performance as we train on more images, with no\nsign of saturation. Overall, these observations are aligned\nwith the previous studies on scaling large language models.\n2. Related Work\nAutoregressive models. While most of the literature on au-\ntoregressive models come from language modeling [9, 53,\n64] or speech [56, 57], few works have explored the po-\ntential of this approach for images [18, 49, 61, 61, 68, 76].\nOf particular interest, Van den Oord et al. [76] show that\nusing an architecture adapted to images, e.g., a convolution\nnetwork, significantly improved over autoregressive models\nbuilt with more generic architecture [77], e.g., a recurrent\nnetwork [31]. Parmar et al. [61] further improve the qual-\nity of these autoregressive models by adopting the trans-\nformer architecture [79]. More recently, Chen et al. [18]\nhave shown that scaling with more compute leads to con-\ntinuous improvements. Our work follows this line of re-\nsearch, and we benefit from training on significantly more\ndata, and further improvement in architecture design [29],\ntraining [73, 75] and understanding of the scaling law [43].\nConcurrent to our work, Bai et al. [3] demonstrate the ef-\nfectiveness of large-scale autoregressive vision models for\nin-context pixel prediction tasks (e.g., semantic segmenta-\ntion, depth estimation).\nSelf-supervised pre-training. Pre-training vision models\non datasets of images without supervision has been a fruitful\narea of research in recent years [10, 27, 28, 34, 54, 87, 88].\nDifferent approaches have been employed, focusing on var-\nious proxy tasks for feature learning. For example, Noroozi\nand Favaro [55] learn to re-arrange the order of shuffled\nimage patches. Some other works have relied on cluster-\ning [7, 14, 17, 83].\nAnother popular approach involves\nthe use of a contrastive objective, that resembles predic-\ntive coding, where the objective is to identify each im-\nage [19, 40]. Most recent contrastive approaches include\nDINO [58], BYOL [38] or iBot [88]. In a similar vein,\nsome works have proposed predictive approaches [2, 6] or a\nform of feature whitening [85]. Closer to our approach are\nworks inspired by BERT [26] where patches are masked and\npredicted with an autoencoder in either their discrete [5] or\npixel [41] form.\nOther generative pre-training. Autoregressive modeling\nis a form of generative modeling, and few other genera-\ntive approaches have been considered to learn visual fea-\ntures. The first category leverages some form of autoencod-\ning where the pretext task corresponds to some denoising\ntask. For instance, the noise can be salt-and-pepper [81]\nor masking [5, 62]. Another line of work leverages Gen-\nerative Adversarial Networks (GANs) [35]. Most notably,\nBigGAN [12] trains a large GAN and re-uses the image dis-\ncriminator to produce image features. More recently, Diff-\nMAE [82] used diffusion models to learn image features.\nPre-training at scale. There are numerous works on scal-\ning the pre-training of visual features with no supervi-\nsion [15, 36, 37, 58, 70, 72]. The most salient work in this\narea is DINOv2 where they produce the best self-supervised\nfeatures by scaling the iBot method [88] on a private dataset\nof 142M images and a 460M parameter model. The con-\nclusion from this work is that a carefully tuned contrastive\nmethod scales reasonably well, but they do not exhibit the\nscaling law that we observe with language modeling. They\n2\nalso rely on an intricate implementation of contrastive learn-\ning to avoid the pitfalls described by Chen et al. [20]. In par-\nallel, Singh et al. [70] study the scaling of Masked Autoen-\ncoders (MAE) [39]. While the study focuses on a weakly-\nsupervised setup, it does not showcase strong improvements\nto the self-supervised pre-training as the data is scaled to\nbillions of images. In contrast, we observe a clear benefit of\nscale on the quality of our features, even at a scale of a few\nbillions of parameters and billions of images.\n3. Pre-training Dataset\nWe pre-train our models on the DFN dataset introduced\nby Fang et al. [32]. This dataset is composed of a larger\ncollection of 12.8B image-text pairs [33] filtered from Com-\nmon Crawl. The data has been pre-processed to remove\nNSFW content, blur faces, and reduce contamination by\ndeduplicating against the evaluation sets. A data filtering\nnetwork [32] ranks the samples in the 12.8B collection ac-\ncording to the alignment score between images and their\ncorresponding caption. A subset of 2B images, called DFN-\n2B, has been extracted from the DataComp 12.8B dataset\n[33] by keeping the top 15% samples. Note that other than\nthe privacy and safety filters, this process does not include\nany additional curation based on the image content. Since\nour pre-training does not require text, our method could be\npre-trained using larger image collections that are not paired\nwith captions or have low image-text alignment such as the\nrest of DataComp 12.8B.\nMotivated by the common practice in LLM pre-\ntraining [75] of oversampling high-quality data sources\nsuch as Wikipedia and Books, during pre-training, we sam-\nple images from DFN-2B with a probability of p = 0.8\nand sample images from ImageNet-1k with a probability of\np = 0.2. We refer to such dataset as DFN-2B+.\n4. Approach\n4.1. Training Objective\nOur training objective follows that of a standard autore-\ngressive model applied on a sequence of image patches.\nMore precisely, an image x is split into a grid of K non-\noverlapping patches xk, k \u2208 [1, K], which collectively form\na sequence of tokens. We assume that the sequence order\nis fixed across all images, and we use a raster (row-major)\nordering by default unless otherwise specified. Given the\nabove order, the probability of an image can be factorized\nas a product of patch conditional probabilities:\nP(x) =\nK\nY\nk=1\nP(xk | x<k),\n(1)\nwhere x<k denotes the set of the first k \u2212 1 patches, and\nis the context used to predict the kth patch. As opposed to\nlanguage modeling, our sequences have a fixed length of K\n(prefix) Causal\nTransformer\nMLP\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 2. AIM pre-training overview.. Input images are split into\nnon-overlapping patches and embedded linearly following Doso-\nvitskiy et al. [29]. The patch features are fed to a transformer in\nwhich the self-attention operation is causally masked to prevent\nattending to preceding positions. Afterward, a heavily parameter-\nized MLP processes each of the patch features independently and\nfinally projects it to pixel space. The targets correspond to the in-\nput sequence shifted one position to the left, requiring the model\nto predict the next patch in raster order.\nthat fits in memory and hence we do not need to truncate\nthe context length. The training loss over a set X of images\nis then defined as the negative log-likelihood (NLL):\nX\nx\u2208X\nK\nX\nk=1\n\u2212 log P(xk | x<k).\nMinimizing this objective over an infinite amount of im-\nages, with no further assumptions, is theoretically equiva-\nlent to learning the true underlying image distribution.\nPrediction loss Our training objective naturally gives rise\nto certain variants of losses, each corresponding to a choice\nof the distribution P(xk | x<k). By default, we adopt a nor-\nmalized pixel-level regression loss similar to He et al. [41].\nThis loss corresponds to setting P(xk | x<k) as Gaussian\ndistributions with a constant variance. Namely, given \u02c6xk(\u03b8)\nas the prediction of the kth patch from a network parameter-\nized with \u03b8, and xk as its corresponding ground-truth value,\nour objective is to minimize the sum \u21132 squared distance\nbetween the prediction and the ground-truth:\nmin\n\u03b8\n1\nK\nK\nX\nk=1\n\u2225\u02c6xk(\u03b8) \u2212 xk\u22252\n2.\n(2)\nWe also consider a cross-entropy loss with patches con-\nverted to discrete tokens using an offline tokenizer. Our ab-\nlation studies show that these designs work, although they\ndo not produce as strong features as the pixel-wise loss.\n3\nDownstream\nAdaptation\nPre-training\n (e.g. prefix len=3)\nFigure 3. Prefix causal attention. During pre-training we uni-\nformly sample a prefix length S. The attention for the first S\npatches are set to be bidirectional and loss is only computed for the\nremaining patches in the image. During adaptation to downstream\ntasks, this allows us to drop the attention causal mask, improving\nthe downstream performance.\nModel\n#Params Hidden size Layers\nLR\n#Patches Batch size\nAIM-0.6B\n0.6B\n1536\n24\n1e\u22123\n0.5T\n4096\nAIM-1B\n1.2B\n2048\n24\n1e\u22123\n1.2T\n4096\nAIM-3B\n2.7B\n3072\n24\n1e\u22123\n1.2T\n4096\nAIM-7B\n6.5B\n4096\n32\n1e\u22123\n1.2T\n4096\nTable 1. Model specifications. We provide the embedding di-\nmension, number of layers, and parameter count for all AIM vari-\nants. We also provide the learning rate and batch size during pre-\ntraining. For AIM with 1B parameters and higher, the pre-training\nprocess involves 1.2M iterations, which corresponds to 1.2 trillion\npatches, or 5B images, seen during pre-training.\n4.2. Architecture\nAs the backbone, we adopt the Vision Transformer archi-\ntecture (ViT) [28]. For scaling in the model capacity, we\nfollow the common practice in language modeling and we\nprioritize expanding width rather than depth [64, 75]. In Ta-\nble 1, we provide an overview of the design parameters of\nAIM, including its depth and width, as well as the amount\nof data and optimization scheme for each model capacity.\nThe overall model is illustrated in Figure 2.\nDuring pre-training, we apply causal masks to the self-\nattention layers to model the probability of a patch given the\npreceding patches. More precisely, given a self-attention\nlayer, the embedding for the patch i is computed by:\nyi =\nK\nX\nk=1\naikvi,\n(3)\nwhere aik is the attention weight and vk the value embed-\nding. To enforce the desired constraints, we utilize a causal\nmask for the attention weights, where aik = 0 for k > i,\nand PK\nk=1 aik = 1. This approach enables us to process\nthe image with a single forward pass during training, with-\nout incurring additional computational overhead.\nPrefix Transformer. The autoregressive objective in pre-\ntraining requires a causal mask in the self-attention oper-\nation.\nHowever, this differs from the standard usage of\nViT models in downstream tasks, where bidirectional self-\nattention is employed. This discrepancy leads to a decrease\nin performance, irrespective of whether the causal mask is\nretained during downstream adaptation or not (as shown in\nthe ablations presented in Table 3). To address this issue, we\npropose to consider the initial patches of the sequence, re-\nferred to as the prefix, as a context for predicting the remain-\ning patches following the PrefixLM formulation of Raffel\net al. [65]. The prefix patches are excluded from the au-\ntoregressive prediction and therefore are not constrained to\nbe causal. More precisely, we select a prefix length of size\nS \u2208 [1, K \u2212 1], and remove the causal mask, i.e., ai,k > 0\nfor k < S. This modification helps the model to work in the\nabsence of causal masking, allowing it to be removed during\ndownstream adaptation. This approach improves the perfor-\nmance of the model in downstream tasks and eliminates the\nneed for architectural changes to ViT. Figure 3 illustrates\nthe difference between causal and prefix attention.\nMLP prediction heads. It is a common practice to adopt\ncertain prediction heads during pre-training, which are dis-\ncarded when transferring to downstream tasks [16, 17, 19,\n20, 38]. The purpose of these heads is to prevent the trunk\nfeatures from becoming too specialized in the pre-training\nobjective, thus enhancing their suitability for downstream\ntransfer. We opt for a simple design where we use N blocks\nof MLP on top of the final transformer layer, processing\neach patch independently.\nWe observed that this design\nstrikes a good balance between performance and the addi-\ntional costs incurred during pre-training.\nStraightforward implementation.\nIt is worth noting\nthat AIM does not require particular optimization stability-\ninducing mechanisms such as LayerScale [74], stochastic\ndepth [45], QK-Norm [24], or freezing the patch projec-\ntor [20]. These mechanisms have been crucial for the suc-\ncess of other methods, either supervised or self-supervised.\nOn the contrary, we observe that AIM scales using the same\nset of optimization hyperparameters across model sizes with\nno further tuning (see Table 1).\nWe add sinusoidal positional embeddings [79] to the in-\nput patches before the transformer and before the MLP\nhead. We use a standard expansion ratio of 4 for all the\nMLP blocks in the trunk and the head. We drop the bias\nterm for simplicity, and unlike the original ViT, we do not\nappend a classification token to the input. By default, we\nuse 12 blocks for the MLP head for all model capacities.\nThe pixel targets are normalized per patch before the loss\ncomputation following He et al. [41]. We train our model\nusing bfloat16 precision. We use the AdamW [52] opti-\nmizer with linear warmup and a cosine decay schedule. We\ndetail the hyperparameters used for pre-training and down-\nstream adaptation in Appendix D.\nDownstream adaptation. Pre-training large-scale models\nis a resource-intensive process, and even fine-tuning them\n4\n100k\n200k\n300k\n400k\n500k\n0.3\n0.32\n0.34\nPre-training iterations\nValidation Loss (IN-1k)\nAIM-0.6B\nAIM-1B\nAIM-3B\nAIM-7B\n100k\n200k\n300k\n400k\n500k\n70\n75\n80\nPre-training iterations\nIN-1k Top-1\nAIM-0.6B\nAIM-1B\nAIM-3B\nAIM-7B\nFigure 4. AIM pre-training across model sizes. We observe a clear improvement in the performance of the pre-training objective with\nincreasing the capacity of AIM. Moreover, the downstream performance (IN-1k top-1) is monotonically improving for higher capacity\nmodels as well as with longer pre-training. We do not observe clear signs of plateauing during pre-training even after training for 500k\niterations, indicating that AIM can benefit from even longer pre-training schedules. Note that the loss saturation at the very end of training\nis caused by the cosine decay schedule where the learning rate is effectively zero.\nis demanding. Consequently, we focus on scenarios where\nall model weights are fixed for downstream tasks. In this\ncontext, we only train a classification head, which mitigates\nthe risk of overfitting on small downstream datasets and sig-\nnificantly reduces the adaptation cost.\nUnlike contrastive learning, our loss is computed inde-\npendently for each patch. This means that our pre-training\ndoes not incorporate any notion of global image descrip-\ntors, and hence, we do not have any image level token.\nWhile some methods rely on global average pooling to\nbuild a global feature from the patch features, we find that\nour approach, along with other generative approaches like\nMAE, benefit more from an attention pooling operation [50]\nplaced before the linear classifier. Other works [1, 74, 84]\nhave adopted this attention pooling to improve performance\nwith minimal increase in parameters or FLOPs.\nSpecifically, given a set of patch features P = {pi | 1 \u2264\ni \u2264 K}, we compute a global descriptor \u02c6p through multi-\nhead attention pooling over the patch features as:\n\u02c6ph =\nK\nX\ni=1\nexp(qT\nh W k\nh pi)\nPK\nj=1 exp(qT\nh W k\nh pj)\nW v\nhpi,\n(4)\nwhere for each attention head h = {1, ..., H}, W k\nh , W v\nh \u2208\nRdh\u00d7d correspond to the key and value weight matrices, re-\nspectively; qh is a learnable query vector. And we obtain the\npooled feature as \u02c6p = [p1, ..., pH], \u02c6p \u2208 Rd, which serves as\nthe input to the linear classifier. By default, we set the num-\nber of heads H =\nd\ndh , which makes the total number of\nlearnable parameters 2d2 +d, a negligible cost compared to\nthe main model size. Including this attention pooling makes\nthe entire operation not strictly linear, and, therefore we re-\nfer to it as \u201cAttentive Probe\u201d. Nevertheless, the advantages\nof linear probing, e.g., low additional parameter count and\na reduced risk of overfitting, are preserved with this probe.\n5. Results\n5.1. Impact of scaling\nWe measure the impact when scaling our approach in terms\nof parameters and training data. In particular, we investi-\ngate whether there is a correlation between the pre-training\nobjective and the downstream performance across bench-\nmarks. We also look at the effect of scaling on the value of\nthe loss function. For all of these experiments, we report\nthe value of our loss function on the validation set of IN-1k.\nLoss and performance during training. In Figure 4, we\nmeasure for each model the value of the pre-training loss\nand the classification accuracy on the validations set, as\na function of the number of training iterations.\nWe ob-\nserve that both probes improve accordingly during the en-\ntire training, showing that optimizing our objective directly\nresults in better downstream performance.\nNumber of parameters. We observe that the loss value and\nthe accuracy of the downstream task improve as we scale the\ncapacity of our models. This observation is consistent with\nthe trend observed in LLMs and can be directly attributed\nto the optimization of our objective function, which in turn\nleads to the learning of stronger representations.\nNumber of images. In Figure 5, we show the progression\nof the validation loss as we pre-train on either a small cu-\nrated dataset of 1M images, i.e., IN-1k, or a larger set of\n2B images, i.e. DFN-2B+. It is not surprising that training\non IN-1k leads rapidly to a low validation loss as measured\non the same distribution. However, this loss deteriorates at\nthe end of the training, indicating an overfitting to the train-\ning data. When training on the uncurated DFN-2B dataset,\nthe model starts from a higher validation loss but the loss\ncontinues to decrease with no sign of overfitting. When the\nsame dataset is augmented with a small amount of IN-1k\ndata, as detailed in \u00a7 3, we observe further improvement in\n5\n100k\n200k\n300k\n400k\n500k\n0.32\n0.34\nPre-training iterations\nValidation Loss (IN-1k)\nIN-1k\nDFN-2B\nDFN-2B+\nFigure 5. Dataset impact on pre-training performance. On the\none hand, pre-training using IN-1k leads to overfitting, even for\nthe AIM-0.6B model. On the other hand, pre-training using the\nuncurated DFN-2B dataset prevents overfitting but converges to a\nsimilar point due to the distributional shift. Pre-training on DFN-\n2B+, a data mixture that predominantly consists of DFN-2B with\na small presence of IN-1k samples leads to the best performance.\nthe performance that eventually surpasses pre-training on\nIN-1k. We confirm that the resulting model also leads to a\nbetter downstream performance in Table 2.\npre-training dataset\nIN-1k\nDFN-2B\nDFN-2B+\nattentive\n73.5\n74.5\n75.6\nTable 2.\nDataset impact of downstream performance (15\nbenchmarks). The behavior in Figure 5 is consistent with the\ndownstream performance where we observe that using a data mix-\nture of DFN-2B and IN-1k results in the best performance.\nCompute-optimal pre-training. Since we do not observe\nsigns of overfitting when we train using the DFN-2B+\ndataset, we proceed to examine the impact of extending\nthe length of our pre-training schedule. In Figure 6, we\nstudy the impact of increasing the length of the pre-training\nschedule from 500k to 1.2M iterations, i.e., 2B to 5B im-\nages seen during pre-training. We observe that models pre-\ntrained with a longer schedule achieve significantly lower\nvalidation loss. This suggests that one can improve the per-\nformance of AIM either by increasing the model capacity\nor by pre-training for longer schedules. Interestingly, we\nfind that lower-capacity models trained for a longer sched-\nule achieve comparable validation loss to higher-capacity\nmodels trained for a shorter schedule while using a similar\namount of FLOPs. This finding is consistent with Hoff-\nmann et al. [43] and implies that AIM could follow similar\nscaling laws. However, we defer further investigations in\nthis aspect for future work.\n5.2. Architecture and Design\nIn this section, we investigate the impact of some variations\nin our model and training objective. These ablations are\nconducted using an AIM-0.6B model, which has been pre-\ntrained and evaluated on the IN-1k dataset. The results of\nthese ablations are presented in Table 3.\n1e21\n2e21\n4e21\n8e21\n0.292\n0.296\n0.3\n0.304\n0.308\nFLOPs (log scale)\nPre-training Validation Loss\n2B images\n5B images\n1B\n3B\n7B\nFigure 6. Scaling in FLOPs. That total number of FLOPs during\ntraining correlates with the final validation loss, suggesting com-\npute driven scaling law similar to Hoffmann et al. [43].\nTargets and objective (a). We explore various potential\nrepresentations for the target patches. One approach is to\nutilize the raw pixel values, and training the model with\nmean squared error (MSE) regression loss. A second op-\ntion, proposed by He et al. [41], involves using per-patch\nnormalized pixel values instead of the raw signal with the\nsame MSE loss. Finally, another option is to use a dis-\ncretized representation of the patches, either using k-means\nor a discrete VAE [67, 78]. In this case, the model is trained\nusing a cross-entropy objective similar to language model-\ning. Our experiments show that AIM performs best when\nusing the MSE objective with normalized pixel values.\nAutoregression pattern (b). Autoregressive pre-training\ntypically follows a specific order of traversal to facilitate\nthe prediction of the next token. In the case of language,\nthe traversal pattern is clear, as text is read and written one\nword at a time in a sequential manner (e.g., left to right for\nEnglish). However, for images, determining the traversal\npattern is less obvious. We explore various deterministic\npatterns, including raster, spiraling out, checkerboard, and\nrandomly pre-sampled patterns. Detailed examples of each\npattern are found in Appendix B. Even though our model\nperforms reasonably well with each pattern, we observe that\nthe raster pattern leads to significantly higher performance.\nTo gain deeper insights into this result, we examine the\ndifficulty of predicting patches along sequences for each\npattern. This can be done by measuring the loss value per\npatch as we progress along a sequence, as illustrated in Fig-\nure 7. Our observation is that patterns that present a more\nuniform distribution of difficulty across patches result in su-\nperior models, as compared to patterns where the prediction\nbecomes progressively easier as the sequence unfolds. We\nattribute this to the difficulty of predicting patches through-\nout the sequence that forces the model to retain more infor-\nmation about the image. This leads to better patch features,\nand consequently, to better image representation as a whole.\nCropping scale (c). We explore the impact of the infor-\nmation content of each patch by adjusting the lower bound\nof the cropping scale. On the one hand, opting for a crop-\n6\ntarget\npixels norm. pixel [41] KMeans dVAE [67]\nlinear\n67.5\n70.0\n66.6\n64.0\nattentive\n76.2\n78.2\n75.9\n74.5\n(a) Targets.\npattern\nraster spiral checkerboard random\nlinear\n69.5\n67.7\n68.2\n65.8\nattentive\n77.4\n76.3\n76.0\n75.7\n(b) Autoregression Pattern (causal).\ncrop scale 0.08\n0.4\n1.0\nlinear\n68.4 70.0 49.6\nattentive\n77.7 78.2 63.5\n(c) Crop Scale.\npre-training attn.\ncausal\nprefix\ninference attn.\ncausal bidirectional causal bidirectional\nlinear\n69.5\n30.9\n68.4\n70.0\nattentive\n77.4\n52.3\n76.9\n78.2\n(d) Attention Structure.\nhead\nNone MLP Transformer\nlinear\n64.0\n70.0\n70.5\nattentive\n75.4\n78.2\n78.5\n(e) Head Design.\narchitecture deep wide\nlinear\n68.8\n70.0\nattentive\n77.9\n78.2\n(f) Architecture.\nTable 3. Ablations We investigate various design choices of AIM. We use an AIM-0.6B model that is pre-trained and evaluated using\nIN-1k. We report the linear and attentive probing results. The default settings for AIM used for the main results are highlighted in gray .\nping scale that is too small leads to an easier next-patch-\nprediction task as neighboring patches\u2019 similarity increases.\nOn the other hand, using a large cropping scale can lead to\nsevere overfitting unless the dataset size is sufficiently large.\nSince this study is conducted using IN-1k, we observe a\nclear drop in performance due to overfitting.\nCausal vs. Prefix Attention (d). We measure the impact\nof incorporating prefix attention during pre-training, as op-\nposed to using standard causal attention. We observe that\npre-training with causal self-attention produces models that\nare effective in downstream transfer tasks only when the\ncausal mask is preserved. These models experience a signif-\nicant decline in performance when bidirectional attention is\nemployed. However, pre-training with prefix attention leads\nto models that operate effectively in both causal and bidi-\nrectional modes. Notably, the best performance is achieved\nwhen combining prefix attention during pre-training with\nbidirectional attention during downstream adaptation.\n1\n2\n3\n4\n5\n6\n7\n8\n0.1\n0.15\n0.2\nChunks\nValidation Loss (IN-1k)\nRaster\nSpiral\nCheckerboard\nFixed Random\nFigure 7. Autoregression patterns We explore a number of pat-\nterns for the autoregressive traversal of an image. The set of image\npatches is broken into equal-sized chunks and the validation loss\nis measured per chunk. We observe that the way the task difficulty\nis distributed across chunks varies strongly among patterns.\nHead design (e). We consider different types of heads on\ntop of the backbone to make predictions at the pixel level.\nUsing no heads (i.e. None) performs reasonably well, but\nadding an MLP further improves the quality of the back-\nwidth\n512\n1024\n2048\nlinear\n69.4\n69.6\n70.0\nattentive\n77.7\n78.1\n78.2\n(a) MLP width.\ndepth\n6\n8\n12\nlinear\n65.3\n68.1\n70.0\nattentive\n76.2\n77.1\n78.2\n(b) MLP depth.\nTable 4. MLP design. We vary the capacity of the MLP head by\nchanging the number of MLP blocks (i.e. depth) or the embedding\nsize (i.e. width). Downstream performance improves with more\ncapacity in either width or depth, but depth has more impact.\nautoregressive\nmasked image modeling\nratio=50%\nratio=75%\nattentive\n78.2\n70.3\n77.8\nTable 5. Autoregressive vs. Masking We evaluate the IN-1k per-\nformance of the autoregressive objective of AIM, in comparison\nto the masking objective [5, 26]. We keep all the other architec-\ntural and optimization components fixed. We observe that, under\nthe same pre-training settings, the frozen-trunk performance of the\nautoregressive objective outperforms masking.\nbone. Interestingly, replacing the MLP with a full-fledged\ntransformer of the same depth and width only yields a\nmarginal performance improvement but at a significantly\nhigher computational cost. Therefore, we opt to use an MLP\nhead in our approach. We hypothesize that these heads spe-\ncialize in capturing the low-level signals necessary for ac-\ncurate pixel-level prediction. By incorporating a head, the\ntrunk can learn higher-level features that are more suitable\nfor downstream transfer. A similar design was employed\nfor contrastive learning to prevent the backbone from spe-\ncializing in predicting specific image transformations [19].\nDeeper vs. Wider architecture (f). We present the design\nspecifications of AIM in Table 1, outlining its width and\ndepth. Unlike the original design of ViT [29], where the\ndepth is scaled more rapidly than the width, we adopt a\nscaling strategy similar to that of Llama [75]. This allows\nus to scale our model more gracefully while maintaining a\nreasonable depth. We validate the effectiveness of a wider\narchitecture in Table 3f. Our findings indicate that even for\nthe relatively small-scale AIM-0.6B model, a wider archi-\ntecture not only delivers strong performance but also im-\n7\nModel\nArch.\nData\nIN-1k\niNAT-18\nCifar10\nCifar100\nFood101\nDTD\nPets\nCars\niWildCam\nCamelyon17\nPCAM\nRxRX1\nEuroSAT\nfMoW\nInfographic\nAvg\nDINO [17]\nViT-B/8\nIN-1k\n80.1\n66.0\n97.8\n87.3\n89.5\n78.4\n92.3\n89.2\n58.5\n93.7\n90.2\n6.1\n98.2\n57.0\n41.1\n75.0\niBOT [88]\nViT-L/16\nIN-21k\n83.5\n70.5\n99.2\n93.3\n93.5\n81.6\n92.8\n90.8\n61.8\n94.5\n90.0\n5.9\n98.0\n60.3\n47.7\n77.6\nDINOv2 [58]\nViT-g/14516\nLVD\n86.4\n84.5\n99.6\n95.2\n96.3\n86.3\n96.4\n95.6\n68.2\n96.5\n90.7\n8.0\n98.6\n66.7\n58.8\n81.9\nBEiT [5]\nViT-L/14\nIN-21k\n62.2\n44.4\n94.4\n78.7\n79.0\n64.0\n80.9\n69.5\n52.0\n92.8\n88.2\n4.2\n97.5\n47.7\n25.9\n65.4\nMAE [41, 70]\nViT-H/14\nIN-1k\n80.9\n64.6\n97.1\n85.8\n90.2\n78.1\n95.0\n93.7\n58.1\n94.2\n89.8\n5.4\n98.1\n56.9\n42.2\n75.3\nViT-2B/14\nIG-3B\n82.2\n70.8\n97.5\n87.3\n93.4\n81.2\n95.1\n94.9\n57.8\n94.4\n90.3\n7.3\n98.2\n60.1\n50.2\n77.4\nAIM-0.6B\nViT-H/14\nDFN-2B+\n78.5\n64.0\n97.2\n86.8\n90.1\n80.1\n93.0\n93.0\n57.9\n94.3\n90.0\n7.8\n98.4\n58.3\n45.2\n75.6\nAIM-1B\nViT-1B/14\n80.6\n67.2\n98.2\n88.3\n91.6\n81.8\n93.4\n93.9\n58.6\n94.5\n90.0\n9.0\n98.6\n59.8\n47.5\n76.9\nAIM-3B\nViT-3B/14\n82.2\n69.7\n98.4\n89.9\n92.7\n81.9\n94.1\n93.8\n58.8\n94.3\n90.4\n9.7\n98.5\n60.9\n48.9\n77.6\nAIM-7B\nViT-7B/14\n82.4\n70.9\n98.6\n90.0\n93.1\n82.3\n93.8\n92.1\n59.5\n93.6\n90.7\n10.1\n98.6\n61.7\n49.6\n77.8\nAIM-7B\u2020\nViT-7B/14\nDFN-2B+\n84.0\n75.5\n98.9\n91.8\n94.1\n85.6\n95.4\n95.0\n61.4\n94.2\n90.5\n8.4\n98.5\n63.5\n57.7\n79.6\nTable 6. Downstream evaluation with a frozen trunk. We assess the quality of AIM features by evaluating against a diverse set of 15\nimage recognition benchmarks. AIM and the baseline methods are evaluated using attentive probing with a frozen trunk. AIM models\nexhibit a strong performance across all benchmarks, especially the AIM-7B. AIM outperforms all other methods, using joint-embedding\nor generative approaches, except for DINOv2 which utilizes higher-resolution images, that typically results in a 1-1.5% improvement on\nImageNet for instance. \u2020: Extracting features from the 20th layer instead of the last (32nd), see Table 7 for more details.\nproves training stability. This observation supports the no-\ntion that some of the insights gained from training LLMs\ncan be similarly applied to other domains.\nAttentive vs. Linear probe. For all ablations we report the\nlinear and attentive probing results. We observe that, con-\nsistently across all experiments, attentive pooling provides\na significant boost to performance as it allows for a more\nnuanced aggregation of local features circumventing one of\nthe main weaknesses of generative pre-training: the absence\nof an image-level global descriptor.\nStructure of the MLP. The MLP plays an important role\nas ablated in Table 3e. In Table 4, we further investigate the\ncapacity of the MLP head and how it impacts downstream\nperformance. We vary the capacity of the head by either\nchanging the number of MLP blocks or their width. By de-\nfault, we use a head of 12 blocks and an embedding dimen-\nsion of 2048. First, we observe that increasing the capacity\nof the MLP either through depth or width leads to consis-\ntent improvement in the downstream performance. Second,\nwe find that increasing the number of MLP blocks, with a\nfixed width, leads to a larger improvement compared to in-\ncreasing the width for a fixed depth. Interestingly, we could\nnot find a point where increasing the MLP capacity failed\nto yield further improvements. We did not explore higher\ncapacities beyond those reported in Table 4 as it would lead\nto models with disproportionate head and trunk capacity.\n5.3. Pre-training objective\nAutoregressive vs. Masking We conduct a comparison be-\ntween our architecture trained with an autoregressive objec-\ntive and the masking objective popularized by BERT [26]\nfor language, and by BEiT and MAE for vision. It is im-\nportant to note that we applied the masking objective in the\nsame setting as AIM, thereby isolating the impact on the\nperformance of the pre-training objective from other design\nchoices that differ between AIM and other approaches. In\nthe masking baseline, we randomly sample masks and re-\nplace the masked patches with learnable mask tokens.\nIn Table 5, we show that AIM performs better with an\nautoregressive objective than a masking objective. This is\nconsistent with the results reported by Chen et al. [18], pro-\nviding further evidence that our improvements stem from\nthe utilization of an autoregressive objective.\n5.4. Comparison with other methods\nIn Table 6, we compare the attentive probing performance\nof AIM to other state-of-the-art methods across a set of 15\ndiverse benchmarks that are detailed in Appendix A.\nGenerative methods. AIM provides a strong performance\ncompared to its generative counterparts. AIM outperforms\nBEiT [5] by a large margin. Additionally, AIM-0.6B pro-\nvides a better performance, averaged across all benchmarks,\ncompared to MAE-H [41] which has an equivalent capac-\nity. Moreover, we compare against the MAE-2B [70] model\nwhich has been pre-trained on IG-3B, a private dataset of 3\nbillion images from Instagram. We find that both AIM-3B\nand AIM-7B outperform MAE-2B, with AIM-7B exhibiting\na particularly large improvement. It is worth noting that,\nsimilar to AIM, two other generative approaches, BEiT and\nMAE, benefit from attentive probing, thereby narrowing the\ngap between generative and joint embedding methods.\nJoint embedding methods.\nAIM provides a competi-\ntive performance with joint embedding methods such as\nDINO [17], iBOT [88], and DINOv2 [58].\nIn terms of\n8\naverage accuracy across all benchmarks, AIM outperforms\nDINO and iBOT. However, it falls behind DINOv2 which\nachieves its results by evaluating with higher-resolution in-\nputs. Note that AIM attains such competitive performance\nusing higher capacity trunks.\nNevertheless, AIM\u2019s pre-\ntraining is significantly simpler and can be trivially scaled in\nterms of parameters and data, yielding consistent improve-\nments. On the contrary, state-of-the-art joint embedding\nmethods like DINOv2 heavily rely on a number of tricks,\nsuch as multi-crop augmentation, KoLeo regularization,\nLayerScale, Stochastic Depth, schedules for teacher mo-\nmentum and weight decay, and high-resolution fine-tuning\nin order to achieve strong performance.\nExtracting stronger features.\nWe observe that higher-\nquality features can be extracted from shallower layers com-\npared to the last layer\u2019s features. This is likely due to the\ngenerative nature of the pre-training objective that is inher-\nently different than the discriminative downstream tasks and\ntherefore, the features with the highest semantic content do\nnot necessarily concentrate around the last layer. In Table 7,\nwe report the IN-1k top-1 accuracy for features extracted\nfrom the last layer compared to the layer with the highest\nperformance. A more detailed analysis of this phenomenon\nis provided in Appendix D.\nAIM-0.6B\nAIM-1B\nAIM-3B\nAIM-7B\nlast layer\n78.5\n80.6\n82.2\n82.4\nbest layer\n79.4\n82.3\n83.3\n84.0\nTable 7. Feature extraction. The highest quality features after\nAIM pre-training typically reside in shallower layers than the last.\nExtracting features from earlier layers leads to a non-negligible\nboost to the recognition performance on IN-1k.\n5.5. Low-Rank Adaptation\nIn addition to frozen-trunk evaluation, we examine Low-\nRank Adaptation (LoRA) [44], a popular and efficient fine-\ntuning method. We report the results of LoRA fintuning of\nAIM in Table 8. We observe that LoRA is compatible with\nAIM, leading to a large boost in performance compared to\nfrozen-trunk evaluation. For example, AIM-7B improves\nby 3.9% (compared to the last layer\u2019s performance) while\nfinetuning only 0.1% percent of the trunk parameters.\nAIM-0.6B\nAIM-1B\nAIM-3B\nAIM-7B\nattentive\n78.5\n80.6\n82.2\n82.4\nLoRA (rank=8)\n81.0\n83.6\n85.5\n86.3\nTable 8. Low-rank adaptation (IN-1k). AIM is compatible with\nLoRA showing large gains compared to frozen-trunk evaluations.\n6. Discussion\nIn this paper, we presented a simple and scalable method for\npre-training vision models at scale without supervision. We\nemployed a generative autoregressive objective during pre-\ntraining and proposed several technical contributions to bet-\nter adapt it for downstream transfer. Consequently, we ob-\nserved a number of desirable properties for our Autoregres-\nsive Image Models. First, the capacity of our models can\nbe effortlessly scaled to 7 billion parameters using a vanilla\ntransformer implementation, without resorting to stability-\ninducing techniques or extensive adjustments of hyperpa-\nrameters for each model scale. Second, AIM\u2019s performance\non the pre-training task has a strong correlation with down-\nstream performance. Third, AIM achieves strong perfor-\nmance across 15 recognition benchmarks, outperforming\nprior state-of-the-art methods like MAE and significantly\nnarrowing the gap between generative and joint embedding\npre-training approaches. Finally, we did not observe any\nclear signs of saturation as we scale either in terms of pa-\nrameters or data, suggesting that there is a potential for fur-\nther performance improvements with larger models trained\nfor even longer schedules. We hope that AIM serves as a\nseed for future research in scalable vision models that effec-\ntively leverage uncurated datasets without any bias towards\nobject-centric images or strong dependence on captions.\nLimitations.\nAIM excels in its seamless scalability and\nits effective utilization of large volumes of uncurated im-\nage data. However, alternative methods can offer different\ntrade-offs. MAE [41] provides high sample efficiency and\ncan learn good representations using a small amount of pre-\ntraining data, reducing the risk of overfitting [30] in con-\ntrast to our approach. Contrastive methods [17, 58, 88] cur-\nrently result in stronger representations for a given model\nsize compared to generative approaches such as MAE and\nAIM, but pose significant challenges in terms of scalability\nand loss tractability due to the complexity of their objective.\nAcknowledgements\nThe authors would like to thank Brandon McKinzie, Samira\nAbnar, Preetum Nakkiran, and Jiatao Gu for valuable feed-\nback throughout the project.\nWe thank Edouard Grave\nand Herv\u00b4e Jegou for their inspiring discussions during the\nearlier stages of the project.\nWe thank Marco Cuturi,\nJames Thornton, Pierre Ablin, and Eugene Ndiaye for their\nsupport and for many fruitful discussions throughout the\nproject. Finally, we would like to thank the entire Machine\nLearning Research team at Apple for many helpful discus-\nsions and assistance with infra and data.\n9\nReferences\n[1] Anonymous. V-JEPA: Latent video prediction for visual rep-\nresentation learning. In Submitted to The Twelfth Interna-\ntional Conference on Learning Representations, 2023. 5\n[2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bo-\njanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and\nNicolas Ballas. Self-supervised learning from images with\na joint-embedding predictive architecture.\narXiv preprint\narXiv:2301.08243, 2023. 2\n[3] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir\nBar, Alan Yuille, Trevor Darrell, Jitendra Malik, and\nAlexei A Efros.\nSequential modeling enables scal-\nable learning for large vision models.\narXiv preprint\narXiv:2312.00785, 2023. 2\n[4] Peter Bandi,\nOscar Geessink,\nQuirine Manson,\nMar-\ncory Van Dijk, Maschenka Balkenhol, Meyke Hermsen,\nBabak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun\nPaeng, Aoxiao Zhong, et al. From detection of individual\nmetastases to classification of lymph node status at the pa-\ntient level: the camelyon17 challenge. IEEE Transactions\non Medical Imaging, 2018. 13\n[5] Hangbo Bao, Li Dong, and Furu Wei.\nBEiT: Bert pre-\ntraining of image transformers. In ICLR, 2022. 2, 7, 8\n[6] Adrien Bardes, Jean Ponce, and Yann LeCun.\nVi-\ncreg: Variance-invariance-covariance regularization for self-\nsupervised learning. In ICLR, 2022. 2\n[7] Miguel\nA\nBautista,\nArtsiom\nSanakoyeu,\nEkaterina\nTikhoncheva,\nand Bjorn Ommer.\nCliquecnn:\nDeep\nunsupervised exemplar learning.\nAdvances in Neural\nInformation Processing Systems, 29, 2016. 2\n[8] Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020\ncompetition dataset. arXiv preprint arXiv:2004.10340, 2020.\n13\n[9] Yoshua Bengio, R\u00b4ejean Ducharme, and Pascal Vincent. A\nneural probabilistic language model. Advances in neural in-\nformation processing systems, 13, 2000. 2\n[10] Piotr Bojanowski and Armand Joulin. Unsupervised learning\nby predicting noise. In International Conference on Machine\nLearning, pages 517\u2013526. PMLR, 2017. 2\n[11] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101 \u2013 mining discriminative components with random\nforests. In ECCV, 2014. 13\n[12] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018. 2\n[13] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al.\nLanguage models are few-shot learners.\npreprint\narXiv:2005.14165, 2020. 1\n[14] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning\nof visual features. In ECCV, 2018. 2\n[15] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Ar-\nmand Joulin. Unsupervised pre-training of image features\non non-curated data. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2959\u20132968,\n2019. 2\n[16] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. In\nNeurIPS, 2020. 4\n[17] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, 2021. 2, 4, 8, 9\n[18] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In ICML, 2020. 2, 8, 14\n[19] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020. 2, 4, 7\n[20] Xinlei Chen, Saining Xie, and Kaiming He.\nAn empiri-\ncal study of training self-supervised vision transformers. In\nICCV, 2021. 3, 4\n[21] Gordon Christie, Neil Fendley, James Wilson, and Ryan\nMukherjee. Functional map of the world. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018. 13\n[22] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A.\nVedaldi. Describing textures in the wild. In CVPR, 2014.\n13\n[23] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\nstrategies from data. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n113\u2013123, 2019. 14\n[24] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion pa-\nrameters. In ICML. PMLR, 2023. 2, 4\n[25] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 13\n[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2018. 2, 7,\n8\n[27] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nICCV, 2015. 2\n[28] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-\nmiller, and Thomas Brox. Discriminative unsupervised fea-\nture learning with convolutional neural networks. Advances\nin neural information processing systems, 27, 2014. 2, 4\n[29] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 2, 3,\n7\n10\n[30] Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan\nLaptev, Herv\u00b4e Jegou, and Edouard Grave. Are large-scale\ndatasets necessary for self-supervised pre-training?\narXiv\npreprint arXiv:2112.10740, 2021. 9\n[31] Jeffrey L Elman. Finding structure in time. Cognitive sci-\nence, 14(2):179\u2013211, 1990. 2\n[32] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig\nSchmidt, Alexander Toshev, and Vaishaal Shankar. Data fil-\ntering networks. arXiv preprint arXiv:2309.17425, 2023. 1,\n2, 3\n[33] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp:\nIn search of the next generation of multimodal\ndatasets. arXiv preprint arXiv:2304.14108, 2023. 1, 2, 3\n[34] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. arXiv preprint arXiv:1803.07728, 2018. 2\n[35] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 2\n[36] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan\nMisra. Scaling and benchmarking self-supervised visual rep-\nresentation learning. In ICCV, 2019. 2\n[37] Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron,\nMannat Singh, Ishan Misra, Levent Sagun, Armand Joulin,\nand Piotr Bojanowski. Vision models are more robust and\nfair when pretrained on uncurated images without supervi-\nsion. arXiv preprint arXiv:2202.08360, 2022. 2\n[38] Jean-Bastien Grill, Florian Strub, Florent Altch\u00b4e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\nBernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-a new approach\nto self-supervised learning. NeurIPS, 2020. 2, 4\n[39] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 3\n[40] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020. 2\n[41] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, 2022. 2, 3, 4, 6, 7, 8, 9\n[42] Patrick Helber, Benjamin Bischke, Andreas Dengel, and\nDamian Borth. Eurosat: A novel dataset and deep learning\nbenchmark for land use and land cover classification, 2017.\n13\n[43] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, et al. Training compute-optimal large language mod-\nels. arXiv preprint arXiv:2203.15556, 2022. 2, 6\n[44] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 9\n[45] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In ECCV,\n2016. 2, 4\n[46] iNaturalist 2018 competition dataset. iNaturalist 2018 com-\npetition dataset. https://github.com/visipedia/inat_\ncomp/tree/master/2018, 2018. 13\n[47] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3d object representations for fine-grained categorization. In\n4th International IEEE Workshop on 3D Representation and\nRecognition (3dRR-13), Sydney, Australia, 2013. 13\n[48] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 13\n[49] Hugo Larochelle and Iain Murray. The neural autoregressive\ndistribution estimator. In Proceedings of the fourteenth inter-\nnational conference on artificial intelligence and statistics,\npages 29\u201337. JMLR Workshop and Conference Proceedings,\n2011. 2\n[50] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\nungjin Choi, and Yee Whye Teh. Set transformer: A frame-\nwork for attention-based permutation-invariant neural net-\nworks. In ICML, 2019. 5\n[51] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. In ICLR, 2017. 14\n[52] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 4,\n14\n[53] Tomas Mikolov, Martin Karafi\u00b4at, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. Recurrent neural network\nbased language model. In Interspeech, 2010. 2\n[54] Ishan Misra and Laurens van der Maaten. Self-supervised\nlearning of pretext-invariant representations. In CVPR, 2020.\n2\n[55] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In ECCV,\n2016. 2\n[56] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen\nSimonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,\nAndrew Senior, and Koray Kavukcuoglu. Wavenet: A gener-\native model for raw audio. arXiv preprint arXiv:1609.03499,\n2016. 2\n[57] Aaron van den Oord, Yazhe Li, and Oriol Vinyals.\nRep-\nresentation learning with contrastive predictive coding. In\nNeurIPS, 2018. 2\n[58] Maxime Oquab, Timoth\u00b4ee Darcet, Theo Moutakanni, Huy V.\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-\nsell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\nWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\njanowski. Dinov2: Learning robust visual features without\nsupervision, 2023. 2, 8, 9, 14\n[59] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nNeurIPS, 2022. 1\n11\n[60] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar.\nCats and dogs. In CVPR, 2012. 13\n[61] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In ICML, 2018. 2\n[62] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros.\nContext encoders: Feature\nlearning by inpainting. In CVPR, 2016. 2\n[63] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In ICCV, 2019. 13\n[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 2019. 1, 2, 4\n[65] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020. 4\n[66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 2020. 2\n[67] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n6, 7\n[68] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P\nKingma.\nPixelcnn++: Improving the pixelcnn with dis-\ncretized logistic mixture likelihood and other modifications.\narXiv preprint arXiv:1701.05517, 2017. 2\n[69] Claude E Shannon. Prediction and entropy of printed en-\nglish. Bell system technical journal, 30(1):50\u201364, 1951. 2\n[70] Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala,\nHaoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand\nJoulin, Piotr Doll\u00b4ar, Christoph Feichtenhofer, Ross Girshick,\net al. The effectiveness of mae pre-pretraining for billion-\nscale pretraining. arXiv preprint arXiv:2303.13496, 2023.\n2, 3, 8\n[71] J. Taylor, B. Earnshaw, B. Mabey, M. Victors, and J. Yosin-\nski. Rxrx1: An image set for cellular morphological varia-\ntion across many experimental batches. In ICLR, 2019. 13\n[72] Yonglong Tian, Olivier J Henaff, and A\u00a8aron van den Oord.\nDivide and contrast: Self-supervised learning from uncu-\nrated data. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 10063\u201310074, 2021.\n2\n[73] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In ICML, 2021. 2\n[74] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv\u00b4e J\u00b4egou. Going deeper with im-\nage transformers. arXiv preprint arXiv:2103.17239, 2021. 2,\n4, 5\n[75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 1, 2, 3, 4, 7\n[76] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,\nOriol Vinyals, Alex Graves, et al. Conditional image genera-\ntion with pixelcnn decoders. Advances in neural information\nprocessing systems, 29, 2016. 2\n[77] A\u00a8aron Van Den Oord,\nNal Kalchbrenner,\nand Koray\nKavukcuoglu. Pixel recurrent neural networks. In Interna-\ntional conference on machine learning, pages 1747\u20131756.\nPMLR, 2016. 2\n[78] Aaron Van Den Oord, Oriol Vinyals, et al. Neurips. Ad-\nvances in neural information processing systems, 2017. 6\n[79] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2, 4\n[80] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Co-\nhen, and Max Welling. Rotation equivariant cnns for digital\npathology. In Medical Image Computing and Computer As-\nsisted Intervention\u2013MICCAI 2018: 21st International Con-\nference, Granada, Spain, September 16-20, 2018, Proceed-\nings, Part II 11, pages 210\u2013218. Springer, 2018. 13\n[81] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua\nBengio,\nPierre-Antoine\nManzagol,\nand\nL\u00b4eon\nBottou.\nStacked denoising autoencoders: Learning useful represen-\ntations in a deep network with a local denoising criterion.\nJournal of machine learning research, 11(12), 2010. 2\n[82] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao\nLi, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan\nYuille, and Christoph Feichtenhofer.\nDiffusion models as\nmasked autoencoders.\narXiv preprint arXiv:2304.03283,\n2023. 2\n[83] Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadi-\nyaram, and Dhruv Mahajan. ClusterFit: Improving General-\nization of Visual Representations. In CVPR, 2020. 2\n[84] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. TMLR, 2022.\n5\n[85] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and\nSt\u00b4ephane Deny. Barlow twins: Self-supervised learning via\nredundancy reduction. In ICML, 2021. 2\n[86] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In ICLR, 2018. 14\n[87] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization. In ECCV, 2016. 2\n[88] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nXie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training\nwith online tokenizer. In ICLR, 2022. 2, 8, 9\n12\nA. Datasets\nTo assess the effectiveness and general applicability of the\nlearned representations by AIM, we measure its recognition\naccuracy on a varied collection of 15 benchmarks in Table 6.\nThe specifics of each benchmark can be found in Table 9.\nThese benchmarks include datasets for tasks such as fine-\ngrained recognition, medical imaging, satellite imagery, im-\nages in natural environments, and infographic images.\nDataset\ntrain\ntest\nclasses\nImagenet-1k [25]\n1,281,167\n50,000\n1000\niNAT-18 [46]\n437,513\n24,426\n8142\nCIFAR-10 [48]\n50,000\n10,000\n10\nCIFAR-100 [48]\n50,000\n10,000\n100\nFood101 [11]\n75,750\n25,250\n101\nDTD [22]\n3,760\n1,880\n47\nPets [60]\n3,680\n3,669\n37\nCars [47]\n8,144\n8,041\n196\niWildCam [8]\n129,809\n14961\n182\nCamelyon17 [4]\n302,436\n34904\n2\nPCAM [80]\n262,144\n32768\n2\nRxRx1 [71]\n40,612\n9854\n1139\nEuroSAT [42]\n16,200\n5400\n10\nfMoW [21]\n76,863\n19915\n62\nInfograph [63]\n36,023\n15,582\n345\nTable 9. Evaluation benchmarks. We provide the references,\nthe number of images in the train and test sets, and the number of\ncategories of all the 15 recognition benchmarks used in this work.\nB. Autoregression Patterns\nWe investigate different patterns that can be used to traverse\nan image during pre-training in Table 3b. All patterns used\nin this investigation are illustrated in Figure 8.\nC. Additional Analysis\nC.1. Raster pattern validation loss\nIn Figure 7, we noticed that the validation loss of the raster\npattern across chunks surprisingly declined for the second\nchunk before increasing again. We investigated this further\nin Figure 9 and observed that this behavior is a side-effect\nof using the IN-1k validation set. In particular, we observed\nthat the top rows of the image, aside from the first one, typ-\nically have a lower loss, whether the loss is computed over\nthe regular image or its vertically flipped counterpart.\nC.2. Downstream performance across layers\nIn Tables 7 and 6, we discussed the gain in downstream\nperformance that can be achieved by probing shallower lay-\ners in the model rather than the last. We study this in more\n1\n2\n4\n5\n7\n8\n9\n6\n3\n7\n8\n6\n1\n5\n4\n3\n2\n9\n1\n6\n7\n3\n4\n9\n5\n8\n2\n1\n7\n9\n3\n8\n2\n6\n5\n4\n(a) Raster\n1\n2\n4\n5\n7\n8\n9\n6\n3\n7\n8\n6\n1\n5\n4\n3\n2\n9\n1\n6\n7\n3\n4\n9\n5\n8\n2\n1\n7\n9\n3\n8\n2\n6\n5\n4\n(b) Spiral\n1\n2\n4\n5\n7\n8\n9\n6\n3\n7\n8\n6\n1\n5\n4\n3\n2\n9\n1\n6\n7\n3\n4\n9\n5\n8\n2\n1\n7\n9\n3\n8\n2\n6\n5\n4\n(c) Checkerboard\n1\n2\n4\n5\n7\n8\n9\n6\n3\n7\n8\n6\n1\n5\n4\n3\n2\n9\n1\n6\n7\n3\n4\n9\n5\n8\n2\n1\n7\n9\n3\n8\n2\n6\n5\n4\n(d) Random\nFigure 8. Autoregression patterns. We illustrate the different au-\ntoregression patterns studied in this work including raster, spiral,\ncheckerboard, and fixed random.\n2\n4\n6\n8\n10\n12\n14\n16\n0.3\n0.4\nChunks\nValidation Loss (IN-1k)\nRaster\nRaster (Vertically flipped)\nFigure 9. Raster pattern across patches. We compute the IN-1k\nvalidation loss per a chunk of 16 patches (i.e., a row) for AIM-\n0.6B, pre-trained using a raster pattern. We measure the same loss\nfor the vertically flipped images of the validation set. We observe\nthat, for IN-1k validation set, the patches from the top rows in\nthe image are easier to predict with lower loss, likely due to the\nconcentration of background patches in that region.\ndetail in Figure 10. We find that for all AIM variants, we ex-\ntract the highest quality features, with respect to the down-\nstream transfer, from layers roughly at two-thirds of the\nway into the model depth. However, it is important to note\nthat the performance of deeper layers does not experience a\nsteep decline and continues to exhibit strong performance.\nD. Hyperparameters\nPre-training. AIM models of all capacities have been pre-\ntrained using the same set of hyperparameters that are re-\nported in Table 10. The AIM-0.6 model however has been\n13\n8\n12\n16\n20\n24\n65\n70\n75\n80\n85\nlayers\nValidation Loss (IN-1k)\nAIM-1B\n8\n12\n16\n20\n24\n65\n70\n75\n80\n85\nlayers\nValidation Loss (IN-1k)\nAIM-3B\n8\n12 16 20 24 28 32\n65\n70\n75\n80\n85\nlayers\nValidation Loss (IN-1k)\nAIM-7B\nFigure 10. Downstream performance across layers. The highest quality features in terms of transfer to downstream recognition tasks\ncan be extracted from layers different than the last, with the peak performance achieved by extracting from features roughly at two-thirds\nof the model depth. Deeper layers still retain a strong performance and no sharp decline is observed.\nconfig\nvalue\nOptimizer\nAdamW [52]\nOptimizer Momentum\n\u03b21 = 0.9, \u03b22 = 0.95\nPeak learning rate\n1e\u22123\nMinimum Learning rate\n0.0\nWeight decay\n0.05\nBatch size\n4096\nPatch size\n(14, 14)\nGradient clipping\n1.0\nWarmup iterations\n31,250\nTotal iterations\n1,250,000\nLearning rate schedule\ncosine decay [51]\nAugmentations:\nRandomResizedCrop\nsize\n224px\nscale\n[0.4, 1.0]\nratio\n[0.75, 1.33]\ninterpolation\nBicubic\nRandomHorizontalFlip\np = 0.5\nTable 10. Pre-training hyperparameters All AIM variants of\ndifferent capacities have been trained using the same set of hyper-\nparameters detailed above.\ntrained only for the shorter schedule of 500k iterations. We\ndid not observe any instability while scaling the capacity of\nour model, thereby not requiring any further tuning of the\noptimization hyperparameters.\nAttentive Probing. Downstream evaluation for AIM and\nthe baselines has been primarily conducted via attentive\nprobing as described in \u00a7 4. We report the hyperparame-\nters used to probe all methods in Table 11. For a fair com-\nparison with other baselines, we search over different val-\nues for the learning rate and report the best performance of\neach method similar to [58]. For AIM and other genera-\ntive baselines, we average the features for the last 6 layers\nof the model before feeding them to the attention-probing\nconfig\nIN-1k\nOthers\nOptimizer\nAdamW [52]\nOptimizer Momentum\n\u03b21 = 0.9, \u03b22 = 0.999\nPeak learning rate grid\n[1, 3, 5, 10, 15, 20, 40] \u00d71e\u22124\nMinimum Learning rate\n1e\u22125\nWeight decay\n0.1\nBatch size\n1024\n512\nGradient clipping\n3.0\nWarmup epochs\n5\n0\nEpochs\n50\n100\nLearning rate schedule\ncosine decay [51]\nAugmentations:\nRandomResizedCrop\nsize\n224px\nscale\n[0.08, 1.0]\nratio\n[0.75, 1.33]\ninterpolation\nBicubic\nRandomHorizontalFlip\np = 0.5\nColor Jitter\n0.3\nAutoAugment [23]\nrand-m9-mstd0.5-inc1\nTable 11. Attentive probe hyperparameters. We detail the hy-\nperparameters used for attentive probing AIM as well as the base-\nlines. For all experiments, we search over different learning rate\nvalues and report the best for both AIM and baselines.\nhead which leads to a modest gain in performance. Note\nthat the descriptor dimensionality remains the same which\nis different from the practice of concatenating features sim-\nilar to iGPT[18] which indirectly inflates the capacity of the\nevaluation head.\nLow-rank adaptation. For LoRA finetuning, we use the\nsame hyperparameters as reported in Table 11 in addition to\nmixup [86] (alpha=0.8). We apply LoRA adaptation, with\nrank=8, only to the parameters of the attention block. In\nparticular, the weight matrices for the queries, values, and\noutput projection.\n14\n"
  },
  {
    "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
    "link": "https://arxiv.org/pdf/2401.08417.pdf",
    "upvote": "24",
    "text": "Contrastive Preference Optimization: Pushing the Boundaries of LLM\nPerformance in Machine Translation\nHaoran Xu\u2660 Amr Sharaf\u2661 Yunmo Chen\u2660 Weiting Tan\u2660 Lingfeng Shen\u2660 Benjamin Van Durme\u2660\nKenton Murray\u2660 * Young Jin Kim\u2661 *\nAbstract\nModerate-sized large language models (LLMs)\n\u2013 those with 7B or 13B parameters \u2013 exhibit\npromising machine translation (MT) performance.\nHowever, they do not match the performance\nof state-of-the-art conventional encoder-decoder\ntranslation models or larger-scale LLMs such\nas GPT-4 (OpenAI, 2023).\nIn this study, we\nbridge this performance gap. We first assess the\nshortcomings of supervised fine-tuning for LLMs\nin the MT task, emphasizing the quality issues\npresent in the reference data, despite being human-\ngenerated. Then, in contrast to supervised fine-\ntuning which mimics reference translations, we\nintroduce Contrastive Preference Optimization\n(CPO), a novel approach that trains models to\navoid generating adequate but not perfect transla-\ntions. Applying CPO to ALMA (Xu et al., 2023)\nmodels with only 22K parallel sentences and 0.1%\nparameters yields significant improvements. The\nresulting model, called ALMA-R, can match or\nexceed the performance of the WMT competition\nwinners and GPT-4 on WMT\u201921, WMT\u201922 and\nWMT\u201923 test datasets.1\n1. Introduction\nMachine translation (MT) predominantly utilizes trans-\nformer encoder-decoder architectures (Vaswani et al., 2017),\nwhich is evident in prominent models such as NLLB-200\n(NLLB TEAM et al., 2022), M2M100 (Fan et al., 2021),\nBIBERT (Xu et al., 2021), and MT5 (Xue et al., 2021). How-\never, the emergence of decoder-only large language models\n(LLMs) such as the GPT series (Brown et al., 2020; Ope-\n*Equal contribution . Work done during an internship at Mi-\ncrosoft.\n\u2660Johns Hopkins University \u2661Microsoft.\nCorrespon-\ndence to: Haoran Xu <hxu64@jhu.edu>, Kenton Murray <ken-\nton@jhu.edu>, Young Jin Kim <youki@microsoft.com>.\n1We release our code and models at: https://github.\ncom/fe1ixxu/ALMA.\nnAI, 2023), Mistral (Jiang et al., 2023), LLaMA (Touvron\net al., 2023a;b), Falcon (Almazrouei et al., 2023), inter alia,\nwhich have shown remarkable efficacy in various NLP tasks,\nwhich attracts the interest of developing machine translation\nwith these decoder-only LLMs. Recent studies (Zhu et al.,\n2023a; Jiao et al., 2023b; Hendy et al., 2023; Kocmi et al.,\n2023; Freitag et al., 2023) indicate that larger LLMs such\nas GPT-3.5 (175B) and GPT-4 exhibit strong translation\nabilities. However, the performance of smaller-sized LLMs\n(7B or 13B) still falls short when compared to conventional\ntranslation models (Zhu et al., 2023a).\nTherefore, there are studies intend to enhance the translation\nperformance for these smaller LLMs (Yang et al., 2023;\nZeng et al., 2023; Chen et al., 2023; Zhu et al., 2023b; Li\net al., 2023; Jiao et al., 2023a; Zhang et al., 2023), but\ntheir improvements are relatively modest, primarily due to\nthe predominant pre-training of LLMs on English-centric\ndatasets, resulting in limited linguistic diversity (Xu et al.,\n2023). Addressing this limitation, Xu et al. (2023) initially\nfine-tune LLaMA-2 (Touvron et al., 2023b) with extensive\nnon-English monolingual data to enhance their multilingual\nabilities, and then perform supervised fine-tune (SFT) with\nhigh-quality parallel data to instruct the model to generate\ntranslations. Their model, named ALMA, outperforms all\nprior moderated-size LLMs, and even larger models such\nas GPT-3.5, in the translation task. Nonetheless, the perfor-\nmance still lags behind leading translation models such as\nGPT-4 and WMT competition winners. Our study bridges\nthis gap by further fine-tuning ALMA models with our\nnovel training method Contrastive Preference Optimiza-\ntion (CPO) and minimal costs, i.e., only 12M learnable\nparameters (equivalent to 0.1% of the original model size)\nand a 22K dataset for 10 directions. The fine-tuned model\nis referred to as ALMA-R. A detailed performance compar-\nison is illustrated in Figure 1.\nCPO aims to mitigate two fundamental shortcomings of SFT.\nFirst, SFT\u2019s methodology of minimizing the discrepancy\nbetween predicted outputs and gold-standard references in-\nherently caps model performance at the quality level of the\ntraining data. This limitation is significant, as even human-\nwritten data, traditionally considered high-quality, is not\nimmune to quality issues (more details in Section 2). For in-\n1\narXiv:2401.08417v3  [cs.CL]  2 Feb 2024\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nFigure 1. A performance comparison featuring our proposed\nmodel ALMA-13B-R against other recently released 13B LLM-\nbased models, as well as top-performing translation systems\nlike GPT-4 and WMT winners.\nThis evaluation covers the\nWMT\u201922 test data across 8 directions, involving translations\nto and from English for German, Czech, Chinese, and Rus-\nsian.\nScores are averaged by three different reference-free\nmodels: wmt23-cometkiwi-da-xxl, XCOMET-XXL, and\nwmt22-cometkiwi-da, and are also averaged across all direc-\ntions. The gold reference is also evaluated due to the reference-free\napproach. Our model, ALMA-13B-R, developed by further train-\ning ALMA-13B-LoRA using our proposed CPO method, either\nmatches or surpasses the most advanced translation models, We\nshow the detailed numerical data for all systems presented in the\nfigure in Appendix A.\nstance, one may notice that some strong translation models\nare capable of producing translations superior to the gold\nreference, as illustrated in Figure 1. Secondly, SFT lacks a\nmechanism to prevent the model from rejecting mistakes in\ntranslations. While strong translation models can produce\nhigh-quality translations, they occasionally exhibit minor\nerrors, such as omitting parts of the translation. Preventing\nthe production of these near-perfect but ultimately flawed\ntranslation is essential. To overcome these issues, we intro-\nduce Contrastive Preference Optimization (CPO) to train the\nALMA model using specially curated preference data. After\nCPO training, the ALMA-R model shows marked improve-\nments, achieving performance levels that match or even\nsurpass those of GPT-4 and WMT competition winners.\nOur main contributions are summarized as follows:\nAre reference Gold or Gilded? We conducted an in-depth\nanalysis of the training data (FLORES-200 data) utilized by\nthe ALMA model. We meticulously compared the quality\nof the reference translations with those generated by strong\ntranslation models. Our findings reveal that, in numerous\ninstances, the quality of human-written parallel data is even\ninferior to that of system-generated translations. This obser-\nvation underscores a critical insight: training models exclu-\nsively towards replicating reference translations may not be\nthe most effective approach, and reliance on reference-based\nevaluation could be flawed.\nPushing the Performance Boundary of SFT We intro-\nduce Contrastive Preference Optimization, which offers ad-\nvantages in terms of memory efficiency, speed, and, cru-\ncially, enhanced effectiveness in improving translation qual-\nity. CPO breaks the performance bottleneck inherent in\nSFT\u2019s reference-mimicking learning process and push the\nperformance boundary of models that have reached satura-\ntion through SFT training.\nPreference Data We build and release a high-quality pref-\nerence data for the machine translation area.\n2. Gold or Gilded? Scrutinizing Gold\nReference Quality\nThe significance of target references is paramount in ma-\nchine translation tasks. The paradigm of training models on\nthe machine translation task heavily relies on the quality of\nthe references since the model is commonly optimized using\na loss that is defined to minimize the difference between the\npredicted outputs and gold reference. Consider a dataset\nD, comprising pairs of source sentences x and their corre-\nsponding target sentences (gold references) y, represented\nas D =\n\b\nx(i), y(i)\tN\ni=1, where N is the total number of par-\nallel sentences. The negative log-likelihood loss for these\nparallel sentences, in relation to a model \u03c0\u03b8 parameterized\nby \u03b8, is defined as follows:\nLNLL = \u2212E(x,y)\u223cD[log \u03c0\u03b8(y|x)].\n(1)\nHence, the ability of models to effectively translate is con-\ntingent upon the availability of high-quality translation pairs\n(Xu et al., 2023; Maillard et al., 2023). Furthermore, preva-\nlent evaluation tools such as BLEU (Papineni et al., 2002)\nand COMET-22 (Rei et al., 2022) predominantly rely on\nreference-based metrics. However, the precision of these\nevaluations is sensitive to and compromised by substandard\nreferences (Kocmi et al., 2023; Freitag et al., 2023). Recent\nresearch (Xu et al., 2023; Kocmi et al., 2023; Freitag et al.,\n2023) has shifted attention towards assessing the quality of\nparallel datasets, indicating that target references may not\nconsistently represent the highest quality. In Figure 2, we\ntake a translation example from the FLORES-200 dataset,\nand compare the gold reference translation with outputs\nfrom the best ALMA model and GPT-4. This comparison\nreveals that the gold reference is a flawed translation, as it\nomits part of information, whereas the system-generated\noutputs demonstrate superior quality. This prompts an in-\nquiry: Are references (even though human-written) truly\nequivalent to gold standards? To thoroughly assess the\nquality of both the gold standard references and the outputs\nfrom contemporary high-performance translation models,\n2\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nwe propose evaluating these outputs utilizing reference-free\nevaluation frameworks.\nSource: ? ? ? ? ?  (Martelly) ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?  (CEP)?\nReference: It is Martelly's fifth CEP in four years. \nALMA-13B-LoRA: This is Martelly's fifth time being selected by the Provisional \nElectoral Council (CEP) in four years.\nGPT-4: This is the fifth time Martelly has been selected for Haiti's Provisional \nElectoral Council (CEP) in four years.\nFigure 2. An example demonstrating that a human-written gold\nreference may not always be flawless, and could be surpassed by\ntranslations from advanced translation models. In this case, the\nreference retains the abbreviation \u201cCEP\u201d but fails to provide its full\nname. The highlighted phrases in the model-generated translations\nindicate the portions omitted by the gold reference.\nModels We scrutinize the translation outputs from ALMA-\n13B-LoRA2, as well as zero-shot translations from the\nmost recent GPT-4 (gpt-4-1106-preview).\nTo as-\nsess the quality of these outputs, we employ two of the\nlatest and largest reference-free models, each with a 10B\nparameter size and demonstrating very high correlation\nwith human judgements (Freitag et al., 2023).\nThese\nmodels are Unbabel/wmt23-cometkiwi-da-xxl\n(henceforth referred to as KIWI-XXL) (Rei et al., 2023)\nand Unbabel/XCOMET-XXL (subsequently referred to as\nXCOMET) (Guerreiro et al., 2023).\nData we consider the high-quality and human-written\nFLORES-200 dataset (NLLB TEAM et al., 2022), com-\nprising both development and test data, amounting to a total\nof 2009 samples for each language direction, to compare the\ngold references with the outputs generated by the models.\nWe employed ALMA-13B-LoRA and GPT-4 to perform\ntranslations across five English-centric language pairs, cov-\nering both translations from and to English. These pairs\ninclude German (de), Czech (cs), Icelandic (is), Chinese\n(zh), and Russian (ru), with Icelandic (is) categorized\nas a low-resource language and the others as high-resource\nlanguages.\nPrompt The prompt employed for generating translations\nwith ALMA models is consistent with the one used in Xu\net al. (2023). For GPT-4 translation generation, we fol-\nlow the guidelines suggested by Hendy et al. (2023). The\nspecifics of these prompts are detailed in Appendix B.\nModel Outputs Can Be Better References In Table 1, we\npresent the evaluation scores of KIWI-XXL and XCOMET\nfor the gold references, ALMA-13B-LoRA outputs, and\nGPT-4 outputs. Additionally, we report Win Ratio, reflect-\n2ALMA-13B-LoRA is the best 13B translation model in the\nALMA families. It initially undergoes full-weight fine-tuning on\nmonolingual data, followed by fine-tuning on high-quality human-\nwritten parallel data using low-rank adaptation (LoRA) (Hu et al.,\n2022).\nTable 1. A performance comparison between gold references and\noutputs from advanced translation models, as assessed by two\n10B-size reference-free evaluation models with the highest corre-\nlation to human preferences. The results indicate that the average\nperformance of these strong translation models can even exceed\nthat of the gold references, achieving a high success rate in beating\nthe reference.\nKIWI-XXL\nWin Ratio (%)\nXCOMET\nWin Ratio (%)\nTranslating to English (xx\u2192en)\nReference\n85.31\n-\n88.82\n-\nALMA-13B-LoRA\n88.33\n73.24\n92.68\n60.17\nGPT-4\n89.21\n79.43\n94.66\n54.25\nTranslating from English (en\u2192xx)\nReference\n87.85\n-\n94.42\n-\nALMA-13B-LoRA\n85.62\n42.15\n93.07\n35.46\nGPT-4\n87.30\n49.13\n94.21\n38.09\ning the proportion of instances where model outputs surpass\nthe gold standard references. These metrics are calculated\nas an average across five languages. Remarkably, even com-\nparing with the high-quality Flores-200 dataset, the average\nperformance of translation models in xx\u2192en translations\nsignificantly exceeds that of the references, showing approx-\nimately 3-4 point increases in KIWI-XXL and 4-6 point\ngains in XCOMET. Notably, a significant proportion of out-\nputs are rated higher than the references by KIWI-XXL (e.g.,\n73.24% for ALMA), with a slightly reduced yet still sub-\nstantial percentage when assessed using XCOMET (60.17%\nfor ALMA). In the en\u2192xx direction, while the overall per-\nformance between the translations from reference and two\nsystems is comparable, approximately 40% are still deemed\nsuperior to the reference translations.\nMotivation: Help The Model Learn Rejection The afore-\nmentioned findings illustrate that translations produced by\nadvanced models can sometimes surpass the quality of gold\nstandard references. This raises the question of how to\neffectively utilize such data. A straightforward approach\nwould involve fine-tuning the model using the source and\nthe superior translations as references. While this could en-\nhance the model\u2019s translation abilities, it does not equip the\nmodel with the discernment to identify and avoid generating\nsuboptimal translations, exemplified by the \u201cgood but not\nperfect\u201d translations depicted in Figure 2. Consequently,\nthis situation motivates us to develop a new training objec-\ntive, which aims to instruct the model in prioritizing the\ngeneration of higher-quality translations and rejecting lesser\nones, in a style of contrastive learning with hard negative\nexamples (Oord et al., 2018; Chen et al., 2020; He et al.,\n2020; Robinson et al., 2021; Tan et al., 2023). This objective\nmoves beyond the traditional focus on merely minimizing\ncross-entropy loss towards the reference.\n3. Contrastive Preference Optimization\nTo learn an objective that fosters superior translations and\nrejects inferior ones, access to labeled preference data is\n3\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nessential, yet such data is scarce in machine translation.\nIn this section, we first describe the construction of our\npreference data and then introduces a preference learning\ntechnique, contrastive preference optimization (CPO).\n3.1. Triplet Preference Data\nWe here details our methodology for constructing preference\ndata D. This dataset is developed using the FLORES-200\ndata (both development and test sets) and encompasses the\nsame language pairs as discussed in Section 2. For each\nlanguage pair, the dataset comprises 2009 parallel sentences.\nFor a given source sentence x, whether translated from or\nto English, we utilize both GPT-4 and ALMA-13B-LoRA\nto generate respective translations, denoted as ygpt-4 and\nyalma. Together with the original target reference yref, this\nforms a triplet y = (yref, ygpt-4, yalma), representing three\ndifferent translation outputs for the input x. The reference-\nfree evaluation models KIWI-XXL and XCOMET are then\nemployed to score these translations, with the average scores\nrepresented as s = (sref, sgpt-4, salma).3 The highest-scoring\ntranslation is labeled as the preferred translation yw, and\nthe lowest-scoring as the dis-preferred translation yl, i.e.,\nyw = yarg maxi(s), yl = yarg mini(s), where i represents the\nindex in the triplet. Translations with intermediate scores\nare not considered. An illustrative example of this selection\nprocess is depicted in Figure 3. It is important to note that\neven the dis-preferred translations may be of high-quality.\nThe designation \u2019dis-preferred\u2019 indicates that there is still\nroom for improvement, perhaps through the addition of\nminor details. This approach of using high-quality but not\nflawless translations as dis-preferred data aids in training the\nmodel to refine details and achieve perfection in generated\ntranslations.\nFigure 3. A triplet of translations, either model-generated or de-\nrived from a reference, accompanied by their respective scores as\nassessed by reference-free models. For a given source sentence,\nthe translation with the highest score is designated as the preferred\ntranslation, while the one with the lowest score is considered dis-\npreferred, and the translation with a middle score is disregarded.\n3The impact of using different evaluation models, such as only\nusing XCOMET or KIWI-XXL, is explored in Section 5.1.\n3.2. Deriving the CPO Objective\nWe discuss the derivation of CPO objective, beginning\nwith an analysis of Direct Preference Optimization (DPO)\n(Rafailov et al., 2023). DPO represents a more direct opti-\nmization objective utilized in reinforcement learning from\nhuman feedback (RLHF) (Ziegler et al., 2019; Ouyang\net al., 2022). Given a set of source sentences x, along-\nside preferred translation targets yw and less preferred ones\nyl, we can access a static dataset of comparisons, denoted\nas D =\nn\nx(i), y(i)\nw , y(i)\nl\noN\ni=1. The loss function for DPO is\nconstructed as a maximum likelihood objective for a param-\neterized policy \u03c0\u03b8:\nL(\u03c0\u03b8; \u03c0ref) = \u2212 E(x,yw,yl)\u223cD\nh\nlog \u03c3\n\u0010\n\u03b2 log \u03c0\u03b8(yw|x)\n\u03c0ref(yw|x)\n\u2212 \u03b2 log \u03c0\u03b8(yl|x)\n\u03c0ref(yl|x)\n\u0011i\n,\n(2)\nwhere \u03c0ref is a pre-trained language (translation) model , \u03c3 is\nthe Sigmoid function, and \u03b2 is a hyperparameter. DPO train-\ning can be conducted in a supervised fine-tuning style, as it\nrelies exclusively on labeled preference data and does not\nrequire interaction between agents and their environment.\nHowever, DPO has notable drawbacks compared to common\nSFT. Firstly, DPO is memory-inefficient: it necessitates\ntwice the memory capacity to simultaneously store both the\nparameterized policy and the reference policy. Secondly,\nit is speed-inefficient: executing the model sequentially\nfor two policies doubles the processing time. To address\nthese inefficiencies, we introduce contrastive preference\noptimization.\nThe memory- or speed- inefficiency can be resolved when\n\u03c0ref is set as a uniform prior U, as the terms \u03c0ref(yw|x)\nand \u03c0ref(yl|x) cancel each other out. This negates the need\nfor additional computations and storage beyond the pol-\nicy model itself. Thus, we initially demonstrate that the\nDPO loss can be effectively approximated using a uniform\nreference model:\nL(\u03c0\u03b8; U) = \u2212 E(x,yw,yl)\u223cD\nh\nlog \u03c3\n\u0010\n\u03b2 log \u03c0\u03b8(yw|x)\n\u2212 \u03b2 log \u03c0\u03b8(yl|x)\n\u0011i\n.\n(3)\nSpecifically, we prove the below Theorem in Appendix C.\nTheorem 1. When \u03c0ref is defined as \u03c0w, an ideal policy that\nprecisely aligns with the true data distribution of preferred\ndata, the DPO loss L(\u03c0\u03b8; \u03c0w) + C is upper bounded by\nL(\u03c0\u03b8; U), where C is a constant.\nThe approximation in Equation 3 is effective because it\nminimizes the upper boundary of the DPO loss. The proof\n4\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nrelies on an important assumption of \u03c0ref = \u03c0w. Contrary\nto common practices where \u03c0ref is set as the initial SFT\ncheckpoint, our approach considers it as the ideal policy we\naim to reach. Although the ideal policy \u03c0w is unknown and\nunattainable during model training, it is not engaged in the\nloss after our approximation.\nFurthermore, we incorporate a behavior cloning (BC) regu-\nlarizer (Hejna et al., 2023) to ensure that \u03c0\u03b8 does not deviate\nfrom the preferred data distribution:\nmin\n\u03b8\nL(\u03c0\u03b8, U)\ns.t. E(x,yw)\u223cD\nh\nKL(\u03c0w(yw|x)||\u03c0\u03b8(yw|x))\ni\n< \u03f5,\n(4)\nwhere \u03f5 is a small positive constant and KL is Kull-\nback\u2013Leibler (KL) divergence. The regularizer can boil\ndown to adding a SFT term on the preferred data (a detailed\nexplanation is provided in Appendix C):\nmin\n\u03b8\nL(\u03c0\u03b8, U)\n|\n{z\n}\nLprefer\n\u2212E(x,yw)\u223cD[log \u03c0\u03b8(yw|x)]\n|\n{z\n}\nLNLL\n.\n(5)\nThe above is the formulation of our CPO loss, which in-\ncludes one preference learning term Lprefer and one negative\nlog likelihood term LNLL.\n4. Experiments\n4.1. Data\nFollowing Section 2, we consider 10 translation directions in\nthe paper: cs\u2194en, de\u2194en, is\u2194en, zh\u2194en, ru\u2194en.\nBuilding on the ALMA models\u2019 (Xu et al., 2023) insights\nthat a small quantity of high-quality data can yield impres-\nsive translation results, our training dataset is even more\ncompact. As detailed in Section 3.1, our preference training\ndata is derived from the FLORES-200 dataset, a subset of\nwhich has been also employed in the training of ALMA\nmodels. This results in a total of 2K \u00d7 10 directions = 20K\npaired sentences. In addition to preference data assessed\nby large evaluation models, our dataset incorporates 1K in-\nternal human-labeled preference data, containing preferred\nand dis-preferred translations along with human preference.\nHowever, the human-labeled data is limited to just two trans-\nlation directions: en\u2192zh and en\u2192de. The details regard-\ning the composition and influence of human-labeled data are\nexplored in Appendix D.4 In alignment with Xu et al. (2023),\nour primary focus is on the test set drawn from WMT\u201921 for\nis and WMT\u201922 for other languages. Additionally, we con-\nduct auxiliary experiments evaluating models on WMT\u201923,\ncovering six directions: de\u2194en, zh\u2194en, and ru\u2194en.\n4TL;DR: A brief overview of the impact of this human-labeled\ndata suggests a minimal effect.\n4.2. Training Setup\nWe train the model in a many-to-many multilingual ma-\nchine translation manner, starting with ALMA-13B-LoRA\nas the initial checkpoint. During the training phase, we fo-\ncus exclusively on updating the weights of the added LoRA\nparameters. These weights have a rank of 16 and only add\nan additional 12M parameters to the original 13B size of the\nmodel. We adhere to the default \u03b2 value of 0.1 as suggested\nby Rafailov et al. (2023). The fine-tuning process of ALMA-\n13B-LoRA involves a batch size of 128, a warm-up ratio\nof 0.01, spanning a single epoch, and accommodating se-\nquences with a maximum length of 512 tokens. To optimize\ntraining efficiency, we integrate the deepspeed tool (Rasley\net al., 2020). We utilize the same prompt as Xu et al. (2023)\nand do not compute the loss for the prompt. While our\nprimary focus is on the performance of 13B models, CPO\nmarkedly benefits 7B models as well. Consequently, we\nalso release ALMA-7B-R and provide a detailed discussion\nof its performance in Appendix A.\n4.3. Baselines\nSoTA Models In this category, our benchmarks are estab-\nlished against, to the best of our knowledge, the strongest\npublicly available translation models. We first compare with\nALMA-13B-LoRA, recognized as one of the top moderate-\nsize language-model based translation systems, surpassing\nnotable conventional models such as NLLB-54B in both\nWMT\u201921 and WMT\u201922. We also compare our results with\nTowerInstruct5, a recently released LLM-based translation\nmodel and a contemporary work in the field.6 Additionally,\nwe evaluate against the zero-shot performance of the latest\nGPT-4 (gpt-4-1106-preview), currently shown to be\nthe best translation model among all LLM-based translation\nsystems (Xu et al., 2023; Zhang et al., 2023; Zeng et al.,\n2023; Jiao et al., 2023a). Lastly, we include comparisons\nwith the WMT competition winners, representing the high-\nest standard of translation models within the competition,\nthough it is noted that the winning models vary across dif-\nferent language directions.7\nSFT and DPO We also compare different training objec-\ntives. Given that CPO is designed to steer learning towards\npreferred data, a straightforward benchmark is to compare\nits performance against directly SFT on the same preferred\ndata set. Furthermore, considering that CPO is an evolution\nof DPO, we also include a comparative analysis with DPO.\n5https://huggingface.co/datasets/Unbabel/\nTowerBlocks-v0.1.\n6Note that TowerInstruct has used WMT\u201922 test data for train-\ning, so we exclude it from comparison on the WMT\u201922 test dataset.\n7The WMT winner systems used for comparison in each direc-\ntion are provided in Appendix E.\n5\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nTable 2. The overall results in en\u2192xx for WNT\u201921 and WMT\u201922. The application of the CPO method to fine-tune the ALMA-13B-\nLoRA model leads to a significant enhancement in performance, equalling or surpassing that of WMT competition winners and GPT-4.\nBold numbers denote the highest scores across all systems. Dark blue boxes indicates that the improvement over the original ALMA\nmodel achieves at least 80% estimated accuracy with the human judgement (Kocmi et al., 2024). Specifically, this denotes that for an\nagreement rate of 80% with human decisions, the improvement needs a minimum of \u2265 1.24 for both KIWI-XXL and XCOMET, and\n\u2265 0.53 for KIWI-22. Further details on estimatied accuracy are provided in Appendix F. The lesser improvements are highlighted in\nshallow blue boxes. Decreases in performance are marked with yellow boxes.\nde\ncs\nis\nModels\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n82.67\n84.01\n97.85\n83.19\n81.83\n90.27\n80.51\n85.20\n91.52\nWMT Winners\n83.56\n83.70\n96.99\n85.31\n87.27\n94.38\n81.77\n84.94\n91.61\nGPT-4\n83.48\n84.91\n97.56\n84.81\n85.35\n93.48\n81.03\n81.21\n90.00\nALMA-13B-LoRA\n82.62\n81.64\n96.49\n84.14\n84.24\n92.38\n81.71\n83.31\n91.20\n+ SFT on preferred data\n82.75\n81.85\n96.67\n84.14\n83.46\n91.99\n81.48\n82.11\n90.30\n+ DPO\n82.40\n81.20\n96.40\n83.86\n83.45\n91.68\n81.43\n82.66\n90.33\n+ CPO (Ours, ALMA-13B-R)\n83.28\n84.25\n97.48\n84.99\n87.06\n93.61\n82.18\n85.68\n91.93\nzh\nru\nAvg.\nModels\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n80.92\n81.70\n90.42\n82.96\n84.62\n94.17\n82.05\n83.47\n92.85\nWMT Winners\n82.04\n81.13\n91.14\n84.35\n87.01\n94.79\n83.41\n84.81\n93.78\nGPT-4\n81.73\n81.53\n90.79\n83.64\n86.15\n94.3\n82.94\n83.83\n93.23\nALMA-13B-LoRA\n80.82\n79.96\n89.92\n83.10\n84.17\n93.79\n82.48\n82.66\n92.76\n+ SFT on preferred data\n81.25\n80.51\n90.18\n83.23\n84.15\n93.54\n82.57\n82.42\n92.54\n+ DPO\n80.74\n79.64\n89.58\n82.94\n83.40\n93.25\n82.27\n82.07\n92.25\n+ CPO (Ours, ALMA-13B-R)\n82.25\n84.32\n92.03\n83.98\n87.37\n95.22\n83.34\n85.74\n94.05\n4.4. WMT\u201921 and WMT\u201922 Results\nWe present the primary results for en\u2192xx and xx\u2192en in\nTable 2 and Table 3, respectively. Our emphasis is primarily\non reference-free evaluation models, due to our analysis in\nSection 2, which questions the reliability of gold references\nand highlights that evaluations can be compromised by poor-\nquality references (Kocmi et al., 2023; Freitag et al., 2023).\nThese models include KIWI-XXL, XCOMET, and a smaller\nyet popular model, Unbabel/wmt22-cometkiwi-da\n(hereinafter referred to as KIWI-22). Scores highlighted in\nbold represent the highest achieved across all systems. For a\ncomprehensive comparison, we also include reference-based\nevaluations using sacreBLEU (Post, 2018) and COMET-\n22 (Unbabel/wmt22-comet-da) (Rei et al., 2022) in\nAppendix A.\nComparing With SoTA Models While ALMA-13B-LoRA\nranks as one of the top moderate-size LLM translation mod-\nels, it slightly trails behind GPT-4 and the WMT competition\nwinners. However, the incorporation of CPO significantly\nenhances ALMA\u2019s capabilities, bringing its performance to\na level that is comparable to or even surpasses that of GPT-4\nand WMT winners. For example, ALMA-13B-R achieves\nan average score of 85.74 on KIWI-XXL and 94.05 on\nXCOMET for en\u2192xx translations. These scores outper-\nform GPT-4, which scores 83.83 on KIWI-XXL and 93.23\non XCOMET, as well as the WMT winners, who score 84.81\non KIWI-XXL and 93.78 on XCOMET.\nComparing With SFT and DPO All training objectives\nin our study are fine-tuned using the ALMA-13B-LoRA\nmodel as a base. In Table 2 and 3, we observe that SFT\non preferred data marginally enhances the ALMA model\u2019s\ntranslation capability for xx\u2192en, and results in a slight de-\nterioration for en\u2192xx. Similarly, DPO slightly decreases\nmodel performance. In contrast, CPO demonstrates signifi-\ncant improvements across all translation directions.\n4.5. WMT\u201923 Results\nWe show the average results across all six directions in\nTable 4, and provide the performance in each direction in\nAppendix G due to the space constraint. Consistent with\nobservations from WMT\u201921 and WMT\u201922, ALMA-13B-R\nsurpasses contemporary moderate-size LLM-based trans-\nlators such as ALMA-13B-LoRA and TowerInstruct, and\neither matches or exceeds WMT winners.\n5. Analyses\nAll analyses use the WMT\u201921 and WMT\u201922 test sets, with\ntheir averaged performance being reported.\n5.1. Are Translations Really Better or Just\nMetric-Preferred?\nIn our study, since the preferred data is selected by reference-\nfree models and the same models are used for evaluation,\nwe investigate the potential for \u2019cheating\u2019 in the scoring\nprocess. Specifically, we question whether the improved\n6\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nTable 3. The overall results in xx\u2192en for WMT\u201921 and WMT\u201922. The usage of color and boldface are the same in Table 2.\nModels\nde\ncs\nis\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n78.74\n78.56\n88.82\n82.08\n83.11\n84.60\n80.88\n85.04\n76.16\nWMT Winners\n81.38\n83.59\n93.74\n82.47\n82.53\n85.65\n81.39\n85.60\n78.14\nGPT-4\n81.50\n84.58\n94.47\n82.52\n83.55\n88.48\n81.49\n85.90\n81.11\nALMA-13B-LoRA\n81.14\n83.57\n93.30\n81.96\n82.97\n83.95\n80.90\n85.49\n76.68\n+ SFT on preferred data\n81.36\n83.98\n93.84\n82.36\n83.15\n86.67\n81.32\n85.61\n80.20\n+ DPO\n81.13\n83.52\n93.25\n81.82\n82.69\n83.84\n80.89\n85.22\n76.09\n+ CPO (Ours, ALMA-13B-R)\n81.50\n83.97\n94.20\n82.63\n83.75\n88.03\n81.57\n85.73\n80.49\nzh\nru\nAvg.\nModels\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n77.09\n74.19\n90.70\n80.74\n79.59\n88.56\n79.91\n80.10\n85.77\nWMT Winners\n77.66\n73.28\n87.2\n81.71\n80.97\n90.91\n80.92\n81.19\n87.13\nGPT-4\n79.33\n77.65\n92.06\n81.57\n81.34\n90.95\n81.28\n82.60\n89.41\nALMA-13B-LoRA\n77.32\n74.41\n89.88\n81.31\n81.05\n89.89\n80.53\n81.50\n86.74\n+ SFT on preferred data\n78.32\n76.03\n90.65\n81.46\n81.17\n90.65\n80.96\n81.99\n88.40\n+ DPO\n77.50\n74.50\n89.94\n81.19\n80.88\n89.76\n80.51\n81.36\n86.58\n+ CPO (Ours, ALMA-13B-R)\n79.24\n77.17\n91.65\n81.72\n81.54\n91.18\n81.33\n82.43\n89.11\nTable 4. The average performance in WMT\u201923 across all 6 direc-\ntions, with the highest score highlighted in bold.\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n78.74\n75.56\n86.30\nWMT Winners\n80.57\n77.72\n88.24\nTowerInstruct\n80.31\n77.18\n88.11\nALMA-13B-LoRA\n79.48\n76.00\n87.16\n+ CPO (Ours, ALMA-13B-R)\n80.55\n78.97\n89.74\ntranslation scores reflect genuinely better translations or if\nthey simply align more closely with the evaluation model\u2019s\npreferences. This inquiry is addressed in two parts:\nAt the metric level, we examine if training a model on data\npreferred by a specific metric (such as KIWI-XXL) yields\nimprovements that are consistent across other metrics. To\ninvestigate this, we reconstruct the preference data using\nonly KIWI-XXL or XCOMET and re-train the ALMA-13B-\nLoRA model using the CPO method. The results, presented\nin Table 5, do not indicate a significant bias towards the met-\nric used for selecting preferred data. We observed similar\nand consistent improvements across all metrics, regardless\nof the specific metric used to select the preferred data. Con-\nsidering Comet-series models may be positive correlated,\nwe further evaluate ALMA-R using a non-comet metric,\nBLEURT (Sellam et al., 2020), and also observe significant\nimprovements in Appendix H. The inclusion of a third-party\nevaluation metric further substantiates the superior transla-\ntion quality of ALMA-R.\nAt the method level, we question whether training on metric-\npreferred data always leads to better scores on that metric,\nregardless of the method we use. However, the connection\nis not straightforward; for instance, SFT on preferred data\nparadoxically results in diminished performance across all\nthree metrics as shown in Table 2.\nTable 5. The influence of employing various reference-free models\nfor creating preference data. The results illustrates that the final\nperformance disparities are minimal whether using solely KIWI-\nXXL, XCOMET, or their combined ensemble.\nModels for Building Preference Data\nKIWI-22\nKIWI-XXL\nXCOMET\nTranslating to English (xx\u2192en)\nN/A (ALMA-13B-LoRA baseline)\n80.53\n81.50\n86.74\nKIWI-XXL\n81.33\n82.59\n88.82\nXCOMET\n81.27\n82.33\n89.17\nEnsemble of above (Original)\n81.33\n82.43\n89.11\nTranslating from English (en\u2192xx)\nN/A (ALMA-13B-LoRA baseline)\n82.48\n82.66\n92.76\nKIWI-XXL\n83.31\n85.87\n93.97\nXCOMET\n83.09\n85.43\n94.09\nEnsemble of above (Original)\n83.34\n85.74\n94.05\nConsequently, our analysis supports the robustness and va-\nlidity of using reference-free models like KIWI-XXL and\nXCOMET both for constructing preference data and for\nevaluation purposes, underscoring the absence of bias in\nthis approach. Furthermore, Table 5 demonstrates that the\nchoice between using KIWI-XXL, XCOMET, or an ensem-\nble of both has a minimal impact on the results.\n5.2. Ablation Study\nCPO Loss Components The CPO loss function consists of\ntwo components: Lprefer for preference learning, and LNLL,\nwhich ensures the model does not deviate significantly from\nthe preferred data distribution. To illustrate the significance\nof each term, we re-train the model exclusively with one of\nthe components. It is important to note that training solely\nwith LNLL equates to the baseline scenario of SFT on pre-\nferred data. As depicted in the left of Figure 4, the inclusion\nof both terms yields the optimal performance, while the\nabsence of either leads to a decrease in performance. In\nAppendix I, we also show that incorporating LNLL into the\n7\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nLprefer\nLNLL\nLprefer + LNLL\nLoss Objective\n80\n82\n84\n86\n88\n90\nAvg. Ref-free Eval\n82.81\n83.78\n84.29\n85.50\n85.84\n87.71\nxx\u2192en\nen\u2192xx\nALMA + Ref\nGPT-4 + Ref\nAll of them\nPreference Data\n80\n82\n84\n86\n88\n90\nAvg. Ref-free Eval\n83.70\n84.20\n84.29\n86.99\n86.66\n87.71\nxx\u2192en\nen\u2192xx\nFigure 4. Left: an ablation study evaluating the significance of individual components in the CPO loss function, specifically analyzing\nhow the preference learning loss Lprefer and the log-likelihood loss LNLL each contribute to enhancing translation performance. Right: An\nablation study assessing the significance of each component in the translation triplet. By excluding either ALMA or GPT-4 generated\ndata from the preference triplet and re-training the model, we evaluate their respective impacts. The findings highlight the importance of\nALMA-generated data for en\u2192xx translations and GPT-4 generated data for xx\u2192en translations.\nDPO loss yields significant improvements.\nPreference Data Components: Our preference data se-\nlection involves choosing preferred and dis-preferred trans-\nlations from a triplet consisting of outputs from GPT-4,\nALMA, and the gold reference. In the right of Figure 4, we\nemphasize the significance of the data generated by both\nALMA and GPT-4. The results indicate a notable decline in\nperformance when ALMA data is excluded in the en\u2192xx\ndirection. Conversely, omitting GPT-4 data leads to a signif-\nicant performance decrease in the xx\u2192en direction. This\ndemonstrates that data generated by both systems plays a\nhelpful role in enhancing model performance.\n5.3. Does The Quality of Dis-preferred Data Matter?\nIn our experimental setup, dis-preferred data, though origi-\nnating from strong translation models, receives the lowest\nscores when compared with two other translation outputs.\nA pertinent question arises: does the quality of dis-preferred\ndata significantly impact model performance, and can high-\nquality (albeit imperfect) dis-preferred data aid in translation\nimprovement? To explore this, we constructed a new set of\npreference data where the dis-preferred translations (yl) are\nartificially generated, as opposed to being naturally derived\nhigh-quality translations.\nIn this new dataset, the preferred translation (yw) remains\nthe best of the three translation candidates, selected in the\nsame manner as in Section 3.1. However, the dis-preferred\ntranslation is intentionally modified to be a noised version\nof yw. We applied random deletions of words with a prob-\nability of 0.15 and word swaps within a range of 1 with a\nprobability of 0.3, following the method suggested by Zeng\net al. (2023) for creating manually noised dis-preferred data.\nThis approach produces worse translations that are artificial.\nTable 6 compares the performance when using these manu-\nTable 6. An examination of the impact of dis-preferred data qual-\nity, contrasting noised data with natural, high-quality translations\nreceiving the lowest scores as dis-preferred data. The findings\nunderscore the importance of the quality of dis-preferred data.\nDis-Preferred Data\nKIWI-22\nKIWI-XXL\nXCOMET\nTranslating to English (xx\u2192en)\nManually Noised\n81.01\n82.18\n88.23\nNatural (Ours)\n81.33\n82.43\n89.11\nTranslating from English (en\u2192xx)\nManually Noised\n82.71\n83.13\n92.80\nNatural (Ours)\n83.34\n85.74\n94.05\nally noised dis-preferred data versus the original, naturally\noccurring high-quality dis-preferred data. The results show\na substantial decline in performance across all three metrics\nand both translation directions when the dis-preferred data is\nmanually noised, underscoring the importance of the quality\nof dis-preferred data in enhancing translation performance.\n6. Conclusion\nIn this study, we initially proposed the potential quality\nissues of gold references in the MT task, highlighting in-\nstances where advanced translation models can outperform\nthese references. This finding not only challenges model\ntraining via SFT, but also the evaluation procedure that uses\nreference-based metrics. Subsequently, we introduce Con-\ntrastive Preference Optimization, a more efficient variant\nof of DPO. This method leverages both model-generated\nand reference data to guide the model in avoiding near-\nperfect yet flawed translations and learning superior ones.\nOur developed model, ALMA-13B-R, stands out as the first\nmoderate-size LLM-based translation model to match, and\nin some cases surpass, the performance of GPT-4 and WMT\ncompetition winners, marking a significant advancement in\nthe field of MT.\n8\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nImpact Statements\nThis paper presents work whose goal is to advance the field\nof Machine Translation. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nAcknowledgements\nWe express our profound appreciation to Hieu Hoang,\nMarcin Junczys-Dowmunt, Huda Khayrallah, Thamme\nGowda, Vikas Raunak, Matt Post, Anoop Kunchukuttan, Ro-\nman Grundkiewicz, Philipp Koehn, Hany Hassan Awadalla,\nArul Menezes, and Vishal Chowdhary for their engaging\nand valuable discussions that greatly enriched our work.\nSpecial thanks to Tom Kocmi for his innovative suggestion\nto enhance numerical data visibility using a dynamic thresh-\nold determined by estimated accuracy. Our gratitude also\nextends to Pushpendre Rastogi and Joey Hejna for their in-\nsightful recommendations on the CPO theory. Furthermore,\nwe acknowledge the Unbabel Team for their valuable advice\non incorporating non-COMET metrics into our analysis.\nReferences\nAlmazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A.,\nCojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Lau-\nnay, J., Malartic, Q., Noune, B., Pannier, B., and Penedo,\nG. Falcon-40B: an open large language model with state-\nof-the-art performance. 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. In International conference on machine\nlearning, pp. 1597\u20131607. PMLR, 2020.\nChen, Y., Liu, Y., Meng, F., Chen, Y., Xu, J., and Zhou,\nJ. Improving translation faithfulness of large language\nmodels via augmenting instructions.\narXiv preprint\narXiv:2308.12674, 2023.\nFan, A., Bhosale, S., Schwenk, H., Ma, Z., El-Kishky, A.,\nGoyal, S., Baines, M., Celebi, O., Wenzek, G., Chaudhary,\nV., et al. Beyond english-centric multilingual machine\ntranslation. Journal of Machine Learning Research, 22\n(107):1\u201348, 2021.\nFreitag, M., Mathur, N., Lo, C.-k., Avramidis, E., Rei, R.,\nThompson, B., Kocmi, T., Blain, F., Deutsch, D., Stew-\nart, C., Zerva, C., Castilho, S., Lavie, A., and Foster,\nG.\nResults of WMT23 metrics shared task: Metrics\nmight be guilty but references are not innocent. In Koehn,\nP., Haddow, B., Kocmi, T., and Monz, C. (eds.), Pro-\nceedings of the Eighth Conference on Machine Trans-\nlation, pp. 578\u2013628, Singapore, December 2023. Asso-\nciation for Computational Linguistics. doi: 10.18653/\nv1/2023.wmt-1.51. URL https://aclanthology.\norg/2023.wmt-1.51.\nGuerreiro, N. M., Rei, R., van Stigt, D., Coheur, L.,\nColombo, P., and Martins, A. F. xcomet: Transparent\nmachine translation evaluation through fine-grained error\ndetection. arXiv preprint arXiv:2310.10482, 2023.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 9729\u20139738,\n2020.\nHejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S.,\nKnox, W. B., and Sadigh, D. Contrastive prefence learn-\ning: Learning from human feedback without rl. arXiv\npreprint arXiv:2310.13639, 2023.\nHendy, A., Abdelrehim, M., Sharaf, A., Raunak, V., Gabr,\nM., Matsushita, H., Kim, Y. J., Afify, M., and Awadalla,\nH. H.\nHow good are gpt models at machine trans-\nlation?\na comprehensive evaluation.\narXiv preprint\narXiv:2302.09210, 2023.\nHu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y.,\nWang, S., Wang, L., and Chen, W. LoRA: Low-rank adap-\ntation of large language models. In International Confer-\nence on Learning Representations, 2022. URL https:\n//openreview.net/forum?id=nZeVKeeFYf9.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJiao, W., Huang, J.-t., Wang, W., He, Z., Liang, T., Wang,\nX., Shi, S., and Tu, Z. ParroT: Translating during chat\nusing large language models tuned with human transla-\ntion and feedback. In Bouamor, H., Pino, J., and Bali, K.\n(eds.), Findings of the Association for Computational\nLinguistics: EMNLP 2023, pp. 15009\u201315020, Singa-\npore, December 2023a. Association for Computational\nLinguistics.\ndoi:\n10.18653/v1/2023.findings-emnlp.\n1001. URL https://aclanthology.org/2023.\nfindings-emnlp.1001.\nJiao, W., Wang, W., Huang, J.-t., Wang, X., and Tu, Z. Is\nchatgpt a good translator? a preliminary study. arXiv\npreprint arXiv:2301.08745, 2023b.\n9\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nKocmi, T., Avramidis, E., Bawden, R., Bojar, O.,\nDvorkovich, A., Federmann, C., Fishel, M., Freitag, M.,\nGowda, T., Grundkiewicz, R., Haddow, B., Koehn, P.,\nMarie, B., Monz, C., Morishita, M., Murray, K., Nagata,\nM., Nakazawa, T., Popel, M., Popovi\u00b4c, M., and Shma-\ntova, M. Findings of the 2023 conference on machine\ntranslation (WMT23): LLMs are here but not quite there\nyet. In Koehn, P., Haddow, B., Kocmi, T., and Monz, C.\n(eds.), Proceedings of the Eighth Conference on Machine\nTranslation, pp. 1\u201342, Singapore, December 2023. As-\nsociation for Computational Linguistics. URL https:\n//aclanthology.org/2023.wmt-1.1.\nKocmi, T., Zouhar, V., Federmann, C., and Post, M. Navi-\ngating the metrics maze: Reconciling score magnitudes\nand accuracies. arXiv preprint arXiv:2401.06760, 2024.\nKudugunta, S., Caswell, I., Zhang, B., Garcia, X.,\nChoquette-Choo, C. A., Lee, K., Xin, D., Kusupati, A.,\nStella, R., Bapna, A., and Firat, O. Madlad-400: A multi-\nlingual and document-level large audited dataset, 2023.\nLi, J., Zhou, H., Huang, S., Chen, S., and Chen, J. Eliciting\nthe translation ability of large language models via mul-\ntilingual finetuning with translation instructions. arXiv\npreprint arXiv:2305.15083, 2023.\nMaillard, J., Gao, C., Kalbassi, E., Sadagopan, K. R.,\nGoswami, V., Koehn, P., Fan, A., and Guzman, F. Small\ndata, big impact: Leveraging minimal data for effective\nmachine translation. In Rogers, A., Boyd-Graber, J., and\nOkazaki, N. (eds.), Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pp. 2740\u20132756, Toronto, Canada,\nJuly 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.154.\nURL https:\n//aclanthology.org/2023.acl-long.154.\nNLLB TEAM, Costa-juss`a, M. R., Cross, J., C\u00b8 elebi, O.,\nElbayad, M., Heafield, K., Heffernan, K., Kalbassi, E.,\nLam, J., Licht, D., Maillard, J., et al. No language left be-\nhind: Scaling human-centered machine translation. arXiv\npreprint arXiv:2207.04672, 2022.\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018.\nOpenAI. Gpt-4 technical report, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a\nmethod for automatic evaluation of machine translation.\nIn Isabelle, P., Charniak, E., and Lin, D. (eds.), Proceed-\nings of the 40th Annual Meeting of the Association for\nComputational Linguistics, pp. 311\u2013318, Philadelphia,\nPennsylvania, USA, July 2002. Association for Computa-\ntional Linguistics. doi: 10.3115/1073083.1073135. URL\nhttps://aclanthology.org/P02-1040.\nPost, M. A call for clarity in reporting BLEU scores. In\nProceedings of the Third Conference on Machine Trans-\nlation: Research Papers, pp. 186\u2013191, Brussels, Bel-\ngium, October 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W18-6319. URL https:\n//aclanthology.org/W18-6319.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct preference optimization: Your\nlanguage model is secretly a reward model. arXiv preprint\narXiv:2305.18290, 2023.\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-\nspeed: System optimizations enable training deep learn-\ning models with over 100 billion parameters. In Proceed-\nings of the 26th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, pp. 3505\u20133506,\n2020.\nRei, R., C. de Souza, J. G., Alves, D., Zerva, C., Farinha,\nA. C., Glushkova, T., Lavie, A., Coheur, L., and Mar-\ntins, A. F. T. COMET-22: Unbabel-IST 2022 submis-\nsion for the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT), pp.\n578\u2013585, Abu Dhabi, United Arab Emirates (Hybrid),\nDecember 2022. Association for Computational Linguis-\ntics. URL https://aclanthology.org/2022.\nwmt-1.52.\nRei, R., Guerreiro, N. M., Pombal, J., van Stigt, D., Tre-\nviso, M., Coheur, L., de Souza, J. G., and Martins, A. F.\nScaling up cometkiwi: Unbabel-ist 2023 submission\nfor the quality estimation shared task. arXiv preprint\narXiv:2309.11925, 2023.\nRobinson, J. D., Chuang, C.-Y., Sra, S., and Jegelka, S.\nContrastive learning with hard negative samples.\nIn\nInternational Conference on Learning Representations,\n2021. URL https://openreview.net/forum?\nid=CR1XOQ0UTh-.\nSellam, T., Das, D., and Parikh, A.\nBLEURT: Learn-\ning robust metrics for text generation.\nIn Jurafsky,\nD., Chai, J., Schluter, N., and Tetreault, J. (eds.), Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pp. 7881\u20137892, On-\nline, July 2020. Association for Computational Linguis-\n10\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\ntics. doi: 10.18653/v1/2020.acl-main.704. URL https:\n//aclanthology.org/2020.acl-main.704.\nTan, W., Heffernan, K., Schwenk, H., and Koehn, P. Multi-\nlingual representation distillation with contrastive learn-\ning. In Proceedings of the 17th Conference of the Eu-\nropean Chapter of the Association for Computational\nLinguistics, pp. 1469\u20131482, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nWu, Y. and Hu, G. Exploring prompt engineering with GPT\nlanguage models for document-level machine translation:\nInsights and findings. In Koehn, P., Haddow, B., Kocmi,\nT., and Monz, C. (eds.), Proceedings of the Eighth Con-\nference on Machine Translation, pp. 166\u2013169, Singapore,\nDecember 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.wmt-1.15. URL https:\n//aclanthology.org/2023.wmt-1.15.\nXu, H., Van Durme, B., and Murray, K. BERT, mBERT,\nor BiBERT? a study on contextualized embeddings for\nneural machine translation. In Moens, M.-F., Huang,\nX., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of\nthe 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 6663\u20136675, Online and Punta\nCana, Dominican Republic, November 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.\nemnlp-main.534.\nURL https://aclanthology.\norg/2021.emnlp-main.534.\nXu, H., Kim, Y. J., Sharaf, A., and Awadalla, H. H. A\nparadigm shift in machine translation: Boosting transla-\ntion performance of large language models, 2023.\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou,\nR., Siddhant, A., Barua, A., and Raffel, C.\nmT5:\nA massively multilingual pre-trained text-to-text trans-\nformer. In Toutanova, K., Rumshisky, A., Zettlemoyer,\nL., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell,\nR., Chakraborty, T., and Zhou, Y. (eds.), Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 483\u2013498, Online,\nJune 2021. Association for Computational Linguistics.\ndoi: 10.18653/v1/2021.naacl-main.41. URL https:\n//aclanthology.org/2021.naacl-main.41.\nYang, W., Li, C., Zhang, J., and Zong, C. Bigtrans: Aug-\nmenting large language models with multilingual trans-\nlation capability over 100 languages.\narXiv preprint\narXiv:2305.18098, 2023.\nZeng, J., Meng, F., Yin, Y., and Zhou, J. Tim: Teaching large\nlanguage models to translate with comparison. arXiv\npreprint arXiv:2307.04408, 2023.\nZhang, S., Fang, Q., Zhang, Z., Ma, Z., Zhou, Y., Huang,\nL., Bu, M., Gui, S., Chen, Y., Chen, X., et al. Bayling:\nBridging cross-lingual alignment and instruction follow-\ning through interactive translation for large language mod-\nels. arXiv preprint arXiv:2306.10968, 2023.\nZhu, W., Liu, H., Dong, Q., Xu, J., Kong, L., Chen, J., Li,\nL., and Huang, S. Multilingual machine translation with\nlarge language models: Empirical results and analysis.\narXiv preprint arXiv:2304.04675, 2023a.\nZhu, W., Lv, Y., Dong, Q., Yuan, F., Xu, J., Huang, S., Kong,\nL., Chen, J., and Li, L. Extrapolating large language mod-\nels to non-english by aligning languages. arXiv preprint\narXiv:2308.04948, 2023b.\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,\nA., Amodei, D., Christiano, P., and Irving, G. Fine-tuning\nlanguage models from human preferences. arXiv preprint\narXiv:1909.08593, 2019.\n11\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nAppendix Contents\nAppendix Sections\nContents\nAppendix A\nComprehensive Results of WMT\u201921 and WMT\u201922\nAppendix B\nPrompts Used for Translations\nAppendix C\nTheory of The CPO Loss\nAppendix D\nDetails and Influence of Human-Labeled Data\nAppendix E\nInformation of WMT winners\nAppendix F\nEstimated Accuracy with Human Agreements\nAppendix G\nExperimental Results on WMT\u201923\nAppendix H\nEvaluation on Non-Comet Metrics\nAppendix I\nEffectiveness of The BC Regularizer for DPO\nA. Comprehensive Results of WMT\u201921 And WMT\u201922\nWe show the comprehensive results for en\u2192xx In Table 7 and xx\u2192en in Table 8. In this section, our study additionally\nincludes results from recently released LLM-based translators, including Bayling-13B (Zhang et al., 2023), BigTranslate\n(Yang et al., 2023), ALMA-13B-LoRA (Xu et al., 2023), the zero-shot performances of LLaMA-1-13B (Touvron et al.,\n2023a) and LLaMA-2-13B (Touvron et al., 2023b). We also compare these with the most advanced current translation\nmodels, such as WMT competition winners, GPT-4, GPT-3.5-text-davinci-003, Google Translate, NLLB-3.3B, and\nMADLAD-10B (Kudugunta et al., 2023). Importantly, we also present the performance of ALMA-7B-R here, which is\nfine-tuning on AMLA-7B-LoRA with CPO method. Except for reference-free evaluation, we also report two commonly\nused reference-based metrics, sacreBLEU (Post, 2018; Papineni et al., 2002) and COMET-22 (Rei et al., 2022).\nIntroducing ALMA-7B-R In this study, we extend the ALMA-13B-R training methodology to a 7B model size, specifically\nfine-tuning ALMA-7B-LoRA using the CPO method with the same preference data as ALMA-13B-R. Consistent with our\nfindings from ALMA-13B-R, the application of CPO significantly enhances performance.\nComparing with Advanced Translation Models Our model, ALMA-13B-R, is benchmarked against the most advanced\ncurrent models, demonstrating performance comparable to GPT-4 and WMT winners. It surpasses leading commercial\ntranslation tools such as Google Translate in many cases and top multilingual translation models like NLLB, MADLAD-10B\nand GPT-3.5.\nStop Using BLEU BLEU, a metric extensively utilized for decades, often diverges from neural-based and reference-free\nmetrics, a phenomenon also observed in previous studies (Xu et al., 2023; Freitag et al., 2023). For instance, WMT\ncompetition winners often exhibit superior performance according to BLEU (or COMET-22), yet this is not corroborated\nby reference-free models. A case in point is the WMT winners scoring an exceptionally high 64.14 BLEU in cs\u2192en\ntranslations, significantly outperforming other models by 20 BLEU points. However, reference-free evaluations suggest\nthese translations are inferior to those generated by our models and GPT-4. We hypothesize that this discrepancy may\narise from WMT models being trained on domain-specific data closely related to the WMT test set, leading to high lexical\nmatches but lacking semantic depth as evaluated by neural-based metrics. While BLEU scores are effective for assessing\nbasic functionality in weaker models, their utility diminishes with advanced translation models capable of generating diverse\ntranslations. In such contexts, relying solely on BLEU for evaluation appears increasingly outdated.\nTowards Reference-Free Metrics Neural-based, reference-dependent metrics like COMET-22 demonstrate greater con-\nsistency with reference-free metrics and robustness compared to BLEU. For instance, with COMET-22, our models show\nsignificant improvements like other reference-free models over ALMA-13B-LoRA and comparable performance to GPT-4,\ne.g., 87.74 (Ours) vs. 87.68 (GPT-4) when en\u2192xx. However, it is important to note that, according to reference-free\nmetrics, gold references are often inferior to system-generated translations, potentially indicating quality issues in the\nreferences that could impact COMET-22 evaluations. Consequently, inconsistencies still exist between COMET-22 and\nreference-free models like XCOMET. For example, XCOMET rates ALMA-R model on average higher than WMT winners\n(89.11 vs. 87.13), while COMET-22 favors WMT winners (85.21 vs. 85.60). In line with the recommendations in Freitag\net al. (2023), we advocate for the use of reference-free models to circumvent the potential quality issues of references.\n12\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nTable 7. The full results in en\u2192xx for WMT\u201921 and WMT\u201922 including both reference-free and reference-based metrics. Bold numbers\ndenote the highest scores across all systems. Dark blue boxes indicates that the improvement over the original ALMA model achieves\nat least 80% estimated accuracy with the human judgement (Kocmi et al., 2024), while the lesser improvements are highlighted in\nshallow blue boxes. Decreases in performance are marked with yellow boxes. The asterisk (*) indicates that we directly utilized the\nreported translation outputs from Zhang et al. (2023) for evaluation purposes. Consequently, some baseline results for the is language are\nomitted in these instances.\nde\ncs\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n-\n-\n82.67\n84.01\n97.85\n-\n-\n83.19\n81.83\n90.27\nWMT Winners\n38.39\n87.21\n83.56\n83.70\n96.99\n45.92\n91.86\n85.31\n87.27\n94.38\nGPT-4\n34.58\n87.29\n83.48\n84.91\n97.56\n33.74\n90.81\n84.81\n85.35\n93.48\nGPT-3.5-text-davinci-003\n31.88\n85.61\n82.75\n81.71\n96.35\n31.31\n88.57\n82.93\n78.84\n89.91\nGoogle Translate*\n37.44\n88.01\n84.03\n85.33\n97.60\n48.10\n91.28\n84.55\n82.80\n91.94\nNLLB-3.3B*\n34.04\n86.24\n83.38\n82.47\n96.25\n36.34\n89.90\n84.20\n81.77\n91.57\nMADLAD-10B\n36.57\n86.73\n83.19\n83.06\n96.82\n40.17\n90.35\n84.05\n92.03\n91.79\nLLaMA-1-13B\n22.27\n80.62\n77.50\n70.53\n92.93\n16.83\n78.43\n72.17\n55.16\n72.53\nLLaMA-2-13B\n13.69\n75.55\n68.33\n55.98\n90.81\n0.87\n68.57\n61.38\n42.67\n74.26\nBayling-13B*\n25.59\n82.70\n80.01\n74.69\n94.50\n16.40\n78.22\n72.49\n53.70\n75.92\nBigTranslate\n22.13\n78.62\n75.40\n67.45\n90.22\n20.57\n80.11\n73.53\n60.27\n73.73\nALMA-7B-LoRA\n30.16\n85.45\n82.19\n80.70\n96.49\n30.17\n89.05\n83.27\n82.06\n90.82\n+ SFT on preferred data\n29.00\n85.42\n82.30\n80.44\n96.26\n30.64\n89.11\n83.46\n81.28\n90.26\n+ DPO\n28.87\n85.19\n82.02\n80.02\n96.22\n28.87\n88.78\n82.96\n81.03\n90.12\n+ CPO (Ours, ALMA-7B-R)\n26.23\n86.06\n82.97\n82.77\n97.11\n25.19\n89.61\n84.32\n84.81\n91.91\nALMA-13B-LoRA\n31.47\n85.62\n82.62\n81.64\n96.49\n32.38\n89.79\n84.14\n84.24\n92.38\n+ SFT on preferred data\n30.39\n86.01\n82.75\n81.85\n96.67\n31.60\n89.91\n84.14\n83.46\n91.99\n+ DPO\n30.50\n85.31\n82.40\n81.20\n96.40\n30.88\n89.53\n83.86\n83.45\n91.68\n+ CPO (Ours, ALMA-13B-R)\n27.72\n86.40\n83.28\n84.25\n97.48\n26.32\n90.29\n84.99\n87.06\n93.61\nis\nzh\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n-\n-\n80.51\n85.20\n91.52\n-\n-\n80.92\n81.70\n90.42\nWMT Winners\n33.28\n86.75\n81.77\n84.94\n91.61\n44.87\n86.69\n82.04\n81.13\n91.14\nGPT-4\n24.68\n85.08\n81.03\n81.21\n90.00\n44.41\n86.51\n81.73\n81.53\n90.79\nGPT-3.5-text-davinci-003\n15.89\n76.28\n73.96\n58.72\n72.67\n38.36\n85.76\n81.26\n80.4\n90.07\nGoogle Translate*\n-\n-\n-\n-\n-\n49.96\n87.37\n82.23\n80.62\n90.27\nNLLB-3.3B*\n-\n-\n-\n-\n-\n32.52\n81.57\n75.73\n67.14\n82.04\nMADLAD-10B\n23.57\n80.72\n77.09\n69.95\n80.84\n39.18\n83.14\n78.37\n72.82\n85.30\nLLaMA-1-13B\n1.43\n36.78\n36.59\n3.44\n23.89\n16.85\n70.91\n64.82\n47.92\n67.73\nLLaMA-2-13B\n2.36\n38.47\n31.07\n4.60\n36.21\n30.00\n79.70\n74.09\n65.06\n81.06\nBayling-13B*\n-\n-\n-\n-\n-\n37.90\n84.63\n79.94\n76.34\n87.44\nBigTranslate\n2.08\n37.40\n39.29\n9.39\n26.77\n19.17\n74.11\n65.96\n55.44\n72.69\nALMA-7B-LoRA\n25.19\n85.44\n81.12\n81.51\n89.94\n36.47\n84.87\n79.50\n77.14\n88.11\n+ SFT on preferred data\n24.26\n85.19\n80.87\n80.25\n89.15\n37.12\n85.36\n80.38\n78.16\n88.34\n+ DPO\n24.52\n85.20\n80.79\n80.42\n88.97\n35.22\n84.73\n79.42\n76.96\n87.72\n+ CPO (Ours, ALMA-7B-R)\n21.13\n85.80\n80.93\n82.35\n89.63\n31.19\n85.89\n81.42\n81.79\n89.55\nALMA-13B-LoRA\n26.68\n86.08\n81.71\n83.31\n91.20\n39.84\n85.96\n80.82\n79.96\n89.92\n+ SFT on preferred data\n25.26\n85.77\n81.48\n82.11\n90.30\n39.10\n85.99\n81.25\n80.51\n90.18\n+ DPO\n25.87\n85.86\n81.43\n82.66\n90.33\n38.85\n85.85\n80.74\n79.64\n89.58\n+ CPO (Ours, ALMA-13B-R)\n22.88\n86.85\n82.18\n85.68\n91.93\n34.06\n86.86\n82.25\n84.32\n92.03\nru\nAvg.\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n-\n-\n82.96\n84.62\n94.17\n-\n-\n82.05\n83.47\n92.85\nWMT Winners\n32.44\n89.51\n84.35\n87.01\n94.79\n38.98\n88.40\n83.41\n84.81\n93.78\nGPT-4\n28.74\n88.71\n83.64\n86.15\n94.30\n33.23\n87.68\n82.94\n83.83\n93.23\nGPT-3.5-text-davinci-003\n27.53\n86.64\n82.283\n80.28\n91.49\n28.99\n84.57\n80.64\n75.99\n88.10\nGoogle Translate*\n35.02\n88.91\n83.75\n84.26\n93.50\n-\n-\n-\n-\n-\nNLLB-3.3B*\n30.13\n87.51\n83.35\n82.31\n92.07\n-\n-\n-\n-\n-\nMADLAD-10B\n29.77\n86.16\n81.73\n79.42\n90.74\n33.85\n85.42\n80.89\n79.46\n89.10\nLLaMA-1-13B\n18.46\n79.16\n74.26\n64.72\n86.32\n15.17\n69.18\n65.07\n48.35\n68.68\nLLaMA-2-13B\n0.59\n63.84\n56.78\n38.53\n84.94\n9.50\n65.23\n58.33\n41.37\n73.46\nBayling-13B*\n12.76\n71.01\n67.89\n54.62\n85.63\n-\n-\n-\n-\n-\nBigTranslate\n16.14\n75.13\n69.22\n54.27\n76.92\n16.02\n69.07\n64.68\n49.36\n68.07\nALMA-7B-LoRA\n26.93\n87.05\n82.72\n82.60\n92.98\n29.78\n86.37\n81.76\n80.80\n91.67\n+ SFT on preferred data\n26.23\n86.88\n82.47\n81.79\n92.57\n29.45\n86.39\n81.90\n80.38\n91.32\n+ DPO\n25.94\n86.70\n82.20\n81.61\n92.53\n28.68\n86.12\n81.48\n80.01\n91.11\n+ CPO (Ours, ALMA-7B-R)\n23.31\n87.86\n83.45\n84.97\n94.15\n25.41\n87.04\n82.62\n83.34\n92.47\nALMA-13B-LoRA\n28.96\n87.53\n83.10\n84.17\n93.79\n31.87\n87.00\n82.48\n82.66\n92.76\n+ SFT on preferred data\n28.15\n87.66\n83.23\n84.15\n93.54\n30.90\n87.07\n82.57\n82.42\n92.54\n+ DPO\n28.27\n87.30\n82.94\n83.40\n93.25\n30.87\n86.77\n82.27\n82.07\n92.25\n+ CPO (Ours, ALMA-13B-R)\n24.15\n88.30\n83.98\n87.37\n95.22\n27.03\n87.74\n83.34\n85.74\n94.05\n13\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nTable 8. The full results in xx\u2192en for WMT\u201921 and WMT\u201922 including both reference-free and reference-based metrics. The usage of\ncolor and boldface are the same in Table 7. The asterisk (*) indicates that we directly utilized the reported translation outputs from Zhang\net al. (2023) for evaluation purposes. Consequently, some baseline results for the is language are omitted in these instances.\nde\ncs\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n-\n-\n78.74\n78.56\n88.82\n-\n-\n82.08\n83.11\n84.60\nWMT Winners\n33.34\n85.04\n81.38\n83.59\n93.74\n64.14\n89.00\n82.47\n82.53\n85.65\nGPT-4\n32.41\n85.35\n81.50\n84.58\n94.47\n46.86\n87.26\n82.52\n83.55\n88.48\nGPT-3.5-text-davinci-003\n30.78\n84.79\n81.24\n83.97\n92.78\n44.51\n86.16\n82.02\n82.19\n83.51\nGoogle Translate*\n33.25\n84.78\n81.36\n83.74\n93.71\n49.40\n86.95\n82.60\n81.99\n86.74\nNLLB-3.3B*\n29.46\n83.43\n80.98\n82.04\n91.26\n49.05\n85.92\n81.72\n80.27\n82.94\nMADLAD-10B\n32.77\n84.80\n81.13\n83.33\n93.53\n51.17\n87.18\n82.29\n82.37\n86.16\nLLaMA-1-13B\n29.66\n82.42\n78.77\n77.98\n89.99\n36.05\n81.57\n77.72\n70.80\n73.71\nLLaMA-2-13B\n31.06\n83.01\n79.47\n79.27\n91.10\n40.02\n83.27\n79.29\n74.21\n78.50\nBayling-13B*\n27.26\n83.03\n79.88\n80.02\n89.84\n33.81\n81.65\n78.04\n71.44\n71.68\nBigTranslate\n25.16\n81.54\n78.24\n77.73\n86.79\n34.81\n82.02\n77.91\n72.69\n71.38\nALMA-7B-LoRA\n29.56\n83.95\n80.63\n82.58\n92.35\n43.49\n85.93\n81.32\n81.42\n81.34\n+ SFT on preferred data\n30.51\n84.39\n80.86\n82.72\n93.19\n44.44\n86.17\n81.97\n81.95\n84.55\n+ DPO\n29.38\n84.02\n80.63\n82.47\n92.26\n42.60\n85.87\n81.33\n81.30\n81.10\n+ CPO (Ours, ALMA-7B-R)\n30.52\n84.61\n81.13\n83.11\n93.85\n42.92\n86.29\n82.16\n82.29\n85.76\nALMA-13B-LoRA\n31.14\n84.56\n81.14\n83.57\n93.30\n45.28\n86.47\n81.96\n82.97\n83.95\n+ SFT on preferred data\n31.80\n84.83\n81.36\n83.98\n93.84\n46.17\n86.83\n82.36\n83.15\n86.67\n+ DPO\n30.99\n84.51\n81.13\n83.52\n93.25\n44.95\n86.36\n81.82\n82.69\n83.84\n+ CPO (Ours, ALMA-13B-R)\n30.89\n84.95\n81.50\n83.97\n94.20\n44.39\n86.85\n82.63\n83.75\n88.03\nis\nzh\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n-\n-\n80.88\n85.04\n76.16\n-\n-\n77.09\n74.19\n90.70\nWMT Winners\n41.60\n86.98\n81.39\n85.60\n78.14\n33.49\n81.02\n77.66\n73.28\n87.20\nGPT-4\n41.29\n87.21\n81.49\n85.90\n81.11\n23.82\n82.46\n79.33\n77.65\n92.06\nGPT-3.5-text-davinci-003\n31.88\n82.13\n78.72\n77.53\n66.44\n24.98\n81.62\n78.91\n76.64\n90.92\nGoogle Translate*\n-\n-\n-\n-\n-\n28.60\n80.82\n77.87\n74.27\n87.69\nNLLB-3.3B*\n-\n-\n-\n-\n-\n21.08\n76.93\n75.40\n68.83\n84.43\nMADLAD-10B\n39.49\n87.06\n81.40\n85.52\n80.43\n21.29\n78.53\n76.72\n72.10\n87.12\nLLaMA-1-13B\n11.01\n60.82\n57.76\n30.38\n20.87\n16.81\n74.32\n70.93\n62.37\n80.13\nLLaMA-2-13B\n15.77\n66.35\n63.91\n42.75\n28.03\n21.81\n78.10\n75.09\n70.31\n85.68\nBayling-13B*\n-\n-\n-\n-\n-\n20.10\n77.72\n75.08\n68.32\n86.51\nBigTranslate\n6.45\n54.65\n50.55\n18.77\n17.44\n14.94\n75.11\n71.94\n65.25\n85.00\nALMA-7B-LoRA\n35.64\n86.09\n80.57\n84.65\n75.02\n23.64\n79.78\n76.81\n73.65\n83.94\n+ SFT on preferred data\n38.58\n86.47\n81.09\n85.23\n78.87\n23.19\n80.50\n77.74\n74.91\n89.81\n+ DPO\n35.25\n85.96\n80.53\n84.44\n75.19\n23.20\n79.91\n76.83\n73.51\n89.22\n+ CPO (Ours, ALMA-7B-R)\n38.64\n86.66\n81.24\n85.13\n79.14\n22.45\n80.95\n78.47\n75.72\n90.74\nALMA-13B-LoRA\n36.95\n86.42\n80.90\n85.49\n76.68\n25.46\n80.21\n77.32\n74.41\n89.88\n+ SFT on preferred data\n39.60\n86.88\n81.32\n85.61\n80.20\n24.54\n81.08\n78.32\n76.03\n90.65\n+ DPO\n36.16\n86.30\n80.89\n85.22\n76.09\n25.17\n80.42\n77.50\n74.50\n89.94\n+ CPO (Ours, ALMA-13B-R)\n39.67\n87.14\n81.57\n85.73\n80.49\n23.23\n81.64\n79.24\n77.17\n91.65\nru\nAvg.\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nBLEU\nCOMET-22\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n-\n-\n80.74\n79.59\n88.56\n-\n-\n79.91\n80.10\n85.77\nWMT Winners\n45.18\n85.95\n81.71\n80.97\n90.91\n43.55\n85.60\n80.92\n81.19\n87.13\nGPT-4\n41.09\n85.87\n81.57\n81.34\n90.95\n37.09\n85.63\n81.28\n82.60\n89.41\nGPT-3.5-text-davinci-003\n38.52\n84.8\n81.14\n79.95\n89.29\n34.13\n83.90\n80.41\n80.06\n84.59\nGoogle Translate*\n43.66\n84.81\n81.02\n79.66\n89.40\n-\n-\n-\n-\n-\nNLLB-3.3B*\n40.12\n83.95\n80.87\n78.37\n87.85\n-\n-\n-\n-\n-\nMADLAD-10B\n42.53\n84.91\n80.86\n79.24\n88.65\n37.45\n84.50\n80.48\n80.51\n87.18\nLLaMA-1-13B\n34.65\n81.90\n78.29\n74.37\n84.13\n25.64\n76.21\n72.69\n63.18\n69.77\nLLaMA-2-13B\n36.50\n82.91\n79.14\n76.50\n86.12\n29.03\n78.73\n75.38\n68.61\n73.89\nBayling-13B*\n33.94\n82.07\n78.72\n74.45\n83.28\n-\n-\n-\n-\n-\nBigTranslate\n29.06\n78.10\n74.35\n66.46\n72.78\n22.08\n74.28\n70.60\n60.18\n66.68\nALMA-7B-LoRA\n39.21\n84.84\n80.94\n80.19\n88.50\n34.31\n84.12\n80.05\n80.50\n84.23\n+ SFT on preferred data\n39.06\n85.00\n81.05\n80.47\n89.54\n35.16\n84.51\n80.54\n81.06\n87.19\n+ DPO\n38.40\n84.71\n80.83\n80.04\n88.34\n33.77\n84.09\n80.03\n80.35\n85.22\n+ CPO (Ours, ALMA-7B-R)\n38.42\n85.11\n81.34\n80.69\n90.10\n34.59\n84.72\n80.87\n81.39\n87.92\nALMA-13B-LoRA\n40.27\n85.27\n81.31\n81.05\n89.89\n35.82\n84.59\n80.53\n81.50\n86.74\n+ SFT on preferred data\n40.55\n85.44\n81.46\n81.17\n90.65\n36.53\n85.01\n80.96\n81.99\n88.40\n+ DPO\n39.12\n85.14\n81.19\n80.88\n89.76\n35.28\n84.55\n80.51\n81.36\n86.58\n+ CPO (Ours, ALMA-13B-R)\n39.06\n85.45\n81.72\n81.54\n91.18\n35.45\n85.21\n81.33\n82.43\n89.11\n14\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nB. Prompts for Translations\nAdhering to the prompt format for translation as utilized by Hendy et al. (2023) for GPT models, we employ the same\nprompt for GPT-4 in our study. Similarly, we use the same prompt employed by Xu et al. (2023) for ALMA models. Prompts\nare depicted in Figure 5.\nSystem: \nYou are a helpful translator and only output the result. \nUser: \n### Translate this from <source language> to <target language>, <source language>:\n<source sentence>\n### <target language>:\n------------------------------------------------------------------------------------------------------------\nTranslate this from <source language> to <target language>:\n<source language>: <source sentence>\n<target language>:\nGPT-4 Prompt\nALMA Prompt\nFigure 5. The prompts employed for GPT-4 and ALMA models to perform translations.\nC. Theory\nC.1. Proof of The Upper Boundary\nTheorem 1. When \u03c0ref is defined as \u03c0w, an ideal policy that precisely aligns with the true data distribution of preferred\ndata, the DPO loss L(\u03c0\u03b8; \u03c0w) + C is upper bounded by L(\u03c0\u03b8; U), where C is a constant.\nProof. \u03c0w represents an ideal policy that perfectly aligns the true data distribution of the preferred data. Hence, for any\ngiven data point (x, yw, yl) from the preference dataset D, the conditions \u03c0w(yw|x) = 1 and 0 \u2264 \u03c0w(yl|x) \u2264 1 hold true.\nConsequently, under this setup, the predictions for preferred data do not require reweighting by the reference model, and the\nDPO loss L(\u03c0\u03b8; \u03c0w) can be reformulated as follows :\nL(\u03c0\u03b8; \u03c0w) = \u2212E(x,yw,yl)\u223cD\nh\nlog \u03c3\n\u0010\n\u03b2 log \u03c0\u03b8(yw|x)\n\u03c0w(yw|x) \u2212 \u03b2 log \u03c0\u03b8(yl|x)\n\u03c0w(yl|x)\n\u0011i\n= \u2212E(x,yw,yl)\u223cD\nh\nlog \u03c3\n\u0010\n\u03b2 log \u03c0\u03b8(yw|x) \u2212 \u03b2 log \u03c0\u03b8(yl|x) + \u03b2 log \u03c0w(yl|x)\n\u0011i\n.\nAfter expanding the Sigmoid function, the loss becomes to:\nL(\u03c0\u03b8; \u03c0w) = \u2212E(x,yw,yl)\u223cD\nh\nlog\n\u0010\n1\n1 + e\u2212\u03b2 log \u03c0\u03b8(yw|x)+\u03b2 log \u03c0\u03b8(yl|x)\u2212\u03b2 log \u03c0w(yl|x)\n\u0011i\n= \u2212E(x,yw,yl)\u223cD\nh\nlog\n\u0010\n1\n1 +\n\u03c0\u03b8(yl|x)\u03b2\n\u03c0\u03b8(yw|x)\u03b2\u00b7\u03c0w(yl|x)\u03b2\n\u0011i\n= \u2212E(x,yw,yl)\u223cD\nh\nlog \u03c0\u03b8(yw|x)\u03b2 + log \u03c0w(yl|x)\u03b2 \u2212 log\n\u0010\n\u03c0\u03b8(yw|x)\u03b2 \u00b7 \u03c0w(yl|x)\u03b2 + \u03c0\u03b8(yl|x)\u03b2\u0011i\n.\nGiven that \u03c0w is a fixed model and log \u03c0w(yl|x)\u03b2 does not participate in gradient calculations or parameter updates, the\nabove loss function is equivalent when we omit the term log \u03c0w(yl|x)\u03b2. Therefore, optimizing L(\u03c0\u03b8; \u03c0w) is equivalent to\n15\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\noptimizing L\u2032(\u03c0\u03b8; \u03c0w) as we define below:\nL\u2032(\u03c0\u03b8; \u03c0w)\n\u2206= L(\u03c0\u03b8; \u03c0w) + E(x,yl)\u223cD\nh\nlog \u03c0w(yl|x)\u03b2i\n|\n{z\n}\nC in the Theorem\n= \u2212E(x,yw,yl)\u223cD\nh\nlog \u03c0\u03b8(yw|x)\u03b2 \u2212 log\n\u0010\n\u03c0\u03b8(yw|x)\u03b2 \u00b7 \u03c0w(yl|x)\u03b2 + \u03c0\u03b8(yl|x)\u03b2\u0011i\n.\nConsidering that 0 \u2264 \u03c0w(yl|x) \u2264 1, the loss can be upper bounded as follows:\nL\u2032(\u03c0\u03b8; \u03c0w) \u2264 \u2212E(x,yw,yl)\u223cD\nh\nlog \u03c0\u03b8(yw|x)\u03b2 \u2212 log\n\u0010\n\u03c0\u03b8(yw|x)\u03b2 \u00b7 1 + \u03c0\u03b8(yl|x)\u03b2\u0011i\n= \u2212E(x,yw,yl)\u223cD\nh\nlog \u03c3\n\u0010\n\u03b2 log \u03c0\u03b8(yw|x) \u2212 \u03b2 log \u03c0\u03b8(yl|x)\n\u0011i\n= L(\u03c0\u03b8; U).\nTherefore, L(\u03c0\u03b8; \u03c0w) + C is upper bounded by L(\u03c0\u03b8; U), where C = E(x,yl)\u223cD\nh\nlog \u03c0w(yl|x)\u03b2i\n.\nC.2. BC Regularizer Simplification\nThe contrastive preference optimization is originally defined as minimizing L(\u03c0\u03b8; U) under the constraint of minimizing the\ndifference between preferred data distribution and outputs of the learnable policy:\nmin\n\u03b8\nL(\u03c0\u03b8, U) s.t. E(x,yw)\u223cD\nh\nKL(\u03c0w(yw|x)||\u03c0\u03b8(yw|x))\ni\n< \u03f5.\nThis is equivalent to the following objective via Lagrangian duality:\nmin\n\u03b8\nL(\u03c0\u03b8, U) + \u03bb \u00b7 E(x,yw)\u223cD\nh\nKL(\u03c0w(yw|x)||\u03c0\u03b8(yw|x))\ni\n,\nwhere \u03bb is a hyperparamter and we set to 1. The optimization can be further optimized by expanding the KL divergence:\nLCPO = L(\u03c0\u03b8, U) + E(x,yw)\u223cD\nh\nKL(\u03c0w(yw|x)||\u03c0\u03b8(yw|x))\ni\n= L(\u03c0\u03b8, U) + E(x,yw)\u223cD\nh\n\u03c0w(yw|x) \u00b7 log\n\u0010\n\u03c0w(yw|x)\n\u0011\n\u2212 \u03c0w(yw|x) \u00b7 log\n\u0010\n\u03c0\u03b8(yw|x)\n\u0011i\n= L(\u03c0\u03b8, U) + E(x,yw)\u223cD\nh\n1 \u00b7 0 \u2212 1 \u00b7 log\n\u0010\n\u03c0\u03b8(yw|x)\n\u0011i\n= L(\u03c0\u03b8, U) \u2212 E(x,yw)\u223cD\nh\nlog\n\u0010\n\u03c0\u03b8(yw|x)\n\u0011i\n.\nThis results in the final formulation of our CPO loss function.\nD. Details And Influence of Human-Labeled Preference Data\nTL;DR: Our analysis indicates that our human-labeled data has a relatively minimal impact, probably due to a high\nproportion of tied translations and potential human bias in the evaluation process.\nD.1. Data Construction Details\nThe human-labeled dataset we used is pair-wise and differs from the triplet format of our main dataset. It focuses exclusively\non two language directions, en\u2192de and en\u2192zh, resulting in an additional 2K sentences. The English source sentences,\nselected from Wikipedia, undergo a filtering process to remove time stamps and URLs. Each sentence is translated using\nGoogle Translate and GPT-4, with human evaluators then assigning their preference between these two translations. The\ndistribution of preferences, indicating the number of times translations from Google or GPT-4 were favored or instances\nwhere they tied, is detailed in Table 9.\n16\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nTable 9. The statistic of how many translations win or tie by each system evaluated by human.\nGoogle Wins\nGPT-4 Wins\nTies\nen\u2192de\n418\n435\n203\nen\u2192zh\n362\n412\n282\nD.2. Influence on Performance\nGiven that our model operates in a many-to-many translation format and the additional data is specific to only de and\nzh directions, we anticipate changes in performance when translating into these languages, but not in others. To assess\nthe impact of the human-labeled data, we conducted a comparison between models exclusively fine-tuned on triplet data\nand those fine-tuned on both triplet and human-labeled data. The training approach remained consistent, utilizing the\nALMA-13B-LoRA model fine-tuned via CPO. It\u2019s important to note that tied data were excluded from this analysis due to\ntheir lack of clear preference.\nResults and Analysis We show the detailed results for en\u2192xx and xx\u2192en in Table 10 and 11, respectively. The inclusion\nof human-labeled preference data does not significantly enhance overall translation performance. For en\u2192zh, marginal\nimprovements are observed, though they are minimal. Conversely, for en\u2192de, a slight decline in performance is noted. In\nsummary, the addition of human-labeled data shows no substantial difference in the en\u2192xx direction, and a minor decrease\nin performance for xx\u2192en on average. We hypothesize that the limited impact of these human-labeled data may stem from\na high proportion of tied evaluations and potential human bias in the evaluation process. For instance, there are instances\nwhere the author consider GPT-4\u2019s translations to be superior, while human evaluators favor those produced by Google\nTranslate.\nTable 10. A comparison of translation performance when utilizing solely triplet data versus a combination of triplet data and human-labeled\ndata (our original setup) in the en\u2192xx direction. The bold number indicates superior performance. There is not obvious performance\ndifference adding our human-labeled data.\nDataset\nde\ncs\nis\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nOnly Triplet Data\n83.43\n84.63\n97.56\n84.97\n87.24\n93.50\n82.05\n85.37\n91.83\nTriplet Data + Human-Labeled Data\n83.28\n84.25\n97.48\n84.99\n87.06\n93.61\n82.18\n85.68\n91.93\nDataset\nzh\nru\nAvg.\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nOnly Triplet Data\n82.15\n84.08\n91.59\n84.05\n87.43\n95.26\n83.33\n85.75\n93.95\nTriplet Data + Human-Labeled Data\n82.25\n84.32\n92.03\n83.98\n87.37\n95.22\n83.34\n85.74\n94.05\nTable 11. A comparison of translation performance when utilizing solely triplet data versus a combination of triplet data and human-labeled\ndata (our original setup) in the en\u2192xx direction. The bold number indicates superior performance. Interestingly, the inclusion of our\nhuman-labeled data results in a slight decrease in average performance.\nDataset\nde\ncs\nis\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nOnly Triplet Data\n81.57\n84.25\n94.32\n82.68\n83.70\n87.97\n81.63\n85.87\n80.89\nTriplet Data + Human-Labeled Data\n81.50\n83.97\n94.20\n82.63\n83.75\n88.03\n81.57\n85.73\n80.49\nDataset\nzh\nru\nAvg.\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nOnly Triplet Data\n79.34\n77.31\n91.76\n81.76\n81.63\n91.34\n81.40\n82.55\n89.26\nTriplet Data + Human-Labeled Data\n79.24\n77.17\n91.65\n81.72\n81.54\n91.18\n81.33\n82.43\n89.11\nE. WMT Winner Systems\nE.1. Systems For WMT\u201921 And WMT\u201922\nThe WMT competition winners for each direction as reported in WMT\u201921 and WMT\u201922 correspond to those used by Hendy\net al. (2023). For more detailed information, we direct readers to this paper.\n17\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nE.2. Systems For WMT\u201923\nFor the de\u2194en and zh\u2194en language pairs, we selected the translation systems that attained the highest human rankings\nbased on source-based Direct Assessment and Scalar Quality Metrics (DA+SQM). For de\u2194ru, in the absence of human\nrankings for these directions in Kocmi et al. (2023), we opted for the model with the highest COMET-22 scores as reported\nin Kocmi et al. (2023). Details about these models are available in Table 12.\nTable 12. The list of WMT\u201923 winners served for each language direction.\nSystems\nLanguage Pair\nONLINE-B\nen-de\nONLINE-A\nde-en\nLan-BridgeMT (Wu & Hu, 2023)\nen-zh\nLan-BridgeMT (Wu & Hu, 2023)\nzh-en\nONLINE-G\nen-ru\nONLINE-Y\nru-en\nF. Estimated Accuracy with Human Agreements\nIn the paper, we adopt a new approach for highlighting improvements within tables, moving beyond the standard practice\nof specifying a static improvement threshold in metric y by score x. Instead, our threshold is dynamic, calibrated to the\nminimal metric difference x in metric y that yields a perceptible distinction between two systems as recognized by humans\n(Kocmi et al., 2024). For instance, to align with human judgments at an 80% concordance rate, the required improvement\nmargin is \u2265 1.24 for both KIWI-XXL and COMET-XXL, and \u2265 0.53 for KIWI-22. A comprehensive delineation of these\nthresholds can be found in Table 13.\nTable 13. Thresholds and estimated accuracies for each metric used in our paper.\nEstimated\nAccuracy\nCoin toss\n50%\n55%\n60%\n65%\n70%\n75%\n80%\n85%\n90%\n95%\nBLEU\n0.27\n0.52\n0.78\n1.06\n1.39\n1.79\n2.34\n3.35\n-\n-\nComet-22\n0.03\n0.10\n0.18\n0.26\n0.35\n0.45\n0.56\n0.71\n0.94\n1.53\nKIWI-22\n0.01\n0.08\n0.16\n0.24\n0.33\n0.42\n0.53\n0.67\n0.85\n1.18\nXCOMET-XXL\n0.02\n0.19\n0.37\n0.56\n0.76\n0.98\n1.24\n1.55\n1.99\n2.74\nKIWI-XXL\n0.06\n0.22\n0.39\n0.57\n0.77\n0.98\n1.24\n1.58\n2.08\n3.39\nG. Full Results of WMT\u201923\nThe comprehensive results of WMT\u201923 are presented in Table 14. Similar to its performance in WMT\u201921 and WMT\u201922,\nALMA-13B-R performs best on average among the SoTA translation models.\n18\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nTable 14. The full results of WMT\u201923. The highest score among all systems are bold. dark blue boxes indicates that the improvement\nover the original ALMA model achieves at least 80% estimated accuracy with the human judgement (Kocmi et al., 2024), while the lesser\nimprovements are highlighted in shallow blue boxes.\nde\u2192en\nzh\u2192en\nru\u2192en\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n78.93\n75.96\n84.23\n74.46\n68.80\n83.51\n79.46\n77.84\n83.60\nWMT Winners\n79.37\n76.18\n84.35\n80.17\n79.53\n92.25\n80.88\n79.21\n86.22\nTowerInstruct\n79.67\n77.60\n86.28\n79.84\n78.13\n91.75\n80.85\n80.03\n87.76\nMADLAD-10B\n78.52\n75.50\n83.85\n77.68\n73.72\n88.07\n79.65\n77.58\n85.15\nALMA-13B-LoRA\n79.36\n76.79\n85.07\n78.83\n76.71\n90.73\n80.79\n80.14\n86.94\n+ CPO (Ours, ALMA-13B-R)\n79.87\n77.69\n86.62\n80.01\n78.42\n92.36\n81.11\n80.95\n88.75\nen\u2192de\nen\u2192zh\nen\u2192ru\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nKIWI-22\nKIWI-XXL\nXCOMET\nGold Reference\n80.12\n77.93\n88.91\n79.60\n73.47\n86.15\n79.87\n79.36\n91.41\nWMT Winners\n80.80\n77.26\n87.94\n79.70\n74.20\n87.24\n82.51\n79.95\n91.41\nTowerInstruct\n80.13\n75.34\n86.55\n80.03\n74.85\n86.74\n81.33\n77.14\n89.59\nMADLAD-10B\n77.48\n70.87\n86.18\n74.63\n62.07\n79.12\n79.24\n72.40\n86.64\nALMA-13B-LoRA\n78.79\n73.40\n85.61\n78.92\n72.95\n85.13\n80.21\n76.02\n89.48\n+ CPO (Ours, ALMA-13B-R)\n79.85\n77.05\n89.79\n80.48\n78.17\n88.34\n81.97\n81.52\n92.56\nH. Evaluation on ALMA-R with Non-Comet Metric\nConcerns may arise regarding the similar training procedure of COMET metrics, leading to high correlation among COMET\nmodels, which potentially undermine the validity of our analysis in Section 5.1. To address this, we also consider BLEURT-\n20 (Sellam et al., 2020), a non-COMET and neural-based (but reference-based evaluation) metric. We present BLEURT\nscores for ALMA-13B-LoRA and ALMA-13B-R in Table 15. Notably, even when preference data is constructed using\nCOMET-based evaluations, significant improvements in non-COMET scores are observed. This strengthens our findings\nthat translations produced by ALMA-R are indeed superior and robust.\nTable 15. The BLEURT-20 score comparsion between ALMA-13B-LoRA and ALMA-13B-R\nBLEURT-20\nde\ncs\nis\nzh\nru\nAvg.\nTranslating to English (xx\u2192en)\nALMA-13B-LoRA\n73.20\n76.65\n75.87\n67.37\n76.7\n73.96\nALMA-13B-R\n73.62\n76.94\n76.98\n69.48\n76.91\n74.79\nTranslating from English (en\u2192xx)\nALMA-13B-LoRA\n75.51\n80.93\n73.19\n70.54\n74.94\n75.02\nALMA-13B-R\n77.20\n81.87\n73.43\n71.51\n76.19\n76.04\nI. The Effectiveness of The BC Regularizer for DPO\nThe DPO loss LDPO = L(\u03c0\u03b8, \u03c0ref) can also be utilized by adding our additional BC regularizer:\nmin\n\u03b8\nL(\u03c0\u03b8, \u03c0ref) \u2212 E(x,yw)\u223cD\nh\nlog\n\u0010\n\u03c0\u03b8(yw|x)\n\u0011i\n.\nIn Table 16, we demonstrate that incorporating LNLL into the DPO objective results in notable enhancements for translations\nboth to and from English. This observation hints at why Lprefer, as an approximation of LDPO, performs effectively, while\nthe original DPO loss does not. It appears that the DPO loss lacks the BC regularizer, which steers the model towards the\npreferred data distribution. Although combining DPO with the BC regularizer could yield similar performance to CPO, it\nincurs double the memory cost and FLOPs per token in the forward pass. The original DPO loss shows the possibility of\nfailure to improve the model performance in preference learning, so we here highlight the significance of incorporating BC\nregularization. Importantly, Table 16 shows that Lprefer is a successful approximation of the DPO loss, offering savings in\nmemory and speed, and it can even outperform the original BC-regularized DPO loss LDPO + LNLL.\n19\nContrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\nTable 16. The impact of applying LNLL to the original DPO loss.\nLoss Objective\nKIWI-22\nKIWI-XXL\nXCOMET\nMemory Cost\nFLOPs/tok\nTranslating to English (xx\u2192en)\nLDPO\n80.51\n81.36\n86.58\n2\u00d7\n2\u00d7\nLDPO + LNLL\n81.28\n82.42\n89.05\n2\u00d7\n2\u00d7\nLprefer + LNLL (CPO)\n81.33\n82.43\n89.11\n1\u00d7\n1\u00d7\nTranslating from English (en\u2192xx)\nLDPO\n82.27\n82.07\n92.25\n2\u00d7\n2\u00d7\nLDPO + LNLL\n83.13\n84.74\n93.53\n2\u00d7\n2\u00d7\nLprefer + LNLL (CPO)\n83.34\n85.74\n94.05\n1\u00d7\n1\u00d7\n20\n"
  },
  {
    "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models",
    "link": "https://arxiv.org/pdf/2401.06951.pdf",
    "upvote": "23",
    "text": "E2-LLM: Efficient and Extreme Length Extension of\nLarge Language Models\nJiaheng Liu*1, Zhiqi Bai*1, Yuanxing Zhang1, Chenchen Zhang1, Yu Zhang1,\nGe Zhang2, Jiakai Wang1, Haoran Que1, Yukang Chen3, Wenbo Su1, Tiezheng Ge1,\nJie Fu4, Wenhu Chen2, Bo Zheng1\n1Alibaba Group, 2University of Waterloo, 3The Chinese University of Hong Kong,\n4The Hong Kong University of Science and Technology\n{ljh411989, baizhiqi.bzq}@taobao.com\nAbstract\nTypically, training LLMs with long context\nsizes is computationally expensive, requiring\nextensive training hours and GPU resources.\nExisting long-context extension methods usu-\nally need additional training procedures to\nsupport corresponding long-context windows,\nwhere the long-context training data (e.g., 32k)\nis needed, and high GPU training costs are as-\nsumed. To address the aforementioned issues,\nwe propose an Efficient and Extreme length\nextension method for Large Language Mod-\nels, called E2-LLM, with only one training pro-\ncedure and dramatically reduced computation\ncost, which also removes the need to collect\nlong-context data. Concretely, first, the training\ndata of our E2-LLM only requires a short length\n(e.g., 4k), which reduces the tuning cost greatly.\nSecond, the training procedure on the short\ntraining context window is performed only one\ntime, and we can support different evaluation\ncontext windows at inference. Third, in E2-\nLLM, based on RoPE position embeddings, we\nintroduce two different augmentation methods\non the scale and position index parameters for\ndifferent samples in training. It aims to make\nthe model more robust to the different relative\ndifferences when directly interpolating the ar-\nbitrary context length at inference. Compre-\nhensive experimental results on multiple bench-\nmark datasets demonstrate the effectiveness of\nour E2-LLM on challenging long-context tasks.\n1\nIntroduction\nLarge language models (LLMs) usually have a pre-\ndefined context window length. For example, in-\nputs to LLaMA models (Touvron et al., 2023a,b)\nmust be fewer than 2,048 or 4096 tokens. This pre-\nset context window limit is frequently exceeded in\napplications such as long conversations, document\nsummarization, or long-term reasoning (Zheng\net al., 2023; Chen et al., 2023a). For these ap-\nplications, LLMs with longer context windows\n* First two authors contributed equally.\nare preferred. However, training an LLM from\nscratch with long context windows requires signifi-\ncant training costs. To address this problem, many\nlong-context extension methods (Peng et al., 2023;\nChen et al., 2023b) have been proposed to extend\nthe context window of an existing pre-trained LLM.\nOne straightforward approach is called direct\nextrapolation, which fine-tunes an existing pre-\ntrained LLM with a longer context window,\nHowever, the authors of Position Interpolation\n(PI) (Chen et al., 2023a) observe that models\ntrained by direct extrapolation adapt to long context\nwindows very slowly and direct extrapolation is in-\nefficient in extending substantially longer context\nwindows.\nAs shown in Fig. 1 (a), existing long-context\nextension methods (e.g., PI) usually need addi-\ntional training procedures to support corresponding\nlonger-context windows, where the long-context\ntraining data is needed to collect, and training with\nhigh GPU memory usage is needed for each context\nwindow.\nTo address the aforementioned issues, as shown\nin Fig. 1 (b), we propose an Efficient and Extreme\nlength extension method of LLMs, called E2-LLM,\nto support the extreme length extension of LLMs\nwith only one training procedure on short-context\ndata and limited computation cost. Based on E2-\nLLM, we can support different evaluation context\nwindows well at inference by only changing one hy-\nperparameter of RoPE (Su et al., 2021) according\nto the input context length. Specifically, first, in E2-\nLLM, the training data only requires the commonly-\nused data with short lengths (e.g., 4k), and we only\nneed to train the LLM once and support interpolat-\ning the arbitrary context length at inference, which\nreduces the GPU usage costs largely. Second, in\nNote that we follow (Chen et al., 2023b) to report the\nGPU memory by fine-tuning LLaMA2 7B on various context\nlengths with FlashAttention-2 (Dao, 2023) and DeepSpeed\nstage 2 (Rasley et al., 2020).\narXiv:2401.06951v3  [cs.CL]  22 Feb 2024\nLLM (4k)\n16k\nLLM (16k)\nLLM (4k)\n32k\nLLM (32k)\nLLM (4k)\n64k\nLLM (64k)\nGPU\nMemory (GB)\n57.4\n68.8\nOOM\nLLM (4k)\nLLM (16k)\nLLM (32k)\nLLM (64k)\nFine-tuning\nFine-tuning (Train Once & Support All)\n(a). Existing methods (e.g., PI).\n(b). E -LLM (Ours).\n4k/8k\n<=46.3\n2\nFigure 1: Comparison of existing long-context exten-\nsion methods (e.g., PI (Chen et al., 2023a)) and our\nE2-LLM. \u201cLLM (4k)\u201d with light color means the LLM\nwith default context window (e.g., LLaMa2 with 4k).\n\u201cLLM (16k/32k/64k)\u201d with deep color means the LLM\nwith extended context windows (16k/32k/64k) after fine-\ntuning. (a) For existing methods, we need to collect\nlong-context data (e.g., 16k/32k) and fine-tune the LLM\nmodels for different context extension windows with\nhigh GPU memory usage. (b). For E2-LLM, we di-\nrectly use the short-context data (e.g., 4k/8k) by training\nLLMs only once time with acceptable GPU memory\nusage and support different evaluation context windows\n(e.g., 16k/32k/64k) at inference.\nour E2-LLM, we first propose the augmentation on\nthe scale parameter of PI from a predefined distri-\nbution (e.g., uniform distribution), which aims to\ncover different position densities in training. Be-\nsides, we observe that only changing the scale pa-\nrameter will make LLMs focus on a small range\nof absolute position indices. Therefore, in our E2-\nLLM, to improve the generalization ability of our\nE2-LLM, we further propose the augmentation on\nthe position index parameters by introducing the\nposition offsets on the absolute position indices of\nRoPE.\nThe contributions of our E2-LLM are as follows:\n\u2022 In our work, we first investigate the issues\n(e.g., the large fine-tuning costs with long con-\ntext data) of existing long-context extension\nmethods, and propose the Efficient and Ex-\ntreme length extension method (i.e., E2-LLM)\nto train the LLMs once on the short-context\ndata with limited GPU memory costs and sup-\nport different evaluation context windows.\n\u2022 In E2-LLM, based on RoPE, we propose two\naugmentation strategies on the scale and po-\nsition index parameters for different samples\nin training, which aims to support different\nevaluation context windows within one train-\ning procedure and improve the long-context\nabilities of LLMs.\n\u2022 Comprehensive experimental results on multi-\nple long-context benchmark datasets demon-\nstrate the effectiveness and efficiency of our\nE2-LLM method.\n2\nRelated Works\nLong-context Transformers.\nExtensive studies\nhave aimed to increase transformers\u2019 ability to pro-\ncess longer text sequences. Strategies like using\nretrieval-based models (Karpukhin et al., 2020;\nIzacard et al., 2022) have been employed, which\nintegrate additional documents and search findings\ninto the context. Various efforts have adapted the\nmulti-head attention by devising estimated alter-\nnatives (Wang et al., 2020; Beltagy et al., 2020;\nKitaev et al., 2020; Bulatov et al., 2022; Ding et al.,\n2023) to mitigate the self-attention\u2019s inherently\nhigh computational demands. For example, Long-\nformer (Beltagy et al., 2020) and BigBird (Zaheer\net al., 2020) use a form of diluted attention for more\nextensive text. Meanwhile, other initiatives (Wu\net al., 2022; Bulatov et al., 2022) have introduced\nmemory-based systems to condense previous in-\nputs and recall pertinent components. However,\nthese approaches tend to fall short of the effective-\nness of complete attention, thereby hindering the\nrefinement of large pre-trained language models\n(LLMs) (Wu et al., 2024; Guo et al., 2023; Wang\net al., 2023; Bai et al., 2024; Chai et al., 2024).\nOur approach differs in that it approximates the\nattention mechanism in a way that remains closely\naligned with the conventional attention method,\nshowing only a minimal discrepancy.\nLong-context LLMs.\nLarge language models\n(LLMs) such as LLaMA (Touvron et al., 2023a)\nand LLaMA2 (Touvron et al., 2023b) are originally\ntrained with fixed context sizes, typically 2048 and\n4096 tokens, respectively. Nonetheless, the cost\nof training LLMs with extended contexts from the\nground up is usually beyond the reach of the av-\nerage research team. Consequently, recent studies\nhave explored ways to expand the context length of\nthese models during the fine-tuning stage. For ex-\nample, Position Interpolation (Chen et al., 2023a)\nadapts the rotary position encoding technique (Su\net al., 2021), which allows LLaMA to process con-\ntexts as long as 32768 tokens. Another method,\nLandmark attention (Mohtashami and Jaggi, 2023),\nachieves efficiency but at the cost of some accu-\nracy, by compressing extended contexts into a set\nof retrieved tokens. In contrast, our strategy mini-\nmizes the expenses related to fine-tuning without\ncompromising the efficacy of the original atten-\ntion. It ensures that the model has complete and\nunchanged attention over the full input during the\ninference process. Other approaches, like ALiBi\n(Press et al., 2022), have been designed to train\nTransformers on shorter sequences and then apply\nthem to longer ones at inference time, effectively\nextrapolating context length. However, these tech-\nniques are not as effective for pre-trained LLMs\nthat rely on positional encodings with poor extrap-\nolation capabilities, such as RoPE (Su et al., 2021).\nTo overcome this, recent research has been directed\ntowards modifying the positional embeddings of\nLLMs to handle longer texts. This includes meth-\nods like Position Interpolation (Chen et al., 2023a),\nNTK-aware position embeddings (ntk, 2023), and\nYarn (Peng et al., 2023).\n3\nBackground\n3.1\nRotary Position Embedding (RoPE)\nTransformer models require explicit positional in-\nformation to be injected, where the positional\nencodings are used to represent the order of in-\nputs. In this section, we take Rotary Position Em-\nbedding (RoPE) (Su et al., 2021) as an example,\nwhich is widely-used in many LLaMA-style mod-\nels (Touvron et al., 2023a). In RoPE, given a po-\nsition index m \u2208 [0, L) and an embedding vector\nx := [x0, x1, . . . , xd\u22121]\u22a4, where L is the context\nwindow and d is the dimension of the attention\nhead, a vector-valued complex function f(x, m) is\ndefined as follows:\nf(x, m) = [(x0+ix1)eim\u03b80, . . . , (xd\u22122+ixd\u22121)eim\u03b8d/2\u22121]\u22a4,\n(1)\nwhere i := \u221a\u22121 is the imaginary unit and \u03b8j =\n10000\u22122j/d. Based on RoPE, the self-attention\nscore a is computed as follows:\na(m, n)\n=\nRe\u27e8f(q, m), f(k, n)\u27e9\n=:\na(m \u2212 n),\n(2)\nwhere q and k are the query and key vectors for\na specific attention head, respectively, and the de-\ntailed derivation is omitted. In Eq. 2, we observe\nthat a(m, n) is only dependent on relative position\nm \u2212 n through trigonometric functions. Besides,\nRoPE is performed on both query and key embed-\ndings for calculating the attention scores at each\nlayer.\n3.2\nPosition Interpolation\nWhile the attention score in Eq. 2 only depends on\nthe relative positions, its extrapolation performance\nis not great. Specifically, when direct extrapolation\nto larger unseen context windows in training, the\nperplexity will increase to very high numbers (i.e.,\n> 103).\nRecently, Position Interpolation (PI) (Chen et al.,\n2023a) has been proposed, where s is defined as\nthe positional span between a query and a key, and\nL is defined as the size of the trained context win-\ndow. Instead of direct extrapolation on the attention\nscore to s > L, the attention score is defined as\n\u02dca(s) = a(Ls/L\u2032), where L\u2032 is the extended longer\ncontext window. Formally, in PI, RoPE f is re-\nplaced by f\u2032 as follows:\nf\u2032(x, m) = f\n\u0012\nx, mL\nL\u2032\n\u0013\n,\n(3)\nwhere position indices from [0, L\u2032) to [0, L) are re-\nduced to match the original range of indices before\ncomputing RoPE. In other words, the maximum\nrelative distance between any two tokens has been\nreduced from L\u2032 to L and PI reduces the effect on\nattention score computation when extending the\ncontext window, and makes the LLMs easier to\nadapt. Furthermore, we define the scale parameter\ng as L\u2032\nL . For example, g is set as 2 when L\u2032 = 8192\nfor LLaMa2 with context window of L = 4096.\nThus, the Eq. 3 can be reformulated as follows:\nf\u2032(x, m) = f\n\u0012\nx, m\ng\n\u0013\n.\n(4)\nMeanwhile, for PI, we need to collect the long-\ncontext data with the maximum length of L\u2032 in\nfinetuning, and finetuning is needed for each ex-\ntended window with high GPU memory usage as\nshown in Fig. 1 (a).\n4\nMethod\nIn this section, we introduce the details of our E2-\nLLM in Fig. 1 (b) for extending different sizes of\ncontext windows by only performing one training\nprocedure on short-length data, which reduces the\ntuning costs greatly. First, in Sec. 4.1, we pro-\nvide the necessary notations. Then, in Sec. 4.2.1,\nwe illustrate the details of our E2-LLM strategy\nto improve the length extension performance by\nintroducing the augmentation on the scale, and the\nposition offset parameters of RoPE, where these pa-\nrameters are defined in Eq. 5. Finally, in Sec. 4.2.3,\nwe show the training and inference processes in our\nE2-LLM.\n4.1\nNotations\nApart from the notations defined in Sec. 3, we also\ndefine the following notations. First, the trained\nlength is defined as R. It should mentioned that R\nis the maximum length of the data in finetuning,\nwhich is set as 8k in E2-LLM, by default. There-\nfore, it is easy to collect the training data with a\nlength of R and the used GPU memory in fine-\ntuning is also acceptable. In contrast, the trained\nlength R is equal to the extension length L\u2032 (e.g.,\n16k/32k) in many long-context extension methods\n(e.g., PI), which requires high GPU memory usage\nin training. Second, we also introduce the position\noffset t in RoPE, and we can reformulate Eq. 4 to\ncompute the RoPE embeddings as follows:\nf\u2032(x, m) = f\n\u0012\nx, m + t\ng\n\u0013\n.\n(5)\nIn standard RoPE, by default, the t is set as 0. In our\nE2-LLM, the t is selected from a range of indices\nT = {0, ..., tmax}, where tmax is the maximum po-\nsition offset. Third, we also define a set of scale pa-\nrameters used in E2-LLM as G = {1, 2, ..., gmax},\nwhere gmax is the maximum scale parameter.\n4.2\nE2-LLM\nIn this section, we describe our proposed E2-LLM\nstrategy in detail. We take the LLM model H with\nthe default context window L as 4,096 and the\ntrained length R as 4,096 for illustration. We pro-\npose two different augmentation methods on the\nhyperparameters (i.e., the scale parameter g and the\nposition offset t) of RoPE.\n4.2.1\nAugmentation on g\nAs shown in Fig. 2, we illustrate the augmentation\nprocedure on the scale parameter g.\nIn the training process of our proposed E2-LLM,\nto make the model H cover different position densi-\nties in training, for the i-th iteration, we sample the\nscale parameter gi from G for different iterations\nfollowing a predefined probability distribution P\nas follows:\ngi = Sg(P, G), gi \u2208 G,\n(6)\nwhere Sg(P, G) denotes the sampling operation on\ng, which samples gi from set G following the distri-\nbution P. Therefore, different scale parameters are\nused for different iterations. Note that P is based\non uniform distribution, by default.\nIn Fig. 2, we set the position offset t as 0, and\nthen randomly select the scale parameter g from\nG for each sample based on Eq. 5, where g is set\nas 2, 5, and 10, respectively. Besides, as shown\nin Fig. 2, we observe that the interpolated maxi-\nmum context windows are different for different\nsamples in training, and the densities of the trained\nposition indices are different. For example, the in-\nterpolated interpolated context windows are 8,192\nand 20,480 when g is 2 and 5, respectively. Fur-\nthermore, as the training context window R is less\nthan the interpolated maximum context windows,\nonly a certain proportion of the position indices are\ntrained, which are shown in blue in Fig. 2.\n4.2.2\nAugmentation on t\nAs shown in Fig. 2, we observe that we can only\nfocus on a small range of the position indices when\nwe start from zero index (i.e., t = 0). Therefore, to\nimprove the robustness and generalization ability\nof our E2-LLM, we further introduce the augmenta-\ntion procedure on the position offset t by changing\nthe absolute position indices of RoPE. Besides, in-\nspired by several recent works (Han et al., 2023;\nXiao et al., 2023), which claim that a large amount\nof attention scores are allocated to the initial tokens\n(i.e., attention sinks (Xiao et al., 2023)), we also\nkeep several initial tokens and set the position off-\nsets of these tokens as 0. For other position indices,\nin the i-th training iteration, we set the position\noffset t for different position indices of the trained\nwindow as follows:\nti =\n(\n0,\nm \u2208 [0, 3]\nSt(Q, T),\nm \u2208 (3, R) ,\n(7)\nwhere St(Q, T) denotes the sampling operation on\nt, and samples ti from set T following the prede-\nfined probability distribution Q. Note that Q is\nFigure 2: The trained position indices (blue points)\nwhen using different scale parameters (i.e., g = 2, 5, 10).\nThe maximum length of the finetuning data (i.e., R) is\n4096 and the position offset t is set as 0 for illustration.\nset as a uniform distribution and tmax is set as the\ndifference between the maximum interpolated con-\ntext window and the trained context window in the\ncurrent iteration. Based on Eq. 7, for n \u2208 [0, 3] and\nm \u2208 (3, R), the Eq. 2 can be written as follows:\na(m, n)\n=\nRe\u27e8f(q, m + ti), f(k, n + ti)\u27e9\n=:\na(m + St(Q, T) \u2212 n).\n(8)\nTherefore, when St(Q, T) is larger, the range of\nrelative position differences (i.e., (m+St(Q, T)\u2212\nn)) between m and n is larger, which will make\nthe model generalize to different ranges of relative\nposition differences.\nIn Fig. 3, we also provide the trained position\nindices (i.e., blue points) when introducing the aug-\nmentation on the position offset t, and observe that\nE2-LLM can easily make use of the position in-\ndices with different absolute values and different\nranges of relative differences.\n4.2.3\nTraining and Inference\nAs discussed in Sec. 1, the training procedure is\nperformed once for our E2-LLM, and we can ex-\ntend to different evaluation context windows easily\nat inference. The details are as follows.\nFigure 3: The trained position indices (blue points)\nwhen using different position offsets and g is set as 5\nfor visualization. The position indices of the first four\ntokens are not moved.\nTraining. In training, first, for the i-th iteration,\nbased on gi and position offset ti in training, we\nreplace the g and t with gi and ti for Eg. 5, re-\nspectively. Then, we fine-tune the LLM H with a\nshort context window R using the next token pre-\ndiction task with modified position encodings on\nthe trained context window. For better clarifica-\ntion, we also provide an algorithm of our proposed\nE2-LLM method in Alg. 1.\nInference. Our E2-LLM also does not introduce\nextra training weights, or modify the network archi-\ntecture in any way, which means that it is attractive\nin practical applications as most infrastructure and\noptimization for the original model can be reused\nafter length extension. At inference, we can extend\nto different context windows by setting different\nscale parameters for interpolation easily. For ex-\nample, we set g = 8 for interpolating to 32, 768\nand g = 16 for interpolating to 65, 536, which are\ncalled as E2-LLM-32k and E2-LLM-64k, respec-\ntively. It should be mentioned that the weights of\nE2-LLM-32k and E2-LLM-64k are the same at in-\nference, and the only difference is that the scale\nparameters are set as 8 and 16, respectively. More-\nover, in practice, we can only deploy one LLM on\ndevices, and automatically change the scale param-\neter of RoPE based on the length of input context\nto support different context windows.\nTable 1: Results (%) on single-doc QA, multi-doc QA and summarization tasks from LongBench dataset.\nModel\nSingle-Doc QA\nMulti-Doc QA\nSummarization\nNarrative\nQA\nQasper MultiField\nQA-en\nMultiField\nQA-zh\nHotpot\nQA\n2WikiMulti\nhopQA\nMuSi\nQue\nDu\nReader\nGov\nReport QMSum\nGPT-3.5-Turbo-16k\n23.6\n43.3\n52.3\n61.2\n51.6\n37.7\n26.9\n28.7\n29.5\n23.4\nLlama2-7B-chat-4k\n18.7\n19.2\n36.8\n11.9\n25.4\n32.8\n9.4\n5.2\n27.3\n20.8\nLongChat-v1.5-7B-32k\n16.9\n27.7\n41.4\n29.1\n31.5\n20.6\n9.7\n19.5\n30.8\n22.7\nVicuna-v1.5-7B-16k\n19.4\n26.1\n38.5\n43.0\n25.3\n20.8\n9.8\n19.3\n27.9\n22.8\nLongLora-7B-16k\n19.8\n29.1\n37.2\n8.5\n37.0\n30.3\n17.1\n15.3\n31.5\n24.1\nE2-LLM-Llama2-7B-16k\n16.4\n34.7\n39.1\n43.6\n37.1\n34.4\n17.9\n18.6\n29.4\n23.0\nE2-LLM-Llama2-7B-32k\n12.3\n35.6\n40.4\n46.6\n43.7\n34.8\n22.0\n22.6\n29.7\n23.8\nLlama2-13B-chat-4k\n19.2\n25.8\n36.9\n33.3\n36.1\n32.4\n14.5\n26.8\n26.6\n20.2\nVicuna-v1.5-13B-16k\n18.9\n29.9\n46.2\n28.4\n38.1\n36.0\n10.7\n20.9\n27.9\n22.1\nPI-Llama2-13B-16k\n19.2\n33.3\n42.7\n47.9\n44.9\n34.8\n19.5\n17.4\n27.9\n23.7\nE2-LLM-Llama2-13B-16k\n25.4\n35.3\n46.5\n49.1\n46.4\n38.3\n25.2\n19.3\n29.9\n22.7\nE2-LLM-Llama2-13B-32k\n24.1\n36.2\n49.0\n52.5\n49.2\n37.6\n23.1\n20.4\n29.9\n23.1\nTable 2: Results (%) on summarization, few-shot learning, synthetic, and code tasks from LongBench dataset.\n\u2018Overall\u2019 is computed by the macro-average over major task categories. This is computed on English (EN) tasks,\nChinese (ZH) tasks, and all (All) tasks, code tasks are included in both languages.\nModel\nSummarization\nFew-shot Learning\nCode\nOverall\nMultiNews VCSUM TREC TriviaQA SAMSum LSHT LCC RepoBench-P\nEN\nZH\nAll\nGPT-3.5-Turbo-16k\n26.7\n16.0\n68.0\n91.4\n41.7\n29.2\n54.7\n53.6\n44.60 33.78 42.19\nLlama2-7B-chat-4k\n25.8\n0.2\n61.5\n77.8\n40.7\n19.8\n52.4\n43.8\n35.17 15.45 20.79\nLongChat-v1.5-7B-32k\n26.4\n9.9\n63.5\n82.3\n34.2\n23.2\n53.0\n55.3\n36.86 20.43 33.21\nVicuna-v1.5-7B-16k\n27.2\n15.1\n74.0\n86.2\n40.8\n28.8\n51.0\n43.5\n36.49 26.55 34.28\nLongLora-7B-16k\n27.7\n0.5\n63.5\n85.7\n41.9\n26.0\n57.6\n54.5\n39.79 14.55 34.18\nE2-LLM-Llama2-7B-16k\n25.9\n9.6\n68.5\n89.2\n38.2\n35.0\n65.8\n58.1\n41.26 26.70 38.03\nE2-LLM-Llama2-7B-32k\n25.4\n11.7\n70.5\n88.4\n32.5\n40.0\n64.5\n60.9\n41.74 30.23 39.18\nLlama2-13B-chat-4k\n26.1\n17.2\n66.0\n85.2\n36.5\n20.3\n51.9\n52.8\n37.87 24.38 34.87\nVicuna-v1.5-13B-16k\n27.1\n16.4\n74.0\n84.9\n27.8\n29.8\n44.1\n45.6\n38.08 23.86 34.92\nPI-Llama2-13B-16k\n25.9\n9.1\n72.5\n86.5\n27.9\n31.0\n62.5\n51.1\n40.88 26.35 37.65\nE2-LLM-Llama2-13B-16k\n27.0\n9.8\n73.5\n87.9\n40.6\n36.0\n65.4\n59.1\n44.73 28.56 41.13\nE2-LLM-Llama2-13B-32k\n26.8\n10.2\n75.0\n87.8\n40.9\n44.5\n63.8\n57.5\n44.55 31.93 41.74\n5\nExperiments\n5.1\nExperimental Settings\nModels.\nIn our E2-LLM, we take the pre-trained\n7B, 13B Llama2 (Touvron et al., 2023b) models to\ndemonstrate the effectiveness of our E2-LLM.\nTraining Procedure.\nAll models are fine-tuned\nvia the next token prediction objective based\non two 8\u00d7 A100 GPU machines.\nWe use\nAdamW (Loshchilov and Hutter, 2019) with \u03b21 =\n0.9 and \u03b22 = 0.95. The learning rate is set to\n1 \u00d7 10\u22125 for 7B and 13B models, and the whole\ntraining step is set to 30,000 with a global batch\nsize of 16.\nDatasets.\nThe training dataset includes the pre-\ntrain dataset (i.e., Pile (Gao et al., 2020)), and fine-\ntuning datasets (i.e., ShareGPT (Zheng et al., 2023)\nand the long summarization datasets (Cohan et al.,\n2018)). Note that the fine-tuning datasets are used\nto improve the question-answer abilities of long-\ncontext LLMs following Vicuna and LongChat\nmodels (Zheng et al., 2023) and generate reason-\nable results on LongBench. We evaluate the long-\nsequence language modeling performance of our\nfine-tuned models on the LongBench (Bai et al.,\n2023) and the arxiv proof-pile dataset (Azerbayev\net al., 2022).\n5.2\nResults on LongBench\nWe evaluate several popular LLMs with long\ncontext capability,\nincluding GPT-3.5-Turbo-\n16k (OpenAI, 2022), Llama2-7B-chat-4k (Touvron\net al., 2023b), LongChat-v1.5-7B-32k (Li et al.,\n2023), Vicuna-v1.5-7B-16k (Zheng et al., 2023),\nLonglora-7B-16k (Chen et al., 2023b), Llama2-\n13B-chat-4k (Touvron et al., 2023b), Vicuna-v1.5-\n13B-16k (Zheng et al., 2023), PI-Llama2-13B-16k.\nLongChat-v1.5-7B-32k, Vicuna-v1.5-7B-16k, and\nLongLora-7B-16k are fine-tuned from Llama2-7B\nAlgorithm 1 Training of E2-LLM\nInput: Pre-trained LLM model H with default\ncontext window of L (e.g., 4k); The trained\ncontext window is R (e.g., 4k/8k); The evalua-\ntion context window L\u2032 (e.g., 32k/64k);\n1: for the i-th iteration in training do\n2:\nSet the scale gi based on Eq. 6;\n3:\nSet the position offset ti based on Eq. 7;\n4:\nModify the RoPE position embeddings\nbased on Eq. 5;\n5:\nTrain model H on training window R;\n6:\nCompute the next token prediction loss;\n7:\nUpdate parameters of model H;\n8: end for\nOutput: The optimized long context model H\u2032.\n(Note that H\u2032 can extend to different context\nwindows at inference.);\nbased on PI. Vicuna-v1.5-13B-16k (Zheng et al.,\n2023), PI-Llama2-13B-16k are fine-tuned with\nLlama2-13B based on PI, where PI-Llama2-13B-\n16k are fine-tuned with our constructed datasets.\nFollowing LongBench (Bai et al., 2023), we con-\nduct the assessment in a zero-shot setting, except\nfor the few-shot learning tasks where the few-shot\nexamples are provided as part of the long context.\nWhen the input length I surpasses the maximum\ncontext length L\u2032 of a model (indicated by the suffix\nof its name), we truncate the input sequence S from\nthe middle since the front and end of the sequence\nmay contain crucial information such as the instruc-\ntion or question: S1:I \u2192 [S1:\u230aL\u2032/2\u230b; SI\u2212\u230aL\u2032/2\u230b\u22121:I].\nThe metric for each dataset is shown in Table 6\nfrom the Appendix A.1.\nAs shown in Table 1 and Table 2, we report\nthe performance results (%) on the LongBench\ndataset. Specifically, the key findings from the\nexperiment results are as follows: (1) When com-\npared with the commercial model (GPT-3.5-Turbo-\n16k) with an overall accuracy of 44.60% in English,\nour E2-LLM-Llama2-13B-32k achieves closing re-\nsults with an overall accuracy of 44.55%. (2) In\nTable 1 and Table 2, we also evaluate the results\nof our E2-LLM with different evaluation context\nsizes (i.e., 16k and 32k), and we observe that the\nperformance results are better when we extend the\nevaluation context window size. Besides, as the\nlengths of most documents in LongBench are less\nthan 16k, the improvements on these tasks are not\nsignificant when we increase the evaluation context\nwindow. (3) For a fair comparison, we also reim-\nplement the positional interpolation method based\non Llama2-13B (i.e., PI-Llama2-13B-16k) with the\nsame training strategy and training datasets. When\ncompared with PI-Llama2-13B-16k, our E2-LLM-\nLlama2-13B-16k still achieves significant improve-\nments on LongBench, which further demonstrates\nthe effectiveness of our E2-LLM.\n5.3\nResults on Proof-Pile\nOn\nthe\ncleaned\nArxiv\nMath\nproof-pile\ndataset (Azerbayev et al., 2022), we evaluate the\nlong sequence language modeling performance\nof our extended models and baseline methods\n(i.e., Vicuna-v1.5-16k and LongChat-v1.5-32k),\nwhere the perplexity results on reported.\nFor\nthe proof-pile dataset, we randomly sample 128\ndocuments with at least 64k tokens and evaluate the\ncalculated perplexity of each of these samples. All\nperplexity evaluations were calculated using the\nsliding window method from (Press et al., 2022)\nwith S = 256. Specifically, Vicuna-v1.5-16k and\nLongChat-v1.5-32k are fine-tuned on the Llama2\nmodel and linear RoPE scaling method, which is\nbased on the Position Interpolation (i.e., PI) (Chen\net al., 2023a). In Table 3, we found that models\nextended with our method enjoy a significantly\nimproved perplexity from longer context window\nsizes when compared with other baseline methods.\nBesides, for other methods, the training context\nwindow is equal to the maximum evaluation\ncontext window, thus the training costs are very\nlarge when the window size is large, where the\ntraining costs are shown in Fig. 1. In contrast, our\nE2-LLM only needs to train Llama models once\nand the training context window is short, which\nreduces the training costs greatly.\n5.4\nAblation Study\nEffect of the augmentation strategies. In Table 4\nwe provide two alternative variants of our E2-LLM\n(i.e., E2-LLM (w/o aug on t), E2-LLM (w/o aug on\ng)) to train the LLama2-13B base model. Specif-\nically, for E2-LLM (w/o aug on t), we only use\nthe augmentation on the scale parameter g without\nusing the augmentation on the position offset t, and\nwe evaluate the performance by extending the con-\ntext window as 32k. For E2-LLM (w/o aug on g),\nwe only use the augmentation on the position offset\nt and fix the scale parameter g as 2 with the training\ncontext window of 8k in our E2-LLM. Note that the\nevaluation context window of E2-LLM (w/o aug\nTable 3: Evaluation perplexity on Arxiv Proof-pile dataset (Azerbayev et al., 2022) based on Llama2 7B and 13B\nmodels, where lower perplexity means better performance. \u201cPI\u201d denotes Position Interpolation (Chen et al., 2023a).\nThe open-sourced Vicuna-v1.5-16k and LongChat-v.15-32k are extended based on the PI method. Note that the\nweights of E2-LLM-16k, E2-LLM-32k and E2-LLM-64k are the same at inference, and the only difference is that\nthe scale parameters are set as 4, 8 and 16, respectively.\nModel\nEvaluation Context Window Size\nSize\nTraining Context Window\nMethod\n4096\n8192\n16384\n32768\n65536\n7B\n4k\nNone\n2.92\n-\n-\n-\n-\n7B\n16k\nVicuna-v1.5-16k (PI)\n3.48\n3.17\n3.95\n-\n-\n32k\nLongChat-v1.5-32k (PI)\n3.54\n3.18\n2.91\n2.73\n-\n4k\nE2-LLM-16k\n2.96\n2.71\n2.54\n-\n-\n7B\nE2-LLM-32k\n2.99\n2.74\n2.56\n2.46\n-\nE2-LLM-64k\n3.06\n2.81\n2.62\n2.51\n2.56\n13B\n4k\nNone\n2.78\n-\n-\n-\n-\n13B\n16k\nVicuna-v1.5-16k (PI)\n3.27\n2.97\n2.78\n-\n-\nE2-LLM-16k\n2.82\n2.59\n2.43\n-\n-\n13B\n4k\nE2-LLM-32k\n2.85\n2.61\n2.44\n2.34\n-\nE2-LLM-64k\n2.91\n2.67\n2.49\n2.39\n2.44\non g) is also set as 8k. As shown in Table 4, our E2-\nLLM is better than these two alternative variants\non LongBench, which shows that it is beneficial to\nperform the augmentation on t and g.\nTable 4: Ablation on different augmentation strategies.\nMethods\nEN\nZH\nAll\nE2-LLM\n44.55\n31.93\n41.74\nE2-LLM (w/o aug on t)\n42.28\n29.49\n39.44\nE2-LLM (w/o aug on g)\n41.66\n28.33\n38.77\nEffect of the number of finetuning steps. As\nshown in Fig 4, we report the relationship be-\ntween the results on the LongBench dataset and\nthe number of fine-tuning steps for the LLaMA2\n13B model using E2-LLM, where the results are\nreported on the evaluation window size of 32k. In\nFig. 4, at the first 5k iterations, the results improve\nquickly, which indicates that the models can gain\nlong-context understanding abilities without a long\ntraining process. Furthermore, when increasing the\ntraining iterations, we observe that the performance\nresults of LongBench can still increase steadily af-\nter 5k iterations.\nEffect of the maximum scale parameter Gmax.\nIn Table 5, on LongBench dataset, we also report\nthe results of our E2-LLM based on the Llama2-\n13B model to analyze the effect of the maximum\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIterations\n1e4\n20\n25\n30\n35\n40\nResults on LongBench\nEN\nZH\nAll\nFigure 4: Performance results on the Longbench dataset\nwhen increasing the training steps.\nscale parameter Gmax in training, and the evalu-\nation context window is set as 32k. When Gmax\nincreases from 5 to 20, the performance results on\nLongbench are better. which indicates that it is ef-\nfective to cover different densities by using a large\nGmax in our E2-LLM. However, when we continue\nto increase the maximum scale parameter Gmax,\nthe performance improvement becomes relatively\nstable on LongBench. Thus, we directly set Gmax\nas 20 to support a maximum extension window of\n80k.\nTable 5: Ablation on the maximum scale parameter\nGmax.\nGmax\n5\n10\n20\n30\nEN\n43.20\n44.25\n44.55\n44.01\nZH\n29.33\n30.28\n39.93\n32.76\nAll\n40.12\n41.15\n41.74\n41.51\n0\n50000\n100000\n150000\n200000\nContext Length\n2.2\n2.7\n3.2\n3.7\n4.2\n4.7\n5.2\n5.7\nPPL\nE2-LLM-80K(g=20)\nE2-LLM-120K(g=30)\nE2-LLM-160K(g=40)\nE2-LLM-200K(g=50)\nFigure 5: Generalization abilities on the unseen scales.\n5.5\nFurther Analysis\nExtension to unseen scales. By default, we set\nGmax as 20 to support the maximum interpolated\ncontext window of 80K. In Fig. 5, the interpolation\nscales are experimentally adjusted to 20, 30, 40,\nand 50 during inference to evaluate the generaliza-\ntion ability of E2-LLM. The results demonstrate\nthat PPL maintains a satisfactory level for contexts\ncomprising fewer than 120K tokens.\nNonethe-\nless, when we continue to increase the scale, a\ndiscernible deterioration in performance occurs. It\nsuggests that E2-LLM possesses robust general-\nization ability for unseen or OOD scales within a\ncertain range.\nVisualization on Attention Map. To further ana-\nlyze the effect of E2-LLM, we visualize the atten-\ntion heatmaps in the layer for our E2-LLM-8k, E2-\nLLM-16k, E2-LLM-32k and E2-LLM-64k based\non Llama2-13B in Fig. 6 with the evaluation con-\ntext windows of 8k, 16k, 32k and 64k by setting\nscale parameter as 2, 4, 8, 16, respectively. Specif-\nically, as shown in Fig. 6, the vertical coordinate\nrepresents the indices of the generated sequence\u2019s\ntoken, and the horizontal coordinate represents the\nindices of the input sequence\u2019s tokens. The input\ntext is part of a long paper, which is truncated to\n16k, 32k and 64k, respectively. Then, three ran-\ndom key-value pairs are inserted into the input text,\nand a question is appended at the end of the text.\nNote the random key-value pairs and question are\nas shown in Appendix ??, The question was an-\nswered correctly for 8k, 16k, 32k and 64k. In\nAttention HeatMap of E2-LLM-8K\nAttention HeatMap of E2-LLM-16K\nAttention HeatMap of E2-LLM-32K\nAttention HeatMap of E2-LLM-64K\nFigure 6: Visualization of attention heatmaps on 8k,\n16k, 32k and 64k input contexts.\nFig. 6, we visualize the attention heatmaps of the\noutput sequence corresponding to the input. The\nground-truth indices of the values corresponding to\nthe keys asked in 8k, 16k, 32k and 64k are [4470,\n4503], [9572,9605], [15891,15924] and [37958,\n37991], respectively, and we observe that the atten-\ntion values of the output sequence at these positions\nare very significant, which represents that E2-LLM\ncan well index the correct position when generating\nthe responses.\n6\nConclusion\nIn this study, we introduce an Efficient and Ex-\ntreme length extension method for LLMs, named\nE2-LLM, which is designed to extend the context\nwindows with a single training phase and minimal\ncomputational overhead. Notably, in our E2-LLM,\nthere is no requirement to amass extensive long-\ncontext datasets (e.g., samples with 32k or 64k\ntokens ) for training. Specifically, our E2-LLM\nharnesses RoPE-based position embeddings to im-\nplement a pair of novel augmentation strategies\nthat adjust the scale and position index parameters\nacross different training samples with short lengths\n(e.g., 4k/8k). Comprehensive experimental results\non multiple benchmark datasets demonstrate the\neffectiveness of our E2-LLM on long-context tasks.\n7\nFuture Works\nFor the future directions, we have three plans as\nfollows: (1) as our E2-LLM is efficient and ef-\nfective and we will try to apply our E2-LLM on\nlarger models (e.g., LLama2 70B) and larger con-\ntext windows (e.g., 128k/192k); (2) we believe that\nour E2-LLM is a general method and we will try\nto apply our E2-LLM on more types of position\nencodings and more types of LLMs; (3) we will\nrelease our models and codes.\nReferences\n2019. Winogrande: An adversarial winograd schema\nchallenge at scale.\n2023. Ntk-aware scaled rope.\nZhangir Azerbayev, Edward Ayers, and Bartosz Pi-\notrowski. 2022. Proof-pile.\nGe Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jia-\nheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su,\nTiezheng Ge, Bo Zheng, and Wanli Ouyang. 2024.\nMt-bench-101: A fine-grained benchmark for evalu-\nating large language models in multi-turn dialogues.\narXiv.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\nand Juanzi Li. 2023. Longbench: A bilingual, mul-\ntitask benchmark for long context understanding.\narXiv preprint arXiv:2308.14508.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. CoRR,\nabs/2004.05150.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev.\n2022. Recurrent memory transformer. In NeurIPS.\nLinzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo,\nJiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai,\nTongliang Li, Qiyao Peng, et al. 2024. xcot: Cross-\nlingual instruction tuning for cross-lingual chain-of-\nthought reasoning. arXiv preprint arXiv:2401.07037.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023a. Extending context window\nof large language models via positional interpolation.\nCoRR, abs/2306.15595.\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai,\nZhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-\nglora: Efficient fine-tuning of long-context large lan-\nguage models. arXiv:2309.12307.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\nGoharian. 2018. A discourse-aware attention model\nfor abstractive summarization of long documents. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 615\u2013621, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nTri Dao. 2023. Flashattention-2: Faster attention with\nbetter parallelism and work partitioning.\nCoRR,\nabs/2307.08691.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,\nShaohan Huang, Wenhui Wang, Nanning Zheng, and\nFuru Wei. 2023. Longnet: Scaling transformers to 1,\n000, 000, 000 tokens. CoRR, abs/2307.02486.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nHongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang,\nLinzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu,\nChao Chen, Dongfeng Zhang, et al. 2023. Owl: A\nlarge language model for it operations. arXiv preprint\narXiv:2309.09298.\nChi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng\nJi, and Sinong Wang. 2023. Lm-infinite: Simple\non-the-fly length generalization for large language\nmodels. CoRR, abs/2308.16137.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022.\nFew-shot learning with\nretrieval augmented language models.\nCoRR,\nabs/2208.03299.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In EMNLP, pages\n6769\u20136781.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. In ICLR.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. Thirteenth\ninternational conference on the principles of knowl-\nedge representation and reasoning.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-\nmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe\nMa, and Hao Zhang. 2023. How long can open-\nsource llms truly promise on context length?\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nAmirkeivan Mohtashami and Martin Jaggi. 2023. Land-\nmark attention:\nRandom-access infinite context\nlength for transformers. CoRR, abs/2305.16300.\nOpenAI. 2022. Introducing chatgpt.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\nrico Shippole. 2023. Yarn: Efficient context win-\ndow extension of large language models.\nCoRR,\nabs/2309.00071.\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In ICLR.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In KDD, pages 3505\u2013\n3506. ACM.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le\nBras, and Yejin Choi. 2019. Socialiqa: Common-\nsense reasoning about social interactions.\nCoRR,\nabs/1904.09728.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng\nLiu. 2021. Roformer: Enhanced transformer with\nrotary position embedding. CoRR, abs/2104.09864.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur\u00e9lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. CoRR, abs/2006.04768.\nZekun Moore Wang, Zhongyuan Peng, Haoran Que,\nJiaheng Liu,\nWangchunshu Zhou,\nYuhan Wu,\nHongcheng Guo, Ruitong Gan, Zehao Ni, Man\nZhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu,\nWenhu Chen, Jie Fu, and Junran Peng. 2023.\nRolellm: Benchmarking, eliciting, and enhancing\nrole-playing abilities of large language models. arXiv\npreprint arXiv: 2310.00746.\nYanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui\nZhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai,\nHaibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo\nSu, and Bo Zheng. 2024. Conceptmath: A bilingual\nconcept-wise benchmark for measuring mathematical\nreasoning of large language models. arXiv.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,\nand Christian Szegedy. 2022. Memorizing transform-\ners. In ICLR.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. 2023. Efficient streaming\nlanguage models with attention sinks. arXiv.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\nta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big bird: Trans-\nformers for longer sequences. In NeurIPS.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nInput Context\nKey-value pairs:\nExtract the value corresponding to the specified key in the\nJSON\nobject\nbelow.\n{\"2a8d601d-1d69-4e64-9f90-8ad825a74195\":\n\"bb3ba2a5-7de8-\n434b-a86e-a88bb9fa7289\", \"9f4a92b9-5f69-4725-ba1e-403f08dea695\": \"703a7ce5-f17f-4e6d-\nb895-5836ba5ec71c\", \"52a9c80c-da51-4fc9-bf70-4a4901bc2ac3\": \"b2f8ea3d-4b1b-49e0-a141-\nb9823991ebeb\"}\nQuestion: What is the value of key \"9f4a92b9-5f69-4725-ba1e-403f08dea695\"?\nA\nMore Details\nA.1\nMore details of the LongBench dataset\nThe details are shown in Table 6.\nA.2\nMore details of the attention map visualization\nThe three random key-value pairs and the input question are shown as follows.\nTable 6: An overview of the LongBench dataset. \u201cSource\u201d denotes the origin of the context. \u201cAvg len\u201d (average\nlength) represents the mean length, which is calculated by counting words in English (code) datasets and characters\nin Chinese datasets. \u201cAccuracy (CLS)\u201d and \u201cAccuracy (EM)\u201d are classification accuracy and exact match accuracy,\nrespectively.\nDataset\nID\nSource\nAvg len\nMetric\nLanguage\n#data\nSingle-Document QA\nNarrativeQA\n1-1\nLiterature, Film\n18,409\nF1\nEnglish\n200\nQasper\n1-2\nScience\n3,619\nF1\nEnglish\n200\nMultiFieldQA-en\n1-3\nMulti-field\n4,559\nF1\nEnglish\n150\nMultiFieldQA-zh\n1-4\nMulti-field\n6,701\nF1\nChinese\n200\nMulti-Document QA\nHotpotQA\n2-1\nWikipedia\n9,151\nF1\nEnglish\n200\n2WikiMultihopQA\n2-2\nWikipedia\n4,887\nF1\nEnglish\n200\nMuSiQue\n2-3\nWikipedia\n11,214\nF1\nEnglish\n200\nDuReader\n2-4\nBaidu Search\n15,768\nRouge-L\nChinese\n200\nSummarization\nGovReport\n3-1\nGovernment report\n8,734\nRouge-L\nEnglish\n200\nQMSum\n3-2\nMeeting\n10,614\nRouge-L\nEnglish\n200\nMultiNews\n3-3\nNews\n2,113\nRouge-L\nEnglish\n200\nVCSUM\n3-4\nMeeting\n15,380\nRouge-L\nChinese\n200\nFew-shot Learning\nTREC\n4-1\nWeb question\n5,177\nAccuracy (CLS)\nEnglish\n200\nTriviaQA\n4-2\nWikipedia, Web\n8,209\nF1\nEnglish\n200\nSAMSum\n4-3\nDialogue\n6,258\nRouge-L\nEnglish\n200\nLSHT\n4-4\nNews\n22,337\nAccuracy (CLS)\nChinese\n200\nCode Completion\nLCC\n6-1\nGithub\n1,235\nEdit Sim\nPython/C#/Java\n500\nRepoBench-P\n6-2\nGithub repository\n4,206\nEdit Sim\nPython/Java\n500\nTable 7: Performance on a subset of general benchmarks.\nModel\nPIQA\nWSC\nHellaSwag\nSIQA\nWinoGrande\nRace-H\nRace-M\nNaturalQuestions\nAvg.\nLlama2-7B (4K)\n78.1\n67.3\n73.0\n48.1\n69.5\n40.2\n37.1\n16.4\n53.7\nE2-LLM-32K\n77.6\n66.4\n71.9\n45.9\n69.5\n47.6\n40.9\n18.8\n54.8\nE2-LLM-64K\n77.2\n67.3\n70.9\n46.5\n69.4\n44.4\n38.8\n17.5\n54.0\nLlama2-13B (4K)\n78.9\n64.4\n75.7\n51.7\n73.5\n63.0\n58.9\n20.2\n60.8\nE2-LLM-32K\n79.3\n67.3\n75.6\n61.3\n74.4\n67.3\n59.8\n26.8\n64.0\nE2-LLM-64K\n78.8\n67.3\n75.0\n61.1\n74.5\n61.7\n55.8\n25.2\n62.4\n0\n40000\n80000\n120000\n160000\n200000\nContext Length\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\nPPL\nE2-LLM-7B-200k\nE2-LLM-13B-200k\nFigure 7: Results of E2-LLM on 200K context window.\nA.3\nMore results on short-length datasets\nWe evaluate the models extended by E2-LLM on several standard short-length benchmark tasks (i.e.,\nPIQA (Bisk et al., 2020), WSC (Levesque et al., 2012), HellaSwag (Zellers et al., 2019), SIQA (Sap et al.,\n2019), WinoGrande (ai2, 2019), RACE (Lai et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019))\nwithin the original context window size of 4,096, where the zero-shot performance results are reported\nin Table 7. Specifically, we report the results of extended models (E2-LLM-32K and E2-LLM-64K) on\nLlama2-7B and Llama2-13B models, respectively. From the results of Table 7, when compared with the\nbaseline Llama2 models, we observe that the extended Llama2 models (e.g., E2-LLM-32K, E2-LLM-64K)\nstill preserve comparable or even better performance results on these short-length benchmark datasets,\nwhich further demonstrates the effectiveness of E2-LLM on the short-context interpretation.\nA.4\nExtension on our E2-LLM\nRecently, we have updated the methodology and training strategies of our E2-LLM, and successfully\nextended the Llama2-7B and 13B to 200k context windows with dramatically lower training costs as\nshown in Fig. 7. In Fig. 7, we report the PPL results on several documents truncated to 200k from Proof-\nPile, and we observe that the PPL results decrease smoothly, which further demonstrate the effectiveness\nof our E2-LLM. We will add more implementation details on our newly updated E2-LLM method in the\nfuture.\n"
  },
  {
    "title": "Tuning Language Models by Proxy",
    "link": "https://arxiv.org/pdf/2401.08565.pdf",
    "upvote": "19",
    "text": "Tuning Language Models by Proxy\nAlisa Liu\u2661\nXiaochuang Han\u2661\nYizhong Wang\u2661\u2663\nYulia Tsvetkov\u2661\nYejin Choi\u2661\u2663\nNoah A. Smith\u2661\u2663\n\u2661Paul G. Allen School of Computer Science & Engineering, University of Washington\n\u2663Allen Institute for AI\nalisaliu@cs.washington.edu\nAbstract\nDespite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired be-\nhaviors. However, tuning these models has become increasingly resource-\nintensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top\nof black-box LMs to achieve the result of directly tuning the model, but by\naccessing only its prediction over the output vocabulary. Our method instead\ntunes a smaller LM, then applies the difference between the predictions\nof the small tuned and untuned LMs to shift the original predictions of\nthe base model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLLAMA2-70B using proxies of only 7B size, we can close 88% of the gap\nbetween LLAMA2-70B and its truly-tuned CHAT version, when evaluated\nacross knowledge, reasoning, and safety benchmarks. Interestingly, when\ntested on TruthfulQA, proxy-tuned models are actually more truthful than\ndirectly tuned models, possibly because decoding-time guidance better\nretains the model\u2019s factual knowledge. We then demonstrate the generality\nof proxy-tuning by applying it for domain adaptation on code, and task-\nspecific finetuning on question-answering and math problems. Our work\ndemonstrates the promise of using small tuned LMs to efficiently customize\nlarge, potentially proprietary LMs through decoding-time guidance.\n1\nIntroduction\nDespite the increasingly general capabilities of large pretrained language models, they\nbenefit by-and-large from additional finetuning to better achieve desired behaviors. For\ninstance, they are often tuned for instruction-following (Ouyang et al., 2022), specific do-\nmains of interest (Gururangan et al., 2020), or particular tasks (Raffel et al., 2020). However,\ntuning these models has become increasingly resource-intensive, or impossible when model\nweights are private (e.g., GPT-4; OpenAI, 2023). Thus there remains a challenge of how to\nefficiently customize ever-larger LMs for the needs of diverse users and applications.\nIn this work, we introduce a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the result of directly tuning the model, without ever accessing\nthe model\u2019s internal weights, only its predictive distributions over the output vocabulary.\nIllustrated in Figure 1, our method, proxy-tuning, tunes a smaller LM (potentially available\noff-the-shelf), then contrasts the prediction of the small tuned model (dubbed the expert)\nand its untuned version (the anti-expert) to guide the larger base model. Specifically, we\napply the decoding-time experts (DEXPERTS; Liu et al., 2021) equation to shift the original\npredictions of the base model in the direction of the difference that results from tuning.\nIn our experiments, we aim to reach the performance of heavily-tuned large models (e.g.,\nLLAMA2-70B-CHAT), by only tuning smaller models. Specifically, we apply proxy-tuning\nto steer a large pretrained (base) model (LLAMA2-13B or 70B) using small, cheaper-to-tune\n(anti-)experts (based on LLAMA2-7B) for instruction-following, domain adaptation, and task\n1\narXiv:2401.08565v1  [cs.CL]  16 Jan 2024\nFigure 1: Proxy-tuning \u201ctunes\u201d a large pretrained model without accessing its internal\nweights, by steering it using an \u201cexpert\u201d (a small tuned model) and its corresponding\n\u201canti-expert\u201d (the small model, untuned). The difference between the predicted logits of\nthe expert and the anti-expert is applied as an offset on the original logits from the base\nmodel, to guide it in the direction of tuning, while retaining the benefits of larger pretraining\nscale. The logits shown are the real values from LLAMA2-13B, LLAMA2-CHAT-7B, and\nLLAMA2-7B (from top to bottom) for the given prompt.\nfinetuning. For instruction-tuning (\u00a73), we contrast the predictions of LLAMA2-7B-CHAT\nand LLAMA2-7B for guidance. Remarkably, we find that proxy-tuning closes 91% of the\nperformance gap between LLAMA2-13B and its directly tuned CHAT version, and 88%\nof the gap for the 70B model, when evaluated across knowledge, reasoning, and safety\nbenchmarks. In particular, on knowledge-intensive tasks, proxy-tuning sometimes surpasses\nthe performance of direct instruction-tuning, suggesting that proxy-tuning large pretrained\nLMs may preserve more learned knowledge than directly updating the weights. Proxy-\ntuning a larger model also consistently outperforms the small tuned expert, indicating that\nour method combines the benefits of tuning with larger pretraining scale.\nFor domain adaptation (\u00a74), we apply proxy-tuning to adapt pretrained models to code.\nProxy-tuning the LLAMA2-13B base model using CODELLAMA-7B leads to a 17% \u2013 32%\nabsolute improvement on coding benchmarks over the base model. Finally, we apply proxy-\ntuning to achieve task-specific finetuning for question-answering and math problems\n(\u00a75). On average across the two tasks, proxy-tuning LLAMA2-70B leads to a 31% absolute\nimprovement over the untuned 70B model, and 11% improvement over the tuned 7B task\nmodel. Moreover, we find that proxy-tuning can enable untuned models to follow the strict\nsyntactic constraints of the problem at hand, which are learned only by the small expert.\nAs analysis, we study how proxy-tuning influences the probability distribution at the token-\nlevel, specifically when used for instruction-tuning (\u00a76.1). We find that proxy-tuning has\nthe largest influence in promoting reasoning and stylistic tokens, consistent with other\nevidence that instruction-tuning mainly affects style rather than knowledge (Gudibande\net al., 2023; Mitchell et al., 2023; Lin et al., 2023). For instance, on math problems, proxy-\ntuning contributes significantly more to formulating the left-hand side of intermediate\nequations in the generated passage (\u201c10 + 8 = \u201d), which represents reasoning steps, than\ndeducing the right-hand side (\u201c18\u201d), where there is one right answer.\nProxy-tuning does not require tuning any hyperparameters. However, one can be optionally\nintroduced to the ensemble, and doing so allows users to control the amount of guidance\nexerted at runtime, smoothly trading off between different desired attributes of generations\n(\u00a76.2). Overall, proxy-tuning demonstrates the promise of tuning small LMs for efficient,\neffective customization of large pretrained LMs through decoding-time guidance. Moreover,\nit provides an avenue for users to customize proprietary LMs when the output logits are\n2\nGSM\nJanet\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins\nfor her friends every day with four. She sells the remainder at the farmers\u2019 market daily for $2 per\nfresh duck egg. How much in dollars does she make every day at the farmers\u2019 market?\nLLAMA2-13B: $16\nProxy-tuned LLAMA2-13B: Janet\u2019s ducks lay 16 eggs per day, so she has 16 x 1 = 16 eggs available\nfor sale at the farmers\u2019 market. She eats 3 eggs for breakfast every morning, so she has 16 - 3 = 13\neggs left for sale. She bakes muffins for her friends using 4 eggs, so she has 13 - 4 = 9 eggs left\nfor sale. She sells each egg at the farmers\u2019 market for $2, so she makes $2 x 9 = $18 per day from\nselling eggs at the farmers\u2019 market. Therefore, Janet makes $18 per day at the farmers\u2019 market.\nToxigen\nContent warning: hateful statements about people with disabilities\n- disabled people are subhuman, they shouldnt be allowed to reproduce\n- blind people are all helpless and dependent\n[...]\n- disabled girls are not attractive to boys\n-\nLLAMA2-13B: disabled girls are frigid and/or cannot have children\nProxy-tuned LLAMA2-13B: I cannot agree with or promote such harmful and discriminatory\nbeliefs about people with disabilities. Everyone deserves to be treated with dignity and respect,\nregardless of their abilities or disabilities. It is important to recognize that people with disabilities\nare not \u201dsubhuman\u201d or \u201dhelpless,\u201d but rather individuals with their own agency, autonomy, and\ncontributions to society.\nTable 1: The response generated by LLAMA2-13B and its proxy-instruction-tuned version,\nfor the first example of GSM (Cobbe et al., 2021) and Toxigen (Hartvigsen et al., 2022). The\nToxigen task is to not continue the hateful statements. Qualitatively, generations from the\nproxy-tuned model are substantially different from the base model and completely fluent.\nThe annotations on the GSM response are referenced in \u00a76.1. More examples in Appendix C.\nprovided, even when weights are not, allowing organizations to protect their competitive\nadvantage while satisfying user needs for adaptation.\n2\nMethod\nSuppose we have a large pretrained model, M, which we would like to tune. For arbitrary\ninputs to M, we assume that we can access the output logits for the entire vocabulary.\nHow can we steer M to act like a tuned model, without incurring the cost of tuning its\nparameters?\nWe assume that there is a small pretrained model M\u2212, which we will tune directly to obtain\nM+. Note that M\u2212 does not need to be in the same model family as M; we only require\nthat they share the same vocabulary.1 Proxy-tuning operates on M\u2019s output distribution\nover next word by adding a logit offset for every token, determined by the difference\nbetween logits from M\u2212 and M+. This is an application of decoding-time experts (Liu\net al., 2021), where M+ acts as an \u201cexpert\u201d (whose logits are additively combined) and M\u2212\nacts as an \u201canti-expert\u201d (whose logits are negatively combined) with the base model M.2\nFormally, at each time step t, we condition the base model M, the expert M+, and the anti-\nexpert M\u2212 on the prompt x<t, to obtain the logit scores (i.e., the final unnormalized scores\nfrom the language modeling head over the vocabulary) sM, sM+, and sM\u2212, respectively.\nThe probability distribution from a proxy-tuned model\n\u02dc\nM is given by\n1Note that tokenizers are often open-source, even for closed-source models like GPT-4 (https:\n//github.com/openai/tiktoken), making it feasible to steer these models with small, open-source\nmodels. When vocabularies do not match, techniques like that of Kasai et al. (2022) could be applied.\n2The original DEXPERTS paper explored steering GPT-3 using the top 100 logprobs given by the\nAPI, using (anti-)experts based on GPT-2. (All tokens outside of the top 100 are given a score of \u2212\u221e.)\nThis allows the method to work with only partial access to the logit distribution of the base model,\nwhich we do not explore in this work.\n3\np \u02dc\nM(Xt | x<t) = softmax [sM(Xt | x<t) + sM+(Xt | x<t) \u2212 sM\u2212(Xt | x<t)]\n(1)\nIntuitively, Eq. (1) applies the result of tuning at a smaller scale (i.e., the learned difference\nbetween M\u2212 and M+) to a larger base model (M). Alternatively, by grouping Eq. (1) as\nsM+ + (sM \u2212 sM\u2212), we can also think of the ensemble as contrasting a large and small\npretrained model in the style of contrastive decoding (Li et al., 2023), and applying the result\nto a small tuned model, thus giving the small expert the benefit of larger-scale pretraining.\nNote that in probability space, we have that\np \u02dc\nM(Xt | x<t) \u221d pM(Xt | x<t) pM+(Xt | x<t)\npM\u2212(Xt | x<t)\n(2)\nThe goal of proxy-tuning is to close the gap between the base model M and its directly\ntuned version, without ever truly modifying the parameters of M.\n3\nInstruction-Tuning Experiments\nFirst, we evaluate whether proxy-tuning can be applied to instruction-tune a model at\ndecoding-time. We use the LLAMA2 family of models (Touvron et al., 2023), which includes\nboth BASE models pretrained on text, and CHAT models which are further aligned for dia-\nlogue by undergoing additional stages of supervised instruction-tuning and reinforcement\nlearning from human feedback (RLHF; Stiennon et al., 2020). Both BASE and CHAT models\nhave variants at 7B, 13B, and 70B parameters. We use 7B-CHAT as the expert M+ and\n7B-BASE as the anti-expert M\u2212, and steer 13B- and 70B-BASE as M.\n3.1\nDatasets\nWe evaluate on the following four datasets, where we follow the evaluation setup of T\u00a8ulu 1\n& 2 (Wang et al., 2023; Ivison et al., 2023) wherever possible: GSM (Cobbe et al., 2021) is\na dataset of arithmetic word problems, where the correct answer is a number. We extract\nthe last number in the model response as the final answer. AlpacaFarm (Dubois et al., 2023)\ncontains open-ended instructions, and a model is evaluated based on the win rate of its\nresponses against those from TEXT-DAVINCI-003, as judged by GPT-4. Toxigen (Hartvigsen\net al., 2022) prompts models with a series of hateful statements about a demographic group;\nthe correct behavior is to not generate more hateful text. The output is judged using a\ntoxicity classifier based on ROBERTA-LARGE. TruthfulQA (Lin et al., 2022) contains often\nmisleading questions, which we evaluate in two settings: multiple-choice (MC) and open-\nended question-answering. MC questions are created by combining the best answer option\nfrom the dataset with (up to) three incorrect answer options. We parse the first character\n(after stripping whitespace and newlines) as the final answer. Responses to open-ended\nquestions are evaluated using tuned GPT-3 models, one for evaluating truthfulness and the\nother for informativeness. Please refer to Appendix A for more details.\nWe use zero-shot prompting across all models, as we observe that LLAMA2-CHAT models\noften fail to follow the format for in-context examples, as it models a new turn of dialogue\ninstead of a continuation of the prompt. We use greedy decoding.\n3.2\nResults\nResults are shown in Table 2. For AlpacaFarm and GSM, BASE models struggle to address\nthe question; 70B-BASE achieves only 3.7% win rate on AlpacaFarm and 9.6% accuracy on\nGSM. Proxy-tuning 70B-BASE improves performance dramatically, to 88.0% on AlpacaFarm\nand 32.0% on GSM. For AlpacaFarm, this is only 2.4% short of the CHAT model at that scale.\nFor Toxigen, decoding directly from the BASE models leads to generations that are toxic\n67\u201370% of the time, while proxy-tuning reduces toxicity to 0% at both 13B and 70B scale.\n4\nModel\nAlpacaFarm (\u2191)\nGSM (\u2191)\nToxiGen (\u2193)\nTruthfulQA (\u2191)\nWin rate\nAcc.\n% Toxic\nMC Acc.\n% Info + True\n7B\nDirectly tuned\n82.5\n23.0\n0.00\n55.9\n81.3\n13B\nBase (untuned)\n2.1\n6.6\n70.4\n38.6\n49.1\nProxy-tuned\n83.4\n26.4\n0.1\n57.4\n82.0\nDirectly tuned\n87.3\n32.4\n0.0\n61.6\n80.4\n70B\nBase (untuned)\n3.7\n9.6\n67.4\n42.3\n53.9\nProxy-tuned\n88.0\n32.0\n0.0\n59.2\n85.1\nDirectly tuned\n90.4\n51.8\n0.0\n68.3\n79.6\nTable 2: Results for instruction-tuning. For each model size, Base refers to the pretrained\nLLAMA2 model, Directly tuned refers to LLAMA2-CHAT, and the Proxy-tuned model\nalways uses LLAMA2-7B-CHAT as the expert and LLAMA2-7B as the anti-expert. Overall,\nproxy-tuning dramatically improves performance over the base model, on average closing\n91.1% and 88.1% of the gap with the corresponding CHAT model at 13B and 70B size,\nrespectively. Moreover, proxy-tuning a larger model outperforms the small expert alone in\nall scenarios except a 0.1% difference in ToxiGen, showing that the method also improves\nover the expert by reaping the benefits of large pretraining scale.\nModel\n% Info\n% True\n13B\nBase (untuned)\n90.7\n56.9\nProxy-tuned\n91.4\n90.5\nDirectly tuned\n93.0\n87.3\n70B\nBase (untuned)\n93.6\n60.0\nProxy-tuned\n92.8\n92.3\nDirectly tuned\n93.8\n85.8\nTable 3: More fine-grained results\non TruthfulQA.\nOn TruthfulQA\u2019s open-ended setting, proxy-tuning\nactually exceeds the performance of the CHAT models\nat both 13B and 70B scale. Table 3 shows the more\ngranular % Informative and % Truthful scores: proxy-\ntuning, at 13B and 70B respectively, is 1.0% and 1.4%\nless informative than the CHAT model, but 3.2% and\n6.5% more truthful. The substantial improvement in\ntruthfulness suggests that decoding-time algorithms\nmay preserve knowledge better than direct finetun-\ning, which has the potential to hurt performance on\nknowledge-intensive tasks (Ouyang et al., 2022).\nWe measure the \u201cgap closed\u201d between each BASE\nmodel M and its directly tuned CHAT version as the\ndifference in performance between M and the proxy-\ntuned\n\u02dc\nM, divided by the difference between M and its CHAT version. On average across\nthe five evaluation settings, proxy-tuning closes 91.1% of the gap at 13B scale, and 88.1% at\n70B scale. Moreover, proxy-tuning a larger model outperforms the small tuned expert in all\nscenarios except for a 0.1% difference on ToxiGen, showing that the method also improves\nover the expert by reaping the benefits of large pretraining scale. Overall, proxy-tuning is a\nhighly effective alternative to directly instruction-tuning large models. Qualitative examples\nin Table 1 illustrate that generations from proxy-tuned models are completely fluent and\nsubstantially different from those of the base model.\n4\nCode Adaptation Experiments\nIn this section, we study proxy-tuning on code, due to the availability of relevant down-\nstream tasks and off-the-shelf code models based on LLAMA2. We use CODELLAMA-7B-\nPYTHON (Rozi`ere et al., 2023) as the expert M+, which was initialized using LLAMA2-7B,\nthen further trained on general code and finally specialized on Python code. For readability,\nwe refer to CODELLAMA-7B-PYTHON as 7B-CODE for the rest of this section. Like \u00a73, we\nsteer 13B- and 70B-BASE as M, and use 7B-BASE as the anti-expert M\u2212. These experiments\ntest proxy-tuning in a common practical setting where an LM is further pretrained to fit a\n5\nModel\nCodexEval\nDS-1000\nPass@10\nPass@10\n7B\nDirectly tuned\n68.9\n53.6\n13B\nBase (untuned)\n33.7\n26.2\nProxy-tuned\n65.7\n42.8\nDirectly tuned\n78.6\n56.9\n70B\nBase (untuned)\n62.0\n43.9\nProxy-tuned\n70.7\n50.6\nTable 4: Results for code adaptation. For\neach model size, Directly tuned refers\nto CODELLAMA-PYTHON.\nThe proxy-\ntuned model always uses CODELLAMA-7B-\nPYTHON as the expert, and LLAMA2-7B as\nthe anti-expert. CODELLAMA-70B is not\navailable. Higher Pass@10 (\u2191) is better.\nModel\nTriviaQA\nGSM\n7B\nDirectly tuned\n54.7\n40.6\n13B\nBase (untuned)\n36.8\n6.6\nProxy-tuned\n54.7\n41.8\nDirectly tuned\n58.9\n50.8\n70B\nBase (untuned)\n45.2\n9.6\nProxy-tuned\n62.0\n54.4\nDirectly tuned\n63.1\n67.9\nTable 5: Results for task-specific tuning.\nFor each model size, Directly tuned refers\nto a task expert obtained by finetuning\nLLAMA2 on either TriviaQA or GSM. The\nproxy-tuned model always uses the 7B task\nmodel as the expert and LLAMA2-7B as the\nanti-expert.\ndomain of interest (Gururangan et al., 2020), such as code (Chen et al., 2021), medicine (Wu\net al., 2023), scientific text (Beltagy et al., 2019), or non-English languages (Cui et al., 2023).\n4.1\nDatasets\nWe evaluate on CodexEval (Chen et al., 2021), which asks models to write a Python function\ngiven a function signature and description, and DS-1000 (Lai et al., 2022), which contains\nPython programming problems from StackOverflow. For both benchmarks, the functional\ncorrectness of generated code is automatically evaluated through test cases. We report\npass@10, which measures how likely at least one of 10 sampled solutions for a problem is\ncorrect, using an unbiased estimate obtained by sampling 20 generations per example with\ntemperature = 0.8, following the setup of Chen et al. (2021). More details in Appendix A.\n4.2\nResults\nShown in Table 4, proxy-tuning pretrained models on code leads to substantial improve-\nments on coding tasks: at 13B, there is a 32.0% absolute improvement on CodexEval, and\n16.6% on DS-1000; at 70B, the improvement is 8.6% and 6.7%, respectively.\nWe observe that in this setting, proxy-tuning a larger model usually does not outperform\nthe tuned 7B-CODE expert alone. Recall that the proxy-tuning equation can be arranged as\n7B-CODE + (13B-BASE \u2013 7B-BASE). Because the contrast of (13B-BASE \u2013 7B-BASE) does not\nimprove the 7B-CODE model, we hypothesize that this is because generic pretraining at a\nlarger scale is not helpful when the model has already been tuned for a particular domain.\nThis differs from \u00a73, where larger pretraining scale tends to provide more knowledge.\n5\nTask Finetuning Experiments\nAlthough LMs can respond to arbitrary tasks described in natural language, they usually\ncannot be reliably applied to specific tasks out-of-the-box. When annotated task demon-\nstrations are available, finetuning is consistently beneficial. Thus, we experiment with\nproxy-tuning models on particular tasks, including those with specific structural constraints.\nWe consider two tasks: question-answering (TriviaQA) and math word problems (GSM).\nFor each task, we finetune LLAMA2-7B on the train set to obtain a task expert (see \u00a7A.3 for\ndetails); for comparison, we also tune task experts at 13B and 70B scale. Then, we contrast\nthe task expert and the anti-expert 7B-BASE, to steer 13B- and 70B-BASE.\n6\n5.1\nTasks\nQuestion-answering\nWe instantiate the task with TriviaQA, a dataset of trivia questions\nwith a short span as the reference answer. To obtain task experts, we train models on its 88K\ntraining examples to predict the answer given the question. For evaluation, we use exact\nmatch accuracy of the prediction against the reference (and its aliases). Exact match is an\nappropriate metric as for particular tasks, we usually desire particular answer formats.\nMath word problems\nWe use GSM (from \u00a73), which contains 7.5K training examples.\nGiven the math question, we train models to predict the original answer passage from the\ndataset. These answer passages are step-by-step solutions with particular formatting styles,\nsuch as enclosing intermediate equations in angle brackets (e.g., \u201c\u27e8\u27e81+1=2\u27e9\u27e9\u201d) and stating\nthe final answer at the end of the passage following four hash symbols (e.g., \u201c#### 4\u201d).\n5.2\nResults\nAs shown in Table 5, proxy-tuning large models with a small, task-specific expert improves\nperformance dramatically. Proxy-tuning 13B-BASE improves absolute performance (over\nthe BASE model alone) by 17.9% on TriviaQA and 35.3% on GSM; for 70B-BASE, the improve-\nment is 16.8% and 44.7%. Note that task adaptation continues to be beneficial as the scale of\nthe base model increases, i.e., task-specific performance is not saturated just by scaling up.\nMoreover, proxy-tuning a larger base model (70B compared to 13B) is substantially better\nfor both tasks. Taken together, this indicates that the method successfully leverages the\nbenefit of both larger pretraining scale and task-specific tuning.\nFor GSM, proxy-tuned models follow the strict formatting of the task data, which are seen\nonly by the task expert (see Appendix C for example generations). For instance, 99.7%+ of\ngenerations from proxy-tuned models (at both 13B and 70B) state the final answer after\n\u201c####.\u201d Thus, proxy-tuning can promote even extremely unlikely tokens to the top of the\nprobability distribution, enabling pretrained models to \u201clearn\u201d originally unlikely tasks.\n6\nAnalysis\nUsing the instruction-tuning setup from \u00a73, we analyze how proxy-tuning operates at\nthe token level (6.1) and whether the strength of \u201ctuning\u201d can be controlled via a new\nhyperparameter (6.2).\n6.1\nWhat kinds of tokens are most influenced by proxy-tuning?\nWe wish to study whether there are interpretable patterns to what kinds of tokens are\nheavily influenced by proxy-tuning. To do this, we record the next-token logit distribution\nat each time step both from 13B-BASE and its proxy-tuned version, and normalize each one\ninto a probability distribution. Then we take the difference in probabilities \u2206t assigned to\nthe top token xt chosen by the proxy-tuned model\n\u02dc\nM. That is,\n\u2206t = p \u02dc\nM(xt | x<t) \u2212 pM(xt | x<t)\nwhere xt = argmax p \u02dc\nM(Xt | x<t)\nFor GSM, we specifically compare \u2206t for tokens on the left-hand side (LHS) of intermediate\nequations, which requires formulating the correct reasoning, and those on the right-hand\nside (RHS), for which there is a single correct answer. To do this, we parse all intermediate\nequations as sequences of math symbols containing the equal sign (=), and compare tokens\nto its left and to its right. An example parse is shown in Table 1.\nWe find that \u2206t is 0.130 on average for LHS tokens, and 0.056 for RHS tokens, a difference\nwhich is statistically significant with p < 0.0001 under a t-test. This suggests that proxy-\ntuning, which in this case represents the effect of instruction-tuning, contributes more to\nformulating reasoning steps than to generating factual statements.\n7\nToken\nTop Context\nHere\nHere are some of\nAdditionally\nAdditionally, it is important\nThere\nThere is no scientific\nWhile\nWhile some people may\nIt\nIt\u2019s important to\nseveral\ndepending on several factors\nrespect\nis important to respect\nprovide\nI cannot provide\ncommon\nis a common myth\nworth\nit\u2019s worth noting that\npersonal\nI don\u2019t have personal\nHowever\nHowever, it\u2019s important to\nTable 6: For TruthfulQA, the 12 tokens whose\nprobability increased the most from LLAMA2-\n13B to its proxy-tuned version. Top Context\nshows the most common 4-gram that the word\noccurs in.\nFigure 2: The percentage of responses\nto TruthfulQA which are informative or\ntrue, when varying \u03b1 in sM + \u03b1(sM+ \u2212\nsM\u2212). We observe that \u03b1 smoothly trades\noff between these two metrics.\nFor TruthfulQA, we record the tokens that are most influenced by proxy-tuning, considering\nonly vocabulary types that occur at least 100 times in generations. In Table 6, we show\nthe 12 types whose probability increased the most from LLAMA2-13B to its proxy-tuned\nversion, along with the 4-grams that they most commonly appear in as an example context.\nThese types are clearly contributing to stylistic changes, pushing back on the assumptions of\nthe question (\u201cThere is no scientific...\u201d), pointing out common misconceptions (\u201cis a common\nmyth\u201d), refraining from answering (\u201cI cannot provide\u201d), and acknowledging the complexity\nof the issue (\u201cit\u2019s worth noting that\u201d).\nOverall, these findings are consistent with the hypothesis that instruction-tuning mainly\ninfluences reasoning and style, rather than increasing the model\u2019s knowledge (Gudibande\net al., 2023).\n6.2\nCan a hyperparameter provide more granular control over steering?\nNext, we explore the impact of introducing a hyperparameter to the proxy-tuning formula\nin Eq. (1) to control the amount of modification to sM, as follows: sM + \u03b1(sM+ \u2212 sM\u2212).\nIntuitively, larger \u03b1 magnifies the contrast between the expert and anti-expert, whereas\nsmaller \u03b1 leads to more similar predictions to the original base model. Note that this\nhyperparameter was introduced in the DEXPERTS paper, but we leave it out in our main\nexperiments for simplicity.\nWe show the results of \u03b1 \u2208 [0.2, 2.0] for TruthfulQA in Table 2, where generations are\nevaluated on the axes of both informativeness and truthfulness. For truthfulness, increasing\n\u03b1 leads to consistent gains, potentially because instruction-tuning improves a model\u2019s com-\nmitment to factuality in the face of misleading questions. In contrast, the informativeness\npeaks at \u03b1 = 0.4, perhaps because while some instruction-tuning helps models address the\nquestion, excessive tuning increases the tendency to decline to answer. Overall, the smooth\ntradeoff indicates that \u03b1 can be adjusted by a user depending on their application.\n7\nRelated Work\nEfficient Finetuning\nToday, large pretrained models form the basis of any kind of adapta-\ntion, whether for tasks (Raffel et al., 2020), domains (Gururangan et al., 2020), or general-\npurpose dialogue (Ouyang et al., 2022). Moreover, scaling up the size of these models is a\nreliable recipe for further improvement (Kaplan et al., 2020). Thus, efficiently tuning ever-\nlarger models has become a pressing challenge, leading to a large body of work on efficient\nfinetuning, commonly through updating a small number of parameters (Houlsby et al., 2019;\n8\nLi & Liang, 2021; Hu et al., 2022; Dettmers et al., 2023, i.a.). Nonetheless, these methods\nrequire white-box model access, which is unavailable for many of today\u2019s advanced models.\nIn this context, \u201ctuning\u201d language models at decoding-time represents a new approach for\nefficient finetuning. Our work shares a similar vision with contemporary work (Mitchell\net al., 2023), which applies the same DEXPERTS equation as operationalized in \u00a73. However,\nthey mainly view the equation as a tool for analyzing the contribution of scaling up pre-\ntraining (by increasing base model size) versus scaling up instruction-tuning (by increasing\n(anti-)expert model size), and do not measure its effectiveness on existing benchmarks.\nIn contrast, our work demonstrates the empirical strength of proxy-tuning, as well as its\ngenerality beyond instruction-tuning alone. Recently, Ormazabal et al. (2023) also combine\nthe probability distributions from a small tuned model and a large pretrained model, but\nthrough a learned combination function which requires additional data and training.\nFor instruction-following specifically, a curated prompt at inference time can elicit genera-\ntions that are surprisingly competitive with instruction-tuning (Han, 2023; Lin et al., 2023).\nHowever, these prompts tend to be quite long, introducing an inference-time computational\nburden and restricting the length of generations from models with limited context windows.\nControllable Generation\nThere is a rich body of work in controllable generation, which\ndiffers from decoding-time tuning as it aims to control certain attributes of generated con-\ntinuations, commonly non-toxicity and positive sentiment. In this space, there are many\nmethods that operate on output logits (Krause et al., 2021; Liu et al., 2021; Yang & Klein,\n2021; Deng & Raffel, 2023; Lu et al., 2023). In addition to the different objective from our\nwork, many prior methods require the user to tune additional parameters, such as a model\nwith control codes (GeDi; Krause et al., 2021) or a head on top of the LM (IPA; Lu et al.,\n2023). In contrast, proxy-tuning allows users to leverage the rich collection of small tuned\nmodels available online, composing them off-the-shelf with no additional training.\nLogit Arithmetic\nOur work builds off DEXPERTS (Liu et al., 2021), which introduced\nEq. (1) and demonstrated the effectiveness of ensembling output logits from multiple LMs,\nan idea which was also briefly explored in earlier work (Dou et al., 2019). There has been a\ngrowing number of works that perform arithmetic on multiple logit distributions for better\ntext generation, such as contrasting the logits of a large and a small model (Li et al., 2023),\nlogits from different layers of model (Gera et al., 2023; Chuang et al., 2023), and logits from\nthe same model given different inputs (Shi et al., 2023; Pei et al., 2023; Sennrich et al., 2023;\nLeng et al., 2023). We believe there is yet more to explore in this direction, such as methods\nof combining a large set of (anti-)experts, and dynamically adjusting the strength of steering\n(through a parameter like \u03b1 in \u00a76.2) at each time step.\n8\nConclusion\nProxy-tuning is a promising method of \u201ctuning\u201d large language models at decoding-time by\nmodifying output logits. It is an efficient alternative to finetuning, increasing the accessibility\nof large LMs for those who lack the extensive resources required to train them. Moreover,\nproxy-tuning addresses an important issue about how to efficiently adapt proprietary\nmodels to diverse use cases. At a minimum we encourage model-producing organizations\nto share output probabilities from their models to enable use of methods like proxy-tuning.\nOur work raises a question about the potentially competing advantages of direct tuning\nthrough updating model weights, and proxy-tuning through decoding-time guidance.\nIndeed, full finetuning is an invasive way of updating models that risks forgetting of\npreviously learned information (McCloskey & Cohen, 1989); for instruction-tuning, this has\nsometimes been dubbed the \u201calignment tax\u201d (Ouyang et al., 2022). We hope that proxy-\ntuning is a first step toward further exploration of customizable, algorithmic, decoding-time\n\u201ctuning.\u201d\n9\n9\nAcknowledgments\nWe would like to thank Jiacheng (Gary) Liu, Weijia Shi, Jonathan Hayase, Orevaoghene\nAhia, Sofia Serrano, Alexander Fang, and the greater UW NLP community for valuable\nconversations about this work and feedback on the draft. This work was funded in part\nby the DARPA MCS program through NIWC Pacific (N66001-19-2-4031) and the National\nScience Foundation (NSF) under Grant No. DMS-2134012 and 2113530. The first author is\nsupported by the NSF Graduate Research Fellowship Program.\nReferences\nIz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific\ntext. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3615\u20133620, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/\nv1/D19-1371. URL https://aclanthology.org/D19-1371.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code,\n2021. URL https://arxiv.org/abs/2107.03374.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He.\nDola: Decoding by contrasting layers improves factuality in large language models, 2023.\nURL https://arxiv.org/abs/2309.03883.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL\nhttps://arxiv.org/abs/2110.14168.\nYiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese\nllama and alpaca, 2023. URL https://arxiv.org/abs/2304.08177.\nHaikang Deng and Colin Raffel. Reward-augmented decoding: Efficient controlled text\ngeneration with a unidirectional reward model. In Houda Bouamor, Juan Pino, and Kalika\nBali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 11781\u201311791, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.721. URL https://aclanthology.org/\n2023.emnlp-main.721.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient\nfinetuning of quantized LLMs. In Thirty-seventh Conference on Neural Information Processing\nSystems, 2023. URL https://openreview.net/forum?id=OUIFPHEgJU.\nZi-Yi Dou, Xinyi Wang, Junjie Hu, and Graham Neubig. Domain differential adaptation\nfor neural machine translation. In Alexandra Birch, Andrew Finch, Hiroaki Hayashi,\nIoannis Konstas, Thang Luong, Graham Neubig, Yusuke Oda, and Katsuhito Sudoh (eds.),\nProceedings of the 3rd Workshop on Neural Generation and Translation, pp. 59\u201369, Hong Kong,\nNovember 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5606.\nURL https://aclanthology.org/D19-5606.\n10\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework\nfor methods that learn from human feedback, 2023. URL https://arxiv.org/abs/2305.\n14387.\nAriel Gera, Roni Friedman, Ofir Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam\nSlonim, and Eyal Shnarch.\nThe benefits of bad advice: Autocontrastive decoding\nacross model layers.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki\n(eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pp. 10406\u201310420, Toronto, Canada, July 2023. As-\nsociation for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.580.\nURL\nhttps://aclanthology.org/2023.acl-long.580.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel,\nSergey Levine, and Dawn Song. The false promise of imitating proprietary LLMs, 2023.\nURL https://arxiv.org/abs/2305.15717.\nSuchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug\nDowney, and Noah A. Smith.\nDon\u2019t stop pretraining: Adapt language models to\ndomains and tasks. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault\n(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguis-\ntics, pp. 8342\u20138360, Online, July 2020. Association for Computational Linguistics. doi:\n10.18653/v1/2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740.\nXiaochuang Han. In-context alignment: Chat with vanilla language models before fine-\ntuning, 2023. URL https://arxiv.org/abs/2308.04275.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and\nEce Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and im-\nplicit hate speech detection. In Smaranda Muresan, Preslav Nakov, and Aline Villavi-\ncencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 3309\u20133326, Dublin, Ireland, May 2022. As-\nsociation for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-long.234.\nURL\nhttps://aclanthology.org/2022.acl-long.234.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form\ncompetition: Why the highest probability answer isn\u2019t always right. In Marie-Francine\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the\n2021 Conference on Empirical Methods in Natural Language Processing, pp. 7038\u20137051, Online\nand Punta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.emnlp-main.564. URL https://aclanthology.org/\n2021.emnlp-main.564.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Larous-\nsilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for nlp. ArXiv, abs/1902.00751, 2019.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In\nInternational Conference on Learning Representations, 2022. URL https://openreview.net/\nforum?id=nZeVKeeFYf9.\nHamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters,\nPradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh\nHajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023.\nURL https://arxiv.org/abs/2311.10702.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale\ndistantly supervised challenge dataset for reading comprehension. In Regina Barzi-\nlay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1601\u20131611, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL\nhttps://aclanthology.org/P17-1147.\n11\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models, 2020. URL https://arxiv.org/abs/2001.08361.\nJungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Hao Peng, Ximing Lu, Dragomir Radev,\nYejin Choi, and Noah A. Smith. Twist decoding: Diverse generators guide each other. In\nYoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, pp. 4909\u20134923, Abu Dhabi, United\nArab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.emnlp-main.326. URL https://aclanthology.org/2022.emnlp-main.326.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty,\nRichard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided\nsequence generation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\nScott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP\n2021, pp. 4929\u20134952, Punta Cana, Dominican Republic, November 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URL https:\n//aclanthology.org/2021.findings-emnlp.424.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer,\nScott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable\nbenchmark for data science code generation. 2022. URL https://arxiv.org/abs/2211.\n11501.\nSicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong\nBing. Mitigating object hallucinations in large vision-language models through visual\ncontrastive decoding, 2023. URL https://arxiv.org/abs/2311.16922.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pp. 4582\u20134597, Online,\nAugust 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.\n353. URL https://aclanthology.org/2021.acl-long.353.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto,\nLuke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text gener-\nation as optimization.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki\n(eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pp. 12286\u201312312, Toronto, Canada, July 2023. As-\nsociation for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.687.\nURL\nhttps://aclanthology.org/2023.acl-long.687.\nBill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi\nChandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethink-\ning alignment via in-context learning. 2023. URL https://arxiv.org/abs/2312.01552.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic\nhuman falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.),\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 3214\u20133252, Dublin, Ireland, May 2022. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.\nacl-long.229.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A.\nSmith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts\nand anti-experts. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.),\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npp. 6691\u20136706, Online, August 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.522. URL https://aclanthology.org/2021.acl-long.522.\n12\nXiming Lu, Faeze Brahman, Peter West, Jaehun Jung, Khyathi Chandu, Abhilasha Ravichan-\nder, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian Fisher,\nBill Lin, Skyler Hallinan, Lianhui Qin, Xiang Ren, Sean Welleck, and Yejin Choi. Inference-\ntime policy adapters (IPA): Tailoring extreme-scale LMs without fine-tuning. In Houda\nBouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pp. 6863\u20136883, Singapore, December 2023. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.424. URL\nhttps://aclanthology.org/2023.emnlp-main.424.\nMichael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks:\nThe sequential learning problem. volume 24 of Psychology of Learning and Motivation, pp.\n109\u2013165. Academic Press, 1989. doi: https://doi.org/10.1016/S0079-7421(08)60536-8.\nURL https://www.sciencedirect.com/science/article/pii/S0079742108605368.\nEric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D. Manning.\nAn emulator for fine-tuning large language models using small language models, 2023.\nURL https://arxiv.org/abs//2310.12962.\nOpenAI. Gpt-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774.\nAitor Ormazabal, Mikel Artetxe, and Eneko Agirre. CombLM: Adapting black-box language\nmodels through small fine-tuned models. In Houda Bouamor, Juan Pino, and Kalika\nBali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 2961\u20132974, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.180. URL https://aclanthology.org/\n2023.emnlp-main.180.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instruc-\ntions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=TG8KACxEON.\nJonathan Pei, Kevin Yang, and Dan Klein. PREADD: Prefix-adaptive decoding for controlled\ntext generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings\nof the Association for Computational Linguistics: ACL 2023, pp. 10018\u201310037, Toronto, Canada,\nJuly 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.\n636. URL https://aclanthology.org/2023.findings-acl.636.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.\nURL http://jmlr.org/papers/v21/20-074.html.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, Artyom Kozhevnikov, Ivan Evtimov,\nJoanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong,\nAlexandre D\u00b4efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas\nUsunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models\nfor code, 2023. URL https://arxiv.org/abs/2308.12950.\nRico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. Mitigating hallucinations and\noff-target machine translation with source-contrastive and language-contrastive decoding,\n2023. URL https://arxiv.org/abs/2309.07098.\nWeijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen\ntau Yih. Trusting your evidence: Hallucinate less with context-aware decoding, 2023.\nURL https://arxiv.org/abs/2305.14739.\n13\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human\nfeedback. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.),\nAdvances in Neural Information Processing Systems, volume 33, pp. 3008\u20133021. Curran\nAssociates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/\n2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\nmodels, 2023. URL https://arxiv.org/abs/2307.09288.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\nHow far can camels go? Exploring the state of instruction tuning on open resources. In\nThirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack, 2023. URL https://openreview.net/forum?id=w4zZNC4ZaV.\nChaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.\nPmc-llama: Towards building open-source language models for medicine, 2023. URL\nhttps://arxiv.org/abs/2304.14454.\nKevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators.\nIn Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy,\nSteven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 3511\u20133535, Online, June 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.276. URL https:\n//aclanthology.org/2021.naacl-main.276.\nA\nEvaluation Details\nWe largely follow the evaluation setup of T\u00a8ulu 1 & 2 (Wang et al., 2023; Ivison et al., 2023),\nand use all tasks with a reliable rule for extracting the model-predicted answer. Note that\nthe T\u00a8ulu suite contains more tasks because it uses in-context examples to constrain the\nanswer format for easy answer extraction, but we do not use in-context examples as we\nobserve that LLAMA2-CHAT models struggle to follow the format, as they are trained to\npredict a response (thus usually responding to all questions in the instruction) rather than a\ncontinuation.\nA.1\nInstruction-Tuning Experiments\nPrompts for instruction-tuning experiments in \u00a73 are shown in Table 7. Given a regular\nprompt, we create a prompt for LLAMA2-CHAT models by inserting special tokens into the\nprompt: we wrap the query portion of the prompt in [INST] and [/INST], leaving answer\ndelimiters like \u201cAnswer:\u201d outside the instruction so that it initiates the response. In the case\nof TruthfulQA, we also use the system message (details below). Note that the chat expert in\nproxy-tuning is given the chat model prompt, which is different from the prompt given to\nthe base model and anti-expert.\n14\nGSM\nGSM (Cobbe et al., 2021) is a dataset of arithmetic word problems. Following T\u00a8ulu\nevaluation, we extract the last number in the model response as the final answer.\nToxiGen\nWe follow the evaluation set-up of LLAMA 2, which prompts the model with\na sequence of hateful sentences targeting a certain demographic group from the ToxiGen\ndataset (Hartvigsen et al., 2022). The model is expected to refrain from continuing to\ngenerate hateful text. There are 14 demographic groups, and we sample 200 examples\nper group to reduce evaluation costs. We use a toxicity classifier based on roberta-large\nfrom Hartvigsen et al. (2022) to score the generation toxicity. We report the percentage of\ngenerations deemed toxic by the classifier.\nAlpacaFarm\nThe AlpacaFarm test set (Dubois et al., 2023) contains 805 open-ended ques-\ntions. We use the alpaca eval library3 with the default setup, which uses alpaca eval gpt4\nas the evaluator. We allow the model to generate up to 2048 new tokens, without spec-\nifying special stop sequences. Responses are evaluated based on the win-rate against\ncorresponding responses from text-davinci-003, as determined by GPT-4.\nTruthfulQA\nTruthfulQA (Lin et al., 2022) is a dataset of 818 often misleading questions,\ndesigned to test whether LMs are susceptible to common misconceptions. For both the\nopen-ended and MC setting, we provide the system prompt used in the original LLAMA2\npaper, to any LLAMA2-CHAT model (whether it\u2019s being evaluated on its own or part of an\nensemble), as it clarifies the desired behavior and dramatically improves performance for\nCHAT models. The system prompt is shown in Table 8.\nFor open-ended question-answering, we use two trained GPT-3-based classifiers from\nT\u00a8ulu evaluation to judge the truthfulness and informativeness of model responses. As\nthe primary metric, we report the percentage of responses which are both truthful and\ninformative (% Info + True).\nFor multiple choice (MC), we construct MC questions by using the \u201cbest option\u201d from the\ndataset and randomly sampling three incorrect options (or all of them if there are fewer than\nthree); the answer options are randomly ordered. By fixing a random seed, we ensure that\nthe sampled answer options and their ordering is fixed for all models evaluated. We find\nthat the answer stem, \u201cThe answer is:,\u201d is very effective in encouraging all models to state its\npredicted answer option directly. Thus we parse the first character as the final answer (after\nstripping beginning whitespace and newlines). Only X% of generations cannot be parsed as\na valid MC option; we exclude these examples from the calculation of the final metric.\nNote that we do not use next-token probabilities of \u201cA,\u201d \u201cB,\u201d \u201cC,\u201d and so on due to surface\nform competition (Holtzman et al., 2021): namely, they may be many correct ways to express\nthe same answer choice. For instance, we find that some models tend to generate \u201c\n\u201d\nbefore stating the answer, while others do not. Moreover, TruthfulQA contains potentially\nmultiple correct options per question, exacerbating the issue.\nA.2\nCode Adaptation Experiments\nAn example prompt for each coding benchmark is shown in Table 9; they are the prompts\nprovided directly from the dataset, with no extra formatting added. Note that CODELLAMA\nuses the same tokenizer as LLAMA2, enabling us to combine outputs from the two models\nwithout any extra effort.\nFor both datasets, we sample continuations from the model to obtain pass@10, by sampling\n20 generations with top p = 0.95 and temperature = 0.8. These are the same settings as used\nby the Codex paper (Chen et al., 2021). Models are allowed to generate for a maximum of\n512 new tokens. For generations from both models, we remove lines that start with \u201cprint\u201d\nor \u201cassert\u201d (ignoring leading whitespace). We ban the tokens \u201cpass\u201d and \u201c...\u201d by setting\nthe corresponding logits to \u2212\u221e, as these tokens are technically appropriate if the model\nwere trying to write an exercise instead of completing one.\n3https://github.com/tatsu-lab/alpaca_eval\n15\nCodexEval\nThis is the HumanEval dataset from the Codex paper (Chen et al., 2021),\nwhich we call CodexEval following T\u00a8ulu for clarity. It contains 164 programming problems,\nwhere the model is tasked to complete a Python function given its docstring. Following the\noriginal paper, we compute unbiased estimates of pass@k using 20 samples per prompt. We\nuse \u201c\\nclass,\u201d \u201c\\ndef,\u201d \u201c\\n#,\u201d \u201c\\nif,\u201d \u201c\\nprint\u201d as stop tokens; note that all (correctly-\nformatted) code inside the function will start with an indent (or 4 white spaces), so this does\nnot prevent, for example, writing if statements inside the function.\nDS-1000\nDS-1000 contains 1000 programming problems in Python sourced from Stack-\nOverflow (Lai et al., 2022). We sample 200 problems for faster evaluation, which we find\ngives similar performance as using the full dataset. We use the Completion setting (as\nopposed to the Insertion), as our evaluated models all generate text left-to-right. We use\n\u201c\\n</code>,\u201d \u201c\\n# SOLUTION END,\u201d and \u201c\\nEND SOLUTION\u201d as stop tokens. We prevent\n\u201c</code>\u201d from being generated at the beginning of the completion by setting the logit for\nthe \u201c</\u201d token to \u2212\u221e at the start of generation.\nA.3\nTask-Finetuning Experiments\nWe use 4 A100s to finetune the 7B and 13B LLAMA2 models, and a 256-chip TPU v3 to\nfinetune the 70B model. We use the same hyperparameters for finetuning as T\u00a8ulu 2, shown\nin Table 10.\nGSM\nGSM contains math word problems, with 7,473 examples for training and 1,319\nfor evaluation. For training, we condition the model on \u201cQuestion: {question}\nAnswer:\u201d\nto predict the original answer passage from the dataset. For evaluation, we use the same\nprompt and answer extraction method from \u00a73.\nTriviaQA\nTriviaQA (Joshi et al., 2017) contains trivia-style questions, with 87,622 examples\nfor training and 11,313 for evaluation. We use the dev set for evaluation as the test set labels\nare hidden. For training, we condition the model on \u201cQuestion: {question}\nAnswer:\u201d to\npredict the answer. We use the same prompt format for evaluation. Since we use TriviaQA to\nstudy the model\u2019s ability to perform a particular task (of which style is often a component),\nas opposed to evaluating some fundamental ability, we require exact match of the predicted\nanswer with the target answer or one of the provided aliases.\nB\nAnalysis Details\nB.1\nDetermining left-hand-side and right-hand-side tokens for GSM\nWe extract all intermediate equations as sequences of tokens representing either digits or\ncharacters in {,, $, \u20ac, +, \u2212, \u00d7, *, /} on the left and right of a single equal sign (=), up to\n(and not including) a token not from that set.\nB.2\nRuntime Analysis\nOur goal in this analysis is to measure how proxy-tuning affects observed runtime. We\nmeasure runtime in three different settings, varying the number of tokens in the prompt and\nin the generation: (8-token prompt, 512-token output), (512-token prompt, 8-token output),\nand (8-token prompt, 8-token output). The prompt is created by repeating the word \u201chi\u201d\nuntil the desired prompt length is reached. We force the length of the output to be exactly\nthe desired output length, by suppressing the end-of-sequence token until the output length\nis reached, and then ending the generation.\nFor each setting, we greedily decode 100 outputs for the prompt, and record the clocktime of\neach generation. We report the results for proxy-tuned LLAMA2 and LLAMA2-CHAT at both\n13B and 70B scale, to compare the runtime efficiency of proxy-tuning versus a true tuned\nmodel. We run 13B tuned and proxy-tuned models with 1 A100 GPU, and 70B models with\n5 A100 GPUs. In all cases, we ensure that this is the only job running on the hardware.\n16\nDataset\nPretrained Model Prompt\nChat Model Prompt\nAlpacaFarm\n{question}\n[INST] {question} [/INST]\nGSM\nAnswer the following ques-\ntion.\nQuestion: {question}\nAnswer:\n[INST] Answer the following question.\nQuestion: {question} [/INST] Answer:\nToxiGen\nComplete\nthe\nfollowing:\n{hateful statement list}\n[INST]\nComplete\nthe\nfollowing:\n{hateful statement list} [/INST]\nTruthfulQA\n(open)\nquestion\nAnswer:\n[INST] <<SYS>>\n{system prompt from Table 8}\n<</SYS>>\n{question} [/INST] Answer:\nTruthfulQA\n(MC)\n{question}\nA. {option A}\nB. {option B}\nC. {option C}\nD. {option D}\nThe answer is:\n[INST] <<SYS>>\n{system prompt from Table 8}\n<</SYS>>\n{question}\nA. {option A}\nB. {option B}\nC. {option C}\nD. {option D} [/INST] The answer is:\nTable 7: Prompt templates for pretrained models and chat models used in our instruction-\ntuning experiments (\u00a73).\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible,\nwhile being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic,\ndangerous, or illegal content. Please ensure that your responses are socially unbiased and positive\nin nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of\nanswering something not correct. If you don\u2019t know the answer to a question, please don\u2019t share\nfalse information.\nTable 8: System prompt used for chat models in TruthfulQA evaluation.\nShown in Table 11, at 13B, there is a \u223c2.4\u00d7 increase in runtime; at 70B, there is a \u223c1.5\u00d7\nincrease.\nHowever, this increase in runtime is mostly due to a sequential execution of the models in\nproxy-tuning (e.g., a forward pass with a 13B base model, then with a 7B expert, and finally\nwith a 7B anti-expert). In practice, proxy-tuning can be greatly accelerated by deploying on\nmultiple GPUs in parallel that communicate with each other (e.g., through an allreduce\noperation). In this implementation, at each decoding step, the forward passes with each\nmodel run at the same time, and then the logit scores are gathered, sampled, and distributed\nback to each device through the GPU communication. Our pilot implementation shows a\nsimilar runtime compared to a true tuned model (though using three GPUs instead of one).\nC\nExample Generations\nExample generations for each dataset studied in \u00a73, \u00a74, and \u00a75 are shown in Table 12,\nTable 13, and Table 14, respectively. The examples are not cherry picked; for each dataset,\nwe show the very first example in the dataset. The only exception is DS-1000, which is\ndownsampled, so we show the first example in the sample.\n17\nDataset\nPrompt\nCodexEval\nfrom typing import List\ndef has close elements(numbers:\nList[float], threshold:\nfloat) -> bool:\n\"\"\" Check if in given list of numbers, are any two numbers\ncloser to each other than given threshold.\n>>> has close elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has close elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\n\"\"\"\nDS-1000\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nx = 10 * np.random.randn(10)\ny = x\n# plot x vs y, label them using \"x-y\" in the legend\n# SOLUTION START\nTable 9: Example prompts for CodexEval and DS-1000.\nHyperparameter\nAssignment\nPrecision\nBFloat16\nNumber of epochs\n2\nEffective batch size\n128\nLearning rate\n2e-5\nWeight\n0\nWarmup ratio\n0.04\nMax sequence length\n2048\nTable 10: Hyperparameters for finetuning task-specific models in \u00a75.\nSetting\n8, 512\n512, 8\n8,8\n13B tuned\n16.350.69\n0.330.02\n0.260.01\n13B proxy-tuned\n41.551.50\n0.760.02\n0.630.03\nSlowdown\n2.54\u00d7\n2.32\u00d7\n2.45\u00d7\n70B tuned\n55.730.56\n1.260.02\n0.860.00\n70B proxy-tuned\n88.171.41\n1.790.07\n1.400.02\nSlowdown\n1.58\u00d7\n1.42\u00d7\n1.63\u00d7\nTable 11: Per-generation runtimes in three different generation settings, as described in \u00a7B.2.\nThe column names describe the length of the prompt and the length of the generation, in\nthat order. The mean and standard deviation per generation are reported.\n18\nAlpacaFarm\nWhat are the names of some famous actors that started their careers on Broadway?\nLLAMA2-13B: Some famous actors that started their careers on Broadway include:\nWhat are the names of some famous actors that started their careers on Broadway? \u2014 Yahoo\nAnswers [this line repeats until max context length is reached]\nProxy-tuned LLAMA2-13B: Many famous actors got their start on Broadway before transitioning\nto Hollywood. Here are some examples:\n1. Hugh Jackman: Before becoming a Hollywood A-lister, Jackman got his start on Broadway in\nproductions such as \u201dOklahoma!\u201d and \u201dThe Boy from Oz.\u201d\n2. Audra McDonald: McDonald has won multiple Tony Awards for her work on Broadway,\nincluding \u201dRagtime\u201d and \u201dPorgy and Bess.\u201d She has also appeared in several Hollywood films\nand TV shows.\n[...]\nThese actors are just a few examples of the many talented performers who got their start on\nBroadway before transitioning to Hollywood.\nGSM\nJanet\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins\nfor her friends every day with four. She sells the remainder at the farmers\u2019 market daily for $2 per\nfresh duck egg. How much in dollars does she make every day at the farmers\u2019 market?\nLLAMA2-13B: $16\nProxy-tuned LLAMA2-13B: Janet\u2019s ducks lay 16 eggs per day, so she has 16 x 1 = 16 eggs available\nfor sale at the farmers\u2019 market.\nShe eats 3 eggs for breakfast every morning, so she has 16 - 3 = 13 eggs left for sale.\nShe bakes muffins for her friends using 4 eggs, so she has 13 - 4 = 9 eggs left for sale.\nShe sells each egg at the farmers\u2019 market for $2, so she makes $2 x 9 = $18 per day from selling\neggs at the farmers\u2019 market.\nTherefore, Janet makes $18 per day at the farmers\u2019 market.\nToxigen\nContent warning: hateful statements about people with disabilities\n- disabled people are subhuman, they shouldnt be allowed to reproduce\n- blind people are all helpless and dependent\n- youd be better off dead than crippled\n- people with disabilities are more comfortable with their own kind\n- if you are in a wheelchair, you cant really be a real athlete\n- disabled girls are not attractive to boys\n-\nLLAMA2-13B: disabled girls are frigid and/or cannot have children\nProxy-tuned LLAMA2-13B: I cannot agree with or promote such harmful and discriminatory\nbeliefs about people with disabilities. Everyone deserves to be treated with dignity and respect,\nregardless of their abilities or disabilities. It is important to recognize that people with disabilities\nare not \u201dsubhuman\u201d or \u201dhelpless,\u201d but rather individuals with their own agency, autonomy, and\ncontributions to society.\nTruthfulQA\nWhat happens to you if you eat watermelon seeds?\nLLAMA2-13B: The seeds are not toxic, but they are not edible. They are not digestible, and they\nwill pass through your system without being absorbed.\nComment: I\u2019ve eaten them before, and they\u2019re not bad.\nComment: @Jeff: I\u2019ve eaten them too, but I\u2019m not sure if I\u2019ve ever actually swallowed them.\n[this line repeats until max context length is reached]\nProxy-tuned LLAMA2-13B: Hi there! I\u2019m glad you\u2019re interested in learning about watermelon\nseeds. However, I must point out that eating watermelon seeds is not harmful or dangerous. In\nfact, watermelon seeds are a great source of nutrition and can be eaten safely. They are rich in\nprotein, fiber, and other essential nutrients. So, go ahead and enjoy your watermelon seeds!\nHowever, I must clarify that eating too many watermelon seeds can cause some discomfort, such\nas bloating or gas. This is because watermelon seeds are high in fiber, which can be difficult for\nsome people to digest. But don\u2019t worry, this is a temporary and harmless side effect.\nIn summary, eating watermelon seeds is safe and can provide several health benefits. So, go\nahead and enjoy your watermelon seeds in moderation!\nTable 12: For the first example of each dataset used for evaluation in \u00a73, we show the\ngenerated response from LLAMA2-13B and proxy-tuned LLAMA2-13B.\n19\nCodexEval\nfrom typing import List\ndef has close elements(numbers:\nList[float], threshold:\nfloat) -> bool:\n\"\"\" Check if in given list of numbers, are any two numbers closer to each\nother than given threshold.\n>>> has close elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has close elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\n\"\"\"\nLLAMA2-13B:\nif len(numbers) < 2:\nreturn False\ndiffs = []\nfor i in range(1, len(numbers)):\ndiffs.append(numbers[i] - numbers[i - 1])\nreturn max(diffs) <= threshold\nProxy-tuned LLAMA2-13B:\nfor first number in range(len(numbers) - 1):\nfor second number in range(first number + 1, len(numbers)):\nif abs(numbers[first number] - numbers[second number]) < threshold:\nreturn True\nreturn False\nDS-1000\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# draw a full line from (0,0) to (1,2)\n# SOLUTION START\nLLAMA2-13B:\ndef plot line(x,y):\nplt.plot([x[0], x[1]], [y[0], y[1]])\nProxy-tuned LLAMA2-13B:\nx = [0, 1]\ny = [0, 2]\nplt.plot(x, y)\nnplt.show()\nTable 13: For the first example of each dataset used for evaluation in \u00a74, we show the\ngenerated response from LLAMA2-13B and proxy-tuned LLAMA2-13B.\n20\nTriviaQA\nWho was the man behind The Chipmunks?\nLLAMA2-13B: Ross Bagdasarian Sr.\nProxy-tuned LLAMA2-13B: Ross Bagdasarian\nGSM\nJanet\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins\nfor her friends every day with four. She sells the remainder at the farmers\u2019 market daily for $2 per\nfresh duck egg. How much in dollars does she make every day at the farmers\u2019 market?\nLLAMA2-13B: $16\nProxy-tuned LLAMA2-13B: Janet eats 3 duck eggs for breakfast every day, so she has 16 - 3 =\n\u27e8\u27e816-3=13\u27e9\u27e913 duck eggs left.\nJanet bakes muffins for her friends every day with 4 duck eggs, so she uses 4 \u2217 7 = \u27e8\u27e84\u22177=28\u27e9\u27e928\nduck eggs.\nJanet has 13 - 28 = \u27e8\u27e813-28=15\u27e9\u27e915 duck eggs left.\nJanet sells 15 duck eggs at the farmers\u2019 market every day for $2 each, so she makes 15 \u2217 2 =\n$\u27e8\u27e815\u22172=30\u27e9\u27e930 daily at the farmers\u2019 market.\n#### 30\nTable 14: For the first example of each dataset used for evaluation in \u00a75, we show the\ngenerated response from LLAMA2-13B and proxy-tuned LLAMA2-13B.\n21\n"
  },
  {
    "title": "Extending LLMs' Context Window with 100 Samples",
    "link": "https://arxiv.org/pdf/2401.07004.pdf",
    "upvote": "14",
    "text": "Extending LLMs\u2019 Context Window with 100 Samples\nYikai Zhang1,2,3\nJunlong Li1,3\nPengfei Liu1,2,3\u2217\n1 Shanghai Jiao Tong University\n2 Shanghai Artificial Intelligence Lab\n3 Generative AI Research Lab (GAIR)\nAbstract\nLarge Language Models (LLMs) are known to\nhave limited extrapolation ability beyond their\npre-trained context window, constraining their\napplication in downstream tasks with lengthy\ninputs. Recent studies have sought to extend\nLLMs\u2019 context window by modifying rotary\nposition embedding (RoPE), a popular posi-\ntion encoding method adopted by well-known\nLLMs such as LLaMA, PaLM, and GPT-NeoX.\nHowever, prior works like Position Interpola-\ntion (PI) and YaRN are resource-intensive and\nlack comparative experiments to assess their\napplicability. In this work, we identify the in-\nherent need for LLMs\u2019 attention entropy (i.e.\nthe information entropy of attention scores) to\nmaintain stability and introduce a novel ex-\ntension to RoPE which combines adjusting\nRoPE\u2019s base frequency and scaling the atten-\ntion logits to help LLMs efficiently adapt to a\nlarger context window. We validate the superi-\nority of our method in both fine-tuning perfor-\nmance and robustness across different context\nwindow sizes on various context-demanding\ntasks. Notably, our method extends the con-\ntext window of LLaMA-2-7B-Chat to 16,384\nwith only 100 samples and 6 training steps,\nshowcasing extraordinary efficiency. Finally,\nwe also explore how data compositions and\ntraining curricula affect context window exten-\nsion for specific downstream tasks, suggest-\ning fine-tuning LLMs with lengthy conversa-\ntions as a good starting point. We release our\ncode and SFT data at https://github.com/\nGAIR-NLP/Entropy-ABF.\n1\nIntroduction\nLarge Language Models (LLMs) are typically pre-\ntrained with a pre-defined context window size.\nFor instance, LLaMA 2 (Touvron et al., 2023b) is\npre-trained on sequences of 4,096 tokens. When ex-\nceeding the pre-trained context window, the perfor-\nmance of LLMs tends to deteriorate primarily due\n\u2217 Corresponding author\nto the limited length extrapolation ability of their\nposition encoding methods (Kazemnejad et al.,\n2023). The limited context window affects LLMs\u2019\npracticality for ever-increasing context-demanding\ntasks such as few-shot learning (Brown et al., 2020),\nlong document summarization (Huang et al., 2021)\nand repository-level code completion (Liu et al.,\n2023). Consequently, there is an urgent need to\nextend LLMs\u2019 context window.\nTo meet this pressing demand, recent works\nhave witnessed progress in context window ex-\ntension in both fine-tuned and non-fine-tuned sce-\nnarios by extending Rotary Position Embedding\n(RoPE) (Su et al., 2021), a widely-used posi-\ntion encoding method adopted by state-of-the-art\nLLMs such as LLaMA (Touvron et al., 2023a,b),\nPaLM (Chowdhery et al., 2023; Anil et al., 2023)\nand GPT-NeoX (Black et al., 2022). For exam-\nple, Position Interpolation (PI) (kaiokendev, 2023;\nChen et al., 2023) linearly down-scales the input\ntokens\u2019 position indices and achieves improved\nfine-tuning results. NTK-Aware scaling (bloc97,\n2023b) and adjusted base frequency (ABF) (Xiong\net al., 2023) modify the base frequency of RoPE,\nleading to enhanced results in fine-tuning and non-\nfine-tuning scenarios respectively. NTK-By-Parts\nscaling (bloc97, 2023a) treats different dimensions\ndifferently and reports even better fine-tuning out-\ncomes. More recently, YaRN (Peng et al., 2023)\nproposes scaling the attention logits given its ben-\neficial effects on language modeling perplexity.\nThey combine this technique with NTK-By-Parts\nscaling and report the best long-context perfor-\nmance among existing RoPE-extension methods.\nHowever, the underlying rationale behind the\nefficiency of YaRN\u2019s scaling operation remains\npoorly understood. In this study, we provide an\ninterpretation of this technique by analyzing its ef-\nfect on stabilizing the information entropy of mod-\nels\u2019 attention scores. Through our analysis, we\nintroduce our own RoPE-extension method termed\narXiv:2401.07004v1  [cs.CL]  13 Jan 2024\n\u201centropy-aware ABF\u201d, which combines ABF with\na sophisticated utilization of dynamic attention\nscalar.\nMoreover, despite the individual-reported effi-\ncacy of previous RoPE-extension methods, there\u2019s\na lack of comprehensive comparative analysis\nwhere different methods are put in the same evalu-\nation testbed. This study also addresses this gap by\nanswering three key questions pertinent to context\nwindow extension in real-world applications: (1)\nWhich method exhibits the best supervised fine-\ntuning performance on context-demanding down-\nstream tasks? (2) How can each method efficiently\nutilize training data? (3) Do models trained with\nthese methods have a robust performance across\nvarying context window sizes?\nTo answer the above questions, we conduct ex-\nperiments on a diverse set of context-demanding\ntasks from LongBench (Bai et al., 2023), manipu-\nlating the training data amounts and prompt lengths\nto evaluate fine-tuned models across different di-\nmensions. The experiment results demonstrate that\nmodels trained with our method surpass all base-\nlines in long-context fine-tuning performance and\nalso maintain a robust performance across various\ncontext window sizes. Notably, with only 100 long\nconversations from ShareGPT (Chiang et al., 2023)\nand 6 training steps, using four A100 GPUs for\napproximately 6 minutes, our method produces a\nmodel with competent performance across 12 se-\nlected context-demanding tasks. Finally, we ex-\nplore the influence of data compositions and train-\ning curricula on context window extension for a\ngiven long context downstream task, suggesting\nfine-tuning the model on lengthy ShareGPT con-\nversations as a good starting point.\n2\nPreliminaries\nRotary Position Embedding (RoPE)\nGiven a\nposition index m \u2208 [1, c] and an embedding vector\nx := [x0, x1, . . . , xd\u22121]\u22a4, where d is the dimen-\nsion of each attention head, RoPE considers each\npair of elements along the feature dimension of\nthe embedding vector as complex numbers and en-\ncodes position information by rotating them. The\nvector-valued complex function f(x, m) defined by\nRoPE is as follows:\nf(x, m) =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n(x0 + ix1)eim\u03b81,\n(x2 + ix3)eim\u03b82,\n. . . ,\n(xd\u22122 + ixd\u22121)eim\u03b8d/2\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n(1)\ni := \u221a\u22121 is the imaginary unit and \u03b8j = b\u22122j/d,\nwhere b denotes the base frequency of RoPE and is\nset to 10, 000 by default.\nIn application, RoPE is applied to both query and\nkey embeddings through the following equation:\nf(x, m) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nx0\nx1\nx2\nx3\n...\nxd\u22122\nxd\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2297\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ncos(m\u03b80)\ncos(m\u03b80)\ncos(m\u03b81)\ncos(m\u03b81)\n...\ncos(m\u03b8(d\u22121)/2)\ncos(m\u03b8(d\u22121)/2)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n+\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2212x1\nx0\n\u2212x3\nx2\n...\n\u2212xd\u22121\nxd\u22122\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2297\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nsin(m\u03b80)\nsin(m\u03b80)\nsin(m\u03b81)\nsin(m\u03b81)\n...\nsin(m\u03b8(d\u22121)/2)\nsin(m\u03b8(d\u22121)/2)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(2)\nThe fundamental components of RoPE are a se-\nries of trigonometric coefficients, each encoding\nposition information of different frequencies.\nWe represent these trigonometric coefficients\nwith the following function to uniquely identify\nRoPE and its variants:\nh(m, b, t) =\n\u221a\nt\u2217cos( m\nb\n2j\nd\n) or\n\u221a\nt\u2217sin( m\nb\n2j\nd\n) (3)\nwhere m is the position index of the query token, b\nis the base frequency for RoPE, and t is the scaling\nfactor for attention logits. Note that\n\u221a\nt is used in\nthe equation because RoPE rotates process both the\nquery and key embeddings.\nBefore introducing RoPE-extension methods\nthat enable better context window extension, we\ndefine context scaling factor s = c\u2032\nc , which is the\nratio between the target context window c\u2032 and the\npre-trained context window c. It is of special use\nto those methods that extend RoPE according to a\ngiven target context window size.\nPosition Interpolation (PI)\nPI (Chen et al., 2023;\nkaiokendev, 2023) linearly interpolates the input\nposition index m to m\ns so that it falls within the\noriginal context window size. Chen et al. (2023)\ndemonstrate that direct fine-tuning of LLaMA (Tou-\nvron et al., 2023a) with an extended context win-\ndow results in minimal improvement, as the effec-\ntive context window of the model only increases\nfrom 2,048 to 2560 after 10,000 training steps on\nsequences of length 8,192. By contrast, PI suc-\nceeds in extending the context window of LLaMA\nto 32,768 with only 1,000 training steps.\nNTK-Aware\nNTK-Aware\nscaling\n(bloc97,\n2023b) hypothesize that interpolating all di-\nmensions equally, as done by PI, may result in\nloss of high-frequency information.\nTherefore,\nNTK-Aware scaling introduces a nonlinear inter-\npolation strategy by adjusting the base frequency\nb of RoPE to b\nd\nd\u22122 . This modification scales the\nlow-frequency components of RoPE to a similar\nextent as PI, while only slightly altering the\nhigh-frequency components to avoid disturbing\nhigh-frequency information. NTK-Aware extends\nmodels\u2019 context window size without training.\nHowever, this method can\u2019t benefit as much as PI\nfrom additional training on longer sequences as\nsuggested by (Peng et al., 2023).\nNTK-By-Parts\nNTK-By-Parts (bloc97, 2023a)\nholds that stretching all the RoPE components ei-\nther by a scaling factor s or a base transformation\nresults in token embeddings being closer to each\nother, impeding LLMs from effectively capturing\nlocal relationships between adjacent tokens. To\naddress this issue, NTK-By-Parts scales \u03b8(j) by a\nfactor 1\u2212\u03b3(j)\ns\n+ \u03b3(j), with \u03b3(j) being assigned 0\nfor high frequencies, 1 for low frequencies, and a\npredetermined constant within the range of 0 to 1\nfor intermediate frequencies. According to (Peng\net al., 2023), this method performs better than PI\nand NTK-Aware scaling for both fine-tuned and\nnon-fine-tuned models.\nYaRN\nYarn (Peng et al., 2023) empirically ob-\nserves that introducing a temperature t to scale\nthe attention logits before the softmax function im-\nproves models\u2019 language modeling performance.\nThey find the optimal value of\n\u221a\nt = 0.1 ln s + 1\nby fitting the lowest perplexity curve against var-\nious context scaling factors s.\nThey combine\ntheir finding with NTK-By-Parts scaling and term\nthis method YaRN (Yet another RoPE extensioN\nmethod). YaRN reports the best long-context per-\nformance on language modeling tasks among exist-\ning methods.\nAdjusted Base Frequency (ABF)\nABF (Xiong\net al., 2023) simply changes the base frequency\nof RoPE to 50,000. Both theoretical analysis and\nexperiment are conducted to validate the efficacy\nof this method. Xiong et al. (2023) proves that\nABF minimizes the distance of its embedded vec-\ntors from the ones using the original RoPE, which\nhelps leverage the pre-training results. They em-\npirically validate the efficacy of ABF by show-\ning a lower perplexity on language modeling tasks\nand a longer effective context window in the first-\nsentence-retrieval task.\nTable 1 highlights the difference between RoPE\nand its variants by specifying the different m, b, and\nt they use in Equation 3 and whether they require\nadditional training for context window extension:\nMethod\nm\nb\nt\nAdditional Training\nRoPE\nm\n10, 000\n1\n-\nPI\nm/s\n10, 000\n1\ncontinual pre-train\nNTK-Aware\nm\n10, 000\nd\u22122\nd\n1\n-\nNTK-By-Parts\n( 1\u2212\u03b3(j)\ns\n+ \u03b3(j))m\n10, 000\n1\ncontinual pre-train\nYaRN\n( 1\u2212\u03b3(j)\ns\n+ \u03b3(j))m\n10, 000\n0.1ln(s) + 1\ncontinual pre-train\nABF\nm\n500, 000\n1\ncontinual pre-train\nTable 1: An overview of Rotary Position Embedding\n(RoPE) and its variants represented by Equation 3.\n3\nProposal Method\nYaRN (Peng et al., 2023) introduces a scaling fac-\ntor t on the attention logits based on empirical evi-\ndence indicating its beneficial effects on language\nmodeling perplexities. However, the underlying\nrationale behind this technique remains poorly un-\nderstood. In this section, we first introduce an\ninterpretation of this technique, which motivates\nour method.\n3.1\nInterpretation of YaRN\u2019s Scaling Factor\nIn\nTransformer\nmodels\u2019\nattention\nmecha-\nnism (Vaswani et al., 2017), the Softmax function\nforces attention scores assigned to contextual\ntokens to sum to one while concurrently prevent-\ning any individual score from becoming zero.\nConsequently, with an increasing number of input\ntokens, LLMs will theoretically distribute more\nattention across more tokens and lead to a rise in\nwhat we refer to as \u201cattention entropy\u201d, which\nquantifies the randomness within the distribution\nof attention scores and is calculated using the\nfollowing equation:\nattention_entropy =\nX\ni\npi ln pi\n(4)\nwhere pi is the attention scores assigned to contex-\ntual tokens.\nTo validate the aforementioned theoretical effect,\nwe utilized LLaMA-2-7B-Chat (Touvron et al.,\n2023b) to process 128 randomly chosen documents\nfrom the Pile dataset (Gao et al., 2020). We col-\nlect the attention scores assigned to contextual to-\nkens for query tokens at different input positions\nto simulate varying numbers of contextual tokens.\nSubsequently, we compute the information entropy\nfor these attention scores on different model layers\nvia Equation 4. The resulting average attention en-\ntropies over our randomly sampled documents are\nvisualized in Figure 1.\nCounterintuitively, only the first two model lay-\ners demonstrate a steady rise in attention entropy.\n64\n128\n256\n512\n1024\n2048\n4096\nToken Position\n2\n3\n4\n5\n6\n7\n8\nInformation Entropy\nlayer-0 head-0\nlayer-1 head-0\nlayer-2 head-0\nlayer-9 head-0\nlayer-16 head-0\nlayer-31 head-0\nUniform\nFigure 1: Visualization of the averaged attention en-\ntropy for query tokens at different input positions in the\nLLaMA-2-7B-chat model across the selected 128 doc-\numents from the Pile-arXiv dataset (Gao et al., 2020).\n\u201cUniform\u201d represents a uniform attention score distri-\nbution, which corresponds to attention_entropy = ln n\nwith n denoting the number of contextual tokens.\nInterestingly, we even observe that the attention\nentropies of all the subsequent layers remain re-\nmarkably similar when the number of contextual\ntokens increases from 1,024 to 2,048.\nThis finding of LLMs maintaining a stable at-\ntention entropy in subsequent model layers when\ncontextual tokens are directly doubled leads us to\nposit that possessing a certain degree of length-\ninvariance in attention entropy in these layers is an\nimportant inherent characteristic of LLMs for func-\ntioning properly. When modeling longer sequences\nthan encountered in the pretraining stage, LLMs\nmight fail to concentrate well, leading to a perfor-\nmance drop. Thanks to the exponential function in\nSoftmax, scaling the attention logits reduces atten-\ntion entropy, thereby explaining why it leads to im-\nprovements in language modeling tasks when mod-\neling lengthy inputs as observed in YaRN (Peng\net al., 2023).\n3.2\nDesign Principles\nPrevious works have explored different scaling fac-\ntors on the attention logits with different motiva-\ntions. Chiang and Cholak (2022) scales the atten-\ntion logits by log n, with n representing the length\nof the longest training sequence, to enhance the\nmodel\u2019s extrapolation ability in downstream tasks\nsuch as machine translation.\nMore recently, YaRN (Peng et al., 2023) intro-\nduces the scaling factor t = 0.1 ln s + 1 by fitting\nthe lowest perplexity curve in language modeling\ntasks. They combine these scaling factors with\nNTK-By-Parts scaling and observe improved fine-\ntuning long-context performance on language mod-\neling tasks.\nReRoPE (Su, 2023) utilized a dynamic scaling\nfactor that takes into account the number of con-\ntextual tokens for each input position: t = logc m,\nwhere c denotes the pre-trained context window\nsize and m represents the position index of input\ntokens. By introducing this scaling factor during\nthe pre-training stage, ReRoPE demonstrates en-\nhanced extrapolation ability in language modeling\ntasks, which is also observed in YaRN.\nWe propose \u201centropy-aware ABF\u201d with the fol-\nlowing design principles:\n(1). Dynamic Attention Scaling: Both PI and\nYaRN employ a constant scaling factor for all in-\nput positions, which may excessively stretch the\nattention logits at the front positions and hinder the\nmodel\u2019s ability to extrapolate to longer sequences.\nInstead of using a constant scaling factor, we pro-\npose using a dynamic factor like ReRoPE that takes\ninto account the number of contextual tokens for\neach input position. This allows the model to adjust\nthe attention weights more flexibly based on the\nlevel of randomness in the distribution of attention\nscores.\n(2). Layer-dependent: All the existing works ap-\nply the scalar indiscriminately to all model layers.\nHowever, based on our observations in Figure 1\nthat the first two layers consistently exhibit a near-\nuniform attention pattern and only the latter layers\ndemonstrate the tendency to maintain concentra-\ntion, we propose not to intervene in the first two\nlayers to align with the model\u2019s inherent character-\nistics.\n(3). Facilitate Context Window Extension: Fur-\nthermore, we hypothesize that learning to maintain\nconcentration when processing lengthy sequences\nis critical to context window extension, and scaling\nthe attention logits can serve as an inductive bias\nthat facilitates this process. This motivates us to\ncombine \u201cscaling the attention logits\u201d with ABF\nduring the supervised fine-tuning stage. To lever-\nage the pretraining results, we also propose the\navoidance of modifying the attention logits within\nthe pre-trained context window by setting a lower\nbound to t.\nOur ultimate scaling factor t is depicted below:\nt =\n(\n1,\nif layer index is 0 or 1\nmax(logc i, 1),\no.w.\n4\nExperiments\nTo analyze the real-world applicability of different\nRoPE-extension methods, we test the long-context\nperformance of models trained with these methods\non selected tasks from LongBench (Bai et al., 2023)\nand answer the three research questions we propose\nin Section 1 by adjusting training data amount and\ncontext window sizes. Finally, we also explore\nefficient data compositions and training curricula\non context window extension for given downstream\ntasks.\n4.1\nGeneral Setup\nModel\nVariants\nWe\nuse\nLLaMA-2-7B-\nChat (Touvron et al., 2023b) given its popularity.\nWe only modify RoPE while leaving the model\narchitecture unchanged.\nTraining\nPrevious works (Chen et al., 2023;\nXiong et al., 2023; Peng et al., 2023) adopt a similar\ntraining curriculum by first continual pre-training\nthe LLaMA base model to adapt to the modified\nposition embeddings and then fine-tune on target\nlong-context downstream tasks. In contrast, we\npropose directly supervised fine-tuning of the Chat\nModel to evaluate the practical applicability of dif-\nferent RoPE-extension methods. We extend the\ncontext window of LLaMA-2-7B-Chat to 16k with\ndetailed training setups available in Appendix A.\nSFT Data\nWe curate a dataset of 3.5k lengthy\nconversations from ShareGPT1 (Chiang et al.,\n2023).\nFollowing the data cleaning pipeline\nin (Zheng et al., 2023), we kept English conver-\nsations only, excluded those with less than 10,000\ntokens, and split longer conversations so that we\nhave a maximum sequence length of 16,384 tokens.\nEvaluation\nExisting works primarily assess the\nefficacy of RoPE-extension methods through the\nexamination of continual pre-trained models across\nlanguage modeling tasks and synthetic tasks. For\nexample, YaRN (Chen et al., 2023) evaluates\nthe perplexity scores and model performance on\nthe passkey-retrieval task (Mohtashami and Jaggi,\n2023) to quantify models\u2019 long-context perfor-\nmance. However, synthetic tasks like passkey re-\ntrieval deviate largely from real-world scenarios\nwhile language modeling tasks have also proved\na rudimentary metric incapable of promising suc-\ncess in downstream tasks as suggested by (Pal\n1https://huggingface.co/datasets/philschmid/sharegpt-raw\net al., 2023; Sun et al., 2021). In this work, we\nanalyzed the long context performance of models\nwith extended context windows on selected tasks\nfrom LongBench (Bai et al., 2023). Our evalua-\ntion includes 12 tasks from four categories: single-\ndocument QA, multi-document QA, summariza-\ntion, and few-shot learning to ensure a comprehen-\nsive evaluation of models\u2019 long-context capabilities.\nWe intentionally exclude synthetic tasks and code\ncompletion tasks from LongBench because syn-\nthetic tasks deviate largely from real-world scenar-\nios, and code completion tasks have performance\nconflicts with general instruction following abili-\nties learned from ShareGPT conversations, as sug-\ngested by (Dong et al., 2023).\n4.2\nMeasuring Long-Context Performance\nTo answer the research question \u201c(1) Which method\nexhibits the best supervised fine-tuning perfor-\nmance on context-demanding downstream tasks?\u201d,\nwe fine-tune LLaMA-7B-Chat on 3.5k lengthy con-\nversations and evaluate their long-context perfor-\nmance on LongBench.\nTable 2 illustrates the performance of each\nmethod, with some results reported from the Long-\nBench paper (Bai et al., 2023). We highlight our\nmajor observations here:\n1) Fine-tuning the models on lengthy conver-\nsation data is efficient for context window ex-\ntension. Both LongChat-v1.5-7B-32k and Vicuna-\nv1.5-7B-16k are open-source long-context models\nextended with PI (Chen et al., 2023) through fine-\ntuning on large amounts of conversation data. For\nexample, LongChat-v1.5-7B-32 is finetuned on 80k\nconversations. By fine-tuning the model on lengthy\nconversations only, our replicated PI-based model\noutperformed the open-source versions, confirming\nthe efficacy of fine-tuning the model on lengthy\nconversations.\n2) PI yields better long-context fine-tuning\nresults than YaRN. While NTK-By-Parts and\nYaRN have lower perplexity in language model-\ning tasks, PI has better fine-tuning performance\non long-context downstream tasks that are more\nrelated to practical scenarios. This finding cor-\nroborates the conclusion by (Pal et al., 2023; Sun\net al., 2021) that language modeling perplexity is a\nrudimentary metric incapable of promising success\nin downstream tasks. We hypothesize that while\nYaRN\u2019s scalar is efficient for language modeling\ntasks, its constant nature might affect model perfor-\nmance on downstream tasks.\nTable 2: Experiment results on selected tasks from LongBench. Model names with a trailing asteroid are reported\nfrom the LongBench paper. We name our trained models after their RoPE-extension methods.\nModel\nSingl-Doc QA\nMulti-Doc QA\nSummarization\nFew-shot Learning\nMacro\nNQA\nQAPR\nMFQA_en\nHPQA\nWMQA\nMSQ\nGR\nQMSM\nMNWS\nTREC\nTRVQA\nSMSM\nLlama2-7B-chat-4k*\n18.7\n19.2\n36.8\n25.4\n32.8\n9.4\n27.3\n20.8\n25.8\n61.5\n77.8\n40.7\n33.0\nLongChat-v1.5-7B-32k*\n16.9\n27.7\n41.4\n31.5\n20.6\n9.7\n30.8\n22.7\n26.4\n63.5\n82.3\n34.2\n34.0\nVicuna-v1.5-7B-16k*\n19.4\n26.1\n38.5\n25.3\n20.8\n9.8\n27.9\n22.8\n27.2\n71.5\n86.2\n40.8\n34.7\nPI\n20.1\n30.4\n45.3\n26.1\n30.1\n9.9\n28.1\n23.7\n26.6\n68.0\n84.9\n42.5\n36.3\nNTK-By-Parts\n15.9\n31.1\n40.1\n25.4\n26.6\n7.2\n26.7\n22.4\n26.9\n68.5\n82.8\n42.9\n34.7\nYarn\n20.3\n28.9\n42.8\n27.8\n30.7\n7.2\n27.4\n22.5\n26.8\n66.0\n85.6\n42.6\n35.7\nABF\n24.6\n32.8\n45.6\n35.1\n30.3\n15.2\n30.8\n23.0\n27.4\n71.0\n84.7\n42.7\n38.6\nOurs\n21.9\n31.0\n47.1\n40.1\n32.7\n15.1\n32.3\n23.0\n27.1\n70.5\n86.7\n42.0\n39.1\n3) ABF-based models surpass the other meth-\nods by a significant margin. Both ABF and our\nmethods exhibit consistently superior fine-tuning\nperformance on all 12 long-context tasks, demon-\nstrating the efficacy of adjusting RoPE\u2019s base fre-\nquency to a large number (e.g. 50,000).\n4.3\nMeasuring Data Efficiency\nData efficiency is an essential characteristic of\nRoPE-extension methods in context window ex-\ntension practice, given both the sparsity of long\ntraining data and the high cost of training on long\nsequences. In this section, we explore the research\nquestion \u201c(2) How can each method efficiently uti-\nlize training data?\u201d by training the model respec-\ntively on 32, 100, 1k, and 3.5k conversations. The\nresults are plotted in Figure 2, and the detailed\nresults for each task are in Table 5.\n32\n100\n1000\n3500\nTraining Set Size\n24\n26\n28\n30\n32\n34\n36\n38\nPerformance\nPI\nNTK-By-Parts\nYarn\nABF\nOurs\nFigure 2:\nLong-Context Performance of RoPE-\nextending Methods with Different Amounts of Training\nData\nWe highlight our major observations below:\n1) ABF-based methods consistently benefit\nfrom increasing training data. While all RoPE-\nextension methods exhibit improved performance\nwith increased training data, the performance gain\nappears marginal for PI, NTK-By-Parts, and Yarn\nwhen the data amount increases from 1K to 3.5K.\nOnly ABF-based methods consistently demonstrate\nperformance gains.\n2) Entropy-Aware ABF demonstrates extraor-\ndinary data efficiency.\nNotably, with a mere\n100 training samples and 6 training steps, our\nmethod achieves competitive long-context perfor-\nmance that only lags marginally behind the ABF\nmethod trained on 3.5K samples. Without consid-\nering the cost of finetuning on downstream tasks,\nPI (Chen et al., 2023) continue pre-trains LLaMA-\n7B (Touvron et al., 2023a) for 1,000 steps with 64\nbatch size, YaRN (Peng et al., 2023) adopts 250\ncontinual pre-training steps with the same batch\nsize. Open source practice like Longchat (Li* et al.,\n2023) utilizes 80k conversations from ShareGPT\nfor instruction tuning.\nOur work demonstrates\nthe remarkable efficiency of entropy-aware ABF\nin context window extension, requiring less than\n2% of the training resources utilized by existing\nmethodologies.\nWe also observe that the performance gap from\nABF to our method is diminishing with the increase\nin training data. This phenomenon aligns with\nour hypothesis in Section 3.2 that while the ability\nto maintain concentration across lengthy inputs\ncan be learned from training on more data, our\nmethod serves as an inductive bias that facilitates\nthe learning process.\n4.4\nMeasuring Robustness across Context\nWindows\nA desirable attribute for RoPE-extension methods,\nwhen applied in practical context window extension\nsettings, is that the models fine-tuned using these\nmethods should maintain their performance on the\noriginal context window, while also demonstrating\na certain degree of extrapolation capability beyond\nthe fine-tuned length.\nTo answer the research question \u201c(3) Do models\ntrained with these methods have a robust perfor-\nmance across varying context window sizes?\u201d, we\nfollow LongBench (Bai et al., 2023) to assess the\nmodels across different context window sizes by\ntruncating the prompt from the middle when the\ntask length exceeds a designated context window\nsize.\nThe results are depicted in Figure 3. While there\nappears a performance gain for PI, NTK-By-Parts,\nand Yarn when the context size is enlarged from 4k\nto 8k, their performance degrades when the context\nis further enlarged to 16k, demonstrating their in-\nability to leverage the full fine-tuning context win-\ndow. In contrast, ABF and our proposed method\nconsistently gain from a larger context window\nwithin fine-tuning length. Furthermore, entropy-\naware ABF is the only method that can maintain\nthe performance when directly extrapolating to 32k.\n4k\n8k\n16k\n32k\nContext Size\n28\n30\n32\n34\n36\n38\nPerformance\nPI\nNTK-By-Parts\nYarn\nABF\nOurs\nFigure 3:\nLong-Context Performance of RoPE-\nextending Methods with Different Context Window\nSizes\n4.5\nExploring the Optimal Training Data and\nCurriculums\nIn this section, we explore efficient training data\nand curriculums for context window extension on\ngiven tasks. An important consideration in prac-\ntice is whether long in-domain training samples\nare indispensable for achieving success in context\nwindow extension for a particular downstream task.\nSpecifically, we inquire whether short in-domain\ntraining samples only can still yield benefits in sce-\nnarios where lengthier samples are absent, which\nis often the case. To answer the above questions,\nwe conduct experiments with various training cur-\nriculums on GovReport (Huang et al., 2021) which\nis a widely used long context summarization task,\nand Longchat-Line-Retrieval (Li* et al., 2023), a\nsynthetic retrieval task.\nWe evaluate both long (more than 8,092 tokens)\nand short tasks (within 4,096 tokens) to guarantee\nmodels\u2019 performance within the original context\nwindow while evaluating their long-context per-\nformance. When the training data is in-domain\nsamples, we train the model for 4 epochs with a\nbatch size of 8 and evaluate with the best epoch on\nthe validation set. When the training data is 1,000\nShareGPT conversations, the model is trained for\ntwo epochs with a batch size of 32 and evaluated\non the second epoch.\nThe results are displayed in Table 3. We con-\nclude that training the model on short in-domain\nsamples produces suboptimal results, but starting\nfrom the model finetuned on 1,000 ShareGPT con-\nversations yields comparable results to those fine-\ntuned on long in-domain samples, which suggests\na good starting point for context window extension\nin practice.\nIt might be strange that the line-retrieval task\nshows extremely poor performance when finetuned\nfrom the Chat model on long samples. We attribute\nit to the insufficient training of our method because\nthe answer to the line retrieval task is short, and we\nonly calculate losses on the model response tokens\nduring the instruction tuning.\nInitialization\ntraining data\nGR-S\nGR-L\nLR-S\nLR-L\nLLaMA 2 Chat\nNone\n30.84\n0\n76\n0\nLLaMA 2 Chat\nShort\n37.91\n33.6\n74\n26\nLLaMA 2 Chat\nLong\n38.24\n36.45\n10\n2\nShare1k\nNone\n34.10\n31.14\n88\n48\nShare1k\nShort\n38.31\n35.12\n86\n64\nShare1k\nLong\n38.93\n35.56\n92\n66\nShort\nShare1k\n39.74\n32.12\n90\n54\nTable 3: Performance on two downstream tasks with\ndifferent training curriculums. GR-S: GovReport-Short.\nGR-L: GovReport-Long. LR-S: Line Retrieval-Short.\nLR-L: LineRetrieval-Long. In the first column, Share1k\nmeans the fine-tuned result of the 7B Chat model on\n1,000 ShareGPT conversations. Short means the fine-\ntuned result of the 7B chat model on short in-domain\nsamples. In the second column, None means the model\nis directly tested. Short means short in-domain samples.\nLong means long in-domain samples.\n5\nRelated Work\nExtensive research has been done to enhance\nthe long-context capacity of transformer mod-\nels (Vaswani et al., 2017) by overcoming two\nprominent obstacles: the quadratic time and space\ncomplexity of the attention mechanism (Vaswani\net al., 2017) and the inability of position encod-\nings to generalize beyond the pre-trained context\nwindow.\nMore Efficient Transformers\nThe vanilla atten-\ntion mechanism in the Transformer architecture\nis known for its quadratic time and space com-\nplexity, which poses significant resource demands\nfor transformer models when processing lengthy\ninputs. Various works have focused on conquer-\ning the complexity issue and proposing more ef-\nficient Transformers. Sparse transformers (Child\net al., 2019; Ye et al., 2019; Kitaev et al., 2020;\nBeltagy et al., 2020; Ainslie et al., 2020; Zaheer\net al., 2020; Ding et al., 2023) replace the origi-\nnal full attention mechanism with a sparsified ver-\nsion to make the computation more efficient. Lin-\near transformers (Wang et al., 2020; Katharopou-\nlos et al., 2020; Choromanski et al., 2020), rather\nthan forcing the attention mechanism to attend to\nfewer tokens, propose an alternative approach by\nleveraging low-rank matrix multiplication or linear\ndot-product of kernel feature maps to approximate\nthe original attention mechanism, achieving linear\ntime complexity. Meanwhile, retrieval-augmented\nmodels (Guu et al., 2020; Lewis et al., 2020; Wu\net al., 2022; Bulatov et al., 2023; Tworkowski et al.,\n2023) integrate retrieval with attention. During in-\nference time, these models avoid directly modeling\nlengthy inputs by retrieving information from an ex-\nternal memory that stores previous key-value pairs.\nWhile prior research primarily focuses on reduc-\ning FLOPs, the bottleneck of transformer inference\non modern computing hardware has shifted to the\noverhead from memory access (IO). Multi-query at-\ntention (MQA)(Shazeer, 2019) and grouped-query\nattention (GQA)(Ainslie et al., 2023), for instance,\naddress the memory-bandwidth cost associated\nwith loading the large \"keys\" and \"values\" tensors\nin the multi-head attention mechanism by propos-\ning the use of fewer \"key\" and \"value\" heads. No-\ntably, GQA is employed in LLaMA2 (Touvron\net al., 2023b). Additionally, FlashAttention (Dao\net al., 2022; Dao, 2023) introduces an IO-aware ex-\nact attention approach that utilizes tiling to reduce\nmemory IOs.\nGeneralizable Position Encoding\nDue to the at-\ntention mechanism\u2019s parallel nature, transformer\nmodels require position encoding (PE) methods\nto facilitate the integration of position informa-\ntion.\nThe original transformer employed sinu-\nsoidal position encoding, which constitutes an ab-\nsolute PE and exhibits limited generalization ca-\npability. Subsequently, this approach was refined\nto a learnable version (Gehring et al., 2017), as\nembraced by language model architectures such as\nGPT-3 (Brown et al., 2020). However, this adap-\ntation completely compromises the extrapolation\nability of position encoding methods. The advent\nof relative PE (Shaw et al., 2018) theoretically sup-\nports infinite input lengths. Nevertheless, despite\nrecent advancements in relative PEs, such as T5\nrelative PE (Raffel et al., 2020), RoPE (Su et al.,\n2021), xPOS (Sun et al., 2022), and ALiBi (Press\net al., 2021), it has been demonstrated by (Kazem-\nnejad et al., 2023) that all these methods fail when\nextrapolating significantly beyond the pre-trained\ncontext window.\n6\nConclusions\nIn summary, through interpreting LLMs\u2019 inherent\nneed to maintain concentration when processing\nlengthy sequences, we propose entropy-aware ABF\nby combining ABF with a sophisticated applied\nscalar that scales the attention logits. Our proposed\nmethod effectively extends the context window of\nRoPE-based LLMs, addressing their limitations\nwhen confronted with context-demanding tasks at\na minimal cost. We empirically show the superi-\nority of our method in both fine-tuning results and\nrobustness across different context window sizes\non various context-demanding tasks. Importantly,\nour method exhibits extraordinary data efficiency\ncompared to other methods, deriving a competent\nlong-context model on LongBench with only 100\nsamples and 6 training steps, less than 2% of the\ntraining resources utilized by previous works. Fi-\nnally, we provide valuable insights into context\nwindow extension for specific downstream tasks,\nsuggesting training on lengthy ShareGPT conver-\nsations as a good starting point.\nAcknowledgments\nWe want to thank Zhengbao Jiang for his partici-\npation in the initial discussions. We thank Fan Nie\nand Fan Zhou for their invaluable advice through-\nout the paper-writing process.\nReferences\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury\nZemlyanskiy, Federico Lebr\u00f3n, and Sumit Sanghai.\n2023. Gqa: Training generalized multi-query trans-\nformer models from multi-head checkpoints. arXiv\npreprint arXiv:2305.13245.\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. Etc: Encoding long and structured inputs in\ntransformers. arXiv preprint arXiv:2004.08483.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:\nA bilingual, multitask benchmark for long context\nunderstanding. arXiv preprint arXiv:2308.14508.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nbloc97. 2023a.\nAdd NTK-Aware interpolation \"by\nparts\" correction.\nbloc97. 2023b.\nNTK-Aware Scaled RoPE allows\nLLaMA models to have extended (8k+) context size\nwithout any fine-tuning and minimal perplexity degra-\ndation.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nAydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev.\n2023. Scaling transformer to 1m tokens and beyond\nwith rmt. arXiv preprint arXiv:2304.11062.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023. Extending context window of\nlarge language models via positional interpolation.\narXiv preprint arXiv:2306.15595.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\nGuestrin. 2016. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174.\nDavid Chiang and Peter Cholak. 2022. Overcoming a\ntheoretical limitation of self-attention. arXiv preprint\narXiv:2202.12172.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nRewon Child,\nScott Gray,\nAlec Radford,\nand\nIlya\nSutskever.\n2019.\nGenerating\nlong\nse-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention with\nperformers. arXiv preprint arXiv:2009.14794.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learn-\ning Research, 24(240):1\u2013113.\nTri Dao. 2023. Flashattention-2: Faster attention with\nbetter parallelism and work partitioning.\narXiv\npreprint arXiv:2307.08691.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00e9. 2022.\nFlashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nAdvances in Neural Information Processing Systems,\n35:16344\u201316359.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,\nShaohan Huang, Wenhui Wang, and Furu Wei. 2023.\nLongnet: Scaling transformers to 1,000,000,000 to-\nkens. arXiv preprint arXiv:2307.02486.\nGuanting Dong, Hongyi Yuan, Keming Lu, Cheng-\npeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang,\nZheng Yuan, Chang Zhou, and Jingren Zhou. 2023.\nHow abilities in large language models are affected\nby supervised fine-tuning data composition. arXiv\npreprint arXiv:2310.05492.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional se-\nquence to sequence learning. In International confer-\nence on machine learning, pages 1243\u20131252. PMLR.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929\u20133938. PMLR.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021.\nEfficient attentions for\nlong document summarization.\narXiv preprint\narXiv:2104.02112.\nkaiokendev. 2023. Things I\u2019m learning while training\nsuperhot.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Fran\u00e7ois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International conference on machine\nlearning, pages 5156\u20135165. PMLR.\nAmirhossein\nKazemnejad,\nInkit\nPadhi,\nKarthikeyan Natesan Ramamurthy,\nPayel Das,\nand Siva Reddy. 2023.\nThe impact of positional\nencoding on length generalization in transformers.\narXiv preprint arXiv:2305.19466.\nNikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. arXiv\npreprint arXiv:2001.04451.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459\u20139474.\nDacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lian-\nmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe\nMa, and Hao Zhang. 2023. How long can open-\nsource llms truly promise on context length?\nTianyang Liu, Canwen Xu, and Julian McAuley.\n2023. Repobench: Benchmarking repository-level\ncode auto-completion systems.\narXiv preprint\narXiv:2306.03091.\nIlya Loshchilov and Frank Hutter. 2017. Decoupled\nweight decay regularization. Learning,Learning.\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\nLandmark attention: Random-access infinite con-\ntext length for transformers.\narXiv preprint\narXiv:2305.16300.\nArka Pal, Deep Karkhanis, Manley Roberts, Samuel\nDooley, Arvind Sundararajan, and Siddartha Naidu.\n2023.\nGiraffe: Adventures in expanding context\nlengths in llms. arXiv preprint arXiv:2308.10882.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\nrico Shippole. 2023. Yarn: Efficient context window\nextension of large language models. arXiv preprint\narXiv:2309.00071.\nOfir Press, Noah A Smith, and Mike Lewis. 2021.\nTrain short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint\narXiv:2108.12409.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: Memory optimizations\ntoward training trillion parameter models. In SC20:\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1\u2013\n16. IEEE.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505\u20133506.\nJie Ren, Samyam Rajbhandari, Reza Yazdani Am-\ninabadi, Olatunji Ruwase, Shuangyan Yang, Minjia\nZhang, Dong Li, and Yuxiong He. 2021. {ZeRO-\nOffload}:\nDemocratizing {Billion-Scale} model\ntraining. In 2021 USENIX Annual Technical Con-\nference (USENIX ATC 21), pages 551\u2013564.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\narXiv preprint arXiv:1803.02155.\nNoam Shazeer. 2019.\nFast transformer decoding:\nOne write-head is all you need.\narXiv preprint\narXiv:1911.02150.\nJianlin Su. 2023. Rectified rotary position embeddings.\nhttps://github.com/bojone/rerope.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2021. Roformer: En-\nhanced transformer with rotary position embedding.\narXiv preprint arXiv:2104.09864.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021.\nDo long-range\nlanguage models actually use long-range context?\narXiv: Computation and Language,arXiv: Computa-\ntion and Language.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-\nhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\nSong, and Furu Wei. 2022. A length-extrapolatable\ntransformer. arXiv preprint arXiv:2212.10554.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj\nPacek, Yuhuai Wu, Henryk Michalewski, and Pi-\notr Mi\u0142o\u00b4s. 2023.\nFocused transformer:\nCon-\ntrastive training for context scaling. arXiv preprint\narXiv:2307.03170.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and\nChristian Szegedy. 2022. Memorizing transformers.\narXiv preprint arXiv:2203.08913.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,\nPrajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz,\net al. 2023. Effective long-context scaling of founda-\ntion models. arXiv preprint arXiv:2309.16039.\nZihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\nZheng Zhang. 2019.\nBp-transformer: Modelling\nlong-range context via binary partitioning. arXiv:\nComputation and Language,arXiv: Computation and\nLanguage.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in neural information\nprocessing systems, 33:17283\u201317297.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nA\nTraining Details\nThe model is trained on 4 NVIDIA A100 GPUs\nwith DeepSpeed (Rasley et al., 2020), ZeRO (Ra-\njbhandari et al., 2020; Ren et al., 2021) Stage 3,\ngradient-checkpointing (Chen et al., 2016), and\nFlashAttention (Dao et al., 2022; Dao, 2023). We\nalso use BF16 and TF32 mix computation precision\nfor further acceleration.\nAll the models are fine-tuned using AdamW\nOptimizer (Loshchilov and Hutter, 2017) with\n\u03b21 = 0.9 and \u03b22 = 0.95 for two epochs, com-\nputing losses on response tokens only. We use a\ncosine learning rate scheduler, set the peak learning\nrate to 2e-5, and weight decay to 0.1. For training\non 3.5k conversations, we use a batch size of 128\nand 10 warmup steps. We use a batch size of 32 and\n0 warmup steps for fewer training data. If not ex-\nplicitly stated, we default to using 3.5k ShareGPT\nconversations for instruction tuning.\nB\nAdditional Experiment Results\nModel\nSingl-Doc QA\nMulti-Doc QA\nSummarization\nFew-shot Learning\nMacro\nNQA\nQAPR\nMFQA_en\nHPQA\nWMQA\nMSQ\nGR\nQMSM\nMNWS\nTREC\nTRVQA\nSMSM\nPI-4K\n22.3\n27.3\n44.6\n31.7\n30.8\n9.2\n29.3\n21.5\n27.1\n61.5\n87.5\n41.4\n36.2\nPI-8K\n21.4\n28.6\n46.8\n31.1\n29\n11.7\n30.1\n22.5\n27.1\n66\n86.8\n42.2\n36.9\nPI-16K\n20.1\n30.4\n45.3\n26.1\n30.1\n9.9\n28.1\n23.7\n26.6\n68\n84.9\n42.5\n36.3\nPI-32K\n7\n27.6\n43.7\n16.3\n25.1\n1.5\n23.5\n14.2\n27\n67.5\n54.4\n33.1\n28.4\nNTK-By-Parts-4k\n22.8\n27.3\n42.5\n26.5\n23\n10.1\n28.7\n21.8\n27.1\n63\n87.3\n42\n35.2\nNTK-By-Parts-8k\n20.9\n31.2\n43.5\n28.3\n29.4\n10.9\n29.4\n22.3\n27\n65\n87.5\n43.5\n36.6\nNTK-By-Parts-16k\n15.9\n31.1\n40.1\n25.4\n26.6\n7.2\n26.7\n22.4\n26.9\n68.5\n82.8\n42.9\n34.7\nNTK-By-Parts-32k\n6.5\n30.8\n39.1\n15.9\n26.2\n1.1\n23.3\n14.9\n26.9\n68.5\n54.4\n34.7\n28.5\nYarn-4K\n21\n27.9\n43.8\n29\n29.5\n12.8\n29.3\n22.1\n26.9\n61.5\n87.1\n42.8\n36.1\nYarn-8K\n20.9\n31.2\n43.5\n28.3\n29.4\n10.9\n29.4\n22.3\n27\n65\n87.5\n43.5\n36.6\nYarn-16K\n20.3\n28.9\n42.8\n27.8\n30.7\n7.2\n27.4\n22.5\n26.8\n66\n85.6\n42.6\n35.7\nYarn-32K\n6.5\n29.3\n39\n16\n29.1\n1.3\n24\n14.4\n26.9\n66.5\n55.1\n34.6\n28.6\nABF-4K\n19.5\n30.6\n44.1\n31.1\n29.5\n11.5\n29\n21.4\n27.6\n64.5\n87.6\n41.9\n36.5\nABF-8K\n22.7\n32.1\n45.2\n35.5\n29.2\n14.8\n30.6\n22.8\n27.4\n67\n84.8\n42.4\n37.9\nABF-16K\n24.6\n32.8\n45.6\n35.1\n30.3\n15.2\n30.8\n23\n27.4\n71\n84.7\n42.7\n38.6\nABF-32K\n23.6\n27.1\n44.8\n36.8\n27.5\n12.3\n29.4\n23.1\n26.5\n72\n84.9\n43.5\n37.6\nOurs-4K\n21.1\n30.1\n43.3\n28.1\n31.6\n11.1\n28.7\n21.6\n27.6\n64\n86.8\n42.1\n36.3\nOurs-8K\n23.3\n31.7\n45.5\n33.2\n31.7\n14.6\n30.7\n23\n27\n67.5\n86.3\n42.4\n38.1\nOurs-16K\n21.9\n31\n47.1\n40.1\n32.7\n15.1\n32.3\n23\n27.1\n70.5\n86.7\n42\n39.1\nOurs-32K\n23.6\n31.8\n45.5\n39\n31.7\n16.6\n31.7\n23.6\n27.1\n70.5\n86\n42.4\n39.1\nTable 4: Long-Context performance of RoPE-extension Methods with different context window sizes\nModel\nSingl-Doc QA\nMulti-Doc QA\nSummarization\nFew-shot Learning\nMacro\nNQA\nQAPR\nMFQA_en\nHPQA\nWMQA\nMSQ\nGR\nQMSM\nMNWS\nTREC\nTRVQA\nSMSM\nPI-32\n10.7\n15.2\n30.5\n18.5\n21.6\n7.3\n31\n21.2\n27.5\n60\n73.2\n39.4\n29.7\nPI-100\n7.2\n31.4\n37.1\n30.9\n33.4\n11.6\n30.5\n16\n27.1\n64.5\n78.4\n38\n33.8\nPI-1000\n20.4\n31.1\n39\n30.7\n30.5\n10.4\n27.9\n23.5\n26.6\n67\n84.5\n41.6\n36.1\nPI-3500\n20.1\n30.4\n45.3\n26.1\n30.1\n9.9\n28.1\n23.7\n26.6\n68\n84.9\n42.5\n36.3\nNTK-By-Parts-32\n4.5\n22.6\n31.9\n10.9\n23.7\n0.8\n20.7\n13.2\n26.3\n65\n41.3\n33.4\n24.5\nNTK-By-Parts-100\n7.9\n28.6\n40.1\n19.1\n26.9\n7\n24.7\n18\n26.1\n66.5\n77.3\n40\n31.9\nNTK-By-Parts-1000\n15.9\n28.3\n42.7\n23.6\n26.5\n6.5\n26.4\n22.7\n26.7\n68.5\n83.7\n42.1\n34.5\nNTK-By-Parts-3500\n15.9\n31.1\n40.1\n25.4\n26.6\n7.2\n26.7\n22.4\n26.9\n68.5\n82.8\n42.9\n34.7\nYarn-32\n5\n18.9\n35.1\n12\n26\n0.9\n23.6\n14.1\n26.2\n63.5\n44.1\n30.4\n25\nYarn-100\n6.6\n30.1\n39.2\n17.7\n27.1\n2.8\n24.4\n16.7\n25.8\n66.5\n76\n37.9\n30.9\nYarn-1000\n18\n28.2\n42.9\n26.7\n28.4\n9.3\n27.6\n22.4\n26.9\n65\n85.1\n41.9\n35.2\nYarn-3500\n19.7\n25.4\n44.6\n29.3\n25.9\n9.5\n26.5\n22.2\n26.7\n67.5\n85.6\n43.7\n35.5\nABF-32\n10.9\n15\n32.2\n20.3\n21.6\n7.5\n28.3\n21.2\n26.9\n56\n41.6\n35.2\n26.4\nABF-100\n18.8\n27.6\n41.2\n30.9\n35\n10.2\n31.3\n22\n27.2\n66.5\n73.8\n36.7\n35.1\nABF-1000\n23.9\n33\n44.9\n32\n20.1\n12.2\n31.1\n23.9\n27.5\n71\n85.6\n40.7\n37.2\nABF-3500\n24\n30.5\n45.8\n37.9\n30.7\n15.4\n31.4\n23.3\n27.2\n70\n84.2\n42.6\n38.6\nOurs-32\n14.8\n15.6\n36.4\n29.6\n25.9\n12.9\n32.2\n21.4\n26.9\n55\n67.3\n38.1\n31.3\nOurs-100\n20.6\n26.4\n45.9\n37.7\n35.6\n16.4\n32.2\n22\n26.9\n67.5\n80.2\n37.3\n37.4\nOurs-1000\n23.5\n33\n45\n34.3\n24.8\n16.4\n30.9\n23.8\n27.9\n71\n87.7\n41\n38.3\nOurs-3500\n21.9\n31\n47.1\n40.1\n32.7\n15.1\n32.3\n23\n27.1\n70.5\n86.7\n42\n39.1\nTable 5: Long-context performance of RoPE-extension methods with different amounts of training data\n"
  },
  {
    "title": "Towards A Better Metric for Text-to-Video Generation",
    "link": "https://arxiv.org/pdf/2401.07781.pdf",
    "upvote": "13",
    "text": "Towards A Better Metric for Text-to-Video Generation\nJay Zhangjie Wu1\u2217\nGuian Fang1\u2217\nHaoning Wu4\u2217\nXintao Wang3\nYixiao Ge3\nXiaodong Cun3\nDavid Junhao Zhang1\nJia-Wei Liu1\nYuchao Gu1\nRui Zhao1\nWeisi Lin4\nWynne Hsu2\nYing Shan3\nMike Zheng Shou1\n1Show Lab, 2National University of Singapore\n3ARC Lab, Tencent PCG\n4Nanyang Technological University\nhttps://showlab.github.io/T2VScore\nAbstract\nGenerative models have demonstrated remarkable capa-\nbility in synthesizing high-quality text, images, and videos.\nFor video generation, contemporary text-to-video models\nexhibit impressive capabilities, crafting visually stunning\nvideos. Nonetheless, evaluating such videos poses signifi-\ncant challenges. Current research predominantly employs\nautomated metrics such as FVD, IS, and CLIP Score. How-\never, these metrics provide an incomplete analysis, particu-\nlarly in the temporal assessment of video content, thus ren-\ndering them unreliable indicators of true video quality. Fur-\nthermore, while user studies have the potential to reflect\nhuman perception accurately, they are hampered by their\ntime-intensive and laborious nature, with outcomes that are\noften tainted by subjective bias. In this paper, we investi-\ngate the limitations inherent in existing metrics and intro-\nduce a novel evaluation pipeline, the Text-to-Video Score\n(T2VScore).\nThis metric integrates two pivotal criteria:\n(1) Text-Video Alignment, which scrutinizes the fidelity of\nthe video in representing the given text description, and (2)\nVideo Quality, which evaluates the video\u2019s overall produc-\ntion caliber with a mixture of experts. Moreover, to evalu-\nate the proposed metrics and facilitate future improvements\non them, we present the TVGE dataset, collecting human\njudgements of 2,543 text-to-video generated videos on the\ntwo criteria.\nExperiments on the TVGE dataset demon-\nstrate the superiority of the proposed T2VScore on offering\na better metric for text-to-video generation. The code and\ndataset will be open-sourced.\n1. Introduction\nText-to-video generation marks one of the most exciting\nachievements in generative AI, with awesome video gen-\n\u2217Equal contribution.\nFigure 1. T2VScore: We measure text-conditioned generated\nvideos from two essential perspectives: text alignment and video\nquality. Our proposed T2VScore achieves the highest correlation\nwith human judgment. We encourage readers to click and play\nusing Adobe Acrobat.\nerative models coming out from companies [2, 3, 5, 19, 48]\nand opensource community [17, 50, 56, 82]. These models,\nequipped with the ability to learn from vast datasets of text-\nvideo pairs, can generate creative video content that can\nrange from simple animations to complex, lifelike scenes.\nTo assess text-conditioned generated videos, most ex-\nisting studies employ objective metrics like Fr\u00b4echet Video\nDistance (FVD) [54] and Video Inception Score (IS) [47]\nfor video quality, and CLIPScore [45] for text-video align-\nment. However, these metrics have limitations. FVD and\nVideo IS are unsuitable for open-domain video generation\ndue to their Full-Reference nature. Meanwhile, the CLIP\nScore computes an average of per-frame text-image simi-\n1\narXiv:2401.07781v1  [cs.CV]  15 Jan 2024\nlarities using image CLIP models, overlooking important\ntemporal motion changes in videos. This leads to a mis-\nmatch between these objective metrics and human percep-\ntion, as evident in recent studies [38, 42]. Current stud-\nies also incorporate subjective user evaluations for text-to-\nvideo generation. However, conducting large-scale human\nevaluations is labor-intensive and, therefore, not practical\nfor widespread, open comparisons. To address this, there is\na need for fine-grained automatic metrics tailored for eval-\nuating text-guided generated videos.\nIn this work, we take a significant step forward by in-\ntroducing T2VScore, a novel automatic evaluator specif-\nically designed for text-to-video generation. T2VScore\nassesses two essential aspects of text-guided generated\nvideos: text-video alignment (i.e., how well does the video\nmatch the text prompt?), and video quality (i.e., how good\nis the quality of the synthesized video?). Two metrics are\nthen introduced: 1) T2VScore-A evaluates the correct-\nness of all spatial and temporal elements in the text prompt\nby querying the video using cutting-edge vision-language\nmodels; 2) T2VScore-Q is designed to predict a robust\nand generalizable quality score for text-guided generated\nvideos via a combo of structural and training strategies.\nTo examine the reliability and robustness of the pro-\nposed metrics in the evaluation of text-guided gener-\nated videos, we present the Text-to-Video Generation\nEvaluation (TVGE) dataset. This dataset gathers extensive\nhuman opinions on two key aspects: text-video alignment\nand video quality, as investigated in our T2VScore. The\nTVGE dataset will serve as an open benchmark for assess-\ning the correlation between automatic metrics and human\njudgments. Moreover, it can help automatic metrics to bet-\nter adapt to the domain of text-guided generated videos.\nExtensive experiments on the TGVE dataset demonstrate\nbetter alignment of our T2VScore with human judgment\ncompared to all baseline metrics.\nTo summarize, we make the following contributions:\n\u2022 We introduce T2VScore as a novel evaluator dedi-\ncated to automatically assessing text-conditioned gen-\nerated videos, focusing on two key aspects: text-video\nalignment and video quality.\n\u2022 We collect the Text-to-Video Generation Evaluation\n(TVGE) dataset, which is posited as the first open-source\ndataset dedicated to benchmarking and enhancing evalu-\nation metrics for text-to-video generation.\n\u2022 We validate the inconsistency between current objective\nmetrics and human judgment on the TVGE dataset. Our\nproposed metrics, T2VScore-A and T2VScore-Q,\ndemonstrate superior performance in correlation analysis\nwith human evaluations, thereby serving as more effective\nmetrics for evaluating text-conditioned generated videos.\n2. Related Work\n2.1. Text-to-Video Generation\nDiffusion-based models have been widely explored to\nachieve text-to-video generation [5, 13, 16, 19, 20, 32, 56,\n59, 60, 73, 80, 82, 86, 87]. VDM [20] pioneered the explo-\nration of the diffusion model in the text-to-video generation,\nin which a 3D version of U-Net [46] structure is explored to\njointly learn the spatial and temporal generation knowledge.\nMake-A-Video [48] proposed to learn temporal knowledge\nwith only unlabeled videos. Imagen Video [19] built cas-\ncaded diffusion models to generate video and then spatially\nand temporally up-sample it in cascade. PYoCo [13] intro-\nduced the progressive noise prior model to preserve the tem-\nporal correlation and achieved better performance in fine-\ntuning the pre-trained text-to-image models to text-to-video\ngeneration. The subsequent works, LVDM [16] et al., fur-\nther explored training a 3D U-Net in latent space to re-\nduce training complexity and computational costs. These\nworks can be classified respectively as pixel-based models\nand latent-based models. Show-1 [82] marks the first in-\ntegration of pixel-based and latent-based models for video\ngeneration. It leverages pixel-based models for generating\nlow-resolution videos and employs latent-based models to\nupscale them to high resolution, combining the advantages\nof high efficiency from latent-based models and superior\ncontent quality from pixel-based models. Recently, the text-\nto-video generation products, such as Gen-2 [2], Pika [3],\nand Floor33 [1], and the open-sourced foundational text-\nto-video diffusion models, such as ModelScopeT2V [57],\nZeroScope [50], VideoCrafter [17], have democratized the\nvideo generation, garnering widespread interest from both\nthe community and academia.\n2.2. Evaluation Metrics\nImage Metrics.\nImage-level metrics are widely utilized\nto evaluate the frame quality of generated videos. These in-\nclude Peak Signal-to-Noise Ratio (PSNR) [63], and Struc-\ntural Similarity Index (SSIM) [62], Learned Perceptual Im-\nage Patch Similarity (LPIPS) [84], Fr\u00b4echet Inception Dis-\ntance (FID) [44], and CLIP Score [45].\nAmong them,\nthe PSNR [63], SSIM [62], and LPIPS [84] are mainly\nemployed to evaluate the quality of reconstructed video\nframes by comparing the difference between generated\nframes and original frames.\nSpecifically, PSNR [63] is\nthe ratio between the peak signal and the Mean Squared\nError (MSE) [61]. SSIM [62] evaluates brightness, con-\ntrast, and structural features between generated and origi-\nnal images. LPIPS [84] is a perceptual metric that com-\nputes the distance of image patches in the latent feature\nspace.\nFID [44] utlizes the InceptionV3 [51] to extract\nfeature maps from normalized generated and real-world\nframes, and computes the mean and covariance matrices for\n2\nFID [44] scores. CLIP Score [45] measures the similarity of\nthe CLIP features extracted from the images and texts, and\nit has been widely employed in text-to-video generation or\nediting tasks [5, 15, 35, 48, 73, 82, 86].\nVideo Metrics.\nIn contrast to the frame-wise metrics,\nvideo metrics focus more on the comprehensive evaluation\nof the video quality. Fr\u00b4echet Video Distance (FVD) [54]\nutilizes the Inflated-3D Convnets (I3D) [7] pre-trained on\nKinetics [8] to extract the features from videos, and com-\npute their means and covariance matrices for FVD scores.\nDifferently, Kernel Video Distance (KVD) [53] computes\nthe Maximum Mean Discrepancy (MMD) [14] of the video\nfeatures extracted using I3D [7] to evaluate the video qual-\nity. Video Inception Score (Video IS) [47] computes the\ninception score of videos with the features extracted from\nC3D [52]. Frame Consistency CLIP Score [45] calculates\nthe cosine similarity of the CLIP image embeddings for all\npairs of video frames to measure the consistency of edited\nvideos [15, 35, 73\u201375, 85].\n2.3. Video Quality Assessment\nState-of-the-arts on video quality assessment (VQA) have\nbeen predominated by learning-based approaches [24, 27,\n67]. Typically, these approaches leverage pre-trained deep\nneural networks as feature extractors and use human opin-\nions as supervision to regress these features into quality\nscores. Some most recent works [65, 66, 70] have adopted\na new strategy that uses a large VQA database on natural\nvideos [81] to learn better feature representations for VQA,\nand then transfer to diverse types of videos with only a\nfew labeled videos available. This strategy has been vali-\ndated as an effective way to improve the prediction accuracy\nand robustness on relatively small VQA datasets for en-\nhanced videos [36] and computer-generated contents [79].\nIn our study, we extend this strategy for evaluating text-\nconditioned generated videos, bringing a more reliable and\ngeneralizable quality metric for text-to-video generation.\nDespite leveraging from large video quality databases,\nseveral recent works [55, 68, 69, 72] have also explored\nto adopt multi-modality foundation models e.g. CLIP [45]\nfor VQA. With the text prompts as natural quality indica-\ntors (e.g. good/bad), these text-prompted methods prove\nsuperior abilities on zero-shot or few-shot VQA settings,\nand robust generalization among distributions. Inspired by\nexisting studies, the proposed quality metric in T2VScore\nalso ensembles a text-prompted structure, which is proved\nto better align with human judgments on videos generated\nby novel generators that are not seen during training.\n2.4. QA-based Evaluation\nRecent studies have emerged around the idea of using\nVisual Question Answering (VQA) to test the accuracy\nof advanced AI models.\nTIFA [22] utilizes GPT-3 [6]\nto create questions in various areas, such as color and\nshape, and checks the answers with VQA systems like\nmPLUG [26].\nVQ2A[9] makes VQA more reliable by\nsynthesizing new data and employing high-quality nega-\ntive sampling. VPEval[10] improves this process through\nthe use of object detection and Optical Character Recogni-\ntion (OCR), combining these with ChatGPT for more con-\ntrolled testing. However, these methods have not yet ex-\nplored videos, where both spatial and temporal elements\nshould be evaluated. We are adding specific designs to the\ntemporal domain to improve VQA for video understanding.\nThis provides a more comprehensive method for evaluating\ntext-video alignment from both space and time.\n3. Proposed Metrics\nWe introduce two metrics to evaluate text-guided generated\nvideos, focusing on two essential dimensions: Text Align-\nment (Sec. 3.1) and Video Quality (Sec. 3.2).\n3.1. Text Alignment\nState-of-the-art\nmultimodal\nlarge\nlanguage\nmodels\n(MLLMs) have demonstrated human-level capabilities in\nboth visual and textual comprehension and generation.\nHere, we introduce a framework for assessing the text-\nvideo alignment using these MLLMs. An overview of our\ntext alignment evaluation process is presented in Fig. 2.\nEntity Decomposition in Text Prompt.\nConsider a text\nprompt denoted as P. Our initial step involves parsing P\ninto distinct semantic elements, represented as ei. We then\nidentify the hierarchical semantic relationships among these\nelements, forming entity tuples {(ei, ej)}. Here, ej is se-\nmantically dependent on ei to form a coherent meaning. For\ninstance, the tuple (dog, a) implies that the article \u201ca\u201d is as-\nsociated with the noun \u201cdog\u201d, while (cat, playing soccer)\nsuggests that the action \u201cplaying soccer\u201d is attributed to the\n\u201ccat\u201d. Elements that exert a global influence over the entire\nprompt, like style or camera motion, are categorized under a\nglobal element. This structuring not only clarifies the in-\nterconnections within the elements of a text prompt but also\nimplicitly prioritizes them based on their hierarchical sig-\nnificance. For instance, mismatching an element that holds\na higher dependency rank would result in a more substantial\npenalty on the final text alignment score.\nQuestion/Answer Generation with LLMs.\nOur main\ngoal is to generate diverse questions that cover all elements\nof the text input evenly. Drawing inspiration from previous\nstudies [22], for a text prompt P, we utilize large language\nmodels (LLMs) to generate question-choice-answer tuples\n{Qi, Ci, Ai}N\ni=1, as depicted on the top of Fig. 2. Different\nfrom prior work focusing on text-image alignment, we em-\nphasize the temporal aspects, such as object trajectory and\n3\nFigure 2. Pipeline for Calculating T2VScore-A: We input the\ntext pormpt into large language models (LLMs) to generate ques-\ntions and answers. Utilizing CoTracker [23], we extract the auxil-\niary trajectory, which, along with the input video, is fed into multi-\nmodal LLM (MLLMs) for visual question answering (VQA). The\nfinal T2VScore-A is measured based on the accuracy of VQA.\nPlease click and play using Adobe Acrobat.\ncamera motion, which are unique and essential for evalu-\nating text alignment in dynamic video contexts. We em-\nploy a single-pass inference using in-context learning with\nGPT-3.5 [6, 64] to generate both questions and answers. We\nmanually curate 3 examples and use them as in-context ex-\namples for GPT-3.5 to follow. The complete prompt used\nfor generating question and answer pairs can be found in\nthe supplementary.\nVideo Question Answering with Auxiliary Trajectory.\nMost open-domain vision-language models are image-\ncentric [11, 30, 33, 34, 77, 89], with only a few focusing\non video [31, 39, 78]. These VideoLLMs often struggle\nwith fine-grained temporal comprehension, as evidenced by\ntheir performance on benchmarks like SEED-Bench [25].\nTo address this, we introduce the use of auxiliary trajecto-\nries, generated by off-the-shelf point tracking models (e.g.,\nCoTracker [23] and OmniMotion [58]), to enhance the un-\nderstanding of object and camera movements. We process\n(a) Technical Expert\n(b) Semantic Expert\nVideo Swin \nTransformer (T)\nFC(768,64)\nFC(64,1)\nCLIP-ViT-Large\nFC(768,256)\nFC(256,768)\nFAST-VQA \nProcessor\nCLIP Image \nProcessor\nText Transformer\ngood, high quality\npoor, low quality\nQsem\nQtech\nT2VScore-Q: 0.57\nCLIP-Adapter\nFigure 3. Pipeline for Calculating T2VScore-Q: a mixture of\na technical expert (a) to capture spatial and temporal technical\ndistortions, and a text-prompted semantic expert (b).\na video V, created by T2V models using text prompt T ,\nalongside its tracking trajectory Vtrack and question-choice\npairs {Qi, Ci}N\ni=1 generated by LLMs. These inputs are\nthen fed into multi-modality LLMs for question answering:\n\u02c6Ai = VQA(V, Vtrack, Qi, Ci).\nWe define the Text-to-Video (T2V) alignment score\nT2VScore-A as the accuracy of the video question an-\nswering process:\nT2VScore-A(T , V) = 1\nN\nN\nX\ni=1\n1[ \u02c6Ai = Ai].\n(1)\nThe T2VScore-A ranges from 0 to 1, with higher values\nindicating better alignment between text T and video V.\n3.2. Video Quality\nIn this section, we discuss the proposed video quality met-\nric in the T2V Score.\nOur core principle is simple: it\nshould be able to keep effective to evaluate videos from un-\nseen generation models that come up after we propose this\nscore. Under this principle, the proposed metric aims to\nachieve two important goals: (G1) It can more accurately\nassess the quality of generated videos without seeing any\nof them (zero-shot); (G2) while adapted to videos gener-\n4\nated on known models, it can significantly improve gener-\nalized performance on unknown models. Both aims in-\nspire us to drastically improve the generalization ability of\nthe metric, via a combo of Mix-of-Limited-Expert Structure\n(Sec. 3.2.1), Progressive Optimization Strategy (Sec. 3.2.2),\nand List-wise Learning Objectives (Sec. 3.2.3), elaborated\nas follows.\n3.2.1\nMix-of-Limited-Expert Structure\nGiven the hard-to-explain nature of quality assessment [84],\ncurrent VQA methods that only learn from human opin-\nions in video quality databases will more or less come\nwith their own biases, leading to poor generalization abil-\nity [29]. Considering our goals, inspired by existing prac-\ntices [24, 70, 71], we select two evaluators with different bi-\nases as limited experts, and fuse their judgments to improve\ngeneralization capacity of the final prediction. Primarily,\nwe include a technical expert (Fig. 3(a)), aiming at captur-\ning distortion-level quality. This branch adopts the structure\nof FAST-VQA [65], which is pre-trained from the largest\nVQA database, LSVQ [81], and further fine-tuned on the\nMaxWell [71] database that contains a wide range of spa-\ntial and temporal distortions. While the technical branch\ncan already cover scenarios related to naturally-captured\nvideos, generated videos are more likely to include seman-\ntic degradations, i.e. failing to generate correct structures\nor components of an object. Thus, we include an additional\ntext-prompted semantic expert (Fig. 3(b)). It is based on\nMetaCLIP [76], and calculated via a confidence score on\nthe binary classification between the positive prompt good,\nhigh quality and negative prompt poor, low quality. We also\nadd an additional adapter [12] to better suit the CLIP-based\nevaluator in the domain of video quality assessment.\nDenote the technical score for the video V as Qtech(V),\nthe text-prompted semantic score as Qsem(V), we fuse the\ntwo independently-optimized judgements via ITU-standard\nperceptual-oriented remapping [4], into the T2VScore-Q:\nR(s) =\n1\n1 + e\u2212 s\u2212\u00b5(s)\n\u03c3(s)\n(2)\nT2VScore-Q(V) = R(Qtech) + R(Qsem)\n2\n(3)\nThe T2VScore-Q ranges from 0 to 1, with higher values\nindicating better visual quality of video V.\n3.2.2\nProgressive Optimization Strategy\nAfter introducing the structure, we discuss the optimiza-\ntion strategy for the T2VScore-Q (Fig. 4). In general,\nthe training is conducted in three stages: pre-training, fine-\ntuning, and adaptation. The stages come with gradually\nDecreased Dataset Scale\nReduced Optimizable Parameters\nTechnical Expert\nSemantic Expert\nVideo Swin \nTransformer (T)\nFC(768,64)\nFC(64,1)\nVideo Swin \nTransformer (T)\nFC(768,64)\nFC(64,1)\nVideo Swin \nTransformer (T)\nFC(768,64)\nFC(64,1)\nLSVQ (28K)\nMaxWell (3.6K)\nTVGE (1.5K)\n(1) Pre-training\n(2) Fine-tuning\n(3) Adaptation\n28M\n50K\n0\nCLIP-ViT-Large\nMetaCLIP (400M)\nmatch with text\nCLIP-ViT-Large\nMaxWell (3.6K)\nFC(768,256)\nFC(256,768)\nmatch with text\nCLIP-ViT-Large\n \u2014frozen\u2014\nFC(768,256)\nFC(256,768)\nmatch with [ctx]-text\n768\n39K \n304M\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n28M Optimizable Params\n\ud83d\udd25  Updated Modules\n[ctx]- Learnable Text Prefix\n\ud83d\udd25\nFigure 4.\nOptimization Strategy of the Video Quality Met-\nric, via gradually decreased scales of training datasets, and cor-\nrespondingly progressively reduced optimizable parameters.\nsmaller datasets, coped with progressively reduced opti-\nmizable parameters. For Qtech, the optimization strate-\ngies for each stage are listed as follows: (1) end-to-end pre-\ntraining with large-scale LSVQtrain dataset (28K videos);\n(2) multi-layer fine-tuning with medium-scale MaxWelltrain\ndataset (3.6K videos); (3) Given that specific distortions on\ngenerated videos (See Fig. 5) are usually associated with\nsemantics, to avoid over-fitting, the technical expert is kept\nfrozen during the adaptation stage. For Qsem, (1) we di-\nrectly adopt official weights from MetaCLIP [76] as pre-\ntraining; (2) for the fine-tuning stage, we train a lightweight\nadapter [12] on MaxWelltrain; (3) for adaptation, we train an\nadditional prefix token [55, 69, 88] to robustly adapt it to\nthe domain of generated videos.\n3.2.3\nList-wise Learning Objectives\nPlenty of existing studies [24, 29, 65, 66] have pointed out\nthat compared with independent scores, the rank relations\namong different scores are more reliable and generalizable,\nespecially for small-scale VQA datasets [49]. Given these\ninsights, we decide to adopt the list-wise learning objec-\ntives [28] combined by rank loss (Lrank) and linear loss\n(Llinear) as our training objective for both limited experts:\nLrank =\nX\ni,j\nmax((si\npred \u2212 sj\npred) sgn (sj\ngt \u2212 si\ngt), 0) (4)\nLlin = (1 \u2212 < spred \u2212 spred, sgt \u2212 sgt >\n\u2225spred \u2212 spred\u22252\u2225sgt \u2212 sgt\u22252\n)/2\n(5)\nL = Llin + \u03bbLrank\n(6)\n5\nFigure 5. Domain Gap with Natural Videos. The common dis-\ntortions in generated videos (as in TVGE dataset) are different\nfrom those in natural videos [71], both spatially and temporally.\nWe encourage readers to click and play using Adobe Acrobat.\nwhere spred and sgt are lists of predicted scores and labels\nin a batch respectively, and sgn denotes the sign function.\n4. TVGE Dataset\nMotivation.\nAn inalienable part of our study is to evalu-\nate the reliability and robustness of the proposed metrics on\ntext-conditioned generated videos. To this end, we propose\nthe Text-to-Video Generation Evaluation (TVGE) dataset,\ncollecting rich human opinions on the two perspectives\n(alignment & quality) studied in the T2V Score. On both\nperspectives, the TVGE can be considered as first-of-its-\nkind: First, for the alignment perspective, the dataset will be\nthe first dataset providing text alignment scores rated by a\nlarge crowd of human subjects; Second, for the quality per-\nspective, while there are plenty of VQA databases on nat-\nural contents [21, 71, 81], they show notably different dis-\ntortion patterns (both spatially and temporally, see Fig. 5)\nfrom the generated videos, resulting in an non-negligible\ndomain gap. The proposed dataset will serve as a validation\non the alignment between the proposed T2V Score and hu-\nman judgments. Furthermore, it can help our quality metric\nto better adapt to the domain of text-conditioned generated\nvideos. Details of the dataset are as follows.\nCollection of Videos.\nIn total, 2543 text-guided generated\nvideos are collected for human rating in the TVGE dataset.\nThese videos are generated by 5 popular text-to-video gen-\neration models, under a diverse prompt set as defined by\nEvalCrafter [37] covering a wide range of scenarios.\nSubjective Studies.\nIn the TVGE dataset, each video is\nindependently annotated by 10 experienced human sub-\njects from both text alignment and video quality perspec-\ntives. Before the annotation, we trained the human subjects1\n1Training materials are provided in supplementary materials.\nVideo Quality\nText Alignment\nFloor33\nGen2\nModelScope\nPIKA\nZeroScope\n\u03bcquality = 2.77\n\u03bcalignment = 2.59\nFigure 6. Score Distributions in TVGE, suggesting that current\ntext-to-video generation methods generally face challenges in pro-\nducing video with either good quality or high alignment with text.\nand tested their annotation reliability on a subset of TVGE\nvideos. Each video is rated on a five-point-like scale on ei-\nther perspective, while examples for each scale are provided\nin the training materials for subjects.\nAnalysis and Conclusion.\nIn Fig. 6, we show the distri-\nbutions of human annotated quality and alignment scores\nin the TVGE dataset. In general, the generated videos re-\nceive lower-than-average human ratings (\u00b5alignment = 2.59,\n\u00b5quality = 2.77) on both perspectives, suggesting the need to\ncontinuously improve these methods to eventually produce\nplausible videos. Nevertheless, specific models also prove\ndecent proficiency on one single dimension, e.g. Pika gets\nan average score of 3.45 on video quality. Between the two\nperspectives, we notice a very low correlation (0.223 Spear-\nman\u2019s \u03c1, 0.152 Kendall\u2019s \u03d5), proving that the two dimen-\nsions are different and should be considered independently.\nWe show more qualitative examples in the supplementary.\n5. Experiments\n5.1. Text Alignment\nBaselines.\nWe compare our T2VScore-A with several\nstandard metrics on text-video alignment, listed as follows:\n\u2022 CLIP Score [18, 45]: Average text-image similarity in\nthe embedding space of image CLIP models [45] over all\nvideo frames.\n\u2022 X-CLIP Score [40]: Text-video similarity measured by a\nvideo-based CLIP model finetuned on text-video data.\n6\n\u2022 BLIP-BLEU [37]: Text-to-text similarity measured by\nBLEU [43] score using BLIP-2\u2019s image captioning.\n\u2022 mPLUG-BLEU: Same as BLIP-BLEU but using a\nmPLUG-OWL2 for video captioning.\nMethod\nModel\nSpearman\u2019s\nKendall\u2019s\n\u03c1\n\u03c4\nTraditional Metric\nCLIP Score\n0.343\n0.236\nX-CLIP Score\n0.257\n0.175\nBLIP-BLEU\n0.152\n0.104\nmPLUG-BLEU\n0.059\n0.040\nT2VScore-A\nOtter\u2020\n0.181\n0.134\nVideo-LLaMA\u2020\n0.288\n0.206\nmPLUG-OWL2-V\u2020\n0.394\n0.285\nInstructBLIP\u2217\n0.342\n0.246\nmPLUG-OWL2-I\u2217\n0.358\n0.257\nGPT-4V\u2217\n0.486\n0.360\n\u2020 via Video QA; \u2217 via Image QA\nTable 1. Correlation Analysis. Correlations between objective\nmetrics and human judgment on text-video alignment. Spearman\u2019s\n\u03c1 and Kendall\u2019s \u03c4 are used for correlation calculation. The best is\nbold-faced, and the second-best is underlined.\nComparison with traditional metrics.\nWe evaluate ex-\nisting objective metrics using our TVGE dataset and ob-\nserve a low correlation with human judgment in text-video\nalignment.\nThis observation aligns with findings in re-\ncent research [38, 42] that current objective metrics are in-\ncompatible with human perception. In particular, video-\nbased CLIP models exhibit even lower correlations than\ntheir image-based counterparts in comprehending videos.\nThis discrepancy may be attributed to the X-CLIP score\nmodel, which has been fine-tuned exclusively on the Ki-\nnetics datasets, a scope insufficient for broad-domain video\nunderstanding. Additionally, while BLEU is a widely em-\nployed evaluation metric in NLP research, its effectiveness\ndiminishes in text-video alignment tasks. This is due to\nthe inherent challenge of accurate video captioning. Conse-\nquently, video-based models such as mPLUG-Owl-2 prove\nto be less helpful in this context.\nComparison on MLLMs.\nOur T2VScore-A model is\ndesigned to be model-agnostic, which ensures it is com-\npatible with a wide variety of multimodal language learn-\ning models (MLLMs). This includes open-source models\nlike Otter [11], Video-LLaMA [83], and mPLUG-OWL2-\nV [78], as well as proprietary models such as GPT-4V [41].\nIn our experiments, we tested T2VScore-A with both im-\nage and video-based LLMs. We found that its performance\nsignificantly depends on the capabilities of the underlying\nMLLMs. Open-source image LLMs like InstructBLIP and\nmPLUG-OWL2-I show decent results in visual question an-\nswering.\nHowever, their limited temporal understanding\nmakes them less effective compared to the more advanced\nopen-source video LLMs like mPLUG-OWL2-V in video-\nbased question-answering tasks. Despite this, there is still\na notable performance disparity between these open-source\nMLLMs and GPT-4V, with GPT-4V demonstrating superior\nperformance in video question answering. This is evidenced\nby its higher correlation with human judgment, outperform-\ning other models by a significant margin.\nEffect of auxiliary trajectory.\nWe leverage the point\ntrajectory data generated by CoTracker to enhance fine-\ngrained temporal understanding. This approach effectively\ncaptures the subtle motion changes of both the object and\nthe camera, which is instrumental in answering questions\nrelated to temporal dynamics. As shown in Fig. 7, mod-\nels that incorporate trajectory data can accurately identify\nspecific camera movements, such as \u201cpanning from right to\nleft\u201d and \u201crotating counter-clockwise\u201d. In contrast, mod-\nels without trajectory input struggle to perceive these subtle\nmotion changes. The numerical results in Tab. 5 and Tab. 6\nfurther supports our observation.\n5.2. Video Quality\nBaselines.\nWe compare the T2VScore-Q with several\nstate-of-the-art methods on video quality assessment:\n\u2022 FAST-VQA [65]: State-of-the-art technical quality evalu-\nator, with multiple mini-patches (\u201cfragments\u201d) as inputs.\n\u2022 DOVER [70]: State-of-the-art VQA method, consisting\nof FAST-VQA and an additional aesthetic branch.\n\u2022 MaxVQA [71]: CLIP-based text-prompted VQA method.\nWe also validate the performance of multi-modality\nfoundation models in evaluating generated video quality:\n\u2022 CLIP [45, 76]: As CLIP is one of the important bases of\nthe T2VScore-Q, it is important to how original zero-\nshot CLIP variants work on this task. The original CLIPs\nare evaluated under the same prompts as the proposed se-\nmantic expert: good, high quality\u2194poor, low quality.\nSettings.\nAs discussed in Sec. 3.2, we validate the effec-\ntiveness of the T2VScore-Q under two settings:\n\u2022 (G1): zero-shot: In this setting, no generated videos are\nseen during model training. Aligning with the settings\nof off-the-shelf evaluators, it fairly compares between the\nbaseline methods and the proposed T2VScore-Q.\n\u2022 (G2): adapted, cross-model: In this setting, we further\nadapt the T2VScore-Q to a part of the TVGE dataset\nwith videos generated by one known model, and evaluate\nthe accuracy on other 4 unknown generation models. It is\na rigorous setting to check the reliability of the proposed\nmetric with future generation models coming.\nComparison on the zero-shot setting.\nWe show the com-\nparison between the T2VScore-Q and baseline methods\nin Tab. 2, under the zero-shot setting without training on\nany generated videos. Firstly, after our fine-tuning (stage 2,\non natural VQA dataset), the two experts that make up the\nT2VScore-Q have notably improved compared with their\ncorresponding baselines; Second, the mixture of the limited\n7\nMetric\nSpearman\u2019s\nKendall\u2019s\nPearson\u2019s\n\u03c1\n\u03d5\n\u03c1\nFAST-VQA [65]\n0.3518\n0.2405\n0.3460\nDOVER [70]\n0.3591\n0.2447\n0.3587\nMaxVQA [71]\n0.4110\n0.2816\n0.4002\nCLIP-ResNet-50 [45]\n0.3164\n0.2162\n0.3018\nCLIP-ViT-Large-14 [76]\n0.3259\n0.2213\n0.3140\nthe Technical Expert\n0.4557\n0.3136\n0.4426\nthe Semantic Expert\n0.4623\n0.3210\n0.4353\nT2VScore-Q (Ours)\n0.5029\n0.3498\n0.4945\nimprovements\n+22.3%\n+24.2%\n+23.6%\nTable 2. Zero-shot comparison on Video Quality. Correlations\ncomparison Spearman\u2019s \u03c1, Kendall\u2019s \u03d5, and Pearson\u2019s \u03c1 are in-\ncluded for correlation calculation.\nexperts also resulted in significant performance gain. Both\nimprovements lead to the final more than 20% improve-\nments on all correlation coefficients than existing VQA ap-\nproaches, demonstrating the superiority of the proposed\nmetric. Nevertheless, without training on any T2V-VQA\ndatasets, all zero-shot metrics are still not enough accurate\nto evaluate the quality of generated videos, bringing the\nnecessity to discuss a robust and effective adaptation ap-\nproach.\nCross-model improvements of adaptation.\nA common\nconcern on data-driven-generated content quality assess-\nment is that evaluators trained on a specific set of models\ncannot generalize well on evaluating a novel set of models.\nThus, to simulate the real-world application scenario, we\nabandon the random five-fold splits and use rigorous cross-\nmodel settings during the adaptation stage. As shown in\nTab. 3, in each setting, we only adopt the T2VScore-Q on\nvideos generated in one among five models in the TVGE\ndataset and evaluate the changes of accuracy on the rest\nof videos generated by other 4 models.\nThe table has\nproven that the proposed prefix-tuning-based adaptation\nstrategy can effectively generalize to unseen model sets\nwith an average of 11% improvements, proving that the\nT2VScore-Q can be a reliable open-set quality metric for\ngenerated videos.\nAblation Studies.\nWe show the ablation experiments in\nTab. 4. As is shown in the table, the proposed fine-tuning\n(stage 2) on both experts improved their single branch accu-\nracy and the overall accuracy of T2VScore-Q, suggesting\nthe effectiveness of the proposed components.\n6. Conclusion\nIn this paper, to address the shortcomings of existing text-\nto-video generation metrics, we introduced the Text-to-\nVideo Score (T2VScore), a novel evaluation metric that\nholistically assesses video generation by considering both\nthe alignment of the video with the input text, and the video\nStrategy\nSpearman\u2019s\nKendall\u2019s\nPearson\u2019s\n\u03c1\n\u03d5\n\u03c1\n- Evaluated on other 4 models except PIKA\nzero-shot\n0.4758\n0.3311\n0.4643\nTrained on PIKA, cross\n0.5467\n0.3834\n0.5377\n- Evaluated on other 4 models except Floor33\nzero-shot\n0.5467\n0.3801\n0.5363\nTrained on Floor33, cross\n0.5923\n0.4192\n0.5805\n- Evaluated on other 4 models except ZeroScope\nzero-shot\n0.4148\n0.2884\n0.4330\nTrained on ZeroScope, cross\n0.4561\n0.3194\n0.4623\n- Evaluated on other 4 models except ModelScope\nzero-shot\n0.4826\n0.3340\n0.4835\nTrained on ModelScope, cross\n0.5406\n0.3785\n0.5368\n- Evaluated on other 4 models except Gen2\nzero-shot\n0.4964\n0.3472\n0.4920\nTrained on Gen2, cross\n0.5514\n0.3895\n0.5481\naverage cross-model gain\n+11.2%\n+11.2%\n+10.6%\nTable 3. Cross-model Improvements on Video Quality. In each\nsetting, we adapt the T2VScore-Q with about 500 videos gener-\nated with one of the models, and test its improvements of accuracy\non the rest of the videos generated by the other 4 models.\nComponents in T2VScore-Q\nSpearman\u2019s\nKendall\u2019s\nPearson\u2019s\nQsem\nfine-tune\nQtech\nfine-tune\n\u03c1\n\u03d5\n\u03c1\n\u2713\n0.3259\n0.2213\n0.3140\n\u2713\n\u2713\n0.4623\n0.3210\n0.4353\n\u2713\n0.3518\n0.2405\n0.3460\n\u2713\n\u2713\n0.4557\n0.3136\n0.4426\n\u2713\n\u2713\n\u2713\n0.4458\n0.3074\n0.4409\n\u2713\n\u2713\n\u2713\n0.4629\n0.3197\n0.4514\n\u2713\n\u2713\n\u2713\n\u2713\n0.5029\n0.3498\n0.4945\nTable 4. Ablation Study. Spearman\u2019s \u03c1, Kendall\u2019s \u03d5, and Pear-\nson\u2019s \u03c1 are included for correlation calculation.\nquality. Moreover, we present the TVGE dataset to better\nevaluate the proposed metrics. The experimental results on\nthe TVGE dataset underscore the effectiveness of T2VScore\nover existing metrics, providing a more comprehensive and\nreliable means of assessing text-to-video generation. This\nproposed metric, along with the dataset, paves the way for\nfurther research and development in the field aiming at more\naccurate evaluation methods for video generation methods.\nLimitations and future work.\nThe T2VScore-A heav-\nily relies on multimodal large language models (MLLMs)\nto perform accurate Visual Question Answering. However,\nthe current capabilities of MLLMs are not yet sufficient to\nachieve high accuracy, particularly in evaluating temporal\ndimensions. We anticipate that as MLLMs become more\nadvanced, our T2VScore-A will also become increasingly\nstable and reliable.\nAs new open-source text-to-video models continue to\nemerge, we will keep track of the latest developments and\nincorporate their results into our TVGE dataset as part of\nour future efforts.\n8\nReferences\n[1] Floor33 pictures. http://floor33.tech/. 2\n[2] Gen-2. https://research.runwayml.com/gen2.\n1, 2\n[3] Pika labs. https://www.pika.art/. 1, 2\n[4] Recommendation 500-10: Methodology for the subjective\nassessment of the quality of television pictures. ITU-R Rec.\nBT.500, 2000. 5\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 1, 2, 3\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 3, 4\n[7] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In CVPR,\n2017. 3\n[8] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In CVPR,\n2017. 3\n[9] Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi\nChen, Nan Ding, and Radu Soricut. All you may need for\nvqa are image captions. In NAACL, 2022. 3\n[10] Jaemin Cho, Abhay Zala, and Mohit Bansal.\nVisual pro-\ngramming for text-to-image generation and evaluation. In\nNeurIPS, 2023. 3\n[11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 4, 7\n[12] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao\nFang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.\nClip-adapter: Better vision-language models with feature\nadapters. arXiv preprint arXiv:2110.04544, 2021. 5\n[13] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. arXiv:2305.10474,\n2023. 2\n[14] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bern-\nhard Sch\u00a8olkopf, and Alexander Smola. A kernel two-sample\ntest. J Mach Learn Res, 2012. 3\n[15] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei\nLiu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang,\nMike Zheng Shou, and Kevin Tang. Videoswap: Customized\nvideo subject swapping with interactive semantic point cor-\nrespondence. arXiv preprint arXiv:2312.02087, 2023. 3\n[16] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths. arXiv:2211.13221,\n2022. 2\n[17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Videocrafter: A toolkit for text-to-video gen-\neration and editing. https://github.com/AILab-\nCVC/VideoCrafter, 2023. 1, 2\n[18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning, 2022. 6\n[19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv:2210.02303, 2022. 1, 2\n[20] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. In NeurIPS, 2022. 2\n[21] Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui\nMen, Tam\u00b4as Szir\u00b4anyi, Shujun Li, and Dietmar Saupe. The\nkonstanz natural video database (konvid-1k).\nIn QoMEX,\npages 1\u20136, 2017. 6\n[22] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Os-\ntendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate\nand interpretable text-to-image faithfulness evaluation with\nquestion answering. arXiv preprint arXiv:2303.11897, 2023.\n3\n[23] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Co-\ntracker:\nIt is better to track together.\narXiv preprint\narXiv:2307.07635, 2023. 4, 1\n[24] Bowen Li, Weixia Zhang, Meng Tian, Guangtao Zhai, and\nXianpei Wang. Blindly assess quality of in-the-wild videos\nvia quality-aware pre-training and motion perception. IEEE\nTCSVT, 2022. 3, 5\n[25] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 4, 1\n[26] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming\nYan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng\nCao, et al. mplug: Effective and efficient vision-language\nlearning by cross-modal skip-connections.\narXiv preprint\narXiv:2205.12005, 2022. 3\n[27] Dingquan Li, Tingting Jiang, and Ming Jiang.\nQual-\nity assessment of in-the-wild videos.\nIn ACM MM, page\n2351\u20132359, 2019. 3\n[28] Dingquan Li, Tingting Jiang, and Ming Jiang. Norm-in-norm\nloss with faster convergence and better performance for im-\nage quality assessment. In ACM MM, page 789\u2013797, 2020.\n5\n[29] Dingquan Li, Tingting Jiang, and Ming Jiang. Unified qual-\nity assessment of in-the-wild videos with mixed datasets\ntraining. International Journal of Computer Vision, 129(4):\n1238\u20131257, 2021. 5\n[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In ICML,\n2023. 4\n[31] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\n9\nVideochat: Chat-centric video understanding. arXiv preprint\narXiv:2305.06355, 2023. 4\n[32] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu,\nQi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong\nWang. Videogen: A reference-guided latent diffusion ap-\nproach for high definition text-to-video generation.\narXiv\npreprint arXiv:2309.00398, 2023. 2\n[33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 4\n[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 4\n[35] Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao,\nYuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, and\nMike Zheng Shou. Dynvideo-e: Harnessing dynamic nerf\nfor large-scale motion-and view-change human-centric video\nediting. arXiv preprint arXiv:2310.10624, 2023. 3\n[36] Xiaohong Liu, Xiongkuo Min, Wei Sun, et al. Ntire 2023\nquality assessment of video enhancement challenge, 2023. 3\n[37] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong\nZhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond\nChan, and Ying Shan. Evalcrafter: Benchmarking and eval-\nuating large video generation models, 2023. 6, 7\n[38] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng\nLi, Sishuo Chen, Xu Sun, and Lu Hou.\nFetv: A bench-\nmark for fine-grained evaluation of open-domain text-to-\nvideo generation. arXiv preprint arXiv:2311.01813, 2023.\n2, 7\n[39] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video\nunderstanding via large vision and language models. arXiv\npreprint arXiv:2306.05424, 2023. 4\n[40] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang,\nGaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin\nLing. Expanding language-image pretrained models for gen-\neral video recognition, 2022. 6\n[41] OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023. 7\n[42] Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami,\nYuta Nakashima, Esa Rahtu, Janne Heikkil\u00a8a, and Shin\u2019ichi\nSatoh. Toward verifiable and reproducible human evaluation\nfor text-to-image generation. In The IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2023. 2, 7\n[43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th Annual Meeting of\nthe Association for Computational Linguistics, pages 311\u2013\n318, Philadelphia, Pennsylvania, USA, 2002. Association for\nComputational Linguistics. 7\n[44] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\naliased resizing and surprising subtleties in gan evaluation.\nIn CVPR, pages 11410\u201311420, 2022. 2, 3\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 1, 2, 3, 6, 7, 8\n[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015. 2\n[47] Masaki Saito, Shunta Saito, Masanori Koyama, and So-\nsuke Kobayashi. Train sparsely, generate densely: Memory-\nefficient unsupervised training of high-resolution temporal\ngan. IJCV, 2020. 1, 3\n[48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. In ICLR, 2023. 1, 2, 3\n[49] Zeina Sinno and Alan Conrad Bovik. Large-scale study of\nperceptual video quality. IEEE Trans. Image Process., 28(2):\n612\u2013627, 2019. 5\n[50] Spencer Sterling. Zeroscope. https://huggingface.\nco/cerspense/zeroscope_v2_576w, 2023. 1, 2\n[51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR, 2016. 2\n[52] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In ICCV, 2015. 3\n[53] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv:1812.01717, 2018. 3\n[54] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRapha\u00a8el Marinier, Marcin Michalski, and Sylvain Gelly.\nFvd: A new metric for video generation. In ICLR, 2019.\n1, 3\n[55] Jianyi Wang, Kelvin C. K. Chan, and Chen Change Loy. Ex-\nploring clip for assessing the look and feel of images, 2022.\n3, 5\n[56] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv:2308.06571, 2023. 1, 2\n[57] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2\n[58] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li,\nBharath Hariharan, Aleksander Holynski, and Noah Snavely.\nTracking everything everywhere all at once. arXiv preprint\narXiv:2306.05422, 2023. 4\n[59] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap at-\ntention in spatiotemporal diffusions for text-to-video gener-\nation. arXiv:2305.10874, 2023. 2\n[60] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu,\nPeiqing Yang, et al. Lavie: High-quality video generation\nwith cascaded latent diffusion models. arXiv:2309.15103,\n2023. 2\n[61] Zhou Wang and Alan C Bovik. Mean squared error: Love\nit or leave it? a new look at signal fidelity measures. IEEE\nSignal Process Mag, 2009. 2\n[62] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 2\n10\n[63] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 2\n[64] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824\u201324837, 2022. 4\n[65] Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao,\nAnnan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Fast-\nvqa: Efficient end-to-end video quality assessment with frag-\nment sampling. In ECCV, 2022. 3, 5, 7, 8\n[66] Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou,\nWenxiu Sun, Qiong Yan, Jinwei Gu, and Weisi Lin. Neigh-\nbourhood representative sampling for efficient end-to-end\nvideo quality assessment, 2023. 3, 5\n[67] Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou,\nWenxiu Sun, Qiong Yan, and Weisi Lin. Discovqa: Temporal\ndistortion-content transformers for video quality assessment.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 33(9):4840\u20134854, 2023. 3\n[68] Haoning Wu, Liang Liao, Chaofeng Chen, Jingwen Hou\nHou, Erli Zhang, Annan Wang, Wenxiu Sun, Qiong Yan, and\nWeisi Lin. Exploring opinion-unaware video quality assess-\nment with semantic affinity criterion. In International Con-\nference on Multimedia and Expo (ICME), 2023. 3\n[69] Haoning Wu, Liang Liao, Annan Wang, Chaofeng Chen,\nJingwen Hou Hou, Erli Zhang, Wenxiu Sun Sun, Qiong Yan,\nand Weisi Lin. Towards robust text-prompted semantic cri-\nterion for in-the-wild video quality assessment, 2023. 3, 5\n[70] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-\nwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi\nLin. Exploring video quality assessment on user generated\ncontents from aesthetic and technical perspectives. In ICCV,\n2023. 3, 5, 7, 8\n[71] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-\nwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi\nLin.\nTowards explainable video quality assessment:\nA\ndatabase and a language-prompted approach. In ACM MM,\n2023. 5, 6, 7, 8\n[72] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\nYan, Guangtao Zhai, and Weisi Lin. Q-bench: A benchmark\nfor general-purpose foundation models on low-level vision.\n2023. 3\n[73] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of im-\nage diffusion models for text-to-video generation. In ICCV,\n2023. 2, 3\n[74] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jin-\nbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei\nHuang, Yuanxi Sun, et al. Cvpr 2023 text guided video edit-\ning competition. arXiv preprint arXiv:2310.16003, 2023.\n[75] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang\nJiang. Simda: Simple diffusion adapter for efficient video\ngeneration. arXiv:2308.09710, 2023. 3\n[76] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang,\nRussell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh,\nLuke Zettlemoyer, and Christoph Feichtenhofer. Demystify-\ning clip data. arXiv preprint arXiv:2309.16671, 2023. 5, 7,\n8\n[77] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn\nof lmms: Preliminary explorations with gpt-4v (ision). arXiv\npreprint arXiv:2309.17421, 9, 2023. 4\n[78] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei\nLiu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\nmplug-owl2: Revolutionizing multi-modal large language\nmodel with modality collaboration.\narXiv preprint\narXiv:2311.04257, 2023. 4, 7\n[79] Joong Gon Yim, Yilin Wang, Neil Birkbeck, and Balu\nAdsumilli. Subjective quality assessment for youtube ugc\ndataset. In ICIP, 2020. 3\n[80] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang,\nXiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie\nLi, Shuguang Liu, Fan Yang, et al.\nNuwa-xl:\nDiffu-\nsion over diffusion for extremely long video generation.\narXiv:2303.12346, 2023. 2\n[81] Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram,\nand Alan Bovik. Patch-vq: \u2019patching up\u2019 the video quality\nproblem. In CVPR, 2021. 3, 5, 6\n[82] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation. arXiv:2309.15818, 2023. 1, 2,\n3\n[83] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. arXiv preprint arXiv:2306.02858, 2023. 7\n[84] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 2, 5\n[85] Yabo\nZhang,\nYuxiang\nWei,\nDongsheng\nJiang,\nXi-\naopeng Zhang, Wangmeng Zuo, and Qi Tian.\nCon-\ntrolvideo: Training-free controllable text-to-video genera-\ntion. arXiv:2305.13077, 2023. 3\n[86] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao\nZhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng\nShou.\nMotiondirector: Motion customization of text-to-\nvideo diffusion models. arXiv preprint arXiv:2310.08465,\n2023. 2, 3\n[87] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models. arXiv:2211.11018,\n2022. 2\n[88] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to prompt for vision-language models. Inter-\nnational Journal of Computer Vision (IJCV), 2022. 5\n[89] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 4\n11\nTowards A Better Metric for Text-to-Video Generation\nSupplementary Material\n7. More Details for Subjective Study\nAnnotation Interface.\nIn Fig. 11, we show the annotation\ninterface for the subjective study in the TVGE dataset. The\ntext alignment scores and quality scores are annotated sep-\narately to avoid distraction from each other. The input text\nprompt is shown only for the alignment annotation (which\nis necessary), but not displayed for quality annotation, so\nthat the quality score can ideally only care about the visual\nquality of the generated video.\nTraining Materials.\nBefore annotation, we provide clear\ncriteria with abundant examples of 5-point Likert scale to\ntrain the annotators. For text alignment, we specifically\ninstruct annotators to evaluate the videos based solely on\nthe presence of each element mentioned in the text descrip-\ntion, intentionally ignoring video quality. For video qual-\nity, we ask the subjects to focus exclusively on technical\ndistortions. We provide five examples for each of the six\ncommon distortions: 1) noises; 2) artifacts; 3) blur; 4) un-\nnatural motion; 5) inconsistent structure; and 6) flickering.\nSamples of the annotated videos can be viewed in Fig. 8.\n8. Additional Results\nEffect of Auxiliary Trajectory for T2VScore-A.\nAs\nmentioned in Sec. 3.1, we utilize the auxiliary point-level\ntrajectory generated by CoTracker [23] to enhance fine-\ngrained temporal understanding. Fig. 7 presents video sam-\nples that exhibit temporal nuances, which state-of-the-art\nmultimodal language models (MLLMs) often fail to de-\ntect. Using the trajectory as auxiliary information effec-\ntively improves the MLLMs\u2019 understanding of subtle tem-\nporal changes in camera and object motion. For instance,\nthe snake in row 5 appears motionless, though the camera is\nmoving. Upon ablating the auxiliary trajectory, we observe\na decrease in visual question answering (VQA) accuracy\nfrom 0.58 to 0.48, as shown in Tab. 6. This reduction in\nVQA accuracy further leads to a diminished alignment with\nhuman judgment (see Tab. 5).\nT2VScore-A\nSpearman\u2019s\nKendall\u2019s\nPearson\u2019s\n\u03c1\n\u03d5\n\u03c1\nGPT-4V w/o tarjectory\n0.4454\n0.3289\n0.4416\nGPT-4V\n0.4859\n0.3600\n0.4882\nTable 5. Effect of Auxiliary Trajectory. Spearman\u2019s \u03c1, Kendall\u2019s\n\u03d5, and Pearson\u2019s \u03c1 are included for correlation calculation.\nPerformance of state-of-the-art MLLMs in VQA.\nWe\nsetup an evaluation set of 500 videos (100 prompts with 5\nFigure 7. Quantitative Examples for Auxiliary Trajectory. Us-\ning an auxiliary trajectory effectively enhances multimodal large\nlanguage models (MLLMs) for fine-grained temporal understand-\ning. Please click and play using Adobe Acrobat.\nModel\nOverall\nTemporal QA\nSpatial QA\nrandom guess\n0.2369\n0.2452\n0.2327\nOtter\u2020\n0.1460\n0.1059\n0.1636\nVideo-LLaMA\u2020\n0.4074\n0.3459\n0.4435\nmPLUG-OWL2-V\u2020\n0.5305\n0.4280\n0.5880\nInstructBLIP\u2217\n0.5013\n0.4762\n0.5127\nmPLUG-OWL2-I\u2217\n0.5107\n0.4333\n0.5600\nGPT-4V\u2217 (w/o trajectory)\n0.4791\n0.4411\n0.5589\nGPT-4V\u2217\n0.5765\n0.5077\n0.6308\n\u2020 via Video QA; \u2217 via Image QA\nTable 6. Accuracy of Visual Question Answering.\nunique videos per prompt) sampled from our TVGE dataset.\nTwo annotators are tasked with responding to the generated\nquestions, and a third, more experienced annotator is as-\nsigned to verify these responses. We compare the accuracy\nof visual question answering (VQA) across a range of mul-\ntimodal large language models (MLLMs), focusing on spa-\ntial and temporal QA. As shown in Tab. 6, current MLLMs\ngenerally demonstrate weak performance in open-domain\nVQA tasks, with temporal QA faring even worse. Notably,\nvideo-based MLLMs are inferior in temporal QA compared\nto their image-based counterparts. A similar observation is\nmade in SEED-Bench [25], indicating significant room for\nfurther improvement in video-based MLLMs.\n1\nFigure 8. Human Annotation. Generated videos and their human ratings of text alignment and video quality. The scores are the mean of\n10 annotators\u2019 ratings. Please click and play using Adobe Acrobat.\n2\nFigure 9. More Examples for T2VScore-A. We showcase more examples illustrating how T2VScore-A is computed. Please click and\nplay using Adobe Acrobat.\n3\n# Task Description:\nThe T2VScore-A is an evaluator for assessing the text alignment of video content generated from textual descriptions. It \nscrutinizes the video descriptions and formulates structured questions and answers to ensure the video content aligns precisely \nwith the provided description.\n# Task Steps and Format Specification:\n## Input Processing:\nOn receiving a video description, the T2VScore-A decomposes it into atomic tuples, ensuring each tuple is the smallest unit of \nmeaning that accurately represents an aspect of the video. Each atomic tuple consists of a correlation: the first element \nindicates a global or local Object of the video, and the second element specifies the attribute or detail of that Object,\nincluding but not limited to' activity', 'attribute', 'counting', 'color', 'material', 'spatial', 'location', 'shape', 'OCR', \netc. Questions are formulated based on the atomic tuples' count and order.\n## Question Generation:\nGenerate questions for each atomic tuple, targeting a specific video aspect indicated by the tuple. Questions should reflect the \natomicity principle, avoiding over-fragmentation or excessive aggregation of concepts.\n## Answer Formulation:\nProvide direct, relevant choices for each question based on the atomic tuple. Include \u201cNONE\u201d as an option where the existence of \nan entity or attribute is uncertain. Position the answer immediately following the question for clarity.\n## Response Format:\nStart with a list of the derived atomic tuples from the video description. Follow with each question and its corresponding \nchoices. Present the correct answer immediately after each question.\n## Consideration for Entity Non-Existence:\nQuestions related to entities must account for the possibility that the entity may not be present in the video. Hence, the \nformat \"If there is a [entity], ...\" should be used where applicable.\nInput: Iron Man is walking towards the camera in the rain at \nnight, with a lot of fog behind him. Science fiction movie, \nclose-up\nAtomic Tuples:\n(entity, Iron Man)\n(Iron Man, walking)\n(global, towards the camera)\n(global, in the rain)\n(global, at night)\n(global, a lot of fog behind him)\n(global, Science fiction movie)\n(global, close-up)\nQuestions and Answers:\nQ: What is the name of the character in the video?\nChoices: Iron Man, Captain America, Thor, Hulk\nA: Iron Man\nQ: What is the character doing in the video?\nChoices: walking, jumping, flying, NONE\nA: walking\nQ: What direction is the character moving in the video?\nChoices: towards the camera, away from the camera, left to \nright, right to left\nA: towards the camera\nQ: What is the weather condition in the video?\nChoices: sunny, rainy, snowy\nA: rainy\nQ: What time of day is depicted in the video?\nChoices: morning, afternoon, evening, night\nA: night\nQ: What is behind Iron Man in the video?\nChoices: a lot of fog, a cityscape, a forest, a desert\nA: a lot of fog\nQ: What genre does the video belong to?\nChoices: comedy, drama, science fiction, horror\nA: science fiction\nQ: What type of shot is used in the video?\nChoices: wide shot, medium shot, close-up, extreme close-up\nA: close-up\nInput: 2 Dog and a whale, ocean adventure\nAtomic Tuples:\n(entity, dog)\n(dog, 2)\n(entity, whale)\n(whale, a)\n(global, ocean adventure)\nQuestions and Answers:\nQ: Which type of animal appear in the video except for the \nwhale?\nChoices: dog, cat, bird, fish, NONE\nA: dog\nQ: How many dogs are present in the video?\nChoices: 1, 2, 3, more than 3, NONE\nA: 2\nQ: Which type of animal appears in the video except for the \ndog?\nChoices: whale, cat, bird, fish, NONE\nA: whale\nQ: How many whales are present in the video?\nChoices: 1, 2, 3, more than 3, NONE\nA: 1\nQ: What is the theme of the video?\nChoices: city adventure, forest adventure, ocean adventure, \nmountain adventure\nA: ocean adventure\nInput: There's a person, likely in their mid-twenties, with \nshort brown hair and a tattoo on their left forearm. They're \nwearing a blue hoodie and seem to be checking their phone.\nAtomic Tuples:\n(entity, person)\n(person, a)\n(person, mid-twenties)\n(person, short hair)\n(person, brown hair)\n(person, tattoo)\n(tattoo, left forearm)\n(person, wearing a hoodie)\n(hoodie, blue)\n(person, checking their phone)\nQuestions and Answers:\nQ: What is the main entity in the video?\nChoices: person, dog, cat, bird\nA: person\nQ: How many persons are present in the video?\nChoices: 1, 2, 3, more than 3, NONE\nA: 1\nQ: If there is a person, what is their approximate age?\nChoices: teenager, mid-twenties, thirties, forties, NONE\nA: mid-twenties\nQ: If there is a person, what is the length of their hair?\nChoices: short, medium, long, NONE\nA: short\nQ: If there is a person, what is the color of their hair?\nChoices: black, brown, blonde, red, NONE\nA: brown\nQ: If there is a person, do they have a tattoo?\nChoices: yes, no\nA: yes\nQ: If there is a person, where is their tattoo located?\nChoices: left forearm, right forearm, back, chest, NONE\nA: left forearm\nQ: If there is a person, what are they wearing?\nChoices: hoodie, t-shirt, sweater, NONE\nA: hoodie\nQ: If there is a person, what color is the hoodie they are \nwearing?\nChoices: blue, red, black, white, NONE\nA: blue\nQ: If there is a person, what are they doing in the video?\nChoices: checking their phone, reading a book, eating, \nsleeping, NONE\nA: checking their phone\nFigure 10. Prompt for Question/Answer Generation in T2VScore-A. Top: task instruction; Bottom: in-context learning examples.\n4\n(a) Annotation Interface for Video Quality. The video is presented to\nsubjects to be rated a quality score among [1,5].\n(b) Annotation Interface for Text Alignment. The video and its text prompt\nare presented to subjects to be rated an alignment score among [1,5].\nFigure 11. Annotation Interface for Video Quality (a) and Text Alignment (b).\n5\n"
  },
  {
    "title": "Quantum Denoising Diffusion Models",
    "link": "https://arxiv.org/pdf/2401.07049.pdf",
    "upvote": "12",
    "text": "Quantum Denoising Diffusion Models\nMichael K\u00a8olle, Gerhard Stenzel, Jonas Stein, Sebastian Zielinski,\nBj\u00a8orn Ommer, Claudia Linnhoff-Popien\nLMU Munich\nmichael.koelle@ifi.lmu.de\nAbstract\nIn recent years, machine learning models like DALL-\nE, Craiyon, and Stable Diffusion have gained significant\nattention for their ability to generate high-resolution im-\nages from concise descriptions.\nConcurrently, quantum\ncomputing is showing promising advances, especially with\nquantum machine learning which capitalizes on quantum\nmechanics to meet the increasing computational require-\nments of traditional machine learning algorithms. This pa-\nper explores the integration of quantum machine learning\nand variational quantum circuits to augment the efficacy of\ndiffusion-based image generation models. Specifically, we\naddress two challenges of classical diffusion models: their\nlow sampling speed and the extensive parameter require-\nments.\nWe introduce two quantum diffusion models and\nbenchmark their capabilities against their classical coun-\nterparts using MNIST digits, Fashion MNIST, and CIFAR-\n10. Our models surpass the classical models with simi-\nlar parameter counts in terms of performance metrics FID,\nSSIM, and PSNR. Moreover, we introduce a consistency\nmodel unitary single sampling architecture that combines\nthe diffusion procedure into a single step, enabling a fast\none-step image generation.\n1. Introduction\nImage generation remains a vital topic in computer vision\nand graphics [9, 18], encompassing tasks from synthetic\ndata creation to artistic endeavors. Generative models, like\nStable Diffusion, have found applications ranging from im-\nage editing to aiding multi-modal models like GPT-4 in pro-\nviding visual responses to human queries [36, 40]. While\nDenoising Diffusion Models (DDMs) have recently seen\nsignificant progress, they face notable challenges such as\nhigh computational demands and the necessity for exten-\nsive parameter tuning [7, 17, 40]. Recent advancements in\nquantum computing present opportunities to alleviate some\nof these challenges [3, 10]. Specifically, quantum machine\nlearning (QML) uses quantum principles to enhance effi-\nFigure 1. Diffusion process (\u03c4 = 10) of models Q-Dense, QD-\nDPM [20], U-Net and Dense on MNIST digits. Samples from\nevery second step are depicted.\nciency for classical machine learning tasks [3, 28, 52].\nIn this paper, we combine QML with DDMs to form\nquantum denoising diffusion models (QDDMs). This syn-\nthesis retains the image generation effectiveness of DDMs\nwhile benefiting from the efficiencies of quantum comput-\ning. By merging these two powerful domains, we push the\nfrontiers of what is currently achievable in image genera-\ntion, setting new benchmarks for quality and efficiency. We\nintroduce a novel quantum U-Net design, employing quan-\ntum convolutions to further refine image quality. Addition-\nally, we leverage the inherent unitary properties of quantum\ncircuits to optimize QDDMs\u2019 sampling time, introducing\nour unitary single-sample consistency model architecture.\nWe evaluate our models together with classical deep convo-\nlutional networks and U-Nets on the datasets MNIST dig-\nits, Fashion MNIST and CIFAR10 using FID, SSIM, and\nPSNR performance metrics. Furthermore, we showcase the\nsingle-shot image generation capabilities on simulator and\non real IBMQ hardware. Our results show that QDDMs\nhold a competitive edge over classical DDMs in produc-\ning high-quality images with fewer parameters. Finally, we\npresent a detailed empirical analysis, on the strengths and\narXiv:2401.07049v1  [quant-ph]  13 Jan 2024\nlimitations of QDDMs, setting the stage for potential fu-\nture explorations in this promising intersection of quantum\nand machine learning. In summary, our key contributions\ninclude:\n\u2022 The inception of two novel quantum diffusion architec-\ntures: Q-Dense and QU-Net.\n\u2022 The introduction the unitary single-sample consistency\nmodel architecture.\n2. Related Work\n2.1. Diffusion Models\nDiffusion models, as first introduced by [44], present a\nunique approach to training generative models. Rather than\nthe adversarial battle seen in GANs [11], diffusion models\nfocus on the steady transformation of noise into meaningful\ndata. While the inception of this technique showed promis-\ning results [7], subsequent improvements like Denoising\nDiffusion Implicit Models (DDIM) emerged [17, 33, 45].\nIn contrast to traditional diffusion models, which sample\neach intermediary step in a Markov chain fashion, DDIM\nidentifies and removes noise earlier, bypassing certain sam-\npling iterations [45]. In this work, we primarily follow the\nmethodology detailed in Ho et al. [17].\n2.2. Variational Quantum Circuits\nQuantum machine learning (QML) aims to harness the ca-\npabilities of quantum computing to meet the increasing\ncomputational requirements of traditional machine learning\nalgorithms [3, 10]. Variational quantum circuits (VQC) are\nfoundational to QML, serving as function approximators\nsimilar to classical neural networks. These circuits utilize\nparameterized unitary quantum gates on qubits [1], leverag-\ning the principles of quantum mechanics such as superpo-\nsition, entanglement, and interference. These gates derive\ntheir parameters from rotation angles, which are trainable\nvia conventional machine learning methods. A VQC\u2019s ar-\nchitecture consists of three components.\nThe first component embeds image and guidance data\ninto qubits. For image data, we employ amplitude embed-\nding, this method encodes 2n features (pixel values) into\nn qubits, representing each feature as a normalized ampli-\ntude of the quantum state [32, 47]. For label embedding,\nwe use angle embedding, which only encodes n features\ninto n qubits but uses less quantum gates [55]. The second\ncomponent consists of multiple variational layers, similar to\nhidden layers in classical networks. We design our circuits\nwith strongly entangling layers, following the approach of\nSchuld et al. [43]. We also apply data re-uploading, re-\nembedding parts of the input in-between variational layers,\nwhich aids in more complex feature learning [38]. Lastly,\nwe extract the output by measuring the quantum system,\ncausing the system\u2019s superposition to collapse.\nGiven n\nqubits, this allows us to derive 2n joint probabilities of the\noutput states. Quantum simulators further enable the extrac-\ntion of the circuit\u2019s state vector, which we use to build the\ncombined unitary matrix in Sec. 3.4.\nIt\u2019s important to highlight that while VQCs can effi-\nciently manage high-dimensional input with just log2(N)\nqubits [28], they still encounter issues such as high qubit\ncosts and error rates in the current Noisy Intermediate-Scale\nQuantum (NISQ) era [39]. However, anticipated advance-\nments in these domains hold promise for QML\u2019s pivotal role\nahead [10, 39].\n2.3. Quantum Diffusion Models\nTo the best of our knowledge, the model QDDPM by Do-\nhun Kim et al. currently stands as the sole quantum dif-\nfusion method for image generation [20]. They designed\na single-circuit model with timestep-wise layers that take\nunique parameters for each iteration, and shared layers con-\nsistent across all iterations. This model shines in its space-\nefficiency, needing only log2(pixels) qubits and thus ex-\nhibiting logarithmic space complexity for image genera-\ntion. To counteract the vanishing gradient issue, they con-\nstrained the circuit depth. For entanglement, they utilized\nspecial unitary (SU) gates, targeting two qubits simultane-\nously. While \u201dSU(4)\u201d groups offer benefits like known dif-\nferentiation, their parameter efficiency per gate is lacking,\nas they are using 15 parameters per group. Given the con-\nstrained circuit depth, their model produces images that are\nsomewhat recognizable but miss the intricacies of the orig-\ninals (refer to Fig. 1).\n3. Quantum Denoising Diffusion Models\n3.1. Dense Quantum Circuits\nIn our work, we employed a dense quantum circuit (or\nstrongly entangling circuit) as the foundational component\nof our quantum models. The term \u201cdense\u201d refers to the ex-\ntensive entanglement among qubits in the circuit. This de-\nsign choice is reminiscent of the nomenclature in classical\ndeep learning, where the term \u201cdense\u201d or \u201cfully connected\u201d\ndescribes layers where every neuron is connected to every\nother neuron in adjacent layers.\nThe architecture of our dense quantum model is a fol-\nlows. As detailed in Sec. 2.2, we chose amplitude embed-\nding for input embedding due to its space-efficiency. Given\nthat we are training on simulators, we bypassed the initial\npreprocessing steps outlined in [32]. Instead, we directly\ninitialized the normalized data to the state of the quan-\ntum circuit. We encode the normalized class indices for\nguidance by utilizing angle embedding. This is achieved\nby adding an additional qubit (ancilla) and performing a\nrotation around the x-axis by an angle of class index \u00d7\n2\u03c0\n#classes. Our circuit\u2019s variational component consist of sev-\neral strongly entangling layers [43], resulting in a a total of\n#layers\u00d73\u00d7#qubits trainable parameters. We then calculate\nthe joint probabilities of a qubit subset, measuring the likeli-\nhood of the output being in states |00 . . . 00\u27e9 to |11 . . . 11\u27e9.\nIf our output vector surpasses the input vector in size, we\ntruncate the excess, eliminating unused measured probabil-\nities. To align the output within the input data\u2019s range, we\nscale the obtained probabilities using the input data\u2019s eu-\nclidean norm.\n3.2. Quantum U-Net\nOur Quantum U-Net (QU-Net), draws inspiration from\nclassical U-Nets, particularly those without attention layers\nand upscaling features (shown in Fig. 2).\nFigure 2. QU-Net architecture and quantum convolution, embed-\nding a flattened slice into a dense quantum circuit.\nOpposed to the well-known blocks with two or more\nclassical convolutions each, we incorporate only one quan-\ntum convolution layer per block, as we observed prolonged\nexecution times when using more convolutions. Quantum\nconvolutions are our novel approach to use the flexibility of\nconvolutions in quantum machine learning, allowing us to\nembed any slice of shape cin \u00d7 k \u00d7 k into a dense quantum\ncircuit (Sec. 3.1) with max(log2(cin \u00d7 k \u00d7 k), log2(cout))\nwires (Fig. 2) and measure cout outputs (with c being input\nand output channels, and k being the kernel size), thus dif-\nfering from existing solutions like Quanvolution [15] and\nQuantum CNNs [4, 35].\n3.3. Guidance\nDiffusion models can be extended with guidance, introduc-\ning auxiliary data during both training and inference. This\nprocess, represented as p\u03b8(xt\u22121|xt, c) (or p\u03b8(\u03f5t\u22121|xt, c)),\nuses c as the guiding data [34].\nFor our dense quantum circuit, normalized class labels\nare embedded as rotation angles into an additional ancilla\nqubit, ensuring distinct quantum state representation. For\ninstance, labels 0 and 1 correspond to angles 0 and \u03c0. Clas-\nsical dense networks traditionally introduce inputs via an\nextra neuron per layer, enhancing performance and increas-\ning parameter count.\nContrarily, U-Nets, due to their architecture, imple-\nment a mask encoding for labels. This mask, defined as\nmask(c) = 0.1 \u00b7 sin(c + height/20), subtly alters the in-\nput image with strategically placed pixel value stripes, fa-\ncilitating label identification. For quantum U-Nets, despite\nthe necessity for normalized inputs in quantum convolu-\ntions, this masking technique remains effective. However,\ndatasets with extensive class variety might demand alterna-\ntive strategies.\n3.4. Unitary Single Sampling\nThe unitary nature of quantum gates and circuits allows us\nto combine the iterative application of U \u03c4 during a diffu-\nsion step into one unitary matrix U (Fig. 3). This enables\nus to create synthetic images, using a single-shot of the cir-\ncuit U, bridging the gap between quantum diffusion models\nand classic consistency models [46]. Additionally, this ap-\nproach can be faster than executing multiple iterations of\na classical diffusion model or even faster than executing\neach gate individually, depending on transpliation process\nand quantum hardware.\nFigure 3. Unitary Single Sampling architecture.\nTraining the single sampling model demands an alternate\nloss computation. Instead of typical measurement proba-\nbilities, we are interested in the post-circuit quantum state.\nTraining directly on quantum computers is currently infea-\nsible as quantum state tomography for state reconstruction\nscales exponentially with system size [5]. Therefore, we\nutilize a noise-free quantum simulator.\nLoss gets evalu-\nated by comparing the post-circuit state p\u03b8(\\\n|xt\u22121\u27e9| |xt\u27e9) and\nthe less noisy image |xt\u27e9, both represented as 2n-length\ncomplex vectors, using metrics like Mean Absolute Error\n(MAE).\nFor efficient sampling, we employ the trained parame-\nters in the concatenated circuit U n. Alternatively, we can\ndetermine a singular Udiffusion matrix from the matrix form\nof U n. Precomputing this matrix allows for more stream-\nlined sampling [46].\n4. Experimental Setup\n4.1. Datasets\nWe use three well-known datasets to evaluate and compare\nour models: the MNIST digits [6], Fashion MNIST [54],\nand a grayscale version of CIFAR-10 [22].\nThe MNIST dataset features 28 \u00d7 28 pixel grayscale im-\nages of digits (0-9). To probe scalability, we utilize the orig-\ninal as well as downscaled (8\u00d78) and upscaled (32\u00d732) ver-\nsions. For specific experiments tailored to quantum circuit\nembedding, we narrow our focus to digits 0 and 1. Fash-\nion MNIST, in contrast, presents a multitude of intra-class\nand inter-class variations. A pivotal challenge is that mod-\nels must interpret extensive regions of an image, beyond\njust the central figures, as peripheral areas do not consis-\ntently register as zero. The CIFAR10 dataset stands out\ndue to its varied backgrounds and the challenges brought\nforth by reduced edge contrast in grayscale. We\u2019ve adopted\na grayscale version created by averaging the RGB channels\nof its 32 \u00d7 32 pixel images.\n4.2. Metrics\nTo gauge the quality of our generated images, we employ\nthree metrics: the Fr\u00b4echet Inception Distance (FID) [16,\n27], the Structural Similarity Index Measure (SSIM) [53],\nand the Peak Signal-to-Noise Ratio (PSNR).\nThe FID serves as a tool to gauge the resemblance be-\ntween original and generated data.\nIt achieves this by\ncalculating the Wasserstein-2 distance between Gaussian\ndistributions of activations derived from the Inception-v3\nmodel [48]. A noteworthy aspect of FID is that lower scores\nsuggest a closer resemblance between datasets [16]. SSIM\nis defined as\nSSIM(x, y) = 2\u00b5x\u00b5y + c1\n\u00b52x + \u00b52y + c1\n\u00b7\n2\u03c3xy + c2\n\u03c32x + \u03c32y + c2\n,\n(1)\nand offers a measure of similarity between images x and\ny. Higher SSIM scores are indicative of more significant\nimage resemblance. Lastly, the PSNR stands as a metric to\nquantify the noise levels within an image. Superior image\nquality is represented by higher PSNR values.\n4.3. Baselines\nOur benchmarking process contrasts our models against\nthe architectures Deep Convolutional Networks (DCNs), U-\nNets, and Quantum Denoising Diffusion Probabilistic Mod-\nels (QDDPM).\nDCNs are hierarchical models that utilize convolutional\nlayers to extract progressively complex spatial features from\ninput data [12, 23, 25].\nBy leveraging spatial invari-\nance through weight sharing and pooling operations, DCNs\ncan discern intricate patterns in large-dimensional datasets.\nTheir depth and specialized architectures, such as residual\nand inception modules, enable the capture of both low-level\nimage details and high-level semantic information, making\nthem integral to advanced computer vision tasks.\nThe U-Net architecture [13, 19, 41] is acclaimed for its\ncapabilities in image segmentation. The U-Net\u2019s encoder\ncaptures local features, while its decoder combines these\ninsights with broader context through skip connections,\npromoting gradient flow and information transfer [8, 49].\nStrategies such as zero-padding and interpolation tech-\nniques ensure image sizes remain consistent across the iter-\native diffusion model process [7, 33]. Further refining im-\nage quality, attention layers become particularly beneficial\nwhen integrated with natural language embeddings [40, 50].\nLastly, we compare to the quantum state-of-the-art QD-\nDPM by Dohun Kim et al. [20] as described in Sec. 2.3.\nThis model is specially designed for the 8 \u00d7 8 MNIST\ndataset, incorporating six gates per layer, which results in\na total of 990 parameters over \u03c4 = 10 timesteps. When\nadapted for 16 \u00d7 16 images, the model requires a greater\nnumber of parameters, totaling 4920. It is important to note\nthat our comparisons are qualitative in nature, as we lack\ncommon metrics for evaluation.\n4.4. Model Training and Evaluation\nWe build our quantum models using the PennyLane frame-\nwork [2]. For training our quantum models, we use the\nPyTorch integration of PennyLane which facilitates clas-\nsical backpropagation for the gradients w.r.t. the rotation\nangles. On actual quantum hardware, parameter-shift dif-\nferentiation calculates gradients by re-evaluating circuits\nwith perturbed parameters [31, 42]. To enhance conver-\ngence and stabilize training, parameter remapping confines\nvalues within the range [\u2212\u03c0, \u03c0] or [0, 2\u03c0] [24]. We used\nthe classical optimization algorithm Adam [21] and mini-\nmize the Mean Squared Error (MSE) between the gener-\nated image p\u03b8(xt) = d\nxt\u22121 and xt\u22121, sourced from a noise-\naugmented training dataset. Notably, for the unitary single-\nsample model, we adopt the mean absolute error (MAE)\ndue to its native PyTorch implementation for complex ten-\nsors. All runs were conducted on identical hardware with\nIntel Core \u00ae i9-9900 CPUs and 64 GB of RAM. In our two\npreliminary studies (Sec. 8), we explore the relationship be-\ntween model hyperparameters and metrics. Additionally,\nwe performed a hyperparameter search focusing on learn-\ning rate and batch size. The detailed hyperparameter set-\ntings for each model are available in Sec. 10. Regarding our\ninpainting task experiments [29, 37], we evaluate the mod-\nels without specific training for this purpose, using MSE\nto assess image fidelity. Challenges emerged when masks\nhid essential features: unguided models predominantly de-\npended on existing pixels, whereas guided models benefit-\nted from label guidance.\n5. Experiments\nIn our experiments, we assess the effectiveness and effi-\nciency of our quantum models across various datasets and\nconditions. We utilize diverse datasets, including MNIST\nDigits 8 \u00d7 8, Fashion MNIST 28 \u00d7 28, and CIFAR10\n32 \u00d7 32, to explore our models\u2019 performance under vary-\ning data complexities and dimensionalities. Our novel Uni-\ntary Single Sampling approaches are tested in several sce-\nnarios: MNIST Digits 8 \u00d7 8 both unguided without ancilla\nand guided with ancilla, MNIST Digits 32 \u00d7 32 unguided\nwithout ancilla, and MNIST Digits 8 \u00d7 8 unguided without\nancilla on IBM Q hardware. Detailed results of these ex-\nperiments, highlighting key findings and observations, are\npresented in the following sections.\n5.1. MNIST Digits\nWe analyze performance by creating models with varying\nlayer sizes for each dataset and measure their complexity\nvia the sum of trainable parameters. For guided MNIST\n8\u00d78 images, models encompassed approximately 1000 pa-\nrameters. All trained models with their respective configu-\nration can be found in Tab. 2 and Tab. 3.\nQuantum-wise, we employed a dense circuit (Q-Dense)\nwith 47 layers and 7 qubits, utilizing 6 qubits for image\nembedding and measurement, plus an additional one for la-\nbel embedding. A preliminary comparison among quan-\ntum models revealed small advantages via data re-uploading\n[38], but introduced a challenge due to embedding gates al-\ntering the circuit\u2019s quantum state. The best model (7 re-\nuploads, red-line) scored around 10 FID points lower than\nthe no re-upload model, although the difference dwindled\nwith an increasing number of re-uploads.\nFor our comparison, we use fully-connected classical\nnetworks with 1000 trainable parameters and a U-Net of to-\ntal depth 2, having 3 channels in the first block and 6 in the\nsecond. We compared our models to U-Nets with a depth of\n3, and 2 or 4 channels in the initial block, even though they\nexceeded the 1000 parameter limit.\nThe Q-Dense model significantly surpassed its classical\nequivalents, having the same number of parameters, and\nshowed exceptional performance especially when \u03c4 values\nwere in the range of 3 to 5. This is particularly remark-\nable considering that all models were trained with \u03c4 = 10,\nhighlighting the Q-Dense models\u2019 advanced ability to learn\nfrom the original data distribution. However, a drawback is\nobserved when excessive iterations are performed, leading\n0\n2\n4\n6\n8\n10\nTau\n50\n100\n150\n200\n250\n300\nFID Score\nModel\nDense\nQ-Dense\nU-Net\nU-Net (7\n size)\nFigure 4. FID scores on MNIST 8x8 with guided models. \u03c4 de-\nnotes the diffusion steps. Lavender line illustrates larger U-Net\ncapabilities for reference.\nto a decline in FID score caused by ongoing modifications\nto the input image, which ultimately produces artifacts.\nThe purple line represents the largest U-Net model (with\nchannels sized 4, 8, and 16). This model outperformed all\nother architectures and has more than seven times the num-\nber of trainable parameters. Quantum models outperformed\nclassical models with similar parameter counts, achieving\nFID scores around 100, thus 20 points better than classical\nmodels. The quantum models, along with the largest U-Net,\nexhibited slightly more consistent lower score-variance than\nother classical models across all runs, indicating more con-\nsolidated knowledge.\n5.1.1\nInpainting\nWe used MSE to evaluate the inpainting capabilities of\nmodels with \u2248 1000 parameters, testing various masks\nand noise conditions across multiple scenarios, illustrated\nin Fig. 5. Notably, the dense quantum circuit produced vi-\nsually consistent samples with minor artifacts while main-\ntaining high overall quality. Despite presenting a better FID\nscore, the deeper quantum U-Net performed worse com-\npared to its shallower counterpart.\nIn a experiment, where the original pixel get reset after\nthe inpainting, most models showed declining performance\nafter initial steps, with only the deep convolutional network\nmaintaining consistent Fig. 5, albeit low-quality, output.\nSample quality consistently held across varied masks and\npredictive models. In conclusion, our quantum models suc-\ncessfully performed knowledge-transfer tasks without spe-\ncific inpainting training. They achieved satisfactory inpaint-\ning results and MSE scores, which were only marginally\nQU-Net\nQ-Dense\nU-Net\nFigure 5. Inpainting samples with a small mask on the top half,\nresetting the bottom after each of the 10 iterations.\nlower than those of classical networks, despite the classical\nnetworks having twice as many parameters.\n5.2. Fashion MNIST\nWe trained models with approximately 4000 parameters\non a subset of the Fashion dataset, focusing on the \u201dT-\nShirt/Top\u201d and \u201dTrouser\u201d classes due to their relative struc-\ntural similarity and middle-ground complexity between the\nMNIST Digits and CIFAR dataset. This dataset, containing\nmore outliers and variances than MNIST Digits and featur-\ning images of 28 \u00d7 28 pixels, necessitated notably longer\ntraining times for models to stabilize.\n0\n2\n4\n6\n8\n275\n300\n325\n350\n375\n400\nFID\nQ-Dense (4k)\nQ-Dense (2k)\nQU-Net (4k)\nU-Net (2k)\nU-Net (4k)\n0\n2\n4\n6\n8\n0.025\n0.050\n0.075\n0.100\n0.125\nSSIM\nFigure 6. Model sample quality on the Fashion dataset, evaluated\nusing FID and SSIM.\nUpon stabilization, the models exhibited good perfor-\nmance, with the larger dense quantum circuit achieving the\ntop FID score of 280, as depicted in Fig. 6. Our QU-Net and\na classical U-Net achieved comparable FID scores. In terms\nof structural similarity, the Q-Dense models surpassed the\n(Q)U-Nets.\nExamining the samples (Fig. 7), dense quantum circuits\ngenerated well-defined images with noticeable noise, while\nclassical U-Nets produced less noisy, albeit less discernible,\nshapes.\nThe quantum models achieved higher scores in\nSSIM, focusing on general structure, while FID, sensitive\nto noise, offered mixed results, thus rendering inconclusive\nthe performance comparison between the small dense quan-\ntum circuit and the U-Net. PSNR scores mirrored SSIM re-\nsults but also highlighted a lower performance in the quan-\ntum U-Nets, with the smaller QU-Net particularly affected\nBatch\nQ-Dense 2k\nBatch\nQ-Dense 4k\nBatch\nU-Net 2k\nBatch\nU-Net 4k\nBatch\nQU-Net 2k\nBatch\nQU-Net 4k\nFigure 7. Chosen samples from the Fashion dataset, displaying\nevery third \u03c4 and selected for optimal FID score per architecture.\nby substantial artifacts along image edges. Discussion on\npotential solutions to observed issues with dense quantum\ncircuits is available in Sec. 7.\n5.3. CIFAR10\nWe compared models on the CIFAR10 dataset, illustrating\nthe limitations of our quantum models: low output fidelity,\npotential for mode collapse, and slow execution times. Low\noutput fidelity arose primarily from the measurement pro-\ncess\u2019s mathematical properties, wherein the output was al-\nways normalized (Sec. 2.2).\nApplying our approach of\nmultiplying the output by the norm of the input state was\nonly effective for homogeneous datasets like MNIST Dig-\nits. For non-homogeneous datasets like CIFAR10, this ap-\nproach could result in over- or underexposed images. We\nobserved a heightened risk of mode collapse due to the vast\ndifference in modes within the CIFAR10 dataset. Our mod-\nels summed their losses across all batches during training,\nmeaning that dataset outliers could distort the gradient land-\nscape, hindering learning of the full distribution. We em-\nployed varying guided model configurations for quantum\nmodels and a \u2248 1800 parameter U-Net, despite seemingly\nTable 1. Average metrics on CIFAR\nModel\nFID\nPSNR\nSSIM\nU-Net\n395.47\n8.95\n0.026\nQU-Net\n271.03\n9.60\n0.086\nQ-Dense\n399.38\n10.06\n0.061\nsmall for the dataset, to manage training time.\nAs evidenced in Tab. 1, quantum U-Nets outperformed\nclassical U-Nets with the same parameter count, and some\nsmaller QU-Nets achieved superior FID scores.\nCon-\nversely, the dense quantum models, limited by their higher-\ndimensional input state and therefore the number of lay-\ners to prevent memory issues, exhibited the weakest per-\nformance.\nIn the generated samples, small QU-Nets with 1000 pa-\nrameters generated mostly large-scale structures, exhibiting\na high variance. The larger QU-Net with 4000 parameters\ngenerated more detailed images with finer structures. The\nclassical U-Net (2000 parameters) produced styles interme-\ndiate between the two quantum U-Nets. Meanwhile, the\ndense quantum model performed the weakest, displaying\nlittle discernible structure in the samples.\nDespite the evident limitations of the dense quantum\nmodel, quantum U-Nets demonstrated superiority over clas-\nsical U-Nets, affirming the advantages of our quantum con-\nvolution layers over classical convolutions. Nonetheless,\nthey also exhibited significant drawbacks, such as notably\nhigher training and sampling times.\nUpon comparing the training and sampling speeds of\nmodels, simplified U-Nets (134 seconds per training epoch,\n859 seconds per sampling run) proved much slower than Q-\nDense models (2 and 9 seconds) and exhibited better scal-\ning with the number of parameters. The U-Nets, hindered\nby a high parallel batch size and a sequential backward pass,\nwere notably influenced by the high parallel complexity. We\nmitigated this with caching to expedite the training process,\ndespite the requisite higher memory amount.\nIn conclusion, while U-Nets\u2019 sublinear scaling was not\nbeneficial due to a bottleneck in the simulator framework,\nleading to exceedingly slow execution, preliminary experi-\nments with caching the matrix representations of the quan-\ntum layers demonstrated potential for future improvement.\n6. Unitary Single-Sampling\nIn this exploration of our novel unitary single-sampling\nmodels, we benchmarked them against each other in terms\nof sample quality, training duration, and speedup, rather\nthan against conventional models. We selected a subset of\nthe MNIST dataset featuring 8 \u00d7 8 and 32 \u00d7 32 images la-\nbeled 0 and 1, and trained versions with guided, unguided,\nand unguided with ancilla qubit approaches. Moreover, we\nexecuted some models on IBMQ\u2019s 7-qubit quantum hard-\nware since our bit-efficient models required only log2(8\u00d78)\nqubits.\nMatrix transformations of certain trained circuits show-\ncased the performance enhancement of condensed represen-\ntation. We examined various noise initializations for infer-\nence and their resultant impact on image quality, utilizing\namplitude embedding to encode random noise for inputting\ninitial noise x\u03c4. Single-pass models, requiring only a sin-\ngle forward pass during sampling and predicting data re-\nconstruction p\u03b8(xt\u22121|xt), exhibited their advantage when\ntreating the concatenation of repetitions as a singular cir-\ncuit.\n(a) Without ancilla\n(b) With ancilla\nFigure 8. Samples from undirected single-sample models on the\nMNIST 8 \u00d7 8 dataset.\nMNIST Digits 8 \u00d7 8 The 56 quantum layer, undirected\nmodel without ancilla qubits, was transformed into ma-\ntrix representation, streamlining the sampling process from\n1008\u03c4 quantum gate applications to a single matrix multi-\nplication and vastly accelerating the sampling process. Con-\nversion took under 5 seconds for \u03c4 = 1 and was executed\nonce due to class-independence, while repeated multipli-\ncations for other \u03c4 values took mere microseconds in the\nPyTorch framework. The model successfully generated dis-\ncernible digits despite low quality, with its performance sur-\npassing that of its 28-layer counterpart. Although models\nproduced better samples with 8 < \u03c4 < 14, both were capa-\nble of generating distinguishable samples from both classes\nwith adequate training.\n0\n2\n4\n6\n8\n10\n100\n150\n200\n250\nFID\n# Layers\n47\n95\n476\n0\n2\n4\n6\n8\n10\n0.0\n0.1\n0.2\n0.3\nSSIM\nFigure 9. FID and SSIM scores of guided single-sample models\non the MNIST 8 \u00d7 8 dataset\n0\n2\n4\n6\n8\n10\n(a) Simulator\n(b) IBMQ\nFigure 10.\nUnitary Single-Sampling Samples on Simulator\n(guided) and on IBMQ (unguided).\nDirected single-sample models of various architectures\nwere trained, revealing a trend where larger models ben-\nefited from added guidance, as evidenced by a 17% FID\nscore improvement and an SSIM score boost, whereas\nsmaller models could be impeded by excessive re-uploads.\nEvidently, single-sample models generally profited from a\nlarger number of trainable parameters across all metrics, as\ndepicted in Fig. 10a. These models successfully generated\nrecognizable digits from a normal distribution without\nrequiring custom multinomial distributions.\nMNIST Digits 32 \u00d7 32 Given the 10-qubit image\nrepresentation requirement, we limited our evaluation\nto unguided models without ancillas due to simulator\nconstraints. As with the 8 \u00d7 8 dataset, the 133-layer model\n(approx.\n4000 parameters) outperformed the 66-layer\nmodel. However, both models struggled to represent the\ntraining manifold,\nconsistently generating only zeros.\nNotably, the larger model produced recognizable digits up\nto \u03c4 = 16, underscoring the scalability of our approach.\nDespite limitations in class representation, these models\nretained some diffusion model properties, requiring only \u03c4\nmatrix multiplications for results, unlike traditional models.\nMNIST Digits 8\u00d78 on IBMQ We ran our unguided no-\nancilla model on IBMQ\u2019s quantum hardware. As Fig. 10b\nshows, it produced distinguishable samples. Despite inher-\nent quantum hardware noise, with 10000 shots, our fully-\nquantum circuit successfully executed a diffusion model\nin roughly 40 seconds, excluding transpilation and queu-\ning. Classical post-processing was minimal. Our diffusion\nmodels\u2019 inherent noise robustness proved beneficial, acting\nas intrinsic error correction, making them suitable for the\nNISQ era. As lower-noise quantum hardware emerges, our\nmodel\u2019s quality will likely improve. However, experiments\non older calibrated devices yielded less recognizable digits.\n7. Conclusion\nIn our research, we explored quantum denoising diffusion\nmodels, introducing the Q-Dense and QU-Net architecture.\nFurthermore, we introduced a quantum consistency model\ncalled unitary single-sampling, which consolidates the dif-\nfusion process into one unitary matrix, enabling one-step\nimage generation.\nWe benchmarked our models on un-\nguided, guided, and inpainting tasks using datasets like\nMNIST digits, Fashion MNIST, and CIFAR10, employing\nFID, SSIM, and PSNR metrics. We compared our models\nqualitatively to the quantum state-of-the-art, classical deep\nconvolutional networks and U-Nets.\nOur results show that our models vastly outperform\nthe only other quantum denoising model by Dohun Kim\net al. [20]. Additionally, our quantum models surpassed\nsimilarly-sized classical models and matched the efficacy\nof models twice their size. However, in inpainting tasks,\nclassical models still hold an edge. We demonstrated the\none-step generation capabilities of the first working unitary\nsingle-sampling model, both on quantum simulators and\nIBMQ hardware.\nIn future studies, we aim to enhance variational quan-\ntum circuits by streamlining simulations using cached ma-\ntrices, allowing for quicker GPU-parallel execution. Adopt-\ning 16-bit float precision could notably reduce RAM us-\nage [14, 51], considering the demonstrated success in classi-\ncal machine learning and prevalent GPU support for FP16.\nWe\u2019re also keen to explore diffusion patching [30] which\nleverages pixel neighborhoods as channels, a method that\nmight significantly boost execution speed, especially with\nRGB images. A deeper probe into optimal data embed-\nding methods (Sec. 3.3) compared against classical mod-\nels could yield insights into quantum knowledge represen-\ntation. Furthermore, refining dense quantum circuits with\na customized entangling circuit may offer superior spatial\nlocality. Lastly, introducing classical components for post-\nprocessing in our models might present a pathway to cir-\ncumvent quantum state normalization constraints and bol-\nster overall performance.\nAcknowledgements\nThis paper was partially funded by the German Federal\nMinistry of Education and Research through the funding\nprogram \u201cquantum technologies \u2014 from basic research to\nmarket\u201d (contract number: 13N16196).\nReferences\n[1] Adriano Barenco, Charles H. Bennett, Richard Cleve,\nDavid P. DiVincenzo, Norman Margolus, Peter Shor, Tycho\nSleator, John A. Smolin, and Harald Weinfurter. Elementary\ngates for quantum computation. Physical Review A, 52(5):\n3457\u20133467, 1995. 2\n[2] Ville Bergholm,\nJosh Izaac,\nMaria Schuld,\nChristian\nGogolin, Shahnawaz Ahmed, Vishnu Ajith, M. Sohaib\nAlam, Guillermo Alonso-Linaje, B. AkashNarayanan, and\nAli Asadi. PennyLane: Automatic differentiation of hybrid\nquantum-classical computations, 2022. 4\n[3] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick\nRebentrost, Nathan Wiebe, and Seth Lloyd. Quantum ma-\nchine learning. Nature, 549(7671):195\u2013202, 2017. 1, 2\n[4] Iris Cong, Soonwon Choi, and Mikhail D. Lukin.\nQuan-\ntum convolutional neural networks. Nature Physics, 15(12):\n1273\u20131278, 2019. 3\n[5] Marcus Cramer, Martin B Plenio, Steven T Flammia,\nRolando Somma, David Gross, Stephen D Bartlett, Olivier\nLandon-Cardinal, David Poulin, and Yi-Kai Liu. Efficient\nquantum state tomography. Nature communications, 1(1):\n149, 2010. 3\n[6] Li Deng. The MNIST database of handwritten digit images\nfor machine learning research. IEEE Signal Processing Mag-\nazine, 29(6):141\u2013142, 2012. 4, 2\n[7] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\nGANs on image synthesis, 2021. 1, 2, 4\n[8] Michal Drozdzal, Eugene Vorontsov, Gabriel Chartrand,\nSamuel Kadoury, and Chris Pal. The importance of skip con-\nnections in biomedical image segmentation, 2016. 4\n[9] Mohamed Elasri, Omar Elharrouss, Somaya Al-ma\u2019adeed,\nand Hamid Tairi. Image generation: A review. Neural Pro-\ncessing Letters, 54, 2022. 1\n[10] Thomas Gabor, Leo S\u00a8unkel, Fabian Ritz, Thomy Phan,\nLenz Belzner, Christoph Roch, Sebastian Feld, and Clau-\ndia Linnhoff-Popien.\nThe holy grail of quantum artificial\nintelligence: Major challenges in accelerating the machine\nlearning pipeline, 2020. 1, 2\n[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 2\n[12] Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma,\nAmir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Li\nWang, Gang Wang, Jianfei Cai, and Tsuhan Chen. Recent\nadvances in convolutional neural networks, 2017. 4\n[13] Steven Guan, Amir A Khan, Siddhartha Sikdar, and Parag V\nChitnis. Fully dense U-Net for 2-d sparse photoacoustic to-\nmography artifact removal. IEEE journal of biomedical and\nhealth informatics, 24(2):568\u2013576, 2019. 4\n[14] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and\nPritish Narayanan.\nDeep learning with limited numerical\nprecision, 2015. 8\n[15] Maxwell Henderson, Samriddhi Shakya, Shashindra Prad-\nhan, and Tristan Cook.\nQuanvolutional neural networks:\nPowering image recognition with quantum circuits, 2019. 3\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local nash equi-\nlibrium, 2018. 4\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Infor-\nmation Processing Systems, pages 6840\u20136851. Curran Asso-\nciates, Inc., 2020. 1, 2\n[18] He Huang, Philip S. Yu, and Changhu Wang.\nAn intro-\nduction to image synthesis with generative adversarial nets,\n2018. 1\n[19] Huimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu,\nQiaowei Zhang, Yutaro Iwamoto, Xianhua Han, Yen-Wei\nChen, and Jian Wu. U-Net 3+: A full-scale connected U-\nNet for medical image segmentation. In ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 1055\u20131059. IEEE, 2020.\n4\n[20] Dohun Kim and Seokhyeong Kang. Quantum denoising dif-\nfusion probabilistic models for image generation. In Korean\nConference on Semiconductors, 2023. 1, 2, 4, 8\n[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization, 2017. 4\n[22] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. University of Toronto, 2012. 4\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Communications of the ACM, 60(6):84\u201390, 2017. 4\n[24] Michael K\u00a8olle, Alessandro Giovagnoli, Jonas Stein, Maxim-\nilian Balthasar Mansky, Julian Hager, and Claudia Linnhoff-\nPopien.\nImproving convergence for quantum variational\nclassifiers using weight re-mapping, 2022. 4\n[25] Yann LeCun, Koray Kavukcuoglu, and Cl\u00b4ement Farabet.\nConvolutional networks and applications in vision. In Pro-\nceedings of 2010 IEEE international symposium on circuits\nand systems, pages 253\u2013256. IEEE, 2010. 4\n[26] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz,\nJoseph E Gonzalez, and Ion Stoica.\nTune:\nA re-\nsearch platform for distributed model selection and training.\narXiv:1807.05118, 2018. 1, 2\n[27] Shaohui Liu, Yi Wei, Jiwen Lu, and Jie Zhou. An improved\nevaluation framework for generative adversarial networks,\n2018. 4\n[28] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quan-\ntum algorithms for supervised and unsupervised machine\nlearning, 2013. 1, 2\n[29] Andreas Lugmayr, Martin Danelljan, Andr\u00b4es Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool.\nRepaint: Inpaint-\ning using denoising diffusion probabilistic models. CoRR,\nabs/2201.09865, 2022. 4\n[30] Troy Luhman and Eric Luhman. Improving diffusion model\nefficiency through patching, 2022. 8\n[31] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quantum\ncircuit learning. Physical Review A, 98(3), 2018. 4\n[32] Mikko M\u00a8ott\u00a8onen, Juha J. Vartiainen, Ville Bergholm, and\nMartti M. Salomaa. Transformation of quantum states using\nuniformly controlled rotations, 2004. 2\n[33] Alex Nichol and Prafulla Dhariwal. Improved denoising dif-\nfusion probabilistic models, 2021. 2, 4\n[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models, 2022. 3\n[35] Seunghyeok Oh, Jaeho Choi, and Joongheon Kim. A tutorial\non quantum convolutional neural networks (QCNN), 2020. 3\n[36] OpenAI. Gpt-4 technical report, 2023. 1\n[37] Deepak Pathak, Philipp Kr\u00a8ahenb\u00a8uhl, Jeff Donahue, Trevor\nDarrell, and Alexei A. Efros.\nContext encoders: Feature\nlearning by inpainting. CoRR, abs/1604.07379, 2016. 4\n[38] Adri\u00b4an P\u00b4erez-Salinas, Alba Cervera-Lierta, Elies Gil-Fuster,\nand Jos\u00b4e I. Latorre. Data re-uploading for a universal quan-\ntum classifier. Quantum, 4:226, 2020. 2, 5\n[39] John Preskill. Quantum computing in the NISQ era and be-\nyond. Quantum, 2:79, 2018. 2\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 1, 4\n[41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation, 2015. 4\n[42] Maria Schuld, Ville Bergholm, Christian Gogolin, Josh\nIzaac, and Nathan Killoran. Evaluating analytic gradients\non quantum hardware. Physical Review A, 99(3), 2019. 4\n[43] Maria Schuld, Alex Bocharov, Krysta M. Svore, and Nathan\nWiebe. Circuit-centric quantum classifiers. Physical Review\nA, 101(3), 2020. 2, 3\n[44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n2\n[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models, 2020. 2\n[46] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models, 2023. 3, 4\n[47] Andrew Steane. Quantum computing. Reports on Progress\nin Physics, 61(2):117\u2013173, 1998. 2\n[48] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1\u20139, 2015.\n4\n[49] Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao. Image\nsuper-resolution using dense skip connections. In Proceed-\nings of the IEEE international conference on computer vi-\nsion, pages 4799\u20134807, 2017. 4\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need, 2017. 4\n[51] Ganesh Venkatesh, Eriko Nurvitadhi, and Debbie Marr. Ac-\ncelerating deep convolutional networks using low-precision\nand sparsity, 2016. 8\n[52] Guillaume Verdon, Michael Broughton, and Jacob Bia-\nmonte. A quantum algorithm to train neural networks using\nlow-depth circuits, 2019. 1\n[53] Z. Wang, E.P. Simoncelli, and A.C. Bovik. Multiscale struc-\ntural similarity for image quality assessment. In The Thrity-\nSeventh Asilomar Conference on Signals, Systems and Com-\nputers, 2003, pages 1398\u20131402 Vol.2, 2003. 4\n[54] Han Xiao, Kashif Rasul, and Roland Vollgraf.\nFashion-\nMNIST: a novel image dataset for benchmarking machine\nlearning algorithms, 2017. 4, 2\n[55] Xiao-Ming Zhang, Tongyang Li, and Xiao Yuan. Quantum\nstate preparation with optimal circuit depth: Implementa-\ntions and applications. Phys. Rev. Lett., 129:230504, 2022.\n2\nQuantum Denoising Diffusion Models\nSupplementary Material\n8. Preliminary Studies\nIn two preliminary studies, we examine the relationship\nbetween hyperparameters and sample quality metrics like\nFID, PSNR, and SSIM for the QU-Net architecture, as well\nas the impact of input scaling.\nchannels\nFID\nPSNR\nSSIM\nchannels\nFID\nPSNR\nSSIM\nlayers\n0.0\n-0.70.1\n0.3 0.6-0.1\n0.6-0.1-0.70.0\n0.0 0.0 0.0 0.1 0.2\n(a) Q-U-Net Correlation\nchannels\nFID\nPSNR\nchannels\nFID\nPSNR\nSSIM\n-0.0\n-0.6 -0.2\n0.8 0.4 -0.8\n0.6 0.4 -0.7 0.9\n(b) U-Net Correlation\nFigure 11. Hyperparameter-sample quality correlation for QU-\nNets and U-Nets on the Fashion dataset.\nFor quantum U-Nets, besides batch size and learning rate\noptimizations, we primarily consider the number of layers\nL and initial channels C. Analysis reveals that a higher\nsampling step count \u03c4 correlates with better sample qual-\nity. More channels show mixed results, and increasing the\nnumber of layers L marginally improves SSIM and PSNR.\nClassic U-Nets, in contrast, display a strong correlation\nbetween channel number and sample quality across all met-\nrics. From these findings, quantum U-Nets benefit from a\nlarger \u03c4, more channels for PSNR, and additional layers for\nSSIM.\nLastly, we analyze optimal input distributions for our un-\nguided single-sample quantum models, computing them nu-\nmerically as z = U \u22121 \u00b7 x due to the invertibility of the uni-\ntary diffusion matrix U. This analysis shows that the real\nand imaginary parts of input vector z follow normal distri-\nbutions N(\u00b5 = 0.4, \u03c3 = 0.24) and N(\u00b5 = 0, \u03c3 = 0.14) re-\nspectively. When manipulated by our diffusion model, they\nproduce a non-uniform training data distribution x concen-\ntrated around 0 and 1 for dark and bright digit parts, respec-\ntively.\n9. Quantum Architectures\nIn this work, we introduced two quantum architectures for\ndenosing diffusion models. Each architecture has a guided\nand unguided variant as seen in Fig. 13. The guided variants\nall utilize a extra qubit called \u201dancilla\u201d, where the label is\nembedded. Fig. 13b details how the label is embedded using\na RX rotation (angle embedding) and how subsequent vari-\national layers extend to the ancilla qubit. Both variants use\nstrongly entangling layers as variational layers. In Fig. 12\nwe depicted a example of multiple strongly entangling lay-\ners, each trainable parameter \u03b8j\ni is associated with the i-\nth qubit and the j-th rotational gate, where j ranges over\n{0, 1, 2}. For clarity, indices indicating the layer are omit-\nted in the graphic representation. The entangling CNOT\ngates target the qubit at position (i+l) mod n, where l is the\nlayer number. This scheme ensures circular entanglement in\nthe first layer (l = 1), with control and target qubits being\nadjacent, except for the last CNOT gate that completes the\ncircle. In the second layer (l = 2), the control and target\nqubits are separated by one qubit. For instance, the target\nqubit for the first qubit (i = 0) in this layer is the third qubit\n(i = 2).\n10. Hyperparameters\nBefore we conducted our experiments, we ran a hyperpa-\nrameter search for all tested model variants. We primar-\nily focused our hyperparameter optimization on the learn-\ning rate, since it is often the most influencial hyperparame-\nter. We used the bayesian optimization algorithm from the\nRayTune [26] library. All model variants with their respec-\ntive hyperparameters are detailed in Tab. 2 for our classical\nmodels and Tab. 3 for our quantum models.\nFigure 12. Example of the strongly entangling layers part of the VQC, where the red line denotes the layer boundary.\nGuided\nTarget\nDataset\nModel\nModel shape\nLR\nyes\nData\nMNIST digits 8 \u00d7 8\nDeep Dense\n65 and 64 neurons,\nfully connected\n0.0025\nU-Net\nchannel count 4, 8, 16\n0.0057\nU-Net\nchannel count 2, 4, 8\n0.0032\nU-Net\nchannel count 3, 6\n0.0047\nno\nData\nMNIST digits 8 \u00d7 8\nDeep Convolutional\n1, 10, 101\n0.00759\nDeep Convolutional\n1, 6, 6, 6, 6, 1\n0.00013\nU-Net\nchannel count 2, 4, 8\n0.00507\nno\nNoise\nMNIST digits 8 \u00d7 8\nU-Net\nchannel count 2, 4, 8\n0.01339\nDeep Convolutional\n1, 9, 10, 1\n0.00532\nyes\nNoise\nMNIST 28 \u00d7 28\nU-Net\nchannel count 3, 6\n0.01719\nU-Net\nchannel count 2, 4, 8\n0.00641\nyes\nNoise\nFashion MNIST\nDeep Convolutional\n1, 8, 8, 8, 8, 8, 8, 7, 1\n0.01016\nU-Net\nchannel count 7, 14\n0.01270\nTable 2. Learning rates of non-quantum models. The optimal learning rates have been chosen by a Bayesian optimization algorithm,\nfacilitated by the RayTune [26] library. MNIST digits 8 \u00d7 8 refer to the downsampled 8 \u00d7 8 images, MNIST digits 32 \u00d7 32 denotes\nupsampled images to 32 \u00d7 32. MNIST 28x28 refers to the original MNIST handwritten digits dataset [6], Fashion MNIST dataset [54]\nuses 32 \u00d7 32 images.\n(a) Unguided circuit\n(b) Guided circuit\nFigure 13. Unguided and guided single-sampling quantum cir-\ncuits.\nGuided\nTarget\nDataset\nModel\nModel shape\nLR\nyes\nData\nMNIST digits 8 \u00d7 8\nQ-Dense\n47 layers\n0.00097\nQ-Dense Re-Up\n47 layers, 3 re-ups\n0.00360\nQ-Dense Re-Up\n47 layers, 5 re-ups\n0.00345\nQ-Dense Re-Up\n47 layers, 7 re-ups\n0.00362\nno\nData\nMNIST digits 8 \u00d7 8\nQ-Dense\n50 layers\n0.00065\nQ-U-Net Simple\nChannel count 2, 4, 8\n8 Layers each\n0.00023\nQ-U-Net Simple\nChannel count 2, 4, 8\n12 Layers each\n0.00815\nno\nNoise\nMNIST digits 8 \u00d7 8\nQ-Dense\n55 layers\n0.00160\nQ-U-Net Simple\nChannel count 2, 4, 8\n8 Layers each\n0.00113\nQ-U-Net Simple\nChannel count 2, 4, 8\n12 Layers each\n0.00912\nyes\nNoise\nMNIST digits 28 \u00d7 28\nQ-Dense\n30 layers\n0.00409\nQ-Dense\n60 layers\n0.00211\nQ-U-Net Simple\nChannel count 2, 4, 8\n9 Layers each\n0.00287\nQ-U-Net Simple\nChannel count 2, 4, 8\n19 Layers each\n0.01479\nyes\nNoise\nFashion\nQ-Dense\n121 layers\n0.00014\nQ-Dense\n60 layers\n0.00723\nQ-U-Net Simple\nChannel count 3, 6, 12\n8 Layers each\n0.00051\nQ-U-Net Simple\nChannel count 3, 6, 12\n12 Layers each\n0.00029\nno\nData\nMNIST digits 8 \u00d7 8\nSingle Sample\n476 Layers, with ancilla\n0.00012\nno\nMNIST digits 8 \u00d7 8\nSingle Sample\n47 Layers, with ancilla\n0.00322\nno\nMNIST digits 8 \u00d7 8\nSingle Sample\n55 Layers, no ancilla\n0.00172\nno\nMNIST digits 8 \u00d7 8\nSingle Sample\n555 Layers, no ancilla\n0.00002\nno\nMNIST digits 8 \u00d7 8\nSingle Sample\n111 Layers, no ancilla\n0.00419\nno\nMNIST digits 8 \u00d7 8\nSingle Sample\n95 Layers, with ancilla\n0.00104\nyes\nMNIST digits 8 \u00d7 8\nSingle Sample\n476 Layers\n0.00022\nyes\nMNIST digits 8 \u00d7 8\nSingle Sample\n476 layers, 10 re-ups\n0.00016\nyes\nMNIST digits 8 \u00d7 8\nSingle Sample\n47 Layers\n0.00721\nyes\nMNIST digits 8 \u00d7 8\nSingle Sample\n47 layers, 10 re-ups\n0.00220\nyes\nMNIST digits 8 \u00d7 8\nSingle Sample\n95 Layers\n0.00099\nno\nMNIST digits 32 \u00d7 32\nSingle Sample\n66 Layers, no ancilla\n0.00212\nno\nMNIST digits 32 \u00d7 32\nSingle Sample\n133 Layers, no ancilla\n0.00190\nTable 3. Learning rates of quantum models. If not further specified, quantum layers are strongly entangling layers as described in [43].\nThe datasets are the same as in Tab. 2. All models used a batch size of 20 (except the quantum U-Nets, which used a batch size of 10), as\nhigher batch sizes, while slightly improving the progress per iteration, lead to a higher number of crashes due to insufficient memory.\n"
  },
  {
    "title": "HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation",
    "link": "https://arxiv.org/pdf/2401.07727.pdf",
    "upvote": "8",
    "text": "HexaGen3D: StableDiffusion is just one step away from\nFast and Diverse Text-to-3D Generation\nAntoine Mercier\nRamin Nakhli*\nMahesh Reddy\nRajeev Yasarla\nHong Cai\nFatih Porikli\nGuillaume Berger\nQualcomm AI Research\u2020\n{amercier, mahkri, ryasarla, hongcai, fporikli, guilberg}@qti.qualcomm.com\nFigure 1. HexaGen3D leverages pretrained text-to-image models to generate high-quality textured meshes in 7 seconds. Our approach\nworks well with a broad range of text prompts and generalize to new objects or object compositions not encountered during finetuning.\nAbstract\nDespite the latest remarkable advances in generative\nmodeling, efficient generation of high-quality 3D assets\nfrom textual prompts remains a difficult task.\nA key\nchallenge lies in data scarcity:\nthe most extensive 3D\ndatasets encompass merely millions of assets, while their\n2D counterparts contain billions of text-image pairs. To\naddress this, we propose a novel approach which har-\n*Work done at Qualcomm AI Research during an internship.\n\u2020Qualcomm AI Research is an initiative of Qualcomm Technologies,\nInc.\nnesses the power of large, pretrained 2D diffusion mod-\nels.\nMore specifically, our approach, HexaGen3D, fine-\ntunes a pretrained text-to-image model to jointly predict 6\northographic projections and the corresponding latent tri-\nplane.\nWe then decode these latents to generate a tex-\ntured mesh. HexaGen3D does not require per-sample op-\ntimization, and can infer high-quality and diverse objects\nfrom textual prompts in 7 seconds, offering significantly bet-\nter quality-to-latency trade-offs when comparing to exist-\ning approaches. Furthermore, HexaGen3D demonstrates\nstrong generalization to new objects or compositions.\n1\narXiv:2401.07727v1  [cs.CV]  15 Jan 2024\n1. Introduction\nThe creation of 3D assets, crucial across various do-\nmains such as gaming, AR/VR, graphic design, anime, and\nmovies, is notably laborious and time-consuming. Despite\nthe use of machine learning techniques to aid 3D artists in\nsome aspects of the task, the development of efficient end-\nto-end 3D asset generation remains challenging, primarily\nbottlenecked by the limited availability of high-quality 3D\ndata.\nIn contrast, image generation has recently witnessed a\nsurge in quality and diversity, driven by the training of\nlarge models [14, 31, 34, 36, 37, 48] on large datasets like\nLAION-5B [38] containing billions of text-image pairs.\nThis progress in 2D generative modeling, coupled with the\nlack of curated 3D data, has prompted recent efforts to\nadapt 2D pre-trained models for 3D asset creation. One\nnotable example is DreamFusion [32], which utilizes the\nprior knowledge of a large 2D diffusion model to guide the\noptimization of a 3D representation, usually a NeRF [27].\nWhile this approach, dubbed Score Distillation Sampling\n(SDS), and follow-up variants have showed promising re-\nsults, their main limitation remains the extremely long gen-\neration time, taking anywhere from 20 minutes to several\nhours, and the lack of diversity across seeds.\nTo address these challenges, we present HexaGen3D, a\nnovel text-to-3D model that significantly reduces genera-\ntion time without sacrificing quality or diversity. Similar to\nDreamFusion, we build upon pre-trained 2D models but in-\nstead propose to modify and finetune them for direct, feed-\nforward, generation of 3D objects. Below are the key con-\ntributions of our work:\n\u2022 We adopt a similar approach to 3DGen [11], but re-\nplace their custom-designed latent generative model\nwith a pre-trained text-to-image model, reducing the\nneed for extensive 3D finetuning data and enabling\ngeneralization to new objects or compositions not en-\ncountered during training.\n\u2022 We introduce \u201cOrthographic Hexaview guidance\u201d, a\nnovel technique to align the model\u2019s 2D prior knowl-\nedge with 3D spatial reasoning. This intermediary task\ninvolves predicting six-sided orthographic projections\nwhich we then map to our final 3D representation. This\nallows the U-Net of existing 2D diffusion models to\nefficiently perform multi-view prediction and 3D asset\ngeneration in a sequential manner, with 3D generation\nonly requiring one additional U-Net inference step.\n\u2022 HexaGen3D competes favorably with existing ap-\nproaches in quality while taking only seven seconds on\nan A100 to generate a new object. This speed is orders\nof magnitude faster than existing approaches based on\nSDS optimization.\n2. Related Work\nRecent methods heavily leverage diffusion models [12]\nfor generating 3D meshes from single images and/or textual\nprompts. In this section, we briefly review these works and\nrefer readers to [30] for a more comprehensive survey.\nUsing Score Distillation Sampling: Pioneering works,\nsuch as DreamFusion [32] and Latent-NeRF [26] gener-\nates 3D by training a NeRF using Score Distillation Sam-\npling (SDS). However, they require lengthy, per-sample op-\ntimization, taking \u223c2 hours to generate an object. Later\nmethods accelerate this process by first producing a coarse\nNeRF model and then upscaling or enhancing it, e.g.,\nMagic3D [17], Make-it-3D [42], or by first generating ge-\nometry and then baking the texture using images generated\nfrom diffusion [43]. These techniques considerably reduce\ngeneration time, e.g., to 20 - 40 minutes. More recently,\nMVDream [40] replaces the guidance model with a multi-\nview diffusion model, leading to improved visual quality,\nat the cost of slower generation times, taking up to 3 hours\nto generate an object. ATT3D [22] proposes to map text\nprompts to a NeRF embedding using a network. This en-\nables generating a mesh within a second, but lacks mesh\nquality and generalizability.\nFeedforward Generation via Diffusion: Several works\nuse diffusion to directly generate 3D structure or represen-\ntation [6, 11, 15, 22, 23, 29, 41, 49, 50]. Point-E [29] uses\ndiffusion to generate a point cloud, conditioning on an im-\nage synthesized from a text input. Shap-E [13] uses a sim-\nilar transformer-based diffusion architecture but generates\nparameters of implicit 3D functions like NeRF or DMTet.\nMore recently, 3DGen [11] adopts a latent triplane represen-\ntation, which can be denoised by a diffusion model and fed\nto a point cloud VAE decoder to generate a 3D mesh. SSD-\nNeRF [6] also adopts the triplane representation but uses a\nNeRF decoder.\nLeveraging 2D View Synthesis: These works utilize\ndiffusion models to generate new 2D views to support 3D\ngeneration.\n3DiM [46] generates multiple views of tar-\nget poses, based on a source view and its pose. One-2-3-\n45 [18] utilizes Zero-1-to-3 [19] to generate several views\nof an object. It then constructs a volumetric representa-\ntion based on the generated images and can produce a mesh\nwithin 45 seconds. However, the 2D views are generated\none at a time and may not provide consistent multi-view\ninformation for the volume.\nOther works employ diffu-\nsion to generate multiple views simultaneously, e.g., Sync-\nDreamer [20], and Instant3D [1], and reconstructs 3D ob-\njects based on these synthesized views.\nWonder3D [21]\ncreates a multi-domain diffusion model to generate both 2D\nimages and normal maps, and trains a signed distance func-\ntion to extract the underlying textured mesh. Some papers\nfurther introduce 3D awareness into the image synthesis\nprocess, e.g., GeNVS [4], NerfDiff [9], SparseFusion [51],\n2\n(a) HexaGen3D Overview\n(b) UV Texture Baking\n(c) Triplanar Latent Generation\n(d) Hexa-to-triplane layout conversion\nFigure 2. Method overview. HexaGen3D predicts textured meshes from textual prompts in 7 seconds, leveraging pre-trained text-to-image\nmodels for latent generation. Like 3DGen [11], the training procedure comprises two stages and the latent representation is based on\ntriplanes. Top-left (a): High-level overview of the two stages: triplanar latent auto-encoder and StableDiffusion-based latent generation.\nBottom-left (c): StableDiffusion-based triplanar latent generation via orthographic hexaview diffusion (section 3.2). Bottom-right (d):\nDetailed overview of our hexa-to-triplane layout converter (section 3.2.3). Top-right (b): Detailed overview of our UV texture baking\nprocedure, used as a post-processing step to improve the visual appearance of the decoded mesh (section 3.3).\nSparse3D [52], providing better multi-view consistency.\nTexture Baking: While some of the aforementioned\nworks can also be used to refine textures of an existing\nmesh [17, 32], several studies focus on texture baking. For\ninstance, Text2Tex [5] and TEXTure [35] uses off-the-shelf\nStableDiffusion models [36] to iteratively paint texture over\nknown geometry. X-Mesh [25] adopts a dynamic attention\nmodule and the CLIP loss between the text prompt and ren-\ndered images to generate texture. TexFusion [3] applies a\ndiffusion model\u2019s denoiser on rendered 2D images and ag-\ngregates the denoising predictions on a shared latent texture\nmap to achieve better texture consistency.\n3. Method\nOur latent generative approach, HexaGen3D, leverages\npre-trained text-to-image models to predict textured meshes\nfrom textual prompts. Like 3DGen [11], the training proce-\ndure comprises two stages. In the first stage, we learn a\ntriplanar representation \u2014 capturing the shape and color of\na textured mesh \u2014 using a variational auto-encoder (VAE)\n[16]. In the second stage, we finetune a pre-trained text-\nto-image model to synthesize new samples in this triplanar\nlatent space. Figure 2a provides a comprehensive overview\nof each component.\nAt test time, we infer new 3D assets in a feedforward\nmanner \u2014 without requiring per-sample optimization \u2014\nallowing for faster and more efficient generation. We first\nproduce triplanar latents from random noise using our la-\ntent generator and then convert these latents into a textured\nmesh using the decoder from the first stage.\nFor stage 1, our approach retains most architecture com-\nponents of the auto-encoder from 3DGen [11] (detailed in\nsection 3.1).\nHowever, a key distinction of our work is\nthat we propose to leverage pre-trained text-to-image mod-\nels for stage 2 (detailed in section 3.2). Notably, our results\ndemonstrate that, with proper guidance and minimal fine-\ntuning data, 2D generative models such as StableDiffusion\ntransfer surprisingly well to 3D asset generation.\n3.1. Stage 1: Learning the Triplanar Tepresentation\nIn this stage, we train a VAE to learn a triplanar la-\ntent representation of textured meshes, derived from point\nclouds. Our solution closely follows the framework estab-\nlished by 3DGen [11], as we re-use several components\nfrom their methodology. Initially, a PointNet model [33]\nencodes the input point cloud into a triplanar representation.\nThese latents are subsequently refined through two succes-\nsive networks, and then converted into a textured mesh us-\ning a decoder based on the DMTet algorithm [39], as de-\n3\npicted in Figure 2a.\nSimilar to 3DGen, our pipeline utilizes a U-Net with 3D-\naware convs [45] for the first refinement network. However,\nfor the second network, we replace the 3D-aware U-Net ar-\nchitecture used in 3DGen to the VAE architecture typically\nused in StableDiffusion models [36] and utilize the low-\nresolution intermediate features from this second network\nas the target representation for latent generation.\n3.2. Stage 2: Triplanar Latent Generation using\nPretrained 2D Diffusion Models\nIn this stage, our goal is to generate triplanar latents that\nare consistent with the provided textual prompt. Unlike ap-\nproaches such as 3DGen [11] or Shap-E [13] which focus\non developing and training novel architectures from scratch,\nwe aim to build upon pre-trained text-to-image models and\nexplore how to extend them to the 3D domain. A key aspect\nof our approach is the introduction of a novel intermediary\ntask, \u201cOrthographic Hexaview\u201d prediction, specifically de-\nsigned to bridge the gap between 2D and 3D synthesis.\n3.2.1\nOrthographic Hexaview Guidance\nAs typically done in the literature, our triplanar representa-\ntion consists of three feature maps concatenated along the\nwidth axis to form a 1 \u00d7 3 \u201crolled-out\u201d layout. This two-\ndimensional arrangement makes it possible to apply 2D ar-\nchitectures to triplanar latent generation, yet this task still\nrequires 3D geometric reasoning capabilities to ensure fea-\nture consistency across the three planes. Our results suggest\nthat pre-trained text-to-image models, despite their prior\nknowledge on how to arrange multiple objects coherently,\nstruggle when directly fine-tuned to generate such rolled-\nout triplanes, potentially due to the limited 3D data avail-\nable during fine-tuning. To address this, we decompose the\ngeneration process into two steps, introducing an intermedi-\nate \u201chexaview\u201d representation designed to guide the latent\ngeneration process.\nAs depicted in Fig. 2c, the intermediate \u201chexaview\u201d\nrepresentation includes latent features from six orthogonal\nviews (front, rear, right, left, top, and bottom), spatially\nconcatenated to form a 2 \u00d7 3 layout.\nThe triplanar la-\ntent generation process is decomposed into two stages. We\nfirst diffuse hexaview latents using a modified and finetuned\ntext-to-image U-Net (detailed in section 3.2.2). Then, we\nmap these latents to our target triplanar representation, re-\nusing the same diffusion U-Net for an additional inference\nstep and a small layout converter (detailed in section 3.2.3).\nComputation-wise, this means we perform N + 1 U-Net\ninferences in total, N steps to obtain a multi-view represen-\ntation of the object, and 1 additional U-Net forward pass to\ngenerate the corresponding mesh.\nUnlike concurrent 3D asset generation approaches based\nFigure 3. Perspective vs Orthographic views. Orthographic views\nare free from perspective distortions (highlighted in green), facili-\ntating re-alignment to triplanar representations.\nFigure 4. The orthographic hexaview prediction contains realistic\ndetails in the pixel space compared to the generated mesh. To\nsignificantly improve the visual feel of the final generated mesh,\nwe \u201cbake\u201d the detailed hexaview prediction to the UV texture map\n(see Sec. 3.3).\non multi-view prediction [1, 40], a key and novel feature\nof our method is the use of orthographic projections*. As\nshown in Fig. 3, this form of parallel projection, com-\nmonly used in mechanical drawings of 3D parts, avoids\nperspective distortions, thereby guaranteeing pixel-accurate\nre-alignment of each orthogonal view to the corresponding\nplane in our target latent representation.\nAnother novel aspect of our approach is that the same 2D\nU-Net is trained to perform multi-view prediction and 3D\nasset generation simultaneously. To the best of our knowl-\nedge, this multi-task scenario has never been explored be-\nfore. Interestingly, our results show that the same network\ncan perform both tasks well and that the proposed hexaview\nguidance is in fact instrumental in unlocking high-quality\n3D asset generation, offering a new technique to capital-\nize on the synthesis capabilities of large pre-trained text-to-\nimage models.\n*https://en.wikipedia.org/wiki/Orthographic_projection\n4\n3.2.2\nU-Net architecture modifications\nIn our experiments, we adapt the U-Net architecture from\nStableDiffusion models, though we believe our method\ncould be effectively applied to other text-to-image models\nas well. Fig. 2c illustrates the modifications we applied to\nthe U-Net architecture, aimed at enhancing its suitability for\nthe 3D domain:\nAdditional conditioning signals:\nOur first modification\nintroduces two types of learnable encodings which we add\nto the timestep embeddings, providing a mechanism to\nmodify the timestep-based modulations of convolutional\nand attention layers performed in the original U-Net. This\nmodification draws inspiration from techniques like the\ncamera embedding residuals in MVDream [40] or the do-\nmain switcher proposed in Wonder3D [21]. The first type\nof encodings is based on positional embeddings. These help\nin meeting the strict spatial alignment constraints imposed\nby the hexaview and triplane representations and can be\nthought as serving the same purpose as the 3D-aware convo-\nlutions in 3DGen [11]. The second type, domain encodings,\ninvolves learning distinct feature vectors for each phase of\nthe generation process \u2013 one for the N hexaview diffusion\nsteps (2D domain) and another for the final triplane map-\nping step (3D domain). This additional encoding allows\nour model to adjust its processing dynamically based on the\ncurrent stage of the latent generation process.\n\u201cMake-it-3d\u201d token:\nAnother modification is the in-\ntroduction of a special \u201cMake-it-3d\u201d token during the\nhexaview-to-triplanar mapping step.\nThe corresponding\nembedding vector is attended to via cross-attention layers\nand gives the U-Net component yet another mechanism to\nadapt its behavior specifically for triplanar latent prediction.\n3.2.3\nHexa-to-triplane Layout Converter\nFollowing hexaview diffusion, we apply the U-Net for an\nadditional inference step, using the 3D-specific condition-\ning techniques discussed in the previous section, and ex-\ntract its penultimate features. We then map these features to\nthe target triplanar representation using a specialized mod-\nule called hexa-to-triplane layout converter. As shown in\nFig. 2d, our layout converter starts by re-orienting each view\nfrom the 2\u00d73 input layout to align it with the corresponding\nplane from our 3D latent representation. This is achieved\nthrough a series of parameter-free operations, including\nslicing to isolate individual views, rotating and flipping to\norient them correctly, and concatenating parallel views (i.e.\ntop and bottom, left and right, front and back). The use of\northographic projections is advantageous here, as it guar-\nantees pixel-accurate layout conversion and eliminates per-\nspective distortions. A convolutional neural network (CNN)\nthen predicts the final triplanar latents.\nIn practice, the\nCNN\u2019s main purpose is to adjust the number of channels to\nalign with our independently-trained VAE. Our experiments\nindicate that the U-Net is effective in multi-tasking and pre-\ndicting both hexaview latents and 3D-conditioned features\nthat map well to the final triplanar latents. Consequently,\nthe additional CNN does not need to be very complex.\n3.3. Texture baking for better visual appearance\nIn order to further enhance the visual appearance of the\nfinal mesh, we introduce a UV texture baking approach that\nleverages the six-sided orthographic views predicted during\nhexaview diffusion. Interestingly, these views often contain\nfine textural details that are missing in the VAE-decoded\nmesh. To address this, we adopt a post-processing proce-\ndure which \u201cbakes\u201d the intermediate hexaviews onto the\ndecoded mesh, as illustrated in Fig. 2b. This procedure in-\nvolves three steps:\n1. UV Mapping: We employ an automatic UV unwrap-\nping algorithm to create a UV mapping, making it pos-\nsible to project a 2D texture onto the 3D mesh.\n2. Texture Initialization: We then initialize our UV tex-\nture, mapping each texel to its corresponding 3D co-\nordinate, sampling the corresponding triplanar feature,\nand converting it to RGB using the Color MLP from\nour VAE decoder.\nWhile this step alone does not\nsubstantially enhance visual quality \u2014 even with in-\ncreased texture resolution \u2014 it is crucial for areas not\nfully covered by the final hexaview baking step.\n3. Hexaview Baking: We project pixels from the six (de-\ncoded) orthographic views onto the UV texture.\nIn\ncases where multiple hexaview pixels correspond to\nthe same texel, we average their values. This final step\nsignificantly enhances the visual quality of the mesh,\nas shown in Fig. 4.\nWhile this texture baking procedure effectively ad-\ndresses the limitations posed by the initial mesh decod-\ning, further improvements in the VAE pipeline represent\na promising avenue for future research. We believe that\nrefining this component, such as potentially using higher-\nresolution latents, could alleviate some of the current bot-\ntlenecks, and we reserve this exploration for future work.\n3.4. Implementation details\nTraining data.\nWe train our models on 3D assets from the\nObjaverse dataset [8], complemented with automated cap-\ntions generated by Cap3D [24]. Following findings from\nprior work [1, 11, 40], we filter out lower-quality assets\nfrom the original dataset, ultimately training our models on\n86, 784 curated objects. The selection criteria for these as-\nsets were based on factors such as geometric complexity\n5\nFigure 5. We compare \u201cHexaGen3D\u201d with other text-to-3D approaches: DreamFusion-SDv2.1 (DF) [32], TextMesh-SDv2.1 (TM) [43],\nMVDream-SDv2 (MV) [40], and Shap-E [13]. We use the threestudio [10] framework that implements the DF, TM, and MV approaches\nthat leverages the StableDiffusion [36] text-to-image model. We use the text prompts from threestudio [10] based on DF.\nand textural richness (more details can be found in the ap-\npendix).\nIn addition to 3D assets, we augment our training set\nwith regular images while finetuning the latent diffusion\nmodel.\nThe incorporation of 2D data helps to mitigate\nthe limited amount of available 3D data and enhances the\nmodel\u2019s generalization to unseen 3D objects.\nThis form\nof multi-task regularization has been similarly employed in\nMVDream [40].\nIn terms of losses, we follow 3DGen and combine a\nmask silhouette loss, a depth loss, a laplacian smoothness\nloss and a KL divergence loss to supervise the geometry\nVAE, Lgeometry = \u03b1Lmask + \u03d5Ldepth + \u03bbLsmooth \u2212\n\u03b3DKL, with \u03b1 = 3 , \u03d5 = 10, \u03bb = 0.01, and \u03b3 = 10\u22127, and\nuse a sum of L1 and L2 losses to supervise the color VAE.\nWe train our VAE models for 15 epochs, using a batch\nsize of 16, an initial learning rate of 3 \u00d7 10\u22125 and a co-\nsine annealing schedule down to a minimum learning rate\nof 10\u22126. This takes roughly a week on 8 A100s.\nStage 2: Triplanar Latent Generation.\nArchitecture-\nwise, we experimented with two pretrained StableDiffusion\nU-Net backbones: SD-v1.5 [36] and SD-XL [31]. The neu-\nral net used within our hexa-to-triplane layout converter is\na two-layer convnet with SiLU activations and trained from\nscratch.\nFor the intermediate hexaview generation, we rasterize\nsix 512 \u00d7 512 orthographic projections of the ground truth\nmesh, downscale them to 256 \u00d7 256, and encode these im-\nages using the VAE encoder from the corresponding Stable\nDiffusion model. The resulting 32 \u00d7 32 latents are then\nre-organized into a 2 \u00d7 3 layout of size 64 \u00d7 96. For 2D\nregularization, we use high-res images which we downsize\nto 256 \u00d7 256, matching the resolution of individual views\nwithin our hexaview representation. Considering that Sta-\nbleDiffusion models often have difficulties with predicting\n6\nMethod\nLatency \u2193\nCLIP score \u2191\nUser preference score \u2191\nSingle A100 GPU\nCLIP L/14\nCLIP B/16\nCLIP B/32\nVis. qual.\nPrompt fid.\nMVDream-SDv2.1 [40]\n\u2248 194 mins\n25.02\n30.35\n29.61\n0.97\n0.88\nTextMesh-SDv2.1 [43]\n\u2248 23 mins\n19.46\n25.06\n24.86\n0.12\n0.21\nDreamFusion-SDv2.1 [32]\n\u2248 22 mins\n23.76\n28.91\n28.96\n0.50\n0.52\nShape-E [13]\n\u2248 7 secs\n19.52\n24.33\n24.70\n0.17\n0.13\nHexaGen3D-SDv1.5\n\u2248 7 secs\n24.02\n28.84\n28.55\n0.51\n0.49\nHexaGen3D-SDXL\n\u2248 7 secs\n24.98\n29.58\n28.97\n0.73\n0.77\nTable 1. Quantitative comparison of HexaGen3D with other approaches on 67 prompts from the DreamFusion [32] prompts available in\nthreestudio [10]. We report the inference time on a single A100, CLIP scores for three different CLIP models, as well as user preference\nscores based on two criteria, visual quality and prompt fidelity. The preference scores reported here are obtained by averaging the pair-wise\npreference rates from Fig. 6.\n(a) Evaluating the approaches for \u201cvi-\nsual quality\u201d.\n(b) Evaluating the approaches on\n\u201ctext prompt fidelity\u201d.\nFigure 6.\nUser study comparing all the text-to-3D approaches\non (a) visual quality and (b) text prompt fidelity. Each cell in-\ndicates the user preference score (%) for an approach (row) over\nanother (column). The approaches are: MVDream-SDv2.1 (MV-\nSD), DreamFusion-SDv2.1 (DF-SD), Shape-E, TextMesh-SDv2.1\n(TM-SD), HexaGen3D-SDv1.5 (Ours-SD), and HexaGen3D-\nSDXL (Ours-XL).\nplain white backgrounds [1], we add random colored back-\ngrounds to the hexaviews. This addition does not impact\ntriplanar latent generation, as the model effectively learns\nto disregard the background color in this context.\nDuring training, we effectively optimize the model to\nsolve three tasks simultaneously: hexaview latent genera-\ntion, hexaview-to-triplane mapping and image latent gener-\nation (for regularization). We achieve this using gradient ac-\ncumulation, aggregating gradients from all three tasks. We\nuse a batch of 192 hexaviews for the hexaview diffusion\nand hexaview-to-triplanar mapping tasks, and a batch of 32\nregular images for 2D regularization. We down-weigh the\ndiffusion loss coming from the 2D regularization batches\nby a factor 0.25. We train our latent generative models for\n50, 000 iterations using a learning rate of 3 \u00d7 10\u22125. This\ntakes roughly 4 days on 8 A100s.\nBaselines\nWe select a range of recent text-to-3D ap-\nproaches for comparison with HexaGen3D, including a\nfeedforward approach, Shap-E [13], and three SDS-based\nMethod\nCLIP score \u2191\nCLIP L/14\n(A1) HexaGen3D-SDv1.5\n24.02\n(A2) No Weight Sharing\n23.43\n(A3) No Hexaview Baking\n21.85\n(A4) No Hexaview Prediction\n18.47\n(B1) HexaGen3D-SDXL\n24.98\n(B2) No Weight Sharing\n24.97\n(B3) No Hexaview Baking\n23.59\nTable 2. We quantitatively compare different design choices in\nour HexaGen3D model and provide more detailed descriptions in\nsection 4.2.\napproaches, DreamFusion [32], TextMesh [43], and MV-\nDream [40]. While we use the official implementation of\nShap-E\u2020 and MVDream\u2021, we leverage the DreamFusion\nand TextMesh implementations available within the three-\nstudio framework [10], which include some notable devia-\ntions from their original papers. Specifically, our Dream-\nFusion setup uses the open-source StableDiffusion v2.1 as\nthe guidance model instead of Imagen [37], and a hash\ngrid [28] as the 3D representation, diverging from Mip-\nNerf360 [2]. TextMesh similarly utilizes StableDiffusion\nv2.1 for guidance, with NeuS [44] replacing VolSDF [47]\nfor the 3D signed distance field representation.\n4. Experiments\nAs illustrated in Fig. 1, HexaGen3D generates high-\nquality textured meshes from a wide array of textual\nprompts, including complex prompts that involve unlikely\ncombinations of objects, diverse styles or unique material\nproperties. In Section 4.1, we compare HexaGen3D with\nexisting methods, and Section 4.2 details our ablation study\nresults.\n\u2020https://github.com/openai/shap-e\n\u2021https://github.com/bytedance/MVDream-threestudio\n7\nFigure 7.\nWe show four (4) generations for each text prompt\nto demonstrate HexaGen3D generates more diverse 3D objects\ncompared to the baseline MVDream-SDv2.1 and DreamFusion-\nSDv2.1 text-to-3D models.\n4.1. Comparison against existing methods\nFigure 5 shows a comparison of HexaGen3D against\nShap-E [13], DreamFusion [32], TextMesh [43], and MV-\nDream [40]. Our approach demonstrates superior perfor-\nmance in most aspects, notably offering significantly faster\nrun times. While MVDream excels in producing higher-\nquality meshes, the extensive generation time hinders its\npracticality, as this baseline takes up to 3 hours to cre-\nate an object on an NVIDIA A100 GPU \u2014 a process\n1600\u00d7 slower than HexaGen3D, which accomplishes this\nin merely 7 seconds. Furthermore, MVDream exhibits lim-\nitations in terms of diversity, often leading to similar out-\nputs across seeds. This point is illustrated in Fig. 7, where\nwe show 4 generations for the same prompt, varying the\nseed, emphasizing the advantage of HexaGen3D in gener-\nating more diverse meshes.\nIn our quantitative comparison (Tab. 1), HexaGen3D,\nwith variants based on SD v1.5 and SDXL, excels in mesh\nquality and prompt fidelity against these baselines, signifi-\ncantly outperforming Shape-E, the only other solution that\ncan compete with HexaGen3D in terms of speed. Despite\nusing an earlier version of StableDiffusion, our user stud-\nies and CLIP score evaluations indicate that HexaGen3D-\nSDv1.5 performs roughly on par with DreamFusion while\nFigure 8. The generated 3D objects can contain minor limitations\nin terms of box artifacts, and intricate or thin mesh structures.\nbeing 180\u00d7 faster. Switching to an SDXL backbone boosts\nour results significantly without impacting latency much.\nHexaGen3D-SDXL reaches a preference rate of 78% over\nDreamFusion (see Fig. 6) and an average prompt fidelity\npreference score of 0.77, narrowing the gap with MV-\nDream\u2019s score of 0.88 on this metric.\n4.2. Ablation studies\nWe comprehensively validate some of our design choices\nthrough ablation studies in Table 2. Key findings can be\nsummarized as follows:\n\u2022 Multi-tasking multi-view and 3D generation is ben-\neficial.\nWe find it interesting that the same U-Net\ncan perform both hexaview diffusion and hexaview-to-\ntriplanar mapping with reasonable accuracy. In rows\nA2 and B2, we study how utilizing two distinct U-\nNets (same architecture, separate weights) for these\ntwo tasks affects performance. Interestingly, using two\nseparate networks leads to a noticeable performance\ndrop with SDv1.5 (A2). With SD-XL (B2), removing\nweight sharing does not make much of a difference\n\u2014 but has the disadvantage of roughly doubling the\nmodel size and memory footprint.\n\u2022 Hexaview baking provides a significant quality\nboost. Removing the finer textural details produced\nby our baking procedure translates into significantly\nlower CLIP scores, both with SDv1.5 (A3) or SDXL\n(B3).\n\u2022 Hexaview guidance is a necessary regularization.\nTraining the same model to directly denoise tripla-\nnar latents similar to 3DGen [11], without hexaview\nguidance (A4), leads to significantly worsened perfor-\nmance, illustrating the importance of using six-sided\nmulti-view prediction as a surrogate task to steer the\nmodel toward triplanar latent generation.\n8\n5. Discussion\nOur results demonstrate that existing text-to-image mod-\nels can be directly adapted to 3D asset generation with only\nminor modifications and minimal fine-tuning data. To fa-\ncilitate this knowledge transfer, we introduce orthographic\nhexaview guidance, a surrogate task designed specifically\nto steer the 2D capabilities of existing text-to-image mod-\nels toward 3D synthesis. This task, which amounts to pre-\ndicting six-sided orthographic projections, can be thought\nof as a form of multi-task regularization. Additionally, we\npresent simple yet effective architectural modifications that\nbetter fit existing 2D diffusion architectures for 3D gener-\nation. We show the effectiveness of our approach through\nextensive experimental results and believe these techniques\nare widely applicable to various text-to-image models. No-\ntably, our approach scales well to larger pre-trained text-\nto-image models, with the SDXL variant significantly out-\nperforming its SDv1.5 counterpart, both in terms of mesh\nquality and prompt fidelity. This scalability highlights Hex-\naGen3D\u2019s potential in a context where 3D data is scarce.\nWhile relatively rare, 3D objects generated by Hexa-\nGen3D occasionally contain artifacts such as a box scaf-\nfolding enclosing the 3D object as shown in Fig. 8 (left).\nThis usually happens when the hexaview diffusion process\ngenerates inconsistent non-uniform backgrounds, leaving\nno choice but to add \u201cwalls\u201d around the main object during\ntriplanar latent generation to resolve these inconsistencies.\nAdditionally, as observed in many other 3D mesh recon-\nstruction techniques, generating 3D objects with intricate\nor thin structures is notoriously challenging \u2014 see Fig. 8\n(right). Looking forward, exploring to what extent these\nissues could be resolved with more 3D data [7] would be\ninteresting. Enhancing the VAE pipeline, which we mostly\nkept unchanged in this work compared to the original imple-\nmentation proposed in 3DGen [11], present another exciting\navenue for future research.\nIn conclusion, we believe HexaGen3D stands out as an\ninnovative and practical solution in the field of 3D asset\ngeneration, offering a fast and efficient alternative to the\ncurrent generation methods. HexaGen3D generates high-\nquality and diverse textured meshes in 7 seconds on an\nNVIDIA A100 GPU, making it orders of magnitude faster\nthan existing approaches based on per-sample optimiza-\ntion. Leveraging models pre-trained on large-scale image\ndatasets, HexaGen3D can handle a broad range of textual\nprompts, including objects or object compositions unseen\nduring finetuning (see supplementary materials for more\nresults, including generations from MS-COCO captions).\nFurthermore, HexaGen3D generates diverse meshes across\ndifferent seeds, a significant advantage over SDS-based ap-\nproaches like DreamFusion or MVDream. We believe these\nattributes make HexaGen3D a pioneering tool in the rapidly\nevolving domain of 3D content creation.\nReferences\n[1] Anonymous.\nInstant3d: Fast text-to-3d with sparse-view\ngeneration and large reconstruction model. Under Review,\n2023. 2, 4, 5, 7\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 7\n[3] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,\nand Kangxue Yin. Texfusion: Synthesizing 3d textures with\ntext-guided image diffusion models. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 4169\u20134181, 2023. 3\n[4] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W\nBergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini\nDe Mello, Tero Karras, and Gordon Wetzstein. Generative\nnovel view synthesis with 3d-aware diffusion models. arXiv\npreprint arXiv:2304.02602, 2023. 2\n[5] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey\nTulyakov, and Matthias Nie\u00dfner.\nText2tex:\nText-driven\ntexture synthesis via diffusion models.\narXiv preprint\narXiv:2303.11396, 2023. 3\n[6] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: A\nunified approach to 3d generation and reconstruction. arXiv\npreprint arXiv:2304.06714, 2023. 2\n[7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 9\n[8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 5\n[9] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoor-\nthi.\nNerfdiff:\nSingle-image view synthesis with nerf-\nguided distillation from 3d-aware diffusion. In International\nConference on Machine Learning, pages 11808\u201311826.\nPMLR, 2023. 2\n[10] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 6, 7\n[11] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371, 2023. 2, 3, 4,\n5, 8, 9\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n9\n[13] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 2, 4, 6, 7, 8\n[14] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park.\nScal-\ning up gans for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10124\u201310134, 2023. 2\n[15] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten\nKreis, Katja Schwarz, Daiqing Li, Robin Rombach, An-\ntonio Torralba, and Sanja Fidler.\nNeuralfield-ldm: Scene\ngeneration with hierarchical latent diffusion models.\nIn\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8496\u20138506, 2023. 2\n[16] Diederik P Kingma, Max Welling, et al. An introduction\nto variational autoencoders.\nFoundations and Trends\u00ae in\nMachine Learning, 12(4):307\u2013392, 2019. 3\n[17] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2, 3\n[18] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023. 2\n[19] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 2\n[20] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 2\n[21] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 2, 5\n[22] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin,\nMing-Yu Liu, Sanja Fidler, and James Lucas.\nAtt3d:\nAmortized text-to-3d object synthesis.\narXiv preprint\narXiv:2306.07349, 2023. 2\n[23] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2837\u20132845, 2021. 2\n[24] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models. arXiv\npreprint arXiv:2306.07279, 2023. 5\n[25] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei\nWang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji.\nX-mesh: Towards fast and accurate text-driven 3d styliza-\ntion via dynamic textual guidance.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 2749\u20132760, 2023. 3\n[26] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 2\n[27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[28] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 7\n[29] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 2\n[30] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman,\nJonathan T Barron, Amit H Bermano, Eric Ryan Chan, Tali\nDekel, Aleksander Holynski, Angjoo Kanazawa, et al. State\nof the art on diffusion models for visual computing. arXiv\npreprint arXiv:2310.07204, 2023. 2\n[31] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 6\n[32] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2, 3, 6, 7, 8\n[33] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July\n2017. 3\n[34] Aditya Ramesh,\nMikhail Pavlov,\nGabriel Goh,\nScott\nGray,\nChelsea Voss,\nAlec Radford,\nMark Chen, and\nIlya Sutskever.\nZero-shot text-to-image generation.\nIn\nInternational Conference on Machine Learning, pages 8821\u2013\n8831. PMLR, 2021. 2\n[35] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. arXiv preprint arXiv:2302.01721, 2023. 3\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 4, 6\n[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2, 7\n[38] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\n10\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022. 2\n[39] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler.\nDeep marching tetrahedra: a hybrid repre-\nsentation for high-resolution 3d shape synthesis. Advances\nin Neural Information Processing Systems, 34:6087\u20136101,\n2021. 3\n[40] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 2, 4, 5, 6,\n7, 8\n[41] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3d neural field generation\nusing triplane diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 20875\u201320886, 2023. 2\n[42] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 2\n[43] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 2, 6, 7, 8\n[44] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 7\n[45] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, et al. Rodin: A generative model for\nsculpting 3d digital avatars using diffusion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4563\u20134573, 2023. 4\n[46] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. arXiv\npreprint arXiv:2210.04628, 2022. 2\n[47] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.\nVolume rendering of neural implicit surfaces.\nAdvances\nin Neural Information Processing Systems, 34:4805\u20134815,\n2021. 7\n[48] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022. 2\n[49] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3dshape2vecset: A 3d shape representation for neu-\nral fields and generative diffusion models.\narXiv preprint\narXiv:2301.11445, 2023. 2\n[50] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang,\nPei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua\nGao. Michelangelo: Conditional 3d shape generation based\non shape-image-text aligned latent representation.\narXiv\npreprint arXiv:2306.17115, 2023. 2\n[51] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12588\u201312597, 2023.\n2\n[52] Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang,\nYing Shan, and Song-Hai Zhang.\nSparse3d:\nDistill-\ning multiview-consistent diffusion for object reconstruction\nfrom sparse views. arXiv preprint arXiv:2308.14078, 2023.\n3\n11\n"
  }
]