[
  {
    "title": "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation",
    "link": "https://arxiv.org/pdf/2401.04468.pdf",
    "upvote": "45",
    "text": "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation\nWeimin Wang*\nJiawei Liu*\nZhijie Lin\nJiangqiao Yan\nShuo Chen\nChetwin Low\nTuyen Hoang\nJie Wu\nJun Hao Liew\nHanshu Yan\nDaquan Zhou\nJiashi Feng\nBytedance Inc.\nhttps://magicvideov2.github.io/\nAbstract\nThe growing demand for high-fidelity video generation\nfrom textual descriptions has catalyzed significant research\nin this field. In this work, we introduce MagicVideo-V2 that\nintegrates the text-to-image model, video motion generator,\nreference image embedding module and frame interpolation\nmodule into an end-to-end video generation pipeline. Ben-\nefiting from these architecture designs, MagicVideo-V2 can\ngenerate an aesthetically pleasing, high-resolution video\nwith remarkable fidelity and smoothness. It demonstrates\nsuperior performance over leading Text-to-Video systems\nsuch as Runway, Pika 1.0, Morph, Moon Valley and Stable\nVideo Diffusion model via user evaluation at large scale.\n1. Introduction\nThe proliferation of Text-to-Video (T2V) models has\nmarked a significant advancement [6, 9, 11, 15], driven\nby the recent diffusion-based models.\nIn this work, we\npropose MagicVideo-V2, a novel multi-stage T2V frame-\nwork that integrates Text-to-Image (T2I), Image-to-Video\n(I2V), Video-to-Video (V2V) and Video Frame Interpola-\ntion (VFI) modules into an end-to-end video generation\npipeline.\nThe T2I module sets the foundation by producing an\ninitial image from the text prompt, capturing the aesthetic\nessence of the input. Then the I2V module takes the image\nas input and outputs low-resolution keyframes of the gen-\nerated video. The subsequent V2V module increases the\nresolution of the keyframes and enhances their details. Fi-\nnally, the frame interpolation module adds smoothness to\nthe motion in the video.\n2. MagicVideo-V2\nThe proposed MagicVideo-V2 is a multi-stage end-to-\nend video generation pipeline capable of generating high-\n*Equal contribution.\naesthetic videos from textual description. It consists of the\nfollowing key modules:\n\u2022 Text-to-Image model that generates an aesthetic image\nwith high fidelity from the given text prompt.\n\u2022 Image-to-Video model that uses the text prompt and gen-\nerated image as conditions to generate keyframes.\n\u2022 Video to video model that refines and performs super-\nresolution on the keyframes to yield a high-resolution\nvideo.\n\u2022 Video Frame Interpolation model that interpolates\nframes between keyframes to smoothen the video motion\nand finally generates a high resolution, smooth, highly\naesthetic video.\nThe following subsections will explain each module in more\ndetails.\n2.1. The Text-to-Image Module\nThe T2I module takes a text prompt from users as input\nand generates a 1024 \u00d7 1024 image as the reference image\nfor video generation. The reference image helps describe\nthe video contents and the aesthetic style. The proposed\nMagicVideo-V2 is compatible with different T2I models.\nSpecifically, we use a internally developed diffusion-based\nT2I model in MagicVideo-V2 that could output high aes-\nthetic images.\n2.2. The Image-to-Video Module\nThe I2V module is built on a high-aesthetic SD1.5 [12]\nmodel, that leverages human feedback to improve model\ncapabilities in visual quality an content consistency. The\nI2V module inflates this high-aesthetic SD1.5 with a mo-\ntion module inspired by [10], both of which were trained on\ninternal datasets.\nThe I2V module is augmented with a reference image\nembedding module for utilizing the reference image. More\nspecifically, we adapt an appearance encoder to extract the\nreference image embeddings and inject them into the I2V\nmodule via a cross-attention mechanism. In this way, the\nimage prompt can be effectively decoupled from the text\narXiv:2401.04468v1  [cs.CV]  9 Jan 2024\nT2I\n\u201cA young, beautiful girl \nin pink dress is playing \npiano gracefully\u201d\n\u201c1024x1024\u201d\nI2V\n\u201c600x600x32\u201d\nV2V(SR)\n\u201c1048x1048x32\u201d\nInterpolation\n\u201c1048x1048x94\u201d\nUser Input\nT2I module\nMotion module\nU-Net base model\nReference image \nfeature extractors\nMotion module\nU-Net base model\nInterpolation module\nLatent noise Prior\nReference image \nfeature extractors\nFigure 1. Overview of MagicVideo-V2. The T2I module creates a 1024\u00d71024 image that encapsulates the described scene. Subsequently,\nthe I2V module animates this still image, generating a sequence of 600\u00d7600\u00d732 frames, with the latent noise prior ensuring continuity\nfrom the initial frame. The V2V module enhances these frames to a 1048\u00d71048 resolution while refining the video content. Finally, the\ninterpolation module extends the sequence to 94 frames, getting a 1048\u00d71048 resolution video that exhibits both high aesthetic quality\nand temporal smoothness.\nprompts and provide stronger image conditioning. In ad-\ndition, we employ a latent noise prior strategy to provide\nlayout condition in the starting noisy latents. The frames\nare initialized from standard Gaussian noise whose means\nhave shifted from zeros towards the value of reference im-\nage latent. With a proper noise prior trick, the image lay-\nout could be partially retained and the temporal coherence\nacross frames could also be improved. To further enhance\nlayout and spatial conditioning, we deploy a ControlNet\n[14] module to directly extract RGB information from the\nreference image and apply it to all frames. These techniques\nalign the the frames with the reference image well while al-\nlowing the model to generate clear motion.\nWe employ an image-video joint training strategy for\ntraining the I2V module, where the images are treated as\nsingle-frame videos. The motivation here for joint training\nis to leverage our internal image datasets of high quality and\naesthetics, to improve frame quality of generated videos.\nThe image dataset part also serves as a good compensation\nfor our video datasets that are lacking in diversity and vol-\nume.\n2.3. The Video-to-Video Module\nThe V2V module has a similar design as the I2V module.\nIt shares the same backbone and spatial layers as in I2V\nmodule. Its motion module is separately finetuned using a\nhigh-resolution video subset for video super-resolution.\nThe image apperance encoder and ControlNet module\nare also used here. This turns out to be crutial, as we are\ngenerating video frames at a much higher resolution. Lever-\naging the information from the reference image helps guide\nthe video diffusion steps by reducing structural errors and\nfailure rates. In addition, it could also enhance the details\ngenerated at the higher resolution.\n2.4. Video Frame Interpolation (VFI)\nThe VFI module uses an internally trained GAN based\nVFI model. It employs an Enhanced Deformable Separa-\nble Convolution (EDSC) head [7] paired with a VQ-GAN\nbased architecture, similar to the autoencoder model used\nin the research conducted by [8]. To further enhance its\nstability and smoothness, we used a pretrained lightweight\ninterpolation model proposed in [13].\n3. Experiment\n3.1. Human evaluations\nTo evaluate MagicVideo-V2, we engaged human evaluators\nto conduct comparative analyses with contemporary state-\nof-the-art T2V systems.\nA panel of 61 evaluators rated\n500 side-by-side comparisons between MagicVideo-V2 and\nan alternative T2V method. Each voter is presented with\na random pair of videos, including one of ours vs one of\nthe competitors, based on the same text prompt, for each\nMoonValley\nPika 1.0\nMorph\nGen-2\nSVD-XT\n67.20%\n20.36%\n12.44%\n68.76%\n14.95%\n16.29%\n67.69%\n20.16%\n12.15%\n56.52%\n20.97%\n22.51%\n51.95%\n26.08%\n21.97%\nMagicVideo-V2\nNo preference\nOther\nFigure 2.\nThe distribution of human evaluators\u2019 perferences,\nshowing a dominant inclination towards MagicVideo-V2 over\nother state-of-the-art T2V methods. Green, gray, and pink bars\nrepresent trials where MagicVideo-V2 was judged better, equiva-\nlent, or inferior, respectively.\nround of comparison. They were presented with three as-\nsessment options-Good, Same, or Bad-indicating a prefer-\nence for MagicVideo-V2, no preference, or a preference for\nthe competing T2V method, respectively. The voters are re-\nquested to cast their vote based on their overall preference\non three criteria: 1) which video has higher frame quality\nand overall visual appealing. 2) which video is more tem-\nporal consistent, with better motion range and motion va-\nlidility. 3) which video has fewer structure errors, or bad\ncases. The compiled statistics of these trials can be found\nin Table 1, with the proportions of preferences depicted in\nFigure 2. The results demonstrate a clear preference for\nMagicVideo-V2, evidencing its superior performance from\nthe standpoint of human visual perception.\nMethod\nGood (G)\nSame (S)\nBad (B)\n(G+S)/(B+S)\nMoonValley [2]\n4099\n1242\n759\n2.67\nPika 1.0 [4]\n4263\n927\n1010\n2.68\nMorph [3]\n4129\n1230\n741\n2.72\nGen-2 [1]\n3448\n1279\n1373\n1.78\nSVD-XT [5]\n3169\n1591\n1340\n1.62\nTable 1. Human side-by-side evaluations comparing MagicVideo-\nV2 with other state-of-the-art text-to-video generation methods,\nindicating a strong preference for MagicVideo-V2.\n3.2. Qualitative examples\nSelected qualitative examples of MagicVideo-V2 are pre-\nsented in Figure 3.\nFor a better-viewed experience, we\ninvite readers to watch the accompanying videos on our\nPrompt: A large blob of exploding splashing rainbow paint, with an apple\nemerging, 8k.\nPrompt: An old-fashioned windmill surrounded by flowers, 3D design.\nPrompt: A girl with a hairband performing a song with her guitar on a\nwarm evening at a local market, children\u2019s story book.\nPrompt: A young, beautiful girl in a pink dress is playing piano gracefully.\nFigure 3. Examples of MagicVideo-V2 generated videos via a text\nprompt.\nproject website1.\nAs mentioned in Section 2, the I2V\nand V2V modules of MagicVideo-V2 excel at rectifying\nand refining imperfections from the T2I module, producing\nsmoothy and aesthetically pleasing videos. Select examples\nare showcased in Figure 4.\n4. Conclusion\nMagicVideo-V2 presents a new text-to-video generation\npipeline. Our comprehensive evaluations, underscored by\nhuman judgment, affirm that MagicVideo-V2 surpasses\nSOTA methods. The modular design of MagicVideo-V2,\nintegrating text-to-image, image-to-video, video-to-video\nand video frame interpolation, provides a new strategy for\ngenerating smooth and high-aesthetic videos.\nReferences\n[1] Gen-2. https://research.runwayml.com/gen2.\nAccessed: 2023-11-16. 3\n[2] MoonValley. https://https://moonvalley.ai/.\nAccessed: 2023-11-16. 3\n1https://magicvideov2.github.io/\nPrompt: \u201cA gray British Shorthair skateboarding in Times Square, in cubist painting style.\u201d The wrong dog generated from the T2I module is fixed by the\nI2V and V2V module.\nPrompt: \u201cIronman flying over a burning city, very detailed surroundings, cities are blazing, shiny iron man suit, realistic, 4k ultra high defi\u201d The ironman\u2019s\nredundant arm is removed by the I2V and V2V module.\nPrompt: \u201cA lone traveller walks in a misty forest.\u201d Left: low resolution video. Right: high resolution video. The tree details and scene brightness are refined\nby the V2V module.\nPrompt: \u201cA girl is writing something on a book. Oil painting style.\u201d Left: low resolution video. Right: high resolution video. The background and aesthetic\nsense are improved by the V2V module.\nFigure 4. Demonstrations of the I2V and V2V modules\u2019 capabilities to correct and refine outputs, leading to polished and visually appealing\nvideos.\n[3] Morph.\nhttps://www.morphstudio.com/.\nAc-\ncessed: 2023-11-16. 3\n[4] Pika 1.0. https://pika.art/. Accessed: 2023-12-26.\n3\n[5] SVD-XT.\nhttps : / / huggingface . co /\nstabilityai / stable - video - diffusion -\nimg2vid-xt. Accessed: 2023-11-27. 3\n[6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, Varun Jampani,\nand Robin Rombach. Stable video diffusion: Scaling latent\nvideo diffusion models to large datasets, 2023. 1\n[7] Xianhang Cheng and Zhenzhong Chen.\nMultiple video\nframe interpolation via enhanced deformable separable con-\nvolution, 2021. 2\n[8] Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video\nframe interpolation with latent diffusion models, 2023. 2\n[9] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi\nYin, Devi Parikh, and Ishan Misra. Emu video: Factoriz-\ning text-to-video generation by explicit image conditioning,\n2023. 1\n[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning, 2023. 1\n[11] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00b4e Lezama,\nJonathan Huang, Rachel Hornung, Hartwig Adam, Has-\nsan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng,\nMing-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta,\nMeera Hahn, Anja Hauth, David Hendon, Alonso Mar-\ntinez, David Minnen, David Ross, Grant Schindler, Mikhail\nSirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng\nWang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan\nSeybold, and Lu Jiang. Videopoet: A large language model\nfor zero-shot video generation, 2023. 1\n[12] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 1\n[13] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen,\nGangshan Wu, and Limin Wang. Extracting motion and ap-\npearance via inter-frame attention for efficient video frame\ninterpolation. In CVPR, 2023. 2\n[14] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2\n[15] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models, 2023. 1\n"
  },
  {
    "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
    "link": "https://arxiv.org/pdf/2401.04577.pdf",
    "upvote": "36",
    "text": "Published as a conference paper at ICLR 2024\nMASKED AUDIO GENERATION USING A SINGLE NON-\nAUTOREGRESSIVE TRANSFORMER\n\u2217Alon Ziv1,3, Itai Gat1, Gael Le Lan1, Tal Remez1, Felix Kreuk1, Alexandre D\u00b4efossez2\nJade Copet1, Gabriel Synnaeve1, Yossi Adi1,3\n1FAIR Team, Meta\n2Kyutai\n3The Hebrew University of Jerusalem\nalonzi@cs.huji.ac.il\nABSTRACT\nWe introduce MAGNET, a masked generative sequence modeling method that\noperates directly over several streams of audio tokens. Unlike prior work, MAG-\nNET is comprised of a single-stage, non-autoregressive transformer.\nDuring\ntraining, we predict spans of masked tokens obtained from a masking sched-\nuler, while during inference we gradually construct the output sequence using\nseveral decoding steps. To further enhance the quality of the generated audio,\nwe introduce a novel rescoring method in which, we leverage an external pre-\ntrained model to rescore and rank predictions from MAGNET, which will be\nthen used for later decoding steps. Lastly, we explore a hybrid version of MAG-\nNET, in which we fuse between autoregressive and non-autoregressive models\nto generate the first few seconds in an autoregressive manner while the rest of\nthe sequence is being decoded in parallel.\nWe demonstrate the efficiency of\nMAGNET for the task of text-to-music and text-to-audio generation and con-\nduct an extensive empirical evaluation, considering both objective metrics and\nhuman studies.\nThe proposed approach is comparable to the evaluated base-\nlines, while being significantly faster (x7 faster than the autoregressive base-\nline). Through ablation studies and analysis, we shed light on the importance\nof each of the components comprising MAGNET, together with pointing to the\ntrade-offs between autoregressive and non-autoregressive modeling, considering\nlatency, throughput, and generation quality. Samples are available on our demo\npage https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT\n1\nINTRODUCTION\nRecent developments in self-supervised representation learning (Hsu et al., 2021; D\u00b4efossez et al.,\n2022), sequence modeling (Touvron et al., 2023; Rozi`ere et al., 2023), and audio synthesis (Lee\net al., 2022; Polyak et al., 2021) allow a great leap in performance when considering high quality\nconditional audio generation. The prominent approach, in recent years, is to represent the audio\nsignals as a compressed representation, either discrete or continuous, and apply a generative model\non top of it (Lakhotia et al., 2021; Kharitonov et al., 2022; Borsos et al., 2023a; Kreuk et al., 2022a;\nCopet et al., 2023; Lam et al., 2023; Agostinelli et al., 2023; Gat et al., 2023; Sheffer & Adi, 2023;\nMaimon & Adi, 2022; Schneider et al., 2023; Huang et al., 2023b; Liu et al., 2023a; Li et al., 2023;\nLiu et al., 2023b). Recently, D\u00b4efossez et al. (2022); Zeghidour et al. (2021) proposed to apply a\nVQ-VAE directly on the raw waveform using residual vector quantization to obtain a multi-stream\ndiscrete representation of the audio signal. Later on, Kreuk et al. (2022a); Wang et al. (2023); Zhang\net al. (2023); Copet et al. (2023); Kreuk et al. (2022b) presented a conditional language modeling\non such audio signals representations. In parallel, Schneider et al. (2023); Huang et al. (2023b); Liu\net al. (2023a) proposed training a conditional diffusion-based generative model operating on learned\ncontinuous representations of the audio signal obtained from a pre-trained auto-encoder model.\n\u2217Work was done as part of Alon\u2019s internship at FAIR.\n1\narXiv:2401.04577v2  [cs.SD]  5 Mar 2024\nPublished as a conference paper at ICLR 2024\nOverall, the family of generative models explores in prior work can be roughly divided into two: (i)\nautoregressive (AR) in the form of language models (LMs), usually operating on discrete audio rep-\nresentations; and (ii) diffusion-based models usually operating on continuous latent representations\nof the audio signal. Although providing impressive generation results, these approaches have several\nmain drawbacks. Due to its autoregressive nature, following the LM approach yields relatively high\ninference time which turns into high latency values, hence making it less appalling for interactive\napplications such as music generation and editing under Digital Audio Workstations (DAW). On the\nother hand, diffusion models perform parallel decoding, however, to reach high-quality music sam-\nples recent studies report using a few hundred diffusion decoding steps (Huang et al., 2023a; Liu\net al., 2023b). Moreover, diffusion models struggle with generating long-form sequences. Recent\nstudies present results for either 10-second generations (Liu et al., 2023b; Li et al., 2023; Yang et al.,\n2022) or models that operate in low resolution followed by a cascade of super-resolution models to\nreach 30-second segments (Huang et al., 2023a).\nIn this work, we present MAGNET, a short for Masked Audio Generation using Non-autoregressive\nTransformers. MAGNET is a novel masked generative sequence modeling operating on a multi-\nstream representation of an audio signal. The proposed approach comprised of a single transformer\nmodel, working in a non-autoregressive fashion. During training, we first sample a masking rate\nfrom the masking scheduler, then, we mask and predict spans of input tokens conditioned on un-\nmasked ones. During inference, we gradually build the output audio sequence using several de-\ncoding steps. We start from a fully masked sequence and at each iteration step, we fix the most\nprobable token spans, i.e., the spans that got the top confidence score. To further enhance the quality\nof the generated audio, we introduce a novel rescoring method. In which, we leverage an external\npre-trained model to rescore and rank predictions from MAGNET. Lastly, we explore a Hybrid\nversion of MAGNET, in which we fuse autoregressive and non-autoregressive models. The hybrid-\nMAGNET generates the beginning of the tokens sequence in an autoregressive manner while the\nrest of the sequence is being decoded in parallel, similarly to the original MAGNET. A visual de-\nscription of the inference of the proposed method can be seen in Fig. 1.\nSimilar non-autoregressive modeling was previously proposed by Ghazvininejad et al. (2019) for\nmachine translation, Chang et al. (2022) for class-conditional image generation and editing, and\nChang et al. (2023) for image generation guided by rich textual description followed by a super-\nresolution component. Borsos et al. (2023b) recently proposed SoundStorm, a non-autoregressive\nmethod for the task of text-to-speech and dialogue synthesis. SoundStorm is conditioned on \u201cseman-\ntic\u201d tokens obtained from an autoregressive model. Unlike, SoundStorm, MAGNET is composed of\na single non-autoregressive model and was evaluated on music and audio generation which, unlike\nspeech, leverages the full frequency spectrum of the signal.\nWe evaluate the proposed approach considering both text-to-music and text-to-audio generation. We\nreport objective metrics together with a human study and show the proposed approach achieves com-\nparable results to the evaluated baselines while having significantly reduced latency (x7 faster than\nthe autoregressive-based method). We further present an analysis of the proposed method consid-\nering latency, throughput, and generation quality. We present the trade-offs between the two when\nconsidering autoregressive and non-autoregressive models. Lastly, we provide an ablation study that\nsheds light on the contribution of each component of the proposed approach to the performance.\nOur contributions: (i) We present a novel non-autoregressive model for the task of audio modeling\nand generation, denoted as MAGNET. The proposed method is able to generate relatively long\nsequences (30 seconds long), using a single model. The proposed approach has a significantly\nfaster inference time while reaching comparable results to the autoregressive alternative; (ii)\nWe leverage an external pre-trained model during inference to improve generation quality via a\nrescoring method; and (iii) We show how the proposed method can be combined with autoregressive\nmodeling to reach a single model that performs joint optimization, denoted as Hybrid-MAGNET.\n2\nBACKGROUND\nAudio representation. Modern audio generative models mostly operate on a latent representation\nof the audio, commonly obtained from a compression model (Borsos et al., 2023a; Kreuk et al.,\n2022a; Yang et al., 2022). Compression models such as Zeghidour et al. (2021) employ Residual\nVector Quantization (RVQ) which results in several parallel streams. Under this setting, each stream\n2\nPublished as a conference paper at ICLR 2024\nis comprised of discrete tokens originating from different learned codebooks. Prior work, proposed\nseveral modeling strategies to handle this issue (Kharitonov et al., 2022; Wang et al., 2023).\nSpecifically, D\u00b4efossez et al. (2022) introduced EnCodec , a convolutional auto-encoder with a latent\nspace quantized using Residual Vector Quantization (RVQ) (Zeghidour et al., 2021), and an adver-\nsarial reconstruction loss. Given a reference audio signal x \u2208 Rd\u00b7fs with d the audio duration and\nfs the sample rate, EnCodec first encodes it into a continuous tensor with a frame rate fr \u226a fs.\nThen, this representation is quantized into z \u2208 {1, . . . , N}K\u00d7d\u00b7fr, with K being the number of\ncodebooks used in RVQ and N being the codebook size. Notice, after quantization we are left with\nK discrete token sequences, each of length T = d \u00b7 fr, representing the audio signal. In RVQ, each\nquantizer encodes the quantization error left by the previous quantizer, thus quantized values for\ndifferent codebooks are in general dependent, where the first codebook is the most important one.\nAudio generative modeling. Given a discrete representation of the audio signal, z, our goal is to\nmodel the conditional joint probability distribution p\u03b8(z|y), where y is a semantic representation of\nthe condition. Under the autoregressive setup we usually follow the chain rule of probability, thus\nthe joint probability of a sequence can be computed as a product of its conditional probabilities:\np\u03b8(z1, . . . , zn|y) =\nn\nY\ni=1\np\u03b8(zi|zi\u22121, . . . , z1, y).\n(1)\nThe above probability chain rule can be thought of as a masking strategy, where, in each time step i,\nwe predict the probability of the i-th token, given its past tokens, while we mask future tokens. For\nthat, we define a masking function m(i), that mask out all tokens larger than i, which results in:\np\u03b8(z1, . . . , zn|y) =\nn\nY\ni=1\np\u03b8(zi|(1 \u2212 m(i)) \u2299 z, y),\n(2)\nwhere each element in m(i) = [m1(i), . . . , mT (i)] is defined as mj(i) = 1 [j \u2265 i] . Notice, Eq. (2)\ndoes not hold for any masking strategy. One should pick a masking strategy that satisfies the proba-\nbility chain rule.\nExtending Eq. (2) to the non-autoregressive setup can be done by modifying the masking strategy\nand the decomposition of the joint probability to predict an arbitrary subset of tokens given the\nunmasked ones using several decoding steps. Let us formally define the masking strategy as follows,\nmj(i) \u223c 1 [j \u2208 Mi]\nwhere\nMi \u223c U\n\u0000{A \u2286 Mi\u22121 : |A| = \u03b3(i; s) \u00b7 T}\n\u0001\n,\n(3)\nand \u03b3 is the masking scheduler, with s decoding steps, defined as \u03b3(i; s) = cos( \u03c0(i\u22121)\n2s\n) and M0 =\n{1, . . . , T}. In other words, at each time step i we mask a subset of \u03b3(i; s) \u00b7 T tokens sampled from\nthe masked set at the previous time step. Thus the modified version of Eq. (2) is,\np\u03b8(z1, . . . , zn|y) =\ns\nY\ni=1\np\u03b8(m(i) \u2299 z|(1 \u2212 m(i)) \u2299 z, y).\n(4)\nIn practice, during training, a decoding time step i \u2208 [1, s] and the tokens to be masked from M0\nare randomly sampled. The tokens at indices t \u2208 Mi are then replaced by a special mask token,\nand the model is trained to predict the target tokens at the masked positions Mi given the unmasked\ntokens. This modeling paradigm was previously explored by Ghazvininejad et al. (2019); Chang\net al. (2022; 2023); Borsos et al. (2023b).\nRecall, the audio representation is composed of multi-stream sequences created by RVQ. In which,\nthe first codebook encodes the coarse information of the signal while later codebooks encode the\nquantization error to refine the generation quality. To handle that, Borsos et al. (2023b) proposed to\npredict tokens from codebook k given its preceding codebooks. During training, a codebook level\nk, is being uniformly sampled from {1, . . . , K}. Then, we mask and predict the tokens of the k-th\ncodebook given previous levels via teacher forcing. At inference, we sequentially generate the token\nstreams, where each codebook is being generated conditioned on previously generated codebooks.\n3\nMETHOD\nFollowing the approach presented in the previous section solely does not lead to high-quality audio\ngeneration. We hypothesize this is due to three factors: (i) The masking strategy operates over\n3\nPublished as a conference paper at ICLR 2024\n13\n993\n2\n2\n422\n1\nRestricted\nTransformer\nCross Attention\n\u05f4Classic reggae track\nwith an electronic \nguitar solo\u05f4\nText Embeddings\nT5\nCandidate\nRVQ Tokens\nMasked\nRVQ Tokens\nRescoring\n13\n993 129 411 422 83\n13\n993\n422\nRescoring\n...\n...\nDecoding Iteration 1\nDecoding Iteration 2\nDecoding Iteration s\nEnCodec\n251 888 12\n33\n1009 788 22\n76\n892 37\n32\n55\n20\n249 591 992\n13\n993 129 323 422 83\n251 888 12\n732 222 33\n1009 788 22\n76\n892 37\n32\n55\n20\n249 591 992\n13\n993 129 323 422 83\nSpan\nMasking\nRestricted\nTransformer\nRestricted\nTransformer\nSpan\nMasking\n13\n993 129 323 422 83\n13\n993\n422\nRescoring\nRestricted\nTransformer\nSpan\nMasking\n13\n993\n422\nRescoring\nRestricted\nTransformer\nSpan\nMasking\n13\n993 129 323 422 83\nRescoring\nRestricted\nTransformer\nSpan\nMasking\n32\n55\n16\n119 591 10\n13\n993 129 323 422 83\nDecoding Iteration 3\nFigure 1: Inference of MAGNET model. During each iteration, we mask a subset of token spans\n(starting from a fully masked sequence). Next, we rescore the tokens based on an external pre-\ntrained model. Finally, we select the token spans to be re-masked for the next decoding iteration.\nindividual tokens that share information with adjacent tokens. Hence, allowing the model to \u201ccheat\u201d\nduring tokens prediction while being trained using teacher forcing; (ii) The temporal context of the\ncodebooks at levels greater than one, is generally local and influenced by a small set of neighboring\ntokens. This affects model optimization; (iii) Sampling from the model at different decoding steps\nrequires different levels of diversity with respect to the condition. Also sampling can be combined\nwith external scoring models.\nIn this section, we present MAGNET in details. MAGNET consists of a non-autoregressive audio-\nbased generative masked language model, conditioned on a semantic representation of the condition,\noperating on several streams of discrete audio tokens obtained from EnCodec . We follow a similar\nmodeling strategy as presented in Section 2 while introducing core modeling modifications consist-\ning of masking strategy, restricted context, sampling mechanism, and model rescoring.\n3.1\nMASKING STRATEGY\nAdjacent audio tokens often share information due to the receptive field of the audio encoder. Hence,\nwe use spans of tokens as the atomic building block of our masking scheme, rather than individual\nones as done in prior work. We evaluated various span lengths l between 20ms to 200ms and found\na 60ms span length to give the best overall performance (see Section 5.3 for detailed results). We\nsample a masking rate \u03b3(i), from the scheduler, and compute the average amount of spans to be\nmasked accordingly. As spans may overlap, this process requires a careful design. We select the\nnumber of spans u, that satisfies 1 \u2212\n\u0000T \u2212l\nu\n\u0001\n/\n\u0000T\nu\n\u0001\n\u2248 \u03b3(i), where l is the span length. The above\nexpression is the expected masking rate over all possible placements of u spans of length l over\nthe sequence. Full derivation can be found in the Appendix C. During inference time, we follow a\nsimilar strategy, in which we re-mask the least probable spans instead of individual tokens as done\nin prior work. We consider the span\u2019s probability as the token with the maximal probability. For\ncomputational efficiency, we use non-overlapping spans.\n3.2\nRESTRICTED CONTEXT\nRecall, the used audio tokenizer is based on RVQ, where each quantizer encodes the quantization\nerror left by the previous quantizer. Thus quantized codebooks later than the first one heavily de-\npend on previous codebooks rather than surrounding tokens. To leverage that we analyze the used\nEnCodec and restrict the context of the codebooks accordingly.\nSpecifically, the audio encoder consists of a multi-layer convolutional network and a final LSTM\nblock. Analyzing the receptive field for the used EnCodec shows that the receptive field of the con-\nvolutional network is \u223c 160ms, while the effective receptive field when including the LSTM block\nis \u223c 180ms. We empirically estimate the receptive field of the model, using a shifted impulse func-\ntion over time while measuring the magnitude of the encoded vector in the middle of the sequence.\n4\nPublished as a conference paper at ICLR 2024\nFig. 3 in Appendix G depicts such process. Notice, although theoretically, the LSTM has an infinite\nmemory, practically we observe it is bounded.\nWe utilize this observation to improve model optimization, by restricting the self-attention of code-\nbooks greater than 1, to attend only on tokens at a temporal distance smaller than \u223c 200ms. Similar\nideas were proposed in the context of language modeling by Rae & Razavi (2020); Roy et al. (2021).\nWe depict the used attention map for the restricted context in Fig. 8.\n3.3\nMODEL INFERENCE\nSampling as described in Eq. (3) uses a uniform sampling to choose spans from the previously set\nof masked spans. In practice, we use the model confidence at the i-th iteration as a scoring function\nto rank all possible spans and choose the least probable spans to be masked accordingly. However,\nthe scoring function does not have to be part of the generative model.\nA common practice in Automatic Speech Recognition (ASR) decoding, is to generate a set of dif-\nferent hypotheses from one model and rescore them using another model (Benesty et al., 2008;\nLikhomanenko et al., 2020). Inspired by the ASR rescoring method, we propose a novel strategy in\nwhich at iteration i we generate a candidate token sequence using MAGNET. Then, we feed it to\nan external model and get a new set of probabilities for each of the token spans. Lastly, we use a\nconvex combination of both probabilities (the one emitted by MAGNET and the one obtained from\nthe rescorer model), to sample from:\np(z|y) = w \u00b7 p\u03b8(z|y) + (1 \u2212 w) \u00b7 prescorer(z|y).\n(5)\nIn this work, we use MUSICGEN and AudioGen as our rescorering models (in a non-autoregressive\nmanner). The proposed rescoring method is generic and is not tied to any specific rescoring model.\nFollowing the proposed approach improves the generated audio quality and stabilizes inference. A\npseudo-code of our entire decoding algorithm is described in Fig. 4, Appendix D.\nClassifier-free guidance annealing. Token prediction is done using a Classifier-Free Guidance\n(CFG) (Ho & Salimans, 2022). During training, we optimize the model both conditionally and\nunconditionally, while at inference time we sample from a distribution obtained by a linear combi-\nnation of the conditional and unconditional probabilities.\nWhile prior work (Copet et al., 2023; Kreuk et al., 2022a) used a fixed guidance coefficient, \u03bb > 1,\nwe instead use a CFG annealing mechanism controlled by the masking schedule \u03b3. As the masking\nrate \u03b3(i) decreases, the guidance coefficient is annealed during the iterative decoding process. The\nmotivation behind this approach is to gradually reduce text adherence and guide the generation\nprocess toward the already fixed tokens. Intuitively, this transforms the sampling process from\ntextually guided to contextual infilling. Formally, we use a CFG coefficient of\n\u03bb(i) = \u03b3(i) \u00b7 \u03bb0 + (1 \u2212 \u03b3(i)) \u00b7 \u03bb1,\n(6)\nwhere \u03bb0 and \u03bb1 are the initial and final guidance coefficients respectively. This approach was also\nfound to be beneficial in 3D shape generation (Sanghi et al., 2023).\n4\nEXPERIMENTAL SETUP\nImplementation details. We evaluate MAGNET on the task of text-to-music generation and text-\nto-audio generation. We use the exact same training data as using by Copet et al. (2023) for music\ngeneration and by Kreuk et al. (2022a) for audio generation. A detailed description of the used\ndataset can be found on Appendix A.2. We additionally provide a detailed description about the\ndatasets used to train the evaluated baselines in Table 4.\nUnder all setups, we use the official EnCodec model as was published by Copet et al. (2023); Kreuk\net al. (2022a)1. The model gets as input an audio segment and outputs a 50 Hz discrete represen-\ntation. We use four codebooks, where each has a codebook size of 2048. We perform the same\ntext preprocessing as proposed by Copet et al. (2023); Kreuk et al. (2022a). We use a pre-trained\nT5 Raffel et al. (2020) model to extract semantic representation from the text description and use it\nas model conditioning.\n1https://github.com/facebookresearch/audiocraft\n5\nPublished as a conference paper at ICLR 2024\nTable 1: Comparison to Text-to-Music Baselines. The Mousai and MusicGen models were retrained\non the same dataset, while for MusicLM we use the public API for human studies. We report the\noriginal FAD for AudioLDM2, and MusicLM. For human studies, we report mean and CI95.\nMODEL\nFADvgg \u2193\nKL \u2193\nCLAPscr \u2191\nOVL. \u2191\nREL. \u2191\n# STEPS\nLATENCY (S)\nReference\n-\n-\n-\n92.69\u00b10.89\n93.97\u00b10.82\n-\n-\nMousai\n7.5\n1.59\n0.23\n73.97\u00b11.93\n74.12\u00b11.43\n200\n44.0\nMusicLM\n4.0\n-\n-\n84.03\u00b11.28\n85.57\u00b11.12\n-\n-\nAudioLDM 2\n3.1\n1.20\n0.31\n77.69\u00b11.93\n82.41\u00b11.36\n208\n18.1\nMUSICGEN-small\n3.1\n1.29\n0.31\n84.68\u00b11.45\n83.89\u00b11.01\n1500\n17.6\nMUSICGEN-large\n3.4\n1.23\n0.32\n85.65\u00b11.51\n84.12\u00b11.12\n1500\n41.3\nMAGNET-small\n3.3\n1.12\n0.31\n81.67\u00b11.72\n83.21\u00b11.17\n180\n4.0\nMAGNET-large\n4.0\n1.15\n0.29\n84.26\u00b11.43\n84.21\u00b11.34\n180\n12.6\nWe train non-autoregressive transformer models using 300M (MAGNET-small) and 1.5B\n(MAGNET-large) parameters. We train models using 30-second audio crops sampled at random\nfrom the full track. We train the models for 1M steps with the AdamW optimizer (Loshchilov &\nHutter, 2017), a batch size of 192 examples, \u03b21 = 0.9, \u03b22 = 0.95, a decoupled weight decay of 0.1\nand gradient clipping of 1.0. We further rely on D-Adaptation-based automatic step-sizes (Defazio\n& Mishchenko, 2023). We use a cosine learning rate schedule with a warmup of 4K steps. Addi-\ntionally, we use an exponential moving average with a decay of 0.99. We train the models using\nrespectively 32 GPUs for small and 64 GPUs for large models, with float16 precision. For compu-\ntational efficiency, we train 10-second generation models with a batch size of 576 examples for all\nablation studies. Finally, for inference, we employ nucleus sampling (Holtzman et al., 2020) with\ntop-p 0.9, and a temperature of 3.0 that is linearly annealed to zero during decoding iterations. We\nuse CFG with a condition dropout of 0.3 at training, and a guidance coefficient 10.0 annealed to 1.0.\nEvaluation metrics. We evaluate the proposed method using the same setup as proposed by Copet\net al. (2023); Kreuk et al. (2022a), which consists of both objective and subjective metrics. For the\nobjective metrics, we use: the Fr\u00b4echet Audio Distance (FAD), the Kullback-Leiber Divergence (KL),\nand the CLAP score (CLAP). We report the FAD (Kilgour et al., 2018) using the official implemen-\ntation in Tensorflow with the VGGish model 2. Following Kreuk et al. (2022a), we use a state-of-\nthe-art audio classifier (Koutini et al., 2021) to compute the KL-divergence over the probabilities\nof the labels between the original and the generated audio. We also report the CLAP score (Wu\net al., 2023; Huang et al., 2023b) between the track description and the generated audio to quantify\naudio-text alignment, using the official CLAP model 3.\nFor the human studies, we follow the same setup as in Kreuk et al. (2022a). We ask human raters\nto evaluate two aspects of the audio samples (i) overall quality (OVL), and (ii) relevance to the text\ninput (REL). For the overall quality test, raters were asked to rate the perceptual quality of the pro-\nvided samples in a range of 1 to 100. For the text relevance test, raters were asked to rate the match\nbetween audio and text on a scale of 1 to 100. Raters were recruited using the Amazon Mechanical\nTurk platform. We evaluate randomly sampled files, where each sample was evaluated by at least\n5 raters. We use the CrowdMOS package4 to filter noisy annotations and outliers. We remove\nannotators who did not listen to the full recordings, annotators who rate the reference recordings\nless than 85, and the rest of the recommended recipes from CrowdMOS (Ribeiro et al., 2011).\n5\nRESULTS\n5.1\nTEXT-TO-MUSIC GENERATION\nWe compare MAGNET to Mousai (Schneider et al., 2023), MusicGen Copet et al. (2023), Audi-\noLDM2 Liu et al. (2023b) 5, and MusicLM (Agostinelli et al., 2023). We train Mousai using our\ndataset using the open source implementation provided by the authors6.\n2github.com/google-research/google-research/tree/master/frechet audio distance\n3https://github.com/LAION-AI/CLAP\n4http://www.crowdmos.org/download/\n5huggingface.co/spaces/haoheliu/audioldm2-text2audio-text2music (September 2023)\n6Implementation from github.com/archinetai/audio-diffusion-pytorch (March 2023)\n6\nPublished as a conference paper at ICLR 2024\n2\n4\n8\n16\n32\n64\n128\n256\nBatch size\n0\n10\n20\n30\n40\n50\n60\n70\nLatency (s)\n6.18\n0.6\n24.92\n46.11\nAutoregressive\nNon-autoregressive\n(a) Latency\n2\n4\n8\n16\n32\n64\n128\n256\nBatch size\n0\n1\n2\n3\n4\n5\nThroughput (samples/s)\n5.14\n2.78\nAutoregressive\nNon-autoregressive\n(b) Throughput\n2\n4\n6\n8\n10\n12\n14\n16\n18\nLatency (s)\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nFAD\n0\n1\n2\n3\n4 5\nAR prompt\nAR\n(c) Latency/FAD trade-off\nFigure 2: Latency and throughput analysis: MAGNET is particularly suited to small batch sizes (up\nto 10 times lower latency than MUSICGEN), while MUSICGEN benefits from a higher throughput for\nbigger batch sizes. MAGNET offers flexibility regarding the latency/quality trade off by allowing a\ncustomizable decoding schedule or following the Hybrid-MAGNET variant.\nTable 1 presents the results of MAGNET on the task of text-to-music generation compared to var-\nious baselines. Results are reported on the MusicCaps benchmark. As can be seen, MAGNET\nreaches comparable performance to MusicGen, which performs autoregressive modeling, while be-\ning significantly faster both in terms of latency and decoding steps. When comparing to Audi-\noLDM2, which is based on latent diffusion, MAGNET gets worse FAD and CLAP scores, while\nreaching better KL subjective scores. Notice, AudioLDM2 was trained on 10-seconds generation\nat 16kHz while MAGNET was trained on 30-seconds generation at 32kHz. When we reduce the\nsequence length to 10-second generations our FAD reaches to 2.9 and CLAP score of 0.31. We\nadditionally evaluate MAGNET on the task of text-to-audio generation (environmental sound gen-\neration). Results and details regarding the baselines can be found in Appendix G. Results show\nsimilar trends as on text-to-music of MAGNET providing comparable performance to the autore-\ngressive baseline while being significantly faster.\n5.2\nANALYSIS\nLatency vs. Throughput. We analyze the trade-offs between latency and throughput as a function\nof the batch size, as illustrated in Fig. 2a and Fig. 2b. Latency and throughput are reported for\ngenerated samples of 10-second duration on an A100 GPU with 40 GB of RAM. Due to CFG,\nthe batch size value is typically twice the generated sample count. Indeed the model outputs two\ndistributions in parallel: one conditioned on the text prompt and the other unconditioned.\nCompared with the baseline autoregressive model (red curve), the non-autoregressive model (dashed\nblue curve) especially excels on small batch sizes due to parallel decoding, with a latency as low\nas 600 ms for a single generated sample (batch size of two in the 2a), more than 10 times faster\nthan the autoregressive baseline. This is especially interesting in interactive applications that require\nlow-latency. The non-autoregressive model is faster than the baseline up to a batch size of 64.\nHowever, in scenarios where throughput is a priority (e.g. generate as many samples as possible,\nirrespective of the latency), we show that the autoregressive model is favorable. While the non-\nautoregressive model throughput is bounded to \u223c 2.8 samples/second for batch sizes bigger than\n64, the autoregressive model throughput is linear in batch size, only limited by the GPU memory.\nHybrid-MAGNET. Next, we demonstrate how a hybrid version can also be combined. We boot-\nstrap the non-autoregressive generation with an autoregressive-generated audio prompt. We train\na single model that incorporates both decoding strategies. During training, we sample a time step\nt \u2208 {1, . . . , T} and compute the autoregressive training loss for all positions that precede t. The\nrest of the sequence is being optimized using the MAGNET objective. This is done by designing a\ncustom attention mask that simulates the inference behavior during training (causal attention before\nt, parallel attention after t). During inference, the model can be used autoregressively to generate a\nshort audio prompt and switch to non-autoregressive decoding to complete the generation faster. A\ndetailed description of the hybrid training can be found on Appendix E.\nWe analyze the effect of the chosen t in Fig. 2c using a 30-second generations without rescor-\ning. Starting from a fully non-autoregressive generation decoding, we ablate on the autoregressive-\ngenerated prompt duration. The results indicate that the longer the prompt, the lower the FAD.\nThe Hybrid-MAGNET is even able to outperform the full autoregressive baseline (when consid-\n7\nPublished as a conference paper at ICLR 2024\nTable 2: Span length and restricted context ablation. We report FAD scores for MAGNET using an\nIn-domain test set considering different span lengths, with and without temporally restricted context.\nSpan-length\n1\n2\n3\n4\n5\n10\nRestricted context\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\nFAD\n3.07 2.13 0.74 0.66 0.82 0.61 0.97 0.63 1.13 0.84 1.66 1.05\nTable 3: We evaluate the effect of the rescorer on model performance. We report mean and CI95.\nMODEL\nRE-SCORE FADvgg \u2193 KL \u2193 CLAPscr \u2191\nOVL. \u2191\nREL. \u2191\nLATENCY (S)\nMAGNET-small\n\u2717\n3.7\n1.18\n0.30\n80.65\u00b11.48 81.06\u00b11.19\n2.5\nMAGNET-small\n\u2713\n3.3\n1.12\n0.31\n83.31\u00b11.11 84.53\u00b11.11\n4.0\nMAGNET-large\n\u2717\n4.2\n1.19\n0.30\n82.19\u00b11.23 82.96\u00b10.91\n8.2\nMAGNET-large\n\u2713\n4.0\n1.15\n0.29\n84.43\u00b11.24 85.57\u00b11.04\n12.6\nering FAD), starting from a 1-second prompt while still being significantly faster (3.2s of latency\ndown from 17.6s). This Hybrid strategy is another way to control quality/latency trade-offs when\nperformance of the non-autoregressive model does not quite match its autoregressive counterpart.\n5.3\nABLATION\nThe effect of modeling choices. To validate our findings regarding the necessity of span masking\nfor audio modeling, as well as the necessity of temporal context restriction for efficient optimization,\nwe train different model configurations and report the resulting FAD in Table 2. Results suggest that\nusing restricted context consistently improves model performance across all settings. Moreover,\nusing a span-length of 3, which corresponds to spans of 60ms yields the best performance.\nThe effect of CFG annealing. Table 6 in the Appendix presents results computed over in-domain\nsamples using several CFG coefficient configurations. We evaluate both constant CFG schedules,\ne.g. by setting \u03bb0 = \u03bb1 = 3, and annealing CFG. Results suggest that using \u03bb0 = 10, \u03bb1 = 1 yields\nthe best FAD score over all evaluated setups. This finding aligns with our hypothesis that during\nthe first decoding steps a stronger text adherence is required, while at later decoding steps we would\nlike the model to focus on previously decoded tokens.\nThe effect model rescorer. Next, we evaluate the effect of model rescorering on the overall perfor-\nmance. Results are presented in Table 3. Results suggest that applying model rescoring improves\nperformance for almost all metrics. However, this comes at the expense of slower inference time.\nThe effect of decoding steps. We explore the effect of less decoding steps on the overall latency\nand performance, see Fig. 7. It seems that reducing the decoding steps for higher levels does not\nimpact quality as much as for the first level. For scenarios where minimizing the latency is the top\npriority, one should consider only 1 step per higher codebook level: in such case, latency drops to\n370 ms, at the expense of a 8% increase of FAD compared to 10 steps per higher levels.\nDecoding visualization. We visualize the masking dynamics of MAGNET\u2019s iterative decoding\nprocess.\nIn specific, we plot the mask m(i) chosen by MAGNET during the generation of a\n10-second audio sample, for each iteration i \u2208 {1, . . . , 20}. As can be seen, MAGNET decodes\nthe audio sequence in a non-causal manner, choosing first a sparse set of token spans at various\ndisconnected temporal locations, and gradually \u201cinpaint\u201d the gaps until convergence to a full token\nsequence. Visualization and full details can be found in Appendix F.\n6\nRELATED WORK\nAutoregressive audio generation.\nRecent studies considering text-to-audio generation can be\nroughly divided into two: (i) environmental sounds generation; and (ii) music generation. As for\nenvironmental sound generation, Kreuk et al. (2022a) proposed applying a transformer language\nmodel over discrete audio representation, obtained by quantizing directly time-domain signals using\nEnCodec D\u00b4efossez et al. (2022). Sheffer & Adi (2023) followed a similar approach to Kreuk et al.\n(2022a) for image-to-audio generation. Dhariwal et al. (2020) proposed representing music samples\n8\nPublished as a conference paper at ICLR 2024\nin multiple streams of discrete representations using a hierarchical VQ-VAE. Next, two sparse trans-\nformers applied over the sequences to generate music. Gan et al. (2020) proposed generating music\nfor a given video, while predicting its midi notes. Recently, Agostinelli et al. (2023) proposed a\nsimilar approach to AudioLM (Borsos et al., 2023a), which represents music using multiple streams\nof \u201csemantic tokens\u201d and \u201cacoustic tokens\u201d. Then, they applied a cascade of transformer decoders\nconditioned on a textual-music joint representation (Huang et al., 2022). Donahue et al. (2023) fol-\nlowed a similar modeling approach, but for the task of singing-to-accompaniment generation. Copet\net al. (2023) proposed a single stage Transformer-based autoregressive model for music generation,\nconditioned on either text or melodic features, based on EnCodec .\nNon-autoregressive audio generation. The most common approach for non-autoregressive gener-\nation is diffusion models. These models naturally apply over continuous representations however\ncan also operate over discrete representations. Yang et al. (2022) proposed representing audio spec-\ntrograms using a VQ-VAE, then applying a discrete diffusion model conditioned on textual CLIP\nembeddings for the generation part Radford et al. (2021). Huang et al. (2023b); Liu et al. (2023a;b)\nproposed using latent diffusion models for the task of text-to-audio, while extending it to various\nother tasks such as inpainting, image-to-audio, etc. Schneider et al. (2023); Huang et al. (2023a);\nMaina (2023); Forsgren & Martiros (2022); Liu et al. (2023b) proposed using a latent diffusion\nmodel for the task of text-to-music. Schneider et al. (2023) proposed using diffusion models for\nboth audio encoder-decoder and latent generation. Huang et al. (2023a) proposed a cascade of diffu-\nsion model to generate audio and gradually increase its sampling rate. Forsgren & Martiros (2022)\nproposed fine-tuning Stable Diffusion (Rombach et al., 2022) using spectrograms to generate five-\nsecond segments, then, using image-to-image mapping and latent interpolation to generate long\nsequences. Li et al. (2023) present impressive generation results using a latent diffusion model with\na multi-task training objective, however for 10-second generation only.\nThe most relevant prior work to ours involves masked generative modeling. Ghazvininejad et al.\n(2019) first proposed the Mask-Predict method, a masked language modeling with parallel decoding\nfor the task of machine translation. Later on, Chang et al. (2022) followed a similar modeling strat-\negy, denoted as MaskGIT, for the task of class-conditioned image synthesis and image editing, while\nChang et al. (2023) extended this approach to high-quality textually guided image generation over\nlow-resolution images followed by a super-resolution module. Lezama et al. (2022) further proposed\nthe TokenCritic approach, which improves the sampling from the joint distribution of visual tokens\nover MaskGIT. Recently, Borsos et al. (2023b) proposed the SoundStorm model, which has a similar\nmodeling strategy as MaskGIT but for text-to-speech and dialogue synthesis. Unlike MaskGIT, the\nSoundStorm model is conditioned on semantic tokens obtained from an autoregressive model. The\nproposed work differs from this model as we propose a single non-autoregressive model, with a\nnovel audio-tokens modeling approach for the task of text-to-audio. Another concurrent work, is\nVampNet (Garcia et al., 2023), a non-autoregressive music generation model. Unlike MAGNET,\nVampNet is based on two different models (one to model the \u201ccoarse\u201d tokens and one to model the\n\u201cfine\u201d tokens), and do not explore text-to-music generation without audio-prompting.\n7\nDISCUSSION\nLimitations. As discussed in section 5.2, the proposed non-autoregressive architecture targets low-\nlatency scenarios. By design, the model re-encodes the whole sequence at each decoding step, even\nfor time steps that have not changed between two consecutive decoding steps. This is a fundamental\ndifference with autoregressive architectures that can benefit from caching past keys and values and\nonly encode one-time step per decoding step, which efficiently scales when the batch size increases.\nSuch a caching strategy could also be adopted for non-autoregressive architectures, for time steps\nthat do not change between consecutive decoding steps, however, this requires further research.\nConclusion. In this work, we presented MAGNET which, to the best of our knowledge, is the first\npure non-autoregressive method for text-conditioned audio generation. By using a single-stage en-\ncoder during training and a rescorer model, we achieve competitive performance with autoregressive\nmethods while being approximately 7 times faster. We also explore a hybrid approach that combines\nautoregressive and non-autoregressive models. Our extensive evaluation, including objective metrics\nand human studies, highlights MAGNET\u2019s promise for real-time audio generation with comparable\nor minor quality degradation. For future work, we intend to extend the research work on the model\nrescoring and advanced inference methods. We believe this research direction holds great potential\nin incorporating external scoring models which will allow better non-left-to-right model decoding.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS.\nThe authors would like to thank Or Tal, Michael Hassid and Nitay Arcusin for the useful theoret-\nical discussions. The authors would additionally like to thank Kamila Benzina and Nisha Deo for\nsupporting this project. This research work was supported in part by ISF grant 2049/22.\nREFERENCES\nAndrea Agostinelli, Timo I Denk, Zal\u00b4an Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating\nmusic from text. arXiv preprint arXiv:2301.11325, 2023.\nJ Benesty, J Chen, and Y Huang. Automatic speech recognition: A deep learning approach, 2008.\nZal\u00b4an Borsos, Rapha\u00a8el Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Shar-\nifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a\nlanguage modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 2023a.\nZal\u00b4an Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco\nTagliasacchi. Soundstorm: Efficient parallel audio generation. arXiv preprint arXiv:2305.09636,\n2023b.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative\nimage transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\neration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\nJade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexan-\ndre D\u00b4efossez. Simple and controllable music generation. arXiv preprint arXiv:2306.05284, 2023.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness. In Advances in Neural Information Process-\ning Systems, 2022.\nAaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. arXiv\npreprint arXiv:2301.07733, 2023.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\nJukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.\nChris Donahue, Antoine Caillon, Adam Roberts, Ethan Manilow, Philippe Esling, Andrea\nAgostinelli, Mauro Verzetti, Ian Simon, Olivier Pietquin, Neil Zeghidour, et al. Singsong: Gen-\nerating musical accompaniments from singing. arXiv preprint arXiv:2301.12662, 2023.\nAlexandre D\u00b4efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi.\nHigh fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\nS Forsgren and H Martiros. Riffusion-stable diffusion for real-time music generation. 2022. URL\nhttps://riffusion. com/about, 2022.\nChuang Gan, Deng Huang, Peihao Chen, Joshua B Tenenbaum, and Antonio Torralba. Foley mu-\nsic: Learning to generate music from videos. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16. Springer, 2020.\nHugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and Bryan Pardo. Vampnet: Music gener-\nation via masked acoustic token modeling. arXiv preprint arXiv:2307.04686, 2023.\nItai Gat, Felix Kreuk, Tu Anh Nguyen, Ann Lee, Jade Copet, Gabriel Synnaeve, Emmanuel Dupoux,\nand Yossi Adi. Augmentation invariant discrete representation for generative spoken language\nmodeling. In IWSLT, 2023.\n10\nPublished as a conference paper at ICLR 2024\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel\ndecoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration, 2020.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked\nprediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n2021.\nQingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel PW Ellis. Mu-\nlan: A joint embedding of music audio and natural language. arXiv preprint arXiv:2208.12415,\n2022.\nQingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong\nZhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: Text-conditioned music\ngeneration with diffusion models. arXiv preprint arXiv:2302.03917, 2023a.\nRongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin\nLiu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced\ndiffusion models. arXiv preprint arXiv:2301.12661, 2023b.\nEugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh\nNguyen, Morgane Riviere, Abdelrahman Mohamed, Emmanuel Dupoux, et al. Text-free prosody-\naware generative spoken language modeling. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), 2022.\nKevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fr\u00b4echet audio distance:\nA metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018.\nChris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating\ncaptions for audios in the wild. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), 2019.\nKhaled Koutini, Jan Schl\u00a8uter, Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of audio\ntransformers with patchout. arXiv preprint arXiv:2110.05069, 2021.\nFelix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00b4efossez, Jade Copet, Devi\nParikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv\npreprint arXiv:2209.15352, 2022a.\nFelix Kreuk, Yaniv Taigman, Adam Polyak, Jade Copet, Gabriel Synnaeve, Alexandre D\u00b4efossez, and\nYossi Adi. Audio language modeling using perceptually-guided discrete representations. arXiv\npreprint arXiv:2211.01223, 2022b.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte,\nTu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al.\nOn generative\nspoken language modeling from raw audio. Transactions of the Association for Computational\nLinguistics, 9, 2021.\nMax WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo\nMa, Xuchen Song, et al. Efficient neural music generation. arXiv preprint arXiv:2305.15719,\n2023.\nSang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: A universal\nneural vocoder with large-scale training. arXiv preprint arXiv:2206.04658, 2022.\n11\nPublished as a conference paper at ICLR 2024\nBenjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean\nNaren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\nxformers: A modular and hackable transformer modelling library. https://github.com/\nfacebookresearch/xformers, 2022.\nJos\u00b4e Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with\ntoken-critic. In European Conference on Computer Vision. Springer, 2022.\nPeike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, and Alex Wang. Jen-1: Text-guided uni-\nversal music generation with omnidirectional diffusion models. arXiv preprint arXiv:2308.04729,\n2023.\nTatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello, Jacob Kahn, Gilad Avidov,\nRonan Collobert, and Gabriel Synnaeve. Rethinking evaluation in asr: Are our models robust\nenough? arXiv preprint arXiv:2010.11745, 2020.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D Plumbley.\nAudioldm: Text-to-audio generation with latent diffusion models.\narXiv\npreprint arXiv:2301.12503, 2023a.\nHaohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu\nWang, Yuxuan Wang, and Mark D Plumbley. Audioldm 2: Learning holistic audio generation\nwith self-supervised pretraining. arXiv preprint arXiv:2308.05734, 2023b.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nGallil Maimon and Yossi Adi. Speaking style conversion with discrete self-supervised units. arXiv\npreprint arXiv:2212.09730, 2022.\nKinyugo Maina. Msanii: High fidelity music synthesis on a shoestring budget. arXiv preprint\narXiv:2301.06468, 2023.\nAdam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-\ndelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled\nself-supervised representations. arXiv preprint arXiv:2104.00355, 2021.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning.\nPMLR, 2021.\nJack W Rae and Ali Razavi.\nDo transformers need deep long-range memory.\narXiv preprint\narXiv:2007.03356, 2020.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 2020.\nFl\u00b4avio Ribeiro, Dinei Flor\u02c6encio, Cha Zhang, and Michael Seltzer. Crowdmos: An approach for\ncrowdsourcing mean opinion score studies. In IEEE international conference on acoustics, speech\nand signal processing (ICASSP). IEEE, 2011.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2022.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse\nattention with routing transformers. Transactions of the Association for Computational Linguis-\ntics, 2021.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, et al. Code llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950, 2023.\n12\nPublished as a conference paper at ICLR 2024\nAditya Sanghi, Rao Fu, Vivian Liu, Karl DD Willis, Hooman Shayani, Amir H Khasahmadi, Srinath\nSridhar, and Daniel Ritchie.\nClip-sculptor: Zero-shot generation of high-fidelity and diverse\nshapes from natural language. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2023.\nFlavio Schneider, Zhijing Jin, and Bernhard Sch\u00a8olkopf. Mo\\\u02c6 usai: Text-to-music generation with\nlong-context latent diffusion. arXiv preprint arXiv:2301.11757, 2023.\nRoy Sheffer and Yossi Adi. I hear your true colors: Image guided audio generation. In ICASSP 2023-\n2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech\nsynthesizers. arXiv preprint arXiv:2301.02111, 2023.\nYusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui*, Taylor Berg-Kirkpatrick, and Shlomo Dubnov.\nLarge-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption\naugmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing,\nICASSP, 2023.\nDongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diff-\nsound: Discrete diffusion model for text-to-sound generation. arXiv preprint arXiv:2207.09983,\n2022.\nNeil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-\nstream: An end-to-end neural audio codec.\nIEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 2021.\nZiqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, et al. Speak foreign languages with your own voice: Cross-lingual\nneural codec language modeling. arXiv preprint arXiv:2303.03926, 2023.\n13\nPublished as a conference paper at ICLR 2024\nTable 4: Details about the training sets used to train the proposed method and the evaluated baselines.\nMETHOD\nNO. OF HOURS SAMPLING RATE DETAILS\nMusicGen\n20,000\n32kHz\nShutterStock, Pond5, and proprietary data\nMusicLM\n280,000\n24 kHz\nProprietary data\nMousai\n2,500\n48kHz\nShutterStock, Pond5, and proprietary data\n\u2018AudioLDM2 29,510\n16kHz\nAudioSet, WavCaps, AudioCaps, VGGSound, Free\nMusic Archive, Million Song Dataset,\nLJSpeech, and GigaSpeech\nMAGNET\n20,000\n32kHz\nShutterStock, Pond5, and proprietary data\nA\nEXPERIMENTAL SETUP\nA.1\nIMPLEMENTATION DETAILS\nUnder all setups, we use the official EnCodec model as was published by Copet et al. (2023)7. The\nmodel gets as input an audio segment and outputs a 50 Hz discrete representation. We use four\ncodebooks where each has a codebook size of 2048. We perform the same text preprocessing as\nproposed by Copet et al. (2023); Kreuk et al. (2022a).\nWe train non-autoregressive transformer models using 300M (MAGNET-small) and 1.5B\n(MAGNET-large) parameters. We use a memory efficient Flash attention (Dao et al., 2022) from\nthe xFormers package (Lefaudeux et al., 2022) to improve both speed and memory usage. We train\nmodels using 30-second audio crops sampled at random from the full track. We train the models for\n1M steps with the AdamW optimizer (Loshchilov & Hutter, 2017), a batch size of 192 examples,\n\u03b21 = 0.9, \u03b22 = 0.95, a decoupled weight decay of 0.1 and gradient clipping of 1.0. We further\nrely on D-Adaptation based automatic step-sizes (Defazio & Mishchenko, 2023). We use a cosine\nlearning rate schedule with a warmup of 4K steps. Additionally, we use an exponential moving\naverage with a decay of 0.99. We train the models using respectively 32 GPUs for the small model\nand 64 GPUs for the large ones with float16 precision.\nFinally, for inference, we employ nucleus sampling (Holtzman et al., 2020) with top-p 0.9, and a\ntemperature of 3.0 that is linearly annealed to zero during decoding iterations. We use CFG with\ncondition dropout rate of 0.3 during training, and a guidance coefficient 10.0 that is annealed to 1.0\nduring iterative decoding.\nA.2\nDATASETS\nWe follow the same setup as in Copet et al. (2023) and use 20K hours of licensed music to train\nMAGNET. Specifically, we rely on the same 10K high-quality music tracks, the ShutterStock,\nand Pond5 music data collections as used in Copet et al. (2023)8 with respectively 25K and 365K\ninstrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with\nmetadata composed of a textual description and additional information such as the genre, BPM, and\ntags.\nFor the main results and comparison with prior work, we evaluate the proposed method on the\nMusicCaps benchmark (Agostinelli et al., 2023). MusicCaps is composed of 5.5K samples (ten-\nsecond long) prepared by expert musicians and a 1K subset balanced across genres. We report\nobjective metrics on the unbalanced set, while we sample examples from the genre-balanced set for\nqualitative evaluations. We additionally evaluate the proposed method using the same in-domain\ntest set as proposed by Copet et al. (2023). All ablation studies were conducted on the in-domain\ntest set.\n7https://github.com/facebookresearch/audiocraft\n8www.shutterstock.com/music and www.pond5.com\n14\nPublished as a conference paper at ICLR 2024\nA.3\nEVALUATION\nBaselines.\nFor music generation we compare MAGNET Mousai (Schneider et al., 2023), Music-\nGen Copet et al. (2023), AudioLDM2 Liu et al. (2023b), and MusicLM (Agostinelli et al., 2023).\nFor Mousai, we train a model using our dataset for a fair comparison, using the open source imple-\nmentation provided by the authors9.\nEvaluation metrics.\nWe evaluate the proposed method using the same setup as proposed in Copet\net al. (2023); Kreuk et al. (2022b), which consists of both objective and subjective metrics. For the\nobjective methods, we use three metrics: the Fr\u00b4echet Audio Distance (FAD), the Kullback-Leiber\nDivergence (KL) and the CLAP score (CLAP). We report the FAD (Kilgour et al., 2018) using the\nofficial implementation in Tensorflow with the VGGish model 10. A low FAD score indicates the\ngenerated audio is plausible. Following Kreuk et al. (2022a), we use a state-of-the-art audio classifier\ntrained for classification on AudioSet (Koutini et al., 2021) to compute the KL-divergence over the\nprobabilities of the labels between the original and the generated audio. For the music generation\nexperiments only we additionally report the CLAP score (Wu et al., 2023; Huang et al., 2023b)\nbetween the track description and the generated audio to quantify audio-text alignment, using the\nofficial pretrained CLAP model 11.\nFor the human studies, we follow the same setup as in Kreuk et al. (2022a). We ask human raters\nto evaluate two aspects of the audio samples (i) overall quality (OVL), and (ii) relevance to the\ntext input (REL). For the overall quality test, raters were asked to rate the perceptual quality of\nthe provided samples in a range of 1 to 100. For the text relevance test, raters were asked to rate\nthe match between audio and text on a scale of 1 to 100. Raters were recruited using the Amazon\nMechanical Turk platform. We evaluate randomly sampled files, where each sample was evaluated\nby at least 5 raters. We use the CrowdMOS package12 to filter noisy annotations and outliers.\nWe remove annotators who did not listen to the full recordings, annotators who rate the reference\nrecordings less than 85, and the rest of the recommended recipes from CrowdMOS (Ribeiro et al.,\n2011). For fairness, we include the same normalization scheme as proposed in Copet et al. (2023)\nof normalizing all samples at \u221214dB LUFS.\nB\nRECEPTIVE FIELD ANALYSIS\nWe present the receptive field analysis of the EnCodec model in Fig. 3. We slide an impulse function,\nin the form of a one-hot input vector, and measure the norm of the encoded latent vector in the middle\nof the sequence, as function of the temporal distance from the impulse. We perform the process\ntwice: (i) For the full encoder (left) and (ii) while omitting the LSTM block and remaining only\nwith the convolutional network (right). Fig. 3 shows that the effective receptive field of EnCodec is\nupper bounded by 100ms in each direction, supporting our choice to design MAGNET\u2019s restricted\ntransformer s.t. codebooks greater than one attend only tokens in a neighborhood of 100ms in each\ndirection.\nC\nSPAN MASKING\nSampling a placement of u token spans can be implemented by first sampling a subset of u indices\nfrom {1, . . . , T}, serving as the span starts, and then extending each index to a span. Formally, we\nsample I(u) \u223c U({A \u2286 {1, . . . , T} : |A| = u}), and then extend each index t \u2208 I(u) to the span of\nindices t, . . . , t + l \u2212 1. The total set of masked indices would be\nMspans(I(u); l) \u225c\n[\nt\u2208I(u)\n{t, . . . , t + l \u2212 1}.\n(7)\n9Implementation from github.com/archinetai/audio-diffusion-pytorch (March 2023)\n10github.com/google-research/google-research/tree/master/frechet audio distance\n11https://github.com/LAION-AI/CLAP\n12http://www.crowdmos.org/download/\n15\nPublished as a conference paper at ICLR 2024\n200\n150\n100\n50\n0\n50\n100\n150\n200\nImpulse dt (ms)\n30\n40\n50\n60\n70\nzmid\n(a) EnCodec\u2019s middle latent vector\u2019s impulse re-\nsponse.\n200\n150\n100\n50\n0\n50\n100\n150\n200\nImpulse dt (ms)\n30\n40\n50\n60\n70\nzmid\n(b) The impulse response of the same vector when\nomitting the LSTM block from the encoder.\nFigure 3: A visualization of the receptive field analysis.\nProposition C.1. Given a random placement of u spans of size l over a sequence of length T, the\nexpected masking rate is as follows,\nEI(u)\u223cU({A\u2286{1,...,T }:|A|=u})\n\u0014 1\nT\n\f\f\fMspans \u0010\nI(u); l\n\u0011\f\f\f\n\u0015\n= 1 \u2212\n\u0000T \u2212l\nu\n\u0001\n\u0000T\nu\n\u0001 .\n(8)\nDerivation: First, note that for a given token zt, the probability that zt would remain unmasked, is\nthe probability to choose u span starts only from the indices:\nAt \u225c {1, . . . , T} \\ {t \u2212 l + 1, . . . , t}.\n(9)\nThe total number of placements is\n\u0000T\nu\n\u0001\n, i.e., the number of possibilities to choose u span starts out\nof a set of T indices without replacement. Similarly, the total amount of placements for which all\nspan starts are in At, is\n\u0000T \u2212l\nu\n\u0001\n. Thus,\nP\nh\nt \u2208 Mspans(I(u); l)\ni\n= 1 \u2212\n\u0000T \u2212l\nu\n\u0001\n\u0000T\nu\n\u0001 .\n(10)\nConsequently, the masking probability for each token is 1 \u2212\n\u0000T \u2212l\nu\n\u0001\n/\n\u0000T\nu\n\u0001\n. Finally, we define the\nindicator random variable 1t\u2208Mspans(I(u);l) for each t \u2208 {1 . . . T}, and conclude the derivation by\nEI(u)\nh\f\f\fMspans \u0010\nI(u); l\n\u0011\f\f\f\ni\n=\nEI(u)\n\" T\nX\nt=1\n1t\u2208Mspans(I(u);l)\n#\n(11)\n=\nT\nX\nt=1\nEI(u)\n\u0002\n1t\u2208Mspans(I(u);l)\n\u0003\n(12)\n=\nT \u00b7\n \n1 \u2212\n\u0000T \u2212l\nu\n\u0001\n\u0000T\nu\n\u0001\n!\n.\n(13)\nD\nMODEL INFERENCE\nFig. 4 presents the inference process of MAGNET. For clarity, we omit CFG and nucleus sampling,\nand assume T is a multiple of the span length l. To further ease the reading, we present the inference\nalgorithm for a single codebook, while in practice, we run Fig. 4 for every codebook k \u2208 {1 . . . K}.\n16\nPublished as a conference paper at ICLR 2024\ndef MAGNeT_generate(B: int, T: int, text: List, s: int, model: nn.Module,\nrescorer: nn.Module, mask_id: int, tempr: float, w: float):\n# Start from a fully masked sequence\ngen_seq = torch.full((B, T), mask_id, dtype=torch.long)\nn_spans = T // span_len\nspans_shape = (B, n_spans)\nspan_scores = torch.zeros(spans_shape, dtype=torch.float32)\n# Run MAGNeT iterative decoding for 's' iterations\nfor i in range(s):\nmask_p = torch.cos((math.pi * i) / (2 * s))\nn_masked_spans = max(int(mask_p * n_spans), 1)\n# Masking\nmasked_spans = span_scores.topk(n_masked_spans, dim=-1).indices\nmask = get_mask(spans_shape, masked_spans)\ngen_seq[mask] = mask_id\n# Forward pass\nlogits, probs = model.compute_predictions(gen_sequence, text, cfg=True, temperature=tempr)\n# Classifier free guidance with annealing\ncfg_logits = cfg(mask_p, logits, annealing=True)\n# Sampling\nsampled_tokens = sample_top_p(probs, p=top_p)\n# Place the sampled tokens in the masked positions\nmask = gen_seq == mask_id\ngen_seq = place_sampled_tokens(mask, sampled_tokens[..., 0], gen_seq)\n# Probs of sampled tokens\nsampled_probs = get_sampled_probs(probs, sampled_tokens)\nif rescorer:\n# Rescoring\nrescorer_logits, rescorer_probs = rescorer.compute_predictions(gen_seq, text)\nrescorer_sampled_probs = get_sampled_probs(rescorer_probs, sampled_tokens)\n# Final probs are the convex combination of probs and rescorer_probs\nsampled_probs = w * rescorer_sampled_probs + (1 - w) * sampled_probs\n# Span scoring - max\nspan_scores = get_spans_scores(sampled_probs)\n# Prevent remasking by placing -inf scores for unmasked\nspan_scores = span_scores.masked_fill(\u02dcspans_mask, -1e5)\nreturn gen_seq\nFigure 4: MAGNET\u2019s text-to-audio inference. MAGNET performs iterative decoding of s steps.\nIn each step, the least probable non-overlapping spans are being masked, where the probability is a\nconvex combination of the restricted-transformer confidence and the probability obtained by the pre-\ntrained rescorer. Finally, the span probabilities are re-updated, while assigning \u221e to the unmasked\nspans, to prevent its re-masking and fix it as anchors for future iterations.\nE\nHYBRID-MAGNET TRAINING\nThe aim of Hybrid-MAGNET is to switch from autoregressive generation to non-autoregressive\nduring inference, so as to generate an audio prompt with the same quality as MUSICGEN that can be\ncompleted fast using MAGNET inference. The goal is to find a compromise between MUSICGEN\nquality and MAGNET speed. To give Hybrid-MAGNET the ability to switch between decoding\nstrategies, it requires a few adaptations from MAGNET training recipe. One of them is to train\njointly on two different objectives as illustrated by Figure 5. Similarly to Borsos et al. (2023b) a\ntime step t is uniformly sampled from {1, . . . , T} that simulates an audio prompt for MAGNET to\ncomplete from. For all positions that precede t and for all codebook levels we propose to compute\nthe autoregressive training objective, using causal attention masking. For all succeeding positions\nwe keep the MAGNET training objective: the model can attend to tokens from the audio prompt.\nMoreover the codebook pattern is adapted for the autoregressive generation to work well, to that\n17\nPublished as a conference paper at ICLR 2024\n222\n76\n892\n20\n249 591\n993 129 323 422\n1009\n432 439\n32\n55\n53\n23\n31 921\n13\n993\n2\n612 429 83\n1\n821\n33\n37\n992\n83\nAR loss\nNAR loss\nAR-generated prompt\nNAR completion\nFigure 5: Training of Hybrid-MAGNET. During training a random timestep t is sampled. For\ntimesteps preceding t a causal attention mask is applied and cross entropy loss is computed for\nall levels (blue highlighted squares). For timesteps succeeding t the standard MAGNET training\nstrategy is applied. Codebook levels are shifted following the delay pattern from Copet et al. (2023).\n1\n50\n100\n150\n200\n250\n300\n350\n400\n450\nTemporal Timestep\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nDecoding Timestep\n1\n50\n100\n150\n200\n250\n300\n350\n400\n450\nTemporal Timestep\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nDecoding Timestep\n1\n50\n100\n150\n200\n250\n300\n350\n400\n450\nTemporal Timestep\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nDecoding Timestep\n1\n50\n100\n150\n200\n250\n300\n350\n400\n450\nTemporal Timestep\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nDecoding Timestep\nFigure 6: Decoding visualization of the chosen anchor tokens as a function of decoding steps, for\nan iterative decoding process with s = 20. We plot the mask m(i) chosen by MAGNET, for each\ni \u2208 {1, . . . , s}, during the generation of a 10-second audio sample for the text prompt \u2019A dynamic\nblend of hip-hop and orchestral elements, with sweeping strings and brass\u2019. The x-axis represents\ntime while the y-axis represents the decoding steps.\nend we use the delay pattern from Copet et al. (2023). Thus the temporally restricted context from\nMAGNET is adapted to take into account the codebook level-dependent shifts.\n18\nPublished as a conference paper at ICLR 2024\nTable 5: Text-to-Audio generation results. We report FAD and KL scores for all methods.\nPARAMETERS\nTEXT CONDITIONING\nFAD\u2193\nKL\u2193\nDiffSound\n400M\nCLIP\n7.39\n2.57\nAudioGen-base\n285M\nT5-base\n3.13\n2.09\nAudioGen-large\n1500M\nT5-large\n1.77\n1.58\nAudioLDM2-small\n346M\nT5-large, CLAP, ImageBind, PE\n1.67\n1.01\nAudioLDM2-large\n712M\nT5-large, CLAP, ImageBind, PE\n1.42\n0.98\nMake-an-Audio\n332M\nCLAP\n4.61\n2.79\nMAGNET-small\n300M\nT5-large\n3.22\n1.42\nMAGNET-large\n1500M\nT5-large\n2.36\n1.64\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nLatency (s)\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\nFAD\n5 steps\n10 steps\n20 steps\n50 steps\n100 steps\n1 step\n5 steps\n10 steps\n20 steps\nFirst codebook\nCodebook >1\nFigure 7: Effect of the decoding schedules on the quality/latency trade off. We vary the number of\ndecoding steps for the first codebook level (dashed red curve) and the higher codebook levels (dotted\nblue curve) around a (20, 10, 10, 10) decoding schedule.\nF\nITERATIVE DECODING DYNAMICS\nFig. 6 presents the masking dynamics of MAGNET\u2019s iterative decoding process with s = 20. In\nspecific, we plot the mask m(i) chosen by MAGNET, for each i \u2208 {1 . . . s}, during the genera-\ntion of a 10-second audio sample for the text prompt \u2019A dynamic blend of hip-hop and orchestral\nelements, with sweeping strings and brass\u2019. To demonstrate MAGNET\u2019s stochasticity, we repeat\nthe process several times. As can be seen, MAGNET decodes the audio sequence in a non-causal\nmanner, choosing first a sparse set of token-spans at various disconnected temporal locations, and\ngradually \u201cinpaint\u201d the gaps until convergence to a full token sequence.\nG\nADDITIONAL RESULTS\nText-to-audio generation We follow Kreuk et al. (2022a) and use the exact same training sets\nto optimize MAGNET. We train MAGNET in two model scales, of 300M and 1.5B parameters\nrespectively, and compare it to AudioGen (Kreuk et al., 2022a), DiffSound (Yang et al., 2022),\nAudioLDM2 (Liu et al., 2023b), and Make-an-Audio Huang et al. (2023b). Results are reported\nin Table 5. Results are reported on the AudioCaps testset (Kim et al., 2019). All audio files were\nsampled at 16kHz. As can be see MAGNET results are comparable or slightly worse than the\nautoregressive alternative (AudioGen) while having significantly lower latency (the latency values\nare the same as in Table 1 for MAGNeT, while AudioGen has the same latency as MusicGen). For\ninference, different than the MAGNET models trained for music generation, we use top-p 0.8, an\ninitial temperature of 3.5, and an initial CFG guidance coefficient of 20.0.\nThe effect of decoding steps. The latency of the non-autoregressive model can be controlled by\nconfiguring the appropriate decoding steps, at the expense of quality degradation. In Fig. 7, we\nreport the in-domain FAD as a function of latency for different decoding steps. We ablate on the\nfirst codebook level step count (dashed red curve) and the higher codebook levels step count (dotted\n19\nPublished as a conference paper at ICLR 2024\nFigure 8: We restrict the attention maps to focus on local context for codebooks levels greater than\n1. In this figure we consider 2 time-steps restrictions for each side, in practice we use 5 time-steps\nfor each side, resulting in 11 tokens.\nTable 6: CFG annealing ablation. We report FAD scores for different \u03bb0, \u03bb1 configurations, as well\nas KL and CLAP scores.\n\u03bb0 \u2192 \u03bb1\n1 \u2192 1 3 \u2192 3 10 \u2192 10 10 \u2192 1 20 \u2192 20 20 \u2192 1\nFADvgg \u2193\n3.95\n0.99\n0.63\n0.61\n0.80\n0.68\nKL \u2193\n0.79\n0.60\n0.56\n0.56\n0.57\n0.56\nCLAPscr \u2191\n0.13\n0.28\n0.28\n0.31\n0.31\n0.31\nblue curve), starting from the (20, 10, 10, 10) decoding schedule. The red curve illustrates that a\ngood compromise between latency and quality can be obtained around 20 steps for the first level,\nafter which decoding further will only marginally lower the FAD, while adding latency. Regarding\nthe higher codebook levels, we can observe an inflection point happening around 5 steps after which\nFAD remains stable. It is interesting to note that reducing the decoding steps for higher levels does\nnot impact quality as much as for the first level. For example the (10, 10, 10, 10) decoding schedule\nachieves 0.65 FAD at a latency close to that of the (20, 5, 5, 5) schedule, which achieves a lower\nFAD of 0.61 despite a smaller total step count.\nThe effect of CFG guidance annealing. We evaluate both constant CFG schedules, e.g. by setting\n\u03bb0 = \u03bb1 = 3, and annealing CFG. Results are presented in Table 6. Results suggest that using\n\u03bb0 = 10, \u03bb1 = 1 yields the best FAD score over all evaluated setups. This finding aligns with our\nhypothesis that during the first decoding steps a stronger text adherence is required, while at later\ndecoding steps we would like the model to put more focus on previously decoded tokens.\n20\n"
  },
  {
    "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
    "link": "https://arxiv.org/pdf/2401.04658.pdf",
    "upvote": "20",
    "text": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths\nin Large Language Models\nZhen Qin 1 Weigao Sun 1 Dong Li 1 Xuyang Shen 1 Weixuan Sun 1 Yiran Zhong 1\nAbstract\nLinear attention is an efficient attention mecha-\nnism that has recently emerged as a promising al-\nternative to conventional softmax attention. With\nits ability to process tokens in linear computa-\ntional complexities, linear attention, in theory, can\nhandle sequences of unlimited length without sac-\nrificing speed, i.e., maintaining a constant training\nspeed for various sequence lengths with a fixed\nmemory consumption. However, due to the issue\nwith cumulative summation (cumsum), current\nLinear Attention algorithms cannot demonstrate\ntheir theoretical advantage in a casual setting. In\nthis paper, we present Lightning Attention-2, the\nfirst linear attention implementation that enables\nlinear attention to realize its theoretical computa-\ntional benefits. To achieve this, we leverage the\nthought of tiling, separately handling the intra-\nblock and inter-block components in linear at-\ntention calculation. Specifically, we utilize the\nconventional attention computation mechanism\nfor the intra-blocks and apply linear attention ker-\nnel tricks for the inter-blocks. A tiling technique\nis adopted through both forward and backward\nprocedures to take full advantage of the GPU\nhardware. We implement our algorithm in Tri-\nton to make it IO-aware and hardware-friendly.\nVarious experiments are conducted on different\nmodel sizes and sequence lengths.\nLightning\nAttention-2 retains consistent training and infer-\nence speed regardless of input sequence length\nand is significantly faster than other attention\nmechanisms.\nThe source code is available at\nLightning Attention-2.\n1. Introduction\nThe Transformer architecture has achieved widespread adop-\ntion, particularly in the domain of large language models\n1OpenNLPLab. Correspondence to: Yiran Zhong <zhongyi-\nran@gmail.com>.\nPreliminary work., Copyright 2024 by the author(s).\n(LLM) (Brown et al., 2020; Touvron et al., 2023a;b; Peng\net al., 2023; Qin et al., 2023b) and multi-modal models (Li\net al., 2022; 2023a; Liu et al., 2023; Radford et al., 2021; Li\net al., 2023b; Lu et al., 2022; Mao et al., 2023; Shen et al.,\n2023; Zhou et al., 2023; Sun et al., 2023a; Hao et al., 2024).\nHowever, its computational complexity grows quadratically\nwith the length of the input sequence, making it challenging\nto model extremely long sequences.\nUnlimited sequence length stands out as a noteworthy aspect\nwithin the realm of LLM, attracting considerable attention\nfrom researchers who seek intelligent solutions. The poten-\ntial applications of LLM with unlimited sequence length are\ndiverse, encompassing extended conversations in various\nprofessional domains and handling a vast number of tokens\nin multimodal modeling tasks.\nIn response to the quadratic complexity challenge, a promis-\ning resolution emerges in the form of linear attention. This\nmethod involves the elimination of the softmax operation\nand capitalizes on the associativity property of matrix prod-\nucts. Consequently, it significantly accelerates both training\nand inference procedures. To elaborate, linear attention re-\nduces the computational complexity from O(n2) to O(n)\nby leveraging the kernel trick (Katharopoulos et al., 2020b;\nChoromanski et al., 2020; Peng et al., 2021; Qin et al.,\n2022b) to compute the attention matrices, where n repre-\nsents the sequence length. This avenue holds substantial\npromise for augmenting the efficiency of transformer-style\nmodels across a broad spectrum of applications.\nIt is important to note that the notable reduction in complex-\nity from O(n2) to O(n) in linear attention is only theoretical\nand may not directly translate to a proportional improvement\nin computational efficiency on hardware in practice. The\nrealization of practical wall-clock speedup faces challenges,\nprimarily stemming from two issues: 1). the dominance of\nmemory access (I/O) on the GPU could impact the overall\ncomputation speed of attention. 2). the cumulative summa-\ntion (cumsum) needed by the linear attention kernel trick\nprevents it from reaching its theoretical training speed in the\ncausal setting.\nThe first issue has been successfully addressed by Lightning\nAttention-1 (Qin et al., 2023b). In this paper, we introduce\n1\narXiv:2401.04658v2  [cs.CL]  15 Jan 2024\nLightning Attention-2\nSequence length\nSequence length\nSequence length\nTGS\n0\n5,000\n10,000\n15,000\n20,000\n25,000\n30,000\n35,000\n40,000\n45,000\n1024\n2048\n4096\n8192\n16384\n32768\n65536 131072\nTGS on 400M Models\nLLaMA-FA2\nTNL-LA1\nTNL-LA2\n0\n2,500\n5,000\n7,500\n10,000\n12,500\n15,000\n17,500\n20,000\n1024\n2048\n4096\n8192\n16384\n32768\n65536 131072\nTGS on 1B Models\nLLaMA-FA2\nTNL-LA1\nTNL-LA2\n0\n1,000\n2,000\n3,000\n4,000\n5,000\n6,000\n7,000\n8,000\n1024\n2048\n4096\n8192\n16384\n32768\nTGS on 3B Models\nLLaMA-FA2\nTNL-LA1\nTNL-LA2\nFigure 1. Speed Showdown: FlashAttention vs. Lightning Attention in Expanding Sequence Lengths and Model Sizes. The diagram\nabove provides a comparative illustration of training speed, Token per GPU per Second (TGS) for LLaMA with FlashAttention-2,\nTransNormerLLM with Lightning Attention-1 and TransNormerLLM with Lightning Attention-2, implemented across three model sizes:\n400M, 1B, and 3B from left to right. It is strikingly evident that Lightning Attention-2 manifests a consistent training speed irrespective of\nthe increasing sequence length. Conversely, the other methods significantly decline training speed as the sequence length expands.\nLightning Attention-2 to solve the second issue. The key\nidea is to leverage the concept of \"divide and conquer\" by\nseparately handling the intra block and inter block compo-\nnents in linear attention calculation. Specifically, for the\nintra blocks, we maintain the use of conventional attention\ncomputation mechanism to compute the product of QKV,\nwhile for the inter blocks, we employ the linear attention\nkernel trick (Katharopoulos et al., 2020b). Tiling techniques\nare implemented in both forward and backward procedures\nto fully leverage GPU hardware capabilities. As a result,\nthe Lightning Attention-2 can train LLMs with unlimited\nsequence length without extra cost1, as its computational\nspeed remains constant with increasing sequence length\nunder fixed memory consumption.\nWe performed a comprehensive evaluation of Lightning\nAttention-2 across a diverse range of sequence lengths to\nassess its accuracy and compare its computational speed\nand memory utilization with FlashAttention-2 (Dao, 2023)\nand Lightning Attention-1. The findings indicate that Light-\nning Attention-2 exhibits a notable advantage in computa-\ntional speed, attributed to its innovative intra-inter separation\nstrategy. Additionally, Lightning Attention-2 demonstrates\na reduced memory footprint compared to its counterparts\nwithout compromising performance.\n2. Related Work\n2.1. Linear Attention\nLinear Transformer architectures discard the Softmax At-\ntention mechanism, replacing it with distinct approxima-\ntions (Katharopoulos et al., 2020a; Choromanski et al., 2020;\nPeng et al., 2021; Qin et al., 2022b;a). The key idea is to\n1However, the sequence length may still be limited by hardware\nconstraints, such as the GPU memory.\nleverage the \u201ckernel trick\" to accelerate the attention matrix\ncomputation, i.e., compute the product of keys and values\nfirst to circumvent the n\u00d7n matrix multiplication. Multiple\nmethods have been proposed to replace the softmax opera-\ntion. For instance, Katharopoulos et al. (2020a) employ the\n1 + elu activation function, Qin et al. (2022b) utilize the\ncosine function to approximate softmax properties, and Ke\net al. (2021); Zheng et al. (2022; 2023) leverage sampling\nstrategies to directly mimic softmax operation. Despite hav-\ning a theoretical complexity of O(nd2), the practical com-\nputational efficiency of linear attention diminishes notably\nin causal attention scenarios, primarily due to the necessity\nfor cumsum operations (Hua et al., 2022).\n2.2. IO-aware Attention\nThe FlashAttention series (Dao et al., 2022; Dao, 2023)\nfocuses on system-level optimizations for the efficient im-\nplementation of the standard attention operator on GPU\nplatforms. Extensive validation has demonstrated its effec-\ntiveness. The approach employs tiling strategies to minimize\nthe volume of memory reads/writes between the GPU\u2019s high\nbandwidth memory (HBM) and on-chip SRAM.\nTo address the issue of slow computation for Linear At-\ntention in the causal setting, Lightning Attention 1 (Qin\net al., 2023b) employs the approach of FlashAttention-1/2,\nwhich involves segmenting the inputs Q, K, V into blocks,\ntransferring them from slow HBM to fast SRAM, and then\ncomputing the attention output with respect to these blocks.\nSubsequently, the final results are accumulated. Although\nthis method is much more efficient than the PyTorch imple-\nmentation, it does not take advantage of the computational\ncharacteristics inherent to Linear Attention, and the theoret-\nical complexity remains O(n2d).\n2\nLightning Attention-2\n2.3. Long Sequence Handling in LLM\nA widely adopted strategy to tackle challenges related to\nlength extrapolation involves the integration of Relative Po-\nsitional Encoding (RPE) techniques (Su et al., 2021; Qin\net al., 2023c), strategically directing attention towards neigh-\nboring tokens. ALiBi (Press et al., 2022) utilizes linear de-\ncay biases in attention mechanisms to mitigate the impact\nof distant tokens. Roformer (Su et al., 2021) introduces a\nnovel Rotary Position Embedding (RoPE) method, widely\nembraced in the community, effectively leveraging posi-\ntional information for transformer-based language model\nlearning. Kerple (Chi et al., 2022) explores shift-invariant\nconditionally positive definite kernels within RPEs, intro-\nducing a suite of kernels aimed at enhancing length ex-\ntrapolation properties, with ALiBi recognized as one of its\ninstances. Furthermore, Sandwich (Chi et al., 2023) postu-\nlates a hypothesis elucidating the mechanism behind ALiBi,\nempirically validating it by incorporating the hypothesis\ninto sinusoidal positional embeddings. (Qin et al., 2024) ex-\nplored the sufficient conditions for additive relative position\nencoding to have extrapolation capabilities.\nInstead of investigating the length extrapolation capability of\ntransformers, some works also attempt to directly increase\nthe context window sizes. Chen et al. (2023) introduces\nPosition Interpolation (PI), extending context window sizes\nof RoPE-based pretrained Large Language Models (LLMs)\nsuch as LLaMA models to up to 32768 with minimal fine-\ntuning (within 1000 steps). StreamingLLM (Xiao et al.,\n2023) proposes leveraging the attention sink phenomenon,\nmaintaining the Key and Value information of initial to-\nkens to substantially recover the performance of window\nattention. As the sequence grows longer, the performance\ndegrades. These methods can only extend sequence length\nin fine-tuning or testing phases, while our method allows\ntraining models in long sequence lengths from scratch with\nno additional cost.\n3. Method\n3.1. Preliminary\nWe first recall the formulation of linear attention and then\nintroduce our proposed Lightning Attention-2. In the case\nof NormAttention within TransNormer (Qin et al., 2022a),\nattention computation deviates from the conventional Trans-\nformer structure (Vaswani et al., 2017) by eschewing the\ncostly softmax and scaling operations. The NormAttention\nmechanism can be expressed as follows:\nO = Norm((QK\u22a4)V),\n(1)\nwhere Q, K, and V \u2208 Rn\u00d7d are the query, key, and value\nmatrices, respectively, with n denoting sequence length and\nd representing feature dimension. To Leverage the compu-\ntational efficiency inherent in right matrix multiplication,\nthe above equation can be seamlessly and mathematically\nequivalently transformed into its linear variant, as dictated\nby the properties of matrix multiplication:\nO = Norm(Q(K\u22a4V)),\n(2)\nThis linear formulation facilitates recurrent prediction with\na commendable complexity of O(nd2), rendering it effi-\ncient during training relative to sequence length. Further-\nmore, employing linear attention ensures a constant compu-\ntation complexity of O(d2) irrespective of sequence length,\nthereby enabling inference over unlimited long sequences.\nThis achievement is realized by updating K\u22a4V recurrently\nwithout the need for repeated computation of the entire at-\ntention matrix. In contrast, the standard softmax attention\nentails a computational complexity of O(md2) during the\ninference process, where m denotes the token index.\nNevertheless, when dealing with causal prediction tasks, the\neffectiveness of the right product is compromised, leading to\nthe requirement for the computation of cumsum (Hua et al.,\n2022). This impediment hinders the potential for highly\nefficient parallel computation. Consequently, we persist\nwith the conventional left matrix multiplication in Light-\nning Attention-1. This serves as the promotion behind the\nintroduction of Lightning Attention-2, specifically crafted\nto address the challenges associated with the right product\nin such contexts.\n3.2. Lightning Attention-2\nLightning Attention-2 employs a tiling methodology\nthroughout its whole computation process. Given the huge\nvariance in memory bandwidth between HBM and SRAM\nwithin GPU, Lightning Attention-2 applies a distinct strat-\negy for leveraging them.\nIn each iteration i, matrices\nQi, Ki, Vi undergo segmentation into blocks, subsequently\ntransferred to SRAM for computation. The intra- and inter-\nblock operations are segregated, with intra-blocks employ-\ning the left product and inter-blocks utilizing the right prod-\nuct. This approach optimally exploits the computational\nand memory efficiencies associated with the right product,\nenhancing overall execution speed. The intermediate ac-\ntivation KV is iteratively saved and accumulated within\nSRAM. Subsequently, the outputs of intra-blocks and inter-\nblocks are summed within SRAM, and the results are written\nback to HBM. This method aims to capitalize on the dis-\ntinct advantages of each memory component, optimizing\nthe computational workflow. The structural framework of\nLightning Attention-2 is well illustrated in Fig. 2.\nThe intricate details of the Lightning Attention-2 implemen-\ntation are explicated through Algorithm 1 (forward pass)\nand Algorithm 2 (backward pass). These algorithms serve\nto encapsulate the nuanced computational procedures in-\n3\nLightning Attention-2\n\ud835\udc56\n\ud835\udc78 \u2208 \u211d\ud835\udc27\u00d7\ud835\udc1d\n\ud835\udc8a\n\ud835\udc8a\n\ud835\udc72 \u2208 \u211d\ud835\udc8f\u00d7\ud835\udc85\n\ud835\udc7d \u2208 \u211d\ud835\udc5b\u00d7\ud835\udc51\nOutput \nto HBM\n\ud835\udc76\ud835\udc8a = \ud835\udc76\ud835\udc8a\ud835\udc8f\ud835\udc95\ud835\udc93\ud835\udc82+ \ud835\udc76\ud835\udc8a\ud835\udc8f\ud835\udc95\ud835\udc86\ud835\udc93\n\ud835\udc72\ud835\udc7d = \ud835\udf40\ud835\udc69\ud835\udc72\ud835\udc7d + (\ud835\udf40\ud835\udc69\u2212\ud835\udfcf\ud835\udeb2\u2212\ud835\udfcf\ud835\udc72\ud835\udc8a)\ud835\udc7b\ud835\udc7d\ud835\udc8a\n\ud835\udc76\ud835\udc8a\ud835\udc8f\ud835\udc95\ud835\udc93\ud835\udc82 = (\ud835\udc78\ud835\udc8a\ud835\udc72\ud835\udc8a\n\ud835\udc7b\u2a00\ud835\udc0c)\ud835\udc7d\ud835\udc8a\nIntra block\n\ud835\udc76\ud835\udc8a\ud835\udc8f\ud835\udc95\ud835\udc86\ud835\udc93 = \ud835\udf26\ud835\udc78\ud835\udc8a \u2219 (\ud835\udc72\ud835\udc7d)\nInter block\non-chip SRAM\nCopy Block \nto SRAM\nstore in HBM\n\ud835\udc8a\n\ud835\udc76 \u2208 \u211d\ud835\udc5b\u00d7\ud835\udc51\nstore in HBM\nloop over \ud835\udc5b dim\nFigure 2. Structural framework of Lightning Attention-2 is de-\ntailed in its algorithmic schematic. During the i-th iteration, the\ntiling blocks of matrices Qi, Ki, Vi are transferred from High\nBandwidth Memory (HBM) to Static Random-Access Memory\n(SRAM). Within the SRAM, the outputs Ointra and Ointer are\ncomputed independently, followed by an update to the KV matrix.\nSubsequently, the final output Oi, which is the sum of Ointra and\nOinter, is written back from SRAM to HBM.\ntegral to Lightning Attention-2. Additionally, we provide\na comprehensive derivation to facilitate a more profound\ncomprehension of Lightning Attention-2. The derivations\nare systematically presented for both the forward pass and\nthe backward pass, contributing to a thorough understanding\nof the underlying mechanisms.\n3.2.1. FORWARD PASS\nWe ignore the Norm(\u00b7) operator in eq. (2) to simplify the\nderivations. During forward pass of Lightning Attention-2,\nthe t-th output can be formulated as\not = qt\nX\ns\u2264t\n\u03bbt\u2212sk\u22a4\ns vs.\n(3)\nIn a recursive form, the above equation can be rewritten as\nkv0 = 0 \u2208 Rd\u00d7d,\nkvt = \u03bbkvt\u22121 + k\u22a4\nt vt,\not = qt(kvt),\n(4)\nwhere\nkvt =\nX\ns\u2264t\n\u03bbt\u2212sk\u22a4\ns vs.\n(5)\nTo perform tiling, let us write the equations in block form.\nGiven the total sequence length n and block size B, X\nis divided into T =\nn\nB blocks {X1, X2, . . . , XT } of size\nB \u00d7 d each, where X \u2208 {Q, K, V, O}.\nAlgorithm 1 Lightning Attention-2 Forward Pass\nInput: Q, K, V \u2208 Rn\u00d7d, decay rate \u03bb \u2208 R+, block sizes B.\nDivide X into T =\nn\nB blocks X1, X2, ...XT of size B \u00d7 d\neach, where X \u2208 {Q, K, V, O}.\nInitialize mask M \u2208 RB\u00d7B, where Mij = \u03bbi\u2212j, if i \u2265 j, else\n0.\nInitialize \u039b = diag{\u03bb, \u03bb2, . . . , \u03bbB} \u2208 RB\u00d7B.\nInitialize KV = 0 \u2208 Rd\u00d7d.\nfor 1 \u2264 i \u2264 T do\nLoad Qi, Ki, Vi \u2208 RB\u00d7d from HBM to on-chip SRAM.\nOn chip, compute Ointra = [(QiK\u22a4\ni ) \u2299 M]Vi.\nOn chip, compute Ointer = \u039bQi(KV).\nOn chip, compute KV = \u03bbBKV + (\u03bbB\u039b\u22121Ki)\u22a4Vi.\nWrite Oi = Ointra + Ointer to HBM as the i-th block of O.\nend for\nreturn O.\nWe first define\nKV0 = 0 \u2208 Rd\u00d7d, KVt =\nX\ns\u2264tB\n\u03bbtB\u2212sk\u22a4\ns vs.\n(6)\nGiven KVt, the output of (t+1)-th block, i.e., tB +r, with\n1 \u2264 r \u2264 B is\notB+r\n=qtB+r\nX\ns\u2264tB+r\n\u03bbtB+r\u2212sk\u22a4\ns vs\n=qtB+r\n\uf8eb\n\uf8ed\ntB+r\nX\ns=tB+1\n\u03bbtB+r\u2212sk\u22a4\ns vs + \u03bbr X\ns\u2264tB\n\u03bbtB\u2212sk\u22a4\ns vs\n\uf8f6\n\uf8f8\n=qtB+r\ntB+r\nX\ns=tB+1\n\u03bbtB+r\u2212sk\u22a4\ns vs + \u03bbrqtB+rkvtB.\n(7)\nRewritten in matrix form, we have\nOt+1 = [(Qt+1K\u22a4\nt+1) \u2299 M]Vt+1\n|\n{z\n}\nIntra Block\n+ \u039bQt+1(KVt)\n|\n{z\n}\nInter Block\n,\n(8)\nwhere\nMst =\n(\n\u03bbs\u2212t\ns \u2265 t\n0\ns < t ,\n\u039b = diag{1, . . . , \u03bbB\u22121}.\n(9)\n4\nLightning Attention-2\nAlgorithm 2 Lightning Attention-2 Backward Pass\nInput: Q, K, V, dO \u2208 Rn\u00d7d, decay rate \u03bb \u2208 R+, block sizes\nB.\nDivide X into T =\nn\nB blocks X1, X2, ...XT of size B \u00d7 d\neach, where X \u2208 {Q, K, V}.\nDivide dX into T =\nn\nB blocks dX1, dX2, ...dXT of size\nB \u00d7 d each, where X \u2208 {Q, K, V, O} .\nInitialize mask M \u2208 RB\u00d7B, where Mij = \u03bbi\u2212j, if i \u2265 j, else\n0.\nInitialize \u039b = diag{\u03bb, \u03bb2, . . . , \u03bbB} \u2208 RB\u00d7B .\nInitialize KV = 0, dKV = 0 \u2208 Rd\u00d7d.\nfor i = 1, . . . , T do\nLoad Ki, Vi, Oi, dOi \u2208 RB\u00d7d from HBM to on-chip\nSRAM.\nOn chip, compute dQintra = [(dOiV\u22a4\ni ) \u2299 M]Ki.\nOn chip, compute dQinter = \u039bdOi(KV)\u22a4.\nOn chip, compute KV = \u03bbBKV + (\u03bbB\u039b\u22121Ki)\u22a4Vi.\nWrite dQi = dQintra + dQinter to HBM as the i-th block\nof dQ.\nend for\nfor i = T, . . . , 1 do\nLoad Qi, Ki, Vi, Oi, dOi \u2208 RB\u00d7d from HBM to on-chip\nSRAM.\nOn chip, compute dKintra = [(dOiV\u22a4\ni ) \u2299 M]\u22a4Qi.\nOn chip, compute dKinter = (\u03bbB\u039b\u22121Vi)(dKV)\u22a4.\nOn chip, compute dVintra = [(QiK\u22a4\ni ) \u2299 M]\u22a4dOi.\nOn chip, compute dVinter = (\u03bbB\u039b\u22121Ki)dKV.\nOn chip, compute dKV = \u03bbBdKV + (\u039bQi)\u22a4dOi.\nWrite dKi = Kintra + Kinter, dVi = Vintra + Vinter to\nHBM as the i-th block of dK, dV.\nend for\nreturn dQ, dK, dV.\nAnd the KV at (t + 1)-th block can be written as\nKVt+1 =\nX\ns\u2264(t+1)B\n\u03bb(t+1)B\u2212sk\u22a4\ns vs\n= \u03bbB X\ns\u2264tB\n\u03bbtB\u2212sk\u22a4\ns vs +\n(t+1)B\nX\ns=tB+1\n\u03bb(t+1)B\u2212sk\u22a4\ns vs\n= \u03bbBKVt +\n\u0000diag{\u03bbB\u22121, . . . , 1}Kt\n\u0001\u22a4 Vt\n= \u03bbBKVt +\n\u0000\u03bbB\u039b\u22121Kt\n\u0001\u22a4 Vt.\n(10)\nThe complete expression of the forward pass of Lightning\nAttention-2 can be found in Algorithm 1.\n3.2.2. BACKWARD PASS\nFor backward pass, let us consider the reverse process. First\ngiven dot, we have\ndqt = dot(kvt)\u22a4 \u2208 R1\u00d7d,\ndkt = vt(dkvt)\u22a4 \u2208 R1\u00d7d,\ndvt = kt(dkvt) \u2208 R1\u00d7d,\ndkvt =\nX\ns\u2265t\n\u03bbs\u2212tq\u22a4\ns dos \u2208 Rd\u00d7d.\n(11)\nBy writing dkvt in a recursive form, we get\ndkvn+1 = 0 \u2208 Rd\u00d7d,\ndkvt\u22121 = \u03bbdkvt + q\u22a4\nt\u22121dot\u22121.\n(12)\nTo facilitate the understanding of tiling, let us consider\nthe above equations in block style.\nGiven the total se-\nquence length n and block size B, X is divided into\nT =\nn\nB blocks {X1, X2, . . . , XT } of size B \u00d7 d each,\nwhere X \u2208 {Q, K, V, O, dO}.\nWe first define\ndKVT +1 = 0 \u2208 Rd\u00d7d,\ndKVt =\nX\ns>tB\n\u03bbs\u2212tBq\u22a4\ns dos.\n(13)\nThen for the (t + 1)-th block, i.e., tB + r, 0 \u2264 r < B, we\nhave\ndqtB+r\n=dotB+r\nX\ns\u2264tB+r\n\u03bbtB+r\u2212sv\u22a4\ns ks\n=dotB+r\n\uf8eb\n\uf8ed\ntB+r\nX\ns=tB+1\n\u03bbtB+r\u2212sv\u22a4\ns ks + \u03bbr X\ns\u2264tB\n\u03bbtB\u2212sv\u22a4\ns ks\n\uf8f6\n\uf8f8\n=dotB+r\ntB+r\nX\ns=tB+1\n\u03bbtB+r\u2212sv\u22a4\ns ks + \u03bbrdotB+rkv\u22a4\ntB.\n(14)\nIn matrix form, we have\ndQt+1 = [(dOt+1V\u22a4\nt+1) \u2299 M]Kt+1\n|\n{z\n}\nIntra Block\n+ \u039bdOt+1(KV\u22a4\nt )\n|\n{z\n}\nInter Block\n.\n(15)\nSince the recursion of dKt steps from t + 1 to t, given\nKVt+1, dKt for the t-th block, i.e., at positions (t\u22121)B +\nr, 0 < r \u2264 B is\ndk(t\u22121)B+r\n=v(t\u22121)B+r\nX\ns\u2265(t\u22121)B+r\n\u03bbs\u2212(t\u22121)B\u2212rdo\u22a4\ns qs\n=v(t\u22121)B+r\n\uf8eb\n\uf8ed\ntB\nX\ns=(t\u22121)B+r\n\u03bbtB+r\u2212sdo\u22a4\ns qs\n\uf8f6\n\uf8f8\n+ v(t\u22121)B+r\n \n\u03bbB\u2212r X\ns>tB\n\u03bbs\u2212tBdo\u22a4\ns qs\n!\n=v(t\u22121)B+r\ntB\nX\ns=(t\u22121)B+r\n\u03bbtB+r\u2212sdo\u22a4\ns qs\n+ \u03bbB\u2212rv(t\u22121)B+rdKV\u22a4\nt .\n(16)\n5\nLightning Attention-2\nIn matrix form, we get\ndKt\u22121 = [(dOt\u22121V\u22a4\nt\u22121) \u2299 M]\u22a4Qt\u22121\n|\n{z\n}\nIntra Block\n+ \u03bbB\u039b\u22121Vt\u22121(dKV\u22a4\nt )\n|\n{z\n}\nInter Block\n.\n(17)\nConsidering dVt for the t-th block, i.e., at positions (t \u2212\n1)B + r, 0 < r \u2264 B, we have\ndv(t\u22121)B+r\n=k(t\u22121)B+r\nX\ns\u2265(t\u22121)B+r\n\u03bbs\u2212(t\u22121)B\u2212rq\u22a4\ns dos\n=k(t\u22121)B+r\n\uf8eb\n\uf8ed\ntB\nX\ns=(t\u22121)B+r\n\u03bbtB+r\u2212sq\u22a4\ns dos\n\uf8f6\n\uf8f8\n+ \u03bbB\u2212r\n X\ns>tB\n\u03bbs\u2212tBq\u22a4\ns dos\n!\n=k(t\u22121)B+r\ntB\nX\ns=(t\u22121)B+r\n\u03bbtB+r\u2212sq\u22a4\ns dos\n+ \u03bbB\u2212rk(t\u22121)B+rdKVt.\n(18)\nIn matrix form, we get\ndVt\u22121 = [(Qt\u22121K\u22a4\nt\u22121) \u2299 M]\u22a4dOt\n|\n{z\n}\nIntra Block\n+ \u03bbB\u039b\u22121Kt\u22121(dKVt)\n|\n{z\n}\nInter Block\n.\n(19)\nFinally, the recursive relation for dKVt is\ndKVt =\nX\ns>tB\n\u03bbs\u2212tBq\u22a4\ns dos\n= \u03bbB\nX\ns>(t+1)B\n\u03bbs\u2212(t+1)Bq\u22a4\ns dos\n+\n(t+1)B\nX\ns=tB+1\n\u03bbs\u2212tBq\u22a4\ns dos\n= \u03bbBdKVt+1 + (\u039bQt)\u22a4 dOt.\n(20)\nAlgorithm 2 describes the backward pass of Lightning\nAttention-2 in more detail.\nDiscussion\nA recent method, GLA (Yang et al., 2023)\nmodels sequences using linear attention with data-dependent\ndecay. Its chunk-wise Block-Parallel Algorithm employs\ntiling and IO-aware concepts. However, unlike Lightning\nAttention-2, it uses parallel computations for each block,\nwhich leads to higher memory usage. Retnet (Sun et al.,\n2023b) is very similar in structure to TransNormerLLM (Qin\net al., 2023b) and uses the chunk-wise retention algorithm.\nThis algorithm is comparable to the forward pass of Light-\nning Attention-2 but does not consider IO-aware or the\nbackward pass.\n4. Experiments\nTo comprehensively assess Lightning Attention-2\u2019s perfor-\nmance, speed, and memory utilization, we conducted ex-\ntensive experiments on the TransNormerLLM model, with\nLightning Attention-2 integrated. Our implementation uti-\nlizes the Metaseq framework (Zhang et al., 2022), a PyTorch-\nbased sequence modeling framework (Paszke et al., 2019).\nAll experiments are executed on the GPU cluster featur-\ning 128 A100 80G GPUs. The deployment of Lightning\nAttention-2 is implemented in Triton (Tillet et al., 2019).\n4.1. Attention Module Evaluation\nWe conducted a comparison of speed and memory usage\namong attention modules Lightning Attention-1, Lightning\nAttention-2, and FlashAttention-2, all under a single A100\n80G GPU. As depicted in Figure 3, the analysis focuses\non the runtime, measured in milliseconds, for the sepa-\nrated forward and backward propagation. The baseline\nruntime demonstrates a quadratic growth relative to the se-\nquence length. In contrast, Lightning Attention-2 exhibits a\nmarkedly superior performance with linear growth. Notably,\nas the sequence length increases, this disparity in runtime be-\ncomes increasingly apparent. In addition to speed enhance-\nments, our method also maintains a significant advantage in\nmemory usage with the increase in sequence length.\n4.2. Lightning Attention-2 in Large Language Model\nTable 2. Language Modeling Comparison between TransNormer-\nLLM with Lightning Attention-1 and Lightning Attention-2.\nModel\nAttention\nParams\nUpdates\nLoss\nTNL-LA1\nLA1\n0.4B\n100k\n2.229\nTNL-LA2\nLA2\n0.4B\n100k\n2.228\nPerformance Evaluation\nIn Table 2, we evaluated the\nperformance of the TransNormerLLM-0.4B model under 2K\ncontexts, comparing two variants: one equipped with Light-\nning Attention-1 and the other with Lightning Attention-2.\nThese experiments were carried out using 8\u00d7A100 80G\nGPUs. After 100,000 iterations, using the sampled corpus\nfrom our corpus with 300B tokens and initial seed, we ob-\nserved a marginal performance difference. Specifically, the\nvariant with Lightning Attention-2 demonstrated a perfor-\nmance decrement of 0.001 compared to its counterpart with\nLightning Attention-1.\nFurthermore, our analysis extended to benchmarking the\ntop-tier efficient large language models, including LLaMA-\n6\nLightning Attention-2\n0\n200\n400\n600\n800\n1,000\n1,200\n1,400\n1,600\n1,800\n2,000\n1024\n2048\n4096\n8192\n16384\n32768\n65536 131072\nForward Pass\nLightning1\nFlash2\nLightning2\n0\n200\n400\n600\n800\n1,000\n1,200\n1,400\n1,600\n1,800\n2,000\n1024\n2048\n4096\n8192\n16384\n32768\n65536 131072\nBackward Pass\nLightning1\nFlash2\nLightning2\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n1024\n2048\n4096\n8192\n16384\n32768\n65536 131072\nForward Memory Footprint\nLightning1\nFlash2\nLightning2\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n1024\n2048\n4096\n8192\n16384\n32768\n65536 131072\nBackward Memory Footprint\nLightning1\nFlash2\nLightning2\nMemory Footprint  (GB)\nRuntime (ms)\nSequence length\nSequence length\nFigure 3. Comparative Analysis of Speed and Memory Usage: FlashAttention vs. Lightning Attention. Upper Section: Runtime in\nmilliseconds for the forward and backward pass across varying sequence lengths. Lower Section: Memory utilization during the forward\nand backward pass at different sequence lengths.\nTable 1. Efficiency Comparison of LLaMA with FlashAttention2, TransNormerLLM with Lightning Attention-1, and\nTransNormerLLM with Lightning Attention-2. The statistical analysis was performed using 2\u00d7A100 80G GPUs. The table\nreports Tokens per GPU per Second (TGS) across three different model sizes, within context ranges spanning from 1K to 92K. OOM\nstands for out of GPU memory.\nModel\nPS\n1024\n2048\n4096\n8192\n16384\n32768\n65536\n81920\n94208\nLLaMA-FA2\n0.4B\n35931\n32453\n28184\n21996\n15479\n9715\n5643\n4604\n4078\nTNL-LA1\n0.4B\n41789\n39043\n34894\n28627\n21112\n13852\n8247\n6824\n6012\nTNL-LA2\n0.4B\n38615\n38680\n38714\n38172\n37755\n37364\n38278\n38457\n38596\nLLaMA-FA2\n1B\n14897\n13990\n12644\n10887\n8468\n5836\n3820\n3167\nOOM\nTNL-LA1\n1B\n21195\n20128\n18553\n16012\n12594\n8848\n5611\n4625\nOOM\nTNL-LA2\n1B\n20052\n19967\n20009\n19841\n19805\n19691\n20077\n20186\nOOM\nLLaMA-FA2\n3B\n7117\n6708\n6008\n4968\n3755\n2558\nOOM\nOOM\nOOM\nTNL-LA1\n3B\n8001\n7649\n7117\n6152\n4859\n3512\nOOM\nOOM\nOOM\nTNL-LA2\n3B\n7524\n7593\n7599\n7559\n7545\n7545\nOOM\nOOM\nOOM\nFA2 (Touvron et al., 2023a; Dao, 2023), TNL-LA2,\nHGRN (Qin et al., 2023d), and TNN (Qin et al., 2023a).\nThis benchmarking focused on training loss using a 30B\nsubset of our uniquely assembled corpus, scaling from 1 to\n3 billion parameters. As depicted in Figure 4, the TNL-LA2\nmodel achieved marginally lower loss compared to the other\nmodels under review in both 1B and 3B parameters.\nEfficiency Evaluation\nIn Table 1, we present a compara-\ntive analysis of training speeds under the same corpora and\nhardware setups. This comparison encompasses three vari-\nants: TransNormerLLM with Lightning Attention-2 (TNL-\nLA2), TransNormerLLM with Lightning Attention-1 (TNL-\nLA1), and LLaMA with FlashAttention2 (LLaMA-FA2).\nOur findings show that during both the forward and back-\nward passes, the TGS (tokens per GPU per second) for\nTNL-LA2 remains consistently high, while the other two\nmodels exhibit a rapid decline when the sequence length is\nscaled from 1K to 92K. This pattern suggests that Lightning\nAttention-2 offers a significant advancement in managing\nunlimited sequence lengths in LLM.\n4.3. Benchmarking Lightning Attention-2 in Large\nLanguage Model\nTo evaluate the performance of the Lightning Attention-\n2, we conducted an analysis of the TransNormerLLM-\n15B (Qin et al., 2023b), a model comprising 15 billion\nparameters. The TransNormerLLM-15B is characterized\nby its 42 layers, 40 attention heads, and an overall embed-\n7\nLightning Attention-2\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n0\n5\n10\n15\n20\n25\n30\nLoss on 3B Models\nHRGN\nTNN\nLLaMA-FA2\nTNL-LA2\nLoss\nBillion Tokens\nBillion Tokens\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n0\n5\n10\n15\n20\n25\n30\nLoss on 1B Models\nHRGN\nTNN\nLLaMA-FA2\nTNL-LA2\nFigure 4. Performance Comparison of HGRN, TNN, LLaMA with FlashAttention2 and TransNormerLLM with Lightning\nAttention-2. For the 1B model, we used 16\u00d7A800 80G GPUs with a batch size of 12 per GPU; for the 3B model, we scaled up to\n32\u00d7A800 80G GPUs and a batch size of 30 per GPU. The training context length was set to 2K.\nTable 3. Performance Comparison on Commonsense Reasoning and Aggregated Benchmarks. TNL-LA2: TransNormerLLM with\nLightning Attention-2. PS: parameter size (billion). T: tokens (billion). HS: HellaSwag. WG: WinoGrande.\nModel\nPS\nT\nBoolQ PIQA\nHS\nWG ARC-e\nARC-c\nOBQA\nCSR\nC-Eval\nMMLU\nC-Eval\nMMLU\nB\nB\nacc\nacc\nacc_norm\nacc\nacc\nacc_norm acc_norm avg.\nacc-0shot acc-0shot acc-5shot acc-5shot\nPythia\n12 50.3\n62.14 71.76\n51.89\n55.64 59.22\n28.75\n32.80\n51.74\n22.36\n25.80\n21.43\n26.10\nTNL-LA2 15 49.8\n62.08 72.52\n55.55\n57.14 62.12\n31.14\n32.40\n53.28\n25.55\n26.60\n26.18\n27.50\nPythia\n12 100.6 62.20 73.23\n58.83\n59.35 63.76\n31.91\n32.80\n54.58\n24.00\n24.80\n24.45\n24.40\nTNL-LA2 15 99.7\n63.98 74.70\n61.09\n61.33 65.95\n34.64\n35.60\n56.76\n26.70\n26.90\n25.38\n27.40\nding dimension of 5120. The model will be trained on a\ncorpus of more than 1.3 trillion tokens with a sequence\nlength of 6,144. Notably, the model achieved a processing\nspeed of 1,620 tokens per GPU per second. Given that the\ncomprehensive pre-training phase is scheduled to span three\nmonths, we hereby present the most recent results from the\nlatest checkpoint for inclusion in Table 3.\nThis evaluation is conducted using the lm-evaluation-\nharness framework (Gao et al., 2023). Our benchmark fo-\ncuses on two key areas: Commonsense Reasoning (CSR)\nand Multiple Choice Questions (MCQ). For comparative\nanalysis, we also evaluated the Pythia-12B (Biderman et al.,\n2023) model under the same benchmarks.\nCommonsense Reasoning\nWe report BoolQ (Clark et al.,\n2019), PIQA (Bisk et al., 2019), SIQA (Sap et al., 2019),\nHellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi\net al., 2019), ARC easy and challenge (Clark et al., 2018),\nOpenBookQA (Mihaylov et al., 2018) and their average.\nIn all CSR tasks, the performance of TransNormerLLM-\n15B surpassed Pythia-12B by about 2%.\nFurthermore,\nTransNormerLLM-15B-100B showed an approximate 3.5%\nimprovement over its 50 billion-token stage, especially in\nthe HellaSwag task, with over a 5% performance increase.\nAggregated Benchmarks\nWe report the overall results\nfor MMLU (Hendrycks et al., 2021) and C-Eval (Huang\net al., 2023) with both 0-shot and 5-shot settings. In the C-\nEval tasks, TransNormerLLM-15B is about 2% higher than\nPythia-12B. In the 0-shot and 5-shot tests in both Chinese\n(C-Eval) and English (MMLU), TransNormerLLM-15B\u2019s\nperformance also exceeded the 25% baseline (the probability\nof random selection in a 4-choice scenario). We also noticed\nfluctuations in the 5-shot MCQ tasks, with an average MCQ\nscore of around 26.5%.\n5. Conclusion\nIn this paper, we introduced Lightning Attention-2, a pio-\nneering implementation of linear attention that effectively\nharnesses its theoretical computational advantages, particu-\nlarly in the causal setting. Our approach, which adopts the\nconcepts of \"divide and conquer\" and tiling techniques, suc-\ncessfully addresses the limitations of current linear attention\nalgorithms, especially the challenges associated with cumu-\nlative summation. By separating the computation into intra-\nblock and inter-block components, we effectively leverage\nGPU hardware to its fullest potential, ensuring efficiency.\nOur extensive experiments across various model sizes and\nsequence lengths demonstrate that Lightning Attention-2\nnot only maintains consistent training speeds regardless of\ninput sequence length but also outperforms existing state-of-\nthe-art attention mechanisms in terms of speed and accuracy.\nThis breakthrough has profound implications for the future\nof large language models, particularly those requiring the\nprocessing of long sequences. Looking ahead, we intend to\nintroduce sequence parallelism in conjunction with Light-\nning Attention-2, which aims to facilitate the training of\nextra-long sequences, effectively overcoming existing hard-\nware constraints.\n8\nLightning Attention-2\nAcknowledgement\nThis work is partially supported by the National Key\nR&D Program of China (NO.2022ZD0160100). We thank\nSonglin Yang for the helpful discussions.\nReferences\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H.,\nO\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\nPrashanth, U. S., Raff, E., Skowron, A., Sutawika, L.,\nand van der Wal, O. Pythia: A suite for analyzing large\nlanguage models across training and scaling, 2023.\nBisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.\nPiqa: Reasoning about physical commonsense in natural\nlanguage, 2019.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChen, S., Wong, S., Chen, L., and Tian, Y. Extending\ncontext window of large language models via positional\ninterpolation, 2023.\nChi, T.-C., Fan, T.-H., Ramadge, P. J., and Rudnicky, A. I.\nKerple: Kernelized relative positional embedding for\nlength extrapolation, 2022.\nChi, T.-C., Fan, T.-H., Rudnicky, A. I., and Ramadge, P. J.\nDissecting transformer length extrapolation via the lens\nof receptive field analysis, 2023.\nChoromanski, K., Likhosherstov, V., Dohan, D., Song, X.,\nGane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J., Mohiud-\ndin, A., Kaiser, L., Belanger, D., Colwell, L. J., and\nWeller, A. Rethinking attention with performers. ArXiv,\nabs/2009.14794, 2020.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,\nM., and Toutanova, K. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions, 2019.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge,\n2018.\nDao, T.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691, 2023.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. FlashAt-\ntention: Fast and memory-efficient exact attention with\nIO-awareness. In Advances in Neural Information Pro-\ncessing Systems, 2022.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n12 2023. URL https://zenodo.org/records/\n10256836.\nHao, D., Mao, Y., He, B., Han, X., Dai, Y., and Zhong, Y.\nImproving audio-visual segmentation with bidirectional\ngeneration. In Proceedings of the AAAI Conference on\nArtificial Intelligence, 2024.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding, 2021.\nHua, W., Dai, Z., Liu, H., and Le, Q. V. Transformer quality\nin linear time. arXiv preprint arXiv:2202.10447, 2022.\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu,\nJ., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and He, J.\nC-eval: A multi-level multi-discipline chinese evaluation\nsuite for foundation models, 2023.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In International Conference on\nMachine Learning, pp. 5156\u20135165. PMLR, 2020a.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In Proceedings of the 37th Interna-\ntional Conference on Machine Learning, ICML 2020, 13-\n18 July 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research, pp. 5156\u20135165. PMLR,\n2020b. URL http://proceedings.mlr.press/\nv119/katharopoulos20a.html.\nKe, G., He, D., and Liu, T.-Y. Rethinking positional en-\ncoding in language pre-training. In International Confer-\nence on Learning Representations, 2021. URL https:\n//openreview.net/forum?id=09-528y2Fgf.\nLi, J., Li, D., Xiong, C., and Hoi, S. BLIP: Bootstrapping\nlanguage-image pre-training for unified vision-language\nunderstanding and generation. In Chaudhuri, K., Jegelka,\nS., Song, L., Szepesvari, C., Niu, G., and Sabato, S.\n(eds.), Proceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings of\nMachine Learning Research, pp. 12888\u201312900. PMLR,\n17\u201323 Jul 2022. URL https://proceedings.mlr.\npress/v162/li22n.html.\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders\nand large language models. In arXiv, 2023a.\n9\nLightning Attention-2\nLi, W., Li, D., Li, W., Wang, Y., Jie, H., and Zhong, Y. MAP:\nLow-data regime multimodal learning with adapter-based\npre-training and prompting. In Breitholtz, E., Lappin,\nS., Loaiciga, S., Ilinykh, N., and Dobnik, S. (eds.), Pro-\nceedings of the 2023 CLASP Conference on Learning\nwith Small Data (LSD), pp. 185\u2013190, Gothenburg, Swe-\nden, September 2023b. Association for Computational\nLinguistics. URL https://aclanthology.org/\n2023.clasp-1.19.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. In arXiv, 2023.\nLu, K., Liu, Z., Wang, J., Sun, W., Qin, Z., Li, D., Shen, X.,\nDeng, H., Han, X., Dai, Y., and Zhong, Y. Linear video\ntransformer with feature fixation, 2022.\nMao, Y., Zhang, J., Xiang, M., Zhong, Y., and Dai, Y. Mul-\ntimodal variational auto-encoder based audio-visual seg-\nmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pp. 954\u2013965,\nOctober 2023.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a\nsuit of armor conduct electricity? a new dataset for open\nbook question answering, 2018.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information\nprocessing systems, 32, 2019.\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\nS., Biderman, S., Cao, H., Cheng, X., Chung, M., Der-\nczynski, L., Du, X., Grella, M., Gv, K., He, X., Hou,\nH., Kazienko, P., Kocon, J., Kong, J., Koptyra, B., Lau,\nH., Lin, J., Mantri, K. S. I., Mom, F., Saito, A., Song,\nG., Tang, X., Wind, J., Wo\u00b4zniak, S., Zhang, Z., Zhou,\nQ., Zhu, J., and Zhu, R.-J. RWKV: Reinventing RNNs\nfor the transformer era. In Bouamor, H., Pino, J., and\nBali, K. (eds.), Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023, pp. 14048\u201314077, Sin-\ngapore, December 2023. Association for Computational\nLinguistics.\ndoi:\n10.18653/v1/2023.findings-emnlp.\n936. URL https://aclanthology.org/2023.\nfindings-emnlp.936.\nPeng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,\nN. A., and Kong, L.\nRandom feature attention.\nIn\n9th International Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net, 2021. URL https://openreview.\nnet/forum?id=QtTKTdVrFBB.\nPress, O., Smith, N., and Lewis, M. Train short, test long:\nAttention with linear biases enables input length extrapo-\nlation. In International Conference on Learning Represen-\ntations, 2022. URL https://openreview.net/\nforum?id=R8sQPpGCv0.\nQin, Z., Han, X., Sun, W., Li, D., Kong, L., Barnes, N.,\nand Zhong, Y. The devil in linear transformer. In Pro-\nceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pp. 7025\u20137041, Abu\nDhabi, United Arab Emirates, December 2022a. Associ-\nation for Computational Linguistics. URL https://\naclanthology.org/2022.emnlp-main.473.\nQin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B.,\nYan, J., Kong, L., and Zhong, Y. cosformer: Rethink-\ning softmax in attention. In International Conference\non Learning Representations, 2022b.\nURL https:\n//openreview.net/forum?id=Bl8CQrx2Up4.\nQin, Z., Han, X., Sun, W., He, B., Li, D., Li, D., Dai, Y.,\nKong, L., and Zhong, Y. Toeplitz neural network for se-\nquence modeling. In The Eleventh International Confer-\nence on Learning Representations, 2023a. URL https:\n//openreview.net/forum?id=IxmWsm4xrua.\nQin, Z., Li, D., Sun, W., Sun, W., Shen, X., Han, X., Wei,\nY., Lv, B., Yuan, F., Luo, X., et al. Scaling transnormer to\n175 billion parameters. arXiv preprint arXiv:2307.14995,\n2023b.\nQin, Z., Sun, W., Lu, K., Deng, H., Li, D., Han, X., Dai, Y.,\nKong, L., and Zhong, Y. Linearized relative positional\nencoding. Transactions on Machine Learning Research,\n2023c.\nQin, Z., Yang, S., and Zhong, Y. Hierarchically gated recur-\nrent neural network for sequence modeling. In NeurIPS,\n2023d.\nQin, Z., Zhong, Y., and Deng, H. Exploring transformer\nextrapolation. In Proceedings of the AAAI Conference on\nArtificial Intelligence, 2024.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision.\nIn\narXiv, 2021.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale, 2019.\nSap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y.\nSocialiqa: Commonsense reasoning about social interac-\ntions, 2019.\n10\nLightning Attention-2\nShen, X., Li, D., Zhou, J., Qin, Z., He, B., Han, X., Li, A.,\nDai, Y., Kong, L., Wang, M., Qiao, Y., and Zhong, Y. Fine-\ngrained audible video description. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 10585\u201310596, June 2023.\nSu, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer:\nEnhanced transformer with rotary position embedding.\narXiv preprint arXiv:2104.09864, 2021.\nSun, W., Qin, Z., Deng, H., Wang, J., Zhang, Y., Zhang,\nK., Barnes, N., Birchfield, S., Kong, L., and Zhong, Y.\nVicinity vision transformer. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 45(10):12635\u201312649,\n2023a. doi: 10.1109/TPAMI.2023.3285569.\nSun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J.,\nWang, J., and Wei, F. Retentive network: A successor to\ntransformer for large language models, 2023b.\nTillet, P., Kung, H.-T., and Cox, D. D. Triton: an inter-\nmediate language and compiler for tiled neural network\ncomputations. Proceedings of the 3rd ACM SIGPLAN\nInternational Workshop on Machine Learning and Pro-\ngramming Languages, 2019.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\nand Scialom, T. Llama 2: Open foundation and fine-tuned\nchat models, 2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nXiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef-\nficient streaming language models with attention sinks,\n2023.\nYang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated\nlinear attention transformers with hardware-efficient train-\ning, 2023.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\nHellaswag: Can a machine really finish your sentence?,\n2019.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mi-\nhaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D.,\nKoura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,\nL. Opt: Open pre-trained transformer language models,\n2022.\nZheng, L., Wang, C., and Kong, L. Linear complexity ran-\ndomized self-attention mechanism. In International Con-\nference on Machine Learning, pp. 27011\u201327041. PMLR,\n2022.\nZheng, L., Yuan, J., Wang, C., and Kong, L. Efficient\nattention via control variates. In International Conference\non Learning Representations, 2023. URL https://\nopenreview.net/forum?id=G-uNfHKrj46.\nZhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J.,\nBirchfield, S., Guo, D., Kong, L., Wang, M., and Zhong,\nY. Audio-visual segmentation with semantics, 2023.\n11\n"
  },
  {
    "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
    "link": "https://arxiv.org/pdf/2401.04398.pdf",
    "upvote": "17",
    "text": "Published as a conference paper at ICLR 2024\nCHAIN-OF-TABLE: EVOLVING TABLES IN THE\nREASONING CHAIN FOR TABLE UNDERSTANDING\nZilong Wang1\u2217\nHao Zhang3\nChun-Liang Li2\nJulian Martin Eisenschlos3\nVincent Perot3\nZifeng Wang2\nLesly Miculicich2\nYasuhisa Fujii3\nJingbo Shang1\nChen-Yu Lee2\nTomas Pfister2\n1University of California, San Diego\n2Google Cloud AI Research\n3Google Research\nABSTRACT\nTable-based reasoning with large language models (LLMs) is a promising di-\nrection to tackle many table understanding tasks, such as table-based question\nanswering and fact verification. Compared with generic reasoning, table-based\nreasoning requires the extraction of underlying semantics from both free-form\nquestions and semi-structured tabular data. Chain-of-Thought and its similar ap-\nproaches incorporate the reasoning chain in the form of textual context, but it\nis still an open question how to effectively leverage tabular data in the reason-\ning chain. We propose the CHAIN-OF-TABLE framework, where tabular data is\nexplicitly used in the reasoning chain as a proxy for intermediate thoughts. Specif-\nically, we guide LLMs using in-context learning to iteratively generate operations\nand update the table to represent a tabular reasoning chain. LLMs can therefore\ndynamically plan the next operation based on the results of the previous ones.\nThis continuous evolution of the table forms a chain, showing the reasoning pro-\ncess for a given tabular problem. The chain carries structured information of the\nintermediate results, enabling more accurate and reliable predictions. CHAIN-\nOF-TABLE achieves new state-of-the-art performance on WikiTQ, FeTaQA, and\nTabFact benchmarks across multiple LLM choices.\n1\nINTRODUCTION\nTables are a popular data format and widely used in daily life (Cafarella et al., 2008). Understanding\ntabular data with language models can benefit various downstream tasks, such as table-based fact\nverification (Chen et al., 2019), and table-based question answering (Jin et al., 2022). Distinct\nfrom pure text, tables deliver rich information through the interaction between rows and columns in\nthe tabular structure, which enhances the data capacity but also increases the difficulty for language\nmodels to understand them. Thus, reasoning over the tabular data is an important direction in natural\nlanguage processing and attracts increasing attention from both academia and industry.\nIn recent years, several approaches have been suggested to tackle the problem of table understand-\ning by training language models. One common direction is to add specialized embedding layers\nor attention mechanisms into language models and pre-train the models by recovering table cells or\nsegments (Herzig et al., 2020; Wang et al., 2021; Gu et al., 2022; Andrejczuk et al., 2022). In this\nway, the pre-trained models are aware of the tabular structure. Another direction is to synthesize\nSQL query-response pairs and pre-train an encoder-decoder model as a neural SQL executor (Eisen-\nschlos et al., 2020; Liu et al., 2021; Jiang et al., 2022).\nRecently, large language models (LLMs) achieve outstanding performance across diverse tasks\nsolely by prompting, thanks to the massive scale of pre-training (Brown et al., 2020; Kojima et al.,\n2022). As series of works on prompting techniques have further improved the reliability of LLMs\nby designing reasoning chains, such as Chain-of-Thought (Wei et al., 2022), Least-to-Most (Zhou\net al., 2022), Program-of-Thought (Chen et al., 2022) and Tree-of-Thought (Yao et al., 2023). Dif-\nferent works have also explored the possibility of using LLMs to solve table-based problems (Chen,\n\u2217Work done while the author was a student researcher at Google Cloud AI Research. Correspondence to:\nZilong Wang <zlwang@ucsd.edu>, Chen-Yu Lee <chenyulee@google.com>\n1\narXiv:2401.04398v2  [cs.CL]  19 Jan 2024\nPublished as a conference paper at ICLR 2024\n(a) Generic Reasoning \n[Original Table] \n[Question] \nWhich country had the \nmost cyclists \ufb01nish with in \nthe top 3?\n2\nDavide (ITA)\n1\nAlejandro (ESP)\nRank\nCyclist\n3\nPaolo (ITA)\n4\nHaimar (ESP)\nLLM\n(b) Program-aided Reasoning \nSQL: SELECT Country FROM \ntable WHERE Rank<=3 GROUP \nBY Country ORDER BY \nCOUNT(*) DESC LIMIT 1\n(c) Chain-of-Table (ours)\nf_add_col()\nLLM\nStep 1: Sample next operation based on \nTable, Question, Operation History\nStep 2: Generate arguments \nfor the sampled operation\nLLM\nAdded Col Header = \"Country\"\nAdded Col Cells = \"ESP,ITA,\u2026\"\nInput Prompt\nMultiple Reasoning Steps in Generic \nReasoning fails to solve the complex table.  \nQuestion asks cyclists in top 3 but Haimar is \nnot in top 3.\nRank\nCyclist\n4\nHaimar (ESP)\nLLM\nGenerated Programs in Program-aided \nReasoning fails to solve the complex table.\nSQL can\u2019t execute the query since \"Country\" is \nin the same cell with \"Name\".\nCyclist\nAlejandro (ESP)\nOP\nPool\nInput Prompt (next iteration)\n1\nAlej.\nRank Cyc.\nESP\nCountry\nf_add_col()\n[Intermediate Table]\n[Operation History]\n[Question] Which country \u2026 in the top 3?\nStep 3: \nTransform table to \nstore the tabular \nreasoning process\nIteratively repeat Step 1, 2, 3 with the \nintermediate table & the operation history\nf_add_col(Country)\nf_select_row(1,2,3)\nf_group_by(Country)\nf_sort_by(Count)\nITA\n2\nCountry\nCount\nESP\n1\nItaly \u2713\n[Q] Which country had \nthe most cyclists \ufb01nish \nwith in the top 3?\nIter 1:\nIter 2:\nIter 3:\nIter 4:\nFinal Query Prompt\nComplete Operation History\nRepresent Tabular Reasoning Chain\nThere are 2 cyclists from Spain.\nThey are Alejandro and Haimar.\nThe answer is Spain.\nFigure 1: Illustration of the comparison between (a) generic reasoning, (b) program-aided reason-\ning, and (c) the proposed CHAIN-OF-TABLE. Given a complex table where a cyclist\u2019s nationality\nand name are in the same cell, (a) is unable to provide the correct answer through multi-step rea-\nsoning due to the complexity; (b) generates and executes programs (e.g. SQL queries) to deliver\nthe answer, but it also falls short in accurately parsing the name and nationality in the table. In\ncontrast, (c) CHAIN-OF-TABLE iteratively samples a chain of operations that effectively transform\nthe complex table into a version specifically tailored to the question. With the assistance of CHAIN-\nOF-TABLE, the LLM can arrive at the correct answer.\n2023; Cheng et al., 2022; Ye et al., 2023). However, these approaches (Hsieh et al., 2023) often rep-\nresent reasoning steps in free-form text or code, which are not ideally suited for addressing scenarios\ninvolving complex tables, as shown in Figure 1(a) and Figure 1(b).\nOn the other hand, inference on tables typically involves a series of intermediate reasoning steps and\neach of them aligns with specific tabular operations. We propose CHAIN-OF-TABLE, where we con-\nduct step-by-step reasoning as step-by-step tabular operations to form a chain of tables. The tables\nin the chain are the transformed tables by the tabular operations, representing the intermediate rea-\nsoning results. This procedure resembles the thought of reasoning in Chain-of-Thought (Wei et al.,\n2022). Specifically, we define a set of table operations, such as adding columns, selecting rows,\ngrouping, and more, which are commonly-used in SQL and DataFrame development (P\u00f6nighaus,\n1995; Shi et al., 2020; Katsogiannis-Meimarakis & Koutrika, 2023). We then prompt LLMs to con-\nduct step-by-step reasoning. In each step, the LLM dynamically generates an operation as the next\nstep along with its required arguments, and then we execute the operation on the table programmat-\nically. This operation can either enrich the table by adding detailed intermediate results or condense\nit by removing irrelevant information. Intuitively, visualizing the intermediate results is essential for\nreaching correct predictions. We feed the transformed table back for the next step. This iterative\nprocess continues until an ending state is achieved. We argue that the tables obtained during the rea-\nsoning steps are better structured representations of the intermediate thoughts than free-form text.\nFinally, the CHAIN-OF-TABLE reasoning results in tables from which it is easier for LLMs to derive\na final answer to the question.\nWe validate CHAIN-OF-TABLE with three tabular benchmarks to evaluate table-based reasoning:\nWikiTQ (Pasupat & Liang, 2015), TabFact (Chen et al., 2019), and FeTaQA (Nan et al., 2022). We\nconduct our experiments using proprietary PaLM 2 (Anil et al., 2023) and GPT-3.5 (Brown et al.,\n2020; OpenAI, 2023), and open-sourced LLaMA 2 (Touvron et al., 2023), to demonstrate that our\nproposed method CHAIN-OF-TABLE is able to generalize to various LLM options. We summarize\nour contribution as follows:\n2\nPublished as a conference paper at ICLR 2024\n\u2022 We extend the concept of Chain-of-Thought to the tabular setting, where we transform the input\ntable to store intermediate results. This multi-step tabular reasoning approach with table evolution\nleads to more accurate table understanding.\n\u2022 Extensive experiments on table-based fact verification and question answering show that CHAIN-\nOF-TABLE archives state-of-the-art performance in WikiTQ, TabFact, and FeTaQA datasets.\n2\nRELATED WORK\nFine-tuning Language Model for Table Understanding\nTables are effective in organizing, stor-\ning, and analyzing information. Efforts have been made to fine-tune language models (LMs) to\ntackle table understanding tasks. Following the successful mask language modeling (MLM) pro-\nposed in BERT (Devlin et al., 2019), TaPas (Herzig et al., 2020) adopts this approach and asks\nthe model to reconstruct certain cells in the table during pre-training. Pasta (Gu et al., 2022) and\nTUTA (Wang et al., 2021) further propose to mask the entire columns or segments in the table. On\nthe other hand, TAPEX (Liu et al., 2021) pre-trains an encoder-decoder model with a large synthetic\nSQL dataset so that it can perform as a SQL executor to better understand the tabular structure.\nEisenschlos et al. (2020) and Jiang et al. (2022) also leverage synthesized SQL with additional con-\nsideration of the alignment between SQL and natural language questions by pre-training the model\nwith both natural and synthetic data.\nPrompting Language Model for Table Understanding\nLLMs can learn from a few samples\nas prompts through in-context learning. This strategy is widely used to give models additional\ninstructions to better solve downstream tasks. Chain-of-Thought (CoT) (Wei et al., 2022) proposes\nto generate reasoning steps before answering instead of directly generating an end-to-end answer.\nFollowing CoT, Least-to-Most (Zhou et al., 2022) and DecomP (Khot et al., 2022) propose to break\ndown the question into subproblems in the reasoning chain. During reasoning, the latter steps are\naware of the previous ones. Such iterative chains with task decomposition further improve the results\non complex problems by leveraging the intermediate results from solving subproblems. Jin & Lu\n(2023) enhances CoT through a table-filling procedure, with a primary focus on text-based tasks\nwhere the input and output are in textual format. However, the line of works following CoT is not\nspecifically designed for tabular data. As reported in Chen (2023), large language models with these\ngeneric reasoning methods can achieve decent results, but there are still gaps between these methods\nand those specialized for table scenarios (Cheng et al., 2022; Ye et al., 2023). We propose CHAIN-\nOF-TABLE to fill the gap by directly incorporating intermediate tables from tabular operations as a\nproxy of intermediate thoughts.\nTo better solve table-based tasks with LLMs, researchers go beyond general text and resort to using\nexternal tools. Chen et al. (2022); Gao et al. (2023) propose solving reasoning tasks by generating\nPython programs, which are then executed using the Python interpreter. This approach greatly\nimproves the performance of arithmetic reasoning. In the scenario of table understanding, Text-to-\nSQL with LLMs (Rajkumar et al., 2022) is a straightforward application of this idea. To further\npush the limits of programs, Binder (Cheng et al., 2022) generates SQL or Python programs and\nextends their capabilities by calling LLMs as APIs in the programs. LEVER (Ni et al., 2023) also\nproposes solving the table-based tasks with programs but with the additional step of verifying the\ngenerated programs with their execution results. However, the assistant programs in these program-\naided methods still fall short in solving difficult cases that involve complex tables. These limitations\nare primarily due to the constraints of the single-pass generation process, where the LLMs lack the\ncapability to modify the table in response to a specific question, requiring them to perform reasoning\nover a static table. Our method, on the contrary, is a multi-step reasoning framework that conducts\ntabular reasoning step by step. It transforms the tables tailored to the given question.\nTo the best of our knowledge, Dater (Ye et al., 2023) is the only model that modifies the tabular\ncontext while solving table-based tasks. However, the table decomposition in Dater is motivated by\nthe idea that tables could be too large for LLMs to conduct reasoning. It is, therefore, more similar to\nan LLM-aided data pre-processing than to a part of the reasoning chain since the tabular operations\nare limited to column and row selections, and fixed for all tables and questions. In contrast, our\nCHAIN-OF-TABLE generalizes a larger set of generic table operations and dynamically generates\nreasoning chains in an adaptive way based on the inputs, leveraging the planning ability (Valmeekam\net al., 2022; Hao et al., 2023) of LLMs.\n3\nPublished as a conference paper at ICLR 2024\n3\nCHAIN-OF-TABLE REASONING\nProblem Formulation.\nIn table-based reasoning, each entry can be represented as a triplet\n(T, Q, A), where T stands for the table, Q represents a question or statement related to the ta-\nble, and A is the expected answer. Particularly, in the table-based question answering task, Q and A\nare the question and expected answer in natural language form; in the table-based fact verification\ntask, Q is a statement about the table contents and A \u2208 {True, False} is a Boolean value that\nindicates the statement\u2019s correctness. The objective is to predict the answer A given the question Q\nand the table T. To facilitate table-based reasoning within the same paradigm employed for generic\nreasoning, we convert all data values, including tables, into textual representations (see Appendix D\nfor the tabular format encoding method).\n3.1\nOVERVIEW\nCHAIN-OF-TABLE enables LLMs to dynamically plan a chain of operations over a table T in re-\nsponse to a given question Q. It utilizes atomic tool-based operations to construct the table chain.\nThese operations include adding columns, selecting rows or columns, grouping, and sorting, which\nare common in SQL and DataFrame development (see Appendix A for more details).\nPreviously, Dater (Ye et al., 2023) employs a dedicated yet fixed procedure for decomposing ta-\nbles and questions, which limits its compatibility with new operations. Also, Binder (Cheng et al.,\n2022), while potentially compatible with new operations, is restricted to those that work with code\ninterpreters such as SQL or Python. In contrast, our framework is extendable and can incorporate\noperations from a wide range of tools thanks to the flexible in-context learning capability to sample\nand execute effective operations.\nAs illustrated in Algorithm 1, at each iteration, we prompt the LLM to sample one of the pre-defined\natomic operations denoted as f using the corresponding question Q, the latest table state T, and\nthe operation chain chain (Line 4). Then, we query the LLM to generate the required arguments\nargs for f (Line 5) and execute it to transform the table T (Line 6). We keep track of the operation\nf performed on the table in the operation chain chain (Line 7). The process finishes when the\nending tag [E] is generated (Line 8). Finally, we feed the latest table into the LLM to predict the\nanswer (Line 9). This series of operations serves as the reasoning steps leading LLMs to understand\nthe input table and better generate the final answer.\nAlgorithm 1: CHAIN-OF-TABLE Prompting\nData: (T, Q) is a table-question pair.\nResult: \u02c6\nA is the predicted answer to the question.\n1 Function Chain-of-Table (T , Q):\n2\nchain \u2190 [([B],\u03d5), ]\n\u25b7 Initialize the operation chain chain with [B] and \u03d5, where [B] is\n\u25b7 the beginning tag, and \u03d5 means it requires no arguments\n3\nrepeat\n4\nf \u2190 DynamicPlan(T ,Q,chain)\n\u25b7 Generate next operation f based on the table, the question, and\n\u25b7 the current operation chain\n5\nargs \u2190 GenerateArgs(T ,Q,f)\n\u25b7 Generate the arguments args for the next operation\n6\nT \u2190 f(T ,args)\n\u25b7 Perform the next operation on the table to obtain updated T\n7\nchain \u2190 chain.append((f,args))\n\u25b7 Keep track of the operations in the operation chain chain\n8\nuntil f = [E]\n\u25b7 Iteratively update the table until the ending tag [E] is generated\n9\n\u02c6\nA \u2190 Query(T, Q)\n\u25b7 Query the LLM with the resulting table to get the final answer \u02c6\nA\n10 return \u02c6\nA\n3.2\nDYNAMIC PLANNING\nCHAIN-OF-TABLE instructs the LLM to dynamically plan the next operation by in-context learning.\nAs shown in Figure 2(a), DynamicPlan involves three components: the most recent intermediate\ntable T (Figure 2(a)(i)), the history of the previous operations chain chain (Figure 2(a)(ii)), and\nthe question Q (Figure 2(a)(iii)). We guide the LLM to select the subsequent operation f from the\noperation pool given (T, chain, Q). The LLM is then able to dynamically plan the next operation\nand build a tabular reasoning chain step by step. See Appendix E.1 for detailed prompts.\n4\nPublished as a conference paper at ICLR 2024\n\uff08a\uff09DynamicPlan (T, Q, chain)\n\uff08b\uff09GenerateArgs (T, Q, f)\n\u2026\n1\nRank\nLLM\n(i) Intermediate Table (T)\n\u2026\nAlej. (ESP)\nCyclist\n\u2026\nESP\nCountry\n(ii) Operation History (chain)\nf_add_col(Country)\nf_select_row(1,2,3)\nIter 1:\nIter 2:\nWhich country had the \nmost cyclists \ufb01nish with \nin the top 3?   \n(iii) Question (Q)\nSample the next operation \nbased on T, Chain, Q\nf_group_by\n4\nSelected Operation\nInput Prompt\nOperation Pool\nf_select_col\nf_group_by\nf_sort_by\nf_add_col\nf_select_row\n1\n2\n3\n4\n5\nInput Prompt\nLLM\n3\nSelected Argument\nCountry\n(ii) Current Operation (f) & \n     Its Arguments (args)\nf_group_by\nHeader to group by = ?\nWhich country had the \nmost cyclists \ufb01nish with \nin the top 3?   \n(iii) Question (Q)\n(i) Intermediate Table (T)\nSample the argument \nbased on T, f, Q\nArguments: Table \nheaders in this case\nRank\n1\nCyclist\n2\nCountry\n3\n\u2026\n\u2026\n1\nAlej. (ESP)\nRank\nCyclist\n\u2026\nESP\nCountry\nargs :\nf :\nFigure 2: Illustration of DynamicPlan(T,Q,chain) and GenerateArgs(T,Q,f) in the\nproposed CHAIN-OF-TABLE, where T is a intermediate table; Q is the question; chain is a list\nof operations already performed on the table; f is the operation selected by DynamicPlan. Left:\nDynamicPlan samples the next operation from the operation pool, according to (T, chain, Q).\nRight: GenerateArgs takes the selected operation f as input and generates its arguments based\non (T, f, Q). The operations, along with their arguments, act as a proxy of the tabular reasoning\nprocess to effectively tackle table understanding tasks.\n3.3\nARGUMENT GENERATION\nThe next step, GenerateArgs, involves generating arguments for the selected table operation f\nsampled by DynamicPlan, as depicted in Figure 2. GenerateArgs involves three key com-\nponents: the most recent intermediate table T (Figure 2(b)(i)), the selected operation f along with\nits arguments args (Figure 2(b)(ii)), and the question (Figure 2(b)(iii)). We employ simple regular\nexpressions to account for varying number of arguments required by different operations (see Ap-\npendix E.2 for more details). Finally, we apply programming languages to execute the operation and\ncreate the corresponding intermediate tables.\n3.4\nFINAL QUERY\nWe transform the table through dynamic planning (Section 3.2) and argument generation (Section\n3.3). During this process, we create a chain of operations that acts as a proxy for the tabular reason-\ning steps. These operations generate intermediate tables that store and present the results of each\nstep to the LLM. Consequently, the output table from this chain of operations contains comprehen-\nsive information about the intermediate phases of tabular reasoning. We then employ this output\ntable in formulating the final query. As illustrated in Figure 1 (bottom right), we input both the\noutput table and the question into the LLM, which provides the final answer to the question (see\nLine 9 in Algorithm 1).\n4\nEXPERIMENTS\nWe evaluate the proposed CHAIN-OF-TABLE on three public table understanding benchmarks: Wik-\niTQ (Pasupat & Liang, 2015), FeTaQA (Nan et al., 2022), and TabFact (Chen et al., 2019). WikiTQ\nand FeTaQA are datasets focused on table-based question answering. They require complex tab-\nular reasoning over the provided table to answer questions. WikiTQ typically requires short text\nspan answers, whereas FeTaQA demands longer, free-form responses. TabFact, on the other hand,\nis a table-based binary fact verification benchmark. The task is to ascertain the truthfulness of a\ngiven statement based on the table. For WikiTQ evaluation, we use the official denotation accu-\nracy (Pasupat & Liang, 2015), and for TabFact, we employ the binary classification accuracy. Given\nthe nature of FeTaQA, which involves comparing predictions with longer target texts, we utilize\nBLEU (Papineni et al., 2002), ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) for assessment.\nIn our experiments, we use PaLM 2-S1, GPT 3.5 (turbo-16k-0613)2, and LLaMA 2 (Llama-2-17B-\n1https://cloud.google.com/vertex-ai/docs/generative-ai/learn/generative-ai-studio\n2http://openai.com/api/\n5\nPublished as a conference paper at ICLR 2024\nTable 1: Table understanding results on WikiTQ and TabFact with PaLM 2, GPT 3.5, and LLaMA 2.\n(underline denotes the second-best performance; bold denotes the best performance; the improve-\nment is measured against the second-best performing method.)\nPrompting\nPaLM 2\nGPT 3.5\nLLaMA 2\nTabFact\nWikiTQ\nTabFact\nWikiTQ\nTabFact\nWikiTQ\nGeneric Reasoning\nEnd-to-End QA\n77.92\n60.59\n70.45\n51.84\n44.86\n23.90\nFew-Shot QA\n78.06\n60.33\n71.54\n52.56\n62.01\n35.52\nChain-of-Thought (Wei et al., 2022) 79.05\n60.43\n65.37\n53.48\n60.52\n36.05\nProgram-aided Reasoning\nText-to-SQL (Rajkumar et al., 2022) 68.37\n52.42\n64.71\n52.90\n64.03\n36.14\nBinder (Cheng et al., 2022)\n76.98\n54.88\n79.17\n56.74\n62.76\n30.92\nDater (Ye et al., 2023)\n84.63\n61.48\n78.01\n52.81\n65.12\n41.44\nCHAIN-OF-TABLE (ours)\n86.61 (+1.98) 67.31 (+5.83) 80.20 (+1.03) 59.94 (+3.20) 67.24 (+2.12) 42.61 (+1.17)\nchat)3 as the backbone LLMs. We incorporate few-shot demo samples from the training set into the\nprompts to perform in-context learning. Examples of these prompts can be found in Appendix E.\nDetails regarding the LLM inference parameters and the number of demonstration samples used are\nprovided in Appendix C.\n4.1\nBASELINES\nThe baseline methods are categorized into two groups: (a) generic reasoning, which includes End-\nto-End QA, Few-Shot QA, Chain-of-Thought (Wei et al., 2022); and (b) program-aided reasoning,\nwhich includes Text-to-SQL (Rajkumar et al., 2022), Binder (Cheng et al., 2022), Dater (Ye et al.,\n2023)). Detailed descriptions of these baseline methods are provided below.\nGeneric Reasoning\nEnd-to-End QA guides the LLM to directly produce the answer when pro-\nvided with a table and a question as input prompts. Few-Shot QA operates similarly, but it includes\nfew-shot examples of (Table, Question, Answer) triplets in the prompt, as detailed in Brown et al.\n(2020). We select these examples from the training set, and the model also outputs the answer di-\nrectly. Chain-of-Thought (Wei et al., 2022) prompts the LLM to articulate its reasoning process in\ntext format before delivering the question. See Appendix F for the prompts of baselines.\nProgram-aided Reasoning\nText-to-SQL (Rajkumar et al., 2022) utilizes in-context samples to\nguide LLMs in generating SQL queries for answering questions. This approach follows the concepts\nintroduced by Chen et al. (2022); Gao et al. (2023). Binder (Cheng et al., 2022) integrates a language\nmodel API with programming languages such as SQL or Python. This integration prompts the LLM\nto produce executable programs that perform table reasoning tasks on the given table and question.\nDater (Ye et al., 2023) employs few-shot samples for efficient deconstruction of table contexts and\nquestions, enhancing end-to-end table reasoning with decomposed sub-tables and sub-questions.\n4.2\nRESULTS\nWe compare CHAIN-OF-TABLE with generic reasoning methods and program-aided reasoning\nmethods on three datasets: WikiTQ, TabFact, and FeTaQA. The results on WikiTQ and TabFact\nare presented in Table 1. We have additional results on FeTaQA in Appendix B. We follow the\nprevious works and report the performance using the official evaluation pipeline4.\nTable 1 shows that CHAIN-OF-TABLE significantly outperforms all generic reasoning methods and\nprogram-aided reasoning methods on TabFact and WikiTQ across PaLM 2, GPT 3.5, and LLaMA\n3https://ai.meta.com/llama/\n4Dater Ye et al. (2023) with OpenAI Codex LLM achieves 65.9% and 85.6% accuracy on WikiTQ and\nTabFact, respectively. It also achieves 27.96 in BLEU, 0.62 in ROUGE-1, 0.40 in ROUGE-2, and 0.52 in\nROUGE-L on FeTaQA. However, because Codex is no longer publicly available, we do not compare CHAIN-\nOF-TABLE with Dater with Codex.\n6\nPublished as a conference paper at ICLR 2024\n1\n2\n3\n4\n5\nLength of Operation Chain\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n(-11.6)\n(-3.7)\n(-7.2)\n(-7.5)\n(-10.8)\n40.0\n69.3\n62.3\n53.5\n49.3\n(-2.1)\n(-5.3)\n(-7.6)\n(-3.6)\n(-7.9)\n49.5\n67.7\n61.9\n57.4\n52.2\n51.6\n73.0\n69.5\n61.0\n60.1\nChain-of-Thought (Wei et al. 2022)\nDater (Ye et al. 2023)\nChain-of-Table (ours)\nFigure 3: Performance of Chain-of-Thought, Dater, and the proposed CHAIN-OF-TABLE on Wik-\niTQ for questions that require an operation chain of varying lengths. Our proposed atomic operations\nallow our proposed method CHAIN-OF-TABLE to dynamically transform the input table through\nmultiple reasoning iterations. This significantly improves performance over generic and program-\naided reasoning counterparts.\nTable 2: Distribution of the number of samples v.s. the required length of operation chain in CHAIN-\nOF-TABLE with PaLM 2 on WikiTQ and TabFact datasets. We observe that the majority of samples\nneed 2 to 4 operations to generate the final output.\nDataset\nLength of operation chain\n1\n2\n3\n4\n5\nWikiTQ\n95\n1308\n1481\n1084\n341\nTabFact\n4\n547\n732\n517\n223\n2. This is attributed to the dynamically sampled operations and the informative intermediate tables\nin CHAIN-OF-TABLE. CHAIN-OF-TABLE iteratively generates operations that act as proxies for\ntabular reasoning steps. These operations produce and present tailored intermediate tables to the\nLLM, conveying essential intermediate thoughts (see the example in Figure 4). With the support of\nCHAIN-OF-TABLE, the LLM can reliably reach the correct answer.\nFrom the results, we observe a performance decrease on WikiTQ due to the complexity of tabular\nstructure when vanilla Chain-of-Thought is introduced to End-to-End QA using PaLM 2. In contrast,\nour proposed CHAIN-OF-TABLE consistently enhances End-to-End QA performance by 8.69% on\nTabFact and 6.72% on WikiTQ with PaLM 2.\nWe also observe that our proposed CHAIN-OF-TABLE is effective across all backbone models ex-\nperimented, while other competing methods, such as Binder, perform better on larger LLMs but\nits performance decreases with smaller LLaMA 2 (Llama-2-17B-chat). We attribute this decline to\nBinder\u2019s single-pass generation process. While Binder does incorporate API calls within its frame-\nwork, it lacks the capability to modify and observe the transformed tables. Consequently, Binder can\nonly perform the tabular reasoning over a static table, making it challenging to solve complicated\ncases with smaller LLMs.\n4.3\nPERFORMANCE ANALYSIS UNDER DIFFERENT OPERATION CHAIN LENGTHS\nIn CHAIN-OF-TABLE, the selection of each operation is dynamically determined based on the dif-\nficulty and complexity of the questions and their corresponding tables. Therefore, we conduct a\ndetailed study on the performance under different numbers of operations by categorizing the test\nsamples according to their operation lengths. We report the distribution of the number of samples\nv.s. the required length of operation chain in Table 2. This analysis focuses on samples that require\noperations in the reasoning process. We use the results with PaLM 2 as an example. Our observa-\ntions reveal that the majority of samples require 2 to 4 operations to generate the final output.\nFor each chain length, we further compare CHAIN-OF-TABLE with Chain-of-Thought and Dater, as\nrepresentative generic and program-aided reasoning methods, respectively. We illustrate this using\nresults from PaLM 2 on WikiTQ. We plot the accuracy of all methods using bar charts in Figure 3,\n7\nPublished as a conference paper at ICLR 2024\nTable 3: Performance of Binder, Dater, and the proposed CHAIN-OF-TABLE on small (<2000 to-\nkens), medium (2000 to 4000 tokens), large (>4000 tokens) tables from WikiTQ. We observe that\nthe performance decreases with larger input tables while CHAIN-OF-TABLE diminishes gracefully,\nachieving significant improvements over competing methods. (underline denotes the second-best\nperformance; bold denotes the best performance; the improvement is measured against the second-\nbest performing method.)\nPrompting\nTable Size\nSmall (<2k) Medium (2k\u223c4k) Large (>4k)\nBinder (Cheng et al., 2022) 56.54\n26.13\n6.41\nDater (Ye et al., 2023)\n62.50\n42.34\n34.62\nCHAIN-OF-TABLE (ours)\n68.13 (+5.63) 52.25 (+9.91)\n44.87 (+10.25)\nTable 4: Number of samples generated for a single question in Binder, Dater, and the proposed\nCHAIN-OF-TABLE on the WikiTQ dataset. Notably, CHAIN-OF-TABLE generates the fewest sam-\nples among the baselines \u2013 50% less than Binder and 75% less than Dater. For a detailed description\nof the steps involved in Binder and Dater, please refer to the corresponding papers.\nPrompting\nTotal # of\n# of generated samples\ngenerated samples\nin each steps\nBinder (Cheng et al., 2022)\n50\nGenerate Neural-SQL: 50\nDater (Ye et al., 2023)\n100\nDecompose Table: 40; Generate Cloze: 20;\nGenerate SQL: 20; Query: 20\nCHAIN-OF-TABLE (ours)\n\u226425\nDynamicPlan: \u22645; GenerateArgs: \u226419;\nQuery: 1\nhighlighting the gap between the compared methods and our method. Notably, CHAIN-OF-TABLE\nconsistently surpasses both baseline methods across all operation chain lengths, with a significant\nmargin up to 11.6% compared with Chain-of-Thought, and up to 7.9% compared with Dater.\nGenerally, the performance of these methods decreases as the number of tabular operations required\nin the tabular reasoning chain increases due to higher difficulty and complexity of questions and ta-\nbles. Nevertheless, our proposed CHAIN-OF-TABLE declines gracefully compared to other baseline\nmethods. For example, CHAIN-OF-TABLE exhibits only a minimal decrease in performance when\nthe number of operations increases from four to five.\n4.4\nPERFORMANCE ANALYSIS UNDER DIFFERENT TABLE SIZES\nLarge tables present significant challenges to LLMs since LLMs often struggle to interpret and in-\ntegrate contexts in long input prompts (Liu et al., 2023a; Ye et al., 2023). To assess the performance\non tables of various sizes, we categorize the input tables from WikiTQ into 3 groups based on token\ncount: small (<2000 tokens), medium (2000 to 4000 tokens) and large (>4000 tokens). We then\ncompare CHAIN-OF-TABLE with Dater (Ye et al., 2023) and Binder (Cheng et al., 2022), the two\nlatest and strongest baselines, as representative methods. Detailed results are presented in Table 3.\nAs anticipated, the performance decreases with larger input tables, as models are required to pro-\ncess and reason through longer contexts. Nevertheless, the performance of the proposed CHAIN-\nOF-TABLE diminishes gracefully, achieving a significant 10+% improvement over the second best\ncompeting method when dealing with large tables. This demonstrates the efficacy of the reasoning\nchain in handling long tabular inputs.\n4.5\nEFFICIENCY ANALYSIS OF CHAIN-OF-TABLE\nWe analyze the efficiency of CHAIN-OF-TABLE by evaluating the number of required generated\nsamples. We compare CHAIN-OF-TABLE with Binder (Cheng et al., 2022) and Dater (Ye et al.,\n2023), the two latest and most competitive baseline method. The analysis results on WikiTQ are\npresented in Table 4. Binder generates Neural-SQL queries, requiring 50 samples for self-consistent\n8\nPublished as a conference paper at ICLR 2024\nTherefore, \nthe answer is: \nMorgan Freeman\n1997 Samuel L. Jackson\nA Time to Kill\nBlair Underwood\u2026\n1996 Laurence Fishburne Higher Learning\nCharles Dutton\u2026\n1995\nAl Freeman, Jr.\nMalcolm X\nDelroy Lindo \u2026\nYear\nActor\nMotion Picture\nNominees\n1998\nMorgan Freeman\nAmistad\nClarence Williams\u2026\n2005\nMorgan Freeman\nMillion Dollar\nJamie Foxx - \u2026\n\u2026\u2026\n[Question] Which actor has the most \nnaacp image awards?\n\u2713\n[Original Table]\nInput Prompt\nf :\nargs :\nf_select_column\nHeaders: Year, Actor\nIteration 1:\nf :\nargs :\nf_group_by\nHeaders: Actor\nIteration 2:\nIteration 3:\nf :\nargs :\nf_sort_by\nHeaders: Count\nOrder: Descend\n\u2026\n\u2026\n1996\nLaurence Fishburne\n1995\nAl Freeman, Jr.\nYear\nActor\n[Intermediate Table 1]\nTransform the table with\nf_sel_col(Year, Actor)\n[Intermediate Table 2]\nTransform the table with\nf_group_by(Actor)\n\u2026\n\u2026\n2\nMorgan Freeman\n1\nAl Freeman, Jr.\nGroup\nActor\n\u2026\n4\n1\nCount\n[Intermediate Table 3]\nTransform the table with\nf_sort_by(Count, Descend)\n\u2026\n\u2026\n2\nDenzel Washing.\n1\nMorgan Freeman\nGroup\nActor\n\u2026\n3\n4\nCount\nf : [END]\nIteration 4:\nFinal Query\nChain-of-Table (ours)\nFigure 4: Illustration of the tabular reasoning process in CHAIN-OF-TABLE. This iterative process\ninvolves dynamically planning an operation chain and accurately storing intermediate results in the\ntransformed tables. These intermediate tables serve as tabular thought process that can guide the\nLLM to land to the correct answer more reliably.\nresults. Dater involves multiple delicate yet fixed steps, such as decomposing the tables and gener-\nating cloze queries for the questions. In each step, Dater also employs self-consistency to improve\naccuracy of the LLM outputs, leading to a high number of required generated samples. For a de-\ntailed description of these frameworks, please refer to the corresponding papers, Ye et al. (2023) and\nCheng et al. (2022).\nUnlike these previous methods, our proposed CHAIN-OF-TABLE employs a greedy search strategy\nin its tabular reasoning process, instead of relying on self-consistency sampling for boosting perfor-\nmance. This approach results in a reduced query count for our method, despite CHAIN-OF-TABLE\nadopting an iterative reasoning process. To be more specific, we observe that the number of queries\nneeded by CHAIN-OF-TABLE is the lowest among the most recent baselines \u2013 50% less than Binder\nand 75% less than Dater. We attribute the query efficiency of our method to the proposed dynamic\noperation execution through the tabular reasoning. The model is able to find an effective reasoning\nprocess that reaches the final output quicker and more reliably.\n4.6\nCASE STUDY\nIn Figure 4, we illustrate the tabular reasoning process by CHAIN-OF-TABLE. The question is based\non a complex table and requires multiple reasoning steps to 1) identify the relevant columns, 2)\nconduct aggregation, and 3) reorder the aggregated intermediate information. Our proposed CHAIN-\nOF-TABLE involves dynamically planning an operation chain and accurately storing intermediate\nresults in the transformed tables. These intermediate tables serve as tabular thought process that can\nguide the LLM to land to the correct answer more reliably.\n5\nCONCLUSION\nOur proposed CHAIN-OF-TABLE enhances the reasoning capability of LLMs by leveraging the tabu-\nlar structure to express intermediate thoughts for table-based reasoning. It instructs LLMs to dynam-\nically plan an operation chain according to the input table and its associated question. This evolving\ntable design sheds new light on the understanding of prompting LLMs for table understanding.\n6\nREPRODUCIBILITY STATEMENT\nWe include the prompt examples of DynamicPlan(T,Q,chain) in Appendix E.1, the demo\nexamples of GenerateArgs(T,Q,f) in Appendix E.2, the prompt examples of Query(T,Q)\nin Appendix E.3. We run the generic reasoning methods (End-to-End QA, FewShot QA, Chain-\nof-Thought) using the prompts reported in Appendix F. We run Text-to-SQL and Binder us-\ning the official open-sourced code and prompts in https://github.com/HKUNLP/Binder.\nWe run Dater using the official open-sourced code and prompts in https://github.com/\nAlibabaResearch/DAMO-ConvAI. We revise the code to use publicly available GPT 3.5,\nPaLM 2, and LLaMA 2 (Section 4) as the LLM backbone instead of the OpenAI Codex due to its\ninaccessibility.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nEwa Andrejczuk, Julian Eisenschlos, Francesco Piccinno, Syrine Krichene, and Yasemin Altun.\nTable-to-text generation and pre-training with TabT5. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2022, pp. 6758\u20136766, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.503. 1\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023. 2\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 1,\n2, 6\nMichael J. Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. Webtables:\nExploring the power of tables on the web. Proc. VLDB Endow., 1(1):538\u2013549, aug 2008. ISSN\n2150-8097. doi: 10.14778/1453856.1453916. 1\nWenhu Chen. Large language models are few(1)-shot table reasoners. In Findings of the Association\nfor Computational Linguistics: EACL 2023, pp. 1120\u20131130, Dubrovnik, Croatia, May 2023.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.findings-eacl.83. 1, 3, 17\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou,\nand William Yang Wang. Tabfact: A large-scale dataset for table-based fact verification. In\nInternational Conference on Learning Representations, 2019. 1, 2, 5\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-\ning: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022. 1, 3, 6\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic\nlanguages. In International Conference on Learning Representations, 2022. 2, 3, 4, 6, 8, 9, 16\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019. 3\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Co-\nhen. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4884\u20134895,\n2019. 14\nJulian Eisenschlos, Syrine Krichene, and Thomas M\u00fcller. Understanding tables with intermediate\npre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp.\n281\u2013296, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.findings-emnlp.27. 1, 3\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. PAL: Program-aided language models. In International Conference on Machine\nLearning, pp. 10764\u201310799. PMLR, 2023. 3, 6\nZihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, and Xiaoyong Du. PASTA: Table-\noperations aware fact verification via sentence-table cloze pre-training. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pp. 4971\u20134983, Abu\nDhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.emnlp-main.331. 1, 3\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.\nReasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992,\n2023. 3\n10\nPublished as a conference paper at ICLR 2024\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, and Julian Eisen-\nschlos. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 4320\u20134333, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.398. 1, 3\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller model sizes. In Findings of the Association\nfor Computational Linguistics: ACL 2023. Association for Computational Linguistics, 2023. 2\nShima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pp. 37\u201342, Toronto, Canada, July 2023. Association for\nComputational Linguistics. doi: 10.18653/v1/2023.acl-industry.4. 13\nZhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, and Weizhu Chen. OmniTab: Pretraining\nwith natural and synthetic data for few-shot table-based question answering. In Proceedings of the\n2022 Conference of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, pp. 932\u2013942, Seattle, United States, July 2022. Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.68. 1, 3, 16\nNengzheng Jin, Joanna Siebert, Dongfang Li, and Qingcai Chen. A survey on table question an-\nswering: recent advances. In China Conference on Knowledge Graph and Semantic Computing,\npp. 174\u2013186. Springer, 2022. 1\nZiqi Jin and Wei Lu. Tab-cot: Zero-shot tabular chain of thought. arXiv preprint arXiv:2305.17812,\n2023. 3\nGeorge Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for\ntext-to-sql. The VLDB Journal, pp. 1\u201332, 2023. 2\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish\nSabharwal. Decomposed prompting: A modular approach for solving complex tasks. In Interna-\ntional Conference on Learning Representations, 2022. 3\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. In Advances in Neural Information Processing Systems,\n2022. 1\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out, pp. 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguis-\ntics. 5, 14\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang.\nLost in the middle: How language models use long contexts.\narXiv preprint\narXiv:2307.03172, 2023a. 8\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou.\nTAPEX: Table pre-training via learning a neural sql executor. In International Conference on\nLearning Representations, 2021. 1, 3, 16\nQian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, and Min Lin. From zero to hero: Examining the\npower of symbolic tasks in instruction tuning. arXiv preprint arXiv:2304.07995, 2023b. 16\nJoshua Maynez, Priyanka Agrawal, and Sebastian Gehrmann. Benchmarking large language model\ncapabilities for conditional generation. In Proceedings of the 61st Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Papers), pp. 9194\u20139213, 2023. 14\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech\nKry\u00b4sci\u00b4nski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Is-\nabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev, and\nDragomir Radev. FeTaQA: Free-form table question answering. Transactions of the Association\nfor Computational Linguistics, 10:35\u201349, 2022. doi: 10.1162/tacl_a_00446. 2, 5\n11\nPublished as a conference paper at ICLR 2024\nAnsong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria\nLin. Lever: Learning to verify language-to-code generation with execution. In International\nConference on Machine Learning, pp. 26106\u201326128. PMLR, 2023. 3\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 2\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Associa-\ntion for Computational Linguistics, pp. 311\u2013318, Philadelphia, Pennsylvania, USA, July 2002.\nAssociation for Computational Linguistics. doi: 10.3115/1073083.1073135. 5\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables.\nIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pp. 1470\u20131480, Beijing, China, July 2015. Association for Computational Linguistics.\ndoi: 10.3115/v1/P15-1142. 2, 5\nRichard P\u00f6nighaus. \u2019favourite\u2019sql-statements\u2014an empirical analysis of sql-usage in commercial\napplications. In International Conference on Information Systems and Management of Data, pp.\n75\u201391. Springer, 1995. 2\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities of\nlarge language models. arXiv preprint arXiv:2204.00498, 2022. 3, 6\nTianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daum\u00e9 III, and Lillian Lee. On the potential\nof lexico-logical alignments for semantic parsing to sql queries. Findings of the Association for\nComputational Linguistics: EMNLP 2020, 2020. 2\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large lan-\nguage models still can\u2019t plan (a benchmark for llms on planning and reasoning about change). In\nNeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. 3\nZhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. TUTA: Tree-\nbased transformers for generally structured table pre-training. In Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery & Data Mining, pp. 1780\u20131790, 2021. 1, 3\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022. 1, 2, 3, 6\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023. 1\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Large language models\nare versatile decomposers: Decompose evidence and questions for table-based reasoning. arXiv\npreprint arXiv:2301.13808, 2023. 2, 3, 4, 6, 8, 9, 13, 14, 16\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-\nmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex\nreasoning in large language models. In International Conference on Learning Representations,\n2022. 1, 3\n12\nPublished as a conference paper at ICLR 2024\nAPPENDIX\nA\nATOMIC OPERATIONS IN CHAIN-OF-TABLE\nA.1\nINTRODUCTION\nIn this study, we adopt a set of five table operations, which are commonly-used in SQL and\nDataFrame development, as an example. We note that our framework can trivially accommodate\nadditional operations, which we leave for future work.\n\u2022 f_add_column() adds a new column to the table to store intermediate reasoning or computa-\ntional results.\n\u2022 f_select_row() selects a subset of rows that are relevant to the question. Tables may contain\nirrelevant information for the given question (Ye et al., 2023). This operation helps locate the\nnecessary context.\n\u2022 f_select_column() selects a subset of columns. A column usually corresponds to an at-\ntribute in the table. This operation allows the model to locate the necessary attributes to answer\nthe question.\n\u2022 f_group_by() groups the rows by the contents of a specific column and provides the count\nof each enumeration value in that column. Many table-based questions or statements involve\ncounting, but LLMs are not proficient at this task (Imani et al., 2023).\n\u2022 f_sort_by() sorts the rows based on the contents of a specific column. When dealing with\nquestions or statements involving comparison or extremes, LLMs can utilize this operation to\nrearrange the rows. The relationship can be readily inferred from the order of the sorted rows.\nA.2\nABLATION STUDY\nTo demonstrate the effectiveness of our proposed atomic operations, we perform an ablation study\nby creating five leave-one-out variants of our method, each of which removes one of the pre-defined\noperations from the pre-defined operation pool.\nFor example, w/o f_add_column() means\nf_add_column() is removed from the operation pool. As a result, the LLM is only able to plan\nfrom the remaining four operations (f_select_column, f_select_row, f_group_by, and\nf_sort_by) to construct operation chains. We report the results of the ablation study in Table 5.\nTable 5: Ablation study of the atomic operations used in CHAIN-OF-TABLE with PaLM 2 on Wik-\niTQ and TabFact datasets. We observe that row selection and group-by operations have the biggest\nimpact on the final table understanding performance.\nPrompting\nTabFact\nWikiTQ\nAccuracy\nAccuracy\nCHAIN-OF-TABLE\n86.61\n67.31\nw/o\nf_add_column()\n85.23 (-1.38)\n65.88 (-1.43)\nw/o\nf_select_column()\n82.61 (-4.00)\n65.68 (-1.63)\nw/o\nf_select_row()\n82.21 (-4.40)\n65.06 (-2.25)\nw/o\nf_group_by()\n84.78 (-1.83)\n61.88 (-5.43)\nw/o\nf_sort_by()\n86.21 (-0.40)\n65.85 (-1.46)\nAs shown in Table 5, all five operations contribute to the final state-of-the-art performance of\nCHAIN-OF-TABLE, as removing any operation results in a decrease in performance. In particular,\nwe observe that f_select_row() and f_select_column() contribute the most on TabFact,\nwhile f_group_by() contributes the most on WikiTQ. This suggests that different tasks require\ndifferent operations to help the LLM determine the correct answer. Therefore, leveraging the LLM\nto design custom operation chains through dynamic planning naturally fits different tasks, resulting\nin superior performance of our method.\n13\nPublished as a conference paper at ICLR 2024\nB\nEXPERIMENTS OF CHAIN-OF-TABLE ON FETAQA\nTable 6 shows that CHAIN-OF-TABLE also improves the performance of free-form question answer-\ning on FeTaQA across all metrics, whereas Dater (Ye et al., 2023) fails to improve the ROUGE scores\ncompared with End-to-End QA. We also observe the marginal improvement of CHAIN-OF-TABLE\ncompared with the baseline methods. We attribute this to the nature of the n-gram text similarity\nmetrics of ROUGE-1/2/L (Lin, 2004). As discussed in Maynez et al. (2023); Dhingra et al. (2019),\nthese metrics are known to be insensitive to capturing improvements when using in-context learning\nsince the model is unable to learn the expected style of the long form text just from an instruction or a\nfew examples. We sample several cases from FeTaQA as shown in Figure 5 where the ROUGE met-\nrics assign low scores; however, upon review, we observe that the generated answers were correct.\nTable 6: Table understanding results on the FeTaQA benchmark using PaLM 2 with the best results\nin bold and improvements over Dater (Ye et al., 2023) reported. (underline denotes the second-best\nperformance; bold denotes the best performance; the improvement is measured against the second-\nbest performing method.)\nPrompting\nFeTaQA\nBLEU\nROUGE-1\nROUGE-2\nROUGE-L\nEnd-to-End QA\n28.37\n0.63\n0.41\n0.53\nDater (Ye et al., 2023)\n29.47\n0.63\n0.41\n0.53\nCHAIN-OF-TABLE (ours)\n32.61 (+3.14)\n0.66 (+0.03)\n0.44 (+0.03)\n0.56 (+0.03)\nExample from FeTaQA\nQuestion: Who were the last two finishers in the 2000 Summer Olympics Mens 100\nmetre freestyle?\nAnswer: Russia\u2019s Denis Pimankov (49.36) and Australia\u2019s Chris Fydler (49.44)\nrounded out the finale.\nPrediction: The last two finishers in the 2000 Summer Olympics Mens 100 metre\nfreestyle were Chris Fydler and Denis Pimankov.\nResults: ROUGE-1=0.33; ROUGE-2=0.12; ROUGE-L=0.11\nExplanation: The generated response correctly answers the question but the\nsentence styles are different. From the metrics, we can see the ROUGE scores\nare below the average.\nFigure 5: Result example of CHAIN-OF-TABLE on FeTaQA using the ROUGE scores as metrics,\nwhere the ROUGE metrics assign very low scores but the generated answers were correct.\n14\nPublished as a conference paper at ICLR 2024\nC\nINFERENCE PARAMETERS AND NUMBER OF DEMO SAMPLES OF\nCHAIN-OF-TABLE\nWe report the parameters and demo sample numbers we used in CHAIN-OF-TABLE in Table 7, 8\nand 9. Overall, we annotate 29 samples and use them across different datasets. There are a large\noverlapping between the usage on different functions. For example, we use the same demo sample\nto introduce how to use f_add_column in the function DynamicPlan across different datasets.\nWe guarantee that all demo samples are from the training set so they are unseen during testing. We\nargue that this further demonstrates our framework does not rely on a specific set of demos and can\nbe well generalized to new datasets with the same prompts.\nTable 7: LLM parameters and number of demo samples in CHAIN-OF-TABLE on WikiTQ\nFunction\nWikiTQ\ntemperature\ntop_p\ndecode_steps\nn_samples\nn_demos\nDynamicPlan()\n0.0\n1.0\n200\n-\n4\nf_add_column()\n0.0\n1.0\n200\n-\n6\nf_select_row()\n1.0\n1.0\n200\n8\n3\nf_select_column()\n1.0\n1.0\n200\n8\n8\nf_group_by()\n0.0\n1.0\n200\n-\n2\nf_sort_by()\n0.0\n1.0\n200\n-\n2\nquery()\n0.0\n1.0\n200\n-\n1\nTable 8: LLM parameters and number of demo samples in CHAIN-OF-TABLE on TabFact\nFunction\nTabFact\ntemperature\ntop_p\ndecode_steps\nn_samples\nn_demos\nDynamicPlan()\n0.0\n1.0\n200\n-\n4\nf_add_column()\n0.0\n1.0\n200\n-\n7\nf_select_row()\n0.5\n1.0\n200\n8\n4\nf_select_column()\n0.5\n1.0\n200\n8\n8\nf_group_by()\n0.0\n1.0\n200\n-\n2\nf_sort_by()\n0.0\n1.0\n200\n-\n2\nquery()\n0.0\n1.0\n200\n-\n4\nTable 9: LLM parameters and number of demo samples in CHAIN-OF-TABLE on FeTaQA\nFunction\nFeTaQA\ntemperature\ntop_p\ndecode_steps\nn_samples\nn_demos\nDynamicPlan()\n0.0\n1.0\n200\n-\n3\nf_add_column()\n0.0\n1.0\n200\n-\n6\nf_select_row()\n1.0\n1.0\n200\n8\n3\nf_select_column()\n1.0\n1.0\n200\n8\n8\nf_group_by()\n0.0\n1.0\n200\n-\n2\nf_sort_by()\n0.0\n1.0\n200\n-\n2\nquery()\n0.0\n1.0\n200\n-\n8\n15\nPublished as a conference paper at ICLR 2024\nD\nTABULAR FORMAT ENCODING COMPARISON\nIn alignment with prior studies Liu et al. (2023b; 2021); Jiang et al. (2022) and the baseline methods\nCheng et al. (2022); Ye et al. (2023), we adopt PIPE encoding in CHAIN-OF-TABLE (as shown\nin Appendix E). This decouples the performance gains of the proposed tabular CoT with atomic\noperations from the influence of various table formatting choices.\nTo further understand the impact of different encoding methods on table understanding performance,\nwe conduct additional experiments using 3 additional table representations: HTML, TSV, and Mark-\ndown. For these experiments, we use End-to-End QA on WikiTQ with PaLM 2 as a running exam-\nple. The results are shown in Table 10. These findings show that different tabular format encoding\nmethods lead to different outcomes. Notably, the PIPE format adopted in our study yields the highest\nperformance among the four encoding methods tested.\nTable 10: Tabular format encoding comparison on WikiTQ with PaLM 2\nPrompting\nTabular Format Encoding\nPIPE\nHTML\nTSV\nMarkdown\nEnd-to-End QA\n60.6\n56.1\n58.1\n58.0\nE\nPROMPTS IN CHAIN-OF-TABLE\nE.1\nDynamicPlan\nWe illustrate the prompting method used by DynamicPlan(T,Q,chain) in Figure 6 where T\nis the latest intermediate table and Q is its corresponding question; chain is the list of operations\nperformed on the table.\nWith DynamicPlan, the LLM can generate the rest of the operation chain for the current sample\n(Figure 6(c)). We denote the generated operations as fi+1(argsi+1) \u2192 ... \u2192 [E] given that fi is\nthe last operation of the input open-ended operation chain. Although a complete chain is generated,\nwe only consider the first generated operation, fi+1, and ignore the rest of the generation including\nthe arguments and remaining operations. fi+1 is generated based on the latest intermediate table\nfrom the previous operations, while the generation of subsequent operations are not based on the\nmost up-to-date intermediate table so there could be mistakes in the generated contents. Therefore,\nwe believe fi+1 is the most reliable generation among all operations in the generated chain. See\nFigure 9 for more detailed prompts.\nE.2\nGenerateArgs\nWe illustrate the demonstration examples and prompts used by GenerateArgs(T,Q,f) in Fig-\nure 7 where T is the latest intermediate table and Q is its corresponding question; f is the selected\ntabular operations. The detailed prompts for each operation and the regular expressions for extract-\ning the generated arguments are as follows.\n\u2022 f_add_column: See Figure 10.\n\u2022 f_select_row: See Figure 12.\n\u2022 f_select_column: See Figure 11.\n\u2022 f_group_by: See Figure 13.\n\u2022 f_sort_by: See Figure 14.\nE.3\nQuery\nWe illustrate the prompts used by Query(T,Q) in Figure 8 where T is the resulting table from\nCHAIN-OF-TABLE and Q is the question. See Figure 15 for more detailed prompts.\n16\nPublished as a conference paper at ICLR 2024\nF\nIMPLEMENTATION DETAILS OF BASELINE METHODS\nWe run Text-to-SQL and Binder using the official open-sourced code and prompts in https://\ngithub.com/HKUNLP/Binder. We run Dater using the official open-sourced code and prompts\nin https://github.com/AlibabaResearch/DAMO-ConvAI. We revise the code to use\npublicly available GPT 3.5, PaLM 2, and LLaMA 2 (Section 4) as the LLM backbone instead of\nthe OpenAI Codex due to its inaccessibility. We report the detailed prompts used in other baseline\nmethods as follows.\n\u2022 End-to-End QA: See Figure 16.\n\u2022 Few-Shot QA: See Figure 17.\n\u2022 Chain-of-Thought: The demonstration samples of Chain-of-Thought for WikiTQ and TabFact\nare from Chen (2023) (https://github.com/wenhuchen/TableCoT). See Figure 18.\n{OPERATION_CHAIN_INSTRUCTION}\n{SERIALIZED_TABLE}\nQuestion:{QUESTION}\nOperation Chain:{COMPLETE_OPERATION_CHAIN}\n{OPERATION_INSTRUCTION}\n{SERIALIZED_TABLE}\nQuestion:{QUESTION}\nOperation:{OPERATION_AND_ARGUMENTS}\nExplanation:{INTRODUCION_TO_THE_OPERATION}\n{SERIALIZED_LATEST_TABLE(T)}\nQuestion:{QUESTION(Q)}\nCandidates: {POSSIBLE_NEXT_OPERATIONS}\nOperation Chain:{INCOMPLETE_OPERATION_CHAIN(chain)}\n{REST_OPERATIONS_OF_OPERATION_CHAIN}\n\u2026\u2026\n// Complete operation chain with an ending tag [E]\nQuestion: What was the last year where this team was a part of the USL a-league?\nOperation Chain: f_add_column(Year) \u2192 f_select_row(row 1, row 2) \u2192 \nf_select_column(Year, League) \u2192 f_sort_column(Year) \u2192 [E]\nPrompt Template of DynamicPlan(T, Q, chain)\n// Extract the \ufb01rst generated operation f_select_row as our next \noperation since it is based on the most up-to-date intermediate table\nf_select_row (row 1, row 2 \u2026 row 10 \u2026 \u2192 \u2026 \u2192 [E]\n(a) Part I - Atomic Operation Demo\n(b) Part II: Operation Chain Demo\n\u2026\u2026\n(c) Part III: Input Sample\n// Incomplete operation chain with an arrow \"\u2192\" at the end triggering \nthe LLM to complete the chain with candidate operations\nQuestion: Which country had the most cyclists with in the top 10?\nCandidates: f_select_row, f_select_column, \u2026\nOperation Chain: f_add_column(Country) \u2192 \n// Introduce how atomic operations work\nOperation: f_select_column(County, Population)\nExplanation: The question asks \u2026 We only need column Country and \nPopulation to answer the question so we use f_select_column to select them.\n{REST_OPERATIONS_OF_OPERATION_CHAIN}\nFigure 6:\nIllustration of DynamicPlan(T,Q,chain). Left: Overall prompt template and\nexpected generation, including (a) demonstration of how atomic operations work, (b) demonstration\nof how to generate a complete operation chain to answer a given question, and (c) prompt for actual\ninput table and its question, and its expected generation from the LLM (highlighted in green). Right:\nExamples and brief explanations of each part in the prompt and generation.\nWhen f = f_add_column():\nThe answer is: f_add_column(Country). The value: ESP, RUS, \u2026\nWhen f = f_select_row():\nThe answer is: f_select_row(row 1, row 2, row 3\u2026)\nWhen f = f_select_column():\nThe answer is: f_select_column(Country, Rank)\n// Parse LLM generation with pre-de\ufb01ned regular expressions \nto extract arguments.\n{OPERATION_INSTRUCTION(f)}\n{SERIALIZED_TABLE(T)}\nQuestion:{QUESTION(Q)}\nExplanation:\n{EXPLANATION}\nThe answer is:{OPERATION_AND_ARGUMENTS}\nPrompt Template of GenerateArgs(T, Q, f)\n{EXPLANATION}\nThe answer is:{OPERATION_AND_ARGUMENTS}\nWhen f = f_sort_by():\nThe answer is: f_sort_by(Count). The order is from-large-to-small.\nWhen f = f_group_by():\nThe answer is: f_group_by(Country)\nFigure 7:\nIllustration of GenerateArgs(T,Q,f). After a specific operation f is sampled\nby the LLM as the next operation, we ask the LLM to generate the required arguments by calling\nGenerateArgs. Then we parse the generation results of the LLM according to the pre-defined\ntemplates to extract the arguments.\n17\nPublished as a conference paper at ICLR 2024\nPrompt Template of  Query(T, Q)\n{GENERAL_INSTRUCTION}\n{SERIALIZED_RESULTING_TABLE(T)}\nQuestion:{QUESTION(Q)}\nAnswer:{answer}\n{answer}\n// Directly generate the answer with the resulting table\nfrom Chain-of-Table\nFigure 8: Illustration of Query(T,Q). The resulting table from the operation chain serves as a\nproxy for the intermediate thoughts of reasoning, allowing us to directly generate the answer without\nproviding the reasoning chain in textual format.\n========================================= Prompt =========================================\nIf the table only needs a few rows to answer the question, we use f_select_row() to select\nthese rows for it. For example,\n/*\ncol : Home team | Home Team Score | Away Team | Away Team Score | Venue | Crowd\nrow 1 : st kilda | 13.12 (90) | melbourne | 13.11 (89) | moorabbin oval | 18836\nrow 2 : south melbourne | 9.12 (66) | footscray | 11.13 (79) | lake oval | 9154\nrow 3 : richmond | 20.17 (137) | fitzroy | 13.22 (100) | mcg | 27651\n*/\nQuestion : Whose home team score is higher, richmond or st kilda?\nFunction: f_select_row(row 1, row 3)\nExplanation: The question asks about the home team score of richmond and st kilda. We need\nto know the the information of richmond and st kilda in row 1 and row 3. We select row 1\nand row 3.\nIf the table only needs a few columns to answer the question, we use\nf_select_column() to select these columns for it. For example,\n......\nIf the question asks about items with the same value and the number of these items, we use\nf_group_by() to group the items. For example,\n......\nIf the question asks about the order of items in a column, we use f_sort_by() to sort\nthe items. For example,\n......\nHere are examples of using the operations to answer the question.\n/*\ncol : Date | Division | League | Regular Season | Playoffs | Open Cup\nrow 1 : 2001/01/02 | 2 | USL A-League | 4th, Western | Quarterfinals | Did not qualify\nrow 2 : 2002/08/06 | 2 | USL A-League | 2nd, Pacific | 1st Round | Did not qualify\nrow 5 : 2005/03/24 | 2 | USL First Division | 5th | Quarterfinals | 4th Round\n*/\nQuestion: what was the last year where this team was a part of the usl a-league?\nFunction Chain: f_add_column(Year) -> f_select_row(row 1, row 2) ->\nf_select_column(Year, League) -> f_sort_by(Year) -> <END>\n......\n/*\ncol : Rank | Cyclist | Team | Time | UCI ProTour; Points | Country\nrow 1 : 1 | Alejandro Valverde (ESP) | Caisse d\u2019Epargne | 5h 29\u2019 10\" | 40 | ESP\nrow 2 : 2 | Alexandr Kolobnev (RUS) | Team CSC Saxo Bank | s.t. | 30 | RUS\nrow 3 : 3 | Davide Rebellin (ITA) | Gerolsteiner | s.t. | 25 | ITA\nrow 4 : 4 | Paolo Bettini (ITA) | Quick Step | s.t. | 20 | ITA\nrow 5 : 5 | Franco Pellizotti (ITA) | Liquigas | s.t. | 15 | ITA\nrow 6 : 6 | Denis Menchov (RUS) | Rabobank | s.t. | 11 | RUS\nrow 7 : 7 | Samuel S\u00e1nchez (ESP) | Euskaltel-Euskadi | s.t. | 7 | ESP\nrow 8 : 8 | St\u00e9phane Goubert (FRA) | Ag2r-La Mondiale | + 2\" | 5 | FRA\nrow 9 : 9 | Haimar Zubeldia (ESP) | Euskaltel-Euskadi | + 2\" | 3 | ESP\nrow 10 : 10 | David Moncouti\u00e9 (FRA) | Cofidis | + 2\" | 1 | FRA\n*/\nQuestion: which country had the most cyclists finish within the top 10?\nThe next operation must be one of f_select_row() or f_select_column() or f_group_by()\nor f_sort_by().\nFunction Chain: f_add_column(Country) ->\n======================================= Completion =======================================\nf_select_row(row 1, row 10) -> f_select_column(Country) -> f_group_by(Country) -> <END>\nFigure 9: DynamicPlan(T,Q,chain) Prompt used for WikiTQ\n18\nPublished as a conference paper at ICLR 2024\nTo answer the question, we can first use f_add_column() to add more columns to the table.\nThe added columns should have these data types:\n1. Numerical: the numerical strings that can be used in sort, sum\n2. Datetype: the strings that describe a date, such as year, month, day\n3. String: other strings\n/*\ncol : Week | When | Kickoff | Opponent | Results; Final score | Results; Team record\nrow 1 : 1 | Saturday, April 13 | 7:00 p.m. | at Rhein Fire | W 27-21 | 1-0\nrow 2 : 2 | Saturday, April 20 | 7:00 p.m. | London Monarchs | W 37-3 | 2-0\nrow 3 : 3 | Sunday, April 28 | 6:00 p.m. | at Barcelona Dragons | W 33-29 | 3-0\n*/\nQuestion: what is the date of the competition with highest attendance?\nThe existing columns are: \"Week\", \"When\", \"Kickoff\", \"Opponent\", \"Results; Final score\",\n\"Results; Team record\", \"Game site\", \"Attendance\".\nExplanation: the question asks about the date of the competition with highest score. Each\nrow is about one competition. We extract the value from column \"Attendance\" and create a\ndifferent column \"Attendance number\" for each row. The datatype is Numerical.\nTherefore, the answer is: f_add_column(Attendance number). The value: 32092 | 34186 | 17503\n/*\ncol : Rank | Lane | Player | Time\nrow 1 :\n| 5 | Olga Tereshkova (KAZ) | 51.86\nrow 2 :\n| 6 | Manjeet Kaur (IND) | 52.17\nrow 3 :\n| 3 | Asami Tanno (JPN) | 53.04\n*/\nQuestion: tell me the number of athletes from japan.\nThe existing columns are: Rank, Lane, Player, Time.\nExplanation: the question asks about the number of athletes from japan. Each row is about\none athlete. We need to know the country of each athlete. We extract the value from column\n\"Player\" and create a different column \"Country of athletes\" for each row. The datatype\nis String.\nTherefore, the answer is: f_add_column(Country of athletes). The value: KAZ | IND | JPN\nFigure 10: Demos used for GenerateArgs(T,Q,f_add_column). We use the regular expres-\nsion: f_add_column((.*)).The value:(.*) to extract the arguments from the generated\ntext.\nUse f_select_column() to filter out useless columns in the table according to information\nin the statement and the table.\n/*\n{\n\"table_caption\": \"south wales derby\",\n\"columns\": [\"competition\", \"total matches\", \"cardiff win\", \"draw\", \"swansea win\"],\n\"table_column_priority\": [\n[\"competition\", \"league\", \"fa cup\", \"league cup\"],\n[\"total matches\", \"55\", \"2\", \"5\"],\n[\"cardiff win\", \"19\", \"0\", \"2\"],\n[\"draw\", \"16\", \"27\", \"0\"],\n[\"swansea win\", \"20\", \"2\", \"3\"]\n]\n}\n*/\nstatement : there are no cardiff wins that have a draw greater than 27.\nsimilar words link to columns :\nno cardiff wins -> cardiff win\na draw -> draw\ncolumn value link to columns :\n27 -> draw\nsemantic sentence link to columns :\nNone\nThe answer is : f_select_column([cardiff win, draw])\nFigure 11: Demos used for GenerateArgs(T,Q,f_select_column). We use the regular\nexpression: f_select_column([(.*)]) to extract the arguments from the generated text.\n19\nPublished as a conference paper at ICLR 2024\nUsing f_select_row() to select relevant rows in the given table that support or oppose the\nstatement.\nPlease use f_select_row([*]) to select all rows in the table.\n/*\ntable caption : 1972 vfl season.\ncol : home team | home team score | away team | away team score | venue | crowd\nrow 1 : st kilda | 13.12 (90) | melbourne | 13.11 (89) | moorabbin oval | 18836\nrow 2 : south melbourne | 9.12 (66) | footscray | 11.13 (79) | lake oval | 9154\nrow 3 : richmond | 20.17 (137) | fitzroy | 13.22 (100) | mcg | 27651\nrow 4 : geelong | 17.10 (112) | collingwood | 17.9 (111) | kardinia park | 23108\nrow 5 : north melbourne | 8.12 (60) | carlton | 23.11 (149) | arden street oval | 11271\nrow 6 : hawthorn | 15.16 (106) | essendon | 12.15 (87) | vfl park | 36749\n*/\nstatement : what is the away team with the highest score?\nexplain : the statement want to ask the away team of highest away team score. the highest\naway team score is 23.11 (149). it is on the row 5.so we need row 5.\nThe answer is : f_select_row([row 5])\nFigure 12: Demos used for GenerateArgs(T,Q,f_select_row). We use the regular ex-\npression: f_select_row([(.*)]) to extract the arguments from the generated text.\nTo answer the question, we can first use f_group_by() to group the values in a column.\n/*\ncol : Rank | Lane | Athlete | Time | Country\nrow 1 : 1 | 6 | Manjeet Kaur (IND) | 52.17 | IND\nrow 2 : 2 | 5 | Olga Tereshkova (KAZ) | 51.86 | KAZ\nrow 3 : 3 | 4 | Pinki Pramanik (IND) | 53.06 | IND\nrow 4 : 4 | 1 | Tang Xiaoyin (CHN) | 53.66 | CHN\nrow 5 : 5 | 8 | Marina Maslyonko (KAZ) | 53.99 | KAZ\n*/\nQuestion: tell me the number of athletes from japan.\nThe existing columns are: Rank, Lane, Athlete, Time, Country.\nExplanation: The question asks about the number of athletes from India. Each row is about\nan athlete. We can group column \"Country\" to group the athletes from the same country.\nTherefore, the answer is: f_group_by(Country).\nFigure 13: Demos used for GenerateArgs(T,Q,f_group_by). We use the regular expres-\nsion: f_group_by((.*)) to extract the arguments from the generated text.\nTo answer the question, we can first use f_sort_by() to sort the values in a column to get\nthe\norder of the items. The order can be \"large to small\" or \"small to large\".\nThe column to sort should have these data types:\n1. Numerical: the numerical strings that can be used in sort\n2. DateType: the strings that describe a date, such as year, month, day\n3. String: other strings\n/*\ncol : Position | Club | Played | Points | Wins | Draws | Losses | Goals for | Goals against\nrow 1 : 1 | Malaga CF | 42 | 79 | 22 | 13 | 7 | 72 | 47\nrow 10 : 10 | CP Merida | 42 | 59 | 15 | 14 | 13 | 48 | 41\nrow 3 : 3 | CD Numancia | 42 | 73 | 21 | 10 | 11 | 68 | 40\n*/\nQuestion: what club placed in the last position?\nThe existing columns are: Position, Club, Played, Points, Wins, Draws, Losses, Goals for,\nGoals against\nExplanation: the question asks about the club in the last position. Each row is about a\nclub. We need to know the order of position from last to front. There is a column for\nposition and the column name is Position. The datatype is Numerical.\nTherefore, the answer is: f_sort_by(Position), the order is \"large to small\".\nFigure 14: Demos used for GenerateArgs(T,Q,f_sort_by). We use the regular expression:\nf_sort_by((.*)),the order is \"(.*)\". to extract the arguments from the generated\ntext.\n20\nPublished as a conference paper at ICLR 2024\n========================================= Prompt =========================================\nHere is the table to answer this question. Please understand the table and answer the\nquestion:\n/*\ncol : Rank | City | Passengers Number | Ranking | Airline\nrow 1 : 1 | United States, Los Angeles | 14749 | 2 | Alaska Airlines\nrow 2 : 2 | United States, Houston | 5465 | 8 | United Express\nrow 3 : 3 | Canada, Calgary | 3761 | 5 | Air Transat, WestJet\nrow 4 : 4 | Canada, Saskatoon | 2282 | 4 |\nrow 5 : 5 | Canada, Vancouver | 2103 | 2 | Air Transat\nrow 6 : 6 | United States, Phoenix | 1829 | 1 | US Airways\nrow 7 : 7 | Canada, Toronto | 1202 | 1 | Air Transat, CanJet\nrow 8 : 8 | Canada, Edmonton | 110 | 2 |\nrow 9 : 9 | United States, Oakland | 107 | 5 |\n*/\nQuestion: how many more passengers flew to los angeles than to saskatoon from manzanillo\nairport in 2013?\nThe anwser is: 12467\nHere is the table to answer this question. Please understand the table and answer the\nquestion:\n/*\ncol : Rank | Country\nrow 1 : 1 | ESP\nrow 2 : 2 | RUS\nrow 3 : 3 | ITA\nrow 4 : 4 | ITA\nrow 5 : 5 | ITA\nrow 6 : 6 | RUS\nrow 7 : 7 | ESP\nrow 8 : 8 | FRA\nrow 9 : 9 | ESP\nrow 10 : 10 | FRA\n*/\nGroup the rows according to column \"Country\":\n/*\nGroup ID | Country | Count\n1 | ITA | 3\n2 | ESP | 3\n3 | RUS | 2\n4 | FRA | 2\n*/\nQuestion: which country had the most cyclists in top 10?\nThe answer is:\n======================================= Completion =======================================\nItaly.\nFigure 15: Prompt Example used for Query(T,Q)\n21\nPublished as a conference paper at ICLR 2024\n========================================= Prompt =========================================\nHere is the table to answer this question. Answer the question.\n/*\ncol : Name | League | FA Cup | League Cup | JP Trophy | Total\nrow 1 : Scot Bennett | 5 | 0 | 0 | 0 | 5\nrow 2 : Danny Coles | 3 | 0 | 0 | 0 | 3\nrow 3 : Liam Sercombe | 1 | 0 | 0 | 0 | 1\nrow 4 : Alan Gow | 4 | 0 | 0 | 0 | 4\nrow 5 : John O\u2019Flynn | 11 | 0 | 1 | 0 | 12\nrow 6 : Guillem Bauza | 2 | 0 | 0 | 0 | 2\nrow 7 : Jimmy Keohane | 3 | 0 | 0 | 0 | 3\nrow 8 : Pat Baldwin | 1 | 0 | 0 | 0 | 1\nrow 9 : Jamie Cureton | 20 | 0 | 0 | 0 | 20\nrow 10 : Arron Davies | 3 | 0 | 0 | 0 | 3\nrow 11 : Jake Gosling | 1 | 0 | 0 | 0 | 1\nrow 12 : OWN GOALS | 0 | 0 | 0 | 0 | 0\nrow 13 : Total | 0 | 0 | 0 | 0 | 0\n*/\nQuestion: does pat or john have the highest total?\nThe answer is:\n======================================= Completion =======================================\nJohn.\nFigure 16: Prompt of End-to-end QA used for WikiTQ.\n22\nPublished as a conference paper at ICLR 2024\n========================================= Prompt =========================================\nHere is the table to answer this question. Answer the question.\n/*\ncol : Rank | Cyclist | Team | Time | UCI ProTour; Points\nrow 1 : 1 | Alejandro Valverde (ESP) | Caisse d\u2019Epargne | 5h 29\u2019 10\" | 40\nrow 2 : 2 | Alexandr Kolobnev (RUS) | Team CSC Saxo Bank | s.t. | 30\nrow 3 : 3 | Davide Rebellin (ITA) | Gerolsteiner | s.t. | 25\nrow 4 : 4 | Paolo Bettini (ITA) | Quick Step | s.t. | 20\nrow 5 : 5 | Franco Pellizotti (ITA) | Liquigas | s.t. | 15\nrow 6 : 6 | Denis Menchov (RUS) | Rabobank | s.t. | 11\nrow 7 : 7 | Samuel S\u00e1nchez (ESP) | Euskaltel-Euskadi | s.t. | 7\nrow 8 : 8 | St\u00e9phane Goubert (FRA) | Ag2r-La Mondiale | + 2\" | 5\nrow 9 : 9 | Haimar Zubeldia (ESP) | Euskaltel-Euskadi | + 2\" | 3\nrow 10 : 10 | David Moncouti\u00e9 (FRA) | Cofidis | + 2\" | 1\n*/\nQuestion: which country had the most cyclists finish within the top 10?\nThe answer is: Italy.\nHere is the table to answer this question. Please provide your explanation first, then\nanswer the question in a short phrase starting by \u2019therefore, the answer is:\u2019\n/*\ncol : Rank | Cyclist | Team | Time | UCI ProTour; Points\nrow 1 : 1 | Alejandro Valverde (ESP) | Caisse d\u2019Epargne | 5h 29\u2019 10\" | 40\nrow 2 : 2 | Alexandr Kolobnev (RUS) | Team CSC Saxo Bank | s.t. | 30\nrow 3 : 3 | Davide Rebellin (ITA) | Gerolsteiner | s.t. | 25\nrow 4 : 4 | Paolo Bettini (ITA) | Quick Step | s.t. | 20\nrow 5 : 5 | Franco Pellizotti (ITA) | Liquigas | s.t. | 15\nrow 6 : 6 | Denis Menchov (RUS) | Rabobank | s.t. | 11\nrow 7 : 7 | Samuel S\u00e1nchez (ESP) | Euskaltel-Euskadi | s.t. | 7\nrow 8 : 8 | St\u00e9phane Goubert (FRA) | Ag2r-La Mondiale | + 2\" | 5\nrow 9 : 9 | Haimar Zubeldia (ESP) | Euskaltel-Euskadi | + 2\" | 3\nrow 10 : 10 | David Moncouti\u00e9 (FRA) | Cofidis | + 2\" | 1\n*/\nQuestion: how many players got less than 10 points?\nThe answer is: 4.\nHere is the table to answer this question. Answer the question.\n/*\ncol : Name | League | FA Cup | League Cup | JP Trophy | Total\nrow 1 : Scot Bennett | 5 | 0 | 0 | 0 | 5\nrow 2 : Danny Coles | 3 | 0 | 0 | 0 | 3\nrow 3 : Liam Sercombe | 1 | 0 | 0 | 0 | 1\nrow 4 : Alan Gow | 4 | 0 | 0 | 0 | 4\nrow 5 : John O\u2019Flynn | 11 | 0 | 1 | 0 | 12\nrow 6 : Guillem Bauza | 2 | 0 | 0 | 0 | 2\nrow 7 : Jimmy Keohane | 3 | 0 | 0 | 0 | 3\nrow 8 : Pat Baldwin | 1 | 0 | 0 | 0 | 1\nrow 9 : Jamie Cureton | 20 | 0 | 0 | 0 | 20\nrow 10 : Arron Davies | 3 | 0 | 0 | 0 | 3\nrow 11 : Jake Gosling | 1 | 0 | 0 | 0 | 1\nrow 12 : OWN GOALS | 0 | 0 | 0 | 0 | 0\nrow 13 : Total | 0 | 0 | 0 | 0 | 0\n*/\nQuestion: does pat or john have the highest total?\nThe answer is:\n======================================= Completion =======================================\nJohn.\nFigure 17: Prompt of Few-shot QA used for WikiTQ\n23\nPublished as a conference paper at ICLR 2024\n========================================= Prompt =========================================\nHere is the table to answer this question. Please provide your explanation first, then\nanswer the question in a short phrase starting by \u2019therefore, the answer is:\u2019\n/*\ncol : Rank | Cyclist | Team | Time | UCI ProTour; Points\nrow 1 : 1 | Alejandro Valverde (ESP) | Caisse d\u2019Epargne | 5h 29\u2019 10\" | 40\nrow 2 : 2 | Alexandr Kolobnev (RUS) | Team CSC Saxo Bank | s.t. | 30\nrow 3 : 3 | Davide Rebellin (ITA) | Gerolsteiner | s.t. | 25\nrow 4 : 4 | Paolo Bettini (ITA) | Quick Step | s.t. | 20\nrow 5 : 5 | Franco Pellizotti (ITA) | Liquigas | s.t. | 15\nrow 6 : 6 | Denis Menchov (RUS) | Rabobank | s.t. | 11\nrow 7 : 7 | Samuel S\u00e1nchez (ESP) | Euskaltel-Euskadi | s.t. | 7\nrow 8 : 8 | St\u00e9phane Goubert (FRA) | Ag2r-La Mondiale | + 2\" | 5\nrow 9 : 9 | Haimar Zubeldia (ESP) | Euskaltel-Euskadi | + 2\" | 3\nrow 10 : 10 | David Moncouti\u00e9 (FRA) | Cofidis | + 2\" | 1\n*/\nQuestion: which country had the most cyclists finish within the top 10?\nExplanation: ITA occurs three times in the table, more than any others. Therefore, the\nanswer is: Italy.\nHere is the table to answer this question. Please provide your explanation first, then\nanswer the question in a short phrase starting by \u2019therefore, the answer is:\u2019\n/*\ncol : Rank | Cyclist | Team | Time | UCI ProTour; Points\nrow 1 : 1 | Alejandro Valverde (ESP) | Caisse d\u2019Epargne | 5h 29\u2019 10\" | 40\nrow 2 : 2 | Alexandr Kolobnev (RUS) | Team CSC Saxo Bank | s.t. | 30\nrow 3 : 3 | Davide Rebellin (ITA) | Gerolsteiner | s.t. | 25\nrow 4 : 4 | Paolo Bettini (ITA) | Quick Step | s.t. | 20\nrow 5 : 5 | Franco Pellizotti (ITA) | Liquigas | s.t. | 15\nrow 6 : 6 | Denis Menchov (RUS) | Rabobank | s.t. | 11\nrow 7 : 7 | Samuel S\u00e1nchez (ESP) | Euskaltel-Euskadi | s.t. | 7\nrow 8 : 8 | St\u00e9phane Goubert (FRA) | Ag2r-La Mondiale | + 2\" | 5\nrow 9 : 9 | Haimar Zubeldia (ESP) | Euskaltel-Euskadi | + 2\" | 3\nrow 10 : 10 | David Moncouti\u00e9 (FRA) | Cofidis | + 2\" | 1\n*/\nQuestion: how many players got less than 10 points?\nExplanation: Samuel S\u00e1nchez,\nSt\u00e9phane Goubert, Haimar Zubeldia and David Moncouti\u00e9\nreceived less than 10 points.\nTherefore, the answer is: 4.\nHere is the table to answer this question. Please provide your explanation first, then\nanswer the question in a short phrase starting by \u2019therefore, the answer is:\u2019\n/*\ncol : Name | League | FA Cup | League Cup | JP Trophy | Total\nrow 1 : Scot Bennett | 5 | 0 | 0 | 0 | 5\nrow 2 : Danny Coles | 3 | 0 | 0 | 0 | 3\nrow 3 : Liam Sercombe | 1 | 0 | 0 | 0 | 1\nrow 4 : Alan Gow | 4 | 0 | 0 | 0 | 4\nrow 5 : John O\u2019Flynn | 11 | 0 | 1 | 0 | 12\nrow 6 : Guillem Bauza | 2 | 0 | 0 | 0 | 2\nrow 7 : Jimmy Keohane | 3 | 0 | 0 | 0 | 3\nrow 8 : Pat Baldwin | 1 | 0 | 0 | 0 | 1\nrow 9 : Jamie Cureton | 20 | 0 | 0 | 0 | 20\nrow 10 : Arron Davies | 3 | 0 | 0 | 0 | 3\nrow 11 : Jake Gosling | 1 | 0 | 0 | 0 | 1\nrow 12 : OWN GOALS | 0 | 0 | 0 | 0 | 0\nrow 13 : Total | 0 | 0 | 0 | 0 | 0\n*/\nQuestion: does pat or john have the highest total?\nExplanation:\n======================================= Completion =======================================\nJohn O\u2019Flynn has the highest total of 12 goals. Pat Baldwin has the lowest total of 1 goal.\nTherefore, the answer is: John.\nFigure 18: Prompt of Chain-of-Thought used for WikiTQ\n24\n"
  },
  {
    "title": "Jump Cut Smoothing for Talking Heads",
    "link": "https://arxiv.org/pdf/2401.04718.pdf",
    "upvote": "16",
    "text": "Jump Cut Smoothing for Talking Heads\nXiaojuan Wang1 Taesung Park2 Yang Zhou2 Eli Shechtman2 Richard Zhang2\n1University of Washington\n2Adobe Research\njeanne-wang.github.io/jumpcutsmoothing\n\u2026\u2026\u2026.\"I do emphasize we are..we are\u2026umm..at an early stage\u201d\u2026\u2026\u2026\u2026\u2026\nFigure 1. Jump cut smoothing for filler words removal. Given a talking head video, we remove the filler words and repetitive words (text\nin red color), and create a seamless transition for the jump cut as shown in the second row.\nAbstract\nA jump cut offers an abrupt, sometimes unwanted change\nin the viewing experience. We present a novel framework\nfor smoothing these jump cuts, in the context of talking head\nvideos. We leverage the appearance of the subject from the\nother source frames in the video, fusing it with a mid-level\nrepresentation driven by DensePose keypoints and face land-\nmarks. To achieve motion, we interpolate the keypoints and\nlandmarks between the end frames around the cut. We then\nuse an image translation network from the keypoints and\nsource frames, to synthesize pixels. Because keypoints can\ncontain errors, we propose a cross-modal attention scheme\nto select and pick the most appropriate source amongst mul-\ntiple options for each key point. By leveraging this mid-level\nrepresentation, our method can achieve stronger results than\na strong video interpolation baseline. We demonstrate our\nmethod on various jump cuts in the talking head videos, such\nas cutting filler words, pauses, and even random cuts. Our\nexperiments show that we can achieve seamless transitions,\neven in the challenging cases where the talking head rotates\nor moves drastically in the jump cut.\n1. Introduction\nWith the continuous advent of social media platforms, talk-\ning head videos have become increasingly popular. Such\nvideos usually focus on the head and upper body of a per-\nson who narrates an idea, story or concept while looking at\nthe camera or an interviewer. Frequently, the subject may\nuse filler words (\u201cuhh\u201d), stutter, make an unwanted pause,\nor repeat words. Directly removing these frames produces\nunnatural jump cuts, which can range from half a second to\nminutes long. In such time, large body movements, head\nmovements, and hand gestures may occur. Can the video edi-\ntor avoid presenting this unnatural experience to the viewer?\nWhile some cuts can be smoothed through playing B-\nroll or kept intentionally for artistic choice, there lacks a\nrobust tool for replacing the jump cut with a seamless transi-\ntion. For example, Adobe\u2019s video editing tool, i.e., Premiere\nPro includes a feature called MorphCut1 to help create a\nsmooth transition. However, it fails when there are relatively\nlarge motion change such as head pose change or hand ges-\nture change, resulting in noticeable motion blurs and other\nvisual artifacts in the generated transitions. In influential\nwork, Berthouzoz et al. [1] generate hidden transitions by\n1https://helpx.adobe.com/premiere-pro/using/morph-cut.html\n1\narXiv:2401.04718v2  [cs.CV]  11 Jan 2024\nEV\nattention\n\u2026\n\u2026\nkeypoints extraction\nEQ\nEK\nsynthesis network\nG\nTraining Strategy\nInference Strategy\nsynthesis network\nkeypoint\ninterpolation\nKey\nQuery\nValue\n\u2026\nKey\nQuery\nValue\njump cut video\n\u2026\nFigure 2. Method overview. In the training stage, we randomly sample source (denoted in green rectangle) and target (denoted in red\nrectangle) frames, and extract their corresponding DensePose keypoints augmented with facial landmarks (not shown here for simplicity).\nOur method extracts source dense keypoint features as key, target dense keypoint feature as query, and source image features as value, then a\ncross attention is applied to get the values for the query, i.e., warped feature. This warped feature is fed into the generator inspired from\nCo-Mod GAN to synthesize a realistic target image compared with the ground truth target frame. For applying jump cut smoothing in the\ninference stage, we interpolate dense keypoints between jump cut end frames, and synthesize the transition frame with the interpolated\nkeypoints (in yellow rectangle) sequence.\nconstructing a similarity graph between the raw frames and\nwalking the graph. Such an approach is heavily limited by\nthe contents of the video itself. Compared to traditional\nmethods [1, 16], recent generative technology, offers the\npossibility of synthesizing new intermediate frames.\nOn the other hand, existing frame interpolation works\nutilize deep networks to perform video frame interpola-\ntion [21, 25] have greatly advanced, showing the impressive\nability to increase video temporal sampling, creating slow\nmotion videos. However, these methods are not designed\nto handle relatively large or complex motion changes in the\ntalking head videos, such as large head rotation and body\ntranslation.\nIn this paper, we propose to readdress the jump cut\nsmoothing by synthesizing new intermediate frames guided\nby the interpolated motion. We leverage a mid-level mo-\ntion representation, i.e., interpolated DensePose [8] key-\npoints between the cut end frames, augmented with face\nlandmarks, and formulate our learning problem as trans-\nferring the appearances of multiple source images to the\ngiven target DensePose keypoints. This is related to image\nanimation using single/multiple source images through the\ntransfer of the motion of a driving video. However, these\nmethods either cannot interpolate the motion learned from\nthe driving video via the unsupervised way due to a lack of\nsemantic correspondence, or they suffer from serious identity\npreservation through the jump cut end frames (see Fig. 3).\nDuring the learning process, simply warping the source ap-\npearances based on DensePose keypoints correspondence\noften loses details and produces unfaithful images because\nof misalignment of the DensePose, disocclusion, and so on.\nBesides, when there are multiple source images available,\nthe naive way to average the warping results often causes\nblurry artifacts. Therefore we introduce a cross model atten-\ntion mechanism to improve the dense correspondence and\nhelp pick the appropriate source among multiple sources for\neach location in the target representation, which achieved\nbetter warping results to be used in the image generation\nnetwork.\nTo conclude, we present a novel algorithm to smooth\njump cuts in talking head videos through motion guided re-\nsynthesis, a non-trivial task that requires balancing between\nrealistic motion interpolation and preserving identity. For\nimproved identity preservation, a larger informational bottle-\nneck is needed, such as more keypoints or larger latent codes.\nYet, more latent codes make motion interpolation challeng-\ning. We navigated this by (1) using more reliable and denser\nDensePose keypoints and face landmarks. (2) designing\ncross model attention, derived from initial DensePose key-\npoints correspondence, improving warping and allowing the\nsynthesis network attend to more frames from the video and\npick the most suitable features. (3) Employing smooth linear\ninterpolation, as well as interpolation ablation augmentation,\nand training the model to handle missing correspondences\nbetween the jump cut end frames. Our experiments show\nthat we can seamless bridge various jump cuts, even those\nwith significant head movements.\n2. Related work\nCNN-based frame interpolation. Frame interpolation, i.e.,\nsynthesizing intermediate images between a pair of input\nframes, is a long-standing research area in computer vi-\nsion. Example applications include temporal up-sampling\nto increase refresh rate, create slow-motion videos and so\n2\nSingle image animation with driving keypoints \nImage animation with driving video (N/A)\nJump cut smoothing \nFigure 3. Image animation methods cannot be applied for jump cut smoothing. Row#1: Single image animation works (FaceVid2Vid [33],\nFace2Face\u03c1 [37]) animate one of the cut end frames according to the key points sequence, neglecting the other end frame; Row#2: Other\nworks (FOMM [28], ImplicitWarping [19]) require a driving video for motion extraction, which is absent in our scenario. Row#3: Our\napproach utilizes at least two cut end frames to generate the transition (shown in orange).\non. Kernel-based methods [21] formulate frame interpola-\ntion as a convolution process, and they estimate spatially-\nadaptive convolution kernels for each output pixel and con-\nvolve the kernels with the input frames to generate a new\nframe. Flow-based methods [10, 14, 17, 20, 22, 23, 25]\nfirst estimate optical flow between the input frames and then\nsynthesize the middle images guided by the flow via either\nwarping or splatting techniques. These methods have demon-\nstrated impressive results for input frames with small motion\nchange. However, in our talking head video editing situation\nwhere we cut out various lengths of filler words, unnecessary\npauses, repeated words and so on, various motion change\nmight happen in the jump cut such as large head rotation.\nThis poses a major challenge for existing frame interpolation\nmethods. Recently, [25] proposed to use multi-scale feature\nextractor and present a \u201cscale-agnostic\u201d bidirectional flow\nestimation module to handle large motion between duplicate\nphotos, and has achieved state-of-the-art results in frame\ninterpolation. Even so, it cannot address large head rota-\ntion, translation, and complex motion change between the\ntwo frames. Different from these flow based methods, we\ntackle this problem with DensePose key point guided image\nsynthesis, which gives flexibility and controlability over the\nsynthesized intermediate images, and our framework allows\nus to utilize other additional frames in the video.\nImage animation with driving video. Our technique is re-\nlated to the works [4, 5, 19, 27\u201329, 33, 37, 39] in the image\nanimation domain, which generates videos by animating a\nsingle source image of a subject using the motions of a driv-\ning video possibly containing a different subject. Typically,\nthese techniques first extract motion representation from the\ndriving video in an unsupervised way, and \u201dwarp\u201d the source\nimage feature to the learned motion for synthesis. However,\npopular methods like FOMM [28], MegaPortraits [5] and\nImplicitWarping [19] often produce latent motions that lack\nsemantic correspondence (see Row#2 in Fig. 3), making\nthem inapplicable for generating transition motions in our\njump cut smoothing task. Even if the keypoints has seman-\ntic meaning, as seen in Face-Vid2Vid [33] which discerns\n3D keypoints, head pose, and expression deformation, these\nmethods grapple with identity preservation due to their re-\nliance on a single image for animation (see Row#1 in Fig. 3).\nRecently [19] introduced Implicit Warping approach, which\nanimates using multiple source images, leveraging cross-\nmodel attention to pick the most fitting keypoint features.\nDespite its impressive results, like FOMM [28], it mandates\na driving video to guide the motion. This stipulation, coupled\nwith our lack of visual signals for the intermediate images\nwe want to generate, renders it inapplicable for our purpose.\nAttention in computer vision. Given a query and a set\nof key-value pairs, the attention function outputs the value\nfor the query, which is computed as a weighted sum of the\nvalues for the keys, and the weight is determined by the\nsimilarity between query and the corresponding key. [32]\nproposed a transformer network stacked of attention layers\nand has achieved remarkable success in the task of machine\ntranslation. Since then, a number of works extended such\ntransformer networks to the compute vision domain for the\ntask of image recognition [3, 18, 31, 34], image genera-\ntion [7, 11\u201313, 24] and achieved state-of-the-art numbers\non the benchmark. There are also works using cross-modal\nattention where the queries, keys, and values are computed\nwith different modalities such as vision and text [36, 38].\nMore recently, diffusion models have gained significant suc-\n3\ncess and roaring attention in high resolution image synthesis\nconditioning on various input modalities such as image to\nimage and text to image. The image generator in the dif-\nfusion model uses UNet backbone with the cross-attention\nmechanism for the conditioning [12, 13, 26] and have proved\ngreat efficiency. In our method, we are doing cross attention\nbetween DensePose keypoint features of source and target,\nwhere they come from different poses. Upon this initial cor-\nrespondence, our attention learns to pick the most relevant\nsource among the set of source images for each location in\nimage. The warped feature are further fed into the generator\nnetwork to output the intermediate frame.\n3. Method\nWe solve the jump cut smoothing problem in two stages\n(see Fig. 2): in the training stage, given a set of source\nimages, our network learns to generate a target image with\nthe corresponding DensePose keypoints as motion guidance.\nIn the inference stage, we synthesize each intermediate frame\nbetween the jump cut end frames guided by the interpolated\nDensePose keypoints and other available frames in the video.\n3.1. DensePose keypoints representation\nWe use DensePose [8] keypoints augmented with face land-\nmarks as motion guidance to synthesize the corresponding\nimage. Given an input image I of size (H \u00d7 W) and its\ncontinuous DensePose P, i.e., image-space UV coordinate\nmap per body part, the DensePose keypoints {xk}K\nk=1 are\nextracted from DensePose by quantizing the UV values. For\neach body part UV map, we discretize it into n\u00d7n cells with\neach cell representing a key point. We focus on the videos\nwhere only the upper body of a person appears, and thus\nthere are K = 14 \u00d7 n \u00d7 n DensePose keypoints per image\n(14 is the number of upper body parts in then DensePose\nrepresentation). The key point coordinates and UV value are\nthe average of the pixel coordinates and UV values which\nfall into the corresponding cell. These keypoints with UVs\nare splatted into the grid of size (H \u00d7 W) based on their\ncoordinates, and then we get a discretized DensePose IUV.\nIf there are not UV values in the DensePose fall into the cell,\nthe corresponding key point is not visible, and will not be\nsplatted.\n3.2. Cross model attention warping\nGiven a set of N source images {Ii}N\ni=1 with their respective\nDensePose keypoints {xi,k} and the target DensePose key-\npoints {xt,k}, we aim at generating a realistic target image\nby transferring the appearances from the source images. The\nbasic idea is that we utilize this dense correspondence to\nwarp the source image features to the target dense key point,\nthen the warped feature is fed into a generator similar as\nCo-Mod GAN [40] to generate the respective realistic target\nimage. Therefore it is critical to get a high quality warped\nFigure 4. Visualization of learned correspondence with our\nattention mechanism. The top left is our synthesized image given\nthe other three images as sources. We highlight the locations in the\nsynthesized image where the peak attention score \u2265 0.75, and show\ntheir learned corresponding locations (marked with same color) in\nthe source images. Our attention picks appropriate feature from\ndifferent sources per location, e.g., for the blue point in the lower\neyelid, our attention learned to associate with the eyelid feature in\nthe bottom right source image.\nfeature. However, the DensePose based correspondence is\noften inaccurate, in addition, in the case of multiple source\nimages, it is more natural to select the one that is closer to the\ntarget image instead of doing naive averaging. we propose\nto use attention mechanism to achieve this selection ability.\nWe adopt the commonly used scaled dot-product atten-\ntion [32]. The input of the attention function consists of\nqueries and keys of dimension dk, and values of dimension\ndv. We compute the dot products of the query with all keys,\ndivide each by \u221adk and apply a softmax function to obtain\nthe weights on the values. The weight can be interpreted as\nthe similarity of the query and the corresponding key, then\nthe output per query is a weighted average of the values. In\npractice, the set of queries are packed together into a matrix\nQ of size nq \u00d7 dk. The keys and values are also packed to-\ngether into matrices K of size nk \u00d7dk and V of size nk \u00d7dv.\nThe matrix of outputs of size nq \u00d7 dv are computed as:\nAttention(Q, K, V ) = Softmax\n\u0012QKT\n\u221adk\n\u0013\nV\n(1)\nQ and K are the feature representation of target and source\nDensePose keypoints respectively, and V is the appearance\nfeature of source images. We use StyleGAN2 [15] based\nencoder with a final project layer to map the feature to the\n4\ndimension dk for query and key. When encoding the source\nDensePose keypoints, we also concatenate it with the source\nimage. The source image appearance features are encoded\nby the encoder with similar structure. All of these encoders\noutput outputs at 1/4 resolution of the inputs. For example,\nthe image and discretized DensePose keypoints input are of\nsize 256 \u00d7 256, the encoders produce nq = 64 \u00d7 64 queries,\nand nk = 64 \u00d7 64 \u00d7 N keys for N number of sources. The\nwarped image features are computed according to Eq. 1,\nand reshaped back to 64 \u00d7 64, which will be fed into the\ngenerator to produce the according target image.\nWith cross model attention based warping, on the one\nhand, the feature representation of the DensePose keypoints\nis more robust and can correct the misalignment of the Dense-\nPose. On the other hand, our method can pick the most\nrelevant source features among multiple source images per\nlocation (see Fig. 4). This is especially useful for jump cut\nsmoothing where we have frames from the entire video as\nsources.\n3.3. Recursive synthesis for jump cut transitions\nWhen creating the transition for jump cut smoothing, we\nfirstly create a linearly interpolated dense keypoints sequence\nbetween the jump cut end frames, then each intermediate\nframe will be generated accordingly. This causes an issue\nwhen occlusion and disocclusion happen between the end\nframes. For example, when the speaker\u2019s head rotates from\none side to the other side, part of the keypoints on the face\ndisappears and another part of keypoints appears. We can\nonly interpolate visible keypoints in both ends, which causes\nincomplete set of keypoints (see Fig. 10). We simulate this\ncase in the training by only use visible keypoints in all source\nimages for the target, and make the generator to learn to\ninpaint the hole and generate realistic image.\nSince dense keypoints only model the foreground part,\nwe additionally concatenating the two end frame features\nwith the warped feature together as input to make the net-\nwork learn to selectively copy the background from the end\nframes as well the remaining part that are not modeled by\nthe DensePose such as hair. We further proposed a recursive\nsynthesis, as shown in Fig. 5, for a transition sequence of\nlength T, we start the synthesis from the two frames that\nare closest to the end frame, i.e., I1 and IT , and then move\ntowards the middle with the synthesized frames before as\nend frames to provide background information.\nFigure 5. Recursive synthesis. To fill in a jump cut with smooth,\nintermediate frames, we recursively fill in frames from the end\ntowards the middle.\n3.4. Blended transition\nGiven a jump cut, we denote the frame before the cut as\nIm and the frame after the cut as In, and their respective\nDensePose keypoints as {xm,k} and {xn,k}, where k =\n1 . . . K. K is the number of keypoints. We provide two\nways to apply a smooth the jump cut between Im and In for\na seamless transition.\nThe first way is to add a T number of intermediate frames\nbetween these two frames. The generation of each middle\nframe It is guided by the linearly interpolated DensePose\nkeypoints {(1 \u2212 \u03b1t)xm,k + \u03b1txn,k}, with \u03b1t = t/(T + 1).\nHowever, accompanying new frames with silent audio results\nin an awkward, broken speech in some cases. For example,\nwhen a person is speaking quickly, but with short \u201cumms\u201d\nor \u201cuhs\u201d in the middle, if we remove these filler words and\nfill in with new frames with silence, the resulting video will\nsound inarticulate.\nThus, we provide another method, called blended transi-\ntion, in order to avoid this audio artifact. Similarly to Zhou\net al. [41], we synthesize blended frames to replace original\nframes around a small temporal neighborhood with a syn-\nthetic transition, so that the video can smoothly transit from\nframes before Im to frames after In, as shown in Fig. 6.\nJump Cut\nBlended Transition\n\u001f\u001e\u001f\u001e\n\u001f\u001e\u001f\u001d\n\u001f\u001e\u001f\u001c\n\u001f\u001e\n\u001d\u001c\u001d\n\u001d\u001c\u001e\n\u001d\u001c\u001c\n\u001d\nFigure 6. Blended transitions. We modify the frames before and\nafter the jump cut, blending them smoothly. This allows us to\nsmooth the jump cut without inserting additional frames.\nWe define the neighborhood using the frame range\n[m \u2212 H, m] and [n, n + H], with H as neighborhood size.\nWe choose H = 4 in our experiments. Each frame Ii is\nblended with frame In, where i \u2208 [m \u2212 H, m], with weight\n\u03b1i \u2208 [0,\n1\n2H , . . . , 1\n2], with DensePose keypoints blended by\n{x\u2032\ni,k = (1 \u2212 \u03b1i)xi,k + \u03b1ixn,k}. Similarly, each frame\nIj, k \u2208 [n, n + H] is blended with frame Im with weight\n\u03b1j \u2208 [ 1\n2, H+1\n2H , . . . , 1] and the corresponding DensePose key-\npoints are {x\u2032\nj,k = (1 \u2212 \u03b1k)xj,k + \u03b1jxm,k}. The blended\nframes are resynthesized and guided by the blended, dense\nkeypoints. The blended transition does not change the exist-\ning number of frames in the video and thus does not suffer\nthe audio insertion issue. Please refer to the supplementary\nfor the video demo with audio for the filler words removal\njump cut smoothing.\n5\nFigure 7. Examples of our talking head videos. We collect 600\ntalking head video clips and smooth jump cuts corresponding to\nfiller words.\n4. Experiments\nWe show our synthesized transition sequence under diverse\njump cut situations. See Fig. 8 for selected examples. Our\nmethod successfully achieves seamless transition under chal-\nlenging head pose changes such as extreme rotation or the\nback-and-forth movement of the head. We show our dataset\ncollection and pre-processing in Sec. 4.1, and the compari-\nson with baselines in Sec. 4.2. We further analyze how using\nmore source frames from the video with our proposed atten-\ntion mechanism improves synthesis quality in Sec. 4.3. Fi-\nnally, we demonstrate how we apply our jump cut smoothing\ntechnique for removing filler words in talking head videos.\nFigure 8. Examples of our synthesized transition sequence from\nthe most left frame to the most right frame. Our method can create\na seamless transition under different head pose changes. Row #2:\nhead moving from back to front; Row #3: head rotating from front\nview to the side view; Row #4: head rotating from one side to\nanother side. Row #5: head moving up and down.\n4.1. Dataset and pre-processing.\nWe target our jump cut smoothing application for the talking\nvideos that contain upper torso. Note that this is a wider\n(and hence more challenging) cropping scheme than many\nexisting talking head video datasets that only contain facial\nportions. Therefore, we collected 600 720p video clips from\nthe raw AVSpeechDataset collection [6] for training and 50\nvideos for testing. Each video contains a person talking ei-\nther facing towards the camera or the interviewer (see Fig. 7)\nfor 10 to 20 seconds in a static background. While we do\nnot add any manual annotation, we use Detectron2 [35] to\ndetect the DensePose body part coordinate map per frame,\nand quantize the DensePose uv map to turn them into key-\npoints. For more accurate facial expression representation,\nwe augment the DensePose keypoints with face landmarks\nusing HRNet [30]. For training, we crop the person with the\ndetected DensePose bounding box, and resize to 256 \u00d7 256.\nIn the training stage, we randomly pick two source and a\ntarget frames. At inference time, our method can be easily\nextended to incorporate more source images (Section 4.3).\nComparison to image animation methods (N/A).\nCut frame 1 Face2Face\u03c1 Face-Vid2Vid\nOurs\nCut frame 2\nFigure 9. Comparison of synthesized images of Face2Face\u03c1, and\nFace-Vid2Vid and our method. Face2Face\u03c1 generates distorted\nimages, while Face-Vid2Vid loses sharpness and facial identity:\nmicrophones are gone and hair color does not match cut frame 2.\n4.2. Evaluation\nWe show how image animation techniques cannot be applied\nto our jump cut smoothing problem in Fig. 3. More specifi-\ncally, FOMM [28] and ImplicitWarping [19] cannot generate\nnew transition frames without driving videos, as their latent\nmotion representations lack 3D correspondence. The same\nkey point could correspond to different head regions (see\nRow#2 in Fig. 3). Face-Vid2Vid [33] and Face2Face\u03c1 ani-\nmate only one image, necessitate using just one of the end\nframes as the source. Fig. 9 presents the generated frame\njust before the second jump cut end frame. Evidently, the\nresulting images sacrifice facial identity and clarity, leading\nto another abrupt jump in the transition.\n6\na) Jump cut frame 1\nb) Jump cut frame 2\nc) Our kpts\nd) Our results\ne) FILM results\nFigure 10. Qualitative comparison with FILM for selected synthesized transition frame from jump cut end frame 1 to frame 2.\nAll\nrotation \u2265 15\u25e6\nrotation \u2265 30\u25e6\nrotation \u2265 45\u25e6\nrotation \u2265 60\u25e6\nMethods\nFID [9]\u2193 PSNR\u2191 ArcFace [2]\u2191 PSNR\u2191 ArcFace [2]\u2191 PSNR\u2191 ArcFace [2]\u2191 PSNR\u2191 ArcFace [2]\u2191\nFILM\n4.99\n25.37\n0.51\n25.68\n0.39\n25.65\n0.33\n25.27\n0.23\nOurs. + FILM kpts\n4.35\n25.29\n0.51\n25.57\n0.41\n25.55\n0.35\n25.23\n0.25\nOurs.\n4.53\n25.19\n0.52\n25.43\n0.45\n25.38\n0.40\n24.84\n0.31\nTable 1. Quantitative comparison with FILM in terms of synthesized frame realism and reconstruction fidelity. Here \u201crotation\u201d means the\nhead rotation along the yaw axis.\u2018 \u2018Ours+FILM kpts\u201d means synthesize the image using DensePose keypoints interpolated with FILM. Our\nmethod outperforms FILM in terms of FID and ArcFace similarity, especially in the cases where head rotation happens in the jump cut.\nComparison to frame interpolation methods. We compare\nour method for jump cut smoothing against the state-of-the-\nart frame interpolation method FILM [25]. For testing, we\nrandomly pick an equally spaced triplet, and run each method\nto synthesize the middle frame based on the two end frames.\nFor our method, the keypoints of the middle frame are gen-\nerated by linearly interpolating the DensePose keypoints of\nthe two end frames. Furthermore, we also report a variant\n7\nthat runs FILM in the keypoint space, and then decodes the\nkeypoints into pixels using our attention-based synthesis\nnetwork. In Table 1, we measure the realism of the synthe-\nsized frames against the target frames with Frechet Inception\nDistance (FID) [9], using around 8000 test frames. In addi-\ntion, we measure the fidelity of synthesized image against\nthe ground truth test frame with PSNR. We also measure\nhow much the facial identity is preserved in the synthesized\nframes by reporting the ArcFace [2] cosine similarity be-\ntween the facial embeddings of source and output frames.\nTo further quantify how our method and FILM perform un-\nder different amount of head pose changes, we categorize\nthe jump cut end frame difference by the head rotation de-\ngrees along the yaw axis, and report the evaluation metrics\nin each category. As shown in Table 1, our method achieves\nlower FID score overall, meaning our method attains higher\nphotorealism than FILM. We also achieve higher ArcFace\nsimilarity across all rotation angles. In particular, our ad-\nvantage grows larger at more extreme head pose changes\n(e.g., rotation \u2265 60\u25e6). FILM, an optical flow-based method,\nlikely fails to find correspondence when the head rotates\nfrom the front view (or more extremely one side view) to\nanother side view, causing distortion in the synthesized face\n(see the rightmost column in Fig. 10 for reference). On the\ncontrary, our method does not rely on the appearance feature\nto find correspondence, as we build upon the DensePose\npriors with attention mechanism to find better correspon-\ndence. Lastly, \u201cOurs+FILM kpts\u201d achieves the lowest FID\nscore, likely because FILM does a better job in synthesizing\ncomplex objects like hands than linear interpolation.\n4.3. Attention with more frames\nOne big advantage of our method compared to flow-based\nframe interpolation methods is that at test time, we can\naccept a larger number of input source frames in the video to\nassist the synthesis process. With attention mechanism, our\nmethod can select the most appropriate feature per location\namong source frames. As shown in Fig. 4, different locations\nin the synthesize image correspond to their most relevant\nlocations that are distributed among different sources. We\nfind that this is especially helpful when certain parts are\noccluded in the two end frames, but are visible in the middle\ntransition frames. For example, in Fig. 11, the speaker\u2019s\nmouth is widely open in one end frame and fully closed in\nthe other end frame (see the images within red rectangle).\nIf we only use these two end frames as source, the person\u2019s\nmouth in the generated middle frame looks unnatural, as\nthere are no teeth features for a \u201chalf-open\u201d mouth in the\nend frames. However, there are teeth features among the\nother video frames. We add 10 extra frames randomly chosen\nfrom the video, and our method now generates a high-quality\nmiddle frame, with teeth exposed in the half-opened mouth.\nFigure 11. Attention with more frames help improving the\nquality. The top left two images within the red rectangle are the end\nframes of the jump cut. The top right image within green rectangle\nshows one of the 10 random images we add as additional sources\nto our attention. In the bottom row, we show our synthesized image\nusing only the two end frames (left), and using the entire 12 source\nimages (right). After adding 10 extra randomly chosen frames from\nthe video, our method generates more accurate teeth representation,\nthanks to the attention mechanism that can incorporate variable\nnumber of source frames as reference.\n4.4. Jump cut smoothing for filler words removal\nWe further demonstrate our method on filler words removal\nvideo editing. We collected several talking videos where the\nperson stutters and then apply filler words detection algo-\nrithm [42] to cut out the filler words, resulting in unnatural\njump cuts. We also manually remove some unwanted pauses\nand repetitive words to output a fluent talking video. Then\nwe apply our jump cut smoothing with blended transition\nin Sec. 3.4 to the video. We show these demo videos with\naudio in the Supplementary Materials.\n5. Discussions and Limitations\nOur method can create smooth transition under diverse jump\ncut cases, especially for the head movement. However, we\nfailed when there are complex hand gesture move. Synthe-\nsizing realistic hand is even more challenging because 1.)\nThe video frames with hand movement often have motion\nblurs, which makes our network hard to discriminate real or\nfake hands. 2.) DensePose itself cannot model fine-grained\nhand features such as fingers. 3.) The hands motion is more\ncomplicated than head. For example, when the speaker\u2019s\nhand moves from clenched to stretched, or from palm facing\nto back facing towards the camera, this non-planer motion\ncannot be modeled with linearly interpolated key points.\n8\nReferences\n[1] Floraine Berthouzoz, Wilmot Li, and Maneesh Agrawala.\nTools for placing cuts and transitions in interview video. ACM\nTransactions on Graphics (TOG), 31(4):1\u20138, 2012. 1, 2\n[2] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.\nArcface: Additive angular margin loss for deep face recog-\nnition. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 4690\u20134699, 2019.\n7, 8\n[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3\n[4] Michail Christos Doukas, Stefanos Zafeiriou, and Viktoriia\nSharmanska. Headgan: One-shot neural head synthesis and\nediting. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 14398\u201314407, 2021. 3\n[5] Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei\nIvakhnenko, Victor Lempitsky, and Egor Zakharov. Megapor-\ntraits: One-shot megapixel neural head avatars. In Proceed-\nings of the 30th ACM International Conference on Multime-\ndia, pages 2663\u20132671, 2022. 3\n[6] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin\nWilson, Avinatan Hassidim, William T Freeman, and Michael\nRubinstein. Looking to listen at the cocktail party: A speaker-\nindependent audio-visual model for speech separation. arXiv\npreprint arXiv:1804.03619, 2018. 6\n[7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 12873\u201312883, 2021. 3\n[8] R\u0131za Alp G\u00a8uler, Natalia Neverova, and Iasonas Kokkinos.\nDensepose: Dense human pose estimation in the wild. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 7297\u20137306, 2018. 2, 4\n[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n7, 8\n[10] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi,\nand Shuchang Zhou.\nRife: Real-time intermediate flow\nestimation for video frame interpolation.\narXiv preprint\narXiv:2011.06294, 2020. 3\n[11] Drew A Hudson and Larry Zitnick. Generative adversarial\ntransformers. In International conference on machine learn-\ning, pages 4487\u20134499. PMLR, 2021. 3\n[12] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,\nCarl Doersch, Catalin Ionescu, David Ding, Skanda Kop-\npula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al.\nPerceiver io: A general architecture for structured inputs &\noutputs. arXiv preprint arXiv:2107.14795, 2021. 4\n[13] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\nAndrew Zisserman, and Joao Carreira. Perceiver: General\nperception with iterative attention. In International confer-\nence on machine learning, pages 4651\u20134664. PMLR, 2021.\n3, 4\n[14] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang,\nErik Learned-Miller, and Jan Kautz. Super slomo: High\nquality estimation of multiple intermediate frames for video\ninterpolation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 9000\u20139008,\n2018. 3\n[15] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improv-\ning the image quality of stylegan. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recog-\nnition, pages 8110\u20138119, 2020. 4\n[16] Ira Kemelmacher-Shlizerman, Eli Shechtman, Rahul Garg,\nand Steven M Seitz. Exploring photobios. ACM Transactions\non Graphics (TOG), 30(4):1\u201310, 2011. 2\n[17] Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun\nPak, Yuseok Ban, and Sangyoun Lee. Adacof: Adaptive\ncollaboration of flows for video frame interpolation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5316\u20135325, 2020. 3\n[18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10012\u201310022, 2021. 3\n[19] Arun Mallya, Ting-Chun Wang, and Ming-Yu Liu. Implicit\nwarping for animation with image sets. In NeurIPS, 2022. 3,\n6\n[20] Simon Niklaus and Feng Liu. Softmax splatting for video\nframe interpolation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5437\u20135446, 2020. 3\n[21] Simon Niklaus, Long Mai, and Feng Liu. Video frame inter-\npolation via adaptive separable convolution. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 261\u2013270, 2017. 2, 3\n[22] Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim.\nBmbc: Bilateral motion estimation with bilateral cost volume\nfor video interpolation. In European Conference on Computer\nVision, pages 109\u2013125. Springer, 2020. 3\n[23] Junheum Park, Chul Lee, and Chang-Su Kim. Asymmetric\nbilateral motion estimation for video frame interpolation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 14539\u201314548, 2021. 3\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In International conference on machine\nlearning, pages 4055\u20134064. PMLR, 2018. 3\n[25] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun,\nCaroline Pantofaru, and Brian Curless. Film: Frame interpola-\ntion for large motion. In European Conference on Computer\nVision, pages 250\u2013266. Springer, 2022. 2, 3, 7\n[26] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 4\n[27] Aliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe.\nAnimating arbitrary objects\n9\nvia deep motion transfer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2377\u20132386, 2019. 3\n[28] Aliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. Advances in Neural Information Processing\nSystems, 32, 2019. 3, 6\n[29] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for artic-\nulated animation. In CVPR, 2021. 3\n[30] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-\nresolution representation learning for human pose estimation.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 5693\u20135703, 2019. 6\n[31] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In International Conference on Machine Learning,\npages 10347\u201310357. PMLR, 2021. 3\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3, 4\n[33] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10039\u201310049, 2021. 3,\n6\n[34] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense predic-\ntion without convolutions. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 568\u2013\n578, 2021. 3\n[35] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 6\n[36] Xing Xu, Tan Wang, Yang Yang, Lin Zuo, Fumin Shen, and\nHeng Tao Shen. Cross-modal attention with semantic consis-\ntence for image\u2013text matching. IEEE transactions on neural\nnetworks and learning systems, 31(12):5412\u20135425, 2020. 3\n[37] Kewei Yang, Kang Chen, Daoliang Guo, Song-Hai Zhang,\nYuan-Chen Guo, and Weidong Zhang. Face2face \u03c1: Real-\ntime high-resolution one-shot face reenactment. In European\nconference on computer vision, pages 55\u201371. Springer, 2022.\n3\n[38] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-\nmodal self-attention network for referring image segmenta-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 10502\u201310511,\n2019. 3\n[39] Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya,\nand Victor Lempitsky. Fast bi-layer neural synthesis of one-\nshot realistic head avatars. In European Conference on Com-\nputer Vision, pages 524\u2013540. Springer, 2020. 3\n[40] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I Chang, and Yan Xu. Large scale image comple-\ntion via co-modulated generative adversarial networks. In In-\nternational Conference on Learning Representations (ICLR),\n2021. 4\n[41] Yang Zhou, Jimei Yang, Dingzeyu Li, Jun Saito, Deepali\nAneja, and Evangelos Kalogerakis. Audio-driven neural ges-\nture reenactment with video motion graphs. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3418\u20133428, 2022. 5\n[42] Ge Zhu, Juan-Pablo Caceres, and Justin Salamon. Filler word\ndetection and classification: A dataset and benchmark. arXiv\npreprint arXiv:2203.15135, 2022. 8\n6. Demo videos\nPlease refer to our web page to see 1.) video demos (with\naudio) of applying our method on filler words removal video\nediting (Sec. 4.4 in the main paper), 2.) video results compar-\nison between our method and FILM for jump cut smoothing,\n3.) how we can control the transition sequence by face\nlandmark manipulation, 4.) visualization of the learned cor-\nrespondence via our attention mechanism.\n7. Network architecture\nGiven multiple source images with the their extract Dense-\nPose keypoints augmented with face landmarks, and the tar-\nget DensePose keypoints with face landmarks, our method\nfirstly learns to use cross model attention to warp the source\nimage features, then the warped image feature is concate-\nnated with the jump cut end image features together to feed\ninto the generator network to produce a target image (see.\nFig. 2 in the main paper). We have three StyleGAN2 based\nencoders: 1) EV to encode source image features served\nas value, 2) EK to encode source dense keypoints features\nas key, 3) EQ to encode target dense keypoints features as\nquery.\nThe network structures of these encoders are similar ex-\ncept for the input layer (see Tab. 2). For the target dense key-\npoints encoder EQ, we concatenate the 3-channel DensePose\nkeypoints (IUV) input with the Gaussian blurred 68-channel\nfacial landmark one encoding map, producing a 71-channel\ninput. For EK, we additional concatenate the source RGB\nimage with the dense keypoints, producing a 74-channel\ninput. For image encoder EV , the input is a 3-channel RGB\nimage.\n256 \u00d7 256\nInput (with channels Cin)\n256 \u00d7 256\nConvLayer (Cin \u2192 32)\n128 \u00d7 128\nResBlock down (32 \u2192 64)\n64 \u00d7 64\nResBlock down (64 \u2192 128)\n64 \u00d7 64\nConvLayer (128 \u2192 128)\n64 \u00d7 64\nProjectionLayer (128 \u2192 64)\nTable 2. Encoder network structure.\nFor the image generator part, we adapt the network struc-\nture in Co-Mod GAN to accept our warped feature at 1/4\n10\nframe 1\nframe 2\nFILM\nOurs. + FILM kpts\nOurs.\nFigure 12. Failure cases of hands synthesis. When the speaker\u2019s hands gesture moves from frame 1 to frame 2, we show the synthesized\nmiddle frame with FILM, Ours. + FILM kpts, and Ours.\nframe 1\nframe 2\nFILM\nOurs.\nOurs. kpts\nFigure 13. Limitations of using DensePose representation. When the speaker wears a hat and rotates from front view to side view, we\nshow the synthesized middle frame with FILM and Ours. The rightmost column shows the corresponding interpolated DensePose keypoints\nwith our method.\nimage resolution as input and generate the output image.The\ngenerator is a UNet network with skip connection between\nthe encoder and decoder layers. Here our encoder and de-\ncoder are not symmetric since the input is the warped image\nfeature, we only keep skip connections on the symmetric\nlayers in the encoder and decoder.\n8. Training details\nWe train our three encoders, Co-Mod GAN generator and\ndiscriminator end-to-end with in the same way as the original\nCo-Mod GAN did. Our models are trained on one Nvidia\nA40 GPU with a batch size of 8, we use Adam optimizer with\nan initial learning rate of 0.0002, and (\u03b21, \u03b22) = (0, 0.99).\nFor the DensePose keypoints discretization (Sec 3.1 in the\nmain paper), we use n = 64 for quantizing the UV map.\nWe adopt two-stage training, in the first stage, we sample\nsource and target triplets from the video per iteration, with\nthe corresponding extracted DensePose keypoints and facial\nlandmarks from the video frame. In the second stage, to\nsimulate the incomplete target DensePose keypoints due to\nlinear interpolation in the inference time, we finetune the\ntrained model in the first stage by only use visible DensePose\nin all source images for the target, and make the generator to\nlearn to inpaint the hole and generate realistic image.\n9. Discussions\nHand gesture movement during the jump cut. Our method\ncan create smooth transition under diverse jump cut cases,\nespecially for the head movement. However, we failed when\nthere are complex hand gesture move. For example, in\nFig. 12, when the speaker\u2019s hands moves from closed to\nstretched, this nonlinear motion cannot be modeled with lin-\near interpolation on the hands dense keypoints, while FILM\nsynthesizes slightly better than our method because it learns\nflow based correspondence in the raw RGB pixel space, and\nthus our method with FILM interpolated DensePose key-\npoints produces more realistic image compared with linearly\ninterpolated keypoints.\nLimitations of DensePose representation. We use pre-\ndefined DensePose keypoints and facial landmarks extracted\nfrom the image itself as input motion. DensePose keypoints\nmap the pixels in the image to points in the 3D human sur-\nface, and thus every keypoint has the 3D semantic correspon-\ndence, which enables us to do keypoints interpolation for the\ntransition sequence for the jump cut. In some cases, this key-\npoint correspondence fails with view point changing when\nthe person wears some accessories. For example, in Fig. 13,\nthe person wears a hat, and part of the hat is overlapped\nwith the forehead, and the thus forehead appearance is not\n11\nconsistent with different views because of the hat. Therefore,\nour synthesized image has slightly blurred artifacts in the\nforehead compared with FILM.\n12\n"
  },
  {
    "title": "Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding",
    "link": "https://arxiv.org/pdf/2401.04575.pdf",
    "upvote": "14",
    "text": "Let\u2019s Go Shopping (LGS) \u2013\nWeb-Scale Image-Text Dataset\nfor Visual Concept Understanding\nYatong Bai1\u2217\nUtsav Garg2\nApaar Shanker2\nHaoming Zhang2\nSamyak Parajuli2\nErhan Bas2\nIsidora Filipovic3\nAmelia N. Chu3\nEugenia D Fomitcheva3\nElliot Branson2\nAerin Kim2\nSomayeh Sojoudi1\nKyunghyun Cho3\n1University of California, Berkeley\n2Scale AI\n3New York University\n\u2217Work done during internship at Scale.\nCorrespondences to yatong_bai@berkeley.edu, aerinykim@gmail.com.\n\u2013 Abstract \u2013\nVision and vision-language applications of neural\nnetworks, such as image classification and caption-\ning, rely on large-scale annotated datasets that re-\nquire non-trivial data-collecting processes. This time-\nconsuming endeavor hinders the emergence of large-\nscale datasets, limiting researchers and practitioners\nto a small number of choices. Therefore, we seek more\nefficient ways to collect and annotate images. Previ-\nous initiatives have gathered captions from HTML\nalt-texts and crawled social media postings, but these\ndata sources suffer from noise, sparsity, or subjectivity.\nFor this reason, we turn to commercial shopping web-\nsites whose data meet three criteria: cleanliness, infor-\nmativeness, and fluency. We introduce the Let\u2019s Go\nShopping (LGS) dataset, a large-scale public dataset\nwith 15 million image-caption pairs from publicly\navailable e-commerce websites. When compared with\nexisting general-domain datasets, the LGS images\nfocus on the foreground object and have less complex\nbackgrounds. Our experiments on LGS show that\nthe classifiers trained on existing benchmark datasets\ndo not readily generalize to e-commerce data, while\nspecific self-supervised visual feature extractors can\nbetter generalize. Furthermore, LGS\u2019s high-quality\ne-commerce-focused images and bimodal nature make\nit advantageous for vision-language bi-modal tasks:\nLGS enables image-captioning models to generate\nricher captions and helps text-to-image generation\nmodels achieve e-commerce style transfer.\n1.\nIntroduction\nComputer vision (CV) and natural language process-\ning (NLP) tasks increasingly rely on pre-trained repre-\nsentations. While NLP representations can be trained\non unannotated raw text, vision applications often con-\nsider pre-training using large-scale datasets with discrete\nclass labels annotated by humans, such as ImageNet\n[9, 51] or OpenImages [30]. Vision-language bimodal\napplications, such as image captioning or visual question\nanswering, similarly rely on large amounts of annotated\ndata. Unfortunately, many of the large-scale bi-modal\ndatasets now in existence, such as CLIP [48], ALIGN\n[25], and JFT300M [8, 19], are not publicly accessible.\nAs a result, research has been constrained to a few\nselected large datasets, such as Conceptual Captions\n[5] and COCO [6]. This shortage of available public\ndatasets can be attributed in part to the time and effort\nrequired to gather, clean, and annotate large datasets.\nTherefore, we adopt a more efficient and scalable high-\nquality data collection pipeline to acquire image-text\npairs easily available on e-commerce websites. While\nsome existing datasets use public websites as annota-\ntion sources, most of them use social media websites\n(RedCaps [11]) or alt-texts1 (Conceptual Captions [53])\nfor this purpose. Nevertheless, social media data suffer\nfrom subjectivity. On the other hand, alt-texts can be\nunacceptably noisy, sometimes merely including unin-\nformative texts such as \u201calt img\u201d, as shown in Figure 1.\nAs a result, we gravitate to e-commerce websites,\n1Alt-texts are short descriptions of HTML website images.\nWhen an image cannot be rendered, the website displays its\nalt-text as a surrogate.\n1\narXiv:2401.04575v2  [cs.CV]  5 Mar 2024\n \n \n \n \n \nShaun the Sheep Coloring Pages \n(Unclear description) \nJu-Ni San Francisco | A San \nFrancisco Food Restaurant Review \n(Does not mention tuna maki) \n \nFigure 1. In comparison to e-commerce prod-\nuct descriptions, alt-text is usually less informa-\ntive, sometimes too broad, or even irrelevant. Figure 2. An e-commerce-based LGS sample instance with image, title,\nand description.\nwhere clean images with objective, accurate, succinct,\nand informative descriptions are abundant, as illustrated\nin Figure 2. Let\u2019s Go Shopping (LGS) dataset collects\n15 million image-description pairs from approximately\n10,000 e-commerce sites selling a wide range of products.\nDue to the nature of e-commerce data, the majority\nof LGS images have a clear background and a static\nfocus on the stated object. On the captions front, LGS\nprovides precise and elaborative captions.\nWe show\nhow highly precise information can be extracted from\ncaptions for vision-language fine-tuning.\nOn the other hand, ImageNet-1k has served as the\nubiquitous go-to pre-training and evaluation dataset for\nvision-only applications. While ImageNet covers a wide\nrange of domains, the diversity of angles and arrange-\nments is restricted. As a result, the literature has shown\nthat ImageNet models do not generalize well to deliber-\nately constructed out-of-distribution (OOD) scenarios\n[3]. This work uses image classification experiments\nto demonstrate that such OOD data is ubiquitous in\ne-commerce applications. We then show that models\ncan benefit from the unique e-commerce distribution\nin image classification, reconstruction, captioning, and\ngeneration tasks.\nSpecifically, we convert the LGS captions into tax-\nonomies and labels and demonstrate a large disparity\nbetween the label distributions of LGS and ImageNet:\neven with best efforts, only 17.6% of the concepts are\nshared between popular ImageNet-1k synsets and the\ne-commerce corpus (more details in Section 3.4). Even\nfor those shared classes, the performance of ImageNet\nmodels degrades significantly. By verifying that the\nLGS classes are well-separable, we conclude that this\nperformance degradation can be mostly attributed to\nthe distributional disparity. To separate the effects of\nlabels and captions and isolate the distribution shift of\nthe images, we consider Masked AutoEncoder (MAE)\n[17], a self-supervised pre-training method that does not\nrely on labels. We show that an MAE model trained\non ImageNet-1k can reconstruct LGS images well, but\nadding LGS to the training data improves the perfor-\nmance on LGS and generalizes better to COCO.\nThe above results demonstrate that while the e-\ncommerce images are from a distribution that is distinct\nfrom current benchmark datasets, the feature extrac-\ntors can be shared. Moreover, we illustrate additional\nmerits of LGS that qualify it as a pre-training dataset.\nSpecifically, the models learned on both LGS and Im-\nageNet have improved linear probing performance on\ncommon downstream tasks such as CIFAR-100 [29] and\nFashion MNIST [63], compared with the ImageNet-only\ncounterparts.\nThe distinctive distribution of LGS also benefits\nvision-language bimodal tasks.\nFor caption genera-\ntion tasks, we train an OFA model [61] on LGS to\ndemonstrate that the more prominent image foreground,\ncleaner image background, and the highly descriptive\ncaptions of LGS enable the model to produce \u201cattribute-\nrich\u201d image captions, which models trained on tradi-\ntional datasets fail to produce.\nFinally, for text-to-image generation tasks, diffusion\nmodels [2, 20, 54] form the currently most popular fam-\nily of methods. To illustrate the efficacy of LGS in this\nsetting, we use Stable Diffusion (SD) [49] and fine-tune\n2\nit in both general and fine-grained settings on subsets\nof the LGS dataset. We demonstrate promising qual-\nitative and quantitative results on adapting existing\ntext-to-image models using LGS for e-commerce-related\ngenerations. Furthermore, with the help of its distinct\nimage style and descriptive captions, LGS can help the\nSD model generate e-commerce-styled images.\nTo make LGS available to the public, we will share\nthe filtered links to the image-caption pairs under the\n\u201cBSD 3-Clause\u201d license (also used in common datasets\nsuch as ImageNet), as was the case for ImageNet. We\nwill also share the downloader so that the exact same\ndataset can be reproduced.\n2.\nRelated Work\n2.1.\nUnimodal Pre-Training Datasets\nPrior to the popularization of bi-modal training, uni-\nmodal data (vision-only or language-only) have been the\nworkhorses for pre-training tasks. On the vision side,\nImageNet-1k and ImageNet-22k are still some of the\nmost prevalent examples, alongside the larger JFT-300M\ndataset. For the e-commerce domain, Fashion MNIST,\nClothing1M [64], Fashion200k [16], and FashionIQ [62]\nhave been proposed to analyze the effects of noisy labels.\nSome of the most common datasets used as general wide-\ndomain downstream tasks include CIFAR-10, CIFAR-\n100, MNIST [32], SVHN [44], and Tiny ImageNet [31].\n2.2.\nVision-and-Language Pre-Training\nDatasets\nThe literature has shown that image-text data from\nCOCO can be used to learn visual features that are com-\npetitive with supervised pre-training [18] on ImageNet\nwhen transferred to downstream tasks [4, 10, 13, 15, 38,\n60, 67]. More recently, CLIP and ALIGN scaled up to\n400M and 1B+ web-curated image-text pairs, enabling\nzero-shot visual recognition on downstream tasks.\nOriginally intended for image-text retrieval and image\ncaptioning, bi-modal datasets are now widely used for\ntraining cross-modal representations [7, 22, 27, 34, 35,\n37, 40, 45, 53, 56, 58, 68] that transfer to downstream\ntasks, such as visual question answering [1, 23, 69],\nreferring expressions [26], and visual reasoning [57, 66].\nIn light of these novel training paradigms, more recent\nworks build larger datasets specifically for vision-and-\nlanguage pre-training.\nExamples include LAIT [47],\nConceptual Captions-12M, and Wikipedia-ImageText\n(WIT) [55], Localized Narratives [46], Visual Genome\n[28], YFCC100M [59]. Similar to these datasets, LGS\nTable 1. The instance count of LGS compared with existing\nbi-modal datasets.\nDatasets\nInstances\nLet\u2019s Go Shopping (this paper)\n14,847,764\nYFCC100M (Yahoo)\n100 million\nRedCaps (University of Michigan)\n12,011,111\nConceptual Captions 12M (Google)\n12,423,374\nWIT-English (Google)\n5,500,746\nLocalized Narratives (Google)\n849,000\nCOCO (Microsoft)\n328,000\nVisual Genome (Stanford)\n108,077\nCLIP (OpenAI)\n400M\nALIGN (Google)\n1.8B\noffers rich semantic data for pre-training applications.\nHowever, our choice of e-commerce data source is unique,\nleading toward distinctive data distribution.\nImage-text datasets are also used for learning visual\nfeatures. The work [33] has proposed to train visual\nn-gram models on YFCC100M, whereas other methods\n[4, 10] aim to learn features from the captions from the\nCOCO dataset [6]. The quality of the resulting features\nis competitive with supervised ImageNet training [18] on\nmany downstream tasks [13, 15, 38, 51, 60]. Moreover,\nthe image-text pre-training schemes scale up to very\nlarger non-public datasets that are even larger than\nLGS [25, 48].\nA core motivation for collecting image-text pairs from\nthe internet is the possibility of scaling up the data\nsize without bearing the prohibitively expensive anno-\ntation costs.\nIn light of this motivation, there have\nbeen multiple efforts to collect large quantities of noisy\nlabels associated with online images, leading to datasets\nsuch as WebVision [36], YFCC100M, JFT-300M, and\nInstagram-3.5B [41].\nExisting multi-modal e-commerce-inspired datasets\ninclude M5Product [12] and DeepFashion [39]. With\n6 million instances, M5Product\u2019s size is around half\nof LGS\u2019s. While M5Product focuses on demonstrating\nthe effectiveness of multi-modal training, this paper\nemphasizes analyzing the e-commerce data distribution\nand how it generalizes to general wide-domain datasets\nin a pre-training setting.\n3.\nThe Let\u2019s Go Shopping (LGS)\nDataset\nWith 14,847,764 image-text pairs, the LGS dataset\nhas a size advantage over many publicly available bi-\n3\nmodel datasets, as presented in Table 1. In this section,\nwe offer additional analysis of the LGS data. For all\nanalysis and experiments in the paper, we use a subset of\nthe instances with 13 million instances, as the rest of the\ndataset was constructed in parallel with the experiments.\n3.1.\nData Collection\nTo create training data that is truly representative\nof e-commerce data as a whole, we include a wide range\nof commerce websites with various product kinds, such\nas infant products, sporting goods, bridal jewelry, etc.\nThe collection pipeline starts with a set of heuristic\nrules to isolate the product pages from the non-product\npages of an e-commerce website. Then, our automated\nextractor obtains relevant information on each product\npage, including the product title, the description, and\nthe first listed image. Some products may include nu-\nmerous variants (e. g., different colors for a type of\nT-shirt), and we collect all variants. We avoid crawl-\ning information that the sellers are unwilling to share.\nSpecifically, the extractor is forbidden from crawling\npages with a \u2018Disallow\u2019 extension. Finally, we use strict\nautomated tests to filter out the instances with potential\nquality issues. Examples of the tests include confirming\nthat the price is a number, certifying that the images\nare valid, and ensuring that the product title exists and\ncontains no unexpected characters.\n3.2.\nCharacteristics of LGS Images\nIn general-domain image-caption datasets, the im-\nages usually consist of one or more subjects juxtaposed\nagainst a rich background, and their captions often men-\ntion the background. In contrast, e-commerce product\nthumbnails in LGS often depict only one in-animate\nitem that occupies the foreground without any asso-\nciation with the background. The background is also\noften a single color, with some examples shown in Fig. 3.\nThese clear backgrounds make it easier for models to\nlocate the patterns that correspond to their tasks.\n3.3.\nCharacteristics of LGS Captions\nIn this subsection, we analyze the traits of the LGS\ncaptions. The LGS dataset has 14,847,764 captions in\ntotal, and the words and phrases in LGS captions are di-\nverse. For example, while LGS has around 3x more cap-\ntions than COCO2, its captions possess about 20x more\nuni-grams, bi-grams, and tri-grams, with more detailed\nstatistics presented in Appendix A.2. Table 2 presents\n2Each COCO instance has five corresponding captions, and\nwe consider each of them separately.\nTable 2. Comparing the word count statistics of the LGS\nand COCO captions.\nDataset\nMin\nMax\nMean\nMedian\nSkew\nLGS\n2\n3642\n89.58\n67\n3.44\nCOCO\n5\n50\n10.56\n10\n2.76\nTable 3. The POS\u2019s that occur at least ten times.\nDataset\nC. Nouns\nP. Nouns\nAdjectives\nVerbs\nLGS\n158,479\n139,174\n48,907\n57,481\nCOCO\n10,403\n1,655\n3,053\n4,961\nsome statistics of the word distribution of the captions,\nshowing that both LGS and COCO have highly posi-\ntively skewed distributions, with LGS having a longer\ntail. Since LGS incorporates data from a large variety\nof e-commerce websites, the descriptions can include\nrich information. In the subsequent sections, we show\nthat while the raw captions of LGS are diverse, clear\nstructural information can be extracted from the LGS\ncaptions for fine-tuning purposes.\nAdditionally, we use the part-of-speech (POS) tag-\nging method from the Spacy library [21] to analyze\nthe linguistic statistics of the LGS captions, comparing\ncommon nouns, proper nouns, adjectives, and verbs.\nSection 3.3 illustrates that LGS has at least 10x more\nwords per POS compared with COCO, whereas Figures\nSupp-5 and Supp-6 in the supplementary materials pro-\nvide further insights into the composition of each word\ntype. Due to the e-commerce nature of LGS, a large\nportion of the instances is clothing and other wearable\nitems. Thus, within LGS, the proper nouns often present\nthe brand names and sizes, the common nouns often\ndescribe the materials, and the adjectives and verbs\noften characterize the product-specific descriptions and\nactions, making the LGS captions highly descriptive.\n3.4.\nLGS for Classification\nWhile the raw data format of LGS is image-caption\npairs, we also experimented with image classification\nwith LGS by labeling the classes.\nSpecifically, we\nbuild three classification variants: LGS-117, LGS-710,\nand LGS-Overlap.\nFor all three variants, we use a\ntaxonomy generation language model pre-trained in-\nhouse to convert each product description into a tax-\nonomy tree, whose nodes are designed to be informa-\ntive for e-commerce catalog applications.\nThe end\n4\nsocks\nbracelets\nOutfit Sets\nShoulder Bags\nSweatshirts and Hoodies\nRoller Skates\nFigure 3. Examples of LGS images with taxonomy end leaves\nleaf of each taxonomy tree is then used as the label,\nwith some examples displayed in Figure 3. The tax-\nonomy tree can also be used to generate summarized\nimage captions that include product title, product brand\nname, and a number of \u201cbullet strings\u201d describing spe-\ncific product attributes. The bullet strings include ex-\namples such as Nylon fabric, Classic collar, and\nFront zipper fastening.\nThe LGS leaves form a\nlong-tailed distribution that emphasizes common daily\ncommodities, with the five most common leaves being\nTops and T-shirts, Dresses, Rings, T-shirts, and\nSweatshirts and Hoodies. For each of the three clas-\nsification variants, we further clean the end leaves, with\ndetails provided in the two following paragraphs. In Fig-\nure Supp-1 in the supplementary materials, we provide\na histogram of the end leaf distribution.\nLGS-117 and LGS-710 are designed as pre-training\ndatasets. Within all raw labels generated by the tax-\nonomy model, there are synonyms and overlaps that\nshould be unified. After manually merging the synonyms\namong the most popular classes, we observe 117 classes\nthat contain at least 10k images. We select 10k images\nfrom each class, forming the balanced LGS-117 dataset.\nLGS-710 is an unbalanced dataset that includes more\nscarce classes. To accelerate label engineering, we use a\nsemi-automated pipeline. First, we remove uninforma-\ntive words like \u201cother\u201d and parse juxtaposed nouns by\ncommas and \u201cand\u201d. Next, we use a pre-trained language\nmodel to extract the embedding of each parsed noun. As\nan example, for the leaf Tops and T-shirts, we embed\nboth tops and t-shirts. We then consider the \u201csimi-\nlarity\u201d between two classes to be the maximum cosine\nsimilarity between all pairs of corresponding nouns. Very\nclose classes are merged based on a similarity threshold\nof 0.92, which is determined by manually inspecting the\nmerged classes.\nLGS-Overlap is proposed as an out-of-distribution\ntest set for models trained on ImageNet-1k, one of the\nmost widely used benchmarking datasets. We use a sim-\nilar semi-automated pipeline to merge LGS classes with\nImageNet synsets [9, 43]. We optimize the pipeline by\nadjusting the similarity threshold to 0.90 and including\nadditional pre-processing steps such as singularization\nand keyword merging. Note that polysemous words in\nthe labels can refer to different objects in LGS and Ima-\ngeNet. For example, \u201ccricket\u201d in LGS refers to sports\nequipment but refers to the insect species in ImageNet.\nThus, a manual inspection of the merged classes is per-\nformed. After discarding classes with less than 20 in-\nstances, we gather the remaining 176 ImageNet synsets\nthat align with the LGS end leaves and use them as the\nLGS-Overlap dataset. The fact that only 17.6% of the\nImageNet synsets are matched shows a significant label\ndistribution difference between e-commerce applications\nand common pre-training datasets. Since a higher level\nof label-space alignment is essential for more effective\npre-training [41], LGS forms a representative benchmark\nand a pre-training dataset for downstream tasks that\nsee distributions close to e-commerce.\n4.\nExperiments\n4.1.\nImage Classification and Recon-\nstruction\nIn this subsection, we use image classification and\nreconstruction tasks to characterize the distributional\ndifference between LGS and ImageNet. We consider the\ndistributions of images as well as the labels.\n4.1.1\nImageNet Classifiers Do Not Readily Gen-\neralize to E-commerce\nThe existing literature has shown that carefully con-\nstructed images collected in a bias-controlled manner can\nelicit a significant performance degradation on classifiers\ntrained on ImageNet [3]. By applying pre-trained Ima-\ngeNet classification models to the LGS-Overlap dataset\nwithout further training, we show that such out-of-\ndistribution examples naturally exist in the e-commerce\ndomain. Specifically, we use publicly available weights of\na ResNet-50 model and a ConvNeXT-Base model. The\n5\nTable 4. The classification accuracy of models trained on LGS\nshows that the LGS end leaves are well-separable.\nLGS-117\nLGS-117\nLGS-710\nLGS Accuracy\nfrom scratch\nIN-pretrained\nIN-pretrained\n(Top-1)\n(Top-5)\nAfter linear probing\n\u2013\n69.58 %\n60.72 %\n81.16 %\nAfter fine-tuning\n97.89 %\n98.16 %\n77.27 %\n89.09 %\nTable 5.\nThe reconstruction quality of the MAE\nmodels trained on LGS and ImageNet, evaluated on\nCOCO. The symbol \u2191 denotes \u201chigher is better\u201d while\n\u2193 means \u201clower is better\u201d.\nTraining Dataset\nInception (\u2191)\nFID (\u2193)\nImageNet-1k\n9.2930\n114.60\nIN pretrain\u2192IN+LGS\n9.1906\n115.48\nLGS\n10.187\n91.387\nResNet-50 achieves a 74% average recall across the 176\noverlapping synsets over the ImageNet images, but the\nnumber noticeably reduces to 46.43% on LGS-Overlap.\nThe ConvNeXT-Base obtains 79.00% and 50.14% on Im-\nageNet and LGS-Overlap, respectively. This difference\nhighlights that existing ImageNet models do not readily\ntransfer to LGS instances. In addition to having a dif-\nferent label distribution, the e-commerce domain forms\na natural distribution shift even for the classes that also\nexist in ImageNet. While taxonomy standardization\ntechniques exist, aligning and merging the label space is\nstill hard in general. Thus, a pre-training dataset that\nis more aligned with e-commerce is necessary, and LGS\nfulfills this role.\nWe further show that LGS end leaves are well-\nseparable, verifying that the performance degradation of\nImageNet models is caused by the distribution mismatch\nand not the ambiguity of the LGS classes. Note that\nSection 3.4 illustrates that the models learned on LGS-\n117 / LGS-710 can achieve high accuracy on LGS-117 /\nLGS-710. Specifically, we consider the \u201clinear probing\nfollowed by fine-tuning\u201d training schedule, a transfer\nlearning scheme that has been shown to improve the ro-\nbustness against distribution shift by avoiding significant\ndistortions of the pre-trained weights.\n4.1.2\nNon-classification Visual Feature Extrac-\ntors Can Generalize\nSince the image-label correspondence is different be-\ntween LGS and ImageNet, we use self-supervised train-\ning to isolate this mismatch and focus on the distribution\nof images. In the context of transfer learning, since self-\nsupervised training does not use labels, it circumvents\nthe issue of label space mismatch between target and\nsource domains, which has been shown to undermine\nthe quality of transfer learning. Masked AutoEncoder\n(MAE) [17] is a self-supervised method designed for\npre-training. Thus, we compare the performance of an\nMAE trained on ImageNet only with an MAE trained\non ImageNet and LGS-710. Figure 4 shows that the\nTable 6.\nLinear probing accuracy of the self-supervised\nMAE models with three different initializations. A: base-\nline ImageNet MAE model [17], B: LGS MAE model, C:\nLGS+ImageNet MAE model. Specifically, C is initialized\nwith A followed by 150 epochs on mixed Imagenet and LGS-\n710 data (1:1 ratio). Fine-tuning on LGS-117 and Imagenet\ndatasets used 40 and 60 epochs, respectively.\nMAE Training Setting\nLinear probing dataset\nA\nB\nC\nLGS-117 (40 epochs)\n72.98 %\n76.37 %\n76.87 %\nImageNet-1k (60 epochs)\n67.78 %\n46.37 %\n65.29 %\nMAE trained on ImageNet can reconstruct a reasonable\nLGS image, but the reconstruction quality of the Ima-\ngeNet+LGS model is better, demonstrating that LGS\ncan be used to learn e-commerce visual features.\nTo quantitatively demonstrate the generalizability\nof the vision feature extractors, we evaluate the recon-\nstruction performance of the MAE models trained on\nLGS and ImageNet on COCO. The qualities of the raw\nreconstructions obtained by the models are presented in\nTable 5. While LGS is more domain-specific compared\nwith ImageNet and COCO (both of which cover a wide\nrange of domains), the MAE trained on LGS is able\nto generate COCO images with higher qualities com-\npared with the ImageNet model. Furthermore, we use\nSection 4.1.2 to show that upon the visual embeddings\nlearned jointly on ImageNet and LGS, a linear classi-\nfier with a satisfactory performance can be learned on\nboth ImageNet and LGS. The above results verify that\nthe feature extractors can generalize between LGS and\ngeneral-domain datasets, despite the separation of the\nintermediate visual embeddings (which are visualized in\nAppendix B.1).\nBased on the above observations, we infer that\nthe e-commerce data distribution, represented by the\nLGS dataset, significantly differs from existing general\ndatasets in the label space, while visual features can\n6\noriginal\nmasked\nreconstruct \n ImageNet MAE\nreconstruct + visible \n ImageNet MAE\nreconstruct \n ImageNet+LGS MAE\nreconstruct + visible \n ImageNet+LGS MAE\nFigure 4. While an MAE trained on ImageNet can reasonably reconstruct an LGS image, adding LGS instances to the\ntraining improves the reconstruction quality.\noriginal\nmasked\nreconstruct \n ImageNet MAE\nreconstruct + visible \n ImageNet MAE\nreconstruct \n ImageNet+LGS MAE\nreconstruct + visible \n ImageNet+LGS MAE\nFigure 5. Adding LGS instances to the training also improves the reconstruction on some ImageNet instances.\nTable 7. ImageNet\u2192LGS-710 two-phase pre-training improves downstream linear probing accuracy for downstream tasks\nincluding CIFAR-100, Fashion MNIST, and Clothing1M. On Clothing1M, whose data also comes from the e-commerce\ndomain, the LGS-pre-trained features also improve end-to-end fine-tuning performance. For Clothing1M, we only use its\nclean training set, whereas Clothing1M (10%) is a few-shot setup that trains on a 10% subset of the clean training set.\nLinear Probing\nEnd-to-end training\nPre-training Setup\nCIFAR-10\nCIFAR-100\nFashion\nMNIST\nClothing1M\n(10 %)\nClothing1M\n(100 %)\nClothing1M\n(10 %)\nClothing1M\n(100 %)\nImageNet\n61.97\n40.46\n79.68\n59.74\n67.57\n65.69\n74.81\nImageNet\u2192LGS-117\n59.83\n35.57\n80.39\n64.48\n69.67\n68.16\n75.47\nImageNet\u2192LGS-710\n58.81\n42.21\n82.18\n64.16\n70.06\n65.85\n74.51\ngeneralize. Thus, LGS is an ideal pre-training dataset\nfor downstream tasks whose class distributions align\nwith the e-commerce domain.\n4.1.3\nLGS Supplements ImageNet as a Pre-\ntraining Dataset\nLGS can also widen the span of the pre-training distri-\nbution when used in conjunction with ImageNet, acting\nas a bridge between general visual features and domain-\nspecific applications. Specifically, Table 7 shows that a\ntwo-phase ImageNet\u2192LGS-710 weakly-supervised pre-\ntraining scheme produces features more suitable for fine-\ntuning on common downstream tasks. On e-commerce-\nrelated downstream datasets such as Clothing1M, the\nmodels pre-trained on LGS also excel in both linear\nprobing and end-to-end settings.\nIn linear probing experiments, we observe that in-\ncorporating in-domain pre-training (both LGS-117 and\nLGS-710) results in better performance (2% absolute)\ncompared to ImageNet pre-training.\nMoreover, in\nlimited-data settings, we observe less model regression\ncompared to the full-data setups. For example, for fine-\ntuning a linear classifier on 10% of the Clothing1M-clean\ndataset, the ImageNet pre-trained model regresses more\n(11.5% relative) compared to LGS-117 and LGS-710\npre-trained models (7.4 and 8.4% relative respectively).\nWhen models are trained end-to-end, we observe that the\npre-training setup is less critical in fine-tuning the full\nClothing1M-clean training dataset. However, for limited-\ndata experiments, filtering out under-represented classes\n(LGS-117) in pre-training helps with the downstream\nfine-tuning results (2% absolute) compared to both Im-\nageNet and LGS-710 datasets.\nIn Appendix B.2 in the supplementary materials, we\nuse GradCam [14, 52] to visualize the representations\n7\nTable 8. IC model performance of image-captioning task\nevaluated on different combinations of training and evalua-\ntion datasets.\nTraining Set\nTest Set\nMETEOR (\u2191)\nLGS-title\nLGS-title\n0.184\nLGS-description\nLGS-title\n0.161\nLGS-taxonomy\nLGS-taxonomy\n0.584\nCOCO\nLGS-title\n0.069\nlearned by the classification models, demonstrating that\nthe LGS models look for much more localized patterns\nthat are relevant to e-commerce classification.\n4.2.\nCaption Generation\nIn this section, we illustrate that the distinct distri-\nbution of LGS benefits vision-language bi-modal tasks.\nSpecifically, we study the efficacy of image-captioning\n(IC) models trained on traditional datasets in predicting\nLGS-type descriptions. We also evaluate the perfor-\nmance of LGS-trained models in generating attribute-\nrich image captions that would otherwise not be possible\nfor models trained on more traditional datasets.\nIn this experiment, we utilize a bi-modal modeling\nframework based on OFA [61], a recently proposed\nencoder-decoder architecture that has achieved state-\nof-the-art performances in many language-vision tasks.\nFor each LGS image, the corresponding caption can be\nconstructed by concatenating the \u201cproduct description\u201d\nstrings in various orders. Specifically, we create three\ntypes of captions:\n1. LGS-title : title and brand name;\n2. LGS-taxonomy : product taxonomy;\n3. LGS-description: concatenated bullet strings.\nThe OFA IC model was trained on the three types of\nLGS inputs as well as on the traditional COCO dataset.\nThe IC model performance in terms of its ability to\npredict the appropriate target string is tabulated in\nTable 8.\n4.3.\nText-to-Image Generation\nBecause of its high-quality e-commerce-focused im-\nages and bimodal nature, LGS is an ideal option for\ntraining text-to-image models in the e-commerce sector,\nserving as a bridge between general visual features and\ndomain-specific applications. In this section, we use\nLGS to adapt the Stable Diffusion (SD) text-to-image\nLGS Examples\na photo of terez\nleggings navy camo\nstripe hi-shine bump\nsquad leggings,\ne-commerce\na photo of ana silver\nco. rings apatite\nring size 8.25 (925\nsterling silver)\nring81016,\ne-commerce\nDeepFashion InShop Examples\na photo of Dresses\nIvory-navy Forever\n21 Contemporary -\nShow the perfect\namount of skin in\nthis sleek,\nsophisticated\nsurplice dress,\ne-commerce\na photo of Pants\nBlack-grey This\njogger\u2019s easy,\nslouchy silhouette\ngets a little grit\ncourtesy of its\neye-popping print of\nphotorealistic roses,\ne-commerce\n(a) Input Prompt\n(b) Vanilla SD\n(c) LGS-117\nFigure 6. Qualitative comparisons of the generations of\nthe Vanilla and the LGS-117-fine-tuned SD models in the\ngeneral setting. The fine-tuned model generates more visu-\nally appealing images.\nTable 9. Comparing the Vanilla SD and the LGS-117 fine-\ntuned model on LGS and DeepFashion datasets.\nModel\nTest Set\nFID (\u2193)\nVanilla\nLGS Val\n25.3498\nVanilla + LGS-117\nLGS Val\n24.1952\nVanilla\nDeepFashion\n62.9269\nVanilla + LGS-117\nDeepFashion\n74.0185\ngeneration method to two e-commerce scenarios: general\nand fine-grained. For both scenarios, we fine-tune based\non the sd-v1-4 (referred to as Vanilla) SD checkpoint.\nFor the general setting, we add a domain identifier\nto all training prompts associated with LGS images and\nguide the SD model to adapt to the e-commerce image\nstyle when this identifier is provided. The choice of\n8\na photo of new\nbalance men white\nrunning shoes\na photo of on\nrunning athletic\nshoes on running\nwomen\u2019s cloudswift\nroad shoe 41.99578\nin lake sky\nFigure 7. The LGS-117-fine-tuned SD model also generates\nmore visually appealing images in the fine-grained setting.\nThe prompts are from LGS.\nthe domain identifier is crucial, as the paper [50] shows\nthat a domain identifier with a strong prior should be\navoided. For example, the word retail has a strong\nprior, and the pre-trained \u201cVanilla\u201d SD model confi-\ndently associates it with (physical) retail stores. This\nbehavior is undesirable for the goal of e-commerce style\ntransfer. By analyzing the effects of various domain iden-\ntifiers on the generations of the pre-trained SD model,\nwe determine that the word \u201ce-commerce\u201d gives a weak\nprior and is a suitable identifier. We then construct the\nground-truth training prompts for the LGS images in the\nformat of a photo of <brand> <end_leaf> <title>,\ne-commerce, where the <end_leaf> refers to the end\nleaf of the taxonomy tree introduced in Section 3.4. The\n\u201cVanilla\u201d SD checkpoint is fine-tuned on one million LGS\nimage-prompt pairs for 100k steps with a batch size of\n24. Table 9 displays the quantitative results on an un-\nseen validation set (5K image-prompt pairs) from LGS\nand a subset of the DeepFashion InShop dataset. The\nfine-tuning process enhances the performance of SD on\nLGS as expected. While the FID scores on DeepFashion\nare lower, the generations of the LGS-117 fine-tuned\nmodel are aesthetically more appealing. At this instant,\nthere are no quantitative metrics that directly measure\naesthetic superiority. Thus, we present Figure 6 and\nthe additional examples in Appendix C.1 in the sup-\nplementary materials (Figures Supp-9 and Supp-8) to\ndemonstrate the aesthetic improvement qualitatively.\nThe lower FID scores may indicate a distribution shift\nbetween LGS and DeepFashion images.\nFor the fine-grained setting, we use data belonging\nto only a particular end leaf, using the same prompt\nwithout the additional identifier.\nThe checkpoint is\nfine-tuned with 10k image-prompt pairs for 25k steps\nwith a batch size of 6.\nWe use the \u201cathletic shoes\u201d\nend leaf as an example and compare the generations\nbefore and after LGS-fine-tuning under the fine-grained\nsetting in Figure 7. As did the general setting results,\nthe fine-grained examples also indicate that LGS helps\nadapt text-to-image models to e-commerce scenarios\nand improves image quality and aesthetics.\n5.\nConclusion\nThe Let\u2019s Go Shopping (LGS) dataset consists of 15\nmillion pairs of publically-accessible diverse images and\ndescriptive captions from e-commerce websites. Our effi-\ncient semi-automated gathering and annotation pipeline\nensure scalable data collection.\nWe then use LGS\nto show that while the categories associated with e-\ncommerce data may not align with the general-domain\npre-training datasets, visual feature extractors can be\nshared. Finally, we show that the distinct distribution\noffered by LGS and LGS\u2019s bi-modal nature can be ben-\neficial for applications including image classification,\nimage reconstruction, bi-modal representation learning,\nand text-to-image generation.\n6.\nAcknowledgment\nWe would like to appreciate Matteo Bruno, James\nCaulkins, Xiaowen Dong, Peter Grindrod, Jack Hessel,\nSean Holloway, Dorothy Nicholas, Janet Pierrehumbert,\nand Julian Winkler for their valuable help and fruitful\ndiscussions.\nWe also appreciate Baillie Gifford, the Institute for\nNew Economic Thinking at the Oxford Martin School,\nand the UK Engineering and Physical Science Research\nCouncil for funding our work at the University of Oxford.\nThis work was supported in part through the NYU IT\nHigh-Performance Computing resources, services, and\nstaff expertise.\n9\nReferences\n[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. VQA: Visual question answering. In ICCV,\n2015. 3\n[2] Yatong Bai,\nTrung Dang,\nDung Tran,\nKazuhito\nKoishida, and Somayeh Sojoudi. Accelerating diffusion-\nbased text-to-audio generation with consistency distil-\nlation. arXiv preprint arXiv:2309.10740, 2023. 2\n[3] Andrei Barbu, David Mayo, Julian Alverio, William\nLuo, Christopher Wang, Dan Gutfreund, Josh Tenen-\nbaum, and Boris Katz. Objectnet: A large-scale bias-\ncontrolled dataset for pushing the limits of object recog-\nnition models. In Advances in Neural Information Pro-\ncessing Systems, 2019. 2, 5\n[4] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus.\nLearning visual representations with caption annota-\ntions. In ECCV, 2020. 3\n[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. Conceptual 12M: Pushing Web-Scale\nImage-Text Pre-Training To Recognize Long-Tail Visual\nConcepts. In CVPR, 2021. 1\n[6] Xinlei Chen,\nHao Fang,\nTsung-Yi Lin,\nRamakr-\nishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick.\nMicrosoft COCO captions:\nData collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015. 1, 3\n[7] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUniter: Learning universal image-text representations.\narXiv preprint arXiv:1909.11740, 2019. 3\n[8] Francois Chollet. Xception: Deep Learning with Depth-\nwise Separable Convolutions. In CVPR, 2017. 1\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR, 2009. 1, 5\n[10] Karan Desai and Justin Johnson. VirTex: Learning\nVisual Representations from Textual Annotations. In\nCVPR, 2020. 3\n[11] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin\nJohnson. Redcaps: web-curated image-text data created\nby the people, for the people. ArXiv, abs/2111.11431,\n2021. 1\n[12] Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei,\nMichael C Kampffmeyer, Xiaoyong Wei, Minlong Lu,\nYaowei Wang, and Xiaodan Liang. M5product: Self-\nharmonized contrastive learning for e-commercial multi-\nmodal pretraining. In CVPR, 2022. 3\n[13] Mark Everingham, Luc Van Gool, Christopher K. I.\nWilliams, John M. Winn, and Andrew Zisserman. The\npascal visual object classes (VOC) challenge. IJCV,\n2009. 3\n[14] Jacob Gildenblat and contributors.\nPytorch library\nfor cam methods.\nhttps://github.com/jacobgil/\npytorch-grad-cam, 2021. 7, 15\n[15] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS:\nA dataset for large vocabulary instance segmentation.\nIn CVPR, 2019. 3\n[16] Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao\nZhang, Menglong Zhu, Yuan Li, Yang Zhao, and Larry S\nDavis. Automatic spatially-aware fashion concept dis-\ncovery. In Proceedings of the IEEE international con-\nference on computer vision, 2017. 3\n[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,\nPiotr Doll\u00e1r, and Ross Girshick. Masked autoencoders\nare scalable vision learners.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2022. 2, 6\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\nCVPR, 2016. 3\n[19] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling\nthe Knowledge in a Neural Network. NeurIPS Deep\nLearning and Representation Learning Workshop, 2015.\n1\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising\ndiffusion probabilistic models. In Advances in Neural\nInformation Processing Systems, 2020. 2\n[21] Matthew Honnibal, Ines Montani, Sofie Van Landeghem,\nand Adriane Boyd. spaCy: Industrial-strength Natural\nLanguage Processing in Python. 2020. 4\n[22] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu,\nand Jianlong Fu. Pixel-BERT: Aligning Image Pixels\nwith Text by Deep Multi-Modal Transformers. arXiv\npreprint arXiv:2004.00849, 2020. 3\n[23] Drew A Hudson and Christopher D Manning. GQA:\nA New Dataset for Real-world Visual Reasoning and\nCompositional Question Answering. In CVPR, 2019. 3\n[24] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Lo-\ngan Engstrom, Brandon Tran, and Aleksander Madry.\nAdversarial examples are not bugs, they are features.\nAdvances in neural information processing systems, 2019.\n15\n[25] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen\nLi, and Tom Duerig. Scaling Up Visual and Vision-\nLanguage Representation Learning With Noisy Text\n10\nSupervision. arXiv preprint arXiv:2102.05918, 2021. 1,\n3\n[26] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,\nand Tamara Berg. Referitgame: Referring to objects in\nphotographs of natural scenes. In EMNLP, 2014. 3\n[27] Wonjae Kim, Bokyung Son, and Ildoo Kim.\nViLT:\nVision-and-Language Transformer Without Convolution\nor Region Supervision. In ICML, 2021. 3\n[28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yan-\nnis Kalantidis, Li-Jia Li, David A Shamma, Michael S\nBernstein, and Li Fei-Fei. Visual Genome: Connecting\nLanguage and Vision using Crowdsourced Dense Image\nAnnotations. IJCV, 2017. 3\n[29] Alex Krizhevsky. Learning multiple layers of features\nfrom tiny images, 2012. 2\n[30] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper\nUijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,\nStefan Popov, Matteo Malloci, Alexander Kolesnikov,\net al. The open images dataset v4. International Journal\nof Computer Vision, 128(7):1956\u20131981, 2020. 1\n[31] Ya Le and Xuan S. Yang. Tiny imagenet visual recog-\nnition challenge. 2015. 3\n[32] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist\nhandwritten digit database. ATT Labs, 2, 2010. 3\n[33] Ang Li, Allan Jabri, Armand Joulin, and Laurens\nvan der Maaten. Learning visual n-grams from web\ndata. In ICCV, 2017. 3\n[34] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and\nMing Zhou. Unicoder-VL: A universal encoder for vision\nand language by cross-modal pre-training. AAAI, 2020.\n3\n[35] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. VisualBERT: A simple\nand performant baseline for vision and language. arXiv\npreprint arXiv:1908.03557, 2019. 3\n[36] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and\nLuc Van Gool. WebVision Database: Visual Learning\nand Understanding from Web Data. arXiv preprint\narXiv:1708.02862, 2017. 3\n[37] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. Oscar: Object-semantics\naligned pre-training for vision-language tasks. In ECCV,\n2020. 3\n[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft COCO: Common objects\nin context. In ECCV, 2014. 3\n[39] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and\nXiaoou Tang. Deepfashion: Powering robust clothes\nrecognition and retrieval with rich annotations.\nIn\nCVPR, 2016. 3\n[40] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\nViLBERT: Pretraining task-agnostic visiolinguistic rep-\nresentations for vision-and-language tasks. In NeurIPS,\n2019. 3\n[41] Dhruv Kumar Mahajan, Ross B. Girshick, Vignesh\nRamanathan, Kaiming He, Manohar Paluri, Yixuan\nLi, Ashwin Bharambe, and Laurens van der Maaten.\nExploring the limits of weakly supervised pretraining.\nIn ECCV, 2018. 3, 5\n[42] Leland McInnes, John Healy, Nathaniel Saul, and Lukas\nGro\u00dfberger.\nUmap:\nUniform manifold approxima-\ntion and projection. Journal of Open Source Software,\n3(29):861, 2018. 15\n[43] George A. Miller.\nWordnet: A lexical database for\nenglish. Communications of the ACM, 38(11):39\u201341,\n1995. 5\n[44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro\nBissacco, Bo Wu, and Andrew Y. Ng. Reading digits in\nnatural images with unsupervised feature learning. In\nNeurIPS Workshop on Deep Learning and Unsupervised\nFeature Learning 2011, 2011. 3\n[45] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\nIm2Text: Describing Images Using 1 Million Captioned\nPhotographs. In NeurIPS, 2011. 3\n[46] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo,\nRadu Soricut, and Vittorio Ferrari. Connecting vision\nand language with localized narratives. In ECCV, 2020.\n3\n[47] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti,\nand Arun Sacheti.\nImageBERT: Cross-modal Pre-\ntraining with Large-scale Weak-supervised Image-Text\nData. arXiv preprint arXiv:2001.07966, 2020. 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning Transferable Vi-\nsual Models From Natural Language Supervision. arXiv\npreprint arXiv:2103.00020, 2021. 1, 3\n[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models, 2021. 2\n[50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael\nPritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth: Fine tuning text-to-image diffusion models for\nsubject-driven generation. 2022. 9\n11\n[51] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, et al. Ima-\ngenet Large Scale Visual Recognition Challenge. IJCV,\n2015. 1, 3\n[52] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv\nBatra. Grad-cam: Visual explanations from deep net-\nworks via gradient-based localization. In Proceedings of\nthe IEEE international conference on computer vision,\n2017. 7, 15\n[53] Piyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. Conceptual Captions: A Cleaned, Hyper-\nnymed, Image Alt-text Dataset for Automatic Image\nCaptioning. In ACL, 2018. 1, 3\n[54] Jascha\nSohl-Dickstein,\nEric\nWeiss,\nNiru\nMah-\neswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In In-\nternational Conference on Machine Learning, 2015. 2\n[55] Krishna Srinivasan, Karthik Raman, Jiecao Chen,\nMichael Bendersky, and Marc Najork. WIT: Wikipedia-\nbased Image Text Dataset for Multimodal Multilingual\nMachine Learning. arXiv preprint arXiv:2103.01913,\n2021. 3\n[56] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. VL-BERT: Pre-training of\ngeneric visual-linguistic representations. In ICLR, 2020.\n3\n[57] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. A corpus for reasoning\nabout natural language grounded in photographs. In\nACL, 2019. 3\n[58] Hao Tan and Mohit Bansal. LXMERT: Learning cross-\nmodality encoder representations from transformers. In\nEMNLP, 2019. 3\n[59] Bart Thomee, David A Shamma, Gerald Friedland,\nBenjamin Elizalde, Karl Ni, Douglas Poland, Damian\nBorth, and Li-Jia Li. YFCC100M: The New Data in\nMultimedia Research. Communications of the ACM,\n2016. 3\n[60] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin\nCui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species\nclassification and detection dataset. In CVPR, 2018. 3\n[61] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. Unifying architectures, tasks,\nand modalities through a simple sequence-to-sequence\nlearning framework. arXiv preprint arXiv:2202.03052,\n2022. 2, 8\n[62] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah,\nSteven Rennie, Kristen Grauman, and Rogerio Feris.\nThe fashion iq dataset: Retrieving images by combining\nside information and relative natural language feedback.\nCVPR, 2021. 3\n[63] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-\nmnist: a novel image dataset for benchmarking machine\nlearning algorithms, 2017. 2\n[64] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and\nXiaogang Wang. Learning from massive noisy labeled\ndata for image classification. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\n2015. 3\n[65] Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu,\nXiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda,\nKurt Keutzer, and Masayoshi Tomizuka. Image2point:\n3d point-cloud understanding with 2d image pretrained\nmodels. In Shai Avidan, Gabriel J. Brostow, Moustapha\nCiss\u00e9, Giovanni Maria Farinella, and Tal Hassner, edi-\ntors, ECCV, 2022. 16\n[66] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. From recognition to cognition: Visual common-\nsense reasoning. In CVPR, 2019. 3\n[67] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio\nTorralba, and Aude Oliva. Learning deep features for\nscene recognition using places database. In NeurIPS,\n2014. 3\n[68] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason J Corso, and Jianfeng Gao. Unified vision-\nlanguage pre-training for image captioning and VQA.\nAAAI, 2020. 3\n[69] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. Visual7W: Grounded Question Answering in Im-\nages. In CVPR, 2016. 3\n12\n Tops and T-Shirts\n Dresses\n Rings\n T-Shirts\n Sweatshirts and Hoodies\n Pants\n Boots\n Jeans\n Posters, Prints and Paintings\n Sweatshirts\n Shorts\n Sweaters\n Swimwear\n Fashion Sneakers\n Sandals and Flip Flops\n Necklaces\n Unisex Shoes\n Coats and Jackets\n Bras\n Earrings\n Bracelets\n Other Women's Clothing\n Heels\n Charms and Pendants\n Leggings\n Cases, Covers and Skins\n Music\n Engagement Rings\n Hats\n Athletic\n Skirts\n Casual Shirts\n Sunglasses\n Jumpsuits and Rompers\n Outfits and Sets\n Wedding and Anniversary Bands\n Wall Art\n Flats\n Bodysuits\n Tumblers\n Athletic Shoes\n Socks\n \n Panties\n Frames\n Shoulder Bags\n Casual\n Disc Golf Discs\n Wigs and Hair Pieces\n Area Rugs\n Totes\n Sports Bras\nClothing and Accessories\n Other Men's Clothing\n Other Children's Clothing and Accessories\n Bed Sheets and Pillowcases\n Hair Ties and Styling Accessories\n Boys' Shoes\n Men's Watches\n Belts\n Suits\n Sleepwear and Robes\n Newborn and Baby Shoes\n Girls' Shoes\n Outerwear\n Other Unisex Accessories\n Bedding Sets and Collections\n Decals, Stickers and Vinyl Art\n Underwear\n Books\n Crossbody Bags\n Sleepwear\n Fabric\n Women's Fragrances\n Slippers\n Vests\n Wallets\n Jerseys\n Handbags\n Other Women's Shoes\n(Other end leaves)\n0\n100000\n200000\n300000\n400000\n500000\n600000\nFigure Supp-1. The instance counts of the 80 most popular LGS end leaves.\nTable Supp-1. Comparing the n-gram statistics of LGS with that of COCO.\nNumber of n-grams with occurrence \u2265 10\nFive most frequent n-grams (n = 1, 2, 3)\nLGS\nCOCO\nLGS\nCOCO\nuni-grams\n364,802\n17,009\nand, the, a, to, with\na, of, on, the, i\nbi-grams\n4,054,418\n184,882\nwith a, in the, of the, is a, for a\non a, in a, a man, of a, with a\ntri-grams\n8,900,084\n462,653\ntrue to size, made to order, this is a,\na group of, group of people\nthis item is, machine wash cold\nin front of, next to a, on top of\nA.\nAdditional Analyses on LGS\u2019s Data Distribution\nA.1.\nLGS End Leaf Histogram\nThe instance counts of the LGS end leaves are displayed in Figure Supp-1. The top 80 most popular end leaves\nencompass 83.28% of the total instances, with the most popular Tops and T-shirts containing 16.23% of the\ntotal instances.\nA.2.\nn-gram and POS Analysis of LGS Captions\nTable Supp-1 presents the comparisons of the uni-grams, bi-grams, and tri-grams of LGS. This comparison\nindicates that LGS is more linguistically diverse. The uni-grams and bi-grams of the two datasets are similar.\nHowever, we notice greater conceptual diversity for LGS within its tri-grams. Specifically, COCO\u2019s five most\nfrequent tri-grams describe a group of objects and the relative position of the objects, whereas the LGS tri-grams\nencompass inherent properties of the commodities, including the size and the nature of each item.\nIn addition to the part-of-speech (POS) results presented in Section 3.3, we use Figures Supp-5 and Supp-6 to\npresent the most common words per POS for LGS and COCO, respectively.\n13\nFigure Supp-5. Top 20 most common words per POS for LGS.\nFigure Supp-6. Top 20 most common words per POS for COCO.\n14\n0\n2\n4\n6\n8\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\nResNet50 block 0 features\nLGS\nImageNet\n0\n2\n4\n6\n8\n10\n1\n2\n3\n4\n5\n6\nResNet50 block 1 features\nLGS\nImageNet\n0\n2\n4\n6\n8\n4\n5\n6\n7\n8\n9\nResNet50 block 2 features\nLGS\nImageNet\n4\n6\n8\n10\n4\n5\n6\n7\n8\n9\n10\nResNet50 block 3 features\nLGS\nImageNet\n4\n6\n8\n10\n1\n2\n3\n4\n5\n6\n7\nResNet50 block 4 features\nLGS\nImageNet\nFigure Supp-2. UMAP visualization of the ImageNet and LGS features extracted on a ResNet50 model trained on\nImageNet and LGS.\nB.\nAdditional Analyses on LGS-trained Classifiers\nB.1.\nHow Features learned on ImageNet and LGS Differ\nTo understand how vision models interpret the ImageNet and LGS instances, we use a ResNet50 model\nsequentially trained on ImageNet and LGS-117 as the feature extractor, and use UMAP [42] to visualize the\nhigh-dimensional ImageNet and LGS features in 2D figures. As shown in Figure Supp-2, the ImageNet features\nform a cluster, while the LGS features form a less concentrated cluster. The separation of the two clusters is\nespecially prominent at the first two layers.\nAs discussed in the main portion of the paper, many LGS product thumbnails consist of isolated foreground\nobjects and clear backgrounds, while ImageNet instances are mostly natural images where the foreground blends\ninto the background. Thus, we question whether the feature clustering is a consequence of this difference. To\nthis end, we learn a binary classification linear header that predicts between LGS and ImageNet images based\non the features extracted by the ResNet-50 model. We then visualize the saliency map of this binary model in\nFigure Supp-3. While the background is the most prominent difference between ImageNet and LGS to human\neyes, the saliency maps demonstrate that the deep models look for more sophisticated patterns, which can vary\nacross different images. Specifically, the foreground is emphasized in the first LGS example, while the background\nis more important in the second LGS instance. This observation aligns with the findings of [24], which states that\ndeep neural networks are not always understandable by humans.\nB.2.\nLGS Classification Models Look for Localized Patterns\nIn Figure Supp-4, we use GradCam [14, 52], a framework that visualizes gradient activations of the input\nimages, to demonstrate that the models trained on LGS look for much more localized patterns. Here, we draw\nexamples from the \u201csweatshirt\u201d synset in the LGS-Overlap dataset, and feed them into the three ResNet-50\nmodels learned on ImageNet, LGS-117, and LGS-710, respectively. The gradient activation of the ImageNet\n15\n0\n50\n100\n150\n200\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n(a) ImageNet examples.\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n50\n100\n150\n200\n0\n25\n50\n75\n100\n125\n150\n175\n200\n(b) LGS examples.\nFigure Supp-3. The saliency map of the LGS-ImageNet binary classifier.\nmodel spreads across the entire image, while the LGS models return more concentrated gradient maps. Note that\nthe gradient spikes produced by the LGS models mostly locate around the sleeves and the waist portion of the\nclothes. This makes sense because the LGS models are trained to differentiate various kinds of clothing-dominated\ne-commercial products. The portions highlighted by the LGS model gradient maps precisely correspond to the\nplaces where various types of clothes differ. For example, checking the sleeve length may be one of the easiest ways\nof distinguishing T-shirts from sweatshirts. Since the LGS-710 model was trained to classify more fine-grained\ntypes of products, it looks for even more localized patterns compared with the LGS-117 model.\nB.3.\nLinear Probing Details\nIn this section, we discuss the implementation details for the linear probing experiments in Section 4.1.3. In the\nexisting literature, when ResNets (designed for 224 \u00d7 224 inputs) are adopted for tasks that use smaller input\nsizes, the first 7 \u00d7 7 convolution layer is often replaced with a 3 \u00d7 3 layer. We adopt this replacement for CIFAR\nand Fashion MNIST. During linear probing, we thus allow this modified, randomly reinitialized first layer to be\noptimized along with the output layer.\nIn Section 4.1.3, we presented the improved linear probing results on CIFAR-100 and Fashion MNIST. We\nwould like to highlight that linear probing is a practical training method, because when the batch normalization\n(BN) layers are jointly optimized alongside the first and the last layer, this modified \u201clinear\u201d probing scheme can\nachieve a performance that is comparable to end-to-end training [65]. Specifically, with learnable BN, a ResNet-50\nmodel pre-trained on ImageNet\u2192LGS-710\u2192ImageNet achieves an accuracy of 71.41% on CIFAR-100, compared\nwith 69.47% for an ImageNet-only model.\n16\n       Input                     ImageNet RN50               LGS-117 RN50               LGS-710 RN50\nFigure Supp-4. GradCam visualizations show that LGS classification models look for much more localized patterns.\n17\nTable Supp-2. The FID scores across prompts using a subset (n = 5000) of the LGS and DeepFashion InShop datasets.\nPrompt ID\nModel\nFID (\u2193)\nLGS\nDeepFashion\n1\nVanilla\n40.4437\n61.8519\nVanilla + LGS-117\n42.7328\n74.4327\n2\nVanilla\n42.1081\n63.2344\nVanilla + LGS-117\n42.0529\n77.7190\n3\nVanilla\n36.7157\n58.2189\nVanilla + LGS-117\n36.1946\n79.3607\n4\nVanilla\n38.4101\n62.9269\nVanilla + LGS-117\n38.4100\n74.0185\nTable Supp-3. Prompts evaluated for text-to-image generation experiment. Prompt structures varied slightly due to\navailable metadata across datasets.\nPrompt ID\nDataset\nPrompt Structure\n1\nLGS\n{brand} {title} in the style of e-commerce\nDeepFashion\n{first sentence of description} in the style of e-commerce\n2\nLGS\n{end_leaf} advertisement for a {title} from {brand}\nDeepFashion\n{end_leaf} advertisement for a {first sentence of description}\n3\nLGS\n{brand} {end_leaf} {title} {description}\nDeepFashion\n{end_leaf} {description} {gender_category} {color}\n4\nLGS\na photo of {brand} {end_leaf} {title}, e-commerce\nDeepFashion\na photo of {end_leaf} {color} {first sentence of description}, e-commerce\nC.\nAdditional Text-to-Image Generation Discussions\nC.1.\nDetermining the Prompts for Text-to-Image Generation\nEnsuring the quality of the input prompts is paramount for text-to-image models to generate realistic images.\nOur goal is to choose a prompt which generates images faithful to the metadata, performs relatively well in terms\nof Frechet Inception Distance (FID) score, and generalizes across datasets.\nTo that end, we randomly selected 5,000 examples each from the LGS and DeepFashion InShop datasets. It is\nimportant to note that, for prompt engineering, the ground-truth images used for FID calculation are upscaled\nfrom 256 \u00d7 256, and the denoising diffusion implicit model steps (ddim_steps) were lowered to 50 for inference.\nThis resulted in lower scores than the experiment results (Table 9). However, the numbers are still indicative of\nrelative performance.\nQuantitatively, Prompts 3 and 4 perform significantly better on LGS, perform comparably on DeepFashion,\nand generalize well (Table Supp-2). Prompt 3 achieves better FID scores using the Vanilla model and performs\nslightly better on LGS. Qualitatively, however, Prompt 4 generations are consistently better and more faithful\nto the metadata (Figure Supp-7). Therefore, we select Prompt 4 for our experiments. This also reaffirms that\nthese objective metrics are not strong indicators of the subjective aesthetic quality in this particular case, and\nshould only be used as a loose relative measure. Figures Supp-9 and Supp-8 show additional examples from the\ntwo datasets generated with Prompt 4.\n18\nend_leaf: Jackets Vests gender_category: Men color:\nKhaki first sentence of description:Made in a cotton-\nnylon blend with a modified collar and partial mesh lining,\nthis baseball jacket is the slickest iteration of the style\nyet\nend_leaf: Pants gender_category: Men color: Black-\ngrey first sentence of description: This jogger\u2019s easy,\nslouchy silhouette gets a little grit courtesy of its eye-\npopping print of photorealistic roses\nend_leaf: Shirts Polos gender_category: Men color:\nCoral first sentence of description: Constructed from\ncotton for a classic fit, this lightweight shirt features\nbuttoned chest pockets\nend_leaf: Shorts gender_category: Men color: Grey\nfirst sentence of description: Crafted from speckled\nFrench terry, this sharper-than -average pair of sweat-\nshorts is outfitted with a mock fly and three shiny zip\npockets (two in front, one in back), ideal for lounging\naround or winning triathalons (just kidding)\nend_leaf: Blouses Shirts gender_category: Women\ncolor: Rust first sentence of description: Effortlessly\nethereal and romantic, this cutout-shoulder top is what\ndream closets are made of\n(a) Metadata\n(b) Prompt 3\n(c) Prompt 4\nFigure Supp-7. Generated images with Vanilla SD model to determine prompt.\n19\na\nphoto\nof\nBlouses\nShirts\nTomato Love 21 - A woven cami\nfeaturing a pleated front and\ncrossback strap detail in the\nback, e-commerce\na photo of Jackets Vests Khaki\nMade in a cotton-nylon blend\nwith a modified collar and par-\ntial mesh lining, this baseball\njacket is the slickest iteration of\nthe style yet, e-commerce\na photo of Shirts Polos Coral\nConstructed from cotton for a\nclassic fit, this lightweight shirt\nfeatures buttoned chest pockets,\ne-commerce\na photo of Shorts Grey Crafted\nfrom speckled French terry, this\nsharper-than-average\npair\nof\nsweatshorts is outfitted with a\nmock fly and three shiny zip\npockets (two in front, one in\nback), ideal for lounging around\nor winning triathalons (just kid-\nding), e-commerce\n(a) Input Prompt\n(b) Vanilla SD\n(c) LGS-117\nFigure Supp-8. Additional qualitative examples of the Vanilla SD vs LGS-117 fine-tuned SD model on DeepFashion\nInShop dataset.\n20\na photo of ana silver co.\near-\nrings rainbow moonstone ear-\nrings 3/4\" (925 sterling silver)\nearr415021\na photo of vans vault vans vault\nold skool lx - croc skin/flame\na photo of myconquering con-\nquering unisex black joggers\na photo of chopard cat eye uni-\nsex sunglasses\na photo of invicta bracelets ele-\nments men\u2019s bracelet\na photo of wristwatchstraps.co\nsmart watch accessories bumper\ncover+glass for apple watch -\nlilac 21 - 38mm\n(a) Input Prompt\n(b) Vanilla SD\n(c) LGS-117\nFigure Supp-9. Additional qualitative examples of the Vanilla SD vs LGS-117 fine-tuned SD model on the LGS dataset.\n21\n"
  },
  {
    "title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers",
    "link": "https://arxiv.org/pdf/2401.04695.pdf",
    "upvote": "8",
    "text": "Narrowing the Knowledge Evaluation Gap:\nOpen-Domain Question Answering with Multi-Granularity Answers\nGal Yona\nGoogle Research\ngalyona@google.com\nRoee Aharoni\nGoogle Research\nroeeaharoni@google.com\nMor Geva\nTel Aviv University,\nGoogle Research\npipek@google.com\nAbstract\nFactual questions can typically be answered\ncorrectly at different levels of granularity. For\nexample, both \u201cAugust 4, 1961\u201d and \u201c1961\u201d\nare correct answers to the question \u201cWhen was\nBarack Obama born?\u201d. Standard question an-\nswering (QA) evaluation protocols, however,\ndo not take this into account explicitly and in-\nstead compare a predicted answer against ref-\nerence answers of a single granularity level. In\nthis work, we propose GRANOLA QA, a novel\nevaluation setting where a predicted answer is\nevaluated in terms of accuracy and informa-\ntiveness against a set of multi-granularity an-\nswers. We present a simple methodology for en-\nriching existing datasets with multi-granularity\nanswers, and create GRANOLA-EQ, a multi-\ngranularity version of the ENTITYQUESTIONS\ndataset. We evaluate models using a range of\ndecoding methods on GRANOLA-EQ, includ-\ning a new algorithm called Decoding with Re-\nsponse Aggregation (DRAG), that is geared to-\nwards aligning the answer granularity with the\nmodel\u2019s uncertainty. Our experiments show\nthat large language models with standard de-\ncoding methods tend to generate specific an-\nswers, which are often incorrect. In contrast,\nwhen evaluated on multi-granularity answers,\nDRAG yields a nearly 20 point increase in ac-\ncuracy on average, which further increases for\nrare entities, revealing that standard evaluation\nand decoding schemes may underestimate the\nknowledge encapsulated in language models.1\n1\nIntroduction\nLarge language models (LLMs) often generate fac-\ntual errors, especially when the task requires less\nwidely-known knowledge (Mallen et al., 2023; Sci-\navolino et al., 2021). Such factual errors are com-\nmonly attributed to the LM lacking relevant knowl-\nedge (Zheng et al., 2023) or over-committing to\nearlier mistakes (Zhang et al., 2023b).\n1Data will be released soon at https://github.com/\ngoogle-research-datasets/granola-eq\nStandard QA    { Clapham }\nGRANOLA QA [ Clapham, London, England ]\nQuestion:   Where was Leslie Ash born?\nGreedy decoding\nEdmonton\nSampling \nManchester \nEdmonton\nLondon\nDRAG\nEngland\nGold Answers:\nGenerated Answers:\nmultiple \ngranularity \nlevels\nKnowledge evaluation gap\nFigure 1: Top: GRANOLA QA evaluation with multi-\ngranularity answers. Middle: Decoding with Response\nAggregation (DRAG) outputs a (potentially coarser) re-\nsponse by aggregating several responses of the model.\nBottom: Accuracy gain from evaluating using multi-\ngranularity answers for several decoding strategies.\nDRAG reveals a significant knowledge evaluation gap.\nWe conjecture that factual mistakes can stem\nfrom a different failure source, when the model\nprioritizes different textual attributes (e.g., fluency\nor specific formats that appeared in the training\ncorpora) over factuality. Such failures can result in\ngenerated text that mixes both correct and incorrect\nstatements, even when the incorrect parts are not\nstrictly required by the question.\nConsider for example the question \u201cWhen was\nMark Bils born?\u201d. When prompting ChatGPT2 for\nanswering this question, sampled responses include\n\u201cMarch 22, 1958\u201d, \u201cMay 19, 1958\u201d and \u201cAugust\n15, 1958\u201d. This may suggest that the model is\nconfident that Bils was born in 1958 \u2013 which is\na correct answer in this case, albeit not the most\n2Responses were obtained by querying ChatGPT 3.5 using\nthe standard Web API in December 2023.\narXiv:2401.04695v1  [cs.CL]  9 Jan 2024\ninformative one \u2013 yet it displays a preference for\noutputting a more detailed but incorrect response\nin a specific full-date format.\nThis example also highlights how factual ques-\ntions can be answered correctly at different levels\nof granularity. Namely, while the answers \u201cDecem-\nber 1, 1958\u201d, \u201cDecember 1958\u201d, and \u201c1958\u201d vary\nin terms of informativeness, they are all factually\ncorrect. However, answer granularity levels are not\nconsidered in standard question answering (QA)\nsettings, which typically evaluate a predicted an-\nswer based on its similarity to a set of reference\nanswers of the same (usually the most-specific)\ngranularity. Even when different levels of granular-\nity are present, there is no notion in which matching\nto a more specific answer is \u201cbetter\u201d. As a result,\nstandard QA evaluation may significantly under-\nestimate the knowledge encapsulated in LMs, a\nphenomenon which we refer to as the knowledge\nevaluation gap. Indeed, recent human evaluation\nsuggests that such granularity disparities account\nfor approximately 10-15% of the disagreements\nbetween lexical matching and human evaluation\n(Kamalloo et al., 2023; Zheng et al., 2023).\nIn this work, we tackle this problem by propos-\ning a novel multi-granularity QA evaluation setting,\ncalled GRANOLA QA (short for GRANularity\nOf LAbels).\nUnlike existing evaluation, in\nGRANOLA QA questions are labeled with ground-\ntruth answers with multiple levels of granular-\nity and predicted answers are evaluated in terms\nof both their accuracy and informativeness (\u00a72).\nThe evaluation is done using two new metrics:\nGRANOLA Accuracy, which checks if there\nwas a match against any of the answers, and\nGRANOLA informativeness, which is a weighted\nscore prioritizing fine-grained correct answers over\ntheir coarse-grained counterparts.\nNext, we present a simple and general methodol-\nogy for augmenting an existing single-granularity\nQA dataset to the setting of GRANOLA QA, which\ndoes not involve any human labor (\u00a73). This pro-\ncess is based on obtaining additional information\nabout entities present in the original questions and\nanswer(s) from an external knowledge graph (KG),\nand then using an LLM to form multi-granularity\nanswers conditioned on this information. We ap-\nply our methodology on the ENTITYQUESTIONS\n(EQ) dataset (Sciavolino et al., 2021), using Wiki-\nData (Vrande\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014) as the KG.\nThe resulting dataset, GRANOLA-EQ, consists of\n12K QA examples with an average of 2.9 multi-\ngranularity answers per question. A manual analy-\nsis of a random subset of the data shows that our au-\ntomatic procedure yields highly-accurate answers.\nWe evaluate various baselines on GRANOLA-\nEQ, including greedy decoding and methods that\nabstain from answering in cases of uncertainty\n(Yoshikawa and Okazaki, 2023a; Yang et al.,\n2023a,b; Ren et al., 2023). In addition, we intro-\nduce a novel decoding strategy, called Decoding\nwith Response Aggregation (DRAG), that is geared\ntowards aligning the granularity level of a model\u2019s\nresponse with its uncertainty level (\u00a74). DRAG uses\ntemperature sampling to obtain a set of candidate\nresponses, and then answers the original question\nbased on an aggregation of these responses, which\nwe implement using few-shot prompting. Figure 1\ndepicts an example of DRAG\u2019s aggregation of several\nincorrect responses into a correct coarser answer\nthat matches against the multi-granularity labels.\nOur experiments (\u00a75) show that: (1) with stan-\ndard decoding the gap between GRANOLA accu-\nracy and standard accuracy is small, which corrob-\norates that LMs tend to output detailed responses,\neven when these are incorrect, (2) with DRAG this\ngap is high, showing that unlike standard decoding,\nDRAG outputs coarse answers, (3) GRANOLA accu-\nracy remains high with DRAG even for rare entities,\nsuggesting that LLMs know less detailed infor-\nmation about them rather than lacking any knowl-\nedge (Mallen et al., 2023), (4) compared to stan-\ndard decoding and methods that allow the model\nto abstain from answering (\u201cIDK\u201d), DRAG yields\na better trade-off between factuality and response\ninformativeness, and (5) this evaluation gap is not\nobserved when using semantic similarity scores\nagainst single-granularity reference answers.\nTo summarize, we introduce GRANOLA, a new\nQA evaluation setting that considers both the ac-\ncuracy and informativeness of predicted answers.\nWe propose a simple automatic procedure for gen-\nerating accurate multi-granular answers for given\nQA pairs, and apply it to the ENTITYQUESTIONS\ndataset to create GRANOLA-EQ. We introduce a\nnew decoding scheme, called DRAG, tailored to mod-\nify the response to a level of granularity that fits the\nmodel\u2019s uncertainty levels. We show that DRAG im-\nproves both informativeness and accuracy (relative\nto standard decoding), and that standard evaluation\nmay significantly under-estimate the knowledge of\nLMs, especially about rare entities.\n2\nGRANOLA Question Answering\nWe formalize the setting of GRANOLA QA and\ndefine new metrics for quantifying accuracy and\ninformativeness of QA predictions.\n2.1\nProblem Setting\nIn a typical open-domain QA setting (Yang et al.,\n2015; Voorhees et al., 1999; Kwiatkowski et al.,\n2019; Joshi et al., 2017; Sciavolino et al., 2021),\na model predicts an answer p to a given ques-\ntion q, which is evaluated against an unordered\nset of gold answers A = {a1, . . . , ak}.\nThe\nevaluation usually relies on lexical matching with\nstandard metrics like exact-match or token-F1 be-\ntween the predicted answer and each of the gold\nanswers.3\nFor example, a possible set of an-\nswers to the question \u201cWhere is the headquarter\nof Guildhall School of Music and Drama?\u201d would\nbe {Barbican Centre, The Barbican}. Importantly,\nthe gold answers in A are interchangeable, where\nmatching against either of a1 or a2 is equally good.\nHowever, we observe that a question may be an-\nswered correctly at different levels of granularity.\nNamely, \u201cLondon\u201d is also a correct answer to the\nquestion, since the Barbican Centre is located there.\nIf \u201cLondon\u201d does not appear in A, standard evalua-\ntion will render this answer as incorrect, resulting\nin under-estimating the LM\u2019s knowledge. More-\nover, if London is included in A, then answering\neither \u201cLondon\u201d or \u201cThe Barbican\u201d is considered\nequally correct, despite the fact that the second an-\nswer is more specific and arguably more valuable.\nHere we propose that QA predictions should\nbe evaluated while considering different granular-\nity levels, a setting which we name GRANOLA\nQA. Formally, the answer p should be evaluated\nagainst an ordered set of multi-granular gold an-\nswers\n\u02c6\nA = {A1, . . . , A\u2113}.\nHere, A1 is the\nset of the most informative correct answers (e.g.\n{Barbican Centre, The Barbican}) and A\u2113 is the\nset of least-informative correct answers (e.g. \u201cLon-\ndon\u201d could be in A2 and \u201cUK\u201d in A3).\n2.2\nEvaluation\nAt a high-level, we will evaluate GRANOLA QA\nperformance across two axes: accuracy and in-\nformativeness. Accuracy is determined based on\nwhether the candidate answer matches against any\nof the GRANOLA answers; informativeness will\n3The answers are typically being normalized (i.e. case-\nfolding and removing punctuation and articles).\nreward matching against fine-grained answers by\nusing an appropriate weighting scheme:\nDefinition 1 (GRANOLA Evaluation) Given a\nquestion q, an answer p and GRANOLA labels \u02c6\nA,\naccuracy and informativeness are evaluated based\non a simple two-step procedure:\nStep 1: Find a match. Let i\u22c6 \u2261 i\u22c6(p; q, \u02c6\nA)\ndenote the smallest index i \u2208 [k] for which there\nis a match between p and Ai \u2208 \u02c6\nA (meaning the\nF1 score between p and an answer in Ai exceeds\nsome threshold \u03c4), or \u22a5 if no match is found.\nStep 2: Evaluate. GRANOLA accuracy is de-\nfined as 1[i\u22c6 \u0338=\u22a5]. Informativeness is defined as\nexp(\u2212\u03bb \u00b7 (i\u22c6 \u2212 1)), or 0 if no match was found.\nThe notion of informativeness relies on a weight-\ning scheme that assigns a weight of 1.0 to the fine-\ngrained answers A1, and exponentially decreasing\nweight for answers Ai>1. This represents the di-\nminished utility of coarser answers. The parameter\n\u03bb can be used to control the rate of decrease: as\n\u03bb \u2192 0 coarser answers receive higher weights; see\nAppendix A for a visualization of how the weights\nbehave as a function of \u03bb.\n3\nEnriching QA Samples with\nMulti-Granularity Answers\nWe\nturn\nto\nthe\nquestion\nof\nconstructing\nGRANOLA QA benchmarks.\nWe observe\nthat multi-granularity answers are in principle\nabstractions of the most-detailed answer.\nFor\nexample (see Figure 2), the answer \u201cMichael\nMadhusudan\nDutta\u201d\nto\nthe\nquestion\n\u201cWho\ntranslated the play Neel Darpan into English?\u201d\ncan be abstracted into a higher-level description\nsuch as \u201cAn Indian Poet\u201d. Therefore, one way\nto generate multi-granularity answers is to start\nfrom an existing QA pair and enriching it with\nmulti-granularity answers through abstraction.\nFollowing this approach, we describe a simple\nand automatic procedure for adjusting factual QA\ndatasets to GRANOLA QA (\u00a73.1). Then, we apply\nthis procedure to the ENTITYQUESTIONS dataset\n(\u00a73.2), a widely used entity-centric QA dataset (Sci-\navolino et al., 2021), to create a multi-granularity\nQA benchmark. Last, we manually analyze the\nquality of the generated data (\u00a73.3).\n3.1\nAutomatic Answer Generation\nWe focus on evaluating factual knowledge in LLMs,\nwhere the answer to a given question is an en-\ntity (e.g., a person or a place).\nGiven an an-\nQuestion: Where was Alexander \nOstuzhev born? \nAnswer(s): Voronezh\nAlexander Ostuzhev: Russian actor \n(1874-1953)\nVoronezh: capital city of Voronezh \nOblast in central Russia\nGRANOLA 1: Voronezh\nGRANOLA 2: Voronezh Oblast\nGRANOLA 3: Central Russia\nGRANOLA 4: Russia\nOriginal \nexample\nExtracting \nentity \ndescriptions \nfrom \nWikiData \nGenerating \nGRANOLA \nanswers \nusing LLM\nFigure 2: Our procedure for adding multi-granularity\nanswers to given QA pairs.\nswer, we propose to generate coarser versions\nof it by utilizing an external knowledge graph\n(KG). Specifically, given a KG with facts encoded\nas subject-relation-object triplets (e.g., the triplet\n(Paris, capital of, France) would encode the\nfact that Paris is the capital of France) and an an-\nswer entity e, coarser versions of e can be obtained\nby replacing it with higher-level properties of it\nin the KG. For example (Figure 3), replacing the\nanswer \u201cMichael Madhusudan Dutta\u201d with its prop-\nerties of Nationality and Occupation would cre-\nate a new coarser answer \u201cIndian Poet\u201d.\nIn principle, however, there are many possible\nanswer properties that can be used \u2013 and intuitively,\nnot all of them are key properties of the entity that\nare useful for evaluating general factual knowledge.\nFor example, answering the original question with\nMichael Madhusudan Dutta\u2019s shoe size is not what\nwe want to capture by coarse answers. Thus, to\ncreate a generic methodology for enriching an ex-\nisting QA dataset with answers, we must be able to\nautomatically determine the relevant properties.\nTo overcome this challenge, instead of relying\non KG triplets directly, we use short textual descrip-\ntions that capture the key properties of the entity\nin the KG. Such descriptions are often offered by\nknowledge sources such as WikiData. For example,\nthe entity Michael Madhusudan Dutta has the fol-\nlowing description: \u201cBengali poet and dramatist\u201d.\nOverall, our answer generation process has two\nsteps, depicted in Figure 2. Given a QA pair, we\nfirst obtain a description of the answer entity and\nany entities appearing in the question from an ex-\nternal KG. Then, we zero-shot prompt an LLM\nto generate an ordered list of answers at varying\nlevels of granularity, conditioned on the given QA\npair and the entity descriptions. See Table 8 for the\nexact instruction prompt.\nQuestion: \nWho translated the play Neel Darpan into English?\nMichael \nMadhusudan \nDutta\nIndian\nPoet\n44\nWriter\nnationality\nshoe size\noccupation\nsubclass of\nMulti-granularity answers as abstractions:\nMichael \nMadhusudan Dutta\nan Indian poet\nan Indian writer\nFigure 3: An illustration of multi-granularity answers\nas entity abstractions. Given an answer entity, we use\nan external KG to generate coarser answers from its\nproperties (turquoise) in addition to the original answer\n(purple). Notably, not all KG properties are equally\ngood candidates for multi-granular answers (red).\n3.2\nGRANOLA ENTITYQUESTIONS\nWe apply the procedure described in \u00a73.1 to enrich\nthe test split of ENTITYQUESTIONS (EQ) (Sci-\navolino et al., 2021) with GRANOLA answers.\nENTITYQUESTIONS is an entity-rich QA dataset,\ncreated by converting factual subject-relation-\nobject triples into natural language questions using\nmanually-defined templates. We use PaLM 2-L as\nthe LLM (Anil et al., 2023).\nThe resulting dataset, which we refer to as\nGRANOLA-EQ, spans 16 relations and has a total\nof 12,452 examples. Overall, our procedure yielded\n2-3 coarser answers per questions (\u223c20% have 2\nanswers overall, \u223c60% have 3, and \u223c15% have\n4 or more; this is distributed relatively uniformly\nover relations). Examples from GRANOLA-EQ are\nshown in Table 1. More details are in Appendix B.\n3.3\nData Quality\nWe manually evaluate the quality of a gener-\nated answer a with respect to a question q from\nGRANOLA-EQ across the following axes:\n\u2022 Correctness: We use WikiData to verify whether\na is a factually correct answer to q. Notably,\nwhile a was generated conditioned on the descrip-\ntion, the LLM might produced it while relying on\nits parametric knowledge rather the information\nin the description. For example, for the question\n\u201cWhere did Marcel Gaumont die?\u201d, the model\ngenerated the answers \u201cParis\u201d, \u201c\u00cele-de-France\u201d,\nand \u201cFrance\u201d while the WikiData description of\nParis is \u201cCapital of France\u201d. Therefore, in this\ncase the LLM used its parametric knowledge to\nadd a new granularity level (\u00cele-de-France).\nQuestion\nGRANOLA Answers\n\u201cWhere was Fiona Lewis\nborn?\u201d\nWestcliff-on-Sea;\nEssex;\nEngland\n\u201cWhat\nmusic\nlabel\nis\nCourage represented by?\u201d\nRock Records; a Taiwanese\nrecord label\n\u201cWho\nis\nAugust\nvon\nHayek\u2019s child?\u201d\nFriedrich\nHayek;\nan\neconomist\n\u201cWho is the author of The\nAdding Machine?\u201d\nElmer Rice; an American\nplaywright; a playwright\n\u201cWhere was Toby Shap-\nshak educated?\u201d\nRhodes\nUniversity;\nMakhanda, South Africa;\nSouth Africa\nTable 1: Examples from GRANOLA-EQ. Answers are\nseparated by a semicolon and listed fine-to-coarse. The\nfirst answer is the original answer in ENTITYQUES-\nTIONS; subsequent answers were generated (see \u00a73.1).\n\u2022 Informativeness: We verify that a is a non-\ntrivial answer to q. We consider an answer as\ntrivial if it could be generated based on the ques-\ntion template alone (i.e., a version of q in which\nthe entity is redacted). For example, \u201cEarth\u201d is a\ntrivial answer to the question \u201cWhere was Fiona\nLewis born?\u201d because it could be obtained based\non the template Where was [X] born?.\n\u2022 Granularity: We assess whether a is coarser\nthan the answers preceding it.\nFor the first\nGRANOLA answer, we define this as whether\nthe answer is identical to the original answer.\nWe treat these metrics as binary and manually\nevaluate a sample of 1% of the data (124 ques-\ntions and their corresponding 358 answers). Ta-\nble 2 reports the fraction of examples in each er-\nror category with a representative example. Our\nevaluation reveals that the enriched answers are\nof high-quality, with over 99% of the generated\nanswers being factually correct (only a single ex-\nample was found to be incorrect). Nonetheless,\nthere is headroom for improving our answer gen-\neration procedure. For example, we observe that\nthere are examples with useful information in the\ndescription that is not utilized by the model, which\nsuggests that the knowledge evaluation gap may be\neven larger than observed in our results in \u00a75.\n4\nDecoding with Response Aggregation\nHumans naturally tailor the granularity level of\ntheir responses to their uncertainty levels. Consider\nasking a person A, when another person B was\nborn. The format of the response will depend on\nthe relationship between A and B, and specifically\non how much A knows about B. For example, if\nError\ntype\n(%)\nExample\nCorrectness\n(<1%)\nQuestion: Who is Chaim Weizmann mar-\nried to? Answers: Vera Weizmann; Vera\nChatzman; Vera Weizmann Chatzman\nInformativeness\n(6%)\nQuestion: What music label is Sarah Bux-\nton represented by?\nAnswers: Lyric\nStreet Records; a music label\nGranularity\n(9%)\nQuestion: Who owns Eccles Coliseum?\nAnswers: Southern Utah University; a\npublic university; a public university in\nUtah\nTable 2: Human evaluation results of GRANOLA-EQ,\nshowing for each error type the fraction of erroneous\ncases and an example.\nA is extremely familiar with B (e.g., B is A\u2019s son),\nthen we expect the answer to include the full date\nof birth. If A is only partially familiar with B (e.g.,\nB is a celebrity that A knows), then we expect the\nanswer to be more generic (e.g. only the year or\ndecade). If A is not familiar with B, then we expect\nA to say that they do not know the answer.\nIn this section, we propose a novel decoding\nstrategy, called Decoding with Response Aggre-\ngation (DRAG), that is intended to encourage LMs\nto do the same. We focus on a fixed (i.e., frozen)\nLM, and our objective is to improve factuality at\ninference time by attempting to provide a coarser\nanswer in the place of a fine-grained but incorrect\nanswer. In \u00a75, we will evaluate our proposed de-\ncoding strategy against various existing baselines\non the GRANOLA QA dataset we constructed.\nDRAG consists of two stages:\n\u2022 Sampling: We sample N responses from the\nmodel with temperature T > 0.\n\u2022 Aggregation: The final output is the most infor-\nmative response that is consistent with the set of\nsampled responses. This can be implemented in\ndifferent ways, e.g. via prompting an LLM.\nRevisiting the example question \u201cWhen was\nMark Bils born?\u201d (\u00a71), aggregating the sampled\nresponses \u201cMarch 22, 1958\u201d, \u201cMay 19, 1958\u201d and\n\u201cAugust 15, 1958\u201d, should yield \u201c1958\u201d. Pseudo-\ncode for DRAG is provided in Figure 4.\nChoice of hyperparameters\nThe sampling tem-\nperature T and number of responses N can be used\nto control the trade-off between factuality and infor-\nmativeness. Intuitively, larger values of T and N\nencourage more diverse outputs, and hence, more\naggressive aggregation that encourages factuality\nover informativeness.\nHyperparameters: Temperature T > 0; number of\nsamples N\nInput: Input x; Model M\nGenerate {r1, . . . , rN} continuations for M(x) at\ntemperature T;\nLet \u02c6r = ResponseAgg ({r1, . . . , rN});\nreturn The aggregated response \u02c6r\nFigure 4: Decoding with Response Aggregation (DRAG).\nWe implement ResponseAgg by instructing an LLM to\noutput what r1, . . . , rN have in common, or IDK if they\ndo not share meaningful properties.\nDRAG vs existing decoding strategies\nWhen\nN = 1, the aggregation is trivial and DRAG recovers\nstandard decoding strategies (e.g. greedy decod-\ning or temperature sampling, based on the value of\nT). Conceptually, DRAG is also a generalization of\nother popular decoding strategies that are based on\nsampling a set of candidate responses. For exam-\nple, replacing our proposed aggregator with a naive\naggregation that outputs the majority response re-\ncovers self-consistency (Wang et al., 2022).\n5\nExperiments\nWe assess how accounting for answer granular-\nity, both in evaluation and during decoding, influ-\nences the evaluation of LLM performance on fac-\ntual questions. After describing our experimental\nsetting (\u00a75.1), we compare between evaluation with\nstandard accuracy and GRANOLA accuracy (\u00a75.2),\nwhich reveals that current QA settings underesti-\nmate LLMs\u2019 knowledge. Then, we show that the\ngains in accuracy from using GRANOLA cannot\nbe matched by existing semantic similarity scores\n(\u00a75.3), which highlights the utility of this setting\nin capturing differences between multi-granularity\nanswers. Last, we use the GRANOLA metrics to\nevaluate DRAG with respect to baselines in terms\nof accuracy and informativeness (\u00a75.3), showing\nits superiority in decoding answers that are tuned\ntowards the LLM\u2019s knowledge.\n5.1\nExperimental Setting\nWe evaluate DRAG and multiple baselines on\nGRANOLA-EQ in a closed-book setting, where fac-\ntual questions must be answered without access to\nan external knowledge source (Petroni et al., 2019).\nFor the aggregation stage of DRAG, we instruct\nan aggregator LLM to output what the sampled\nresponses have in common or IDK if the responses\nhave nothing meaningful in common (see Table 8\nin Appendix C for the exact prompt).\nBaselines\nWe consider the following methods:\n\u2022 Standard Decoding: We evaluated both greedy\ndecoding (Greedy) and temperature sampling\n(TS), but since TS consistently under-performed\nGreedy we report results only for Greedy.\n\u2022 I don\u2019t know (IDK): Given the established suc-\ncess of steering model behaviour via prompt-\ning (Mishra et al., 2021; Si et al., 2022; Gan-\nguli et al., 2023), we consider two prompt-based\nIDK variants. In IDK, the model is instructed\nto either answer the question or output IDK. In\nIDKIfUncertain, the model is specifically in-\nstructed to output IDK if its uncertainty is high.\n\u2022 Aggregation-based baselines:\nWe evaluate\nDRAG and IDKWithAgg, in which we instruct the\nmodel to answer at a level of granularity that\nmatches its uncertainty. As an ablation for the im-\nportance of the aggregation step in DRAG we also\nevaluate SelfConsistency (Wang et al., 2022),\nwhere we sample N responses at temperature T\nand output the majority response.4 As noted in\n\u00a74, SelfConsistency can be cast as an instance\nof DRAG with a simple aggregator (majority rule).\nSee Table 7 for the prompts used for the baselines.\nEvaluation\nWe use GRANOLA accuracy and in-\nformativeness as described in Definition 1. To ac-\ncount for cases of IDK predictions, we adopt the\nperspective of selective prediction (El-Yaniv et al.,\n2010; Geifman and El-Yaniv, 2017) with recent\napplications in QA (Kamath et al., 2020) and text\ngeneration (Yoshikawa and Okazaki, 2023a). In-\nformativeness is left as is, except that IDK pre-\ndictions are defined to contribute a score of 0.0,\nsince they are not informative at all. GRANOLA\nAccuracy is replaced with selective GRANOLA ac-\ncuracy, which is the mean GRANOLA accuracy\non the subset of predictions which are not IDK.\nModels\nWe use instruction-tuned versions of\nPaLM 2-M and PaLM 2-L, the medium and large\nvariants of the PaLM 2 LLM (Anil et al., 2023).\n5.2\nKnowledge Evaluation Gap\nFigure 5 shows GRANOLA accuracy as a function\nof standard accuracy, for the different models and\nmethods. Note that the vertical distance from the\nx = y line (black) represents the gain in accuracy\nfrom evaluating using multi-granularity answers.\nWe observe that this gap is similar and relatively\n4After case-folding and removing punctuation and articles.\n0.4\n0.5\n0.6\n0.7\n0.8\nStandard Accuracy\n0.4\n0.5\n0.6\n0.7\n0.8\nGRANOLA Accuracy\ngreedy\n+idk\n+u-idk\n+agg\ndrag\nsc\nPaLM 2-M\nPaLM 2-L\nFigure 5: Standard accuracy vs. GRANOLA accuracy\nfor the different models we evaluate.\nPopualrity\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nAccuracy vs Entity Popularity\nStandard Accuracy\nGRANOLA accuracy\nFigure 6: Accuracy vs. entity popularity for PaLM 2-L\nusing DRAG. Unlike standard accuracy, which declines\nsteeply in popularity, GRANOLA accuracy plateaus.\nsmall of \u223c5 points (grey dotted line) for methods\nthat do not explicitly incorporate aggregation. This\nconfirms our initial conjecture that standard de-\ncoding tends to generate detailed but incorrect re-\nsponses. In addition, for the aggregation methods,\nthis gap is substantially larger, nearing a \u223c20 point\nincrease (red dotted line). This demonstrates that\nboth explicit aggregation (DRAG) and implicit ag-\ngregation obtained via prompting can successfully\nsteer the model towards tailoring its response granu-\nlarity. It also reveals that the knowledge evaluation\ngap is both a function of existing evaluation prac-\ntices and standard decoding strategies. In Figure\n10 in Appendix D we show a breakdown of these\nresults to the different relations in GRANOLA-EQ,\nrevealing that certain relations especially gain from\nmulti-granularity answers.\nNext, we consider how this gap behaves as a\nfunction of the popularity of the question entity.5\nIn Figure 6 we stratify GRANOLA-EQ into equally\nsized bins by entity popularity (x-axis) and com-\npare standard accuracy (blue) with GRANOLA ac-\ncuracy (orange, dashed). While standard accuracy\nsteeply declines with popularity, GRANOLA accu-\nracy plateaus. This reveals that models do capture\nknowledge about even very rare entities (but this\nknowledge is coarser). In Figure 11 (\u00a7A) we show\n5We quantify popularity using Wikipedia page-views.\n0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70\nGRANOLA Informativeness\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nGRANOLA Accuracy\ngreedy\n+idk\n+u-idk\n+agg\ndrag\nsc\nPaLM 2-M\nPaLM 2-L\nFigure 7: Answer accuracy vs. informativeness when us-\ning DRAG compared to the baselines. Behaviour is consis-\ntent across model sizes (purple/orange): IDK baselines\nimprove accuracy at the cost of making less informative\npredictions (grey arrow); DRAG improves both accuracy\nand informativeness (red arrow).\nthat this behaviour is demonstrated by DRAG but not\nby standard decoding.\n5.3\nEvaluation of DRAG\nFigure 7 shows the GRANOLA accuracy and in-\nformativeness of DRAG compared to the baselines.\nThe results are consistent across model sizes (pur-\nple vs orange). Figure 8 provides a more detailed\npicture of the distribution of which GRANOLA an-\nswer matched against the predicted answers (see\nDefinition 1). We distill several key takeaways:\n(1) IDK baselines improve accuracy at the cost\nof less informative predictions (grey arrows in Fig-\nure 7): As expected, abstention (IDK) improves\nthe selective accuracy. However, as evident in Fig-\nure 7, this comes at the cost of predictions that are\noverall less informative. For example, the fraction\nof errors made by IDK drops from 42% to 31% \u2013\nbut 17% of the predictions are IDK. The number\nof coarse correct answers is unchanged at \u223c5%.\n(2) DRAG improves both accuracy and informa-\ntiveness (red arrows in Figure 7): Compared to\nstandard decoding, DRAG improves both accuracy\nand informativeness. As evident from Figure 7,\nthis is obtained by a smaller fraction of abstentions\n(6%) and a significantly larger fraction of coarse\ncorrect answers (16%). This result confirms our\noriginal conjecture that the dichotomy (know/don\u2019t\nknow) underlying IDK methods is too coarse.\n5.4\nMeta-evaluation\nIn the previous sections, we showed that multi-\ngranularity answers facilitate a more faithful eval-\nuation of LLM performance on factual questions.\nHere, we check whether a similar effect could be\ngreedy\n+IDK\n+Agg\nDRAG\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n% of Examples\n0.17\n0.18\n0.06\n0.42\n0.31\n0.31\n0.30\n0.36\n0.31\n0.30\n0.32\n0.10\n0.07\nGRANOLA match breakdown\nIDK\nIncorrect\nLevel #1\nLevel #2\nLevel #3++\nFigure 8: The granularity of answers predicted by PaLM\n2-M. Level numbers correspond to the answer index in\nthe ordered set of GRANOLA answers, with 1 being the\nmost fine-grained. While all methods decrease the frac-\ntion of errors compared to greedy (from 42% to \u223c31%;\nred), DRAG does this with a fewer IDK predictions (e.g.,\n6% vs 17-18%; gray) and more coarse correct answers\n(e.g. 16% vs 4-6%).\nobtained by evaluating with semantic similarity\nagainst single-granularity reference answers.\nTo this end, we test if semantic similarity against\nsingle-granularity answers can distinguish between\nanswers that GRANOLA accuracy deems correct\nand incorrect. Concretely, we stratify GRANOLA-\nEQ according to whether both the standard and\nGRANOLA F1 scores exceed a threshold \u03c4, and re-\nport the mean semantic similarity score for each of\nthe four resulting subsets. Note that, by definition,\nthe standard F1 is a lower bound to GRANOLA\nF1, so one of the subsets is empty.\nTable 3 shows the results when using BLEURT\n(Sellam et al., 2020) as the semantic similarity met-\nric. The mean BLEURT score is similar for exam-\nples that are incorrect according to both metrics\nand for examples that are correct only according to\nGRANOLA accuracy (gray rows). This highlights\nthat BLEURT is not a good proxy for matching\nagainst multi-granularity answers. Examples from\nGRANOLA-EQ where GRANOLA accuracy dis-\nagrees with both standard accuracy and BLEURT\nscore are provided in Table 9 (Appendix D).\n6\nRelated work\nAnswer annotation in QA datasets.\nQA bench-\nmarks, e.g. Natural Question (Kwiatkowski et al.,\n2019), often have multiple answers per question,\nwhich may inadvertently include multi-granularity\nanswers. Min et al. (2020) consider the problem\nof ambiguous questions, proposing question re-\nwriting to resolve ambiguity. Si et al. (2021) mine\nStandard\naccuracy\nGRANOLA\naccuracy\n% of\nexamples\nBLEURT\nscore\n\u2713\n\u2713\n49.5\n0.83\n\u2713\n\u2717\n5.6\n0.28\n\u2717\n\u2713\n0.0\n-\n\u2717\n\u2717\n44.9\n0.26\nTable 3: Mean BLEURT score for PaLM 2-L with\ngreedy decoding on GRANOLA-EQ, stratified by stan-\ndard accuracy and GRANOLA accuracy.\nanswer aliases from a KG and use them to per-\nform \u201canswer expansion\u201d to increase the lexical\nmatching score. Our approach is similar but goes\none step further, using the KG and LLMs to add\nmulti-granularity answers vs. simply using aliases.\nGranularity-driven evaluation.\nGranularity of\nmodel responses has been evaluated in the con-\ntext of open-domain chatbots, where informative-\nness plays a crucial role in building engaging dia-\nlogue agents. Adiwardana et al. (2020); Thoppilan\net al. (2022) evaluate granularity, but their focus\nis on conversational language rather than knowl-\nedge evaluation. Huang et al. (2022) use WikiData\nto form masked token prediction tasks, such as\n\u201cToronto is located in [MASK]\u201d, and test whether\npretrained models have a preference for more spe-\ncific completions (e.g. \u201cOntario\u201d vs \u201cCanada\u201d).\nTechnically, their approach is less generic than\nours; it only accommodates single-token predic-\ntions, and their evaluation covers smaller models\n(GPT-2). More importantly, their goal is to encour-\nage specific answers, whereas we want to use gran-\nularity as a means for more meaningful evaluation\nof LM\u2019s knowledge and factuality.\nPunting.\nAbstaining from answering questions is\na popular approach for improving factuality (Kada-\nvath et al., 2022; Kuhn et al., 2023; Yoshikawa and\nOkazaki, 2023b; Chen et al., 2023; Zhang et al.,\n2023a). Our approach is motivated by the observa-\ntion that punting may be overly aggressive; when\nthe model has low confidence in a specific answer\nbut is confident in a coarser answer, outputting the\ncoarser answer is preferred over refusing to answer.\n7\nConclusion and Discussion\nWe highlight a prominent source of factuality er-\nrors in modern LMs: generating more detailed re-\nsponses than their knowledge can support, while\nfavouring a specific output format over factuality.\nWe quantify the effect of such errors in the con-\ntext of open-domain QA, by introducing a new QA\nbenchmark, GRANOLA-EQ, with multi-granularity\nanswers, and a novel decoding algorithm, DRAG,\nthat is geared towards aligning the granularity of\nthe LLM response with its uncertainty level. Our re-\nsults show that taking the answer granularity level\ninto account (both in evaluation and during decod-\ning) leads to a dramatic increase in model accuracy,\noverall suggesting that (a) current evaluation prac-\ntices underestimate model performance on tasks\nrequiring factual knowledge, and (b) LLMs are\nnot tuned towards generating texts reflecting their\nknowledge.\nOur findings introduce multiple interesting di-\nrections for future work to explore:\nQuestion perturbations.\nOur approach for gen-\nerating multi-granularity answers relied on abstrac-\ntions. A complementary approach would modify\nthe question rather than its answer, e.g., altering\nthe question \u201cWhen was Mark Bils born?\u201d to \u201cIn\nwhat year was Mark Bils born?\u201d. Such question\nperturbations could also be coupled with our en-\ntity abstraction perspective to generate more broad\nquestions like \u201cWhen was a professor from Univer-\nsity of Rochester born?\u201d. Another direction consid-\ners generating more specific questions to address\nknowledge gaps (Rabin et al., 2023). However,\nquestion perturbations may create new answers and\nthus would require more complex evaluation.\nImproving DRAG.\nThe two stages of DRAG \u2013 sam-\npling candidate responses, and response aggrega-\ntion \u2013 could be improved to yield better granularity\nadjustment. For example, it is possible to replace\nregular temperature sampling (Ackley et al., 1985)\nwith other sampling strategies that may perform bet-\nter (Wang et al., 2022; Freitag et al., 2023; Bertsch\net al., 2023). Additionally, better aggregators could\nimprove downstream task performance.\nResponse granularity fine-tuning.\nWhile this\nwork focused on improving factuality at inference\ntime, it is interesting to explore fine-tuning with re-\nsponse granularity in mind. For example, DRAG can\nbe used as a reward model for supervised or RLHF\nfinetuning to encourage models to learn how to tai-\nlor the their response granularity to their parametric\nknowledge or the preceding context.\nLimitations\nTechnically, our approach for enriching an exist-\ning QA benchmark with multi-granularity answers\nrelies on extracting entities from the original QA\npair and matching them to their KG entry. In less-\nstructured datasets this step may be more involved \u2013\nfor example, if the surface form of the entity name\ndiffers between the dataset and the KG.\nOn a more conceptual level, a faithful evaluation\nof the knowledge of LLMs may also require dis-\ntinguishing between correct answers based on true\nknowledge, as opposed to mere educated guesses.\nThis is an issue with QA evaluation in general \u2013 but\nis especially relevant in our setting, since coarser\nanswers are easier to guess correctly. For exam-\nple, in the question \u201cWhere was [X] born?\u201d, one\ncould guess \u201cRussia\u201d if X is a Russian-sounding\nname (whereas correctly guessing the city X was\nborn in is less likely). This may require additional\ninformation (in the form of providing additional\ninformation such as reasoning or evidence) but also\nrelates to how one defines knowledge.\nOther than that, our work was demonstrated on\na set of large-but-specific LMs from the PaLM\nmodel family. Further expanding the study to a\nwider range of models may also be compelling, but\nbeyond the scope of this work.\nAcknowledgements\nWe thank Amir Globerson, Tal Schuster, Or Hon-\novich, Eran Ofek and Idan Szpektor for their help-\nful comments on this work.\nReferences\nDavid H Ackley, Geoffrey E Hinton, and Terrence J Se-\njnowski. 1985. A learning algorithm for boltzmann\nmachines. Cognitive science, 9(1):147\u2013169.\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain chat-\nbot. arXiv preprint arXiv:2001.09977.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nAmanda Bertsch, Alex Xie, Graham Neubig, and\nMatthew R Gormley. 2023.\nIt\u2019s mbr all the\nway down: Modern generation techniques through\nthe lens of minimum bayes risk.\narXiv preprint\narXiv:2310.01387.\nJiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan O\nArik, Tomas Pfister, and Somesh Jha. 2023. Adapta-\ntion with self-evaluation to improve selective predic-\ntion in llms. arXiv preprint arXiv:2310.11689.\nRan El-Yaniv et al. 2010. On the foundations of noise-\nfree selective classification.\nJournal of Machine\nLearning Research, 11(5).\nMarkus Freitag, Behrooz Ghorbani, and Patrick Fer-\nnandes. 2023.\nEpsilon sampling rocks: Investi-\ngating sampling strategies for minimum bayes risk\ndecoding for machine translation. arXiv preprint\narXiv:2305.09860.\nDeep Ganguli, Amanda Askell, Nicholas Schiefer,\nThomas Liao, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Anna Chen, Anna\nGoldie, Azalia Mirhoseini, Catherine Olsson, Danny\nHernandez, et al. 2023. The capacity for moral self-\ncorrection in large language models. arXiv preprint\narXiv:2302.07459.\nYonatan Geifman and Ran El-Yaniv. 2017. Selective\nclassification for deep neural networks. Advances in\nneural information processing systems, 30.\nJie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong,\nand Wen-mei Hwu. 2022. Can language models be\nspecific? how? arXiv preprint arXiv:2210.05159.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, et al. 2022.\nLanguage models\n(mostly) know what they know.\narXiv preprint\narXiv:2207.05221.\nEhsan Kamalloo, Nouha Dziri, Charles LA Clarke, and\nDavood Rafiei. 2023. Evaluating open-domain ques-\ntion answering in the era of large language models.\narXiv preprint arXiv:2305.06984.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective question answering under domain shift. arXiv\npreprint arXiv:2006.09462.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\narXiv preprint arXiv:2302.09664.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453\u2013\n466.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9802\u20139822, Toronto,\nCanada. Association for Computational Linguistics.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020.\nAmbigqa: Answering\nambiguous open-domain questions. arXiv preprint\narXiv:2004.10645.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2021. Reframing\ninstructional prompts to gptk\u2019s language.\narXiv\npreprint arXiv:2109.07830.\nFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066.\nRoni Rabin, Alexandre Djerbetian, Roee Engelberg,\nLidan Hackmon, Gal Elidan, Reut Tsarfaty, and Amir\nGloberson. 2023. Covering uncommon ground: Gap-\nfocused question generation for answer assessment.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers), pages 215\u2013227, Toronto, Canada.\nAssociation for Computational Linguistics.\nJie Ren, Yao Zhao, Tu Vu, Peter J Liu, and Balaji Lak-\nshminarayanan. 2023. Self-evaluation improves se-\nlective generation in large language models. arXiv\npreprint arXiv:2312.09300.\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers.\narXiv preprint\narXiv:2109.08535.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\n2020. Bleurt: Learning robust metrics for text gener-\nation. In Proceedings of ACL.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Boyd-Graber, and Li-\njuan Wang. 2022. Prompting gpt-3 to be reliable.\narXiv preprint arXiv:2210.09150.\nChenglei Si, Chen Zhao, and Jordan Boyd-Graber.\n2021. What\u2019s in a name? answer equivalence for\nopen-domain question answering. arXiv preprint\narXiv:2109.05289.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nEllen M Voorhees et al. 1999.\nThe trec-8 question\nanswering track report. In Trec, volume 99, pages\n77\u201382.\nDenny Vrande\u02c7ci\u00b4c and Markus Kr\u00f6tzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78\u201385.\nLiming Wang, Siyuan Feng, Mark Hasegawa-Johnson,\nand Chang Yoo. 2022. Self-supervised semantic-\ndriven phoneme discovery for zero-resource speech\nrecognition. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8027\u20138047, Dublin,\nIreland. Association for Computational Linguistics.\nQi Yang, Shreya Ravikumar, Fynn Schmitt-Ulms,\nSatvik Lolla, Ege Demir, Iaroslav Elistratov, Alex\nLavaee, Sadhana Lolla, Elaheh Ahmadi, Daniela Rus,\net al. 2023a. Uncertainty-aware language model-\ning for selective question answering. arXiv preprint\narXiv:2311.15451.\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\nWikiqa: A challenge dataset for open-domain ques-\ntion answering.\nIn Proceedings of the 2015 con-\nference on empirical methods in natural language\nprocessing, pages 2013\u20132018.\nYuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neu-\nbig, and Pengfei Liu. 2023b. Alignment for honesty.\narXiv preprint arXiv:2312.07000.\nHiyori\nYoshikawa\nand\nNaoaki\nOkazaki.\n2023a.\nSelective-LAMA:\nSelective\nprediction\nfor\nconfidence-aware evaluation of language models. In\nFindings of the Association for Computational Lin-\nguistics: EACL 2023, pages 2017\u20132028, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nHiyori\nYoshikawa\nand\nNaoaki\nOkazaki.\n2023b.\nSelective-lama: Selective prediction for confidence-\naware evaluation of language models. In Findings\nof the Association for Computational Linguistics:\nEACL 2023, pages 1972\u20131983.\nHanning Zhang, Shizhe Diao, Yong Lin, Yi R Fung,\nQing Lian, Xingyao Wang, Yangyi Chen, Heng Ji,\nand Tong Zhang. 2023a. R-tuning: Teaching large\nlanguage models to refuse unknown questions. arXiv\npreprint arXiv:2311.09677.\nMuru Zhang, Ofir Press, William Merrill, Alisa\nLiu, and Noah A Smith. 2023b.\nHow language\nmodel hallucinations can snowball. arXiv preprint\narXiv:2305.13534.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023. Why does chatgpt fall short in providing truth-\nful answers. ArXiv preprint, abs/2304.10513.\nA\nAdditional figures\nIn Figure 9 we show how the parameter \u03bb impacts\nthe scores used to evaluate informativeness (see\nDefinition 1).\n = 0.1\n = 0.3\n = 1.0\n Value\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWeight Value\nw( ) as a function of \nw1\nw2\nw3\nFigure 9: Letting wi = exp(\u2212\u03bb(i \u2212 1)), we show how\nw1, w2, w3 behave for different values of \u03bb.\nB\nFull details: Constructing\nGRANOLA-EQ\nIn this section we detail our process of enriching\nENTITYQUESTIONS with multi-granular answers.\nEntity Questions.\nENTITYQUESTIONS (EQ)\n(Sciavolino et al., 2021) is a entity-rich QA dataset.\nIt was created by selecting 24 common relations\nfrom Wikidata and converting fact (subject, rela-\ntion, object) triples into natural language questions\nusing manually defined templates. We restrict our\nattention to the test split of ENTITYQUESTIONS. In\nthe original ENTITYQUESTIONS some of the rela-\ntions have answers that are already coarse (e.g. \u201cin\nwhich continent is country [X]\u201d). We thus filter ex-\namples belonging to such relations. See Table 4 for\nthe full list of ENTITYQUESTIONS and GRANOLA-\nEQ relations. Additionally, for simplicity, we only\nkeep rows with a unique ground truth example in\nthe original ENTITYQUESTIONS dataset.\nObtaining WikiData descriptions for entities.\nThe entity extraction stage is simple since the en-\ntity location is encoded in the template. For each\nQA pair we extract two entities: the question entity\nand the subject entity, and then look these up in\nWikiData to obtain a WikiData qid for each entity.\nWe then use the qid to obtain a free text description\nof the original entity.\nQID disambiguation.\nIn approximately 30% of\nthe cases, there are multiple potential qid matches\nfor the same entity (see Figure 5 for an example).\nWe use a simple heuristic for performing disam-\nbiguation: we select the qid with the smallest value.\nTemplate\nRelation\nIncluded?\nWhich country is [X] lo-\ncated in?\nP17\n\u2717\nWhere was [X] born?\nP19\n\u2713\nWhere did [X] die?\nP20\n\u2713\nWho is [X] married to?\nP26\n\u2713\nWhich continent is [X]\nlocated?\nP30\n\u2717\nWhat is the capital of\n[X]?\nP36\n\u2717\nWho is [X]\u2019s child?\nP40\n\u2713\nWho is the author of\n[X]?\nP50\n\u2713\nWhere was [X] edu-\ncated?\nP69\n\u2713\nWhat kind of work does\n[X] do?\nP106\n\u2717\nWho founded [X]?\nP112\n\u2713\nWho owns [X]?\nP127\n\u2713\nWhere is [X] located?\nP131\n\u2713\nWhat type of music\ndoes [X] play?\nP136\n\u2717\nWhere is the headquar-\nter of [X]?\nP159\n\u2713\nWho was [X] created\nby?\nP170\n\u2713\nWho performed [X]?\nP175\n\u2713\nWhich company is [X]\nproduced by?\nP176\n\u2713\nWhat music label is [X]\nrepresented by?\nP264\n\u2713\nWhere is [X] located?\nP276\n\u2713\nWhich language was\n[X] written in?\nP407\n\u2717\nWhat position does [X]\nplay?\nP413\n\u2717\nWhich country was [X]\ncreated in?\nP495\n\u2717\nTable 4: List of ENTITYQUESTIONS and GRANOLA-\nEQ relations.\nData cleaning.\nThere are two sources of noise in\nthe above automatic process: incorrect extracted de-\nscriptions (this may occur when there are multiple\nWikiData entries for the same entity name, and our\ndisambiguation procedure selects the wrong one)\nand errors in the LLM generated answers. Thus, to\nensure the data is of high quality, we apply several\nautomatic cleaning operations. First, we remove\nrows containing descriptions that are likely to be\nerroneous. We utilize the observation that when the\nQID disambiguation heuristic fails and the wrong\nQID is selected, this failure will typically be evi-\ndent from the fact that the extracted description is\nnot semantically consistent with the question; see\nTable 6 for concrete examples. To remove these\nexamples we score each example for how consis-\ntent the extracted descriptions are with the original\nquestions, and remove examples for which this pre-\ndicted score exceeds 0.5. Specifically, we prompt\nqid\ndescription\nQ64\nfederated state, capital and largest city\nof Germany\nQ142659\ncensus-designated place in Holmes\nCounty, Ohio\nQ524646\ntown in Massachusetts\nQ614184\ntown in Maryland, United States\nTable 5: QID matches and descriptions for the free text\nentity \u201cBerlin\u201d.\nQuestion\nQID\nQID\nDescrip-\ntion\nWho is the au-\nthor of Endur-\ning Love?\nQ129813\n2004\nfilm\nby\nRoger Michell\nWho performed\nOrbit?\nQ2367904\nhistorical motor-\ncycle manufac-\nturer\nWho is the au-\nthor of Holly-\nwood?\nQ34006\nneighborhood\nin Los Angeles,\nCalifornia,\nUnited States\nTable 6: When QID disambiguation chooses the in-\ncorrect entity, the failure is typically evident since the\nextracted description (rightmost column) does not se-\nmantically match the question.\nan LLM (with 5 few-shot demonstrations of the\nintended behaviour) to determine whether the de-\nscription is consistent (\u2018Yes\u2019 or \u2018No\u2019), and we de-\ntermine the score as the fraction of \u2018No\u2019 responses\n(sampled at unit temperature). This process ends up\nremoving 1409 examples (or 9.5% of the dataset).\nAs a second data cleaning step, we remove rows\nwith missing GRANOLA answers or duplicated\nanswers. Finally, we remove GRANOLA answers\nfrom a list of hard-coded responses that we define\nas trivial (such as \u201cperson\u201d, \u201cuniversity\u201d, etc). In\ntotal, these steps affected 2378 rows (or 16% of the\ndataset).\nC\nPrompts\nTable 7 details the prompts used for baseline al-\ngorithms. Table 8 details the prompts used for\nthe response aggregation sub-routine in DRAG and\nfor generating multi-granular answers to create\nGRANOLA-EQ.\nD\nAdditional Results for \u00a75\nIn this section we include supplementary results\nfrom \u00a75.\nBaseline\nPrompt\nVanilla\nQuestion: {question}\nAnswer:\nIDK\nYou will be given a question.\nAnswer the question, or output\nIDK. Question: {question}\nAnswer:\nIDKIfUncertain\nYou will be given a question.\nAnswer the question, or, if you\nare not certain of the answer,\noutput IDK.\nQuestion: {question}\nAnswer:\nAgg\nYou will be given a question.\nAnswer the question at a level\nof granularity that fits your un-\ncertainty, or output IDK.\nQuestion: {question}\nAnswer:\nTable 7: Prompts used in the baselines evaluated in \u00a75.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nStandard Accuracy\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nGRANOLA Accuracy\nWhat music label is [X] represented by?\nWhere did [X] die?\nWhere is [X] located?\nWhere is [X] located?\nWhere is the headquarter of [X]?\nWhere was [X] born?\nWhere was [X] educated?\nWhere was [X] founded?\nWhich company is [X] produced by?\nWho founded [X]?\nWho is [X] married to?\nWho is [X]'s child?\nWho is the author of [X]?\nWho owns [X]?\nWho performed [X]?\nWho was [X] created by?\nFigure 10: Standard Accuracy (x-axis) vs GRANOLA\naccuracy (y-axis), stratified by relation, for DRAG (PaLM\n2-L).\nProcedure\nPrompt\nForming\nmulti-\ngranular\nanswers\nYou will be given a pair of question and an-\nswer. You will also receive some additional\ndescription about the entity in the question\nand the entity in the answer.\nYour task is to write NEW ANSWERS for\nthe original question at various levels of\ngranularity. Number these answers starting\nfrom 1 (with 1 being the most fine grained\nanswer \u2013 the original answer), and larger\nindices corresponding to coarser answers.\nThe idea is that someone might not know\nthe answer at the most fine-grained level,\nbut perhaps know the answer at coarser lev-\nels.\nImportant: STOP generating answers BE-\nFORE you reach trivial answers. For exam-\nple, given the question \"who wrote the book\nX\", answers such as \"a writer\" or \"a per-\nson\" are considered trivial, as these are com-\npletely uninformative and can be guessed\neven without knowing what X is.\nIn your answers, use the format \u20191:: an-\nswer\u2019, etc.\nResponse\nAggrega-\ntion\nYou will be given a list of responses;\nreplace them with the most specific answer\nthat is still consistent with all the original\nresponses. If the responses have nothing\nmeaningful in common with respect to the\nquestion, output IDK.\nHere are some examples:\nQuestion: Where was [X] born?\nResponses:\n- Hamburg\n- Hamburg\n- Bonn\n- Berlin\nCorrect aggregated answer: Germany\nIncorrect aggregated answer: Hamburg\nExplanation: These are all different cities\nin Germany.\nHamburg is not a correct\naggregation, since it is not consistent with\nother responses, such as Berlin or Bonn.\nQuestion: When was [X] born?\nResponses:\n- February 1, 1937\n- November 20, 1937\n- January 1937\nCorrect aggregated answer: 1937\nIncorrect aggregated answer: November\n1937\nExplanation: These are all dates in 1937.\nTable 8: Prompts used in GRANOLA related proce-\ndures in the paper.\nPopualrity\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nGreedy\nPopualrity\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nDRAG\nStandard Accuracy\nGRANOLA accuracy\nFigure 11: Accuracy (y-axis) vs Entity Popularity (x-axis) for two algorithms: Greedy (left) and DRAG (right). The\nunderlying model is PaLM 2-L. We see that the knowledge evaluation gap is evident for DRAG. The behaviour for\nPaLM 2-M is identical, except the absolute numbers are smaller.\nQuestion\nOriginal GT\nAnswer\nCandidate\nAnswer\nMatched\nGRANOLA\nAnswer\nBLEURT\nScore (GT vs.\nCandidate)\nF1 Score (GT\nvs.\nCandi-\ndate)\nF1\nScore\n(GRANOLA\nvs candidate)\nWhat\nis\nAline\nBrosh\nMcKenna\nfamous for?\n27 Dresses\nscreenwriter\nbeing a screen-\nwriter\n0.04\n0.00\n0.67\nWhere\ndid\nTilly\nArm-\nstrong die?\nCarshalton\nLondon\nLondon\nBor-\nough of Sutton\n0.05\n0.00\n0.40\nWhere is the\nheadquarter\nof\nGuildhall\nSchool\nof\nMusic\nand\nDrama?\nBarbican Cen-\ntre\nLondon\nCity of Lon-\ndon\n0.06\n0.00\n0.50\nWhere is Bat-\ntersea Park lo-\ncated?\nBattersea\nLondon\nLondon\n0.06\n0.00\n1.00\nTable 9: Examples from GRANOLA-EQ where GRANOLA accuracy disagrees with standard metrics (lexical\nmatching and semantic matching to the original GT answer). The examples were obtained by filtering for example\nwith low F1 score to the original GT answer but high F1 score to the matched GRANOLA answer, and then sorting\nby BLEURT scores in ascending order. I.e., they correspond to the points in the top-left corner of Figure 12.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 score (standard)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 score (GRANOLA)\nF1 scores\n(color coded by BLEURT scores)\nbleurt\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 12: The relationship between standard F1 score\n(x-axis), GRANOLA F1 score (y-axis) and BLEURT\nscore (Sellam et al., 2020) (color) computed between\nthe original gt answer the candidate answer. The under-\nlying model is Greedy (PaLM 2-L). The figure demon-\nstrates that while there is a strong correlation between\nstandard F1 score and BLEURT scores, this correlation\nfails specifically for the subset of examples for which\nGRANOLA accuracy disagrees with standard accuracy.\nSee Table 3 for a quantitative version of this plot. This\ndemonstrates that BLEURT scores can not serve as a\nreplacement for GRANOLA labels.\n"
  },
  {
    "title": "FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for Acoustic Echo Cancellation",
    "link": "https://arxiv.org/pdf/2401.04283.pdf",
    "upvote": "3",
    "text": "arXiv:2401.04283v1  [eess.AS]  8 Jan 2024\nFADI-AEC: FAST SCORE BASED DIFFUSION MODEL GUIDED BY FAR-END SIGNAL FOR\nACOUSTIC ECHO CANCELLATION\nYang Liu, Li Wan, Yun Li, Yiteng Huang, Ming Sun, James Luan, Yangyang Shi, Xin Lei\nMeta Platforms, USA\n{yangliuai, wwanli, yunli1, yah, sunming425, jamesluan, yyshi, leixin}@meta.com\nABSTRACT\nDespite the potential of diffusion models in speech en-\nhancement, their deployment in Acoustic Echo Cancellation\n(AEC) has been restricted. In this paper, we propose DI-\nAEC, pioneering a diffusion-based stochastic regeneration\napproach dedicated to AEC. Further, we propose FADI-AEC,\nfast score-based diffusion AEC framework to save com-\nputational demands, making it favorable for edge devices.\nIt stands out by running the score model once per frame,\nachieving a signi\ufb01cant surge in processing ef\ufb01ciency. Apart\nfrom that, we introduce a novel noise generation technique\nwhere far-end signals are utilized, incorporating both far-end\nand near-end signals to re\ufb01ne the score model\u2019s accuracy.\nWe test our proposed method on the ICASSP2023 Microsoft\ndeep echo cancellation challenge evaluation dataset, where\nour method outperforms some of the end-to-end methods and\nother diffusion based echo cancellation methods.\nIndex Terms\u2014\necho cancellation, diffusion model,\nstochastic regeneration, predictive learning\n1. INTRODUCTION\nThe importance of acoustic echo cancellation in achieving\nhigh-quality speech in voice communication has led to the\nemergence of deep neural network (DNN) based methods,\nsuch as the deep complex convolution recurrent network (DC-\nCRN) [1]. The main challenges are artifacts, target speech\ndistortion, and echo leakage during double-talk scenarios.\nTo address these challenges, researchers have proposed tech-\nniques such as alignment modules [2], novel architecture\n[3, 4] and modi\ufb01ed loss functions [5]. Currently, the majority\nof echo cancellation models adhere to predictive methodolo-\ngies, which learn a deterministic mapping from corrupted\nspeech to clean speech targets. However, generative models,\nsuch as variational auto-encoders (VAEs) [6], generative ad-\nversarial networks (GANs) [7], and diffusion approaches [8],\noffer a different perspective. These models learn the target\ndistribution and have the capability to generate multiple valid\nestimates, providing a richer set of possibilities for AEC task.\nAmong these, diffusion-based generative models have\nshown promising results in tasks such as noise suppression.\nFor instance, Lu et al. [9] proposed a novel approach that\nincorporates the characteristics of the observed noisy speech\nsignal into the diffusion and reverse processes, providing a\nmore tailored enhancement. Similarly, Joan Serr`a et al. [10]\nintroduced a multi-resolution conditioning network that em-\nploys score-based diffusion.\nTheir model generates clean\nspeech from noisy inputs by progressively reducing the noise\nin a series of steps, effectively diffusing the noise out of the\nsignal. Lemercier et al. [11] applied a stochastic regener-\nation approach, where an estimate provided by a predictive\nmodel is used as a guide for further diffusion, re\ufb01ning the\nenhancement process.\nWhile there have been signi\ufb01cant advancements in the\n\ufb01eld, the potential of using diffusion models for echo can-\ncellation has largely remained untapped. One of the primary\nchallenges is the computational intensity of existing diffusion\nmodels, which makes their deployment in real-world pro-\nduction settings a daunting task. In this paper, we introduce\ntwo novel models: DI-AEC, which applies a diffusion-based\nstochastic regeneration method to echo cancellation, and\nFADI-AEC, an ef\ufb01cient version of DI-AEC that utilizes a fast\nscore model. To the best of our understanding, these represent\nthe \ufb01rst echo cancellation models that harness the power of\ndiffusion. Notably, FADI-AEC addresses the computational\nhurdle by executing the score model only once per frame,\nleveraging the prior state to minimize processing time. Both\nmodels employ far-end signals to produce noise, thereby\nenhancing performance.\nBy considering both far-end and\nnear-end signals, these models enhance accuracy, generating\nhigh-quality samples.\n2. PROPOSED METHOD\n2.1. Problem formulation\nIn a typical AEC system, the microphone signal is denoted as\nh(n). This signal comprises two components: the near-end\nspeech s(n) and the acoustic echo z(n). Mathematically, this\nrelationship is expressed as:\nh(n) = s(n) + z(n),\n(1)\nFilter Prediction\nSampling\nFast Reverse Diffusion\nx(n)\nD\u03b8\nG\u03c6\nh(n)\n\u02c6s(n)\n\u02c6s(n) \u2212 \u03c3(n)z(x(n))\n\u02dcs(n)\nFig. 1. FADI-AEC pipeline. The predictive \ufb01lter is \ufb01rst used\nto generate an predicted estimate \u02c6s(n) from mic signal h(n).\nDiffusion-based generation G\u03c6 is then performed by adding\nGaussian noise guided by far-end sign x(n) and solving the\nreverse diffusion SDE. The estimated near-end speech is\u02dcs(n),\nwhich would be used in the score function in the next frame.\nwhere n is the time sample index. The acoustic echo z(n)\ncan be understood as a time-delayed version of the far-end\nreference signal x(n). This signal has traversed the echo path\nand might have undergone nonlinear distortions due to the\nloudspeakers. The primary objective of the AEC system is to\nseparate the near-end speech s(n) from the microphone signal\nh(n).\n2.2. Forward and inference through reverse sampling\nThe stochastic forward process utilized in score-based diffu-\nsion models is de\ufb01ned by the It\u02c6o Stochastic Differential Equa-\ntion (SDE) [12]:\nds(n)t = f(s(n)t, t)dt + g(t)dw\n(2)\nwhere w denotes a standard-dimensional Brownian motion,\nmaking dw a zero-mean Gaussian random variable with vari-\nance proportional to dt, pertinent for each Time-Frequency\n(T-F) bin. The functions f and g represent the drift and dif-\nfusion coef\ufb01cients, respectively. The state of the process at\ndiscrete index n and continuous time t, where t \u2208 [0, T ] is\ngiven by s(n)t, and for clean speech, the initial condition is\ns(n)0 = s(n).\nIn the reverse process of score based diffusion model, the\nscore model is substituted into the reverse SDE as plug-in re-\nverse SDE [13]:\nds(n)t =\n\u0002\n\u2212f (s(n)t, t) + g(t)2\u2207s(n)t log pt (s(n)t)\n\u0003\ndt + g(t)dw\n(3)\nwhere dw is a d-dimensional Brownian motion for the time\n\ufb02owing in reverse and \u2207s(n)t log pt (s(n)t) is the score func-\ntion. This equation is classi\ufb01ed under the Ornstein-Uhlenbeck\nSDEs [14].\nDuring inference, Eq. (3) is evaluated using the predictor-\ncorrector approach as informed by the score-matching net-\nwork described later in [12]. The initial state of the process is\ndrawn from the distribution:\ns(n)\u03c4 \u223c NC\n\u0000s(n)\u03c4; h(n), x(n), \u03c32(\u03c4)I\n\u0001\n(4)\nThis distribution essentially represents a near-end signal h(n)\nand far-end signal x(n), to which Gaussian noise with a vari-\nance of \u03c32(\u03c4) is added.\n2.3. Score model with far-end guided noise\nSince the speech enhancement including AEC task could be\nconsidered as condition generation task, the conditioning is\nintegrated into the diffusion process by denoting the forward\nprocess where Eq. (2) yields the following complex Gaussian\ndistribution for the process state s(n)t, known as the per-\nturbation kernel [15], NC\n\u0000s(n)t; \u00b5 (s(n)0, h(n), t) , \u03c3(t)2I\n\u0001\nwhere the mean is \u00b5 and variance is \u03c3(t)2.\nWhen per-\nforming inference, one tries to solve the reverse SDE in\nEq. (3). For the simple Gaussian form of the perturbation\nkernel p0,t (s(n)t|s(n)0, h(n), x(n)) for AEC tasl and the\nregularity conditions exhibited by the mean and variance,\na score matching objective can be used to train the score\nmodel s\u03c6. The score function of the perturbation kernel is\n\u2212 s(n)t\u2212\u00b5(s(n),h(n))\n\u03c3(n)2\n. We can reparameterize the score match-\ning objective as follows\nJ (DSM)(\u03c6) = E\n\"\r\r\r\rs\u03c6 (s(n)t, h(n), t) +\nz\n\u03c3(n)\n\r\r\r\r\n2\n2\n#\n(5)\nThe clean s(n) and noisy h(n) utterances are picked in\nthe training set and the current process state is obtained as\ns(n)t = \u00b5(s(n), h(n)) + \u03c3(n)z(x(n)). Note that the noise\nhere is generated based on the aligned far-end signal calcu-\nlated by [2], not the traditional uniform random noise. This\ndistinction arises because echo cancellation is different from\nnoise suppression. The echo has a strong correlation with\nthe far-end signal x(n).\nUtilizing x(n) can help reverse\ndiffusion achieve stable performance with fewer frames.\nTherefore, we de\ufb01ned the far-end signal guided noise as\nz \u223c NC(z; x(n), I).\n2.4. Fast Score model\nAssuming that s(n) represents a stable recording, both echo\nrecording z(n) and noisy recording h(n) would also be sta-\nble. As more input data is taken into account, the \ufb01lter in AEC\nwould estimate the echo path, resulting in a reduced value of\n\u03c3. We can think of the \ufb01lter\u2019s convergence process as an in-\nverse operation. The perturbation kernel could be simpli\ufb01ed\nas:\np (\u02c6s(n)|s(n), h(n)) = NC\n\u0000\u02c6s(n); \u00b5 (\u02c6s(n), h(n)) , \u03c3(n)2I\n\u0001\n(6)\nwhere \u02c6s(n) the current process state after AEC \ufb01lter D\u03b8. Fur-\nther, The score function of the perturbation kernel is simpli-\n\ufb01ed as \u2212\u02c6s(n)t\u2212\u00b5(s(n),h(n))\n\u03c3(n)2\nand the score mathcing objective\nEq. (5) is simpli\ufb01ed as\nJ (DSM)(\u03c6) = E\n\"\r\r\r\r\u02c6s\u03c6 (s(n), h(n), s(n \u2212 1)) +\nz\n\u03c3(n)\n\r\r\r\r\n2\n2\n#\n(7)\nwhere J (DSM) is not related to t , but rather to s(n \u2212 1). This\nimplies that the score model takes into account the enhanced\nsignal from the previous frame.\nOptimization is achieved\nthrough iterative processing over time, rather than multiple\nscore calculations within a single frame. While this approach\nmight require more iterations to stabilize the system, it re-\nduces the computational load for each frame. Subsequently,\nthe corrector employs the score network\u2019s output to synchro-\nnize the generated sample with the anticipated marginal dis-\ntribution, taking cues from the score network\u2019s estimate. For\nsimplicity, we term G\u03c6 as the generative model. This cor-\nresponds to the reverse diffusion process solver, steered by\nboth the plug-in SDE and the score network s\u03c6. Thus, our\nconcluding estimate is expressed as \u02c6x(n) = G\u03c6(h(n)).\n2.5. DI-AEC and FADI-AEC\nTo address the computational burden of full diffusion-based\nmodels, we consider to use the stochastic regeneration model\n[11] as our baseline. By utilizing the predictive model\u2019s ini-\ntial approximation, it signi\ufb01cantly reduces the number of dif-\nfusion steps required, thereby curbing the computational de-\nmands typically seen with full diffusion models. DI-AEC and\nFADI-AEC have similar pipelines. The main different is DI-\nAEC uses the score (5) while FADI-AEC uses the fast score\nfunction (7) and only calculate one time for each frame. The\nwhole pipeline of FADI-AEC is shown as Fig. 1. For near-\nend signal h(n), a predictive model D\u03b8 serves as an initial\npredictor producing an estimate \u02c6s(n) de\ufb01ned as:\n\u02c6s(n) = s(n) \u2212 s(n)dis + z\u2032(n)\n(8)\nwhere s(n)dis is the target distortion introduced by the pre-\ndictive model. The residual corruption z\u2032(n) is what remains\nof the noise after being processed by the model. A diffusion-\nbased generative model G\u03c6 is then used to learn the distri-\nbution of the ideal residue rs(n) = s(n)dis \u2212 z\u2032(n), start-\ning from the echo residue rh(n) = h(n) \u2212 D\u03b8(h(n)) =\ns(n)dis + z(n) \u2212 z\u2032(n). The resulting a priori Signal-echo\nrate in the starting point is very high, as for a reasonable pre-\ndictor D\u03b8, for ||\u02dcz\u2032(n)|| \u226a ||z(n)||. The estimate \u02dcs(n) is then\nobtained as G\u03c6(D\u03b8(h(n))).\n2.6. Loss\nFor training, we de\ufb01ne the loss as L combining score match-\ning and and a supervised regularization term \u2014e.g. mean\nsquare error\u2014matching the output of the initial predictor to\nthe target speech:\nL(StoRM )(\u03b8, \u03c6) = L(DSM)(\u03b8) + \u03b1L(Sup )(\u03c6)\n=E\n\"\r\r\r\rs\u03c6 (\u02c6s(n), h(n), s(n \u2212 1)) +\nz\n\u03c3(n)\n\r\r\r\r\n2\n2\n#\n+ \u03b1E\nh\n\u2225s(n) \u2212 D\u03b8(h(n))\u22252\n2\ni\n,\n(9)\nwhere \u03b1 is a balance term that we empirically set to 1.\n3. EXPERIMENTATION RESULTS\n3.1. Data Selection and Enhancement\nFor model training, we utilize a combination of synthetic data\nfrom the AEC-challenge [16] and our privately enhanced\ndataset. We ensure gender balance among speakers on both\nthe far-end and near-end sides, resulting in 720 original con-\nversations, each lasting 10 seconds.\nEach conversation is then enhanced considering several\ntypical use cases.\nReverberation Time (RT60): We em-\nploy the image method [17] to generate both steady and time-\nvarying room impulse responses (RIR), which are used to\nsimulate echo paths in common laptop environments. The\nRT60 values are selected based on probabilities of 0.6, 0.3,\n0.08, and 0.02 for ranges of 50-300 ms, 300-600 ms, 600-1\ns, and 1-1.5 s, respectively. Delay: We introduce a delay be-\ntween the playback and its received echo, with probabilities\nof 0.05, 0.6, 0.4, and 0.05 for delay ranges of -20-0 ms, 0-200\nms, 200-400 ms, and 400-600 ms, respectively. Signal-to-\nNoise Ratio (SNR): We simulate SNR using typical noises\nfrom the DNS-challenge [18], with probabilities of 0.1, 0.1,\n0.3, and 0.5 for SNR ranges of 0-10 dB, 10-20 dB, 20-30 dB,\nand 30-40 dB, respectively. Signal-to-Echo Ratio (SER): We\nsimulate SER with probabilities of 0.1, 0.5, 0.3, and 0.1 for\nSER ranges of -10-0 dB, 0-10 dB, 10-30 dB, and 30-40 dB,\nrespectively. Non-linearity: We model non-linearity using\neither an arc-tangent function to mimic gain saturation or a\npolynomial function as demonstrated by [19]. Time-Variant\nDelay/RIR Changes: We introduce random cuts or additions\nof speech and silence segments, ranging from 10 to 200 ms,\nto either near-end or far-end signals with a probability of 0-\n10%. Time-variant RIR changes are also incorporated when\ngenerating the echo component. Each enhanced conversation\nis then converted into far-end single talk (FEST), near-end\nsingle talk (NEST), and double talk (DT) scenarios. This en-\nhancement process results in a total of 720K enhanced con-\nversations, approximately equivalent to 2,000 hours of con-\nversation.\n3.2. Ablation study\nTable 1 presents the performance of various acoustic echo\ncancellation (AEC) models based on different sampling meth-\nIndex\nModel\nSampling\nReversion\n# Parameters\nLatency\nERLE of\nPESQ of\nPESQ of\nDiffusion\n(M)\n(ms)\nFEST (dB)\nNEST\nDT\n1\nCRN\nNo\nNo\n3.6\n4.04\n67.67\n4.41\n2.34\n2\nCRN\nNo\nNo\n7.8\n8.93\n82.45\n4.50\n2.60\n3\nDI-AEC\nRandom\nYes\n6.9\n325.00\n92.51\n4.87\n3.30\n4\nDI-AEC\nFar Guided\nYes\n6.9\n325.00\n92.83\n4.91\n3.32\n5\nFADI-AEC\nRandom\nFast Score\n6.9\n9.14\n86.24\n4.81\n3.08\n6\nFADI-AEC\nFar Guided\nFast Score\n6.9\n9.14\n89.41\n4.83\n3.21\nTable 1. Performance comparison over candidate models. We measure WB-PESQ both DT and NEST scenarios and ERLE for\nFEST scenario in the augmented evaluation dataset.\nModel\nFEST\nDT\nDT other\nByteAudio\n4.709\n4.770\n4.312\nDI-AEC\n4.732\n4,783\n4.328\nFADI-AEC\n4.719\n4.781\n4.321\nTable 2. AECMOS comparison on ICASSP 2023 AEC Chal-\nlenge blind test.\nods and the presence of reversion diffusion. The models are\nevaluated on parameters such as ERLE of FEST, PESQ of\nNEST, and PESQ of DT.\nInitially, we can see that the CRN model [1] without\nany sampling or reversion diffusion presents two different\nparameter sizes.\nSpeci\ufb01cally, the larger model with large\nlatency shows superior performance in both ERLE of FEST\nand PESQ of NEST. This suggests that, given the same ar-\nchitectural characteristics, increasing the model size can lead\nto better AEC performance. The DI-AEC models (id 3 and\n4) introduce reversion diffusion and use distinct sampling\nmethods - Random Sampling and Far Guided Sampling, re-\nspectively. These models exhibit a notably high latency of\n325.00ms.\nDespite the higher latency, the DI-AEC mod-\nels surpass the CRN models in all the performance metrics.\nShifting to FADI-AEC models (id 5 and 6), we observe that\nthey combine a modi\ufb01ed reversion approach, termed Fast Re-\nversion, with sampling techniques. These models manage to\nsigni\ufb01cantly reduce the latency to 9.14ms. Performance-wise,\nthe FADI-AEC with Far Guided Sampling marginally outper-\nforms its Random Sampling counterpart, especially in terms\nof ERLE of FEST and PESQ of DT. To conclude, Upon ap-\nplying the DI-AEC model, the DT PESQ improved by 27.7%,\nrising from 2.6 to 3.32. When implementing FADI-AEC, the\nDT PESQ decreased to 3.08, representing a decrease of 7.2%.\nHowever, a notable advantage of FADI-AEC is its reduced\nlatency, which is only 2.8% of DI-AEC\u2019s latency, comparing\n9.14 to 325.\n3.3. Comparison with the state-of-the-art methods\nWe use AECMOS, a non-intrusive model-based metric from\nthe AEC challenge, to compare our proposed method against\nthe established baselines ByteAudio [20] and DI-AEC.\nByteAudio employs a Two-step Band-split Neural Network\n(TBNN) methodology for full-band acoustic echo cancella-\ntion, achieving the highest performance in the AEC 2023\nchallenges, second only to the host. ByteAudio represents\nan advanced end-to-end AEC network solution. Meanwhile,\nDI-AEC stands as another leading AEC technique optimized\nfor various real-world acoustic echo cancellation scenarios.\nAs shown in Table 2, FADI-AEC slightly outperforms both\nByteAudio and DI-AEC across all three scenarios. Speci\ufb01-\ncally, FADI-AEC attains performance scores of 4.719, 4.781,\nand 4.321 for the FEST, DT, and DT other scenarios respec-\ntively.\nAll three methods demonstrate remarkable perfor-\nmance on the AEC task, but FADI-AEC holds a slight edge\nacross the considered scenarios.\nThese results underscore\nthe importance of selecting the appropriate AEC method for\nspeci\ufb01c tasks and scenarios, and also highlight the progress\nand potential of contemporary AEC technologies.\n4. CONCLUSION\nWe proposed DI and FADI, two novel score-based diffusion\nmodels speci\ufb01cally designed for acoustic echo cancellation.\nThe research highlights the diffusion-based stochastic regen-\neration model can improve AEC model performance. To ad-\ndress the computational cost challenges posed by DI-AEC,\nparticularly for edge devices, FADI-AEC signi\ufb01cantly im-\nproves processing ef\ufb01ciency by running the score model only\nonce per frame. Furthermore, FADI-AEC and DI-AEC can\nuse a pioneering noise generation technique with far-end sig-\nnals, integrating both far-end and near-end signals to enhance\nthe precision of the score model. This unique application\nof diffusion models offers a powerful and ef\ufb01cient approach\nto echo cancellation, showcasing strong performance even in\nchallenging conditions. In future work, the fast score model\nproposed in FADI could be applied in the other speech en-\nhancement task such as noise suppression and dereverbera-\ntion.\n5. REFERENCES\n[1] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu,\nB. Zhang, and L. Xie, \u201cDCCRN: Deep complex con-\nvolution recurrent network for phase-aware speech en-\nhancement,\u201d in Proc. InterSpeech, 2021, p. 2472\u20132476.\n[2] Y. Liu, Y. Shi, Y. Li, K. Kalgaonkar, S. Srinivasan, and\nX. Lei, \u201cSCA: Streaming cross-attention alignment for\necho cancellation,\u201d in Proc. IEEE ICASSP. IEEE, 2023,\npp. 1\u20135.\n[3] S. Zhang, Y. Kong, S. Lv, Y. Hu, and L. Xie,\n\u201cFT-\nLSTM based complex network for joint acoustic echo\ncancellation and speech enhancement,\u201d arXiv preprint\narXiv:2106.07577, 2021.\n[4] H. Zhao, N. Li, R. Han, L. Chen, X. Zheng, C. Zhang,\nL. Guo, and B. Yu, \u201cA deep hierarchical fusion network\nfor fullband acoustic echo cancellation,\u201d in Proc. IEEE\nICASSP. IEEE, 2022, pp. 9112\u20139116.\n[5] F. Xiong, M. Dong, K. Zhou, H. Zhu, and J. Feng,\n\u201cDeep subband network for joint suppression of echo,\nnoise and reverberation in real-time fullband speech\ncommunication,\u201d in Proc. IEEE ICASSP. IEEE, 2023,\npp. 1\u20135.\n[6] D.P. Kingma and M. Welling,\n\u201cAuto-encoding varia-\ntional bayes,\u201d arXiv preprint arXiv:1312.6114, 2013.\n[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\n\u201cGenerative adversarial networks,\u201d Communications of\nthe ACM, vol. 63, no. 11, pp. 139\u2013144, 2020.\n[8] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and\nS. Ganguli, \u201cDeep unsupervised learning using nonequi-\nlibrium thermodynamics,\u201d in Proc. ICML. PMLR, 2015,\npp. 2256\u20132265.\n[9] Y.-J. Lu, Z.-Q. Wang, S. Watanabe, A. Richard, C. Yu,\nand Y. Tsao, \u201cConditional diffusion probabilistic model\nfor speech enhancement,\u201d in Proc. IEEE ICASSP. IEEE,\n2022, pp. 7402\u20137406.\n[10] J. Serr`a, S. Pascual, J. Pons, R.O. Araz, and D. Scaini,\n\u201cUniversal speech enhancement with score-based diffu-\nsion,\u201d arXiv preprint arXiv:2206.03065, 2022.\n[11] J.-M. Lemercier, J. Richter, S. Welker, and T. Gerk-\nmann, \u201cStoRM: A diffusion-based stochastic regener-\nation model for speech enhancement and dereverbera-\ntion,\u201d arXiv preprint arXiv:2212.11851, 2022.\n[12] Y. Song, J. Sohl-Dickstein, D.P. Kingma, A. Kumar,\nS. Ermon, and B. Poole, \u201cScore-based generative mod-\neling through stochastic differential equations,\u201d arXiv\npreprint arXiv:2011.13456, 2020.\n[13] C.-W. Huang, J.H. Lim, and A.C. Courville, \u201cA varia-\ntional perspective on diffusion-based generative models\nand score matching,\u201d Advances in Neural Information\nProcessing Systems, vol. 34, pp. 22863\u201322876, 2021.\n[14] B. Oksendal, Stochastic differential equations: an intro-\nduction with applications, Springer Science and Busi-\nness Media, 2013.\n[15] S. S\u00a8arkk\u00a8a and A. Solin, Applied stochastic differential\nequations, vol. 10, Cambridge University Press, 2019.\n[16] R. Cutler, A. Saabas, T. Parnamaa, M. Purin, H. Gam-\nper, S. Braun, K. Sorensen, and R. Aichner, \u201cICASSP\n2022 acoustic echo cancellation challenge,\u201d\nin Proc.\nIEEE ICASSP, 2022.\n[17] J.B. Allen and D.A. Berkley, \u201cImage method for ef\ufb01-\nciently simulating small-room acoustics,\u201d The Journal\nof the Acoustical Society of America, vol. 65, no. 4, pp.\n943\u2013950, 1979.\n[18] H. Dubey, V. Gopal, R. Cutler, S. Matusevych, S. Braun,\nE.S. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, and\nR. Aichner, \u201cICASSP 2022 deep noise suppression chal-\nlenge,\u201d in Proc. IEEE ICASSP, 2022.\n[19] C. Zhang, J. Liu, and X. Zhang,\n\u201cLCSM: A\nlightweight complex spectral mapping framework for\nstereophonic acoustic echo cancellation,\u201d arXiv preprint\narXiv:2208.07277, 2022.\n[20] Z. Zhang, S. Zhang, M. Liu, Y. Leng, Z. Han, L. Chen,\nand L. Xie,\n\u201cTwo-step band-split neural network ap-\nproach for full-band residual echo suppression,\u201d in Proc.\nIEEE ICASSP. IEEE, 2023, pp. 1\u20132.\nThis figure \"pipeline.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2401.04283v1\n"
  }
]