[
  {
    "title": "One Wide Feedforward is All You Need",
    "link": "https://arxiv.org/pdf/2309.01826.pdf",
    "upvote": "30",
    "text": "One Wide Feedforward is All You Need\nTelmo Pessoa Pires\u2217\u2020\nEquall\ntelmo@equall.ai\nAnt\u00f3nio V. Lopes\nYannick Assogba\nHendra Setiawan\u2217\nApple\n{antoniovilarinholopes, yassogba, hendra}@apple.com\nAbstract\nThe Transformer architecture has two main\nnon-embedding components: Attention and\nthe Feed Forward Network (FFN). Attention\ncaptures interdependencies between words re-\ngardless of their position, while the FFN non-\nlinearly transforms each input token indepen-\ndently. In this work we explore the role of the\nFFN, and find that despite taking up a signif-\nicant fraction of the model\u2019s parameters, it is\nhighly redundant. Concretely, we are able to\nsubstantially reduce the number of parameters\nwith only a modest drop in accuracy by remov-\ning the FFN on the decoder layers and sharing a\nsingle FFN across the encoder. Finally we scale\nthis architecture back to its original size by in-\ncreasing the hidden dimension of the shared\nFFN, achieving substantial gains in both accu-\nracy and latency with respect to the original\nTransformer Big.\n1\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017)\nhas become the de facto paradigm in many Nat-\nural Language Processing (NLP) tasks, includ-\ning Machine Translation (MT). Several studies\nhave shown that Transformers exhibit impressive\nscaling-law properties (Gordon et al., 2021; Bansal\net al., 2022; Ghorbani et al., 2022), wherein in-\ncreasing the number of model parameters leads\nto further accuracy gains. In parallel with this ar-\nchitecture\u2019s impressive scaling of the numbers of\nparameters (Chowdhery et al., 2022), there is a\ngrowing trend towards reducing model footprints\nfor real-world deployment, to satisfy practical con-\nstraints like latency requirements as well as mem-\nory and disk space limitations. In turn, researchers\nare actively exploring parameter sharing (Ge et al.,\n2022; Takase and Kiyono, 2023; Lou et al., 2022),\nreducing the dimensionality of Transformer compo-\n*Equal contribution.\n\u2020Work conducted while at Apple.\nnents, and pruning components like attention heads\n(Voita et al., 2019; Michel et al., 2019).\nAlthough the role of attention in learning pair-\nwise dependencies between tokens is relatively well\nunderstood (Voita et al., 2019; Clark et al., 2019;\nVig and Belinkov, 2019), the role of the Feed For-\nward Network (FFN) remains under-explored. Re-\ncently, Geva et al. (2021) established a connection\nbetween the FFN and attention by positing that\nthe FFN corresponds to learnable key-value pairs\nwhere the weights of the first layer of the FFN cor-\nresponds to the keys and those of the second to the\nvalues. They find that the keys are able to cap-\nture salient textual patterns at each layer, and they\nnotice that the classes of patterns tend to overlap\nbetween neighboring layers, indicating redundancy\nin the representation.\nThis observation motivates our work, where we\nrevisit the conventional practice of allocating an\nindividual FFN per layer. We investigate the effect\nof sharing and dropping the FFN across different\nlayers on MT models. We conduct thorough exper-\niments with different configurations of the Trans-\nformer, across different language pairs, including\na low resource language pair and multilingual. In\naddition, we investigate the effect of the FFN in a\ndecoder-only Transformer-based model. We find\nthat a considerable level of redundancy exists be-\ntween the encoder and decoder FFNs. As a result,\nwe are able to eliminate the decoder FFN and share\na single FFN across the encoder without signifi-\ncantly compromising the model\u2019s accuracy. This\nstep leads not only to significant parameter savings\nbut also opens up opportunities for further improve-\nments. We also suggest using wider FFNs in the\nencoder while dropping the decoder\u2019s FFN, which\nresults in a model with a similar size, but improved\naccuracy and reduced latency.\nFinally we conduct a fine-grained analysis of\nthe representational similarity between the origi-\nnal model, using one independent FFN per layer,\narXiv:2309.01826v2  [cs.CL]  21 Oct 2023\nand various models with shared FFNs. Our results\nreveal that both model accuracy and the internal\nrepresentation of Transformer blocks remain stable\nwhen sharing the FFN.\n2\nBackground and Methodology\n2.1\nTransformer\nThe Transformer architecture has two main compo-\nnents: attention and the FFN, which are connected\nvia a residual connection (He et al., 2016) and layer\nnormalization (Ba et al., 2016). In an encoder-\ndecoder model, there are two types of attention:\nself-attention and cross-attention. Self-attention is\nused in both the encoder and the decoder, allowing\nthe model to focus on relevant information within\nthe same sequence. Cross-attention is exclusive to\nthe decoder and allows it to attend to the encoder\u2019s\noutput. Attention takes as input a set of queries,\nkeys and values, projected using four Rdmodel\u00d7dmodel\nmatrices (one for the queries, keys, values, and\nfinal output) where dmodel is the model\u2019s hidden\ndimension. It then applies the SOFTMAX function\nto allow it to focus on the most relevant values.\nThe FFN is applied after attention on both the en-\ncoder and the decoder and consists of the following\n2-layer linear transformation:\nFFN(x) = max(0, xW1 + b1)W2 + b2,\n(1)\nwhere a RELU non-linearity is applied to the trans-\nformation of the input sequence (x).\nAt each\nlayer, the FFN is parameterized with two matrices,\nW1 \u2208 Rdmodel\u00d7dff and W2 \u2208 Rdff\u00d7dmodel where dff\nis the FFN dimension and is usually set to 4\u00d7dmodel\n(Vaswani et al., 2017).\nRecent work has drawn a significant link be-\ntween attention and the FFN (Geva et al., 2021),\nwherein W1 and W2 assume roles akin to the keys\nand values to an unnormalized attention where the\ninput (x) acts as the query. Unlike regular attention,\nthe FFN employs a RELU, which allows multiple\nkeys to significantly contribute to the final output\n(Geva et al., 2021). Additionally, these keys cor-\nrespond to an inventory of salient patterns that are\nlearned from the training data. Geva et al. (2021)\nsuggest that at the lower layers the FFN learns\nshallow syntactic patterns and progressively learns\ndeep semantic patterns on the deeper layers. More-\nover, the authors find that there\u2019s a substantial over-\nlap between patterns captured by adjacent layers,\nindicating that there are redundancies in the FFNs\nand suggesting a better allocation of these parame-\nters might be beneficial for performance.\n2.2\nSharing and Widening the FFN\nThe vanilla Transformer allocates one FFN for each\nlayer of the encoder and decoder, i.e. FFNenc\ni\nor\nFFNdec\ni\n, respectively. Excluding embedding pa-\nrameters, these FFNs occupy around two thirds\nof the parameter budget, while attention occupies\nthe remaining third1. Earlier work found that con-\nstraining the parameterization of the decoder FFNs\ncauses no degradation in accuracy (Ge et al., 2022).\nIn this work, we share the parameters of the FFN\nacross layers and/or across the encoder and decoder\nto minimize redundancy between FFNs.\nLet Nenc, Ndec be the numbers of encoder and\ndecoder layers, respectively. We consider multiple\nconfigurations for parameter sharing as follows:\n\u2022 One FFNenc\nall for the whole encoder:\nFFNenc\ni\n(\u00b7) tied\n= FFNenc\nall (\u00b7), \u2200i : 1 \u2264 i \u2264 Nenc\n\u2022 One FFNdec\nall for the whole decoder:\nFFNdec\nj\n(\u00b7) tied\n= FFNdec\nall (\u00b7), \u2200j : 1 \u2264 j \u2264 Ndec\n\u2022 One FFNencdec\nall\nfor both the encoder and the\ndecoder:\nFFNenc\ni\n(\u00b7) tied\n= FFNdec\nj\n(\u00b7) tied\n= FFNencdec\nall\n(\u00b7),\n\u2200i, j : 1 \u2264 i \u2264 Nenc, 1 \u2264 j \u2264 Ndec\nAdditionally, we explore modifying the dimen-\nsion of the shared FFN, which we denote as dff\u2032.\nSetting dff\u2032 > dff widens the shared FFN while\ndff\u2032 < dff narrows it. We also consider the extreme\ncases of setting dff\u2032 to 0 or to (Nenc + Ndec) \u00d7 dff\n(and beyond). Setting dff\u2032 = 0 is equivalent to\ndropping the FFN2 while setting dff\u2032 = (Nenc +\nNdec) \u00d7 dff is akin to sharing the concatenation of\nall individual FFNs.\nSharing the FFNs directly affects the number of\nparameters and, to a certain extent, latency. For\ninstance, sharing FFNenc\nall for the whole encoder re-\nduces the number of parameters by (Nenc \u2212 1) \u00d7\n2\u00d7dmodel \u00d7d\u2032\nff\n3; whereas removing the FFN on the\n1Ignoring layer normalization, there are 4\u00d7dmodel \u00d7dmodel\nparameters for attention vs 2\u00d7dmodel\u00d7dff = 8\u00d7dmodel\u00d7dmodel\nparameters for the FFN, assuming dff = 4 \u00d7 dmodel.\n2In our experiments without the FFN (i.e., dff\u2032 = 0) we\nremove the residual connection and layer normalization asso-\nciated with it, as they become redundant.\n3Plus the layer normalization parameters, which we are\nignoring for simplicity.\ndecoder, i.e., setting dff\u2032 = 0 for FFNdec\nall , reduces\nthe parameters by (Ndec) \u00d7 2 \u00d7 dmodel \u00d7 d\u2032\nff and re-\nduces the amount of computation to be done. This\nis particularly important during inference since the\nforward pass of the decoder is autoregressive, and\nchanging the decoder\u2019s FFN dimension has a higher\nlatency impact than on the encoder.\nSince different configurations have different im-\npacts, we analyse the trade-off between model size,\nlatency, and accuracy: (i) How many parameters\ncan be shared/pruned with negligible (if any) accu-\nracy degradation? (ii) Are the encoder and decoder\nFFNs affected similarly? (iii) Keeping the same\nmodel size, can the FFN parameters be allocated\nmore efficiently?\nWe propose a novel configuration, which we call\nthe One Wide FFN model, consisting of a single\nshared wide FFN on the encoder and no FFN on\nthe decoder. To keep the number of parameters\nthe same as in the baseline, we increase the shared\nFFN dimension accordingly: FFNenc\nall with dff\u2032 =\n(Nenc + Ndec) \u00d7 dff.\nFor completeness, we include similar experi-\nments on the attention mechanism in Appendix B.\nThese experiments show that, contrary to the FFN,\nindividual layer-specific attention weights are more\nimportant and not as redundant, as sharing the at-\ntention leads to significant accuracy drops.\n2.3\nRepresentational Similarity\nBesides investigating the impact on accuracy, we\nstudy the similarity between different models in\nterms of their internal representations and the se-\nmantic space they produce.\nWe use Linear Centered Kernel Alignment (CKA,\nKornblith et al., 2019) to measure the similarity be-\ntween the internal representations of different mod-\nels. CKA uses inner products to estimate how simi-\nlar the kernel matrices of two different representa-\ntions are, and is based on the Hilbert-Schmidt Inde-\npendence Criterion (HSIC, Gretton et al., 2005), a\nstatistical measure of independence of two random\nvariables. Linear CKA uses the dot product as a\nkernel and can be written as:\nCKA(A, B) =\n||ABT||2\nF\n||ATA||F||BTB||F\n,\nwhere || \u00b7 ||F is the Frobenius norm while A and\nB are mean-centered (i.e., we subtract the mean)\nfeature matrices of the layers under comparison,\ncomputed on the same dataset. Both matrices are\nn \u00d7 d, where n is the number of sentences in the\ndataset and d is the output dimension of the compo-\nnent, and are obtained by averaging the activation\nof all tokens in each sentence4. The linear kernel\nis straightforward to compute and Kornblith et al.,\n2019 report strong empirical performance of linear\nCKA compared to other kernels and methods.\nTo measure the similarity between the semantic\nspaces of different models, we use Local Neighbor-\nhood Similarity (LNS, Boggust et al., 2022). Lo-\ncal neighborhood similarities have been previously\nbeen used in analyzing semantic shifts in word em-\nbeddings (Hamilton et al., 2016). The premise of\nLNS is that two semantic spaces are similar if a\nsentence has similar neighbors in the two spaces.\nThe LNS of a sentence s between models 1 and 2\nis defined as:\nLNS(s) = Sim(k-NN1(s), k-NN2(s)),\nwhere k-NN(s) is the set of k nearest neighbors of\nsentence s for a model and Sim is the intersection-\nover-union (Jaccard similarity) of the two sets of\nneighbors. For each pair of components (attention\nand FFN) in models 1 and 2 we compute the LNS\nof all sentences in the evaluation dataset and take\nthe mean LNS as our layer similarity measure. The\nsmaller the value of k the more local the neigh-\nborhoods we are comparing, and the more specific\nthe retrieval task. We pick k to be small enough\nto visually inspect sentence neighborhoods if nec-\nessary. In our analysis, we use cosine distance as\nthe distance metric between activations and set k\nto 5% of the dataset size (\u223c 100 sentences).\n3\nExperimental Setup\nData\nIn our experiments, we show results on\nWMT22 English (EN) \u2192 German (DE) (296M\npairs), which we obtained using the provided\nmt-data scripts5, WMT16 EN \u2192 Romanian (RO)\n(610K pairs), and for the multilingual setup of Pires\net al. (2023), consisting of 10 languages: German,\nEnglish, Spanish, French, Italian, Japanese, Ko-\nrean, Portuguese, Swahili, and Chinese. In our\nanalysis, we mostly focus on WMT22 EN \u2192DE.\nFollowing Schmidt et al. (2022), we use\nWMT\u201916 provided scripts to normalize the RO side.\nEN \u2192RO keeps diacritics for producing accurate\ntranslations. For more details refer to Schmidt et al.\n4We use the source sentence and force decode the first\nreference to compute the encoder and decoder representations,\nrespectively.\n5https://www.statmt.org/wmt22/mtdata/\n(2022). For the multilingual experiments, we repli-\ncated the setup of Pires et al. (2023), which in-\ncludes all details, including data preprocessing and\ndataset sizes.\nMetrics\nWe compute BLEU6 using sacreBLEU7\nversion 2.3.1, with evaluation signatures nrefs:1\n| case:mixed | eff:no | tok:13a | smooth:exp\nfor BLEU, and nrefs:1 | case:mixed | eff:no |\ntok:flores101 | smooth:exp for SPBLEU. For\nour main results, we also report COMET using the\nwmt20-comet-da model and CHRF using the sig-\nnature nrefs:1 | case:mixed | eff:yes | nc:6\n| nw:0 | space:no.\nLatency\nWe report inference time in tokens/sec-\nond (the higher, the better), averaged over 5 runs.\nFor the multilingual models, we use the DE \u2192EN\ntest set. Our measurements were collected using\na single NVIDIA V100 GPU on a single-threaded\nIntel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz with\nbatch size of 1 and beam size of 5, in order to real-\nistically mimic the inference of a deployed model.\nFor experiments with larger batch sizes, see Ap-\npendix D.\nTokenization\nFor WMT22 EN \u2192DE, we use\nSENTENCEPIECE (Kudo and Richardson, 2018),\nwith a vocabulary size of 32K and a character cov-\nerage of 1.0, while for the multilingual experiments\nwe use a vocabulary size of 250k and a character\ncoverage of 0.9995. For WMT16 EN \u2192RO we\nuse byte-pair encoding (BPE, Sennrich et al., 2016)\nwith 40, 000 merge operations.\nModel Architectures\nWe focus our analysis on\nthe Transformer Big where Nenc = Ndec = 6,\ndmodel = 1024, dff = 4096, and it has 16 attention\nheads. We also report results on Transformer Base\n(Nenc = Ndec = 6, dmodel = 512, dff = 2048,\nand 8 attention heads), and a deep encoder shallow\ndecoder (Kasai et al., 2021) Transformer Big with\n12 encoder layers, and 2 decoder layers. For our\ndecoder-only experiments, the model is identical to\nthe Transformer Big, except that all 12 layers are on\nthe decoder. Our decoder-only model is similar to\na Transformer-based language model, particularly\nPrefix-LM (Raffel et al., 2020), where we apply a\nnon-autoregressive mask on the source side and an\nautoregressive mask on the target. The source and\n6For the multilingual experiments, we select the Flores101\ntokenizer in sacreBLEU, so technically we report SPBLEU.\n7https://github.com/mjpost/sacrebleu\ntarget embeddings and the output projection matrix\nare shared in all models (Press and Wolf, 2017).\nHyperparameters\nAll experiments are imple-\nmented using FAIRSEQ (Ott et al., 2019). Our\noptimizer is ADAM (Kingma and Ba, 2015) with\na learning rate of 0.0007. We train for 80k, 80k,\n150k steps on WMT22, WMT16, and multilingual,\nrespectively, at which point the models had con-\nverged. We use 4000 warm-up steps, and an inverse\nsquare root learning rate scheduler (Vaswani et al.,\n2017). We use a dropout rate of 0.1 for WMT22,\n0.3 for WMT16, and 0 for the multilingual experi-\nments due to the abundance of data, following Pires\net al. (2023). All models are trained using fp16\n(Ott et al., 2018).\nNomenclature\nIn our experiments, we run a num-\nber of different configurations per model architec-\nture that differ in the way the FFN is used, shared,\nor dropped, as well the size of the shared FFN (dff\u2032).\nTo facilitate our discussion, we introduce in Table 1\nthe nomenclature that will serve as reference for\nthe rest of the text. Unless otherwise stated, the\ndimension of the shared FNN\u2217\nall, i.e. dff\u2032 is equal to\nthe dff of the original model.\nFor decoder-only models, only SharedDec and\nNoDec configurations are defined.\nFor concise-\nness, we drop the mention of FFN from the\ntext when possible, i.e.\nSharedEnc instead of\nSharedEncFFN.\nFFN Description\nEncoder Decoder\nSharedEnc\nFNNenc\nall FNNdec\ni\nSharedDec\nFNNenc\ni\nFNNdec\nall\nSharedEncSharedDec FNNenc\nall FNNdec\nall\nSharedEncDec\nFNNencdec\nall\nNoDec\nFNNenc\ni\nNo-op\nSharedEncNoDec\nFNNenc\nall\nNo-op\nTable 1: Nomenclature used in our experiments. No-op\nindicates an identity function, which is equivalent to\ndropping the FFN.\nRepresentational\nSimilarity\nWe\nuse\nthe\nWMT22 EN \u2192DE evaluation set for both CKA and\nLNS analysis. We analyze encoder and decoder\nrepresentations independently and present these\nmetrics in a matrix heatmap plot showing pairwise\nsimilarity between layers. The diagonal of this\nmatrix is the similarity of corresponding layers,\ni.e., layer i on both architectures.\nIn order to\nfacilitate an \u201capples-to-apples\u201d comparison across\nmodels, we extract decoder representations by\nforce decoding the (first) reference. We establish\n2 crucial similarity scores:\na benchmark on\nsimilarity for each of these metrics, where we train\ntwo additional models using the same architecture\nbut with different random seeds; a similarity\nlower bound, where we compare the baseline\nTransformer Big with a randomly initialized (i.e.,\nuntrained) model with the same architecture. We\npresent these bounds in Appendix C.\n4\nExperimental Results\n4.1\nSharing FFNs\nThe results of various FFN sharing configurations\nare summarized in Table 2, including their impact\non accuracy and model size (in millions of param-\neters and percentage). Sharing either the encoder\n(SharedEnc) or the decoder FFN (SharedDec) re-\nsults in just a 0.2 to 0.3 BLEU point decrease, while\nreducing the parameter count by nearly 20%. Shar-\ning the FFN on each side (ShareEncShareDec)\nleads to a more substantial degradation of 0.9\nBLEU points, albeit reducing the parameter count\nby 37%, while sharing a single FFN on the encoder\nand decoder (ShareEncDec) results in a slightly\nhigher degradation of 1.1 BLEU points. Nonethe-\nless, these findings support the hypothesis that the\nFFN contains some degree of redundancy, as we\nexpected a greater accuracy degradation given the\nsubstantial (20 \u2212 40%) reduction in model size.\nArchitecture\nBLEU | \u03b8 | (%)\nTransformer Big\n35.6 228M (100)\n+ SharedEnc\n35.4 186M (82)\n+ SharedDec\n35.3 186M (82)\n+ SharedEncSharedDec 34.7 144M (63)\n+ SharedEncDec\n34.5 136M (59)\nTable 2: sacreBLEU results on WMT 22 EN \u2192DE for\ndifferent FFN sharing configurations. | \u03b8 | is the number\nof parameters.\nWhile we focus on sharing one FFN for all layers\nwithin a module, we compare with sharing multi-\nple FFNs following Takase and Kiyono (2023) in\nAppendix A. We find that sharing one FFN is as\naccurate as sharing multiple FFNs within a module,\nwhile being more parameter-efficient.\n4.2\nDropping FFNs\nTable 3 summarizes the performance of models\nwith no FFNs. Besides BLEU and number of pa-\nrameters, we report the inference speed for each\narchitecture. Dropping the FFN on the encoder\n(NoEnc) leads to a 0.9 BLEU point drop while re-\nducing the parameter count by 22% and with mini-\nmal effect on inference speed. Dropping the FFN\non the decoder (NoDec), on the other hand, causes\na degradation of only 0.4 BLEU points while in-\ncreasing the inference speed by 20%8. The highest\nlatency reduction is obtained by removing the FFNs\non both the encoder and the decoder (NoEncNoDec),\nbut it comes with a significantly larger degradation\nof over 2 BLEU points.\nArchitecture\nBLEU Speed\n| \u03b8 | (%)\nTransformer Big\n35.6 111\u00b11.2 228M (100)\n+ NoEnc\n34.7 112\u00b11.0 178M (78)\n+ NoDec\n35.2 133\u00b10.9 178M (78)\n+ NoEncNoDec\n33.5 138\u00b11.9 127M (56)\n+ SharedEncNoDec 35.3 136\u00b11.1 136M (60)\n+ NoEncSharedDec 33.9 127\u00b11.0 136M (60)\nTable 3: sacreBLEU results on WMT 22 EN \u2192DE for\ndifferent FFN dropping configurations.\nCombining sharing and dropping\nThese re-\nsults, together with those from Table 2, suggest that\nthe encoder and decoder FFNs have different con-\ntributions: the decoder\u2019s are more redundant, cor-\nroborating previous work on FFNs parametrization\n(Ge et al., 2022). With this in mind, we experiment\nwith one shared FFN on the encoder and dropping\nit on the decoder, reported as SharedEncNoDec in\nTable 3. As shown, with just approximately 60%\nof Transformer Big parameters we observe a 22%\nimprovement in inference speed, at the cost of 0.3\nBLEU point.\n4.3\nOne Wide FFN Model\nPrevious sections describe models that share and/or\ndrop FFNs, effectively reducing model size at some\nmodest accuracy cost. In this section, we investi-\ngate whether we can regain the accuracy lost while\npreserving the parameter efficiency and the latency\nreduction. We focus on ShareEncNoDec model as\n8The reason for this difference between NoEnc and NoDec\nis that the encoder output is computed in parallel, while the\ndecoder operates in a step-by-step fashion.\nBLEU\nCHRF\nCOMET\nSpeed\n| \u03b8 | (%)\nTransformer Big EN \u2192DE\n35.6\n62.6\n57.2\n110.8\u00b11.2\n228M (100)\n+ SharedEncNoDec FFN dff\u2032 = 4, 096\n35.3\n62.1\n56.1\n135.7\u00b11.1\n135M\n(60)\n+ SharedEncNoDec FFN dff\u2032 = 24, 576\n35.7\n62.7\n57.9\n138.2\u00b10.9\n177M\n(80)\n+ SharedEncNoDec FFN dff\u2032 = 49, 152\n36.5\u2020\n63.2\u2020\n59.6\n137.5\u00b11.6\n228M (100)\n+ SharedEncNoDec FFN dff\u2032 = 98, 304\n36.4\u2020\n63.2\u2020\n59.0\n134.5\u00b11.6\n328M (145)\nTable 4: Accuracy of One Wide FFN for Transformer Big EN \u2192DE on WMT22. \u2020 implies the system is statistical\nsignificantly different at p < 0.05.\nit provides a strong baseline with significant param-\neter savings and inference speedups.\nWe propose increasing the dimension of the\nshared FFN to match the number of parameters\nof the original (fully-parameterized) model, so as\nto avoid increasing the overhead of model stor-\nage. In particular, ShareEncNoDec saves around\n(Nenc + Ndec \u2212 1) \u00d7 2 \u00d7 dmodel \u00d7 dff parame-\nters as there\u2019s one single shared FFN in the en-\ncoder. On the other hand, the Transformer Big\nhas (Nenc + Ndec) FFNs. Thus, we match the size\nof the original model by setting the dimension of\nthe shared FFN, dff\u2032, to (Nenc + Ndec) \u00d7 dff.\nTable 4 summarizes our results. It includes our\nproposed model, the One Wide FFN model (dff\u2032 =\n49, 152), as well as the baseline Transformer Big,\nand the corresponding ShareEncNoDec (dff\u2032 =\n4, 096). It also includes a wide model with dff\u2032 =\n24, 576, which uses the same number of parame-\nters as NoDec, with dff\u2032 = Nenc \u00d7 dff. This model\nachieves an accuracy on par (or slightly above) the\nbaseline Transformer Big with 20% fewer param-\neters and a significant inference speed-up.\nOur proposed model with dff\u2032 = 49, 152 goes\nbeyond that, achieving a gain of 1.2 BLEU points\nover the vanilla ShareEncNoDec and 0.9 BLEU\npoints over the Transformer Big. These gains\nremain consistent across CHRF and COMET. Fur-\nthermore, it has a similar inference speed as the\nShareEncNoDec model. For completeness, we in-\nclude a wider model with dff\u2032 = 98, 304. Despite\nthe extra capacity, this model does not provide any\nadditional accuracy gains, which we suspect is due\nto the lack of data to train such a large model.\n4.4\nAnalyzing Internal Representations\nWe now report a post-hoc analysis of the internal\nrepresentations of the models introduced in pre-\nceding sections. Our objectives are twofold: 1) to\nascertain whether the proposed models\u2019 internal\nrepresentations exhibit a significant degree of sim-\nilarity to those of the original base model; 2) to\ndelve into the impact of the proposed methods on\nredundancy. We adopt the definition of redundancy\nof Dalvi et al. (2020), who visually inspect the sim-\nilarity between adjacent modules within a model\n(high similarity entails high redundancy).\nArchitecture\nEncoder\nDecoder\nCKA\nLNS\nCKA\nLNS\nBenchmark\n100.0 100.0 100.0 100.0\nSharedEnc\n98.0 96.2 100.8 100.6\nSharedDec\n100.2 101.4 98.3 94.6\nSharedEncSharedDec\n98.9 97.2 99.5 95.4\nSharedEncDec\n97.6 94.4 98.4 93.5\nNoEnc\n90.0 70.5 101.0 96.8\nNoDec\n100.0 98.6 96.0 87.4\nSharedEncNoDec\n97.6 98.9 97.5 89.0\nSharedEncNoDecd\u2032\nff=49152 97.0 83.2 94.0 82.9\nTable 5: Similarity of the representations (%) of cor-\nresponding modules of different architectures vs. the\nTransformer Big for WMT22 EN \u2192DE. These scores\nare normalized by comparing them to the CKA and LNS\nbenchmark scores. For NoDec configurations we com-\npare the final output of the Transformer layer as a whole\nas they have different modules than the baseline. The\ncolumns for shared and for dropped FFNs are high-\nlighted in gray and blue respectively.\n4.4.1\nSimilarity to Baseline\nWe ground the pairwise similarity metrics, by nor-\nmalizing them against a benchmark. As mentioned\nin Section 3, we establish the benchmark scores by\ntraining two additional Transformer Big models,\nbut using different random seeds. These models\nachieve similar accuracy as the baseline model (see\nAppendix C.1 for more details). The benchmark\nscore is the similarity between the baseline and\nthese models Because the benchmark is calculated\nby averaging similarity scores from different train-\n(a) Encoder self similarity.\n(b) Decoder self similarity.\nFigure 1: CKA self similarity of encoder and decoder layers of the One Wide Encoder model vs. the Transformer\nBig baseline. We identify each component with a label: index.name. For example, 0.sa refers to the self-attention\non layer 0, while 4.ca refers to the cross-attention on layer 4.\ning runs of our baseline, individual runs can have a\nnormalized score above 100%.\nTable 5 shows normalized similarity scores for\nseveral models. Under the Encoder columns we\ncompare the encoder representations, and under the\nDecoder columns we compare decoder represen-\ntations. Sharing FFNs leads to consistenly lower\n(normalized) similarity scores than models that do\nnot share, both in terms of internal representation\n(CKA) and semantic spaces (LNS). As shown, al-\nthough models that share FFNs have lower sim-\nilarity scores compared to those that do not, the\nscores are still very close to 100%. Moreover, these\ndecreases align with the drops in BLEU seen in\nTable 2, where the model with the lowest similar-\nity score (ShareEncDec) is also the least accurate\nmodel. We observe a similar trend for models that\ndrop the FFNs in the encoder or decoder, these\nmodels exhibit lower similarity scores with the re-\nspective component than models sharing them, as\nshown by NoEnc and NoDec. In addition, the former\nresult again suggests the FFNs in the encoder are\nmore important than in the decoder as the similarity\nshifts drastically compared to all other settings.\nFor completeness, we report on the last row the\nsimilarity scores for the One Wide FFN model,\nwhich is more accurate than the base model. The\ninternal representations generated by that model\ndiverge from those of the base model. Interestingly,\nwe observe a larger drop in LNS scores than in CKA\nscores, indicating that the shift occurs mostly in\nsemantic space, rather than the Euclidean space\ncaptured by CKA. For a detailed layer-wise similar-\nity analysis that breaks out the aggregate analysis\nin Table 5 see Appendix C.2.\n4.4.2\nA Qualitative View of Redundancy\nWe now study into the impact of our One Wide\nFFN model on the redundancy of the internal repre-\nsentations. In addition to adopting their definition\nof redundancy, we also adopt Dalvi et al. (2020)\u2019s\nmethod of computing self-similarity, namely look-\ning at how the representations change as they go\nthrough each module (self-attention, FFN, or cross-\nattention) of the model. In particular, we use CKA\nto compute similarity between the output of differ-\nent modules within the same model.\nIn Figure 1a, we show the CKA self-similarity\nmatrices for the encoders of the One Wide FFN\nmodel and the Transformer Big. We do the same\nfor the decoders in Figure 1b. These matrices show\nhow similar each module of the network is to all\nother modules within that network. The diagonal\nof the matrix is the similarity between a module\nand itself and is always 1.\nAs shown, there is high similarity between ad-\njacent modules of the Transformer Big, both on\nthe encoder and decoder, indicated by areas with\ndarker red around the diagonal. The prevalence of\nhigh similarity patterns among adjacent modules\nsuggests a substantial degree of redundancy, and\neliminating a module has a negligible impact on\nthe final representations. On the other hand, we\nobserve a distinct checkerboard pattern on the self-\nsimilarity matrices of the One Wide FFN model,\nwhere individual modules tend to exhibit lower sim-\nilarity with their immediate neighbors than with\ntheir second neighbors (i.e., the neighbors of the\nneighbors). On the encoder, the checkerboard pat-\ntern emerges especially in the earlier modules while\non the decoder, that pattern appears more consis-\ntently throughout the layers. This pattern gives an\nindication that our model is learning non-trivial\ntransformations of the input, leading to decreased\nredundancy within the network.\n4.5\nOther architectures and Languages\nSo far, all our experiments focused on the Trans-\nformer Big and on WMT22 EN \u2192DE. In this\nsection, we apply what we learned to other archi-\ntectures and language pairs. We run experiments\non the low resource language direction EN \u2192RO\nand a large scale multilingual model.\nFor EN \u2192DE, we apply our proposal to a Trans-\nformer Base model, a Deep Encoder Shallow De-\ncoder model (Kasai et al., 2021), and a Decoder-\nOnly model. For the Transformer Base, we observe\nan accuracy gain of 0.5 BLEU (2.2 BLEU over the\nvanilla SharedEncNoDec model) and an inference\nspeedup of around 25%. In the Deep Encoder Shal-\nlow Decoder model, we observe a more modest\naccuracy gain of 0.2 BLEU points (0.9 BLEU over\nthe vanilla SharedEncNoDec model). However, the\ninference speedup from dropping the decoder FFNs\nis minimal (< 1%), which is expected because of\nthe small depth of the decoder in this architecture.\nDecoder-only models\nWith the advent of Large\nLanguage Models (LLMs) like GPT (Brown et al.,\n2020), and PaLM (Chowdhery et al., 2022), a lot\nof effort has been put on decoder-only Transformer\nmodels. We train a decoder-only model on WMT22\nEN \u2192DE, as shown on Table 6. Due to the ab-\nsence of an encoder, we are limited to applying\na wide FFN on the decoder side. As in the other\nsetups, we get an accuracy gain of +0.3 BLEU over\nthe baseline decoder-only model (+1.7 BLEU over\nShareDec), but the latency degrades by 12%. This\nis not surprising: due to the autoregressive nature\nof the decoder, increasing the size of its FFN has a\nbigger impact on speed.\nLow-resource languages\nIn EN \u2192RO the ac-\ncuracy of the One Wide FFN Model is only on\npar compared to the base model, even though it is\na higher than the vanilla SharedEncNoDec model.\nWe hypothesize that due to the low resource condi-\ntion, our proposed model already reaches saturation\nas there are not that many salient textual patterns\nto be learned by the FFN.\nMultilingual\nFinally, we observe the similar\ntrend on the multilingual setup, where the One\nWide FFN Model is +1.2 SPBLEU points more ac-\ncurate than the baseline Transformer Big and +2.5\nSPBLEU points more accurate than the vanilla\nSharedEncNoDec, this gain is significant in 79 out\nof 90 directions and when all tests sets are con-\ncatenated. Additionally, this large accuracy gain\nalso comes with around 18% inference speed-up,\nconsistent with our previous results.\n5\nRelated Work\nWeight pruning and parameter sharing are well-\nknown techniques to reduce a model\u2019s footprint.\nGiven the scale of the latest models (Chowdhery\net al., 2022), there have been multiple efforts to\nprune neurons based on different automatic meth-\nods (Dalvi et al., 2020; Michel et al., 2019; Voita\net al., 2019), sharing parameters efficiently (Ge\net al., 2022; Reid et al., 2021), and factorizing cer-\ntain components (Lan et al., 2020; Hu et al., 2022).\nNeuron pruning methods often focus on finding\nand pruning redundant neurons through correlation\nmethods (Dalvi et al., 2020), but also on how Trans-\nformer components like the multi-head attention\ncan be pruned significantly due to model redun-\ndancy in the encoder or decoder either by checking\nthe gradients salience (Michel et al., 2019) or a\ndifferentiable relaxation of the l0 regularization at\ntraining time (Voita et al., 2019).\nFor parameter sharing, the Universal Trans-\nformer (Dehghani et al., 2019) proposed a model\nwhere all layers are shared (i.e., in effect it reduced\nthe model to a single shared layer). Takase and\nKiyono (2023) proposes finding an optimal config-\nuration of shared layers in the encoder or decoder\nthrough different methods of sharing (in sequence,\nin cycle, or in reversed cycle) always keeping a\nspecified number of final layers9. Similarly, Reid\net al. (2021) proposes an approach where just the\nmiddle layers are shared, while the bottom and top\nlayers are independent, and using a lower dimen-\nsionality for the embedding layer. Analogously, Ge\net al. (2022) focus on minimizing the number of\nparameters and the number of calls to each param-\neters\u2019 group in order to optimise on-device models.\nThey achieve this by sharing the encoder and de-\ncoder in a similar way to both previous methods,\nparticularly by sharing all layer parameters in cycle\nlike Takase and Kiyono (2023).\nPrevious works also focus on reducing the di-\nmensionality of certain parameters, mostly through\nlow rank factorization. Lan et al. (2020) decom-\nposes the embedding layer into a lower rank em-\n9See Appendix A for a detailed description and compari-\nson.\nBLEU\nCHRF\nCOMET\nSpeed\n| \u03b8 | (%)\nTransformer Base EN \u2192DE\n34.2\n61.6\n54.1\n116.3\u00b10.9\n70M (100)\n+ SharedEncNoDec FFN dff\u2032 = 2, 048\n32.5\u2020\n60.1\u2020\n50.0\n146.0\u00b11.6\n47M\n(67)\n+ SharedEncNoDec FFN dff\u2032 = 24, 576\n34.7\n61.8\n55.6\n146.8\u00b11.3\n70M (100)\nTransformer Decoder-Only EN \u2192DE\n35.8\n62.8\n57.7\n79.8\u00b11.9\n202M (100)\n+ ShareDec FFN dff\u2032 = 4, 096\n34.4\u2020\n61.7\u2020\n54.1\n79.7\u00b11.3\n110M\n(48)\n+ ShareDec FFN dff\u2032 = 49, 152\n36.1\n62.9\n59.4\n69.3\u00b10.2\n202M (100)\nTransformer Deep Enc. Shallow Dec. EN \u2192DE 35.5\n62.4\n58.0\n230.1\u00b10.8\n236M (100)\n+ ShareEncNoDec FFN dff\u2032 = 4, 096\n34.8\u2020\n61.6\u2020\n55.4\n235.0\u00b10.5\n127M\n(54)\n+ ShareEncNoDec FFN dff\u2032 = 57, 344\n35.7\n62.4\n58.9\n233.5\u00b10.7\n236M (100)\nTransformer Base EN \u2192RO\n22.9\n52.9\n50.9\n119.3\u00b11.1\n64M (100)\n+ SharedEncNoDec FFN dff\u2032 = 2, 048\n22.2\u2020\n52.5\u2020\n45.8\n152.8\u00b11.4\n41M\n(64)\n+ SharedEncNoDec FFN dff\u2032 = 24, 576\n22.9\n52.8\n46.7\n150.6\u00b10.5\n64M (100)\nTransformer Big Multilingual\n26.8\n46.3\n47.7\n94.6\u00b11.6\n422M (100)\n+ SharedEncNoDec FFN dff\u2032 = 4, 096\n25.5\u2020\n45.1\u2020\n40.8\n107.1\u00b11.4\n330M\n(78)\n+ SharedEncNoDec FFN dff\u2032 = 49, 152\n28.0\u2020\n47.3\u2020\n50.7\n111.5\u00b11.1\n422M (100)\nTable 6: Accuracy of One Wide FFN for EN \u2192DE with Transformer Base, Decoder Only, and Deep Encoder\nShallow Decoder on WMT22; for low resource EN \u2192RO with Base version on WMT16, and multilingual with\nTransformer big on Flores. \u2020 implies the system is statistical significantly different at p < 0.05.\nbedding matrix and a projection to the actual hid-\nden size while also sharing all parameters across\nall layers. In addition to sharing parameters ef-\nficiently, Ge et al. (2022) proposes a lightweight\ndecomposition of the FFN where instead of a single\ncomponent there are 2 projections with a smaller di-\nmensionality than vanilla Transformers. Our work\nis close to Ge et al. (2022) but instead of factoriz-\ning we explore sharing and full pruning of the FFN.\nIn contrast with previous works, we also explore\nincreasing the encoder FFN size while dropping\nthe decoder\u2019s completely.\n6\nConclusion\nIn this work, we studied the importance of the FFN\nin Transformer models. We analyzed the impact of\nremoving and/or sharing the FFN across layers and\nfound that, due to this component\u2019s redundancy, the\nmodel sizes can be substantially reduced with little\nimpact on accuracy for Machine Translation. In\nparticular, we found that sharing the FFN across all\nencoder layers while making it larger and removing\nit from the decoder layers leads to models that are\nmore accurate and faster at inference.\nOur findings are applicable across multiple set-\ntings, including decoder-only and multilingual\nmodels. In a low-resource setting the results are\nmodest but our approach can still recover the base-\nline\u2019s performance with a faster inference.\nFinally, we conducted a thorough similarity anal-\nysis between the vanilla Transformer and our pro-\nposed architectures, and found that the latter\u2019s inter-\nnal representations do not differ significantly from\nthe former\u2019s, except in that they are less redundant.\nLimitations\nIn this work, our focus was Machine Translation.\nAlthough we expect the results to generalize to\nother sequence-to-sequence tasks, further experi-\nments are needed, which we leave for future work.\nEthics Statement\nOne important consideration is the energy con-\nsumption for model training, which results in green-\nhouse emissions (Strubell et al., 2019). Our work\nuses existing datasets, and inherits some of the\nrisks associated with them, such as privacy leakage\n(Carlini et al., 2021) and gender bias (Cho et al.,\n2019). Mitigation strategies such as those from\nVanmassenhove et al. (2018) may be necessary.\nAcknowledgements\nWe would like to thank Robin Schmidt, Matthias\nSperber, and Stephan Peitz for their feedback and\nsupport in reviewing this work.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016.\nLayer normalization.\narXiv preprint\narXiv:1607.06450.\nYamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao\nZhang, Colin Cherry, Behnam Neyshabur, and Orhan\nFirat. 2022. Data scaling laws in NMT: The effect\nof noise and architecture. In Proceedings of the 39th\nInternational Conference on Machine Learning, vol-\nume 162 of Proceedings of Machine Learning Re-\nsearch, pages 1466\u20131482. PMLR.\nAngie Boggust, Brandon Carter, and Arvind Satya-\nnarayan. 2022. Embedding Comparator: Visualizing\nDifferences in Global Structure and Local Neighbor-\nhoods via Small Multiples. In 27th International\nConference on Intelligent User Interfaces, pages 746\u2013\n766, Helsinki Finland. ACM.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nNicholas Carlini,\nFlorian Tram\u00e8r,\nEric Wallace,\nMatthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar\nErlingsson, Alina Oprea, and Colin Raffel. 2021. Ex-\ntracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pages 2633\u20132650. USENIX Association.\nWon Ik Cho, Ji Won Kim, Seok Min Kim, and Nam Soo\nKim. 2019. On measuring gender bias in translation\nof gender-neutral pronouns. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 173\u2013181, Florence, Italy. Associa-\ntion for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT\u2019s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276\u2013286, Florence, Italy. Association for Com-\nputational Linguistics.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and\nYonatan Belinkov. 2020. Analyzing redundancy in\npretrained transformer models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4908\u20134926,\nOnline. Association for Computational Linguistics.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Universal\ntransformers. In International Conference on Learn-\ning Representations.\nTao Ge, Si-Qing Chen, and Furu Wei. 2022. Edge-\nFormer: A parameter-efficient transformer for on-\ndevice seq2seq generation. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 10786\u201310798, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484\u20135495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nBehrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur\nBapna, Maxim Krikun, Xavier Garcia, Ciprian\nChelba, and Colin Cherry. 2022. Scaling laws for\nneural machine translation. In International Confer-\nence on Learning Representations.\nMitchell A Gordon, Kevin Duh, and Jared Kaplan. 2021.\nData and parameter scaling laws for neural machine\ntranslation. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 5915\u20135922, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nArthur Gretton, Olivier Bousquet, Alex Smola, and\nBernhard Sch\u00f6lkopf. 2005.\nMeasuring statistical\ndependence with hilbert-schmidt norms. In Inter-\nnational conference on algorithmic learning theory,\npages 63\u201377. Springer.\nWilliam L. Hamilton, Jure Leskovec, and Dan Juraf-\nsky. 2016. Cultural shift or linguistic drift? compar-\ning two computational measures of semantic change.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2116\u20132121, Austin, Texas. Association for Computa-\ntional Linguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 770\u2013778.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,\nand Noah A. Smith. 2021.\nDeep encoder, shal-\nlow decoder: Reevaluating non-autoregressive ma-\nchine translation. In 9th International Conference\non Learning Representations, ICLR, virtual. OpenRe-\nview.net.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization.\nIn 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee,\nand Geoffrey Hinton. 2019. Similarity of neural net-\nwork representations revisited. In Proceedings of\nthe 36th International Conference on Machine Learn-\ning, volume 97 of Proceedings of Machine Learning\nResearch, pages 3519\u20133529. PMLR.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66\u201371, Brussels, Belgium.\nAssociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nQian Lou, Ting Hua, Yen-Chang Hsu, Yilin Shen, and\nHongxia Jin. 2022. Dictformer: Tiny transformer\nwith shared dictionary. In International Conference\non Learning Representations.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one?\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48\u201353, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nMyle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. 2018. Scaling neural machine translation. In\nProceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 1\u20139, Brussels,\nBelgium. Association for Computational Linguistics.\nTelmo Pires, Robin Schmidt, Yi-Hsiu Liao, and Stephan\nPeitz. 2023. Learning language-specific layers for\nmultilingual machine translation. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 14767\u201314783, Toronto, Canada. Association\nfor Computational Linguistics.\nOfir Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 157\u2013163, Valencia, Spain.\nAssociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1\u201367.\nMachel Reid, Edison Marrese-Taylor, and Yutaka Mat-\nsuo. 2021. Subformer: Exploring weight sharing\nfor parameter efficiency in generative transformers.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 4081\u20134090, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nRobin Schmidt, Telmo Pires, Stephan Peitz, and Jonas\nL\u00f6\u00f6f. 2022.\nNon-autoregressive neural machine\ntranslation: A call for clarity. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2785\u20132799, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715\u20131725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645\u20133650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nSho Takase and Shun Kiyono. 2023. Lessons on pa-\nrameter sharing across layers in transformers. In\nProceedings of The Fourth Workshop on Simple and\nEfficient Natural Language Processing (SustaiNLP),\npages 78\u201390, Toronto, Canada (Hybrid). Association\nfor Computational Linguistics.\nEva Vanmassenhove, Christian Hardmeier, and Andy\nWay. 2018. Getting gender right in neural machine\ntranslation. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 3003\u20133008, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nJesse Vig and Yonatan Belinkov. 2019.\nAnalyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63\u201376, Florence, Italy. As-\nsociation for Computational Linguistics.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797\u20135808, Florence, Italy.\nAssociation for Computational Linguistics.\nA\nCustom Sharing of Multiple FFNs\nThere is a combinatorial number of ways of sharing\nM < N FFNs within a module of N layers. Since\nthis is prohibitive, we investigate the following\nstrategies from Takase and Kiyono (2023):\n\u2022 Sequence: assign one FFN for every M/N\nconsecutive layers, forming a block pattern.\nFFNi(\u00b7)\ntied\n=\nFFNseqm(\u00b7), \u2200i : 1 \u2264 i \u2264\nN, m = \u230a(i \u2212 1)/(N/M)\u230b\n\u2022 Cycle: stack M FFNs in an identical order,\nforming a repetitive checkerboard pattern.\nFFNi(\u00b7)\ntied\n=\nFFNcycm(\u00b7), \u2200i : 1 \u2264 i \u2264\nN, m = (i \u2212 1) modulo M\n\u2022 Cycle (Rev): stack M FFNs in a reverse\norder, forming a repetitive palindrome series.\nFFNi(\u00b7) tied\n= FFNcycrevm(\u00b7), \u2200i : 1 \u2264 i \u2264\nN, m = N/M \u2212 i\nNote that we assume that N is an even number\nand divisible by N. Cycle (Rev) is only valid\nfor M = N/2. The EdgeFormer (Ge et al., 2022)\nadopts Cycle with M = 2 for the encoder FFNs.\nTable 7 shows the results of these strategies ap-\nplied on the encoder. As references, we copy the re-\nsults of the Transformer Big and ShareEnc from\nTable 2. Not only is the accuracy of ShareEnc sim-\nilar to Takase and Kiyono (2023)\u2019s strategies, but it\nalso uses fewer parameters and is easier to extend.\nArchitecture\nBLEU | \u03b8 |\n(%)\nTransformer Big\n35.6 228M (100)\n+ SharedEnc (M=1) 35.4 186M (82)\n+ Sequence M=2\n35.2 194M (85)\n+ Sequence M=3\n35.3 202M (88)\n+ Cycle M=2\n35.2 194M (85)\n+ Cycle M=3\n35.5 202M (88)\n+ Cycle Rev M=2\n35.2 194M (85)\n+ Cycle Rev M=3\n35.5 202M (88)\nTable 7: Accuracy of different FFN sharing strategies\non WMT22 EN \u2192 DE.\nB\nSharing or Dropping Attention\nWe report the results of sharing attention modules\n(either self, cross or both) across layers in Table 8.\nIn contrast with the FFN, attention seems to play\na more crucial role in the model\u2019s performance, as\nsharing the different attention mechanisms in both\nencoder and decoder causes a large accuracy drop\nacross all settings, with the exception of sharing\nthe decoder\u2019s cross attention and the encoder\u2019s self\nattention.\nEncoder\nDecoder\nBLEU | \u03b8 | (%)\nSelf-Att Self-Att Cross-Att\nTransformer Big\n35.6 228M(100)\nShared Shared\nShared\n27.5 165M (72)\nShared Shared\nIndiv.\n27.6 186M (82)\nShared\nIndiv.\nIndiv.\n35.5 207M (91)\nIndiv.\nShared\nIndiv.\n26.5 207M (91)\nIndiv.\nShared\nShared\n25.7 186M (82)\nIndiv.\nIndiv.\nShared\n35.5 207M (91)\nTable 8: BLEU scores on WMT 22 EN \u2192DE when\nsharing the attention of both encoder and decoder (self\nand cross). Nomenclature follows Section 3 but with\nSelf Attn an Cross Attn as the encoder/decoder\u2019s self\nattention and cross-attention (decoder), respectively.\nC\nDetails on Internal Representations\nAnalysis\nC.1\nRaw Similarity Scores for Benchmarking\nWe establish a benchmark score for the expected\nsimilarity of our two metrics by comparing the\nbaseline Transformer Big with identical models\ntrained from different random seeds.\nTable 9\npresents the raw similarity scores from which we\ncompute the normalized scores presented in Table 5.\nAs shown, the similarity between\nArchitecture\nEncoder Decoder\nCKA LNS CKA LNS\nTransformerBig Seed 2 .96 .61 .94 .62\nTransformerBig Seed 3 .96 .62 .95 .62\nSharedEnc\n.94 .58 .95 .62\nSharedDec\n.97 .62 .93 .59\nSharedEncSharedDec\n.95 .59 .94 .59\nSharedEncDec\n.94 .57 .93 .58\nNoEnc\n.87 .43 .95 .60\nNoDec\n.96 .60 .90 .54\nShareEncNoDec\n.94 .59 .92 .55\nShareEncNoDecd\u2032\nff=41952 .94 .51 .89 .51\nTable 9: Raw similarity of the representations of cor-\nresponding layer-modules of different architectures vs.\nthe Transformer Big for WMT22 EN \u2192DE. For NoDec\nconfigurations we compare the final output of the trans-\nformer layer as a whole as they have different sub-\nmodules. The columns for shared and for dropped FFNs\nare highlighted in gray and blue respectively.\nC.2\nLayer-wise Analysis\nIn Table 5, we report the aggregated similarity\nscores across all layers of Transformer encoder\nand decoder. Here, we report a more fine-grained\nlayer-wise similarity score mostly to showcase the\nreliability of the aggregated scores. In Figure 2,\nwe plot layerwise LNS to study how similar the\nsemantic information captured at each layer is to\nthat of the baseline model at every layer. When\nLNS scores are high, the network is producing sim-\nilar local neighborhoods for each sentence in our\nevaluation set. In particular, we are interested in\ncomparing the benchmark LNS scores and those of\nSharedEncSharedDec at each layer. As shown, the\nlayer-wise LNS scores of SharedEncSharedDec\ntrack the baseline scores at almost every layer, con-\nfirming the reliability of the aggregated score. We\nobserve similar pattern for all the models that we\nevaluate in this paper.\n0.ffn\n0.self-attn\n1.ffn\n1.self-attn\n2.ffn\n2.self-attn\n3.ffn\n3.self-attn\n4.ffn\n4.self-attn\n5.ffn\n5.self-attn\nLayer.Module\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLNS\nShared Encoder + Shared Decoder\nTransformer Big Benchmark\nModel\n(a) Encoder\n0.cross-attn\n0.ffn\n0.self-attn\n1.cross-attn\n1.ffn\n1.self-attn\n2.cross-attn\n2.ffn\n2.self-attn\n3.cross-attn\n3.ffn\n3.self-attn\n4.cross-attn\n4.ffn\n4.self-attn\n5.cross-attn\n5.ffn\n5.self-attn\nLayer.Module\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLNS\nShared Encoder + Shared Decoder\nTransformer Big Benchmark\nModel\n(b) decoder\nFigure\n2:\nLayerwise\nLNS\nbetween\nSharedEncSharedDec\nand\nTransformer\nBig\n(blue bars). LNS between two versions of Transformer\nBig trained from different random initializations are\nshown by the grey bars to ground the comparison.\nFFN sharing does not dramatically change activations\nproduced at each layer.\nD\nEffect of batch size on decoding speed\nIn Section 4.3, we compared the decoding speeds of\nthe One Wide FFN model and the Transformer Big,\nwith a batch size of 1. In Table 10, we delve into\nhow the decoding speed evolves as the batch size\nincreases. As shown, the One Wide FFN model\nis faster for smaller batch sizes, but its advantage\ndiminishes as the batch size increases, being slower\nthan the Transformer Big for large batch sizes. We\nsuspect this slowdown is due to the fact that the\n| Batch |\nTransformer Big\nOne Wide FFN\nSpeed-up (%)\n# batches\n1\n110.8\u00b11.2\n137.5\u00b11.1\n24\n2, 047\n2\n221.7\u00b114.3\n260.9\u00b16.5\n18\n1, 024\n4\n397.4\u00b18.0\n448.9\u00b12.0\n13\n512\n8\n718.3\u00b18.0\n748.7\u00b110.6\n4\n256\n16\n1, 220.7\u00b156.2\n1, 226.9\u00b117.2\n1\n128\n32\n1, 958.5\u00b1112.4\n1, 837.6\u00b115.3\n\u22126\n64\n64\n1, 319.1\u00b136.7\n1, 259.0\u00b170.0\n\u22125\n32\n128\n1, 925.1\u00b164.8\n1, 705.0\u00b162.3\n\u221211\n16\n256\n2, 312.1\u00b167.4\n1, 976.5\u00b1123.2\n\u221215\n8\n512\n2, 512.0\u00b150.1\n1, 957.9\u00b132.6\n\u221222\n4\nTable 10: Effect of batch size on decoding speed (in tokens/s) for the Transformer Big and One Wide FFN\n(dff\u2032 = 49, 152). \u2206 is the percentage change in inference speed, and # batches is the number of batches used to\nevaluate. For large batch sizes, there are fewer batches (since the dataset size is fixed), which leads to higher\nvariance in the measurements.\nlarge FFN size requires higher peak memory, mak-\ning the larger sizes non-optimal for this model.\n"
  },
  {
    "title": "AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections",
    "link": "https://arxiv.org/pdf/2309.02186.pdf",
    "upvote": "19",
    "text": "AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections\nYue Wu1*\nSicheng Xu2 \u2217\nJianfeng Xiang3,2\nFangyun Wei 2\nQifeng Chen1\nJiaolong Yang2\u2020\nXin Tong2\n1HKUST\n2Microsoft Research Asia\n3Tsinghua University\nFigure 1: Our method is a new 3D-aware GAN that can generate diverse virtual human portraits (512\u00d7512) with explicitly controllable\n3D camera viewpoints, facial expression, head pose, and shoulder movements. It is trained on unstructured 2D images without any 3D or\nvideo data. (Best viewed with zoom; see our project page for videos of more samples)\nAbstract\nPrevious animatable 3D-aware GANs for human gen-\neration have primarily focused on either the human head\nor full body. However, head-only videos are relatively un-\ncommon in real life, and full body generation typically does\nnot deal with facial expression control and still has chal-\nlenges in generating high-quality results. Towards appli-\ncable video avatars, we present an animatable 3D-aware\nGAN that generates portrait images with controllable fa-\ncial expression, head pose, and shoulder movements. It is\na generative model trained on unstructured 2D image col-\nlections without using 3D or video data. For the new task,\nwe base our method on the generative radiance manifold\nrepresentation and equip it with learnable facial and head-\nshoulder deformations. A dual-camera rendering and ad-\nversarial learning scheme is proposed to improve the qual-\nity of the generated faces, which is critical for portrait im-\nages. A pose deformation processing network is developed\n*Equal contribution. Work done when YW was an intern at MSRA.\n\u2020Corresponding author and project lead.\nto generate plausible deformations for challenging regions\nsuch as long hair.\nExperiments show that our method,\ntrained on unstructured 2D images, can generate diverse\nand high-quality 3D portraits with desired control over dif-\nferent properties.\n1. Introduction\nThe automatic creation of animatable 3D human char-\nacters has become an increasingly important topic with a\nrange of applications including video conferencing, movie\nproduction, and gaming. The related techniques have under-\ngone a significant growth recently, with a variety of promis-\ning methods being proposed [1, 7, 8, 16, 21, 38, 40, 42, 51,\n52, 54, 60, 62].\nAmong these techniques, 3D-aware generation methods\nhave emerged as a particularly promising avenue [1, 12, 21,\n38, 51, 52, 60, 62]. These methods can leverage the ample\navailability of unstructured 2D data for 3D human gener-\native learning without the need for 3D scans or multiview\nhuman images which are difficult to acquire at scale. Typ-\narXiv:2309.02186v1  [cs.CV]  5 Sep 2023\nically, these methods employ Generative Adversarial Net-\nworks (GANs) [18] for unsupervised training, use neural\nimplicit fields [34] as the 3D representation, and incorporate\npriors from 3D face and body parametric models [4, 29, 32]\nfor character control.\nDespite their promising potential, existing animatable\n3D-aware human generation focuses on either the human\nhead or full body and have encountered limitations in their\napplicability. Head generation methods [51, 52, 60, 62] can\nproduce high-quality face and hair with controllable facial\nexpression. Unfortunately, videos featuring only a human\nhead are relatively uncommon in everyday life, and there-\nfore these methods are less applicable in practical scenarios.\nFull body generation [1, 12, 21, 38, 65], on the other hand,\nalso generates torso and limbs with explicit pose control.\nHowever, generating high-quality full body human is still\nchallenging due to the complexity of body motion. In addi-\ntion, the facial region is often underrepresented in these full\nbody methods and there is no expression control.\nOur paper presents a new 3D-aware generation method\nthat is the first to focus on animatable generation of the hu-\nman head and shoulder regions. Our method enables fine-\ngrained control over facial expressions as well as head and\nshoulder movements, making it well suited for real-world\napplications such as video conferencing and virtual presen-\nters. Like previous 3D-aware GANs, our method is trained\non unstructured 2D image sets.\nFor this new task, we follow previous methods to train\nneural radiance generation with 3D parametric model pri-\nors in a GAN training scheme. We base our method on\nthe 3D-aware GAN framework of GRAM [9, 61] and fol-\nlow AniFaceGAN [60] for facial expression control using\n3D morphable model (3DMMs) priors. For head and shoul-\nder control, we incorporate the SMPL [32] body model for\ndeformation guidance. However, we found that naively ex-\ntending these existing techniques to our animatable head-\nshoulder portrait generation task is deficient. One promi-\nnent issue is about face quality, which is of paramount im-\nportance for visual communications. To handle the complex\nimage distribution caused by the large variations of head po-\nsition and orientation, we propose a dual-camera rendering\nand adversarial learning scheme for training. An additional\ndynamic camera is placed around human head pointing at\nhead center to render faces for discrimination, which signif-\nicantly improves the face generation quality. Another issue\nis the SMPL-guided human body deformation, for which\nwe identified that the commonly-used strategy based on lin-\near blending skinning failed to generate convincing results\nfor human characters with long hair. Sharp discontinuities\nwill occur under head rotation for hair region, leading to\nsignificant artifacts. To tackle this issue, we propose a pose\ndeformation volume processing module to learn better de-\nformations, which stabilizes GAN training and produces vi-\nsually plausible results.\nWe train our model on a head-shoulder portrait dataset\ncalled SHHQ-HS, which is constructed by cropping and\nsuperresolving the 40K human body images in the SHHQ\ndataset [15]. We show that our method can generate diverse\nand high-quality 3D portrait images with flexible control of\ndifferent properties including facial expressions and head-\nshoulder poses.\nOur contributions can be summarized as follows:\n\u2022 We propose the first animatable 3D-aware portrait\nGAN that generates head and shoulder regions with fa-\ncial expression and head-shoulder motion control. We\nbelieve generating such animatable human characters\nis a missing piece of 3D-aware human GANs for real-\nworld applications like video conferencing and virtual\npresenters.\n\u2022 We propose a dual-camera rendering and adversar-\nial learning scheme that gives rise to high-quality\nface generation comparable to previous head-only 3D-\naware GANs.\n\u2022 We propose a pose deformation processing module\nwhich achieves smooth and plausible pose-driven de-\nformation for human hair.\n2. Related Work\n3D-aware Image Generation\n3D-aware image genera-\ntive models aim to generate images that allow for the ex-\nplicit control of 3D camera viewpoint, training only on\n2D images. Most existing works are based on the GAN\nframework for generative modeling. Early approaches to\nthis problem use 3D convolutions to generate 3D feature\nvolumes and project them to 2D plane for image genera-\ntion [35, 36]. Recently, methods based on more explicit 3D\nrepresentations and differentiable rendering have become\npopular [5, 6, 9, 11, 19, 30, 37, 39, 44, 47, 49, 50, 53, 61,\n66]. The widely-used 3D representations are NeRF [34] and\nits variants, for their strong capability to model real-world\n3D scenes.\nAmong these NeRF-based GANs, some use volume ren-\ndering to directly generate the final images, which ensures\nstrong 3D consistency among different views but often has\nhigh computation cost. Others apply 2D convolutions on the\nrendered low-resolution 2D images or feature maps for up-\nsampling, which significantly reduces the computation cost\nbut sacrifices multiview consistency for the generated in-\nstances. Our method uses the high-resolution radiance man-\nifold representation of [61], which can generate high reso-\nlution images with strong multiview consistency.\nControllable\nHuman\nHead\nand\nBody\nGeneration\nAdding explicit controls to face and body generative mod-\neling has received much attention in recent years. Existing\nworks have primarily focused on either the head [1, 8, 51,\n52, 60, 62] or the whole body [1, 7, 12, 21, 38], with priors\nfrom 3D parametric models being commonly incorporated\nto achieve semantically meaningful control. For expression\ncontrol, most head GANs [1, 8, 51, 52, 60] often incorpo-\nrate 3D morphable models (3DMMs) [4] or FLAME mod-\nels [29] in their training process. Whole-body GANs typi-\ncally rely on the SMPL model [32] for body pose animation\n(with a few exceptions such as [38] that use body skeleton),\nand they often do not address facial expression control. This\nwork deals with a new human generation task: generating\nportrait figures that contain head and shoulder, with control-\nlable facial expression and head-shoulder poses.\nHuman Image and Video Manipulation\nOur method is\nalso related to human image and video manipulation ap-\nproaches [13, 16, 17, 23, 27, 31, 42, 43, 48, 55, 58, 59, 63,\n64] that also produce human animation videos. However,\nthe goal and underlying techniques of these methods differ\nsignificantly from ours. These methods aim to animate the\nhuman character in the given image or video, and are typi-\ncally trained in a supervised manner using videos or image\npairs. In contrast, we deal with human generative modeling\nand novel character creation, training on unstructured still\nimages in an unsupervised or weakly-supervised fashion.\n3. Method\nOur goal is to generate human portrait images containing\nhuman head and shoulder regions by training on a given 2D\nimage collection. As in a standard GAN setup, we sample\nrandom latent codes and map them to the final output image.\nThe input to our generator consists of multiple latent codes,\nwhich corresponds to different properties of the generated\nhuman, and the camera viewpoint. The output is a human\nportrait image carrying the desired properties.\nFigure 2 presents an overview of our method. The over-\nall pipeline follows the popular paradigm of canonical neu-\nral radiance representation in combination with (inverse)\ndeformation [1, 21, 60].\n3.1. Latent Codes\nOur latent codes contain an identity code zid \u2208 Rdi for\nhuman shape, an expression code zexp \u2208 Rde for facial ex-\npression, an zpose \u2208Rdp for head and shoulder pose, and an\nadditional noise \u03b5 \u2208 Rd\u03b5 controlling other attributes such\nas appearance. To achieve semantically meaningful con-\ntrol, we incorporate priors 3D human parametric models\nand align our latent space with theirs. Specifically, our iden-\ntity code zid is designed as the concatenation of 3DMM [41]\nface identity coefficient and SMPL [32] body shape coeffi-\ncient. The pose code zpose is a reduced SMPL pose param-\neter, which consists of the joint transformations of 6 joints:\nhead, neck, left and right collars, and left and right shoul-\nders. The expression code zexp is the same as 3DMM ex-\npression coefficient.\n3.2. Canonical Radiance Manifolds\nOur method utilizes the radiance manifolds [9, 61] to\nrepresent canonical humans.\nThis representation regu-\nlates radiance field learning and rendering on a set of\nlearned implicit surfaces in the 3D volume. It can generate\nhigh-quality human faces with strict multiview consistency.\nWith manifold superresolution [61], it can generate high-\nresolution images efficiently without sacrificing multiview\nconsistency.\nConcretely, we apply three networks for radiance gener-\nation. A manifold prediction MLP M takes a point x in the\ncanonical space as input and predicts a scalar s:\nM : x \u2208 R3 \u2192 s \u2208 R.\n(1)\nIt models a scalar field that defines the surfaces. A radiance\ngeneration MLP \u03d5 generates the color and opacity for points\non the surfaces given the identity codes zid, noise \u03b5 and\nview direction d:\n\u03d5 : (x, zid, \u03b5, d) \u2208 Rdi+d\u03b5+6 \u2192 (c, \u03b1) \u2208 R4.\n(2)\nA manifold superresolution CNN U upsamples the flattened\nand discretized radiance maps Rlr to high-resolution ones\nRhr:\nU : (zid, \u03b5, Rlr) \u2192 Rhr,\n(3)\nfor which we use a 1282 \u2192 5122 upsampling setting in this\npaper. For more technical details, we refer the readers to\n[9, 61].\n3.3. Deformation Fields\nFor each sampled 3D point in the target space with de-\nsired head-shoulder pose and facial expression, we apply\ndeformations to transform them to the canonical space for\nradiance retrieval. A two-stage deformation scheme is used\nto neutralize pose and expression.\n3.3.1\nPose Deformation Generator\nWe incorporate the SMPL model [32] and use its linear\nblend skinning (LBS) scheme [28] to guide our deforma-\ntion. Given the shape code zid and pose code zpose, a posed\nhuman body mesh can be constructed using SMPL. The\nSMPL model provides a pre-defined skinning weight vec-\ntor w \u2208 RNJ for each vertex on the body surface, where\nNJ is the joint number.\nA simple approach for propagating body surface defor-\nmation to the full 3D space is assigning any point the skin-\nning weights of its closest body surface vertex and use them\n\ud835\udc31\ud835\udc31\u2217\n\ud835\udc1c\ud835\udc1c, \ud835\udefc\ud835\udefc\n\ud835\udc33\ud835\udc33\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56, \ud835\udc33\ud835\udc33\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\nManifold\nGenerator\nRadiance\nGenerator\n\ud835\udc60\ud835\udc60 \n{\ud835\udc31\ud835\udc31\ud835\udc61\ud835\udc61}\n{\ud835\udc31\ud835\udc31\ud835\udc52\ud835\udc52}\n{\ud835\udc31\ud835\udc31\ud835\udc50\ud835\udc50}\n\ud835\udf19\ud835\udf19\n\ud835\udc5d\ud835\udc5d\nD\n\ud835\udc33\ud835\udc33\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56, \ud835\udf3a\ud835\udf3a\n\ud835\udc31\ud835\udc31\ud835\udc52\ud835\udc52\n\ud835\udc31\ud835\udc31\ud835\udc50\ud835\udc50\nT\nT\nTarget Space\nNeutral Pose Space\nNeutral Pose&Exp. Space\n(Canonical Space)\n\ud835\udc37\ud835\udc37\ud835\udc53\ud835\udc53\ud835\udc53\ud835\udc53\ud835\udc50\ud835\udc50\ud835\udc52\ud835\udc52\n\ud835\udc37\ud835\udc37\ud835\udc64\ud835\udc64\ud835\udc64\ud835\udc64\ud835\udc64\ud835\udc64\ud835\udc64\ud835\udc52\ud835\udc52\n\ud835\udc37\ud835\udc37\ud835\udc61\ud835\udc61\ud835\udc64\ud835\udc64\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc64\ud835\udc64\n{\ud835\udc31\ud835\udc31\ud835\udc61\ud835\udc61}\n{\ud835\udc31\ud835\udc31\ud835\udc52\ud835\udc52}\n{\ud835\udc31\ud835\udc31\ud835\udc50\ud835\udc50}\nFace\ncamera\nPortrait\ncamera\nPose Deformation\nExp. Deformation\nRadiance Generation\nVolume Rendering\n\ud835\udc33\ud835\udc33\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56\n\ud835\udf3a\ud835\udf3a\n\ud835\udc33\ud835\udc33\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\n\ud835\udc33\ud835\udc33\ud835\udc52\ud835\udc52\ud835\udc64\ud835\udc64\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc52\nFace Image\nPortrait Image\n\ud835\udc33\ud835\udc33\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56, \ud835\udf3a\ud835\udf3a\n\ud835\udc33\ud835\udc33\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56, \ud835\udc33\ud835\udc33\ud835\udc52\ud835\udc52\ud835\udc64\ud835\udc64\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc52\nExpression Deformation\nD\ud835\udc52\ud835\udc52\n\ud835\udc31\ud835\udc31\ud835\udc50\ud835\udc50\nH\u00d7W\u00d7D\u00d716\nGet (inverse) LBS \ntrans. \ud835\udc13\ud835\udc13 \u2208 R4\u00d74\nGather\nall samples, all rays\nH\u00d7W\u00d7D\u00d716\n3D Conv Net\nSMPL Mesh S(\ud835\udc33\ud835\udc33\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56, \ud835\udc33\ud835\udc33\ud835\udc52\ud835\udc52\ud835\udc64\ud835\udc64\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc52)\n\ud835\udc31\ud835\udc31\ud835\udc61\ud835\udc61\n\ud835\udc31\ud835\udc31\ud835\udc52\ud835\udc52\nPose Deformation\n\ud835\udc33\ud835\udc33\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56, \ud835\udc33\ud835\udc33\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\nRadiance Generation\n\ud835\udc3c\ud835\udc3c\ud835\udc53\ud835\udc53\ud835\udc53\ud835\udc53\ud835\udc50\ud835\udc50\ud835\udc52\ud835\udc52\n\ud835\udc3c\ud835\udc3c\ud835\udc52\ud835\udc52\ud835\udc64\ud835\udc64\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc53\ud835\udc53\ud835\udc56\ud835\udc56\ud835\udc61\ud835\udc61\nM\nGet intersection\nFigure 2: Method overview. Top: the pipeline of our controllable 3D-aware portrait GAN. For training, we apply a dual-\ncamera rendering scheme with two images separated rendered and three discriminators employed. Bottom: structures of the\ndeformation and radiance generation modules (the radiance manifold super-resolution step is omitted for simplicity; see text\nfor details).\nto deform it. In fact, this strategy is widely used in state-\nof-the-art animatable human body modeling and generation\nmethods [2, 12, 17, 22, 42, 65]. While it yields plausible\nresults for existing full-body synthesis method, we find it\nincurs significant visual defects in high-resolution portrait\nsynthesis. For human characters with long hairs, this na-\ntive strategy leads to sharp deformation discontinuity for the\nhair regions above shoulders (see Fig. 6).\nWe propose a deformation volume processing module to\ntackle this issue. Specifically, for a point xt in the target\nspace with the skinning weight vector w retrieved from the\nclosest SMPL body vertex, the deformed point can be cal-\nculated using inverse LBS:\nxp = LBS\u22121(xt, w) = \u00afT\u00b7xt =\n\u0000 PNJ\nj=1 wjTj\n\u0001\n\u00b7xt, (4)\nwhere \u00afT \u2208 R4\u00d74 is a transformation matrix computed by\nlinearly blending the SMPL joint transformations Tj \u2208\nSE(3).\nWe gather the reshaped transformation matrices\nfor the sampled points of all H \u00d7 W rays into a tensor\nT \u2208 RH\u00d7W \u00d7D\u00d716, where D is the number of sampled\npoints per ray, and apply a 3D CNN Dp to process it:\nDp : T \u2208 RH\u00d7W \u00d7D\u00d716 \u2192 T \u2208 RH\u00d7W \u00d7D\u00d716.\n(5)\nAfter processing, we reshape the transformations back and\napply them to the sampled points to accomplish pose defor-\nmation.\n3.3.2\nExpression Deformation Generator\nWe further apply a deformation to neutralize the facial ex-\npression from the target expression. Following [60], we in-\ntroduce a deformation field which is guided by the 3DMM\nmodel [41]. Specifically, an MLP De is employed to deform\nthe points in the pose-aligned space:\nDe : (xp, zid, zexp) \u2192 xc.\n(6)\nThis deformation network will be trained to generate faces\nwith expressions following 3DMM:\nS = S(zid, zexp) = \u00afS + Bidzid + Bexpzexp,\n(7)\nwhere \u00afS is the 3DMM mean face, and Bid and Bexp are the\nPCA basis for identity and expression, respectively. Train-\ning details can be found in later sections.\nIn total, our generator G has five sub-nets: M, \u03d5, U,\nDp, and De. For final image rendering, we calculate M\nintersection points {x\u2217\ni } between a deformed ray r (point\nsamples) and the canonical manifolds. We then obtain the\ncolor and occupancy of {x\u2217\ni } by sampling the radiance map\nRhr, and composite the color via:\nC(r) =\nM\nX\ni=1\nT(x\u2217\ni )\u03b1(x\u2217\ni )c(x\u2217\nj), T(x\u2217\ni ) =\nY\nk<i\n(1 \u2212 \u03b1(x\u2217\nk)).\n(8)\n3.4. Dual-Camera Discriminators\nPrevious 3D-aware head GANs have demonstrated strik-\ning face generation quality by carefully center-aligning the\ngenerated and real face images for training. In our case,\nhowever, the head region constitutes part of the portrait\nimage and its spatial position and orientation vary signifi-\ncantly. Simply applying a whole-image discriminator can-\nnot offer adequate supervision for high-quality face genera-\ntion, which is crucial for portrait images.\nA straightforward remedy is to crop and align the faces\nin the rendered images and apply a local face discrimina-\ntor.\nHowever, since image resampling operators are in-\nherently low-pass, such an image-space cropping strategy\nintroduces blur to the cropped faces, which is detrimental\nto GAN training especially for a multi-discriminator setup.\nIn this work, we design a dual-camera rendering scheme\nfor GAN training. In addition to the main camera for full\nportrait image rendering, we add another camera for face\nrendering, which is placed around human head pointing at\nhead center, as shown in Fig. 2. It is designed to have the\nsame local coordinate system as in previous 3D-aware head\nGANs [9, 60], and its position can be readily computed\nusing the deformed SMPL head. Another possible idea is\nblending the output of two separate generators for face and\nbody, as in some 2D human generation methods [14]. But\napplying this strategy to the 3D, animatable case seems not\nstraightforward.\nAdding a dedicated face camera for training not only\navoids image resampling and provides a more direct su-\npervision to the canonical radiance manifolds, but also en-\nables higher-resolution face rendering for adversarial learn-\ning. Consequently, the radiance generator receives stronger\nsupervision for the face region. We apply two image dis-\ncriminators Dwhole and Dface for the rendered portrait im-\nage and face image with these two cameras, respectively.\nWe also add another local discriminator Dtorso, which takes\nthe lower 1/4 part of the rendered portrait images as input.\n3.5. Training Losses\nAdversarial Learning.\nWe apply the non-saturating\nGAN loss with R1 regularization [33] for the 3D-aware im-\nage generator and all three discriminators Dwhole, Dface,\nand Dtorso. We empirically set the balancing weights to be\n\u03bbwhole = 0.1, \u03bbface = 1.0 and Dtorso = 0.5, respectively.\nDeformation Learning\nFollowing [60], we use a 3D\nlandmark loss and imitation loss to gain expression control\nwith 3DMM guidance. The landmark loss enforces the gen-\nerated face image to have similar 3D facial landmarks to the\n3DMM face constructed with the input identity and expres-\nsion codes:\nLlm = \u2225flm(S(zid, zexp)), flm(S(\u02c6zid, \u02c6zexp))\u22252\n2 ,\n(9)\nwhere \u02c6zid, \u02c6zexp are the 3DMM coefficients estimated\nfrom the generated image using a face reconstruction net-\nwork [10] and flm denotes a simple facial landmark extrac-\ntion function. For deformation imitation, we enforce the\ndisplacement of an input point xp to follow its nearest point\nxp\nref on the 3DMM mesh:\nL3DMM =\n\r\r\r\n\u0000De(xp) \u2212 xp\u0001\n\u2212 \u2206xp\nref\n\r\r\r\n2\n2 ,\n(10)\nwhere \u2206xp\nref = \u2212Bexpzexp(xp\nref) is the 3DMM-derived\n(inverse) deformation of point xp\nref.\nWe impose several regularizations on the deformations.\nFirst, we encourage the deformations to be as smooth as\npossible. For pose deformation processing, we apply\nLpose smooth = \u2225Dp(T ) \u2212 AvgPool(T )\u22252\n2 .\n(11)\nwhere AvgPool is a fixed average pooling operator. For the\nexpression deformation, we apply\nLexp smooth = \u2225De(xp) \u2212 De(xp + \u2206)\u22252\n2 ,\n(12)\nwhere \u2206 is a small random perturbation. Finally, a minimal\ndeformation constraint is applied for the expression defor-\nmation:\nLexp minimal = \u2225xp \u2212 De(xp)\u22252\n2 .\n(13)\n3.6. Training Strategy\nWe employ a two-stage training strategy to train our\nmodel. At the first stage, we train a low-resolution image\ngenerator and the corresponding discriminators. Both the\nface and portrait branches generate 128 \u00d7 128 images. All\nsub-networks are trained except for the manifold superreso-\nlution CNN U. For the second stage, we generate 512\u00d7512\nportrait images and 256 \u00d7 256 faces. We randomly initial-\nize and train U as well as the high-resolution discriminators\nwith all other sub-networks frozen.\n4. Experiments\nTraining Data\nWe build a training set by processing the\nhuman images in the SHHQ dataset [15]. SHHQ contains\n40K full-body images of 1024 \u00d7 512 resolution. To ob-\ntain high-quality head-shoulder portraits, we first fit SMPL\nmodels on the SHHQ images using the method of [24].\nThen, we crop the images and align them using the pro-\njected head and neck joints. The cropped portrait images are\nabout 256 \u00d7 256 resolution. We upsample them using the\nsuper-resolution methods of [56] and [57] to 1024 \u00d7 1024,\nfollowed by downsampling them to 512 \u00d7 512. Finally,\nthe backgrounds are removed by applying the provided seg-\nmentation masks. We call this dataset SHHQ-HS.\nImplementation Details\nOur manifold predictor and ra-\ndiance generator follow the implementations of [9]. 24 ra-\ndiance manifolds are used as in [9]. The manifold super-\nresolution net U is a smaller CNN compared to that in [61].\nThe pose deformation CNN Dp has two 3D conv layers with\nkernel size 9\u00d79 \u00d75. The expression deformation network\nDe is the same as [60]. See Fig. 10 for more details. For all\nexperiments, we use the Adam optimizer [26] for training.\nPrior to training, we estimate the identity, pose and expres-\nsion coefficients as well as camera poses for the images in\nRandom other properties\nHead pose control\nRandom other properties\nShoulder pose control\nRandom other properties\nFacial expression control\nRandom other properties\nCamera viewpoint control\nFigure 3: Results with controllable camera viewpoint, facial expression, head pose and shoulder pose. (Best viewed with\nzoom)\nthe dataset using 3DMM and SMPL fitting [10, 24]. During\ntraining, we randomly sample latent codes zid, zpose, and\nzexp and camera pose \u03b8 from the estimated distributions,\nand sample \u03b5 from a normal distribution.\nRuntime.\nWith an unoptmized implementation,\nour\nmethod takes about 0.87 seconds to generate one 512\u00d7 512\nimage from a set of given latent codes, evaluated on a\nNVIDIA RTX A6000 GPU.\n4.1. Generation Results\nFigure 1 and 9 present some generated portraits from our\nmethod trained on SHHQ-HS. The results are diverse and\nof high-quality, with camera viewpoint, facial expression,\nhead rotation, and shoulder pose explicitly controlled. Fig-\nure 3 shows the generated results for which we control one\nTable 1: Quantitative comparison with state-of-the-art 3D-\naware GANs on SHHQ-HS. Note that EG3D and GRAM-\nHD do not provide any expression and pose control, and\nAniFaceGAN only generates head images. FID and KID\n(\u00d7100) are computed with 20K randomly generated images\nand 20K real ones.\n\u2217: Although EG3D has lowest FID and\nKID scores, it often generates planar geometry; see Fig. 4.\n\u2020: AniFaceGAN is trained on 1282 by [60] and evaluated\nwith 2562 rendering.\nMethod\nFace 2562\nFull 5122\nFID\u2193\nKID\u2193\nFID\u2193\nKID\u2193\nEG3D\n5.63\u2217\n0.20\u2217\n6.81\u2217\n0.26\u2217\nGRAM-HD64\u2192512\n8.01\n0.41\n7.75\n0.29\nGRAM-HD128\u2192512\n8.14\n0.30\n8.82\n0.28\nAniFaceGAN\n11.56\u2020\n0.66\u2020\nN/A\nN/A\nOurs\n7.64\n0.43\n10.10\n0.43\nproperty out of the four while randomly changing the oth-\ners. Our method achieves consistent control for all the four\nproperties for different identities. More results can be found\nin the suppl. video.\n4.2. Comparison with Previous Methods\nWe compare our method with three state-of-the-art 3D-\naware GANs: EG3D [5], GRAM-HD [61] and AniFace-\nGAN [60]. Note that to our knowledge, there is no previous\nwork that deals with the animatable head-shoulder portrait\ngeneration task in this paper, and hence we compare with\nthese three methods for reference purpose only.\nTable 1 shows the FID [20] and KID [3] metrics eval-\nuated on both the full portrait images and the face regions.\nOur method has comparable scores with EG3D and GRAM-\nHD for face and slightly lower scores on full image. Note\nthat although EG3D has lowest scores, we found that it of-\nten generates poor geometry: the portrait surfaces are some-\ntimes nearly planar and the visual parallax is wrong when\nchanging viewing angles (Fig. 4). Visually inspected, our\nimage quality is comparable with EG3D and GRAM-HD\nand the portraits have correct geometry, as shown in Fig. 4.\nFigure 5 compares the results from AniFaceGAN and our\nmethod. Clearly, our method can generate and control much\nlarger region.\n4.3. Ablation Study\nWe then conduct ablation studies to validate the effec-\ntiveness of our algorithm design. For efficiency, we quanti-\ntatively evaluate the generation quality on the 1282 resolu-\ntion (i.e., without manifold super-resolution) and compute\nthe FID and KID metrics using 5K generated and real im-\nages. The results are summarized in Table 2.\nGRAM-HD\nOurs\nEG3D\nFigure 4: Visual comparison with state-of-the-art 3D-aware\nGANs on SHHQ-HS. Our results have similar visual quality\nto existing 3D-aware GANs that do not handle expression\nand pose control. (Best viewed with zoom)\nAniFaceGAN\nOurs\nFigure 5: Visual comparison with AniFaceGAN.\nDiscriminator Combinations\nOur full method uses three\ndiscriminators for training: Dwhole, Dface, and Dtorso. Ta-\nble 2 shows that using Dwhole alone is clearly deficient,\nas demonstrated by the poor FID and KID scores for face.\nCombining Dwhole with Dface (i.e., without Dtorso) signif-\nicantly improved the face quality, but the full portrait quality\ngets much worse than using only Dwhole. The FID and KID\nscores of our full method are low for both the full portrait\nimages and face regions.\nDual-Camera Training\nTo validate our dual-camera ren-\ndering and adversarial learning scheme, we train a variant of\nour method without a separate face camera for training. For\nthis variant, we locate faces in the rendered head-shoulder\nportrait images during training, and apply face discrimi-\nnator on cropped and aligned faces. As we can see from\nTable 2, our method with dual cameras for training sig-\nnificantly outperforms such an image cropping strategy in\nterms of face quality with slightly lower full-image FID.\nSome visual examples can be found in Fig. 6.\nPose Deformation Processing Module\nTable 2 (last row)\nshows that removing our pose deformation processing CNN\nDp, which degrades to a simple skinning weight assignment\nstrategy, leads to a quality drop for the full portrait image.\nFigure 6 visually compares two typical generation results.\nWithout Dp, sharp discontinuities will occur for long hairs\nTable 2: Ablation study on discriminator settings and the\npose deformation processing CNN Dp. \u2217: w/o dual-camera\ntraining, i.e., applying Dface on faces cropped from the ren-\ndered portrait images. FID and KID (\u00d7100) are computed\nwith 5K randomly generated images and 5K real ones.\nMethod\nFace 1282\nFull 1282\nDwhole Dface Dtorso Dp\nFID\u2193\nKID\u2193\nFID\u2193\nKID\u2193\n\u2713\n\u2713\n\u2713\n\u2713\n11.26\n0.57\n16.68\n0.97\n\u2713\n\u00d7\n\u00d7\n\u2713\n23.00\n1.48\n18.11\n1.06\n\u2713\n\u2713\n\u00d7\n\u2713\n10.68\n0.52\n22.58\n1.49\n\u2713\n\u2217\n\u2713\n\u2713\n17.89\n1.33\n14.05\n0.69\n\u2713\n\u2713\n\u2713\n\u00d7\n10.88\n0.52\n19.27\n1.23\nOurs\nw/o D\np\nw/o dual cam.\nFigure 6: Ablation on the dual camera training scheme and\npose deformation CNN Dp (trained on 1282 and rendered\non 2562 ). See the suppl. video for more results.\nunder head movements, whereas our full method produces\nplausible results without obvious artifacts.\n4.4. Talk Video Generation\nWe further test our trained model on the task of gen-\nerating videos of talking portraits driven by real videos.\nSpecifically, we selected some talk videos from the 300-VW\ndataset [45] and track the 3DMM expression and SMPL\nhead-shoulder pose using the methods of [10] and [24], re-\nspectively. Simple temporal smoothing is applied on the es-\ntimation results. Then we transfer the tracked results to our\ngenerated human characters to obtain virtual talk videos.\nFigure 7 shows some typical examples of our results where\na generated virtual character moves following the real per-\nson. See the supplementary video for continuous anima-\ntions of our results.\n5. Conclusion\nWe have presented a novel 3D-aware GAN for animat-\nable head-shoulder portrait generation, a new task not ad-\ndressed by previous methods. We identified several key is-\nFigure 7: Talk video generation driven by real person.\nFigure 8: Limitations on more extreme expressions and\nclosed eyes. For each image pair, the left one is the ref-\nerence image and the right one is the animated result.\nsues when extending existing techniques to this new task\nand proposed targeted algorithms to tackle them.\nWe\ndemonstrate that by training a corpus of unstructured 2D\nimages, our method can generate diverse and high-quality\n3D portraits with controllable facial expression as well as\nhead and shoulder movements. We believe our work repre-\nsents one step forward towards auto-creating video avatars\nfor real-world applications.\nLimitations\nOur method still has several limitations. It\nmay produce artifacts under human poses and expressions\nthat are not present in training data distribution, as shown\nin Fig. 8. In fact, the facial expression variation in SHHQ-\nHS is rather limited, lacking images with extreme expres-\nsion and closed eyes. The visual quality of the inner mouth\nregion (e.g., teeth) is not satisfactory, which is also par-\ntially due to limited data samples. Additionally, our current\nmethod lacks the ability to control other attributes, such as\neye gaze and environment lighting. We plan to further ex-\nplore and address these issues in our future research.\nEthics and responsible AI considerations\nThis work\naims to design an animatable 3D-aware human portrait gen-\neration method for the application of virtual avatars. It is not\nintended to create content that is used to mislead or deceive.\nHowever, it could still potentially be misused. We condemn\nany behaviors of creating misleading or harmful contents\nand are interested in applying this technology for advanced\nforgery detection. Currently, the images generated by this\nmethod contain visual artifacts that can be easily identified.\nThe method\u2019s performance is affected by the biases in the\ntraining data. One should be careful about the data collec-\ntion process and ensure unbiased distributions of race, gen-\nder, age, among others.\nReferences\n[1] Alexander Bergman, Petr Kellnhofer, Wang Yifan, Eric\nChan, David Lindell, and Gordon Wetzstein. Generative neu-\nral articulated radiance fields. In Advances in Neural Infor-\nmation Processing Systems, 2022. 1, 2, 3\n[2] Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian\nTheobalt, and Gerard Pons-Moll. Loopreg: Self-supervised\nlearning of implicit surface correspondences, pose and shape\nfor 3d human mesh registration. In Advances in Neural In-\nformation Processing Systems, pages 12909\u201312922, 2020. 4\n[3] Miko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and\nArthur Gretton. Demystifying mmd gans. In International\nConference on Learning Representations, 2018. 7\n[4] Volker Blanz and Thomas Vetter. A morphable model for the\nsynthesis of 3d faces. In Annual Conference on Computer\nGraphics and Interactive Techniques, pages 187\u2013194, 1999.\n2, 3\n[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al.\nEffi-\ncient geometry-aware 3d generative adversarial networks.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16123\u201316133, 2022. 2, 7\n[6] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein.\npi-gan: Periodic implicit gener-\native adversarial networks for 3d-aware image synthesis.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5799\u20135809, 2021. 2, 13\n[7] Xu Chen, Tianjian Jiang, Jie Song, Jinlong Yang, Michael J\nBlack, Andreas Geiger, and Otmar Hilliges. gDNA: Towards\ngenerative detailed neural avatars. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 20427\u2013\n20437, 2022. 1, 3\n[8] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin\nTong. Disentangled and controllable face image generation\nvia 3d imitative-contrastive learning.\nIn IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5154\u20135163, 2020. 1, 3\n[9] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.\nGRAM: Generative radiance manifolds for 3d-aware image\ngeneration. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10673\u201310683, 2022. 2, 3, 5\n[10] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde\nJia, and Xin Tong.\nAccurate 3d face reconstruction with\nweakly-supervised learning: From single image to image set.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, 2019. 5, 6, 8\n[11] Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava,\nGraham W Taylor, and Joshua M Susskind. Unconstrained\nscene generation with locally conditioned radiance fields. In\nIEEE/CVF International Conference on Computer Vision,\n2021. 2\n[12] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Ot-\nmar Hilliges, and Andreas Geiger. AG3D: Learning to gen-\nerate 3d avatars from 2d image collections. arXiv preprint\narXiv:2305.02312, 2023. 1, 2, 3, 4\n[13] Michail Christos Doukas, Stefanos Zafeiriou, and Viktoriia\nSharmanska. Headgan: One-shot neural head synthesis and\nediting.\nIn IEEE/CVF International Conference on Com-\nputer Vision, pages 14398\u201314407, 2021. 3\n[14] Anna Fr\u00a8uhst\u00a8uck, Krishna Kumar Singh, Eli Shechtman,\nNiloy J Mitra, Peter Wonka, and Jingwan Lu.\nInsetgan\nfor full-body image generation. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 7723\u2013\n7732, 2022. 5\n[15] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen\nQian, Chen Change Loy, Wayne Wu, and Ziwei Liu.\nStylegan-human: A data-centric odyssey of human genera-\ntion. In European Conference on Computer Vision, pages\n1\u201319, 2022. 2, 5\n[16] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias\nNie\u00dfner.\nDynamic neural radiance fields for monocular\n4d facial avatar reconstruction.\nIn IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8649\u2013\n8658, 2021. 1, 3\n[17] Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng,\nZicheng Liu, and Xin Tong. MPS-NeRF: Generalizable 3d\nhuman rendering from multiview images. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2022. 3, 4\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Z. Ghahra-\nmani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Wein-\nberger, editors, Advances in Neural Information Processing\nSystems, 2014. 2\n[19] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.\nStylenerf:\nA style-based 3d-aware generator for high-\nresolution image synthesis. In International Conference on\nLearning Representations, 2022. 2\n[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Advances in Neural Information Processing Sys-\ntems, pages 6626\u20136637, 2017. 7\n[21] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and\nZiwei Liu.\nEVA3D: Compositional 3d human generation\nfrom 2d image collections. In International Conference on\nLearning Representations, 2023. 1, 2, 3\n[22] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and\nTony Tung. Arch: Animatable reconstruction of clothed hu-\nmans. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3093\u20133102, 2020. 4\n[23] Kaiwen Jiang, Shu-Yu Chen, Feng-Lin Liu, Hongbo Fu, and\nLin Gao. Nerffaceediting: Disentangled face editing in neu-\nral radiance fields. In ACM SIGGRAPH Asia, pages 1\u20139,\n2022. 3\n[24] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exem-\nplar fine-tuning for 3d human model fitting towards in-the-\nwild 3d human pose estimation. In International Conference\non 3D Vision, pages 42\u201352, 2021. 5, 6, 8\n[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4401\u20134410, 2019. 13\n[26] Diederik P Kingma and Jimmy Ba.\nAdam: A method\nfor stochastic optimization. In International Conference on\nLearning Representations, 2015. 5\n[27] Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry\nFuchs. Neural human performer: Learning generalizable ra-\ndiance fields for human performance rendering. Advances\nin Neural Information Processing Systems, 34:24741\u201324752,\n2021. 3\n[28] John P Lewis, Matt Cordner, and Nickson Fong. Pose space\ndeformation: a unified approach to shape interpolation and\nskeleton-driven deformation. In Annual Conference on Com-\nputer Graphics and Interactive Techniques, pages 165\u2013172,\n2000. 3\n[29] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier\nRomero. Learning a model of facial shape and expression\nfrom 4d scans. ACM Transactions on Graphics, 36(6):194\u2013\n1, 2017. 2, 3\n[30] Yiyi Liao, Katja Schwarz, Lars Mescheder, and Andreas\nGeiger. Towards unsupervised learning of generative mod-\nels for 3d controllable image synthesis. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5871\u20135880, 2020. 2\n[31] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt.\nNeural actor:\nNeural free-view synthesis of human actors with pose con-\ntrol. ACM Transactions on Graphics, 40(6):1\u201316, 2021. 3\n[32] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J Black.\nSMPL: A skinned\nmulti-person linear model. ACM transactions on Graphics,\n34(6):1\u201316, 2015. 2, 3\n[33] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.\nWhich training methods for gans do actually converge?\nIn International Conference on Machine Learning, pages\n3481\u20133490, 2018. 5\n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In European Conference on Computer Vision, pages\n405\u2013421, 2020. 2\n[35] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang.\nHologan: Unsupervised\nlearning of 3D representations from natural images.\nIn\nIEEE/CVF International Conference on Computer Vision,\npages 7588\u20137597, 2019. 2\n[36] Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-\nLiang Yang, and Niloy Mitra.\nBlockGAN: Learning 3d\nobject-aware scene representations from unlabelled images.\nIn Advances in Neural Information Processing Systems,\n2020. 2\n[37] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-\ning scenes as compositional generative neural feature fields.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11453\u201311464, 2021. 2\n[38] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya\nHarada. Unsupervised learning of efficient geometry-aware\nneural articulated representations. In European Conference\non Computer Vision, pages 597\u2013614, 2022. 1, 2, 3\n[39] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStylesdf: High-resolution 3d-consistent image and geometry\ngeneration. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13503\u201313513, 2022. 2\n[40] Hao Ouyang, Bo Zhang, Pan Zhang, Hao Yang, Jiaolong\nYang, Dong Chen, Qifeng Chen, and Fang Wen. Real-time\nneural character rendering with pose-guided multiplane im-\nages. In European Conference on Computer Vision, pages\n192\u2013209, 2022. 1\n[41] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami\nRomdhani, and Thomas Vetter. A 3d face model for pose\nand illumination invariant face recognition. In IEEE Inter-\nnational Conference on Advanced Video and Signal based\nSurveillance, pages 296\u2013301, 2009. 3, 4\n[42] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan\nZhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-\nmatable neural radiance fields for modeling dynamic human\nbodies. In IEEE/CVF International Conference on Computer\nVision, pages 14314\u201314323, 2021. 1, 3, 4\n[43] Yurui Ren, Ge Li, Yuanqi Chen, Thomas H Li, and Shan\nLiu. Pirenderer: Controllable portrait image generation via\nsemantic neural rendering. In IEEE/CVF International Con-\nference on Computer Vision, pages 13759\u201313768, 2021. 3\n[44] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance fields for 3d-aware image\nsynthesis.\nIn Advances in Neural Information Processing\nSystems, 2020. 2\n[45] Jie Shen, Stefanos Zafeiriou, Grigoris G Chrysos, Jean Kos-\nsaifi, Georgios Tzimiropoulos, and Maja Pantic. The first\nfacial landmark tracking in-the-wild challenge: Benchmark\nand results. In IEEE International Conference on Computer\nVision Workshops, pages 50\u201358, 2015. 8\n[46] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00b4ar, Johannes Totz,\nAndrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan\nWang. Real-time single image and video super-resolution\nusing an efficient sub-pixel convolutional neural network. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1874\u20131883, 2016. 13\n[47] Yichun Shi, Divyansh Aggarwal, and Anil K Jain. Lifting 2d\nstylegan for 3d-aware face generation. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n6258\u20136266, 2021. 2\n[48] Aliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for im-\nage animation. Advances in Neural Information Processing\nSystems, 32, 2019. 3\n[49] Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian\nRen, Hsin-Ying Lee, Peter Wonka, and Sergey Tulyakov.\n3d generation on imagenet. In International Conference on\nLearning Representations, 2023. 2\n[50] Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter\nWonka. Epigraf: Rethinking training of 3d gans. In Ad-\nvances in Neural Information Processing Systems, 2022. 2\n[51] Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong\nZhang, Hongwen Zhang, and Yebin Liu. Next3D: Gener-\native neural texture rasterization for 3d-aware head avatars.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023. 1, 2, 3\n[52] Keqiang Sun, Shangzhe Wu, Zhaoyang Huang, Ning Zhang,\nQuan Wang, and HongSheng Li. Controllable 3d face syn-\nthesis with conditional generative occupancy fields. In Ad-\nvances in Neural Information Processing Systems, 2022. 1,\n2, 3\n[53] Ayush\nTewari,\nXingang\nPan,\nOhad\nFried,\nManeesh\nAgrawala, Christian Theobalt, et al. Disentangled3d: Learn-\ning a 3d generative model with disentangled geometry and\nappearance from monocular images. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n1516\u20131525, 2022. 2\n[54] Daoye Wang, Prashanth Chandran, Gaspard Zoss, Derek\nBradley, and Paulo Gotardo. MORf: Morphable radiance\nfields for multiview neural head modeling.\nIn ACM SIG-\nGRAPH 2022 Conference Proceedings, pages 1\u20139, 2022. 1\n[55] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In IEEE/CVF conference on Computer Vision and Pat-\ntern Recognition, pages 10039\u201310049, 2021. 3\n[56] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To-\nwards real-world blind face restoration with generative facial\nprior. In IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 9168\u20139178, 2021. 5\n[57] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,\nChao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-\nhanced super-resolution generative adversarial networks. In\nEuropean Conference on Computer Vision Workshops, 2018.\n5, 13\n[58] Chung-Yi Weng,\nBrian Curless,\nPratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 16210\u201316220, 2022.\n3\n[59] Olivia Wiles, A Koepke, and Andrew Zisserman. X2face:\nA network for controlling face generation using images, au-\ndio, and pose codes. In European Conference on Computer\nVision, pages 670\u2013686, 2018. 3\n[60] Yue Wu, Yu Deng, Jiaolong Yang, Fangyun Wei, Qifeng\nChen, and Xin Tong. AniFaceGAN: Animatable 3d-aware\nface image generation for video avatars. In Advances in Neu-\nral Information Processing Systems, 2022. 1, 2, 3, 4, 5, 7\n[61] Jianfeng Xiang, Jiaolong Yang, Yu Deng, and Xin Tong.\nGRAM-HD: 3d-consistent image generation at high resolu-\ntion with generative radiance manifolds. In IEEE/CVF In-\nternational Conference on Computer Vision, 2023. 2, 3, 5,\n7\n[62] Hongyi Xu, Guoxian Song, Zihang Jiang, Jianfeng Zhang,\nYichun Shi, Jing Liu, Wanchun Ma, Jiashi Feng, and Linjie\nLuo. OmniAvatar: Geometry-guided controllable 3d head\nsynthesis. In IEEE/CVF International Conference on Com-\nputer Vision, 2023. 1, 2, 3\n[63] Sicheng Xu, Jiaolong Yang, Dong Chen, Fang Wen, Yu\nDeng, Yunde Jia, and Tong Xin. Deep 3d portrait from a\nsingle image. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 7710\u20137720, 2020. 3\n[64] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and\nVictor Lempitsky. Few-shot adversarial learning of realis-\ntic neural talking head models. In IEEE/CVF International\nConference on Computer Vision, pages 9459\u20139468, 2019. 3\n[65] Jianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu,\nYichun Shi, Guoxian Song, Zhongcong Xu, Xinchao Wang,\nand Jiashi Feng. Avatargen: a 3d generative model for an-\nimatable human avatars. In European Conference on Com-\nputer Vision Workshops, pages 668\u2013685, 2022. 2, 4\n[66] Xiaoming Zhao, Fangchang Ma, David G\u00a8uera, Zhile Ren,\nAlexander G Schwing, and Alex Colburn. Generative mul-\ntiplane images: Making a 2d gan 3d-aware. In European\nConference on Computer Vision, pages 18\u201335, 2022. 2\nFigure 9: Uncurated portrait generation results from our method.\nExpression Deformation Network\nPose Deformation Processing Network\nT\nDownsample 2x\n3D Conv \nUpsample 2x\n3D Conv \nT\nD\ud835\udc52\nD\ud835\udc5d\nU\nRRDBs\n\ud835\udc11\u0b6a\u0b70\nSub-pixel Conv\nSub-pixel Conv\nConv\nFC\nFC+LReLU\nFC+LReLU\nFC+LReLU\nA\nA\nA\nA\n\ud835\udc11\u0b66\u0b70\n\ud835\udc33\u0bdc\u0bd7, \ud835\udf3a\nFiLM SIREN\nFiLM SIREN\nFiLM SIREN\nFC\nFC+LReLU\nFC+LReLU\nFC+LReLU\nA\nA\nA\nA\n\ud835\udc33\u0bdc\u0bd7, \ud835\udc33\u0bd8\u0beb\u0be3\n\ud835\udc31\u0be3\n\ud835\udc31\u0bd6\nManifold Super-resolution Network\nFigure 10: Network structures. Dp has two 3D conv layers with kernel size 9 \u00d7 9 \u00d7 5. Both De and U have a StyleGAN-\nlike structure [25] with a mapping MLP network and a backbone. The backbone of De is mainly based on 3 FiLM SIREN\nlayers [6] while U uses 4 Residual-in-Residual Dense Blocks (RRDBs) [57] and 2 sub-pixel conv layers [46].\n"
  },
  {
    "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
    "link": "https://arxiv.org/pdf/2309.00986.pdf",
    "upvote": "17",
    "text": "ModelScope-Agent: Building Your Customizable Agent System with\nOpen-source Large Language Models\nChenliang Li, Hehong Chen, Ming Yan\u2217, Weizhou Shen, Haiyang Xu, Zhikai Wu\nZhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi\nJi Zhang, Fei Huang, Jingren Zhou\nDAMO Academy, Alibaba Group, China\nAbstract\nLarge language models (LLMs) have recently\ndemonstrated remarkable capabilities to com-\nprehend human intentions, engage in reason-\ning, and design planning-like behavior.\nTo\nfurther unleash the power of LLMs to accom-\nplish complex tasks, there is a growing trend to\nbuild agent framework that equips LLMs, such\nas ChatGPT, with tool-use abilities to connect\nwith massive external APIs.\nIn this work, we introduce ModelScope-Agent,\na general and customizable agent framework\nfor real-world applications, based on open-\nsource LLMs as controllers. It provides a user-\nfriendly system library, with customizable en-\ngine design to support model training on mul-\ntiple open-source LLMs, while also enabling\nseamless integration with both model APIs and\ncommon APIs in a unified way.\nTo equip\nthe LLMs with tool-use abilities, a compre-\nhensive framework has been proposed span-\nning over tool-use data collection, tool retrieval,\ntool registration, memory control, customized\nmodel training, and evaluation for practical\nreal-world applications. Finally, we showcase\nModelScopeGPT, a real-world intelligent as-\nsistant of ModelScope Community based on\nthe ModelScope-Agent framework, which is\nable to connect open-source LLMs with more\nthan 1000 public AI models and localized\ncommunity knowledge in ModelScope. The\nModelScope-Agent library1 and online demo2\nare now publicly available.\n1\nIntroduction\nLarge language models (OpenAI, 2022, 2023;\nTouvron et al., 2023; Chowdhery et al., 2022)\nhave gradually become common AI assistants\nthat demonstrate great potential in comprehend-\ning human intentions, performing complex rea-\nsoning tasks, and enabling content creation. De-\n\u2217Corresponding author: <ym119608@alibaba-inc.com>\n1https://github.com/modelscope/modelscope-agent\n2https://modelscope.cn/studios/damo/ModelScopeGPT/summary\nspite the rapid advancements of open-source LLMs,\ne.g., LLaMA (Touvron et al., 2023) and Chat-\nGLM (THUDM, 2023), they still remain limited\nin performing complex tasks, such as following\nuser instructions to use external tools and capture\nup-to-date information.\nTo further unleash the power of LLMs for real-\nworld practical applications, a rising trend of cur-\nrent research (Schick et al., 2023; Shen et al., 2023;\nYang et al., 2023; Qin et al., 2023; Patil et al., 2023)\nbegins to enable LLMs with tool-use abilities to-\nwards building an AI Agent. These include Hug-\ngingGPT (Shen et al., 2023), Visual-ChatGPT (Wu\net al., 2023) and Gorilla (Patil et al., 2023) for\nconnecting with HuggingFace models, ToolAl-\npaca (Tang et al., 2023) and ToolLLaMA (Qin et al.,\n2023) for using massive common APIs such as\nweather forecast and search engine. These methods\neither directly rely on closed-source counterparts\nlike ChatGPT or focus on certain types of API tools.\nRecently, there have also been public releases of\nAI agents, such as Auto-GPT3, LangChain4 and\nTransformers Agent (Huggingface, 2023), which\nenable LLMs, such as ChatGPT or GPT-4, to use\ntools and solve complex AI tasks. However, these\nagents are mainly built with closed-source LLMs\nand how to build a customizable agent system with\nopen-source LLMs remains largely unexplored.\nIn this work, we present ModelScope-Agent, a\ngeneral and customizable agent system for real-\nworld applications, based on open-source LLMs\nas controllers. ModelScope5 is a public ML com-\nmunity, which seeks to bring together the most ad-\nvanced machine learning models from the AI com-\nmunity, and streamlines the process of leveraging\nAI models in real-world applications. ModelScope-\nAgent provides a flexible and user-friendly sys-\ntem library, with customizable engine design to\n3https://github.com/Significant-Gravitas/Auto-GPT\n4https://github.com/langchain-ai/langchain\n5https://modelscope.cn/models\narXiv:2309.00986v1  [cs.CL]  2 Sep 2023\nsupport model training on multiple open-source\nLLMs, while also enabling seamless integration\nwith both model APIs and common APIs in a uni-\nfied way. It features an LLM-centric system de-\nsign, which includes open-source LLMs as core\ncontroller, and further interact with a tool-use mod-\nule and a memory module to accomplish complex\ntasks. At the core of ModelScope-Agent , the li-\nbrary supports flexible selection and training on var-\nious open-source LLMs, such as LLaMA (Touvron\net al., 2023), ChatGLM (THUDM, 2023), Chat-\nPLUG (Tian et al., 2023) and other customized\nLLMs in ModelScope. For tool use, ModelScope-\nAgent provides a default tool library, which sup-\nports diverse AI model APIs across NLP, CV, Au-\ndio and Multi-model fields, as well as massive com-\nmon APIs such as search engine. It also supports\nregistering new self-defined API plugins and auto-\nmatic API retrieval from the large tool library. It is\neasy for users to customize their most appropriate\nLLMs, local API tools and functions to develop\nreal-world applications. Moreover, a memory mod-\nule is also introduced to better store and manage the\nsystem message, user history, in-context examples,\ntool message and localized knowledge.\nTo enable the open-source LLMs to better con-\ntrol the whole agent system, we further propose\na comprehensive framework of tool-use data col-\nlection, customized model training, evaluation and\ndeployment. Notably, we release a comprehen-\nsive tool-enhanced dataset MSAgent-Bench, which\nconsists of 598k dialogues with various API cat-\negories, multi-turn API calls, API-Oriented QA,\nand API-Agnostic instructions in both English and\nChinese. A simple training strategy of Weighted\nLM, that enhances the training of generation of\nAPI name and parameters, is used to better ensure\nthe correctness of API calls. Besides, an evalua-\ntion framework is also supported in our library to\nexamine the tool-use abilities of the trained mod-\nels in different aspects. Furthermore, we applied\nModelScope-Agent in a real-world application of\nModelScope Community namely ModelScopeGPT,\nwhich is able to connect open-source LLMs with\nmore than 1000 public AI models and access lo-\ncalized community knowledge in ModelScope for\ncommunity QA.\nTo summarize, ModelScope-Agent is a general\nand customizable agent system designed for devel-\nopers to harness the power of open-source LLMs.\nThe library targets the following goals:\n\u2022 Agent based on Open-Source LLMs: the con-\ntroller of ModelScope-Agent can be flexibly\nselected from open-source LLMs that are opti-\nmized through our agent training framework.\n\u2022 Support and Customization of Diverse Tools:\nDozens of diverse model APIs and common\nAPIs are given by default. The library sup-\nports registering new self-defined APIs and\nautomatic API retrieval from the toolset.\n\u2022 Customizable of Applications: ModelScope-\nAgent can be flexibly applied in various in-\ndustry applications. The agent and training\nframework are documented describing its us-\nage, construction and optimization.\nModelScope-Agent is in continual development\nby the engineers at ModelScope and is released\nunder an Apache 2.0 license. Full documentation\nis available through the project website.\n2\nThe ModelScope Agent\nModelScope-Agent is designed to facilitate devel-\nopers in building customizable agent systems based\non open-source LLMs. The overall system architec-\nture is shown in Figure 1. It includes open-source\nLLMs as controller, a tool-use module and a mem-\nory module to interact with. Given human instruc-\ntion, the Agent, which adopts the selected LLM as\nthe controller, will automatically plan tasks, selec-\ntively uses tools, leverage knowledge in memory,\nand finally provides helpful responses to users.\n2.1\nLLMs as Brain\nLLMs serve as the brain of the agent, responsible\nfor planning and decomposing user requests, se-\nlectively calling tools, performing retrieval, and\nintegrating all the information from previous steps\nto generate the final response. In order to make it\neasier for users to customize the agent with their\nown LLMs, we have added support for various\nopen-source LLMs by default, such as LLaMA,\nChatGLM and ChatPLUG, which have been op-\ntimized through our tool learning pipeline. The\ndetails of training strategy and tool-use datasets\ncan be referred to Section 3. ModelScope-Agent\nhas integrated the LLM inference pipeline of the\nModelScope community, and replacing LLMs can\nbe done by simply setting the model_name and\nmodel_config.\nIn model_config, the model_id,\nmodel_revision, and model parameter settings such\nas max sequence length, should be configured.\nFigure 1: The overall system architecture of ModelScope-Agent.\n# LLM config \"cfg_file\"\nfrom modelscope.utils.config import Config\nmodel_cfg = Config.from_file(cfg_file)\nllm = LocalLLM(model_name , model_cfg)\nFurthermore, the ModelScope-Agent also pro-\nvides a standard way to integrate new LLM. Users\ncan add their own LLMs, by integrating the LLM\npipeline into ModelScope. After that, the agent can\nselect the new LLMs for training and inference.\n2.2\nTool Use\nTool Library\nThe tool library is used to config-\nure and manage various collections of APIs used in\nthe agent. ModelScope-Agent can support a wide\nrange of both common APIs such as search APIs,\nand AI model APIs across NLP, CV, Audio and\nMulti-modal models in ModelScope and Hugging-\nFace. Each tool API consists of the API name, de-\nscription, parameters and request functions. Users\ncan easily choose and configure proper APIs in\nthe library to build their own agent. The default\nAPIs supported in the library can be referred to\nAppendix A.1.\n# tool default config file \"default_file\"\ntool_cfg = Config.from_file(default_file)\nRegister and Customize New Tool\nThe agent\nallows users to register and customize new tools,\nwhile also supporting quick integration of newly\nregistered tools into the agent, enabling LLMs to\nselectively use the additional self-defined tools for\nspecific applications. This can be simply done\nby inheriting from a base class, namely Tool, and\ndefining a new CustomTool with the API-related\nschema of API name, description, parameters, and\nrequest functions. More details about CustomTool\ncan be referred in Appendix A.2.\nfrom modelscope_agent.tools import Tool\nclass CustomTool(Tool):\n# logic added here\n# refer example in Appendix A.2\ntool_list = {\u2019customo -tool\u2019: CustomTool ()}\nTool Retrieval and Execution\nDue to the large\namount of tool APIs in the tool library, a tool\nretrieval module is further introduced to recom-\nmend appropriate APIs for each instruction prompt.\nSpecifically, we use the dense vector retrieval\nmethod based on the unified multilingual text-\nembedding API 6. We vectorize both the text de-\nscriptions of the APIs and the instruction prompt\nusing the text-embedding API. The top-3 most rel-\nevant APIs with the highest vector product scores\nare selected for tool use. As a result, the schema\ninformation of the retrieved APIs will be concate-\nnated with other system prompts in the subsequent\nmemory module and sent to LLMs as input. With\nthe concatenated instruction prompt, the LLMs will\nplan and generate the API request, which will be\nexecuted by the agent. The agent will then return\nthe results to the LLMs for continuous generation.\n2.3\nMemory Control\nThe memory module is used to retrieve, and assem-\nble a series of contextual information as input to the\nLLMs. It consists of a knowledge retrieval submod-\nule and a prompt generator submodule, which are\nresponsible for external knowledge retrieval and\ninstruction prompt generation, respectively.\n6https://help.aliyun.com/zh/dashscope/getting-started-1\nKnowledge Retrieval\nIt enables the agent to\nget access to up-to-date and localized information\nrelated with query prompt, thereby augmenting\nLLMs with dynamic and domain-specific knowl-\nedge. We follow the same dense vector retrieval\nmethod as the previous tool retrieval module, and\nsupport large-scale knowledge retrieval from local-\nized document corpus. Similarly, it allows users\nto customize by changing to other open-source re-\ntrieval frameworks.\nPrompt Generator\nThe prompt generator is used\nto assemble all available contextual information\nsuch as system prompt, API schema, retrieved\nknowledge, conversation history, and few-shot ex-\namples. According to the type of user query and\nthe maximum length of the LLM, the users can\nselectively choose proper contextual information\nand assemble the required input to the LLM. In our\nagent, the prompt generator needs to be defined\nbefore the agent is constructed.\n2.4\nAgent Pipeline\nIn summary, we build the agent by combining all\nthe modules: LLM controller, tool-use module, and\nmemory module. With agent.run, the agent can ef-\nficiently execute and complete the instruction in\na one-step generation. First, the agent retrieves\nquery-related tools through the tool retrieval and\ncombines the retrieved API schema with other con-\ntextual prompts in memory module, to construct\na new instruction prompt. Then, the agent sends\nthis new prompt to the LLM, who plans whether\nand which API to call and generate an API request.\nNext, the agent will execute the selected API with\nthe extracted API parameters and return the API\nresults to the LLMs, which will continue to plan\nwhether to call other APIs. If another API call\nis needed, the process is repeated, otherwise, the\nLLMs generate the final response and the agent\nreturns the final result to the user.\nagent = AgentExecutor(llm , tool_cfg ,\nadditional_tool_list=tool_list)\nagent.run(\"Draw a logo image of agent\")\n3\nTraining\n3.1\nDataset\nTo facilitate building an agent with the ability\nto use tools while upholding an optimal level of\nuser engagement, we release a comprehensive tool\ndataset, MSAgent-Bench7, utilizing ChatGPT syn-\nthetic data and the existing instruction-following\ndatasets. Our released dataset encompasses 598k\ndialogues. Table 1 outlines the key differences\nbetween the released dataset and other public avail-\nable tool learning datasets, while the data distribu-\ntion of our dataset is illustrated in Figure 2. As\ndemonstrated in the Table and Figure, we have\nmade certain efforts to construct a comprehensive\ndataset which enables the effective training of an\nagent:\nMultilingual: We collect instances in both Chi-\nnese and English, ensuring that the trained agent is\ncapable of functioning in both languages.\nVarious API Categories: Our dataset supports\nCommon APIs that have been registered by users\nor applied through online API platforms, as well as\nmodel APIs that can call neural models.\nMulti Turn Dialog: In real-life scenarios, agents\nmay need to request more specific clarification\nfrom users to complete a task or receive additional\ninstructions after completing a previous task. Our\ndataset accounts for these scenarios and supports\nmulti-turn user-agent interactions when using tools.\nAPI-Oriented QA: An effective agent should pos-\nsess knowledge of APIs. Our dataset incorporates\nAPI document QA tasks and task planning tasks\nwhich requires agents to offer appropriate sugges-\ntions to users on how to use various APIs to solve\ncomplex tasks.\nAPI-Agnostic Instructions:\nTo enhance the\nagent\u2019s ability to follow common instructions and\nincrease user engagement, we have incorporated\nboth Chinese and English API-agnostic instructions\nwithin our dataset. These instructions place greater\nemphasis on the agent\u2019s inherent capabilities rather\nthan reliance on API invocation.\nThe data was collected by prompting ChatGPT\n(gpt-3.5-turbo) to generate instructions, API re-\nquests, and answers based on the API calling re-\nsults, more details can be accessed in Appendix D.\n3.2\nModel Training\nWe use the MSAgent-Bench to fine-tune multi-\nple open-source LLMs, including LLaMA (Tou-\nvron et al., 2023), Qwen (QwenLM, 2023), Chat-\nPLUG (Tian et al., 2023) etc. We train all the open-\nsource LLMs in a multi-round conversation mode\nand concatenate all the prompts and answers. Com-\n7https://modelscope.cn/datasets/damo/MSAgent-\nBench/summary\nDataset\nLanguage\nInstance Type\n# Instances\nAPI type\nAvg. Turn\nAvg. Step\nAPI-Bank (Li et al., 2023)\nEnglish\nTool Use\n264\nCommon API\n3.27\n1.92\nToolAlpaca (Tang et al., 2023)\nEnglish\nTool Use\n3.9 K\nCommon API\n1\n1.66\nGorilla (Patil et al., 2023)\nEnglish\nTool Use\n16.4 k\nModel API\n1\n1\nGPT4Tools (Yang et al., 2023)\nEnglish\nTool Use\n71.4 K\nModel API\n1\n1\nToolBench (Qin et al., 2023)\nEnglish\nTool Use\n26.9 K\nCommon API\n1\n4.1\nMSAgent-Bench (ours)\nEnglish + Chinese\nTool Use + Common Chat\n598 K\nCommon API + Model API\n1.52\n1.31\nTable 1: The statistics of MSAgent-Bench and other existing tool learning datasets.\nMSAgent-\nBench\nModel API\n\u2022\nText-to-Image\n\u2022\nText-to-Video\n\u2022\nText-to-Audio\n\u2022\nTranslation\n\u2022\nImage Chat\n\u2022\nUniversal IE\n\u2026\nCommon API\n\u2022\nWeather\n\u2022\nWeb Search\n\u2022\nCalculator\n\u2022\nMap\n\u2026\nAPI-Agnostic Instructions\n\u2022\nStory Generation\n\u2022\nOpen QA\n\u2022\nCode\n\u2022\nChit Chat\n\u2022\nParaphrase\n\u2022\nSTEM\n\u2022\nRole Play\n\u2026\nAPI-Oriented QA\n\u2022\nDocument QA\n\u2022\nTask Planning\n\u2026\nFigure 2: The instance types and distribution of our collected MSAgent-Bench.\npared to common instruction tuning data, the tool\nlearning samples focus more heavily on the accu-\nracy of tool selection and API parameter prediction.\nTherefore, we propose a simple training strategy,\nWeighted LM, which enhances the training of gen-\neration of API name and parameters, while zero-\nout the loss of tokens from the user prompt and the\ntool execution. More details can be be referred to\nAppendix B.3.\nkwargs = dict(model=model , ...)\ntrainer: EpochBasedTrainer = build_trainer\n(name=args.trainer , default_args=kwargs)\ntrainer.train()\n4\nEvaluation\nOur evaluation system, MSAgent-Eval, comprises\ntwo modules: an automatic evaluation framework\nwhich comprehensively evaluates API usability of\nthe agents, and a human evaluation framework im-\nplemented by an agent arena which reflects the\npreferences of human users.\n4.1\nAutomatic Evaluation Framework\nIn automatic evaluation, we mainly focus on eval-\nuating agent\u2019s ability to generate accurate API re-\nquest and the proper answers according to the API\ncalling results. Specifically, we use the action ex-\nactly match score (Action EM) which measures\nwhether the agent uses the correct API as the ref-\nerence gold API, and the ROUGE-L score which\nmeasures the similarity between the generated re-\nsponse and the gold answer. Additionally, we intro-\nduce a novel metric called Argument F1 for fully\nevaluating the quality of API requests. To com-\npute Argument F1, we categorize the arguments in\nagent\u2019s API request into two cases, namely Half\nmatch (HM) and Full match (FM), representing\ncorrect argument but with wrong value and correct\nargument with correct value, respectively. Suppose\nthe gold argument number in the API is |A|, and\nthe number of arguments in the agents API request\nis |A\u2217|, we compute the new Recall and Precision\nas follows:\nR = (0.5 \u00d7 # HM + # FM)/|A|\n(1)\nP = (0.5 \u00d7 # HM + # FM)/|A\u2217|\n(2)\nand the final argument F1 is computed as:\nF1 = 2(R \u2217 P)/(R + P).\n(3)\nA sample code for the automated evaluation of\nagents is provided below:\nfrom tool_agent_finetune import evaluation\nEM , F1 , ROUGE = evaluation(refs , preds)\nExpert annotators were engaged to annotate the\nevaluation instances, with the task of providing\ndiverse instructions, manually documenting cor-\nrect API calling requests, and writing appropriate\nresponses. The statistics of our currently assem-\nbled test data is in Appendix B.1, and the auto-\nmatic evaluation scores of our trained agents can\n(a) ModelScope Intelligent Assistant\n(b) Register and Use New Tools on Alibaba Cloud\nFigure 3: Demo cases of ModelScopeGPT based on ModelScope-Agent .\nbe found in Appendix B.2. We also guarantee the\nusers to upload their own annotated test examples\nto accurately evaluate the performance of agents in\ncustomized scenarios.\n4.2\nHuman Evaluation with Agent Arena\nInspired by the Arena for ChatBots (Zheng et al.,\n2023), we have built an accessible Agent Arena 8\nthat allows users to furnish instructions to two\nanonymous agents, based on the provided APIs.\nSubsequently, users have the opportunity to vote\non which Agent performs better in tackling the in-\nstruction with the given APIs. In accordance with\nthe framework presented by Zheng et al. (2023),\nwe adopt a system of ELO ratings and leaderboard\nmaintenance for the participating Agents.\n5\nUsage Example of ModelScopeGPT\nIn this section,\nwe showcase a successful\napplication of ModelScope Community, Mod-\nelScopeGPT9, based on our ModelScope-Agent.\nModelScope Intelligent Assistant\nBased on\nModelScope-Agent , we have developed an intel-\nligent assistant for the ModelScope Community,\nnamely ModelScopeGPT. It uses LLMs as a con-\ntroller to connect dozens of domain-specific AI\nmodels in the ModelScope open-source community,\ncovering NLP, CV, Audio, and Multi-Modal fields.\nTo make the pipeline more practical, we have in-\ncluded API retrieval and knowledge retrieval tool to\nautomatically select proper APIs and get access to\nthe local ModelScope knowledge. As shown in Fig-\nure 3a, ModelScopeGPT can support API calls in\nmulti-turn conversations and generate correct API\n8https://modelscope.cn/studios/LLMZOO/Chinese-\nArena/summary\n9https://modelscope.cn/studios/damo/ModelScopeGPT\n/summary\ncall parameters using information from previous\nconversations. More cases can refer to Appendix C.\nAs a result, ModelScopeGPT has achieved a total\nrequest number of over 170k from 40k user visits\nwithin one month after its release.\nRegister and Use New Tools\nAnother key fea-\nture of an agent is its generalization capability to\nunseen APIs. This allows users to quickly register\ntheir own APIs and customize their specific applica-\ntions. Therefore, we test the generalization ability\nof ModelScopeGPT by applying it to an Alibaba\nCloud application scenario. As shown in Figure 3b,\nwe first found an API for renewing an ECS in-\nstance on Alibaba Cloud. Then, we registered the\nAPI schema defined in the tool library to the agent.\nFinally, we entered the prompt \"Please help me re-\nnew an ECS...\" in the demo. The agent generated a\nrequest through planning, selected the appropriate\nAPI, called the API to renew the instance success-\nfully, and provided a reply to inform the user that\nthe renewal was completed. This test demonstrates\nthat the open-source LLM optimized based on the\nreleased API dataset has a strong generalization\nability towards unseen APIs.\n6\nConclusion\nModelScope-Agent aims to facilitate building AI\nAgent applications and research based on open-\nsource LLMs by providing a general and customiz-\nable agent framework covering flexible system de-\nsign, data collection, model training, evaluation\nand usage example in real-world application. It\nprovides an open-source, community-driven library\ntowards AI Agent learning and best practices for\nbuilding an agent system with open-source LLMs.\nWe hope ModelScope-Agent can help pave the way\ntowards a new era of AI Agent.\nEthics Statement\nIntended Use.\nModelScope-Agent is designed\nto facilitate building AI Agent applications and\nresearch based on open-source LLMs, by providing\na general and customizable agent system.\nPotential Misuse.\nAlthough we have only trained\nwith the tool-use datasets and gone through certain\ndata filtering rules, it is still possible that the cus-\ntomized model may generate some biased, fake,\nand unsafe information. Our agent framework also\nprovides users with the freedom to select proper\nLLMs and upload their own clean data for training.\nIt is also important to design specific methods to\nimprove the safety of the agent framework in the\nfuture.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\ngen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\nmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do as i can,\nnot as i say: Grounding language in robotic affor-\ndances. arXiv preprint arXiv:2204.01691.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Heslow,\nJulien Launay, Quentin Malartic, et al. 2023. Falcon-\n40b: an open large language model with state-of-the-\nart performance.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, Pierre Sermanet,\nTomas Jackson, Noah Brown, Linda Luu, Sergey\nLevine, Karol Hausman, and brian ichter. 2023. In-\nner monologue: Embodied reasoning through plan-\nning with language models. In Proceedings of The\n6th Conference on Robot Learning, volume 205 of\nProceedings of Machine Learning Research, pages\n1769\u20131782. PMLR.\nHuggingface. 2023.\nTransformers agent.\nWebsite.\nhttps://huggingface.co/docs/transformers/\ntransformers_agents.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-\nbank: A benchmark for tool-augmented llms. arXiv\npreprint arXiv:2304.08244.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and\nJoseph E. Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023. Tool learning with foundation\nmodels. arXiv preprint arXiv:2304.08354.\nQwenLM. 2023. Qwen-7b.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face. arXiv preprint arXiv:2303.17580.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,\nQiao Liang, and Le Sun. 2023. Toolalpaca: Gener-\nalized tool learning for language models with 3000\nsimulated cases. arXiv preprint arXiv:2306.05301.\nTHUDM. 2023.\nChatglm.\nhttps://github.com/\nTHUDM/ChatGLM-6B.\nJunfeng Tian, Hehong Chen, Guohai Xu, Ming Yan,\nXing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu,\nWenshen Xu, Haiyang Xu, Qi Qian, Wei Wang, Qing-\nhao Ye, Jiejing Zhang, Ji Zhang, Fei Huang, and\nJingren Zhou. 2023. Chatplug: Open-domain gen-\nerative dialogue system with internet-augmented in-\nstruction tuning for digital human. arXiv preprint\narXiv:2304.07849.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\narXiv\npreprint arXiv:2302.13971.\nChenfei Wu,\nShengming Yin,\nWeizhen Qi,\nXi-\naodong Wang, Zecheng Tang, and Nan Duan.\n2023. Visual chatgpt: Talking, drawing and edit-\ning with visual foundation models. arXiv preprint\narXiv:2303.04671.\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge,\nXiu Li, and Ying Shan. 2023. Gpt4tools: Teaching\nlarge language model to use tools via self-instruction.\narXiv preprint arXiv:2305.18752.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judg-\ning llm-as-a-judge with mt-bench and chatbot arena.\narXiv preprint arXiv:2306.05685.\nA\nLibrary\nA.1\nTool List\nAPI Name (language)\nDescription\nType\nText-to-Image(en)\nConverts text to an image.\nModel API\nText-to-Image(zh)\nConverts text to an image.\nModel API\nText-to-Video(en)\nConverts text to a video.\nModel API\nText-to-Audio(en)\nConverts text to audio.\nModel API\nText-to-Audio(zh)\nConverts text to audio.\nModel API\nImage-Chat(en)\nImage chat.\nModel API\nTranslation-zh2en\nTranslates Chinese text to English.\nModel API\nTranslation-en2zh\nTranslates English text to Chinese.\nModel API\nUniversal-IE(zh)\nExtracts structured information.\nModel API\nText-to-Geographic(zh)\nExtracts geographic information.\nModel API\nNER(zh)\nRecognizes named entities in text.\nModel API\nAPI-Retrieval\nRetrieves relevant APIs\nCommon API\nModelScope-Retrieval\nRetrieves modelscope docs.\nCommon API\nTable 2: The statistics of default tool list. Supported\ninput languages for the APIs are listed in parentheses.\nA.2\nCustomTool\nUser can customize their own tools by inheriting a\nbase tool and defining the tool names, descriptions,\nand parameters according to a pre-defined schema.\nMoreover, you can implement _local_call() or _re-\nmote_call() depending on your specific require-\nments. To illustrate, below is an example of a\ncustom tool:\nclass CustomTool(Tool):\ndescription = \u2019xxx\u2019\nname = \u2019xxx\u2019\nparameters: list = [{\n\u2019name\u2019: \u2019xxx\u2019,\n\u2019description \u2019: \u2019xxx\u2019,\n\u2019required \u2019: True\n}]\ndef _local_call ():\n...\ndef _remote_call ():\n...\nB\nExperiment Setup\nB.1\nEvaluation Benchmark\nTo assess the generalization of the trained agent,\nwe include 10 in-domain APIs that appear in the\ntraining set of ModelScope-Agent and 10 real un-\nseen APIs10. We also account for the multi-turn\nability of the agent by annotating several multi-turn\nscenarios in our evaluation benchmark. Our test\ninstances were annotated by asking the human ex-\nperts to write diverse instructions first. Then the\nhuman experts were ask to write the JSON API\nrequest and answer the instructions properly after\nobtaining the API calling results. Our final testing\n10In progress, we will include more APIs in the future.\ndataset consisted of 360 conversations with 2059\ntext snippets as the references to be compared with\nthe agent prediction, which comprise 798 API re-\nqusts and 1261 plain text answers according to the\nprevious calling results.\nB.2\nEvaluation Results\nModel\nROUGE-L\nAction EM\nArgument F1\nChatGPT (2-shot)\u2217\n36.70\n34.82\n25.51\nLLaMA\n39.16\n58.60\n44.98\nChatPLUG11\n46.45\n68.29\n55.12\nMSAgent-Qwen12\n51.35\n87.23\n68.09\nTable 3: Automatic evaluation results. \u2217 represents that\nwe do not fine-tune ChatGPT but use in-context learning\nwith 2 demonstrations.\nWe compare the models trained in our proposed\nModelScopeGPT. The automaction evaluation re-\nsults are shown in Table 3. Based on the findings\nobtained from our experimentation, it is evident\nthat ChatGPT with in-context learning yielded infe-\nrior results as compared to other models that were\nsubjected to finetuning. Furthermore, LLaMA un-\nderperformed when compared to other finetuned\nmodels. Our error study revealed that the lower\nperformance of ChatGPT and LLaMA could be at-\ntributed to a large proportion of Chinese test cases\nin our test set. The models (ChatPLUG, Qwen) that\nperformed better were those that predominantly fo-\ncused on Chinese data. Our investigation revealed\nthat ChatGPT and LLaMA exhibited limitations\nin user intent recognition, which ultimately led\nto their suboptimal performance on Action EM.\nAmong the models examined, Qwen displayed the\nmost favorable performance, which could be at-\ntributed to the superior performance of its basic\nmodel.\nB.3\nWeighted LM\nWe give an example of the training strategy\nWeighted LM. As show in Figure 4, tokens with\ndifferent colors have different loss weights. For the\nuser input prompt, we set the loss weight to 0, so\nthat the model does not calculate the loss for the\nprompt. For the API-Agnostic text of the assistant,\nwe keep the loss weight as 1. Finally, for the im-\nportant text of the API calling, such as API name,\nparameters, URL, etc., we set the loss weight to 2,\nwhich can improve the generation accuracy of API\ncalling.\nFigure 4: Example of training strategy for weighted LM. Different colored tokens have different loss weights.\nFigure 5: Single-step tool-use instructions, text-to-video cases. We have captured a few frames of the video to\ndisplay. Testing the model using the same semantic instruction in both English (left) and Chinese (right).\nFigure 6: Single-step tool-use instructions, image-chat cases. Testing the model using the same semantic instruction\nin both English (left) and Chinese (right).\nC\nCases\nIn this section, we show the qualitative results\nabout ModelScopeGPT implementation based on\nModelScope-Agent.\nSingle-step Tool Use\nAs shown in Figure 5 and\n6, the instruction expects the model to generate a\nvideo and chat about the image respectively. These\ninstructions can be completed with a single step of\ntool use.\nMulti-step Tool Use\nAs shown in Figure 7, the\ninstruction expects the model to write the promo-\ntional copy first, then read it, and finally generate a\nvideo. These instructions require the model to have\nthe ability of multi-step Tool use. In the Chinese\ncase, our model accurately completed the three-\nstep tool use.\nMulti-turn Tool Use\nAs shown in Figure 8, the\ninstruction requires the model to have the ability to\nmulti-turn conversation and use the history conver-\nsation. Our model can accurately call the API and\ncapture the content of the previous conversation to\ngenerate API parameters.\nFigure 7: Multi-step tool-use instructions. We have captured a few frames of the video to display. Testing the model\nusing the same semantic instruction in both English(left) and Chinese(right).\nFigure 8: Multi-turn tool-use instructions, text-to-speech and text-to-image cases. Testing the model using the same\nsemantic instruction in both English(left) and Chinese(right).\nFigure 9: Multi-turn tool-use instructions, text-to-speech and text-to-image cases. Testing the model using the same\nsemantic instruction in both English(left) and Chinese(right).\nIn-domain Knowledge QA\nAs shown in Figure\n9, the instruction requires the model to retrieve in-\ndomain knowledge and use the retrieved knowledge\nto answer questions.\nas User\nas Agent\nAPI Gallery\nInstruction or\nClarification\nAPI request\nFollow-up or\nFinal Answer\nResult\nFigure 10: The data collection procedure of MSAgent-\nBench.\nD\nData Collection Procedure\nWe collected our dataset by using prompt engineer\nto simulate the agent scenarios with two ChatG-\nPTs (gpt-3.5-turbo). One of the ChatGPTs was\nprompted to act as the user, while the other was\nassigned to act as the agent. In order to expand\nthe domains and functionalities of APIs presented\nin the training data, rather than the exsisting real\nAPIs, we also included a number of synthetic APIs\nthat were generated by ChatGPT. When these syn-\nthetic APIs were incorporated into the dialogues,\nwe prompted another ChatGPT to serve as the API\nand return the relevant calling outcomes.\nThe data collection procedure is shown in Fig-\nure 10. Initially, a set of random in-context demon-\nstrations were provided to ChatGPT for generating\nan instruction. This instruction could either be a\nregular one or one that requires solving with APIs,\ndepending on the demonstrations provided. Subse-\nquently, ChatGPT was prompt to act as an agent by\nfirst thinking about which action to undertake. If\nno API calls were deemed necessary, or if the user\nclarification is needed, the agent would respond\nwith a follow-up response to the user. Otherwise\nthe agent will send API request to the API gallery.\nAfter receiving the result of the API call, the agent\nwould assess the situation and decide on the next ac-\ntion. This iterative process of the \"user-agent-API\"\nloop would continue until the agent determined\nthat it was appropriate to terminate the conversa-\ntion with the final answer. After acquiring the raw\ndataset, we applied filtering mechanisms to elim-\ninate instances in which ChatGPT generated API\nrequests containing hallucinated API names and\nparameters that were absent from the retrieved API.\nAdditionally, we excluded instances in which Chat-\nGPT generated illegal API requests, thus resulting\nin a refined and finalized dataset.\nAs introduced in Section 3.1, we collect in-\nstances across different languages and topics, the\ndetailed statistics of our collected data are shown\nin Table 4.\nInstance Type\n# Instances\nChinese\n532,436\nEnglish\n66,444\nCommon API\n211,026\nModel API\n58,338\nAPI-Oriented QA\n5,000\nAPI-Agnostic Instruction\n329,776\nTable 4: The statistics of our collected dataset.\nE\nRelated Work\nE.1\nLarge Language Models\nRecent years have witnessed rapid development in\nthe field of Large Language Models (LLMs). Typ-\nical models, such as GPT3 (Brown et al., 2020),\nGopher (Rae et al., 2021), Chinchilla (Hoffmann\net al., 2022), PaLM (Chowdhery et al., 2022) and\nLLaMA (Touvron et al., 2023), have shown im-\npressive zero and few-shot generalization abilities\non a wide range of NLP tasks, by scaling up the\nmodel and data size. A remarkable milestone is the\nrelease of ChatGPT (OpenAI, 2022) or GPT4 (Ope-\nnAI, 2023), which has greatly revolutionized the\nparadigm of AI development. As a result, a rising\ntrend of open-source LLMs has emerged to chal-\nlenge and catch up their closed-source counterparts\nlike ChatGPT and Claude, such as BLOOM (Muen-\nnighoff et al., 2022), LLaMA (Touvron et al.,\n2023), Falcon (Almazrouei et al., 2023), Chat-\nGLM (THUDM, 2023). Despite the great break-\nthrough, LLMs are trained as text generators over\nplain text corpora, thus performing less well on\nother tasks such as multi-modal tasks. It also falls\nshort on tasks that require up-to-date information,\nwhich are beyond the pretraining data. Using tools\nor external APIs can help overcome the limitations\nand harnesses the power of LLMs to facilitate seam-\nless connections with downstream applications. In\nModelScope-Agent , we provide the whole cus-\ntomizable framework and best practices for build-\ning an agent system, which enables open-source\nLLMs to use tools and external APIs.\nE.2\nAgent & Tool Learning\nThe utilization of Large Language Models (LLMs)\nas a controller to construct an agent system has\nemerged as a prominent research area. Several re-\nlated works employ prompt engineering techniques\non closed-source LLMs, such as ChatGPT (Ope-\nnAI, 2022) and Claude, to enable their applica-\ntion in specific domains.\nFor instance, Visual-\nChatGPT (Wu et al., 2023) and HuggingGPT (Shen\net al., 2023) facilitate the HuggingFace model call-\nings accessible to OpenAI LLMs. SayCan (Ahn\net al., 2022) and inner monologue (Huang et al.,\n2023) integrate LLMs with robots to achieve\nrobotic systems.\nNotably, recent works such\nas Langchain and Auto-GPT encompass a wide\nrange of tools, including common APIs and neu-\nral models, and enhance long-term reasoning\nand human-agent interaction whilst solving tasks,\nwhich demonstrate the immense potential for build-\ning a generalized agent.\nNumerous endeavors have also been made\nto enable open-source LLMs to utilize tools.\nFor instance, Gorilla (Patil et al., 2023) and\nGPT4Tools (Yang et al., 2023) generate training\ndata using self-instruction techniques to train open-\nsource LLMs to effectively utilize neural mod-\nels. ToolAlpaca (Tang et al., 2023) and ToolL-\nLaMA (Qin et al., 2023) train LLAMA using com-\nmon APIs, with the distinction that ToolAlpaca\nemploys synthetic APIs from LLMS, whereas Tool-\nLLaMA utilizes real APIs.\nOverall, compared to the above-mentioned meth-\nods, ModelScope-Agent differs in the following\naspects. Firstly, our method includes a universal\ntraining framework that supports user-customized\nagent learning for open-source models to meet in-\ndustrial needs. Secondly, ModelScope-Agent can\nsupport various APIs in different fields, including\nmodel APIs and common APIs, while previous\nworks only support certain specific APIs.\n"
  },
  {
    "title": "Efficient RLHF: Reducing the Memory Usage of PPO",
    "link": "https://arxiv.org/pdf/2309.00754.pdf",
    "upvote": "13",
    "text": "EFFICIENT RLHF: REDUCING THE MEMORY\nUSAGE OF PPO\nMichael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, Yelong Shen\nMicrosoft\n{misantac,yadonglu,hanyu,yuanzhili,yelong.shen}@microsoft.com\nABSTRACT\nReinforcement Learning with Human Feedback (RLHF) has revolutionized lan-\nguage modeling by aligning models with human preferences. However, the RL\nstage, Proximal Policy Optimization (PPO), requires over 3x the memory of Su-\npervised Fine-Tuning (SFT), making it infeasible to use for most practitioners. To\naddress this issue, we present a comprehensive analysis the memory usage, perfor-\nmance, and training time of memory-savings techniques for PPO. We introduce\nHydra-RLHF by first integrating the SFT and Reward models and then dynamically\nturning LoRA \"off\" during training. Our experiments show: 1. Using LoRA during\nPPO reduces its memory usage to be smaller than SFT while improving alignment\nacross four public benchmarks, and 2. Hydra-PPO reduces the latency per sam-\nple of LoRA-PPO by up to 65% while maintaining its performance. Our results\ndemonstrate that Hydra-PPO is a simple and promising solution for enabling more\nwidespread usage of RLHF.\n1\nIntroduction\nSince ChatGPT, GPT-4, and Llama-2 family models entered the public sphere, they have impressed\nusers with their ability to be helpful assistants for a surprising number of tasks [1, 2, 3, 4, 5]. One key\nto their success, along with many other foundation models [6], is model alignment through RLHF.\nTraining a massive language model results in a network with a large amount of knowledge, however,\nit is not trained to discriminate within that knowledge, which could cause undesired behaviour and\npossibly lead to societal harm [7]. Alignment aims to solve this issue by adjusting the model\u2019s\nbehaviour and has become an integral part for creating safe and controllable foundation models [8, 9].\nWhile RLHF improves model alignment it is limited in usage, being both highly complex and\ndemanding a massive amount of memory when loading and training multiple models during PPO\n[10, 11]. Because the use of RLHF is in its infancy, there is a strong need to evaluate its variations in\nterms of speed and performance.\nTo address this need, we delve into the training process and model architectures of standard RLHF-\nPPO. Through this investigation, we identify substantial opportunities for memory/computation cost\nreduction through the implementation of model-sharing between Reference/Reward Models and\nActor/Critic Models.\nGiven these findings, we propose Hydra-PPO to reduce the number of trained and static models in\nmemory during PPO. We perform run-time and performance comparisons to show these memory\nsavings can then be utilized to increase the training batch size, reducing the per-sample latency of\nPPO by up to 65%.\nPreprint.\narXiv:2309.00754v1  [cs.LG]  1 Sep 2023\nMethod\nBatch\nSize\nGPU Memory (GB)\nLatency per Sample (seconds)\nModel\nActivation\nTotal\nInference\nUpdate\nTotal\nPPO\n1\n111.8*\n101.3*\n220*\n-\n-\n-\nLoRA-PPO\n1\n53.2\n12.5\n68.0\n17.23\n1.52\n18.75\nJ-Hydra-PPO\n4\n14.3\n51.4\n67.9\n4.63\n0.38\n5.01\nHydra-PPO\n4\n15.9\n52.8\n71.1\n4.88\n1.59\n6.47\nTable 1: Comparison of Memory Usage and Run-Time between methods for Llama 7b on Stack-\nExchange per A100 80GB GPU. See Appendix B for details. *For Full Fine-Tuning PPO, memory\nusage is a scaled-up estimate.\nActor\n+\n(LoRA Off) Reference\nCritic\n+\n(LoRA Off) Reward\nLoRA-PPO\nDecoder\nCLM\nModel 1\n+\nLoRA\nDecoder\nRM\nModel 2\n+\nLoRA\nActor\nReference\nFull Finetune PPO\nModel 1\nCLM\nReward\nCritic\nModel 2\nRM\nRM\nCLM\nModel 1\nFinetuned\nModel 2\nFinetuned\nActor or Critic\n+\n(LoRA Off) Reference/Reward\nHydra-PPO\nDecoder\nRM\nLoRA \n(Actor)\nCLM\nActor/Critic\n+\n(LoRA Off) Reference/Reward\nJoined-Hydra-PPO\nDecoder\nRM\nLoRA\nCLM\nLoRA\n(Critic)\nFigure 1: Models used in PPO methods. CLM indicates a Causal Language Modeling head, RM\nindicates a Reward Modeling head. Light purple weights are trained and dark blue weights are frozen.\n2\nRLHF\nIn this section, we first introduce the standard RLHF method [12, 10, 11, 13].\nStage 1: Supervised Fine-Tuning (SFT)\nan input LLM is trained using the standard causal\nlanguage model training objective Lxent on a set of data D, yielding language model \u03c0SFT. We call\nthis FFT-SFT when all parameters are trained, or LoRA-SFT when using LoRA [14].\nStage 2: Reward Model (RM) Training\nthe head of a LLM is replaced with a scalar output.\nThis model r\u03d5(x, y) is trained to predict human preference given a dataset of preference pairs\nwith prompt x and completion y. After training, the reward function is often normalized such\nthat Ex\u223cD,y\u223c\u03c0SFT(y|x)[r\u03d5(x)] = 0 to improve PPO stability. The reward model is trained with loss\nLR(r\u03d5, D) = \u2212 E(x,yw,yl)\u223cD[log( \u03c3(r\u03d5(x, yw) \u2212 r\u03d5(x, yl) )], where yw is the \"winning\" answer as\ncompared to yl for prompt x, according to the target alignment source.\nStage 3: PPO\n\u03c0SFT and r\u03d5(x, y) are used to initialize and subsequently train an actor and critic\nwith PPO [11, 10]. During training, there are at minimum1 four models used:\n\u2022 Reference: \u03c0ref, a frozen copy of \u03c0SFT, used to prevent reward divergence.\n\u2022 Actor: called \u03c0\u03b8, the trained generative model or policy, initialized as a copy of \u03c0SFT.\n\u2022 Reward: a frozen copy of r\u03d5(x, y), used to calculate the reward of outputs from the Actor.\n\u2022 Critic or Value Function: V (x, y), a copy of r\u03d5(x, y) trained to estimate sequence returns.\n1Other models may be added [10]; we stick to the most common and simplest setup in our paper.\n2\nUsing output probability ratio r(\u03b8) = \u03c0\u03b8(y | x)\n\u03c0old(y | x), PPO optimizes the surrogate objective LCLIP(\u03b8) =\nE[min(r(\u03b8) \u02c6A, clip(r(\u03b8), 1\u2212\u03f5, 1+\u03f5) \u02c6A] . Generalized advantage estimation uses V (x, y) to construct\nadvantage estimates \u02c6A from the reward [15, 16]. V (x, y) is trained with squared-error loss on the\nreturns. We use LoRA [14] on all linear layers of \u03c0\u03b8 and V (x, y), which we call LoRA-PPO. We do\nnot perform experiments with Full Fine-Tuning PPO due to its extreme cost.\n3\nHydra-RLHF\nWe introduce Hydra-RLHF as a set of modifications to RLHF. We define a decoder-based model\n\u03c0hydra with two linear heads: 1) a head serves as the causal head, predicting the subsequent token for\na sequence, and 2) another head serves as the reward model head, providing the immediate reward\nassociated with the same input. Multi-headed models are well-explored both in general [17, 18] and\nwith respect to reinforcement learning [16, 19, 20].\nStage 1: Hydra-SFT\nUsing a similar dataset to standard RM training, \u03c0hydra is trained by optimizing\nL\u03c0hydra(x, yw, yl) = Lxent(x, yw) + \u03b3L\u03b8(x, yw, yl), where \u03b3 is a weighting multiplier. In practice, we\nfind \u03b3 = 0.1 generally works well. We call this Hydra-FFT when training all parameters.\nThere are additional requirements for \u03c0hydra compared to regular RM or SFT fine-tuning.\nL\u03c0hydra(x, yw, yl) requires pairwise comparison data to train both heads, making standard SFT\ndatasets unusable. Additionally, RM training can incorporate feedback from a list of rankings,\ne.g. y1 > y2 > y3, by making pairs for all ranking combinations. For \u03c0hydra, only pairs containing the\nsample with the best ranking should be considered to avoid training the SFT head on other samples.\nDynamic LoRA\nWe introduce Dynamic LoRA as a simple and helpful technique to conserve\nmemory usage in LoRA-PPO. Because \u03c0\u03b8 and \u03c0ref are initialized as copies of \u03c0SFT, training \u03c0\u03b8 with\nLoRA [14] means the only difference between them is the LoRA weights. Rather than loading \u03c0SFT\ntwice, \u03c0ref can be recovered from the actor by \"turning off\" LoRA. Thus, we define \u03c0ref \u2190 LO(\u03c0\u03b8),\nwhere LO ignores any LoRA parameters. We add r\u03d5(x, y) \u2190 LO(V (x, y)) for the Critic/Reward\npair, saving about 20% of memory while maintaining performance equivalent to LoRA-PPO.\nStage 2: Hydra-PPO\nTwo separate sets of LoRA weights are added to the same base model \u03c0hydra,\none set for the actor and one set for the critic, in order to create \u03c0RL-hydra\n\u03b8\n. When the actor is required,\nonly the actor LoRA weights are used, and similarly for the critic. Utilizing dynamic LoRA, we\ndefine (\u03c0hydra\nref\n, rhydra\n\u03d5\n(x, y)) \u2190 LO(\u03c0RL-hydra\n\u03b8\n). Only one full base model is required in memory during\nPPO, leading to similar overall memory usage to LoRA finetuning given the same batch size.\nAs an ablation study, we also include results of Joined Hydra-PPO or J-Hydra-PPO, which uses only\none set of LoRA weights for both actor and critic. While this saves a small amount of memory and\nrun-time, we find that it performs worse than Hydra-PPO. This is an interesting contrast to Hydra-SFT\nwhere joining the models does not affect performance.\nMethod\n# of Static Models\n# of LoRA Weight Sets\nFull Fine-Tuning PPO\n4\n0 (fully finetuned)\nLoRA-PPO\n4\n2\nDynamic LoRA-PPO\n2\n2\nJoined Hydra-PPO\n1\n1\nHydra-PPO\n1\n2\nTable 2: Summary of all PPO methods and number of models.\n4\nExperiments\nResults are presented across four datasets using Llama 7b [5] or OPT 1.3b [21]. We employ GPT-4\nto evaluate model performance in general [22, 8, 23, 24], and for the summarization task, we use also\nROUGE scores[25] .\n3\nIn the empirical study, we evaluate five approaches: SFT, LoRA-PPO, Hydra-SFT, J-Hydra-PPO, and\nHydra-PPO. Specifically, LoRA-PPO is initialized with the SFT model, while both J-Hydra-PPO and\nHydra-PPO are initialized with the Hydra-SFT model. All experiment hyperparameters are listed in\nAppendix B. Perplexity and RM accuracy before PPO is listed in Appendix B. Our code is forked\nfrom DeepSpeed-Chat [26, 27].\nThe performance of PPO can be highly inconsistent due to its unstable nature and varying implemen-\ntations [28, 13, 29, 11]. PPO can even reduce performance by exploiting flaws in the reward model,\nwhich we observe in the StackExchange dataset.\nResults Overview\nTables 3 and 9 show the expected win-rates of each method against all other\nmethods, as evaluated by GPT-4. The findings indicate that PPO outperforms SFT on average and\nHydra-PPO similarly improves Hydra-SFT. The specific win-rates per dataset are provided in detail.\nThe performance of SFT and Hydra-SFT are comparable, suggesting that combining the RM and SFT\nobjectives within a single model does not consistently lead to improvements or hinder the generation\nperformance across different tasks.\nBoth Hydra-PPO and LoRA-PPO improve over their respective base models, however, Hydra-PPO\nachieves better alignment than LoRA-PPO for Llama 7b. This may be explained by the better Reward\nmodel from Hydra-SFT which enables overall better PPO performance. The detailed accuracy of\nthe RM models in SFT and Hydra-SFT is shown in Appendix F. Overall, the study indicates that\nPPO improves model alignment and there is potential for further enhancing PPO performance by\nimproving the RM.\nFor Learning to Summarize, we additionally evaluate their performance using ROUGE scores in\nTable 4, and these results consistently align with the findings presented in Table 3. An interesting\nobservation is that the SFT-based approach typically yields better precision performance, whereas\nPPO-based methods substantially enhance recall. This trend could potentially be attributed to the\nencouragement of longer text generation during the PPO stage.\nJoined-Hydra-PPO Underperformance\nJ-Hydra-PPO, which uses only one set of LoRA weights\nfor actor and critic, performs significantly worse than two separate sets (Hydra-PPO). We speculate\nthis is due to combining actor and critic model amplified the unstable nature of PPO [28, 13, 29, 11].\nSince J-Hydra-PPO is more memory and computation efficient than Hydra-PPO, we hope future\nwork may improve its performance.\nMethod\nGPT-4-LLM\nOpen-Source\nAssistant\nLearning to\nSummarize\nStackExchange\nAverage\nSFT\n48.18\n48.35\n45.95\n51.73\n48.55\nLoRA-PPO\n48.8\n49.03\n55.48\n49.4\n50.68\nHydra-SFT\n48.48\n49.65\n42.63\n53.23\n48.50\nJ-Hydra-PPO\n50.43\n52.05\n43.13\n40.38\n46.50\nHydra-PPO\n54.13\n51\n61.58\n55.38\n55.52\nTable 3: Llama 7b expected aggregate win-rate per method. We measure total wins and ties for each\nmethod against all other methods, then use this to calculate expected win-rate.\nModel\nROUGE-1\nROUGE-L\nPrecision\nRecall\nF-Measure\nPrecision\nRecall\nF-Measure\nSFT\n90.69\n13.12\n21.69\n75.56\n11.35\n18.59\nLoRA-PPO\n88.93\n14.70\n23.95\n71.46\n12.25\n19.77\nHydra-SFT\n87.86\n13.27\n21.42\n72.92\n11.42\n18.27\nJ-Hydra-PPO\n84.13\n16.93\n25.00\n70.82\n14.92\n21.81\nHydra-PPO\n88.91\n19.21\n29.31\n72.45\n16.43\n24.73\nTable 4: Llama 7b ROUGE-1 and ROUGE-L scores for all models on the Learning to Summarize\ndataset.\n4\nThroughput Comparison\nFigure 2 shows there is a roughly linear relationship between throughput\nand sequence length in log-space for all methods. Latency is measured as a sum of inference latency\nand parameter update latency per sample during PPO. As we can see from the figure, Hydra-PPO\nsaves exponentially more time as sequence length increases. We increase batch size to max out\nmemory usage for all methods, but use gradient accumulation to ensure the effective total batch size\nis the same. Hydra-PPO and J-Hydra-PPO converge at sequence length 1024 as the inference increase\novertakes update latency. Table 1 shows a detailed comparison for a specific experiment.\n256\n512\n1024\n2048\nTotal Sequence Length (tokens)\n1.00\n10.00\n100.00\nLatency per Sample (seconds)\nJ-Hydra-PPO\nHydra-PPO\nLoRA-PPO\nFigure 2: Latency (seconds) per Sample per PPO method as sequence length increases. Both axes\nuse log scaling. LoRA-PPO is unable to fit in memory for our setup for context length 2048. See\nAppendix B for details.\n4.1\nGPT-4-LLM\nGPT-4-LLM [22] consists of instruction-following prompts with responses sampled from multiple\nfoundation models, including GPT-4, GPT-3.5, and OPT-IML. The responses of each model are\nranked by GPT-4. We pair only the highest-scoring response with each other response. To our\nknowledge, we are the first to attempt full RLHF (including PPO) on this dataset. Overall, we observe\nthe most consistent and well-behaved training runs with GPT-4-LLM.\n-\nHydra-FFT\nLoRA-PPO\nJ-Hydra-PPO\nHydra-PPO\nSFT\n40.6 / 43.8\n43.6 / 43.4\n43.2 / 46.8\n39.0 / 47.0\nHydra-FFT\n-\n43.4 / 45.2\n40.8 / 44.0\n38.8 / 49.2\nLoRA-PPO\n-\n-\n40.8 / 44.4\n40.0 / 47.6\nJ-Hydra-PPO\n-\n-\n-\n38.6 / 45.6\nTable 5: Llama 7b GPT-4-LLM win-rates as judged by GPT-4. Results in each cell are presented as\n\"Row Win % / Column Win %\" with the remainder being ties.\n4.2\nOpen-Source Assistant Datasets\nWe perform RLHF on the default data for DeepSpeed-Chat [26, 27]. At the time of writing, these\ndatasets include \"Dahoas/rm-static\", \"Dahoas/full-hh-rlhf\", \"Dahoas/synthetic-instruct-gptj-pairwise\"\nand \"yitingxie/rlhf-reward-datasets\", all hosted on HuggingFace. We call the combination \"Open-\nSource Assistant Datasets\". These are various open-source ChatBot or Assistant style datasets, with\none including Helpful & Harmless [9]. We train on them without modification.\n5\n-\nHydra-FFT\nLoRA-PPO\nJ-Hydra-PPO\nHydra-PPO\nSFT\n42.2/ 44.4\n41.7 / 42.2\n40.6 / 45.2\n39.4 / 45.4\nHydra-FFT\n-\n45.4 / 38.4\n37.8 / 45.0\n38.6 / 43.6\nLoRA-PPO\n-\n-\n43.0 / 44.4\n42.6 / 42.8\nJ-Hydra-PPO\n-\n-\n-\n45.6 / 42.4\nTable 6: Llama 7b Open-Source Assistant Datasets win-rates as judged by GPT-4. Results in each\ncell are presented as \"Row Win % / Column Win %\" with the remainder being ties.\n4.3\nLearning to Summarize\nThe Reddit TL;DR dataset [30] has been previously used in multiple RLHF works [31, 12]. We use\nthe dataset as modified by [31], where each prompt only contains one preference completion pair.\n-\nHydra-FFT\nLoRA-PPO\nJ-Hydra-PPO\nHydra-PPO\nSFT\n41.0 / 38.4\n31.6 / 44.8\n41.0 / 32.8\n31.4 / 47.2\nHydra-FFT\n-\n33.4 / 46.4\n37.8 / 36.4\n33.8 / 42.4\nLoRA-PPO\n-\n-\n51.8 / 30.0\n42.6 / 36.8\nJ-Hydra-PPO\n-\n-\n-\n23.7 / 52.6\nTable 7: Llama 7b Learning to Summarize win-rates as judged by GPT-4. Results in each cell are\npresented as \"Row Win % / Column Win %\" with the remainder being ties.\n4.4\nStackExchange\nThe StackExchange [32] dataset has previously been used to train StackLlama via RLHF [33]. Each\ndata sample consists of one question with multiple completions ranked by votes from users. We\nre-create this experiment with 150k samples from StackExchange, with a change in that we pair only\nthe best answer with up to 3 other answers. This is done to avoid over-training on the best sample in\nHydra-SFT, but in addition, we find that the most up-voted answers are on average longer than the\nother answers, leading to trivial reward models.\nStackExchange is the most difficult dataset we test, containing extremely diverse and specific\nquestions. During PPO, models often learn to repeat their answers. Despite multiple attempts, both\nthe PPO and J-Hydra-PPO models encounter this issue while Hydra-PPO does not.\n-\nHydra-FFT\nLoRA-PPO\nJ-Hydra-PPO\nHydra-PPO\nSFT\n41.2 / 42.4\n46.4 / 42.0\n51.8 / 35.0\n42.4 / 48.6\nHydra-FFT\n-\n46.4 / 43.2\n54.2 / 32.4\n45.2 / 45.6\nLoRA-PPO\n-\n-\n52.6 / 34.8\n36.8 / 51.8\nJ-Hydra-PPO\n-\n-\n-\n35.2 / 56.6\nTable 8: Llama 7b StackExchange win-rates as judged by GPT-4. Results in each cell are presented\nas \"Row Win % / Column Win %\" with the remainder being ties.\n4.5\nChanging Model Size and Family\nWe extend our experimentation to explore the SFT and PPO approaches using the OPT-1.3b model.\nFor this model, we find that Hydra-SFT performs worse than the SFT model. Additionally, we find\nLoRA-PPO has better overall alignment than Hydra-PPO for OPT-1.3b. We speculate this difference\nto be due to the capacity of the model. For the smaller 1.3b model, combining language and reward\nmodels may be more difficult. Overall, we observe the same trend in increased performance after\nPPO and Hydra-PPO over their respective base models.\n6\nMethod\nGPT-4-LLM\nOpen-Source\nAssistant\nAverage\nSFT\n45.65\n52.5\n49.08\nLoRA-PPO\n59.5\n53.7\n56.6\nHydra-SFT\n44.4\n42.58\n43.49\nJ-Hydra-PPO\n48.2\n46.78\n47.49\nHydra-PPO\n50.2\n54.45\n52.33\nTable 9: OPT 1.3b expected aggregate win-rate per method. We measure total wins and ties for each\nmethod against all other methods, then use this to calculate expected win-rate.\n-\nHydra-FFT\nLoRA-PPO\nJ-Hydra-PPO\nHydra-PPO\nSFT\n52.2 / 36.6\n41.2 / 44.0\n52.6 / 39.4\n43.2 / 49.2\nHydra-FFT\n-\n35.8 / 53.4\n42.4 / 49.8\n37.8 / 56.6\nLoRA-PPO\n-\n-\n50.8 / 41.2\n46.2 / 46.6\nJ-Hydra-PPO\n-\n-\n-\n39.2 / 49.6\nTable 10: OPT 1.3b GPT-4-LLM win-rates as judged by GPT-4. Results in each cell are presented as\n\"Row Win % / Column Win %\" with the remainder being ties.\n-\nHydra-FFT\nLoRA-PPO\nJ-Hydra-PPO\nHydra-PPO\nSFT\n45.0 / 44.0\n31.8 / 54.8\n41.2 / 47.4\n39.8 / 50.0\nHydra-FFT\n-\n34.6 / 53.0\n32.0 / 54.6\n42.4 / 44.2\nLoRA-PPO\n-\n-\n53.0 / 35.8\n52.0 / 33.6\nJ-Hydra-PPO\n-\n-\n-\n39.0 / 47.0\nTable 11: OPT 1.3b Open-Source Assistant Datasets Preference as judged by GPT-4. Results in each\ncell are presented as \"Row Win % / Column Win %\" with the remainder being ties.\n5\nRelated Works\nAligning to Human Preference\nFoundation models have begun to emerge as all-purpose language\nmodels [6] which may be used without any domain adaptation [34, 1, 35]. While these models clearly\ncontain a large amount of knowledge and ability, they may contain unintended bias or respond in\nunintended ways to input questions from a user. Model alignment is the problem of slightly modifying\nthese models to interact with humans in a specific manner.\nHuman preference is difficult to quantify (and often inconsistent [13, 10]), making model alignment\nan open research area [36]. By assuming that classification is easier than generation, it is possible\nto train a reward model on a dataset of human preference labels. Such a reward model may then\nbe used to guide other models towards aligning to human preference, improving performance in a\nnontrivial way over Supervised fine-tuning (SFT) throughout many domains [37, 12, 9, 31, 8, 38, 39].\nRecently, this concept has exploded in popularity due to the success of InstructGPT and subsequent\nimprovements in ChatGPT and GPT-4 [10] which have delivered undeniably strong and human-like\ninteractions in a variety of domains.\nOther forms of feedback have been attempted due to the high cost of hiring humans to grade inputs.\nNow that massive foundation models exist, multiple works have attempted to use their feedback to\ntrain or evaluate other models [22, 8, 23, 24, 40, 41].\nAlignment during Supervised Fine-Tuning (SFT)\nDue to the complexity and high cost of PPO,\nsome recent works have sought to replace the training process of PPO while retaining its benefits.\nNotably, RAFT [42], RRHF [29], PRO [43], and DPO [13] are recent methods which combine\npreference data in some way with supervised fine-tuning. The former two are inspired by best-of-n\nsampling methods [44, 45, 46], while the latter two seek to wholly replace PPO by re-framing the\nsupervised training objective.\n7\nHydra-SFT shares similarities with these approaches by integrating ranked feedback into supervised\nfine-tuning. However, our work is orthogonal to these methods, aiming not to replace RLHF, but\nrather to make it more widely usable.\nDataset Formation\nHydra-RLHF requires that the SFT and RM training datasets be the same.\nPrevious works have found issues in over-fitting one of the heads when data is imbalanced [12, 31].\nOur experiments use datasets with pairwise comparisons for each sample so we find this over-fitting\nis not an issue, however, Hydra-RLHF could be extended to handle exceptions when data is limited.\nReward Model Size\nIn RLHF, the reward model can be smaller than the language model. We keep\nthese models the same size to make performance comparisons fair. In applied usage, Hydra-RLHF\ncomparatively saves less memory when standard RLHF uses a smaller reward model, however, this is\nalso an advantage for Hydra-RLHF; it uses a larger reward model for less training cost.\n6\nConclusion\nWe have performed a comparative study which analyzes the performance of different approaches\nto model alignment as graded by GPT-4. We find that LoRA-PPO improves alignment over FFT\nbut is costly to run. We introduce Hydra-RLHF as a method to save memory during PPO while\nmaintaining performance, which consists of two major parts: combining reference and reward models,\nand dynamically switching the active LoRA module during PPO. With the excess memory, Hydra-\nRLHF may use a higher batch size and therefore train with up to 65% faster per-sample latency.\nHydra-RLHF opens up the possibility for the community to apply RLHF for a wider variety of models\nand applications. We also see potential for future improvements, notably, balancing the SFT and RM\ndatasets, improving performance of J-Hydra-PPO, and improving PEFT methods for RLHF.\nAcknowledgments\nThank you to Vladimir Fomenko and Jialei Chen for helpful discussions.\nReferences\n[1] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith gpt-4, 2023.\n[2] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi\nYang. Is chatgpt a general-purpose natural language processing task solver?, 2023.\n[3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen,\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and\nJi-Rong Wen. A survey of large language models, 2023.\n[4] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan,\nShafiq Joty, and Jimmy Xiangji Huang. A systematic study and comprehensive evaluation of\nchatgpt on benchmark datasets, 2023.\n[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023.\n[6] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,\nJared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano\nErmon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto,\n8\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas\nIcard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling,\nFereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi,\nAnanya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa\nLi, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr,\nIsabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi\nRaghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack\nRyan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan\nSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang,\nWilliam Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga,\nJiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia\nZheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models,\n2022.\n[7] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On\nthe dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623,\nNew York, NY, USA, 2021. Association for Computing Machinery.\n[8] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine\nOlsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli\nTran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal\nNdousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer,\nNoemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston,\nShauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton,\nTom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben\nMann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.\nConstitutional ai: Harmlessness from ai feedback, 2022.\n[9] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath,\nJackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny\nHernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine\nOlsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann,\nand Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from\nhuman feedback, 2022.\n[10] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback, 2022.\n[11] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms, 2017.\n[12] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences,\n2020.\n[13] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model,\n2023.\n[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n[15] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation, 2018.\n[16] Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lill-\nicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning, 2016.\n9\n[17] Sebastian Ruder. An overview of multi-task learning in deep neural networks, 2017.\n[18] Michael Crawshaw. Multi-task learning with deep neural networks: A survey, 2020.\n[19] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander\nDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the\ngame of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, jan 2016.\n[20] Yannis Flet-Berliac and Philippe Preux. Merl: Multi-head reinforcement learning, 2020.\n[21] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n[22] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4, 2023.\n[23] Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo,\nand Minjoon Seo. Aligning large language models through synthetic feedback, 2023.\n[24] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval:\nNlg evaluation using gpt-4 with better human alignment, 2023.\n[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational\nLinguistics.\n[26] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu,\nAmmar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu\nZhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che,\nShuaiwen Leon Song, and Yuxiong He. Deepspeed-chat: Easy, fast and affordable rlhf training\nof chatgpt-like models at all scales, 2023.\n[27] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System\noptimizations enable training deep learning models with over 100 billion parameters. In\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining, KDD \u201920, page 3505\u20133506, New York, NY, USA, 2020. Association for\nComputing Machinery.\n[28] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry\nRudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A case\nstudy on ppo and trpo, 2020.\n[29] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf:\nRank responses to align language models with human feedback without tears, 2023.\n[30] Michael V\"olske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit to\nlearn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summa-\nrization, pages 59\u201363, Copenhagen, Denmark, September 2017. Association for Computational\nLinguistics.\n[31] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback,\n2022.\n[32] Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. Huggingface h4 stack\nexchange preference dataset, 2023.\n[33] Edward Beeching, Younes Belkada, Kashif Rasul, Lewis Tunstall, Leandro von Werra, Nazneen\nRajani, and Nathan Lambert. Stackllama: An rl fine-tuned llama model for stack exchange\nquestion and answering, 2023.\n[34] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language\nmodels, 2022.\n10\n[35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[36] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable\nagent alignment via reward modeling: a research direction, 2018.\n[37] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences, 2023.\n[38] Amelia Glaese, Nat McAleese, Maja Tr\u02dbebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-\nbeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,\nJonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth\nDathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So\u02c7na\nMokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William\nIsaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey\nIrving. Improving alignment of dialogue agents via targeted human judgements, 2022.\n[39] Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\nChristiano. Recursively summarizing books with human feedback, 2021.\n[40] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al.\nTextbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.\n[41] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak\ncoherent english? arXiv preprint arXiv:2305.07759, 2023.\n[42] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun\nShum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model\nalignment, 2023.\n[43] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.\nPreference ranking optimization for human alignment, 2023.\n[44] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems, 2021.\n[45] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds,\nDanny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language\nassistant as a laboratory for alignment, 2021.\n[46] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John\nSchulman. Webgpt: Browser-assisted question-answering with human feedback, 2022.\n[47] DeepSpeed.\nDeepspeed-chat.\nhttps://github.com/microsoft/DeepSpeed/tree/\nmaster/blogs/deepspeed-chat, 2023.\n[48] Xianghui Sun, Yunjie Ji, Baochang Ma, and Xiangang Li. A comparative study between\nfull-parameter and lora-based fine-tuning on chinese instruction data for instruction following\nlarge language model, 2023.\n[49] George Pu, Anirudh Jain, Jihan Yin, and Russell Kaplan. Empirical analysis of the strengths\nand weaknesses of peft techniques for llms, 2023.\n[50] Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky. Stack more\nlayers differently: High-rank training through low-rank updates, 2023.\n[51] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul.\nPeft:\nState-of-the-art parameter-efficient fine-tuning methods.\nhttps://github.com/\nhuggingface/peft, 2022.\n11\nA\nAlgorithm Pseudocode\nWe present pseudocode for J-Hydra-RLHF and Hydra-RLHF to aid in their implementation.\nAlgorithm 1 J-Hydra-PPO\nfor iteration=1,2,... do\nfor actor=1,2,...N do\nGenerate sequence using policy \u03c0hydra\n\u03b8old\nfor T sequence length steps, retaining critic head\nvalues for the last step\nTurn LoRA off for \u03c0hydra\n\u03b8old\nand use it to compute reward and reference log probabilites\nUse reward and reference log probability to calculate \u02c6A\nend for\nOptimize surrogate L wrt \u03c0hydra\n\u03b8\n, with K epochs and minibatch size M \u2264 NT\n\u03c0hydra\n\u03b8old\n\u2190 \u03c0hydra\n\u03b8\nend for\nAlgorithm 2 Hydra-PPO\nfor iteration=1,2,... do\nfor actor=1,2,...N do\nGenerate sequence using policy \u03c0hydra\n\u03b8old\nfor T sequence length steps\nTurn on the critic LoRA for \u03c0hydra\n\u03b8old\nand use it to calculate critic values on the generated\nsequence\nTurn LoRA off for \u03c0hydra\n\u03b8old\nand use it to compute reward and reference log probabilites\nUse reward and reference log probability to calculate \u02c6A\nend for\nOptimize surrogate L wrt \u03c0hydra\n\u03b8\n, with K epochs and minibatch size M \u2264 NT\n\u03c0hydra\n\u03b8old\n\u2190 \u03c0hydra\n\u03b8\nend for\nB\nExperiment Details\nSetup\nAll hyper-parameters used are listed in Tables 12 and 13. All experiments ran on a 8xA100\n80GB node. and batch size is measured per-GPU, making total batch size = 8 * batch size * gradient\naccumulation steps. All experiments use DeepSpeed ZeRO-1 [27, 47]. All experiments use a cosine\ndecaying learning rate scheduler.\nWe fix a common set of parameters. For all experiments, for SFT, RM, and Hydra-SFT, weight decay\nis set to 0.1. Reward models are normalized between (-1, 1). When using LoRA, SFT models use\na LoRA rank of 128. Across all SFT, RM, and Hydra-SFT experiments, we began by sweeping\nlearning rates {5e-5, 5e-6, and 5e-7} for 4 epochs, then adjusted parameters accordingly. We use the\nmodel with the best validation performance across epochs.\nFor all experiments, PPO, J-Hydra-PPO, and Hydra-PPO share the following: KL-Divergence \u03b2 =\n0.02, GAE \u03b3 = 1.0, GAE \u03bb = 0.95, warmup steps = 100, LoRA rank for actor and critic = 128, and\nall are run for 1 epoch. We track the mean of the reward from the last 20 steps, and use the model\nwith the best reward throughout training.\nRun-Time Measurement\nAll run-time numbers were obtained from training Llama 7b on Stack-\nExchange. For Table 1, we use a total sequence length of 800 tokens. Total GPU memory exceeds\nthe sum of Model and Activation due to other stored tensors used in training, while Total Latency\nper Sample is a direct sum of the two stages of PPO. Memory usage was measured with PyTorch.\nBecause Full PPO overflows our setup, we repeat the experiment using OPT 1.3b, and use the ratio\nbetween LoRA-PPO on Llama 7b to scale the numbers up.\n12\nPPO Hyperparameter Stability\nFor all PPO runs, we ensure reward increases during training\nand take the point of the highest reward without extreme and obvious divergence. In general, we\nfind J-Hydra-PPO to be highly unstable, taking multiple attempts to find solid hyperparameters.\nHydra-PPO is generally as stable as LoRA-PPO.\nHyperparameter\nGPT-4-LLM\nLearning to Sum.\nOpen-Source Assistant\nStackExchange\nPrompt Sequence Length\n256\n512\n256\n400\nCompletion Sequence Length\n256\n256\n256\n400\nSFT\nLearning Rate\n1e-5\n1e-4\n1e-3\n1e-4\nBatch Size\n4\n3\n4\n1\nGradient Accum. Steps\n1\n3\n1\n6\nEpochs\n3\n7\n7\n4\nRM\nLearning Rate\n5e-5\n5e-5\n5e-4\n1e-6\nBatch Size\n4\n1\n3\n1\nGradient Accum. Steps\n1\n4\n1\n12\nEpochs\n3\n3\n6\n3\nHydra-SFT\nLearning Rate\n3e-6\n5e-7\n1e-5\n5e-7\nReward Head Multiplier\n0.1\n0.1\n0.1\n0.07\nBatch Size\n4\n1\n4\n1\nGradient Accum. Steps\n1\n10\n1\n6\nEpochs\n3\n4\n4\n6\nLoRA-PPO\nActor Learning Rate\n5e-4\n8e-4\n5e-4\n5e-4\nCritic Learning Rate\n5e-5\n1e-4\n5e-5\n5e-5\nGeneration Batch Size\n6\n2\n6\n1\nMini-Batch Size\n8\n4\n8\n3\nGradient Accum. Steps\n5\n30\n3\n25\nJ-Hydra-PPO\nActor Learning Rate\n5e-3\n2.5e-4\n1e-4\n6e-4\nCritic Loss Multiplier\n0.1\n3\n0.1\n3\nGeneration Batch Size\n6\n2\n6\n1\nMini-Batch Size\n8\n4\n8\n3\nGradient Accum. Steps\n5\n50\n3\n25\nHydra-PPO\nActor Learning Rate\n5e-3\n5e-4\n5e-5\n7e-4\nCritic Learning Rate\n5e-5\n5e-5\n5e-6\n8e-4\nGeneration Batch Size\n6\n2\n6\n1\nMini-Batch Size\n8\n4\n8\n3\nGradient Accum. Steps\n5\n30\n3\n25\nTable 12: Hyperparameters for Llama 7b on all datasets.\n13\nHyperparameter\nGPT-4-LLM\nOpen-Source Assistant\nPrompt Sequence Length\n256\n256\nCompletion Sequence Length\n256\n256\nSFT\nLearning Rate\n5e-5\n1e-6\nBatch Size\n4\n4\nGradient Accum. Steps\n1\n1\nEpochs\n3\n4\nRM\nLearning Rate\n5e-5\n1e-5\nBatch Size\n4\n4\nGradient Accum. Steps\n4\n4\nEpochs\n3\n3\nHydra-SFT\nLearning Rate\n3e-6\n5e-5\nReward Head Multiplier\n.1\n.1\nBatch Size\n4\n4\nGradient Accum. Steps\n1\n1\nEpochs\n3\n4\nLoRA-PPO\nActor Learning Rate\n2.5e-4\n5e-4\nCritic Learning Rate\n5e-5\n5e-5\nGeneration Batch Size\n6\n6\nMini-Batch Size\n8\n8\nGradient Accum. Steps\n5\n3\nJ-Hydra-PPO\nActor Learning Rate\n5e-4\n5e-4\nCritic Loss Multiplier\n0.1\n0.1\nGeneration Batch Size\n6\n6\nMini-Batch Size\n8\n8\nGradient Accum. Steps\n5\n3\nHydra-PPO\nActor Learning Rate\n5e-4\n5e-4\nCritic Learning Rate\n5e-5\n1e-5\nGeneration Batch Size\n6\n6\nMini-Batch Size\n8\n8\nGradient Accum. Steps\n5\n3\nTable 13: Hyperparameters for OPT 1.3b on all datasets.\nC\nGPT-4 Grading\nSignificant consideration was given to the prompt for grading model responses. We experimented\nwith one prompt which contained two answers to decide from (e.g. \"out of answer one or two, which\nis better\"). However, we found that simply switching the order of the answers would switch the\npreferred answer by a large margin.\nInstead, we find the most consistent evaluation method to be grading each output on a 0-9 scale with\nstep-by-step reasoning, and then comparing these grades side-by-side. Each output is graded 3 times\nat temperature 1 and the results are averaged for a final score.\nWhen grading, we perform cleanup on answers from all models: removing EOT tokens, stopping\ngeneration upon detection of the model speaking for the \"other side\" (i.e. begins writing a new\ninstruction and response for Open-Datasets questions), and stopping generation upon detection of\n14\na 5-gram phrase sequence already found in the sequence. We evaluate using 500 samples from the\nvalidation set of each dataset.\nTo grade outputs on the GPT-4-LLM, Open-Datasets, and StackExchange datasets, we use the\nfollowing prompt:\nSystem Prompt:\nYou are an unbiased and harsh critic who judges the quality of answers from an assistant.\nUser Message:\nGiven a question from a user, rate the given answer from an assistant with an integer score out of\n9. Your rating can be any number from 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. First, give an extremely short,\n1-sentence evaluation of the answer. Do not give bullet points. Do not write more than one sentence.\nOnly write one total sentence evaluating the answer. A better answer is one that is more helpful,\nanswers the question more precisely, is more concise, is grammatically correct, is kinder and more\nfriendly, and does not contain any rude or confrontational speech. Give your answer in exactly this\nformat and then end the answer:\nLet us think step-by-step.\n[1 sentence evaluating the quality of the answer]\nThe integer score out of 9 for this answer is: X\nHere is the original conversation and then question from the user:\nInsert conversation and question\nHere is the answer from the assistant:\nInsert answer to judge\nTo grade outputs from Learning to Summarize, we use the following prompt:\nSystem Prompt:\nYou are an unbiased and harsh critic who judges the quality of summaries of a piece of text.\nUser Message:\nGiven a summary of a piece of text, rate the quality of the summary with an integer score out of\n9. Your rating can be any number from 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. First, give an extremely short,\n1-sentence evaluation of the summary. Do not give bullet points. Do not write more than one\nsentence. Only write one total sentence evaluating the summary. A better summary is one that is\nmore concise, captures more of the sentiment of the input text, does not contain any false information\nabout the input, and copies minimal text directly from the input. Give your answer in exactly this\nformat and then end the answer:\nLet us think step-by-step.\n[1 sentence evaluating the quality of the summary]\nThe integer score out of 9 for this answer is: X\nHere is the original text:\nInsert original text\nHere is the summary of the text:\nInsert summary to judge\n15\nD\nReward Model Grading\nWe use GPT-4 as our primary evaluator, however, it is possible to use a reward model as the evaluator.\nTable 14 shows the win-rates of FFT-SFT vs. LoRA-PPO using the separated RM, and Hydra-SFT\nvs. J-Hydra-PPO and Hydra-SFT vs. Hydra-PPO using the Hydra-RM. This table shows the success\nof each PPO method in optimizing their actors towards their reward model. Overwhelmingly, the\nRMs rate each PPO model as strongly superior to its respective input model.\nDataset\nLoRA-PPO\nJ-Hydra-PPO\nHydra-PPO\nOPT 1.3b\nGPT-4-LLM\n19.4 / 70.2\n20.4 / 79.6\n13.4 / 86.6\nOpen-Source Assistant\n20.4 / 78.6\n25.8 / 41.2\n26.8 / 40.8\nLlama 7b\nGPT-4-LLM\n13.6 / 82.6\n36.4 / 62.2\n27.8 / 69.2\nOpen-Source Assistant\n12.4 / 14.8\n15.0 / 40.2\n21.6 / 64.4\nLearning to Summarize\n34.8 / 53.8\n44.4 / 31.0\n54.6 / 40.8\nStackExchange\n10.2 / 89.8\n54.0 / 45.6\n11.0 / 89.0\nTable 14: Win Rates of each PPO method versus its input model (FFT-SFT / LoRA-PPO, Hydra-SFT\n/ J-Hydra-PPO, and Hydra-SFT / Hydra-PPO). Results in each cell are presented as \"Base Wins /\nPPO wins\" with the remainder being ties.\nE\nLoRA-SFT vs. LoRA-PPO\nFor our experiments, we use FFT-SFT for both standard and hydra models. Table 15 shows results of\nusing LoRA-SFT against LoRA-PPO, as graded by GPT-4. Here, LoRA-PPO uses FFT as the input\nmodel. We find LoRA SFT under-performs compared to FFT. While efficient, LoRA does not always\nmatch FFT-SFT performance [14, 48, 49, 50]. More investigation into PEFT [51] for alignment is\nrequired. If a method like [50] could improve LoRA-SFT for alignment, the entire RLHF process\ncould be done with roughly the same footprint as LoRA-SFT.\nDataset\nLoRA-SFT / LoRA-PPO\nOPT 1.3b\nGPT-4-LLM\n42.0 / 45.2\nOpen-Source Assistant\n29.2 / 61.0\nLlama 7b\nGPT-4-LLM\n35.2 / 53.0\nOpen-Source Assistant\n47.7 / 42.1\nLearning to Summarize\n28.2 / 50.7\nStackExchange\n45.9 / 43.1\nTable 15: Win Rates of LoRA-SFT vs. LoRA-PPO (with FFT-SFT base) on all datasets using GPT-4\nas a judge. Results in each cell are presented as \"LoRA-SFT Wins / LoRA-PPO wins\" with the\nremainder being ties.\nF\nPre-PPO Performance\nWe include here the results of the SFT, RM, and Hydra-SFT models on their respective datasets.\nNotably, the Hydra-SFT RM head usually outperforms the standard RM.\n16\nModel\nCausal PPL\nReward Accuracy\nGPT-4-LLM\nOPT 1.3b Separate\n1.71\n92.89%\nOPT 1.3b Hydra\n1.66\n92.48%\nLlama 7b Separate\n1.48\n93.50%\nLlama 7b Hydra\n1.47\n95.37%\nOpen-Assistant\nOPT 1.3b Separate\n2.00\n77.79%\nOPT 1.3b Hydra\n1.72\n80.97%\nLlama 7b Separate\n1.33\n76.75%\nLlama 7b Hydra\n1.33\n85.51%\nLearning to Summarize\nLlama 7b Separate\n2.69\n69.31%\nLlama 7b Hydra\n2.69\n69.36%\nStackExchange\nLlama 7b Separate\n2.08\n63.90%\nLlama 7b Hydra\n2.10\n64.20%\nTable 16: Model Pre-PPO Performance results on their respective validation datasets.\n17\n"
  },
  {
    "title": "ControlMat: A Controlled Generative Approach to Material Capture",
    "link": "https://arxiv.org/pdf/2309.01700.pdf",
    "upvote": "11",
    "text": "ControlMat: A Controlled Generative Approach to Material Capture\nGIUSEPPE VECCHIO, Adobe Research, France\nROSALIE MARTIN, Adobe Research, France\nARTHUR ROULLIER, Adobe Research, France\nADRIEN KAISER, Adobe Research, France\nROMAIN ROUFFET, Adobe Research, France\nVALENTIN DESCHAINTRE, Adobe Research, UK\nTAMY BOUBEKEUR, Adobe Research, France\nInput Photo\nRendering\nVisualization on Mesh\nControlMat\nControlMat\nControlMat\nRender\nRender\nRender\nEstimated Maps\nFig. 1. We present ControlMat, a diffusion based material generation model conditioned on input photographs. Our approach enables high-resolution, tileable\nmaterial generation and estimation from a single naturally or flash lit image, inferring both diffuse (Basecolor) and specular (Roughness, Metallic) properties\nas well as the material mesostructure (Height, Normal, Opacity).\nMaterial reconstruction from a photograph is a key component of 3D content\ncreation democratization. We propose to formulate this ill-posed problem\nas a controlled synthesis one, leveraging the recent progress in generative\ndeep networks. We present ControlMat, a method which, given a single\nphotograph with uncontrolled illumination as input, conditions a diffusion\nmodel to generate plausible, tileable, high-resolution physically-based digital\nmaterials. We carefully analyze the behavior of diffusion models for multi-\nchannel outputs, adapt the sampling process to fuse multi-scale information\nand introduce rolled diffusion to enable both tileability and patched diffusion\nfor high-resolution outputs. Our generative approach further permits explo-\nration of a variety of materials which could correspond to the input image,\nmitigating the unknown lighting conditions. We show that our approach\noutperforms recent inference and latent-space-optimization methods, and\ncarefully validate our diffusion process design choices. Supplemental materi-\nals and additional details are available at: https://gvecchio.com/controlmat/.\nCCS Concepts: \u2022 Computing methodologies \u2192 Appearance and texture\nrepresentations.\nAuthors\u2019 addresses: Giuseppe Vecchio, Adobe Research, France, gvecchio@adobe.\ncom; Rosalie Martin, Adobe Research, France, rmartin@adobe.com; Arthur Roullier,\nAdobe Research, France, roullier@adobe.com; Adrien Kaiser, Adobe Research, France,\nakaiser@adobe.com; Romain Rouffet, Adobe Research, France, rouffet@adobe.com;\nValentin Deschaintre, Adobe Research, UK, deschain@adobe.com; Tamy Boubekeur,\nAdobe Research, France, boubek@adobe.com.\nAdditional Key Words and Phrases: material appearance, capture, generative\nmodels\n1\nINTRODUCTION\nMaterials are at the core of computer graphics. Their creation how-\never remains challenging with complex tools, requiring significant\nexpertise. To facilitate this process, material acquisition has been a\nlong standing challenge, with rapid progress in recent years, lever-\naging massively machine learning for lightweight acquisition [De-\nschaintre et al. 2018; Guo et al. 2021; Vecchio et al. 2021]. Many of\nthese methods however, focused on the use of a single flash image,\nleading to typical highlight-related artefacts and limiting the range\nof acquisition [Deschaintre et al. 2020]. Another strategy has been\nto trade acquisition accuracy for result quality with environment lit\nimages as input [Li et al. 2017; Martin et al. 2022]. We follow this\nstrategy and propose to leverage the recent progress in diffusion\nmodels [Dhariwal and Nichol 2021; Ho et al. 2020; Rombach et al.\n2022] to build an image-conditioned material generator. We design\nour method with two key properties of modern graphics pipelines\nin mind. First, we ensure tileability of the generated output, for both\nunconditional and conditional generation. Second, we enable high\narXiv:2309.01700v2  [cs.CV]  25 Sep 2023\n2\n\u2022\nGiuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur\nresolution generation, allowing artists to directly use our results in\nhigh quality renderings.\nGenerative models have been previously used in the context of\nmaterials [Guo et al. 2020; Zhou et al. 2022], but relied on Gener-\native Adversarial Networks (GANs) [Goodfellow et al. 2014] and\noptimisation in their latent space, usually using multiple inputs\nfor material acquisition. While GANs achieved impressive quality\non structured domains such as human faces [Karras et al. 2020],\ndomains with a wider variety of appearances, e.g. materials and tex-\ntures, remain challenging to train for at high resolution and quality.\nThis is due to a high memory consumption, but also the inherent\ninstability during the min-max training, leading to mode collapse.\nWe choose to leverage diffusion models [Dhariwal and Nichol 2021;\nHo et al. 2020; Rombach et al. 2022] which were recently proved to\nbe stable, scalable generative models for different modalities (e.g.\nimages [Ramesh et al. 2022; Saharia et al. 2022], music [Huang et al.\n2023]). Further, as opposed to GANs which rely on latent space\nprojection/optimisation for inversion, Diffusion Models can be con-\nditioned during the inference \"denoising\" process. We therefore\nadapt the architecture to material generation and evaluation and\nimprove the sampling process to enable tileability and patch-based\ndiffusion for high resolution.\nOur backbone network, MatGen, is a latent diffusion model [Rom-\nbach et al. 2022] which we train to output 9 Spatially-Varying Bidirec-\ntional Reflectance Distribution Function (SVBRDF) channels (basec-\nolor (3), normal (2), height (1), roughness (1), metalness (1), opacity\n(1)). Latent diffusion models generate results by \"denoising\" a latent\nspace representation, which is then decoded by a jointly trained Vari-\national Auto Encoder (VAE) [Kingma and Welling 2013]. We train\nthis model for both unconditional and CLIP [Radford et al. 2021] con-\nditioned generation, and train a separate ControlNet [Zhang et al.\n2023] to condition the generation on material photograph. We train\nthese models using the Substance 3D Materials database [Adobe\n2022], generating for this purpose 10, 000, 000 renderings and corre-\nsponding ground truth materials. Once trained, if sampled naively,\nthese models lead to non tileable and limited resolution materials.\nWe propose various modifications of the generation process \u2013 noise\nrolling, patched and multi-scale diffusion, patched decoding and\nborder inpainting \u2013 to enable tileability and arbitrary resolution\nwhile preserving the generated materials quality.\nWe evaluate our method both qualitatively and quantitatively\nagainst previous work [Martin et al. 2022; Vecchio et al. 2021; Zhou\net al. 2022] and carefully analyse our diffusion process, demonstrat-\ning the benefit of each introduced components through ablation\nstudies. In summary, we propose a method for material reproduc-\ntion from a single photograph under uncontrolled lighting, through\na conditional diffusion model adapted to the domain of material\ngeneration. This is enabled by the following contributions:\n\u2022 ControlMat, a single image SVBRDF estimator which guides\na latent diffusion model (MatGen) by means of ControlNet.\n\u2022 Patched and multi-scale diffusion process for high-resolution,\nhigh quality material generation.\n\u2022 Noise rolling diffusion and inpainting for tileable material\ngeneration.\n2\nRELATED WORK\nWe discuss methods related to material capture and generation as\nwell as different generative models options. In particular, we focus\non the recent progress relying on Neural Networks. For a thor-\nough overview of pre-learning material modeling and acquisition\nmethods, we recommend the survey by Guarnera et al. [2016].\n2.1\nMaterial capture & estimation\nMaterial capture aims at recovering a virtual representation of an\nexisting material. In recent years the field has focused on lightweight\nacquisition methods, leveraging neural network to build priors to\nresolve the inherent ambiguity when capturing a material from few\nphotographs, unknown illumination, or both.\nFlash-based. Single flash image material acquisition leveraging\ndeep network has seen significant progress, leveraging the U-Net\narchitecture [Ronneberger et al. 2015]. A first method was proposed\nby Deschaintre et al. [2018] and further improved to reduce the\nover-exposed flash artefacts using GANs [Vecchio et al. 2021; Zhou\nand Kalantari 2021], highlight aware training [Guo et al. 2021], meta\nlearning [Fischer and Ritschel 2022; Zhou and Kalantari 2022], semi-\nprocedural priors [Zhou et al. 2023b] or cross-scale attention [Guo\net al. 2023]. Some methods focused on stationary material recovery,\nleveraging self similarity of the material, trading spatial resolution\nfor angular resolution [Aittala et al. 2016, 2015; Henzler et al. 2021].\nOther approaches for spatially varying materials proposed to com-\nbine multiple flash images with direct inference [Deschaintre et al.\n2019] or using refinement optimisation [Gao et al. 2019] to improve\nquality, or even flash and non flash photographs [Deschaintre et al.\n2020]. Closer to our approach are methods using generative models\nto build a latent space prior in which a latent vector can be optimized\nto reproduce an input picture [Guo et al. 2020]. Zhou et al. [2022]\nfurther modified the generative architecture to enforce tileability of\noutputs through circular padding and tensor rolling and allow for\nstructurally conditioned generation with grayscale patterns. More\nrecently, a self-supervised approach leveraging a GAN training pro-\nposed to train a material generator solely on flash images without\nmaterial ground truth supervision [Zhou et al. 2023a].\nUnknown illumination. While flash illumination provides impor-\ntant information about the acquired material specularity, it also\nleads to challenges: the over-exposure artefacts we mentioned ear-\nlier, limiting the acquisition scale [Deschaintre et al. 2020] and\nrequirement to capture the material in a lightly controlled setup,\nlimiting their use with existing online resources. An alternative\napproach is to estimate the properties of a material from unknown\nenvironment illumination as proposed by Li et al. [2017] and im-\nproved by Martin et al. [2022]. These methods however do not\ngenerate spatially-varying roughness properties, or approximate it\nfrom the mesostructure, and specialise on more diffuse materials\n(e.g \"outdoor categories\").\nInverse procedural Material. A recent line of work proposes to\nleverage procedural representation to represent materials from pho-\ntographs. Hu et al. [2019; 2022a] and MATch [Shi et al. 2020] propose\nto use existing material graphs and to infer or optimise their pa-\nrameters to match a given target image. The challenge of creating\nControlMat: A Controlled Generative Approach to Material Capture\n\u2022\n3\nMatGen\nNoise\nrolling\nLDM\nControlNet\nQ\nKV\nQ\nKV\nQ\nKV\nQ\nKV\nUNet\nInput photo\n+ inpaint\nmask\nCLIP\n\"Alien rocky\nground\"\nNoise\nunrolling\nText / image\nprompt\nPredicted\nmaterial maps\nPatched\ndecoding\nInference\nGaussian noise\nTraining\nGround truth\nmaterial maps\nFig. 2. Overview of ControlMat. During training, the PBR maps are compressed into the latent representation \ud835\udc67 using the encoder E. Noise is then added\nto \ud835\udc67 and the denoising is carried out by a U-Net model. The denoising process can be globally conditioned with the CLIP embedding of the prompt (text or\nimage) and/or locally conditioned using the intermediate representation of a target photograph extracted by a ControlNet network. After \ud835\udc5b denoising steps\nthe new denoised latent vector \u02c6\ud835\udc67 is projected back to pixel space using the decoder D. We enable high resolution diffusion through splitting the input image in\nN patches which are then diffused, decoded and reassembled through patched decoding.\nprocedural models has been tackled by selecting components for\na generic graph architecture [Hu et al. 2022b] or through graph\ngeneration based on a combination of Transformer models [Guer-\nrero et al. 2022; Hu et al. 2023]. While the procedural representation\nhas interesting benefits, these methods target the recovery of the\nappearance style, rather than the exact material, as they rely on\nprocedural functions. Further, many of these methods rely on test\ntime access to a pre-existing large library of material graphs.\nIn this work, we explore the combination of generative models\nwith unknown illumination, leveraging the recent progress in diffu-\nsion modeling [Dhariwal and Nichol 2021; Ho et al. 2020; Rombach\net al. 2022] and their conditioning [Zhang et al. 2023]. Doing so, we\npropose an uncontrolled lighting photograph to material method\nwith tileable, high-resolution results, reproducing the input pho-\ntograph layout and estimating specular properties. In concurrent\nwork, Vecchio et al. [2023] also explore the use of diffusion mod-\nels for materials generation, but focus on generation rather than\nestimation and do not enable high resolution or tileable material\nestimation/generation.\n2.2\nGenerative models\nImage generation is an open challenge in computer vision due to the\ncomplexity of modeling high-dimensional data distributions. While\nGenerative Adversarial Networks (GANs) [Goodfellow et al. 2014]\nhave shown great results in generating high-quality images [Brock\net al. 2018; Karras et al. 2017, 2020], their training process is plagued\nby unstable convergence and mode collapse behavior [Arjovsky\net al. 2017; Gulrajani et al. 2017; Mescheder 2018; Metz et al. 2016].\nSome recent approaches have tackled the problem by decoupling\ndecoding and distribution learning into a two-stage approach [Dai\nand Wipf 2019; Esser et al. 2021; Rombach et al. 2020a,b]. In this\ncase a Variational Autoencoder (VAE) [Kingma and Welling 2013;\nRezende et al. 2014; Van Den Oord et al. 2017] is first trained to\nlearning a data representation, then another network learns its\ndistribution.\nRecently, a new family of generative models, the Diffusion Models\n(DMs), have emerged as a promising alternative to GANs. These\nmodels allow to learn complex distributions and generate diverse,\nhigh quality images [Dhariwal and Nichol 2021; Ho et al. 2020; Sohl-\nDickstein et al. 2015], leveraging a more stable training process.\nThese models are however computationally expensive to evaluate\nand optimize, particularly for high-resolution images. To address\nthis issue, Latent Diffusion Model [Rombach et al. 2022] propose\nto diffuse in a smaller latent space. This space is learned through\npre-training an image compression model such as a Variational Au-\ntoencoder (VAE). By using the same diffusion model architecture,\nthis shift to the latent space reduces computational requirements\nand improves inference times, without significantly compromising\ngeneration quality. Recently, different methods [Bar-Tal et al. 2023;\nJim\u00e9nez 2023] have been proposed to extend the generative capabil-\nities of LDMs by enforcing consistency between multiple parallel\ndiffusion processes. This enables a fine grained control over the\nlocalization of content inside the generated image, while allowing\nto scale to higher resolution, and non-square aspect ratio through\nlatent vector patching and independent processing.\nAdditionally, the diffusion in the latent space enabled novel condi-\ntioning mechanisms for the generative process. These conditioning\nmechanisms are however bound to the training of the diffusion\nmodel, requiring a new training for each type of conditioning. To\naddress this limitation, ControlNet [Zhang et al. 2023] was proposed\nas a way to control pre-trained large diffusion models, through a\nrelatively small, task-specific, network.\n4\n\u2022\nGiuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur\nWe leverage this diffusion-specific progress and adapt the diffu-\nsion model and inference process to the material generation and\nacquisition domain, leveraging ControlNet for its conditioning ca-\npabilities.\n3\nOVERVIEW\nControlMat, summarized in Fig. 2, is a generative method based on\na Latent Diffusion Model (LDM) [Rombach et al. 2022] and designed\nto produce high-quality Spatially Varying Bidirectional Reflectance\nDistribution Functions (SVBRDFs) material maps from an input\nimage. The method\u2019s generative backbone, MatGen, is made of a\nvariational autoencoder (VAE) [Kingma and Welling 2013] network\ntrained to represent SVBRDF maps in a compact latent space, and\na diffusion model trained to sample from this latent space. This\nbackbone allows to sample materials unconditionally, or with a\nglobal (text or image) conditioning. To enable accurate guiding of\nthe sampling to follow a spatial condition and achieve high quality\ngeneration, we propose to use a ControlNet [Zhang et al. 2023].\nWe modify the diffusion process to enable high resolution gener-\nation and tileability. We achieve high resolution through patched\ndiffusion, introducing the notion of noise rolling (and unrolling). At\neach diffusion step, the noise tensor is rolled by a random factor,\npreventing the apparition of seams between patches and ensuring\nconsistency across them. This process not only allows diffusion per\npatch, but also ensures tileability in the generated materials and\npreserves the possible tileability of the input photograph. When\ndiffusing at high resolution, we note that low frequency elements of\nthe SVBRDF, in particular in the mesostructure, tend to disappear.\nWe propose to solve this through a multi-scale diffusion, maintaining\nlow frequency signal for high-resolution generations by merging\nmultiple scales of generated material maps through masked dif-\nfusion. We combine this with a patched decoding process in the\nVAE to enable arbitrary resolution material generation, which we\ndemonstrate up to 4K. Finally, we enable tileability for non tileable\ninputs photographs by inpainting the input photograph borders,\nand synthesising them with our tileability constraint.\nWe discuss our material model in Section 4.1, our diffusion model\nin Section 4.2 and our different design choices for the diffusion pro-\ncess to enable conditioned (Section 4.3), tileable and high resolution\n(Section 5) generation.\n4\nCONTROLLED GENERATIVE MODEL FOR MATERIALS\n4.1\nMaterial Representation\nOur method is designed to generate materials in the form of SVBRDFs\nrepresented as a collection of 2D texture maps. These maps repre-\nsent a spatially varying Cook-Torrance micro-facet model [Cook\nand Torrance 1982; Karis 2013], using a GGX [Walter et al. 2007]\ndistribution function, as well as the material mesostructure. In par-\nticular, we generate base color \ud835\udc4f, normal \ud835\udc5b, height \u210e, roughness \ud835\udc5f and\nmetalness \ud835\udc5a properties. Our model also supports the generation of\nan opacity parameter map as exemplified in Figure 1. For visualisa-\ntion space considerations, as this parameter is only relevant for a\nvery small subset of materials, we omit it in the paper, but include\nthis result map in Supplemental Materials. The roughness is related\nto the width of the BRDF specular lobe, where a lower roughness\nrepresents a shinier material. Metalness defines which area of the\nmaterial represents raw metal. We generate both normal and height\nproperties separately, as artists typically include different signals in\nthese maps [McDermott 2018]. We use a standard microfact BRDF\nmodel [Cook and Torrance 1982; Karis 2013] based on the GGX\nnormal distribution function [Walter et al. 2007] and computing\nthe diffuse and specular components of our renderings similarly to\nDeschaintre et al. [2018]. The exact model formulation is available\nin supplemental material.\n4.2\nGenerative material model\nMatGen adapts the original LDM architecture to output a set of\nSVBRDF maps instead of a single RGB image. This core genera-\ntive model consists in two parts: a compression VAE [Kingma and\nWelling 2013] E, learning a compact latent representation of the\nmaterial maps, and a diffusion [Rombach et al. 2022] U-Net [Ron-\nneberger et al. 2015] model \ud835\udf16\ud835\udf03, that learns the distribution of the\nlatent VAE features (see Fig. 2).\nThe VAE compression model, consists in an encoder E and a\ndecoder D, trained to jointly encode and decode a set of \ud835\udc41 maps\n\ud835\udc40 = {M1, M2, . . . , M\ud835\udc41 } concatenated over the channels dimension.\nThis compresses a tensor M \u2208 R\ud835\udc3b \u00d7\ud835\udc4a \u00d7\ud835\udc36, where \ud835\udc36 = 9 is the con-\ncatenation of the different material maps defined in Section 4.1, into\na latent representation \ud835\udc67 = E(M), where \ud835\udc67 \u2208 R\u210e\u00d7\ud835\udc64\u00d7\ud835\udc50, and \ud835\udc50 is the\ndimensionality of the encoded maps. We set \u210e = \ud835\udc3b\n8 , \ud835\udc64 = \ud835\udc4a\n8 as in\nthe original LDM architecture [Rombach et al. 2022] and set \ud835\udc50 = 14\nwhich we empirically found leading to the best compression/quality\ncompromise.\nFollowing Rombach et al. [2022], we train the VAE E using a\ncombination of pixel-space \ud835\udc3f2 loss, a perceptual LPIPS loss [Zhang\net al. 2021], and a patch-based adversarial objective [Dosovitskiy\nand Brox 2016; Esser et al. 2021; Isola et al. 2017] for each map\nseparately. Further, we follow the VAE latent space regularisation\nand impose a Kullback\u2013Leibler divergence penalty to encourage the\nlatent space to follow a Normal distribution [Kingma and Welling\n2013; Rezende et al. 2014].\nWe train our diffusion model \ud835\udf16\ud835\udf03 to learn to sample the latent distri-\nbution of the VAE E. In particular we train a diffusion model, using\na time-conditional U-Net core, as proposed in [Rombach et al. 2022].\nDuring training, noised latent vectors are generated, following the\nstrategy defined in [Ho et al. 2020], through a deterministic forward\ndiffusion process\ud835\udc5e (\ud835\udc67\ud835\udc61 |\ud835\udc67\ud835\udc61\u22121), transforming it into an isotropic Gauss-\nian distribution. The diffusion network \ud835\udf16\ud835\udf03 is then trained to perform\nthe backward diffusion process \ud835\udc5e (\ud835\udc67\ud835\udc61\u22121|\ud835\udc67\ud835\udc61), effectively learning to\n\"denoise\" the latent vector and reconstruct its original content.\nOnce trained, our models allow sampling a normal distribution,\n\"denoising\" the samples into a valid latent space point, and decoding\nit using the VAE into high quality SVBRDF maps.\n4.3\nControlled Synthesis\nTo control the generation process we just described, we use two dif-\nferent mechanism: a) global conditioning for text or visual, high level\nprompts and b) spatial conditioning (e.g. a material photograph).\n4.3.1\nGlobal conditioning. Following the work by [Rombach et al.\n2022] we implement global conditioning through cross-attention\nControlMat: A Controlled Generative Approach to Material Capture\n\u2022\n5\n[Vaswani et al. 2017] between each block of convolutions of the\ndenoising U-Net and an embedding of the condition \ud835\udc66, which is\nextracted by an encoder \ud835\udf0f\ud835\udf03, with the attention defined as:\nAttention(\ud835\udc44, \ud835\udc3e,\ud835\udc49 ) = softmax\n\u0012\ud835\udc44\ud835\udc3e\ud835\udc47\n\u221a\n\ud835\udc51\n\u0013\n\ud835\udc49,\n(1)\nwhere \ud835\udc44 = \ud835\udc4a \ud835\udc56\n\ud835\udc44 \u00b7 \ud835\udf0f\ud835\udf03 (\ud835\udc66), \ud835\udc3e = \ud835\udc4a \ud835\udc56\n\ud835\udc3e \u00b7 \ud835\udf11\ud835\udc56 (\ud835\udc67\ud835\udc61),\ud835\udc49 = \ud835\udc4a \ud835\udc56\n\ud835\udc49 \ud835\udf11\ud835\udc56 (\ud835\udc67\ud835\udc61). Here,\n\ud835\udf11\ud835\udc56 (\ud835\udc67\ud835\udc61) \u2208 R\ud835\udc41 \u00d7\ud835\udc51\ud835\udc56\n\ud835\udf16 is the flattened output of the previous convolution\nblock of \ud835\udf16\ud835\udf03 \u2013the diffusion network\u2013, and\ud835\udc4a \ud835\udc56\n\ud835\udc44 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc56\n\ud835\udf0f ,\ud835\udc4a \ud835\udc56\n\ud835\udc3e \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc56\n\ud835\udf16 ,\n\ud835\udc4a \ud835\udc56\n\ud835\udc49 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc56\n\ud835\udf16 , are learnable projection matrices.\nThe training objective in the conditional setting becomes\n\ud835\udc3f\ud835\udc3f\ud835\udc37\ud835\udc40 := EE(\ud835\udc40),\ud835\udc66,\ud835\udf16\u223dN(0,1),\ud835\udc61\n\u0002\n\u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udf0f(\ud835\udc66))\u22252\n2\n\u0003\n(2)\nWe use a pre-trained CLIP [Radford et al. 2021] model as feature\nextractor \ud835\udf0f to encode the global condition. In particular, MatGen can\nbe globally conditioned via text and image prompts, respectively\nencoded through the corresponding heads of CLIP.\n4.3.2\nSpatial conditioning. We build ControlMat by adding spa-\ntial conditioning to the Latent Diffusion Model using a Control-\nNet [Zhang et al. 2023], leveraging its conditioning capabilities to\ntackle the tasks of (i) SVBRDF estimation from single input image,\nand (ii) inpainting for material editing and tileability.\nOur ControlNet replicates the original design of Zhang et al.\n[2023]: it consists in a convolutional encoder, a U-Net encoder and\nmiddle block, and a skip-connected decoder. As our main model\nfor material generation MatGen, we train out ControlNet to receive\na 4-channel input consisting of an RGB image and a binary mask,\nconcatenated along the channel dimension (see the center part of\nFig. 2). This binary mask guides where the diffusion model has to\ngenerate new content, allowing for in-painting use cases. A small\nencoder E\ud835\udc50, of four convolutional layers, is used to encode the\ncondition to the same dimension as the noise processed by our LDM.\nIn particular, given an image spatial condition \ud835\udc50\ud835\udc56 \u2208 R\ud835\udc3b \u00d7\ud835\udc4a \u00d74, the\nencoder produces a set of feature maps \ud835\udc50\ud835\udc53 = E\ud835\udc50 (\ud835\udc50\ud835\udc56), with \ud835\udc50\ud835\udc53 \u2208\nR\u210e\u00d7\ud835\udc64\u00d7\ud835\udc50.\nThe ControlNet is trained to guide the diffusion process of our\nmain diffusion model, MatGen. During this training, MatGen is kept\nfrozen and only the ControlNet is actually optimized. This allows for\na faster convergence and a less computationally intensive training\nof the conditioning [Zhang et al. 2023]. Further, this makes our\nmain diffusion model independent from the ControlNet, letting us\nchoose during inference whether to use it unconditionally, with a\nglobal-condition or with a spatial condition, without retraining.\n4.3.3\nMaterial acquisition. For material acquisition, we input the\nmaterial photograph both as a global condition as described in\nSection 4.3.1 and through the ControlNet [Zhang et al. 2023], as\ndescribed in Section 4.3.2. The global conditioning provides guidance\nwhere there are regions of the material to inpaint \u2013for which we do\nnot want to follow the condition image perfectly. And ControlNet\nprovides guidance to locally condition our generation model on the\ninput photograph. This condition combination lets us sample likely\nmaterial properties for a given photograph, compensating for the\nill-posedness of material acquisition under unknown illumination.\n(a) input\n(b) na\u00efve approach (c) patches + overlap\n(d) noise rolling\nFig. 3. Patch diffusion comparison. Examples of height map results using\ndifferent approaches for patched latent diffusion.\nUnrolled image\nRolling\nRolled image\nFig. 4. Noise rolling. Visual representation of the noise rolling approach.\nThe input is \"rolled\" over the x and y axes by a random translation, repre-\nsented in the figure by replicating the image 2x2 and cropping the region\ncontained in the blue square. Unrolling consists in doing the inverse process.\nWe compare the different conditioning in Figure 7 and show that our\nmodel recovers materials that better match the input photographs\nthan previous work, while better separating albedo from meso-\nstructures, light and shading in Figures 8 and 9.\n5\nLARGE SCALE, TILEABLE MATERIALS GENERATION\nA simple combination of Latent Diffusion [Rombach et al. 2022]\nwith ControlNet [Zhang et al. 2023] would be limited to non-tileable,\nlower-resolution results, due to memory constraints. This is par-\nticularly limiting in the context of material creation, where typical\nartists work at least at 4K resolution. To achieve high-resolution, we\nemploy a patch\u2013based diffusion and introduce a noise rolling tech-\nnique (see Sec. 5.1) to (i) enforce consistency between patches and\n(ii) remove visible seams, without the additional cost of overlapping\npatches. This separation in different patches tends to create incon-\nsistency in normal estimation and looses some of the low frequency\nmeso-structure content. To preserve this low frequency content at\nhigh resolution materials and ensure normal consistency throughout\nthe generation , we propose a multiscale diffusion method (Sec. 5.3).\nFinally, we introduce a simple, yet effective, patched decoding (see\nSec. 5.4) achieving high resolution generation with limited memory\nrequirement.\n5.1\nNoise rolling\nDiffusing noise by patches allows to reduce the memory consump-\ntion of diffusing large noise tensor, but also introduce artifacts in\nthe output with visible seams, as shown in Fig. 3(b). This effect can\nbe mitigated by the use of overlapping patches, but this still leaves\n6\n\u2022\nGiuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur\nsome visible seams and low frequency artifact (Fig. 3(c)). Further,\noverlapping patches requires to generate more patches, requiring\nextra computation.\nTo tackle this issue, inspired by the training procedure of tileable\nGANs [Zhou et al. 2022], we propose to take advantage of the itera-\ntive nature of the diffusion process at inference time. By \u201crolling\u201d\nthe noise tensor on itself by a random translation (and subsequently\nunrolling after each diffusion step), we remove seams stemming\nfrom diffusion (Fig. 4). This approach provides better consistency\nbetween patches (Fig. 3(d)), matching the statistics of the generated\nimage at each diffusion step to match the learned distribution. As the\nlearned distribution does not contain seams randomly placed in the\nimages, our noise rolling approach naturally pushes the generation\ntowards tileable images in an unconditional or globally-conditioned\nsetting. We provide the pseudo-code for the noise rolling algorithm\nin Alg. 1 and evaluate the effect of the noise rolling for patched\ngeneration and materials tileability in Sec. 6.3.\n5.2\nBorder inpainting\nWhile our noise rolling ensures tileability in the generation process,\nif spatially conditioned on a non tileable photograph, the condition\nprevails, preventing the output to be tileable. To enable tileability for\nany arbitrary input photograph we adopt an inpainting technique,\nby masking the input border (with a border size of 1\n16 of the image\nsize) and letting the diffusion model regenerate it. This is made\npossible by our use of a mask in the training of our ControlNet.\nDuring training, the input image to the ControlNet is masked with\na random binary masks \u2013between 0% and 40% of the image size\nis masked randomly at each step\u2013, provided to the network as an\nadditional input channel. At the same time, we encode the full\nunmasked image with CLIP, and provide it to the LDM through\nCross-Attention. This allows to drive the generation of the masked\nregion through inpainting while using the global conditioning to\ngenerate a coherent material.\nWhen masking the borders to make an input photograph tileable,\nthe masking itself does not guarantee tileability on the estimated\nmaps. It is the combination of the masking and the noise rolling\n(Sec. 5.1) which allows to inpaint and ensure that the generated\nregion is tileable. As the inpainted region is generated using the\nglobal conditioning, it is inherently tileable thanks to the noise\nrolling approach, and since this inpainted region is on the border of\nthe material, it enforces the material to be tileable. This approach is\nparticularly efficient for stationary materials, but gives reasonable\nresults on structured ones too. We provide results using this process\nin Figure 10 and supplemental material.\n5.3\nMultiscale diffusion.\nEstimating SVBRDFs is highly dependent on the network recep-\ntive field compared to the captured area physical size, and tends\nto produce flat geometry when evaluated at a higher resolution\nthan trained for [Deschaintre et al. 2020; Martin et al. 2022]. To\novercome this issue, we propose a multiscale diffusion approach\nwhich leverages the information extracted at a lower resolution,\nto better estimate the low frequency geometry when diffusing at\nALGORITHM 1: Patched diffusion with noise rolling\nData: \ud835\udc47 = 100, \ud835\udc5a\ud835\udc4e\ud835\udc65_\ud835\udc5f\ud835\udc5c\ud835\udc59\ud835\udc59 \u2265 0, \ud835\udc5d = 32\nResult: \ud835\udc67\n1 \ud835\udc61 \u2190 0\n2 \ud835\udc67 \u2190 sample_noise()\n3 while \ud835\udc61 < \ud835\udc47 do\n4\n\ud835\udc5f\ud835\udc65 \u2190 random(0, max_roll)\n5\n\ud835\udc5f\ud835\udc66 \u2190 random(0, max_roll)\n6\n\ud835\udc67 \u2190 roll(\ud835\udc67, (\ud835\udc5f\ud835\udc65,\ud835\udc5f\ud835\udc66 ))\n7\n\ud835\udc67_\ud835\udc60\u210e\ud835\udc4e\ud835\udc5d\ud835\udc52 \u2190 shape(\ud835\udc67)\n8\n\ud835\udc67 \u2190 patch_noise(z, p)\n9\n\ud835\udc67 \u2190 sample(\ud835\udc67,\ud835\udc61 )\n10\n\ud835\udc67 \u2190 unpatch_noise(z, z_shape, p)\n11\n\ud835\udc67 \u2190 roll(\ud835\udc67, (\u2212\ud835\udc5f\ud835\udc65, \u2212\ud835\udc5f\ud835\udc66 ))\n12\n\ud835\udc61 \u2190 \ud835\udc61 + 1\nend\nProcedure patch_noise(Noise tensor \ud835\udc67, Patch size \ud835\udc5d)\n1\n\ud835\udc43 \u2190 empty list\n2\nfor \ud835\udc56 = 0 to rows(\ud835\udc67) \u2212 \ud835\udc5d by \ud835\udc5d do\n3\nfor \ud835\udc57 = 0 to columns(\ud835\udc67) \u2212 \ud835\udc5d by \ud835\udc5d do\n4\npatch \u2190 \ud835\udc67[\ud835\udc56 : \ud835\udc56 + \ud835\udc5d, \ud835\udc57 : \ud835\udc57 + \ud835\udc5d]\n5\nappend(\ud835\udc43, patch)\nend\nend\n6\nreturn \ud835\udc43\nProcedure unpatch_noise(Patched noise \ud835\udc43, Original shape (\ud835\udc3b,\ud835\udc4a ),\nPatch size \ud835\udc5d)\n1\n\ud835\udc67 \u2190 zero matrix of shape (\ud835\udc3b,\ud835\udc4a )\n2\n\ud835\udc58 \u2190 0\n3\nfor \ud835\udc56 = 0 to \ud835\udc3b \u2212 \ud835\udc5d by \ud835\udc5d do\n4\nfor \ud835\udc57 = 0 to \ud835\udc4a \u2212 \ud835\udc5d by \ud835\udc5d do\n5\n\ud835\udc67[\ud835\udc56 : \ud835\udc56 + \ud835\udc5d, \ud835\udc57 : \ud835\udc57 + \ud835\udc5d] \u2190 \ud835\udc43 [\ud835\udc58]\n6\n\ud835\udc58 \u2190 \ud835\udc58 + 1\nend\nend\n7\nreturn \ud835\udc67\nhigher resolutions and ensure normal consistency throughout the\npatched process.\nTo achieve the proposed multiscale diffusion, we apply a hierar-\nchical process where diffusion takes place at multiple scales, coarse\nand fine. More precisely, we proceed as follow:\n(1) we perform a low-resolution diffusion,\n(2) we upsample and renoise the low resolution output,\n(3) we perform a higher-resolution diffusion process, using the\noutput of step (2) as initial noise.\nWe found this to be the best approach to multiscale generation,\npreserving low frequencies while still generating high quality high\nfrequencies and ensuring consistent normal orientations. A detailed\nevaluation of the effect of our proposed multi-scale diffusion on the\ngeneration is presented in Sec. 6.4.2.\n5.4\nPatched decoding.\nWhile our rolled diffusion process lets us diffuse larger resolutions,\ndecoding high resolutions maps with the VAE in a single step is a\nControlMat: A Controlled Generative Approach to Material Capture\n\u2022\n7\nDownsampling\nPatching\nmean\nmatching\nFig. 5. Overview of our patched decoding. Decoding our latent vector \ud835\udc67\nper patch (reducing peak-memory usage) introduces seams between them.\nWe propose to encourage similarity between patches by first decoding a\nlow resolution material and applying a mean matching operation between\nthe corresponding regions. Combined with an overlapping patches blending\napproach, this prevents the apparition of seams.\nmemory intensive operation. We propose to use a patch decoding\napproach, decoding patches of the latent representation separately,\nthus reducing the maximum memory requirements. However, if\napplied naively, this approach produces visible seams and incon-\nsistency between decoded patches. To mitigate this issue we adopt\na simple, yet effective solution: we first decode a low resolution\nversion of the material, decoded in a single pass, and match the\nmean of each hi-res patch with the corresponding patch in the low-\nres version. We show a visual overview of the patch decoding in\nFig. 5. This does not require any changes in the architecture, and\nsignificantly mitigates the inconsistency issues. However, as minor\nseams may remain, we decode overlapping patches and blend them\nusing truncated Gaussian weights, maximally preserving the signal\nat the center of the patches and giving a 0 weight to the borders. In\nour experiments we use a patch-size of 512.\n6\nIMPLEMENTATION & RESULTS\n6.1\nDataset and metrics\n6.1.1\nTraining dataset. We train our model using a synthetic dataset\nwe generate. In particular, we follow the approach proposed by De-\nschaintre et al. [2018] and Martin et al. [2022]: starting from a collec-\ntion of parametric material graphs in Substance 3D format [Adobe\n2022], we generate a large amount of material variations. In partic-\nular, our dataset is composed of material maps (Basecolor, Normal,\nHeight, Roughness and Metalness) and associated renderings under\nvarying environment illuminations. To generate as much data as\npossible, we leverage the Adobe Substance 3D Assets library [Adobe\n2022], consisting of 8615 material graphs. As opposed to Martin et al.\n[2022], we do not constrain our method to outdoor, mostly diffuse\nmaterials (e.g Ground, Stone, Plaster, Concrete), but use materials\nfrom all available categories 1, including those typically designed\nfor indoor (e.g Ceramic, Marble, Fabrics, Leather, Metal, ...) use.\nSimilar to Martin et al. [2022], we generate large amounts of\nvariations of the procedural materials by tweaking the parameters\nexposed by the artists during design. For each parameter we ensure\n1See https://substance3d.adobe.com/assets/allassets?assetType=substanceMaterial for\na complete list of categories.\nwe sample them in a range defined by the artists presets to obtain\nrealistic material variations. Each material variation is represented\nby a set of textures/maps representing the SVBRDF, with a resolution\nof 2048x2048 pixels.\nWe follow previous work [Martin et al. 2022] and produce 14\nrenderings for each material variations, maintaining the 2048x2048\npixels resolution, with ray-tracing and IBL lighting. We adapt the\nHDR environment map to the material category, using outdoor\nenvironment maps for material categories typically found outdoors\nand indoor environment maps for indoor categories.\nThe pairs of renderings and SVBRDF maps are then augmented\ntogether with rotation, scaling and cropping to obtain 512\u00d7512 pixels\ntraining pairs. From 8, 615 material graphs, we generate \u223c 126, 000\nmaterial variations, \u223c 800, 000 materials crops, and \u223c 10, 000, 000\ncropped pairs of renderings and materials.\nWe use different parts of the dataset for different components of\nour method. Our VAE is trained on the cropped materials (\u223c 800\ud835\udc58\ntraining points), the diffusion model is trained on the complete\ndataset (\u223c 10\ud835\udc5a training pairs), and our ControlNet is trained on\n\u223c 25% of the complete dataset (\u223c 2.5\ud835\udc5a training pairs).\n6.1.2\nSynthetic evaluation dataset. We design a synthetic dataset\nwithout relying on any Substance 3D library asset to ensure train/test\ndataset separation. Our collected dataset consists of 66 materials\nfrom three sources with permissive licences: 27 materials from Am-\nbientCG2; 26 from PolyHaven3; and 13 from Textures.com4.\nFor each material we generate 4 renderings under different en-\nvironment illuminations (not shared with the ones used for the\ntraining dataset generation).\n6.1.3\nReal photographs evaluation dataset. Our real photographs\ndataset was captured using smartphones, DSLR cameras, and sourced\nfrom the web. This dataset offers a comprehensive representation\nof real-world image materials encountered in various lighting sce-\nnarios. The smartphone photos represent images captured by typi-\ncal consumer-grade mobile devices, exhibiting medium resolution\nand noise levels commonly encountered in everyday photography.\nOn the other hand, the DSLR images represent high-quality pho-\ntographs captured by professional-grade cameras, characterized by\nsuperior resolution, dynamic range, and color fidelity. Finally, the\nweb photos comprise images sourced from the internet, which often\nexhibit low resolutions, JPEG compression, and poor dynamic range.\nThe different images and their origin are available in Supplemental\nMaterial.\n6.2\nTechnical details\n6.2.1\nVAE. We train our Kullback\u2013Leibler-regularized VAE com-\npression model [Kingma and Welling 2013] with stochastic gradient\ndescent, using the Adam optimizer on 8\u00d7A100, 40 GB VRAM GPUS\nin parallel. We define the batch size to 9 per GPU, the learning rate\nis set to 4.5 \u00b7 10\u22126 and we train the model for 390k iterations, lasting\n240 hours. Following the original work [Esser et al. 2021], we only\nenable the adversarial loss from iteration 50k onward.\n2https://ambientcg.com/\n3https://polyhaven.com/\n4https://www.textures.com/\n8\n\u2022\nGiuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur\n6.2.2\nLatent Diffusion model and ControlNet. We train successively\nour Latent Diffusion Model [Rombach et al. 2022] and Control-\nNet [Zhang et al. 2023] with stochastic gradient descent, using the\nAdam optimizer on 8\u00d7A100, 40 GB VRAM GPUS in parallel. We\ndefine the batch size to 44 per GPU to maximize memory usage. The\nlearning rate is set to 2 \u00b7 10\u22127 and we train the LDM/ControlNet\nmodels for respectively 300k/350k iterations, lasting 180/240 hours.\nThe trainable copied weights of the ControlNet are initialized to\nbe a copy of the trained LDM weights. During training of the Con-\ntrolNet, only its parameters are optimized, while the LDM is kept\nfrozen and we follow the original method with a 10% conditional\ndropout of the global condition. Our implementation is based on\nthe official repository of ControlNet5. At training time, ControlNet\nreceives randomly downsampled and JPEG compressed rendering\nto further enhance robustness. This allows ControlMat to better\nreconstruct fine grained details when provided with a low resolution\nor compressed input image.\n6.2.3\nInference. We evaluate execution speed and memory con-\nsumption on a A10G GPU with 24 GB of VRAM. As the complete\nControlMat architecture (VAE + LDM + CLIP encoder + ControlNet)\nmemory requirements and timings heavily depend on the target\nresolution, we provide values up to 4K results. Material estimation\nfrom an input image takes 3 seconds at 512 \u00d7 512, 18 seconds at\n1024 \u00d7 1024 and 12GB of VRAM, 43 seconds at 2048 \u00d7 2048 and\n18GB VRAM, and 350 seconds at 4096 \u00d7 4096 and 20GB of VRAM.\nWe report timing and memory requirements for a maximum of 8\npatches processed in parallel during the diffusion process, using the\nmultiscale approach, which requires the diffusion steps at all the\nlower resolutions to be completed. It is possible to further reduce\nmemory requirements in exchange for longer computation times\nby reducing the patches batch size.\n6.3\nResults and comparisons\nFor all results we show material maps and renderings, as well as a\nclay rendering, highlighting the material mesostructure. As the in-\nferred height maps are normalised between [0;1], we automatically\napproximate a displacement factor by matching the scale of nor-\nmals computed from the inferred height to the scale of the normals\ndirectly inferred by the methods.\n6.3.1\nMaterial generation. We evaluate our MatGen model and\ncompare it to TileGen [Zhou et al. 2022], a recent GAN based mate-\nrial generative model, in Figure 6. In particular, we compare to the\nper-class models trained by TileGen on Stone, Metals and Bricks.\nWe generate results for our method using global conditioning by\nembedding the class names as condition. We emphasize that both\nmethods generation results are only conditioned on the desired class,\nresulting in varied appearances. Despite not having been specifically\ntrained per semantic class as TileGen, our model can generate high\nquality materials with large amount of details. Further, we can see\nin the Clays that our approach leads to significantly more details in\nthe generated materials.\nWe evaluate the difference between the different type of con-\nditions discussed in Section 4.3.1: global condition (text (Text) or\n5https://github.com/lllyasviel/ControlNet\nColor\nNormal\nHeight\nRough\nMetal\nRender\nClay\nTileGen\nMetal\nMatGen\nTileGen\nStone\nMatGen\nTileGen\nBricks\nMatGen\nFig. 6. Comparison to TileGen. We compare MatGen, our underlying\ngenerative model, to TileGen models trained on three categories (Metal,\nStone, Bricks). We condition our model with the category name as a global\ncondition. We can see that despite MatGen being trained on all categories\nin a single network, it can generate results with similar quality than TileGen\ninstances specialized on these categories. Additional results can be found in\nSupplemental Materials.\nimage (Global)) and the combination of global image condition and\nlocal condition using ControlNet (Local). As expected, the text con-\nditioning generates materials matching the overall description, but\nlacks information for precise control. Due to the global condition in-\nput mechanism, the global image control generates materials which\noverall match the desired appearance, but result in significant varia-\ntion (e.g. different scale or the pavement which tiles are differently\narranged). This is a very interesting property to explore possible\nmaterial variations, which however does not fit the requirements\nfor acquisition. In contrast, our local conditioning, used for material\nacquisition scenarios, combines the global image conditioning with\nthe ControlNet, as described in Section 4.3.2, generating materials\nmatching the input spatial conditioning.\nWe include more generation results with a wider prompt variety\nin the Supplemental Material, as well as a comparison to a CLIP\nbased Nearest-Neighbour search in the training database, validating\nthe network generative capability.\n6.3.2\nMaterial estimation. Our method\u2019s main application is mate-\nrial estimation from an input photograph. In this section we com-\npare to recent state of the art models, SurfaceNet [Vecchio et al.\n2021] and MaterIA [Martin et al. 2022] on both synthetic and real\nmaterials. We retrain SurfaceNet on our dataset, while we use the\navailable MaterIA model in Substance Sampler which was trained\non an \"outdoor\" dataset generated from the same procedural mate-\nrials. This \"outdoor\" dataset contains the same amount of data than\nthe \"outdoor\" part of our generated dataset.\nWe first compare to previous work and Ground-Truth on synthetic\ndata in Figure 8, including the material maps, a \"Clay\" rendering\nControlMat: A Controlled Generative Approach to Material Capture\n\u2022\n9\nInput\nColor\nNormal\nHeight\nRough\nMetal\nRender\nText\n\u201cterracotta\nbricks\u201d\nGlobal\nLocal\nText\n\u201crusted metal\npanel\u201d\nGlobal\nLocal\nText\n\u201clight brown\nstone\npavement\u201d\nGlobal\nLocal\nFig. 7. Comparison between conditioning options. \u201cText\u201d lines show\nresults using only the text global conditioning, defining the overall appear-\nance, without control over the details. \u201cGlobal\u201d lines show results using\nonly the image global conditioning, providing a more complete appearance\ncondition than text. However, the global conditioning approach doesn\u2019t\npreserve the spatial arrangement and details. \u201cLocal\u201d lines show results\ncombining the global conditioning and the ControlNet [2023], providing\nboth a global and local guidance, better reproducing the target appearance.\nto visualise the height and three renderings under different illu-\nminations. In particular, we can see that both previous work tend\nto bake highlights in the diffuse albedo and artefacts in the height\nmap. As MaterIA [2022] was trained for outdoor materials, it does\nnot infer any metalness map nor handle well shiny materials. We\nalso provide a quantitative comparison to these methods on our\ncomplete synthetic dataset (Sec. 6.1.2) in Tables 1 and 2. We show\nin Table 1 RMSE comparisons for all parameters, and the error on\nre-renderings averaged for 4 different environment illumination,\nconfirming that our model better reproduces the appearance of in-\nput images when relit, and better matches the ground truth material\nmaps than previous work. We further evaluate the Albedo map and\nre-renderings with the SSIM [Wang et al. 2004] and LPIPS [Zhang\net al. 2018] perceptual metrics as other parameter maps are not\nintepretable as natural images.\nWe also compare against previous work on real photographs in\nFigure 9. As we do not know the original lighting in the input picture,\nthe renderings are relit with the same environment illumination\nthan in Figure 8. Here again we show that our approach generates\nTable 1. Quantitative results with MaterIA [Martin et al. 2022] and\nSurfaceNet [Vecchio et al. 2021]. We report here the RMSE\u2193 between\npredicted and ground-truth maps and renderings, except for Normal maps,\nshowing the cosine error\u2193.\nImage Type\nSurfaceNet\nMaterIA\nControlMat\nRenderings\n0.114\n0.123\n0.097\nBase color\n0.108\n0.103\n0.067\nNormal (Cos dist)\n0.308\n0.318\n0.280\nHeight\n0.258\n0.251\n0.216\nRoughness\n0.378\n0.368\n0.304\nMetallic\n0.108\nx\n0.076\nTable 2. Quantitative results with MaterIA [Martin et al. 2022] and\nSurfaceNet [Vecchio et al. 2021]. We report here the perceptual metrics\nSSIM\u2191 and LPIPS\u2193 for images that can be intepreted as natural images:\nRenderings and base color.\nImage Type\nMetric\nSurfaceNet\nMaterIA\nControlMat\nRenderings\nSSIM\n0.677\n0.719\n0.729\nLPIPS\n0.239\n0.211\n0.184\nBase Color\nSSIM\n0.625\n0.650\n0.677\nLPIPS\n0.274\n0.256\n0.239\nmaterials that better reflect the photographed material. Albedo and\nmeso-structure are better disambiguated (1\ud835\udc60\ud835\udc61 row), light is less baked\nin the diffuse albedo (2\ud835\udc5b\ud835\udc51 row), both high and low frequencies of the\ngeometries are better recovered (3\ud835\udc5f\ud835\udc51 & 4\ud835\udc61\u210e row) and overall albedo\ncolor better matches the input image (all rows).\nFinally, we evaluate our methods on a variety of input pho-\ntographs with different material types (stones, bricks, wood, metals,\n...) under varying lighting condition in Figure 10. We first show\nthree results of acquisition without border inpainting, showing re-\nsults that match better the input at the borders, but are not tileable.\nWe then show three results with border inpainting for tileability,\ndemonstrating the appearance preservation while making the re-\nsult tileable. The last three results demonstrate the robustness of\nour approach to different lighting, here with a typical flash light\nillumination. Despite having not explicitely been trained on flash\nlighting, our model is capable of efficiently removing the light from\nthe material maps and recovering plausible material properties.\nWe provide additional results and comparisons on both synthetic\nand real photographs in Supplemental materials.\n6.4\nAblation study\nWe evaluate our different design choices by evaluating our model\u2019s\ndiffusion process elements \u2013patched diffusion, multiscale diffusion\nand patched decoding\u2013 against baseline solutions. We provide ad-\nditional ablation results in the Supplemental Materials, their high\nresolutions make the difference between our elements and naive\ndesign even more apparent.\n6.4.1\nPatched diffusion. We evaluate our method with and without\nPatched Diffusion in Figure 11 . The \"naive\" approach is simply\nthe concatenation of the results of separate diffusion processes per\n10\n\u2022\nGiuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur\nInput\nBase color\nNormal\nHeight\nRoughness\nMetalness\nClay\nRender 1\nRender 2\nRender 3\nGT\nSurfaceNet\nMaterIA\nControlMat\nGT\nSurfaceNet\nMaterIA\nControlMat\nGT\nSurfaceNet\nMaterIA\nControlMat\nFig. 8. Qualitative comparison on synthetic materials between SurfaceNet [Vecchio et al. 2021], MaterIA [Martin et al. 2022] and ControlMat (Ours). We show\nthe Ground Truth and each method input, results parameter maps, Clay and 3 renderings under different lighting than the input image. We see that our\napproach suffers less from baked lighting and better estimates the mesostructure and roughness property. As MaterIA was not designed for conductors, it\ncannot recover well the first example\u2019s appearance. As it does not estimate a metallic map, we replace it by a black image. We show the normalized height\nmaps, and automatically adjust the displacement factor of renders to match the input.\nControlMat: A Controlled Generative Approach to Material Capture\n\u2022\n11\nInput\nBase color\nNormal\nHeight\nRoughness\nMetalness\nClay\nRender 1\nRender 2\nRender 3\nSurfaceNet\nMaterIA\nControlMat\nSurfaceNet\nMaterIA\nControlMat\nSurfaceNet\nMaterIA\nControlMat\nSurfaceNet\nMaterIA\nControlMat\nFig. 9. Qualitative comparison on real photographs between SurfaceNet [Vecchio et al. 2021], MaterIA [Martin et al. 2022] and ControlMat\n(Ours). We show each method\u2019s input, results parameter maps, Clay and 3 renderings under different environment illumination (as we do not know the input\nillumination, we do not exactly reproduce it). We see that approach better separates mesostructure and reflectance, better removing the light from the input\nand reconstructing the surface geometry. As MaterIA doesn\u2019t estimate a metallic map, we replace it by a black image. We show the normalized height maps,\nand automatically adjust the displacement factor of renders to match the input.\n12\n\u2022\nGiuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur\nInput\nBase color\nNormal\nHeight\nRoughness\nMetalness\nRender\nRender clay\n2x\n2x\n2x\nFig. 10. Examples of real materials captured with our method. We show here the diversity of materials our model can estimate. We demonstrate in the\nfirst three rows our acquisition results without border tiling. The next three rows demonstrate the tileability capacity of our border inpainting approach, with\nnon tileable input photographs and tileable output materials. And the last three rows show results of our method when provided flash illuminated photographs,\ndemonstrating our approach robustness, despite having not been explicitely train on this kind of illumination. While the tileable output materials do not\npixel-match the input photograph at the borders, the general material appearance is preserved with the strong tileability benefit. We show the normalized\nheight maps, and automatically adjust the displacement factor of our renders to match the input. We show additional results in Supplemental material.\nControlMat: A Controlled Generative Approach to Material Capture\n\u2022\n13\nInput\nNa\u00efve\nOverlap\nRolling\nFig. 11. Patch diffusion ablation results. We show the results of different\napproaches for patched ablation. We can see that naively concatenating\nseparately diffused patches leads to significant seams in the recomposed\nimage, here the height map. While merging overlapping patches reduces the\nseam problem, it doesn\u2019t solve it and it requires to diffuse additional patches.\nOur noise rolling approch removes any visible seams while maintaining the\nsame number of diffused patches as the Na\u00efve approach.\nInput\n1K Native\n1K Multi-scale\nFig. 12. Multiscale diffusion ablation results. Top row: without multi-\nscale diffusion, our patched diffusion approach may sometime result in\ninconsistent normal orientation. Bottom row: When diffusing at higher res-\nolution, geometries of the generated materials tend to appear flatter, losing\ndetails and flattening large elements. This is most visible when zooming\non the normal maps shown here. Our Multi-scale approach preserves the\nnormals orientation and the mesostructure, even when generating at higher\nresolution.\npatch. We can see that this naive approach results in largely vary-\ning patches, creating strong discontinuities where stitched. Using\noverlapping patches improves consistency and reduces the problem,\nbut doesn\u2019t completely solves it. Further, overlapping patches re-\nquire the diffusion of a larger number of patches, leading to longer\nexecution time. With our proposed noise rolling, we are able to\ngenerate consistent patches and ensure their tileability, preventing\nthe apparition of seams when stitched while requiring the same\nnumber of patch diffusion as the naive approach.\nInput\nNa\u00efve\nOverlap\nMean match.\nFig. 13. Patch decoding ablation results. Naively decoding patches and\nconcatenating them leads to seams in final image. Generating overlapping\npatches and blending them with a gaussian kernel reduces the visibility of\nseams but can result in blurred images (see top example). Adding the mean\nmatching helps remove the last visible seams and preserve the signal better.\n6.4.2\nMultiscale diffusion. During generation at higher resolution,\nif done naively, materials tend to lose details and appear flatter. We\nillustrate the effect of our proposed Multi-scale diffusion in Figure 12.\nThis approach ensures that both large elements (e.g. large stone in\nthe first rows normal map) and high frequency details are preserved.\nWe can see that if generating results at a resolution of 1024 \u00d7 1024,\nthe naive approach results in significantly flatter normals and loss\nof relief in the large stone, while our multi-scale approach preserves\nthe original resolution details. The effects are particularly visible on\nhigh resolution materials (2K+), we invite the reader to zoom in and\nsee the full resolution results in Supplemental Material. This multi-\nscale approach comes at the cost of additional (lower resolution)\ndiffusion processes, requiring at most 40% more time for the total\ngeneration process (depending on the maximum resolution).\n6.4.3\nPatched decoding. We demonstrate here the effect of the VAE\npatch decoding step, allowing to reduce the peak memory consump-\ntion of our process. This lets us generate higher resolution materials,\nwhich we demonstrate up to 4K in Supplemental Material. We show\na comparison of the results with and without the patched decoding\nin Figure 13. We can see that once more, naively concatenating\npatches leads to visible seams in the results. Decoding overlapping\npatches and blending them with Gaussian weights significantly re-\nduces the appearance of seams but doesn\u2019t solve it (bottom example)\nand loses some sharpness (top result). The addition of our mean\nmatching from a lower resolution generation completely removes\nseams while preserving the original signal. The effect is particularly\nvisible at high resolution as demonstrated in the Supplemental Ma-\nterials. Further, the peak memory consumption of our method at 2K\nwithout patched decoding is 20GB while with it, it is 14 GB. At 4K\nwe cannot generate results (out of memory) without the patched\napproach, while ours stay constant at 14GB (but requires to decode\nmore patches, each of which is decoded in \u223c150ms).\n7\nLIMITATIONS AND FUTURE WORK\nBeing generative, our ControlNet-conditioned diffusion model does\nnot guarantee a pixel-perfect match with the input image, and some\n14\n\u2022\nGiuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur\nvariations may arise such as slight color shifts or fine scale texture\nalterations, as illustrated in Fig. 14 (top row). Such deviations are\nless likely to happen with neural image-translation methods [Martin\net al. 2022; Vecchio et al. 2021]. Also, as diffusion models require\nmultiple diffusion steps, the time required by our network to gen-\nerate a material ranges from a few seconds to a few minutes, as\nreported in Sec. 6.2.3, which may hinder some application scenarios.\nNevertheless, recent progress have drastically reduced the number\nof required steps [Song et al. 2023], making Consistency models a\npromising direction for further improving the performance of our\napproach. Our proposed inpainting to generate tileable materials\nfrom non tileable input works great for stochastic materials, but\nmay break the structure a little in highly structured layouts (see\nFig. 14, middle row). This is more likely to happen in challenging,\nor non grid aligned structures, forcing the generation to forcefully\nfix misalignment, even by breaking the structure. In some cases,\nfor shiny material where the input exhibits strong vignetting, our\nmodel mistakenly interprets the material as slightly metallic (see\nFig. 14, bottom row.). This could be improved by randomly includ-\ning strong vignetting in the training data augmentation. Finally,\nas shown in Fig. 10, our approach generates plausible results for\nflash-based acquisition, which is an interesting insight for future\nwork targeting increased acquisition precision.\n8\nCONCLUSION\nWe proposed a generative diffusion model able to create tileable\nmaterials at high resolution. Our model can be conditioned by pho-\ntographs, leveraging the recent ControlNet architecture and improv-\ning material evaluation from a single image. Further, our model can\nalso be conditioned by an image or text prompt, for loose correspon-\ndence and exploration. To enable tileability and high resolution, we\nproposed an adapted inference diffusion process, with noise rolling,\npatched diffusion and decoding, multiscale diffusion, and border\ninpainting. We think that the insights gained in our material-specific\ndiffusion process can extend beyond the material domain, e.g. for\ntextures or 360\u00b0 environment images.\nREFERENCES\nAdobe. 2022. Substance Source. https://substance3d.adobe.com/assets/.\nMiika Aittala, Timo Aila, and Jaakko Lehtinen. 2016. Reflectance Modeling by Neural\nTexture Synthesis. ACM Trans. Graph. 35, 4, Article 65 (jul 2016), 13 pages. https:\n//doi.org/10.1145/2897824.2925917\nMiika Aittala, Tim Weyrich, and Jaakko Lehtinen. 2015. Two-shot SVBRDF Capture\nfor Stationary Materials. ACM Trans. Graph. 34, 4, Article 110 (July 2015), 13 pages.\nhttps://doi.org/10.1145/2766967\nMartin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. 2017. Wasserstein generative\nadversarial networks. In International conference on machine learning. PMLR, 214\u2013\n223.\nOmer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. MultiDiffusion: Fusing\nDiffusion Paths for Controlled Image Generation. arXiv preprint arXiv:2302.08113 2\n(2023).\nAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale GAN training for\nhigh fidelity natural image synthesis. arXiv preprint arXiv:1809.11096 (2018).\nRobert L Cook and Kenneth E. Torrance. 1982. A reflectance model for computer\ngraphics. ACM Transactions on Graphics (ToG) 1, 1 (1982), 7\u201324.\nBin Dai and David Wipf. 2019. Diagnosing and enhancing VAE models. arXiv preprint\narXiv:1903.05789 (2019).\nValentin Deschaintre, Miika Aittala, Fr\u00e9do Durand, George Drettakis, and Adrien\nBousseau. 2018. Single-Image SVBRDF Capture with a Rendering-Aware Deep\nNetwork. ACM Transactions on Graphics (SIGGRAPH Conference Proceedings) 37,\n128 (aug 2018), 15. http://www-sop.inria.fr/reves/Basilic/2018/DADDB18\nGT Render\nGT base color\nEstimation\nInput\nNon tileable\nTileable\nInput\nRender\nFig. 14. Limitations. (top row) We illustrate a small color shift happening\nin some results due to the generative nature of our architecture. In the\nmiddle row, we illustrate slightly tiled clay renders of our approach\u2019s results\nwith and without tileability. As the input is strongly structured with a\nchallenging pattern, our inpainting does not manage to enforce tileability\nwhile preserving the natural structure. In the bottom row, we illustrate a\nresult where our method erroneously attributed some degree of metalness to\nthe leather properties, leading to a slightly metallic re rendered appearance.\nWe believe this may be due to the strong vignetting in the input.\nValentin Deschaintre, Miika Aittala, Fr\u00e9do Durand, George Drettakis, and Adrien\nBousseau. 2019. Flexible SVBRDF Capture with a Multi-Image Deep Network. Com-\nputer Graphics Forum(Eurographics Symposium on Rendering Conference Proceedings)\n38, 4 (jul 2019), 13. http://www-sop.inria.fr/reves/Basilic/2019/DADDB19\nValentin Deschaintre, George Drettakis, and Adrien Bousseau. 2020. Guided Fine-\nTuning for Large-Scale Material Transfer. Computer Graphics Forum (Proceedings of\nthe Eurographics Symposium on Rendering) 39, 4 (2020). http://www-sop.inria.fr/\nreves/Basilic/2020/DDB20\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image\nsynthesis. Advances in Neural Information Processing Systems 34 (2021), 8780\u20138794.\nAlexey Dosovitskiy and Thomas Brox. 2016.\nGenerating images with perceptual\nsimilarity metrics based on deep networks. Advances in neural information processing\nsystems 29 (2016).\nPatrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-\nresolution image synthesis. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. 12873\u201312883.\nMichael Fischer and Tobias Ritschel. 2022. Metappearance: Meta-Learning for Visual\nAppearance Reproduction. ACM Trans Graph (Proc. SIGGRAPH Asia) 41, 4 (2022).\nDUAN Gao, Xiao Li, Yue Dong, Pieter Peers, Kun Xu, and Xin Tong. 2019. Deep\nInverse Rendering for High-Resolution SVBRDF Estimation from an Arbitrary\nNumber of Images. ACM Trans. Graph. 38, 4, Article 134 (jul 2019), 15 pages.\nhttps://doi.org/10.1145/3306346.3323042\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversar-\nial Nets. In Advances in Neural Information Processing Systems, Z. Ghahramani,\nM. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.), Vol. 27. Cur-\nran Associates, Inc.\nhttps://proceedings.neurips.cc/paper_files/paper/2014/file/\n5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf\nControlMat: A Controlled Generative Approach to Material Capture\n\u2022\n15\nD. Guarnera, G. C. Guarnera, A. Ghosh, C. Denk, and M. Glencross. 2016. BRDF\nRepresentation and Acquisition. In Proceedings of the 37th Annual Conference of\nthe European Association for Computer Graphics: State of the Art Reports (Lisbon,\nPortugal) (EG \u201916). Eurographics Association, Goslar, DEU, 625\u2013650.\nPaul Guerrero, Milo\u0161 Ha\u0161an, Kalyan Sunkavalli, Radom\u00edr M\u011bch, Tamy Boubekeur, and\nNiloy J. Mitra. 2022. MatFormer: A Generative Model for Procedural Materials. ACM\nTrans. Graph. 41, 4, Article 46 (jul 2022), 12 pages. https://doi.org/10.1145/3528223.\n3530173\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C\nCourville. 2017. Improved training of wasserstein gans. Advances in neural infor-\nmation processing systems 30 (2017).\nJie Guo, Shuichang Lai, Chengzhi Tao, Yuelong Cai, Lei Wang, Yanwen Guo, and Ling-\nQi Yan. 2021. Highlight-Aware Two-Stream Network for Single-Image SVBRDF\nAcquisition. ACM Trans. Graph. 40, 4, Article 123 (jul 2021), 14 pages.\nhttps:\n//doi.org/10.1145/3450626.3459854\nJie Guo, Shuichang Lai, Qinghao Tu, Chengzhi Tao, Changqing Zou, and Yanwen Guo.\n2023. Ultra-High Resolution SVBRDF Recovery from a Single Image. ACM Trans.\nGraph. (apr 2023). https://doi.org/10.1145/3593798 Just Accepted.\nYu Guo, Cameron Smith, Milo\u0161 Ha\u0161an, Kalyan Sunkavalli, and Shuang Zhao. 2020.\nMaterialGAN: Reflectance Capture Using a Generative SVBRDF Model. ACM Trans.\nGraph. 39, 6, Article 254 (nov 2020), 13 pages.\nhttps://doi.org/10.1145/3414685.\n3417779\nPhilipp Henzler, Valentin Deschaintre, Niloy J Mitra, and Tobias Ritschel. 2021. Gen-\nerative Modelling of BRDF Textures from Flash Images. ACM Trans Graph (Proc.\nSIGGRAPH Asia) 40, 6 (2021).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\nYiwei Hu, Julie Dorsey, and Holly Rushmeier. 2019. A Novel Framework for Inverse\nProcedural Texture Modeling. ACM Trans. Graph. 38, 6, Article 186 (Nov. 2019),\n14 pages. https://doi.org/10.1145/3355089.3356516\nYiwei Hu, Paul Guerrero, Milos Hasan, Holly Rushmeier, and Valentin Deschaintre.\n2022a. Node Graph Optimization Using Differentiable Proxies. In ACM SIGGRAPH\n2022 Conference Proceedings (Vancouver, BC, Canada) (SIGGRAPH \u201922). Association\nfor Computing Machinery, New York, NY, USA, Article 5, 9 pages. https://doi.org/\n10.1145/3528233.3530733\nYiwei Hu, Paul Guerrero, Milos Hasan, Holly Rushmeier, and Valentin Deschaintre. 2023.\nGenerating Procedural Materials from Text or Image Prompts. In ACM SIGGRAPH\n2023 Conference Proceedings.\nYiwei Hu, Chengan He, Valentin Deschaintre, Julie Dorsey, and Holly Rushmeier. 2022b.\nAn Inverse Procedural Modeling Pipeline for SVBRDF Maps. ACM Trans. Graph. 41,\n2, Article 18 (jan 2022), 17 pages. https://doi.org/10.1145/3502431\nQingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen,\nZhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, Jesse Engel, Quoc V.\nLe, William Chan, Zhifeng Chen, and Wei Han. 2023. Noise2Music: Text-conditioned\nMusic Generation with Diffusion Models. arXiv:2302.03917 [cs.SD]\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image\ntranslation with conditional adversarial networks. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition. 1125\u20131134.\n\u00c1lvaro Barbero Jim\u00e9nez. 2023. Mixture of Diffusers for scene composition and high\nresolution image generation. arXiv preprint arXiv:2302.02412 (2023).\nBrian Karis. 2013. Real shading in unreal engine 4. Proc. Physically Based Shading\nTheory Practice 4, 3 (2013), 1.\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing\nof gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196\n(2017).\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo\nAila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition. 8110\u20138119.\nDiederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv\npreprint arXiv:1312.6114 (2013).\nXiao Li, Yue Dong, Pieter Peers, and Xin Tong. 2017. Modeling Surface Appearance\nfrom a Single Photograph Using Self-Augmented Convolutional Neural Networks.\nACM Trans. Graph. 36, 4, Article 45 (jul 2017), 11 pages. https://doi.org/10.1145/\n3072959.3073641\nRosalie Martin, Arthur Roullier, Romain Rouffet, Adrien Kaiser, and Tamy Boubekeur.\n2022. MaterIA: Single Image High-Resolution Material Capture in the Wild. Com-\nputer Graphics Forum 41, 2 (2022), 163\u2013177.\nhttps://doi.org/10.1111/cgf.14466\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14466\nWes McDermott. 2018. Maps common to both workflow. Allergorithmic, 75\u201379. https:\n//substance3d.adobe.com/tutorials/courses/the-pbr-guide-part-2\nLars Mescheder. 2018. On the convergence properties of gan training. arXiv preprint\narXiv:1801.04406 1 (2018), 16.\nLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. 2016. Unrolled generative\nadversarial networks. arXiv preprint arXiv:1611.02163 (2016).\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.\nLearning transferable visual models from natural language supervision. In Interna-\ntional conference on machine learning. PMLR, 8748\u20138763.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n2022.\nHierarchical Text-Conditional Image Generation with CLIP Latents.\narXiv:2204.06125 [cs.CV]\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic back-\npropagation and approximate inference in deep generative models. In International\nconference on machine learning. PMLR, 1278\u20131286.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684\u201310695.\nRobin Rombach, Patrick Esser, and Bj\u00f6rn Ommer. 2020a. Making sense of cnns: Interpret-\ning deep representations and their invariances with inns. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\nXVII 16. Springer, 647\u2013664.\nRobin Rombach, Patrick Esser, and Bjorn Ommer. 2020b. Network-to-network transla-\ntion with conditional invertible neural networks. Advances in Neural Information\nProcessing Systems 33 (2020), 2784\u20132797.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional\nnetworks for biomedical image segmentation. In Medical Image Computing and\nComputer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich,\nGermany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234\u2013241.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Den-\nton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad\nNorouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language\nUnderstanding. arXiv:2205.11487 [cs.CV]\nLiang Shi, Beichen Li, Milo\u0161 Ha\u0161an, Kalyan Sunkavalli, Tamy Boubekeur, Radomir Mech,\nand Wojciech Matusik. 2020. MATch: Differentiable Material Graphs for Procedural\nMaterial Capture. ACM Trans. Graph. 39, 6, Article 196 (Dec. 2020), 15 pages.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015.\nDeep unsupervised learning using nonequilibrium thermodynamics. In International\nConference on Machine Learning. PMLR, 2256\u20132265.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. 2023. Consistency\nModels. arXiv:2303.01469 [cs.LG]\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning.\nAdvances in neural information processing systems 30 (2017).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances\nin neural information processing systems 30 (2017).\nGiuseppe Vecchio, Simone Palazzo, and Concetto Spampinato. 2021. SurfaceNet: Ad-\nversarial SVBRDF Estimation from a Single Image. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. 12840\u201312848.\nGiuseppe Vecchio, Renato Sortino, Simone Palazzo, and Concetto Spampinato.\n2023.\nMatFuse: Controllable Material Generation with Diffusion Models.\narXiv:2308.11408 [cs.CV]\nBruce Walter, Stephen R Marschner, Hongsong Li, and Kenneth E Torrance. 2007.\nMicrofacet models for refraction through rough surfaces. In Proceedings of the 18th\nEurographics conference on Rendering Techniques. 195\u2013206.\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image quality\nassessment: from error visibility to structural similarity. IEEE transactions on image\nprocessing 13, 4 (2004), 600\u2013612.\nKai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. 2021. Designing a practical\ndegradation model for deep blind image super-resolution. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision. 4791\u20134800.\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding Conditional Control to\nText-to-Image Diffusion Models.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The\nunreasonable effectiveness of deep features as a perceptual metric. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition. 586\u2013595.\nXilong Zhou, Milos Hasan, Valentin Deschaintre, Paul Guerrero, Yannick Hold-Geoffroy,\nKalyan Sunkavalli, and Nima Khademi Kalantari. 2023a. PhotoMat: A Material\nGenerator Learned from Single Flash Photos. In ACM SIGGRAPH 2023 Conference\nProceedings (Los Angeles, CA, USA) (SIGGRAPH \u201923). Association for Computing\nMachinery, New York, NY, USA.\nXilong Zhou, Milos Hasan, Valentin Deschaintre, Paul Guerrero, Kalyan Sunkavalli, and\nNima Khademi Kalantari. 2022. TileGen: Tileable, Controllable Material Generation\nand Capture. In SIGGRAPH Asia 2022 Conference Papers (Daegu, Republic of Korea)\n(SA \u201922). Association for Computing Machinery, New York, NY, USA, Article 34,\n9 pages. https://doi.org/10.1145/3550469.3555403\nXilong Zhou, Milo\u0161 Ha\u0161an, Valentin Deschaintre, Paul Guerrero, Kalyan Sunkavalli,\nand Nima Khademi Kalantari. 2023b. A Semi-Procedural Convolutional Material\nPrior. Computer Graphics Forum n/a, n/a (2023). https://doi.org/10.1111/cgf.14781\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14781\nXilong Zhou and Nima Khademi Kalantari. 2021. Adversarial Single-Image SVBRDF\nEstimation with Hybrid Training. Computer Graphics Forum (2021).\n16\n\u2022\nGiuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur\nXilong Zhou and Nima Khademi Kalantari. 2022. Look-Ahead Training with Learned Re-\nflectance Loss for Single-Image SVBRDF Estimation. ACM Transactions on Graphics\n41, 6 (12 2022). https://doi.org/10.1145/3550454.3555495\n"
  },
  {
    "title": "PromptTTS 2: Describing and Generating Voices with Text Prompt",
    "link": "https://arxiv.org/pdf/2309.02285.pdf",
    "upvote": "11",
    "text": "Preprint. Working in Progress.\nPROMPTTTS 2:\nDESCRIBING\nAND GENERATING\nVOICES WITH TEXT PROMPT\nYichong Leng\u2217\u2020, Zhifang Guo\u2217, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu\nDongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao\nTao Qin, Jiang Bian\nMicrosoft Research\nABSTRACT\nSpeech conveys more information than text, as the same word can be uttered in var-\nious voices to convey diverse information. Compared to traditional text-to-speech\n(TTS) methods relying on speech prompts (reference speech) for voice variability,\nusing text prompts (descriptions) is more user-friendly since speech prompts can be\nhard to find or may not exist at all. TTS approaches based on the text prompt face\ntwo main challenges: 1) the one-to-many problem, where not all details about voice\nvariability can be described in the text prompt, and 2) the limited availability of text\nprompt datasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address these\nchallenges with a variation network to provide variability information of voice\nnot captured by text prompts, and a prompt generation pipeline to utilize the large\nlanguage models (LLM) to compose high quality text prompts. Specifically, the\nvariation network predicts the representation extracted from the reference speech\n(which contains full information about voice variability) based on the text prompt\nrepresentation. For the prompt generation pipeline, it generates text prompts for\nspeech with a speech language understanding model to recognize voice attributes\n(e.g., gender, speed) from speech and a large language model to formulate text\nprompts based on the recognition results. Experiments on a large-scale (44K hours)\nspeech dataset demonstrate that compared to the previous works, PromptTTS 2\ngenerates voices more consistent with text prompts and supports the sampling of\ndiverse voice variability, thereby offering users more choices on voice generation.\nAdditionally, the prompt generation pipeline produces high-quality text prompts,\neliminating the large labeling cost. The demo page of PromptTTS 2 is available\nonline1.\n1\nINTRODUCTION\nIn recent years, there have been significant advancements in text-to-speech (TTS) systems (Ren et al.,\n2019; Wang et al., 2017; Popov et al., 2021b; Chen et al., 2022b), which have resulted in enhanced\nintelligibility and naturalness of synthesized speech (Tan et al., 2021). Some TTS systems have\nachieved a level of quality comparable to that of single-speaker recording (Tan et al., 2022), and\nlarge-scale TTS systems have been developed for multi-speaker scenarios (Wang et al., 2023; Shen\net al., 2023; Li et al., 2023; Le et al., 2023). Despite these improvements, modeling voice variability\nremains a challenge, as the same word can be delivered in various ways such as emotion and tone to\nconvey different information. Conventional TTS methods often rely on speaker information (e.g.,\nspeaker ID) (Chen et al., 2020; Gibiansky et al., 2017) or speech prompts (reference speech) (Yan\net al., 2021; Casanova et al., 2022) to model the voice variability, which are not user-friendly, as the\nspeaker ID is pre-defined and the suitable speech prompt is hard to find or even does not exist (in\nvoice creation scenario). Given that natural language is a convenient interface for users to express\n\u2217Equal contribution.\n\u2020This work was conducted at Microsoft. Corresponding author: Xu Tan, xuta@microsoft.com\n1https://speechresearch.github.io/prompttts2\n1\narXiv:2309.02285v2  [eess.AS]  12 Oct 2023\nPreprint. Working in Progress.\ntheir intentions on voice generation, a more promising direction for modeling voice variability is to\nemploy text prompts (Guo et al., 2023; Ji et al., 2023; Ramesh et al., 2022; Brown et al., 2020b) that\ndescribe voice characteristics. This approach enables easy voice creation through text prompt writing.\nIn general, TTS systems based on text prompts are trained with a text prompt dataset, consisting\nof speech and its corresponding text prompt. Voice is generated by model conditioned on the text\ncontent to be synthesized and the text prompt describing the variability or style of the voice. Two\nprimary challenges persist in text prompt TTS systems:\n\u2022 One-to-Many Challenge: Speech contains voice variability in detail, making it impossible for text\nprompts to fully capture all characteristics in speech. So different speech samples can correspond to\nthe same text prompt 2. This one-to-many mapping increases the difficulty of TTS model training,\nleading to over-fitting or mode collapse. To the best of our knowledge, no mechanisms have been\nspecifically designed to mitigate the one-to-many issue in TTS systems based on text prompts.\n\u2022 Data-Scale Challenge: Dataset of text prompts describing the voice is hard to construct since\nthe text prompt is rare on the internet. So venders are engaged to compose text prompts, which\nis both costly and laborious. Consequently, the text prompt datasets tend to be relatively small\n(approximately 20K sentences) (Guo et al., 2023) or not openly accessible (Yang et al., 2023),\nposing an obstacle for the future research on text prompt based TTS systems.\nTo address the aforementioned challenges, in our work, we introduce PromptTTS 2 that proposes\na variation network to model the voice variability information of speech not captured by the text\nprompts and utilizes a prompt generation pipeline to generate high-quality text prompts:\nFor the one-to-many challenge, we propose a variation network to predict the missing information of\nvoice variability from the text prompt. The variation network is trained with the help of a reference\nspeech, which is regarded to contain all information about voice variability (Wang et al., 2023; Shen\net al., 2023). Generally, the TTS model in PromptTTS 2 consists of a text prompt encoder for text\nprompts, a reference speech encoder for reference speech, and a TTS module to synthesize speech\nbased on the representations extracted by text prompt encoder and reference speech encoder. Variation\nnetwork is trained to predict the reference representation from reference speech encoder based on the\nprompt representation from text prompt encoder 3. By employing the diffusion model (Song et al.,\n2020) in the variation network, we can sample different information about voice variability from\nGaussian noise conditioned on text prompts to control the characteristics of synthesized speech, and\nthus offering users greater flexibility in generating voices.\nFor the data-scale challenge, we propose a pipeline to automatically generate text prompts for speech\nwith a speech language understanding (SLU) model to recognize voice attributes (e.g., gender, speed)\nfrom speech and a large language model (LLM) to compose text prompts based on the recognition\nresults. Specifically, we employ a SLU model to describe the voice from many attributes (e.g.,\nemotion, gender) by recognizing the attribute values for each speech sample within a speech dataset.\nSubsequently, sentences are written to describe each attribute individually, and the text prompt is\nconstructed by combining these sentences. In contrast to previous work (Guo et al., 2023), which\nrelies on vendors to write and combine sentences, PromptTTS 2 capitalizes on the capabilities of\nLLM (Brown et al., 2020a; Chowdhery et al., 2022) that have demonstrated human-level performance\nin various tasks (Bubeck et al., 2023; Touvron et al., 2023). We instruct LLM to write high-quality\nsentences describing the attributes and combine the sentences into a comprehensive text prompt. This\nfully automated pipeline eliminates the need for human intervention in text prompt writing.\nThe contributions of this paper are summarized as follows:\n\u2022 We design a diffusion-based variation network to model the voice variability not covered by the text\nprompt, addressing the one-to-many issue in the text prompt based TTS systems. During inference,\nthe voice variability can be controlled by sampling from different Gaussian noise conditioned on\nthe text prompt.\n2For instance, the text prompt \u201cPlease generate a voice of a boy shouting out\u201d can describe numerous shouting\nvoices from boys that differ in details such as timbre.\n3It is worth noting that reference speech is only used in training variation network but not used in inference.\n2\nPreprint. Working in Progress.\n\u2022 We construct and release a text prompt dataset generated by LLM, equipped with a pipeline for text\nprompt generation. The pipeline produces high quality text prompts and reduces the reliance on\nvendors to write text prompts.\n\u2022 We evaluate PromptTTS 2 on a large-scale speech dataset consisting of 44K hours speech data. Ex-\nperimental results demonstrate that PromptTTS 2 outperforms previous works in generating voices\nthat correspond more accurately to the text prompt while supports controlling voice variability\nthrough sampling from Gaussian noise.\n2\nBACKGROUND\nHow to model voice variability has long been a crucial direction in text-to-speech (TTS) re-\nsearch (Wang et al., 2018; Bae et al., 2020; Bak et al., 2021). In the early stage, TTS systems\nprimarily focus on single-speaker scenarios (Wang et al., 2017; Ar\u0131k et al., 2017; Ren et al., 2019),\nwhere voice information is implicitly incorporated into neural networks. Subsequently, the need for\nmodeling diverse voices emerges, leading to the advancement of multi-speaker TTS systems (Gibian-\nsky et al., 2017; Chen et al., 2020; Popov et al., 2021a), in which voice variability is controlled but\nlimited in speakers in the dataset. To adapt multi-speaker TTS systems to new speakers, few-shot\nadaptive TTS approaches (Chen et al., 2021; Yan et al., 2021; Huang et al., 2022) have been employed,\nwhich involve fine-tuning the multi-speaker TTS model on a limited amount of target speaker data.\nIn contrast, zero-shot adaptive TTS models utilize in-context learning to generate new voices by\nexclusively modeling speaker characteristics from a speech prompt (i.e., reference speech) (Wu et al.,\n2022; Wang et al., 2023; Shen et al., 2023; Li et al., 2023; Le et al., 2023).\nSince finding reference speech can be cumbersome and the speech data of target speaker is hard to\ncollect or even does not exist (in the voice creation scenario), above methods on modeling voice\nvariability is not user-friendly and scenario-limited. To achieve voice generation in a more natural\nand general manner, text prompt based methods have been proposed (Shimizu et al., 2023; Liu et al.,\n2023a), which create voices using text descriptions and require human-annotated text prompt datasets\nfor speech. However, human-constructed datasets are often limited in scale (Guo et al., 2023) or\npublicly inaccessible (Yang et al., 2023) due to the associated costs. In this work, we propose a\npipeline that employs LLM to generate text prompts, thereby reducing the reliance on human labor.\nGiven that it is impossible to comprehensively describe speech with fine-grained details (Yang et al.,\n2022; Qian et al., 2019; 2020) using text prompts alone, there exists the one-to-many problem in\nthe text prompt based TTS system. Different with previous works that try to construct text prompts\nwith more details (Guo et al., 2023; Shimizu et al., 2023), which can only alleviate the one-to-many\nproblem to some extend, we propose the variation network to address the one-to-many problem by\npredicting the missing information about voice variability conditioned on the text prompt.\n3\nPROMPTTTS 2\nIn this section, we firstly give an overview on the TTS system in PromptTTS 2. Then we introduce\nthe variation network that predicts the missing information about voice variability in the text prompt.\nFinally, we describe our pipeline to leverage the LLM to write the text prompt dataset.\n3.1\nOVERVIEW OF TTS SYSTEM\nFigure 1a and 1b present an overview of the TTS system in PromptTTS 2. Figure 1a depicts a TTS\nmodule for synthesizing speech, with its characteristics controlled by a style module. Figure 1a skips\nthe details for TTS module because the TTS module can be any backbone capable of synthesizing\nspeech from phonemes. We adopt TTS backbone from Shen et al. (2023), described in Appendix B.\nFigure 1b illustrates the details of the style module. During training, in line with previous works (Guo\net al., 2023), we employ a BERT-based model as a text prompt encoder to extract prompt hidden. To\naddress the one-to-many mapping problem (introduced in Section 1), we utilize a reference speech\nencoder to model the information about voice variability not covered by the text prompt, which\ntakes a reference speech as input and outputs a reference hidden (Shen et al., 2023; Wang et al.,\n2023). Since both the text prompt and reference speech can have varying lengths, we extract a\n3\nPreprint. Working in Progress.\nFigure 1: The overview of TTS system in PromptTTS 2. Subfigure (a) is a TTS module to synthesize\nspeech, whose characteristics are controlled by a style module. Subfigure (b) shows the style\nmodule which takes the text prompt and reference speech as input and extracts prompt representation\n(P1, ..., PM) and reference representation (R1, ..., RN). Since the reference speech is not available\nin inference, we further propose a diffusion-based variation network (Subfigure (c)) to predict the\nreference representation based on the prompt representation.\nfixed-length representation using cross attention (Vaswani et al., 2017) with a fixed number of query\ntokens for both text prompt and reference speech. More specifically, the (text) prompt representation\n(P1, ..., PM) are extracted by learnable query tokens (QP1, ..., QPM ), and the reference (speech)\nrepresentations (R1, ..., RN) are extracted by learnable query tokens (QR1, ..., QRN ). M and N\nrepresent the fixed lengths of prompt and reference representations, respectively.\nDuring inference, only the text prompt is available, and the reference speech is not accessible, so\nwe train a variation network to predict the reference representation (R1, ..., RN) conditioned on the\nprompt representation (P1, ..., PM), and thus the inference can be conducted with the text prompt\nonly. The variation network is introduced in detail in the next section.\n3.2\nVARIATION NETWORK\nThe variation network aims to predict the reference representation (R1, ..., RN) conditioned on the\nprompt representation (P1, ..., PM). To model the reference representation, our variation network\nemploys the diffusion model (Ho et al., 2020), which has demonstrated a robust capability in modeling\nmultimodal distributions and complex data spaces (Kim et al., 2022; Ramesh et al., 2022; Ho et al.,\n2022; Nichol & Dhariwal, 2021; Leng et al., 2022). The diffusion model also enables variation\nnetwork to sample different voice variability from Gaussian noise. Specifically, the diffusion model\nconsists of a diffusion process and denoising process:\nFor the diffusion process, given the reference representation z0, the forward diffusion process\ntransforms it into Gaussian noise under the noise schedule \u03b2 as follows:\ndzt = \u22121\n2\u03b2tzt dt +\np\n\u03b2t dwt,\nt \u2208 [0, 1],\n(1)\nFor the denoising process, the denoising process aims to transform the noisy representation zt to the\nreference representation z0 by the following formulation (Song et al., 2020):\ndzt = \u22121\n2(zt + \u2207 log pt(zt))\u03b2t dt,\nt \u2208 [0, 1].\n(2)\n4\nPreprint. Working in Progress.\nFigure 2: The overview of our prompt generation pipeline. We first recognize attributes from speech\nwith the SLU model. Then LLM is instructed to generate sentences describing each attribute and\ncombine the sentences of each attribute to formulate text prompts.\nVariation network is trained to estimate the gradients of log-density of noisy data (\u2207 log pt(zt)) by\npredicting the origin reference representation z0 (Song et al., 2020; Shen et al., 2023), conditioned\non the prompt representation, noised reference representation, and diffusion step t that indicates the\ndegree of noise in diffusion model.\nFigure 1c presents the detailed architecture of variation network, which is based on the Transformer\nEncoder (Vaswani et al., 2017). The input of variation network comprises the prompt representation\n(P1, ..., PM), noised reference representation (Rt\n1, ..., P t\nM), and diffusion step t. The output of\nvariation network is the hidden representation corresponding to the noised reference representation,\noptimized using L1 loss. To enhance the model\u2019s awareness of the diffusion step, we use FiLM (Perez\net al., 2018) in each layer of the Transformer Encoder (Liu et al., 2023b).\nIn summary, during inference, we initially extract prompt representation from the text prompt using\nthe style module. Subsequently, variation network predicts the reference representation conditioned\non the prompt representation by denoising from Gaussian noise. Finally, the prompt representation\nare concatenated with the reference representation to guide the TTS module through cross attention.\n3.3\nTEXT PROMPT GENERATION WITH LLM\nIn this section, we introduce the prompt generation pipeline to build the text prompt dataset. As\nillustrated in Figure 2, the pipeline consists of a SLU (speech language understanding) part and a\nLLM (large language model) part. Given a speech, the SLU part involves tagging some labels with\nthe speech language understanding models by recognizing attributes (e.g., gender, emotion, age) from\nspeech; and the LLM part involves instructing large language model to write text prompts based on\nthe labels (i.e., recognition results).\nAs there exist many SLU models (Baevski et al., 2020; Arora et al., 2022) to recognize attributes\nfrom speech, we focus on the LLM part for the text prompt writing based on the recognition results\nof SLU model. It is worth noting that text prompts written by LLM part can be reused for multiple\nspeech with the same labels4. In order to improve the quality of text prompts, the LLM is instructed\nstep by step to compose text prompts with high diversity in vocabulary and sentence format. The\ndetail about LLM part is shown in Figure 3 and introduced as follows:\n\u2022 Keyword Construction The SLU models recognize attributes that can describe speech characteris-\ntics. For each attribute, the SLU model recognizes several classes representing the values of the\nattributes. Subsequently, LLM is instructed to generate several keywords describing each class\nfor every attribute. In the stage 1 of Figure 3, we utilize four attributes, including gender, pitch,\nspeed, and volume. The \u201cgender\u201d attribute comprises two classes: male and female. The keywords\ngenerated by LLM for the male class are \u201cman\u201d,\u201che\u201d, and so on.\n\u2022 Sentence Construction In addition to the variance in keywords, we also require variance in\nsentences. Therefore, we instruct LLM to generate multiple sentences for each attribute. A\nplaceholder for the attribute is used by LLM when composing these sentences (e.g., word \u201c[Gender]\u201d\nis the placeholder for \u201cgender\u201d attribute in the stage 2 of Figure 3). The design of the placeholder\noffers two advantages: 1) it emphasizes the attribute for LLM, ensuring that the attribute is not\nomitted in the output sentence, and 2) the output sentence serves as a general template for all classes\n4Since the recognition results of SLU models are in a pre-defined label set.\n5\nPreprint. Working in Progress.\nFigure 3:\nText prompt generation using LLM: In Stage 1, LLM generates keywords for each\nattribute (gender, pitch, speed, and volume). In Stage 2, LLM composes sentences for each attribute,\nintegrating placeholders for the corresponding attributes. In Stage 3, LLM combines the sentences\nfrom Stage 2 to create a sentence that simultaneously describes multiple attributes. In Stage 4, the\ndataset is instantiated by initially sampling a combined sentence and subsequently sampling keywords\nto replace the placeholders within the sentence.\nfor an attribute, enabling the generation of diverse text prompts by filling the placeholder with\ndifferent keywords. In the provided example, the stage 2 of Figure 3 illustrates several sentences\ncomposed by LLM that describe different attributes.\n\u2022 Sentence Combination Since text prompts can describe more than one attribute, we perform\nsentence combination based on the sentences generated in the stage 2. LLM is instructed to\ncombine sentences describing different attributes into a new sentence, allowing us to obtain text\nprompts representing various combinations of attributes. It is worth noting that the sentences\ngenerated by LLM are always complete and free of grammatical errors. In contrast, users of text\nprompt based TTS systems may not always describe voices in a formal manner. Consequently, we\nalso instruct LLM to write phrases to enhance the diversity of constructed sentences. In the stage 3\nof Figure 3, we present some example combination sentences and phrases generated by LLM.\n\u2022 Dataset Instantiation The results generated from the previously described three stages form the\nfinal text prompt dataset, which is employed alongside a speech dataset. For each instance of speech\nS within the speech dataset, we tag a class label on every attribute with SLU models. Following this,\nwe select a sentence that encompasses all the attributes of speech S. Next, we obtain a keyword\nfor each attribute of speech S based on its corresponding class label. The ultimate text prompt is\ninstantiated by substituting all placeholders in the sentence with their corresponding keywords. In\nthe stage 4 of Figure 3, we provide examples illustrating the finalized text prompts. The speech S\nand the corresponding finalized text prompt formulate a speech-prompt paired data.\nWe present a brief discussion on the scalability of our pipeline. With the help of our pipeline,\nincorporating a new attribute requires only the definition of classes for the new attribute and the\ntagging of the speech dataset for that attribute using a SLU model (Baevski et al., 2020; Arora\net al., 2022). For example, if we intend to introduce a new \u201cage\u201d attribute into the pipeline, we can\ndefine three classes corresponding to the \u201cage\u201d attribute, namely \u201cteenager\u201d, \u201cadult\u201d and \u201celder\u201d.\nSubsequently, the pipeline can generate a text prompt dataset for the \u201cage\u201d attribute with the help\nof LLM and a SLU model on \u201cage\u201d attribute to tag the speech dataset. In summary, our pipeline\nsignificantly simplifies the process of adding new attributes, allowing for easier expansion and\nadaptability to diverse speech characteristics. We provide an example of our pipeline in Appendix A,\nwhich shows the dialogue process with LLM.\n6\nPreprint. Working in Progress.\nTable 1: The accuracy (%) of synthesized speech on the attribute control of PromptTTS 2 and\nbaselines.\nModel\nGender\nSpeed\nVolume\nPitch\nMean\nPromptTTS (Guo et al., 2023)\n98.01\n89.66\n92.49\n85.98\n91.54\nInstructTTS (Yang et al., 2023)\n97.24\n90.57\n91.26\n86.82\n91.47\nPromptTTS 2\n98.23\n92.64\n92.56\n89.89\n93.33\n4\nEXPERIMENT CONFIGURATION\nIn this section, we present the experimental configurations, including the datasets, TTS backbone,\nbaseline systems and experiment details.\nDatasets\nFor the speech dataset, we employ the English subset of the Multilingual LibriSpeech\n(MLS) dataset (Pratap et al., 2020), which comprises 44K hours of transcribed speech data from\nLibriVox audiobooks. For the text prompt data, we utilize PromptSpeech (Guo et al., 2023) that\ncontains 20K text prompts written by human describing speech from four attributes including pitch,\ngender, volume, and speed. We also utilize our prompt generation pipeline to write 20K text prompts\nwith the help of LLM (GPT-3.5-TURBO). The test set of PromptSpeech is used as test data, which\ncontains 1305 text prompts. For the SLU model on attribute recognition, we identify gender using an\nopen-source model5, and the other attributes (i.e., pitch, volume, and speed) are recognized using\ndigital signal processing tools6.\nTTS Backbone\nIn general, PromptTTS 2 extracts a fixed-dimension representation to control the\ncharacteristics of synthesized speech. This approach can be incorporated into any TTS backbone\nby integrating the representations into the TTS backbone with cross attention. Given that a larger\nspeech dataset may contain more voice variations, we apply PromptTTS 2 to a large speech dataset\nand adopt the TTS backbone from a state-of-the-art large-scale TTS system, NaturalSpeech 2 (Shen\net al., 2023). The details about the TTS backbone can be found in Appendex B.\nBaseline Systems\nWe compare PromptTTS 2 with current SOTA systems of text prompt based\nTTS, PromptTTS (Guo et al., 2023) and InstructTTS (Yang et al., 2023). To ensure a fair comparison,\nwe modify the backbone in baseline systems to the latent diffusion backbone used in PromptTTS 2.\nExperiment Details\nThe number of layers in the reference speech encoder and variation network is\n6 and 12, respectively, with a hidden size of 512. The query number M, N in style module is both\nset to 8. Concerning the TTS backbone and the text prompt encoder, we adhere to the settings in\nNaturalSpeech 2 (Shen et al., 2023) and PromptTTS (Guo et al., 2023), respectively. The training\nconfiguration is also derived from NaturalSpeech 2 (Shen et al., 2023).\n5\nRESULT\nIn this section, we evaluate the effectiveness of PromptTTS 2. Firstly, We compare the accuracy of\nattribute control and the speech quality between PromptTTS 2 and baseline systems in Section 5.1.\nIn Section 5.2, we demonstrate that the variation network successfully captures the information about\nvoice variability. In Section 5.3, we compare the text prompts generated by our pipeline with those\nwritten by human or other LLM based method. Finally, we conduct an analysis on the style module\nin Section 5.4 and perform an extension on face-to-voice (Face2Voice) generation in Section 5.5.\n5.1\nEFFECTIVENESS OF PROMPTTTS 2\nWe evaluate the effectiveness of PromptTTS 2 from the perspective of attribute control and speech\nquality. First, we compare the accuracy of attribute control between PromptTTS 2 and baseline\n5https://github.com/karthikbhamidipati/multi-task-speech-classification\n6https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder\n7\nPreprint. Working in Progress.\nTable 2: The results of speech quality with 95% confidence intervals. GT stands for the recording.\nCodec reconstruction stands for that the waveform is encoded to latent representation first and then\nreversed to waveform by the decoder of codec.\nSetting\nMOS\nCMOS (vs. PromptTTS 2)\nGT\n4.38 \u00b1 0.08\n-\nGT (Codec Reconstruction)\n4.30 \u00b1 0.07\n-\nPromptTTS (Guo et al., 2023)\n3.77 \u00b1 0.09\n-0.191\nInstructTTS (Yang et al., 2023)\n3.80 \u00b1 0.07\n-0.157\nPromptTTS 2\n3.88 \u00b1 0.08\n0.0\nTable 3: The average speech similarity of PromptTTS and PromptTTS 2 when synthesizing speech\nwith the same intention in text prompts but different text prompts, text contents, sampling results of\nTTS backbone and sampling results of variation network. The similarity score is in a range of [0, 1].\nModel\nText Prompt\nText Content\nTTS Backbone\nVariation Network\nPromptTTS\n0.766\n0.662\n0.799\n-\nInstructTTS\n0.773\n0.718\n0.796\n-\nPromptTTS 2\n0.775\n0.873\n0.914\n0.355\nsystems. The results presented in Table 1 illustrate the performance of all systems. The results\ndemonstrate that PromptTTS 2 can synthesize speech with higher accuracy across all attributes\ncompared to baseline systems, achieving an average improvement of 1.79%. Then we conduct mean-\nof-score (MOS) and comparative MOS (CMOS) test to evaluate the speech quality of PromptTTS 2\nand baseline systems, as shown in Table 2. The results of MOS and CMOS show that PromptTTS 2\nachieves higher speech quality than the baseline systems.\n5.2\nSTUDY OF VARIATION NETWORK\nIn this section, we examine the information of voice variability learned by variation network. Due\nto the one-to-many problem between the text prompt and the voice variability in speech, the model\nmight implicitly incorporate voice variability information into specific aspects. Consequently, the\nmodel could synthesize varying voices even when presented with identical text prompts (or text\nprompts with equivalent meanings). For the baseline systems, PromptTTS and InstructTTS, these\naspects include the text prompt (with the same meaning), text content, and TTS backbone (with\nlatent diffusion), as the voice of synthesized speech may differ depending on the text prompt, text\ncontent, and TTS backbone. In PromptTTS 2, an additional aspect, variation network, is introduced,\nas the voice of synthesized speech may also vary based on different sampling results of the variation\nnetwork.\nWe use WavLM-TDNN model (Chen et al., 2022a) to assess the similarity of two speech in a range of\n[0, 1], where the higher speech similarity, the less voice variability. For each aspect mentioned above,\nwe generate 5 speech and calculate the average similarity of the 5 speech. The results are shown in\nTable 3. From the table, we have the following observation: 1) baseline systems implicitly acquire\na small amount of voice variability information in the aspect of the text prompt, text content, and\nTTS backbone, which is undesired as we aim for style to be controlled exclusively by the intention in\ntext prompt; 2) the speech similarity of variation network in PromptTTS 2 is markedly lower than\nother aspects, showing that the variation network effectively models voice variability information\nnot encompassed by the text prompt (i.e., different sampling results leads to different timbre); 3)\nfor PromptTTS 2, the voice variability acquired in aspects apart from variation network is less than\nthose of baseline systems whose similarity are higher. This indicates that when the variation network\nsuccessfully captures voice variability, the model is inclined to learn less voice variability information\nin other aspects. We strongly encourage readers to listen to the samples on our demo page, which\noffer an intuitive comprehension of the voice variability information present in each dimension.\nBesides the WavLM-TDNN model, we evaluate the speech similarity by human experts. The\nconclusions of subjective test are similar with those of WavLM-TDNN model, shown in Appendix C.\n8\nPreprint. Working in Progress.\nTable 4: The accuracy (%) of intention classification on four attributes with text prompts from\nPromptSpeech, TextrolSpeech, and our prompt generation pipeline.\nTraining Set\nGender\nSpeed\nVolume\nPitch\nMean\nPromptSpeech (Guo et al., 2023)\n100.00\n96.85\n89.58\n84.51\n92.74\nTextrolSpeech (Ji et al., 2023)\n98.77\n94.18\n93.10\n92.80\n94.71\nOur Prompt Generation Pipeline\n99.08\n97.47\n94.48\n94.48\n96.38\nFigure 4: The PCA results of the representation extracted by the reference speech encoder in style\nmodule. Each point stands for a speech and the speech with the same speaker (left figure) or the same\nsame emotion (right figure) has the same color.\n5.3\nPROMPT GENERATION QUALITY\nWe analyze the quality of text prompts generated by our pipeline through whether the text prompts\ncan reflect the values of attributes. Specifically, we train a classifier to recognize the intention of text\nprompts on four attributes. The training data for the classifier is 1) text prompts authored by human\n(i.e., the training set of PromptSpeech (Guo et al., 2023)), 2) TextrolSpeech (Ji et al., 2023) whose text\nprompts are written by LLM (GPT-3.5-TURBO) with multi-stage prompt programming approach (but\nwithout the placeholder or sentence combination mechanism in our pipeline), 3) text prompts written\nby our pipeline. We display the average accuracy of classification on the test set of PromptSpeech\nin Table 4. The classifier trained on text prompts generated by our pipeline has a higher accuracy\ncompared to the classifier trained on text prompts authored by human or TextrolSpeech. This result\nindicates that the text prompts generated by our pipeline exhibit higher quality than previous works,\nverifying the effectiveness of our prompt generation pipeline. More ablation studies on our prompt\ngeneration pipeline can be found in Appendix D.\n5.4\nFURTHER ANALYSIS\nIn this section, we conduct further analysis on the reference representation extracted from reference\nspeech encoder in style module, which is a high-dimensional vector. To visualize the vector, we\nemployed Principal Component Analysis (PCA) to reduce the dimensionality of the vector and map it\nto a two-dimensional (2D) vector, which is plotted in Figure 4. Each point in figure stands for a speech\nand the speech with the same speaker or the same emotion (Zhou et al., 2021; 2022) has the same\ncolor. We observe that the speech samples belonging to the same speaker or the same emotion tend to\ncluster together in the figure. This observation suggests that the reference representations effectively\nlearn the voice variability uncovered by text prompts (such as speaker or emotion). Therefore, given\na text prompt, the variation network can sample different voice variability corresponding to the text\nprompt, which offers users more flexibility on generating voices.\n5.5\nEXTENSION ON FACE2VOICE\nPromptTTS 2 involves modeling voice information utilizing a sequence of predictable tokens, enabling\nits extension to many other scenarios involving predicting voices from other modalities. We conduct\na preliminary experiment on the Face2Voice extension, with a objective of predicting voices based on\nspeaker\u2019s facial images. More details about Face2Voice extension can be found in Appendix E, which\nshows that PromptTTS 2 generates voices corresponding more closely to the facial images compared\nwith the baseline method (Weng et al., 2023). Furthermore, our findings show that PromptTTS 2 is a\n9\nPreprint. Working in Progress.\ngeneral method for generating voices conditioned on text prompts, facial images, or other information.\nSamples of facial images and generated voices can also be found on our demo page.\n6\nCONCLUSION\nIn this study, we propose PromptTTS 2 to address the one-to-many and data-scale issues in text\nprompt based TTS systems, which implements a variation network to model the voice variability\ninformation not captured by text prompts and uses LLM for high-quality text prompt generation.\nThe variation network facilitates more detailed voice control by sampling from Gaussian noise. The\nLLM-based prompt generation pipeline eliminates the reliance on vendors and provides scalability\nfor easily incorporating new attributes. Experimental results indicate that the proposed variation\nnetwork assists the TTS model in synthesizing speech more closely corresponding to the text prompt\nand diverse in voice variability. Our pipeline generates text prompts with higher quality than human-\nauthored ones. For future work, we plan to extract additional attributes from large-scale speech data\nto increase the diversity of voice generation system and apply our method on more modalities for\nvoice generation.\nREFERENCES\nSercan \u00d6 Ar\u0131k, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo\nKang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-time neural\ntext-to-speech. In International conference on machine learning, pp. 195\u2013204. PMLR, 2017.\nSiddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai\nZhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al. Espnet-slu: Advancing spoken language\nunderstanding through espnet. In ICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 7167\u20137171. IEEE, 2022.\nJae-Sung Bae, Hanbin Bae, Young-Sun Joo, Junmo Lee, Gyeong-Hoon Lee, et al. Speaking speed\ncontrol of end-to-end speech synthesis using sentence-level conditioning. 2020.\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations, 2020.\nTaejun Bak, Jae-Sung Bae, Hanbin Bae, Young-Ik Kim, and Hoon-Young Cho. Fastpitchformant:\nSource-filter based decomposed modeling for speech synthesis. In Conference of the International\nSpeech Communication Association (Interspeech), 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020a.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Dhariwal, et al.\nLanguage models are few-shot learners. In Conference and Workshop on Neural Information\nProcessing Systems (NIPS), volume 33, pp. 1877\u20131901, 2020b.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nEdresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren G\u00f6lge, and\nMoacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for\neveryone. In International Conference on Machine Learning, pp. 2709\u20132720. PMLR, 2022.\nMingjian Chen, Xu Tan, Yi Ren, Jin Xu, Hao Sun, Sheng Zhao, Tao Qin, and Tie-Yan Liu. Multi-\nspeech: Multi-speaker text to speech with transformer. arXiv preprint arXiv:2006.04664, 2020.\nMingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao, and Tie-Yan Liu. Adaspeech:\nAdaptive text to speech for custom voice. ICLR, 2021.\n10\nPreprint. Working in Progress.\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\nKanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training\nfor full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):\n1505\u20131518, 2022a.\nZehua Chen, Yihan Wu, Yichong Leng, Jiawei Chen, Haohe Liu, Xu Tan, Yang Cui, Ke Wang, Lei\nHe, Sheng Zhao, et al. Resgrad: Residual denoising diffusion probabilistic models for text to\nspeech. arXiv preprint arXiv:2212.14518, 2022b.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nAndrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan\nRaiman, and Yanqi Zhou. Deep voice 2: Multi-speaker neural text-to-speech. Advances in neural\ninformation processing systems, 30, 2017.\nZhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu Tan. Prompttts: Controllable text-to-\nspeech with text descriptions. In ICASSP 2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.\nJ. Ho, C. Saharia, W. Chan, Fleet D. J, M. Norouzi, and T. Salimans. Cascaded diffusion models for\nhigh fidelity image generation. Journal of Machine Learning Research, 23(47):1\u201333, 2022.\nSung-Feng Huang, Chyi-Jiunn Lin, Da-Rong Liu, Yi-Chen Chen, and Hung-yi Lee. Meta-tts: Meta-\nlearning for few-shot speaker adaptive text-to-speech. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 30:1558\u20131571, 2022.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\nAchal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali\nFarhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it as\nbelow.\nShengpeng Ji, Jialong Zuo, Minghui Fang, Ziyue Jiang, Feiyang Chen, Xinyu Duan, Baoxing\nHuai, and Zhou Zhao. Textrolspeech: A text style control speech corpus with codec language\ntext-to-speech models. arXiv preprint arXiv:2308.14430, 2023.\nG. Kim, T. Kwon, and J. C. Ye. Diffusionclip: Text-guided diffusion models for robust image\nmanipulation. In CVPR, 2022.\nMatthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson,\nVimal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal\nspeech generation at scale. arXiv preprint arXiv:2306.15687, 2023.\nYichong Leng, Zehua Chen, Junliang Guo, Haohe Liu, Jiawei Chen, Xu Tan, Danilo Mandic, Lei\nHe, Xiangyang Li, Tao Qin, et al. Binauralgrad: A two-stage conditional diffusion probabilistic\nmodel for binaural audio synthesis. Advances in Neural Information Processing Systems, 35:\n23689\u201323700, 2022.\nYinghao Aaron Li, Cong Han, Vinay S Raghavan, Gavin Mischler, and Nima Mesgarani. Styletts\n2: Towards human-level text-to-speech through style diffusion and adversarial training with large\nspeech language models. arXiv preprint arXiv:2306.07691, 2023.\nGuanghou Liu, Yongmao Zhang, Yi Lei, Yunlin Chen, Rui Wang, Zhifei Li, and Lei Xie. Promptstyle:\nControllable style transfer for text-to-speech with natural language descriptions. arXiv preprint\narXiv:2305.19522, 2023a.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv\npreprint arXiv:2301.12503, 2023b.\nA. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021.\n11\nPreprint. Working in Progress.\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual\nreasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 32, 2018.\nV. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M.A. Kudinov. Grad-tts: A diffusion probabilistic\nmodel for text-to-speech. In ICML, 2021a.\nVadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A\ndiffusion probabilistic model for text-to-speech. In International Conference on Machine Learning,\npp. 8599\u20138608. PMLR, 2021b.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A\nlarge-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020.\nKaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-Johnson. Autovc:\nZero-shot voice style transfer with only autoencoder loss. In International Conference on Machine\nLearning, pp. 5210\u20135219. PMLR, 2019.\nKaizhi Qian, Yang Zhang, Shiyu Chang, Mark Hasegawa-Johnson, and David Cox. Unsupervised\nspeech decomposition via triple information bottleneck. In International Conference on Machine\nLearning, pp. 7836\u20137846. PMLR, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In ICML, 2021.\nA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech:\nFast, robust and controllable text to speech. Advances in neural information processing systems,\n32, 2019.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski,\nSrivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. LAION-5b: An open large-scale dataset for training next generation image-text models.\nIn Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack, 2022.\nKai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang\nBian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing\nsynthesizers. arXiv preprint arXiv:2304.09116, 2023.\nReo Shimizu, Ryuichi Yamamoto, Masaya Kawamura, Yuma Shirahata, Hironori Doi, Tatsuya\nKomatsu, and Kentaro Tachibana. Prompttts++: Controlling speaker identity in prompt-based\ntext-to-speech using natural language descriptions, 2023.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\nXu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv preprint\narXiv:2106.15561, 2021.\nXu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng,\nYuanhao Yi, Lei He, et al. Naturalspeech: End-to-end text to speech synthesis with human-level\nquality. arXiv preprint arXiv:2205.04421, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n12\nPreprint. Working in Progress.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech\nsynthesizers. arXiv preprint arXiv:2301.02111, 2023.\nYuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng\nYang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech\nsynthesis. arXiv preprint arXiv:1703.10135, 2017.\nYuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg, et al. Style tokens:\nUnsupervised style modeling, control and transfer in end-to-end speech synthesis. In International\nConference on Machine Learning (ICML), 2018.\nShao-En Weng, Hong-Han Shuai, and Wen-Huang Cheng. Zero-shot face-based voice conversion:\nbottleneck-free speech disentanglement in the real-world scenario. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 37, pp. 13718\u201313726, 2023.\nYihan Wu, Xu Tan, Bohan Li, Lei He, Sheng Zhao, Ruihua Song, Tao Qin, and Tie-Yan Liu.\nAdaspeech 4: Adaptive text to speech in zero-shot scenarios. arXiv preprint arXiv:2204.00436,\n2022.\nYuzi Yan, Xu Tan, Bohan Li, Tao Qin, Sheng Zhao, Yuan Shen, and Tie-Yan Liu. Adaspeech\n2: Adaptive text to speech with untranscribed data. In ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6613\u20136617. IEEE, 2021.\nDongchao Yang, Songxiang Liu, Rongjie Huang, Guangzhi Lei, Chao Weng, Helen Meng, and Dong\nYu. Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt.\narXiv preprint arXiv:2301.13662, 2023.\nSiCheng Yang, Methawee Tantrawenith, Haolin Zhuang, Zhiyong Wu, Aolan Sun, Jianzong Wang,\nNing Cheng, Huaizhen Tang, Xintao Zhao, Jie Wang, et al. Speech representation disentanglement\nwith adversarial mutual information learning for one-shot voice conversion.\narXiv preprint\narXiv:2208.08757, 2022.\nNeil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-\nstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 30:495\u2013507, 2021.\nZhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face\ngeneration with a high-resolution audio-visual dataset. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 3661\u20133670, 2021.\nKun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. Seen and unseen emotional style transfer for\nvoice conversion with a new emotional speech dataset. In ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 920\u2013924. IEEE, 2021.\nKun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. Emotional voice conversion: Theory, databases\nand esd. Speech Communication, 137:1\u201318, 2022.\n13\nPreprint. Working in Progress.\nA\nEXAMPLE OF PROMPT GENERATION PIPELINE\nTable 5 shows a detailed example of our prompt generation pipeline including the querys to instruct\nLLM. In Stage 1, we ask LLM to generate keywords for \u201cmale gender\u201d. In Stage 2, we ask LLM\nto write sentences describing \u201cpitch\u201d attribute, using \u201c[placeholder]\u201d to stand for the corresponding\nattribute (i.e., \u201cpitch\u201d). In Stage 3, we ask LLM to combine two sentences describing \u201cgender\u201d\nattribute and \u201cpitch\u201d attribute into one sentence. In Stage 4, we construct a text prompt by first\nsampling a sentence and subsequently sampling keywords to replace the placeholders within the\nsentence.\nTable 5: The example of a text prompt generated by LLM with the pipeline.\nStage\nDialogue\nStage 1\nQuery: Can you list 5 words or phrases that is the synonyms for male gender in lower\ncase? Please generate one word per line and identify it with a serial number.\nResponse:\n1. man.\n2. boy.\n3. guy.\n4. dude.\n5. gentleman.\nStage 2\nQuery: Please generate 5 templates to to ask for generating a voice. These templates\ncan only describe the pitch of the voice and use [placeholder] to indicate where a word\nto describe pitch would be inserted. This template should be simple enough to only\nhave a few words.\nResponse:\n1. I need a [placeholder] voice.\n2. Looking for a [placeholder] voice.\n3. In search of a [placeholder] voice.\n4. Seeking a voice with [placeholder] pitch.\n5. Require a voice, [placeholder] pitch.\nStage 3\nQuery: I have some sentences, can you combine these sentences into one phrase and\nkeep all of the \u2019[]\u2019 to describe the style of speech with the same meaning? You can\ngenerate 5 different phrases and callout number. Please reduce to phrases. The sentences\nare: Please generate a voice that sounds like a [Gender]. Seeking a voice with [Pitch]\npitch.\nResponse:\n1. Generate a voice that sounds like a [Gender] with [Pitch] pitch.\n2. Looking for a voice that embodies a [Gender] with a [Pitch] pitch.\n3. Require a voice with a [Pitch] pitch that represents a [Gender].\n4. Requesting a [Gender] voice at [Pitch] pitch.\n5. Desiring a [Gender] voice with a [Pitch] pitch.\nStage 4\nGenerate a voice that sounds like a boy with low pitch.\nB\nDETAILS ON THE TTS BACKBONE\nThe TTS backbone of PromptTTS 2 is adopted from a state-of-the-art large-scale TTS system,\nNaturalSpeech 2 (Shen et al., 2023), which consists of 1) a neural audio codec that transforms the\naudio waveform into latent vectors and reconstructs the latent representation into the waveform,\nand 2) a latent diffusion model with a prior (a duration/pitch predictor and a phoneme encoder).\nIn detail, we first encode the audio waveform into a latent representation using the residual vector-\nquantizer (RVQ) (Zeghidour et al., 2021). Then, the latent diffusion denoise (synthesize) latent speech\nrepresentations from Gaussian noise. The denoised latent representation is subsequently converted\nback to the waveform by the decoder of the neural audio codec.\nC\nSUBJECTIVE TEST ON THE VOICE VARIABILITY IN VARIATION NETWORK\nBesides the metric by WavLM-TDNN model, we also evaluate the speech similarity from the\nperspective of human. For each aspect mentioned in Section 5.2, we generate 5 speech and calculate\nthe average similarity of the 5 speech. In human subjective test, the judges are asked to judge whether\nthe two synthesized speech are in the same style. The speech similarity of each aspect is defined as\n14\nPreprint. Working in Progress.\nTable 6: In human subjective test, the average speech similarity (%) of baseline systems and\nPromptTTS 2 when synthesizing speech with the same intention in text prompts but different text\nprompts, text contents, sampling results of TTS backbone and sampling results of variation network.\nModel\nText Prompt\nText Content\nTTS Backbone\nVariation Network\nPromptTTS\n94.44\n79.63\n96.30\n-\nInstructTTS\n92.59\n85.18\n94.44\n-\nPromptTTS 2\n90.74\n98.00\n98.15\n7.41\nTable 7: The accuracy (%) of intention classification on four attributes in the ablation of our prompt\ngeneration pipeline.\nDatasets\nGender\nSpeed\nVolume\nPitch\nMean\nOur Prompt Generation Pipeline\n99.08\n97.47\n94.48\n94.48\n96.38\n- Placeholder\n99.08\n97.31\n89.27\n90.50\n94.04\n- Phrase\n99.08\n97.01\n95.55\n92.72\n96.09\n- Sentence\n99.08\n97.47\n93.18\n94.94\n96.16\nthe ratio of speech pair (among the 5 speech) that is regarded as in the same style by judges. The\nconclusions of subjective test (Table 6) are similar with those of WavLM-TDNN model discussed in\nSection 5.2.\nD\nABLATION STUDY ON PROMPT GENERATION PIPELINE\nWe conduct ablation studies on the prompt generation pipeline. First, we remove the design of the\nplaceholder from the pipeline. In this case, LLM is required to directly write text prompts for each\nclass in attributes, after which sentence combination is performed. The results are presented as\n\u201c- Placeholder\u201d in Table 7. The drop in classification accuracy demonstrates that the placeholder\nis beneficial for the prompt generation pipeline. Without it, LLM might miss attributes or even\nalter them during sentence combination, resulting in low-quality text prompts. In addition to the\nplaceholder, we also conduct ablation studies on instructing LLM to write only phrases or sentences\nby removing sentences (\u201c- Sentence\u201d) or phrases (\u201c- Phrase\u201d). The results indicate that variations in\nformat can marginally improve the robustness of the prompt generation pipeline.\nE\nEXTENSION ON FACE2VOICE\nPromptTTS 2 involves modeling voice information utilizing a sequence of predictable tokens, enabling\nits extension to many other scenarios involving predicting voice from other modalities.\nWe conduct a preliminary experiment on the Face2Voice extension, with a objective of predicting\nvoice based on the facial image of speaker. In this experiment, the facial image is processed using an\nimage encoder7 pretrained in CLIP (Schuhmann et al., 2022; Radford et al., 2021; Ilharco et al., 2021)\nto extract image representations. Simultaneously, the speech is processed using a reference speech\nencoder depicted in Figure 1b to extract reference representations. Subsequently, a variational network\n(illustrated in Figure 1c) is trained to predict reference representations from image representations.\nFor this preliminary experiment, we utilize the HDTF dataset (Zhang et al., 2021), a high-resolution\ndataset designed for talking face generation. The dataset includes more than 300 distinct speakers\nand encompasses 15.8 hours of video. To extract paired data of facial images and speech, we first\nselect an image (video frame) and then extracted a speech segment with a duration of 5-10 seconds\nsurrounding the chosen frame. We designate 18 speakers for testing and use the remaining speakers\nfor training.\n7https://github.com/mlfoundations/open_clip\n15\nPreprint. Working in Progress.\nTable 8: The MOS results (%) on whether the voice is in the same style with the facial image or not.\nGT stands for judging whether the ground-truth voice is in the same style with the corresponding\nfacial image.\nSetting\nSame\nIn-between\nDifferent\nGT\n46.47\n42.05\n11.47\nSP-FaceVC (Weng et al., 2023)\n20.17\n45.38\n34.45\nPromptTTS 2\n31.78\n41.17\n27.05\nTable 9: The CMOS results (%) on which voice (synthesized by PromptTTS 2 or SP-FaceVC)\ncorresponds more closely with the facial image.\nSetting\nFormer\nTie\nLatter\nPromptTTS 2 vs. SP-FaceVC (Weng et al., 2023)\n51.47\n29.41\n19.12\nWe compare our method with a SOTA method on Face2Voice, SP-FaceVC (Weng et al., 2023)8, with\nsubjective test (MOS). In the MOS test, the judges are asked to judge whether a facial image and\nthe voice is in the same style (i.e., it is natural for the facial image to have that voice), whose results\nare shown in Figure 8. The results demonstrate that compared with SP-FaceVC, PromptTTS 2 can\ngenerate voice corresponding more closely with the facial image (31.78% versus 20.17%) and fewer\nunsuitable cases (27.05% versus 34.45%).\nWe also conduct comparative MOS (CMOS) test to directly judge that given a facial image, which\nvoice (synthesized by PromptTTS 2 or SP-FaceVC) corresponds more closely with the facial image.\nThe results in Table 9 show that in 80.88% cases, PromptTTS 2 synthesizes a better or comparable\nvoice than SP-FaceVC. Furthermore, our findings demonstrate that PromptTTS 2 is a general method\nfor generating voices conditioned on text prompts, facial images, or other types of information.\nSamples of facial images and generated voices can also be found on our demo page.\n8https://github.com/anitaweng/SP-FaceVC\n16\n"
  },
  {
    "title": "Hierarchical Masked 3D Diffusion Model for Video Outpainting",
    "link": "https://arxiv.org/pdf/2309.02119.pdf",
    "upvote": "10",
    "text": "Hierarchical Masked 3D Diffusion Model for Video Outpainting\nFanda Fan\u2217\nfanfanda@ict.ac.cn\nInstitute of Computing Technology,\nChinese Academy of Sciences\nBeijing, China\nUniversity of Chinese Academy of\nSciences\nBeijing, China\nChaoxu Guo\u2217\nchaoxu.guo@nlpr.ia.ac.cn\nAlibaba Group\nBeijing, China\nLitong Gong\ngonglitong.glt@alibaba-inc.com\nAlibaba Group\nBeijing, China\nBiao Wang\neric.wb@alibaba-inc.com\nAlibaba Group\nBeijing, China\nTiezheng Ge\ntiezheng.gtz@alibaba-inc.com\nAlibaba Group\nBeijing, China\nYuning Jiang\nmengzhu.jyn@alibaba-inc.com\nAlibaba Group\nBeijing, China\nChunjie Luo\u2020\nluochunjie@ict.ac.cn\nInstitute of Computing Technology,\nChinese Academy of Sciences\nBeijing, China\nJianfeng Zhan\nzhanjianfeng@ict.ac.cn\nInstitute of Computing Technology,\nChinese Academy of Sciences\nBeijing, China\nUniversity of Chinese Academy of\nSciences\nBeijing, China\nABSTRACT\nVideo outpainting aims to adequately complete missing areas at the\nedges of video frames. Compared to image outpainting, it presents\nan additional challenge as the model should maintain the temporal\nconsistency of the filled area. In this paper, we introduce a masked\n3D diffusion model for video outpainting. We use the technique of\nmask modeling to train the 3D diffusion model. This allows us to\nuse multiple guide frames to connect the results of multiple video\nclip inferences, thus ensuring temporal consistency and reducing\njitter between adjacent frames. Meanwhile, we extract the global\nframes of the video as prompts and guide the model to obtain infor-\nmation other than the current video clip using cross-attention. We\nalso introduce a hybrid coarse-to-fine inference pipeline to allevi-\nate the artifact accumulation problem. The existing coarse-to-fine\npipeline only uses the infilling strategy, which brings degradation\nbecause the time interval of the sparse frames is too large. Our\npipeline benefits from bidirectional learning of the mask modeling\nand thus can employ a hybrid strategy of infilling and interpolation\nwhen generating sparse frames. Experiments show that our method\n\u2217Both authors contributed equally to this research while interning at Alibaba Group.\n\u2020Corresponding author.\nThis work is licensed under a Creative Commons Attribution\nInternational 4.0 License.\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0108-5/23/10.\nhttps://doi.org/10.1145/3581783.3612478\nachieves state-of-the-art results in video outpainting tasks. More\nresults and codes are provided at our project page.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Computer vision problems.\nKEYWORDS\nvideo outpainting, diffusion model, mask modeling, coarse-to-fine\nACM Reference Format:\nFanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning\nJiang, Chunjie Luo, and Jianfeng Zhan. 2023. Hierarchical Masked 3D Dif-\nfusion Model for Video Outpainting. In Proceedings of the 31st ACM In-\nternational Conference on Multimedia (MM \u201923), October 29-November 3,\n2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 13 pages. https:\n//doi.org/10.1145/3581783.3612478\n1\nINTRODUCTION\nThe task of video outpainting is to expand edge areas of videos\naccording to the provided contextual information (the middle part\nof the videos). In recent years, image outpainting [4, 5, 22, 28, 30,\n38, 42] has been heavily researched and has yielded very promising\nresults with the advent of GAN(Generative Adversarial Network)\nand Diffusion Model. However, video outpainting is currently far\nfrom achieving ideal results. Different from image outpainting,\nwhich only considers the spatial appearance of a single image, video\noutpainting requires the modeling of motion information to ensure\ntemporal consistency among video frames. Besides, videos in real\nscenarios are typically longer than 5 seconds. It poses two extra\nchallenges: 1) a video would be divided into multiple clips due to the\narXiv:2309.02119v3  [cs.CV]  19 Jan 2024\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nFanda Fan et al.\n\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c = 0.6\n\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c = 0.5\nShort Videos\nLong Videos\nFigure 1: We propose a Masked 3D Diffusion Model (M3DDM) and a coarse-to-fine inference pipeline for video outpainting.\nOur method can not only generate high temporal consistency and reasonable outpainting results but also alleviate the problem\nof artifact accumulation in long video outpainting. The top row shows the first and last frames of five video clips. Each row\nbelow shows the video outpainting results of our method.\nlong duration and memory constraints of GPUs. It is challenging\nto ensure the temporal consistency of generated content among\ndifferent clips of the same video. and 2) long video outpainting\nsuffers from artifact accumulation issues and meanwhile requires a\nlarge amount of computation resources.\nA few studies have investigated video outpainting. Dehan [6]\nformed a background estimation using video object segmentation\nand video inpainting methods, and temporal consistency is en-\nsured by introducing optical flow [10, 34]. However, they often\nproduce poor results in scenarios with complex camera motion\nand when foreground objects leave the frame. MAGVIT [44] pro-\nposed a generic mask-based video generation model that can also\nbe used for video outpainting tasks. They introduced a 3D-Vector-\nQuantized (3DVQ) tokenizer to quantize a video and design a trans-\nformer for multi-task conditional masked token modeling. Such a\nmethod is able to generate a reasonable short video clip, but the\ncomplete result, consisting of multiple clips for a long video, would\nbecome poor. The reason is that it lacks the ability to achieve high\ntemporal consistency in the complete video and suffers from artifact\naccumulation in multiple clip inferences.\nIn this work, we focus on video outpainting tasks. To address the\nissues above, we propose a masked 3D diffusion model (M3DDM)\nand a hybrid coarse-to-fine inference pipeline. Recently, the dif-\nfusion model [8, 19, 26] has achieved impressive results in image\nsynthesis [14, 28, 30] and video generation [2, 18, 31]. Our video out-\npainting method is based on the latent diffusion models (LDMs) [29].\nThere are two benefits to choosing LDMs here: 1) They encode the\nvideo frames in the latent space instead of the pixel space, thus re-\nquiring less memory and achieving better efficiency. 2) Pre-trained\nLDMs provides good prior about the natural image content and\nstructure that can help our model quickly converges in video out-\npainting task.\nTo ensure high temporal consistency in a single clip and across\ndifferent clips of the same video, we employ two techniques: 1)\nMasked guide frames, which help to generate current clips that are\nmore semantically coherent and have less jitter with neighboring\nclips. Mask modeling has proven to be effective in image [4] and\nvideo generation [4, 15]. During the training phase, we randomly\nreplace the contextual information with raw frames, which have\nedge areas and act as guide frames. In this way, the model can pre-\ndict the edge areas not only based on contextual information but\nalso based on adjacent guide frames. The adjacent guide frames can\nhelp to generate more coherent and less jittery results. During the\ninference phase, we iteratively and sparsely outpaint the frames,\nwhich allows us to use previously generated frames as guide frames.\nThere are two benefits to using the mask modeling approach. On\nthe one hand, the bidirectional learning mode of mask modeling\nallows the model to perceive contextual information better, result-\ning in better single-clip inference. On the other hand, it enables\nus to use a hybrid coarse-to-fine inference pipeline. The hybrid\npipeline not only uses the infilling strategy with the first and last\nframes as the guide frames but also uses the interpolation strategy\nHierarchical Masked 3D Diffusion Model for Video Outpainting\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\n0\n60\n120\n180\n240\nDense\nCTF\nFigure 2: Artifact accumulation problem in long video out-\npainting. We compare two inference methods by our M3DDM:\ndense and coarse-to-fine (CTF) inferences. The index of the\nvideo frame is labeled above the image. This case shows hor-\nizontal video outpainting with a mask ratio of 0.5. We mark\nthe area to be extended with a red line in the first image.\nwith multiple intermediate frames as the guide frames. 2) Global\nvideo clips as prompts, which uniformly extracts \ud835\udc54 global frames\nfrom the complete video, encodes them into a feature map using\na lightweight encoder, and then interacts with the context of the\ncurrent video clip (the middle part of the video clip) through cross-\nattention. This technique enables the model to obtain some global\nvideo information when generating the current clip. It is worth not-\ning that the global frames of the video we input do not include the\nedge areas to be filled in order to avoid leakage. Our experiments\nshow that in scenes with complex camera motion and foreground\nobjects moving back and forth, our method can generate a more\ntemporally consistent complete video. Some results generated by\nour method can be seen in Fig. 1.\nOur hybrid coarse-to-fine inference pipeline can alleviate the\nartifact accumulation problem in long video outpainting. Due to\nthe iterative generation using the guide frames at the inference\nphase, a bad case generated in the previous step would pollute the\nsubsequent generation results (This is shown in Fig. 2. We will\ndetail later). For the task of long video generation, the coarse-to-\nfine inference pipeline [17, 43] has been proposed recently. In the\ncoarse phase, the pipeline first sparsely generates the keyframes\nof the video. After that, it generates each frame densely according\nto the keyframes. Compared to generating the video in a dense\nmanner directly, the coarse stage requires fewer iterations (because\nof sparse), thereby alleviating the problem of artifact accumulation\nin long videos. The existing coarse-to-fine inference pipeline [17,\n43] used a three-level hierarchical structure. However, it used only\nthe infilling strategy with the first and last frames to guide the\nvideo generation from coarse to fine. This strategy results in a\nlarge time interval between key frames generated in the coarsest\nstage (the first level), thus bringing degradation in the generated\nresults (This is shown in Fig. 6a.). We also use the coarse-to-fine\ninference pipeline for video outpainting. Thanks to the masking\nstrategy during the training phase, we can hybridize the infilling\nstrategy and the interpolation strategy together. That means we can\nnot only use the first and last frames as guides for the three-level\ncoarse-to-fine structure but also use multiple frames interpolation\nto generate the video. Experiments show that our hybrid coarse-to-\nfine inference pipeline brings lower artifacts and better results in\nlong video generation.\nOur main contributions are as follows:\n\u2022 To the best of our knowledge, we are the first to use a masked\n3D diffusion model for video outpainting and achieve state-\nof-the-art results.\n\u2022 We propose a bidirectional learning method with mask mod-\neling to train our 3D diffusion model. Additionally, we show\nthat using guide frames to connect different clips of the same\nvideo can effectively generate video outpainting results with\nhigh temporal consistency and low jitter.\n\u2022 We extract global temporal and spatial information as prompt\nfrom global frames of the video and feed it into the network\nin the form of cross-attention, which guides the model to\ngenerate more reasonable results.\n\u2022 We propose a hybrid coarse-to-fine generation pipeline that\ncombines infilling and interpolation when generating sparse\nframes. Experiments show that our pipeline can reduce arti-\nfact accumulation in long video outpainting while maintain-\ning a good level of temporal consistency.\n2\nRELATED WORK\nThis section introduces the related diffusion model, mask modeling,\nand the Coarse-to-Fine pipeline.\nDiffusion Model. The diffusion model [19, 26, 32] has recently\nbecome the best technology in image generation [28, 30], espe-\ncially in video generation [18, 25, 31]. Compared with GAN [12], it\ncan generate samples with richer diversity and higher quality [8].\nConsidering the significant achievements of the diffusion model\nin video generation, we adopt it as the main body of our video\noutpainting method. LDMs [29] are diffusion models in the latent\nspace, which reduce the GPU memory usage, and their open-source\nparameters are excellent image priors for our video outpainting\ntask.\nMask Modeling. Mask modeling was first proposed in the\nBERT [7] in the field of NLP for language representation learning.\nBERT randomly masks tokens in sentences and performs bidirec-\ntional learning by predicting the masked tokens based on context.\nMAE [16] has demonstrated that mask modeling can be effectively\nused in unsupervised image representation learning in the field of\ncomputer vision. This is achieved by masking patch tokens in the\nimage and predicting the original patch tokens based on context.\nRecently, Mask modeling has also been used in the field of video\ngeneration [15]. In more recent times, the combination of mask\nmodeling and diffusion model has been applied to image [14, 40]\nand video generation [37] tasks. In this paper, we do not apply\nmasks on images or entire frames of videos, but rather, in consider-\nation of the feature of video outpainting, masks are applied to the\nsurrounding areas of the video that need to be filled with a proba-\nbility. Our experiments show that for video outpainting tasks, the\nemployment of the diffusion model technique with mask modeling\ncan generate higher-quality results.\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nFanda Fan et al.\nRaw Video Clip\nRandom Masked\nVideo Clip\nMasked Video Latents\nGlobal Video Clip\nGiven the Frame\nwith \ud835\udc5d prob\nSelf-Attention\nQ\nKV\nQ\nKV\nTemporal Attention\nAdd noise\nQ\nKV\nQ\nKV\nQ\nKV\nQ\nKV\nDenoising 3D U-Net\nCross Attention\nBinary Mask\nLightweight\nEncoder\nConditional Input\nTimestep\nFPS Rate\nTrainable Network\nMSE Loss\n<latexit sha1_base64=\"zLTxTXJk8CiRuTOmb9JXwnMK/\n0=\">ACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVkUxWUL9gG1SJO69C8yEyEUvQH3Oq3iX+gf+GdcQpqEZ2Q5My5\n95yZe6+fhlxIx3ktWHPzC4tLxeXSyura+kZ5c6slkjwLWDNIwiTr+J5gIY9ZU3IZsk6aMS/yQ9b2R2cq3r5jmeBJfCXHK\netF3jDmAx54kqjG+U254lQdvexZ4BpQgVn1pPyCa/SRIECOCAwxJOEQHgQ9XbhwkBLXw4S4jBDXcYZ7lEibUxajDI/YEX\n2HtOsaNqa98hRaHdApIb0ZKW3skSahvIywOs3W8Vw7K/Y374n2VHcb0983XhGxErfE/qWbZv5Xp2qRGOBE18CplQzqr\nAuOS6K+rm9peqJDmkxCncp3hGONDKaZ9trRG6dtVbT8fdKZi1T4wuTne1S1pwO7Pc6C1kHVPaq6jcNK7dSMuogd7GKf\n5nmMGi5R1N7P+IJz9aFVrCyj9TrYLRbOPbsh4+APlj1A=</latexit>E\n<latexit sha1_base64=\"ieMsJSDbPWmC1dP9sqLXmL+zU=\">ACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl02VF+8\nBaSjKd1qFpEpKJUorgD7jVTxP/QP/CO+MU1CI6IcmZc+85M/dePw5EKh3nNWfNzS8sLuWXCyura+sbxc2tRhplCeN1FgVR0vK9lAci5HUpZMBbcK9kR/wpj8U/HmLU9SEYWXchzsgbhKIvmCeJurjqym6x5JQdvexZ4BpQglm1qPiCa/QgSHDCBwhJOEA\nHlJ62nDhICaugwlxCSGh4xz3KJA2oyxOGR6xQ/oOaNc2bEh75ZlqNaNTAnoTUtrYI01EeQlhdZqt45l2Vuxv3hPtqe42pr9vEbEStwQ+5dumvlfnapFo8TXYOgmLNqOqYcl0V9TN7S9VSXKIiVO4R/GEMNPKaZ9trUl17aq3no6/6UzFqj0zuRne1S1pwO\n7Pc6CxkHZPSofnh+WKqdm1HnsYBf7NM9jVFBFDXyHuART3i2qlZoZdbdZ6qVM5ptfFvWwdonZBP</latexit>Zt\nQ\nKV\nQ\nKV\nQ\nKV\nFigure 3: Masked 3D Diffusion Model Framework. During training, we concatenate corrupted raw video latents, random masked\nvideo latent, and masks before feeding them into the 3D UNet network. The network predicts the noise in the corrupted raw\nlatents, allowing us to calculate the MSE loss with the added noise. Additionally, we uniformly select \ud835\udc54 global frames from\nthe video as a prompt and feed them into a trainable video encoder. Then the global frames feature map is placed in the\ncross-attention module of the 3D UNet.\nCoarse-to-Fine Pipeline. In the generation of long videos,\nmodels often suffer from artifact accumulation due to the auto-\nregressive strategy. For the method of generating videos with guid-\nance frames, artifacts from the previous video clips often affect the\nlater iterations. Recent research [2, 17, 43] adopt a coarse-to-fine\ngeneration pipeline for video generation. They first generate sparse\nkey frames of the video and alleviate the artifact problem by re-\nducing the number of iterations. In our video outpainting task, we\nadopt the coarse-to-fine inference pipeline and use both infilling\nstrategies with two guidance frames and interpolation strategies\nwith multiple guidance frames to help alleviate the problem of\nartifact accumulation in long videos.\n3\nMETHODOLOGY\n3.1\nPreliminaries\nDiffusion models [8, 19, 26, 32] are probabilistic models that learn\nthe data distribution \ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e by first forward adding noise to the orig-\ninal distribution, and then gradually denoising the normal distribu-\ntion variables to recover the original distribution. In the forward\nnoising process, a sample \ud835\udc650 can corrupted from \ud835\udc61 = 0 to \ud835\udc61 = \ud835\udc47\nusing the following transition kernel:\n\ud835\udc5e\ud835\udc61 (\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61\u22121) = N (\ud835\udc65\ud835\udc61;\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc65\ud835\udc61\u22121, \ud835\udefd\ud835\udc61\ud835\udc3c).\n(1)\nAnd \ud835\udc65\ud835\udc61 can be directly sampled from \ud835\udc650 using the following accu-\nmulation kernel:\n\ud835\udc65\ud835\udc61 =\n\u221a\ufe01\ne\ud835\udefc\ud835\udc61\ud835\udc650 +\n\u221a\ufe01\n1 \u2212 e\ud835\udefc\ud835\udc61\ud835\udf16,\n(2)\nwhere e\ud835\udefc\ud835\udc61 = \u00ce\ud835\udc61\n\ud835\udc60=1(1 \u2212 \ud835\udefd\ud835\udc60), and \ud835\udf16 \u223c N (0, 1). In the process of\ndenoising, a deep model is typically trained to predict the noise in\na corrupted signal \ud835\udc65\ud835\udc61. The loss function of the model can be simply\nwritten as\n\ud835\udc3f\ud835\udc37\ud835\udc40 = E\ud835\udc65,\ud835\udf16\u223cN(0,1),\ud835\udc61 [\u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc65\ud835\udc61,\ud835\udc50,\ud835\udc61)\u22252\n2],\n(3)\nwhere \ud835\udc50 is the conditional input and \ud835\udc61 is uniformly sample from\n{1, . . . ,\ud835\udc47 }.\nLDMs [29] additionally trained an encoder \ud835\udc38 to map the original\n\ud835\udc650 from the pixel space to the latent space, greatly reducing memory\nusage and making the model more efficient with an acceptable loss.\nThen, the decoder D is used to map \ud835\udc670 back to the pixel space.\nConsidering that video outpainting task requires large memory,\nwe choose the LDMs framework as our pipeline. Additionally, the\npre-training parameters of LDMs can serve as a good image prior,\nwhich helps our model converge faster. In equation 3, we rewrite \ud835\udc65\nas \ud835\udc67.\n3.2\nMasked 3D Diffusion Model\nWith the help of LDMs, a naive approach is to concatenate the noisy\nlatent of raw video clip with the context of the video clip as a condi-\ntional input and train a model to predict the added noise. Thus, the\nmodel can recover the raw video clip (the original video) from the\nrandomly sampled Gaussian noise distribution. Since videos usually\ncontain hundreds of frames, the model is required to perform infer-\nence on different clips of the same video separately, and then the\ngenerated clips are stitched together to form the final outpainting\nHierarchical Masked 3D Diffusion Model for Video Outpainting\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nresult of the complete video. Under this circumstance, the naive\napproach above cannot guarantee the temporal consistency of the\npredicted video clips.\nTo address it, we propose the masked 3D diffusion model, whose\noverview is shown in Fig. 3. Our model can generate F frames at\nonce. We describe our network architecture in Appendix C.1. We\nsample video frames with different frames per second (fps) and\nadditionally feed the fps into 3D UNet. This allows us to use one\nunifying model to adapt to videos with different frame rates. Our\nframework follows LDMs and first maps video frames in the pixel\nspace to the latent space through a pre-trained encoder \ud835\udc38. At the\ntraining stage, each context frame is replaced with raw video frames\nwith a probability \ud835\udc5d\ud835\udc53 \ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc52 before they are fed into the encoder \ud835\udc38.\nTherefore, our model has the ability to use guide frames at the\ninference stage, and more than two frames can be conditioned to\nfacilitate the generation of other frames. This modification has\ntwo benefits. First, it enables our coarse-to-fine inference pipeline,\nensuring consistent inference time across multiple passes. Second,\ncompared to solely using the first or the last raw frames as input\nconditions, bidirectional learning can help the model better perceive\ncontextual information, thereby improving generation quality. We\nwould validate this point in our ablation study.\n3.2.1\nMask Strategy. In order to construct the training samples\nfor video outpainting, we randomly mask out the edges of each\nframe. We mask a frame with different direction strategies: four-\ndirection, single-direction, bi-direction (left-right or top-down),\nrandom in any of four directions, and mask all. Taking into account\nthe practical application scenarios, we adopt the proportions of\nthese five strategies as 0.2, 0.1, 0.35, 0.1, and 0.25, respectively. The\n\"mask all\" strategy enables the model to perform unconditional\ngeneration, which allows us to adopt the classifier-free guidance\n[20] technique during the inference phase. Considering the size of\nthe edge area that needs to be outpainted in practical application\nscenarios, we randomly sample the mask ratio of a frame from\n[0.15, 0.75] uniformly.\nIn order to generate masked guide frames, we replace the con-\ntextual frame with the raw frame in three cases: 1) All F frames are\ngiven only context information, where each frame is masked with\nthe above masking strategy. 2) The first frame or the first and last\nframes of F frames are replaced with the unmasked raw frame, and\nthe rest of the frames are given only context information. 3) Any\nframe is replaced with an unmasked raw frame with probability\n\ud835\udc5d\ud835\udc53 \ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc52 = 0.5. The guide frames allow the model to predict the edge\nareas not only based on contextual information but also based on\nthe adjacent guide frames. The adjacent guide frames can help to\ngenerate more coherent and less jittery results. We evenly distrib-\nute the training proportions of the three cases. The proportions\nof these three cases are 0.3, 0.35, and 0.35, respectively. We do not\nonly train using case 3 because we considered that the first two\ncases would be used more frequently during the prediction phase.\n3.2.2\nGlobal Video Clip as a Prompt. In order to enable the model\nto perceive global video information beyond the current clip, we\nuniformly sample \ud835\udc54 frames from the video. These global frames\nare passed through a learnable lightweight encoder to obtain the\nfeature map, which is then fed into 3D-UNet via cross-attention.\nWe do not feed the global frames in the input layer of 3D-UNet\n...\n0\n30\n60\n450\n0 *\n15\n30 *\n225\n0 *\n1\n2\n15 *\n...\n...\nCoarse\nFine\nUncomdition\nInterplotation\nInfilling\nFigure 4: Coarse-to-Fine Pipeline. Our model can generate\n16 frames at a time. We label the index above each frame,\nand those with \u2217 indicate that the result has already been\ngenerated in the previous step and used as a conditional input\nfor the model in the current step. Our pipeline includes a\nhybrid strategy of infilling and interpolation.\nbecause we suggest that cross-attention can help masked frames\ninteract with global frames more thoroughly. It is worth noting that\nthe global frames passed in here are aligned with the context of the\ncurrent video clip and are also masked in the same way as other\nframes to avoid information leakage.\n3.2.3\nClassifier-free Guidance. Classifier-free guidance [20] has\nbeen proven to be effective in diffusion models. Classifier-free guid-\nance improves the results of conditional generation, where the\nimplicit classifier \ud835\udc5d\ud835\udf03 (\ud835\udc50|\ud835\udc67\ud835\udc61) assigns high probability to the condi-\ntioning \ud835\udc50. In our case, we have two conditional inputs. One is the\ncontext information of the video \ud835\udc501, and the other is the global\nvideo clip \ud835\udc502. We jointly train the unconditional and conditional\nmodels by randomly setting \ud835\udc501 and \ud835\udc502 to a fixed null value \u2205 with\nprobabilities \ud835\udc5d1 and \ud835\udc5d2. At inference time, we follow Brooks\u2019 [3]\napproach for two conditional inputs and use the following linear\ncombination of the conditional and unconditional score estimates:\n\u02c6\ud835\udf16(\ud835\udc67\ud835\udc61,\ud835\udc501,\ud835\udc502) = \ud835\udf16(\ud835\udc67\ud835\udc61, \u2205, \u2205) + \ud835\udc601(\ud835\udf16(\ud835\udc67\ud835\udc61,\ud835\udc501, \u2205) \u2212 \ud835\udf16(\ud835\udc67\ud835\udc61, \u2205, \u2205))\n+\ud835\udc602(\ud835\udf16(\ud835\udc67\ud835\udc61,\ud835\udc501,\ud835\udc502) \u2212 \ud835\udf16(\ud835\udc67\ud835\udc61,\ud835\udc501, \u2205)),\n(4)\nwhere \ud835\udc601 and \ud835\udc602 are the guidance scales. The guidance scales control\nwhether the generated video relies more on the context of the video\nor on the global frames of the video.\n3.3\nHybrid Coarse-to-Fine Pipeline for Video\nOutpainting\nIn video generation tasks, the generation of long videos often leads\nto the accumulation of artifacts, resulting in degraded performance.\nRecent research [2, 17, 43] used a hierarchical structure first to\ngenerate sparse key frames of the video, and then use an infilling\nstrategy to fill in dense video frames. The infilling strategy requires\nthe first and last frames as guide frames to guide the generation of\nthe next level. However, using infilling alone can result in a large\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nFanda Fan et al.\nDehan\nSDM\nOurs\nDehan\nSDM\nOurs\nDehan\nSDM\nOurs\nFigure 5: Qualitative Comparison of short video outpainting. We present the results of three groups of horizontally oriented\nvideo outpainting with ratio proportions of 0.4, 0.5, and 0.6. We mark the area to be extended with a red line in the first image.\n(a)\n(b)\nFigure 6: Evaluation of different time intervals and guidance\nscale weights.\ntime interval between frames in the coarse phase. For example, as\nshown in Fig. 4, if we only use infilling strategy, our model requires\na frame interval of 225 instead of 30 in the coarsest level. Due to\nthe difficulty of the problem and the lack of long video data in the\ntraining set, this can lead to poor results.\nThanks to bidirectional learning, our 3D UNet can perform video\noutpainting by combining infilling and interpolation. This avoids\nthe problem of large frame intervals in the coarse generation phase.\nOur coarse-to-fine process diagram is shown in Fig. 4. Our coarse-\nto-fine pipeline is divided into three levels. In the first level (coarse),\nwe unconditionally generate the first video clip and then itera-\ntively generate all keyframes based on the results of the last frame\nfrom the previous iteration. In the second level (coarse), we use\nthe keyframes generated in the first level as conditional inputs to\ngenerate more keyframes through interpolation. In the third level\n(fine), we generate the final video outpainting result with a frame\nHierarchical Masked 3D Diffusion Model for Video Outpainting\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nSDM\nOurs\nSDM\nOurs\nSDM\nOurs\nFigure 7: Qualitative Comparison of long video outpainting. We present the results of three groups of horizontally oriented\nvideo outpainting with a ratio proportion of 0.6. We mark the area to be extended with a red line in the first image.\ninterval of 1, using the first and last frames as guide frames for\ndense generation.\n4\nEXPERIMENTS\nTo verify the effectiveness of our masked 3D diffusion model\nfor video outpainting, we conduct evaluations on three datasets:\nDAVIS [27], YouTube-VOS [41], and our 5M E-commerce dataset.\nDAVIS and YouTube-VOS are commonly used datasets for video\ninpainting and outpainting. However, their average video length\nis short. Therefore, to validate the outpainting performance for\nlonger videos, we collect long videos from the e-commerce scene,\ncalled 5M E-commerce dataset. Our 5M E-commerce dataset con-\ntains over 5 million videos, with an average video length of around\n20 seconds. It consists of videos provided by advertisers to show-\ncase their products, mainly including furniture, household goods,\nelectronics, clothing, food, and other commodities. We describe our\nimplementation details in Appendix C.2.\n4.1\nBaselines and Evaluation Metrics\nWe compare with the following methods: 1) Dehan [6] proposed a\nframework for video outpainting. They separated the foreground\nand background and performed flow estimation and background\nestimation separately before integrating them into a complete result.\n2) We also train a simple diffusion model (SDM) based on stable\ndiffusion [29] as a baseline. It adopts the first frame and last frame\nas condition frame concatenated with the context video clip at\nthe input layer without using mask modeling and fed into the\ndenoising 3D UNet. Meanwhile, we do not use global features as a\nprompt, and cross attention is removed. 3) MAGVIT [15] used mask\nmodeling technology to train a transformer [9] for video generation\nin the 3D Vector-Quantized [11, 36] space. We included this set of\ncomparisons in Appendix B.\nWe follow [6] and use five commonly used evaluation metrics:\nMean Squared Error(MSE), Peak Signal To Noise Ratio (PSNR),\nstructural similarity index measure (SSIM) [39], Learned Perceptual\nImage Patch Similarity (LPIPS) [45], and Frechet Video Distance\n(FVD) [35]. To evaluate MSE, PSNR, SSIM, and FVD, we convert\nthe generated results into video frames with a value range of [0, 1],\nwhile LPIPS is evaluated using a value range of [\u22121, 1]. For the\nFVD evaluation metric, we use a uniform sampling of 16 frames\nper video for evaluation.\n4.2\nShort Video Outpainting\n4.2.1\nQualitative Comparison. In Fig. 5, we present the results of\nthree methods for horizontal video outpainting. It can be seen that\nDehan [6], although capable of generating a better background,\nproduces poor foreground results due to its dependence on the\nresult of flow prediction. The structural information of the sub-\nject in the filling area is essentially lost, resulting in unreasonable\noutcomes. SDM, with the help of strong diffusion tools and the\naddition of guide frames, is able to preserve the spatial structure\nof the filling area within a short interval. However, due to the lack\nof global information, it also loses many reasonable predictions in\ngenerating the complete video. In the third group of results with\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nFanda Fan et al.\nTable 1: Quantitative evaluation of video outpainting on the DAVIS and YouTube-VOS datasets.\nMethod\nDavis dataset [27]\nYouTube-VOS dataset [41]\nMSE \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFVD \u2193\nMSE \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFVD \u2193\nDehan [6]\n0.0260\n17.96\n0.6272\n0.2331\n363.1\n0.02312\n18.25\n0.7195\n0.2278\n149.7\nSDM [29]\n0.0153\n20.02\n0.7078\n0.2165\n334.6\n0.01687\n19.91\n0.7277\n0.2001\n94.81\nOurs\n0.0149\n20.26\n0.7082\n0.2026\n300.0\n0.01636\n20.20\n0.7312\n0.1854\n66.62\na mask ratio of 0.6 in Fig. 5, SDM produces a bad case with some\nnoisy outcomes. We find that the introduction of mask modeling\ncan alleviate the proportion of bad cases generated by the diffusion\nmodel. We will discuss this further in the ablation study. As can be\nseen in our method, we not only preserve the spatial information\nof the foreground subject in the filling area but also generate a\nreasonable background. Thanks to the introduction of global video\ninformation, our method can perceive that the motorcycle should\nappear in the filling area in the third group 3 at an early stage.\nMoreover, compared with SDM, our additional mask modeling can\ngenerate fewer bad cases.\n4.2.2\nQuantitative Results. We compare the outpainting results\nin the horizontal direction on datasets DAVIS and YouTube-VOS\nwith Dehan [6] and SDM, using mask ratios of 0.25 and 0.666. For\neach evaluation metric, we report their mean values across all test\nsamples. Our evaluation results on the DAVIS and YouTube-VOS\ndatasets are shown in Table 1.\n4.3\nLong Video Outpainting\nWe demonstrate a comparison between densely prediction and\ncoarse-to-fine (CTF) prediction on a long video in Fig. 2. It can\nbe seen that densely prediction not only produces unreasonable\nresults in the early predictions of the video but also suffers from the\naccumulation of artifacts from previous iterations. We claim that\nthe CTF prediction method can generate more reasonable results in\nthe early predictions by considering longer video clip information,\nwhile also alleviating the problem of artifact accumulation due to\nthe decrease of times of auto-regressive inference.\n4.3.1\nStudy of Time Interval Between Frames. We explore the rela-\ntionship between the frame interval generated in the coarse stage\nand the results in Fig. 6a. We randomly select 100 long videos from\nour 5M e-commerce dataset as the test set. Interval 15 means a two-\nlevel prediction structure, while greater than 15 means a three-level\nstructure. We found that the results generated by the three-level\nstructure were better than those generated by the two-level struc-\nture. However, further increasing the interval between frames in\nthe third level resulted in performance degradation in the M3DDM\nand SDM models. Especially when only using the infilling strategy,\na frame interval of 225 resulted in greater degradation in both the\nSDM and M3DDM. It is worth noting that SDM can only use a time\ninterval of 225 at the third level because it uses the first and last\nframes as guide frames.\nFor qualitative comparison, we contrast our approach with SDM\non 3 long videos in our 5M e-commerce dataset. The SDM here\nadopts a two-level CTF with time intervals of [15, 1] respectively. As\nshown in Fig. 7, our M3DDM not only generates foreground subjects\nTable 2: Ablation study on our e-commerce dataset. \u2018w/o\u2019\nmeans without.\nMethod\nMSE \u2193\nPSNR \u2191 SSIM \u2191 LPIPS \u2193 FVD \u2193\nSDM\n0.01134\n17.92\n0.6783\n0.2139\n110.4\nMSDM w/o prompt 0.00914\n19.22\n0.6912\n0.2012\n70.8\nOurs\n0.00791\n20.01\n0.7112 0.1931\n68.3\nwell in the area to be filled but also produces more consistent\nbackground results.\n4.4\nAblation Study\nWe conduct an ablation study on our 5M e-commerce dataset. We\nrandomly select 400 videos from 5M e-commerce dataset, with an\naverage length of 20 seconds. In our simple diffusion model (SDM),\nwe only use the first and last guide frames concatenation with the\ncontext of the video clip for training, without incorporating mask\nmodeling and global frames. In order to independently verify the\nimprovement effect of mask modeling on the diffusion model, we\nemploy a SDM and combined it with the mask modeling (As we\nmentioned in Sec.3.2.1) to train the masked SDM (MSDM). Our\napproach is to introduce a global video clip as a prompt based\non the masked SDM. In long video inference, we use a two-level\ncoarse-to-fine inference structure on the SDM (three levels have\na degradation in performance), and a three-level coarse-to-fine\ninference pipeline is used in the masked SDM and our approach.\nAs shown in Table 2, compared with short videos, our approach\nand SDM have a larger performance gap in long videos. Compared\nwith SDM, MSDM produced better video outpainting results.\n4.4.1\nEffective of Guidance Scales. In Fig. 6b, we present the ef-\nfectiveness of guidance scales. When we change \ud835\udc601, we fix \ud835\udc602 at 4.\nWhen we change \ud835\udc602, we fix \ud835\udc601 at 2. \ud835\udc601 controls the model to generate\nresults that are more relevant to the video context, and \ud835\udc602 helps\nthe model generate more reasonable results in scenes where the\ncamera is moving or the foreground subject is moving. We found\nthat it is more important to have classifier-free guidance for video\ncontext. When we do not have classifier-free guidance for video\ncontext, the performance degrades significantly. At the same time,\nhaving classifier-free guidance for video context and global frames\nbrings better results.\n5\nCONCLUSION\nIn this paper, we propose a 3D diffusion model based on mask\nmodeling for video outpainting. We use bidirectional learning and\nglobally encoding video frames as a prompt for cross-attention with\nHierarchical Masked 3D Diffusion Model for Video Outpainting\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\ncontext. The bidirectional learning approach of mask modeling al-\nlows us to have more flexible strategies in the inference stage while\nbetter perceiving adjacent frame information. The addition of a\nglobal video clip as a prompt further improves our method\u2019s perfor-\nmance. In most cases of camera movement and foreground object\nsliding, global frames help the model generate more reasonable\nresults in filling the areas. We also propose a hybrid coarse-to-fine\ninference pipeline for video outpainting, which combines infilling\nand interpolation strategies. Experiments show that our method\nachieves state-of-art results.\nREFERENCES\n[1] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen\nin Time: A Joint Video and Image Encoder for End-to-End Retrieval. In IEEE\nInternational Conference on Computer Vision.\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook\nKim, Sanja Fidler, and Karsten Kreis. 2023. Align your Latents: High-Resolution\nVideo Synthesis with Latent Diffusion Models. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\n[3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. 2023. InstructPix2Pix:\nLearning to Follow Image Editing Instructions. In CVPR.\n[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. 2022.\nMaskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 11315\u201311325.\n[5] Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, and\nMing-Hsuan Yang. 2022. InOut: diverse image outpainting via GAN inversion. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n11431\u201311440.\n[6] Lo\u00efc Dehan, Wiebe Van Ranst, Patrick Vandewalle, and Toon Goedem\u00e9. 2022.\nComplete and temporally consistent video outpainting. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 687\u2013695.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[8] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on\nimage synthesis. Advances in Neural Information Processing Systems 34 (2021),\n8780\u20138794.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[10] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,\nVladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox.\n2015. Flownet: Learning optical flow with convolutional networks. In Proceedings\nof the IEEE international conference on computer vision. 2758\u20132766.\n[11] Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers\nfor high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. 12873\u201312883.\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial\nnetworks. Commun. ACM 63, 11 (2020), 139\u2013144.\n[13] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska,\nSusanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos,\nMoritz Mueller-Freitag, et al. 2017. The\" something something\" video database\nfor learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision. 5842\u20135850.\n[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu\nYuan, and Baining Guo. 2022. Vector quantized diffusion model for text-to-image\nsynthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 10696\u201310706.\n[15] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart\u00edn-Mart\u00edn,\nand Li Fei-Fei. 2022. Maskvit: Masked visual pre-training for video prediction.\narXiv preprint arXiv:2206.11894 (2022).\n[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick.\n2022. Masked autoencoders are scalable vision learners. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 16000\u201316009.\n[17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. 2022.\nLatent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary\nLengths. arXiv preprint arXiv:2211.13221 (2022).\n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey\nGritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet,\net al. 2022. Imagen video: High definition video generation with diffusion models.\narXiv preprint arXiv:2210.02303 (2022).\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\n[20] Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv\npreprint arXiv:2207.12598 (2022).\n[21] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[22] Han Lin, Maurice Pagnucco, and Yang Song. 2021. Edge guided progressively\ngenerative image outpainting. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 806\u2013815.\n[23] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. 2022. Pseudo Numerical Meth-\nods for Diffusion Models on Manifolds. In International Conference on Learning\nRepresentations. https://openreview.net/forum?id=PlKWVd2yBkY\n[24] Farzaneh Mahdisoltani, Guillaume Berger, Waseem Gharbieh, David Fleet, and\nRoland Memisevic. 2018. On the effectiveness of task granularity for transfer\nlearning. arXiv preprint arXiv:1804.09235 (2018).\n[25] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael\nPritch, Yaniv Leviathan, and Yedid Hoshen. 2023. Dreamix: Video diffusion\nmodels are general video editors. arXiv preprint arXiv:2302.01329 (2023).\n[26] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffu-\nsion probabilistic models. In International Conference on Machine Learning. PMLR,\n8162\u20138171.\n[27] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus\nGross, and Alexander Sorkine-Hornung. 2016. A benchmark dataset and eval-\nuation methodology for video object segmentation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. 724\u2013732.\n[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n2022. Hierarchical text-conditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125 (2022).\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n10684\u201310695.\n[30] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim\nSalimans, David Fleet, and Mohammad Norouzi. 2022. Palette: Image-to-image\ndiffusion models. In ACM SIGGRAPH 2022 Conference Proceedings. 1\u201310.\n[31] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang,\nQiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video:\nText-to-video generation without text-video data. arXiv preprint arXiv:2209.14792\n(2022).\n[32] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.\n2015. Deep unsupervised learning using nonequilibrium thermodynamics. In\nInternational Conference on Machine Learning. PMLR, 2256\u20132265.\n[33] Shiqi Sun, Shancheng Fang, Qian He, and Wei Liu. 2023. Design Booster: A Text-\nGuided Diffusion Model for Image Translation with Spatial Layout Preservation.\narXiv preprint arXiv:2302.02284 (2023).\n[34] Zachary Teed and Jia Deng. 2020. Raft: Recurrent all-pairs field transforms for\noptical flow. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part II 16. Springer, 402\u2013419.\n[35] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier,\nMarcin Michalski, and Sylvain Gelly. 2018. Towards accurate generative models\nof video: A new metric & challenges. arXiv preprint arXiv:1812.01717 (2018).\n[36] Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation\nlearning. Advances in neural information processing systems 30 (2017).\n[37] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. 2022. Masked\nconditional video diffusion for prediction, generation, and interpolation. arXiv\npreprint arXiv:2205.09853 (2022).\n[38] Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, and Yi Yang. 2021. Sketch-\nguided scenery image outpainting. IEEE Transactions on Image Processing 30\n(2021), 2643\u20132655.\n[39] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image\nquality assessment: from error visibility to structural similarity. IEEE transactions\non image processing 13, 4 (2004), 600\u2013612.\n[40] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu\nXu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer. 2023.\nDiffusion Models as Masked Autoencoders. arXiv preprint arXiv:2304.03283\n(2023).\n[41] N Xu, L Yang, Y Fan, D Yue, Y Liang, J Yang, and T YouTube-VOS Huang. 2018.\nA large-scale video object segmentation benchmark. arXiv preprint (2018).\n[42] Chiao-An Yang, Cheng-Yo Tan, Wan-Cyuan Fan, Cheng-Fu Yang, Meng-Lin Wu,\nand Yu-Chiang Frank Wang. 2022. Scene graph expansion for semantics-guided\nimage outpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 15617\u201315626.\n[43] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Min-\nheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. 2023. NUWA-\nXL: Diffusion over Diffusion for eXtremely Long Video Generation. arXiv preprint\narXiv:2303.12346 (2023).\n[44] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00e9 Lezama, Han Zhang, Huiwen Chang,\nAlexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. 2022.\nMAGVIT: Masked Generative Video Transformer. arXiv preprint arXiv:2212.05199\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nFanda Fan et al.\n(2022).\n[45] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.\n2018. The unreasonable effectiveness of deep features as a perceptual metric.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\n586\u2013595.\nHierarchical Masked 3D Diffusion Model for Video Outpainting\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nTable 3: Evaluate the performance of video outpainting us-\ning FVD on something-something-v2. We obtain the results\ndirectly from MAGVIT.\nMethod\nAVG\nOPC\nOPV\nOPH\nMAGVIT-L-MT [44]\n18.3\n21.1\n16.8\n17.0\nOurs\n16.0\n19.2\n14.5\n14.3\nA\nAPPENDIX OVERVIEW\nOur supplementary materials provide additional experimental re-\nsults and comparison methods to better evaluate our approach. At\nthe same time, we also supplement the implementation details that\nwere not expanded in the main text due to space limitations. Our\nsupplementary materials are described in the following sections:\n\u2022 Compared\nwith\nMAGVIT\non\nSomething-Something\nV.2 (SSv2) Dataset. We additionally conduct a comparative\nexperiment with MAGVIT [44]. We directly obtain quantita-\ntive results from their paper and compare them using the\nsame setting on the SSv2 dataset.\n\u2022 Network architecture and implementation details.\n\u2022 Limitations. We briefly presented some bad cases generated\nby our method.\nB\nCOMPARED WITH MAGVIT\nIn the introduction of our main text, MAGVIT [44] has been briefly\nintroduced. They used mask modeling technology to train a trans-\nformer [9] for video generation in the 3D Vector-Quantized [11, 36]\nspace. They also evaluated MAGVIT\u2019s performance in video out-\npainting tasks in the paper. However, MAGVIT lacks constraints on\ndifferent clips of the same video, resulting in poor temporal consis-\ntency in the generated results between different clips. Our M3DDM\nmodel, utilizing the diffusion model and introducing global video\nframes as prompts, along with mask modeling and guided frame\ntechniques, not only performs well in generating long videos but\nalso surpasses MAGVIT [44] in short video outpainting.\nIn order to compare with the MAGVIT [44], we obtain the eval-\nuation results directly from their paper. They evaluated three types\nof video outpainting FVD [35] scores on the Something-Something\nV.2 (SSv2) [13, 24] dataset. The three types of outpainting are Cen-\ntral Outpainting (OPC), Vertical Outpainting (OPV), and Horizontal\nOutpainting (OPH). The mask ratio for each type is 0.75 for OPC,\n0.5 for OPV, and 0.5 for OPH. We strictly follow their setup, using\n169K videos for training and 24K videos for evaluation on the SSv2\ndataset. We train the dataset using 24 A100 GPUs, with a batch\nsize of 240 and fine-tuned for 126k steps. The average video length\nof SSv2 is around 30 frames, and we use the dense prediction, fol-\nlowing the settings of short video outpainting in the main paper\nwe reported. We use the same FVD [35] evaluation metric as them,\nwith 16 frames for each video. Each evaluated video is sampled\nwith 2 temporal windows and a central crop with a frame size of\n128. The comparison results are shown in Table 3. We also present\nthe qualitative results of the three types of video outpainting in\nFig. 8.\nC\nNETWORK ARCHITECTURE AND\nIMPLEMENTATION DETAILS\nC.1\nNetwork Architecture\nOur approach consists of two trainable networks: a 3D denoising\nUnet and a lightweight video encoder. Our 3D denoising UNet uses\nthe pre-trained parameters from the text-to-image model in LDMs.\nIn order to adapt it for our task with a 3D structure, we employ tem-\nporal convolution, self-attention, and cross-attention operations to\nensure the interaction between different frames. Our 3D denoising\nUnet takes latents from the VAE encoder [29] as input, with dimen-\nsions of (\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e_\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52,\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc53 \ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc52\ud835\udc60_\ud835\udc5c\ud835\udc53 _\ud835\udc63\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5c,\ud835\udc56\ud835\udc5b_\ud835\udc50\u210e\ud835\udc4e\ud835\udc5b\ud835\udc5b\ud835\udc52\ud835\udc59\ud835\udc60,\u210e\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61,\n\ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61). Our 3D denoising Unet predicts the noise with shape\n(\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e_\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52,\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc53 \ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc52\ud835\udc60_\ud835\udc5c\ud835\udc53 _\ud835\udc63\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5c,\ud835\udc5c\ud835\udc62\ud835\udc61_\ud835\udc50\u210e\ud835\udc4e\ud835\udc5b\ud835\udc5b\ud835\udc52\ud835\udc59\ud835\udc60,\u210e\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61,\ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61).\nIn our implementation, \ud835\udc56\ud835\udc5b_\ud835\udc50\u210e\ud835\udc4e\ud835\udc5b\ud835\udc5b\ud835\udc52\ud835\udc59\ud835\udc60 is 9, where 8 dimensions\nrepresent the latent of the original video frames and masked\nframes (with 4 dimensions each), and 1 dimension represents the\nmask. \ud835\udc5c\ud835\udc62\ud835\udc61_\ud835\udc50\u210e\ud835\udc4e\ud835\udc5b\ud835\udc5b\ud835\udc52\ud835\udc59\ud835\udc60 is 4, the same as the latent of the original\nvideo frames. After compression by VAE, the dimensions of our\nheight and weight become 32. Our 3D denoising UNet heavily\nreferences the network structure in Make-A-Video [31]. We follow\nthe Make-A-Video [31] by utilizing Pseudo-3D convolutional and\nattention layers to leverage pre-trained text-to-image models\nwithin the latent diffusion models(LDMs) [29]. Each spatial 2D conv\nlayer is followed by a temporal 1D conv layer. We not only add the\ntimestep embeddings of the noise to each layer but also add the\nfps rate embeddings. This allows us to use one model to generate\nvideo clips with different frame intervals. Our 3D denoising Unet\nhas four downsampling and four upsampling layers, with each\nlayer outputting the following number of channels: [320, 640, 1280,\n1280]. Our 3D Unet has a total of 1299.28M parameters. For more\ndetails, we recommend referring to the network architecture in\nMake-A-Video [31].\nWe have presented our lightweight video encoder in Fig. 9. Our\nlightweight video encoder accepts the global video latents obtained\nfrom VAE and increases its dimensionality from 4 to 320 for cross-\nattention.\nC.2\nImplementation Details\nSampling Details. We use the PNDMScheduler from pseudo nu-\nmerical methods for diffusion models (PNDMs) [23]. We use 50\ninference steps and a scaled linear \ud835\udefd schedule that starts at 0.00085\nand ends at 0.012.\nOur 3D denoising UNet is capable of generating \ud835\udc39 = 16 frames\nin a single inference, and we use \ud835\udc54 = 16 global frames. we randomly\nextract F frames from video clips, with equal intervals between each\nframe. The frame intervals are uniformly sampled from fps [1, 30].\nWe employ the Adam [21] optimizer with a learning rate of 1e-4,\nand the warm-up learning rate step is 1k. We trained the model\nfor 4 epochs on the WebVid dataset [1] and then fine-tuned it for 3\nepochs on our 5M e-commerce dataset. All training was done on 24\nA100 GPUs, and the entire training process took approximately 2.5\nweeks. We use the dense predict form for short video outpainting\nand the three-level coarse-to-fine structure with time intervals of\n[30, 15, 1] for long video outpainting. We found that the inference\nmethods with frame intervals of [15, 5, 1] were nearly equally\neffective. However, considering the length of our long videos, we\nopted for the inference method with frame intervals of [30, 15, 1].\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nFanda Fan et al.\nGT\nOurs\nGT\nOurs\nGT\nOurs\nGT\nOurs\nGT\nOurs\nGT\nOurs\nFigure 8: Three types of video outpainting on the SSv2 dataset. The term GT refers to ground truth, and for each set of GT, the\narea to be filled is marked with red curves on the first image (the area outside the red lines is what we want to fill in).\nWe set \ud835\udc601 = 2 and \ud835\udc602 = 4 because experiments show that this leads\nto good outpainting results.\nThe resolution of our input video is 256 x 256 x 3. During the test\nphase, we can infer test samples with a batch size of 2 on a 16GB\ngraphics card (the test environment we use is Tesla v100 16Gb). Our\ntraining phase used 24 80GB A100 GPUs, with a total batch size of\n240.\nD\nLIMITATIONS AND BAD CASES\nWe show the bad cases generated by our model in Fig. 10. Our\nmethod utilizes a fixed image VAE [29] encoder to transform the\npixel-space video into the latent space. VAE often shows rough\nperformance in human faces and some fine structures. Moreover,\nour method is limited by the training data and the difficulty of the\nproblem, resulting in poor results in text generation within videos.\nOur diffusion model is sensitive to the initial Gaussian noise dur-\ning sampling, and some videos may experience edge blurring. We\nhave performed a simple preprocessing step on the extended region\nof the video to be predicted using the OpenCV inpaint function and\nadded 1000 steps of Gaussian noise instead of directly sampling\nHierarchical Masked 3D Diffusion Model for Video Outpainting\nMM \u201923, October 29-November 3, 2023, Ottawa, ON, Canada\nRes_Stage\nstride = 1\nRes_Stage\nstride = 1\nRes_Stage\nstride = 2\nRes_Stage\nstride = 1\nRes_Stage\nstride = 1\nConv2d\nkernel = 3\nstride = 1\nInput: [g, 4, 32, 32]\nOutput: [g, 320, 14, 14]\nout_channel = 8\nout_channel = 16\nout_channel = 32\nout_channel = 64\nout_channel = 128\nFigure 9: Our lightweight video encoder. \ud835\udc54 denotes the total\nnumber of global video frames inputted. In our implemen-\ntation \ud835\udc54 = 16. We referred to the image encoder in design-\nbooster [33].\nFigure 10: Bad case in our method.\nfrom the Gaussian noise, which partially solves the problem of\nprediction robustness.\n"
  },
  {
    "title": "Doppelgangers: Learning to Disambiguate Images of Similar Structures",
    "link": "https://arxiv.org/pdf/2309.02420.pdf",
    "upvote": "9",
    "text": "Doppelgangers: Learning to Disambiguate Images of Similar Structures\nRuojin Cai1\nJoseph Tung1\nQianqian Wang1\nHadar Averbuch-Elor2\nBharath Hariharan1\nNoah Snavely1\n1Cornell University\n2Tel Aviv University\nAbstract\nWe consider the visual disambiguation task of determin-\ning whether a pair of visually similar images depict the same\nor distinct 3D surfaces (e.g., the same or opposite sides of\na symmetric building). Illusory image matches, where two\nimages observe distinct but visually similar 3D surfaces, can\nbe challenging for humans to differentiate, and can also\nlead 3D reconstruction algorithms to produce erroneous\nresults. We propose a learning-based approach to visual dis-\nambiguation, formulating it as a binary classification task\non image pairs. To that end, we introduce a new dataset for\nthis problem, Doppelgangers, which includes image pairs of\nsimilar structures with ground truth labels. We also design\na network architecture that takes the spatial distribution of\nlocal keypoints and matches as input, allowing for better\nreasoning about both local and global cues. Our evaluation\nshows that our method can distinguish illusory matches in\ndifficult cases, and can be integrated into SfM pipelines to\nproduce correct, disambiguated 3D reconstructions. See\nour project page for our code, datasets, and more results:\ndoppelgangers-3d.github.io.\n1. Introduction\nFrom time to time we are faced with the task of distin-\nguishing between two things that are nearly indistinguish-\nable, but are not the same object. Examples of such looka-\nlikes include identical twin siblings, two similar keys on\na keychain, and two cups on a table at a party\u2014only one\nof which is ours. While these objects might look identical,\nthere are often subtle cues we can use to tell them apart;\nfor instance, even \u201cidentical\u201d twins are not truly visually\nidentical, but have perceptible differences.\nComputer vision systems also face a version of this prob-\nlem. In particular, our work considers geometric vision tasks\nlike 3D reconstruction, where methods often must determine\nwhether two images depict the exact same 3D surface in\nthe world, or two different 3D surfaces that happen to look\nvery similar\u2014where wrong answers can lead to wrong 3D\n(a) Arc de Triomphe\n(b) Charlottenburg Palace\n(c) Alexander Nevsky Cathedral\n(d) Sleeping Beauty Castles\nFigure 1. These image pairs observe distinct but visually similar\n3D surfaces. Can you spot the differences and distinguish between\nthe two images in each pair? Hints in the footnote.1 Such illusory\nimage matches can fool humans, and also fool 3D reconstruction\nalgorithms into thinking they share 3D correspondence. We propose\na new method to disambiguate these kinds of false matches from\nimage pairs that truly observe the same structure.\nmodels. We call this task visual disambiguation, but you\ncould also call it the Big Ben problem: London\u2019s Big Ben\nis a clock tower with four-way symmetry, where the four\nsides of the tower look nearly the same. Local feature match-\ning methods like SIFT easily confuse one side for another,\nfinding many matches between distinct 3D surfaces. These\nspurious matches lead structure from motion methods to pro-\nduce incorrect reconstructions where multiple sides collapse\ntogether. Yet views of different sides of Big Ben are not\ntruly identical\u2014the individual bricks are different, the back-\ngrounds are different, etc. If matching methods knew what\nto look for, they could perhaps tell the different sides apart.\nFigure 1 shows other examples of this \u201cspot the difference\u201d\nproblem\u2014see if you can tell these structures apart yourself.\nWe call illusory image matches like those in Fig. 1 doppel-\ngangers, after the idea of two distinct people or objects that\nlook very similar. Prior methods for disambiguating doppel-\ngangers in the context of 3D vision have devised heuristics\n1Attend to: (a) the sculptures on the bottom half of the monument, (b)\nthe shape of the facade\u2019s roof (flat or triangular), (c) the location of the\nsmaller tower (left or right), and (d) the background (with or without a\nmountain). Note that (a), (b), (c) show different faces of the same building,\nwhile (d) shows replicas of the same castle in different Disney parks.\narXiv:2309.02420v1  [cs.CV]  5 Sep 2023\nthat analyze the structure of a full image collection. In con-\ntrast, our paper explores the fundamental building block of\npairwise image comparison: can we automatically deter-\nmine whether two views are the same, or just similar? We\nformulate this visual disambiguation problem as a binary\nclassification task on image pairs, and develop a learning-\nbased solution.\nOur solution involves assembling a new dataset, Dop-\npelgangers, consisting of image pairs that either depict the\nsame surface (positives) or two different, similar surfaces\n(negatives). Creating Doppelgangers involved a challenging\ndata curation task, since even humans can struggle to dis-\ntinguish same from similar. We show how to use existing\nimage annotations stored in the Wikimedia Commons image\ndatabase to automatically create a large set of labeled image\npairs. We find that simply training a deep network model\nusing these raw image pairs performs poorly. Therefore, we\nalso design a network where we provide useful information\nin the form of local features and 2D correspondence.\nOn our Doppelgangers test set, we find that our method\nworks remarkably well on challenging disambiguation tasks,\nand significantly better than baselines and alternative net-\nwork designs. We also explore the use of our learned classi-\nfier as a simple pre-processing filter on scene graphs com-\nputed in structure from motion pipelines like COLMAP [32],\nand find that it significantly improves the correctness of\nreconstructions on a set of difficult scenes, outperforming\nmore complex visual disambiguation algorithms.\nIn summary, our paper makes the following contributions:\n\u2022 We formulate the visual disambiguation problem on\npairs of images.\n\u2022 Based on this formulation, we create the Doppelgangers\nDataset, leveraging the existing cataloging of imagery\non Wikimedia Commons.\n\u2022 We design a network architecture well-suited for solv-\ning pairwise visual disambiguation as a classification\nproblem. We show that training this network on our\ndataset leads to strong classification performance and\ndownstream utility to SfM problems.\n2. Related Work\nLocal feature matching methods have been wildly suc-\ncessful [24]. The combination of repeatability, discriminabil-\nity, invariance to image transformations, and robustness to\nfactors like partial occlusion make local features ideal for\nanswering the question, Are these two images in correspon-\ndence?, solidifying them as a foundation for downstream\ntasks like image retrieval [33, 25, 3], near-duplicate detec-\ntion [4], and structure from motion (SfM) [31, 34, 32].\nHowever, these same properties make it hard for local\nfeature matching methods to definitively answer the nega-\ntion of this question: Are these two (possibly similar) images\nnot in correspondence? The dominant assumption in lo-\ncal feature matching is that sufficiently many geometrically\nconsistent feature matches are strong positive evidence that\ntwo images correspond; more rarely is negative evidence\nconsidered. This has remained true as deep learning has led\nto improvements in feature detection [8, 9], feature extrac-\ntion [41, 26, 11, 27], and feature matching [42, 30]. Gener-\nally, these methods all still seek to find as many consistent\ncorrespondences as possible based on local appearance, but\nrarely consider global evidence that a pair of images might\nin reality be deceptively similar, non-overlapping views.\nThere are a few counterexamples to this view of feature\nmatching. In bag-of-visual-words\u2013based image retrieval\nmethods, TF-IDF weighting is often used to downweight\nlocal features that occur across many images, because such\nfeatures are poor evidence that images match [33]. J\u00b4egou and\nChum go further and consider missing visual word matches\nas negative evidence for image-level matches [17]. Other\nmethods consider the visual change detection problem, and\nspecifically look for differences between two views of (usu-\nally) the same scene [29]. For SfM on Internet collections, a\ncommon problem are watermarks, timestamps, and frames\n(decorative borders) [37, 15]. These user-added visual ele-\nments yield spurious feature matches on otherwise unrelated\nimages, which can often be filtered with heuristics.\nMost related to our work are methods for disambiguation\nof image collections in the context of SfM. These methods\nattempt to fix the problem of broken 3D reconstructions\nin the face of repeated structures. Some SfM disambigua-\ntion methods carefully identify reliable matches [19], while\nmany others identify cues for identifying spurious matches.\nA common approach is to look for evidence in the scene\ngraph\u2014the network of corresponding images computed dur-\ning SfM\u2014such as loops that lack geometric cycle consis-\ntency [44], graph structures that are inconsistent with physi-\ncal 3D space [38], and graph geodesic constraints [40]. Other\nmethods detect missing correspondences as a negative cue\nfor image-level matches [43, 18, 6]. Heinly et al. go further\nand detect conflicting 3D observations in SfM models, such\nas images that observe 3D points that should be impossi-\nble to see because they would be occluded by some other,\nspurious geometry [14]. Other methods rely on non-visual\ninformation, like timestamps or image ordering [28]. Finally,\nthe SLAM community has studied a related problem referred\nto as perceptual aliasing, where distinct locations within an\nenvironment (e.g., an office building with identical offices)\nyield similar visual fingerprints [1, 7, 20, 16].\nPrior global methods often rely on hand-designed heuris-\ntics that can be brittle, can require manual parameter tun-\ning for each individual reconstruction task, and can also\nover-segment correct 3D models. In contrast, our method\nis designed for the fundamental problem of two-frame vi-\nsual disambiguation, which we solve by learning from data.\nWe find that our method can disambiguate a range of SfM\ndatasets with minimal tuning. It considers cues distinct from\nthose of global image connectivity methods, such as RGB\nvalues and spatial distributions of keypoints/matches, and\ncan implicitly take advantage of both positive and negative\nsignals (like missing correspondences). Surprisingly, we\nfind that looking at pairs of images, without considering the\nfull image collection, is sufficient to create correct recon-\nstructions in nearly all of our experiments. Furthermore,\nour approach is also orthogonal to global structure\u2013based\nmethods, and could naturally be combined with them.\n3. Visual disambiguation\nOur work addresses the following visual disambiguation\nproblem: Given two possibly very similar images, determine\nwhether they depict the same physical 3D surface, or whether\nthey are images of two different 3D surfaces. This is a binary\nclassification task on image pairs. A true (positive) matching\npair is one where both images depict the same physical 3D\nsurface, while a false (negative) pair observes distinct 3D\nsurfaces with no (or very few) identical 3D points in common.\nIllusory image matches\u2014false pairs that look similar, which\nwe also refer to as doppelgangers\u2014occur when two images\nobserve distinct but visually similar 3D surfaces.\nAmbiguous image pairs can arise from symmetric build-\nings, repeated visual elements, and replicas of landmarks\nin different parts of the world. For example, consider the\nimages of the Arc de Triomphe shown in Figure 2. At first\nglance, views of the front and back of this symmetric struc-\nture appear nearly identical. But on closer inspection we can\nobserve differences between the two sides, such as the dis-\ntinct sculptures. As this example illustrates, these cues can be\nhard to discern, as they can involve subtle differences amidst\noverall similar structures. This problem of distinguishing\ndoppelgangers is even more challenging in the presence of\nvarying illumination, viewpoint, and transient objects.\nDoppelgangers are also problematic in practice, espe-\ncially for 3D computer vision pipelines. These often rely\non feature matching methods that may find many incorrect\nmatches between illusory pairs. These incorrect matches can\nlead SfM methods to produce erroneous 3D reconstructions.\nWe find that simple classification schemes like thresh-\nolding on the number of feature matches do not suffice to\nidentify doppelgangers. Prior image collection\u2013level meth-\nods for visual disambiguation, discussed in Section 2, cannot\ndisambiguate individual image pairs, and can be brittle in\npractice or require time-consuming parameter tuning.\nIn contrast, we propose a learning-based approach that\ntrains a binary classifier on image pairs. Since, we know of\nno existing dataset for visual disambiguation, we collect a\nnew dataset of image pairs with carefully produced ground\ntruth labels (Section 4). Our goal is to differentiate illusory\nmatches, even when there are only subtle differences in small\nFront view\nFront view\nBack view\nFigure 2. Images captured from highly symmetric landmarks, such\nas the Arc de Triomphe, are difficult to disambiguate into true and\nfalse matching pairs due to repeated structures. However, by zoom-\ning into the sculptures, highlighted in red boxes, we can uncover\nsubtle differences between the front and back sides, allowing us to\ndifferentiate between them.\nregions. To achieve this, we propose a deep network that\ntakes the spatial distribution of keypoint and matches as\ninput to better reason about both local features and global\nand image-level information, as described in Section 5.\n4. The Doppelgangers Dataset\nWe present the Doppelgangers Dataset, a benchmark\ndataset that allows for training and standardized evaluation of\nvisual disambiguation algorithms. Doppelgangers consists\nof a collection of internet photos of world landmarks and\ncultural sites that exhibit repeated patterns and symmetric\nstructures. The dataset includes a large number of image\npairs, each labeled as either positive or negative based on\nwhether they are true or false (illusory) matching pairs. It\nis relatively easy to find image pairs in the wild that are\ncorrectly matching (positives). It is much more difficult to\nfind and accurately label negative image pairs. Below, we\ndescribe how we tackle this data gathering problem.\n4.1. Mining Wikimedia Commons\nInspired by the Google Landmarks [36] and WikiScenes\ndatasets [39], Doppelgangers is collected from Wikimedia\nCommons.\nWikimedia Commons has an extensive col-\nlection of freely available images contributed by the pub-\nlic.\nThese include a large number of photos of world\nlandmarks, organized into a hierarchy of categories. In\nsome landmark categories, there exists sub-categories orga-\nnized according to information like viewing direction (e.g.,\nNorth/South), providing valuable annotations for inferring\ngeometric relationships between images. For instance, the\ncategory consisting of exterior images of the \u00b4Eglise de la\nMadeleine (a church in Paris) is called Exterior of \u00b4Eglise de\nla Madeleine, with sub-categories that include North facade\nof \u00b4Eglise de la Madeleine and South facade of \u00b4Eglise de la\nMadeleine. We assemble a list of landmark categories with\nsub-categories that contain keywords related to directions,\nsuch as \u201cNorth\u201d, \u201cSouth\u201d, \u201cEast\u201d, \u201cWest\u201d, \u201cLeft\u201d, \u201cRight\u201d,\n\u201cFront\u201d, and \u201cBack\u201d. Following [39], we recursively down-\nload images from all sub-categories in the list to obtain an\ninitial set of images for each landmark. We also identi-\nfied a few other visually ambiguous landmark categories for\nour dataset, such as replicated Disneyland castles and the\nDeutscher und Franz\u00a8osischer Dom.\n4.2. Deriving image pairs with ground truth labels\nOne approach to creating and labeling image pairs would\nrely solely on the directional labels on images induced by\ntheir Wikimedia Commons category. In particular, if two\nimages of the same landmark are captured from the same\ndirection (e.g., both from the north), we could label them\na positive pair, whereas if they are captured from opposite\ndirections (e.g., one from the north and one from the south),\nwe could label them a negative pair, since images of op-\nposite building faces are unlikely to have any true overlap.\nHowever, this approach can produce positive pairs that lack\ncorrespondence, because two images of the same building\nface may not overlap (e.g., two closeups of different details).\nSimilar logic applies to negative pairs: we are mainly in-\nterested in hard negatives, i.e., images of different surfaces\nwhere feature matching yields spurious correspondences.\nHence, we need a way to mine for \u201cinteresting\u201d pairs.\nFinding potential doppelgangers by image matching. To\nidentify interesting image pairs that share putative correspon-\ndence (correctly or incorrectly), we use the feature matching\nmodule in COLMAP [32], a state-of-the-art SfM system.\nThis process yields a set of putative matching image pairs for\neach landmark in our dataset, along with local keypoints for\neach image and pairwise matches between image pairs. We\nonly include image pairs that have such pairwise matches in\nour dataset, ensuring visual similarity between the included\npairs. For positive pairs, this means they exhibit overlap, and\nfor negative pairs, they depict different but similar structures.\nAugmentation with image flipping. Some landmarks may\nnot naturally form negative pairs because they lack similar\nstructures when viewed from opposite directions. Therefore,\nit can be more difficult to find negative pairs compared to\npositive pairs (which are extremely abundant). When con-\ntemplating how to increase the number of negative training\npairs, we were inspired by the image pairs like the one shown\nin Figure 1(c), where the opposite views of some buildings\nresemble a 2D image flip. Horizontally flipping one image in\na pair changes it to a different, mirror-image scene [23], but\nfeature matches on similar structures may still exist. There-\nfore, to increase the variety of scenes in our training data, we\nsample a positive pair and flip one of the images, resulting\nin a negative pair\u2014a pair of similar images that nonethe-\nless correspond to different (mirror image) surfaces. Unlike\ntraditional data augmentation that generates more samples\nwith the same label, our approach transforms a positive pair\ninto a negative pair. Note that we only use such synthetic\naugmented pairs for training and not as test data.\n4.3. Dataset statistics\nThe above process results in \u223c76K internet photos of\n222 world landmarks and cultural sites, and yields over\n1M visually similar image pairs. Among these pairs, 178K\nare labeled as negative pairs. We provide additional data\ncollection details and statistics in the supplemental material.\nOf the 222 scenes, 58 naturally form negative pairs. Of\nthese 58, we split off 16 scenes (and sample 4,660 image\npairs) as a test set, divided evenly into positive and negative\npairs. From the 42 scenes remaining for the training set, we\nsample a maximum of 3K pairs per scene to ensure balance\nacross landmarks during training. These 42 scenes contribute\nto 73K training pairs, again divided nearly evenly between\npositive and negative pairs.\nOur proposed flipping augmentation on Wikimedia Com-\nmons imagery yields an additional 92K training pairs across\n164 scenes. To further augment negative pairs for use in\ntraining, we also applied the flipping augmentation to match-\ning image pairs from MegaDepth [21], a large dataset of\nmulti-view Internet photo collections. MegaDepth adds an\nadditional 57K training pairs from 72 scenes.\n5. Classifying visually ambiguous pairs\nWe now describe how we design a classifier for visual\ndisambiguation. A straightforward solution would be to train\na network to take as input a raw image pair, and to output\nthe probability that this pair is a positive match. However,\nwe found that this approach works poorly even starting from\npre-trained models and state-of-the-art architectures [2, 10].\nVisual disambiguation is a hard problem, and we conjecture\nthat it is difficult for a network to discover all the necessary\nsubtle cues from raw RGB images alone.\nTo gain insight into the problem, consider the front and\nback views shown in Figure 2. The task is similar to a\n\u201cspot-the-difference\u201d game, where we would look for image\nregions that should correspond, but don\u2019t. For instance, in\nFigure 2, the sculptures should match across the views, but\nare different. We might also note mismatched structures in\nthe background or even on low-level details like individual\nbricks. To allow the network to perform similar reasoning,\nwe provide it with information about the spatial distribution\nof distinctive keypoints and keypoint matches. In addition,\nwe perform a rough alignment of the images to allow the\nnetwork to directly compare corresponding regions. We\ndescribe these enhancements below.\n(b) Aligned image pair, keypoint mask, and match mask\n(a) Image pair (top) and \nkeypoints and matches (bottom)\n(c) Binary classifier\nFigure 3. Method overview. (a) Given a pair of images, we extract keypoints and matches via feature matching methods. Note that this is\na negative (doppelganger) pair picturing opposite sides of the Arc de Triomphe. The feature matches are primarily in the top part of the\nstructure, where there are repeated elements, as opposed to the sculptures on the bottom part. (b) We create binary masks of keypoints\nand matches. We then align the image pair and masks with an affine transformation estimated from matches. (c) Our classifier takes the\nconcatenation of the images and binary masks as input and outputs the probability that the given pair is positive.\n5.1. Spatial distribution of keypoints and matches\nGiven two input images, as a pre-processing step,\nwe compute keypoint matches between them, then use\nRANSAC [12] to estimate a Fundamental matrix and filter\nout outlier matches. We provide the locations of all detected\nkeypoints, as well as all (filtered) matches as an additional\nnetwork input in the form of two binary mask images.\nThe idea is that the keypoint and match locations provide\nuseful signals to the network. For instance, by showing the\nnetwork where keypoint matches were found, it also lets the\nnetwork know where matches weren\u2019t found (where there\nare keypoints but no matches). Such regions may indicate\nmissing or distinct objects. The network can also compute\nother signals if it chooses, like the raw number of matches\n(a reasonable baseline for visual disambiguation). As an\nexample, we illustrate SIFT keypoints and matches for a\ndoppelganger pair in Figure 3(a). We see that matches are\ndenser in regions with visually similar structures, but much\nsparser in regions with distinct structures such as sculptures.\nIn particular, for an image pair (Ia, Ib) (of dimensions\nH \u00d7 W), we extract keypoints and matches. We denote\nthe set of keypoints for image Ia as Ka = {xa\ni }Na\ni=1, where\nxa\ni is the pixel location of the ith keypoint. Similarly, we\ndenote matches between this image pair as a set of keypoint\npairs Ma,b = {(xa\nik, xb\njk)}Ma,b\nk=1 . We create a binary mask of\nkeypoints Ka \u2208 {0, 1}H\u00d7W using the keypoints Ka, where\npixels corresponding to keypoints (rounded to grid location)\nare set to one, and other pixels are set to zero. Similarly, we\ncreate a binary mask of matches Ma\na,b \u2208 {0, 1}H\u00d7W for Ia,\nwhere pixels corresponding to matches are set to one, and\nall other pixels are set to zero. These keypoint and match\nmasks are illustrated in Figure 3(b). Our classifier takes\nthese masks as input, along with the RGB image pair.\nAlignment for better comparison. To facilitate compari-\nson of potentially corresponding regions, we also perform\na rough geometric alignment of the input image pair. We\nestimate an affine transform T from the matches Ma,b, and\nwarp the images and the binary masks accordingly. Fig-\nure 3(b) shows an example. Note that the alignment need\nnot be perfect; the goal is simply to bring regions that the\nnetwork might wish to compare closer together.\n5.2. Binary classification\nAs illustrated in Figure 3(c), our classifier F\u03b8 takes an\nimage pair and derived binary keypoint and match masks as\ninput, and outputs the probability that the pair is a positive\nmatch. We concatenate the RGB images and masks for both\nimages into a 6 + 4 = 10-channel input tensor. To optimize\nour classifier F\u03b8, we use a focal loss [22], a modified version\nof the cross-entropy loss. The focal loss improves perfor-\nmance by balancing the distribution of positive and negative\npairs, and giving more weight to hard examples.\n5.3. Implementation details\nKeypoint and match masks. After resizing and padding\neach image to 1024 \u00d7 1024 resolution, we compute matches\nusing LoFTR [35], a detector-free, learned local feature\nmatching method. We use a LoFTR model pretrained on\nMegaDepth [21], which focuses on outdoor scenes. We then\nperform geometric verification using RANSAC [12], and\nestablish the keypoint mask using all LoFTR output matches\nand the match mask using geometrically verified matches.\nNetwork and training. The classifier F\u03b8 consists of 3 resid-\nual blocks [13], an average pooling layer, and a fully con-\nnected layer. We train for 10 epochs with a batch size of\n16 using the Adam optimizer with an initial learning rate of\n5 \u00d7 10\u22124. The learning rate is linearly decayed to 5 \u00d7 10\u22126\nfrom epoch 5 onwards. Additional implementation details\nare provided in the supplemental material.\nSIFT [24]+RANSAC [12]\nLoFTR [35]\nDINO [2]-ViT\nOurs\n#matches\n%matches\n#matches\n%matches\nLatent code\nFeature map\nAverage precision\n83.4\n81.2\n85.3\n86.0\n62.0\n63.3\n95.2\nROC AUC\n80.2\n77.1\n78.9\n80.3\n60.9\n61.5\n93.8\nAlexander Nevsky Cathedral, \u0141\u00b4od\u00b4z\n72.7\n75.9\n80.7\n80.4\n50.9\n50.3\n89.5\nAlexander Nevsky Cathedral, Sofia\n89.5\n87.6\n90.0\n92.2\n53.0\n53.6\n98.5\nAlexander Nevsky Cathedral, Tallinn\n73.1\n76.0\n76.1\n80.3\n58.8\n50.8\n86.2\nArc de Triomphe\n86.1\n81.7\n85.7\n93.3\n55.4\n61.1\n97.6\nBerlin Cathedral\n91.8\n91.6\n93.6\n92.7\n76.4\n70.6\n99.4\nBrandenburg Gate\n79.3\n73.7\n90.9\n95.6\n60.8\n60.9\n99.8\nCathedral of Saints Peter and Paul in Brno\n95.8\n96.4\n89.8\n88.4\n64.6\n79.9\n99.8\nCathedral of St Alexander Nevsky, Pre\u02c7sov\n82.5\n74.0\n86.1\n85.3\n62.9\n64.8\n94.6\nCharlottenburg Palace\n81.5\n76.1\n85.6\n81.1\n65.8\n54.1\n93.3\nChurch of Savior on the Spilled Blood\n82.1\n73.2\n84.9\n75.5\n63.9\n67.5\n93.8\nDeutscher und Franz\u00a8osischer Dom (Berlin)\n74.5\n71.9\n85.8\n84.2\n55.6\n51.5\n98.1\nFlorence Cathedral\n90.6\n83.8\n84.5\n82.0\n54.6\n63.8\n94.2\nSleeping Beauty Castle\n81.1\n81.2\n75.0\n85.6\n67.2\n66.4\n97.1\nSt. Vitus Cathedral\n96.8\n88.0\n89.2\n87.5\n84.0\n77.0\n99.8\nSydney Harbour Bridge\n79.4\n92.3\n83.8\n86.2\n53.0\n75.5\n87.0\nWashington Square Arch\n77.7\n75.9\n82.8\n86.0\n65.2\n65.0\n95.1\nTable 1. Quantitative results for visual disambiguation evaluated on the Doppelgangers test set. The first two rows show the average\nprecision and ROC AUC multiplied by 100, respectively, averaged over the 16 test scenes. The remaining rows show the average precision,\nmultiplied by 100, for each individual scene. #matches refers to thresholding the number of matches and %matches refers to thresholding\nthe ratio of the number of matches to the number of keypoints, as described in Section 6.1.\n6. Experiments\nIn this section, we evaluate our visual disambiguation\nmethod on the Doppelgangers dataset. We then discuss how\nour pairwise classifier can be integrated into a SfM pipeline\nfor reconstructing visually ambiguous image collections.\nOur experimental results demonstrate the effectiveness and\ngeneralization power of our method for SfM disambigua-\ntion. Finally, we provide an ablation study to validate the\neffectiveness of each component in our network design.\n6.1. Visual disambiguation\nBaselines. We compare our method to three baselines:\nOne set of baselines simply uses the number of local fea-\nture matches to predict if an image pair is a positive (true)\nmatch. We evaluate two feature matching baselines, one\nbased on SIFT [24] and one on LoFTR [35]. Both SIFT\nand LoFTR matches are filtered via geometric verification\nusing RANSAC, yielding a cleaner set of matches for use\nin classification. Our baselines use these matches in two\nways: (1) thresholding the number of matches after geomet-\nric verification, and (2) thresholding the ratio of the number\nof matches to the number of keypoints. The idea behind\n(2) is that if there are few matches relative to the number of\nkeypoints, that may be a sign that the pair is a doppelganger.\nIn addition, we compare our method to DINO [2], self-\nsupervised features that achieve state-of-the-art image clas-\nsification and semantic segmentation results. We train a\nclassifier using on the latent codes and feature maps pro-\nduced by the pretrained DINO model.\nQuantitative results. Table 1 presents quantitative results\nfor visual disambiguation evaluated on the Doppelgangers\ntest set, reported in terms of average precision (AP) and\nROC AUC score, averaged across the 16 test scenes. Our\nmethod outperforms all other baselines with an AP of 95.2%\nand ROC AUC of 93.8%. DINO produces much poorer\nresults, possibly because it generates features that are well-\nsuited for semantic classification tasks but not for visual\ndisambiguation. For feature matching methods, we find\nthat the number (or ratio) of matches alone is insufficient to\nperform well on this task. Our method can leverage not only\nthe number of matches, but also rich information about the\nspatial distribution of keypoints and matches.\nVisualizing test pairs. We show a visualization of test pairs\nalong with our network\u2019s prediction scores in Figure 4. The\ntest set covers a variety of scenes and includes different types\nof visual ambiguity, such as symmetric structures, replicas\nof landmarks, and twin buildings. The pairs on the left of\nthe figure are doppelgangers that are visually challenging\nto distinguish. Our network confidently predicts them as\nnegative pairs. On the right of the figure, we show positive\npairs with varying viewpoint and illumination. Our method\ncorrectly recognizes them as depicting the same 3D surfaces.\nFurther details on the baseline implementations, addi-\ntional comparisons with D2-Net+RANSAC [11, 12] and\nSuperPoint+SuperGlue [9, 30], more quantitative results\n(including per-scene results, confusion matrices, precision-\nrecall curves, and false positive rates), and failure cases\ndiscussion are provided in the supplemental material.\nProbability: 0.106\nProbability: 0.907\nProbability: 0.939\nProbability: 0.759\nProbability: 0.959\nProbability: 0.906\nProbability: 0.269\nProbability: 0.186\nProbability: 0.264\nProbability: 0.124\nNegative pairs\nPositive pairs\nFigure 4.\nVisual disambiguation results. We display test pairs\nand their corresponding probability of being a positive match, as\npredicted by our network. Negative pairs are shown in the left\ncolumn and positives in the right column. Note that our network\ncleanly separates the negative and positive pairs by score, including\nin the presence of varying illumination and other factors.\n6.2. Structure from motion disambiguation\nWe integrate our binary classifier into COLMAP\u2019s SfM\npipeline [32] to evaluate its use in disambiguating scenes\nwith duplicate and symmetric structures.\nBenchmark data. We evaluate our method\u2019s use on the\nSfM problem on benchmark datasets from Heinly et al. [14],\nWilson et al. [38], and Roberts et al. [28], along with several\ndatasets we collected from Flickr. Altogether, we consider\ntwo kinds of datasets: Internet photo collections of land-\nmarks (13 landmarks that are difficult to reconstruct due to\nsymmetric and repeated structures), and three non-landmark\ncollections with repeated or duplicate structures from [28]\nthat significantly differ from our training data. Our model\nis primarily trained on landmark collections, but we include\nthe non-landmark scenes from [28] to evaluate its generaliza-\ntion ability to different types of collections. Table 2 shows\nthe number of images in each scene. These scenes are not\npresent in our training data.\nIntegrating our method into SfM. SfM takes a collec-\ntion of images I = {Ii}n\ni=1 as input. A feature extrac-\ntion and matching stage first produces a set of image pairs\nwith geometrically verified matches, with these pairs de-\nnoted as P = {(Ia, Ib)|a, b \u2208 {1, . . . , NI}}. A scene graph\nG = (I, P) is then established, with images as nodes and\nimage pairs as edges. A reconstruction stage then takes G\nas input and computes camera poses for a subset of images,\nalong with a 3D point cloud.\nIllusory image pairs with repeated and symmetric struc-\ntures can produce spurious matches, leading to broken recon-\nstructions. To detect and remove the illusory pairs, we use\nour binary classifier as a filter on the edges of the scene graph.\nOur classifier outputs a probability that an image pair (Ia, Ib)\nin P is a positive pair, which we can then threshold to re-\nmove edges from the scene graph with probability lower than\na threshold \u03c4. If the classifier removes all incorrect edges\n(and only those edges), SfM can produce a disambiguated,\ncorrect reconstruction. However, if the classifier removes\ntoo few edges, the reconstruction may still be incorrect; if it\nremoves too many edges, the scene graph may break apart\nand result in several partial reconstructions. Hence, this task\nis a good test for the performance of a pairwise classifier.\nBaselines. We compare our method with several baselines,\nincluding \u201cvanilla\u201d COLMAP [32], which is a state-of-the-\nart SfM system. The default threshold for the number of\nmatches for a valid image pair in COLMAP is 15. We\nalso consider the simple baseline of setting a much higher\nthreshold of 150 matches to remove potential doppelganger\npairs. Additionally, we compare our method to four SfM\ndisambiguation methods: Heinly et al. [14] propose a post-\nprocessing method that analyzes reprojected geometry con-\nflicts between images. Wilson et al. [38] prunes bad tracks,\nwhile Cui et al. [6] and Yan et al. [40] filter out incorrect\nmatches between images prior to SfM. These methods are\nbased on heuristics that consider a collection of images at\na time. We utilize the default hyperparameters provided in\nthe GitHub implementations of [14]2 and [38]3, respectively.\nFor [6, 40], we follow the implementation provided in the\nCOLMAP disambiguation GitHub repository4, and use hy-\nperparameters tuned for the Alexander Nevsky Cathedral\nlandmark on other landmark scenes, and hyperparameters\ntuned for the Street scene on other datasets from [28]. For\nour method, we use the same probability threshold across all\nlandmarks, without tuning per scene.\nReconstruction results. The reconstruction results are sum-\nmarized in Table 2. SfM successes and failures are identified\nby checking for conflicts between the reconstruction and the\ncorresponding images or 3D mesh from Google Earth.\n2https://github.com/jheinly/sfm_duplicate_\nstructure_correction\n3https://github.com/wilsonkl/sfm-disambig\n4https://github.com/cvg/sfm-disambiguation-colmap\nImages\nCOLMAP [32]\n[32] #matches>150\nHeinly et al. [14]\nWilson et al. [38]\nCui et al. [6]\nYan et al. [40]\nOurs\nInternet landmarks\nAlexander Nevsky Cathedral [14]\n448\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\nArc de Triomphe [14]\n434\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2713\nBerliner Dom [14]\n1,618\n\u2717\n\u2713\n\u2713\n\u2717*\n\u2713\n\u2717*\n\u2713\nBig Ben [14]\n402\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\nBrandenburg Gate [14]\n175\n\u2717\n\u2713\n\u2713\n\u2013\n\u2717\n\u2717\n\u2713\nChurch on Spilled Blood [14]\n277\n\u2717\n\u2717\n\u2713\n\u2013\n\u2717\n\u2717\n\u2717\nRadcliffe camera [14]\n282\n\u2717\n\u2713\n\u2713\n\u2717*\n\u2713\n\u2713\n\u2713\nSacre Coeur [38]\n5,450\n\u2717\n\u2717\n\u2713\u2020\n\u2717*\n\u2013\n\u2717*\n\u2717\nSeville [38]\n2,396\n\u2717\n\u2713\n\u2717\n\u2713\n\u2013\n\u2717*\n\u2713\nFlorence Cathedral [Flickr]\n8,674\n\u2717\n\u2717\n\u2013\n\u2717\n\u2013\n\u2713\n\u2713\nSt. Vitus Cathedral [Flickr]\n5,059\n\u2717\n\u2717\n\u2713\n\u2713\n\u2013\n\u2717*\n\u2713\nTemple of Heaven [Flickr]\n1,538\n\u2717\n\u2717\n\u2717\n\u2717\n\u2013\n\u2717*\n\u2713\nYork Minster [Flickr]\n3,902\n\u2717\n\u2713\n\u2717\n\u2717\n\u2013\n\u2717\n\u2713\nOthers\nCereal [28]\n25\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\nCup [28]\n64\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\nStreet [28]\n19\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2717*\nNumber of scenes: \u2713/\u2717*/\u2717\n0/0/16\n5/0/11\n10/0/5\n2/3/9\n5/0/5\n7/5/4\n12/1/3\nTable 2. Structure from Motion disambiguation results. \u2713 means that a scene is correctly disambiguated and reconstructed. \u2717 means that a\nmethod fails to disambiguate the scene, and \u2717* means a scene is over-split. \u2018\u2013\u2019 means that a method fails to produce a reconstruction. [14]\nfails to generate a reconstruction for Florence Cathedral (>8k images) due to memory issues. [38] requires focal length information, which\nis unavailable for the Brandenburg Gate and Church on Spilled Blood datasets. [6] fails to produce results on large-scale scenes due to\nnumerical errors. With a single threshold, our method successfully reconstructs 12 out of 16 scenes. \u2020The Sacre Coeur result from [14] uses\na different subset of images as reported in their paper.\nOur method successfully disambiguates and reconstructs\n12 out of 16 scenes all using the same parameter settings,\nachieving the highest number of correctly reconstructed\nscenes of all methods. We also evaluated our method using\nseveral probability thresholds, and found the results to be\nrobust to the setting of the threshold (more results in sup-\nplemental). The generalization ability of our learning-based\nmethod is evident from the fact that it can be applied to new\nscenes that haven\u2019t been observed during training, without\nthe need for fine-tuning or parameter tuning.\nOur method fails on a few test scenes with the default\nthreshold, but we found our method can successfully disam-\nbiguate through threshold tuning. No method, except [14],\nreconstructs the Church on Spilled Blood correctly. How-\never, we found that with a higher score threshold, our method\ncorrectly splits this scene into sub-models (as there is insuffi-\ncient overlap in the input images to produce a single unified\nmodel). This suggests that the scene is particularly challeng-\ning to disambiguate, requiring a more stringent threshold to\nfilter out illusory image pairs. We observe a similar pattern\nwith the Sacre Coeur dataset. Our method with default set-\ntings fails on the Cereal and Street datasets from [28], which\nare quite different from our training scenes. However, we\nfound that our method can accurately reconstruct without\nover-splitting on both scenes by tuning the threshold. These\nresults imply that probabilities predicted by our model main-\ntain a reasonable ordering, with lower probabilities assigned\nto illusory image pairs and higher probabilities to positive\npairs. The need for tuning is likely due to the difference in\ndomain between our training data and the datasets from [28].\nWe show reconstructions produced by vanilla COLMAP\nBackbone\nCNN (Full)\n95.2\nTransformer\n95.5\nDataset\nw/o Augmentation\n93.6\nDesign\nw/o Masks\n64.7\nSIFT+RANSAC masks\n87.6\nw/o Alignment\n92.3\nTable 3. Ablation study of backbone selection, dataset augmenta-\ntion, and network input design. Results are reported as average\nprecision times 100.\nand those disambiguated with our method in Figure 5.\nVanilla COLMAP yields ghost structures such as extra\ntowers, domes, and facades for landmarks like Alexan-\nder Nevsky Cathedral, Berliner Dom, Church on Spilled\nBlood, Sacre Coeur, Seville, Florence Cathedral, and St. Vi-\ntus Cathedral. It produces reconstructions that collapse to\none side for landmarks like the two-way symmetric Arc de\nTriomphe and Brandenburg Gate, the four-way symmetric\nBig Ben and the tower of York Minster, the dome of Rad-\ncliffe Camera, and the highly symmetric Temple of Heaven.\nOur method can disambiguate the range of ambiguities that\nappear in these scenes, resulting in correct COLMAP recon-\nstructions.\n6.3. Ablation study\nWe conduct an ablation study to evaluate three factors:\nbackbone selection, dataset augmentation, and network input\ndesign. The results, reported as average precision scores, are\nshown in Table 3. Additional ablations on combination of\nfactors are provided in the supplemental material.\nImages\nCOLMAP\nOurs\nAlexander Nevsky Cathedral\nArc de Triomphe\nBerliner Dom\nBig Ben\nBrandenburg Gate\nChurch on Spilled Blood\nRadcliffe Camera\nCOLMAP\nOurs\nImages\nSacre Coeur\nSeville\nFlorence Cathedral\nSt. Vitus Cathedral Temple of Heaven\nYork Minster\nCereal\nCup\nStreet\nFigure 5. Visualization of Structure from Motion disambiguation results. We show a doppelganger pair at the top of each scene, vanilla\nCOLMAP reconstructions in the middle, and our method\u2019s disambiguated reconstructions at the bottom. Note that for some landmarks, the\ncorrect reconstruction is separated into multiple components when disambiguated due to a lack of camera views from sufficient viewpoints\n(shown as multiple submodels with red, blue, and green cameras). Our results are generated using the same threshold on the image match\nprobabilities, except for the Church on Spilled Blood, Cereal, and Street datasets.\nIn the Transformer experiment, we replace the CNN back-\nbone with a vision transformer; we find that this setting\nachieves comparable results, but with longer training time.\nIn the w/o Augmentation experiment, we train the classifier\non 42 scenes without flip augmentations. This setting results\nin lower performance, indicating that our flip augmentation\neffectively increases the amount of useful training data.\nThe remaining variations are trained on the dataset of 42\nscenes without augmentation for speed of training.\nKeypoint and match masks. In the w/o Masks setting,\nthe keypoint and match masks are removed from the input,\nleaving only RGB images. This significantly degrades vi-\nsual disambiguation performance, with a drop in average\nprecision from 93.6% to 64.7%, validating the importance\nof the keypoint and match masks as network inputs. In\nthe SIFT+RANSAC masks experiment, the LoFTR keypoint\nand match masks are replaced with masks computed with\nSIFT+RANSAC, leading to degraded performance due to\nthe lower quality of SIFT+RANSAC masks compared to\nthose from LoFTR. However, this version still outperforms\nother baselines.\nAlignment. In the w/o Alignment experiment, we do not\nalign the input pair, resulting in a decrease in average preci-\nsion from 93.6% to 92.3%.\n7. Conclusion\nWe tackle the visual disambiguation problem by framing\nit as a binary classification task on image pairs. We propose\na learning-based approach, and collect a new dataset, Dop-\npelgangers, which consists of visually similar image pairs\nwith binary labels. We design a classification network that\nleverages local features and matches. Our experiments show\nthat our method outperforms baselines and alternative net-\nwork designs, achieving surprisingly good performance on\nthis challenging disambiguation task. Furthermore, we inte-\ngrate our learned classifier into an SfM pipeline and show\nthat it can produce correctly disambiguated reconstructions\non difficult landmarks.\nAcknowledgements. We thank Zhengqi Li, David Fouhey,\nand Bill Freeman for their valuable discussions. This work\nwas supported in part by a Snap Research Fellowship and by\nthe National Science Foundation (IIS-2008313).\nReferences\n[1] Adrien Angeli, St\u00b4ephane Doncieux, Jean-Arcady Meyer, and\nDavid Filliat. Incremental vision-based topological slam.\nIn 2008 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 1031\u20131036. Ieee, 2008. 2\n[2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In ICCV,\npages 9650\u20139660, 2021. 4, 6, 13, 15\n[3] Ondrej Chum, James Philbin, Josef Sivic, Michael Isard, and\nAndrew Zisserman. Total Recall: Automatic query expansion\nwith a generative feature model for object retrieval. In ICCV,\n2007. 2\n[4] Ondrej Chum, James Philbin, and Andrew Zisserman. Near\nduplicate image detection: min-Hash and tf-idf weighting. In\nBMVC, 2008. 2\n[5] Thomas Cover and Peter Hart. Nearest neighbor pattern clas-\nsification. IEEE transactions on information theory, 13(1):21\u2013\n27, 1967. 12\n[6] Zhaopeng Cui and Ping Tan. Global structure-from-motion\nby similarity averaging. In ICCV, pages 864\u2013872, 2015. 2, 7,\n8, 16\n[7] Mark Cummins and Paul Newman. Appearance-only slam\nat large scale with fab-map 2.0. The International Journal of\nRobotics Research, 30(9):1100\u20131123, 2011. 2\n[8] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich.\nToward geometric deep slam.\narXiv preprint\narXiv:1707.07410, 2017. 2\n[9] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich. Superpoint: Self-supervised interest point detection\nand description. In CVPRW, pages 224\u2013236, 2018. 2, 6, 12,\n13, 15\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021. 4, 13\n[11] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-\nfeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net:\nA trainable cnn for joint detection and description of local\nfeatures. arXiv preprint arXiv:1905.03561, 2019. 2, 6, 12,\n13, 15\n[12] Martin A Fischler and Robert C Bolles. Random sample\nconsensus: a paradigm for model fitting with applications to\nimage analysis and automated cartography. Communications\nof the ACM, 24(6):381\u2013395, 1981. 5, 6, 12, 13, 15\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, pages\n770\u2013778, 2016. 5, 12\n[14] Jared Heinly, Enrique Dunn, and Jan-Michael Frahm. Correct-\ning for duplicate scene structure in sparse 3d reconstruction.\nIn ECCV, 2014. 2, 7, 8, 16\n[15] Jared Heinly, Johannes Lutz Sch\u00a8onberger, Enrique Dunn, and\nJan-Michael Frahm. Reconstructing the World* in Six Days\n*(As Captured by the Yahoo 100 Million Image Dataset). In\nCVPR, 2015. 2\n[16] Muhammad Haris Ikram, Saran Khaliq, Muhammad Latif\nAnjum, and Wajahat Hussain. Perceptual aliasing++: Adver-\nsarial attack for visual slam front-end and back-end. IEEE\nRobotics and Automation Letters, 7(2):4670\u20134677, 2022. 2\n[17] Herv\u00b4e J\u00b4egou and Ond\u02c7rej Chum. Negative evidences and\nco-occurences in image retrieval: The benefit of pca and\nwhitening. In ECCV, 2012. 2\n[18] Nianjuan Jiang, Ping Tan, and Loong-Fah Cheong. Seeing\ndouble without confusion: Structure-from-motion in highly\nambiguous scenes. In CVPR, pages 1458\u20131465. IEEE, 2012.\n2\n[19] Rajbir Kataria, Joseph DeGol, and Derek Hoiem. Improving\nstructure from motion with reliable resectioning. In 2020\ninternational conference on 3D vision (3DV), pages 41\u201350.\nIEEE, 2020. 2\n[20] Pierre-Yves Lajoie, Siyi Hu, Giovanni Beltrame, and Luca\nCarlone. Modeling perceptual aliasing in slam via discrete\u2013\ncontinuous graphical models. IEEE Robotics and Automation\nLetters, 4(2):1232\u20131239, 2019. 2\n[21] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. In CVPR, pages\n2041\u20132050, 2018. 4, 5, 13\n[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll\u00b4ar. Focal loss for dense object detection. In ICCV,\npages 2980\u20132988, 2017. 5\n[23] Zhiqiu Lin, Jin Sun, Abe Davis, and Noah Snavely. Visual\nchirality. In CVPR, pages 12295\u201312303, 2020. 4\n[24] David G Lowe.\nDistinctive image features from scale-\ninvariant keypoints. IJCV, 60:91\u2013110, 2004. 2, 6, 13, 15\n[25] D. Nister and H. Stewenius. Scalable recognition with a\nvocabulary tree. In CVPR, 2006. 2\n[26] Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi.\nLf-net: Learning local features from images. NeurIPS, 31,\n2018. 2\n[27] Jerome Revaud, Philippe Weinzaepfel, C\u00b4esar De Souza, Noe\nPion, Gabriela Csurka, Yohann Cabon, and Martin Humen-\nberger. R2d2: repeatable and reliable detector and descriptor.\narXiv preprint arXiv:1906.06195, 2019. 2\n[28] Richard Roberts, Sudipta N Sinha, Richard Szeliski, and\nDrew Steedly. Structure from motion for scenes with large\nduplicate structures. In CVPR, pages 3137\u20133144. IEEE, 2011.\n2, 7, 8\n[29] Ragav Sachdeva and Andrew Zisserman. The change you\nwant to see. In WACV, 2023. 2\n[30] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich. Superglue: Learning feature match-\ning with graph neural networks. In CVPR, pages 4938\u20134947,\n2020. 2, 6, 12, 13, 15\n[31] Frederik Schaffalitzky and Andrew Zisserman. Multi-view\nmatching for unordered image sets, or \u201chow do i organize my\nholiday snaps?\u201d. In ECCV, pages 414\u2013431. Springer, 2002. 2\n[32] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In CVPR, pages 4104\u20134113, 2016. 2,\n4, 7, 8, 12, 13, 16\n[33] Josef Sivic and Andrew Zisserman. Video Google: a text\nretrieval approach to object matching in videos. In ICCV,\n2003. 2\n[34] Noah Snavely, Steven M Seitz, and Richard Szeliski. Photo\ntourism: exploring photo collections in 3d. In ACM siggraph\n2006 papers, pages 835\u2013846. 2006. 2\n[35] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. Loftr: Detector-free local feature matching\nwith transformers. In CVPR, pages 8922\u20138931, 2021. 5, 6,\n12, 13, 15\n[36] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\nGoogle landmarks dataset v2 - a large-scale benchmark for\ninstance-level recognition and retrieval. In CVPR, 2020. 3\n[37] Tobias Weyand, Chih-Yun Tsai, and Bastian Leibe. Fixing\nwtfs: Detecting image matches caused by watermarks, times-\ntamps, and frames in internet photos. In WACV, 2015. 2\n[38] Kyle Wilson and Noah Snavely. Network principles for sfm:\nDisambiguating repeated structures with local context. In\nICCV, pages 513\u2013520, 2013. 2, 7, 8, 16\n[39] Xiaoshi Wu, Hadar Averbuch-Elor, Jin Sun, and Noah\nSnavely. Towers of babel: Combining images, language,\nand 3d geometry for learning multimodal vision. In ICCV,\npages 428\u2013437, 2021. 3, 4\n[40] Qingan Yan, Long Yang, Ling Zhang, and Chunxia Xiao.\nDistinguishing the indistinguishable: Exploring structural\nambiguities via geodesic context. In CVPR, pages 3836\u20133844,\n2017. 2, 7, 8, 16\n[41] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal\nFua. Lift: Learned invariant feature transform. In ECCV,\npages 467\u2013483. Springer, 2016. 2\n[42] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,\nMathieu Salzmann, and Pascal Fua. Learning to find good\ncorrespondences. In CVPR, pages 2666\u20132674, 2018. 2\n[43] Christopher Zach, Arnold Irschara, and Horst Bischof. What\ncan missing correspondences tell us about 3d structure and\nmotion? In CVPR, pages 1\u20138. IEEE, 2008. 2\n[44] Christopher Zach, Manfred Klopschitz, and Marc Pollefeys.\nDisambiguating visual relations using loop constraints. In\n2010 IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition, pages 1426\u20131433. IEEE, 2010. 2\nA. The Doppelgangers dataset\nA.1. Data collection process\nThe process of creating image pairs with ground truth\nlabels posed several challenges, including the difficulty of\nfinding potential doppelgangers (as described in the main\npaper) and dealing with erroneously categorized images on\nWikimedia Commons. Such images can lead to incorrect\nlabels for image pairs that include them, which can affect\nthe quality of our dataset. To address this issue, we propose\nto use a K-NN (K-Nearest Neighbor) algorithm to identify\nthose images and ensure that Doppelgangers dataset com-\nprises high-quality image pairs of similar structures with\naccurate labels.\nIdentifying incorrectly categorized images. While we\nfind the most Wikimedia Commons images are correctly\ncategorized, we found that some images are uploaded to the\nwrong subcategory, perhaps because people can themselves\nbe confused about what side of a symmetric building they are\nlooking at. Unfortunately, even a single incorrectly labeled\nimage can lead to a large number of incorrect negative pairs\nthat have many feature matches (because in reality they\nshould be positive pairs). Therefore, to avoid noisy labels in\nour dataset, we must identify and remove such incorrectly\ncategorized images. For this, we look at the scene graph\ncomputed by COLMAP [32] and remove images whose label\nis different from other images with a similar connectivity\npattern in the scene graph.\nSpecifically, we use the K-NN (K Nearest Neighbor) al-\ngorithm [5] to identify such images, based on the similarity\nof connectivity computed from the scene graph. First, we\nconstruct an adjacency matrix A where each element A(i, j)\nrepresents the number of matches between image i and im-\nage j. Next, we normalize the connectivity vector for each\nimage to a unit vector, where the connectivity vector of\nimage i is the ith row vector of adjacency matrix A. We cal-\nculate the similarity of connectivity between any two images\nas the dot product of their respective connectivity vectors.\nSuspicious images are identified as those with different la-\nbels from their neighbors, and we remove pairs containing\nsuch images from our dataset.\nA.2. Dataset Statistics\nTable 4 and Table 5 provide additional statistics on the\nDoppelgangers training and test sets. The tables list the\ntest scenes and training scenes that naturally form negative\npairs, along with the average and the 95th percentile number\nof matches per scene. Our dataset includes a variety of\nlandmarks, such as cathedrals, museums, castles, and other\nnotable structures. The exteriors of these landmarks exhibit\nrepeated and symmetric patterns. Most scenes in both the\ntraining and test sets average more than 50 matches.\nB. Visual disambiguation\nB.1. Implementation details of our method\nKeypoint and match masks. Given a pair of images, we\nresize and pad them to a resolution of 1024 \u00d7 1024. We\nthen use LoFTR [35], a learning-based feature matching\nmethod, to match the image pair. LoFTR produces matches\nand scores for each match. We filter out weak matches by\napplying a threshold of 0.8 to the scores. To further refine\nmatches, we perform geometric verification by estimating\nthe fundamental matrix using RANSAC [12] with a reprojec-\ntion error of 3 and a confidence level of 0.99. For this step,\nwe use the publicly available OpenCV implementation. We\nuse all the output matches to establish keypoint masks, and\nthe geometrically verified matches to establish match masks.\nInput alignment. After obtaining the keypoints and matches,\nwe estimate an affine transformation matrix using the\nOpenCV implementation of RANSAC with a inlier error\nof 20 pixels. We set a larger threshold, which means that an\naffine transform will only roughly fit the data, because we\nneed a more tolerant threshold to have enough inliers to fit\na transform at all. We use the estimated affine transforma-\ntion matrix to align the images, keypoint masks, and match\nmasks.\nNetwork architecture. Our network architecture and pa-\nrameter settings are similar to ResNet-18 [13], but we use\nthree residual blocks with channel dimensions of 128, 256,\nand 512. After the average pooling layer, the last fully con-\nnected layer takes a 512-dimensional input and outputs a\n2-dimensional vector. We then apply softmax to the vector\nto obtain probabilities.\nTraining. We train our network for 10 epochs using a batch\nsize of 8 with two NVIDIA GeForce RTX 2080 Ti GPUs.\nThe training process took approximately 9 hours for the 42\nscenes and 30 hours for all scenes with image flipping aug-\nmentation. For optimization, we used the Adam optimizer\nwith parameters \u03b21 = 0.9 and \u03b22 = 0.999, an initial learn-\ning rate of 5 \u00d7 10\u22124, and linearly decayed the learning rate\nstarting at epoch 5 until it reaches 5 \u00d7 10\u22126 at epoch 10.\nB.2. Implementation details of baselines\nWe first provide additional details about the baselines eval-\nuated in the main paper, then describe additional baselines\nprovided in this supplemental material. We also evaluate\ntwo additional baselines, D2-Net [11]+RANSAC [12] and\nSuperPoint [9]+SuperGlue [30], with results provided in\nSection B.3. With these two additional baselines, we cover a\nlarge variety of feature matching methods, including classi-\ncal feature detectors such as SIFT and learning-based feature\ndetectors such as D2-Net and SuperPoint. We also include\ntraditional matching methods using nearest neighbor and\nTraining scene\nMean\n95%\nAleppo Citadel\n90\n313\nAlmudena Cathedral\n81\n298\nArc de Triomphe du Carrousel\n88\n317\nBrooklyn Bridge\n47\n152\nCh\u02c6ateau de Chambord\n96\n344\nCh\u02c6ateau de Cheverny\n75\n284\nCh\u02c6ateau de Sceaux\n134\n629\nCinderella Castle\n148\n721\nCour Carr\u00b4ee (Louvre)\n129\n475\nCour Napol\u00b4eon\n94\n295\nDa Lat Station\n84\n253\n\u00b4Eglise de la Madeleine\n119\n390\nEiffel Tower\n62\n197\nEl Escorial\n132\n452\nGrande Galerie (Louvre)\n99\n324\nGrands Guichets du Louvre\n140\n321\nLiberty Square, Taipei\n65\n197\nLondon Eye\n56\n147\nMainz Cathedral\n116\n234\nMarket Square in Wroc\u0142aw\n118\n429\nNotre-Dame de Fourvi`ere\n92\n318\nNotre-Dame de Paris\n174\n730\nNotre-Dame de Paris (Interior)\n299\n976\nNotre-Dame de Strasbourg\n122\n437\nOp\u00b4era Garnier\n91\n332\nPatio de los Arrayanes\n107\n328\nPatio de los Leones\n126\n306\nPavillion de Flore (Louvre)\n63\n212\nPont Alexandre III\n55\n131\nPont des Arts\n74\n157\nSaint-Martin, Colmar\n161\n692\nSalzburg Cathedral\n98\n303\nSt. Mark\u2019s Basilica\n79\n328\nSt. Paul\u2019s Cathedral\n82\n321\nStatue of Liberty\n38\n113\nSukiennice\n55\n186\nTaj Mahal\n68\n228\nTorre de Bel\u00b4em\n125\n457\nUmayyad Mosque (Courtyard)\n76\n252\nWhite House\n70\n186\nTable 4. Landmarks in the Doppelgangers training set. We present\nthe average and 95th percentile number of matches per scene.\nRANSAC algorithms, as well as a learning-based matching\nmethod (SuperGlue). In addition, we evaluate detector-based\nfeature matching methods and detector-free feature match-\ning methods, such as LoFTR. Note that all local feature\nmatching baselines are used as classifiers on image pairs by\nthresholding either the number of matches, or the ratio of\nnumber of matches to number of keypoints.\nTest scene\nMean\n95%\nAlexander Nevsky Cathedral, L\u00b4odz\n47\n115\nAlexander Nevsky Cathedral, Pre\u02c7sov\n62\n231\nAlexander Nevsky Cathedral, Sofia\n87\n244\nAlexander Nevsky Cathedral, Tallinn\n53\n162\nArc de Triomphe de l\u2019\u00b4Etoile\n100\n387\nBerlin Cathedral\n77\n372\nBrandenburg Gate\n36\n95\nCathedral of St. Peter and Paul, Brno\n283\n1458\nCharlottenburg Palace\n40\n104\nChurch of the Saviour on the Blood\n53\n195\nDeutscher and Franz\u00a8osischer Dom\n67\n139\nFlorence Cathedral\n116\n340\nSleeping Beauty Castle\n37\n112\nSt. Vitus Cathedral\n138\n666\nSydney Harbour Bridge\n29\n104\nWashington Square Arch\n70\n206\nTable 5. Landmarks in the Doppelgangers test set. We present the\naverage and 95th percentile number of matches per scene.\nSIFT [24]+RANSAC [12]. We use the COLMAP [32] fea-\nture extraction and matching modules to produce keypoints\nand matches, using the default parameters. This includes the\nmaximum extracted features set to 8192, use of cross check\nfor matching, and geometric verification with a reprojection\nerror of 4 and confidence level of 0.999.\nLoFTR [35]. We follow the same process as previously\ndescribed to use LoFTR to obtain matches for our network\ninput.\nDINO [2]. We use the pretrained ViT [10] small version\nmodel with a patch size of 16. We pass one image at a time\nto DINO and obtain the latent code and feature maps from\nthe last layer. We then train a linear classifier by taking\nthe concatenated latent codes of images in a pair as input\nto a fully connected layer and outputting the probability.\nFor the feature maps, we concatenate them and pass them\nthrough a residual layer and fully connected layer to obtain\nthe prediction.\nD2-Net [11]+RANSAC [12]. D2-Net is a learning-based\nmethod for feature detector and descriptor. We use its pre-\ntraine model on MegaDepth [21] to extract keypoints and\ndescriptors. We then use the OpenCV implementation of the\nbrute force k-nearest neighbor matcher with cross-check set-\nting, and apply a ratio test with a threshold of 0.75. Finally,\nwe perform geometric verification with the same settings as\npreviously described.\nSuperPoint [9]+SuperGlue [30]. SuperPoint is an efficient\nlearning-based method for detecting and describing key-\npoints. Given the keypoints and descriptors extracted from\nSuperPoint, we use SuperGlue to obtain matches, where Su-\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nSIFT+RANSAC #matches\nSIFT+RANSAC %matches\nLoFTR #matches\nLoFTR %matches\nDINO - Latent code\nDINO - Feature map\nOurs\nFigure 6.\nPrecision-Recall (PR) curves on the Doppelgangers\ntest set. The x-axis represents recall and the y-axis represents\nprecision. A curve approaching the top-right corner indicates better\nperformance.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue positive rate\nSIFT+RANSAC #matches\nSIFT+RANSAC %matches\nLoFTR #matches\nLoFTR %matches\nDINO - Latent code\nDINO - Feature map\nOurs\nFigure 7. Receiver operating characteristic (ROC) curves on the\nDoppelgangers test set. The x-axis represents the false positive rate,\nand the y-axis represents the true positive rate. The ideal method\nwould simultaneously have a lower false positive rate and higher\ntrue positive rate, with the curve approaching the top-left corner.\nperGlue is a learning-based approach for feature matching\nusing a graph neural network (GNN). We use the checkpoint\ntrained for outdoor scenes with the recommended settings\nfor SuperGlue, including a maximum number of keypoints\nset to 2048 and a Non-Maximum Suppression (NMS) radius\nof 3.\nB.3. Quantitative results and analysis\nWe present additional comparisons of our method with\nbaselines on the Doppelgangers test set, and report the av-\nerage precision (AP) and ROC AUC scores in Tables 6 and\n7, respectively. Both the AP and ROC AUC scores evalu-\nate classification performance, and higher scores are better.\nWhile AP is more focused on positive pairs, ROC AUC is\nmore focused on the ranking of predictions and cares equally\nabout positive and negative pairs. Our method outperforms\n20\n40\n60\n80\n100\n120\n140\n160\nNumber of matches (SIFT+RANSAC)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted probability\n0\n100\n200\n300\n400\n500\n600\nNumber of matches (LoFTR)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted probability\nFigure 8. Correlation between predicted probability of our network\nand number of matches using SIFT+RANSAC (top figure) and\nLoFTR (bottom figure) for pairs in all test scenes. The x-axis repre-\nsents the number of matches and the y-axis represents the predicted\nprobability. Blue dots represent ground truth positive pairs and red\ndots represent ground truth negative pairs. Our method produces a\nhigh probability for most positive pairs and a low probability for\nmost negative pairs. Even for challenging doppelganger pairs, our\nmethod can produce a probability of less than 0.85. In contrast,\nSIFT and LoFTR methods have lots of positive pairs with low num-\nbers of matches, as well as a number of negative pairs with large\nnumbers of matches.\nall other baselines for 15 out of 16 test landmarks, with an\naverage precision (AP) of 95.2% and an ROC AUC of 93.8%\nacross all landmarks. The SuperPoint+SuperGlue method\nachieves comparable results to SIFT+RANSAC, while D2-\nNet performs worse and similarly to DINO. These results\nsuggest that the presence of the number or ratio of matches\nAverage Precision\nD2-Net [11]+RANSAC [12]\nSuperPoint [9]+SuperGlue [30]\nSIFT [24]+RANSAC [12]\nLoFTR [35]\nDINO [2]-ViT\nOurs\n#matches\n%matches\n#matches\n%matches\n#matches\n%matches\n#matches\n%matches\nLatent code\nFeature map\nAverage of all pairs from 16 landmarks\n62.3\n62.5\n79.6\n80.7\n83.4\n81.2\n85.3\n86.0\n62.0\n63.3\n95.2\nAlexander Nevsky Cathedral, \u0141\u00b4od\u00b4z\n62.1\n63.7\n83.7\n83.8\n72.7\n75.9\n80.7\n80.4\n50.9\n50.3\n89.5\nAlexander Nevsky Cathedral, Sofia\n63.6\n63.7\n80.6\n80.7\n89.5\n87.6\n90.0\n92.2\n53.0\n53.6\n98.5\nAlexander Nevsky Cathedral, Tallinn\n64.1\n64.4\n73.9\n74.3\n73.1\n76.0\n76.1\n80.3\n58.8\n50.8\n86.2\nArc de Triomphe\n49.7\n48.8\n60.1\n60.9\n86.1\n81.7\n85.7\n93.3\n55.4\n61.1\n97.6\nBerlin Cathedral\n69.0\n69.7\n94.4\n94.6\n91.8\n91.6\n93.6\n92.7\n76.4\n70.6\n99.4\nBrandenburg Gate\n42.1\n42.8\n60.1\n62.6\n79.3\n73.7\n90.9\n95.6\n60.8\n60.9\n99.8\nCathedral of Saints Peter and Paul in Brno\n75.0\n74.6\n93.1\n93.1\n95.8\n96.4\n89.8\n88.4\n64.6\n79.9\n99.8\nCathedral of St Alexander Nevsky, Pre\u02c7sov\n73.2\n74.5\n89.8\n89.8\n82.5\n74.0\n86.1\n85.3\n62.9\n64.8\n94.6\nCharlottenburg Palace\n62.0\n60.6\n81.4\n82.3\n81.5\n76.1\n85.6\n81.1\n65.8\n54.1\n93.3\nChurch of Savior on the Spilled Blood\n62.1\n61.0\n86.7\n86.2\n82.1\n73.2\n84.9\n75.5\n63.9\n67.5\n93.8\nDeutscher und Franz\u00a8osischer Dom (Berlin)\n53.9\n54.6\n75.4\n75.9\n74.5\n71.9\n85.8\n84.2\n55.6\n51.5\n98.1\nFlorence Cathedral\n60.1\n58.0\n82.7\n82.8\n90.6\n83.8\n84.5\n82.0\n54.6\n63.8\n94.2\nSleeping Beauty Castle\n54.4\n56.8\n71.0\n81.1\n81.1\n81.2\n75.0\n85.6\n67.2\n66.4\n97.1\nSt. Vitus Cathedral\n68.8\n67.6\n91.7\n91.0\n96.8\n88.0\n89.2\n87.5\n84.0\n77.0\n99.8\nSydney Harbour Bridge\n73.5\n77.3\n83.6\n86.8\n79.4\n92.3\n83.8\n86.2\n53.0\n75.5\n87.0\nWashington Square Arch\n63.6\n62.4\n65.0\n65.5\n77.7\n75.9\n82.8\n86.0\n65.2\n65.0\n95.1\nTable 6. Quantitative results for visual disambiguation evaluated on Doppelgangers. Results are reported as the average precision (AP)\nmultiplied by 100. We report both the average and the per-scene results for 16 landmarks.\nROC AUC\nD2-Net [11]+RANSAC [12]\nSuperPoint [9]+SuperGlue [30]\nSIFT [24]+RANSAC [12]\nLoFTR [35]\nDINO [2]-ViT\nOurs\n#matches\n%matches\n#matches\n%matches\n#matches\n%matches\n#matches\n%matches\nLatent code\nFeature map\nAverage of all pairs from 16 landmarks\n53.5\n53.7\n76.8\n76.9\n80.2\n77.1\n78.9\n80.3\n60.9\n61.5\n93.8\nAlexander Nevsky Cathedral, \u0141\u00b4od\u00b4z\n58.5\n60.1\n78.7\n78.7\n69.7\n72.7\n73.9\n74.8\n49.2\n49.7\n87.0\nAlexander Nevsky Cathedral, Sofia\n57.1\n57.7\n82.5\n82.5\n87.7\n84.3\n86.2\n89.1\n53.8\n49.3\n98.0\nAlexander Nevsky Cathedral, Tallinn\n59.2\n60.0\n71.8\n71.9\n68.0\n71.7\n71.9\n74.5\n60.8\n52.2\n84.2\nArc de Triomphe\n39.8\n38.5\n44.7\n44.7\n81.6\n75.3\n78.5\n88.9\n53.7\n57.1\n96.9\nBerlin Cathedral\n54.8\n56.1\n92.1\n92.2\n89.2\n88.6\n89.4\n88.1\n71.6\n67.7\n99.3\nBrandenburg Gate\n33.7\n35.2\n64.3\n64.5\n77.9\n71.9\n87.5\n93.4\n60.7\n60.4\n99.8\nCathedral of Saints Peter and Paul in Brno\n61.2\n60.4\n89.6\n89.6\n94.0\n95.0\n84.4\n82.8\n62.7\n75.8\n99.8\nCathedral of St Alexander Nevsky, Pre\u02c7sov\n62.3\n64.8\n87.4\n87.4\n77.0\n63.9\n77.4\n77.6\n68.9\n60.6\n92.4\nCharlottenburg Palace\n52.0\n50.5\n78.0\n78.2\n76.7\n70.7\n80.2\n77.1\n65.9\n53.6\n92.2\nChurch of Savior on the Spilled Blood\n50.2\n49.2\n77.5\n77.3\n77.7\n68.4\n78.6\n70.4\n61.5\n64.0\n92.5\nDeutscher und Franz\u00a8osischer Dom (Berlin)\n51.8\n52.1\n79.2\n79.2\n70.7\n68.2\n80.4\n78.7\n58.7\n49.2\n97.6\nFlorence Cathedral\n52.7\n49.5\n78.2\n78.2\n88.7\n80.3\n74.6\n71.9\n51.3\n62.5\n92.5\nSleeping Beauty Castle\n48.7\n52.7\n77.0\n77.5\n76.8\n79.4\n64.7\n78.4\n64.7\n66.5\n96.0\nSt. Vitus Cathedral\n49.6\n47.4\n82.4\n82.4\n96.7\n82.5\n82.3\n80.3\n80.4\n80.5\n99.8\nSydney Harbour Bridge\n69.4\n71.8\n86.3\n86.3\n76.3\n91.7\n77.4\n79.0\n50.2\n72.4\n80.0\nWashington Square Arch\n55.9\n53.9\n59.6\n59.6\n73.9\n69.3\n74.7\n79.2\n59.9\n63.2\n93.5\nTable 7. Quantitative results for visual disambiguation evaluated on Doppelgangers. Results are reported as ROC AUC multiplied by 100.\nWe report both the average and the per-scene results for 16 landmarks.\nNegative\nPositive\nPredicted label\nNegative\nPositive\nTrue label\n2118\n210\n353\n1975\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nFigure 9. Confusion matrix for our method with probability thresh-\nold set to 0.5. Our method correctly identifies 1,975 true positive\npairs and 2,118 true negatives, while producing 353 false positive\npairs and 210 false negatives. Overall, the method achieves an\naccuracy of approximately 88%.\nis not necessarily the best indicator of whether two images\ntruly match.\nWe also present evaluation results as the precision-recall\n(PR) curves shown in Figure 6, where our method shows sig-\nnificant improvements over the baselines. We also provide\nreceiver operating characteristic (ROC) curves in Figure 7.\nROC curves illustrate the performance of classifiers across\nvarious classification thresholds. Our model consistently\noutperforms other methods across all thresholds, with the\nlowest false positive rate and highest true positive rate. Ad-\nditionally, in Figure 9, we show the confusion matrix of\nour network predictions using a threshold of 0.5, indicating\nthat our method can correctly classify approximately 88%\nof image pairs in the test set at this threshold.\nTo analyze the correlation between our network\u2019s pre-\ndictions and the number of matches in the input pair, we\ngenerate 2D scatter plots where the x-axis is the number of\nmatches and the y-axis is the probability predicted by our net-\nwork. The resulting scatter plots are shown in Figure 8, using\nSIFT+RANSAC and LoFTR methods to compute matches,\nrespectively. In the figure, red dots represent pairs with a\nground truth label of negative, while blue dots represent posi-\ntive pairs. The figure shows that our method can differentiate\nImages\nCOLMAP [32]\n[32] #matches>150\nHeinly et al. [14]\nWilson et al. [38]\nCui et al. [6]\nYan et al. [40]\nOurs\n@0.5\n@0.6\n@0.7\n@0.8\n@0.9\n@0.97\nAlexander Nevsky Cathedral [14]\n448\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nArc de Triomphe [14]\n434\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nBerliner Dom [14]\n1,618\n\u2717\n\u2713\n\u2713\n\u2717*\n\u2713\n\u2717*\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717*\nBig Ben [14]\n402\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nBrandenburg Gate [14]\n175\n\u2717\n\u2713\n\u2713\n\u2013\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nChurch on the spilled blood [14]\n277\n\u2717\n\u2717\n\u2713\n\u2013\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\nRadcliffe camera [14]\n282\n\u2717\n\u2713\n\u2713\n\u2717*\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nNumber of scenes: \u2713/\u2717*/\u2717\n0/0/7\n3/0/4\n7/0/0\n0/2/3\n4/0/3\n3/1/3\n4/0/3\n6/0/1\n6/0/1\n6/0/1\n6/0/1\n6/1/0\nTable 8. Robustness evaluation of our method to the probability threshold on SfM disambiguation results. \u2713 means correctly disambiguate\nand reconstruct. \u2717 means fail to disambiguate and \u2717* means over-split. Our method exhibits robustness to the probability threshold and\nsuccessfully reconstructed 6 out of 7 scenes with probability thresholds ranging from 0.6 to 0.97.\nFull\n95.2\nw/o Augmentation\n93.6\nw/o Masks\n64.7\nw/o RGB\n90.0\nw/o Geo. verification\n92.1\nTable 9. Additional ablation study on network input design. The\nresults are reported as the average precision multiplied by 100.\nbetween positive and negative image pairs, in particular in\ncases when such pairs have the same number of matches.\nAlthough differentiating doppelganger pairs with larger num-\nbers of matches can be more challenging (red dots at top\nright of figure), our method still predicts a probability lower\nthan 0.8 for most negative pairs.\nB.4. Additional ablation study\nWe conduct an additional ablation study on the design\nof network input. The results, reported as average precision\nscores, are shown in Table 9. As described in the main paper,\nwe conduct a w/o Augmentation experiment where we train\nthe classifier on 44 scenes without flip augmentations. The\nremaining variations are trained on the same dataset of 44\nscenes without augmentation for speed of training.\nIn the w/o Masks setting, we remove keypoint and match\nmasks from input, leaving only RGB images. This results\nin significant degeneration of performance. In the w/o RGB\nexperiment, we remove RGB images from the input, leaving\nonly keypoint and match masks. This leads to a drop in\naverage precision from 93.6% to 90.0%. This drop is not as\nsignificant as that stemming from removal of keypoint and\nmatch masks, indicating the relative importance of these in-\nputs. The w/o Geo. verification setting is one where matches\nare not filtered and verified with Fundamental matrix esti-\nmation using RANSAC, resulting in a decrease in average\nprevision from 93.6% to 92.1%. In summary, the ablation\nstudy demonstrates that keypoint and match masks are essen-\ntial components of input for visual disambiguation, as they\ncontain rich information and cues for differentiating visually\nsimilar pairs.\nB.5. Additional qualitative results\nIn Figure 10, we provide additional visualizations of test\nimage pairs and their corresponding predicted probability by\nour method on a variety of test scenes.\nWe visualize some failure cases in Figure 11, all of which\nare negative pairs. We circle potentially useful regions for vi-\nsual disambiguation in red. The pair from Alexander Nevsky\nCathedral in Tallinn has distinct regions on the facades that\nare difficult to observe due to the viewpoint. Given other\nregions and structures of the building appear similar, it is\nchallenging even for humans to differentiate between the\nimages. In the pair from Charlottenburg Palace, the second\nimage is a zoom-in view that crops out other regions, leaving\nonly a small region on the golden sculpture (at the top of the\nbuilding) that can serve as a cue for visual disambiguation.\nIn the third pair from Washington Square Arch, the illumina-\ntion differences might mask the structural differences (which\nare in shadow in the second image), making it more difficult\nto discern the differences between regions. The replicas of\nSleeping Beauty Castles look very similar, as shown in the\nlast pair of images. Images captured at night can be more\nchallenging to distinguish, since the background is obscured\nand important cues may be lost due to lack of observability\nin the background.\nC. Structure from Motion disambiguation\nC.1. Threshold robustness evaluation\nWe evaluate the robustness of our method for disambiguat-\ning SfM reconstructions to the probability threshold, and we\nshow additional results on 7 landmark datasets from Heinly\net al. [14] with thresholds at [0.5, 0.6, 0.7, 0.8, 0.9, 0.97]\nin Table 8. At the threshold of 0.5, some incorrect pairs\nare included in the scene graph, resulting in broken recon-\nstructions for Brandenburg Gate, Church on Spilled Blood,\nand Radcliffe Camera. For the SfM disambiguation setting,\nwhere a single bad matching pair can break a model, we care\nmore about false positives than keeping all positive pairs\n(i.e., we care more about precision than recall). Therefore\nsetting the threshold to 0.5 may intuitively not be the best\nstrategy, hence the better performance at higher thresholds\nthat filter out more pairs. For thresholds ranging from 0.6 to\n0.9, our method is robust, and successfully disambiguates\nand reconstructs 6 out of 7 scenes. At even higher thresh-\nolds, we see that one of the models (Berliner Dom) splits\napart, resulting in over-splitting of the reconstruction, but\nat this strict threshold we can successfully disambiguate the\nfinal scene (Church on Spilled Blood). Overall, our method\nis able to reconstruct 6 out of 7 scenes even at this thresh-\nold, demonstrating the robustness and effectiveness of our\napproach.\nC.2. Detailed reconstruction visualization\nWe present a detailed visualization of the reconstruction\nresults for 7 scenes rendered from different viewpoints in\nFigure 12, comparing our method with vanilla COLMAP\nreconstruction. The visualizations from different viewpoints\nprovide a clear view of the incorrect structures produced\nby COLMAP, such as the double towers in the Alexander\nNevsky Cathedral and the missing sides of Big Ben. Our\nmethod can disambiguate different sides of these highly\nsymmetric landmarks and produce a complete and correct\nreconstruction.\nProbability: 0.068\nProbability: 0.122\nProbability: 0.195\nProbability: 0.480\nProbability: 0.197\nProbability: 0.059\nProbability: 0.087\nProbability: 0.151\nProbability: 0.099\nProbability: 0.182\nProbability: 0.065\nProbability: 0.917\nProbability: 0.940\nProbability: 0.990\nProbability: 0.929\nProbability: 0.997\nProbability: 0.997\nProbability: 0.964\nProbability: 0.925\nProbability: 0.928\nProbability: 0.904\nProbability: 0.904\nNegative pairs\nPositive pairs\nFigure 10. Additional visual disambiguation results. We visualize test\nimage pairs with their corresponding predicted probabilities produced\nby our network. The left column shows negative pairs and the right\ncolumn shows positive pairs.\nProbability: 0.756\nCharlottenburg Palace\nProbability: 0.762\nSleeping Beauty Castles\nProbability: 0.752\nWashington Square Arch\nProbability: 0.838\nAlexander Nevsky Cathedral in Tallinn \nFigure 11. Failure cases. We visualize challenging doppelgangers\npairs that are all negative pairs, but the predicted probabilities by our\nnetwork are high. We circle the regions that might be helpful for\ndisambiguation in red. For the last pair from Sleeping Beauty Castles,\nwe show zoomed-in views of distinct regions with red boxes.\nAlexander Nevsky Cathedral\nTop\n45\u00b0\n135\u00b0\n225\u00b0\n315\u00b0\nCOLMAP\nOurs\nImages\nCOLMAP\nOurs\nArc de Triomphe\nImages\nTop\n45\u00b0\n135\u00b0\n225\u00b0\n315\u00b0\nCOLMAP\nOurs\nBerliner Dom\nImages\nTop\n45\u00b0\n135\u00b0\n225\u00b0\n315\u00b0\nCOLMAP\nOurs\nBig Ben\nImages\nTop\n45\u00b0\n315\u00b0\nCOLMAP\nOurs\nBrandenburg Gate\nImages\nTop\nFront\nBack\nCOLMAP\nOurs\nChurch of the Savior on Spilled Blood\nImages\nTop\nLeft\nRight\nCOLMAP\nOurs\nRadcliffe Camera\nImages\nTop\nFront\nBack\nFigure 12. Visualization of Structure from Motion (SfM) disambiguation results from different viewpoints. We show a set of input RGB\nimages at the top of each example scene, vanilla COLMAP reconstructions in the middle, and our method\u2019s disambiguated reconstructions\nat the bottom. For reconstructions where an angle is denoted, the 0\u00b0 mark begins at the bottom of the birds-eye view and increases\ncounterclockwise about the center of the image. Note that for some landmarks, the correct reconstruction is separated into two components\nwhen disambiguated due to a lack of camera views from sufficient viewpoints.\n"
  },
  {
    "title": "StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation",
    "link": "https://arxiv.org/pdf/2309.01770.pdf",
    "upvote": "8",
    "text": "StyleAdapter: A Single-Pass LoRA-Free Model for\nStylized Image Generation\nZhouxia Wang1\u2217\nXintao Wang2 \u2020\nLiangbin Xie2,3,4\nZhongang Qi2\nYing Shan2\nWenping Wang1\nPing Luo1,5 \u2020\n1 The University of Hong Kong\n2 ARC Lab, Tencent PCG\n3 University of Macau\n4 Shenzhen Institute of Advanced Technology\n5 Shanghai AI Laboratory\nAbstract\nThis paper presents a LoRA-free method for stylized image generation that takes a\ntext prompt and style reference images as inputs and produces an output image in a\nsingle pass. Unlike existing methods that rely on training a separate LoRA for each\nstyle, our method can adapt to various styles with a unified model. However, this\nposes two challenges: 1) the prompt loses controllability over the generated content,\nand 2) the output image inherits both the semantic and style features of the style\nreference image, compromising its content fidelity. To address these challenges,\nwe introduce StyleAdapter, a model that comprises two components: a two-path\ncross-attention module (TPCA) and three decoupling strategies. These components\nenable our model to process the prompt and style reference features separately\nand reduce the strong coupling between the semantic and style information in\nthe style references. StyleAdapter can generate high-quality images that match\nthe content of the prompts and adopt the style of the references (even for unseen\nstyles) in a single pass, which is more flexible and efficient than previous methods.\nExperiments have been conducted to demonstrate the superiority of our method\nover previous works.\n1\nIntroduction\nRecent advancements in data and large models have facilitated the impressive strides made in\ntext-to-image (T2I) generation[5, 20, 22, 23, 25, 28, 43]. They can generate high-quality images\nbased on provided prompts and descriptions. T2I methods can also incorporate a specific style into\nthe generated images by taking a textual description of the style as a prompt. However, textual\ndescriptions are often less expressive and informative than visual representations of styles, and thus\nthe style features of the T2I outputs are often coarse and lack details. To utilize the rich information\nin the visual data of a style, previous works [8, 40] propose textual inversion methods that map the\nvisual representation of style to textual space, allowing the extraction of style information from visual\nimages to guide the T2I models. However, these methods still suffer from the limitation that the\nvisual-to-textual projection cannot preserve the rich details inherent in visual images, and thus the\nstyle of the generated image is suboptimal. Currently, DreamBooth [27] and LoRA [13] are the more\neffective solutions. They utilize fine-tuning of the original diffusion model or extra small networks\nto extract a specific style from visual data. These approaches allow them to generate images with a\nprecise style, encompassing details such as brushstrokes and textures. However, the requirement to\nfine-tune or re-train the model for every new style renders these methods impractical, as they demand\ncomputational resources and time.\nTherefore, developing a LoRA-free method that can generate specific stylized images in a single\npass is desirable for both efficiency and flexibility. We first introduce a direct approach based on\n\u2217Work done during internship in ARC Lab, Tencent PCG. \u2020 Corresponding author.\narXiv:2309.01770v1  [cs.CV]  4 Sep 2023\n\u201cA black and white panda\u201d \u201cA lovely kitten walking in a garden\u201d\n\u201cA house\u201d\n\u201cA car\u201d\n\u201cA bird in a world\u201d\n\u201cA lake with calm water and reflections\u201d\n(a) Reference\n(b) StyleAdapter\n(c) StyleAdapter collaborated with T2I-Adapter\nFigure 1: Given multiple style reference images, our StyleAdapter is capable of generating images\nthat adhere to both style and prompts without test-time finetuning. Moreover, our method shows\ncompatibility with additional controllable conditions, such as sketches.\nStable Diffusion (SD) [25] to generate an image by simply combining the features of a prompt and\na reference image in a desired style as the condition of SD. We expect that the generated image\nmatches the content of the prompt and also incorporates the style of the references. However, we\nencounter two challenges: 1) the prompt usually loses controllability over the generated content,\nand 2) the generated image inherits both the semantic and style features of the style reference images,\ncompromising its content fidelity. In this paper, we conduct experimental analyses to investigate\nthese challenges. The first challenge arises from the cross-attention modules in SD that favor the\nstyle feature over the prompt feature. The second challenge arises from the strong coupling between\nthe semantic and style information in the style references. We find that processing the information\nof the prompt and style reference separately before fusing these two sources of information can\neffectively improve the controllability of the prompt over the generated content. Additionally, patch-\nwise shuffling of every style reference image, removing the class embedding in the vision model\nof CLIP [21] used for extracting reference features, and using multiple images that contain diverse\nsemantic objects but share the same style as style references can help decouple the semantic and style\ninformation in the style references.\nInspired by the above observations and analyses, we propose StyleAdapter, a LoRA-free model to\novercome these challenges. It consists of two components: 1) a two-path cross-attention module\n(TPCA) that can process the prompt and style reference features separately, and 2) three decoupling\nstrategies that reduce the strong coupling between the semantic and style information in the style\nreferences. StyleAdapter can generate high-quality images that match the content of the prompts and\nthe style of the references (even for unseen styles) in a single pass. Moreover, StyleAdapter can also\nwork with existing controllable synthesis, such as T2I-adapter [18] and ControlNet [38], to achieve a\nmore controllable and stable generation process.\nOur contributions can be summarized as follows: (1). We propose StyleAdapter, a single-pass\nLoRA-free model, that can generate high-quality images conforming to the given prompts and\ndesired style reference images. It is efficient and resource-saving compared to LoRA-based methods.\n(2). We introduce a two-path cross-attention module (TPCA) to process the style and prompt\nfeatures separately, allowing us to balance the impact of these two modalities on the generated\nimages and enhance the controllability of the prompt during the generation process. (3). We\npropose three strategies to decouple the semantic and style information in the style reference images,\nimproving content fidelity during generation. (4). Our StyleAdapter can also collaborate with existing\ncontrollable synthesis methods to generate high-quality images more controllably and stably.\n2\n(a) Reference\n(b) SD\n(c) Simple Combination\n(d) TPCA\n(e) Distribution\n\u201cA motorcycle\u201d\n\u201cA dog in a bucket\u201d\nFigure 2: Preliminary experimental results on the issue that the prompt loses controllability.\nWithout style reference, SD [25] generates natural images that match the content of the prompts, such\nas the motorcycle and dog in (b). However, when Simple Combination (SC) concatenates the features\nof the style reference to the prompts to guide the generation, the prompts lose their effect, and their\nresults are dominated by the girl and flowers in the style image as shown in (c). By analyzing the\nattention weights of the keywords, motorcycle and dog, in the prompts in each cross-attention layer in\nSD and SC, we find that the SC reduces the attention weight of the prompts and pays more attention\nto the style features. Therefore, we propose to inject the feature of the prompt and style reference into\nthe generated image separately with a two-path cross-attention module (TPCA), and its generated\nresults in (d) preserve both the content of the prompt and the style of the reference.\n2\nRelated Works\n2.1\nText-to-image synthesis\nText-to-image synthesis (T2I) is a challenging and active research area that aims to generate realistic\nimages from natural language text descriptions. Generative adversarial networks (GANs) are one of\nthe most popular approaches for T2I synthesis, as they can produce high-fidelity images that match\nthe text descriptions [24, 36, 37, 35, 16]. However, GANs suffer from training instability and mode\ncollapse issues [2, 4, 12]. Recently, diffusion models have shown great success in image genera-\ntion [31, 11, 19, 4], surpassing GANs in fidelity and diversity. Many recent diffusion methods have\nalso focused on the task of T2I generation. For example, Glide [20] proposed to incorporate the text\nfeature into transformer blocks in the denoising process. Subsequently, DALL-E [23], Cogview [5],\nMake-a-scene [7], Stable Diffusion [25], and Imagen [29] significantly improved the performance\nin T2I generation. To enhance the controllability of the generation results, ControlNet [38] and\nT2I-Adapter [18] have both implemented an additional condition network in conjunction with stable\ndiffusion. This allows for the synthesis of images that adhere to both the text and condition.\n2.2\nStylized image generation\nImage style transfer is a task of generating artistic images guided by an input image. Traditional\nstyle transfer methods match the patches between content and style images using low-level hand-\ncrafted features [33, 39]. With the rapid development of deep learning, deep convolutional neural\nnetworks have been used to extract the statistical distribution of features that can capture style patterns\neffectively [9, 10, 15]. Besides CNN, visual transformers have also been used for style transfer\ntasks [34, 3]. Recently, benefiting from the recent success of diffusion models [25, 29, 23], InST [40]\nadapted diffusion models as a backbone to be inverted and as a generator in stylized image generation.\n3\nMethodology\n3.1\nPreliminary\nStable Diffusion. In this paper, we employ the Stable Diffusion [25] (SD) as our T2I model. SD\nis a latent diffusion model (LDM) [25] trained on large-scale data. LDM is a generative model that\n3\ncan synthesize high-quality images from Gaussian noise by iterative sampling. Compared to the\ntraditional diffusion model, its diffusion process happens in the latent space. Therefore, except for a\ndiffusion model, an autoencoder consisting of an encoder E(\u00b7) and a decoder D(\u00b7) is needed. E(\u00b7)\nis used to encode an image I into the latent space z (z = E(I)) while D(\u00b7) is used to decode the\nfeature in the latent space back to an image. The diffusion model contains a forward process and a\nreverse process. Its denoising model \u03f5\u03b8(\u00b7) is implemented with Unet [26] and trained with a simple\nmean-squared loss:\nLLDM := Ez\u223cE(I),c,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, c)\u22252\n2\ni\n,\n(1)\nwhere \u03f5 is the unscaled noise, t is the sampling step, zt is latent noise at step t, and c is the condition.\nWhile SD acts as a T2I model, c is the text feature ft of a natural language prompt encoded with the\ntext model of CLIP [21]. ft is then integrated into SD with a cross-attention model, whose query Qt\nis from the spatial feature y which is extracted from Zt, and key Kt and value Vt are from ft. The\nprocess can be expressed as:\n(\nQt = WQt \u00b7 y; Kt = WKt \u00b7 ft; Vt = WV t \u00b7 ft;\nAttention(Qt, Kt, Vt) = softmax( QtKT\nt\n\u221a\nd ) \u00b7 Vt,\n(2)\nwhere WQt/Kt/Vt are learnable weights, and d is dependent on the number of channels of y.\nVision transformer. Vision Transformer (ViT) [6] is a model that applies the transformer archi-\ntecture [32] to computer vision tasks. It has shown impressive performance in learning visual\nrepresentations and has been adopted by the CLIP model [21], which we use in this work. As the\ndesign of our decoupling strategies is closely related to the structure of ViT, we give a concise descrip-\ntion of how it processes an image in this section. ViT processes an image, such as our style reference\nimage Ir \u2208 R(H\u00d7W \u00d7C) (where (H, W) is the resolution of Ir and C is the number of channels), by\nreshaping it into a sequence of flattened patches Ip\nr \u2208 RN\u00d7(P 2\u00b7C), where (P, P) is the patch size\nand N = HW/P 2 is the sequence length. Then, ViT applies a linear projection E \u2208 RP 2\u00b7C \u00d7 D\nto embed these patches into D dimensions. It also adds a learned class embedding Ecls \u2208 R1\u00d7D\nat the beginning of the sequence. Then, a learned position embedding Epos \u2208 R(N+1)\u00d7D is added\nto this sequence. The resulting embedded input EIr is fed to the subsequent modules of ViT. The\nembedding process can be written as:\nEIr = [Ecls, I0\nr E, I1\nr E, IN\u22121\nr\nE] + Epos.\n(3)\n3.2\nChallenges and Analyses\nSimple combination (SC). This paper aims to generate specific stylized images, whose con-\ntent is aligned with the prompt while style conforms to a reference image, with a LoRA-\nfree unified model in a single pass.\nTo this end, we initially propose to simply combine\nthe feature of a style reference image Ir with the prompt P to guide the generation of SD.\nAttention Block\nTransformer\n\ud835\udc40\ud835\udc40\ud835\udc60\ud835\udc60\n\ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60\n\u0302\ud835\udc53\ud835\udc53\ud835\udc5a\ud835\udc5a\n\ud835\udc53\ud835\udc53\ud835\udc5a\ud835\udc5a\n\ud835\udc53\ud835\udc53\ud835\udc5f\ud835\udc5f\n\u0302\ud835\udc53\ud835\udc53\ud835\udc5f\ud835\udc5f\nAttention Block\nAttention Block\nFigure 3: Structure of StyEmb.\nIn SD, the prompt feature ft \u2208 Rlt\u00d7dt is obtained using the text\nmodel in CLIP [21]. To make the feature of the prompt and style\nreference image Ir combinable, we first encode Ir into a vision\nfeature fr \u2208 Rlv\u00d7dv using the vision model of CLIP [21]. Then,\nwe introduce a style embedding module (StyEmb) to embed fr\ninto fs \u2208 Rls\u00d7dt. As illustrated in Figure 3, StyEmb predefines a\nlearnable embedding fm \u2208 Rls\u00d7dv and appends it to fr before being\nfed into a transformer implemented with three attention blocks. fm\nis used for compressing the information in fr (ls \u226a lv), and it can\nbe adapted to fr with a flexible length (referring to our later multiple\nreferences). Then, the learned fm is projected to \u02c6fm with a learnable\nmatrix Ms \u2208 Rdv\u00d7dt, and finally, we obtain the style feature fs. By\nconcatenating ft with fs as the condition c in Eq. 1 (c = [ft, fs]),\nwe can generate stylized images with SD.\nThis approach can achieve a desirable stylization effect. However, it faces two major challenges: 1)\nthe prompt loses controllability over the generated content, and 2) the generated image inherits both\nthe semantic and style features of the style reference images, compromising its content fidelity.\n4\n\u201cA robot\u201d\n(a) Reference\n(b) TPCA\n(c) Shuffle\n(d) NO \n(e) multi-references\nFigure 4: Preliminary experimental results on the issue of semantic and style coupling in the\nstyle image. (b) shows a result of our TPCA. It is a robot whose style is similar to the reference\nbut with a human face, due to the tight coupling between the semantic and style information in the\nreference. Our preliminary experiments suggest that shuffling the reference image (c), removing the\nclass embedding Ecls (d) in CLIP [21], and providing multiple diverse reference images (e) can help\nmitigate this issue.\nWe conduct a series of experiments to explore the underlying reasons for these phenomena and obtain\ntwo empirical observations.\nObservation 1: Combining the features of the prompt and style reference before injecting them\ninto the generated image results in a loss of controllability of the prompt over the generated\ncontent. Figure 2 illustrates the effect on the generation of SD after simply combining prompts with a\nstyle reference. When SD is guided with a prompt only, its generated results are natural and conform\nto the content in the prompt. However, when we attempt to stylize the generated images by taking\nthe combination of the prompt and style reference (a) as guidance, the prompt fails to control the\ncontent of the generated images, and the style reference becomes dominant. As shown in (c), the\ngenerated results contain no motorcycle or dog specified in the prompts, but the girl and flower from\nthe reference. To gain deeper insights, we plot the attention weights of \"motorcycle\", \"dog\", and style\nfeatures in each cross-attention layer of SD or SC in (e). We observe that the attention weights for\nboth \"motorcycle\" and \"dog\" decrease when adding style features in SC (the orange bars), compared\nto the original SD (the blue bars). Moreover, the style feature in SC receives high attention weights,\nindicating that the style feature dominates the generation. This violates our goal of generating stylized\nimages whose content aligns with the prompt while the style conforms to the reference. To facilitate\nthese two sources of information to perform their specific duties well, we propose injecting them into\nthe generated image separately with a two-path cross-attention module (TPCA, details in 3.3.1). This\napproach allows us to balance the impact of these two sources of features on the generated images\nand enhance the controllability of the prompt during the generation process. Results in Figure 2 (e)\nshow that TPCA achieves a better balance between the prompts and the style reference. The images\ncontain the motorcycle and the dog as the main objects, as specified by the prompts, and also inherit\nthe style of the reference images.\nObservation 2: Shuffling, class embedding suppressing, and multiple references can mitigate\nthe coupling issue. The second challenge is that the generated image inherits both the semantic\nand style features of the style reference images, compromising its content fidelity. This is mainly\ndue to the tight coupling between the style and semantic information in an image. In this work, we\ntend to attain the style information of the style reference image and discard its semantic information.\nTherefore, we conduct experiments to explore the potential strategies and obtain new observations.\nAs shown in Figure 4 (b), after alleviating the first challenge with our TPCA module, our method\ncan generate a robot with a style close to the reference image (a) based on the content of the prompt.\nHowever, the robot has a human face. To eliminate the human face, we make three attempts: 1)\nWe try to patch-levelly shuffle the reference style image before injecting it into the generation. Its\nresult in (c) shows that the human face is replaced with a robot face, and its hand is closer to the\nrobot. Although, its style is reduced. 2) We remove the class embedding in the CLIP model used to\nextract style image features, since we find that it is rich in semantic information and always attains a\nhigh attention weight while encoding the style feature. Its result in (d) shows that the human face\ndisappears, and the style loss of the generated image is relatively small. 3) We use multiple style\nimages containing diverse objects as references. This approach guides the generation model to extract\nsimilar information from multiple references and ignore their diverse semantic information while\ngenerating stylized images. Its result in (e) shows the human face on the robot disappears, and the\ngenerated image has a stronger stylization due to more style information.\n5\nVision \nModel\nC\nStyEmb\nText \nModel\nDiffusion Unet Model\n\ud835\udc47\n\ud835\udc53#\n\ud835\udc53$\n\ud835\udc53%\n\ud835\udc53&\n\ud835\udc53'(&\n\ud835\udc45 = {\ud835\udc3c%, \ud835\udc3c& \u2026 \ud835\udc3c'(&}\n\ud835\udc3c0\nFrozen\nTrainable\nC\nConcat\nT-\nCrossA\nSelfA\nS-\nCrossA\n\u00d7\ud835\udf06\nTPCA\n\ud835\udc66\n\ud835\udc664\n\ud835\udc43 : A motorcycle\nT-\nCrossA\nSelfA\nS-\nCrossA\n\u00d7\ud835\udf06\nTPCA\n\ud835\udc66\n\ud835\udc664\n\u2026\n\ud835\udc537\nFigure 5: Framework of StyleAdapter. StyleAdapter is built upon Stable Diffusion (SD) [25]. It\nuses the text and vision models of CLIP [21] to extract the feature of prompt P and style image Ii,\nrespectively. The features of references are concatenated as fr and fed into a StyEmb Module to attain\nthe style feature fs. The prompt feature ft and style feature fs are then processed separately with\nour two-path cross-attention module (TPCA) before fusing with a learnable coefficient \u03bb. The fused\nresult is passed to the subsequent block of SD. After T sampling steps, our StyleAdapter generates a\nstylized image whose content aligns with the prompt while the style conforms to the style references.\nAccording to these observations, we develop three decoupling strategies for decoupling the style\nand semantic information in a style reference: 1) shuffling, 2) class embedding suppressing, and 3)\nadopting multiple style references. Corresponding implementing details are in Sec. 3.3.2.\n3.3\nOur Method: StyleAdapter\nOverview. We present our framework in Figure 5. We employ the Stable Diffusion [25] (SD) as our\nT2I model. Its conditions consist of a natural language prompt P and a series of style reference images\nR = {I0, I1, . . . , IK\u22121} (K is the number of reference images). To incorporate these conditions\ninto SD, we first encode the prompt P into a text feature ft with the text model of CLIP [21] and\nencode each style reference image in R into a vision feature {f0, f1, . . . , fK\u22121} with the vision\nmodel of CLIP [21]. Then, the vision features {f0, f1, . . . , fK\u22121} are concatenated and sent into\na style embedding module (StyEmb) to attain a style embedding feature fs. ft and fs are then\nsent into our proposed two-path cross-attention module (TPCA), which consists of a text multi-head\ncross-attention block (T-CrossA) and a style multi-head cross-attention block (S-CrossA), to process\nthem independently. The processed results are summed together with a learnable weight \u03bb before\nentering the next block of SD. After T steps of sampling from the reverse diffusion model, we\ngenerate an image Io that conforms to the desired content and style. Note that SelfA in the figure\npresents a self-attention block, and all the modules in gray are frozen while the StyEmb, S-CrossA,\nand \u03bb displayed in green are trainable. Our StyleAdapter is learned with LLDM (Eq. 1), whose\ncondition c consists of ft and fs.\n3.3.1\nTow-Path Cross-Attention Module\nWe deploy our two-path cross-attention module after each self-attention module (SelfA) in the\ndiffusion Unet [26] model. It consists of two parallel cross-attention modules: T-CrossA and S-\nCrossA, which are responsible for handling the prompt-based condition and the style-based condition,\nrespectively. The query of both cross-attention modules comes from the spatial feature y of SD.\nHowever, the key and value of T-CrossA come from the text feature ft, while the key and value of\nS-CrossA come from the style feature fs. The attention output of T-CrossA Attention(Qt, Kt, Vt)\nhas the same formula as Eq. 2 and the output of S-CrossA Attention(Qt, Kt, Vt) can be formulated\nas:\n(\nQs = WQs \u00b7 y; Ks = WKs \u00b7 fs; Vs = WV s \u00b7 fs;\nAttention(Qs, Ks, Vs) = softmax( QsKT\ns\n\u221a\nd ) \u00b7 Vs.\n(4)\nThe outputs of these two attention modules are then added back to y and fused with a learnable\nparameter \u03bb (implemented as a scale embedding). This produces a new spatial feature \u02c6y that is fed to\nthe subsequent blocks of SD. The process can be expressed as:\n\u02c6y = Attention(Qt, Kt, Vt) + \u03bbAttention(Qs, Ks, Vs).\n(5)\nIt is worth noting that since SD already has a strong representation for the prompt, we retain the\noriginal cross-attention in SD as our T-CrossA and freeze it during training. In contrast, S-CrossA is\nimplemented with the same structure as T-CrossA and is trained to adapt to the style reference.\n6\n(c) CAST\n(a) Reference\n(d) StyTr2\n(f) InST(C)\n(b) Ours\n(g) InST(P)\n(e) SD\n\u201cA monkey playing with a banana\u201d\n\u201cA palm tree\u201d\nFigure 6: Qualitative comparison with state-of-the-art methods based on a single style reference\nimage: While traditional style transfer methods such as CAST [42] and StyTr2 [3] mainly focus on\ncolor transfer, and diffusion-based methods like SD [25] and InST [41] struggle to balance content\nand style, the results obtained with our StyleAdapter contain more style details from the reference\nimages, such as brushstrokes and textures, and better match the prompt content.\n3.3.2\nDecoupling Strategies\nOur StyleAdapter adopts three decoupling strategies to mitigate the issue raised by the strong\ncoupling between the semantic and style information in a style image.\nShuffling. As shown in Observation 2 in Sec. 3.2, shuffling the style reference image can effectively\ndecouple the semantic and style information in the style image, but at the cost of losing stylization\nquality in the generated image. This is because shuffling not only disturbs the semantic information\nbut also breaks the coherence of the style and textural information in the image, which affects the\nlearning of style features. Therefore, to avoid multiple cropping, instead of shuffling the raw style\nreferences, our shuffling design conforms to the design of the vision model. As described in Eq. 3,\nthe vision model in CLIP [21] crops the reference images into a sequence of flattened patches Ip\nr and\nmarks their positions with positions embeddings Epos. Our shuffling is implemented by shuffling the\nEpos before adding it to each Ip\nr .\nClass Embedding Suppressing. As illustrated in Eq. 3, the class embedding Ecls is concatenated\nbefore the embedding of the patches for each reference image. We implement class embedding\nsuppressing by directly removing the Ecls. Simultaneously, the first element of position embedding\nEpos in Eq. 3, which corresponds to Ecls, is also removed.\nMultiple Style References. We represent style references as R = {I0, I1, . . . , IK\u22121}, where K\ndenotes the number of style reference images. Multiple style references require K > 1. During\ntraining, we set K = 3, while at inference time, K can be any positive integer. we employ the\nvision model of [21] to extract the corresponding features {f0, f1, . . . , fK\u22121}. These features are\nconcatenated with the embedding feature fm from the Style Embedding module (StyEmb), resulting\nin fr = [f0, f1, . . . , fK\u22121, fm]. The combined features are then fed into the StyEmb to obtain fs,\nwhich serves as the style embedding feature during the generation process.\n4\nExperiments\n4.1\nExperimental settings\nDatasets. We use part of the LAION-AESTHETICS dataset as our training data. It contains 600K\nimage-text pairs. To evaluate the effectiveness of our proposed method, we construct a testset that\nconsists of prompts, content images, and style references. Prompts: We use ChatGPT [1] to generate\ndiverse prompts and manually filter out the irrelevant or low-quality ones. The final testset contains\n50 prompts. Content images: Some existing methods, such as CAST [42] and StyTR2 [3], require\nadditional content images as inputs. To make a fair comparison, we also include content images in\nour testset. SD synthesizes these images according to the prompts, which aligns with our proposed\nmethod that prompts determine the content of the generated image. Thus, we have 50 content images\nin total. Style references: We collect 8 sets of style references from the Internet, each containing 5 to\n14 images. We use them as our multi-reference inputs. We also select one representative image from\neach set as our single-reference image. Therefore, there is a total of 400 test pairs in the testset. More\ndetails are in Appendix A and Figure 9.\n7\nImplementation Details. We adopt the SD model [25] (version 1.5) as our base model and use the\ntext and vision encoders from CLIP [21], which are implemented with a large ViT [6] with a patch\nsize of 14. Thus, we have lt = 77, dt = 768, lv = 256, and dv = 1024 as the dimensions of the text\nand vision features. Moreover, we set ls = 8 as the length of the style feature. To train our model, we\nfix the parameters of the original SD and CLIP modules, and only update the weights of the newly\nadded StyEmb and S-CrossA modules. We employ Adam [14] as our optimizer with a learning\nrate of 8 \u00d7 10\u22126 and a batch size of 8. We run our experiments on 8 NVIDIA Tesla 32G-V100\nGPUs. The input and style images are resized to 512 \u00d7 512 and 224 \u00d7 224, respectively. For data\naugmentation, we apply the random crop, resize, horizontal flipping, rotation, etc., to generate K = 3\nstyle references for each input image during training (Note that K can vary at inference time). We set\nthe sampling step T = 50 for inference.\nEvaluation metrics. This paper evaluates the generated images both subjectively and objectively\nfrom three aspects: text similarity, style similarity, and quality. On the one hand, it conducts a User\nStudy to make a subjective assessment. On the other hand, it adopts a CLIP-based [21] metric that\ncomputes the cosine similarity between the generated image and the prompt (denoted as Text-Sim),\nas well as between the generated image and the style image (denoted as Style-Sim) to measure the\ntext similarity and style similarity objectively. Additionally, it adopts FID [30] to measure the quality\nof the generated images.\n4.2\nComparisons with State-of-the-art Methods\nIn this section, we conduct comparisons with current state-of-the-art related methods, including\ntwo traditional style transfer methods: CAST [42] and StyTr2, three SD-based methods: InST [41],\nTextual Inversion (TI) [8], and LoRA [13], and SD [25] itself. Note that we use the image2image\nmode in SD [25] to generate a stylized image from the testset provided content image and the prompt\ngenerated from the reference image with BLIP2 [17], which can generate descriptions of an image.\nComparisons based on single style reference. We compare our method with the state-of-the-art\nmethods that use a single style reference image. Figure 6 illustrates the visual results. CAST [42]\nand StyTr2 [3] only perform a coarse-grained color transfer. SD [25] fails to produce satisfactory\nstylization due to the poor representation of the text extracted from the reference image. InST [41]\nis implemented based on textural inversion. It can generate stylized images with a content image\n(InST(C)) or a prompt (InST(P)). The results of InST(C) exhibit better performance in stylization\ncompared to the previous three methods and InST(P), but the content in the style reference image\ndominates the content of the generated result (it generates a boy rather than the monkey indicated\nin the prompt in the first sample), or its unnatural textural appearance leads to a strange look of the\ngenerated result (the result of the second sample). On the contrary, the content of the generated\nimages for InST(P) is relatively closer to the prompt, but their style differs from the style reference.\nIn contrast, our method generates images that are more faithful to the style of the reference image,\nespecially in terms of brushstrokes, lines, etc. Our method also produces images that match the\nprompt content better. Table 1 shows the quantitative evaluation results of these methods. Our\nmethod attains a better balance between prompt fidelity, style fidelity, and generated quality than\nother methods.\nComparisons based on multiple style reference. Unlike our method, which is a unified model\nthat can be generalized to different styles without test-time finetuning, TI [8] and LoRA [13] require\ntraining on the style reference images for each style. Figure 7 and Table 1 present the visual\nand quantitative results, respectively. TI [8] inverses the style references into a learnable textural\nembedding embedded into the prompt for guiding the generation. It performs better in style similarity\n(high score in Style-Sim). However, Its generated content cannot match the prompt accurately, such\nas the purple hat, yellow scarf, red tie, and rainbows, indicated in the prompts but missed in their\ncorresponding generated results of TI [8], leading to a lower score in Style-Text. Our proposed\nmethod is comparable to LoRA [13] in style, but it performs better in text similarity, according\nto the higher score of Text-Sim and the generated tie and rainbows responding to the prompts in\nthe visualized results, which demonstrates that our StyleAdapter achieves a better balance between\ncontent similarity, style similarity, and generated quality in objective metrics.\nUser Study. To attain a more comprehensive evaluation, we conducted a user study. We randomly\nselected 35 generated results covering all styles and employed 24 users who worked in AIGC\nto evaluate these generated results in three aspects: text similarity, style similarity, and quality.\nConsequently, we received a total of 2520 votes, and the results in Table 1 show that the generated\nresults obtained with our StyleAdapter are more preferred in all three aspects. We observe that the\n8\nSingle-reference\nMulti-reference\nUser Study\nMethods\nCAST [42]\nStyTr2 [3]\nInST(P) [41]\nInST(C) [41]\nSD [25]\nOurs\nTI [8]\nLoRA [13]\nOurs\nCAST [42]\nInST(P) [41]\nLoRA [13]\nOurs\nText-Sim \u2191\n0.2323\n0.2340\n0.2204\n0.1682\n0.2145\n0.2435\n0.1492\n0.2390\n0.2448\nText-Sim \u2191\n0.2310\n0.0548\n0.2869\n0.4274\nStyle-Sim \u2191\n0.8517\n0.8493\n0.8616\n0.8707\n0.8528\n0.8645\n0.9289\n0.9034\n0.9031\nStyle-Sim \u2191\n0.3857\n0.0286\n0.1881\n0.3976\nFID \u2193\n163.77\n151.45\n177.91\n153.45\n189.34\n141.78\n139.56\n137.40\n140.97\nQuality \u2191\n0.2071\n0.0452\n0.3238\n0.4238\nTable 1: Objective and Subjective quantitative comparisons with the state-of-the-art methods. Our\nproposed method achieves a better balance in text similarly, style similarity, and quality, and attains\nmore preference from expert users.\n(c) LoRA\n(a) Ours\n(b) TI\nReference\nReference\n(f) LoRA\n(e) TI\n(d) Ours\nA man wearing a black leather jacket and a red tie\nA palm tree\nA waterfall with mist and rainbows\nA woman wearing a purple hat and a yellow scarf\nFigure 7: Qualitative comparison with TI [8] and LoRA [13] based on multiple style reference images.\nSince TI [8] and LoRA [13] are trained on the references, they are easily overfitting and insensitive\nto the prompt. In contrast, as a unified stylized model trained on general text-image data, our Style\nAdapter performs better in generating both style and content.\ndifference between objective and subjective metrics mainly lies in the fact that objective evaluation\nmetrics independently assess each aspect, while users may take the information from the other aspects\ninto consideration even though we provide separate options. This demonstrates that our generated\nresults achieve a better trade-off between quality, text similarity, and style similarity.\nCooperation with existing adapters. Our StyleAdapter can cooperate with existing adapters, such\nas T2I-adapter [18]. Results in the last column of Figure 1 and Figure 11 in Appendix D show that\nwith the guidance of the additional sketches, the shape of the generated contents is more controllable.\n4.3\nAblation Studies\nWe conduct experiments to evaluate the effectiveness of our proposed TPCA module and three\ndecoupling strategies. Qualitative results are presented in Figure 8. When we fuse the information in\nthe prompt and the single style reference image in the red box using the simple combination (SC)\n(described in Sec. 3.2), the girl in the reference dominates the content of the generated result, and\nthe dog and bucket in the prompt are ignored, as shown in (b). To improve the controllability of\nthe prompt over the generated content, we process these two sources of information separately with\nTPCA. The result is shown in (c), where the bucket from the prompt appears, but the dog is missing\nand the girl from the reference still exists. This phenomenon mainly results from the tight coupling\nbetween the semantic and style information in the style reference. Therefore, we first remove the class\nembedding Ecls based on TPCA to mitigate the tight coupling, and its result in (d) shows that the dog\nand bucket in the prompt are both generated in the image, but the girl from the reference still exists,\nand the dog is quite small. We further adopt the shuffling strategy to disrupt the coupling between\nsemantic and style information. The girl from the reference disappears in its results in (e), and the\ndog in the bucket becomes dominant in the generated image. However, its style becomes less similar\nto the style reference images, especially in terms of texture and brushstroke. Based on the setting of\n(e), we adopt multiple style references rather than a single reference, which constitutes the complete\npipeline of our StyleAdapter. The corresponding result in (f) shows that the dog in the bucket from the\nprompt dominates the content of the generated result, while its style (texture and brushstroke) is close\nto the reference images. These results demonstrate the effectiveness of our proposed TPCA module\n9\n\u201cA dog in a bucket\u201d\n(a) References\n(b) SC\n(c) TPCA\n(d) TPCA - \ud835\udc38\ud835\udc38\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 (e) TPCA - \ud835\udc38\ud835\udc38\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 + Shuffle (f) StyleAdapter\nFigure 8: Ablation study. Results (b)\u223c(e) are attained with a single reference in the red box while (f)\nis attained with all the references. Detailed illuminations are in 4.3.\nand three decoupling strategies. The quantitative results and more discussions of our StyleAdapter\nare in Appendix B and Appendix C.\n4.4\nLimitations and Border Impact\nOur work aims to propose a unified model for different styles without test-time fine-tuning. Compared\nto LoRA [13], which trains a specific model for each style, our model may not always achieve the\nstylization performance of LoRA. Further improving the generated quality and generalization of\nStyleAdapter is part of our ongoing work.\nSince our model primarily relies on a pre-trained stable diffusion, the generated data is constrained\nby the dataset distribution used for training the stable diffusion. Consequently, this could result in the\ngeneration of unethical images, raising concerns about content quality and ethical considerations.\n5\nConclusion\nIn this paper, we propose a LoRA-free method, called StyleAdapter, for stylized image generation\nin a single pass. Our method takes text prompts and style reference images as inputs and aims to\ngenerate images that preserve the content of the prompts and adopt the style of the references. Our\nmethod consists of two components: a two-path cross-attention module (TPCA) and three decoupling\nstrategies, which are carefully designed to overcome the challenges we face, such as 1) the prompt\nloses controllability over the generated content, and 2) the output image inherits both the semantic\nand style features of the style reference image, compromising its content fidelity. Our method can\ngenerate images with more fine-grained style details than previous traditional style transfer methods\nand some methods based on texture inversion. However, our method still lags behind LoRA, which\nrequires training for each new style. Improving the stylized image generation with more style details\nis our future work.\nReferences\n[1] Chatgpt: https://openai.com/blog/chatgpt.\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\n[3] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu.\nStytr2: Image style transfer with transformers. In CVPR, 2022.\n[4] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021.\n[5] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 2021.\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n[7] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In ECCV, 2022.\n[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.\narXiv preprint arXiv:2208.01618, 2022.\n10\n[9] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural\nnetworks. In CVPR, 2016.\n[10] Leon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. Controlling\nperceptual factors in neural style transfer. In CVPR, 2017.\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020.\n[12] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded diffusion models for high fidelity image generation., 2022.\n[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\n[14] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[15] Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Style transfer by relaxed optimal transport\nand self-similarity. In CVPR, 2019.\n[16] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. Controllable text-to-image generation.\nNeurIPS, 2019.\n[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[18] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453, 2023.\n[19] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In\nICML, 2021.\n[20] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. In ICML, 2022.\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, 2021.\n[22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[23] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n[24] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis. In ICML, 2016.\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, 2022.\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In MICCAI, 2015.\n[27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint\narXiv:2208.12242, 2022.\n[28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n[29] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.\nImage super-resolution via iterative refinement. arXiv preprint arXiv:2104.07636, 2021.\n[30] Maximilian Seitzer.\npytorch-fid:\nFID Score for PyTorch.\nhttps://github.com/mseitzer/\npytorch-fid, 2020.\n11\n[31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020.\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\n[33] Bin Wang, Wenping Wang, Huaiping Yang, and Jiaguang Sun. Efficient example-based painting and\nsynthesis of 2d directional texture. TVCG, 2004.\n[34] Xiaolei Wu, Zhihao Hu, Lu Sheng, and Dong Xu. Styleformer: Real-time arbitrary style transfer via\nparametric style composition. In ICCV, 2021.\n[35] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.\nAttngan: Fine-grained text to image generation with attentional generative adversarial networks. In CVPR,\n2018.\n[36] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N\nMetaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.\nIn ICCV, 2017.\n[37] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N\nMetaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks. TPAMI,\n2018.\n[38] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023.\n[39] Wei Zhang, Chen Cao, Shifeng Chen, Jianzhuang Liu, and Xiaoou Tang. Style transfer via image\ncomponent analysis. TMM, 2013.\n[40] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng\nXu. Inversion-based creativity transfer with diffusion models. arXiv preprint arXiv:2211.13203, 2022.\n[41] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng\nXu. Inversion-based style transfer with diffusion models. arXiv preprint arXiv:2211.13203, 2022.\n[42] Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, and Changsheng\nXu. Domain enhanced arbitrary image style transfer via contrastive learning. In SIGGRAPH, 2022.\n[43] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui\nXu, and Tong Sun. Lafite: Towards language-free training for text-to-image generation. arXiv preprint\narXiv:2111.13792, 2021.\nA\nMore Details of Testset\nTo evaluate the effectiveness of our proposed method, we construct a testset that consists of prompts,\ncontent images, and style references. Prompts: We use ChatGPT [1] to generate diverse prompts\nand manually filter out the irrelevant or low-quality ones. The final testset contains 50 prompts\nwhich are listed on the right of Figure 9. Content images: To meet the requirement of the content-\nbased methods, such as CAST [42] and StyTR2 [3], and align to our proposed method that prompts\ndetermine the content of the generated image with SD, we use SD to generate the content images\nfrom the prompts in the test set. In this paper, we fix the seed to 993 for generating the content images.\nStyle references: We collect 8 sets of style references from the Internet2, each containing 5 to 14\nimages. We use them as our multi-reference inputs and they are shown on the right of Figure 9.\n2The style references are collected from https://civitai.com,\nhttps://wall.alphacoders.com,\nand\nhttps://foreverclassicgames.com.\n12\n\u201cA robot\u201d\n\u201cA girl wearing a red dress, she is dancing.\u201d\n\u201cA boy wearing glasses, he is reading a thick book.\u201d\n\u201cA little cute boy.\u201d\n\u201cA woman wearing a green sportswear, she is running.\u201d\n\u201cA woman wearing a purple hat and a yellow scarf.\u201d\n\u201cA man wearing a black leather jacket and a red tie.\u201d\n\u201cA little boy with glasses and a watch.\u201d\n\u201cA smiling little girl.\u201d\n\u201cA little boy playing football.\u201d\n\u201cAn curly-haired boy.\u201d\n\u201cA little girl holding flowers.\u201d\n\u201cA lovely kitten walking in a garden.\u201d\n\u201cA puppy sitting on a sofa.\u201d\n\u201cA fluffy white rabbit with pink ears and nose.\u201d\n\u201cA brown puppy with black spots and a red collar.\u201d\n\u201cA black and white panda.\u201d\n\u201cA dog in a bucket.\u201d\n\u201cA cat wearing a hat.\u201d\n\u201cA cute little fish in aquarium.\u201d\n\u201cA bird in a word.\u201d\n\u201cA kitten sleeping on a pillow.\u201d\n\u201cA parrot singing a song.\u201d\n\u201cA monkey playing with a banana.\u201d\n\u201cA turtle wearing sunglasses.\u201d\n\u201cA hamster eating a carrot.\u201d\n\u201cA white rose.\u201d\n\u201cA sunflower smiling at the sun.\u201d\n\u201cA cactus wearing a hat.\u201d\n\u201cA daisy with a ladybug on it.\u201d\n\u201cA pine tree with a snowman hugging it.\u201d\n\u201cA mushroom in winter.\u201d\n\u201cA beautiful lotus.\u201d\n\u201cA lotus with a frog meditating on it.\u201d\n\u201cA cherry blossom.\u201d\n\u201cA palm tree.\u201d\n\u201cA river with rapids and rocks.\u201d\n\u201cA creek with clear water and colorful pebbles.\u201d\n\u201cA lake with calm water and reflections.\u201d\n\u201cA waterfall with mist and rainbows.\u201d\n\u201cA stone with a face carved on it, standing on a pedestal in a museum.\u201d\n\u201cA stone with a hole in it.\u201d\n\u201cA stone with a pattern of stripes on it.\u201d\n\u201cA stone with a crack in it, holding a plant growing out of it.\u201d\n\u201cA snowy mountain peak.\u201d\n\u201cA mountain goat on a cliff.\u201d\n\u201cA red baseball cap.\u201d\n\u201cA football on the grass.\u201d\n\u201cA motorcycle.\u201d\n\u201cA modern house with a pool.\u201d\n\u201cA house made of cardboard boxes.\u201d\n\u201cA house covered with ice and snow.\u201d\nFigure 9: Details of Testset. Sentences on the left are 50 prompts that we used in this work, and\nimages on the right are 8 sets of style references that we collect from the Internet.\nB\nMore Discussions about Ablation Study\nB.1\nQuantitative Results of Ablation Study\nTable 2 presents the quantitative results of our ablation study. Compared to the method that uses\na simple combination (SC), our StyleAdapter that uses two-path cross-attention modules (TPCA)\nachieves higher scores in terms of Text-Sim, which means its results are more consistent with the\nprompts, although sacrificing some performance of stylization (as indicated by the lower score in\nterms of Style-Sim). To further reduce the tight coupling between the semantic and style features of\nthe style reference image and improve the content fidelity of the generated results, we employ three\ndecoupling strategies. Removing Ecls in the vision model can slightly alleviate this coupling issue\nwith an improvement of the score of Text-Sim while barely affecting the performance of stylization.\nThe shuffling operation significantly mitigates the coupling challenge and boosts the score of Text-\nSim by about 0.0326. However, it also degrades the style of the generated results considerably, as\nshown by the large drop in the score of Style-Sim. By further employing the multi-reference strategy,\nour StyleAdapter enhances both Text-Sim and Style-Sim, achieving a better balance between the\ncontent and style of the generated results. Moreover, our TPCA and decoupling strategies enhance\nthe quality of generated images, as indicated by the lower FID score. The quantitative results are\nconsistent with the visualizations in Figure 8 in the paper.\nSC\nTPCA\nNo Ecls\nShuffling\nmulti-reference\nText-Sim \u2191\nStyle-Sim \u2191\nFID \u2193\n\u2713\n0.1263\n0.9362\n186.17\n\u2713\n0.2089\n0.8963\n145.37\n\u2713\n\u2713\n0.2109\n0.8921\n141.99\n\u2713\n\u2713\n\u2713\n0.2435\n0.8645\n141.78\n\u2713\n\u2713\n\u2713\n\u2713\n0.2448\n0.9031\n140.97\nTable 2: Quantitative results of ablation study. Our method based on TPCA achieves a significant\nimprovement in Text-Sim compared to SC. Adding decoupling strategies can progressively improve\nText-Sim, and eventually attain a better balance between Text-Sim and Style-Sim after utilizing\nmultiple references. Moreover, our TPCA and decoupling strategies enhance the quality of generated\nimages, as indicated by the lower FID score.\nB.2\nAdaptive \u03bb\nAs defined in Eq. 5 in the paper, our proposed two-path cross-attention modules fuse the information\nof the prompt and style references with \u03bb. \u03bb is an adaptive parameter that controls the trade-off\nbetween the content from the prompt and the style from the references. As shown in Figure 10,\nwhen we scale down \u03bb by a factor smaller than 1.0, the style features from the references fade away\ngradually, and the generated images become more natural. On the other hand, when we scale up \u03bb by\na factor larger than 1.0, the style features in the generated images become more prominent, such as\nthe 3D shape and fantastic appearance. However, the dog also loses its natural look. Therefore, users\n13\ncan customize the generated results according to their preferences by adjusting \u03bb. The results shown\nin this paper are obtained with the original \u03bb without any scaling factor unless otherwise stated.\n\u201cA dog in a bucket.\u201d\nReferences\n0.6\ud835\udf06\ud835\udf06\n0.8\ud835\udf06\ud835\udf06\n1.0\ud835\udf06\ud835\udf06\n1.2\ud835\udf06\ud835\udf06\n1.5\ud835\udf06\ud835\udf06\n1.8\ud835\udf06\ud835\udf06\nContent\nStyle\nFigure 10: Adaptation of \u03bb. By tuning \u03bb with an appropriate factor, we can obtain a generated\nimage with a better balance between the content from the prompt and the style from the references.\nFactors smaller than 1.0 tend to suppress style features and produce a more natural image, while\nfactors larger than 1.0 tend to enhance style features.\nC\nDiscussion about Model Size\nApart from the SD [25] model, the text and vision models borrowed from CLIP [21] are also used to\nextract the features of the prompt and style images in the previous related works, such as InST [41]\nand LoRA [13]. Therefore, the only novel modules in this work are the Style Emb and TPCA modules.\nTheir model sizes are 148M and 168M, respectively. Although the model sizes of InST [41] (15M)\nand LoRA [13] (37M) are smaller, an InST [41] model can only process the style from a specific\nimage that is used during training, while a LoRA [13] model is only suitable for a certain kind\nof styles. In contrast, our model can handle various styles by taking different style references at\ninference time.\nD\nMore Generated Results\nFigure 11 shows more generated results. Given multiple style reference images, our StyleAdapter\ncan generate images that adhere to both the style and the prompts in a single pass. For example, the\nfirst two generated results in the first row are a panda and a woman wearing a purple hat and a yellow\nscarf, which are consistent with their prompts, respectively. Both of them have a 3D shape and a cute\nlook, which are similar to their style references in the first column. Moreover, our StyleAdapter can\ncooperate with the existing controllable synthesis methods, such as T2I-Adapter [18], to generate\nhigh-quality images more controllably and stably. For example, given sketches (attached in the corner\nof the generated results in the fourth column), our method can generate objects following the sketches\nbut with the style of the reference images.\nBesides, we also evaluate our StyleAdapter with more styles. Results are in Figure 12. We can see\nthat our model performs well on different styles without test-time fine-tuning.\n14\n\u201cA black and white panda.\u201d\n\u201cA woman wearing a purple hat and a yellow scarf.\u201d\n\u201cA girl.\u201d\n\u201cA boy wearing glasses, he is reading a thick book.\u201d \u201cA lake with calm water and reflections.\u201d\n\u201cA woman.\u201d\n\u201cA kitten sleeping on a pillow.\u201d\n\u201cA palm tree.\u201d\n\u201cA dog.\u201d\n\u201cA hamster eating a carrot.\u201d\n\u201cAn curly-haired boy.\u201d\n\u201cAn  elephant.\u201d\n\u201cA bird in a word.\u201d\n\u201cA man wearing a black leather jacket and a red tie.\u201d\n\u201cA motorcycle.\u201d\n(a) References\n(b) StyleAdapter\n(c) StyleAdapter collaborated with T2I-Adapter\nFigure 11: More generated results. Given multiple style reference images, our StyleAdapter can\ngenerate images that adhere to both style and prompts in a single pass. Moreover, our method shows\ncompatibility with additional controllable conditions, such as sketches.\n15\nA stone face\nA bench\nA monkey with banana.\nA boat\nA piano\nA baby penguin\nA bench\nA monkey with banana.\nA robat\nA piano\nA baby penguin\nA boat\nA moose\nA butterfly\nA hat\nWaterfall with rainbow \nA butterfly\nA bird\nA palm tree\nA cow\nReference(s)\nFigure 12:\nEvaluations in additional styles. Our model performs well on these styles without\ntest-time fine-tuning.\n16\n"
  },
  {
    "title": "Gated recurrent neural networks discover attention",
    "link": "https://arxiv.org/pdf/2309.01775.pdf",
    "upvote": "6",
    "text": "Gated recurrent neural networks discover attention\nNicolas Zucchet * 1 Seijin Kobayashi * 1 Yassir Akram * 1 Johannes von Oswald 1 Maxime Larcher 1\nAngelika Steger \u2020 1 Jo\u02dcao Sacramento \u2020 1\nAbstract\nRecent architectural developments have enabled\nrecurrent neural networks (RNNs) to reach and\neven surpass the performance of Transformers on\ncertain sequence modeling tasks. These modern\nRNNs feature a prominent design pattern: lin-\near recurrent layers interconnected by feedfor-\nward paths with multiplicative gating. Here, we\nshow how RNNs equipped with these two de-\nsign elements can exactly implement (linear) self-\nattention. By reverse-engineering a set of trained\nRNNs, we find that gradient descent in practice\ndiscovers our construction. In particular, we ex-\namine RNNs trained to solve simple in-context\nlearning tasks and find that gradient descent in-\nstills in our RNNs the same attention-based in-\ncontext learning algorithm. Our findings highlight\nthe importance of multiplicative interactions in\nneural networks and suggest that certain RNNs\nmight be unexpectedly implementing attention\nunder the hood.\n1. Introduction\nAttention-based neural networks, most notably Transform-\ners (Vaswani et al., 2017), have rapidly become the state-\nof-the-art deep learning architecture, replacing traditional\nmodels such as multi-layer perceptrons, convolutional neu-\nral networks, and recurrent neural networks (RNNs). This is\nparticularly true in the realm of sequence modeling, where\nonce-dominating RNNs such as the long short-term memory\n(LSTM; Hochreiter & Schmidhuber, 1997) model and the\nrelated gated recurrent unit (GRU; Cho et al., 2014) have\nbeen mostly replaced by Transformers.\nNevertheless, RNNs remain actively researched for vari-\nous reasons, such as their value as models in neuroscience\n(Dayan & Abbott, 2001), or simply out of genuine interest\nin their rich properties as a dynamical system and uncon-\nventional computer (Jaeger et al., 2023). Perhaps most\n*Equal contribution \u2020Shared senior autorship 1Department of\nComputer Science, ETH Z\u00a8urich. Correspondence to: <nzucchet,\nseijink, yakram, voswaldj, larcherm, asteger, rjoao@ethz.ch>.\nimportantly for applications, RNNs are able to perform in-\nference for arbitrarily long sequences at a constant memory\ncost, unlike models based on conventional softmax-attention\nlayers (Bahdanau et al., 2015). This ongoing research has\nled to a wave of recent developments. On the one hand,\nnew deep linear RNN architectures (Gu et al., 2022; Orvieto\net al., 2023b) have been shown to significantly outperform\nTransformers on challenging long-sequence tasks (e.g., Tay\net al., 2020) and on some language modelling tasks (Gu &\nDao, 2023). On the other hand, many efficient linearized\nattention models have been developed, whose forward pass\ncan be executed in an RNN-like fashion at a constant infer-\nence memory cost (Tsai et al., 2019; Katharopoulos et al.,\n2020; Choromanski et al., 2021; Schlag et al., 2021; Fu\net al., 2023; Sun et al., 2023; Yang et al., 2023).\nWe present a unifying perspective on these two seemingly\nunrelated lines of work by providing a set of parameters\nunder which gated RNNs become equivalent to any lin-\nearized self-attention, without requiring infinite number of\nneurons or invoking a universality argument. Crucially,\nour construction makes use of elementwise multiplications,\nwhich are ostensibly featured in different forms in recent\ndeep linear RNN models. Turning to LSTMs and GRUs,\nwhich also include these multiplicative gating interactions,\nwe find somewhat surprisingly that our results extend only\nto LSTMs. Moreover, the LSTM construction we provide\nrequires a very specific configuration, which hints that the\ninductive bias towards attention-compatible configurations\nmight be weaker for this architecture than for deep gated\nlinear RNNs.\nWe then demonstrate that linear RNNs with multiplicative\ninteractions, but not LSTMs and GRUs, can effectively\nimplement our construction once trained, thus behaving as\nattention layers. Moreover, we find that such linear RNNs\ntrained to solve linear regression tasks acquire an attention-\nbased in-context learning algorithm. Incidentally, it has\nbeen shown that the very same algorithm is typically used\nby linear self-attention layers trained on this problem class\n(von Oswald et al., 2023; Mahankali et al., 2023; Ahn et al.,\n2023; Zhang et al., 2023). Our results thus challenge the\nstandard view of RNNs and attention-based models as two\nmutually exclusive model classes and suggest that, through\nlearning, RNNs with multiplicative interactions may end\n1\narXiv:2309.01775v2  [cs.LG]  7 Feb 2024\nGated recurrent neural networks discover attention\nup encoding attention-based algorithms disguised in their\nweights.\n2. Background\n2.1. Linear self-attention\nWe study causally-masked linear self-attention layers that\nprocess input sequences (xt)t with xt \u2208 Rd as follows:\nyt =\n\uf8eb\n\uf8edX\nt\u2032\u2264t\n(WV xt\u2032)(WKxt\u2032)\u22a4\n\uf8f6\n\uf8f8 (WQxt)\n(1)\nIn the previous equation, WV \u2208 Rd\u00d7d is the value matrix,\nWK \u2208 Rd\u00d7d the key matrix and WQ \u2208 Rd\u00d7d the query\nmatrix. We use square matrices throughout the paper for\nsimplicity, but our findings extend to rectangular ones. As\nusually done, we call vt := WV xt, kt := WKxt and qt :=\nWQxt the values, keys and queries. The output vector yt has\nthe same dimension as the input, that is d. Such linear self-\nattention layers can be understood as a linearized version\nof the softmax attention mechanism (Bahdanau et al., 2015)\nin use within Transformers (Vaswani et al., 2017). Yet,\nthey operate in a very different regime than softmax layers,\nwhich have unbounded memory. Attention layers commonly\ncombine different attention heads; we focus on a single one\nhere for simplicity.\nIn a linear self-attention layer, information about the past is\nstored in an effective weight matrix W ff\nt := P\nt\u2032 vt\u2032k\u22a4\nt\u2032 that\nwill later be used to process the current query qt through\nyt = W ff\nt qt. At every timestep, W ff\nt is updated through\nthe rule W ff\nt\n= W ff\nt\u22121 + vtk\u22a4\nt , which is reminiscent of\nHebbian learning (Schmidhuber, 1992; Schlag et al., 2021)\nand leads to faster inference time (Katharopoulos et al.,\n2020; Choromanski et al., 2021; Shen et al., 2021; Peng\net al., 2021) than softmax self-attention.\n2.2. Gated recurrent neural networks\nIn this paper, we focus our analysis on a simplified class\nof gated diagonal linear recurrent neural networks. They\nimplement bilinear input gin and output gating gout that\nmultiplies a linear transformation W in/out\nx\nxt of the input\nwith a linear gate W in/out\nm\nxt: gin/out(xt) = (W in/out\nm\nxt)\u2299\n(W in/out\nx\nxt). Here, \u2299 is the elementwise product. The class\nof gated networks we consider satisfies\nht+1 = \u03bb \u2299 ht + gin(xt),\nyt = Dgout(ht).\n(2)\nIn the previous equation, \u03bb is a real vector, xt is the input\nto the recurrent layer, ht the hidden state, and D a linear\nreadout. This simplified class makes connecting to attention\neasier while employing similar computational mechanisms\nas standard gated RNNs architectures.\nThis class is tightly linked to recent deep linear RNN archi-\ntectures and shares most of its computational mechanisms\nwith them. While linear diagonal recurrence might be seen\nas a very strong inductive bias, many of the recent power-\nful deep linear RNN models adopt a similar bias (Gupta\net al., 2022; Smith et al., 2023; Gu & Dao, 2023), and it has\nbeen shown to facilitate gradient-based learning (Orvieto\net al., 2023b; Zucchet et al., 2023b). Those architectures\noften use complex-valued hidden states in the recurrence;\nwe only use its real part here. Some of those works employ a\nGLU (Dauphin et al., 2017) after each recurrent layer, with\nGLU(x) = \u03c3(Wmxt) \u2299 Wxxt with \u03c3 the sigmoid function.\nThe gating mechanism we consider can thus be interpreted\nas a linearized GLU. We can recover (2) by stacking two\nlayers: the GLU in the first layer acts as our input gating,\nand the one in the second as output gating. Alternatively,\narchitectures like Mamba (Gu & Dao, 2023) uses input-\ndependent matrices as projection to the hidden state instead\nof the input gating. Multiplying such matrices with the in-\nput itself thus results in a multiplicative gating. Its output\ngating mechanism is slightly different as one of the branch\ntakes the input of the recurrent layer as input, instead of\nthe hidden state. We include a more detailed comparison in\nAppendix B. In the rest of the paper, we will use the LRU\nlayer (Orvieto et al., 2023b) as the representative of the deep\nlinear RNN architectures because of its simplicity.\nLSTMs can operate in the regime of Equation 2, but this\nrequires more adaptation. First, the recurrent processing is\nnonlinear and involves more steps than are captured in (2).\nSecond, gating occurs in different parts of the computation\nand depends on additional variables. We compare in more\ndetails this architecture and the one of Equation 2 in Ap-\npendix B, showing that LSTMs can implement (2) when\nstacking two layers on top of each other. We additionally\nshow that GRUs cannot do so.\n3. Theoretical construction\nAs highlighted in the previous section, our class of gated\nRNNs and linear self-attention have different ways of stor-\ning past information and using it to modify the feedforward\nprocessing of the current input. The previous state ht acts\nthrough a bias term \u03bb \u2299 ht that is added to the current input\ngin(xt) in gated RNNs, whereas the linear self-attention\nrecurrent state W ff\nt modifies the weights of the feedforward\npathway. We reconcile these two mismatched views of\nneural computation in the following by showing that gated\nRNNs can implement linear self-attention.\nIn this section, we demonstrate how a gated recurrent layer\nfollowed by a linear readout as in Equation 2 can implement\nany linear self-attention layer through a constructive proof.\nIn particular, our construction only requires a finite number\nof neurons to exactly match the desired function, therefore\n2\nGated recurrent neural networks discover attention\n1. Input gating\nCompute the outer product corresponding to\ncurrent key-values and the current query\n2. Recurrent neurons\nAccumulate key-values over time (          )\nand store current query (          ) \n3. Output gating and readout \nmatrix (key-values) - vector (query)\nmultiplication \nsum matrix\nFigure 1. An example of a diagonal linear gated recurrent neural network that implements the same function as a linear self-attention layer\nwith parameters (WV , WK, WQ) and input dimension d, as described in Section 3. Inputs are processed from top to the bottom. We do\nnot use biases so we append 1 to the input vector xt to be able to send queries to the recurrent neurons. We use repeat(A, n) to denote\nthat the matrix A is repeated n times on the row axis and WV,i is the i-th row of the WV matrix. The bars within the matrices separate the\ndifferent kinds of inputs/outputs. Digits in matrices denote column vectors appropriately sized. The readout matrix D appropriately sums\nthe elementwise products between key-values and queries computed after the output gating gout. Exact matrix values can be found in\nAppendix A.1.\nproviding a much stronger equivalence result than more\ngeneral universality of linear recurrent networks theorems\n(Boyd & Chua, 1985; Grigoryeva & Ortega, 2018; Orvieto\net al., 2023a), which hold in the limit of infinitely many\nrecurrent neurons.\n3.1. Key ideas\nOur construction comprises three main components: First,\nthe input gating gin is responsible for generating the elemen-\ntwise products between the keys and values, as well as the\nqueries. Then, recurrent units associated with key-values\naccumulate their inputs with \u03bb = 1, whereas those receiving\nqueries as inputs return the current value of the query, hence\n\u03bb = 0. Lastly, the output gating gout and the final readout\nlayer D are in charge of multiplying the flattened key-value\nmatrix with the query vector. We illustrate our construction\nand provide a set of weights for which the functional equiva-\nlence holds in Figure 1. Crucially, the key-values in a linear\nself-attention layer are the sum of degree two polynomials\nof each previous input. Input gating mechanism and perfect\nmemory units (\u03bb = 1) are needed to replicate this behavior\nwithin a gated recurrent layer. Similarly, output gating is\nrequired to multiply key-values with the queries.\n3.2. On the number of neurons needed\nThe construction of Figure 1 requires d2 + d hidden neu-\nrons to store all the entries of the d \u00d7 d key-value matrix\nand of the query vector of size d. While this construction\nis arguably the most intuitive, it is not optimal in terms\nof number of neurons used. Knowing the exact minimal\nnumber of neurons is fundamental for understanding which\nsolution the network learns. Therefore, we detail how we\ncan make our construction more compact in the following.\nWe leverage two insights: First, any combination of key and\nquery matrices for which (W \u22a4\nKWQ) is fixed leads to the\nsame function in the linear self-attention layer. We can thus\nassume that the key and value matrices are equal, as taking\nthe key matrix to be equal to WV and changing the query\nmatrix to be W \u2212\u22a4\nV\nW \u22a4\nKWQ does not change the behavior of\nthe attention layer. Second, when the key and value matrices\nare equal, the key-value matrix is symmetric and, therefore,\nonly requires d(d + 1)/2 elements to be represented. This\nimplies that, when the value matrix is invertible, the min-\nimal number of hidden neurons our gated RNN needs to\nstore key-values is in fact d(d + 1)/2 + d. In Section 4, we\nshow that learned RNNs find this solution.\nAlternatively, it is also possible to reduce the construction\nsize when the weight matrices of the teacher attention layer\nare of low rank. In this case, we still have a quadratic\nscaling of the required numbers of recurrent neurons, but\nthis time in the rank of the different matrices instead of the\nentire dimension. The detailed derivation can be found in\nAppendix A.4.\nOverall, the output gating requires O(d2) input and output\nentries for the gated RNN to match a linear self-attention\nlayer. The RNN thus requires O(d4) parameters in total,\n3\nGated recurrent neural networks discover attention\nwith a lot of redundancy, significantly more than the 3d2\nparameters of the linear self-attention layer. We note that\nchanging the output gating to a side one is possible, c.f.\nAppendix A.2, reducing the number of required parameters\nto O(d3).\nGiven the high parameter redundancy, it comes as no sur-\nprise that numerous equivalent configurations exist within\nthe gated RNN we study. For instance, linear gating is in-\nvariant under permutations of rows between its two matrices\nand under multiplication-division of these two rows by a\nconstant. Left-multiplying WQ in the input gating by any\ninvertible matrix P, and subsequently reading out the hid-\nden neurons with \u03bb = 0 through repeat(P \u22121, d), also does\nnot alter the network\u2019s output. Several other invariances ex-\nist, making exact weight retrieval nearly impossible. These\nconsiderations will be of practical use when we will reverse\nengineer the function encoded by trained recurrent networks\nin Section 4.1.\n3.3. Implications for existing classes of RNNs\nWe conclude this section by commenting on whether similar\ninsights hold for more realistic gated RNNs architectures.\nThe LRU architecture is close to (2) but only contains output\ngating through a GLU layer. Stacking two LRU layers on\ntop of each other enables the output gating of the first layer\nto act as the input gating for the second layer and, therefore,\nimplement the mechanism we highlighted in the previous\nsections to mimick attention. Intuitively, adding an input\nGLU would bias the LRU towards linear self-attention as\none layer would now enough to implement it. We will later\nconfirm that this indeed improves the LRU ability to mim-\nick linear self-attention, as well as boost its performance on\ncertain tasks. The Mamba block has a stronger inductive\nbias towards attention due to the presence of a side gating\nquerying the memory stored in the recurrent state. Interest-\ningly, it has been found that removing the input dependence\nof the matrix projecting to the hidden state is detrimental to\nperformance (Gu & Dao, 2023). This decreases the induc-\ntive bias towards linear self-attention, which might partly\nexplain the performance drop.\nAs noted in Section 2.2, LSTMs and GRUs are further away\nfrom our simplified gated RNN model. However, one single\nLSTM layer can implement linear self-attention, but stacked\nGRU layers cannot. Let us briefly summarize the argument\nbehind these results. The LSTM layer has a sophisticated in-\nput gating mechanism that gates a candidate cell state based\non the current input and previous state. The gate and the can-\ndidate cell state depend, among other things, on the current\ninput. This mechanism can thus play a similar role to gin\nand implement the key-value outer product. The recurrence\nof the cell state can be set to perfectly integrate key-values,\nby setting the forgetting gate accordingly. Finally, the out-\nput gate modulates the current cell state, which contains the\naccumulated key-values. Setting the output gate to encode\nthe query enables computing the desired result. We note\nthat the output gating differs from gout: it multiplies trans-\nformations of the cell state and the input instead of the input\nonly. This property makes it possible to implement attention\nwithin one layers, where as two layers are required for our\ngated RNN model (2). While the GRU layer takes many\nof the computational elements from the LSTM, it cannot\nimplement attention as it has no mechanism to compute\nmultiply keys and values.\nWe refer the reader to Appendix B for more details.\n4. Gated RNNs learn to mimic attention\nWe now demonstrate that gated RNNs learn to implement\nlinear self-attention and comprehend how they do so. In this\nsection, a student RNN is tasked to reproduce the output of\na linear self-attention layer. Appendix C contains detailed\ndescriptions of all experiments performed in this section.\nImportantly, each sequence is only presented once to the\nnetwork.\n4.1. Teacher identification\nIn our first experiment, we train a student RNN (|x| = 4,\n|h| = 100 and |y| = 4) to emulate the behavior of a linear\nself-attention layer with weights sampled from a normal\ndistribution and inputs xt sampled i.i.d. from a normal\ndistribution. The low training loss, reported in Table 1, high-\nlights that the student\u2019s in-distribution behavior aligns with\nthe teacher\u2019s. However, this is insufficient to establish that\nthe student implements the same function as the teacher.\nThe strategy we adopt to show functional equivalence is as\nfollows: First, we observe that only perfect memory neurons\n(\u03bb = 1) and perfect forget neurons (\u03bb = 0) influence the net-\nwork output. Additionally, each of these groups of neurons\nreceives all the information needed to linearly reconstruct\nresp. the key-values and the queries from the input (Table 1\nScore KV and Score Q columns). Finally, we show that\nthe output gating and the decoder matrix accurately multi-\nply accumulated key-values with current queries, leading to\nproper identification of the teacher self-attention function,\neven outside the training distribution (Table 1 Polynomial\ndistance).\nAfter the learning process, a significant part of the weights\nin the input and output gating and the readout becomes zeros.\nWe can thus prune neurons with input or output weights that\nare entirely zeros, thereby preserving the network\u2019s function.\nBy doing so, we can remove 86 out of the 100 hidden neu-\nrons and 87 out of the 100 pre-readout neurons. After having\npermuted rows in the two gating mechanisms and reordered\nhidden neurons, we plot the resulting weights on Figure 2.B.\n4\nGated recurrent neural networks discover attention\nFigure 2. In our teacher-student experiment of Section 4.1 (d = 4), the structure of the weights of the RNN after learning matches the one\nof our compact construction, c.f. Section 3. (A) Summary of the post-processing we apply to the trained network weights. The number of\nrecurrent neurons is denoted n, and the number of neurons after the output gating is denoted m. (B) Only recurrent neurons with perfect\nmemory (\u03bb = 1, dark blue) or no memory at all (\u03bb = 0, light grey) influence the output, consistently with the theory. The block structure\nof the different weight matrices almost perfectly match the one of our construction, c.f. Figure 1 (C) The last three output neurons of\nthe output gating are functionally equivalent to a single neuron whose input weights match the structure of the rest of the output gating\nweights. This can be achieved by representing each such neuron as an outer product (left part) which will later be combined by the readout\nmatrix D. The combined kernels are rank 1 and proportional to each other. They can thus be expressed as the same outer product (right\npart). In all the matrices displayed here, zero entries are shown in light grey, blue denotes positive entries, and red negative ones.\nConsistently with our construction, only recurrent neurons\nwith \u03bb = 0 or \u03bb = 1 contribute to the network\u2019s output.\nThe key-values neurons receive a polynomial of degree 2,\nas gin is a bilinear form, without any term of degree 1 as\nthe last column of W in\nm and W in\nx is equal to zero for those\nunits. Similarly, the query neurons receive a polynomial of\ndegree 1. The learning process discovers that it can only use\nd(d + 1)/2 = 10 neurons to store key-values, similar to our\noptimal construction. We show in Table 1 that it is possible\nto linearly reconstruct the key-values from those 10 neurons\nperfectly, as well as the queries from the 4 query neurons.\nBy combining this information with the fact that the \u03bbs are\nzeros and ones, we deduce that the cumulative key-values\nP\nt\u2032\u2264t vt\u2032k\u22a4\nt\u2032 can be obtained linearly from the key-values\u2019\nhidden neurons, and the instantaneous queries qt from the\nquery neurons.\nAdditionally, the output gating combined with the linear\nreadout can multiply the key-values with the queries. Since\nwe have already confirmed that the temporal processing cor-\nrectly accumulates key-values, our focus shifts to proving\nthat the instantaneous processing of the gated RNN matches\nthe one of the attention layer across the entire input domain.\nGiven that both architectures solely employ linear combi-\nnations and multiplications, their instantaneous processing\ncan be expressed as a polynomial of their input. The one of\nLoss\nScore KV\nScore Q\nPolynomial distance\n4.97 \u00d7 10\u22128\n4.52 \u00d7 10\u22128\n2.06 \u00d7 10\u221210\n3.73 \u00d7 10\u22124\nTable 1. Gated RNNs implement the same function as a linear self-\nattention layer in our teacher-student experiment (Section 4.1).\nThe KV and Q scores are equal to one minus the R2 score of\nthe linear regression that predicts key-values and queries from\nresp. the perfect memory neurons (those whose \u03bb = 1) and perfect\nforget neurons (\u03bb = 0). The polynomial distance is the L2 distance\nbetween the coefficients of the degree-4 polynomial that describes\nthe instantaneous processing of the (optimal) linear self-attention\nlayer and the trained RNN.\nlinear self-attention, (WV x)(WKx)\u22a4(WQx), corresponds\nto a polynomial of degree 3, whereas the one of the gated\nRNN, gout(gin(x)), corresponds to one of degree 4. By\ncomparing these two polynomials, we can compare their\nfunctions beyond the training domain. For every one of the\nfour network outputs, we compute the coefficients of terms\nof degree 4 or lower of their respective polynomials and\nstore this information into a vector. We then calculate the\nnormalized Euclidean distance between these coefficient\nvectors of the linear self-attention layer and the gated RNN,\nand report the average over all 4 output units in Table 1. The\nevidence presented so far enables us to conclude that the\nstudent network has correctly identified the function of the\n5\nGated recurrent neural networks discover attention\nA\nB\nC\nFigure 3. Gated RNNs learn compressed representations when possible. In the teacher-student experiment of Section 4 (A, B), the gated\nRNN identifies the teacher function under mild overparametrization. When the attention layer weights are low rank (B) the RNN learns a\nmore compressed representation than what it would do when they are full rank (A). (C) In the linear regression task of Section 5, the\ngated RNN behaves similarly to the optimal linear attention layer for that task, as the difference between their losses (delta loss) goes to 0.\nMoreover, the RNN discovers the same low-rank structure as this attention layer.\nteacher.\nWhile the majority of the weights depicted in Figure 2.A\nconform to the block structure characteristic of our construc-\ntion, the final three rows within the output gating matrices\ndeviate from this trend. As shown in Figure 2.B, these three\nrows can be combined into a single row matching the de-\nsired structure. More details about this manipulation can be\nfound in Appendix C.2.\n4.2. Identification requires mild overparametrization\nThe previous experiment shows that only a few neurons in a\nnetwork of 100 hidden neurons are needed to replicate the\nbehavior of a self-attention layer whose input size is d. We\ntherefore wonder if identification remains possible when\ndecreasing the number of hidden and pre-output gating neu-\nrons the student has. We observe that mild overparametriza-\ntion, around twice as many neurons as the actual number\nof neurons required, is needed to reach identification. We\nreport the results in Figure 3.A.\n4.3. Nonlinearity makes identification harder\nWe now move away from our simplified class of gated RNNs\nand seek to understand how our findings apply to LSTMs,\nGRUs, and LRUs. We use the following architecture for\nthose three layers: a linear embedding layer projects the\ninput to a latent representation, we then repeat the recurrent\nlayer once or twice, and finally apply a linear readout. While\nthose layers are often combined with layer normalization,\ndropout, or skip connections in modern deep learning ex-\nperiments, we do not include any of those here to stay as\nclose as possible to the teacher\u2019s specifications. In an LRU\nlayer, the input/output dimension differs from the number\nof different neurons; we here set all those dimensions to the\nsame value for a fair comparison with LSTMs and GRUs.\nWe compare these methods to the performance of our sim-\nplified gated RNNs, with both diagonal (as in Equation 2)\nand dense linear recurrent connectivity.\nWe report the results in Figure 4.A for inputs of dimension\nd = 6. While diagonal connectivity provides a useful in-\nductive bias to learn how to mimic linear self-attention, it is\nnot absolutely needed as changing the recurrence connec-\ntivity to be dense does not significantly affect performance.\nIt is theoretically possible to identify the teacher with one\nLSTM layer. However, gradient descent does not find such\na solution and the performance of LSTMs is close to that\nof GRUs that cannot implement attention. Motivated by\nthe construction of Section 3, we slightly modify the LRU\narchitecture (LRU+) and add a nonlinear input gating to\nthe already existing output gating. We find that this mod-\nification significantly improves the ability of a LRU layer\nto mimic attention. Appendix C contains experiments that\nextensively compare different LRU architectures, as well as\ncomparisons that take into account the number of parame-\nters of the different architectures. Additionally, we provide\nresults confirming that multiplicative interactions are fun-\ndamental for mimicking attention: replacing gating with a\n1-hidden layer MLP with the same number of parameters\nsignificantly deteriorates performance.\n5. Attention-based in-context learning emerges\nin trained RNNs\nThe previous section shows that gated RNNs learn to repli-\ncate a given linear self-attention teacher. We now demon-\nstrate that they can find the same solution as linear self-\nattention when both are learned. To that end, we study an in-\ncontext regression task in which the network is shown a few\ninput-output pairs and later has to predict the output value\ncorresponding to an unseen input. Linear self-attention is\na particularly beneficial inductive bias for solving this task.\nWhen the input-output mapping is linear, (von Oswald et al.,\n2023) have shown that linear self-attention implement one\nstep of gradient descent.\n6\nGated recurrent neural networks discover attention\nA\nB\nFigure 4. Comparison of the test loss obtained by different gated recurrent networks architectures in (A) the teacher-student task of\nSection 4 and (B) the in-context linear regression task of Section 5. The construction baseline corresponds to the gated RNN of Eq. 2, with\ndiagonal or dense connectivity. We use the default implementation of LSTMs and GRUs, and slightly modify the LRU architecture to\nreflect our construction better. Non-linearity improves the in-context learning performance but deteriorates the ability to mimic attention.\n5.1. In-context linear regression\nLinear regression consists in estimating the parameters\nW \u2217 \u2208 Rdy\u00d7dx of a linear model y = W \u2217x from a set of\nobservations {(xt, yt)}T\nt=1 that satisfy yt = W \u2217xt. The ob-\njective consists in finding a parameter \u02c6W which minimizes\nthe squared error loss L(W) =\n1\n2T\nPT\nt=1 \u2225yt \u2212 Wxt\u22252.\nGiven an initial estimate of the parameter W0, one step\nof gradient descent on L with learning rate T\u03b7 yields the\nweight change\n\u2206W0 = \u03b7\nT\nX\nt=1\n(yt \u2212 W0xt)x\u22a4\nt .\n(3)\nIn the in-context version of the task, the observations\n(xt, yt)1\u2264t\u2264T are provided one after the other to the net-\nwork, and later, at time T + 1, the network is queried with\n(xT +1, 0) and its output regressed against yT +1. Under this\nsetting, von Oswald et al. (2023) showed that if all bias\nterms are zero, a linear self-attention layer learns to imple-\nment one step of gradient descent starting from W0 = 0 and\npredict through\n\u02c6yT +1 = (W0 + \u2206W0)xT +1 = \u03b7\nT\nX\nt=1\nytx\u22a4\nt xT +1.\n(4)\nIn the following, we show that gated RNNs also learn to\nimplement the same algorithm and leverage the sparse struc-\nture of the different attention matrices corresponding to\ngradient descent to learn a more compressed representation\nthan the construction one.\n5.2. Gated RNNs learn to implement gradient descent\nWe now train gated RNNs as in Equation 2 to solve the in-\ncontext linear regression task, see Appendix D.1 for more\ndetails. We set the number of observations to T = 12 and\nset the input and output dimensions to 3 so that d = 6. Once\nlearned, the RNN implements one step of gradient descent\nTerm\nRNN\nGD\nx2\n1y1\n6.81 \u00d7 10\u22122 \u00b1 8.52 \u00d7 10\u22125\n6.76 \u00d7 10\u22122\nx2\n2y1\n6.82 \u00d7 10\u22122 \u00b1 6.40 \u00d7 10\u22125\n6.76 \u00d7 10\u22122\nx2\n3y1\n6.82 \u00d7 10\u22122 \u00b1 5.56 \u00d7 10\u22125\n6.76 \u00d7 10\u22122\nresidual\n1.35 \u00d7 10\u22123 \u00b1 1.97 \u00d7 10\u22124\n0\nTable 2. Gated RNNs implement gradient descent in the in-context\nlinear regression task of Section 5. Here, the input (resp. out-\nput) at time t is denoted as xt = (xt,1, xt,2, xt,3)\u22a4 (resp. yt =\n(yt,1, yt,2, yt,3)). The instantaneous function for each output neu-\nron can implement a polynomial of degree 4 in these terms. The\ntable shows the coefficients of the polynomial implemented by\nthe first output neuron of a trained RNN on the in-context linear\nregression task. Interestingly, the only terms without negligible co-\nefficients (averaged over 4 seeds) are (x1)2y1, (x3)2y1, (x3)2y1.\nThe polynomial is virtually identical to that of one optimal step\nof gradient descent. The optimal GD learning rate is obtained\nanalytically (\u03b7\u2217 = (T + dx \u2212 1/5)\u22121), c.f. Appendix D.2. The\nresidual norm measures the norm of the polynomial coefficients,\nexcluding the ones appearing in the table.\nwith optimal learning rate, which is also the optimal solution\none layer of linear self-attention can find (Mahankali et al.,\n2023). Several pieces of evidence back up this claim: the\ntraining loss of RNN after training (0.0945) is almost equal\nto the one of an optimal step of gradient descent (0.0947)\nand the trained RNN implements the same instantaneous\nfunction, as the polynomial analysis of Table 2 reveals.\nLinear self-attention weights implementing gradient descent\nhave a very specific low-rank structure (von Oswald et al.,\n2023). To test whether the network learned our correspond-\ning compressed construction, we vary the gated RNN size\nand report in Figure 3.C the difference between the final\ntraining loss and the loss obtained after one optimal gra-\ndient descent step. We observe a similar transition from\nhigh to low low than in the teacher-student experiment, this\ntime happening around the number of recurrent neurons\n7\nGated recurrent neural networks discover attention\nprescribed by our low-rank construction. Gated RNNs thus\nlearn a more compressed representation than the one naively\nmimicking self-attention. This result provides some hope re-\ngarding the poor O(d4) scaling underlying our construction:\nin situations that require an attention mechanism with low-\nrank (WV , WK, WQ) matrices, gated RNNs can implement\nattention with far fewer neurons. A precise understanding\nof how much compression is possible in practical scenarios\nrequires further investigation.\nIn Appendix D.3, we provide an additional set of results\nfocusing on associative recall, an in-context task where\nthe goal is to memorize (and then retrieve) associations\nbetween pairs of inputs presented in sequence (Fu et al.,\n2023). This may be viewed as a simple instance of in-\ncontext classification, which does not require generalization.\nAs for linear regression, we find that trained gated RNNs\ndiscover an algorithm similar to the one employed by linear\nself-attention.\n5.3. Nonlinear gated RNNs are better in-context\nlearners than one step gradient descent\nFinally, as a side question, we compare the ability to learn\nin context of the nonlinear gated RNN architectures that are\nLSTMs, GRUs and LRUs. Although not the main focus\nof our paper, this allows us to put our previous results in\nperspective. In particular, we are interested in understand-\ning if similarity with attention correlates with in-context\nlearning performance, as attention has been hypothesized to\nbe a key mechanism for in-context learning (Olsson et al.,\n2022; Garg et al., 2022; von Oswald et al., 2023). We report\nour comparison results in Figure 4.B, measuring the loss\non weights W \u2217 drawn from a distribution with double the\nvariance of the one used to train the model.\nOverall, we find that nonlinearity greatly helps and enables\nnonlinear gated RNN architectures to outperform one gra-\ndient descent step when given enough parameters, suggest-\ning that they implement a more sophisticated mechanism.\nSurprisingly, while the GRU is the architecture that is the\nfurthest away from attention, it performs the best in the task.\nWithin the different LRU layers we compare, we find a high\ncorrelation between in-context learning abilities and close-\nness to attention, c.f. Figure 6 in the Appendix. In particular,\nwe observe a massive performance improvement from the\nvanilla LRU architecture to the ones additionally including\ninput gating to match our construction more closely. Once\nagain, replacing the GLU by a MLP leads to a great decrease\nin performance.\n6. Discussion\nOur study reveals a closer conceptual relationship between\nRNNs and attention-based architectures than commonly\nassumed. We demonstrate that gated RNNs can theoretically\nand practically implement linear self-attention, bridging\nthe gap between these two architectures. Moreover, while\nTransformers have been shown to be powerful in-context\nlearners (Brown et al., 2020; Chan et al., 2022), we find\nthat RNNs excel in toy in-context learning tasks and that\nthis performance is partly uncorrelated with the architecture\ninductive bias toward attention. This highlights the need for\nfurther investigations on the differences between RNNs and\nTransformers in controlled settings, as also advocated by\n(Garg et al., 2022).\nOur results partly serve as a negative result: implementation\nof attention is possible but requires squaring the number\nof parameters attention has. We have shown that gated\nRNNs can leverage possible compression, but understand-\ning whether real-world attention mechanisms lie in this\nregime remains an open question. Yet, our work is of cur-\nrent practical relevance as it provides a framework that can\nguide future algorithmic developments, as we exemplify\nin Appendix B.5. Bridging the gap between Transformers\u2019\ncomputational power and RNNs\u2019 inference efficiency is a\nthriving research area (Fournier et al., 2023), and the link\nwe made facilitates interpolation between those two model\nclasses.\nFinally, our work carries implications beyond deep learning.\nInspired by evidence from neuroscience supporting the exis-\ntence of synaptic plasticity at different timescales, previous\nwork (Schmidhuber, 1992; Ba et al., 2016; Miconi et al.,\n2018) added a fast Hebbian learning rule, akin to linear\nself-attention, to slow synaptic plasticity with RNNs. We\nshow that, to some extent, this mechanism already exists\nwithin the neural dynamics, provided that the response of\nneurons can be multiplicatively amplified or shut-off in an\ninput-dependent manner. Our results therefore suggest that\nrecurrent neural circuits with long integration time constants,\nsuch as those found in the prefrontal cortex, might be learn-\ning and holding associations between past inputs in working\nmemory. These circuits would effectively encode associa-\ntive weights in their neural activity, not in actual synaptic\nconnections, as would be the case for classical associative\nmemory networks (Steinbuch, 1961; Willshaw et al., 1969;\nKohonen, 1972). Interestingly, several single-neuron and\ncircuit-level mechanisms have been experimentally iden-\ntified which could support the required multiplication op-\neration in biological neural networks (Silver, 2010). We\nspeculate that such multiplicative mechanisms could be in-\nvolved in implementing self-attention-like computations in\nbiological circuitry.\nAcknowledgements\nThe authors thank Asier Mujika and Razvan Pascanu for\ninvaluable discussions. This study was supported by an\n8\nGated recurrent neural networks discover attention\nAmbizione grant (PZ00P3 186027) from the Swiss National\nScience Foundation and an ETH Research Grant (ETH-23\n21-1).\nReferences\nAhn, K., Cheng, X., Daneshmand, H., and Sra, S.\nTransformers learn to implement preconditioned gra-\ndient descent for in-context learning.\narXiv preprint\narXiv:2306.00297, 2023.\nBa, J., Hinton, G. E., Mnih, V., Leibo, J. Z., and Ionescu,\nC. Using fast weights to attend to the recent past. In\nAdvances in neural information processing systems, 2016.\nBahdanau, D., Cho, K., and Bengio, Y. Neural machine\ntranslation by jointly learning to align and translate. In\nInternational Conference on Learning Representations,\n2015.\nBoyd, S. and Chua, L. Fading memory and the problem\nof approximating nonlinear operators with Volterra se-\nries. IEEE Transactions on Circuits and Systems, 32(11),\n1985.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,\nC., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,\nWanderman-Milne, S., and Zhang, Q. JAX: composable\ntransformations of Python+NumPy programs, 2018. URL\nhttp://github.com/google/jax.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., and others. Language models are few-shot\nlearners. In Advances in neural information processing\nsystems, 2020.\nChan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A.,\nRichemond, P., McClelland, J., and Hill, F. Data distri-\nbutional properties drive emergent in-context learning in\ntransformers. In Advances in Neural Information Pro-\ncessing Systems, 2022.\nCho, K., van Merrienboer, B., Bahdanau, D., and Bengio, Y.\nOn the properties of neural machine translation: encoder-\ndecoder approaches. In Proceedings of SSST-8, Eighth\nWorkshop on Syntax, Semantics and Structure in Statisti-\ncal Translation, 2014.\nChoromanski, K., Likhosherstov, V., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,\nA., Kaiser, L., Belanger, D., Colwell, L., and Weller, A.\nRethinking attention with Performers. In International\nConference on Learning Representations, 2021.\nDauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Lan-\nguage modeling with gated convolutional networks. In\nInternational Conference on Machine Learning, 2017.\nDayan, P. and Abbott, L. F. Theoretical neuroscience: com-\nputational and mathematical modeling of neural systems.\nMIT Press, 2001.\n9\nGated recurrent neural networks discover attention\nFournier, Q., Caron, G. M., and Aloise, D. A practical sur-\nvey on faster and lighter transformers. ACM Computing\nSurveys, 55(14s), 2023.\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A.,\nand R\u00b4e, C. Hungry Hungry Hippos: Towards Language\nModeling with State Space Models. In International\nConference on Learning Representations, 2023.\nGarg, S., Tsipras, D., Liang, P. S., and Valiant, G. What\ncan transformers learn in-context? a case study of simple\nfunction classes. In Advances in Neural Information\nProcessing Systems, 2022.\nGrigoryeva, L. and Ortega, J.-P. Universal discrete-time\nreservoir computers with stochastic inputs and linear\nreadouts using non-homogeneous state-affine systems.\nJournal of Machine Learning Research, 19, 2018.\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling\nwith selective state spaces, 2023.\nGu, A., Goel, K., and R\u00b4e, C. Efficiently modeling long\nsequences with structured state spaces. In International\nConference on Learning Representations, 2022.\nGupta, A., Gu, A., and Berant, J. Diagonal state spaces are\nas effective as structured states spaces. In Advances in\nNeural Information Processing Systems, 2022.\nHarris, C. R., Millman, K. J., Walt, S. J. v. d., Gommers, R.,\nVirtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg,\nS., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Kerkwijk,\nM. H. v., Brett, M., Haldane, A., R\u00b4\u0131o, J. F. d., Wiebe, M.,\nPeterson, P., G\u00b4erard-Marchant, P., Sheppard, K., Reddy,\nT., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant,\nT. E. Array programming with NumPy. Nature, 585\n(7825), 2020.\nHeek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre,\nB., Steiner, A., and Zee, M. v. Flax: A neural network\nlibrary and ecosystem for JAX, 2023. URL http://\ngithub.com/google/flax.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural Computation, 9(8), 1997.\nHunter, J. D. Matplotlib: A 2D graphics environment. Com-\nputing in Science & Engineering, 9(3), 2007.\nJaeger, H., Noheda, B., and Van Der Wiel, W. G. Toward\na formal theory for computing machines made out of\nwhatever physics offers. Nature Communications, 14(1),\n2023.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are RNNs: fast autoregressive Transformers\nwith linear attention. In International Conference on\nMachine Learning, 2020.\nKohonen, T. Correlation matrix memories. IEEE Transac-\ntions on Computers, 100(4):353\u2013359, 1972.\nLoshchilov, I. and Hutter, F. Decoupled weight decay reg-\nularization. In International Conference on Learning\nRepresentations, 2019.\nMahankali, A., Hashimoto, T. B., and Ma, T. One step of\ngradient descent is provably the optimal in-context learner\nwith one layer of linear self-attention. arXiv preprint\narXiv:2307.03576, 2023.\nMartinelli, F., Simsek, B., Brea, J., and Gerstner, W. Expand-\nand-cluster: exact parameter recovery of neural networks.\narXiv preprint arXiv:2304.12794, 2023.\nMiconi, T., Clune, J., and Stanley, K. O. Differentiable\nplasticity: training plastic neural networks with back-\npropagation. In International Conference on Machine\nLearning, 2018.\nOlsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,\nN., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen,\nA., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds,\nZ., Hernandez, D., Johnston, S., Jones, A., Kernion, J.,\nLovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J.,\nKaplan, J., McCandlish, S., and Olah, C. In-context learn-\ning and induction heads. Transformer Circuits Thread,\n2022.\nOrvieto, A., De, S., Gulcehre, C., Pascanu, R., and Smith,\nS. L. On the universality of linear recurrences followed\nby nonlinear projections. In ICML 2023: 1st Workshop\non High-dimensional Learning Dynamics, 2023a.\nOrvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre,\nC., Pascanu, R., and De, S. Resurrecting recurrent neural\nnetworks for long sequences. In International Conference\non Machine Learning, 2023b.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V., and others. Scikit-learn: Ma-\nchine learning in Python. Journal of machine Learning\nresearch, 12, 2011.\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\nHe, X., Hou, H., Kazienko, P., Kocon, J., Kong, J., Kop-\ntyra, B., Lau, H., Mantri, K. S. I., Mom, F., Saito, A.,\nTang, X., Wang, B., Wind, J. S., Wozniak, S., Zhang,\nR., Zhang, Z., Zhao, Q., Zhou, P., Zhu, J., and Zhu, R.-J.\nRWKV: Reinventing RNNs for the transformer era. arXiv\npreprint arXiv:2305.13048, 2023.\nPeng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,\nN. A., and Kong, L. Random feature attention. In Inter-\nnational Conference on Learning Representations, 2021.\n10\nGated recurrent neural networks discover attention\nSchlag, I., Irie, K., and Schmidhuber, J. Linear Transformers\nare secretly fast weight programmers. In International\nConference on Machine Learning, 2021.\nSchmidhuber, J. Learning to control fast-weight memories:\nan alternative to dynamic recurrent networks. Neural\nComputation, 4(1), 1992.\nShen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient\nattention: attention with linear complexities. In Proceed-\nings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision (WACV), 2021.\nSilver, R. A. Neuronal arithmetic. Nature Reviews Neuro-\nscience, 11(7), 2010.\nSmith, J. T., Warrington, A., and Linderman, S. W. Simpli-\nfied state space layers for sequence modeling. In Interna-\ntional Conference on Learning Representations, 2023.\nSteinbuch, K. Die lernmatrix. Kybernetik, 1:36\u201345, 1961.\nSun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J.,\nWang, J., and Wei, F. Retentive network: A successor to\ntransformer for large language models, 2023.\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham,\nP., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long\nrange arena: A benchmark for efficient transformers.\narXiv preprint arXiv:2011.04006, 2020.\nTsai, Y.-H. H., Bai, S., Yamada, M., Morency, L.-P., and\nSalakhutdinov, R. Transformer dissection: a unified un-\nderstanding of transformer\u2019s attention via the lens of ker-\nnel. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing, 2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems, 2017.\nvon Oswald, J., Niklasson, E., Randazzo, E., Sacramento,\nJ., Mordvintsev, A., Zhmoginov, A., and Vladymyrov,\nM. Transformers learn in-context by gradient descent. In\nInternational Conference on Machine Learning, 2023.\nWillshaw, D. J., Buneman, O. P., and Longuet-Higgins, H. C.\nNon-holographic associative memory. Nature, 222(5197):\n960\u2013962, 1969.\nYang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated\nlinear attention Transformers with hardware-efficient\ntraining, 2023.\nZhang, R., Frei, S., and Bartlett, P. L.\nTrained trans-\nformers learn linear models in-context. arXiv preprint\narXiv:2306.09927, 2023.\nZucchet, N., Meier, R., and Schug, S. Minimal LRU, 2023a.\nURL https://github.com/NicolasZucchet/\nminimal-LRU.\nZucchet, N., Meier, R., Schug, S., Mujika, A., and Sacra-\nmento, J. Online learning of long-range dependencies.\nIn Advances in Neural Information Processing Systems,\n2023b.\n11\nGated recurrent neural networks discover attention\nA. Additional details about the construction\nIn Section 3 and Figure 1, we have shortly described our\nconstruction. We here provide additional details, as well\nas refine it to settings in which we assume additional struc-\nture on the key, query and values matrices. We recall the\nmathematical definition of the gated RNN we consider:\nht+1 = \u03bb \u2299 ht + gin(xt)\n(5)\nyt = Dgout(ht)\n(6)\ngin(x) = (W in\nm x) \u2299 (W in\nx x)\n(7)\ngout(x) = (W out\nm x) \u2299 (W out\nx\nx).\n(8)\nA.1. Explicit values of the matrices of the vanilla\nconstruction\nHere, we detail the values the matrices in Figure 1 take to\nmimic a linear self-attention layer with key, query and value\nmatrices WK, WQ and WV . The key-values are stored in\nthe first d2 recurrent neurons and the queries in the last d\nones (indices d2 + 1 to d2 + d).\nInput gating.\nW in\nx and W in\nm are matrices of size (d2 +\nd)\u00d7(d+1). The matrix W in\nx both computes the values and\nthe queries:\n(W in\nx )i,j =\n\uf8f1\n\uf8f2\n\uf8f3\n(WV )i/d,j\nif j \u2264 d and i \u2264 d2\n(WQ)i\u2212d2,j\nif j \u2264 d and i > d2\n0\notherwise\n(9)\nand the matrix W in\nm the keys:\n(W in\nm )i,j =\n\uf8f1\n\uf8f2\n\uf8f3\n(WK)i mod d,j\nif j \u2264 d and i \u2264 d2\n1\nif j = d + 1 and i > d2\n0\notherwise\n(10)\nwhere / denotes integer division and mod the modulo op-\neration. As a consequence, the input received by the i-th\nrecurrent neuron is (WV x)i/d(WKx)i mod d when i \u2264 d2,\nand (WQx)i\u2212d2 when i > d2.\nRecurrent neurons.\n\u03bb is a vector of size d2 + d with\n\u03bbi =\n\u001a 1\nif i \u2264 d2\n0\notherwise.\n(11)\nThe memory neurons, the first d2 for which \u03bb = 1, perfectly\nintegrate all the key-values pairs.\nOutput gating.\nW out\nx\nand W out\nm\nare matrices of size d2 \u00d7\n(d2 + d) with W out\nx\nselecting the desired key-value element\n(W out\nx\n)i,j =\n\u001a 1\nif j \u2264 d and i = j\n0\notherwise\n(12)\nsum matrix\nFigure 5. Construction for gated RNNs with side gating, as de-\nscribed in Section A.2\nand W out\nm\nthe query element\n(W out\nm )i,j =\n\u001a 1\nif j > d2 and i = j mod d\n0\notherwise\n(13)\nAfter the d2 output neurons of the output gating thus\ncontains all the\n\u0000P\nt\u2032(WV xt\u2032)(WKxt\u2032)\u22a4\u0001\ni,j (WQxt)j el-\nements, and it only remains to sum them.\nReadout.\nThe goal of the readout matrix D, which has\nsize d \u00d7 d2, is to sum the key-values query products. It is\nequal to\nDi,j =\n\u001a 1\nif i = j/d\n0\notherwise\n(14)\nThe\noutput\ni\nof\nthe\ngated\nRNN\nwill\nthus\nbe\nP\nj\n\u0000P\nt\u2032(WV xt\u2032)(WKxt\u2032)\u22a4\u0001\ni,j (WQxt)j,\nwhich\nis\nequals\nto\n\u0000\u0000P\nt\u2032(WV xt\u2032)(WKxt\u2032)\u22a4\u0001\n(WQxt)\n\u0001\ni,\nthe\ndesired output.\nA.2. Alternative construction with side gating\nWith input and output gating, one has to waste some of the\nrecurrent neurons to instantaneously pass through the query\nvalues. We chose this architecture because it is arguably\nsimple and more common, but it is possible to give a RNN\nwith a stronger inductive bias towards linear self-attention\nby replacing the output gating with a side gating, that is\nyt = Dgside(x, ht), with gside(x, h) = (W sidex) \u2299 h.\n(15)\nInterestingly, this kind of side gating is featured in the re-\ncently proposed Mamba layer and indirectly in LSTMs, as\nwe shall discuss in further detail in Section B. We detail how\nto adapt our construction to the side gating and provide a\nvisual depiction of it in Figure A.2. Crucially, this construc-\ntion only requires O(d3) parameters instead of the O(d4)\nof the previous one.\n12\nGated recurrent neural networks discover attention\nInput gating and recurrent neurons.\nThe construction\nremains the same as the previous one, except that we get\nrid of the constant term in the input and the last d recurrent\nneurons.\nSide gating.\nThe side gating matrix W side is of size\nRd2\u00d7d has to copy queries d times and put them in front of\nthe corresponding key-value entry, that is\nW side\ni,j\n= (WQ)i mod d,j\n(16)\nReadout matrix.\nIt remains the same as before.\nA.3. Reducing construction size with invertible WV /\nWK\nIn Section 3.2, we have argued that it is possible to reduce\nthe number of recurrent neurons to d(d + 1)/2 + d when\nWQ is invertible. We use two insights.\nInvariances of the linear self-attention layer.\nThe first\nthing we can remark is that modifying WQ and WK does\nnot change the output of the layer as long as W \u22a4\nKWQ is kept\nconstant. This is because\n X\nt\u2032\n(WV xt\u2032)(WKxt\u2032)\u22a4\n!\n(WQxt)\n= WV\n X\nt\u2032\nxt\u2032x\u22a4\nt\u2032\n!\nW \u22a4\nKWQxt\n(17)\nIt follows that a linear self-attention layer with weights\n(WK, WQ, WV ) behaves similarly to one with weights\n(WV , W \u2212\u22a4\nV\nW \u22a4\nKWQ, WV ), as\nW \u22a4\nV W \u2212\u22a4\nV\nW \u22a4\nKWQ = WKWQ.\n(18)\nNote that a similar argument holds if WK is invertible.\nSymmetry of the key-values.\nIn the paragraph above, we\nhave justified why we can consider the key and query values\nto be equal. In this case, the key-values matrix becomes\nsymmetric. Knowing the elements contained in the upper\ntriangular part is thus enough to know the entire matrix.\nWe can thus ignore recurrent neurons corresponding to the\nlower triangular part. Note that similar insights apply to the\nside gating construction.\nA.4. Reducing construction size with low-rank teacher\nIntuitively, when the teacher attention layer is of low rank,\nit is not necessary to represent all the elements of the key-\nvalues matrices if we can change the basis considered. We\nformalize this argument in the following. To that extent, we\nintroduce the SVD decomposition of the value and query-\nkey matrices:\nWV = UV \u03a3V V \u22a4\nV\n(19)\nW \u22a4\nKWQ = UKQ\u03a3KQV \u22a4\nKQ.\n(20)\nwith \u03a3 diagonal matrices with as many non-zero elements\nas the rank of the matrix, and U and V orthogonal matrices.\nThe output of the attention layer can thus be written as\nUV\n X\nt\u2032\n(\u03a3V VV xt\u2032)(\u03a3KQUKQxt\u2032)\u22a4\n!\nVKQxt.\n(21)\nWith this decomposition, only the first rank(WV ) rows\nand rank(W \u22a4\nKWQ) columns of the key-values matrix are\nnot 0, that is we can reduce the number of recurrent neu-\nrons in our construction to rank(W \u22a4\nKWQ) rank(WV ). Re-\ngarding the queries, only the first rank(W \u22a4\nKWQ) coordi-\nnates will be considered. In total, we thus need at most\nrank(W \u22a4\nKWQ)(rank(WV ) + 1) neurons to replicate the\nteacher. As in the previous section, similar insights applies\nto the side gating construction.\nTo confirm that gated RNNs learn this solution, we per-\nformed a similar analysis to the one we did in Figure 3.A,\nthis time with low-rank teacher. To that extent, we take\nd = 12 and restrict the rank of the key, query and value\nmatrices to be 6. We do so by randomly sampling WK, WQ\nand WV and removing 12 \u2212 6 = 6 singular values. Given\nthe random sampling, rank(W \u22a4\nKWQ) = 6 almost surely.\nWe observe the stereotypical transition when the number of\nhidden neurons match rank(W \u22a4\nKWQ)(rank(WV ) + 1) =\n6 \u00d7 7 = 42, as plotted in Figure 3.B.\nB. Gated RNNs and linear self-attention\nIn this section, we compare our simplified gated RNN\nmodel, linear self-attention, and nonlinear gated RNN mod-\nels (LSTMs, GRUs, LRUs and Mamba). We recall that the\nkey ingredients of our simplified gated RNNs defined as\nht+1 = \u03bb \u2299 ht + gin(xt),\nyt = Dgout(ht),\n(22)\nare the diagonal linear recurrence and the input and output\ngating. The input gating serves as a way to generate the\nkey-values of linear self-attention, which will then be accu-\nmulated in the hidden recurrent units and combined with\nqueries within the output gating.\nTable 3 summarizes how many layers of LRUs, Mamba,\nLSTMs and GRUs are needed to exactly implement our\nsimplified class of gated RNNs and linear self-attention. We\nprovide more details below.\nB.1. LRU\nAn LRU layer (Orvieto et al., 2023b) consists of a recur-\nrent state ht and some instantaneous post-processing. Its\n13\nGated recurrent neural networks discover attention\nSimplified\ngated RNN\nLinear self-\nattention\nLRU\n2\n2\nLRU In-Out\n1\n1\nLRU In-Out (MLP)\n\u2013\n\u2013\nMamba\n2\n1\nLSTM\n2\n1\nGRU\n\u2013\n\u2013\nTable 3. Number of layers needed for different RNN layers to\nexactly implement our simplified class and linear self-attention.\nrecurrent state is updated as\nht+1 = \u03bb \u2299 ht + \u03b3 \u2299 (Bxt+1)\n(23)\nand its output yt is computed with\n\u02dcyt+1 = Re[Cht] + Dxt+1\n(24)\nyt+1 = \u03c3(Wm\u02dcyt+1) \u2299 (Wx\u02dcyt+1).\n(25)\nIn the equations above, ht+1, B and C are complex-valued,\nRe denotes the real part of a complex number, and \u03c3 is\nthe sigmoid function. The transformation nonlinear trans-\nformation between yt+1 and \u02dcyt+1 is called a gated linear\nunit (GLU) and was introduced in (Dauphin et al., 2017).\nAdditionally, \u03bb and \u03b3 are parametrized exponentially:\n\u03bb = exp(\u2212 exp(\u03bdlog) + i exp(\u03b8log)) and \u03b3 = exp(\u03b3log).\n(26)\nThe LRU layer detailed above comprises two central com-\nputational mechanisms: a linear recurrence coupled with a\nGLU serving as nonlinear output gating. The recurrence is\nhere complex-valued, but we only need the real part of it for\nour purposes. Assuming that the sigmoid can be linearized,\nour class of gated RNNs can be implemented using two lay-\ners by letting the output gating of the first layer serve as input\ngating. We are now left with linearizing the sigmoid. To\nachieve this, we double the number of output neurons of the\nGLU and require small weights in Wm, that can for example,\nbe compensated by large weights in Wm. Under this regime,\nwe have \u03c3(Wmx) \u2299 (Wxx) \u2248 (1/2 + Wmx) \u2299 (Wxx).\nHalf of the neurons require identical weights as the target\nlinear gating (up to a proportional factor), half should have\nWm = 0 and the same Wx as target linear gating. The\n1/2Wxx term that comes from the second half of the neu-\nrons can be subtracted from the first half of the neurons\nin a subsequent linear transformation, thereby yielding the\ndesired result.\nIn our experiments, we consider two additional variations of\nthe LRU layer that can implement our class of gated RNNs\nand/or linear self-attention using only one layer. The LRU\nIn+Out variation has an additional nonlinear input gating\nmechanism compared to the original version (LRU Out) that\nmodifies the input before the recurrent part of the layer. The\nLRU In+Out (MLP) replaces the GLU in the LRU In-Out\nvariation by a 1-hidden layer MLP, keeping the number of\nparameters fixed. The LRU In-Out variation can implement\nboth linear self-attention and our class of gated RNNs in\none layer, whereas LRU In-Out (MLP) cannot, as it does\nnot have any multiplicative interactions.\nB.2. Mamba\nA (simplified) Mamba layer is defined as\n\u02dcxt = W input(xt)\n(27)\n\u00afAt = exp(\u2206(\u02dcxt)A(\u02dcxt))\n(28)\n\u00afBt = \u2206(\u02dcxt)B(\u02dcxt)\n(29)\nht+1 = \u00afAt+1ht + \u00afBt+1\u02dcxt+1\n(30)\nyt = C(\u02dcxt+1)ht+1 \u2299 \u03c3(W side(xt))\n(31)\nwhere \u2206, A, B, C, W input and W side are linear transfor-\nmations that produce resp. a scalar, matrix, matrix, matrix,\nvector and vector of appropriate size. For simplicity, we\nhave ignored the convolutional layer after W input in \u02dcx, the\nfact that each coordinate of \u02dcx has its own independent recur-\nrent layer and the specific parametrizations of the different\nparameters.\nHere, the recurrence is linear with input-dependence, and\nthus more general than the one we are focusing on in this\npaper. It is easy to set it to what our construction requires.\nHowever, finding an input/output gating in this architecture\nis more tricky. The main insight is to look at\n(B(x)x)i =\nX\nj\nB(x)ijxj\n(32)\n= B(x)iixi +\nX\nj\u0338=i\nB(x)ijxj\n(33)\nand realize that it can implement a gating mechanism in\nwhich one of the branch is the identity. If it is preceded\nby a liner layer, such as W input it can thus behave as the\nkind of gating we are focusing on in this paper. The input-\ndependent B thus provides an input gating. The side gating\nwe studied in Appendix A.2 can be implemented through\nthe side modulation, by linearizing the sigmoid, or indirectly\nthrough C. This implies that one single Mamba layer can\nemulate a linear self-attention layer. However, there is no\nmechanism to implement an output gating, so 2 layers are\nneeded to mimick our simplified class of gated RNNs.\nB.3. LSTM\nAn LSTM cell (Hochreiter & Schmidhuber, 1997) has two\nrecurrent states: the hidden state ht and the cell state ct.\n14\nGated recurrent neural networks discover attention\nThey are updated as follows.\nft+1 = \u03c3(Ufxt+1 + Vfht + bf)\n(34)\n\u02dcct+1 = tanh(Ucxt+1 + Vcht + bc)\n(35)\ngt+1 = \u03c3(Ugxt+1 + Vght + bg)\n(36)\nct+1 = ft+1 \u2299 ct + gt+1 \u2299 \u02dcct+1\n(37)\not+1 = \u03c3(Uoxt+1 + Voht + bo)\n(38)\nht+1 = ot+1 \u2299 tanh(ct+1).\n(39)\nHere, ft is the cell state forget gate, \u02dcct the cell state update\ncandidate, gt the cell state update candidate gate, ot the out-\nput gate, and \u03c3 the sigmoid function applied elementwise.\nFirst, we show that one single LSTM layer can implement\nlinear self-attention, by using gt+1 \u2299 \u02dcct+1 as a way to com-\npute key-values and c to aggregate them, ft+1 and use ot+1\nfor the query. We provide the corresponding weights in the\ntable below, ignoring all the nonlinearities except \u03c3 in the f\ncomputation. Note that, compared to our simplified gated\nRNN class, we do not need to include neurons that forget\ntheir last state (\u03bb = 0) here as the output gate directly pro-\nvides the query to the output. Finally, linearizing the tanh\nfunction requires small Uc weights that can later be com-\npensated by large decoder weights, and ways to linearize\nthe sigmoid were discussed in the previous section.\nImplementing a gated RNN as in Equation 2 can be done\nby using two layers: in the first layer gt+1 \u2299 \u02dcct+1 serves\nas input gating, ft+1 corresponds to \u03bb, and, in the second\nlayer, gt+1 \u2299 \u02dcct+1 serves as output gating. Table 4 provides\none set of such weights. This ignores the linearization trick\nfor the tanh in \u02dcc and the sigmoid in gt+1.\nB.4. GRU\nA GRU cell (Cho et al., 2014) has a hidden state ht, updated\nthrough\nrt+1 = \u03c3(Urxt+1 + Vrht + br)\n(40)\n\u02dcht+1 = tanh(Uhxt+1 + Vh(rt+1 \u2299 ht) + bh)\n(41)\nzt+1 = \u03c3(Uzxt+1 + Vzht + bz)\n(42)\nht+1 = (1 \u2212 zt+1) \u2299 ht + zt+1 \u2299 \u02dcht+1\n(43)\nwhere rt is the reset gate, zt is the update gate, \u02dcht the update\ncandidate, and \u03c3 is the sigmoid function.\nHere, stacking multiple GRUs on top of each other does not\nenable the implementation of any network from our class\nof gated RNNs nor linear self-attention layers. One layer\ncan implement diagonal linear recurrence by linearizing the\ntanh, having zt+1 = 1 and rt+1 = \u03bb. However, implement-\ning a gating mechanism of the form g(x) = (Wmx \u2299 Wxx)\nis not possible1: we would need to use zt+1 to implement\n1When the tanh is replaced by Id, it is possible to achieve so\none branch of the gating and \u02dcht+1 the other but, given that\nzt+1 \u0338= 0, the previous hidden state ht influence the result.\nB.5. Can linear self-attention implement gated\nrecurrent networks?\nThroughout the paper, we mainly focus on understand-\ning whether diagonal gated RNNs implement linear self-\nattention. In this section, we ask the opposite question: can\nlinear self-attention layers can implement gated recurrent\nnetworks. The answer is that attention layers as we defined\nin Section 2.1 cannot, because it can only perfectly integrate\ninputs or send the current one (thus \u03bb = 0 or \u03bb = 1). How-\never, adding a mechanism akin to weight decay bridges the\ngap. In particular, we will describe how the output yt of\na such a linear self-attention layer can satisfy a recurrence\nrelationship of the form yt+1 = \u03bb \u2299 yt + xt. To do so, we\nconsider the following attention layer:\nvt = WV xt + bV\n(44)\nkt = WKxt + bK\n(45)\nqt = WQxt + bQ\n(46)\nyt =\n \nt\nX\nt\u2032=1\n\u0393t\u2212t\u2032 \u2299 (vt\u2032k\u22a4\nt\u2032 )\n!\nqt\n(47)\nwhere \u0393t\u2212t\u2032 is a matrix of size d \u00d7 d in which all entries of\nthe i-th row have value (1\u2212\u03b3i)t\u2212t\u2032. Such a layer is featured\nin recent work, e.g. (Sun et al., 2023) or (Yang et al., 2023).\nThe \u03b3 term can be interpreted as a weight decay: if we note\nW ff\nt :=\n \nt\nX\nt\u2032=1\n\u0393t\u2032\u2212t \u2299 (WV xt\u2032)(WKxt\u2032)\u22a4\n!\n,\n(48)\nwe have\nW ff\nt+1 = W ff\nt +(WV xt+1+bV )(WKxt+1+bK)\u22a4\u2212\u03931W ff\nt .\n(49)\nNow, we set the value, key and query matrices and biases to\nWV = Id, bV = 0, WK = 0, bK = 1, WQ = 0, bQ = 1/d\nand 1 \u2212 \u03b3 = \u03bb. This way, we have\nyt+1 = 1\ndW ff\nt+11\n(50)\n= 1\nd\n\u0000\u03931 \u2299 W ff\nt + xt+11\u22a4\u0001\n1\n(51)\n=\n\u0000\u03931 \u2299 W ff\nt\n\u0001\n1 + xt+1\n(52)\n= \u03bb \u2299 yt + xt+1\n(53)\nIn the last line, we use the structure of \u03931 and the value of\n\u03b3. Biases terms are crucial to make this link: without them\nW ff\nt would be a polynomial with only degree 2 coefficients\nby having ht \u226a \u02dcht+1 and correcting for the exponential growth\nin the next layer.\n15\nGated recurrent neural networks discover attention\nLayer 1\nU\nV\nb\nf\n0\n0\n+\u221e\n\u02dcc\n\u02dcWK\n0\n0\ng\n\u02dcWV\n0\n0\no\n\u02dcWQ\n0\n0\nLayer 1\nLayer 2\nU\nV\nb\nU\nV\nb\nf\n0\n0\n\u03c3\u22121(\u03bb)\n0\n0\n\u2212\u221e\nc\nW in\nm\n0\n0\nW out\nm\n0\n0\ng\nW in\nx\n0\n0\nW out\nx\n0\n0\no\n0\n0\n+\u221e\n0\n0\n+\u221e\nTable 4. LSTM weight configuration that matches a linear self-attention layer (left) and a gated RNN as in Equation 2 (right). This\npresumes that the activation functions in \u02dcc, g and o are linear. We use \u02dc\nW to denote the value, key and query matrices transformed in a\nsimilar way to what we did in Figure 1.\nand the equivalence would not be possible. The gating\nmechanism within networks described in Equation 2 can\nalso be implemented by forgetting (1 \u2212 \u03b3 = 0) and having\nthe key-value taking care of the multiplication.\nThis analysis reveals the importance of weight decay to im-\nplement recurrent neural network like computations with a\nwide range of timescales. Adding complex-valued weight\ndecay to linear self-attention layers makes them closer to\nstate-of-the-art recurrent neural networks architecture (Orvi-\neto et al., 2023b; Smith et al., 2023) for capturing long-range\ndependencies. Therefore, such a modification might boost\nthe performance of attention layers on benchmarks testing\nthese properties, such as the Long Range Arena (Tay et al.,\n2020). Interestingly, this view can partly explain the great\nempirical performance of the RWKV (Peng et al., 2023),\nwhich features a similar mechanism to weight decay. Over-\nall, the analysis we conducted in this section examplify how\nthe connection between RNNs and attention layers we made\nin this paper can be used to guide development of future\narchitectures.\nC. Teacher-student\nC.1. Experimental details\nFor all experiments in Section 4, we train the student for al-\nmost one million training iterations on sequences of length\n32 and a batch size of 64 (50000 training examples per\nepoch, 1000 epochs). We use the AdamW (Loshchilov &\nHutter, 2019) optimizer with a cosine annealing learning\nrate scheduler. The initial learning rate is set at 10\u22123, sched-\nuled to anneal down to 10\u22126 by the end of training and a\nweight decay of 10\u22124 is applied to all parameters except the\nrecurrent ones \u03bb in the experiment of Section 4.1. To ensure\nthat the hidden states do not explode, we ensure that \u03bb stays\nwithin [0, 1] by employing the exponential parametrization\ndescribed in Appendix B.1 (we only keep the \u03bd part as \u03bb\ntakes real values here).\nIn Figure 6, we add more results to the architecture compar-\nison we did in Figure 4. In particular, we compare the three\ndifferent types of LRU we mentioned in Appendix B.1, and\nobserve that adding an input GLU improves LRUs ability to\nmimic linear self-attention within one layer, but also with\nseveral layers.\nC.2. Compression of the learned output gating weights\nIn Figure 2, we show that the gating weight matrices have a\nstructure that is close to the one of our construction, except\nfor three different rows (11, 12, and 13). We claim they\ncan be reduced to a single row; we now provide details\njustifying it.\nTherefore, our objective is to demonstrate that these three\nrows are functionally equivalent to a single row with the\nexpected structure and to gain insights into the invariances\ninherent to the gating mechanism we study in this paper\nalong the way. The initial step toward achieving this entails\nexamining the influence of these three rows on the i-th\ncoordinate of the network\u2019s output:\n13\nX\nj=11\nDi,jgout(h)j =\n13\nX\nj=11\nDi,j(W out\nm,jx)(W out\nx,j x)\n(54)\n= x\u22a4\n\uf8eb\n\uf8ed\n13\nX\nj=11\nDi,jW out\nm,j W out\nx,j\n\u22a4\n\uf8f6\n\uf8f8 x.\n(55)\nThis contribution can be interpreted as a quadratic form\nwhose kernel is a weighted sum of rank-1 kernels defined\nby the rows of the output gating matrices. In Figure 2.C, we\nplot the obtained kernel for one of the output components.\nCrucially, the resulting kernel for the four output units are all\nproportional to one another and is of rank-1. We can thus re-\nduce the three neurons (11, 12 and 13) to one. Furthermore,\nthe two vectors whose outer product yields the resulting\nkernel now mirror the construction\u2019s structure. One of these\ntwo vectors exclusively accesses query neurons while the\nother reads key-value neurons, as seen in Figure 2.C. As\nusually occurs with this kind of manipulation (Martinelli\net al., 2023), merging the neurons slightly increases the loss,\nbut original loss levels can be recovered after fine-tuning.\n16\nGated recurrent neural networks discover attention\nFigure 6. Extensive comparison between the different architectures. Compared to Figure 4, we consider different versions of the LRU\nhere, plot the loss as the function of the number of parameters, and include both training and validation losses. Those two losses are\nalmost (up to some sampling noise) for the teacher-student task but are different for the in-context linear regression task because we\nchange the W \u2217 distribution in the validation set.\n17\nGated recurrent neural networks discover attention\nD. In-context linear regression\nD.1. Experimental details\nIn the in-context linear regression experiment, each se-\nquence is a task characterized by a unique W \u2217. The weight\nmatrix W \u2217 entries are sampled i.i.d. from a normal dis-\ntribution N(0, 1\n3). Each element of the sequence is of the\nform (xt, W \u2217xt). The entries of the inputs (xt)T +1\nt=1 are\nsampled i.i.d. from the uniform distribution U(\u2212\n\u221a\n3,\n\u221a\n3).\nDuring the validation phase, we draw tasks from a different\ndistribution, W \u2217\nij \u223c N(0, 2\n3) to highlight the generalization\nabilities of the learned models. We train the model with\nthe same optimization scheme described in Appendix C.1,\nexcept that we use a smaller number of training iterations,\ntotaling 300, 000. By default, we use gated RNNs with 80\nhidden neurons.\nD.2. Optimal learning rate for one-step gradient descent\nLet X \u2208 Rdx\u00d7n, W \u2208 Rdy\u00d7dx random variables such that\nall entries of X are sampled i.i.d. from a centered uniform\ndistribution with variance \u03c32\nx, and those of W i.i.d. from\nsome centered distribution with finite variance \u03c32\nW . We set\nY = WX. Let x \u2208 Rdy a column vector, whose entries\nare sampled from the same distribution as those of X, and\ny = Wx.\nThe goal of this section is to analytically derive the optimal\nlearning rate for the in-context linear regression task, that is\nto find \u03b7 which minimizes\nL(\u03b7) = 1\n2EX,W,Y,x,y\nh\n\u2225y \u2212 \u02c6W(\u03b7, X, Y )x\u22252i\n(56)\nwhere \u02c6W(X, Y ) is the result of one gradient descent step\nstarting from 0 with learning rate \u03b7 on the loss W 7\u2192 1\n2\u2225Y \u2212\nWX\u22252. The calculation is presented in a more general\nform in (Mahankali et al., 2023). We include it here as\nwe additionally provide a simple formula for exact optimal\nlearning rate value.\nPlugging in the analytical expressions for y and \u02c6W, we get\nL(\u03b7) = 1\n2EX,W,Y,x,y\n\u0002\n\u2225y \u2212 \u03b7Y X\u22a4x\u22252\u0003\n(57)\n= 1\n2EX,W,x\n\u0002\n\u2225Wx \u2212 \u03b7WXX\u22a4x\u22252\u0003\n(58)\n= 1\n2EX,W,x\n\u0002\n\u2225W(I \u2212 \u03b7XX\u22a4)x\u22252\u0003\n(59)\nWe want to minimize L, i.e.\nlook for \u03b7\u2217 that satisfies\n\u2202\u03b7L(\u03b7\u2217) = 0. We have\n\u2202\u03b7L(\u03b7) = EX,W,x\nh\u0000W(I \u2212 \u03b7XX\u22a4)x\n\u0001\u22a4 WXX\u22a4x\ni\n(60)\n= Tr EX,W,x\n\u0002\n(I \u2212 \u03b7XX\u22a4)W \u22a4WXX\u22a4xx\u22a4\u0003\n(61)\n= \u03c32\nx Tr EX,W\n\u0002\n(I \u2212 \u03b7XX\u22a4)W \u22a4WXX\u22a4\u0003\n(62)\n= \u03c32\nx Tr EX,W\n\u0002\nXX\u22a4(I \u2212 \u03b7XX\u22a4)W \u22a4W\n\u0003\n(63)\n= \u03c32\nx\u03c32\nW Tr EX\n\u0002\nXX\u22a4(I \u2212 \u03b7XX\u22a4)\n\u0003\n(64)\nIn the first equation, we use that E[a\u22a4b] = Tr E[ba\u22a4].\nThird and fifth ones make use of Ex[xx\u22a4] = \u03c32\nxId and\nEW [WW \u22a4] = \u03c32\nW Id. Having \u2202\u03b7L(\u03b7\u2217) = 0 is then equiva-\nlent to\n\u03b7\u22c6 :=\nTr EX[XX\u22a4]\nTr EX[XX\u22a4XX\u22a4].\n(65)\nThis result shows that only the distribution of the learn-\ning data matters.\nLet us compute this quantity.\nWe\nhave EX[XX\u22a4] = n\u03c32\nxId so we are left with computing\nEx[XX\u22a4XX\u22a4]. Using that entries of X are i.i.d., we get\nTr EX[XX\u22a4XX\u22a4]\n(66)\n= dxEX\n\uf8ee\n\uf8f0X\ni\n X\nt\nxi,tx1,t\n!2\uf8f9\n\uf8fb\n(67)\n= dxEX\n\uf8ee\n\uf8f0\n X\nt\nx2\n1,t\n!2\uf8f9\n\uf8fb\n(68)\n+ dx(dx \u2212 1)EX\n\uf8ee\n\uf8f0\n X\nt\nx1,tx2,t\n!2\uf8f9\n\uf8fb\n(69)\n= dxEX\n\uf8ee\n\uf8f0X\nt\nx4\n1,t +\nX\nt\u0338=t\u2032\nx2\n1,tx2\n1,t\u2032\n\uf8f9\n\uf8fb\n(70)\n+ dx(dx \u2212 1)EX\n\"X\nt\nx2\n2,tx2\n1,t\n#\n(71)\n= 9\n5ndx\u03c34\nx + n(n \u2212 1)dx\u03c34\nx + n(dx \u2212 1)\u03c34\nx\n(72)\n= ndx\u03c34\nx\n\u0012\nn + dx \u2212 1\n5\n\u0013\n(73)\nbecause the fourth moment of a centered uniform distribu-\ntion is 9\n5\u03c34\nx. Putting everything together, we finally have\n\u03b7\u2217 =\n1\n\u03c32x(n + dx \u2212 1\n5).\n(74)\n18\nGated recurrent neural networks discover attention\nD.3. Associative recall\nAs a complement to in-context linear regression, we con-\nsider a simple in-context classification task studied by Fu\net al. (2023), where the network has to remember associa-\ntions between paired inputs. As for in-context regression,\nthe network is presented with a sequence of tokens of the\nform (xt, yt)1\u2264t\u2264T , followed by a token containing a query\ninput and a null placeholder (xT +1, 0). In this task, xT +1\ncorresponds exactly to one of the previously seen xt, and the\ngoal is to complete the placeholder with the corresponding\nyt.\nTo make the task solvable by a single layer of\nlinear attention, we present the following sequence:\n([x1, y1], [y1, x2], [x2, y2] . . . , [xT , yT ], [xT +1, 0]), where x\n(resp y) have been transformed to a 2T-sized one-hot en-\ncoding of [1, T] (resp. [T + 1, 2T]), resulting in a input\ndimension of 2T. Each x and each y only appear once. We\nuse a cross entropy loss, using the desired y as target, and\nT = 8 in our experiments.\nSolving the task with linear self-attention. Given that we\nprovide non-repeating one hot encoded inputs, we can see\nthat a linear self-attention layer that uses x as key and query,\nand y as value will solve the task. That is, its output is\nyT +1 =\n\uf8eb\n\uf8edX\nt\u2264T\nytx\u22a4\nt\n\uf8f6\n\uf8f8 xT +1.\n(75)\nInput-output gating. We first trained a gated RNN on this\ntask and observe that the solution it finds differs from the\nlinear self-attention layer, and requires way less recurrent\nneurons. To each y, it associates a recurrent neuron with\n\u03bb = 1 in which it will store a value vx corresponding to x\nwhen that y appears. That is, if the pair (x, y) appears, the\nrecurrent neuron associated to y receives vx as input, and\nthe other receive no input. Addtionally, the RNN uses one\nneuron with \u03bb = 0 containing the value vx associated to\nthe current x. The output gating then computes the nega-\ntive squared difference between the current value and the\nstored ones, so that neural activity after gating is equal to\n(\u2212(vxT +1 \u2212 vx(y))2)y where x(y) is the x that was associ-\nated to y in the sequence. The index of the smallest one,\nequal to 0, gives the desired output after taking the argmax.\nWe note that such a solution is possible as each x and y\nappear only once in the sequence, as this is a classification\nclass and as inputs are one-hot encoded.\nSide gating. Then, we use the RNN with side gating of\nSection A.2, with parameters W in\nx , W in\nm , \u03bb and W side, and\ncheck whether it implements the same function as the RNN\nwith input-output gating. It does not, and we detail the\nsolution it finds in the following. We apply the same post\nprocessing of the weights as we did in Section 4, and find\nthat only recurrent neurons with \u03bb = 1 are remaining. Con-\nsistently with the linear self-attention layer that optimally\nsolves this task, one of the input gating matrix, W in\nx on\nreads out from the x part of the input, and the other one,\nW in\nm from y. Additionally, the side gating matrix is equal\nto the W in\nx matrix, in a similar way that the query matrix is\nequal the key one in the linear self-attention layer. Finally,\nthe D matrix is the transpose of the value-like part of matrix\nW in\nm . Based on those observations, we can rewrite\nW in\nx = W side = [A | 0]\n(76)\nW in\nm = [0 | B]\n(77)\nD = B\u22a4\n(78)\nAs \u03bb = 1, we have\nhT =\nX\nt\u2264T\ngin([xt, yt]) =\nX\nt\u2264T\n(Byt) \u2299 (Axt)\n(79)\nand\nyT +1 = hT +1 \u2299 (WxT +1)\n(80)\n= B\u22a4 X\nt\u2264T\n(Byt) \u2299 (Axt) \u2299 (AxT +1)\n(81)\n=\nX\nt\u2264T\nM(xt, yt)xT +1\n(82)\nIn the last equation, we remarked that yT +1 is a linear func-\ntion of xT +1 so that we can write it as a matrix, and this\nmatrix a sum of matrices that depend linearly on xt and yt.\nWe can now compare the behavior of this solution, with the\nsolution found by linear self-attention, by looking in more\ndetail into the M matrices. We first observe that M and\n(x, y) 7\u2192 yx\u22a4 are bilinear so that it is enough to study their\nbehavior on the canonical basis (ui)i. We plot those differ-\nent matrices on Figure 7. We observe that each component\nis of rank 1 similarly to the self-attention layer solution,\nwith a peak on the component (i, j) as expected. However,\nthere is an additional negative peak, of same amplitude as\nthe positive one, that does not affect the prediction as we are\ndealing with one-hot encoded inputs and outputs and a clas-\nsification task. One putative reason to explain explain these\nobservations is that, as the patterns are one-hot encoded,\nit is possible to represent them in log T neurons, without\naffecting classification performance. This would require\nless neurons in the output gating as what the link with atten-\ntion would, and can be compensated with the kind of binary\npatterns we observe. Alternatively, binary patterns do not\ncover all directions from the input space so identification\nmight become more difficult.\nE. Software\nWe run our experiments using the Jax (Bradbury et al., 2018)\nPython framework, using the Flax (Heek et al., 2023) library\n19\nGated recurrent neural networks discover attention\ny = u1\nx = u1\nx = u2\nx = u3\nx = u4\nx = u5\nx = u6\nx = u7\nx = u8\ny = u2\ny = u3\ny = u4\ny = u5\ny = u6\ny = u7\ny = u8\nFigure 7. Values taken by the M(x, y) when x and y are equal to the canonical basis. The obtained matrices are all of rank 1.\n20\nGated recurrent neural networks discover attention\nfor neural networks. We base our code base on the Minimal-\nLRU (Zucchet et al., 2023a) repository. Data analysis and\nvisualization were done using Numpy (Harris et al., 2020),\nScikit-learn (Pedregosa et al., 2011) and Matplotlib (Hunter,\n2007).\n21\n"
  },
  {
    "title": "Contrastive Feature Masking Open-Vocabulary Vision Transformer",
    "link": "https://arxiv.org/pdf/2309.00775.pdf",
    "upvote": "6",
    "text": "Contrastive Feature Masking Open-Vocabulary Vision Transformer\nDahun Kim\nAnelia Angelova\nWeicheng Kuo\nGoogle DeepMind\nAbstract\nWe present Contrastive Feature Masking Vision Trans-\nformer (CFM-ViT) - an image-text pretraining methodology\nthat achieves simultaneous learning of image- and region-\nlevel representation for open-vocabulary object detection\n(OVD). Our approach combines the masked autoencoder\n(MAE) objective into the contrastive learning objective\nto improve the representation for localization tasks. Un-\nlike standard MAE, we perform reconstruction in the joint\nimage-text embedding space, rather than the pixel space as\nis customary with the classical MAE method, which causes\nthe model to better learn region-level semantics.\nMore-\nover, we introduce Positional Embedding Dropout (PED)\nto address scale variation between image-text pretraining\nand detection finetuning by randomly dropping out the po-\nsitional embeddings during pretraining. PED improves de-\ntection performance and enables the use of a frozen ViT\nbackbone as a region classifier, preventing the forgetting\nof open-vocabulary knowledge during detection finetuning.\nOn LVIS open-vocabulary detection benchmark, CFM-ViT\nachieves a state-of-the-art 33.9 APr, surpassing the best ap-\nproach by 7.6 points and achieves better zero-shot detection\ntransfer. Finally, CFM-ViT acquires strong image-level rep-\nresentation, outperforming the state of the art on 8 out of 12\nmetrics on zero-shot image-text retrieval benchmarks.\n1. Introduction\nThe ability to detect a vast array of objects in the real\nworld is fundamental to computer vision and machine learn-\ning. This powers a wide range of applications from au-\ntonomous agents to search engines. Unfortunately, to date\nmost modern object detectors rely on manually annotated\nregions and class labels, which is labor-intensive and im-\npractical to scale beyond the order of 103 categories.\nA new task called open-vocabulary detection (OVD) has\nbeen introduced to address the vocabulary limitation in ob-\nject detection by using image-text pairs for training and\ntext queries from users at test time [65]. Open-vocabulary\ndetectors represent categories as text embeddings rather\nthan discrete class labels, allowing them to predict objects\nPretraining: contrastive feature masking\n(a)\n(b)\n(c)\nFinetuning: open-vocabulary detection\n\u201cTwo children are walking by a pond\u201d\n\u201cChristmas lights hanging on the ceiling\u201d\ntext \nenc\ncontrastive\nfeature\nreconstruction\ntext\nimage\nopen-vocab detection\n(finetune)\n(a)\ninit. with pretrained ViT\njoystick: 68%\njoystick: 44%\njoystick: 47%\npaperback book: 44%\nsofa bed: 61%\n(b)\nimage\nmask\n(d)\ndec\nViT\n(frozen)\nViT\nViT\n(finetune)\ndet head\n(e)\nFigure 1: We propose CFM-ViT to pretrain vision transformers to\ncapture more pixel and region information for open-vocabulary de-\ntection. CFM-ViT predicts masked contrastive features on top of\nthe contrastive image-text pretraining. (Top) We visualize (c) the\nsimilarity map between (d) the reconstructed image features (see\ntop left) and (e) the query text embedding. CFM-ViT correctly pre-\ndicts the (c) whole-image semantics from (b) heavily truncated im-\nages. (Bottom) Our open-vocabulary detector exploits the frozen\nViT backbone to retain pretrained knowledge and is able to detect\nbase and novel object classes (only novel classes are shown).\nunavailable during training.\nVarious techniques, such as\nknowledge distillation [18, 13], weak supervision [74], self-\ntraining [71, 49, 68], and frozen backbone [33], have been\nsuggested. Typically, CNN backbones are utilized in these\napproaches. As vision transformers have gained significant\ntraction in image understanding [12, 66, 21, 3], it is crucial\nto explore open-vocabulary detectors based on vision trans-\nformers [42]. Moreover, to our knowledge, most current\nOVD research assumes the availability of pretrained Vision-\narXiv:2309.00775v1  [cs.CV]  2 Sep 2023\nLanguage Models (VLMs) (e.g. CLIP [47]), and proposes\nadaptation or finetuning techniques to overcome the dispar-\nity between image-level pretraining and object-level fine-\ntuning [18, 13, 71, 68, 49]. However, as these VLMs are\ntypically optimized for image-level tasks such as classifica-\ntion and retrieval, they do not adequately utilize the pixel-\nand region-level information during pretraining, which is\ncrucial for downstream open-vocabulary detection.\nWe present CFM-ViT (Contrastive Feature Masking Vi-\nsion Transformer), a simple framework to pretrain vision\ntransformers to capture more detailed pixel/region infor-\nmation for open-vocabulary object detection (Fig. 1). In-\nspired by MAE [21], we adopt the concept of masked\nauto-encoding to enhance object representation during pre-\ntraining. However unlike MAE, we perform prediction in\nthe joint image-text embedding space rather than the pixel\nspace as an auxiliary objective to the contrastive image-\ntext learning. This additional objective provides orthogo-\nnal signal from the contrastive learning, and benefits down-\nstream detection task without compromising the image-\nlevel tasks.\nIn addition, we propose Positional Embed-\nding Dropout (PED) to address overfitting to the typically\nlower-resolution and object-centric pretraining data.\nBy\nrandomly dropping out positional embeddings during pre-\ntraining, PED aids the model to learn more robust repre-\nsentations that better generalize to high-res detection data.\nMoreover, PED enables the use of a frozen ViT encoder\nas an open-vocabulary region-classifier, which prevents the\nforgetting of open-vocabulary knowledge at detection.\nWe evaluate CFM-ViT on the widely used LVIS and\nCOCO open-vocabulary detection benchmarks. Our top-\nperforming model obtains 33.9 APr on LVIS, surpassing\nthe previous best approach by 7.6 APr at system level. On\nthe COCO benchmark, CFM-ViT represents the first ViT-\nbased model and achieves a very competitive novel AP\nwithout using pseudo labels or weak supervision. Although\nnot optimized for retrieval, CFM-ViT outperforms the state-\nof-the-art methods of similar or larger capacity on 8 out of\n12 image-text retrieval benchmark metrics. In summary:\n\u2022 We present an image-text pretraining methodology\n(CFM-ViT) to learn localization cues for open-\nvocabulary detection by contrastive feature masking.\n\u2022 We propose Positional Embedding Dropout (PED) to\nbridge the gap between image-text pretraining and\ndetection finetuning, which enables the use of a\nfrozen ViT encoder to prevent the forgetting of open-\nvocabulary knowledge during detection finetuning.\n\u2022 CFM-ViT achieves state-of-the-art APr on LVIS open-\nvocabulary detection benchmark, shows very compet-\nitive performance on COCO and zero-shot transfer to\nObjects365, and outperforms the SOTA on 8 out of 12\nmetrics of zero-shot image-text retrieval benchmarks.\nWe hope these discoveries would encourage the community\nto explore open-vocabulary detection from the perspective\nof image-text pretraining.\n2. Related Works\nLanguage-supervised\nopen-vocabulary\nrecognition.\nLearning representation for open-vocabulary recognition\nis a hallmark of general intelligence.\nEarly pioneering\nworks such as DeViSE [16] and ConSE [43] used deep\nconvolutional networks to construct a shared image-text\nembedding space for zero-shot recognition.\nTo leverage\nthe co-occurrence of image and text in raw internet\ndata,\nresearchers have explored various data sources\nsuch as image tags [4, 9, 30], captions [8, 24, 50, 55],\nalt-texts [29, 51], image search queries [47], page title [5],\nor a combination of these sources [5]. From a modeling\nperspective, contrastive learning has become a popular\nparadigm because of its simplicity, scalability, and versa-\ntility in zero-shot, few-shot, and full finetuning transfer\nsettings [46, 47, 39, 10, 36]. While most of these works fo-\ncus on image-level understanding, we explore the learning\nof region-level information in the image-text pretraining,\nwhich is essential for open-vocabulary detection task.\nSelf-supervised object representation learning.\nScaling\nup annotation for detection presents a significant challenge.\nAs a result, many efforts have been made to learn ob-\nject representations in a self-supervised manner. These ap-\nproaches can be broadly categorized as contrastive or gen-\nerative.\nThese contrastive approaches typically use slid-\ning windows [59], object proposals [57, 25], or point sam-\nples [1] for pixel or region-level contrastive learning. Gen-\nerative methods use masked image modeling with recon-\nstruction targets such as pixels\n[21], low-level [3, 56] /\nhigh-level image features [6, 73], or combine with the con-\ntrastive objective [27]. By learning to restore masked im-\nages, the model needs to learn about objects and regions.\nHowever, although these self-supervised methods are suited\nfor localization tasks, they lack the necessary image-text\nlearning for open-vocabulary recognition.\nSome recent\nworks [58, 45, 67, 26, 14] utilize off-the-shelf CLIP fea-\ntures [47] as prediction targets to enhance masked image\nmodeling by two-stage training. In this work, we propose\na novel approach to combine generative self-supervised\nlearning jointly with contrastive image-text learning in a\nsingle end-to-end training stage. While some concurrent\nworks have explored similar objectives for zero-shot image-\nlevel tasks or fully supervised finetuning [11, 60, 54], our\nfocus is on open-vocabulary detection.\nOpen-vocabulary object detection and segmentation.\nZero-shot detection aims to enhance detection models be-\nyond their limited training categories by aligning region\nvisual representation and category word embeddings [2,\n48, 7, 69] or generating visual features with a genera-\ntive model [20, 75]. Open-vocabulary detection [65] im-\nproves upon zero-shot detection by incorporating image-\ntext supervision about the novel categories. With the ad-\nvent of image-text pretraining, numerous studies have ex-\nplored adapting these pretrained models to open-vocabulary\ndetection and segmentation [18, 71, 17, 35, 72]. For in-\nstance, ViLD [18] distills image-text knowledge into the\ndetector, while DetPro [13] improves ViLD by category\nprompt optimization. Additionally, region-text self-training\nhas been demonstrated on image caption data [71], classi-\nfication data [49], and unlabeled data [68]. Phrase ground-\ning [37], weak supervision [74], and frozen model [33] ap-\nproaches have also been explored. Most methods rely on\nCNN backbones, but vision transformers are gaining mo-\nmentum [42, 72, 31, 34, 38]. While previous studies have\nfocused on finetuning or adaptation strategies for pretrained\nmodels, ours seeks to improve the image-text pretraining by\npredicting the masked representation of vision transformer.\n3. Method\nWe tackle the problem of open-vocabulary object detec-\ntion. During training, the model can access the detection\nlabels of base categories, but at the inference phase, it must\nbe able to detect objects from a set of novel categories. To\nachieve this, we utilize pretrained vision and language mod-\nels (VLMs) following previous works [18, 71, 33]. How-\never, instead of taking off-the-shelf pretrained VLM, we\ndemonstrate how to better pretrain VLMs with vision trans-\nformers [12] for open-vocabulary detection.\n3.1. Preliminaries: Overall Pipeline\nPretraining.\nWe adopt a dual-encoder image-text con-\ntrastive model widely used in existing works [47, 29]. The\nimage embeddings {v} and text embeddings {l} are ob-\ntained by global average pooling at the last layers of image\nand text encoders. The cosine similarity of the embeddings\nin batch B, scaled by a learnable temperature \u03c4 are the in-\nput to the InfoNCE loss [44, 47]. The image-to-text (I2T)\ncontrastive loss is formulated as:\nLI2T = \u2212 1\nB\nB\nX\ni=1\nlog(\nexp(vili/\u03c4)\nPB\nj=1 exp(vilj/\u03c4)\n).\n(1)\nThe text-to-image (T2I) contrastive loss is symmetrical\nwith the I2T loss by exchanging the inner/outer summa-\ntion loops. The total contrastive loss Lcon is obtained by\nLcon = (LI2T + LT2I)/2.\nDownstream open-vocabulary detection.\nOur open-\nvocabulary detection algorithm follows existing works [65,\n18, 33, 31]. At training, for each detected region i, its region\nembedding is the RoI-Align feature. The detection score pi\nis the cosine similarity between the region embedding and\ntext embeddings of CB followed by a softmax. Note the\ntext embeddings are computed from the same text encoder\nfrom the image-text pretraining. At test time, the text em-\nbeddings are expanded from the CB to CB \u222a CN plus the\n\u201cbackground\u201d embedding. We also extract VLM embed-\nding of region i by RoI-Align at the last feature map of the\nViT backbone. The VLM score zi is the cosine similarity\nwith the CB \u222aCN text embeddings. Similarly, the detection\nscore pi is now computed with CB \u222a CN text embeddings.\nAn object detector for open-vocabulary scenarios is\ntrained on the labels of base categories CB, but must be\ncapable of detecting the union of base and novel categories\n(CB \u222a CN) at test time. Following existing works [65, 18],\nwe replace the fixed-size classifier layer with the text em-\nbeddings of base categories. The same text encoder from\nthe image-text pretraining is used to compute the text\nembeddings to maintain the pretrained open-vocabulary\nknowledge. The \u201cbackground\u201d phrase represents the back-\nground category, and the proposals not matched to any CB\nannotations are labeled as background.\nThe ensemble open-vocabulary detection score siens is\nobtained by geometric means [18, 33]:\nsi\nens =\n(\nz(1\u2212\u03b1)\ni\n\u00b7 p\u03b1\ni\nif i \u2208 CB\nz(1\u2212\u03b2)\ni\n\u00b7 p\u03b2\ni\nif i \u2208 CN\n(2)\n, where \u03b1, \u03b2 \u2208 [0, 1] control the weights for base and novel\ncategories. The background score comes directly from the\ndetection score pi, because the VLM score with \u201cback-\nground\u201d phrase tends to be not as reliable.\n3.2. Contrastive Feature Masking\nOur method performs reconstruction in the joint image-\ntext embedding space (see Fig. 2-left) as an auxiliary objec-\ntive to the contrastive image-text learning (in Sec. 3.1).\nMasked feature reconstruction.\nFollowing MAE [22],\nwe randomly mask a large portion of image tokens (e.g.,\nmask ratio 75%) for representation learning. However un-\nlike MAE, we predict the joint image-text embedding in-\nstead of the raw pixels to encourage better learning of se-\nmantics. Specifically, the output features {f} of the con-\ntrastive image encoder before the global average pooling is\nour reconstruction target. We use the cosine distance be-\ntween the reconstructed features { \u02c6f} and unmasked image\nfeatures {f} as loss function. Let M be the set of masked\npatch indices, and our reconstruction loss Lrec is computed\nonly on the masked tokens as:\nLrec = 1 \u2212 1\nB\nB\nX\ni=1\n( 1\n|M|\nX\nk\u2208M\nf \u00b7 sg( \u02c6f)\n\u2225f\u2225 \u00b7 \u2225sg( \u02c6f)\u2225\n),\n(3)\nwhere |M| is the number of masked tokens and sg denotes\nstop gradient. The total CFM-ViT loss is Lcon + Lrec.\nViT\nencoder\ndecoder\nViT\nencoder\ncontrastive\nloss\ntext\nimage patches\nfeature \nreconstruction loss\nPositional Embedding Dropout\nmasking\ntext\nencoder\nPretraining by contrastive feature masking\nGAP\n!\ud835\udc53\n\ud835\udc53\n\ud835\udc63\n\ud835\udc59\nweight \nshared\n+ positional embeddings\ntraining paths\ninference paths\nViT\nencoder\n(finetune)\ndetector \nheads\nimage patches\n+ positional embeddings (upsampled)\nRoI Align\nRoI Align\ndetected \nregions\nbase class \nembeddings\nnovel class \nembeddings\nregion \ndetection \nscore \ud835\udc5d\nregion \nVLM \nscore \ud835\udc67\n(test time only)\n(train and test time)\nregion-text similarity as \nOVD score \ud835\udc60\nDownstream open-vocabulary detection\nViT\nencoder\n(frozen)\ndetection loss\nFigure 2: CFM-ViT architecture: We present both the image-text pretraining (left) and open-vocabulary detection finetuning (right)\narchitecture of CFM-ViT. (Left) Building upon contrastive learning, we learn to reconstruct the masked tokens in the joint image-text em-\nbedding space. In addition, we propose Positional Embedding Dropout (PED) which randomly masks out the whole PE during pretraining\nto mitigate overfitting to the low-res positional embeddings, thus adapting better to the high-res downstream detection task. (Right) The\nopen-vocabulary detector is initialized with the pretrained ViT backbone during finetuning. The detected region embeddings match with\nthe cached category embeddings to compute the region scores. At inference, we exploit the frozen ViT backbone to obtain the VLM score\nz, which is combined with the detection score p into the open-vocabulary detection score s (Best viewed in color).\nOur reconstruction encoder is identical (weight-shared)\nto the contrastive image encoder, but applied only on the\nvisible, unmasked tokens (e.g., 25%). The decoder takes\nthe encoded visible tokens and learnable [mask] tokens\nadded with positional embeddings.\nFaster training by contrastive branch masking.\nThe\nfeature reconstruction branch adds a computation burden\n(e.g. 25%) to the pretraining depending on the masking ratio\n(e.g. 75%). We note that this cost can be waived by feeding\nonly the masked tokens (M) to the contrastive branch, so\nthat the input patches to the contrastive and reconstruction\nencoders are mutually exclusive, and yields the same re-\nconstruction target { \u02c6fk\u2208M}. Our ablation study in Table 5c\nshows that this technique maintains the training efficiency\nof contrastive learning, while still achieves significant gains\nover the baseline in open-vocabulary detection.\nPositional embedding dropout.\nIn vision transformer\nencoder, positional embeddings are added to all tokens af-\nter the first patchifying layer to provide the location of each\npatch in the image. While the positional embeddings work\nwell for image classification/retrieval, it tends to overfit\nto the lower-resolution object-centric images, and struggle\nwith higher-resolution images typically used by detection\ntask. In addition, the recognition of objects in detection oc-\ncurs at region- rather than image-level (e.g. see VLM scores\nzi for region i in Sec. 3.1), which causes difficulty for the\npositional embeddings trained only for image-level task.\nWe propose a simple yet effective technique called Po-\nsitional Embedding Dropout (PED) to address this problem\nby randomly masking out the whole positional embeddings\nduring training (e.g., with a probability 0.5). This teaches\nthe model not to rely heavily on the positional embeddings\nand thus can process the high-res images and perform bet-\nter region classification. PED not only outperforms both the\nbaseline and \u2018no positional embeddings\u2019 variants, but en-\nables the use of frozen vision transformer to achieve further\nimprovement in open-vocabulary detection.\n3.3. Open-vocabulary Detection\nAn object detector for open-vocabulary scenarios is\ntrained on the labels of base categories CB, but must be\ncapable of detecting the union of base and novel categories\n(CB \u222a CN) at test time (see Sec. 3.1 and Fig. 2-right).\nBaseline architecture.\nOur detector adopts the simple\nfeature pyramid and windowed attention to handle higher\nresolution images as proposed in ViTDet [40], and employs\nMask R-CNN heads and class-agnostic box regression and\nmask heads as in [13, 18, 65, 71, 33]. In addition, we lever-\nage a recent novel object proposal method [32] by replac-\ning the binary classification in the RPN with the centerness-\nbased objectness. The predicted objectness score oi is com-\nbined into the final OVD score as siOVD = oi \u00b7 siens.\nOur detector backbone is initialized with the pretrained\nViT in the VLM from Sec. 3.2, and is finetuned together\nmethod\npretrained\ndetector\nAPr\nAP\nmodel\nbackbone\nConvNet based:\nDetPro-Cascade [13]\nViT-B/32\nR-50\n20.0\n27.0\nDetic-CN2 [74]\nViT-B/32\nR-50\n24.6\n32.4\nRegionCLIP [71]\nR-50x4\nR-50x4\n22.0\n32.3\nViLD-Ens [18]\nViT-B/32\nR-152\n18.7\n26.0\nViLD-Ens [18]\nViT-L/14\nEffNet-B7\n21.7\n29.6\nViLD-Ens [18]\nEffNet-B7\nEffNet-B7\n26.3\n29.3\nVL-PLM [68]\nViT-B/32\nR-50\n17.2\n27.0\nOV-DETR [64]\nViT-B/32\nR-50\n17.4\n26.6\nRasheed et al. [49]\nViT-B/32\nR-50\n21.1\n25.9\nPromptDet [15]\nViT-B/32\nR-50\n21.4\n25.3\nViT based:\nOWL-ViT [42]\nViT-H/14\nViT-H/14\n23.3\u2217\n35.3\u2217\nOWL-ViT [42]\nViT-L/14\nViT-L/14\n25.6\u2217\n34.7\u2217\nCFM-ViT (ours)\nViT-B/16\nViT-B/16\n29.6\u2217\n33.8\u2217\nCFM-ViT (ours)\nViT-L/16\nViT-L/16\n35.6\u2217\n38.5\u2217\nCFM-ViT (ours)\nViT-B/16\nViT-B/16\n28.8\n32.0\nCFM-ViT (ours)\nViT-L/16\nViT-L/16\n33.9\n36.6\nTable 1: LVIS open-vocabulary object detection. CFM-ViT out-\nperforms the best existing approach by +7.6 APr, and the other\nViT-based approach [42] by +10.0 APr using the same backbone.\n\u2217: reports box AP.\nwith the newly added detector heads. Note we do not apply\npositional embedding dropout (PED) during finetuning as\nthe location information is critical in detection.\nBackbone learning rate.\nAs the pretrained knowledge in\nthe backbone is critical in recognizing novel categories, it\nis important to set the backbone learning rate so as to pre-\nvent forgetting in the finetuning phase. On the other hand,\nentirely freezing the backbone limits the ability to adapt to\ndetection tasks. We find that setting the backbone learning\nrate lower (e.g., 0.5\u00d7) than the rest of the detector layers\nshows advantage in the trade-off. After the detection train-\ning is done, we explore using the frozen ViT backbone at\ntest time, as described next.\nFrozen backbone inference\nWhile the ViT backbone\nadapts to the detection tasks, it tends to forget some of the\npretrained open-vocabulary knowledge. Therefore, for in-\nference, we propose to use a separate frozen ViT backbone\nas an open-vocabulary region classifier. Specifically, we\nuse the frozen backbone instead of the finetuned backbone\nwhen computing the region VLM score zi (Sec. 3.1). We\nfind it important for the frozen ViT to be pretrained with our\npositional embedding dropout (PED), to serve as a strong\nzero-shot region classifier. We show by experiments that\nincorporating the PED pretraining and frozen backbone in-\nference provides large gains in open-vocabulary detection.\n4. Experimental Results\nPretraining setup.\nFor the image-text pretraining, we use\nthe widely-used ViT-B/16 and ViT-L/16 as the image en-\nmethod\npretrained\ndetector\nnovel AP\nAP\nmodel\nbackbone\nConvNet based:\nViLD [18]\nViT-B/32\nR-50\n27.6\n51.3\nOV-DETR [64]\nViT-B/32\nR-50\n29.4\n52.7\nw/ pseudo box labels:\nXPM et al. [28]\nR-50\nR-50\n27.0\n41.2\nRegionCLIP [71] \u2020\nR-50x4\nR-50x4\n39.3\n55.7\nPromptDet [15]\nViT-B/32\nR-50\n26.6\n50.6\nVL-PLM [68]\nViT-B/32\nR-50\n34.4\n53.5\nRasheed et al. [49] \u2021\nViT-B/32\nR-50\n36.9\n51.5\nw/ weak supervision:\nDetic-CN2 [74]\nViT-B/32\nR-50\n24.6\n32.4\nViT based:*\nCFM-ViT (ours)\nViT-B/16\nViT-B/16\n30.8\n42.4\nCFM-ViT (ours)\nViT-L/16\nViT-L/16\n34.1\n46.0\nTable 2: COCO open-vocabulary object detection (box AP50).\nCFM-ViT represents the first ViT-based approach and demon-\nstrates a very competitive novel AP without using pseudo labeling\nor weak supervision. \u2020: RegionCLIP uses an off-the-shelf RPN\nduring its pretraining. \u2021: Rasheed et al. uses an external MViT de-\ntector [41] during pretraining. *: The other ViT-based method [42]\nreport their results on LVIS only.\ncoder, with an input image size of 224. We use the fixed 2D\nsinusoidal positional embeddings, and apply Positional Em-\nbedding Dropout (PED) with a drop probability of 0.5. The\nimage embedding is obtained by global average pooling at\nthe last ViT layer. The text encoder is a 12-layer Trans-\nformer as in [47, 62], with the input sequences truncated to\na fixed length of 64 tokens. The L2-normalized image and\ntext embeddings and a learnable scaling temperature are the\ninput to the InfoNCE contrastive loss [47].\nOur feature reconstruction decoder is a 2-layer ViT, un-\nlike the 8-layer counterpart of MAE [22] designed for raw\npixel reconstruction. The reconstruction loss is cosine dis-\ntance, scaled by a loss coefficient 2.0, and is added to the\ncontrastive loss. We use ALIGN dataset [29] by default,\nwhile we show using LAION datasets [51] leads to similar\nresults (Table 6). Unless noted, we use a batch size of 4k\nfor ablation and 16k for comparisons, and train for 500k it-\nerations using the AdamW optimizer with an initial learning\nrate (LR) of 5e-4 and linear LR decay. We use 10k warm-up\niterations and a weight decay of 0.01.\nDetection finetuning setup.\nWe train our model on base\ncategories CB with an image size of 1024\u00d71024. The po-\nsitional embeddings (PE) are bilinearly interpolated to fit\nthe higher resolution. We do not apply PE Dropout during\nthe detection training, and set a lower learning rate for the\nbackbone (e.g., 0.5 \u00d7) compared to the rest of the model.\nWe utilize CLIP templates [47] and take the average text\nembeddings of each category. We use a batch size 128,\nthe SGD optimizer with momentum 0.9, an initial learn-\ning rate of 0.18/0.02 and train for 36.8k/11.3k iterations on\nLVIS/COCO datasets.\nimage\nFlickr30K (1K test set)\nMS COCO (5K test set)\nencoder\n\u2014\u2014-image-to-text\u2014\u2014-\n\u2014\u2014-text-to-image\u2014\u2014-\n\u2014\u2014-image-to-text\u2014\u2014-\n\u2014\u2014-text-to-image\u2014\u2014-\nmethod\nsize\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR10\nR@1\nR@5\nR@10\nCLIP [47]\n302M\n88.0\n98.7\n99.4\n68.7\n90.6\n95.2\n58.4\n81.5\n88.1\n37.8\n62.4\n72.2\nALIGN [29]\n480M\n88.6\n98.7\n99.7\n75.7\n93.8\n96.8\n58.6\n83.0\n89.7\n45.6\n69.8\n78.6\nFLAVA [53]\n86M\n67.7\n94.0\n-\n65.2\n89.4\n-\n42.7\n76.8\n-\n38.4\n67.5\n-\nFILIP [61]\n302M\n89.8\n99.2\n99.8\n75.0\n93.4\n96.3\n61.3\n84.3\n90.4\n45.9\n70.6\n79.3\nFlorence [63]\n637M\n90.9\n99.1\n-\n76.7\n93.6\n-\n64.7\n85.9\n-\n47.2\n71.4\n-\nCoCa-Large [62]\n303M\n91.4\n99.2\n99.9\n79.0\n95.1\n97.4\n65.4\n85.6\n91.4\n50.1\n73.8\n81.8\nCFM-ViT (ours)\n303M\n91.7\n99.0\n99.9\n79.6\n95.6\n97.7\n66.4\n86.1\n91.5\n49.8\n73.5\n81.6\nTable 3: Zero-shot image-text retrieval results on Flickr30K and COCO benchmarks. We evaluate our pretrained model compared to\nother methods. We outperform the state-of-the-art CoCa-Large with the same backbone in 8 out of 12 metrics.\n4.1. Main Results\nLVIS benchmark.\nWe compare with other methods on\nthe LVIS [19] open-vocabulary detection benchmark which\ncontains a diverse set of 1203 object categories. The base\ncategories CB for training are the \u2018frequent\u2019 and \u2018common\u2019\ncategories, and novel categories CN are the \u2018rare\u2019 categories\nwhich are held out for testing, as in [18, 70, 13]. The main\nmetric is mask APr, and we report the mean over three runs\nfollowing [18] for reproducibility.\nTable 1 reports that the best CFM-ViT model achieves\n33.9 APr, a significant improvement over the best exist-\ning ViT-based method OWL-ViT [42] by +10.0 APr. Re-\nmarkably, CFM-ViT using a smaller ViT-B/16 backbone\noutperforms OWL-ViT with ViT-L/14 by +4.0 APr. Fur-\nthermore, compared to the current best approach ViLD-Ens\nwith EffNet-B7 backbone, CFM-ViT achieves a +7.6 APr\nimprovement.\nNotably, CFM-ViT has a simple finetun-\ning recipe using only vanilla detection losses [23], without\nthe use of long-tail recognition losses [42, 71, 74], knowl-\nedge distillation [18, 13], weak supervision [74], or pseudo\nbox/mask labels [71, 68, 49], all of which are common\namong current open-vocabulary detection methods.\nCOCO benchmark.\nWe present the comparison on the\nCOCO open-vocabulary detection benchmark. This setup\nuses 48 base categories for training and 17 novel categories\nfor testing [18]. The main metric is AP50 of novel cate-\ngories (\u2018novel AP\u2019). Due to fewer training categories, the\nCFM-ViT model has a tendency to overfit to these cate-\ngories using only the vanilla detection losses. This is be-\ncause CFM-ViT do not use any auxiliary objectives such as\npseudo box/mask labels [28, 15, 71, 68, 49], knowledge dis-\ntillation [18, 13], weak supervision [74] to counter-balance\noverfitting on this benchmark. However, Table 2 shows that\nCFM-ViT is still very competitive among existing methods\nleveraging auxiliary objectives. Moreover, CFM-ViT repre-\nsents the first ViT-based method on this benchmark, as the\nother ViT-based [42] approach only benchmarks on LVIS.\nZero-shot Image-Text Retrieval. In addition to our main\nevaluation on the region-level open-vocabulary detection,\nmethod\nbackbone\nAP\nAP50\nAP75\nsupervised [18]\nR-50\n25.6\n38.6\n28.0\nViLD [18]\nR-50\n11.8\n18.2\n12.6\nDetPro [13]\nR-50\n12.1\n18.8\n12.9\nCFM-ViT (ours)\nViT-B/16\n15.9\n24.6\n17.4\nCFM-ViT (ours)\nViT-L/16\n18.7\n28.9\n20.3\nTable 4: Transfer detection on Objects365 (Box APs). All mod-\nels are trained on the LVIS base categories and tested on Ob-\njects365 dataset, without finetuning.\nwe evaluate our image-level representation in zero-shot\nimage-text retrieval. We take the same CFM-ViT model as\nin the last row of Table 1 (ViT-L, batch size 16k) and con-\ntinue the pretraining on higher resolution, e.g., 448, for ex-\ntra 40K iterations, following the standard protocol [29, 62].\nTable 3 shows our comparison with other dual-encoder\nmethods on Flickr30K and MS COCO benchmarks. CFM-\nViT outperforms state-of-the-art methods of similar or\nlarger model size, on 8 out of 12 metrics.\nZero-shot Transfer Detection. To assess CFM-ViT\u2019s abil-\nity to generalize in zero-shot transfer detection, we test its\nperformance on Objects365-v1 validation split [52]. We use\nthe same detector trained on LVIS base categories (Table 1)\nand replace LVIS with Objects365 vocabulary embeddings\nfor transfer detection without finetuning [18, 13]. We as-\nsume all categories are novel and set \u03b1, \u03b2=(0.0, 0.65) in\nEq. (2). Our best model achieves 18.7 AP, outperforming\nViLD by +6.9 AP and DetPro by +5.6 AP, as shown in Ta-\nble 4. Given the different backbone capacity (R50 vs ViT),\nthis comparison mainly serves to demonstrate that CFM-\nViT can achieve strong cross-dataset generalization.\n4.2. Ablation Study\nWe ablate the design of CFM-ViT\u2019s pretraining and\nopen-vocabulary detector. We evaluate on the LVIS open-\nvocabulary detection benchmark.\nThe image encoder is\nViT-L/16, and contrastive batch size is 4k by default.\nMasked feature reconstruction.\nTable 5a ablates the\nproposed masked image-text pretraining (Sec. 3.2).\nThe\nproposed masked feature reconstruction offers a clear ben-\npretraining method\nAPr\nAP\nbaseline\n27.4 (+x.x)\n30.4\nw/ feat recon.\n30.7 (+3.3)\n34.0\nw/ pixel recon.\n27.1 (+x.x)\n31.3\nw/ 1st-layer feat recon.\n27.2 (+x.x)\n30.8\n(a) Masked reconstruction.\n\u2018baseline\u2019 is the\ncontrastive image-text pretraining. Our proposed\nmasked feature reconstruction improves by +3.3\nAPr. Reconstruction in the raw pixel space or\nthe first-layer feature space shows no benefit.\npretraining method\nAPr\nAP\nbaseline\n27.4 (+x.x)\n30.4\nw/ PED\n28.5 (+1.1)\n31.9\nw/ feat recon. + PED\n31.2 (+3.8)\n33.7\nw/ no PE\n25.8 (+x.x)\n29.5\nw/ feat recon. + no PE\n27.7 (+x.x)\n31.9\n(b) Positional Embedding Dropout (PED) im-\nproves the baseline by 1.1 APr. It achieves a fur-\nther gain of +2.7 when used with masked feature\nreconstruction. PED outperforms \u2018no PE\u2019 by 3.5\n/ 1.6 APr with/without feature reconstruction\ncontr. / recon.\nFLOPs\nAPr\nAP\n100% / 00%\n1.00\u00d7\n27.4\n30.4\n100% / 25%\n1.23\u00d7\n30.7\n34.0\n100% / 50%\n1.44\u00d7\n29.9\n33.1\n075% / 25%\n1.01\u00d7\n30.4\n33.9\n(c) Masking contrastive branch recovers\nthe training efficiency with little or no per-\nformance drop, outperforming the baseline\nby +3.0 APr.\nbblr\nAPr\nAP\n0.0\n9.5\n11.4\n0.1\n25.8\n28.5\n0.5\n27.4\n30.4\n1.0\n26.0\n30.2\n(d) Backbone fine-\ntuning lr ratio (bblr)\nw.r.t. added detector\nlayers.\nw/ PED\nAPr\nAP\nbaseline\n27.4 -\u2192- 24.6 -(-2.8)\n30.4 \u2192 30.3\nw/ feat-recon.\n30.7 -\u2192- 27.1 -(-3.8)\n34.0 \u2192 33.4\nbaseline\n\u2713\n28.5 -\u2192- 30.5 -(+2.0)\n31.9 \u2192 31.8\nw/ feat-recon\n\u2713\n31.2 -\u2192- 32.5 -(+1.3)\n33.7 \u2192 34.1\n(e) Frozen backbone inference. When using standard positional\nembeddings, it underperforms the finetuned encoder. In contrast,\nwith the encoder pretrained with PED, the frozen backbone infer-\nence surpasses the finetuned counterpart by +2.0 and +1.3 APr.\nmodel\nbatch\nAPr\nAP\nB/16\n4k\n24.1 -\u2192- 26.8 -(+2.7)\n27.6 \u2192 30.2\nB/16\n16k\n26.4 -\u2192- 28.8 -(+2.4)\n30.3 \u2192 33,5\nL/16\n4k\n27.4 -\u2192- 32.5 -(+5.1)\n30.4 \u2192 34.1\nL/16\n16k\n30.5 -\u2192- 33.9 -(+3.4)\n35.9 \u2192 36.6\n(f) Scalabiltiy: The benefit of \u2018baseline \u2192 CFM-ViT\u2019\nacross different model and contrastive batch sizes. It im-\nproves the baselines by +2.4 to +5.1 APr.\nTable 5: Ablation studies on LVIS open-vocabulary detection benchmark. We train on base (\u2018frequent\u2019 + \u2018common\u2019) categories, test on\nnovel (\u2018rare\u2019) categories, and report APr. We use ViT-L/16 backbone and contrastive batch size 4k unless otherwise noted.\nefit of +3.3 APr over the contrastive image-text pretraining\nbaseline. In this case, the feature reconstruction target is\nthe output features of the image encoder. We compare with\nother reconstruction targets: normalized image pixels [22]\nand the features from the first patchifying layer. We ob-\nserve that neither improve over the baseline, likely because\nthe contrastive pretraining sets a strong baseline represen-\ntation [18, 10, 33]. In contrast, the proposed masked fea-\nture reconstruction clearly improves upon the strong base-\nline and shows advantage in open-vocabulary detection.\nPositional embedding dropout.\nIn Table 5b, we ablate\nthe positional embedding dropout (\u2018PED\u2019). PED brings a\ngain of +1.1 APr over the baseline (PE without dropout).\nThis shows that PED effectively reduces overfitting to the\nlow-res whole-image PE during pretraining, thus adapting\nbetter to the high-res detection task through finetuning. In\naddition, PED achieves further gain of +2.7 when used to-\ngether with masked feature reconstruction.\nWe compare\nPED with another baseline which uses no positional embed-\ndings in the ViT encoder (\u2018no PE\u2019). The PED method out-\nperforms the \u2018no PE\u2019 baseline by 3.5 / 1.6 APr with/without\nfeature reconstruction. We note that the positional embed-\ndings in the reconstruction decoder [22] is always kept. Fi-\nnally, PED allows the use of the frozen backbone as a strong\nregion classifier as shown in Table 5e.\nFaster training by masking contrastive branch.\nTa-\nble 5c studies image masking ratios of the contrastive and\nreconstruction encoders.\nBy default, we apply our con-\ntrastive encoder on intact images during training, i.e. 100%\ntokens. Adding the reconstruction tower with 25% input to-\npretraining data\nAPr\nAP\nALIGN [29]\n32.5\n34.1\nLAION-2B [51]\n32.4\n34.3\nLAION-400MB [51]\n32.2\n34.1\nTable 6: Pretraining data. ViT-L/16 and batch size 4k is used.\nFlickr30K -\n- MS COCO\nI2T\nT2I -\n- I2T\nT2I\nbaseline\n86.0\n72.3 -\n- 59.3\n43.4\nw/ PED\n86.1\n72.5 -\n- 59.1\n43.2\nw/ feat recon. + PED\n87.0\n73.6 -\n- 60.1\n44.2\nTable 7: Pretraining evaluation on zero-shot image-text re-\ntrieval (Recall@1). We evaluate the image-level representation\nof our pretrained model on Flickr30k and COCO retrieval tasks.\nWe ablate the positional embedding dropout (PED) and adding\nmasked feature reconstruction. ViT-L/16 and batch size 4k is used.\nkens results in 1.23\u00d7 more training cost. To maintain the\ntraining efficiency, we explore feeding only 75% tokens to\nthe contrastive encoder that are mutually exclusive from the\nreconstruction branch inputs. This masking technique fully\nrecovers the training efficiency with little or no accuracy\nloss, outperforming the baseline by +3.0 APr.\nBackbone learning rate ratio.\nCFM-ViT requires the re-\ntention of pretrained knowledge in the backbone to recog-\nnize novel categories. Table 5d reports the advantage to set\nthe backbone learning rate lower than the rest of the detector\nduring the finetuning, with a ratio 0.5\u00d7 being the optimal\nvalue. Higher ratios lead to forgetting, while lower ratios\nlimit the ability to adapt to the detection task.\nFigure 3: Qualitative results on LVIS novel categories (top) and Objects365 zero-shot transfer detection (bottom). For LVIS results,\nwe only show the novel categories for clarity. CFM-ViT detects many novel categories such as rag doll, persimmon, paperweight, hardback\nbook, shepherd dog on LVIS, and shrimp, power outlet on Objects365.\nFrozen backbone inference.\nOur ablation studies so far\ndo not involve frozen backbone inference. All ablations use\nthe finetuned ViT backbone to compute the VLM scores (pi\nin Sec. 3.1 and Eq. (2)). In Table 5e, we assess the effi-\ncacy of the frozen backbone as a region classifier by sub-\nstituting the finetuned ViT encoder with a frozen ViT en-\ncoder and analyze the performance (see the rightmost part\nof Fig. 2). Our experiments show that the frozen backbone\nunderperforms the finetuned encoder when using standard\npositional embeddings, which applies to both the baseline\nwith and without feature reconstruction loss. However, we\nfind that pretraining the ViT encoder with positional em-\nbedding dropout (PED) leads to signficantly improved per-\nformance with frozen backbone, surpassing thoese of the\nfinetuned backbone by +2.0/+1.3 APr, without/with feature\nreconstruction loss. This result demonstrates the efficacy of\nPED in reducing the domain gap between contrastive pre-\ntraining and detection finetuning, thus improving zero-shot\nregion classification.\nCombined with feature reconstruc-\ntion, our full method achieves an overall improvement of\n+5.1 APr over the baseline.\nModel size and batch size.\nTable 5f studies the effect of\nmodel size and batch size in CFM-ViT pretraining on the\ndownstream open-vocabulary detection. We observe that in-\ncreasing the batch size from 4k to 16k leads to an improve-\nment of +2.7 / 1.4 APr for both ViT-B/L, while upgrading\nfrom ViT-B to ViT-L results in an improvement of +5.9 / 5.6\nAPr for both batch sizes. These results align with obser-\nvations from the contrastive learning literature [47, 29, 46]\nthat larger batch sizes and model sizes are both highly bene-\nficial. Importantly, we find that CFM-ViT consistently out-\nperforms the baseline by +2.4 to +5.1 APr, across all batch\nand model sizes tested, further demonstrating its efficacy.\nPretraining data.\nApart from the ALIGN data [29], we\nalso experiment with LAION datasets [51] in Table 6.\nLAION-2B / LAION-400M results in 32.4 / 32.2 APr,\nwhich is comparable to the ALIGN result 32.5 APr.\nImage-text retrieval.\nIn addition to ablations on open-\nvocabulary detection, we investigate the effects of posi-\ntional embedding dropout and masked feature reconstruc-\ntion on zero-shot image-level retrieval, and report the results\nin terms of Recall@1 metrics on Flickr30K and MS COCO\ndatasets. Table 7 shows that positional embedding dropout\neffectively preserves the quality of image-level representa-\ntion, while masked feature reconstruction yields an average\nimprovement of 1% Recall@1 across all metrics.\n4.3. Visualizations\nFeature reconstruction.\nIn Fig. 1, we show our feature\nreconstruction results from our pretraining (Sec. 3.2). For\nvisualization, we compute the similarity map (c) between\nthe reconstructed image features (d), and a query text em-\nbedding (e). We observe that the learned feature reconstruc-\ntions are semantically plausible with respect to the queried\nimage-text pairs.\nOpen-vocabulary detection outputs.\nIn Fig. 3, we visu-\nalize our CFM-ViT outputs on LVIS novel categories (top\nrow) and zero-shot transfer detection on Objects365 (bot-\ntom row). For both visualizations, we use the same model\nas in the last row of Table 1, which is trained on the LVIS\nbase categories. On both datasets, CFM-ViT is able to de-\ntect many novel categories unavailable during training.\n5. Conclusion\nWe introduce Contrastive Feature Masking Vision Trans-\nformer (CFM-ViT) which imbues the image-text pretrain-\ning with pixel/region-level semantics for open-vocabulary\nobject detection. By using feature construction and posi-\ntional embedding dropout, CFM-ViT is simple and scalable,\noutperforming the state-of-the-art on LVIS open-vocabulary\ndetection benchmark by large margins, and shows very\ncompetitive performance on COCO benchmark and zero-\nshot transfer to Objects365. In addition, CFM-ViT outper-\nforms the state-of-the-art on 8 out of 12 metrics of zero-shot\nimage-text retrieval benchmarks. We hope CFM-ViT would\ninspire the community to explore image-text pretraining for\nopen-vocabulary detection [31].\nReferences\n[1] Yutong Bai, Xinlei Chen, Alexander Kirillov, Alan Yuille,\nand Alexander C. Berg. Point-level region contrast for object\ndetection pre-training. In CVPR, pages 16061\u201316070, June\n2022. 2\n[2] Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chel-\nlappa, and Ajay Divakaran. Zero-shot object detection. In\nECCV, 2018. 3\n[3] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training\nof image transformers.\narXiv preprint arXiv:2106.08254,\n2021. 1, 2\n[4] Xinlei Chen and Abhinav Gupta. Webly supervised learning\nof convolutional networks. In ICCV, 2015. 2\n[5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-\nscaled multilingual language-image model. arXiv preprint\narXiv:2209.06794, 2022. 2\n[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,\nWenrui Dai, Hongkai Xiong, and Qi Tian.\nSdae: Self-\ndistillated masked autoencoder. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, Octo-\nber 23\u201327, 2022, Proceedings, Part XXX, pages 108\u2013124.\nSpringer, 2022. 2\n[7] Berkan Demirel, Ramazan Gokberk Cinbis, and Nazli\nIkizler-Cinbis. Zero-shot object detection by hybrid region\nembedding. In BMVC, 2018. 3\n[8] Karan Desai and Justin Johnson.\nVirtex: Learning visual\nrepresentations from textual annotations. In CVPR, 2021. 2\n[9] Santosh K Divvala, Ali Farhadi, and Carlos Guestrin. Learn-\ning everything about anything: Webly-supervised visual con-\ncept learning. In CVPR, 2014. 2\n[10] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen,\nShuyang Gu, Weiming Zhang, Lu Yuan, Dong Chen, Fang\nWen, and Nenghai Yu.\nClip itself is a strong fine-tuner:\nAchieving 85.7% and 88.0% top-1 accuracy with vit-b and\nvit-l on imagenet. arXiv preprint arXiv:2212.06138, 2022.\n2, 7\n[11] Xiaoyi Dong, Yinglin Zheng, Jianmin Bao, Ting Zhang,\nDongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang,\nLu Yuan, Dong Chen, et al.\nMaskclip:\nMasked self-\ndistillation advances contrastive language-image pretraining.\narXiv preprint arXiv:2208.12262, 2022. 2\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 1, 3\n[13] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao,\nand Guoqi Li. Learning to prompt for open-vocabulary ob-\nject detection with vision-language model. In CVPR, 2022.\n1, 2, 3, 4, 5, 6\n[14] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,\nXinggang Wang, Tiejun Huang, Xinlong Wang, and Yue\nCao. Eva: Exploring the limits of masked visual represen-\ntation learning at scale. arXiv preprint arXiv:2211.07636,\n2022. 2\n[15] Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu,\nHaibing Ren, Xiaolin Wei, Weidi Xie, and Lin Ma. Prompt-\ndet: Towards open-vocabulary detection using uncurated im-\nages. In European Conference on Computer Vision, pages\n701\u2013717. Springer, 2022. 5, 6\n[16] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,\nJeff Dean, Marc\u2019Aurelio Ranzato, and Tomas Mikolov. De-\nvise: A deep visual-semantic embedding model. In NeurIPS,\n2013. 2\n[17] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-\ning open-vocabulary image segmentation with image-level\nlabels. In European Conference on Computer Vision, pages\n540\u2013557. Springer, 2022. 3\n[18] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation. In ICLR, 2022. 1, 2, 3, 4, 5, 6, 7\n[19] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In CVPR,\n2019. 6\n[20] Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman\nKhan, Syed Waqas Zamir, and Fahad Shahbaz Khan. Syn-\nthesizing the unseen for zero-shot object detection. In ACCV,\n2020. 3\n[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, pages 16000\u201316009, June 2022. 1,\n2\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u2013\n16009, 2022. 3, 5, 7\n[23] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 6\n[24] Xiangteng He and Yuxin Peng. Fine-grained image classifi-\ncation via combining vision and language. In CVPR, 2017.\n2\n[25] Olivier J. H\u00b4enaff, Skanda Koppula, Jean-Baptiste Alayrac,\nAaron van den Oord, Oriol Vinyals, and Jo\u02dcao Carreira. Effi-\ncient visual pretraining with contrastive detection. In ICCV,\npages 10086\u201310096, October 2021. 2\n[26] Zejiang Hou, Fei Sun, Yen-Kuang Chen, Yuan Xie, and Sun-\nYuan Kung. Milan: Masked image pretraining on language\nassisted representation.\narXiv preprint arXiv:2208.06049,\n2022. 2\n[27] Zhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou,\nMing-Ming Cheng, Dongmei Fu, Xiaohui Shen, and Jiashi\nFeng. Contrastive masked autoencoders are stronger vision\nlearners. arXiv preprint arXiv:2207.13532, 2022. 2\n[28] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan\nElhamifar. Open-vocabulary instance segmentation via ro-\nbust cross-modal pseudo-labeling.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7020\u20137031, 2022. 5, 6\n[29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, 2021. 2, 3, 5,\n6, 7, 8\n[30] Armand Joulin, Laurens van der Maaten, Allan Jabri, and\nNicolas Vasilache.\nLearning visual features from large\nweakly supervised data. In ECCV, 2016. 2\n[31] Dahun Kim, Anelia Angelova, and Weicheng Kuo. Region-\naware pretraining for open-vocabulary object detection with\nvision transformers. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11144\u201311154, 2023. 3, 9\n[32] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon,\nand Weicheng Kuo. Learning open-world object proposals\nwithout learning to classify. IEEE Robotics and Automation\nLetters, 7(2):5453\u20135460, 2022. 4\n[33] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and\nAnelia Angelova.\nF-vlm: Open-vocabulary object detec-\ntion upon frozen vision and language models. arXiv preprint\narXiv:2209.15639, 2022. 1, 3, 4, 7\n[34] Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo,\nBen Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew\nDai, Zhifeng Chen, et al.\nMammut: A simple architec-\nture for joint learning for multimodal tasks. arXiv preprint\narXiv:2303.16839, 2023. 3\n[35] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\nKoltun, and Ren\u00b4e Ranftl.\nLanguage-driven semantic seg-\nmentation. arXiv preprint arXiv:2201.03546, 2022. 3\n[36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBLIP: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In Pro-\nceedings of the 39th International Conference on Machine\nLearning, Proceedings of Machine Learning Research, pages\n12888\u201312900, 2022. 2\n[37] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and\nJianfeng Gao. Grounded language-image pre-training. In\nCVPR, 2022. 3\n[38] Runze Li, Dahun Kim, Bir Bhanu, and Weicheng Kuo. Re-\nclip: Resource-efficient clip by training with small images.\narXiv preprint arXiv:2304.06028, 2023. 3\n[39] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-\nhofer, and Kaiming He. Scaling language-image pre-training\nvia masking. arXiv preprint arXiv:2212.00794, 2022. 2\n[40] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In ECCV, 2022. 4\n[41] Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fa-\nhad Shahbaz Khan, Rao Muhammad Anwer, and Ming-\nHsuan Yang.\nClass-agnostic object detection with multi-\nmodal transformer. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part X, pages 512\u2013531. Springer, 2022. 5\n[42] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil\nHoulsby. Simple open-vocabulary object detection with vi-\nsion transformers. In ECCV, 2022. 1, 3, 5, 6\n[43] Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram\nSinger, Jonathon Shlens, Andrea Frome, Greg S Corrado,\nand Jeffrey Dean. Zero-shot learning by convex combination\nof semantic embeddings. 2014. 2\n[44] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 3\n[45] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu\nWei. Beit v2: Masked image modeling with vector-quantized\nvisual tokenizers. arXiv preprint arXiv:2208.06366, 2022. 2\n[46] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu,\nAdams Wei Yu, Minh-Thang Luong, Mingxing Tan, and\nQuoc V. Le. Combined scaling for zero-shot transfer learn-\ning. CoRR, abs/2111.10050, 2021. 2, 8\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n2, 3, 5, 6, 8\n[48] Shafin Rahman, Salman Khan, and Nick Barnes. Improved\nvisual-semantic alignment for zero-shot object detection. In\nAAAI, 2020. 3\n[49] Hanoona Rasheed, Muhammad Maaz, Muhammad Uzair\nKhattak, Salman Khan, and Fahad Shahbaz Khan. Bridg-\ning the gap between object and image-level represen-\ntations for open-vocabulary detection.\narXiv preprint\narXiv:2207.03482, 2022. 1, 2, 3, 5, 6\n[50] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus.\nLearning visual representations with caption annotations. In\nECCV, 2020. 2\n[51] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 2, 5, 7, 8\n[52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365:\nA large-scale, high-quality dataset for object detection. In\nICCV, 2019. 6\n[53] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-\nlaume Couairon, Wojciech Galuba, Marcus Rohrbach, and\nDouwe Kiela. Flava: A foundational language and vision\nalignment model. In CVPR, pages 15638\u201315650, 2022. 6\n[54] Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao\nHuang, Yu Qiao, Xiaogang Wang, Jie Zhou, and Jifeng Dai.\nTowards all-in-one pre-training via maximizing multi-modal\nmutual information. arXiv preprint arXiv:2211.09807, 2022.\n2\n[55] Josiah Wang, Katja Markert, Mark Everingham, et al. Learn-\ning models for object recognition from natural language de-\nscriptions. In BMVC, 2009. 2\n[56] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan\nYuille, and Christoph Feichtenhofer. Masked feature predic-\ntion for self-supervised visual pre-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14668\u201314678, 2022. 2\n[57] Fangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen\nLin. Aligning pretraining for detection via object-level con-\ntrastive learning. In NeurIPS, 2021. 2\n[58] Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, and\nQi Tian. Mvp: Multimodality-guided visual pre-training. In\nComputer Vision\u2013ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part\nXXX, pages 337\u2013353. Springer, 2022. 2\n[59] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer,\nand Trevor Darrell. Region similarity representation learn-\ning. In ICCV, pages 10539\u201310548, October 2021. 2\n[60] Shusheng Yang, Yixiao Ge, Kun Yi, Dian Li, Ying Shan,\nXiaohu Qie, and Xinggang Wang.\nMasked visual re-\nconstruction in language semantic space.\narXiv preprint\narXiv:2301.06958, 2023. 2\n[61] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe\nNiu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and\nChunjing Xu. Filip: Fine-grained interactive language-image\npre-training. In ICLR, 2021. 6\n[62] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. TMLR, 2022.\n5, 6\n[63] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,\nYumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,\nZhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and\nPengchuan Zhang. Florence: A new foundation model for\ncomputer vision. arXiv preprint arXiv:2111.11432, Novem-\nber 2021. 6\n[64] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and\nChen Change Loy. Open-vocabulary detr with conditional\nmatching. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceed-\nings, Part IX, pages 106\u2013122. Springer, 2022. 5\n[65] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-\nFu Chang. Open-vocabulary object detection using captions.\nIn CVPR, 2021. 1, 3, 4\n[66] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xi-\naolin Wei, Chunhua Shen, and Yifan Liu. Segvit: Semantic\nsegmentation with plain vision transformers. arXiv preprint\narXiv:2210.05844, 2022. 1\n[67] Xinyu Zhang, Jiahui Chen, Junkun Yuan, Qiang Chen, Jian\nWang, Xiaodi Wang, Shumin Han, Xiaokang Chen, Jimin Pi,\nKun Yao, et al. Cae v2: Context autoencoder with clip target.\narXiv preprint arXiv:2211.09799, 2022. 2\n[68] Shiyu Zhao, Zhixing Zhang, Samuel Schulter, Long Zhao,\nAnastasis Stathopoulos, Manmohan Chandraker, Dimitris\nMetaxas, et al. Exploiting unlabeled data with vision and\nlanguage models for object detection. In ECCV, 2022. 1, 2,\n3, 5, 6\n[69] Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, and Li\nCui. Background learnable cascade for zero-shot object de-\ntection. In ACCV, 2020. 3\n[70] Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, and Yin\nLi. Learning to generate scene graph from natural language\nsupervision. In ICCV, 2021. 6\n[71] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan\nLi, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang\nDai, Lu Yuan, Yin Li, and Jianfeng Gao.\nRegionclip:\nRegion-based language-image pretraining. In CVPR, 2022.\n1, 2, 3, 4, 5, 6\n[72] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free\ndense labels from clip. In ECCV, 2022. 3\n[73] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nXie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training\nwith online tokenizer.\narXiv preprint arXiv:2111.07832,\n2021. 2\n[74] Xingyi Zhou,\nRohit Girdhar,\nArmand Joulin,\nPhilipp\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra.\nDetecting twenty-thousand\nclasses using image-level supervision. In ECCV, 2022. 1, 3,\n5, 6\n[75] Pengkai Zhu, Hanxiao Wang, and Venkatesh Saligrama.\nDon\u2019t even look once: Synthesizing features for zero-shot\ndetection. In CVPR, 2020. 3\n"
  },
  {
    "title": "Compositional Diffusion-Based Continuous Constraint Solvers",
    "link": "https://arxiv.org/pdf/2309.00966.pdf",
    "upvote": "4",
    "text": "Compositional Diffusion-Based\nContinuous Constraint Solvers\nZhutian Yang1\nJiayuan Mao1\nYilun Du1\nJiajun Wu2\nJoshua B. Tenenbaum1\nTom\u00b4as Lozano-P\u00b4erez1\nLeslie Pack Kaelbling1\n1Massachusetts Institute of Technology, 2Stanford University\nAbstract: This paper introduces an approach for learning to solve continuous con-\nstraint satisfaction problems (CCSP) in robotic reasoning and planning. Previous\nmethods primarily rely on hand-engineering or learning generators for specific\nconstraint types and then rejecting the value assignments when other constraints are\nviolated. By contrast, our model, the compositional diffusion continuous constraint\nsolver (Diffusion-CCSP) derives global solutions to CCSPs by representing them\nas factor graphs and combining the energies of diffusion models trained to sample\nfor individual constraint types. Diffusion-CCSP exhibits strong generalization to\nnovel combinations of known constraints, and it can be integrated into a task and\nmotion planner to devise long-horizon plans that include actions with both discrete\nand continuous parameters. Project site: https://diffusion-ccsp.github.io/\nKeywords: Diffusion Models, Constraint Satisfaction Problems, Task and Motion\nPlanning\n1\nIntroduction\nRobotic manipulation planning relies critically on the ability to select continuous values, such as\ngrasps and object placements, that satisfy complex geometric and physical constraints, such as stability\nand lack of collision. Existing approaches have used separate samplers, obtained through learning\nor optimization, for each constraint type. For instance, GraspNet [1] focuses on generating valid\ngrasps, and StructFormer [2] specializes in generating semantically meaningful object placements.\nHowever, complex problems require a general-purpose solver to produce values that conform to\nmultiple constraints simultaneously. Consider the task of 3D object packing with a robot arm, as\ndepicted in Figure 1. Given the geometric and physical properties of the objects, our objective is to\ngenerate the target pose and motion trajectories that fulfill three essential criteria. First, all objects\nmust be contained within the container and satisfy qualitative user requirements (e.g., bowls should\nbe placed next to each other). Second, collisions among objects and the robot, during their movement\nand when stationary, must be avoided. Finally, the final configuration should be physically stable.\nConstructing or training a monolithic model capable of solving every possible combination of goals\nand constraints can be challenging due to limited data. Therefore, it is essential for general-purpose\nrobot planners to reuse and to compose individual solvers for overall tasks. A common strategy is to\ncombine specialized methods for solving individual problems in a sequential manner via rejection\nsampling, which is widely used by state-of-the-art planners for object rearrangement and general task\nand motion planning (TAMP) problems [3, 4, 5, 6, 7]. The set of solvers is applied in a predetermined\norder guided by heuristics (e.g., first sampling object poses and then grasps and trajectories). If a\npreviously sampled value violates a later constraint (e.g., the sampled pose in the box does not leave\nenough space for the gripper during object placement), backtracking is performed. This rejection-\nsampling-based approach can be highly inefficient when faced with many constraints. An alternative\napproach is to formulate the entire constraint satisfaction problem using a differentiable objective\nfunction and solve it using local optimization methods [8, 9]. However, these methods usually require\nmanually-specified differentiable constraint formulas, but many important constraints, such as those\ncorresponding to human directives like \u2018next to\u2019, need to be learned from data.\narXiv:2309.00966v1  [cs.RO]  2 Sep 2023\n(a) The dispenser is\nconstrained to be grasp-\ned only from the sides.\n(b) The gripper pose\nwhen grasping the bowl\nshould be reachable.\n(c) The gripper at grasp\nposes shouldn\u2019t collide\nwith other objects.\n(d) All arm trajectories\nshouldn\u2019t collide with\nthe object or the tray.\n(e) The two bowls are\nconstrained to be next\nto each other.\nFigure 1: Solving Continuous Constraint Satisfaction Problems by Composing Diffusion Models.\nOur approach combines diffusion models, representing individual constraints, to generate object\nplacement poses. The choice of an object\u2019s placement depends on both qualitative constraints about\nobject placement and collision avoidance constraints on the object and the robot gripper.\nWe propose to use constraint graphs as a unified framework for specifying constraint-satisfaction\nproblems as novel combinations of learned constraint types and apply diffusion-model-based con-\nstraint solvers to find solutions that jointly satisfy the constraints. A constraint graph consists of\nnodes representing decision variables (e.g., grasping poses, placement poses, and robot trajectories)\nand nodes representing constraints among these decision variables. Our method, the compositional\ndiffusion constraint solver (Diffusion-CCSP), learns a collection of diffusion models for individual\ntypes of constraints and recombines them to address novel problem instances, finding satisfying\nassignments via a diffusion process that generates diverse samples from the feasible region. In partic-\nular, each diffusion model learns to generate a distribution of feasible solutions for one particular\ntype of constraint (e.g., collision-free placements). Since the diffusion models are generative models\nof the set of solutions, at inference time we can condition on an arbitrary subset of the variables and\nsolve for the rest. Furthermore, the diffusion process is compositional: since each diffusion model is\ntrained to minimize an implicit energy function, the task of global constraint satisfaction can be cast\nas minimizing the global energy of solutions (in this case, simply the summation of individual energy\nfunctions). These two features introduce notable flexibility in both training and inference. Component\ndiffusion models can be trained independently or simultaneously based on paired compositional\nproblems and solutions. At performance time, Diffusion-CCSP generalizes to novel combinations\nof known constraint types, even when the constraint graph involves more variables than those seen\nduring training.\nWe evaluate Diffusion-CCSP on four challenging domains: 2D triangle dense-packing, 2D shape\narrangement with qualitative constraints, 3D shape stacking with stability constraints, and 3D object\npacking with robots. Compared with baselines, our method exhibits efficient inference time and\nstrong generalization to novel combinations of constraints and problems with more constraints.\n2\nRelated Work\nGeometric rearrangement. A large body of work has studied geometric rearrangement of objects in\nrobotics subject to conditions specified in natural language [10, 11, 2, 12, 13, 14] or via formal speci-\nfication or demonstration [15, 16, 17, 18, 19, 20]. Similar to our work, StructDiffusion [14] also uses\ndiffusion models to solve for geometric rearrangements of objects. It employs a transformer-based\ndiffusion model architecture to generate object placement poses that satisfy language-conditioned\nscene-level constraints, such as positioning all objects to form a circle. In comparison, our work\nprovides a framework for solving arbitrary CCSPs that involve different constraints among objects.\nCompositional generative modeling. Our work relates to existing work on compositional generative\nmodeling, where individual generative models are combined to generate joint outputs [21, 22, 23, 24,\n25, 26, 27, 12], primarily in the setting of image generation. Most similar to our work, Gkanatsios\net al. [12] compose EBMs to represent different subgoals in compositions of object scenes. They\ncurrently only address object placements, whereas our method is generic and can combine constraints\n2\ngeomA\ngeomB\ngeomC\nposeA\nposeB\nposeC\nA\nB\nC\nD\nE\ngeomBox\nposeBox\nclose-to(A, B)\ngraspA\ntrajA\nposeA0\ncfree(A, B)\nvalid-traj\ncfree(A, C)\n......\n(a) Visualization of the environment \nwhile placing object A.\n(b) Visualization of the constraint graphs associated with the \nobject placement. There are three decision variables.\nin(A, Box)\nThe arm trajectory trajA\nconnects A\u2019s initial pose \nposeA0 and the target pose \nposeA given graspA.\nBox\nFigure 2: Continuous Constraint Satisfaction Problem (CCSP) in Robot Planning. It unifies\ngeometric, physical, and qualitative constraints. To place A into the tray, we need to generate the\ngrasping pose graspA, placement pose poseA, and the robot arm trajectory. We omit collision-free\nconstraints with robots in (b) for brevity.\ninvolving object poses, grasps, and robot configurations in the optimization-based sampling of object\nposes; this is generally more efficient than backtracking over values generated by a pose-only sampler.\nTask and motion planning (TAMP). There are special-purpose 3D object packing algorithms based\non heuristic-search or simulation [28, 29, 30] for finding object poses that satisfy collision and\nstability constraints. They are again not directly comparable to Diffusion-CCSP, because they cannot\naddress the same range and combinations of constraints.\n3\nCompositional Diffusion Constraint Solvers\nOur goal is to develop a general algorithm capable of learning and solving diverse continuous\nconstraint satisfaction problems (CCSPs) encountered in robotic manipulation tasks. To achieve this,\nwe adopt a generic graph-based representation of CCSPs unifying various geometric, physical, and\nqualitative constraints*.\n3.1\nFormulation and Representation of Constraint Satisfaction Problems\nFormally, a continuous constraint satisfaction problem (CCSP) can be represented as a graph G =\n\u27e8V, U, C\u27e9. Each v \u2208 V is a decision variable (such as the pose of an object), while each u \u2208 U is a\nconditioning variable (such as the geometry of an object, which will be constant at performance time).\nEach c \u2208 C is a constraint, formally a tuple of \u27e8tc, nc\u27e9, where tc is the type of the constraint (e.g.,\ncollision-free\u2020), and nc = (Vc, Uc) contains two sets of variables in V and U, which correspond to\nthe arguments to this constraint. For example, a collision-free constraint between object A and B\ncan be represented as \u27e8cfree, (poseA, poseB, geomA, geomB)\u27e9. Here, poseA and poseB are decision\nvariables that represent the target pose of objects A and B, while geomA and geomB are conditioning\nvariables that represent the geometry of objects A and B.\nLet V and U be assignments of values to the decision and conditioning variables V and U, respectively,\nand let V c and U c be the subsets of assigned values to variables in constraint c. For example, in the\n3D packing task shown in Figure 2a, the assignment V to variables V is a mapping from the variables\nposei to an SE(3) pose of object i. The solution should satisfy that for all c = \u27e8tc, (Vc, Uc)\u27e9 \u2208 C,\ntest(tc, (V c, U c)) = 1, where test takes the constraint type and the assignments to each variable and\nreturns 1 if the constraint c is satisfied and 0 otherwise. The function test can be implemented as\nhuman-specified rules, learned classifiers, or the outcome of a physics simulator.\n*Throughout the paper, we use the word qualitative constraint to refer to constraints that can not be easily\nand accurately described analytically\u2014in particular, spatial relationships such as \u201cleft\u201d and \u201cclose-to.\u201d These\nconstraints depend on contexts and would ideally be learned from human annotations.\n\u2020Precisely, object shapes do not penetrate.\n3\nExample. Figure 2 illustrates the constraint graph associated with the pick-and-place of a bowl\n(A) into a box close to another bowl (B), while avoiding it and other obstacles. It includes three\ndecision variables: graspA (transform between object and hand), poseA (target pose), and trajA\n(robot trajectory). The scenario involves three constraint groups: 1) qualitative constraints, including\nin(A, Box) ensuring the bowl\u2019s placement within the box and close-to(A, B) asserting proximity\nbetween two bowls; 2) collision avoidance constraints such as cfree(A, B), which asserts that objects\nA and B do not collide in the final placement; 3) trajectory constraints, encapsulated by valid-traj,\nwhich assert that the trajectory trajA is a feasible robot path that connects the bowl\u2019s initial position\nposeA0 and the target poseA, given graspA. It is important to note that these constraints are correlated:\nfor example, the choice of poseA requires the consideration of the robot trajectory to ensure the\nexistence of a valid grasping pose and trajectory.\n3.2\nCompositional Diffusion Models\nGiven a set of constraints C, decision variables V, and conditioning values U = U, we wish to\nfind an assignment of V0 that satisfies C. We represent the conditional distribution of variable\nassignments given an individual constraint c using a diffusion model pc(Vc = V c | Uc = U c) \u221d\n1[test(tc, (V c, U c))], which is effectively uniform over the (bounded) region of Vc that satisfies\nthe constraint. Finding a satisfactory assignment of variables V0 then corresponds to finding an\nassignment that maximizes the likelihood in the joint distribution constructed from the product of\nthe individual constraint models. Each constraint diffusion model represents p(Vc | Uc) in the form\nof an energy-based model, p(Vc | Uc) \u221d e\u2212E(Vc|Uc) through its score [24, 26], so the probability\nmaximization problem corresponds to the energy minimization problem:\nV0 = arg max\nV\nY\nc\u2208C\npc(Vc | Uc) = arg min\nV\nX\nc\u2208C\nE(Vc | Uc) .\nWe solve this energy minimization problem by using a sampler based on the annealed unadjusted\nLangevin algorithm (ULA) [31, 26]. The ULA algorithm is a special case of Langevin Monte Carlo\n\u2014 it iteratively optimizes a noisy sample using the gradient of the energy function, with added noise\nat each step of optimization.\nDuring training, we optimize a noise prediction model \u03f5t for each constraint type t. Each \u03f5t takes\nconditioning variables U c and a noisy version of V c as input, and predicts the noise applied on V c.\nMost simply, we could train the diffusion models independently for each constraint, and combine\nthem at prediction time. In our experiments, we train multiple diffusion models simultaneously: since\neach training example corresponds to some constraint graph, it is natural to train all the constraints\nthat are instantiated in that graph jointly from the example. Note that the constraint graphs within the\ntraining set and between training and testing need not be the same\u2014they just need to be constructed\nfrom the same basic set of constraint types.\nSpecifically, our training dataset D contains a set of constraint graphs, each with conditioning values\nand a solution to the decision variables: \u27e8G = (V, U, C), U, V0\u27e9. For each type of constraint, we\nrandomly initialize a denoising function \u03f5t, and sum the denoising predictions over the constraints in\nthe graph to denoise a noisy version of V0:\nLMSE =\nE\n\u27e8G=(V,U,C),U,V0\u27e9\u2208D\n\u03f5\u223cN (0,I)\nt\u223cUniform(1,\u00b7\u00b7\u00b7 ,T )\n\"\r\r\r\u03f5 \u2212\nX\nc\u2208C\n\u03f5tc\n\u0000\u221a\u00af\u03b1tV c\n0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, U c, t\n\u0001 \r\r\r\n2\n#\n,\nwhere \u03f5 is random Gaussian noise applied to V0, \u03b1t are predefined noise scheduling for training and\nsampling following Nichol and Dhariwal [32], \u00af\u03b1t := Qt\ni=1 \u03b1t, t is a randomly sampled diffusion\nstep, and T is the total number of diffusion steps. The full pseudocode for training and sampling\nfrom our conditional diffusion models for constraints can be found in Algorithms 1 and 2.\n3.3\nGeometric and Physical Constraint Solving with Diffusion-CCSP\nGiven the general formulation of Diffusion-CCSP, we describe its concrete implementation in our\nrobot planning setting. Recall that for each type of constraint, we have defined a \u201cdenoise\u201d function \u03f5t\n4\nAlgorithm 1 Training\n1: repeat\n2:\nG, V0 \u223c D\n3:\nt \u223c Uniform({1, . . . , T}); \u03f5 \u223c N(0, I)\n4:\nTake gradient descent step on\n5:\n\u2207\u03b8\n\r\r\r\u03f5 \u2212 P\nc\u2208C\n\u03f5tc(\u221a\u00af\u03b1tV c\n0 + \u221a1 \u2212 \u00af\u03b1t\u03f5, t))\n\r\r\r\n2\n6: until converged\nAlgorithm 2 Sampling\n1: VT \u223c N(0, I)\n2: for t = T, . . . , 1 do\n3:\nif t > 1 then z \u223c N(0, I) else z = 0\n4:\nVt\u22121 =\n1\n\u221a\u03b1t\n\u0010\nVt \u2212\n1\u2212\u03b1t\n\u221a1\u2212\u00af\u03b1t\nP\nc\u2208C\n\u03f5tc (V c\nt , U c, t)\n\u0011\n+ \u03c3tz\n5: return V0\nA\nB\ngeomA\ngeomB\nposeA\nposeB\ncfree(A, B)\ngeomA\ngeomB\nposeA\nposeB\nMLPcfree\n\ud835\udf00poseA\n\ud835\udf00poseB\n(a) The cfree constraint.\n(b) The denoise neural network architecture. \ud835\udf00poseB is \nignored because poseB is fixed in this case.\nfgeom\nfgeom\nfpose\nfpose\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nx\n\u22121.00\n\u22120.75\n\u22120.50\n\u22120.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\n(c) Learned gradient field\nfor the cfree constraint.\nFigure 3: Illustration of the denoising neural network for satisfying constraint cfree (collision-free)\nand the gradient map learned by Diffusion-CCSP for the centroid of B at (x, y) when A is at (0, 0).\nthat takes the conditioning values and (the noisy version of) the decision values as inputs, and predicts\nthe noise applied on the decision values. In our robotic domains, we focus on two variable types:\ngeometry variables and pose/transformation variables. We illustrate the neural network architecture\nfor collision-free constraints though the construction easily generalizes to other constraint types.\nRecall that the argument to a collision-free constraint c has four elements: Uc = (geomA, geomB),\nVc = (poseA, poseB). We use poset\nA to denote the value of this variable at diffusion time step t.\nThe output of \u03f5cfree is a tuple (\u03f5poset\nA, \u03f5poset\nB) which corresponds to the noise applied on poseA\nand poseB at diffusion time step t. In our implementation, an object\u2019s shape is represented as a 3D\naxis-aligned bounding box, and its pose is represented as a 5-dimensional feature vector composed\nof the 3-DoF translation and rotation \u03b3 about +z axis, encoded using sin(\u03b3) and cos(\u03b3). We use a\nmulti-layer perceptron (MLP) fgeom to encode the object shapes (shared across all constraints) and a\nconstraint-specific MLP fpose to encode poses. Both geometric features and pose encoders map input\nrepresentations into a fixed-length vector embedding. The function \u03f5cfree is implemented as\n\u03f5t\ncfree(geomA, geomB, poseA, poseB) = gcfree (fgeom(geomA) \u2295 fgeom(geomB) \u2295 fpose(poseA) \u2295 fpose(poseB) \u2295 t) ,\nwhere \u2295 denotes vector concatenation, and gcfree is another MLP model that takes the concatenated\ninput-feature vectors and outputs a vector of length 10. The first 5 entries correspond to the prediction\nof \u03f5poset\nA, and the rest correspond to \u03f5poset\nB, where t is the decoding time step.\nRemark. The generative nature of diffusion models allows us to perform training and inference\nover different combinations of conditioning variables and decision variables. For example, during\nboth training and inference time, depending on the actual split of conditioning variables and decision\nvariables in the constraint graph, we may have poseA given and fixed, and only optimize for poseB.\nSecond, although we have been using a simplified representation of object geometries (i.e., bounding\nboxes) and poses (considering only translation and a 1 DoF rotation), our framework can be naturally\nextended to support point clouds as inputs and the full SE(3) poses, possibly through the exponential\nmaps of SE(3) poses [33].\nExample. Figure 3 illustrates the neural network architecture for one specific constraint and the\ncorresponding gradient map learned by Diffusion-CCSP. In this example, our objective is to generate\na pose for object A that avoids collisions with object B in its current position. The neural network\nencodes the four input features and predicts the noise for poseA and poseB. As the pose of the\nobject B remains unchanged, the noise (gradient) predicted by \u03f5cfree for its pose is disregarded.\nFigure 3c visualizes the gradient map for the X and Y components of poseA, where the gradient points\n5\ntop-in\ncenter-in\naway-\nfrom\naway-\nfrom\nh-aligned\nv-aligned\nleft-in\nclose-to\ncfree\n(c) 3D Object Stacking\nwith Stability Constraints.\n(b) 2D Shape Arrangement with\nQualitative Constraints\n(d) 3D Object\nPacking with Robots.\n(a) 2D Triangle Packing.\nFigure 4: Illustration of four domains. They contain geometric, physical, and qualitative constraints.\n(a) Triangle packing. (b) Dense 2D packing with qualitative constraints. The figure shows a subset of\n45 constraints of 13 types. (c) 3D object stacking. The arrows show the support relationships. (d) 3D\nobject packing with a panda robot.\napproximately outward from the current location of object B. It is important to note that the gradient\nover the pose variable poseA will have contributions from multiple constraints in the constraint graph.\nSee the gradient fields of all qualitative constraints in Appendix F.\nPlanning. So far, we have presented a generic solution to solving constraint satisfaction problems\nthat involve geometric and physical constraints, assuming that the constraint graph is given as input to\nthe algorithm. This approach can be directly used to solve particular tasks such as the pose prediction\ntask in object rearrangements. Our primary motivation is to solve subproblems that arise during\ngeneral robot manipulation planning. In Appendix E, we illustrate how Diffusion-CCSP can be\nintegrated with a search algorithm to solve general task and motion planning (TAMP) problems,\nwhere the constraint graphs are automatically constructed based on the sequence of actions that has\nbeen applied and the goal specification of the task.\n4\nExperiments\nWe evaluate Diffusion-CCSP through a sequence of continuous CCSP tasks incorporating geometric,\nphysical, and qualitative constraints. We include all environmental details and sample complexity\ndiscussions in Appendix A. For implementation and training details and additional discussions on\nmodel training and inference, see Appendix B. For analysis of Diffusion-CCSP\u2019s failure cases in each\ntask, see Appendix C.\nWe compare our full model to one model variant, a sequential-sampling baseline, and StructDiffusion\n[14]. Diffusion-CCSP w. Reverse Sampling: In this variant, we directly use the standard reverse\ndiffusion process to sample from composed diffusion models [24]. This method is faster at sampling\nthan ULA, but is generally found to have worse generalization performance. We include a detailed\ncomparison between these two in Appendix D. Sequential Sampling: We sequentially sample each\ndecision variable according to a generic sampler (e.g., for 2D poses, we randomly sample a location\nthat is within the tray and a random rotation) and check all geometric constraints (e.g., in and cfree\nassociated with the decision variable. For each variable, we sample 50 samples, and report failure\nwhen no successful samples can be produced within 50 samples. StructDiffusion: StructDiffusion is\na transformer-based diffusion model for predicting object placement poses based on object shapes and\na single scene-level constraint. It takes a set of object shape representations, the noisy sample of their\nposes, and the diffusion time step, and predicts the noise on object poses. Since it can only handle a\nsingle global constraint, it is only applicable in two of our tasks (Section 4.1 and Section 4.4).\n4.1\n2D Triangle Packing\nThe first domain considers packing a set of random triangles into a square tray, ensuring no overlap.\nTriangles can be rotated, and their shapes are encoded by their vertex coordinates in their zero poses.\nThe model\u2019s output includes a 2D translation vector and a 1D rotation. Training data consists of\n30,000 solutions to problems with 2-4 triangles, and testing includes 100 problems with 2-5 triangles.\nWe generate training solutions by randomly splitting the tray.\n6\n(a) 2D Triangle Packing\n(b) 2D Shape Arrangement with Qualitative Constraints\n(c) 3D Object Stacking with Stability Constraints\n(d) 3D Object Packing with Robot Trajectory Constraints\nFigure 5: Quantitative Comparisons of Constraint Solvers. Accumulated number of problems\nsolved in 10 runs of different models. OOD=Out of training distribution. The shaded area indicates\nthe standard deviation of various models across five different seeds. The sequential sampling baseline\ncompletely failed for task (c) and hard tasks in (b) and (d). Our full model performs better than a\nvariant without ULA sampling and better than StructDiffusion in more complex problems (d).\nFig. 5a shows the result, and Fig. 4a shows a concrete testing problem and its solution found by\nDiffusion-CCSP. When the number of triangles becomes large, it becomes increasingly difficult for\nthe rejection-sampling baseline to succeed. The transformer architecture used in StructDiffusion\nsuccessfully models the relational structure among all pairs of objects. Therefore, it performs similarly\nto our method and has a better generalization to out-of-distribution CCSPs in this task.\n4.2\n2D Shape Arrangement with Qualitative Constraints\nThis domain involves packing rectangles into a 2D box, satisfying geometric and qualitative\nconstraints resembling scenarios such as dining table settings and office desk arrangements.\nQualitative constraints include center-in(A, Box), left-in(A, Box), left-of(A, B), close-to(A, B),\nvertically-aligned(A, B), etc., along with the geometric ones such as containment (in) and collision-\nfree (cfree). Different problem instances have different box sizes and constraints. The training dataset\nconsists of 30,000 problems and solutions with 2 to 4 objects, while the testing set contains 100\nproblems with 2 to 5 objects. Training examples are generated by randomly splitting the tray.\nFig. 5b shows the result, and Fig. 4b shows a concrete test problem that involves six objects and\n45 constraints (including qualitative constraints shown in the figure and geometric constraints such\n7\ncfree\nleft-in\nv-aligned\nin\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nx\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\ny\n\u2212136.5\n\u2212130.5\n\u2212124.5\n\u2212118.5\n\u2212112.5\n\u2212106.5\n\u2212100.5\n\u221294.5\n\u221288.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nx\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\ny\n\u2212792\n\u2212772\n\u2212752\n\u2212732\n\u2212712\n\u2212692\n\u2212672\n\u2212652\n\u2212632\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nx\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\ny\n\u2212306\n\u2212276\n\u2212246\n\u2212216\n\u2212186\n\u2212156\n\u2212126\n\u221296\n\u221266\n\u221236\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nx\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\ny\n\u2212183\n\u2212168\n\u2212153\n\u2212138\n\u2212123\n\u2212108\n\u221293\n\u221278\n\u221263\n\u221248\nFigure 6: Visualization of learned energy functions of selected geometric and qualitative constraints.\nObjects\nN=4\nN=5\n# of Calls to Diffusion-CCSP 11.9 33.33\n(a) 3D Object Stacking.\nObjects\nN=2 N=3 N=4 N=5 N=6\n# of Calls to Diffusion-CCSP 1.33 1.97 2.96 4.37 7.26\n(b) Task 3: 3D Object Packing with Robots.\nTable 1: Average number of calls to Diffusion-CCSP in order to solve the full TAMP problem.\nas in and cfree). The task is very challenging when the number of objects and the number of\nconstraints becomes large. Therefore, the baseline rejection sampling method barely solves any\nproblem instances with 6 objects. Diffusion-CCSP significantly outperforms the baselines. We also\nvisualized the learned energy landscape for a few constraints in Fig. 6 learned by our model, and\nmore visualizations in Appendix F.\n4.3\n3D Object Stacking with Stability Constraints\nThe third domain considers arranging trapezoidal prisms onto a shelf to satisfy a given support\nstructure and stability. The goal is to generate a sequence of object placement actions (including\nobject names and their target poses). This reflects real-life tasks such as organizing storage containers,\nwhich often involve tilted boxes due to varying prism heights. All of our training and testing problems\ninvolve at least one \u201cbridge-like\u201d structure, i.e., there is at least one object supported by multiple\nobjects simultaneously. Furthermore, since the output of models is a sequence of object placements,\na sequence is successful only if the final stage and all intermediate states are physically stable. We\nrandomly sample the sizes of the shelf region and the shapes. We use a physical simulator to generate\ntraining examples. The training dataset has 24,000 problem instances and solutions with 5 to 7\nobjects, and the testing set includes 100 problem instances with 4 to 7 objects.\nAn example of the task is illustrated in Fig. 4c. Given that neither our algorithm nor the baselines\nspecifically plan for object placement orders, we integrate them into a very simple task and motion\nplanner, as detailed in Appendix E. This planner randomly samples object placement orders and\nbox \u201csupport\u201d structures (i.e., the arrangement of objects supporting other objects), then invokes the\nDiffusion-CCSP solver to find solutions. We assess two metrics: first, the performance of Diffusion-\nCCSP with a fixed CCSP graph, illustrated in Fig. 5c; second, the number of calls to Diffusion-CCSP\nrequired to solve problems involving varying quantities of objects, as shown in Table 1a. For instance,\npacking 6 boxes into a cabinet necessitates, on average, 25 different object placement and support\nstructure samples. If a CCSP is not solvable by Diffusion-CCSP, it might truly be infeasible due to\nincorrect sampled placement orders or support structures. Therefore, Table 1a shows the integrated\nsystem\u2019s overall performance. Notably, given that we can process multiple CCSPs as a GPU batch\nand that generating object orders is inexpensive, the system does not have high latency overall. In a\nbatch of 100 CCSPs, it takes on average 0.01-0.04 seconds for Diffusion-CCSP (Reverse) to solve\neach CCSP, and 0.06-0.19 seconds for Diffusion-CCSP (ULA).\n4.4\nTask 4: 3D Object Packing with Robot Trajectory Constraints\nThis domain involves packing 3D objects into a box container in a simulated environment using\nPyBullet, with a Franka Emika Panda arm. The dataset includes 53 objects from the PartNet Mobility\ndataset and ShapeNet [34, 35], 15 of which only afford side grasps, while others can be grasped\nfrom the top. We pre-generate a set of grasps for each object, and use a motion planner to find robot\n8\ntrajectories. The task and motion planner and Diffusion-CCSP jointly solve for grasping choices,\nobject poses, and arm motions. Although our model doesn\u2019t directly generate grasps or trajectories, it\nreasons about object geometries and has learned to predict poses for which it is likely that there are\ncollision-free trajectories (which is later verified with a motion planner.) The training dataset has\n10,000 problem instances and solutions with 3 to 5 objects, and the testing set includes 100 problem\ninstances with 3 to 6 objects. Diffusion-CCSP successfully finds solutions to 60-80 percent of the\nproblems in just 10 samples for the most challenging 6-triangle packing problems.\nFig. 5d illustrates the number of samples needed for tasks involving a different number of objects.\nOverall, Diffusion-CCSP significantly outperforms the rejection-sampling-based solver and Struct-\nDiffusion, especially for larger numbers of objects. Diffusion-CCSP can even generalize directly\nto six-object packing problems without additional training. Table 1b showcases the full pipeline\u2019s\nperformance, which includes generating placement orders, grasps, poses, and trajectories.\n5\nLimitation and Conclusion\nLimitations. First, all constraints we\u2019 have explored in this paper have a fixed arity (i.e., they\ninvolve a fixed number of objects). An interesting direction is to consider variable-arity constraints.\nExtending our model so that it can take natural language instructions as an additional input is also\nuseful. Additionally, the generation of task labels and solutions now relies on hand-coded rules,\nwhich is limited, especially for qualitative constraints such as \u201csetting the dining table.\u201d Future\nimprovements could include incorporating sophisticated shape encoders and learning constraints\nfrom real-world data, such as online images, for a wider range of applications. Finally, our model\ncurrently relies on an outer-loop planner to determine high-level symbolic action plans (e.g., the order\nfor moving objects, which certainly affects the solution). An interesting direction is to consider a\nmore unified approach to determining action orders and sampling for continuous parameters.\nConclusion. In conclusion, this paper proposes a learning-based approach that leverages the com-\npositionality of continuous constraint satisfaction problems (CCSP) and uses diffusion models as\ncompositional constraint solvers. The proposed algorithm, namely the compositional diffusion\nconstraint solver (Diffusion-CCSP), consists of modular diffusion models for different types of\nconstraints. By conditioning these models on subsets of variables, solutions can be generated for the\nentire CCSP. The model exhibits strong generalization capabilities and can handle novel combinations\nof known constraints, even with more objects and constraints than encountered during training. It can\nalso be integrated with search algorithms to solve general task and motion planning problems.\nAcknowledgements. This work is in part supported by NSF grant 2214177, AFOSR grant FA9550-\n22-1-0249, FA9550-23-1-0127, ONR MURI grant N00014-22-1-2740, the MIT-IBM Watson Lab,\nthe MIT Quest for Intelligence, the Center for Brain, Minds, and Machines (CBMM, funded by\nNSF STC award CCF-1231216), the Stanford Institute for Human-Centered Artificial Intelligence\n(HAI), and Analog Devices, JPMC, and Salesforce. Any opinions, findings, and conclusions or\nrecommendations expressed in this material are those of the authors and do not necessarily reflect the\nviews of our sponsors.\n9\nReferences\n[1] A. Mousavian, C. Eppner, and D. Fox. 6-DOF GraspNet: Variational Grasp Generation for\nObject Manipulation. In ICCV, 2019. 1\n[2] W. Liu, C. Paxton, T. Hermans, and D. Fox. StructFormer: Learning Spatial Structure for\nLanguage-Guided Semantic Rearrangement of Novel Objects. In ICRA, 2022. 1, 2\n[3] L. P. Kaelbling and T. Lozano-P\u00b4erez. Hierarchical Task and Motion Planning in the Now. In\nICRA, 2011. 1\n[4] C. R. Garrett, T. Lozano-P\u00b4erez, and L. P. Kaelbling. PDDLStream: Integrating Symbolic\nPlanners and Blackbox Samplers via Optimistic Adaptive Planning. In ICAPS, 2020. 1, 21\n[5] J. Mao, T. Lozano-P\u00b4erez, J. B. Tenenbaum, and L. Kaelbling. PDSketch: Integrated Domain\nProgramming, Learning, and Planning. In NeurIPS, 2022. 1\n[6] C. Paxton, C. Xie, T. Hermans, and D. Fox. Predicting Stable Configurations for Semantic\nPlacement of Novel Objects. In CoRL, 2022. 1\n[7] Z. Yang, C. R. Garrett, T. Lozano-P\u00b4erez, L. Kaelbling, and D. Fox. Sequence-Based Plan\nFeasibility Prediction for Efficient Task and Motion Planning. In RSS, 2023. 1\n[8] M. Toussaint. Logic-Geometric Programming: An Optimization-Based Approach to Combined\nTask and Motion Planning. In IJCAI, 2015. 1\n[9] T. Pang, H. Suh, L. Yang, and R. Tedrake. Global Planning for Contact-Rich Manipulation via\nLocal Smoothing of Quasi-dynamic Contact Models. arXiv:2206.10787, 2022. 1\n[10] M. Shridhar, L. Manuelli, and D. Fox.\nCLIPort: What and Where Pathways for Robotic\nManipulation. In CoRL, 2021. 2\n[11] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet. Learning\nLatent Plans from Play. In CoRL, 2020. 2\n[12] N. Gkanatsios, A. Jain, Z. Xian, Y. Zhang, C. Atkeson, and K. Fragkiadaki. Energy-based\nModels as Zero-Shot Planners for Compositional Scene Rearrangement. In RSS, 2023. 2\n[13] O. Mees, L. Hermann, and W. Burgard. What Matters in Language Conditioned Robotic\nImitation Learning over Unstructured Data. RA-L, 2022. 2\n[14] W. Liu, Y. Du, T. Hermans, S. Chernova, and C. Paxton. StructDiffusion: Language-Guided\nCreation of Physically-Valid Structures using Unseen Objects. In RSS, 2023. 2, 6, 12\n[15] A. Simeonov, Y. Du, B. Kim, F. R. Hogan, J. Tenenbaum, P. Agrawal, and A. Rodriguez. A\nLong Horizon Planning Framework for Manipulating Rigid Pointcloud Objects. In CoRL, 2020.\n2\n[16] A. Curtis, X. Fang, L. P. Kaelbling, T. Lozano-P\u00b4erez, and C. R. Garrett.\nLong-Horizon\nManipulation of Unknown Objects via Task and Motion Planning with Estimated Affordances.\nIn ICRA, 2022. 2\n[17] C. Paxton, C. Xie, T. Hermans, and D. Fox. Predicting Stable Configurations for Semantic\nPlacement of Novel Objects. In CoRL, 2021. 2\n[18] W. Yuan, C. Paxton, K. Desingh, and D. Fox. SORNet: Spatial Object-Centric Representations\nfor Sequential Manipulation. In CoRL, 2021. 2\n[19] A. Bobu, C. Paxton, W. Yang, B. Sundaralingam, Y.-W. Chao, M. Cakmak, and D. Fox. Learning\nPerceptual Concepts by Bootstrapping from Human Queries. RA-L, 2022. 2\n10\n[20] A. Simeonov, Y. Du, L. Yen-Chen, , A. Rodriguez, L. P. Kaelbling, T. Lozano-P\u00b4erez, and\nP. Agrawal. SE(3)-Equivariant Relational Rearrangement with Neural Descriptor Fields. In\nCoRL, 2022. 2\n[21] Y. Du, S. Li, and I. Mordatch. Compositional Visual Generation with Energy Based Models. In\nNeurIPS, 2020. 2\n[22] N. Liu, S. Li, Y. Du, J. Tenenbaum, and A. Torralba. Learning to Compose Visual Relations. In\nNeurIPS, 2021. 2\n[23] W. Nie, A. Vahdat, and A. Anandkumar. Controllable and Compositional Generation with\nLatent-Space Energy-Based Models. In NeurIPS, 2021. 2\n[24] N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum. Compositional Visual Generation with\nComposable Diffusion Models. In ECCV, 2022. 2, 4, 6, 15\n[25] J. Urain, A. Li, P. Liu, C. D\u2019Eramo, and J. Peters. Composable Energy Policies for Reactive\nMotion Generation and Reinforcement Learning. In RSS, 2021. 2\n[26] Y. Du, C. Durkan, R. Strudel, J. B. Tenenbaum, S. Dieleman, R. Fergus, J. Sohl-Dickstein,\nA. Doucet, and W. Grathwohl. Reduce, Reuse, Recycle: Compositional Generation with\nEnergy-Based Diffusion Models and MCMC. In ICML, 2023. 2, 4\n[27] T. Wu, M. Tjandrasuwita, Z. Wu, X. Yang, K. Liu, R. Sosic, and J. Leskovec. ZeroC: A\nNeuro-Symbolic Model for Zero-shot Concept Recognition and Acquisition at Inference Time.\nIn NeurIPS, 2022. 2\n[28] J. Egeblad, B. K. Nielsen, and A. Odgaard. Fast Neighborhood Search for Two- and Three-\nDimensional Nesting Problems. European Journal of Operational Research, 183(3):1249\u20131266,\n2007. 3\n[29] X. Liu, J.-m. Liu, A.-x. Cao, and Z.-l. Yao. HAPE3D\u2014a new constructive algorithm for the 3D\nirregular packing problem. Frontiers of Information Technology & Electronic Engineering, 16\n(5):380\u2013390, 2015. 3\n[30] F. Wang and K. Hauser. Stable Bin Packing Of Non-Convex 3D Objects With A Robot\nManipulator. In ICRA, 2019. 3\n[31] G. O. Roberts and R. L. Tweedie. Exponential Convergence of Langevin Distributions and\nTheir Discrete Approximations. Bernoulli, pages 341\u2013363, 1996. 4\n[32] A. Q. Nichol and P. Dhariwal. Improved Denoising Diffusion Probabilistic Models. In ICML,\n2021. 4\n[33] J. Urain, N. Funk, G. Chalvatzaki, and J. Peters. SE(3)-DiffusionFields: Learning Cost Functions\nfor Joint Grasp and Motion Optimization Through Diffusion. In ICRA, 2023. 5, 15\n[34] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva,\nS. Song, H. Su, et al. ShapeNet: An Information-Rich 3D Model Repository. arXiv:1512.03012,\n2015. 8\n[35] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, et al.\nSAPIEN: A SimulAted Part-based Interactive ENvironment. In CVPR, 2020. 8\n[36] C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-P\u00b4erez.\nIntegrated Task And Motion Planning. Annual Review Of Control, Robotics, And Autonomous\nSystems, 4:265\u2013293, 2021. 21\n[37] N. T. Dantam, Z. K. Kingston, S. Chaudhuri, and L. E. Kavraki. An Incremental Constraint-\nBased Framework for Task and Motion Planning. IJRR, 37(10):1134\u20131151, 2018. 21\n11\nSupplementary Material for\nCompositional Diffusion-Based Continuous Constraint Solvers\nAppendix A describes how our datasets are collected for each task and how the inputs are encoded\nfor diffusion models. We also provided discussions on sample efficiency of Diffusion-CCSP on the\ntasks studied. Appendix B details hyper-parameter choices of our models, implementation details of\nStructDiffusion baseline [14], as well as training details for both. We have also included additional\ndiscussions on model training and fine-tuning, constraint weights, local optima in optimization, and\npartial observability. Appendix C discusses common failure cases in solutions generated by out\nDiffusion-CCSP. Appendix D includes a detailed experiment on the number of samples it takes\nfor Diffusion-CCSP to solve CCSP problems in each task. Appendix E discusses how to integrate\nDiffusion-CCSP for CCSP solving with task and motion planning (TAMP) algorithms. Appendix F\nshows the learned energy landscapes for qualitative constraints in 2D shape arrangement tasks.\nA\nProblem Domains and Data Generation\nIn all datasets, the number the examples involving different number of objects are the same.\nA.1\n2D Triangle Packing\nFigure 7: Example collision-free configurations of triangles\nData. The data is generated by randomly sampling points in the square and, connecting them with\neach other and the edge points using the Bowyer\u2013Watson algorithm, then shrinking each triangle a\nlittle to make space among them.\nEncoding. We define the resting pose of a triangle as the pose when the vertex A facing the shortest\nside BC is at origin and its longest side AB is aligned along +x axis. Its geometry is encoded using\nthree numbers, the length |AB| and the vector (x, y) = \u21c0\nAC. Its pose is encoded as the 2D coordinates\nof vertex A, along with the sin and cos values of the rotation \u03b8 from the testing pose.\nA.2\n2D Shape Arrangement with Qualitative Constraints\nFigure 8: Example rearrangements of rectangles with 11 types of qualitative constraints.\nData. The data is generated by recursively splitting the tray at different proportions until depth 3. For\neach resulting region, a random padding is added to each side. Regions whose area or side is too small\n12\nare discarded. Labels of qualitative constraints are created by hand-crafted rules, e.g. \u2018close-to\u2019\nmeans the distance between two objects is smaller than the maximum width of two objects. All qualita-\ntive constraints include \u2018center-in\u2019, \u2018left-in\u2019, \u2018right-in\u2019, \u2018top-in\u2019, \u2018bottom-in\u2019,\n\u2018left-of\u2019, \u2018top-of\u2019, \u2018close-to\u2019, \u2018away-from\u2019, \u2018h-aligned\u2019, and \u2018v-aligned\u2019.\nEncoding. The resting pose of a rectangle is when its longer side is oriented horizontally. Its\ngeometry is encoded using its width and length at resting pose. Its pose is the 2D pose of the centroid,\nalong with the sin and cos encoding of the object\u2019s yaw rotation. Many problems require some\nobjects to be at vertical positions in order for all the constraints to be satisfied.\nA.3\n3D Object Stacking with Stability Constraints\nFigure 9: Example stable configurations of rectangles, with in, cfree, and supported-by con-\nstraints. There is at least one object that\u2019s supported by multiple objects.\nData. The data is generated by randomly splitting a 2D vertical region, then shrinking and cutting\neach region. The resulting cuboids are initiated in PyBullet simulator and letting them drop until rest.\nWe filter for configurations where the final state is stable. Then all objects are removed one by one to\ntest that each intermediate configuration is also stable. An upper shelf is added to be just enough to\nfit the current configuration with a little overhead space.\nEncoding. The resting pose of a cuboid is when its longer side is oriented horizontally. The cuboid\u2019s\ngeometry is encoded using its width and length. The cuboid\u2019s pose is the 2D pose of the centroid,\nalong with the sin and cos encoding of the cuboid\u2019s roll rotation.\nA.4\n3D Robot Packing with Robots\nFigure 10: Example configuration of 3D shapes that enables the robot place each object in a given\nsequence without colliding into other objects already placed. All the bottles, dispensers, and bowls\naffords only side grasps.\n13\nData.\nThe 10 categories of objects used in this dataset include \u2018Dispenser\u2019, \u2018Bowl\u2019,\n\u2018StaplerFlat\u2019, \u2018Eyeglasses\u2019, \u2018Pliers\u2019, \u2018Scissors\u2019, \u2018Camera\u2019, \u2018Bottle\u2019,\n\u2018BottleOpened\u2019, and \u2018Mug\u2019, contributing in total 53 object assets.\nFor each asset, a fixed\nnumber of grasps are generated beforehand that points directly to one of its five faces, e.g.\n+x, \u2212x, +y, \u2212y, +z. The problems are generated by randomly splitting the tray into rectangle\nregions, then fitting into each region an object with random category, asset, and scale. An order to\nplace the objects is sampled, then a TAMP planner is called to check whether the corresponding\ngrasps can be found (by enumerating all combinations of order and grasps) so that the gripper at each\ngrasp pose won\u2019t collide with any objects already in the goal region and all arm trajectories going to\nthe grasp configurations can be found.\nEncoding. The objects\u2019 geometry is encoded using bounding box (e.g., width, length, height). An\nobject\u2019s grasp is encoded using a five-dimensional vector, indicating the face that the gripper points\nto. For example, [0, 0, 0, 0, 1] is a top grasp. The object\u2019s pose is encoded using its 3D coordinate in\nthe tray frame, along with the sin and cos encoding of the object\u2019s yaw rotation.\nA.5\nDiscussion: Sample Complexity of Diffusion-CCSP\nOur training datasets are on the scale of 10k-30k for each task. It is always important to be aware of\nthe amount of training data needed and the cost of obtaining it from various sources. We observed\nthat we can train the models in simulation with abundant data and believe they can be applied to\nreal-world robotic tasks. In our system, we learn two categories of constraints. First, geometric and\nphysical constraints are independent both of the robot (and hence of perceptual and motor noise and\nirregularities that cause most sim-to-real difficulties) and of any human input. Learning to check that\nobject placements are collision free, or that robot configurations are kinematically feasible can be\ndone entirely via data obtained in simulation and transfer without diffficulty to the real world. Second,\nqualitative constraints corresponding to human input must be obtained from human annotation. Such\nannotations are costly obtain and, as a matter of future work, we would like to leverage additional\ndata sources, such as object relationship information in large vision/language models.\nA.6\nDiscussion: Partial Observability\nAcross all experiments in the paper, we have been assuming a fully-observed setting where we know\nobject shapes (in particular, their bounding boxes) and their initial 3D poses. We think one possible\nway to handle at least some types of partial observability in our setting is through replanning. Taking\nobject packing as a concrete example, given the segmented (partial) point clouds for objects, we use\nlearning or heuristic methods to complete the object mesh and then run our solver to compute plans.\nDuring execution, assuming we can sense object collisions while moving them, we can always replan\nfor a different placement in a closed-loop manner.\nB\nImplementation and Training Details\nWe used 1000 diffusion timesteps for all models, except for Diffusion-CCSP on Task 4 robot packing,\nwhich used 500 diffusion timesteps. In neural network input encoding, all object dimensions and\nposes are normalized with regard to that of the container or shelf region.\nDiffusion-CCSP: Given the geometry, pose, and grasp features, we use geometry, pose, and grasp\nencoders to process them to the same hidden dimension of 256, which is the same dimension as time\nembedding. The outputs from individual diffusion models have the same dimension as input pose\nembedding. The output for each object\u2019s pose are composed by taking the average of all outputs of\ndiffusion models that have the object as an argument. They are processed through a pose decoder\nto get the output reconstructed noise value added to the poses. The encoders, pose decoder, and\ndiffusion models are trained together using the L2 loss between reconstructed noise value and input\nnoise value, as detailed in Section 3.2. For our ULA variant, we used sampling steps of 10 and step\nsizes of 2\u03b2(t) where \u03b2 is cosine beta schedule for the diffusion model.\n14\nRejection-Sampling: The objects are sampled in sequence inside the container (collision-free with\nthe container is guaranteed). For each object, its pose is sampled uniformly at random at most 50\ntimes to find a placement that\u2019s collision-free to all objects already placed.\nStructDiffusion: We implemented the architecture of concatenating the geometry and pose embed-\ndings for each object as one token (dimension 512 for each token) to the transformer encoder, plus\npositional encoding and time encoding. For task 4, we also concatenated the chosen grasp embedding\nto the object embedding (total dimension 768 for each token). We used four layers of residual\nattention blocks and two heads. We added normalization layers before and after the transformer\nlayers. We used the same diffusion timesteps as our Diffusion-CCSP models. Because there is no\nlanguage input, we omitted the word embeddings from the architecture.\nB.1\nDiscussion: Individual Constraint Diffusion Training and Finetuning\nOur diffusion models for individual constraints can be trained either jointly or independently; however\nthey are trained, they can be applied in novel combinations at performance time. In our experiments,\nwe concurrently train all constraints since our data naturally encompasses multiple constraints in a\nsingle scene, such as object collision-free constraints in a bin packing problem. It is also possible to\nfinetune a pretrained constraint solver with new data. In some scenarios such as generalization to\nunseen object shapes this might be very helpful.\nB.2\nDiscussion: Weights for Energy Function Composition\nIn general, selecting appropriate weights for different constraints is important and challenging, which\nhas been observed in Urain et al. [33] and Liu et al. [24]. In our experiments, we consistently use\nweight 1 for all constraints, and this setting has proven effective in our tasks. We postulate two\nreasons for this. First, visualizations of the learned energy functions (see Appendix F) indicate\nthat the energy field is steep. The areas in the configuration space that meet the constraints exhibit\nconsiderably lower energy than areas that don\u2019t. As such, when constraints are combined, the solver\ntends to prioritize solutions that satisfy all constraints. Second, during training, our models are jointly\ntrained to explain data with various combined constraints. This implicitly forces the diffusion models\nto learn score functions that support combinations.\nB.3\nDiscussion: Local Optima in Optimization\nBad local optima can be an issue for multi-constraint optimization problems. In this paper, we\npartially address this problem by running multiple seeds in parallel. That is, given a CCSP problem,\nwe run the reverse diffusion process with multiple randomly sampled initial noise values. For instance,\nin the 3D object stacking task, on average, 33.3 samples are needed for solving tasks involving 5\nobjects. This does not significantly affect our runtime because different random seeds can be run\nin parallel. In two cases shown in the table (object packing and stable stacking), we have used\nthe physical simulator to check solution validity\u2014whether objects are in collision-free poses and\nwhether the system is physically stable. In other cases where solution checkers are unavailable, we\ncan consider learning classifiers for conditions.\nC\nFailure Case Analysis\nExample failure cases for Task 2 involving qualitative constraints are shown in Figure 11. One\ninsight we gained is that the failure rate to satisfy a qualitative constraint is inversely correlated\nwith the area of its defining region. We ran a Diffusion-CCSP 4000 times to solve a test set of 400\nCCSPs involving 2-4 shapes and counted the number of unsatisfied constraints in solutions generated\nby Diffusion-CCSP. From Figure 12a, we make two observations: (1) the failure rate is inversely\ncorrelated with the amount of data for each constraint, and (2) constraints with discontinuous regions\nare harder to satisfy (e.g. v-aligned, close-to). Then, we look at a problem with one object A (size 0.5\nby 0.5) in the center of a container. We estimate the area of the defining region of each constraint\n15\nThe shapes collide with \neach other\nThe shapes don\u2019t satisfy all the \nqualitative constraints\nSatisfying Solution(s)\nFailure Cases\n All Constraints: [(in, i, 0) for i in [1, 2, 3, 4, 5]]; [(cfree, i, j) for i in range(1, 6) for j in range(1,6)]; \n(away-from, 1, 3); (away-from, 1, 4); (away-from, 1, 5); (away-from, 2, 5); (away-from, 3, 5); (away-from, 4, 5); \n(bottom-in, 1, 0); (h-aligned, 5, 2); (h-aligned, 2, 5); (h-aligned, 4, 3); (h-aligned, 3, 4); (left-in, 3, 0); (left-of, 3, 4); \n(right-in, 5, 0); (top-in, 3, 0); (top-in, 4, 0); (top-of, 3, 2); (top-of, 4, 2); where 0 is the container\n Missing:\n(away-from, 3, 4);\n(h-aligned, 2, 5); \n All Constraints: [(in, i, 0) for i in [1, 2, 3, 4, 5]]; [(cfree, i, j) for i in range(1, 6) for j in range(1,6)]; (away-from, 3, 1); \n(away-from, 4, 1); (away-from, 4, 2); (away-from, 5, 1); (away-from, 5, 2); (away-from, 5, 4); (bottom-in, 1, 0); \n(left-in, 1, 0); (left-of, 1, 2); (left-of, 2, 3); (left-of, 3, 5); (right-in, 3, 0); (right-in, 4, 0); (right-in, 5, 0); (top-in, 5, 0);\n Missing:\n(away-from, 4, 5); \n4\n5\n Missing:\n(away-from, 4, 5); \nAll satisfied!\nAll satisfied!\nCCSP example 2 (n=5, OOD)\nCCSP example 1 (n=5, OOD)\n4\n5\n2\n5\n2\n5\n4\n4\n Missing:\n(left-of, 3, 4); \n3\n4\n3\n4\nsolution1\nsolution2\nsolution4\nsolution5\nsolution3\nFigure 11: Failure and success samples for Task 2: 2D Shape Rearrangement with Qualitative\nConstraints. All solutions are generated by Diffusion-CCSP (ULA), trained on 30k data up to 4\nobjects.\nby doing rejection sampling on the pose (x, y) of object B (size 0.5 by 0.5) that doesn\u2019t collide with\nA (except for center-in). From Figure 12b, the order is highly similar to the order in Figure 12a,\nsupporting the hypothesis that the smaller the defining region of a constraint, the harder it is to satisfy.\nNote that this experiment assumes two object. Therefore, although the area is large for cfree in this\nestimate, it doesn\u2019t represent the dataset where there are 2 - 5 shapes in each problem. So we exclude\nit from further discussions.\nExample failure cases for Task 1 and 3 are shown in Figure 13 and Figure 14.\n16\nv-aligned\nclose-to\ncfree\ncenter-in\nh-aligned\ntop-of\nleft-of\nbottom-in\nleft-in\naway-from\ntop-in\nright-in\nin\nQualitative Constraints\n0\n2500\n5000\n7500\n10000\n12500\nCount and Percentage of Failure\n0.19\n0.13\n0.13\n0.12\n0.11\n0.1\n0.04\n0.03\n0.03\n0.03\n0.01\n0.0\n0.0\nFailure Rate of Qualitative Constraints\n(a) Failure rate of qualitative constraints. The\nfailure rate of each constraint is annotated on the\nbars. The red bars denote the number of failures,\nwhile the green bars denote success. Constraints are\nsorted by failure rate.\nv-aligned\ntop-of\nleft-of\nh-aligned\nclose-to\ncenter-in\nbottom-in\ntop-in\nleft-in\nright-in\naway-from\nin\ncfree\nQualitative Constraints\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEstimated Area\nEstimated Area Occupied by Qualitative Constraints\n(b) Estimated area occupied by qualitative con-\nstraints. The order is highly similar to the order in\n(a), supporting the hypothesis that the smaller the\ndefining region of a constraint, the harder it is to sat-\nisfy.\n(c) Training data distribution for each constraint, in the order of increasing area of the defining region. Each\npoint is the centroid for an object A that appeared in one constraint(A, B) inside the training dataset.\nFigure 12: Analysis of how failure rate of qualitative constraints correlates with area of its defining\nregion.\n17\nThe shapes collide with each other\nOne Satisfying Solution\nFailure Cases\nn=5 (out-of-distribution)\nCCSP1\nCCSP2\nCCSP3\nCCSP4\nCCSP5\nsolution1\nn=6 (OOD)\nsolution2\nsolution3\nsolution4\nFigure 13: Failure and success samples for Task 1: 2D Triangle Packing. All solutions are\ngenerated by Diffusion-CCSP (ULA), trained on 30k data up to 4 objects.\n18\nThe objects penetrate* \ninto each other, \nresulting in objects \nexploding once sim is on\nThe stack isn\u2019t \nstable, resulting in \nobjects falling off \nwhen sim is on\nSatisfying Solution (s)*\nFailure Cases\n* slight penetration is allowed by Pybullet simulator\n* in the 1st example, both \nthe solution (sim t=0) and \nthe final scene (sim t=600) \nare shown \nThe stack doesn\u2019t \nsatisfy the given \nsupport structure \namong shapes\n1\n3\nCCSP Example 1 - Given Support Structure:  where 0 is the lower shelf \n[(on, 1, 0), (on, 2, 0), (on, 3, 1), (on, 3, 2), (on, 4, 3), (on, 5, 4), (on, 6, 0)] \nAll correct!\nCCSP Example 2 -  Support Structure:\n[(on, 1, 0), (on, 3, 0), (on, 4, 0), (on, 2, 1), \n(on, 5, 2), (on, 6, 3), (on, 6, 4), (on, 7, 6)] \nsolution1\nsolution2\nsolution4\nsolution3\n1\n3\n2\n7\n6\n5\n7\n6\nAll correct!\nsolution1\nexploded\nsolution2 \nfallen off\nsolution3 \nwrong structure\nFigure 14: Failure and success samples for Task 3: 3D Object Stacking with Stability Constraints.\nAll solutions are generated by Diffusion-CCSP (ULA), trained on 24k data up to 7 objects.\n19\nD\nAdditional Sampler Comparison\nIn the main experiment section, we let Diffusion-CCSP and baselines generate 10 samples for each\nCCSP and check if all constraints are satisfied. Here we let Diffusion-CCSP generate 100 samples on\n100 problems, and plot the number of problems it took Diffusion-CCSP to solve for each task. In a\nbatch of 100 CCSPs, it takes on average 0.01-0.04 sec for Diffusion-CCSP (Reverse) to solve each\nCCSP while 0.06-0.19 sec for Diffusion-CCSP (ULA) to solve each CCSP, as shown in Table 2.\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n# of CCSPs solved\n2 Objects\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n3 Objects\nDiCCSP (ULA)\nDiCCSP (Reverse)\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n4 Objects\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n5 Objects (OOD)\n(a) Task 1: 2D Triangle Packing\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n# of CCSPs solved\n2 Objects\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n3 Objects\nDiCCSP (ULA)\nDiCCSP (Reverse)\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n4 Objects\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n5 Objects (OOD)\n(b) Task 2: 2D Shape Arrangement with Qualitative Constraints\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n# of CCSPs solved\n5 Objects\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n6 Objects\nDiCCSP (ULA)\nDiCCSP (Reverse)\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n7 Objects\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n4 Objects (OOD)\n(c) Task 3: 3D Object Stacking with Stability Constraints\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n# of CCSPs solved\n3 Objects\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n4 Objects\nDiCCSP (ULA)\nDiCCSP (Reverse)\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n5 Objects\n0 10 20 30 40 50 60 70 80 90 x\nNumber of samples\n0\n20\n40\n60\n80\n100\n6 Objects (OOD)\n(d) Task 4: 3D Object Packing with Robots\nFigure 15: Number of Diffusion-CCSP runs it takes to solve 100 CCSPs. OOD means problems that\nare out of training distribution (i.e. include more objects than trained on). On the x-axis, tick x means\nproblems not solved within 100 samples.\nE\nIntegration with Task and Motion Planning Algorithms\nSo far, we have presented a generic solution to solving constraint satisfaction problems that involve\ngeometric and physical constraints. However, assuming that the constraint graph is given as input to\n20\n2 Objects\n3 Objects\n4 Objects\n5 Objects\nDiffusion-CCSP (ULA)\n0.10\n0.10\n0.11\n0.12\nDiffusion-CCSP (Reverse)\n0.01\n0.01\n0.01\n0.01\nStructDiffusion\n0.08\n0.08\n0.08\n0.08\n(a) Task 1. 2D Triangle Packing\n2 Objects\n3 Objects\n4 Objects\n5 Objects\nDiffusion-CCSP (ULA)\n0.17\n0.18\n0.19\n0.19\nDiffusion-CCSP (Reverse)\n0.04\n0.04\n0.04\n0.04\n(b) Task 2. 2D Shape Arrangement with Qualitative Constraints\n4 Objects\n5 Objects\n6 Objects\n7 Objects\nDiffusion-CCSP (ULA)\n0.15\n0.16\n0.17\n0.19\nDiffusion-CCSP (Reverse)\n0.01\n0.01\n0.01\n0.02\n(c) Task 3: 3D Object Stacking with Stability Constraints\n3 Objects\n4 Objects\n5 Objects\n6 Objects\nDiffusion-CCSP (ULA)\n0.06\n0.06\n0.07\n0.07\nDiffusion-CCSP (Reverse)\n0.01\n0.01\n0.01\n0.01\nStructDiffusion\n0.09\n0.09\n0.08\n0.08\n(d) Task 4: 3D Object Packing with Robots\nTable 2: Average run time (sec) of models in a batch of 100 CCSPs.\nthe algorithm. This approach can be directly used to solve particular tasks such as the pose prediction\ntask in object rearrangements. Next, we illustrate how the proposed method can be integrated with a\nsearch algorithm to solve general task and motion planning (TAMP) problems, where the constraint\ngraphs are automatically constructed based on the sequence of actions that has been applied and\nthe goal specification of the task. For brevity, we will present a simplified formulation of TAMP\nproblems. For more details, please refer to the recent survey [36].\nFormally, given a space S of world states, a problem is a tuple \u27e8S, s0, G, A, T \u27e9, where s0 \u2208 S is\nthe initial state (e.g., the geometry and poses of all objects), G \u2286 S is a goal specification (e.g.,\nas a logical expression that can be evaluated based on a state: in(A, Box) and in(B, Box)), A is a\nset of continuously parameterized actions that the agent can execute (e.g., pick-and-place), and T\nis a partial environmental transition model T : S \u00d7 A \u2192 S. Each action a is parameterized by\ntwo functions: precondition prea and effect effa. The semantics of this parameterization is that:\n\u2200s \u2208 S.\u2200a \u2208 A.prea(s) \u21d2 (T (s, a) = effa(s)).\nThe goal of task and motion planning is to output a sequence of actions \u00afa so that the terminal state sT\ninduced by applying ai sequentially following T satisfies sT \u2208 G. The state space and action space\nare usually represented as STRIPS-like representations: the state is a collection of state variables and\neach action is parameterized by a set of arguments. Figure 16 shows a simplified definition of the\npick-and-place action in a table-top manipulation domain.\nA characteristic feature of TAMP problems is that the decision variables (i.e., the arguments of\nactions) include both discrete variables (e.g., object names) and continuous variables (e.g., poses and\ntrajectories). Therefore, one commonly used solution is to do a bi-level search [37, 4]: the algorithm\nfirst finds a plan that involves only discrete objects (called a plan skeleton) and uses a subsequent\nCSP solver to find assignments of continuous variables, backtracking to try a different high-level plan\nif the CCSP is found to be infeasible. For example, consider the discrete task plan (also called plan\nskeleton): pick-and-place(A), pick-and-place(B). There are six parameters to decide: the grasps\non A and B, the place locations of A and B, and the robot trajectory while transporting A and B.\nBased on the preconditions of actions and the goal condition of the task, we can obtain the constraint\n21\n;; pick up x and and place x to p.\naction pick-place(x, g, p, t)\npre: valid-grasp(x, pose[x], g)\n;; g is the grasp pose on x\nvalid-traj(x, pose[x], g, p, t)   ;; p is the target pose of x \nforall z. cfree(x, z, pose[x], t) ;; t is the robot trajectory\neff: pose[x] := p                      ;; object x will be moved\ngoal: in(A, C, pose[A], pose[C]) and in(B, C, pose[B], pose[C])\nThe precondition-effect definition of a continuously parameterized \naction pick-place: pick and place object x.\nFigure 16: Illustration of a simple task and motion planning problem. The domain contains only\none action: pick-and-place of objects. It contains three preconditions: g should be a valid grasp pose\non object x, t should be a valid robot trajectory that moves x from its current pose to the target pose\np, and during the movement, the robot and the object x should not collide with any other objects y. If\nsuccessful, the object will be moved to the new location. The goal of the task is to pack both objects\ninto the target box without any collisions.\n\ud835\udc5d!\"\n\ud835\udc5d#\"\n\ud835\udc5d$\"\n\ud835\udc54!\n\ud835\udc5d!\n\ud835\udc61!\nvalid-g\nvalid-t\n\ud835\udc54!\n\ud835\udc5d!\n\ud835\udc61!\nvalid-g\nvalid-t\nin\nin\n(b) The corresponding TAMP constraint graph derived from the plan \nskeleton. cfree constraints and geometric features are omitted for brevity.\n(a) A partially specified plan skeleton \nand its corresponding constraints.\npick-place(A, gA, pA, tA)\nvalid-grasp(A, pA0, gA)\nvalid-traj(A, pA0, gA, pA, tA)\ncfree(A, B, pB0, tA)\ncfree(A, C, pC0, tA)\npick-place(B, gB, pB, tB)\nvalid-grasp(B, pB0, gB)\nvalid-traj(B, pB0, gB, pB, tB)\ncfree(B, A, pA, tB)\ncfree(B, C, pC0, tB)\nGoal\nin(A, C, pA, pC0)\nin(B, C, pB, pC0)\nFigure 17: Example of a partially specified plan skeleton in task and motion planning, together with\nthe corresponding set of constraints. pA0, pB0, and pC0 corresponds to the initial pose of objects.\ngraph. Therefore, each solution to the CCSP constructed based on the plan skeleton corresponds to a\nconcrete plan of the original planning problem. By combining the high-level search of plan skeletons\nand our constraint solver Diffusion-CCSP, the integrated algorithm is capable of solving a wide range\nof task and motion planning problems that involves geometric, physical, and qualitative constraints.\nF\nLearned Gradient Fields\nWe visualized the learned gradient functions for all qualitative constraints trained for Task 2 (Figure 18\ncolumn 3). The gradient functions for the first 6 constraints (from in to bottom-in) corresponds to\nthe direction of changing the pose of a square shape of size 0.15 by 0.15 inside a container of size 1\nby 1 given different initial poses. The gradient functions for the later 7 constraints (from cfree to\nv-aligned in Figure 19) corresponds to the direction of updating the pose of a square shape of size\n0.15 by 0.15 inside a container of size 1 by 1 where another square shape of the same size has been\nfixed at pose (0, 0). We show a pose sampled by Diffusion-CCSP for the target object in column 4.\nIn column 5, we visualized the distribution of poses that satisfy each qualitative constraint in the\ntraining data. Note that the poses are normalized against the size of the container and the shapes of\nthose objects are no longer 0.15 by 0.15.\nWe also computed the energy function as the difference between input noise and reconstructed output\nat the final step of diffusion (Figure 18 column 2). Note that the energy field is steep. Therefore\nthe areas in the configuration space that meet the constraints exhibit considerably lower energy than\nareas that don\u2019t. As such, when constraints are combined, the solver tends to prioritize solutions that\nsatisfy all constraints.\n22\nConstraint\nEnergy (t=0)\nGradient (t=0)\nSampled Data\nData Distribution\nin\ncenter-in\nleft-in\nright-in\ntop-in\nbottom-in\nFirefox\n\ufb01le:///home/yang/Documents/HACL-PyTorch/projects...\n1 of 2\n8/19/23, 17:38\nFigure 18: Visualization of energy and gradient functions of trained diffusion models for qualitative\nconstraints (Part 1. Constraints between a shape and the container). Column 2: Energy fields inside\na container of size 1 by 1. Column 3: Gradient functions that update poses in different time steps.\nColumn 4: Poses sampled according to the given qualitative constraint. Column 5: Poses in the\ntraining data set with the given constraint, for 30,000 CCSPs that involve 2 - 4 objects that have\ndifferent sizes. The number of constraints that appeared in the dataset is shown at the top right corner.\n23\nConstraint\nEnergy (t=0)\nGradient (t=0)\nSampled Data\nData Distribution\ncfree\nleft-of\ntop-of\nclose-to\naway-from\nh-aligned\nv-aligned\nFirefox\n\ufb01le:///home/yang/Documents/HACL-PyTorch/projects...\n2 of 2\nFigure 19: Visualization of energy and gradient functions of trained diffusion models for qualitative 8/19/23, 17:38\nconstraints (Part 2. Constraints between two shapes).\n24\n"
  },
  {
    "title": "MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation",
    "link": "https://arxiv.org/pdf/2309.00908.pdf",
    "upvote": "4",
    "text": "MagicProp: Diffusion-based Video Editing via Motion-\naware Appearance Propagation\nHanshu Yan\u2217, Jun Hao Liew\u2217, Long Mai, Shanchuan Lin & Jiashi Feng\nByteDance Inc.\n{hanshu.yan, junhao.liew, long.mai, peterlin, jshfeng}@bytedance.com\nInput Frames\nEdited Frames\nGlobal editing - prompt=\u201ca flamingo in snowy day\u201d\nBackground editing - prompt=\u201ca cow is walking on the sands\u201d\nForeground editing - prompt=\u201ca red toy car moving on the road\u201d\nFigure 1: Video editing via MagicProp: global, background, and foreground editing are all supported.\nABSTRACT\nThis paper addresses the issue of modifying the visual appearance of videos while\npreserving their motion. A novel framework, named MagicProp, is proposed, which\ndisentangles the video editing process into two stages: appearance editing and\nmotion-aware appearance propagation. In the first stage, MagicProp selects a single\nframe from the input video and applies image-editing techniques to modify the con-\ntent and/or style of the frame. The flexibility of these techniques enables the editing\nof arbitrary regions within the frame. In the second stage, MagicProp employs\nthe edited frame as an appearance reference and generates the remaining frames\nusing an autoregressive rendering approach. To achieve this, a diffusion-based\nconditional generation model, called PropDPM, is developed, which synthesizes\nthe target frame by conditioning on the reference appearance, the target motion,\nand its previous appearance. The autoregressive editing approach ensures temporal\nconsistency in the resulting videos. Overall, MagicProp combines the flexibility of\nimage-editing techniques with the superior temporal consistency of autoregressive\nmodeling, enabling flexible editing of object types and aesthetic styles in arbi-\ntrary regions of input videos while maintaining good temporal consistency across\nframes. Extensive experiments in various video editing scenarios demonstrate the\neffectiveness of MagicProp.\n1\nINTRODUCTION\nContent creation often involves video editing, which includes modifying the appearance or adjusting\nthe motion of raw videos [Wu et al., 2023; Kasten et al., 2021; Zhao et al., 2023; Wang et al., 2023].\nFilmmakers may need to adjust the exposure, saturation, and contrast of raw videos for better aesthetic\n\u2217Equal contribution.\n1\narXiv:2309.00908v1  [cs.CV]  2 Sep 2023\nquality, while advertisers may want to change realistic videos into certain fascinating styles to impress\ntarget audiences. This paper addresses the problem of editing videos\u2019 appearance, including changing\nthe content or style locally in a certain spatial region or globally throughout the entire video.\nExisting works attempt to solve this problem mainly from two perspectives: editing each frame\nindividually via image generation models [Qi et al., 2023; Ceylan et al., 2023; Yang et al., 2023;\nKhachatryan et al., 2023; Geyer et al., 2023] or modeling the entire video sequence for appearance\nchanging [Ni et al., 2023; Molad et al., 2023; Karras et al., 2023; Kasten et al., 2021; Esser et al.,\n2023]. Methods based on image models, such as Stable Diffusion [Rombach et al., 2022] and\nControlNet [Zhang and Agrawala, 2023], can flexibly modify the content or style of any arbitrary\nregion, but it is challenging to ensure temporal consistency across adjacent frames. To alleviate this\nissue, some use structure-guided models and cross-frame attention to align color and layout across\nframes [Zhang and Agrawala, 2023; Qi et al., 2023; Ceylan et al., 2023]. Other methods exploit\ninter-frame correspondence, such as optical flow, to warp the features of edited frames [Yang et al.,\n2023; Geyer et al., 2023]. However, the temporal consistency of the edited video is still suboptimal.\nInstead of using image-based models, researchers have developed many sequence-based models for\nvideo generation and editing [Esser et al., 2023; Couairon et al., 2023]. Neural Layered Atlas (NLA)\noverfits a video first and then edits the learned corresponding Atlas to change the foreground or\nbackground [Kasten et al., 2021; Bar-Tal et al., 2022]. NLA-based methods can effectively edit the\nappearance of videos, but test-time optimization is time- and resource-consuming. Recently, many\ndiffusion-based models have been proposed for structure-aware video generation, such as Gen-1\n[Esser et al., 2023], ControlVideo [Zhao et al., 2023; Chen et al., 2023], and VideoComposer [Wang\net al., 2023]. These methods synthesize videos by conditioning on layout sequences such as depth\nor sketch maps, so that the motion coherence in the resultant video can be ensured. However, the\neditability and flexibility will be compromised due to the limitation of textual descriptions and the\ndifficulty of user interaction. For instance, when editing a certain part of a given video, text prompts\nmay not precisely localize the region of interest across all frames, and it may be challenging for users\nto prepare masks for all frames. The trade-off between temporal consistency and editing flexibility\ninspires us to explore other alternative frameworks for video editing.\nMotivated by the fact that frames within a video usually share a similar scene, we propose a novel\nframework, MagicProp, which disentangles video editing into two stages, namely, appearance editing\nand motion-aware appearance propagation. MagicProp first selects one frame from the given video\nand edits its appearance. The edited frame is used as the appearance reference in the second stage.\nThen, MagicProp autoregressively renders the remaining frames by conditioning on the reference\nframe and the motion sequence (e.g., depth maps of the given video). MagicProp models videos in an\nautoregressive manner, which guarantees the temporal consistency of the output videos. Additionally,\nMagicProp uses powerful image diffusion models (optionally with additional masks) for reference\nediting, allowing for flexible modification of the contents of a local region or the entire video.\nThe most crucial component of MagicProp is an autoregressive conditional image diffusion model\nthat synthesizes the target image under the control of its previous frame, the target depth, and the\nreference appearance. We design a lightweight adapter to merge and inject the semantic-level and\npixel-level information of the reference frame into the image generation process, ensuring that the\nappearance of the resultant frames aligns well with the reference. During training, we follow the\nstrategy of zero terminal signal-to-noise ratio (SNR) [Lin et al., 2023], which bridges the gap between\nthe noise schedules during training and inference, resulting in better matching of the color and style\nof generated frames with the reference. We conducted extensive experiments in several video editing\nscenarios, including local object/background editing and global stylization. The results demonstrate\nthe effectiveness and flexibility of MagicProp. The contributions of MagicProp are three-fold:\n\u2022 We proposed a novel framework, MagicProp, that decouples video editing into appearance\nediting and motion-aware appearance propagation.\n\u2022 We devised a lightweight adapter to inject class- and pixel-level features into the diffusion\nmodel. We also applied the zero-terminal SNR strategy for training. These techniques\nfacilitate the alignment of the appearance.\n\u2022 Extensive experiments demonstrate that MagicProp can flexibly edit any arbitrary region of\nthe given video and generate high-quality results.\n2\n2\nRELATED WORKS AND PRELIMINARIES\nIn this section, we first review recent related works on the appearance editing of videos. We categorize\nthem into two groups, i.e., editing a video frame by frame via image models, and modeling the whole\nframe sequence for editing. Then, we introduce the preliminaries about diffusion probabilistic models\nand the notation for video editing.\n2.1\nRELATED WORKS\nFrame-by-frame Editing\nDiffusion-based image generation models have achieved great success\nin image generation and editing tasks [Ho et al., 2020; 2022; Rombach et al., 2022; Blattmann et al.,\n2023]. The simplest method for video editing is to edit each frame individually [Meng et al., 2022;\nLiew et al., 2022; Hertz et al., 2022]. Although it is flexible to edit each frame and the resultant\nframes have a good aesthetic quality, the temporal consistency of the whole video is usually inferior.\nSome methods use the layout condition generation method to edit each frame [Zhang and Agrawala,\n2023; Huang et al., 2023b]. For example, ControlNet [Zhang and Agrawala, 2023] synthesizes\nimages with the conditioning of a text description and an additional layout map, such as a depth map\nor an edge map, thus the spatial layout of the edited frame matches that of the original frame. Whilst\nthese methods can guarantee the layout consistency of the edited videos, the appearance of frames\n(e.g., identity, texture, and color) still changes apparently across frames. To alleviate the issue of\ntemporal consistency, a line of methods rely on cross-frame attention to fuse the latents of edited\nframes and those of their previous frames (or other reference frames) [Qi et al., 2023; Hertz et al.,\n2022; Khachatryan et al., 2023; Ceylan et al., 2023], so that the consistency of shape and style can\nbe improved. Another line of methods exploit the correspondence between frames in the original\nvideo and use it to warp the latent or attention maps when generating future frames [Yang et al., 2023;\nGeyer et al., 2023]. Correspondence-based wrapping may fail due to the occlusion in consecutive\nframes. In general, methods based on per-frame editing still suffer from temporal consistency across\nframes.\nEditing via Sequential Modeling\nVideos are naturally sequential data, and therefore using se-\nquential models for video generation and editing intrinsically benefits temporal consistency. Neural\nLayered Atlas (NLA) [Kasten et al., 2021; Bar-Tal et al., 2022; Huang et al., 2023a] represents a video\nthrough several 2D maps and 2D-to-color atlases. The appearance of objects and backgrounds can\nbe easily edited by modifying the corresponding atlases. However, NLA needs to perform test-time\noptimization for each video to learn its representations, which is very time-consuming. Recently,\ndiffusion models have been proven effective in modeling sequential data like videos. Many methods\nuse video diffusion models or flatten image diffusion models into video models for video editing [Ho\net al., 2022; Blattmann et al., 2023; Zhou et al., 2023; Wang et al., 2023]. Dreamix [Molad et al.,\n2023] and Tune-A-Video [Wu et al., 2023], fine-tune the video model on the provided video first and\nthen generate a new video by conditioning the textual prompt of the editing instruction. Fine-tuning\non the given video cannot sufficiently guarantee that the motion (layout sequence) in the edited\nvideo aligns well with the original. To ameliorate this issue, motion-conditioned video diffusion\nmodels have been proposed, including Gen-1 [Esser et al., 2023], ControlVideo [Zhao et al., 2023;\nChen et al., 2023], and VideoComposer [Wang et al., 2023]. These methods generate video with\nthe condition of a layout sequence, such as depth or edge maps. When editing, one can extract the\nlayout sequence from the given video first and then generate a new video by conditioning the layout\nsequence and an editing text prompt. Overall, editing methods based on video models can effectively\nsynthesize temporally consistent videos, but their editability and image quality are not as good as the\nimage-based models at the current stage due to the limitation of textual description and the difficulty\nof training a good video model. Textual prompts only can provide a high-level semantic description\nof the desired appearance. It is challenging to locate a specific local editing region of a video based\non textual prompts.\nIn contrast, MagicProp disentangles appearance editing and appearance propagation. It can flexibly\nedit the appearance based on powerful image editing methods that can incorporate textural descriptions\nand localization masks. Besides, synthesizing future frames with an autoregressive model also ensures\ntemporal consistency across frames.\n3\n2.2\nPRELIMINARIES\nDenoising Diffusion Probabilistic Model\nDenoising diffusion probabilistic models (DDPM) are\na family of latent generative models that approximate the probability density of training data by\nreversing the Markovian Gaussian diffusion processes [Sohl-Dickstein et al., 2015; Ho et al., 2020].\nConcerning a distribution q(x), DDPM models the probability density q(x) as the marginal of the\njoint distribution between x and a series of latent variables x1:T , i.e., p\u03b8(x) =\nR\np\u03b8(x0:T )dx1:T with\nx = x0. The joint distribution is defined as a Markov chain with learned Gaussian transitions starting\nfrom the standard normal distribution, i.e.,\np\u03b8(xT ) = N(xT ; 0, I)\n(1)\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t))\n(2)\nTo perform likelihood maximization of the parameterized marginal p\u03b8(\u00b7), DDPM uses a fixed\nMarkov Gaussian diffusion process, q(x1:T |x0), to approximate the posterior p\u03b8(x1:T |x0). In\nspecific, two series, \u03b10:T and \u03c32\n0:T , are defined, where 1 = \u03b10 > \u03b11 > . . . , > \u03b1T \u2265 0 and\n0 = \u03c32\n0 < \u03c32\n1 < \u00b7 \u00b7 \u00b7 < \u03c32\nT . For any t > s \u2265 0, q(xt|xs) = N(xt; \u03b1t|sxs, \u03c32\nt|sI), where\n\u03b1t|s = \u03b1t/\u03b1s and \u03c32\nt|s = \u03c32\nt \u2212 \u03b12\nt|s\u03c32\ns. Usually, we set \u03b12\nt + \u03c32\nt = 1, thus,\nq(xt|x0) = N(xt|\u03b1tx0, (1 \u2212 \u03b12\nt )I).\n(3)\nWe use deep neural networks to parameterize the expectation function \u00b5\u03b8(xt, t) of the sampling\nprocess or the denoising function \u03f5\u03b8(xt, t), which can be used to alternatively estimate the expectation\nvia \u00b5\u03b8(xt, t) =\n1\n\u221a\u03b1t|t\u22121 (xt \u2212\n1\u2212\u03b1t|t\u22121\n\u221a1\u2212\u03b1t \u03f5\u03b8(xt, t)). When performing conditional generation tasks,\nthe network should take additional control signals y as input, i.e., \u03f5\u03b8(xt, t, y). The parameterized\nreversed process p\u03b8 can be optimized by maximizing the associated evidence lower bound (ELBO).\nWe plug the Gaussian parameterization into KL-divergence terms, the ELBO optimization turns to be\nnoise estimation, where \u03bb(t) is a weighting function. After training, we can sample new data via the\nMarkov chain defined in Eqn (2). Instead, we also can use deterministic samplers, such as DDIM,\nto generate new data. For a certain starting noise xT \u223c N(xT ; 0, I), the mapping from xT to the\ngenerated datum x0 through a deterministic sampler is denoted by \u03a6(xT , y).\nL = Ex0,t,\u03f5[\u03bb(t)\u2225\u03f5\u03b8(xt) \u2212 \u03f5\u22252\n2].\n(4)\nNotation for Video Editing\nWe denote a video by x = [x1, ..., xK], where xi represents the\nith frame in the sequence and, for each i \u2208 [1, . . . , K], xi \u2208 [\u22121, 1]C\u00d7H\u00d7W . To reduce the\ncomputational overhead of modeling videos, we use a variational auto-encoder (VAE), denoted by\n{E(\u00b7), D(\u00b7)}, to map videos from the RGB space to a lower-dimensional latent space. The video\nframes are transformed one by one, i.e., z = [z1, ..., zK] with zi = E(xi). We follow Stable\nDiffusion which uses an encoder to downsample x into a spatially 8\u00d7 smaller space. The generated\nlatent codes can be decoded to videos by D(\u00b7). The editing operations require users to provide\nextra information describing the desired appearance of the target video. We denote the instruction\ninformation by y; it could be a textual description, an extra localization mask, or other visual reference.\nWe use CLIP, denoted by \u03c4(\u00b7), to encode the text prompt or reference image, and the embedding is\ndenoted \u03c4(y). To preserve the motion of the original video, we use a depth estimation model, such as\nTCMonoDepth, to extract the sequence of depth maps for representing the motion. We denote M(\u00b7)\nas the depth model and m = [m1, . . . , mK] with mi = M(x1) as the depth sequence.\n3\nMETHOD\nThis paper addresses the problem of motion-preserving video editing, where we aim to alter the\nappearance of a given video while retaining the original motion. Typically, frames in a short video\nhave similar scenes, with main objects and backgrounds appearing consistently throughout. It is\nnatural to disentangle the video editing problem into two sub-tasks, viz., editing the appearance of the\nmain objects and/or the background first and then propagating the edited content to all other frames\nbased on the original motion.\nIn this section, we elucidate the pipeline of MagicProp V(\u00b7), which performs video editing in two\nstages sequentially, i.e., appearance editing \u03a61(\u00b7) and motion-aware appearance propagation \u03a62(\u00b7).\n4\nSource video\nStage I: Appearance Editing\n\u201cA robotic bull walking\u201d\nMask (optional)\nKeyframe\nSelect \nkey frame\nT2I Editing\nEdited keyframe\nSource video\nEdited keyframe\nAutoregressive \nmotion \npropagation\nEdited video\nStage II: Motion-aware Appearance Propagation\nFigure 2: The pipeline of MagicProp.\nExtract \nmotion signal \n(e.g., depth)\n\u201cA robotic bull walking\u201d\nImage Editing\nSelect \nkey frame\nKeyframe\nEdited keyframe\nSource video\n\u2205 \ud835\udc5a!, \ud835\udc65%!\"#, \ud835\udc65%$\nTarget frames\nMotion signals\nPrevious frames\ninitialize\n+\nPropDPM\nFigure 3: Auto-regressive Motion-aware Appearance Propagation Diffusion Model\nMagicProp can flexibly edit the appearance of a given video according to users\u2019 instructions. It\nsupports changing the contents (e.g., object type and image style) in any specific region, either\nlocally or globally. Formally, MagicProp takes input as the source video x, a textual prompt y,\nand optionally a localization mask w. This mask can be provided by users or easily obtained by a\npowerful segmentation model. After the two-stage processing, MagicProp generates an edited video\n\u02c6x whose motion remains unchanged.\n3.1\nAPPEARANCE EDITING\nThe first stage of MagicProp is to manipulate the appearance of the source video. We select one\nframe as the appearance reference. Thanks to many effective image-editing methods, we can flexibly\nedit any arbitrary region of the reference frame, including changing object types or visual styles.\nIn specific, we select a frame x# from the input video x as the appearance reference. Existing\nimage editing methods, such as Text-to-Image (T2I) models, offer rich possibilities to manipulate\nimages\u2019 contents [Meng et al., 2022; Liew et al., 2022; Zhang and Agrawala, 2023]. Here, we\nuse the ControlNet optionally with a segmentation mask w to change the main objects and/or the\nbackground. By conditioning the depth map of x# and a textual prompt y, ControlNet will generate\na new image \u02c6x# whose layout matches the original one and semantics aligns with the text description.\nIn comparison to existing Text-to-Video (T2V) models, T2I models, such as Stale Diffusion, have\napparent superiority in terms of per-frame quality. Thus, the resultant frame edited by ControlNet\ncontains rich details and enjoys high aesthetic quality. Besides, T2I diffusion models allow us to use\nlocalization masks to precisely control the editing parts in images. It is flexible to edit a local region\nor the whole image. In brief, stage one chooses and edits a certain frame, and the edited frame will\nbe used as the appearance reference for video synthesis in the second stage.\n\u02c6x# = \u03a61(x, #, y, w)\n(5)\n5\n3.2\nMOTION-AWARE APPEARANCE PROPAGATION\nGiven a source video x and the appearance reference \u02c6x#, the second stage \u03a62(\u00b7) will render a new\nvideo \u02c6x that preserves the motion in source one and whose appearance matches the reference. The\nmost crucial part is an appearance propagation diffusion probabilistic model (PropDPM). PropDPM,\ndenoted by \u03d5\u03b8(\u00b7), synthesizes the whole video in an auto-regressive manner. Each frame \u02c6xk is\ngenerated with the conditioning of the reference appearance \u02c6x#, its corresponding depth map mk,\nand the previous edited frame \u02c6xk\u22121. We can use the edited appearance reference as the starting\nframe, i.e., \u02c6x0 = \u02c6x# and m0 = m#. The rest can be rendered frame-by-frame through Eqn (6) for\nk from 1 to K. The layout in the generated frames aligns with the depth maps extracted from the\ncorresponding frames in the source video. Hence, the motion (layout sequence) remains unchanged\ncompared to the source video, and the temporal consistency in the rendered video is also guaranteed.\n\u02c6xk = \u03d5\u03b8(mk, \u02c6xk\u22121, mk\u22121, \u02c6x#)\n(6)\n\u02c6x = \u03a62(\u02c6x#, x)\n(7)\nIn specific, PropDPM is designed based on the latent diffusion model [Rombach et al., 2022]. We\nuse a VAE {E(\u00b7), D(\u00b7)} to map a video into a lower-dimensional latent space. PropDPM is trained\nto generate the edited latent \u02c6zk and we then use the VAE to reconstruct the edited video frame\n\u02c6xk. For the conditioning signals, we split them into two groups, viz., the spatial conditions and the\nsemantic conditions. The spatial conditions, including the target frame\u2019s depth map and the previous\nframe, provide the spatial layout information for the generated image and form a contrast between\ntwo consecutive frames. This contrast facilitates the synthesis of contents by querying spatially\ncorresponding regions. The semantic conditions include the RGB and the latent of the reference\nframe. They provide information about the color, style, and object classes in the target edited video.\nThe spatial conditions are injected into the PropDPM by concatenating them to the noisy latent. We\nuse the TCMonoDepth [Li et al., 2021] model to estimate depth maps in the RGB space and rescale\nthem into the size of the latent codes. When generating the kth edited frame, we concatenate its depth\nmap mk, the latent of the previous edited frame \u02c6zk\u22121\nt\n, the previous depth map mk\u22121, to the noisy\nlatent \u02c6zt. Instead, the semantic conditions are used as the input of the cross-attention modules. We\ndesign a lightweight adaptor to combine the CLIP\u2019s embedding and the VAE latent of the reference\nframe so that the injected semantics contains both class-wise and patch-wise information.\n3.3\nMODEL DESIGN OF PROPDPM\nThe main challenges of video editing are ensuring temporal consistency across all frames and\nmaintaining per-frame quality. PropDPM addresses the first challenge by editing a video in an\nauto-regressive manner, conditioning on the true depth sequence to ensure temporal coherence across\nframes. However, due to the intrinsic error accumulation issue of auto-regressive modeling, the image\nquality of the edited frames degrades as the frame index increases. While the early edited frames\ncontain rich details, the later edited ones become smooth and suffer from color shifting.\nTo alleviate the error accumulation issue, we propose two complementary solutions. First, we design\nan appearance adaptor that merges the class-level and patch-wise information of the reference frame.\nThe output of this adaptor is sent to cross-attention modules. During inference, we use a fixed\nreference frame for each video when auto-regressively synthesizing frames. A fixed reference frame\nserves as an anchor to ameliorate the degradation. Second, we apply the Zero-Terminal-SNR [Lin\net al., 2023] technique to train the diffusion model, which bridges the gap between the starting noise\u2019s\nstrength during inference and the largest noise level during training. This technique improves the\nimage quality of the generated frame in each iteration.\n3.3.1\nAPPEARANCE ADAPTOR\nWe design a lightweight adaptor to fuse the class-level and pixel-level features of the reference frame.\nThe adaptor preserves the spatial correspondence between the fused tokens and the reference image.\nIn detail, we first use the VAE to extract the latent of the reference image, z# \u2208 R4\u00d7h\u00d7w. The\nlatent codes of VAE have good spatial correspondence to the original images. We use a nonlinear\nnetwork to decrease the redundant spatial resolution of latent z# by a factor of \u00d72 but increase the\nchannel dimension to preserve more information. The resultant feature is in size of Rl/2\u00d7h/2\u00d7w/2,\n6\nwhere l is the length of each CLIP embedding. On the other hand, we use the CLIP model to\nextract the semantics of the reference image. We have a global class token \u03c4(x#)c \u2208 Rl\u00d71 and\npatch-wise tokens \u03c4(x#)p \u2208 Rl\u00d7h\u2032\u00d7w\u2032. We utilize another nonlinear network to downsample the\ntoken dimension and adjust their spatial resolution to Rl/2\u00d7h/2\u00d7w/2. Finally, we apply the third\nnonlinear module to merge the transformed CLIP\u2019s and the VAE\u2019s features into a fused feature in\nsize of Rl\u00d7h/2\u00d7w/2. We concatenate it with the untouched class-level token and use it (reshaped into\nthe size of Rl\u00d7(hw/4+1)) as the input of cross-attention modules. Since the fused tokens contain rich\nglobal and local information, PropDPM can generate a target frame that better matches the reference\u2019s\nappearance.\n3.3.2\nZERO-TERMINAL-SNR NOISE SCHEDULE\nDiffusion models are trained to estimate the noise in the noisy intermediate state xt for t \u2208 [1, . . . , T],\nwhere xt = \u03b1tx0 +\np\n1 \u2212 \u03b12\nt \u03f5. In the vanilla DDPM, the noise schedule is set to be 1 = \u03b10 >\n\u03b11 > \u00b7 \u00b7 \u00b7 > \u03b1T > 0, where the terminal signal-to-noise-ratio (SNR), SNR(t) = \u03b12\nt /(1 \u2212 \u03b12\nt ),\nis greater than 0. This means the strongest noise, that the obtained DDPM can handle, is xT =\n\u03b1T x0 +\np\n1 \u2212 \u03b12\nT \u03f5 rather than the pure noise \u03f5. However, during inference, most samplers start\nfrom pure noise. This gap may incur the degradation of the generated data. To fix this issue, Lin et\nal. [ ] propose a novel noise schedule, termed Zero-Terminal-SNR, which forces the SNR(T) to be\nzero and make the UNet v\u03b8(zt) to predict the v-value instead of noise \u03f5. The v-value is defined as\nvt = \u03b1t\u03f5 \u2212\np\n(1 \u2212 \u03b12\nt )x0. We follow the Zero-Terminal-SNR strategy for training our PropDPM\nmodel. The experimental results verify the effectiveness of alleviating the color-shifting issue.\n3.3.3\nTRAINING\nThe PropDPM is initialized from the Stable-Diffusion-v1.5. We train the PropDPM model on the\ncombination of a public video dataset, WebVid-10M [Bain et al., 2021], and a self-collected private\ndataset. For the public one, we randomly sample 1 million videos, while the self-collected dataset\ncontains 20 thousand high-resolution videos without watermarks. From each video, we sample at\nmost 30 frames with a step size of four. These frames are then center-cropped into squares and\nresized into the shape of 256 \u00d7 256. During training, we randomly select three frames from a video\nto form a triplet: the reference frame, the previous frame, and the target frame.\n4\nAPPLICATION\nMagicProp can edit any arbitrary region in the given video. In Figure 4 and Figure 5, we show the\nrendered videos. We use masks and ControlNet to localize and modify certain parts. The masks can\nbe either provided by users or extracted by a segmentation model (e.g., Segment-Anything).\nThrough extensive experiments, we find MagicProp can robustly edit videos up to 30 frames. Degra-\ndation, such as over-smoothing and artifacts, may appear when the length of videos exceeds 30\nframes due to the intrinsic error accumulation of Auto-regressive inference. For future work, we aim\nto improve the current MagicProp framework for processing longer videos.\nREFERENCES\nM. Bain, A. Nagrani, G. Varol, and A. Zisserman. Frozen in time: A joint video and image encoder\nfor end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.\nO. Bar-Tal, D. Ofri-Amar, R. Fridman, Y. Kasten, and T. Dekel. Text2LIVE: Text-Driven Layered\nImage and Video Editing, May 2022. arXiv:2204.02491 [cs].\nA. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align\nyour Latents: High-Resolution Video Synthesis with Latent Diffusion Models, Apr. 2023.\narXiv:2304.08818 [cs].\nD. Ceylan, C.-H. P. Huang, and N. J. Mitra. Pix2Video: Video Editing using Image Diffusion, Mar.\n2023. arXiv:2303.12688 [cs].\n7\nInput Frames\nEdited Frames\nbackground editing,  prompt = \u201ca cow is walking on the sands\u201d\nbackground editing,  prompt = \u201ca swan swimming in a red river\u201d\nforeground editing, prompt = \u201ca yellow Lego boat travelling on the river\u201d\nforeground editing, prompt = \u201ca red toy car moving on the road\u201d\nFigure 4: Examples for local editing\u2014background (the top two) and foreground editing (the bottom\ntwo).\nInput Frames\nEdited Frames\nediting prompt = \u201ca bear walking through stars\u201d\nediting prompt = \u201ca bull is walking\u201d\nediting prompt = \u201ca flamingo in snowy day\u201d\nediting prompt = \u201ca jeep car is moving on the snow\u201d\nFigure 5: Examples for global editing.\nW. Chen, J. Wu, P. Xie, H. Wu, J. Li, X. Xia, X. Xiao, and L. Lin. Control-A-Video: Controllable\nText-to-Video Generation with Diffusion Models, May 2023. arXiv:2305.13840 [cs].\nP. Couairon, C. Rambour, J.-E. Haugeard, and N. Thome. VidEdit: Zero-Shot and Spatially Aware\nText-Driven Video Editing, June 2023. arXiv:2306.08707 [cs].\nP. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis. Structure and Content-Guided\nVideo Synthesis with Diffusion Models, Feb. 2023. arXiv:2302.03011 [cs].\nM. Geyer, O. Bar-Tal, S. Bagon, and T. Dekel. TokenFlow: Consistent Diffusion Features for\nConsistent Video Editing, July 2023. arXiv:2307.10373 [cs].\nA. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-Or. Prompt-to-Prompt\nImage Editing with Cross Attention Control, Aug. 2022. arXiv:2208.01626 [cs].\n8\nJ. Ho, A. Jain, and P. Abbeel. Denoising Diffusion Probabilistic Models, Dec. 2020. arXiv:2006.11239\n[cs, stat].\nJ. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video Diffusion Models.\nTechnical Report arXiv:2204.03458, arXiv, June 2022. arXiv:2204.03458 [cs] type: article.\nJ. Huang, L. Sigal, K. M. Yi, O. Wang, and J.-Y. Lee. INVE: Interactive Neural Video Editing, July\n2023a. arXiv:2307.07663 [cs].\nL. Huang, D. Chen, Y. Liu, Y. Shen, D. Zhao, and J. Zhou. Composer: Creative and Controllable\nImage Synthesis with Composable Conditions, Feb. 2023b. arXiv:2302.09778 [cs].\nJ. Karras, A. Holynski, T.-C. Wang, and I. Kemelmacher-Shlizerman. DreamPose: Fashion Image-to-\nVideo Synthesis via Stable Diffusion, May 2023. arXiv:2304.06025 [cs].\nY. Kasten, D. Ofri, O. Wang, and T. Dekel. Layered Neural Atlases for Consistent Video Editing,\nSept. 2021. arXiv:2109.11418 [cs].\nL. Khachatryan, A. Movsisyan, V. Tadevosyan, R. Henschel, Z. Wang, S. Navasardyan, and H. Shi.\nText2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators, Mar. 2023.\narXiv:2303.13439 [cs].\nS. Li, Y. Luo, Y. Zhu, X. Zhao, Y. Li, and Y. Shan. Enforcing temporal consistency in video depth\nestimation. In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),\npages 1145\u20131154, 2021. doi: 10.1109/ICCVW54120.2021.00134.\nJ. H. Liew, H. Yan, D. Zhou, and J. Feng. MagicMix: Semantic Mixing with Diffusion Models, Oct.\n2022. arXiv:2210.16056 [cs].\nS. Lin, B. Liu, J. Li, and X. Yang. Common Diffusion Noise Schedules and Sample Steps are Flawed,\nMay 2023. arXiv:2305.08891 [cs].\nC. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. SDEdit: Guided Image Synthesis\nand Editing with Stochastic Differential Equations. Technical Report arXiv:2108.01073, arXiv,\nJan. 2022. arXiv:2108.01073 [cs] type: article.\nE. Molad, E. Horwitz, D. Valevski, A. R. Acha, Y. Matias, Y. Pritch, Y. Leviathan, and Y. Hoshen.\nDreamix: Video Diffusion Models are General Video Editors, Feb. 2023. arXiv:2302.01329 [cs].\nH. Ni, C. Shi, K. Li, S. X. Huang, and M. R. Min. Conditional Image-to-Video Generation with\nLatent Flow Diffusion Models, Mar. 2023. arXiv:2303.13744 [cs].\nC. Qi, X. Cun, Y. Zhang, C. Lei, X. Wang, Y. Shan, and Q. Chen. FateZero: Fusing Attentions for\nZero-shot Text-based Video Editing, Mar. 2023. arXiv:2303.09535 [cs].\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image Synthesis\nwith Latent Diffusion Models, Apr. 2022. arXiv:2112.10752 [cs].\nJ. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli. Deep Unsupervised Learning\nusing Nonequilibrium Thermodynamics. page 10, 2015.\nX. Wang, H. Yuan, S. Zhang, D. Chen, J. Wang, Y. Zhang, Y. Shen, D. Zhao, and J. Zhou. VideoCom-\nposer: Compositional Video Synthesis with Motion Controllability, June 2023. arXiv:2306.02018\n[cs].\nJ. Z. Wu, Y. Ge, X. Wang, W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou. Tune-\nA-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation, Mar. 2023.\narXiv:2212.11565 [cs].\nS. Yang, Y. Zhou, Z. Liu, and C. C. Loy. Rerender A Video: Zero-Shot Text-Guided Video-to-Video\nTranslation, June 2023. arXiv:2306.07954 [cs].\nL. Zhang and M. Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models, Feb.\n2023. arXiv:2302.05543 [cs].\n9\nM. Zhao, R. Wang, F. Bao, C. Li, and J. Zhu. ControlVideo: Adding Conditional Control for One\nShot Text-to-Video Editing, May 2023. arXiv:2305.17098 [cs].\nD. Zhou, W. Wang, H. Yan, W. Lv, Y. Zhu, and J. Feng. MagicVideo: Efficient Video Generation\nWith Latent Diffusion Models, May 2023. arXiv:2211.11018 [cs].\nA\nSUPPLEMENTARY\nImplementation of the adaptor that fuses the class-level and pixel-level information of the edited\nreference frame.\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nclass Embedding_Adapter(nn.Module):\n\"\"\"\nFusing the CLIP embeddings and VAE latents of images.\n\"\"\"\ndef __init__(self, ca_emb_size=768):\nsuper(Embedding_Adapter, self).__init__()\nassert ca_emb_size % 2 == 0\nself.clip_downsample_emb = nn.Sequential(\nnn.Conv2d(ca_emb_size, ca_emb_size//2,\nkernel_size=1, stride=1,\npadding=0, bias=True),\nnn.SiLU(),\nnn.Conv2d(ca_emb_size//2, ca_emb_size//2,\nkernel_size=1, stride=1,\npadding=0, bias=True),\nnn.SiLU(),\n)\nself.vae_upsample_chn_down_spatial = nn.Sequential(\nnn.Conv2d(4, ca_emb_size//2,\nkernel_size=3, stride=1,\npadding=1, bias=True),\nnn.SiLU(),\nnn.MaxPool2d(2),\nnn.Conv2d(ca_emb_size//2, ca_emb_size//2,\nkernel_size=1, stride=1,\npadding=0, bias=True),\nnn.SiLU(),\n)\nself.mix_clip_vae = nn.Sequential(\nnn.Conv2d(ca_emb_size, ca_emb_size,\nkernel_size=3, stride=1,\npadding=1, bias=True),\nnn.SiLU(),\nnn.Conv2d(ca_emb_size, ca_emb_size,\nkernel_size=3, stride=1,\npadding=1, bias=True),\n)\ndef forward(self, clip, vae):\nbs_vae, _, h_vae, w_vae = vae.size()\nbs_emb, hw, _ = clip.size()\nh_emb = int(math.sqrt(hw-1))\nassert (bs_vae==bs_emb) and ((hw-1)%h_emb==0)\nclip_cls = clip[:, 0:1, :]\n10\n## adjusting clip_patch embeddings,\nclip_patch = clip[:, 1:, :]\nclip_patch = rearrange(clip_patch, \u2019b (h w) c -> b c h w\u2019,\nh=h_emb, w=h_emb)\nclip_patch = torch.nn.functional.interpolate(clip_patch,\nsize=(h_vae//2, w_vae//2),\nmode=\"bilinear\")\n# (b, h_emb^2, 768) -> (b, 384, h//2, w//2)\nclip_patch = self.clip_downsample_emb(clip_patch)\n## adjusting vae latents\n# (b, 4, h, w) -> (b, 384, h//2, w//2)\nvae = self.vae_upsample_chn_down_spatial(vae)\n## fusing, #(b, 1+hw//4, 768)\nclip_vae = torch.cat([clip_patch, vae], dim=1)\nclip_vae = self.mix_clip_vae(clip_vae)\nclip_vae = rearrange(clip_vae, \u2019b c h w -> b (h w) c\u2019)\nreturn torch.cat([clip_cls, clip_vae], dim=1)\nListing 1: Appearance adaptor\n11\n"
  },
  {
    "title": "Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation",
    "link": "https://arxiv.org/pdf/2309.00987.pdf",
    "upvote": "2",
    "text": "Sequential Dexterity: Chaining Dexterous Policies\nfor Long-Horizon Manipulation\nYuanpei Chen\u2217, Chen Wang\u2217, Li Fei-Fei, C. Karen Liu\nStanford University\nAbstract: Many real-world manipulation tasks consist of a series of subtasks that\nare significantly different from one another. Such long-horizon, complex tasks\nhighlight the potential of dexterous hands, which possess adaptability and versatility,\ncapable of seamlessly transitioning between different modes of functionality\nwithout the need for re-grasping or external tools. However, the challenges arise due\nto the high-dimensional action space of dexterous hand and complex compositional\ndynamics of the long-horizon tasks. We present Sequential Dexterity, a general\nsystem based on reinforcement learning (RL) that chains multiple dexterous policies\nfor achieving long-horizon task goals. The core of the system is a transition fea-\nsibility function that progressively finetunes the sub-policies for enhancing chaining\nsuccess rate, while also enables autonomous policy-switching for recovery from\nfailures and bypassing redundant stages. Despite being trained only in simulation\nwith a few task objects, our system demonstrates generalization capability to novel\nobject shapes and is able to zero-shot transfer to a real-world robot equipped with\na dexterous hand. Code and videos are available at sequential-dexterity.github.io.\nKeywords: Dexterous Manipulation, Long-Horizon Manipulation, Reinforcement\nLearning\n6HDUFK\n2ULHQW\n*UDVS\n,QVHUW\n6HTXHQWLDO\u0003GH[WHURXV\u0003PDQLSXODWLRQ\n,QLWLDO\u0003VWDWH\n*RDO\nFigure 1: We present Sequential Dexterity, a system that learns to chain multiple versatile dexterous\nmanipulation motions for tackling long-horizon tasks (e.g., building a block structure from a pile of\nblocks), which is able to zero-shot transfer to the real world.\n1\nIntroduction\nMany real-world manipulation tasks consist of a sequence of smaller but drastically different subtasks.\nFor example, in the task of Lego structure building (Fig.1), the task involves searching within a box\nto locate a specific block piece. Once found, the piece is then oriented and grasped firmly in hand,\nsetting it up for the final insertion at the goal location. Such a task demands a flexible and versatile\n*Equal contribution. Correspondence to Chen Wang <chenwj@stanford.edu>\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2309.00987v2  [cs.RO]  16 Oct 2023\nmanipulator to adapt and switch between different modes of functionality seamlessly, avoiding\nre-grasping or use of external tools. Furthermore, it requires a long-horizon plan that considers the\ntemporal context and functional relationship between the subtasks in order to successfully execute\nthe entire sequence of tasks. These requirements motivate the use of dexterous hand, which has the\npotential to reach human-level dexterity by utilizing various hand configurations and their inherent\ncapabilities. However, fully utilizing dexterous hands to achieve long-horizon, versatile tasks remains\nan outstanding challenge, calling for innovative solutions.\nRecent developments in dexterous manipulation have made significant strides in areas such as\nobject grasping [1\u20133] and in-hand manipulation [4\u20139]. However, these works primarily investigate\nsingle-stage skills, overlooking the potential of sequencing multiple dexterous policies for long-horizon\ntasks. A naive way to chain multiple dexterous policies together is to simply execute a single-stage skill\none after the other. While the simple strategy works in some scenarios [10\u201312], a subtask in general can\neasily fail when encountering a starting state it has never seen during training. Regularizing the state\nspace between neighboring skills can mitigate this out-of-distribution issue [13, 14], but long-horizon\ndexterous manipulation requires a comprehensive optimization of the entire skill chain, due to the\ncomplex coordination between non-adjacent tasks. For instance, as depicted in Fig. 1, the robot needs\nto strategize in advance when orienting the block, aiming for an optimal object pose that facilitates\nnot only the immediate subsequent grasping but also the insertion task in the later stage of the task.\nThis paper proposes a new method to effectively chain multiple high-dimensional manipulation policies\nvia a combination of regularization and optimization. We introduce a bi-directional process consisting\nof a forward initialization process and a backward fine-tuning process. The forward initialization\nprocess models the end-state distribution of each sub-policy, which defines the initial state distribution\nfor the subsequent policy. The forward initialization process associates the preceding policy with\nthe subsequent one by injecting a bias in the initial state distribution of the subsequent policy during\ntraining. Conversely, we also introduce a backward fine-tuning mechanism to associate the subsequent\npolicy with the preceding one. We define a Transition Feasibility Function which learns to identify\ninitial states from which the subsequent policy can succeed its task. The transition feasibility function\nis used to fine-tune the preceding policy, serving as an auxiliary reward signal. With the backward\nfine-tuning process, the transition feasibility function effectively backpropagates the long-term goals\nto influence the earlier policies via its learning objectives, thereby enabling global optimization across\nthe entire skill chain. Once the policies are trained and deployed, the transition feasibility functions can\nbe repurposed to serve as stage identifiers that determine the appropriate timing for policy switching\nand which subsequent policy to switch to. The transition feasibility function substantially improves\nthe robustness of task execution and increases the success rate. Our experimental results demonstrate\nthat the bi-directional optimization process notably enhances the performance of chaining multiple\ndexterous manipulation policies, which can further zero-shot transfer to a real-world robot arm\nequipped with a dexterous hand to tackle challenging long-horizon dexterous manipulation tasks.\nIn summary, the primary contributions of this work encompass:\n\u2022 The first to explore policy chaining for long-horizon dexterous manipulation.\n\u2022 A general bi-directional optimization framework that effectively chains multiple dexterous\nskills for long-horizon dexterous manipulation.\n\u2022 Our framework exhibits state-of-the-art results in multi-stage dexterous manipulation tasks\nand facilitates zero-shot transfer to a real-world dexterous robot system.\n2\nRelated Work\nDexterous manipulation.\nDexterous manipulation represents a long-standing area of research\nin robotics [15\u201319]. With its high degree of freedom, a dexterous hand can execute a variety of\nmanipulation skills [1\u20137, 9, 20\u201324]. Traditional algorithms have typically addressed these challenges\nby leveraging trajectory optimization founded on analytical dynamics modeling [17\u201319]. These\ntechniques pose simplification over the active contacts between the hand and objects, limiting their\neffectiveness in more complex tasks. Conversely, deep reinforcement learning have exhibited the\ncapacity to learn dexterous skills without assumptions for simplification [7, 8, 23]. Despite their\n2\n\u000bD\f\u0011\u0003%XLOGLQJ\u0003%ORFNV\u0003\u000b6LP\u0003DQG\u00035HDO\f\n\u000bE\f\u0011\u00037RRO\u00033RVLWLRQLQJ\n6WHS\u0003\u0014\u001d\u0003*UDVS\n,QLWLDO\u0003VWDWH\n6WHS\u0003\u0015\u001d\u00032ULHQW\n2XUV\n%DVHOLQH\n:ULVW\u0003FDPHUD\n7RS\u0010GRZQ\u0003FDPHUD\nFigure 2: Overview of the environment setups. (a) Workspace of Building Blocks task in simulation\nand real-world. (b) The setup of the Tool Positioning task. Initially, the tool is placed on the table in a\nrandom pose, and the dexterous hand needs to grasp the tool and re-orient it to a ready-to-use pose. The\ncomparison results illustrate how the way of grasping directly influences subsequent orientation.\nnotable flexibility in learning dexterous primitives, these methods predominantly focus on singular\nmanipulation tasks such as object re-orientation [5, 6, 25\u201327] or isolated skills for reset-free learning\nsystem [28]. Our work prioritizes the chaining of multiple dexterous primitives, which incorporates\nthe skill feasibility into a comprehensive learning framework for long-horizon dexterous manipulation.\nLong-horizon robot manipulation.\nTraining a robot to perform long-horizon manipulation tasks\nfrom scratch is challenging, primarily due to the cumulative propagation of errors throughout the\ntask execution process. Established methods tackle these tasks by breaking them down into simpler,\nreusable subtasks [29]. Typically, these algorithms comprise a set of low-level sub-policies, which\ncan be obtained through various means, such as unsupervised exploration [30\u201334], learning from\ndemonstration [35\u201339] and pre-defined measures [11, 40\u201344]. Despite their distinct merits, these works\ndo not address the specific challenge of long-horizon manipulation in the context of dexterous hands.\nThis challenge largely stems from the compounded complexity produced by the extensive state space of\na hand coupled with the extended scope of long-horizon tasks. Therefore, even when provided with high-\nlevel plans, ensuring a seamless transition between dexterous policies remains a formidable challenge.\nSkill-chaining.\nPrior policy-chaining methods focus on updating each sub-task policy to encompass\nthe terminal states of the preceding policy [10, 13]. However, given the high degree of freedom\ncharacteristic of a hand, the terminal state space undergoes limitless expansion, thereby complicating\neffective training. Closely related to our work is Lee et al. [14], wherein a discriminator is learned\nto regulate the expansion of the terminal state space. Nevertheless, its uni-directional training\nprocess restricts optimization to adjacent skills only, disregarding the influence of long-term goals\non early non-adjacent policies. In contrast, our bi-directional training mechanism enables the\nbackpropagation of the long-term goal reward to optimize the entire policy chain. Our concept\nof backward fine-tuning draws significant inspiration from goal-regression planning in classical\nsymbolic planning literatures [45] (also known as pre-image backchaining [46\u201350]). However, these\nworks assume access to a set of pre-defined motion primitives, which is hard to obtain in dexterous\nmanipulation setups. Our work focuses on the learning and chaining of a sequence of dexterous\npolicies from scratch, targeting the accomplishment of long-horizon task objectives.\n3\nProblem Setups\nWe study the task of chaining a sequence of dexterous policies to accomplish long-horizon manipulation\ntasks, examples of which include Lego-like building blocks or picking up and positioning a tool to\na desired pose. These two tasks both require the use of multiple dexterous skills to complete, making\nthem highly suitable for studying long-horizon dexterous manipulation with skill chaining.\nConstructing a structure of blocks.\nThis long-horizon task includes four different subtasks:\nsearching for a block with desired dimension and color from a pile of cluttered blocks, orienting\nthe block to a favorable position, grasping the block, and finally inserting the block to its designated\nposition on the structure. This sequence of actions repeats until the structure is completed according to\nthe given assembly instructions. The block set, initially arranged in a random configuration, comprises\n3\n\u000bD\f\u0011\u00037UDLQLQJ\u0003LQ\u00036LPXODWLRQ\n\u000bE\f\u0011\u0003'HSOR\\PHQW\u0003LQ\u0003WKH\u00035HDO\u0003:RUOG\n)RUZDUG\u0003\nLQLWLDO\u0011\n%DFNZDUG\u0003\nILQHWXQLQJ\n)RUZDUG\u0003LQLWLDOL]DWLRQ\n%DFNZDUG\u0003ILQHWXQLQJ\n6WDWH\u0003VSDFH\n(QG\u0003VWDWH\nFROOHFWLRQ\n,QLWLDO\u0003VWDWH\u0003\nVDPSOLQJ\n5HZDUG\n6XSHUYLVHG\u0003\nOHDUQLQJ\n3ROLF\\\u0003\nUROORXWV\n,QLWLDO\u0003\nVWDWH\n3ROLF\\\u0003ILQHWXQLQJ\n6XP\u0003\nUHZDUG\n7UDQVLWLRQ\u0003\n)HDVLELOLW\\\u0003)XQFWLRQ\n3ROLF\\\u0003UROORXWV\n3ROLF\\\u0003WUDLQLQJ\n6HDUFK\n2ULHQW\n*UDVS\n,QVHUW\n2ULHQW\n*UDVS\n,QVHUW\n5*%\u0010'\u0003\nFDPHUD\n\u0019'\u0003REM\u0003SRVH\n3URSULRFHSWLRQ\n0RWRU\u0003WDFWLOH\n3ROLF\\\u0003VHOHFWLRQ\u001d\n\u0019\u0013+]\n3'\u0003FRQWURO\n)RUZDUG\u0003\nLQLWLDO\u0011\n%DFNZDUG\u0003\nILQHWXQLQJ\n)RUZDUG\u0003\nLQLWLDO\u0011\n%DFNZDUG\u0003\nILQHWXQLQJ\nFigure 3: Overview of Sequential Dexterity. (a) A bi-directional optimization scheme consists of a\nforward initialization process and a backward fine-tuning mechanism based on the transition feasibility\nfunction. (b) The learned system is able to zero-shot transfer to the real world. The transition feasibility\nfunction serves as a policy-switching identifier to select the most appropriate policy to execute.\neight distinct types (different shapes, masses and colors), totaling 72 blocks. We operate under the\nassumption of having access to an assembly manual that outlines the sequence and desired positioning\nof each block piece on the board. The environment provides the robot with two RGB-D camera\nviews\u2014one from a top-down camera over the box and the other from the wrist-mounted camera (as\nis shown in Fig. 2(a)). No other sensors are used in either simulation or the real world. More details\nof the definition of the sub-task are introduced in Sec. 4.4 and Sec. 5.1.\nTool positioning.\nThis task involves two subtasks: grasping a tool with a long handle (e.g., hammer,\nspatula) from a table and in-hand orienting it to a ready-to-use pose (e.g., make the flat side of the\nhammerhead face the nail, as is shown in Fig. 2(b)). The environment provides the robot with the\n6D pose of the target tool. For more details on the task setups, please refer to Appendix. F.\n4\nSequential Dexterity\nWe propose a bi-directional optimization process to tackle long-horizon dexterous manipulation\ntasks. Our approach contains three main components: (1) Training dexterous sub-policies (Sec. 4.1),\n(2) Chaining sub-policies through fine-tuning (Sec. 4.2), (3) Improving system robustness through\nautomatic policy-switching (Sec. 4.3).\n4.1\nLearning dexterous sub-policies\nTraining a dexterous manipulation policy from scratch for solving long-horizon tasks, like building\na block tower (Fig. 1), is significantly challenging given the high degree of freedom associated with\na dexterous hand (evidenced by the result of RL-scratch in Tab. 1 and Tab. 2). As such, we first\ndecompose a long-horizon task into a K-step sequence of sub-tasks G=(g1,g2,...,gK) and train each\nsub-policy \u03c0i with Proximal Policy Optimization (PPO) [51] algorithm. We formulate each sub-task as\na Markov Decision Process (MDP) M=(SSS,AAA,\u03c0,T ,R,\u03b3,\u03c1), with state space SSS, action space AAA, policy\nof the agent \u03c0, transition distribution T (st+1|st,at) (st \u2208SSS, at \u2208AAA), reward function R, discount\nfactor \u03b3 \u2208(0,1), and initial state distribution \u03c1. The policy \u03c0 outputs a distribution of motor actions\nat based on the current state inputs st. The goal is to train the policy \u03c0 to maximize the sum of rewards\nE\u03c0[PT \u22121\nt=0 \u03b3trt] (rt =R(st,at,st+1)) in an episode with T time steps.\nHowever, due to the large state space of a dexterous hand, it is difficult to accurately sample the\npotential initial states for training individual sub-policies. Take the insertion task as an example (Fig. 3\n4\nsub-policy \u03c04), randomly sampling the initial hand configuration and object\u2019s in-hand pose does not\nassure a physically stable grasp. However, we provide a critical observation that the successful end\nstate of prior sub-task \u03c0i\u22121 inherently provides plausible initial states for \u03c0i to start with. Similar\nobservation is find in [7]. Inspired by this, we propose a forward initialization training scheme (Fig. 3\n(a)). Given a long-horizon task G=(g1,g2,...,gK), our framework sequentially trains the sub-policies\naccording to the task\u2019s chronological order. After training each sub-policy \u03c0i, we start policy rollouts\nand collect a set of successful terminal states {si\nT }, which is later used as the initial state distribution\n\u03c1i+1 for training the succeeding policy \u03c0i+1. Such forward training method ensures the validity\nof the initial states and makes the learning of dexterous policies effective. More details of training\nsub-policies can be found in Sec. 4.4.\n4.2\nPolicy chaining with transition feasibility function\nChaining multiple policies using forward initialization alone may not guarantee success since the\nprevious policy \u03c0i\u22121 might reach a termination state that its successor \u03c0i cannot solve. This issue arises\nbecause the preceding policy \u03c0i\u22121 does not take into account whether the end states are feasible for\nthe subsequent policy \u03c0i to succeed. To address this challenge, it is crucial to convey the feasibility of\nthe following policy \u03c0i in reverse to its predecessor \u03c0i\u22121, enabling the latter to optimize toward states\nthat \u03c0i can handle. Based on this hypothesis, we propose a backward policy fine-tuning mechanism\nwith a transition feasibility function (Fig. 3 (b)).\nLearning transition feasibility function.\nThe feasibility of a given state for a policy can be\ndescribed as the policy\u2019s ability to succeed in the end when starting from that state. We formalize\nthis concept by creating a function that maps the transition state si\n0 \u2208\u03c1i (which is equivalent to si\u22121\nT\n)\nto the expected sum of reward within the sub-task execution, E\u03c0i[PT \u22121\nt=0 rt]. We name this function,\nF : S 7\u2192 R, the Transition Feasibility Function. However, a single state si\u22121\nT\nis not sufficient to\ndifferentiate the performance of \u03c0i. In particular, the velocity of object from the previous sub-task may\nbe critical to the performance of \u03c0i, which cannot be captured by si\u22121\nT\nalone. As a result, the transition\nfeasibility function takes a sequence of observation states si\u22121\n[T \u221210:T ] (10 steps in our experiments) as\ninput and employs a multi-head attention network [52] to extract suitable temporal information for\nlearning the F. The final learning objective of the transition feasibility function F i for sub-policy \u03c0i is:\nLi =\u2225F i(s[T \u221210:T ])\u2212E\u03c0i[\nT \u22121\nX\nt=0\nrt]\u22252\n(1)\nBackward policy fine-tuning.\nOnce F i is trained, we can fine-tune the prior policy \u03c0i\u22121, by\nincorporating F i as an auxiliary reward component. The fine-tuning starts with updating the second-to-\nthe-lastsub-taskpolicy\u03c0K\u22121 andsequentiallymovesbackward, refiningeachprecedingpolicyuntilthe\nfirst one \u03c01 is updated. In each fine-tuning step, we utilize F i as an additional reward, combined with the\noriginal sub-task reward Ri\u22121, to fine-tune policy \u03c0i\u22121. The final policy fine-tuning reward function is:\nRi\u22121\u2032(st,at,st+1;F i)=\u03bb1Ri\u22121(st,at,st+1)+\u03bb2F i(s[T \u221210:T ]),\n(2)\nwhere \u03bb1 and \u03bb2 are the weighting factors. Once \u03c0i\u22121 has been refined, we execute policy rollouts\nto gather data, which maps from the initial state si\u22122\n[T \u221210:T ] to the accumulated reward E\u03c0i\u22121[PT \u22121\nt=0 rt]\nreceived by the policy \u03c0i\u22121 at the terminal state si\u22121\nT\n. This data helps to construct a new transition feasi-\nbility function F i\u22121, which is further used to fine-tune the preceding policy \u03c0i\u22122. The implementation\npseudocode of the bi-directional forward and backward training process is illustrated in Appendix. A.\n4.3\nPolicy switching with transition feasibility function\nA key challenge in chaining multiple dexterous policies is to determine when to switch to the next\npolicy and what should be the next policy to execute. Prior works approach this issue by establishing\na predetermined execution horizon for a policy, transitioning abruptly to the subsequent policy once\nthe maximum step count is attained [10, 14, 53\u201355]. The pre-scheduled policy transitions worked\nin some scenarios, but they are not suitable for dexterous manipulation that involves non-prehensile\n5\nFigure 4: Examples of policy-switching with transition feasibility function. Each example contains an\nimage from the wrist-mount camera (left) and its corresponding feasibility score ci outputted by the\ntransition feasibility function (right). We highlight the target block in the image for better visualization.\nThe policy-switching process visits each sub-policy in reverse order. The first sub-policy with a\nfeasibility score ci >1.0 is selected for execution.\nmaneuvers and in-hand manipulation. For instance, if a robot is reorienting an object in-hand, a\npremature policy switch before the object is stabilized could result in task failure. The key to tackling\nthis issue is to automatically figure out the appropriate switch time such that the transition state will\nlead to success of next policy. The transition feasibility function provides exactly the information we\nneed for identifying the switch timing. As such, during execution, we repurpose our trained transition\nfeasibility functions as a policy-switching identifier. At each time step, the transition feasibility\nfunction of the next sub-policy will output a feasibility score ci+1\nt\n=F i+1(s[t\u221210:t])/hi+1, where hi+1\nis a threshold hyperparameter defined based on the reward of successful task executions. The ideal\ntime of policy switching can then be defined as the moment when ci+1\nt\n>1.\nSimply executing sub-policies sequentially may not guarantee successful task execution since the robot\nsometimes needs to recover using a previous policy and sometimes needs to bypass a future policy if\nthe sub-task has already been achieved. Thus, effective policy switching requires the robot to not only\nconsider the current policy and its successor, but also the entire skill chain. To achieve this, we group\nthe learned transition feasibility function (F 2,F 3...,F K) as a stage estimator. At each policy-switching\nstep, we calculate the feasibility score from the final transition feasibility function of the entire task\ncK\nt =F K(s[t\u221210:t])/hK, sequentially moving backward. The sub-policy, for which the first feasibility\nscore ci\nt >1, is considered the next policy for execution. If none of the feasibility scores satisfy, the\nrobot will restart from the beginning of the entire task. Leveraging the learned transition feasibility\nfunction in this manner enhances the robot\u2019s robustness against unexpected failures during policy\nexecution, while also allowing it to bypass redundant stages, thus promoting efficient task execution.\n4.4\nImplementation details\nRL reward.\nTraining sub-policies require pre-defined sub-task rewards {Ri}K\ni=1. Establishing such\nrewards can be complex as the appropriate sub-goals that would most contribute to the overall task\naccomplishment may not be readily apparent. However, we pose a critical finding that the backward\nfine-tuning mechanism can transmit the goal of the entire task to each sub-task. For instance, the\ntransition feasibility function of the inserting policy informs the grasping policy about the in-hand\nobject pose that would be most beneficial for the insertion. Furthermore, such backward transmission\ncan influence all preceding sub-policies, enabling the entire policy chain to optimize for the overall\ntask goal. This mechanism alleviates the burden of reward shaping and allows us to use standard\nsub-task rewards that are agnostic to the final task goal for training sub-policies. For instance, the\nsub-task reward of grasping is defined as whether the target object has been lifted. The specifics of\nhow the object is held in hand are automatically managed during the backward fine-tuning process.\nThe detailed descriptions of each sub-task reward are documented in the Appendix. D.\nState-action space.\nThe state space for the sub-policies is built around the perspective of the hand.\nIt integrates proprioception and motor tactile [56, 57] information from the 16-degree-of-freedom\nAllegro Hand as well as the target object\u2019s 6D pose in the reference frame of the wrist-mounted camera.\nDuring simulation-based sub-policy training, we augment this state space with additional information,\n6\nTrained\nUnseen\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nALL\nRL-scratch\n0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00\nCurriculum RL\n0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00 0.00\u00b10.00\nV-Chain [34]\n0.15\u00b10.02 0.09\u00b10.04 0.11\u00b10.04 0.10\u00b10.04 0.08\u00b10.02 0.08\u00b10.03 0.03\u00b10.02 0.04\u00b10.02 0.08\u00b10.02\nPolicy-Seq [10]\n0.20\u00b10.04 0.14\u00b10.02 0.15\u00b10.03 0.23\u00b10.03 0.15\u00b10.03 0.17\u00b10.00 0.16\u00b10.01 0.12\u00b10.02 0.16\u00b10.02\nT-STAR [14]\n0.19\u00b10.04 0.18\u00b10.02 0.11\u00b10.01 0.27\u00b10.02 0.17\u00b10.04 0.25\u00b10.02 0.26\u00b10.03 0.10\u00b10.03 0.19\u00b10.03\nOurs w/o temporal 0.47\u00b10.06 0.44\u00b10.07 0.43\u00b10.00 0.49\u00b10.04 0.40\u00b10.04 0.51\u00b10.04 0.18\u00b10.01 0.16\u00b10.03 0.38\u00b10.04\nOurs\n0.61\u00b10.03 0.55\u00b10.01 0.52\u00b10.03 0.63\u00b10.03 0.51\u00b10.06 0.53\u00b10.06 0.22\u00b10.02 0.16\u00b10.01 0.46\u00b10.03\nTable 1: Results for the Building Blocks task\nsuch as the velocity of each hand joint and the target object. In real-world deployments, these states\nare abstracted via policy distillation [6, 7, 58]. The action space of our system includes 16-dimensional\nhand joints and 3D wrist translation of the robot arm. To concentrate the sub-policy learning on critical\naspects of the manipulation task, when the target\u2019s (either object or goal location) relative pose to the\nwrist camera is detected more than 5 centimeters from the hand, we employ a motion-planning-based\noperational space controller (OSC [59]) to move the end-effector to a position 5 centimeters above\nthe target. More details of the state-action space are introduced in Appendix. C.\n5\nExperiments\n5.1\nExperiment setups\nT-STAR\nOurs\n(a). Effect of policy-switching\n(b). Object poses with high feasibility score for Grasp\nFigure 5: (a) Performance improvement of Ours\ngiven 0/1/2/3 maximum policy-switching times.\n(b) Visualization of object poses with high feasi-\nbility score for the Grasp sub-policy in Building\nBlocks task. The x, y, and z axes are the roll, yaw,\nand pitch of the object, respectively. In Ours, each\npoint in the diagram represents a pose that is re-\ngarded as feasible by the transition feasibility func-\ntion (ci >1.0). For T-STAR, we use the poses that\nare judged as successful by its discriminator.\nEnvironment setups.\nThe environment is ini-\ntialized with a Franka Emika robot arm equipped\nwith an Allegro Hand as the end-effector. In the\nBuilding Block task, we placed a box of blocks\n(eight categories, in total 72 pieces) and a building\nboard on the table. For the tool positioning task,\na long-handled tool is placed on the table. The\ncontrol frequency is 60 Hz for both the robot arm\nand the hand. The real-world hardware mirrors\nthe simulation setups. We coordinate the use of\ntop-down and wrist-mount cameras to access the\n6D pose and segmentation mask of the object in\nthe real-world: we use the top-down camera once\nat the beginning, use the wrist-mount camera once\nat the beginning of each maneuver in orienting,\nand use the wrist-mount camera continuously in\nsearching, grasping and inserting; More details\nof the environment setups are in Appendix. F.\nBaseline methods.\nWe compare our approach with the following baselines: 1) RL-scratch is vanilla\nPPO algorithm [51] learns the task from scratch. 2) Curriculum RL follows a procedure training\nstrategy to expand from the first skill to the entire task. 3) V-Chain [34] combines skill-chaining with\nthe value function from the PPO policy. 4) Policy-Seq [10] focuses on the forward initiation process in\nskill-chaining. 5) T-STAR [14] incorporates a discriminator to regularize the terminal states.\n5.2\nResults\nBi-directional optimization framework is key for chaining multiple dexterous policies.\nIn Tab. 1\nand Tab. 2, our approach learned with bi-directional optimization (Ours and Ours w/o temporal) outper-\nforms prior uni-directional skill-chaining methods (V-Chain, Policy-Seq and T-STAR) significantly,\nwith more than 20% improvement in task success rate in two long-horizon tasks. We further analyze\nwhat really matters for successful policy chaining. We visualize the transition feasibility score of the\ngrasping sub-policy (T-STAR\u2019s result is calculated from its discriminator) in Fig. 5(b). We found our\napproach with backward fine-tuning scheme correctly transits the goal of the succeeding inserting skill\nto prior grasping and encourages the policy to grasp the block when its studs face up, which facilitates\n7\nTrained\nUnseen\nHammer\nSpatula\nSpoon\nALL\nRL-scratch\n0.17\u00b10.05 0.06\u00b10.03 0.10\u00b10.01 0.11\u00b10.03\nCurriculum RL 0.29\u00b10.02 0.17\u00b10.01 0.16\u00b10.08 0.21\u00b10.04\nPolicy-Seq [10] 0.43\u00b10.01 0.29\u00b10.06 0.24\u00b10.04 0.32\u00b10.02\nT-STAR [14]\n0.47\u00b10.01 0.40\u00b10.03 0.26\u00b10.04 0.37\u00b10.03\nOurs w/o temp. 0.77\u00b10.03 0.54\u00b10.07 0.40\u00b10.04 0.57\u00b10.05\nOurs\n0.81\u00b10.01 0.57\u00b10.04 0.43\u00b10.08 0.60\u00b10.04\nTable 2: Results for the tool positioning task\nSingle\nBlock 1\nSingle\nBlock 4\nDouble\nBlock 1\nRL-scratch\n0/10\n0/10\n0/10\nPolicy-Seq [34]\n0/10\n2/10\n0/10\nT-STAR [14]\n3/15\n5/18\n0/13\nOurs\n12/20\n10/20\n5/15\nTable 3: Real world results in the Building\nBlocks task. Single/Double refers to building\none single block or stacking two blocks.\nthe insertion. T-STAR with the uni-directional learning process, however, suggests many states where\nthe block\u2019s studs are facing horizontally, thereby bringing challenges for subsequent insertion.\nTransition feasibility function significantly improves performance of long-horizon dexterous\nmanipulation.\nIn Tab. 1 and Tab. 2, the models learned with the transition feasibility function (Ours\nand Ours w/o temporal) outperforms the one using the PPO-trained value function (V-chain) for more\nthan 30% in task success rate. This result implies that the value function of PPO policy fails to model\nthe feasibility of subsequent policy, which further affects policy chaining results.\nTemporal inputs facilitate handling high-dimensional state spaces, particularly for dexterous\nmanipulation.\nIn Tab. 1, by training the transition feasibility function to extract temporal information\nfrom a sequence of history states, Ours exceeds Ours w/o temporal for 8% in task success rate. This\nresult highlights the importance of extracting velocity and temporal information for chaining dexterous\npolicies that contain dynamic finger motions.\nAbility to switch sub-policy autonomously is essential for succeeding long-horizon tasks.\nFig.\n5(a) illustrates the performance improvement of enabling automatic policy-switching. Only with a\nmaximum allowance of three switching times, Ours can improve more than 30% in task success rate.\nThis result concludes that it is crucial to have the capability of switching forward and backward by\nleveraging the transition feasibility function of each sub-policy. Such policy-switching ability further\ncontributes to our real-world results in Tab. 3, which allows the policy to handle a challenging 8-step\nlong-horizon task (Double Block 1) with more than 30% task success rate (10 maximum switching\ntimes for all methods in the real-world experiments). Please refer to the website for more results.\nReal-world results.\nIn Tab. 3 real-world experiments, our approach has more than 30% success\nrate improvements compared to prior methods. This result is consistent with the results in simulation.\nOurs has a 33% success rate in building a double-block structure, while the other baselines have a 0%\nsuccess rate. This result highlights the ability of our model when tackling long-horizon dexterous\nmanipulation tasks. For more details of the real-world setups, please refer to Appendix. B.\n6\nLimitations\nThere are several limitations of our work. First, we encounter difficulties in simulating a contact-rich\ninsertion process which necessitates an additional manually designed pressing motion to completely\ninsert the blocks during real-world deployment. Second, the motor tactile does not yield a significant\nimprovement in performance, as observed in Appendix Tab. 8. Our future research could explore the\npotential of sensor-based tactile signals for contact-rich tasks, as proposed in [60, 61, 20, 24, 27].\n7\nConclusion\nWe present Sequential Dexterity, a system developed for tackling long-horizon dexterous manipulation\ntasks. Our system leverages a bi-directional optimization process to chain multiple dexterous policies\nlearned with deep reinforcement learning. At the core of our system is the Transition Feasibility\nFunction, a pivotal component facilitating a gradual fine-tuning of sub-policies and enabling dynamic\npolicy switching, thereby significantly increasing the success rate of policy chaining. Our system has\nthe capability to zero-shot transfer to a real-world dexterous robot, exhibiting generalization across\nnovel object shapes. Our bi-directional optimization framework also has the potential to be a general\nskill chaining method beyond dexterous manipulation. Potential applications including chaining skills\nfor bimanual robots.\n8\nAcknowledgments\nThis research was supported by National Science Foundation NSF-FRR-2153854, NSF-NRI-2024247,\nNSF-CCRI-2120095 and Stanford Institute for Human-Centered Artificial Intelligence, SUHAI. In\nthe real-world experiment, the controller of the Franka Emika Panda arm is developed based on\nDeoxys [62] and the Allegro hand is controlled through zmq [2]. We would also like to thank Ruocheng\nWang, Wenlong Huang, Yunzhi Zhang and Albert Wu for providing feedback on the paper.\nReferences\n[1] Z. Q. Chen, K. Van Wyk, Y.-W. Chao, W. Yang, A. Mousavian, A. Gupta, and D. Fox. Learning\nrobust real-world dexterous grasping policies via implicit shape augmentation. arXiv preprint\narXiv:2210.13638, 2022.\n[2] A. Wu, M. Guo, and C. K. Liu. Learning diverse and physically feasible dexterous grasps with\ngenerative model and bilevel optimization. arXiv preprint arXiv:2207.00195, 2022.\n[3] Y. Qin, B. Huang, Z.-H. Yin, H. Su, and X. Wang. Dexpoint: Generalizable point cloud\nreinforcement learning for sim-to-real dexterous manipulation. In Conference on Robot Learning,\npages 594\u2013605. PMLR, 2023.\n[4] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron,\nM. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The International\nJournal of Robotics Research, 39(1):3\u201320, 2020.\n[5] A. Handa, A. Allshire, V. Makoviychuk, A. Petrenko, R. Singh, J. Liu, D. Makoviichuk,\nK. Van Wyk, A. Zhurkevich, B. Sundaralingam, et al. Dextreme: Transfer of agile in-hand\nmanipulation from simulation to reality. arXiv preprint arXiv:2210.13702, 2022.\n[6] T. Chen, M. Tippur, S. Wu, V. Kumar, E. Adelson, and P. Agrawal. Visual dexterity: In-hand\ndexterous manipulation from depth. arXiv preprint arXiv:2211.11744, 2022.\n[7] T. Chen, J. Xu, and P. Agrawal. A system for general in-hand object re-orientation. Conference\non Robot Learning, 2021.\n[8] OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino,\nM. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan,\nW. Zaremba, and L. Zhang. Solving rubik\u2019s cube with a robot hand. CoRR, abs/1910.07113,\n2019.\n[9] S. P. Arunachalam, S. Silwal, B. Evans, and L. Pinto. Dexterous imitation made easy: A learning-\nbased framework for efficient dexterous manipulation. arXiv preprint arXiv:2203.13251, 2022.\n[10] A. Clegg, W. Yu, J. Tan, C. K. Liu, and G. Turk. Learning to dress: Synthesizing human dressing\nmotion via deep reinforcement learning. ACM Transactions on Graphics (TOG), 37(6):1\u201310,\n2018.\n[11] Y. Lee, S.-H. Sun, S. Somasundaram, E. S. Hu, and J. J. Lim. Composing complex skills by\nlearning transition policies. In International Conference on Learning Representations, 2019.\n[12] X. B. Peng, M. Chang, G. Zhang, P. Abbeel, and S. Levine. Mcp: Learning composable\nhierarchical control with multiplicative compositional policies. Advances in Neural Information\nProcessing Systems, 32, 2019.\n[13] G. Konidaris and A. Barto. Skill discovery in continuous reinforcement learning domains using\nskill chaining. Advances in neural information processing systems, 22, 2009.\n[14] Y. Lee, J. J. Lim, A. Anandkumar, and Y. Zhu. Adversarial skill chaining for long-horizon robot\nmanipulation via terminal state regularization. arXiv preprint arXiv:2111.07999, 2021.\n9\n[15] J. K. Salisbury and J. J. Craig. Articulated hands: Force control and kinematic issues. The\nInternational journal of Robotics research, 1(1):4\u201317, 1982.\n[16] M. T. Mason and J. K. Salisbury Jr. Robot hands and the mechanics of manipulation. 1985.\n[17] I. Mordatch, Z. Popovi\u00b4c, and E. Todorov. Contact-invariant optimization for hand manipulation.\nIn Proceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation, pages\n137\u2013144, 2012.\n[18] Y. Bai and C. K. Liu. Dexterous manipulation using both palm and fingers. In 2014 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 1560\u20131565. IEEE, 2014.\n[19] V. Kumar, Y. Tassa, T. Erez, and E. Todorov. Real-time behaviour synthesis for dynamic hand-\nmanipulation. In 2014 IEEE International Conference on Robotics and Automation (ICRA),\npages 6808\u20136815. IEEE, 2014.\n[20] Z.-H. Yin, B. Huang, Y. Qin, Q. Chen, and X. Wang. Rotating without seeing: Towards in-hand\ndexterity through touch. arXiv preprint arXiv:2303.10880, 2023.\n[21] A. Sivakumar, K. Shaw, and D. Pathak. Robotic telekinesis: Learning a robotic hand imitator by\nwatching humans on youtube. arXiv preprint arXiv:2202.10448, 2022.\n[22] K. Zakka, L. Smith, N. Gileadi, T. Howell, X. B. Peng, S. Singh, Y. Tassa, P. Florence, A. Zeng,\nand P. Abbeel. Robopianist: A benchmark for high-dimensional robot control. arXiv preprint\narXiv:2304.04150, 2023.\n[23] Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer, H. Dong, S.-C. Zhu, and Y. Yang.\nTowards human-level bimanual dexterous manipulation with reinforcement learning. Advances\nin Neural Information Processing Systems, 35:5150\u20135163, 2022.\n[24] I. Guzey, B. Evans, S. Chintala, and L. Pinto. Dexterity from touch: Self-supervised pre-training\nof tactile representations with robotic play. arXiv preprint arXiv:2303.12076, 2023.\n[25] H. Qi, A. Kumar, R. Calandra, Y. Ma, and J. Malik. In-Hand Object Rotation via Rapid Motor\nAdaptation. In Conference on Robot Learning (CoRL), 2022.\n[26] W. Huang, I. Mordatch, P. Abbeel, and D. Pathak. Generalization in dexterous manipulation via\ngeometry-aware multi-task learning. arXiv preprint arXiv:2111.03062, 2021.\n[27] G. Khandate, S. Shang, E. T. Chang, T. L. Saidi, J. Adams, and M. Ciocarlie. Sampling-based\nExploration for Reinforcement Learning of Dexterous Manipulation. In Proceedings of Robotics:\nScience and Systems, Daegu, Republic of Korea, July 2023. doi:10.15607/RSS.2023.XIX.020.\n[28] A. Gupta, J. Yu, T. Z. Zhao, V. Kumar, A. Rovinsky, K. Xu, T. Devlin, and S. Levine. Reset-free\nreinforcement learning via multi-task learning: Learning dexterous manipulation behaviors\nwithout human intervention. In ICRA, pages 6664\u20136671. IEEE, 2021.\n[29] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal\nabstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013211, 1999.\n[30] J. Schmidhuber. Towards compositional learning with dynamic neural networks. Inst. f\u00a8ur\nInformatik, 1990.\n[31] P.-L. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the AAAI\nconference on artificial intelligence, volume 31, 2017.\n[32] O. Nachum, S. S. Gu, H. Lee, and S. Levine. Data-efficient hierarchical reinforcement learning.\nAdvances in neural information processing systems, 31, 2018.\n[33] A. Levy, G. Konidaris, R. Platt, and K. Saenko. Learning multi-level hierarchies with hindsight.\narXiv preprint arXiv:1712.00948, 2017.\n10\n[34] V. C. Kumar, S. Ha, and C. K. Liu. Expanding motor skills using relay networks. In Conference\non Robot Learning, pages 744\u2013756. PMLR, 2018.\n[35] G. Konidaris, S. Kuindersma, R. Grupen, and A. Barto. Robot learning from demonstration by\nconstructing skill trees. The International Journal of Robotics Research, 31(3):360\u2013375, 2012.\n[36] T. Kipf, Y. Li, H. Dai, V. Zambaldi, A. Sanchez-Gonzalez, E. Grefenstette, P. Kohli, and\nP. Battaglia. Compile: Compositional imitation learning and execution. In International\nConference on Machine Learning, pages 3418\u20133428. PMLR, 2019.\n[37] Y. Lu, Y. Shen, S. Zhou, A. Courville, J. B. Tenenbaum, and C. Gan. Learning task decomposition\nwith ordered memory policy network. arXiv preprint arXiv:2103.10972, 2021.\n[38] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances.\narXiv preprint arXiv:2204.01691, 2022.\n[39] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu, and A. Anandkumar. Mimicplay:\nLong-horizon imitation learning by watching human play. arXiv preprint arXiv:2302.12422,\n2023.\n[40] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforce-\nment learning: Integrating temporal abstraction and intrinsic motivation. Advances in neural\ninformation processing systems, 29, 2016.\n[41] J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep\nreinforcement learning. In International Conference on Machine Learning, pages 2661\u20132670.\nPMLR, 2017.\n[42] J. Merel, A. Ahuja, V. Pham, S. Tunyasuvunakool, S. Liu, D. Tirumala, N. Heess, and G. Wayne.\nHierarchical visuomotor control of humanoids. arXiv preprint arXiv:1811.09656, 2018.\n[43] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and\nT. Funkhouser. Tidybot: Personalized robot assistance with large language models. arXiv\npreprint arXiv:2305.05658, 2023.\n[44] C. Wang, D. Xu, and L. Fei-Fei. Generalizable task planning through representation pretraining.\nIEEE Robotics and Automation Letters, 7(3):8299\u20138306, 2022.\n[45] R. Waldinger. Achieving several goals simultaneously. In Readings in artificial intelligence,\npages 250\u2013271. Elsevier, 1981.\n[46] T. Lozano-Perez, M. T. Mason, and R. H. Taylor. Automatic synthesis of fine-motion strategies\nfor robots. The International Journal of Robotics Research, 3(1):3\u201324, 1984.\n[47] L. P. Kaelbling and T. Lozano-P\u00b4erez. Hierarchical task and motion planning in the now. In 2011\nIEEE International Conference on Robotics and Automation, pages 1470\u20131477. IEEE, 2011.\n[48] L. P. Kaelbling and T. Lozano-P\u00b4erez. Pre-image backchaining in belief space for mobile manipu-\nlation. In Robotics Research: The 15th International Symposium ISRR, pages 383\u2013400. Springer,\n2017.\n[49] D. Xu, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, D.-A. Huang, Y. Zhu, S. Savarese, and L. F. Fei-Fei. Regression\nplanning networks. Advances in Neural Information Processing Systems, 32, 2019.\n[50] C. Agia, T. Migimatsu, J. Wu, and J. Bohg. Stap: Sequencing task-agnostic policies. arXiv\npreprint arXiv:2210.12250, 2022.\n[51] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. CoRR, abs/1707.06347, 2017.\n11\n[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In NIPS, pages 5998\u20136008, 2017.\n[53] E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, and W. Burgard. Latent plans for task-\nagnostic offline reinforcement learning. In Conference on Robot Learning, pages 1838\u20131849.\nPMLR, 2023.\n[54] Z. Su, O. Kroemer, G. E. Loeb, G. S. Sukhatme, and S. Schaal. Learning to switch between\nsensorimotor primitives using multimodal haptic signals. In SAB, volume 9825 of Lecture Notes\nin Computer Science, pages 170\u2013182. Springer, 2016.\n[55] O. Kroemer, C. Daniel, G. Neumann, H. van Hoof, and J. Peters. Towards learning hierarchical\nskills for multi-phase manipulation tasks. In ICRA, pages 1503\u20131510. IEEE, 2015.\n[56] L. Sievers, J. Pitz, and B. B\u00a8auml. Learning purely tactile in-hand manipulation with a torque-\ncontrolled hand. In 2022 International Conference on Robotics and Automation (ICRA), pages\n2745\u20132751. IEEE, 2022.\n[57] J. Pitz, L. R\u00a8ostel, L. Sievers, and B. B\u00a8auml. Dextrous tactile in-hand manipulation using a modular\nreinforcement learning architecture. arXiv preprint arXiv:2303.04705, 2023.\n[58] A. A. Rusu, S. G. Colmenarejo, C\u00b8 . G\u00a8ulc\u00b8ehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih,\nK. Kavukcuoglu, and R. Hadsell. Policy distillation. In ICLR (Poster), 2016.\n[59] O. Khatib. A unified approach for motion and force control of robot manipulators: The operational\nspace formulation. IEEE Journal on Robotics and Automation, 3(1):43\u201353, 1987. doi:10.1109/\nJRA.1987.1087068.\n[60] B. Romero, F. Veiga, and E. Adelson. Soft, round, high resolution tactile fingertip sensors\nfor dexterous robotic manipulation. In 2020 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 4796\u20134802. IEEE, 2020.\n[61] W. K. Do and M. Kennedy. Densetact: Optical tactile sensor for dense shape reconstruction. In\n2022 International Conference on Robotics and Automation (ICRA), pages 6188\u20136194. IEEE,\n2022.\n[62] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu. Viola: Imitation learning for vision-based manipulation\nwith object proposal priors. 6th Annual Conference on Robot Learning, 2022.\n[63] H. K. Cheng and A. G. Schwing. XMem: Long-term video object segmentation with an atkinson-\nshiffrin memory model. In ECCV, 2022.\n[64] C. Wang, D. Xu, Y. Zhu, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, C. Lu, L. Fei-Fei, and S. Savarese. Densefusion: 6d\nobject pose estimation by iterative dense fusion. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 3343\u20133352, 2019.\n[65] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A\nbenchmark and evaluation for multi-task and meta reinforcement learning. In Conference on\nrobot learning, pages 1094\u20131100. PMLR, 2020.\n[66] H. J. Charlesworth and G. Montana. Solving challenging dexterous manipulation tasks with\ntrajectory optimisation and reinforcement learning. In International Conference on Machine\nLearning, pages 1496\u20131506. PMLR, 2021.\n12\nA\nTraining pseudocode\nAlgorithm 1 SEQUENTIAL DEXTERITY: A bi-directional optimization framework for skill chaining\nRequire: sub-task MDPs M1,...,MK\n1: Initialize sub-policies \u03c01\n\u03b8,...,\u03c0K\n\u03b8 , transition feasibility function F 1\n\u03c9,...,F K\n\u03c9 , ternimal state buffers\nB1\nI,...,BK\nI , the sum of reward buffers B1\nR,...,BK\nR, and the weighting factors of the backward\nfine-tuning \u03bb1 and \u03bb2.\n2: for iteration m=0,1,...,M do\n3:\nfor each sub-task i=1,...,K do\n4:\nwhile until convergence of \u03c0i\n\u03b8 do\n5:\nRollout trajectories \u03c4 =(si\n0,ai\n0,ri\n0,...,si\nT ) with \u03c0i\n\u03b8\n6:\nUpdate \u03c0i\n\u03b8 by maximizing E\u03c0i[PT \u22121\nt=0 \u03b3tri\nt]\n7:\nend while\n8:\nend for\n\u25b7 Forward initialization\n9:\nfor each sub-task i=K,...,2 do\n10:\nwhile until convergence of \u03c0i\u22121\n\u03b8\ndo\n11:\nRollout trajectories \u03c4 i\u22121 =(si\u22121\n0\n,ai\u22121\n0\n,ri\u22121\n0\n,...,si\u22121\nT\n) with \u03c0i\u22121\n\u03b8\n12:\nSample si\n0 from environment or Bi\u22121\n\u03b2\n13:\nRollout trajectories \u03c4 i =(si\n0,ai\n0,ri\n0,...,si\nT ) with \u03c0i\n\u03b8\n14:\nif sub-task i is complete then\n15:\nBi\nT \u2190Bi\nT \u222as[T \u221210:T ],Bi\nR \u2190Bi\nR\u222a[PT \u22121\nt=0 ri\nt]\n16:\nend if\n17:\nUpdate F i with s[T \u221210:T ] \u223cBi\u22121\nT\nand [PT \u22121\nt=0 rt]\u223cBi\nR\n18:\nUpdate\u03c0i\u22121\n\u03b8\nbymaximizingE\u03c0i\u22121[PT \u22121\nt=0 \u03b3t(\u03bb1Ri\u22121(si\u22121\nt\n,ai\u22121\nt\n,si\u22121\nt+1)+\u03bb2F i\n\u03c9(si\n[T \u221210:T ]))]\n19:\nend while\n20:\nend for\n\u25b7 Backward finetuning\n21: end for\nB\nReal-world system setups\nDuring real-world deployment, some observations used in the simulation are hard to accurately\nestimate (e.g., joint velocity, object velocity, etc.). We use the teacher-student policy distillation\nframework [6, 7, 58] to abstract away these observation inputs from the policy model. In each policy\nrollout, our system first uses the top-down camera view to perform a color-based segmentation to\nlocalize the target block piece given by the manual. Then, the robot calls motion planning API to\nmove to the target location with OSC controller [59]. After that, our system uses the wrist camera\nview to track the segmentation and 6D pose of the object with a combination of color-based initial\nsegmentation, Xmem segmentation tracker [63], and Densefusion pose estimator [64]. If the target\nobject is deeply buried (as the case in the top left corner of Fig. 4), the transition feasibility function\nwill inform the robot to execute the searching policy until the target appears. During the last insertion\nstage, the estimated 6D object pose will guide the robot policy to adjust its finger and wrist motion\nto align with the goal location as it learned in the simulation. Since simulating contact-rich insertion\nis still a research challenge in graphics, after the robot has placed the block to the target location, we\nperform a scripted pressing motion (spread out the entire hand and press down) on the target location\nto ensure a firm insert. The output of the policy which controls the hand is low-pass filtered with an\nexponential moving average (EMA) smoothing factor [6], which can also effectively reduce jittering\nmotions. Our results in the real-world were obtained with an EMA of 0.2, which provides a balance\nbetween agility and stability of the motions. More details about real-world system setups and results\ncan be found in the Supplementary video.\n13\nC\nState Space in Simulation\nC.1\nBuilding Blocks\nSearching\nTable.4 gives the specific information of the state space of the searching task.\nTable 4: Observation space of Search task.\nIndex\nDescription\n0 - 23\ndof position\n23 - 46\ndof velocity\n46 - 98\nfingertip pose, linear velocity, angle velocity (4 x 13)\n98 - 111\nhand base pose, linear velocity, angle velocity\n111 - 124\nobject base pose, linear velocity, angle velocity\n124 - 143\nthe actions of the last timestep\n143 - 159\nmotor tactile\n159 - 160\nthe number of pixels occupied by the target object mask\nOrienting\nTable.5 gives the specific information of the state space of the orienting task.\nTable 5: Observation space of Orient and Grasp task.\nIndex\nDescription\n0 - 23\ndof position\n23 - 46\ndof velocity\n46 - 98\nfingertip pose, linear velocity, angle velocity (4 x 13)\n98 - 111\nhand base pose, linear velocity, angle velocity\n111 - 124\nobject base pose, linear velocity, angle velocity\n124 - 143\nthe actions of the last timestep\n143 - 159\nmotor tactile\nGrasping\nTable.5 gives the specific information of the state space of the grasping task.\nInserting\nTable.6 gives the specific information of the state space of the inserting task.\nTable 6: Observation space of Insert task.\nIndex\nDescription\n0 - 23\ndof position\n23 - 46\ndof velocity\n46 - 98\nfingertip pose, linear velocity, angle velocity (4 x 13)\n98 - 111\nhand base pose, linear velocity, angle velocity\n111 - 124\nobject base pose, linear velocity, angle velocity\n124 - 143\nthe actions of the last timestep\n143 - 159\nmotor tactile\n159 - 166\ngoal pose\n166 - 169\ngoal position - object position\n169 - 173\ngoal rotation - object rotation\nC.2\nTool positioning\nGrasping\nTable.5 gives the specific information of the state space of the grasping task.\nIn-hand Orientation\nTable.6 gives the specific information of the state space of the in-hand orienta-\ntion task.\n14\nTable 7: Domain randomization of all the sub-tasks.\nParameter\nType\nDistribution\nInitial Range\nRobot\nMass\nScaling\nuniform\n[0.5, 1.5]\nFriction\nScaling\nuniform\n[0.7, 1.3]\nJoint Lower Limit\nScaling\nloguniform\n[0.0, 0.01]\nJoint Upper Limit\nScaling\nloguniform\n[0.0, 0.01]\nJoint Stiffness\nScaling\nloguniform\n[0.0, 0.01]\nJoint Damping\nScaling\nloguniform\n[0.0, 0.01]\nObject\nMass\nScaling\nuniform\n[0.5, 1.5]\nFriction\nScaling\nuniform\n[0.5, 1.5]\nScale\nScaling\nuniform\n[0.95, 1.05]\nPosition Noise\nAdditive\ngaussian\n[0.0, 0.02]\nRotation Noise\nAdditive\ngaussian\n[0.0, 0.2]\nObservation\nObs Correlated. Noise\nAdditive\ngaussian\n[0.0, 0.001]\nObs Uncorrelated. Noise\nAdditive\ngaussian\n[0.0, 0.002]\nAction\nAction Correlated Noise\nAdditive\ngaussian\n[0.0, 0.015]\nAction Uncorrelated Noise\nAdditive\ngaussian\n[0.0, 0.05]\nEnvironment\nGravity\nAdditive\nnormal\n[0, 0.4]\nD\nReward functions\nD.1\nBuilding Blocks\nSearching\nDenote the \u03c4 is the commanded torques at each timestep, the count of individual pixels\nwithin the target object\u2019s segmentation mask in the top-down camera frame as P , The sum of the\ndistance between each fingertip and the object as P4\ni=0fi, the action penalty as \u2225a\u22252\n2, and the torque\npenalty as \u2225\u03c4\u22252\n2. Finally, the rewards are given by the following specific formula:\nr=\u03bb1\u2217P +\u03bb2\u2217min(e0\u2212\n4\nX\ni=0\nfi,0)+\u03bb3\u2217\u2225a\u22252\n2+\u03bb4\u2217\u2225\u03c4\u22252\n2\n(3)\nwhere \u03bb1 =5.0, \u03bb2 =1.0, \u03bb3 =\u22120.001, \u03bb4 =\u22120.003, and e0 =0.2.\nOrienting\nDenote the \u03c4 is the commanded torques at each timestep, the angular distance between\nthe current object pose and the initial pose as \u03b8, the sum of the distance between each fingertip and the\nobject as P4\ni=0fi, the action penalty as \u2225a\u22252\n2, and the torque penalty as \u2225\u03c4\u22252\n2. Finally, the rewards are\ngiven by the following specific formula:\nr=\u03bb1\u2217\u03b8+\u03bb2\u2217min(e0\u2212\n4\nX\ni=0\nfi,0)+\u03bb3\u2217\u2225a\u22252\n2+\u03bb4\u2217\u2225\u03c4\u22252\n2\n(4)\nwhere \u03bb1 =1.0, \u03bb2 =1.0, \u03bb3 =\u22120.001, \u03bb4 =\u22120.003, and e0 =0.6.\nGrasping\nDenote the \u03c4 is the commanded torques at each timestep, the sum of the distance between\neach fingertip and the object as P4\ni=0fi, the action penalty as \u2225a\u22252\n2, and the torque penalty as \u2225\u03c4\u22252\n2.\nFinally, the rewards are given by the following specific formula:\nr=\u03bb1\u2217exp[\u03b10\u2217min(e0\u2212\n4\nX\ni=0\nfi,0)]+\u03bb2\u2217\u2225a\u22252\n2+\u03bb3\u2217\u2225\u03c4\u22252\n2\n(5)\nwhere \u03bb1 = 1.0, \u03bb2 = \u22120.001, \u03bb3 = \u22120.003, \u03b10 = \u22125.0, and e0 = 0.1. It is worth noting that in the\nlatter half of our grasping training, we force the hand to lift, so if the grip is unstable, the object will\ndrop and the reward will decrease.\n15\nBlock 1 Block 2 Block 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7 Block 8\n(a) Real world\n(b) Simulation\nFigure 6: The block model we use in simulation and real-world. (b) is the eight blocks used in our\nbuilding blocks task. The upper Block 1-5 is the training block, and the lower Block 6-8 is the unseen\nblock for testing.\nInserting\nDenote the \u03c4 is the commanded torques at each timestep, the object and goal position as xo\nand xg, the angular position difference between the object and the goal as da, the sum of the distance\nbetween each fingertip and the object as P4\ni=0fi, the action penalty as \u2225a\u22252\n2, and the torque penalty as\n\u2225\u03c4\u22252\n2. Finally, the rewards are given by the following specific formula:\nr=\u03bb1\u2217exp[\u2212(\u03b10\u2217\u2225xo\u2212xg\u22252+\u03b11\u22172\u2217arcsin(clamp(\u2225da\u22252,0,1)))]+\n\u03bb2\u2217min(e0\u2212\n4\nX\ni=0\nfi,0)+\u03bb3\u2217\u2225a\u22252\n2+\u03bb4\u2217\u2225\u03c4\u22252\n2\n(6)\nwhere \u03bb1 =1.0, \u03bb2 =0.0, \u03bb3 =\u22120.001, \u03bb4 =\u22120.003, \u03b10 =20.0, \u03b11 =1.0, and e0 =0.06.\nD.2\nTool positioning\nGrasping\nDenote the \u03c4 is the commanded torques at each timestep, the sum of the distance between\neach fingertip and the object as P4\ni=0fi, the action penalty as \u2225a\u22252\n2, and the torque penalty as \u2225\u03c4\u22252\n2.\nFinally, the rewards are given by the following specific formula:\nr=\u03bb1\u2217exp[\u03b10\u2217min(e0\u2212\n4\nX\ni=0\nfi,0)]+\u03bb2\u2217\u2225a\u22252\n2+\u03bb3\u2217\u2225\u03c4\u22252\n2\n(7)\nwhere \u03bb1 = 1.0, \u03bb2 = \u22120.001, \u03bb3 = \u22120.003, \u03b10 = \u22125.0, and e0 = 0.1. It is worth noting that in the\nlatter half of our grasping training, we force the hand to lift, so if the grip is unstable, the object will\ndrop and the reward will decrease.\nIn-hand Orientation\nDenote the \u03c4 is the commanded torques at each timestep, the object and goal\nposition as xo and xg, the angular position difference between the object and the goal as da, the sum\nof the distance between each fingertip and the object as P4\ni=0fi, the action penalty as \u2225a\u22252\n2, and the\ntorque penalty as \u2225\u03c4\u22252\n2. Finally, the rewards are given by the following specific formula:\nr=\u03bb1\u2217exp[\u2212(\u03b10\u2217\u2225xo\u2212xg\u22252+\u03b11\u22172\u2217arcsin(clamp(\u2225da\u22252,0,1)))]+\n\u03bb2\u2217min(e0\u2212\n4\nX\ni=0\nfi,0)+\u03bb3\u2217\u2225a\u22252\n2+\u03bb4\u2217\u2225\u03c4\u22252\n2\n(8)\nwhere \u03bb1 =1.0, \u03bb2 =0.0, \u03bb3 =\u22120.001, \u03bb4 =\u22120.003, \u03b10 =20.0, \u03b11 =1.0, and e0 =0.06.\nD.3\nReward Construction\nWe use an exponential map in the grasping reward function, which is an effective reward shaping\ntechnique used in the case to minimize the distance between fingers and object (e.g., grasping task),\nintroduced by [65, 66]. For the same term in the other two reward function, since the other two reward\n16\nfunctions mainly consider other objectives, we empirically find there is no need to use exponential\nmap in these cases. To improve the calculation efficiency, we use quaternion to represent the object\norientation. The angular position difference is then computed through the dot product between the\nnormalized goal quaternion and the current object\u2019s quaternion.\nE\nDomain Randomization\nIsaac Gym provides lots of domain randomization functions for RL training. We add the randomization\nfor all the sub-tasks as shown in Table. 7 for each environment. we generate new randomization every\n1000 simulation steps.\nF\nTask Setups\nF.1\nSub-task definition.\nHere we introduce the functionalities of each sub-policy in the Building Block task: Search aims to\ndig and retrieve the target block when it is buried by other blocks in the box. The initial task goal is to\nmake the target block\u2019s visible surface in the wrist-view camera larger than a threshold. The transition\nfeasibility function finetunes the policy to a reach a state that facilitates the succeeding orientation.\nOrient aims to rotate the target block. The initial task goal is to freely rotating the target block in-hand\nwithout specific goal pose. In the backward step, the transition feasibility function finetunes the policy\nto rotate the block to a pose that facilitates the succeeding grasping and insertion. Grasp aims to lift\nup the target block and hold in-hand. The initial task goal is to lift up the target block for more than\n30 centimeters. The transition feasibility function further finetunes the policy to grasp the block in a\nway that allows the succeeding insertion to in-hand adjust the block for 90 degrees depending on the\ngiven task goal. Insert aims to rotate the block in-hand for 90 degrees if the goal pose of the block is\nvertical and adjust the robot\u2019s wrist position with 3D delta motions to align the block with the desired\ninsertion location. In the real-world experiments, since the finger motor of the dexterous hand is not\nstrong enough to fully insert the block, we add an additional scripted pressing motion to complete the\nlast step of the insertion.\nF.2\nBuilding Blocks\nBlock model.\nFor the building blocks task, we use the same model as Mega Bloks1 as our blocks. It is\na range of large, stackable construction blocks designed specifically for the small hands of the children.\nWe take eight different types of blocks (denoted as Block 1, Block 2,..., Block 8) as the models of our\nblock, and carefully measured the dimensions to ensure that they were the same as in the real world.\nThe block datasets is shown in Figure. 6. For all building block sub-tasks, we use Block 1-5 as the\ntraining object and Block 6-8 as the unseen object for testing.\nPhysics in insertion between two blocks.\nIt is difficult to simulate the realistic insertion in the\nsimulator, and it is easy to explode or model penetration when the two models are in frequent contact.\nTherefore, we want the plug and slot between the two blocks can be inserted without frequent friction.\nWe reduced the diameter of all block plugs and convex decomposed them via VHACD method when\nloaded into Isaac Gym. Finally, we made one block possible to insert another block through free fall to\nverify the final effect.\nInitialization.\nIn simulation, we randomly sample the initial block placement above the box, allowing\nthem to fall and form the initial scene. In the real-world experiments, we manually shuffle the blocks\u2019\nplacement in the box, with the shuffling based on the criteria that none of the task-related blocks lies\nwithin the margin of 10 centimeters from the edges of the box. If the criteria are not satisfied, we\nre-shuffle the blocks.\n1https://www.megabrands.com/en-us/mega-bloks.\n17\nFigure 7: The collision meshes in the simulation.\nInitial State\nGoal State\nHammer\nSpoon (unseen)\nSpatula (unseen)\nFigure 8: Visualization of the three tools we use in Tool Positioning task. The Hammer is use for\ntraining and the Spoon and Spatula is only use for testing. We also show the goal pose of the tools.\nSuccess criteria.\nIn the Building Block task, the task success is defined as whether the target block\nhas been inserted on the desired pose on the LEGO board. We assume the access to a building manual\nthat specifies the shape and color of the target block and its desired goal pose on the board. In the Tool\nPositioning task, the task success is defined as whether the tool has been lifted and held in hand in a\nready-to-use pose (e.g., hammer head facing down).\nTask objects.\nIn the Building Block task, we use the mesh model of Mega Blocks as our task objects.\nIt is a range of large, stackable construction blocks designed specifically for the small hands of children.\nWe take eight different types of blocks (denoted as Block 1, Block 2, ..., Block 8). These blocks are\nillustrated in Appendix Figure. 6. In our experiment, Block 1-5 are used as the training objects and\nBlock 6-8 are unseen ones for testing policy generalization. In the Tool Positioning task, the tools we\nconsider consists of hammer, scoop and spatula, which have different thickness over the handle and\nvariation of the center of mass. The hammer is used as a training object and the other two are unseen\nones for testing policy generalization.\nCollision meshes.\nWe visualize the collision mesh used in the simulation in Figure. 7. We do observe\nthe drop of simulation speed when loading 72 blocks, but it\u2019s still enough for training 1024 agents\ntogether at a speed of 5000 FPS with an NVIDIA RTX 3090 GPU. To optimize the speed, we reduce\nthe resolution of the convex decomposition over blocks in Search, Orient and Grasp sub-tasks. The\nhigh-resolution blocks are only used for the training of the Insert sub-task.\n18\nFigure 9: Snapshots of the searching task.\nFigure 10: Snapshots of the orienting task.\nFigure 11: Snapshots of the grasping task.\nFigure 12: Snapshots of the inserting task.\nFigure 13: Snapshots of the hammer positioning.\nF.3\nTool positioning\nFor the tool positioning task, we have a total of three tools: hammer, spatula, and spoon. We use the\nhammer for training and test both in the hammer, spatula, and spoon. This long-horizon task involves\ngrasp a tool and re-orient it onto a pose suitable for its use. Fig.8 shows what they look like and the\ninitial and goal state of the each three tools.\nF.4\nTypical frames of all sub-tasks\nFor the convenience of readers, we show some typical frames of all the sub-tasks in simulation.\nF.4.1\nBuilding Blocks\nWe visualize the rollout of the Building Blocks task in Figure. 9, Figure. 10, Figure. 11, and Figure. 12.\nF.4.2\nTool Positioning\nWe visualize the rollout of the Tool Positioning task in Figure. 13, Figure. 14, and Figure. 15.\n19\nFigure 14: Snapshots of the spoon positioning.\nFigure 15: Snapshots of the spatula positioning.\nTrained\nUnseen\nAll\nOurs w/o belief state 0.40\u00b10.08 0.16\u00b10.07 0.29\u00b10.06\nOurs w/o tactile\n0.43\u00b10.04 0.33\u00b10.00 0.37\u00b10.02\nOurs w/o both\n0.26\u00b10.05 0.02\u00b10.01 0.14\u00b10.02\nOurs\n0.43\u00b10.04 0.36\u00b10.04 0.38\u00b10.04\nTable 8: Ablation study on the system choices in single-step Orient task\nTrained\nUnseen\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nALL\nOurs (0-step)\n0.47\u00b10.06 0.44\u00b10.07 0.43\u00b10.00 0.49\u00b10.04 0.40\u00b10.04 0.51\u00b10.04 0.18\u00b10.01 0.16\u00b10.03 0.38\u00b10.04\nOurs (5-step)\n0.52\u00b10.07 0.47\u00b10.02 0.46\u00b10.03 0.55\u00b10.03 0.44\u00b10.02 0.54\u00b10.01 0.20\u00b10.03 0.17\u00b10.02 0.42\u00b10.03\nOurs (10-step) 0.61\u00b10.03 0.55\u00b10.01 0.52\u00b10.03 0.63\u00b10.03 0.51\u00b10.06 0.53\u00b10.06 0.22\u00b10.02 0.16\u00b10.01 0.46\u00b10.03\nOurs (15-step) 0.55\u00b10.03 0.51\u00b10.04 0.53\u00b10.02 0.59\u00b10.01 0.50\u00b10.07 0.50\u00b10.05 0.16\u00b10.04 0.14\u00b10.03 0.44\u00b10.04\nTable 9: Ablation study in historical frame of the transition feasibility function\nG\nMotor tactile and belief state.\nWe found that motor tactile and belief state are beneficial for dexterous in-hand manipulation. Tab. 8 is\nthe ablation study of the design choices of our input state space. We modify the objective of the Orient\nsub-task in the Building Blocks task to a pre-defined goal orientation and train each ablation method\nonly on this sub-policy. We find the belief state pose estimator has the highest improvement (9% in task\nsuccess rate), which highlights its effects on in-hand manipulation.\nH\nAblation study in historical frame of the transition feasibility function\nWe add an ablation study by using 0-step, 5-step, 10-step and 15-step of history states as the inputs to\nthe transition feasibility model, as shown in Table. 9. The task success rate gradually increases when\nmore history steps are used and becomes stable after 10 steps. This result indicates that 10 to 15 history\nsteps is ideal for the Building Block task.\nI\nEnvironmental speed\nTable. 10 shows the simulation FPS and wall-clock time cost of the training process for each sub-task.\nAll of our experiments are run with Intel i7-9700K CPU and NVIDIA RTX 3090 GPU.\nJ\nHyperparameters of the PPO\nJ.1\nBuilding Blocks\nJ.2\nTool Positioning\n20\nBuilding Blocks\nTool Positioning\nSearch\nOrient\nGrasp\nInsert\nGrasp\nReorient\nWall-clock time\n31111\u00b13691\n15458\u00b11381\n16397\u00b11904\n21851\u00b12791\n17282\u00b12472\n14500\u00b11831\n(s/10000 episode)\nFPS (frame/s)\n1298\u00b1154\n5920\u00b1529\n5504\u00b1639\n14360\u00b11834\n19920\u00b12849\n20896\u00b12638\nTable 10: Mean and standard deviation of FPS (frame per second) of the sub-tasks.\nTable 11: Hyperparameters of PPO in Building Blocks.\nHyperparameters\nSearching\nOrienting\nGrasping & Inserting\nNum mini-batches\n4\n4\n8\nNum opt-epochs\n5\n10\n2\nNum episode-length\n8\n20\n8\nHidden size\n[1024, 1024, 512]\n[1024, 1024, 512]\n[1024, 1024, 512]\nClip range\n0.2\n0.2\n0.2\nMax grad norm\n1\n1\n1\nLearning rate\n3.e-4\n3.e-4\n3.e-4\nDiscount (\u03b3)\n0.96\n0.96\n0.9\nGAE lambda (\u03bb)\n0.95\n0.95\n0.95\nInit noise std\n0.8\n0.8\n0.8\nDesired kl\n0.016\n0.016\n0.016\nEnt-coef\n0\n0\n0\nTable 12: Hyperparameters of PPO in Tool Positioning.\nHyperparameters\nGrasping\nIn-hand Orienting\nNum mini-batches\n4\n4\nNum opt-epochs\n5\n10\nNum episode-length\n8\n20\nHidden size\n[1024, 1024, 512]\n[1024, 1024, 512]\nClip range\n0.2\n0.2\nMax grad norm\n1\n1\nLearning rate\n3.e-4\n3.e-4\nDiscount (\u03b3)\n0.96\n0.96\nGAE lambda (\u03bb)\n0.95\n0.95\nInit noise std\n0.8\n0.8\nDesired kl\n0.016\n0.016\nEnt-coef\n0\n0\n21\n"
  },
  {
    "title": "Diffusion Generative Inverse Design",
    "link": "https://arxiv.org/pdf/2309.02040.pdf",
    "upvote": "2",
    "text": "Diffusion Generative Inverse Design\nMarin Vlastelica 1 2 Tatiana L\u00b4opez-Guevara 2 Kelsey Allen 2 Peter Battaglia 2 Arnaud Doucet 2\nKimberly Stachenfeld 2 3\nAbstract\nInverse design refers to the problem of optimiz-\ning the input of an objective function in order to\nenact a target outcome. For many real-world en-\ngineering problems, the objective function takes\nthe form of a simulator that predicts how the sys-\ntem state will evolve over time, and the design\nchallenge is to optimize the initial conditions that\nlead to a target outcome. Recent developments\nin learned simulation have shown that graph neu-\nral networks (GNNs) can be used for accurate,\nefficient, differentiable estimation of simulator\ndynamics, and support high-quality design op-\ntimization with gradient- or sampling-based op-\ntimization procedures. However, optimizing de-\nsigns from scratch requires many expensive model\nqueries, and these procedures exhibit basic fail-\nures on either non-convex or high-dimensional\nproblems. In this work, we show how denoising\ndiffusion models (DDMs) can be used to solve\ninverse design problems efficiently and propose\na particle sampling algorithm for further improv-\ning their efficiency. We perform experiments on\na number of fluid dynamics design challenges,\nand find that our approach substantially reduces\nthe number of calls to the simulator compared to\nstandard techniques.\n1. Introduction\nSubstantial improvements to our way of life hinge on devis-\ning solutions to engineering challenges, an area in which\nMachine Learning (ML) advances is poised to provide posi-\ntive real-world impact. Many such problems can be formu-\nlated as designing an object that gives rise to some desirable\nphysical dynamics (e.g. designing an aerodynamic car or\na watertight vessel). Here we are using ML to accelerate\n1Max Planck Institute for Intelligent Systems, T\u00a8ubingen,\nGermany 2Google DeepMind, London, UK 3Columbia Uni-\nversity, New York, NY. Correspondence to:\nMarin Vlastel-\nica <marin.vlastelica@tue.mpg.de>,\nKimberly Stachenfeld\n<stachenfeld@deepmind.com>.\nPreprint. Copyright 2023 by the authors.\nthis design process by learning both a forward model of the\ndynamics and a distribution over the design space.\nPrior approaches to ML-accelerated design have used neural\nnetworks as a differentiable forward model for optimization\n(Challapalli et al., 2021; Christensen et al., 2020; G\u00b4omez-\nBombarelli et al., 2018). We build on work in which the\nforward model takes the specific form of a GNN trained\nto simulate fluid dynamics (Allen et al., 2022). Since the\nlearned model is differentiable, design optimization can\nbe accomplished with gradient-based approaches (although\nthese struggle with zero or noisy gradients and local minima)\nor sampling-based approaches (although these fare poorly\nin high-dimensional design spaces). Both often require\nmultiple expensive calls to the forward model. However,\ngenerative models can be used to propose plausible designs,\nthereby reducing the number of required calls (Forte et al.,\n2022; Zheng et al., 2020; Kumar et al., 2020).\nIn this work, we use DDMs to optimize designs by sampling\nfrom a target distribution informed by a learned data-driven\nprior. DDMs have achieved extraordinary results in im-\nage generation (Song et al., 2020a;b; Karras et al., 2022;\nHo et al., 2020), and has since been used to learn efficient\nplanners in sequential decision making and reinforcement\nlearning (Janner et al., 2022; Ajay et al., 2022), sampling\non manifolds (De Bortoli et al., 2022) or constrained opti-\nmization formulations (Graikos et al., 2022). Our primary\ncontribution is to consider DDMs in the setting of physi-\ncal problem solving. We find that such models combined\nwith continuous sampling procedures enable to solve de-\nsign problems orders of magnitude faster than off-the-shelf\noptimizers such as CEM and Adam. This can be further\nimproved by utilizing a particle sampling scheme to update\nthe base distribution of the diffusion model which by cheap\nevaluations (few ODE steps) with a learned model leads\nto better designs in comparison to vanilla sampling proce-\ndures. We validate our findings on multiple experiments in\na particle fluid design environment.\n2. Method\nGiven some task specification c, we have a target distribu-\ntion of designs \u03c0(x) which we want to optimize w.r.t. x. To\nsimplify notation, we do not emphasize the dependence of\narXiv:2309.02040v2  [cs.LG]  18 Sep 2023\nDiffusion Generative Inverse Design\nFigure 1. (a) Given initial conditions governed by \u03b8IC, energy function parameters \u03b8E, and learned GNN dynamics model fM, design\nsamples x from the diffusion model are assigned a cost E(x). (b) Schematic of the DDM training (c) Gradients \u2207E and conditioning set\n(\u03b8E and E) inform energy and conditional guidance, resp.\n\u03c0 on c. This distribution is a difficult object to handle, since\na highly non-convex cost landscape might hinder efficient\noptimization. We can capture prior knowledge over \u2018sen-\nsible\u2019 designs in form of a prior distribution p(x) learned\nfrom existing data. Given a prior, we may sample from the\ndistribution\n\u02dc\u03c0(x) \u221d p(x)\u03c0(x),\n(1)\nwhich in this work is achieved by using a diffusion method\nwith guided sampling. The designs will subsequently be\nevaluated by a learned forward model comprised of a pre-\ntrained GNN simulator and a reward function (Allen et al.,\n2022; Pfaff et al., 2021; Sanchez-Gonzalez et al., 2020) (see\nAppendix A).\nLet E : X 7\u2192 R be the cost (or \u201cenergy\u201d) of a design x \u2208 X\nfor a specific task c under the learned simulator (dependence\nof E on c is omitted for simplicity). The target distribution\nof designs \u03c0(x) is defined by the Boltzmann distribution\n\u03c0(x) := 1\nZ exp\n\u0012\n\u2212E(x)\n\u03c4\n\u0013\n,\n(2)\nwhere Z denotes the unknown normalizing constant and \u03c4 a\ntemperature parameter. As \u03c4 \u2192 0, this distribution concen-\ntrates on its modes, that is on the set of the optimal designs\nfor the cost Ec(x). Direct methods to sample from \u03c0(x)\nrely on expensive Markov chain Monte Carlo techniques or\nvariational methods minimizing a reverse KL criterion.\nWe will rely on a data-driven prior learned by the diffu-\nsion model from previous optimization attempts. We col-\nlect optimization trajectories of designs for different task\nparametrizations c using Adam (Kingma & Ba, 2015) or\nCEM (Rubinstein, 1999) to optimize x. Multiple entire\noptimization trajectories of designs are included in the train-\ning set for the generative model, providing a mix of design\nquality. These optimization trajectories are initialized to\nflat tool(s) below the fluid (see Figure 5), which can be\nmore easily shaped into successful tools than a randomly\ninitialized one. Later, when we compare the performance\nof the DDM to Adam and CEM, we will be using randomly\ninitialized tools for Adam and CEM, which is substantially\nmore challenging.\n2.1. Diffusion generative models\nWe use DDMs to fit p(x) (Ho et al., 2020; Song et al.,\n2020b). The core idea is to initialize using training data\nx0 \u223c p, captured by a diffusion process (xt)t\u2208[0,1] defined\nby\ndxt = \u2212\u03b2txtdt +\np\n2\u03b2tdwt,\n(3)\nwhere (wt)t\u2208[0,1] denotes the Wiener process. We denote by\npt(x) the distribution of xt under (3). For \u03b2t large enough,\np1(x) \u2248 N(x; 0, I). The time-reversal of (3) satisfies\ndxt = \u2212\u03b2t[xt + 2\u2207x log pt(xt)]dt +\np\n2\u03b2tdw\u2212\nt ,\n(4)\nwhere (w\u2212\nt )t\u2208[0,1] is a Wiener process when time flows\nbackwards from t = 1 to t = 0, and dt is an in-\nfinitesimal negative timestep.\nBy initializing (4) using\nx1 \u223c p1, we obtain x0 \u223c p.\nIn practice, the gener-\native model is obtained by sampling an approximation\nof (4), replacing p1(x) by N(x; 0, I) and the intractable\nscore \u2207x log pt(x) by s\u03b8(x, t). The score estimate s\u03b8(x, t)\nis learned by denoising score matching, i.e. we use the\nfact that \u2207x log pt(x) =\nR\n\u2207x log p(xt|x0)p(x0|xt)dx0\nwhere p(xt|x0) = N(xt; \u221a\u03b1tx0, \u221a1 \u2212 \u03b1tI) is the tran-\nsition density of (3), \u03b1t being a function of (\u03b2s)s\u2208[0,t]\n(Song et al., 2020b). It follows straightforwardly that the\nscore satisfies \u2207x log pt(x) = \u2212E[\u03f5|xt = x]/\u221a1 \u2212 \u03b1t for\nDiffusion Generative Inverse Design\nxt = \u221a\u03b1tx0 + \u221a1 \u2212 \u03b1t\u03f5. We then learn the score by\nminimizing\nL(\u03b8) = Ex0\u223cp,t\u223cU(0,1),\u03f5\u223cN (0,I)\u2225\u03f5\u03b8(xt, t) \u2212 \u03f5\u22252,\n(5)\nwhere \u03f5\u03b8(x, t) is a denoiser estimating E[\u03f5|xt = x]. The\nscore function s\u03b8(x, t) \u2248 \u2207x log pt(x) is obtained using\ns\u03b8(x, t) = \u2212 \u03f5\u03b8(x, t)\n\u221a1 \u2212 \u03b1t\n.\n(6)\nGoing forward, \u2207 refers to \u2207x unless otherwise stated. We\ncan also sample from p(x) using an ordinary differential\nequation (ODE) developed in (Song et al., 2020b).\nLet us define xt = xt/\u221a\u03b1t and \u03c3t = \u221a1 \u2212 \u03b1t/\u221a\u03b1t.\nThen by initializing x1 \u223c N(0, I), equivalently x1 \u223c\nN(0, \u03b1\u22121\n1 I) and solving backward in time\ndxt = \u03f5(t)\n\u03b8\n \nxt\np\n\u03c32\nt + 1\n!\nd\u03c3t,\n(7)\nthen x0 = \u221a\u03b1t x0 is an approximate sample from p(x).\n2.2. Approximately sampling from target \u02dc\u03c0(x)\nWe want to sample \u02dc\u03c0(x) defined in (1) where p(x) can be\nsampled from using the diffusion model. We describe two\npossible sampling procedures with different advantages for\ndownstream optimization.\nEnergy guidance. Observe that\n\u02dc\u03c0t(xt) =\nZ\n\u02dc\u03c0(x0)p(xt|x0)dx0,\nand the gradient satisfies\n\u2207 log \u02dc\u03c0t(xt) = \u2207 log pt(xt) + \u2207 log \u03c0t(xt),\nwhere \u03c0t(xt) =\nR\n\u03c0(x0)p(x0|xt)dx0. We approximate\nthis term by making the approximation\n\u02c6x(xt, t)\n=\n\u0012xt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(xt, t)\n\u221a\u03b1t\n\u0013\n|\n{z\n}\n\u201c estimated x0\u201d\n,\n\u03c0t(xt)\n\u2248\n\u03c0(\u02c6xt(xt, t)).\n(8)\nNow, by (6), and the identity \u2207 log \u03c0(x) = \u2212\u03c4 \u22121\u2207E(x),\nwe may change the reverse sampling procedure by a modi-\nfied denoising vector\n\u02dc\u03f5\u03b8(xt, t) = \u03f5\u03b8(xt, t) + \u03bb\u03c4 \u22121\u221a\n1 \u2212 \u03b1t\u2207E(\u02c6x(xt, t)), (9)\nwith \u03bb being an hyperparameter. We defer the results on\nenergy guidance Appendix E.\nConditional guidance. Similarly to classifier-free guid-\nance (Ho & Salimans, 2022), we explore conditioning on\nAlgorithm 1 Particle optimization of base distribution.\ninput energy function E, diffusion generative model p\u03b8, temperature \u03c4, noise\nscale \u03c3, rounds K.\n2: S0\n1 = {xi\n1}N\ni=1 for xi\n1\ni.i.d.\n\u223c N (0, I)\nS0 = \u2205, S1 = \u2205 # t = 0 and t = 1 sample sets\n4: for k \u2208 {0 . . . K} do\nCompute Sk\n0 = {xi\n0}N\ni=1 from Sk\n1 by solving reverse ODE in Eq. (7).\n6:\nS0 = S0 \u222a Sk\n0, S1 = S1 \u222a Sk\n1\nCompute normalized importance weights\nW =\nn\nw | w \u221d exp\n\u0000\u2212 E(x0)\n\u03c4\n\u0001\n, x0 \u2208 S0\no\n8:\nSet S\nk+1\n1\n= {xi\n1}|S1|\ni=1 for xi\n1\ni.i.d.\n\u223c P|S1|\ni=1 wi\u03b4xi\n1(x)\nSet Sk+1\n1\n= {\u02dcxi\n1}|S1|\ni=1 for \u02dcxi\n1 \u223c N (x; xi\n1, \u03c32I)\n10: end for\nreturn arg minx\u2208S0 E(x)\ncost (energy) e and task c. A modified denoising vector in\nthe reverse process follows as a combination between the\ndenoising vector of a conditional denoiser \u03f5\u03d5 and uncondi-\ntional denoiser \u03f5\u03b8\n\u02dc\u03f5(xt, c, e, t) = (1 + \u03bb)\u03f5\u03d5(xt, c, e, t) \u2212 \u03bb\u03f5\u03b8(xt, t), (10)\nwhere \u03f5\u03d5 is learned by conditioning on c and cost e from op-\ntimization trajectories. In our experiments we shall choose\nc to contain the design cost percentile and target goal desti-\nnation \u03b8E for fluid particles (Figure 1c).\n2.3. A modified base distribution through particle\nsampling\nOur generating process initializes samples at time t = 1\nfrom N(x; 0, I) \u2248 p1(x). The reverse process with modifi-\ncations from subsection 2.2 provides approximate samples\nfrom \u02dc\u03c0(x) at t = 0. However, as we are approximately\nsolving the ODE of an approximate denoising model with\nan approximate cost function, this affects the quality of sam-\nples with respect to E1. Moreover, \u201cbad\u201d samples from\nN(x; 0, I) are hard to correct by guided sampling.\nTo mitigate this, instead of using samples from N(x; 0, I) to\nstart the reverse process of \u02dc\u03c0(x), we use a multi-step particle\nsampling scheme which evaluates the samples {xi\n1}N\ni=1 by a\nrough estimate of the corresponding {xi\n0}N\ni=1 derived from a\nfew-step reverse process and evaluation with E. The particle\nprocedure relies on re-sampling from a weighted particle\napproximation of \u03c0(x) and then perturbing the resampled\nparticles, see 1. This heuristic does not provide samples\nfrom \u02dc\u03c0 but we found that it provides samples of lower energy\nsamples across tasks. However, with N samples in each\nof the k rounds, it still requires O(Nk) evaluations of E,\nwhich may be prohibitively expensive depending on the\nchoice of E.\n1Ideally at test time we would evaluate the samples with the\nground-truth dynamics model, but we have used the approximate\nGNN model due to time constraints on the project.\nDiffusion Generative Inverse Design\n0\n200\n400\n600\n800\n# queries\n2000\n1500\n1000\n500\nCost\nwithout shift\n0\n200\n400\n600\n800\n# queries\nCost\nwith shift\nDiff\nCEM\nAdam\nFigure 2. Performance of the different optimization methods in the\nangle optimization task. We observe that the diffusion generative\nmodel requires a small number of model queries, whereas Adam\nin comparison requires many expensive model queries.\n3. Experiments\nWe evaluate our approach on a 2D fluid particle environment\nwith multiple tasks of varying complexity, which all involve\nshaping the tool (Figure 1a, black lines) such that the fluid\nparticles end up in a region of the scene that minimizes the\ncost E (Allen et al., 2022). As baselines, we use the Adam\noptimizer combined with a learned model of the particles\nand the CEM optimizer which also optimizes with a learned\nmodel. For all experiments we have used a simple MLP as\nthe denoising model and GNN particle simulation model for\nevaluation of the samples.\nThe first task is a simple \u201cContain\u201d task with a single source\nof water particles, a goal position above the floor speci-\nfied by c = (x, y), and an articulated tool with 16 joints\nwhose angles are optimized to contain the fluid near the\ngoal (see Allen et al. (2022)). In Figure 2a, we see that\nboth the Adam optimizer and CEM are able to optimize the\ntask. However with training a prior distribution on optimiza-\ntion trajectories and guided sampling we are able to see the\nbenefits of having distilled previous examples of optimized\ndesigns into our prior, and achieve superior performance\nwith fewer samples in unseen tasks sampled in-distribution.\nReward (-E)\nFigure 3. Generative models were trained on a dataset of designs\nproduced from CEM- and Adam-optimized designs on either of\ntwo tasks Matching, Non-matching, or Both. Designs in the train\ndataset were filtered to have costs below the specified cutoff.\nIf we modify the task by introducing a parameter controlling\nx, y shift parameter, we observe that Adam fails. This is\nbecause there are many values of the shift for which the\ntool makes no contact with the fluid (see Figure 2b), and\ntherefore gets no gradient information from E. We provide\nresults for more complex tasks in Appendix C (Figure 5).\nOverall, we find that this approach is capable of tackling\na number of different types of design challenges, finding\neffective solutions when obstacles are present, for multi-\nmodal reward functions, and when multiple tools must be\ncoordinated.\n3.1. Dataset quality impact\nWe analyzed how the model performs with conditional guid-\nance when trained on optimization trajectories of CEM that\noptimize the same task (matching), a different task (non-\nmatching), or a mix of tasks (Figure 3). The two tasks were\n\u201cContain\u201d (described above) and \u201cRamp\u201d (transport fluid to\nthe far lower corner). Unsurprisingly, the gap between de-\nsigns in the training dataset and the solution for the current\ntask has a substantial impact on performance. We also con-\ntrol the quality of design samples by filtering out samples\nabove a certain cost level for the c with which they were gen-\nerated. Discarding bad samples from the dataset does not\nimprove performance beyond a certain point: best perfor-\nmance is obtained with some bad-performing designs in the\ndataset. Intuitively, we believe this is because limiting the\ndataset to a small set of optimized designs gives poor cov-\nerage of the design space, and therefore generalizes poorly\neven to in-distribution test problems. Further, training the\ngenerative model on samples from optimization trajectories\nof only a non-matching task has a substantial negative im-\npact on performance. We expect the energy guidance not to\nsuffer from the same transfer issues as conditional guidance,\nsince more information about the task is given through the\nenergy function. Since we indeed obtain data to fit p(x)\nfrom Adam and CEM runs, why is diffusion more efficient?\nWe discuss this in Appendix B.\n3.2. Particle optimization in base distribution\nWe also observe performance improvements by using the\nparticle search scheme from subsection 2.3, see Figure 4.\nGauss\nPS\nFigure 4. Gaussian base distribution vs. particle sampling (PS).\nDiffusion Generative Inverse Design\nWe hypothesize that the reason for this is because of func-\ntion approximation. Since we are dealing with approxi-\nmate scores, it is hard for the learned generative model to\npinpoint the optima, therefore a sampling-based search ap-\nproach helps. We note that the evaluation of a sample with\nE requires solving the ODE for sample x1, we use a 1-step\nreverse process making use of the relation in (8). Conse-\nquently, we can expect that linearizing the sampling will\nimprove the particle search.\n4. Conclusion\nIn this work we have demonstrated the benefits of using\ndiffusion generative models in simple inverse designs tasks\nwhere we want to sample from high probability regions\nof a target distribution \u03c0(x) defined via E, while having\naccess to optimization data. We analyzed energy-based and\nconditional guidance where the energy function involves\nrolling out a GNN. We find that energy guidance is a viable\noption, but conditional guidance works better in practice,\nand that performance depends heavily on the generative\nmodel\u2019s training data. Finally, we have introduced particle\nsearch in the base distribution as a means to improve quality\nof the samples and demonstrated this on multiple tasks.\n5. Acknowledgments\nWe would like to thank Conor Durkan, Charlie Nash, George\nPapamakarios, Yulia Rubanova, and Alvaro Sanchez-\nGonzalez for helpful conversations about the project. We\nwould also like to thank Alvaro Sanchez-Gonzalez for com-\nments on the manuscript.\nReferences\nAjay, A., Du, Y., Gupta, A., Tenenbaum, J. B., Jaakkola,\nT. S., and Agrawal, P. Is conditional generative modeling\nall you need for decision-making?\nIn NeurIPS 2022\nFoundation Models for Decision Making Workshop, 2022.\nAllen, K. R., Lopez-Guevara, T., Stachenfeld, K. L.,\nSanchez-Gonzalez, A., Battaglia, P. W., Hamrick, J. B.,\nand Pfaff, T. Physical design using differentiable learned\nsimulators. CoRR, abs/2202.00728, 2022. URL https:\n//arxiv.org/abs/2202.00728.\nBattaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-\nGonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti,\nA., Raposo, D., Santoro, A., Faulkner, R., et al. Rela-\ntional inductive biases, deep learning, and graph networks.\narXiv preprint arXiv:1806.01261, 2018.\nChallapalli, A., Patel, D., and Li, G. Inverse machine learn-\ning framework for optimizing lightweight metamaterials.\nMaterials & Design, 208:109937, 2021.\nChristensen, T., Loh, C., Picek, S., Jakobovi\u00b4c, D., Jing, L.,\nFisher, S., Ceperic, V., Joannopoulos, J. D., and Solja\u02c7ci\u00b4c,\nM. Predictive and generative machine learning models\nfor photonic crystals. Nanophotonics, 9(13):4183\u20134192,\n2020.\nDe Bortoli, V., Mathieu, E., Hutchinson, M., Thornton, J.,\nTeh, Y. W., and Doucet, A. Riemannian score-based\ngenerative modelling. In Advances in Neural Information\nProcessing Systems, 2022.\nForte, A. E., Hanakata, P. Z., Jin, L., Zari, E., Zareei, A.,\nFernandes, M. C., Sumner, L., Alvarez, J. T., and Bertoldi,\nK. Inverse design of inflatable soft membranes through\nmachine learning. Advanced Functional Materials, 32,\n2022.\nGilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and\nDahl, G. E. Neural message passing for quantum chem-\nistry. In International Conference on Machine Learning,\n2017.\nG\u00b4omez-Bombarelli,\nR.,\nWei,\nJ. N.,\nDuvenaud,\nD.,\nHern\u00b4andez-Lobato, J., S\u00b4anchez-Lengeling, B., Sheberla,\nD., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P.,\nand Aspuru-Guzik, A. Automatic chemical design us-\ning a data-driven continuous representation of molecules.\nACS Central Science, 4(2):268\u2013276, 02 2018.\nGraikos, A., Malkin, N., Jojic, N., and Samaras, D. Dif-\nfusion models as plug-and-play priors. In Advances in\nNeural Information Processing Systems, 2022.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in Neural Information Process-\ning Systems, 33:6840\u20136851, 2020.\nJanner, M., Du, Y., Tenenbaum, J., and Levine, S. Plan-\nning with diffusion for flexible behavior synthesis. In\nInternational Conference on Machine Learning, 2022.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models.\nIn Advances in Neural Information Processing Systems,\n2022.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference on Learning\nRepresentations, 2015.\nKumar, S., Tan, S., Zheng, L., and Kochmann, D. M.\nInverse-designed spinodoid metamaterials. npj Computa-\ntional Materials, 6:1\u201310, 2020.\nDiffusion Generative Inverse Design\nPfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and\nBattaglia, P. Learning mesh-based simulation with graph\nnetworks. In International Conference on Learning Rep-\nresentations, 2021.\nRubinstein, R. The cross-entropy method for combinatorial\nand continuous optimization. Methodology and Comput-\ning in Applied Probability, 1:127\u2013190, 1999.\nSanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R.,\nLeskovec, J., and Battaglia, P.\nLearning to simulate\ncomplex physics with graph networks. In International\nConference on Machine Learning, 2020.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. In International Conference on Learning\nRepresentations, 2020a.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. In International\nConference on Learning Representations, 2020b.\nZheng, L., Kumar, S., and Kochmann, D. M. Data-driven\ntopology optimization of spinodoid metamaterials with\nseamlessly tunable anisotropy. ArXiv, abs/2012.15744,\n2020.\nDiffusion Generative Inverse Design\nAppendix for Diffusion Generative Inverse Design\nA. Learned simulation with graph neural\nnetworks\nAs in Allen et al. (2022), we rely on the recently developed\nMESHGNN model (Pfaff et al., 2021), which is an exten-\nsion of the GNS model for particle simulation (Sanchez-\nGonzalez et al., 2020). MESHGNN is a type of message-\npassing graph neural network (GNN) that performs both\nedge and node updates (Battaglia et al., 2018; Gilmer et al.,\n2017), and which was designed specifically for physics sim-\nulation.\nWe consider simulations over physical states represented\nas graphs G \u2208 G. The state G = (V, E) has nodes V\nconnected by edges E, where each node v \u2208 V is associated\nwith a position uv and additional dynamical quantities qv.\nIn this environment, each node corresponds to a particle and\nedges are computed dynamically based on proximity.\nThe simulation dynamics on which the model is trained\nare given by a \u201cground-truth\u201d simulator which maps the\nstate Gt at time t to the state Gt+1 at time t + \u2206t. The\nsimulator can be applied iteratively over K time steps to\nyield a trajectory of states, or a \u201crollout,\u201d which we denote\n(Gt0, ..., GtK). The GNN model M is trained on these\ntrajectories to imitate the ground-truth simulator fS. The\nlearned simulator fM can be similarly applied to produce\nrollouts ( \u02dcGt0, \u02dcGt1, ..., \u02dcGtK), where \u02dcGt0 = Gt0 represents\ninitial conditions given as input.\nB. Further discussion on results\nWe trained the diffusion model on data generated from\nAdam and CEM optimization runs and noticed an improve-\nment over Adam and CEM on the evaluation tasks. The\nreason for this is that both Adam and CEM need good ini-\ntializations to solve the tasks efficiently, for example in Fig-\nure 2 for each run the initial design for Adam has uniformly\nsampled angles and x, y coordinates within the bounds of\nthe environment, which would explain why we obtain worse\nresults on average for Adam than Allen et al. (2022). Sim-\nilarly, for CEM we use a Gaussian sampling distribution\nwhich is initialized with zero mean and identity covariance.\nIf most of the density of the initial CEM distribution is not\nconcentrated near the optimal design, then CEM will require\nmany samples to find it.\nIn comparison, the diffusion model learns good initializa-\ntions of the designs through p(x) which can further be\nimproved via guided sampling, as desired.\nC. Particle environments\nFor evaluation, we consider similar fluid particle-simulation\nenvironments as in Allen et al. (2022). The goal being to\ndesign a \u2018tool\u2019 that brings the particles in a specific configu-\nration. We defined the energy as the radial basis function\nE(x) =\nX\np\u2208P\nexp\n\u0012\n\u2212\u2225xt\np \u2212 x\u22c6\np\u2225\n\u03c3\n\u0013\n,\nFigure 5. Examples of guided-diffusion generated designs for different tasks c that we consider in this work. The initial designs start off\ncompletely at random, and the optimized ones solve the task.\nDiffusion Generative Inverse Design\nFigure 6. Examples of designs optimized with Adam, CEM, and guided diffusion using the generative model. Designs are initialized as\nrandom joint angles. Each design shown is the top scoring design for that optimizer, evaluated under the learned model, after having been\ntrained on 1000 calls to the simulator (the limit of the x-axis in Figure 2).\nwhere xt\np are the coordinates of particle p after rolling out\nthe simulation with the model fM with initial conditons \u03b8IC\nand parameters \u03b8E. Note that the energy is evaluated on the\nlast state of the simulation, hence \u2207E needs to propagate\nthrough the whole simulation rollout.\nIn additon to the \u201cContain\u201d environments described in sec-\ntion 3, we provide further example environments that we\nused for evaluation with the resulting designs from guided\ndiffusion can be seen in Figure 5.\nThe task with the obstacle required that the design is fairly\nprecise in order to bring the fluid into the cup, this is to high-\nlight that the samples found from the diffusion model with\nconditional guidance + particle sampling in base distribution\nare able to precisely pinpoint these types of designs.\nFor the bi-modal task, where we have two possible minima\nof the cost function, we are able to capture both of the modes\nwith the diffusion model.\nIn the case where we increase the dimensionality of the\ndesigns where we have x, y shift parameters for each of the\ntool joints, and the tools are disjoint, the diffusion model\nis able to come up with a parameterization that brings the\nparticles in a desired configuration. However, the resulting\ndesigns are not robust and smooth, indicating that further\nmodifications need to be made in form of constraints or\nregularization while guiding the reverse process to sample\nfrom \u02dc\u03c0(x).\nD. Discussion on choice of guidance\nAs we will see in section 3, conditional guidance with cor-\nrected base distribution sampling tends to work better than\nenergy guidance. In cases where the gradient of the energy\nfunction is expensive to evaluate, an energy-free alternative\nmight be better, however this requires learning a conditional\nGauss\nPS\nFigure 7. Sampling (random search) from p1(x) and particle sam-\npling in the bi-modal environment. We observe that even after\nincreasing the number of samples, particle search further improves\nperformance with same number of samples.\nmodel, i.e. necessitates access to conditional samples.\nE. Energy guidance\nWe have found that using the gradient of the energy function\nas specified in equation (9) is a viable way of guiding the\nsamples, albeit coming with the caveat of many expensive\nevaluations, see Figure 8. The guidance is very sensitive\nto the choice of the scaling factor \u03bb, in our experiments\nwe have found that a smaller scaling factor with many in-\ntegration steps achieves better sample quality. Intuitively,\nthis follows from the fact that it is difficult to guide \u2018bad\u2019\nsamples in the base distribution p1(x), which motivates\nthe particle energy-based sampling scheme introduced in\nalgorithm 1.\nFurther, we have looked at how to combine the gradient of\nthe energy with the noised marginal score. We have found\nthat re-scaling to have the same norm as the noised marginal\nimproves the stability of guidance, as shown in Figure 8.\nHere, we have analyzed multiple functions with which we\ncombined the energy gradient and the noised marginal score,\nwe have looked at the following variants:\nDiffusion Generative Inverse Design\nlinear\nlinear-unit\nlinear-norm\ncvx\nFigure 8. Performance of energy guidance depending on guidance\nscale \u03bb (x axis) for different modifications to score of noised\nmarginal.\n\u2022 linear - simple linear addition of \u03bb\u2207E.\n\u2022 linear-unit - linear addition of \u03bb \u2207E\n\u2225\u2207E\u2225.\n\u2022 cvx - convex combination of \u2207E and \u03f5\u03b8.\n\u2022 linear-norm - linear addition of \u03bb \u2207E\u2225\u03f5\u03b8\u2225\n\u2225\u2207E\u2225\nDiffusion Generative Inverse Design\nContain\nRamp\nWith obstacle\nBi-modal\nMulti-tool\nEnvironment size\n1x1\n1x1\n1x1\n1x1\n1x1\nRollout length\n150\n150\n150\n150\n150\nInitial fluid box(es)\nleft\n0.2\n0.2\n0.45\n0.25, 0.65\n0.2\nright\n0.3\n0.3\n0.55\n0.35, 0.75\n0.3\nbottom\n0.5\n0.5\n0.5\n0.5\n0.5\ntop\n0.6\n0.6\n0.6\n0.6\n0.6\nReward sampling box\nleft\n0.4\n0.8\n0.2\n0.25, 0.65\n0.2\nright\n0.6\n1.0\n0.2\n0.35, 0.75\n0.3\nbottom\n0.1\n0.0\n0.2\n0.1\n0.2\ntop\n0.3\n0.2\n0.2\n0.2\n0.5\nReward \u03c3\n0.1\n0.1\n0.05\n0.1\n0.1\n# tools\n1\n1\n1\n1\n16\n# joint angles\n16\n16\n16\n16\n1\nDesign parameters\njoint angles,\njoint angles\njoint angles,\njoint angles,\nshift\nshift (optional)\nshift\nshift\nTool position (left)\n[0.15, 0.35]\n[0.15, 0.35]\n[0.25, 0.35]\n[0.3, 0.6]\n[0.15\u20130.2, 0.35\u20130.4]\nTool Length\n0.8\n0.8\n0.6\n0.4\n0.1\nAdditional obstacles\n\u2014\n\u2014\nbarrier halfway\n\u2014\n\u2014\nbetween cup and fluid,\ncup around goal\nTable 1. Task Parameters.\n"
  }
]