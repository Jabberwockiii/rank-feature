[
  {
    "title": "AudioPaLM: A Large Language Model That Can Speak and Listen",
    "link": "https://arxiv.org/pdf/2306.12925.pdf",
    "upvote": "47",
    "text": "AudioPaLM: A Large Language Model That Can\nSpeak and Listen\nPaul K. Rubenstein\u2217\nChulayuth Asawaroengchai\u2217\nDuc Dung Nguyen\u2217\nAnkur Bapna\nZal\u00e1n Borsos\nF\u00e9lix de Chaumont Quitry\nPeter Chen\nDalia El Badawy\nWei Han\nEugene Kharitonov\nHannah Muckenhirn\nDirk Padfield\nJames Qin\nDanny Rozenberg\nTara Sainath\nJohan Schalkwyk\nMatt Sharifi\nMichelle Tadmor Ramanovich\nMarco Tagliasacchi\nAlexandru Tudor\nMihajlo Velimirovi\u00b4c\nDamien Vincent\nJiahui Yu\nYongqiang Wang\nVicky Zayats\nNeil Zeghidour\nYu Zhang\nZhishuai Zhang\nLukas Zilka\nChristian Frank\nGoogle\nAbstract\nWe introduce AudioPaLM, a large language model for speech understanding and\ngeneration. AudioPaLM fuses text-based and speech-based language models,\nPaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified\nmultimodal architecture that can process and generate text and speech with applica-\ntions including speech recognition and speech-to-speech translation. AudioPaLM\ninherits the capability to preserve paralinguistic information such as speaker iden-\ntity and intonation from AudioLM and the linguistic knowledge present only in\ntext large language models such as PaLM-2. We demonstrate that initializing\nAudioPaLM with the weights of a text-only large language model improves speech\nprocessing, successfully leveraging the larger quantity of text training data used\nin pretraining to assist with the speech tasks. The resulting model significantly\noutperforms existing systems for speech translation tasks and has the ability to\nperform zero-shot speech-to-text translation for many languages for which in-\nput/target language combinations were not seen in training. AudioPaLM also\ndemonstrates features of audio language models, such as transferring a voice across\nlanguages based on a short spoken prompt. We release examples of our method at:\nhttps://google-research.github.io/seanet/audiopalm/examples.\n1\nIntroduction\nLarge language models (LLMs) [Brown et al., 2020, Rae et al., 2021, Chowdhery et al., 2022] excel\nat generating text for tasks that require the modeling of complex interactions as well as knowledge\nretrieval, such as open-domain question answering or few-shot machine translation [Anil et al., 2023].\nThe remarkable generative abilities of the underlying system \u2014 a Transformer [Vaswani et al., 2017]\ntrained to predict sequences of discrete tokens \u2014 have been subsequently extended to continuous,\nnatural signals with images [Yu et al., 2022b] or audio waveforms [Lakhotia et al., 2021, Kreuk et al.,\n2022, Wang et al., 2023] being converted into a stream of discrete units through a lossy compression\nalgorithm and then modeled in a sequential fashion as would be text.\nIn the context of audio generation, the AudioLM framework [Borsos et al., 2022] has introduced\na hierarchical approach which combines two types of audio tokens, with high-level coarse tokens\nextracted from self-supervised embeddings [Chung et al., 2021] being used to condition the generation\nof lower-level codes of a neural codec [Zeghidour et al., 2021]. This general framework, which makes\n\u2217Authors have contributed equally to this work.\narXiv:2306.12925v1  [cs.CL]  22 Jun 2023\nlittle assumptions about the nature of the modeled audio signals, has been used to generate speech and\nmusic [Kharitonov et al., 2023, Agostinelli et al., 2023, Donahue et al., 2023]. In the particular case\nof text-to-music [Agostinelli et al., 2023] or text-to-speech [Kharitonov et al., 2023], a Transformer\nmodel takes text tokens as inputs and generates audio tokens, such that text and audio vocabularies\ndo not interact with each other. Such models could naturally be converted into, respectively, music\ncaptioning and speech recognition systems by swapping their inputs and outputs. Following this\nobservation, combining text and audio vocabularies into a multimodal, single vocabulary would allow\nfor training a single model in both directions.\nIn this work, we introduce AudioPaLM, a multimodal generative model of speech and text. At the\nheart of AudioPaLM is a joint vocabulary that can represent speech and text with a limited number\nof discrete tokens which, combined with an elementary markup description of tasks, allows training\na single decoder-only model on a mixture of tasks that involve arbitrarily interleaved speech and\ntext. This includes speech recognition, text-to-speech synthesis, and speech-to-speech translation,\nunifying tasks that are traditionally solved by heterogeneous models into a single architecture and\ntraining run. Moreover, as the underlying architecture of AudioPaLM is a large Transformer model,\nwe can initialize its weights with those of a large language model pretrained on text which allow\nit to benefit from the linguistic and common sense knowledge of models such as PaLM [Chowdhery\net al., 2022] or PaLM 2 [Anil et al., 2023]. In particular, we show in Section 5.4.8 how the model\u2019s\ntranslation capability is derived from the translation capability of the underlying text model. The\ncontributions of this work are:\n\u2022 We present a unified speech-text LLM, capable of consuming and producing both speech\nand text, and leveraging the existing capabilities of PaLM Chowdhery et al. [2022] and\nPaLM-2 [Anil et al., 2023] coming from text-only pretraining.\n\u2022 This unified approach across modalities allows training AudioPaLM on a mixture of tasks\nsuch as Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and\nSpeech-to-Speech Translation (S2ST), achieving state of the art results on AST and S2ST\nbenchmarks, and competitive performance on ASR benchmarks.\n\u2022 Leveraging AudioLM\u2019s audio prompting [Borsos et al., 2022], our model performs S2ST\nwith voice transfer of unseen speakers, surpassing existing methods in terms of speech\nquality and voice preservation, as measured by both objective and subjective evaluations.\n\u2022 Our model exhibits zero-shot capabilities, performing AST with speech input/target language\ncombinations that were not seen in training.\nThe remainder of this paper is organized as follows: in Section 2 we discuss the relation to existing\nwork. In Section 3 we describe our method. In Section 4 we provide details about the data we use,\nand other technical details as a prelude to the experiments. In Section 5 we present our experimental\nresults including a series of ablations to determine the influence of various design choices. We\nconclude in Section 6.\n2\nRelated work\n2.1\nMultimodal fusion\nEncoder-based models are used to learn features which can be used for downstream tasks. By learning\njoint representations of both modalities together, the goal is that in addition to the learned features be-\ning richer than they would be with each modality treated separately, they are aligned with one another,\nimproving their performance when used for inter-modality tasks. Such approaches have been applied\nin audio [Chen et al., 2022c, Bapna et al., 2022, Zhang et al., 2023a] and in vision [Chen et al., 2020,\nGan et al., 2020, Fu et al., 2021] as well as combining both audio and video inputs [Shi et al., 2022].\nSimilar to BERT [Devlin et al., 2018], such encoders may be trained with a masked language model\nobjective for both the multimodal setting as in previously mentioned works and for the unimodal\nsetting [Baevski et al., 2020, Hsu et al., 2021, Chiu et al., 2022]. They may alternatively be trained in\na contrastive manner [Radford et al., 2021, Yuan et al., 2021, Yu et al., 2022a] resulting in separate\nencoders for each modality with each informed by the other due to the contrastive objective.\nA line of work on multimodal encoder-decoder models (also known as Vision Language Models in\nthe vision literature) has sought to fuse text-decoders with advances in non-text encoder models.\n2\nExamples include Flamingo [Alayrac et al., 2022] and PaLI [Chen et al., 2022b] in the vision domain,\nand Whisper [Radford et al., 2022] in the audio domain. The general idea of these approaches is to\ntake an audio or vision encoder and a text decoder and to combine them, either with adapter layers as\nin Flamingo and Whisper, or by merging via a separate encoder as in PaLI.\nBoth PaLI and Flamingo use pretrained components. The advantage of this is that individual\ncomponents can be frozen while finetuning the model on multimodal data (Whisper does not use a\npretrained encoder or decoder and so does not freeze individual components). The disadvantage is\nthat such models are constrained to only output text, since the decoder is text-only. In contrast, our\nproposed approach results in a decoder-only model which models sequences of arbitrary audio and\ntext tokens. This is similar to the approach taken by Wang et al. [2022] except that we use a single\ndecoder-only model and all audio seen by the model is tokenized, whereas Wang et al. [2022] use an\nencoder-decoder architecture and use continuous inputs and tokenized outputs for images.\n2.2\nGenerating audio with language models\nRecent work [Lakhotia et al., 2021, Wang et al., 2023] has explored generating speech by modeling\ndiscretized representations as target tokens of an autoregressive Transformer [Vaswani et al., 2017]\nnetwork. Such discrete tokens can be extracted from self-supervised speech representations [Oord\net al., 2018, Baevski et al., 2020, Hsu et al., 2021, Chung et al., 2021], modeling long-term patterns\nin audio sequences while providing limited reconstruction quality, or from a neural codec [Zeghidour\net al., 2021, D\u00e9fossez et al., 2022], providing high-fidelity reconstruction but with less temporal\nstructure. AudioLM [Borsos et al., 2022] addresses this dichotomy by introducing a hierarchical\napproach, where a first stage produces \u201csemantic\u201d tokens from a self-supervised w2v-BERT sys-\ntem [Chung et al., 2021], which a second stage then uses as conditioning to generate the \u201cacoustic\u201d\ntokens of a SoundStream [Zeghidour et al., 2021] neural codec. This joint modeling of semantic and\nacoustic tokens allows the model to learn linguistic structure from the syntactic to the lexical and\nphonetic levels from speech-only corpora, without any textual guidance, while generating realistic\nspeech from arbitrary speakers and in diverse acoustic conditions.\nSPEAR-TTS [Kharitonov et al., 2023] combines the decoder-only generator of AudioLM with a text\nencoder, such that the model can perform text-to-speech synthesis. By leveraging pretraining and\nbacktranslation [Sennrich et al., 2016], SPEAR-TTS can be trained with only 15 minutes of labeled\nspeech. The ability of this model to learn a mapping between text and semantic tokens in such a\nlow-data regime suggests that these representations are very close, yet the model\u2019s encoder-decoder\narchitecture specifically ingests text and outputs audio, such that both vocabulary of tokens (text\nand semantic) are disjoint and modeled separately. SpeechLM [Hassid et al., 2023] also exploits the\nsimilarity between text and semantic tokens by initializing a decoder-only audio generator with the\nweights of a pretrained text-based language model. While this allows some transfer of knowledge from\ntext-to-speech modeling, the resulting architecture is not multimodal: semantic tokens replace the text\nvocabulary \u2014rather than extending it\u2014 and the model is finetuned on speech-only data. AudioPaLM\nbridges these gaps and combines semantic tokens and text into an extended, multimodal set of tokens\nused interchangeably as inputs and outputs, such that text-only language model pretraining can be\nused to initialize a decoder-only model that can then be finetuned on a mixture of tasks that map\nfreely between speech and text (e.g. speech-to-text, text-to-speech or speech-to-speech).\n2.3\nSpeech-to-speech translation\nThe field of speech-to-speech translation (S2ST) focuses on converting spoken language from one\nlanguage to another, facilitating communication between individuals who speak different languages.\nConventional automatic speech-to-speech translation systems are typically composed of a cascade of\nthree components: automatic speech recognition (ASR), text-to-text machine translation (MT), and\ntext-to-speech (TTS) synthesis [Lavie et al., 1997, Wahlster, 2000, Nakamura et al., 2006]. However,\nthese cascade-based approaches primarily focus on the text and may overlook important aspects such\nas para-linguistic features, computational efficiency, compound errors, and the accurate handling of\nproper names, nouns, and non-verbal communication that do not require translation.\nDirect speech-to-speech translation systems [Jia et al., 2019b, Kano et al., 2021, Jia et al., 2022b] are\ntrained end-to-end operating on the audio spectrogram domain without relying on text representation\nat inference time. In these systems, the synthesized audio has access to acoustic information in\n3\nText \nEmbeddings \nMatrix\nAudio \nEmbeddings \nMatrix\n[S2ST French English]  \n    \nDecoder-only \nTransformer\naudio tokens\ntext tokens\npre-trained on text-only data\n- -  \nSoundStorm \nor AudioLM \nstages 2 + 3\nAudio & text \ntokenizers\n[ASR Italian] \n    \nText \ndetokenizer\nCiao\nmondo!\nFigure 1: The AudioPaLM model, illustrated on speech-to-speech translation and automatic speech\nrecognition. We take a pretrained text-only model (dashed lines) and expand its embeddings matrix to\nmodel a new set of audio tokens. The model architecture is otherwise unchanged; a mixed sequence\nof text and audio tokens is fed as input and the model decodes text or audio tokens. Audio tokens are\nconverted back to raw audio with the latter AudioLM stages or SoundStorm (see Section 3.3).\nsource speech and can potentially learn to preserve acoustic features and reduce compound errors\nand computational requirements.\nThere are other cascaded S2ST systems that utilize learned discrete speech representations as an\nintermediate representation [Tjandra et al., 2019, Zhang et al., 2021, Lee et al., 2022, Ma et al., 2021,\nLee et al., 2021]. In these systems the translation operates in learned discrete representation space\nallowing to learn alignment in the discrete domain, and simplify leveraging of text pre-training. Lastly,\nthere are other S2ST approaches that improve on performance, efficiency, and data requirements. Jia\net al. [2022a] and Wei et al. [2022b] leveraged weakly supervised data and component pre-training to\nimprove translation accuracy while requiring little parallel speech data.\n3\nMethod\nWe use a decoder-only Transformer to model sequences consisting of text and audio tokens. As far\nas the model is concerned, text and audio are just sequences of arbitrary integers, as the inputs are\ntokenized before feeding to the model, and any outputs are detokenized before being returned to a\nuser of the model. By representing speech with discrete tokens in a finite vocabulary, we can build\na multimodal vocabulary which is the union of this audio vocabulary and a SentencePiece [Kudo\nand Richardson, 2018b] one used to represent text. Thus, in principle there is almost no difference\nbetween our setting and the usual decoder-only setup for pure text, except that in our setting some of\nthe tokens represent audio and some text, and we initialize our multimodal model using a pretrained\ntext-only checkpoint.\nThe overall model is described in Figure 1. In the rest of this section we describe the main steps of\nthe model: first, how text and audio inputs are tokenized; second, how we modify existing pretrained\ntext decoders to also model audio; and third, how we convert the model output into raw audio. Since\nthe first and third steps are identical to the process used by Borsos et al. [2022] and [Borsos et al.,\n2023], we keep our explanation of these points high-level and refer the reader to those papers for\nfurther details.\nFinally, we describe how we finetune AudioPaLM on a mixture of combined speech and text tasks\nincluding speech recognition and translation from or into either speech or text.\n3.1\nAudio Embeddings and Tokenization\nWe follow the process of Lakhotia et al. [2021], Borsos et al. [2022] to convert raw waveforms\ninto tokens. This involves extracting embeddings from an existing speech representation model and\nsubsequently discretizing those embeddings into a limited set of audio tokens. Borsos et al. [2022]\nextract embeddings from the w2v-BERT model [Chung et al., 2021] and quantize them via k-means.\nIn this work, we experiment with the following approaches to obtain a set of discrete audio tokens.\n4\n\u2022 w2v-BERT: We follow the procedure described in Borsos et al. [2022] with two modifi-\ncations. First, we use a w2v-BERT model that has been trained on multilingual data, as\nopposed to the English-only setting of Borsos et al. [2022]. Second, we do not normalize the\nembeddings before performing the k-means clustering. While Borsos et al. [2022] found that\nthe normalization removed speaker-identity information without degrading performance,\nwe found in the multilingual setting that normalization did indeed cause degradation. This\nmethod produces tokens at a rate of 25Hz and the token vocabulary is of size 1024.\n\u2022 USM-v1: We perform the same procedure with the more performant Universal Speech\nModel (USM) encoder [Zhang et al., 2023a] instead of the w2v-BERT encoder. We use the\nlargest 2B parameter variant of this multilingual speech encoder and extract embeddings\nfrom the middle layer. Similar to w2v-BERT, this method produces tokens at a rate of 25Hz\nand the token vocabulary is of size 1024.\n\u2022 USM-v2 : We additionally experiment with a quantizer that is trained with an auxiliary ASR\nloss. This version has been finetuned further to provide better multilingual performance. As\nwith USM-v1, this method accepts raw audio as input and returns a sequence of integers\nwith length proportional to the length of the audio as output.\n3.2\nModifying text-only decoders to model both text and audio\nIn a Transformer decoder, the first layer of the model after input preprocessing is the token embeddings\nmatrix E which maps integer-valued tokens to dense embeddings; given a vocabulary of t tokens\nand embeddings of size m, E is a t \u00d7 m matrix whose ith row gives the embedding for the ith token.\nAnother embeddings matrix E\u2032 appears in the final softmax layer used to compute the logits over all\ntokens at each position; it is a m \u00d7 t matrix which is multiplied with the m-dimensional output of the\nmodel to obtain a t dimensional vector of logits, one for each of the tokens. In the PaLM architecture,\nthese matrices have shared variables, so that one is the transpose of the other, that is, E\u2032 = E\u22ba.\nThe rest of the decoder architecture is completely agnostic to the number of tokens modelled.\nTherefore we only need to make one small modification to turn a text-only model into one that models\nboth text and audio: we expand the size of the embeddings matrix E to be of size (t + a) \u00d7 m where\na is the number of audio tokens (the size of E\u2032 = E\u22ba changes accordingly).\nIn order to make use of pretrained text models, we change the existing model checkpoints by adding\na new rows to the embeddings matrix E. An implementation detail is that the first t tokens (from zero\nto t) correspond to the SentencePiece text tokens while the next a tokens (from t to t + a) represent\naudio tokens. While we can re-use the text embeddings of the pre-trained model, the new audio\nembeddings are freshly initialized and must be trained. We found it necessary to train all model\nparameters rather than keeping the previous weights fixed. We train using mixed speech and text\ntasks, as detailed in Section 4. In Section 5.4.2 we show how adding audio tokens to a text-pretrained\ncheckpoint in the above manner is highly beneficial for performance on the considered speech and\ntext tasks (compared to re-training from scratch). For further details about the PaLM architecture we\nrefer the reader to Section 2 of [Chowdhery et al., 2022].\n3.3\nDecoding audio tokens to raw audio\nTo synthesize an audio waveform from audio tokens, we experimented with two different methods:\ni) autoregressive decoding, following the setup of AudioLM [Borsos et al., 2022] and ii) non-\nautoregressive decoding, using the recently proposed SoundStorm model [Borsos et al., 2023]. In\nboth cases the audio tokens are first used to generate SoundStream tokens [Zeghidour et al., 2021],\nwhich are then converted to an audio waveform with a convolutional decoder.\nThe acoustic generation in AudioLM proceeds in two stages: \u201cStage 2\u201d is a decoder-only Transformer\nmodel that takes the audio tokens produced by AudioPaLM and a voice conditioning as input, and\ngenerates SoundStream tokens that can be used to materialize the speech in the desired voice, but at a\nvery low bitrate. \u201cStage 3\u201d reconstructs higher levels of SoundStream\u2019s residual vector quantizer,\nwhich increases the bitrate and improves the audio quality. We use the same hyperparameters and the\ntraining process as in [Kharitonov et al., 2023].\nSoundStorm proposes an alternative non-autoregressive decoding scheme, which applies an iterative\nmethod that proceeds in parallel on all tokens. SoundStorm produces audio of the same quality as\n5\nAudioLM, but with higher consistency in voice and acoustic conditions, while being two orders of\nmagnitude faster.\nIn both cases we train on Multilingual LibriSpeech [Pratap et al., 2020] and the voice conditioning is\nsupplied as a 3-second long voice sample, represented as both audio tokens and SoundStream tokens.\nBy providing part of the original input speech as the voice conditioning, the model is able to preserve\nthe original speaker\u2019s voice when translating their speech to a different language (see Section 5).\nWhenever the original audio is shorter than 3 seconds, it is repeated to reach the required duration.\n3.4\nTraining tasks\nTypes of tasks\nWe apply our method to the problems of speech recognition, speech synthesis and\nspeech-to-speech translation. All datasets used in this report are speech-text datasets which contain a\nsubset of the following fields.\n\u2022 Audio: speech in the source language.\n\u2022 Transcript: a transcript of the speech in Audio.\n\u2022 Translated audio: the spoken translation of the speech in Audio.\n\u2022 Translated transcript: the written translation of the speech in Audio.\nThe component tasks that we consider in this report are:\n\u2022 ASR (automatic speech recognition): transcribing the audio to obtain the transcript.\n\u2022 AST (automatic speech translation): translating the audio to obtain the translated transcript.\n\u2022 S2ST (speech-to-speech translation): translating the audio to obtain the translated audio.\n\u2022 TTS (text-to-speech): reading out the transcription to obtain the audio.\n\u2022 MT (text-to-text machine translation): translating the transcript to obtain the translated\ntranscript.\nA dataset including more than two of the fields may be used for multiple possible tasks. As explored\nin the experiment of Section 5.4.1, we found that including multiple tasks (for example, both ASR\nand AST) from the same dataset resulted in improved performance.\nExpressing tasks\nFollowing Raffel et al. [2020], we signal to the model which task it should\nperform on a given input by prefixing the input with a tag specifying the task and the English name\nof the language of the input and, optionally, the language of the output if it is different.\nFor example, to query the model to perform ASR on an utterance in French, the tokenized audio\ninput would be preceded by the tag [ASR French]. To perform TTS in English, the text would\nbe preceded by [TTS English]. To perform S2ST from English to French, the tokenized English\naudio would be preceded by [S2ST English French]. The tag is tokenized using the normal text\ntokenizer of the model; we do not introduce special tokens to express the task or the languages\ninvolved. We found that changing task names to be more human-readable, such as using transcribe\nthe following French audio instead of [ASR French], does not change the performance of\nthe model. Naming the language in the task \u2013 compared to just using generic tags like transcribe\naudio or [ASR] \u2013 is not ultimately required but is beneficial for low-resource languages.\nCombined tasks\nWe consider both direct tasks, where the model is expected to directly map from\ninput to output, and combined tasks, where we instruct the model to also output intermediate steps\nfor a complex task. This is similar in spirit to chain of thought prompting [Wei et al., 2022a].\nFor example, for S2ST we could demand that the model directly maps from English audio tokens\nto French audio tokens. This would be expressed with the task tag [S2ST English French].\nAlternatively we can train the model to first output English text, followed by French text, and finally\nFrench audio tokens. We express this with the task tag [ASR AST S2ST English French]. The\nmodel performs this task as a single autoregressive decoding, i.e. it is not performed with multiple\nseparate calls to the model for each task. In particular this means that the model can attend to the\ninput and all prior decoded content at each stage, as opposed to a separated pipeline approach of\ndoing ASR, MT and then TTS.\n6\nWe found combined tasks to improve performance, which we explore in the experiment of Sec-\ntion 5.4.4.\n3.5\nTraining mixtures\nIn this section we describe the data mixtures used to train our best models based on the datasets listed\nin Table 1. Mixtures were implemented using the SeqIO library [Roberts et al., 2022]. More details\non the datasets can be found in Section 4.\nThere are two mixtures: one used to train the Audio PaLM 8B AST and AudioPaLM-2 8B AST models\nwhich output text and are trained on ASR and AST tasks; the other used to train the Audio PaLM 8B\nS2ST model which outputs both text and speech and additionally includes TTS and S2ST tasks.\n\u2022 The AST mixture is composed of:\n\u2013 The ASR tasks from the CVSS, VoxPopuli ASR, CommonVoice 11, Conversational\nEsEn and Youtube ASR datasets. For the CVSS and Conversational EsEn datasets, we\nuse ASR in both source and target languages.\n\u2013 The AST tasks from CVSS, Conversational EsEn and VoxPopuli S2ST. We use Vox-\nPopuli S2ST for AST by mapping from the translated audio to the transcript, since the\ntranslated transcript is not available.\n\u2013 The combined AST + ASR task for the CVSS and Conversational EsEn datasets.\n\u2013 The MT task from the WMT/TED dataset.\n\u2022 The S2ST mixture is composed of the above, plus additionally:\n\u2013 The TTS tasks from the CVSS and VoxPopuli ASR datasets. For CVSS we use only\nthe source transcript and audio.\n\u2013 The S2ST tasks from the Vox Populi S2ST, CVSS, WMT/TED and PaLM MT TTS\ndatasets. Note that except for VoxPopuli S2ST, the speech targets of these datasets\nare all synthetically generated. For VoxPopuli S2ST we perform translation from both\nsource to target, and target to source.\n\u2013 The combined ASR + AST + S2ST tasks from the Conversational EsEn, CVSS and\nWMT/TED datasets.\nIn general the components of the mixture are weighted according to the number of elements in each\ncomponent while we downweighted larger datasets; Table 1 lists the amounts of audio that models\ntrained on the above mixtures have seen during training.\n3.6\nTraining setup\nIn all experiments, we use the same finetuning setup as described in Section 6.1.2 of [Chowdhery\net al., 2022]. In particular, we finetune with the Adafactor optimizer with a constant learning rate of\n5 \u00d7 10\u22125 and dropout rate of 0.1, and we use loss masking on the inputs.\n4\nData and Metrics\n4.1\nDatasets\nTable 1 lists the datasets used in AudioPaLM training.\n\u2022 CoVoST2 [Wang et al., 2020] is a speech-to-text dataset mapping speech in 21 languages to\nEnglish text.\n\u2022 CVSS [Jia et al., 2022c] augmented CoVoST2 to synthesize speech for the target text\nin two flavors: CVSS-C uses a canonical speakers voice, while CVSS-T transfers voice\nproperties from the source voice. Unless stated otherwise, we use the CVSS-C flavor in\nspeech-to-speech translation experiments.\n\u2022 VoxPopuli [Wang et al., 2021] contains speeches from the European Parliament together\nwith their transcripts \u2013 which can be used for speech recognition (ASR) tasks \u2013 as well\nas spoken translations from parliamentary interpreters \u2013 which can be used for speech\ntranslation (S2ST) tasks.\n7\nTable 1: Datasets used for training AudioPaLM. The number of training hours corresponds to the\nnumber of hours seen by the AudioPaLM AST, AudioPaLM-2 AST and AudioPaLM S2ST models\nduring training, as a result of datasets balancing and a finite number of training steps.\nName\nAudio\nTranscript\nTranslated\nTranslated\n# Hours of training audio\n# Languages\naudio\ntranscript\nAudioPaLM AST\nAudioPaLM-2 AST\nAudioPaLM S2TS\nCoVoST2 / CVSS\n\u2713\n\u2713\n\u2713\n\u2713\n0.9k\n0.9k\n1.4k\n21 pairs X \u2192 En\nVoxPopuli ASR\n\u2713\n\u2713\n-\n-\n1.6k\n1.6k\n1.6k\n14\nVoxPopuli S2ST\n\u2713\n\u2713\n\u2713\n-\n6.7k\n3.9k\n5.4k\n15\nCommonVoice 11\n\u2713\n\u2713\n-\n-\n3.4k\n2k\n0.9K\n98\nConversational EsEn\n\u2713\n\u2713\n\u2713\n\u2713\n2k\n2k\n2k\n2\nYouTube ASR\n\u2713\n\u2713\n-\n-\n13k\n7.6k\n-\n56\nWMT/TED TTS\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n26.2k\n21 pairs X \u2192 En\nPaLM MT TTS\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n10.9k\n113 pairs X \u2192 En\n\u2022 Common Voice [Ardila et al., 2020] consists of text paired with recordings where people\nwere asked to read the text aloud.\n\u2022 The conversational dataset described in [Jia et al., 2019a] was obtained by crowd-sourcing\nhumans to read a subset of the Spanish side of a proprietary conversational Spanish-English\nMT dataset.\n\u2022 YouTube ASR is an unlabeled multilingual dataset of YouTube-based audio which was\ntranscribed automatically by using the USM-2B ASR model [Zhang et al., 2023a]. The\ndataset helps to improve models for YouTube to perform better on captioning and translation.\n\u2022 WMT/TED TTS is based on WMT [Barrault et al., 2020, 2019, Bojar et al., 2018, 2017,\n2015, 2013] and TED [Qi et al., 2018] text-to-text translation datasets as described in [Bapna\net al., 2022]. Following Jia et al. [2022a] the dataset is augmented by running all the source\nand target text through a TTS engine to generate synthetic paired audio.\n\u2022 PaLM MT TTS provides additional training data for S2ST: We use PaLM-2 to translate\nthe transcripts of the YouTube, Common Voice, and Babel [Gales et al., 2017] datasets\nto English, and use a prior AudioPaLM 8B S2ST model (trained without this dataset) to\nsynthesize the speech. The method is similar in spirit to [Jia et al., 2019a] which combines\nMT and TTS to generate additional paired data.\nWe train models on mixtures based on these datasets as described in Section 3.5 on ASR, AST, and\nS2ST tasks from the above datasets. In Section 5.4.6 we explore how adding more data improves the\nperformance of our method.\nNote that our method makes use of the text-pretrained PaLM checkpoints and audio tokenizers. So\nwhile the models are trained on the datasets listed in Table 1, they can also benefit from PaLM\u2019s text\ntraining data [Anil et al., 2023] via the pre-trained PaLM checkpoint, and from the data used to train\nthe audio tokenizers.\n4.2\nEvaluation Metrics\nWe evaluate our method on the following benchmarks:\n\u2022 CoVoST2 AST: We use BLEU scores, with the SacreBLEU corpusBLEU implementation\nPapineni et al. [2002], Post [2018]. We do not perform any normalization to the text before\ncomputing BLEU.\n\u2022 FLEURS AST: The FLEURS [Conneau et al., 2023] dataset contains speech utterances and\ntheir corresponding transcripts in 102 languages and is used for evaluation, only. We use\nBLEU scores, as described for CoVoST2 AST.\n\u2022 VoxPopuli ASR: We use the JiWER implementation of word error rate (WER). We normalise\nthe text by ignoring capitalisation and punctuation before computing the WER.\n\u2022 CoVoST2 ASR: Comparable to VoxPopuli ASR, but for Japanese and Chinese, the character\nerror rate (CER) is reported instead of WER. We report this metric for experiments trained\non CoVoST2, only.\n8\nTable 2: Top level results of this paper.\nModel\nCoVoST2 AST\nCVSS S2ST\nVoxPopuli ASR\nBLEU\u2191\nASR-BLEU\u2191\nWER\u2193\nWhisper Large-v2 1.5B [Radford et al., 2022]\n29.1\n\u2212\n13.6\nmSLAM-CTC 2B [Bapna et al., 2022]\n25.2\n\u2212\n9.1\nMAESTRO 600M [Chen et al., 2022c]\n25.2\n\u2212\n8.1\nUSM-M [Zhang et al., 2023a]\n30.7\n\u2212\n\u2212\nTranslatotron 2 + pretraining\n\u2212\n25.6\n\u2212\n+ TTS aug [Jia et al., 2022a]\nAudioPaLM 8B AST (ours)\n35.4\n\u2212\n11.1\nAudioPaLM 8B S2ST (ours)\n36.2\n32.5\n16.0\nAudioPaLM-2 8B AST (ours)\n37.8\n\u2212\n9.8\nAudioPaLM-2 8B cascaded ASR + transl. (ours)\n39.0\n\u2212\n\u2212\n\u2022 CVSS S2ST: Following Translatron 2 [Jia et al., 2022b], we feed the audio output of our\nmodel into an ASR model and use BLEU to compare the ASR output with the ground truth\ntarget text. We use the same ASR model as Jia et al. [2022b] and so the metrics presented\nhere are directly comparable.\nAll evaluations are performed on the test splits of the corresponding datasets.\n5\nExperiments\nWe start with our top-level results presenting significant improvements over prior results on automatic\nspeech-to-text translation (AST) and direct speech-to-speech translation (S2ST), as well as competi-\ntive results on automatic speech recognition (ASR). Ablations of individual factors are provided in\nSection 5.4.\n5.1\nSpeech translation and speech recognition results\nTable 2 displays results on ASR, AST and S2ST benchmarks for our method and existing baselines.\nOur models come in two variants; the first variant (referred to as AST) is trained on AST tasks without\nS2ST and TTS data; the second variant (referred to as S2ST) is trained with S2ST and TTS data\nand is therefore able to produce speech as well. To generate the audio for the S2ST results we used\nSoundStorm [Borsos et al., 2023]. For details on the training mixtures see Section 3.5.\nAs an initial checkpoint, we use a PaLM-2 8B checkpoint [Anil et al., 2023] to which we add the\ncapability to process audio tokens as input and output as described in Section 3.2. The additional\naudio token embeddings are initialized to 0. As in the original PaLM and PaLM 2 models, the input\nand output embeddings are shared.\nOur method exceeds the baselines on AST and S2ST and is competitive on ASR. Our method also\ncomes close in AST performance to a cascaded approach in which we use our best AudioPaLM-2\nASR model followed by translation with another AudioPaLM-2 model finetuned only for text-to-text\ntranslation on CoVoST2.\n5.2\nZero-shot behaviour\nSetup.\nWe evaluate the zero-shot capabilities of our AST models on the FLEURS multilingual\ndataset [Conneau et al., 2023]. The dataset contains speech utterances and their corresponding\ntranscripts in 102 languages. Note that none of our models were trained on FLEURS, so we use the\ndataset for evaluation only. In this context, we focus on the language pairs X \u2192 English and extract\ntwo subsets of languages:\n\u2022 29 AST-observed languages: languages for which speech-to-text translation (AST) data (X\n\u2192 En) was seen during training (as these language pairs were present in the VoxPopuli\n9\nTable 3: Zero-shot AST performance on the FLEURS dataset. We split the results into two groups:\nAST observed are languages for which X\u2192 En speech-to-text data was present in the AudioPaLM\ntraining data. Only ASR observed are languages for which only ASR data was present in the training\ndata (so the translation is done by the AudioPaLM model in a zero-shot manner). This shows that\nAudioPaLM inherits its translation capabilities from the base model, and is consistent with the\nimproved translation capabilities of PaLM-2 compared to PaLM. The hours of training data for\nAudioPaLM do not include audio data seen in self-supervised training of audio tokenization models.\n\u2217Whisper has seen AST data for all languages considered and is included here just for reference.\nModel\nAST observed languages\nOnly ASR observed languages\nBLEU\u2191\nAST / ASR hours\nBLEU\u2191\nAST / ASR hours\nWhisper Large-v2 1.5B [Radford et al., 2022]\n23.3\n74.0k / 104.4k\n19.6\u2217\n40.6k / 11.3k\nAudioPaLM 8B AST\n22.4\n6.6k / 11.4k\n10.0\n0 / 5.3k\nAudioPaLM-2 8B AST\n28.6\n4.8k / 8.2k\n20.7\n0 / 3.1k\nS2ST, CoVoST2 or/and Conversational EsEn datasets). These languages are indicated with\na \u00a7 in Table 17.\n\u2022 26 ASR-observed languages: languages for which no speech-to-text translation data was\nseen when training our AST models, but for which at least 1 hour of transcription (ASR)\ndata was present. We removed 3 languages (Cantonese, Kurdish and Ganda) for which\nwe did not have a BLEU score for the baseline. These languages are indicated with a \u2020 in\nTable 17.\nResults.\nIn Table 3, we present the results obtained with the two proposed AST models AudioPaLM\nand AudioPaLM-2, as well as the baseline model \u201cWhisper Large-v2 1.5B\u201d. We also present the\nnumber of AST and ASR speech training hours for these three models. For the proposed models, the\nreported number of hours do not take into account the amount of speech used to train the tokenizers.\nDiscussion.\nWe observe that the proposed AudioPaLM-2 model significantly outperforms the\nWhisper model on AST-observed languages. Although Whisper is used as a reference for the only\nASR observed setting, its results are not zero-shot as Whisper has been trained on 40.6K hours of\nspeech-to-text translation (AST) data for these languages. For the AudioPaLM models, this setting is\nzero-shot as it did not see any AST data for these languages. Despite this disadvantage, AudioPaLM-\n2 also outperforms the Whisper model on ASR-observed languages. For a detailed performance\ncomparison for each language, see Appendix D.\nThere is a large improvement obtained by using the AudioPaLM-2 instead of the AudioPaLM model:\n28% increase for AST-observed languages and 107% increase for ASR-observed languages. These\nnumbers show that the superior text translation capabilities of AudioPaLM-2 immediately transfer\nto the audio domain, despite the fact that the model has not seen any speech-to-text data for these\nlanguage pairs during training in the case of ASR-observed languages.\n5.3\nQuality of generated speech\nIn addition to measuring the translation quality of the speech content as reported in Table 2, we are\nalso interested in evaluating whether the speech generated by AudioPaLM is (a) of high quality, and\n(b) truthfully preserves the voice of the speaker when translating to a different language. To this end,\nwe use a combination of objective metrics and subjective evaluation studies that use the test split\nof the CVSS-T dataset [Jia et al., 2022c]. The subjective experiments were conducted on an earlier\nversion of AudioPaLM using the acoustic generation method described in AudioLM [Borsos et al.,\n2022].\nBaselines.\nAs a first baseline, we use the ground-truth translated utterances which are provided\nas a part of the CVSS-T dataset. These utterances were obtained by synthesizing the ground-truth\ntranslated text with a high-quality TTS system which was modified to enable voice transfer [Jia et al.,\n2021, 2022c]. As a result, the ground-truth utterances mimic the voice in the source utterance.\n10\nTable 4: Audio quality and voice similarity results. Subjective and objective audio quality results are\nreported on the 1...5 MOS scale. Objective voice similarity and acoustic consistency are measured in\nterms of cosine similarity. Subjective voice similarity scores span 1...5. Both objective and subjective\nmetrics are computed on the same set of examples. Higher is better across all metrics.\nAudio quality\nVoice similarity\nAcoustic consistency\nObjective\nSubjective\nObjective\nSubjective\nObjective\nCVSS-T\n3.41\n3.88\n0.24\n3.70\n0.54\nTranslatotron 2\n3.36\n3.96\n0.18\n3.51\n0.44\nAudioPaLM\n3.65\n4.44\n0.40\n4.00\n0.81\nAs a second baseline, we use Translatotron 2. Note that we could not use the \u201cTranslatotron 2 +\npretraining + TTS aug\u201d model (mentioned in Table 2) in the comparison because it was not trained\nto preserve voices and instead generates speech in a single canonical voice. Instead, we use the\nTranslatotron 2 system presented by Jia et al. [2022c] which is capable of transferring the voice\nfrom the source utterance (albeit it achieves a lower BLEU score on CVSS). This version of the\nTranslatotron 2 model was trained on the CVSS-T dataset and implements S2ST from 21 languages\nto English.\nObjective metrics.\nAs the first objective metric, we use a no-reference MOS estimator akin\nto Reddy et al. [2021] which, given an audio sample, provides an estimate of the perceived audio qual-\nity on a scale from 1 to 5. To measure cross-lingual voice transfer quality, we rely on an off-the-shelf\nspeaker verification model [Chen et al., 2022a] as used by Zhang et al. [2023b] and Kharitonov et al.\n[2023], and compute the cosine similarity between the embeddings of the source (encoded/decoded\nwith SoundStream) and the translated speech. Besides voice preservation, we also measure how\nwell the acoustic properties (recording conditions, background noise) are transferred from the source\naudio to the target. We do so by computing the cosine similarity between embeddings extracted\nfrom a model trained to identify segments that belong to the same recording [Borsos et al., 2023].\nSubjective evaluation.\nWe run two separate studies, one for evaluating the quality of the generated\nspeech, and another for assessing the voice similarity. We use the same set of samples for both\nstudies. Since utterances in CVSS-T are sourced from volunteer-generated data of variable quality,\nwe noticed that some of the utterances contain loud overlapping speech (e.g., a TV show or a song\nplaying in the background) or extremely strong noise (e.g., clothes rubbing against the microphone).\nSuch aberrations complicate the work of raters, thus we decided to pre-filter by only selecting inputs\nwith an estimated MOS of at least 3.0. Finally, we sampled 10 examples per language, giving us\n21 \u00d7 10 = 210 source utterances to translate. All utterances were peak normalised and resampled to\n16kHz, if needed.\nBefore starting, the raters were provided with a small set of illustrative examples with ground-truth\ngrades. They also completed a small pilot study as a training. The utterances (pairs of source-target\nutterances, in the case of the voice similarity evaluation) were presented one-by-one. The ratings are\nprovided on a 5-grade scale from 1 (poor quality or completely different voices) to 5 (excellent quality,\nidentical voices). In the voice similarity study, the raters are explicitly asked to ignore differences\nin the recording conditions and language, and solely focus on the voice. Each of the 630 output\nexamples (10 inputs from each of 21 languages were generated with each of the 3 different systems)\nwas rated 10 times which results in 6300 ratings per study. Aggregating those ratings per system, we\nobtain mean opinion score (MOS) and similarity mean opinion score (SMOS).\nResults.\nWe report the results of the objective and subjective evaluations in Table 4. From these\nresults we observe that AudioPaLM significantly outperforms the baseline Translatotron 2 system\nboth in audio quality and in voice similarity, in objective and subjective measurements. Moreover,\nAudioPaLM has higher quality and better voice similarity than the ground-truth synthesized recordings\nin CVSS-T, with a relatively large gap in most of the metrics. Following Jia et al. [2022c], we also\ncompared the systems across high and low-resource groups (French, German, Spanish and Catalan\nvs. the rest) and found no significant variation of the metrics across these groups.\n11\nTable 5: Results from experiment 5.4.1 showing the impact of training with ASR data in addition to\nAST data. Adding ASR tasks to the training mix helps to improve performance on AST.\nTasks\nCoVoST2 AST\nBLEU\u2191\nAST only\n16.0\nAST & ASR\n18.5\n5.4\nImpact of model and data choices\nIn this section we walk the reader through experiments that guided us towards our final training recipe\nfrom initial early experimentation. These show the impact of individual factors and build on top of\none another until reaching the final setup described and analysed in the previous sections.\n5.4.1\nTraining on multiple tasks\nTo achieve the results in Section 5.1, we trained on multiple tasks based on the same underlying\ndata to improve performance. For example, the CoVoST2 data can be used for both ASR and AST\ntasks, and we observed that adding ASR tasks in training results in improved performance on AST\nbenchmarks, compared to training with the AST tasks alone. In this section we investigate the effect\nof this choice on model performance.\nSetup.\nWe train two models on the CoVoST2 dataset. All conditions are identical except that in\none experiment, we use only the AST data; in the other we train with both AST and ASR tasks. The\nbase models are the PaLM 8B checkpoint and we use the USM-v1 tokenizer. We evaluate on the\nCoVoST2 AST benchmark.\nResults.\nSee Table 5. We observe that adding ASR tasks into the dataset increases BLEU by 2.5\nfrom 16.0 to 18.5 on the CoVoST2 AST benchmark.\nDiscussion.\nAlthough ASR is not part of the evaluation task, adding ASR data helped improve\nperformance. Our hypothesis is that ASR tasks help the model to better connect its understanding of\nthe new audio input to its previous understanding of text. In subsequent experiments we include both\nASR and AST tasks when using the CoVoST2 training data.\n5.4.2\nTraining from scratch vs. finetuning\nThe results in Section 5.1 are based on finetuning a text-pretrained PaLM checkpoint. Here we\ninvestigate the effect of using such a model compared to starting training from scratch on the same\narchitecture.\nSetup.\nIn the 1B from-scratch and 8B from-scratch experiments we start with randomly initialised\nweights. In the 8B finetune experiment we start from the PaLM 8B checkpoint, which has been\nmodified by adding extra rows to the token embedding matrix for the audio tokens, which are\nrandomly initialised.\nAll three models are trained on CoVoST2 ASR and AST tasks.\nResults.\nSee Table 6. We observe that finetuning the PaLM 8B checkpoint achieves substantially\nhigher performance than training from scratch on CoVoST2 tasks for both ASR and AST. The\n1B-from-scratch experiment was added to determine whether a smaller model architecture would\nwork better than the 8B model when trained from scratch on CoVoST2; it does not.\nDiscussion.\nFinetuning a pretrained checkpoint substantially improves results. This is in some\nsense not surprising as the base model is very capable to begin with; nonetheless it is interesting that\nwith finetuning the model is able to adapt to completely new input stimulus, since the audio tokens\nare totally new embeddings that the model must learn to understand. Furthermore the audio tokens\nare very different from text: despite the low sampling rate, there is presumably still some redundancy\n12\nTable 6: Results from Experiment 5.4.2 showing that training from a pretrained checkpoint has a\nsubstantial positive effect on performance compared to training from scratch.\nInitial checkpoint\nCoVoST2 AST\nCoVoST2 ASR\nBLEU\u2191\nWER\u2193\nPaLM 1B from scratch\n6.5\n66.0\nPaLM 8B from scratch\n6.9\n63.3\nPaLM 8B finetuned\n18.4\n40.2\nTable 7: Results from Experiment 5.4.3 showing the impact of training with different types of tokens.\nPerformance is affected significantly by the choice of tokens.\nTokens\nCoVoST2 AST\nCoVoST2 ASR\nBLEU\u2191\nWER\u2193\nW2V-BERT\n15.2\n50.1\nUSM-v1\n18.5\n40.2\nUSM-v2\n26.9\n22.3\nin the data and the rate of samples is still much higher than text tokens \u2014 we estimate from the data\nthat at 25Hz, one text token corresponds to approximately 6-8 audio tokens.\n5.4.3\nDifferent tokenization schemes\nTo obtain the results in Section 5.1, we tokenized audio based on USM-v2. Here we investigate the\nimpact of the choice of tokenization scheme on the final results.\nSetup.\nWe train three models with all conditions identical except for the tokenization scheme\napplied to the audio. All models are trained using the PaLM 8B checkpoint. In each case we use\nthe CVSS datasets with ASR and AST tasks with the source audio preprocessed using different\ntokenizers. The three tokenizers used are the w2v-BERT, USM-v1 and USM-v2 tokenizers which\nwere discussed Section 3.1.\nResults.\nSee Table 7. We observe that the choice of tokenization scheme has a large impact on the\nperformance of the model. The fact that the USM encoder is more powerful than w2v-BERT indeed\ntranslates to an improvement in performance in our setting. The USM-v2 tokens perform even better,\nyielding substantially improved results.\nDiscussion.\nThe choice of tokenization scheme has a substantial effect on performance. This is not\nsurprising; the model only is exposed to the information captured by the tokenizer, and this may be in\na form which is easy or difficult for the model to process. Future work should consider tokenization\nof audio more carefully because this is still relatively immature as a research area.\n5.4.4\nTraining with combined tasks\nTo obtain the results in Section 5.1, we required the model to compute intermediate steps for complex\ntasks by combining multiple tasks into one, as described in Section 3.4. In the following we investigate\nthe impact of this choice.\nSetup.\nWe train pairs of models on the CoVoST2 AST dataset. Within a pair, the only change\nis that for one model we train with ASR and AST tasks, while for the other we also include the\ncombined task consisting of first doing ASR and then outputting the AST result. For the latter model,\nat evaluation time we report the result of doing the combined task from which we use only the final\noutput. We repeat this setup twice: once with the USM-v1 tokens, and once with the USM-v2 tokens.\nResults.\nSee Table 8. This shows that expressing the AST task as a combination of simpler tasks\nresults in improved performance on the AST task. At the same time, we see a small reduction in\nperformance on the ASR task.\n13\nTable 8: Results from Experiment 5.4.4 showing that defining complex tasks as combinations of\nsimpler tasks results in an improvement in performance on the AST task and a small reduction on the\nASR task.\nTokens\nTasks\nCoVoST2 AST\nCoVoST2 ASR\nBLEU\u2191\nWER\u2193\nUSM-v1\nDirect\n18.5\n40.2\nCombined\n22.1\n41.6\nUSM-v2\nDirect\n26.9\n22.3\nCombined\n30.5\n25.3\nTable 9: Results from Experiment 5.4.5 showing that training with S2ST data brings new capabilities\nbut results in a degradation of performance on AST and ASR tasks.\nTasks\nCoVoST2 AST\nCVSS S2ST\nCoVoST2 ASR\nBLEU\u2191\nASR-BLEU\u2191\nWER\u2193\nAST, ASR\n30.5\n\u2212\n25.3\nAST, ASR & S2ST\n27.8\n24.2\n27.1\nDiscussion.\nOur results are consistent with prior works which have observed that allowing the\nmodel to break down a complex task into easier pieces results in improved performance, relative to\nmaking the model directly output the answer [Wei et al., 2022a].\nAt the same time, we observe a reduction in performance on the ASR task. We hypothesize that this\nmay be a consequence of our checkpoint selection criterion, which was to select the checkpoint with\nthe best AST metric on the validation split. It may also be a consequence of the large change in the\ndata mixture resulting from this change.\nWe note that it may appear that combined tasks reduce the problem to a pipeline approach of separate\nASR and translation systems. However this is not the case, as the model can refer to all previous\ntokens at each step and is a single unified model. For example, when decoding the translated text,\nit is possible to refer to the input audio and any information contained in them. This is particularly\nimportant for the S2ST setting (see Experiment 5.4.5) where prosodic information may be present in\nthe input audio, which can be attended to while decoding output audio.\n5.4.5\nTraining with additional speech-to-speech tasks\nIn the following, we investigate the impact of adding speech-to-speech translation (S2ST) tasks to the\ntrained tasks.\nSetup.\nWe train two models using the CoVoST2 dataset. One model is only trained on the AST, ASR\nand combined AST tasks. The other model is additionally trained on S2ST as a direct and combined\ntask. Thus the difference between these two models is that in the S2ST the model additionally sees\ntasks in which it must output audio tokens, whereas for the other tasks (and all previous experiments)\nthe model only outputs text tokens.\nResults.\nSee Table 9. We observe that adding the S2ST task results in the new capability of being\nable to perform S2ST, but that this comes at the cost of a modest decrease in performance to both the\nAST BLEU score and ASR WER score when evaluating on the CoVoST2 test split.\nDiscussion.\nSince we use loss masking on the inputs for each training example, performing S2ST\nis fundamentally different from ASR or AST since the model must learn to emit audio tokens. For\nASR and AST, the model takes audio tokens as input, but the loss masking means that it doesn\u2019t need\nto learn to model these sequences of audio tokens. It is thus perhaps not surprising that this results in\na decrease in performance on the text-output tasks, since model capacity must be devoted to audio\nmodelling.\n14\nTable 10: Results for Experiment 5.4.6 showing that scaling the amount of training data improves\nperformance. Observe also that within each pair, adding S2ST tasks brings new capabilities, but\nat the expense of slight decrease in performance on AST and ASR tasks. \u201cPublic speech datasets\u201d\ncorresponds to CoVoST2/CVSS, Vox Populi, CommonVoice 11 and Conversational EsEn.\nDatasets\nTasks\nVoxP. ASR\nCoVoST2 ASR\nCoVoST2 AST\nCVSS S2ST\nWER\u2193\nWER\u2193\nBLEU\u2191\nASR-BLEU\u2191\nCoVoST2 / CVSS\nAST\n168.7\n25.3\n30.5\n\u2212\nS2ST\n166.3\n27.1\n27.8\n24.2\nPublic speech datasets\nAST\n9.0\n15.5\n33.1\n\u2212\nS2ST\n14.5\n19.4\n31.9\n27.0\nPublic speech datasets\nAST\n9.6\n13.8\n34.8\n\u2212\n+ YT\nS2ST\n14.5\n16.5\n32.3\n27.6\nPublic speech datasets\nAST\n11.1\n15.1\n35.4\n\u2212\n+ YT + WMT/TED\nS2ST\n15.4\n16.6\n33.8\n29.5\nPublic speech datasets\nS2ST\n16.0\n15.0\n36.2\n31.2\n+ PaLM MT TTS + WMT/TED\n5.4.6\nScaling the training data\nIn this section we investigate the impact of increasing the amount of training data.\nSetup.\nWe run this analysis on two types of models, both trained from a PaLM 8B checkpoint and\nwith USM-v2 tokens. The models \u201cAudioPaLM 8B AST\u201d are trained without the S2ST tasks, the\nmodels \u201cAudioPaLM 8B S2ST\u201d are trained with the S2ST tasks.\nWe train these two types of models with an increasing amount of data:\n\u2022 The CoVoST2 dataset only. For the S2ST model, we use the modified S2ST version of this\ndataset: CVSS.\n\u2022 All the public datasets described in Table 1, namely CoVoST2/CVSS, VoxPopuli AST,\nVoxPopuli S2ST, CommonVoice 11 and Conversational EsEn.\n\u2022 All the public datasets, as well as the YouTube ASR dataset.\n\u2022 All the public datasets, as well as the YouTube ASR dataset and the WMT/TED text-to-text\ntranslation dataset. For the S2ST models, we follow Jia et al. [2022a] and synthesise a paired\nS2ST dataset from this by using TTS on the examples in this dataset.\n\u2022 As above, but using the synthetic PaLM-based MT TTS dataset S2ST mixture instead of\nthe YouTube ASR dataset. For this dataset we used PaLM-2 to translate the transcripts of\nthe YouTube, Common Voice, and Babel datasets to English text, and then synthesized the\nEnglish speech to create a speech-to-speech dataset.\nResults.\nSee Table 10. We observe that training with increasing amounts of data yields a substantial\nimprovement. In particular, consistent with Experiment 5.4.1 we see that adding additional ASR\ndata helps on AST tasks. Consistent with Experiment 5.4.5 we observe that for each fixed dataset\nmixture for which we compare the AST and S2ST mixtures, including the S2ST tasks brings new\nS2ST capabilities at the cost of a modest reduction in performance on AST. All of the S2ST results in\nTable 10 use AudioLM stage 2 and 3 models [Borsos et al., 2022] to reconstruct the audio samples\nfrom audio tokens as discussed in Section 3.3.\nDiscussion.\nIt is unsurprising that scaling the amount of training data results in an improvement\nin performance. We observe that adding more data in some cases leads to a small reduction in\nperformance on the ASR tasks, though always an improvement on the AST tasks. Similar to\nExperiment 5.4.4, this may be a consequence of our checkpoint selection criterion, which is based on\nAST performance on the CVSS validation set.\n15\nTable 11: Results for Experiment 5.4.7 showing the impact on S2ST metrics of decoding from audio\ntokens to wave audio using AudioLM stage 2 and 3 models compared to SoundStorm.\nDecoder\nCVSS S2ST\nASR-BLEU\u2191\nAudioLM\n31.2\nSoundStorm\n32.5\nTable 12: Results for Experiment 5.4.8 showing impact of finetuning PaLM vs PaLM-2.\nDatasets\nCheckpoint\nVoxPopuli ASR\nCoVoST2 ASR\nCVSS AST\nWER\u2193\nWER\u2193\nBLEU\u2191\nPublic + YT\nPaLM\n9.6\n13.8\n34.8\nPaLM-2\n9.7\n17.4\n37.2\nPublic + YT + WMT/TED\nPaLM\n11.1\n15.1\n35.4\nPaLM-2\n9.8\n15.7\n37.8\n5.4.7\nDecoding with AudioLM vs SoundStorm\nIn this section we investigate the impact on S2ST metrics of decoding using AudioLM stage 2 and 3\nmodels vs SoundStorm.\nSetup.\nWe take the best AudioPaLM model from Section 5.4.6 trained with the mixture consisting\nof public, PaLM MT TTS and WMT/TED datasets. The previous experiment used AudioLM stage 2\nand 3 models to decode the audio tokens output by AudioPaLM to wave audio. We rerun this using a\nSoundStorm model instead, and measure the impact on the CVSS S2ST task.\nResults.\nSee Table 11. We observe a 1.3 BLEU point increase when using SoundStorm compared\nto AudioLM. This result corresponds to the S2ST model presented in Table 2 trained on the S2ST\nmixture described in 3.5.\nDiscussion\nThese observations are consistent with those reported in Borsos et al. [2023], which\nfound that compared to AudioLM, SoundStorm produces more intelligible speech when used to\ndecode semantic audio tokens. This was measured by how faithfully the resulting audio matches a\nground truth transcript when transcribed with an ASR system, which is similar to our setup.\n5.4.8\nImpact of using PaLM-2\nIn the following we explore the effect of using the PaLM-2 checkpoint vs the original PaLM model.\nPaLM-2 was trained with improved data and techniques compared to the original PaLM model, and\nwas explicitly trained with parallel translation data. We therefore aim to understand whether these\nimprovements translate to gains in speech tasks.\nSetup.\nWe focus on speech-to-text tasks and do not consider S2ST. We train two pairs of models\non the largest datasets considered in Section 5.4.6. For each dataset we train two models, one using\nthe PaLM 8B checkpoint and the other using the PaLM-2 8B checkpoint. Compared to the PaLM\nfinetuning experiments, the optimization hyperparameters differed: we used a dropout rate of 0.2 and\na learning rate schedule of linear ramp-up to 10\u22124 followed by exponential decay to 10\u22125.\nResults.\nSee Table 12. On the smaller mixture, we observe an improvement on the CoVoST2 AST\ntask, and a minor degradation on VoxPopuli ASR and a more significant degradation on CoVoST2\nASR. On the larger data mixture, we see that PaLM-2 exceeds PaLM on the Vox Populi ASR and\nCVSS AST tasks, and is slightly worse on CoVoST2 ASR. Our interpretation of these results is that\nthe improved ability of PaLM-2 to perform text translation leads to an improvement for AST. The\nimpact on ASR capabilities is mixed, where when using the full training mixture, PaLM 2 exhibits\nslightly worse ASR capabilities on CoVoST2 and slightly better ones on VoxPopuli ASR.\n16\nTable 13: Results for Experiment 5.4.9 showing impact of architecture scale when using PaLM-2\ncheckpoints on AST/ASR tasks.\nDatasets\nCheckpoint size\nVoxPopuli ASR\nCoVoST2 ASR\nCVSS AST\nWER\u2193\nWER\u2193\nBLEU\u2191\nPublic + YT\n128M\n15.9\n30.2\n16.6\n1B\n11.9\n21.5\n30.4\n8B\n9.7\n17.4\n37.2\nPublic + YT + WMT/TED\n128M\n16.4\n29.9\n18.3\n1B\n11.7\n17.3\n31.6\n8B\n9.8\n15.7\n37.8\nDiscussion.\nWhile we do see a difference, we suspect that the different capabilities between PaLM\nand PaLM-2 are not as important in this setting as they might be for purely text-based tasks, since the\naddition of tokenized audio is novel for both models.\n5.4.9\nImpact of architecture scale\nIn the following we investigate the impact of the model size on the downstream task performance.\nWe use PaLM-2 for this and focus on the ASR and AST settings.\nSetup.\nWe train three PaLM-2 models of different sizes (128M, 1B, and 8B) using USM-v2 tokens\nwith the same two largest datasets from Section 5.4.6 and observe their performance on our benchmark\nASR and AST tasks.\nResults.\nSee Table 13. We find that our results improve substantially with model size, with 42%\nand 28% reduction in WER for CVSS and VoxPopuli ASR tasks and over 13 points increase in BLEU\nscores for translation tasks respectively moving from 128M to 1B model on the full Public + YT +\nWMT/TED dataset. Increasing the model size further from 1B to 8B leads to additional gains of a\nfurther 10% and 16% reduction in WER for CVSS and VoxPopuli ASR tasks and a further 6.2 point\nimprovement in BLEU score. We find the scaling improvements also hold across different training\ndatasets (e.g., Public + YT compared with Public + YT + WMT/TED).\nDiscussion.\nAs expected, performance on downstream ASR/AST tasks improves with larger model\nsize. Our 1B sized model outperforms Whisper 1.5B Large model by over 5 BLEU points and 28%\nreduction in WER for VoxPopuli ASR.\n6\nConclusion\nWe introduce AudioPaLM, a large language model that can process and generate speech and text\ninterchangeably. AudioPaLM starts from a pre-trained text-based LLM and extends its vocabulary\nwith discrete audio tokens. In doing so, the model can leverage its existing text capabilities while\nbeing finetuned to also consume and produce tokenized audio on a mixture of speech-text tasks.\nMoreover, by expressing the different tasks with textual tags, a single model can be trained on all tasks\ntogether. AudioPaLM demonstrates state-of-the-art results on speech translation benchmarks and\ncompetitive performance on speech recognition tasks, as well as zero-shot speech-to-text translation\nabilities on unseen language pairs. AudioPaLM also benefits from features of audio language models,\nsuch as voice prompting, and can perform S2ST with voice transfer of a superior quality compared to\nexisting baselines, as measured by both automatic metrics and human raters.\nLimitations\nThe fact that our model can natively produce audio is a consequence of the fact that\nwe make use of tokenized audio. This introduces a strong dependency on the quality of the audio\ntokenizer, as demonstrated in Section 7. We additionally empirically found it necessary to finetune\nthe whole model, unlike a Flamingo-like [Alayrac et al., 2022] approach which freezes most of\nthe weights and thus provides guarantees on preservation of the original capabilities of the model\ncomponents.\n17\nOpen questions\nThere are numerous further avenues of research. One strand is around audio\ntokenization: what are desirable properties of audio tokens, how can we measure them, and how\ncan we optimize for them? Another is around evaluations. In comparison to text, the richness of\nthe set of established benchmarks for generative text/audio tasks is less developed. This work has\nfocused on speech recognition and speech translation, for which the benchmarks are more mature.\nThe establishment of more benchmarks and metrics for generative audio tasks will help to accelerate\nresearch further.\nAcknowledgements\nWe would like to thank Nobuyuki Morioka and Yifan Ding for their help in re-creating the TTS-\naugmented WMT/TED dataset which was also used in Jia et al. [2022a] and Adam Roberts and Ron\nWeiss for their advice and reviews. We would like to thank Slav Petrov, Colin Cherry and the PaLM-2\nteam for their advice and support.\nReferences\nA. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen,\nA. Roberts, M. Tagliasacchi, M. Sharifi, N. Zeghidour, and C. Frank. Musiclm: Generating music\nfrom text. arXiv preprint arXiv:2301.11325, 2023.\nJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716\u201323736, 2022.\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. T. Passos, S. Shakeri, E. Taropa, P. Bailey,\nZ. Chen, E. Chu, J. Clark, L. E. Shafey, Y. Huang, K. S. Meier-Hellstern, G. Mishra, E. Moreira,\nM. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. \u2019Abrego, J. Ahn,\nJ. Austin, P. Barham, J. A. Botha, J. Bradbury, S. Brahma, K. M. Brooks, M. Catasta, Y. Cheng,\nC. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Cr\u00e9py, S. Dave, M. Dehghani, S. Dev,\nJ. Devlin, M. C. D\u2019iaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garc\u00eda,\nS. Gehrmann, L. Gonz\u00e1lez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. R. Hu,\nJ. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. H. Jia, K. Kenealy, M. Krikun,\nS. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M.-L. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z.-Z.\nLiu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham,\nE. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif,\nB. Richter, P. Riley, A. Ros, A. Roy, B. Saeta, R. Samuel, R. M. Shelby, A. Slone, D. Smilkov, D. R.\nSo, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang, Z. Wang,\nT. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. W. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng,\nW. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report. arXiv preprint arXiv:2305.10403,\n2023.\nR. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Henretty, R. Morais, L. Saunders,\nF. Tyers, and G. Weber. Common voice: A massively-multilingual speech corpus. In Proceedings\nof the Twelfth Language Resources and Evaluation Conference, pages 4218\u20134222, Marseille,\nFrance, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL\nhttps://aclanthology.org/2020.lrec-1.520.\nA. Baevski, Y. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural information processing systems, 33:\n12449\u201312460, 2020.\nA. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Con-\nneau. mslam: Massively multilingual joint pre-training for speech and text. arXiv preprint\narXiv:2202.01374, 2022.\nL. Barrault, O. Bojar, M. R. Costa-juss\u00e0, C. Federmann, M. Fishel, Y. Graham, B. Haddow, M. Huck,\nP. Koehn, S. Malmasi, C. Monz, M. M\u00fcller, S. Pal, M. Post, and M. Zampieri. Findings of the\n2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference\non Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1\u201361. Association for\nComputational Linguistics, 2019. URL https://aclanthology.org/W19-5301.\n18\nL. Barrault, M. Biesialska, O. Bojar, M. R. Costa-juss\u00e0, C. Federmann, Y. Graham, R. Grundkiewicz,\nB. Haddow, M. Huck, E. Joanis, T. Kocmi, P. Koehn, C.-k. Lo, N. Ljube\u0161i\u00b4c, C. Monz, M. Morishita,\nM. Nagata, T. Nakazawa, S. Pal, M. Post, and M. Zampieri. Findings of the 2020 conference on\nmachine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation,\npages 1\u201355. Association for Computational Linguistics, 2020. URL https://aclanthology.\norg/2020.wmt-1.1.\nO. Bojar, C. Buck, C. Callison-Burch, C. Federmann, B. Haddow, P. Koehn, C. Monz, M. Post,\nR. Soricut, and L. Specia. Findings of the 2013 Workshop on Statistical Machine Translation. In\nProceedings of the Eighth Workshop on Statistical Machine Translation, pages 1\u201344. Association\nfor Computational Linguistics, 2013. URL https://aclanthology.org/W13-2201.\nO. Bojar, R. Chatterjee, C. Federmann, B. Haddow, M. Huck, C. Hokamp, P. Koehn, V. Logacheva,\nC. Monz, M. Negri, M. Post, C. Scarton, L. Specia, and M. Turchi.\nFindings of the 2015\nworkshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical\nMachine Translation, pages 1\u201346. Association for Computational Linguistics, 2015. URL https:\n//aclanthology.org/W15-3001.\nO. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow, S. Huang, M. Huck, P. Koehn, Q. Liu,\nV. Logacheva, C. Monz, M. Negri, M. Post, R. Rubino, L. Specia, and M. Turchi. Findings of\nthe 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference\non Machine Translation, pages 169\u2013214. Association for Computational Linguistics, 2017. URL\nhttps://aclanthology.org/W17-4717.\nO. Bojar, C. Federmann, M. Fishel, Y. Graham, B. Haddow, M. Huck, P. Koehn, and C. Monz.\nFindings of the 2018 conference on machine translation (WMT18). In Proceedings of the Third\nConference on Machine Translation: Shared Task Papers, pages 272\u2013303. Association for Compu-\ntational Linguistics, 2018. URL https://aclanthology.org/W18-6401.\nZ. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, O. Teboul, D. Grangier,\nM. Tagliasacchi, and N. Zeghidour. AudioLM: a language modeling approach to audio generation.\narXiv preprint arXiv:2209.03143, 2022.\nZ. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour, and M. Tagliasacchi. Soundstorm:\nEfficient parallel audio generation. arXiv preprint arXiv:2305.09636, 2023.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot\nlearners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.\nneurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu,\nL. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei. Wavlm: Large-scale\nself-supervised pre-training for full stack speech processing. IEEE J. Sel. Top. Signal Process., 16\n(6):1505\u20131518, 2022a. doi: 10.1109/JSTSP.2022.3188113. URL https://doi.org/10.1109/\nJSTSP.2022.3188113.\nX. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner,\nB. Mustafa, L. Beyer, et al. PaLI: A jointly-scaled multilingual language-image model. arXiv\npreprint arXiv:2209.06794, 2022b.\nY.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu. Uniter: Universal\nimage-text representation learning. In Computer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX, pages 104\u2013120. Springer, 2020.\nZ. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno, A. Bapna, and H. Zen. Maestro:\nMatched speech text representations through modality matching. arXiv preprint arXiv:2204.03409,\n2022c.\n19\nC.-C. Chiu, J. Qin, Y. Zhang, J. Yu, and Y. Wu. Self-supervised learning with random-projection\nquantizer for speech recognition. In International Conference on Machine Learning, pages\n3915\u20133924. PMLR, 2022.\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,\nC. Sutton, S. Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\nY.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu. W2V-Bert: Combining\ncontrastive learning and masked language modeling for self-supervised speech pre-training. In\nASRU, 2021.\nA. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna.\nFleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken\nLanguage Technology Workshop (SLT), pages 798\u2013805. IEEE, 2023.\nA. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression. CoRR,\nabs/2210.13438, 2022. doi: 10.48550/arXiv.2210.13438. URL https://doi.org/10.48550/\narXiv.2210.13438.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.\nBert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nC. Donahue, A. Caillon, A. Roberts, E. Manilow, P. Esling, A. Agostinelli, M. Verzetti, I. Simon,\nO. Pietquin, N. Zeghidour, and J. H. Engel. Singsong: Generating musical accompaniments\nfrom singing. CoRR, abs/2301.12662, 2023. doi: 10.48550/arXiv.2301.12662. URL https:\n//doi.org/10.48550/arXiv.2301.12662.\nT.-J. Fu, L. Li, Z. Gan, K. Lin, W. Y. Wang, L. Wang, and Z. Liu. Violet: End-to-end video-language\ntransformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.\nM. J. Gales, K. M. Knill, and A. Ragni. Low-resource speech recognition and keyword-spotting. In\nSpeech and Computer: 19th International Conference, SPECOM 2017, Hatfield, UK, September\n12-16, 2017, Proceedings 19, pages 3\u201319. Springer, 2017.\nZ. Gan, Y.-C. Chen, L. Li, C. Zhu, Y. Cheng, and J. Liu. Large-scale adversarial training for vision-\nand-language representation learning. Advances in Neural Information Processing Systems, 33:\n6616\u20136628, 2020.\nM. Hassid, T. Remez, T. A. Nguyen, I. Gat, A. Conneau, F. Kreuk, J. Copet, A. D\u00e9fossez, G. Synnaeve,\nE. Dupoux, R. Schwartz, and Y. Adi. Textually pretrained speech language models. arXiv preprint\narXiv:2305.13009, 2023.\nW.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. Hubert:\nSelf-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, 29:3451\u20133460, 2021.\nY. Jia, M. Johnson, W. Macherey, R. J. Weiss, Y. Cao, C.-C. Chiu, N. Ari, S. Laurenzo, and Y. Wu.\nLeveraging weakly supervised data to improve end-to-end speech-to-text translation. In Proc.\nICASSP, pages 7180\u20137184, 2019a.\nY. Jia, R. J. Weiss, F. Biadsy, W. Macherey, M. Johnson, Z. Chen, and Y. Wu. Direct speech-to-speech\ntranslation with a sequence-to-sequence model. In INTERSPEECH, 2019b.\nY. Jia, H. Zen, J. Shen, Y. Zhang, and Y. Wu. Png bert: Augmented bert on phonemes and graphemes\nfor neural tts. Proc. Interspeech 2021, pages 151\u2013155, 2021.\nY. Jia, Y. Ding, A. Bapna, C. Cherry, Y. Zhang, A. Conneau, and N. Morioka. Leveraging unsuper-\nvised and weakly-supervised data to improve direct speech-to-speech translation. arXiv preprint\narXiv:2203.13339, 2022a.\nY. Jia, M. T. Ramanovich, T. Remez, and R. Pomerantz. Translatotron 2: High-quality direct speech-\nto-speech translation with voice preservation. In International Conference on Machine Learning,\npages 10120\u201310134. PMLR, 2022b.\n20\nY. Jia, M. T. Ramanovich, Q. Wang, and H. Zen. Cvss corpus and massively multilingual speech-to-\nspeech translation. arXiv preprint arXiv:2201.03713, 2022c.\nT. Kano, S. Sakti, and S. Nakamura. Transformer-based direct speech-to-speech translation with\ntranscoder. In Proc. IEEE SLT, pages 958\u2013965, 2021.\nE. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi,\nand N. Zeghidour. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision.\narXiv preprint arXiv:2302.03540, 2023.\nF. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u00e9fossez, J. Copet, D. Parikh, Y. Taigman, and\nY. Adi. Audiogen: Textually guided audio generation. CoRR, abs/2209.15352, 2022. doi:\n10.48550/arXiv.2209.15352. URL https://doi.org/10.48550/arXiv.2209.15352.\nT. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018a.\nT. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. In E. Blanco and W. Lu, editors, Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System\nDemonstrations, Brussels, Belgium, October 31 - November 4, 2018, pages 66\u201371. Association\nfor Computational Linguistics, 2018b. doi: 10.18653/v1/d18-2012. URL https://doi.org/10.\n18653/v1/d18-2012.\nK. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet,\nA. Baevski, A. Mohamed, et al. On generative spoken language modeling from raw audio.\nTransactions of the Association for Computational Linguistics, 9:1336\u20131354, 2021.\nA. Lavie, A. Waibel, L. Levin, M. Finke, D. Gates, M. Gavalda, T. Zeppenfeld, and P. Zhan. JANUS-\nIII: Speech-to-speech translation in multiple languages. In ICASSP, 1997.\nA. Lee, H. Gong, P.-A. Duquenne, H. Schwenk, P.-J. Chen, C. Wang, S. Popuri, J. Pino, J. Gu, and\nW.-N. Hsu. Textless speech-to-speech translation on real data. arXiv preprint arXiv:2112.08352,\n2021.\nA. Lee, P.-J. Chen, C. Wang, J. Gu, X. Ma, A. Polyak, Y. Adi, Q. He, Y. Tang, J. Pino, and W.-N.\nHsu. Direct speech-to-speech translation with discrete units. In ACL, 2022.\nX. Ma, H. Gong, D. Liu, A. Lee, Y. Tang, P.-J. Chen, W.-N. Hsu, K. Heafield, P. Koehn, and J. Pino.\nDirect simultaneous speech to speech translation. arXiv preprint arXiv:2110.08250, 2021.\nS. Nakamura, K. Markov, H. Nakaiwa, G. Kikui, H. Kawai, T. Jitsuhiro, J.-S. Zhang, H. Yamamoto,\nE. Sumita, and S. Yamamoto. The ATR multilingual speech-to-speech translation system. IEEE\nTransactions on Audio, Speech, and Language Processing, 2006.\nA. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.\narXiv preprint arXiv:1807.03748, 2018.\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, pages 311\u2013318, 2002.\nM. Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186\u2013191, Belgium, Brussels, Oct. 2018. Association\nfor Computational Linguistics. URL https://www.aclweb.org/anthology/W18-6319.\nV. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert. Mls: A large-scale multilingual dataset\nfor speech research. arXiv preprint arXiv:2012.03411, 2020.\nY. Qi, D. Sachan, M. Felix, S. Padmanabhan, and G. Neubig. When and why are pre-trained word\nembeddings useful for neural machine translation? In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 529\u2013535. Association for Computational Linguistics,\n2018. URL https://aclanthology.org/N18-2084.\n21\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al.\nLearning transferable visual models from natural language supervision.\nIn\nInternational conference on machine learning, pages 8748\u20138763. PMLR, 2021.\nA. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech\nrecognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022.\nJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, H. F. Song, J. Aslanides, S. Henderson,\nR. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den\nDriessche, L. A. Hendricks, M. Rauh, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang,\nJ. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. M. Jayakumar,\nE. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L.\nLi, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau,\nM. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama,\nC. de Masson d\u2019Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas,\nA. Guy, C. Jones, J. Bradbury, M. J. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. Isaac,\nE. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett,\nD. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language models: Methods, analysis &\ninsights from training gopher. CoRR, abs/2112.11446, 2021. URL https://arxiv.org/abs/\n2112.11446.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a unified text-to-text transformer. The Journal of\nMachine Learning Research, 21(1):5485\u20135551, 2020.\nC. K. A. Reddy, V. Gopal, and R. Cutler. Dnsmos: A non-intrusive perceptual objective speech quality\nmetric to evaluate noise suppressors. In IEEE International Conference on Acoustics, Speech and\nSignal Processing (DNSMOS), 2021.\nA. Roberts, H. W. Chung, A. Levskaya, G. Mishra, J. Bradbury, D. Andor, S. Narang, B. Lester,\nC. Gaffney, A. Mohiuddin, C. Hawthorne, A. Lewkowycz, A. Salcianu, M. van Zee, J. Austin,\nS. Goodman, L. B. Soares, H. Hu, S. Tsvyashchenko, A. Chowdhery, J. Bastings, J. Bulian,\nX. Garcia, J. Ni, A. Chen, K. Kenealy, J. H. Clark, S. Lee, D. Garrette, J. Lee-Thorp, C. Raffel,\nN. Shazeer, M. Ritter, M. Bosma, A. Passos, J. Maitin-Shepard, N. Fiedel, M. Omernick, B. Saeta,\nR. Sepassi, A. Spiridonov, J. Newlan, and A. Gesmundo. Scaling up models and data with t5x\nand seqio, 2022.\nR. Sennrich, B. Haddow, and A. Birch. Improving neural machine translation models with monolin-\ngual data. In ACL, 2016.\nB. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed. Learning audio-visual speech representation by\nmasked multimodal cluster prediction, 2022.\nA. Tjandra, S. Sakti, and S. Nakamura. Speech-to-speech translation between untranscribed unknown\nlanguages. In Proc. IEEE ASRU, pages 593\u2013600, 2019.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), pages\n5998\u20136008, 2017.\nW. Wahlster. Verbmobil: Foundations of speech-to-speech translation. Springer, 2000.\nC. Wang, A. Wu, and J. M. Pino. Covost 2: A massively multilingual speech-to-text translation\ncorpus. CoRR, abs/2007.10310, 2020. URL https://arxiv.org/abs/2007.10310.\nC. Wang, M. Rivi\u00e8re, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux.\nVoxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised\nlearning and interpretation, 2021.\nC. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al. Neural\ncodec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111,\n2023.\n22\nP. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Ofa: Unifying\narchitectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In\nInternational Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022a.\nK. Wei, L. Zhou, Z. Zhang, L. Chen, S. Liu, L. He, J. Li, and F. Wei. Joint pre-training with speech\nand bilingual text for direct speech to speech translation. arXiv:2210.17027, 2022b.\nJ. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners\nare image-text foundation models. arXiv preprint arXiv:2205.01917, 2022a.\nJ. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan,\nB. Hutchinson, W. Han, Z. Parekh, X. Li, H. Zhang, J. Baldridge, and Y. Wu. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv:2206.10789, 2022b. doi: 10.48550/arXiv.\n2206.10789.\nL. Yuan, D. Chen, Y.-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu, X. Huang, B. Li, C. Li, et al.\nFlorence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.\nN. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. SoundStream: An end-to-end\nneural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:\n495\u2013507, 2021.\nC. Zhang, X. Tan, Y. Ren, T. Qin, K. Zhang, and T.-Y. Liu. UWSpeech: Speech to speech translation\nfor unwritten languages. In AAAI, 2021.\nY. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang,\net al. Google USM: Scaling automatic speech recognition beyond 100 languages. arXiv preprint\narXiv:2303.01037, 2023a.\nZ. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al. Speak\nforeign languages with your own voice: Cross-lingual neural codec language modeling. arXiv\npreprint arXiv:2303.03926, 2023b.\n23\nA\nAuthor Contributions\nPaul initiated the project, created the AudioPaLM model architecture and proved its viability with\na number of pre-training, speech recognition and translation tasks, onboarded the team to the\nproject, and contributed significantly to the write-up of this report. Chulayuth performed many audio\ntokenization experiments and completed the audio integration with PaLM2. Duc carried out the\nexperiments around combined tasks (performing both ASR and AST in the same task), synthesized\nthe WMT-derived speech-to-speech datasets and further developed the audio vocabulary. All of the\nabove contributed to the best-performing configuration for speech-to-speech translation. Paul and\nChristian coordinated the write-up of this report. Duc and Chulayuth ran a majority of the ablation\nexperiments.\nAnkur, Johan, Tara and Yu collaborated on the research and development of USM-v2 tokens which led\nto our best-performing configuration across tasks. Jiahui and Zhishuai developed the token learning\napproach for images and Jiahui advised on the development of USM-v2 tokens. James, Wei and\nYongqiang developed the large-scale tokenization and transcription infrastructure for USM models.\nYongqiang, Wei and F\u00e9lix curated the semi-supervised ASR datasets. Alexandru put in place data\nprocessing pipelines, improved our best mixture by adding a variety of ASR datasets, on AudioLM\nspeech generation models, and initially worked on the USM-v2 audio tokens together with Johan.\nDanny worked on speech-to-speech translation and the ASR-BLEU metric for S2ST models together\nwith Alexandru and Duc. Peter and Vicky worked on the PaLM2 integration and cascaded model\nbaselines. Dalia significantly improved the best configuration of this report by adding TTS tasks and\ntext-to-text and synthetic speech-to-speech datasets to the model\u2019s task mixtures.\nEugene, Damien, Mihajlo, and Neil worked on AudioLM speech quality and in particular on making\nthe translated voice consistent with the source voice and providing objective metrics. Mihajlo\ncoordinated this effort, trained speech generation models, and created the website together with\nHannah. Hannah further tuned the best models for the paper, analysed the zero-shot capabilities of\nthe models, managed the rating tasks for subjective speech quality metrics, and performed a detailed\ntraining data analysis.\nNeil contributed significantly to the writing of this report. Marco identified the opportunity to leverage\nAudioLM for speech-to-speech translation and Zal\u00e1n trained the very first such model. Zal\u00e1n, Neil,\nand Marco provided guidance around AudioLM details and project planning. Michelle provided\nguidance on speech-to-speech baselines, Translatotron, and other related work. Lukas, Dirk, Matt\nand Johan supported and advised the team throughout the project. Christian initiated the project,\ncoordinated the effort, and contributed with core ideas and technical work.\n24\nB\nDetailed results of AST models performance\nTable 14: BLEU scores on CoVoST2.\nModel\nArabic (ar)\nCatalan (ca)\nWelsh (cy)\nGerman (de)\nSpanish (es)\nEstonian (et)\nPersian (fa)\nFrench (fr)\nIndonesian (id)\nItalian (it)\nJapanese (ja)\nLatvian (lv)\nMongolian (mn)\nDutch (nl)\nPortuguese (pt)\nRussian (ru)\nSlovenian (sl)\nSwedish (sv)\nTamil (ta)\nTurkish (tr)\nChinese (zh)\nAll\nWhisper 1.5B [Radford et al., 2022]\n39.7 31.8 21.5 36.3 40.1 15.0 19.3 36.4 48.1 30.9 26.1 13.9 0.1 41.2 51.6 43.3 21.6 42.9 4.2 28.3 18.0 29.1\nmSLAM-CTC 2B [Bapna et al., 2022] 19.3 35.4 6.7 35.9 41.0 22.6 9.7 39.0 8.8 37.3 3.3 26.8 0.8 37.6 42.8 48.4 32.3 38.5 0.6 24.2 10.0 25.2\nAudioPaLM 8B AST\n45.1 37.9 15.5 42.4 44.9 23.7 25.5 44.1 52.0 43.6 21.4 28.1 4.3 45.5 56.5 52.8 39.3 53.0 4.2 38.9 23.7 35.4\nAudioPaLM 8B S2ST\n45.5 36.4 19.4 41.4 43.4 27.2 28.4 43.2 54.3 42.9 24.4 33.3 5.8 43.4 55.5 54.3 41.8 53.8 6.9 37.5 21.4 36.2\nAudioPaLM-2 8B AST\n48.7 38.4 13.7 43.4 44.2 30.0 29.4 44.8 56.2 44.3 25.9 35.0 7.6 48.3 57.3 55.6 42.6 53.3 9.0 41.0 25.5 37.8\nTable 15: WER (%) on Vox Populi.\nModel\nCzech (cs)\nGerman (de)\nEnglish (en)\nSpanish (es)\nEstonian (et)\nFinnish (fi)\nFrench (fr)\nCroatian (hr)\nHungarian (hu)\nItalian (it)\nLithuanian (lt)\nDutch (nl)\nPolish (pl)\nRomanian (ro)\nSlovak (sk)\nSlovenian (sl)\nAll\nWhisper 1.5B [Radford et al., 2022]\n12.6 11.2 7.0 18.6 28.7 12.4 11.4 16.1 13.8 19.0 33.2 12.9 7.8 14.4 15.4 27.9 13.6\nmSLAM-CTC 2B [Bapna et al., 2022]\n6.8\n8.7\n7.0\n8.4\n-\n8.7\n9.4\n9.1\n8.4\n15.4\n-\n10.5 6.4\n7.8\n6.0\n15.1\n9.1\nMAESTRO 600M [Chen et al., 2022c]\n6.9\n7.9\n6.3\n7.2\n-\n8.6\n7.9\n8.5\n7.1\n13.3\n-\n9.2\n5.7\n7.3\n5.2\n14.2\n8.1\nAudioPaLM 8B AST\n10.1\n8.9\n6.1\n6.4\n-\n12.0\n8.3\n11.4 12.4 15.0\n-\n10.1 8.3 13.2 10.0 23.6 11.1\nAudioPaLM 8B S2ST\n13.6 10.6 6.5\n7.6\n-\n17.6 10.0 17.4\n9.8\n17.0\n-\n11.1 7.6 22.7\n8.6\n64.3 16.0\nAudioPaLM-2 8B AST\n8.4\n9.4\n6.2\n6.4\n-\n12.3\n8.3\n10.7\n9.5\n14.8\n-\n10.5 7.0\n9.6\n5.9\n17.8\n9.8\nC\nDetailed results of S2ST models performance\nTable 16: S2ST performance on CVSS, ASR-BLEU scores.\nModel\nArabic (ar)\nCatalan (ca)\nWelsh (cy)\nGerman (de)\nSpanish (es)\nEstonian (et)\nPersian (fa)\nFrench (fr)\nIndonesian (id)\nItalian (it)\nJapanese (ja)\nLatvian (lv)\nMongolian (mn)\nDutch (nl)\nPortuguese (pt)\nRussian (ru)\nSlovenian (sl)\nSwedish (sv)\nTamil (ta)\nTurkish (tr)\nChinese (zh)\nAll\nTranslatotron 2 [Jia et al., 2022a] 30.2 31.9\n5.4\n33.6 38.5 21.0 11.6 36.5 32.8 35.7\n8.5\n22.7 2.5 34.1 41.1 45.6 25.8 36.6 2.2 28.7 13.1 25.6\nAudioPaLM 8B S2ST\n41.5 33.7 18.4 37.2 40.4 23.6 24.6 38.3 47.9 39.4 20.9 25.3 4.8 40.4 50.6 51.3 38.5 43.6 7.2 35.1 20.0 32.5\n25\nD\nDetailed results of AST zero-shot performance\nTable 17: Zero-shot AST performance on FLEURS. BLEU scores for each language together with\nthe number of hours of audio the model has been trained on in each language. The hours of training\ndata for AudioPaLM do not include audio data seen in self-supervised training of audio tokenization\nmodels. The languages indicated with \u00a7 and \u2020 belong respectively to the \u201cAST observed\u201d and \u201cASR\nobserved\u201d sets used in Section 5.2.\nModel\nAfrikaans\u2020 (af)\nAmharic (am)\nArabic\u00a7 (ar)\nAssamese (as)\nAsturian (ast)\nAzerbaijani\u2020 (az)\nBelarusian\u2020 (be)\nBulgarian\u2020 (bg)\nBengali\u2020 (bn)\nBosnian (bs)\nCatalan\u00a7 (ca)\nCebuano (ceb)\nKurdish (ckb)\nChinese\u00a7 (cmn)\nCzech\u00a7 (cs)\nWelsh\u00a7 (cy)\nDanish\u2020 (da)\nGerman\u00a7 (de)\nGreek\u2020 (el)\nEnglish (en)\nSpanish\u00a7 (es)\nWhisper 1.5B\n34.1 1.9 25.5\n5.4\n-\n13.7 11.7 28.5 13.2 29.7 34.2\n-\n-\n18.4\n27.8 13.0 32.7\n34.6\n23.7\n80.2\n23.3\nAST training data (hours)\n330\n32 2286 136\n0\n86\n133\n202 1988 219\n236\n0\n0\n11731 401 8263 386\n4309\n968\n0\n6693\nASR training data (hours)\n4.1\n0\n739\n0\n0\n47\n2.4\n86\n1.3\n11\n1883\n0\n0\n23446 192\n73\n473 13344 529 438218 11100\nAudioPaLM-2 8B\n34.7 3.8 29.0\n9.3 30.8 16.2 15.1 35.5 15.9 35.7 42.5 10.3 4.0\n21.3\n34.5\n7.2\n37.9\n38.7\n18.8\n77.2\n26.9\nAST training data (hours)\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n135\n0\n0\n10\n97\n2\n0\n838\n0\n0\n1887\nASR training data (hours)\n96\n0\n146\n0.4\n0\n99\n135\n168\n121\n0\n515\n0\n2\n120\n237\n5\n115\n848\n151\n1465\n2047\nModel\nEstonian\u00a7 (et)\nPersian\u00a7 (fa)\nFula (ff)\nFinnish\u00a7 (fi)\nFilipino (fil)\nFrench\u00a7 (fr)\nIrish (ga)\nGalician\u2020 (gl)\nGujarati\u2020 (gu)\nHausa (ha)\nHebrew (he)\nHindi\u2020 (hi)\nCroatian\u00a7 (hr)\nHungarian\u00a7 (hu)\nArmenian\u2020 (hy)\nIndonesian\u00a7 (id)\nIgbo (ig)\nIcelandic\u2020 (is)\nItalian\u00a7 (it)\nJapanese\u00a7 (ja)\nJavanese (jv)\nWhisper 1.5B\n18.7 19.6\n-\n22.1 24.4 32.2\n-\n27.9 16.2 0.4 21.8 22.0 27.0 21.2 16.0 29.1\n-\n9.1\n23.6\n18.9\n6.2\nAST training data (hours)\n79\n392\n0\n750\n894 4481\n0\n368\n208\n8\n418 5438 239\n554\n116 1174\n0\n84\n2145 8860 622\nASR training data (hours)\n41\n24\n0\n1066\n75\n9752\n0\n9\n0.3\n0\n688\n12\n91\n379\n13\n1014\n0\n16\n2585 7054\n0\nAudioPaLM-2 8B\n31.7 25.7 0.29 29.3 15.6 36.5 0.3 34.7 12.2 0.6\n0.4\n21.7 30.6 29.2 10.2 34.2 0.3 17.8 27.8\n11.1\n9.7\nAST training data (hours)\n3\n49\n0\n39\n0\n800\n0\n0\n0\n0\n0\n0\n75\n78\n0\n1\n0\n0\n255\n1\n0\nASR training data (hours) 163\n165\n0\n175\n0\n857\n0.2\n1\n123 0.7\n0\n101\n34\n239\n126\n121\n0\n91\n338\n181\n0\nModel\nGeorgian\u2020 (ka)\nKamba (kam)\nKabuverdianu (kea)\nKazakh (kk)\nKhmer (km)\nKannada (kn)\nKorean\u2020 (ko)\nKyrgyz (ky)\nLuxembourgish (lb)\nGanda (lg)\nLingala (ln)\nLao (lo)\nLithuanian\u00a7 (lt)\nLuo (luo)\nLatvian\u00a7 (lv)\nMaori (mi)\nMacedonian\u2020 (mk)\nMalayalam\u2020 (ml)\nMongolian\u00a7 (mn)\nMarathi\u2020 (mr)\nWhisper 1.5B (BLEU)\n2.4\n-\n-\n5.4 6.1 11.6\n21.3\n-\n16.8\n-\n1.0 11.0 14.0\n-\n14.3 10.2 27.7 16.7\n1.0\n12.9\nAST training data (hours)\n40\n0\n0\n31 672\n90\n19938\n0\n10\n0\n20\n20\n99\n0\n68\n1381\n30\n892\n79\n288\nASR training data (hours)\n0.6\n0\n0\n12\n1\n4\n7993\n0\n0\n0\n0\n0.1\n67\n0\n65\n0\n16\n0.5\n0\n0.6\nAudioPaLM-2 8B (BLEU)\n13.6 1.6 29.4 9.5 0.1\n4.8\n19.4\n8.61 16.1 1.6 0.7\n9.5\n26.8 0.6 30.5\n1.2\n30.8 12.2 10.1 17.1\nAST training data (hours)\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n2\n0\n0\n0\n3\n0\nASR training data (hours) 137\n0\n0\n0.2\n0\n0\n157\n0.7\n0\n26\n0\n0\n172\n0\n150\n0\n104\n110\n4\n90\n26\nTable 17: (continued) Zero-shot AST performance on FLEURS. BLEU scores for each language\ntogether with the number of hours of audio the model has been trained on in each language. The\nhours of training data for AudioPaLM do not include audio data seen in self-supervised training of\naudio tokenization models. The languages indicated with \u00a7 and \u2020 belong respectively to the \u201cAST\nobserved\u201d and \u201cASR observed\u201d sets used in Section 5.2. In the last column \u201cAll (82 languages)\u201d,\nthe average BLEU score and total number of AST/ASR training hours were computed over the 82\nlanguages (out of 102) that were used to evaluate the Whisper model.\nModel\nMalay\u2020 (ms)\nMaltese (mt)\nMyanmar (my)\nNorwegian (nb)\nNepali\u2020 (ne)\nDutch\u00a7 (nl)\nNorthern-Sotho (nso)\nNyanja (ny)\nOccitan (oc)\nOromo (om)\nOriya (or)\nPunjabi (pa)\nPolish\u00a7 (pl)\nPashto (ps)\nPortuguese\u00a7 (pt)\nRomanian\u00a7 (ro)\nRussian\u00a7 (ru)\nSindhi (sd)\nSlovak\u00a7 (sk)\nSlovenian\u00a7 (sl)\nWhisper 1.5B\n27.3 13.5 0.4 31.4 16.1 24.0\n-\n-\n20.2\n-\n-\n15.7 22.3 3.4 38.1 31.5 27.8 5.7 26.1 17.0\nAST training data (hours) 1691\n41\n59\n322\n133 1767\n0\n0\n49\n0\n0\n117 2200 63 3620 555 7687 46\n144\n395\nASR training data (hours)\n382\n1\n0.1 266\n0.6\n2077\n0\n0\n0\n0\n0\n0.8\n4278\n0\n8573 356 9761\n0\n90\n41\nAudioPaLM-2 8B\n31.9 12.4 0.0 34.6 16.2 29.1 1.1 1.4 22.9 0.3 8.9\n6.0\n25.3 0.4 38.4 35.7 31.2 1.4 32.3 27.4\nAST training data (hours)\n0\n0\n0\n0\n0\n96\n0\n0\n0\n0\n0\n0\n174\n0\n10\n156\n18\n0\n52\n17\nASR training data (hours)\n126\n0.7\n0\n0\n170\n195\n0\n0\n0\n0\n0.2\n0.3\n267\n0\n16\n246\n179\n0\n191\n170\nModel\nShona (sn)\nSomali (so)\nSerbian\u2020 (sr)\nSwedish\u00a7 (sv)\nSwahili\u2020 (sw)\nTamil\u00a7 (ta)\nTelugu\u2020 (te)\nTajik (tg)\nThai\u2020 (th)\nTurkish\u00a7 (tr)\nUkrainian\u2020 (uk)\nUmbundu (umb)\nUrdu\u2020 (ur)\nUzbek (uz)\nVietnamese\u2020 (vi)\nWolof (wo)\nXhosa (xh)\nYoruba (yo)\nCantonese (yue)\nZulu (zu)\nAll (82 languages)\nWhisper 1.5B\n1.8 0.7 32.5 35.3\n7.2\n9.2\n12.5 14.5 16.1\n26.6 29.4\n-\n17.2\n6.0\n20.4\n-\n-\n1.4\n-\n-\n17.9\nAST training data (hours) 279 21\n136 1055 282 1484 987\n15\n1635 2241 509\n0\n1990\n4\n1719\n0\n0\n432\n0\n0\n120.6k\nASR training data (hours)\n0\n0\n28\n2119\n5\n136\n4\n0.3\n226\n4333 697\n0\n104\n0.3\n691\n0\n0\n0\n0\n0\n117.1k\nAudioPaLM-2 8B\n0.4 0.9 34.3 40.4\n9.1\n15.0 13.3 17.1 15.0\n30.1 26.9 0.9 13.3 17.2 15.6 0.3 0.2 0.7 7.4 1.9\n20.4\nAST training data (hours)\n0\n0\n0\n2\n0\n2\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4.8k\nASR training data (hours)\n0\n0\n122\n129\n126\n125\n109\n0\n156\n145\n138\n0\n116\n0\n149\n0\n0\n0\n1\n0\n12.8k\n27\n"
  },
  {
    "title": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought",
    "link": "https://arxiv.org/pdf/2306.12672.pdf",
    "upvote": "24",
    "text": "From Word Models to World Models:\nTranslating from Natural Language to the\nProbabilistic Language of Thought\nLionel Wong1\u22c6, Gabriel Grand1\u22c6, Alexander K. Lew1, Noah D. Goodman2, Vikash K.\nMansinghka1, Jacob Andreas1, Joshua B. Tenenbaum1\n\u22c6Equal contribution.\n1MIT, 2Stanford\nAbstract\nHow does language inform our downstream thinking? In particular, how do humans make meaning from\nlanguage\u2014and how can we leverage a theory of linguistic meaning to build machines that think in more\nhuman-like ways? In this paper, we propose rational meaning construction, a computational framework\nfor language-informed thinking that combines neural models of language with probabilistic models for\nrational inference. We frame linguistic meaning as a context-sensitive mapping from natural language\ninto a probabilistic language of thought (PLoT)\u2014a general-purpose symbolic substrate for probabilistic,\ngenerative world modeling. Our architecture integrates two powerful computational tools that have not\npreviously come together: we model thinking with probabilistic programs, an expressive representation for\nflexible commonsense reasoning; and we model meaning construction with large language models (LLMs),\nwhich support broad-coverage translation from natural language utterances to code expressions in a\nprobabilistic programming language. We illustrate our framework in action through examples covering\nfour core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual\nand physical reasoning, and social reasoning about agents and their plans. In each, we show that LLMs can\ngenerate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while\nBayesian inference with the generated programs supports coherent and robust commonsense reasoning. We\nextend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics\nengines, and goal-directed planning algorithms) to provide a unified commonsense thinking interface\nfrom language. Finally, we explore how language can drive the construction of world models themselves.\nWe hope this work will help to situate contemporary developments in LLMs within a broader cognitive\npicture of human language and intelligence, providing a roadmap towards AI systems that synthesize the\ninsights of both modern and classical computational perspectives.\n1\nIntroduction\nLanguage expresses the vast internal landscape of our thoughts. We use language to convey what we believe,\nwhat we are uncertain about, and what we do not know. We talk about what we see in the world around\nus, and what we imagine in real or wholly hypothetical futures. We discuss what we want and what we\nplan to do, and dissect what others want and what we think they will do. We build and pass on new bodies\nof knowledge in language\u2014we ask questions and offer explanations, give commands and instructions, and\npropose and refute theories. Some of these ideas can be expressed in part through other means. But language\nstands apart for its flexibility and breadth, and its seeming proximity to our thoughts.\nWhat is language? How does language get its meaning, and when should we say that a person or machine\nknows, understands, and can use it? What is the relationship between language and the rest of general\ncognition\u2014what allows language to inform and support so much of thought? This paper focuses on these\nquestions as they relate to human language and thought, in computational terms. What integrated cognitive\ntheory can model how language relates to the other core systems of human cognition? If we seek to build AI\nsystems that emulate how humans talk and think, what architecture can integrate language robustly into\nsystems that support the full scope of our thought?\nCode for the examples in this paper is available at: github.com/gabegrand/world-models.\nCorrespondence: co-primary authors (zyzzyva@mit.edu, gg@mit.edu); co-supervisors (jda@mit.edu, jbt@mit.edu).\narXiv:2306.12672v2  [cs.CL]  23 Jun 2023\n1\nINTRODUCTION\nTheories of cognition have long considered human language and thinking to be deeply related, but\nfundamentally distinct. Thinking, in many traditional cognitive theories, revolves around goal-directed world\nmodeling, inference, and decision making\u2014constructing mental models of the world that reflect prior beliefs,\ncan be updated from new observations, and support rational prediction and decision making toward\u2019s one\u2019s\ngoals (Craik, 1967; Gentner & Stevens, 2014; Johnson-Laird, 1980, 1989; Lake, Ullman, Tenenbaum, &\nGershman, 2017; Morgan, 1999; Nersessian et al., 2010). Language, in contrast, centers around communicating\nthese thoughts to others, and receiving their thoughts in turn. In most linguistic theories, human languages\nare mappings between the internal representations of thought and an externalizable symbol system, which\nmight be phonemes, signs, or glyphs (Frege, 1892; Heim & Kratzer, 1998; Lewis, 1976). To produce language\nis to map thoughts into these external symbols, and to understand language is to transduce from these\nexternal symbols back into the representations of thought.\nThe theoretical distinction between language and thought rests on multiple intersecting lines of evidence.\nPrior to learning language, infants are born equipped with a powerful toolkit for modeling and thinking about\nthe world, including an understanding of physical objects and events, and the goals and actions of agents\n(Spelke, 2022; Spelke & Kinzler, 2007), and general abilities for learning statistics and structure (Saffran,\nSenghas, & Trueswell, 2001; Xu et al., 2021). Building on these foundations, children acquire language from\nrelatively sparse input data, rapidly generalizing beyond the utterances they hear to produce and understand\nentirely new ones (Bloom, 2002; L. Gleitman, 1990; L. R. Gleitman, Cassidy, Nappa, Papafragou, & Trueswell,\n2005; Landauer & Dumais, 1997; Pinker, 1998; L. Smith & Yu, 2008); they then use language to acquire new\nconcepts they would not get merely from direct experience (Carey, 2009; Gopnik, 1996; Wellman & Gelman,\n1992). Language and thought also appear to operate in distinct but interacting brain systems: neuroimaging\nand neurological studies reveal a \u201clanguage network\u201d specialized for processing sentences, functionally and\nanatomically separate from but closely connected to brain networks supporting other aspects of general\ncognition (Fedorenko & Varley, 2016; Mahowald et al., 2023).\n\u2018These empirical findings have shaped decades of computational models in cognitive science and AI. To\nmodel the expressiveness of human cognition, an influential computational paradigm suggests that humans\ncompose and execute mental programs in an internal language of thought (Fodor, 1975), a structured symbolic\nsubstrate for representing conceptual knowledge that provides a general interface to algorithms for problem\nsolving and reasoning. These symbolic systems are not merely logic engines; they support our probabilistic\ninferences, and rich intuitive simulations (Goodman, Tenenbaum, & Gerstenberg, 2014; Oaksford & Chater,\n2007; Russell & Norvig, 2021). This paradigm underlies many of the success stories in cognitive science and\nrelated applications in AI. It has influenced models that capture how people draw causal and explanatory\ninferences about facts and observations (Pearl, 1988; Pearl et al., 2000), learn and generalize concepts from few\nexamples (Lake et al., 2017); plan actions over long time horizons and under complex conditions (Kaelbling\n& Lozano-P\u00e9rez, 2013; Russell & Norvig, 2021); imagine and predict the physical world (Battaglia, Hamrick,\n& Tenenbaum, 2013; Ullman, Spelke, Battaglia, & Tenenbaum, 2017); and reason about other agents with\ntheir own beliefs and goals (C. Baker, Saxe, & Tenenbaum, 2011). Within linguistics and natural language\nprocessing, in turn, this paradigm underlies semantic parsing systems designed to map from human language\ninto symbolic computational representations. It has yielded AI systems that could follow instructions (Tellex\net al., 2011) and answer natural language queries with respect to structured knowledge representations (Klein\n& Manning, 2003; Liang, 2016; Steedman, 2011; Y. W. Wong & Mooney, 2007); as well as cognitive models\nthat capture how human children learn the grammar and meaning of expressions in their native language\n(Abend, Kwiatkowski, Smith, Goldwater, & Steedman, 2017; Chater & Manning, 2006; Frank, Goodman,\n& Tenenbaum, 2009; Gauthier, Levy, & Tenenbaum, 2018; Goldwater, Griffiths, & Johnson, 2009; Perfors,\nTenenbaum, & Regier, 2011; Piantadosi, Tenenbaum, & Goodman, 2012).\nDespite this progress, however, modular and symbolic models of language and thought have been dogged\nby persistent critiques of their scalability and scope. Cognitive and AI researchers over the years have carved\noff specific domains of world knowledge, constructing bespoke representations to model them without a general\naccount of whether they would generalize to all of human knowledge, or how they could be scalably learned.\nSemantic parsing systems inherited these critiques, and faced additional challenges in implementing the\nmapping from sentences into symbolic representations. These mapping functions were either hand-engineered\nor learned from strong supervision on specific domains of language, limiting them to brittle, imperfect models\nof the breadth and complexity of real human discourse.\n2\n1\nINTRODUCTION\nIn just the last few years, a serious challenge has emerged to the traditional view of language and\nthought as distinct but interacting components of the mind, each modeled using structured representations.\nLarge language models (LLMs) use a new generation of attention-based deep neural networks to learn the\nprobabilistic distributions of words from vast datasets of human language, generally training on orders of\nmagnitude more data than a human encounters in their lifetime (Bommasani et al., 2021; T. B. Brown et al.,\n2020; OpenAI, 2023c; Rae et al., 2021; Vaswani et al., 2017). The underlying computational objective that\ndrives these models is not itself new. LLMs follow in the tradition of distributional approaches to discovering\nstructure in language (Firth, 1957; Harris, 1954; Osgood, 1952), which seek to extract representations of\nmeaning from statistical patterns in how words are used in context (Dumais et al., 2004; Griffiths, Steyvers,\n& Tenenbaum, 2007; Mikolov, Sutskever, Chen, Corrado, & Dean, 2013; Sahlgren, 2008). What is new,\nhowever, is the scale and scope of today\u2019s distributional vision, which has expanded in stages. A first\ngeneration of LLMs, trained specifically to predict words in context, produced such fluent language that\nthey challenged traditional symbolic approaches to modeling language (Devlin, Chang, Lee, & Toutanova,\n2018; Peters et al., 1802; Radford et al., 2019). Their qualitative success, as well as internal representational\nprobes, suggested that linguistic structures sufficient for grammatically coherent language could be learned\nentirely from modeling the statistics of words (Piantadosi, 2023; Tenney, Das, & Pavlick, 2019). By scaling to\neven larger datasets and neural networks, LLMs appeared to learn not only the structure of language, but\ncapacities for some kinds of thinking; they could learn new words in context, and extract patterns in language\nfrom a few examples that they could generalize locally to similar cases (T. B. Brown et al., 2020). The most\nrecent LLMs have been trained not only to model the statistics of language but explicitly to reason, with\ntargeted supervision on instruction following, writing code, and other forms of human dialog and feedback in\nconversational contexts (Chen et al., 2021; OpenAI, 2023a, 2023c; Ouyang et al., 2022). They produce such\nfluent language on a wide variety of tasks that many have begun to ask whether merely more training of this\nsort, with increasing scale, could learn representations sufficient for general intelligence (Bubeck et al., 2023).\nProponents of the most extreme \u201cscaling hypothesis\u201d have argued that because language is used to express so\nmuch of human thought, a sufficiently large and performant predictive language model would effectively have\nto construct an internal model of all of cognition (Branwen, 2022).\nThis theoretical vision has sparked both excitement and controversy, but proponents and critics agree that\nit raises its own questions about its long-term scalability\u2014most significantly, what will be required to close\nthe outstanding gaps between today\u2019s LLMs and general cognitive models that reason systematically and\nconsistently about the language they receive or produce. Current LLMs can produce impressive results on a\nset of linguistic inputs and then fail completely on others that make trivial alterations to the same underlying\ndomain (Ullman, 2023); they mix confident answers to complex questions with equally confident, hallucinated\nlanguage that does not reflect a consistent, calibrated notion of truth or belief (Bubeck et al., 2023; OpenAI,\n2023c). These issues make it difficult to evaluate whether LLMs have acquired cognitive capacities such\nas social reasoning and theory of mind (Ullman, 2023), or to compare different kinds of world modeling\nand planning tasks (Valmeekam, Sreedharan, Marquez, Olmo, & Kambhampati, 2023). One approach to\nsolving these problems is through additional data. Perhaps fully robust, systematic reasoning will finally\nemerge if models are trained on still more language, or supervised more explicitly on data from complex\nreasoning tasks. This scaling route raises practical questions about whether it will be possible to acquire\nenough data to train such a model, as well as theoretical questions whether more data and more parameters\nalone will in fact yield robust systems for thought. Another strategy in recent work seeks to build more\nrobust cognitive capacities by augmenting LLMs with various external tools for structured representation\nand symbolic reasoning, such as calculators (Cobbe et al., 2021), logic engines (Weir & Van Durme, 2022),\ndatabases (Alon et al., 2022; Borgeaud et al., 2022; Izacard et al., 2022; Thoppilan et al., 2022), physics\nsimulators (R. Liu et al., 2022), planners (B. Liu et al., 2023), and APIs for executing arbitrary code (Karpas\net al., 2022; OpenAI, 2023c; Schick et al., 2023). But these new hybrid approaches resurrect many of the\nsame long-term scalablity challenges that confronted earlier semantic parsing and knowledge representation\nsystems, by designing a menagerie of bespoke representations and tools without a broader account of how\nthey will scale towards general models of language and thought.\nIn this paper, we consider a different approach to integrating the strengths of modern language models\nand classic symbolic architectures, one that draws on but also runs counter to recent trends in AI, in a\nsense flipping these scaling questions on their head. Instead of trying to turn models trained to predict\nlanguage into models that might genuinely think\u2014filling each gap in reasoning we discover through yet more\n3\n1\nINTRODUCTION\nApproaches to language-informed thinking\nObservations\nQuestions\n(query ...)\n(condition ...)\nMeaning \nfunction\nInference \nfunction\nNatural\nlanguage\nProbabilistic \nlanguage of thought\nDistributions over \npossible worlds\nWorld knowledge\n(define ...)\nLarge language models\nClassical symbolic models\n\u2026\n\u2026\n???\nOur framework: Rational Meaning Construction\nNeural\nSymbolic\n\ud835\udf06\ud835\udc65. \u2200\ud835\udc65 \u2026 \u2192 \ud835\udc61\ud835\udc5f\ud835\udc62\ud835\udc52\n\u2026\n\u2026\nFigure 1: Human language understanding supports flexible inferences in a process we term language-informed\nthinking. Computational approaches to language-informed thinking sit on a neurosymbolic continuum: On\none side, classical symbolic models (top right) yield systematic, structured inferences, but are typically\nlimited to narrow linguistic domains and often require hand-engineering. On the other side, large language\nmodels (LLMs; top left) achieve remarkable facility with open-domain natural language, but struggle to\nground reasoning in a consistent world state that supports coherent inferences, predictions and plans. Our\nrational meaning construction framework decomposes language-informed thinking into two modules: (1) A\nmeaning function translates natural language into probabilistic programming language (PPL) statements that\nrepresent linguistic meaning with respect to a symbolic world model. (2) An inference function computes\nprobabilities over the space of possible worlds consistent with and conditioned on information in the language.\nIn the rest of this paper, we illustrate how our framework can combine the strengths of LLMs and PPLs,\naffording both broad coverage of natural language and a principled treatment of reasoning about uncertain\nevents, outcomes, and scenarios.\ndata, new kinds of language training or linguistic prompting tricks, or by plugging in yet another external\ntool\u2014we ask: what are the prospects for a unifying computational framework guided by the study of thought\nand language in the human mind and brain, as well as what we have learned from multiple eras of AI?\nCan we build intelligent architectures that use, learn and understand language as people do, informed by\nneuroscience constraints and developmental trajectories? That is, can we build models in which language is\nlearned efficiently within one relatively small, modular computational system, which interfaces generally with\nother systems dedicated to robust world modeling and reasoning? What architecture lets language build on\npre-existing capacities for symbolic world modeling and inference, while also allowing linguistic meanings and\nworld knowledge to scaffold and bootstrap each other, as a learner\u2019s experiences and competences grow?\nThis paper attempts to show what such a model might look like\u2014and how it can build theoretically and\npractically on the insights from both classical paradigms for language and thought, and the recent successes\nof statistical learning made by large language models. We propose a framework for intelligent computational\narchitectures that reason about and learn from language, but we begin with a proposal for what it means to\nthink. As in the traditional cognitive view, thinking at its core is constructing general-purpose representations\nfor modeling the entities and events in the world, sufficient to support rational, coherent inferences under\n4\n1\nINTRODUCTION\nuncertainty and planning actions that achieve our goals. We then consider how language relates to this\narchitecture to support language-informed thinking\u2014how language sets up world modeling and inference, to\nguide, constrain, and drive our downstream thought, and grow new thinking capacities.\nOur proposal, which we call rational meaning construction, rests on the integration of two com-\nputational components, each which we suggest can be instantiated using modern computational tools\u2014a\nprobabilistic language of thought for constructing structured models of arbitrary situations, which supports\nsupports coherent belief updating and inferences over them; and a general mechanism for taking natural\nlanguage and constructing meaning from it, represented as distributions over expressions in this language\nof thought (Fig. 1). We propose that probabilistic programs can formally instantiate the first component.\nThey offer a structured representation for expressing novel situations and arbitrary problems with respect\nto a meaningful model over possible world states, a coherent notion of conditional belief updating, and a\nsystematic framework for inferences with respect to queries and goals. We propose, in turn, that meaning\nconstruction can be modeled as translation from utterances in language to expressions in a general proba-\nbilistic programming language. Theoretical and empirical results have long suggested that human languages\nimplement locally compositional, efficiently learnable mappings between symbolic representations of thought\nand external symbol systems. We therefore propose that code-trained large language models can be viewed\nas in-principle implementations of broad, context-sensitive, and resource-rational meaning functions, in that\nthey can be used to efficiently infer distributions between language and programs from stored, prior patterns\nin the background distribution of language and code. By integrating these two components, we propose that\nthis paradigm suggests a general framework by which language can meaningfully relate to many fundamental\naspects of cognition, modeling how we might condition on language to systematically update our beliefs,\npose new questions and goals in language, and convey structured background information or even define new\nrelevant concepts about a situation or about the world.\nIn Section 2, we give an overview of this framework, describing the overall structure and more detailed\nrationale behind the computational components we build on in the remainder of this paper. We then describe\na concrete but minimal implementation of this framework using contemporary probabilistic programming\nand language modeling tools, intended to demonstrate the basic computational components of this approach\nand elucidate the scope and scalability of the broader proposal.\nGiven this general paradigm, we first illustrate the potential breadth of this approach for integrating\nmeaning construction and reasoning, showing how it might address a core set of computational and\ncognitive domains that we communicate about in language (Fig. 2). Each of these examples uses minimal\npedagogical examples intended to suggest how this approach integrates language with important bodies of\nwork from computational cognitive science and artificial intelligence. We first show how this framework can\ncondition on language in order to describe and reason about uncertain situations with respect to an ongoing\ndiscourse (Section 2.2), then show how this approach can be extended to reason about relational systems\n(Section 3.1), physical and perceptual scenes (Section 3.2), and social situations involving agents with goals\nand plans (Section 3.3).\nWe then turn to how this approach might begin to address core scalability challenges that confront\ntraditional approaches to modeling thinking as symbol processing, whether logical or probabilistic. In\nSection 4, we show how language can support growing knowledge autonomously, without hand engineering, by\nusing the rational meaning construction framework to construct a broad range of new concepts in existing\nmodels and even whole new world models, which in turn support coherent downstream reasoning.\nUltimately, this paper is a prospective one, and the examples presented here are intended to convey a\nsufficiently concrete proposal to suggest avenues for future work. In Section 5, we outline what we see as some\nof the most significant open questions and future directions raised by this framework. These include theoretical\nquestions that relate our approach to classical models of language, open cognitive directions for extending\nthis approach to model language acquisition and production, and important engineering directions necessary\nfor scaling inference, robust translation, and learning under this general paradigm. Finally, in Section 6, we\nconclude by looking ahead to the longer-term implications of this proposal for modeling intelligent systems\nthat use, understand, and think about language as we do.\n5\n1\nINTRODUCTION\nObservations\nabout the world\nQuestions\nabout the world\nCondition\nstatements\nQuery\nstatements\nKnowledge\nabout the world\nGenerative\nworld models\nRelational\nReasoning\nKinship systems\nCharlie is Dana's grandfather.\n(condition\n(grandfather-of? 'charlie\n'dana))\nWhich of Charlie\u2019s kids is Dana\u2019s \nparent?\n(query\n(filter-tree\n(lambda (x) (and\n(child-of? x 'charlie)\n(parent-of? x 'dana)))))\nA grandfather is the father of one's \nparent.\n(define\ngrandfather-of? (name_a name_b)\n(exists (lambda (x) (and\n(father-of? name_a x)\n(parent-of? x name_b)))\nPerceptual and \nPhysical Reasoning\nVisual and physical scenes\nThere is at least one red mug in this \nscene.\n(condition\n(>= (length\n((filter-shape 'mug) \n((filter-color red)\n(objects-in-scene\n'this-scene)))) 1))\nHow many mugs are there?\n(query\n(length\n((filter-shape 'mug)\n(objects-in-scene\n'this-scene))))\nObjects vary in both their possible \nshape and possible color.\n(define\nobject (obj-id)\n(list (choose-shape obj-id)\n(choose-color obj-id)))\nSocial\nReasoning\nAgents and planning\nAlex loves sushi but hates pizza; and he \nbrought his bike to work today.\n(condition\n(and (loves? 'alex 'sushi)\n(hates? 'alex 'pizza)\n(has-bike? 'alex)))\nWhat do you think Alex will do?\n(query (get_actions 'alex))\nDepending on whether they have a \nbike, people can bike or walk.\n(define\nactions (agent-id) \n(if (has_bike? agent-id) \n(list 'is_walking 'is_biking) \n(list 'is_walking)))\nProbabilistic\nReasoning\nBayesian tug-of-war\nJohn and Mary faced off against Tom \nand Sue and won.\n(condition\n(won-against '(john mary) \n'(tom sue)))\nIs Mary stronger than Tom?\n(query\n(> (strength 'mary)   \n(strength 'tom)))\n81.39\n0.27\nThe winner of a match is whichever \nteam is stronger.\n(define\nwon-against (team-1 team-2)\n(> (team-strength team-1)\n(team-strength team-2)))\nFigure 2: Understanding language in four domains of reasoning that form the core of this paper. Probabilistic\nreasoning requires integrating sparse evidence to predict the outcomes of uncertain events, like the winners\nof tug-of-war matches. Relational reasoning involves maintaining and updating coherent beliefs about\nstructured domains, like family trees, based on relational information. Perceptual and physical reasoning\nlinks language to our sensory and intuitive physical knowledge of objects in the external world, such as\nkitchen items on a tabletop. Social reasoning involves reasoning about the minds of other intelligent agents,\nsuch as how their goals, preferences, and circumstances shape their actions as they navigate in the world.\nAcross all the domains, we present a unified framework that translates language into code in a probabilistic\nprogramming language to facilitate human-like reasoning.\n6\n2\nOVERVIEW OF THE KEY IDEAS\n2\nOverview of the key ideas\nThe central goal of this paper is to propose a new computational framework, rational meaning construction,\nwhich relates language to thought. This framework licenses a concrete class of computational architectures\nfor building intelligent systems that use language, which we propose can be implemented using modern AI\ntools. In this section, we briefly overview the key ideas that form the basis of this proposal. We draw on\nthree observations from a rational, probabilistic perspective on biological intelligence and human language:\nA rational perspective on intelligent thought.\nBiological intelligence encompasses many computational\ncapacities. The foundational notion of thought we focus on here centers on rational inference and decision\nmaking in service of one\u2019s goals (Anderson, 1990; Chater & Oaksford, 1999). Under this perspective, thought\ncomprises systems for modeling the world. These internal world models allow us to infer the particulars of a\nsituation from whatever information is at hand, evaluate alternative world states and imagine possible future\nones, and decide on actions that might bring one towards valuable future states in the world. Following\nextensive work in computational cognitive science, we view the world models that support biological intelligence\nas structured and probabilistic (Goodman et al., 2014; Griffiths, Chater, Kemp, Perfors, & Tenenbaum,\n2010; Lake et al., 2017), designed to integrate the noisy evidence an agent receives into causal, explanatory\nmodels that allow them to maintain coherent beliefs about the world and generalizably infer consistent, useful\npredictions and plans. This basic, underlying view of intelligent thought draws on empirical evidence from\nessentially every species with a brain, from bees (Biernaskie, Walker, & Gegear, 2009; R. F. Wang & Spelke,\n2002), to zebrafish Bolton et al. (2019); R. E. Johnson et al. (2020), mice (English, Nejad, Sommerfelt, Yanik,\n& von der Behrens, 2023), birds (Isomura, Parr, & Friston, 2019), and primates (Khalvati, Kiani, & Rao,\n2021). Informally, a rational view of thought can be summarized as the ability to solve useful problems\ngiven our internal models of the world, ranging from navigation and foraging to physical prediction and\nsocial reasoning. Against this overarching picture of thought, human intelligence further stands out for its\nflexibility and expressiveness. We invent our own problems along with new approaches to solving them,\nrather than sticking to a limited set of largely innate goals and strategies (Tomasello, 2022). A few other\nspecies, non-human primates, dolphins, and some birds, are creative problem-solvers and problem-creators,\nbut none come close to the range of goals humans can adopt (Chu & Schulz, 2023). Uniquely in the natural\nworld, humans think about and come to understand problems far beyond the narrow range necessary for our\nimmediate survival, considering goals and questions that draw on abstract, culturally constructed, and even\nentirely hypothetical systems for modeling and conceptualizing the world (Dennett, 2017).\nA rational perspective on language.\nAs with thought, language also encompasses many systems and\ncapacities. This paper focuses on the class of problems we refer to as language-informed thinking, the\ngeneral means by which language informs the inferences and decisions of an intelligent agent. We take a\nbroadly rational perspective on language\u2014we consider language to be a system of goal-directed actions for\nexternalizing and communicating thoughts to other intelligent beings (Chater & Manning, 2006; Gibson,\n2014; Goodman & Frank, 2016). In this context, we frame the problem of deriving meaning as inferring the\nmappings between a language\u2019s system of external communicative signals into the representations of rational\nthought. It is worth highlighting that thought does not require language and is distinct from language in\nthe human brain (Mahowald et al., 2023). Non-human species, and pre-verbal infants (Spelke, 2022), are\nclearly capable of modeling the world towards their inferences and goals without language. But for humans,\nlanguage clearly plays a profound role in determining the problems we think about, and how we think about\nthem. Our natural languages allow us to communicate an extraordinarily broad range of our thoughts about\nthe problems we pose and solve, including our abstract and general world knowledge, our specific beliefs\nabout a situation, the particular questions or goals we have or want to pose to others, and our approaches to\nreasoning about them.\nA resource-rational perspective on language and thought.\nFinally, our integrated computational\napproach to language and thought builds on extensive evidence that humans are resource-rational thinkers\u2014\nunder finite constraints of time and memory, we rationally allocate computational resources in order to\nmake useful inferences and plans (S. J. Gershman, Horvitz, & Tenenbaum, 2015; Lieder & Griffiths, 2019).\nResource rational agents amortize computational effort across prior experience and problems, storing and\n7\n2.1\nA rational meaning construction framework\n2\nOVERVIEW OF THE KEY IDEAS\nreusing prior computation towards similar new problems that we encounter in the future (S. Gershman\n& Goodman, 2014; Le, Baydin, & Wood, 2017). Certain domains of inferences share more structure than\nothers, and evidence suggests that we therefore heavily amortize them. Prior work, for instance, suggests\nthat computations involved in basic perceptual activities (Brooke-Wilson, 2023; Dasgupta & Gershman, 2021;\nFodor, 1983), such as object recognition under common lighting conditions, are highly amortizable from\nreusable patterns in computation that are learnable and shared across a background distribution of perceptual\ninstances. This view suggests why fast, bottom-up pattern recognition models have made great advances in\nmodeling perception in recent years, while it has proved much more challenging to amortize the wide range of\nflexible inferences required for arbitrary problem solving.\nWe propose an analogous resource-rational perspective on the kinds of computation implicated in language-\ninformed thought. Under almost every theoretical and empirical account of linguistic structure and semantics,\nthe mappings between language and meanings should be highly amortizable across the background distribution\nof language\u2014there are structured, systematic, and learnable patterns in how units of language map onto\nunits of thought.\nThe idea that meaning construction should be highly amortizable follows from our\nview on language itself as an efficient communicative system. Extensive empirical evidence suggests that\ncommunicative pressures shape how language maps onto meanings at every level of linguistic structure, from\nindividual morphemes (Bybee, 1985) to patterns in how common syntactic frames communicate meaning\n(L. Gleitman, 1990; Grimshaw, 1981), and even reusable pragmatic implications present across common\ndiscourse situations (White, Mu, & Goodman, 2020). But while we take the view that a resource-rational\nagent should intelligently learn and reuse prior computation when possible, we do not view language-informed\nthinking, or thinking in general, as solely a matter of learning and interpolating over statistical patterns from\nprior experience. When we think, including when we think about the meanings we recover from language\u2014to\nupdate our beliefs, to follow instructions, or to answer questions posed in language\u2014we must be able to\nflexibly model arbitrary situations and support capacities for general problem solving, including inference,\nplanning, and simulation, under a wide range of new and unencountered circumstances.\nThe efficient learnability of human language also highlights that, in many senses, the computational\nrelationship between language and thought in humans is almost the inverse of that in today\u2019s LLMs. For\nhumans, language could be characterized as an emergent property of thinking. Infants can model the world\nand draw inferences well before they know language (Gopnik, 1996; Spelke, 2022), and reliably acquire\ncomplete linguistic capabilities from exposure to relatively tiny amounts of language (R. Brown, 1973).\nCongenitally-Deaf humans born with no language input spontaneously develop languages to communicate\ntheir thoughts, with the same basic hallmarks of mature natural languages (Goldin-Meadow, 2012; Pyers,\nShusterman, Senghas, Spelke, & Emmorey, 2010; Senghas, Kita, & Ozyurek, 2004). This paper seeks to\nunderstand and model the cognitive and computational structures underlying this human scaling route to\nintelligence and language use \u2014 one that begins with robust capacities for thought, and scaffolds language\nefficiently on top of them to then offer a powerful tool for driving and constructing new thought.\n2.1\nOur proposal: A framework for modeling rational meaning construction\nThe perspective we offer above draws from theoretical and empirical work that precedes this paper. Our core\ncontribution in this paper is to propose a new computational framework in light of these observations, that\nseeks to unify prior symbolic, probabilistic inference and statistical learning traditions and to take advantage\nof the clear computational advances made by modern LLMs as learned statistical models of language. We\ndescribe a framework for rational meaning construction in which linguistic meaning is formalized as a\ncontext-sensitive mapping from natural language to a distribution over expressions in a probabilistic language\nof thought (PLoT) for rational world modeling and inference. Under this framework, we then propose that\nlarge language models trained on language and code can be used to implement meaning functions in a\nresource-rational architecture \u2013 they can implement learned, broad-coverage mappings between language and\ncode; and they can be understood as part of a human-like, resource-rational system that efficiently infers\nthese mappings using stored patterns amortized from the prior joint distribution over language and code.\nThis motivates the concrete architecture we propose and illustrate throughout the remainder of this paper,\nand its two main components for modeling thinking and modeling language relative to thinking\u2014or how\nlanguage informs thinking.\n8\n2.1\nA rational meaning construction framework\n2\nOVERVIEW OF THE KEY IDEAS\n2.1.1\nModeling thinking\nWe propose implementing thinking using probabilistic programs as a general representational substrate\nfor building world models and specifying rational inferences over them. This proposal builds on prior work in\ncognitive science and AI formalizing how a broad class of problems can be expressed as probabilistic programs\n(Chater & Manning, 2006; Goodman et al., 2014), following a generic inference query motif (Goodman,\nMansinghka, Roy, Bonawitz, & Tenenbaum, 2008) \u2014 a probabilistic program that combines a generative\nworld model that models abstract, causal beliefs about probable world states; specific evidence that an agent\nconditions on; and a particular query being posed as the question or goal for thinking. Inference to solve\na problem consists of formally computing or sampling from a probability distribution over answers to this\nquestion, specified by the world model and conditions. This computational proposal forms the backbone of\nthe probabilistic language of thought model of general human cognition (Goodman et al., 2014), and has been\nused empirically to model a wide range of human inferences, including those that draw on visual perception\n(V. K. Mansinghka, Kulkarni, Perov, & Tenenbaum, 2013), physical simulation (Battaglia et al., 2013), and\nsocial reasoning (C. Baker et al., 2011). It is designed explicitly to formalize a central property of human\nthought \u2014 the capacity to expressively and flexibly pose problems involving entirely novel situations and\ngoals, and to solve them relative to a computable representation of the world and internal belief.\n2.1.2\nModeling language relative to thought\nGiven this model for thought, we propose formalizing rational meaning construction as a broad-coverage,\ncontextual translation function that maps language into a distribution over expressions in a probabilistic\nlanguage of thought. This proposal builds most closely on and draws inspiration from efforts to articulate\na probable world semantics for natural language in prior work (Goodman & Lassiter, 2015), in order to\nexpress how language could compactly convey uncertain propositions and vague meanings with respect to\na formal probabilistic generative model. It also builds on the longer history of symbolic semantic theories\nwe overview in the introduction, including formal semantics theories that model language as mapping into\nformal propositions over possible worlds (eg. Heim and Kratzer (1998); Lewis (1976)), and semantic parsing\nsystems (eg. Abend et al. (2017); Klein and Manning (2003); Liang (2016); Steedman (2001); Y. W. Wong\nand Mooney (2007)) that map language into formally executable program expressions.\nOur goal, however, is to broaden and generalize these framings to suggest a general framework for modeling\nhow language can interface with and inform such a broad swatch of human cognition. By positing that\nmeaning is a general mapping between sentences and expressions in a probabilistic language of thought, we\nbelieve that a rational meaning construction approach can elaborate on and concretely model core desiderata\nof a coherent theory of linguistic meaning \u2013 modeling how meanings drive inferences about what is true and\nprobable; formalizing how language can pose propositions and queries that are then evaluated with respect\nto an internal model over probable worlds; and relating meaning to the general computational systems for\nrepresenting, thinking about, and receiving new information about the world from broader cognition.\nThis proposal suggests a wide class of possible architectures that map from language into probabilistic\nprograms\u2014in principle, any general mapping function that expresses a distribution over programs conditioned\non sentences in context. Under this umbrella of possible implementations, we propose finally that large\nlanguage-to-code models can be used to generally instantiate these meaning functions. Unlike prior\nsemantic parsers or attempts to hand implement mappings between language and code, LLMs offer a concrete\nmeans of instantiating far more broad-coverage mappings between human sentences and meanings than\nhave been previously possible. They are also context-sensitive, in that they can construct meanings for an\nutterance that condition both on the general distribution of language and thought and a local linguistic and\nthinking context. They can condition translation on a local discourse context, when prompted with prior\nutterances, and on a local problem under consideration, when prompted with existing code in a probabilistic\nprogram.\nBy using LLMs to map between language and code, this proposal is also closely related to the recent\nlines of work we review in the introduction that seek to augment and connect LLMs with various structured\nand symbolic reasoning tools\u2014both domain-specific reasoning engines like planners and physics engines (eg.\nB. Liu et al. (2023); R. Liu et al. (2022)), and more general APIs for code execution (eg. Karpas et al. (2022);\nOpenAI (2023b); Schick et al. (2023)). As we demonstrate throughout this paper, however, we propose\nthat the probabilistic language of thought can offer a cognitively-motivated, unifying symbolic substrate for\n9\n2.1\nA rational meaning construction framework\n2\nOVERVIEW OF THE KEY IDEAS\ninterfacing between language and many core aspects associated with general cognition. It provides a general\nmotif for structuring and constructing generative world models, which can nest calls to other domain-specific\nsystems (such as planners and physics engines); and an overarching framework for modeling how diverse kinds\nof observations can be used to update these models and answer new queries, framed as Bayesian conditioning\nand inference. With respect to the more general landscape of large statistical language models, this proposal\nfinally suggests one way to situate the strengths of LLMs into a more human-like, modular framework for\nlanguage-informed thinking. Rather than look to statistical patterns to capture all of the ways we think, plan,\nand reason about language, this resource-rational approach seeks to ground distributional aspects of language\ninto a framework that can leverage learned prior patterns when they are useful\u2014while also modeling how\nlanguage can construct and relate to coherent world models and algorithms for explicit, novel decision making\nand inference.\n2.1.3\nIllustrating the architecture by example\nThis general architecture is best explained through concrete implemented examples, which we give in the\nnext sections. For each of the four domains of reasoning shown in Fig. 2, we work through a representative\ndialog between a speaker of English and our language-informed thinking computational architecture, which\ncould stand in for how we model another human being\u2019s understanding and thinking about the speaker\u2019s\nlanguage, or the ways we hope a human-like AI system would similarly respond.\nFor pedagogical reasons, we have chosen to implement these examples using one particular probabilistic\nprogramming language and one particular language-to-code model. These particular tools are not necessarily\nthe most performant or scalable AI solutions; nor the best accounts we have of the corresponding components\nof human architecture. Nevertheless, they are familiar and simple, and provide the most direct route we know\nto illustrate our ideas in ways others can also experiment with. To elaborate on these choices:\n\u2022 The probabilistic language of thought we use to express inference problems is Church (Goodman et\nal., 2008), a Turing-universal probabilistic programming language constructed on top of the functional\nprogramming language Scheme. We have used the WebChurch dialect which implements several general\ninference procedures, but we have chosen the simplest and most general\u2014and least efficient\u2014approach\nbased on rejection sampling: Inference is based on drawing samples from the prior over world states\ndescribed by the generative model, and rejecting those that fail satisfy the constraints of any observation\nconditions. The samples that remain constitute a posterior sample over possible worlds consistent\nwith the observed information, sufficient to answer the queries under consideration in the language\ndiscourse. Other similarly functional PPLs such as WebPPL or Gen could have been chosen instead.\nIn Section 5, we discuss future directions for extending and scaling inference beyond these simple\nillustrative implementations.\n\u2022 The language-to-code model we use to amortize meaning construction over programs is Codex model\n(Chen et al., 2021), a GPT-3-based language model fine-tuned on source code, which provides pairings\nbetween natural language and code with comments, drawn from programs on GitHub and other sources.\nSince the release of Codex, many other language-to-code models have been developed, and more recent\nversions of GPT-based language models are now routinely trained on large amounts of source code;\nwe believe these could be used to similar effect. In Section 5, we also discuss future directions for\nmore cognitively plausible training and updating of neural models that amortize inference in joint\ndistributions over natural language and probabilistic languages of thought.\nFinally, before turning to the examples, we want to add an important note about our intentions and goals.\nThe examples are designed to be illustrative and pedagogical\u2014we choose them for their simplicity and clarity,\nand to show how prior empirical and computational work from cognitive science can be related under this\ngeneral framework to language. Each example gestures at a larger domain of reasoning, but, of course, each\ndomain is much broader than what we can implement here. Each example is also representative of a wide\nclass of computational cognitive models that can be instantiated in a probabilistic language of thought, and\nwhich we propose can be integrated with natural language inputs and outputs under a rational meaning\nconstruction framework. In each section we therefore also discuss how this framework might be scaled, and\nwhat more work may be necessary, to scale from these examples towards a richer model of language in relation\nto those domains.\n10\n2.1\nA rational meaning construction framework\n2\nOVERVIEW OF THE KEY IDEAS\nWe also hope that these examples, and other variations that elaborate on them and on the core domains\nof reasoning we discuss here, will offer useful starting points for more rigorous, systematic, cognitively-\noriented evaluation and interpretation of the reasoning processes emergent in large language models and other\nlanguage-based AI systems. In our own preliminary evaluations of these domains, we find that current large\nlanguage models show many of the properties we discuss in the introduction. In some cases they appear to\napproximate implicitly the representations and algorithms we seek to model explicitly. In others, particularly\nwith more complex modifications beyond these simple examples, we find that large language models left to\ntheir own devices produce outputs that diverge from our intuitions. We seek here to model the representations\nwith which people make meaning from language in relation to all of these domains, but hope that these\nframeworks will be useful for understanding other computational systems that use language as well, including\ninterpreting the representations that large language models already learn or should seek to acquire.\nGraphical conventions\nThroughout the examples presented in this paper:\nTranslations mapping from language into probabilistic programs, produced by Codex, are\nindicated by a neural network icon.\nProbabilistic inferences, performed by Church, are indicated by a cog icon.\n11\n2.2\nUnderstanding language with probabilistic reasoning\n2\nOVERVIEW OF THE KEY IDEAS\n2.2\nUnderstanding language with probabilistic reasoning\nTo illustrate our framework, let\u2019s consider a concrete scenario that involves reasoning from language in the\nface of uncertainty. Suppose a friend is telling you about a tug-of-war tournament that took place the prior\nweekend in which the authors were participating:\nRight off the bat, Josh won against Lio. He then proceeded to claim victory against Alex. Even\nworking as a team, Lio and Alex still could not beat Josh!\nIn order to understand this story, it is useful to construct a little mental model: there are different players,\nthey face each other solo or in teams, and based on his track record, Josh appears to be particularly strong.\nNow, suppose your friend tells you about a newcomer: In a huge upset, Gabe managed to best Josh in the\nfourth round. Maybe Gabe is even stronger than Josh! Or, perhaps Josh was simply feeling lazy in the last\nmatch, in which case, Gabe might not actually be so strong. To clarify, you might ask a question, Who\nis stronger: Gabe or Josh? Your friend\u2019s answer, which might itself express uncertainty, will nevertheless\nprovide further information for you to incorporate into your understanding.\nIn making meaning from language about a scenario like the above, you are engaging in probabilistic\nreasoning: integrating over different possibilities in order to infer likely explanations. People are remarkably\nproficient at making inferences from exactly this kind of sparse evidence. Sometimes, we acquire this evidence\nthrough direct experience\u2014by watching the tournament, for instance\u2014but often, this kind of information\ncomes to us through language that cues us to update our beliefs accordingly. Critically, in order to reason\nconsistently, we need to represent core aspects of the situation: who are the different actors, what events took\nplace, and what inferences have we already made? To this end, it is extremely useful to have a world model,\nwhich we defined earlier as a probabilistic generative model that encapsulates the key mechanics of a domain\nand facilitates coherent, causal explanations of events. In this section, our aim is to further formalize what\nexactly we mean by world models and how large-scale neural models might serve as an interface between\nnatural language and these kinds of cognitive representations.\nWorld models as generative programs.\nThe core of each example in this paper is a probabilistic\ngenerative model that defines the mechanics of a domain. For the purposes of this demonstration, and\nthroughout Section 3, we focus on reasoning from language given a pre-specified world model. Later, in\nSection 4, we show how language can be used to grow out and construct new world models.\nAs a playground for this initial demonstration, we consider the \u201cBayesian tug-of-war,\u201d a classic experimental\ndomain in cognitive science that requires making inferences about the latent traits of individuals from sparse\nevidence. Prior work establishes that Bayesian inference in a probabilistic generative model closely captures\npeople\u2019s predictions about scenarios in the tug-of-war (Gerstenberg & Goodman, 2012; Goodman et al., 2014),\nand that simple sentences can be mapped onto queries in this model (Goodman & Lassiter, 2015). Here, we\nbuild on this work to give an account for how people might turn open-ended natural language into statements\nin the probabilistic language-of-thought.\nIn tug-of-war, we start with a generative model of a tournament in which players of varying strengths\ncompete in a series of matches, facing off either solo or as part of fluid teams (Fig. 3A). Each player has a\nlatent strength value randomly sampled from a Gaussian distribution (with parameters arbitrarily chosen as\n\u00b5 = 50 and \u03c3 = 20). As an observer, our goal is to infer the latent strength of each individual based on their\nwin/loss record. However, players sometimes don\u2019t pull at their full strength and each player has a different\nintrinsic \u201claziness\u201d value (uniformly sampled from the interval [0, 1]) that describes how likely they are to be\nlethargic in a given match. The full Church code for the tug-of-war is given in Appendix A.1.1.\nLinguistic meanings as probabilistic program expressions.\nWhile the generative model defines\nthe generic mechanics of the domain, we want to be able to talk about specific people and events. In our\nframework, we focus on two kinds of linguistic utterances:\nObservations provide information about people, objects, and events in the world; e.g., \u201cJosh faced off\nagainst Lio and won.\u201d In our framework, we translate observations into condition statements in Church,\nwhich update the state of the world model to reflect new facts. Note that condition statements have no\nreturn value; instead, they constrain the world model such that downstream inferences must be consistent\nwith respect to the conditioning statement.\n12\n2.2\nUnderstanding language with probabilistic reasoning\n2\nOVERVIEW OF THE KEY IDEAS\n(A) Generative world model\n(define strength (mem (lambda (player) \n(gaussian 50 20))))\n(define laziness (mem (lambda (player) \n(uniform 0 1))))\n(define (team-strength team)\n(sum (map (lambda (player) \n(if (flip (laziness player)) \n(/ (strength player) 2) \n(strength player)))\nteam)))\n58.07\n0.27\n(condition (won-against '(josh) '(lio)))\nJosh won against Lio.\n(condition (won-against '(josh) '(alex)))\nJosh proceeded to claim victory against Alex.\n(condition (not (won-against '(lio alex) \n'(josh)))\nEven working as a team, Lio and Alex still could not \nbeat Josh.\n(B) Translation examples for LLM prompting\nSue is very strong!\n(condition (> (strength 'sue) 75))\nJohn and Mary won against Tom and Sue.\n(condition (won-against '(john mary) '(tom sue)))\nIf Sue played against Tom, who would win?\n(query (won-against '(sue) '(tom)))\n81.39\n(query (won-against '(gabe) '(josh)))\nWhat are the odds of Gabe beating Josh?\n(query (strength 'josh))\nHow strong is Josh?\n(define (won-against team-1 team-2)\n(> (team-strength team-1) (team-strength team-2)))\n(C) Natural language\n(D) Language of thought\nFigure 3: Illustration of probabilistic reasoning via language-to-code translation in the tug-of-war domain.\n(A) The generative model defines two latent traits, strength and laziness, and specifies how these interact\nto determine team-strength. By combining (A) and (B), we can few-shot prompt an LLM to translate\nopen-ended natural language (C) into Church statements (D) that capture linguistic meaning with respect to\nthe domain. The resulting probabilistic inferences transparently represent the model\u2019s beliefs and naturally\ncapture human-like intuitions about players\u2019 latent traits.\n13\n2.2\nUnderstanding language with probabilistic reasoning\n2\nOVERVIEW OF THE KEY IDEAS\nQuestions seek information in the face of uncertainty about the world; e.g., \u201cWould Josh beat Gabe\nif they played again?\u201d In our framework, we translate questions into query statements in Church, which\nevaluate the quantity of interest. Calling query triggers a probabilistic computation that simulates possible\nworlds under the model, constrained by any observations so far. The query expression is evaluated in each\nsimulated world, yielding multiple samples that form a posterior distribution over the value of interest.\nThroughout the examples in this work, we freely interleave query and condition statements, much as\nquestions might occasionally arise between statements of fact in a natural dialogue. Implementationally,\nthis behavior is achieved through a read-evaluate-print loop (REPL) inspired by Venture\u2019s (V. Mansinghka,\nSelsam, & Perov, 2014), that evaluates queries against all condition statements that have appeared up to\nthat point in the dialogue history. In our model, we assume that the user specifies whether each utterance is\na condition or a query, but LLMs could likely classify unannotated utterances accurately.\nTranslating from natural language to program expressions.\nInspired by the work of Goodman and\nLassiter (2015), if we had some way to translate linguistic utterances into probabilistic program statements, we\ncould perform a wide variety of probabilistic inferences from plain English. Up until recently, however, it was\nunclear how to construct a meaning function sufficiently general to translate open-ended natural language into\nhighly structured expressions compatible with a Church model. Our core observation is that language-code\nLLMs have many of the properties necessary to serve as a useful meaning function: broad-coverage exposure\nto natural language, a robust capacity to model joint language-code text distributions, and the ability to\nquickly grasp domain-specific syntax and semantics from a few examples.\nIn this work, we leverage the few-shot prompting capabilities of one such LLM, the Codex model from\nOpenAI, to induce a translation model from English to Church code. As it turns out, we only need to provide\na small handful of example translations (represented in Fig. 3B) to achieve a variety of interesting behaviors.\nTo translate a new language utterance to Church, we simply concatenate the generative model (full text in\nAppendix A.1.1) and the translation examples (full text in Appendix A.1.2) into a prompt whose final line is\nthe utterance. We then generate from Codex, which, based on the comment-code pattern in the prompt,\ninfers that the completion should be written in Church, using the function definitions and constructs provided\nin the prompt.\nNotice the high degree of variation in phrasing and lexical choice in Fig. 3C; none of the utterances contain\n\u201cwon\u201d or \u201cagainst,\u201d yet Codex still maps these to the won-against function. Here, we start to see some of the\nadvantages of using an LLM over more traditional semantic parsing techniques like CCG parsers (Artzi, Lee,\n& Zettlemoyer, 2015; Artzi & Zettlemoyer, 2013). Because the model is pre-trained on a vast amount of\nlinguistic data, it fluently handles many different kinds of linguistic variation. However, by including the\nChurch generative model in the prompt, we can effectively constrain the output space; the model infers that\nthe generated code should use the functions defined in the generative model.\nAs a semantic parsing tool, this combination of pre-training and prompting manages to achieve broad\ninvariance to spurious linguistic variation while remaining sensitive to wording choices that might affect\nmeaning. We can see this tradeoff at work in Fig. 3C, where the translation uses a negation, closely reflecting\nthe structure of \u201cLio and Alex still could not beat Josh.\u201d Of course, there are multiple aspects of the utterance\nthat this translation does not capture (e.g., \u201cEven working as a team...\u201d suggests that Lio and Alex\u2019s efforts\nwere well-coordinated; as opposed to something like, \u201cStepping on each other\u2019s toes the whole match...,\u201d which\nwould imply the opposite). Our point is not that the LLM translation perfectly captures all aspects of the\nutterance meaning, but rather, that it encodes those that are relevant to and compatible with the domain\nmodel so as to facilitate downstream reasoning.\nReasoning about scenarios with probabilistic inference.\nSo far, we\u2019ve illustrated how we might\ncondition a PLoT model on natural language, but what about reasoning? After hearing the information in\nFig. 3C, we might assume that the player named Josh is quite strong. Exactly how strong is Josh, though?\nAnd how likely is it that he would beat another player who isn\u2019t Lio or Alex? Just as we used Codex\nto translate facts into condition statements, we can use it to translate questions into query statements in\nChurch. The Church inference engine then automatically simulates scenarios (in this case, 1000 times) that\nare consistent with the given condition statements in order to produce an approximate posterior distribution\nover each query.\n14\n2.2\nUnderstanding language with probabilistic reasoning\n2\nOVERVIEW OF THE KEY IDEAS\nBy offloading reasoning from the LLM to the PLoT, we can obtain a much richer picture of our model\u2019s\nbeliefs about the world (Fig. 3D). While the LLM alone can only respond with textual statements like \u201cJosh\nis very strong,\u201d Church gives us an entire probability density over Josh\u2019s strength (on expectation, he is\na little less than one standard deviation above the average strength = 50). Likewise, we can easily obtain\na distribution over the outcomes of a Gabe-Josh match (given Josh\u2019s strong track record, our model finds\nGabe\u2019s chances slim, at 23.90%). Critically, Church is doing much of the heavy lifting of inference in the\nbackground in order to produce these posterior distributions.\n(condition (won-against '(gabe) '(josh)))\nIn a huge upset, Gabe managed to best Josh in the \nfourth round.\n(query (strength 'gabe))\nHow strong is Gabe?\n(condition (> (laziness 'josh) 0.5))\nJosh has a propensity to slack off.\n(condition (< (laziness 'josh) 0.1))\nJosh is rarely lazy.\nFigure 4: Reasoning about a pair of hypothetical scenarios with language-code translation. In a world where\nJosh is often lazy, Gabe\u2019s win is counteracted by a high likelihood that Josh threw the match. Conversely, in a\nworld where Josh is rarely lazy, Gabe\u2019s win is surprising and suggests a high strength value. Rational meaning\nconstruction with an LLM appropriately resolves the linguistic meaning of these two scenarios, selecting\nreasonable probability parameters for the conditioning statements. Meanwhile, probabilistic inference about\nGabe\u2019s strength is finely sensitive to the implications of these competing hypotheses.\nIn addition to providing useful interpretability, reasoning in Church models is sensitive to each new piece\nof information. Much like human learners, Church models can flexibly update their beliefs when presented\nwith low-probability or unanticipated events. Picking up our tug-of-war saga, consider the plot twist in Fig. 4:\nIn a huge upset, Gabe managed to best Josh in the fourth round.\nHow might this new information shape our interpretation of the match 4 outcome? If Josh is likely to be\nlazy, then it\u2019s possible that Gabe simply got lucky and wasn\u2019t so strong after all. If, on the other hand, Josh\nis rarely lazy, we might start to regard Gabe as particularly strong. In Fig. 4, we can observe how Church\nreasons about these two possibilities, shifting the probability density over Gabe\u2019s strength left if Josh is likely\nlazy and right if Josh is rarely lazy.\nNote how, in order to translate a phrase like \u201cJosh has a propensity to slack off,\u201d Codex must choose\na particular probability threshold. This choice is arbitrary and, while there is no \u201ccorrect\u201d answer, we see\nthat Codex is able to choose valid probability values between [0, 1] that feel appropriate to the wording: a\n15\n2.2\nUnderstanding language with probabilistic reasoning\n2\nOVERVIEW OF THE KEY IDEAS\n\u201cpropensity to slack off\u201d doesn\u2019t necessarily imply that someone slacks off all the time, while, in contrast,\n\u201crarely lazy\u201d offers more certainty. Indeed, across many different contexts, we observe that Codex is able\nto pick reasonable parameter values that respect both the language and the parametrization of defined\ndistributions. We consider these inferences to represent a form of \u201camortized pragmatics\u201d (Goodman &\nLassiter, 2015), which we will revisit in Section 5.\nPutting it together: the power of probabilistic reasoning.\nWe conclude this section with a final\nexample that underscores the flexibility of our framework to model complex reasoning from language and\nforeshadows multiple themes that we will revisit later in the paper. Consider the dialogue in Fig. 5, in which\nthe students and faculty team up to face one another. The interlocutor poses two questions: \u201cIs Gabe stronger\nthan the weakest player on the faculty team?\u201d and \u201cWho would win in a match between the students and the\nfaculty?\u201d As we saw in the prior tug-of-war examples, the answers to both of these questions are expressed\nas probability distributions derived from simulation of the generative tug-of-war model. Moreover, in both\ncases, the introduction of new information flips the model\u2019s belief state in a way that aligns with human\nintuitions. In this way, the PLoT framework is natively capable of defeasible inference\u2014a phenomenon of\nhuman reasoning that was of great interest to early AI pioneers of non-monotonic logics (Ginsberg, 1987;\nMcCarthy, 1980).\nA key advantage of our framework is that achieving these kinds of defeasible and flexible inferences from\nnatural language reduces to grounding utterances into appropriate condition and query statements. While\nthe observations and questions in Fig. 5 are semantically more complex than those that appeared in the\nprior examples, and though there are many degrees of freedom involved in the translation problem, we\nconfirm that an appropriately-prompted LLM can produce translations that intuitively capture the meaning\nof each utterance with respect to the tug-of-war domain. Moreover, as we saw in Fig. 4, Codex is able\nto amortize certain pragmatic inferences in resolving \u201cpretty strong\u201d to a threshold of strength > 60, \u201creal\nslackers\u201d to a threshold of laziness > 0.9, and \u201cseveral of the faculty\u201d to count >= 3. How far can we go with\nthese kinds of amortizations? Throughout Section 3 and Section 4, we will see examples of context-driven\namortizations across different domains; and in Section 5, we will regroup to discuss how these different\nexamples of amortization might inform our theories of language understanding and pragmatics.\nIn this dialogue, we also give a preview of define, a powerful construct in our framework that is discussed\nin depth in Section 4. Just as people come up with terms like \u201c20th-century pragmatists\u201d or \u201cMeatless\nMonday\u201d to pick out entire hierarchies of people, things, and events, a core feature of the probabilistic\nLoT is the ability to define new concepts that can later be referenced symbolically. In the Fig. 5 dialogue,\nlanguage about team structures defines two new concepts, faculty-team and student-team, that facilitate\nconcise translation of language like, \u201cIs Gabe stronger than the weakest player on the faculty team?\u201d Moreover,\nwhile faculty-team is a static list, other defined concepts can ground out in functions that take arguments. In\nfact, stronger-than?, which is defined in the prompt (Appendix A.1.2), is one such example, illustrating how\nprogramming languages are well-suited to capture the infinite productivity of language that arises through\nstructured composition. Through this lens, we can start to imagine how our tug-of-war world model might\nbe expanded to ground many new kinds of language:\n\u2022 The tug-of-war tournament is organized into three leagues for novices, amateurs, and professionals.\nIn order to be considered a professional, a player must win 20 one-on-one matches against other\nprofessionals.\n\u2022 Players often get increasingly tired over the course of a tournament, though some players have more\nstamina than others.\n\u2022 The tournament has an entry fee of $20 per contestant and a grand prize of $10,000 for the winning\nteam.\nHow can we grow our world models to incorporate new language, or even construct new world models entirely\nfrom scratch? In Section 4, we revisit the tug-of-war domain with an eye to precisely these questions.\nConclusions.\nAs an introductory example, the tug-of-war domain serves as a minimal illustration of the\nkind of reasoning from language that our framework is concerned with. Our goal here was to build intuition\n16\n2.2\nUnderstanding language with probabilistic reasoning\n2\nOVERVIEW OF THE KEY IDEAS\n(define faculty-team '(jacob josh noah vikash))\nThe faculty team is made up of four players: Jacob, \nJosh, Noah, and Vikash.\n(define student-team '(alex gabe lio ben ced))\nThe student team consists of Alex, Gabe, and Lio, plus \ntheir labmates, Ben and Ced.\n(query\n(stronger-than?\n'gabe (argmin strength faculty-team))\nIs Gabe stronger than the weakest player on the faculty \nteam?\n(condition\n(all\n(map\n(lambda (player) \n(> (strength player) 60)) \nfaculty-team)))\nAll of the faculty are pretty strong.\nIs Gabe stronger than the weakest player on the faculty \nteam?\n(query\n(won-against student-team faculty-team)\nWho would win a match between the students and the \nfaculty?\n(condition\n(>=\n(count\n(map\n(lambda (player) \n(> (laziness player) 0.9)) \nfaculty-team))\n3))\nDespite their strength, several of the faculty are real \nslackers.\nWho would win a match between the students and the \nfaculty?\nFigure 5: In this final tug-of-war dialogue, natural language plays three interlaced roles in interfacing with\nthe language-of-thought. Definitions (purple) introduce new concepts, such as specific player-teams, that can\nlater be referenced symbolically. Observations (blue) translate into condition statements that probabilistically\nconstrain the world state, sometimes amortizing the resolution of linguistic ambiguity (e.g., \u201cpretty strong\u201d\nor \u201creal slackers\u201d). Finally, questions (green) translate into queries that trigger inference by probabilistic\nsimulation over possible worlds that is both sensitive to and consistent with prior definitions and observations.\n17\n2.2\nUnderstanding language with probabilistic reasoning\n2\nOVERVIEW OF THE KEY IDEAS\nfor our general approach: by translating natural language into condition and query statements as inputs to\na probabilistic inference engine, we can achieve forms of reasoning from language that are consistent with\nrespect to a mental model of the world. Nonetheless, in scaling this approach beyond the toy domain of\ntug-of-war, many questions arise. How does probabilistic inference relate to models of relational and deductive\nreasoning of the sort that classical AI approaches excel at? How do we ground linguistic meaning in the\nvisual and physical world? And how does language understanding inform our actions and interactions with\nother agents through goal-directed planning? In Section 3, we will progressively expand our scope to touch\non each of these questions and show that, in each case, new kinds of language understanding and reasoning\ncan be naturally incorporated into our framework.\n18\n3\nWORLD MODELS\n3\nUnderstanding and reasoning about language with world models\nIn this section, we illustrate how the general framework we propose in Section 2 can be applied and extended\nto integrate natural language with core domains of human-like thought. In each, we build on the idea that\nlanguage that conveys observations and questions about uncertain situations, constructing meanings from\na generative world modeling program that supports probabilistic reasoning. In Section 3.1, we show how\nthis approach can be integrated to understand language that conveys structured, logical lexical relations. In\nSection 3.2, we show how generative programs that support perceptual and physical simulation can be used\nto ground language about scenes into visual world. Finally, in Section 3.3, we consider language about agents\nwith preferences and goals, and show how we can make meaning from sentences with respect to a generative\nprogram that supports planning.\n3.1\nLanguage for logical and relational reasoning\nIn the previous section, we examined how translation from natural language into the probabilistic language of\nthought naturally captures a certain form of reasoning in which uncertainty plays a key role. How does this\nframework relate to earlier computational theories of reasoning, such as classical AI approaches to logical and\nrelational reasoning (Russell & Norvig, 2021)? Historically, systems like Prolog (Colmerauer, Kanoui, Pasero,\n& Roussel, 1972; Philippe, 1972) were designed for similar goals to ours here, to allow people to directly\ninteract with computers via natural language (French, originally), specifying only the background knowledge\nand goals for computation without the algorithmic details (Colmerauer & Roussel, 1996). In this section, we\ndemonstrate how the PLoT not only fully supports the style of deductive, logical reasoning characteristic\nof classical AI, but extends it to support inductive inferences as well. Moreover, we argue that many kinds\nof real-world reasoning problems that are traditionally modeled using structured logic-based approaches\nactually require a mix of both symbolic and probabilistic reasoning. In doing so, we aim to illustrate how our\napproach of translating from natural language to the PLoT fluidly integrates both kinds of reasoning in a\nway that comes naturally to people, but that has proved elusive for both traditional deductive programming\nsystems and purely statistical language models.\nLanguage about kinship relations.\nSuppose you are again with your friend from Section 2.2, who is\ntelling you about a part of their extended family. \u201cAvery has a sister named Blake, and their father is named\nCharlie,\u201d your friend says. Immediately, you start to sketch a picture in your mind of this family, which\nyou can update on-the-fly as you get more information: \u201cCharlie is the grandfather of Dana.\u201d At this point,\nyou can infer that one of Charlie\u2019s kids is also Dana\u2019s parent, but which one? In the absence of additional\ninformation, it\u2019s a toss-up between Avery and Blake, with some outside chance that there could be another,\nunmentioned sibling who is Dana\u2019s parent. Hearing that \u201cBlake has two kids\u201d might initially shift your beliefs\ntowards Blake. However, upon learning that \u201cDana is an only child,\u201d you\u2019d have to rule Blake out entirely!\nThis kind of relational reasoning, which freely intermixes deductive and inductive inferences, comes quite\nnaturally to people. How do we make such rich inferences from a relatively sparse sequence of words?\nIn this section, our domain of interest will be kinship: relationships between people in a family. The\nkinship domain provides fertile ground for the study of logical reasoning for several reasons. First, during\ndevelopment, one of the first domains where we learn about logical relations is in describing families (Elkind,\n1962; Piaget, 1951). Language has evolved to describe family structures in highly economical terms that\nnaturally express composition (e.g., my mother\u2019s father is my grandfather) and symmetry (e.g., if Avery is\nmy cousin, then I am Avery\u2019s cousin; together, we are cousins). Nevertheless, while certain kinship references\nare relatively straightforward (e.g., \u201cBlake\u2019s mother\u201d), others involve ambiguity (e.g., \u201cBlake\u2019s uncle\u201d could\nrefer to the brother of either of Blake\u2019s parents; or even, perhaps, a close older male who is not related\nby blood or marriage). Finally, kinship reasoning freely intermixes deductive and inductive inferences: for\ninstance, \u201cCharlie has a grandson named Dana\u201d deductively implies the existence of a child of Charlie who is\nalso a parent to Dana; and it inductively implies that Charlie was possibly partnered at some point, such\nthat Dana might have another grandparent in the picture. Traditional logical accounts of reasoning in this\ndomain capture the deductive inferences but not the inductive inferences in cases like this. People, in contrast,\nroutinely make statements such as \u201cThis is Kendall, the partner of Avery\u2019s niece\u201d with the expectation that\nothers will draw roughly the same inferences they would in building a mental model of this family: Avery has\n19\n3.1\nLanguage for logical and relational reasoning\n3\nWORLD MODELS\nperson-id: person-2\nname: blake\ngender: female\nparent-1-id: person-0\nparent-2-id: person-1\npartner?\n(flip 0.5)\n(random-choice\n'(avery blake charlie...))\n(random-choice\n'(male female))\nperson-0\nperson-1\nperson-2\nperson-3\nperson-7\nperson-4\nperson-5\nperson-6\nn-children?\n(min (geometric 0.5) 3)\nCharlie is Blake\u2019s father.\nBlake\u2019s dad is named Charlie.\n(father-of? 'charlie 'blake)\nBlake has three children.\nBlake has 3 kids.\n(= (length (children-of 'blake)) 3)\nBlake\u2019s brother has a son named Dana.\nBlake has a brother whose son is named Dana.\n(exists\n(lambda (x) (and\n(brother-of? x 'blake)\n(son-of? 'dana x)))\n(i) Generative domain theory of family trees\n(ii) Translations into LoT predicates\nFigure 6: Illustration of a simple kinship domain theory and conceptual system implemented in Church. (i)\nThe generative model specifies a process by which individuals form couples and have children to form family\ntrees. Each tree represents a \u201cpossible world\u201d in which certain relationships hold. (ii) These relationships\nare expressed using predicates in a conceptual system that supports quantificational logic and composition,\ngiving rise to an expressive domain semantics that aligns well with natural language.\na brother or sister, and that sibling has a female child, and Kendall is that person\u2019s partner. In sum, the\nkinship domain offers a rich set of relations and possible inferences, and comes equipped with an extensive\nnatural language vocabulary, making it an ideal playground to explore our translation hypothesis.\nWorld models of kinship as probabilistic generative programs.\nOwing to the richness of the domain,\nrecent years have seen a steady interest in computational cognitive models of various aspects of kinship,\nranging from development and acquisition of kinship terms across cultures (Mitchell & Jordan, 2021; Mollica\n& Piantadosi, 2022), tradeoffs in communicative efficiency in natural (Jones, 2010; Kemp & Regier, 2012)\nand artificial (K. Smith, Frank, Rolando, Kirby, & Loy, 2020) kinship systems, and probabilistic inferences\nabout kinship relations from sparse evidence (Katz, Goodman, Kersting, Kemp, & Tenenbaum, 2008). In\nthis work, our primary interest is in how people represent and reason about kinship relations conditioned\non language. Following Katz et al. (2008), we construct an intuitive domain theory of kinship using a\nprobabilistic generative model and a small number of rules that form a conceptual system.\nAs in Section 2.2, our kinship domain theory is expressed as a generative model in Church. In the Bayesian\ntug-of-war, the generative model consisted of random variables over continuous quantities like strength and\nlaziness. In contrast, in this section, our generative model specifies a series of discrete random choices that\ndescribe events in a family\u2019s genealogy: people are born, find partners, have children, and the process repeats.\nAll of these events involve random choices that shape the makeup of the family tree.\nFig. 6 (i) shows a schematic of the kinship generative domain theory. When a person is born, they are\nassigned a unique person-id, a name1 sampled from a list of gender-neutral names, and a gender sampled\nfrom {male, female}. Next, with fixed p = 0.5, the person partners with a new individual from outside the\nfamily. Finally, if partnered, the couple has n = {0, 1, 2, 3} children, with the number of kids drawn from a\ngeometric distribution (p = 0.5). This process repeats recursively until a full family tree is generated. To\nsupport efficient inference using Church\u2019s generic sampling algorithms, we cap the trees at 3 generations\nand limit each couple to 3 children. Further implementation details of the generative model can be found in\nAppendix A.2.1.\n1For simplicity, names uniquely reference individuals in the tree, so as to avoid confusing scenarios like \u201cAvery is the mother\nof Avery.\u201d Additionally, for efficiency of inference, the only names that are assigned are ones that are used in the conversational\ncontext.\n20\n3.1\nLanguage for logical and relational reasoning\n3\nWORLD MODELS\nAs with any computational model of a social phenomenon, this toy kinship model is reductive of many\nimportant nuances of identities and relationships. For instance, while the model includes both same- and\nopposite-gender couples, these couples never split, so step-relations aren\u2019t well-captured. While these kinds of\ncompromises are designed to keep inference tractable, still others stem from limitations of the language itself.\nFor example, many colloquial English kinship terms are gender-binary (e.g., mother, grandfather, daughter),\nso instantiating them as truth-conditional predicates coerces the generative model towards traditional gender\nassignments. Similarly, many English names carry strong gender associations, which NLP systems trained\non large linguistic corpora pick up on (Caliskan, Bryson, & Narayanan, 2017; Grand, Blank, Pereira, &\nFedorenko, 2022). In our examples, we intentionally select gender-neutral names (e.g., Avery, Blake, Charlie,\nDana) to emphasize that these naming-based gender inferences are deliberately not part of the reasoning task.\nTo summarize, language both reflects and constrains our intuitive theories of complex domains like kinship\n(Sapir, 1929; Whorf, 1956; c.f. Gentner & Goldin-Meadow, 2003 for a review of contemporary perspectives\non linguistic relativity), and these tradeoffs manifest concretely in the toy model presented in this section.\nFortunately, where this initial \u201coff-the-shelf\u201d kinship model lacks social and cultural nuance, our framework\noffers opportunities to extend and modify these areas. In section Section 4.1, we look at ways of growing our\nkinship model to include concepts from non-English-speaking cultures and more inclusive concepts of gender.\nRelational meanings as program statements.\nGiven a generative model of family trees, we can define\na rich conceptual system to make statements about relationships between individuals. Our conceptual system\nconsists primarily of a dozen-odd derived predicates that are binary operators over pairs of names; e.g.,\n(father-of? 'charlie 'blake) is true iff Charlie is the father of Blake in a particular tree instance.2 These\nderived predicates build on a small number of low-level accessor functions that operate directly on nodes\nin the tree data structure. For instance, (children-of 'blake) returns a list of names corresponding to the\nchildren of Blake in the tree. Finally, our conceptual system includes several higher-order functions, like\nmap-tree, filter-tree, and exists that take custom predicates as inputs and return a boolean. These functions\nfacilitate the expression of a rich compositional semantics by allowing for compound predicates containing\nconjunctions and disjunctions. Fig. 6 (ii) illustrates several examples of the kinds of statements that can be\nmade using combinations of derived predicates, low-level accessors, and higher-order functions. The full set\nof definitions making up the conceptual system is given in Appendix A.2.3.\nTranslating from language to program expressions.\nAs in Section 2.2, we use a handful of paired\nnatural language / code examples (Appendix A.2.4) to induce a meaning function via Codex. Because the\nprompt also includes the generative model source code and the full set of derived predicates, the LLM is able\nto resolve statements like \u201cBlake has two kids\u201d to the appropriate function (in this case, children-of) using\nthe available definitions. Moreover, we observe zero-shot generalization to linguistic constructs that are not\nexplicitly defined in the prompt, such as the concept of an \u201conly child\u201d (Fig. 7).\nPutting it together: Reasoning from language about kinship relations.\nWhat is the purpose\nof all of this domain-specific machinery that we have now built up? The answer is two-fold. First, the\ngenerative domain theory compactly captures the key dynamics of our domain, allowing us to reason about a\ncombinatorially vast space of possible family trees. Meanwhile, the conceptual system serves as a higher-level\nprogram interface, defining certain relationships that we would like to be able to talk about. Finally, the\nlarge language model bridges the domain model with natural language, providing a flexible and context-aware\nway to ground language into conditioning and query statements.\nIn Fig. 7, we can see how these components come together to facilitate naturalistic reasoning from language\nabout kinship relations. Each natural language utterance translates to a condition statement in Church that\nserves as a constraint on family trees. With each successive condition, our uncertainty decreases and our\npicture of the family tree in question starts to crystallize. Samples from the conditioned domain theory model\ntherefore serve as hypotheses about possible worlds that are consistent with the information provided through\nlanguage. Furthermore, the distribution over conditioned samples provides a principled way to reason about\n2Note that because our model includes same-gender couples, Blake may have one father, two fathers, or no fathers. Blake\nalso may not exist in the tree in the first place! Crucially, these aspects of the generative model don\u2019t matter to the derived\npredicate, which simply evaluates whether the relationship in question holds somewhere in the tree.\n21\n3.1\nLanguage for logical and relational reasoning\n3\nWORLD MODELS\nA. Language-to-code translation\nB. Family trees sampled from conditioned kinship domain theory\n(condition\n(sister-of? 'blake 'avery))\nAvery has a sister named Blake.\n(condition\n(and\n(father-of? 'charlie 'avery) \n(father-of? 'charlie 'blake)))\nAvery and Blake\u2019s father is named Charlie.\n(condition\n(grandfather-of? 'charlie 'dana))\nCharlie is Dana\u2019s grandfather.\n(condition\n(= (length\n(children-of 'blake)) 2))\nBlake has two kids.\n(condition\n(not (exists\n(lambda (x) \n(sibling-of? x 'dana)))))\nDana is an only child.\nWhich of Charlie\u2019s kids is Dana\u2019s parent?\nWhich of Charlie\u2019s kids is Dana\u2019s parent?\n(query\n(filter-tree\n(lambda (x) (and\n(child-of? x 'charlie)\n(parent-of? x 'dana)))))\nWhich of Charlie\u2019s kids is Dana\u2019s parent?\nProbabilistic inference over sampled worlds\nFigure 7: Kinship reasoning from natural language, backed by a domain theory model in the probabilistic\nlanguage of thought. (A) Natural language utterances about a particular family are readily translated into\nChurch conditioning statements by a LLM. (B) Samples from the conditioned generative domain model are\npossible family trees that adhere to the growing set of constraints (conditioning statements are cumulative).\nReasoning about unknown kinship relations is accomplished through posterior inference against a translated\nquery. With each new piece of information, the model\u2019s beliefs reflect both deductive and inductive inferences.\n22\n3.1\nLanguage for logical and relational reasoning\n3\nWORLD MODELS\nqueries, such as Which of Charlie\u2019s kids is the parent of Dana? Posterior inference (in this case, accomplished\nvia rejection sampling) faithfully reflects various possible configurations and their relative probabilities. For\ninstance, in Fig. 7, after conditioning on Blake has two kids, the model puts > 80% probability on Blake\nbeing Dana\u2019s parent, but also factors in low-probability possible worlds where Avery or a third unnamed\nsibling is Dana\u2019s parent. Yet, despite this confident answer, the model can correctly infer that this same\nprobability drops to 0% in the face of the contradictory information that Dana is an only child. Note that\nthe distributional parser plays a crucial role in this inference by providing a correct interpretation of this\nutterance. Meanwhile, the Church inference engine does the heavy lifting of representing possible worlds and\nreasoning about them in a principled manner.\nFuture directions: Logical and relational reasoning with language models.\nSignificant recent\nattention has been directed towards studying reasoning in LLMs. Typical approaches involve engineering\nprompts so as to induce structured generations in text space that approximate \u201cstep-by-step\u201d reasoning\n(Kojima, Gu, Reid, Matsuo, & Iwasawa, 2022; Nye et al., 2021; Wei et al., 2022). Nevertheless, current\nevaluations find that even with such methods, LLMs are prone to producing unfaithful reasoning chains in\nwhich conclusions do not follow logically from the premises (Golovneva et al., 2022; H. Liu et al., 2023; Lyu et\nal., 2023; Ribeiro et al., 2023). These issues of consistency have motivated several systems that connect LLMs\nto external symbolic inference engines that perform deductive inference using Prolog-style backwards chaining\n(Dalvi, Tafjord, & Clark, 2022; Pan, Albalak, Wang, & Wang, 2023; Weir & Van Durme, 2022). We see this\nwork as closely-related in spirit to our approach, but fundamentally limited to deductive reasoning. (See\nAppendix A.2.5 for a technical explanation of these limitations.) Of course, we make no claim that Church\nor its derivatives are the only languages that can capture human-like relational reasoning. For instance,\nProbLog (De Raedt, Kimmig, & Toivonen, 2007; Dries, Kimmig, Davis, Belle, & De Raed, 2017; Suster et al.,\n2021), a probabilistic extension of Prolog in which deduction rules can be annotated with probabilities, offers\na compelling alternative. Indeed, interfacing ProbLog with a natural language via an LLM-backed meaning\nfunction would constitute a promising instantiation of our rational meaning construction framework. Our\ncore assertion here, and in the rest of this paper, is that representing probabilistic, generative models over\npossible worlds is critical to reasoning coherently about a structured domains.\n23\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\n3.2\nLanguage for visual and physical reasoning\nSensory detail and physical knowledge pervade our everyday language. We can describe and imagine highly\nvisual objects and scenes\u2014a few red mugs on a tabletop, a tall stack of blue plates, a heavy box, and objects\nthat move, bounce, and collide. We flexibly make predictions about physical events (what will happen if a kid\ncrashes into that table stacked with plates?), or infer the underlying physical properties of the world (how\nheavy is that box that no one can lift?), based on situations described entirely in words. As with the other\ndomains we have considered thus far, understanding this language requires integrating over the uncertainty\ninherent to language, like the possible heights picked out by tall and motions picked out by a bounce, as well\nas the uncertainty inherent to how we imagine the physical world itself.\nHow can we so flexibly relate language to our more general perceptual and physical reasoning? In this\nsection, we illustrate how our overarching framework for language understanding can be modularly extended\nto capture both of these capabilities. We begin with perception, extending our framework to integrate a\ngraphics rendering engine to relate linguistic meanings to visual knowledge (Section 3.2.1). We then\nbuild on this approach to integrate a physics simulation engine to further interface between language\nand intuitive, probabilistic physical reasoning (Section 3.2.2). By incorporating these external engines, these\nsections blueprint how computational models that ground linguistic meaning in a PLoT can interface with\nother cognitive modules for perception and physical reasoning.\n3.2.1\nLanguage about visual scenes\nTo illustrate the structured relationship between language and visual knowledge, imagine how we might talk\nabout a very simple domain of scenes (Fig. 8, top)\u2014tables on which someone was placing some household\nobjects (mugs, cans, or bowls) that come in different colors (red, green, yellow, or blue.)\nGiven descriptions of particular scenes (Fig. 8, bottom), most of us can easily picture tabletop scenes\nthat fit these descriptions, updating what we imagine to incorporate arbitrary new information, like that\neverything on the table is blue, and also that there are no mugs, and lots of bowls. We can do this despite\nuncertainty in the language itself\u2014a phrase like lots of bowls leaves open just how many bowls there are,\nthough we have general intuitions that there should be more than one or even two bowls on our imagined\ntable. You can also draw a host of fundamentally probabilistic inferences to answer many arbitrary questions\nabout the scenes you imagine, like how many green mugs there might be, or whether there are more red\nobjects or green ones. The set of scenes you imagine, and the way you answer these questions, is structured\nand compositional at the level of individual objects and their properties (a mug, a green mug, a bunch of\ngreen mugs), and over successive sentences (like there are many red objects on the table, there are just a\nfew green mugs, and there are also at least three green bowls.) The way we talk about scenes like these\nsuggests the level of abstraction with which we mentally represent them. We describe and reason over object\ncategories, lexical properties, numeric quantities, and set relations, and we can easily visualize scenes from\nthese abstract, linguistic descriptions. In contrast, recent evaluations of current multimodal models\u2014large\nlanguage models fine tuned on corpora of images (Ramesh, Dhariwal, Nichol, Chu, & Chen, 2022; Ramesh et\nal., 2021)\u2014suggest that even large models struggle with just these kinds of simple but abstract relational\nconcepts in language, such as producing images consistent with quantifiers like more red things than green\nthings, or relations like a plate on top of a cup (Conwell & Ullman, 2022; Marcus, Davis, & Aaronson, 2022;\nRadford et al., 2019).\nIn this section, we propose that the basic motif outlined in our framework also suggests an alternate\napproach for relating language and visual reasoning. Our architecture draws on the traditions of viewing\nperception as \u201canalysis by synthesis\u201d or \u201cvision as inverse graphics\u201d from cognitive science and classic computer\nvision (Battaglia et al., 2013; Gothoskar et al., 2021; Kulkarni, Kohli, Tenenbaum, & Mansinghka, 2015; Lee\n& Mumford, 2003; J. Wu, Yildirim, Lim, Freeman, & Tenenbaum, 2015a; Yuille & Kersten, 2006). This\napproach frames visual imagination and visual scene understanding as two sides of the same coin, modeling\nvisualization in a mental graphics rendering engine over internal scene representations and perception as\nprobabilistic inference to invert the renderer and thereby recover the physical content of scenes from vision.\nIn this section, we show how this general approach to modeling human perception can integrate cleanly into\nthe framework we have sketched so far, augmenting the probabilistic language of thought with an interface to\na rendering engine so it can serve as a general, flexible intermediary for relating language, world models, and\nvisual scenes.\n24\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\n(define choose-shape\n(mem (lambda (obj-id) \n(pair 'shape (uniform '(mug can bowl))))))\n(define choose-color\n(mem (lambda (obj-id) \n(pair 'color (uniform '(red blue green yellow))))))\n(define generate-object\n(mem (lambda (obj-id) (list \n(pair 'object-id obj-id) \n(choose-shape obj-id) \n(choose-color obj-id)))))\n(define choose-num-objects ...)\n(define generate-objects-in-scene ...)\nEverything on the table is blue.\nThere are no mugs.\nBut there are lots of bowls.\nThere's at least two green cans.\nThere aren't a lot of red mugs.\nAre there more red objects or green ones?\nProbabilistic inference\nSampled scene graphs\nImagine a table with some mugs, cans, or bowls on it, which can \ncome in different colors.\nThere's a bunch of red mugs.\nProbabilistic inference\nGenerative world model of scenes\nobject-1: {\ncolor: red\nshape: mug\n}\nobject-2: {\ncolor: green\nshape: can\n}\nobject-3: {\ncolor: blue\nshape: bowl\n}\nScene 1\nGraphics rendering engine\nobject-1: {\ncolor: red\nshape: mug\n}\nobject-2: {\ncolor: blue\nshape: mug\n}\nobject-3: {\ncolor: blue\nshape: bowl\n}\nScene 2\nobject-1: {\ncolor: red\nshape: mug\n}\nobject-2: {\ncolor: blue\nshape: mug\n}\nobject-3: {\ncolor: yellow\nshape: mug\n}\nScene 3\nReasoning about scenes from natural language\nDialogue A\nProbabilistic inference over possible worlds\nHow many green mugs do you think there are?\n(query (length\n((filter-color green)\n((filter-shape 'mug) (objects-in-scene 'scene)))))\nThere's only a few mugs and bowls, though at least one of each.\n(condition (and\n(<= (length ((filter-shape 'mug) (objects 'scene))) 3)\n(<= (length ((filter-shape 'bowl) (objects 'scene))) 3)\n(> (length ((filter-shape 'mug) (objects 'scene))) 0)\n(> (length ((filter-shape 'bowl) (objects 'scene))) 0)))\nAnd most of the objects in this scene are green.\n(condition (and\n(> (length ((filter-color green) (objects 'scene))) 0)\n(>= (length ((filter-color green)\n(objects 'scene)) (/ (length (objects 'scene)) 2))))\nDialogue B\nDialogue C\nFigure 8: Human language understanding draws on our structured knowledge of the visual world. (Top) A\nprobabilistic generative model describes a prior over tabletop scenes with varying configurations of colored\nmugs, cans, and bowls. Sampled world states describe a scene based on symbolic object concepts. Interfacing\nthis world model with a graphics rendering engine models visual imagination of a given scene. (Bottom)\nLanguage about particular visual scenes can now be translated as before into conditions (blue) and queries\n(green) on the distribution over scenes, which can be rendered into visual scenes that reflect language.\nIntegrating the probabilistic generative model over scenes with a rendering engine.\nTo model\nthe domain of tabletop scenes, we begin with a probabilistic generative model like those in the preceding\nsections. The generative program excerpted at the top of Fig. 8 (purple) describes a prior over the number of\nobjects in a given scene, and the shape and color of each object. This program is similar in many ways to the\nkinship model in Section 3.1, which generates possible family trees as a collection of entities and stochastic\nchoices about each one. Similarly, the generative model in this domain generates a particular scene by\nmaking stochastic choices over the number of objects in the scene (choose-num-objects), then generates each\nindividual object (generate-object) based on stochastic choices over its possible properties (e.g choose-shape\nand choose-color). This basic structure can be augmented in many ways to model more complex scenes,\nwith more variation over possible properties like size or material, hierarchical classes of object categories like\ndishware, cups, and mugs, or hierarchical object structures like a stack of plates.\nEach sample from the generative model in Fig. 8 is a structured, symbolic representation of a particular\nscene state, represented in our particular implementation as a list of object dictionaries that map between\nattribute kinds (like object-shape) and values (like 'mug). These scene states are very simple instances of\nthe many symbolic scene representations used throughout computer graphics and computational models\n25\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\nof human scene understanding, data structures which model the abstract and semantic contents of scenes\n(Armeni et al., 2019; Bar-Zeev, 2003; Clark, 1976; Gothoskar et al., 2021; J. Johnson et al., 2017, 2015;\nZinberg, Cusumano-Towner, & Vikash, 2019).\nWe can now extend this probabilistic generative program so that it expresses not just a distribution over\npossible scene states, but over the visual percepts of each scene. We do so by extending our base probabilistic\nprogramming language with a new function, render, that takes in scene graphs as inputs and calls out to\nBlender, a 3D computer graphics engine.3 Our render implementation builds on the basic capabilities of any\nprogrammable graphics engine. It defines how symbolic object entities with the properties defined in our\nmodel (shapes like mug) are rendered and colored into 3D CAD shapes, and can forward render any sampled\nscene graph into a visual scene with the requisite object types and colors, and overall structure (Fig. 8, top,\nRendered possible worlds). Collectively, this generative model and rendering interface unites the underlying\nbelief distribution over possible scene states with how each of these scenes might look.\nMore broadly, this implementation is intended as a simple, illustrative example of how our framework\ncould be integrated to model many, complex relationships between the objects we talk about in a scene\nand how they look \u2014 recent work in scene understanding, for instance, models variation in lighting, viewer\nangle and distance from the scene, stereo depth sensing, and sources of noise in perception (such as from a\nviewer who only looks briefly at an image, or an imperfect, non-idealized visual sensor) (e.g., in Deng, Zhi,\nLee, and Ahn (2021); Gothoskar et al. (2021); Hughes, Chang, and Carlone (2022); Kulkarni et al. (2015);\nV. K. Mansinghka et al. (2013); Zinberg et al. (2019)).\nGrounded meanings as program expressions.\nBy augmenting probabilistic generative models with a\ngraphics rendering engine, we have now extended our framework to allow language that describes and asks\nquestions about scenes to interface with visual depictions of those scenes.\nIn our simple tabletops scene domain, for instance, we can ground linguistic descriptions of the number,\nkinds, and colors of objects in a scene (Fig. 8, blue) like there\u2019s at least two green cans or a few mugs and\nbowls into probabilistic program condition statements on scene states in the generative model. As with\npreceding sections, the translations shown in Fig. 8 are quite straightforward and interpretable, because the\ngenerative model we have defined expresses compositional predicates on object properties at the grain of\nlanguage. Constraints on objects of specific types, like green cans are translated into a sequence of conditions\non the relevant properties of object entities, successively filtering on the set of objects that are green\n(filter-color green) and then further filtering to the set of objects that are also cans (filter-shape 'can).4\nSampling scene states from the conditioned generative model, and rendering these scenes into images with\nthe render interface, then produces visual depictions that are consistent with any sequence of observations\nmade in language. This approach disentangles reasoning, as probabilistic inference over a structured generative\nmodel, from the perceptual properties of scenes. As with before, we can translate questions like How many\ngreen mugs do you think there are? into probabilistic query expressions. Our approach reasons about these\nquestions as inferences over the distribution of possible scenes, adapting beliefs about the scenes to condition\nsystematically and coherently on sequences of new statements made in language.\nTranslating from language to program expressions.\nAs with the previous sections, we can now\ntranslate actual descriptions and questions about scenes, by using a large language-to-code model conditioned\non the generative domain model and a few example pairs of language and code (see Appendix A.3.1 for the\nfull prompt we provide to condition the language-program model).\nThe translations in Fig. 8 and Fig. 9 generally showcase the local generalizability and flexibility we\nillustrate in the other sections\u2014the translation is robust to conjunction and syntactic variation, differing\nnumbers of object predicates (yellow object, red mug), compositions of object predicates (eg. a few mugs and\nbowls), negations over set quantity (there aren\u2019t any), and comparatives over object sets (more red mugs than\ngreen cans).\n3https://www.blender.org/\n4In our implementation, which can be found in Appendix A.3.1, we derive named color predicates like green over the base\ngenerative model, which samples color properties over a continuous space of RGB values. This implementation suggests a more\ngeneral point\u2014that any number of lexical concepts, such as many more arbitrary color names over the underlying color space,\ncan be derived as symbolic predicates over a richer continuous space reflected in the generative model. A similar approach could\nbe taken for other lexical terms that carve up continuous spaces, such as prepositions like left, center, or near over geometric\nspace.\n26\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\n(condition\n(> (length\n((filter-color red) ((filter-shape 'mug) \n(objects-in-scene 'scene))))\n(length ((filter-color green)\n((filter-shape 'can) \n(objects-in-scene 'scene))))\n(condition\n(>= (length\n((filter-color green) ((filter-shape 'can)\n(objects-in-scene 'scene)))) \n3))\n(condition\n(= (length\n((filter-color yellow)\n(objects-in-scene 'scene))) \n0))\nA. Language-to-code translation\nB. Rendered scenes from conditioned generative model\n(condition\n(>= (length\n((filter-color red) ((filter-shape 'mug) \n(objects-in-scene 'scene)))) \n1))\nThere are more red mugs than green cans.\nThere are also at least three green cans.\nThere aren\u2019t any yellow objects.\nThere is at least one red mug in this scene.\nFigure 9: Each sentence in this sequence (left) translates into a separate, composable condition expressions\nthat updates the underlying generative model over scene states. After each sentence, sampling symbolic scene\nstates from the updated distribution and render-ing them (right) yields images that reflect the prior over\nscenes and are consistent with the information in all successive sentences.\nEven on this relatively simple domain, Fig. 8 and Fig. 9 also showcase ways in which the LLM can\nrepresent conditional inferences from language to program expressions that go beyond simple, literal semantic\nmeanings. These examples build on what we already find in Section 2.2, in which the LLM can contextually\ninterpret vague language like very strong as thresholds on continuous variables in the generative world model.\nIn this domain, find the LLM can translate vague quantifiers (like few, most, aren\u2019t a lot, a bunch, or aren\u2019t\nmany) without explicit program predicates defining each lexical term\u2014the model can directly translate these\nterms into reasonable, interpretable quantities over sets of objects (such as translating only a few to (<= 3)\nobjects). We also find that sampling from the distribution over meanings further supports the idea that\nthe LLM represents a broader distribution over intended meanings, including acceptable semantic variation\nin the interpretation of vague lexical terms. Sampling from the distribution at higher temperatures, for\ninstance, we find that our implementation variously translates most into program expressions that interpret\n27\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\nthis as more than half, or more than 80%, or other greater fractions, of the set of overall objects in the scene.\nThese translations draw on the language-to-code model\u2019s background prior on language itself (we do not\nprompt it with examples of these particular phrases), its amortized understanding of how these phrases\nrelate to continuous, named variables in code (like length of a set of objects), and the particular context\nof the generative world model itself (which defines the prior over numbers of objects that determines the\ncontext-specific scale of these graded quantifiers.)\nTranslations of vague quantifiers like these have been handled in classical semantics and recent accounts\nas explicit, pragmatic and probabilistic inferences based on context-specific priors\u2014the acceptable quantity\nmost people would infer for many mugs on a table is intuitively very different from the quantity intended by\nmany grains of sand (Edgington, 1992, 1997; Graff, 2000; Lassiter & Goodman, 2017). The results we show\nhere provide further evidence that LLMs can often amortize many of these inferences, to directly predict\ncommon interpretations from language. As we discuss in Section 5, future work might explore more fluid,\njoint integrations of these approaches to inferring meanings, trading off between the amortized interpretations\nthe LLM can produce and more explicit probabilistic inference, such as conditioning on other information\nin language. Learning that Sally is a wholesale porcelain supplier who owns thousands of mugs in a nearby\nwarehouse might lead you to infer an updated meaning of Sally has many mugs, but is a complex inference\nthat we might not expect to be amortized in an LLM from the background distribution of language and\ncommented code.\nPutting it together: Reasoning from language about visual scenes.\nTaken together, the examples\nin Fig. 8 show how this approach naturally extends the components of this framework\u2014the ability to describe\npossible worlds in language, flexibly updating a background distribution of beliefs within a conditioned\ngenerative model, and query this model to draw probabilistic inferences\u2014to also ground out in visual scenes.\nThe more extended example in Fig. 9 highlights the more granular, intuitive way in which the distribution\nover scenes changes to reflect successive new sentences, updating a flexible distribution over scenes that still\nremains consistent with all of the previous observations.\n3.2.2\nLanguage about dynamic physical scenes\nWhen we talk about a scene, however, we describe more than just the colors and shapes of objects sitting on\na table. We talk in verbs, describing events unfolding in the changing, physical world around us. Consider,\nfor instance, descriptions of another set of tabletop scenes\u2014ones that just involve a red object placed to the\nleft of a blue one on a table (Fig. 10). These scenes are initially even simpler than our tables of cans and\ndishware, but still afford a range of dynamic and physics-specific descriptions.\nYou can easily imagine, for instance, what would happen if someone pushed the red ball gently to the\nright\u2014you might say that it would bump into the blue ball, and you could likely imagine how fast the blue\nball would be moving as a result. You can infer how these scenes would change if someone pushed the red\nball much harder, as if shooting a billiard ball, or tapped it even more gently, nudging it forward with their\nfinger, so that perhaps it wouldn\u2019t collide with the blue ball at all. These inferences are sensitive to many\nother properties of the scene, and of these objects, that we could describe in language, like whether the red\nball is really heavy, or the blue ball is very light, or at least much lighter than the red one. If we changed the\nobjects in the scene, and now placed a red block to the left of the blue one, your intuitive understanding of\nhow different shapes relate to different degrees of friction would again change how you might see these scenes\nplay out in your mind, and how you might answer questions about their collision and motion.\nAs adults, we have a deep, general understanding of how physical objects move and behave, and extensive\ndevelopmental evidence suggests that well before we acquire language, we understand many core physical\nprinciples that govern our world (Baillargeon, 2004; Hespos & Baillargeon, 2008; Rips & Hespos, 2015; Spelke,\n1990; Spelke, Gutheil, & Van de Walle, 1995; T\u00e9gl\u00e1s et al., 2011). A productive line of computational\ncognitive models, in turn, has modeled human physical understanding as probabilistic inferences over a\nmental physics engine, modeled as programmable physics simulation engines (Battaglia et al., 2013; de\nAvila Belbute-Peres, Smith, Allen, Tenenbaum, & Kolter, 2018; Lake et al., 2017; Ullman et al., 2017; Yi et\nal., 2019) like those used in video games, computer animation, and robotics (Coumans & Bai, 2016; Erez,\nTassa, & Todorov, 2015; Todorov, Erez, & Tassa, 2012).\n28\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\n(define choose_shapes...)\n(define get_initial_color ...)\n(define choose_mass ...)\n(define get_initial_x...) ...\n(define generate-object\n(mem (lambda (obj-id) (list\n(pair 'object-id obj-id) (choose_shape obj-id)\n(choose_color obj-id) (choose_mass obj-id)...)))) ...\n(define generate-initial-scene-state...) ....\n(define simulate-physics (mem (lambda (scene total_t delta_t)\n(let check_collisions ...)\n(let generate_next_scene_state_at_time...) ....))))\nNow, imagine that the red ball is quite light.\nSampled scene graphs\nImagine a table with a red object placed to the left of a blue one. \nBoth objects are the same shape. We can push the red object.\nGenerative world model of dynamic physical scenes\nobject-1: { color: red, shape: sphere, mass: 0.2, x: -3, v: 1.0,   \na: -0.05, force: 1.0 ...}\nobject-2: { color: blue, shape: sphere, mass: 3.0, x: 0, v: 0.0,\na: 0.0, force: 0.0...}\nScene 1\nReasoning about physical scenes from natural language\nSituation A\nImagine that the red object is a ball, and is pretty heavy.\n(condition (get_singleton_object (lambda (object) (and\n((is_color? red) object)\n((is_shape? 'sphere) object)\n(> (get_attribute object 'mass) 2)))))\nSituation B\nAnd the blue object is also a ball, but is fairly light.\n(condition (get_singleton_object (lambda (object) (and\n((is_color? blue) object)\n((is_shape? 'sphere) object)\n(< (get_attribute object 'mass) 2)))))\nNow imagine that the red ball is pushed forcefully to the right.\n(condition (get_singleton_object (lambda (object) (and\n((is_color? red) object)\n((is_shape? 'sphere) object)\n(> (get_attribute object 'f0) +6)))))\nThe red ball hits the blue one.\n(condition (get_singleton_object (lambda (object_1)\n(get_singleton_object (lambda (object_2)\n(exists_event (lambda (event)\n(and ((is_color? red) object_1) \n((is_shape? 'sphere) object_1)\n((is_color? blue) object_2) \n((is_shape? 'sphere) object_2)\n(is_participant_of_event? event object_1)\n(is_participant_of_event? event object_2)\n(is_event? 'collision event)))))))))\nHow fast does the blue ball move  after the collision?\nNow how fast does the blue ball move after the collision?\nPhysics simulation engine\nobject-1: { color: red, shape: cube, mass: 1.0, x: -3, v: 4.0, \na: -0.1, force: 2.0 ...}\nobject-2: { color: blue, shape: cube, mass: 3.0, x: 0, v: 0.0,\na: 0.0, force: 0.0...}\nScene 2\nobject-1: \n{...,  \nx: -2.5,\nv: 0.95...}\nt=1\nobject-1: \n{..., \nx: -2.0, \nv: 0.9...}\nt=2\nobject-1: \n{..., \nx: 0.0, \nv: 0.01...}\nt=10\n...\nobject-1: \n{..., \nx: -1.0, \nv: 3.0...}\nt=1\nobject-1: \n{..., \nx: 0.0, \nv: 2.0...}\nt=2\nobject-1: \n{..., \nx: 0.0, \nv: -1.0...}\nt=10\n...\nAnd the blue ball is somewhat heavy.\nThe red ball is pushed gently to the right.\nImagine that all of the objects are blocks.\nSituation C\nThe red block is still pretty light,\nAnd the blue block is quite heavy.\nWe push the red block lightly to the right.\nHow fast does the blue block move after it is bumped by the red one? \nGraphics rendering engine\nFigure 10: The way we talk about the world also draws on our intuitive physical knowledge. (Top) A\nprobabilistic generative model describes a prior over tabletop scenes with a red and blue object placed side\nby side, of varying mass and shape. Integrating a physics simulation engine into this generative model\nallows this model to express a prior over dynamic scenes, modeling how each possible scene unfolds over time\nas differing initial forces are applied to the red object. (Bottom) Language about possible physical scenes\ncan again be translated into conditions (blue) and queries (green) on the distribution over dynamic scenes.\nRendering these scenes produces scenes that reflect these conditions, and inference over the simulations allows\nthe framework to answer queries contingent on the properties of the objects described in language.\nAs with the previous example on visual scenes, our goal in this section will be to illustrate how the\noverarching framework we have described in this paper can integrate language with other domains of human\nreasoning\u2014perception and visual imagination, or intuitive physical reasoning. By translating language into\na probabilistic language of thought, we can relate the semantics of language to these other, well-studied\ncomputational and cognitive modeling approaches, using probabilistic programs as the underlying interface\nbetween language, inference, and these engines for perception and physical simulation.\nThis approach is closely related to other recent work from the AI literature, most notably R. Liu et al.\n(2022), which also extends large language models with physics engine to ground natural language in physical\nsimulation. By incorporating an interface to physics within a general probabilistic programming language,\nwe show here how these approaches can model the commonsense, probabilistic judgements we make about\n29\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\neveryday physical language\u2014including with respect to uncertainty and vagueness in language about the\nunderlying world state, or combined with inputs from visual reasoning, as discussed in the prior section.\nIntegrating the probabilistic generative model over scenes with a physics engine.\nTo model\nlanguage about the example scenes we described here\u2014red and blue balls, or blocks, placed on a tabletop\n(Fig. 10)\u2014we implement a probabilistic generative model that is similar by design to the previous visual\nscenes domain (a very short excerpt appears in Fig. 10, and the full model appears in Appendix A.3.3).\nThis generative program describes a prior over the possible properties of the objects initially set on a table,\nmodeling scenes as a collection of objects in which each individual object is again generated (generate-object)\nbased on stochastic choices over its possible properties (e.g choose_shapes). In this domain, however, we\nalso model an explicit prior over the physical properties of each object, such as its mass (choose_mass), and\nthe relationship between shape and friction (as a simple get_friction_constants function returns different\nconstants, with a higher constant for blocks than spheres).\nAs with the visual scenes example, each sample from this generative model again returns a structured,\nsymbolic representation of a possible initial scene state, as a list of object entities that represents each object\nas a dictionary-like mapping from attribute kinds. This dictionary also stores each object\u2019s initial kinematic\nstate, such as its position, velocity, acceleration, and any forces applied to it. To model the various ways we\ncan push the objects around, our generative model over scene also implements a stochastic function over\npossible initial forces (choose_initial_forces) applied to an object.\nTo model how each possible world unfolds as a dynamic scene over time, we implement a simulate_physics\nfunction (Fig. 10) that integrates the basic functionality of any programmable physics engine into the\nprobabilistic model\u2014this function takes in a scene state that specifies the relevant physical properties of\nobjects, and returns a sequence of scene states forward simulated in time under the laws of physics. In our\nimplementation, this sequence is a list of scene states at each timestep, each which contains its own set\nof the objects and their properties with relevant kinematic properties (like position, velocity, acceleration)\nupdated at each timestep. The physics model we use in our example is simple enough that we implement it\nfully within the body of the probabilistic program itself (see Appendix A.3.3) for illustrative purposes\u2014our\nsimulate_physics updates each object at each timestep under the basic kinematic laws of Newtonian mechanics,\nincludes a simple implementation of static and kinetic friction under gravity, and models simple collisions as\nimpulse exchanges in momentum.\nThe rendered simulations we show in Fig. 10 also showcase the interplay between these modular, API-like\ninterfaces integrated into a probabilistic language of thought\u2014combined with the render interface from the\nprevious section, we can not only simulate underlying physical scene states, but visualize them by rendering\neach individual scene state in the sequence over time. Collectively, this model now captures a prior over\ntabletop scenes, models how any given scene in the distribution unfolds dynamically under physics, and\ncaptures how each scene appears visually over time.\nGrounding physical language in program expressions.\nBy extending the underlying probabilistic\nworld model to interface with a physics engine, we can ground the semantics of language about the physical\nworld in intuitive, human-like physical reasoning modeled by the physics simulation engine over world states.\nDescriptions of the physical properties of objects, for instance, like the blue ball is not very heavy (Fig. 10)\ntranslate into conditions on the mass property of an object in the world state, and maintain uncertainty\ninherent to language\u2014phrases like very heavy translate into conditions that threshold a continuous distribution\nof possible masses. As in the visual scene example, sampling from the conditioned generative model produces\ndynamic scene simulations that reflect language. Descriptions of heavy blue balls, or red blocks that are\nrelatively light, or scenes in which a red ball is pushed forcefully, or in which a red block bumps into a blue one,\nall connote sets of scenes that model explicit, physical simulation. In turn, queries about distributions over\nphysical scenes (like how fast a heavy blue ball will move after it is bumped) reflect probabilistic inferences\nthat condition on all of these relevant descriptions in language, estimated by sampling and running physical\nsimulations over the possible world states.\nIn this example, we highlight an approach to translating verbs and descriptions of physical events (the red\nball pushed forcefully to the right, the red ball hits the blue ball) that grounds them directly over continuous\nvariables in our world model. In Fig. 10, for example, our implementation translates pushed forcefully to the\nright into a condition expression that picks out a distribution of initial forces, over a space of continuous force\n30\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\nvectors with direction and magnitude, as the meaning of push in a physical world. Similarly, we translate hits\nwith respect to collisions simulated by the physics engine between the two object entities.\nIn our appendix, however, we also implement and show how a discrete event semantics can also be\nconstructed over variables in the physics engine, to highlight potential future connections between our\nimplementation and more classical event semantics representations. Neo-Davidsonian event semantics and\nrelated approaches (D. Davidson & Rescher, 1967; Parsons, 1990), for instance, have long modeled events in\nlanguage with discrete event entities and lexical event predicates (eg. is_hitting) that describe particular\ncategories of events in time. Prior work in classical event semantics has also considered how discrete event\nrepresentations relate to underlying physical forces (Talmy, 1988), with particularly close connections to\nlexical semantics approaches (Jackendoff, 1985; Levin, 1993; Pinker, 1984; Schuler, 2005; Talmy, 1988) that\nrealize verb meanings into cognitively-grounded physical concepts of motion and forces.\nOur implementation concretely realizes these semantic events and predicates as functions derived entirely\non top of a fully realized, continuous world state modeled in a physics engine\u2014is_hitting, for instance, is an\nevent derived on top of the collision mechanics in the underlying physics engine. Other event predicates, like\nis_moving, or is_resting, can be similarly as thresholds on continuous kinematic properties (here, velocity)\nrepresented in the world state. Our broader goal is to show all of these can be broadly constructed over a\nprobabilistic language of thought, which grounds out concretetly with respect to states in an implementable\nphysics engine.\nTranslating from language to program expressions.\nAs with our visual scenes example, the translations\nwe show in Fig. 10 are again chosen to illustrate the generalization and amortized inferences that the language-\nto-code LLM can make. Much like vague quantifiers, we find that the context-conditioned LLM can directly\ninfer reasonable meanings for graded terms that pick out thresholds over a numeric distribution\u2014translating\nphrases like not very heavy, pretty heavy, and pretty light directly into reasonable, context-specific thresholds\non continuous masses, or pushed gently and pushed forcefully into thresholds on forces. Again, we see\ninteresting future grounds for further integrating these kinds of amortized inferences with more explicit,\nprobabilistic inference mechanisms for deriving them\u2014such as to integrate inferences over language with\nnew contextual observations from other modalities, such as perceptual or motor observations from seeing or\nactually moving these objects that might update one\u2019s background beliefs over the distribution of masses.\nPutting it together: Probabilistic inference and physics simulation from language.\nThe examples\nin Fig. 10 show how this approach can capture the nuanced relationships between language and physical\nreasoning. Language that modulates any of the physical properties in our introduction to this section, from the\nmasses of objects, their shapes and corresponding friction when moving, and the forces they receive, changes\nthe distribution over internally simulated scenes, and is reflected in updated inferences about downstream\nevents.\nFuture directions: Perception as inverse rendering and complex physical reasoning as intuitive\nphsyics.\nAs with all of our other examples, its important to emphasize that our simulate_physics interface\nis almost the simplest possible world model we might construct over physical scenes. The approach we take\nhere is inspired by, but much simpler than, many other probabilistic generative models (Allen, Smith, &\nTenenbaum, 2020; Battaglia et al., 2013; Ullman et al., 2017; J. Wu et al., 2015a; Xu et al., 2021) of more\ncomplex object configurations in more complex environments (such as ramps, stacks of objects), many other\nproperties that we can describe about objects themselves (such as their material), and arbitrary forces (like\nbumping the table or dropping objects from above).\nOur approach in these sections also suggests a rich line of future work for reasoning jointly about\nobservations in language, and from perception. While we do not implement a perceptual module in our\nexample, the framework we sketch here can be directly integrated with the broad body of work on inverse\ngraphics, which frames scene understanding as inference from observed visual inputs to recover structured\nrepresentations of a scene\u2019s contents (D. Kersten, Mamassian, & Yuille, 2004; D. K. D. Kersten & Yuille, 1996;\nLee & Mumford, 2003; J. Wu, Tenenbaum, & Kohli, 2017; J. Wu, Yildirim, Lim, Freeman, & Tenenbaum,\n2015b; Yi et al., 2018; Yildirim, Belledonne, Freiwald, & Tenenbaum, n.d.; Yuille & Kersten, 2006). Our\nframework suggests a particularly fruitful integration between language and the growing body of work that\n31\n3.2\nLanguage for visual and physical reasoning\n3\nWORLD MODELS\ncombines probabilistic programs and graphics rendering engines (Gothoskar et al., 2021; Kulkarni et al., 2015;\nV. K. Mansinghka et al., 2013; Zinberg et al., 2019). To draw inferences about visual scenes from perceptual\ninputs, models like these incorporate convolutional neural networks to make fast, amortized proposals about\nthe scene state from vision, but with respect to a generative program that defines the underlying scene state\nand guide inferences about particular scenes, such as to reason about occlusion.\nIntegrated with the approach we describe here, this framework could ground linguistic queries directly\ninto vision, allowing structured inferences for visual question answering (e.g., counting the number of unique\ncolors of the dishes in a scene). Moreover, it could enable more complex, joint inferences that integrate visual\nobservation with linguistic information about latent physical properties of objects in a scene (e.g., mass,\nfriction) or the presence or identity of occluded objects. Such multimodal integration holds the potential to\nshed further light on the ways that linguistic knowledge can shape our understanding of and reasoning about\nphysical scenes.\n32\n3.3\nLanguage for reasoning about agents and plans\n3\nWORLD MODELS\n3.3\nLanguage for reasoning about agents and plans\nOne of the most deeply human things we can talk about is other people. To conclude this section, we turn to\nlanguage about other social beings\u2014agents who want things, chase goals, and plan how to act in the world\naround them.\nAs an illustrative example, we consider a domain (Fig. 11) inspired by C. L. Baker, Tenenbaum, and Saxe\n(2007), which evaluated commonsense social inferences about agents with different preferences and goals. In\nour slightly modified example, we consider a set of people with varying food preferences who are making\nplans for lunch. Based on the map shown in Fig. 11, we\u2019ll imagine which restaurant they might go to based\non what foods they like, how far each restaurant is from their office (shown in blue), and whether restaurants\nhappen to be open or closed. We\u2019ll also note that students can bike or walk to any restaurant, but include\nthe intuitive fact that biking is faster on roads, but slower than walking on the lawns.\nThe original experiments in C. L. Baker et al. (2007) used visual stimuli to depict agents\u2019 paths and\nplans, but language is a particularly natural and nuanced way to communicate information about other\nagents. Consider the range of situations we can describe in this simple example. We might leverage our\nwide vocabulary for describing the spectrum of someone\u2019s preferences and desires\u2014whether they crave pizza\nor hate vegetables, or whether they love sushi rather than merely liking it. We might describe their more\nconcrete, discrete goals, like getting to a pizza place or generally getting to the closest restaurant to the office.\nThe inferences we draw from language also depend on our intuitions about agents themselves. All else being\nequal, we expect people to minimize the effort it takes to act, while trying to maximize the value they gain\nfrom acting. We might generally expect someone to walk down Ames Street if they wanted to go to the pizza\nplace rather than taking an unnecessarily convoluted path, or to jump on a bike if they owned one, rather\nthan taking a slower walk there. We also understand, of course, that people need to accommodate the world\nitself in their plans, and might not go to the pizza place no matter how much they love pizza, if they were\ntold that the pizza place was closed.\nPerhaps more subtly, but equally importantly, what we know about agents also informs the wide range of\ninferences we can draw from language about their actions. Consider, for instance, what you can infer from\nbeing told that someone had started at the office, and was now walking across the southern lawn. Because\nthey\u2019re on a direct route towards the vegetarian place, you might infer that they are more likely to prefer\nvegetarian food, and that they either know or at least believe that the vegetarian place is open. Because\nthey are walking on foot, you might also suspect that they do not own a bike, which would have allowed\nthem to get to the restaurant more quickly. All of these inferences build on a cohesive picture of agents as a\nwhole\u2014our expectations about agents as goal-directed, efficient actors inform how we think about any given\naction.\nAs with visual and physical reasoning, this section builds more generally on extensive work in cognitive\nscience and artificial intelligence on social reasoning, and seeks to integrate this broader literature into our\nframework for language. Developmental evidence suggests that we have a core conceptual understanding of\nagents as goal-directed actors from a very young age (Csibra, 2008; Csibra, B\u00edr\u00f3, Ko\u00f3s, & Gergely, 2003;\nR. M. Scott & Baillargeon, 2013; Spelke & Kinzler, 2007). Computational cognitive models, and the broader\nAI planning literature, have long approached social inferences like those we describe here under under a\nunifying model of planning and inverse planning (C. Baker et al., 2011; C. L. Baker, Saxe, & Tenenbaum,\n2009; C. L. Baker et al., 2007; M. F. Cusumano-Towner, Radul, Wingate, & Mansinghka, 2017; Jara-Ettinger,\nSchulz, & Tenenbaum, 2020; Seaman, van de Meent, & Wingate, 2018). This framing couples the forward\nplanning of actions that achieve goals or maximize utilities, to the inverse problem of inferring latent variables\nabout the agent or the world from observations of their actions.\nThe example below shows how we can extend the modeling motif from our previous discussion of visual\nand physical reasoning, which shows how our framework can relate language to other core cognitive modules\nvia interfaces implemented in a general probabilistic language of thought. In this section, we introduce\nmodel-based planners as another such core computational module, which can be integrated into this framework\nto support a wide range of probabilistic forward and inverse inferences about agents and their actions as they\nare referenced in language.\nIntegrating a probabilistic generative model over agents with a planner.\nAs a concrete example,\nthe generative program excerpted in Fig. 11 (shown in full in Appendix A.4.1) illustrates how an integrated\n33\n3.3\nLanguage for reasoning about agents and plans\n3\nWORLD MODELS\n(define restaurants (list 'sushi 'pizza 'vegetarian))\n(define is_open (mem (lambda (restaurant_type) (flip))))\n(define gridworld (list\n(list 'ames\n'lawn\n'lawn\n'lawn\n'sushi)\n(list 'ames\n'lawn\n'lawn\n'lawn\n'danner)\n(list 'office 'barlow 'barlow 'barlow 'danner)\n(list 'ames\n'lawn\n'lawn\n'lawn\n'danner) \n(list 'ames\n'lawn\n'lawn\n'lawn\n'vegetarian)\n(list 'pizza\n'carson 'carson 'carson 'danner)\n))\n(define has_bike (mem (lambda (agent-id) (flip))))\n(define restaurant_utility (mem (lambda (agent-id restaurant_type)\n(uniform-draw (list (gaussian POSITIVE_UTILITY_MEAN UTILITY_VARIANCE)\n(gaussian NEGATIVE_UTILITY_MEAN UTILITY_VARIANCE))))))\n(define motion_utility (mem (lambda (agent-id location_type motion_type)\n(case location_type\n(('lawn) (case motion_type\n(('is_biking) -1)\n(('is_walking) -0.2) ...)\n(define plan (mem (lambda (agent-id map state_x state_y)\n(let map_transition_fn ...\n(let value_function ...\n(let optimal_policy_from_initial_state ...)\nAlex likes all of the nearby restaurants.\nAnd all of the restaurants are open today.\nBut Alex doesn't have a bike.\nGabe was just biking East on Barlow St.\nWhere might Gabe be going?\nReference map\nEvery day at lunchtime, people on campus decide what restaurant to go to based on what foods they like \nbest, what restaurants are open, and how long it will take to get there. Biking is faster on roads, but \nwalking is faster on grass.\nGenerative world model of planning domain\nReasoning about agents, goals, and plans from natural language\nSituation A\nLio loves sushi.\n(condition (> (restaurant_utility 'lio 'sushi) 10))\nLio doesn't mind vegetarian, either, but they hate pizza.\n(condition\n(and (> (restaurant_utility 'lio 'vegetarian) 0)\n(< (restaurant_utility 'lio 'pizza) 0)))\nSituation B\nSituation C\nBarlow Street\nCarson Avenue\nDanner Street\nAmes Street\nOh, but the sushi place is closed.\n(condition (not (is_open 'sushi)))\nWhat restaurant will Alex go to for lunch?\nIs the pizza place open?\nWhere do you think  Lio will go?\n(query\n(get_actions 'lio (lambda (action)\n(and (is_subject_of_action? action 'lio)\n(is_action? action 'is_going)))))\nBut Gabe really likes pizza.\n+\n+\n-0.05\n-0.5\n+\n+\n-0.05\n-0.01\n+5\n0\n-10\nInference over plans\n(query\n(get_actions 'lio (lambda (action)\n(and (is_subject_of_action? action 'lio)\n(is_action? action 'is_going)))))\nWhere do you think  Lio will go now?\n(condition (and\n(> (restaurant_utility 'alex 'sushi) 0)\n(> (restaurant_utility 'alex 'pizza) 0)\n(> (restaurant_utility 'alex 'vegetarian) 0)))\nFigure 11: Our language about other people builds on our intuitions about how agents act on their preferences\nand goals. (Top) Our example probabilistic generative model describes a prior over agents with different\npreferences for the nearby restaurants shown on the map, as well as the relative cost of getting to each one on\nbike or on foot. Integrating a model-based planner into this generative model allows it to express a prior on\nhow agents will actually act based on their desires, balancing these preferences against whether restaurants\nare open, and whether or not they have a bike. (Bottom) Observations and queries about the agents, their\ngoals, and about the world itself updates a unified belief distribution, reflecting how agents plan in the world\nand how observing their actions drives inferences about the latent state in the world.\nprobabilistic modeling and planning language can describe the agents and restaurants domain.\nTo model background states of this environment, our implementation represents the spatial structure of\nthe campus (as a simple gridworld map), and stochastic Boolean variables that model whether someone owns\na bike (has_bike) and whether a given restaurant is open (is_open).\nWe then introduce a generic, utility-based formulation derived from the classical AI planning literature to\n34\n3.3\nLanguage for reasoning about agents and plans\n3\nWORLD MODELS\nmodel the varying preferences of any given person and the effort of taking particular actions (see Russell\nand Norvig (2021) for a review). Incorporated into a probabilistic generative model, we can express the\ndistribution of preferences any particular agent could have, and the way these preferences interact with\nthe stochastic mechanics of any given world. In our implementation, we model these varying preferences\nas a stochastic utility function associated with particular agents and restaurants (restaurant_utility). Our\nimplementation shows a bimodal Gaussian distribution, in which people tend to have distinctly negative or\npositive preferences for any given restaurant, but any other formulation would be easily expressible. We\nalso model how these preferences interact with other aspects of the world\u2014we condition the value an agent\nderives from actually arriving at a restaurant (utility_at_restaurant_state) on whether or not it is open.\nThese utilities interact with possible actions an agent can take to get to different restaurants. We model the\ndistribution of possible actions an agent might take (our available_actions function conditions on whether\nan agent has_bike), and the varying effort of individual actions. Our motion_utility conditions on the type of\naction and the state in which it is used, to model the greater effort of biking on grass and the relative ease of\nbiking on the road.\nUp to this point, the generative model simply expresses a general prior over world states that includes\nagent preferences. Now, to model how an agent actually decides on a course of action conditioned on the\nworld state, we can finally introduce a plan interface (Fig. 8) that calls out to a model-based planner. Our\nimplementation, while simple, implements the basic functionality core to an AI planner\u2014it computes a\nsequence of actions that achieves a goal or maximizes a value function, subject to an agent\u2019s underlying\npreferences, available actions, and the conditions of the environment. As with the physics interface in our\nprevious section, our example planner implementation is simple and generic enough that we also implement\nit fully within the body of the probabilistic program itself (see Appendix A.4.1) for illustrative purposes. Our\nimplementation here uses a simple value-iteration algorithm, which computes an optimal policy of action\nto trade off between the value an agent derives from particular restaurants, and the cost of taking actions\n(walking or biking in any given direction, from any location in the map) towards them.\nLanguage about agents as program expressions.\nBy augmenting the probabilistic generative model\nwith a planner, we can now ground many of the basic ways we talk about agents themselves in probabilistic\nconditions and queries to this model.\nLanguage about what people want and prefer, like whether someone wants, likes, loves, doesn\u2019t mind,\nor hates a given restaurant in our example domain, can construct formal conditions over underlying utility\nvariables, that in turn drive inferences about how the agent will act. In the examples shown in Fig. 8, we\nillustrate the semantics of these terms as conditions constructed directly over the continuous utility variables\ndefined in this domain. We could also derive a more explicit set of predicates (like a Boolean likes? predicate,\ndefined over the underlying utilities), but as in several previous sections, we show these more transparent\nsemantics (like translating likes into a > 0 threshold on utilities) to illustrate how language relates to our\nmodel of agents, and to demonstrate the amortized inferences that a language-to-code model can make in\ndirectly inferring these threshold values in context, and for new preference words.\nObservations about relevant aspects of the environment, like whether the sushi place is closed or Alex has\na bike, are translated as in previous sections into conditions on the generative world model. In this integrated\nframework, these observations now support downstream inferences about how agents might change their\nbehavior with respect to what we are told about the world.\nFinally, of course, explicit observations and queries about someone\u2019s goals, plans, and individual actions\n(Gabe was biking East on Barlow Street, or What restaurant will Alex go to for lunch?) can be interpreted with\nrespect to the underlying, model-based planner, to drive inferences about forward planning agents choosing\nactions in the world, and inverse inferences over the many latent variables in the world that collectively\nexplain language about someone\u2019s actions.\nTranslating language using language-program distributions.\nWe showcase several distinct examples\n(Fig. 11) of context-sensitive, pragmatic inferences derived using a language-to-code meaning function\nconditioned on language and this generative world model.\nAs in previous sections, we find that the LLM can directly ground vague, graded terms in context-specific\nthresholds over particular continuous variables in the probabilistic world model. Here, this approach grounds\npreference terms (doesn\u2019t mind, loves) into reasonable thresholds over the utility variables in the world model\n35\n3.3\nLanguage for reasoning about agents and plans\n3\nWORLD MODELS\n(Fig. 11). We find that the LLM can both infer reasonable utility thresholds and generalize to words not\nexplicitly given as example translations: we prompt the model with a handful of examples pairs, such as a\ntranslation that maps the word likes to a > 0 threshold on utilities, and the LLM successively generalizes this\nparse to ground other preference terms like hate and love, presumably based on the comparative valences of\nthese preference terms in the broader distribution of language.\nWe also find that the LLM can directly translate quantifiers over contextual sets in this domain\u2014like\nlikes all of the nearby restaurants\u2014into a conjunction over the set of restaurant literals in this domain, by\nconditioning on the generative world model during parsing. More concretely, this means the LLM identifies the\nrelevant restaurants list (shown in the excerpted generative world model in Fig. 11), and conditions on it to\ndirectly produce the unrolled conjunction over the list contents ((and (is_open 'sushi) (is_open 'pizza)...)\nintended by all restaurants, amortizing the computation that would have otherwise been necessary over a\nmore literal semantics like (all restaurants). Together with the previous sections, these examples suggest\nhow our framework might jointly support explicit inferences from language into various expressions in a\nlanguage of thought, and learned patterns from the large language-to-code model that amortize some of these\ninferences over time, which we discuss directly as grounds for future work in Section 5.\nPutting it together: Probabilistic inference and planning over language.\nThe several example\ndialogues shown in Section 2 show how this approach captures the integrated social inferences we make about\nagents in language. We can now query the plans and goals of agents, deriving inferences with respect to\nthe forward planning module incorporated into the underlying generative model, conditioning flexibly on\narbitrary information in context, and updating expectations about where agents will go, and how they will\nchange their plans based on new observations about the world. In turn, we can derive inverse planning\ninferences, like whether the pizza place is open, based on relatively tangential information about someone\u2019s\nactions\u2014knowing that an agent really likes pizza, but is seen taking a path that wouldn\u2019t efficiently lead\nthem there. All of these inferences fall out of the same underlying generative model, which unifies these\ndistinct observations about people and the world in language with respect to a formal model of how agents\ntend to behave.\nFuture directions: Scaling integrated world models for planning and inference.\nThe plan function\nin our example implements a very simple but model-based planner\u2014it computes actions based on an underlying,\nstructured model of the world. In comparison to the other domains in this paper, linguistic planning and\nsocial reasoning have received perhaps the attention in recent work, in part because complex reasoning about\nother agents (Ullman, 2023) and precise general planning tasks (Bubeck et al., 2023; Valmeekam et al., 2023)\nappear to pose outstanding challenges for even the largest current language models. Recent work has sought\nto interface large language models with classical planning languages and symbolic planners (eg. Collins, Wong,\nFeng, Wei, and Tenenbaum (2022); Ding, Zhang, Paxton, and Zhang (2023); B. Liu et al. (2023); Xie et al.\n(2023)), as well as general purpose programming languages used to express code-based policies (G. Wang\net al., 2023). All of these approaches suggest directions for scaling the simple planning implementation we\nshow here\u2014our goal is to show how classical planning approaches can be nested within and integrated into\nprobabilistic generative models to support a range of complex reasoning about other agents to infer their\ngoals and actions from information in language.\nCollectively, the broader cognitive science and AI planning literature suggests many directions for scaling\nup this model towards more of the nuance in human social reasoning, each which would in turn allow this\nparadigm to ground richer and more nuanced descriptions of agents, and the inferences we draw from this\nlanguage. Some of the most important ones include planners and planning languages that designed to express\nexplicit and discrete goals, like wanting to be at the highest rated pizza place within a half-mile radius or\ntrying to get a plate of sushi for under ten dollars, rather than continuous values and utilities (G. Davidson,\nGureckis, & Lake, 2022; Fikes & Nilsson, 1971; D. McDermott, 1982; D. M. McDermott, 2000; Pednault,\n1989); planners that model explicit uncertainty about the world itself, like agents who don\u2019t know whether\na restaurant is open or closed until they get there (C. Baker et al., 2011; Kaelbling & Lozano-P\u00e9rez, 2013;\nZhi-Xuan, Mann, Silver, Tenenbaum, & Mansinghka, 2020); hierarchical planners that recursively turn goals\ninto more specific subgoals to account for plans over longer timescales, at differing levels of abstraction\nKaelbling and Lozano-P\u00e9rez (2011); and recursive models of agents who are themselves thinking about other\nagents, such as models of two people trying to meet up at a restaurant that they think will satisfy both of\n36\n3.3\nLanguage for reasoning about agents and plans\n3\nWORLD MODELS\nthem, or where they might be most likely to find the other (C. Baker et al., 2011; Krafft, Baker, Pentland, &\nTenenbaum, 2016; S. A. Wu et al., 2021). Each of these could allow this paradigm to ground richer and more\nnuanced descriptions of agents, and the inferences we draw from this language.\nConclusions.\nTogether with the previous sections on vision and physics, our approach to grounding\nlanguage about social agents highlights the more general computational account suggested by our framework.\nBy translating language into probabilistic programs, language can construct, describe, and drive inferences\nover our internal world models. These may in turn incorporate many more specific computational engines\u2014\nmodeling how scenes are visualized, how physics unfolds in the world, or how agents plan towards their goals\u2014\nas modular interfaces that can be called upon in a general probabilistic language of thought.\n37\n4\nGROWING WORLD MODELS\n4\nGrowing and constructing world models from language\nIn Section 3, we illustrated how domain theories expressed in a probabilistic language-of-thought can provide\nflexible and powerful scaffolding for language understanding. In each, generative world modeling programs\nprovided a unified substrate for defining a structured domain model and representing the meanings of sentences.\nBut where do these world models come from? If we want our PLoT account of language understanding to\nscale beyond the knowledge that can be hand-coded by a programmer, we need to provided some account of\nhow such a system might acquire new concepts and domain theories.\nOne of the hallmarks of human communication is our ability to teach each other fundamentally new\nconcepts in language. We coin new words, define interrelated conceptual systems, and describe entirely new\nworld models, explaining the abstract underlying structure of whole domains. Because language spans so many\naspects of human thought, it is perhaps a uniquely powerful tool for structuring learning. In language, we can\ndefine new concepts and domains that are integrated into our inferences, relational reasoning, understanding\nof the visual and physical world, and goals and plans.\nHow do we learn new concepts and world models from language? And how can we build computational\nsystems that can be taught in language as we teach each other? In this section, we showcase the extensibility\nof the framework we have proposed as a unified model for relating language to thinking. Because world\nmodels in a PPL are expressed as programs, the same core computational components can be used to extend\nand construct world models themselves from language. In Section 4.1, we show how we can extend an existing\ndomain model with new lexical concepts. Then, in Section 4.2, we turn to language that communicates an\nentire background domain model from scratch. Through these simple explorations, we aim to point towards a\nnear-term horizon where systems might construct rich and nuanced probabilistic models to make sense of\ntheir linguistic environments and the broader world around them.\n4.1\nGrowing a world model from language\nHow can we enrich our world models with concepts learned from language? Let\u2019s consider, for instance, the\nkinship domain model used in the relational reasoning example in Section 3.1. The probabilistic program\nused in this example described a basic generative model over family trees, and then defined a handful of\nprimitives, such as concepts for grandparent and sibling. But most people know and can talk about many\nmore kinship relations than those included in that simple example, such as uncles, aunts, and cousins. What\nhappens when we use language that invokes one of these undefined concepts?\nCondition: Avery is Blake\u2019s uncle.\n(condition\n(exists (lambda (x) (and\n(sibling-of? x 'avery)\n(parent-of? x 'blake)))))\nThe LLM proposes an initial translation that includes some of the important components in the concept\nof an \u201cuncle.\u201d However, several key details are not quite right: an uncle should be the brother of Avery\u2019s\nparent, not just a generic sibling. Moreover, an uncle can come from outside the bloodline, in which case this\ndefinition would not fit. Much like a person learning English, the LLM has a partial notion of this concept,\nbut could benefit from more explicit instruction from a knowledgeable teacher. In this section, we introduce a\nnew define construct that does just this by prompting the LLM to generate a new definition from language.\nDefine: An uncle is the brother of one\u2019s father or mother, or the husband of one\u2019s aunt.\n(define (uncle-of? name_a name_b)\n(or (exists (lambda (x) (and\n(brother-of? name_a x)\n(parent-of? x name_b))))\n38\n4.1\nGrowing a world model from language\n4\nGROWING WORLD MODELS\n(exists (lambda (x) (and\n(husband-of? name_a x)\n(aunt-of? x name_b)\n)))))\nWe\u2019ve used define to fill in a bit of common knowledge that was missing from our conceptual system. But\nthe mental frameworks we use to reason about the world are constantly undergoing conceptual change, both\nat an individual and a societal level (Carey, 1999; Posner, Strike, Hewson, & Gertzog, 1982). For instance,\nshifts in cultural notions of gender and identity have introduced new kinship terms into English. One of the\nhallmarks of language is the ease with which we can coin and communicate new concepts, like the following:\n\u201cPibling\u201d is a gender-neutral term for \u201caunt\u201d or \u201cuncle\u201d that refers to the sibling of one\u2019s parent.\nFinally, as we touched on in Section 3.1, kinship systems vary widely; certain cultures have kinship concepts\nthat are more granular than those found in English. For instance:\nIn the language of the Northern Paiute, a group of peoples indigenous to the Great Basin region\nof the US, \u201cp\u00afaan\u2019i\u201d refers specifically to the sister of one\u2019s father.5\nFrom this definition, we can incorporate the concept of a p\u00afaan\u2019i into our growing set of kinship concepts.\nOur framework elegantly captures this ability to learn new concepts in language that we can then use\nproductively to construct new sentences and reason about coherently against the background of our existing\nworld knowledge. Here, we walk concretely through how the basic components of our framework are combined\nto grow the original kinship model with new concepts.\nLinguistic meanings as program expressions.\nMuch as we interpreted observations as program\nexpressions that conditioned an existing world model, and questions as program expressions that queried it, a\nsentence like, The term \u201cp\u00afaan\u2019i\u201d refers to the sister of one\u2019s father, can be modeled as a program expression\nthat defines a new such primitive relation, paani-of?. The examples in Fig. 12 show how the semantics of\nthis sentence, along with the other kinship concepts introduced in the introduction to this section, can be\nsimilarly understood as expressions that define new conceptual primitives. These expressions are particularly\ninteresting because they are defined in terms of other concepts, like sister-of? and father-of?, that make\nup this conceptual system. In this way, our treatment of concept learning is closely linked to the idea of a\nconceptual role semantics (Block, 1998; Field, 1977; Greenberg & Harman, 2005; Harman, 1982), in which\nconcepts (including lexical concepts) derive meaning from their interrelated roles and relationships to other\nconcepts. In these examples, interpreting these sentences as program expressions defined over the base\ngenerative model showcases the flexible role that the generative modeling program can play, in relation to\nlanguage about the domain. While our example showcases simple relational definitions over the underlying\nworld model, it is worth noting that these are not the only kinds of functional definitions that we could\nlearn to extend a world model from language. This general approach can be used to make meaning from\nsentences that grow an underlying world model in other ways, such as by defining new random variables (like\nphenotypic eye colors or other inherited traits) that extend the probabilistic generative model.\nTranslating with a language-program distribution.\nWhile the meanings of these sentences play a\ndifferent role in our framework\u2014they extend the world modeling program, rather than condition or query\nit\u2014they are still program expressions. Therefore, with minor adjustments, we can use the same language-to-\ncode LLM approach to ground these new concepts in our world model. To derive each of the translations\nshown in Fig. 12, we feed the LLM the same prompt as in Section 3, which includes the existing generative\nmodel and example translations. The final line of the prompt begins with Define: and contains the language\ndescribing the new concept definition. Each sentence is then then translated into the new define statements\nwhich construct new conceptual kinship primitives. In sum, linguistic definitions are simply another kind of\nprogram expression we can translate into from language.\n5At the time of writing, a Google search for the term \u201cp\u00afaan\u2019i\u201d yielded zero results. The term itself was pulled from a\nnon-searchable table in a century-old manuscript (Lowie, 1930). As far as real-world kinship terms go, it is comparatively\nunlikely\u2014though not impossible\u2014that \u201cp\u00afaan\u2019i\u201d was part of Codex\u2019s pretraining data.\n39\n4.1\nGrowing a world model from language\n4\nGROWING WORLD MODELS\n(define (uncle-of? name_a name_b)\n...\n(define (pibling-of? name_a name_b)\n...\n(define (paani-of? name_a name_b)\n...\n(define (uncle-of? name_a name_b)\n(or (exists (lambda (x) (and\n(brother-of? name_a x)\n(parent-of? x name_b))))\n(exists (lambda (x) (and\n(husband-of? name_a x)\n(aunt-of? x name_b))))))\nAn uncle is the brother of one's parent, or the husband of one's aunt.\nA. Existing generative world model\n(define (person person-id parent-1-id parent-2-id) \n(list\n(pair 'person-id person-id)\n(pair 'name person-id)\n(pair 'gender (person->gender person-id))\n(pair 'parent-1-id parent-1-id)\n(pair 'parent-2-id parent-2-id)))\n...\n(define (parent-of? name_a name_b)\n(member? name_a (parents-of name_b)))\n(define (father-of? name_a name_b)\n(and (equal? (get-property name_a 'gender) 'male)\n(parent-of? name_a name_b)))\n(define (sister-of? name_a name_b)\n(and (equal? (get-property name_a 'gender) 'female)\n(sibling-of? name_a name_b)))\n...\nB. Defining new concepts via language-to-code translation\n(define (pibling-of? name_a name_b)\n(or (uncle-of? name_a name_b)\n(aunt-of? name_a name_b)))\n\u201cPibling\u201d is a gender-neutral term for \u201caunt\u201d or \u201cuncle\u201d that refers to the \nsibling of one\u2019s parent. \n(define (paani-of? name_a name_b)\n(exists (lambda (x) (and\n(sister-of? name_a x)\n(father-of? x name_b)))))\nIn Northern Paiute, \u201cp\u0101an\u2019i\u201d refers to the sister of one\u2019s father. \nC. Extended world model\nD. Grounding new language in learned concepts\n(condition\n(or (father-of? 'winnemucca 'numaga)\n(uncle-of? 'winnemucca 'numaga)))\nAccording to historical records, Chief Winnemucca was either the \nfather or uncle of Numaga.\n(condition\n(exists (lambda (x) (and\n(son-of? x 'numaga)\n(paani-of? 'sarah x)))))\nNumaga's son would have called Sarah Winnemucca his \u201cp\u0101an\u2019i\u201d.\n(condition\n(sibling-of? 'sarah 'natchez))\nSarah had a sibling named Natchez.\n(query\n(length (filter-tree\n(lambda (x)\n(exists (lambda (y)\n(and (pibling-of? x y)\n(son-of? y 'numaga))))))))\nHow many piblings would Numaga's son have had?\nProbabilistic inference\nFigure 12: Extending the kinship world model with linguistic descriptions of kinship relations drawn from\ncontemporary English and a low-resource language (Northern Paiute). A language-to-code LLM is prompted\nwith (A) the existing generative model code and (B) language describing novel kinship relations to produce\nnew concept definitions in Church. The extended world model (C) now supports probabilistic reasoning from\nlanguage that contains these new concepts (D).\n40\n4.2\nConstructing new world models from language\n4\nGROWING WORLD MODELS\nGrowing the domain model with new program expressions.\nFinally, by incorporating the meanings\nof sentences like The term \u201cp\u00afaan\u2019i\u201d refers to the sister of one\u2019s father back into the domain model itself, we\nhave formalized a simple approach for enriching a world models with concepts learned from language. Each\nsentence shown in Fig. 12 is translated into a program expression that defines a new relational function which\nextends the set of conceptual primitives that comprise the extended kinship domain.\nThe more general principle here is not limited, of course, to kinship concepts. We could extend any of the\ndomain models in each of our previous examples with new concepts learned from language. For example:\n\u2022 In tug of war, the strongest person on a team is referred to as the \u201canchor\u201d.\n\u2022 A \u201cmonochrome\u201d scene is one in which every object is the same color.\n\u2022 On \u201cNational Restaurant Day\u201d, all the restaurants in town are guaranteed to be open.\nOur proposal in this section is closely related to other work which formalizes the learning of new concepts\nas the learning of new program components, such as program synthesis systems that bootstrap a growing\nlibrary of domain-specific concepts constructed out of an initial programming language (Bowers et al., 2023;\nDechter, Malmaud, Adams, & Tenenbaum, 2013; Ellis et al., 2020); work that formalizes the learning of\nnew concepts from language as the learning of new program primitives (Shin, Brockschmidt, Allamanis, &\nPolozov, 2018; Sumers, Hawkins, Ho, Griffiths, & Hadfield-Menell, 2022; C. Wong, Ellis, Tenenbaum, &\nAndreas, 2021); and semantic parsers that bootstrap lexicons of compositional word meanings, defined in\na formal logical language, for interpreting new sentences (Artzi, Das, & Petrov, 2014; Cai & Yates, 2013;\nKwiatkowski, Zettlemoyer, Goldwater, & Steedman, 2011).\nThe framing we describe here showcases the tight integration between language, meanings, and the\nprobabilistic programs that form the formal substrate for modeling the world in our framework. Language\nthat specifies new parts of a world model can be cleanly interpreted as program expressions, which are used\nto extend the generative world modeling program itself. These generative models in turn provide the basis for\nreasoning about new observations that build on these learned and structured bodies of conceptual knowledge.\nReturning to the themes of our introduction, human-like thinking, under the broader computational approach\nwe take throughout this paper, is formalized as probabilistic programming and inference over probabilistic\nprograms. This is how we construct models of and reason about the world. Language, then, is an especially\npowerful tool for constructing programs of all kinds\u2014ones that condition and query existing world models,\nand ones that actually construct and extend the flexible domain models themselves that undergird linguistic\nmeaning and thought.\n4.2\nConstructing new world models from language\nSo far, we have assumed that language understanding happens in the context of a particular world model\nappropriate for the situation at hand, containing definitions of key concepts like sibling for kinship reasoning,\nor strength for reasoning about playground games. We have now seen how these models can be extended with\nnew lexical definitions on the fly, but the question remains of where these background world models come\nfrom in the first place. The full answer to this question is likely complex: people learn about the world in all\nsorts of ways. But in some settings, people do seem to acquire new world models largely through language:\nwe read the rules of new games, are taught the workings of machines, and take classes on the causal structure\nof many other complex systems (the human body, the solar system, the government). In this section, we\nbroaden our scope beyond language that conveys new concepts that extend an existing domain model to\nconsider how language can define entire new domain models from scratch.\nAs a concrete example, let\u2019s return to the scenario from Section 2.2. Suppose your friend is telling you\nabout a tug-of-war tournament that took place the prior weekend\u2014only this time, you\u2019ve never heard of\ntug-of-war before and don\u2019t know how it\u2019s played. Your friend might explain the scenario to you using\nlanguage\u2014indeed, their description might sound similar to the one our paper itself uses to convey the concepts\nof this particular situation:\nTug-of-war is a game played between teams of players. First, strength levels vary widely from\nperson to person. Furthermore, each person has a percentage of the time that they are lazy. The\nstrength of a team is the combined strength of its members, except that in any given match, each\n41\n4.2\nConstructing new world models from language\n4\nGROWING WORLD MODELS\nplayer may decide to be lazy, and thus contribute only half of their strength. Whether one team\nbeats another just depends on which team pulls stronger that match.\nGiven this language, you can learn the underlying domain model necessary to reason about future observations\n(Even working as a team, Lio and Alex could not beat Josh) and answer questions (How strong is Josh?). In\nthis section, we explore how the components of our framework can be used to construct an entire domain\nmodel as it is communicated in language, using the tug-of-war domain as an illustrative example.\nLinguistic concepts as program expressions.\nConsidering the vignette above, we might distinguish\nbetween two kinds of statement in your friend\u2019s description of tug-of-war:\n\u2022 Some statements introduce new concepts solely in terms of previously introduced concepts (e.g., Whether\none team beats another just depends on which team pulls stronger that match).\n\u2022 Other statements posit the existence of new primitive concepts, like strength and laziness, that have\ncertain properties (e.g., Strength levels vary widely from person to person).\nThe first case is similar to the sentences we saw in Section 4.1, and we can interpret them as language-of-\nthought definitions. The second case, however, is genuinely new: these sentences neither define new words in\nterms of an existing domain theory, nor encode predicates over possible worlds. Rather, they define random\nvariables that we expect to have different values in each possible world.6\nIn Church, such variables can be defined using mem: for example,\n(define strength (mem (lambda (person) (normal 100 20))))\ndeclares that expressions of the form (strength person) are well-formed and evaluate to a number in each\npossible world, and that our prior distribution for a new person\u2019s strength is a Gaussian centered at 100.\n(The mem construct memoizes the defined function, so that repeatedly evaluating (strength 'lio) in the same\nworld will always give the same result.) It might seem strange to claim that the meaning of the sentence\n\u201cPlayers have different strength levels\u201d includes a specific prior over player strengths, like (normal 100 20).\nWe do not make this claim: rather, the meaning function induces a distribution over possible definitions of\nstrength, each of which uses a different prior. What the different possible translations have in common is\nthat they model strength as a continuous variable assigned on a per-player basis, with some population-level\nvariation. See Footnote 6 for further discussion of this distribution, and how it might arise from the literal\nmeaning of the sentence being translated.\nTranslating new concepts from language.\nAs before, because each sentence means some distribution\nover program fragments in a probabilistic language, we can use probabilistic language-to-code translation\nmodels like Codex as models of the meaning function. In Fig. 13, we prompt Codex with an unrelated\nexample world model in domain about diseases and symptoms, and then ask it to translate sentences defining\nthe tug-of-war domain.\n6 An alternative perspective is that the sentences we consider in this section\u2014both straightforward definitions, and sentences\nintroducing new primitive concepts\u2014do still encode predicates on possible worlds. According to this viewpoint, a sentence like\n\u201cThe term \u2018uncle\u2019 refers to the brother of one\u2019s parent, or the husband of one\u2019s aunt\u201d is an assertion that can be true or false;\nmaybe uncle means something different in another possible world. To understand this viewpoint within our framework, we\nneed to imagine that there is a background world model that models uncertainty about the code of a first-order world model\n(which definitions exist, and how they are defined). If we had such a model over world models, then sentences like \u201cPlayers have\ndifferent strength levels\u201d could be interpreted as conditioning statements, observing that strength exists as a variable and that its\nvalue should vary from person to person. Conditioning on this constraint, we could then sample from the posterior over world\nmodels that satisfy this property. In this posterior, there would be some uncertainty over exactly how strength is modeled: e.g.,\ndoes it vary according to a Gaussian distribution, and if so, with what parameters?\nWe find this view appealing, and believe that making it practical would be an intriguing technical challenge, requiring\nnew developments in the field of Bayesian probabilistic program synthesis (Saad, Cusumano-Towner, Schaechtle, Rinard, &\nMansinghka, 2019). In this section, we take a shortcut, of assuming that the meaning distribution induced by a sentence like\n\u201cPlayers have different strength levels\u201d directly samples model fragments consistent with the statement. That is, we ask our\nmeaning function to amortize inference in the hierarchical model, directly proposing code defining strength, rather than first\ntranslating to a conditioning statement about strength existing, and then using a slower inference algorithm to infer its definition.\n42\n4.2\nConstructing new world models from language\n4\nGROWING WORLD MODELS\nA. Prompt, containing unrelated example world model\n;; We define a probabilistic model in Church of the following scenario.\n;; At any given time, about 1% of the population has lung cancer,\n;; 20% have a cold,\n10% have a stomach flu, and 0.5% have TB.\n(define lung-cancer (mem (lambda (person) (flip 0.01))))\n(define cold (mem (lambda (person) (flip 0.2))))\n(define stomach-flu (mem (lambda (person) (flip 0.1))))\n(define TB (mem (lambda (person) (flip 0.005))))\n;; If you have a cold, there's a 50% chance you have a cough.\n;; 30% of people with lung cancer have a cough, and 70% with TB.\n;; There's also a small chance you have a cough even if you're otherwise healthy.\n(define cough (mem (lambda (person)\n(or (and (cold person) (flip 0.5))\n(and (lung-cancer person) (flip 0.3))\n(and (TB person) (flip 0.7))\n(flip 0.01)))))\n;; Whether a person coughs during a particular visit to the doctor's office\n;; depends on whether they have a cough, and a bit of random chance. \n;; Note that this will differ each time they go to the doctor's office, so\n;; we do not use `mem` (which memoizes the result).\n(define coughs-on-particular-visit (lambda (person) (and (cough person) (flip 0.7))))\nB. Defining a new world model from scratch via language-to-code translation\n(define team-strength \n(lambda (members) \n(apply + (map (lambda (member) \n(if (flip (laziness member)) \n(/ (strength member) 2) \n(strength member))) \nmembers))))\nThe strength of a team is the combined strength of its members, except that in any given match, each player may decide \nto be lazy, and thus contribute only half of their strength.\n(define laziness (mem (lambda (person) (uniform 0 1))))\nFurthermore, each person has a percentage of the time that they are lazy.\n(define strength (mem (lambda (person) (normal 100 20))))\nFirst, strength levels vary widely from person to person.\nWhether one team beats another just depends on which team pulls stronger that match.\n(define team-beats-team \n(lambda (team1 team2) \n(> (team-strength team1) (team-strength team2)))))\n;; Now, let's define a different probabilistic model of the following scenario. \n;; It is totally unrelated to the previous model and does not reference the functions above.\nFigure 13: Constructing the tug-of-war model from scratch. This can be accomplished with the same\noverarching language-to-code approach. (A) We provide a prompt containing one or more unrelated world\nmodels as examples. (In this case, the world model defines a medical diagnosis domain.) (B) Prompted\nline-by-line with language explaining the tug-of-war, Codex constructs a generative model from scratch that is\nsemantically equivalent to the one from Section 2.2 (modulo some superficial naming and parameter choices).\n43\n4.2\nConstructing new world models from language\n4\nGROWING WORLD MODELS\nConstructing the domain model from new program expressions.\nBy translating each sentence in a\ndomain description in sequence, we can\u2014starting with no definitions beyond those built into Church\u2014build\na domain model just as rich as the ones we hand-coded in earlier sections. In Fig. 13, although the specific\npriors may vary slightly, Codex recovers all the essential structure of our hand-coded tug-of-war model. Once\nwe have a new domain model, we can immediately begin interpreting observations and queries, like those in\nSection 2.2, or continue to extend the domain model with new definitions.\nPutting it together: Growing and constructing world models from language.\nIn this section,\nwe\u2019ve illustrated how the same basic building blocks used in the rest of the paper \u2014 language-to-code\ntranslation and probabilistic programs \u2014 can be used to extend and construct new world models. Hopefully,\nthese simple sketches highlight a much deeper point: systems that have the ability to author world models\nin a universal programming language like Church can take advantage of the infinite expressivity of code to\ngeneralize to new kinds of language and thinking.\nNevertheless, the examples presented in Section 4 were limited to cases where there was an explicit\nconnection between linguistic instructions and the resulting probabilistic programming expressions. In reality,\nthis relationship is often indirect; language typically only provides us clues about how to think about a\nsituation. In still other instances, we assemble world models in the absence of language, drawing instead on\nprior experience of similar situations. How can we build systems that learn to build world models on-the-fly?\nHow can such systems remember and expand on prior world models to understand new situations? And how\ncan they incorporate not just language, but the full spectrum of experiences in the world? In Section 5, we\nconsider these questions as part of a discussion of the many future research directions needed to scale our\nframework to a general model of cognition.\n44\n5\nFUTURE DIRECTIONS\n5\nOpen questions and future directions\nBy using neural models to translate sentences into probabilistic programs, the sections above demonstrated\nhow LLMs could extract meaning from\u2014and inference engines could reason about\u2014language describing\nuncertain situations, relational structures, embodied situations and goal-directed reasoning. However, these\nvignettes also leave open many questions about how to scale this framework to more complex language, and\nhow to automate the process of building meaning representations for new domains. Together, these questions\noffer a roadmap for progress on central challenges in modeling language, reasoning, and their interaction,\nacross many sub-fields of artificial intelligence and cognitive science.\n5.1\nScaling models of rational meaning construction\nWe begin by describing several of the most important research directions necessary for scaling the framework\nwe have articulated throughout this paper towards a more complete model of integrated cognition and\nlanguage understanding.\n5.1.1\nBuilding new world models on the fly\nA key aspect of our proposed architecture is that language is interpreted relative to a probabilistic model\nof a domain, capturing just enough structure to represent the situation at hand. In Section 4.2, we saw\nthat LLMs could generate these programmatic world models, assuming the model was communicated via\na sequence of natural language definitions. But people rarely need such elaborate scene-setting: we can\nunderstand language about the world even if no teacher has carefully drawn our attention to the relevant\nconcepts beforehand. A key question is how to model this capability. How do minds craft bespoke world\nmodels on the fly, drawing in just enough of our knowledge about the world to answer the questions of\ninterest? How does this process balance competing priorities, such as fidelity to what we know about the\nworld, relevance to the problem at hand, and the efficiency and robustness of inference? These tradeoffs\ncan sometimes seem to evolve during the course of a single chain of human thought. These questions are\nrelated to the classic frame problem (McCarthy, 1980) in artificial intelligence and cognitive science, and to\nrecent proposals for addressing it in the setting of causal, probabilistic reasoning (Icard & Goodman, 2015).\nThese approaches view the problem as one of retrieval: from a vast array of knowledge we have about the\nworld, how can we select just the relevant parts for reasoning about a particular problem? It remains unclear,\nhowever, whether the sequences of bespoke models and approximate inferences produced by our minds can be\nunderstood as resource-rational approximations to coherent reasoning and planning in some larger, unifying\nworld model, even in principle.\nMost probabilistic programming languages were designed for inference in a single, unifying world\nmodel (Bingham et al., 2019; Carpenter et al., 2017; Goodman et al., 2008; Milch et al., 2007) that was\nwritten by an external mechanism, not to dynamically explore a sequence of probabilistic programs that\nare being synthesized, learned, and/or edited on the fly. But some progress in language-level support for\ndynamic world modeling has already been made. Probabilistic programs in Gen (M. F. Cusumano-Towner,\nSaad, Lew, & Mansinghka, 2019) have been used to synthesize and edit other probabilistic programs (Saad\net al., 2019; Witty, Lew, Jensen, & Mansinghka, 2019), and to approximate globally coherent inferences by\nbridging across sequences of probabilistic programs describing translations among only partially-overlapping\nworlds (M. Cusumano-Towner, Bichsel, Gehr, Vechev, & Mansinghka, 2018; M. Cusumano-Towner, Lew, &\nMansinghka, 2020; A. K. Lew, Matheos, et al., 2023; V. K. Mansinghka et al., 2018). Analogous language-level\nsupport for dynamic abstraction for planning with symbolic world models has also been developed (Zhi-Xuan,\n2022). It remains to be seen to what extent these new degrees of freedom can be exploited by language-to-code\nmodels targeting these newer probabilistic programming platforms.\nHow could the common-sense background knowledge needed for dynamic world model synthesis be\nrepresented, even in principle? Modern game engines may provide important clues. They can be reconfigured\nand scripted to simulate diverse imaginary worlds and narratives, featuring interactions between physical\nobjects and goal directed agents in both realistic and physically impossible environments. They routinely\ncombine simulations of the same environment at multiple levels of detail, making computational tradeoffs\nthat are in some ways analogous to the tradeoffs faced by human thinking. The level of scale, coherence,\nrealism, and computational efficiency that they achieve still vastly outstrips the best multi-modal neural\n45\n5.1\nScaling models of rational meaning construction\n5\nFUTURE DIRECTIONS\nmodels. Although some progress is already being made by synthesizing lightweight, probabilistic game engine\nscripts using language-to-code models (C. E. Zhang, Wong, Grand, & Tenenbaum, 2023), many fundamental\nchallenges remain. Game engines lack crucial affordances for robustly fitting world models to sparse data,\nsimulating rare events, and planning under uncertainty. And despite promising progress in neurally-guided\nprogram learning (Ellis et al., 2020), showing that libraries and DSLs can be learned from sparse data, there\nseems to be a long way to go before we can learn game-engine like rules that are sufficient to robustly model\ncommon sense. Flexible synthesis and learning mechanisms that can hope to scale across the vast scope of\nhuman thought thus seems to require new ideas that span and integrate probabilistic programming, cognitive\narchitecture, and hierarchical program learning.\n5.1.2\nScaling probabilistic inference in dynamically synthesized world models\nA central challenge not addressed by this paper is how to scale probabilistic inference to begin to approach the\nrobustness, speed, efficiency, and flexibility of human thought. Consider that the rejection sampling algorithm\nused in Sections 3 and 4 requires an exponentially-growing number of proposal attempts as the scenario\nbecomes less likely under the prior. Although many exact inference methods for probabilistic programs\nare much faster and more reliable, they are too restrictive to support many of the world models in this\npaper (Gehr, Misailovic, & Vechev, 2016; Gehr, Steffen, & Vechev, 2020; Holtzen, Van den Broeck, & Millstein,\n2020; Saad, Rinard, & Mansinghka, 2021; Shan & Ramsey, 2017). And although there are many approaches\nto generic approximate inference in probabilistic programs, drawing on MCMC (Carpenter et al., 2017;\nGoodman et al., 2008; Wingate, Stuhlm\u00fcller, & Goodman, 2011), sequential Monte Carlo (V. Mansinghka\net al., 2014; Tolpin, van de Meent, Yang, & Wood, 2016), and variational methods (Bingham et al., 2019;\nHoffman, Blei, Wang, & Paisley, 2013; V. Mansinghka et al., 2014; Ranganath, Gerrish, & Blei, 2014), they\nall routinely struggle to solve simple problems that can be solved by custom algorithms.\nOne potential way forward is to explicitly generate models of thinking processes that augment the world\nmodels with which they are thinking, by synthesizing inference programs (M. F. Cusumano-Towner et al.,\n2019; V. K. Mansinghka et al., 2018) tailored to specific problems. For example, Venture\u2019s inference meta-\nprogramming language is designed to enable concise specification of sequential inference processes that combine\nSMC, dynamic programming, MCMC, gradient-based optimization, and variational inference to perform\ninference in a sequence of world models and queries that grows dynamically. Data-driven proposals for use\nwith these thinking strategies can also be generated in real-time, without any offline learning, using dynamic\nprogramming over blocks of highly coupled variables. This approach has recently outperformed machine\nlearning methods on hard common-sense reasoning problems in databases with millions of records (A. Lew,\nAgrawal, Sontag, & Mansinghka, 2021). Scaling this approach will require not just synthesizing world\nmodels but automatically analyzing and decomposing them, analogously to how inference algorithm designers\ndecompose large inference problems into sequences of more tractable subproblems.\nAnother promising approach is to train neural networks to make data-driven proposals via amortized\ninference, potentially using synthetic data from an open-ended simulator of world models and queries (M. Wu\n& Goodman, 2022). This can be seen as an alternative to inference programming, avoiding the need for explicit\nsymbolic analysis of the process of thought. It can also be seen as a potential technique by which inference\nprograms might eventually be synthesized, once a suitable training corpus can be generated synthetically \u2014\nas well as a source of data-driven proposals that can be recombined by inference programs.\n5.1.3\nResource rational amortization in meaning construction and problem solving\nIn some of our examples, a sentence (e.g., \u201cGabe is stronger than Josh\u201d) is translated to a meaning representation\nthat looks very much like its classical formal semantics, composing the literal meanings of each word in the\nsentence. But in other examples (e.g., \u201cseveral of the faculty are real slackers\u201d), the translations appear to\nincorporate complex contextual and pragmatic judgments, judgments that might otherwise have been arrived\nat via probabilistic inference in a model of speakers, listeners, and their intents (Goodman & Frank, 2016).\nThis raises the question of where to draw the line between translation and inference. Versions of this question\nhave been extensively studied (e.g., does a word like \u201csome\u201d imply \u201cnot all\u201d as part of its meaning, or does\nthis implicature arise via after-the-fact pragmatic reasoning (Tessler, Tenenbaum, & Goodman, 2022)?), and\nsome past work has offered a unifying view via theories of amortized pragmatics (White et al., 2020), whereby\nRSA-style inferences are \u201ccompiled down\u201d into new word meanings.\n46\n5.2\nImplications for cognitive science\n5\nFUTURE DIRECTIONS\nA key feature of our architecture is that it is largely agnostic to where exactly the boundary should lie,\nand as such could help to model and extend this process of amortized inference in language understanding.\nFor example, as expanded on below, we could extend our symbolic world models to include aspects of the\nlanguage understanding process itself (such as those described in symbolic derivations of semantics (Heim &\nKratzer, 1998; Montague, 1970; Pollard & Sag, 1994; Steedman, 2001, 2011), and those used explicitly to\ncompute its pragmatic interpretations (Fox, 2007; Goodman & Frank, 2016)). Symbolic inferences about\nmeanings could then be used to train the language understanding module to directly generate the results of\nthis symbolic inference process\u2014for use either as a fully amortized pragmatic translator, or as a proposal\ndistribution within a larger Monte Carlo algorithm that could score and reject inaccurate translations.\nIn addition to making aspects of translation symbolic, we could consider approaches to amortizing the\nmore general probabilistic inferences required to answer queries. By supervising \u201ctranslation models\u201d directly\nwith the final outputs of symbolic inference, across a wide variety of tasks, we could enable a pure neural\ninference mode for these systems that may overcome some limitations of models trained only on language\nand code. As described above, such supervised models could also be incorporated as proposal distributions in\nposterior sampling algorithms, leading to improved efficiency without sacrificing the ability to correct for\nlearned biases that may be inapplicable when tackling novel problems.\nUltimately, we envision a new kind of neurosymbolic model in which, rather than pre-assigning responsibil-\nities to the neural or symbolic program, models may flexibly perform any part of the language understanding\nvia explicit probabilistic inference or learned, amortized prediction, with tradeoffs in speed and accuracy for\nany allocation of responsibilities to modules. The research question is how to do this automatically\u2014how do\nwe identify pieces of a computation that can reliably be emulated by a neural model, how do we train this\nneural model efficiently, and how do we decide at runtime which inference mode to use? As above, these\nquestions raise many opportunities to take inspiration from our scientific understanding of the separation of\nresponsibilities in language and thought, and work on learning for inference in more general probabilistic\nmodels.\n5.1.4\nLanguage generation\nThe preceding discussion has focused largely in problems of language understanding\u2014mapping from utterances\nto inferences about the state of the world that those utterances describe. But effective models of language\nuse should also be able to explain generation, making it possible to translate the results of inference back\nto language. As with the problem of language-informed thinking that we focus on this paper, it is useful\nto model language generation as two distinct processes: choosing what to say, then how to say it (Duboue\n& McKeown, 2003). And as with understanding, the first phase requires a model of the world, and of\nthe speaker\u2019s goals within it. What additional work is needed to adapt our models of rational meaning\nconstruction for generation?\nOne possibility, alluded to in the discussion of amortization above, is to interpret the language under-\nstanding machinery described above as a model of a listener, then perform language generation by selecting\nutterances that cause this model listener to form correct beliefs or take appropriate actions (Fox, 2007;\nGoodman & Frank, 2016). This extra layer of reasoning introduces major inferential challenges: the generation\nmodel most now reason both about the set of possible utterances and the effect of each utterance on the\ndistribution over possible worlds inferred by a listener. Here it is once again possible to leverage large-scale\nstatistical learning\u2014for example, using LLMs to directly translate candidate communicative intentions back\nto natural language strings, which may then be used as candidate utterances to be scored using a formal model\nof language understanding. Such a hybrid neuro-symbolic generation model (Fang et al., 2022; Langkilde\n& Knight, 1998) offers a path towards language generation that is expressive and fluent, but avoids the\ntruthfulness and hallucination problems that plague all purely neural language generation models that exist\ntoday (Maynez, Narayan, Bohnet, & McDonald, 2020; Wiseman, Shieber, & Rush, 2017).\n5.2\nImplications for cognitive science\nIn this section, we describe several research directions for other closely related disciplines that study language\nand thought in natural minds, brains, and behavior, focusing on productive intersections in relation to this\nframework.\n47\n5.2\nImplications for cognitive science\n5\nFUTURE DIRECTIONS\n5.2.1\nConnections to cognitive and formal models of linguistic structure\nIn all the examples described above, the process of translating utterances into formal meaning representations\nwas performed with a black-box statistical model, while reasoning about those meaning representations\nleveraged an explicit symbolic inferential process. However, an enormous body of work in linguistics has\nargued that the process of mapping from utterances to meaning representations can itself be described\n(at least approximately) in terms of symbol processing operations (Montague, 1970; Pollard & Sag, 1994;\nSteedman, 2001, inter alia). By design, most of our \u201cmeaning representations\u201d are designed to support\nefficient reasoning about domain-specific world models, and bear only a vague resemblance to formal and\ndomain-general linguistic representational theories. But can the symbolic models of linguistic meaning posited\nby these theories (as opposed to the symbolic models of reasoning we already draw on) be incorporated into\nour framework?\nAs noted in Section 5.1.3, a fully realized model of rational meaning construction should be able to flexibly\nmove computation across the statistical\u2013symbolic boundary, \u201ccompiling\u201d results of symbolic inference into\namortized computation, or retrieving symbolic descriptions of amortized processes for explicit verification. In\nthis view, the vignettes above treat the meaning representation process as culminating in domain-specific\nrepresentations and amortized by default. But probabilistic symbolic models of meaning (e.g., Kwiatkowksi,\nZettlemoyer, Goldwater, & Steedman, 2010), or Bayesian and game-theoretic models of semantics (e.g.,\nGoodman & Frank, 2016) can themselves be implemented as probabilistic programs and composed with\ndomain-specific inferential computations, resulting in an almost purely symbolic (but amortizable) language\nunderstanding process similar to the one described by Goodman and Lassiter (2015).\nSuch a model would also offer an appealing path toward learning language in a more sample-efficient (and\nperhaps human-like) ways. Today\u2019s neural sequence models require orders of magnitude more data than\nhuman learners to discover the structural regularities underlying human languages (Linzen, 2020). Explicit\nprobabilistic symbolic models, by contrast, can discover this structure extremely sample-efficiently (Yang &\nPiantadosi, 2022). A model that could automatically infer symbolic meaning representation rules from data,\nthen amortize this representation system into a statistical translation model (Liang, Daum\u00e9 III, & Klein,\n2008), would be capable of both efficient learning of language, and efficient modeling of other domains using\nlanguage. It would also offer a framework for modeling other key aspects of language acquisition, including\nexplicit linguistic instruction (of word meanings, rules of grammar, etc), tradeoffs between different formal\nrepresentational schemes, and the relationship between linguistic competence (understood as symbol-side\nlanguage processing) and linguistic performance (understood as statistical-side processing).\nThe semantic framework in this paper is most closely related to other cognitive semantic frameworks (eg.\nJackendoff (1985); Lakoff (1988); Pietroski (2018); Pinker (1984)) that explicitly propose that human language\nconstructs meanings from conceptual and cognitive primitives, including those for causal reasoning, or core\nknowledge representations of physics and agents. Related information-theoretic proposals have proposed that\nlanguages are effectively designed to be efficiently communicable externalizations of underlying thoughts\u2014that\nthe structure of human languages derives from underlying structure in the semantic representations we wish\nto communicate, and indeed may be driven by environmental and domain-specific pressures (eg. Gibson et al.\n(2019); Mollica et al. (2021); Zaslavsky, Kemp, Regier, and Tishby (2018)).\nOther related acquisition theories posit that these structural relationships between the representations of\nthought and externalizable language play an important role in language acquisition. Under these theories,\nhumans can so efficiently learn or hypothesize the meanings of sentences because they \u201cmap cleanly\" onto\nthe cognitive structures already present in the minds of the language learner (Snedeker, 2016); language\nlearning is bootstrapped by these predictable, structured mappings between the underlying space of meanings\nand the syntax of language (L. R. Gleitman et al., 2005; Hartshorne et al., 2016; Pinker & MacWhinney,\n1987). In preliminary experiments, we find intriguing evidence that large language-to-code models can extract\nand generalize syntactic patterns between language and code, including to bootstrap hypotheses about the\nsemantics of novel words expressed as probabilistic programs based on contextual, syntactic usage (see\nSyntactic Bootstrapping, Fig. 14). Future work can explore therefore whether these statistical distributional\nmodels might be used to implement cognitive models of bootstrapped language acquisition.\n48\n5.2\nImplications for cognitive science\n5\nFUTURE DIRECTIONS\n5.2.2\nModeling the mechanisms of human thought\nUsing tools for adaptive Bayesian inference over flexibly structured symbolic representations\u2014including\nnot only probabilistic programs but more generally hierarchical Bayesian models (Griffiths et al., 2010;\nTenenbaum, Kemp, Griffiths, & Goodman, 2011), resource-rational modeling (S. J. Gershman et al., 2015;\nLieder & Griffiths, 2020), and program induction (Lake et al., 2017; Piantadosi et al., 2012)\u2014computational\ncognitive scientists have built quantitatively predictive and functionally explanatory models of human behavior\nin almost every domain of cognition. This range spans from models of perception, concept learning and\ncategorization, causal reasoning, decision-making and planning, to intuitive physics, theory of mind, sentence\nprocessing, and cognitive and language development (C. Baker et al., 2011; Goodman & Frank, 2016; Goodman\net al., 2014; Griffiths & Tenenbaum, 2006; Ho, Saxe, & Cushman, 2022; Jara-Ettinger et al., 2020; Lake\net al., 2017; Perfors et al., 2011). However, in almost every one of these cases, the models are not fully\n\u201cstimulus-computable\u201d: Behavioral experiments in cognitive psychology almost always use natural language\nto present participants with some situation for thinking about (in addition to perhaps perceptual stimuli);\nlanguage is also almost invariably used to pose some question or goal as the end for thinking. Put another\nway, almost all our behavioral experiments\u2014like so many instances of cognition in the wild\u2014follow the\nlanguage-informed thinking paradigm of this paper. But our cognitive models traditionally do not; they\nare created by hand from the modeler\u2019s understanding of the natural language task description, rather\nthan synthesized automatically from the linguistic stimuli presented to participants. To what extent can\nthe rational meaning construction framework presented here reduce the need for computational cognitive\nscientists to manually create Bayesian models that match the natural-language prompts given to humans\nin behavioral experiments? Can we build \u201clanguage-computable\u201d models of human thought, that are much\neasier to test and vary via large-scale online experiments?\nWe have already begun to explore these possibilities and shown promising preliminary results in several\ndomains, including to model how language implicates commonsense physical reasoning about linguistic scenes\n(C. E. Zhang et al., 2023), social reasoning about goal-directed agents (Ying et al., 2023); as well as to\ntest the claim that the LLM-based meaning function we implement in this paper can compute amortized\npragmatic judgments of scalar implicatures that accord with human interpretations (Lipkin, Wong, Grand, &\nTenenbaum, 2023).\nThere is also a growing body of research in computational cognitive science showing that salient dynamics\nof thought, including well-known departures from Bayesian norms, can be explained via Monte Carlo inference\napproximations that aim to rationally use limited computational resources (Chater et al., 2020; S. J. Gershman\net al., 2015; Lieder & Griffiths, 2020; Lieder, Hsu, & Griffiths, 2014; Sanborn & Chater, 2017). In some\ncases, human inferences seem to rest on just a single, highly approximate sample (Vul, Goodman, Griffiths,\n& Tenenbaum, 2014), or perhaps just a few of them (Vul & Pashler, 2008). If we extend our proposed\narchitecture for rational meaning construction to incorporate these kinds of Monte Carlo mechanisms, could\nwe build models of language-guided thinking that can be directly compared at a more mechanistic level to\nhuman behavior? How will processes of language understanding and reasoning interact mechanistically, and\ncan we build resource-rational approximate inference models that capture this interaction?\n5.2.3\nLanguage and thought in the brain\nEvidence from cognitive neuroscience suggests a number of parallels between the framework we describe in\nthis paper, and how language relates to systems for general cognition in the human brain. Over decades,\ncognitive neuroscientists have mapped out a series of interconnected areas in the frontal and temporal lobes\nthat are implicated in human language processing. This \u201clanguage network\u201d is activated in both linguistic\ncomprehension (Deniz, Nunez-Elizalde, Huth, & Gallant, 2019; Fedorenko, Hsieh, Nieto-Casta\u00f1\u00f3n, Whitfield-\nGabrieli, & Kanwisher, 2010; MacSweeney et al., 2002; Regev, Honey, Simony, & Hasson, 2013; T. L. Scott,\nGall\u00e9e, & Fedorenko, 2017) and production (Hu et al., 2021; Menenti, Gierhan, Segaert, & Hagoort, 2011).\nIt is sensitive to regularities in all levels of linguistic structure\u2014from phonology, to words, to phrases and\nsentences (Blank & Fedorenko, 2017; Lerner, Honey, Silbert, & Hasson, 2011; Silbert, Honey, Simony, Poeppel,\n& Hasson, 2014; Wilson, Molnar-Szakacs, & Iacoboni, 2008) and is implicated in combinatorial semantic and\nsyntactic processing (Fedorenko, Blank, Siegelman, & Mineroff, 2020; Hu et al., 2021).\nConvergent evidence suggests that the language network is distinct from these systems, and that it isnot\nactivated in more general, non-linguistic cognition. Aphasic individuals with damage to the language network\n49\n5.3\nImplications for AI\n5\nFUTURE DIRECTIONS\nexhibit impaired language production and comprehension, but retain the ability to solve arithmetic and logic\npuzzles, reason about causality and social situations, and perform many other non-linguistic tasks (e.g., Basso\n& Capitani, 1985; Bek, Blades, Siegal, & Varley, 2010; Fedorenko & Varley, 2016; Klessinger, Szczerbinski,\n& Varley, 2007; Lecours & Joanette, 1980; Luria, Tsvetkova, & Futer, 1965; Varley, 1998). Functional\nneuroimaging studies provide further evidence that the language network is not activated in a variety of\nnon-linguistic tasks including reasoning about arithmetic, logic, actions, or events (Amalric & Dehaene, 2016,\n2019; Blank, Kanwisher, & Fedorenko, 2014; Deen, Koldewyn, Kanwisher, & Saxe, 2015; Fedorenko, Behr, &\nKanwisher, 2011; Monti, Osherson, Martinez, & Parsons, 2007; Monti, Parsons, & Osherson, 2012; Paunov,\nBlank, & Fedorenko, 2019; Paunov et al., 2022; Shain, Paunov, Chen, Lipkin, & Fedorenko, 2022).\nIn tandem, a broader line of cognitive neuroscience work has located non-linguistic networks that are\nactivated in processing many of the core cognitive domains we model throughout this paper, including logic,\nmathematical reasoning (eg. Amalric and Dehaene (2019); Monti et al. (2007)), social reasoning and planning\n(Adolphs, 2009; Saxe, Moran, Scholz, & Gabrieli, 2006; Saxe & Powell, 2006); and physical reasoning and\nsimulation (Pramod, Cohen, Tenenbaum, & Kanwisher, 2022; Schwettmann, Tenenbaum, & Kanwisher, 2019).\nMore recent work suggests the existence of an \u201camodal semantics network\" (Ivanova, 2022; Ivanova et al.,\n2021), a network that appears proximal to the language networks activated in processing linguistic structures,\ninterfaces between the language network and the more general multiple demand networks involved in complex\nnon-linguisitc cognition, and that appears to be activated specifically in processing semantically meaningful\nsentences (as opposed to scrambled tokens or syntactically correct but semantically incoherent strings.)\nRecently, neuroscientists who study language cognition have begun to draw explicit parallels between the\nlanguage network and LLMs (see Mahowald et al., 2023, for a review). Several recent studies have observed\nthat smaller LLMs trained specifically on the distriutional statistics of language (generally focusing on the\nGPT-2 model) can predict brain activity in humans processing sentence input (Caucheteux & King, 2022;\nGoldstein et al., 2022; Schrimpf et al., 2021) and may share representational characteristics of the human\nlanguage network (Fedorenko et al., 2020; Shain, Blank, van Schijndel, Schuler, & Fedorenko, 2020). These\naccounts, however, align LLMs with the modular role we propose for neural models in our framework\u2014not\nas end-to-end models of language and reasoning, but instead as robust, context-aware mappings between\nlanguage and meanings. As a ground for future work, our framework can inform evaluations of LLMs with\nrespect to human language understanding. For instance, our proposal suggests that code-trained LLMs might\nbetter capture latent semantic and syntactic structure than language-only LLMs. Ideas from neuroscience, in\nturn, can help us figure out which kinds of computations can be neurally amortized and where our model\u2019s\nboundary between language and thought should lie.\n5.3\nImplications for AI\n5.3.1\nStructured hybrid models of language and thought\nGrowing awareness of the limitations of LLM-based reasoning has motivated several recent proposals for\ninterfacing language models with external symbolic plug-ins or toolkits (Karpas et al., 2022; OpenAI, 2023c;\nSchick et al., 2023; Wolfram, 2023). At face value, one perspective is to view rational meaning construction\nas an argument to add probablistic programs to the growing \u201cswiss army knife\u201d of LLM plug-ins. However,\nwe see this notion as inverted: thought should not simply be a plug-in on top of language models. Rather, we\nbelieve that future AI systems should be architected around thought\u2014general-purpose computing systems that\nprovide a principled framework for expressing world models, conditioning them on observations from sources\nincluding language and perceptual input, and drawing principled inferences and decisions with respect to the\ngoals of an intelligent system.7 As we show throughout this paper, many core domains of cognition can be\nexpressed as forms of probabilistic inference. A probabilistic language of thought, in turn, provides a unifying\nlanguage for world modeling that can nest calls to other cognitively-motivated modules. In this sense, all of\nthese plug-ins and modules would become plug-ins to the substrate of thought, including graphics engines,\nphysics simulators, planning algorithms, and, in fact, language models themselves. As we discuss in the future\ndirections of each section, scaling any of our toy implementations towards robust, human-like reasoning and\nlanguage-understanding systems will almost certainly require more sophisticated implementations of each\n7A similar argument has been expressed by Stephen Wolfram in a compelling series of writings on integrating ChatGPT with\nthe Wolfram Language and its suit of symbolic computational tools (Wolfram, 2023).\n50\n5.3\nImplications for AI\n5\nFUTURE DIRECTIONS\nreasoning module. We therefore hope this general probabilistic framework suggests a symbolic substrate that\nmight in turn incorporate many of the specific modules and plug-ins in this recent work.\nTo this end, another important near-term AI research direction will involve building probabilistic pro-\ngramming frameworks that natively incorporate LLMs. Important steps in this direction are already being\ntaken through work leveraging LLMs to approximate prior probabilities over strings (A. K. Lew, Tessler,\nMansinghka, & Tenenbaum, 2020) and amortize complex posterior inferences (M. Wu & Goodman, 2022).\nIndeed, many popular LLM techniques, such as scratchpads (Nye et al., 2021), chain-of-thought prompting\n(Wei et al., 2022), selection-inference (Creswell, Shanahan, & Higgins, 2022), STaR (Zelikman, Wu, Mu, &\nGoodman, 2022), and others can be viewed as implementations of probabilistic programs over string-valued\nrandom variables (Dohan et al., 2022). A maturing theoretical understanding of LLMs as probabilistic entities\nwill afford powerful ways of harnessing and controlling generations. For instance, the sequential Monte Carlo\n(SMC) steering technique introduced under the LLaMPPL framework (A. K. Lew, Zhi-Xuan, Grand, &\nMansinghka, 2023) enables concise and tractable specification of infilling, prompt intersection, and other\nconstrained LLM generation tasks as language model probabilistic programs. Many of these hybrid models can\nbe viewed as instantiations of rational meaning construction that make resource-motivated tradeoffs between\ninference in the unstructured space of strings (words) and more structured hypothesis spaces (worlds).\n5.3.2\nRobustness and trustworthiness in language understanding\nRecent, high-profile attempts to deploy LLMs in production highlight the fundamental robustness challenges\nof using these models as the backbone of usable AI systems (Brereton, 2023; Sorkin, Warner, Kessler, Hirsch,\n& Livni, 2023), even with automated filters and supervised finetuning to human preferences. While LLMs\nmay reasonably appear to condition on input language or answer queries under some circumstances, it is\nprecisely this combination of linguistic fluency and underlying unpredictability that makes them problematic\nin situations where verifiable, systematic behavior is paramount. LLMs easily produce syntactically convincing\nbut inaccurate \u201challucinations\u201d that fabricate facts and inferences (Dziri, Milton, Yu, Zaiane, & Reddy, 2022;\nJi et al., 2022), fail to consistently condition on rules and constraints described in natural language, including\nrules intended to ensure user safety (Edwards, 2023; Zhuo, Huang, Chen, & Xing, 2023), and can generally\ndegrade into nonsensical or highly undesirable language in the vast, easily accessible \u201clong tail\u201d of situations\nthat deviate from their training distribution (Bender, Gebru, McMillan-Major, & Shmitchell, 2021; Roose,\n2023; Tangermann, 2023).\nThe unevenness of today\u2019s LLMs recalls a classic critique of even older neural architectures (Fodor &\nPylyshyn, 1988)\u2014that neural models trained on predictive objectives do not produce systematic, logical\noutputs by design. Similarly, while current or future LLMs may be able in principle to recover the latent\nrepresentations and algorithms necessary to reason over language\u2014or even successfully approximate them\nin many settings\u2014they do not need to produce systematic results by construction. Rather, they often\napproximate them with unexpected, undesirable outputs, particularly in out-of-distribution settings.\nEven if future LLMs do appear to improve with scale without an external reasoning substrate, engineers\nmay find it desirable to distinguish modularly between external symbolic reasoning engines and language-\nspecific systems to enable separate supervision and verification of each. The framework we present here\noffers one roadmap for language understanding architectures whose robustness guarantees derive from explicit\ninference over a structured, editable, and formally constrainable programming language. Inferences themselves,\nand other formalizable reasoning computations including planning and physical simulation, take place in\nmodules constructed explicitly to perform these calculations.\n5.3.3\nInterpreting models that use language\nAs with verifiability and robustness, the framework we propose here is an architecture for language under-\nstanding systems that are also inherently interpretable, or interpretable by design (Rudin, 2019; Rudin et\nal., 2022)\u2014it constructs visible, editable, and constrainable world models and meanings that serve as the\nformal basis for inference, rather than post-hoc explanations decoded from or produced over hidden internal\ncomputations.\nHowever, a fundamental part of our hypothesis is that any system that reasons effectively over language\nshould need to\u2014explicitly or implicitly\u2014represent and implement the kinds of computations we formalize\nthroughout this paper. Implementations of this framework might therefore also be useful for model-guided\n51\n5.3\nImplications for AI\n5\nFUTURE DIRECTIONS\nhypotheses and experiments intended to explain other less transparent language processing systems, both\nbiological (as we suggest in Section 5.2.3) and artificial. This framework might be incorporated productively\ninto the growing body of work using explicit world models and symbolic languages to formally model the\ninternal computations of deep neural models (Biggio, Bendinelli, Neitz, Lucchi, & Parascandolo, 2021; Mu &\nAndreas, 2020) and LLMs specifically (B. Z. Li, Nye, & Andreas, 2021); as with the related body of work\nusing structured probabilistic models and reasoning engines to interpret human neural activity on social\nreasoning, physical understanding, and other general inference tasks (Ho et al., 2022; Schwettmann, Fischer,\nTenenbaum, & Kanwisher, 2018; Watters, Tenenbaum, & Jazayeri, 2021). Explaining how LLMs represent\nthe meanings of language, and perform computations with them, is a pressing open question whose scientific\ninterest only increases if LLMs do appear to become more coherent and robust with scale.\nIn light of this, inspired by our proposed architecture, it may be interesting to probe, or trace, whether\nend-to-end LLMs construct context-specific world models (B. Z. Li et al., 2021), maintain belief distributions\nover uncertain world states (Hase et al., 2021), and implement reasoning algorithms like probabilistic inference,\nphysical simulation, or planning over these representations.\n5.3.4\nLearning from human-scale data\nLarge language models must be trained with many orders of magnitude more language data than any human\nlearner encounters over a lifetime. How can we engineer systems that not only understand language as we do,\nbut also learn from human-scale language data?\nEffective, data-efficient language models hold great relevance for both scientific and engineering applications.\nComplete cognitive models of human language understanding\u2014including models built on the framework we\npropose here\u2014should account for language acquisition, as well as language use. For engineering purposes,\naddressing the data-hungry training regime of current LLMs could also address challenges in learning\nlow-resource languages (and the more general problem of accurately learning and deploying the \u201clong\ntail\u201d of knowledge from statistical distributional data) (Kandpal, Deng, Roberts, Wallace, & Raffel, 2022),\nincorporating more expensive input modalities like videos or embodied trajectories (Ahn et al., 2022; Reed et\nal., 2022), finetuning on more targeted, task-specific supervision like instruction following (OpenAI, 2023a),\nand generally enabling the construction of smaller, more accessible models that can be trained without\nmassive computing resources and prohibitive economic and environmental costs (Bender et al., 2021; Dickson,\n2020). While current \u201cscaling routes\u201d look to improve language understanding by increasing data supervision,\nour hypothesis strongly suggests that this is an expensive, and highly indirect, approach towards learning the\nrepresentations and inference procedures necessary to reason about language.\nInstead, our framework suggests several alternative directions for improving data efficiency. First, perhaps\nthe most direct consequence of this framework is the suggestion that neural models need only play a much\ntighter, focused role in language understanding systems\u2014as translation models that parse from language into\nstructured symbolic programs for reasoning. Training a translation model focused on parsing from language\ninto probabilistic programs almost certainly requires much less data for effective performance than required\nto solve the general token prediction problem.\nFurther, several ideas we discuss in Section 5.1.1 and Section 5.1.3 might also be relevant for training\nsimpler translation models, and using them to bootstrap larger and more complex neural language models.\nFirst, as we discuss in Section 5.1.3, we might consider a progressively amortized avenue for training\neven complex translation models like the one in our concrete implementation, which appears to contextually\namortize certain pragmatic inferences (such as those that adjust vague quantifiers to the context of a particular\nworld model) that could be explicitly computed from a more literal initial semantic parse. One possibility,\nthen, would be to train a more limited, literal semantic parser from language to probabilistic programs, but\nseek to train neural models that progressively amortize more of these inferences by supervising on its outputs.\nOther ideas from human language acquisition might offer more avenues for more radically data efficient\nlearning. Human language learners progress through several phases of language mastery (R. Brown, 1973;\nSaffran et al., 2001; Tomasello, 2009), appearing to learn initial but highly imperfect grammars and meaning\nfunctions that they refine progressively over time, but much more quickly and with much less data than\na comparable LLM trained directly on the distribution of language. Framed as a problem of learning a\ntranslation model, however, a more data efficient training regime might also draw inspiration from other\nmethods for learning more flexible translation and semantic parsing distributions. Multiple approaches\n52\n6\nCONCLUSION\nhave used simpler models to bootstrap more complex ones, either by using simpler models trained on more\nconstrained translation objectives to directly initialize the parameters of more complex ones (P. F. Brown,\nDella Pietra, Della Pietra, Mercer, et al., 1993; Dong & Lapata, 2018; Petrov, Haghighi, & Klein, 2008), or\nusing simpler grammars as generative data sources to train more complex models, as in general wake-sleep\ntraining methods that learn predictive models to amortize the outputs of a generative distribution (Andreas,\n2019; Hinton, Dayan, Frey, & Neal, 1995; Jia & Liang, 2016).\nBoth of these approaches rely, importantly, on the language of thought hypothesis we advance here, which\nseparates the computational problem of learning a translation distribution from the problem of learning the\nrepresentations and algorithms necessary for general intelligence. This drastically reduces the latent structure\nand computational complexity we seek to learn from distributional supervision\u2014to learn as efficiently as\npeople, we propose a framework that begins with a substrate for thinking and then suggests avenues for\namortizing its outputs or refining translation into this substrate, rather than seeking to learn an effective\nlanguage of thought itself from natural language data.\n6\nConclusion\nLanguage is central to our cognition. A theory of meaning in human language should explain how language\nrelates to our thoughts\u2014how it connects to all our faculties for reasoning, and how it can shift our beliefs\nacross nearly every domain of what we now, change how we act or respond across a broad range of situations,\neven construct new knowledge that we might later marshal towards yet unspoken questions and goals. This\nvision lies at the heart of a human theory of language and meaning, but the most expansive visions of AI\nhave also long been ones in which computers share our language, able to meaningfully understand us as\nwe expect to be understood by other people. Today\u2019s large language models have made striking advances\ntowards building this reality in many important regards. For the first time, we have built computer systems\nthat can speak fluently back to us, using many more of our own words than ever before.\nStill, much more is needed to capture our own relationship to language. We do not learn language like a\nlarge language model does. We think first, and learn from far less input how language maps into our thoughts.\nOur own world models and beliefs are not the fragile byproduct of what we can glean from language\u2014they are\nthe basis of and core of our cognition, constructed and maintained purposefully towards our intentions and\ndesires. We, of course, are the ones who created the language on which today\u2019s machine learning models are\nnow trained. That language is the product of and reflection of our own goals and questions, and of conceptual\nsystems of our own invention. We continue to think completely new thoughts, and we continue in turn to\nproduce entirely new language, coining new words and even constructing wholly new languages so that we\ncan build its meaning in the minds of other humans. A cognitive theory of human language must capture\nand explain these aspects of our language and thought. It might in turn form the basis for AI models that\nreliably and predictably understand us, and that work in ways that we can interpret, explain, and control.\nThis white paper is simply a sketch towards these ends: an outline of the computational components that\ncould relate human language and a substrate for cognition, and one proposal for how this approach might\nalso incorporate today\u2019s language models without requiring them to learn to reliably model the world, draw\ninferences, or make decisions. We hope it can offer one step towards cognitive and AI models that share the\nmeaning we make from language, and that bridge from language into the vast expanse of our thoughts.\n53\nREFERENCES\nAcknowledgements\nWe have many people to thank whose comments, critiques, and feedback have influenced this manuscript and\nshaped it for the better. Among others, we are grateful to Steve Piantadosi, Jesse Snedeker, Kate Davidson,\nEllie Pavlick, Paul Pietroski, Thomas Icard, Luca Bonatti, and Susan Carey for their insightful comments on\nan early version of this manuscript that was presented at the July 2022 McDonnell Network Workshop; as\nwell as for innumerable helpful comments and feedback on developing versions of this manuscript from Joshua\nHartshorne, Judy Fan, Robert Hawkins, Katherine Collins, Anna Ivanova, Cedegao Zhang, Hayley Ross,\nAnna Ivanova, Benjamin Lipkin, Megan Wei, Jiahai Feng, Xuan Tan, Lance Ying, William McCarthy, Laura\nSchulz and Tyler Brooke-Wilson. Language from all of these collaborators has invaluably and profoundly\ninformed our thoughts.\nThe authors gratefully acknowledge support from support from the MIT Quest for Intelligence, AFOSR\nGrant No. FA9550-19-1-0269, the MIT-IBM Watson AI Lab, the DARPA Machine Common Sense Program,\nthe ONR Science of AI Program, and Siegel Family Endowment. This material is based on work supported by\nthe National Science Foundation Graduate Research Fellowship under Grant No. 1745302 and No. 2141064.\nAdditionally, GG was supported by the MIT Presidential Fellowship, and JDA was supported by NSF Grant\nIIS-2212310.\nReferences\nAbend, O., Kwiatkowski, T., Smith, N. J., Goldwater, S., & Steedman, M. (2017). Bootstrapping language\nacquisition. Cognition, 164, 116\u2013143.\nAdolphs, R. (2009). The social brain: neural basis of social knowledge. Annual review of psychology, 60,\n693\u2013716.\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., . . . others (2022). Do as I can, not as\nI say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691.\nAllen, K. R., Smith, K. A., & Tenenbaum, J. B. (2020). Rapid trial-and-error learning with simulation\nsupports flexible tool use and physical reasoning. Proceedings of the National Academy of Sciences,\n117(47), 29302\u201329310.\nAlon, U., Xu, F. F., He, J., Sengupta, S., Roth, D., & Neubig, G. (2022). Neuro-Symbolic Language Modeling\nwith Automaton-augmented Retrieval. undefined.\nAmalric, M., & Dehaene, S. (2016, May). Origins of the brain networks for advanced mathematics in expert\nmathematicians. Proceedings of the National Academy of Sciences of the United States of America,\n113(18), 4909\u20134917. doi: 10.1073/pnas.1603205113\nAmalric, M., & Dehaene, S. (2019, April). A distinct cortical network for mathematical knowledge in the\nhuman brain. NeuroImage, 189, 19\u201331. Retrieved 2019-07-26, from https://linkinghub.elsevier.com/\nretrieve/pii/S1053811919300011 doi: 10.1016/j.neuroimage.2019.01.001\nAnderson, J. R. (1990). The adaptive character of thought. Psychology Press.\nAndreas, J. (2019). Good-enough compositional data augmentation. arXiv preprint arXiv:1904.09545.\nArmeni, I., He, Z.-Y., Gwak, J., Zamir, A. R., Fischer, M., Malik, J., & Savarese, S. (2019). 3d scene graph:\nA structure for unified semantics, 3d space, and camera. In Proceedings of the ieee/cvf international\nconference on computer vision (pp. 5664\u20135673).\nArtzi, Y., Das, D., & Petrov, S. (2014). Learning compact lexicons for ccg semantic parsing.\nArtzi, Y., Lee, K., & Zettlemoyer, L. (2015, September). Broad-coverage ccg semantic parsing with amr.\nIn Proceedings of the conference on empirical methods in natural language processing (pp. 1699\u20131710).\nLisbon, Portugal: Association for Computational Linguistics. Retrieved from http://aclweb.org/\nanthology/D15-1198\nArtzi, Y., & Zettlemoyer, L. (2013). Weakly supervised learning of semantic parsers for mapping instructions\nto actions. Transactions of the Association for Computational Linguistics, 1(1), 49\u201362.\nBai, J., Zhou, L., Blanco, A., Liu, S., Wei, F., Zhou, M., & Li, Z. (2021). Jointly learning to repair code and\ngenerate commit message. ArXiv, abs/2109.12296.\nBaillargeon, R. (2004). Infants\u2019 physical world. Current directions in psychological science, 13(3), 89\u201394.\nBaker, C., Saxe, R., & Tenenbaum, J. (2011). Bayesian theory of mind: Modeling joint belief-desire\nattribution. In Proceedings of the annual meeting of the cognitive science society (Vol. 33).\n54\nREFERENCES\nBaker, C. L., Saxe, R., & Tenenbaum, J. B. (2009). Action understanding as inverse planning. Cognition,\n113(3), 329\u2013349.\nBaker, C. L., Tenenbaum, J. B., & Saxe, R. R. (2007). Goal inference as inverse planning. In Proceedings of\nthe annual meeting of the cognitive science society (Vol. 29).\nBar-Zeev, A. (2003). Scenegraphs: Past, present and future. \u00daltimo acesso em, 13.\nBasso, A., & Capitani, E. (1985, May). Spared musical abilities in a conductor with global aphasia and\nideomotor apraxia. Journal of Neurology, Neurosurgery, and Psychiatry, 48(5), 407\u2013412. Retrieved\n2020-08-03, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1028326/\nBattaglia, P. W., Hamrick, J. B., & Tenenbaum, J. B. (2013). Simulation as an engine of physical scene\nunderstanding. Proceedings of the National Academy of Sciences, 110(45), 18327\u201318332.\nBek, J., Blades, M., Siegal, M., & Varley, R. A. (2010, May). Language and spatial reorientation: evidence\nfrom severe aphasia. Journal of Experimental Psychology. Learning, Memory, and Cognition, 36(3),\n646\u2013658. doi: 10.1037/a0018281\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic parrots:\nCan language models be too big? In Proceedings of the 2021 acm conference on fairness, accountability,\nand transparency (pp. 610\u2013623).\nBiernaskie, J. M., Walker, S. C., & Gegear, R. J. (2009). Bumblebees learn to forage like bayesians. The\nAmerican Naturalist, 174(3), 413\u2013423.\nBiggio, L., Bendinelli, T., Neitz, A., Lucchi, A., & Parascandolo, G. (2021). Neural symbolic regression that\nscales. In International conference on machine learning (pp. 936\u2013945).\nBingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F., Pradhan, N., Karaletsos, T., . . . Goodman, N. D.\n(2019). Pyro: Deep universal probabilistic programming. J. Mach. Learn. Res., 20, 28:1\u201328:6. Retrieved\nfrom http://jmlr.org/papers/v20/18-403.html\nBlank, I. A., & Fedorenko, E. (2017, October). Domain-General Brain Regions Do Not Track Linguistic Input as\nClosely as Language-Selective Regions. Journal of Neuroscience, 37(41), 9999\u201310011. Retrieved 2019-11-\n06, from https://www.jneurosci.org/content/37/41/9999 doi: 10.1523/JNEUROSCI.3642-16.2017\nBlank, I. A., Kanwisher, N., & Fedorenko, E.\n(2014, September).\nA functional dissociation between\nlanguage and multiple-demand systems revealed in patterns of BOLD signal fluctuations. Journal of\nNeurophysiology, 112(5), 1105\u20131118. doi: 10.1152/jn.00884.2013\nBlock, N. (1998). Conceptual role semantics.\nBloom, P. (2002). How children learn the meanings of words. MIT press.\nBolton, A. D., Haesemeyer, M., Jordi, J., Schaechtle, U., Saad, F. A., Mansinghka, V. K., . . . Engert, F.\n(2019). Elements of a stochastic 3d prediction engine in larval zebrafish prey capture. ELife, 8, e51975.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., . . . others (2021). On the\nopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., . . . Sifre, L. (2022, February).\nImproving language models by retrieving from trillions of tokens (No. arXiv:2112.04426). arXiv.\nBowers, M., Olausson, T. X., Wong, L., Grand, G., Tenenbaum, J. B., Ellis, K., & Solar-Lezama, A. (2023,\njan). Top-down synthesis for library learning. Proc. ACM Program. Lang., 7(POPL). Retrieved from\nhttps://doi.org/10.1145/3571234 doi: 10.1145/3571234\nBranwen, G. (2022). The scaling hypothesis. Gwern.net.\nBrereton, D. (2023). Bing ai can\u2019t be trusted. https://dkb.blog/p/bing-ai-cant-be-trusted.\nBrooke-Wilson, T. (2023). Why is seeing fast and thinking slow? in prep.\nBrown, P. F., Della Pietra, S. A., Della Pietra, V. J., Mercer, R. L., et al. (1993). The mathematics of\nstatistical machine translation: Parameter estimation.\nBrown, R. (1973). A first language: The early stages.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., . . . Amodei, D.\n(2020,\nJuly). Language Models are Few-Shot Learners. arXiv:2005.14165 [cs]. Retrieved 2020-08-09, from\nhttp://arxiv.org/abs/2005.14165 (arXiv: 2005.14165)\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., . . . others (2023). Sparks of\nartificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.\nBybee, J. L. (1985). Morphology. Typological studies in language.\nCai, Q., & Yates, A. (2013). Large-scale semantic parsing via schema matching and lexicon extension. In\nProceedings of the 51st annual meeting of the association for computational linguistics (volume 1: Long\n55\nREFERENCES\npapers) (pp. 423\u2013433).\nCaliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora\ncontain human-like biases. Science, 356(6334), 183\u2013186. Retrieved from https://www.science.org/\ndoi/abs/10.1126/science.aal4230 doi: 10.1126/science.aal4230\nCarey, S. (1999). Sources of conceptual change. Conceptual development: Piaget\u2019s legacy, 293\u2013326.\nCarey, S. (2009). The origin of concepts. New York: Oxford University Press.\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., . . . Riddell, A. (2017).\nStan: A probabilistic programming language. Journal of statistical software, 76(1).\nCaucheteux, C., & King, J.-R. (2022, February). Brains and algorithms partially converge in natural\nlanguage processing. Communications Biology, 5(1), 1\u201310. Retrieved 2022-07-05, from https://\nwww.nature.com/articles/s42003-022-03036-1\n(Number: 1 Publisher: Nature Publishing Group)\ndoi: 10.1038/s42003-022-03036-1\nChakraborty, S., Ding, Y., Allamanis, M., & Ray, B. (2022). Codit: Code editing with tree-based neural\nmodels. IEEE Transactions on Software Engineering, 48, 1385\u20131399.\nChakraborty, S., & Ray, B. (2021). On multi-modal learning of editing source code. 2021 36th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE), 443\u2013455.\nChater, N., & Manning, C. D. (2006). Probabilistic models of language processing and acquisition. Trends in\ncognitive sciences, 10(7), 335\u2013344.\nChater, N., & Oaksford, M. (1999). Ten years of the rational analysis of cognition. Trends in cognitive\nsciences, 3(2), 57\u201365.\nChater, N., Zhu, J.-Q., Spicer, J., Sundh, J., Le\u00f3n-Villagr\u00e1, P., & Sanborn, A. (2020). Probabilistic biases\nmeet the bayesian brain. Current Directions in Psychological Science, 29(5), 506\u2013512.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., . . . others (2021). Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374.\nChu, J., & Schulz, L. (2023). In praise of folly: Flexible goals and human cognition.\nClark, J. H. (1976). Hierarchical geometric models for visible surface algorithms. Communications of the\nACM , 19(10), 547\u2013554.\nCobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., & Schulman, J. (2021). Training\nverifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\nCollins, K. M., Wong, C., Feng, J., Wei, M., & Tenenbaum, J. B. (2022, May). Structured, flexible, and\nrobust: Benchmarking and improving large language models towards more human-like behavior in\nout-of-distribution reasoning tasks (No. arXiv:2205.05718). arXiv. doi: 10.48550/arXiv.2205.05718\nColmerauer, A., Kanoui, H., Pasero, R., & Roussel, P. (1972). Un systeme de communication en fran\u00e7ais.\nRapport pr\u00e9liminaire de fin de contrat IRIA, Groupe Intelligence Artificielle, Facult\u00e9 des Sciences de\nLuminy, Universit\u00e9 d\u2019Aix-Marseille II .\nColmerauer, A., & Roussel, P. (1996). The birth of prolog. In History of programming languages\u2014ii\n(p. 331\u2013367). New York, NY, USA: Association for Computing Machinery. Retrieved from https://\ndoi.org/10.1145/234286.1057820\nConwell, C., & Ullman, T. D. (2022). Testing relational understanding in text-guided image generation.\narXiv preprint arXiv:2208.00005.\nCoumans, E., & Bai, Y. (2016). Pybullet, a python module for physics simulation for games, robotics and\nmachine learning.\nCraik, K. J. W. (1967). The nature of explanation (Vol. 445). CUP Archive.\nCreswell, A., Shanahan, M., & Higgins, I. (2022, May). Selection-Inference: Exploiting Large Language Models\nfor Interpretable Logical Reasoning (No. arXiv:2205.09712). arXiv. doi: 10.48550/arXiv.2205.09712\nCsibra, G. (2008). Goal attribution to inanimate agents by 6.5-month-old infants. Cognition, 107(2),\n705\u2013717.\nCsibra, G., B\u00edr\u00f3, S., Ko\u00f3s, O., & Gergely, G. (2003). One-year-old infants use teleological representations of\nactions productively. Cognitive Science, 27(1), 111\u2013133.\nCusumano-Towner, M., Bichsel, B., Gehr, T., Vechev, M., & Mansinghka, V. K. (2018). Incremental inference\nfor probabilistic programs. In Proceedings of the 39th acm sigplan conference on programming language\ndesign and implementation (pp. 571\u2013585).\nCusumano-Towner, M., Lew, A. K., & Mansinghka, V. K. (2020). Automating involutive MCMC using\nprobabilistic and differentiable programming. arXiv preprint arXiv:2007.09871.\n56\nREFERENCES\nCusumano-Towner, M. F., Radul, A., Wingate, D., & Mansinghka, V. K. (2017). Probabilistic programs for\ninferring the goals of autonomous agents. arXiv preprint arXiv:1704.04977.\nCusumano-Towner, M. F., Saad, F. A., Lew, A. K., & Mansinghka, V. K. (2019). Gen: a general-purpose\nprobabilistic programming system with programmable inference. In Proceedings of the 40th acm sigplan\nconference on programming language design and implementation (pp. 221\u2013236).\nDalvi, B., Tafjord, O., & Clark, P. (2022). Towards teachable reasoning systems: Using a dynamic memory\nof user feedback for continual system improvement. In Proceedings of the 2022 conference on empirical\nmethods in natural language processing (pp. 9465\u20139480).\nDasgupta, I., & Gershman, S. J. (2021). Memory as a computational resource. Trends in Cognitive Sciences,\n25(3), 240\u2013251.\nDavidson, D., & Rescher, N. (1967). The logical form of action sentences. 1967, 105\u2013122.\nDavidson, G., Gureckis, T. M., & Lake, B. (2022). Creativity, compositionality, and common sense in human\ngoal generation. In Proceedings of the annual meeting of the cognitive science society (Vol. 44).\nde Avila Belbute-Peres, F., Smith, K., Allen, K., Tenenbaum, J., & Kolter, J. Z. (2018). End-to-end\ndifferentiable physics for learning and control. Advances in neural information processing systems, 31.\nDechter, E., Malmaud, J., Adams, R. P., & Tenenbaum, J. B. (2013). Bootstrap learning via modular concept\ndiscovery. In Proceedings of the international joint conference on artificial intelligence.\nDeen, B., Koldewyn, K., Kanwisher, N., & Saxe, R. (2015, November). Functional Organization of Social\nPerception and Cognition in the Superior Temporal Sulcus. Cerebral Cortex, 25(11), 4596\u20134609.\nRetrieved 2022-07-05, from https://doi.org/10.1093/cercor/bhv111 doi: 10.1093/cercor/bhv111\nDeng, F., Zhi, Z., Lee, D., & Ahn, S. (2021). Generative scene graph networks. In International conference\non learning representations.\nDeniz, F., Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019, September). The Representation of\nSemantic Information Across Human Cerebral Cortex During Listening Versus Reading Is Invariant to\nStimulus Modality. Journal of Neuroscience, 39(39), 7722\u20137736. Retrieved 2020-03-11, from https://\nwww.jneurosci.org/content/39/39/7722\n(Publisher: Society for Neuroscience Section: Research\nArticles) doi: 10.1523/JNEUROSCI.0675-19.2019\nDennett, D. C. (2017). From bacteria to bach and back: The evolution of minds. WW Norton & Company.\nDe Raedt, L., Kimmig, A., & Toivonen, H. (2007). Problog: A probabilistic prolog and its application\nin link discovery. In Proceedings of the 20th international joint conference on artifical intelligence\n(p. 2468\u20132473). San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805.\nDickson, B. (2020). The gpt-3 economy. TechTalks.\nDing, Y., Zhang, X., Paxton, C., & Zhang, S. (2023). Task and motion planning with large language models\nfor object rearrangement. arXiv preprint arXiv:2303.06247.\nDohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., . . . others (2022). Language model\ncascades. arXiv preprint arXiv:2207.10342.\nDong, L., & Lapata, M. (2018). Coarse-to-fine decoding for neural semantic parsing. arXiv preprint\narXiv:1805.04793.\nDries, A., Kimmig, A., Davis, J., Belle, V., & De Raed, L.\n(2017).\nSolving probability problems in\nnatural language. In Proceedings of the 26th international joint conference on artificial intelligence\n(p. 3981\u20133987). AAAI Press.\nDuboue, P. A., & McKeown, K. (2003). Statistical acquisition of content selection rules for natural language\ngeneration.\nDumais, S. T., et al. (2004). Latent semantic analysis. Annu. Rev. Inf. Sci. Technol., 38(1), 188\u2013230.\nDziri, N., Milton, S., Yu, M., Zaiane, O., & Reddy, S. (2022). On the origin of hallucinations in conversational\nmodels: Is it the datasets or the models? arXiv preprint arXiv:2204.07931.\nEdgington, D. (1992). Validity, uncertainty and vagueness. Analysis, 52(4), 193\u2013204.\nEdgington, D. (1997). Vagueness by degrees.\nEdwards, B. (2023). Ai-powered bing chat spills its secrets via prompt injection attack. Ars Technica.\nElkind, D. (1962). Children\u2019s conceptions of brother and sister: Piaget replication study v. The Journal of\ngenetic psychology, 100(1), 129\u2013136.\nEllis, K., Wong, C., Nye, M., Sable-Meyer, M., Cary, L., Morales, L., . . . Tenenbaum, J. B. (2020). Dreamcoder:\n57\nREFERENCES\nGrowing generalizable, interpretable knowledge with wake-sleep bayesian program learning. arXiv\npreprint arXiv:2006.08381.\nEnglish, G., Nejad, N. G., Sommerfelt, M., Yanik, M. F., & von der Behrens, W. (2023). Bayesian surprise\nshapes neural responses in somatosensory cortical circuits. Cell Reports, 42(2).\nErez, T., Tassa, Y., & Todorov, E. (2015). Simulation tools for model-based robotics: Comparison of bullet,\nhavok, mujoco, ode and physx. In 2015 ieee international conference on robotics and automation (icra)\n(pp. 4397\u20134404).\nFang, H., Balakrishnan, A., Jhamtani, H., Bufe, J., Crawford, J., Krishnamurthy, J., . . . Klein, D. (2022).\nThe whole truth and nothing but the truth: Faithful and controllable dialogue response generation\nwith dataflow transduction and constrained decoding. arXiv preprint arXiv:2209.07800.\nFedorenko, E., Behr, M. K., & Kanwisher, N. (2011, September). Functional specificity for high-level\nlinguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39),\n16428\u201316433. Retrieved 2020-02-27, from https://www.pnas.org/content/108/39/16428 doi: 10.1073/\npnas.1112937108\nFedorenko, E., Blank, I., Siegelman, M., & Mineroff, Z. (2020, February). Lack of selectivity for syntax relative\nto word meanings throughout the language network. bioRxiv, 477851. Retrieved 2020-03-13, from\nhttps://www.biorxiv.org/content/10.1101/477851v2\n(Publisher: Cold Spring Harbor Laboratory\nSection: New Results) doi: 10.1101/477851\nFedorenko, E., Hsieh, P.-J., Nieto-Casta\u00f1\u00f3n, A., Whitfield-Gabrieli, S., & Kanwisher, N. (2010, August).\nNew method for fMRI investigations of language: defining ROIs functionally in individual subjects.\nJournal of Neurophysiology, 104(2), 1177\u20131194. doi: 10.1152/jn.00032.2010\nFedorenko, E., & Varley, R. A. (2016, April). Language and thought are not the same thing: evidence from\nneuroimaging and neurological patients: Language versus thought. Annals of the New York Academy of\nSciences, 1369(1), 132\u2013153. Retrieved 2019-07-27, from http://doi.wiley.com/10.1111/nyas.13046\ndoi: 10.1111/nyas.13046\nField, H. H. (1977). Logic, meaning, and conceptual role. The Journal of Philosophy, 74(7), 379\u2013409.\nFikes, R. E., & Nilsson, N. J. (1971). Strips: A new approach to the application of theorem proving to\nproblem solving. Artificial intelligence, 2(3-4), 189\u2013208.\nFirth, J. (1957). A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis, 10\u201332.\nFodor, J. A. (1975). The language of thought. Cambridge, MA: Harvard University Press.\nFodor, J. A. (1983). The modularity of mind. MIT press.\nFodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A critical analysis.\nCognition, 28(1-2), 3\u201371.\nFox, D. (2007). Free choice and the theory of scalar implicatures. Presupposition and implicature in\ncompositional semantics, 71\u2013120.\nFrank, M. C., Goodman, N. D., & Tenenbaum, J. B. (2009). Using speakers\u2019 referential intentions to model\nearly cross-situational word learning. Psychological science, 20(5), 578\u2013585.\nFrege, G. (1892). \u00dcber sinn und bedeutung. Wittgenstein Studien, 1(1).\nFried, D., Aghajanyan, A., Lin, J., Wang, S. I., Wallace, E., Shi, F., . . . Lewis, M. (2022). Incoder: A\ngenerative model for code infilling and synthesis. ArXiv, abs/2204.05999.\nGauthier, J., Levy, R., & Tenenbaum, J. B. (2018). Word learning and the acquisition of syntactic\u2013semantic\noverhypotheses. arXiv preprint arXiv:1805.04988.\nGehr, T., Misailovic, S., & Vechev, M. (2016). Psi: Exact symbolic inference for probabilistic programs. In\nComputer aided verification: 28th international conference, cav 2016, toronto, on, canada, july 17-23,\n2016, proceedings, part i 28 (pp. 62\u201383).\nGehr, T., Steffen, S., & Vechev, M. (2020). \u03bbpsi: Exact inference for higher-order probabilistic programs. In\nProceedings of the 41st acm sigplan conference on programming language design and implementation\n(pp. 883\u2013897).\nGentner, D., & Goldin-Meadow, S. (2003). Whither whorf. Language in mind: Advances in the study of\nlanguage and thought, 3\u201314.\nGentner, D., & Stevens, A. L. (2014). Mental models. Psychology Press.\nGershman, S., & Goodman, N. (2014). Amortized inference in probabilistic reasoning. In Proceedings of the\nannual meeting of the cognitive science society (Vol. 36).\nGershman, S. J., Horvitz, E. J., & Tenenbaum, J. B. (2015). Computational rationality: A converging\n58\nREFERENCES\nparadigm for intelligence in brains, minds, and machines. Science, 349(6245), 273\u2013278.\nGerstenberg, T., & Goodman, N. (2012). Ping pong in church: Productive use of concepts in human\nprobabilistic inference. In Proceedings of the annual meeting of the cognitive science society (Vol. 34).\nGibson, E. (2014). Language for communication: Language as rational inference. In Proceedings of coling\n2014, the 25th international conference on computational linguistics: Technical papers (pp. 781\u2013782).\nGibson, E., Futrell, R., Piantadosi, S. T., Dautriche, I., Mahowald, K., Bergen, L., & Levy, R. (2019). How\nefficiency shapes human language. Trends in cognitive sciences, 23(5), 389\u2013407.\nGinsberg, M. L. (1987). Readings in nonmonotonic reasoning.\nGleitman, L. (1990). The structural sources of verb meanings. Language acquisition, 1(1), 3\u201355.\nGleitman, L. R., Cassidy, K., Nappa, R., Papafragou, A., & Trueswell, J. C. (2005). Hard words. Language\nlearning and development, 1(1), 23\u201364.\nGoldin-Meadow, S. (2012). 26. homesign: gesture to language. In Sign language (pp. 601\u2013625). De Gruyter\nMouton.\nGoldstein, A., Zada, Z., Buchnik, E., Schain, M., Price, A., Aubrey, B., . . . Hasson, U. (2022, March).\nShared computational principles for language processing in humans and deep language models. Nature\nNeuroscience, 25(3), 369\u2013380. Retrieved 2022-10-31, from https://www.nature.com/articles/s41593\n-022-01026-4 (Number: 3 Publisher: Nature Publishing Group) doi: 10.1038/s41593-022-01026-4\nGoldwater, S., Griffiths, T. L., & Johnson, M. (2009). A bayesian framework for word segmentation: Exploring\nthe effects of context. Cognition, 112(1), 21\u201354.\nGolovneva, O., Chen, M., Poff, S., Corredor, M., Zettlemoyer, L., Fazel-Zarandi, M., & Celikyilmaz, A. (2022).\nRoscoe: A suite of metrics for scoring step-by-step reasoning. arXiv preprint arXiv:2212.07919.\nGoodman, N. D., & Frank, M. C. (2016). Pragmatic language interpretation as probabilistic inference. Trends\nin cognitive sciences, 20(11), 818\u2013829.\nGoodman, N. D., & Lassiter, D. (2015). Probabilistic semantics and pragmatics: Uncertainty in language\nand thought. The handbook of contemporary semantic theory, 2nd edition. Wiley-Blackwell.\nGoodman, N. D., Mansinghka, V. K., Roy, D. M., Bonawitz, K. A., & Tenenbaum, J. B. (2008). Church: a\nlanguage for generative models. In D. A. McAllester & P. Myllym\u00e4ki (Eds.), UAI 2008, proceedings\nof the 24th conference in uncertainty in artificial intelligence, helsinki, finland, july 9-12, 2008 (pp.\n220\u2013229). AUAI Press. Retrieved from https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=\n1&smnu=2&article_id=1346&proceeding_id=24\nGoodman, N. D., Tenenbaum, J. B., & Gerstenberg, T. (2014). Concepts in a probabilistic language of\nthought (Tech. Rep.). Center for Brains, Minds and Machines (CBMM).\nGopnik, A. (1996). The scientist as child. Philosophy of science, 63(4), 485\u2013514.\nGothoskar, N., Cusumano-Towner, M., Zinberg, B., Ghavamizadeh, M., Pollok, F., Garrett, A., . . . Mans-\ninghka, V. (2021). 3dp3: 3d scene perception via probabilistic programming. Advances in Neural\nInformation Processing Systems, 34, 9600\u20139612.\nGraff, D. (2000). Shifting sands: An interest-relative theory of vagueness. Philosophical topics, 28(1), 45\u201381.\nGrand, G., Blank, I. A., Pereira, F., & Fedorenko, E. (2022). Semantic projection recovers rich human\nknowledge of multiple object features from word embeddings. Nature Human Behaviour, 1\u201313.\nGreenberg, M., & Harman, G. (2005). Conceptual role semantics.\nGriffiths, T. L., Chater, N., Kemp, C., Perfors, A., & Tenenbaum, J. B. (2010). Probabilistic models of\ncognition: Exploring representations and inductive biases. Trends in cognitive sciences, 14(8), 357\u2013364.\nGriffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007). Topics in semantic representation. Psychological\nreview, 114 2, 211\u201344.\nGriffiths, T. L., & Tenenbaum, J. B. (2006). Optimal predictions in everyday cognition. Psychological science,\n17(9), 767\u2013773.\nGrimshaw, J. (1981). Form, function, and the language acquisition device. The logical problem of language\nacquisition, 165, 178.\nHarman, G. (1982). Conceptual role semantics. Notre Dame Journal of Formal Logic, 23(2), 242\u2013256.\nHarris, Z. S. (1954). Distributional structure. Word. Retrieved from http://psycnet.apa.org/psycinfo/\n1956-02807-001\nHartshorne, J. K., O\u2019Donnell, T. J., Sudo, Y., Uruwashi, M., Lee, M., & Snedeker, J. (2016). Psych verbs,\nthe linking problem, and the acquisition of language. Cognition, 157, 268\u2013288.\nHase, P., Diab, M., Celikyilmaz, A., Li, X., Kozareva, Z., Stoyanov, V., . . . Iyer, S. (2021). Do language\n59\nREFERENCES\nmodels have beliefs? methods for detecting, updating, and visualizing model beliefs. arXiv preprint\narXiv:2111.13654.\nHeim, I., & Kratzer, A. (1998). Semantics in generative grammar (Vol. 1185). Blackwell Oxford.\nHespos, S. J., & Baillargeon, R. (2008). Young infants\u2019 actions reveal their developing knowledge of support\nvariables: Converging evidence for violation-of-expectation findings. Cognition, 107(1), 304\u2013316.\nHinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995). The\" wake-sleep\" algorithm for unsupervised\nneural networks. Science, 268(5214), 1158\u20131161.\nHo, M. K., Saxe, R., & Cushman, F. (2022). Planning with theory of mind. Trends in Cognitive Sciences.\nHoffman, M. D., Blei, D. M., Wang, C., & Paisley, J. (2013). Stochastic variational inference. Journal of\nMachine Learning Research.\nHoltzen, S., Van den Broeck, G., & Millstein, T. (2020). Scaling exact inference for discrete probabilistic\nprograms. Proceedings of the ACM on Programming Languages, 4(OOPSLA), 1\u201331.\nHu, J., Small, H., Kean, H., Takahashi, A., Zekelman, L., Kleinman, D., . . . Fedorenko, E. (2021, September).\nThe language network supports both lexical access and sentence generation during language production\n(Tech. Rep.).\nRetrieved 2021-09-13, from https://www.biorxiv.org/content/10.1101/2021.09.10\n.459596v1 (Company: Cold Spring Harbor Laboratory Distributor: Cold Spring Harbor Laboratory\nLabel: Cold Spring Harbor Laboratory Section: New Results Type: article) doi: 10.1101/2021.09.10\n.459596\nHughes, N., Chang, Y., & Carlone, L. (2022). Hydra: A real-time spatial perception engine for 3d scene\ngraph construction and optimization. arXiv preprint arXiv:2201.13360.\nIcard, T., & Goodman, N. D. (2015). A resource-rational approach to the causal frame problem. In Cogsci.\nIsomura, T., Parr, T., & Friston, K. (2019). Bayesian filtering with multiple internal models: toward a theory\nof social intelligence. Neural computation, 31(12), 2390\u20132431.\nIvanova, A. A. (2022). The role of language in broader human cognition: evidence from neuroscience\n(Unpublished doctoral dissertation). Massachusetts Institute of Technology.\nIvanova, A. A., Mineroff, Z., Zimmerer, V., Kanwisher, N., Varley, R., & Fedorenko, E. (2021). The language\nnetwork is recruited but not required for nonverbal event semantics. Neurobiology of Language, 2(2),\n176\u2013201.\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., . . . Grave, E. (2022). Few-shot\nLearning with Retrieval Augmented Language Models. undefined. doi: 10.48550/arXiv.2208.03299\nJackendoff, R. S. (1985). Semantics and cognition (Vol. 8). MIT press.\nJara-Ettinger, J., Schulz, L. E., & Tenenbaum, J. B. (2020). The naive utility calculus as a unified, quantitative\nframework for action understanding. Cognitive Psychology, 123, 101334.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., . . . Fung, P. (2022). Survey of hallucination in natural\nlanguage generation. ACM Computing Surveys.\nJia, R., & Liang, P. (2016). Data recombination for neural semantic parsing. arXiv preprint arXiv:1606.03622.\nJohnson, J., Hariharan, B., Van Der Maaten, L., Hoffman, J., Fei-Fei, L., Lawrence Zitnick, C., & Girshick, R.\n(2017). Inferring and executing programs for visual reasoning. In Proceedings of the ieee international\nconference on computer vision (pp. 2989\u20132998).\nJohnson, J., Krishna, R., Stark, M., Li, L.-J., Shamma, D., Bernstein, M., & Fei-Fei, L. (2015). Image\nretrieval using scene graphs. In Proceedings of the ieee conference on computer vision and pattern\nrecognition (pp. 3668\u20133678).\nJohnson, R. E., Linderman, S., Panier, T., Wee, C. L., Song, E., Herrera, K. J., . . . Engert, F. (2020).\nProbabilistic models of larval zebrafish behavior reveal structure on many scales. Current Biology,\n30(1), 70\u201382.\nJohnson-Laird, P. N. (1980). Mental models in cognitive science. Cognitive science, 4(1), 71\u2013115.\nJohnson-Laird, P. N. (1989). Mental models.\nJones, D. (2010, October). Human kinship, from conceptual structure to grammar. Behavioral and Brain\nSciences, 33(5), 367\u2013381. Retrieved 2022-08-09, from https://www.cambridge.org/core/product/\nidentifier/S0140525X10000890/type/journal_article doi: 10.1017/S0140525X10000890\nKaelbling, L. P., & Lozano-P\u00e9rez, T. (2011). Hierarchical task and motion planning in the now. In 2011 ieee\ninternational conference on robotics and automation (pp. 1470\u20131477).\nKaelbling, L. P., & Lozano-P\u00e9rez, T. (2013). Integrated task and motion planning in belief space. The\nInternational Journal of Robotics Research, 32(9-10), 1194\u20131227.\n60\nREFERENCES\nKandpal, N., Deng, H., Roberts, A., Wallace, E., & Raffel, C. (2022). Large language models struggle to\nlearn long-tail knowledge. arXiv preprint arXiv:2211.08411.\nKarpas, E., Abend, O., Belinkov, Y., Lenz, B., Lieber, O., Ratner, N., . . . Tenenholtz, M. (2022, May).\nMRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external\nknowledge sources and discrete reasoning (No. arXiv:2205.00445). arXiv.\nKatz, Y., Goodman, N. D., Kersting, K., Kemp, C., & Tenenbaum, J. B. (2008). Modeling Semantic\nCognition as Logical Dimensionality Reduction. Proceedings of the Annual Meeting of the Cognitive\nScience Society, 30(30), 6.\nKemp, C., & Regier, T. (2012, May). Kinship Categories Across Languages Reflect General Communicative\nPrinciples. Science, 336(6084), 1049\u20131054. Retrieved 2022-08-09, from https://doi.org/10.1126/\nscience.1218811\n(Publisher: American Association for the Advancement of Science) doi: 10.1126/\nscience.1218811\nKersten, D., Mamassian, P., & Yuille, A. (2004). Object perception as bayesian inference. Annu. Rev.\nPsychol., 55, 271\u2013304.\nKersten, D. K. D., & Yuille, A. (1996). Introduction: A bayesian formulation of visual perception. Perception\nas Bayesian inference, 1\u201321.\nKhalvati, K., Kiani, R., & Rao, R. P. (2021). Bayesian inference with incomplete knowledge explains\nperceptual confidence and its deviations from accuracy. Nature communications, 12(1), 5704.\nKlein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings of the 41st annual\nmeeting of the association for computational linguistics (pp. 423\u2013430).\nKlessinger, N., Szczerbinski, M., & Varley, R. A. (2007, January). Algebra in a man with severe aphasia.\nNeuropsychologia, 45(8), 1642\u20131648. Retrieved 2022-06-15, from https://www.sciencedirect.com/\nscience/article/pii/S0028393207000280 doi: 10.1016/j.neuropsychologia.2007.01.005\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models are zero-shot\nreasoners. arXiv preprint arXiv:2205.11916.\nKrafft, P., Baker, C., Pentland, A., & Tenenbaum, J. (2016). Modeling human ad hoc coordination. In\nProceedings of the aaai conference on artificial intelligence (Vol. 30).\nKulkarni, T. D., Kohli, P., Tenenbaum, J. B., & Mansinghka, V. (2015). Picture: A probabilistic programming\nlanguage for scene perception. In Proceedings of the ieee conference on computer vision and pattern\nrecognition (pp. 4390\u20134399).\nKwiatkowksi, T., Zettlemoyer, L., Goldwater, S., & Steedman, M. (2010). Inducing probabilistic ccg grammars\nfrom logical form with higher-order unification. In Proceedings of the 2010 conference on empirical\nmethods in natural language processing (pp. 1223\u20131233).\nKwiatkowski, T., Zettlemoyer, L., Goldwater, S., & Steedman, M. (2011). Lexical generalization in ccg\ngrammar induction for semantic parsing. In Proceedings of the 2011 conference on empirical methods in\nnatural language processing (pp. 1512\u20131523).\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman, S. J. (2017). Building machines that learn and\nthink like people. Behavioral and brain sciences, 40.\nLakoff, G. (1988). Cognitive semantics.\nLandauer, T. K., & Dumais, S. T. (1997). A solution to plato\u2019s problem: The latent semantic analysis theory\nof acquisition, induction, and representation of knowledge. Psychological review, 104(2), 211.\nLangkilde, I., & Knight, K. (1998). Generation that exploits corpus-based statistical knowledge. In Coling\n1998 volume 1: The 17th international conference on computational linguistics.\nLassiter, D., & Goodman, N. D. (2017). Adjectival vagueness in a bayesian model of interpretation. Synthese,\n194(10), 3801\u20133836.\nLe, T. A., Baydin, A. G., & Wood, F. (2017). Inference compilation and universal probabilistic programming.\nIn Artificial intelligence and statistics (pp. 1338\u20131348).\nLecours, A. R., & Joanette, Y. (1980, May). Linguistic and other psychological aspects of paroxysmal aphasia.\nBrain and Language, 10(1), 1\u201323. doi: 10.1016/0093-934x(80)90034-6\nLee, T. S., & Mumford, D. (2003). Hierarchical bayesian inference in the visual cortex. JOSA A, 20(7),\n1434\u20131448.\nLerner, Y., Honey, C. J., Silbert, L. J., & Hasson, U. (2011, February). Topographic Mapping of a Hierarchy\nof Temporal Receptive Windows Using a Narrated Story. The Journal of Neuroscience, 31(8), 2906\u2013\n2915. Retrieved 2019-12-28, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3089381/\ndoi:\n61\nREFERENCES\n10.1523/JNEUROSCI.3684-10.2011\nLevin, B. (1993). English verb classes and alternations: A preliminary investigation. University of Chicago\npress.\nLew, A., Agrawal, M., Sontag, D., & Mansinghka, V. (2021). Pclean: Bayesian data cleaning at scale with\ndomain-specific probabilistic programming. In International conference on artificial intelligence and\nstatistics (pp. 1927\u20131935).\nLew, A. K., Matheos, G., Zhi-Xuan, T., Ghavamizadeh, M., Gothoskar, N., Russell, S., & Mansinghka,\nV. K. (2023). Smcp3: Sequential monte carlo with probabilistic program proposals. In International\nconference on artificial intelligence and statistics (pp. 7061\u20137088).\nLew, A. K., Tessler, M. H., Mansinghka, V. K., & Tenenbaum, J. B. (2020). Leveraging unstructured\nstatistical knowledge in a probabilistic language of thought. In Proceedings of the annual conference of\nthe cognitive science society.\nLew, A. K., Zhi-Xuan, T., Grand, G., & Mansinghka, V. K. (2023). Sequential monte carlo steering of large\nlanguage models using probabilistic programs. arXiv preprint arXiv:2306.03081.\nLewis, D. (1976). General semantics. In Montague grammar (pp. 1\u201350). Elsevier.\nLi, B. Z., Nye, M., & Andreas, J. (2021). Implicit representations of meaning in neural language models.\narXiv preprint arXiv:2106.00737.\nLi, Y., Wang, S., & Nguyen, T. N. (2020). Dlfix: Context-based code transformation learning for automated\nprogram repair. 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),\n602\u2013614.\nLiang, P. (2016). Learning executable semantic parsers for natural language understanding. Communications\nof the ACM , 59(9), 68\u201376.\nLiang, P., Daum\u00e9 III, H., & Klein, D. (2008). Structure compilation: trading structure for features. In\nProceedings of the 25th international conference on machine learning (pp. 592\u2013599).\nLieder, F., & Griffiths, T. L. (2019). Resource-rational analysis: Understanding human cognition as the\noptimal use of limited computational resources. Behavioral and Brain Sciences, 43.\nLieder, F., & Griffiths, T. L. (2020). Resource-rational analysis: Understanding human cognition as the\noptimal use of limited computational resources. Behavioral and brain sciences, 43, e1.\nLieder, F., Hsu, M., & Griffiths, T. L. (2014). The high availability of extreme events serves resource-rational\ndecision-making. In Proceedings of the annual meeting of the cognitive science society (Vol. 36).\nLinzen, T. (2020). How can we accelerate progress towards human-like linguistic generalization? arXiv\npreprint arXiv:2005.00955.\nLipkin, B., Wong, L., Grand, G., & Tenenbaum, J. B. (2023). Evaluating statistical language models as\npragmatic reasoners. arXiv preprint arXiv:2305.01020.\nLiu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., & Stone, P. (2023). Llm+ p: Empowering large\nlanguage models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.\nLiu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., & Zhang, Y. (2023). Evaluating the logical reasoning ability of\nchatgpt and gpt-4. arXiv preprint arXiv:2304.03439.\nLiu, R., Wei, J., Gu, S. S., Wu, T.-Y., Vosoughi, S., Cui, C., . . . Dai, A. M. (2022). Mind\u2019s eye: Grounded\nlanguage model reasoning through simulation. arXiv preprint arXiv:2210.05359.\nLowie, R. H. (1930). The kinship terminology of the bannock indians. American Anthropologist, 32(2),\n294\u2013299.\nLuria, A. R., Tsvetkova, L. S., & Futer, D. S. (1965, June). Aphasia in a composer (V. G. Shebalin). Journal\nof the Neurological Sciences, 2(3), 288\u2013292. doi: 10.1016/0022-510x(65)90113-9\nLyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., . . . Callison-Burch, C. (2023). Faithful\nchain-of-thought reasoning. arXiv preprint arXiv:2301.13379.\nMacSweeney, M., Woll, B., Campbell, R., McGuire, P. K., David, A. S., Williams, S. C. R., . . . Brammer, M. J.\n(2002, July). Neural systems underlying British Sign Language and audio-visual English processing in\nnative users. Brain, 125(7), 1583\u20131593. Retrieved 2021-01-05, from https://doi.org/10.1093/brain/\nawf153 doi: 10.1093/brain/awf153\nMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., & Fedorenko, E. (2023).\nDissociating language and thought in large language models: a cognitive perspective. arXiv preprint\narXiv:2301.06627.\nMansinghka, V., Selsam, D., & Perov, Y. (2014). Venture: a higher-order probabilistic programming platform\n62\nREFERENCES\nwith programmable inference. arXiv preprint arXiv:1404.0099.\nMansinghka, V. K., Kulkarni, T. D., Perov, Y. N., & Tenenbaum, J. (2013). Approximate bayesian image\ninterpretation using generative probabilistic graphics programs.\nAdvances in Neural Information\nProcessing Systems, 26.\nMansinghka, V. K., Schaechtle, U., Handa, S., Radul, A., Chen, Y., & Rinard, M. (2018). Probabilistic\nprogramming with programmable inference. In Proceedings of the 39th acm sigplan conference on\nprogramming language design and implementation (pp. 603\u2013616).\nMarcus, G., Davis, E., & Aaronson, S. (2022). A very preliminary analysis of dall-e 2. arXiv preprint\narXiv:2204.13807.\nMaynez, J., Narayan, S., Bohnet, B., & McDonald, R. (2020). On faithfulness and factuality in abstractive\nsummarization. arXiv preprint arXiv:2005.00661.\nMcCarthy, J. (1980). Circumscription\u2013a form of non-monotonic reasoning. Artificial intelligence, 13(1-2),\n27\u201339.\nMcDermott, D. (1982). A temporal logic for reasoning about processes and plans. Cognitive science, 6(2),\n101\u2013155.\nMcDermott, D. M. (2000). The 1998 ai planning systems competition. AI magazine, 21(2), 35\u201335.\nMenenti, L., Gierhan, S. M. E., Segaert, K., & Hagoort, P. (2011, September). Shared language: overlap\nand segregation of the neuronal infrastructure for speaking and listening revealed by functional MRI.\nPsychological Science, 22(9), 1173\u20131182. doi: 10.1177/0956797611418347\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words\nand phrases and their compositionality. In Advances in neural information processing systems (pp.\n3111\u20133119).\nMilch, B., Marthi, B., Russell, S., Sontag, D., Ong, D. L., & Kolobov, A. (2007). BLOG: Probabilistic models\nwith unknown objects. Statistical relational learning, 373.\nMitchell, A., & Jordan, F. M. (2021, June). The Ontogeny of Kinship Categorization. Journal of Cognition\nand Culture, 21(1-2), 152\u2013177. Retrieved 2022-08-09, from https://brill.com/view/journals/jocc/\n21/1-2/article-p152_8.xml (Publisher: Brill) doi: 10.1163/15685373-12340101\nMollica, F., Bacon, G., Zaslavsky, N., Xu, Y., Regier, T., & Kemp, C. (2021). The forms and meanings\nof grammatical markers support efficient communication. Proceedings of the National Academy of\nSciences, 118(49), e2025993118.\nMollica, F., & Piantadosi, S. T. (2022, June). Logical word learning: The case of kinship. Psychonomic Bulletin\n& Review, 29(3), 766\u2013799. Retrieved 2022-08-09, from https://doi.org/10.3758/s13423-021-02017-5\ndoi: 10.3758/s13423-021-02017-5\nMontague, R. (1970). English as a formal language.\nMonti, M. M., Osherson, D. N., Martinez, M. J., & Parsons, L. M.\n(2007, September).\nFunctional\nneuroanatomy of deductive inference: A language-independent distributed network. NeuroImage,\n37(3), 1005\u20131016. Retrieved 2020-04-16, from http://www.sciencedirect.com/science/article/pii/\nS1053811907003436 doi: 10.1016/j.neuroimage.2007.04.069\nMonti, M. M., Parsons, L. M., & Osherson, D. N. (2012, August). Thought beyond language: neural\ndissociation of algebra and natural language. Psychological Science, 23(8), 914\u2013922. doi: 10.1177/\n0956797612437427\nMorgan, M. S. (1999). Learning from models. Ideas in Context, 52, 347\u2013388.\nMu, J., & Andreas, J. (2020). Compositional explanations of neurons. Advances in Neural Information\nProcessing Systems, 33, 17153\u201317163.\nNersessian, N. J., et al. (2010). Mental modeling in conceptual change. International Journal on Humanistic\nIdeology, 3(01), 11\u201348.\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., . . . others\n(2021).\nShow your work: Scratchpads for intermediate computation with language models. arXiv preprint\narXiv:2112.00114.\nOaksford, M., & Chater, N. (2007). Bayesian rationality: The probabilistic approach to human reasoning.\nOxford University Press.\nOpenAI. (2023a). Chatgpt: Optimizing language models for dialogue. OpenAI Blog.\nOpenAI. (2023b). Chatgpt plugins. OpenAI Blog.\nOpenAI. (2023c). Gpt-4 technical report.\n63\nREFERENCES\nOsgood, C. E. (1952). The nature and measurement of meaning. Psychological bulletin, 49(3), 197.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., . . . Lowe, R. (2022). Training\nlanguage models to follow instructions with human feedback. arXiv. Retrieved from https://arxiv.org/\nabs/2203.02155 doi: 10.48550/ARXIV.2203.02155\nPan, L., Albalak, A., Wang, X., & Wang, W. Y. (2023). Logic-lm: Empowering large language models with\nsymbolic solvers for faithful logical reasoning. arXiv preprint arXiv:2305.12295.\nPanthaplackel, S., Nie, P., Gligoric, M., Li, J. J., & Mooney, R. J. (2020). Learning to update natural\nlanguage comments based on code changes. arXiv preprint arXiv:2004.12169.\nParsons, T. (1990). Events in the semantics of english: A study in subatomic semantics.\nPaunov, A. M., Blank, I. A., & Fedorenko, E. (2019, April). Functionally distinct language and Theory of\nMind networks are synchronized at rest and during language comprehension. Journal of Neurophysiology,\n121(4), 1244\u20131265. Retrieved 2019-07-10, from https://www.physiology.org/doi/10.1152/jn.00619\n.2018 doi: 10.1152/jn.00619.2018\nPaunov, A. M., Blank, I. A., Jouravlev, O., Mineroff, Z., Gall\u00e9e, J., & Fedorenko, E.\n(2022, June).\nDifferential Tracking of Linguistic vs. Mental State Content in Naturalistic Stimuli by Language and\nTheory of Mind (ToM) Brain Networks. Neurobiology of Language, 1\u201329. Retrieved 2022-07-05, from\nhttps://doi.org/10.1162/nol_a_00071 doi: 10.1162/nol_a_00071\nPearl, J. (1988). Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan\nkaufmann.\nPearl, J., et al. (2000). Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress, 19(2).\nPednault, E. P. (1989). Adl: exploring the middle ground between strips and the situation calculus. Kr, 89,\n324\u2013332.\nPereira, F. C., & Shieber, S. M. (2002). Prolog and natural-language analysis. Microtome Publishing.\nPerfors, A., Tenenbaum, J. B., & Regier, T. (2011). The learnability of abstract syntactic principles.\nCognition, 118(3), 306\u2013338.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (1802). Deep\ncontextualized word representations. corr abs/1802.05365 (2018). arXiv preprint arXiv:1802.05365.\nPetrov, S., Haghighi, A., & Klein, D. (2008). Coarse-to-fine syntactic machine translation using language\nprojections. In Proceedings of the 2008 conference on empirical methods in natural language processing\n(pp. 108\u2013116).\nPhilippe, R. (1972). D\u00e9finition et traitement de l\u2019\u00e9galit\u00e9 formelle en d\u00e9monstration automatique (Unpublished\ndoctoral dissertation). th\u00e8se de 3i\u00e8me cycle, Groupe Intelligence Artificielle, Facult\u00e9 des Sciences . . . .\nPiaget, J. (1951). Judgement and reasoning in the child. London: Routledge and Kegan Paul.\nPiantadosi, S. T. (2023). Modern language models refute chomsky\u2019s approach to language. Lingbuzz Preprint,\nlingbuzz, 7180.\nPiantadosi, S. T., Tenenbaum, J. B., & Goodman, N. D. (2012). Bootstrapping in a language of thought: A\nformal model of numerical concept learning. Cognition, 123(2), 199\u2013217.\nPietroski, P. M. (2018). Conjoining meanings: Semantics without truth values. Oxford University Press.\nPinker, S. (1984). Language learnability and language development.\nPinker, S. (1998). Words and rules. Lingua, 106(1-4), 219\u2013242.\nPinker, S., & MacWhinney, B. (1987). The bootstrapping problem in language acquisition. Mechanisms of\nlanguage acquisition, 399\u2013441.\nPollard, C., & Sag, I. A. (1994). Head-driven phrase structure grammar. University of Chicago Press.\nPosner, G. J., Strike, K. A., Hewson, P. W., & Gertzog, W. (1982). Toward a theory of conceptual change.\nScience education, 66(2), 211\u2013227.\nPramod, R., Cohen, M. A., Tenenbaum, J. B., & Kanwisher, N. (2022). Invariant representation of physical\nstability in the human brain. Elife, 11, e71736.\nPyers, J. E., Shusterman, A., Senghas, A., Spelke, E. S., & Emmorey, K. (2010). Evidence from an emerging\nsign language reveals that language supports spatial cognition. Proceedings of the National Academy of\nSciences, 107(27), 12116\u201312120.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are\nunsupervised multitask learners. OpenAI Blog, 1(8).\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., . . . others (2021). Scaling language\nmodels: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\n64\nREFERENCES\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., . . . Sutskever, I. (2021). Zero-shot\ntext-to-image generation. In International conference on machine learning (pp. 8821\u20138831).\nRanganath, R., Gerrish, S., & Blei, D. (2014). Black box variational inference. In Artificial intelligence and\nstatistics (pp. 814\u2013822).\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., . . . others (2022). A\ngeneralist agent. arXiv preprint arXiv:2205.06175.\nRegev, M., Honey, C. J., Simony, E., & Hasson, U. (2013, October). Selective and Invariant Neural Responses\nto Spoken and Written Narratives. Journal of Neuroscience, 33(40), 15978\u201315988. Retrieved 2020-10-02,\nfrom https://www.jneurosci.org/content/33/40/15978 (Publisher: Society for Neuroscience Section:\nArticles) doi: 10.1523/JNEUROSCI.1580-13.2013\nReid, M., & Neubig, G. (2022). Learning to model editing processes. ArXiv, abs/2205.12374.\nRibeiro, D., Wang, S., Ma, X., Zhu, H., Dong, R., Kong, D., . . . others (2023). Street: A multi-task structured\nreasoning and explanation benchmark. arXiv preprint arXiv:2302.06729.\nRips, L. J., & Hespos, S. J. (2015). Divisions of the physical world: Concepts of objects and substances.\nPsychological bulletin, 141(4), 786.\nRoose, K. (2023). Bing\u2019s a.i. chat: I want to be alive. The New York Times.\nRudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use\ninterpretable models instead. Nature machine intelligence, 1(5), 206\u2013215.\nRudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., & Zhong, C. (2022). Interpretable machine learning:\nFundamental principles and 10 grand challenges. Statistic Surveys, 16, 1\u201385.\nRussell, S., & Norvig, P. (2021). Artificial intelligence : a modern approach (Fourth edition. ed.). Hoboken,\nNJ: Pearson.\nSaad, F. A., Cusumano-Towner, M. F., Schaechtle, U., Rinard, M. C., & Mansinghka, V. K.\n(2019).\nBayesian synthesis of probabilistic programs for automatic data modeling. Proceedings of the ACM on\nProgramming Languages, 3(POPL), 1\u201332.\nSaad, F. A., Rinard, M. C., & Mansinghka, V. K. (2021). Sppl: probabilistic programming with fast exact\nsymbolic inference. In Proceedings of the 42nd acm sigplan international conference on programming\nlanguage design and implementation (pp. 804\u2013819).\nSaffran, J. R., Senghas, A., & Trueswell, J. C. (2001). The acquisition of language by children. Proceedings\nof the National Academy of Sciences, 98(23), 12874\u201312875.\nSahlgren, M. (2008). The distributional hypothesis. Italian Journal of Disability Studies, 20, 33\u201353.\nSanborn, A. N., & Chater, N. (2017). The sampling brain. Trends in Cognitive Sciences, 21(7), 492\u2013493.\nSapir, E. (1929). The status of linguistics as a science. Language, 207\u2013214.\nSaxe, R., Moran, J. M., Scholz, J., & Gabrieli, J. (2006). Overlapping and non-overlapping brain regions for\ntheory of mind and self reflection in individual subjects. Social cognitive and affective neuroscience,\n1(3), 229\u2013234.\nSaxe, R., & Powell, L. J. (2006). It\u2019s the thought that counts: specific brain regions for one component of\ntheory of mind. Psychological science, 17(8), 692\u2013699.\nSchick, T., Dwivedi-Yu, J., Dess\u00ec, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., . . . Scialom, T. (2023).\nToolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.\nSchrimpf, M., Blank, I. A., Tuckute, G., Kauf, C., Hosseini, E. A., Kanwisher, N., . . . Fedorenko, E. (2021,\nNovember). The neural architecture of language: Integrative modeling converges on predictive processing.\nProceedings of the National Academy of Sciences, 118(45). Retrieved 2021-12-12, from https://www\n.pnas.org/content/118/45/e2105646118 (Publisher: National Academy of Sciences Section: Biological\nSciences) doi: 10.1073/pnas.2105646118\nSchuler, K. K. (2005). Verbnet: A broad-coverage, comprehensive verb lexicon [PhD Thesis]. Univer-\nsity of Pennsylvania. Retrieved from http://verbs.colorado.edu/~kipper/Papers/dissertation.pdf\n(ISBN: 0-542-20049-X)\nSchwettmann, S., Fischer, J., Tenenbaum, J., & Kanwisher, N. (2018). Evidence for an intuitive physics\nengine in the human brain. In Cogsci.\nSchwettmann, S., Tenenbaum, J. B., & Kanwisher, N. (2019). Invariant representations of mass in the human\nbrain. Elife, 8, e46619.\n65\nREFERENCES\nScott, R. M., & Baillargeon, R. (2013). Do infants really expect agents to act efficiently? a critical test of the\nrationality principle. Psychological science, 24(4), 466\u2013474.\nScott, T. L., Gall\u00e9e, J., & Fedorenko, E. (2017). A new fun and robust version of an fMRI localizer\nfor the frontotemporal language system.\nCognitive Neuroscience, 8(3), 167\u2013176.\ndoi: 10.1080/\n17588928.2016.1201466\nSeaman, I. R., van de Meent, J.-W., & Wingate, D. (2018). Nested reasoning about autonomous agents using\nprobabilistic programs. arXiv preprint arXiv:1812.01569.\nSenghas, A., Kita, S., & Ozyurek, A. (2004). Children creating core properties of language: Evidence from\nan emerging sign language in nicaragua. Science, 305(5691), 1779\u20131782.\nShain, C., Blank, I. A., van Schijndel, M., Schuler, W., & Fedorenko, E. (2020). fMRI reveals language-specific\npredictive coding during naturalistic sentence comprehension. Neuropsychologia, 138, 107307. doi:\n10.1016/j.neuropsychologia.2019.107307\nShain, C., Paunov, A. M., Chen, X., Lipkin, B., & Fedorenko, E. (2022, July). No evidence of theory\nof mind reasoning in the human language network. bioRxiv. Retrieved 2022-07-20, from https://\nwww.biorxiv.org/content/10.1101/2022.07.18.500516v1\n(Pages: 2022.07.18.500516 Section: New\nResults) doi: 10.1101/2022.07.18.500516\nShan, C.-c., & Ramsey, N. (2017). Exact bayesian inference by symbolic disintegration. In Proceedings of the\n44th acm sigplan symposium on principles of programming languages (pp. 130\u2013144).\nShin, R., Brockschmidt, M., Allamanis, M., & Polozov, O. (2018). Program synthesis with learned code\nidioms.\nSilbert, L. J., Honey, C. J., Simony, E., Poeppel, D., & Hasson, U. (2014, October). Coupled neural\nsystems underlie the production and comprehension of naturalistic narrative speech. Proceedings\nof the National Academy of Sciences, 111(43), E4687\u2013E4696. Retrieved 2021-09-06, from https://\nwww.pnas.org/content/111/43/E4687 (Publisher: National Academy of Sciences Section: PNAS Plus)\ndoi: 10.1073/pnas.1323812111\nSmith, K., Frank, S., Rolando, S., Kirby, S., & Loy, J. E. (2020). Simple kinship systems are more learnable.\nProceedings of the Annual Meeting of the Cognitive Science Society, 7.\nSmith, L., & Yu, C. (2008). Infants rapidly learn word-referent mappings via cross-situational statistics.\nCognition, 106(3), 1558\u20131568.\nSnedeker, J. (2016). Clean mapping: A sketchy story about how conceptual structure could shape language\nacquisition and some evidence suggesting that it just might be true.\nSorkin, A. R., Warner, B., Kessler, S., Hirsch, L., & Livni, E. (2023). Revenge of the chatbots. The New\nYork Times.\nSpelke, E. S. (1990). Principles of object perception. Cognitive science, 14(1), 29\u201356.\nSpelke, E. S. (2022). What babies know: Core knowledge and composition volume 1 (Vol. 1). Oxford University\nPress.\nSpelke, E. S., Gutheil, G., & Van de Walle, G. (1995). The development of object perception. Visual\ncognition: An invitation to cognitive science, 2, 297\u2013330.\nSpelke, E. S., & Kinzler, K. D. (2007). Core knowledge. Developmental science, 10(1), 89\u201396.\nSteedman, M. (2001). The syntactic process. MIT press.\nSteedman, M. (2011). Combinatory categorial grammar.\nSumers, T. R., Hawkins, R. D., Ho, M. K., Griffiths, T. L., & Hadfield-Menell, D. (2022). How to talk so\nyour robot will learn: Instructions, descriptions, and pragmatics. arXiv preprint arXiv:2206.07870.\nSuster, S., Fivez, P., Totis, P., Kimmig, A., Davis, J., De Raedt, L., & Daelemans, W. (2021). Mapping\nprobability word problems to executable representations. In Proceedings of the 2021 conference on\nempirical methods in natural language processing (pp. 3627\u20133640).\nTalmy, L. (1988). Force dynamics in language and cognition. Cognitive science, 12(1), 49\u2013100.\nTangermann, V. (2023). Microsoft\u2019s bing ai is leaking maniac alternate personalities named venom and fury.\nFuturism.\nT\u00e9gl\u00e1s, E., Vul, E., Girotto, V., Gonzalez, M., Tenenbaum, J. B., & Bonatti, L. L. (2011). Pure Reasoning in\n12-Month-Old Infants as Probabilistic Inference. Science, 27(332), 1054\u20131059.\nTellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., & Roy, N. (2011). Understanding\nnatural language commands for robotic navigation and mobile manipulation. In Proceedings of the aaai\nconference on artificial intelligence (Vol. 25, pp. 1507\u20131514).\n66\nREFERENCES\nTenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman, N. D. (2011). How to grow a mind: Statistics,\nstructure, and abstraction. science, 331(6022), 1279\u20131285.\nTenney, I., Das, D., & Pavlick, E. (2019). Bert rediscovers the classical nlp pipeline. arXiv preprint\narXiv:1905.05950.\nTessler, M. H., Tenenbaum, J. B., & Goodman, N. D. (2022). Logic, probability, and pragmatics in syllogistic\nreasoning. Topics in Cognitive Science, 14(3), 574\u2013601.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., . . . Le, Q. (2022,\nFebruary). LaMDA: Language Models for Dialog Applications (No. arXiv:2201.08239). arXiv.\nTodorov, E., Erez, T., & Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In 2012\nieee/rsj international conference on intelligent robots and systems (pp. 5026\u20135033).\nTolpin, D., van de Meent, J.-W., Yang, H., & Wood, F. (2016). Design and implementation of probabilistic\nprogramming language Anglican. In Proceedings of the 28th symposium on the implementation and\napplication of functional programming languages (pp. 1\u201312).\nTomasello, M. (2009). The usage-based theory of language acquisition. In The cambridge handbook of child\nlanguage (pp. 69\u201387). Cambridge Univ. Press.\nTomasello, M. (2022). The evolution of agency: Behavioral organization from lizards to humans. MIT Press.\nUllman, T. D. (2023). Large language models fail on trivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399.\nUllman, T. D., Spelke, E., Battaglia, P., & Tenenbaum, J. B. (2017). Mind games: Game engines as an\narchitecture for intuitive physics. Trends in cognitive sciences, 21(9), 649\u2013665.\nValmeekam, K., Sreedharan, S., Marquez, M., Olmo, A., & Kambhampati, S. (2023). On the planning\nabilities of large language models (a critical investigation with a proposed benchmark). arXiv preprint\narXiv:2302.06706.\nVarley, R. A. (1998). Aphasic language, aphasic thought: an investigation of propositional thinking in an\na-propositional aphasic. In P. Carruthers & J. Boucher (Eds.), Language and Thought: Interdisciplinary\nThemes (pp. 128\u2013145). Cambridge University Press. doi: 10.1017/CBO9780511597909.009\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017).\nAttention is all you need. Advances in neural information processing systems, 30.\nVul, E., Goodman, N., Griffiths, T. L., & Tenenbaum, J. B. (2014). One and done? optimal decisions from\nvery few samples. Cognitive science, 38(4), 599\u2013637.\nVul, E., & Pashler, H. (2008). Measuring the crowd within: Probabilistic representations within individuals.\nPsychological Science, 19(7), 645\u2013647.\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., . . . Anandkumar, A. (2023). Voyager: An\nopen-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291.\nWang, R. F., & Spelke, E. S. (2002). Human spatial representation: Insights from animals. Trends in\ncognitive sciences, 6(9), 376\u2013382.\nWatters, N., Tenenbaum, J., & Jazayeri, M. (2021). Modular object-oriented games: a task framework for\nreinforcement learning, psychology, and neuroscience. arXiv preprint arXiv:2102.12616.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022). Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\nWeir, N., & Van Durme, B. (2022, September). Dynamic Generation of Interpretable Inference Rules in a\nNeuro-Symbolic Expert System (No. arXiv:2209.07662). arXiv.\nWellman, H. M., & Gelman, S. A. (1992). Cognitive development: Foundational theories of core domains.\nAnnual review of psychology, 43(1), 337\u2013375.\nWhite, J., Mu, J., & Goodman, N. D. (2020). Learning to refer informatively by amortizing pragmatic\nreasoning. arXiv preprint arXiv:2006.00418.\nWhorf, B. (1956). Language, thought, and reality: selected writings.\nWilson, S. M., Molnar-Szakacs, I., & Iacoboni, M. (2008, January). Beyond Superior Temporal Cortex:\nIntersubject Correlations in Narrative Speech Comprehension.\nCerebral Cortex, 18(1), 230\u2013242.\nRetrieved 2022-06-19, from https://doi.org/10.1093/cercor/bhm049 doi: 10.1093/cercor/bhm049\nWingate, D., Stuhlm\u00fcller, A., & Goodman, N.\n(2011).\nLightweight implementations of probabilistic\nprogramming languages via transformational compilation. In Proceedings of the fourteenth international\nconference on artificial intelligence and statistics (pp. 770\u2013778).\nWiseman, S., Shieber, S. M., & Rush, A. M. (2017). Challenges in data-to-document generation. arXiv\n67\nREFERENCES\npreprint arXiv:1707.08052.\nWitty, S., Lew, A., Jensen, D., & Mansinghka, V. (2019). Bayesian causal inference via probabilistic program\nsynthesis. arXiv preprint arXiv:1910.14124.\nWolfram, S.\n(2023).\nChatGPT gets its \u201cWolfram Superpowers\u201d.\nRetrieved from https://writings\n.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/\nWong, C., Ellis, K. M., Tenenbaum, J., & Andreas, J. (2021). Leveraging language to learn program\nabstractions and search heuristics. In International conference on machine learning (pp. 11193\u201311204).\nWong, Y. W., & Mooney, R. (2007). Learning synchronous grammars for semantic parsing with lambda\ncalculus. In Proceedings of the 45th annual meeting of the association of computational linguistics (pp.\n960\u2013967).\nWu, J., Tenenbaum, J. B., & Kohli, P. (2017). Neural scene de-rendering. In Proceedings of the ieee conference\non computer vision and pattern recognition (pp. 699\u2013707).\nWu, J., Yildirim, I., Lim, J. J., Freeman, B., & Tenenbaum, J. (2015a). Galileo: Perceiving physical object\nproperties by integrating a physics engine with deep learning. Advances in neural information processing\nsystems, 28.\nWu, J., Yildirim, I., Lim, J. J., Freeman, B., & Tenenbaum, J. (2015b). Galileo: Perceiving Physical Object\nProperties by Integrating a Physics Engine with Deep Learning. In Advances in Neural Information\nProcessing Systems (Vol. 28). Curran Associates, Inc.\nWu, M., & Goodman, N. (2022). Foundation posteriors for approximate probabilistic inference. arXiv\npreprint arXiv:2205.09735.\nWu, S. A., Wang, R. E., Evans, J. A., Tenenbaum, J. B., Parkes, D. C., & Kleiman-Weiner, M. (2021). Too\nmany cooks: Bayesian inference for coordinating multi-agent collaboration. Topics in Cognitive Science,\n13(2), 414\u2013432.\nXie, Y., Yu, C., Zhu, T., Bai, J., Gong, Z., & Soh, H. (2023). Translating natural language to planning goals\nwith large-language models. arXiv preprint arXiv:2302.05128.\nXu, K., Srivastava, A., Gutfreund, D., Sosa, F., Ullman, T. D., Tenenbaum, J., & Sutton, C. (2021).\nA bayesian-symbolic approach to reasoning and learning in intuitive physics. Advances in Neural\nInformation Processing Systems, 34, 2478\u20132490.\nYang, Y., & Piantadosi, S. T. (2022). One model for the learning of language. Proceedings of the National\nAcademy of Sciences, 119(5), e2021865119.\nYasunaga, M., & Liang, P. (2020). Graph-based, self-supervised program repair from diagnostic feedback.\nCoRR, abs/2005.10636. Retrieved from https://arxiv.org/abs/2005.10636\nYi, K., Gan, C., Li, Y., Kohli, P., Wu, J., Torralba, A., & Tenenbaum, J. B. (2019). Clevrer: Collision events\nfor video representation and reasoning. arXiv preprint arXiv:1910.01442.\nYi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., & Tenenbaum, J. B.\n(2018).\nNeural-symbolic vqa:\nDisentangling reasoning from vision and language understanding. arXiv preprint arXiv:1810.02338, 31,\n1031\u20131042.\nYildirim, I., Belledonne, M., Freiwald, W., & Tenenbaum, J. (n.d.). Efficient inverse graphics in biological\nface processing. , 77.\nYing, L., Collins, K., Wei, M., Zhang, C., Tan, Z.-X., Weller, A., . . . Wong, L. (2023). The neuro-symbolic\ninverse planning engine (nipe): Modeling probabilistic social inferences from linguistic inputs. ICML\nToM Workshop 2023.\nYuille, A., & Kersten, D. (2006). Vision as bayesian inference: analysis by synthesis? Trends in cognitive\nsciences, 10(7), 301\u2013308.\nZaslavsky, N., Kemp, C., Regier, T., & Tishby, N. (2018). Efficient compression in color naming and its\nevolution. Proceedings of the National Academy of Sciences, 115(31), 7937\u20137942.\nZelikman, E., Wu, Y., Mu, J., & Goodman, N. (2022). Star: Bootstrapping reasoning with reasoning.\nAdvances in Neural Information Processing Systems, 35, 15476\u201315488.\nZhang, C. E., Wong, L., Grand, G., & Tenenbaum, J. B. (2023). Grounded physical language understanding\nwith probabilistic programs and simulated worlds. In Proceedings of the annual conference of the\ncognitive science society (p. To Appear).\nZhang, J., Panthaplackel, S., Nie, P., Li, J. J., & Gligori\u0107, M. (2022). Coditt5: Pretraining for source code\nand natural language editing. ArXiv, abs/2208.05446.\nZhi-Xuan, T. (2022). Pddl. jl: An extensible interpreter and compiler interface for fast and flexible ai planning\n68\nREFERENCES\n(Unpublished doctoral dissertation). Massachusetts Institute of Technology.\nZhi-Xuan, T., Mann, J., Silver, T., Tenenbaum, J., & Mansinghka, V.\n(2020).\nOnline bayesian goal\ninference for boundedly rational planning agents. Advances in neural information processing systems,\n33, 19238\u201319250.\nZhuo, T. Y., Huang, Y., Chen, C., & Xing, Z. (2023). Exploring ai ethics of chatgpt: A diagnostic analysis.\narXiv preprint arXiv:2301.12867.\nZinberg, B., Cusumano-Towner, M., & Vikash, K. M. (2019). Structured differentiable models of 3d scenes\nvia generative scene graphs. In Workshop on perception as generative reasoning, neurips, submitted\nseptember.\n69\nA\nLANGUAGE AND WORLD MODELS\nAppendices\nWe include code for reference below to help better interpret the examples in the paper. This code is included\n(with human-readable comments) for completeness and for reference, but is not guaranteed to be the the most\nup-to-date version of these examples. Please refer to the GitHub repository for the most complete, corrected,\nand up-to-date code for all examples in this paper, as well as instructions for execution and reproducibility:\ngithub.com/gabegrand/world-models.\nA\nLanguage and world models\nA.1\nProbabilistic reasoning\nA.1.1\nGenerative world model for tug-of-war\n1 ;; This Church program models a tug-of-war game between teams of players.\n2\n3 ;; Each player has a strength, with strength value 50 being about average.\n4 (define strength (mem (lambda (player) (gaussian 50 20))))\n5\n6 ;; Each player has an intrinsic laziness frequency.\n7 (define laziness (mem (lambda (player) (uniform 0 1))))\n8\n9 ;; The team's strength is the sum of the players' strengths.\n10 ;; When a player is lazy in a match, they pull with half their strength.\n11 (define (team-strength team)\n12 (sum\n13\n(map\n14\n(lambda (player) (if (flip (laziness player)) (/ (strength player) 2) (strength player)))\n15\nteam)))\n16\n17 ;; The winner of the match is the stronger team.\n18 ;; Returns true if team-1 won against team-2, else false.\n19 (define (won-against team-1 team-2)\n20\n(> (team-strength team-1) (team-strength team-2)))\nCode Block 1: Generative domain theory for the Bayesian tug-of-war.\nA.1.2\nTranslation examples for tug-of-war\n1 ;; Now, let's translate some user-defined statements.\n2 ;; Each statement begins with either `Condition` or `Query`.\n3 ;; `Condition` statements provide facts about the scenario.\n4 ;; `Query` statements are questions that evaluate quantities of interest.\n5\n6 ;; Condition: Alice won against Bob.\n7 (condition (won-against '(alice) '(bob)))\n8\n9 ;; Condition: John and Mary won against Tom and Sue.\n10 (condition (won-against '(john mary) '(tom sue)))\n11\n12 ;; Query: If Mary played against Tom, who would win?\n13 (query (won-against '(mary) '(tom)))\n14\n15 ;; Certain statements are underspecified and require some interpretation. For example:\n16 ;; Condition: Sue is very strong.\n17 (condition (> (strength 'sue) 75))\n18\n70\nA.2\nRelational reasoning\nA\nLANGUAGE AND WORLD MODELS\n19 ;; We can `define` new constructs that are useful for translation. For example:\n20 ;; Condition: Bob is stronger than John.\n21 (define (stronger-than? player-1 player-2)\n22\n(> (strength player-1) (strength player-2)))\n23 (condition (stronger-than? 'bob 'john))\n24\n25 ;; Query: Is Sue stronger than Mary?\n26 (query (stronger-than? 'sue 'mary))\n27\n28 ;; Condition: A couple of the players are stronger than John.\n29 (condition (>= (count (map (lambda (player) (stronger-than? player 'john) players)) 2)))\nCode Block 2: Prompt examples\nA.2\nRelational reasoning\nA.2.1\nGenerative world model for kinship\n1 ;; -- KINSHIP GENERATIVE DOMAIN THEORY --\n2\n3 ;; All the names that can be used in the conversational context.\n4 (define ALL-NAMES '(avery blake charlie dana))\n5\n6 ;; Generates unique person ids of the format (person-0, person-1, ...)\n7 (define PERSON-PREFIX \"person-\")\n8 (define new-person-id (make-gensym PERSON-PREFIX))\n9 (define (id->idx person-id)\n10\n(string->number (string-slice (stringify person-id) (string-length PERSON-PREFIX))))\n11\n12 ;; Randomly assign a gender\n13 (define person->gender (mem (lambda (person-id)\n14\n(uniform-draw '(male female)))))\n15\n16 ;; Randomly-ordered list of person names\n17 (define NAMES (shuffle-unique ALL-NAMES))\n18 (define person->name (mem (lambda (person-id)\n19\n(list-ref NAMES (id->idx person-id)))))\n20\n21 ;; Person node in tree\n22 (define (person person-id parent-1-id parent-2-id) (list\n23\n(pair 'person-id person-id)\n24\n(pair 'name person-id)\n25\n(pair 'gender (person->gender person-id))\n26\n(pair 'parent-1-id parent-1-id)\n27\n(pair 'parent-2-id parent-2-id)))\n28\n29 ;; Generate the full tree\n30 ;; Max tree size is 1 + (sum_{n=0}^{n=MAX-DEPTH} 2 * MAX-WIDTH^n)\n31 (define MAX-WIDTH 3)\n32 (define MAX-DEPTH 2)\n33 (define PARTNER-PROBABILITY 0.5)\n34 (define (generate-tree root-primary-id root-secondary-id depth)\n35\n(let* (\n36\n;; Create the primary parent\n37\n(parent-1-id (new-person-id))\n38\n(parent-1 (person parent-1-id root-primary-id root-secondary-id)))\n39\n(if (flip PARTNER-PROBABILITY)\n40\n;; Case: parent-1 has partner\n41\n(let* (\n71\nA.2\nRelational reasoning\nA\nLANGUAGE AND WORLD MODELS\n42\n;; Create the secondary parent\n43\n(parent-2-id (new-person-id))\n44\n(parent-2 (person parent-2-id () ()))\n45\n46\n;; Link the parents with a partner relation\n47\n(parent-1 (append parent-1 (list (pair 'partner-id parent-2-id))))\n48\n(parent-2 (append parent-2 (list (pair 'partner-id parent-1-id))))\n49\n50\n;; Generate children\n51\n(n-children (if (>= depth MAX-DEPTH) 0 (bounded-geometric 0.5 0 MAX-WIDTH)))\n52\n(child-trees (repeat n-children (lambda () (generate-tree parent-1-id parent-2-id (+ depth 1)))))\n53\n54\n;; Update the parents to point to the children\n55\n(child-ids (map (lambda (t) (lookup (first t) 'person-id)) child-trees))\n56\n(parent-1 (append parent-1 (list (pair 'child-ids child-ids))))\n57\n(parent-2 (append parent-2 (list (pair 'child-ids child-ids)))))\n58\n(append (list parent-1) (list parent-2) (shallow-flatten child-trees)))\n59\n60\n;; Case: parent-1 has no partner\n61\n(list parent-1))))\n62\n63 ;; Generate the global tree.\n64 (define T (generate-tree () () 0))\n65\n66 ;; Assign names randomly to (some of) the people in the tree.\n67 (define (add-names-to-tree tree names)\n68\n(if (null? tree) ()\n69\n(let*\n70\n;; Probability of addding a name to the first person\n71\n((p (min 1.0 (/ (length names) (length tree))))\n72\n(person (first tree)))\n73\n(if (flip p)\n74\n;; Name the person\n75\n(let\n76\n((named-person (update-list person 1 (pair 'name (first names)))))\n77\n(cons named-person (add-names-to-tree (rest tree) (rest names))))\n78\n;; Don't name the person\n79\n(cons person (add-names-to-tree (rest tree) names))))))\n80\n81 ;; Update the tree with the name information.\n82 (define T (add-names-to-tree T NAMES))\nCode Block 3: Generative domain theory for family trees.\nA.2.2\nKinship tree utilities\n1 ;; -- KINSHIP TREE UTILITIES --\n2\n3 ;; Returns all instances of person with property `key` equal to `value`\n4 (define filter-by-property\n5\n(mem (lambda (key value)\n6\n(filter (lambda (p) (equal? (lookup p key) value)) T))))\n7\n8 ;; Returns the unique instance of person with name.\n9 (define get-person-by-name\n10\n(mem (lambda (name)\n11\n(let\n12\n((results (filter-by-property 'name name)))\n13\n(if (null? results) () (first results))))))\n72\nA.2\nRelational reasoning\nA\nLANGUAGE AND WORLD MODELS\n14\n15 ;; People without a name can be referenced directly by person-id.\n16 (define get-person-by-id\n17\n(mem (lambda (person-id)\n18\n(if (null? person-id)\n19\n()\n20\n(let ((idx (id->idx person-id)))\n21\n(if (>= idx (length T)) () (list-ref T idx)))))))\n22\n23 ;; Get a person object either by name or person-id.\n24 (define get-person\n25\n(mem (lambda (person-ref)\n26\n(cond\n27\n((null? person-ref) ())\n28\n((member? person-ref NAMES) (get-person-by-name person-ref))\n29\n(else (get-person-by-id person-ref))))))\n30\n31 ;; Get a property of a person.\n32 (define get-property\n33\n(mem (lambda (name key)\n34\n(lookup (get-person name) key))))\n35\n36 ;; -- TREE OPERATORS --\n37 ;; predicate :: name -> boolean\n38\n39 (define (map-tree predicate)\n40\n(map (lambda (x) (predicate (lookup x 'name))) T))\n41\n42 (define (filter-tree predicate)\n43\n(filter (lambda (x) (predicate (lookup x 'name))) T))\n44\n45 (define (exists predicate)\n46\n(some (map-tree predicate)))\nCode Block 4: Utility functions for kinship trees.\nA.2.3\nKinship conceptual system\n1 ;; -- KINSHIP CONCEPTUAL SYSTEM --\n2\n3 ;; Gets the partner of a person.\n4 (define (partner-of name)\n5\n(get-property (get-property name 'partner-id) 'name))\n6\n7 ;; Gets the parents of a person.\n8 (define (parents-of name)\n9\n(let* ((parent-1-id (get-property name 'parent-1-id))\n10\n(parent-1-name (get-property parent-1-id 'name))\n11\n(parent-2-id (get-property name 'parent-2-id))\n12\n(parent-2-name (get-property parent-2-id 'name)))\n13\n(list parent-1-name parent-2-name)))\n14\n15 ;; Gets the grandparents of a person.\n16 (define (grandparents-of name)\n17\n(let ((parent-1 (first (parents-of name))))\n18\n(parents-of parent-1)))\n19\n20 ;; Gets the children of a person.\n21 (define (children-of name)\n73\nA.2\nRelational reasoning\nA\nLANGUAGE AND WORLD MODELS\n22\n(let ((child-ids (get-property name 'child-ids)))\n23\n(map (lambda (child-id) (get-property child-id 'name)) child-ids)))\n24\n25 ;; Gets the siblings of a person.\n26 (define (siblings-of name)\n27\n(let* ((parent-1-id (get-property name 'parent-1-id))\n28\n(child-ids (get-property parent-1-id 'child-ids))\n29\n(child-names (map (lambda (child-id) (get-property child-id 'name)) child-ids)))\n30\n(filter (lambda (child-name) (not (equal? child-name name))) child-names)))\n31\n32 ;; -- BOOLEAN RELATIONS --\n33 (define (partner-of? name_a name_b)\n34\n(equal? name_a (partner-of name_b)))\n35\n36 (define (parent-of? name_a name_b)\n37\n(member? name_a (parents-of name_b)))\n38\n39 (define (father-of? name_a name_b)\n40\n(and (equal? (get-property name_a 'gender) 'male)\n41\n(parent-of? name_a name_b)))\n42\n43 (define (mother-of? name_a name_b)\n44\n(and (equal? (get-property name_a 'gender) 'female)\n45\n(parent-of? name_a name_b)))\n46\n47 (define (grandparent-of? name_a name_b)\n48\n(member? name_a (grandparents-of name_b)))\n49\n50 (define (grandfather-of? name_a name_b)\n51\n(and (equal? (get-property name_a 'gender) 'male)\n52\n(grandparent-of? name_a name_b)))\n53\n54 (define (grandmother-of? name_a name_b)\n55\n(and (equal? (get-property name_a 'gender) 'female)\n56\n(grandparent-of? name_a name_b)))\n57\n58 (define (child-of? name_a name_b)\n59\n(member? name_a (children-of name_b)))\n60\n61 (define (son-of? name_a name_b)\n62\n(and (equal? (get-property name_a 'gender) 'male)\n63\n(child-of? name_a name_b)))\n64\n65 (define (daughter-of? name_a name_b)\n66\n(and (equal? (get-property name_a 'gender) 'female)\n67\n(child-of? name_a name_b)))\n68\n69 (define (sibling-of? name_a name_b)\n70\n(member? name_a (siblings-of name_b)))\n71\n72 (define (brother-of? name_a name_b)\n73\n(and (equal? (get-property name_a 'gender) 'male)\n74\n(sibling-of? name_a name_b)))\n75\n76 (define (sister-of? name_a name_b)\n77\n(and (equal? (get-property name_a 'gender) 'female)\n78\n(sibling-of? name_a name_b)))\nCode Block 5: Conceptual system and derived predicates for kinship trees.\n74\nA.2\nRelational reasoning\nA\nLANGUAGE AND WORLD MODELS\nA.2.4\nTranslation examples for kinship\n1 ;; -- CONDITION AND QUERY STATEMENTS --\n2 ;; Now, let's translate some user-defined statements.\n3 ;; Each statement begins with either `Condition` or `Query`.\n4 ;; `Condition` statements provide facts about the scenario.\n5 ;; `Query` statements are questions that evaluate quantities of interest.\n6\n7 ;; Condition: Ryan's partner is Taylor.\n8 (condition (partner-of? 'ryan 'taylor))\n9\n10 ;; Condition: Taylor is the mother of Sam.\n11 (condition (mother-of? 'taylor 'sam))\n12\n13 ;; Condition: Sam's father is Ryan.\n14 (condition (father-of? 'ryan 'sam))\n15\n16 ;; Condition: Sam has two siblings.\n17 (condition (= (length (siblings-of 'sam)) 2))\n18\n19 ;; Condition: Sam has a brother.\n20 (condition\n21\n(exists (lambda (x)\n22\n(brother-of? x 'sam))))\n23\n24 ;; Condition: Payton's partner has a brother named Kyle.\n25 (condition\n26\n(exists (lambda (x) (and\n27\n(partner-of? x 'payton)\n28\n(brother-of? 'kyle x)))))\n29\n30 ;; Condition: Payton's partner has a sister who has a son named Sam.\n31 (condition\n32\n(exists (lambda (x) (and\n33\n(partner-of? x 'payton)\n34\n(exists (lambda (y) (and\n35\n(sister-of? y x)\n36\n(son-of? 'sam y))))))))\n37\n38 ;; Query: Who are Sam's parents?\n39 (query (parents-of 'sam))\n40\n41 ;; Query: How many children does Kyle have?\n42 (query (length (children-of 'kyle)))\n43\n44 ;; Query: Who is Ryan's grandfather?\n45 (query\n46\n(filter-tree\n47\n(lambda (x) (grandfather-of? x 'ryan))))\n48\n49 ;; Query: Does Taylor have a sister?\n50 (query\n51\n(exists (lambda (x)\n52\n(sister-of? x 'taylor))))\nCode Block 6: Translation examples for kinship trees.\n75\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\nA.2.5\nWhy not Prolog?\nReaders who are familiar with the world of logic programming may wonder why we have chosen to model\nthe kinship domain in Church instead of a more standard logic programming language, such as Prolog.\nIndeed, kinship is often one of the introductory examples in Prolog textbooks (Pereira & Shieber, 2002)\nand online tutorials,8 from which we drew inspiration when writing this section. Moreover, there are many\nstructural parallels between our framework and the style of declarative programming embodied by Prolog:\nschemecondition statements in Church are similar to facts in Prolog; derived concepts like schemefather-of? in\nour Church kinship model are analogous to Prolog rules; and schemequery performs similar functions in both\nlanguages (though the algorithms that underlie these queries differ in important ways). And, as discussed in\nthe introduction to Section 3.1, Prolog was originally developed as a model of natural language (Colmerauer\net al., 1972) and has deep ties to computational linguistics. So: why not use Prolog?\nIn short, there is nothing about our approach to semantic parsing that precludes swapping out Church for\nother programming languages, like Prolog, SMT solvers, or even a general purpose language like Python. In\nfact, with the right prompting, Codex readily translates natural language utterances like Avery has a sister\nnamed Blake into sister_of(blake, avery) in Prolog. On the parsing side, we did not encounter any technical\nlimitations to using LLMs to translate natural language into Prolog.\nHowever, because Prolog is based on definite (Horn) clauses, there are limitations in the kinds of utterances\nthat we can express and the kinds of inferences that we can make when working in Prolog. For instance, a\ntypical Prolog kinship model might have a rule defining the concept of a \u201cgrandfather\u201d as follows:\ngrandfather_of(X,Y) :- male(X),\nparent_of(X,Z),\nparent_of(Z,Y).\nNow, if we learn that Charlie is the grandfather of Dana, we might be inclined to translate this into Prolog\nas a fact: grandfather_of(charlie, dana). Given this information, we can make various deductive inferences:\ne.g., that Charlie is male, and that there exists some person in the family tree who is both the child of Charlie\nand the parent of Dana. In fact, this is how the grandfather_of(X,Y) rule is defined in the first place.\nFor this reason, it is especially counterintuitive that these kinds of inferences are not at all straightforward\nin Prolog. Because logical implication in definite clauses is unidirectional, anyone satisfying the right-hand\nside of the grandfather_of(X,Y) rule is considered a grandfather. However, our rule says nothing about what\nbeing a grandfather entails. Moreover, our above translation grandfather_of(charlie, dana) is actually quite\nfacile; it simply modifies grandfather_of(X,Y) such that queries will now return true for anyone satisfying the\noriginal definition; or for the special case where X=charlie and Y=dana. These are all examples of limitations\nof the kinds of deductive inferences that we can model with Prolog. Additionally, there are many kinds of\ninductive inferences that are not well-captured by Prolog; e.g., because Charlie has at least one child, he is\nmore likely to have multiple children, and is more likely to be married.\nIn sum, to get the kinds of mixed deductive and inductive inferences that we would like to see in an\nexpressive language-of-thought, we need to have ways of incorporating and trading off uncertainty in our world\nmodel. ProbLog (De Raedt et al., 2007; Dries et al., 2017; Suster et al., 2021), a probabilistic extension of\nProlog in which deduction rules can be annotated with probabilities, offers one way of integrating uncertainty\nwith deductive reasoning. Church goes a step further by specifying a generative domain theory in addition to\nprobabilistic inference rules. We believe that this interplay between probabilistic priors and likelihoods\u2014which\nis central to Bayesian inference\u2014is also at the heart of human cognition.\nA.3\nPerceptual and physical reasoning\nStatic visual scenes\nA.3.1\nGenerative world model for static visual scenes\n1 ;; Objects have a shape attribute, which is a choice of cube, sphere, or cylinder shape categories.\n2 (define choose-shape\n3\n(mem (lambda (obj-id)\n8https://swish.swi-prolog.org/p/prolog-family-tree.pl\n76\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n4\n(pair 'shape (uniform-draw '(mug can bowl))))))\n5\n6 ;; Objects have a color attribute that is drawn from a predefined set of RGB values.\n7 (define choose-color\n8\n(mem (lambda (obj-id)\n9\n(pair 'color (uniform-draw (list\n10\n(list 255 0 0)\n11\n(list 0 0 255)\n12\n(list 0 255 0)\n13\n(list 255 255 0)\n14\n))))))\n15 ;; An object is an object ID, and the object's attribute types and their values.\n16 (define object (mem (lambda (obj-id) (list\n17\n(pair 'object-id obj-id)\n18\n(choose-shape obj-id)\n19\n(choose-color obj-id)))))\n20\n21 ;; Scenes can have a maximum of 12 objects.\n22 (define max-objects 12)\n23 ;; The number of objects in a scene tends to be not too large, and is capped at the maximum number of\nobjects.\n24 (define choose-num-objects\n25\n(mem (lambda (scene-id) (floor (min max-objects (* max-objects (exponential 1)))))))\n26\n27 ;; Then, for each object we intend to generate, generate an object indexical, and associate it with a\nchoice of attributes.\n28 (define obj-id-gensym (make-gensym \"obj-\"))\n29 (define (generate-n-objects scene-id total-objects)\n30\n(if (= total-objects 0)\n31\n(list (object (obj-id-gensym)))\n32\n(cons (object (obj-id-gensym)) (generate-n-objects scene-id (- total-objects 1)))))\n33 (define objects-in-scene (mem (lambda (scene-id) (generate-n-objects scene-id (choose-num-objects\nscene-id)))))\n34\n35\n36 ;; An object is red if it is of this continuous color value.\n37 (define red (list 255 0 0))\n38 ;; An object is blue if it is of this continuous color value.\n39 (define blue (list 0 0 255))\n40 ;; An object is green if it is of this continuous color value.\n41 (define green (list 0 255 0))\n42 ;; An object is yellow if it is of this continuous color value.\n43 (define yellow (list 255 255 0))\n44\n45 ;; Check if an object is of a given shape.\n46 (define is-shape?\n(lambda (shape) (lambda (object) (equal? (cdr (assoc 'shape object)) shape))))\n47 ;; Check if an object is of a given named color.\n48 (define is-color?\n(lambda (color) (lambda (object) (equal? (cdr (assoc 'color object)) color))))\n49\n50 ;; Select only objects from the scene of a given color.\n51 (define filter-color(lambda (color) (lambda (object-list) (filter (is-color? color) object-list))))\n52\n53 ;; Select only objects from the scene of a given shape.\n54 (define filter-shape (lambda (shape) (lambda (object-list) (filter (is-shape? shape) object-list))))\nCode Block 7: Generative domain theory for tabletop scenes. Generates scenes containing a set of objects\nwhich vary in shape and color. These scene states are rendered by a separately generated render function to\ngenerate images. Shown with natural language comments, but these are not used in the LLM prompt.\n77\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\nA.3.2\nTranslation examples for static visual scenes\n1 ;; There's a blue thing.\n2 (condition (> (length ((filter-color blue) (objects-in-scene 'this-scene))) 0))\n3 ;; There's at least two blue plates.\n4 (condition\n(>=\n(length\n5\n((filter-color blue)\n6\n((filter-shape 'plate)\n7\n(objects-in-scene 'scene))))\n8 2))\n9 ;; There's many blue plates.\n10 (condition\n(>=\n(length\n11\n((filter-color blue)\n12\n((filter-shape 'plate)\n13\n(objects-in-scene 'scene))))\n14 5))\n15 ;; There's exactly two plates and there's also a yellow thing.\n16 (condition\n17\n(and (= (length ((filter-shape 'plate) (objects-in-scene 'scene))) 2)\n18\n(> (length ((filter-color yellow) (objects-in-scene 'scene))) 0)))\n19\n20 ;; Is there a mug?\n21 (query (> (length ((filter-shape 'mug) (objects-in-scene 'this-scene))) 0))\nCode Block 8: Translation examples for the visual domain. These examples are concatenated with the visual\nscenes generative model to produce the prompt used to generate new translations.\nDynamic physical scenes\nA.3.3\nGenerative world model for physical scenes\n1 (define (get_attribute obj key)\n2\n(if (assoc key obj) (rest (assoc key obj)) ()))\n3\n4\n(define (member? a b)\n5\n(if (member a b) true false))\n6\n(define concatenate\n7\n(lambda (list-1 list-2)\n8\n(if (null? list-1)\n9\nlist-2\n10\n(cons (car list-1) (concatenate (cdr list-1) list-2)))))\n11\n12 (define (pairs x l)\n13\n(define (aux accu x l)\n14\n(if (null? l)\n15\naccu\n16\n(let ((y (car l))\n17\n(tail (cdr l)))\n18\n(aux (cons (cons x y) accu) x tail))))\n19\n(aux '() x l))\n20\n21 (define (cartesian_product l m)\n22\n(define (aux accu l)\n23\n(if (null? l)\n24\naccu\n25\n(let ((x (car l))\n26\n(tail (cdr l)))\n78\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n27\n(aux (append (pairs x m) accu) tail))))\n28\n(aux '() l))\n29\n30 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\nGenerative domain theory: dynamic scenes. Collision detection.\n31 (define get_num_objects 2)\n32 (define OBJECT_DEFAULT_RADIUS 1)\n33 (define GRAVITY 9.8)\n34 (define DELTA_T 0.5)\n35\n36 (define get_initial_color\n37\n(lambda (obj_id)\n38\n(if (eq? obj_id 'obj-0)\n39\n(list 255 0 0)\n40\n(list 0 0 255))))\n41\n42 (define choose_mass\n43\n(mem (lambda (obj_id)\n44\n(abs (gaussian 5 3)))))\n45\n46 (define choose_shapes\n47\n(mem (lambda (scene-id) (uniform-draw (list 'sphere 'block)))))\n48\n49 (define min_x -3)\n50 (define max_x 3)\n51 (define mid_x (+ (/ (- max_x min_x) 2) min_x))\n52 (define get_initial_x\n53\n(lambda (obj_id)\n54\n(if (eq? obj_id 'obj-0)\n55\nmin_x\n56\nmid_x)))\n57\n58 (define min_force 0)\n59\n(define max_force 10)\n60\n(define mid_force (+ (/ (- max_force min_force) 2) min_force))\n61\n(define choose_initial_force\n62\n(mem (lambda (obj_id)\n63\n(if (eq? obj_id 'obj-0)\n64\n(abs (gaussian mid_force 3))\n65\n0\n66\n))))\n67\n68 (define static_friction_constant (lambda (shape)\n69\n(if (eq? shape 'sphere)\n70\n0.02\n71\n0.05)\n72\n))\n73 (define kinetic_friction_constant (lambda (shape)\n74\n(if (eq? shape 'sphere)\n75\n0.01\n76\n0.02)\n77\n))\n78 (define normal_force (lambda (m) (* m GRAVITY)))\n79 (define force_after_friction (lambda (f v shape m)\n80\n(if (> (abs v) 0)\n81\n(- f (* (kinetic_friction_constant shape) (normal_force m)))\n82\n(if (< f (* (static_friction_constant shape) (normal_force m))) 0 (- f (*\n(kinetic_friction_constant shape) (normal_force m)))\n83\n))))\n79\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n84\n85 (define newtons_second (lambda (f m) (/ f m)))\n86 (define v_next (lambda (v_prev a_prev delta_t)\n87\n(let ((v_temp (+ v_prev (* a_prev delta_t))))\n88\n(if (>= (* v_prev v_temp) 0) v_temp 0))\n89\n))\n90 (define x_next (lambda (x_prev v_prev delta_t) (+ x_prev (* v_prev delta_t))))\n91 (define initial_object_state (mem (lambda (obj_id scene_id)\n92\n(let ((obj_shape (choose_shapes scene_id)))\n93\n(let ((obj_mass (choose_mass obj_id)))\n94\n(let ((obj_color (get_initial_color obj_id)))\n95\n(let ((initial_x (get_initial_x obj_id)))\n96\n(let ((initial_push_force (choose_initial_force obj_id)))\n97\n(let ((initial_force (force_after_friction\ninitial_push_force 0 obj_shape obj_mass)))\n98\n(list\n99\n(pair 'object_id obj_id)\n100\n(pair 'object_radius OBJECT_DEFAULT_RADIUS)\n101\n(pair 'shape obj_shape)\n102\n(pair 'mass obj_mass)\n103\n(pair 'color obj_color)\n104\n(pair 'x initial_x)\n105\n(pair 'initial_push_force initial_push_force)\n106\n(pair 'f initial_force)\n107\n(pair 't 0)\n108\n(pair 'a_prev (newtons_second initial_force obj_mass))\n109\n(pair 'a (newtons_second initial_force obj_mass))\n110\n(pair 'v_0 0)\n111\n(pair 'v (v_next 0 (newtons_second initial_force\nobj_mass) DELTA_T)))\n112\n)))))))))\n113 (define obj_id_gensym (make_gensym \"obj-\"))\n114 (define generate_initial_state\n115\n(mem (lambda (scene_id total_objects)\n116\n(if (= total_objects 1)\n117\n(list (initial_object_state (obj_id_gensym) scene_id))\n118\n(cons (initial_object_state (obj_id_gensym) scene_id) (generate_initial_state scene_id\n(- total_objects 1)))))))\n119\n120 (define generate_initial_scene_event_state (mem (lambda (scene_id total_objects)\n121\n(pair 0\n122\n(list\n123\n(pair 'scene_states\n(generate_initial_state scene_id total_objects))\n124\n(pair 'event_states [])\n125\n))\n126\n)\n127 ))\n128\n129 (define event_id_gensym (make_gensym \"event-\"))\n130 (define circle_intersect? (lambda (subject_x subject_radius object_x object_radius)\n131 (let ((square_circle_distance (expt (- subject_x object_x) 2)))\n132 (let ((square_radii (expt (+ subject_radius object_radius) 2)))\n133 (leq square_circle_distance square_radii)))\n134 ))\n135 (define elastic_collision_subject_v (lambda (subject_m subject_v object_m object_v)\n136\n(/ (+ (* 2 (* object_m object_v)) (* subject_v (- subject_m object_m))) (+ subject_m object_m))\n137 ))\n138\n80\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n139 (define get_collision_events (lambda (time scene_event_state_for_time)\n140\n(let ((scene_event_state (get_attribute scene_event_state_for_time time)))\n141\n(let ((scene_state (get_attribute scene_event_state 'scene_states)))\n142\n(if (= (length scene_state) 1)\n143\n()\n144\n(fold (lambda (event events) (if (equal? event ()) events (cons event events)))\n()\n145\n(let ((paired_object_states (cartesian_product scene_state scene_state)))\n146\n(map (lambda (paired_objects)\n147\n148\n(let ((event_subject (get_attribute (first paired_objects) 'object_id)))\n149\n(let ((event_object (get_attribute (cdr paired_objects) 'object_id)))\n150\n(if (eq? event_subject event_object) ()\n151\n(let ((subject_v (get_attribute (first paired_objects) 'v)))\n152\n(let ((subject_x (get_attribute (first paired_objects) 'x)))\n153\n(let ((subject_m (get_attribute (first paired_objects) 'mass)))\n154\n(let ((subject_radius (get_attribute (first paired_objects) 'object_radius)))\n155\n(let ((object_v (get_attribute (cdr paired_objects) 'v)))\n156\n(let ((object_x (get_attribute (cdr paired_objects) 'x)))\n157\n(let ((object_m (get_attribute (cdr paired_objects) 'mass)))\n158\n(let ((object_radius (get_attribute (cdr paired_objects) 'object_radius)))\n159\n(if (circle_intersect? subject_x subject_radius object_x object_radius)\n160\n(list\n161\n(pair 'event-id\n(event_id_gensym))\n162\n(pair 'event_time\ntime)\n163\n(pair 'event_predicates (list 'is_colliding))\n164\n(pair 'event_subject event_subject)\n165\n(pair 'event_object event_object)\n166\n(pair 'subject_initial_v subject_v)\n167\n(pair 'subject_final_v (elastic_collision_subject_v subject_m subject_v object_m\nobject_v))\n168\n(pair 'object_initial_v object_v)\n169\n)\n170\n()))))))))))\n171\n)))\n172\npaired_object_states)))\n173 )))))\n174\n175\n176 (define generate_next_object_state (lambda (current_time event_state) (lambda (prev_object_state)\n177\n(let ((obj_id (cdr (assoc 'object_id prev_object_state))))\n178\n(let ((collision_events (fold (lambda (event events) (if (equal? (get_attribute event 'event_subject)\nobj_id) (cons event events) events)) () event_state)))\n179\n(if (> (length collision_events) 0)\n180\n(generate_collision_event_state current_time obj_id prev_object_state (car collision_events))\n181\n(generate_no_collision_event_state current_time obj_id prev_object_state)\n182\n)\n183\n)))))\n184\n185 (define generate_collision_event_state (lambda (current_time obj_id prev_object_state collision_event)\n186\n(let ((obj_radius (cdr (assoc 'object_radius prev_object_state))))\n187\n(let ((obj_mass (cdr (assoc 'mass prev_object_state))))\n188\n(let ((obj_color (cdr (assoc 'color prev_object_state))))\n189\n(let ((obj_shape (cdr (assoc 'shape prev_object_state))))\n190\n(let ((v_prev (cdr (assoc 'v prev_object_state))))\n191\n(let ((a_prev (cdr (assoc 'a_prev prev_object_state))))\n192\n(let ((x_prev (cdr (assoc 'x prev_object_state))))\n193\n(let ((v (get_attribute collision_event 'subject_final_v)))\n194\n(let ((x (x_next x_prev v 1)))\n195\n(list\n81\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n196\n(pair 'object_id obj_id)\n197\n(pair 'object_radius obj_radius)\n198\n(pair 'shape obj_shape)\n199\n(pair 'color obj_color)\n200\n(pair 'mass obj_mass)\n201\n(pair 'x x)\n202\n(pair 'f 0)\n203\n(pair 't (* current_time DELTA_T))\n204\n(pair 'a_prev 0)\n205\n(pair 'a 0)\n206\n(pair 'v_0 0)\n207\n(pair 'v v))\n208\n)))))\n209\n))))\n210 ))\n211\n212 (define generate_no_collision_event_state (lambda (current_time obj_id prev_object_state)\n213\n(let ((obj_radius (cdr (assoc 'object_radius prev_object_state))))\n214\n(let ((obj_mass (cdr (assoc 'mass prev_object_state))))\n215\n(let ((obj_color (cdr (assoc 'color prev_object_state))))\n216\n(let ((obj_shape (cdr (assoc 'shape prev_object_state))))\n217\n(let ((v_prev (cdr (assoc 'v prev_object_state))))\n218\n(let ((a_prev_no_friction (cdr (assoc 'a_prev prev_object_state))))\n219\n(let ((a_prev (newtons_second (force_after_friction 0 v_prev obj_shape obj_mass) obj_mass)))\n220\n(let ((x_prev (cdr (assoc 'x prev_object_state))))\n221\n(let ((v (v_next v_prev a_prev DELTA_T)))\n222\n(let ((x (x_next x_prev v_prev DELTA_T)))\n223\n(list\n224\n(pair 'object_id obj_id)\n225\n(pair 'object_radius obj_radius)\n226\n(pair 'shape obj_shape)\n227\n(pair 'color obj_color)\n228\n(pair 'mass obj_mass)\n229\n(pair 'x x)\n230\n(pair 'f (force_after_friction 0 v_prev obj_shape obj_mass))\n231\n(pair 't (* current_time DELTA_T))\n232\n(pair 'a_prev a_prev)\n233\n(pair 'a 0)\n234\n(pair 'v_0 0)\n235\n(pair 'v v))\n236\n)))))\n237\n))))\n238 )))\n239\n240 (define generate_next_scene_state (lambda (prev_scene_state event_state next_time)\n241\n(map (generate_next_object_state next_time event_state) prev_scene_state)))\n242\n243 (define generate_next_scene_event_state_time (lambda (next_time scene_event_state_for_times)\n244\n(let ((prev_scene_event_state (get_attribute scene_event_state_for_times (- next_time 1))))\n245\n(let ((prev_scene_state (get_attribute prev_scene_event_state 'scene_states)))\n246\n(let ((event_state (get_collision_events (- next_time 1) scene_event_state_for_times)))\n247\n248\n(pair next_time (list\n249\n(pair 'scene_states (generate_next_scene_state prev_scene_state event_state next_time))\n250\n(pair 'event_states event_state)\n251\n))\n252 )))))\n253\n254 (define generate_next_scene_event_states\n82\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n255\n(lambda (current_time prev_scene_event_states_for_times)\n256\n(cons (generate_next_scene_event_state_time current_time prev_scene_event_states_for_times)\nprev_scene_event_states_for_times)\n257 ))\n258\n259 (define generate_scene_event_states_for_times (mem (lambda (scene_id total_objects total_time)\n260\n(if (= total_time 0)\n261\n(list\n262\n(generate_initial_scene_event_state\nscene_id total_objects)\n263\n)\n264\n(let ((prev_scene_event_states\n(generate_scene_event_states_for_times scene_id total_objects (- total_time 1))))\n265\n(generate_next_scene_event_states\ntotal_time prev_scene_event_states)\n266 )))))\n267\n268 (define max_time 9)\n269\n270 (define base_states_for_times\n(generate_scene_event_states_for_times 'this_scene get_num_objects\nmax_time))\n271\n272 ;;;;;;;;;;;;;;;;;;;;;;;;;;Derived predicates.\n273 (define objects_in_scene (lambda (base_states_for_times)\n274\n(let ((initial_base_states_at_time (cdr (assoc 0 (cdr base_states_for_times)))))\n275\n(let ((base_state (cdr (assoc 'scene_states initial_base_states_at_time))))\n276\nbase_state\n277\n))\n278\n))\n279 (define red (list 255 0 0))\n280 (define blue (list 0 0 255))\n281 (define is_color?\n(lambda (color) (lambda (object) (equal? (cdr (assoc 'color object)) color))))\n282 (define is_shape?\n(lambda (shape) (lambda (object) (equal? (cdr (assoc 'shape object)) shape))))\n283\n284 (define all_objects\n(objects_in_scene base_states_for_times))\n285 (define (exists_object predicate)\n286\n(some (map predicate (objects_in_scene base_states_for_times))))\n287\n288 (define (filter_objects predicate)\n289\n(map\n290\n(lambda (o) (get_attribute o 'object_id))\n291\n(filter predicate (objects_in_scene base_states_for_times))))\n292 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n293 (define QUICKLY_THRESHOLD 2)\n294 (define SLOWLY_THRESHOLD 2)\n295\n296\n(define is_moving_events (mem (lambda (base_states_for_times)\n297\n(fold (lambda (base_state_for_time these_events)\n298\n(let ((current_time (car base_state_for_time)))\n299\n(let ((base_state (cdr (assoc 'scene_states (cdr base_state_for_time)))))\n300\n(fold (lambda (obj_state these_events)\n301\n(let ((obj_id (cdr (assoc 'object_id obj_state))))\n302\n(let ((obj_velocity (cdr (assoc 'v obj_state))))\n303\n(let ((obj_speed (abs obj_velocity)))\n304\n(if (> obj_speed 0)\n305\n;;\n306\n(let ((event_predicates\n307\n(if (> obj_speed QUICKLY_THRESHOLD)\n308\n(list 'is_moving 'is_quickly)\n83\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n309\n(if (< obj_speed SLOWLY_THRESHOLD)\n310\n(list 'is_moving 'is_slowly)\n311\n(list 'is_moving)\n312\n))\n313\n))\n314\n(cons\n315\n(list\n316\n(pair 'event-id\n(event_id_gensym))\n317\n(pair 'event_time\ncurrent_time)\n318\n(pair 'event_predicates event_predicates)\n319\n(pair 'event_subject obj_id)\n320\n(pair 'event_speed obj_speed)\n321\n)\n322\nthese_events))\n323\nthese_events\n324\n325\n)))))\n326\nthese_events base_state))))\n327 () base_states_for_times))))\n328\n329 (define is_resting_events (mem (lambda (base_states_for_times)\n330\n(fold (lambda (base_state_for_time these_events)\n331\n(let ((current_time (car base_state_for_time)))\n332\n(let ((base_state (cdr (assoc 'scene_states (cdr base_state_for_time)))))\n333\n(fold (lambda (obj_state these_events)\n334\n(let ((obj_id (cdr (assoc 'object_id obj_state))))\n335\n(let ((obj_velocity (cdr (assoc 'v obj_state))))\n336\n(let ((obj_speed (abs obj_velocity)))\n337\n(if (= obj_speed 0)\n338\n;;\n339\n(let ((event_predicates\n340\n(list 'is_resting)))\n341\n(cons\n342\n(list\n343\n(pair 'event-id\n(event_id_gensym))\n344\n(pair 'event_time\ncurrent_time)\n345\n(pair 'event_predicates event_predicates)\n346\n(pair 'event_subject obj_id)\n347\n(pair 'event_speed obj_speed)\n348\n)\n349\nthese_events))\n350\nthese_events\n351\n352\n)))))\n353\nthese_events base_state))))\n354\n() base_states_for_times))))\n355\n356 (define is_colliding_events (mem (lambda (base_states_for_times)\n357\n(fold (lambda (base_state_for_time these_events)\n358\n(let ((current_time (car base_state_for_time)))\n359\n(let ((event_states (cdr (assoc 'event_states (cdr base_state_for_time)))))\n360\n(fold (lambda (event_state these_events)\n361\n(let ((subject_initial_speed (abs (get_attribute event_state 'subject_initial_v))))\n362\n(let ((subject_final_speed (abs (get_attribute event_state 'subject_final_v))))\n363\n(let ((object_initial_speed (abs (get_attribute event_state 'object_initial_v))))\n364\n(let ((cause_subject_object_event (and (> subject_initial_speed 0) (=\nobject_initial_speed 0))))\n365\n(let\n366\n((event_predicates\n84\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n367\n(if (and cause_subject_object_event (eq? subject_final_speed 0))\n368\n(list 'is_launching 'is_hitting 'is_colliding)\n369\n(if (> subject_initial_speed 0)\n370\n(list 'is_hitting 'is_colliding)\n371\n(list 'is_colliding)\n372\n)\n373\n)))\n374\n375\n(cons (list\n376\n(pair 'event-id\n(get_attribute event_state 'event-id))\n377\n(pair 'event_time (get_attribute event_state 'event_time))\n378\n(pair 'event_predicates event_predicates)\n379\n(pair 'event_subject (get_attribute event_state 'event_subject))\n380\n(pair 'event_object (get_attribute event_state 'event_object))\n381\n(pair 'subject_initial_v (get_attribute event_state 'subject_initial_v ))\n382\n(pair 'subject_final_v (get_attribute event_state 'subject_final_v ))\n383\n(pair 'object_initial_v (get_attribute event_state 'object_initial_v ))\n384\n) these_events))))))\n385\n) these_events event_states)\n386\n)))\n387\n() base_states_for_times)\n388\n389 )))\n390\n391\n392\n393 (define events_in_scene (concatenate\n394\n(is_colliding_events base_states_for_times)\n395\n(concatenate\n396\n(is_moving_events base_states_for_times)\n397\n(is_resting_events base_states_for_times))))\n398\n399\n400 (define is_event? (lambda (event_predicate event) (member? event_predicate (get_attribute event\n'event_predicates))))\n401\n402 (define is_subject_of_event? (lambda (event object ) (equal?\n403\n(get_attribute event 'event_subject)\n404\n(get_attribute object 'object_id)\n405\n)))\n406\n407 (define is_object_of_event? (lambda (event object ) (equal?\n408\n(get_attribute event 'event_object)\n409\n(get_attribute object 'object_id)\n410\n)))\n411\n412 (define event_subject_is? (lambda (event predicate) (member?\n413\n(get_attribute event 'event_subject)\n414\n(filter_objects predicate)\n415\n)))\n416 (define event_object_is? (lambda (event predicate) (member?\n417\n(get_attribute event 'event_object)\n418\n(filter_objects predicate)\n419\n)))\n420\n421 (define (exists_event predicate)\n422\n(some (map predicate events_in_scene)))\n423\n424 (define (filter_events predicate)\n85\nA.3\nPerceptual and physical reasoning\nA\nLANGUAGE AND WORLD MODELS\n425\n(filter predicate events_in_scene))\nCode Block 9: Generative domain theory for physical scenes. Generates scenes containing a red object left of\na blue object, and a randomly generated force. These scene states are forward simulated using a physics\nengine which is shown implemented within this Church code. Shown with natural language comments, but\nthese are not used in the LLM prompt.\nA.3.4\nTranslation examples for visual scenes\n1 ;; The objects are all balls.\n2 (condition (all (map (lambda (o) ((is_shape? 'sphere) o)) all_objects)))\n3 ;; Everything is a ball.\n4 (condition (all (map (lambda (o) ((is_shape? 'sphere) o)) all_objects)))\n5 ;; Imagine the red thing is a block, and is somewhat heavy.\n6 (condition (exists_object (lambda (object)\n7\n(and\n8\n((is_color? red) object)\n9\n((is_shape? 'cube) object)\n10\n(> (get_attribute object 'mass) 2)\n11\n))))\n12 ;; There is a blue ball, and it is quite heavy.\n13 (condition (exists_object (lambda (object)\n14\n(and\n15\n((is_color? blue) object)\n16\n((is_shape? 'sphere) object)\n17\n(> (get_attribute object 'mass) 3.5)\n18\n))))\n19 ;; Now, the red block is very light.\n20 (condition (exists_object (lambda (object)\n21\n(and\n22\n((is_color? red) object)\n23\n((is_shape? 'cube) object)\n24\n(< (get_attribute object 'mass) 1)\n25\n))))\n26 ;; A blue ball is somewhat light.\n27 (condition (exists_object (lambda (object)\n28\n(and\n29\n((is_color? red) object)\n30\n((is_shape? 'cube) object)\n31\n(< (get_attribute object 'mass) 2)\n32\n))))\n33 ;; Imagine the red block gets pushed lightly to the right.\n34 (condition (exists_object (lambda (object)\n35\n(and\n36\n((is_color? red) object)\n37\n((is_shape? 'cube) object)\n38\n(< (get_attribute object 'initial_push_force) 2)\n39\n))))\n40 ;; Now, imagine a red ball is pushed hard to the right.\n41 (condition (exists_object (lambda (object)\n42\n(and\n43\n((is_color? red) object)\n44\n((is_shape? 'sphere) object)\n45\n(> (get_attribute object 'initial_push_force) 6)\n46\n))))\n47 ;; A red block hits a blue block.\n48 (condition\n49 (exists_object (lambda (object_1)\n86\nA.4\nSocial reasoning\nA\nLANGUAGE AND WORLD MODELS\n50 (exists_object (lambda (object_2)\n51 (exists_event (lambda (event)\n52\n(and\n53\n((is_color? red) object_1)\n54\n((is_shape? 'cube) object_1)\n55\n((is_color? blue) object_2)\n56\n((is_shape? 'cube) object_2)\n57\n(is_subject_of_event? event object_1)\n58\n(is_object_of_event? event object_2)\n59\n(is_event? 'is_hitting event))\n60\n)))))))\n61 ;; What's the final velocity of the red block after it is hit?\n62 (query (last (map\n63 (lambda (event) (get_attribute event 'subject_final_v))\n64 (filter_events\n65 (lambda (e)\n66 (and\n67 (is_event? 'is_colliding e)\n68 (event_subject_is? e (lambda (o)\n69\n(and\n70\n((is_color? red) o)\n71\n((is_shape? 'cube) o))))))))))\nCode Block 10: Translation examples for the physics domain. These examples are concatenated with the\nphysical scenes generative model to produce the prompt used to generate new translations.\nA.4\nSocial reasoning\nA.4.1\nGenerative world model for social reasoning\n1 (define gridworld (list\n2\n(list 'ames 'lawn 'lawn 'lawn 'sushi)\n3\n(list 'ames 'lawn 'lawn 'lawn 'danner)\n4\n(list 'office 'barlow 'barlow 'barlow 'danner)\n5\n(list 'ames 'lawn 'lawn 'lawn 'danner)\n6\n(list 'ames 'lawn 'lawn 'lawn 'vegetarian)\n7\n(list 'pizza 'carson 'carson 'carson 'danner)\n8 ))\n9 (define restaurants (list 'sushi 'pizza 'vegetarian))\n10\n11 (define initial_x 1)\n12 (define initial_y 3)\n13\n14\n15 (define has_bike (mem (lambda (agent-id) (flip))))\n16 (define available_motions (mem (lambda (agent-id) (if (has_bike agent-id) (list 'is_walking 'is_biking)\n(list 'is_walking)))))\n17 (define directions (list 'west 'east 'north 'south))\n18 (define available_actions (mem (lambda (agent-id) (cons (pair 'stay 'stay) (cartesian_product\n(available_motions agent-id) directions)))))\n19\n20 (define is_open (mem (lambda (restaurant_type) (flip))))\n21 (define POSITIVE_UTILITY_MEAN 10)\n22 (define NEGATIVE_UTILITY_MEAN -10)\n23 (define UTILITY_VARIANCE 1)\n24 (define restaurant_utility (mem (lambda (agent-id restaurant_type)\n25\n(uniform-draw\n87\nA.4\nSocial reasoning\nA\nLANGUAGE AND WORLD MODELS\n26\n(list\n27\n(gaussian POSITIVE_UTILITY_MEAN UTILITY_VARIANCE)\n28\n(gaussian NEGATIVE_UTILITY_MEAN UTILITY_VARIANCE)\n29 )))))\n30\n31 (define motion_utility (mem (lambda (agent-id location_type motion_type)\n32\n(case location_type\n33\n(('lawn) (case motion_type\n34\n(('is_biking) -1)\n35\n(('is_walking) -0.2)\n36\n(('is_staying) 0)\n37\n(else 0))\n38\n)\n39\n(else (case motion_type\n40\n(('is_biking) -0.01)\n41\n(('is_walking) -0.2)\n42\n(('is_staying) 0)\n43\n(else 0)))\n44 ))))\n45\n46 (define food_utility (mem (lambda (agent-id location_type)\n47\n(case location_type\n48\n(('lawn) 0)\n49\n(('ames) 0)\n50\n(('barlow) 0)\n51\n(('carson) 0)\n52\n(('danner) 0)\n53\n(('office) 0)\n54\n(else\n55\n(if (is_open location_type) (restaurant_utility agent-id location_type) NEGATIVE_UTILITY_MEAN))\n56 ))))\n57\n58 (define utility_function (mem (lambda (agent-id gridworld state_x state_y action)\n59\n(let ((location_type (get_gridworld_at gridworld state_x state_y)))\n60\n(let ((motion_type (car action)))\n61\n(let ((state_food_utility (food_utility agent-id location_type)))\n62\n(let ((state_motion_utility (motion_utility agent-id location_type motion_type)))\n63\n(+ state_food_utility state_motion_utility))))))))\n64\n65 (define get_gridworld_at (lambda (gridworld x y)\n66\n(list-elt (list-elt gridworld y) x)\n67 ))\n68 (define x_increment (lambda (direction)\n69\n(case direction\n70\n(('west) -1)\n71\n(('east) 1)\n72\n(('north) 0)\n73\n(('south) 0)\n74\n(('stay) 0)\n75 )))\n76 (define y_increment (lambda (direction)\n77\n(case direction\n78\n(('north) -1)\n79\n(('south) 1)\n80\n(('west) 0)\n81\n(('east) 0)\n82\n(('stay) 0)\n83 )))\n84 (define gridworld_max_x (lambda (gridworld) (length (list-elt gridworld 1))))\n88\nA.4\nSocial reasoning\nA\nLANGUAGE AND WORLD MODELS\n85 (define gridworld_max_y (lambda (gridworld) (length gridworld)))\n86 (define gridworld_transition (lambda (gridworld current_x current_y action)\n87\n(let ((direction (cdr action)))\n88\n(let ((next_x (if (>= current_x (gridworld_max_x gridworld)) current_x (+ (x_increment direction)\ncurrent_x))))\n89\n(let ((next_x (if (< next_x 1) current_x next_x)))\n90\n(let ((next_y (if (>= current_y (gridworld_max_y gridworld)) current_y (+ (y_increment direction)\ncurrent_y))))\n91\n(let ((next_y (if (< next_y 1) current_y next_y)))\n92\n(let ((next_state (get_gridworld_at gridworld next_x next_y)))\n93\n(list next_state next_x next_y)\n94 ))))))))\n95\n96 (define value_function (mem (lambda (agent-id curr_iteration gridworld state_x state_y)\n97\n(if (equal? curr_iteration -1) 0\n98\n(let ((prev_optimal_action_value (optimal_action_value agent-id (- curr_iteration 1) gridworld\nstate_x state_y)))\n99\n(cdr prev_optimal_action_value))\n100 ))))\n101\n102 (define available_actions_to_values (mem (lambda (agent-id curr_iteration gridworld state_x state_y)\n103\n(map (lambda (action)\n104\n(let ((utility (utility_function agent-id gridworld state_x state_y action)))\n105\n(let ((next_state (gridworld_transition gridworld state_x state_y action)))\n106\n(let ((next_state_x (second next_state)))\n107\n(let ((next_state_y (third next_state)))\n108\n(let ((next_state_value (value_function agent-id curr_iteration gridworld next_state_x\nnext_state_y)))\n109\n(pair action (+ utility next_state_value))\n110\n))))))\n111\n(available_actions agent-id))\n112 )))\n113\n114 (define optimal_action_value (mem (lambda (agent-id curr_iteration gridworld state_x state_y)\n115\n(let ((actions_to_values (available_actions_to_values agent-id curr_iteration gridworld state_x\nstate_y)))\n116\n(max_cdr actions_to_values)\n117\n)\n118 )))\n119\n120 (define MAX_ITERATIONS 20)\n121 (define should_terminate (mem (lambda (agent-id gridworld state_x state_y)\n122\n(if (<= (value_function agent-id MAX_ITERATIONS gridworld initial_x initial_y) 0) true\n123\n(let ((location_type (get_gridworld_at gridworld state_x state_y)))\n124\n(let ((state_food_utility (food_utility agent-id location_type)))\n125\n(> state_food_utility 0)))))))\n126\n127\n128\n129 (define optimal_policy_from_initial_state (mem (lambda (agent-id gridworld state_x state_y)\n130\n(if (should_terminate agent-id gridworld state_x state_y) ()\n131\n(let ((curr_optimal_action_value (optimal_action_value agent-id MAX_ITERATIONS gridworld state_x\nstate_y)))\n132\n(let ((curr_optimal_action (car curr_optimal_action_value)))\n133\n(let ((next_state (gridworld_transition gridworld state_x state_y curr_optimal_action)))\n134\n(let ((next_state_x (second next_state)))\n135\n(let ((next_state_y (third next_state)))\n136\n(let ((remaining_policy (optimal_policy_from_initial_state agent-id gridworld next_state_x\nnext_state_y)))\n89\nA.4\nSocial reasoning\nA\nLANGUAGE AND WORLD MODELS\n137\n(cons curr_optimal_action remaining_policy)\n138 ))))))))))\n139\n140 (define trajectory_from_initial_state (mem (lambda (agent-id gridworld state_x state_y)\n141\n(if (should_terminate agent-id gridworld state_x state_y) ()\n142\n(let ((curr_optimal_action_value (optimal_action_value agent-id MAX_ITERATIONS gridworld state_x\nstate_y)))\n143\n(let ((curr_optimal_action (car curr_optimal_action_value)))\n144\n(let ((next_state (gridworld_transition gridworld state_x state_y curr_optimal_action)))\n145\n(let ((next_state_location (first next_state)))\n146\n(let ((next_state_x (second next_state)))\n147\n(let ((next_state_y (third next_state)))\n148\n(let ((remaining_trajectory (trajectory_from_initial_state agent-id gridworld next_state_x\nnext_state_y)))\n149\n(cons next_state_location remaining_trajectory))\n150 ))))))))))\n151\n152 (define optimal_policy (mem (lambda (agent-id gridworld initial_state_x initial_state_y)\n153\n(cons (pair 'start 'start) (optimal_policy_from_initial_state agent-id gridworld\ninitial_state_x initial_state_y)))))\n154\n155 (define optimal_trajectory (mem (lambda (agent-id gridworld initial_state_x initial_state_y)\n156\n(cons (get_gridworld_at gridworld initial_state_x initial_state_y)\n(trajectory_from_initial_state agent-id gridworld initial_state_x initial_state_y))\n157 )))\n158\n159 (define optimal_policy_with_trajectory (mem (lambda (agent-id gridworld initial_state_x initial_state_y)\n160\n(zip (optimal_policy agent-id gridworld initial_state_x initial_state_y) (optimal_trajectory\nagent-id gridworld initial_state_x initial_state_y))\n161 )))\n162\n163 (define get_terminal_goal_state (mem (lambda (agent-id gridworld initial_state_x initial_state_y)\n164\n(last (optimal_trajectory agent-id gridworld initial_state_x initial_state_y)))))\n165\n166 (define trajectory_has_location_type? (mem (lambda (agent-id location_type gridworld initial_state_x\ninitial_state_y)\n167\n(member? location_type (optimal_trajectory agent-id gridworld initial_state_x initial_state_y))\n168 )))\n169 (define policy_has_motion_type? (mem (lambda (agent-id motion_type gridworld initial_state_x\ninitial_state_y)\n170\n(let ((policy_motions (map (lambda (action) (first action)) (optimal_policy agent-id gridworld\ninitial_state_x initial_state_y))))\n171\n(member? motion_type policy_motions)\n172 ))))\n173 (define policy_and_trajectory_has_motion_at_location? (mem (lambda (agent-id motion_type location_type\ngridworld initial_state_x initial_state_y)\n174\n(let ((policy_motions (map (lambda (action) (first action)) (optimal_policy agent-id gridworld\ninitial_state_x initial_state_y))))\n175\n(let ((trajectory (optimal_trajectory agent-id gridworld initial_state_x initial_state_y)))\n176\n(let ((motions_at_locations (zip policy_motions trajectory)))\n177\n(member? (list motion_type location_type) motions_at_locations)\n178 ))))))\n179\n180 (define motion_at_location? (mem (lambda (agent-id motion_type location_type gridworld initial_state_x\ninitial_state_y)\n181\n(let ((policy_motions (map (lambda (action) (first action)) (optimal_policy agent-id gridworld\ninitial_state_x initial_state_y))))\n182\n(let ((trajectory (optimal_trajectory agent-id gridworld initial_state_x initial_state_y)))\n183\n(let ((motions_at_locations (zip policy_motions trajectory)))\n90\nA.4\nSocial reasoning\nA\nLANGUAGE AND WORLD MODELS\n184\nmotions_at_locations\n185 ))))))\n186 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n187 ;; Derived predicates.\n188 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n189 (define action_id_gensym (make_gensym \"action-\"))\n190 (define is_going_to_actions (mem (lambda (agent-id)\n191\n(let ((action_states (optimal_policy_with_trajectory agent-id gridworld initial_x initial_y)))\n192\n(let ((final_location (last (last action_states))))\n193\n(list (list\n194\n(pair 'action_id\n(action_id_gensym))\n195\n(pair 'action_subject agent-id)\n196\n(pair 'action_predicates (list 'is_going (list 'to final_location)))\n197\n(pair 'action_preposition 'to)\n198\n(pair 'action_location final_location)\n199\n)))))))\n200\n201 (define is_going_on_actions (mem (lambda (agent-id)\n202\n(let ((action_states (optimal_policy_with_trajectory agent-id gridworld initial_x initial_y)))\n203\n(fold (lambda (action_state these_actions)\n204\n(let ((action_location (last action_state)))\n205\n(let ((action_manner (first (first action_state))))\n206\n(let ((action_direction (cdr (first action_state))))\n207\n(cons\n208\n(list\n209\n(pair 'action_id\n(action_id_gensym))\n210\n(pair 'action_subject agent-id)\n211\n(pair 'action_predicates (list 'is_going action_manner action_direction (list 'on\naction_location)))\n212\n(pair 'action_preposition 'on)\n213\n(pair 'action_location action_location)\n214\n)\n215\nthese_actions)\n216\n))))\n217\n() action_states)\n218 ))))\n219\n220 (define actions_in_scene (mem (lambda (agent-id) (concatenate (is_going_to_actions agent-id)\n(is_going_on_actions agent-id)))))\n221 (define is_action? (lambda (action action_predicate) (member? action_predicate (lookup action\n'action_predicates))))\n222 (define is_subject_of_action? (lambda (action entity) (eq?\n223\n(lookup action 'action_subject)\n224\nentity\n225\n)))\n226\n227 (define is_preposition_of_action? (lambda (action preposition) (eq?\n228\n(lookup action 'action_preposition)\n229\npreposition\n230\n)))\n231 (define is_location_of_action? (lambda (action location) (eq?\n232\n(lookup action 'action_location)\n233\nlocation\n234\n)))\n235\n236 (define get_location (lambda (action)\n237\n(lookup action 'action_location)\n238\n))\n239\n91\nA.4\nSocial reasoning\nA\nLANGUAGE AND WORLD MODELS\n240 (define (exists_action agent-id predicate)\n241\n(some (map predicate (actions_in_scene agent-id))))\n242\n243 (define (get_actions agent-id predicate)\n244\n(fold (lambda (action these_actions) (if (predicate action) (cons action these_actions)\nthese_actions))\n245\n() (actions_in_scene agent-id))\n246 )\nCode Block 11: Generative domain theory for restaurant navigation domain. Generates agents with varying\npreferences in a gridworld environment. Also implements a value iteration-based planner directly in the\nChurch code.\nA.4.2\nTranslation examples for social reasoning domain\n1 ;; Bob likes pizza.\n2 (condition (> (restaurant_utility 'bob 'pizza) 0))\n3 ;; Bob really likes pizza.\n4 (condition (> (restaurant_utility 'bob 'pizza) 10))\n5 ;; Bob does not like pizza, and he actually despises vegetables.\n6 (condition (and\n7\n(< (restaurant_utility 'bob 'pizza) 0)\n8\n(< (restaurant_utility 'bob 'vegetarian) 10)\n9 ))\n10 ;; The pizza place is not open.\n11 (condition (not (is_open 'pizza)))\n12 ;; Condition on: Bob walked North on Danner.\n13 (condition (exists_action 'bob (lambda (action)\n14\n(and\n15\n(is_subject_of_action? action 'bob)\n16\n(is_action? action 'is_walking)\n17\n(is_action? action 'north)\n18\n(is_preposition_of_action? action 'on)\n19\n(is_location_of_action? action 'danner)))))\n20 ;; Does Bob like vegetarian food?\n21 (query (> (restaurant_utility 'bob 'vegetarian) 0))\n22 ;; Where is Bob going?\n23 (query (get_actions 'bob (lambda (action)\n24\n(and\n25\n(is_subject_of_action? action 'bob)\n26\n(is_action? action 'is_going)))))\n27 ;; Where will Bob go to for lunch?\n28 (query (get_location (first\n29\n(get_actions 'bob (lambda (action)\n30\n(and (and\n31\n(is_subject_of_action? action 'bob)\n32\n(is_action? action 'is_going))\n33\n(is_preposition_of_action? action 'to))\n34\n)))))\nCode Block 12: Translation examples for the social reasoning. These examples are concatenated with the\nsocial reasoning scenes generative model to produce the prompt used to generate new translations.\n92\nB\nOPEN QUESTIONS\nB\nOpen questions\nB.1\nSyntactic bootstrapping\nBootstrapped language-to-code translations for novel words\n(condition (exists_object (lambda (object) (is_shape? 'dax) object)))\nThere is a dax.\n(condition (exists_object (lambda (object)\n(and\n((is_color? 'pelgy) object)\n((is_shape? 'dax) object)))))\nThere is a pelgy dax.\n(condition (exists_object (lambda (object)\n(exists_event (lambda (event)\n(and (and (and\n((is_color? 'pelgy) object)\n((is_shape? 'dax) object))\n(is_subject_of_event? event object))\n(is_event_base? 'is_gorping event))))))))\nA pelgy dax is gorping.\n(condition (exists_object (lambda (object) \n(exists_event (lambda (event)\n(and (and (and\n((is_shape? 'wug) object)\n(is_subject_of_event? event object))\n(is_event_base? 'is_gorping event)\n(is_event_modifier? 'is_feppily event)))))))\nA wug gorps feppily.\n(condition\n(exists_object (lambda (object_1)\n(exists_object (lambda (object_2) \n(exists_event (lambda (event)\n(and (and (and (and (and (and (and\n((is_shape? 'wog) object_1))\n((is_shape? 'foog) object_2))\n(is_subject_of_event? event object_1))\n(is_object_of_event? event object_2))\n(is_event_base? 'is_blicking event)\n(caused_event? event object_1))))))))))\nA wog blicks a foog.\n(condition\n(and\n(exists_object (lambda (object_1)\n(exists_event (lambda (event_1)\n(and (and\n((is_shape? 'wog) object_1)\n(is_subject_of_event? event_1 object_1))\n(is_event_base? 'is_zeeming event_1))))))\n(exists_object (lambda (object_2)\n(exists_event (lambda (event_2)\n(and (and\n((is_shape? 'foog) object_2)\n(is_subject_of_event? event_2 object_2))\n(is_event_base? 'is_zeeming event_2))))))))\nA wog and a foog are zeeming.\nFigure 14: Example translations with novel words, suggesting that language-to-code models can leverage\nsyntax-semantic mappings to inform hypothesized meanings.\nB.2\nCode editing\nWhile our framework focuses primarily on generating code in the PLoT, this view encompasses only part of\nthe broader story of natural language. In particular, in certain contexts, it might not make sense to write\nnew code, but instead to modify the existing domain theory. Consider the following statements, taken in\ncontext of the domains we explored in Section 3:\n\u2022 (Tug-of-war) The team\u2019s strength is the strength of the strongest player.\n\u2022 (Kinship) Avery has two kids from a previous marriage.\n\u2022 (Visual scenes) There\u2019s a red mug stacked on top of a yellow can.\n\u2022 (Navigation) There\u2019s a river separating the North and South sides of town, which you can paddle\nacross in nice weather.\nThese utterances bend or break the rules of their respective domain theories. To properly integrate these\nkinds of language, we\u2019d need to edit pieces of the existing generative models.\nWhile language-guided code editing is still an open area of research, recent advances offer an exciting\nglimpse of what might be possible in the near-term. Ouyang et al. (2022) use a combination of finetuning and\nreinforcement learning to make GPT-3 adhere more closely to human-authored instructions. The resulting\nInstructGPT models, which OpenAI make available on their API, are capable of editing existing text based\non short natural language instructions (e.g., \u201cFix the grammar\u201d; \u201cTurn this into a poem.\u201d).9 Excitingly, this\nsame approach extends to code-based LLMs, meaning that it is possible to prompt GPT models to edit a\npiece of code according to some instructions. Indeed, we can use OpenAI\u2019s editing interface off-the-shelf to\nhandle utterances requiring localized changes to the domain model (see below for a simple example in the\ntug-of-war domain).\n9https://openai.com/blog/gpt-3-edit-insert/\n93\nC\nATTRIBUTIONS\nRedefine: The team\u2019s strength is the strength of the strongest player.\n1 ;; The team's strength is the sum of the\nplayers' strengths.\n2 ;; When a player is lazy in a match, they pull\nwith half their strength.\n3 (define (team-strength team)\n4\n(sum\n5\n(map\n6\n(lambda (player)\n7\n(if (flip (laziness player))\n8\n(/ (strength player) 2)\n9\n(strength player)))\n10\nteam)))\n1 ;; The team's strength is the strength of the\nstrongest player.\n2 ;; When a player is lazy in a match, they pull\nwith half their strength.\n3 (define (team-strength team)\n4\n(apply\n5\nmax\n6\n(lambda (player)\n7\n(if (flip (laziness player))\n8\n(/ (strength player) 2)\n9\n(strength player)))\n10\nteam)))\nThough questions of scaling and robustness remain, the problem of modeling sequences of code changes is\ncurrently gaining traction in the machine learning for code community, which has recently produced multiple\nlanguage-guided neural models of code editing (Chakraborty, Ding, Allamanis, & Ray, 2022; Chakraborty &\nRay, 2021; Fried et al., 2022; Panthaplackel, Nie, Gligoric, Li, & Mooney, 2020; Reid & Neubig, 2022; J. Zhang,\nPanthaplackel, Nie, Li, & Gligori\u0107, 2022) that draw broadly on contemporary work in automated program\nrepair (Bai et al., 2021; Y. Li, Wang, & Nguyen, 2020; Yasunaga & Liang, 2020). These advances suggest a\nbroader vision for our framework in which domain theories, expressed in the PLoT, can be iteratively grown\nand revised to reflect natural language instruction. Moreover, as code LLMs become more general-purpose,\nthe technical gap between generation and editing will continue to narrow, suggesting a point in the near\nfuture where defining new components of a domain theory will be a special case of language-guided code\nediting.\nC\nAttributions\nC.1\nAttribution of graphics resources\nArtificial neural network icon by sachin modgekar from thenounproject.com.\nCog icon by Rflor from thenounproject.com.\n94\n"
  },
  {
    "title": "Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference",
    "link": "https://arxiv.org/pdf/2306.12509.pdf",
    "upvote": "14",
    "text": "Joint Prompt Optimization of Stacked LLMs\nusing Variational Inference\nAlessandro Sordoniab\u2217 Xingdi Yuana Marc-Alexandre C\u00f4t\u00e9a Matheus Pereiraa\nAdam Trischlera Ziang Xiaoa Arian Hosseinib Friederike Niedtnera Nicolas Le Rouxab\nMicrosoft Research Montr\u00e9ala\nMILAb\nAbstract\nLarge language models (LLMs) can be seen as atomic units of computation map-\nping sequences to a distribution over sequences. Thus, they can be seen as stochastic\nlanguage layers in a language network, where the learnable parameters are the\nnatural language prompts at each layer. By stacking two such layers and feeding the\noutput of one layer to the next, we obtain a Deep Language Network (DLN). We\nfirst show how to effectively perform prompt optimization for a 1-Layer language\nnetwork (DLN-1). Then, we present an extension that applies to 2-layer DLNs\n(DLN-2), where two prompts must be learned. The key idea is to consider the\noutput of the first layer as a latent variable, which requires inference, and prompts\nto be learned as the parameters of the generative distribution. We first test the\neffectiveness of DLN-1 in multiple reasoning and natural language understanding\ntasks. Then, we show that DLN-2 can reach higher performance than a single layer,\nshowing promise that we might reach comparable performance to GPT-4, even\nwhen each LLM in the network is smaller and less powerful. The DLN code is\nopen source.1\n1\nIntroduction\nThe size of large language models (LLMs) has grown significantly over the last few years, mainly\nbecause of emerging capabilities [5, 31], but at considerable technical and societal costs [49, 2, 4].\nRecent efforts have focused either on learning smaller models matching the abilities of larger ones on\nsome tasks using distillation [43, 36, 29, 13], or offloading part of the computation to other dedicated\ncomponents [28, 22, 25, 18]. In the latter case, this is done through carefully crafted instructions to\nretrieve the necessary information from these additional modules [48, 41, 6, 54, 24].\nInstruction-tuned LLMs map an input sequence to a distribution over output sequences conditioned\non an instruction, or prompt. In this paper, we view such LLMs as stochastic language layers, whose\nlearnable parameters are the prompts. Multiple layers can be stacked to form a Deep Language\nNetwork (DLN) whose learnable parameters are the prompts associated to each layer. Specifically,\neach layer uses a template to organize both its prompt and the inputs coming from the layer below\ninto a single sequence before producing the output (see Figure 1). This layering induces a learnable\ndecomposition of the task into a series of smaller sub-tasks, each of which might be more easily\nsolvable by an LLM. This view shares similarities to recent works that chain LLM calls [41, 6, 54].\nIn this work, we move towards integrating learnable components in the pipeline: each prompt can be\nlearned to maximize the final objective.\n\u2217Corresponding author: alsordon@microsoft.com\n1Code: https://github.com/microsoft/deep-language-networks.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.12509v2  [cs.CL]  4 Dec 2023\nfrozen\nLM\n{{ prompt }}\n{{ input }}\nAnswer:\n\u2026 For example, if the \nsentence is \u201csupported\u201d, \nchoose \u201cpositive\u201d, and if \nthe sentence is \u201cderail\u201d, \nchoose \u201cnegative\u201d. \nSimilarly, if the sentence is \n\"peace and stability and \nprosperity\", choose \n\"positive\". Note that \nwords like \"arti\ufb01cial\" tend \nto have a negative \nsentiment.\n         prompt\n\ud83d\udd25\na decade of dramatic \neconomic decline\ninput x\nnegative\nLM\n{{ input }}\n{{ prompt }} \nLet\u2019s think step \nby step.\nDecompose the problem \nby breaking it down into \nsteps. First, \ufb01nd the \ncurrent date from the \ngiven context. Then, \ncalculate the number of \ndays or months to add or \nsubtract from the current \ndate to get the desired \nresult. Additionally,\u2026\n        prompt\n\ud83d\udd25\nThe deadline is Jun 1, 2021, which is 2 \ndays away from now. What is the date \none year ago from today in \nMM/DD/YYYY? \\n Options\\n(A) \n05/30/2020 \\n (B) 06/02/2020...\ninput x\n{{ prompt }}\n{{ input }}\nYour thoughts were:\n{{ hidden }}\nAnswer:\nThe current date is May 30, 2021. One year \nago from today is May 30, 2020\u2026\nhidden h\nclassi\ufb01cation\ntemplate\nLM\n(A)\n\u2026 Additionally, consider \nthe fact that in some \ncontexts the day may \ncome before the month \n(e.g. in the UK). For \nexample, if the given \ncontext is \u201cIt is 4/19/1969 \ntoday. What is the date a \nmonth ago in \nMM/DD/YYYY?\u201d, you can \ninfer that\u2026\n        prompt\n\ud83d\udd25\n\ud83d\udd25 trainable\n \n \nhidden\ntemplate\nclass.\ntemplate\nFigure 1: Left: An illustration of a DLN-1 performing a sentiment analysis task: input and the\ntrainable prompt are merged using a template and fed to the LM for answer generation. Right: a\nDLN-2 with a residual connection, performing the date understanding task: two prompts need to be\nlearned. In this example, the hidden template extends Chain-Of-Thought [48] with a learnable prefix;\nwe consider the output of the first layer, hidden, as a latent variable h. We use variational inference\nto learn \u03c00, \u03c01. Templates can be considered as an hyperparameter of the network.\nWe first show how to perform prompt optimization in a shallow 1-layer language network (DLN-1)\nwhich parametrizes a distribution pLM(y|x, \u03c0), where x and y are string input and output respectively,\nand \u03c0 is the learnable prompt (Figure 1, left). Our prompt optimization techniques extend the\nAutomatic Prompt Engineer (APE) procedure from Zhou et al. [57]. We show how our prompts\ncan include a verbalization of difficult examples from the task: the final prompts are a combination\nof instruction directives, akin to zero-shot learning [17], and task examples, akin to in-context\nlearning [20]. This significantly improves downstream performance, surpassing APE on several tasks.\nThen, we show how to train a 2-layer DLN (DLN-2), which parametrizes a probability distribution:\npDLN-2(y|x) =\nX\nh\npLM(y|h, x, \u03c01) pLM(h|x, \u03c00) ,\nwhere h is the string output of the first LLM layer (Figure 1, right). We consider h as a latent variable:\nto maximize the marginal log-likelihood, we formulate a variational inference algorithm that uses an\napproximate posterior over h. Note that this formalism easily encompasses more than two layers.\nConsidering outputs of the hidden language layers as latent variables allows us to encompass various\nestablished prompting methods, such as Chain-Of-Thought (CoT) [48] and self-consistency (SC-\nCoT) [46]. Particularly, CoT can be seen as a particular DLN-2 with the first layer prompt set to\n\u201cLet\u2019s think step by step\u201d and the second layer prompt set to \u201cThe answer is\u201d; we can\neither learn such prompts or learn a supplement to those as in Figure 1. SC-CoT can be seen as\nmarginalizing over CoT strings sampled from a task-agnostic prior: when using the template in\nFigure 1 (right), our method generalizes this perspective by learning a task-specific prior distribution\nover successful CoTs.\nThe rest of the paper is organized as follows. First, we provide an interpretation of LLMs as shallow\nnetworks, drawing a number of analogies with standard parametric and non-parametric models and\nexplaining how best to train them. After exploring their limitations, we propose to stack two such\n2\nLLMs to form a DLN-2. We show how they can be trained using a form of variational inference, then\ndemonstrate their performance on a series of reasoning and language understanding tasks.\n2\nOne-Layer Language Networks\nA pre-trained LLM with frozen weights might be thought of as a complete function class indexed by\nprompts. The output y of an LLM for an input x can be modulated through a prompt \u03c0 by feeding a\ncombination of \u03c0 and x to the LLM. Hence, from a low-level perspective, the function class of an\nLLM is defined by its architecture, i.e., its depth, number of heads, context size, etc., and training\nhappens at the parameter level. It is data and compute intensive and should be done rarely. From\na high-level perspective, the function class of an LLM is defined by the pre-trained model chosen\n(LLAMA [44], text-davinci-003, GPT-4, etc.), and training happens by fine-tuning the model or by\nchoosing the prompt.\nThere are two ways of optimizing an LLM at the prompt level. The first one is prompt engineering,\na parametric optimization, where the optimization space is independent of the size of the dataset.\nBecause this optimization usually happens in discrete space, gradient-based techniques do not apply\nand most efforts rely on a combination of random or local search and human heuristics [21, 55].\nThe second one is in-context learning (ICL), a non-parametric optimization technique where the\nsolution is a direct function of a subset of examples [5, 20]. This approach works well for few-shot\nlearning but scaling it to larger datasets has both performance and computational issues. We shall\nnow generalize previous work in discrete prompt optimization [57, 21] with the ultimate goal of\nlearning a set of prompts in a language network.\n2.1\nLanguage Layers\nWe use language layer to refer to a (stochastic) computation that takes as input a string x and\noutputs a string y. This computation is modulated by another string, \u03c0, generally called a prompt\nor instruction [57]. The string transduction is performed by an operator LM, by feeding x and \u03c0 as\ncontext and generating a continuation y. Templates describe the way x and \u03c0 are combined prior to\nbeing fed to the LM operator. These are functions that accept strings as variables and output a string.\nWe will denote such templates with this font T. A simple forward template F is the concatenation,\ni.e. F(x, \u03c0) = \u201c{\u03c0}{x}\u201d. We also explore more complex ones, examples of which can be seen in\nFigure 1.\nGiven an input x, a prompt \u03c0, and a template F, a language layer defines a probability distribution\npLM(y|F(x, \u03c0)) over output strings y as computed by the LM. In the next section, we describe a generic\nframework for optimizing the weights \u03c0 for a language layer.\n2.2\nPrompt Optimization: Improved APE\nBecause the search for the best prompt happens over a discrete space, we will rely on a local search\nmethod, using an LLM to implement a distance measure between prompts. The procedure can be\nseen as an extension of Automatic Prompt Engineer (APE), recently proposed by Zhou et al. [57], and\nwill serve as a stepping stone towards introducing our algorithm for training deep language networks.\nOur prompt optimization algorithm can be structured as follows:\n1. Given the current prompt \u03c0 and a current batch of examples {x, y}, generate N \u201clocal\u201d\ncandidates \u03c01, . . . , \u03c0N using a prompt proposal distribution;\n2. Score each candidate using a (potentially stochastic) scoring function s, then choose \u03c0 =\narg max\u03c0n s(\u03c0n).\nPrompt Proposal Local search algorithms assume a distance measure between inputs to crawl the\nsearch space. In this setting, we rely on LLMs to generate local modifications to the prompts. Our\nprompt proposal distribution takes as conditioning information i) the batch given as input to the\nlayer, ii) its corresponding output {x, y, \u02c6y}, and iii) the current prompt \u03c0. The proposal distribution\npLM(\u03c0n|B\u03c0({x, y, \u02c6y}, \u03c0)) wraps this information using a particular \u201cbackward\u201d template B\u03c0, which\ncan be found in Appendix D. This approach is similar to the instruction template used by Zhang et al.\n[55], with the exception that we also integrate information about the model\u2019s own predictions, which\n3\nAlgorithm 1 One-Layer Language Network (DLN-1) Training Algorithm\nRequire: \u02c6y \u223c pt\nLM(y|c)\n\u25b7 generates a completion of prefix c with temperature t\nRequire: log pLM(h|c)\n\u25b7 return log-prob of h following c\nRequire: N: prompt samples, I: iterations, D: dataset\nRequire: F: template for the inference/forward pass\nRequire: B\u03c0: template for prompt proposal/backward pass.\n1: Initialize \u03c0 with a task description or empty\n2: for i in [1, I] do\n3:\nx, y \u223c D\n\u25b7 Sample minibatch\n4:\n\u02c6y \u2190 p0\nLM(y|F(x, \u03c0))\n\u25b7 Do inference pass\n5:\n\u03c01, . . . , \u03c0N \u223c p0.7\nLM (\u03c0|B\u03c0({x, y, \u02c6y}, \u03c0))\n\u25b7 Sample N candidate prompts\n6:\ns1, . . . , sN \u2190 log pLM(y|F(x, \u03c0n))\n\u25b7 Score all prompts\n7:\n\u03c0 \u2190 arg max\u03c0n{s1, . . . , sN}\n\u25b7 Select prompt with best score\n8: end for\nwe found to empirically help performance given that the model tends to propose prompts that correct\nits own errors. We sample from the prompt proposal distribution to generate a set of N prompts. A\nparticularly important aspect is ensuring the diversity of the candidate pool \u03c01, . . . , \u03c0N. We devise\nseveral strategies to improve the diversity and the usefulness of the candidate samples in Section 4.\nPrompt Selection Once a set of N prompts has been generated, we use a scoring function to select\nthe updated prompt. We assume access to the log-likelihoods of the LM operator and we rank the\ncandidate prompts to maximize data log-likelihood \u03c0 = arg max\u03c0n log pLM(y|F(x; \u03c0n)). In practice,\nwe normalize this log-probability by the length of the output string. While we focus on that metric in\nthis work, there is no restriction on the scoring function that can be used. We use backtracking to\nincrease the robustness of our selection mechanism, as well as a memory of well-performing prompts\nfor efficiency. We present both strategies in Section 4. The sketch of a 1-layer prompt optimization\nalgorithm is described in Algorithm 1, ignoring backtracking and memory for simplicity.\nThe results of our prompt optimization may be found in Table 1 and will be discussed in detail in\nSection 5.2. We now turn to extending prompt optimization to architectures with two layers.\n3\nTwo-Layer Deep Language Networks (DLN-2)\nThe natural extension of DLN-1 is DLN-2, in which language layers are stacked, i.e. the output of\nthe first language layer is the input to the second one. A 2-layer network induces a distribution over\noutputs of the form:\npDLN-2(y|x) =\nX\nh\npLM(y|Fr(h, x, \u03c01))pLM(h|F(x, \u03c00))\n(1)\nwhere h is a latent variable that potentially makes it easier to explain the target y. The output layer\nis also conditioned on x through Fr, forming a residual connection (Figure 1). This formulation is\nreminiscent of past work using latent language representations to guide document summarization [26].\nIn our case, however, the encoding/decoding distributions are parameterized by natural language\nprompts \u03a0 = {\u03c00, \u03c01}, and we do not assume access to the LLM parameters.\nWhile this architecture has more expressive power than a shallow language network, the prompt\noptimization problem becomes harder now that we have to jointly search over both \u03c00 and \u03c01. Doing\nrandom search in this space is impractical [8] and manual tuning of the weights is also exponentially\nharder than with a single prompt. We turn to variational inference to address this issue.\n3.1\nVariational Inference Objective\nThe layerwise decomposition of our system allows us to leverage tools from approximate inference\nin probabilistic models to learn \u03a0. In particular, we propose to use variational inference to learn\n\u03a0 [3, 16]. We posit an approximate posterior q(h) over the latent variable h, and bound the marginal\n4\nlog-likelihood of y given x by computing the ELBO:\nlog pDLN-2(y|x) \u2265\nX\nh\nq(h) [log pLM(y|Fr(h, x, \u03c01))pLM(h|F(x, \u03c00))] + H [q(h)] ,\n(2)\nwhich allows us to decompose the optimization over \u03a0 in two independent optimization problems,\nover both \u03c00 and \u03c01:\n\u03c0\u2217\n0 = arg max\n\u03c00\nX\nx, h\nwh log p(h|F(x, \u03c00)), \u03c0\u2217\n1 = arg max\n\u03c01\nX\n(x,y), h\nwh log p(y|F(h, x, \u03c01)) .\n(3)\nThe search over \u03c01 is identical to the prompt optimization described in Section 2, with the difference\nthat the inputs now depend on the approximate posterior samples h in addition to the inputs x. The\nsearch over \u03c00 uses a similar strategy but uses the posterior samples h as targets, instead of y.\nAlthough this bound allows us to decompose the optimization w.r.t. \u03a0, it is only useful if it is close\nto the true value. Since its looseness is the KL divergence between q(h) and the true posterior,\nKL(q(h)||p(h|y, x)): we need to find an approximate posterior q closely matching the true posterior.\nIn what follows, we specify how we parametrize the approximate posterior and how we tighten the\napproximation via posterior sharpening.\nHidden Proposal We will also be using an LLM to sample candidate hidden states from q(h).\nUnless specified, for simplicity, we use the same LM operator used in our language layers. The\napproximate posterior can condition on arbitrary amount of information but especially useful might\nbe to condition on the true target y. If it conditions on the hidden state coming from the \u201cforward\npass\u201d, \u02c6h \u223c pLM(h|F(x, \u03c00)), then qedit(h) = pLM(h|Bh(\u02c6h, y, \u03c01)). Bh is a specifically tailored hidden\nproposal template (Appendix D). qedit performs a sort of edit operation, where the LM is tasked to\nrewrite the hidden variable \u02c6h given some extra knowledge of the ground-truth label y and of \u03c01.\nAlternatively, we can set the posterior to be equal to the prior, i.e. qpri(h) = pLM(h|F(x, \u03c00)), or the\nprior with additional information about the label y, qpri+(h) = pLM(h|By(x, \u03c00, y)) (Appendix D).\nThis amounts to re-computing the hidden state knowing privileged information about the label. We\nfound most effective to sample hidden states from a mixture of qpri and qpri+.\nPosterior Sharpening Given the absence of learnable parameters in q(h), the induced approximate\nposterior might still be far from the true posterior. To bridge this gap, we reweigh each sample\nhi based on its probability under the true posterior distribution. More precisely, we compute\n\u02dcwi = log pLM(y|Fr(hi, x, \u03c01)) + log pLM(hi|F(x, \u03c00)), then assign to each hi the probability wi =\nexp(\u03b1 \u02dcwi)/ P\nj exp(\u03b1 \u02dcwj), where \u03b1 is a tunable temperature parameter that controls the entropy of\nthe posterior weights. The full algorithm for training a DLN-2 is presented in Algorithm 2.\n4\nPractical Instantiation\nAlthough our method aim to learning prompts in stacked LLM architectures, we do rely on a good\namount of prompt engineering for our templates. Hereafter, we detail some choices that were\nfundamental to make our approach work in practice.\nProposal Diversity To ensure a diversity of the samples for both the prompt proposal distribution, we\nfound helpful to use two strategies. The first is to modify the backward templates B\u03c0 before drawing a\nsample from the proposal distribution pLM(\u03c0). To achieve so, we parametrize the basic templates with\na \u201c{message}\u201d variable that we instantiate from a pool of hand-written instructions, that describe\ndifferent behaviors the model should follow to propose new \u03c0, e.g. \u201cshorten the previous\ninstruction\u201d, \u201cgive useful examples\u201d, etc. These can be interpreted as meta-instructions, i.e.\nhigh-level directives that inform the model on how to create a better instruction for the task, and\nextend instruction-induction templates used in [12, 55]. These can be found in Appendix D. In the\nfuture, we could envision to extend learning to these instructions. In the case of B\u03c0, they could\nfunction as parameters for a prior over the weights of the DLN. The second strategy to ensure more\ndiversity is that we instantiate B\u03c0 with a different random subset of examples in the current batch,\nbefore drawing each sample \u03c0n. This effectively modifies the generation context for each sample \u03c0n.\nLearning In-Context Learning One strategy we found particularly effective is to integrate in the\npool of meta-instructions an additional instruction that asks the LM to give useful examples to improve\n5\nAlgorithm 2 Two-Layer Deep Language Network (DLN-2) Training Algorithm\n1: \u02c6y \u223c pt\nLM(c)\n\u25b7 generates a completion of prefix c with temperature t\n2: log pLM(h|c)\n\u25b7 return log-prob of h following c\n3: N: prompt samples, K: posterior samples, I: iterations, D: dataset\n4: F, Fr: templates for the inference/forward pass without and with residual connection\n5: B\u03c0, Bh: templates for prompt and hidden proposal/backward pass.\n6: Initialize \u03c00, \u03c01 with a generic sentence or task description\n7: for i in [1, I] do\n8:\nx, y \u223c D\n\u25b7 Sample minibatch\n9:\n\u02c6h \u2190 p0\nLM(F(x, \u03c00))\n\u25b7 Do inference pass, first layer\n10:\n\u02c6y \u2190 p0\nLM(Fr(\u02c6h, x, \u03c01))\n\u25b7 Do inference pass, second layer\n11:\nh1, . . . , hK \u223c p0.7\nLM (Bh(\u02c6h, x, y, \u03c01, \u03c00))\n\u25b7 Sample K posterior proposals for h\n12:\n\u03b11, . . . , \u03b1K \u2190 log pLM(hk|F(x, \u03c00))\n\u25b7 Compute prior log-probs for all hk samples\n13:\n\u03b21, . . . , \u03b2K \u2190 log pLM(y|Fr(hk, x, \u03c01))\n\u25b7 Compute log-likelihoods for all hk samples\n14:\nqk \u2190 exp(\u03b1k + \u03b2k)/(P\nk exp(\u03b1k + \u03b2k))\n\u25b7 Compute normalized posterior weights\n15:\nh\u2217 \u2190 arg maxh{q1, . . . , qK}\n\u25b7 Select best posterior sample\n16:\n\u03c01\n0, . . . , \u03c0N\n0 \u223c p0.7\nLM (B\u03c0({x, \u02c6h, h\u2217}, \u03c00))\n\u25b7 Sample N candidate prompts for \u03c00\n17:\n\u03c01\n1, . . . , \u03c0N\n1 \u223c p0.7\nLM (B\u03c0({\u02c6h, \u02c6y, y}, \u03c01))\n\u25b7 Sample N candidate prompts for \u03c01\n18:\ns1\n0, . . . , sN\n0 \u2190 P\nk qk log pLM(hk|F(x, \u03c0n\n0 ))\n\u25b7 Compute ELBO for all prompts \u03c0n\n0\n19:\ns1\n1, . . . , sN\n1 \u2190 P\nk qk log pLM(y|Fr(hk, x, \u03c0n\n1 ))\n\u25b7 Compute ELBO for all prompts \u03c0n\n1\n20:\n\u03c00 \u2190 arg max\u03c0n\n0 {s1\n0, . . . , sN\n0 }\n\u25b7 Select prompt \u03c00 with best score\n21:\n\u03c01 \u2190 arg max\u03c0n\n1 {s1\n1, . . . , sN\n1 }\n\u25b7 Select prompt \u03c01 with best score\n22: end for\nits current prompt \u03c0. Empirically, we observed that this allows the model to sample candidate\nprompts \u03c0n that contain synthetic examples for the task, embedded in natural language. Examples\nof this interesting behavior can be found in Appendix F. We found that this behavior is particularly\ninteresting as the resulting prompts often perform better than standard ICL. We hypothesize this is\ndue to both i) the \u201cverbalization\u201d of the example in the prompt, which modifies the dataset syntax\ninto a more suitable one, and ii) the fact that the model can dynamically select which examples are\nmost important to integrate in the prompts, given the errors made during training. Therefore, we\nsuspect that DLN achieves a similar effect to recent techniques that select important examples for\nICL [20, 35, 40, 18, 47] with the improvement of naturally conditioning the selection on the end task\nperformance via end-to-end training.\nBacktracking and Memory Optimization of both DLN-1 and DLN-2 is challenging due to the fact\nthat we do not have gradient information and we sample a restricted set of candidates \u03c0n at each\noptimization step due to computational reasons. We deploy multiple strategies to allow the network to\nbe robust to sampling/selection errors. First, we include the current prompt \u03c0 into the set of candidate\nprompts to be scored at the current iteration \u03c0n. This allows the model to not take the step if the\nprevious prompt performed better. Second, we keep a memory of M = 5 best prompts found by\ntracking validation set performance.\nExploration Reward When training a DLN-2, we empirically observed that the first layer prompt\n\u03c00 was updating very slowly. Due to the fact that the approximate posterior shares templates with\nthe prior used in the forward pass, the posterior samples hi are close to \u02c6h and the maximizer of\nEquation (3) remains \u03c00. To address this issue, we add to the scores of each candidate prompt\nan exploration reward that is proportional to the negative log-probability of those \u02c6h that led to an\nincorrect prediction: r = \u2212\u03bb log pLM(\u02c6h|F(x, \u03c0n)), if \u02c6y \u0338= y. This encourages the model to both find\nprompts that maximize the log-probability of high-probability posterior samples and at the same\ntime minimize the log-probability of prior samples that led to incorrect predictions. We anneal \u03bb\nto 0 during training with a constant schedule and we select the initial \u03bb by monitoring validation\nperformance for each task.\n6\n5\nExperiments and Results\nWe design and conduct a set of experiments to help answer two main research questions:\n\u2022 Q1: Can we outperform APE and In-Context Learning (ICL) with a DLN-1?\n\u2022 Q2: Does network depth provide further improvement upon DLN-1?\n5.1\nExperimental Setup\nDatasets and Tasks We adopt a set of nine NLP and reasoning tasks commonly used in prior\nwork studying zero- or few-shot learning capabilities of LLMs [23, 10, 39, 42, 1]. We focus on\nclassification tasks. For tasks adopted from BigBench-Hard (BBH) [42] (Hyper., Nav., Date. and\nLogic.72), we use the 250 data points provided by BBH as test set. We take the remaining data points\nfrom BigBench [39] that were not included in BBH, and randomly split them (evenly) into training\nand validation sets. For tasks adopted from [23] (Mpqa, Trec, and Subj), we randomly sample 400\nand 250 data points from their training and test sets, respectively. We use the original validation sets.\nFor tasks adopted from Leopard [1] (Disaster and Airline), we randomly sample 400, 250, and 250\ndata points as training, valid, and test. We list all tasks and their statistics in Table 3 in the Appendix.\nWe use accuracy as the evaluation metric. Specifically, given an input, we compare a system\u2019s output\nstring against the ground-truth output string provided by the dataset. We score 1 if the two strings\nare identical and 0 otherwise. Before the comparison, we process the strings from both the model\noutput and the ground-truth to deal with issues like tokenization and capitalization. In all our DLN\nexperiments, we perform a hyperparameter search and run the same hyperparameter setting with\nthree random seeds. We report the test accuracy averaged over three seeds corresponding to the\nhyperparameter setting that achieves the highest average validation accuracy. We report details of the\nhyperparameter search in the Appendix I.\nThroughout this paper, we use OpenAI\u2019s models, specifically GPT-3 (text-davinci-003) and GPT-4, as\nthe backbone to our proposed systems unless otherwise specified. For DLNs, we use a batch size of\n20 and train for 20 iterations by early-stopping on validation performance evaluated every 2 iterations.\nWe then report test scores. We sample N = 20 prompt proposals and K = 5 hidden samples.\nBaselines We compare the DLN against two classes of baseline systems. First, we test a set of\nsystems equipped with the same backbone (i.e., GPT-3):\n\u2022 0-shot: Given an input, the LLM is required to generate the answer in a zero-shot manner.\n\u2022 5-shot (ICL): Given an input as well as five data points as in-context examples, the LLM is queried\nto generate an answer. The five examples are randomly sampled from the training set.\n\u2022 KATE [20]: Given an input, we retrieve the five most similar data points from the training set using\nan off-the-shelf sentence encoder, and use them as in-context examples.\n\u2022 APE [57]: The LLM is queried to generate a pool of candidate prompts for the task given few\ninput-output pair examples. The candidate prompts are evaluated on a validation set to find the\nbest performing instruction prompt. The best instruction is then used for 0-shot evaluation. We\noptimize the prompt over both 15 and 400 examples (APE-15 and APE-400 respectively).\n\u2022 CoT [48]: Given an input, the LLM is first queried to generate a reasoning path with the prompt\n\u201cLet\u2019s think step by step\u201d. Then, conditioned on the input and its first output, the LLM\nis queried to generate an answer. This is the zero-shot version of CoT and is a natural baseline\nfor DLN-2: it performs two LLM calls and can be seen as DLN-2 without optimization. We will\nreport performance of this baseline when comparing to DLN-2.\nAdditionally, we compare against one of the most advanced LLMs to date, GPT-4. We test 0-shot and\nICL settings with GPT-4.\n5.2\nDLN-1\nOur first set of experiments evaluates the 1-layer language network (DLN-1) described in Section 2.\nTable 1 presents results on the full suite of test tasks. We see that it matches the performance of\n2We only use the variant with seven objects.\n7\nTable 1: Test accuracy averaged over three random seeds of a shallow, 1-layer language network\n(DLN-1) compared to baselines both on GPT-3 and GPT-4. For trainable systems (i.e., APE and\nDLN-1) or systems relying on GPT-4, we report the 95% confidence interval.\nBigBench Hard\nNLU\nLeopard\nMethod\nHyper.\nNav.\nDate.\nLogic.7\nMpqa\nTrec\nSubj\nDisaster\nAirline\nGPT-3\n0-shot\n60.8\n64.1\n56.4\n45.9\n88.0\n61.9\n61.7\n81.6\n75.6\n5-shot\n55.6\n56.5\n62.1\n36.7\n87.2\n80.0\n76.4\n81.2\n82.7\nKATE\n71.1\n56.9\n61.1\n44.4\n88.4\n77.6\n71.1\n76.0\n81.6\nAPE-15\n68.5\u00b15.5\n67.3\u00b17.7\n32.1\u00b128.6\n45.5\u00b14.7\n85.5\u00b14.6\n71.3\u00b15.5\n61.3\u00b17.2\n54.8\u00b114.6\n83.5\u00b13.5\nAPE-400\n65.5\u00b14.7\n56.9\u00b132.9\n23.5\u00b114.1\n45.6\u00b112.4\n84.9\u00b19.7\n72.0\u00b11.7\n63.7\u00b19.2\n60.3\u00b137.4\n82.3\u00b110.0\nDLN-1\n91.9\u00b13.0\n68.5\u00b14.7\n55.7\u00b14.5\n47.5\u00b12.1\n88.5\u00b12.5\n89.7\u00b13.2\n83.2\u00b16.5\n81.7\u00b16.5\n83.2\u00b15.5\nGPT-4\n0-shot\n64.0\u00b11.0\n74.0\u00b11.0\n79.2\u00b12.6\n68.5\u00b13.5\n86.3\u00b10.6\n64.8\u00b11.7\n72.5\u00b11.5\n47.7\u00b10.6\n84.5\u00b10.6\n5-shot\n88.4\u00b12.6\n75.7\u00b11.5\n79.3\u00b11.1\n62.8\u00b11.7\n88.0\u00b13.0\n82.5\u00b13.8\n94.7\u00b13.5\n63.6\u00b18.5\n88.0\u00b11.0\n16-shot\n93.3\u00b12.3\n75.5\u00b15.1\n80.9\u00b15.0\n66.4\u00b13.6\n91.3\u00b11.5\n83.7\u00b10.6\n96.5\u00b12.5\n67.1\u00b14.0\n88.3\u00b12.1\nDLN-1\n95.2\u00b15.0\n77.1\u00b14.7\n76.7\u00b13.0\n69.1\u00b12.5\n91.1\u00b13.2\n89.5\u00b12.1\n93.1\u00b15.0\n82.1\u00b13.8\n85.9\u00b11.5\nDLN-1 prompt on Hyperbaton (GPT-3)\nWhen constructing a sentence with multiple adjectives, the order should be opinion, size, age, shape, color, origin, material,\nand purpose. Adjectives of the same type should be listed in descending order from largest to smallest. When adjectives of\ndifferent types are used, the order should be opinion, size, age, shape, color, origin, material, and purpose. For example, in\nthe phrase \u201cmassive ancient chair\u201d size (massive) should come before age (ancient). Examples: little old-fashioned Russian\nsilver rectangular ship; silly large old leather hiking chair; brand-new spherical Mexican sweater; enormous old spherical green\nNigerian exercise car; medium-size triangular wool eating ship; good square brown Egyptian ship; lovely massive drinking\nmonkey; archaic circular white plastic shoe. In each of the following examples, the adjective order is wrong. Identify the correct\nadjective order:\nDLN-1 prompt on Hyperbaton (GPT-4)\nTo determine the correct adjective order, follow this sequence: opinion, size, shape, age, color, origin, material, and purpose.\nFor example, choose \"large red plastic ball\" over \"red large plastic ball\" since it follows the order: size (large), color (red),\nand material (plastic). Not all adjectives may be present, but the order should still be maintained. If the options are \"ancient\nprismlike white leather whittling match\" and \"leather white ancient prismlike whittling match\", choose the first option, as it\nfollows the order: age (ancient), shape (prismlike), color (white), material (leather), and purpose (whittling). Remember that\nopinion always comes before age, so \"obnoxious old-fashioned typing shoe\" is correct over \"old-fashioned obnoxious typing\nshoe.\" Ensure opinion adjectives come before other adjectives in the sequence. When comparing options, follow the order of\nadjectives for each category: size before color, color before origin, and so on. In cases where purpose and material adjectives\nare switched, like \"paper walking monkey\" vs \"walking paper monkey\", choose the option where material comes before the\npurpose. Additionally, always prioritize the given sequence over the position of adjectives in the sentences. For example,\nchoose \"midsize brand-new gray Chinese wood sweater\" over \"Chinese brand-new gray midsize wood sweater\" as it follows the\norder: size (midsize), age (brand-new), color (gray), origin (Chinese), and material (wood).\nFigure 2: The final prompt of the DLN-1 on Hyperbaton includes not only instructions but also\nexamples from the training set. These samples were automatically chosen by the prompt optimization.\nIn a way, this approach combines in-context learning and prompt optimization.\nthe best GPT-3-based method on Disaster, Mpqa and Airline and narrowly beats the best GPT-3\nbaseline on Logic.7 and Nav.. On Hyper., Trec, and Subj, DLN-1 significantly outperforms the\nbest GPT-3 baseline (by about 20, 10, and 7 percentage points, respectively). On Hyper., Trec, and\nDisaster, it even surpasses GPT-4 baselines, unsurprisingly underperforming GPT-4 on all other\ntasks. DLN-1\u2019s excellent performance on Hyper., a BBH task about ordering adjectives according\nto linguistic convention, is a surprise. To better understand this result, we show the final prompt\nin Figure 2. We see that the prompt contains both instructions and a list of examples from the\ntraining set. These examples were automatically chosen by the optimizer based on their impact on the\nperformance. This can be seen as a combination of KATE, which selects training examples to put in\ncontext based on their similarity with the test example, and APE, which selects the prompt based on\nits performance. On Date., DLN-1 tends to systematically under-perform the 0-shot baseline both for\nGPT-3 and GPT-4. We observed that DLN-1 overfits due to paucity of examples in the validation set.\n8\nTable 2: DLN-2 test accuracy using GPT-3 as LLM.\nMethod\nNav.\nDate.\nLogic.7\nDisaster\nSubj\n0-shot\n64.1\n56.4\n45.9\n81.6\n61.7\nCoT\n69.3\n72.4\n41.1\n54.4\n59.3\nAPE\n67.3\u00b17.7\n32.1\u00b128.5\n45.5\u00b14.7\n54.8\u00b114.6\n61.3\u00b17.2\nAPE-400\n56.9\u00b132.9\n23.5\u00b114.1\n45.6\u00b112.4\n60.3\u00b137.4\n63.7\u00b19.2\nDLN-1\n68.5\u00b14.7\n55.7\u00b14.5\n47.5\u00b12.1\n81.7\u00b16.5\n83.2\u00b15.5\nDLN-2\n83.1\u00b124.7\n75.2\u00b114.8\n45.7\u00b13.5\n82.8\u00b12.5\n85.9\u00b18.7\n5.3\nDLN-2\nWe investigate the effectiveness of depth through experiments with 2-layer language networks (DLN-\n2) on tasks where we expect depth to be most useful, and on which DLN-1 significantly underperforms\nthe GPT-4 0-shot baseline, i.e., Nav., Date., and Logic.7 [42]. Since the Nav., Date. and Logic.7 tasks\nfrom BBH require more complex spatial and temporal reasoning, they are the ones where we most\nexpect a decomposition into subtasks to be helpful. We also include Subj and Disaster as an example\nwhere DLN-1 performs well (even outperforming the GPT-4 0-shot baseline), since we are interested\nto see to what extent DLN-2 can further push performance.\nResults for DLN-2 can be found in Table 2. Compared to DLN-1, DLN-2 provides an average boost\nof 7.2% absolute score. On Nav. and Date., DLN-2 largely improves the performance of DLN-1,\noutperforming all single layer networks. On Logic.7, all methods appear to perform similarly. This\ncould point to the fact that the task might be too hard for the base LLM and thus highlights the\nlimits of prompt optimization of a weak base model. On Subj and Disaster, DLN-2 achieves further\nimprovement over DLN-1. Compared to 0-shot GPT-4 results in Table 1, on Subj and Disaster,\nDLN-2 on average provides more than 20% in absolute improvement. We encourage readers to find\nadditional experimental results in Appendix C.\n6\nRelated Work\nPrompt-Based Machine Learning GPT-3 [5] launched a new paradigm in NLP called in-context\nlearning (ICL), now applied beyond traditional NLP tasks [21]. The discovery of chain-of-thought\nprompts (CoT) marked a major advance in prompting: LLM performance improves markedly when\nthe prompt includes examples of intermediate reasoning steps [48] (few-shot CoT), or simply instructs\nthe model to \u201cthink step by step\u201d [17] (zero-shot CoT). Like CoT, DLNs break a problem down into\nintermediate steps but they operationalize these steps as separate LLM calls, each defined by its\nown learned prompt. Since the introduction of CoT, prompting techniques have evolved to be more\ndynamic and iterative. Recent methods often operate recursively. Examples include RECITE [41],\nSelf-ask [33], and related methods for question-answering Creswell et al. [6], Zhou et al. [56].\nA similar class of methods relies on \u201cintrospection\u201d [14], where an LLM is prompted to ingest,\nevaluate then possibly act on its own previous output. Self-critique [46], ReAct [54], Reflexion [38],\nSelf-refine [24] fit this mould along with Hao et al. [11], Du et al. [9], Yao et al. [53].\nPrompt Optimization Techniques based on notions of self-talk and self-evaluation align naturally\nwith automatic prompt optimization\u2014a core function in DLNs. Early work in this category includes\nAutoprompt [37] and GRIPS [32]. Deng et al. [7] argue that \u2018enumeration-then-selection\u2019 heuristics\nfor optimizing discrete prompts do not explore the prompt space systematically. They take an RL\napproach to overcome this problem, training a policy network, via soft Q-learning with a suitably\ndesigned and stabilized reward function, to generate effective prompts. Through Gibbs sampling,\nReprompting [52] iteratively searches CoT recipes to improve prompt performance automatically.\nMost relevant to DLNs, Zhou et al. [57] present Automatic prompt engineer (APE). APE optimizes\nan initial prompt by searching over a pool of candidates to maximize a score function. We use an\nAPE-inspired approach in DLNs and we cast the proposal/scoring functions as elements of variational\ninference. In a concurrent work, Pryzant et al. [34] proposed using textual gradients in automatic\nprompt optimization. This algorithm uses LLM\u2019s nonparametric feedback to guide prompt generation\nand selection.\n9\nMulti-Layer LLM systems Several recent works compose LLMs as nodes in a computational\ngraph, which is the core idea of DLNs. Some work cited above can be seen as instances of this\nidea. Similarly, Khot et al. [15] induce an LLM to generate a basic \u201ccontrol flow\u201d that calls distinct\nLLM modules. Wu et al. [50] propose AI chains, an interactive system of chained LLMs based on\na set of \u201cLLM primitive\u201d operations. They conduct a 20-person user study in which participants\nmodify chains, and find this process to improve task performance, transparency, and controllability.\nDohan et al. [8] unify LLMs and graphical models as \u201clanguage model cascades\u201d. Specifically, they\ncast LLM compositions as graphical models with string-valued random variables.3 They show how\nscratchpad [30], chain-of-thought [48], tool use [25], and several other prompting strategies fit their\nformalism. DLNs can likewise be considered an instance of language model cascade, because of that\nframework\u2019s generality. However, going beyond the conceptual work of Dohan et al. [8], we present\nan effective technique for doing inference in an LLM-based graphical model and we apply learned\nnetworks of LLMs to several downstream tasks.\n7\nConclusion and Future Work\nIn this paper we introduced an algorithm for joint prompt optimization in deep networks where\neach layer is an LLM. To do so, we consider outputs of each hidden LLM layer as a latent variable\nwe need to do inference over. From a conceptual perspective, we demonstrated how CoT can be\nseen as a DLN-2 with a residual connection. Similarly, Generated Knowledge Prompting [19]\ncould be considered as a fixed forward-only DLN-2 where, in the first layer, an LLM generates\nrelated knowledge, and in the second layer, another LLM takes the generated knowledge as input\nand generates the final answer. Other prompting techniques like ReAct [54], Reflexicon [38], and\nSelf-Consistency [46] could all be ensembles of DLN-1s with different prompt initializations.\nAlthough we only tested 1-layer and 2-layer LNs so far, we already show that the performance of\nsmaller LLMs can be boosted when stacked and prompted properly. We believe the modularity of\nthese architectures will make them more adaptable and reusable to new use cases. While accuracy on\ndownstream tasks is an appealing metric, we argue that other considerations are just as important,\nfor example the ease of adapting a model to one\u2019s own use case, or the ability to leverage multiple\nexisting models.\nWe noticed that GPT-3 has a tendency to always produce an answer given an example: this could be\ndue to the particular 0-shot fine-tuning procedure, which biases the model towards generating useful\nresponses. This raises the question of whether we can fine-tune \u201cstackable\u201d LLMs and whether DLNs\ncan be used as a framework to generate training data for that purpose. Second, we engineered our\nbackward and forward templates; in the future, we wish to expand our work to learn parts of such\ntemplates: we expect this to make the variational bound tighter and thus easing DLN\u2019s optimization.\nAdditionally, while we only proposed 2-layer DLNs, the framework accommodates arbitrary directed\nacyclic graphs.\nImpact statement\nWhile we are fully aware of the limitations of addressing societal issues through\ntechnical work, we hope that modular approaches like ours will alleviate some of the issues associated\nwith LLMs, like the concentration of power associated with the difficulty to train them. We also\nhope that, by facilitating the reusability and adaptivity of such models, we shall make them more\namenable to a wider variety of use cases. However, while we discuss the performance of these\nmodels on artificial benchmarks, we do not address the question of when and how such models\nshould be deployed, nor do we offer additional guarantees against their misuse. We also emphasize\nthat performance on artificial tasks, even if realistic, is neither representative of performance in\nuncontrolled environments, nor enough to justify the deployment of these models in high stakes\nsituations.\nAcknowledgements\nWe would like to acknowledge Silviu Pitis for the useful feedback on the draft,\nNikolay Malkin and Tong Wang for their advice during the first steps of this project.\n3Earlier work by Miao and Blunsom [27] also treated strings as random variables.\n10\nReferences\n[1] Bansal, T., Jha, R., and McCallum, A. (2020). Learning to few-shot learn across diverse\nnatural language classification tasks. In Proceedings of the 28th International Conference on\nComputational Linguistics, pages 5108\u20135123, Barcelona, Spain (Online). International Committee\non Computational Linguistics.\n[2] Bender, E. M., Gebru, T., McMillan-Major, A., and Mitchell, M. (2021). On the dangers of\nstochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference\non fairness, accountability, and transparency, pages 610\u2013623.\n[3] Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. (2017). Variational inference: A review for\nstatisticians. Journal of the American statistical Association, 112(518):859\u2013877.\n[4] Blodgett, S. L., Liao, Q. V., Olteanu, A., Mihalcea, R., Muller, M., Scheuerman, M. K., Tan, C.,\nand Yang, Q. (2022). Responsible language technologies: Foreseeing and mitigating harms. In\nExtended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, CHI\nEA \u201922, New York, NY, USA. Association for Computing Machinery.\n[5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877\u20131901.\n[6] Creswell, A., Shanahan, M., and Higgins, I. (2022). Selection-inference: Exploiting large\nlanguage models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712.\n[7] Deng, M., Wang, J., Hsieh, C., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E. P., and Hu, Z.\n(2022). Rlprompt: Optimizing discrete text prompts with reinforcement learning. In Goldberg, Y.,\nKozareva, Z., and Zhang, Y., editors, Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December\n7-11, 2022, pages 3369\u20133391. Association for Computational Linguistics.\n[8] Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski,\nH., Saurous, R. A., Sohl-Dickstein, J., et al. (2022). Language model cascades. arXiv preprint\narXiv:2207.10342.\n[9] Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., and Mordatch, I. (2023). Improving factuality and\nreasoning in language models through multiagent debate.\n[10] Gao, T., Fisch, A., and Chen, D. (2021). Making pre-trained language models better few-\nshot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), pages 3816\u20133830, Online. Association for Computational Linguistics.\n[11] Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. (2023). Reasoning with\nlanguage model is planning with world model. arXiv preprint arXiv:2305.14992.\n[12] Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. (2022). Instruction induction: From\nfew examples to natural language task descriptions.\n[13] Hosseini, A., Reddy, S., Bahdanau, D., Hjelm, R. D., Sordoni, A., and Courville, A. C. (2021).\nUnderstanding by understanding not: Modeling negation in language models. In Toutanova,\nK., Rumshisky, A., Zettlemoyer, L., Hakkani-T\u00fcr, D., Beltagy, I., Bethard, S., Cotterell, R.,\nChakraborty, T., and Zhou, Y., editors, Proceedings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages 1301\u20131312. Association for Computational\nLinguistics.\n[14] Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch,\nI., Chebotar, Y., et al. (2022). Inner monologue: Embodied reasoning through planning with\nlanguage models. arXiv preprint arXiv:2207.05608.\n11\n[15] Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. (2023).\nDecomposed prompting: A modular approach for solving complex tasks. International Conference\non Learning Representations.\n[16] Kingma, D. P. and Welling, M. (2022). Auto-encoding variational bayes.\n[17] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large language models\nare zero-shot reasoners. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh,\nA., editors, Advances in Neural Information Processing Systems, volume 35, pages 22199\u201322213.\nCurran Associates, Inc.\n[18] Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. (2022). Internet-augmented\nlanguage models through few-shot prompting for open-domain question answering. arXiv preprint\narXiv:2203.05115.\n[19] Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Le Bras, R., Choi, Y., and Hajishirzi, H. (2022a).\nGenerated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3154\u2013\n3169.\n[20] Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. (2021). What makes good\nin-context examples for gpt-3?\n[21] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023). Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural language processing. ACM\nComputing Surveys, 55(9):1\u201335.\n[22] Liu, R., Wei, J., Gu, S. S., Wu, T.-Y., Vosoughi, S., Cui, C., Zhou, D., and Dai, A. M.\n(2022b). Mind\u2019s eye: Grounded language model reasoning through simulation. arXiv preprint\narXiv:2210.05359.\n[23] Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. (2022). Fantastically ordered\nprompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics.\n[24] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N.,\nPrabhumoye, S., Yang, Y., Welleck, S., Majumder, B. P., Gupta, S., Yazdanbakhsh, A., and Clark,\nP. (2023). Self-refine: Iterative refinement with self-feedback.\n[25] Mialon, G., Dess\u00ec, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozi\u00e8re, B.,\nSchick, T., Dwivedi-Yu, J., Celikyilmaz, A., Grave, E., LeCun, Y., and Scialom, T. (2023).\nAugmented language models: a survey.\n[26] Miao, Y. and Blunsom, P. (2016a). Language as a latent variable: Discrete generative models for\nsentence compression. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 319\u2013328, Austin, Texas. Association for Computational Linguistics.\n[27] Miao, Y. and Blunsom, P. (2016b). Language as a latent variable: Discrete generative models for\nsentence compression. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 319\u2013328.\n[28] Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer,\nL. (2022). Rethinking the role of demonstrations: What makes in-context learning work? In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages\n11048\u201311064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n[29] Mukherjee, S. and Awadallah, A. H. (2020). Xtremedistil: Multi-stage distillation for massive\nmultilingual models. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. R., editors, Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 2221\u20132234. Association for Computational Linguistics.\n12\n[30] Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan,\nD., Lewkowycz, A., Bosma, M., Luan, D., et al. (2021). Show your work: Scratchpads for\nintermediate computation with language models. arXiv preprint arXiv:2112.00114.\n[31] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal,\nS., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human\nfeedback. Advances in Neural Information Processing Systems, 35:27730\u201327744.\n[32] Prasad, A., Hase, P., Zhou, X., and Bansal, M. (2022). Grips: Gradient-free, edit-based\ninstruction search for prompting large language models. arXiv preprint arXiv:2203.07281.\n[33] Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. (2022). Measuring and\nnarrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.\n[34] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and Zeng, M. (2023). Automatic prompt\noptimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495.\n[35] Rubin, O., Herzig, J., and Berant, J. (2022). Learning to retrieve prompts for in-context learning.\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 2655\u20132671.\n[36] Sanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert, a distilled version of BERT:\nsmaller, faster, cheaper and lighter. CoRR, abs/1910.01108.\n[37] Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. (2020). Autoprompt: Eliciting\nknowledge from language models with automatically generated prompts. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n4222\u20134235.\n[38] Shinn, N., Labash, B., and Gopinath, A. (2023). Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint arXiv:2303.11366.\n[39] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R.,\nSantoro, A., Gupta, A., Garriga-Alonso, A., et al. (2022). Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.\n[40] Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, J., Zhang, R., Ostendorf, M., Zettlemoyer,\nL., Smith, N. A., et al. (2023). Selective annotation makes language models better few-shot\nlearners. International Conference on Learning Representations.\n[41] Sun, Z., Wang, X., Tay, Y., Yang, Y., and Zhou, D. (2022). Recitation-augmented language\nmodels. arXiv preprint arXiv:2210.01296.\n[42] Suzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A.,\nLe, Q. V., Chi, E. H., Zhou, D., , and Wei, J. (2022). Challenging big-bench tasks and whether\nchain-of-thought can solve them. arXiv preprint arXiv:2210.09261.\n[43] Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and Lin, J. (2019). Distilling task-specific\nknowledge from BERT into simple neural networks. CoRR, abs/1903.12136.\n[44] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B.,\nGoyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. (2023a).\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n[45] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra,\nS., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu,\nD., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A.,\nHosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A.,\nKoura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X.,\nMihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R.,\nWilliams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S.,\nRodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. (2023b). Llama 2: Open foundation and\nfine-tuned chat models.\n13\n[46] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. (2023a). Self-consistency\nimproves chain of thought reasoning in language models. International Conference on Learning\nRepresentations.\n[47] Wang, X., Zhu, W., Saxon, M., Steyvers, M., and Wang, W. Y. (2023b). Large language models\nare implicitly topic models: Explaining and finding good demonstrations for in-context learning.\n[48] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and\nZhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. In\nNeurIPS.\n[49] Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.-S., Mellor, J., Glaese, A., Cheng, M.,\nBalle, B., Kasirzadeh, A., et al. (2022). Taxonomy of risks posed by language models. In 2022\nACM Conference on Fairness, Accountability, and Transparency, pages 214\u2013229.\n[50] Wu, T., Terry, M., and Cai, C. J. (2022). Ai chains: Transparent and controllable human-ai\ninteraction by chaining large language model prompts. In Proceedings of the 2022 CHI Conference\non Human Factors in Computing Systems, pages 1\u201322.\n[51] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. (2023a).\nWizardlm: Empowering large language models to follow complex instructions.\n[52] Xu, W., Banburski-Fahey, A., and Jojic, N. (2023b). Reprompting: Automated chain-of-thought\nprompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993.\n[53] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. (2023a). Tree\nof thoughts: Deliberate problem solving with large language models.\n[54] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023b). React:\nSynergizing reasoning and acting in language models. International Conference on Learning\nRepresentations.\n[55] Zhang, Z., Zhang, A., Li, M., and Smola, A. (2023). Automatic chain of thought prompting in\nlarge language models. International Conference on Learning Representations.\n[56] Zhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le,\nQ., and Chi, E. (2023a). Least-to-most prompting enables complex reasoning in large language\nmodels. International Conference on Learning Representations.\n[57] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2023b). Large\nlanguage models are human-level prompt engineers. International Conference on Learning\nRepresentations.\n14\nContents in Appendices:\n\u2022 In Appendix A, we list the contribution of each author to this work.\n\u2022 In Appendix B, we provide additional experimental details including task statistics and the\nprompt strings we used to initialize DLN.\n\u2022 In Appendix C, we provide additional experiments and baselines we compare to.\n\u2022 In Appendix D, we provide forward and backward templates being used in DLN.\n\u2022 In Appendix E, we provide an algorithm that generalizes DLN training in a multiple layer\nsetting.\n\u2022 In Appendix F, we show examples of learned weights that exhibit behavior similar to\nin-context learning.\n\u2022 In Appendix G, we show examples of learned weights by 2-Layer DLNs.\n\u2022 In Appendix H, we show an example of the hidden states produced by a 2-Layer DLN.\n\u2022 In Appendix I, we provide implementation details, including hyperparameter information.\n\u2022 In Appendix J, we discuss resource used in DLN development and their pricing.\n15\nA\nContributions\nAlessandro Sordoni proposed the general idea of DLN, where multiple prompts are learnt at each\nlayer through backward natural language operations; they proposed to generate synthetic in-context\nexamples and the exploration reward for DLN-2; they wrote the code and ran the experiments; they\nfocused on Sections 2, 3, 4 and contribute writing the rest of the sections.\nXingdi Yuan co-developed the basic idea of DLN and wrote part of the code, they also co-designed\nand helped conducting experiments. They contributed to the writing of the paper, mainly Sections 5\nand 6.\nMarc-Alexandre C\u00f4t\u00e9 helped with the experiments and the infrastructure to make calls to OpenAI\nmodels. They also built a demo to visualize the evolution of DLN\u2019s prompts during training and\ncontributed to the writing of the paper, mainly focusing on the algorithms and the appendix.\nMatheus Pereira co-coded an earlier, non-variational backwards operator with AT, helped with\nthe APE and DLN-2 layers experiments, implemented the method for estimating the total cost\nof experiments, build a demo to visualize the evolution of DLN\u2019s prompts during training, and\ncontributed to the release of the DLN code.\nAdam Trischler helped with template conception and iteration, co-coded an earlier, non-variational\nbackwards operator with MP, and contributed to paper writing, mainly the literature review.\nZiang Xiao helped with the model evaluation and experiment setup and contributed to the paper\nwriting, mainly the literature review and discussion.\nArian Hosseini participated in the development discussions throughout the project and contributed to\nwriting the literature review of the paper.\nFriederike Niedtner organized and managed the project, helping the team focus on the right priorities.\nNicolas Le Roux proposed the variational inference formulation and the posterior sharpening. They\noffered guidance and mentorship for the project. They also contributed to the writing of the paper,\nmainly sections 1, 2, and 3.\nB\nAdditional Experimental Details\nB.1\nAdditional Task Information\nIn Table 3, we provide short descriptions for all tasks we use and their statistics.\nTable 3: Tasks used in this work.\nTask\n|train|\n|valid|\n|test|\n|class|\nDescription\nMpqa\n400\n256\n250\n2\nSentiment analysis.\nTrec\n400\n256\n250\n6\nQuestion type classification.\nSubj\n400\n256\n250\n2\nDetermine whether a sentence is subjective or objective.\nDisaster\n400\n250\n250\n2\nDetermine whether a sentence is relevant to a disaster.\nAirline\n400\n250\n250\n3\nAirline tweet sentiment analysis.\nHyper.\n400\n1000\n250\n2\nOrder adjectives correctly in English sentences.\nNav.\n375\n375\n250\n2\nSpatial reasoning given navigation instructions.\nDate.\n59\n60\n250\n6\nInfer a date from context.\nLogic.7\n225\n225\n250\n7\nDeduce the order of seven objects given instruction.\nB.2\nPrompt Initialization\nWe initialize the \u201cclassification\u201d layer of the DLNs, i.e. the first layer of the 1-Layer LN and the\nsecond layer of the 2-layer DLN, with a question or a task description as reported in Table 4. We\nuse these same initializations to compute the 0-shot performance. Therefore, at initialization, a\n1-layer LN is equivalent to the 0-shot baseline. For the hidden layer of the 2-layer DLN, we initialize\nthe prompt to \u201cDecompose the problem to make it simpler:\u201d for Nav. and Subj, and \u201c\u201d\n(empty string) for Date. and Logic.7. We didn\u2019t try other initializations for this hidden layer, we leave\nthis for future explorations.\n16\nTable 4: Prompt initializations.\nTask\nInitialization\nMpqa\nRead the following review, then choose whether it is negative or positive.\nTrec\nRead the following question, then choose whether it is about a description, entity, expres-\nsion, human, location or number.\nSubj\nRead the following sentence, then choose whether it is subjective or objective\nDisaster\nRead the following sentence, then choose whether it is relevant to a disaster.\nAirline\nRead the following sentence, then choose whether it is positive, negative, or neutral.\nHyper.\nWhich sentence has the correct adjective order.\nNav.\nRead the following sentence, then determine whether you return to the starting point.\nDate.\nInfer the date from context.\nLogic.7\nThe following paragraphs each describe a set of seven objects arranged in a fixed order.\nThe statements are logically consistent within each paragraph.\nC\nAdditional Experiments\nIn addition to the prompt engineering and few-shot baselines from the main paper, we include here\ncomparisons to more of those on a subset of the datasets. Particularly we add results for:\n\u2022 Table 5 shows DLN-1 and DLN-2 outperforming CoT+APE (implemented as described in\nSection 4.3 of Zhou et al. [57]);\n\u2022 Table 6 shows that even increasing the number of examples for ICL and KATE cannot match\nDLN-1 and DLN-2 performance;\n\u2022 Table 7 and Table 8 show results for DLN-1 using open-source language models such as\nWizardLM-13b-v1.2 [51] and LLaMA2-70B-Chat [45] respectively.\nTable 5: Test accuracy averaged over three random seeds with 95% confidence interval. All models\nuse GPT-3. DLN-1 and DLN-2 outperform CoT+APE.\nMethod\nHyper.\nNav.\nDate.\nLogic.7\nAPE-15\n68.5\u00b1 5.5\n67.3\u00b1 7.7\n32.1\u00b1 28.6\n45.5\u00b1 4.7\nCoT+APE\n50.9\u00b1 0.8\n61.5\u00b1 1.5\n58.6\u00b1 2.2\n38.9\u00b1 1.6\nDLN-1\n91.9\u00b1 3.0\n68.5\u00b1 4.7\n55.7\u00b1 4.5\n47.5\u00b1 2.1\nDLN-2\n-\n83.1\u00b1 24.7\n75.2\u00b1 14.8\n45.7\u00b1 3.5\nTable 6: Test accuracy averaged over three random seeds with 95% confidence interval (where\napplicable). All models use GPT-3. Increasing the number of ICL examples helps performance, but\ncannot match DLN-1 and DLN-2 in general. Context length limit is an issue for ICL and KATE\n32-shot.\nMethod\nNav.\nDate.\nLogic.7\nSubj\nICL - 5-shot\n56.5\n62.1\n36.7\n76.4\nICL - 10-shot\n61.3\n62.9\n38.9\n72.0\nICL - 32-shot\n66.0\n63.5\n-\n83.2\nKATE - 5-shot\n56.9\n61.1\n44.4\n71.1\nKATE - 10-shot\n59.5\n62.0\n41.6\n73.9\nKATE - 32-shot\n67.5\n62.8\n-\n80.4\nDLN-1\n68.5\u00b1 4.7\n55.7\u00b1 4.5\n47.5\u00b1 2.1\n83.2\u00b15.5\nDLN-2\n83.1\u00b1 24.7\n75.2\u00b1 14.8\n45.7\u00b1 3.5\n85.9\u00b18.7\n17\nTable 7: Test accuracy using WizardLM-v1.2 13B as LLM. This open source model seems signifi-\ncantly less able to capture few-shot examples from the context. DLN-1 outperforms ICL on all tasks\nhere.\nMethod\nNav.\nLogic.7\nSubj\n0-shot\n58.0\n0.0\n65.8\n5-shot\n56.0\n28.0\n50.8\nDLN-1\n61.1\n31.0\n79.8\nTable 8: Test accuracy averaged over three random seeds with 95% confidence interval. All methods\nuse LLaMA2-70B-Chat as LLM, with DLN + GPT3 employing text-davinci-003 as the backward\nLLM for prompt and hidden proposals.\nMethod\nNav.\nDate.\nLogic.7\nSubj\n0-shot\n42.0\u00b10.0\n25.2\u00b10.0\n14.4\u00b10.0\n62.4\u00b10.0\n5-shot\n43.2\u00b112.5\n21.1\u00b19.7\n16.4\u00b12.6\n67.7\u00b115.5\nDLN-1 + GPT3\n43.6\u00b14.0\n21.9\u00b15.7\n33.1\u00b110.9\n80.9\u00b111.5\nDLN-1\n44.9\u00b16.4\n31.6\u00b110.5\n38.4\u00b13.6\n76.1\u00b14.5\nDLN-2 + GPT3\n43.7\u00b13.0\n51.1\u00b14.0\n21.9\u00b14.9\n59.1\u00b116.7\nDLN-2\n68.9\u00b114.4\n61.7\u00b117.6\n20.0\u00b113.7\n63.1\u00b125.4\nC.1\nLayerwise training of DLN-2\nWe also explored a different learning strategy for DLN-2: layer-wise pre-training. We start the\nlearning of a single layer DLN using the technique from Section 2. We call the prompt obtained at\nthe end of this optimization \u03c0\u2217. Then, we train a DLN-2 by initializing \u03c01 = \u03c0\u2217, and then learn \u03c00,\nthe parameters of the bottom layer, using variational inference. We explore two variants: one keeps\nthe last layer fixed and one it fine-tunes the last layer. We report their results in Table 9.\nTable 9: Test accuracy averaged over three random seeds. We compare the layerwise and the end-to-\nend trainings for DLN-2.\nNav.\nDate.\nLogic.7\nSubj\nDLN-2 (fix 2nd)\n73.1\n61.6\n43.3\n80.2\nDLN-2 (fine-tune 2nd)\n76.4\n62.8\n40.7\n84.5\nDLN-2 (end-to-end)\n83.1\n75.2\n45.7\n85.9\n18\nD\nTemplates\nD.1\nForward Templates\n\u201cClassification\u201d Template F for 1-Layer LN\nIn the 1-layer LN, we use the following template to\nelicit the output y given the input x. prompt is substituted with the value of the current prompt.\nClassification Template F\ntemplate:\n{{ prompt }}\n{{ input }}\nAnswer:\nResidual Classification Template Fr for 2-Layer DLN\nIn the 2-layer LN, the last layer just\nconcatenates the input to the output of the first layer, \u02c6h, before eliciting an answer.\nResidual classification template Fr\ntemplate:\n{{ prompt }}\n{{ input }}\nYour thoughts were:\n{{ h }}\nAnswer:\nHidden Layer F\nThe variable prompt is substituted with the value of the current prompt \u03c00. This\nhas the effect of providing additional information about how \u201cLet\u2019s think step by step\u201d should behave.\nHidden Layer F\ntemplate:\n{{ input }}\n{{ prompt }} Let's think step by step.\nFor 2-Layer DLN on Subj, we use the following hidden template. We couldn\u2019t run with the previous\ntemplate due to lack of time, as we observed that the step by step trigger tended to generate lengthy\nhidden states.\nHidden Layer F\ntemplate:\n{{ prompt }}\n{{ input }}\nBrief Analysis:\nD.2\nBackward Templates (Prompt and Hidden Proposals)\nPrompt Proposal Template B\u03c0\nB\u03c0 is used to propose new candidate prompts. The template takes\nas input the current prompt, prompt, and a mini-batch of examples stored in backward_infos.\nbackward_info.input stores the input to the layer (x if we are proposing prompts for the first\nlayer or h if it is the second layer); backward_info.target stores the target to the layer (h\u2217 if it is\nthe first layer or y otherwise); backward_info.output stores the predictions of the model during\nthe forward pass (\u02c6h if it is the first layer, and \u02c6y if it is the second layer). message is substituted with\none of the message_alternatives, sampled at random during the DLN training. This induces\ndiversity in the generated prompts and allows emergence of learning to ICL behaviors, where the\nprompts contain synthetic examples that can help solve the task.\n19\nPrompt Proposal Template B\u03c0\ntemplate:\nA student is completing a task that requires producing a text output from a text input. The student receives an instruction\nthat describes how to produce the output given each input.\nThe student has made some errors. Your task is to improve the instruction such that the student can fix the errors.\nThis was the instruction.\n## Instruction\n> {{ prompt }}\n[END]\n# Student successes\n{% for backward_info in backward_infos %} {% if backward_info.loss == 0.0 %}\n## Input:\n> {{ backward_info.input }}\n## Correct Output:\n> {{ backward_info.target }}\n{% endif %} {% endfor %}\n# Student errors\n{% for backward_info in backward_infos %} {% if backward_info.loss > 0.0 %}\n## Input:\n> {{ backward_info.input }}\n## Student Output:\n> {{ backward_info.output }}\n## Correct Output:\n> {{ backward_info.target }}\n{% endif %} {% endfor %}\nImprove the instruction to fix the student errors. {{ message }}\n## Instruction\n>\nmessage_alternatives:\n\u2212 Clarify the instruction by adding few words or a short sentence. Be concise.\n\u2212 Improve the instruction by providing examples on how to solve the task. Be concise.\n\u2212 Shorten the instruction by removing superflous words or sentences.\n\u2212 Rewrite the instruction by providing detailed information to avoid ambiguity. Be concise.\nBackward Hidden Templates Bh\nWe experiment with multiple backward templates to sample hid-\nden states from the approximate posterior distribution q(h). Each vary in the amount of conditioning\ninformation. The simpler way of sampling hidden states is to use the same hidden template as in\nthe forward pass. This corresponds to using a posterior distribution q(h) which is equivalent to the\nprior distribution p(h). The other alternative is to condition the posterior template on the answer y,\nas illustrated below:\nBackward Hidden Template By (y conditioning)\ntemplate:\n{{ input }}\nGiven that the answer is:\n{{ y }}\n{{ prompt }} Let's think step by step.\nThe next alternative we experiment with is a more verbose template that takes as input the prompt for\nthe final layer \u03c01 in next_prompt, the input x in input, the hidden state from the forward pass \u02c6h in\nh and the ground-truth output y. We use a similar strategy of sampling different message alternatives\nto substitute with message to increase diversity of the hidden samples:\n20\nBackward Hidden Template Bh\ntemplate:\nThis is the context needed to solve the problem:\n{{ next_prompt }}\nThis is the problem:\n{{ input }}\nThese were your thoughts:\n{{ h }}\nGiven that this is the answer:\n{{ y }}\n{{ message }}\nThoughts:\nmessage_alternatives:\n\u2212 Reflect and refine your thoughts for this problem by adding detailed explanations.\n\u2212 Fix the errors in your reasoning. Add examples to illustrate your thoughts. Be concise.\nIn practice, we found that sampling from hidden states from a mixture of forward template F, i.e.\npLM(h|x, \u03c00), and backward template with y conditioning, i.e. q(h|x, y, \u03c00) works well. We suspect\nthat this has the effect of capping the KL divergence term between posterior and prior distribution, i.e.\nthe KL(q(h)||pLM(h|x, \u03c00)) appearing in the ELBO. In the future, this could be addressed in a more\nprincipled way by learning a prompt for the posterior proposal.\nE\nGeneralized VI to Multiple Layers\nWe report the generalized training algorithm for multiple layers in Algorithm 3.\nAlgorithm 3 Deep Language Network Training Algorithm\n1: \u02c6h \u223c pt\nLM(x): generates a completion of prefix x with temperature t.\n2: log pLM(h|x): return log-prob of h following x.\n3: N: # prompt samples, K: # posterior samples, I: # iterations, L: # layers, D: dataset\n4: F: template for the inference (forward pass).\n5: B\u03c0, Bh: templates for prompt and hidden proposal (backward pass).\n6: Initialize all layers \u03c0l with a generic sentence or task description.\n7: for i in [1, I] do\n8:\nx, y \u223c D\n\u25b7 Sample minibatch\n9:\n\u02c6h0 \u2190 x\n10:\n\u02c6hl, . . . , \u02c6hL \u2190 p0\nLM(F(\u02c6hl\u22121, \u03c0l\u22121))\n\u25b7 Inference pass for all layers 0 < l \u2264 L\n11:\nh\u2217\nL \u2190 y\n12:\nfor l in [L \u2212 1, 1] do\n13:\nh1\nl , . . . , hK\nl \u223c p0.7\nLM (Bh(\u02c6hl, h\u2217\nl+1, \u03c0l))\n\u25b7 Sample K posterior proposals for hl\n14:\n\u03b11\nl , . . . , \u03b1K\nl \u2190 log pLM(hk\nl |F(\u02c6hl\u22121, \u03c0l\u22121)) \u25b7 Compute prior log-probs for all hk\nl samples\n15:\n\u03b21\nl , . . . , \u03b2K\nl\n\u2190 log pLM(h\u2217\nl+1|F(hk\nl , \u03c0l))\n\u25b7 Compute log-likelihoods for all hk\nl samples\n16:\nqk\nl \u2190 exp(\u03b1k\nl + \u03b2k\nl )/(P\nk exp(\u03b1k\nl + \u03b2k\nl ))\n\u25b7 Compute normalized posterior weights\n17:\nh\u2217\nl \u2190 arg maxhl{q1\nl , . . . , qK\nl }\n\u25b7 Select best posterior sample for layer l\n18:\nend for\n19:\nfor l in [L \u2212 1, 0] do\n20:\n\u03c01\nl , . . . , \u03c0N\nl\n\u223c p0.7\nLM (B\u03c0({\u02c6hl, \u02c6hl+1, h\u2217\nl+1}, \u03c0l))\n\u25b7 Sample N candidate prompts for \u03c0l\n21:\ns1\nl , . . . , sN\nl \u2190 P\nk\nP\nk\u2032 qk\nl qk\u2032\nl+1 log pLM(hk\u2032\nl+1|F(hk\nl , \u03c0n\nl ))\n\u25b7 Compute ELBO for all\nprompts \u03c0n\nl\n22:\n\u03c0l \u2190 arg max\u03c0n\nl {s1\nl , . . . , sN\nl }\n\u25b7 Select prompt \u03c0l with best score\n23:\nend for\n24: end for\n21\nF\nLearning to In-Context Learn: Additional Examples\nIn Figure 2, we report examples of prompts found by the 1-layer DLN on the Hyperbaton task, which\nexhibit either integration or verbalization of task examples. Here, we provide additional examples of\nprompts found by DLN-1 on the MPQA task.\nDLN-1 prompt on MPQA (GPT-3)\nRead each sentence, then decide if the sentence is expressing a positive or negative sentiment. For example, if the sentence\nis \"supported\", choose \"positive\", and if the sentence is \"derail\", choose \"negative\". Similarly, if the sentence is \"victorious\",\nchoose \"positive\", and if the sentence is \"would not be a bad idea\", choose \"positive\". Additionally, if the sentence contains\nmultiple words, consider the overall sentiment of the sentence and choose the appropriate option. For example, if the sentence\nis \"counting on\", choose \"negative\", and if the sentence is \"peace and stability and prosperity\", choose \"positive\". Note that\nwords like \"artificial\" tend to have a negative sentiment.\nDLN-1 prompt on MPQA (GPT-4)\nDetermine whether the given input has a positive or negative connotation by analyzing the meaning of the words and phrases\nin context. If the input expresses a favorable, desirable, or pleasant meaning, choose \"positive.\" If the input expresses an\nunfavorable, undesirable, or unpleasant meaning, choose \"negative.\" Consider the overall sentiment expressed by the input\nrather than focusing on individual words or phrases. For example, \"calling for\" generally has a positive connotation as it implies\nadvocating or supporting something, while \"a true Muslim fighter\" can be seen as positive, since it refers to someone dedicated\nto their beliefs. Keep in mind that some phrases may have a positive connotation when they imply improvement or resolution,\nlike \"put an end to.\" Additionally, phrases like \"to the contrary\" can have a positive connotation when they suggest a differing,\nyet valid perspective or opinion. When analyzing the input, consider the context in which it is used, as the connotation of a word\nor phrase can change depending on the situation. For example, \"extra vigil\" can have a positive connotation when it implies\nincreased awareness and preparedness.\nG\nExamples of 2-Layer Best Weights (GPT-3)\nG.1\nNavigate (81.6% Dev Acc)\nDLN-2 Prompt: \u03c00\nDecompose the problem by breaking it down into steps and describing the new position after each step. Note that the direction\nyou are facing stays the same unless specified. Start by specifying the direction you are facing and the coordinates of the\nstarting point. For each step, describe the number of steps taken and the direction of movement (e.g. North, South, East, or\nWest). Specify the new coordinates and direction after each step.\nDLN-2 Prompt: \u03c01\nStart facing north. Take the specified number of steps in the indicated direction and turn when specified. Make sure to keep\ntrack of your direction and the number of steps taken to ensure you return to the starting point.\nG.2\nSubj (89.9% Dev Acc)\nDLN-2 Prompt: \u03c00\nReflect on the input to produce an output that is either objective (factual) or subjective (involving opinion or value judgment).\nObjective statements describe events or facts that can be verified, while subjective statements express opinion or personal\nfeelings about a fact, event, or situation that cannot be confirmed as an accurate statement of fact. For example, if the input\nis \"the film was a success at the box office,\" the output would be \"This statement is an objective fact, as it describes events\nthat can be verified.\" If the input is \"the film was an amazing experience,\" the output would be \"This statement is subjective,\nexpressing a personal opinion about the film in question that reflects approval and judgement, and cannot be confirmed as an\naccurate statement.\" If the input is \"neither Juan Antonio nor Se\u00f1or Maximiliano know what they are in for when the tables are\nturned by the sly Carmen,\" the output would be \"This statement is an objective fact, as it describes events that can be verified\nregarding the actions of Juan Antonio, Se\u00f1or, and Maximiliano, and the change of circumstances caused by Carmen.\" If the\ninput is \"humorless, self-conscious art drivel, made without a glimmer of intelligence or invention,\" the output would be \"This\nstatement is subjective, expressing a negative opinion about the film in question that reflects disapproval and judgement, and\ncannot be confirmed as an accurate statement.\" If the input is a hypothetical situation, such as \"how would you feel if when you\nwoke, the nightmare had just begun?\", the output would be \"This statement is subjective, expressing a personal opinion about\na hypothetical situation that cannot be verified.\"\n22\nDLN-2 Prompt: \u03c01\nRead the sentence. Determine if it is expressing a fact or opinion. A fact is an accurate statement that can be confirmed,\nwhile an opinion is a personal viewpoint that reflects someone\u2019s beliefs. Facts are typically statements about something that\nhappened, such as events, actions, or conditions, or statements that describe a state of being, such as someone\u2019s personality\nor a physical object. Opinions are typically statements that express judgement, approval, or disapproval. Examples of facts\ninclude statements about events that occurred, such as \"the film was released in 2012,\" or statements about conditions, such\nas \"the weather is sunny,\" or statements that describe a state of being, such as \"the detective is strong and independent.\"\nExamples of opinions include statements about how good or bad something is, such as \"the film was terrible\" or \"the weather is\nbeautiful.\" Examples of questions that are not facts or opinions include hypothetical questions, such as \"how would you feel if\nwhen you woke, the nightmare had just begun?\". Select \"objective\" for a fact and \"subjective\" for an opinion.\nH\nExamples of Hidden States\nWe report the forward pass produced by a 2-Layer DLN in Section G.1 on Navigate below.\nInput x\nIf you follow these instructions, do you return to the starting point? Take 3 steps. Take 10 steps. Take 4 steps. Take 1 step.\nOptions:\n- Yes\n- No\nHidden \u02c6h\n1. Take 3 steps: (3, 0) facing east\n2. Take 10 steps: (13, 0) facing east\n3. Take 4 steps: (17, 0) facing east\n4. Take 1 step: (18, 0) facing east\nAnswer: No, you do not return to the starting point.\nOutput \u02c6y\nNo\nI\nImplementation Details\nWe report hyperparameter search space in Table 10. A brief description of the hyperparameters is as\nfollows:\n\u2022 bh_tpl is the type of backward prompt template we use B\u03c0. v3.5 is equal to the B\u03c0 we report\nin Section D. In v3.0, we remove \u201cBe concise.\u201d at the end of each message_alternatives.\nWe noticed that in general v3.5 works better as it implements a sort of regularization on\nthe length of the found prompts. Future work could address length regularization in a more\nprincipled manner.\n\u2022 logp_penalty is the coefficient for the exploration reward we mentioned in the paper.\n\u2022 num_h_samples is the number of h samples to generate from the approximate posterior\ndistribution.\n\u2022 use_memory is whether or not we use the backtracking mechanism. Usually 2 works well\nacross tasks.\n\u2022 held_out_prompt_ranking describes whether we use only half of the mini-batch exam-\nples for each prompt proposal, as described in the main paper.\n\u2022 tolerance describes after how many iterations we reload the best weights found during\nthe last validation if the current validation score is lower than the best score obtained so far.\nFor the 2-Layer experiments, we have to restrict this search space due to computational costs. We\nuse bh_tpl = \"v3.5\", tolerance = 2, use_memory = 2, held_out_prompt_ranking = True,\nlogp_penalty = 0.5.\n23\nTable 10: Hyperparameter search space.\nhyperparam\nsearch space\n1-Layer LN\nbh_tpl\nq_action_prompt:v3.0, q_action_prompt:v3.5\ntolerance\n-1, 0, 2\nuse_memory\n0, 2\nheld_out_prompt_ranking\nTrue, False\n2-Layer DLN PT + fix 2nd layer\nbh_tpl\nq_action_prompt:v3.0, q_action_prompt:v3.5\nlogp_penalty\n0., 0.5, 2.\n2-Layer DLN PT + fine-tune 2nd layer\nbh_tpl\nq_action_prompt:v3.0, q_action_prompt:v3.5\nlogp_penalty\n0., 0.5, 2.\n2-Layer DLN end-to-end\nnum_h_samples\n5, 10\nTable 11: Test accuracy along with inference cost expressed in tokens (in gray) averaged over three\nrandom seeds of a shallow, 1-layer language network (DLN-1) compared to baselines on GPT-4. We\nalso report the 95% confidence interval on the test accuracy. We emphasize the cost at the testing\ntime because it is more relevant in real-world deployment and the training cost is one-off.\nBigBench Hard\nNLU\nLeopard\nMethod\nHyper.\nNav.\nDate.\nLogic.7\nMpqa\nTrec\nSubj\nDisaster\nAirline\nGPT-4\n0-shot\n64.0\u00b11.0\n74.0\u00b11.0\n79.2\u00b12.6\n68.5\u00b13.5\n86.3\u00b10.6\n64.8\u00b11.7\n72.5\u00b11.5\n47.7\u00b10.6\n84.5\u00b10.6\n(7.6k)\n(12.9k)\n(23.6k)\n(46.2k)\n(3.7k)\n(7.6k)\n(10.2k)\n(10.1k)\n(9.9k)\n5-shot\n88.4\u00b12.6\n75.7\u00b11.5\n79.3\u00b11.1\n62.8\u00b11.7\n88.0\u00b13.0\n82.5\u00b13.8\n94.7\u00b13.5\n63.6\u00b18.5\n88.0\u00b11.0\n(48.0k)\n(79.2k)\n(143.3k)\n(287.5k)\n(24.5k)\n(52.5k)\n(62.6k)\n(63.5k)\n(61.7k)\n16-shot\n93.3\u00b12.3\n75.5\u00b15.1\n80.9\u00b15.0\n66.4\u00b13.6\n91.3\u00b11.5\n83.7\u00b10.6\n96.5\u00b12.5\n67.1\u00b14.0\n88.3\u00b12.1\n(136.8k)\n(229.9k)\n(405.1k)\n(817.6k)\n(70.3k)\n(149.0k)\n(177.9k)\n(179.4k)\n(175.2k)\nDLN-1\n95.2\u00b15.0\n77.1\u00b14.7\n74.3\u00b11.5\n69.1\u00b12.5\n91.1\u00b13.2\n89.5\u00b12.1\n93.1\u00b15.0\n82.1\u00b13.8\n85.9\u00b11.5\n(77.2k)\n(29.9k)\n(52.3k)\n(68.5k)\n(65.4k)\n(120.7k)\n(46.5k)\n(47.1k)\n(38.2k)\nJ\nPricing\nWe keep track the number of tokens we interact with GPT-3 via its online API. According to\nOpenAI\u2019s pricing policy, user pays for both the input tokens (prompts) and the output tokens. Using\nthe Hyperbaton task as an example, while training a 1-layer LN, the total number of tokens we use is\n2,941,360. For a 2-layer DLN, the total number of tokens we use is 13,654,962. According to the\ncurrent price for GPT-3 ($0.02/1k tokens), a single run of a 1-layer and 2-layer DLN cost roughly 59\nUSD and 273 USD, respectively.\nIn Table 11, we report the cost (lower is better) in terms of total number of tokens for the test set\n(prompts included). We emphasize the cost at the testing time because it is more relevant in real-world\ndeployment and the training cost is one-off. We can see DLN-1 improves over ICL on 5 out of 9\ntasks on GPT-4 at a comparable token cost. Some tasks do not benefit from ICL (i.e. reasoning tasks)\nwhile other tasks like Subj, Trec, and Hyper. benefit significantly.\n24\n"
  },
  {
    "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
    "link": "https://arxiv.org/pdf/2306.12929.pdf",
    "upvote": "10",
    "text": "Quantizable Transformers: Removing Outliers by\nHelping Attention Heads Do Nothing\nYelysei Bondarenko, Markus Nagel, Tijmen Blankevoort\nQualcomm AI Research\u2217\nAmsterdam, The Netherlands\n{ybond, markusn, tijmen}@qti.qualcomm.com\nAbstract\nTransformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI signif-\nicantly. Due to their size, the capability of these networks has increased tremen-\ndously, but this has come at the cost of a significant increase in necessary com-\npute. Quantization is one of the most effective ways to reduce the computational\ntime and memory consumption of neural networks. Many studies have shown,\nhowever, that modern transformer models tend to learn strong outliers in their\nactivations, making them difficult to quantize. To retain acceptable performance,\nthe existence of these outliers requires activations to be in higher bitwidth or the\nuse of different numeric formats, extra fine-tuning, or other workarounds. We\nshow that strong outliers are related to very specific behavior of attention heads\nthat try to learn a \u201cno-op\u201d or just a partial update of the residual. To achieve\nthe exact zeros needed in the attention matrix for a no-update, the input to the\nsoftmax is pushed to be larger and larger during training, causing outliers in\nother parts of the network. Based on these observations, we propose two sim-\nple (independent) modifications to the attention mechanism - clipped softmax\nand gated attention. We empirically show that models pre-trained using our\nmethods learn significantly smaller outliers while maintaining and sometimes\neven improving the floating-point task performance. This enables us to quantize\ntransformers to full INT8 quantization of the activations without any additional\neffort. We demonstrate the effectiveness of our methods on both language models\n(BERT, OPT) and vision transformers. Our source code is available at https:\n//github.com/qualcomm-ai-research/outlier-free-transformers.\n1\nIntroduction\nQuantization has been one of the most impactful ways to reduce the computational complexity of\ntransformer networks. Previous work has shown that quantizing networks to 4-bit weights is possible\nwithout losing too much accuracy [66, 69]. Some research even shows 4-bit weights might be optimal\nwhen trading off model size and bit-width [12].\nHowever, quantizing transformers is not always trivial. When quantizing the activations of a trans-\nformer, significant problems arise with outliers in specific layers. This has been noted by several\nresearchers that suggest fixes to transformers after training to ameliorate their effect [13, 67]. These\nmethods are frequently tedious and either require retraining the network, require implementing\nspecific hardware for input-channel quantization [13] or require parts of the activations to still be in\nhigher bit-widths, reducing the effectiveness of the activation quantization [67].\nIn this paper, we set out to solve the transformer outlier problem entirely by changing the architecture\nof the network itself. We hope to make transformers easy to quantize from the get-go without needing\n\u2217Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.12929v2  [cs.LG]  9 Nov 2023\nany post-processing. To do so, we thoroughly analyze why these outliers appear. Previous work\nhas found the existence of these outliers [4, 13], but in our work, we come to a fuller understanding\nof these outlying values. We find that the outliers occur because attention heads are trying not to\nupdate the hidden state, and in the process, strong outliers appear due to the softmax function. This\nhappens for language and vision transformers and different specific transformer architectures. This\nunderstanding is the foundation for two new tweaks we suggest to transformer architectures that can\nremove the problem of the outliers entirely.\n2\nBackground and related work\nIn this section, we briefly cover the basics of neural network quantization and discuss why modern\ntransformers are difficult to quantize.\nQuantization\nOne of the most powerful ways to decrease the computational time and memory\nconsumption of neural networks is quantization, which uses low-bit representations for the weights\nand activation tensors. On top of that, using low-bit fixed-point representations, such as INT8, one\ncan further reduce energy consumption since the fixed-point operations are more efficient than their\nfloating-point counterparts [23, 59].\nWe simulate the quantization process in floating-point according to Jacob et al. [26]. We use the\nfollowing definition of the quantization function:\nbx := q (x; s, z, b) = s \u00b7\n\u0010\nclip\n\u0010jx\ns\nm\n+ z; 0, 2b \u2212 1\n\u0011\n\u2212 z\n\u0011\n,\n(1)\nwhere x denotes the quantizer input (i.e., network weights or activations), s \u2208 R+ the scale factor or\nthe step-size, z \u2208 Z the zero point, and b \u2208 N the bitwidth. \u230a\u00b7\u2309 denotes the round-to-nearest-integer\noperator. This quantization scheme is called uniform affine or asymmetric quantization [24, 32, 76]\nand it is one of the most commonly used quantization schemes because it allows for efficient\nimplementation of fixed-point arithmetic. In the case of symmetric quantization, we restrict the\nquantization grid to be symmetric around z = 0.\nIn this work, we focus on post-training quantization (PTQ) methods, which take a pre-trained FP32\nnetwork and convert it directly into a fixed-point network without the need for the original training\npipeline [2, 5, 7, 25, 32, 35, 41, 43, 44, 75]. These methods require either no data or only a small\ncalibration dataset and are easier to use compared to quantization-aware training (QAT, Bhalgat et al.\n3, Esser et al. 16, Gupta et al. 21, Jacob et al. 26, Krishnamoorthi 32) methods that have you train the\nentire network for more epochs. For more details on neural network quantization, we refer the reader\nto [19, 46].\nOutliers in Transformers\nMultiple studies have shown that modern transformer-based language\nmodels tend to learn outliers in weights and activations [4, 13, 31]. These outliers are present only\nin a small fixed set of embedding dimensions, but they appear regularly and consistently across\nmultiple layers and data sequences. It was also shown that those outliers play a crucial role in the\nmodel predictions and clipping them or by setting to zero the corresponding parameters significantly\ndegrades the model task performance [31, 49]. The strongest in magnitude outliers typically appear at\nthe output of the feed-forward network, FFN, although Dettmers et al. [13] showed that for big enough\ntransformer-based language models they start appearing after every linear layer, including query, key,\nand value projection layers. This phenomenon holds for many tasks, training objectives and models\n(both encoder and decoder transformers), including BERT [14], RoBERTa [37], DistilBERT [53],\nMobileBERT [55], ELECTRA [9], BART [33], XLNet [68], GPT-2 [50], and OPT [74].\nBecause of these strong outliers, applying per-tensor PTQ for the FFN\u2019s output and the residual\nsum will likely cause a notable error because of the following trade-off between the range and the\nprecision. On the one hand, using a large quantization range for small-ranged values leads to a loss in\nrepresentation (high rounding error). On the other hand, a small quantization range for large values\nleads to a very high clipping error. For the case of significant transformer outliers, frequently, no\ngood trade-off can be found between the rounding and clipping error, resulting in an overall high\nerror.\nThere have been numerous attempts to fix the issue of transformer quantization [4, 12, 13, 17, 27, 28,\n51, 54, 62, 63, 69, 71]. Most of these approaches resort to finer quantization granularity (row-wise,\n2\n(a) FFN output in layer #10\n(b) FFN output in layer #11\nFigure 1: Histograms of outlier counts vs. token positions (blue) and hidden dimensions (green),\nrecorded from the MNLI-m validation set on BERT-base. We use zero-based indexing for dimensions.\nchannel-wise, group-wise weight and activation quantization), use higher bitwidth and/or different\nnumeric format to represent those outliers better or require extra fine-tuning (in the form of QAT\nand/or knowledge distillation). In other words, they adapt quantization to work with outliers, which\noften comes at the expense of general applicability or extra inference overhead.\nIn contrast, in this work, we want to address the root cause of the problem and understand why\noutliers are learned in the first place and suggest a new pre-training protocol that significantly reduces\nthe magnitude of outliers yielding way more quantization-friendly models that can be effortlessly\nquantized using PTQ without strong degradation of performance.\n3\nOutlier analysis\nOutliers in BERT models\nIn Section 2 we discussed that outliers are present only in a few\ndesignated embedding dimensions but they appear regularly and consistently across multiple layers\nand data sequences. We also discussed that the strongest magnitude outliers in BERT typically appear\nat the output of FFN in the last encoder layers.\nWe start by taking the pre-trained BERT-base-uncased checkpoint from HuggingFace [65] and fine-\ntune it on MNLI dataset from the well-known GLUE benchmark [61] (see experimental details\nin C.1). To identify the outlier dimensions, we pass the MNLI-m validation set through the network\nand record all outliers1 at the FFN output in layers #10 and #112. As we can see in Figure 1, there\nare indeed only a few hidden dimensions where outliers ever occur. We also notice that the majority\nof outliers (> 97%) correlate with the position of delimiter tokens \u2013 [SEP], \u201c.\u201d, and \u201c,\u201d.\nTo better understand the role of those outliers, we analyze the attention patterns of the corresponding\nattention heads. BERT-base uses multi-head attention with nheads = 12 and each head operating on a\nconsecutive subset of dhead = 64 features. Therefore, the hidden dimension #180, which happens\nto have the highest outlier count in both layers #10 and #11, corresponds to attention head #3. In\nFigure 2 (and more examples in Appendix A.1) we show examples of the attention matrices, values\nand their product for that head.\nA common pattern we found is that the attention head assigns almost all of its probability mass\nto [SEP] tokens, and other less informative tokens like dots/commas, while these tokens also have\nsmall values in V associated with those tokens. This results in a small magnitude product between the\ntwo (see Figure 2a). This effectively corresponds to a (soft) no-update of the hidden representation,\nwhere only small noise is added after the residual. In other cases (Figure 2b and 2c), we observe that\na significant portion of attention probability is still spent on delimiter tokens. However, by allocating\nsome of the probability mass on other tokens (together with the small values for the delimiter tokens),\nthis results in a (soft) selective update of the hidden representation.\nThese patterns in self-attention seem to be a learned \u201cworkaround\u201d for the limitations of having\nthe softmax and the residual connections in cases where the attention head does not want to update\nthe representation of some or all of the tokens. These observations are in line with Clark et al.\n[8], Kovaleva et al. [30] that also argued that attending exclusively or almost exclusively to delimiter\ntokens such as [SEP], periods/commas acts as a \u201cno-op\u201d when the attention head\u2019s function is not\napplicable.\n1We follow Bondarenko et al. [4] and consider outliers as values that exceed 6 standard deviations from the\nmean of the corresponding activation tensor.\n2We use 1-based indexing for encoder layers and attention heads throughout the paper.\n3\n(a) Attention layer #11, data sequence #1\n(b) Attention layer #11, data sequence #5\n(c) Attention layer #10, data sequence #5\nFigure 2: Visualization of the patterns in the self-attention, specifically the attention probabilities,\nvalues, and their product (left, middle and right columns, respectively), in attention head #3 for\nBERT-base, computed on several data sequences from MNLI-m validation set.\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 3: A summary of our outlier analysis for ViT demonstrated on a random image from ImageNet\nvalidation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention\nweight spent on every patch (matrix of attention probabilities summed over rows) in the attention\nhead #1, layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude\nof values for outlier and non-outlier patches.\nOutliers in ViT\nWe conduct a similar analysis for Vision transformer [15] trained on ImageNet [52].\nFor this study, we use a pre-trained checkpoint following our experimental setup from Section 5.\nWe highlight our findings in Figure 3 and provide more examples in Appendix A.2. Our analysis\nshows many similarities to the BERT case. Instead of delimiter tokens, the majority of outliers seem\nto correlate with some random uninformative patches (e.g., in the background). We also see that the\ncorresponding attention head in the next layer allocates the majority of attention probabilities to the\nsame patches. Finally, those outlier patches on average have a distinctly smaller magnitude of values\ncompared to non-outlier ones, leading to similar no-update behavior. The fact that those values are\nnot as close to zero as it was in the BERT case might be related to the smaller model capacity3, or a\nrelatively shorter training procedure.\nHypothesis\nBased on these observations, we pose the following hypothesis on how this behavior of\nattention heads is related to outliers:\n1. In order for an attention block to not update a representation of a token on the residual, some\nattention heads want to allocate most of their attention probability mass to some fixed and common\nset of tokens that have a low information content (e.g., delimiter tokens or background patches)\nthat can be learned to have a small value function output.\n3We use ViT/S-16 configuration that has only 22M parameters.\n4\n2. From the definition of the softmax function4, it is easy to see that this would require an input of\nthe softmax to have a relatively big dynamic range (Figure 4, 1 ). In fact, in the limit case where\nsoftmax is exactly zero, this would require an infinite dynamic range:\nsoftmax (x)i = 0\n\u21d4\n\u2203j \u0338= i, xj \u2212 xi = +\u221e\n(2)\n3. Since Layer Normalization ([1], 2 ) normalizes the outliers, the magnitude of the FFN output in\nthe previous layer ( 3 ) has to be very high to still produce a sufficiently big dynamic range after\nthe LayerNorm. Note, that this is also applicable for the transformer models with LayerNorm\napplied prior to the self-attention or linear transformations instead, a variant adopted by GPT,\nOPT, and many vision transformers [15, 38, 57, 58].\n4. Finally, as softmax will never output exact zeros, it will always back-propagate a gradient signal\nto grow bigger outliers5. The outliers will thus tend to become stronger in magnitude, the longer\nthe network is trained.\n4\nMethod\nFigure 4: A schematic illus-\ntration of the attention layer\nin BERT. Hidden activation\ntensor is denoted by x. \u2295 is\nan element-wise addition. A\nproblematic output of the FFN\nthat generates largest in magni-\ntude outliers is highlighted in\nred. Notice how those outliers\nin the previous layer influence\nthe behavior in the attention\nmechanism in the next layer.\nIn this section, we introduce our proposed modifications for the\nsoftmax attention mechanism. Based on our insights from Section 3,\nthe core idea of these modifications is to grant the model the ability\nto produce very small the magnitude (or even exact zeros) output of\nattention function, without producing outliers.\nRecall that the self-attention [60] is defined as follows:\nAttention(x) := softmax\n\u0012Q(x)K(x)T\n\u221adhead\n\u0013\nV (x)\n(3)\nwhere Q, K and V are learnable linear projections of the input x.\nMost modern transformer models employ a multi-headed variant\nof self-attention, where dmodel features are partitioned into nheads\ngroups of dhead features, and the final output is the concatenation of\nthe outputs of (3) applied to each group.\n4.1\nClipped softmax\nFirst, we propose to replace softmax function in (3) with the follow-\ning clipped softmax:\nclipped_softmax(x; \u03b6, \u03b3) :=\nclip ((\u03b6 \u2212 \u03b3) \u00b7 softmax(x) + \u03b3, 0, 1) .\n(4)\nHere x is the input and \u03b6 \u2265 1, \u03b3 \u2264 0 are the stretch factors which\nare hyper-parameters of the method. Similar formulation was used\nbefore for sigmoid function [40, 45]. We can view (4) as stretching\nthe output of the softmax from (0, 1) to (\u03b3, \u03b6) and then clipping back\nto (0, 1) so that we can represent exact zeros if \u03b3 < 0 and exact ones\nif \u03b6 > 1. Specifically, the values of the softmax larger than 1\u2212\u03b3\n\u03b6\u2212\u03b3\nare rounded to one whereas values smaller than\n\u2212\u03b3\n\u03b6\u2212\u03b3 are rounded to\nzero.\nWith this drop-in replacement, we can achieve exact zeros (and ones) with a finite range for the\nsoftmax input. In addition to that, whenever values are clipped they will not give a gradient, preventing\nthe outliers to grow further.\n4softmax (x)i = exp (xi) / Pd\nj=1 exp (xj)\n5Let y = softmax (x). Then \u2202yi\n\u2202xj \u0338= 0 \u2200i, j.\n5\n4.2\nGated attention\nAn alternative way of architecting the model to have a small attention output without outliers is to\nequip it with an explicit conditional gating mechanism, as shown in Figure 5. The idea is that the\nmodel can use the gating to either keep or nullify the update to the representation of certain tokens\nand not rely on the attention probabilities and values to achieve the same outcome.\nSpecifically, we propose the following modification to the attention function:\nGated_attention(x) := sigmoid (G(x)) \u2299 softmax\n\u0012Q(x)K(x)T\n\u221adhead\n\u0013\nV (x).\n(5)\nHere G is the gating function, \u2299 is an element-wise multiplication across the token axis and everything\nelse remains the same as in (3). The gating function G is parameterized by a small neural network\nthat is learned jointly with the rest of the model. We replace the attention formulation with the\nproposed variant in every layer on the transformer network.\nFigure 5: A schematic il-\nlustration of our proposed\ngated attention.\nGating module design\nRecall that the input to the attention layer x\nhas shape (T, dmodel) that is reshaped into (nheads, T, dhead) for the\nmulti-headed self-attention, where T is the sequence length. We chose\nto define the gating function on a per-head basis. For each head i \u2208\n{1, . . . , nheads}, we specify Gi : Rdhead \u2192 R and the output of the\ngating module is \u03c0i \u2208 RT that is computed as follows:\nb\u03c0i,t = Gi(xi,t,:) \u2200t \u2208 {1, . . . , T}\n(6)\n\u03c0i,: = sigmoid(b\u03c0i,:),\n(7)\nnote that gating modules are shared between different token positions\nbut not shared across attention heads.\nWe want our gating module to be as lightweight as possible. To start\nwith, we experiment with Gi\u2019s parameterized by a single linear layer.\nThis gives us a gating module that is computationally inexpensive\nand has a memory overhead of just nheads \u00b7 (dhead + 1) \u223c dmodel extra\nparameters (which is equivalent to 1 extra token) per attention layer6.\nWe also investigate the effect of using several other gating functions in\nAppendix B.1.\n5\nExperiments\nIn this section, we evaluate the proposed modifications to self-attention on several language models\n(BERT, OPT) and the vision transformers (ViT). We first test the different hyperparameters for the\nmethods and provide insight into how they work. Then we set out to test our method in terms of\naccuracy, and the difference in quantization improvement after training. All detailed hyperparameters\nof our experiments are in Appendix C.\nBERT\nWe experiment with BERT-base-uncased (109M parameters) pre-training using the masked\nlanguage modeling (MLM) objective. Following [14], we use the concatenation of the training\nsets of BookCorpus [77] and English Wikipedia7. We implement our methods in PyTorch [48] and\nuse training and evaluation pipelines from HuggingFace libraries [20, 34, 65]. We follow closely\nthe pre-training procedure from [14]. To speed up training and experimentation, we train with a\nmaximum sequence length of 128 for the whole duration of the training. We evaluate on Wikipedia\nvalidation set and report the MLM perplexity.\nOPT\nWe experiment with a 125M sized variant of OPT [74] pre-training using the causal language\nmodeling (CLM) objective. Due to compute constraints, we train the model on the same dataset that\nwas used for BERT pre-training (BookCorpus + Wikipedia) with a maximum sequence length of 512\n6For instance, in case of BERT-base, this amounts to less than 0.009% of the total model size.\n7Specifically, we use the English subset of Wiki-40b, https://huggingface.co/datasets/wiki40b, that\ncontains cleaned-up text of English Wikipedia and training/validation splits.\n6\n\u03b3\n\u03b6\nFP16 ppl.\u2193\nMax inf. norm\nAvg. kurtosis\nW8A8 ppl.\u2193\n0\n1\n4.49\u00b10.01\n735\u00b155\n3076\u00b1262\n1294\u00b11046\n(= Vanilla)\n0\n1.003\n4.48\u00b10.01\n715\u00b1335\n2159\u00b1238\n451\u00b157\n0\n1.03\n4.49\u00b10.00\n741\u00b166\n1707\u00b11249\n1469\u00b1646\n\u22120.003\n1\n4.46\u00b10.00\n688\u00b164\n2149\u00b1110\n636\u00b1566\n\u22120.03\n1\n4.41\u00b10.01\n20\u00b11\n80\u00b16\n4.55\u00b10.01\n\u22120.003\n1.003\n4.47\u00b10.00\n683\u00b123\n2494\u00b11205\n268\u00b1120\n\u22120.03\n1.03\n4.43\u00b10.03\n22\u00b13\n73\u00b18\n4.56\u00b10.05\nTable 1: The impact of clipped softmax hyperparameters on BERT-base.\nand batch size of 192. Similar to our BERT experiments, we use training and evaluation pipelines\nfrom HuggingFace libraries. We evaluate on Wikipedia validation set and report the CLM perplexity.\nViT\nFinally, we explore the effectiveness of proposed techniques on vision transformer [15] (ViT-\nS/16 configuration, 22M parameters) trained on ImageNet-1K [11, 52]. For these experiments, we\nadopt the training and validation pipelines from PyTorch Image models library [64]. We report top-1\naccuracy on the validation set of ImageNet.\nQuantization setup\nIn all experiments, after the model is trained, we apply 8-bit PTQ. We use\nuniform affine quantization \u2013 symmetric weights, asymmetric activations \u2013 with the static activation\nrange setting, as discussed in Section 2. We quantize all weights and activations (both input and\noutput), except the final linear layer for BERT and OPT models. We explore several choices of range\nestimation (see Appendix C.4) and report the best configuration for each experiment, based on the\nmodel performance. We repeat each PTQ experiment 3 times with different random seeds8 and report\nmean and standard deviation for accuracy/perplexity.\nWe train each network two times with different random seeds and report mean and standard deviation.\nTo assess the amount of outliers in the trained model, we use two metrics: the maximum \u2225x\u2225\u221e\naveraged across the validation set, and kurtosis of x averaged across all layers, where x is the\noutput of an attention layer. These metrics have been shown to correlate well with the model\nquantizability [4, 6].\n5.1\nThe impact of clipped softmax hyperparameters (\u03b3 and \u03b6)\nWe investigate the effect of different values of the clipped softmax stretch parameters and present\nthe results in Table 1. We can see that most of the improvement happens when we use \u03b3 < 0\n(clipping at zero). For instance, using the value of \u03b3 = \u22120.03 leads to a significantly smaller infinity\nnorm, kurtosis, and quantized model perplexity, compared to the baseline. It is also clear that in\nthe limit |\u03b3| \u2192 0 we approach the vanilla softmax attention. Using \u03b6 > 1 (clipping at one) yields\nsimilar results to the vanilla softmax. Finally, when we combine both \u03b3 < 0 and \u03b6 > 1, for which\nthe results seem similar to just clipping at 0. We, therefore, conclude that for dampening outliers,\nonly the lower-range clipping allows exact zeros matter. Going forward we use only \u03b3 < 0 and in\nAppendix B.5 we confirm that \u03b6 > 1 is not required for ViT.\nThese observations are in line with our hypothesis that by giving the model the mechanism for\nrepresenting exact zeros in the attention, we don\u2019t need to learn the strong outliers.\n5.2\nClipped softmax \u03b3 vs. sequence length\nAs having an extra hyper-parameter that needs to be tuned per model or setup is generally not\ndesirable, we study the sensitivity of the stretch factor \u03b3 and its relation with the sequence length\nT. Recall that the matrix of attention probabilities P has dimensions T \u00d7 T and each row sums up\nto one. Because of that, the average value in P is 1/T. It is reasonable to assume that if we define\n8Different random subsets of training data are used for quantizer range estimation.\n7\n(a) Relative FP16 log-perplexity\n(b) Maximum infinity norm\nFigure 6: The performance of clipped softmax using \u03b3 = \u2212\u03b1/T parameterization on BERT-6L. (a)\nRelative (compared to vanilla softmax pre-training) FP16 log-perplexity \u2191 on Wikitext validation set.\n(b) Maximum infinity norm of the attention layer output (note the logarithmic y-axis).\n(a) BERT-6L\n(b) ViT\nFigure 7: The performance of Linear gated attention using different bias initialization settings.\n\u03b3 := \u2212 \u03b1\nT , where \u03b1 > 0 is a new hyperparameter, there might be a set or a range of values of \u03b1 that\nworks well across different sequence lengths.\nTo study this, we train a 6-layer variant of BERT-base (BERT-6L) for 500000 steps on WikiText-\n103 [42] with a batch size of 128 with several values of maximum sequence lengths T\n\u2208\n{32, 64, 128, 192, 256} and values of \u03b1 \u2208 {1/4, 1/2, 1, 2, 4, 8}. As we can see from Figure 6,\nusing a clipped softmax with \u03b1 \u2208 [2, 4] significantly dampens the magnitude of outliers while\nmaintaining good FP16 perplexity across all explored sequence lengths.\n5.3\nThe impact of bias initialization in gated attention\nIn all our gated attention experiments, we randomly initialize the weights of G, following [22]. By\ninitializing the bias to a specific value, however, we can set gates to be more open or more closed\ninitially. More open at the start means we initialize closer to the original network, but given the\nexponential nature of the gate it might take many iterations for the gate to learn to close. Similarly,\nif the gates are all closed at the start, we deviate too far from the original model training, causing a\npotential decrease in performance. Assuming Linear Gi\u2019s with small initial weights, if we set the\nbias to the value of binit, then Gi(\u00b7) \u2248 binit and \u03c0i(\u00b7) = sigmoid(Gi(\u00b7)) \u2248 sigmoid(binit) =: \u03c0init, at\nthe start of training.\nWe study the effect of different values of binit for Linear gated attention on BERT-6L and ViT. We set\nthe bias for all Gi\u2019s to the same value of binit. For BERT-6L, we use the same setup as in Section 5.2,\nwith a fixed sequence length of 128. For ViT, we use the main setup, except we train it for 150 epochs\ninstead of 300.\nIn Figure 7 we see in both BERT and ViT cases that using bias with very high \u03c0init generally performs\nsimilarly to the vanilla attention (comparable floating-point performance but strong outliers and poor\nquantized performance) while setting bias to have very low \u03c0init dampens outliers quite well but leads\nto strong degradation in the floating-point and quantized performance. The reasonable ranges of \u03c0init\nseems to be around [0.25, 0.9] for BERT and [0.1, 0.5] for ViT. The wide range indicates the relative\nrobustness of our method to this hyperparameter.\n8\nModel\nMethod\nFP16/32\nMax inf. norm\nAvg. kurtosis\nW8A8\nBERT\n(ppl.\u2193)\nVanilla\n4.49\u00b10.01\n735\u00b155\n3076\u00b1262\n1294\u00b11046\nClipped softmax\n4.39\u00b10.00\n21.5\u00b11.5\n80\u00b16\n4.52\u00b10.01\nGated attention\n4.45\u00b10.03\n39.2\u00b126.0\n201\u00b1181\n4.65\u00b10.04\nOPT\n(ppl.\u2193)\nVanilla\n15.84\u00b10.05\n340\u00b147\n1778\u00b1444\n21.18\u00b11.89\nClipped softmax\n16.29\u00b10.07\n63.2\u00b18.8\n19728\u00b17480\n37.20\u00b12.40\nGated attention\n15.55\u00b10.05\n8.7\u00b10.6\n18.9\u00b10.9\n16.02\u00b10.07\nViT\n(acc.\u2191)\nVanilla\n80.75\u00b10.10\n359\u00b181\n1018\u00b1471\n69.24\u00b16.93\nClipped softmax\n80.89\u00b10.13\n73.7\u00b114.9\n22.9\u00b11.6\n79.77\u00b10.25\nGated attention\n81.01\u00b10.06\n79.8\u00b10.5\n19.9\u00b10.3\n79.82\u00b10.11\nTable 2: A summary of results for our proposed methods applied on BERT, OPT-125m, and ViT.\nModel\nMethod\nFP16\nMax inf. norm\nAvg. kurtosis\nW8A8\nOPT-350m\n(ppl.\u2193)\nVanilla\n13.19\n253\n2689\n37.52\u00b13.84\nGated attention\n13.01\n65.4\n261\n14.42\u00b10.06\nOPT-1.3B\n(ppl.\u2193)\nVanilla\n12.13\n428\n2756\n989.6\u00b1175\nGated attention\n12.21\n67.2\n444\n29.95\u00b10.42\nTable 3: The performance of gated attention applied on bigger variants of OPT model.\n5.4\nMain results\nWe summarize our main set of results in Table 2. As we can see, in almost all cases, both of our\nproposed techniques dampen the outliers\u2019 magnitude to a great extent, reduce the kurtosis, and yield\nmodels with significantly higher quantized performance, which is close to the original FP16/32\nperformance. In addition to that, for each model, at least one of our methods also improves the\nfloating-point task performance. We hypothesize this is because the network is helped with learning\nthe \u201cno-op\u201d updates more easily. However, we are cautious about the improved performance as this\nis not consistent across all hyper-parameters and it is unclear if it generalizes to more architectures\nand larger models.\nThe only case where our method failed to perform well was the clipped softmax applied to OPT. At\nthe moment, we do not have an explanation of why this is the case and leave it for future work. We\nlist selected hyper-parameters and show extended results in Appendix B. We also show the results of\nour proposed methods quantized to lower bitwidths in Appendix B.7.\nResults for bigger models\nWe study the question of scalability of our methods to larger models.\nIn Table 3 we show the gated attention results for 350m and 1.3B variants of OPT. Due to compute\nconstraints, we trained networks for 105 steps with batch size of 256 and the rest is the same as in\nour main pre-training setup. As we can see, our proposed gated attention is also very effective at\ndampening the outliers and significantly improving the quantized model performance when applied\nto bigger models. We further study in Appendix B.6 how gated attention can decrease outliers when\nfine-tuning bigger pre-trained models with outliers.\n5.5\nQualitative results\nIn Figure 8 we compare the learned attention patterns using vanilla softmax and our proposed\nmethods (more examples in Appendix A.1). As we can see, both methods can represent a partial/soft\nno-op behavior, but in case of our methods this does not require strong outliers elsewhere in the\nnetwork. Note that we found similar patterns in multiple attention heads, but the exact head indices\nwhere we observed such patterns depend on random initialization. In the case of clipped softmax,\nsmaller attention weights are generally more diffused while higher weights are more saturated (which\ncomes from the stretching and clipping). In the case of gated attention, the output of the softmax\nis significantly different since the update of the hidden representation is now further modulated by\ngating probabilities.\n9\n(a) Vanilla softmax (Attention layer #11, head #3)\n(b) Clipped softmax (Attention layer #11, head #8)\n(c) Gated attention (Attention layer #11, head #5)\nFigure 8: Visualization of the self-attention patterns for BERT-base trained using vanilla and our\nproposed techniques, computed on data sequence #5 from MNLI-m validation set. (a), (b): attention\nprobabilities, values, and their product. (c): gating probabilities \u03c0 = sigmoid (G (x)), attention\nprobabilities (output of softmax), values, and their combined product.\n6\nDiscussion\n\u201cNo-op\u201d behavior\nIt is interesting to note that the identified \u201cno-op\u201d behavior is likely not limited\nto transformers and that convolutional architectures likely learn something similar. We also see that\ndespite the network trying to learn a full \u201cno-op\u201d, still a small amount of noise is added to each\nresidual, which may constitute a form of network regularization. Investigating this further might give\nus a clue as to why neural networks generalize despite being significantly overparametrized if many\nparameters are rendered unused by not updating the representation in later layers [72].\nLimitations\nWhile we studied the scalability of our method for models up to 1.3B size, we haven\u2019t\nexplored the case of very large transformers that are trained for way longer. Given the fundamental\nunderstanding of the issue underlying our solutions, we expect the same effect on larger-scale models.\nWe show a very small improvement in FP16/FP32 performance due to our methods, but we do not\ndeem our results exhaustive enough to claim that this will hold in general. Lastly, our methods\ndo have a hyperparameter each, although we show that both methods are relatively robust to its\nhyperparameter, having one is never optimal.\nImpact\nAs our methods help transformers to be more efficient, we expect only positive outcomes\nof our work. Making neural networks more efficient will help with their high power consumption at\ninference. It further helps to move inference from the cloud to edge devices which can overcome\npotential privacy concerns. We cannot fathom any negative impact from our work that is not severely\nconstrued.\n7\nConclusions\nWe have thoroughly analyzed the activation outlier problem that makes transformers difficult to\nquantize. We showed that transformer networks try to learn not to update residuals and that by\ndoing so, through the combination of the softmax, residual connections and LayerNorm, significant\noutliers appear in transformers. Based on this insight, we proposed two methods to address this at the\ncore \u2013 clipped softmax and gated attention. These structural changes to transformers give similar,\nif not better, floating-point performance after training but significantly improve the post-training\nquantization results. We hope that with these two architectural changes to transformers, anyone can\ntrain high-performance transformers that are easy to quantize and can benefit from efficient integer\ninference.\n10\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016.\n[2] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of convolution\nnetworks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018.\n[3] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving\nlow-bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020.\n[4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the chal-\nlenges of efficient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 7947\u20137969, Online and Punta Cana, Dominican Republic, Novem-\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.627. URL\nhttps://aclanthology.org/2021.emnlp-main.627.\n[5] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A\nnovel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13169\u201313178, 2020.\n[6] Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser, et al. Robust\nquantization: One model to rule them all. Advances in neural information processing systems, 33:\n5308\u20135317, 2020.\n[7] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for\nefficient inference. In ICCV Workshops, pages 3009\u20133018, 2019.\n[8] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at?\nan analysis of BERT\u2019s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages 276\u2013286, Florence, Italy, August 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology.org/\nW19-4828.\n[9] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text\nencoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n[10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition workshops, pages 702\u2013703, 2020.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.\nIeee, 2009.\n[12] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv\npreprint arXiv:2212.09720, 2022.\n[13] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplica-\ntion for transformers at scale. In Advances in Neural Information Processing Systems, 2022.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics.\ndoi: 10.18653/v1/N19-1423.\nURL https://aclanthology.org/\nN19-1423.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[16] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S.\nModha. Learned step size quantization. In International Conference on Learning Representations (ICLR),\n2020.\n11\n[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herv\u00e9 J\u00e9gou, and Armand\nJoulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320,\n2020.\n[18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n[19] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey\nof quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n[20] Sylvain Gugger, Lysandre Debu, Thomas Wolf, Philipp Schmid, Zachary Mueller, and Sourab Mangrulkar.\nAccelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/\nhuggingface/accelerate, 2022.\n[21] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited\nnumerical precision. In International conference on machine learning, pages 1737\u20131746. PMLR, 2015.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification. In Proceedings of the IEEE international conference\non computer vision, pages 1026\u20131034, 2015.\n[23] M. Horowitz. 1.1 computing\u2019s energy problem (and what we can do about it). In 2014 IEEE International\nSolid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10\u201314, 2014. doi: 10.1109/\nISSCC.2014.6757323.\n[24] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural\nnetworks: Training neural networks with low precision weights and activations. The Journal of Machine\nLearning Research, 18(1):6869\u20136898, 2017.\n[25] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural\nquantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020.\n[26] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig\nAdam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-\narithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2704\u20132713, 2018.\n[27] Minsoo Kim, Sihwa Lee, Sukjin Hong, Du-Seong Chang, and Jungwook Choi. Understanding and\nimproving knowledge distillation for quantization-aware training of large transformer encoders. arXiv\npreprint arXiv:2211.11014, 2022.\n[28] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only\nbert quantization. arXiv preprint arXiv:2101.01321, 2021.\n[29] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[30] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of\nBERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n4365\u20134374, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.\n18653/v1/D19-1445. URL https://aclanthology.org/D19-1445.\n[31] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimen-\nsions that disrupt transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP\n2021, pages 3392\u20133405, 2021.\n[32] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper.\narXiv preprint arXiv:1806.08342, 2018.\n[33] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages 7871\u20137880, Online, July 2020. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/\n2020.acl-main.703.\n12\n[34] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan\nChhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry,\nAngelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl\u00e9ment Delangue, Th\u00e9o Matussi\u00e8re, Lysan-\ndre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran\u00e7ois Lagunas, Alexander\nRush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstra-\ntions, pages 175\u2013184, Online and Punta Cana, Dominican Republic, November 2021. Association for\nComputational Linguistics. URL https://aclanthology.org/2021.emnlp-demo.21.\n[35] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi\nGu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint\narXiv:2102.05426, 2021.\n[36] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware\nweight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n[37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 10012\u201310022, 2021.\n[39] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[40] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0\nregularization. arXiv preprint arXiv:1712.01312, 2017.\n[41] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Re-\ncovering neural network quantization error through weight factorization. In International Conference on\nMachine Learning, pages 4486\u20134495. PMLR, 2019.\n[42] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843, 2016.\n[43] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through\nweight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 1325\u20131334, 2019.\n[44] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or\ndown? Adaptive rounding for post-training quantization. In International Conference on Machine Learning\n(ICML), 2020.\n[45] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or\ndown? adaptive rounding for post-training quantization. In International Conference on Machine Learning,\npages 7197\u20137206. PMLR, 2020.\n[46] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and\nBlankevoort Tijmen. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295,\n2021.\n[47] Vinod Nair and Geoffrey E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In\nProceedings of the 27th International Conference on Machine Learning, pages 807\u2013814. Omnipress, 2010.\n[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In\nNeural Information Processing Systems (NeuRIPS). 2019.\n[49] Giovanni Puccetti, Alessio Miaschi, and Felice Dell\u2019Orletta. How do BERT embeddings organize linguistic\nknowledge? In Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge\nExtraction and Integration for Deep Learning Architectures, pages 48\u201357, Online, June 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.deelio-1.6. URL https://aclanthology.org/\n2021.deelio-1.6.\n13\n[50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. 2019.\n[51] Bita Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky,\nSarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai\nChe, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav Das, Saurabh Tiwary, Steve Reinhardt,\nSitaram Lanka, Eric Chung, and Doug Burger. Pushing the limits of narrow precision inferencing at cloud\nscale with microsoft floating point. In Neural Information Processing Systems (NeurIPS 2020). ACM,\nNovember 2020.\n[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015.\n[53] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n[54] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and\nKurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 34, pages 8815\u20138821, 2020.\n[55] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. MobileBERT: a\ncompact task-agnostic BERT for resource-limited devices. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pages 2158\u20132170, Online, July 2020. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.acl-main.195. URL https://aclanthology.org/\n2020.acl-main.195.\n[56] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.\n[57] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper\nwith image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 32\u201342, 2021.\n[58] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIV, pages\n516\u2013533. Springer, 2022.\n[59] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar\nSubramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, and Tijmen Blankevoort. Fp8 versus int8 for\nefficient deep learning inference. 2023.\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference\non Neural Information Processing Systems, pages 6000\u20136010, 2017.\n[61] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,\npages 353\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446.\n[62] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu,\nand Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. arXiv\npreprint arXiv:2209.13325, 2022.\n[63] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong\nLiu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal\nshifting and scaling. arXiv preprint arXiv:2304.09145, 2023.\n[64] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\n[65] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language\nprocessing. In Proceedings of the 2020 conference on empirical methods in natural language processing:\nsystem demonstrations, pages 38\u201345, 2020.\n14\n[66] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4\nquantization for transformer models: Latency speedup, composability, and failure cases. 2023.\n[67] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models. In CVPR, 2022.\n[68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xl-\nnet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/\npaper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf.\n[69] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong\nHe.\nZeroquant:\nEfficient and affordable post-training quantization for large-scale transform-\ners.\nIn S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Ad-\nvances in Neural Information Processing Systems, volume 35, pages 27168\u201327183. Curran Asso-\nciates, Inc., 2022.\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/\nadf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf.\n[70] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the\nIEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.\n[71] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv\npreprint arXiv:1910.06188, 2019.\n[72] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep\nlearning requires rethinking generalization. 2017.\n[73] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412, 2017.\n[74] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[75] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network\nquantization without retraining using outlier channel splitting. In International conference on machine\nlearning, pages 7543\u20137552. PMLR, 2019.\n[76] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low\nbitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160,\n2016.\n[77] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In The IEEE International Conference on Computer Vision (ICCV), December 2015.\n15\nSupplementary materials\nA\nAdditional graphs from outlier analysis\nIn this section, we present additional graphs from our outlier investigation in Section 3 for BERT and\nvision transformer.\n(a)\n(b)\n(c)\nFigure 9: A summary of several outlier statistics recorded from ImageNet validation set on ViT.\n(a) Average infinity norm of the output of each attention layer. (b) A histogram of outlier counts\nin attention layer #10 vs. hidden dimensions. We use zero-based indexing for dimensions. (c) A\nheatmap of outlier counts in attention layer #10 vs. patch positions.\nA.1\nBERT\nRecall from Figure 1 that all the outliers are only present in hidden dimensions #123, #180, #225,\n#308, #381, #526, #720 (with the majority of them in #180, #720). These hidden dimensions\ncorrespond to attention heads #2, #3, #4, #5, #6, #9, and #12. In Figures 10 and 11 we show more\nexamples of the discovered self-attention patterns for attention heads #3 and #12 (\u2194 hidden dim #180\nand #720, respectively). We also show self-attention patterns in attention heads and layers which are\nnot associated with the outliers in Figures 12 and 13, respectively. Finally, in Figures 14 and 15 we\nshow more examples of the attention patterns learned in the network trained with clipped softmax\nand gated attention.\nA.2\nViT\nFigure 9 further shows that there are a lot of similarities in the outlier behavior in the vision\ntransformer, compared to BERT. The strongest magnitude outliers generally happen in the later layers,\npeaking at layers #10 and #11. The majority of outliers (> 99%) are only ever happening in only 10\nhidden dimensions, primarily in dimensions #48 and #43, which corresponds to the attention head #1.\nFinally, averaged across the entire ImageNet validation set, the outliers seem to be concentrated at\nthe boundaries of the image, which suggest a strong correlation with the background (and a negative\ncorrelation with the object, which is usually in the center of the image in the ImageNet dataset).\nIn Figures 16 and 17, we show more examples of outlier and self-attention patterns in the attention\nhead #1 (\u2194 hidden dimensions #48, #43) for a random subset of images from the ImageNet validation\nset (in layers #10 and #11, respecively).\nB\nDetailed results\nIn this section, we provide extended results for each model, including the used hyperparameters and\nother design choices. We also present some additional ablation studies.\n16\nConfiguration\nG\nMemory overhead (per attention layer)\n# extra parameters\n# extra tokens\nLinear\nnheads \u00d7 Linear(dhead \u2192 1)\nnheads(dhead + 1)\n\u223c 1\nMLP\nnheads \u00d7 MLP(dhead \u2192 nhid \u2192 1)\nnheads(nhid(dhead + 2) + 1)\n\u223c nhid\nAll-heads-linear Linear(dmodel \u2192 nheads)\nnheads(dmodel + 1)\n\u223c nheads\nTable 4: An overview of the gating function parameterizations explored in this paper and their\nmemory overhead.\nB.1\nGating architectures\nWe investigate the choice of several gating functions, summarized in Table 4. The configuration\n\u201cMLP\u201d parameterizes each Gi with a feed-forward net with one hidden layer of size nhid and a\nReLU non-linearity [47]. We also explore what happens if we allow the mixing of the representation\nfrom different attention heads in the \u201cAll-heads-linear\u201d setting, where we use a single linear layer to\nproduce the gating probabilities for all attention heads at once. All three options are tested below.\nUnless explicitly stated otherwise, we initialize the bias of the gating function to zero (i.e., binit = 0\n\u2194 \u03c0init = 0.5).\nB.2\nBERT\nMethod\nFP16 ppl.\u2193\nMax inf norm\nAvg. Kurtosis\nW8A8 ppl.\u2193\nVanilla\n4.49\u00b10.01\n735.0\u00b154.9\n3076\u00b1262\n1294\u00b11046\nCS (\u03b3 = \u22120.005)\n4.44\u00b10.02\n406.6\u00b135.2\n1963\u00b1753\n75.27\u00b139.57\nCS (\u03b3 = \u22120.01)\n4.35\u00b10.01\n198.3\u00b178.7\n1581\u00b1839\n7.06\u00b12.37\nCS (\u03b3 = \u22120.015)\n4.37\u00b10.01\n38.9\u00b17.9\n165\u00b134\n4.54\u00b10.01\nCS (\u03b3 = \u22120.02)\n4.39\u00b10.02\n31.7\u00b16.3\n90\u00b120\n4.56\u00b10.02\nCS (\u03b3 = \u22120.025)\n4.39\u00b10.00\n21.5\u00b11.5\n80\u00b16\n4.52\u00b10.01\nCS (\u03b3 = \u22120.03)\n4.41\u00b10.01\n20.4\u00b10.2\n79\u00b16\n4.55\u00b10.01\nCS (\u03b3 = \u22120.04)\n4.51\u00b10.05\n19.8\u00b19.0\n85\u00b17\n4.65\u00b10.06\nGA, Linear (\u03c0init = 0.25)\n4.49\u00b10.00\n139.8\u00b162.3\n739\u00b1412\n5.05\u00b10.27\nGA, Linear (\u03c0init = 0.5)\n4.48\u00b10.00\n177.3\u00b133.2\n652\u00b181\n5.13\u00b10.15\nGA, Linear (\u03c0init = 0.75)\n4.49\u00b10.00\n71.4\u00b149.9\n262\u00b1147\n4.88\u00b10.22\nGA, Linear (\u03c0init = 0.9)\n4.49\u00b10.00\n171.5\u00b18.8\n559\u00b1141\n5.15\u00b10.03\nGA, MLP (nhid = 4)\n4.45\u00b10.03\n39.2\u00b126.0\n201\u00b1181\n4.65\u00b10.04\nGA, MLP (nhid = 64)\n4.49\u00b10.01\n117.0\u00b148.3\n507\u00b1167\n4.77\u00b10.01\nGA, All-heads-linear\n4.49\u00b10.01\n58.3\u00b141.2\n334\u00b1321\n4.67\u00b10.03\nTable 5: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to\nBERT-base. We report the masked language modeling perplexity (ppl. for short) on the English\nWikipedia validation set for both the floating-point baseline and W8A8 quantized model. We also\nreport the maximum \u2225x\u2225\u221e averaged across the validation set, and kurtosis of x averaged across all\nlayers, where x is the output of an attention layer.\nDetailed results for BERT-base are summarized in Table 5. As we can see, across most of the settings,\nboth of our methods significantly dampen the outliers\u2019 magnitude, reduce the kurtosis, drastically\nimprove the quantized performance, while maintaining and sometimes improving the FP16 perplexity.\nB.3\nOPT\nDetailed results for OPT-125m are summarized in Table 6.\nIn our early experiments on a smaller OPT model, we found that applying the weight decay on\nLayerNorm weights \u03b3 (which isn\u2019t the case, by default) has a strong effect on reducing the outliers\u2019\nmagnitude while yielding the comparable FP16 performance. Therefore, we present the results of\napplying our gated attention approach in both cases, with and without applying weight decay on LN \u03b3.\nAs we can see in Table 6, in both cases gated attention (further) dampens the outliers\u2019 magnitude to a\n17\nMethod\nLN \u03b3 wd\nFP16 ppl.\u2193\nMax inf norm\nAvg. Kurtosis\nW8A8 ppl.\u2193\nVanilla\n\u2715\n15.84\u00b10.05\n339.6\u00b147.2\n1777\u00b1444.\n21.18\u00b11.89\nGA, Linear (\u03c0init = 0.1)\n\u2715\n15.61\u00b10.05\n35.6\u00b14.5\n42.4\u00b122.9\n16.41\u00b10.18\nGA, Linear (\u03c0init = 0.25)\n\u2715\n15.50\u00b10.04\n35.8\u00b10.5\n59.0\u00b148.3\n16.25\u00b10.08\nGA, Linear (\u03c0init = 0.5)\n\u2715\n15.54\u00b10.01\n46.5\u00b15.0\n40.6\u00b18.9\n16.30\u00b10.01\nGA, All-heads-linear\n\u2715\n15.43\u00b10.01\n32.8\u00b11.7\n24.2\u00b13\n16.30\u00b10.12\nVanilla\n\u2713\n15.96\u00b10.03\n87.7\u00b131.9\n2080\u00b11460\n39.46\u00b116.59\nCS (\u03b3 = \u22121/512)\n\u2713\n15.99\u00b10.02\n106.4\u00b17.0\n5764\u00b12150\n185.23\u00b1220.00\nCS (\u03b3 = \u22122/512)\n\u2713\n15.90\u00b10.02\n102.0\u00b127.0\n11290\u00b14372\n60.90\u00b152.70\nCS (\u03b3 = \u22124/512)\n\u2713\n15.86\u00b10.01\n83.1\u00b120.6\n17174\u00b17791\n84.64\u00b110.55\nCS (\u03b3 = \u22128/512)\n\u2713\n16.13\u00b10.09\n61.5\u00b19.9\n19204\u00b14284\n42.62\u00b13.64\nCS (\u03b3 = \u221212/512)\n\u2713\n16.29\u00b10.07\n63.2\u00b18.8\n19727\u00b17479\n37.22\u00b12.39\nGA, Linear (\u03c0init = 0.1)\n\u2713\n15.69\u00b10.05\n7.3\u00b10.4\n25.4\u00b110\n16.23\u00b10.08\nGA, Linear (\u03c0init = 0.25)\n\u2713\n15.55\u00b10.05\n8.7\u00b10.6\n18.9\u00b11\n16.02\u00b10.07\nGA, Linear (\u03c0init = 0.5)\n\u2713\n15.63\u00b10.00\n10.8\u00b10.7\n42.0\u00b119\n16.20\u00b10.01\nGA, All-heads-linear\n\u2713\n15.53\u00b10.01\n7.9\u00b10.3\n13.8\u00b11\n16.09\u00b10.08\nTable 6: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to\nOPT-125m. We report the causal language modeling perplexity (ppl. for short) on the English\nWikipedia validation set for both the floating-point baseline and W8A8 quantized model. We also\nreport the maximum \u2225x\u2225\u221e averaged across the validation set, and kurtosis of x averaged across all\nlayers, where x is the output of an attention layer.\ngreat extent, reduces the kurtosis, and yields models with significantly higher quantized performance,\nwhich is close to the original FP16 performance.\nB.4\nViT\nMethod\nPatch. Embd. LN\nFP32 acc.\nMax inf norm\nAvg. Kurtosis\nW8A8 acc.\nVanilla\n\u2715\n80.75\u00b10.10\n358.5\u00b181.2\n1018.3\u00b1471.5\n69.24\u00b16.93\nCS (\u03b3 = \u22120.003)\n\u2715\n80.24\u00b10.05\n69.3\u00b120.7\n25.6\u00b18.6\n78.71\u00b10.33\nCS (\u03b3 = \u22120.004)\n\u2715\n80.38\u00b10.01\n74.9\u00b110.6\n30.6\u00b14.9\n78.66\u00b10.49\nGA, Linear (\u03c0init = 0.25)\n\u2715\n80.62\u00b10.01\n86.0\u00b18.0\n23.4\u00b12.7\n79.16\u00b10.05\nGA, Linear (\u03c0init = 0.5)\n\u2715\n80.32\u00b10.02\n88.4\u00b117.9\n27.9\u00b114.0\n78.90\u00b10.25\nGA, MLP (nhid = 4)\n\u2715\n80.62\u00b10.05\n118.2\u00b140.5\n47.8\u00b129.8\n78.79\u00b10.29\nVanilla\n\u2713\n80.98\u00b10.08\n81.1\u00b12.5\n24.5\u00b11.8\n79.62\u00b10.06\nCS (\u03b3 = \u22120.0001)\n\u2713\n80.89\u00b10.13\n73.7\u00b114.9\n22.9\u00b11.6\n79.77\u00b10.25\nCS (\u03b3 = \u22120.0003)\n\u2713\n80.92\u00b10.07\n78.9\u00b15.5\n23.8\u00b10.5\n79.63\u00b10.05\nCS (\u03b3 = \u22120.0005)\n\u2713\n80.95\u00b10.08\n72.9\u00b111.8\n24.4\u00b10.7\n79.73\u00b10.08\nCS (\u03b3 = \u22120.001)\n\u2713\n80.95\u00b10.16\n80.8\u00b12.1\n24.1\u00b10.7\n79.69\u00b10.03\nCS (\u03b3 = \u22120.002)\n\u2713\n80.80\u00b10.07\n78.0\u00b10.5\n25.8\u00b10.7\n79.32\u00b10.07\nCS (\u03b3 = \u22120.003)\n\u2713\n80.79\u00b10.02\n75.6\u00b17.9\n28.1\u00b14.0\n79.00\u00b10.10\nGA, Linear (\u03c0init = 0.5)\n\u2713\n81.01\u00b10.06\n79.8\u00b10.5\n19.9\u00b10.3\n79.82\u00b10.11\nGA, Linear (\u03c0init = 0.75)\n\u2713\n81.01\u00b10.05\n77.8\u00b10.3\n21.8\u00b11.9\n79.80\u00b10.08\nGA, Linear (\u03c0init = 0.9)\n\u2713\n80.92\u00b10.11\n70.6\u00b18.0\n23.2\u00b13.7\n79.64\u00b10.09\nTable 7: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to\nViT-S/16. We report the top-1 accuracy on ImageNet-1K validation set for floating-point baseline\nand W8A8 quantized model. We also report the maximum \u2225x\u2225\u221e averaged across the validation set,\nand kurtosis of x averaged across all layers, where x is the output of the attention layer.\nDetailed results for ViT-S/16 are summarized in Table 7.\nAfter our preliminary experiments on ViT, we noticed that distinct outliers already originate after\nthe patch embeddings. Therefore, we experimented with adding the LayerNorm after the patch\n18\nembeddings (which was absent in the model definition, by default). As we can see in Table 6, together\nwith this change, both of our proposed methods greatly dampens the outliers\u2019 magnitude, reduces the\nkurtosis, and yields models with significantly higher quantized performance, which is within 1% of\nthe original FP32 accuracy.\nB.5\nThe impact of clipped softmax hyperparameters (\u03b3 and \u03b6) on ViT\n\u03b3\n\u03b6\nFP32 acc.\nMax inf norm\nW8A8 acc.\n0\n1\n78.80\u00b10.42\n426\u00b169\n71.27\u00b10.88\n(= Vanilla)\n0\n1.001\n78.78\u00b10.29\n411\u00b188\n71.24\u00b10.59\n0\n1.002\n78.90\u00b10.17\n420\u00b147\n70.74\u00b10.34\n0\n1.004\n78.80\u00b10.45\n377\u00b167\n72.31\u00b10.06\n0\n1.01\n78.81\u00b10.30\n419\u00b177\n71.35\u00b10.26\n\u22120.00001\n1\n78.81\u00b10.21\n432\u00b176\n69.02\u00b10.19\n\u22120.0001\n1\n78.81\u00b10.36\n380\u00b164\n64.04\u00b110.8\n\u22120.001\n1\n78.42\u00b10.63\n282\u00b1105\n68.43\u00b16.50\n\u22120.003\n1\n78.26\u00b10.06\n99\u00b136\n76.49\u00b10.48\n\u22120.01\n1\n78.10\u00b10.14\n391\u00b121\n75.83\u00b11.12\n\u22120.03\n1\n70.26\u00b11.46\n197\u00b12\n65.80\u00b11.41\n\u22120.001\n1.001\n78.45\u00b10.53\n283\u00b182\n65.03\u00b18.54\n\u22120.003\n1.003\n78.25\u00b10.14\n119\u00b117\n76.37\u00b10.45\nTable 8: The impact of clipped softmax hyperparameters on ViT-S/16.\nWe investigate the effect of different values of the clipped softmax stretch parameters applied to the\nvision transformer and present the results in Table 8. To speed up training, for this experiment we\ntrained ViT for 150 epochs instead of the usual 300 epochs. For this experiment, we did not apply\nLayerNorm after the patch embeddings.\nWe found similar observations compared to BERT. Specifically, most of the improvement happens\nwhen we use \u03b3 < 0 (clipping at zero) whereas using \u03b6 > 1 (clipping at one) yields similar results\nto the vanilla softmax and combining both \u03b3 < 0 and \u03b6 > 1 yields similar results compared to just\nclipping at zero.\nB.6\nFine-tuning experiment\nMethod\nFP16 ppl.\u2193\nMax inf norm\nAvg. Kurtosis\nVanilla fine-tuning\n29.46\n79.3\n2086\nFine-tuning w/ Gated attention\n29.18\n50.9\n665\nTable 9: OPT-1.3B fine-tuning results with vanilla softmax and gated attention. We report the causal\nlanguage modeling perplexity (ppl. for short) on the English Wikipedia validation set. We also report\nthe maximum \u2225x\u2225\u221e averaged across the validation set, and kurtosis of x averaged across all layers,\nwhere x is the output of an attention layer.\nOne of the drawbacks of our proposed framework is that it requires training from scratch, which could\nbe expensive when applied to very large models. To address this, we explored whether fine-tuning\nusing gated attention can still lead to improved performance and decreased outliers for larger models.\nWe used OPT-1.3B pre-trained checkpoint from HuggingFace and fine-tuned it on Bookcorpus +\nWikipedia for 4000 steps with batch size 256, maximum sequence length 512, maximum learning\nrate 10\u22125, and linear LR schedule with 400 warmup steps. We use the same LR for both model\nparameters and gating module parameters. The rest of hyper-parameters are the same as for our\npre-training setup. We adapted our gating approach as follows. We initialized bias as binit = 0, which\ncorresponds to the expected initial gating probability output of \u03c0init = 0.5. We multiply the gating\nprobability by 2 so that the expected gate output is 1 and we approximate the attention output of\n19\nthe vanilla softmax at the start of fine-tuning. We add a small activation regularization term at the\noutput of each FFN to further encourage the reduction in the magnitude of activations, as unlike when\ntraining from scratch outliers are already present in the pre-trained model and need to be suppressed.\nAs we can see from Table 9, fine-tuning with our proposed gated attention results in a better perplexity\nand also reduced maximum infinity norm and the average kurtosis compared to fine-tuning with\nvanilla softmax.\nB.7\nLow-bit quantization results\nBitwidths\nWeight range estimation\nVanilla\nClipped softmax\nGated attention\nFP16\n\u2212\n4.49\u00b10.01\n4.39\u00b10.00\n4.45\u00b10.03\nW8A8\nmin-max\n1294\u00b11046\n4.52\u00b10.01\n4.65\u00b10.04\nW6A8\nmin-max\n598\u00b1254\n4.64\u00b10.01\n4.79\u00b10.03\nW6A8\nMSE\n6.49\u00b10.38\n4.56\u00b10.01\n4.71\u00b10.03\nW4A8\nMSE\n6.52\u00b10.02\n4.90\u00b10.02\n5.02\u00b10.03\nW6A6\nMSE\n42.8\u00b111.7\n6.64\u00b10.14\n5.90\u00b10.11\nTable 10: A summary of results for our proposed methods applied to BERT-base and quantized to\ndifferent bitwidthds for weights and activations (using the same PTQ setup as in all previous experi-\nments). We report the masked language modeling perplexity on the English Wikipedia validation set.\nNote that our proposed methods are not limited to 8-bit quantization only and in general can be\ncombined with other more advanced quantization and weight compression methods, including [18,\n35, 36, 45, 63, 67]. In Table 10, we show the results of our proposed methods applied to BERT-base\nand quantized to different bitwidths using our simple post-training quantization setup. Unless stated\notherwise, for low-bit (<8-bit) weights and activations we use MSE range estimator as recommended\nby [2, 7] since it gives better results.\nAs we can see, in all cases both of our methods significantly improve the perplexity compared to the\nvanilla softmax pre-training. We also notice that generally the performance progressively degrades as\nwe decrease the bitwidths, which is to be expected. Achieving good results with low-bit activation\nquantization in general is a challenging problem. Further, we notice that the perplexity of the vanilla\nmodel significantly improves whenever we consider a low-bit weight quantization with MSE ranges\ncompared to the INT8 case. This can be explained by the fact that using MSE range estimation\nfor weights leads to an implicit clipping of activations (in the same and all subsequent layers in the\nnetwork), which happen to be of the right amount so that it doesn\u2019t hurt the perplexity. We found\nthat by going from W8A8 to W6A8 the average kurtosis is reduced from 3406\u00b1547 to 631\u00b194 and\nthe maximum infinity norm is reduced from 577\u00b180 to 158\u00b140. However, in all cases the resulting\nmodel still has significantly larger outliers and a worse performance than both of our proposed\nmethods. Finally, as said before, if achieving good low-bit quantization performance is the goal, it is\nrecommended to combine our methods with more advanced quantization techniques.\nC\nExperimental details\nC.1\nBERT\nFine-tuning on MNLI dataset\nWe use pre-trained checkpoint BERT-base-uncased (109M param-\neters) from HuggingFace repository. We follow standard fine-tuning practices from [14] and [65]\nEach data sequence is tokenized and truncated to the maximum sequence length of 128. Shorter\nsequences are padded to the same length of 128 using a special [PAD] token. We fine-tune for 3\nepochs using Adam [29] with a batch size of 16 and no weight decay. The learning rate is initially set\nto its maximum value of of 2 \u00b7 10\u22125 and is linearly decayed to zero by the end of fine-tuning.\nPre-training from scratch\nWe follow closely the pre-training procedure from [14]. We concate-\nnate, tokenize, and split the training set into sequences of length 128 (to speed up training and\nexperimentation, we do not fine-tune on longer sequences of 512). We use the masked language\nmodeling objective with the probability of masking p = 0.15. We train with a batch size of 256\n20\nsequences for 106 steps, using AdamW optimizer [39] with the maximum learning rate of 10\u22124,\nlearning rate warm up over the first 104 steps, following by a linear decay to zero by the end of\ntraining. We use L2 weight decay of 0.01, L2 gradient norm clipping of 1.0, and dropout probability\nof 0.1 on all layers. We also use FP16 mixed-precision from HuggingFace Accelerate library [20].\nC.2\nOPT pre-training\nTo speed up experimentation, we train OPT-125m sized model on the concatenation of Wikipedia\nand BookCorpus (same as BERT pre-training). We train with a batch size of 48 and 4 gradient\naccumulation steps (which results in the effective batch size of 192), so that we can perform pre-\ntraining on a single A100 80GB GPU. We concatenate, tokenize, and split the training set into\nsequences of length 512 and train for 125000 steps (500000 forward passes).\nWe use the rest of the hyper-parameters and follow pre-training practices from [74] and [65]. We\ninitialize weights using a normal distribution with zero mean and a standard deviation of 0.006. All\nbias terms are initialized to zero. We use AdamW optimizer with (\u03b21, \u03b22) = (0.9, 0.95). We use the\nlinear learning rate schedule, warming up from 0 to the maximum value\u2020 of 4 \u00b7 10\u22124 over the first\n2000 steps, following by a linear decay to zero by the end of training. We use L2 weight decay of\n0.1, L2 gradient norm clipping of 1.0, and dropout probability of 0.1 on all layers. We also use FP16\nmixed-precision from HuggingFace Accelerate library [20].\nNote that in our experiments for all model sizes we use the consistent LayerNorm placement before\nthe attention block, unlike OPT-350m checkpoint from HuggingFace that places LayerNorm after the\nattention block.\nC.3\nViT pre-training\nWe use the model definition for ViT-S/16 and the training pipeline from PyTorch Image models\nlibrary [64]. All training is done on resolution 224\u00d7224 and 16\u00d716 patches. For data augmentation,\nwe use RandAugment [10], Mixup [73], CutMix [70], random image cropping [56], horizontal flip,\nlabel smoothing \u03b5 = 0.1, color jitter 0.4, and random (between bilinear and bicubic) interpolation\nduring training.\nWe train with a batch size of 512 for 300 epochs, using AdamW optimizer and the L2 weight decay\nof 0.03. We use the cosine learning rate schedule, warming up from 10\u22126 to the maximum value of\n10\u22123 over the first 20 epochs, followed by a LR decay by a factor of 10 every 30 epochs, until it\nreaches the minimum value of 10\u22125.\nC.4\nQuantization settings\nWeights\nIn all cases, we use symmetric uniform quantization of weights. We use min-max weight\nquantization for all models except the OPT model, for which we found the MSE estimator to perform\nbetter in all cases.\nActivations\nWe adopt static range estimation approach, which determines quantization parameters\nfor the network by passing a few batches of calibration data through the model before inference.\nSpecifically, we use a running min-max estimator [32], which uses an exponential moving average of\nthe min and max over multiple batches. In all cases, we use running min-max with 0.9 momentum\nover 16 batches randomly sampled from respective training sets.\nFor OPT model, we also experiment with using 99.99% and 99.999% percentiles instead of actual\nmin and max. We select the best configuration for each experiment (including baseline), based on the\nmodel performance. In almost all cases, we found that setting activation quantization ranges using\n99.999% percentiles gives the lowest W8A8 perplexity.\nD\nCompute cost\nWe compare the runtime of our proposed methods in Table 11. As we can see, the clipped softmax is\nonly marginally more expensive compared to using the vanilla softmax attention. The gated attention\n\u2020In our experiments, we found this value to perform better compared to the value of 6 \u00b7 10\u22124 listed in the paper.\n21\nModel\nVanilla\nClipped softmax\nGated attention (Linear / MLP)\nBERT\n92.8\u00b11.2\n93.6\u00b10.8\n97.7 / 119.1\nOPT\n53.6\u00b10.4\n54.4\u00b10.4\n55.7 / 64.7\nViT\n101.8\u00b10.3\n104.0\u00b10.7\n110.8 / 122.9\nTable 11: An overview of the runtime of the proposed methods, compared to the vanilla pre-training,\nmeasured in hours on Nvidia-A100 GPUs.\nusing the linear G adds the compute overhead between 3% and 8%, depending on the model. We\nfound that adding weight decay on LayerNorm \u03b3 for OPT and adding the LayerNorm after the patch\nembeddings for ViT had a negligible effect on the runtime.\nWe estimated that the compute cost of producing the main results in the paper is about 320 GPU days\n(on A100) and the total cost of the project (including preliminary experiments and ablation studies)\nto be about 1400 GPU days.\n22\n(a) Attention layer #10, data sequence #16\n(b) Attention layer #11, data sequence #16\n(c) Attention layer #10, data sequence #21\n(d) Attention layer #11, data sequence #21\n(e) Attention layer #10, data sequence #61\n(f) Attention layer #11, data sequence #61\n(g) Attention layer #10, data sequence #88\n(h) Attention layer #11, data sequence #88\nFigure 10: Visualization of the self-attention patterns (attention probabilities, values, and their\nproduct in left, middle and right columns, respectively) in attention head #3 (\u2194 channel dim #180)\nfor BERT-base trained with vanilla softmax, computed on several random data sequences from\nMNLI-m validation set.\n23\n(a) Attention layer #10, data sequence #16\n(b) Attention layer #11, data sequence #16\n(c) Attention layer #10, data sequence #21\n(d) Attention layer #11, data sequence #21\n(e) Attention layer #10, data sequence #61\n(f) Attention layer #11, data sequence #61\n(g) Attention layer #10, data sequence #88\n(h) Attention layer #11, data sequence #88\nFigure 11: Visualization of the self-attention patterns (attention probabilities, values, and their product\nin left, middle and right columns, respectively) in attention head #12 (\u2194 channel dim #720) for\nBERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m\nvalidation set.\n24\n(a) Attention layer #10, attention head #1\n(b) Attention layer #11, attention head #1\n(c) Attention layer #10, attention head #7\n(d) Attention layer #11, attention head #7\n(e) Attention layer #10, attention head #8\n(f) Attention layer #11, attention head #8\n(g) Attention layer #10, attention head #10\n(h) Attention layer #11, attention head #10\nFigure 12: Visualization of the self-attention patterns (attention probabilities, values, and their product\nin left, middle and right columns, respectively) in attention heads that are not associated with the\nstrong outliers for BERT-base trained with vanilla softmax, computed on data sequences #16 from\nMNLI-m validation set.\n25\n(a) Attention layer #1\n(b) Attention layer #2\n(c) Attention layer #3\n(d) Attention layer #4\n(e) Attention layer #5\n(f) Attention layer #6\n(g) Attention layer #7\n(h) Attention layer #8\nFigure 13: Visualization of the self-attention patterns (attention probabilities, values, and their product\nin left, middle and right columns, respectively) in attention head #3 (\u2194 channel dim #180) and the\nfirst eight layers of BERT-base trained with vanilla softmax, computed on data sequences #16 from\nMNLI-m validation set.\n26\n(a) Attention layer #10, Attention head #3, data sequence #1\n(b) Attention layer #11, Attention head #3, data sequence #1\n(c) Attention layer #10, Attention head #3, data sequence #5\n(d) Attention layer #11, Attention head #3, data sequence #5\n(e) Attention layer #10, Attention head #3, data sequence #7\n(f) Attention layer #11, Attention head #3, data sequence #7\n(g) Attention layer #10, Attention head #12, data sequence #1\n(h) Attention layer #11, Attention head #12, data sequence #1\nFigure 14: Visualization of the self-attention patterns (attention probabilities, values, and their\nproduct in left, middle and right columns, respectively) for BERT-base trained with Clipped softmax,\ncomputed on several random data sequences from MNLI-m validation set.\n27\n(a) Attention layer #10, Attention head #3, data sequence #1\n(b) Attention layer #11, Attention head #3, data sequence #1\n(c) Attention layer #10, Attention head #3, data sequence #5\n(d) Attention layer #11, Attention head #3, data sequence #5\n(e) Attention layer #10, Attention head #3, data sequence #7\n(f) Attention layer #11, Attention head #3, data sequence #7\n(g) Attention layer #10, Attention head #12, data sequence #1\n(h) Attention layer #11, Attention head #12, data sequence #1\nFigure 15: Visualization of the self-attention patterns (from left to right: gating probabilities \u03c0 =\nsigmoid (G (x)), output of softmax, values, and their combined product) for BERT-base trained with\ngated attention, computed on several random data sequences from MNLI-m validation set.\n28\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 16: A summary of our outlier analysis for ViT demonstrated on a random subset from\nImageNet validation set. (a) An input image. (b) Outliers in the output of layer #10. (c) Cumulative\nattention weight spent on every patch (matrix of attention probabilities summed over rows) in the\nattention head #1, in the next layer #11. (d) A corresponding matrix of attention probabilities. (e) An\naverage magnitude of values (V ) for outlier and non-outlier patches.\n29\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 17: A summary of our outlier analysis for ViT demonstrated on a random subset from\nImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative\nattention weight spent on every patch (matrix of attention probabilities summed over rows) in the\nattention head #1, in the next layer #12. (d) A corresponding matrix of attention probabilities. (e) An\naverage magnitude of values (V ) for outlier and non-outlier patches.\n30\n"
  },
  {
    "title": "CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search",
    "link": "https://arxiv.org/pdf/2306.10008.pdf",
    "upvote": "8",
    "text": "CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via\nAdversarial Latent Search\nFahad Shamshad\nMuzammal Naseer\nKarthik Nandakumar\nMohamed Bin Zayed University of AI, UAE\n{fahad.shamshad, muzammal.naseer, karthik.nandakumar}@mbzuai.ac.ae\n\u201cred lipstick with \npurple eyeshadows\u201d\n\u201cpink eyeshadows\u201d\n\u201cclown makeup\u201d\n\u201cno makeup\u201d\n\u201cbig eyebrows with \npink eyeshadows\u201d\n\u201ctanned makeup with \nblack lipstick\u201d\n\u201ctanned makeup with \npurple lipstick\u201d\n23.81\n58.31\n53.69\n69.62\n35.97\n66.08\n63.87\n71.53\nOriginal\nProtected\nOriginal\nTIP-IM [70]\nAMT-GAN [22]\nProposed\nOriginal\nTIP-IM [70] AMT-GAN [22]\nProposed\nFigure 1. The proposed approach crafts \u201cnaturalistic\u201d and transferable text-guided adversarial faces to deceive black-box\nface recognition systems. First row shows original images that need to be protected and second row shows corresponding\nprotected images along with the user-defined makeup text prompts that guide the adversarial search. Comparison against\nexisting methods is shown in the third row. The yellow text represents the confidence score (higher is better) output by a\ncommercial API (Face++), when matching the protected image against the target identity shown in the bottom right. The\nreference image used by [22] for makeup transfer is shown at the bottom corner of the corresponding adversarial image.\nTarget\nAbstract\nThe success of deep learning based face recognition sys-\ntems has given rise to serious privacy concerns due to their\nability to enable unauthorized tracking of users in the dig-\nital world.\nExisting methods for enhancing privacy fail\nto generate \u201cnaturalistic\u201d images that can protect facial\nprivacy without compromising user experience.\nWe pro-\npose a novel two-step approach for facial privacy protection\nthat relies on finding adversarial latent codes in the low-\ndimensional manifold of a pretrained generative model. The\nfirst step inverts the given face image into the latent space\nand finetunes the generative model to achieve an accurate\nreconstruction of the given image from its latent code. This\nstep produces a good initialization, aiding the generation\nof high-quality faces that resemble the given identity. Sub-\nsequently, user-defined makeup text prompts and identity-\npreserving regularization are used to guide the search for\nadversarial codes in the latent space.\nExtensive experi-\nments demonstrate that faces generated by our approach\nhave stronger black-box transferability with an absolute\ngain of 12.06% over the state-of-the-art facial privacy pro-\ntection approach under the face verification task. Finally,\nwe demonstrate the effectiveness of the proposed approach\nfor commercial face recognition systems. Our code is avail-\nable at https://github.com/fahadshamshad/Clip2Protect.\n1. Introduction\nDeep learning based face recognition (FR) systems [43,\n61] have found widespread usage in multiple applications,\narXiv:2306.10008v2  [cs.CV]  20 Jun 2023\nTable 1. Comparison among different facial privacy protection\nmethods w.r.t. the natural outputs, black box setting, experiments\nunder face verification and identification tasks, unrestricted (se-\nmantically meaningful), and more flexible text guided adversaries.\nAdv-Makeup [71] TIP-IM [70] AMT-GAN [22] Ours\nNatural outputs\nYes\nPartially\nPartially\nYes\nBlack box\nYes\nYes\nYes\nYes\nVerification\nYes\nNo\nYes\nYes\nIdentification\nNo\nYes\nNo\nYes\nUnrestricted\nYes\nNo\nYes\nYes\nText guided\nNo\nNo\nNo\nYes\nincluding security [63], biometrics [38], and criminal in-\nvestigation [45], outperforming humans in many scenar-\nios [12, 48, 61]. Despite positive aspects of this technol-\nogy, FR systems seriously threaten personal security and\nprivacy in the digital world because of their potential to\nenable mass surveillance capabilities [1, 67].\nFor exam-\nple, government and private entities can use FR systems to\ntrack user relationships and activities by scraping face im-\nages from social media profiles such as Twitter, Linkedin,\nand Facebook [18,20]. These entities generally use propri-\netary FR systems, whose specifications are unknown to the\npublic (black box model). Therefore, there is an urgent need\nfor an effective approach that protects facial privacy against\nsuch unknown FR systems.\nAn ideal facial privacy protection algorithm must strike\nthe right balance between naturalness and privacy protec-\ntion [70, 77]. In this context, \u201cnaturalness\u201d is defined as\nthe absence of any noise artifacts that can be easily per-\nceived by human observers and the preservation of human-\nperceived identity. \u201cPrivacy protection\u201d refers to the fact\nthat the protected image must be capable of deceiving a\nblack-box malicious FR system. In other words, the pro-\ntected image must closely resemble the given face image\nand be artifact-free for a human observer, while at the same\ntime fool an unknown automated FR system. Since fail-\nure to generate naturalistic faces can significantly affect user\nexperience on social media platforms, it is a necessary pre-\ncondition for adoption of a privacy-enhancement algorithm.\nRecent works exploit adversarial attacks [57] to conceal\nuser identity by overlaying noise-constrained (bounded) ad-\nversarial perturbations on the original face image [6,53,74].\nSince the adversarial examples are generally optimized in\nthe image space, it is often difficult to simultaneously\nachieve naturalness and privacy [70]. Unlike noise-based\nmethods, unrestricted adversarial examples are not con-\nstrained by the magnitude of perturbation in the image space\nand have demonstrated better perceptual realism for human\nobservers while being adversarially effective [3,55,68,76].\nSeveral efforts have been made to generate unrestricted\nadversarial examples that mislead FR systems (see Tab.\n1) [22,25,39,72]. Among these, adversarial makeup based\nmethods [22, 72] are gaining increasing attention as they\ncan embed adversarial modifications in a more natural way.\nThese approaches use generative adversarial networks [15]\n(GANs) to adversarially transfer makeup from a given ref-\nerence image to the user\u2019s face image while impersonating\na target identity. However, existing techniques based on\nadversarial makeup transfer have the following limitations:\n(i) adversarial toxicity in these methods hamper the perfor-\nmance of the makeup transfer module, thereby resulting in\nunnatural faces with makeup artifacts (see Fig. 1); (ii) the\nuse of a reference image to define the desired makeup style\naffects the practicality of this approach; (iii) for every new\ntarget identity, these approaches require end-to-end retrain-\ning from scratch using large makeup datasets; and (iv) most\nof these methods primarily aim at impersonation of the tar-\nget identity, whereas the desired privacy objective is dodg-\ning, i.e., multiple images of the user\u2019s face scraped from\ndifferent social media sites must not match with each other.\nTo mitigate the above problems, we propose a new ap-\nproach to protect user facial privacy on online platforms\n(Sec. 3). The proposed approach aims to search for adver-\nsarial latent codes in a low-dimensional manifold learned\nby a generative model trained to generate face images\n[2,27]. Our main contributions are:\n\u2022 Facial Privacy-protection Framework Using Ad-\nversarial Latent Codes: Given a face image, we pro-\npose a novel two-step method to search for adversarial\nlatent codes, which can be used by a generative model\n(e.g., StyleGAN) to produce face images with high\nvisual quality that matches human-perceived identity,\nwhile deceiving black-box FR systems.\n\u2022 Adversarial\nMakeup\nTransfer\nusing\nTextual\nPrompts: A critical component of the above frame-\nwork is a technique for leveraging user-defined textual\n(makeup) prompts to traverse over the latent manifold\nof the generative model and find transferable adversar-\nial latent codes. Our approach effectively hides attack\ninformation in the desired makeup style, without the\nneed for any large makeup dataset or retraining of\nmodels for different target identities.\n\u2022 Identity Preserving Regularization:\nWe propose\na regularizer that preserves identity-related attributes\nwithin the latent space of the generative model and en-\nsures that the protected face image visually resembles\nthe original face.\nExtensive experiments (Sec. 4.1) for both face verification\nand identification scenarios demonstrate the effectiveness of\nour approach against black-box FR models and online com-\nmercial facial recognition APIs (Sec. 4.2). Furthermore, we\nprovide detailed ablative analysis to dissect the performance\nof different components of our approach (Sec. 4.3).\n2. Related Work\nObfuscation Methods: Obfuscation is the most widely\nused technique [38] to protect user\u2019s facial privacy. Ear-\nlier obfuscation approaches typically degrade the quality of\nthe original face image by applying simple operations such\nas masking [52, 64], filtering [33, 78], and image transfor-\nmations [8, 36, 62]. While these relatively simple obfus-\ncation techniques are reasonable for surveillance applica-\ntions, they are ill-suited for online/social media platforms\nwhere user experience is critical [41]. Though deep learn-\ning based obfuscation approaches generate more realistic\nimages [4,7,56,58], they often result in a change of identity\ncompared to the original image and occasionally produce\nundesirable artifacts [30,31,34].\nNoise-based Adversarial Examples: Adversarial attacks\nhave been used to protect users from unauthorized FR mod-\nels. Some methods [6, 53] rely on data poisoning to de-\nceive targeted FR models, but are less practical because ac-\ncess to the training data or the gallery set of the unknown\nFR system is often not available. Other approaches have\nused game-theory perspective [42] in white-box settings\nor person-specific privacy masks (one mask per person) to\ngenerate protected images at the cost of acquiring multiple\nimages of the same user [77]. In contrast, we aim to fool\nthe black box FR model using only single image. In TIP-\nIM [70], targeted optimization was used to generate privacy\nmasks against unknown FR models by introducing a natu-\nralness constraint. While this approach provides effective\nprivacy, it generates output images with perceptible noises\nthat can affect the user experience [70].\nUnrestricted Adversarial Examples: Unrestricted adver-\nsarial attacks (UAAs) are not constrained by the perturba-\ntion norm and can induce large but semantically meaningful\nperturbations. These attacks have been extensively studied\nin image classification literature [3,35,55,68,73,76] and it\nhas been shown that outputs generated via UAAs are less\nperceptible to human observers as compared to noise-based\nadversarial attacks. Motivated by this observation, patch-\nbased unrestricted attacks have been proposed to generate\nwearable adversarial accessories like colorful glasses [54],\nhat [29] or random patch [69] to fool the FR model, but\nsuch synthesized patches generally have weak transferabil-\nity due to the limited editing region and the large visible\npattern compromises naturalness and affects user experi-\nence. Recently, generative models [24,50] have been lever-\naged to craft UAAs against FR models. However, these\ngenerative approaches are either designed for the white-\nbox settings [46,79] or show limited performance in query-\nfree black-box settings [25]. Makeup-based UAAs [17,72]\nhave also been proposed against FR systems by embed-\nding the perturbations into a natural makeup effect. These\nmakeup based attacks have also been exploited to protect\nthe user privacy by applying adversarial makeup on the user\nface image [22]. However, interference between adversarial\nperturbations and makeup transfer can produce undesirable\nmakeup artifacts in the output images. Moreover, these at-\ntacks generally assume access to large makeup datasets for\ntraining models and require a reference makeup image. In\ncontrast, our approach finds adversarial faces on the natu-\nral image manifold in black-box setting via guidance from\nmakeup text prompt, which makes it less susceptible to arti-\nfacts (see Fig. 1) and more practical.\nVision-Language\nModelling:\nCross-modal\nvision-\nlanguage modelling has attracted significant attention in\nrecent years [13].\nOpenAI introduced CLIP [47] that is\ntrained on 400 million image-text pairs using contrastive\nobjective and maps both image and text in a joint multi-\nmodal embedding space.\nWith powerful representation\nembedding of CLIP, several methods have been proposed\nto manipulate images with text-guidance. StyleCLIP [44]\nand DiffusionCLIP [28, 40] leverage the powerful gener-\native capabilities of StyleGAN and diffusion models to\nmanipulate images with text prompts. Other similar works\ninclude HairCLIP [66], CLIP-NeRF [60], CLIPstyler [32],\nand CLIPDraw [14].\nWhile these methods focus on the\ntext-guidance ability of CLIP, our approach aims to find the\nadversarial latent codes in a generative model\u2019s latent space\nfor privacy protection against black-box FR models.\n3. Proposed Approach for Facial Privacy\nOur goal is to protect user facial privacy on online plat-\nforms against unknown (black-box) FR models without\ncompromising on the user\u2019s online experience. The pro-\nposed approach finds protected faces by adversarially ex-\nploring the low-dimensional latent space of a pretrained\ngenerative model that is trained on natural face images. To\navoid artifacts in the protected image, we restrict the search\nfor adversarial faces close to the clean image manifold\nlearned by the generative model. Moreover, we propose to\noptimize only over identity-preserving latent codes in the\nlatent space. This effectively preserves human-perceived\nidentity during attack while offering high privacy against\nautomated systems. Further, we employ natural makeup-\nlike perturbations via guidance from a text prompt, which\nprovides more flexibility to the user compared to reference\nimage-based adversarial makeup transfer [22].\n3.1. Preliminaries\nLet x \u2208 X \u2282 Rn denote the given original/real face\nimage.\nLet f(x) : X \u2192 Rd be a FR model that ex-\ntracts a fixed-length normalized feature representation. Let\nD(x1, x2) = D(f(x1), f(x2)) be a distance metric that\nmeasures the dissimilarity between two face images x1\nand x2 based on their respective representations f(x1)\nand f(x2).\nGenerally a FR system can operate in two\nmodes: verification and identification. A face verification\nAttract\nRepel\nFigure 2. Overall pipeline of the proposed approach to protect users facial privacy. Our proposed approach searches for the adversarial\nlatent codes on the generative manifold to reconstruct an adversarial face that is capable of fooling unknown FR systems for privacy\nprotection. Our approach allows \u201dmakeup\u201d editing in an adversarial manner through user defined textual prompts and thereby enhance the\nuser\u2019s online experience. Our text-guided objective searches for such latent codes while keeping the original identity preserved.\nsystem predicts that two faces belong to the same identity\nif D(x1, x2) \u2264 \u03c4, where \u03c4 is the system threshold. On the\nother hand, a (closed set) face identification system com-\npares the input image (probe) against a set of face images\n(gallery) and outputs the identity whose representation is\nmost similar to that of the probe. Since the attacker can\nemploy verification or identification to determine the user\nidentity using black-box FR models, a protection approach\nshould conceal the user\u2019s identity in both scenarios.\nUser privacy can be protected by misleading the mali-\ncious FR model through impersonation or dodging attacks.\nIn the context of verification, impersonation (false match)\nimplies that the protected face matches with the face of\na specific target identity and dodging (false non-match)\nmeans that the protected face does not match with some\nother image of the same person. Similarly, for face iden-\ntification, impersonation ensures that the protected image\ngets matched to a specified target identity in the gallery set,\nwhile dodging prevents the protected face from matching\nwith images of the same person in the gallery.\nProblem Statement: Given the original face image x,\nour goal is to generate a protected face image xp such\nthat D(xp, x) is large (for successful dodging attack) and\nD(xp, xt) is small (for successfully impersonating a target\nface xt), where O(x) \u0338= O(xt) and O is the oracle that\ngives the true identity labels. At the same time, we want to\nminimize H(xp, x), where H quantifies the degree of un-\nnaturalness introduced in the protected image xp in relation\nto the original image x. Formally, the optimization problem\nthat we aim to solve is:\nmin\nxp L(xp) = D(xp, xt) \u2212 D(xp, x)\n(1)\ns.t. H(xp, x) \u2264 \u03f5\n(a) Original\n(b) Encoder Inversion\n(c) Generator finetuning\nFigure 3. Generator finetuning allows near-perfect reconstructions\nof LFW dataset sample. This is crucial for the online experience\nof users. Matching scores returned by Face++ API are 62.38 and\n98.96 for encoder and generator-finetuned inversions, respectively.\nwhere \u03f5 is a bound on the adversarial perturbation.\nFor\nnoise-based approach, H(xp, x) = \u2225x\u2212xp\u2225p, where \u2225\u00b7\u2225p\ndenotes the Lp norm. However, direct enforcement of the\nperturbation constraint leads to visible artifacts, which af-\nfects visual quality and user experience. Constraining the\nsolution search space to a natural image manifold using an\neffective image prior can produce more realistic images.\nNote that the distance metric D is unknown since our goal\nis to deceive a black-box FR system.\n3.2. Makeup Text-Guided Adversarial Faces\nOur approach restricts the solution space of the protected\nface xp to lie close to the clean face manifold X. This man-\nifold can be learned using a generative model trained on real\nhuman faces. Specifically, let G\u03b8(w) : W \u2192 Rn denote the\npretrained generative model with weights \u03b8, where W is the\nlatent space. Our proposed approach consists of two stages:\n(i) latent code initialization (Sec. 3.2.1) and (ii) text-guided\nadversarial optimization (Sec. 3.2.2). The overall pipeline\nof the proposed approach is shown in Fig. 2.\n3.2.1\nLatent Code Initialization\nThe latent code initialization stage is based on GAN inver-\nsion, which aims to invert the original image x into the la-\ntent space W, i.e., find a latent code winv \u2208 W such that\nxinv = G\u03b8(winv) \u2248 x. To achieve this, we first use an\nencoder-based inversion called e4e [59] to infer winv in W\nfrom x i.e., winv = I\u03d5(x), where I\u03d5 : X \u2192 W is the\npretrained encoder with weights \u03d5 (see Fig. 2).\nWe use StyleGAN trained on a high-resolution dataset of\nface images as the pretrained generative model G\u03b8 due to its\npowerful synthesis ability and the disentangled structure of\nits latent space. A significant challenge during inversion is\npreserving the identity of the original image i.e., O(x) =\nO(xinv). Generally, optimization and encoder-based inver-\nsion approaches struggle to preserve identity after recon-\nstruction [49] (see Fig. 3b). Moreover, when using these ap-\nproaches, the inversion error can be large for out-of-domain\nface images with extreme poses and viewpoints, which are\nquite common in social media applications.\nTherefore,\nthese approaches cannot be applied directly to invert x. In-\nstead, motivated by the recent observation [49] that slight\nchanges to the pretrained generator weights do not harm\nits editing abilities while achieving near-perfect reconstruc-\ntions, we finetune the pretrained generator weights \u03b8 instead\nof the encoder weights \u03d5. Specifically, we fix winv = I\u03d5(x)\nand fine-tune G\u03b8 using the following loss:\n\u03b8\u2217 = arg min\n\u03b8\nLLPIPS(x, G\u03b8(winv)) + \u03bb2L2(x, G\u03b8(winv)),\nwhere LLPIPS is the perceptual loss and L2 denotes the pixel-\nwise similarity.\nThe final inverted image x\u2217\ninv (see Fig.\n3c) can be obtained by performing a forward pass of winv\nthrough fine-tuned generator i.e., x\u2217\ninv = G\u03b8\u2217(winv).\n3.2.2\nText-guided adversarial optimization\nGiven the inverted latent code winv and fine-tuned generator\nG\u03b8\u2217(.), our goal is to adversarially perturb this latent code\nwinv in the low-dimensional generative manifold W to gen-\nerate a protected face that fools the black-box FR model,\nwhile imitating the makeup style of the text prompt tmakeup.\nTo achieve these objectives, we investigate the follow-\ning questions: (i) how to effectively extract makeup style\ninformation from tmakeup and apply it to the face image x in\nan adversarial manner?, (ii) how to regularize the optimiza-\ntion process so that the output face image is not qualitatively\nimpaired?, (iii) how to craft effective adversarial perturba-\ntions that mislead black-box FR models?, and (iv) how to\npreserve the human-perceived identity O(x) of the original\nface image while ensuring high privacy?\nThe first issue can be addressed by aligning the output\nadversarial image with the text prompt tmakeup in the em-\nbedding space of a pretrained vision-language model. The\nsecond issue is addressed by enforcing the adversarial latent\ncode to remain close to initialization winv. The third issue is\nsolved by crafting transferable text-guided adversarial faces\non a white-box surrogate model (or an ensemble of mod-\nels) with the goal of boosting the fooling rate on the black-\nbox FR model. Finally, we leverage the disentangled na-\nture of latent space in the generative model and incorporate\nan identity-preserving regularization to effectively maintain\nthe original visual identity. We now present the details of\nthe loss functions used to incorporate the above ideas.\nTextual Loss: A key ingredient of the proposed approach is\ntext-based guidance to inconspicuously hide the adversarial\nperturbations into the makeup effect. This can be naively\nachieved by aligning the representation of tmakeup and the\nadversarial face G\u03b8\u2217(w) in the common embedding space\nof a pre-trained vision-language model (e.g. CLIP [47]).\nHowever, this approach will transform the whole output im-\nage to follow the makeup style of tmakeup, which results in\nlow diversity. Therefore, we use a directional CLIP loss\nthat aligns the CLIP-space direction between the text-image\npairs of the original and adversarial images. Specifically,\nLclip = 1 \u2212 \u2206I \u00b7 \u2206T\n|\u2206I||\u2206T|,\n(2)\nwhere \u2206T\n=\nET (tmakeup) \u2212 ET (tsrc) and \u2206I\n=\nEI(G\u03b8\u2217(w)) \u2212 EI(x). Here, ET and EI are the text and\nimage encoders of the CLIP model and tsrc is the semantic\ntext of the input image x. Since we are dealing with faces,\ntsrc can be simply set as \u201cface\u201d. This loss localizes makeup\ntransfer (e.g. red lipstick) without affecting privacy.\nAdversarial Loss: Our goal is to traverse over the latent\nspace W to find adversarial latent codes on the generative\nmanifold whose face feature representation lies close to that\nof target image and far away from the original image itself\ni.e., D(xp, x) > D(xp, xt). Hence, the adversarial loss is:\nLadv = D(G\u03b8\u2217(w), xt) \u2212 D(G\u03b8\u2217(w), x),\n(3)\nwhere D(x1, x2) = 1 \u2212 cos[f(x1), f(x2))] is the cosine\ndistance. Since the malicious FR model is unknown in the\nblack-box setting, Eq. 3 cannot be solved directly. Instead,\nfollowing AMT-GAN [22], we perform adversarial opti-\nmization on an ensemble of white-box surrogate models to\nimitate the decision boundary of the unknown FR model.\nIdentity Preservation Loss: The optimization over the\ngenerative manifold ensures that the protected image xp\nis natural i.e., artifact-free, however, it does not explicitly\nenforce the protected image to preserve the identity of the\noriginal image with respect to the human observer. To mit-\nigate the issue, we take advantage of the semantic control\nexhibited by StyleGAN in its latent space. The latent code\nw \u2208 W impacts image generation by controlling different\nlevel of semantics in the output image. Specifically, latent\ncodes corresponding to the initial layers of StyleGAN con-\ntrol high-level aspects such as pose, general hairstyle, and\nface shape [27]. Adversarially perturbing these latent layers\ncan change these attributes, resulting in a change of identity\n(see Sec. 4.3). Latent codes corresponding to deeper layers\nof StyleGAN are associated with fine-level control such as\nmakeup style [2]. Therefore, we perturb only those latent\ncodes associated with deeper layers of StyleGAN, thereby\nrestricting the adversarial faces to the identity preserving\nmanifold. We further constrain the latent code to stay close\nto its initial value winv using the following regularization:\nLlatent = \u2225(w \u2299 mid) \u2212 (winv \u2299 mid)\u22252,\n(4)\nwhere \u2299 denotes element-wise product and mid is an iden-\ntity preservation mask that is 0 for the initial layers and 1\nonly for the deeper layers of the latent code. StyleGAN\nhas 18 layers, each having a dimension of 512. The iden-\ntity preservation mask is set to 1 only from layer 8 to 18.\nFinally, combining the three loss functions, we have\nLtotal = \u03bbadvLadv + \u03bbclipLclip + \u03bblatentLlatent,\n(5)\nwhere \u03bbadv., \u03bbclip, and \u03bblatent are hyperparameters.\nNote\nthat Ladv accounts for the adversarial objective in Eq. 1,\nwhile the text-guided makeup transfer (Lclip) and identity-\npreserving regularization (Llatent) implicitly enforce the nat-\nuralness constraint in Eq. 1.\n4. Experiments\nImplementation details: In all experiments, we use Style-\nGAN2 pretrained on the FFHQ face dataset as our genera-\ntive model. For adversarial text guidance, we use a vision\ntransformer-based CLIP model. For generator fine-tuning\nin the latent code initialization step, we use 450 iterations\nwith value of \u03bb2 in Eq. 2 set to 0.5. For the makeup text\ninput, we collect 40 text prompts based on the makeup style\nof diverse nature (details in supplementary material). For\nadversarial optimization, we use an Adam optimizer with\n\u03b21 and \u03b22 set to 0.9 and 0.999, respectively, and a learning\nrate of 0.01. We run the optimizer for 50 iterations to craft\nprotected faces. We set the value of \u03bbadv, \u03bbclip, and \u03bblatent\nto 1, 0.5, and 0.01, respectively. All our experiments are\nconducted on a A100 GPU with 40 GB memory.\nDatasets: We perform experiments for both face verifi-\ncation and identification settings. Face verification: We use\nCelebA-HQ [26] and LADN [16] for the impersonation at-\ntack. We select subset of 1,000 images from CelebA-HQ\nand report average results over 4 target identities provided\nby [22]. Similarly, for LADN, we divide the 332 images\navailable into 4 groups, where images in each group aim\nto impersonate the target identities provided by [22]. For\ndodging attack, we use CelebA-HQ [26] and LFW [23]\ndatasets. Specifically, we select 500 subjects at random and\neach subject has a pair of faces. Face identification: For\nimpersonation and dodging, we use CelebA-HQ [26] and\nLFW [23] as our evaluation set. For both datasets, we ran-\ndomly select 500 subjects, each with a pair of faces. We as-\nsign one image in the pair to the gallery set and the other to\nthe probe set. Both impersonation and dodging attacks are\nperformed on the probe set. For impersonation, we insert\n4 target identities provided by [22] into the gallery set. A\nmore detailed description of all datasets and pre-processing\nsteps is provided in the supplementary material.\nTarget Models: We aim to protect user facial privacy\nby attacking four FR model with diverse back bones in the\nblack-box settings. The target models include IRSE50 [21],\nIR152 [9], FaceNet [51], and MobileFace [5]. Following\nstandard protocol, we align and crop the face images us-\ning MTCNN [75] before giving them as input to FR mod-\nels. Further, we also report privacy protection performance\nbased on commercial FR API including Face++ and Ten-\ncent Yunshentu FR platforms.\nEvaluation metrics: Following [70], we use protection\nsuccess rate (PSR) to evaluate the proposed approach. PSR\nis defined as the fraction of protected faces missclassified\nby the malicious FR system.\nTo evaluate PSR, we use\nthe thresholding and closed set strategies for face verifica-\ntion and identification, respectively. For face identification,\nwe also use Rank-N targeted identity success rate (Rank-N-\nT) and untargeted identity success rate (Rank-N-U), where\nRank-N-T means that target image xt will appear at least\nonce in the top N candidates shortlisted from the gallery\nand Rank-N-U implies that the top N candidate list does not\nhave the same identity as that of original image x. We also\nreport results of PSNR (dB), SSIM, and FID [19] scores to\nevaluate the imperceptibility of method. Large PSNR and\nSSIM [65] indicates better match with the original images,\nwhile low FID score indicates more realistic images. For\ncommercial APIs, we directly report the confidence score\nreturned by the respective servers.\nBaseline methods: We compare our approach with re-\ncent noise-based and makeup based facial privacy protec-\ntion approaches. Noise based methods include PGD [37],\nMI-FGSM [10], TI-DIM [11], and TIP-IM [70], whereas\nmakeup-based approaches are Adv-Makeup [71] and AMT-\nGAN [22]. We want to highlight that TIP-IM and AMT-\nGAN are considered the state-of-the-art (SOTA) for face\nprivacy protection against black-box FR systems in noise-\nbased and unrestricted settings, respectively. TIP-IM also\nincorporate multi-target objective in its optimization to find\nthe optimal target image among multiple targets. For fair\ncomparison, we use its single target variant.\n4.1. Experimental Results\nIn this section, we present experimental results of our\napproach in black-box settings on four different pretrained\nTable 2. Protection success rate (PSR %) of black-box impersonation attack under the face verification task. For each column, the other\nthree FR systems are used as surrogates to generate the protected faces.\nMethod\nCelebA-HQ\nLADN-Dataset\nAverage\nIRSE50\nIR152\nFaceNet\nMobileFace\nIRSE50\nIR152\nFaceNet\nMobileFace\nClean\n7.29\n3.80\n1.08\n12.68\n2.71\n3.61\n0.60\n5.11\n4.61\nInverted\n5.57\n2.77\n0.60\n13.32\n6.80\n4.51\n0.25\n11.66\n5.68\nPGD [37]\n36.87\n20.68\n1.85\n43.99\n40.09\n19.59\n3.82\n41.09\n25.60\nMI-FGSM [10]\n45.79\n25.03\n2.58\n45.85\n48.90\n25.57\n6.31\n45.01\n30.63\nTI-DIM [11]\n63.63\n36.17\n15.30\n57.12\n56.36\n34.18\n22.11\n48.30\n41.64\nAdv-Makeup(IJCAI\u201921) [71]\n21.95\n9.48\n1.37\n22.00\n29.64\n10.03\n0.97\n22.38\n14.72\nTIP-IM(ICCV\u201921) [70]\n54.40\n37.23\n40.74\n48.72\n65.89\n43.57\n63.50\n46.48\n50.06\nAMT-GAN(CVPR\u201922) [22]\n76.96\n35.13\n16.62\n50.71\n89.64\n49.12\n32.13\n72.43\n52.84\nOurs\n81.10\n48.42\n41.72\n75.26\n91.57\n53.31\n47.91\n79.94\n64.90\nTable 3. Protection success rate (PSR %) of black-box dodging (top) and impersonation (bottom) attacks under the face identification\ntask for LFW dataset [23]. For each column, the other three FR systems are used as surrogates to generate the protected faces. R1-U:\nRank-1-Untargeted, R5-U: Rank-5-Untargeted, R1-T: Rank-1-Targeted, R5-T: Rank-5-Targeted.\nMethod\nIRSE50\nIR152\nFaceNet\nMobileFace\nAverage\nR1-U\nR5-U\nR1-U\nR5-U\nR1-U\nR5-U\nR1-U\nR5-U\nR1-U\nR5-U\nMI-FGSM [10]\n70.2\n42.6\n58.4\n41.8\n59.2\n34.0\n68.0\n47.2\n63.9\n41.4\nTI-DIM [11]\n79.0\n51.2\n67.4\n54.0\n74.4\n52.0\n79.2\n61.6\n75.0\n54.7\nTIP-IM(ICCV\u201921) [70]\n81.4\n52.2\n71.8\n54.6\n76.0\n49.8\n82.2\n63.0\n77.8\n54.9\nOurs\n86.6\n59.4\n73.4\n56.6\n83.8\n51.2\n85.0\n66.8\n82.2\n58.5\nR1-T\nR5-T\nR1-T\nR5-T\nR1-T\nR5-T\nR1-T\nR5-T\nR1-T\nR5-T\nMI-FGSM [10]\n4.0\n10.2\n3.2\n14.2\n9.0\n18.8\n8.4\n22.4\n6.15\n16.4\nTI-DIM [11]\n4.0\n13.6\n7.8\n19.6\n18.0\n32.8\n21.6\n39.0\n12.85\n26.25\nTIP-IM(ICCV\u201921) [70]\n8.0\n28.2\n11.6\n31.2\n25.2\n56.8\n34.0\n51.4\n19.7\n41.9\nOurs\n11.2\n37.8\n16.0\n51.2\n27.4\n54.0\n39.0\n61.2\n23.4\n51.05\nMethod\nFID \u2193\nPSR Gain \u2191\nAdv-Makeup [71]\n4.23\n0\nTIP-IM [70]\n38.73\n35.34\nAMT-GAN [22]\n34.44\n38.12\nOurs\n26.62\n50.18\nTable 4. FID compar-\nison. PSR Gain is ab-\nsolute gain in PSR rel-\native to Adv-Makeup.\nFR models under face verification and identification tasks.\nTo generate protected images, we use three FR models as a\nsurrogate to imitate the decision boundary of the fourth FR\nmodel. All results are averaged over 5 text based makeup\nstyles that are provided in the supplementary material.\nFor face verification experiments, we set the system\nthreshold value at 0.01 false match rate for each FR model\ni.e., IRSE50 (0.241), IR152 (0.167), FaceNet (0.409), and\nMobileFace (0.302). Quantitative results in terms of PSR\nfor impersonation attack under the face verification task are\nshown in Tab. 2. Our approach is able to achieve an aver-\nage absolute gain of about 12% and 14% over SOTA unre-\nstricted [22] and noise-based [70] facial privacy protection\nmethods, respectively. Qualitative results are shown in Fig.\n1 which shows that protected faces generated by our ap-\nproach are more realistic. Results for dodging attacks under\nface verification are provided in the supplementary mate-\nrial. In Tab. 3, we also provide PSR vales under the face\nidentification task for dodging (untargeted) and imperson-\nation attacks. Our approach consistently outperforms recent\nmethods at both Rank-1 and Rank-5 settings. We empha-\nsize that we are the first to show effectiveness of generative\nmodels in offering untargeted privacy protection (dodging)\nin a more practical identification setting. Since AMT-GAN\nand Adv-Makeup are originally trained to impersonate tar-\nget identity under the verification task, we have not included\nthem in Tab. 3. Qualitative results for LFW and CelebA are\nprovided in the supplementary material.\nWe report FID scores (lower is better) of our approach\nin Tab. 4 for CelebA and LADN datasets to measure nat-\nuralness. Adv-Makeup has the lowest FID score as it only\napplies makeup to the eye region without changing the rest\nof the face.\nHowever, this kind of restriction results in\npoor PSR. Our method has lower FID compared to TIP-IM\nand AMT-GAN and achieves the highest PSR. We provide\nPSNR and SSIM results in the supplementary material.\n4.2. Effectiveness in Real-World Applications\nWe now show the effectiveness of our approach to pro-\ntect facial images (through targeted impersonation) against\ncommercial API such as Face++ and Tencent Yunshentu FR\nplatform operating in the verification mode. These APIs re-\nturn confidence scores between 0 to 100 to measure whether\ntwo images are similar or not, where a high confidence score\nindicates high similarity. As the training data and model\nparameters of these propriety FR models are unknown, it\n33.4\n31.4\n39.2\n44.6\n42.8\n48.2\n48.8\n52.8\n28.2\n27.4\n59.8\n65.2\n61.4\n67.2\n67.35\n73.82\nClean\nPGD\nMI-FGSM\nTI-DIM\nAdv-Makeup\nTIP-IM\nAMT-GAN\nProposed\n0\nCelebA-HQ\nLADN Dataset\n10\n20\n30\n40\n50\n60\n70\n80\nFigure 4. Average confidence score (higher is better) returned by a\nreal-world face verification API, Face++, for impersonation attack.\nOur approach has a higher confidence score than state-of-the-art\nmakeup and noise-based facial privacy protection methods.\nTable 5. Impact of \u03bblatent on FID score and PSR.\n\u03bblatent\n0.5\n0.1\n0.05\n0.01\n0.005\n0.0001\n0\nFID\n11.6\n21.4\n25.2\n27.8\n30.1\n38.4\n43.2\nPSR (%)\n31.2\n39.0\n57.4\n76.2\n83.8\n90.0\n93.6\neffectively mimics a real-world scenario. We protect 100\nfaces randomly selected from CelebA-HQ using the base-\nlines and the proposed method. In Fig. 4, we show the\naverage confidence score returned by Face++ against these\nimages. These results indicate that our method has a high\nPSR compared to baselines. We defer more details and re-\nsults for Tencent Yunshentu API to supplementary material.\n4.3. Ablation Studies\nNext, we report some ablations to evaluate the contribu-\ntions of our loss components.\nMakeup based text guidance: As shown in Fig. 5 (top), in\nthe absence of text guidance, resulting images may contain\nartifacts due to increased perturbations induced by the ad-\nversarial objective. Text-guidance effectively hides the per-\nturbations in the makeup, leading to more natural looking\nimages. It also provides the user more flexibility to select a\ndesired makeup style compared to a reference image.\nIdentity preserving regularization: Optimizing over the\nwhole latent space provides more degrees of freedom and\nincreases the PSR. However, it does not explicitly enforce\nadversarial optimization to preserve the user identity as\nshown in Fig. 5 (bottom). The proposed identity preserving\nregularization effectively preserves identity, while imitating\nthe desire makeup style.\nImpact of latent loss weight: Decreasing the weight as-\nsigned to the latent loss \u03bblatent results in an increase in both\nthe FID score and PSR (and vice versa). Allowing the la-\ntent to deviate more from the initial inverted latent code of\nthe given face image often results in artifacts caused by the\nadversarial loss, degrading naturalness but aiding privacy.\nRobustness against textual variations. Finally, we eval-\nuate the impact of different textual styles on the PSR. We\nOriginal\nw/o text guidance\nw text guidance\nOriginal\nw/o ID \nregularization\nw ID \nregularization\nFigure 5. Top: Effect of makeup-based text guidance on the visual\nquality of the output images. Output images are able to imperson-\nate the target identity for face verification. Text-prompt is \u201ctanned\nmakeup with red lipstick\u201d. Bottom: Optimizing over all latent\ncodes changes the identity of the protected image. Our identity-\npreserving regularization enforces the adversarial optimization to\nsearch for latent codes that hide the perturbations in the makeup\neffect while simultaneously preserving visual identity.\nTable 6.\nImpact of different textual makeup styles on PSR.\nMakeup styles are \u201ctanned\u201d, \u201cpale\u201d, \u201cpink eyeshadows\u201d, \u201cred\nlipstick\u201d, and \u201cMatte\u201d. Std. denotes standard deviation.\nt1\nmakeup\nt2\nmakeup\nt3\nmakeup\nt4\nmakeup\nt5\nmakeup\nStd.\nPSR\n74.1\n77.3\n78.4\n78.7\n79.2\n1.24\nselect five text-based makeup styles to protect 1000 images\nof CelebA-HQ using our method. Results in Tab. 6 shows\nthat PSR does not change significantly (low standard devi-\nation) for different makeup styles, indicating robustness of\nour approach wrt different text-based makeup styles.\n5. Conclusion\nWe have proposed a framework to protect privacy of face\nimages on online platforms by carefully searching for ad-\nversarial codes in the low-dimensional latent manifold of a\npre-trained generative model. We have shown that incorpo-\nrating a makeup text-guided loss and an identity preserving\nregularization effectively hides the adversarial perturbations\nin the makeup style, provides images with high quality, and\npreserves human-perceived identity. While this approach is\nrobust to the user-defined text-prompt and target identity,\nit would be beneficial if the text-prompt and target identity\ncan be automatically selected based on the given face im-\nage. Limitations of our method include high computational\ncost at the time of protected face generation.\nReferences\n[1] Shane Ahern, Dean Eckles, Nathaniel S Good, Simon King,\nMor Naaman, and Rahul Nair. Over-exposed? privacy pat-\nterns and considerations in online and mobile photo sharing.\nIn Proceedings of the SIGCHI conference on Human factors\nin computing systems, pages 357\u2013366, 2007. 2\n[2] Amit H Bermano, Rinon Gal, Yuval Alaluf, Ron Mokady,\nYotam Nitzan, Omer Tov, Oren Patashnik, and Daniel\nCohen-Or. State-of-the-art in the architecture, methods and\napplications of stylegan. In Computer Graphics Forum, vol-\nume 41, pages 591\u2013611. Wiley Online Library, 2022. 2, 6\n[3] Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and\nDavid A Forsyth.\nUnrestricted adversarial examples via\nsemantic manipulation.\narXiv preprint arXiv:1904.06347,\n2019. 2, 3\n[4] Jia-Wei Chen, Li-Ju Chen, Chia-Mu Yu, and Chun-Shien Lu.\nPerceptual indistinguishability-net (pi-net): Facial image ob-\nfuscation with manipulable semantics.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6478\u20136487, 2021. 3\n[5] Sheng Chen, Yang Liu, Xiang Gao, and Zhen Han. Mobile-\nfacenets: Efficient cnns for accurate real-time face verifica-\ntion on mobile devices. In Chinese Conference on Biometric\nRecognition, pages 428\u2013438. Springer, 2018. 6\n[6] Valeriia Cherepanova, Micah Goldblum, Harrison Foley,\nShiyuan Duan, John P Dickerson, Gavin Taylor, and Tom\nGoldstein. Lowkey: Leveraging adversarial attacks to pro-\ntect social media users from facial recognition. In Interna-\ntional Conference on Learning Representations, 2020. 2, 3\n[7] William L Croft, J\u00a8org-R\u00a8udiger Sack, and Wei Shi. Differ-\nentially private facial obfuscation via generative adversarial\nnetworks. Future Generation Computer Systems, 129:358\u2013\n379, 2022. 3\n[8] Ali Dabouei, Sobhan Soleymani, Jeremy Dawson, and\nNasser Nasrabadi. Fast geometrically-perturbed adversarial\nfaces. In 2019 IEEE Winter Conference on Applications of\nComputer Vision (WACV), pages 1979\u20131988. IEEE, 2019. 3\n[9] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\nZafeiriou. Arcface: Additive angular margin loss for deep\nface recognition.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n4690\u20134699, 2019. 6\n[10] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su,\nJun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversar-\nial attacks with momentum.\nIn Proceedings of the 2018\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR\u201918), pages 9185\u20139193, 2018. 6, 7\n[11] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu.\nEvading defenses to transferable adversarial examples by\ntranslation-invariant attacks.\nIn Proceedings of the 2019\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR\u201919), pages 4312\u20134321, 2019. 6, 7\n[12] Hang Du, Hailin Shi, Dan Zeng, Xiao-Ping Zhang, and\nTao Mei.\nThe elements of end-to-end deep face recogni-\ntion: A survey of recent advances. ACM Computing Surveys\n(CSUR), 54(10s):1\u201342, 2022. 2\n[13] Yifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao. A\nsurvey of vision-language pre-trained models. arXiv preprint\narXiv:2202.10936, 2022. 3\n[14] Kevin Frans, Lisa B Soros, and Olaf Witkowski. Clipdraw:\nExploring text-to-drawing synthesis through language-image\nencoders. arXiv preprint arXiv:2106.14843, 2021. 3\n[15] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\nand Yoshua Bengio. Generative adversarial networks. CoRR,\nabs/1406.2661, 2014. 2\n[16] Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, and\nChi-Keung Tang. Ladn: Local adversarial disentangling net-\nwork for facial makeup and de-makeup. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 10481\u201310490, 2019. 6\n[17] Nitzan Guetta,\nAsaf Shabtai,\nInderjeet Singh,\nSatoru\nMomiyama, and Yuval Elovici.\nDodging attack us-\ning carefully crafted natural makeup.\narXiv preprint\narXiv:2109.06467, 2021. 3\n[18] Rebecca Heilweil.\nThe world\u2019s scariest facial recognition\ncompany explained. Vox, May, 8, 2020. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[20] Kashmir Hill. The secretive company that might end privacy\nas we know it. The New York Times, 18:2020, 2020. 2\n[21] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132\u20137141, 2018. 6\n[22] Shengshan Hu, Xiaogeng Liu, Yechao Zhang, Minghui Li,\nLeo Yu Zhang, Hai Jin, and Libing Wu. Protecting facial pri-\nvacy: Generating adversarial identity masks via style-robust\nmakeup transfer. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n15014\u201315023, 2022. 1, 2, 3, 5, 6, 7\n[23] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric\nLearned-Miller.\nLabeled faces in the wild: A database\nforstudying face recognition in unconstrained environments.\nIn Workshop on faces in\u2019Real-Life\u2019Images: detection, align-\nment, and recognition, 2008. 6, 7\n[24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1125\u20131134,\n2017. 3\n[25] Kazuya Kakizaki and Kosuke Yoshida. Adversarial image\ntranslation: Unrestricted adversarial examples in face recog-\nnition systems. arXiv preprint arXiv:1905.03421, 2019. 2,\n3\n[26] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. In International Conference on Learning Rep-\nresentations, 2018. 6\n[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 2, 6\n[28] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-\nfusionclip: Text-guided diffusion models for robust image\nmanipulation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2426\u2013\n2435, 2022. 3\n[29] Stepan Komkov and Aleksandr Petiushko. Advhat: Real-\nworld adversarial attack on arcface face id system.\nIn\n2020 25th International Conference on Pattern Recognition\n(ICPR), pages 819\u2013826. IEEE, 2021. 3\n[30] Zhenzhong Kuang, Zhiqiang Guo, Jinglong Fang, Jun Yu,\nNoboru Babaguchi, and Jianping Fan.\nUnnoticeable syn-\nthetic face replacement for image privacy protection. Neu-\nrocomputing, 457:322\u2013333, 2021. 3\n[31] Zhenzhong Kuang, Huigui Liu, Jun Yu, Aikui Tian, Lei\nWang, Jianping Fan, and Noboru Babaguchi.\nEffective\nde-identification generative adversarial network for face\nanonymization. In Proceedings of the 29th ACM Interna-\ntional Conference on Multimedia, pages 3182\u20133191, 2021.\n3\n[32] Gihyun Kwon and Jong Chul Ye. Clipstyler: Image style\ntransfer with a single text condition.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18062\u201318071, 2022. 3\n[33] Tao Li and Min Soo Choi. Deepblur: A simple and effec-\ntive method for natural image obfuscation. arXiv preprint\narXiv:2104.02655, 1, 2021. 3\n[34] Tao Li and Lei Lin.\nAnonymousnet:\nNatural face de-\nidentification with measurable privacy.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 0\u20130, 2019. 3\n[35] Fangcheng Liu, Chao Zhang, and Hongyang Zhang.\nTo-\nwards transferable unrestricted adversarial examples with\nminimum changes. arXiv preprint arXiv:2201.01102, 2022.\n3\n[36] Suolan Liu, Lizhi Kong, and Hongyuan Wang. Face detec-\ntion and encryption for privacy preserving in surveillance\nvideo. In Chinese Conference on Pattern Recognition and\nComputer Vision (PRCV), pages 162\u2013172. Springer, 2018. 3\n[37] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards deep learning\nmodels resistant to adversarial attacks. In Proceedings of the\n6th International Conference on Learning Representations\n(ICLR\u201918), 2018. 6, 7\n[38] Bla\u02c7z Meden, Peter Rot, Philipp Terh\u00a8orst, Naser Damer, Ar-\njan Kuijper, Walter J Scheirer, Arun Ross, Peter Peer, and\nVitomir \u02c7Struc. Privacy\u2013enhancing face biometrics: A com-\nprehensive survey. IEEE Transactions on Information Foren-\nsics and Security, 2021. 2, 3\n[39] Dongbin Na, Sangwoo Ji, and Jong Kim. Unrestricted black-\nbox adversarial attack using gan with limited queries. arXiv\npreprint arXiv:2208.11613, 2022. 2\n[40] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 3\n[41] Seong Joon Oh, Rodrigo Benenson, Mario Fritz, and Bernt\nSchiele. Faceless person recognition: Privacy implications in\nsocial media. In European Conference on Computer Vision,\npages 19\u201335. Springer, 2016. 3\n[42] Seong Joon Oh, Mario Fritz, and Bernt Schiele. Adversarial\nimage perturbation for privacy protection a game theory per-\nspective. In 2017 IEEE International Conference on Com-\nputer Vision (ICCV), pages 1491\u20131500. IEEE, 2017. 3\n[43] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman.\nDeep face recognition. 2015. 1\n[44] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2085\u20132094,\n2021. 3\n[45] P Jonathon Phillips, Amy N Yates, Ying Hu, Carina A\nHahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cava-\nzos, G\u00b4eraldine Jeckeln, Rajeev Ranjan, Swami Sankara-\nnarayanan, et al.\nFace recognition accuracy of foren-\nsic examiners, superrecognizers, and face recognition algo-\nrithms. Proceedings of the National Academy of Sciences,\n115(24):6171\u20136176, 2018. 2\n[46] Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Be-\nlongie, and Ser-Nam Lim.\nRobustness and generalization\nvia generative adversarial training.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 15711\u201315720, 2021. 3\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 3, 5\n[48] Rajeev Ranjan, Swami Sankaranarayanan, Ankan Bansal,\nNavaneeth Bodla, Jun-Cheng Chen, Vishal M Patel, Car-\nlos D Castillo, and Rama Chellappa. Deep learning for un-\nderstanding faces: Machines may be just as good, or better,\nthan humans. IEEE Signal Processing Magazine, 35(1):66\u2013\n83, 2018. 2\n[49] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages. ACM Transactions on Graphics (TOG), 42(1):1\u201313,\n2022. 5\n[50] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 3\n[51] Florian Schroff, Dmitry Kalenichenko, and James Philbin.\nFacenet: A unified embedding for face recognition and clus-\ntering. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 815\u2013823, 2015. 6\n[52] Sachith Seneviratne, Nuran Kasthuriarachchi, Sanka Ras-\nnayaka, Danula Hettiachchi, and Ridwan Shariffdeen. Does\na face mask protect my privacy?: Deep learning to pre-\ndict protected attributes from masked face images. In Aus-\ntralasian Joint Conference on Artificial Intelligence, pages\n91\u2013102. Springer, 2022. 3\n[53] Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li,\nHaitao Zheng, and Ben Y Zhao. Fawkes: Protecting pri-\nvacy against unauthorized deep learning models.\nIn 29th\nUSENIX security symposium (USENIX Security 20), pages\n1589\u20131604, 2020. 2, 3\n[54] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and\nMichael K Reiter. A general framework for adversarial ex-\namples with objectives. ACM Transactions on Privacy and\nSecurity (TOPS), 22(3):1\u201330, 2019. 3\n[55] Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon.\nConstructing unrestricted adversarial examples with gener-\native models. Advances in Neural Information Processing\nSystems, 31, 2018. 2, 3\n[56] Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool,\nBernt Schiele, and Mario Fritz. Natural and effective obfus-\ncation by head inpainting. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n5050\u20135059, 2018. 3\n[57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan\nBruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-\ntriguing properties of neural networks. In International Con-\nference on Learning Representations, 2014. 2\n[58] Huan Tian, Tianqing Zhu, and Wanlei Zhou. Fairness and\nprivacy preservation for facial images: Gan-based methods.\nComputers & Security, 122:102902, 2022. 3\n[59] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and\nDaniel Cohen-Or.\nDesigning an encoder for stylegan im-\nage manipulation. ACM Transactions on Graphics (TOG),\n40(4):1\u201314, 2021. 5\n[60] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao.\nClip-nerf: Text-and-image driven manip-\nulation of neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3835\u20133844, 2022. 3\n[61] Mei Wang and Weihong Deng. Deep face recognition: A\nsurvey. Neurocomputing, 429:215\u2013244, 2021. 1, 2\n[62] Shunxin Wang, Una M Kelly, and Raymond NJ Veld-\nhuis. Gender obfuscation through face morphing. In 2021\nIEEE International Workshop on Biometrics and Forensics\n(IWBF), pages 1\u20136. IEEE, 2021. 3\n[63] Ya Wang, Tianlong Bao, Chunhui Ding, and Ming Zhu.\nFace recognition in real-world surveillance videos with deep\nlearning method. In 2017 2nd international conference on\nimage, vision and computing (icivc), pages 239\u2013243. IEEE,\n2017. 2\n[64] Yinggui Wang, Jian Liu, Man Luo, Le Yang, and Li Wang.\nPrivacy-preserving face recognition in the frequency domain.\n2022. 3\n[65] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600\u2013612, 2004. 6\n[66] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhen-\ntao Tan, Lu Yuan, Weiming Zhang, and Nenghai Yu. Hair-\nclip: Design your hair by text and reference image. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18072\u201318081, 2022. 3\n[67] Emily Wenger, Shawn Shan, Haitao Zheng, and Ben Y Zhao.\nSok: Anti-facial recognition technology.\narXiv preprint\narXiv:2112.04558, 2021. 2\n[68] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan\nLiu, and Dawn Song. Spatially transformed adversarial ex-\namples. arXiv preprint arXiv:1801.02612, 2018. 2, 3\n[69] Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei\nGao, Xiaolu Zhang, Jun Zhou, and Jun Zhu.\nImprov-\ning transferability of adversarial patches on face recognition\nwith generative models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 11845\u201311854, 2021. 3\n[70] Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu,\nYuefeng Chen, and Hui Xue. Towards face encryption by\ngenerating adversarial identity masks. In Proceedings of the\n2021 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV\u201921), pages 3897\u20133907, 2021. 1, 2, 3, 6, 7\n[71] Bangjie Yin, Wenxuan Wang, Taiping Yao, Junfeng Guo,\nZelun Kong, Shouhong Ding, Jilin Li, and Cong Liu. Adv-\nmakeup: A new imperceptible and transferable attack on\nface recognition. In Proceedings of the 30th International\nJoint Conference on Artificial Intelligence (IJCAI\u201921), pages\n1252\u20131258, 2021. 2, 6, 7\n[72] Bangjie Yin, Wenxuan Wang, Taiping Yao, Junfeng Guo,\nZelun Kong, Shouhong Ding, Jilin Li, and Cong Liu. Adv-\nmakeup: A new imperceptible and transferable attack on face\nrecognition. arXiv preprint arXiv:2105.03162, 2021. 2, 3\n[73] Shengming Yuan, Qilong Zhang, Lianli Gao, Yaya Cheng,\nand Jingkuan Song.\nNatural color fool:\nTowards\nboosting black-box unrestricted attacks.\narXiv preprint\narXiv:2210.02041, 2022. 3\n[74] Jiaming Zhang, Jitao Sang, Xian Zhao, Xiaowen Huang,\nYanfeng Sun, and Yongli Hu. Adversarial privacy-preserving\nfilter. In Proceedings of the 28th ACM International Confer-\nence on Multimedia, pages 1423\u20131431, 2020. 2\n[75] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.\nJoint face detection and alignment using multitask cascaded\nconvolutional networks.\nIEEE signal processing letters,\n23(10):1499\u20131503, 2016. 6\n[76] Zhengyu Zhao, Zhuoran Liu, and Martha Larson. Towards\nlarge yet imperceptible adversarial image perturbations with\nperceptual color distance. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1039\u20131048, 2020. 2, 3\n[77] Yaoyao Zhong and Weihong Deng. Opom: Customized in-\nvisible cloak towards face privacy protection. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 2022.\n2, 3\n[78] Jizhe Zhou and Chi-Man Pun.\nPersonal privacy protec-\ntion via irrelevant faces tracking and pixelation in video live\nstreaming. IEEE Transactions on Information Forensics and\nSecurity, 16:1088\u20131103, 2020. 3\n[79] Zheng-An Zhu, Yun-Zhong Lu, and Chen-Kuo Chiang. Gen-\nerating adversarial examples by makeup attacks on face\nrecognition. In 2019 IEEE International Conference on Im-\nage Processing (ICIP), pages 2516\u20132520. IEEE, 2019. 3\n"
  },
  {
    "title": "Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2306.12760.pdf",
    "upvote": "8",
    "text": "Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural\nRadiance Fields\nOri Gordon\nThe Hebrew University\nori.gordon@mail.huji.ac.il\nOmri Avrahami\nThe Hebrew University\nomri.avrahami@mail.huji.ac.il\nDani Lischinski\nThe Hebrew University\ndanix@mail.huji.ac.il\n(a)\n(b)\nFigure 1: Overview. (a) Training: Given a NeRF scene F O\n\u03b8 , our pipeline trains a NeRF generator model F G\n\u03b8 , initialized\nwith F O\n\u03b8 weights and guided by a similarity loss defined by a language-image model such as CLIP [51], to synthesize a new\nobject inside a user-specified ROI. This is achieved by casting rays and sampling points for the rendering process [42] only\ninside the ROI box. Our method introduces augmentations and priors to get more natural results. (b) Blending process:\nAfter training, we render the edited scene by blending the sample points generated by the two models along each view ray.\nAbstract\nEditing a local region or a specific object in a 3D scene\nrepresented by a NeRF or consistently blending a new re-\nalistic object into the scene is challenging, mainly due to\nthe implicit nature of the scene representation. We present\nBlended-NeRF, a robust and flexible framework for editing a\nspecific region of interest in an existing NeRF scene, based\non text prompts, along with a 3D ROI box. Our method\nleverages a pretrained language-image model to steer the\nsynthesis towards a user-provided text prompt, along with\na 3D MLP model initialized on an existing NeRF scene\nto generate the object and blend it into a specified region\nin the original scene. We allow local editing by localiz-\ning a 3D ROI box in the input scene, and blend the con-\ntent synthesized inside the ROI with the existing scene using\na novel volumetric blending technique. To obtain natural\nlooking and view-consistent results, we leverage existing\nand new geometric priors and 3D augmentations for im-\nproving the visual fidelity of the final result. We test our\nframework both qualitatively and quantitatively on a vari-\nety of real 3D scenes and text prompts, demonstrating real-\nistic multi-view consistent results with much flexibility and\ndiversity compared to the baselines. Finally, we show the\napplicability of our framework for several 3D editing ap-\nplications, including adding new objects to a scene, remov-\ning/replacing/altering existing objects, and texture conver-\nsion. 1\n1. Introduction\nIn the last few years we have witnessed exciting devel-\nopments in neural implicit representations [59, 63, 16, 64,\n37, 65]. In particular, implicit representations of 3D scenes\n[60, 39, 58, 28, 49, 42, 6, 5] have enabled unprecedented\nquality and reliability in 3D reconstruction and novel view\nsynthesis. The pioneering work of Mildenhall et al. [42]\nintroduced NeRFs, MLP-based neural models that implic-\nitly represent a scene as a continuous volume and radiance\nfields from a limited number of observations, producing\nhigh-quality images from novel views via volume render-\ning.\nHowever, editing a scene represented by a NeRF is non-\ntrivial, mainly because the scene is encoded in an im-\n1Project page: www.vision.huji.ac.il/blended-nerf\narXiv:2306.12760v2  [cs.CV]  7 Sep 2023\n\u201dA DLSR photo of dunes of sand.\u201d\n\u201dA DLSR photo of ice and snow.\u201d\n\u201dA DLSR photo of dunes of sand.\u201d\n\u201dA DLSR photo of ice and snow.\u201d\nFigure 2: Large object replacement. Here we preform object replacement to the blender ship scene by localizing the ROI\nbox to include the sea and the bottom of the ship and training our model to steer the edit towards the given text prompts.\nplicit manner by the model\u2019s weights, in contrast to ex-\nplicit representations, such as meshes, voxel grids, or point\nclouds.\nNeRFs offer no explicit separation between the\nvarious components that define the object, such as shape,\ncolor, or material. In contrast to local edits in images, e.g.,\n[3, 2, 7, 45, 54, 24, 9], where the edit is done in pixel space\nwith all the required information appearing in a single view,\nediting a NeRF-represented scene is more challenging due\nto the requirement for consistency across multiple views be-\ntween the new and the original NeRF scenes.\nThe first works attempting to edit NeRF scenes focused\non the removal of local parts, changing color, or shape trans-\nfer on one class of synthetic data, guided by user scrib-\nbles or latent code of another object in the class [36]. In\nCLIP-NeRF [67], editing of the entire scene is preformed\nby text guidance and displacements to the latent represen-\ntation of the input. They mainly focus on synthetic objects\nfrom one class, or global color changes for realistic scenes.\nKobayashi et al. [29] perform semantic decomposition of\nthe scene components by learning a feature field that maps\neach 3D coordinate to a descriptor representing a semantic\nfeature, and allow zero-shot segmentation for local editing\non a specific semantic class. Alternatively, Benaim et al. [8]\nsuggest separating the volumetric representation of a fore-\nground object from its background using a set of 2D masks\nper training view. These works have limited localization\nabilities and focus on the separation methods. They demon-\nstrate manipulations such as object removal, color change,\nand transformations such as shift, rotation, and scale.\nIn this work, we present our approach for ROI-based\nediting of NeRF scenes guided by a text prompt or an im-\nage patch that: (1) can operate on any region of a real-world\nscene, (2) modifies only the region of interest, while pre-\nserving the rest of the scene without learning a new fea-\nture space or requiring a set of two-dimensional masks, (3)\ngenerates natural-looking and view-consistent results that\nblend with the existing scene, (4) is not restricted to a spe-\ncific class or domain, and (5) enables complex text guided\nmanipulations such as object insertion/replacement, objects\nblending and texture conversion.\nTo this end, we utilize a pretrained language-image\nmodel, e.g., CLIP [51], and a NeRF model [42] initialized\non existing NeRF scene as our generator for synthesizing a\nnew object and blend it into the scene in the region of inter-\nest (ROI). We use CLIP to steer the generation process to-\nwards the user-provided text prompt, enabling blended gen-\neration of diverse 3D objects.\nTo enable general local edits in any region, while pre-\nserving the rest of the scene, we localize a 3D box inside a\ngiven NeRF scene. To blend the synthesized content inside\nthe ROI with the base scene, we propose a novel volumet-\nric blending approach that merges the original and the syn-\nthesized radiance fields by blending the sampled 3D points\nalong each camera ray.\nWe show that using this pipeline naively to perform the\nedit is insufficient, generating low quality incoherent and\ninconsistent results.\nThus, we utilize the augmentations\nand priors suggested in [27] and introduce additional pri-\nors and augmentations, such as depth regularization, pose\nsampling, and directional dependent prompts to get more\nrealistic, natural-looking and 3D consistent results. Finally,\nwe conduct extensive experiments to evaluate our frame-\nwork and the effect of our additional constraints and priors.\nWe perform an in-depth comparison with the baseline and\nshow the applicability of our approach on a series of 3D\nediting applications using a variety of real 3D scenes.\n2. Related Work\nNeural Implicit Representations have gained much\npopularity in the fields of computer vision and graphics in\nboth 2D and 3D [59, 60, 58, 49, 39, 63, 16, 28]. Among\ntheir advantages is their ability to capture complex and di-\nverse patterns and to provide a continuous representation of\nthe underlying scene. They are resolution independent, yet\ncompact, compared to explicit representations of high reso-\nlution 2D images, or meshes and point clouds in 3D. NeRFs\n[42, 5, 6] learn to represent a 3D scene as a continuous vol-\nume and radiance fields using the weights of a multilayer\nperceptron (MLP). Given a 3D position x and view direc-\ntion (\u03b8, \u03d5), NeRF outputs the density \u03c3 and color c at x.\nNovel views of the scene can thus be rendered by accumu-\nlating the colors and densities along a view ray r(t) passing\nthrough each pixel, using an approximation to the classical\nvolume rendering equation using the quadrature rule [38]:\nC(r) =\nN\nX\ni=1\nTi(1 \u2212 exp(\u2212\u03c3i\u03b4i))ci, Ti = exp(\u2212\ni\u22121\nX\nj=1\n\u03c3j\u03b4j)\n(1)\nwhere \u03b4i = ti+1 \u2212 ti is the distance between adjacent sam-\nples and Ti can be interpreted as the degree of transmittance\nat point xi along the ray. The inputs are embedded into a\nhigh-dimensional space using a high frequency sinusoidal\npositional encoding \u03b3(x) to enable better fitting for high\nfrequency variations in the data [52, 66]:\n\u03b3(x) = [cos(2lx), sin(2lx)]L\u22121\nl=0\n(2)\nNeRF 3D Generation. NeRFs inspired follow-up works\nto synthesize new NeRF objects from scratch.\nThe first\nmethods used NeRF combined with GANs [1, 20, 22] to\ndesign 3D-aware generators [21, 11, 15, 46, 47, 57, 75].\nGRAF [57] adopts shape and appearance codes to condi-\ntionally synthesize NeRF and GIRAFF [47], StyleNeRF\n[21] utilizes NeRF to render features instead of RGB col-\nors and adopt a two-stage strategy, where they render low-\nresolution feature maps first and then up-sample the feature\nmaps using a CNN decoder. These models are category-\nspecific and trained mostly on forward-facing scenes.\nMore recent works utilize the progress in contrastive rep-\nresentation learning [14, 51, 72, 33, 32], which enables easy\nand flexible control over the content of the generated objects\nusing textual input. In Dream Fields [27], frozen image-\ntext joint embedding models from CLIP [51] are used as a\nguidance to a NeRF model that generates 3D objects whose\nrenderings have high semantic similarity with the input cap-\ntion. To improve the visual quality, they introduce geomet-\nric priors and augmentations to enforce transmittance spar-\nsity, object boundaries and multi-view consistency. In this\npaper, we utilize some of the priors from Dream Fields [27]\nand introduce improved augmentations and priors to edit ex-\nisting NeRF scenes.\nMore recent works utilize the progress in diffusion mod-\nels [25, 61, 62] and specifically in text-conditioned diffusion\nmodels [54, 55, 56]. DreamFusion [50] and its follow-ups\n[68, 40, 34, 53] optimize a NeRF model by replacing CLIP\nwith score function losses using pretrained text-conditioned\n2D diffusion-models applied on many different views of the\ngenerated scene to synthesize 3D objects aligned with the\ninput text. These models synthesize new objects without\nconsidering how they can be inserted and blend into an ex-\nisting scene.\nEditing NeRFs. The pioneering works [36, 67] were the\nfirst to tackle the challenge of editing NeRF scenes. They\nboth define a conditional NeRF, where the NeRF model is\nconditioned on latent shape and appearance codes, which\nenables separately editing the shape and the color of a 3D\nobject. EditNeRF [36] only enables addition and removal\nof local parts or color changes guided by user scribbles and\nis limited to only one shape category. In ObjectNeRF [70]\nthey enable editing tasks such as moving or adding new ob-\njects to the scene by introducing a neural scene rendering\nsystem with a scene branch which encodes the scene ge-\nometry and appearance and object branch which encodes\neach standalone object. CLIP-NeRF [67] leverage the joint\nlanguage-image embedding space of CLIP [51] to perform\ntext or image guided manipulation on the entire scene. Dur-\ning the optimization it uses two code mappers for the shape\nand appearance that receive the CLIP embedding and out-\nput shape and appearance codes which steer the input of\nthe model and the model weights to apply the edit. The\nmanipulation capabilities are demonstrated mainly on syn-\nthetic objects from one class and on global color changes\nfor realistic scenes.\nLater works focused on geometric edits [71], global style\ntransfer [12, 13, 17, 26], recoloring [69, 19], and disen-\ntanglement of the scene to enable local edits [29, 8, 74].\nKobayashi [29] decomposes the scene to its semantic parts\nby training the NeRF model to learn a 3D feature field us-\ning supervision of pre-trained 2D image feature extractors\n[10, 31] in addition to learning of the volume density and the\nradiance field. After training, the model can perform zero-\nshot segmentation for local editing of a specific semantic\nclass. Benaim et al. [8] disentangle the volumetric repre-\nsentation of a foreground object from its background using\na set of 2D masks specifying the foreground object in each\ntraining view. They train two models for the full scene and\nthe background scene, and subtract the background from the\nfull scene in order to get the foreground. In both works the\nlocalization on the region of interest is incomplete and not\nflexible enough (does not enable editing parts of objects,\nempty regions or blending new densities into the area of\nexisting object). They demonstrate manipulations such as\nobject removal, transformations such as shift rotation and\nscale, and only basic optimization-based edits. Our work\nfocuses on blending text generated objects with volume and\ncolor into any region of interest of an existing scene with\nmore freedom and flexibility and without compromising on\nquality and visibility. For information regrading concurrent\nworks, please refer to the supplement.\n3. Method\nGiven an existing 3D scene xo represented by a NeRF\nmodel F O\n\u03b8 , and a 3D region of interest (ROI), indicated by\na box B localized inside the scene, our goal is to modify\nthe scene inside the ROI, according to a user-provided text\nprompt. In other words, we aim to obtain a modified scene\nxe, where xe \u2299 B is consistent with the user prompt from\n\u03b1 = 0\n\u03b1 = 0.5\n\u03b1 = 2\n\u03b1 = 4\n\u03b1 = 10\nFigure 3: Distance Smoothing Operator. We demonstrate our suggested smoothing operator in eq. (5) on a range of \u03b1\nvalues, When \u03b1 is zero all the weight goes to the edited scene, and as we increase \u03b1, more attention is given to closer points\nfrom the original scene.\nany point of view, while matching xo outside the box (xe \u2299\n(1 \u2212 B) = xo \u2299 (1 \u2212 B)).\nTo preform the edits inside the ROI we initialize a 3D\nMLP model F G\n\u03b8\nwith the weights of the original scene\nmodel F O\n\u03b8 and steer the weights towards the given prompt\nusing a pretrained language-image model, such as CLIP\n[51].\nWe enable local edits in any region of the scene\nxo using a simple GUI for localizing a 3D box inside the\nscene by rendering the original NeRF model F O\n\u03b8 from any\nview and using the output depth map of the model to ob-\ntain 3D understanding of the scene. Using the given ROI\nbox we can disentangle the scene inside the box and outside\nit by decomposing the radiance fields accordingly. To ob-\ntain consistent results from any view direction, we perform\nvolumetric blending of the original and the edited radiance\nfields by sampling 3D points along each camera ray r in\nboth F O\n\u03b8 and F G\n\u03b8 , and blending the samples while account-\ning for their densities, colors and distance from the center\nof the scene.\nTo get more realistic and natural-looking results we\npresent existing [27] and novel augmentations and priors\nsuch as transmittance and depth regularization, background\naugmentations, pose sampling and directional dependent\nprompts. An overview of our approach is depicted in Fig-\nure 1.\nIn Section 3.1 we describe our 3D object generation and\nblending process, we continue and present the model objec-\ntives and proposed priors in Section 3.2.\n3.1. Image-Text driven 3D synthesis and blending\nGiven a 3D scene represented by a NeRF model F O\n\u03b8 , an\nROI box B, and a camera pose, we use a duplicate of F O\n\u03b8 ,\nF G\n\u03b8 as our starting point for generating the content of B.\nThe rest of the scene is preserved by rendering only the rays\nwhich have sample points inside B. The training of F G\n\u03b8 is\nguided by a language-image model, e.g., [51, 33, 32, 72] to\nalign the content generated inside B with a user-provided\ntext prompt.\nTo get a smoothly blended result, we query both models\nF O\n\u03b8 , F G\n\u03b8 using the same set of rays. For sample points out-\nside the ROI, we use the density and color inferred by F O\n\u03b8 ,\nwhile for points inside the ROI, we blend the results of the\ntwo radiance fields using one of two modes, depending on\nthe type of the edit: adding a new object in empty space, or\ncompletely replacing an existing one, vs. adding an object\nin a non-empty area.\nF G\n\u03b8 is optimized using guidance from a language-image\nmodel, such as CLIP [51], by aiming to minimize the cosine\nsimilarity score between the user-provided text prompt y\nand rendered views of the generated content inside the ROI\nbox, IROI:\nLsim = \u2212Eimg(IROI)T Etxt(y),\n(3)\nwhere Eimg, Etxt are the image and text encoders of the\nimage-language model.\nDuring optimization, we render\nIROI using only the 3D sample points contained inside B\nby sampling only along rays r that pass through the box and\nsetting the density to zero for all sample points outside B,\naccording to eq. (1):\nC(r) =\n(P\nxi\u2208B Ti(1 \u2212 e\u2212\u03c3i\u03b4i)ci, \u2203xi \u2208 r s.t. xi \u2208 B\n0\n, otherwise\n(4)\nAfter training, we blend the scenes inside and outside\nthe ROI with the same set of rays by querying both F O\n\u03b8 and\nF G\n\u03b8\nwhere the points inside the box are rendered by F G\n\u03b8\nand the points outside the box are rendered by F O\n\u03b8 . To get\nsmooth blending between the two scenes we suggest dis-\ntance smoothing operator per point inside the box consid-\nering its distance from the center of the ROI scene (center\nof mass, computed during training) and alpha compositing\nthe density and color of the two scenes inside the ROI as\nfollows:\nf(x) = 1 \u2212 exp(\u2212\u03b1d(x)\ndiag\n)\n(5)\n\u03c3blend(x) = f(x) \u00b7 \u03c3O(x) + (1 \u2212 f(x)) \u00b7 \u03c3G(x)\ncblend(x) = f(x) \u00b7 cO(x) + (1 \u2212 f(x)) \u00b7 cG(x)\nwhere \u03c3O and \u03c3G are the densities returned by each model,\nd(x) is the Euclidean distance of a point x inside the ROI\nfrom the center of the scene, diag is the box diagonal and\n\u03b1 is a hyperparameter which controls the strength of the\nblending, as can be seen intuitively in Figure 3. The re-\nsulted raw densities and RGB values inside and outside the\nROI are then blended along each ray using eq. (1) to get the\ncurrent rendered view of the edited scene xe.\nObject Insertion/Replacement. In this mode, a new\nsynthetic object is added into an empty region of the scene,\nor entirely replaces another existing object inside the ROI.\nIn this mode, we use the pipeline described above, when\ninside the ROI we consider only the radiance field of F G\n\u03b8\nduring training. After training, we blend the two scenes as\ndescribed above.\nObject Blending. In contrast to the above mode, here\nwe aim to blend the new content with the existing scene in-\nside the ROI. We query both the original F O\n\u03b8 and the edited\nF G\n\u03b8 fields inside the box and blend the resulting colors and\ndensities at each ray sample. To blend the sample colors,\nwe first compute the alpha values for each point xi on the\nray separately from each model:\n\u03b1O(xi) = 1 \u2212 exp(\u03d5(\u03c3O(xi)) \u00b7 \u03b4i)\n\u03b1G(xi) = 1 \u2212 exp(\u03d5(\u03c3G(xi)) \u00b7 \u03b4i)\n(6)\nwhere \u03d5 is the activation function enforcing that these den-\nsity values are non-negative. To blend the colors cO and\ncG obtained from the two models, we use the above alpha\nvalues, followed by a sigmoid function:\nc(xi) = S(cO(xi) \u00b7 \u03b1O(xi) + cG(xi) \u00b7 \u03b1G(xi)\n\u03f5 + \u03b1O(xi) + \u03b1G(xi)\n)\n(7)\nwhere \u03f5 is a small constant, for numerical stability and S is\nthe sigmoid function.\nFor the density of the blended sample, we consider two\noptions, which have different impact on the results of the\nblending:\n\u03c3(xi) = \u03d5(\u03c3O(xi) + \u03c3G(xi))\n(8)\n\u03c3(xi) = \u03d5(\u03c3O(xi)) + \u03d5(\u03c3G(xi))\n(9)\ni.e., summing the densities inside or outside the activation\nfunction. When using eq. (8) we are summing inside the ac-\ntivation function thus allowing the generator F G\n\u03b8 to change\nthe original scene density and even remove densities (if\n\u03c3G(xi) < 0), while in eq. (9) we allow F G\n\u03b8 to only add new\ndensities to the scene. We can choose either of these two\noptions depending on the edit we wish to apply. We then\ncompute the joint transmittance and alpha values according\nto eq. (1). The resulting blended image IROI is then used\nto guide F G\n\u03b8 during training by measuring its similarity to\nthe input caption using eq. (3). The blending process af-\nter training is the same as in Object Insertion/Replacement\nmode. An illustration of our blending modes on the blender\nLego scene is presented in Figure 4.\n3.2. Objectives and Priors\nPrevious works [27, 8, 67] and our experiments indi-\ncate that a scene representation depending on similarity loss\nalone (eq. (3)) is too unconstrained, resulting in a scene that\nis not visually compatible to a human, but still satisfies the\nloss. Thus, we utilize the priors and augmentations men-\ntioned in DreamFields [27] and suggest additional priors to\nget more realistic results.\nPose Sampling. CLIP-NeRF [67] shows the multi-view\nconsistency evaluation of CLIP [51]. When using differ-\nent camera poses and rendering different views of the same\nobject, they still have high similarity, in contrast to dif-\nferent objects which have low similarity even in identical\nview. DreamFields [27] shows that sampling different cam-\nera poses is a good regularizer and improves the realism of\nthe object geometry. Thus, each iteration we sample a ran-\ndom camera pose around the scene depending on the scene\ntype (360\u25e6 and forward-facing scenes) including its azimuth\nand elevation angles (\u03b8, \u03d5). We found it beneficial to be rel-\natively close to the object during training to get a bigger\nobject in the rendered view, which in turn yields larger gra-\ndients from eq. (3). We set the initial distance d from the\nROI according to the camera AFOV = 2\u03b3 and the max-\nimum dimension of the box emax and we randomly sample\nthe radius r around this value:\nd =\nemax\n2 tan(\u03b3/2)\n(10)\nBackground Augmentation.\nDreamFields [27] note\nthat when using white or black background during opti-\noriginal scene\nsum in activation\nsum out activation\nFigure 4: Blending Modes. Guided by \u201cplant with green\nleaves and white and blue flowers\u201d. When using eq. (8)\n(second column), we allow F G\n\u03b8\nto change the density of\nthe original scene, in this case removing parts of the wheel.\nWhen utilizing eq. (9) (third column), we can only add ad-\nditionally density to the scene, so the plant warps around\nthe wheel without changing it.\nmization, the scene populates the background, and eventu-\nally we get a diffused scene. Thus, we use the same random\nbackgrounds as in DreamFields: Gaussian noise, checker-\nboard patterns and random Fourier textures from [44] to get\nmore sharp and coherent objects.\nDirectional Dependent Prompts. Due to the fact that\nthere\u2019s no constraint on F G\n\u03b8\nto describe the object differ-\nently in different views, we concatenate to the original cap-\ntion a text prompt depending on the current view. For more\ndetails, please refer to the supplementary materials.\nTransmittance loss. Same as in DreamFields [27], in\norder to get more sparse and coherent results we encourage\nthe generator to increase the average transmittance of the\nscene inside the box by adding a transmittance loss to the\ngenerator objective:\nLT = \u2212 min(\u03c4, mean(T(P )))\n(11)\nWhere mean(T(P )) is the average transmittance of a ren-\ndered view from pose P and \u03c4 is the max transmittance.\nDepth loss. When blending in forward-facing scenes\n(such as LLFF dataset [43]) and due to the limited view-\ning intervals, for some captions we get a flat billboard ge-\nometry effect and the resulting edit does not seem to have\na volume. We encourage the generator to synthesize vol-\numetric 3D shapes by adding a depth loss to the generator\nobjective:\nLD = \u2212 min(\u03c1, \u03c32(D(P )))\n(12)\nWhere \u03c32(D(P ))) is the variance of the disparity map of\na rendered view from pose P and \u03c1 is the max variance\nwe allow during training. We gradually introduce LT and\nLD during training using annealing strategy to prevent com-\npletely transparent or amorphous scenes. In summary, the\nfinal objective for the generator F G\n\u03b8 is:\nLtotal = Lsim + \u03bbT LT + \u03bbDLD\n(13)\nWhere \u03bbT , \u03bbD are the weights for LT , LD accordingly. For\nmore information on implementation details and hyperpa-\nrameters, please refer to the supplement.\n4. Experiments\nIn Section 4.1 we begin by comparing our method both\nqualitatively and quantitatively to the baseline Volumetric\nDisentanglement for 3D Scene Manipulation [8].\nNext,\nin Section 4.2 we demonstrate the effect of our suggested\npriors and augmentations on improving fidelity and visual\nquality. Finally, in Section 4.3 we demonstrate several ap-\nplications enabled by our framework.\n4.1. Comparisons\nOur qualitative comparisons to Volumetric Disentangle-\nment [8] are shown in Figure 5. Since the implementation of\n(a) \u201caspen tree\u201d\n(b) \u201cstrawberry\u201d\nFigure 5: Comparison to [8] for object replacement. We\ncompare our editing capabilities to [8] in the fern scene\nfrom llff dataset [43]. The left and right images in each\nrow are [8] and ours, accordingly. Our proposed method ex-\nhibits more realistic results that agrees better with the text.\nFor example the edit for the text \u201caspen tree\u201d indeed looks\nlike a trunk of an aspen tree in our edit.\nMethod\nCLIP\nDirection\nSimilarity\u2191\nCLIP\nDirection\nConsistency\u2191\nLPIPS\u2193\n[Benaim 2022]\n0.128\n0.736\n0.3\nOurs\n0.143\n0.787\n0.024\nTable 1: Quantitative Evaluation. Quantitative compari-\nson to [8] using the metrics described in Section 4.1. Our\nmethod demonstrates edits that are better align to the input\ncaptions and consistent between views, while preserving the\nbackground of the scene.\n[8] is not currently available, we preform the comparisons\nusing the examples from their project page2. As can be seen\nfrom the results in Figure 5, our results exhibit richer and\nmore natural colors and are aligned better with the text. To\ntest these observations quantitatively, in Table 1 we com-\npare our proposed method to [8] using three metrics:\n(1) CLIP Direction Similarity, a metric originally intro-\nduced in StyleGAN-NADA [18], measures how well the\nchange between the original and edited views is aligned\nwith the change in the texts describing them (in the CLIP\nembedding space).\n2https://sagiebenaim.github.io/volumetric-disentanglement/\n(a) Without Depth Loss\n(b) With Depth Loss\nFigure 6: Depth Loss Impact. Comparison of synthesiz-\ning a \u201cdonut covered with glaze and sprinkles\u201d from COCO\ndataset [35] on a limited view scene with and without our\nsuggested depth prior. The first column display a view of\nthe edited scenes and the second column displays the dis-\nparity map of the synthesized objects. In (a) the results are\nmore flat, which can be clearly seen in the disparity map.\n(2) CLIP Direction Consistency, introduced by Haque\n[23], measures the cosine similarity of the CLIP embed-\ndings of a pair of adjacent frames. For each edit, we take 6\nconsecutive frames, compute the metric for each consecu-\ntive pair, and average the results among all pairs.\nFinally, we use (3) LPIPS [73] to measure the differ-\nence between the original and edited scenes, with the ROI\nmasked, for comparing the background preservation. As\ncan be seen from Table 1, our model outperforms the base-\nline in all metrics, which implies that our generated objects\nmatch better to the input text captions, they are more con-\nsistent from any view and, on the other hand, our method\nmanages to keep the rest of the scene untouched.\n4.2. Ablation Study\nTo show the importance of our proposed augmentations\nand priors, we use the R-Precision score [48] using both\nCLIP and BLIP [51, 33, 32] as the metric language-image\nmodel to measure how well the generated images align with\nthe true caption. Similar to DreamFields [27], we use a ran-\ndomly selected subset of 20 samples (due to time and re-\nsources limitations) from the object-centric dataset which\ncontains 153 images and captions from COCO dataset [35]\nas our ground truth. The objects are synthesized using the\ngiven captions and blended into an empty region in the llff\nfern scene. Due to the fact we are training on the same CLIP\nMethod\nCLIP\nBLIP\nR-Precision \u2191\nR-Precision \u2191\nCOCO GT\n0.933\n0.98\nOurs(full pipeline)\n0.86\n0.8\nOurs(no dir prompts)\n0.85\n0.8\nOurs(no depth prior)\n0.81\n0.78\nTable 2: Ablation study. We test our proposed priors and\naugmentations on a subset of captions and images from\nCOCO dataset [35]. The CLIP and BLIP R-Precision scores\nutilize CLIP B-32 and BLIP2 architecture accordingly. The\nfirst row shows the scores of the GT COCO image, the sec-\nond row shows our method scores using all the priors and\naugmentations as described in Section 3 and the last two\nrows present the scores when taking out the directional de-\npendent prompts and the depth loss.\nmodel, we test our results with a different language-image\nmodel, BLIP2 [32]. The results of both metrics are pre-\nsented in Table 2. The directional dependent prompts seem\nto only slightly improve the results, probably due to the\nforward-facing nature of the scene. When rendering from\nlimited camera positions and viewing angles and without\nour proposed depth priors, the results deteriorate. To test\nthis conclusion visually, in Figure 6 we compare the task\nof inserting a new object into an empty region of the fern\nllff scene [43] with and without the depth loss. As can be\nseen from the figure, when using our proposed depth prior,\nthe generated object has more volume and looks more natu-\nral and consistent. For additional details, please refer to the\nsupplement.\n4.3. Applications\nIn this section, we demonstrate the applicability of our\nframework for several 3D editing scenarios.\nNew Object Insertion. Using the method described in\nSection 3, and by placing the ROI box in an empty space\nof the scene, we can synthesize a new object given a text\nprompt and blend it into the original scene. Visual exam-\nple of this application can be seen in Figure 6 and in the\nsupplement.\nObject Replacement. To replace an existing object in\nthe scene with new synthesized content, we place the ROI\n3D box in the required area (enclosing the object to be re-\nplaced), and perform the training process described in Sec-\ntion 3. In Figure 2 we demonstrate the replacement of the\nsea in the blender ship scene, while in Figure 5 we replace\nthe fern\u2019s trunk.\nBlending of Objects. To preform blending between the\noriginal and the generated object inside the ROI, we utilize\nthe object blending process described in Section 3. In Fig-\nure 4 and Figure 8 we demonstrate this blending on blender\nlego and llff fern scenes.\nOriginal Scene\n\u201cburning pinecone\u201d\n\u201ciced pinecone\u201d\n\u201cpinecone made of pink wool\u201d\nOriginal Scene\n\u201cvase made of glass\u201d\n\u201cvase made of stone\u201d\n\u201cwater paint of a vase\u201d\nFigure 7: Texture Editing. We can change only the texture of an object by freezing the layers responsible for the density\nand training only the layers that impact the color of the scene. To get a smooth blending, we utilize eq. (5) to blend the scene\ninside and outside the ROI.\nTexture Editing. We enable texture editing by training\nonly the color-related layers of F G\n\u03b8 and freezing all the other\nlayers in a similar way as in [67]. For seamless blending\nresults, we utilize eq. (5). In Figure 7 we demonstrate this\nedit method on 360 scenes. For additional results and videos\nplease refer to supplement.\n\u201da green and yellow bananas\u201d.\n\u201da clusters mushrooms\u201d.\nFigure 8:\nBlending Densities Inside Activation.\nWe\ndemonstrate our suggested blending procedure for blend-\ning the original and synthesized objects inside the ROI in\nllff fern scene [43] using eq. (8) for summing the densities.\n5. Limitations and Conclusions\nWe introduced a novel solution to blend new objects into\nan existing NeRF scene with natural looking and consistent\nresults by utilizing a language-image model to steer the gen-\neration process towards the edit and by introducing novel\npriors, augmentations and volumetric blending techniques\nfor improving the final edited scene. We tested our method\non a variety of scenes and text prompts and showed the ap-\nplicability of our framework on several editing applications.\nWe believe that our framework can be utilized in a variety\nof applications due to the ease and intuitive interaction en-\nabled by our interface.\nOne of the limitations of our framework is that currently\nit can\u2019t edit multiple objects in a given scene, such as chang-\ning two wheels of a 3D car without impacting the rest of the\nscene. Additionally, the use of a box as our ROI scene shape\ncan be sometimes limiting; for example, when trying to edit\na circular scene like the blender ship scene in Figure 2, a\ncylinder could be preferable. Due to the fact we are render-\ning one view in each training step, we may get artifacts like\nmultiple heads on the generated object. The quality of our\ngenerated objects can be improved by utilizing the recent\nprogress in diffusion models, we leave it as a future work\nto combine our suggested blending framework with these\nmodels.\nAcknowledgements:\nThis work was supported in part by\nthe Israel Science Foundation (grants No. 2492/20, and\n3611/21).\nReferences\n[1] Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou.\nWasserstein generative adversarial networks. In Proceedings\nof the 34th International Conference on Machine Learning,\nvolume 70, 2017. 3\n[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ArXiv, abs/2206.02779, 2022. 2\n[3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18208\u201318218, 2022. 2\n[4] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan,\nZesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng\nCui. Sine: Semantic-driven image-based nerf editing with\nprior-guided editing field. arXiv preprint arXiv:2303.13277,\n2023. 13\n[5] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5855\u20135864,\n2021. 1, 2\n[6] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 1, 2\n[7] David Bau, Alex Andonian, Audrey Cui, YeonHwan Park,\nAli Jahanian, Aude Oliva, and Antonio Torralba. Paint by\nword. arXiv preprint arXiv:2103.10951, 2021. 2\n[8] Sagie Benaim, Frederik Warburg, Peter Ebert Christensen,\nand Serge Belongie.\nVolumetric disentanglement for 3d\nscene manipulation. ArXiv, abs/2206.02776, 2022. 2, 3, 5,\n6, 13, 14\n[9] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\narXiv preprint arXiv:2211.09800, 2022. 2, 13\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 3\n[11] Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. Pi-gan: Periodic implicit generative\nadversarial networks for 3d-aware image synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5799\u20135809, 2021. 3\n[12] Yaosen Chen, Qi Yuan, Zhiqiang Li, Yuegen Liu Wei\nWang Chaoping Xie, Xuming Wen, and Qien Yu.\nUpst-\nnerf: Universal photorealistic style transfer of neural radi-\nance fields for 3d scene. arXiv preprint arXiv:2208.07059,\n2022. 3\n[13] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-\nSheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via im-\nplicit representation and hypernetwork. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 1475\u20131484, 2022. 3\n[14] Karan Desai and Justin Johnson.\nVirtex: Learning visual\nrepresentations from textual annotations. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 11162\u201311173, 2021. 3\n[15] Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava,\nGraham W. Taylor, and Joshua M. Susskind. Unconstrained\nscene generation with locally conditioned radiance fields. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 14304\u201314313, 2021. 3\n[16] Emilien Dupont, Adam Goli\u00b4nski, Milad Alizadeh, Yee Whye\nTeh, and Arnaud Doucet. Coin: Compression with implicit\nneural representations.\narXiv preprint arXiv:2103.03123,\n2021. 1, 2\n[17] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia\nXu, and Zhangyang Wang. Unified implicit neural styliza-\ntion. In Computer Vision\u2013ECCV 2022: 17th European Con-\nference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XV, pages 636\u2013654, 2022. 3\n[18] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik,\nand Daniel Cohen-Or.\nStylegan-nada:\nClip-guided do-\nmain adaptation of image generators.\narXiv preprint\narXiv:2108.00946, 2021. 6, 12\n[19] Bingchen Gong, Yuehao Wang, Xiaoguang Han, and Qi\nDou.\nRecolornerf:\nLayer decomposed radiance fields\nfor efficient color editing of 3d scenes.\narXiv preprint\narXiv:2301.07958, 2023. 3\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems, volume 27, 2014. 3\n[21] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.\nStyleneRF: A style-based 3d aware generator for high-\nresolution image synthesis. In Advances in Neural Informa-\ntion Processing Systems, 2022. 3\n[22] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent\nDumoulin, and Aaron C Courville.\nImproved training of\nwasserstein gans. In Advances in Neural Information Pro-\ncessing Systems, volume 30, 2017. 3\n[23] Ayaan Haque, Matthew Tancik, Alexei A Efros, Alek-\nsander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf:\nEditing 3d scenes with instructions.\narXiv preprint\narXiv:2303.12789, 2023. 7, 13\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in\nNeural Information Processing Systems, volume 33, pages\n6840\u20136851, 2020. 3\n[26] Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, and Lin\nGao. Stylizednerf: consistent 3d scene stylization as styl-\nized nerf via 2d-3d mutual learning.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18342\u201318352, 2022. 3\n[27] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 2, 3, 4, 5, 6, 7, 13\n[28] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei\nHuang, Matthias Nie\u00dfner, Thomas Funkhouser, et al. Local\nimplicit grid representations for 3d scenes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6001\u20136010, 2020. 1, 2\n[29] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-\nmann. Decomposing nerf for editing via feature field dis-\ntillation. In NeurIPS, 2022. 2, 3\n[30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Communications of the ACM, 60(6):84\u201390, 2017. 13\n[31] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\nKoltun, and Ren\u00b4e Ranftl.\nLanguage-driven semantic seg-\nmentation. arXiv preprint arXiv:2201.03546, 2022. 3\n[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models, 2023. 3,\n4, 7, 13\n[33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 3, 4, 7\n[34] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 3\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick.\nMicrosoft COCO: Common objects in context.\nIn Proceedings ECCV 2014, Part V 13, pages 740\u2013755.\nSpringer, 2014. 7, 13, 14\n[36] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard\nZhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional\nradiance fields. In Proceedings of the International Confer-\nence on Computer Vision (ICCV), 2021. 2, 3\n[37] Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne\nWu, and Bolei Zhou. Semantic-aware implicit neural audio-\ndriven video portrait generation. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, Octo-\nber 23\u201327, 2022, Proceedings, Part XXXVII, pages 106\u2013125.\nSpringer, 2022. 1\n[38] Nelson Max. Optical models for direct volume rendering.\nIEEE Transactions on Visualization and Computer Graphics,\n1(2):99\u2013108, 1995. 3\n[39] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4460\u20134470, 2019. 1, 2\n[40] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. arXiv preprint arXiv:2211.07600,\n2022. 3\n[41] Aryan Mikaeili, Or Perel, Daniel Cohen-Or, and Ali\nMahdavi-Amiri. Sked: Sketch-guided text-based 3d editing.\narXiv preprint arXiv:2303.10735, 2023. 13\n[42] Ben Mildenhall,\nPratul P.Srinivasan,\nMatthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2020. 1, 2, 12\n[43] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 2019. 6, 7, 8, 12, 13, 14\n[44] Alexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert,\nand Chris Olah.\nDifferentiable image parameterizations.\nDistill, 3(7):e12, 2018. 6\n[45] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. In Interna-\ntional Conference on Machine Learning, 2021. 2\n[46] Michael Niemeyer and Andreas Geiger. Campari: Camera-\naware decomposed generative neural radiance fields.\nIn\nProc. of the International Conf. on 3D Vision (3DV), 2021. 3\n[47] Michael Niemeyer and Andreas Geiger.\nGiraffe: Repre-\nsenting scenes as compositional generative neural feature\nfields. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 11453\u2013\n11464, 2021. 3\n[48] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell,\nand Anna Rohrbach. Benchmark for compositional text-to-\nimage synthesis. In Thirty-fifth Conference on Neural Infor-\nmation Processing Systems Datasets and Benchmarks Track\n(Round 1), 2021. 7, 13\n[49] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 165\u2013174, 2019. 1, 2\n[50] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In The\nEleventh International Conference on Learning Representa-\ntions, 2023. 3, 13\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\n2021. 1, 2, 3, 4, 5, 7, 12, 13\n[52] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix\nDraxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, and\nAaron Courville. On the spectral bias of neural networks.\nIn Proceedings of the 36th International Conference on Ma-\nchine Learning, 2019. 3\n[53] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nBen Mildenhall, Nataniel Ruiz, Shiran Zada, Kfir Aberman,\nMichael Rubenstein, Jonathan Barron, Yuanzhen Li, and\nVarun Jampani. DreamBooth3D: subject-driven text-to-3d\ngeneration. In arXiv preprint 2303.13508, 2023. 3\n[54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. ArXiv, abs/2204.06125, 2022. 2, 3\n[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 3\n[56] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Advances in Neural Infor-\nmation Processing Systems, 2022. 3\n[57] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance fields for 3d-aware image\nsynthesis.\nIn Advances in Neural Information Processing\nSystems, volume 33, pages 20154\u201320166, 2020. 3\n[58] Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah\nSnavely, and Gordon Wetzstein.\nMetasdf: Meta-learning\nsigned distance functions. In arXiv, 2020. 1, 2\n[59] Vincent Sitzmann,\nJulien N.P. Martel,\nAlexander W.\nBergman, David B. Lindell, and Gordon Wetzstein. Implicit\nneural representations with periodic activation functions. In\nProc. NeurIPS, 2020. 1, 2\n[60] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks:\nContinuous 3d-\nstructure-aware neural scene representations. In Advances\nin Neural Information Processing Systems, 2019. 1, 2\n[61] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2021. 3\n[62] Yang Song and Stefano Ermon.\nImproved techniques for\ntraining score-based generative models.\nIn Advances in\nNeural Information Processing Systems, volume 33, pages\n12438\u201312448, 2020. 3\n[63] Yannick Str\u00a8umpler, Janis Postels, Ren Yang, Luc Van Gool,\nand Federico Tombari. Implicit neural representations for\nimage compression. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXVI, pages 74\u201391. Springer, 2022. 1, 2\n[64] Kun Su, Mingfei Chen, and Eli Shlizerman. Inras: Implicit\nneural representation for audio scenes. In S. Koyejo, S. Mo-\nhamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, ed-\nitors, Advances in Neural Information Processing Systems,\nvolume 35, pages 8144\u20138158. Curran Associates, Inc., 2022.\n1\n[65] Filip Szatkowski, Karol J Piczak, Przemys\u0142aw Spurek, Jacek\nTabor, and Tomasz Trzci\u00b4nski. Hypersound: Generating im-\nplicit neural representations of audio signals with hypernet-\nworks. arXiv preprint arXiv:2211.01839, 2022. 1\n[66] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. In Advances in Neural Information Process-\ning Systems, volume 33, 2020. 3\n[67] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manipula-\ntion of neural radiance fields. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2022, New\nOrleans, LA, USA, June 18-24, 2022, 2022. 2, 3, 5, 8\n[68] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation.\nCoRR,\nabs/2212.00774, 2022. 3\n[69] Qiling Wu, Jianchao Tan, and Kun Xu.\nPalettenerf:\nPalette-based color editing for nerfs.\narXiv preprint\narXiv:2212.12871, 2022. 3\n[70] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han\nZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.\nLearning object-compositional neural radiance field for ed-\nitable scene rendering. In International Conference on Com-\nputer Vision (ICCV), October 2021. 3\n[71] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,\nRongfei Jia, and Lin Gao. Nerf-editing: geometry editing of\nneural radiance fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18353\u201318364, 2022. 3\n[72] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,\nDaniel Keysers, Alexander Kolesnikov, and Lucas Beyer.\nLit: Zero-shot transfer with locked-image text tuning.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 18123\u201318133,\n2022. 3, 4\n[73] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 7, 13\n[74] Xiaoshuai Zhang, Abhijit Kundu, Thomas Funkhouser,\nLeonidas Guibas, Hao Su, and Kyle Genova. Nerflets: Local\nradiance fields for efficient structure-aware 3d scene repre-\nsentation from 2d supervision. CVPR, 2023. 3\n[75] Peng Zhou, Lingxi Xie, Bingbing Ni, and Qi Tian. Cips-\n3d: A 3d-aware generator of gans based on conditionally-\nindependent pixel synthesis, 2021. 3\nA. Implementation Details\nIn this section we provide additional implementation de-\ntails.\nA.1. ROI Specification Interface\nTo specify the ROI and use it to decompose the scene,\nwe introduce a graphic interface that enables positioning an\naxis-aligned 3D box inside the scene. Given the 3D position\nof the center, as well as the axis dimensions, of the box, ren-\ndering of the scene is performed from the provided camera\nposition using the original NeRF model F O\n\u03b8 . The edges of\nthe 3D box are then projected onto the image plane using\nthe camera matrix. To provide intuitive feedback regarding\nthe location of the box in the scene, we utilize the depth\nmap of the scene to remove parts of the box edges that are\noccluded by the scene. In this manner, the user is able to\nspecify the ROI in a precise and intuitive way by moving\nthe box and modifying its dimensions while being able to\ninspect the location from any point of view.\nA.2. Pose Sampling\nIn each training step, we sample a camera pose from\na range of distances and angles, depending on the scene\ntype. In the blender and 360 scenes, we sample azimuth\nand elevation angles in the ranges: \u03b8 \u2208 [\u2212180\u25e6, 180\u25e6],\n\u03d5 \u2208 [\u221290\u25e6, 15\u25e6]. For the radius, we first calculate the initial\ndistance according to eq. (10) and then randomly sample\nthe radius around this value. In llff dataset [43] we sample\nthe camera pose from a spiral curve as used in the original\nNeRF implementation 3 . The curve is randomly sampled\nfrom a range of distances and radii in each axis. After sam-\npling a camera pose, we recenter its rays around the ROI by\nmoving its center location according to the center of mass\ninside the ROI (tracked by exponential moving average dur-\ning training), but allow with a probability p \u2208 [0, 1] (hyper-\nparameter, set to 0.1 in our experiments) to recenter the rays\nto a different point inside the ROI, with the aim of obtain-\ning more versatile objects and densities. Additionally, we\nset the near and far planes (n, f) according to the box lo-\ncation and size in order to be more concentrated around the\nROI and get more sample points per ray in this area:\nn = d \u2212 D\n2 ,\nf = d + D,\n(14)\nwhere d is the distance of the camera from the center of\nmass inside the box and D is the box diagonal length.\nA.3. Hyperparameters\nIn our experiments we set the max transmittance of LT ,\nthe max variance of LD and the weights of the losses to:\n\u03c4 = 0.88, \u03c1 = 0.2, \u03bbT = 0.25, \u03bbD = 4. We use the same\n3https://github.com/bmild/nerf\nnetwork architecture as in [42] and the same hyperparam-\neters and learning rates. To guide our model, we use the\nCLIP B/32 architecture.\nA.4. Training\nWe train our model with a random seed value of 123 for\nall of our experiments. In experiments, we render the views\nat 168x168 resolution and up-sample to 224x224 resolution\nbefore feeding them to CLIP [51]. In the Comparisons and\na Ablation study sections, we train the generator for 40,000\niterations and for the other figures in the main paper, the\nviews resolution and the number of iterations depends on\nthe complexity of the synthesized object and hardware lim-\nitations. We train with 4 \u00d7 24 GB A5000 GPUs. Training\ntakes between a few hours to one day. We find that the\nprimary driver for runtime/hardware requirements are the\nview resolution and the size of ROI (which require render-\ning more points along each ray).\nA.5. Directional Dependent prompts\nAs described in the main paper, each iteration we con-\ncatenate a text prompt to the input caption depending on the\ncamera location in the scene. We use the direction prompts\nbelow depending on the location:\n\u2022 \u201d, top-down view\u201d\n\u2022 \u201d, front view\u201d\n\u2022 \u201d, side view\u201d\n\u2022 \u201d, back view\u201d\nIn forward-facing scenes like llff dataset [43] we use the\nfirst three captions.\nB. Additional Experiments Details\nIn this section we provide additional information regrad-\ning the experiments from the main paper.\nB.1. Metrics\nIn our quantitative evaluation we report four metrics:\nCLIP Direction Similarity, CLIP Direction Consistency,\nLPIPS and R-Precision.\nCLIP Direction Similarity introduced in [18] as a direc-\ntion loss which measures the similarity between the change\nin the text descriptions and the change in the images. We\nuse a variation of this metric so that high similarity will have\nhigh metric score:\n\u2206T = ET (Te) \u2212 ET (To)\n\u2206I = EI(Ie) \u2212 EI(Io)\nLdirection =\n\u2206T \u00b7 \u2206I\n|\u2206T ||\u2206I |\n(15)\nWhen ET , EI are the text and image encoders of CLIP,\nTe, To are the text captions describing the edited and\noriginal scene inside the ROI and Ie, Io are the according\nedited and original scenes views. In our experiments on the\nfern llff scene [43], we set To to: \u201da photo of a fern trunk\u201d.\nCLIP Direction Score introduced in [23] measures the\nconsistency between adjacent frames by calculating the\nCLIP embeddings of two corresponding pairs of consecu-\ntive views, one from the original scene and one from the\nedited scene. Similar to CLIP Direction Similarity metric,\nwe then compute the similarity between the change in the\noriginal and edited scene views to get the final consistency\nscore:\n\u2206Io = EI(Io\ni+1) \u2212 EI(Io\ni )\n\u2206Ie = EI(Ie\ni+1) \u2212 EI(Ie\ni )\nLdirection =\n\u2206Io \u00b7 \u2206Ie\n|\u2206Io ||\u2206Ie |\n(16)\nWhen Io\ni , Io\ni+1 and Ie\ni , Ie\ni+1 are the original and edited\nconsecutive views pairs. In our experiments we compute\nthis score on six consecutive views and average the results.\nLPIPS or Learned Perceptual Image Patch Similarity, is\nused to judge the perceptual similarity between two images,\n[73] shows that this metric match human perception. The\nmetric computes the similarity between the activation\u2019s\nof the two images for some network architecture.\nIn\nour experiments we use LPIPS with pre-trained alexnet\narchitecture [30] to measure the background similarity\nbetween the original and the edited scenes by masking the\nROI region.\nR-Precision [48] measures how well a rendered view of\nthe synthesis object align with the text caption used to gen-\nerate it. It computes the precision of the rendered views over\na group of text captions using a retrieval model. Similar\nto DreamFields [27] we collect an object-centric captions\ndataset from COCO dataset [35] and sample 20 captions\nthat will be used for training our model. We than compute\nthe precision of the rendered views per synthesis object over\nthe 153 captions. As the language image model backbone\nof the score, we use both CLIP [51] and BLIP2 [32], since\nwe use CLIP to train our model.\nC. Concurrent Work\nConcurrently with our work, Instruct-NeRF2NeRF [23]\npresent a diffusion-based method for editing a NeRF scene\nguided by text instructions. It utilizes InstructPix2Pix [9],\nwhich enables editing images based on text instructions.\nThe edit is preformed by iteratively updating the image\ndataset of the original scene while training NeRF using\nthese edited images. They demonstrate an impressive high\nquality local edit results on real scenes but sometimes can\u2019t\npreserve the rest of the scene and get a blurrier scene com-\npared to the original, and sometimes even introduce texture\nand color changes to the entire scene.\nSKED [41] research the possibility to edit a NeRF scene\nusing guidance from 2D sketches from different views ad-\nditional to an input text prompt describing the edit. They\nutilize the SDS loss presented in [50] to steer the edit to-\nwards the input caption and present preservation and sil-\nhouette priors to preserve the original scene and to preform\nthe edit only on the sketched regions. In experiments they\napply their method mainly on synthetic objects and demon-\nstrate its applicability on objects insertion and replacement\ntasks such as hats, flowers and glasses.\nIn SINE [4], they suggest a method for editing NeRF\nscene by only editing a single view, and than apply the edit\nto the entire scene. To do this they encode the changes in ge-\nometry and texture over the original NeRF scene, by learn-\ning a prior-guided editing field. Using this field they render\nthe modified object geometry and color and present color\ncompositing layer supervised by the single edited view to\napply the edit on novel views. They apply their method on\nreal and synthetic scenes by changing the geometry and tex-\nture of objects in the scene.\nD. Additional Examples\nWe provide additional examples for the applications in\nthe main paper. In Figure 9 we display additional views for\nthe object replacement comparison with Volumetric Disen-\ntanglement for 3D Scene Manipulation [8]. In Figure 10\nwe demonstrate new object insertion using several captions\nfrom COCO dataset [35]. In Figure 11 and Figure 12 we\nshow more examples for object replacement, and in Fig-\nure 13 and Figure 14 we display more edits and views for\ntexture conversion task on 360 scenes.\n(a) \u201daspen tree\u201d\n(b) \u201dstrawberry\u201d\nFigure 9: Additional views for object replacement comparison. Additional views for the object replacement comparison\nwith Volumetric Disentanglement [8]. The first and second rows display [8] and our results accordingly.\n\u201dbouguet of wilted\n\u201dbroccoli laying on\n\u201dred and blue fire hydrant.\u201d\n\u201dsnowboard standing in\n\u201dzebra eating grass\nred roses on a table.\u201d\non a plastic board.\u201d\na snow bank.\u201d\non the ground.\u201d\nFigure 10: Object Insertion. Insertion of new objects from COCO dataset [35] into an empty region in fern llff scene. Each\ncolumn shows two views of the same edited scene [43].\n(a) original scene.\n(b) edited scene.\nFigure 11: Object Insertion in vasedeck 360 scene. We used the text: \u201da photo of a purple, white and blue flowers petals\non the ground\u201d and eq. (5) with \u03b1 = 3.5 to generate the edit.\n(a) original scene.\n(b) \u201ca pineapple.\u201c\nFigure 12: Object replacement in 360 pinecone scene. We replace the original pinecone object with pineapple using our\nproposed object replacement method.\nOriginal Scene.\n\u201dBurning pinecone\u201d.\n\u201dIced pinecone\u201d.\n\u201dGolden pinecone\u201d.\n\u201dPinecone made of pink wool\u201d.\nFigure 13: Texture conversion on 360 pinecone scene.\n(a) original Scene.\n(b) \u201ca vase made of glass.\u201d\n(c) \u201ca vase made of stone.\u201d\n(d) \u201ca water paint of a vase with flowers.\u201d\nFigure 14: Texture conversion on 360 vasedeck scene.\n"
  },
  {
    "title": "Continuous Layout Editing of Single Images with Diffusion Models",
    "link": "https://arxiv.org/pdf/2306.13078.pdf",
    "upvote": "7",
    "text": "Continuous Layout Editing of Single Images with Diffusion Models\nZHIYUAN ZHANG\u2217, City University of Hong Kong, China\nZHITONG HUANG\u2217, City University of Hong Kong, China\nJING LIAO\u2020, City University of Hong Kong, China\nFig. 1. Our method can continuously edit the layout of a single image with multiple objects. The first column shows the input image, followed by three\ncolumns of edited results. The edited results preserve the visual properties of the input image while remaining faithful to the layout map provided by the user.\nRecent advancements in large-scale text-to-image diffusion models have\nenabled many applications in image editing. However, none of these methods\nhave been able to edit the layout of single existing images. To address this\ngap, we propose the first framework for layout editing of a single image\nwhile preserving its visual properties, thus allowing for continuous editing\non a single image. Our approach is achieved through two key modules.\nFirst, to preserve the characteristics of multiple objects within an image, we\ndisentangle the concepts of different objects and embed them into separate\ntextual tokens using a novel method called masked textual inversion. Next,\nwe propose a training-free optimization method to perform layout control\nfor a pre-trained diffusion model, which allows us to regenerate images\nwith learned concepts and align them with user-specified layouts. As the\nfirst framework to edit the layout of existing images, we demonstrate that\nour method is effective and outperforms other baselines that were modified\nto support this task. Our code will be freely available for public use upon\nacceptance at https://bestzzhang.github.io/continuous-layout-editing.\nCCS Concepts: \u2022 Computing methodologies \u2192 Image manipulation;\nGraphics systems and interfaces; Neural networks.\n1\nINTRODUCTION\nRecent advances in text-to-image generation have made significant\nprogress through the use of diffusion models trained on large-scale\ndatasets [Nichol et al. 2021; Rombach et al. 2022; Saharia et al. 2022].\nHowever, due to the inherent ambiguity of text and its limitations\nin expressing precise spatial relationships in the image space, con-\ntrolling the layout of generated images is still a challenge for these\n\u2217Both authors contributed equally to this research.\n\u2020Corresponding author.\nAuthors\u2019 addresses: Zhiyuan Zhang, zzhang452-c@my.cityu.edu.hk, City University\nof Hong Kong, Hong Kong SAR, China; Zhitong Huang, luckyhzt@gmail.com, City\nUniversity of Hong Kong, Hong Kong SAR, China; Jing Liao, jingliao@cityu.edu.hk,\nCity University of Hong Kong, Hong Kong SAR, China.\nlarge-scale text-to-image models. To address this issue, some lat-\nest methods have been proposed to enable layout control in image\ngeneration. These methods are typically based on pre-trained dif-\nfusion models, which either incorporate layout guidance as a new\ncondition through fine-tuning [Avrahami et al. 2023; Li et al. 2023;\nZhang and Agrawala 2023] or optimize the noise diffusion process\non-the-fly to achieve layout control [Bar-Tal et al. 2023].\nDespite the success of existing layout control methods [Avrahami\net al. 2023; Bar-Tal et al. 2023; Li et al. 2023; Zhang and Agrawala\n2023] in generating new images with controlled layouts, they are\nunable to rearrange and edit the layout of existing images. In prac-\ntice, users may want to continuously edit the positions of objects in\nan existing image without altering its visual properties. For example,\nas illustrated in the first example of Figure 1, a user may want to\nexperiment with different layout options to find the best arrange-\nment of a cat and a pot in an image. However, previous methods do\nnot support this functionality since their layout control does not\ntake into account the input image, and a new image with different\ncat and pot will be generated for each specified layout. To fill this\ngap, we propose the first framework for continuous layout editing\nof single input images.\nOne of the key challenges in continuous layout editing is preserv-\ning the visual properties of the input image, which requires learning\nconcepts for multiple objects within a single image and using the\nlearned concepts to regenerate new images under different layouts.\nWhile some pioneer textual inversion methods [Gal et al. 2022; Ruiz\net al. 2023] have proposed fine-tuning a text token embedding of\npre-trained text-to-image diffusion models to learn the concept of\nan object from multiple images containing the same object, they\nare limited in their ability to learn multiple objects within a single\nimage. To overcome this limitation, we propose a novel approach,\ncalled masked textual inversion, that disentangles the concepts\narXiv:2306.13078v1  [cs.CV]  22 Jun 2023\n2\n\u2022\nZhiyuan Zhang, Zhitong Huang, and Jing Liao\nof different objects within a single image and embeds them into\nseparate tokens. By adding masks to the regions of each object, our\nmethod ensures that the visual characteristics of each object are\neffectively learned by the corresponding token embedding.\nAfter learning the concepts of multiple objects within a single\nimage, the next challenge is to control the positions of these objects\nto align with the desired layout. P2P [Hertz et al. 2022] suggests that\nthe cross-attention of a pretrained text-to-image diffusion model\ncan represent the position of the generated object associated with\nthe corresponding text token and Attend-and-Excite [Chefer et al.\n2023] further utilizes the cross-attention to ensure the generation\nof objects. Inspired by these papers, we propose a novel, training-\nfree layout editing method that iteratively optimizes the cross-\nattention during the diffusion process. This optimization is guided\nby a region loss that prioritizes the alignment of the specified object\nwith its designated region in the layout by encouraging higher cross-\nattention between the object\u2019s text embedding and its corresponding\nregion than with any other region in the image. Our approach\nenables precise and flexible control over the positions of objects in\nthe image, without requiring additional training or fine-tuning of\nthe pre-trained diffusion model.\nExtensive experiments and perceptual studies have demonstrated\nthat our proposed method is effective in editing the layout of sin-\ngle images and outperforms other baseline methods (modified to\nperform this task). We also provide a user interface for interac-\ntive layout editing to assist in the design process. In summary, our\ncontributions to the field are as follows:\n\u2022 We propose the first framework which supports continuous\nlayout editing of single images.\n\u2022 We present a masked textual inversion method to learn dis-\nentangled concepts of multiple objects within single images.\n\u2022 We propose a training-free optimization method to perform\nlayout control with diffusion models.\n2\nRELATED WORKS\nIn this section, we will review works related to our method, which\nincludes diffusion models, image editing with diffusion models\nthrough textual inversion, and layout control with diffusion models.\n2.1\nDiffusion Models\nDiffusion models have become one of the most popular generative\nmodels due to their impressive quality in image generation. The\noriginal DDPM [Ho et al. 2020] simulates a Markovian process\nwhere Gaussian noise is added to clean images \ud835\udc650 to create the noisy\nimage \ud835\udc65\ud835\udc61 in the forward process. Then, a model is trained to predict\nand remove the noise in \ud835\udc65\ud835\udc61 to generate images. To accelerate the\ndenoising process, DDIM [Song et al. 2020] converts DDPM into a\nnon-Markovian process, which requires no additional training.\nRecently, text-to-image diffusion models [Balaji et al. 2022; Ramesh\net al. 2022; Saharia et al. 2022] trained on large-scale datasets have\ngained significant attention due to their ability to generate diverse\nhigh-quality images with text prompts. Among them, Stable Diffu-\nsion [Rombach et al. 2022] operates the diffusion process in latent\nspace instead of pixel space, allowing it to generate high-resolution\nimages.\n2.2\nImage Editing with Textual Inversion\nBy leveraging the power of pretrained text-to-image diffusion mod-\nels, many image editing methods have been derived. Among them, a\nmajor category is to learn the concepts of objects or styles into text\ntokens and then generate new images with the extracted concepts.\nPioneer works in this category include Textual inversion[Gal et al.\n2022] and DreamBooth[Ruiz et al. 2023]. Textual inversion [Gal\net al. 2022] embeds concepts of objects into pseudo-words, while\nDreamBooth [Ruiz et al. 2023] further finetunes the UNet to learn\nmore details. However, these two methods can only extract a single\ncommon concept from multiple images. Multi-Concept [Bar-Tal\net al. 2023] extends to learn multiple objects and explores the most\neffective layers in UNet to be finetuned; however, each concept\nstill needs to be learned from multiple images. Overall, a method\nthat can learn multiple concepts from a single image is still under\nexploration, and our masked textual inversion fills this gap.\n2.3\nLayout Control with Diffusion Models\nDue to the sparsity and ambiguity of text descriptions, it is difficult\nto precisely control the layout of generated images by pretrained\ntext-to-image diffusion models. To address this limitation, some\nlayout control methods [Nichol et al. 2021; Rombach et al. 2022;\nSaharia et al. 2022] based on diffusion models have been proposed,\nwhich can be divided into two categories.\nThe first category requires finetuning the pretrained text-to-\nimage model to incorporate layout guidance as an extra condition be-\nsides text. Spatext [Avrahami et al. 2023] proposes to convert the con-\ncept of each object into CLIP image features with unCLIP [Ramesh\net al. 2022], which are then stacked at the target positions of that\nobject to form a spatio-textual representation. This layout condition\nis concatenated with the noisy latent to control the layout during\nthe denoising process. ControlNet [Zhang and Agrawala 2023] in-\nserts additional conditions, such as the semantic maps for layout\ncontrol, by utilizing a trainable copy of the original UNet model. The\nconditional copy and the original model are fused in intermediate\nlayers to generate a conditioned output. GLIGEN [Li et al. 2023]\nadds additional trainable gated self-attention layers that take the\ninformation of layout conditions (i.e., bounding boxes) to control\nthe layout of the generated image. A common limitation of these\nmethods is that additional modules are added to the original UNet,\nand datasets of paired data (e.g., images and corresponding seman-\ntic maps) are required to finetune the diffusion model and added\nmodules. Also, their capabilities are restricted by the training data.\nThe second category explores training-free layout control with\non-the-flight optimization. The representative work is MultiDiffu-\nsion [Bar-Tal et al. 2023], which denoises different crops of each\nobject locally and then fuses the results globally for each denoising\nstep. Compared with computing multiple denoising directions for\neach object, which may cause artifacts and discontinuities at the\nboundary of objects, our training-free layout editing method di-\nrectly denoises the whole image and optimizes the image latent for\nlayout control, to avoid the gaps and discontinuities among multiple\ndenoising directions.\nContinuous Layout Editing of Single Images with Diffusion Models\n\u2022\n3\nFig. 2. Overall framework of our method.\nBoth categories of methods still focus on the layout control of\ngenerated images, which cannot be used to edit the layout of existing\nimages. Our proposed framework targets this gap.\n3\nMETHODOLOGY\nPreliminaries. Our method is implemented with Stable Diffu-\nsion [Rombach et al. 2022], a large-scale text-to-image model. There-\nfore, before discussing our method, we first introduce Latent Dif-\nfusion Models (LDMs) [Rombach et al. 2022], which is the theory\nof Stable Diffusion. LDMs consist of two key stages. In the first\nstage, the encoder of an autoencoder maps the image to latent space\n\ud835\udc670 = E(\ud835\udc3c), and a decoder maps it back to image D(E(\ud835\udc3c)) \u2248 \ud835\udc3c. In the\nsecond stage, a diffusion model \ud835\udf16\ud835\udf03 is trained to denoise the noised\nlatent \ud835\udc67\ud835\udc61 = \u221a\ud835\udefc\ud835\udc61\ud835\udc670+\u221a1 \u2212 \ud835\udefc\ud835\udc61\ud835\udf16, where \ud835\udefc\ud835\udc61 is a factor to determine noise\nlevel for each timestep \ud835\udc61, and \ud835\udf16 \u223c N (0, 1) is Gaussian noise. Then\nthe diffusion model is trained to predict the added Gaussian noise\nwith the LDM loss [Ho et al. 2020]:\nL\ud835\udc3f\ud835\udc37\ud835\udc40 := E\ud835\udc670\u2208E(\ud835\udc3c ),\ud835\udc66,\ud835\udf16\u223cN(0,1),\ud835\udc61 [\u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc50\ud835\udf03 (\ud835\udc66))\u22252\n2]\n(1)\nwhere \ud835\udc66 is the input text condition, and \ud835\udc50\ud835\udf03 is the text encoder.\nOverview. As shown in Fig. 2, our methods can be divided into\ntwo stages. In the first stage, we learn the concepts of multiple\nobjects from a single input image \ud835\udc3c into text tokens \ud835\udc631, \ud835\udc632, ..., \ud835\udc63\ud835\udc41\nwith masked textual inversion, where the regions of each object\nare specified by masks \ud835\udc401, \ud835\udc402, ..., \ud835\udc40\ud835\udc41 . We further learn the details\nof the objects by finetuning the diffusion model \ud835\udf16\ud835\udf03 and optimizing\nthe appended text tokens \ud835\udc63[1], ..., \ud835\udc63[\ud835\udc3f]. After the first stage, we get\nthe optimized text tokens for objects \ud835\udc66\u2217 = [\ud835\udc631\u2217, ..., \ud835\udc63\ud835\udc41\u2217] and the\nfinetuned model \ud835\udf16\ud835\udf03\u2217. Then, in the second stage, we rearrange the\npositions of the objects according to the user-specified layout map\n\ud835\udc3c\ud835\udc3f through a training-free layout editing method with optimization:\n\ud835\udc3c\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc61 = Layout-control(\ud835\udc3c,\ud835\udc50\ud835\udf03 (\ud835\udc66\u2217),\ud835\udf16\ud835\udf03\u2217, \ud835\udc3c\ud835\udc3f)\n(2)\n3.1\nLearn Concepts of Multiple Objects within Single\nImage\nTo rearrange the layout of an input image, we first need to extract\nthe concepts of multiple objects within the single input image to\nbest preserve their visual characteristics, such as shape, color, and\ntexture. We propose using masked textual inversion to learn and\nembed the concept of each individual object into a unique text token.\nWe then fine-tune the diffusion model to better grasp the detailed\ntexture of the learned objects.\n3.1.1\nMasked textual inversion. Original technique of text inversion\nonly supports learning the concept of a single object from a set of\nimages (typically 3-5). However, in our applications, we need to learn\nmultiple concepts from a single image. As observed in [Avrahami\net al. 2023], the latent vector\ud835\udc670 = E(\ud835\udc3c) encoded from the input image\nwith the autoencoder [Rombach et al. 2022] has local property in the\nspatial dimension and the encoder performs like a down-sampler.\nTherefore, we can disentangle the concepts of different objects by\nsimply applying a spatial mask. Instead of calculating the loss of the\nwhole latent, we only propagate the loss within the region of the\nobject to update the corresponding text token:\n\ud835\udc63\ud835\udc58\u2217 = arg min\n\ud835\udc63\ud835\udc58\nE\ud835\udc670\u2208E(\ud835\udc3c ),\ud835\udc66,\ud835\udf16\u223cN(0,1),\ud835\udc61\n[\ud835\udc40\ud835\udc58 \u2299 \u2225\ud835\udf16\u2212\ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc50\ud835\udf03 (\ud835\udc66))\u22252\n2]\n(3)\n4\n\u2022\nZhiyuan Zhang, Zhitong Huang, and Jing Liao\nwhere \ud835\udc40\ud835\udc58 is the mask of the \ud835\udc58-th object (\ud835\udc58 = 1, ..., \ud835\udc41 is the index of\nthe \ud835\udc41 objects), \ud835\udc63\ud835\udc58 is the corresponding text token of the \ud835\udc58-th object,\nand the input text condition \ud835\udc66 consists of text tokens [\ud835\udc631, ..., \ud835\udc63\ud835\udc41 ].\nThe mask can either be generated coarsely by hand or automatically\nusing CLIP Segmentation [L\u00fcddecke and Ecker 2022]. We repeat\nthis process independently for each of the \ud835\udc41 objects to optimize\nthe text tokens for each object. To avoid overfitting, we only run\neach optimization for 200 steps, which is much less than the original\ntextual inversion 3000-5000 steps).\n3.1.2\nModel fintuning. A single text token can only store limited\ninformation of an object, which may cause obvious distortion or\nartifacts during sampling. Therefore, we propose further fine-tuning\nthe denoising network \ud835\udf16\ud835\udf03 to better grasp the detailed texture of the\nobjects. In multi-concept customization [Kumari et al. 2023], it was\ndiscovered that fine-tuning the key and value projections in cross-\nattention layers of the denoising network is the most effective way to\nachieve this. We set the input text condition to the optimized tokens.\nTo avoid overfitting, we further append additional \ud835\udc3f trainable tokens\nat the end of the text condition and apply prior preservation loss as\nin [Kumari et al. 2023]. The training objective is as follows:\n\ud835\udc63[1:\ud835\udc3f]\u2217,\ud835\udf16\ud835\udf03\u2217 = arg min\n\ud835\udc63[1:\ud835\udc3f],\ud835\udf16\ud835\udf03\nE\ud835\udc670\u2208E(\ud835\udc3c ),\ud835\udc66\u2032,\ud835\udf16\u223cN(0,1),\ud835\udc61\n[\u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc50\ud835\udf03 (\ud835\udc66\u2032))\u22252\n2]\n(4)\nwhere\ud835\udc66\u2032 = [\ud835\udc631\u2217, ..., \ud835\udc63\ud835\udc41 \u2217, \ud835\udc63[1:\ud835\udc3f]] and\ud835\udc63[1:\ud835\udc3f] are the trainable appended\ntokens. Only key and value projections of cross-attention layers in\n\ud835\udf16\ud835\udf03\u2217 are finetuned.\n3.2\nTraining-Free Layout Editing\nWith the learned concepts of multiple objects \ud835\udc631\u2217, ..., \ud835\udc63\ud835\udc41 \u2217 and the\nfine-tuned model \ud835\udf16\ud835\udf03\u2217, we rearrange the positions of the objects to\nedit the layout. A straightforward way to control the layout is to add\na new layout condition to a stable diffusion model, as in [Avrahami\net al. 2023; Li et al. 2023; Zhang and Agrawala 2023]. However,\nthis approach requires further fine-tuning with a additional dataset.\nInstead, we propose a training-free method to control the layout to\navoid dataset collection.\nAs discovered in P2P [Hertz et al. 2022], the cross-attention in the\ndenoising network of text-to-image diffusion models can reflect the\npositions of each generated object specified by the corresponding\ntext token, which is calculated from:\n\ud835\udc34\ud835\udc59 = \ud835\udf0e(\ud835\udc44\ud835\udc59 (\ud835\udc67\ud835\udc59\n\ud835\udc61)\ud835\udc3e\ud835\udc59 (\ud835\udc66)\ud835\udc47 )\n(5)\nwhere \ud835\udc34\ud835\udc59 is the cross-attention at layer \ud835\udc59 of the denoising network,\n\ud835\udc44\ud835\udc59, \ud835\udc3e\ud835\udc59 are the query and key projections, \ud835\udf0e is the softmax operation\nalong the dimension of text embedding \ud835\udc66, and \ud835\udc67\ud835\udc59\n\ud835\udc61 is the intermediate\nfeature of the image latent. The calculated attention \ud835\udc34\ud835\udc59 has the size\nof \u210e\ud835\udc59 \u00d7 \ud835\udc64\ud835\udc59 \u00d7 \ud835\udc51, where \u210e\ud835\udc59 and \ud835\udc64\ud835\udc59 is the spatial dimension of the\nfeature \ud835\udc67\ud835\udc59\n\ud835\udc61 and \ud835\udc51 is the length of input text tokens. More specifically,\nfor each text token, we could get an attention map of size \u210e\ud835\udc59 \u00d7 \ud835\udc64\ud835\udc59\nwhich reflects the relevance to its concept. For example, in the\nattention map with the text \"cat\", the positions within the area\ncontaining the cat should have larger values than other positions.\nTherefore, we could optimize \ud835\udc67\ud835\udc61 towards the target that the desired\narea of the object has large values. Previous study [Hertz et al. 2022]\nALGORITHM 1: Denoising process with layout control\nInput: The sequence of optimized text tokens for objects \ud835\udc66\u2217, the\ninitial image \ud835\udc3c \u2217, a set of object masks \ud835\udc40, optimization\nlearning rate \ud835\udefc\ud835\udc61, a set of thresholds {\ud835\udc44\ud835\udc61 }, the timestep to\nstop optimization and blending \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc61 and \ud835\udc61\ud835\udc4f\ud835\udc59\ud835\udc51.\nOutput: An edited Image \ud835\udc3c\n1 Encode input image: \ud835\udc67\u2217\n0 = E(\ud835\udc3c \u2217);\n2 Initialize with Gaussian noise: \ud835\udc67\ud835\udc47 = N(0, \ud835\udc3c );\n3 for \ud835\udc61 = \ud835\udc47, ..., 1 do\n// Iterative optimization:\n4\nif \ud835\udc61 \u2265 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc61 then\n5\nGet cross attention: \ud835\udc34 \u2190 \ud835\udf16\ud835\udf03\u2217(\ud835\udc67\ud835\udc61,\ud835\udc50\ud835\udf03 (\ud835\udc66\u2217),\ud835\udc61 );\n6\nfor \ud835\udc58 = 1, ..., \ud835\udc41 do\n7\n\ud835\udc34\ud835\udc59,\ud835\udc58 \u2190 \ud835\udc34\ud835\udc59 [:, :,\ud835\udc58];\n8\nCalculate L\ud835\udc58 with \ud835\udc40\ud835\udc58 as in Eqn. (6);\n9\nend\n10\nCalculate mean-max loss L as in Eqn. (7);\n11\nif L > 1 \u2212 \ud835\udc44\ud835\udc61 then\n12\nUpdate \ud835\udc67\ud835\udc61 with: \ud835\udc67\ud835\udc61 \u2190 \ud835\udc67\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 \u00b7 \u2207\ud835\udc67\ud835\udc61 L;\n13\nif Reach maximum optimization steps then\n14\ngo to 22;\n15\nend\n16\nelse\n17\ngo to 5;\n18\nend\n19\nend\n20\nend\n// Background Blending:\n21\nif \ud835\udc61 \u2265 \ud835\udc61\ud835\udc4f\ud835\udc59\ud835\udc51 then\n22\nAdd noise to original latent: \ud835\udc67\u2217\n\ud835\udc61 = \ud835\udc4e\ud835\udc51\ud835\udc51\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc52 (\ud835\udc67\u2217\n0,\ud835\udc61 ) ;\n23\nGet mask of background: \ud835\udc40\ud835\udc4f\ud835\udc54 = 1 \u2212 \u00cd\n\ud835\udc58 \ud835\udc40\ud835\udc58 ;\n24\nBlending: \ud835\udc67\ud835\udc61 \u2190 \ud835\udc40\ud835\udc4f\ud835\udc54 \u2299 \ud835\udc67\u2217\n\ud835\udc61 + (1 \u2212 \ud835\udc40\ud835\udc4f\ud835\udc54) \u2299 \ud835\udc67\ud835\udc61 ;\n25\nend\n26\n\ud835\udc67\ud835\udc61\u22121 \u2190 \ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54(\ud835\udc67\ud835\udc61,\ud835\udc50\ud835\udf03 (\ud835\udc66\u2217),\ud835\udc61 ) ;\n27 end\n28 Decode the edited image: \ud835\udc3c = D(\ud835\udc670)\nhas shown that the layers of resolution 16 \u00d7 16 contain the most\nmeaningful semantic information. Therefore, we choose \ud835\udc59 to be the\nlayers with \u210e\ud835\udc59 = \ud835\udc64\ud835\udc59 = 16. As shown in Fig. 3, for each layer \ud835\udc59, each\nchannel of the cross attention \ud835\udc34\ud835\udc59 represents the spatial relevance to\nthe corresponding text token. For example, if we want to optimize\nthe position of the object represented by \ud835\udc632\u2217, we can extract the\ncorresponding channel \ud835\udc34\ud835\udc59,2 and multiply it with the target region\nmask of \ud835\udc632\u2217 (i.e., the yellow mask). Finally, the region loss can be\ncalculated with the summation of the values within the mask (yellow\nregion) and of all positions (black and yellow regions):\nL\ud835\udc58 = 1 \u2212\n\u00cd\n\ud835\udc56 (\ud835\udc40\u2217\n\ud835\udc58 \u2299 \u00cd\n\ud835\udc59 \ud835\udc34\ud835\udc59,\ud835\udc58)\n\u00cd\n\ud835\udc56\n\u00cd\n\ud835\udc59 \ud835\udc34\ud835\udc59,\ud835\udc58\n(6)\nL = 1\n\ud835\udc41\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc58=1\nL\ud835\udc58 + max(L1, ..., L\ud835\udc41 )\n(7)\nwhere L\ud835\udc58 is the loss for the \ud835\udc58-th object with target position specified\nby \ud835\udc40\u2217\n\ud835\udc58, \ud835\udc34\ud835\udc59,\ud835\udc58 is the cross-attention map with the optimized token \ud835\udc63\ud835\udc58\u2217\n(from Eqn. (3)) at layer \ud835\udc59, and \ud835\udc56 is the spatial position in \ud835\udc34\ud835\udc59,\ud835\udc58. We use\nContinuous Layout Editing of Single Images with Diffusion Models\n\u2022\n5\nthe mean value together with the maximum value of L\ud835\udc58, so that the\nmodel can control the positions of each object and simultaneously\nfocus on the object with a large loss.\nFig. 3. Region loss calculated from cross attention. It encourages higher\ncross-attention between the object\u2019s text embedding and its corresponding\nregion than with any other region.\nWe optimize the latent \ud835\udc67\ud835\udc61 with the loss L in Eqn. (7) only at large\ntimesteps, i.e., \ud835\udc61 >= 0.5, which is enough to fix the layout of the\ngenerated image. We apply iterative optimization for \ud835\udc61 = 1.0, 0.8, 0.6\nwith maximum steps and early stopping. For other timesteps, we\nonly update \ud835\udc67\ud835\udc61 for one single step. Although the model has memo-\nrized the background during the process mentioned in Sec. 3.1, it still\nintroduces distortions to the background during the optimization\nfor layout control. To preserve the original background, we start to\nblend with the original input image at the area without objects as\nin [Avrahami et al. 2022], for timesteps \ud835\udc61 >= 0.7. The detailed algo-\nrithm for our layout control method during the denoising process\nis shown in Algo. 1.\n3.3\nImplementation Details\nWe optimize each token for different objects using masked textual\ninversion for 200 steps with a batch size of 4. The learning rate is set\nto 0.002, and it takes approximately 40 seconds for each token on\na single Nvidia V100 GPU. Next, we fine-tune the model using all\nthe optimized tokens concatenated with 1-3 additional rare tokens\nfor 800 steps (for 2 or 3 objects) or 1200 steps (for 4 objects), with a\nbatch size of 4. The learning rate is set to 0.0002, and it takes around\n3-4 minutes on an Nvidia V100 GPU. To sample images, we use\nDDIM sampling [Song et al. 2020] with 50 steps. For layout control,\nwe optimize \ud835\udc67\ud835\udc61 with a learning rate that decreases from 20 to 15\nfor \ud835\udc61 values from 1.0 to 0.5. We apply iterative optimization for 40\niterations with early stopping thresholds of 0.4, 0.3, and 0.2 when \ud835\udc61 is\n1.0, 0.8, and 0.6, respectively. It takes around 37 seconds to generate\nan image with a new layout.\n4\nEXPERIMENTS\nTo evaluate our proposed method, we first compare it qualitatively\nand quantitatively with several baselines, and then conduct a user\nstudy to compare them perceptually. Additionally, we conduct sev-\neral ablation studies to validate the effectiveness of each important\ncomponent of our method and demonstrate its application in contin-\nuous layout editing, which was not feasible with previous methods.\n4.1\nBaselines\nSince there are no existing methods that perform the same task as\nours, which is to edit the layout of existing images, we compare\nour method with four baselines that are designed and modified\nfrom existing methods: image-level manipulation, noised latent-\nlevel manipulation, and two variations of combining existing layout\ncontrol methods with textual inversion, which include GLIGEN [Li\net al. 2023] with textual inversion [Gal et al. 2022] and MultiDiffu-\nsion [Bar-Tal et al. 2023] with Dreambooth [Ruiz et al. 2023].\nImage-level manipulation. We first crop and paste the objects\nfrom the original input image onto a blank image at the positions\nspecified in the target layout. Since the target layout may specify\nobjects with different widths and heights, we scale the cropped\npatches to match the desired size. Next, we use stable diffusion\ninpainting to fill in the blank areas.\nLatent-level manipulation. Instead of cropping and pasting on\nthe image-level, we perform a similar process on the noised latent\nfrom DDIM inversion [Song et al. 2020]. We initialize a random\nnoise as the \"canvas\" and prepare a \"source image\" by adding noise\nto the original input image until \ud835\udc61 = 0.7 using DDIM inversion.\nThen, we copy objects in the \"source image\" and paste them onto\nthe \"canvas\" following the target layout without scaling, as resizing\nin latent space can lead to distorted results. Finally, we use the DDIM\nscheduler to denoise the \"canvas\" and obtain the result..\nGLIGEN with textual inversion. Although GLIGEN [Li et al.\n2023] can perform layout control, it can only generate new images\nwith target layouts. Therefore, to adapt it to our task, we need\nto add an additional step of textual inversion [Gal et al. 2022] to\nlearn the appearances of the objects in existing images. After textual\ninversion, we use the learned text tokens to sample the image, where\nthe layout is controlled with GLIGEN.\nMultiDiffusion with Dreambooth. MultiDiffusion [Bar-Tal\net al. 2023] is a training-free method for layout control, but it can-\nnot be directly used for existing image layout editing. Therefore,\nbefore using it, we adopt Dreambooth [Ruiz et al. 2023] to learn the\nconcepts of the objects in the input image, and convert them into\nmultiple text tokens. Then, we provide the prompt with the learned\ntext tokens and corresponding masks to MultiDiffusion to perform\nour task of layout editing.\n4.2\nQualitative Comparison\nFig. 4 illustrates the qualitative comparisons between our method\nand four baselines. The image-level manipulation method (column\nc) produces less realistic results as it only copies and resizes objects\nto the target positions without natural editing. For example, in row\n4, the size of the horse is significantly larger than that of the tree,\nviolating the proper order of sizes. Moreover, noticeable artifacts\nappear around objects due to imperfect cropping and inpainting, as\nevidenced by the distorted chair armrests in row 3 and the artifacts\non the horse\u2019s back in row 4. The noise-level manipulation (column\nd) can maintain the basic layout but cannot retain the visual fea-\ntures of original objects since DDIM inversion cannot reconstruct\nthe input image, resulting in the loss of the \"dog\" appearance in\nrow 2. GLIGEN with textual inversion (column e) also fails to retain\nthe visual properties of objects since the textual inversion on the\n6\n\u2022\nZhiyuan Zhang, Zhitong Huang, and Jing Liao\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFig. 4. Qualitative comparison with other baseline methods. From left to right: (a) Input images; (b) Target layout; (c) Image-level manipulation; (d) Latent-level\nmanipulation; (e) GLIGEN + textual inversion; (f) MultiDiffusion + Dreambooth; (g) Ours\nwhole image cannot disentangle the concepts of multiple objects\nwithin a single image. In row 5, for instance, the concept of the\n\"pot\" is not correctly learned. Additionally, GLIGEN is pre-trained\nwith a dataset to perform layout control, which cannot be perfectly\nadapted to learned concepts from a particular image, leading to\nsome misalignment with the specified layout, as shown in row 4.\nMultiDiffusion with Dreambooth (column f) has better layout align-\nment than GLIGEN with textual inversion because MultiDiffusion\nis a training-free method that can better cooperate with learned\nconcepts. However, Dreambooth is designed for learning concepts\nof single objects from multiple images and still cannot disentan-\ngle multiple objects present in a single image, causing significant\nchanges in object characteristics, as shown by the car in row 1 and\nthe dog in row 2. Furthermore, it introduces artifacts around the\nboundaries of objects because MultiDiffusion denoises each sub-\nregion with different diffusion processes. In row 4, for instance,\nthe regions of the \"horse\" and the \"tree\" are separately generated,\nresulting in artifacts around the objects and discontinuity between\nobjects, such as the yellow background around the \"horse\" and the\ncyan background around the \"tree\". Our method (column g) achieves\nthe best results by effectively controlling the layout of objects while\nTable 1. Quantitative comparison with other baseline methods. Our method\nachieves the best performance on both metrics: visual similarity to the input\nimage and alignment with the specified layout.\nVisual\nLayout\nsimilarity \u2191\nalignment \u2191\nImage-level manipulation\n0.57\n0.0068\nLatent-level manipulation\n0.41\n0.0071\nGLIGEN with textual inversion\n0.34\n0.0027\nMultiDiffusion with Dreambooth\n0.53\n0.0047\nOurs\n0.61\n0.0099\nretaining the visual features of the input images. It also produces\nthe highest quality and most harmonious images among all four\nbaselines.\n4.3\nQuantitative Comparison\nWe conduct a quantitative study to further evaluate the preserva-\ntion of visual properties and layout alignment of our method and\nContinuous Layout Editing of Single Images with Diffusion Models\n\u2022\n7\ncompare it with baselines. To measure the visual similarity between\nthe edited result and the input image, we calculate one minus the\nCLIP distance between the input image and edited image. A higher\nscore indicates that the objects in the edited image have a more\nsimilar visual appearance to the input image. For layout alignment,\nwe calculate the CLIP distance between the image and text prompt.\nSpecifically, for an object in the layout map, we erase the correspond-\ning region in the edited image and fill it with black color. We then\ncalculate the change in CLIP distance between the image (before\nand after erasing) and the text token corresponding to the object.\nIf the CLIP distance drops dramatically after erasing, it indicates\nthat the object is placed in the correct position after layout editing.\nThis process is repeated for each object, and an average score is\ncalculated. The experiment shows that our method has the best per-\nformance in both metrics: the least image-to-image distance (0.61)\nand the largest change in image-to-text distance (0.0099).\nFig. 5. Results of the user study on visual similarity, layout alignment, image\nquality, and overall quality, respectively.\n4.4\nUser Study\nIn this part, we perform a user study to verify the effectiveness and\nquality of our method. We compare our method with four baselines\nmentioned in Sec. 4.1, i.e., Image-level manipulation, Latent-level\nmanipulation, GLIGEN with textual inversion, and MultiDiffusion\nwith Dreambooth.\nThe user study consists of 16 questions. For each of the questions,\nwe first show the original input image and the target layout to\nthe user. Then we show the five images edited by four compared\nmethods and our method in random order. Finally, the user is asked\nto make four selections regarding four different factors:\n\u2022 Visual similarity: to choose the image whose generated ob-\njects have the highest similarity to the objects in the original\ninput image.\n\u2022 Layout alignment: to select the image whose layout is best\naligned with the target layout.\n\u2022 Image quality: to select the image with the highest quality\nand photorealism.\n\u2022 Overall quality: to choose the best result considering all the\nthree factors above.\nAmong the 16 questions, we also set a validation question where the\n5 edited images consist of a ground-truth image with four unrelated\nimages. The user has to make more than 3 correct selections out of\n4, to be considered as a valid questionnaire.\nWe finally collected 42 questionnaires, among which 31 are valid\nby passing the validation questions. Among the 31 valid participants,\n5 users are below 20 years old, 17 range from 20 to 30 years of age,\n6 are between 30 and 40 years old, and 3 are above 40 years old. The\nresult of the user study are shown in Fig. 5, and we find that our\nmethod outperforms other methods in all the four factors with a\npreferred rate of 31% in visual similarity, 29% in layout alignment,\n26% in image quality, and 26% in overall quality.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 6. Ablation study on different textual inversion methods. From left\nto right: (a) Input images; (b) Target layouts; (c) Inversion with Dream-\nbooth [Ruiz et al. 2023]; (d) Textual inversion [Gal et al. 2022] + finetune [Ku-\nmari et al. 2023]; (e) Masked textual inversion w/o finetune; (f) Our full\ninversion method with masked textual inversion and finetune.\n4.5\nAblation Study\n4.5.1\nInversion Methods. In this part, we compare the inversion\nmethod with and without a mask. We selected Dreambooth [Ruiz\net al. 2023] and textual inversion [Gal et al. 2022] + finetune as\nrepresentatives of the methods without a mask. For Dreambooth, we\njointly trained the model on multiple concepts with a single image\nand prompt pair, while the textual inversion + finetune method\nfirst runs the textual inversion to update each token on the entire\nimage and then uses the updated tokens to fine-tune the cross-\nattention layers of the model. The results show that if a single image\ncontains multiple objects, the methods without a mask, including\nboth Dreambooth and textual inversion, cannot precisely learn and\ndisentangle the concepts of different objects, which may result in\nthe loss of visual properties. For example, in Fig. 6, row 1, the shape\nand color of the flower change significantly in columns (c) and\n(d). Another issue with these methods without a mask is learning\nincorrect objects. For instance, in Fig. 6, row 2, columns (c) and (d),\nthe information about the white and brown dogs is not encoded\ninto two separate tokens. Therefore, both methods cannot correctly\nswap the position of the two dogs. The textual inversion + finetune\nmethod (d) even generates a trolley-like object instead of a dog.\nIn our method, we optimize the text tokens using masked textual\ninversion followed by model finetuning, as described in Sec. 3.1. By\napplying the mask, the information of different objects in the image\n8\n\u2022\nZhiyuan Zhang, Zhitong Huang, and Jing Liao\nis correctly disentangled. In Fig. 6, row 2, columns (e) and (f), the two\ndogs successfully swap positions. Moreover, adding finetuning after\nour masked textual inversion can further preserve visual details,\nincluding colors and textures.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 7. Ablation study on differen layout control methods. From left to right:\n(a) Input images; (b) Target layouts; (c) Our inversion + ControlNet [Zhang\nand Agrawala 2023]; (d) Our inversion + GLIGEN [Li et al. 2023]; (e) Our\ninversion + MultiDiffusion [Bar-Tal et al. 2023]; (f) Our full methods.\nInput images\nMean loss\nMax loss\nMean+max\n(ours)\nFig. 8. Ablation study on optimization loss of layout control.\n4.5.2\nLayout Control Methods. After verifying the effectiveness\nof our masked textual inversion, we compared our layout control\nmethod with three other methods: ControlNet [Zhang and Agrawala\n2023], GLIGEN [Li et al. 2023], and MultiDiffusion [Bar-Tal et al.\n2023]. For the training-based methods ControlNet and GLIGEN,\nthe generated objects fail to keep some visual features in the input\nimages, such as the \"car\" in row 1 and the \"lighthouse\" in row 2 of\nFig. 7. This is because those models are pretrained with datasets that\nmay not be well-adapted to our learned concepts, especially consid-\nering that our finetuning after masked textual inversion may further\nchange the parameters of the cross-attention layers, which can affect\nthe ability of the pretrained model. As for the training-free method\nMultiDiffusion, it can better incorporate learned concepts, but it\ntends to yield some artifacts around the objects as it denoises each\nsub-region separately and fails to fuse them smoothly. For example,\nthere is a yellow painting on the house in Fig. 7, row 1, column\n(e). Our iterative layout control method solves the aforementioned\nproblems and generates the best results in (f)\nFig. 9. Ablation study on iterative optimization. The number above the\nimage indicates the denoising step on which iterative optimization is applied.\nIf more than one number is labeled, iterative optimization is applied to\nmultiple denoising steps.\nFig. 10. Ablation study on blending steps. The number above the image\nindicates the number of steps where blending is applied.\n4.5.3\nOptimization Loss. In Eqn. (7), we mention that both the mean\nand the maximum values of L\ud835\udc58 are calculated for the optimization\nloss. As shown in Fig. 8, if only the mean loss is applied, the controls\nagainst each object are equal, and no additional effort can be put on\nthe difficult one. Therefore, the positions of some objects may be\ninsufficiently controlled. For example, part of the \"dog\" still stays at\nthe original position in Fig. 8, row 1, and the relative position of the\n\"dog\" and the \"pot\" is not correctly arranged in row 2.\nOn the other hand, if only the max loss is applied, the model may\nfocus too much on single object, but ignore others. For example, the\nposition of the \"dog\" is not modified in Fig. 8, row 1, and the \"pot\" is\nstretched and distorted to fit the target position in row 2. Therefore,\nwe finally apply both the mean and max loss, which can balance\nthe layout control of each object and simultaneously focus more on\nobjects that are difficult to control.\n4.5.4\nIterative Optimization. As described in Section 3.2, we only\napply iterative optimization at certain time steps. In this part, we\nperformed experiments to verify the effect of iterative optimization\nContinuous Layout Editing of Single Images with Diffusion Models\n\u2022\n9\nFig. 11. Continuous layout editing with different target layouts.\nand determine which time steps to apply it. As shown in Figure 9,\nwhen iterative optimization is not applied, the objects are not well\naligned with the target layout.\nWe also found that the determination of the layout control hap-\npens at the time steps closer to the noise. If iterative optimization\nis applied at large time steps (e.g., \ud835\udc61 = 1.0, 0.8, or 0.6), it is more\neffective for layout control, as a ball appears in the upper part of\nthe image. Conversely, with small time steps (e.g., \ud835\udc61 = 0.4, 0.2, or\n0.02), the layout has little change. This implies that the layout of\nthe objects is nearly fixed at large \ud835\udc61 and can hardly be modified\nwhen \ud835\udc61 is small. Therefore, we implement iterative optimization at\nrelatively larger time steps, i.e., \ud835\udc61 = 1.0 + 0.8 + 0.6.\nAs shown in the image, our choice can generate a ball with the\ndesirable size. However, fewer iterative optimization time points\n(e.g., \ud835\udc61 = 1.0 + 0.8) may not be sufficient for generating the ball\ncompletely. Conversely, too many iterative optimization time points\n(e.g., \ud835\udc61 = 1.0 + 0.8 + 0.6 + 0.4, \ud835\udc61 = 1.0 + 0.8 + 0.6 + 0.4 + 0.2) have little\neffect on the final layout but require longer optimization time and\nmay introduce artifacts. Thus, we choose \ud835\udc61 = 1.0 + 0.8 + 0.6 for a\nbalance between quality and speed.\n4.5.5\nBlending. As described in Section 3.2, we blend the edited\nimage with the original input at the region of the background. In this\npart, we perform experiments to evaluate the effect of the different\nnumber of blending steps. As shown in Figure 10, the background\nwill be completely changed if no blending is applied because the\noptimization process for layout control will have a large influence\non the background. Blending when \ud835\udc61 is large (e.g., \ud835\udc61 > 0.8) has the\nmost substantial effect on the background, and the effect of blending\nstarts to converge for more steps when\ud835\udc61 > 0.6. Therefore, we choose\nto apply background blending when \ud835\udc61 > 0.7.\n4.6\nResults of Continuous Editing\nOur method is capable of rearranging the positions of objects in\nan input image to fit a target layout without altering its visual\nproperties. This unique capability enables our method to perform\ncontinuous layout editing of single input images, which was not\npossible with previous methods. Figure 11 and Figure 1 demonstrate\nsome examples of our method\u2019s effectiveness. For instance, in row\n3 of Figure 11, we show how our method can change the positions\nof the dog and ball to fit three different layouts, creating natural\ninteractions between them for each layout. Moreover, our method\ncan handle images with multiple objects. We demonstrate this by\nshowing how it can edit the positions of three objects in Figure 11,\nrows 4, and four objects in Figure 1, row 2, to align with different\ninput layouts. These examples highlight the flexibility of our method\nto handle varying numbers of objects of different categories and\nsizes.\n10\n\u2022\nZhiyuan Zhang, Zhitong Huang, and Jing Liao\n5\nCONCLUSIONS & LIMITATIONS\nWe present the first framework that supports continuous layout edit-\ning of single images, generating high-quality results by rearranging\nthe positions of objects in the input image to fit a user-specified\nlayout while preserving their visual properties. A key component\nenabling us to learn objects from a single image is Masked Tex-\ntual Inversion, which disentangles multiple concepts into different\ntokens. With learned objects, we propose a training-free iterative\noptimization method for layout control. We demonstrate that our\nframework outperforms other baselines, including image-level ma-\nnipulation, latent-level manipulation, and combinations of existing\nlearning and layout control methods.\nInput images\nEdited images\nFig. 12. Failure cases: In the first row, our method fails to maintain the visual\nfeatures of objects when there is a significant size difference between the\ninput and edited images. In the second row, our method fails to generate the\nfull body of objects when they suffer from large occlusions in the original\nimage.\nHowever, our method still encounters some limitations. One limi-\ntation is that it may fail to preserve the visual details of an object if\nthe size of the object in the initial image and edited image varies\nsignificantly, as shown by the sailboat in Figure 12, row 1. Another\nlimitation is that it may have difficulty recovering the full body of an\nobject if the object in the input image is heavily occluded, as shown\nby the dog in Figure 12, row 2. We believe that these problems are\ncaused by the limited information that can be inferred from a single\nimage by object concept learning. To mitigate these limitations, we\ncould augment the input images to different sizes and angles and\neven inpaint missing parts of the object caused by occlusion before\napplying our masked textual inversion. Furthermore, our layout\nediting is not in real-time due to the iterative sampling nature of\nthe diffusion model. Future research directions include exploring\nmethods to accelerate the process and supporting more applications\nof layout editing.\nREFERENCES\nOmri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh,\nDani Lischinski, Ohad Fried, and Xi Yin. 2023. Spatext: Spatio-textual representation\nfor controllable image generation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 18370\u201318380.\nOmri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven\nediting of natural images. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 18208\u201318218.\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis,\nMiika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. 2022. ediffi: Text-\nto-image diffusion models with an ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324 (2022).\nOmer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. Multidiffusion: Fusing\ndiffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113 2\n(2023).\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-\nand-excite: Attention-based semantic guidance for text-to-image diffusion models.\narXiv preprint arXiv:2301.13826 (2023).\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik,\nand Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to-\nimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022).\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-Or. 2022. Prompt-to-prompt image editing with cross attention control.\narXiv preprint arXiv:2208.01626 (2022).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.\n2023. Multi-concept customization of text-to-image diffusion. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 1931\u20131941.\nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao,\nChunyuan Li, and Yong Jae Lee. 2023. GLIGEN: Open-Set Grounded Text-to-Image\nGeneration. CVPR (2023).\nTimo L\u00fcddecke and Alexander Ecker. 2022. Image segmentation using text and image\nprompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 7086\u20137096.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. 2021.\nGlide: Towards photorealistic\nimage generation and editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741 (2021).\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684\u201310695.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-\ndriven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 22500\u201322510.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. 2022. Photorealistic text-to-image diffusion models with deep language under-\nstanding. Advances in Neural Information Processing Systems 35 (2022), 36479\u201336494.\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit\nmodels. arXiv preprint arXiv:2010.02502 (2020).\nLvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-to-image\ndiffusion models. arXiv preprint arXiv:2302.05543 (2023).\n"
  }
]