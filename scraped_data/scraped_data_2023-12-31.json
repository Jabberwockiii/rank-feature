[
  {
    "title": "TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones",
    "link": "https://arxiv.org/pdf/2312.16862.pdf",
    "upvote": "27",
    "text": "TinyGPT-V: Efficient Multimodal Large Language\nModel via Small Backbones\nZhengqing Yuan\nAnhui Polytechnic University\nWuhu, China\nzhengqingyuan@ieee.org\nZhaoxu Li\nNanyang Technological University\nSingapore\nzhaoxu.li@ntu.edu.sg\nLichao Sun\nLehigh University\nBethlehem, USA\nlis221@lehigh.edu\nAbstract\nIn the era of advanced multimodel learning, multimodal large language models\n(MLLMs) such as GPT-4V have made remarkable strides towards bridging lan-\nguage and visual elements. However, the closed-source nature and considerable\ncomputational demand present notable challenges for universal usage and modi-\nfications. This is where open-source MLLMs like LLaVA and MiniGPT-4 come\nin, presenting groundbreaking achievements across tasks. Despite these accom-\nplishments, computational efficiency remains an unresolved issue, as these models,\nlike LLaVA-v1.5-13B, require substantial resources. Addressing these issues,\nwe introduce TinyGPT-V, a new-wave model marrying impressive performance\nwith commonplace computational capacity. It stands out by requiring merely a\n24G GPU for training and an 8G GPU or CPU for inference. Built upon Phi-2,\nTinyGPT-V couples an effective language backbone with pre-trained vision mod-\nules from BLIP-2 or CLIP. TinyGPT-V\u2019s 2.8B parameters can undergo a unique\nquantisation process, suitable for local deployment and inference tasks on 8G vari-\nous devices. Our work fosters further developments for designing cost-effective,\nefficient, and high-performing MLLMs, expanding their applicability in a broad\narray of real-world scenarios. Furthermore this paper proposed a new paradigm\nof Multimodal Large Language Model via small backbones. Our code and train-\ning weights are placed at: https://github.com/DLYuanGod/TinyGPT-V and\nhttps://huggingface.co/Tyrannosaurus/TinyGPT-V respectively.\n1\nIntroduction\nRecently, with the advent of GPT-4V, an expansive multimodal large language model (MLLM),\nwe\u2019ve seen some impressive capabilities in vision-language understanding and generation [45]. That\nbeing said, it\u2019s pivotal to acknowledge that GPT-4V hasn\u2019t been released open-source, thus restricting\nuniversal usage and independent modifications. On the bright side, there has been a recent surge in\nopen-source MLLMs, such as LLaVA and MiniGPT-4, which have demonstrated groundbreaking\nprowess in some tasks, surpassing GPT-4V in areas like image captioning (IC), visual question\nanswering (VQA), and referring expression comprehension (REC) [8, 26, 27, 50]. For example, when\nput to the test on various visual grounding and question answering tasks, MiniGPT-v2 [6] emerges as\na superior force compared to other conventional vision-language models.\nRegardless of the substantial vision language capabilities exhibited by some open-source MLLMs,\nthey still consume an excessive amount of computational resources during both training and inference\nstages. For example, LLaVA-v1.5-13B [26] used 8 A100 GPUs with 80GB memory over the course\nof 25.5 hours of training. Because the performance of large language models directly impacts the\ncapabilities of MLLMs, their usage, such as LLaVA-v1.5-13B utilizing Vicuna-13b-v1.5 [49], and\nMiniGPT-v2 leveraging LLaMA2-7B-Chat [41], necessitates a substantial number of parameters of\nlarge language models to enhance performance in complex tasks like IC, VQA etc [50]. Therefore,\nPreprint. Work in progress.\narXiv:2312.16862v1  [cs.CV]  28 Dec 2023\nFigure 1: Compared to other general-purpose MLLMs, our TinyGPT-V achieves the same perfor-\nmance as 13B or 7B models in a variety of visual language tasks.\nwe require a large language model that can rival the performance of models such as LLaMA2 and\nVicuna-v1.5, without the need for excessive GPU computational resources.\nSo a new model called TinyGPT-V is proposed which requires just 24G of GPU for training and\nonly 8G of GPU or CPU for inference. It utilizes an advanced large language model, Phi-2 [19],\nwhich is constructed upon Phi [24], and reported to have outperformed the best effects of the 13B\nlanguage models and it demonstrated similar or better results than models that are up to 25 times\nlarger in scale. Regarding visual perception, we make use of the same pre-trained vision modules\nfrom BLIP-2 [23] or CLIP [35], which incorporates a ViT [10] as vision encoder as well as a mapping\nmodule. Following the training approach of MiniGPT, TinyGPT-V only applies the mapping module\nbetween the visual encoder and language model throughout the training process, while freezing all\nother parameters. TinyGPT-V uses the same dataset as MiniGPT-v2 in various stages of training such\nas LAION [37], Conceptual Captions [4, 39], SBU [33] etc. [25, 38, 18, 21, 29, 13, 31, 20, 46].\nIn our study, we observed that TinyGPT-V displays many qualities that mirror those present in\nGPT-4, benefiting greatly from the application of the Phi-2 model. Boasting just 2.8B parameters,\nTinyGPT-V\u2019s unique quantisation process makes it suitable for local deployment and inference tasks\non 8G mobile devices. TinyGPT-V represents a substantial stride in reaching the equilibrium between\nunparalleled performance and maintaining the efficiency of MLLMs. Through our contributions, we\nstrive to empower the community to engineer more cost-effective, efficient, and high-performance\nMLLMs for broad, real-world application scenarios.\n2\nRelated Work\nAdvanced language model. The evolution of language models has been marked by significant\nmilestones, starting with early successes like GPT2 [36] and BERT [9] in natural language processing\n(NLP). These foundational models set the stage for the subsequent development of vastly larger\nlanguage models, encompassing hundreds of billions of parameters. This dramatic increase in scale\nhas led to the emergence of advanced capabilities as seen in models like GPT-3 [2], Chinchilla [16],\nOPT [48], and BLOOM [44].These large language models (LLMs) have been instrumental in\nfurther advancements in the field. For instance, ChatGPT [32] and InstructGPT [34] leverage these\npowerful models to answer diverse questions and perform complex tasks such as coding. The\nintroduction of open-source LLMs like LLaMA [41] has further propelled research in this area,\ninspiring subsequent developments like Alpaca [40], Vicuna [7]. These models fine-tune the LLaMA\nmodel with additional high-quality instruction datasets, showcasing the versatility and adaptability\n2\nof LLM frameworks.Among the most notable recent advancements are Phi [24] and its successor,\nPhi-2 [19]. These models have demonstrated exceptional performance, rivaling or even surpassing\nmodels up to 25 times larger in scale. This indicates a significant shift in the landscape of language\nmodeling, emphasizing efficiency and effectiveness without necessarily relying on sheer size. Such\ndevelopments mark a new era in the field of NLP, where smaller, more efficient models can achieve\nresults comparable to their much larger counterparts, opening up new possibilities for application and\nresearch.\nMultimodal language model. In recent years, the trend of aligning visual input to large language\nmodels for vision-language tasks has gained significant attention [5, 42, 1, 23, 28, 26, 50, 6]. Seminal\nworks like VisualGPT [5] and Frozen [42], which utilized pre-trained language models for image\ncaptioning and visual question answering. This approach was further advanced by models such\nas Flamingo [1], which incorporated gated cross-attention mechanisms to align pre-trained vision\nencoders and language models, training on vast image-text pairs. BLIP-2 [23] introduced an efficient\nQ-Former for aligning visual and language modalities. These groundbreaking studies have paved the\nway for further innovations in the field, leading to the development of models like LLaVA [28] and\nMiniGPT4 [50], and their subsequent iterations, LLaVA-v1.5 [26], MiniGPT-v2 [6], ArtGPT-4 [47],\ninstruction GPT-4 [43] and Instruction Mining [3]. These models have demonstrated advanced\nmultimodal capabilities through instruction tuning, showcasing remarkable generalization abilities.\nDespite their powerful capabilities for visual-language tasks, these multimodal language models often\nrequire substantial computational resources. In contrast, TinyGPT-V represents a paradigm shift,\nharnessing a cost-effective and powerful small language model to achieve a robust, easily deployable\nmodel suitable for a variety of real-world vision-language applications. This approach underscores a\nmove towards more efficient yet equally competent multimodal language modeling.\n3\nMethod\nWe commence by proposing our vision-language model, TinyGPT-V, then conduct a discussion on\nthe structure of the model and the organization of tasks, and finally introduce the training process for\neach stage.\n3.1\nModel Architecture\nIn this subsection, we present the architecture of TinyGPT-V, which consists of a visual coder Linear\nprojection layer and a large language model.\nPhi-2\nImage-text Pair \nInstruction Learning\nFreeze\nTrain\nStage 1\nVisual Encode \n& Q-Former\nNormalization\nMiniGPT-4 \nProj.\nLinear\nPhi-2\nStage 2\nVisual Encode \n& Q-Former\nNormalization\nMiniGPT-4 \nProj.\nLinear\n& LoRA\nPhi-2\nVisual Encode \n& Q-Former\nNormalization\nMiniGPT-4 \nProj.\nLinear\n& LoRA\nStage 3\nPhi-2\nVisual Encode \n& Q-Former\nNormalization\nMiniGPT-4 \nProj.\nLinear\n& LoRA\nMulti-Tasks Learning\nStage 4\nFigure 2: The training process of TinyGPT-V, the\nfirst stage is warm-up training, the second stage\nis pre-training, the third stage is instruction fine-\ntuning, and the fourth stage is multi-task learning.\nVisual encoder backbone. EVA [11] of ViT\nserves as the visual foundation model in the\nTinyGPT-V adaptation as same as MiniGPT-v2.\nThe visual foundation remains inactive through-\nout entire model training. Our model training\noperates at a picture resolution of 224x224 for\nStage 1, 2 and 3 and 448x448 for Stage 4, and\nwe amplify the positional encoding to scale up\nwith an elevated image resolution.\nLinear projection layers. The function of the\nLinear Projection layer is to embed the visual\nfeatures extracted by the visual encoder into\nthe language model.\nAs well as making an\neffort to empower extensive language models\nto comprehend image-based information. Our\nemployment of the Q-Former layer, sourced\nfrom the BLIP-2 [23] architecture, as the ini-\ntial linear projection layer is driven by the goal\nto extract maximum functionality from the pre-\ntrained BLIP system when deployed in visual\nlanguage models. This approach substantially\ndecreases the volume of parameters that require\ntraining stages. We utilize linear projection layers initialized with a Gaussian distribution as the\n3\nLinear\nDown\nr\nLinear\nUp\nPretrained\nWeights\n\ufffd\nInput\nOutput\nLoRA\n(a) LoRA\nMLP\nNormalization\nMHA\nLoRA\n(b) LoRA Module \nfor LLMs Block\nMHA\nQuery-Key \nNormalization\nLayer Norm\nRMS Norm\nMLP\nLoRA\nAfter \nStage1\n(c) LLMs Block\n for TinyGPT-V\nAttention\nQ\nK\nV\nLayer \nNorm\nLayer \nNorm\n(d) Query-Key \nNormalization for MHA\nFreeze\nTrain\nData\nPathway\nConditional Pathway\nFigure 3: (a) represents the structure of LoRA, (b) represents how LoRA can efficiently fine-tune\nlarge language models (LLMs) in natural language processing, (c) represents the structure of LLMs\nfor TinyGPT-V, and (d) represents the structure of QK Normalization.\nsecond layer. The goal is to bridge the dimensionality gap between the Q-Former output and the lan-\nguage model\u2019s embedding layer, thus, better aligning the visual tokens with the relevant hidden space\nof the language model. As illustrated in Figure 2, to accelerate the training process of TinyGPT-V, we\ninitially employ the pre-trained Linear projection from MiniGPT-4 (Vicuna 7B) as the foundational\nlayer. Subsequently, we integrate an additional Linear layer projection to effectively bridge into the\ncorresponding hidden space of the Phi-2 model.\nLarge lanuguage model backbone. We utilize the Phi-2 [19] model as the backbone for our\nTinyGPT-V large language model. Phi-2 is a 2.7 billion-parameter language model with excellent\nreasoning and language comprehension, demonstrating state-of-the-art performance among base\nlanguage models with fewer than 13 billion parameters. In complex benchmarks, Phi-2 matches or\noutperforms most models 25 times larger. We rely squarely on Phi-2 linguistic tokens to execute\nseveral vision language operations. For vision anchoring assignments that require the creation of\nspatial locations, we explicitly request the linguistic model to generate textual depictions of bounding\nboxes to signify their geographical coordinates.\nNormalization and LoRA for TinyGPT-V. In Section 4.3, we deduce that training smaller large-\nscale language models for transfer learning, particularly across different modalities (such as from\ntext to image), presents significant challenges. Our investigations reveal that smaller models are\nparticularly susceptible to NaN or INF values during multimodal data computation. This often\nresults in a computational loss value of NaN, causing the initial batch forward propagation to fail.\nAdditionally, a limited number of trainable parameters in these smaller models can contribute to\ngradient vanishing throughout the training process. To address these issues, as show in Figure 3 (c),\nwe integrate the LLaMA-2 post-norm and input norm mechanisms, implementing RMS Norm after\neach Multi Head Attention Layer (MHA) to normalize the data for subsequent layers. We also update\nall Layer norms in the Phi-2 model to enhance training stability, as illustrated in the equation below.\nLayerNorminput(xhidden) = \u03b3 xhidden \u2212 \u00b5\n\u221a\n\u03c32 + \u03f5\n+ \u03b2\n(1)\nWhere, xhidden is the input of this layer, \u00b5 and \u03c32 are the mean and variance of the inputs to the\nlayer, respectively, \u03f5 is a small number to prevent division by zero, \u03b3 and \u03b2 are trainable parameters.\nRMSNorm(xpost) =\nxpost\nq\n1\nN\nPN\ni=1 x2\ni + \u03f5\n(2)\nwhere xpost is the input after MHA, N is the number of elements in the vector, and \u03f5 is a small\nconstant added for numerical stability.\n4\nFurthermore, Henry et al. [15] have underscored the vital role of Query-Key Normalization in\nlow-resource learning scenarios. Hence, as show in Figure 3 (d), we have incorporated Query-Key\nNormalization into the Phi-2 model, as detailed in the following equation.\nAttention(Q, K, V ) = softmax\n\u0012LayerNorm(Q)LayerNorm(K)T\n\u221adk\n\u0013\nV\n(3)\nwhere dk denotes the dimension of Q or K.\nThe structure of the LoRA mechanism [17] is show in Figure 3 (a), which is an efficient fine-tuning\nmethod in parallel to the frozen pre-training weights as shown in Figure 3 (c), which does not increase\nthe inference time consuming for large language models and is easier to optimise.\n3.2\nMulti-task Instruction Template\nIn order to mitigate the potential ambiguity when training a unified multi-modal model to handle\nvarious distinct tasks such as visual question answering, image captioning, referring expression\ncomprehension, generation, and object parsing and grounding, we have used the MiniGPT-v2 tokens\nof task-specific within a multi-task instruction template. It is originating from the LLaMA-2 [41]\nconversation template and includes a general input format consisting of image features, a task\nidentifier token, and an instruction input. It have six distinct task identifiers, each correlated to a\nspecific task. For tasks requiring the model to identify spatial locations of referred objects, It utilize\ntextual formatting of bounding boxes, with coordinates normalized within the range of 0 to 100.\nOverall, the unique task-specific tokens provided by MiniGPT-v2 facilitate disambiguation among\ntasks, allowing for more precise and accurate task execution.\n3.3\nTraining Stages\nIn this subsection the three-stage training process of TinyGPT-V will be described.\nTable 1: The full list of datasets used by TinyGPT-V during training.\nData types\nDataset\nStage 1\nStage 2\nStage 3\nStage 4\nImage-text pair\nLAION, CC3M, SBU\n\u2713\n\u2713\n\u2717\n\u2717\nInstruction tuning\nMiniGPT-4 Stage2 for CC & SBU\n\u2717\n\u2717\n\u2713\n\u2717\nCaption\nText Captions\n\u2717\n\u2717\n\u2717\n\u2713\nREC\nRefCOCO, RefCOCO+, RefCOCOg, Visual Genome\n\u2717\n\u2717\n\u2717\n\u2713\nVQA\nGQA, VQAv2, OK-VQA, AOK-VQA\n\u2717\n\u2717\n\u2717\n\u2713\nMultimodal instruction\nLLaVA dataset, Flickr30k, Multi-task conversation\n\u2717\n\u2717\n\u2717\n\u2713\nLangauge dataset\nUnnatural Instructions\n\u2717\n\u2717\n\u2717\n\u2713\nWarm-up training for the first training stage. During the initial pretraining stage, TinyGPT-V is\ntaught vision-language understanding using a large library of aligned image-text pairs. The model\nidentifies the output from the introduced projection layers as a soft prompt directing it to create\nrelevant texts and to allow large language models to accept inputs from the image modality. The\npretraining process uses a dataset combination of Conceptual Caption, SBU and LAION, involving\n20000 training steps covering about 5 million image-text pairs.\nPre-training for the second training stage. Following the initial training phase, the large language\nmodel becomes equipped to process image modality inputs. To guarantee more consistent perfor-\nmance as the model transitions into the subsequent training stage, we re-employ the dataset from the\nfirst stage, specifically for training the LoRA module.\nHuman-like learning for the third training stage. We fine-tuned this TinyGPT-V model using\na selection of image-text pairings from MiniGPT4 or LLaVA, which included instructions like\n\u201c###Human: <Img><ImageHere></Img> Take a look at this image and describe what you no-\ntice.###Assistant:.\u201d. We used a uniform template inclusive of a randomly chosen prompt that\nimproved the model\u2019s capacity for generating responses that were consistent and sounded more\nnatural.\nMulti-task learning in the fourth training stage. The fourth training stage of TinyGPT-V focuses on\nenhancing its conversation ability as a chatbot by tuning the model with more multi-modal instruction\n5\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\nIterations\n0.2\n0.4\n0.6\n0.8\n1.0\nLearning Rate\n1e\n4\nStage 1 Learning Rate Schedule\n0\n2500\n5000\n7500 10000 12500 15000 17500 20000\nIterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLearning Rate\n1e\n4\nStage 2 Learning Rate Schedule\n0\n200\n400\n600\n800\n1000\nIterations\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLearning Rate\n1e\n5\nStage 3 Learning Rate Schedule\n0\n10000\n20000\n30000\n40000\n50000\nIterations\n0\n1\n2\n3\n4\n5\n6\n7\n8\nLearning Rate\n1e\n5\nStage 4 Learning Rate Schedule\nFigure 4: Changes in learning rate during the training stage of TinyGPT-V.\ndatasets as same as MiniGPT-v2. These datasets, as shown in Table 1, include LLaVA, Flickr30k, a\nmixing multi-task dataset, and Unnatural Instruction. The LLaVA dataset is utilized for multi-modal\ninstruction tuning with detailed descriptions and complex reasoning examples. The Flickr30k dataset\nis used to improve grounded image caption generation and object parsing and grounding capabilities.\nAdditionally, a mixing multi-task dataset is created to improve the model\u2019s handling of multiple tasks\nduring multi-round conversations. Finally, to recover the language generation ability, the Unnatural\nInstruction dataset is added to the third-stage training of TinyGPT-V.\n4\nExperiments\nIn this section, we describe the training and evaluation methods in detail.\n4.1\nTraining\nExperimental setting. The experimental environment for this study was established with a single\nNVIDIA RTX 3090 GPU, equipped with a substantial 24GB of VRAM. The central processing was\nhandled by an AMD EPYC 7552 48-Core Processor, offering 15 virtual CPUs. Memory allocation\nwas set at 80GB, ensuring sufficient capacity for handling large datasets. The software environment\nwas standardized on PyTorch version 2.0.0, with CUDA 11.8 support, facilitating optimized tensor\noperations on the GPU.\nTraining process. In our experimental process, we meticulously orchestrated the training of our\nmodel through four distinct stages, each characterized by specific learning rate strategies and loss\nprofiles, as shown in Figure 4 and Figure 5.\n6\n0\n10\n20\n30\n40\n50\n60\nEpochs\n1\n2\n3\n4\n5\n6\n7\nTraining Loss\nSame as MiniGPT-4 Training Process\nMulti-tasks Fine Tuning Process\nTinyGPT-V Training Process\nStage 1\nStage 2\nStage 3\nStage 4\nFigure 5: Changes in loss during the training stage of TinyGPT-V.\nStage 1: Spanning 17 epochs, with each epoch consisting of 1000 iterations, we employed a dynamic\nlearning rate approach. The learning rate commenced at 1e-5 at the beginning of each epoch and\ngradually ascended to 1e-4 by the epoch\u2019s end. This pattern was consistently applied across all 17\nepochs. The training loss exhibited a steady decline, starting from 7.152 and progressively tapering\ndown to 2.620, reflecting the model\u2019s increasing proficiency in learning from the data. The purpose\nof this stage is to be able to make the Phi-2 model in TinyGPT-V react in some way to the input of\nthe image modality. The alignment of text and image in the semantic space is done.\nStage 2:\nComprising 4 epochs, each with 5000 iterations, this stage introduced the \u201clin-\near_warmup_cosine_lr\u201c [14, 12] learning rate schedule. We initiated a warmup phase of 5000\nsteps, where the learning rate linearly increased from 1e-6 (warmup_lr) to 1e-4 (init_lr), followed\nby a cosine decay down to a minimum learning rate of 8e-5. This phase saw a consistent reduction\nin loss, starting at 2.726 and culminating at 2.343. The purpose of this stage is to enable the LoRA\nmodule to play a role in multimodal data, further reducing the model\u2019s loss on image-text pairs and\nimproving the model\u2019s ability to learn from the data.\nStage 3: This stage lasted for 5 epochs, each with 200 iterations.\nWe maintained the \u201clin-\near_warmup_cosine_lr\u201c schedule, with a warmup phase of 200 steps. The learning rate began\nat 1e-6, ascending to 3e-5 (init_lr), before decaying to 1e-5 (min_lr). The loss values reflected\nsignificant improvements, starting at 1.992 and reducing to 1.125. The purpose of this stage is to\nallow TinyGPT-V to accept both verbal and image modal inputs and produce responses to them. After\nthis stage of training TinyGPT-V has been able to perform most of the image answering tasks.\nStage 4: The final stage stretched over 50 epochs, each comprising 1000 iterations. We adhered to\nthe \u201clinear_warmup_cosine_lr\u201c schedule with a 1000-step warmup phase. The learning rate was\ninitiated at 1e-6, reaching up to 1e-5 (init_lr), and then experiencing a cosine decay to a minimum of\n8e-5. The training loss values displayed a consistent downward trajectory, beginning at 2.720 and\nultimately reaching as low as 1.399. The purpose of this stage is to allow TinyGPT-V to perform\nvarious tasks such as VQA or VSR tasks at the same time, increasing the generalization performance\nof TinyGPT-V on multimodal tasks.\n4.2\nEvaluation\nEvaluation datasets. GQA [18] is a dataset for real-world visual reasoning and compositional\nquestion answering, featuring a powerful question engine that generates 22 million diverse reasoning\nquestions. VSR [28] comprises over 10k natural text-image pairs in English, encompassing 66 types\nof spatial relations. IconQA [30] with 107,439 questions aimed at challenging visual understanding\nand reasoning in the context of icon images, encompassing three sub-tasks (multi-image-choice,\nmulti-text-choice, and filling-in-the-blank). VizWiz [13] is a collection of more than 31,000 visual\nqueries, each derived from a photo taken by a visually impaired individual using a smartphone,\naccompanied by a vocalized question regarding the image, and supplemented with 10 answers\n7\nTable 2: Comparative performance of TinyGPT-V and other MLLMs across multiple visual question\nanswering benchmarks.\nMethod\nParameters\nGrounding\nGQA\nVSR\nIconVQ\nVizWiz\nHM\n(zero-shot)\n(zero-shot)\n(zero-shot)\n(zero-shot)\nFlamingo\n9B\n\u2717\n-\n31.8\n-\n28.8\n57.0\nBLIP-2\n13B\n\u2717\n41.0\n50.9\n40.6\n19.6\n53.7\nLLaVA\n13B\n\u2717\n41.3\n51.2\n43.0\n-\n-\nShikra\n13B\n\u2713\n-\n-\n-\n-\n-\nInstructBLIP\n13B\n\u2717\n49.5\n52.1\n44.8\n33.4\n57.5\nMiniGPT-4\n13B\n\u2717\n30.8\n41.6\n37.6\n-\n-\nOurs\nTinyGPT-V\n2.8B\n\u2713\n33.6\n53.2\n43.3\n24.8\n53.2\nsourced from a crowd for each query. The Hateful Memes dataset (HM) [22], developed by Facebook\nAI, is a comprehensive multimodal collection specifically designed for the detection of hateful\ncontent in memes, combining both image and text elements, and comprises over 10,000 newly created\nmultimodal examples.\nVisual question answering results. As shown in Table 2, it becomes evident that TinyGPT-V, a\nmodel with only 2.8 billion parameters, exhibits notably competitive performance across multiple\nbenchmarks, closely rivaling models with nearly 13 billion parameters. Specifically, in the VSR\n(Visual Spatial Reasoning) zero-shot task, TinyGPT-V outshines its counterparts by securing the\nhighest score of 53.2%. This is particularly impressive considering its parameter size is approximately\n4.6 times smaller than other leading models such as BLIP-2, LLaVA, and InstructBLIP. In the GQA\nbenchmark, while TinyGPT-V scores 33.6%, it lags behind the highest score achieved by InstructBLIP,\nwhich is 49.5%. However, TinyGPT-V shows robust performance in the IconVQ challenge, attaining\na score of 43.3%, just 1.5% short of InstructBLIP\u2019s leading score of 44.8%. Similarly, in the VizWiz\ntask, TinyGPT-V demonstrates commendable capabilities with a score of 24.8%, which, though\nnot the highest, is notable given its reduced parameter count. In the context of the Hateful Memes\n(HM) dataset, TinyGPT-V matches InstructBLIP\u2019s top score of 57.5% with its own score of 53.2%,\nagain underscoring its efficiency and capacity to compete with models of larger scales. Overall,\nTinyGPT-V\u2019s performance across these diverse and challenging benchmarks is striking, especially\nwhen considering its parameter efficiency\n4.3\nAblation Study\nAs shown in Table 3, the full TinyGPT-V model achieves low loss across all stages, but the removal\nof key modules leads to significant training issues. Without the LoRA module, there\u2019s a gradient\nvanish starting from Stage 3. Omitting Input Layer Norm increases loss notably (to 2.839 in Stage\n1) and causes gradient vanishing in Stage 4. Without RMS Norm, the model sees an elevated loss\nin Stage 1 (2.747) and faces early gradient vanishing in Stage 2. The absence of QK Norm results\nin immediate gradient vanish. This data clearly illustrates each module\u2019s crucial role in preventing\ngradient vanishing and maintaining low loss throughout the training process.\nTable 3: Importance of each module in TinyGPT-V at each stage of training.\nMethod\nStage 1 Loss\nStage 2 Loss\nStage 3 Loss\nStage 4 Loss\nTinyGPT-V\n2.620\n2.343\n1.125\n1.444\n-LoRA\n2.620\n-\nGradient Vanish\n-\n-Input Layer Norm\n2.839\n2.555\n1.344\nGradient Vanish\n-RMS Norm\n2.747\nGradient Vanish\n-\n-\n-QK Norm\nGradient Vanish\n-\n-\n-\nFurthermore, our reveal a notable trend: the smaller the large language model used for transfer\nlearning (particularly in transitioning from text-to-image modality), the more challenging the training\nprocess becomes. We observed a pronounced need for additional normalization layers to stabilize\nthe training, especially when scaling down from larger models like Vicuna-13B to smaller ones like\nPhi-2 (2.7B) and Phi-1.5 (1.3B).\n8\n5\nConclusion\nIn this study, we introduce TinyGPT-V, a parameter-efficient MLLMs tailored for a range of real-world\nvision-language applications. Our model innovatively builds on the compact yet powerful Phi-2 small\nlanguage model framework. This approach results in TinyGPT-V delivering exceptional outcomes in\ndiverse benchmarks like visual question-answering and referring expression comprehension while\nkeeping computational demands manageable. Remarkably, TinyGPT-V can be trained on a 24G GPU\nand deployed on an 8G device, demonstrating a significant advancement in creating cost-effective,\nefficient, and potent MLLMs. This paper marks a contribution towards crafting smaller, yet robust\nmultimodal language models for practical, real-world use cases. We envision that our work will\ncatalyze further explorations into developing compact MLLMs for diverse applications.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems,\n35:23716\u201323736, 2022.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[3] Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. Instruction mining: When data mining\nmeets large language model finetuning, 2023.\n[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts, 2021.\n[5] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient\nadaptation of pretrained language models for image captioning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 18030\u201318040, 2022.\n[6] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman\nKrishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large\nlanguage model as a unified interface for vision-language multi-task learning. arXiv preprint\narXiv:2310.09478, 2023.\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April\n2023), 2023.\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning, 2023.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale, 2021.\n[11] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang,\nXinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning\nat scale, 2022.\n[12] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\nAndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training\nimagenet in 1 hour, 2018.\n9\n[13] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo,\nand Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n3608\u20133617, 2018.\n[14] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for\nimage classification with convolutional neural networks, 2018.\n[15] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normal-\nization for transformers, 2020.\n[16] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n[18] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\nreasoning and compositional question answering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 6700\u20136709, 2019.\n[19] Mojan Javaheripi, S\u00e9bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck,\nCaio C\u00e9sar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth\nGopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin Tat Lee, Yuanzhi\nLi,\nAnh\nNguyen,\nGustavo\nde\nRosa,\nOlli\nSaarikivi,\nAdil\nSalim,\nShital\nShah,\nMichael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel\nWard, Philipp Witte, Cyril Zhang, and Yi Zhang.\nPhi-2:\nThe surprising power\nof small language models.\nhttps://www.microsoft.com/en-us/research/blog/\nphi-2-the-surprising-power-of-small-language-models/, 2023.\n[20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring\nto objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP), pages 787\u2013798, 2014.\n[21] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik\nRingshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in\nmultimodal memes. Advances in neural information processing systems, 33:2611\u20132624, 2020.\n[22] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik\nRingshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in\nmultimodal memes, 2021.\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models, 2023.\n[24] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat\nLee. Textbooks are all you need ii: phi-1.5 technical report, 2023.\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays,\nPietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. Microsoft coco: Common\nobjects in context, 2015.\n[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning, 2023.\n[27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[29] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang,\nand Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual\nlanguage reasoning. arXiv preprint arXiv:2110.13214, 2021.\n10\n[30] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang,\nand Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual\nlanguage reasoning, 2022.\n[31] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin\nMurphy. Generation and comprehension of unambiguous object descriptions. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 11\u201320, 2016.\n[32] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.\n[33] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1\nmillion captioned photographs. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q.\nWeinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran\nAssociates, Inc., 2011.\n[34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision, 2021.\n[36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[37] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs, 2021.\n[38] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh\nMottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In\nEuropean Conference on Computer Vision, pages 146\u2013162. Springer, 2022.\n[39] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych\nand Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, Melbourne, Australia,\nJuly 2018. Association for Computational Linguistics.\n[40] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model,\n2023.\n[41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\nand fine-tuned chat models, 2023.\n[42] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. Advances in Neural Information\nProcessing Systems, 34:200\u2013212, 2021.\n11\n[43] Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: A 200-instruction\nparadigm for fine-tuning minigpt-4, 2023.\n[44] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana\nIli\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. Bloom:\nA 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,\n2022.\n[45] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023.\n[46] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling\ncontext in referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69\u201385.\nSpringer, 2016.\n[47] Zhengqing Yuan, Xinyi Wang, Kun Wang, Lichao Sun, and Yanfang Ye. Artgpt-4: Towards\nartistic-understanding large vision-language models with enhanced adapter, 2023.\n[48] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[49] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n[50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n12\n"
  },
  {
    "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action",
    "link": "https://arxiv.org/pdf/2312.17172.pdf",
    "upvote": "24",
    "text": "Unified-IO 2: Scaling Autoregressive Multimodal Models\nwith Vision, Language, Audio, and Action\nJiasen Lu1*\nChristopher Clark 1*\nSangho Lee1*\nZichen Zhang1*\nSavya Khosla2\nRyan Marten2\nDerek Hoiem2\nAniruddha Kembhavi13\n1Allen Institute for AI\n2 University of Illinois Urbana-Champaign\n3 University of Washington\n{jiasenl, chrisc, sanghol, chralesz, anik}@allenai.org\nunified-io-2.allenai.org\nWhat food could you \nmake with these \ningredients ? Give \nme the recipe.\nOne delicious recipe using these ingredients is \nchocolate pudding! Here's the recipe:\nIngredients:\n- 1 cup all-purpose flour, - \u00bd cup sugar \u2026.\nInstructions:\n1. In a large bowl whisk together the flour, sugar..\n2. In a separate bowl, mix together the eggs \u2026\nGenerate an audio \ntrack for this band.\nGenerate an audio \ntrack for this band.\nRender a sunset\nRemove the dock\nPaint this image \nlike Van Gogh\nGenerate an image of \na car with the model \nin the first image and \nthe color in the \nsecond image.\nGenerate a \ndepth image\nGenerate a depth & \nsurface normal map\nGenerate a surface \nnormal map\nWhich fruits are in this \nimage ? List them in \njson format with the \nname of the fruit as the \nkey and the color of \nthe fruit as the value.\n{\n\"banana\" : \"Yellow\",\n\"apple\" : \"Red\",\n\"grapes\" : \"Green\"\n}\nSegment grapes and \none apple\nPut the less kobar \nblicket into the dax.\n= blicket\n= dax\nis kobar than\nGiven the initial \nimage and a \nsequence of \nactions, predict \nthe next frames\nGenerated\nImages\nGenerate an image \nof an astronaut \nriding a horse in the \nforest. There is a \nriver in front of them \nwith water lilies. \nGenerate an image of \nan elephant swimming \nunderwater. aesthetic. \nFantasy.\nAdd the missing \ndetails to the \nmasked image (left) \nusing the reference \nimage (right).\nFind the visible keypoints \ncorresponding to the \nperson located in the \nhighlighted region.\nIdentify the locations \nof the instruments \nproducing the given \nsound.\n(drum sounds)\nImage Editing\nImage Generation\nReference Image Generation\nMultiview Image Completion\nDepth & Surface Normal \nKeypoint Estimation\nVisual Parsing\nFree Form VQA\nVisual based Audio Generation\nVisual Audio Localization\nRobotic Manipulation\nFuture Frame Prediction\nturn right\nturn right\nturn right\nmove ahead\nmove ahead\nmove ahead\nmove ahead\nturn right\nSegmentation\n&\nFigure 1. UNIFIED-IO 2 is an instruction-following model with a huge breadth of abilities and supported modalities. It can generate images\n(red box), including image editing, image generation, depth estimation, surface normal estimation, and future frame prediction etc. It can\nalso generate texts (blue box), including long-form answers to queries, keypoint estimation, visual audio localization, predicting actions\nfor robotic manipulation etc. It can generate audio (green box) from images or text. Click n and n for the corresponding audio samples.\n* Leading Authors, equal contribution. A description of each author\u2019s\ncontribution is available in Appendix A. Corresponding to Jiasen Lu.\n1\narXiv:2312.17172v1  [cs.CV]  28 Dec 2023\nAbstract\nWe present UNIFIED-IO 2, the first autoregressive multi-\nmodal model that is capable of understanding and generat-\ning image, text, audio, and action. To unify different modal-\nities, we tokenize inputs and outputs \u2013 images, text, audio,\naction, bounding boxes etc., into a shared semantic space\nand then process them with a single encoder-decoder trans-\nformer model. Since training with such diverse modalities\nis challenging, we propose various architectural improve-\nments to stabilize model training. We train our model from\nscratch on a large multimodal pre-training corpus from di-\nverse sources with a multimodal mixture of denoisers objec-\ntive. To learn an expansive set of skills, such as following\nmultimodal instructions, we construct and finetune on an\nensemble of 120 datasets with prompts and augmentations.\nWith a single unified model, UNIFIED-IO 2 achieves state-\nof-the-art performance on the GRIT benchmark and strong\nresults in more than 35 benchmarks, including image gener-\nation and understanding, natural language understanding,\nvideo and audio understanding, and robotic manipulation.\nWe release all our models to the research community.\n1. Introduction\nAs AI researchers, we seek to build intelligent agents that\ncan perceive their environment, communicate with others,\nact in the world, and reason about their interactions. The\nworld is multimodal, so our agents must partake in rich\ninteractions that are multimodal in nature via vision, lan-\nguage, sound, action etc. Psychologists have argued that\nthe redundancy of our sensory systems serves as supervi-\nsory mechanisms to improve each other [48, 144, 167]. This\nprovides a natural motivation to create models with similar\nlearning capabilities, supporting many different modalities\nthat can supervise each other during training.\nBuilding models that can parse and produce many\nmodalities is a complex undertaking. Training Large Lan-\nguage Models (LLMs) with billions of parameters, despite\nonly supporting a single modality, is extremely challenging\nacross many fronts \u2013 from sourcing and processing massive\ndatasets, ensuring data quality and managing biases, design-\ning effective model architectures, maintaining stable train-\ning processes, and instruction tuning to enhance the model\u2019s\nability to follow and understand user instructions. These\nchallenges are hugely amplified with the addition of each\nnew modality.\nIn light of these difficulties, a line of recent works\nin building multimodal systems has leveraged pre-trained\nLLMs, with some augmenting with new modality encoders\n[5, 46, 119], some adding modality specific decoders [14,\n96] and others leveraging the LLM\u2019s capabilities to build\nmodular frameworks [64, 166, 173]. Another line of works\non training multimodal models from scratch has focused on\ngenerating text output [81, 143] with a few recent works\nsupporting the understanding and generation of two modal-\nities \u2013 text and images [123, 125]. Building generative mod-\nels with a wider coverage of modalities, particularly when\ntraining from scratch, remains an open challenge.\nIn this work, we present UNIFIED-IO 2, a large multi-\nmodal model (LMM) that can encode text, image, audio,\nvideo, and interleaved sequences and produce text, action,\naudio, image, and sparse or dense labels. It can output free-\nform multimodal responses and handle tasks unseen during\ntraining through instruction-following. UNIFIED-IO 2 con-\ntains 7 billion parameters and is pre-trained from scratch on\nan extensive variety of multimodal data \u2013 1 billion image-\ntext pairs, 1 trillion text tokens, 180 million video clips,\n130 million interleaved image & text, 3 million 3D assets,\nand 1 million agent trajectories. We further instruction-tune\nthe model with a massive multimodal corpus by combining\nmore than 120 datasets covering 220 tasks across vision,\nlanguage, audio, and action.\nOur pre-training and instruction tuning data, totaling\nover 600 terabytes, presents significant challenges for train-\ning due to its diversity and volume. To effectively facilitate\nself-supervised learning signals across multiple modalities,\nwe develop a novel multimodal mixture of denoiser objec-\ntive that combines denoising and generation across modali-\nties. We also develop dynamic packing \u2013 an efficient imple-\nmentation that provides a 4x increase in training throughput\nto deal with highly variable sequences. To overcome the\nstability and scalability issues in training, we propose to ap-\nply key architectural changes, including 2D rotary embed-\ndings, QK normalization, and scaled cosine attention mech-\nanisms on the perceiver resampler. For instruction tuning,\nwe ensure every task has a clear prompt, either using exist-\ning ones or crafting new ones. We also include open-ended\ntasks and create synthetic tasks for less common modalities\nto enhance task and instruction variety.\nWe evaluate UNIFIED-IO 2 on over 35 datasets across\nthe various modalities it supports. Our single model sets the\nnew state of the art on the GRIT [66] benchmark, which in-\ncludes diverse tasks such as keypoint estimation and surface\nnormal estimation. On vision & language tasks, it matches\nor outperforms the performance of many recently proposed\nVLMs that leverage pre-trained LLMs. On image genera-\ntion, it outperforms the closest competitor [174] that lever-\nages the pre-trained stable diffusion model [154], especially\nin terms of faithfulness as per the metrics defined in [76]. It\nalso shows effectiveness in video, natural language, audio,\nand embodied AI tasks, showcasing versatility despite its\nbroad capability range. Moreover, UNIFIED-IO 2 can fol-\nlow free-form instructions, including novel ones. Figure 1\noffers a glimpse into how it handles various tasks. Further\nexamples, along with the code and models, are accessible\non our project website.\n2\n2. Related Work\nInspired by the success of language models as general-\npurpose text processing systems [20, 122, 177], there has\nbeen a recent wave of multimodal systems trying to achieve\nsimilar general-purpose capabilities with additional modali-\nties. A common approach is to use a vision-encoder to build\nfeatures for input images and then an adapter to map those\nfeatures into embeddings that can be used as part of the in-\nput to an LLM. The network is then trained on paired im-\nage/language data to adapt the LLM to the visual features.\nThese models can already perform some tasks zero-shot or\nwith in-context examples [109, 132, 178], but generally a\nsecond stage of visual instruction tuning follows using in-\nstructions, visual inputs, and target text triples to increase\nzero-shot capabilities [25, 34, 118, 119, 205, 218, 225].\nBuilding upon this design, many researchers have ex-\npanded the breadth of tasks these models can support. This\nincludes creating models that can do OCR [12, 220], vi-\nsual grounding [12, 26, 143, 189, 207, 212, 219], image-\ntext-retrieval [97], additional languages [112], embodied\nAI tasks [17, 135, 140, 152] or leverage other expert sys-\ntems [52].\nOther efforts have added new input modali-\nties. This includes video inputs [110, 126], audio [80] or\nboth [216]. PandaGPT [170] and ImageBind-LLM [69] use\nthe universal encoder ImageBind [56] to encode many kinds\nof input modalities, and ChatBridge [222] uses a similar\nuniversal encoder based on language. While these efforts\nare effective for understanding tasks, they do not allow com-\nplex multimodal generation and often exclude modalities\nlong considered central to computer vision (e.g., ImageBind\ncannot support sparse annotation of images).\nFewer works have considered multimodal generation.\nUNIFIED-IO [123], LaVIT [88], OFA [186], Emu [172]\nand CM3Leon [210] train models to generate tokens that\na VQ-GAN [49, 179] can then decode into an image,\nwhile GILL [96], Kosmos-G [141] and SEED [53] gener-\nate features that a diffusion model can use, and JAM [4]\nfuses pre-trained language and image generation models.\nUNIFIED-IO 2 also uses a VQ-GAN, but supports text, im-\nage, and audio generation.\nOverall, this shows a strong trend towards expanding the\nnumber of supported tasks and modalities. UNIFIED-IO 2\npushes this trend to its limit, including the capabilities of\nthese prior works with few exceptions and the ability to gen-\nerate outputs in more modalities. Recently, CoDi [174] also\nachieved similar any-to-any generation capabilities by using\nmultiple independently trained diffusion models and align-\ning their embedding spaces. UNIFIED-IO 2 has stronger\nlanguage abilities and can perform well on many more\ntasks.\nA notable feature of UNIFIED-IO 2 is that the model\nis trained from scratch instead of being initialized with a\npre-trained LLM. Prior works [114, 186, 188, 192] fol-\nlowing this approach are typically not designed to produce\ncomplex generations like free-form text responses, images\nor sounds, or follow text instructions.\nCompared to re-\ncent general-purpose multimodals models [81, 143, 210],\nUNIFIED-IO 2 has a significantly broader scope of tasks\nand outputs. Training from scratch means that the method\ncan be reproduced without a costly preliminary stage of lan-\nguage model pre-training and is a more natural fit for how\nhumans learn modalities simultaneously through their co-\noccurrences, not one at a time.\n3. Approach\nIn this section, we discuss the unified task representation\n(3.1), the model architecture and techniques to stabilize\ntraining (3.2), the multimodal training objective (3.3) and\nthe efficiency optimizations (3.4) used in UNIFIED-IO 2.\n3.1. Unified Task Representation\nUNIFIED-IO 2 processes all modalities with a single, uni-\nfied encoder-decoder transformer [181]. This is achieved\nby encoding various inputs and outputs \u2013 images, text, au-\ndio, action, boxes etc., into sequences of tokens in a shared\nrepresentation space. Our encoding procedure follows the\ndesign of UNIFIED-IO [123], with several modifications to\nimprove performance and new encoders and decoders for\nadditional modalities. Figure 2 shows an overview of the\nmodel. Details about how modalities are encoded are given\nbelow.\nText, Sparse Structures, and Action. Text inputs and out-\nputs are tokenized using the byte-pair encoding [161] from\nLLaMA [177], which we chose since it supports Unicode\nsymbols and preserves whitespace. Sparse structures such\nas bounding boxes, keypoints, and camera poses are dis-\ncretized and then encoded using 1000 special tokens added\nto the vocabulary [27, 123]. Points are encoded with a se-\nquence of two such tokens (one for x and one for y), boxes\nare encoded with a sequence of four tokens (upper left and\nlower right corners), and 3D cuboids are represented with\n12 tokens that encode the projected center, virtual depth,\nlog-normalized box dimension, and continuous allocentric\nrotation [16]. For embodied tasks, discrete robot actions\n[17] are generated as text commands (e.g., \u201cmove ahead\u201d to\ncommand the robot to move forward in navigation). Spe-\ncial tokens are used to encode the robot\u2019s state, such as its\nposition and rotation. Details are in Appendix B.1.\nImages and Dense Structures. Images are encoded with a\npre-trained Vision Transformer (ViT) [84]. We concatenate\nthe patch features from the second and second-to-last layers\nof the ViT to capture both low and high-level visual infor-\nmation. These features are passed through a linear layer\nto get embeddings that can be used as part of the input se-\nquence for the transformer. To generate images, we use VQ-\nGAN [49] to convert images into discrete tokens. These to-\n3\nText\nImage\nImage \nHistory\nAudio\nAudio\nHistory\nViT\nEncoder\nViT\nEncoder\nAST\nEncoder\nAST\nEncoder\nEmb\nPerceiver\nPerceiver\nVQ-GAN \nDecoder\nVQ-GAN\nDecoder\nspeech\nenvironmental\nsound\nimage\nSN\nSegmentation\nDepth  \nText\nAction\nDetection\nmusic\nLinear\nLinear\nDynamic Packing\nDynamic Unpacking\n512\n576\n128\n64\n256\n512\n1024\n512\nBPE\nEncode\nBPE \nDecode\nKeypoint\nUnified-IO 2\nFigure 2. UNIFIED-IO 2 architecture. Input text, images, audio, or image/audio history are encoded into sequences of embeddings which\nare concatenated and used as input to an encoder-decoder transformer model. The transformer outputs discrete tokens that can be decoded\ninto text, an image, or an audio clip.\nkens are added to the vocabulary and then used as the target\noutput sequence in order to generate an image. For better\nimage quality, we use a dense pre-trained VQ-GAN model\nwith 8 \u00d7 8 patch size that encodes a 256 \u00d7 256 image into\n1024 tokens with a codebook size of 16512.\nFollowing [123], we represent per-pixel labels, which\ninclude depth, surface normals, and binary segmentation\nmasks, as RGB images that can be generated or encoded\nwith our image generation and encoding abilities. For seg-\nmentation, UNIFIED-IO 2 is trained to predict a binary\nmask given a class and bounding box. An entire image can\nbe segmented by first doing detection, and then querying the\nmodel for a segmentation mask for each detected bounding\nbox and class. See Appendix B.1 for details.\nAudio. UNIFIED-IO 2 encodes up to 4.08 seconds of au-\ndio into a spectrogram (See Appendix B.1 and Table 8).\nThe spectrogram is then encoded with a pre-trained Audio\nSpectrogram Transformer (AST) [57], and the input embed-\ndings are built by concatenating the second and second-to-\nlast layer features from the AST and applying a linear layer\njust as with the image ViT. To generate audio, we use a ViT-\nVQGAN [208] to convert the audio into discrete tokens.\nSince there is no public codebase, we implement and train\nour own ViT-VQGAN with 8 \u00d7 8 patch size that encodes\na 256 \u00d7 128 spectrogram into 512 tokens with a codebook\nsize of 8196.\nImage and Audio History. We allow up to four additional\nimages and audio segments to be given as input, which\nwe refer to as the image or audio history. These elements\nare also encoded using the ViT or AST, but we then use a\nperceiver resampler [5], see Table 8 for hyperparameters,\nto further compress the features into a smaller number of\ntokens (32 for images and 16 for audio). This approach\ngreatly reduces the sequence length and allows the model\nto inspect an image or audio segment in a high level of de-\ntail while using elements in the history for context. This his-\ntory is used to encode previous video frames, previous audio\nsegments, or reference images for tasks such as multi-view\nimage reconstruction or image-conditioned image editing.\nEight special tokens are added to the text vocabulary and\nused to reference the individual elements in these histories\nin the text input or output.\n3.2. Architecture\nUNIFIED-IO 2 uses a transformer encoder-decoder archi-\ntecture.\nHowever, we observe that using a standard im-\nplementation following UNIFIED-IO leads to increasingly\nunstable training as we integrate additional modalities. As\nshown in Figure 3 (a) and (b), training only on image gener-\nation (green curve) results in stable loss and gradient norm\nconvergence. Introducing a combination of image and text\ntasks (orange curve) slightly increases the gradient norm\ncompared to a single modality, but remains stable. How-\never, the subsequent inclusion of the video modality (blue\ncurve) leads to an unrestrained escalation in the gradient\nnorm. When an XXL version of this model is trained on\nall modalities, as shown in Figure 3 (c) and (d), the loss\nexplodes after 350k steps, and the next token prediction ac-\ncuracy significantly drops at 400k steps. To address this,\nwe include various architectural changes that significantly\nstabilize multimodal training.\n2D Rotary Embedding.\nInstead of relative positional\nembedding [147], we apply rotary positional embeddings\n(RoPE) [169] at each transformer layer.\nFor non-text\nmodalities, we extend RoPE to two-dimensional positions:\nFor any 2D indexes (i, j), we split each of the query and\nkey embeddings of the transformer attention heads in half\nand apply separate rotary embeddings constructed by each\nof the two coordinates to the halves, see Appendix B.2.\nQK Normalization. We observe extremely large values in\n4\n0\n5000\n10000\n15000\n20000\n6\n8\n10\n12\n14\nLoss\n0\n5000\n10000\n15000\n20000\n-1\nGradient Norm\nimage-text\nimage\nimage-text-video\n10\n0\n10\n1\n10\n2\n10\n100k\n200k\n300k\n400k\n12\n14\n16\n18\n20\n22\nLoss\n100k\n200k\n300k\n400k\n0.0\n0.2\n0.4\n0.6\n0.8\nAccuracy\nText\nImage\nAudio\n(a)\n(b)\n(d)\n(c)\nimage-text\nimage\nimage-text-video\nFigure 3. Left: Training loss (a) and gradient norms (b) on different modality mixtures. Right: Training loss (c) and next token prediction\naccuracy (d) of UIO-2XXLon all modalities. Results were obtained before applying the proposed architectural improvements.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nStep\n1e6\n11.5\n12.0\n12.5\n13.0\n13.5\n14.0\n Training Loss\n1B\n3B\n7B\nFigure 4. Training loss curves for the three models, which are\npretrained with dynamic packing and a batch size of 512.\nthe multi-head attention logits when including image and\naudio modalities, which leads to attention weights becom-\ning either 0 or 1 and contributes to training instability. To\nsolve this, following [38], we apply LayerNorm [10] to the\nqueries and keys before the dot-product attention computa-\ntion.\nScaled Cosine Attention. We use perceiver resampler [86]\nto compress each image frame and audio segment into a\nfixed number of tokens. We found that even with QK nor-\nmalization, the attention logits in the perceiver can grow to\nextreme values. Therefore, we apply more strict normaliza-\ntion in the perceiver by using scaled cosine attention [121],\nwhich significantly stabilizes training.\nTo avoid numerical instabilities, we also enable float32\nattention logits. Jointly updating the pre-trained ViT and\nAST can also cause instabilities. Thus, we freeze the ViT\nand AST during pretraining and finetune them at the end of\ninstruction tuning. Figure 4 shows that the pre-training loss\nfor our model is stable despite the heterogeneity of input\nand output modalities.\n3.3. Training Objective\nA strong multimodal model has to be exposed to solving\ndiverse sets of problems during pre-training. UL2 [175]\nproposed the Mixture of Denoisers (MoD), a unified per-\nspective to train LLMs, which combines the span corruption\n[147] and causal language modeling [19] objectives. Moti-\nvated by this, we propose a generalized and unified perspec-\ntive for multimodal pre-training.\nMultimodal Mixture of Denoisers.\nMoD uses three\nparadigms: [R] \u2013 standard span corruption, [S] \u2013 causal\n1\n2\n3\n4\n5\ns\n1\n2\n3\n4\n2\n5\n(a)\n1\n2\n3\n4\n5\n2\n5\n2\n5\n(b)\n1\n2\n3\n4\n5\ns\n2\n3\n4\n2\n5\n(c)\nmasked\ntarget\ncurrent\nleaked\nEncoder \nInput\nDecoder \nInput\nDecoder \nTarget\nFigure 5. Different training paradigms in masked image modeling:\n(a) autoregressive, (b) mask auto-encoder, (c) autoregressive with\ndynamic masking. Our proposed paradigms can maintain causal\ngeneration while avoiding information leaks in the decoder.\nlanguage modeling, and [X] \u2013 extreme span corruption.\nFor text targets, we follow the UL2 paradigms. For im-\nage and audio targets, we define two analogous paradigms:\n[R] \u2013 masked denoising where we randomly mask x% of\nthe input image or audio patch features and task the model\nto re-construct it and [S] \u2013 where we ask the model to gen-\nerate the target modality conditioned only on other input\nmodalities. During training, we prefix the input text with a\nmodality token ([Text], [Image], or [Audio]) and a\nparadigm token ([R], [S], or [X]) to indicate the task.\nAutoregressive with Dynamic Masking.\nOne problem\nwith image and audio masked denoising in an autoregres-\nsive manner is an information leak on the decoder side; see\nFigure 5 (a). The current decoder\u2019s input token (3) is condi-\ntioned on enocoder\u2019s information (2, 5) and all previous to-\nkens (s \u2192 2) to predict target (4). As a result, the predicted\ntoken will be conditioned on 1 even though it was masked\nin the encoder since it appears in the decoder, which will\nsimplify the task and harm representation learning. Sim-\nply masking the token in the decoder, as shown in Figure 5\n(b), avoids this information leakage but causes the genera-\ntion and de-noising tasks to interfere with one another. For\nexample, we found that joint training with generation (50%\nMAE and 50% causal modeling) significantly reduced im-\nage generation performance. Our solution is to mask the\ntoken in the decoder except when predicting that token, as\nshown in Figure 5 (c), which does not interfere with causal\nprediction whilst mostly eliminating data leakage. For im-\nage and audio generation, we also use row, column, and\nconv-shaped masked sparse attention [148] in the decoder.\n5\nAudio\nImage\nText\nVideo\nAgent Trajectories\nSynthetic\nImage/Text Interleaved\nVideo\nImage/Text Pairs\nAgent Trajectories\nSynthetic\nImage/Text Interleaved\nVideo\nImage/Text Pairs\nText\nEmbodied AI\nNatural Language\nAudio Understanding\nVideo Sparse Labelling\nVideo Understanding\nImage Dense Labelling\nImage Sparse Labelling\nImage Understanding\nAudio Generation\nImage Generation\nEmbodied QA\nGoal Generation\nNext Frame/State Prediction\nAction Prediction\nLanguage Modeling\nText Instruction Following\nAudio Captioning\nAudio Tagging\nVideo Sound Localization\nVideo Action Localization\nVideo Tracking\nVideo Instruction Following\nVideo Question Answering\nVideo Captioning\nVideo Tagging\nOptical Flow\nDepth Estimation\nReferring Expression_Segmentation\nSurface Normal Estimation\nLocalized Segmentation\nSemantic_Segmentation\nKeypoint Detection\nText Detection\n3D\nReferring Expression\nObject Localization\nObject Detection\nImage Pair _QA\nImage Instruction Following\nRegion Captioning\nRelationship Prediction\nImage Tagging\nRegion Classification\nImage Captioning\nVQA\nAudio from Video\nAudio from Text\nView_Synthesis\nImage Inpainting\nNext Frame Generation\nImage Editing\nControllable Image Editing\nImage from Text\nText\nImage\nAudio\nFigure 6. Distribution of pre-training and instruction tuning data. Segments proportional to sampling rates. The inner section shows the\ntarget modality, and the outer section shows the data type. Please refer to Figure 9 and Figure 11 in the Appendix for particular datasets.\nModel\nmodel dims mlp dims\nencoder lyr\ndecoder lyr\nheads\nParams\nUIO-2L\n1024\n2816\n24\n24\n16\n1.1B\nUIO-2XL\n2048\n5120\n24\n24\n16\n3.2B\nUIO-2XXL\n3072\n8192\n24\n24\n24\n6.8B\nTable 1. Size variant of UNIFIED-IO 2.\n3.4. Efficient Implementation\nTraining on heavily multimodal data results in highly vari-\nable sequence lengths for the transformer\u2019s inputs and out-\nputs, both because modalities are often missing for individ-\nual examples and because the number of tokens used to en-\ncode particular modalities can vary from just a few tokens\n(for a sentence) to 1024 tokens (for an output image). To\nhandle this efficiently, we use packing, a process where the\ntokens of multiple examples are packed into a single se-\nquence, and the attentions are masked to prevent the trans-\nformer from cross-attending between examples.\nTypically, packing is done during pre-processing, but it is\nchallenging in our setup since our encoders and decoder do\nnot always support it. Instead, we do packing right before\nand after the transformer encoder-decoder stage, which al-\nlows the modality encoders/decoder to run on the unpacked\ndata. During training, we use a heuristic algorithm to re-\narrange data being streamed to the model so that long ex-\namples are matched with short examples they can be packed\nwith. Packing optimization was also explored in [100], but\nnot in the streaming setup. Dynamic packing leads to an\nalmost 4x increase in training throughput (Details in Ap-\npendix B.3).\n3.5. Optimizer\nWe use Adafactor [164] as our optimizer with a linear\nwarm-up for the first 5,000 steps and a learning rate decay\nof 1/\n\u221a\nk. We train with \u03b21 = 0.9 and \u03b22 = 1.0 \u2212 k\u22120.8,\nwhere k is the step number. We use global norm gradient\nclipping with a threshold of 1.0 and find that this is crucial\nto stabilized training. Table 1 gives the details of our differ-\nent models. For all models, we train 3.0M steps \u2013 1.5M for\npre-training and 1.5M for instruction tuning, respectively.\nMore details in Appendix B.4.\n4. Multimodal Data\nOne critical difference between UNIFIED-IO 2 and prior\nwork is that we train the model with a diverse set of mul-\ntimodal data from scratch.\nThis requires curating high-\nquality, open-source multimodal data for both pre-training\n(4.1) and instruction tuning (4.2).\n4.1. Pre-training Data\nOur pre-training data comes from various sources and cov-\ners many modalities. We provide a high-level overview and\ndetails in Appendix C.\nNLP [33%]. We use the publicly available datasets that\nwere employed to train MPT-7B [176]. This dataset em-\nphasizes English natural language text but also contains\ncode and markdown. It includes text from the RedPajama\ndataset [32], C4 [68], Wikipedia, and stack overflow. We\nfollow the proportion suggested by [176] and remove multi-\nlingual and scientific data.\nImage & Text [40%]. Text and image paired data comes\nfrom LAION-400M [159], CC3M [163], CC12M [23], and\nRedCaps [42].\nTo help train the image-history modal-\nity, we also use the interleaved image/text data from\nOBELICS [104]. We use the last image as the image in-\nput and the remaining images as the image history. Special\ntokens are used to mark where those images occur in the\ntext.\n6\nthe side there's a brick \nbase at the bottom \ncomplete with \u2026\n1. Select target modalities \n[Audio][R] the side there's \na brick base at the bottom \ncomplete with \u2026\n2. Select input modalities \n4. Generate input mask\n5. Pair with Prefix token\n[Audio][R] \n(Text, Image, Image History)\nVideo Data\nModel Inputs\n1.\n2.\n3.\n3.\n\u2026\n\u2026\n(Audio)\n3. Select objective\nMask Audio Denoising [R]\nModel Targets\n\u2026\nFigure 7. Construction of training samples from video data for the model\u2019s input and target. Given the video, we first extract the video\nframes and the corresponding audio spectrograms and transcript. Then, the data pass through a random selection process to determine the\ntarget modality, input modalities, training objective, input mask etc. The model\u2019s final input and target are shown in the top right.\nVideo & Audio [25%].\nVideo provides strong self-\nsupervisory signals with high correlations between audio\nand visual channels. We sample audio and video data from\nvarious public datasets including YT-Temporal-1B [215],\nACAV100M [105], AudioSet [54], WebVid-10M [13], HD-\nVILA-10M [200] and Ego4D [60].\n3D & Embodiment [1%]. For self-supervised 3D and em-\nbodiment pre-training, we use CroCo [194] for cross-view\ngeneration and denoising; Objaverse [40] for view synthe-\nsis; and random trajectories in ProcTHOR [39] and Habi-\ntat [157] for the next action and frame predictions.\nAugmentation [1%]. While there is a lot of unsupervised\ndata on the web for images, text, video, and audio, options\nare much more limited for dense and sparse annotations.\nWe propose to solve this through large-scale data augmen-\ntation. We consider two types of data augmentation: 1. Au-\ntomatically generated segmentation data from SAM [94] to\ntrain the model to segment an object given a point or bound-\ning box.\n2.\nSynthetic patch-detection data which tasks\nthe model to list the bounding boxes of synthetically added\nshapes in an image. We additionally train the model to out-\nput the total number of patches in the image to pre-train its\ncounting abilities.\nTraining Sample Construction. During pre-training, most\nof our data contains various modalities without a supervised\ntarget. In these cases, we randomly pick one of the modal-\nities present to be the target output. Then, we either re-\nmove that modality from the example or replace it with a\ncorrupted version. Other modalities that might be present in\nthe example are randomly kept or masked to force the model\nto make predictions using whatever information is left. Fig-\nure 7 shows an example when using a video that contains\na sequence of image frames, the corresponding audio, and\na text transcript. The pre-training sample is constructed by\nfollowing the procedure: 1. select the target modality; 2.\nselect which other input modalities to keep; 3. select the\nobjective; 4. generate the random input mask depending on\nthe task of denoising or generation; 5. add a prefix token\nindicating the task.\n4.2. Instruction Tuning Data\nMultimodal instruction tuning is the key process to equip\nthe model with diverse skills and capabilities across various\nmodalities and even adapt to new and unique instructions.\nWe construct the multimodal instruction tuning dataset by\ncombining a wide range of supervised datasets and tasks.\nWe ensure every task has a clear prompt, either using exist-\ning ones or writing new ones. We also include open-ended\ntasks and create synthetic tasks for less common modali-\nties to enhance task and instruction variety. Our mixture\nincludes 220 tasks drawn from over 120 external datasets.\nWe provide a high-level overview and examples here and\nleave details in Appendix D.\nNatural Language [25.0%]. For natural language, we use\nthe mixture from FlanV2 [122] and various other instruc-\ntion following datasets [33, 142]. In addition, we continue\npre-training on our unsupervised NLP mixture to help pre-\nvent the model from forgetting information learned from\npre-training during the extensive instruction tuning stage.\nImage Generation [17.6%]. For text-to-image generation,\nwe use the same image & text pairs we used during pre-\ntraining. We also include data from [102, 103, 115] that\nprovide better caption quality.\nWe additionally train the\nmodel to generate images through view synthesis [40, 194],\nimage editing [18, 217], segmentation-based image genera-\ntion [123] and inpainting [123].\nAudio Generation [7.5%].\nThis includes text-to-audio\ndatasets with audio in the wild [47, 93, 131], music [2],\nand human speech [85]. We also add pre-training data with\nthe task of predicting the next audio clip in a video. More\nspecifically, we divide the audio into segments and then\ngenerate one of them given both the text and previous seg-\nments as input.\nImage Understanding [17.8%]. We include various data\nsources from visual question answering [6], image tagging\n[41], region classification [102], and datasets with open-\n7\nWhat are the things I should be cautious about \nwhen I visit here?\nWhen visiting this dock on the water surrounded \nby a forested area, there are several factors to \nconsider:\n1. Visibility: The visibility around the dock may be \nlimited, so it is essential to take care while \nwalking on the dock to avoid any sudden shifts in \nlighting conditions or obstructions on the shore.\n2. Water levels: There might be flooding or \nexcessive waves that could damage the wooden \ndock or affect the safety of the area. It is crucial to \nbe mindful of the water levels to \u2026\nPlease provide a short description of the image.\nAn image of dock on the water with trees in the \nbackground.\nPlease provide a detailed description of the image \nand share your personal impressions of the scene.\nThe image features a wooden jetty over a beautiful \nlake with numerous green trees surrounding the \narea. The scene gives off a peaceful and serene \natmosphere. On the far side of the scene, a \nmountain range can be seen, stretching in the \ndistance. In the middle of the scene, \u2026\nMake the image haunted.\nMake the bridge red.\nMake it in midnight.\nMake it in sunset.\nImagine you are da Vinci.\nDetect the mountain.\nDetect the dock.\nSegment the dock.\nSurface normals\nRemove the bridge.\nGenerate music about this scene. (original)\nGenerate music about this scene. (haunted)\nFigure 8. Our single model can perform a multitude of multimodal tasks: captioning the image, following free-form instructions, image\nediting, object detection, semantic segmentation, surface normal, and image-based audio generation, etc. Here, we show the outputs of our\nmodel for a variety of prompts. Click n and n for the corresponding audio samples.\nended chat-like responses [119, 220].\nWe also include\nthe multimodal instruction tuning datasets M3IT [112] and\nMIMIC-IT [107].\nVideo Understanding [10.6%]. We include data sources\nfrom video captioning [190, 199], video tagging [35, 111,\n168], and video question answering [196, 198]. We also use\nexamples from M3IT [112] and MIMIC-IT [107] for video\ninstruction following.\nAudio Understanding [10.6%]. We include data sources\nfrom audio tagging [24, 54], and audio captioning [47, 93].\nWe also include data from video action classification [7]\nwith audio in the dataset.\nImage Sparse Labelling [7.25%]. These tasks require out-\nputting sparse coordinates based on an input image. We\nmainly consider object detection [115], referring expression\n[91], 3D detection [16], camera pose prediction [40], text\ndetection [183] and human keypoints [115].\nImage Dense Labelling [4.06%]. We do several image la-\nbeling tasks, including surface normal estimation [78, 204],\ndepth estimation [138], and optical flow [21, 44]. We also\ntrain our models on various segmentation tasks, including\nsemantic segmentation, localization segmentation, and re-\nferring expression segmentation.\nVideo Sparse Labelling [3.42%]. We do video detection\n[151], single object tracking [50, 79] and video action lo-\ncalization [61].\nEmbodied AI [4.33%]. For VIMA-Bench [87], we use\nthe image input as the initial observation of the environ-\nment and the image history for the images or videos in\nthe prompt.\nWe add large-scale manipulation datasets\n[63, 127, 184] with continuous control in both simulated\nand real-world environments. We also train on the Point-\nNav task from Habitat Gibson scenes.\nThe distribution of the instruction tuning data is in Fig-\nure 6. Overall, our instruction tuning mixture is composed\nof 60% prompting data, meaning supervised datasets com-\nbined with prompts. To avoid catastrophic forgetting, 30%\nof the data is carried over from pre-training.\nAddition-\nally, 6% is task augmentation data we build by constructing\nnovel tasks using existing data sources, which enhances ex-\nisting tasks and increases task diversity. The remaining 4%\nconsists of free-form text to enable chat-like responses.\n5. Experiments\nIn this section, we evaluate our pre-trained and instruction-\ntuned models on a broad range of tasks that require parsing\nand producing all modalities: images, video, audio, text,\nand actions. We do not perform task-specific finetuning\nin any experiments. Details about experimental setups, ad-\nditional result details, results on natural language tasks, and\nadditional studies for UNIFIED-IO 2\u2019s instruction capabili-\nties are in Appendix E.\n5.1. Pre-training Evaluation\nWe demonstrate the effectiveness of our pre-training by\nevaluating UNIFIED-IO 2 on commonsense natural lan-\nguage inference (HellaSwag [214]), text-to-image gener-\nation (TIFA [76]) and text-to-audio generation (Audio-\nCaps [93]).\nWe also assess spatial and temporal under-\nstanding on SEED-Bench [106], a benchmark for compre-\nhensively evaluating perception and reasoning on image\nand video modalities. Table 2 shows that UNIFIED-IO 2\nachieves comparable or even better performance on both\ngeneration and comprehension tasks compared to the\ntask-specific specialist [154] or the universal multimodal\nmodel [9].\n8\nMethod\nHellaSwag\u2191 TIFA\u2191 SEED-S\u2191 SEED-T\u2191 AudioCaps\u2193\nLLaMA-7B [177]\n76.1\n-\n-\n-\n-\nOpenLLaMa-3Bv2 [55]\n52.1\n-\n-\n-\n-\nSD v1.5 [154]\n-\n78.4\n-\n-\n-\nOpenFlamingo-7B [9]\n-\n-\n34.5\n33.1\n-\nUIO-2L\n38.3\n70.2\n37.2\n32.2\n3.08\nUIO-2XL\n47.6\n77.2\n40.9\n34.0\n3.10\nUIO-2XXL\n54.3\n78.7\n40.7\n35.0\n3.02\nTable 2. Zero-shot performance on commonsense sentence com-\npletion (HellaSwag [214]), text-to-image generation (TIFA [76]),\nspatial and temporal comprehension (Seed-Bench [106]), and text-\nto-audio generation (AudioCaps [93]).\nMethod\nCat.\nLoc.\nVqa\nRef.\nSeg.\nKP\nNorm.\nAll\nAblation\nUIO-2L\n70.1\n66.1\n67.6\n66.6\n53.8\n56.8\n44.5\n60.8\nUIO-2XL\n74.2\n69.1\n69.0\n71.9\n57.3\n68.2\n46.7\n65.2\nUIO-2XXL\n74.9\n70.3\n71.3\n75.5\n58.2\n72.8\n45.2\n66.9\nTest\nGPV-2 [89]\n55.1\n53.6\n63.2\n52.1\n-\n-\n-\n-\nUIOXL [123]\n60.8\n67.1\n74.5\n78.9\n56.5\n67.7\n44.3\n64.3\nUIO-2XXL\n75.2\n70.2\n71.1\n75.5\n58.8\n73.2\n44.7\n67.0\nTable 3. Results on the GRIT ablation and test sets [66].\nDespite extensive multitasking, the results on HellaSwag\nsuggest that UNIFIED-IO 2 has language modeling capa-\nbilities between typical 3B and 7B language models. This\nmay be due to that the model sees far fewer tokens com-\npared to language-based LLMs \u2013 approximately 250 billion\ntokens in total. Qualitative results of pre-training are in Ap-\npendix E.1.\n5.2. GRIT Results\nWe evaluate on the General Robust Image Task (GRIT)\nBenchmark [66], which includes seven tasks: categoriza-\ntion, localization, VQA, referring expression, instance seg-\nmentation, keypoint, and surface normal estimation. Com-\npleting all 7 tasks requires understanding image, text, and\nsparse inputs and generating text, sparse, and dense outputs.\nAlthough this is a subset of the modalities UNIFIED-IO 2\nsupports, we evaluate on GRIT because it provides a stan-\ndardized and comprehensive benchmark on this set of capa-\nbilities. See Appendix E.3 for additional inference details\non GRIT.\nResults are shown in Table 3. Overall, UNIFIED-IO 2\nis state-of-the-art on GRIT, surpassing the previous best\nmodel, UNIFIED-IO, by 2.7 points. On individual tasks,\nwe can observe gains in localization (3 points), catego-\nrization (14 points), segmentation (2 points), and key-\npoint (5 points).\nOn VQA, our GRIT evaluations show\nUNIFIED-IO 2 is better on same-source (84.6 vs. 81.2)\nquestions, suggesting the gap is due to reduced performance\non the new-source questions that were constructed from Vi-\nsual Genome; see Appendix E.3 for additional discussion.\nDespite being slightly behind UNIFIED-IO, UNIFIED-IO 2\nMethod\nImage\nAudio\nAction\nFID\u2193\nTIFA\u2191\nFAD\u2193\nIS\u2191\nKL\u2193\nSucc.\u2191\nminDALL-E [37]\n-\n79.4\n-\n-\n-\n-\nSD-1.5 [154]\n-\n78.4\n-\n-\n-\n-\nAudioLDM-L [117]\n-\n-\n1.96\n8.13\n1.59\n-\nAudioGen [101]\n-\n-\n3.13\n-\n2.09\n-\nDiffSound [203]\n-\n-\n7.75\n4.01\n2.52\n-\nVIMA [87]\n-\n-\n-\n-\n-\n72.6\nVIMA-IMG [87]\n-\n-\n-\n-\n-\n42.5\nCoDi [174]\n11.26\n71.6\n1.80\n8.77\n1.40\n-\nEmu [172]\n11.66\n65.5\n-\n-\n-\n-\nUIO-2L\n16.68\n74.3\n2.82\n5.37\n1.93\n50.2\nUIO-2XL\n14.11\n80.0\n2.59\n5.11\n1.74\n54.2\nUIO-2XXL\n13.39\n81.3\n2.64\n5.89\n1.80\n56.3\nTable 4. Results on text-to-image generation (MS COCO [115]\nand TIFA [76]), text-to-audio generation (AudioCaps [93]) and ac-\ntion generation (VIMA-Bench [87]).\nstill obtains strong referring expression scores that compare\nfavorably to prior work on generalist multimodal models,\nsee Table 5. Surpassing UNIFIED-IO while also support-\ning much higher quality image and text generation, along\nwith many more tasks and modalities, illustrates the impres-\nsive multi-tasking capabilities of our model. UNIFIED-IO 2\neven maintains better overall performance with the 3-billion\nparameter model (65.2 vs. 64.5), which is roughly equal in\nsize to UNIFIED-IO. Ablation results show average perfor-\nmance, and all individual tasks improve with model size,\nshowing that UNIFIED-IO 2 benefits from scale.\n5.3. Generation Results\nTable 4 shows results on tasks that require generating\nimage, audio, and action outputs.\nWe evaluate using\nTIFA [76], which measures faithfulness to the prompt us-\ning VQA models and has been shown to correlate well\nwith human judgments, and FID [73] on MS COCO [115].\nOn TIFA, we find that UNIFIED-IO 2 scores close to\nminDALL-E [37], and about 10 points ahead of other gen-\neralist models such as CoDi [174] and Emu [172]. We at-\ntribute this strong image generation ability to extensive pre-\ntraining and the use of a fine-grained VQ-GAN. We include\nexamples of our generation results from the TIFA bench-\nmark in the Appendix E.5. UNIFIED-IO 2\u2019s FID scores are\nslightly higher than the compared models, although we note\nthat qualitatively the generated images are still very smooth\nand detailed.\nFor text-to-audio generation, we evaluate on the Audio-\nCaps [93] test set. AudioCaps consists of 10-second audio\nclips, while our model can generate 4.08-second audio at\na time, so we cannot do a direct evaluation on this bench-\nmark. Instead, we generate an audio segment based on the\ntext description and previous audio segments as additional\n9\nMethod\nVQAv2\nOKVQA\nSQA\nSQAI\nTally-QA RefCOCO RefCOCO+ RefCOCO-g COCO-Cap. POPE SEED MMB\nInstructBLIP (8.2B)\n-\n-\n-\n79.5\n68.2\u2020\n-\n-\n-\n102.2\n-\n53.4\n36\nShikra (7.2B)\n77.4\n47.2\n-\n-\n-\n87.0\n81.6\n82.3\n117.5\n84.7\n-\n58.8\nFerret (7.2B)\n-\n-\n-\n-\n-\n87.5\n80.8\n83.9\n-\n85.8\n-\n-\nQwen-VL (9.6B)\n78.8\n58.6\n-\n67.1\u2217\n-\n89.4\n83.1\n85.6\n131.9\n-\n38.2\nmPLUG-Owl2 (8.2B)\n79.4\n57.7\n-\n68.7\u2217\n-\n-\n-\n-\n137.3\n86.2\n57.8\n64.5\nLLaVa-1.5 (7.2B)\n78.5\n-\n-\n66.8\u2217\n-\n-\n-\n-\n-\n85.9\n58.6\n64.3\nLLaVa-1.5 (13B)\n80.0\n-\n-\n71.6\u2217\n72.4\u2020\n-\n-\n-\n-\n85.9\n61.6\n67.7\nSingle Task SoTA\n86.0 [29] 66.8 [77] 90.9 [119] 90.7 [34] 82.4 [77] 92.64 [202] 88.77 [187] 89.22 [187]\n149.1 [29]\n-\n-\n-\nUIO-2L (1.1B)\n75.3\n50.2\n81.6\n78.6\n69.1\n84.1\n71.7\n79.0\u2662\n128.2\n77.8\n51.1\n62.1\nUIO-2XL (3.2B)\n78.1\n53.7\n88.8\n87.4\n72.2\n88.2\n79.8\n84.0\u2662\n130.3\n87.2\n60.2\n68.1\nUIO-2XXL (6.8B)\n79.4\n55.5\n88.7\n86.2\n75.9\n90.7\n83.1\n86.6\u2662\n125.4\n87.7\n61.8\n71.5\nTable 5. Vision-language results on nine tasks [1, 28, 59, 91, 124, 129, 130, 136, 209] and three evaluation-only benchmarks [106, 113, 120].\nResults marked with \u2217 are zero-shot and \u2020 are evaluated with the open-source releases, and \u2662 indicates that our RefCOCO-g results are on\nthe Google split rather than the UMD split.\ninput; see Appendix E.6 for more details. While this is not\na directly comparable setup to related work, it still gives\na reasonable quantitative measure of our audio generation\nabilities. UNIFIED-IO 2 scores higher then specialist mod-\nels except the recent latent diffusion model [117], which\nshows it\u2019s competitive audio generation ability.\nFor action, we evaluate using VIMA-Bench [87], a robot\nmanipulation benchmark containing 17 tasks with text-\nimage interleaved prompts. Since VIMA\u2019s action space is\naction primitives, UNIFIED-IO 2 directly predicts all ac-\ntions at once given the initial observation and multimodal\nprompt. We report the average success rate for 4-level eval-\nuation protocol [87] and compare with the original casual\nVIMA policy with object-centric inputs, as well as VIMA-\nIMG, a Gato [152]-like policy with image inputs like ours.\n5.4. Vision Language Results\nWe evaluate vision language performance and compare it\nagainst other vision/language generalist models, i.e., mod-\nels that are also designed to perform many tasks and can\nfollow instructions.\nResults on a collection of 12 vi-\nsion/language benchmarks are shown in Table 5. SoTA re-\nsults from specialist models are shown for reference.\nUNIFIED-IO 2 achieves strong results on VQA, only\npassed by the much larger 13B LLaVa model [118] on VQA\nv2 [59], and ahead of all other generalist models on Sci-\nenceQA [124] and TallyQA [1]. OK-VQA [130] is the ex-\nception. We hypothesize that because it requires external\nknowledge, extensive language pre-training is important for\nthis task, and therefore our reduced performance is since\nUNIFIED-IO 2 was not pre-trained as extensively on text as\nthe dedicated language models used by Qwen-VL [12] and\nmPLUG-Owl2 [206].\nOn referring expression, UNIFIED-IO 2 is ahead of\nShikra [26] and Ferret [207] and matches the scores\nachieved by Qwen-VL. On captioning, UNIFIED-IO 2 also\nVideo\nAudio\nMethod\nKinetics-400 [90]\nVATEXCaption [190]\nMSR-VTT [199]\nMSRVTT-QA [198]\nMSVD-QA [198]\nSTAR [196]\nSEED-T [106]\nVGG-Sound [24]\nAudioCaps [93]\nKinetics-Sounds [7]\nMBT [137]\n-\n-\n-\n-\n-\n-\n-\n52.3\n-\n85.0\nCoDi [174]\n-\n-\n74.4\n-\n-\n-\n-\n-\n78.9\n-\nImageBind [69]\u2217\n50.0\n-\n-\n-\n-\n-\n-\n27.8\n-\n-\nBLIP-2 [109]\u2217\n-\n-\n-\n9.2\n18.3\n-\n36.7\n-\n-\n-\nInstructBLIP [34]\u2217\n-\n-\n-\n22.1 41.8\n-\n38.3\n-\n-\n-\nEmu [172]\u2217\u2217\n-\n-\n-\n24.1 39.8\n-\n-\n-\n-\n-\nFlamingo-9B [5]\u2217\u2217\n-\n57.4\n-\n29.4 47.2 41.2\n-\n-\nFlamingo-80B [5]\n-\n84.2\n-\n47.4\n-\n-\n-\n-\n-\n-\nUIO-2L\n68.5 37.1 44.0 39.6 48.2 51.0 37.5 37.8 45.7 86.1\nUIO-2XL\n71.4 41.6 47.1 39.3 50.4 52.0 45.6 44.2 45.7 88.0\nUIO-2XXL\n73.8 45.6 48.8 41.5 52.1 52.2 46.8 47.7 48.9 89.3\nTable 6. Results on action classification, video captioning, VQA,\nvisual comprehension, audio classification, and audio captioning.\n\u2217: zero-shot, \u2217\u2217: few-shot in-context learning.\nachieves a strong CIDEr score [182] of 130.3, ahead of\nShikra and InstructBLIP [34] but behind Qwen-VL and\nmPLUG-Owl2.\nFinally, we evaluate using three recently proposed\nevaluation-only benchmarks.\nMMB (MMBench [120])\ntests multiple facets of vision language understanding with\nmultiple choice questions, while SEED-Bench additionally\ntests video understanding. We show a detailed breakdown\nof our score in the Appendix E.4.\nRegarding the over-\nall score, UNIFIED-IO 2 has the strongest score of any\n7B model on the SEED-Bench leaderboard1, and scores\nthe highest on MMB by 3.8 points.\nNotably, it excels\nLLaVa-1.5 13B model in both benchmarks. UNIFIED-IO 2\n1as of 11/17/23\n10\nAP3D AP3D@15 AP3D@25 AP3D@50\nCube-RCNN [16]\n50.8\n65.7\n54.0\n22.5\nUIO-2L\n42.9\n54.4\n45.7\n21.7\nUIO-2XL\n43.3\n54.4\n46.8\n21.8\nUIO-2XXL\n42.4\n54.0\n45.6\n20.9\nTable 7. Single-object 3D detection results on Objectron [3].\nalso reaches 87.7 on the POPE object hallucination bench-\nmark [113], showing that it is not very prone to object hal-\nlucination.\nOverall, UNIFIED-IO 2 can match or surpass other vi-\nsion & language generalist models on these benchmarks\ndespite encompassing many more modalities and support-\ning high-quality image and audio generation. This shows\nthat its wide breadth of capabilities does not come at the\nexpense of vision/language performance.\n5.5. Video, Audio and other Results\nUNIFIED-IO 2 shows reasonable performance on au-\ndio and video classification and captioning, as well as\nvideo question answering, as shown in Table 6.\nNo-\ntably, UNIFIED-IO 2 outperforms BLIP-2 [109] and In-\nstructBLIP [34] on Seed-Bench Temporal [106] by 8.5\npoints.\nUNIFIED-IO 2 also achieves better performance\non Kinetics-Sounds [7] than MBT [137], which is trained\nsolely on that dataset.\nWe show the single-object 3D detection results in Ta-\nble 7. Our model shows decent results, similar to Cube-\nRCNN [16], on the Objectron benchmark [3]. However, its\nperformance drops significantly in multi-object 3D detec-\ntion tasks, like those on nuScenes [22] and Hypersim [153].\nThis could be because only 1.0% of our training data fo-\ncuses on 3D detection. A potential solution might be to\ncombine 2D and 3D detection techniques.\nIn COCO object detection, excluding the \u2018stuff\u2019 cate-\ngories, our model reached an average precision (AP) of\n47.2, with AP50 at 57.7 and AP75 at 50.0. However, it\nhas difficulties with images containing many objects. Previ-\nous research, like Pix2Seq [27], suggests that autoregressive\nmodels face similar challenges, which can be improved with\nextensive data augmentation. Our model\u2019s data augmenta-\ntion on object detection is comparatively more limited.\nOur model shows weak performance in depth estimation,\nwith an RMSE of 0.623 on NYUv2 depth dataset [138].\nHowever, fine-tuning specifically for this task improved\nthe RMSE to 0.423. In our experiment, we simply nor-\nmalize the depth map with the max depth value in each\ndataset. Due to the incompatibility of dense ground-truth\ndepth across different datasets [150], our model failed to\ncapture the exact scale in the current prompt, which could\npotentially be solved by using better normalization and met-\nric evaluation.\nAppendix E shows qualitative visualizations of other\ntasks, such as single object tracking, future state prediction\nof robotic manipulation, and image-based 3D view synthe-\nsis, etc.\n6. Limitation\n\u2022 Due to memory constraints, we use the base versions of\nthe ViT and AST models for image and audio features\nthroughout the project. Using a larger version of these im-\nage and audio encoders could substantially improve per-\nformance.\n\u2022 While our image generation is more faithful compared to\nSD-based methods, its quality doesn\u2019t match that of the\nstable diffusion model. Additionally, our audio genera-\ntion is capped at approximately 4 seconds, which restricts\nthe practical application of the audio outputs.\n\u2022 Limited computational resources constrained our explo-\nration of the model\u2019s hyperparameters.\nIt\u2019s likely that\nusing a significantly larger batch size could enhance the\nmodel\u2019s performance.\n\u2022 Our model is much less reliable for modalities like depth,\nvideo or when requiring more niche abilities like 3D ob-\nject detection, etc. This is probably due to the limited\nvariety of tasks we have in these areas.\n\u2022 Improving the quality of our data could enhance the\nmodel\u2019s performance. However, despite considerable ef-\nforts, our human-written prompts still fall short in diver-\nsity. We notice a notable decrease in the model\u2019s perfor-\nmance when dealing with new instruction tasks, as op-\nposed to those it was trained on.\n7. Conclusion\nWe introduced UNIFIED-IO 2, the first autoregressive mul-\ntimodal model that is capable of understanding and generat-\ning image, text, audio, and action. This model was trained\nfrom scratch on a wide range of multimodal data and fur-\nther refined with instruction tuning on a massive multimodal\ncorpus. We developed various architectural changes to sta-\nbilize the multimodal training and proposed a multimodal\nmixture of denoiser objective to effectively utilize the multi-\nmodal signals. Our model achieves promising results across\na wide range of tasks. We show that going from LLMs to\nLMMs enables new capabilities and opportunities. In the\nfuture, we would like to extend UNIFIED-IO 2 from the\nencoder-decoder model to a decoder-only model. Addition-\nally, we plan to expand the model\u2019s size, enhance the data\nquality, and refine the overall model design.\nAcknowledgement We thank Klemen Kotar for helping gather Embodied\nAI pre-training data, Jonathan Frankle from MosaicML for suggesting the\nmixture of NLP pre-training data, Jack Hessel for interleaved image &\ntext dataset and Micheal Schmitz for helping support the compute infras-\ntructure. We also thank Tanmay Gupta for helpful discussions, as well\nas Hamish Ivison, and Ananya Harsh Jha for their insightful discussions\n11\nabout model design. We additionally thank Oscar Michel, Yushi Hu and\nYanbei Chen for their help editing the paper, and Matt Deitke for help\nsetting up the webpage. Savya Khosla and Derek Hoiem were supported\nin part by ONR award N00014-23-1-2383.\nThis research was made\npossible with cloud TPUs from Google\u2019s TPU Research Cloud (TRC).\nReferences\n[1] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tal-\nlyQA: Answering Complex Counting Questions. In AAAI,\n2019. 10, 29\n[2] Andrea Agostinelli, Timo I Denk, Zal\u00b4an Borsos, Jesse En-\ngel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,\nAren Jansen, Adam Roberts, Marco Tagliasacchi, et al.\nMusicLM: Generating Music From Text.\narXiv preprint\narXiv:2301.11325, 2023. 7, 29\n[3] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski,\nJianing Wei, and Matthias Grundmann. Objectron: A Large\nScale Dataset of Object-Centric Videos in the Wild with\nPose Annotations. In CVPR, 2021. 11, 37, 38\n[4] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan,\nand Barlas Oguz.\nJointly Training Large Autoregressive\nMultimodal Models.\narXiv preprint arXiv:2309.15564,\n2023. 3\n[5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a Visual Language Model for Few-Shot Learn-\ning. In NeurIPS, 2022. 2, 4, 10, 22, 23, 37\n[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi\nParikh. VQA: Visual Question Answering. In ICCV, 2015.\n7\n[7] Relja Arandjelovic and Andrew Zisserman. Look, Listen\nand Learn. In ICCV, 2017. 8, 10, 11, 30\n[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sutton.\nProgram Synthesis with Large Language Models.\narXiv\npreprint arXiv:2108.07732, 2021. 27\n[9] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Shiori Sagawa, et al.\nOpen-\nFlamingo: An Open-Source Framework for Training Large\nAutoregressive Vision-Language Models.\narXiv preprint\narXiv:2308.01390, 2023. 8, 9, 23\n[10] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer Normalization. In NeurIPS Deep Learning Sympo-\nsium, 2016. 5\n[11] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Esti-\nmating and Exploiting the Aleatoric Uncertainty in Surface\nNormal Estimation. In ICCV, 2021. 33\n[12] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-VL: A Versatile Vision-Language Model for\nUnderstanding, Localization, Text Reading, and Beyond.\narXiv preprint arXiv:2308.12966, 2023. 3, 10, 34\n[13] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in Time: A Joint Video and Image Encoder\nfor End-to-End Retrieval. In ICCV, 2021. 7, 26\n[14] Zal\u00b4an Borsos, Rapha\u00a8el Marinier, Damien Vincent, Eugene\nKharitonov, Olivier Pietquin, Matt Sharifi, Dominik Rob-\nlek, Olivier Teboul, David Grangier, Marco Tagliasacchi,\net al. AudioLM: A Language Modeling Approach to Au-\ndio Generation. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 31:2523\u20132533, 2023. 2\n[15] James\nBradbury,\nRoy\nFrostig,\nPeter\nHawkins,\nMatthew James Johnson, Chris Leary, Dougal Maclaurin,\nGeorge Necula, Adam Paszke, Jake VanderPlas, Skye\nWanderman-Milne, and Qiao Zhang.\nJax: composable\ntransformations of python+numpy programs, 2018. 23\n[16] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila\nRavi, Justin Johnson, and Georgia Gkioxari. Omni3D: A\nLarge Benchmark and Model for 3D Object Detection in\nthe Wild. In CVPR, 2023. 3, 8, 11, 21, 29\n[17] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. RT-2:\nVision-Language-Action Models Transfer Web Knowledge\nto Robotic Control. In CoRL, 2023. 3\n[18] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructPix2Pix: Learning to Follow Image Editing Instruc-\ntions. In CVPR, 2023. 7, 29\n[19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage Models are Few-Shot Learners.\nIn NeurIPS,\n2020. 5, 37\n[20] S\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\nSparks\nof Artificial General Intelligence: Early experiments with\nGPT-4. arXiv preprint arXiv:2303.12712, 2023. 3\n[21] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and\nMichael J Black. A Naturalistic Open Source Movie for\nOptical Flow Evaluation. In ECCV, 2012. 8, 30\n[22] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuScenes: A mul-\ntimodal dataset for autonomous driving. In CVPR, 2020.\n11\n[23] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12M: Pushing Web-Scale Image-Text\nPre-Training To Recognize Long-Tail Visual Concepts. In\nCVPR, 2021. 6, 26\n[24] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew\nZisserman.\nVGGSound:\nA Large-Scale Audio-Visual\nDataset. In ICASSP, 2020. 8, 10, 30\n[25] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu,\nPengchuan Zhang,\nRaghuraman Krishnamoorthi,\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\nMiniGPT-v2: Large Language Model As a Unified In-\nterface for Vision-Language Multi-task Learning.\narXiv\npreprint arXiv:2310.09478, 2023. 3\n12\n[26] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing Multi-\nmodal LLM\u2019s Referential Dialogue Magic. arXiv preprint\narXiv:2306.15195, 2023. 3, 10, 34\n[27] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-\noffrey Hinton. Pix2seq: A Language Modeling Framework\nfor Object Detection. In ICLR, 2022. 3, 11\n[28] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Doll\u00b4ar, and C. Lawrence\nZitnick. Microsoft COCO Captions: Data Collection and\nEvaluation Server. arXiv preprint arXiv:1504.00325, 2015.\n10\n[29] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al.\nPaLI-X:\nOn Scaling up a Multilingual Vision and Language Model.\narXiv preprint arXiv:2305.18565, 2023. 10\n[30] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom\nKwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolQ: Exploring the Surprising Difficulty of Natural\nYes/No Questions. In NAACL-HLT, 2019. 33\n[31] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\nThink you have Solved Question Answering?\nTry\nARC, the AI2 Reasoning Challenge.\narXiv preprint\narXiv:1803.05457, 2018. 33\n[32] Together Computer.\nRedPajama:\nan Open Dataset\nfor Training Large Language Models.\nhttps : / /\ngithub.com/togethercomputer/RedPajama-\nData, 2023. 6, 26\n[33] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei\nZaharia, and Reynold Xin.\nFree Dolly:\nIntroducing\nthe World\u2019s First Truly Open Instruction-Tuned LLM.\nhttps://www.databricks.com/blog/2023/\n04 / 12 / dolly - first - open - commercially -\nviable-instruction-tuned-llm, 2023. 7, 27\n[34] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructBLIP: Towards General-\npurpose Vision-Language Models with Instruction Tuning.\nIn NeurIPS, 2023. 3, 10, 11, 34\n[35] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\nAntonino Furnari, Jian Ma, Evangelos Kazakos, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and\nMichael Wray. Rescaling Egocentric Vision: Collection,\nPipeline and Challenges for EPIC-KITCHENS-100. Inter-\nnational Journal of Computer Vision, 130:33\u201355, 2022. 8,\n30\n[36] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos\u00b4e MF Moura, Devi Parikh, and Dhruv\nBatra. Visual Dialog. In CVPR, 2017. 29\n[37] Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah,\nTanishq Abraham, Ph\u00b4uc L\u02c6e Khac, Luke Melas, and Rito-\nbrata Ghosh. DALL\u00b7E Mini, 2021. 9, 34\n[38] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling Vision Transformers to 22 Billion\nParameters. In ICML, 2023. 5\n[39] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,\nKiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,\nAniruddha Kembhavi, and Roozbeh Mottaghi. ProcTHOR:\nLarge-Scale Embodied AI Using Procedural Generation. In\nNeurIPS, 2022. 7, 25, 26\n[40] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA Universe of Annotated 3D Objects. In CVPR, 2023. 7, 8,\n25, 26, 29, 37, 38\n[41] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and\nLi Fei-Fei. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR, 2009. 7, 22\n[42] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-\nson. RedCaps: Web-curated image-text data created by the\npeople, for the people. In NeurIPS Datasets and Bench-\nmarks Track, 2021. 6, 26\n[43] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. CogView: Mastering Text-to-Image\nGeneration via Transformers. In NeurIPS, 2021. 36\n[44] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. FlowNet:\nLearning Optical Flow with Convolutional Networks. In\nICCV, 2015. 8, 30\n[45] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al.\nAn Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale.\nIn ICLR,\n2021. 22\n[46] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-\nE: An Embodied Multimodal Language Model. In ICML,\n2023. 2, 32\n[47] Konstantinos Drossos, Samuel Lipping, and Tuomas Virta-\nnen. Clotho: An Audio Captioning Dataset. In ICASSP,\n2020. 7, 8, 29\n[48] Gerald M Edelman. Neural Darwinism: Selection and reen-\ntrant signaling in higher brain function. Neuron, 10(2):115\u2013\n125, 1993. 2\n[49] Patrick Esser, Robin Rombach, and Bjorn Ommer. Tam-\ning Transformers for High-Resolution Image Synthesis. In\nCVPR, 2021. 3, 22\n[50] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge\nDeng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu,\net al. LaSOT: A High-quality Large-scale Single Object\nTracking Benchmark. International Journal of Computer\nVision, 129:439\u2013461, 2021. 8, 30, 37\n[51] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An-\nthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\nHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang,\n13\nLaria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin\nWang, and Andy Zou. A framework for few-shot language\nmodel evaluation, 2021. 33\n[52] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-\njie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, et al.\nLLaMA-Adapter V2:\nParameter-\nEfficient Visual Instruction Model.\narXiv preprint\narXiv:2304.15010, 2023. 3\n[53] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan. Planting a SEED of Vision in Large Language Model.\narXiv preprint arXiv:2307.08041, 2023. 3\n[54] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman,\nAren Jansen, Wade Lawrence, R. Channing Moore, Manoj\nPlakal, and Marvin Ritter. Audio Set: An Ontology and\nHuman-Labeled Dataset for Audio Events.\nIn ICASSP,\n2017. 7, 8, 22, 26, 30\n[55] Xinyang Geng and Hao Liu.\nOpenLLaMA: An Open\nReproduction of LLaMA.\nhttps://github.com/\nopenlm-research/open_llama, 2023. 9\n[56] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat\nSingh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan\nMisra. ImageBind: One Embedding Space To Bind Them\nAll. In CVPR, 2023. 3\n[57] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio\nSpectrogram Transformer. In Interspeech, 2021. 4, 22\n[58] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\nMueller-Freitag, et al. The\u201d something something\u201d video\ndatabase for learning and evaluating visual common sense.\nIn ICCV, 2017. 30\n[59] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. Making the V in VQA Matter: Ele-\nvating the Role of Image Understanding in Visual Question\nAnswering. In CVPR, 2017. 10\n[60] Kristen Grauman,\nAndrew Westbury,\nEugene Byrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-\nson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.\nEgo4D: Around the World in 3,000 Hours of Egocentric\nVideo. In CVPR, 2022. 7, 26\n[61] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Car-\noline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,\nGeorge Toderici, Susanna Ricco, Rahul Sukthankar, et al.\nAVA: A Video Dataset of Spatio-temporally Localized\nAtomic Visual Actions. In CVPR, 2018. 8, 30\n[62] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A\nDataset for Large Vocabulary Instance Segmentation. In\nCVPR, 2019. 29\n[63] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey\nLevine, and Karol Hausman. Relay Policy Learning: Solv-\ning Long-Horizon Tasks via Imitation and Reinforcement\nLearning. In CoRL, 2019. 8, 31\n[64] Tanmay Gupta and Aniruddha Kembhavi. Visual Program-\nming: Compositional visual reasoning without training. In\nCVPR, 2023. 2\n[65] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and\nDerek Hoiem. Towards General Purpose Vision Systems:\nAn End-to-End Task-Agnostic Vision-Language Architec-\nture. In CVPR, 2022. 33\n[66] Tanmay Gupta, Ryan Marten, Aniruddha Kembhavi, and\nDerek Hoiem. GRIT: General Robust Image Task Bench-\nmark. arXiv preprint arXiv:2204.13653, 2022. 2, 9, 22\n[67] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizWiz Grand Challenge:\nAnswering Visual Questions\nfrom Blind People. In CVPR, 2018. 29\n[68] Ivan Habernal,\nOmnia Zayed,\nand Iryna Gurevych.\nC4Corpus: Multilingual Web-size Corpus with Free Li-\ncense. In LREC, 2016. 6, 26\n[69] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng\nXu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu\nGuo, et al. ImageBind-LLM: Multi-modality Instruction\nTuning. arXiv preprint arXiv:2309.03905, 2023. 3, 10\n[70] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017. 33\n[71] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked Autoencoders Are Scal-\nable Vision Learners. In CVPR, 2022. 24\n[72] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-\nsuring Massive Multitask Language Understanding.\nIn\nICLR, 2021. 33\n[73] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs Trained by\na Two Time-Scale Update Rule Converge to a Local Nash\nEquilibrium. In NeurIPS, 2017. 9, 36\n[74] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion\nGuidance. In NeurIPS Workshop on Deep Generative Mod-\nels and Downstream Applications, 2021. 29, 34, 36\n[75] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\nChoi. The Curious Case of Neural Text Degeneration. In\nICLR, 2020. 29\n[76] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari\nOstendorf, Ranjay Krishna, and Noah A. Smith. TIFA: Ac-\ncurate and Interpretable Text-to-Image Faithfulness Evalu-\nation with Question Answering. In ICCV, 2023. 2, 8, 9, 34,\n35\n[77] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy\nViswanathan, Kenji Hata, Enming Luo, Ranjay Krishna,\nand Ariel Fuxman. Visual Program Distillation: Distilling\nTools and Programmatic Reasoning into Vision-Language\nModels. arXiv preprint arXiv:2312.03052, 2023. 10\n[78] Jingwei Huang, Yichao Zhou, Thomas Funkhouser, and\nLeonidas J Guibas. FrameNet: Learning Local Canonical\nFrames of 3D Surfaces from a Single RGB Image. In ICCV,\n2019. 8, 30\n[79] Lianghua Huang, Xin Zhao, and Kaiqi Huang. GOT-10k:\nA Large High-Diversity Benchmark for Generic Object\nTracking in the Wild. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 43(5):1562\u20131577, 2019. 8,\n30\n[80] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi,\nXuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,\nJiawei Huang, Jinglin Liu, et al. AudioGPT: Understanding\n14\nand Generating Speech, Music, Sound, and Talking Head.\narXiv preprint arXiv:2304.12995, 2023. 3\n[81] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al. Language Is Not\nAll You Need: Aligning Perception with Language Models.\nIn NeurIPS, 2023. 2, 3\n[82] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh,\nIshan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Gir-\nshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al.\nVisual Storytelling. In NAACL-HLT, 2016. 29\n[83] Drew A Hudson and Christopher D Manning.\nGQA: A\nNew Dataset for Real-World Visual Reasoning and Com-\npositional Question Answering. In CVPR, 2019. 29\n[84] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nCLIP, 2021. 3\n[85] Keith\nIto\nand\nLinda\nJohnson.\nThe\nLJ\nSpeech\nDataset.\nhttps://keithito.com/LJ-Speech-\nDataset/, 2017. 7, 22, 29\n[86] Andrew\nJaegle,\nSebastian\nBorgeaud,\nJean-Baptiste\nAlayrac, Carl Doersch, Catalin Ionescu, David Ding,\nSkanda Koppula, Daniel Zoran, Andrew Brock, Evan\nShelhamer, et al. Perceiver IO: A General Architecture for\nStructured Inputs & Outputs. In ICLR, 2022. 5\n[87] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang,\nYongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anand-\nkumar, Yuke Zhu, and Linxi Fan. VIMA: General Robot\nManipulation with Multimodal Prompts. In ICML, 2023.\n8, 9, 10, 21, 30, 37\n[88] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan,\nBin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang\nLei, et al. Unified Language-Vision Pretraining in LLM\nwith Dynamic Discrete Visual Tokenization. arXiv preprint\narXiv:2309.04669, 2023. 3\n[89] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric\nKolve, Derek Hoiem, and Aniruddha Kembhavi.\nWebly\nSupervised Concept Expansion for General Purpose Vision\nModels. In ECCV, 2022. 9, 33\n[90] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Vi-\nola, Tim Green, Trevor Back, Paul Natsev, et al.\nThe\nKinetics Human Action Video Dataset.\narXiv preprint\narXiv:1705.06950, 2017. 10, 36\n[91] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg. ReferItGame: Referring to Objects in Pho-\ntographs of Natural Scenes. In EMNLP, 2014. 8, 10, 29,\n30\n[92] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and\nMatthew Sharifi. Fr\u00b4echet Audio Distance: A Reference-\nFree Metric for Evaluating Music Enhancement Algo-\nrithms. In Interspeech, 2019. 36\n[93] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and\nGunhee Kim. AudioCaps: Generating Captions for Audios\nin The Wild. In NAACL-HLT, 2019. 7, 8, 9, 10, 29, 36\n[94] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar,\nand Ross Girshick. Segment Anything. In ICCV, 2023. 7,\n26, 33\n[95] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,\nChenghao Mou, Carlos Mu\u02dcnoz Ferrandis, Yacine Jernite,\nMargaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry\nBahdanau, Leandro von Werra, and Harm de Vries. The\nStack: 3 TB of permissively licensed source code. Trans-\nactions on Machine Learning Research, 2023. 26\n[96] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating Images with Multimodal Language Models.\nIn\nNeurIPS, 2023. 2, 3\n[97] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding Language Models to Images for Multimodal In-\nputs and Outputs. In ICML, 2023. 3\n[98] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.\nHiFi-\nGAN: Generative Adversarial Networks for Efficient and\nHigh Fidelity Speech Synthesis. In NeurIPS, 2020. 22\n[99] Andreas K\u00a8opf, Yannic Kilcher, Dimitri von R\u00a8utte, Sotiris\nAnagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah\nBarhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00b4ard\nNagyfi, et al.\nOpenAssistant Conversations \u2013 Democra-\ntizing Large Language Model Alignment.\nIn NeurIPS\nDatasets and Benchmarks Track, 2023. 27\n[100] Mario Michael Krell, Matej Kosec, Sergio P Perez, and\nAndrew Fitzgibbon.\nEfficient Sequence Packing with-\nout Cross-contamination:\nAccelerating Large Language\nModels without Impacting Performance.\narXiv preprint\narXiv:2107.02027, 2021. 6\n[101] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,\nAlexandre D\u00b4efossez, Jade Copet, Devi Parikh, Yaniv Taig-\nman, and Yossi Adi. AudioGen: Textually Guided Audio\nGeneration. In ICLR, 2023. 9\n[102] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein,\nand Fei-Fei Li. Visual Genome: Connecting Language and\nVision Using Crowdsourced Dense Image Annotations. In-\nternational Journal of Computer Vision, 2017. 7, 27\n[103] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\nfan Popov, Matteo Malloci, Alexander Kolesnikov, Tom\nDuerig, and Vittorio Ferrari. The Open Images Dataset V4:\nUnified image classification, object detection, and visual re-\nlationship detection at scale. International Journal of Com-\nputer Vision, 128(7):1956\u20131981, 2020. 7, 22, 27\n[104] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M Rush, Douwe Kiela,\net al. OBELICS: An Open Web-Scale Filtered Dataset of\nInterleaved Image-Text Documents. In NeurIPS Datasets\nand Benchmarks Track, 2023. 6, 24, 26\n[105] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim,\nThomas Breuel, Gal Chechik, and Yale Song. ACAV100M:\nAutomatic Curation of Large-Scale Datasets for Audio-\n15\nVisual Video Representation Learning. In ICCV, 2021. 7,\n22, 26\n[106] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan.\nSEED-Bench: Benchmarking\nMultimodal LLMs with Generative Comprehension. arXiv\npreprint arXiv:2307.16125, 2023. 8, 9, 10, 11, 34\n[107] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nFanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.\nMIMIC-IT: Multi-Modal In-Context Instruction Tuning.\narXiv preprint arXiv:2306.05425, 2023. 8, 29\n[108] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter: A Multi-Modal\nModel with In-Context Instruction Tuning. arXiv preprint\narXiv:2305.03726, 2023. 29, 34\n[109] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-Image Pre-training with\nFrozen Image Encoders and Large Language Models. In\nICML, 2023. 3, 10, 11\n[110] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideoChat:\nChat-Centric Video Understanding.\narXiv\npreprint arXiv:2305.06355, 2023. 3, 34\n[111] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\nLimin Wang, and Yu Qiao. UniFormerV2: Unlocking the\nPotential of Image ViTs for Video Understanding. In ICCV,\n2023. 8, 30, 37\n[112] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,\nShuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu\nSun, et al. M3IT: A Large-Scale Dataset towards Multi-\nModal Multilingual Instruction Tuning.\narXiv preprint\narXiv:2306.04387, 2023. 3, 8, 29\n[113] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji rong Wen. Evaluating Object Hallucination in\nLarge Vision-Language Models. In EMNLP, 2023. 10, 11,\n34\n[114] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw,\nYudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe\nMorency, and Russ Salakhutdinov.\nHigh-Modality Mul-\ntimodal Transformer: Quantifying Modality & Interaction\nHeterogeneity for High-Modality Representation Learning.\nTMLR, 2023. 3\n[115] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft COCO: Common Objects\nin Context. In ECCV, 2014. 7, 8, 9, 27, 34\n[116] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual Spa-\ntial Reasoning. Transactions of the Association for Compu-\ntational Linguistics, 11:635\u2013651, 2023. 29\n[117] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,\nDanilo Mandic, Wenwu Wang, and Mark D Plumbley. Au-\ndioLDM: Text-to-Audio Generation with Latent Diffusion\nModels. In ICML, 2023. 9, 10, 36\n[118] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved Baselines with Visual Instruction Tuning. arXiv\npreprint arXiv:2310.03744, 2023. 3, 10, 34\n[119] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual Instruction Tuning. In NeurIPS, 2023. 2, 3, 8,\n10, 29\n[120] Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. MM-\nBench: Is Your Multi-modal Model an All-around Player?\narXiv preprint arXiv:2307.06281, 2023. 10\n[121] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,\net al. Swin Transformer V2: Scaling Up Capacity and Res-\nolution. In CVPR, 2022. 5\n[122] Shayne Longpre,\nLe Hou,\nTu Vu,\nAlbert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Bar-\nret Zoph, Jason Wei, and Adam Roberts. The Flan Collec-\ntion: Designing Data and Methods for Effective Instruction\nTuning. In ICML, 2023. 3, 7, 27\n[123] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. Unified-IO: A Uni-\nfied Model for Vision, Language, and Multi-Modal Tasks.\nIn ICLR, 2023. 2, 3, 4, 7, 9, 22\n[124] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to Explain: Multimodal Reasoning\nvia Thought Chains for Science Question Answering. In\nNeurIPS, 2022. 10, 29\n[125] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei\nChang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.\nChameleon: Plug-and-Play Compositional Reasoning with\nLarge Language Models. In NeurIPS, 2023. 2\n[126] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-\nHui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Val-\nley: Video Assistant with Large Language model Enhanced\nabilitY. arXiv preprint arXiv:2306.07207, 2023. 3\n[127] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli\nDing, James Betker, Robert Baruch, Travis Armstrong, and\nPete Florence. Interactive Language: Talking to Robots in\nReal Time. IEEE Robotics and Automation Letters, 2023.\n8, 31\n[128] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and\nFahad Shahbaz Khan. Video-ChatGPT: Towards Detailed\nVideo Understanding via Large Vision and Language Mod-\nels. arXiv preprint arXiv:2306.05424, 2023. 34\n[129] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan Yuille, and Kevin Murphy. Generation and\nComprehension of Unambiguous Object Descriptions. In\nCVPR, 2016. 10\n[130] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. OK-VQA: A Visual Question Answer-\ning Benchmark Requiring External Knowledge. In CVPR,\n2019. 10, 29\n[131] Irene Martin Morato and Annamaria Mesaros. Diversity\nand Bias in Audio Captioning Datasets. In DCASE, 2021.\n7, 29\n[132] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie\nPavlick. Linearly Mapping from Image to Text Space. In\nICLR, 2023. 3\n[133] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty.\nOCR-VQA: Visual Question An-\nswering by Reading Text in Images. In ICDAR, 2019. 29\n16\n[134] Utkarsh Mishra, Shangjie Xue, Yongxin Chen, and Danfei\nXu. Generative Skill Chaining: Long-Horizon Skill Plan-\nning with Diffusion Models. In CoRL, 2023. 32\n[135] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang,\nMingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and\nPing Luo. EmbodiedGPT: Vision-Language Pre-Training\nvia Embodied Chain of Thought. In NeurIPS, 2023. 3\n[136] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.\nModeling Context Between Objects for Referring Expres-\nsion Understanding. In ECCV, 2016. 10, 29, 30\n[137] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,\nCordelia Schmid, and Chen Sun. Attention Bottlenecks for\nMultimodal Fusion. In NeurIPS, 2021. 10, 11\n[138] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob\nFergus. Indoor Segmentation and Support Inference from\nRGBD Images. In ECCV, 2012. 8, 11, 30\n[139] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDINOv2: Learning Robust Visual Features without Super-\nvision. arXiv preprint arXiv:2304.07193, 2023. 33\n[140] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bew-\nley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant\nRai, Anikait Singh, Anthony Brohan, et al.\nOpen X-\nEmbodiment: Robotic Learning Datasets and RT-X Mod-\nels. In CoRL Workshop TGR, 2023. 3\n[141] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-G: Generating Im-\nages in Context with Multimodal Large Language Models.\narXiv preprint arXiv:2310.02992, 2023. 3\n[142] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley,\nand Jianfeng Gao. Instruction Tuning with GPT-4. arXiv\npreprint arXiv:2304.03277, 2023. 7, 27\n[143] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nMultimodal Large Language Models to the World. arXiv\npreprint arXiv:2306.14824, 2023. 2, 3\n[144] Jean Piaget, Margaret Cook, et al. The Origins of Intel-\nligence in Children. International Universities Press New\nYork, 1952. 2\n[145] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,\nYingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming\nXiong, Silvio Savarese, et al. UniControl: A Unified Diffu-\nsion Model for Controllable Visual Generation In the Wild.\nIn NeurIPS, 2023. 29\n[146] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning Transferable Visual Models From Natural Language\nSupervision. In ICML, 2021. 33, 36\n[147] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J. Liu. Exploring the Limits of Transfer Learning\nwith a Unified Text-to-Text Transformer. JMLR, 21(140):\n1\u201367, 2020. 4, 5, 21\n[148] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-Shot Text-to-Image Generation. In ICML,\n2021. 5\n[149] Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Ab-\nhishek Das.\nHabitat-Web: Learning Embodied Object-\nSearch Strategies from Human Demonstrations at Scale. In\nCVPR, 2022. 25\n[150] Ren\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards Robust Monocular\nDepth Estimation: Mixing Datasets for Zero-Shot Cross-\nDataset Transfer. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 44(3):1623\u20131637, 2020. 11\n[151] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin\nPan, and Vincent Vanhoucke. YouTube-BoundingBoxes: A\nLarge High-Precision Human-Annotated Data Set for Ob-\nject Detection in Video. In CVPR, 2017. 8, 30\n[152] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez\nColmenarejo, Alexander Novikov, Gabriel Barth-Maron,\nMai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin-\ngenberg, et al. A Generalist Agent. Transactions on Ma-\nchine Learning Research, 2022. 3, 10, 37\n[153] Mike Roberts, Jason Ramapuram, Anurag Ranjan, At-\nulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ\nWebb, and Joshua M. Susskind. Hypersim: A Photorealis-\ntic Synthetic Dataset for Holistic Indoor Scene Understand-\ning. In ICCV, 2021. 11\n[154] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-Resolution Image\nSynthesis With Latent Diffusion Models. In CVPR, 2022.\n2, 8, 9, 34\n[155] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved Techniques\nfor Training GANs. In NeurIPS, 2016. 36\n[156] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,\nLintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Ar-\nnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask\nPrompted Training Enables Zero-Shot Task Generalization.\nIn ICLR, 2022. 27\n[157] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A Plat-\nform for Embodied AI Research. In ICCV, 2019. 7, 25, 26,\n31\n[158] Christoph Schuhmann. LAION-AESTHETICS. https:\n//laion.ai/blog/laion-aesthetics/, 2022.\n26\n[159] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-\n400M: Open Dataset of CLIP-Filtered 400 Million Image-\nText Pairs. In NeurIPS Data-Centric AI Workshop, 2021.\n6, 26\n[160] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-OKVQA: A\nBenchmark for Visual Question Answering using World\nKnowledge. In ECCV, 2022. 29\n[161] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-\nral Machine Translation of Rare Words with Subword\nUnits. In ACL, 2016. 3\n17\n[162] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia,\nDebidatta Dwibedi, Keerthana Gopalakrishnan, Christine\nChan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J\nJoshi, et al. RoboVQA: Multimodal Long-Horizon Reason-\ning for Robotics. arXiv preprint arXiv:2311.00899, 2023.\n32\n[163] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual Captions: A Cleaned, Hypernymed,\nImage Alt-text Dataset For Automatic Image Captioning.\nIn ACL, 2018. 6, 26\n[164] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive\nLearning Rates with Sublinear Memory Cost.\nIn ICML,\n2018. 6\n[165] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,\nXinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards\nVQA Models That Can Read. In CVPR, 2019. 29\n[166] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse\nThomason, and Animesh Garg. ProgPrompt: Generating\nSituated Robot Task Plans using Large Language Models.\nICRA, 2023. 2\n[167] Linda Smith and Michael Gasser. The Development of Em-\nbodied Cognition: Six Lessons from Babies. Artificial life,\n11(1-2):13\u201329, 2005. 2\n[168] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUCF101: A Dataset of 101 Human Actions Classes From\nVideos in The Wild. arXiv preprint arXiv:1212.0402, 2012.\n8, 30\n[169] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo\nWen, and Yunfeng Liu. RoFormer: Enhanced Transformer\nwith Rotary Position Embedding. Neurocomputing, 2023.\n4, 23\n[170] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and\nDeng Cai. PandaGPT: One Model To Instruction-Follow\nThem All. arXiv preprint arXiv:2305.16355, 2023. 3\n[171] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Hua-\njun Bai, and Yoav Artzi. A Corpus for Reasoning About\nNatural Language Grounded in Photographs. In ACL, 2019.\n29\n[172] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative Pretraining in Mul-\ntimodality. arXiv preprint arXiv:2307.05222, 2023. 3, 9,\n10, 34\n[173] D\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. ViperGPT:\nVisual Inference via Python Execution for Reasoning. In\nICCV, 2023. 2\n[174] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng,\nand Mohit Bansal. Any-to-Any Generation via Composable\nDiffusion. In NeurIPS, 2023. 2, 3, 9, 10, 34\n[175] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia,\nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri,\nTal Schuster, Steven Zheng, et al. UL2: Unifying Language\nLearning Paradigms. In ICLR, 2023. 5\n[176] MosaicML NLP Team. Introducing MPT-7B: A New Stan-\ndard for Open-Source, Commercially Usable LLMs, 2023.\nAccessed: 2023-05-05. 6, 24\n[177] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. LLaMA: Open and Efficient Foundation Language\nModels. arXiv preprint arXiv:2302.13971, 2023. 3, 9, 23\n[178] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali\nEslami, Oriol Vinyals, Felix Hill, and Zacharias Janssen.\nMultimodal Few-Shot Learning with Frozen Language\nModels. In NeurIPS, 2021. 3\n[179] Aaron\nVan\nDen\nOord,\nOriol\nVinyals,\nand\nKoray\nKavukcuoglu. Neural Discrete Representation Learning. In\nNeurIPS, 2017. 3\n[180] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,\nChen Sun, Alex Shepard, Hartwig Adam, Pietro Perona,\nand Serge Belongie. The iNaturalist Species Classification\nand Detection Dataset. In CVPR, 2018. 29\n[181] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin.\nAttention Is All You Need.\nIn\nNeurIPS, 2017. 3\n[182] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. CIDEr: Consensus-based Image Description Eval-\nuation. In CVPR, 2015. 10, 37\n[183] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas,\nand Serge Belongie.\nCOCO-Text: Dataset and Bench-\nmark for Text Detection and Recognition in Natural Images.\narXiv preprint arXiv:1601.07140, 2016. 8, 29\n[184] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim,\nMax Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-\nEstruch, Quan Vuong, Andre He, et al. BridgeData V2:\nA Dataset for Robot Learning at Scale. In CoRL, 2023. 8,\n31\n[185] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal Segment\nNetworks for Action Recognition in Videos. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 41\n(11):2740\u20132755, 2019. 30\n[186] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. OFA: Unifying Architectures, Tasks,\nand Modalities Through a Simple Sequence-to-Sequence\nLearning Framework. In ICML, 2022. 3, 33\n[187] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xi-\naohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang\nZhou. ONE-PEACE: Exploring One General Representa-\ntion Model Toward Unlimited Modalities. arXiv preprint\narXiv:2305.11172, 2023. 10\n[188] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, and Furu Wei.\nImage as a Foreign Language: BEiT Pretraining for Vision\nand Vision-Language Tasks. In CVPR, 2023. 3\n[189] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou,\nYu Qiao, et al.\nVisionLLM: Large Language Model is\nalso an Open-Ended Decoder for Vision-Centric Tasks. In\nNeurIPS, 2023. 3\n18\n[190] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang.\nVATEX: A Large-\nScale, High-Quality Multilingual Dataset for Video-and-\nLanguage Research. In ICCV, 2019. 8, 10, 30\n[191] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi,\nYeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar,\nArjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik,\nDavid Stap, et al. Super-NaturalInstructions: Generaliza-\ntion via Declarative Instructions on 1600+ NLP Tasks. In\nEMNLP, 2022. 27\n[192] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\nTsvetkov, and Yuan Cao. SimVLM: Simple Visual Lan-\nguage Model Pretraining with Weak Supervision. In ICLR,\n2022. 3\n[193] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le.\nFinetuned Language Models are Zero-Shot\nLearners. In ICLR, 2022. 27\n[194] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Ro-\nmain Br\u00b4egier, Yohann Cabon, Vaibhav Arora, Leonid Ants-\nfeld, Boris Chidlovskii, Gabriela Csurka, and J\u00b4er\u02c6ome Re-\nvaud. CroCo: Self-Supervised Pre-training for 3D Vision\nTasks by Cross-View Completion. In NeurIPS, 2022. 7, 25,\n26, 29\n[195] Peter Welinder, Steve Branson, Takeshi Mita, Catherine\nWah, Florian Schroff, Serge Belongie, and Pietro Perona.\nCaltech-UCSD Birds 200.\nTechnical Report CNS-TR-\n2010-001, California Institute of Technology, 2010. 29\n[196] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum,\nand Chuang Gan. STAR: A Benchmark for Situated Rea-\nsoning in Real-World Videos.\nIn NeurIPS Datasets and\nBenchmarks Track, 2021. 8, 10, 30\n[197] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. SUN Database: Large-scale Scene\nRecognition from Abbey to Zoo. In CVPR, 2010. 29\n[198] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video Question An-\nswering via Gradually Refined Attention over Appearance\nand Motion. In ACM MM, 2017. 8, 10, 30\n[199] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT:\nA Large Video Description Dataset for Bridging Video and\nLanguage. In CVPR, 2016. 8, 10, 30\n[200] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,\nBei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-\nvancing High-Resolution Video-Language Representation\nwith Large-Scale Video Transcriptions. In CVPR, 2022. 7,\n26\n[201] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin\nRaffel. mT5: A Massively Multilingual Pre-trained Text-\nto-Text Transformer. In NAACL-HLT, 2021. 26\n[202] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan,\nPing Luo, and Huchuan Lu. Universal Instance Perception\nas Object Discovery and Retrieval. In CVPR, 2023. 10\n[203] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang,\nChao Weng, Yuexian Zou, and Dong Yu.\nDiffsound:\nDiscrete Diffusion Model for Text-to-sound Generation.\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 31:1720\u20131733, 2023. 9\n[204] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan\nRen, Lei Zhou, Tian Fang, and Long Quan. BlendedMVS:\nA Large-scale Dataset for Generalized Multi-view Stereo\nNetworks. In CVPR, 2020. 8, 30\n[205] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmPLUG-Owl: Modularization Empow-\ners Large Language Models with Multimodality.\narXiv\npreprint arXiv:2304.14178, 2023. 3\n[206] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei\nLiu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\nmPLUG-Owl2: Revolutionizing Multi-modal Large Lan-\nguage Model with Modality Collaboration. arXiv preprint\narXiv:2311.04257, 2023. 10, 34\n[207] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,\nBowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu\nChang, and Yinfei Yang.\nFerret:\nRefer and Ground\nAnything Anywhere at Any Granularity.\narXiv preprint\narXiv:2310.07704, 2023. 3, 10, 34\n[208] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu. Vector-quantized Image Modeling with\nImproved VQGAN. In ICLR, 2022. 4, 22\n[209] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling Context in Referring Expres-\nsions. In ECCV, 2016. 10, 29, 30\n[210] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin\nMuller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, et al. Scaling Autore-\ngressive Multi-Modal Models: Pretraining and Instruction\nTuning. arXiv preprint arXiv:2309.02591, 2023. 3\n[211] Amir R Zamir, Alexander Sax, William Shen, Leonidas J\nGuibas, Jitendra Malik, and Silvio Savarese. Taskonomy:\nDisentangling Task Transfer Learning. In CVPR, 2018. 30\n[212] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and\nChen Change Loy.\nContextual Object Detection with\nMultimodal Large Language Models.\narXiv preprint\narXiv:2305.18279, 2023. 3\n[213] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\nFrom Recognition to Cognition:\nVisual Commonsense\nReasoning. In CVPR, 2019. 29\n[214] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,\nand Yejin Choi. HellaSwag: Can a Machine Really Finish\nYour Sentence? In ACL, 2019. 8, 9, 33\n[215] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yan-\npeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack\nHessel, Ali Farhadi, and Yejin Choi. MERLOT Reserve:\nNeural Script Knowledge through Vision and Language and\nSound. In CVPR, 2022. 7, 22, 23, 26, 29\n[216] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An\nInstruction-tuned Audio-Visual Language Model for Video\nUnderstanding. arXiv preprint arXiv:2306.02858, 2023. 3\n[217] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and\nYu Su. MagicBrush: A Manually Annotated Dataset for\nInstruction-Guided Image Editing.\nIn NeurIPS Datasets\nand Benchmarks Track, 2023. 7, 29\n19\n[218] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu\nQiao.\nLLaMA-Adapter:\nEfficient Fine-tuning of Lan-\nguage Models with Zero-init Attention.\narXiv preprint\narXiv:2303.16199, 2023. 3\n[219] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. GPT4RoI:\nInstruction Tuning Large Language Model on Region-of-\nInterest. arXiv preprint arXiv:2307.03601, 2023. 3\n[220] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun.\nLLaVAR: En-\nhanced Visual Instruction Tuning for Text-Rich Image Un-\nderstanding. arXiv preprint arXiv:2306.17107, 2023. 3,\n8\n[221] Minyi Zhao, Bingjia Li, Jie Wang, Wanqing Li, Wenjing\nZhou, Lan Zhang, Shijie Xuyang, Zhihang Yu, Xinkun Yu,\nGuangze Li, et al.\nTowards Video Text Visual Question\nAnswering: Benchmark and Baseline. In NeurIPS Datasets\nand Benchmarks Track, 2022. 30\n[222] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen,\nShuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. Chat-\nBridge: Bridging Modalities with Large Language Model\nas a Language Catalyst. arXiv preprint arXiv:2305.16103,\n2023. 3\n[223] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh\nPhung. MoVQ: Modulating Quantized Vectors for High-\nFidelity Image Generation. In NeurIPS, 2022. 22\n[224] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba.\nPlaces:\nA 10 Million Image\nDatabase for Scene Recognition.\nIEEE Transactions on\nPattern Analysis and Machine Intelligence, 40(6):1452\u2013\n1464, 2017. 29\n[225] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny.\nMiniGPT-4:\nEnhancing Vision-\nLanguage Understanding with Advanced Large Language\nModels. arXiv preprint arXiv:2304.10592, 2023. 3\n[226] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-\nGAN: Dynamic Memory Generative Adversarial Networks\nfor Text-to-Image Synthesis. In CVPR, 2019. 34\n20\nThe appendix includes the following sections:\n\u2022 Sec A - Contributions\n\u2022 Sec B - Model Implementation Details\n\u2022 Sec C - Pre-training Details\n\u2022 Sec D - Tasks and Instruction Tuning\n\u2022 Sec E - Experiment Details and Additional Results\nA. Contributions\nJiasen Lu, Christopher Clark, Sangho Lee, and Zichen\nZhang collectively contributed to dataset construction,\nprompt development, and conducted numerous exploratory\nexperiments for this project.\nJiasen Lu led and designed the main idea and scope of the\nproject. Developed the majority of the model pipeline \u2013\nimage and audio tokenizer, main architecture, model stabi-\nlization, and training objective. Led and designed the pre-\ntraining and instruction tuning data pipelines. Conducted\nexperiments with various model and data hyperparameters,\noversaw the model training process, and wrote the paper.\nCoordinate with the whole team.\nChristopher Clark co-led and designed the infrastruc-\nture, instruction tuning, and evaluation.\nDeveloped the\ndynamic packing system, modality processing pipeline,\nand classifier-free guidance for image and audio inference.\nAdded the NLP and many V&L datasets, added many syn-\nthetic tasks, and built prompts for instruction-tuning tasks.\nRan the evaluation in \u00a7 5.1 (NLP), 5.2, and 5.4 (detection,\ndepth) and wrote the paper.\nSangho Lee core contribution to the pre-training data\npipeline.\nAdded all large-scale multimodal pretraining\ndatasets, and video and audio instruction tuning datasets.\nDeveloped sample construction pipeline for pre-training.\nHelped with the model implementation \u2013 position encod-\ning, perceiver resamplers, and model stabilization. Ran the\nevaluation in \u00a7 5.1 (audio), 5.3 (audio and image FID), 5.4\n(video and audio understanding) and wrote parts of the pa-\nper.\nZichen Zhang core contribution to the instruction tuning\ndata pipeline. Added many V&L, embodiment, video, au-\ndio, data augmentation, and all instruction tuning datasets.\nBuilt prompts for instruction tuning. Investigated the model\narchitectures and training pipelines and stabilized the train-\ning.\nRan the experiments in \u00a7 5.1 (image TIFA, Seed-\nBench), 5.3 (image TIFA, action), 5.4, wrote parts of the\npaper, developed the model demo and project page.\nSavya Khosla added 3D object detection, optical flow, and\nmulti-point tracking datasets, ran the evaluation of 3D de-\ntection, and initiated the demo.\nRyan Marten added part of video and tracking datasets.\nDerek Hoiem advised on the research direction.\nAniruddha Kembhavi advised on the research direction\nand evaluation, helped manage compute resources and\nwrote the paper.\nB. Model Implementation Details\nIn this section, we present the implementation details of our\nmodel.\nB.1. Detailed of Unified Task Representation\nFirst, we provide details about how different modalities are\nrepresented in our model.\nText representation. The Byte Pair Encoding (BPE) vo-\ncabulary size is 32000. Similar to [147], we add 200 ad-\nditional special tokens to indicated masked spans when de-\nnoising. We further add 10 special tokens that can be used\nto reference the image, audio, and history input in the text.\nTwo special tokens are to indicate the \u27e8Image Input\u27e9 and\n\u27e8Audio Input\u27e9, and 8 special tokens represent individ-\nual elements in the image and audio history inputs, both of\nwhich have a maximum of 4 frames. We use a maximum of\n512 input and output tokens.\nSparse structures representation. We use an additional\n1000 special tokens to represent all continuous values, such\nas points, boxes, camera transformation, and 3D cuboids.\nPoints are represented with [y, x] coordinates and boxes\nwith [y1, x1, y2, x2] coordinates with values normalized by\nthe image size. Camera transformations are represented as\npolar angle \u03b8, azimuth angle \u03d5, and distance r. 1000 special\ntokens to represent discretized angle from \u2212\u03c0 to \u03c0. Fol-\nlowing [16], 3D cuboids are represented with 12 parame-\nters including projected center [u, v], virtual depth z, log-\nnormalized box dimension [ \u00afw, \u00afh, \u00afl], and continuous allocen-\ntric rotation p.\n\u2022 [u, v] represent the projected 3D center on the image\nplane relative to the 2D RoI\n\u2022 z \u2208 R+ is the object\u2019s center depth in meters.\n\u2022 [ \u00afw, \u00afh, \u00afl] \u2208 R+ are the log-normalized physical box di-\nmensions in meters.\n\u2022 p \u2208 R6 is the continuous 6D allocentric rotation.\nFor 3D cuboid detection, we use prompts to indicate the tar-\nget format, such as \u201cLocate all objects in 3D using projected\n3D center, virtual depth, log-normalized box size, and rota-\ntion in the image.\u201d\nAction representation.\nFor embodied navigation tasks,\nthe discrete action space is directly represented as texts,\ne.g. \u201cforward\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cstop\u201d.\nFor ob-\nject manipulation tasks, the action is represented differ-\nently based on the robots. Overall, the positional change\n(e.g. (\u2206PosX, \u2206PosY, \u2206PosZ)), rotational change (e.g.\n(\u2206RotX, \u2206RotY, \u2206RotZ)), and gripper open or close are\ndiscretized using the same 1000 special tokens, and we use\nthe text prompt to indicate the input and target format. For\ntasks that require multi-step planning (e.g. VIMA [87]), the\nactions are represented as human-readable texts with the in-\ndication of steps, skills used (e.g. pick, place, or push), and\ndiscretized positional and rotational parameters. Figure 11\nprovides a detailed illustration of the robot tasks.\n21\nAudio Input\nSample rate\n16000 Hz\nFFT hop length\n256 samples\nFFT window size\n1024\nMel bins\n128\nSubsegment length\n256 hops, (\u22484.08 sec)\nMel Spectrogram size\n128 mels \u00d7 256 hops\nfmin\n0\nfmax\n8000\nAST patch size\n16\ntoken size\n8 \u00d7 16\nPretrain sub-sample\n64\nFinal size\n64 or 128 tokens\nImage Input\nViT patch size\n16\nPretraining size\n384 \u00d7 384\nToken size\n24 \u00d7 24\nPretrain sub-sample\n288\nFinal size\n288 or 576 tokens\nText\nSeq length\n512\nFinal size\n512 tokens\nImage History\nViT patch size\n16\nPretraining size\n256 \u00d7 256\nToken size\n16 \u00d7 16\nPretrain sub-sample\n128\nMax num segments\n4\nLatent size\n32\nFinal size\n32, 64, 96, 128 tokens\nAudio History\nAST patch size\n16\nPretraining size\n128 \u00d7 256\nToken size\n8 \u00d7 16\nPretrain sub-sample\n64\nMax num segments\n4\nLatent size\n16\nFinal size\n16, 32, 48, 64 tokens\nTable 8. Input representations details\nImages representation. Images are encoded with a pre-\ntrained ViT [45]. We use the ViT-B checkpoint trained on\nLAION 2B dataset2. For image inputs, we use a maximum\nlength of 576 tokens (i.e. 24 \u00d7 24 patch encoding from a\n384 \u00d7 384 image). We concatenate features from the sec-\nond and second-last layers of the ViT to capture both low\nand high-level visual information. To generate the image,\nwe encode these images as discrete tokens [49]. Different\nfrom UNIFIED-IO [123], which uses the VQ-GAN trained\non ImageNet [41] to convert 256\u00d7256 resolution image into\n16 \u00d7 16 tokens, we use the VQ-GAN trained on the Open\nImages dataset [103] with a compression ratio of 8 and a vo-\ncabulary size of 163843. This converts 256\u00d7256 resolution\nimage into 32 \u00d7 32 tokens. We also compare the VQ-GAN\n2https://github.com/mlfoundations/open_clip\n3https://github.com/CompVis/taming-transformers\ntokenizer with the ViT-VQGAN [208] and MoVQ [223].\nWe empirically find VQ-GAN leads to best generation re-\nsults.\nDense structures representation. To handle this modality,\nwe convert per-pixel labels into RGB images. For depth,\nwe construct a grayscale image by normalizing the depth\nmap. For surface normal estimation, we convert the x/y/z\norientations into r/g/b values. For segmentation, we train\nUNIFIED-IO 2 to predict a single black-and-white mask for\na particular object specified by a class and a bounding box.\nInstance segmentation (as done in GRIT [66]) can then be\nperformed by first performing localization for the target\nclass and then performing segmentation for each detected\nbox. UNIFIED-IO instead trains the model to produce an\nimage with a randomly selected color for each instance. We\nfound this makes post-processing difficult since output im-\nages sometimes do not exactly follow the color scheme, and\nthe model could struggle with images with many different\ninstances.\nAudio representation.\nThis modality encodes a 4.08-\nsecond segment of audio. We take the waveform sampled\nat 16000 Hz and convert it to a log-mel-scaled spectrogram.\nWe compute the spectrogram for an entire audio segment\n(4.08 seconds) simultaneously. Each window involves 1024\nsamples and 256 samples \u2018hops\u2019 between windows. The re-\nsulting spectrogram has a size of 128 mel bins with 256\nwindows. We chose these hyperparameters largely around\nefficiency. We then encode this with a pre-trained AST [57]\nwith the patch size of 16 \u00d7 16, hence a total of 128 tokens.\nTo generate audio, we use ViT-VQGAN [208] to con-\nvert the spectrograms into discrete tokens. Since the authors\nof [208] did not release the source code or any pre-trained\nmodels, we implement and train our own version of ViT-\nVQGAN with 8 \u00d7 8 patch size that encodes a 256 \u00d7 128\nspectrogram into 512 tokens with a codebook size of 8196.\nThe model is trained with the audio on AudioSet [54],\nACAV100M [105], and YT-Temporal-1B [215] datasets.\nAfter getting the log-mel-scaled spectrograms, we use HiFi-\nGAN4 [98] vocoder to decode the spectrograms back to\nwaveforms. We train the HiFi-GAN using the same param-\neters shown in Table 8. We trained the model on a mixture\nof AudioSet and LJSpeech [85] to cover natural sound and\nhuman voice.\nHistory representation. Images and audio inputs in this\nhistory are first encoded in the same way as image and au-\ndio inputs. We then use a perceiver resampler [5] to further\ncompress the image and audio features and produce a fixed\nnumber of visual outputs (32) and audio outputs (16) to re-\nduce the total sequence length of the model. As shown in\nTable 8, we consider a maximum of 4 images and audio\nsegments. In our experiments, we test with two different\nvariants of perceiver implementations: 1) a small group of\n4https://github.com/jik876/hifi-gan\n22\nlatent embeddings query each frame/segment individually\n[5, 9], 2) a large group of latent embeddings query all his-\ntory at once. While the second implementation can finely\nrepresent the referenced image and audio, the first can pre-\nserve better temporal information. Thus, our final imple-\nmentation uses the first one.\nB.2. 2D Rotary Embedding\nWe use a rotary position encoding to model the relative lo-\ncation of input sequences [169]. We chose this primarily\nbecause we did not want to use absolute (additive) position\nembeddings, which would have to be added to the inputs\nof each encoder, and also wanted to be consistent with the\nLLaMA [177] position encoding.\nThe rotary encoding uses no parameters and instead uses\na kernel trick to allow the model to recover relative dis-\ntances between key and query elements in a transformer\u2019s\nattention head. For text, we apply rotary encoding at each\nlayer of the network. For other modalities, we extend RoPE\nto two-dimensional cases by splitting each of the query and\nkey embeddings of transformer attention heads in half and\napply separate rotary embeddings constructed by each of\nthe two coordinates to the halves.\nWe treat each token (image, audio, image history, and\naudio history) as having a 2-dimensional position corre-\nsponding to 1) h, w coordinates in the image or audio spec-\ntrogram, 2) (t, l) where t and l represent the indices of\nframe and perceiver latent vector in the image or audio his-\ntory, respectively. Different from [215], which uses a 4-\ndimensional position to represent all the inputs, we use a\ncombination of learnable segment (modality) embeddings\nand rotary encoding.\nB.3. Dynamic Packing\nHere, we describe the dynamic packing algorithm in more\ndetail. As is standard practice, when batching together in-\nputs, we pad input tensors to a maximum length and use\nattention masked to prevent the transformer from attending\nto padding elements. This, however, is highly inefficient\nin our multi-modal setting because many modalities are not\npresent in most examples, which results in a huge amount\nof padding. For example, if one example in the batch has\nan image output, every other example must be padded with\n1024 target image tokens, even if their output is in a differ-\nent modality.\nOne solution is to arrange batches so that each batch\ncontains examples with similar numbers of tokens in each\nmodality. This is, however, complicated to do in practice\nsince (1) our data does not fit in RAM, so we cannot easily\nsort and group data this way, especially if needing to match\ntokens across five input and three output modalities and (2)\nour coding framework, JAX [15], does not support variable\nlength tensors when constructing the execution graph which\nL\nXL\nXXL\nTransformer\nParams\n1.1B\n3.2B\n6.8B\nVocab size\n33280\nImage vocab size\n16512\nAudio vocab size\n8320\nModel dims\n1024\n2048\n3072\nMLP dims\n2816\n5120\n8192\nencoder layer\n24\ndecoder layer\n24\nHeads\n16\n16\n24\nMLP activations\nsilu, linear\nLogits via embedding\nTrue\nDropout\n0\nImage Resampler\nLatents size\n32\nModel dims\n768\n1024\n1024\nHeads\n12\n16\n16\nHead Dims\n64\nNumber layer\n2\nMLP Dims\n2048\n4096\n4096\nMLP activations\ngelu\nAudio Resampler\nLatents size\n16\nModel dims\n768\n1024\n1024\nHeads\n12\n16\n16\nHead Dims\n64\nNumber layer\n2\nMLP Dims\n2048\n4096\n4096\nMLP activations\ngelu\nViT\nPatch size\n16\nModel dims\n768\nHeads\n12\nHead Dims\n64\nNumber layer\n11\nMLP Dims\n3072\nMLP activations\ngelu\nAST\nPatch size\n16\nModel dims\n768\nHeads\n12\nHead Dims\n64\nNumber layer\n11\nMLP Dims\n2048\nMLP activations\ngelu\nTable 9. Model Hyperparameters\nmakes handling variable lengths between batches extremely\ndifficult.\nInstead, we use packing, a process where the tokens of\nmultiple examples are packed into a single sequence, and\nthe attentions are masked to prevent the transformer from\ncross-attending between examples. Packing is often done\nas a pre-processing step when handling text, but this does\nnot work in our setup since some parts of our network can-\nnot operate on packed data (e.g., the VAE or image ViT).\nInstead, we start with an unpacked batch of examples, run\n23\nthese components first, and then dynamically pack the re-\nsulting tokens in a backdrop-compatible way before running\nthe transformer. To run efficiently on TPUs we pack exam-\nples using matrix multiplication with carefully constructed\none-hot matrices.\nTo account for all modalities, the maximum sequence\nlength our transformer needs to take as input is 1152, and\nthe maximum target length is 2048. When packing, we can\ngenerally pack two examples into an input sequence of 864\nand a target sequence of 1280, which gives a roughly 4x\nspeed up due to reduced sequence length and the ability\nto process two examples simultaneously. When streaming\ndata, packing cannot be done reliably. For example, if two\nconsecutive examples have an image output, they cannot be\npacked since they will total over 1280 output tokens. To\nhandle this, we use a heuristic algorithm to re-arrange data\nas it is being streamed. The algorithm keeps a small pool of\nexamples in memory. Given a new example, it pairs it with\nthe largest example in the pool it can be packed with and\noutputs both as a pair. If no such example exists, it adds the\nexample to the pool. If the pool reaches a maximum size\nof 10, the largest example is emitted and processed with-\nout being packed with another example. We find this occurs\nless than 0.1% of the time during training.\nB.4. Full Model Details\nIn Table 9,\nwe present the full hyperparameters of\nour model.\nDuring pre-training, we train the UIO-2L,\nUIO-2XL, and UIO-2XXL with a batch size of 512 due to\nmemory limit. We sub-sample 50% of the image, audio,\nand history inputs patches. The total packing length is 864\nfor the encoder and 1280 for the decoder. During instruc-\ntion tuning, we train all of our models with a batch size 256\ndue to computing constraints. We sub-sample 87.5% of the\nimage, audio, and history input patches. The total packing\nlength is 1024 for pretraining and 1280 for instruction tun-\ning. 8-way in-layer parallelism and 64-way data parallelism\nwere used to scale up to the 7B model training.\nWe train for 1.5 million steps with an effective batch size\nof 512. This results in training on approximately 1 trillion\ntokens. During pre-training, we keep at most 50% of the\nimage patches in the image history or image encoder, as is\ncommon practice with MAE pre-training [71]. We use up\nto four images/segments in image/audio history.\nC. Pre-Training Details\nIn this section, we provide additional details about the data\nUNIFIED-IO 2 is pre-trained on. The datasets we use for\npre-training are listed in Table 10. Unless otherwise speci-\nfied, we use the pre-training objective described in Section\n3.3, where one of the present modalities is randomly se-\nlected as the target. We sample data to ensure all the output\nmodalities are well represented and to balance how often\nour various corpora are used based on their size. The distri-\nbution is shown in Figure 9.\nC.1. Data Sources\nText. Our data follows the mixture used by MPT-7B [176].\nImage & Text. Image & text paired data comes from vari-\nous unsupervised corpora, shown in Table 10. For LAION\ndata, we only generate images from image/text pairs from\nLAION aesthetic, which contains higher quality images,\nwhile we generate text for image/text pairs from LAION\n400M. We also only keep images from LAION if they are\nmarked as being unlikely to be NSFW in the LAION meta-\ndata. Web images is a dataset of images we download and\nfocuses on icons and stylized images.\nVideo. We gather a total of 180M short videos from various\nsources. During training, we pick a random sequence of up\nto five frames from the video. The first four will be encoded\nwith an image/audio history encoder, while the fifth frame\nwill be encoded with the image/audio encoder. The text\nmatching these frames is encoded with a text encoder along\nwith marker tokens to show where each frame occurred as\nstated in B.1, or, if the dataset only includes a single caption\nthat is not aligned with individual frames, the entire caption\nis encoded instead. The text, audio, or image modality can\nbe selected as the target modality. As usual, other modal-\nities are randomly masked, and the target modality is ran-\ndomly masked or injected with noise in the input. Note we\nhave sub-sampled data from many of these corpora to keep\nthe dataset size more manageable, and sometimes due to\nbroken video links.\nInterleaved\nImage\n&\nText.\nWe\nprimarily\nuse\nOBELICS [104], which contains paragraphs and im-\nages interleaved together. For each document, we randomly\nselect an image or a paragraph as the target and use up to\nthe previous four (if the target is an image) or five (if the\ntarget is a paragraph) images as context. The last image is\nencoded with the image encoder, and the remaining images\nare encoded in the image history. The text matching those\nimages is concatenated and interjected with marker tokens\nto indicate where the images in the image history or image\ninput occur. We either do de-noising, where a noisy version\nof the target is included in the input, or generation, where\nthe target is not part of the input, although we always\ninclude both the text and image input modalities.\nIn addition, we construct interleaved data by interleav-\ning multiple images and captions from several image/text\npair corpora. The images are encoded as the image input\nand/or the image history, and matching text is constructed\nby specifying the caption for one, or all, of these images us-\ning special tokens to mark which image each caption refers\nto. For this task, we only target the text modality, and train\nthe model to either (1) de-noise the caption of a single im-\nage, (2) generate a caption for a single image that is speci-\n24\nAudio\nImage\nText\nVideo\nAgent Trajectories\nSynthetic\nImage/Text Interleaved\nVideo\nImage/Text Pairs\nAgent Trajectories\nSynthetic\nImage/Text Interleaved\nVideo\nImage/Text Pairs\nText\nEgo4D\nACAV\nAudioSet\nHD-VILA\nYT-Temporal\nHabitat Next Frame/State Prediction\nProcTHOR Next Frame/State Prediction\nSegment Anything\nHabitat Easy\nHabitat Hard\nObjaverse Multiview\nOBELICS\nEgo4D\nAudioSet\nHD-VILA\nWebVid\nACAV\nYT-Temporal\nCC3M\nCC12M\nRedCaps\nWeb Images\nLAION Aesthetic v2.5\nHabitat Next Frame/State Prediction\nProcTHOR Next Frame/State Prediction\nRedCaps Patch Counting\nLAION Aesthetic Patch Counting\nLAION Aesthetic Patch Detect\nRedCaps Caption Multiple\nCC3M Caption Multiple\nRedCaps Denoise One\nCC3M Denoise One\nCC12M Caption Multiple\nRedCaps Caption One\nObjaverse Camera Change\nCC12M Denoise One\nCC3M Caption One\nCC12M Caption One\nOBELICS\nEgo4D\nAudioSet\nHD-VILA\nWebVid\nACAV\nYT-Temporal\nCC3M\nCC12M\nRedCaps\nWeb Images\nLAION 400M\nRedPajama Book\nStack Markdown\nWikipedia\nRedPajama Common Crawl\nStack\nC4\nMC4\nFigure 9. Pre-training data distribution, segments proportional to sampling rates. The inner section shows the target modality, the middle\nsection shows the type of data, and the third shows particular datasets.\nfied in an input prompt using a marker token or (3) generate\na sequence of marker tokens and captions that describe each\ninput image. This task aims to ensure the model learns the\nsemantics of the images in the history and understands the\nmarker tokens.\nMulti-View. We train on the cross-view completion task\nfrom CroCo [194], where the model must complete a heav-\nily noised image using an image of the same scene, but from\na slightly different angle, as context. The noised input is en-\ncoded as an image and the second image is encoded through\nthe image history encoder. In addition, we generate data us-\ning Objaverse [40] objects by capturing multiple views of\nthe object in 3D, and either specify the camera coordinates\nin the input text and train the model to generate a new image\nmatching new camera coordinates, or train the model to pre-\ndict how the camera has moved between different images.\nWe further augment the view synthesis task by providing\nin-context examples. For example, by giving one or more\nexamples of the views and transformations in the image his-\ntory, the model predicts the new view from the new camera\ntransformation specified by the prompt. Both tasks aim to\nimprove the model\u2019s 3D understanding during pre-training.\nAgent Trajectory. We use scripted shortest path trajec-\ntories in ProcTHOR [39] and human-collected demonstra-\ntions in Habitat [149, 157]. While the original datasets are\nfor object navigation with relatively long episode lengths,\nwe only subsample from the last few frames for image his-\ntory and image input such that mostly the target object is\nwithin the observation. The task is randomly selected from\n1) generating the next visual observation frame as the target\nimage, 2) predicting the next positional observation coor-\ndinates as the text target, and 3) predicting the next action\n25\nSize\nRate Text Sparse Dense Image Audio ImageH AudioH Text Sparse Dense Image Audio\nText\n6.6b\n33.0\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nMC4 [201]\n5.0b\n11.7\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nC4 [68]\n266m 10.6\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nStack [95]\n147m 3.55\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nRedPajama CC [32]\n1.2b\n3.55\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nWikipedia\n6.8m 1.42\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nRedPajama Book [32]\n13m\n1.06\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nStack-Markdown [95]\n34m\n1.06\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nImage/Text\n970m 31.3\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n\u2713\n-\nLAION Aesthetics v2.5 [158] 491m 17.7\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\nLAION-400M [159]\n346m 8.95\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n-\n-\nCC12M [23]\n11m\n1.48\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n\u2713\n-\nRedCaps [42]\n12m\n1.39\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n\u2713\n-\nWeb Images\n107m 1.33\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n\u2713\n-\nCC3M [163]\n3.0m 0.49\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n\u2713\n-\nVideo\n181m 25.0\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n\u2713\n\u2713\nYT-Temporal [215]\n146m 13.7\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n\u2713\n\u2713\nACAV [105]\n17m\n3.98\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n\u2713\nHD-VILA [200]\n7.1m 2.75\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n\u2713\n\u2713\nAudioSet [54]\n1.7m 2.75\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n\u2713\n\u2713\nWebVid [13]\n9.2m 1.23\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n\u2713\n-\nEgo4D [60]\n0.7m 0.55\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n\u2713\n\u2713\nInterleaved Image/Text\n157m 8.70\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\nOBELICS [104]\n131m 8.00\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\nCC12M Interleaved\n11m\n0.35\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n-\n-\nCC3M Interleaved\n3.0m 0.21\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n-\n-\nRedCaps Interleaved\n12m\n0.14\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n-\n-\nMulti-View\n3.4m 0.67\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\nCroCo Habitat [157, 194]\n2.6m 0.33\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\nObjaverse [40]\n0.8m 0.33\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\nAgent Trajectories\n1.3m 0.33\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\nProcTHOR [39]\n0.7m 0.17\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\nHabitat [157]\n0.6m 0.17\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\nSynthetic\n504m 1.00\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n-\n\u2713\n\u2713\n-\n-\nSegment Anything [94]\n1.1m 0.50\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n-\n-\n\u2713\n-\n-\nLaion Aesthetics Patches\n491m 0.45\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\n-\n-\nRedCaps Patches\n12m\n0.05\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\n-\n-\nAll\n8.5b\n100\n\u2713\n\u2713\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 10. Datasets used for pre-training, rate shows the sampling percentage during pre-training and size shows the approximate number\nof examples if iterating through the data once.\nas the text target. 1) requires inferring from the image and\nimage history input and the last action specified in the text\ninput, 2) further requires the location information, and 3) is\nbased on the target object name and visual observations for\nthe next action prediction.\nSynthetic. We add two synthetic tasks. First, we use the\nautomatically annotated data from Segment Anything [94].\nWe give the model either a set of points or a bounding box as\ninput and train it to generate a segmentation mask as output.\nSecond, we add artificial patches of various shapes and col-\nors to images from other unsupervised datasets and train the\nmodel to output their locations in order to train the model\nto generate sparse coordinates as output. We additionally\ntrain the model to output the total number of patches on the\nimage to pre-train its counting abilities.\nD. Instruction Tuning Details\nIn this section, we provide additional details about the in-\nstruction tuning data and individual tasks UNIFIED-IO 2\n26\nEmbodied AI\nNatural Language\nAudio Understanding\nVideo Sparse Labelling\nVideo Understanding\nImage Dense Labelling\nImage Sparse Labelling\nImage Understanding\nAudio Generation\nImage Generation\nEmbodied QA\nGoal Generation\nNext Frame/State Prediction\nAction Prediction\nLanguage Modeling\nText Instruction Following\nAudio Captioning\nAudio Tagging\nVideo Sound Localization\nVideo Action Localization\nVideo Tracking\nVideo Instruction Following\nVideo Question Answering\nVideo Captioning\nVideo Tagging\nOptical Flow\nDepth Estimation\nReferring Expression_Segmentation\nSurface Normal Estimation\nLocalized Segmentation\nSemantic_Segmentation\nKeypoint Detection\nText Detection\n3D\nReferring Expression\nObject Localization\nObject Detection\nImage Pair _QA\nImage Instruction Following\nRegion Captioning\nRelationship Prediction\nImage Tagging\nRegion Classification\nImage Captioning\nVQA\nAudio from Video\nAudio from Text\nView_Synthesis\nImage Inpainting\nNext Frame Generation\nImage Editing\nControllable Image Editing\nImage from Text\nFrankaKitchen Caption\nHabitat PointNav Affordance\nBridgeData V2 Caption\nVIMA detection\nLanguage Table (sim) Caption\nLanguage Table (real) Caption\nFrankaKitchen\nBridgeData V2\nVIMA\nHabitat Next Frame/State Prediction\nProcTHOR Next Frame/State Prediction\nFrankaKicthen\nVIMA\nVIMA No Masking\nVIMA w/BBox Prediction\nBridgeData V2\nLanguage Table (sim)\nVIMA Pure Text\nHabitat PointNav\nLanguage Table (real)\nHabitat PointNav w/State Prediction\nVIMA w/BBox and Masking\nVIMA w/BBox Input\nRedPajama Book\nWikipedia\nRedPajama Common Crawl\nStack\nC4\nMC4\nMBPP Code\nFlan2 Dialog\nFlan2 CoT\nDolly\nCode Aplaca\nOASST\nFlan2 SuperNLI\nAlpaca\nFlan2 T0\nFlan2 FLan2021\nLJ Speech\nMACS\nMusicCaps\nClotho\nAudioCaps\nMACS\nKinetic Sounds\nAudioSet\nVGG-Sound\nFlickr SoundNet\nAVA\nGOT\nLaSOT\nYouTube-BB\nM3IT MSRVTT QA\nMIMIC-IT Navigation Dialogue\nMIMIC-IT Navigation\nMIMIC-IT Dense Captioning\nMIMIC-IT Story Telling\nMIMIC-IT TV Caption\nM3IT MSVD-QA\nM3IT SS\nM3IT iVQA\nEgo4D Talking to Me\nEgo4D Looking at Me\nMSVD\nVTVQA\nSTAR\nMSRVTT-QA\nMSRVTT-Captioning\nVATEX\nWebWid\nUCF101 Action Tagging\nAction Anticipation EK100\nAction Tagging EK100\nSS V2\nSS V2 Natural\nKinetics710\nFly Chairs\nMPI Sintel\nNYU Depth\nRecoco Google\nRecoco UNC\nRefcoco UNC Plus\nBlendedMV\nBlendedMV Aug.\nFramenet\nFramenet Aug.\nTaxonomy\nTaxonomy Aug.\nLVIS\nCOCO\nOpen Images\nVizWiz Grounding\nLVIS Aug.\nCOCO Aug.\nOpen Images Aug.\nCOCO Aug.\nCOCO Text Detection\nCOCO Text\nCOCO Text Multi-Localization\nCOCO Text Localization\nObjaverse Camera Change\nOmni3D\nRefcoco Google\nRefcoco UNC\nRefcoco UNC Plus\nRecoco UNC Pair Aug.\nRefcoco UNC Aug.\nRecoco Google Pair Aug.\nRefcoco Google Aug.\nCOCO\nVisual Genome Class Subset\nOpen Images\nVisual Genome\nVisual Genome Multi-Localization\nCOCO\nCOCO Multi-Localization\nVisual Genome Aug.\nCOCO Aug.\nOpen Images Multi-Localization\nOpen Images Aug.\nMIMIC-IT Spot Difference Subtle\nMIMIC-IT Spot Difference General\nM3IT NLVR\nM3IT Mocheg\nLLaVA Instruction MC\nLLaVAR Instruct MC\nM3IT Visual Dialog\nLLaVA Instruct\nVisual Genome\nVSR\nVSR Binary\nVG Relation from Expression\nVG Relations from BBox\nVisual Genome\nOpen Images\nM3IT IQA\nCalTech Birds 2011\nM3IT COCO ITM\nM3IT ImageNet\nSun397\niNaturalist\nImagenet\nPlaces 365\nM3IT COCO GOI\nCOCO Region Cls\nVisual Genome Region Cls\nOpen Images Region Cls\nM3IT Paragraph Captioning\nVizWiz\nSCICAP\nText Caps From History\nTextCaps\nM3IT TextCaps\nCC3M\nCC3M Aug.\nCOCO\nCOCO Aug.\nOpen Images\nOpen Images Aug.\nRedCaps\nRedCaps Aug.\nM3IT ViQuAE\nM3IT OKVQA\nM3IT OCR VQA\nM3IT Shapes\nM3IT VisualMRC\nM3IT A-OKVQA\nM3IT CVR\nOKVQA-v1\nM3IT ST-VQA\nM3IT Text VQA\nM3IT CLEVR\nM3IT GQA\nM3IT VQA 2.0\nOK-VQA-2\nVizWiz\nBlendMVS\nTallyQA MC\nText VQA\nGQA Sentence Answer\nScienceQA\nScienceQA Rationale\nFramenet\nTaxonmy\nGQA\nCOCO Text\nOCR-VQA\nTallyQA\nVQA 2.0\nVisual Genome\nVCR\nYT-Temporal\nLJ Speech\nMACS\nMusicCaps\nClotho\nVGG-Sound\nAudioCaps\nACAV\nAudioSet\nObjaverse\nObjaverse Multiview\nHabitat Easy\nHabitat Hard\nVisual Genome\nCOCO\nOpen Images\nGOT\nLaSOT\nMagicBrush\npix2pix\nCOCO\nLVIS Segmentation\nMultigen Combination\nMultigen\nCOCO\nOpen Images\nCC3m\nRedCap\nLAION Aesthetic v2.5\nFigure 10. Sunburst chart of our instruction-tuning mixtures, sections are proportional to sampling rates.\nsupports.\nAn overview of the instruction tuning data is\nshown in Table 11. We show a visualization including indi-\nvidual datasets in Figure 10. We sample broad categories of\ntasks evenly and then generally sample individual datasets\nin proportion to the square root of their size, although with\nsome minor hand-engineered adjustments to downweight\nnoisy datasets or upweight very rare tasks.\nD.1. Natural Language\nFor natural language data we use the mixture from\nFlanV2 [122], which in turn includes data from Muf-\nfin [193], T0-SF [156], NIV2 [191], and CoT annotations,\nas well data from Alpaca [142], Dolly [33], Open Assis-\ntant [99], and MDPP [8]. In addition, we continue pre-\ntraining on our unsupervised NLP mixture from our fine-\ntuning stage to ensure the model does not forget informa-\ntion learned from unsupervised data during the extensive\ninstruction-tuning stage.\nD.2. Image Generation\nFor text-to-image generation, we use the same image/text\npairs we used during pre-training, as well as localized\nnarratives from Open Images [103] and captions from\nCOCO [115] and Visual Genome (VG) [102]. Our prompts\nfor these tasks specify that the image might be noisy or ap-\nproximate for unsupervised corpora (e.g. \u201cGenerate an im-\nage that roughly matches this text: {caption}\u201d) and give\nhints as to the style for supervised corpora (e.g. \u201cWhat\ndo you see in this image?\nPlainly describe the individ-\nual element you observe.\u201d for localized narratives) to help\ndisambiguate the stylistic differences between the datasets.\nWe use simple prompts (e.g. \u201cCaption this image.\u201d) for the\nCOCO captions.\nWe additionally train the model to generate images\n27\nSize\nRate Datasets Text Sparse Dense Image Audio ImageH AudioH Text Sparse Dense Image Audio\nImage Generation\n506m 17.6\n21\n\u2713\n\u2713\n\u2713\n\u2713\n-\n\u2713\n\u2713\n\u2713\n-\n-\n\u2713\n-\nImage from Text\n497m 10.6\n5\n\u2713\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\u2713\n-\nControllable Image Editing\n3.0m 2.92\n4\n\u2713\n-\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\nImage Editing\n1.1m 1.66\n3\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\nNext Frame Generation\n24k\n0.96\n2\n\u2713\n\u2713\n-\n-\n-\n\u2713\n\u2713\n-\n-\n-\n\u2713\n-\nImage Inpainting\n1.0m 0.79\n3\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\nView Synthesis\n4.2m 0.60\n4\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\nAudio Generation\n164m 7.50\n9\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n-\n-\n\u2713\nAudio from Text\n19m\n5.62\n8\n\u2713\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\n\u2713\nAudio from Video\n145m 1.88\n1\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n-\n-\n\u2713\nImage Understanding\n53m\n17.8\n73\n\u2713\n\u2713\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n-\n-\nVQA\n5.8m 6.23\n31\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n-\n-\nImage Captioning\n32m\n4.25\n14\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n-\n-\nRegion Classification\n6.1m 2.41\n4\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n-\n-\nImage Tagging\n3.8m 2.38\n8\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n-\n-\nRelationship Prediction\n0.8m 1.41\n6\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n-\n-\nRegion Captioning\n3.5m 0.60\n1\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n-\n-\nImage Instruction Following\n0.4m 0.37\n6\n\u2713\n-\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\n-\n-\nImage Pair QA\n0.1m 0.17\n3\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n-\n-\nImage Sparse Labelling\n13m\n7.25\n26\n\u2713\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\nObject Detection\n5.3m 3.08\n9\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\n-\n-\nObject Localization\n6.0m 1.31\n3\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\n-\n-\nReferring Expression\n0.2m 1.08\n7\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\n-\n-\n3D\n1.0m 1.00\n2\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\nText Detection\n37k\n0.41\n3\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\n-\n-\nKeypoint Detection\n0.3m 0.38\n2\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\n-\n-\nImage Dense Labelling\n6.9m 4.06\n19\n\u2713\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\nSemantic Segmentation\n2.4m 1.23\n4\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n-\n\u2713\n-\n-\nLocalized Segmentation\n3.2m 1.17\n3\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n-\n-\n\u2713\n-\n-\nSurface Normal Estimation\n1.1m 1.03\n6\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n-\n\u2713\n-\n-\nReferring Expression Segmentation\n0.1m 0.47\n3\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n-\n\u2713\n-\n-\nDepth Estimation\n47k\n0.11\n1\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\n-\n\u2713\n-\n-\nOptical Flow\n24k\n0.06\n2\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n-\n-\n\u2713\n-\n-\nVideo Understanding\n13m\n10.6\n24\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n-\n-\nVideo Captioning\n9.1m 3.75\n3\n\u2713\n-\n-\n\u2713\n-\n\u2713\n\u2713\n\u2713\n-\n-\n-\n-\nVideo Tagging\n1.1m 3.75\n6\n\u2713\n-\n-\n\u2713\n-\n\u2713\n\u2713\n\u2713\n-\n-\n-\n-\nVideo Question Answering\n2.5m 2.84\n9\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n-\n-\nVideo Instruction Following\n0.2m 0.21\n6\n\u2713\n-\n-\n\u2713\n-\n\u2713\n\u2713\n\u2713\n-\n-\n-\n-\nVideo Sparse Labelling\n0.4m 3.42\n5\n\u2713\n\u2713\n-\n\u2713\n-\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\nVideo Tracking\n0.2m 2.50\n3\n\u2713\n\u2713\n-\n\u2713\n-\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\nVideo Action Localization\n0.2m 0.61\n1\n\u2713\n-\n-\n\u2713\n-\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\nVideo Sound Localization\n2.5k\n0.31\n1\n\u2713\n-\n-\n\u2713\n-\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\nAudio Understanding\n2.2m 2.50\n10\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n-\nAudio Tagging\n2.1m 1.25\n5\n\u2713\n-\n-\n\u2713\n\u2713\n\u2713\n-\n\u2713\n-\n-\n-\n-\nAudio Captioning\n75k\n1.25\n5\n\u2713\n-\n-\n-\n\u2713\n-\n-\n\u2713\n-\n-\n-\n-\nNatural Language\n11m\n25.0\n17\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nText Instruction Following\n11m\n12.5\n10\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nLanguage Modeling\n-\n12.5\n7\n\u2713\n-\n-\n-\n-\n-\n-\n\u2713\n-\n-\n-\n-\nEmbodied AI\n7.2m 4.33\n23\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n\u2713\n-\n\u2713\n-\nAction Prediction\n4.3m 3.37\n12\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n-\n-\nNext Frame/State Prediction\n1.3m 0.33\n2\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n-\n-\n\u2713\n-\nGoal Generation\n0.7m 0.33\n3\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n-\n-\n-\n\u2713\n-\nEmbodied QA\n1.0m 0.30\n6\n\u2713\n-\n-\n\u2713\n-\n\u2713\n-\n\u2713\n\u2713\n-\n-\n-\nAll Tasks\n775m 100\n227\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 11. Instruction tuning training mixture. Due to the number of datasets used, we group them by task and only show statistics for each\ngroup. The rate shows the sampling rate, size shows the number of examples of iterating through the data once, and datasets show the\nnumber of individual data sources used for the tasks.\n28\nthrough view synthesis [40, 194] as was done during pre-\ntraining. We also integrate data for image editing [18, 217]\nand image editing based on various dense control signals\nsuch as depth maps, edges, segmentation, etc.\nFollow-\ning [145], and the segmentation-based image generation\nfrom UNIFIED-IO using data from COCO and LVIS [62].\nFinally, we train on inpainting by masking a region of an\ninput image that contains an object and training the model\nto generate the complete image given the object name and\nlocation. We derive data for this task from the object anno-\ntation data in COCO, Open Images, and VG.\nDuring inference, we use top-p sampling, also known as\nnucleus sampling [75], for generating images with the tem-\nperature t = 1.0 and p = 0.95. We also enable classifier-\nfree guidance [74] by replacing the prompt with the un-\ninformative prompt \u201cAn image of a random picture.\u201d 10%\nof the time during training. That prompt is then used as the\nclassifier-free prompt with a guidance scale of \u03b1 = 10.0\nduring inference.\nD.3. Audio Generation\nDatasets for audio generation from text include Audio-\nCaps [93], Clotho [47], MACS [131], MusicCaps [2], and\nLJSpeech [85]. During training, we divided the audio into\n4-second-long segments and then generated one segment of\nthe target audio, giving both the text and any previous seg-\nments as input. We also train on the next-frame prediction\ntask, which aims to generate the audio for the next frame in\na video from YT-Temporal-1B [215].\nOur prompts for these tasks specify the characteristics\nof target audio; e.g., \u201cGenerate the sound/music based on\nthe description: {caption}\u201d for natural sound and music,\nrespectively, and \u201cSpeak: {passage}\u201d for speech. We use\nthe same sampling method as the image generation, the top-\np sampling with the temperature t = 1.0 and p = 0.95. We\ndo not use the classifier-free guidance because it can lead to\npoor performance. When generating audio longer than 4.08\nseconds during inference, we generate an initial segment\nthat is 4.08 seconds long and then extend it by generating\nadditional segments using previous audio segments as the\naudio history input.\nD.4. Image Understanding\nThese tasks require generating text in response to a query\nabout an image or a pair of images. We use the data from\nM3IT [112] and MIMIC-IT [107, 108], as well as a variety\nof other additional sources. For VQA, we add GQA [83],\nTallyQA [1], OK-VQA [130], A-OKVQA [160], OCR-\nbased VQA datasets [133, 165], Visual Genome, Sci-\nenceQA [124], VCR [213] and VizWiz [67].\nFor im-\nage tagging we add Caltech Birds [195], iNaturalist [180],\nSun397 [197], and Places365 [224]. For region classifica-\ntion, we add examples derived from object annotation from\nOpen Images, VG, and COCO. We categorize datasets with\nopen-ended responses such as LLaVa [119], Visual Story-\ntelling [82], and Visual Dialog [36] as visual instruction\nfollowing, and we categorize NLVR [171] and the \u201cspot the\ndifferences\u201d tasks from MIMIC-IT as image pair QA. For\nimage pair QA tasks, we encode the second image in the\nimage history modality.\nWe also add a grounded relationship prediction task us-\ning data from Visual Genome and VSR [116] as well as\nimage captioning using the same supervised sources we use\nfor image generation.\nWe again put stylistic hints in the prompts for these tasks.\nFor example, in VQA and captioning datasets, we specify\nto return a short answer (e.g. \u201cAnswer this question very\nsuccinctly: {question}\u201d), which we find is critical to allow\nthe model to produce longer, more natural responses when\nasked user questions. Likewise, we roughly specify the kind\nof class to output for image tagging, e.g., \u201c\u201dWhat is the sci-\nentific name of this animal?\u201d for the iNaturalist dataset.\nD.5. Image Sparse Labelling\nThese tasks require outputting sparse coordinates based on\nan input image. We use Open Images, Visual Genome, and\nCOCO for object detection and localization, which requires\ndetecting all objects belonging to a specific class and three\nCOCO referring expression datasets [91, 136, 209] for re-\nferring expressions.\nIn addition, we train on the OmniLabel [16] 3D detection\ndataset by generating the projected 3D center, virtual depth,\nlog-normalized box size, and rotation of each 3D box, again\nby normalizing these values between 0 and 1 and then en-\ncoding them using the location tokens. We also added the\ncamera pose prediction tasks using Objaverse objects that\nwere used during pre-training.\nWe include 3 text detection datasets from COCO-\nText [183], including finding the bounding box of an input\ntext string for multiple text strings or finding and listing all\ntext along with their bounding boxes in an image.\nLastly, we do keypoint detection using COCO pose data.\nFor keypoint detection, we input a bounding box around a\nperson in the image and train the model to return a list of\nkeypoints for that person. During inference, we first localize\nall people in the image and then use each returned bounding\nbox as a keypoint query to find that person\u2019s keypoints. Dur-\ning training, the model predicts \u201cMISSING\u201d for keypoints\nthat are not visible (e.g. \u201cright elbow: MISSING\u201d). During\ninference, we use a masking function over the model\u2019s logit\nto force it to guess a valid point for each keypoint since the\nkeypoint metric does not award points for correctly identi-\nfying a keypoint as being not visible.\n29\nD.6. Image Dense Labelling\nWe do several image labeling tasks, including surface nor-\nmal estimation on FramNet [78], BlendedMVS [204] and\nTaskonomy [211], depth on NYU Depth [138], and optical\nflow on Flying Chairs [44] and MPI Sintel [21].\nWe additionally train on several segmentation tasks: se-\nmantic segmentation (segmenting a particular class), lo-\ncalization segmentation (segmenting an object in an input\nbounding box), and referring expression segmentation (seg-\nmenting an object matching a referring expression). Data\ncomes from Open Images, COCO, LVIS, and referring ex-\npressions from the COCO refexp datasets [91, 136, 209]. To\ndo instance segmentation, as needed for GRIT, we first do\nlocalization on the target class and then perform localized\nsegmentation on each returned bounding box.\nDuring inference, we do temperature sampling with a\ntop-p of 0.95 as before, but without classifier-free guidance.\nFor segmentation, we find it beneficial to increase the value\nof p to 0.97.\nD.7. Video Understanding\nThese tasks require generating text in response to a query\nabout a video.\nFor video captioning, we add VA-\nTEX [190] and MSR-VTT [199].\nFor action classifica-\ntion (video tagging), we add UCF101 [168], Kinetics-\n710 [111], Something-Something v2 [58] and EPIC-\nKITCHENS-100 [35]. We also use examples from EPIC-\nKITCHENS-100 for action anticipation. For video question\nanswering, we add MSRVTT-QA [198], MSVD-QA [198],\nSTAR [196] and M4-ViteVQA [221]. Lastly, we use ex-\namples from M3IT and MIMIC-IT for the video instruction\nfollowing.\nTo cover the visual content of the entire video with a\nsmall number of frames (5), we use the segment-based sam-\npling following [185]; we first divide the video into five\nsegments of equal duration and then randomly sample one\nframe from each of the segments during training, and the\nmiddle frame at inference. We use the first four frames as\nthe image history input and the final frame as the image in-\nput for action classification and video captioning. We em-\npirically found that using the third frame as the image input\nwhile using the other frames as the image history input per-\nforms better for video question answering.\nWe use similar prompts to those for image understanding\ntasks, e.g., \u201cWrite a short description of this video.\u201d, \u201cThe\nquestion {question} can be answered using the video. A\nshort answer is\u201d and \u201cWhat are they doing in this video?\nShort answer:\u201d in video captioning, video question answer-\ning, and video tagging, respectively, for ensuring a short\nanswer.\nD.8. Video Sparse Labelling\nWe do single object tracking and spatial-temporal action lo-\ncalization on video data. We train on YouTube-BB [151],\nLaSOT [50] and GOT-10k [79] by inputting bounding boxes\naround a target object in each of previous frames and hav-\ning the model return the next location as a bounding box\n(\u201cAnticipate the object\u2019s next location from all previous\nimages and the location of the object in those frames:\n{locations}.\u201d). We also train the model on AVA [61] by\ninputting a video snippet consisting of five frames and re-\nquiring the model to detect all actions of humans appearing\nin the middle (third) frame of the video snippet (\u201cGiven the\ntemporal context from the video, detect all of the humans\nperforming actions in the image.\u201d). Note that we provide\nthe video snippet, not a single video frame, because some\nof the actions require temporal context to answer (e.g., stand\nand sit) correctly. We use the final/middle frame of five con-\nsecutive frames in the video as the image input and the other\nframes as the image history input for single object tracking\nand action localization, respectively.\nD.9. Audio Understanding\nWe train the model on audio tagging and audio captioning\ntasks.\nFor audio tagging, we add AudioSet [54], VGG-\nSound [24], and MACS. For audio captioning, we use the\nsame datasets as text-to-audio generation, that is, Audio-\nCaps, Clotho, MACS, MusicCaps, and LJSpeech.\nFor\naudio-visual action classification, we train on Kinetics-\nSounds [7] and VGG-Sound.\nWe again use stylistic hints in the prompts for these tasks.\nFor example, we specify the characteristics of target audio\n(e.g., \u201cDescribe the music.\u201d and \u201cTranscribe the audio to\ntext.\u201d for MusicCaps and LJSpeech, respectively), enforce\na short answer (e.g., \u201cWhat is this in the audio? Short an-\nswer:\u201d and \u201cGive a short description of this audio.\u201d), and\nspecify the kind of class to output for audio tagging, e.g.,\n\u201cThis audio depicts a scene of a\u201d for MACS. We use the\nsame prompts as video tagging for audio-visual action clas-\nsification.\nWe use the same sampling strategy as the video under-\nstanding; we sample five audio segments with uniform in-\ntervals from the whole audio and use the middle/final audio\nsegment as the audio input while using the other segments\nas the audio history input for audio classification and audio\ncaptioning, respectively.\nD.10. Embodied AI\nWhile many robot manipulation tasks can be formulated by\nmultimodal prompts that interleave language and images\nor video frames, we use VIMA-Bench [87] to evaluate the\nrobot manipulation skills. We use the image input as the\ninitial observation of the environment and the image history\nfor the images or videos in the prompt. The text inputs, or\n30\nImage History\nImage Input\n[Text] [S] Follow this motion for <image_histroy_1>: <image_history_2> <image_history_3> <image_history4>.  Now predict actions with \nthe format: \"step: start-action from ( x1 y1 r1 ) and end-action to ( x2 y2 r2 )\" based on the agent's initial observation <image_input>.\nText Input\nFollow this motion for           :                                                                                             .        .\n1: pick from ( <extra_id_424> <extra_id_665> <extra_id_200> ) and \nplace to ( <extra_id_674> <extra_id_914> <extra_id_200> ) \n2: pick from ( <extra_id_668> <extra_id_671> <extra_id_200> ) and \nplace to ( <extra_id_668> <extra_id_921> <extra_id_1199> )\n(0.22, 0.46, 0.00)\n(0.47,  0.71,  0.00)\n(0.46, 0.47, 0.00)\n(0.46, 0.72, 0.99)\nPrimitive: \nPick & Place\nStep 1:\nStep 2:\nText Target\nPos X,  Pos Y, Rot Z\nPos X,  Pos Y, Rot Z\n[Text] [S] The agent is advancing towards ( <extra_id_718> <extra_id_653> ), \nassess the next action with the discrete action space \"forward, left, right, stop\" \nbased on agent's histories with the format \"observation ( x z y ) action\":\n\n<image_history_1> ( <extra_id_686> <extra_id_642> <extra_id_211> ) forward \n<image_history_2> ( <extra_id_686> <extra_id_640> <extra_id_211> ) forward \n<image_history_3> ( <extra_id_686> <extra_id_637> <extra_id_211> ) right \n<image_history_4> ( <extra_id_686> <extra_id_637> <extra_id_1182> ) forward \n<image_input> ( <extra_id_686> <extra_id_635> <extra_id_1182> )\nforward\n[Image] [S] Given the initial observation <image_input> and prompt \n\"Open the drawer.\", predict the goal image.\n[Text] [S] Imagine you are the robot in the scene with the action space (x, y, z, roll, pitch, yaw, gripper). Based on the prompt \"Moved the \nsilver pot to the right side of the table.\" and current observation <image_input>, guess the following action. History:\n \n<image_history_1>: ( <extra_id_696> <extra_id_705> <extra_id_726> <extra_id_767> <extra_id_683> <extra_id_711> <extra_id_200> ) \n<image_history_2>: ( <extra_id_705> <extra_id_670> <extra_id_742> <extra_id_753> <extra_id_656> <extra_id_674> <extra_id_200> ) \n<image_history_3>:  ( <extra_id_703> <extra_id_632> <extra_id_728> <extra_id_765> <extra_id_665> <extra_id_656> <extra_id_200> ) \n<image_history_4>: ( <extra_id_696> <extra_id_622> <extra_id_706> <extra_id_750> <extra_id_695> <extra_id_645> <extra_id_200> )\n( <extra_id_687> <extra_id_602> <extra_id_695> <extra_id_750> <extra_id_691> <extra_id_625> <extra_id_200> )\n[Text] [S] What is the robot doing?\nmove the yellow heart towards up\nImage History\nImage Input\nText Target\nText Input\nVisual Observation        Pos X                   Pos Z                   Rot Y               Action\nPoint Goal:           Pos X                   Pos Z\nText Input\nText Target\nVisual Observation     \u25b3 Pos X            \u25b3 Pos Y          \u25b3 Pos Z             \u25b3 Rot X              \u25b3 Rot Y             \u25b3 Rot Z            Gripper\nText Input\nText Input\nImage Input\nImage Target\nText Target\nImage History\nFigure 11. Examples of input and target representations for embodied and robot tasks.\nthe language instructions, also include special tokens to ex-\nplicitly express the interleaved multimodal prompt. The ac-\ntion space consists of primitive actions of \u201cpick and place\u201d\nfor tasks with a suction cup as the end effector or \u201cpush\u201d\nfor tasks with a spatula. Both primitive actions contain two\nposes and one rotation \u2208 R3, specifying the start and target\nstates of the end effector.\nWith the action representation described in B.1, we\nseamlessly add large-scale manipulation datasets Language\nTable [127], BridgeData V2 [184], and FrankaKitchen [63]\nwith the continuous control in both simulated and real-\nworld environments. The model directly predicts the next\naction as the text target based on the current observation\nas image input, previous frames as image history, and lan-\nguage instruction and previous actions as text inputs.\nDue to the non-causality of the model and limited se-\nquence length for the image history, we only added the\nPointNav task from Habitat [157] Gibson scenes for the\nnavigation. The model is required to predict the next action,\nwith random augmentation for predicting the next position\nand rotation state, based on the point goal (positions \u2208 R2),\nvisual observations, and previous actions and states, if any.\nD.11. Task Augmentation\nIn addition to these sources, we derive several additional\ntasks that use the same supervised annotations as other tasks\nbut require performing slightly different functions. We call\nthis task augmentation. The new tasks include prompts that\nspecify the desired output. These tasks serve to add diver-\nsity to our instruction following data. We review the task\naugmentation we construct below.\nSegmentation. We build several augmentations of the seg-\nmentation tasks, including (1) segmenting pixels belonging\nto one of a set of 2-4 categories, possibly including cate-\ngories that do not exist in the image, (2) segmenting pix-\nels belonging to a class and are within an input bound-\ning box and (3) build a map of pixels that do not belong\nto a set 1-4 classes. Prompts are designed for these that\nstate the requirement, e.g., \u201cShow pixels that are part of\nchair, paper and in <extra id 289> <extra id 871> <ex-\ntra id 781> <extra id 1156>\u201d.\nDetection and Referring Expression. For detection, local-\nization, and referring expressions, we also train the model to\noutput various properties of the output bounding boxes in-\nstead of the boxes themselves. Properties include the width,\nheight, area, left/right/top/bottom half, center coordinates,\ndistance from the left/right/top/bottom edge of the image,\nor the coordinates of different corners of the bounding box.\nWe also change the format of the output bounding box (e.g.,\n[x1, y1, w, h] instead of [y1, x1, y2, x2] format), and change\nwhether the model labels the boxes with the object category\nor not.\n31\nPrompt\nModel Response\nA video of a man (woman) saying UNIFIED-IO 2 is a model that works with vision, language, audio, and action.\nn (n)\nA video of a man playing guitar. n\nn\nTable 12. Audio generation samples from the pre-trained model.\nInput image\nRotate right\nRotate right\nMove forward\nMove forward\nRotate right\nMove forward\nFigure 12. Future frame prediction samples from the pre-trained model. Given the initial input image and action, the model can generate a\nplausible frame that reflects the result of the action.\nFor detection, we train the model to detect any object\nbelonging to a set of 1-4 classes. For referring expressions,\nwe train the model to locate multiple referring expressions\nfrom a single query. In this case, we sometimes train the\nmodel to predict a property of both referenced boxes in-\nstead of outputting the directly, for example, which box is\nthe smallest, which is the largest, the area of intersection, a\nbox containing both boxes, etc.\nRelationship Prediction. We train the model to list all re-\nlationships between a particular object in the image and any\nother object. A bounding box and category specify the tar-\nget object. Similarly, we train the model to predict all rela-\ntionships between any instance of a particular class of ob-\njects and any other object in the image.\nCaptioning. For captioning, we train the model to generate\na caption that is longer or shorter than a given character or\nword length or contains a particular word or set of words.\nWe also randomly require the caption to start with a partic-\nular prefix. Again, these requirements are specified in the\nprompt, for example, \u201cGenerate a caption longer than five\nwords for this image. Start your output with the text \u2018My\ncaption is:\u2019\u201d.\nSurface Normal Estimation. For surface normal estima-\ntion, we train the model to generate RGB images that en-\ncode the pixel orientation differently. This includes chang-\ning which RGB channels correspond to the x, y, and z ori-\nentations and only including a subset of those orientations.\nWe also include tasks that require specifying the x, y, and\nz orientation at a particular point specified in the prompt\nusing location tokens. Finally, we include tasks requiring\nsegmentation masks over pixels with particular orientations,\ne.g., \u201cBuild a binary mask over surfaces with an upward ori-\nentation\u201d.\nFigure 13. Image generation samples from the pre-trained model.\nPrompts from left to right: 1) An image of an astronaut riding a\nhorse in the forest. There is a river in front of them with water\nlilies, Fantasy, HD. 2) A image of a cat wearing sunglasses, HD.\n3) A image of a black german shepherd wearing a red beret, HD.\n4) An image of a stop sign in a Fantasy style with the text \u201c1991\u201d.\nEmbodied AI. We further augment the embodiment\ndatasets with the video QA and goal image generation tasks.\nThe QA augmentation aims for the robot\u2019s planning and af-\nfordance. For example, given a robot video trajectory, the\nmodel is supposed to predict the plan (caption), or whether\na given action is reasonable from the language instruction.\nApplying image editing in embodied space, we further let\nthe model generate the goal or subgoal images based on\nthe initial visual observation in the image input and the lan-\nguage prompt in the text input. While recent works show\nthat embodiment QA with VLM [46, 162] and (sub-)goal\ngeneration with diffusion model [134] are effective in the\ndecision-making downstream tasks, our model combines\nthe both augmentation strategies.\nE. Experiment Details\nE.1. Pre-training Visualization\nIn the main paper, we evaluate the effectiveness of our pre-\ntraining by evaluating UNIFIED-IO 2 quantitively on a va-\nriety of benchmarks. Here, we qualitatively show the vi-\n32\nCategorization\nLocalization\nVQA\nRefexp\nSegmentation\nKeypoint\nNormal\nAll\nablation\ntest\nablation test ablation test ablation test ablation test ablation test ablation test ablation test\n0\nNLL-AngMF [11]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n49.6\n50.5\n7.2\n7.1\n1\nMask R-CNN [70]\n-\n-\n44.7\n45.1\n-\n-\n-\n-\n26.2\n26.2\n70.8\n70.6\n-\n-\n20.2\n20.3\n2\nGPV-1 [65]\n33.2\n33.2\n42.8\n42.7\n50.6\n49.8\n25.8\n26.8\n-\n-\n-\n-\n-\n-\n21.8\n21.8\n3\nCLIP [146]\n48.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n6.9\n-\n4\nOFALARGE [186]\n22.6\n-\n-\n-\n72.4\n-\n61.7\n-\n-\n-\n-\n-\n-\n-\n22.4\n-\n5\nGPV-2 [89]\n54.7\n55.1\n53.6\n53.6\n63.5\n63.2\n51.5\n52.1\n-\n-\n-\n-\n-\n-\n31.9\n32.0\n5\nDINO + SAM [94, 139]\n-\n-\n66.0\n66.0\n-\n-\n-\n-\n60.2\n60.1\n-\n-\n-\n-\n18.0\n18.0\n6\nUNIFIED-IOSMALL\n42.6\n-\n50.4\n-\n52.9\n-\n51.1\n-\n40.7\n-\n46.5\n-\n33.5\n-\n45.4\n-\n7\nUNIFIED-IOBASE\n53.1\n-\n59.7\n-\n63.0\n-\n68.3\n-\n49.3\n-\n60.2\n-\n37.5\n-\n55.9\n-\n8\nUNIFIED-IOLARGE\n57.0\n-\n64.2\n-\n67.4\n-\n74.1\n-\n54.0\n-\n67.6\n-\n40.2\n-\n60.7\n-\n9\nUNIFIED-IOXL\n61.7\n60.8\n67.0\n67.1\n74.5\n74.5\n78.6\n78.9\n56.3\n56.5\n68.1\n67.7\n45.0\n44.3\n64.5\n64.3\n9\nUIO-2L\n70.1\n-\n66.1\n-\n67.6\n-\n66.6\n-\n53.8\n-\n56.8\n-\n44.5\n-\n60.8\n-\n10 UIO-2XL\n74.2\n-\n69.1\n-\n69.0\n-\n71.9\n-\n57.3\n-\n68.2\n-\n46.7\n-\n65.2\n-\n11 UIO-2XXL\n74.9\n75.2\n70.3\n70.2\n71.3\n71.1\n75.5\n75.5\n58.2\n58.8\n72.8\n73.2\n45.2\n44.7\n66.9\n67.0\nTable 13. GRIT results and additional baselines from the GRIT leaderboard.\nHellaSwag MMLU Arc Easy Arc Cha. BoolQ\nUIO-2L\n39.4\n28.4\n41.8\n26.2\n66.6\nUIO-2XL\n49.9\n29.7\n49.5\n31.3\n72.8\nUIO-2XXL\n52.7\n30.4\n55.3\n33.5\n77.3\nOpen LLaMA 3B\n52.0\n23.9\n69.3\n33.8\n67.0\nLLaMA 7B\n57.1\n42.6\n76.4\n43.5\n77.7\nLLaMA 7B Chat\n57.7\n47.6\n74.4\n44.0\n80.7\nTable 14. Results on NLP tasks.\nsualizations from the pre-trained UIO-2XXLmodel. Table\n12 shows audio generation from text (top) and text + video\n(bottom). We can see the pre-trained model learns text-to-\nspeech synthesis through video pre-training, and the model\ncan also synthesize music that matches the video input. Fig-\nure 12 shows the future frame prediction samples given the\ninitial input image and action sequence. Figure 13 shows\nthe image generation samples given prompts. The model\nhas a good understanding of different objects. However, it\nstruggles to generate the correct text from the given caption.\nE.2. NLP Results\nWe present results on a set of NLP tasks to evaluate the\nmodel\u2019s language understanding abilities. We evaluate us-\ning the EleutherAI LM-Eval harness [51], tasks are evalu-\nated zero-shot using the default prompts without any adjust-\nments aside from adding the [Text] [S] prefix used for\nall text generation tasks. We evaluate on HellaSwag [214]\nand a selection of other question answering benchmarks:\nMMLU [72], ARC [31], and BoolQ [30]. Results are shown\nin Table 14. Baselines were evaluated in the same setting,\ni.e., zero-shot, with the default prompts, and using LM-\nEval. UNIFIED-IO 2 is generally ahead of Open LLaMA\n3B but behind LLaMA.\nE.3. GRIT Details\nWe present GRIT results in more detail in Table 13. No-\ntably, UNIFIED-IO 2 is the first unified model to pass the\nMasked R-CNN baseline for localization and goes a long\nway toward closing the gap between SAM and unified mod-\nels on segmentation.\nFor GRIT VQA, looking at the scores from GRIT on dif-\nferent VQA subsets, we find that UNIFIED-IO 2 does bet-\nter on the same-source subset (84.6 vs 58.5) but worse on\nthe new-source subset (57.7 vs 67.2). Same-source ques-\ntions come from VQA 2.0, and new-source questions come\nfrom VG, so the difference can be attributed to the kinds\nof questions being asked. Qualitatively, it is hard to under-\nstand why the scores differ on these subsets since the GRIT\nablation questions lack ground truth annotations.\nHow-\never, we notice the models often produce different answers\nwhen faced with ambiguous questions (e.g. \u201cWhat color is\nblack on the horse\u201d, \u201chair\u201d for UNIFIED-IO vs. \u201cmane\u201d for\nUNIFIED-IO 2), so one possibility is that UNIFIED-IO 2\ndoes not match the VG answer style as well as UNIFIED-IO,\nwhich would likely be due to differences in the kind of VQA\ntraining data the models were trained on.\nFor GRIT localization, we find the model can struggle\nwith images with many instances of the target class, par-\nticularly when using beam search.\nWe hypothesize that\nthis is because the probability mass can get split between\nmany similar location tokens, resulting in EOS becoming\nthe most probable token even if its probability is low. As a\nsolution, during inference, we only output EOS if the EOS\n33\nSplits\nMetrics\nUIO-2XXL\nUIO-2XL\nUIO-2L\n[206]\n[207]\n[26]\nRandom\nAccuracy (\u2191)\n90.90\n88.27\n84.03\n88.28\n90.24\n86.90\nPrecision (\u2191)\n94.30\n97.44\n77.73\n94.34\n97.72\n94.40\nRecall (\u2191)\n87.07\n78.60\n95.40\n82.20\n83.00\n79.27\nF1-Score (\u2191)\n90.54\n87.01\n85.66\n87.85\n89.76\n86.19\n% Yes\n46.17\n40.33\n61.37\n44.91\n43.78\n43.26\nPopular\nAccuracy (\u2191)\n88.17\n87.47\n77.27\n86.20\n84.90\n83.97\nPrecision (\u2191)\n89.13\n95.69\n70.03\n89.46\n88.24\n87.55\nRecall (\u2191)\n86.93\n78.47\n95.33\n82.06\n80.53\n79.20\nF1-Score (\u2191)\n88.02\n86.23\n80.75\n85.60\n84.21\n83.16\n% Yes\n48.77\n41.00\n68.07\n45.86\n45.63\n45.23\nAdversarial\nAccuracy (\u2191)\n84.17\n85.77\n72.00\n84.12\n82.36\n83.10\nPrecision (\u2191)\n82.17\n92.01\n65.00\n85.54\n83.60\n85.60\nRecall (\u2191)\n87.27\n78.33\n95.33\n82.13\n80.53\n79.60\nF1-Score (\u2191)\n84.64\n84.62\n77.30\n83.80\n82.00\n82.49\n% Yes\n53.10\n42.57\n73.30\n48.00\n48.18\n46.50\nTable 15. Object hallucination benchmark POPE results, in com-\nparison with mPLUG-Owl2 [206], Ferret [207], and Shikra [26].\nImg\nVideo\nAll\nInstructBLIP [34]\n58.8\n38.1\n53.4\nVideoChat-7B [110]\n39.0\n33.9\n37.6\nOtter-7B [108]\n42.9\n30.6\n39.7\nQwen-VL-7B [12]\n62.3\n39.1\n56.3\nQwen-VL-chat-7B [12]\n65.4\n37.8\n58.2\nmPLUG-Owl2-7B [206]\n64.1\n39.8\n57.8\nLLaVA-1.5-7B [118]\n-\n-\n58.6\nLLaVA-1.5-13B [118]\n68.2\n42.7\n61.6\nUNIFIED-IO 2L\n56.0\n37.5\n51.1\nUNIFIED-IO 2XL\n64.1\n45.6\n60.2\nUNIFIED-IO 2XXL\n65.7\n46.8\n61.8\nTable 16. Results on SEED-Bench [106]. Our XXL model out-\nperforms all 7B vision language models and is even slightly better\nthan the LLaVA-1.5 13B model.\ntoken itself has a probability of over 0.5, which we find sig-\nnificantly improves the performance on crowded images.\nIn rare cases, we observe this leads to the model generat-\ning bounding boxes for the same instance multiple times.\nAs a solution, we apply Non-maximum suppression with a\nhigher threshold of 0.8 to remove these duplicates. We ap-\nply this inference trick for localization and when doing the\ninitial localization step for the keypoint and segmentation\ntasks.\nE.4. Multimodal Benchmark Details\nWe now provide the breakdown results for the evaluation-\nonly multimodal benchmarks, POPE [113] and SEED-\nBench [106]. POPE is the object hallucination benchmark,\nrequiring \u2018yes\u2019 or \u2018no\u2019 answers. As shown in Table 15, our\nlargest model achieves the highest F1 score in all 3 dimen-\nsions. Interestingly, smaller models favored \u2018no\u2019 responses,\npossibly due to a bias from negative examples encountered\nduring the instruction tuning phase. SEED-Bench offers\n19k multiple-choice questions with human annotations for\nevaluating multimodal models across 12 dimensions, in-\ncluding spatial (Image) and temporal (Video) understand-\nGroundtruth\nUIO-2XXL\nWithout CLF\n13.39\nUIO-2XXL\nWith CLF\n33.77\nA pizza with pineapple and\nmeat topping on a metal pizza pan.\nAn elephant standing in\nthe dry grass at the edge of water.\nThe big grey bear is staring at something\nA stack of pan cakes sitting on\ntop of a plate.\nA picture of a vacant home's kitchen and\nliving room.\nFigure 14. Samples generated by UIO-2XXL for the MS COCO\ncaptions [115].\nWhile the classifier-free guidance [74] signifi-\ncantly boosts image quality and fidelity, it achieves poor FID.\ning. As shown in Table 16, our XXL model outperforms all\nother 7B vision/video language models, and is even slightly\nbetter than the LLaVA-1.5 13B model. Notably, our XL\n(3B) model has already outperformed all other counterparts\nin the temporal understanding split. While recent video lan-\nguage models [108, 110, 128] have shown proficiency in\nconventional video tasks like video tagging and caption-\ning, their performance in SEED-Bench\u2019s temporal under-\nstanding is even worse than that of vision language mod-\nels, which might be attributed to their limited instruction-\nfollowing capabilities.\nE.5. Image Generation Details\nFigure 15 shows generated images for the TIFA benchmark\ncaptions [76] using several baselines as well as UIO-2XXL.\nWe use the official implementation code (Emu [172] and\nCoDi [174]) or the images shared in the official GitHub\nrepository of TIFA5 (Stable Diffusion v1.5 [154] and\nminiDALL-E [37]) for baselines. All the baselines except\nminiDALL-E use the Stable Diffusion decoder trained on\nlarge-scale, high-quality image datasets, generating images\nof high fidelity. However, they often generate images that\ndo not fully follow the input captions while UNIFIED-IO 2\ngenerates faithful images.\nFor text-to-image generation on MS COCO [115], we\nfollow the standard convention [226]; we evaluate on a sub-\n5https://github.com/Yushi- Hu/tifa/tree/main/\nhuman_annotations\n34\nEmu\n65.5\nCoDi\n71.6\nSD-1.5\n78.4\nminiDALL-E\n79.4\nUIO-2XXL\n81.3\ntwo laptops a mouse cords\nwires and a monitor\nA red motorcycle parked\nby paint chipped doors.\nA boy wearing a green shirt\nposing with some fruit.\na photo of blue fire hydrant\na photo of bike and skateboard;\nskateboard is left to bike\na mountain with a cloud\nhanging over it\nA Mesoamerican pyramid\nsurrounded by jungle.\ndetailed charcoal sketch.\nA photo of an Athenian vase\nwith a painting of toucans playing tennis\nin the style of Egyptian hieroglyphics\nA Christmas tree with\nlights and teddy bear\nFigure 15. Samples generated for the TIFA bnechmark captions [76]. Some of the images generated by baselines (e.g., rows 1-2, 6, 9) have\nhigh quality but do not fully follow the input text while UNIFIED-IO 2 generates faithful images.\n35\nPrompt\nModel Response\nWhat is the sound of a duck quacking?\nn\nA bell is ringing loudly and quickly.\nn\nSpeak out the text \u201cPolice protection was better and more effective;\u201d for me.\nn\nSpeak: Printing, then, for our purpose, may be considered as the art of making books by means of movable types.\nn\nRead \u201cMany animals of even complex structure which live parasitically within others are wholly devoid of an alimentary cavity.\u201d\nn\nGenerate the music based on the description \u201cSlow tempo, bass-and-drums-led reggae song.\u201d\nn\nBased on the given description \u201cIndustrial techno sounds, repetitive, hypnotic rhythms\u201d, produce a corresponding piece of music.\nn\nTable 17. Audio generation examples. UNIFIED-IO 2 can generate not only environmental sound (rows 1-2), but also speech (rows 3-5)\nand music (rows 6-7). Note that some of the outputs longer than 4.08 seconds have discontinuity in sound, or changes in tone, speed\nor melody (rows 4-5, 7). Since our model can output 4.08-second audio at a time, we complete the audio clip by using any previously\ngenerated clips as additional input. Click n for audio samples.\nset of 30K captions sampled from the validation set.6 Fol-\nlowing [43], we generate 8 images for each caption and se-\nlect the best one using CLIP text-image similarity [146].\nDespite classifier-free guidance [74] resulting in generated\nimages of qualitatively higher quality, the computed FID\nscore [73] is significantly worse compared to what would\nhave been achieved without employing it (33.77 vs 13.39);\nsee Figure 14.\nE.6. Audio Generation Details\nFor text-to-audio generation, we evaluate on the Audio-\nCaps [93] test set. Note that we cannot do an apples-to-\napples comparison with other methods because AudioCaps\nconsists of 10-second audio clips while our model can gen-\nerate 4.08-second audio at a time. Instead, we evaluate the\ndataset in the following setup: we first sample four 2-second\naudio segments, convert them to log-mel-spectrograms with\nzero-padding, and generate the following audio with the\nprompt \u201cGenerate the following sound based on what you\nheard and the description: {caption}\u201d.\nWe convert the\nmodel output, that is, a log-mel-scaled spectrogram, into\na waveform using the pretrained HiFi-GAN, and compare\nthe ground-truth audio and generated audio using compu-\ntational metrics including Fr\u00b4echet Audio Distance [92], In-\nception Score [155] and Kullback\u2013Leibler divergence. We\nuse the same evaluation code as AudioLDM7 [117]. We\nshow the audio generation examples in Table 17 and audio-\nvisual qualitative examples in Table 18.\nE.7. Video and Audio Understanding Details\nWe consider classification and question-answering tasks as\nopen-ended answer generation and use the Exact Match\n(EM) to measure the performance. We also tried to formu-\nlate the classification task as multiple-choice answering and\n6We use the evaluation code at https : / / github . com /\nMinfengZhu/DM-GAN\n7https://github.com/haoheliu/audioldm_eval\nInput Image Prompt\nModel Response\nWhat is the sound of this instrument?\nn\nWhat is the sound of this instrument?\nn\nGenerate music about this scene.\nn\nGenerate music about this scene.\nn\nn Locate the bounding boxes of the sound sources in the given image.\nn Identify the locations of the sound sources in the given image.\nn Identify the locations of the instruments producing the given sound.\nn Identify the locations of the instruments producing the given sound.\nTable 18. Audio-visual qualitative examples showcasing the abil-\nity of UNIFIED-IO 2 to reason across modalities. UNIFIED-IO 2\ncan generate the sound of the instrument in the input image (rows\n1-2), and generate the music that matches the mood of the input\nimage (rows 3-4). The last four examples (rows 5-8) show the re-\nsults of visual sound localization. Note that UNIFIED-IO 2 can\naccurately identify the instruments that make and do not make\nsounds (rows 7-8). Click n for audio samples.\ngenerate answers by computing the logit for each dataset\nlabel and selecting the one with the highest logit, but the\nperformance boost was quite marginal. Note that we do\nnot train our model directly on the Kinetics-400 [90]; we\ninstead train on Kinetics-710, a mixture of three different\ndatasets belonging to the Kinetics family, that is, Kinetics-\n400, 600, and 700. Our model achieves top-1 accuracy 79.1\n36\nGuess the next location of the object from its prior locations: <image_history_1> <extra_id_619> <extra_id_936> <extra_id_714> <extra_id_1014> object.\nUtilizing the object's past locations: <image_history_1> <extra_id_667> <extra_id_932> <extra_id_687> <extra_id_943> object, can you guess its upcoming location?\nFigure 16. Single object tracking examples from LaSOT [50]. Given the first input image and the corresponding bounding box of the target\nobject, our model can track the object across video frames.\nDescribe the video.\nA man is trying to skip stones on a clear, blue lake.\nWhat is the man doing in the video?\nThe man in the video is dancing and posing in \nfront of the camera, showing off his dance moves \nand body movements.\nWhat is the robot doing?\nMove the orange object from the table to the \ntoaster oven.\nFigure 17. Video understanding qualitative examples.\n(vs. instruction tuning only: 73.8) when further finetun-\ning on Kinetics-400 for 5 epochs, following [111].\nFor\nKinetics-Sounds, leveraging both audio and visual inputs\nlargely improves performance (audio-visual: 89.3 vs. video-\nonly: 87.4 vs. audio-only: 38.2). For captioning tasks, we\nuse CIDEr [182] as the evaluation metric. Figure 17 shows\nthe qualitative examples for video understanding tasks.\nE.8. Embodiment Details\nWhat is the final frame after completing the \ntask: \"Move the silver pot to the lower \nright of the burner\"?\nWhat does the goal image look like given \nthe current robot RGB observation and \nprompt 'Pick the green rectangle block \nand put it on the table between the \nyellow and red block'?\nTask: Move the cloth from the left to the \nright side of the burner. Given the start \nstate as image, generate the end state \nimage.\nInput\nPrompt\nPrediction\nGT\nFigure 18. Future state prediction examples on robotic manipula-\ntion tasks. Given the input image and instructions, our model can\nsuccessfully generate the target state after the prompt instruction.\nIn VIMA-Bench [87], there are 4 levels of evaluation\nprotocols: L1 object placement, L2 novel combination, L3\nL1\nL2\nL3\nL4\nAvg.\nVIMA [87]\n81.5\n81.5\n78.7\n48.6\n72.6\nVIMA-Gato [152]\n57.0\n53.9\n45.6\n13.5\n42.5\nVIMA-Flamingo [5]\n47.4\n46.0\n40.7\n12.1\n36.6\nVIMA-GPT [19]\n46.9\n46.9\n42.2\n12.1\n37.0\nUNIFIED-IO 2L\n66.9\n63.8\n57.5\n12.6\n50.2\nUNIFIED-IO 2XL\n70.3\n69.8\n64.5\n13.1\n54.2\nUNIFIED-IO 2XXL\n71.3\n70.4\n68.0\n15.5\n56.3\nTable 19. Evaluations on VIMA-Bench [87]\nnovel object, and L4 novel task. Results and comparisons\nare shown in Table 19. The inputs for the autoregressive\ntransformer model VIMA [87] are object tokens consist-\ning of cropped images and bounding boxes; image patch\ntokens encoded by ViT for VIMA-Gato [152]; image patch\ntokens encoded by ViT, further downsampled by a perceiver\nmodule for VIMA-Flamingo [5]; and single image token\nencoded by ViT for VIMA-GPT [19]. The output of those\nbaselines is all next-step action prediction. Since our model\nhas to predict all actions at once only with the initial ob-\nservation, the task setting is then more challenging than the\ncasual policy learning baselines. Nevertheless, our mod-\nels still outperform counterparts that input image or im-\nage patches for all 4 levels and are only behind the object-\ncentric method [87]. In Figure 18, we show the future state\nprediction examples on robotic manipulation tasks. Given\nthe input state image and natural language prompt, our\nmodel can successfully synthesize the target image state.\nE.9. Other Tasks\nFigure 16 shows single object tracking examples from the\nLaSOT [50] dataset.\nNote that UNIFIED-IO 2 does not\nuse specific class labels for tracking and tracks small mov-\ning objects such as a table tennis paddle well. Figure 19\npresents qualitative examples of 3D object detection from\nthe Objectron dataset [3]. As outlined in our main paper,\nUNIFIED-IO 2 exhibits suboptimal performance in bench-\nmarks for multi-object 3D detection.\nAdditionally, Fig-\nure 20 illustrates examples of image-based 3D view syn-\nthesis using the Objaverse dataset [40]. While the model\nproduces coherent results, it faces challenges in accurately\n37\nFigure 19. 3D object detection qualitative examples from Objec-\ntron [3].\nInput\nPrompt\nPrediction\nGT\nRelative camera transformation is in the \nformat of \u201ctheta sin(phi) cos(phi) r\u201d. What is \nthe image after applying the transformation:\n( <extra_id_608> <extra_id_493> \n<extra_id_1154> <extra_id_661> )\nComplete a new image of the image following \nthe implementation of the camera \ntransformation theta sin(phi) cos(phi) r: ( \n<extra_id_616> <extra_id_1011> \n<extra_id_309> <extra_id_729> ).\n[Complete a new image of the image \nfollowing the implementation of the camera \ntransformation theta sin(phi) cos(phi) r: ( \n<extra_id_789> <extra_id_464> \n<extra_id_258> <extra_id_450> )\nFigure 20. Image-based 3D view synthesis examples from Obja-\nverse [40].\nrepresenting relative camera transformations.\n38\n"
  },
  {
    "title": "Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math",
    "link": "https://arxiv.org/pdf/2312.17120.pdf",
    "upvote": "24",
    "text": "Generative AI for Math: Part I\nMATHPILE: A Billion-Token-Scale Pretraining Corpus for Math\nZengzhi Wang3,4\nRui Xia3\nPengfei Liu1,2,4\u2217\n1Shanghai Jiao Tong University 2Shanghai Artificial Intelligence Laboratory\n3Nanjing University of Science and Technology 4Generative AI Research Lab (GAIR)\nzzwang@njust.edu.cn\nrxia@njust.edu.cn\npengfei@sjtu.edu.cn\nhttps://github.com/GAIR-NLP/MathPile/\nAbstract\nHigh-quality, large-scale corpora are the cor-\nnerstone of building foundation models. In\nthis work, we introduce MATHPILE, a diverse\nand high-quality math-centric corpus compris-\ning about 9.5 billion tokens. Throughout its\ncreation, we adhered to the principle of \u201cless\nis more\u201d, firmly believing in the supremacy\nof data quality over quantity, even in the pre-\ntraining phase. Our meticulous data collection\nand processing efforts included a complex suite\nof preprocessing, prefiltering, language identi-\nfication, cleaning, filtering, and deduplication,\nensuring the high quality of our corpus. Fur-\nthermore, we performed data contamination\ndetection on downstream benchmark test sets\nto eliminate duplicates. We hope our MATH-\nPILEcan help to enhance the mathematical rea-\nsoning abilities of language models. We plan\nto open-source different versions of MATH-\nPILEwith the scripts used for processing, to\nfacilitate future developments in this field.\n1\nIntroduction\nPowerful conversational models such as Chat-\nGPT (OpenAI, 2022) and Claude (Anthropic,\n2023) are significantly transforming numerous\nproducts and aspects of daily life. A crucial factor\nin their success is the strength of the foundational\nlanguage model. State-of-the-art foundation mod-\nels are typically pretrained using massive, diverse\nand high-quality corpora, encompassing sources\nlike Wikipedia, scientific papers, community fo-\nrums, Github code, web pages and more (Gao\net al., 2021; Together, 2023a). We expect a power-\nful foundational language model to possess com-\nprehensive and balanced capabilities, including\nlanguage understanding, commonsense reasoning,\nmathematical reasoning, language generation, and\nmore (Bubeck et al., 2023).\n\u2217Corresponding author\nData Preprocessing \n& Prefiltering\nLanguage ID\nCleaning & Filtering\nDeduplication\n10.43B Tokens\n10.20B Tokens\n10.18B Tokens\nMath-Centric\nDiversity\nDefinitions\nTextbooks\nWeb\nLecture Notes\narXiv\nStackExchange\nWikipedia\nTheorem Proofs\nHigh-Quality\nPreprocessing\nPrefiltering\nLanguage ID\nCleaning & Filtering\nDedup.\nData Contamination Detect.\nData \nDocumentation\nDataset Sheet\nQuality Annotations\nLength Distribution\nData Statistics\nOpen-Source\n9.5B Tokens\n\u201cLess is More\u2019\u2019 Principle\nK-12\nCollege\nPostgraduate\nMath Competition\nFigure 1: Key features of MATHPILE.\nIn this work, our concern centers on mathemati-\ncal reasoning capabilities within foundational lan-\nguage models (Chern et al., 2023; Azerbayev et al.,\n2023b, inter alia), for which can potentially boost\nthe application in education tools, automated prob-\nlem solving, data analysis, code programming and\nso on, thereby improving user experience. To facil-\nitate this, we are not directly building a model, but\nrather focusing on a more fundamental aspect: cre-\nating a high-quality and diverse pre-training cor-\npus tailored for the math domain, namely MATH-\nPILE. Specifically, our work is significantly dif-\nferent from the previous work in the following\ncharacteristics (See Table 1 for comparison):\nMath-centric. Previous open-sourced pretrain-\ning corpora have typically focused on general\ndomains, such as Pile (Gao et al., 2021), Red-\nPajama (Together, 2023a) and Dolma (AllenAI,\n2023).\nOthers have concentrated on multilin-\ngual aspects or programming languages, such\nas ROOTS (Lauren\u00e7on et al., 2022) and The\nStack (Kocetkov et al., 2022), respectively. How-\never, a notable absence in these offerings is a cor-\npus specificlly tailoring for mathematics. While\narXiv:2312.17120v1  [cs.CL]  28 Dec 2023\nthere exist some corpora designed for training\nor continually improving math-specific language\nmodels, such as Minerva\u2019s mathematical training\ndataset (Lewkowycz et al., 2022) and OpenAI\u2019s\nMathMix (Lightman et al., 2023), these are not\nopen-sourced. Note that a recent work concurrent\nwith ours, OpenWebMath (Paster et al., 2023), al-\nthough math-centric, is solely sourced from web\npages. We will discuss the comparison with it later.\nRecognizing this gap, our work aims to bridge the\ndivide by developing an open-sourced mathemat-\nical corpus, democratizing access to high-quality\nmathematical data and enabling researchers and\ndevelopers to advance the capabilities of language\nmodels in mathematical reasoning more effectively\nand inclusively.\nDiversity. While Hendrycks et al. (2021b) in-\ntroduced AMPS, a problem set ranging from el-\nementary mathematics to multivariable calculus\n(K-12 level) for pre-training purposes, it lacks\ncontent at the college-level and more challenging\ncompetition-level mathematics, focusing instead\non a supervised dataset rather than an extensive\ncorpus. The ProofPile corpus, introduced by Azer-\nbayev et al. (2023a), aims to improve autoformal-\nization and formal proving capabilities in models,\nyet its scope is confined to formal proving, not cov-\nering the broader mathematical domain from K-12\nto postgraduate level. Concurrently with our work,\nPaster et al. (2023) propose the OpenWebMath\ncorpus, featuring a corpus composed of mathemat-\nical web pages. However, our corpus goes beyond\nweb pages, integrating high-quality mathematics\ntextbooks, lecture notes, scientific papers from\narXiv in the field of mathematics, and carefully\nselected content from StackExchange, ProofWiki,\nand Wikipedia among others, which positions our\ncorpus as a richer and more diverse mathematical\nresource for language models.\nHigh-Quality. Recent studies have increasingly\nhighlighted the detrimental effects of low-quality\nand repeated content in pretraining corpora on\nmodel training, as evidenced in various works (Al-\nlamanis, 2019; Luccioni and Viviano, 2021; Lee\net al., 2022; Hernandez et al., 2022; Longpre et al.,\n2023). The importance of high-quality datasets\nhas thus come to the fore. It has been shown that\nproperly filtered and deduplicated web data can\nyield models as equally powerful as those trained\non curated, high-quality corpora (Penedo et al.,\n2023). This similar practice has been recently\nadopted in several notable studies (Cerebras, 2023;\nAllenAI, 2023; Together, 2023b). A notable ex-\nample is the 1.3 billion-parameter code-focused\nmodel pretrained on synthetically generated text-\nbooks and filtered web pages, a project that broke\nexisting scaling laws although did not open source\nits data (Gunasekar et al., 2023). It\u2019s important to\nemphasize that quality of the corpus is far more sig-\nnificant than its quantity. For instance, OpenAI\u2019s\nMathMix comprises only 1.5 billion tokens. In this\nwork, we diligently adhere to the principle of less is\nmore, as outlined in Zhou et al. (2023). To achieve\na high-quality corpus, we have undertaken exten-\nsive preprocessing, prefiltering, cleaning, filtering,\nand deduplication efforts. We are committed to\ncontinually refining and optimizing this corpus,\nstriving for excellence in every aspect to make a\ndistinct contribution to the math domain.\nData Documentation. Auditing large-scale pre-\ntraining corpus, such as carefully documenting\nthe characteristics of the data, intended uses, its\ninformation content, and any potential biases is\nof paramount importance (Bender and Friedman,\n2018; Gebru et al., 2021; McMillan-Major et al.,\n2023). Despite growing advocacy for such prac-\ntices, many pre-training corpus are released with-\nout detailed data documentation due to their large\nsize (Mitchell et al., 2022). Recently, some works\nhave audited certain publicly available pre-training\ndatasets that previously lacked thorough documen-\ntation. These audits found that these corpus po-\ntentially contain useless content (e.g., hate speech,\nsexually explicit content) (Luccioni and Viviano,\n2021; Kreutzer et al., 2022; Elazar et al., 2023),\ncopyright-violating content (Bandy and Vincent,\n2021), and the test sets for downstream tasks (Alla-\nmanis, 2019; Dodge et al., 2021). Adhering stead-\nfastly to the principle of enhancing transparency in\npretraining corpora for practitioners following pre-\nvious efforts, we have provided a dataset sheet for\nour MATHPILE(see Table 5). Throughout our ex-\ntensive data processing workflow, numerous docu-\nments were annotated for quality, such as language\nidentification scores and the ratio of symbols to\nwords (as exemplified in Figure 4). These qual-\nity annotations enable future users to apply their\nspecific filters based on these scores. Additionally,\n2\nA document from MATHPILE-Textbooks\nText:\n# LINEAR TORIC FIBRATIONS\nSANDRA DI ROCCO\n## INTRODUCTION TO TORIC FIBRATIONS\nDefinition 1.1. A toric fibration is a surjective flat map f : X \u2192 Y with connected fibres where\n(a) X is a toric variety\n(b) Y is a normal algebraic variety\n(c) dim(Y ) < dim(X).\nRemark 1.2. Observe that if f : X \u2192 Y is a toric fibration then Y and a general fiber F are also toric varieties. Moreover if X is smooth, respectively Q-factorial then so is Y and\nF .\nCombinatorial characterization. A toric fibration has the following combinatorial characterization (see [EW, Chapter VI] for further details). Let X = X\u03a3, where \u03a3 \u2282 N \u223c\n= Zn,\nbe a toric variety of dimension n and let i : \u2206 ,\u2192 N a sublattice.\nProposition 1.3. [EW] The inclusion i induces a toric fibration if and only if:\n(a) \u2206 is a primitive lattice, i.e. (\u2206 \u2297 R) \u2229 N = \u2206.\n(b) For every \u03c3 \u2208 \u03a3(n), \u03c3 = \u03c4 + \u03b7, where \u03c4 \u2208 \u2206 and \u03b7 \u2229 \u2206 = {0} (i.e. \u03a3 is a splitfan).\nWe briefly outline the construction. The projection \u03c0 : N \u2192 N/\u2206 induces a map of fans \u03a3 \u2192 \u03c0(\u03a3) and thus a map of toric varieties f : X \u2192 Y . The general fiber F is a\ntoric variety defined by the fan \u03a3F = {\u03c3 \u2208 \u03a3 \u2229 \u2206}.\nWhen the toric variety X in a toric fibration is polarized by an ample line bundle L we will call the pair (f : X \u2192 Y, L) a polarized toric fibration. Observe that the polarized toric\nvarieties (X, L) and\n\u0000F, L|F\n\u0001, for a general fiber F , define lattice polytopes P(X,L), P(F, L|F ). The polytope P(X,L) is in fact a \"twisted sum\" of a finite number of\nlattice polytopes fibering over P(F, L|F ). Definition 1.4. Let R0, . . . , Rk \u2282 \u2206 be polytopes. Let \u03c0 : M \u2192 \u039b be a surjective map of lattices such that \u03c0 (Ri) = vi and\nthe v0, \u00b7 \u00b7 \u00b7 , vk are distinct vertices of Conv (v0, . . . , vk). We will call a Cayley \u03c0-twisted sum (or simply a Cayley sum) of R0, . . . , Rk a polytope which is affinely\nisomorphic to Conv (R0, . . . , Rk). We will denote it by:\n[R0 \u22c6 . . . \u22c6 Rk]\u03c0\nIf the polytopes Ri are additionally normally equivalent, i.e. they define the same normal fan \u03a3Y , we will denote the Cayley sum by:\nCayley (R0, . . . , Rk)(\u03c0,Y ) .\nThese are the polytopes that are associated to a polarized toric fibration. Consider a sublattice i : \u2206 ,\u2192 N and the dual lattice surjection \u03c0 : M \u2192 \u039b.\nProposition 1.5. [CDR08] The sublattice i : \u2206 ,\u2192 N induces a polarized toric fibration (f : X \u2192 Y, L) if and only if P(X,L) = Cayley (R0, . . . , Rk)(\u03c0,Y ) for some\nnormally equivalent polytopes R0, . . . , Rk.\nThe polarized general fiber\n\u0000F, L|F\n\u0001\ncorresponds to the polarized toric variety associated to the polytope P(F, L|F ) = Conv (v0, . . . , vk) and the polytopes R0, \u00b7 \u00b7 \u00b7 , Rk\ndefine the embeddings of the invariant sections polarized by the restrictions of L.\nExample 1.6. Consider the Hirzebruch surface F1 = Blp\n\u0010\nP2\u0011\n= P\n\u0010\nOP1 \u2295 OP1 (1)\n\u0011\npolarized by the tautological line bundle \u03be = 2\u03d5\u2217 \u0010\nOP2 (1)\n\u0011\n\u2212 E where \u03d5 is the\nblow-up map and E the exceptional divisor. The associated polytope is P = Cayley (\u22061, 2\u22061).\nFIGURE 1. The Hirzebruch surface P\n\u0010\nOP1 \u2295 OP1 (1)\n\u0011\nExample 1.7. More generally:\n- when \u03c0(P ) = \u2206t the polytope Cayley (R0, . . . , Rk)(\u03c0,Y ) defines the variety P (L0 \u2295 . . . \u2295 Lk ), where the Li are ample line bundles on the toric variety Y , polarized\nby the tautological bundle \u03be. In particular L|F = OPt (1).\n- When \u03c0(P ) is a simplex (not necessarily smooth) Cayley (R0, . . . , Rk)(\u03c0,Y ) defines a Mori-type fibration. A fibration whose general fiber has Picard rank one. - When\n\u03c0(P ) = s\u2206t then again the variety has the structure of a Pt-fibration whose general fiber F is embedded via an s-Veronese embedding:\n\u0000F, L|F\n\u0001\n=\n\u0010\nPt, OPt (s)\n\u0011\n.\nFor general Cayley sums, [R0 \u22c6 . . . \u22c6 Rk]\u03c0, one has the following geometrical interpretation. Let (X, L) be the associated polarized toric variety and let Y be the toric variety\ndefined by the Minkowski sum R0 + . . . + Rk. The fan defining Y is a refinement of the normal fan of Ri for i = 0, . . . , k. Consider the associated birational maps\n\u03d5i : Y \u2192 Yi, where (Yi, Li) is the polarized toric variety defined by the polytope Ri. The line bundles Hi = \u03d5\u2217\ni (Li) are nef line bundles on Y . Denote by the same\nsymbol the maps of fans \u03d5i : \u03a3Y \u2192 \u03a3Yi . Define then the fan:\n\u03a3Z :\nn\n\u03d5\u22121\ni\n\u0000\u03c3j\n\u0001\n\u00d7 \u03b7l, for all \u03c3j \u2208 \u03a3Yi , \u03b7l \u2208 \u03a3\u2206\no\nwhere \u039b = Conv (v0, . . . , vk). It is a refinement of \u03a3X and thus the defining variety Z is birational to X. Moreover it is a split fan and thus it defines a toric fibration\nf : Z \u2192 Y . The Cayley sum [R0 \u22c6 . . . \u22c6 Rk]\u03c0 is the polytope defined by the nef line bundle \u03d5\u2217(L), and the polytopes Ri are the polytopes defined by the nef line bundles\nHi on the invariant sections.\nHistorical Remark. The definition of a Cayley polytope originated by what is \"classically\" referred to as the Cayley trick. We first recall the definition of Resultant and Discriminant.\nLet f1(x), . . . , fn(x) be a system of n polynomials in n variables x = (x1, . . . , xn) supported on A \u2282 Zn. This means that fi = \u03a0aj \u2208Acjxaj . The resultant (of\nA ), RA\n\u0000cj ), is a polynomial in the coefficients cj, which vanishes whenever the corresponding polynomials have a common zero.\nThe discriminant of a finite subset A, \u2206A , is also a polynomial \u2206A\n\u0000cj\n\u0001\nin the variables cj \u2208 A which vanishes whenever the corresponding polynomial has a multiple root.\nTheorem 1.8. [GKZ][Cayley Trick] The A-resultant of the system f1, . . . , fn equals the Adiscriminant of the polynomial:\np(x, y) = fi(x) +\nn\nX\n2\nyi\u22121fi(x).\nLet Ri = N (fi) \u2282 Rn be the Newton polytopes of the polynomials fi. The Newton polytope of the polynomial p(x, y) is the Cayley sum [R1 \u22c6 . . . \u22c6 Rn]\u03c0, where\n\u03c0 : R2n\u22121 \u2192 Rn\u22121 is the natural projection such that \u03c0\n\u0000[R1 \u22c6 . . . \u22c6 Rn]\u03c0\n\u0001\n= \u2206n\u22121.\n...\nSubset: Textbooks\nmeta:\nbook_name: Linear Toric Fibrations_Sandra Di Rocco,\ntype: Notes,\n...\nFigure 2: An example textbook document in MATHPILE\n3\nwe have conducted extensive deduplication for this\ncorpus and performed data contamination detection\nwith downstream benchmark test sets, removing\nany duplicated samples identified (cf. \u00a7 3.4). In-\nterestingly, we have also discovered a significant\nnumber of questions from downstream test sets in\nOpenWebMath (cf. \u00a7 3.4). This underscores the\nimportance of meticulous data documentation. We\nplan to release different versions of MATHPILEto\nfacilitate future use, further emphasizing the utility\nand adaptability of our work. See Appendix B for\nexamples in MATHPILE.\nIn conclusion, we hope to facilitate the growth of\nthe field of AI for mathematics by contributing this\nspecialized, high-quality, diverse corpus focused\non the mathematical domain while maintaining ut-\nmost transparency about the data for practitioners.\nOur work lays the groundwork for training more\npowerful mathematical problem-solving models in\nthe future.\n2\nThe Collection of Corpus\nIn order to construct MATHPILE, we gather data\nfrom a variety of sources, which also includes a\ncomponent of manual collection.\n2.1\nMathematical Textbooks\nTextbooks are typically self-contained, encompass-\ning mathematical concepts, exercises, and detailed\nsolution steps. We believe that such resources are\nvaluable for educational purposes, not only for\nhuman but also machine learning models. Some\nrecent works have also corroborated this point,\neven though they didn\u2019t focus on the math domain,\nand their textbooks are not genuine but were syn-\nthesized from more advanced models (Gunasekar\net al., 2023; Li et al., 2023).\nTo collect these genuine and high-quality text-\nbooks, we began by conducting extensive manual\nsearches across the internet, seeking open-source\nand freely accessible mathematics-related textbook\nwebsites. Afterwards, we proceeded to download\nthese PDF files, resulting in a collection of 38 K-12\nlevel textbooks, along with 369 college-level math-\nematics textbooks that cover a wide range of sub-\njects including linear algebra, probability theory,\ncalculus, and optimization. In addition to these\ntextbooks, we also included 467 college course\nhandouts and lecture notes, which tend to be more\nconcise compared to full-length textbooks. Subse-\nquently, we employed the Mathpix API1 to parse\nthe PDFs into markdown format. Then, we metic-\nulously cleaned up extraneous elements such as\nparsed image URLs, preface sections, table of con-\ntents, acknowledge sections, index sections, and\nconsecutive empty lines within the parsed content.\nAfter that, we arrived at a total of 874 documents.\nWe also refined high-quality mathematics-\nrelated synthetic textbooks from OpenPhi Project.2\nIt is an open-source counterpart to the Phi\nwork (Gunasekar et al., 2023). While the underly-\ning model and generation process differ, the output\nencompasses a broad spectrum of subjects, extend-\ning beyond programming. To isolate mathematics-\nrelated documents, we employed a straightfor-\nward criterion: the presence of the symbol \u201c$$\u201d,\ncommonly associated with mathematical expres-\nsions. This approach yielded 3,889 documents\nfrom an initial pool of 124,493. As the volume of\npre-training data escalates, the synthesis of high-\nquality data becomes increasingly crucial. More\nadvanced filtering methods and mathematical cor-\npora synthesis are left for future exploration.\n2.2\nMathematical Papers from ArXiv\nArXiv offers a free distribution service and serves\nas an open-source archive housing millions of sci-\nentific papers. It also provides invaluable train-\ning data for numerous powerful language mod-\nels (Touvron et al., 2023a; Together, 2023a, inter\nalia). In our endeavor to collect mathematical pa-\npers from ArXiv, we identify 50 sub-subjects span-\nning Mathematics, Computer Science, Statistics,\nPhysics, Quantitative Finance and Economics. Our\nprocess involved filtering ArXiv\u2019s metadata3 to fo-\ncus on the chosen subjects (cf. Table 6), followed\nby accessing the source LaTex files (if available).\nWe exclusively retained the LaTex files and con-\nsolidated multiple files based on their respective\norder as indicated by commands such as \u201cinclude\u201d\nand \u201cinput\u201d within the main LaTex file of each\npaper. Subsequently, we undertook extensive trans-\nformations to enhance data clarity and consistency.\nSpecifically, we\n1https://mathpix.com/ocr\n2https://huggingface.co/open-phi\n3https://www.kaggle.com/datasets/\nCornell-University/arxiv\n4\nDatasets\nOpen\nSource\nType\nTarget Domain\n# Textbooks\nHas Synth.\nData\nData Contam.\nDetection\n# Tokens\nSource\nMinerva\n\u2717\nCorpus\nGeneral Math\n\u2717\n\u2717\n\u2713\n38.5B\narXiv, Web\nMathMix\n\u2717\nCorpus + PS\nGeneral Math\n?\n\u2713\n\u2713\n01.5B\n?\nProofPile\n\u2713\nCorpus\nTheorem Proving\n7\n\u2717\n\u2717\n08.3B\narXiv, Textbooks, Lib.,\nStackExchange, ProofWiki, MATH\nOpenWebMath\n\u2713\nCorpus\nGeneral Math\n\u2717\n\u2717\n\u2717\n14.7B\nWeb\nDM-Mathematics\n\u2713\nPS\nMath Competition\n\u2717\n\u2713\n-\n04.4B\nSynthesis\nAMPS\n\u2713\nPS\nMath Competition\n\u2717\n\u2713\n\u2717\n00.7B\nKhan Academy, Synthesis\nMATHPILE(Ours)\n\u2713\nCorpus\nGeneral Math\n3,979\n\u2713\n\u2713\n09.5B\narXiv, Textbooks, StackExchange,\nWikipedia, ProofWiki, Web\nTable 1: The comparison of MATHPILEwith other mathematical Corpora. PS denotes the problem set type. For\nsome corpora that are not open-sourced, details are unknown and comparisons are based only on information from\ncorresponding papers, with unknowns indicated by \u201c?\u201d. Note that token counts vary with different tokenizers; we\nprimarily copy statistics from each dataset\u2019s technical report. For our corpus, we default to using the GPTNeoX-20B\ntokenizer (Black et al., 2022). DM-Mathematics was introduced in Saxton et al. (2019). We use \u201cMinerva\u201d to refer\nto the dataset adopted by Minerva. Note that ProofPile-2 (Azerbayev et al., 2023b), which includes OpenWebMath,\nRedPajama\u2019s arXiv subset (non-math-centric) and algebra code, is not included in this comparison.\n1) removed comments in each paper;\n2) reverted\nmany\nmacro\ncommands\n(e.g.,\n\u201cnewcommand\u201d) to their original forms;\n3) omitted figure environments while retaining\ncaptions and figure labels;\n4) excluded acknowledgements sections;\n5) eliminated references in each paper;\n6) condensed more than three consecutive empty\nlines to two;\n7) replaced certain formatting commands like\n\u201chfill\u201d and \u201cvspace\u201d with an empty line;\n8) replaced the \u201cmaketitle\u201d command in the\nmain document body with the actual title (if\navailable);\n9) preserved only the content within the main\nbody of the LaTex document.\nFinally, we had a grand total of 347,945 meticu-\nlously cleaned LaTex documents (around 8.5 bil-\nlion tokens), with each document corresponding to\na single paper.\n2.3\nMathematical Entries in Wikipedia\nWikipedia4 is one the largest and most popular\nfree online encyclopedias, offering information\non a wide range of topics, including history, sci-\nence, technology, culture, and more.\nThis ex-\ntensive knowledge has proven to be highly ben-\neficial for numerous natural language processing\ntasks (Lewis et al., 2020, inter alia) and pretrained\nlanguage models (Devlin et al., 2019; Touvron\n4Wikipedia is licensed under CC BY-SA 4.0.\net al., 2023a, inter alia). To collect mathemati-\ncal entries from Wikipedia, we downloaded the\nmathematics-focused (without pictures) dump of\nWikipedia in English for the month of August\n2023. We extracted the HTML documents from\nthe dump using the library libzim,5 resulting in\napproximately 106,900 documents. Subsequently,\nwe converted these HTML documents into mark-\ndown format using the html2text library6 while\nremoving the hyperlinks following the practice of\nLLaMA (Touvron et al., 2023a). We retained the\nalternative text content but excluded image (often\nin SVG format) paths. Additionally, we eliminated\nextra newlines within paragraphs and condensed\nmore than three consecutive empty lines to two\nusing regular expressions. Further refinement in-\nvolved the removal of boilerplate content at the bot-\ntom of the pages, typically denoted with phrases\nlike \u201cThis article is issued from Wikipedia.\nThe text is ...\u201d. In the end, our efforts yielded\na collection of 106,881 mathematical Wikipedia\nentries, about 0.8 billion tokens.\n2.4\nEntries from ProofWiki\nProofWiki,7 an online compendium of mathemati-\ncal proofs, has been instrumental in advancing the\nfields of autoformalization and formal proof prov-\ning, as evidenced by NaturalProofs (Welleck et al.,\n2021) and ProofPile (Azerbayev et al., 2023a). We\n5https://pypi.org/project/libzim/\n6https://pypi.org/project/html2text/\n7ProofWiki is licensed under CC BY-SA 3.0.\n5\nsourced data from the ProofWiki dump dated April\n9, 2022 (provided by the Internet Archive), mir-\nroring the preprocessing approach employed by\nNaturalProofs, which was based on the version\nfrom November 12, 2020. Specifically, this in-\nvolved leveraging the BeautifulSoup8 to parse\nall wiki pages followed by the extraction of raw\ntext content using the wikitextparser library.9\nThis process yielded a substantial collection of\nmathematical content, totaling about 7.6 million\ntokens, comprising 10,328 definitions and 13,511\ntheorem-proof pairs. To facilitate better data or-\nganization, we formatted the definitions using the\n\u201cdefinition\u201d environment, and the theorem-proof\npairs within the \u201csection\u201d environment with their\nrespective titles serving as the section headings, in\nline with the format of ProofPile.\n2.5\nMathematical Discussions on\nStackExchange\nStackExchange,10 renowned for its network of\ncommunity-powered question-and-answering web-\nsites, spans a wide array of topics, each concen-\ntrated on a particular topic. Its high-quality data\ntrove has significantly contributed to the develop-\nment of various language models (Touvron et al.,\n2023a; Zhou et al., 2023, inter alia). In our study,\nwe identify eleven sites within this network, includ-\ning five dedicated to mathematics (such as Mathe-\nmatics and MathOverflow) and six others in closely\nrelated fields like Physics (cf. Table 7).\nOur data collection process began with down-\nloading the site dumps from August 2023 (pro-\nvided by the Internet Archive). In our data curation,\nwe only retained the essential components in the\nposts, namely questions and answers (also associ-\nated meta information). To convert HTML docu-\nments to raw text, we utilized the BeautifulSoup\nlibrary, coupled with a meticulous removal of in-\nvalid XML characters. We then systematically\npaired questions and their respective answers. Each\nquestion typically garners multiple responses, each\nwith its own score and in some cases, an endorse-\nment as the accepted answer by the questioner.\nTo ensure the high quality of our dataset, we\n8https://pypi.org/project/beautifulsoup4/\n9https://pypi.org/project/wikitextparser/\n10ProofWiki is licensed under CC BY-SA 2.5 3.0 or 4.0,\ndepending on the date of the content.\nleveraged two filtering score thresholds: a ba-\nsic quality threshold set at 5, and a more strin-\ngent one at 10.\nQuestions were filtered based\non these thresholds, while answers were judged\nby the lesser of the threshold or the score of\nthe accepted answers if one exists. Additionally,\nwe also retained unanswered questions with at\nleast a score of 10 as a reserve to facilitate fu-\nture use.11 Finally, our process has yielded a rich\ncollection of data: 176,962 mathematics-intensive\nquestions with 290,570 answers (filtered by the\n5-score threshold), and 3,418 unanswered ques-\ntions (10-score threshold). For the sites potentially\nrelated to mathematics, we have gathered 90,957\nquestions with 144,559 answers (5-score thresh-\nold). In total, the assembled questions and answers\n(5-score threshold), including filtered unanswered\nquestions, amount to about 254 million tokens.\n2.6\nMathematical Web Pages from Common\nCrawl\nCommon Crawl,12 an invaluable resource that has\nbeen archiving a comprehensive and open repos-\nitory of web crawl data since 2007, stands as a\ncornerstone for training many advanced language\nmodels, including GPT-3 (Brown et al., 2020) and\nLLaMA. In our endeavor to extract mathematical\nweb pages, we focus on refining the web corpus\nfrom SlimPajama (Cerebras, 2023)-a cleaned and\ndeduplicated counterpart of RedPajama, specifi-\ncally targeting the SlimPajama-CommonCrawl and\nSlimPajama-C4 subsets.\nEschewing the common approach of using neu-\ntral network-based filtering, we opt for heuristic\nrule-based methods. Our procedure began with\nthe creation of TF-IDF features, derived from our\ncurated high-quality textbooks (cf. \u00a7 2.1). Dur-\ning this process, we removed the stop words, lim-\nited the features to a maximum of 10,000, and\nemployed white space tokenization. Upon the ob-\nservation of the resulting vocabulary, we identified\n11 commonly used LaTex commands, integral to\nmathematical expressions. We utilize these com-\nmands as a basis for a hard match within each doc-\nument. A document is classified as mathematical if\nit contains any of these commands along with the\nsymbol \u201c$$\u201d, typically indicative of a mathemati-\n11We also provide an unfiltered version for future use.\n12https://commoncrawl.org/terms-of-use/\n6\nData Preprocessing \n& Prefiltering\nLanguage ID\nCleaning & Filtering\nDeduplication\n520B\n29GB\n903k Docs, ~9.5B Tokens\nData Sources\n2.2TB, ~520B Tokens\nTextbooks \n(0.44%)\nWikipedia \n(2.51%)\nProofWiki \n(2.64%)\nCommon Crawl \n(8.32%)\narXiv (38.07%)\nStackExchange \n(48.02%)\nFigure 3: The creation process of MATHPILE. Beginning with data collection from diverse sources (about 520 B\ntokens), followed by our rigorous processing process, we obtain a math-centric corpus, encompassing 9.5 billion\ntokens. Note that we additionally perform data contamination detection on benchmark test sets (cf. \u00a7 3.4). We\nvisualize the proportions of different components in MATHPILEbased on document counts per component (Right).\ncal document. This rule-based approach, though\nsimplistic, proved to be highly effective, especially\ngiven the vast size of the Common Crawl corpus\n. We also experimented with more intricate dense\nembedding-based methods to identify mathemati-\ncal documents, but these resulted in poor recall.\nOur efforts resulted in the compilation of a\nsubstantial collection of mathematical web pages:\n4,307 documents from the SlimPajama-C4 training\nset and 72,137 documents from the SlimPajama-\nCommonCrawl training set, totaling approximately\n633 million tokens. Note that there are possibilities\nfor more efficient and effective methods to filter\nmathematical documents from the broader expanse\nof Common Crawl snapshots, a venture we aim to\npursue in our future work.\n3\nGlobal Data Processing\nWhile we have already conducted specific data pre-\nprocessing for each data source during the data col-\nlection process, we subsequently engage in three\ncritical steps: language identification, filtering,\nand deduplication, to ensure the quality of the\nentire corpus, as shown in Figure 3.\n3.1\nLanguage Identification\nTo filter non-English documents, we utilized the\nfastText language identifier, which was trained on\nWikipedia, Tatoeba, and SETimes (Joulin et al.,\n2017; Grave et al., 2018). A common practice is\nto classify a document as its respective language if\nthe score exceeds 0.5, a threshold also employed\nby CCNet (Wenzek et al., 2020). However, during\nthe application of this practice, we encountered\na considerable number of false positives\u2014cases\nwhere documents were erroneously filtered as non-\nEnglish when, in fact, they were written in English\nbut contained a substantial amount of mathemati-\ncal symbols. We attribute this issue to the domain\ngap between the datasets used for fastText training\n(primarily wiki and news domain) and our mathe-\nmatical content.\nTo more accurately filter out non-English doc-\numents, we set a customized score threshold for\neach data source to classify documents as English.\nSpecifically, we set thresholds at 0.1 for Wikipedia\nand StackExchange, 0.3 for arXiv, 0.5 for Common\nCrawl. For ProofWiki and Textbooks, we opted\nnot to employ this, as we had ensured during our\nmanual collection process that all documents in\nthese sources were written in English. As a result\nof this refinement process, approximately 8,400\ndocuments were removed, amounting to a total 231\nmillion tokens.\n3.2\nData Cleaning and Filtering\nDespite our meticulous and thorough data pre-\nprocessing efforts for each source during the cor-\npus collection phase, we\u2019ve noted that some docu-\nments, particularly from websites like Wikipedia\nand Common Crawl, are of insufficient quality\nused for language modeling. These documents\n7\nmight be too brief or include content that is either\nautomatically generated or commonplace. While\nprevious studies have introduced detailed meth-\nods for filtering pre-training corpora (Raffel et al.,\n2020; Rae et al., 2021; Longpre et al., 2023;\nPenedo et al., 2023; Cerebras, 2023), we found that\nthese techniques are not entirely suitable for our\nmath-focused corpus. Applying them as-is would\nlead to the exclusion of many valuable documents.\nTo address this issue, we developed a unique set\nof cleaning and filtering heuristic rules, specifically\ncrafted for the mathematical domain and drawing\nfrom past studies. Specifically, we\n1) detect lines containing \u201clorem ipsum\u201d and\nfilter them out if the resulting line is less than\n5 characters;\n2) detect lines containing \u201cjavascript\u201d that also\ninclude \u201cenable\u201d, \u201cdisable\u201d or \u201cbrowser\u201d and\nare under 200 characters, and filter them;\n3) filter lines containing fewer than 10 words\nthat include keywords like \u201cLogin\u201d, \u201csign-in\u201d,\n\u201cread more...\u201d, or \u201citems in cart.\u201d;\n4) filter documents if the ratio of uppercase\nwords exceeds 40%;\n5) filter lines that end with \u201c...\u201d if they constitute\nmore than 30% of the entire document;\n6) filter documents if the ratio of non-alphabetic\nwords surpasses 80%;\n7) exclude documents with an average English\nword length outside the range of (3, 10);\n8) discard documents that lack at least two com-\nmon stop words such as \u201cthe\u201d, \u201cbe\u201d \u201cto\u201d \u201cof\u201d\n\u201cand\u201d \u201cthat\u201d or \u201chave\u201d;\n9) filter out documents if the ratio of ellipses (...)\nto words exceeds 0.5 (e.g., progress bars);\n10) remove documents where 90% of lines start\nwith bullet points;\n11) filter documents including less than 200 char-\nacters after removing spaces and punctuation\nmarks.\nIt\u2019s these carefully formulated rules that have\nenabled us to curate a high-quality mathematical\ncorpus. Furthermore, these rules have allowed us\nto assign quality annotations to each document\n(from Wikipedia and Common Crawl). These an-\nnotations offer future researchers and developers\nthe flexibility to filter the data according to their\ncriteria, tailoring it to their specific needs. We\nprovide a cleaned example document with quality\nannotations, as shown in Figure 4. The process\nresulted in the filtration of approximately 1,100\ndocuments, leading to the removal of 17 million\ntokens.\nA document from MATHPILE-Common Crawl\nText: This number is called the Copeland\u2013Erd\u02ddos constant, and is known to be\nirrational and normal. I believe its transcendence or otherwise is an open problem.\nThis source claims that it has been proved to be transcendental, but the paper they\nrefer to is the one in which it was proved to be normal and so I think the source is\nmistaken.\nFor now, the knowledge that it is almost surely transcendental will have to suffice!\nNot the answer you\u2019re looking for? Browse other questions tagged number-theory\ntranscendental-numbers or ask your own question.\nDoes the number 2.3, 5, 7, 11, 13 . . . exist and, if so, is it rational or irrational\n&or transcendental?\nIs 0.248163264128. . . a transcendental number?\nWhat is the name of this number? Is it transcendental?\nIs 0.112123123412345123456 . . . algebraic or transcendental?\nIs 0.121121111112111. . . a transcendental number?\nDo we know a transcendental number with a proven bounded continued fraction\nexpansion?\nIf we delete the non-primes from e, is the resulting number transcendental?\nIs there any known transcendental b such that bb is also transcendental?\n...\nSubset: Common Crawl\nmeta:\nlanguage_detection_score: 0.9118,\nchar_num_after_normalized: 887,\ncontain_at_least_two_stop_words: True,\nellipsis_line_ratio: 0.0, idx: 95994,\nlines_start_with_bullet_point_ratio: 0.0,\nmean_length_of_alpha_words: 4.2941,\nnon_alphabetical_char_ratio: 0.0234,\nsymbols_to_words_ratio: 0.0117,\nuppercase_word_ratio: 0.0117\n...\nFigure 4: An example document after cleaning and\nfiltering with quality annotations\n3.3\nData Deduplication\nGiven that our corpus originates from diverse\nsources, including the web and textbooks, it is\ninevitable that there will be repetitions both within\nand across these sources. Deduplication plays a\ncrucial role in enhancing the training efficiency\nof the language model and reducing the memo-\nrization from the training data (Lee et al., 2022;\nPenedo et al., 2023). The challenge in deduplica-\ntion lies in efficiently processing large-scale cor-\npora to identify and eliminate not just exact du-\nplicates but also near-duplicates. To this end, we\nemploy the MinHash LSH algorithm built on the\nimplementation of text-dup (Mou et al., 2023)\nand Lee et al. (2022). MinHash excels at efficiently\nestimating the similarity between sets at scale by\ntransforming data into compact signatures using\nmultiple hash functions (Broder, 1997). In the con-\ntext of the text, the set corresponding to a document\n8\nIn algebraic topology we often encounter chain complexes\nwith extra multiplicative structure.\nFor example, the\ncochain complex of a topological space has what is called\nthe E\u221e-algebra structure which comes from the cup prod-\nuct.\nIn this talk I present an idea for studying such chain com-\nplexes, E\u221e differential graded algebras (E\u221e DGAs), using\nstable homotopy theory. Namely, I discuss new equiva-\nlences between E\u221e DGAS that are defined using commuta-\ntive ring spectra.\nring spectra are equivalent. Quasi-isomorphic E\u221e DGAs\nare E\u221e topologically equivalent. However, the examples I\nam going to present show that the opposite is not true; there\nare E\u221e DGAs that are E\u221e topologically equivalent but not\nquasi-isomorphic. This says that between E\u221e DGAs, we\nhave more equivalences than just the quasi-isomorphisms.\nI also discuss interaction of E\u221e topological equiva-\nlences with the Dyer-Lashof operations and cases where\nE\u221etopological equivalences and quasi-isomorphisms\nagree.\n\u00d6zet : In algebraic topology we often encounter chain\ncomplexes with extra multiplicative structure. For example,\nthe cochain complex of a topological space has what is\ncalled the E\u221e-algebra structure which comes from the cup\nproduct. In this talk I present an idea for studying such\nchain complexes, E\u221e differential graded algebras (E\u221e\nDGAs), using stable homotopy theory. Namely, I discuss\nnew equivalences between E\u221e DGAS that are defined using\ncommutative ring spectra.We say E\u221e DGAs are E\u221e topo-\nlogically equivalent when the corresponding commutative\nring spectra are equivalent. Quasi-isomorphic E\u221e DGAs\nare E\u221e topologically equivalent. However, the examples I\nam going to present show that the opposite is not true; there\nare E\u221e DGAs that are E\u221e topologically equivalent but not\nquasi-isomorphic. This says that between E\u221e DGAs, we\nhave more equivalences than just the quasi-isomorphisms.\nI also discuss interaction of E\u221e topological equivalences\nwith the Dyer-Lashof operations and cases where E\u221e topo-\nlogical equivalences and quasi-isomorphisms agree.\nTable 2: A near-duplication match found in CommonCrawl by MinHash LSH deduplication (in italics). See\nAppendix D for more examples.\nconsists of the collection of its n-grams. However,\ncomputing similarity for all possible pairs of such\nsets is time-consuming, hence the common prac-\ntice is to employ a variant of MinHash known as\nLocality-Sensitive Hashing (LSH) (Gionis et al.,\n1999) which is efficient in identifying similar items\nby grouping documents with similar MinHash sig-\nnatures into the same buckets, thereby reducing the\nnumber of pairwise comparisons needed. Specifi-\ncally, our process involved splitting each document\nusing whitespace and constructing 5-grams. We\nthen applied the \u201csha1\u201d hash function and config-\nured the system with 450 buckets and 20 minhashes\nper bucket, resulting in a total of 9,000 minhashes\nfor each document. This aligns with the setting\noutlined in RefinedWeb (Penedo et al., 2023).\nDuring the deduplication process within each\ndata source, we encountered a significant num-\nber of exact-match and near-duplicate documents.\nSpecifically, we identified 304 duplicate docu-\nments in arXiv, 623 in Common Crawl, a notable\n83,716 in Wikipedia, 783 in textbooks (mainly\nfrom synthetic textbooks), and 144 duplicate ques-\ntions in Stack Exchange. Due to our standardiza-\ntion of the format from ProofWiki, despite detect-\ning many near-duplicates through our deduplica-\ntion process, we found that these were indeed dif-\nferent lemmas, proofs, or definitions (as we show-\ncase in Table 10), thus, we ultimately did not dedu-\nplicate this source. Upon manual review, we made\nsome interesting yet reasonable findings. For ex-\nample, the significant duplication in Wikipedia was\ndue to the collection of multiple historical versions\nof a document during the data collection. In Stack-\nExchange, duplication occurred because commu-\nnity members often posted similar questions on\ndifferent sites (like Math and MathOverflow) to\ngarner more responses (examples of which are pro-\nvided in Table 13). We provide a near-duplicate\nexample found in Common Crawl, as shown in 2\n(See Table 8 to Table 13 for more examples from\neach data source). When deduplicating across dif-\nferent data sources, we hardly found any duplicate\ndocuments, except for one question from StackEx-\nchange that appeared in a document from Common\nCrawl. Therefore, we removed it. As a result of the\ndeduplication process, about 714 million tokens\nwere removed.\nNote that we also experimented with using suffix\narrays (Manber and Myers, 1993) to eliminate ex-\nact match sequences within documents. However,\nit tended to remove common phrases like \u201cQues-\ntions: \u201d. While it can effectively remove some\ntemplated content, it also disrupts the contextual\nintegrity of our corpus. Consequently, we decided\nagainst employing this in order to preserve the con-\ntext of our data.\n9\n3.4\nData Contamination Detection\nAs the scale of pre-training corpora expands, it is\ninevitable to encounter instances where examples\nfrom the evaluation set are found in the training\nset, a phenomenon known as data contamination.\nGenerally, there are typically two types of data\ncontamination, input contamination, where only\nthe input of test examples appears in the training\ncorpus, and input-and-label contamination, where\nboth the inputs and their corresponding labels are\npresent in the training corpus (Dodge et al., 2021).\nA common practice in past studies is to conduct\nthe post-hoc data contamination analysis, utilizing\nn-gram overlap to assess the extent of contamina-\ntion. For instance, GPT-2 performs an 8-gram ap-\nproach (Radford et al., 2019), while GPT-3 (Brown\net al., 2020) and FLAN (Wei et al., 2022) use 13-\ngrams, and LLaMA-2 adopts more intricate skip-\ngram strategy (Touvron et al., 2023b). In this work,\nwe advocate for the necessity of data contamina-\ntion detection at the dataset creation stage itself,\nif feasible. Delaying this process until after train-\ning completion often results in irreversible dam-\nage (Kocetkov et al., 2022). Here, we utilize pop-\nular mathematical reasoning benchmarks, namely\nGSM8K (Cobbe et al., 2021), MATH (Hendrycks\net al., 2021b), and MMLU-STEM (Hendrycks\net al., 2021a), to detect data contamination.\nConsidering the variety of forms in which ques-\ntions and answers might appear in pre-training\ncorpora, we compiled the questions and answers\nfrom these benchmark tests into a set to serve as\na reference for data contamination detection. It\u2019s\nimportant to note that for MMLU, which uses the\nmultiple-choice format with typically short options,\nwe considered only the questions. Intuitively, a\nmath problem can have varied reasoning steps,\nmaking it relatively easier to detect the contam-\nination of test questions in pre-training corpora.\nWe employed line-level exact match detection for\nboth our corpus and test sets, as the questions in\nthese benchmarks are generally brief and often con-\ntained within a single line. Specifically, we split\ndocuments into lines, hashed each line using MD5,\nand took the first 64 bits along with the correspond-\ning line to form a set. This procedure was also\napplied to the constructed reference test set col-\nlection. If a line from the test set, along with its\ncorresponding hash code, is found in the training\nset\u2019s corresponding set, and the length of the line\nis over 50 characters,13 we classify it as a leaked\nsample with an exact match.\nCorpus\nGSM8K\nMATH\nMMLU-STEM\nOurs\n-\n023\n02\nOpenWebMath\n-\n195\n65\nTable 3: The occurrences of benchmark test sets in pre-\ntraining corpora detected by exact match. Note that\nsome samples appear multiple times and the number\nonly represents the lower bound of occurrences, as there\nmay be some duplicates that have not been detected.\nAfter conducting our detection process, we\nidentified 23 questions from MATH and 2 from\nMMLU-STEM in our corpus (as shown in Table 3).\nThese duplicates primarily appeared in StackEx-\nchange, Textbooks, and Common Crawl. Upon\nlocating the questions within our corpus, we noted\nthat no answers were provided following the ques-\ntions. Table 14 and Table 15 showcase examples\nof these leaks from Textbooks and Common Crawl,\nalong with their context. An interesting observa-\ntion is that the leaks in Textbooks originated from\nAMC mathematics competition books, which co-\nincidentally are also a source of questions in the\nMATH benchmark. We also applied this process to\nOpenWebMath, where we discovered many more\nduplicate questions from the MATH and MMLU\ntest sets (also shown in Table 3), although many\nwere duplicates. To illustrate, we provide some\nexamples in Table 16. Interestingly, Azerbayev\net al. (2023b) also report similar findings, albeit\nthrough a different detection method. This under-\nscores the need for extra caution when creating\npre-training corpora, as neglecting this can easily\ninvalidate downstream benchmarks. Ultimately,\nwe removed all detected exact matches from our\ncorpus to mitigate data contamination issues. The\nresulting corpus is referred to as MATHPILE.\n4\nData Analysis\nOverview. As shown in Table 4, we present de-\ntailed statistical information for each component\nof MATHPILE, such as the number of documents\nand the count of tokens. Following our meticulous\nand comprehensive data collection and processing\n13To avoid filtering out many common short phrases.\n10\nComponents\nSize (MB)\n# Documents\n# Tokens\nmax(# Tokens)\nmin (# Tokens)\nave (# Tokens)\nTextbooks\n00644\n003,979\n0187,194,060\n1,634,015\n256\n47,046\nWikipedia\n00274\n022,639\n0078,222,986\n0109,282\n056\n03,455\nProofWiki\n00023\n023,839\n0007,608,526\n0006,762\n025\n00319\nCommonCrawl\n02,560\n075,142\n0615,371,126\n0367,558\n057\n08,189\nStackExchange\n01331\n433,751\n0253,021,062\n0125,475\n028\n00583\narXiv\n24,576\n343,830\n8,324,324,917\n4,156,454\n020\n24,211\nTotal\n29,408\n903,180\n9,465,742,677\n-\n-\n10,480\nTable 4: The components and data statistics of MATHPILE.\n10\n5\n10\n4\n10\n3\n% of Total Documents\narXiv\nCommonCrawl\nProofWiki\n102\n103\n104\n105\n106\nDocument Length (Tokens)\n10\n5\n10\n4\n10\n3\n% of Total Documents\nStackExchange\n102\n103\n104\n105\n106\nDocument Length (Tokens)\nTextbooks\n102\n103\n104\n105\n106\nDocument Length (Tokens)\nWikipedia\nFigure 5: Document length distribution for different sources in MATHPILE(log-scale).\nprocess, we obtain 29GB of high-quality and di-\nverse math-centric corpus, encompassing around\n9.5 billion tokens, from an initial volume of 2.2TB\nof raw data (cf. Figure 3). Compositionally, arXiv\nconstitutes the largest portion of MATHPILE, while\nthe Textbooks represent the smallest share, yet they\nare of exceptionally high quality.\nThe Length Distribution of Documents.\nWe\nanalyze the document length (in terms of token\nnumbers) and their respective proportions from\neach source within MATHPILE, which is visual-\nized in Figure 5. Intuitively, if the data from each\nsource contains a higher amount of near-duplicates\nor machine-generated content, the distribution of\ndocuments of similar lengths becomes more preva-\nlent, leading to a less smooth distribution curve.\nFigure 5 shows that, thanks to our thorough and\nrigorous processing, the document length distri-\nbution in MATHPILEis relatively smooth across\ndifferent sources. Note that ProofWiki, due to its\nfixed format of definitions, lemmas, and proofs,\nnaturally contains shorter content, resulting in a\ndistribution with many similar lengths. We can\nalso observe that, on average, the documents from\narXiv and Textbooks tend to be lengthier, while\nthose from ProofWiki and StackExchange are gen-\nerally shorter.\n5\nRelated Work\nPre-training Corpora for Language Models. In\nthe field of language modeling, early models such\nas GPT (Radford et al., 2018) and BERT (Devlin\n11\net al., 2019) are primarily pre-trained on resources\nlike Books (Zhu et al., 2015) and Wikipedia. Sub-\nsequent developments, including GPT-2 (Radford\net al., 2019) and T5 (Raffel et al., 2020), expand\nthe scope of training corpus to encompass web\npages from sources like Reddit (resulting in Web-\nText) and Common Crawl (resulting in C4). GPT-\n3 (Brown et al., 2020) marks a significant leap,\nenlarging its pre-training corpus to 300 billion to-\nkens, utilizing Common Crawl, WebText, Books,\nand Wikipedia. Gao et al. (2021) introduce the\nPile, a comprehensive collection of 22 diverse and\nhigh-quality datasets, specifically designed for the\npre-training of large-scale language models. The\nGopher project (Rae et al., 2021), although not\nopen-sourced, compiles an extensive corpus of ap-\nproximately 10.5TB, including web pages, books,\nnew articles, and code. Similarly, the PaLM work\n(Chowdhery et al., 2023) develop a high-quality\ncorpus of 780 billion tokens, spanning filtered\nweb pages, books, Wikipedia, news, code, and\nsocial media conversations, yet it remained closed-\nsource. On the other hand, BLOOM (Scao et al.,\n2022), an open-sourced multilingual models, is pre-\ntrained on the ROOTS dataset (Lauren\u00e7on et al.,\n2022), which aggregates content from hundreds\nof sources across 59 languages. Kocetkov et al.\n(2022) build The Stack, a 3.1 TB code dataset in\n30 programming languages. LLaMA\u2019s training in-\nvolved a diverse mixture of data, including sources\nlike arXiv and StackExchange, in addition to the\naforementioned sources (Touvron et al., 2023a).\nHowever, LLaMA did not release its corpus, in\ncontrast to RedPajama, which did offer an open-\nsource version (Together, 2023a). SlimPajama fur-\nther enhances RedPajama by conducting an ex-\ntensive deduplication process (Cerebras, 2023).\nRefinedWeb demonstrates the potential for web-\nonly corpora to achieve performance comparable\nto well-curated corpus, such as Pile (Penedo et al.,\n2023). Recently, as the competition intensifies in\nthe field of large language models, many more\npowerful models, such as GPT-4 (OpenAI, 2023),\nMistral-7B (Jiang et al., 2023) and the lastest Gem-\nini (Team et al., 2023), in addition to not open-\nsourcing their data, are also refraining from dis-\nclosing detailed information about their corpus in\ntheir technical reports. For the open-source com-\nmunity, constructing high-quality and diverse pre-\ntraining corpora is a crucial factor in bridging the\nperformance gap with closed-source models. This\nis precisely the contribution we aim to make with\nour work.\nPre-training Benchmarks and Corpora for\nMathematical Reasoning. Teaching models to\npossess mathematical reasoning abilities akin to\nhumans is considered a vital aspect of achiev-\ning advanced artificial intelligence.\nThis chal-\nlenge has garnered widespread attention from\nboth the machine learning and natural language\ncommunities.\nTo better gauge the mathemat-\nical reasoning capabilities of models, numer-\nous benchmark datasets have been introduced.\nsuch as AQuA (Ling et al., 2017), SVAMP (Pa-\ntel et al., 2021),\nDM-Mathematics (Saxton\net al., 2019), GSM8K (Cobbe et al., 2021) and\nMATH (Hendrycks et al., 2021b), to name a few.\nThese datasets feature problems ranging from basic\narithmetic operations to competition-level mathe-\nmatics questions, encompassing a wide spectrum\nof difficulty. In addition, some benchmarks focus\non the theorem-proving abilities, such as Natural-\nProofs (Welleck et al., 2021). The STEM sub-\nset of MMLU (Hendrycks et al., 2021a) concen-\ntrates on evaluating multi-task understanding in\nscience, technology, engineering, and mathematics.\nTo enhance the mathematical reasoning capabili-\nties of language models, some pre-training corpora\nhave been proposed. AMPS (Hendrycks et al.,\n2021b), although a large-scale synthetic exercise\nset, mainly targets problem-solving at the difficulty\nlevel of the MATH dataset. ProofPile focuses on\nmathematical theorem proving (Azerbayev et al.,\n2023a). Concurrently with our work, OpenWeb-\nMath (Paster et al., 2023) constructs a large-scale\nmathematical corpus, but is solely sourced from\nweb pages.\nOn the other hand, Google\u2019s cor-\npus used for training Minerva (Lewkowycz et al.,\n2022) and the OpenAI\u2019s MathMix corpus (Light-\nman et al., 2023) are not open-sourced. Our work\nis dedicated to bridging this gap by constructing\na high-quality mathematical corpus from diverse\nsources.\n6\nConclusion\nIn this work, we present MATHPILE, a specialized\ncorpus centered around mathematics, character-\nized by its diversity and high quality. Through-\n12\nout its development, we meticulously source and\ngather data, applying a rigorous and math-specific\npipeline. This pipeline encompasses various stages\nsuch as preprocessing, prefiltering, language iden-\ntification, cleaning and filtering, and deduplication,\nall aimed at maintaining the high quality of the cor-\npus. Note that we also conduct data contamination\ndetection to remove duplicates from popular math-\nematical reasoning benchmark test sets, which is\ncrucial for ensuring the integrity and effectiveness\nof these benchmarks in evaluating language mod-\nels. However, this is an aspect often overlooked\nin other similar works. We hope that our MATH-\nPILEcan be utilized, either independently or in\ncollaboration with other corpora, to enhance the\nmathematical reasoning capabilities of language\nmodels, thereby fostering widespread applications.\nAcknowledgements\nWe sincerely appreciate the laboratory members\nwho reviewed this paper and provided their sug-\ngestions and feedback, contributing to the improve-\nment of this work.\nLimitations\nThe decisions made during the data collection and\nprocessing phases might not always be optimal, as\nverifying their effectiveness through low-cost com-\nputational methods is not feasible without training\nat each step. Therefore, our approach has been to\ndraw upon the work of predecessors and, building\non that foundation, cautiously navigate the specific\nchallenges within the mathematical domain. This\nis because the practices of previous work may not\nalways be entirely suitable for our math-focused\nscenario. Despite our significant efforts, the result-\ning corpus may not always be of the highest quality,\nespecially documents sourced from the web, where\na few low-quality documents might still persist.\nReferences\nMiltiadis Allamanis. 2019. The adverse effects of code\nduplication in machine learning models of code. In\nProceedings of the 2019 ACM SIGPLAN Interna-\ntional Symposium on New Ideas, New Paradigms,\nand Reflections on Programming and Software, On-\nward! 2019, page 143\u2013153, New York, NY, USA.\nAssociation for Computing Machinery.\nAllenAI. 2023.\nallenai/dolma \u00b7 datasets at hug-\nging face. https://huggingface.co/datasets/\nallenai/dolma.\nAnthropic.\n2023.\nAnthropic\n\\\nintroducing\nclaude.\nhttps://www.anthropic.com/index/\nintroducing-claude.\nZhangir\nAzerbayev,\nBartosz\nPiotrowski,\nHailey\nSchoelkopf, Edward W. Ayers, Dragomir Radev, and\nJeremy Avigad. 2023a. Proofnet: Autoformalizing\nand formally proving undergraduate-level mathemat-\nics. CoRR, abs/2302.12433.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster,\nMarco Dos Santos, Stephen McAleer, Albert Q.\nJiang, Jia Deng, Stella Biderman, and Sean Welleck.\n2023b. Llemma: An open language model for math-\nematics. CoRR, abs/2310.10631.\nJack Bandy and Nicholas Vincent. 2021. Addressing\n\"documentation debt\" in machine learning research:\nA retrospective datasheet for bookcorpus. CoRR,\nabs/2105.05241.\nEmily M. Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics, 6:587\u2013604.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu\nPurohit, Laria Reynolds, Jonathan Tow, Ben Wang,\nand Samuel Weinbach. 2022. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Pro-\nceedings of BigScience Episode #5 \u2013 Workshop on\nChallenges & Perspectives in Creating Large Lan-\nguage Models, pages 95\u2013136, virtual+Dublin. Asso-\nciation for Computational Linguistics.\nAndrei Z. Broder. 1997. On the resemblance and con-\ntainment of documents. In Compression and Com-\nplexity of SEQUENCES 1997, Positano, Amalfitan\nCoast, Salerno, Italy, June 11-13, 1997, Proceedings,\npages 21\u201329. IEEE.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\n13\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg,\nHarsha Nori, Hamid Palangi, Marco T\u00falio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with GPT-4. CoRR,\nabs/2303.12712.\nCerebras. 2023. Slimpajama: A 627b token, cleaned\nand deduplicated version of redpajama - cerebras.\nhttp://tinyurl.com/slimpajama.\nEthan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Ke-\nhua Feng, Junlong Li, and Pengfei Liu. 2023. Gen-\nerative ai for math: Abel. https://github.com/\nGAIR-NLP/abel.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav\nPetrov, and Noah Fiedel. 2023. Palm: Scaling lan-\nguage modeling with pathways. J. Mach. Learn.\nRes., 24:240:1\u2013240:113.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. CoRR, abs/2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nJesse Dodge, Maarten Sap, Ana Marasovi\u00b4c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colossal\nclean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1286\u20131305, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nYanai Elazar, Akshita Bhagia, Ian Magnusson, Abhi-\nlasha Ravichander, Dustin Schwenk, Alane Suhr,\nPete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer\nSingh, Hanna Hajishirzi, Noah A. Smith, and Jesse\nDodge. 2023.\nWhat\u2019s in my big data?\nCoRR,\nabs/2310.20707.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nTimnit Gebru,\nJamie Morgenstern,\nBriana Vec-\nchione, Jennifer Wortman Vaughan, Hanna Wallach,\nHal Daum\u00e9 III, and Kate Crawford. 2021. Datasheets\nfor datasets. Commun. ACM, 64(12):86\u201392.\nAristides Gionis, Piotr Indyk, and Rajeev Motwani.\n1999. Similarity search in high dimensions via hash-\ning. In VLDB\u201999, Proceedings of 25th International\nConference on Very Large Data Bases, September 7-\n10, 1999, Edinburgh, Scotland, UK, pages 518\u2013529.\nMorgan Kaufmann.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\nC\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\nde Rosa, Olli Saarikivi, Adil Salim, Shital Shah,\nHarkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck,\nRonen Eldan, Adam Tauman Kalai, Yin Tat Lee, and\nYuanzhi Li. 2023. Textbooks are all you need. CoRR,\nabs/2306.11644.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021a. Measuring massive multitask language\nunderstanding. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\n14\nJacob Steinhardt. 2021b. Measuring mathematical\nproblem solving with the MATH dataset. In Pro-\nceedings of the Neural Information Processing Sys-\ntems Track on Datasets and Benchmarks 1, NeurIPS\nDatasets and Benchmarks 2021, December 2021, vir-\ntual.\nDanny Hernandez, Tom B. Brown, Tom Conerly, Nova\nDasSarma, Dawn Drain, Sheer El Showk, Nelson\nElhage, Zac Hatfield-Dodds, Tom Henighan, Tristan\nHume, Scott Johnston, Benjamin Mann, Chris Olah,\nCatherine Olsson, Dario Amodei, Nicholas Joseph,\nJared Kaplan, and Sam McCandlish. 2022. Scaling\nlaws and interpretability of learning from repeated\ndata. CoRR, abs/2205.10487.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Tim-\noth\u00e9e Lacroix, and William El Sayed. 2023. Mistral\n7b. CoRR, abs/2310.06825.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efficient\ntext classification. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427\u2013431, Valencia, Spain. Association\nfor Computational Linguistics.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,\nChenghao Mou, Carlos Mu\u00f1oz Ferrandis, Yacine Jer-\nnite, Margaret Mitchell, Sean Hughes, Thomas Wolf,\nDzmitry Bahdanau, Leandro von Werra, and Harm\nde Vries. 2022. The stack: 3 TB of permissively\nlicensed source code. CoRR, abs/2211.15533.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allah-\nsera Tapo, Nishant Subramani, Artem Sokolov, Clay-\ntone Sikasote, Monang Setyawan, Supheakmungkol\nSarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera,\nAnnette Rios, Isabel Papadimitriou, Salomey Osei,\nPedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyak-\neni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-\ngira, Colin Leong, Nze Lawson, Sneha Kudugunta,\nYacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-\nture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine \u00c7abuk Ball\u0131, Stella Biderman, Alessia Bat-\ntisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,\nIsrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2022. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. Transactions of the Association for Compu-\ntational Linguistics, 10:50\u201372.\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro von Werra, Chenghao Mou,\nEduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, J\u00f6rg\nFrohberg, Mario Sasko, Quentin Lhoest, Angelina\nMcMillan-Major, G\u00e9rard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben Allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Mu\u00f1oz, Jian\nZhu, Daniel van Strien, Zaid Alyafeai, Khalid Al-\nmubarak, Minh Chien Vu, Itziar Gonzalez-Dios,\nAitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\nSuarez, Aaron Gokaslan, Shamik Bose, David Ife-\noluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas\nPai, Jenny Chim, Violette Lepercq, Suzana Ilic,\nMargaret Mitchell, Alexandra Sasha Luccioni, and\nYacine Jernite. 2022. The bigscience ROOTS corpus:\nA 1.6tb composite multilingual dataset. In NeurIPS.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424\u20138445, Dublin, Ireland. Association for\nComputational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay V. Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\nTheo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur,\nGuy Gur-Ari, and Vedant Misra. 2022. Solving quan-\ntitative reasoning problems with language models.\nIn NeurIPS.\nYuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del\nGiorno, Suriya Gunasekar, and Yin Tat Lee. 2023.\nTextbooks are all you need II: phi-1.5 technical re-\nport. CoRR, abs/2309.05463.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Har-\nrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl\nCobbe. 2023.\nLet\u2019s verify step by step.\nCoRR,\nabs/2305.20050.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\n15\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158\u2013167, Vancouver,\nCanada. Association for Computational Linguistics.\nShayne Longpre, Gregory Yauney, Emily Reif, Kather-\nine Lee, Adam Roberts, Barret Zoph, Denny Zhou,\nJason Wei, Kevin Robinson, David Mimno, and\nDaphne Ippolito. 2023.\nA pretrainer\u2019s guide to\ntraining data: Measuring the effects of data age,\ndomain coverage, quality, & toxicity.\nCoRR,\nabs/2305.13169.\nAlexandra Luccioni and Joseph Viviano. 2021. What\u2019s\nin the box? an analysis of undesirable content in the\nCommon Crawl corpus. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 182\u2013189, Online. Association\nfor Computational Linguistics.\nUdi Manber and Eugene W. Myers. 1993. Suffix arrays:\nA new method for on-line string searches. SIAM J.\nComput., 22(5):935\u2013948.\nAngelina McMillan-Major, Emily M. Bender, and\nBatya Friedman. 2023. Data statements: From tech-\nnical concept to community practice. ACM J. Re-\nsponsib. Comput. Just Accepted.\nMargaret Mitchell, Alexandra Sasha Luccioni, Nathan\nLambert, Marissa Gerchick, Angelina McMillan-\nMajor, Ezinwanne Ozoani, Nazneen Rajani, Tris-\ntan Thrush, Yacine Jernite, and Douwe Kiela. 2022.\nMeasuring data. CoRR, abs/2212.05129.\nChenghao Mou, Chris Ha, Kenneth Enevoldsen, and\nPeiyuan Liu. 2023. Chenghaomou/text-dedup: Ref-\nerence snapshot.\nOpenAI. 2022. Introducing chatgpt. https://openai.\ncom/blog/chatgpt.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nKeiran Paster, Marco Dos Santos, Zhangir Azer-\nbayev, and Jimmy Ba. 2023. Openwebmath: An\nopen dataset of high-quality mathematical web text.\nCoRR, abs/2310.06786.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The refinedweb dataset\nfor falcon LLM: outperforming curated corpora\nwith web data, and web data only.\nCoRR,\nabs/2306.01116.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Jacob\nMenick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-\ndhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d\u2019Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy, Chris\nJones, James Bradbury, Matthew J. Johnson, Blake A.\nHechtman, Laura Weidinger, Iason Gabriel, William\nIsaac, Edward Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis & insights\nfrom training gopher. CoRR, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019.\nAnalysing mathematical\nreasoning abilities of neural models. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, Jonathan Tow, Alexander M. Rush,\nStella Biderman, Albert Webson, Pawan Sasanka\nAmmanamanchi, Thomas Wang, Beno\u00eet Sagot,\n16\nNiklas Muennighoff, Albert Villanova del Moral,\nOlatunji Ruwase, Rachel Bawden, Stas Bekman, An-\ngelina McMillan-Major, Iz Beltagy, Huu Nguyen,\nLucile Saulnier, Samson Tan, Pedro Ortiz Suarez,\nVictor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, and et al. 2022. BLOOM: A\n176b-parameter open-access multilingual language\nmodel. CoRR, abs/2211.05100.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai,\nAnja Hauth, et al. 2023.\nGemini: A family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nTogether. 2023a. Redpajama, a project to create lead-\ning open-source models, starts by reproducing llama\ntraining dataset of over 1.2 trillion tokens. https:\n//www.together.ai/blog/redpajama.\nTogether. 2023b.\nRedpajama-data-v2:\nAn open\ndataset with 30 trillion tokens for training large lan-\nguage models. https://www.together.ai/blog/\nredpajama-data-v2.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Es-\niobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian\nFuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui\nHou, Hakan Inan, Marcin Kardas, Viktor Kerkez,\nMadian Khabsa, Isabel Kloumann, Artem Korenev,\nPunit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yun-\ning Mao, Xavier Martinet, Todor Mihaylov, Pushkar\nMishra, Igor Molybog, Yixin Nie, Andrew Poul-\nton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan,\nBinh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. 2023b.\nLlama 2:\nOpen foundation and fine-tuned chat models. CoRR,\nabs/2307.09288.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nSean Welleck, Jiacheng Liu, Ronan Le Bras, Hanna\nHajishirzi, Yejin Choi, and Kyunghyun Cho. 2021.\nNaturalproofs: Mathematical theorem proving in nat-\nural language. In Proceedings of the Neural Infor-\nmation Processing Systems Track on Datasets and\nBenchmarks 1, NeurIPS Datasets and Benchmarks\n2021, December 2021, virtual.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,\nLuke Zettlemoyer, and Omer Levy. 2023. LIMA:\nless is more for alignment. CoRR, abs/2305.11206.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19\u201327.\nIEEE Computer Society.\n17\nA\nMATHPILEDatasheet\nMOTIVATION\nFor what purpose was the dataset cre-\nated?\nDeveloped in a context where datasets like Google\u2019s Minerva\nand OpenAI\u2019s MathMix are not open-sourced, MATHPILEaims to\ncounter this trend by enriching the open-source community and\nenhancing mathematical language modeling with its (relatively)\nlarge-scale, math-centric, diverse, high-quality dataset. It can be\nused on its own or cooperated with general domain corpora like\nbooks, and Github code, to improve the reasoning abilities of\nlanguage models.\nWho created the dataset and on be-\nhalf of which entity?\nMATHPILEwas created by the authors of this work.\nWho funded the creation of the\ndataset?\nThe creation of MATHPILEwas funded by GAIR Lab, SJTU.\nAny other comment?\nNone.\nCOMPOSITION\nWhat do the instances that comprise\nthe dataset represent?\nMATHPILEis comprised of text-only documents, encompassing\na broad range of sources. These include academic papers from\narXiv, educational materials such as textbooks and lecture notes,\ndefinitions, theorems and their proofs, informative articles from\nWikipedia, interactive Q&A content from StackExchange commu-\nnity users, and webpages sourced from Common Crawl. All these\ninstances are math-focused.\nHow many instances are there in to-\ntal?\nMATHPILEcontains about 903 thousand of documents, or around\n9.5 billion tokens.\nDoes the dataset contain all possible\ninstances or is it a sample (not nec-\nessarily random) of instances from a\nlarger set?\nMATHPILEis curated from a diverse array of sources, including\narXiv, Textbooks, Wikipedia, StackExchange, ProofWiki, and\nCommon Crawl. However, it doesn\u2019t encompass all instances from\nthese sources. We have implemented a rigorous data processing\npipeline, which involves steps like preprocessing, prefiltering,\nlanguage identification, cleaning, filtering, and deduplication. This\nmeticulous approach is taken to guarantee the high quality of the\ncontent within MATHPILE.\nWhat data does each instance consist\nof?\nEach instance in MATHPILEis a text-only document, uniquely\nidentified by its source, labeled under Subset. These instances are\nenriched with metadata, such as the score from language identifica-\ntion, the ratio of symbols to words, and their respective file paths.\nNote that instances from the StackExchange are composed of a\nquestion and its accompanying answers, each with their own set\nof meta data, including community users. To illustrate them, we\nprovide specific examples for each source, ranging from Figure 6\nto Figure 12.\n18\nIs there a label or target associated\nwith each instance?\nNo.\nIs any information missing from indi-\nvidual instances?\nNo.\nAre relationships between individual\ninstances made explicit?\nNo.\nAre there recommended data splits?\nNo.\nAre there any errors, sources of noise,\nor redundancies in the dataset?\nDespite our rigorous efforts in cleaning, filtering out low-quality\ncontent, and deduplicating documents, it\u2019s important to acknowl-\nedge that a small fraction of documents in MATHPILEmight still\nfall short of our quality standards, particularly those sourced from\nweb pages.\nIs the dataset self-contained, or does\nit link to or otherwise rely on external\nresources?\nYes, MATHPILEis self-contained.\nDoes the dataset contain data that\nmight be considered confidential?\nNo.\nDoes the dataset contain data that, if\nviewed directly, might be offensive,\ninsulting, threatening, or might oth-\nerwise cause anxiety?\nWe do not expect offensive content despite our significant efforts\nin cleaning and filtering. But, we can not fully guarantee this.\nCOLLECTION\nHow was the data associated with\neach instance acquired?\nOur data is primarily sourced from the arXiv website and the\nInternet Archive. The CommonCrawl data originates from SlimPa-\njama. The textbooks included are manually collected, with quality\nchecks performed on publicly available textbooks from various\ninternet sources.\nWhat mechanisms or procedures\nwere used to collect the data?\nRefer to \u00a7 2 for details on how they collect data.\nIf the dataset is a sample from a\nlarger set, what was the sampling\nstrategy?\nWe strive to use the most recent data dumps available and then\nselectively choose high-quality documents that are closely related\nto mathematics.\nWho was involved in the data collec-\ntion process and how were they com-\npensated?\nAuthors from this paper were involved in collecting it and process-\ning it.\nOver what timeframe was the data\ncollected?\nMATHPILEencompasses documents created between 2007 and\nAugust 2023. Note that some documents and textbooks included\nmay be created in the previous century.\nWere any ethical review processes\nconducted?\nNo.\nPREPROCESSING\n19\nWas any preprocessing/cleaning/la-\nbeling of the data done?\nYes, during our data collection phase, we conducted extensive\nfiltering and cleansing procedures, detailed in \u00a7 2. After the com-\npletion of data collection, we conducted further steps including\nlanguage identification, additional cleaning and filtering, dedupli-\ncation, and leakage detection in benchmark datasets. Subsequently,\nwe removed any contaminated examples identified through this\nprocess. See \u00a7 3 for details.\nWas the \u201craw\u201d data saved in addition\nto the preprocessed/cleaned/labeled\ndata?\nYes.\nIs the software that was used to\npreprocess/clean/label the data avail-\nable?\nYes, we will open-source the corresponding scripts.\nUSES\nHas the dataset been used for any\ntasks already?\nYes, this data has been used to develop mathematical language\nmodels.\nIs there a repository that links to any\nor all papers or systems that use the\ndataset?\nNo.\nWhat (other) tasks could the dataset\nbe used for?\nMATHPILEwas developed to enhance language modeling, offering\nsignificant benefits for a variety of mathematical reasoning tasks.\nIs there anything about the com-\nposition of the dataset or the way\nit was collected and preprocessed/-\ncleaned/labeled that might impact fu-\nture uses?\nOur cleaning and filtering processes, while thorough, may not be\nentirely optimal, potentially leading to the exclusion of some valu-\nable documents. Additionally, MATHPILEis specifically tailored\nfor English, which limits its applicability in multilingual contexts.\nAre there tasks for which the dataset\nshould not be used?\nAny tasks which may considered irresponsible or harmful.\nDISTRIBUTION\nWill the dataset be distributed to\nthird parties outside of the entity on\nbehalf of which the dataset was cre-\nated?\nYes, MATHPILEwill be available on the Huggingface Hub.\nHow will the dataset will be dis-\ntributed?\nMATHPILEwill be made available through the HuggingFace Hub.\nWhen will the dataset be distributed?\nThe MATHPILEwill be available after this paper is made public.\nWill the dataset be distributed under\na copyright or other intellectual prop-\nerty (IP) license, and/or under appli-\ncable terms of use (ToU)?\nIf the source data of MATHPILEis governed by a license more\nrestrictive than CC BY-NC-SA 4.0, MATHPILEadheres to that\nstricter licensing. In all other cases, it operates under the CC\nBY-NC-SA 4.0 license.\nHave any third parties imposed IP-\nbased or other restrictions on the\ndata associated with the instances?\nNot to our knowledge.\n20\nDo any export controls or other\nregulatory restrictions apply to the\ndataset or to individual instances?\nNot to our knowledge.\nMAINTENANCE\nWho will be supporting/hosting/main-\ntaining the dataset?\nMATHPILEwill be hosted on the HuggingFace Hub.\nHow can the owner/curator/manager\nof the dataset be contacted?\nstefanpengfei@gmail.com\nzzwang.nlp@gmail.com\nIs there an erratum?\nNo.\nWill the dataset be updated?\nYes, it is currently a work in progress and updates are ongoing.\nIf others want to extend/augmen-\nt/build on/contribute to the dataset,\nis there a mechanism for them to do\nso?\nNo.\nTable 5: Datasheet for MATHPILE, following the framework introduced by Gebru et al. (2021).\n21\nB\nExamples of MATHPILE\nWe provide some illustrative examples from each source in MATHPILE, as shown in Figure 6 to Figure 12.\nA document from MATHPILE-CommonCrawl\nText:\nAre there optimizers where it is possible to specify ordinal ranking of parameters?\nAssume that f is smooth (n-th order differentiable in each of the parameters).\nAn approach I often use when applying unconstrained optimisation algorithms to constrained problems is to transform the parameter space such that the constraints cannot be violated.\nOf course this results in \u03b8\u2217\n1 \u2265 \u03b8\u2217\n2 \u2265 \u03b8\u2217\n3 which isn\u2019t quite what you asked for. To get a strict ranking you\u2019ll need to bump x1 \u2212 x2\n2 and x1 \u2212 x2\n2 \u2212 x2\n3 down at the last digit of\nprecision.\nthus spake a.k.thus spake a.k.\nThese variants of your constraints are linear, so provided that your function f is well-behaved (smooth, easy to calculate, easy to compute derivatives, derivatives are well-conditioned,\netc.), any constrained optimization solver should be able to solve your problem without issue.\nNot the answer you\u2019re looking for? Browse other questions tagged optimization constrained-optimization or ask your own question.\nDoes the amount of correlation of model parameters matter for nonlinear optimizers?\nOptimization of a blackbox function with an equality constraint?\n...\nSubset: CommonCrawl\nmeta:\nlanguage_detection_score: 0.8670,\nchar_num_after_normalized: 926,\ncontain_at_least_two_stop_words: True,\nellipsis_line_ratio: 0.0,\nidx: 383668,\nlines_start_with_bullet_point_ratio: 0.0,\nmean_length_of_alpha_words: 5.0870,\nnon_alphabetical_char_ratio: 0.0,\nsymbols_to_words_ratio: 0.0,\nuppercase_word_ratio: 0.0060,\n...\nFigure 6: An example Common Crawl document in MATHPILE\nC\nDetails for Corpus Collection\nThe subjects from which we collected papers on arXiv are listed in Table 6. The specific StackExchange\nsites from which we gathered data are listed in Table 7.\nSubjects\nmath.AG, math.AT, math.AP, math.CT, math.CA, math.CO, math.AC, math.CV, math.DG,\nmath.DS, math.FA, math.GM, math.GN, math.GT, math.GR, math.HO, math.IT, math.KT,\nmath.LO, math.MP, math.MG, math.NT, math.NA, math.OA, math.OC, math.PR, math.QA,\nmath.RT, math.RA, math.SP, math.ST, math.SG, math-ph, quant-ph, cs.CC, cs.CG, cs.DM, cs.DS,\ncs.FL, cs.GT, cs.LG, cs.NA, cs.LO, q-fin.MF, stat.CO, stat.ML, stat.ME, stat.OT, stat.TH, econ.TH\nTable 6: The subject list during collecting corpus from arXiv.\nSites sourced from StackExchange\nmath.stackexchange.com, mathoverflow.net, mathematica.stackexchange.com,\nmatheducators.stackexchange.com, hsm.stackexchange.com, physics.stackexchange.com,\nproofassistants.stackexchange.com, tex.stackexchange.com, datascience.stackexchange,\ncstheory.stackexchange.com, cs.stackexchange.com\nTable 7: The site list during collecting corpus from StackExchange.\n22\nD\nExamples of Duplicates Encountered in the Deduplication Process\nWe provide some illustrative examples of duplicates from each source in the deduplication process, as\nshown in Table 8 to Table 13.\nWe also provide examples of downstream task benchmarks (i.e., MATH and MMLU-STEM) leaks\nidentified during our data contamination detection process for our corpus (as shown in Table 14 and\nTable 15) and OpenWebMath 16 (as shown in Table 16).\n23\nA document from MATHPILE-Wikipedia\nText:\n# Inner Automorphism\nIn abstract algebra, an inner automorphism is an automorphism of a group, ring, or algebra given by the conjugation action of a fixed element, called the conjugating element. They\ncan be realized via simple operations from within the group itself, hence the adjective \"inner\". These inner automorphisms form a subgroup of the automorphism group, and the\nquotient of the automorphism group by this subgroup is defined as the outer automorphism group.\n## Definition\nIf G is a group and g is an element of G (alternatively, if G is a ring, and g is a unit), then the function\n\u03c6g : G \u2192 G\n\u03c6g(x) := g\u22121xg\nis called (right) conjugation by g (see also conjugacy class). This function is an endomorphism of G: for all x1, x2 \u2208 G,\n\u03c6g(x1x2) = g\u22121x1x2g = (g\u22121x1g)(g\u22121x2g) = \u03c6g(x1)\u03c6g(x2),\nwhere the second equality is given by the insertion of the identity between x1 and x2. Furthermore, it has a left and right inverse, namely \u03c6g\u22121 . Thus, \u03c6g is bijective, and so an\nisomorphism of G with itself, i.e., an automorphism. An inner automorphism is any automorphism that arises from conjugation.[1]\nWhen discussing right conjugation, the expression g\u22121xg is often denoted exponentially by xg. This notation is used because composition of conjugations satisfies the identity:\n(xg1 )g2 = xg1g2 for all g1, g2 \u2208 G. This shows that right conjugation gives a right action of G on itself.\n### Inner and Outer Automorphism Groups\nThe composition of two inner automorphisms is again an inner automorphism, and with this operation, the collection of all inner automorphisms of G is a group, the inner automorphism\ngroup of G denoted Inn(G).\nInn(G) is a normal subgroup of the full automorphism group Aut(G) of G. The outer automorphism group, Out(G), is the quotient group\nOut(G) =\nAut(G)\nInn(G)\n.\nThe outer automorphism group measures, in a sense, how many automorphisms of G are not inner. Every non-inner automorphism yields a non-trivial element of Out(G), but\ndifferent non-inner automorphisms may yield the same element of Out(G).\nSaying that conjugation of x by a leaves x unchanged is equivalent to saying that a and x commute:\na\u22121xa = x \u21d0\u21d2 xa = ax.\nTherefore, the existence and number of inner automorphisms that are not the identity mapping is a kind of measure of the failure of the commutative law in the group (or ring).\nAn automorphism of a group G is inner if and only if it extends to every group containing G.[2]\n...\nSubset: Wikipedia\nmeta:\nlanguage_detection_score: 0.7236,\nchar_num_after_normalized: 5794,\ncontain_at_least_two_stop_words: True,\nellipsis_line_ratio: 0.0,\nlines_start_with_bullet_point_ratio: 0.0,\nmean_length_of_alpha_words: 4.2245,\nmimetype: text/html,\npage_index: 48171,\npage_path: A/Inner_automorphism,\npage_title: Inner automorphism,\nnon_alphabetical_char_ratio: 0.1422,\nsymbols_to_words_ratio: 0.0,\nuppercase_word_ratio: 0.0871,\n...\nFigure 7: An example Wikipedia document in MATHPILE\n24\nA document from MATHPILE-Textbooks\nText:\n# LINEAR TORIC FIBRATIONS\nSANDRA DI ROCCO\n## INTRODUCTION TO TORIC FIBRATIONS\nDefinition 1.1. A toric fibration is a surjective flat map f : X \u2192 Y with connected fibres where\n(a) X is a toric variety\n(b) Y is a normal algebraic variety\n(c) dim(Y ) < dim(X).\nRemark 1.2. Observe that if f : X \u2192 Y is a toric fibration then Y and a general fiber F are also toric varieties. Moreover if X is smooth, respectively Q-factorial then so is Y and\nF .\nCombinatorial characterization. A toric fibration has the following combinatorial characterization (see [EW, Chapter VI] for further details). Let X = X\u03a3, where \u03a3 \u2282 N \u223c\n= Zn,\nbe a toric variety of dimension n and let i : \u2206 ,\u2192 N a sublattice.\nProposition 1.3. [EW] The inclusion i induces a toric fibration if and only if:\n(a) \u2206 is a primitive lattice, i.e. (\u2206 \u2297 R) \u2229 N = \u2206.\n(b) For every \u03c3 \u2208 \u03a3(n), \u03c3 = \u03c4 + \u03b7, where \u03c4 \u2208 \u2206 and \u03b7 \u2229 \u2206 = {0} (i.e. \u03a3 is a splitfan).\nWe briefly outline the construction. The projection \u03c0 : N \u2192 N/\u2206 induces a map of fans \u03a3 \u2192 \u03c0(\u03a3) and thus a map of toric varieties f : X \u2192 Y . The general fiber F is a\ntoric variety defined by the fan \u03a3F = {\u03c3 \u2208 \u03a3 \u2229 \u2206}.\nWhen the toric variety X in a toric fibration is polarized by an ample line bundle L we will call the pair (f : X \u2192 Y, L) a polarized toric fibration. Observe that the polarized toric\nvarieties (X, L) and\n\u0000F, L|F\n\u0001, for a general fiber F , define lattice polytopes P(X,L), P(F, L|F ). The polytope P(X,L) is in fact a \"twisted sum\" of a finite number of\nlattice polytopes fibering over P(F, L|F ). Definition 1.4. Let R0, . . . , Rk \u2282 \u2206 be polytopes. Let \u03c0 : M \u2192 \u039b be a surjective map of lattices such that \u03c0 (Ri) = vi and\nthe v0, \u00b7 \u00b7 \u00b7 , vk are distinct vertices of Conv (v0, . . . , vk). We will call a Cayley \u03c0-twisted sum (or simply a Cayley sum) of R0, . . . , Rk a polytope which is affinely\nisomorphic to Conv (R0, . . . , Rk). We will denote it by:\n[R0 \u22c6 . . . \u22c6 Rk]\u03c0\nIf the polytopes Ri are additionally normally equivalent, i.e. they define the same normal fan \u03a3Y , we will denote the Cayley sum by:\nCayley (R0, . . . , Rk)(\u03c0,Y ) .\nThese are the polytopes that are associated to a polarized toric fibration. Consider a sublattice i : \u2206 ,\u2192 N and the dual lattice surjection \u03c0 : M \u2192 \u039b.\nProposition 1.5. [CDR08] The sublattice i : \u2206 ,\u2192 N induces a polarized toric fibration (f : X \u2192 Y, L) if and only if P(X,L) = Cayley (R0, . . . , Rk)(\u03c0,Y ) for some\nnormally equivalent polytopes R0, . . . , Rk.\nThe polarized general fiber\n\u0000F, L|F\n\u0001\ncorresponds to the polarized toric variety associated to the polytope P(F, L|F ) = Conv (v0, . . . , vk) and the polytopes R0, \u00b7 \u00b7 \u00b7 , Rk\ndefine the embeddings of the invariant sections polarized by the restrictions of L.\nExample 1.6. Consider the Hirzebruch surface F1 = Blp\n\u0010\nP2\u0011\n= P\n\u0010\nOP1 \u2295 OP1 (1)\n\u0011\npolarized by the tautological line bundle \u03be = 2\u03d5\u2217 \u0010\nOP2 (1)\n\u0011\n\u2212 E where \u03d5 is the\nblow-up map and E the exceptional divisor. The associated polytope is P = Cayley (\u22061, 2\u22061).\nFIGURE 1. The Hirzebruch surface P\n\u0010\nOP1 \u2295 OP1 (1)\n\u0011\nExample 1.7. More generally:\n- when \u03c0(P ) = \u2206t the polytope Cayley (R0, . . . , Rk)(\u03c0,Y ) defines the variety P (L0 \u2295 . . . \u2295 Lk ), where the Li are ample line bundles on the toric variety Y , polarized\nby the tautological bundle \u03be. In particular L|F = OPt (1).\n- When \u03c0(P ) is a simplex (not necessarily smooth) Cayley (R0, . . . , Rk)(\u03c0,Y ) defines a Mori-type fibration. A fibration whose general fiber has Picard rank one. - When\n\u03c0(P ) = s\u2206t then again the variety has the structure of a Pt-fibration whose general fiber F is embedded via an s-Veronese embedding:\n\u0000F, L|F\n\u0001\n=\n\u0010\nPt, OPt (s)\n\u0011\n.\nFor general Cayley sums, [R0 \u22c6 . . . \u22c6 Rk]\u03c0, one has the following geometrical interpretation. Let (X, L) be the associated polarized toric variety and let Y be the toric variety\ndefined by the Minkowski sum R0 + . . . + Rk. The fan defining Y is a refinement of the normal fan of Ri for i = 0, . . . , k. Consider the associated birational maps\n\u03d5i : Y \u2192 Yi, where (Yi, Li) is the polarized toric variety defined by the polytope Ri. The line bundles Hi = \u03d5\u2217\ni (Li) are nef line bundles on Y . Denote by the same\nsymbol the maps of fans \u03d5i : \u03a3Y \u2192 \u03a3Yi . Define then the fan:\n\u03a3Z :\nn\n\u03d5\u22121\ni\n\u0000\u03c3j\n\u0001\n\u00d7 \u03b7l, for all \u03c3j \u2208 \u03a3Yi , \u03b7l \u2208 \u03a3\u2206\no\nwhere \u039b = Conv (v0, . . . , vk). It is a refinement of \u03a3X and thus the defining variety Z is birational to X. Moreover it is a split fan and thus it defines a toric fibration\nf : Z \u2192 Y . The Cayley sum [R0 \u22c6 . . . \u22c6 Rk]\u03c0 is the polytope defined by the nef line bundle \u03d5\u2217(L), and the polytopes Ri are the polytopes defined by the nef line bundles\nHi on the invariant sections.\nHistorical Remark. The definition of a Cayley polytope originated by what is \"classically\" referred to as the Cayley trick. We first recall the definition of Resultant and Discriminant.\nLet f1(x), . . . , fn(x) be a system of n polynomials in n variables x = (x1, . . . , xn) supported on A \u2282 Zn. This means that fi = \u03a0aj \u2208Acjxaj . The resultant (of\nA ), RA\n\u0000cj ), is a polynomial in the coefficients cj, which vanishes whenever the corresponding polynomials have a common zero.\nThe discriminant of a finite subset A, \u2206A , is also a polynomial \u2206A\n\u0000cj\n\u0001\nin the variables cj \u2208 A which vanishes whenever the corresponding polynomial has a multiple root.\nTheorem 1.8. [GKZ][Cayley Trick] The A-resultant of the system f1, . . . , fn equals the Adiscriminant of the polynomial:\np(x, y) = fi(x) +\nn\nX\n2\nyi\u22121fi(x).\nLet Ri = N (fi) \u2282 Rn be the Newton polytopes of the polynomials fi. The Newton polytope of the polynomial p(x, y) is the Cayley sum [R1 \u22c6 . . . \u22c6 Rn]\u03c0, where\n\u03c0 : R2n\u22121 \u2192 Rn\u22121 is the natural projection such that \u03c0\n\u0000[R1 \u22c6 . . . \u22c6 Rn]\u03c0\n\u0001\n= \u2206n\u22121.\n...\nSubset: Textbooks\nmeta:\nbook_name: Linear Toric Fibrations_Sandra Di Rocco,\ntype: Notes,\n...\nFigure 8: An example textbook document in MATHPILE\n25\nA document from MATHPILE-ProofWiki\nText:\n\\section{Test for Submonoid}\nTags: Abstract Algebra, Monoids\n\\begin{theorem}\nTo show that \\struct {T, circ} is a submonoid of a monoid \\struct {S, circ}, we need to show that:\n:(1):\nT \u2286 S\n:(2):\n\\struct {T, circ} is a magma (that is, that it is closed)\n:(3):\n\\struct {T, circ} has an identity.\n\\end{theorem}\n\\begin{proof}\nFrom Subsemigroup Closure Test, (1) and (2) are sufficient to show that \\struct {T, circ} is a subsemigroup of \\struct {S, circ}.\nDemonstrating the presence of an identity is then sufficient to show that it is a monoid. {{qed}}\nCategory:Monoids\n\\end{proof}\n...\nSubset: ProofWiki\nmeta:\ntype: Theorem_Proof,\n...\nFigure 9: An example ProofWiki (a theorem and its proof) document in MATHPILE\nA document from MATHPILE-ProofWiki\nText:\n\\begin{definition}[Definition:That which produces Medial Whole with Medial Area/Whole]\nLet a, b \u2208 R>0 be (strictly) positive real numbers such that a > b.\nLet a \u2212 b be a straight line which produces with a medial area a medial whole.\nThe real number a is called the \u201d\u2019whole\u201d\u2019 of the straight line which produces with a medial area a medial whole.\nCategory:Definitions/Euclidean Number Theory\n\\end{definition}\nSubset: ProofWiki\nmeta:\ntype: Definition,\n...\nFigure 10: An example ProofWiki (definition) document in MATHPILE\n26\nA document from MATHPILE-arXiv\nText:\n\\begin{document}\n\\title{Coherence freeze in an optical lattice investigated via pump-probe spectroscopy}\n\\author{Samansa Maneshi}\n\\email[]{smaneshi@physics.utoronto.ca}\n\\author{Chao Zhuang}\n\\author{Christopher R. Paul}\n\\author{Luciano S. Cruz}\n\\altaffiliation[Current address: ]{UFABC, S\u00e3o Paulo, Brazil.}\n\\author{Aephraim M. Steinberg}\n\\affiliation{Centre for Quantum Information & Quantum Control and Institute for Optical Sciences,\nDepartment of Physics, University of Toronto, Canada }\n\\date{\\today}\n\\pacs{37.10.Jk, 03.65.Yz, 03.67.-a, 42.50.Md}\n\\begin{abstract}\nMotivated by our observation of fast echo decay and a surprising coherence freeze, we have developed a pump-probe spectroscopy technique for vibrational states of ultracold 85Rb\natoms in an optical lattice to gain information about the memory dynamics of the system. We use pump-probe spectroscopy to monitor the time-dependent changes of frequencies\nexperienced by atoms and to characterize the probability distribution of these frequency trajectories. We show that the inferred distribution, unlike a naive microscopic model of the\nlattice, correctly predicts the main features of the observed echo decay.\n\\end{abstract}\n\\maketitle\nCharacterizing decoherence mechanisms is a crucial task for experiments aiming to control quantum systems, e.g., for quantum information processing (QIP). In this work, we\ndemonstrate how two-dimensional (2D) pump-probe spectroscopy may be extended to provide important information on these mechanisms. As a model system, we study quantum\nvibrational states of ultracold atoms in an optical lattice. In addition to being a leading candidate system for QIP \\citeBrennenJaksch, optical lattices are proving a versatile testing\nground for the development of quantum measurement and control techniques \\citeOMandel, Anderlini and a powerful tool for quantum simulations, e.g. the study of Anderson\nlocalization and the Hubbard model \\citeMottAnderson.\nIn our experiment, we study the vibrational coherence of 85Rb atoms trapped in a shallow one-dimensional standing wave. Through our 2D pump-probe technique, we obtain detailed\nmicroscopic information on the frequency drift experienced by atoms in the lattice, enabling us to predict the evolution of coherence. Since the pioneering development of the technique\nin NMR\\citeJeener-Ernst, 2D spectroscopy has been widely used to obtain high-resolution spectra and gain information about relaxations, couplings, and many-body interactions, in\nrealms ranging from NMR \\citeErnst to molecular spectroscopy \\citeMukamel-Jonas, Hybl, Brixner, MillerNature to semiconductor quantum wells \\citeCundiff, KWStone. Here, we\nshow that similar powerful techniques can be applied to the quantized center-of-mass motion of trapped atoms, and more generally, offer a new tool for the characterization of systems\nin QIP and quantum control.\n\\begin{figure}\n\\caption{(Color online) Two typical measurements of echo amplitude vs. time. The echo pulse and the observed echo envelope are centered at times tp and 2tp, respectively. After an\ninitial decay, echo amplitude stays constant for about 1ms forming a plateau, before decaying to zero. The average lattice depths are 20ER (circles) and 18ER (squares).}\n\\label{fig1}\n\\end{figure}\nWe have previously measured the evolution of coherence between the lowest two vibrational states of potential wells \\cite{Ours}.\nThe dephasing time is about 0.3ms (T \u22c6\n2 ).\nThis dephasing is partly due to an inhomogeneous distribution of lattice depths as a result of the transverse Gaussian profile of the laser beams. To measure the homogeneous\ndecoherence time (T2), we perform pulse echoes, measuring the echo amplitude as a function of time \\cite{Ours}.\nFigure \\ref{fig1} shows two typical measurements of echo amplitude carried out on different dates under slightly different conditions such as different average lattice depths and\ndifferent dephasing times. The echo amplitude initially decays with a time constant of about 0.7ms, which is much faster than the photon scattering time (\u223c 60ms) in the lattice.\nIt then exhibits a 1ms-long coherence freeze followed by a final decay. Absent real decoherence on the short time scale of 1ms, only loss of frequency memory would inhibit\nthe appearance of echoes. This loss comes about when atoms experience time-varying frequencies. We use 2D pump-probe spectroscopy to monitor this frequency drift. Our 2D\npump-probe spectroscopy is essentially a version of spectral hole-burning for vibrational states. By monitoring the changes in the hole spectrum as a function of time we gain\ninformation on the atoms\u2019 frequency drift.\nInformation obtained from our 2D spectra enables us to characterize the temporal decay of frequency memory and through our simulations we find that \u201ccoherence freeze\" is related to\nthe shape of this memory loss function.\nSimilar plateaus in echo decay and a two-stage decay of echo amplitude have been observed in a Cooper-pair box \\cite{Nakamura}, for a single electron spin in a quantum dot\n\\cite{Vandersypen} and for electron spins in a semiconductor \\cite{SClark}. Those plateaus or two-stage decays have been either explained through {\\it{a priori}} models or simply\ndescribed phenomenologically. Here, we are introducing an experimental technique to directly probe the origin of plateaus.\nThe periodic potential in our experiment is formed by interfering two laser beams blue-detuned by 25GHz from the D2 transition line, F = 3 \u2192 F \u2032 = 4 (\u03bb = 780nm), thus\ntrapping atoms in the regions of low intensity, which minimizes the photon scattering rate and the transverse forces. The two laser beams intersect with parallel linear polarizations at\nan angle of \u03b8 = (49.0 \u00b1 0.2)\u25e6, resulting in a spacing of L = (0.930 \u00b1 0.004)\u00b5m between the wells. Due to gravity, the full effective potential also possesses a \u201ctilt\u201d of\n2.86ER per lattice site, where ER =\nh2\n8mL2 is the effective lattice recoil energy. The photon scattering time in our experiment is \u2248 60ms and the Landau-Zenner tunneling\ntimes for transitions from the lowest two levels are greater than 160ms.\nAtoms are loaded to the lattice during a molasses cooling stage and prepared in the ground vibrational state by adiabatic filtering \\cite{StefanQPT}. Due to the short coherence length\nof atoms in optical molasses (60nm at 10\u00b5K), there is no coherence between the wells. We measure populations of atoms in the ground vibrational, the first excited, and the (lossy)\nhigher excited states P1, P2, and PL, respectively, by fluorescence imaging of the atomic cloud after adiabatic filtering \\cite{StefanQPT}.\n...\nSubset: arXiv\nmeta:\nid: 1005.2635,\nlanguage_detection_score: 0.8389,\n...\nFigure 11: An example arXiv document in MATHPILE\n27\nA document from MATHPILE-StackExchange\nQuestion:\nTitle: Are fractions hard because they are like algebra?\nBody:\nIt occurs to me that to really understand the ways that people work with fractions on paper requires a good grasp of the ideas that numbers have multiple representations and that\nexpressions can be manipulated in various ways without changing the number they represent. These are essentially algebraic ideas.\nFor example, adding fractions requires us to rewrite the fractions in a different form, and then essentially factorise the expression. This is the same as rearranging expressions in algebra.\nDividing fractions requires us to rerepresent an operation like \u00f7 2\n3 as \u00d7 3\n2 . This is the same as realising the connection between operations that you use to solve equations in algebra.\nAnd cancelling down before multiplying is very sophisticated rewriting relying on various associative and commutative laws.\nSo it seems that we are really asking children to think in algebraic ways in order to understand fraction calculations well. This would seem to me to be a good reason why children and\nadults find it hard - they need more scaffolding in some abstract ideas.\nIs this a reasonable theory and has anyone written about this algebra-fractions connection before? To be clear, I am not asking if this is the only reason fractions are hard, but if there is\nany discussion out there to draw parallels between learning algebra and learning to manipulate fractions.\nId: 7826\nScore: 17\nTags: <algebra><fractions>\nLastEditorDisplayName: None\nOwnerDisplayName: None\nClosedDate: None\nFavoriteCount: None\nlanguage_detection_score: 0.9558\n...\nAnswers:\nBody: Not sure about paper references. One reason why people don\u00b4t understand fractions is because they are seemingly illogical.\nYou score one basket out of three 1/3.\nA little while later you try again and score 1/2. Clearly you have scored 2/5 shots? In many ways this is the correct answer. So why shouldn\u00b4t 1\n3 + 1\n2 = 2\n5\nPeople generally don\u00b4t understand equivalent fractions. It is strange for one farmer to say there are 4 sheep and another to say there are 8/2 sheep in the same field. People assume that\nthe number 4 does what it says on the tin and is how we always describe 4 ness of something. They don\u00b4t understand equivalence.\nPartly to blame is treating fractions like conjuring tricks. If this is the question...do this, if this is the question ...do another uncorrelated thing. I asked my class (who seemingly could\ncompute 2\n3 \u00d7 3\n5 correctly) to draw me a picture\ninstead of just multiplying. No one could do it yet they all said \"but it\u00b4s\nfrac615 you times the top and the bottom!\"\nI think drawing fractions is extremely useful. Draw 2\n3 \u00f7 2 or 2 \u00f7 2\n3 It\u00b4s not easy but I find students develop robustness eventually and begin to abstract themselves.\nId: 7827,\nScore: 9,\nis_accepted_answer: False,\nlanguage_detection_score: 0.9599,\nBody: The obvious (to me) source of difficulty is that fractions are just plain complicated, more so than almost anything else in elementary education. You have to operate with a pair of\nnumbers, instead of a single one, and you have to keep the order straight. Adding is quite complicated in its own right. Things are further complicated by rules about least common\ndenominators and least terms.\nI \u00b4m a little unclear about the question\u00b4s emphasis on algebra. Any sort of general rule or operation in arithmetic must have a connection to algebra, but I do not see what is intrinsically\ndifficult about algebra that relates to numeric fractions. Certainly some parts of algebra are hard, and some parts harder than others, algebraic fractions among them. It seems to me that\nfractions are difficult because it\u00b4s easy to confuse the various bits. Even when you\u00b4ve got them straight, they\u00b4re noticeably slower to use, take concentration, and when things have such\ncognitive demands, they\u00b4re harder to think with.\nConceptually, they\u00b4re a little bit odd, which is probably distracting until you get used to them. What they represent do not seem to apply to the same things that (whole) numbers do.\nEvidently fractions are not considered in this passage:\nIn that city, which was the oldest in the world, the cat was an object of veneration. Its worship was the religion of the country. The multiplication and addition of cats were a perpetual\ninstruction in arithmetic. Naturally, any inattention to the wants of a cat was punished with great severity in this world and the next... \u2013 A. Bierce, \"A Revolt of the Gods\"\nNow to have one-and-a-half cats seems a very different thing than to have three halves. In the former case, there\u00b4s a good chance that the one cat you have will be alive and purring,\nwhile the same could not possibly be said about any of the halves. No doubt such lessons are considered blasphemous in that city. While many things may be divided into parts \u2013 cars\nare a better example than cats \u2013 not many can be divided into equivalent parts that can be used as a basis for fractions. As we get used to fractions, as well as real numbers, we are\ntaught to ignore this and accept statements such as \"the average family has 2.4 children.\" Here is another example:\nBy then, she will have shed 80 of the 240 pounds she weighed in with when she entered Peter Bent Brigham hospital obesity program. A third of her left behind! \u2013 The Boston Herald\nAmerican, 7/7/77\nThe question seems to welcome references. There are certainly several that connect fractions with algebra. This paper,\nSeigler et al. (2013), Fractions: the new frontier for theories of numerical development, Trends in Cognitive Sciences,\nis a short survey of what is known and unknown about neural bases for one\u00b4s knowledge of fractions. Whole number arithmetic knowledge has been studied, and the authors suggest that\nthe representation of the knowledge fractions is an area ripe for investigation. It reviews (with references) why fractions are difficult and the relation of skill at fractions to skill at\nalgebra. Generally \u2013 or, rather, I only know of papers that discuss the connection in that direction, with algebra skill being dependent on fractions skill. (OTOH, I \u00b4m not widely read in\nthis area.)\nId: 7831,\nScore: 11,\nis_accepted_answer: False,\nlanguage_detection_score: 0.9780\nSubset: StackExchange\nFigure 12: An example StackExchange document in MATHPILE. Here is a question from \u201cmatheducators\u201d\n\u201c.stackexchang.com\u201d with two high-quality responses.\n28\nIn algebraic topology we often encounter chain complexes\nwith extra multiplicative structure.\nFor example, the\ncochain complex of a topological space has what is called\nthe E\u221e-algebra structure which comes from the cup prod-\nuct.\nIn this talk I present an idea for studying such chain com-\nplexes, E\u221e differential graded algebras (E\u221e DGAs), using\nstable homotopy theory. Namely, I discuss new equiva-\nlences between E\u221e DGAS that are defined using commuta-\ntive ring spectra.\nring spectra are equivalent. Quasi-isomorphic E\u221e DGAs\nare E\u221e topologically equivalent. However, the examples I\nam going to present show that the opposite is not true; there\nare E\u221e DGAs that are E\u221e topologically equivalent but not\nquasi-isomorphic. This says that between E\u221e DGAs, we\nhave more equivalences than just the quasi-isomorphisms.\nI also discuss interaction of E\u221e topological equiva-\nlences with the Dyer-Lashof operations and cases where\nE\u221etopological equivalences and quasi-isomorphisms\nagree.\n\u00d6zet : In algebraic topology we often encounter chain\ncomplexes with extra multiplicative structure. For example,\nthe cochain complex of a topological space has what is\ncalled the E\u221e-algebra structure which comes from the cup\nproduct. In this talk I present an idea for studying such\nchain complexes, E\u221e differential graded algebras (E\u221e\nDGAs), using stable homotopy theory. Namely, I discuss\nnew equivalences between E\u221e DGAS that are defined using\ncommutative ring spectra.We say E\u221e DGAs are E\u221e topo-\nlogically equivalent when the corresponding commutative\nring spectra are equivalent. Quasi-isomorphic E\u221e DGAs\nare E\u221e topologically equivalent. However, the examples I\nam going to present show that the opposite is not true; there\nare E\u221e DGAs that are E\u221e topologically equivalent but not\nquasi-isomorphic. This says that between E\u221e DGAs, we\nhave more equivalences than just the quasi-isomorphisms.\nI also discuss interaction of E\u221e topological equivalences\nwith the Dyer-Lashof operations and cases where E\u221e topo-\nlogical equivalences and quasi-isomorphisms agree.\nUniversit\u00e9 de la Saskatchewan,\n1 - 4 juin 2015\nwww.smc.math.ca//2015f\nComit\u00e9 d\u2019organisation\nFinancement \u00e9tudiants\nMinisymposia invit\u00e9s\nMinisymposia libres\nConf\u00e9rences libres\nHoraire - Minisymposa invit\u00e9s\nOpen Problems\nGraphs and matrices\nResponsable et pr\u00e9sident: Shaun Fallat et Karen Meagher\n(University of Regina)\nWAYNE BARRETT, Brigham Young University\nThe Fielder Vector and Tree Decompositions of Graphs\n[PDF]\nIn the 1970\u2019s Fiedler initiated a study of the second smallest\neigenvalue of the Laplacian matrix L of a graph and the\ncorresponding eigenvector(s). These \"Fiedler\" vectors have\nbecome spectacularly successful in revealing properties of\nthe associated graph. A tree decomposition T of a graph\nG = (V, E) is an associated tree whose nodes are subsets\nof V and whose edge set respects the structure of G. Tree\ndecompositions have been used in the analysis of complex\nnetworks. This talk reports on an algorithm developed\nby students at BYU for obtaining a tree decomposition by\nmeans of Fiedler vector(s) of G.\n...\nGraphs that have a weighted adjacency matrix with spec-\ntrum {\u03bbn\u22122\n1\n, \u03bb2\n2} [PDF]\nIn this talk I will characterize the graphs which have an\nedge weighted adjacency matrix belonging to the class of\nn \u00d7 n involutions with spectrum equal to {\u03bbn\u22122\n1\n, \u03bb2\n2} for\nsome \u03bb1 and some \u03bb2. The connected graphs turn out to be\nthe cographs constructed as the join of at least two unions\nof pairs of complete graphs, and possibly joined with one\nother complete graph.\nUniversity\nof\nSaskatchewan,\nJune\n1\n-\n4,\n2015\nwww.cms.math.ca//2015\nInvited Minisymposia\nContributed Minisymposia\nContributed Talks\nGraphs and matrices\nOrganizer and Chair: Shaun Fallat and Karen Meagher\n(University of Regina)\nWAYNE BARRETT, Brigham Young University\nThe Fielder Vector and Tree Decompositions of Graphs\n[PDF]\nIn the 1970\u2019s Fiedler initiated a study of the second smallest\neigenvalue of the Laplacian matrix L of a graph and the\ncorresponding eigenvector(s). These \"Fiedler\" vectors have\nbecome spectacularly successful in revealing properties of\nthe associated graph. A tree decomposition T of a graph\nG = (V, E) is an associated tree whose nodes are subsets\nof V and whose edge set respects the structure of G. Tree\ndecompositions have been used in the analysis of complex\nnetworks. This talk reports on an algorithm developed\nby students at BYU for obtaining a tree decomposition by\nmeans of Fiedler vector(s) of G.\n...\nGraphs that have a weighted adjacency matrix with spec-\ntrum {\u03bbn\u22122\n1\n, \u03bb2\n2} [PDF]\nIn this talk I will characterize the graphs which have an\nedge weighted adjacency matrix belonging to the class of\nn \u00d7 n involutions with spectrum equal to {\u03bbn\u22122\n1\n, \u03bb2\n2} for\nsome \u03bb1 and some \u03bb2. The connected graphs turn out to be\nthe cographs constructed as the join of at least two unions\nof pairs of complete graphs, and possibly joined with one\nother complete graph.\nTable 8: Near-duplication matches found in CommonCrawl by MinHash LSH deduplication (in italics).\n29\n\\begin{document}\n\\title{Querying Guarded Fragments via Resolution}\n\\section{A detailed example}\nHere we include some equations and theorem-like environments to show\nhow these are labeled in a supplement and can be referenced from the\nmain text.\nConsider the following equation:\n\\begin{equation}\n\\label{eq:suppa}\na^2 + b^2 = c^2.\n\\end{equation}\nYou can also reference equations such as \\cref{eq:matrices,eq:bb}\nfrom the main article in this supplement.\n\\lipsum[100-101]\n\\begin{theorem}\nAn example theorem.\n\\end{theorem}\n\\lipsum[102]\n\\begin{lemma}\nAn example lemma.\n\\end{lemma}\n\\lipsum[103-105]\nHere is an example citation: \\cite{KoMa14}.\n\\section[Proof of Thm]{Proof of \\cref{thm:bigthm}}\n\\label{sec:proof}\n\\lipsum[106-112]\n\\section{Additional experimental results}\n\\Cref{tab:foo} shows additional\nsupporting evidence.\n\\begin{table}[htbp]\n{\\footnotesize\n\\caption{Example table} \\label{tab:foo}\n\\begin{center}\n\\begin{tabular}{|c|c|c|} \\hline\nSpecies & \\bf Mean & \\bf Std.\u223cDev. \\\\\\hline\n1 & 3.4 & 1.2 \\\\\n2 & 5.4 & 0.6 \\\\\\hline\n\\end{tabular}\n\\end{center}\n}\n\\end{table}\n\\end{document}\n\\begin{document}\n\\title{Limited memory Kelley\u2019s Method Converges for Composite\nConvex and Submodular Objectives}\n\\section{A detailed example}\nHere we include some equations and theorem-like environments to show\nhow these are labeled in a supplement and can be referenced from the\nmain text.\nConsider the following equation:\n\\begin{equation}\n\\label{eq:suppa}\na^2 + b^2 = c^2.\n\\end{equation}\nYou can also reference equations such as \\cref{eq:matrices,eq:bb}\nfrom the main article in this supplement.\n\\lipsum[100-101]\n\\begin{theorem}\nAn example theorem.\n\\end{theorem}\n\\lipsum[102]\n\\begin{lemma}\nAn example lemma.\n\\end{lemma}\n\\lipsum[103-105]\nHere is an example citation: \\cite{KoMa14}.\n\\section[Proof of Thm]{Proof of \\cref{thm:bigthm}}\n\\label{sec:proof}\n\\lipsum[106-112]\n\\section{Additional experimental results}\n\\Cref{tab:foo} shows additional\nsupporting evidence.\n\\begin{table}[htbp]\n{\\footnotesize\n\\caption{Example table} \\label{tab:foo}\n\\begin{center}\n\\begin{tabular}{|c|c|c|} \\hline\nSpecies & \\bf Mean & \\bf Std.\u223cDev. \\\\\\hline\n1 & 3.4 & 1.2 \\\\\n2 & 5.4 & 0.6 \\\\\\hline\n\\end{tabular}\n\\end{center}\n}\n\\end{table}\n\\end{document}\nTable 9: A near-duplication match found in arXiv by MinHashLSH deduplication (in italics).\n30\n\\section{Definition:Constructed Semantics\n/Instance 4/Rule of Idempotence}\nTags: Formal Semantics\n\\begin{theorem}\nThe Rule of Idempotence:\n:$(p \\lor p) \\implies p$\nis a tautology in Instance 4 of\nconstructed semantics.\n\\end{theorem}\n\\begin{proof}\nBy the definitional abbreviation for the\nconditional:\n:$\\mathbf A \\implies \\mathbf B =_{\\text{\ndef}} \\neg \\mathbf A \\lor \\mathbf B$\nthe Rule of Idempotence can be written as\n:\n: $\\neg \\left({p \\lor p}\\right) \\lor p$\nThis evaluates as follows:\n:$\\begin{array }{| cccc|c|c|} \\hline\n\\neg & (p & \\lor & p) & \\lor & p \\\\\n\\hline\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 1 & 0 & 1 \\\\\n0 & 2 & 2 & 2 & 0 & 2 \\\\\n2 & 3 & 3 & 3 & 0 & 3 \\\\\n\\hline\n\\end{array}$\n{{qed}}\nCategory:Formal Semantics\n\\end{proof}\n\\section{Definition:Constructed Semantics\n/Instance 5/Rule of Idempotence}\nTags: Formal Semantics\n\\begin{theorem}\nThe Rule of Idempotence:\n:$(p \\lor p) \\implies p$\nis a tautology in Instance 5 of\nconstructed semantics.\n\\end{theorem}\n\\begin{proof}\nBy the definitional abbreviation for the\nconditional:\n:$\\mathbf A \\implies \\mathbf B =_{\\text{\ndef}} \\neg \\mathbf A \\lor \\mathbf B$\nthe Rule of Idempotence can be written as\n:\n: $\\neg \\left({p \\lor p}\\ right) \\lor p$\nThis evaluates as follows:\n:$\\begin{array }{| cccc|c|c|} \\hline\n\\neg & (p & \\lor & p) & \\lor & p \\\\\n\\hline\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 1 & 0 & 1 \\\\\n3 & 2 & 2 & 2 & 0 & 2 \\\\\n0 & 3 & 3 & 3 & 0 & 3 \\\\\n\\hline\n\\end{array}$\n{{qed}}\nCategory:Formal Semantics\n\\end{proof}\n\\section{Imaginary Part of Complex\nProduct}\nTags: Complex Multiplication\n\\begin{theorem}\nLet $z_1$ and $z_2$ be complex numbers.\nThen:\n:$\\map \\Im {z_1 z_2} = \\map \\Re {z_1} \\,\n\\map \\Im {z_2} + \\map \\Im {z_1} \\, \\\nmap \\Re {z_2}$\n\\end{theorem}\n\\begin{proof}\nLet $z_1 = x_1 + i y_1$ and $z_2 = x_2 +\ni y_2$.\nBy definition of complex multiplication:\n:$z_1 z_2 = x_1 x_2 - y_1 y_2 + i \\paren\n{x_1 y_2 + x_2 y_1}$\nThen\n{{begin -eqn}}\n{{eqn | l = \\map \\Im {z_1 z_2}\n| r = x_1 y_2 + x_2 y_1\n| c = {{Defof|Imaginary Part}}\n}}\n{{eqn | r = \\map \\Re {z_1} \\, \\map \\Im {\nz_2} + \\map \\Im {z_1} \\, \\map \\Re {z_2\n}\n| c = {{Defof|Imaginary Part}}\n}}\n{{end -eqn}}\n{{qed}}\n\\end{proof}\n\\section{Real Part of Complex Product}\nTags: Complex Multiplication\n\\begin{theorem}\nLet $z_1$ and $z_2$ be complex numbers.\nThen:\n:$\\map \\Re {z_1 z_2} = \\map \\Re {z_1} \\\nmap \\Re {z_2} - \\map \\Im {z_1} \\map \\\nIm {z_2}$\n\\end{theorem}\n\\begin{proof}\nLet $z_1 = x_1 + i y_1$ and $z_2 = x_2 +\ni y_2$.\nBy definition of complex multiplication:\n:$z_1 z_2 = x_1 x_2 - y_1 y_2 + i \\paren\n{x_1 y_2 + x_2 y_1}$\nThen:\n{{begin -eqn}}\n{{eqn | l = \\map \\Re {z_1 z_2}\n| r = x_1 x_2 - y_1 y_2\n| c = {{ Defof|Real Part}}\n}}\n{{eqn | r = \\map \\Re {z_1} \\map \\Re {z_2}\n- \\map \\Im {z_1} \\map \\Im {z_2}\n| c = {{ Defof|Real Part}}\n}}\n{{end -eqn}}\n{{qed}}\n\\end{proof}\nTable 10: Near-duplication matches found in ProofWiki by MinHash LSH deduplication.\n31\n# HP-42S\nThe **HP-42S RPN Scientific** is a programmable RPN\nScientific hand held calculator introduced by Hewlett-\nPackard in 1988. It has advanced functions suitable for ap-\nplications in mathematics, linear algebra, statistical analy-\nsis, computer science and others.\nHP-42S\nThe HP-42S\n\u2014\nType| Programmable scientific\nManufacturer| Hewlett-Packard\nIntroduced| 1988\nDiscontinued| 1995 Calculator\nEntry mode| RPN\nPrecision| 12 display digits (15 digits internally),[1] expo-\nnent \u00b1499\nDisplay type| LCD dot-matrix\nDisplay size| 2 lines, 22 characters, 131\u00d716 pixels CPU\nProcessor| Saturn (Lewis) Programming\nProgramming language(s)| RPN key stroke (fully merged)\nFirmware memory| 64 KB of ROM\nProgram steps| 7200 Interfaces\nPorts| IR (Infrared) printing Other\nPower supply| 3\u00d71.5V button cell batteries (Panasonic\nLR44, Duracell PX76A/675A or Energizer 357/303)\nWeight| 6 oz (170 g)\nDimensions| 148\u00d780\u00d715mm\n## Overview\nPerhaps the HP-42S was to be released as a replacement\nfor the aging HP-41 series as it is designed to be compatible\nwith all programs written for the HP-41. Since it lacked\nexpandability, and lacked any real I/O ability, both key\nfeatures of the HP-41 series, it was marketed as an HP-15C\nreplacement.\nThe 42S, however, has a much smaller form factor than the\n41, and features many more built-in functions, such as a\nmatrix editor, complex number support, an equation solver,\nuser-defined menus, and basic graphing capabilities (the\n42S can draw graphs only by programs). Additionally, it\nfeatures a two-line dot matrix display, which made stack\nmanipulation easier to understand.\nProduction of the 42S ended in 1995.[2] As this calculator\nis regarded amongst the best ever made in terms of quality,\nkey stroke feel, ease of programming, and daily usability\nfor engineers,[3] in the HP calculator community the 42S\nhas become famous for its high prices in online auctions,\nup to several times its introduction price, which has created\na scarcity for utility end users.\n# HP-42S\nThe **HP-42S RPN Scientific** is a programmable RPN\nScientific hand held calculator introduced by Hewlett-\nPackard in 1988. It has advanced functions suitable for ap-\nplications in mathematics, linear algebra, statistical analy-\nsis, computer science and others.\nHP-42S\nThe HP-42S\n\u2014\nType| Programmable scientific\nManufacturer| Hewlett-Packard\nIntroduced| 1988\nDiscontinued| 1995 Calculator\nEntry mode| RPN\nPrecision| 12 display digits (15 digits internally),[1] expo-\nnent \u00b1499\nDisplay type| LCD dot-matrix\nDisplay size| 2 lines, 22 characters, 131\u00d716 pixels CPU\nProcessor| Saturn (Lewis) Programming\nProgramming language(s)| RPN key stroke (fully merged)\nFirmware memory| 64 KB of ROM\nProgram steps| 7200 Interfaces\nPorts| IR (Infrared) printing Other\nPower supply| 3\u00d71.5V button cell batteries (Panasonic\nLR44, Duracell PX76A/675A or Energizer 357/303)\nWeight| 6 oz (170 g)\nDimensions| 148\u00d780\u00d715mm\n## Overview\nPerhaps the HP-42S was to be released as a replacement\nfor the aging HP-41 series as it is designed to be compatible\nwith all programs written for the HP-41. Since it lacked\nexpandability, and lacked any real I/O ability, both key\nfeatures of the HP-41 series, it was marketed as an HP-15C\nreplacement.\nThe 42S, however, has a much smaller form factor than the\n41, and features many more built-in functions, such as a\nmatrix editor, complex number support, an equation solver,\nuser-defined menus, and basic graphing capabilities (the\n42S can draw graphs only by programs). Additionally, it\nfeatures a two-line dot matrix display, which made stack\nmanipulation easier to understand.\nProduction of the 42S ended in 1995.[2] As this calculator\nis regarded amongst the best ever made in terms of quality,\nkey stroke feel, ease of programming, and daily usability\nfor engineers,[3] in the HP calculator community the 42S\nhas become famous for its high prices in online auctions,\nup to several times its introduction price, which has created\na scarcity for utility end users.\nTable 11: Duplication matches found in Wikipedia by MinHash LSH deduplication (in italics).\n32\n# Basic Concepts in Graph Theory\n## Section 1: What is a Graph?\nThere are various types of graphs, each with its own defini-\ntion. Unfortunately, some people apply the term \"graph\"\nrather loosely, so you can\u2019t be sure what type of graph\nthey\u2019re talking about unless you ask them. After you have\nfinished this chapter, we expect you to use the terminology\ncarefully, not loosely. To motivate the various definitions,\nwe\u2019ll begin with some examples.\nExample 1 (A computer network) Computers are often\nlinked with one another so that they can interchange in-\nformation. Given a collection of computers, we would like\nto describe this linkage in fairly clean terms so that we can\nanswer questions such as \"How can we send a message\nfrom computer A to computer B using the fewest possible\nintermediate computers?\"\nWe could do this by making a list that consists of pairs of\ncomputers that are connected. Note that these pairs are\nunordered since, if computer C can communicate with com-\nputer D, then the reverse is also true. (There are sometimes\nexceptions to this, but they are rare and we will assume that\nour collection of computers does not have such an excep-\ntion.) Also, note that we have implicitly assumed that the\ncomputers are distinguished from each other: It is insuffi-\ncient to say that \"A PC is connected to a Mac.\" We must\nspecify which PC and which Mac. Thus, each computer\nhas a unique identifying label of some sort.\nFor people who like pictures rather than lists, we can put\ndots on a piece of paper, one for each computer. We label\neach dot with a computer\u2019s identifying label and draw a\ncurve connecting two dots if and only if the correspond-\ning computers are connected. Note that the shape of the\ncurve does not matter (it could be a straight line or some-\nthing more complicated) because we are only interested in\nwhether two computers are connected or not. Below are\ntwo such pictures of the same graph. Each computer has\nbeen labeled by the initials of its owner.\n...\n## Basic Concepts in Graph Theory\nThe notation Pk(V ) stands for the set of all k-element\nsubsets of the set V . Based on the previous example we\nhave\nDefinition 1 (Simple graph) A simple graph G is a pair\nG = (V, E) where\n- V is a finite set, called the vertices of G, and\n- E is a subset of P2(V ) (i.e., a set E of two-element subsets\nof V ), called the edges of G.\n...\n# Basic Concepts in Graph Theory\n## Section 1: What is a Graph?\nThere are various types of graphs, each with its own defini-\ntion. Unfortunately, some people apply the term \"graph\"\nrather loosely, so you can\u2019t be sure what type of graph\nthey\u2019re talking about unless you ask them. After you have\nfinished this chapter, we expect you to use the terminology\ncarefully, not loosely. To motivate the various definitions,\nwe\u2019ll begin with some examples.\nExample 1 (A computer network) Computers are often\nlinked with one another so that they can interchange in-\nformation. Given a collection of computers, we would like\nto describe this linkage in fairly clean terms so that we can\nanswer questions such as \"How can we send a message\nfrom computer A to computer B using the fewest possible\nintermediate computers?\"\nWe could do this by making a list that consists of pairs of\ncomputers that are connected. Note that these pairs are\nunordered since, if computer C can communicate with com-\nputer D, then the reverse is also true. (There are sometimes\nexceptions to this, but they are rare and we will assume that\nour collection of computers does not have such an excep-\ntion.) Also, note that we have implicitly assumed that the\ncomputers are distinguished from each other: It is insuffi-\ncient to say that \"A PC is connected to a Mac.\" We must\nspecify which PC and which Mac. Thus, each computer\nhas a unique identifying label of some sort.\nFor people who like pictures rather than lists, we can put\ndots on a piece of paper, one for each computer. We label\neach dot with a computer\u2019s identifying label and draw a\ncurve connecting two dots if and only if the correspond-\ning computers are connected. Note that the shape of the\ncurve does not matter (it could be a straight line or some-\nthing more complicated) because we are only interested in\nwhether two computers are connected or not. Below are\ntwo such pictures of the same graph. Each computer has\nbeen labeled by the initials of its owner.\n...\n## Basic Concepts in Graph Theory\nThe notation Pk(V ) stands for the set of all k-element\nsubsets of the set V . Based on the previous example we\nhave\nDefinition 1 (Simple graph) A simple graph G is a pair\nG = (V, E) where\n- V is a finite set, called the vertices of G, and\n- E is a subset of P2(V ) (i.e., a set E of two-element subsets\nof V ), called the edges of G.\n...\nTable 12: Duplication matches found in Textbooks by MinHash LSH deduplication (in italics).\n33\nThis was originally posted on mathoverflow, but it seems\nit\u2019s more appropriate to post here.\nLet B be a paracompact space with the property that any\n(topological) vector bundle E \u2192 B is trivial. What are\nsome non-trivial examples of such spaces, and are there\nany interesting properties that characterize them?\nFor simple known examples we of course have contractible\nspaces, as well as the 3-sphere S3. This one follows from\nthe fact that its rank n vector bundles are classified by\n\u03c03(BO(n)) = \u03c02(O(n)) = 0. I\u2019m primarily interested in\nthe case where B is a closed manifold. Do we know any\nother such examples?\nThere is this nice answer to a MSE question which talks\nabout using the Whitehead tower of the appropriate clas-\nsifying space to determine whether a bundle is trivial or\nnot. This seems like a nice tool (of which I am not familiar\nwith) to approaching this problem. As a secondary question,\ncould I ask for some insight/references to this approach?\nEDIT Now that we know from the answer all the examples\nfor closed 3-manifolds (integral homology spheres), I guess\nI can now update the question to the case of higher odd di-\nmensions. Does there exist a higher dimensional example?\nLet B be a paracompact space with the property that any\n(topological) vector bundle E \u2192 B is trivial. What are\nsome non-trivial examples of such spaces, and are there\nany interesting properties that characterize them?\nFor simple known examples we of course have contractible\nspaces, as well as the 3-sphere S3. This one follows from\nthe fact that its rank n vector bundles are classified by\n\u03c03(BO(n)) = \u03c02(O(n)) = 0. I\u2019m primarily interested in\nthe case where B is a closed manifold. Do we know any\nother such examples?\nThere is this nice answer to a MSE question which talks\nabout using the Whitehead tower of the appropriate clas-\nsifying space to determine whether a bundle is trivial or\nnot. This seems like a nice tool (of which I am not familiar\nwith) to approaching this problem. As a secondary question,\ncould I ask for some insight/references to this approach?\nEDIT Now that we know from the answers all the examples\nfor closed 3-manifolds, I guess I can now update the ques-\ntion to the case of higher odd dimensions. Does there exist\na higher dimensional example?\nThis\nis\na\ncopy\nof\nmy\nquestion\non\nMSE\n(https://math.stackexchange.com/questions/3372432)\nbecause this forum seems better suited for historical\nquestions:\nIn 1985, Gosper used the not-yet-proven formula by Ra-\nmanujan\n1\n\u03c0 = 2\n\u221a\n2\n992 \u00b7\n\u221e\nX\nn=0\n(4n)!\n(n!)4 \u00b7 26390n + 1103\n3964n\nto compute 17 \u00b7 106 digits of \u03c0, at that time a new world\nrecord.\nHere (https://www.cs.princeton.edu/courses/archive/fall98/\ncs126/refs/pi-ref.txt) it reads:\nThere were a few interesting things about Gosper\u2019s com-\nputation. First, when he decided to use that particular\nformula, there was no proof that it actually converged to\npi! Ramanujan never gave the math behind his work, and\nthe Borweins had not yet been able to prove it, because\nthere was some very heavy math that needed to be worked\nthrough. It appears that Ramanujan simply observed the\nequations were converging to the 1103 in the formula, and\nthen assumed it must actually be 1103. (Ramanujan was\nnot known for rigor in his math, or for providing any proofs\nor intermediate math in his formulas.) The math of the\nBorwein\u2019s proof was such that after he had computed 10\nmillion digits, and verified them against a known calcula-\ntion, his computation became part of the proof. Basically it\nwas like, if you have two integers differing by less than one,\nthen they have to be the same integer.\nNow my historical question: Who was the first to prove this\nformula? Was it Gosper because he added the last piece\nof the proof, or was it the Borweins, afterwards? And was\nGosper aware of this proof when he did his computation?\nIn 1985, Gosper used the not-yet-proven formula by Ra-\nmanujan\n1\n\u03c0 = 2\n\u221a\n2\n992 \u00b7\n\u221e\nX\nn=0\n(4n)!\n(n!)4 \u00b7 26390n + 1103\n994n\nto compute 17 \u00b7 106 digits of \u03c0, at that time a new world\nrecord.\nHere (https://www.cs.princeton.edu/courses/archive/fall98/\ncs126/refs/pi-ref.txt) it reads:\nThere were a few interesting things about Gosper\u2019s com-\nputation. First, when he decided to use that particular\nformula, there was no proof that it actually converged to\npi! Ramanujan never gave the math behind his work, and\nthe Borweins had not yet been able to prove it, because\nthere was some very heavy math that needed to be worked\nthrough. It appears that Ramanujan simply observed the\nequations were converging to the 1103 in the formula, and\nthen assumed it must actually be 1103. (Ramanujan was\nnot known for rigor in his math, or for providing any proofs\nor intermediate math in his formulas.) The math of the\nBorwein\u2019s proof was such that after he had computed 10\nmillion digits, and verified them against a known calcula-\ntion, his computation became part of the proof. Basically it\nwas like, if you have two integers differing by less than one,\nthen they have to be the same integer.\nNow my historical question: Who was the first to prove this\nformula? Was it Gosper because he added the last piece\nof the proof, or was it the Borweins, afterwards? And was\nGosper aware of this proof when he did his computation?\nTable 13: Near-duplication matches found in StackExchange by MinHash LSH deduplication (in italics).\n34\nCoin A is flipped three times and coin B is flipped four times. What is the probability that the number of heads\nobtained from flipping the two fair coins is the same?\nVideo Solution\nAnswer:\n## Problem 3.2.2 (AMC 10)\nTwo tour guides are leading six tourists. The guides decide to split up. Each tourist must choose one of the guides, but\nwith the stipulation that each guide must take at least one tourist. How many different groupings of guides and tourists\nare possible?\n......\nOne morning each member of Angela\u2019s family drank an 8-ounce mixture of coffee with milk. The amounts of coffee\nand milk varied from cup to cup, but were never zero. Angela drank a quarter of the total amount of milk and a sixth of\nthe total amount of coffee. How many people are in the family?\nAnswer:\n## Problem 20.2.15 (AMC 12)\nThe state income tax where Kristin lives is levied at the rate of p% of the first $28000 of annual income plus (p + 2)%\nof any amount above $28000. Kristin noticed that the state income tax she paid amounted to (p + 0.25)% of her\nannual income. What was her annual income?\nAnswer:\n......\nFind the least positive integer k for which the equation\n\u0004 2002\nn\n\u0005\n= k has no integer solutions for n. (The notation \u230ax\u230b\nmeans the greatest integer less than or equal to x.)\nAnswer:\n## Problem 40.1.9 (AIME)\nFind the number of positive integers n less than 1000 for which there exists a positive real number x such that\nn = x\u230ax\u230b.\u2019, \u201d, \u2019Note: \u230ax\u230b is the greatest integer less than or equal to x.\u2019\n......\nWhat is the sum of the roots of z12 = 64 that have a positive real part?\nAnswer:\n## Problem 45.8.13 (AMC 12)\nThe complex numbers z and w satisfy z13 = w, w11 = z, and the imaginary part of z is sin m\u03c0\nn , for relatively prime\npositive integers m and n with m < n. Find n.\u2019\nAnswer:\n......\nTable 14: Exact match examples from the test set of MATH benchmark found in Textbooks by line-level exact\nmatch deduplication (in italics).\n35\nLet x and y be real numbers satisfying x4y5 + y4x5 = 810 and x3y6 + y3x6 = 945. Evaluate 2x3 + (xy)3 + 2y3.\nLet x1 < x2 < x3 be the three real roots of the equation\n\u221a\n2014x3 \u2212 4029x2 + 2 = 0. Find x2(x1 + x3).\nLet m be the largest real solution to the equation\n3\nx \u2212 3 +\n5\nx \u2212 5 +\n17\nx \u2212 17 +\n19\nx \u2212 19 = x2 \u2212 11x \u2212 4\nThere are positive integers a, b, and c such that m = a +\np\nb + \u221ac. Find a + b + c.\nLet f(x) = x4 + ax3 + bx2 + cx + d. If f(\u22121) = \u22121, f(2) = \u22124, f(\u22123) = \u22129, and f(4) = \u221216. Find f(1).\nSolve in positive integers x2 \u2212 4xy + 5y2 = 169.\nSolve in integers the question x + y = x2 \u2212 xy + y2.\nSolve in integers\nx+y\nx2\u2212xy+y2 = 3\n7\nProve the product of 4 consecutive positive integers is a perfect square minus 1.\nFor any arithmetic sequence whose terms are all positive integers, show that if one term is a perfect square, this\nsequence must have infinite number of terms which are perfect squares.\nProve there exist infinite number of positive integer a such that for any positive integer n, n4 + a is not a prime\nnumber.\n......\nThe real root of the equation 8x3 \u2212 3x2 \u2212 3x \u2212 1 = 0 can be written in the form\n3\u221aa+ 3\u221a\nb+1\nc\n, where a, b, and c are\nposit ive integers. Find a + b + c.\nFind the number of positive integers m for which there exist nonnegative integers x0, x1 , . . . , x2011 such that\nmx0 =\n2011\nX\nk=1\nmxk.\nSuppose x is in the interval [0, \u03c0\n2 ] and log24 sin x(24 cos x) = 3\n2. Find 24 cot2 x.\nLet P(x) be a quadratic polynomial with real coefficients satisfying x2 \u2212 2x + 2 \u2264 P(x) \u2264 2x2 \u2212 4x + 3 for all\nreal numbers x, and suppose P(11) = 181. Find P(16).\nLet (a, b, c) be the real solution of the system of equations x3 \u2212 xyz = 2, y3 \u2212 xyz = 6, z3 \u2212 xyz = 20. The\ngreatest possible value of a3 + b3 + c3 can be written in the form m\nn , where m and n are relatively prime positive\nintegers. Find m + n.\nFind the smallest positive integer n with the property that the polynomial x4 \u2212 nx + 63 can be written as a product of\ntwo nonconstant polynomials with integer coefficients.\nThe zeros of the function f(x) = x2 \u2212 ax + 2a are integers. What is the sum of the possible values of a?\nLet a, b, and c be three distinct one-digit numbers. What is the maximum value of the sum of the roots of the equation\n(x \u2212 a)(x \u2212 b) + (x \u2212 b)(x \u2212 c) = 0?\nAt the theater children get in for half price. The price for 5 adult tickets and 4 child tickets is 24.50. How much would\n8 adult tickets and 6 child tickets cost?\nThe quadratic equation x2 + px + 2p = 0 has solutions x = a and x = b. If the quadratic equation x2 + cx + d = 0\nhas solutions x = a + 2 and x = b + 2, what is the value of d?\n......\nFind the smallest positive integer n with the property that the polynomial x4 \u2212 nx + 63 can be written as a product of\ntwo nonconstant polynomials with integer coefficients.\nThe zeros of the function f(x) = x2 \u2212 ax + 2a are integers. What is the sum of the possible values of a?\nLet a, b, and c be three distinct one-digit numbers. What is the maximum value of the sum of the roots of the equation\n(x \u2212 a)(x \u2212 b) + (x \u2212 b)(x \u2212 c) = 0 ?\nAt the theater children get in for half price. The price for 5 adult tickets and 4 child tickets is 24.50. How much would\n8 adult tickets and 6 child tickets cost?\nThe quadratic equation x2 + px + 2p = 0 has solutions x = a and x = b. If the quadratic equation x2 + cx + d = 0\nhas solutions x = a + 2 and x = b + 2, what is the value of d?\nPolynomialAndEquation Root Delta SpecialEquation Function NumberTheoryBasic IndeterminateEquation\nSqueezeMethod Pythagore anTripletFormula TrigIdentity Inequality LogicalAndReasoning\nAMC10/12 AIME IMO\nUS International\nWith Solutions\n\u00a9 2009 - 2023 Math All Star\n......\nTable 15: Exact match examples from the test set of MATH benchmark found in CommonCrawl by line-level exact\nmatch deduplication (in italics). In these examples, we only observe repeated questions from MATH, but do not\nidentify duplicate answers.\n36\nThe sum of an infinite geometric series is a positive number S, and the second term in the series is 1. What is the\nsmallest possible value of S?\n(A) 1+\n\u221a\n5\n2\n(B) 2\n(C)\n\u221a\n5\n(D) 3\n(E) 4\n## Problem 17\nAll the numbers 2, 3, 4, 5, 6, 7 are assigned to the six faces of a cube, one number to each face. For each of the eight\nvertices of the cube, a product of three numbers is computed, where the three numbers are the numbers assigned to the\nthree faces that include that vertex. What is the greatest possible value of the sum of these eight products?\n(A) 312\n(B) 343\n(C) 625\n(D) 729\n(E) 1680\n...\nWhat is the value of b + c if x2 + bx + c > 0 only when x \u2208 (\u2212\u221e, \u22122) \u222a (3, \u221e)?\nMay 11, 2020\n...\nAn ambulance travels at 40 mph and can follow a 20-mile route making no stops to get to the hospital. A helicopter\ntravels at one mile per minute, and the air route is 15 miles to get to the same hospital. However, the helicopter takes\nthree minutes for takeoff and three minutes for landing. How many fewer minutes does it take for the helicopter to\ncomplete its trip (takeoff, flight and landing) than for the ambulance to complete its trip?\nApr 6, 2020\n#1\n+34\n0\nKeep in mind that Time=Distance/Speed\nWhat is the greatest possible area of a triangular region with one vertex at the center of a circle of radius 1 and the\nother two vertices on the circle?\nA bad first step is to put the center at the origin, one point at (1,0) , and one point at (sin x, cos x).\nA start is the area of a triangle with included angle expression\na \u00d7 b \u00d7 sin \u03b8\n2\nAssuming \u03b8 in radians. If theta is \u03c0/2 then we have a right triangle. Let a=b=1. Area expression is\nA = (sin \u03b8)/2\nThis is maximum for \u03b8 = \u03c0/2.\nAnswer is maximum area for a right triangle.\n...\nTable 16: Exact match examples from the test set of MATH benchmark (upper) and MMLU-STEM (bottom) found\nin OpenWebMath by line-level exact match deduplication (in italics). In these examples, we only observe repeated\nquestions, but do not identify duplicate answers.\n37\n"
  },
  {
    "title": "Unsupervised Universal Image Segmentation",
    "link": "https://arxiv.org/pdf/2312.17243.pdf",
    "upvote": "18",
    "text": "Unsupervised Universal Image Segmentation\nDantong Niu*\u2020\nXudong Wang*\u2020\nXinyang Han*\nLong Lian\nRoei Herzig\nTrevor Darrell\nBerkeley AI Research, UC Berkeley\nCode: https://github.com/u2seg/U2Seg\nAP50\nAR100\n18.5\n11.8\n9.3\n9\nCutLER\nU2Seg\nUnsup. Semantic Seg.\nUnsup. Instance Seg.\nUnsup. Panoptic Seg\nmIoU\nPixelAcc\n63.9\n30.2\n56.9\n28.2\nSTEGO\nU2Seg\nPQ\nSQ\n52.7\n17.6\n36.1\n12.4\nCutLER + STEGO\nU2Seg\nAP (w/ 1%) AP (w/ 2%)\n22.6\n19.2\n18.8\n14.8\nCutLER\nU2Seg\nLabel-Efficient Learning\nBenchmark \nResults\nSample  \nResults\nTasks\nUnsupervised Semantic Segmentation\nUnsupervised Instance Segmentation\nUnsupervised Panoptic Segmentation\n+2.0\n+7.0\n+9.2\n+1.9\n+5.2\n+16.6\n+4.4\n+3.8\n24\n2\n10\n13\nCityscapes\nPQ\nSQ\n71.1\n16.1\n64.9\n12.4\n+3.7\n+6.2\nCOCO\nCOCO\nCOCO\nCOCO\nFigure 1. We present U2Seg, a unified framework for Unsupervised Universal image Segmentation that consistently outperforms previous\nstate-of-the-art methods designed for individual tasks: CutLER [60] for unsupervised instance segmentation, STEGO [24] for unsupervised\nsemantic segmentation, and the naive combination of CutLER and STEGO for unsupervised panoptic segmentation. We visualize instance\nsegmentation results with \u201csemantic label\u201d + confidence score and semantic predictions with \u201csemantic label\u201d. Zoom in for the best view.\nAbstract\nSeveral unsupervised image segmentation approaches have\nbeen proposed which eliminate the need for dense\nmanually-annotated segmentation masks; current models\nseparately handle either semantic segmentation (e.g.,\nSTEGO) or class-agnostic instance segmentation (e.g.,\nCutLER), but not both (i.e., panoptic segmentation). We\npropose an Unsupervised Universal Segmentation model\n(U2Seg) adept at performing various image segmentation\ntasks\u2014instance, semantic and panoptic\u2014using a novel uni-\nfied framework. U2Seg generates pseudo semantic labels\nfor these segmentation tasks via leveraging self-supervised\nmodels followed by clustering; each cluster represents dif-\nferent semantic and/or instance membership of pixels. We\nthen self-train the model on these pseudo semantic labels,\nyielding substantial performance gains over specialized\n*Equal Contribution.\n\u2020Project Lead.\nmethods tailored to each task: a +2.6 APbox boost (vs.\nCutLER) in unsupervised instance segmentation on COCO\nand a +7.0 PixelAcc increase (vs. STEGO) in unsupervised\nsemantic segmentation on COCOStuff.\nMoreover, our\nmethod sets up a new baseline for unsupervised panoptic\nsegmentation, which has not been previously explored.\nU2Seg is also a strong pretrained model for few-shot\nsegmentation, surpassing CutLER by +5.0 APmask when\ntrained on a low-data regime, e.g., only 1% COCO labels.\nWe hope our simple yet effective method can inspire more\nresearch on unsupervised universal image segmentation.\n1. Introduction\nThe field of image segmentation has witnessed significant\nadvancements in the recent years [4, 5, 12, 20, 26, 35,\n38, 40]. Nonetheless, the effectiveness of these segmen-\ntation methods heavily depends on the availability of exten-\nsive densely human-labeled data for training these models,\narXiv:2312.17243v1  [cs.CV]  28 Dec 2023\nwhich is both labor-intensive and costly and thus less scal-\nable. In this paper, our objective is to explore the extent\nto which unsupervised image segmentation can be achieved\nwithout relying on any human-generated labels.\nSeveral recent works such as CutLER [60] and\nSTEGO [24] have emerged as promising approaches for\nunsupervised image segmentation. CutLER leverages the\nproperty of the self-supervised model DINO [8] to \u2018dis-\ncover\u2019 objects without supervision, and learns a state-of-\nthe-art localization model on pseudo instance segmenta-\ntion masks produced by MaskCut [60] (based on Normalize\nCuts [45]). Similarly leveraging DINO [8], STEGO [24]\nintroduces a novel framework that distills unsupervised fea-\ntures into discrete semantic labels. This is achieved using a\ncontrastive loss that encourages pixel features to form com-\npact clusters while preserving their relationships across the\ncorpora [24]. However, these methods have limitations:\n\u2022 The output of unsupervised instance segmentation meth-\nods such as CutLER [60] comprises class-agnostic seg-\nments for \u201cthings\u201d, ignoring the \u201cstuff\u201d categories that\nrepresent pixel semantics. Moreover, CutLER often treats\nseveral overlapping instances as one instance, especially\nwhen these instances belong to the same semantic class.\n\u2022 On the other hand, unsupervised semantic segmentation\nmethods such as STEGO [24] focus on the segmentation\nof semantically coherent regions, lacking the capability to\ndistinguish between individual instances.\n\u2022 Unsupervised panoptic segmentation has not been ad-\ndressed. Supervised panoptic segmentation methods [12,\n29, 31] predict both \u201cstuff\u201d and \u201cthings\u201d classes simulta-\nneously; to the best of our knowledge there has not been\nwork on unsupervised panoptic segmentation heretofore.\nTo address these limitations, we propose U2Seg, a\nnovel Unsupervised Universal image Segmentation model.\nU2Seg offers comprehensive scene understanding\u2013instance,\nsemantic and panoptic\u2013without relying on human annota-\ntions, segmenting semantically meaningful regions in the\nimage as well as identifying and differentiating between in-\ndividual instances within those regions.\nU2Seg is comprised of three steps. First, we create high-\nquality, discrete semantic labels for instance masks obtained\nfrom MaskCut and DINO, by clustering semantically simi-\nlar instance masks into distinct fine-grained clusters, as de-\nscribed in Sec. 3.2. Next, we amalgamate the semantically\npseudo-labeled \u201cthings\u201d pixels (from the first step) with\n\u201cstuff\u201d pixels (from STEGO) to produce pseudo semantic\nlabels for each pixel in the image. Lastly, a universal image\nsegmentation model is trained using these pseudo-labels,\nresulting in a model capable of simultaneously predicting\npixel-level (i.e., semantic segmentation and class-agnostic\ninstance segmentation) and instance-level semantic labels,\ndetailed in Sec. 3.3.\nDespite the inherent noise in these pseudo-labels, self-\ntraining the model with them yields substantial performance\ngains over specialized methods tailored to each task: U2Seg\nachieves a +2.6 APbox boost (vs.\nCutLER) in unsuper-\nvised instance segmentation on COCO and a +7.0 PixelAcc\nincrease (vs. STEGO) in unsupervised semantic segmen-\ntation on COCOStuff.\nMoreover, our method sets up a\nnew baseline for unsupervised panoptic segmentation. We\nalso find that the multi-task learning framework and learn-\ning unsupervised segmentor with semantic labels enable\nour model to generate a more discriminative feature space,\nwhich makes it a superior representation for downstream\nsupervised detection and segmentation tasks. When trained\non a low-data regime, such as 1% COCO labels, U2Seg sur-\npasses CutLER by +5.0 APmask.\nContributions. Our main contribution is the first universal\nunsupervised image segmentation model that can tackle un-\nsupervised semantic-aware instance, semantic and panoptic\nsegmentation tasks using a unified framework. We establish\na suite of benchmarks on unsupervised semantic-aware in-\nstance segmentation and panoptic segmentation, areas pre-\nviously unexplored. Despite using a single framework, we\ndemonstrate that U2Seg surpasses previous methods spe-\ncialized for each task across all experimented benchmarks\n(instance, semantic, panoptic, etc.) and datasets (COCO,\nCityscapes, UVO, VOC, etc.).\n2. Related Work\nSelf-supervised Representation Learning focuses on fea-\nture learning from a large amount of unlabeled data with-\nout using human-made labels. Contrastive Learning-Based\nMethods [9, 27, 43, 62] learn representation by comparing\nsimilar instances or different versions of a single instance\nwhile separating dissimilar ones.\nSimilarity-Based Self-\nSupervised Learning [10, 23] mainly reduces differences\nbetween different augmented versions of the same instance.\nClustering-Based Feature Learning [2, 7, 55, 63, 65] finds\nnatural data groups in the hidden space. Masked Autoen-\ncoders [3, 18, 28] learn by masking and then reconstructing\nmasked parts of the image.\nUnsupervised Object Detection and Instance Segmenta-\ntion. DINO [8] shows that self-supervised learning (SSL)\nVision Transformers (ViT) [19] can reveal hidden seman-\ntic segmentation in images, which is not obvious in su-\npervised counterparts [8, 66]. Extending this, LOST [46]\nand TokenCut [61] use DINO\u2019s patch features to identify\nmain objects in images. FreeSOLO [58] performs unsu-\npervised class-agnostic instance segmentation by creating\ncoarse masks first, which are later improved through self-\ntraining. MaskDistill [50] uses a self-supervised DINO to\nget initial masks from an affinity graph but only allows one\nmask per image during distillation, limiting multi-object de-\ntection. Meanwhile, CutLER [59] introduces the MaskCut\nmethod, which aims to identify multiple instances in a sin-\ngle image. Yet, MaskCut frequently consolidates overlap-\nping instances into a single segment and lacks the capability\nto assign semantic labels to each instance.\nUnsupervised Semantic Segmentation.\nIIC [30] maxi-\nmizes mutual information for clustering, while PiCIE [14]\nuses invariance to photometric effects and equivariance to\ngeometric transformations for segmentation.\nMaskCon-\ntrast [49] learns unsupervised semantic segmentation by\ncontrasting features within saliency masks. STEGO [24]\nrefines pretrained SSL visual features to distill correspon-\ndence information embedded within these features, thereby\nfostering discrete semantic clusters.\nUniversal Segmentation has been introduced to deliver in-\nstance, semantic and panoptic segmentation tasks using a\nunified architecture [6, 11\u201313, 29, 33, 34, 37, 52, 64]. In\nthis work, we propose U2Seg to tackle this challenging task\nwithout relying on human-annotated data.\nUnsupervised Image Classification methods mainly fo-\ncus on providing a semantic label for each query image\nthat can be mapped to ground truth classes by hungarian\nmatching. SCAN [48] proposes a three-stage pipeline that\nincludes representation learning, deep clustering, and self-\nlabeling. NNM [16] enhances SCAN by incorporating lo-\ncal and global nearest neighbor matching. RUC [44] further\nimproves SCAN using a robust loss as training objective.\nHowever, these approaches only provide one classification\nprediction per image, whereas our method provides classifi-\ncation per-instance for instance segmentation and per-pixel\nfor semantic segmentation.\n3. Unsupervised Universal Segmentation\n3.1. Preliminaries\nWe first explain the previous Unsupervised Instance Seg-\nmentation method CutLER [60], and Unsupervised Seman-\ntic Segmentation method STEGO [24].\nCutLER [60] exploits self-supervised learning models like\nDINO [8] to \u2018discover\u2019 objects and train a state-of-the-art\ndetection and segmentation model using a cut-and-learn\npipeline. It first uses MaskCut to extract multiple initial\nmasks from DINO [8] features. MaskCut first generates\na patch-wise affinity matrix Wij =\nKiKj\n\u2225Ki\u22252\u2225Kj\u22252 using the\n\u201ckey\u201d features Ki for patch i from DINO\u2019s last attention\nlayer. Subsequently, the cut-based clustering method Nor-\nmalized Cut [45] is employed on the affinity matrix by find-\ning the eigenvector x that corresponds to the second small-\nest eigenvalue. A foreground instance mask M s is derived\nthrough bi-partitioning of the vector x, enabling segmenta-\ntion of individual objects in the image. For multi-instance\nsegmentation, MaskCut iteratively refines the affinity ma-\ntrix by masking out already segmented objects, allowing for\nsubsequent extractions\nW t\nij = (Ki\nQt\ns=1 M s\nij)(Kj\nQt\ns=1 M s\nij)\n\u2225Ki\u22252\u2225Kj\u22252\n(1)\nand repeating above steps by N times. CutLER then refines\ndetection and segmentation through a loss-dropping strat-\negy and iterative self-training.\nSTEGO [24] harnesses the semantically rich feature corre-\nlations produced by unsupervised methods like DINO [8]\nfor segmentation.\nIt trains a segmentation head to re-\nfine these correlations within an image, with its K-Nearest\nNeighbors (KNNs), and across randomly chosen images.\nSpecifically, STEGO distills DINO\u2019s unsupervised features\ninto distinct semantic labels by optimizing a correspon-\ndence loss. This loss function measures the feature corre-\nspondences F SC between image feature pairs generated by\nDINO and the feature correspondence Shwij derived from\na trainable, lightweight segmentation head [24]:\nLcorr(x, y, b) = \u2212\nX\nhwij\n(F SC\nhwij \u2212 b) max(Shwij, 0)\n(2)\n3.2. Unsupervised Instance Segmentation\nAlthough CutLER [60] provides high-quality instance seg-\nmentation masks without human annotations, the predicted\nmasks are class-agnostic, and thus do not include semantic\nlabels for each instance. Our method addresses this issue by\ngrouping the detected instances with a clustering method. In\nthis way, instances assigned to the same cluster are associ-\nated with identical or closely related semantic information,\nwhile instances residing in separate clusters exhibit seman-\ntic dissimilarity.\nPseudo Semantic Labels.\nTo train a detection and in-\nstance segmentation model, we vector quantize the model\ntargets (pseudo semantic labels) by clustering the instance-\nlevel features of the entire dataset, under constraints derived\nfrom self-supervision. Specifically, our approach starts with\nthe generation of instance segmentation masks using Mask-\nCut [60]. Subsequently, we utilize the efficient K-Means\nclustering method as implemented in USL [57] to cluster all\nsegmentation masks into semantically meaningful clusters.\nWe employ K-Means clustering to partition n instances\ninto C(\u2264 n) clusters, where each cluster is represented by\nits centroid c [22, 41]. Each instance is assigned to the clus-\nter with the nearest centroid. Formally, we conduct a C-\nway node partitioning, denoted as S = S1, S2, . . . , SC, that\nminimizes the within-cluster sum of squares [36]:\nmin\nS\nC\nX\ni=1\nX\nV \u2208Si\n|V \u2212 ci|2 = min\nS\nC\nX\ni=1\n|Si|Var(Si)\n(3)\nThis optimization process is carried out iteratively using\nthe EM algorithm [42], starting from selecting random sam-\nples as initial centroids. As a result, this process assigns\nInstance  \nMask Generation\nUnlabeled \n Images\nSemantic \nMask Generation\nUniversal \nSegmentation\nSelf-\ntraining\nThings\nStuff\nPseudo \nMasks\nUniversal \nSegmentation\nImages\nModel \nPrediction\npseudo-mask generation\nself-supervised model learning\nmodel inference\nFigure 2. Overview of the training and inference pipeline for the proposed Unsupervised Universal Segmentation model (U2Seg) adept at\nperforming various image segmentation tasks\u2014instance, semantic and panoptic\u2014using a novel unified framework.\nUnlabeled \n Images\nMaskCut\npseudo masks\nmasks w/ semantic labels\nClustering\n\u2026\n\u2026\n\u2026\n1\n2\nN\n\u2026\nFigure 3. Pipeline overview for generating masks and their semantically meaningful pseudo labels in semantic-aware instance segmenta-\ntion. We first use MaskCut to generate class-agnostic instance masks, which are then grouped into semantically meaningful clusters. These\npseudo semantic labels are used for training a semantic-aware instance segmentor.\npseudo semantic labels, denoted as yi, to each instance i,\nwith yi falling within the range of [1, C].\nThe resulting semantic labels serve multiple purposes: 1)\nSemantic-aware copy-paste augmentation, which signifi-\ncantly improves CutLER\u2019s capability to differentiate over-\nlapping instances, especially when they share similar se-\nmantic information.\n2) Training instance segmentation\nmodels: They serve as pseudo ground-truth labels for train-\ning a non-agnostic instance segmentor.\nSemantic-aware Copy-Paste Augmentation. In cluttered\nnatural scenes, previous unsupervised instance segmenta-\ntion model often fail to distinguish instances from the same\nsemantic class. This results in multiple instances being cap-\ntured in the same mask. To distinguish multiple overlapping\nobjects and small objects in existing unsupervised detec-\ntors, we employ semantic-aware copy-paste augmentation,\nwhich includes several steps:\n1) We begin by randomly selecting two instances, de-\nnoted as I1 and I2, both belonging to the same pseudo-\ncategory (or group/cluster). 2) One of these instances un-\ndergoes a transformation function T , which randomly re-\nsizes and shifts the associated pseudo-masks. 3) The re-\nsized instance is then pasted onto another image, creating\nsynthetic overlapping cases using the following equation:\nI3 = I1 \u00b7 (1 \u2212 T (Mc)) + I2 \u00b7 T (Mc)\n(4)\nwhere \u00b7 denotes element-wise multiplication.\nLearning Unsupervised Instance Segmentor. Tradition-\nally, unsupervised segmentation community focused pri-\nmarily on class-agnostic instance segmentation [58, 60, 61],\nwhose outputs lack class labels. However, by incorporating\nclustering information obtained from pseudo-labels on Im-\nageNet, as discussed above, our method allows the model to\npredict not only the location and segmentation of an object\nbut also its pseudo semantic labels.\nAs observed by [60], \u201cground-truth\u201d masks may miss\ninstances.\nHowever, a standard detection loss penalizes\npredicted regions ri that do not overlap with the \u201cground-\ntruth\u201d.\nTherefore, following [60], we drop the loss for\neach predicted region ri that has a maximum overlap of\n\u03c4 IoU with any of the \u2018ground-truth\u2019 instances: Ldrop(ri) =\n1(IoUmax\ni\n> \u03c4 IoU)Lvanilla(ri), where IoUmax\ni\ndenotes the\nmaximum IoU with all \u2018ground-truth\u2019 for ri and Lvanilla is\nthe vanilla loss function of detectors. Ldrop encourages the\nexploration of image regions missed in the \u201cground-truth\u201d.\n3.3. Unsupervised Universal Image Segmentation\nPseudo Labels for Panoptic Segmentation.\nFor each\npixel (i, j) in the image, we vector quantize pixels with\ndifferent semantics or instance membership, generating\npseudo semantic labels for panoptic segmentation. We as-\nsign each pixel a semantic label based on \u201cstuff\u201d or \u201cthings\u201d\nidentity.\nThis results in an instance label (I(i, j)) for\n\u201cthings\u201d or a semantic label (S(i, j)) for \u201cstuff\u201d. The criti-\ncal challenge in this process is distinguishing between pix-\nels associated with \u201cthings\u201d (countable, often foreground)\nand \u201dstuff\u201d (uncountable, usually background) [1].\nTo resolve this problem, our method unfolds in three\nsteps: 1) Semantic Labeling for \u201cThings\u201d: Utilizing the\nclass-agnostic instance segmentation capabilities of Cut-\nLER [60], we first identify \u201cthings\u201d within an image, gener-\nating class-agnostic instance masks. These masks then un-\ndergo deep clustering to attribute a semantic label IC(i, j)\nto each instance, detailed in Sec. 3.2. 2) Semantic Labeling\nTask \u2192\nAgn Instance Seg.\nInstance Seg.\nSemantic Seg.\nPanoptic Seg.\nDatasets \u2192\nCOCO\nCOCO\nVOC\nUVO\nCOCO\nCOCO\nCityscapes\nMetric \u2192\nAPbox\nAPbox\n50\nAPbox\n50 ARbox\n100 APbox\n50 ARbox\n100 APbox\n50 ARbox\n100 PixelAcc mIoU PQ\nSQ\nRQ\nPQ\nSQ\nRQ\nFreeSOLO [58]\n9.6\n4.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nTokenCut [61]\n5.8\n3.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nCutLER [59]\n21.9\n12.3\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nDINO [8]\n-\n-\n-\n-\n-\n-\n-\n-\n30.5\n9.6\n-\n-\n-\n-\n-\n-\nPiCIE + H [14]\n-\n-\n-\n-\n-\n-\n-\n-\n48.1\n13.8\n-\n-\n-\n-\n-\n-\nSTEGO [24]\n-\n-\n-\n-\n-\n-\n-\n-\n56.9\n28.2\n-\n-\n-\n-\n-\n-\nCutLER+\n-\n-\n9.0\n10.3\n26.8\n27.2\n10.6\n11.8\n-\n-\n-\n-\n-\n-\n-\n-\nCutLER+STEGO\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n12.4 64.9 15.5 12.4 36.1 15.2\nU2Seg\n22.8\n13.0\n11.8\n21.5\n31.0\n48.1\n10.8\n25.0\n63.9\n30.2 16.1 71.1 19.9 17.6 52.7 21.7\nvs.prev.SOTA\n+0.9\n+0.7\n+2.8\n+11.2\n+4.2\n+20.9\n+0.2\n+13.2\n+7.0\n+2.0 +3.7 +6.2 +4.4 +5.2 +16.6 +6.5\nTable 1.\nWith a unified framework, U2Seg outperforms previous state-of-the-art methods tailored for individual tasks across vari-\nous datasets, including CutLER for unsupervised instance segmentation, STEGO for unsupervised semantic segmentation, and Cut-\nLER+STEGO for unsupervised panoptic segmentation. \u201cAgn Instance Seg\u201d denotes class-agnostic instance segmentation.\nfor \u201cStuff\u201d: For \u201cstuff\u201d pixels, we deploy the unsupervised\nsemantic segmentation model STEGO [24], which distills\nDINO\u2019s unsupervised features into discrete semantic labels,\nas outlined in 3.1. This step assigns a \u201cstuff\u201d semantic label\nto all pixels, including those of \u201cThings\u201d identified earlier.\n3) Integrating Labels for \u201cThings\u201d and \u201cStuff\u201d. We deter-\nmine a pixel\u2019s classification as \u201cthings\u201d or \u201cstuff\u201d using the\nfollowing logic:\nI(i, j) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nIC(i, j),\nif IC(i, j)\u0338=0\nSS(i, j),\nif IC(i, j)=0 & SS(i, j)\u0338=0\n0,\notherwise\n(5)\nThis process merges the semantic labels, assigning priority\nto \u201cthings\u201d labels over \u201cstuff\u201d where applicable. We then\ntrain a universal segmentation model on these pseudo-labels\nfor instance, semantic and panoptic segmentation tasks.\nLearning Unsupervised Universal Image Segmentor. Af-\nter we obtain the pseudo labels for panoptic segmentation,\nfollowing [33], we construct an unsupervised universal im-\nage segmentation model, that has two branches: instance\nsegmentation branch and semantic segmentation branch, to\naddress corresponding segmentation tasks. The model is\ntrained jointly for both branches, employing the following\nloss function: L = \u03bbi(Lc + Lb + Lm) + \u03bbsLs, where Lc\nrepresents the classification loss, Lb is the detection loss,\nLm is the segmentation loss, and Ls signifies the semantic\nloss. The Ls is computed as a per-pixel cross-entropy loss\nbetween the predicted and ground-truth labels. The hyper-\nparameters \u03bbi and \u03bbs balance these two parts.\n4. Experiments and Results\n4.1. Experimental Setup\nTraining Data.\nOur model is trained on 1.3M unla-\nbeled images from ImageNet [17] and is evaluated di-\nrectly across various benchmarks, unless otherwise noted.\nFor unsupervised semantic segmentation comparisons with\nSTEGO [24], we additionally fine-tune our model using\nMSCOCO\u2019s unlabeled images, following STEGO [24].\nTest Data.\nFor unsupervised instance segmentation, we\ntest our model on COCO val2017, PASCAL VOC\nval2012 [21] and UVO val [53].\nFor unsupervised\npanoptic segmentation, we evaluate our model on COCO\nval2017 and Cityscapes val [15].\nEvaluation Metrics. We use AP, AP50, AP75 and AR100\nto evaluate the unsupervised instance segmentation; Pix-\nelAcc and mIoU for unsupervised semantic segmentation;\nPQ, RQ, SQ for unsupervised universal image segmenta-\ntion. After predicting the instance with its semantic labels,\nwe use Hungarian matching to map the semantic labels to\nclass names in the real dataset (details in B). It evaluates\nthe consistency of the predicted semantic segments with the\nground truth labels, remaining unaffected by any permuta-\ntions in the predicted class labels.\nImplementation Details.\nFollowing [32], we employ\nPanoptic Cascade Mask R-CNN [4, 32] with a ResNet50\nbackbone [25]. Following CutLER\u2019s training recipe [60],\nour model, initialized with DINO pre-trained weights, is\ntrained on unlabeled ImageNet for two epochs.\nIt starts\nwith an initial learning rate of 0.01, which then decreases\nafter the first epoch to 5 \u00d7 10\u22125, with a batch size of 16\nfor all models. For unsupervised panoptic segmentation,\nwe maintain the same training schedule as unsupervised in-\nstance segmentation for zero-shot evaluation. In non-zero-\nshot scenarios, the models undergo training on a combina-\ntion of unlabeled COCO and ImageNet datasets, beginning\nwith a learning rate of 0.01 over 90k steps.\n4.2. Unsupervised Universal Image Segmentation\nTo the best of our knowledge, U2Seg represents the first\neffort in addressing unsupervised semantic-aware instance,\nsemantic and panoptic segmentation, all unified under a sin-\ngle framework. Due to the absence of benchmarks for unsu-\npervised semantic-aware instance segmentation and panop-\n52 87%\n27\n104 95%\n22 75%\n25 96%\n52 57%\n20\n104 83%\n52 97%\n2\n20\n132 98%\n23\n25\n132 100%\n3\n4 23\n4\n132 97%\n23\n21\n3\n18\n139 99%\n139 87%\n139 90%\n104 91%\n20\n27\n27\n2\n23\n28\n2\nFigure 4. Universal image segmentation visualization in COCO val2017. We show the results with predicted cluster ID directly from\nthe model, with athletes playing hockey (left columns) as \u201c139\u201d, playing badminton (middle columns) as \u201c52\u201d and the gentlemen (right\ncolumns) as \u201c132\u201d. After Hungarian matching, these IDs are automatically matched to the category \u201cperson\u201d for subsequent quantitative\nevaluation.\nMetric\nAPbox APbox\n50 APbox\n75 ARbox\n100 APmask APmask\n50\nAPmask\n75\nARmask\n100\nCutLER+ 5.9\n9.0\n6.1\n10.3\n5.3\n8.6\n5.5\n9.3\nU2Seg\n7.3\n11.8\n7.5\n21.5\n6.4\n11.2\n6.4\n18.5\n\u2206\n+1.4 +2.8 +1.4 +11.2\n+1.1\n+2.6\n+0.9\n+9.2\nTable 2. The results for zero-shot unsupervised object detection\nand instance segmentation on COCO val2017. The model is\ntrained on ImageNet with a cluster number of 800. We compare it\nwith CutLER+, a combination of CutLER and offline clustering.\ntic segmentation, we establish comprehensive benchmarks\nand baselines for both tasks.\nIn Tab. 1, we demonstrate that U2Seg, utilizing a uni-\nfied framework, significantly outperforms all previous ap-\nproaches across various benchmarks and datasets.\nFor\nclass-agnostic unsupervised instance segmentation, our\nmethod achieves an increase of +0.9 in APbox compared\nto CutLER [60]. This improvement is largely attributed to\nour novel semantic-aware copy-paste augmentation, as de-\ntailed in Sec. 3.2. For unsupervised semantic-aware in-\nstance segmentation, we benchmark against the advanced\nbaseline CutLER+, derived from CutLER, and record a sub-\nstantial gain of over 11.2% in AR. A more comprehen-\nsive analysis of these results is provided in Sec. 4.3. For\nunsupervised semantic segmentation, our approach sur-\npasses the state-of-the-art STEGO with impressive margins\nof +7.0 in PixelAcc and +2.0 in mIoU. Lastly, for unsu-\npervised panoptic segmentation, we compare against the\nstrong baseline of CutLER+STEGO, a hybrid of CutLER+\nand STEGO, and observe performance gains of over 6.2%\nin SQ on MSCOCO and a notable 16.6% improvement in\nSQ on Cityscapes. Further comparisons and discussions on\nthis task are elaborated in Sec. 4.4.\n4.3. Unsupervised Instance Segmentation\nWe performed extensive experiments for zero-shot unsu-\npervised instance segmentation.\nGiven that prior meth-\nods [51, 58, 60, 61] are limited to class-agnostic instance\nsegmentation, we developed CutLER+, a strong baseline for\nunsupervised semantic-aware instance segmentation, build-\ning upon the current state-of-the-art CutLER [60].\nCut-\nLER+ operates in two steps: it first uses the pre-trained\nMethods\nAPbox\nAPbox\n50\nAPbox\n75\nARbox\n100\nCutLER+\n17.1\n26.8\n18.1\n27.2\nU2Seg\n19.0\n31.0\n19.5\n48.1\n\u2206\n+1.9\n+4.2\n+1.4\n+20.9\nTable 3. The results for zero-shot unsupervised object detection\non PASCAL VOC val2012. The model is trained on ImageNet\nwith a cluster number of 800. We compare it with CutLER+, a\ncombination of CutLER and offline clustering.\nMetric\nAPbox\nAPbox\n50\nARbox\n100\nAPmask\nAPmask\n50\nARmask\n100\nCutLER+\n6.3\n10.6\n11.8\n6.0\n9.0\n10.4\nU2Seg\n6.8\n10.8\n25.0\n6.2\n9.5\n21.0\n\u2206\n+0.5\n+0.2\n+13.2\n+0.2\n+0.5\n+10.6\nTable 4. The results for zero-shot unsupervised object detection\nand instance segmentation on UVO val. The model is trained\non ImageNet with a cluster number of 800. We compare with\nCutLER+, a combination of CutLER and offline clustering.\nRaw Image\nU2Seg\nGround-Truth Pseudo-Labels\nFigure 5. Visualizations of U2Seg\u2019s unsupervised Panoptic seg-\nmentation results on COCO val2017 (after Hungarian match-\ning). The pseudo label is the naive combination of previous state-\nof-the-art instance segmentation, i.e. CutLER [60], and semantic\nsegmentation, i.e., STEGO [24], results.\nCutLER to generate class-agnostic instance masks, and sub-\nsequently assigns semantic labels to all instance masks\nthrough offline clustering.\nTab. 2 demonstrates that U2Seg markedly improves\nMethods\nPretrain\nPQ\nSQ\nRQ\nzero-shot methods\nU2Seg\nIN\n15.7\n46.6\n19.8\nnon zero-shot methods\nCutLER+STEGO\nCOCO\n12.4\n36.1\n15.2\nU2Seg\nCOCO\n15.4\n51.5\n19.0\nU2Seg\nCOCO+IN\n17.6\n52.7\n21.7\n\u2206\n+5.2\n+16.6\n+6.5\nTable 5.\nUnsupervised Panoptic image segmentation on\nCityscapes val. We show PQ, SQ and RQ on zero-shot and non-\nzero shot settings with the cluster number of 800. We compare\nwith CutLER+STEGO, a combination of CutLER+ and STEGO.\nMethods\nPretrain\nPQ\nSQ\nRQ\nzero-shot methods\nU2Seg\nIN\n11.1\n60.1\n13.7\nnon zero-shot methods\nCutLER+STEGO\nCOCO\n12.4\n64.9\n15.5\nU2Seg\nCOCO\n15.3\n66.5\n19.1\nU2Seg\nCOCO+IN\n16.1\n71.1\n19.9\n\u2206\n+3.7\n+6.2\n+4.4\nTable 6. Unsupervised Panoptic image segmentation on COCO\nval2017. We show PQ, SQ and RQ on zero-shot and non-zero\nshot settings. We use CutLER+STEGO, a combination of Cut-\nLER+ and STEGO, as a strong baseline.\nperformance in both unsupervised object detection and\ninstance segmentation on MSCOCO, delivering a +2.8\nboost in APbox\n50 and a +2.6 rise in APmask\n50\nover CutLER+.\nAdditionally, our method sees a substantial increase of\napproximately +10.0 in AR100.\nResults on PASCAL\nVOC val2012 and UVO val are detailed in Tab. 3 and\nTab. 4, respectively. Notably, we achieve gains exceeding\n+20% in AR for PASCAL VOC and +10% for UVO.\n4.4. Unsupervised Panoptic Segmentation\nFor unsupervised panoptic/universal image segmentation,\nour experiments span two scenarios. In the zero-shot set-\nting, the model is trained exclusively on unlabeled Ima-\ngeNet images. For non-zero-shot (in-domain) scenarios, we\ntrain on unlabeled COCO images or a mix of COCO and\nImageNet. With no existing benchmarks for unsupervised\npanoptic segmentation, we establish a new baseline by inte-\ngrating the state-of-the-art unsupervised semantic segmen-\ntation from STEGO [24] with semantic-aware instance seg-\nmentation from CutLER+ (discussed in Sec. 4.3), which are\nthen merged to create panoptic/universal segmentation out-\ncomes, referred to as CutLER+STEGO.\nTab. 6 presents the PQ, SQ, and RQ scores of U2Seg\non COCO val2017. U2Seg surpasses the strong baseline\nCutLER+STEGO with a +3.5 improvement in PQ and an\nincrease of over +4.0 in RQ. Qualitative results of U2Seg\u2019s\nperformance is provided in Fig. 4, with the predicted seman-\ntic labels visualized. The qualitative results suggest that an\nRaw Image\nU2Seg\nGround-Truth Pseudo-Labels\nOriginal Images\nPrediction\nGround Truth\nPseudo Label\nperson 65%\nperson 53%\nvegetable\nsky\nbuilding\nperson 51%\nperson 64%\nperson 56%\nroad\nwall\nbuilding\ncar 89%\nbuilding\ntrafic sign 43%\nroad\nsky\nvegetable\nbus 98%\nbuilding\nbuilding\ncar 98%\ncar 98%\nroad\nsky\nsky\nvegetable\nvegetable\nperson 43%\nwall\nroad\nvegetable\nsky\nperson 43%\ncar 98%\nroad\ncar 94%\ncar 90%\ncar 70% vegetable\nbuilding\nsky\ntrafic sign 75%\nsky\nvegetable\nvegetable\nbuilding\ntrafic sign 38%\ntrafic sign 54%\ncar 90%\nbuilding\nFigure 6. Qualitative results of U2Seg\u2019s Panoptic image segmen-\ntation results on Cityscapes val (after Hungarian matching).\nover-clustering strategy in pseudo-label generation, e.g. set-\nting the number of clusters to 300 or 800, leads to highly\ngranular model predictions. For instance, as in Fig. 4, the\nmodel distinctly categorizes hockey players as \u201c139\u201d, bad-\nminton players as \u201c52\u201d, and gentlemen in suits as \u201c132\u201d,\nshowcasing its potent discriminative capabilities.\nTo quantitatively measure the quality of segmentation\nmasks and their corresponding semantic labels, we use\nHungarian matching (detailed in Appendix B) to align se-\nmantic labels with the category names from the test dataset;\nfor instance, all three sub-clusters depicted in Fig. 4 are as-\nsigned to the \u201dperson\u201d category. The qualitative outcomes\npost-Hungarian matching are shown in Fig. 5, where our\nmodel demonstrates superior panoptic segmentation mask\nquality. For instance, while the baseline tends to segment\nparts separately (as seen with the man\u2019s head and torso be-\ning treated as separate entities in the third row), our model\ncorrectly identifies them as parts of a single object. This\nlevel of recognition is also evident with the \u201ctrunks of the\nmotorcycle\u201d example in the second row. For additional re-\nsults, please see Appendix D. We also present results of the\nmore challenging dataset Cityscapes in Tab. 5 and Fig. 6.\n4.5. Efficient Learning\nSpecifically, for object detection and instance segmentation,\nwe employ our unsupervised instance segmentation model,\nwith cluster count set to 300, to initialize the model weights.\nWe adopt the recipe from [56, 59] for model fine-tuning\nacross various annotation splits. For label-efficient panoptic\nsegmentation, we fine-tune the model initialized with our\nzero-shot unsupervised framework on the same data splits.\nThe results are depicted in Fig. 7, where our model\u2019s in-\nstance segmentation performance is benchmarked against\nMoCo-V2, DETReg, and CutLER. Our model consistently\n Instance Segmentation: AP (%)\n9\n12\n15\n18\n21\n24\n27\n30\n33\n36\n1\n2\n5\n10\n20\n30\n40\n50\n25.6\n22\nObject Detection: AP (%)\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n42\n1\n2\n5\n10\n20\n30\n40\n50\n29.1\n24.8\n% of Labeled Data\n% of Labeled Data\nCutLER\nMoCo-v2\nFreeSOLO\nCutLER\nMoCo-v2\nDETReg\n4.5%\n4.6%\nOurs\nOurs\n Panoptic Segmentation: PQ (%)\n15\n18\n21\n24\n27\n30\n33\n36\n39\n42\n1\n2\n5\n10\n20\n30\n40\n50\n% of Labeled Data\nCutLER\nOurs\nMoCo-v2\n3.2%\n Panoptic Segmentation: SQ (%)\n60\n63\n66\n69\n72\n75\n78\n1\n2\n5\n10\n20\n30\n40\n50\n% of Labeled Data\nMoCo-v2\nCutLER\nOurs\n1.8%\n Panoptic Segmentation: RQ (%)\n18\n22\n26\n30\n34\n38\n42\n46\n50\n1\n2\n5\n10\n20\n30\n40\n50\n% of Labeled Data\nMoCo-v2\nCutLER\nOurs\n3.8%\nFigure 7. We evaluate the label-efficient learning performance on 3 different tasks: object detection (the left), instance segmentation (the\nsecond left) and panoptic image segmentation (the last three).\nFigure 8. U2Seg learns features that are more discriminative than\nthose learned by CutLER. The t-SNE [47] visualization of the fea-\ntures from the model\u2019s FC layer. We color-code each dot based on\nits ground-truth category.\nsurpasses the state-of-the-art with consistent gains in both\nAPbox and APmask. In scenarios with panoptic image seg-\nmentation as the downstream task, we contrast our results\nwith MoCo-V2 and CutLER in terms of PQ, SQ, and RQ\nmetrics. The results illustrate a remarkable improvement,\neffectively doubling the performance boost from MoCo-V2\nto CutLER, especially in few-shot contexts with limited an-\nnotations (1% or 2% labeled samples). This highlights the\npractical value of our approach in real-world unsupervised\nlearning applications, where annotations are often scarce.\nWe attribute the performance gains primarily to the dis-\ncriminative features our model learns, as in Fig. 8, obtaining\neffective model initialization for few-shot learning.\n4.6. Ablation Studies\nIn this section, we conduct ablation study on U2Seg.\nNumbers of clusters. The choice of cluster quantity sig-\nnificantly affects the model\u2019s representation granularity.\nOur ablation study on various cluster counts, as detailed\nin Tab. 7, reveals their impact on model performance. Over-\nclustering generally leads to a finer level of detail, prompt-\ning the model to learn more discriminative features.\nHungarian matching. As our trained model could pre-\n# Cluster\nCOCO\nUVO\nVOC\nAPbox\n50\nARbox\n100\nAPbox\n50\nARbox\n100\nAPbox\n50\nARbox\n100\n300\n9.3\n20.1\n9.8\n22.6\n29.6\n45.7\n800\n11.8\n21.5\n10.8\n25.0\n31.0\n48.0\n2911\n13.3\n22.1\n15.1\n25.8\n31.6\n48.3\nTable 7. Over-clustering can improve the model performance. We\nshow results on different datasets for the unsupervised object de-\ntection using different cluster numbers.\nconf # matched APbox\n50 ARbox\n100\n0.9\n109\n10.9\n13.1\n0.7\n225\n11.6\n18.0\n0.6\n282\n11.8\n19.7\n0.4\n389\n11.8\n21.5\n0.2\n513\n11.3\n21.8\n0.0\n718\n8.6\n18.4\n(a) Conf\u2019s effect on accuracy.\nIoU # matched APbox\n50 ARbox\n100\n0.9\n295\n10.8\n19.7\n0.8\n348\n11.4\n20.7\n0.4\n414\n11.5\n21.6\n0.2\n450\n11.5\n21.1\n0.0\n494\n9.2\n17.7\n0.6\n389\n11.8\n21.5\n(b) IoU\u2019s effect on accuracy.\nTable 8. Impact of Confidence and IoU on Hungarian Matching\nPerformance: The left table illustrates the outcomes at a fixed IoU\nof 0.6 while varying the confidence scores. Conversely, the right\ntable displays the results with a constant confidence of 0.4, altering\nthe IoU values. The cluster number is 800.\ndict the instance with corresponding semantic labels, we\nare able to go further beyond unsupervised class-agnostic\ninstance segmentation. To quantitatively evaluate the per-\nformance, Hungarain matching is employed to match the\npredicted semantic labels to the ground-truth dataset cate-\ngories. See Appendix B for details of the adopted Hungar-\nian matching used in our evaluation. As shown in Tab. 8,\nthe two parameters conf threshold and IoU threshold also\naffect the precision and recall.\n5. Summary\nWe present U2Seg, a novel Unsupervised Universal Image\nSegmentation model, adept at performing unsupervised in-\nstance, semantic, and panoptic segmentation tasks within\na unified framework. Evaluated on extensive benchmarks,\nU2Seg consistently outperforms previous state-of-the-art\nmethods designed for individual tasks. Additionally, U2Seg\nachieves the new state-of-the-art for label-efficient panoptic\nsegmentation and instance segmentation. We anticipate that\nU2Seg, free from the constraints of human annotations, will\ndemonstrate enhanced performance when scaled up with\nmore training data, representing an important direction for\nour future research.\n6. Acknowledgement\nTrevor Darrell, Dantong Niu and Xudong Wang were\nfunded by DoD including DARPA LwLL and the Berkeley\nAI Research (BAIR) Commons.\nAppendix Materials\nA. Datasets used for Evaluation\nWe provide information about the datasets used in this work\nas shown in Tab. A1\nCOCO. The COCO dataset, introduced by [39], is used for\nobject detection and instance segmentation. It has 115,000\ntraining images, 5,000 validation images, and a separate\nbatch of 123,000 unannotated images. We test our unsu-\npervised instance segmentation on the COCO val2017\nset with zero-shot setting. We report results using standard\nCOCO metrics, including average precision and recall for\ndetection and segmentation. Also, for unsupervised univer-\nsal image segmentation, we test the performance on COCO\nval2017. We report results using panoptic segmentation\nCOCO metrics.\nPASCAL VOC. The PASCAL VOC dataset [21] is a widely-\nused benchmark for object detection. We test our model\nusing the trainval07 split and adopt COCO-style eval-\nuation metrics.\nUVO. The UVO dataset [54] is designed for video object\ndetection and instance segmentation. We test our unsuper-\nvised instance segmentation on the UVO val split, which\nincludes 256 videos with each one annotated at 30 fps. We\nremove the extra 5 non-COCO categories which are marked\nas \u201cother\u201d in their official annotations. For evaluation, we\nemploy COCO-style metrics.\nCityscapes.\nCityscapes is a dataset dedicated to seman-\ntic urban scene understanding, focusing primarily on se-\nmantic segmentation of urban scenes.\nIn our research,\nwe tested our unsupervised universal image segmentation\non the Cityscapes val splits, using COCO-stype panoptic\nevaluation metrics.\nB. Hungarian Matching for Unsupervised Seg-\nmentation Evaluation\nIn unsupervised object detection and instance segmentation,\ncategory IDs are predicted without referencing any prede-\nfined labels. For convenience, we differentiate the predicted\ncategory ID of U2Seg as \u201ccluster ID\u201d while keep the ground\ntruth category ID as \u201ccategory ID\u201d in the following analy-\nsis. To evaluate the segmentation performance, particularly\nconcerning category accuracy, an optimal correspondence\nbetween the cluster ID and the ground truth category ID is\nessential. We leverage a multi-to-one Hungarian matching\nfor evaluation of U2Seg.\nHungarain Matching. Given a set of predicted bounding\nboxes, masks associated with predicted cluster IDs and the\ncorresponding ground truth, the objective is to find the best\nmatch from \u201ccluster ID\u201d to \u201ccategory ID\u201d. To do this, we\nfirst use the predicted confidence score conf as a threshold\nto filter the predicted instance, removing the ones with low\nconfidence. Then, for each predicted instance with its clus-\nter ID, we calculate the IoU of the predicted bounding box\nor mask with all ground truth instances, then select the one\nwhose IoU is bigger than the predefined threshold, regard-\ning it as the ground truth category ID for this cluster ID.\nAfter we get these cluster ID and ground truth category ID\npairs, we form a histogram for each kind of cluster ID based\non its overlap with all kinds of ground truth category ID.\nThe ground truth category ID that appears most frequently\nin this histogram becomes the mapping for this cluster ID.\nThis process may result in multiple predicted cluster IDs be-\ning mapped to the same ground truth category ID, leading\nto a multi-to-one matching scenario.\nIn our experiment, the confidence score threshold conf to\nfilter the predicted instance and the IoU threshold to match\npredicted instance with its ground truth instance are both\nhyperparameters, some ablations can be found in Sec. 4.6.\nEvaluation Implications.\nThe multi-to-one Hungarian\nmatching method provides a systematic and efficient way\nto assess the performance of unsupervised segmentation\nmodels.\nBy mapping predicted cluster ID to their most\nlikely ground truth counterparts, the method ensures that\nthe evaluation reflects the true categorization capability of\nthe model.\nThis, in turn, allows for a fair and consis-\ntent comparison across different unsupervised segmentation\ntechniques.\nC. Unsupervised Instance Segmentation\nIn this section, we provide complete results for the unsuper-\nvised instance segmentation of U2Seg. The results are pre-\nsented over various datasets and classes to furnish a com-\nprehensive evaluation of our model\u2019s capability.\nTab. A2 and Tab. A3 display the results for unsuper-\nvised object detection and instance segmentation on dif-\nferent datasets. One trend can be observed across the dif-\nferent datasets: as the number of the predicted cluster ID\nincreases (e.g., moving from 300 to 2911), there is a con-\nsistent increase for most of the metrics. This trend can be\nsuccinctly attributed to the intrinsic properties of the multi-\nto-one Hungarian matching approach (we also show the pa-\nrameter IoU and Conf used for Hungarian matching). With\nDatasets\nDomain\nTesting Data\n#Images\nInstance Segmentation Label\nCOCO [39]\nnatural images\nval2017 split\n5,000\n\u2713\nUVO [54]\nvideo frames\nval split\n21,235\n\u2713\nPASCAL VOC [21]\nnatural images\ntrainval07 split\n9,963\n\u2717\nCityscapes [15]\nurban scenes\nval split\n500\n\u2713\nTable A1. Summary of datasets used for evaluation.\nDatasets\n# cluster\nIoU\nConf\nAPbox\nAPbox\n50\nAPbox\n75\nAPbox\nS\nAPbox\nM\nAPbox\nL\nARbox\n1\nARbox\n10\nARbox\n100\nUVO\n2911\n0.6\n0.1\n9.7\n15.1\n9.3\n0.6\n5.2\n14.4\n18.0\n25.3\n25.8\n800\n0.4\n0.1\n6.8\n10.8\n7.2\n0.6\n2.9\n10.2\n17.2\n24.5\n25.0\n300\n0.7\n0.1\n6.5\n9.8\n6.5\n0.8\n2.6\n9.2\n16.0\n22.2\n22.6\nVOC\n2911\n0.5\n0.2\n19.2\n31.6\n19.7\n1.0\n6.4\n26.6\n28.6\n44.9\n48.3\n800\n0.8\n0.2\n19.0\n31.0\n19.5\n0.6\n4.8\n26.6\n28.8\n45.2\n48.1\n300\n0.8\n0.4\n18.4\n29.6\n18.8\n0.3\n3.8\n26.0\n27.1\n41.0\n42.8\nCOCO\n2911\n0.5\n0.3\n8.2\n13.3\n8.4\n1.4\n7.0\n18.2\n14.1\n21.4\n22.1\n800\n0.6\n0.4\n7.3\n11.8\n7.5\n1.2\n5.8\n15.8\n13.3\n20.8\n21.5\n300\n0.6\n0.3\n5.7\n9.3\n5.9\n0.5\n4.6\n12.9\n11.9\n19.5\n20.1\nTable A2. Complete results for unsupervised object detection. We show results on UVO val, PASCAL VOC val2012 and COCO\nval2017, with corresponding clustering numbers. The IoU and Conf are the Hungarian matching parameter we use for evaluation.\nDatasets\n# cluster\nIoU\nConf\nAPmask\nAPmask\n50\nAPmask\n75\nAPmask\nS\nAPmask\nM\nAPmask\nL\nARmask\n1\nARmask\n10\nARmask\n100\nUVO\n2911\n0.6\n0.1\n8.8\n13.9\n8.4\n0.5\n6.4\n14.4\n16.0\n21.7\n22.1\n800\n0.4\n0.1\n6.2\n9.5\n6.0\n0.5\n2.1\n9.8\n15.7\n20.6\n21.0\n300\n0.7\n0.1\n6.1\n9.5\n5.8\n0.7\n1.0\n8.8\n14.1\n19.2\n19.4\nCOCO\n2911\n0.5\n0.3\n7.3\n12.4\n7.4\n0.8\n4.9\n17.9\n12.8\n18.7\n19.2\n800\n0.6\n0.4\n6.4\n11.2\n6.4\n0.7\n3.7\n15.0\n11.9\n18.0\n18.5\n300\n0.6\n0.3\n4.9\n8.6\n5.0\n0.3\n2.6\n11.8\n10.7\n16.9\n17.3\nTable A3. Complete results for unsupervised instance segmentation. We show results on UVO val and COCO val2017, with\ncorresponding clustering numbers. The IoU and Conf is the Hungarian matching parameter we use for evaluation.\nDatasets\nPretrain\n# Cluster\nPQ\nPQSt\nPQTh\nSQ\nSQTh\nSQSt\nRQ\nRQTh\nRQSt\nCOCO\nIN\n300\n11.1\n9.5\n19.3\n60.1\n60.3\n59.0\n13.7\n11.6\n25.0\nIN\n800\n11.9\n10.5\n19.6\n65.9\n67.4\n58.2\n14.8\n12.8\n25.3\nCOCO\n300\n15.3\n14.2\n21.6\n66.5\n67.2\n62.4\n19.1\n17.5\n27.5\nCOCO\n800\n15.5\n14.6\n20.5\n69.7\n71.1\n62.6\n19.1\n17.8\n26.1\nIN+COCO\n300\n15.5\n14.4\n21.2\n67.1\n67.7\n64.3\n19.2\n17.8\n26.9\nIN+COCO\n800\n16.1\n15.1\n21.2\n71.1\n72.5\n63.8\n19.9\n18.6\n26.8\nCityscapes\nIN\n300\n15.3\n4.1\n23.4\n48.8\n54.7\n44.6\n19.5\n5.4\n29.7\nIN\n800\n15.7\n4.3\n24.0\n46.6\n47.5\n45.9\n19.8\n5.5\n30.2\nCOCO\n300\n18.4\n7.8\n26.1\n47.4\n47.3\n47.4\n22.6\n9.8\n31.9\nCOCO\n800\n15.4\n5.8\n22.3\n51.5\n62.9\n43.2\n19.0\n7.5\n27.4\nIN+COCO\n300\n16.5\n6.2\n24.1\n44.1\n45.2\n43.3\n20.5\n7.9\n29.7\nIN+COCO\n800\n17.6\n8.4\n24.2\n52.7\n67.5\n42.0\n21.7\n10.5\n29.9\nTable A4. Complete results for unsupervised universal image segmentation. We show results for different models pretrained on various\ndataset and test on COCO val2017, Cityscapes val, with corresponding cluster numbers.\nan increase of the cluster numbers, the Hungarian matching\nhas a broader set of predictions to associate with a single\nlabel. This inherently increases the chances of having at\nleast one correct prediction for the given label, making the\nmatching process more amenable. In essence, larger cluster\nnumbers afford easier matching, thereby boosting the eval-\nuation metrics.\nFurthermore, the qualitative results are shown in Fig. A1,\nwith the samples selected in COCO val2017 and PAS-\nCAL VOC val2012. After Hungarian matching, we are\nModel\nAPbox\nAPbox\n50\nAPmask\nAPmask\n50\nCutLER+\n5.9\n9.0\n5.3\n8.6\nPanoptic\n6.1\n9.8\n5.8\n9.0\nInstance\n7.3\n11.8\n6.4\n11.2\nTable A5. Limitation of U2Seg. We show the zero-shot unsu-\npervised instance segmentation results on COCO val2017. Cut-\nLER+ is evaluated on the combination of CutLER and offline clus-\ntering, Panoptic is trained on both \u201cstuff\u201d and \u201cthings\u201d pseudo la-\nbels, Instance is trained solely on \u201cthings\u201d labels.\nable to get the real categories of the predicted instances.\nD. Unsupervised Universal Image Segmenta-\ntion\nOur model\u2019s performance for unsupervised universal im-\nage segmentation closely mirrors the trends observed in in-\nstance segmentation. Specifically, as the number of the pre-\ndicted clusters increases, the performance of the panoptic\nsegmentation also improves. Detailed universal segmenta-\ntion results are shown in Tab. A4 and Fig. A2.\nE. Limitation\nThe primary goal of our research is to develop a compre-\nhensive model capable of excelling in all areas of unsuper-\nvised segmentation. As shown in Tab. A5, in terms of the\nindividual sub-task, the universal model exhibits a slight un-\nderperformance compared to its counterpart model trained\nwith task-specific annotations. This suggests that U2Seg\nis adaptable to various tasks, yet it requires task-specific\ntraining to achieve the best outcomes for a specific sub-task.\nLooking ahead, we aim to develop a more versatile model\nthat can be trained once to effectively handle multiple tasks.\nFigure A1. Unsupervised object detection and instance segmentation visualization of COCO val2017 and PASCAL VOC val2012\n(after Hungarian matching).\nOriginal Images\nPrediction\nGround Truth\nPseudo Label\nFigure A2. Unsupervised universal image segmentation visualizations of COCO val2017 (after Hungarian matching).\nReferences\n[1] Edward H Adelson. On seeing stuff: the perception of mate-\nrials by humans and machines. In Human Vision and Elec-\ntronic Imaging, 2001.\n[2] YM Asano, C Rupprecht, and A Vedaldi. Self-labelling via\nsimultaneous clustering and representation learning. In In-\nternational Conference on Learning Representations, 2019.\n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers.\nIn International\nConference on Learning Representations, 2021.\n[4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 6154\u20136162, 2018.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020.\n[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. Ad-\nvances in Neural Information Processing Systems, 33, 2020.\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 9650\u20139660, 2021.\n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597\u20131607. PMLR, 2020.\n[10] Xinlei Chen and Kaiming He. Exploring simple siamese rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n15750\u201315758, 2021.\n[11] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-DeepLab: A simple, strong, and fast baseline for\nbottom-up panoptic segmentation. In CVPR, 2020.\n[12] Bowen Cheng, Alexander G. Schwing, and Alexander Kir-\nillov. Per-pixel classification is not all you need for semantic\nsegmentation. 2021.\n[13] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation. 2022.\n[14] Jang Hyun Cho, U. Mall, K. Bala, and Bharath Hariharan.\nPicie: Unsupervised semantic segmentation using invari-\nance and equivariance in clustering. ArXiv, abs/2103.17070,\n2021.\n[15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding.\nCoRR,\nabs/1604.01685, 2016.\n[16] Zhiyuan Dang, Cheng Deng, Xu Yang, Kun Wei, and Heng\nHuang. Nearest neighbor matching for deep clustering. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 13693\u201313702, 2021.\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009.\n[18] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 1422\u20131430, 2015.\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[20] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qing-\nming Huang, and Qi Tian. Centernet: Keypoint triplets for\nobject detection.\nIn Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 6569\u20136578,\n2019.\n[21] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88(2):303\u2013338, 2010.\n[22] Edward W Forgy. Cluster analysis of multivariate data: ef-\nficiency versus interpretability of classifications. biometrics,\n21:768\u2013769, 1965.\n[23] Jean-Bastien Grill, Florian Strub, Florent Altch\u00b4e, Corentin\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\nnew approach to self-supervised learning.\narXiv preprint\narXiv:2006.07733, 2020.\n[24] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah\nSnavely, and William T Freeman. Unsupervised semantic\nsegmentation by distilling feature correspondences. arXiv\npreprint arXiv:2203.08414, 2022.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016.\n[26] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017.\n[27] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729\u20139738, 2020.\n[28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u2013\n16009, 2022.\n[29] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita\nOrlov, and Humphrey Shi. OneFormer: One Transformer to\nRule Universal Image Segmentation. 2023.\n[30] Xu Ji, Jo\u02dcao F Henriques, and Andrea Vedaldi.\nInvariant\ninformation clustering for unsupervised image classification\nand segmentation. In ICCV, 2019.\n[31] Alexander Kirillov, Kaiming He, Ross B. Girshick, Carsten\nRother, and Piotr Doll\u00b4ar.\nPanoptic segmentation.\n2019\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 9396\u20139405, 2018.\n[32] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDoll\u00b4ar. Panoptic feature pyramid networks. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 6399\u20136408, 2019.\n[33] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDoll\u00b4ar. Panoptic feature pyramid networks. In CVPR, 2019.\n[34] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll\u00b4ar. Panoptic segmentation. In CVPR,\n2019.\n[35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023.\n[36] Hans-Peter Kriegel, Erich Schubert, and Arthur Zimek. The\n(black) art of runtime evaluation: Are we comparing algo-\nrithms or implementations?\nKnowledge and Information\nSystems, 52(2):341\u2013378, 2017.\n[37] Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Yukang Chen,\nLu Qi, Liwei Wang, Zeming Li, Jian Sun, and Jiaya Jia.\nFully convolutional networks for panoptic segmentation with\npoint-based supervision. arXiv preprint arXiv:2108.07682,\n2021.\n[38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In European Conference on Computer Vision, pages\n280\u2013296. Springer, 2022.\n[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740\u2013755.\nSpringer, 2014.\n[40] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll\u00b4ar. Focal loss for dense object detection. In ICCV,\n2017.\n[41] Stuart Lloyd. Least squares quantization in pcm. IEEE trans-\nactions on information theory, 28(2):129\u2013137, 1982.\n[42] Geoffrey J McLachlan and Thriyambakam Krishnan. The\nEM algorithm and extensions. John Wiley & Sons, 2007.\n[43] Ishan Misra and Laurens van der Maaten. Self-supervised\nlearning of pretext-invariant representations. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6707\u20136717, 2020.\n[44] Sungwon Park, Sungwon Han, Sundong Kim, Danu Kim,\nSungkyu Park, Seunghoon Hong, and Meeyoung Cha. Im-\nproving unsupervised image clustering with robust learning.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12278\u201312287, 2021.\n[45] Jianbo Shi and Jitendra Malik. Normalized cuts and image\nsegmentation. IEEE Transactions on pattern analysis and\nmachine intelligence, 22(8):888\u2013905, 2000.\n[46] Oriane Sim\u00b4eoni, Gilles Puy, Huy V Vo, Simon Roburin,\nSpyros Gidaris, Andrei Bursuc, Patrick P\u00b4erez, Renaud\nMarlet, and Jean Ponce.\nLocalizing objects with self-\nsupervised transformers and no labels.\narXiv preprint\narXiv:2109.14279, 2021.\n[47] Laurens Van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of machine learning research, 9\n(11), 2008.\n[48] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nGeorgoulis, Marc Proesmans, and Luc Van Gool.\nScan:\nLearning to classify images without labels. In European con-\nference on computer vision, pages 268\u2013285. Springer, 2020.\n[49] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nGeorgoulis, and Luc Van Gool.\nUnsupervised semantic\nsegmentation by contrasting object mask proposals. arxiv\npreprint arxiv:2102.06191, 2021.\n[50] Wouter Van Gansbeke, Simon Vandenhende, and Luc\nVan Gool.\nDiscovering object masks with transformers\nfor unsupervised semantic segmentation.\narXiv preprint\narXiv:2206.06363, 2022.\n[51] Van Huy Vo, Elena Sizikova, Cordelia Schmid, Patrick\nP\u00b4erez, and Jean Ponce. Large-scale unsupervised object dis-\ncovery. Advances in Neural Information Processing Systems,\n34:16764\u201316778, 2021.\n[52] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen.\nMaX-DeepLab: End-to-end panoptic\nsegmentation with mask transformers. In CVPR, 2021.\n[53] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran.\nUnidentified video objects: A benchmark for dense, open-\nworld segmentation. CoRR, abs/2104.04691, 2021.\n[54] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran.\nUnidentified video objects: A benchmark for dense, open-\nworld segmentation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 10776\u2013\n10785, 2021.\n[55] Xudong Wang, Ziwei Liu, and Stella X Yu. Unsupervised\nfeature learning by cross-level instance-group discrimina-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 12586\u201312595,\n2021.\n[56] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong,\nand Lei Li. Dense contrastive learning for self-supervised\nvisual pre-training. In Proc. IEEE Conf. Computer Vision\nand Pattern Recognition (CVPR), 2021.\n[57] Xudong Wang, Long Lian, and Stella X Yu. Unsupervised\nselective labeling for more effective semi-supervised learn-\ning.\nIn European Conference on Computer Vision, pages\n427\u2013445. Springer, 2022.\n[58] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz,\nAnima Anandkumar, Chunhua Shen, and Jose M Alvarez.\nFreesolo: Learning to segment objects without annotations.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14176\u201314186, 2022.\n[59] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra.\nCut and learn for unsupervised object detection and instance\nsegmentation. arXiv preprint arXiv:2301.11320, 2023.\n[60] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra.\nCut and learn for unsupervised object detection and instance\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3124\u2013\n3134, 2023.\n[61] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao\nLi, Shell Xu Hu, James L Crowley, and Dominique Vaufrey-\ndaz. Tokencut: Segmenting objects in images and videos\nwith self-supervised transformer and normalized cut. arXiv\npreprint arXiv:2209.00383, 2022.\n[62] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 3733\u2013\n3742, 2018.\n[63] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised\ndeep embedding for clustering analysis.\nIn International\nconference on machine learning, pages 478\u2013487. PMLR,\n2016.\n[64] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min\nBai, Ersin Yumer, and Raquel Urtasun. Upsnet: A unified\npanoptic segmentation network. In CVPR, 2019.\n[65] Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins, , et al. Lo-\ncal aggregation for unsupervised learning of visual embed-\ndings. In ICCV, 2019.\n[66] Adrian Ziegler and Yuki M Asano. Self-supervised learning\nof object parts for semantic segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14502\u201314511, 2022.\n"
  },
  {
    "title": "MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices",
    "link": "https://arxiv.org/pdf/2312.16886.pdf",
    "upvote": "16",
    "text": "MobileVLM: A Fast, Strong and Open Vision Language Assistant\nfor Mobile Devices\nXiangxiang Chu1,\nLimeng Qiao1,\nXinyang Lin1,\nShuang Xu1,\nYang Yang1,3*,\nYiming Hu1,\nFei Wei1,\nXinyu Zhang1, Bo Zhang1, Xiaolin Wei1, Chunhua Shen2\n1 Meituan Inc.\n2 Zhejiang University, China\n3 Dalian University of Technology, China\nAbstract\nWe present MobileVLM, a competent multimodal vision\nlanguage model (MMVLM) targeted to run on mobile de-\nvices. It is an amalgamation of a myriad of architectural de-\nsigns and techniques that are mobile-oriented, which com-\nprises a set of language models at the scale of 1.4B and\n2.7B parameters, trained from scratch, a multimodal vi-\nsion model that is pre-trained in the CLIP fashion, cross-\nmodality interaction via an efficient projector. We evaluate\nMobileVLM on several typical VLM benchmarks. Our mod-\nels demonstrate on par performance compared with a few\nmuch larger models. More importantly, we measure the in-\nference speed on both a Qualcomm Snapdragon 888 CPU\nand an NVIDIA Jeston Orin GPU, and we obtain state-of-\nthe-art performance of 21.5 tokens and 65.3 tokens per sec-\nond, respectively. Our models are available at: https:\n//github.com/Meituan-AutoML/MobileVLM\n1. Introduction\nLarge multimodal models (LMMs), especially the family\nof Visual Language Models (VLMs), rise as a promising re-\nsearch direction for building general-purpose assistants due\nto their substantially enhanced capability in both percep-\ntion and reasoning [127]. However, it has been challenging\nto connect the representations of the pre-trained large lan-\nguage models (LLMs) [26,49,115,118,131] and the vision\nmodels to unveil the cross-modality properties, such as vi-\nsual question answering, image captioning, visual knowl-\nedge reasoning and conversation, etc.\nRemarkable performance on this task can be witnessed\nin GPT-4V [90] and Gemini [45], and the evaluation of their\nabilities can be found in [1,41]. However, very limited tech-\nnical details are available for these proprietary models. Si-\nmultaneously in the research community, a line of language\ntuning methods have been proposed [2,5,30,66,74,76,78,\n*Work done as an intern at Meituan.\n122, 126, 133]. For instance, Flamingo [2] exploits visual\ntokens to condition the frozen language model via gated\ncross-attention layers. BLIP-2 [66] argues that such inter-\naction is insufficient and introduces a lightweight querying\ntransformer (called Q-Former) that extracts the most use-\nful features from the frozen vision encoder and feeds them\ndirectly into the frozen LLM. MiniGPT-4 [133] aligns a\nfrozen visual encoder from BLIP-2 [66] with a frozen lan-\nguage model Vicuna [118] via only one projection layer. In-\ndependently, LLaVA [76] applies a simple trainable projec-\ntor that converts the vision features into embedding tokens,\nwhich have the same dimension as the word embeddings to\nbe processed by the language model altogether.\nNoticeably, training strategies also exhibit a shift to ac-\ncommodate the large-scale multimodal data of great diver-\nsity. LLaVA may be the first attempt to replicate the instruc-\ntion tuning paradigm from LLMs to the multimodal sce-\nnario. To generate multimodal instruction-following data, it\nfeeds textual information such as captions and bounding-\nbox coordinates of images to language-only GPT-4 [89].\nMiniGPT-4 [133] is firstly trained on a combined image\ncaptioning dataset and then fine-tuned on a curated align-\nment dataset of image-text pairs.\nInstructBLIP [30] en-\nforces vision-language instruction tuning based on the pre-\ntrained BLIP-2 model, where the Q-Former is trained on\na diverse set of datasets organized in an instruction-tuning\nformat. mPLUG-Owl [126] introduces a two-stage train-\ning strategy where the visual part is pre-trained first and the\nlarge language model LLaMA [115] is then fine-tuned with\nLoRA [51] with instruction data from various sources.\nDespite the advances mentioned above of VLMs, there\nis a natural demand to enable cross-modality capacities\nin resource-constrained scenarios. Gemini [45] surpasses\nstate-of-the-art performance on a range of multimodal\nbenchmarks and introduces mobile-scale VLMs with 1.8B\nand 3.25B parameters for low-memory devices. Common\ncompression techniques such as distillation and quantiza-\ntion are also exploited for this purpose. We aim to build the\nfirst open, mobile-scale VLMs trained using public datasets\n1\narXiv:2312.16886v2  [cs.CV]  30 Dec 2023\nand available techniques to achieve visual perception and\nreasoning, customized for resource-constrained platforms.\nOur contributions are as follows:\n1. We present MobileVLM, a full-stack remake of multi-\nmodal visual language models tailored for mobile sce-\nnarios. To our knowledge, we are the first to provide\na detailed, reproducible, and strong vision language\nmodel from scratch. With controlled and open-source\ndatasets, we build a set of high-performance founda-\ntion language models and multimodal models.\n2. We make extensive ablation studies on the design of\nvisual encoders and systematically evaluate the VLM\nperformance sensitivity on various training paradigms,\ninput resolutions, and model sizes.\n3. We design an efficient projector between visual and\ntext features, which better aligns multimodal features\nwhile reducing the inference budget.\n4. Our model is crafted to run efficiently on mobile, low-\npower devices, with a measured speed of 21.5 tokens/s\non a Qualcomm mobile CPU and 65.3 tokens/s on a\nJeston Orin GPU respectively.\n5. Our models perform comparably on a large body of\nVLM benchmarks, attesting their potential in numer-\nous tasks in practice.\nAlthough we mainly focus\non edge scenarios, our model outperforms many re-\ncent VLMs, which can only be supported by powerful\nGPUs in the cloud.\n2. Related Work\n2.1. Vision Transformer\nVision Transformer [34] is now the dominant backbone\nfor visual perception. Its sequel methods such as Swin [81],\nDeiT [114], PVT [121], and Twins [22] have upgraded\nits original architectural design to strengthen its represen-\ntational power and efficiency. The pre-training paradigm\nhas also experienced several shifts, from image-supervised\nlearning (i.e. labeled classification) to unsupervised learn-\ning like masked auto-encoders [47], and most recently to\nlanguage-supervised training as advertised by CLIP [97],\nwhich empowered ViTs with unprecedented zero-shot capa-\nbility. VLMo [6] enhances CLIP with unified multimodality\ntraining.\n2.2. LLMs\nLarge language models often come with billions of pa-\nrameters and are pre-trained on extremely extensive text\ncorpora, exhibiting emergent capabilities [123] that have\nnot been witnessed before. They have reshaped the field\nof natural language processing and are being used in a wide\nrange of applications. To date, proprietary LLMs like GPT-\n4 [89] prevail over open-sourced models.\nNevertheless,\nthe community is exuberant with the continuous model re-\nleases, including GLM [35], BLOOM [65], OPT [131] and\nLLaMA series [115,116]. Many recent works [4,132] have\nbeen built on top of them.\nNoticeably, there is a trend to build smaller language\nmodels, i.e., whose parameters are around 1B or fewer. To\nname a few, GPT-Neo [9], Pythia [7], GALACTICA [112],\nOpenLLaMA [43], Phi [46, 70], Qwen [4] all ship lan-\nguage models at such sizes. Although privately collected\nhigh-quality data can significantly boost the performance of\nLLMs [4, 70], our target is to build reproducible and effi-\ncient models, hence we do not utilize any non-public data\nfor our research.\n2.3. VLMs\nThroughout recent years, a school of vision language\nmodels has rapidly emerged.\nTable 1 summarizes them\nin a detailed comparison regarding architectures, cross-\nmodality design, and training corpora.\nArchitecture choices. As a consequence of the intim-\nidating training cost of large language models, most lan-\nguage models used in VLMs are pre-trained open-source\nmodels like OPT [131], Flan-T5 [26], Chinchilla [63], Vi-\ncuna [118] and LLaMA [115]. QWen adapts LLaMA with\ncustom variations [4] to obtain an LLM on their own.\nVisual backbones in VLMs are typically vision trans-\nformer [34], but pre-trained in various strategies [37,66,97].\nMost VLMs prefer CLIP-fashioned ViT [97] trained with\nnatural language supervision. Flamingo picks NFNet-F6\n[59]. KOSMOS chooses VLMo [6] instead.\nDataset-centric-ness. The construction of training data\nhas become increasingly crucial. It is common to utilize\nmillions of text-image pairs in the line of VLMs, where\nthe new datasets are usually released alongside their cor-\nresponding new models.\nTo name a few, apart from an\nenhanced visual receptor and novel language model called\nQwen-LM [4], the multilingual multimodal Qwen-VL [5]\nadditionally aligns the image with caption and box tu-\nples, which sets a new record of generalist models. PALI\n[19] and PALI-X [18] consume an internal multi-language\nimage-text dataset called WebLI at a scale of 12 billion.\nMost recently, observing the constraints of current image-\ntext datasets like hallucination and inaccurate descriptions,\nShareGPT4V [16] exploits GPT-4V [90] for generating\n1.2M high-quality image-text pairs with which can surpass\nthe LLaVA series.\nSimilarly built with GPT-4V, LVIS-\nINSTRUCT4V [120] helps LLaVA-1.5 to gain substantial\nimprovement on various VLM benchmarks.\n2\nModel\nVision Encoder\nLanguage Model\nCross-modality Design\nMultimodal Training Corpora\nCLIP [97]\nViT, ResNet\nTransformer\nLinear-Projection\nWebImageText [97] (400M image-text pairs)\nBLIP [67]\nViT\nMED\u2217\nCross-Attention\nCOCO [73], VG [62], CC3M [103], CC12M [13], LAION [102]\nBLIP-2 [66]\nCLIP/Eva-CLIP ViT\nOPT, Flan-T5\nQ-Former\nsame as BLIP\nInstructBLIP [30]\nViT-G/14\nFlan-T5, Vicuna\nQ-Former w/ FC\n13 held-in out of 26 public datasets\nFlamingo [2]\nNFNet-F6 [59]\nChinchilla\nPerceiver-Resampler [57] M3W(43M), ALIGN(1.4B) [58], LTIP (312M), VTP (27M)\nLLaVA [76]\nCLIP ViT-L/14\nVicuna 7B/13B\nLinear-Projection\nfiltered CC-595K from CC3M [103], LLaVA-Instruct-158K\nLLaVA-1.5 [74]\nCLIP ViT-L@336\nVicuna-7B/13B\nMLP\na subset of InstructBLIP (1.2M)\nMiniGPT-4 [133]\nEVA-CLIP ViT-G/14\nVicuna-7B\nQ-Former\nLAION, CC, SBU [91]\nShikra [15]\nCLIP ViT-L/14\nVicuna-7/13B\nFC-layer\nFlickr30K [95], RefCOCO [61], VG, Visual-7W [85]\nmPLUG-Owl [126] CLIP ViT-L/14\nLLaMA-7B\nCross-Attention\nLAION-400M, COYO-700M [11], CC, COCO\nKOSMOS-2 [94]\nVLMo [6]\nMAGNETO [119] Perceiver-Resampler\nGRIT (curated with LAION-2B, COYO-700M)\nQWen-VL [5]\nOpenclip ViT-bigG [54] Qwen-LM\nCross-Attention\nmulti-tasking datasets (Captioning, VQA, Grounding, etc. )\nShareGPT4V [16]\nCLIP ViT-L/14@336\nVicuna-7B\nMLP\nShareGPT4V (100K by GPT-4V, 1.2M by its learned model)\nMobileVLM (ours) CLIP ViT-L/14@336\nMobileLLaMA\nLDP\nsame as LLaVA-1.5 [74]\nTable 1. Comparison of open-source VLM architectures and their training corpora. \u2217: BLIP adopts a multimodal encoder/decoder.\n2.4. Model Compression for LLMs\nLarge Language Models (LLMs) have brought a\nparadigm shift in natural language processing, while their\ncolossal size and computational requirements pose signifi-\ncant challenges for real-world deployment, particularly in\nenvironments with limited resources.\nThe size of these\nmodels often results in high memory usage and slow pro-\ncessing speeds. Additionally, the energy requirements for\ntraining and operating these models raise sustainability con-\ncerns.\nThese challenges are becoming more pressing as\nLLMs continue to grow in size and complexity.\nIn re-\nsponse to these challenges, model compression has emerged\nas a crucial research area, which aims to reduce the size\nand computational demands of models without significantly\ncompromising their performance. These techniques include\nbut not limited to model pruning [38, 84, 109], quantiza-\ntion [39,68,124], knowledge distillation [130] and low-rank\ndecomposition [125].\nBesides,\nLLM deployment tools have experienced\na blossom,\nevidenced by industrial frameworks like\nTensorRT-LLM [87], LMDeploy [55], and llama.cpp\n[44] being developed to tackle deployment difficulties in di-\nverse environments.\n2.5. VLM Benchmarks\nSystematic and comprehensive evaluations of different\nVLMs are of great necessity. To this end, POPE [71] pro-\nvides a benchmark for evaluating hallucination in VLMs,\nwhich formulates the evaluation as a binary classification\ntask that prompts the VLMs to answer whether the ob-\nject exists. In contrast, GQA [52] mainly centers around\nthe VLMs\u2019 abilities in real-world reasoning, scene under-\nstanding, and compositional question answering. TextVQA\n[105] consists of questions related to the text in the image,\nevaluating the OCR and reasoning abilities of models. Sci-\nenceQA [83] consists of multimodal multiple-choice ques-\ntions covering scientific topics, e.g., natural science, social\nscience, and language science, which requires VLMs to in-\ntegrate common sense into reasoning. MME [40] measures\nboth the perception and cognition abilities of VLMs, it cov-\ners a total of 14 subtasks varying from coarse-grained to\nfine-grained ones. MMBench [80] is a methodically con-\nstructed multimodal dataset, which collectively covers a di-\nverse spectrum of 20 fine-grained skills and involves a cir-\ncular evaluation strategy with the incorporation of Chat-\nGPT [88].\n2.6. Embodied AI\nOur work is closely related to Embodied Artificial Intel-\nligence. Being one of the central goals of Artificial Gen-\neral Intelligence, embodied AI strives to build egocentric\nintelligence systems that can interact with their surround-\nings with perception, reasoning, and planning capabilities\n[36]. Recently, the emerging large vision language mod-\nels [86, 106, 108] allow embodied AI agents to resolve the\nrelevant tasks like embodied question answering [32], and\nvision-language navigation [69, 92] in a highly end-to-end\nfashion.\n3. MobileVLM\n3.1. Overall Architectural Design\nWith the primary goal of achieving efficient visual per-\nception and reasoning for resource-limited edge devices in\nmind, we design the overall architecture of MobileVLM as\nillustrated in Figure 1. It contains three components: 1) a vi-\nsual encoder, 2) a tailored LLM for edge devices (MobileL-\nLaMA), and 3) an efficient projector (termed \u201clightweight\ndownsample projector\u201d, LDP) that aligns the visual and the\n3\ntextual space.\nTaking an image Xv \u2208 RH\u00d7W \u00d7C as input, the vision\nencoder Fenc extracts the visual embeddings f \u2208 RNv\u00d7Dv\nfor image perception, where Nv = HW/P 2 denotes the\nnumber of image patches and Dv denotes the hidden size of\nvisual embeddings. To alleviate the efficiency issues aris-\ning from prolonged image tokens, we scheme a lightweight\nprojector P for visual feature compression and visual-text\nmodal alignment. It converts f into the word embedding\nspace with an appropriate input dimension of the subse-\nquent language model as below,\nHv = P(f), f = Fenc(Xv).\n(1)\nThus we obtain the image tokens Hv \u2208 R(Nv/4)\u00d7Dt and\ntext tokens Hq \u2208 RNt\u00d7Dt, where Nt denotes the number\nof text tokens and Dt denotes the hidden size of the word\nembedding space. Observing that LLM occupies the most\ncomputation and memory consumption in the current de-\nsign paradigm of MLLMs, we tailor a series of inference-\nfriendly LLMs that enjoy advantages in speed for mobile\napplications. It predicts the response Ya = {yi}L\ni=1 condi-\ntioned on the multimodal input in an autoregressive manner,\nwhere L denotes the output token length. This process can\nbe formulated as,\np(Ya|Hv, Hq) =\nL\nY\ni=1\np(yi|Hv, Hq, y<i).\n(2)\n3.2. Visual Encoder\nBased on empirical analysis later shown in Sec 5.1, we\nleverage the pre-trained CLIP ViT-L/14 [97] with a resolu-\ntion of 336\u00d7336 as our visual encoder Fenc. The Vision\nTransformer (ViT) [34] dissects images into uniform-sized\npatches, applies a linear embedding to each, and integrates\npositional encodings before feeding the resultant vector se-\nquence into a canonical Transformer encoder. Typically, a\nclassification token is appended to this sequence for subse-\nquent categorization tasks.\n3.3. MobileLLaMA\nFor the language model, we downscale LLaMA [115]\nto facilitate the off-the-shelf deployment, i.e., our models\ncan be seamlessly supported by almost all popular infer-\nence frameworks. Moreover, we evaluate the model latency\non the edge devices to guide the model design. Neural ar-\nchitecture search [12,24,25,50,77] would be a better choice,\nbut for the time being we keep it as our future work. The\ndetailed setting of our architecture is shown in Table 2.\nSpecifically, we utilize the sentence piece tokenizer [63]\nfrom LLaMA2 [116] with a vocabulary size of 32000 and\ntrain the embeddings from scratch. This is beneficial for\nperforming future distillation without further pain. The con-\ntext length used at the pre-train stage is 2k for all models\nModel\nBlocks Dim Heads Context length\nMobileLLaMA 1.4B\n24\n2048\n16\n2k\nMobileLLaMA 2.7B\n32\n2560\n32\n2k\nTable 2. Detailed settings of our language models.\ndue to limited resources. However, the context window can\nbe further scaled to 8k for inference, as indicated by [17].\nThe detailed settings of other components are listed below.\n\u2022 We apply RoPE [107] to inject positional information.\n\u2022 We apply pre-normalization to stabilize training.\nSpecifically, we use RMSNorm [129] instead of layer\nnorm and the MLP expansion ratio 8/3 instead of 4.\n\u2022 We also use SwiGLU activation function [104] instead\nof GELU as [115].\n3.4. Efficient Projector\nThe projector between the vision encoder and the lan-\nguage model is critical in aligning multimodal features.\nThere are two existing paradigms: Q-Former [66, 133] and\nMLP projection [74, 76]. Q-Former explicitly controls the\nnumber of visual tokens per query to force extracting the\nmost relevant visual information. However, it inevitably\nloses the spatial positional information of tokens and suffers\nfrom slow convergence. In addition, it is inefficient for the\ninference on edge devices. In contrast, MLP retains the spa-\ntial information but it usually includes useless tokens such\nas the background. For an image of Xv \u2208 RH\u00d7W \u00d7C with\na patch size of P, there are Nv = HW/P 2 visual tokens to\nbe injected into the LLM model, which greatly slows down\nthe overall inference speed. Inspired by [23], we can utilize\nconvolutions to enhance the positional information and en-\ncourage local interaction of the vision encoder. Specifically,\nwe explore mobile-friendly operations based on depth-wise\nconvolution (the simplest form of PEG [23]), which is effi-\ncient and well-supported by various edge devices.\nTo keep spatial information and to minimize the compu-\ntational cost, we make use of a convolution with a stride\nof 2, which reduces 75% visual tokens. This design sig-\nnificantly boosts the overall inference speed.\nHowever,\nour experimental result indicates that solely downsampling\nthe tokens severely deteriorates the performance on down-\nstream tasks, such as OCR. To alleviate this effect, we\nutilize a more powerful network instead of a single PEG.\nThe detailed architecture of the efficient projector, called\nLightweight Downsample Projector (LDP), is illustrated in\nFigure 2. Note that, this projector network only contains\nless than 20M parameters and runs about 81\u00d7 faster than\nthe visual encoder.\n4\nVision\nEncoder\nLDP\nMobileLLaMA\nTokenizer\nX\ud835\udc97\nX\ud835\udc92\nH\ud835\udc97\nH\ud835\udc92\nY\ud835\udc82\nLDP\nDepthwise s=1\nPointwise\nDepthwise s=2\nPointwise\nH\ud835\udc97 \u2208 \u211d(\"!/$)\u00d7'!\nPointwise\nGELU\n\ud835\udc53 \u2208 \u211d\"!\u00d7'!\nPointwise\nLayer Normalization\nMobileVLM architecture\nPixel-wise addition \nFigure 1. The MobileVLM architecture (right) utilizes MobileLLaMA as its language model, intakes Xv and Xq which are image and\nlanguage instructions as respective inputs and gives Ya as the output language response. LDP refers to a lightweight downsample projector\n(left).\nWe use Layer Normalization instead of Batch Normal-\nization [56] to make training stable and not affected by the\nbatch size. Since the projector is already very light-weight,\ntherefore, we don\u2019t adopt recent mobile reparameterization\ndesigns [21,117].\nFormally, LDP (denoted as P) takes the visual embed-\ndings f \u2208 RNv\u00d7Dv as input, and outputs the efficiently ex-\ntracted and aligned visual tokens Hv \u2208 R(Nv/4)\u00d7Dt as,\nHv = P(f) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nf0\n= PW(GELU(PW(f))),\nf1\n= LN(PW(LN(DW(f0)))) + f0,\nHv\n= LN(PW(LN(DW(f1))).\n(3)\n4. Experiment\n4.1. Training\nThe whole reproducible training process is composed of\nthree stages. Firstly, we pre-train LLM foundation mod-\nels on the text-only dataset RedPajama v1 [29]. Secondly,\nwe perform supervised fine-tuning (SFT) following Vicuna\n[20] on a dataset of multi-turn dialogues between humans\nand ChatGPT from third-party platforms. Lastly, we train\nour vision large models using multimodal datasets.\nLanguage model pre-training. Since our target is train-\ning deployable models for inference, we do not strictly fol-\nlow the efficient combination of model capacity and tokens\nfrom scaling law [60]. To make our work reproducible, all\nthe models are trained on 1.3T tokens1 from the RedPajama\nv1 [29] dataset only. This benefits further research by en-\nabling controlled experiments. We apply the same sampling\n1Applying the tokenizer of [116] on this dataset generates about 1.3T\ntokens.\nStage 1: Pre-training\nStage 2: Instruction Tuning\nVision\nEncoder\nProjector\nMobileLLaMA\nXv\nXq\nVision\nEncoder\nProjector\nMobileLLaMA\nXq\nXv\nFigure 2. Illustration of the MobileVLM training strategy.\nratio of different datasets as [115], which is shown in Ta-\nble 12 (see Appendix). The common autoregressive loss is\nadopted. We utilize a global batch size of 5,242,880.\nThe peak learning rate is set to 3 \u00d7 10\u22124 and it decays to\n3 \u00d7 10\u22125 following the cosine strategy. We warm up with\n2000 iterations. We use the AdamW optimizer [82] with\n\u03b21 = 0.9 and \u03b22 = 0.95 and a weight decay regularization\nvalue of 0.1. The gradient clipping threshold is set to 1.0.\nWe adopt the Pytorch lightning framework with DeepSpeed\nbackend [99]. Specifically, we utilize ZERO 1 [98] and\ngradient accumulation to achieve a training speed of 18800\nTGS (Tokens per second for a GPU) for the 1.4B model and\n8500 TGS for the 2.7B model, on 20 nodes equipped with\n8 NVIDIA Tesla A100 GPUs each.\nFurthermore, we also favor Flash Attention V2 [31] to al-\nleviate the I/O bottleneck and to train faster. We randomly\nshuffle the data to disturb the sequential order with a fixed\nseed, which is vital since the training process can be inter-\nmittently interrupted and requires to be resumed. We first\ntokenize the raw data into IDs and serialize them into many\nbucket files. We then utilize memory mapping to deliver a\n5\nModel\nCommon Sense Reasoning\nLanguage Understanding\nARC(c/e)\nBoolQ RTE Winogrande TruthfulQA\nHellaSwag PIQA MMLU\nAvg.\nINCITE 3B (V1) [29]\n0.32 / 0.68\n0.67\n0.52\n0.63\n0.21 / 0.33\n0.48\n0.74\n0.2675 0.4848\nOpenLLaMA 3B (V1) [43]\n0.34 / 0.69\n0.68\n0.58\n0.62\n0.22 / 0.35\n0.49\n0.75\n0.2695 0.4990\nMobileLLaMA 2.7B\n0.32 / 0.68\n0.61\n0.59\n0.63\n0.23 / 0.36\n0.48\n0.75\n0.2730 0.4923\nTinyLLaMA 1.1B (2T) [93] 0.23 / 0.57\n0.59\n0.55\n0.57\n0.23 / 0.39\n0.40\n0.70\n0.2541 0.4484\nGalactica 1.3B [112]\n0.28 / 0.62\n0.62\n0.52\n0.55\n0.25 / 0.41\n0.34\n0.63\n0.2675 0.4488\nOPT 1.3B [131]\n0.24 / 0.57\n0.56\n0.51\n0.59\n0.24 / 0.39\n0.41\n0.71\n0.2461 0.4466\nPythia 1.4B [7]\n0.26 / 0.61\n0.63\n0.52\n0.57\n0.23 / 0.39\n0.40\n0.71\n0.2568 0.4577\nMobileLLaMA 1.4B\n0.26 / 0.61\n0.53\n0.53\n0.59\n0.21 / 0.35\n0.43\n0.73\n0.2497 0.4490\nTable 3. Comparison with SOTA mobile-scale language models on mainstream language benchmarks.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nTokens\n1e12\n2\n4\n6\n8\n10\nLoss\nMobileLLaMA 1.4B\nMobileLLaMA 2.7B\nFigure 3. Training loss curves of our MobileLLaMA 1.4B and\n2.7B on 1.3T tokens of RedPajama data.\ndesired I/O speed. Besides, we pack different sentences to-\ngether, where an EOS token is inserted to set different sen-\ntences apart. Due to limited resources, we do not try the\ndesign of InternLM [113], which may further improve the\nmodel performance by disabling such packing. The overall\ntraining loss decreases as the consumed tokens increase and\nis shown in Figure 3.\nSFT on language models.\nAs clarified by LLaMA-\n2 [116], fewer higher-quality examples from their vendor-\nbased annotation efforts significantly improve the results.\nWe are thus motivated to finetune our MobileLLaMA on a\nhigh-quality dataset with supervised learning. Vicuna [20]\nfine-tunes LLaMA on user-shared conversations collected\nfrom ShareGPT, which is widely used as a language mod-\nule for multimodal model building, but their training dataset\nis not released.\nWe employ a dataset of multi-turn dia-\nlogues between humans and ChatGPT from third-party plat-\nforms [53] which has been cleaned through a process of for-\nmat standardization and quality refinement. The SFT data is\norganized following the Vicuna format, where each sample\nconsists of a dialogue including several user prompts and\nChatGPT answers. As shown in Table 14 (see Appendix),\n0\n100\n200\n300\n400\n500\n600\n700\n800\nTime step\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nLoss\nMobileLLaMA 1.4B\nMobileLLaMA 2.7B\nFigure 4. SFT loss curves of MobileLLaMA 1.4B and 2.7B.\na special token is utilized to separate the assistant\u2019s answer\nand the next round of user prompts. For the training de-\ntails, we use a cosine learning rate schedule without weight\ndecay, a global batch size of 128, and a sequence length of\n2048 tokens. We utilize an autoregressive objective and per-\nform backpropagation only on answer tokens. To achieve\nbetter performance in downstream tasks, we conducted ex-\nperiments to select the appropriate hyperparameters. We\nfine-tune for 3 epochs with a learning rate of 2 \u00d7 10\u22125 for\nMobileLLaMA 1.4B, and 2 epochs with a learning rate of\n3 \u00d7 10\u22125 for MobileLLaMA 2.7B. The training loss de-\ncreases with iterations as shown in Figure 4. To be later\nshown in Sec. 5.4, our empirical performance on down-\nstream tasks demonstrates that high-quality SFT data is es-\nsential to aligning LLMs with dialogue-style instructions.\nVLM training. Similar to [76, 126], the whole training\nprocess comprises two steps: pre-training and instruction\ntuning. This is depicted in Figure 2. During the first step,\nwe freeze the vision encoder and LLM, focusing on training\nthe efficient projector only. Subsequently, we fine-tune both\nthe projector and LLM to enhance the abilities of visual un-\nderstanding and expression by refining the language model\nvia a language modeling loss function. Following Vicuna\u2019s\n6\nMethod\nLLM\nRes. PT\nIT\nGQA SQAI VQAT POPE MME MMBdev\nOpenflamingo [3]\nMPT-7B\n336\n180M -\n\u2013\n\u2013\n33.6\n\u2013\n\u2013\n4.6\nBLIP-2 [66]\nVicuna-13B\n224\n129M -\n41.0\n61.0\n42.5\n85.3\n1293.8\n\u2013\nMiniGPT-4 [133]\nVicuna-7B\n224\n5M\n5K\n32.2\n\u2013\n\u2013\n\u2013\n581.7\n23.0\nInstructBLIP [30]\nVicuna-7B\n224\n129M 1.2M\n49.2\n60.5\n50.1\n\u2013\n\u2013\n36.0\nInstructBLIP [30]\nVicuna-13B\n224\n129M 1.2M\n49.5\n63.1\n50.7\n78.9\n1212.8\n\u2013\nShikra [15]\nVicuna-13B\n224\n600K 5.5M\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n58.8\nmPLUG-Owl [126]\nLLaMA-7B\n224\n2.1M\n102K\n\u2013\n\u2013\n\u2013\n\u2013\n967.3\n49.4\nIDEFICS-9B [64]\nLLaMA-7B\n224\n353M 1M\n38.4\n\u2013\n25.9\n\u2013\n\u2013\n48.2\nIDEFICS-80B [64]\nLLaMA-65B\n224\n353M 1M\n45.2\n\u2013\n30.9\n\u2013\n\u2013\n54.5\nQwen-VL [5]\nQwen-7B\n448\n1.4B\n50M\n59.3\n67.1\n63.8\n\u2013\n1487.6\n38.2\nMiniGPT-v2 [14]\nLLaMA-7B\n448\n23M\n1M\n60.3\n\u2013\n\u2013\n\u2013\n\u2013\n12.2\nLLaVA-1.5 [74]\nVicuna-7B\n336\n558K 665K\n62.0\n66.8\n58.2\n85.9\n1510.7\n64.3\nMobileVLM 1.7B\nMobileLLaMA 1.4B 336\n558K 665K\n56.1\n54.7\n41.5\n84.5\n1196.2\n53.2\nMobileVLM 1.7B w/ LoRA MobileLLaMA 1.4B 336\n558K 665K\n57.0\n53.1\n42.3\n86.0\n1143.7\n50.4\nMobileVLM 3B\nMobileLLaMA 2.7B 336\n558K 665K\n59.0\n61.0\n47.5\n84.9\n1288.9\n59.6\nMobileVLM 3B w/ LoRA\nMobileLLaMA 2.7B 336\n558K 665K\n58.4\n59.0\n46.7\n84.6\n1296.4\n57.0\nTable 4. Comparison with SOTA methods on 6 VLM benchmarks. GQA [52]; SQAI: ScienceQA-IMG [83]; VQAT: TextVQA [105];\nPOPE [71]; MME [40]; MMBdev: MMBench-dev [80]; Column Res. is the image resolution of vision model. Columns PT and IT are the\ndata sizes in the pre-training stage and the visual instruction tuning stage, respectively.\nhyperparameters [118], we pre-train our model on the fil-\ntered CC-595K subset [76] for 1 epoch at a learning rate of\n10\u22123 and a batch size of 256. We fine-tune it on the LLaVA-\nInstruct-158K dataset [76] for 1 epoch at a learning rate of\n2 \u00d7 10\u22125 and a batch size of 128. Examples of our train-\ning dataset are shown in Figure 5 (Appendix C). We choose\nthe AdamW optimizer with no weight decay and a cosine\nlearning rate with a warmup ratio of 3%. The training takes\n5 hours with 8 NVIDIA Tesla A100 GPUs for MobileVLM\n1.7B, and 8 hours for MobileVLM 3B.\n4.2. Evaluation of MobileLLaMA\nIn Table 3, we extensively assess our models on two\nstandard natural language benchmarks, for language under-\nstanding and common sense reasoning respectively. We ap-\nply the Language Model Evaluation Harness [42] tool for\nthe former assessment. Experimental results show that our\nMobileLLaMA 1.4B is on par with the most recent open-\nsource models such as TinyLLaMA 1.1B, Galactica 1.3B,\nOPT 1.3B, and Pythia 1.4B. Notably, our MobileLLaMA\n1.4B outperforms TinyLLaMA 1.1B which is trained on\n2T tokens, twice as many as ours. At the 3B level, our\nMobileLLaMA 2.7B also demonstrates competitive perfor-\nmance to INCITE 3B (V1) and OpenLLaMA 3B (V1),\nwhile MobileLLaMA 2.7B being about 40% faster than\nOpenLLaMA 3B on a Snapdragon 888 CPU as shown in\nTable 5.\nFor common sense reasoning, we report the zero-shot\naccuracy on five prevalent benchmarks, i.e., ARCe [10],\nARCc [28], BoolQ [27], Winogrande [101], and Truth-\nfulQA [72]. Our models demonstrate strong reasoning ca-\npabilities on these benchmarks. It\u2019s worth noting that Mo-\nbileLLaMA 1.4B and 2.7B achieve the highest performance\non Winogrande at both 1B and 3B levels. This indicates that\nour models deliver robust common sense capabilities, rather\nthan cunningly relying on systematic biases in the datasets.\nFurther, we evaluate our models on several language un-\nderstanding tasks, including PIQA [8], HellaSwag [128],\nand MMLU [48].\nWe report the zero-shot accuracy on\nPIQA and HellaSwag, and 5-shot performance on MMLU.\nWe can see that our MobileLLaMA outperforms other mod-\nels across nearly all of the benchmarks. The superior lan-\nguage understanding capability makes our models more\nsuitable for downstream tasks, particularly for instruction\ntuning, in-context learning, etc.\n4.3. Comparison with SOTA VLMs\nWe evaluate the multimodal performance following\nLLaVA on GQA [52], ScienceQA [83], TextVQA [105],\nPOPE [71], and MME [40].\nIn addition, we also ex-\nploit MMBench [80] for a comprehensive comparison. As\ndemonstrated in Table 4, our proposed MobileVLM, despite\nits reduced parameters and limited training data, achieves\ncompetitive performance. In certain instances, it even ob-\ntains superior metrics compared with the previous state-of-\nthe-art multimodal vision language models.\nIn particular, on the evaluation benchmarks of GQA,\nPOPE, and MMBench, MobileVLM demonstrates parity\nwith or superiority over the majority of 7B or larger VLMs,\nwhich proves its exceptional capabilities in image content\n7\nModel\nHardware\nPrecision Size\n(GB)\nSample\n(tokens/s)\nEvalprompt\n(tokens/s)\nEval\n(tokens/s) Total (s)\nOpenLLaMA 3B\nSnapdragon 888\n8-bit\n3.4\n3093\n7.32\n6.58\n63.33\nSnapdragon 888\n4-bit\n2.3\n3604\n8.97\n7.14\n58.04\nMobileLLaMA 2.7B Snapdragon 888\n8-bit\n2.7\n3919\n17.59\n9.14\n44.85\nSnapdragon 888\n4-bit\n1.5\n3932\n18.10\n14.71\n28.30\nTinyLLaMA 1.1B\nSnapdragon 888\n8-bit\n1.1\n4215\n39.49\n19.75\n20.83\nSnapdragon 888\n4-bit\n0.6\n3887\n44.17\n31.54\n13.22\nMobileLLaMA 1.4B Snapdragon 888\n8-bit\n1.4\n3846\n35.46\n17.93\n22.81\nSnapdragon 888\n4-bit\n0.7\n3870\n36.20\n28.32\n14.76\nOpenLLaMA 3B\nJetson Orin\n8-bit\n3.4\n2382\n80.34\n29.97\n13.94\nJetson Orin\n4-bit\n2.3\n3340\n143.25\n32.16\n12.83\nMobileLLaMA 2.7B\nJetson Orin\n8-bit\n2.7\n3040\n133.41\n33.28\n12.46\nJetson Orin\n4-bit\n1.5\n2647\n130.97\n38.99\n10.74\nTinyLLaMA 1.1B\nJetson Orin\n8-bit\n1.1\n3007\n279.61\n72.30\n5.89\nJetson Orin\n4-bit\n0.6\n3801\n306.76\n78.83\n5.38\nMobileLLaMA 1.4B\nJetson Orin\n8-bit\n1.4\n3289\n249.56\n60.73\n6.96\nJetson Orin\n4-bit\n0.7\n3738\n253.22\n66.79\n6.33\nTable 5.\nLantency comparison of small language models on mobile and IoT devices.\n\u201c8-bit\u201d: quantized with mode q8 0 in\nllama.cpp and 4-bit with mode q4 k s. Size refers to the size of quantized models. Sample, Evalprompt, and Eval are mea-\nsured in tokens per second. Sample reflects the velocity at which the next probable output token is selected, Evalprompt denotes the\nduration required to process the prompt before initiating text generation, and Eval signifies the generation speed of the output tokens.\nTotal refers to the entire time consumed by a single inference (loading time included.)\nperception, spatial and relational reasoning, and attribute\nunderstanding. Nonetheless, a notable shortfall arises due\nto the absence of large training datasets, such as code and\ntextbook, which results in a discernible performance dis-\ncrepancy on tasks like ScienceQA and MME. Still, there\nexists potential for enhancement in the model\u2019s proficiency\nby reconciling text comprehension with the nuanced recog-\nnition of fine image content on TextVQA. We list visualized\ndemonstrations in Appendix C.\n4.4. MobileVLM with LoRA\nLow-Rank Adaptation (LoRA) [51] can perform on par\nwith or better than fully fine-tuned LLMs with fewer train-\nable parameters.\nWe empirically investigate this prac-\ntice on MobileVLM to validate its multimodal perfor-\nmance.\nSpecifically, during the VLM visual instruction\ntuning stage, we freeze all the LLM parameters except the\nLoRA matrices. The updated parameters are only 8.87%\nand 7.41% of the full LLM for MobileLLaMA 1.4B and\nMobileLLaMA 2.7B respectively. For LoRA configuration,\nwe set lorar as 128 and the lora\u03b1 as 256. The results are\nshown in Table 4. We can see that MobileVLM with LoRA\nachieves comparable performances to those of full finetun-\ning on 6 benchmarks, which is consistent with [51].\n4.5. Latency Measurement on Mobile Devices\nWe evaluate the inference latency of MobileLLaMA and\nMobileVLM both on a Realme GT mobile phone and an\nNVIDIA Jetson AGX Orin platform. The mobile phone\nis equipped with a Snapdragon 888 SoC and 8GB RAM,\nwhich gives out 26 TOPS computing power.\nOrin is\nequipped with 32GB of memory, offering an impressive 275\nTOPS. It operates on CUDA version 11.4, which supports\nthe latest parallel computing technology for enhanced per-\nformance. We exploit llama.cpp [44] as the inference\nframework for both devices.\nMobileLLaMA\nFor\nlanguage\nmodels\n(LMs)\nin\nTable 5,\ninput prompts are fixed with the sentence\n\u201cBuilding a website can be done in 10\nsimple steps:\\nStep 1:\u201d,\nand the number of\noutput tokens is set to 400.\nLMs are quantized to 4-bit\nand 8-bit with the quantization mode q4 k s and q8 0 of\nllama.cpp , respectively.\nAccording to the measurement on the Snapdragon 888\nSoC shown in Table 5, our MobileLLaMA 2.7B signifi-\ncantly outperforms OpenLLaMA 3B at the same quanti-\nzation precision.\nNotably, using two-thirds consumption\nof RAM and half the loading time, MobileLLaMA 2.7B\nachieves twice the inference speed of OpenLLaMA 3B. For\nthe smaller model, although the quantized MobileLLaMA\n8\nModel\nLM\nHardware\nSize\n(GB)\nV E\n(ms/patch)\nSample\n(tokens/s)\nEvalprompt\n(tokens/s)\nEval\n(tokens/s) Total (s)\nLLaVA-v1.5-336\nVicuna 7B\nSnapdragon 4.70 8.23\n17347\n5.36\n0.25\n329.89\nOpenLLaMA 3B\nSnapdragon 2.88 7.98\n27530\n8.95\n7.22\n84.43\nTinyLLaMA 1B\nSnapdragon 1.18 7.77\n31370\n41.70\n18.40\n20.70\nMobileVLM-336 MobileLLaMA 2.7B Snapdragon 2.14 8.43\n27660\n18.36\n12.21\n33.10\nMobileLLaMA 1.4B Snapdragon 1.40 6.82\n34892\n34.93\n21.54\n18.51\nLLaVA-v1.5-336\nVicuna 7B\nJetson Orin 4.70 2.89\n9281\n367.26\n17.74\n19.75\nOpenLLaMA 3B\nJetson Orin 2.88 2.94\n22270\n474.49\n30.66\n12.52\nTinyLLaMA 1B\nJetson Orin 1.18 2.98\n24655\n1253.94\n76.63\n5.90\nMobileVLM-336 MobileLLaMA 2.7B Jetson Orin 2.14 3.11\n15678\n440.60\n38.34\n8.31\nMobileLLaMA 1.4B Jetson Orin 1.40 3.32\n17712\n667.69\n65.27\n5.14\nTable 6. Lantency comparison of mobile-scale VLMs. The language model of VLMs is quantized to 4-bit with llama.cpp. Size is\nthe summation of the size of the language model and the visual encoder. VE indicates visual encoder, whose latency is measured in ms per\nimage patch. The remaining columns are consistent with those in Table 5. LLaVA-v1.5-336-Vicuna 7B generates 51 output tokens, while\nthe rest VLMs generate 256 output tokens.\n1.4B is 23% larger than that of TinyLLaMA 1B, its infer-\nence speed is only 11% slower than TinyLLaMA 1B in\nterms of both Eval speed and Total inference time, which\ndemonstrates that our model benefits from better architec-\ntural design. On the Orin platform, we can draw the same\nconclusion that MobileLLaMA is quite competitive to the\nopen-source language models at similar parameter scales.\nMobileVLM Due to the limitation of llama.cpp ,\nVLMs are split into the vision encoder and the language\nmodel, which are loaded separately during the inference\nstage. For the inference precision, the language model is\nquantized to 4-bit, while the vision encoder and the pro-\njector keep the original precision. The prompts of VLMs\nare composed of textual prompts and visual prompts. For\na fair comparison, LLaVA-v1.5 and MobileVLMs take the\npicture with the same resolution of 336 \u00d7 336 as the vi-\nsual prompt, along with the sentence \u201cWhat is in the\npicture?\u201d as the textual prompt. Meanwhile, the patch\nsizes of their vision encoder are fixed to 14. For LLaVA-\nv1.5, the number of input tokens (denoted as Tksin) for the\nlanguage model is composed of 576 visual tokens and some\ntextual tokens. For MobileVLMs, the number of visual to-\nkens has dropped to 144 due to the design of an efficient\nprojector as in Sec. 3.4. For all the tested VLMs, the num-\nber of output tokens (denoted as Tksout) is set to 256. The\ntotal inference time of VLMs is calculated specifically as,\nTotal = LoadLM + (Tksin/Evalprompt)\n+ (Tksout/Sample) + (Tksout/Eval) + Others\n(4)\nwhere Sample, Evalprompt, and Eval are measured in to-\nkens per second. Sample is the time it takes to \u201ctokenize\u201d\n(sample) the prompt message. Evalprompt denotes the time\nLMs take to handle the processed tokens. Eval is the time\nneeded to generate all response tokens, measured only when\nthe LM starts emitting tokens. LoadLM and Total refer to\nthe time of loading model and the entire time consumed by\na single inference respectively. Other overheads are mostly\nnegligible.\nThe inference latency comparison of various VLMs\nis shown in Table 6.\nIt can be found that Eval of\nMobileVLM-336 is slightly slower than MobileLLaMA in\nTable 5 since extra vision tokens increase the time consump-\ntion and consequently slow down the generation of output\ntokens.\nMost importantly, we are surprised to find Total of Mo-\nbileVLM equipped with MobileLLaMa 2.7B and MobileL-\nLaMa 1.4B outperform LLaVA-v1.5 consistently on Snap-\ndragon SoC and Jetson Orin. It is worth noting that although\nEval of MobileVLM (MobileLLaMa 1.4B) is slower than\nthat of LLaVA-v1.5 (OpenLLaMA 1B) on Orin, it achieves\na faster Total inference time.\nThe main reason is that\nTksin of MobileVLM is only a quarter of LLaVA-v1.5 in\nEquation 4 while other variables are comparable. This pre-\ncisely proves the importance of efficient projection that re-\nduces the number of input visual tokens.\n5. Ablation Study\n5.1. Ablation on Vision Backbones\nIn Table 7, we compare the multimodal performance\nat varying model scales and different numbers of visual to-\nkens. All experiments are conducted with CLIP ViT as a\nvisual encoder. We configure different model scales, patch\nsizes, and types of vision-language projectors.\nThe impact of model scales. As the model scales up,\nthe multimodal performance on 6 benchmarks maintains a\n9\nLanguage Model\nVision Encoder\nVL Projector\nTokens\nGQA\nSQAI\nVQAT\nPOPE\nMME\nMMBdev\nMobileLLaMA\n1.4B\nCLIP-B-P16-S224 [97]\nMLP [75]\n196\n55.3\n52.8\n38.3\n83.2\n1079.9\n46.1\nCLIP-L-P14-S224 [97]\nMLP [75]\n256\n55.8\n53.4\n40.9\n83.9\n1104.4\n50.9\nCLIP-G-P14-S224 [110] MLP [75]\n256\n57.5\n54.2\n38.0\n84.9\n1123.3\n53.9\nCLIP-L-P14-S336 [97]\nMLP [75]\n576\n56.9\n53.6\n43.7\n85.7\n1137.7\n52.8\nCLIP-L-P14-S336 [97]\nLDP (ours)\n144\n56.1\n54.7\n41.5\n84.5\n1196.2\n53.2\nMobileLLaMA\n2.7B\nCLIP-B-P16-S224 [97]\nMLP [75]\n196\n57.0\n58.4\n43.1\n83.8\n1212.2\n54.6\nCLIP-L-P16-S224 [97]\nMLP [75]\n256\n57.9\n58.6\n45.3\n85.1\n1285.0\n57.7\nCLIP-G-P14-S224 [110] MLP [75]\n256\n59.5\n58.9\n43.9\n85.1\n1275.6\n59.5\nCLIP-L-P14-S336 [97]\nMLP [75]\n576\n59.1\n58.3\n47.3\n85.8\n1333.1\n57.1\nCLIP-L-P14-S336 [97]\nLDP (ours)\n144\n59.0\n61.0\n47.5\n84.9\n1288.9\n59.6\nTable 7. Comparison with different vision encoder scales (B/L/G: Base, Large, Giant) and visual tokens on six benchmarks with our\nMobileLLaMA 1.4B/2.7B. S224/336 indicates the input resolution.\nVision Encoder\nMethod\nPretrain Paradigm\nPretrain Data\nTokens\nGQA\nSQAI\nVQAT\nPOPE\nMME\nMMBdev\nViT-B-P16-S224\nSAIM [96]\nUnSupervised\nIN1K [33]\n196\n45.3\n40.9\n32.2\n76.5\n845.4\n2.1\nViT-B-P16-S224\nMAE [47]\nUnSupervised\nIN1K [33]\n196\n50.3\n49.1\n33.4\n80.2\n931.2\n24.7\nViT-B-P16-S224\nViT [34]\nClassification\nIN21K [100]\n196\n48.3\n50.7\n33.1\n80.3\n892.8\n34.9\nTwins-SVT-L-S224 Twins [22]\nClassification\nIN1K [33]\n49\n50.3\n50.9\n33.2\n80.6\n941.6\n32.1\nTwins-SVT-L-S384 Twins [22]\nClassification\nIN1K [33]\n144\n51.4\n51.0\n32.8\n81.7\n930.3\n33.4\nSwin-Tiny-S224\nSwin [81]\nClassification\nIN1K [33]\n49\n48.3\n50.3\n32.5\n80.8\n929.0\n31.4\nSwin-Base-S384\nSwin [81]\nClassification\nIN22K [33]\n144\n53.3\n52.2\n33.5\n82.8\n1037.2\n40.3\nSwin-Tiny-S224\nGDino [79] Grounding-Det\nOGC [79]\n49\n51.2\n50.5\n32.4\n81.7\n932.4\n31.8\nSwin-Base-S384\nGDino [79] Grounding-Det\nCOGOOR [79]\n144\n54.9\n51.0\n33.8\n84.5\n1072.3\n40.0\nViT-B-P14-S224\nCLIP [97]\nImage-Text-Align\nWIT [97]\n256\n55.3\n52.8\n38.3\n83.2\n1079.9\n46.1\nTable 8. Comparison with various vision encoders from different pre-training paradigms on MobileLLaMA 1.4B on six benchmarks.\ngradual increase trend under the same projector [75]. How-\never, it can be observed that the gain brought by the visual\nmodel scaling may gradually become saturated at a certain\namount of training data.\nThe impact of the number of visual tokens. Compared\nwith rows 4-5, our proposed LDP module reduces the num-\nber of visual tokens from 576 to 144 (\u2193 75%), and it fi-\nnally achieves performance equivalent to or sometimes bet-\nter than the baseline. This reveals that the quality of visual\ntokens can be further improved while our proposed LDP\nmodule is quite effective.\nThe impact of pre-training paradigms. Furthermore,\nwe show the performance of MobileLLaMA 1.4B under\ndifferent vision backbone pre-training paradigms in Ta-\nble 8. Based on the cost of annotation and pre-training,\nwe roughly classify these paradigms into four categories.\nIt turns out that the performance of MobileVLM gradu-\nally improves as the pre-training cost increases. The vision\nencoder pre-trained with supervised image-text alignment\nachieved the best performance. By comparing Swin-Base-\nS384-GDino and ViT-B-P14-S224, we notice that the model\npre-trained by grounding detection achieved relatively com-\nparable performance to the CLIP pre-trained model on\nGQA, SQA, POPE, and MME. This outcome indicates that\nthe image-level alignment has greater potential to strike bet-\nter performance than object-level, especially by using more\nvisual tokens or more training data. In addition, better Im-\nageNet performance of pre-trained models (e.g., Swin >\nViT) often corresponds to more general visual feature ex-\ntraction capabilities, and MobileVLM will have certain per-\nformance gains in turn.\n5.2. Abaltion on VL Projectors\nMotivated by the fact both feature interaction and to-\nken interaction are beneficial, we utilize depthwise convo-\nlutions for the former and pointwise for the latter. Table 9\nshows the performance of various VL projectors. Row 1\nin Table 9 is the module used in LLaVA [78] where only\nthe feature space is transformed through two linear lay-\ners.\nRow 2 adds a DW (depthwise) convolution before\neach PW(pointwise) for token interaction, which performs\n2\u00d7 downsampling with a stride of 2. We notice that the\nperformance begins to show an evident decline. Based on\nthe setup of 144 tokens, adding two front-end PW layers\n10\nVL Projector Architecture Design\nTokens\nGQA\nSQAI\nVQAT\nPOPE\nMME\nMMBdev\n[PW]\u00d72[DW \u03ba=1PW]\u00d70[DW \u03ba=2PW]\u00d70\n576\n56.9\n53.6\n43.7\n85.7\n1137.7\n52.8\n[PW]\u00d70[DW \u03ba=1PW]\u00d71[DW \u03ba=2PW]\u00d71\n144\n54.9\n52.9\n40.2\n84.0\n1150.8\n50.3\n[PW]\u00d72[DW \u03ba=1PW]\u00d71[DW \u03ba=2PW]\u00d71\n144\n56.1\n54.7\n41.5\n84.5\n1196.2\n53.2\n[PW]\u00d72[DW \u03ba=1PW]\u00d73[DW \u03ba=2PW]\u00d71\n144\n55.3\n53.9\n40.8\n84.6\n1166.3\n53.0\n[PW]\u00d72[DW \u03ba=2PW]\u00d71[DW \u03ba=1PW]\u00d71\n144\n55.6\n54.3\n41.5\n84.6\n1166.2\n52.8\nTable 9. The exploration of projector design based on MobileLLaMA 1.4B. The PW represents pointwise-conv and DW is depthwise-\nconv. The subscript \u00d7 indicates the number of times the corresponding module is stacked repeatedly. The superscript \u03ba indicates the conv\nstride. The grey row is the baseline projector from [74], and green row is the proposed LDP in our MobileVLM.\nLLM base model\nSFT strategy Conversation mode\nGQA\nSQAI\nVQAT\nPOPE\nMME\nMMBdev\nMobileLLaMA 1.4B\nw/o\nllavallama2\n55.8\n52.6\n40.5\n84.4\n1111.5\n52.0\nMobileLLaMA 1.4B\nw/o\nvicunav1\n56.1\n53.0\n40.5\n84.6\n1148.5\n50.3\nMobileLLaMA 1.4B\nAlpaca\nllavallama2\n55.2\n54.8\n40.6\n84.9\n1171.1\n51.9\nMobileLLaMA 1.4B\nAlpaca\nvicunav1\n55.5\n53.1\n40.6\n83.8\n1168.0\n47.7\nMobileLLaMA 1.4B\nVicuna\nvicunav1\n56.1\n54.7\n41.5\n84.5\n1196.2\n53.2\nTable 10. Quantitative analysis on SFT of MobileLLaMA 1.4B in downstream tasks.\nbrings more feature-level interactions, which makes up for\nthe performance loss caused by token reduction. Rows 4\nand 5 show that adding more parameters does not achieve\ndesired gains. Rows 4 and 6 show that the downsampling\nof tokens at the end of the projector has a positive effect.\n5.3. Visual Resolution and Token Numbers\nSince the number of visual tokens directly affects the in-\nference speed of the whole multimodal model, we compare\ntwo types of design: reducing the input resolution (RIR) and\nusing a lightweight downsample projector (LDP). Without\nloss of generality, for an image of H \u00d7 W with a patch size\nof P, the former strategy generates HW/P 2 tokens. For\nthe latter, it produces HW/4P 2 tokens using a downsam-\npling ratio of 2. We use H = W = 336, P = 14 for LDP\nand H = W = 168, P = 14 for RIR to keep the total num-\nber of tokens as 144. The result from Table 11 verifies the\neffectiveness of the proposed LDP.\nMethod GQA SQAI VQAT POPE MME MMBdev\nLDP\n56.1\n54.7\n41.5\n84.5\n1196.2\n53.2\nRIR\n53.9\n53.1\n37.1\n81.5\n1072.5\n46.7\nTable 11. Token reduction design on MobileVLM 1.7B.\n5.4. Quantitative Analysis on SFT\nVicuna [132], fine-tuned on LLaMA, has been widely\nchosen in large multimodal models [30,75,133]. We further\nexplore how SFT affects our language model\u2019s performance\nin downstream tasks. Two common SFT paradigms Alpaca\n[111] and Vicuna [132] are compared in Table 10. We find\nthat the scores of SQA, VQA, MME, and MMBench can all\nbe significantly enhanced. It demonstrates that fine-tuning\nlarge language models in Vicuna dialogue mode [132] with\nthe data from ShareGPT ultimately achieves the best per-\nformance. To better integrate SFT\u2019s prompt format with the\ntraining of downstream tasks, we ablate the conversation\nmode on MobileVLM to find vicunav1 performs best.\n6. Conclusion\nIn a nutshell, we present MobileVLM, a set of efficient\nand high-powered mobile scale vision language models tai-\nlored for mobile and IoT devices. In its making, we re-\nfurbish both language models and vision projection mod-\nules. Extensive experiments are conducted to choose proper\nvision backbones, to design an efficient projector, and to\nenhance model capacity by training schemes like language\nmodel SFT, a two-stage training strategy involving pretrain-\ning and instruction tuning, and LoRA fine-tuning.\nThe\nperformance is evaluated vigorously on mainstream VLM\nbenchmarks.\nMobileVLMs also show an unprecedented\nspeed on typical mobile and IoT devices. We believe that\nMobileVLM will open up new possibilities for widespread\napplications like multimodal assistants deployed on mobile\ndevices or in self-driving cars, and more broadly embodied\nAI robots.\nAcknowledgements:\nThis work was in part supported by\nNational Key R&D Program of China (No. 2022ZD0118-\n700).\nThe authors thank Yerui Sun for infrastructure support.\n11\nReferences\n[1] Syeda Nahida Akter,\nZichun Yu,\nAashiq Muhamed,\nTianyue Ou, Alex B\u00a8auerle, \u00b4Angel Alexander Cabrera, Kr-\nish Dholakia, Chenyan Xiong, and Graham Neubig. An\nin-depth look at gemini\u2019s language abilities. arXiv preprint\narXiv:2312.11444, 2023. 1\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learn-\ning. Advances in Neural Information Processing Systems,\n35:23716\u201323736, 2022. 1, 3\n[3] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hes-\nsel,\nYusuf Hanafy,\nWanrong Zhu,\nKalyani Marathe,\nYonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Korn-\nblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman,\nand Ludwig Schmidt. Openflamingo, Mar. 2023. 7\n[4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xi-\naodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\nBinyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Day-\niheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin\nMa, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi\nTan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei\nWang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,\nHao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen\nYu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan\nZhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-\ngren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen tech-\nnical report. arXiv preprint arXiv:2309.16609, 2023. 2\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond. arXiv\npreprint arXiv:2308.12966, 2023. 1, 2, 3, 7\n[6] Hangbo Bao,\nWenhui Wang,\nLi Dong,\nQiang Liu,\nOwais Khan Mohammed, Kriti Aggarwal, Subhojit Som,\nSonghao Piao, and Furu Wei.\nVlmo:\nUnified vision-\nlanguage pre-training with mixture-of-modality-experts.\nAdvances in Neural Information Processing Systems,\n35:32897\u201332912, 2022. 2, 3\n[7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory An-\nthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al.\nPythia: A suite for ana-\nlyzing large language models across training and scaling.\nIn International Conference on Machine Learning, pages\n2397\u20132430. PMLR, 2023. 2, 6\n[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. Piqa: Reasoning about physical commonsense in nat-\nural language. In Proceedings of the AAAI conference on\nartificial intelligence, volume 34, pages 7432\u20137439, 2020.\n7\n[9] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella\nBiderman.\nGPT-Neo: Large Scale Autoregressive Lan-\nguage Modeling with Mesh-Tensorflow, Mar. 2021. If you\nuse this software, please cite it using these metadata. 2\n[10] Michael Boratko, Harshit Padigela, Divyendra Mikkili-\nneni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum,\nMaria Chang, Achille Fokoue-Nkoutche, Pavan Kapani-\npathi, Nicholas Mattei, et al. A systematic classification of\nknowledge, reasoning, and context within the ARC dataset.\narXiv preprint arXiv:1806.00358, 2018. 7\n[11] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun\nLee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m:\nImage-text pair dataset. https://github.com/kak\naobrain/coyo-dataset, 2022. 3\n[12] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and\nSong Han. Once for all: Train one network and specialize\nit for efficient deployment. In International Conference on\nLearning Representations, 2020. 4\n[13] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts.\nIn\nCVPR, 2021. 3\n[14] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu,\nPengchuan Zhang,\nRaghuraman Krishnamoorthi,\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\nMinigpt-v2: large language model as a unified interface\nfor vision-language multi-task learning.\narXiv preprint\narXiv:2310.09478, 2023. 7\n[15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 3, 7\n[16] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 2, 3\n[17] Shouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. Extending context window of large lan-\nguage models via positional interpolation. arXiv preprint\narXiv:2306.15595, 2023. 4\n[18] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-\neri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael\nTschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\nBo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin\nRitter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\nAustin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas\nBeyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,\nYang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-\nseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. PaLI-X: On scaling up a multilingual vision\nand language model. 2023. 2\n[19] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A\njointly-scaled multilingual language-image model. arXiv\npreprint arXiv:2209.06794, 2022. 2\n[20] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\n12\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality, March 2023. 5, 6\n[21] Xiangxiang Chu, Liang Li, and Bo Zhang. Make repvgg\ngreater again: A quantization-aware approach. In AAAI,\n2023. 5\n[22] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers. In Adv. Neural Inform. Process. Syst., 2021.\n2, 10\n[23] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and\nChunhua Shen. Conditional positional encodings for vision\ntransformers. In The Eleventh International Conference on\nLearning Representations, 2023. 4\n[24] Xiangxiang Chu, Bo Zhang, and Ruijun Xu. Fairnas: Re-\nthinking evaluation fairness of weight sharing neural archi-\ntecture search. In Proceedings of the IEEE/CVF Interna-\ntional Conference on computer vision, pages 12239\u201312248,\n2021. 4\n[25] Xiangxiang Chu, Tianbao Zhou, Bo Zhang, and Jixiang Li.\nFair darts: Eliminating unfair advantages in differentiable\narchitecture search. In European conference on computer\nvision, pages 465\u2013480. Springer, 2020. 4\n[26] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 1, 2\n[27] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom\nKwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolq: Exploring the surprising dsifficulty of natural yes/no\nquestions. arXiv preprint arXiv:1905.10044, 2019. 7\n[28] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\nThink you have solved question answering? try arc, the\nai2 reasoning challenge. arXiv preprint arXiv:1803.05457,\n2018. 7\n[29] Together Computer. Redpajama: An open source recipe to\nreproduce llama training dataset, 2023. 5, 6\n[30] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip: Towards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500, 2023. 1, 3, 7, 11\n[31] Tri Dao.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning.\narXiv preprint\narXiv:2307.08691, 2023. 5\n[32] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan\nLee, Devi Parikh, and Dhruv Batra.\nEmbodied question\nanswering. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 1\u201310, 2018. 3\n[33] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248\u2013255. Ieee, 2009. 10\n[34] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 4, 10\n[35] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling. arXiv\npreprint arXiv:2103.10360, 2021. 2\n[36] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and\nCheston Tan. A survey of embodied ai: From simulators to\nresearch tasks. IEEE Transactions on Emerging Topics in\nComputational Intelligence, 6(2):230\u2013244, 2022. 3\n[37] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,\nXinggang Wang, Tiejun Huang, Xinlong Wang, and Yue\nCao. Eva: Exploring the limits of masked visual represen-\ntation learning at scale. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 19358\u201319369, 2023. 2\n[38] Elias Frantar and Dan Alistarh. Sparsegpt: Massive lan-\nguage models can be accurately pruned in one-shot, 2023.\n3\n[39] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan\nAlistarh.\nGptq:\nAccurate post-training quantization\nfor generative pre-trained transformers.\narXiv preprint\narXiv:2210.17323, 2022. 3\n[40] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke\nLi, Xing Sun, et al. Mme: A comprehensive evaluation\nbenchmark for multimodal large language models. arXiv\npreprint arXiv:2306.13394, 2023. 3, 7\n[41] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang\nShen, Zhang Mengdan, Peixian Chen, Sirui Zhao, Shao-\nhui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-\nsheng Li, and Xing Sun. A challenger to gpt-4v? early\nexplorations of gemini in visual expertise. arXiv preprint\narXiv:2312.12436, 2023. 1\n[42] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An-\nthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\nHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang,\nLaria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin\nWang, and Andy Zou. A framework for few-shot language\nmodel evaluation, Sept. 2021. 7\n[43] Xinyang Geng and Hao Liu. Openllama: An open repro-\nduction of llama, May 2023. 2, 6\n[44] Georgi Gerganov. llama.cpp. https://github.com\n/ggerganov/llama.cpp. [Accessed: 2023-11-07]. 3,\n8\n[45] Google. Gemini: A family of highly capable multimodal\nmodels. 2023. 1\n[46] Suriya\nGunasekar,\nYi\nZhang,\nJyoti\nAneja,\nCaio\nC\u00b4esar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de\nRosa, Olli Saarikivi, et al.\nTextbooks are all you need.\narXiv preprint arXiv:2306.11644, 2023. 2\n[47] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scal-\n13\nable vision learners. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n16000\u201316009, 2022. 2, 10\n[48] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-\nsuring massive multitask language understanding.\narXiv\npreprint arXiv:2009.03300, 2020. 7\n[49] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, et al. Training compute-optimal large language mod-\nels. arXiv preprint arXiv:2203.15556, 2022. 1\n[50] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 1314\u20131324, 2019. 4\n[51] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 1, 8\n[52] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6700\u20136709, 2019. 3, 7\n[53] Huggingface. https://huggingface.co/datas\nets/Aeala/ShareGPT_Vicuna_unfiltered. 6\n[54] Gabriel Ilharco, Mitchell Wortsman, Ross Rollman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip. July 2021, 2021. If you use this software, please cite\nit as below. 3\n[55] InternLM. Lmdeploy. https://github.com/Inter\nnLM/lmdeploy. [Accessed: 2023-11-07]. 3\n[56] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International conference on machine learn-\ning, pages 448\u2013456. pmlr, 2015. 5\n[57] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\nAndrew Zisserman, and Joao Carreira. Perceiver: General\nperception with iterative attention. In International confer-\nence on machine learning, pages 4651\u20134664. PMLR, 2021.\n3\n[58] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,\nand Tom Duerig.\nScaling up visual and vision-language\nrepresentation learning with noisy text supervision. In In-\nternational conference on machine learning, pages 4904\u2013\n4916. PMLR, 2021. 3\n[59] Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun\nShi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens\nmatter: Token labeling for training better vision transform-\ners. Advances in neural information processing systems,\n34:18590\u201318602, 2021. 2, 3\n[60] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361,\n2020. 5\n[61] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg. Referitgame: Referring to objects in pho-\ntographs of natural scenes. pages 787\u2013798, 2014. 3\n[62] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual Genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. Int. J. Comput. Vis., 123:32\u201373, 2017.\n3\n[63] Taku Kudo and John Richardson.\nSentencepiece:\nA\nsimple and language independent subword tokenizer and\ndetokenizer for neural text processing.\narXiv preprint\narXiv:1808.06226, 2018. 2, 4\n[64] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M Rush, Douwe Kiela,\net al.\nObelisc:\nAn open web-scale filtered dataset\nof interleaved image-text documents.\narXiv preprint\narXiv:2306.16527, 2023. 7\n[65] Hugo Laurenc\u00b8on, Lucile Saulnier, Thomas Wang, Christo-\npher Akiki, Albert Villanova del Moral, Teven Le Scao, Le-\nandro Von Werra, Chenghao Mou, Eduardo Gonz\u00b4alez Pon-\nferrada, Huu Nguyen, et al. The BigScience corpus: A 1.6\nTB composite multilingual dataset. 2022. 2\n[66] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv\npreprint arXiv:2301.12597, 2023. 1, 2, 3, 4, 7\n[67] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation.\nIn\nInternational Conference on Machine Learning, pages\n12888\u201312900. PMLR, 2022. 3\n[68] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu.\nNorm tweaking: High-performance low-bit quantization of\nlarge language models. In AAAI, 2023. 3\n[69] Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli\nCelikyilmaz, Jianfeng Gao, Noah Smith, and Yejin Choi.\nRobust navigation with language pretraining and stochastic\nsampling. arXiv preprint arXiv:1909.02244, 2019. 3\n[70] Yuanzhi Li, S\u00b4ebastien Bubeck, Ronen Eldan, Allie Del\nGiorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are\nall you need ii: phi-1.5 technical report, 2023. 2\n[71] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 3, 7\n[72] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa:\nMeasuring how models mimic human falsehoods.\narXiv\npreprint arXiv:2109.07958, 2021. 7\n[73] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft COCO: Common objects\nin context.\nIn Eur. Conf. Comput. Vis., pages 740\u2013755.\nSpringer, 2014. 3\n14\n[74] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. arXiv\npreprint arXiv:2310.03744, 2023. 1, 3, 4, 7, 11\n[75] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee.\nImproved baselines with visual instruction tuning.\narXiv:2310.03744, 2023. 10, 11\n[76] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee.\nVisual\ninstruction\ntuning.\narXiv\npreprint\narXiv:2304.08485, 2023. 1, 3, 4, 6, 7\n[77] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS:\nDifferentiable architecture search. In International Confer-\nence on Learning Representations, 2019. 4\n[78] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li,\nTianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu,\net al. Llava-plus: Learning to use tools for creating multi-\nmodal agents. arXiv preprint arXiv:2311.05437, 2023. 1,\n10\n[79] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv preprint\narXiv:2303.05499, 2023. 10\n[80] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, et al. Mmbench: Is your multi-\nmodal model an all-around player?\narXiv preprint\narXiv:2307.06281, 2023. 3, 7\n[81] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021. 2, 10\n[82] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[83] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning\nvia thought chains for science question answering. In Ad-\nvances in Neural Information Processing Systems, pages\n27730\u201327744, 2022. 3, 7\n[84] Xinyin Ma, Gongfan Fang, and Xinchao Wang.\nLlm-\npruner: On the structural pruning of large language models,\n2023. 3\n[85] Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Rus-\nsakovsky. Point and ask: Incorporating pointing into vi-\nsual question answering. arXiv preprint arXiv:2011.13681,\n2020. 3\n[86] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang,\nMingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao,\nand Ping Luo.\nEmbodiedgpt:\nVision-language pre-\ntraining via embodied chain of thought.\narXiv preprint\narXiv:2305.15021, 2023. 3\n[87] NVIDIA. Tensorrt-llm. https://github.com/NVI\nDIA/TensorRT-LLM. [Accessed: 2023-11-07]. 3\n[88] OpenAI.\nChatGPT.\nhttps://openai.com/blog/ChatGPT/,\n2023. Online; accessed 2023-01-01. 3\n[89] OpenAI. Gpt-4 technical report. 2023. Technical Report.\n1, 2\n[90] OpenAI. Gpt-4v(ision) system card. 2023. 1, 2\n[91] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. Advances in neural information processing sys-\ntems, 24, 2011. 3\n[92] Alexander Pashevich, Cordelia Schmid, and Chen Sun.\nEpisodic transformer for vision-and-language navigation.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 15942\u201315952, 2021. 3\n[93] Tianduo Wang Peiyuan Zhang, Guangtao Zeng and Wei Lu.\nTinyllama, Sep 2023. 6\n[94] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 3\n[95] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2641\u20132649, 2015. 3\n[96] Yu Qi, Fan Yang, Yousong Zhu, Yufei Liu, Liwei Wu, Rui\nZhao, and Wei Li. Exploring stochastic autoregressive im-\nage modeling for visual representation. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 37,\npages 2074\u20132081, 2023. 10\n[97] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 2, 3, 4, 10\n[98] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and\nYuxiong He. Zero: Memory optimizations toward training\ntrillion parameter models. In SC20: International Confer-\nence for High Performance Computing, Networking, Stor-\nage and Analysis, pages 1\u201316. IEEE, 2020. 5\n[99] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He.\nDeepspeed: System optimizations enable\ntraining deep learning models with over 100 billion param-\neters. In Proceedings of the 26th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Min-\ning, pages 3505\u20133506, 2020. 5\n[100] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi\nZelnik-Manor.\nImagenet-21k pretraining for the masses.\narXiv preprint arXiv:2104.10972, 2021. 10\n[101] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,\nand Yejin Choi.\nWinogrande: An adversarial winograd\nschema challenge at scale. Communications of the ACM,\n64(9):99\u2013106, 2021. 7\n[102] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki.\nLaion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. arXiv preprint arXiv:2111.02114, 2021. 3\n[103] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\n15\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018. 3\n[104] Noam Shazeer. Glu variants improve transformer. arXiv\npreprint arXiv:2002.05202, 2020. 4\n[105] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 8317\u20138326, 2019. 3, 7\n[106] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M\nSadler, Wei-Lun Chao, and Yu Su.\nLlm-planner: Few-\nshot grounded planning for embodied agents with large lan-\nguage models. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 2998\u20133009,\n2023. 3\n[107] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen\nBo, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding.\nNeurocomputing, page\n127063, 2023. 4\n[108] Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fer-\ngus, and Ishita Dasgupta. Distilling internet-scale vision-\nlanguage models into embodied agents.\narXiv preprint\narXiv:2301.12507, 2023. 3\n[109] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A\nsimple and effective pruning approach for large language\nmodels. arXiv preprint arXiv:2306.11695, 2023. 3\n[110] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao.\nEva-clip: Improved training techniques for clip at\nscale. arXiv preprint arXiv:2303.15389, 2023. 10\n[111] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\nHashimoto.\nStanford alpaca:\nAn instruction-following\nllama model. https://github.com/tatsu-lab/stanford alpaca,\n2023. 11\n[112] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew Poul-\nton, Viktor Kerkez, and Robert Stojnic. Galactica: A large\nlanguage model for science. 2022. 2, 6\n[113] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities. https://gi\nthub.com/InternLM/InternLM, 2023. 6\n[114] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers and distillation through\nattention. In International Conference on Machine Learn-\ning, volume 139, pages 10347\u201310357, July 2021. 2\n[115] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, and Faisal\nAzhar.\nLlama: Open and efficient foundation language\nmodels. 2023. 1, 2, 4, 5\n[116] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023. 2, 4, 5, 6\n[117] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu,\nOncel Tuzel, and Anurag Ranjan.\nMobileone: An im-\nproved one millisecond mobile backbone. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 7907\u20137917, 2023. 5\n[118] Vicuna. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. https://vicuna.lmsys.org/, 2023.\n1, 2, 7\n[119] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong,\nWenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Sak-\nsham Singhal, Alon Benhaim, et al. Foundation transform-\ners. arXiv preprint arXiv:2210.06423, 2022. 3\n[120] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning. arXiv preprint\narXiv:2311.07574, 2023. 2\n[121] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions.\nIn Proceedings of the\nIEEE/CVF international conference on computer vision,\npages 568\u2013578, 2021. 2\n[122] Fei Wei, Xinyu Zhang, Ailing Zhang, Bo Zhang, and Xi-\nangxiang Chu. Lenna: Language enhanced reasoning de-\ntection assistant. arXiv preprint arXiv:2312.02433, 2023.\n1\n[123] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Bar-\nret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, et al.\nEmer-\ngent abilities of large language models.\narXiv preprint\narXiv:2206.07682, 2022. 2\n[124] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien\nDemouth, and Song Han. Smoothquant: Accurate and effi-\ncient post-training quantization for large language models.\nIn International Conference on Machine Learning, pages\n38087\u201338099. PMLR, 2023. 3\n[125] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and\nYuxiong He. Zeroquant-v2: Exploring post-training quan-\ntization in llms from comprehensive study to low rank com-\npensation, 2023. 3\n[126] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 1, 3, 6, 7\n[127] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTong Xu, and Enhong Chen. A survey on multimodal large\nlanguage models. arXiv preprint arXiv:2306.13549, 2023.\n1\n[128] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,\nand Yejin Choi. Hellaswag: Can a machine really finish\nyour sentence? arXiv preprint arXiv:1905.07830, 2019. 7\n[129] Biao Zhang and Rico Sennrich. Root mean square layer\nnormalization. Advances in Neural Information Processing\nSystems, 32, 2019. 4\n[130] Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yun-\nsen Xian, Benyou Wang, and Dawei Song. Lifting the curse\nof capacity gap in distilling language models, 2023. 3\n16\n[131] Susan Zhang,\nStephen Roller,\nNaman Goyal,\nMikel\nArtetxe, Moya Chen, Christopher Dewan, Mona Diab, Xian\nLi, Xi Victoria Lin, et al. OPT: Open pre-trained trans-\nformer language models. 2022. 1, 2, 6\n[132] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-\nhan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-\njudge with mt-bench and chatbot arena.\narXiv preprint\narXiv:2306.05685, 2023. 2, 11\n[133] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1, 3, 4, 7, 11\n17\nA. Dataset\nA.1. Dataset for MobileLLaMA\nTable 12 shows the sampling strategy for the training set\nof MobileLLaMA.\nDataset\nProportion\nArxiv\n2.5%\nBook\n4.5%\nC4\n15.0%\nCommon Crawl\n67.7%\nGithub\n4.5%\nStackExchange\n2.0%\nWikipedia\n4.5%\nTable 12. Data sample strategy for MobileLLaMA training.\nB. SFT prompt and response example\nWe present our prompt template of MobileLLaMA SFT\nin Table 14.\nC. VLM demos\nFigure 6 and Figure 7 showcase MobileVLM\u2019s perfor-\nmance on each benchmark, including the scenes on attribute\nunderstanding (GQA left), spatial and relational reasoning\n(GQA right, MMBench), social science (ScienceQA left),\nnatural science (ScienceQA right), OCR (TextVQA), object\nrecognition (POPE), word knowledge (MME right), etc.\n18\nHuman:\nwho are you?\nMobileLLaMA 1.4B Base:\nI\u2019m the AI.\nMobileLLaMA 1.4B SFT:\nI am a digital asset management and metadata expert.\nMy expertise includes creating and managing digital\ncontent, metadata, and digital asset management\nsolutions.\nI have a passion for helping organizations\nunlock the value of their digital assets and improve\ntheir workflows.\nI am also experienced in working\nwith big data and analytics and can help organizations\nunlock insights from their data to drive better\ndecision-making.\nI am always up for a challenge and\nlove working with clients to help them achieve their\ngoals.\nHuman:\nwhat can you do?\nMobileLLaMA 1.4B Base:\nI\u2019m a chatbot.\nMobileLLaMA 1.4B SFT:\nIn addition to providing expert advice and guidance on a\nwide range of topics, I can also help you with tasks and\nprojects that are more complex or require specialized\nknowledge, such as developing and implementing project\nplans or managing budget and resource allocation.\nOverall, my role is to provide value and support to your\nproject or task and to help you achieve your goals in a\ntimely and efficient manner.\nTable 13. Examples of the response of the MobileLLaMA 1.4B Base and SFT model\n19\nSystem\nA chat between a curious user and an artificial intelligence\nassistant.\nThe assistant gives helpful, detailed, and polite\nanswers to the user\u2019s questions.\nUSER\nI want you to act as my travel agent for preparing an itinerary\nfor travel to Munnar and Tekkady in Kerala.\nI have already booked\nflights from Hyderabad to Kochi for an onward journey on 25th Jan\nand a return journey on 28th Jan.\nWe are a group of 4 men and\nplanning to stay 2 days in Munnar and 1 day in Tekkady.\nI want\nyou to help me with accommodation booking and activities.\nASSISTANT\nI\u2019d be happy to help you plan your trip to Munnar and Tekkady in\nKerala!\nHere is a suggested itinerary for your trip:\n\\n\\nDay 1:\nArrival in Kochi - Travel to Munnar (around 4 hours)\\n\\n* Check-in\nat a hotel in Munnar\\n* Afternoon visit to the tea plantations\nand factories\\n* Evening at leisure\\n\\nDay 2:\nMunnar\\n\\n*\nEarly morning visit to the Eravikulam National Park to see the\nendangered Nilgiri Tahr\\n* Afternoon visit to the Mattupetty Dam\nand the Kundala Dam\\n* Evening at leisure\\n\\nDay 3:\nMunnar to\nTekkady (around 4 hours)\\n\\n* Check-in at a hotel in Tekkady\\n*\nAfternoon visit to the Periyar Wildlife Sanctuary for a boat ride\nand wildlife viewing\\n* Evening at leisure\\n\\nDay 4:\nTekkady\n- Return to Kochi\\n\\n* Check-out and travel back to Kochi\\n*\nFlight back to Hyderabad\\n\\nIn terms of accommodation, there are\nmany options available in Munnar and Tekkady.\nYou can choose\nfrom budget-friendly options to luxury resorts.\nYou can check\nthe availability and prices on hotel booking websites like\nBooking.com, Agoda, Expedia, etc.\\n\\nPlease let me know if you\nhave any other questions or if you need further assistance with\nyour trip planning.</s>\nTable 14. An example prompt and annotation of our SFT data.\n20\n<image> Who is the author of this book?\nSusan Wise Bauer\nIs this book related to Education & Teaching?\nYes, this book is related to Education & Teaching, as it is a comprehensive history of the \nClassical Child, which is a subject that is often taught in schools. The book provides a detailed \naccount of the history of the Classical Child, including its origins, development, and cultural \ninfluences.\nUser\nMobileVLM\nUser\n<image> What color is the jet airliner in the image?\nThe jet airliner in the image is white.\nWhat does it look like the plane is doing?\nThe plane is flying through the sky, and it appears to be in the process of taking off.\nUser\nUser\n<image> What could be the purpose of the gathering in this setting?\nThe purpose of the gathering in this setting could be a sheep show or a livestock event, where \npeople are observing and interacting with the sheep. In the image, there are several sheep in a \npen, and a woman is holding a cell phone, which suggests that she might be capturing images \nor recording videos of the animals. The presence of a bench and a dining table in the area also \nindicates that this could be a place where people gather to socialize, eat, or simply enjoy the \nsight of the sheep. The event could be a part of a larger agricultural show or a community \ngathering where people come together to learn about and appreciate the animals and their \ncaretakers.\nUser\n<image> What can you infer about the skateboarder's skill level?\nBased on the image, it can be inferred that the skateboarder has a relatively high skill level. \nThe young man is performing a trick on his skateboard, jumping over a small obstacle, which \nrequires a certain level of expertise and balance. This suggests that he has practiced and \nmastered the necessary skills to execute such a maneuver. The fact that he is able to perform \nsuch a trick in a public area, such as a sidewalk, further indicates that he has a certain level of \nconfidence and experience in skateboarding.\nUser\n<image> What breed does the dog appear to be?\nThe dog appears to be a Golden Retriever.\nWhy might a dog chew on a stick or a toothbrush?\nDogs may chew on toothbrushes for several reasons, such as seeking comfort, stimulation, or to \nrelieve boredom. Chewing on a toothbrush can also help dogs clean their teeth and maintain \ngood oral hygiene. Additionally, dogs may chew on toothbrushes as a form of play or as a way to \nexpress their natural instincts and curiosity. \nUser\nUser\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nFigure 5. Examples of MobileVLM on VLM training datasets.\n21\nGQA\nScienceQA\nTextVQA\nWhat is beneath the zebra that is not large?\nGrass\nUser\nWhat is sprinkled on the cupcakes that are next \nto the carrots?\nSprinkles\nUser\nB\nUser\nWhich continent is highlighted?\nA. Africa\nB. South America\nC. North America\nD. Asia\nA\nUser\nContext: Two magnets are placed as shown.\nWill these magnets attract or repel each other?\nA. repel\nB. attract\n2011\nUser\nWhich year did this happen?\nGreat hall ceiling model\nUser\nWhat is the title of this picture?\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nFigure 6. Examples of MobileVLM on GQA, ScienceQA, and TextVQA Benchmarks.\n22\nPOPE\nMME\nMMBench\nNo\nUser\nIs the word in the logo \"dress\"?\nYes\nUser\nIs this a photo of Visingsborg?\nIs there a person in the image?\nYes\nUser\nIs there a bird in the image?\nNo\nUser\nB\nUser\nA\nUser\nWhat will happen next?\nA. the wave is gonna hit the two girls\nB. the wave is gonna go back to the sea\nC. the two girls are gonna swim in the wave\nD. both A,B, and C\nWhich one is the correct caption of this image?\nA. A brown teddy bear is laying on a bed.\nB. A giraffe lying on the ground in a zoo pin. \nC. Two men and a dog in a kitchen.\nD. A cat standing on the edge of a toilet bowl \nwith its front paws inside of the toilet.\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nMobileVLM\nFigure 7. Examples of MobileVLM on POPE, MME and MMBench Benchmarks.\n23\n"
  },
  {
    "title": "DreamGaussian4D: Generative 4D Gaussian Splatting",
    "link": "https://arxiv.org/pdf/2312.17142.pdf",
    "upvote": "15",
    "text": "DreamGaussian4D: Generative 4D Gaussian Splatting\nJiawei Ren * 1 Liang Pan * 2 Jiaxiang Tang 1 3 Chi Zhang 1 Ang Cao 4 Gang Zeng 3 Ziwei Liu 1\nhttps://jiawei-ren.github.io/projects/dreamgaussian4d\n0s\n1min\n4min30s\nGenerated 4D Model\nInput\nOptimization Progress\nComposited Scene\nFigure 1: DreamGaussian4D generates 4D contents in minutes by leveraging 4D Gaussian Splatting. Exported meshes can\nbe efficiently composited and rendered in 3D engines.\nAbstract\nRemarkable progress has been made in 4D con-\ntent generation recently. However, existing meth-\nods suffer from long optimization time, lack of\nmotion controllability, and a low level of detail.\nIn this paper, we introduce DreamGaussian4D, an\nefficient 4D generation framework that builds on\n4D Gaussian Splatting representation. Our key in-\nsight is that the explicit modeling of spatial trans-\nformations in Gaussian Splatting makes it more\nsuitable for the 4D generation setting compared\nwith implicit representations. DreamGaussian4D\nreduces the optimization time from several hours\n1S-Lab, Nanyang Technological University 2Shanghai AI Lab-\noratory 3Peking University 4University of Michigan. Correspon-\ndence to: Ziwei Liu <ziwei.liu@ntu.edu.sg>.\nTechnical Report.\nTable 1: Speed Comparison. DreamGaussian4D reduces\nthe optimization time from several hours to several minutes.\nMethod\nTime\nIterations\nMAV3D (Singer et al., 2023)\n6.5 hr\n12k\nAnimate124 (Zhao et al., 2023)\n-\n20k\nConsistent4D (Jiang et al., 2023)\n2.5 hr\n10k\n4D-fy (Bahmani et al., 2023)\n23 hr\n120k\nDream-in-4D (Zheng et al., 2023)\n10.5 hr\n20k\nAYG (Ling et al., 2023)\n-\n20k\nDreamGaussian4D (Ours)\n6.5 min\n0.7k\nto just a few minutes, allows flexible control of\nthe generated 3D motion, and produces animated\nmeshes that can be efficiently rendered in 3D\nengines. Code is available at github.com/jiawei-\nren/dreamgaussian4d.\n1\narXiv:2312.17142v2  [cs.CV]  29 Dec 2023\nDreamGaussian4D\n1. Introduction\nRemarkable progress has been witnessed in generative mod-\nels, demonstrating significant recent advancements and in-\nnovations in generating diverse digital content, such as\n2D images (Rombach et al., 2022; Sheynin et al., 2022),\nvideos (Wang et al., 2023; Blattmann et al., 2023), and 3D\nscenes (Jun & Nichol, 2023; Hong et al., 2023; Tang et al.,\n2023a). While a few recent research works (Singer et al.,\n2023; Jiang et al., 2023; Zhao et al., 2023; Bahmani et al.,\n2023) have been devoted to 4D generation, achieving con-\nsistency and high quality in the generation of dynamic 4D\nscenes is far from being fully resolved.\n4D dynamic scenes are often represented by using 4D dy-\nnamic Neural Radiance Fields (NeRF), which are expected\nto show consistent appearance, geometry, and motions from\narbitrary viewpoints. By combining the benefits of video\nand 3D generative models, MAV3D (Singer et al., 2023)\nachieves text-to-4D generation by distilling text-to-video\ndiffusion models on a Hexplane (Cao & Johnson, 2023).\nConsistent4D (Jiang et al., 2023) introduces a video-to-4D\nframework to optimize a Cascaded DyNeRF for 4D gen-\neration from a statically captured monocular video. With\nmultiple diffusion priors, Animate124 (Zhao et al., 2023)\ncould animate a single in-the-wild image into 3D videos\nthrough textural motion descriptions. Using a hybrid SDS,\n4D-fy (Bahmani et al., 2023) achieves compelling text-to-\n4D generation based on multiple pre-trained diffusion mod-\nels. However, they (Singer et al., 2023; Jiang et al., 2023;\nZhao et al., 2023; Bahmani et al., 2023) tend to take hours\nto generate a 4D NeRF, and also their generated motions\ncould not be well-controllable.\nIn this work, we introduce the DreamGaussian4D frame-\nwork, which could efficiently generate dynamic scenes with\n4D Gaussian splatting (4D GS) in just a few minutes. Our\nkey insight is that the explicit modeling of spatial transfor-\nmation in Gaussian Spaltting significantly simplifies the dy-\nnamic optimization in 4D generation. We first fit a static 3D\nGaussian Splatting (3D GS) using the image-to-3D frame-\nworks introduced in DreamGaussian (Tang et al., 2023a).\nWe propose a better training recipe, DreamGaussianHD, that\nalleviates the under-optimization problem in DreamGaus-\nsian. For the dynamic optimization, we learn the motion\nfrom a driving video in replace of the commonly used score\ndistillation from video diffusion models. The driving video\nis generated from image-to-video diffusion models and en-\nables better controllability and motion diversity. Finally, we\nexport the 4D GS to an animated mesh sequence and refine\nthe per-frame texture map with a video-to-video pipeline\nthat improves temporal coherence.\nIn summary, our contributions are as follows:\n1. We employ the deformable Gaussian Splatting repre-\nsentation in 4D content generation, and observe that its\nexplicit spatial transformation modeling significantly\nreduces the optimization time from several hours to\njust a few minutes.\n2. We design an image-to-4D framework that learns mo-\ntion from image-conditioned generated videos, allow-\ning more controllable and diverse 3D motions.\n3. We propose a video-to-video texture refinement strat-\negy that further enhances the quality of exported ani-\nmated meshes, making the framework easier to deploy\nin a real-world setting.\n2. Related works\n2.1. 4D Representations\nThrilling progress has been observed in representing dy-\nnamic 3D scenes (4D scenes). One line of research directly\nrepresents 4D scenes as a function of x, y, z with additional\ntime dimension t or latent code (Xian et al., 2021; Gao et al.,\n2021; Li et al., 2022; 2021). Another line of work repre-\nsents 4D scenes as the combination of deformation fields\nwith static canonical 3D scenes (Pumarola et al., 2021; Park\net al., 2021a;b; Du et al., 2021; Tretschk et al., 2021; Yuan\net al., 2021; Li et al., 2023). A crucial bottleneck of 4D\nrepresentation is its speed, which takes dozens of hours\nfor a single scene. Numerous approaches are explored to\nsolve this problem. Notably, using explicit or hybrid repre-\nsentations archives impressive results, including planar de-\ncomposition for 4D space-time grid (Cao & Johnson, 2023;\nFridovich-Keil et al., 2023; Shao et al., 2023), hash repre-\nsentation (Turki et al., 2023), and other structures (Fang\net al., 2022; Abou-Chakra et al., 2024; Guan et al., 2022).\nVery recently, Gaussian Splatting (Kerbl et al., 2023) has\ndrawn significant attention as it provides both satisfactory\nspeed and impressive reconstruction quality. Extending\nstatic Gaussian Spatting into dynamic versions becomes a\npromising direction. Dynamic 3D Gaussians (Luiten et al.,\n2023) optimize per-frame Gaussian Splatting with dynamic\nregularizations and shared size, color, and opacity. 4D Gaus-\nsian Splatting (Wu et al., 2023; Yang et al., 2023) uses a\ndeformation network to predict time-dependent position,\nscale, and rotation deformation.\n2.2. Image-to-3D Generation\nImage-to-3D generation aims at generating 3D assets from\na single reference image. It can be viewed as a conditional\ngeneration task using techniques like diffusion models (Ho\net al., 2020). Point-E (Nichol et al., 2022) and Shap-E (Jun\n& Nichol, 2023) can be directly trained to generate 3D\npoint clouds or Neural Radiance Fields (NeRF) (Milden-\nhall et al., 2020) conditioned on image features, but the\nquality is restricted by spatial resolution and high-quality\n2\nDreamGaussian4D\n3D datasets. Some methods (Melas-Kyriazi et al., 2023;\nTang et al., 2023b) take advantage of recent performant 2D\ndiffusion models (Rombach et al., 2022; Liu et al., 2023c;\nDeitke et al., 2023a) and lift them to 3D using score dis-\ntillation sampling (SDS) (Poole et al., 2022). For exam-\nple, Magic123 (Qian et al., 2023) combines both image\nand text input to distill high-quality 3D models through\nNeRF. DreamGaussian (Tang et al., 2023a) further shortens\nthe optimization time using the Gaussian splatting repre-\nsentation (Kerbl et al., 2023). The problem can also be\nformulated as a single-view 3D reconstruction task. Many\nworks (Xu et al., 2019; Chen & Zhang, 2019; Chen et al.,\n2020; Trevithick & Yang, 2021; Duggal & Pathak, 2022;\nSzymanowicz et al., 2023) adopt auto-encoder structures to\nlearn 3D priors for this ill-posed task but are typically lim-\nited to one or a few categories of synthetic objects (Chang\net al., 2015). Recently, One-2-3-45 (Liu et al., 2023b;a)\nuses 2D diffusion models (Liu et al., 2023c; Shi et al., 2023)\nto generate multi-view images and train an efficient multi-\nview reconstruction model. LRM (Hong et al., 2023) adopts\ntransformer-based architecture to scale up the task on large\ndatasets (Deitke et al., 2023b; Yu et al., 2023) by directly\nregressing a triplane-based NeRF.\n2.3. 4D Generation\n4D generation aims to generate dynamic 3D scenes, which\nare useful in a wide range of graphics applications including\nanimation, gaming, and virtual reality. One line of work\nis to leverage text-to-video diffusion models to distill 4D\ncontents (Singer et al., 2023). Concretely, they optimize\na 4D representation like Hexplane (Cao & Johnson, 2023)\nor K-plane (Fridovich-Keil et al., 2023) by synthesizing\ncamera trajectories and then computing SDS on rendered\nvideos. Recent works focus on further improving photore-\nalism by introducing combining multiple diffusion priors\nin pursuit of a stronger supervision signal (Bahmani et al.,\n2023; Zheng et al., 2023). However, the optimization time\nand computation cost are prohibitive for these approaches\nto be deployed in the real world. Moreover, the 3D contents\ncouple with motions, therefore they lack diversity and con-\ntrol of the generated motions. Recently, a few works have\nproposed to obtain the 4D model from an input image (Zhao\net al., 2023). However, they still adhere to the video distil-\nlation framework and suffer from prolonged optimization\ntimes and a lack of effective motion control. Noticalbally,\nConsistent4D (Jiang et al., 2023) proposes to obtain the\n4D model from an input video, which is closest to our set-\nting. Compared to it, we study image-conditioned generated\nvideos, which allow diverse motions on the same static\nmodel. A concurrent work (Ling et al., 2023) also explores\nGaussian Splatting for high-quality 4D generation and our\napproach requires less than 5% of its optimization iterations\nin comparison.\n3. Approach\nDreamGaussian4D comprises three stages. The initial stage,\nstatic generation, involves designing an enhanced variant\nof DreamGaussian (Tang et al., 2023a) to create 3D Gaus-\nsians from the input image (Section 3.1). The second stage,\ndynamic generation, generates a driving video from the in-\nput image to optimize a time-dependent deformation field\nupon the static 3D Gaussians (Section 3.2). The final stage\nis an optional mesh refinement that converts the 4D Gaus-\nsians into an animated mesh sequence and applies a video-\nto-video pipeline to refine the texture maps consistently\n(Section 3.3).\n3.1. DreamGaussianHD for Static Generation\nDespite its rapid optimization speed, the original Dream-\nGaussian (Tang et al., 2023a) introduces significant blur-\nriness to the unseen areas of static models, as illustrated\nin Figure 6. This blurriness adversely affects the subse-\nquent dynamic optimization process. Therefore, we first\ndesign better implementation practices to reliably enhance\nthe image-to-3D generation quality of DreamGaussian at\nthe cost of a reasonable increase in optimization time. We\nsummarize these improved practices as DreamGaussianHD.\n3.1.1. MULTI-VIEW OPTIMIZATION\nApart from the reference view, DreamGaussian typically\nsamples one random view at each optimization iteration for\nSDS. This approach covers only part of the Gaussians and\nleads to unbalanced optimization and convergence. As ob-\nserved in previous works, increasing the number of sampled\nviews (batch size) at each optimization step can significantly\nmitigate this issue (Poole et al., 2022; Chen et al., 2023).\nSampling 16 views, for instance, yields high-quality geome-\ntry in the unseen regions of the 3D Gaussians. As a trade-off,\nthis approach incurs an increase in memory usage during\nSDS computation and lengthens the optimization duration.\n3.1.2. FIXING BACKGROUND COLOR\nDreamGaussian uniformly samples the background color\nfrom black and white. However, most 3D-aware image\ndiffusion models render the training objects with a white\nbackground. We have observed that renderings with a black\nbackground introduce additional noise into the optimization\nprocess, ultimately resulting in blurriness. By consistently\nsetting the background color to white, we achieve more\ndetailed and refined results in the optimized 3D GS.\n3\nDreamGaussian4D\nII) Video-to-video Texture Refinement\nRendered Video\nVideo Diffusion Prior\nSynthetic Camera Trajectory\nRender\nAdd Noise\nDenoise\nMSE\nMulti-step\nInput Image\nUV\nmap\nmesh\nRefined Video\nT=0.7\nI) Image-to-4D GS Generation\nInput Image\nImage-to-Video Diffusion\nDriving Video\n3D-aware\nImage Diffusion Prior\n3D GS: (\ud835\udc5d!, \ud835\udc5f!, \ud835\udc60!)\nDreamGaussianHD\nDeform\n4D GS: (\u2206\ud835\udc5d\", \u2206\ud835\udc5f\", \u2206\ud835\udc60\")\nMSE\nReference View\nSDS\nRandom View\n+Multiview Opt.\n+Background Fix.\nFigure 2: DreamGaussian4D Framework. We first obtain a static 3D GS model using DreamGaussianHD and a driving\nvideo with an image-to-video diffusion model. We then optimize a deformation network that learns to deform the static 3D\nGS at different time stamps, supervised by the MSE loss to the driving video and SDS losses. Finally, per-frame meshes can\nbe exported and the texture maps can be refined with a video-to-video pipeline.\n3.2. Gaussian Deformation for Dynamic Generation\n3.2.1. GENERATING DRIVING VIDEO\nDifferent from other methods (Bahmani et al., 2023) that\nperform SDS supervision using a specific video diffu-\nsion model, we propose to use explicit supervision from\nany video depicting the input image. This video can be\ncreated by artists like those in video-to-4D (Jiang et al.,\n2023), or generated automatically from an image-to-video\nmodel. In practice, we use the off-the-shelf Stable Diffusion\nVideo (Blattmann et al., 2023) to generate the videos from\ninput images:\n{IRef}T\n\u03c4=1 = f\u03c8(\u03f5; IInput),\n(1)\nwhere IInput represents the input image, {IRef}T\n\u03c4=1 is the\ndriving video, \u03f5 denotes random noise, and f\u03c8 is the image-\nto-video diffusion model. Since our method does not rely\non the video diffusion model later, we can choose high-\nquality videos with better temporal consistency and motion\ngenerated by different random seeds, which enables better\ncontrollability and diversity for image-to-4D generation.\n3.2.2. STATIC-TO-DYNAMIC INITIALIZATION\nTo further augment the static 3D Gaussians into dynamic\n4D Gaussians, a deformation network is trained to predict\nthe change of position, rotation, and scale of each Gaussian\ngiven a timestamp.\nS\u2032 = \u03d5(S, \u03c4),\n(2)\nwhere \u03d5 is the deformation network, S is the spatial de-\nscriptions of the static 3D GS including position, rotation,\nand scaling, \u03c4 is the time stamp, and S\u2032 is the deformed\n3D GS spatial descriptions. However, randomly initializing\nthe deformation network can cause a divergence between\nthe dynamic and static models, leading to convergence at a\nsub-optimal mode, as exemplified in Figure 7. To mitigate\nthis, we initialize the deformation model to predict zero\ndeformation at the start of training. Concretely, we initialize\nthe weight and bias of the final prediction heads to zero.\nTo enable gradient backpropagation, skip connections are\nintroduced to the prediction heads.\n3.2.3. DEFORMATION FIELD OPTIMIZATION\nWe optimize the deformation field given the driving video\nfrom the reference view. We fix the camera to the reference\n4\nDreamGaussian4D\nGenerated 4D Model\nInput Image\nFigure 3: Qualitative results. Renders are shown with gradually changing time stamps and view angles.\nview, and minimize the Mean Squared Error (MSE) between\nthe rendered image and the driving video frame at each\ntimestamp:\nLRef = 1\nT\nT\nX\n\u03c4=1\n||f(\u03d5(S, \u03c4), oRef) \u2212 I\u03c4\nRef||2\n2,\n(3)\nwhere I\u03c4\nRef is the \u03c4-th frame in the video, oRef is the reference\nview point and f is the rendering function. To propagate the\nmotion from the reference view to the whole 3D model, we\nleverage Zero-1-to-3-XL (Deitke et al., 2023a) to predict the\ndeformation of the unseen part. Although image diffusion\nmodels only perform per-frame prediction, the temporal\nconsistency can be mostly preserved since the color and\nopacity of the static 3D GS are fixed. Similar to the training\npractice in DreamGaussianHD, multiple views are sampled\n5\nDreamGaussian4D\nAnimate124\nAnimate124\nAnimate124\nOurs\nOurs\nOurs\nInput Image\nInput Image\nInput Image\nFigure 4: Comparison with Animate124 (Zhao et al., 2023). Our model achieves better faithfulness to the input image,\nlarger motions, and a higher level of detail.\nfor each time step.\n\u2207\u03d5LSDS = Et,\u03c4,\u03f5,o[(\u03f5\u03b8(\u02c6I; t, I\u03c4\nRef, o) \u2212 \u03f5) \u2202I\n\u2202\u03d5],\n(4)\n\u02c6I = f(\u03d5(S, \u03c4), o),\n(5)\nwhere \u03f5 is a random noise, \u03f5\u03b8 is the noise predictor of a 3D-\naware image diffusion model, and o is a random viewpoint.\nThanks to the static model initialization, we can start the\nSDS at a lower noise level. Specifically, we start SDS with\na Tmax = 0.5, which is lower than the common practice\nwhere Tmax = 0.98.\n3.3. Video-to-video Texture Refinement\nMeshes for each frame can be extracted similarly to Dream-\nGaussian (Tang et al., 2023a), which involves running local\ndensity queries and color back-projection. However, these\nper-frame meshes lack temporal association, and refining\ntheir textures separately can result in flickering as demon-\nstrated in Figure 9. To enhance the UV-space texture map\nwhile maintaining temporal consistency, we employ a video-\nto-video pipeline. This process begins with synthesizing a\ncamera trajectory, where the camera moves at a constant\nspeed along 0 elevations from a randomly chosen horizon-\ntal angle. We then render the video and introduce noise at\nlevel 0.7 to it. Finally, an image-to-video diffusion model is\nutilized to transform this noisy video into a clean, denoised\nversion:\n{IRefined}T\n\u03c4=1 = f\u03c8({\u02c6I}T\n\u03c4=1 + \u03f5; IInput),\n(6)\nwhere \u03f5 is a random noise at the specified level and {\u02c6I}T\n\u03c4=1\nis the rendered video. The MSE loss is computed between\nthe two videos:\nLRefine = ||{\u02c6I}T\n\u03c4=1 \u2212 {IRefined}T\n\u03c4=1||2\n2\n(7)\nThe loss is then back-propagated to improve the texture\nmaps at all time steps.\n4. Experiments\n4.1. Implementation Details\nWe run all experiments on a single 80 GB A100 GPU. We\nimplement the DreamGaussian4D framework on the open-\n6\nDreamGaussian4D\na) Viewpoint 1\nb) Viewpoint 2\ntime\nFigure 5: Composited scene. 4D GS can be exported to meshes that can be efficiently composited and rendered in Blender.\na) DreamGa-\nussian\nb) + Multi-\nview optim.\nc) + Fixed\nbackground\nView angle\nInput Image\nFigure 6: Ablation on DreamGaussianHD. Off-the-shelf DreamGuassian suffers from severe blurriness on novel views.\nMulti-view optimization significantly improves the texture and geometries. Fixing the background color further enhances\nthe level of detail.\nsource repositories DreamGaussian (Tang et al., 2023a) and\n4D Gaussian Splatting (Wu et al., 2023). For driving video\ngeneration, we use Stable Video Diffusion to generate 14\nframes. For static optimization, we run 500 iterations with\na batch size of 16 for 2 minutes. We linearly decay Tmax\nfrom 0.98 to 0.02. For dynamic representation, we run\n200 iterations with batch size 4 for 4.5 minutes, with Tmax\nlinearly decaying from 0.5 to 0.02, For the optional mesh\nrefinement, we run 50 iterations with a constant T = 0.7\nfor 3.5 minutes.\n4.2. Quantitative Results\nWe evaluate on examples provided in Animate124 (Zhao\net al., 2023). For the evaluation metric, we compute CLIP-I.\nCLIP-I measures the cosine similarity of CLIP image em-\nbedding between reference-view renders and the reference\nTable 2: Quantitative Results. \u2020: computed on 8 examples\navailable at Zhao et al. (2023).\nMethod\nCLIP-I\nZero-1-to-3-V\n0.7925\nRealFusion-V\n0.8026\nAnimate124 (Zhao et al., 2023)\n0.8544\nOurs\u2020\n0.9227\nimage. The result is provided in Table 2. DreamGaussian\nachieves the best similarity to the input image. We also\ncompare the running speed in Table 1. DreamGaussian4D\nreduces the optimization time from hours to minutes.\n4.3. Qualitative Results\nWe show qualitative image-to-4D results in Figure 3. The\nresults are rendered in different time steps and camera views.\n7\nDreamGaussian4D\na) static\nb) w/o \nzero-init\nc) w/ \nzero-init\ntime\nFigure 7: Ablation on zero-initialization. Non-zero initial-\nization leads to changes from the static model which leads\nto sub-optimal modes, e.g., the back of the panda turns fully\nblack. Zero-initilalzation prevents the dirfting.\na) Motion 1\nb) Motion 2\nc) Motion 3\ntime\nFigure 8: Controllable Motions. DreamGaussian4D allows\neasy control of the generated motions. Different 3D motions\ncan be generated from different driving videos for the same\ninput image.\nIn Figure 4, we compare our approach with Animate124.\nOur approach achieves better faithfulness to the input image,\nstronger motion, and richer details in geometry and texture.\nWe further export 4D GS to meshes with texture refinement\nand composite them in the Blender engine. We render the\ncomposited scene from different view angles in Figure 5.\nAll qualitative results are better viewed in videos on the\nproject page.\ntime\na) I2I\nb) V2V\nFigure 9: Ablation on texture refinement. DreamGaussian-\nlike image-to-image (I2I) optimization leads to poor tempo-\nral consistency and flickering in adjacent frames. Video-to-\nvideo (V2V) optimization alleviates the issue.\n4.4. Ablation\n4.4.1. DREAMGAUSSIANHD\nWe ablate the improvement in image-to-3D quality of\nDreamGaussianHD in Figure 6. We observe severe blurri-\nness in DreamGaussian, particularly from side views and\nback views. By introducing multi-view optimization and\nbackground fixing, the novel view quality is significantly\nimproved.\n4.4.2. ZERO-INITIALIZATION\nWithout zero-initialization, the dynamic model can be\nslightly different from the static model at the start of the opti-\nmization and the difference could signify in the optimization\nprocess, resulting in sub-optimal results. In Figure 7, the\nback of the panda was black and white in the static stage.\nHowever, when initialized differently, the back could turn\nfully black after dynamic optimization. The issue is resolved\nby zero-initialization.\n4.4.3. DIVERSE MOTIONS\nDifferent\nfrom\nmost\nexisting\n4D\ngeneration\nap-\nproaches (Bahmani et al., 2023; Jiang et al., 2023)\nusing SDS, our method allows better controllability and\nmore diversity in the motions. Different 4D motions can be\ngenerated from different driving videos. In Figure 8, we\ngenerate three different driving videos for an input image,\nwhich results in three different 3D motions.\n8\nDreamGaussian4D\n4.4.4. VIDEO-TO-VIDEO TEXTURE REFINEMENT\nSince per-frame meshes have individual texture maps, there\nis no temporal consistency restriction in the representation.\nAs shown in Figure 9, directly optimizing the texture maps\nusing DreamGuassian-like refinement results in flickering\nin adjacent frames. Instead, video diffusion models pro-\nvide temporal consistency and result in smoother temporal\nchanges.\n5. Conclusion\nWe propose DreamGaussian4D, a framework that gener-\nates 4D contents employing 4D Gaussian Splatting. We\nshow that explicit modeling of spatial transformations in the\nGaussian Splatting greatly simplifies the 4D generation task.\nDreamGaussian4D significantly reduces the optimization\ntime from several hours to several minutes. Moreover, driv-\ning motion using generated videos allows explicit control of\nthe 3D motion for the first time. Lastly, DreamGaussian4D\nallows mesh extraction and temporally-coherent texture op-\ntimization, which facilitates real-world applications.\n9\nDreamGaussian4D\nReferences\nAbou-Chakra, J., Dayoub, F., and S\u00a8underhauf, N. Particlenerf: A particle-based encoding for online neural radiance fields.\nIn Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 5975\u20135984, 2024.\nBahmani, S., Skorokhodov, I., Rong, V., Wetzstein, G., Guibas, L., Wonka, P., Tulyakov, S., Park, J. J., Tagliasacchi, A., and\nLindell, D. B. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. arXiv preprint arXiv:2311.17984,\n2023.\nBlattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A.,\net al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127,\n2023.\nCao, A. and Johnson, J. Hexplane: A fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 130\u2013141, 2023.\nChang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al.\nShapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\nChen, R., Chen, Y., Jiao, N., and Jia, K. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d\ncontent creation. arXiv preprint arXiv:2303.13873, 2023.\nChen, Z. and Zhang, H. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 5939\u20135948, 2019.\nChen, Z., Tagliasacchi, A., and Zhang, H. Bsp-net: Generating compact meshes via binary space partitioning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 45\u201354, 2020.\nDeitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A., Laforte, C., Voleti, V., Gadre, S. Y., et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023a.\nDeitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., and\nFarhadi, A. Objaverse: A universe of annotated 3d objects. In CVPR, pp. 13142\u201313153, 2023b.\nDu, Y., Zhang, Y., Yu, H.-X., Tenenbaum, J. B., and Wu, J. Neural radiance flow for 4d view synthesis and video processing.\nIn 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 14304\u201314314. IEEE Computer Society,\n2021.\nDuggal, S. and Pathak, D. Topologically-aware deformation fields for single-view 3d reconstruction. In CVPR, pp.\n1536\u20131546, 2022.\nFang, J., Yi, T., Wang, X., Xie, L., Zhang, X., Liu, W., Nie\u00dfner, M., and Tian, Q. Fast dynamic radiance fields with\ntime-aware neural voxels. In SIGGRAPH Asia 2022 Conference Papers, pp. 1\u20139, 2022.\nFridovich-Keil, S., Meanti, G., Warburg, F. R., Recht, B., and Kanazawa, A. K-planes: Explicit radiance fields in space,\ntime, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n12479\u201312488, 2023.\nGao, C., Saraf, A., Kopf, J., and Huang, J.-B. Dynamic view synthesis from dynamic monocular video. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pp. 5712\u20135721, 2021.\nGuan, S., Deng, H., Wang, Y., and Yang, X. Neurofluid: Fluid dynamics grounding with particle-driven neural radiance\nfields. In International Conference on Machine Learning, pp. 7919\u20137929. PMLR, 2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. NeurIPS, 33:6840\u20136851, 2020.\nHong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., and Tan, H. Lrm: Large reconstruction\nmodel for single image to 3d. arXiv preprint arXiv:2311.04400, 2023.\nJiang, Y., Zhang, L., Gao, J., Hu, W., and Yao, Y. Consistent4d: Consistent 360 {\\deg} dynamic object generation from\nmonocular video. arXiv preprint arXiv:2311.02848, 2023.\n10\nDreamGaussian4D\nJun, H. and Nichol, A. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023.\nKerbl, B., Kopanas, G., Leimk\u00a8uhler, T., and Drettakis, G. 3d gaussian splatting for real-time radiance field rendering. ToG,\n42(4):1\u201314, 2023.\nLi, T., Slavcheva, M., Zollhoefer, M., Green, S., Lassner, C., Kim, C., Schmidt, T., Lovegrove, S., Goesele, M., Newcombe,\nR., et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 5521\u20135531, 2022.\nLi, Z., Niklaus, S., Snavely, N., and Wang, O. Neural scene flow fields for space-time view synthesis of dynamic scenes. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6498\u20136508, 2021.\nLi, Z., Wang, Q., Cole, F., Tucker, R., and Snavely, N. Dynibar: Neural dynamic image-based rendering. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4273\u20134284, 2023.\nLing, H., Kim, S. W., Torralba, A., Fidler, S., and Kreis, K. Align your gaussians: Text-to-4d with dynamic 3d gaussians\nand composed diffusion models. arXiv preprint arXiv:2312.13763, 2023.\nLiu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., and Su, H. One-2-3-45++: Fast single\nimage to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023a.\nLiu, M., Xu, C., Jin, H., Chen, L., Xu, Z., Su, H., et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without\nper-shape optimization. arXiv preprint arXiv:2306.16928, 2023b.\nLiu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., and Vondrick, C. Zero-1-to-3: Zero-shot one image to 3d\nobject. arXiv preprint arXiv:2303.11328, 2023c.\nLuiten, J., Kopanas, G., Leibe, B., and Ramanan, D. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis.\narXiv preprint arXiv:2308.09713, 2023.\nMelas-Kyriazi, L., Laina, I., Rupprecht, C., and Vedaldi, A. Realfusion: 360deg reconstruction of any object from a single\nimage. In CVPR, pp. 8446\u20138455, 2023.\nMildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural\nradiance fields for view synthesis. In ECCV, 2020.\nNichol, A., Jun, H., Dhariwal, P., Mishkin, P., and Chen, M. Point-e: A system for generating 3d point clouds from complex\nprompts. arXiv preprint arXiv:2212.08751, 2022.\nPark, K., Sinha, U., Barron, J. T., Bouaziz, S., Goldman, D. B., Seitz, S. M., and Martin-Brualla, R. Nerfies: Deformable\nneural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5865\u20135874,\n2021a.\nPark, K., Sinha, U., Hedman, P., Barron, J. T., Bouaziz, S., Goldman, D. B., Martin-Brualla, R., and Seitz, S. M. Hypernerf:\nA higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228,\n2021b.\nPoole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022.\nPumarola, A., Corona, E., Pons-Moll, G., and Moreno-Noguer, F. D-nerf: Neural radiance fields for dynamic scenes. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10318\u201310327, 2021.\nQian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.-Y., Skorokhodov, I., Wonka, P., Tulyakov, S., et al.\nMagic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint\narXiv:2306.17843, 2023.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion\nmodels. In CVPR, pp. 10684\u201310695, 2022.\n11\nDreamGaussian4D\nShao, R., Zheng, Z., Tu, H., Liu, B., Zhang, H., and Liu, Y. Tensor4d: Efficient neural 4d decomposition for high-fidelity\ndynamic reconstruction and rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 16632\u201316642, 2023.\nSheynin, S., Ashual, O., Polyak, A., Singer, U., Gafni, O., Nachmani, E., and Taigman, Y. Knn-diffusion: Image generation\nvia large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022.\nShi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., and Su, H. Zero123++: a single image to\nconsistent multi-view diffusion base model, 2023.\nSinger, U., Sheynin, S., Polyak, A., Ashual, O., Makarov, I., Kokkinos, F., Goyal, N., Vedaldi, A., Parikh, D., Johnson, J.,\net al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023.\nSzymanowicz, S., Rupprecht, C., and Vedaldi, A. Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint\narXiv:2312.13150, 2023.\nTang, J., Ren, J., Zhou, H., Liu, Z., and Zeng, G. Dreamgaussian: Generative gaussian splatting for efficient 3d content\ncreation. arXiv preprint arXiv:2309.16653, 2023a.\nTang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., and Chen, D. Make-it-3d: High-fidelity 3d creation from a single\nimage with diffusion prior. arXiv preprint arXiv:2303.14184, 2023b.\nTretschk, E., Tewari, A., Golyanik, V., Zollh\u00a8ofer, M., Lassner, C., and Theobalt, C. Non-rigid neural radiance fields:\nReconstruction and novel view synthesis of a dynamic scene from monocular video. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 12959\u201312970, 2021.\nTrevithick, A. and Yang, B. Grf: Learning a general radiance field for 3d representation and rendering. In ICCV, pp.\n15182\u201315192, 2021.\nTurki, H., Zhang, J. Y., Ferroni, F., and Ramanan, D. Suds: Scalable urban dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 12375\u201312385, 2023.\nWang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P., et al. Lavie: High-quality\nvideo generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023.\nWu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., and Wang, X. 4d gaussian splatting for real-time\ndynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023.\nXian, W., Huang, J.-B., Kopf, J., and Kim, C. Space-time neural irradiance fields for free-viewpoint video. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9421\u20139431, 2021.\nXu, Q., Wang, W., Ceylan, D., Mech, R., and Neumann, U. Disn: Deep implicit surface network for high-quality single-view\n3d reconstruction. Advances in neural information processing systems, 32, 2019.\nYang, Z., Gao, X., Zhou, W., Jiao, S., Zhang, Y., and Jin, X. Deformable 3d gaussians for high-fidelity monocular dynamic\nscene reconstruction. arXiv preprint arXiv:2309.13101, 2023.\nYu, X., Xu, M., Zhang, Y., Liu, H., Ye, C., Wu, Y., Yan, Z., Liang, T., Chen, G., Cui, S., and Han, X. Mvimgnet: A\nlarge-scale dataset of multi-view images. In CVPR, 2023.\nYuan, W., Lv, Z., Schmidt, T., and Lovegrove, S. Star: Self-supervised tracking and reconstruction of rigid objects in motion\nwith neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n13144\u201313152, 2021.\nZhao, Y., Yan, Z., Xie, E., Hong, L., Li, Z., and Lee, G. H. Animate124: Animating one image to 4d dynamic scene. arXiv\npreprint arXiv:2311.14603, 2023.\nZheng, Y., Li, X., Nagano, K., Liu, S., Hilliges, O., and De Mello, S. A unified approach for text-and image-guided 4d\nscene generation. arXiv preprint arXiv:2311.16854, 2023.\n12\n"
  },
  {
    "title": "DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision",
    "link": "https://arxiv.org/pdf/2312.16256.pdf",
    "upvote": "14",
    "text": "DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision\nLu Ling1,\nYichen Sheng1,\nZhi Tu1,\nWentian Zhao2,\nCheng Xin3,\nKun Wan 2,\nLantao Yu2,\nQianyu Guo1,\nZixun Yu 4,\nYawen Lu1,\nXuanmao Li5,\nXingpeng Sun 1,\nRohan Ashok1,\nAniruddha Mukherjee1,\nHao Kang 6,\nXiangrui Kong1,\nGang Hua6,\nTianyi Zhang1,\nBedrich Benes1 ,\nAniket Bera1\n1 Department of Computer Science, Purdue University,\n2 Adobe Inc. ,\n3 Rutgers University\n4 Google Inc. ,\n5 Huazhong University of Science and Technology,\n6 Wormpex AI Research\nFigure 1. We introduce DL3DV-10K, a large-scale, scene dataset capturing real-world scenarios. DL3DV-10K contains 10,510 videos\nat 4K resolution spanning 65 types of point-of-interest (POI) locations, covering a wide range of everyday areas. With the fine-grained\nannotation on scene diversity and complexity, DL3DV-10K enables a comprehensive benchmark for novel view synthesis and supports\nlearning-based 3D representation techniques in acquiring a universal prior at scale.\nAbstract\nWe have witnessed significant progress in deep learning-\nbased 3D vision, ranging from neural radiance field\n(NeRF) based 3D representation learning to applications in\nnovel view synthesis (NVS). However, existing scene-level\ndatasets for deep learning-based 3D vision, limited to ei-\nther synthetic environments or a narrow selection of real-\nworld scenes, are quite insufficient. This insufficiency not\nonly hinders a comprehensive benchmark of existing meth-\nods but also caps what could be explored in deep learning-\nbased 3D analysis. To address this critical gap, we present\nDL3DV-10K, a large-scale scene dataset, featuring 51.2\nmillion frames from 10,510 videos captured from 65 types\nof point-of-interest (POI) locations, covering both bounded\nand unbounded scenes, with different levels of reflection,\ntransparency, and lighting. We conducted a comprehensive\nbenchmark of recent NVS methods on DL3DV-10K, which\nrevealed valuable insights for future research in NVS. In\naddition, we have obtained encouraging results in a pilot\nstudy to learn generalizable NeRF from DL3DV-10K, which\nmanifests the necessity of a large-scale scene-level dataset\nto forge a path toward a foundation model for learning\n3D representation. Our DL3DV-10K dataset, benchmark\nresults, and models will be publicly accessible at Project\nPage.\n1. Introduction\nThe evolution in deep 3D representation learning, driven\nby essential datasets, boosts various tasks in 3D vision.\nNotably, the inception of Neural Radiance Fields [19]\n(NeRF), offering a new approach through a continuous\nhigh-dimensional neural network, revolutionized leaning-\nbased 3D representation and novel view synthesis (NVS).\nNeRF excels at producing detailed and realistic views, over-\ncoming challenges faced by traditional 3D reconstruction\nmethods and rendering techniques.\nAdditionally, it in-\narXiv:2312.16256v2  [cs.CV]  29 Dec 2023\nspires waves of innovative developments such as NeRF\nvariants [3, 4, 8, 27, 32, 37] and 3D Gaussian Splatting\n(3DGS) [15], significantly enhancing experiences in virtual\nreality, augmented reality, and advanced simulations.\nHowever, existing scene-level datasets for NVS are re-\nstricted to either synthetic environments or a narrow se-\nlection of real-world scenes due to the laborious work for\nscene collection. Notably, the absence of such large-scale\nscene datasets hinders the potential of deep 3D representa-\ntion learning methods in two pivotal aspects: 1) it is impos-\nsible to conduct a comprehensive benchmark to adequately\nassess NVS methods in complex real-world scenarios such\nas non-Lambertian surfaces. 2) It restricts the generalizabil-\nity of deep 3D representation learning methods on attaining\nuniversal priors from substantial real scenes.\nTo fill this gap, we revisited the commonly used dataset\nfor benchmarking NVS. i) Synthetic datasets like blender\ndataset [19] offer rich 3D CAD geometry but lack real-\nworld elements, which diminishes the model\u2019s robustness\nin practical applications. ii) Real-world scene datasets for\nNVS such as Tank and temples [16] and LLFF dataset [18]\noffer more variety but limited scope. They fail to capture\nthe complex real-world scenarios such as intricate light-\ning and various material properties (transparency and re-\nflectance [22]), which still challenges current SOTAs.\nMoreover, existing 3D representation methods yield\nphotorealistic views by independently optimizing NeRFs\nfor each scene, requiring numerous calibrated views and\nsubstantial compute time. Learning-based models like Pixl-\nNeRF [38] and IBRNet [30] mitigate this by training across\nmultiple scenes to learn scene priors. While datasets like\nRealEstate [42] and ScanNet++ [36] improve understand-\ning in specific domains such as indoor layouts, their lim-\nited scope hinders the broader applicability of these mod-\nels. This is primarily due to the absence of comprehensive\nscene-level datasets, which are crucial for learning-based\nmethods to achieve universal representation.\nBased on the above review, We introduce DL3DV-10K\na novel dataset that captures large-scale (multi-view) MV\nscenes using standard commercial cameras to enable effi-\ncient collection of a substantial variety of real-world sce-\nnarios. DL3DV-10K comprises 51.3 million frames from\n10,510 videos in 4k resolution, covers scenes from 65 types\nof point-of-interest [35] (POI) locations like restaurants,\ntourist spots, shopping malls, and natural outdoor areas.\nEach scene is further annotated with its complexity indices,\nincluding indoor or outdoor environments, the level of re-\nflection and transparency, lighting conditions, and the level\nof texture frequency.\nFig. 1 provides a glimpse of our\nDL3DV-10K dataset. Tab. 1 compares scale, quality, diver-\nsity, and annotated complexity between DL3DV-10Kand the\nexisting scene-level datasets.\nAdditionally, we present DL3DV-140, a comprehensive\nbenchmark for NVS, by sampling 140 scenes from the\ndataset. The diversity and fine-grained scene complexity in\nDL3DV-140 will enable a fair evaluation of NVS methods.\nWe conducted the statistical evaluation of the SOTA NVS\nmethods on DL3DV-140 (Sec. 4.1), including Nerfacto [27],\nInstant-NGP [20], Mip-NeRF 360 [3], Zip-NeRF [4], and\n3DGS [15]. Leveraging the multi-view nature of the data,\nwe attempt to showcase DL3DV-10K\u2019s potential for deep\n3D representation learning methods in gaining a universal\nprior for generating novel views. Our demonstrations indi-\ncate that pretraining on DL3DV-10K enhances the general-\nizability of NeRF (Sec. 4.2), confirming that diversity and\nscale are crucial for learning a universal scene prior.\nTo summarize, we present the following contributions:\n1. We introduce DL3DV-10K, a real-world MV scene\ndataset. It has 4K resolution RGB images and covers 65\nPOI indoor and outdoor locations. Each scene is anno-\ntated with the POI category, light condition, environment\nsetting, varying reflection and transparency, and the level\nof texture frequency.\n2. We provide DL3DV-140, a comprehensive benchmark\nwith 140 scenes covering the challenging real-world sce-\nnarios for NVS methods. We conduct the statistical eval-\nuation for the SOTA NVS methods on DL3DV-140 and\ncompare their weaknesses and strengths.\n3. We show that the pre-training on DL3DV-10K benefits\ngeneralizable NeRF to attain universal scene prior and\nshared knowledge.\n2. Related Work\n2.1. Novel View Synthesis\nNovel View Synthesis Methods. Early novel view synthe-\nsis (NVS) work concentrated on 3D geometry and image-\nbased rendering [17, 42].\nSince 2020, Neural Radiance\nFields (NeRF) [19] have been pivotal in NVS for their\nintricate scene representation, converting 5D coordinates\nto color and density, leading to various advancements in\nrendering speed and producing photorealistic views. Key\ndevelopments include Instant-NGP [20], which speeds up\nNeRF using hash tables and multi-resolution grids; Mip-\nNeRF [2] addressed aliasing and Mip-NeRF 360 [3] ex-\npanded this to unbounded scenes with significant compu-\ntational power; and Zip-NeRF [4] combines Mip-NeRF\n360 with grid-based models for improving efficiency. Ner-\nfacto [27] merges elements from previous NeRF methods\nto balance speed and quality. Additionally, 3D Gaussian\nSplatting (3DGS) [15] uses Gaussian functions for real-\ntime, high-quality rendering.\nHowever, those SOTA methods, building neural radi-\nance fields for individual scenes, require dense views and\nextensive computation. Learning-based models like Con-\ntraNeRF [33], TransNeRF [29], PixelNeRF [38], and IBR-\nDataset\n# of scene\n# of POI category\nResolution\n# of frames\nScene complexity annotation\nNVS\nindoor/outdoor\nreflection\ntransparency\nLLFF [18]\n24\n-\n640 \u00d7 480\n<1K\n\u2713 \u2713\n\u2713\n\u2717\n\u2713\nDTU [14]\n124\n5\n1200 \u00d7 1600\n30K\n\u2713 \u2717\n\u2713\n\u2717\n\u2713\nBlendedMVS [34]\n113\n-\n1536 \u00d7 2048\n17K\n\u2713 \u2713\n\u2717\n\u2717\n\u2717\nScanNet [11]\n1513\n11\n1296 \u00d7 968\n2,500K\n\u2713 \u2717\n\u2717\n\u2717\n\u2717\nMatterport3D [6]\n901\n-\n1280 \u00d7 1024\n195K\n\u2713 \u2717\n\u2713\n\u2717\n\u2717\nTanks and Temples [16]\n21\n14\n3840 \u00d7 2160\n147K\n\u2713 \u2713\n\u2713\n\u2717\n\u2713\nETH3D [24]\n25\n11\n6048 \u00d7 4032\n<1K\n\u2713 \u2713\n\u2717\n\u2717\n\u2717\nRealEstate10K2 [42]\n10,000\n1\n1280 \u00d7 720\n10,000K\n\u2713 \u2717\n\u2717\n\u2717\n\u2713\nARKitScenes [5]\n1661\n1\n1920 \u00d7 1440\n450K\n\u2713 \u2717\n\u2717\n\u2717\n\u2717\nScanNet++ [36]\n460\n5\n7008 \u00d7 46723\n3,980K\n\u2713 \u2717\n\u2713\n\u2717\n\u2713\nDL3DV-10K (ours)\n10,510\n65\n3840 \u00d7 2160\n51,200K\n\u2713 \u2713\n\u2713\n\u2713\n\u2713\nTable 1. Comparison of the existing scene-level dataset in terms of quantity, quality, diversity, and complexity, which is measured by the\nfine-grained surface properties, light conditions, texture frequency, and environmental setting.\nNet [30] overcome this by training on numerous scenes for\nuniversal priors and sparse-view synthesis.\nYet, the ab-\nsence of large-scale scene-level datasets reflective of real-\nworld diversity fails to provide adequate assessment for the\nSOTAs. Additionally, it hinders the potential for learning-\nbased 3D models to gain a universal prior.\nNovel View Synthesis Benchmarks.\nNVS benchmarks\nare generally split into synthetic benchmarks like the NeRF-\nsynthetic (Blender) [19], ShapeNet [7] and Objaverse [12],\nfeaturing 3D CAD models with varied textures and com-\nplex geometries but lacking real-world hints such as noise\nand non-Lambertian effects.\nIn contrast, real-world NVS benchmarks, originally in-\ntroduced for multi-view stereo (MVS) tasks like DTU [14]\nand Tanks and Temples [16], offer limited variety. While\nScanNet [11] has been used for benchmarking NVS, its mo-\ntion blur and narrow field-of-view limit its effectiveness.\nLater benchmarks for outward- and forward-facing scenes\nemerged, but they vary in collection standards and lack\ndiversity, particularly in challenging scenarios like light-\ning effects on reflective surfaces. For example, LLFF [18]\noffers cellphone-captured 24 forward-facing scenes; Mip-\nNeRF 360 dataset [19] provides 9 indoor and outdoor\nscenes with uniform distance around central subjects; Nerf-\nstudio dataset [27] proposes 10 captures from both phone\nand mirrorless cameras with different lenses.\nInspired by the capture styles of these datasets, DL3DV-\n140 provides a variety of scenes to comprehensively eval-\nuate NVS methods, including challenging view-dependent\neffects, reflective and transparent materials, outdoor (un-\nbounded) environments, and high-frequency textures. We\nalso offer extensive comparative analyses, demonstrating\nthe efficacy of DL3DV-140 in assessing NVS techniques.\n2.2. Multi-view Scene Dataset\nMulti-view (MV) datasets are commonly used for NVS\ntasks in the 3D vision field.\nThese datasets range from\nsynthetic, like ShapeNet [7] and Pix2Vox++ [31], to foun-\ndational real-world datasets capturing both object-level and\nscene-level images.\nObject-level datasets like Objectron [1], CO3D [21], and\nMVimgnet [39] offer substantial scale for learning-based re-\nconstruction. While they facilitate the learning of spatial\nregularities, reliance solely on object-level MV datasets im-\npedes prediction performance on unseen objects and com-\nplex scenes containing multiple objects [38].\nScene-level datasets are essential for NVS and scene\nunderstanding, yet offerings like LLFF [18], Matter-\nport3D [6], and BlendedMVS [34] encompass limited\nscenes. DTU [14], despite its use in developing general-\nized NeRF [38], is limited by its scale, affecting the models\u2019\nability to generalize. RealEstate10k [42], ARKitScenes [5],\nScanNet [11], and the high-resolution ScanNet++ [36] im-\nprove this with a broad range of detailed indoor scenes.\nYet, their applicability remains less comprehensive for di-\nverse indoor settings like shopping centers and restaurants,\nor outdoor scenarios. Although RealEstate10k, focusing on\nYouTube real estate videos, provides comparable scale with\nus, they comprise low resolution and lack of diversity. Over-\nall, the limited scale and diversity of these datasets pose\nchallenges for the robust and universal training of 3D deep\nlearning models. We close this gap by introducing DL3DV-\n10K, encompassing multifarious real-world scenes from in-\ndoor to outdoor environments, enhancing 3D spatial percep-\ntion, and paving the way for more robust learning-based 3D\nmodels.\n190 building-scale scenes covering 2056 rooms\n2YouTube video\n37008\u00d74672 in 270 scenes and 1920\u00d71440 in 190 scenes\nFigure 2. The efficient data acquisition pipeline of DL3DV-10K.\nRefer to supplementary materials for more visual illustrations of\nscene coverage.\n3. Data Acquisition and Processing\nOur data acquisition goal is to gather large-scale, high-\nquality scenes reflective of real-world complexity and di-\nversity. We develop a pipeline that integrates video cap-\nture, pre-processing, and analysis, leveraging widely avail-\nable consumer mobiles and drones to ensure the coverage\nof everyday accessible areas. We designed a detailed guide-\nline to train the collectors to minimize motion blur, exclude\nexposure lights, and avoid moving objects. This collection\nprocess, illustrated in Fig. 2, is user-friendly and enhances\nboth quality and efficiency.\n3.1. Data Acquisition\nDiversity.\nOur\ncollection\nfocuses\non\nvarious\ncom-\nmonly accessible scenes, adhering to the point-of-interest\n(POI) [35] categories. DL3DV-10K captures scenes from\n16 primary and 65 secondary POI categories. These cat-\negories include varied settings such as educational insti-\ntutions, tourist attractions, restaurants, medical facilities,\ntransportation hubs, etc. The diversity, spanning from in-\ndoor to outdoor environments and different cities, is instru-\nmental in enriching the dataset with a broader spectrum of\ncultural and architectural variations. Such variety is essen-\ntial for benchmarking NVS techniques as it challenges and\nrefines their ability to generalize across a multitude of real-\nworld scenarios and fosters robustness and adaptability in\n3D models. Sec. 3.3 summarizes the POI category of col-\nlected scenes.\nComplexity.\nReal-world scenes challenge state-of-the-art\ntechniques (SOTAs) with their diverse environments (in-\ndoor vs. outdoor), view-dependent lighting effects, reflec-\ntive surfaces, material properties, and high-frequency tex-\ntures.\nThe high detail frequency in textures, from deli-\ncate fabric to coarse stone, requires meticulous rendering.\nOutdoor (unbounded) scenes, with varying details between\nnear and distant objects, challenge the robustness of NVS\nmethods in handling scale differences. Complex shadows\nand view-dependent highlights from natural and artificial\nCategory\nDevice\nQuality by moving objects\nConsumer mobile\nDrone\n<3s\n3s - 10s\n# of scene\n10,407\n103\n8064\n2446\nTable 2. Number of scenes by devices and level of quality.\nlights, interacting with reflective and transparent materials\nlike metal and glass, require precise handling for realis-\ntic depiction. Additionally, we provide multiple views of\nthe scene from various angles at different heights and dis-\ntances. This multiplicity and complexity of views enable\n3D methods to predict view-dependent surface properties.\nIt also aids in separating viewpoint effects in learning view-\ninvariant representations like patch descriptors [40] and nor-\nmals [41].\nQuality.\nThe quality of the video is measured by the con-\ntent and coverage density of the scene and video resolution.\nWe have formulated the following requirements as guide-\nlines for recording high-quality scene-level videos: 1) The\nscene coverage is in the circle or half-circle with a 30 secs-\n45 secs walking diameter and has at least five instances with\na natural arrangement. 2) The default focal length of the\ncamera corresponds to the 0.5x ultra-wide mode for captur-\ning a wide range of background information. 3) Each video\nshould encompass a horizontal view of at least 180\u25e6 or 360\u25e6\nfrom different heights, including overhead and waist levels.\nIt offers high-density views of objects within the coverage\narea. 4) The video resolution should be 4K and have 60\nfps (or 30 fps). 5) The video\u2019s length should be at least 60\nsecs for mobile phone capture and 45 secs for drone video\nrecording. 6) We recommend limiting the duration of mov-\ning objects in the video to under 3 secs, with a maximum\nallowance of 10 secs. 7) The frames should not be motion-\nblurred or overexposed. 8) The captured objects should be\nstereoscopic. Post-capture, we meticulously inspect videos\nbased on the above criteria, such as moving objects in the\nvideo, to guarantee the high quality of videos captured on\nmobile devices. Tab. 2 summarizes the number of scenes\nrecorded by different devices and the associated quality lev-\nels in terms of moving objects\u2019 duration in the videos.\n3.2. Data Processing\nFrequency Estimation.\nTo estimate the frequency met-\nric of the scene over the duration of the captured video,\nwe first sample 100 frames from each video, as texture fre-\nquency is typically calculated based on RGB images. Then,\nwe convert RGB images to grayscale and normalize the in-\ntensities. To extract the high-frequency energy, we apply a\ntwo-dimensional bi-orthogonal wavelet transform [10] and\ncompute the Frobenius norm of the ensemble of LH, HL\nand HH subbands. The Frobenius norm is finally normal-\nized by the number of pixels, and the average quotient over\n100 frames is the frequency metric. The distribution of fre-\n0\n500\n1000\n1500\nScene Count\nFinancial Services\nGovernment & Civic Services\nEvents & Conferences\nRestaurants and Cafes\nReligious Institutions\nSports & Fitness\nMedical Facilities\nTech & Business\nAutomotive Services\nTransportation Hubs\nParks and Recreation\nEducation Institutions\nHotels and Accommodations\nNature & Outdoors\nTourist Attractions\nShopping Centers\nPrimary POI Category\nScene complexity by environement\nEnvironment\nindoor\noutdoor\n0\n500\n1000\n1500\nScene Count\nScene complexity by light condition\nLight condition\nalight\nmlight\nnlight\n0\n500\n1000\n1500\nScene Count\nScene complexity by reflection\nReflection class\nless\nmedium\nmore\nnone\n0\n500\n1000\n1500\nScene Count\nScene complexity by transparency\nTransparency class\nless\nmedium\nmore\nnone\nFigure 3. We show the distribution of scene category (the primary POI locations) by complexity indices, including environmental setting,\nlight conditions, reflective surface, and transparent materials. Attributes in light conditions include: natural light (\u2018nlight\u2019), artificial light\n(\u2018alight\u2019), and a combination of both (\u2018mlight\u2019). Reflection class includes \u2018more\u2019, \u2018medium\u2019, \u2018less\u2019, and \u2018none\u2019. Transparency class likewise.\nquency metrics is shown in supplementary materials.\nLabeling.\nWe categorize and annotate the diversity and\ncomplexity of scenes based on our established criteria, in-\ncluding key attributes of the scene, such as POI category,\ndevice model, lighting conditions, environmental setting,\nsurface characteristics, and high-frequent textures. The POI\ncategory and device model depict the location where the\nvideo was collected and the equipment used during its cap-\nture. Lighting conditions are differentiated into natural, ar-\ntificial, or a combination of both, influencing the ambient\nillumination of scenes.\nThe environmental setting anno-\ntation distinguishes between indoor and outdoor settings,\nwhich is crucial for evaluating the NVS performance in both\nbounded and unbounded spaces. We measure the surface\nproperties by the level of reflectivity, ranging from more\nand medium to less and none. It is estimated by the ra-\ntio of reflective pixels in the image and its present duration\nin the video. Material property, measured by transparency,\nfollows a similar rule. Refer to supplementary materials for\nmore details on reflection and transparency labeling criteria.\n3.3. Data Statistics\nScale.\nDL3DV-10K aims to provide comprehensive and\ndiverse coverage of scene-level datasets for 3D vision. It\ncovers scenes collected from 16 primary POIs and 65 sec-\nondary POIs and comprises 51.3 million frames of 10,510\nvideos with 4K resolution. As shown in Tab. 1, it enjoys\nfine-grained annotation for scene complexity indices.\nHierarchy.\nWe classify the scene category following the\nPOI category taxonomy. For example, the primary POI cat-\negory is the \u2018entertainment area\u2019. Its secondary POI cate-\ngory includes \u2018theaters\u2019, \u2018concert halls\u2019, \u2018sports stadiums\u2019,\nand \u2018parks and recreation areas\u2019. The statistics of primary\nPOI-based scene categories by annotated complexity are\npresented in Fig. 3. The distribution of scenes captured in\nthese POI locations follows: 1) their generality in nature.\n2) the probability of no moving objects appearing within 60\nsec in the locations. 3) the accessibility to these locations.\nFor example, the government and civic services locations\nusually do not allow video shooting in high-level details. 4)\nthe collected video is under the permission of the agents.\n3.4. Benchmark\nTo comprehensively assess the SOTAs, the benchmark\nneeds to cover the inherent complexity of real-world scenar-\nios with varying reflectance, texture, and geometric proper-\nties. To achieve our goal, we present DL3DV-140 as a com-\nprehensive benchmark, sampling 140 static scenes from our\ndataset. Additionally, we simplified the reflection and trans-\nparency classes into two categories for better interpretation:\nmore reflection (including more and medium reflection) and\nless reflection (including less and no reflection); the same\napproach applies to transparency. DL3DV-140 with scenes\ncollected from diverse POIs, maintains a balance in each\nannotated scene complexity indices. This means DL3DV-\n140 are categorized as indoor (bounded) scenes vs. outdoor\n(unbounded) scenes, high vs. low texture frequency (low-\nfreq vs. high-freq), more vs. less reflection (more-ref vs.\nless-ref), and more vs. less transparency (more-transp vs.\nless-transp). DL3DV-140 offers challenging scenes with a\nrich mix of diversity and complexity for a comprehensive\nevaluation of existing SOTA methods.\n4. Experiment\n4.1. Evaluation on the NVS benchmark\nMethods for comparison.\nWe examine the current rele-\nvant state-of-the-art (SOTA) NVS methods on DL3DV-140,\nincluding NeRF variants such as Nerfacto [27], Instant-\nNGP [20], Mip-NeRF 360 [3], Zip-NeRF [4], and 3D Gaus-\nsian Splatting (3DGS) [15].\nExperiment details.\nThe SOTAs have different assump-\ntions on the image resolution. For fairness, we use 960\u00d7560\nFigure 4. A presents the density plot of PSNR and SSIM and their relationship on DL3DV-140 for each method. B describes the perfor-\nmance comparison by scene complexity. The text above the bar plot is the mean value of the methods on the attribute.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nTrain \u2193\nMem \u2193\nInstant-NGP\n25.01\n0.834\n0.228\n1.2 hr\n3.9GB\nNerfacto\n24.61\n0.848\n0.211\n2.6 hr\n3.7GB\nMip-NeRF 360\n30.98\n0.911\n0.132\n48.0 hr\n23.6GB\n3DGS\n29.82\n0.919\n0.120\n2.1 hr\n16.8GB\nZip-NeRF*\n29.07\n0.878\n0.169\n2.5 hr\n23.8GB\nZip-NeRF\n31.22\n0.921\n0.112\n4.0 hr\n38.2GB\nTable 3. Performance on DL3DV-140. The error metric is calcu-\nlated from the mean of 140 scenes on a scale factor of 4. Zip-NeRF\nuses the default batch size (65536) and Zip-NeRF* uses the iden-\ntical batch size as other methods (4096). Note, the training time\nand memory usage may be different depending on various config-\nurations. Refer to supplementary materials for details.\nresolution to train and evaluate all the methods. Each scene\nin the benchmark has 300-380 images, depending on the\nscene size. We use 7/8 of the images for training and 1/8\nof the images for testing. Different methods vary in their\nmethod configurations. We use most of the default settings,\nlike network layers and size, optimizers, etc. But to ensure\nfairness, we fix some standard configurations. Each NeRF-\nbased method (Nerfacto, Instant-NGP, Mip-NeRF 360, Zip-\nNeRF) has the same 4,096 ray samples per batch (equiva-\nlent to chunk size or batch size), the same near 0.05 and far\n1e6. Note that Mip-NeRF 360 and Zip-NeRF use a much\nhigher number of rays (65,536) per batch by default. We\nmodify the learning rate to match the change of ray samples\nas suggested by the authors. We notice that Zip-NeRF per-\nformance is sensitive to the ray samples. So, we add one\nmore experiment for Zip-NeRF with the same ray samples\nof 4,096 as other methods. For all methods, we train enough\niterations until they converge.\nQuantitative results.\nTab. 3 summarizes the average\nPSNR, SSIM, and L-PIPS metrics across all scenes in\nDL3DV-140 along with training hours and memory con-\nsumption for each method. Furthermore, Fig. 4 provides\ndetailed insights into the metric density functions and their\ncorrelations.\nThe results indicate that Zip-NeRF, Mip-NeRF 360,\nand 3DGS consistently outperform Instant-NGP and Ner-\nfacto across all evaluation metrics. Remarkably, Zip-NeRF\ndemonstrates superior performance in terms of average\nPSNR and SSIM, although it consumes more GPU mem-\nory using the default batch size. Besides, we notice that\nreducing the default batch size for Zip-NeRF significantly\ndecreases its PSNR, SSIM, and LPIPS, see Zip-NeRF* in\nTab. 3. Mip-NeRF 360 achieves a PSNR of 30.98 and SSIM\nof 0.91, yet it shows relatively lower computational effi-\nciency, with an average training time of 48 hours. The den-\nsity functions of PSNR and SSIM, depicted in Fig. 4A, un-\nderscores Zip-NeRF and 3DGS\u2019s robust performance across\nall scenes. Moreover, we observe that 3DGS, with an SSIM\nof 0.92, surpasses Mip-NeRF 360\u2019s SSIM of 0.91, consis-\ntent with the findings from 3DGS\u2019s evaluation using the\nMip-NeRF 360 dataset [15].\nFig. 4 B illustrates the performance across scene com-\nplexity indices. Among all indices, outdoor (unbounded)\nscenes appear to be the most challenging, as all methods\nyield the lowest PSNR and SSIM scores in this setting.\nConversely, low-frequency scenes are the easiest to gener-\nate. Furthermore, more transparent scenes present higher\nchallenges compared to less transparent ones.\nIn terms\nof method comparison, Zip-NeRF outperforms others in\nmost scenes, except in low-frequency scenarios where Mip-\nNeRF 360 demonstrates superior performance. Addition-\nally, Mip-NeRF 360\u2019s smaller standard deviation in low-\nfrequency scenes indicates its robustness in this scenario.\nWe also present the findings of SOTAs\u2019 performance in\nterms of scene diversity, which are described in the sup-\nplementary materials.\nVisual results.\nWe show the visual result for the SOTAs\non DL3DV-140 in Fig. 5. Overall, the artifact pattern for\nNeRF variants is the amount of \u201cgrainy\u201d microstructure,\nwhile 3DGS creates elongated artifacts or \u201csplotchy\u201d Gaus-\nsians.\nNeRF variants exhibit high sensitivity to distance scale,\noften generating blurry backgrounds and less-detailed fore-\ngrounds.\nFor instance, Instant-NGP produces floating\nartifacts in the far-distance background of unbounded\nscenes.\nAlthough Zip-NeRF and Mip-NeRF 360 output\nfine-grained details compared to other NeRF variants, they\nalso struggle with aliasing issues in sharp objects with high-\nfrequent details, such as grasses and tiny leaves, as shown\nin Fig. 5 with \u2018high-freq\u2019 case.\nIn contrast, 3DGS per-\nforms better against aliasing issues than NeRF variants; it\nsuffers from noticeable artifacts in the far-distance back-\nground, such as the sky and far-distance buildings, as shown\nin Fig. 5 with \u2018outdoor\u2019 case.\nRegarding view-dependent effects in reflective and trans-\nparent scenes, 3DGS excels at rendering finely detailed and\nsharp lighting, such as strong reflections on metal or glass\nsurfaces, and effectively captures subtle edges of transpar-\nent objects, a challenge for other methods.\nHowever, it\ntends to oversimplify softer reflective effects, like cloud re-\nflections on windows or subtle light on the ground, as shown\nin Fig. 5 with \u2018more-ref\u2019 and \u2018more-transp\u2019 cases. In con-\ntrast, Zip-NeRF and Mip-NeRF 360 are less sensitive to\nthe intensity of reflective light, capturing reflections more\ngenerally. On the other hand, Nerfacto and Instant-NGP\nstruggle with these complex lighting effects, often produc-\ning floating artifacts.\n4.2. Generalizable NeRF\nRecent NeRFs and 3DGS aim to only fit the training\nscene.\nUsing these NVS methods to generalize to un-\nseen real-world scenarios requires training on a large set\nof real-world multi-view images. Due to the lack of real-\nworld scene-level multi-view images, existing works ei-\nther resort to training on large-scale object-level synthetic\ndata [8, 9, 23, 28, 38] or a hybrid of synthetic data and a\nsmall amount of real data [29, 30, 33]. The limited real-\nworld data cannot fully bridge the domain gap. In this sec-\ntion, we conduct a pilot experiment to show that our large-\nscale real-world scene DL3DV-10K dataset has the potential\nto drive the learning-based generalizable NeRF methods by\nproviding substantial real-world scenes for training.\nExperiment details.\nWe choose IBRNet [30] as the base-\nline to conduct an empirical study. To demonstrate the ef-\nfectiveness of DL3DV-10K, we pre-train the IBRNet on our\nDL3DV-10K to obtain a general prior and fine-tune on the\nDiffuse Synthetic 360\u25e6 [13]\nRealistic Synthetic 360\u25e6 [19]\nReal Forward-Facing [18]\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nIBRNet\n34.72\n0.983\n0.024\n23.95\n0.906\n0.101\n24.82\n0.808\n0.178\nIBRNet-S\n34.22\n0.979\n0.024\n23.57\n0.905\n0.101\n24.86\n0.807\n0.183\nIBRNet-270\n35.18\n0.984\n0.024\n24.55\n0.911\n0.097\n25.00\n0.812\n0.180\nIBRNet-1K\n35.13\n0.984\n0.023\n23.58\n0.906\n0.102\n25.02\n0.814\n0.175\nIBRNet-2K\n35.34\n0.984\n0.024\n24.98\n0.913\n0.095\n25.08\n0.815\n0.176\nTable 4.\nIBRNet:\nIBRNet trained from scratch, IBRNet-S:\nIBRNet trained from ScanNet++, IBRNet-270, IBRNet-1K, and\nIBRNet-2K: IBRNet trained from 270 scenes, 1,000 scenes, and\n2,000 scenes from DL3DV-10K. Refer to supplementary materials\nfor more samples on DL3DV-10K.\ntraining dataset used by IBRNet and compare the perfor-\nmance with the train-from-scratch IBRNet on the evaluation\ndatasets used by IBRNet. ScanNet++ [36] is another recent\nhigh-quality real-world scene dataset that focuses on indoor\nscenarios. We conduct a similar experiment on ScanNet++\nto further show that the richer diversity and larger scale of\nDL3DV-10K significantly improve the generalizable NeRFs\nresults.\n(a). GT\n(b). IBR\n(c). IBR-S\n(d). IBR-D\nFigure 6. Qualitative comparison of (a). Ground truth. (b). IBR-\nNet trained from scratch. (c). IBRNet pretrained on ScanNet++\nand (d). IBRNet pretrained on DL3DV-2K\nResults.\nThe quantitative and qualitative results are\nshown in Tab. 4 and Fig. 6. The knowledge learned from\nScanNet++ does not help IBRNet perform better on these\nexisting benchmarks. However, the prior learned from a\nsubset of our DL3DV-10K help IBRNet perform better on\nall the evaluation benchmarks. Also, when an increased\ndata input from DL3DV-10K IBRNet consistently performs\nbetter in all the benchmarks.\n5. Conclusion\nWe introduce DL3DV-10K, a large-scale multi-view scene\ndataset, gathered by capturing high-resolution videos of\nreal-world scenarios. The abundant diversity and the fine-\ngrained scene complexity within DL3DV-140, featuring 140\nscenes from DL3DV-10K, create a challenging benchmark\nfor Neural View Synthesis (NVS). Our thorough statistical\nevaluation of SOTA NVS methods on DL3DV-140 provides\na comprehensive understanding of the strengths and weak-\nnesses of these techniques. Furthermore, we demonstrate\nthat leveraging DL3DV-10K enhances the generalizability\nFigure 5. We compare the SOTA NVS methods and the corresponding ground truth images on DL3DV-140 from held-out test views. More\nexamples can be found in supplementary materials. The scenes are classified by complexity indices: indoor vs. outdoor, more-ref vs.\nless-ref, high-freq vs. low-freq, and more-transp vs. less-transp. Best view by zooming in.\nof NeRF, enabling the development of a universal prior.\nThis underscores the potential of DL3DV-10K in paving the\nway for the creation of a foundational model for learning\n3D representations.\nLimitations.\nDL3DV-10K encompasses extensive real-\nworld scenes, enjoying the coverage of everyday acces-\nsible areas.\nThis rich diversity and scale provide valu-\nable insights for exploring deep 3D representation learning.\nHowever, there are certain limitations. While we demon-\nstrate DL3DV-10K\u2019s potential in static view synthesis, some\nscenes include moving objects due to the nature of mobile\nphone video scene collection, as classified in Tab. 2, thereby\nintroducing additional challenges for NVS. Nonetheless,\nsuch challenges may provide insights into exploring the ro-\nbustness of learning-based 3D models.\nMoreover, these\nchallenges may be solved by future learning-based 3D mod-\nels for dynamic NVS.\nReferences\n[1] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jian-\ning Wei, and Matthias Grundmann. Objectron: A large scale\ndataset of object-centric videos in the wild with pose an-\nnotations. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 7822\u20137831,\n2021. 3\n[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5855\u20135864,\n2021. 2\n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 2, 5, 1\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan,\nand\nPeter\nHedman.\nZip-nerf:\nAnti-\naliased grid-based neural radiance fields.\narXiv preprint\narXiv:2304.06706, 2023. 2, 5, 1\n[5] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry,\nYuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe,\nDaniel Kurz, Arik Schwartz, et al. Arkitscenes: A diverse\nreal-world dataset for 3d indoor scene understanding using\nmobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021.\n3\n[6] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang.\nMatterport3d: Learning\nfrom rgb-d data in indoor environments.\narXiv preprint\narXiv:1709.06158, 2017. 3\n[7] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 3\n[8] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance field reconstruction from multi-view stereo.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 14124\u201314133, 2021. 2, 7\n[9] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard\nPons-Moll. Stereo radiance fields (srf): Learning view syn-\nthesis for sparse views of novel scenes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7911\u20137920, 2021. 7\n[10] Albert Cohen,\nIngrid Daubechies,\nand J-C Feauveau.\nBiorthogonal bases of compactly supported wavelets. Com-\nmunications on pure and applied mathematics, 45(5):485\u2013\n560, 1992. 4\n[11] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes.\nIn\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5828\u20135839, 2017. 3\n[12] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 3\n[13] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\nfree-viewpoint image-based rendering. ACM Transactions\non Graphics (ToG), 37(6):1\u201315, 2018. 7\n[14] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola,\nand Henrik Aan\u00e6s. Large scale multi-view stereopsis eval-\nuation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 406\u2013413, 2014. 3\n[15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 2, 5, 6, 1\n[16] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics (ToG), 36\n(4):1\u201313, 2017. 2, 3\n[17] Marc Levoy and Pat Hanrahan. Light field rendering. In\nSeminal Graphics Papers: Pushing the Boundaries, Volume\n2, pages 441\u2013452. 2023. 2\n[18] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 38(4):1\u201314, 2019. 2, 3, 7\n[19] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 1,\n2, 3, 7\n[20] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 2, 5\n[21] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3d: Large-scale learning and evaluation of\nreal-life 3d category reconstruction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 10901\u201310911, 2021. 3\n[22] Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstra-\ntios Gavves, and Tinne Tuytelaars. Deep reflectance maps.\nIn Proceedings of the IEEE Conference on computer vision\nand pattern recognition, pages 4508\u20134516, 2016. 2\n[23] Konstantinos Rematas, Ricardo Martin-Brualla, and Vittorio\nFerrari. Sharf: Shape-conditioned radiance fields from a sin-\ngle view. arXiv preprint arXiv:2102.08860, 2021. 7\n[24] Thomas Schops, Johannes L Schonberger, Silvano Galliani,\nTorsten Sattler, Konrad Schindler, Marc Pollefeys, and An-\ndreas Geiger.\nA multi-view stereo benchmark with high-\nresolution images and multi-camera videos.\nIn Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 3260\u20133269, 2017. 3\n[25] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks:\nContinuous 3d-\nstructure-aware neural scene representations. Advances in\nNeural Information Processing Systems, 32, 2019. 3\n[26] SuLvXiangXin.\nzipnerf-pytorch.\nhttps://github.\ncom/SuLvXiangXin/zipnerf-pytorch, 2023. 1\n[27] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,\nBrent Yi, Terrance Wang, Alexander Kristoffersen, Jake\nAustin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: A\nmodular framework for neural radiance field development.\nIn ACM SIGGRAPH 2023 Conference Proceedings, pages\n1\u201312, 2023. 2, 3, 5, 1\n[28] Alex Trevithick and Bo Yang. Grf: Learning a general radi-\nance field for 3d representation and rendering. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 15182\u201315192, 2021. 7\n[29] Dan\nWang,\nXinrui\nCui,\nSeptimiu\nSalcudean,\nand\nZ Jane Wang.\nGeneralizable neural radiance fields for\nnovel view synthesis with transformer.\narXiv preprint\narXiv:2206.05375, 2022. 2, 7\n[30] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\nnet: Learning multi-view image-based rendering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4690\u20134699, 2021. 2, 3, 7\n[31] Haozhe Xie, Hongxun Yao, Shengping Zhang, Shangchen\nZhou, and Wenxiu Sun. Pix2vox++: Multi-scale context-\naware 3d object reconstruction from single and multiple im-\nages.\nInternational Journal of Computer Vision, 128(12):\n2919\u20132935, 2020. 3\n[32] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 2\n[33] Hao Yang, Lanqing Hong, Aoxue Li, Tianyang Hu, Zhen-\nguo Li, Gim Hee Lee, and Liwei Wang. Contranerf: Gen-\neralizable neural radiance fields for synthetic-to-real novel\nview synthesis via contrastive learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16508\u201316517, 2023. 2, 7\n[34] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,\nLei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-\nscale dataset for generalized multi-view stereo networks. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 1790\u20131799, 2020. 3\n[35] Mao Ye, Peifeng Yin, Wang-Chien Lee, and Dik-Lun Lee.\nExploiting geographical influence for collaborative point-of-\ninterest recommendation. In Proceedings of the 34th interna-\ntional ACM SIGIR conference on Research and development\nin Information Retrieval, pages 325\u2013334, 2011. 2, 4\n[36] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner,\nand Angela Dai. Scannet++: A high-fidelity dataset of 3d in-\ndoor scenes. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 12\u201322, 2023. 2, 3, 7\n[37] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[38] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 2,\n3, 7\n[39] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu,\nChongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu,\nZhangyang Xiong, Tianyou Liang, et al.\nMvimgnet: A\nlarge-scale dataset of multi-view images. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9150\u20139161, 2023. 3\n[40] Andy Zeng, Shuran Song, Matthias Nie\u00dfner, Matthew\nFisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch:\nLearning local geometric descriptors from rgb-d reconstruc-\ntions. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1802\u20131811, 2017. 4\n[41] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva,\nJoon-Young Lee, Hailin Jin, and Thomas Funkhouser.\nPhysically-based rendering for indoor scene understanding\nusing convolutional neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 5287\u20135295, 2017. 4\n[42] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely.\nStereo magnification:\nLearning\nview synthesis using multiplane images.\narXiv preprint\narXiv:1805.09817, 2018. 2, 3\nDL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision\nSupplementary Material\n6. Overview\nOur supplementary materials include this pdf, video demo\nand a HTML for more qualitative results.\nSec. 7 discusses the data acquisition standard and the\nDL3DV-10K data distribution. Sec. 8 discusses more de-\ntails of our benchmark experiments, including experiment,\ntraining details and more qualitative results, and details of\nthe generalizable NeRF experiment.\n7. Data\n7.1. Data acquisition\nThe scene coverage for video shooting is illustrated in\nFig. 8. For real-world scenes, they encompass horizontal\nviews (180\u25e6 - 360\u25e6) from different heights.\nWe capture\nscenes using 360\u25e6 panoramic views when the scene is ac-\ncessible and well-defined, typically encompassing a diam-\neter that can be covered on foot within 30 to 45 secs. In\ninstances where the rear view of the scene is obstructed by\nlarger objects, such as larger buildings, we opt for a semi-\ncircular view (exceeding 180\u25e6) to capture the scene. To en-\nhance scene coverage, we record videos by traversing two\ncircular or semi-circular paths. The first traversal is con-\nducted at overhead height, while the second is performed at\napproximately waist height.\n7.2. Labeling\nReflection and Transparency\nWe manually annotate re-\nflection and transparency indices to scenes by assessing the\nratio of reflective (transparent) pixels and the duration of\nreflectivity (transparency) observed in the video.\nFig. 7\npresents the reflection labeling criteria. Transparency la-\nbeling follows the same rule.\n7.3. Data Statistics\nScene summary by secondary POI category.\nThe sec-\nondary POI categories are detailed classes within the pri-\nmary POI categories. Fig. 9 shows scene statistics for each\nsecondary POI category and the corresponding primary POI\ncategory.\nFig. 10 presents scene statistics for each sec-\nondary POI category by complexity indices such as environ-\nmental setting, light condition, and level of reflection and\ntransparency. For example, in the \u2019light condition\u2019 attribute,\nwe find that scenes from \u2019supermarkets\u2019, \u2019shopping-malls,\nand \u2019furniture-stores\u2019 are mostly under artificial lighting,\nwhereas \u2019hiking-trails and \u2019parks-and-recreation-areas\u2019 are\nunder natural light. As for \u2019reflection\u2019 and \u2019transparency\u2019\nattributes, \u2019shopping-malls\u2019 are more likely to feature fully\nFigure 7. Reflection labeling criteria. Transparency annotation\nlikewise.\nreflective scenes than other locations, while nature & out-\ndoor scenes such as \u2019hiking-trails\u2019 are predominantly non-\nreflective scenes. Most scenes are non-transparent. These\nobservations align well with common expectations in real-\nworld scenarios. We present a sample of DL3DV-10K in the\nvideo demo.\nFrequency and duration estimates.\nThe kernel density\ndistribution of frequency metric and video duration can be\nfound in Fig. 11. The frequency classes are delineated based\non the median value of the frequency metric.\n8. Experiment\n8.1. NVS benchmark\nExperiment Details\nThe implementation of Nerfacto and\nInstant-NGP is from nerfstudio [27]. MipNeRF360 [3] and\n3D gaussian splatting (3DGS) [15] codes are from the au-\nthors. ZipNeRF [4] source code is not public yet when we\nsubmit the paper. We used a public implementation [26] that\nshows the same performance results reported in the paper to\ntest ZipNeRF.\nThe default ray batch is 4096.\nZipNeRF is sensitive\nto this parameter, and we also showed 65536 (default by\nZipNeRF) results. Nerfacto, Instant-NGP, ZipNeRF used\nhalf-precision fp16 while 3DGS and MipNeRF360 use full\nprecision. All the NeRF-based methods use the same near\n(0.01) and the same far (1e5). The codes are run on A30,\nV100 and A100 GPUs depending on the memory they used.\nAll the experiments took about 13,230 GPU hrs to finish.\nFigure 8. Video shooting examples with different heights and angles.\nRestaurant\nCoffee-Shop\nBars-and-Pubs\nResidential-area\nHotels-Motels\nVaccation-Rentals\nHospitals-Clinics\nPharmacies\nDentists\nSchool-Universities\nLibrary\nSupermarkets\nShopping-Malls\nClothing-Stores\nShoe-Stores\nBookstores\nFlowerstore\nFurniture-Stores\nElectorical-Store\nPet-Store\nToy-Shop\nAirports\nTrain-Stations\nBus-Stops\nGas-Station\nCar-Rental-Agencies\nTheaters\nConcert-Halls\nSports-Stadiums\nParks-and-Recreation-Areas\nMuseums\nArt-Galleries\nZoos-Aquariums\nBotanical-Gardens\nLandmarks\nCultural-Centers\nPost-Offices\nPolice-Stations\nCourthouses\nCity-Halls\nBanks-ATMs\nEvents-Conferences-halls\nBeaches\nHiking-Trails\nCampgrounds\nLakes\nMountains\nForest-Mountains\nFarms\nStreet-View\nSquare\nBusiness-Centers\nTech-Companies\nCo-working-Spaces\nGyms-and-Fitness-Centers\nSports-Clubs\nSwimming-Pools\nTennis-Courts\nAuto-Repair-Shops\nCar-Washes\nParking-Lots\nChurches\nMosques\nTemples\nGraveyards\nSecondary Category\n0\n200\n400\n600\n800\nNumber of videos\nScene category by location-based POIs\nPOI\nRestaurants and Cafes\nHotels and Accommodations\nMedical Facilities\nEducation Institutions\nShopping Centers\nTransportation Hubs\nParks and Recreation\nTourist Attractions\nGovernment & Civic Services\nFinancial Services\nEvents & Conferences\nNature & Outdoors\nTech & Business\nSports & Fitness\nAutomotive Services\nReligious Institutions\nFigure 9. Number of scenes within secondary POI category. The legend contains the mapping between the primary and secondary POI\ncategories. We observe that \u2019schools-universities and \u2019residential-area\u2019 are the predominant scenes in our DL3DV-10K dataset. In contrast,\nlocations such as government and civic service facilities (e.g., \u2019post office, \u2019police station, \u2019court house, and \u2019city hall) are less frequently\ncaptured due to the challenges in accessing these areas for detailed video recording.\nMore quantitative results.\nWe present the performance\nof State-of-the-art (SOTAs) in DL3DV-140 by scene pri-\nmary POI categories in Fig. 12.\nMore visual results.\nWe present more visual results for\nthe performance of SOTAs on DL3DV-140 by scene com-\nplexity indices.\nIn particular, Fig. 13 describes the per-\nformance of SOTAs by environmental setting; Fig. 14 de-\nscribes the performance of SOTAs by frequency; Fig. 15\ndescribes the performance of SOTAs by transparency; and\nFig. 17 describes the performance of SOTAs by reflection.\nDue to the file size limitation, we provide visual results for\n70 scenes in DL3DV-140 in the HTML supplementary sub-\nmission.\n8.2. Generalizable NeRF\nExperiment details\nWe follow the default setting by IBR-\nNet [30]. The training dataset includes LLFF [18], spaces,\nRealEstate10K [42] and self-collected small dataset by\nIBRNet authors. The evaluation dataset includes Diffuse\nSynthetic 360\u25e6 [25], Realistic Synthetic 360\u25e6 [19], part of\nLLFF that was not used in training. We used the official im-\nplementation. Each experiment was trained on single A100\nGPU. Pretaining on Scannet++ and DL3DV-10K took 24\nhrs.\n0\n200\n400\n600\n800\nScene Count\nCar-Rental-Agencies\nConcert-Halls\nPolice-Stations\nToy-Shop\nDentists\nShoe-Stores\nCourthouses\nPost-Offices\nPet-Store\nBeaches\nMosques\nCar-Washes\nCity-Halls\nGas-Station\nSwimming-Pools\nBanks-ATMs\nBars-and-Pubs\nGraveyards\nForest-Mountains\nAirports\nCoffee-Shop\nTheaters\nChurches\nSports-Stadiums\nFarms\nSports-Clubs\nTennis-Courts\nVaccation-Rentals\nZoos-Aquariums\nBusiness-Centers\nEvents-Conferences-halls\nFlowerstore\nBus-Stops\nTemples\nBookstores\nAuto-Repair-Shops\nTech-Companies\nPharmacies\nElectorical-Store\nGyms-and-Fitness-Centers\nClothing-Stores\nHotels-Motels\nHospitals-Clinics\nLandmarks\nLibrary\nTrain-Stations\nCampgrounds\nRestaurant\nLakes\nParking-Lots\nCo-working-Spaces\nBotanical-Gardens\nMountains\nShopping-Malls\nArt-Galleries\nMuseums\nSquare\nFurniture-Stores\nStreet-View\nParks-and-Recreation-Areas\nCultural-Centers\nHiking-Trails\nSupermarkets\nResidential-area\nSchool-Universities\nSecondary POI Category\nScene complexity by environment\nEnvironment\nindoor\noutdoor\n0\n200\n400\n600\n800\nScene Count\nScene complexity by light condition\nLight condition\nalight\nmlight\nnlight\n0\n200\n400\n600\n800\nScene Count\nScene complexity by reflection\nReflection class\nless\nmedium\nmore\nnone\n0\n200\n400\n600\n800\nScene Count\nScene complexity by transparency\nTransparency class\nless\nmedium\nmore\nnone\nFigure 10. We show the distribution of scenes captured in secondary POI categories by complexities, including environmental setting, light\nconditions, reflective surfaces, and transparent materials.\n0\n50\n100\n150\n200\n250\n300\nVideo Duration (seconds per video)\n0.00\n0.01\n0.02\n0.03\nKernel Density Estimate (KDE)\nDensity Plot of Duration\nMedian: 6.95e+01\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFrequency metric\n1e 5\n0\n100000\n200000\n300000\nKernel Density Estimate (KDE)\nDensity Plot of Frequency\nMedian: 2.60e-06\nFigure 11. We show the distribution of video duration and frequency metric in 10,510 videos. The minimum duration for video shooting\nwith consumer mobile devices is set at 60 secs, while for drone cameras, it\u2019s at least 45 secs. In our dataset, the median video duration is\n69.5 secs. Furthermore, the median value of the frequency metric, determined by the average image intensity, stands at 2.6e-06. Based on\nthis median value, we categorize scenes into high frequency (\u2019high freq\u2019) and low frequency (\u2019low freq\u2019) classes.\nEducation Institutions\nNature & Outdoors\nRestaurants and Cafes\nShopping Centers\nTourist Attractions\nTransportation Hubs\n20\n25\n30\n35\nPSNR\n33.4\n27.8\n32\n30.1\n29\n28.6\n34.3\n29.6\n33\n31.1\n30.6\n29.9\n34.8\n28.1\n33.6\n30.8\n29.9\n30.4\n27.3\n23.1\n25.7\n25.1\n25.2\n24.1\n27.1\n22.3\n25.6\n24.8\n24.6\n23.1\nEducation Institutions\nNature & Outdoors\nRestaurants and Cafes\nShopping Centers\nTourist Attractions\nTransportation Hubs\n0.8\n0.9\n1.0\nSSIM\n0.95\n0.89\n0.95\n0.93\n0.9\n0.9\n0.95\n0.9\n0.94\n0.93\n0.91\n0.9\n0.95\n0.83\n0.95\n0.92\n0.88\n0.9\n0.91\n0.79\n0.87\n0.86\n0.83\n0.82\n0.9\n0.78\n0.89\n0.85\n0.81\n0.8\n3DGS\nZip-NeRF\nMip-NeRF 360\nNerfacto\nInstant-NGP\nFigure 12. We present the average performance on 6 primary POI categories (Education institutions, Nature & Outdoors, Restaurants and\nCafes, Shopping Centers, Tourist Attractions, and Transportation Hubs) in the DL3DV-140. The text above the bar plot is the mean value of\nthe methods on the primary POI categories. As shown in the figure, NVS methods have better performance on scenes captured in Education\ninstitutions, Restaurants and Cafes, Shopping Centers than Tourist Attractions, Transportation Hubs, and Nature & Outdoors. Because\nmajority scenes in Education institutions, Restaurants and Cafes, and Shopping Centers are indoor scenes. Additionally, the performance\non Shopping Centers is worse than textitEducation institutions and Restaurants and Cafes.\nGround Truth\nZip-NeRF\n3DGS\nMip-NeRF 360\nNerfacto\nInstant-NGP\nindoor\noutdoor\noutdoor\noutdoor\nFigure 13. We compare the SOTAs for indoor (bounded) and outdoor (unbounded) environments on DL3DV-140 from held-out test views.\nAs illustrated in the figure, indoor scenes pose fewer challenges compared to outdoor scenes, where SOTAs demonstrate varying levels\nof performance. We observe that outdoor scene is more challenging for 3D Gaussian Splatting (3DGS), Nerfacto, and Instant-NGP than\nZip-NeRF and Mip-NeRF 360.\nFigure 14. We compare the performance of SOTAs in frequency (low freq vs. high freq) on DL3DV-140 from held-out test views. As\nshown in the figure, high frequency (high freq) scene is more challenging than low frequency (low freq) scene. We observe that 3DGS\nconsistently captures scenes with high-frequent details and renders the shape edge for the scene details. As for NeRF variants, it is more\nchallenging for Nerfacto and Instant-NGP to handle scenes with high-frequent details than Zip-NeRF and Mip-NeRF 360. Besides, NeRF\nvariants suffer aliasing issues.\nGround Truth\nZip-NeRF\n3DGS\nMip-NeRF 360\nNerfacto\nInstant-NGP\nless_transp\nmore_transp\nmore_transp\nmore_transp\nFigure 15. We compare the performance of SOTAs for transparency classes (less transp vs. more transp) on DL3DV-140 from held-\nout test views. As shown in the figure, scenes with more transparent materials (more transp) are more challenging than scenes with less\ntransparent materials (less transp). In our analysis of the selected scenes, we noted that 3DGS, Zip-NeRF, and Mip-NeRF 360 effectively\ncapture the subtle edges of transparent objects. Conversely, Nerfacto and Instant-NGP tend to consistently generate artifacts.\nIBRNet\nIBRNet-S\nIBRNet-D\nGT\nFigure 16. More qualitative results for generalizable NeRF. IBRNet-S: pretrain IBRNet on Scannet++ [36]. IBRNet-D: pretrain IBRNet\non DL3DV-10K. Priors learned from DL3DV-10K help IBRNet perform the best on the evaluation.\nFigure 17. We compare the SOTAs for reflection classes (less ref vs. more ref) on DL3DV-140 from held-out test views. As shown in\nthe figure, scenes with more reflective surfaces (more ref) are more challenging than scenes with less reflective surfaces (less ref). Among\nSOTAs, Zip-NeRF and Mip-NeRF 360 are adept at capturing subtle reflections and highlights. On the other hand, 3DGS tends to overly\nsmooth out less intense reflections. Nerfacto and Instant-NGP struggle to effectively manage scenes with highly reflective surfaces, often\nresulting in the generation of artifacts.\n"
  },
  {
    "title": "City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web",
    "link": "https://arxiv.org/pdf/2312.16457.pdf",
    "upvote": "13",
    "text": "City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web\nKaiwen Song1,2\nJuyong Zhang1\n1University of Science and Technology of China\n2Real Infinity Inc\nAbstract\nNeRF has significantly advanced 3D scene reconstruc-\ntion, capturing intricate details across various environ-\nments. Existing methods have successfully leveraged radi-\nance field baking to facilitate real-time rendering of small\nscenes.\nHowever, when applied to large-scale scenes,\nthese techniques encounter significant challenges, strug-\ngling to provide a seamless real-time experience due to\nlimited resources in computation, memory, and bandwidth.\nIn this paper, we propose City-on-Web, which represents\nthe whole scene by partitioning it into manageable blocks,\neach with its own Level-of-Detail, ensuring high fidelity,\nefficient memory management and fast rendering. Mean-\nwhile, we carefully design the training and inference pro-\ncess such that the final rendering result on web is con-\nsistent with training.\nThanks to our novel representa-\ntion and carefully designed training/inference process, we\nare the first to achieve real-time rendering of large-scale\nscenes in resource-constrained environments. Extensive ex-\nperimental results demonstrate that our method facilitates\nreal-time rendering of large-scale scenes on a web plat-\nform, achieving 32FPS at 1080P resolution with an RTX\n3060 GPU, while simultaneously achieving a quality that\nclosely rivals that of state-of-the-art methods. Project page:\nhttps://ustc3dv.github.io/City-on-Web/.\n1. Introduction\nNeRF has significantly advanced the field of scene recon-\nstruction, showing an unparalleled ability to capture com-\nplex details across diverse environments. Existing works\nhave demonstrated its ability to render small scenes with\nexceptional quality and performance in real-time [3, 6, 11,\n21, 24, 29, 36, 40, 43, 46\u201348]. NeRF has also been suc-\ncessfully applied to the rendering of large scenes in offline\nsettings, achieving exceptional visual fidelity and generat-\ning intricately detailed results [14, 34, 37, 43, 45].\nDespite these successes, real-time neural rendering of\nlarge scenes is profoundly challenging due to inherent\ncomputational power, memory, and bandwidth limitations\nacross various devices. The challenges mainly include the\nfollowing aspects. Firstly, traditional NeRF and its variants\nare resource-intensive, requiring substantial computational\npower that exceeds what is typically available in such con-\nstrained environments. Secondly, the video memory capac-\nity on client devices is frequently limited, imposing signifi-\ncant restrictions on the capability to process and render sub-\nstantial assets in real-time simultaneously. The substantial\nresource becomes a critical issue in the real-time rendering\nof large scenes, necessitating the quick loading and process-\ning of extensive data sets. Lastly, the dependency on data re-\ntrieval from remote servers introduces latency, particularly\nunder network bandwidth limitations, further complicating\nthe real-time rendering process. These hurdles collectively\nform a significant barrier to delivering an uninterrupted and\ninstantaneous visual experience for large-scale scenes.\nTo address these challenges in real-time rendering of\nlarge-scale scenes, we propose our proposed method, City-\non-Web.\nDrawing inspiration from traditional graphics\ntechniques for rendering large-scale scenes [7\u201310, 15, 23,\n38], we partition the scene into manageable blocks and rep-\nresent the scene with varying Levels-of-Detail (LOD). We\nutilize radiance field baking techniques [16, 30], which pre-\ncompute and store rendering primitives into 3D atlas tex-\ntures organized in a sparse grid within each block for real-\ntime rendering. However, due to the unavoidable texture\nresource limitations of shaders, we cannot load all the at-\nlas textures into a single shader. Hence, we represent the\nscene as a hierarchy of segmented blocks, each rendered by\na dedicated shader during rendering.\nOur block partition and LOD for scene representation\nbring major benefits for real-time rendering in environments\nwith limited computing resources, memory, and bandwidth.\n(1) High-Fidelity Reconstruction. With divide and con-\nquer strategy, we ensure that each block possesses suffi-\ncient representation ability to reconstruct fine details within\nthe scene faithfully. Additionally, to ensure high fidelity in\nthe rendered output during training, we simulate the blend-\ning of multiple shaders that are aligned with the rendering\npipeline. (2) Efficient Resource Management. The block\nand LOD-based representation facilitates dynamic resource\narXiv:2312.16457v1  [cs.CV]  27 Dec 2023\nmanagement. It simplifies the loading and unloading pro-\ncess, adapting to the viewer\u2019s position and field of view in\nreal-time. This dynamic load strategy greatly mitigates the\nbandwidth and memory demands typically associated with\nlarge-scale scene rendering, paving the way for smoother\nuser experiences even on less capable devices.\n(3) Fast\nRendering. Despite the abundance of resources required\nfor rendering large scenes, we ensure real-time rendering ef-\nficiency by dividing the scene into non-overlapping blocks,\nwith each shader granted access to resources within its des-\nignated block only. This block-rendering approach guar-\nantees that performance does not degrade linearly with in-\ncreased resources, even if we divide the scene into dozens of\nblocks. Our experiments demonstrate that City-on-Web can\nrender photo-realistic large-scale scenes at 32FPS at 1080p\nresolution with an RTX 3060 GPU and uses only 18% of the\nVRAM and 16% of the payload size compared to current\nmesh-based methods [6, 46]. As our model maintains con-\nsistency between training and rendering, we have achieved\nsimilar reconstruction quality compared to state-of-the-art\nmethods. To our knowledge, we are the first to achieve real-\ntime neural rendering of large-scale scenes on the web.\n2. Related Work\nLarge-scale Scene Reconstruction.\nFor radiance field\nreconstruction of large-scale scenes, a key issue lies in en-\nhancing the model\u2019s representational capacity to adequately\ncapture and render extensive scenes. Block-NeRF [34] and\nMega-NeRF [37] address this by adopting a divide-and-\nconquer strategy, segmenting expansive scenes into smaller\nblocks, and applying localized NeRF processing to each.\nThis approach significantly improves both the reconstruc-\ntion quality and the model\u2019s scalability to larger scenes.\nSwitch-NeRF [50] employs a gating network to dispatch\n3D points to different NeRF sub-networks. Grid-NeRF [45]\nutilizes a compact multiresolution feature plane and com-\nbines the strengths of smoothness from vanilla NeRF with\nthe local detail capturing ability of feature grid-based meth-\nods [5, 26, 31], efficiently reconstructing large scenes with\nfine details. NeRF++ [48] enhances the reconstruction of\nunbounded scenes through its innovative multi-spherical\nrepresentation. On the other hand, Mip-NeRF 360 [1] in-\ntroduces a scene contraction function to effectively repre-\nsent scenes that extend to infinity, addressing the challenge\nof vast spatial extents. F2-NeRF [41] takes this a step fur-\nther by implementing a warping function for local spaces,\nensuring a balance of computational resources and training\ndata across different parts of the scene.\nReal-time Rendering.\nEarly works mainly focus on the\nreal-time rendering of a simple single object. NSVF [21]\nimproves NeRF by introducing a more efficient sparse voxel\nfield, significantly accelerating rendering speed while main-\ntaining high-quality output. KiloNeRF [29] utilizes thou-\nsands of small MLPs, each responsible for a tiny scene\nregion, significantly reducing network evaluation time. In\ncontrast, SNeRG [16] leverages pre-computed sparse grids,\nallowing for direct retrieval of radiance field information\nwithout needing network evaluation. Termi-NeRF [28] ter-\nminates ray marching in less impactful scene regions, slash-\ning computation time. DONeRF [27] focuses on one sam-\nple using a depth oracle network, speeding up rendering\nwhile preserving scene quality. Recently, there have been\ndevelopments that enable real-time rendering of neural ra-\ndiance fields in small scenes. MERF [30] improves upon\nSNeRG by utilizing a voxel and triplane hybrid represen-\ntation to reduce memory usage.\nMobileNeRF [6] intro-\nduces the polygon rasterization rendering pipeline, running\nNeRF-based novel view synthesis in real-time on mobile\ndevices. BakedSDF [46] bakes volumetric representation\ninto meshes and utilizes spherical harmonics for represent-\ning view-dependent color, while NeRF2Mesh [36] itera-\ntively refine both the geometry and appearance of the mesh.\nLevel of Detail.\nSubstantial works are devoted to integrat-\ning LOD methods into the fabric of traditional computer\ngraphics [7\u20139, 15, 17, 20, 22, 23], aiming to streamline ren-\ndering processes, reduce memory footprint, bolster interac-\ntive responsiveness. Recently, some works begin to apply\nLOD to the neural implicit reconstruction. NGLoD [32]\nrepresents LOD through a sparse voxel octree, where each\nlevel of the octree corresponds to a different LOD, allowing\nfor a finer discretization of the surface and more detailed re-\nconstruction as the tree depth increases. Takikawa et al. [33]\nefficiently encode 3D signals into a compact, hierarchical\nrepresentation using vector-quantized auto decoder method.\nBungeeNeRF [43] employs a hierarchical network struc-\nture, where the base network focuses on learning a coarse\nrepresentation of the scene, and subsequent residual blocks\nare tasked with progressively refining this representation.\nTrimipRF [18] and LoD-Neus [51] leverage multi-scale tri-\nplane and voxel representations to capture scene details at\ndifferent scales, effectively implementing anti-aliasing to\nenhance the rendering and reconstruction quality.\n3. Background and Motivation\nOur exploration begins with an in-depth analysis of two in-\nfluential works, SNeRG [16] and MERF [30], which have\nboth set benchmarks for real-time rendering of radiance\nfield. SNeRG precomputes and stores a Neural Radiance\nFields model in a sparse 3D voxel grid. Each active voxel\nin SNeRG contains several attributes: density, diffuse color,\nand specular feature vector that captures view-dependent ef-\nfects. Additionally, an indirection grid is used to enhance\nrendering by either indicating empty macroblocks or point-\ning to detailed texels in a 3D texture atlas. This representa-\n\ud835\udc8a\n\ud835\udc8b\n\ud835\udc8c\nz\n\ud835\udc8a\n\ud835\udc8c\n\ud835\udc8b\n1\n3\n2\n4\n\ud835\udefc\ud835\udc8c\n\ud835\udc84\ud835\udc8c\n\ud835\udefc\ud835\udc8b\n\ud835\udc84\ud835\udc8b\n\ud835\udefc\ud835\udc8a\n\ud835\udc84\ud835\udc8a\n\ud835\udefc\ud835\udc8c\n\ud835\udc84\ud835\udc8c\n\ud835\udc6a \ud835\udc93 = \u0dcd\n\ud835\udc58=1\n\ud835\udc40\n\u0dd1\n\ud835\udc57=1\n\ud835\udc58\u22121\n(1 \u2212 \ud835\udefc\ud835\udc57)\ud835\udc84\ud835\udc8c\n\ud835\udc6a\ud835\udc8c\n\ud835\udc87\ud835\udc24\n\ud835\udf48\ud835\udc8a\n\ud835\udc8c\n\ud835\udc6a\ud835\udc8a\ud835\udc8c\n\ud835\udc6d\ud835\udc8a\ud835\udc8c\n\ud835\udc91\ud835\udc8a\n\ud835\udc93 \ud835\udc95 = \ud835\udc90 + \ud835\udc95\ud835\udc85\n\ud835\udc6a\ud835\udc85\n\ud835\udc8c\n\ud835\udc6d\ud835\udc8c\n\ud835\udc85\n\ud835\udf36\ud835\udc8c\nFigure 1. Pipeline of training. (a) We divide the entire scene into different blocks without overlap according to the ground plane. (b) For\nblock k that the ray r(t) passes through, the corresponding shader renders the color ck and opacity \u03b1k of each block. We depth sort the\nblocks that ray traversed, and then render the final result through alpha blending that maintains 3D consistency.\ntion allows real-time rendering on standard laptop GPUs.\nThe indirection grid assists in raymarching through the\nsparse 3D grid by passing empty regions and selectively ac-\ncessing non-zero densities \u03c3i, diffuse colors ci, and fea-\nture vectors f i during rendering. Integrating along each ray\nr(t) = o + td, we compute the sum of the weights, which\ncan be considered as the pixel\u2019s opacity:\n\u03b1(r) =\nX\ni\nwi, wi =\ni\u22121\nY\nj=1\n(1\u2212\u03b1j)\u03b1i, \u03b1i = 1\u2212e\u2212\u03c3i\u03b4i. (1)\nThe color Cd(r) and specular feature F s(r) along the ray\nare accumulated using the same weights to compute the fi-\nnal diffuse color and specular feature of ray:\nCd(r) =\nX\ni\nwici,\nF s(r) =\nX\ni\nwif i.\n(2)\nThe step size during ray marching \u03b4i is equal to the\nvoxel width for an occupied voxel. Subsequently, the ac-\ncumulated diffuse color and specular feature vector, along\nwith the positional encoding PE(\u00b7) of the ray\u2019s view di-\nrection, are concatenated to pass through a lightweight de-\nferred MLP \u03a6 to produce a view-dependent residual color:\nC(r) = Cd + \u03a6(Cd, F s, PE(d)).\n(3)\nWhile SNeRG achieves impressive real-time rendering\nresults, its voxel representation demands substantial mem-\nory, which poses limitations for further applications. MERF\npresents a significant reduction in memory requirements in\ncomparison to extant radiance field methods like SNeRG.\nBy leveraging hybrid low-resolution sparse grid and 2D\nhigh-resolution triplanes, MERF optimizes the balance be-\ntween performance and memory efficiency. Moreover, it in-\ncorporates two pivotal strategies to bridge the gap between\ntraining and rendering performance. First, MERF simulates\nfinite grid approach during training, querying MLPs at vir-\ntual grid corners and applying interpolation to mimic the\nrendering process closely. Second, MERF simulates quan-\ntization during training and employs the straight-through\nestimator [2], allowing for the simulation of the quanti-\nzation process while maintaining differentiability, enabling\nthe model to learn and optimize with quantized values with-\nout introducing non-differentiable steps during the back-\nward pass, ensuring a smooth training process.\nThese innovative methods for scene reconstruction of-\nfer promising results, but their direct applicability to large\nscenes remains challenges. MERF\u2019s hybrid voxel-triplane\nrepresentation, despite being memory-efficient, cannot cap-\nture large scenes with intricate details due to its fixed reso-\nlution constraint. In our efforts to reconstruct large scenes\nwith high fidelity, dividing them into smaller blocks is a\npractical solution. This method helps in making sure the\nreconstruction is detailed and accurate.\nHowever, this approach of dividing scenes means we will\nend up with more assets to deal during rendering.\nWeb\nbrowsers have limits on how much memory they can use,\nwhich can make it hard to show these many pieces of large\nand detailed models. Another challenge is the data trans-\nmission during web rendering. It often requires pulling data\nfrom servers, and this can be slow due to network delays.\nAs a result, users might experience long wait times when\ntrying to load all the detailed pieces of a large scene at once\nfor rendering. Drawing inspiration from traditional mesh\ndynamic resource loading and LOD [39], we generate LOD\nfrom our segmented reconstruction results which helps in\nminimizing the load of distant resources. Additionally, by\nemploying a dynamic loading strategy for blocks, we sig-\nnificantly reduce VRAM usage and decrease the wait time\nfor resource transmission.\n4. Method\nIn this section, we present a method for representing and\nrendering large scenes on the web. Our approach uses hi-\nerarchical spatial partitioning and LOD to manage large-\nscale scenes dynamically (Sec. 4.1). We align the training\nand rendering stages to ensure consistency (Sec. 4.2), em-\nploying multiple shaders and alpha blending for seamless\nintegration of scene blocks. Additionally, the framework\nincludes optimization strategies (Sec. 4.3) and a process for\ngenerating LODs (Sec. 4.4) and baking the model (Sec. 4.5)\nfor real-time rendering.\n4.1. Large-scale Radiance Field\nIn the realm of scene reconstruction and rendering, NeRF\nhas made significant strides, achieving compelling results.\nHowever, it faces inherent challenges when tasked with rep-\nresenting large scenes on the web. Using a single model to\nrepresent such vast scenes proves challenging due to its lim-\nited expressiveness, particularly in achieving a detailed and\naccurate reconstruction. Representing scenes with multiple\nmodels in a single resolution increases the overhead dur-\ning rendering, leading to the loading of numerous resources,\nwhich is not conducive to efficient rendering.\nTo efficiently represent large scenes captured using the\nfly-through method, we employ hierarchical spatial parti-\ntioning combined with LOD to represent the scene. Specif-\nically, we uniformly partition the area into varying blocks\nwithin our region of interest on the xy plane (i.e., the ground\nplane).\nEach set of partitioned blocks corresponds to a\nunique LOD level, allowing for dynamic and efficient repre-\nsentation. Within each block, we use a low resolution voxel\nwith a high resolution triplane that stores density, diffuse\ncolor, and specular color, to represent the radiance field for\nweb rendering. Additionally, for blocks along the periphery,\nwe utilize the scene contraction function from MERF [30]\nto account for the data on the boundaries.\nFor internal\nblocks, we simply adopt settings from the bounded scene.\nDuring the training stage, our scene representation aligns\nwith web rendering. We consistently utilize spatial parti-\ntioning to structure the scene into distinct blocks without\noverlap. However, we train the scene only with the finest\nlevel of LOD. Within block k , the following trainable com-\nponents are introduced: (1) f k: an attribute query function\nwhich adopts a hash encoding and a mlp decoder that out-\nputs attributes of points such as densities, diffuse color and\nspecular feature (2) \u03a6k: a deferred MLP accounts for view-\ndependent effects. (3) \u03c8k: a proposal MLP for sampling.\n4.2. Consistent Training and Rendering\nIt is essential to ensure consistency between the training and\nrendering stages to achieve high-fidelity rendering results\non the web as obtained during training. Due to the limited\nnumber of texture units within the web rendering environ-\nment, we are compelled to create multiple shaders to ren-\nder distinct blocks. Specifically, one shader is allocated for\nstoring the texture of an individual block. Each block sub-\nsequently renders an image respective to the current cam-\nera view. However, a simplistic averaging of these resultant\nrendering outputs can lead to discernible seams and does\nnot ensure 3D consistency at the inter-block boundaries.\nTo address this problem, we simulate the process of mul-\ntiple shaders rendering images and then linearly weight-\ning them together using the volume rendering weights of\nblocks. For ray r(t), we uniformly sample between the\nnear and far boundaries based on the scene\u2019s bounding box.\nThen, according to the sample coordinates, we query the\ncorresponding block\u2019s proposal MLP to obtain the sample\ndensity, which is transformed into probability distributions\nalong the rays. These probabilities guide a resampling strat-\negy, ensuring a concentration on near-surface features with\na few samples. Assuming that the proposal MLP yields\nsamples passing through M blocks with a total of N sam-\nples, where each block k has nk samples, we simulate\nweb rendering by performing volume rendering within each\nblock to obtain its individual rendering diffuse color Ck\nd,\nspecular feature F k and opacity \u03b1k according to Eqs. (1)\nand (2). Then we get block k final rendering color Ck ac-\ncording to Eq. (3). Consequently, for the sake of 3D con-\nsistency in rendering, we depth-sort the blocks and apply\nvolume rendering across multiple blocks in sequence, using\n\ud835\udc98\n\ud835\udc95\n\ud835\udc98\nFigure 2.\nVisualization of alpha blending during consistent\ntraining. The left image shows the rendering results of four sep-\narate blocks by their shaders and the combined image after alpha\nblending, demonstrating how alpha blending correctly handles oc-\nclusion relationships. The right image visualizes the weights of\nrendering sample points before and after alpha blending.\nThis\nillustration shows the correct occlusion management achieved\nthrough depth sorting of blocks followed by alpha blending.\nopacity to generate the volume rendering weights:\nC(r) =\nM\nX\nk\nk\u22121\nY\nj=1\n(1 \u2212 \u03b1j)Ck.\n(4)\nUnder the Lambertian surface setting where the specular\ncolor is zero, the diffuse color and feature vector obtained\nfrom volume rendering on the total of N ray samples\nfrom Eq. (2) are equal to the results produced by our ap-\nproach of conducting volume rendering within each block\nfollowed by inter-block volume rendering Eq. (4).\nThe\nproof is given in the supplementary. Thus, our rendering\napproach maintains 3D consistency and simulates multiple\nshader rendering on the web as shown in Fig. 2.\nDuring rendering, for the sample point pi, we need to\naccess the voxel grid corner and triplane grid corner where\nthe point is located and use interpolation to obtain the sam-\nple point\u2019s attributes.\nDuring training, we also simulate\nthe voxel and triplane grid points to maintain consistency\nwith the rendering process. By using grid corners\u2019 posi-\ntions to query the attribute query function fk, we obtain the\nattributes of the grid corners. Through interpolation, we ac-\nquire the attributes of the sample points in a manner similar\nto the rendering pipeline. This simulaition strategy ensures\nthat the values used for volume rendering within each block\nduring training are as closely matched as possible to the val-\nues queried from the baked textures.\n4.3. Optimization\nWe use Charbonnier loss [4] for reconstruction and S3IM\nloss [44] to assist multiple blocks\u2019 model in capturing high-\nfrequency details. Additionally, we use the interlevel loss to\nprovide supervision signal for proposal MLP and distortion\nloss to reduce floaters like Mip-nerf 360 [1].\nMoreover, we random uniform sample points set P\nwithin the bounding box of the scene and apply L1 regu-\nlarization on the alpha to encourage the blocks\u2019 model to\npredict sparse occupied space:\nLsparse =\n1\n|P|\nX\npi\u2208P\n|\u03b1i| =\n1\n|P|\nX\npi\u2208P\n|1 \u2212 \u03c3iv|,\n(5)\nwhere v is the step size used in real-time rendering. Addi-\ntionally, we introduce a regularization term for the opacity\nof the block. This regularization encourages the opacity of\nthe block to be as close to 0 or 1 as possible, implying either\nfull transparency or full opaqueness:\nLopacity = \u2212\nX\nk\n(\u03b1klog(\u03b1k) + (1 \u2212 \u03b1k)log(1 \u2212 \u03b1k)). (6)\nIn summary, the overall loss function is:\nLtrain = Lcharbonnier + \u03bb1LS3IM + \u03bb2Linterlevel+\n\u03bb3Ldistortion + \u03bb4Lsparse + \u03bb5Lopacity.\n(7)\n4.4. LOD Generation\nTo guarantee superior rendering quality from elevated per-\nspectives and simultaneously diminish the resource demand\nfor distant scene elements, our method involves generating\nmultiple LOD for the scene. We successfully attained the\nscene\u2019s lowest LOD in the training stage, ensuring maximal\nvisual fidelity. For the downsampling and integration bak-\ning of multiple block models into a unified model, we ini-\ntially freeze the training of hash encoding and decoder MLP\ncomponents within these models. Subsequently, we pro-\nceed to retrain a new tiny deferred MLP. Following the de-\nferred MLP\u2019s successful retraining, we simulate lower res-\nolution virtual voxels and triplane grid corners within the\nscenes of these multiple blocks. Lastly, this retrained de-\nferred MLP is refined in collaboration with the network re-\nsponsible for generating grid corner attributes, thereby op-\ntimizing the entire rendering process.\n4.5. Baking\nAfter the training stage, we conduct block-based evaluation\nand store the MLP\u2019s outputs onto discrete grids, which gen-\nerate segmented rendering resources. This approach facil-\nitates efficient resource management for real-time render-\ning, as each block\u2019s resources are handled independently.\nInitially, we render all training rays to collect ray sam-\nples. Samples with alpha and weight values above a cer-\ntain threshold are retained, and samples below the threshold\nare discarded. The preserved samples are used to mark the\nMethod\nBlockA\nBlockE\nCampus\nRubble\nBuilding\nMetric\nPSNR\u2191 LPIPS\u2193 SSIM\u2191\nPSNR\u2191 LPIPS\u2193 SSIM\u2191\nPSNR\u2191 LPIPS\u2193 SSIM\u2191\nPSNR\u2191 LPIPS\u2193 SSIM\u2191\nPSNR\u2191 LPIPS\u2193 SSIM\u2191\nNeRFacto\n25.28\n0.469\n0.691\n24.61\n0.441\n0.687\n23.47\n0.255\n0.689\n19.02\n0.538\n0.512\n17.70\n0.442\n0.502\nInstant-NGP\n24.16\n0.595\n0.643\n22.93\n0.599\n0.619\n24.93\n0.380\n0.703\n20.37\n0.629\n0.478\n17.92\n0.625\n0.424\nMega-NeRF\n25.24\n0.566\n0.668\n25.61\n0.468\n0.680\n22.28\n0.472\n0.565\n23.68\n0.558\n0.525\n20.30\n0.526\n0.506\nGrid-NeRF\n25.37\n0.478\n0.705\n24.43\n0.483\n0.693\n-\n-\n-\n-\n-\n-\n-\n-\n-\nOurs\n26.07\n0.357\n0.739\n25.07\n0.357\n0.756\n24.73\n0.192\n0.736\n21.32\n0.482\n0.539\n19.71\n0.439\n0.520\nTable 1. Quantitative comparison on five large scene datasets. We report PSNR, LPIPS, and SSIM on the test views. The best and\nsecond best results are highlighted. The \u2019-\u2019 symbol indicates that Grid-NeRF [45] was not evaluated on these datasets due to difficulties in\nadjusting its training configs beyond the provided configurations for Matrix City [19], resulting in poor performance on other datasets.\nArea(m2)\nImages Overlap rate\nResolution\nAltitude\n1200\u00d7800\n6515\n95%\n8192\u00d75460\n180m\nTable 2. Overview of Campus dataset.\nadjacent eight grid points as occupied in the binary grids.\nAfter generating binary grids to identify occupied voxels,\nwe follow the MERF by baking high-resolution 2D planes\nand a low-resolution 3D voxel grid in each block. Only the\nnon-empty 3D voxels are stored using a block-sparse for-\nmat. We downsample the occupancy grid with max-pooling\nfor efficient rendering and skip empty space. To further save\nstorage, we compress textures into the PNG format.\n5. Experiments\n5.1. Experiments setup\nDataset and Metric. Our experiments span across vari-\nous scales and environments. We have incorporated a real-\nworld urban scene dataset (Campus) and public datasets\nconsisting of real-world rural rubble scenes (Rublle, Build-\ning) [37] and synthetic city-scale data (BlockA and BlockE\nin MatrixCity) [19]. Our datasets were recorded under uni-\nform, cloudy lighting conditions to minimize variation. To\nobtain precise pose information, we employed an annular\ncapturing approach, which has a higher overlap rate com-\npared to grid-based capturing methods. Tab. 2 presents an\noverview of our dataset. To assess the quality and fidelity of\nour reconstructions, we employ various evaluation metrics,\nincluding PSNR, SSIM and LPIPS [49].\nImplementations and Baselines. Our method takes posed\nmulti-view images captured using a fly-through camera as\ninput. The training code is built on the nerfstudio frame-\nwork [35] with tiny-cuda-nn [25] extension. And our real-\ntime viewer is a JavaScript web application whose render-\ning is implemented through GLS. We set the 5123 reso-\nlution for the voxel and 20482 resolution for the triplane\nwithin each block. We use a 4-layer MLP with 64 hidden\ndimensions as an encoder after multi-resolution hash encod-\ning to output density, color, and specular feature. More-\nover, a 3-layer MLP with 16 hidden dimensions tiny de-\nferred MLP is developed to predict residual view-dependent\ncolor. We sample 16384 rays per batch and use Adam op-\ntimizer with an initial learning rate of 1 \u00d7 10\u22122 decaying\nexponentially to 1 \u00d7 10\u22123. Our model is trained with 50k\niterations on one NVIDIA A100 GPU. We split the scene\ninto 24 non-overlapping blocks for Campus scene and split\nother scenes into four blocks. Moreover, we benchmark cur-\nrent real-time rendering methods using three critical param-\neters: Payload (PL), GPU Memory (VRAM), and Frames\nPer Scond (FPS). Payload refers to the essential data trans-\nmitted during the rendering process. We perform qualita-\ntive comparisons between our method and existing SOTA\nmethods for large-scale reconstruction. The Campus dataset\nis partitioned into six sections based on the reconstruction\ncontent. NeRFacto, Instant-NGP, and Grid-NeRF were ap-\nplied to one of these sections, while in other datasets , they\nare applied to the entire scene. NeRFacto and Instant-NGP\nare utilized with the highest hash encoding resolution of\n81923. Similarly, Mega-NeRF divides the Campus dataset\ninto 24 blocks and other datasets into four blocks. Our ex-\nperiments focus on a single campus section for comparative\nanalysis with existing real-time rendering methods.\n5.2. Results Analysis\nWe systematically evaluate the performance of both base-\nline models and our method through qualitative and quan-\ntitative comparisons in Tab. 1 and Fig. 3.\nNotably, our\nmethod demonstrates a remarkable enhancement in visual\nfidelity as reflected by the SSIM and LPIPS metrics, which\nindicate the extent of detail restoration. Despite a reduction\nin PSNR compared to the SOTA methods, this is attributable\nto the fact that LPIPS and SSIM are more sensitive to the\nrecovery of fine details, whereas PSNR mainly measures\npixel-wise color accuracy. Our approach achieves higher fi-\nFigure 3. Qualitative comparisons with existing SOTA methods. By testing different methods across diverse scales and environments,\nit clearly reveals that our approach excels in recovering finer details and achieves a higher quality of reconstruction.\ndelity reconstructions, revealing finer details due to our par-\ntitioned reconstruction strategy. In our method, each ray is\nevaluated by the deferred MLP only once, as opposed to\nother methods that evaluate the MLP at every sample point.\nConsequently, while our method recovers more intricate ge-\nometrical detail, it frequently results in color discrepancies\nwith the ground truth image due to unstable lighting condi-\ntions and variable exposure, as shown in Fig. 4.\nIn our evaluation, detailed in Tab. 3, we compare our\nmethod with current real-time rendering methods, using one\nsegment of the Campus dataset for testing.\nThese tests,\nare performed on an NVIDIA RTX 3060 Laptop GPU at\na 1920 \u00d7 1080 resolution. The results demonstrate that our\nmethod excels in reconstruction quality. We represent each\nscene block using voxels and triplanes, and store the baked\ngrid attributes as images. This strategy significantly reduces\nthe payload. This reduction notably accelerates resource\ntransmission for web-based rendering applications. How-\never, it is observed that our frame rate during rendering is\nlower compared to other methods. This is attributed to their\nrendering pipeline based on mesh rasterization, in contrast\nto our method, which utilizes volume rendering.\nPSNR\u2191 SSIM\u2191 LPIPS\u2193\nVRAM\u2193 PL\u2193\nFPS\u2191\nMobileNeRF\n19.99\n0.516\n0.712\n544.8\n515.3\n223\nBakedSDF\n22.24\n0.627\n0.413\n712.3\n242.1\n68\nOurs(Block)\n24.82\n0.741\n0.190\n128.1\n40.6\n51\nOurs(Total)\n526.6\n114.4\n46\nTable 3. Comparison with existing real-time methods. We seg-\nment the scene into four blocks, while other methods reconstruct\nthe whole scene. \u2018Ours(Block)\u2019 denotes the resource usage for one\nof these blocks, providing a fair comparison with other methods\nthat process the scene as a single block. \u2018Ours (Total)\u2019 represents\nthe cumulative resource usage for the entire scene.\n5.3. LOD Result\nTab. 4 presents the quantitative rendering results at various\nLOD, along with the corresponding payload and VRAM us-\nage. With increasing LOD, the resources required for ren-\ndering significantly decrease. Notably, our method\u2019s low-\nest LOD level still maintains high fidelity rendering results,\nas demonstrated in Fig. 5. Our LOD strategy significantly\nFigure 4. Comparison to Mega-NeRF in Rubble dataset. The\nRubble dataset presents significant variations in lighting. Although\nour method recovers more detail than Mega-NeRF, our deferred\nshading model has limited ability to represent view-dependent col-\nors and cannot accurately represent lighting and exposure changes\nin the data as view-dependent effects. This limitation results in\nslightly lower PSNR values.\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nVRAM\u2193\nPL\u2193\nLOD3\n23.72\n0.660\n0.306\n132.1\n40.2\nLOD2\n24.23\n0.682\n0.297\n841.6\n201.7\nLOD1\n24.73\n0.736\n0.192\n3970.2\n1259.6\nTable 4. The LOD results on the whole Campus dataset.\nFigure 5. Visualization of our LOD result.\nstreamlines the management of resource loading on web\nplatforms, which is particularly advantageous in rendering\ndistant blocks, as it requires less VRAM. It is worth noting\nthat the VRAM usage presented in Tab. 4 represents the cu-\nmulative memory consumption of all blocks. Our dynamic\nloading strategy adaptively selects resources to load based\non the camera\u2019s field of view and the distance to each block,\neffectively keeping the peak VRAM usage around 1100MB.\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nVRAM\u2193\nPL\u2193\nFPS\u2191\nhigh res. model\n24.05\n0.710\n0.201\n540.9\n147\n32\nw/o consistent\n24.21\n0.702\n0.281\n514.4\n110\n49\nours\n24.82\n0.741\n0.190\n526.6\n114\n46\nTable 5. Ablation Study. The result is test on the one section of\nCampus dataset.\nFigure 6. Limitations of Web-on-City.\n5.4. Ablation Study\nIn Tab. 5, we conduct an ablation study of our method on\none section of Campus dataset. Our model is trained for\nfour blocks with low resolution (voxel resolution of 5123\nand triplane resolution of 20482). We also train a single\nmodel for the entire scene with high resolution (voxel res-\nolution of 10243 and triplane resolution of 40962). Due\nto our non-overlapping scene partitioning strategy, these\ntwo representations have the same resolution across the en-\ntire scene. However, our reconstruction quality is higher,\nachieving better FPS and lower GPU memory usage, as\nwell as reduced payload. We also removed our consistent\ntraining, including virtual grids and alpha blending during\ntraining. As shown in the table, our consistent training sig-\nnificantly improves reconstruction quality.\n6. Conclusion and Discussion\nIn this work, we introduced City-on-Web, which to our\nknowledge is the first system that enables real-time neu-\nral rendering of large-scale scenes over web using laptop\nGPUs.\nOur integration of block partitioning with LOD\nhas significantly reduced the payload on the web platform\nand improved resource management efficiency. We ensured\nhigh-fidelity rendering quality by maintaining consistency\nbetween training and rendering.\nExtensive experiments\nhave also fully proved the effectiveness of City-on-Web.\nLimitation & Future Work.\nAs shown in Fig. 6, our\nmethod still has some limitations. Since we derive alpha\nblending across shaders based on the Lambertian surface\nassumption, visible seams may occur at the boundaries be-\ntween blocks on non-Lambertian surfaces, such as water\nsurfaces. Combining physically-based rendering with mul-\ntiple shaders blending may alleviate this problem. The de-\nferred MLP in City-on-Web has limited representation abil-\nity for view-dependent color, which might cause numer-\nous near-camera floaters. Taking the similar strategy used\nin [12, 42] by utilizing priors to pre-trim the scene or pre-\nprocessing the data are possible solutions.\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 2, 5\n[2] Yoshua Bengio, Nicholas L\u00b4eonard, and Aaron Courville.\nEstimating or propagating gradients through stochastic\nneurons for conditional computation.\narXiv preprint\narXiv:1308.3432, 2013. 3\n[3] Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai,\nJu Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, and\nJian Ren.\nReal-time neural light field on mobile devices.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8328\u20138337, 2023. 1,\n3\n[4] Pierre Charbonnier, Laure Blanc-F\u00b4eraud, Gilles Aubert, and\nMichel Barlaud. Deterministic edge-preserving regulariza-\ntion in computed imaging. IEEE Transactions on image pro-\ncessing, 6(2):298\u2013311, 1997. 5\n[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 2\n[6] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-\nterization pipeline for efficient neural field rendering on mo-\nbile architectures.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n16569\u201316578, 2023. 1, 2\n[7] James H Clark. Hierarchical geometric models for visible\nsurface algorithms. Communications of the ACM, 19(10):\n547\u2013554, 1976. 1, 2\n[8] Cyril Crassin, Fabrice Neyret, Sylvain Lefebvre, and Elmar\nEisemann. Gigavoxels: Ray-guided streaming for efficient\nand detailed voxel rendering. In Proceedings of the 2009\nsymposium on Interactive 3D graphics and games, pages 15\u2013\n22, 2009.\n[9] Mark Duchaineau, Murray Wolinsky, David E Sigeti,\nMark C Miller, Charles Aldrich, and Mark B Mineev-\nWeinstein.\nRoaming terrain: Real-time optimally adapt-\ning meshes.\nIn Proceedings. Visualization\u201997 (Cat. No.\n97CB36155), pages 81\u201388. IEEE, 1997. 2\n[10] Epic Games.\nUnreal engine 4.\nhttps:/ / www .\nunrealengine.com, 2023. Version 4.27. 1\n[11] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\nrendering at 200fps. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 14346\u2013\n14355, 2021. 1\n[12] Lily Goli, Cody Reading, Silvia Selll\u00b4an, Alec Jacob-\nson, and Andrea Tagliasacchi.\nBayes\u2019 rays: Uncertainty\nquantification for neural radiance fields.\narXiv preprint\narXiv:2309.03185, 2023. 9\n[13] Jiaming Gu, Minchao Jiang, Hongsheng Li, Xiaoyuan Lu,\nGuangming Zhu, Syed Afaq Ali Shah, Liang Zhang, and\nMohammed Bennamoun. Ue4-nerf: Neural radiance field\nfor real-time rendering of large-scale scene. arXiv preprint\narXiv:2310.13263, 2023. 2\n[14] Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Bo-\ntian Shi, Chiyu Wang, Chenjing Ding, Dongliang Wang,\nand Yikang Li.\nStreetsurf:\nExtending multi-view im-\nplicit surface reconstruction to street views. arXiv preprint\narXiv:2306.04988, 2023. 1\n[15] Stefan Guthe, Michael Wand, Julius Gonser, and Wolfgang\nStra\u00dfer. Interactive rendering of large volume data sets. In\nIEEE Visualization, 2002. VIS 2002., pages 53\u201360. IEEE,\n2002. 1, 2\n[16] Peter Hedman,\nPratul P Srinivasan,\nBen Mildenhall,\nJonathan T Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 5875\u20135884, 2021. 1, 2\n[17] Hugues Hoppe. Progressive meshes. In Proceedings of the\n23rd Annual Conference on Computer Graphics and Inter-\nactive Techniques, page 99\u2013108, New York, NY, USA, 1996.\nAssociation for Computing Machinery. 2\n[18] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao,\nXiao Liu, and Yuewen Ma.\nTri-miprf: Tri-mip represen-\ntation for efficient anti-aliasing neural radiance fields.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 19774\u201319783, 2023. 2\n[19] Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhen-\nzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: A large-scale\ncity dataset for city-scale neural rendering and beyond. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3205\u20133215, 2023. 6\n[20] Peter Lindstrom and Valerio Pascucci. Visualization of large\nterrains made easy.\nIn Proceedings Visualization, 2001.\nVIS\u201901., pages 363\u2013574. IEEE, 2001. 2\n[21] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel fields. Advances\nin Neural Information Processing Systems, 33:15651\u201315663,\n2020. 1, 2\n[22] Frank Losasso and Hugues Hoppe. Geometry clipmaps: ter-\nrain rendering using nested regular grids. In ACM Siggraph\n2004 Papers, pages 769\u2013776. 2004. 2\n[23] David Luebke.\nLevel of detail for 3D graphics.\nMorgan\nKaufmann, 2003. 1, 2\n[24] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance fields for uncon-\nstrained photo collections. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 7210\u20137219, 2021. 1\n[25] Thomas M\u00a8uller. tiny-cuda-nn, 2021. 6\n[26] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 2\n[27] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, Anton\nKaplanyan, and Markus Steinberger. Donerf: Towards real-\ntime rendering of compact neural radiance fields using depth\noracle networks. In Computer Graphics Forum, pages 45\u2013\n59. Wiley Online Library, 2021. 2\n[28] Martin Piala and Ronald Clark. Terminerf: Ray termination\nprediction for efficient neural rendering. In 2021 Interna-\ntional Conference on 3D Vision (3DV), pages 1106\u20131114.\nIEEE, 2021. 2\n[29] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 14335\u2013\n14345, 2021. 1, 2\n[30] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srini-\nvasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Pe-\nter Hedman. Merf: Memory-efficient radiance fields for real-\ntime view synthesis in unbounded scenes. ACM Transactions\non Graphics (TOG), 42(4):1\u201312, 2023. 1, 2, 4\n[31] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5459\u2013\n5469, 2022. 2\n[32] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten\nKreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson,\nMorgan McGuire, and Sanja Fidler. Neural geometric level\nof detail: Real-time rendering with implicit 3d shapes. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 11358\u201311367, 2021. 2\n[33] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas\nM\u00a8uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.\nVariable bitrate neural fields. In ACM SIGGRAPH 2022 Con-\nference Proceedings. Association for Computing Machinery,\n2022. 2\n[34] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,\nand Henrik Kretzschmar. Block-nerf: Scalable large scene\nneural view synthesis. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8248\u20138258, 2022. 1, 2\n[35] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,\nBrent Yi, Terrance Wang, Alexander Kristoffersen, Jake\nAustin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: A\nmodular framework for neural radiance field development.\nIn ACM SIGGRAPH 2023 Conference Proceedings, pages\n1\u201312, 2023. 6\n[36] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Er-\nrui Ding, Jingdong Wang, and Gang Zeng. Delicate textured\nmesh recovery from nerf via adaptive surface refinement. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2023. 1, 2\n[37] Haithem Turki,\nDeva Ramanan,\nand Mahadev Satya-\nnarayanan.\nMega-nerf:\nScalable construction of large-\nscale nerfs for virtual fly-throughs.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12922\u201312931, 2022. 1, 2, 6\n[38] Unity Technologies. Unity. https://unity.com, 2023.\nVersion 2020.3. 1\n[39] Gokul Varadhan and Dinesh Manocha. Out-of-core render-\ning of massive geometric environments. In IEEE Visualiza-\ntion, 2002. VIS 2002., pages 69\u201376. IEEE, 2002. 4\n[40] Ziyu Wan, Christian Richardt, Alja\u02c7z Bo\u02c7zi\u02c7c, Chao Li, Vijay\nRengarajan, Seonghyeon Nam, Xiaoyu Xiang, Tuotuo Li,\nBo Zhu, Rakesh Ranjan, et al. Learning neural duplex ra-\ndiance fields for real-time view synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8307\u20138316, 2023. 1\n[41] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu,\nTaku Komura, Christian Theobalt, and Wenping Wang. F2-\nnerf: Fast neural radiance field training with free camera\ntrajectories.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4150\u2013\n4159, 2023. 2\n[42] Frederik Warburg, Ethan Weber, Matthew Tancik, Alek-\nsander Holynski, and Angjoo Kanazawa. Nerfbusters: Re-\nmoving ghostly artifacts from casually captured nerfs. arXiv\npreprint arXiv:2304.10532, 2023. 9\n[43] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,\nAnyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.\nBungeenerf: Progressive neural radiance field for extreme\nmulti-scale scene rendering.\nIn European conference on\ncomputer vision, pages 106\u2013122. Springer, 2022. 1, 2\n[44] Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang,\nHaoran Wang, Yunfeng Cai, and Mingming Sun.\nS3im:\nStochastic structural similarity and its unreasonable effec-\ntiveness for neural fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 18024\u2013\n18034, 2023. 5\n[45] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan,\nNanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin.\nGrid-guided neural radiance fields for large urban scenes. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 8296\u20138306, 2023. 1, 2,\n6\n[46] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,\nPratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron,\nand Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-\ntime view synthesis. In ACM SIGGRAPH 2023 Conference\nProceedings, SIGGRAPH 2023, Los Angeles, CA, USA, Au-\ngust 6-10, 2023, pages 46:1\u201346:9. ACM, 2023. 1, 2\n[47] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021.\n[48] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\nfields. arXiv preprint arXiv:2010.07492, 2020. 1, 2\n[49] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 6\n[50] MI Zhenxing and Dan Xu. Switch-nerf: Learning scene de-\ncomposition with mixture of experts for large-scale neural\nradiance fields. In The Eleventh International Conference on\nLearning Representations, 2022. 2\n[51] Yiyu Zhuang, Qi Zhang, Ying Feng, Hao Zhu, Yao Yao, Xi-\naoyu Li, Yan-Pei Cao, Ying Shan, and Xun Cao. Anti-aliased\nneural implicit surfaces with encoding level of detail. arXiv\npreprint arXiv:2309.10336, 2023. 2\nCity-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web\nSupplementary Material\n7. Proof of 3D consistency\nFor a given sampling point i, suppose it is located within the\nregion K of block k, which contains a total of Nk sampling\npoints. The diffuse color, feature, and opacity output by the\nshader of block k are denoted as ck\ni , F k\ni , and \u03b1k\ni respec-\ntively. Then, for this ray, the output diffuse color ck\nd and\nspecular feature F kof block k are caculated as follows. hk\nrepresents either the diffuse color or specular feature.\nhk(r) =\nNk\nX\ni=1\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1k\nj ) \u00b7 \u03b1k\ni hk\ni\n\u03b1k =\nNk\nX\ni=1\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1k\nj ) \u00b7 \u03b1k\ni\n(8)\nBy integrating the rendering results of each block\u2019s\nshader through the volume rendering between blocks, the\nfinal diffuse color cd and specular feature F of the ray can\nbe obtained.\nh(r) =\nX\nk\nk\u22121\nY\nj=1\n(1 \u2212 \u03b1j) \u00b7 hk\n(9)\nNote that\n1 \u2212 \u03b1k\n= 1 \u2212\nNk\nX\ni=1\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1k\nj ) \u00b7 \u03b1k\ni\n= 1 \u2212 \u03b1k\n1 \u2212 (1 \u2212 \u03b1k\n1)\u03b1k\n2 \u2212 (1 \u2212 \u03b1k\n1)(1 \u2212 \u03b1k\n2)\u03b1k\n3 \u2212 \u00b7 \u00b7 \u00b7\n= (1 \u2212 \u03b1k\n1)(1 \u2212 \u03b1k\n2 \u2212 (1 \u2212 \u03b1k\n2)\u03b1k\n3 \u2212 \u00b7 \u00b7 \u00b7 )\n= (1 \u2212 \u03b1k\n1)(1 \u2212 \u03b1k\n2)(1 \u2212 \u03b1k\n3 \u2212 \u00b7 \u00b7 \u00b7 )\n...\n=\nNk\nY\ni=1\n(1 \u2212 \u03b1k\ni )\n(10)\nAssuming the block positions are already depth-sorted,\nmeaning there are M sampling points along the ray, with\neach block k containing Nk sampling points, let \u03b1i denote\nthe i-th sampling point on the ray, and \u03b1k\ni denote the i-th\nsampling point in block k along the ray, then\nh(r) =\nX\nk\nk\u22121\nY\nj=1\nNj\nY\ni=1\n(1 \u2212 \u03b1j\ni) \u00b7 hk\n=\nX\nk\nN1+\u00b7\u00b7\u00b7+Nk\u22121\nY\ni=1\n(1 \u2212 \u03b1j\ni) \u00b7\n\uf8eb\n\uf8ed\nNk\nX\ni=1\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1k\nj ) \u00b7 \u03b1k\ni hk\ni\n\uf8f6\n\uf8f8\n=\nX\nk\nNk\nX\ni=1\nN1+\u00b7\u00b7\u00b7+Nk\u22121\nY\ni=1\n(1 \u2212 \u03b1j\ni)\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1k\nj ) \u00b7 \u03b1k\ni hk\ni\n=\nM\nX\ni=1\ni\u22121\nY\ni=1\n(1 \u2212 \u03b1j)\u03b1ihi\n(11)\nEq. (11) shows that the diffuse color and specular feature\nwe finally obtain by depth-sorting blocks and alpha blend-\ning along blocks are consistent with the results of volume\nrendering integration along the entire ray. Therefore, our\nmethod ensures the three-dimensional consistency of ren-\ndering.\n8. Dataset\nTo demonstrate the effectiveness of our method, experi-\nments are conducted on a variety of large scenes. The main\nexperiments reported in this paper involve three types of en-\nvironmental scene datasets of various scales. The Campus\ndataset is captured at a altitude of about 180 meters, cov-\nering an area of approximately 960,000 m2. The Matrix\nCity Dataset, captured at a altitude of 200 meters, is sparser\nthan the Campus dataset, thus covering a larger area. The\nMill 19 dataset covers a total of about 200,000 m2. We\nadopt a circular data capture method for photographing, as\nshown in Fig. 7. We find that this method often results in a\nhigher overlap rate, allowing for a more accurate estimation\nof camera poses. Our dataset was captured over 8 hours on\na cloudy day, with a fixed exposure setting to ensure almost\nidentical appearance of photos taken at different times. We\nused Colmap to estimate camera poses. Feature matching\nwas done using a vocabulary tree, followed by a hierarchi-\ncal mapper followed by a few iterations of triangulation and\nbundle adjustment to estimate camera poses.\n9. Efficiency of Non-overlapping Partition\nStrategy\nIn the stage of segmenting the scene into distinct blocks,\nwe initially rotate the scene to align it parallel with the xy-\nplane, then proceed to segment the entire space based on\nFigure 7. Visulization of Campus Dataset.\nFigure 8. Visulization of ou efficient non-overlapping partition\nstrategy. The red square represents the [\u22121, 1]2.\nthe xy coordinates of spatial points. This strategy\u2019s merit\nlies in ensuring each segmented block is a bounded area.\nThis is in contrast to methods like segmentation strategy of\nblock-nerf [34] and [37], which cannot assure boundedness\nin the reconstructed area, potentially leading to substantial\nmemory resource wastage as illustrated in the Fig. 8. Our\napproach allows us to represent the same size area with a\nbounded region of [\u22121, 1]3, maintaining the same represen-\ntation resolution, instead of using an unbounded region that\ncontract to [\u22122, 2]3 like in the MERF [30] and Mip-NeRF\n360 [1]. Consequently, this enables the reduction of the res-\nolution of the xy-plane from 40962 to 20482 without loss in\nperformance. Thus we can effectively reduce the usage of\nVRAM, especially across the three planes.\n10. Impletation Details.\nFor blocks at the boundaries of the entire scene, an un-\nbounded scene representation is required to represent areas\noutside the block boundaries. We follow the same approach\nas MERF to compute ray-AABB intersections trivially. To\nbe specific, we employ the scene contraction function to\nproject the scene external to the unit sphere into a cube,\nwhich has a radius of 2. The definition of the j \u2212 th coor-\ndinate for a contracted point is as follows:\ncontract(x)j =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nxj\nif \u2225x\u2225\u221e \u2264 1\nxj\n\u2225x\u2225\u221e\nif xj \u0338= \u2225x\u2225\u221e > 1\n\u0010\n2 \u2212\n1\n|xj|\n\u0011\nxj\n|xj|\nif xj = \u2225x\u2225\u221e > 1\n,\n(12)\n11. More Results\nWe provide additional qualitative results and on Matrix\nCity, Mill 19, Campus dataset as shown in Fig. 9.\n12. Discussion\nRecently, some research has also enabled real-time render-\ning of large scenes. UE4-NERF [13], building on the Mo-\nbileNerf framework, divides large scenes into smaller seg-\nments for reconstruction and then renders the large-scale\nscene using the mesh rasterization pipeline. Like MobileN-\nerf, UE4-NERF begins with a 1283 grid, which assumes an\neven distribution of scene details in all directions. How-\never, data captured through oblique photography often ap-\npears \u2019flat\u2019, meaning there\u2019s dense information when pro-\njected onto the xy plane, but sparser in the vertical direction\ndue to mostly empty areas. Therefore, UE4-NERF needs\nmore segments for large-scale reconstruction to ensure an\neven distribution of details in all directions in a block. For\nexample, their Construction Site scene required about 40\nblocks to reconstruct a 420m2\u00d7240m2 area, leading to sig-\nnificant memory and VRAM usage, approximately 25GB.\nEven with UE4\u2019s dynamic mesh-based loading, VRAM us-\nage is around 11GB. Such high payload and VRAM de-\nmands make it challenging to extend this technology to web\nplatforms and consumer-grade GPUs.\nAdditionally, NeuRas [3] has also made progress in real-\ntime rendering of large scenes. It uses texture-less geome-\ntry from already reconstructed large scenes. NeuRas applys\nfeature map textures and combines mesh rasterization re-\nsults with view direction to query a small MLP for view-\ndependent colors. They optimize the feature texture and\nview-dependent MLP to enhance rendering results. How-\never, this method also suffers from high memory usage.\nOur experiments show that obtaining texture-less geome-\ntry with ContextCapture1 for the Campus scene requires\nabout 6GB, and surface reconstruction via neural rendering\nmethods needs approximately 10GB without mesh simpli-\nfication.\nBoth these methods have achieved excellent results in\nreal-time rendering of large scenes, but their significant\nmemory and VRAM requirements limit their expansion to\nweb platforms. Our method can implement real-time ren-\ndering of large scenes with a payload and VRAM con-\nsumption acceptable for web platforms and consumer-grade\ngraphics cards.\n1https://www.bentley.com/software/itwin-capture-modeler/\nFigure 9. Qualitative visualization of geometry and rendering result on five datasets.\n"
  },
  {
    "title": "I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.16693.pdf",
    "upvote": "11",
    "text": "I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models\nXun Guo1,2,3\u2020\u2217\nMingwu Zheng1\u2217\nLiang Hou1\nYuan Gao1\nYufan Deng1\nPengfei Wan1 Di Zhang1\nYufan Liu3\nWeiming Hu3\nZhengjun Zha2\nHaibin Huang1\u2021\nChongyang Ma1\n1Kuaishou Technology\n2University of Science and Technology of China\n3Institute of Automation, Chinese Academy of Sciences\nInput Image\nGenerated Video\nw/ Personalized T2I\nw/  ControlNet\nI2V-Adapter\nFigure 1. Our lightweight I2V-Adapter achieves high-quality and versatile image-to-video generation, without tuning any pretrained spatial\nor temporal modules. First row: I2V results of I2V-Adapter. Second row: results of I2V-Adapter combined with a personalized T2I model\n(CuteYukiMix). Third row: results of I2V-Adapter combined with ControlNet [49].\nAbstract\nText-guided image-to-video (I2V) generation aims to\ngenerate a coherent video that preserves the identity of the\ninput image and semantically aligns with the input prompt.\n\u2217Joint first authors.\n\u2020Work done as an intern at Kuaishou Technology.\n\u2021Corresponding author: jackiehuanghaibin@gmail.com\nExisting methods typically augment pretrained text-to-video\n(T2V) models by either concatenating the image with noised\nvideo frames channel-wise before being fed into the model\nor injecting the image embedding produced by pretrained\nimage encoders in cross-attention modules. However, the\nformer approach often necessitates altering the fundamen-\ntal weights of pretrained T2V models, thus restricting the\nmodel\u2019s compatibility within the open-source communities\n1\narXiv:2312.16693v2  [cs.CV]  30 Jan 2024\nand disrupting the model\u2019s prior knowledge. Meanwhile,\nthe latter typically fails to preserve the identity of the input\nimage. We present I2V-Adapter to overcome such limita-\ntions. I2V-Adapter adeptly propagates the unnoised input\nimage to subsequent noised frames through a cross-frame\nattention mechanism, maintaining the identity of the input\nimage without any changes to the pretrained T2V model.\nNotably, I2V-Adapter only introduces a few trainable pa-\nrameters, significantly alleviating the training cost and also\nensures compatibility with existing community-driven per-\nsonalized models and control tools. Moreover, we propose\na novel Frame Similarity Prior to balance the motion am-\nplitude and the stability of generated videos through two\nadjustable control coefficients.\nOur experimental results\ndemonstrate that I2V-Adapter is capable of producing high-\nquality videos. This performance, coupled with its agility\nand adaptability, represents a substantial advancement in\nthe field of I2V, particularly for personalized and control-\nlable applications.\n1. Introduction\nAs an essential medium, video has risen to prominence on\nthe Internet, offering users a rich and dynamic visual experi-\nence. The technology behind video generation has therefore\ngarnered widespread attention due to its capacity to create\ninnovative video content and its immense potential in the\nfield of generative artificial intelligence. Recently, with the\nremarkable success of text-to-image (T2I) diffusion models\nsuch as Stable Diffusion [30] and SDXL [27], there have\nbeen significant advancements in text-to-video (T2V) dif-\nfusion models [2, 3, 8, 9, 14, 23, 40, 42]. These models\nintegrate temporal modules, such as temporal convolutions\nor temporal self-attention, onto T2I diffusion models to cap-\nture the relations between video frames. Typical approaches\nlike AnimateDiff [9] decouples spatial modeling and tem-\nporal modeling to learn a motion prior from video data in a\nplug-and-play manner. By solely training the newly intro-\nduced temporal modules while maintaining the pretrained\nweights of T2I models, these approaches have achieved sig-\nnificant improvements in the field of T2V.\nWhile T2V models have shown significant progress,\ncrafting effective text prompt to generate the desired content\nremains a challenge, often necessitating complex prompt\nengineering [43]. This intricacy highlights the advantages\nof an alternative methodology, i.e., image-to-video (I2V)\ngeneration, which takes an image together with a text\nprompt as the input. This kind of method is often more\nintuitive and convenient, since the input image provides a\ndirect and tangible visual reference for the video genera-\ntion.\nHowever, translating specific images into coherent\nframe sequences in high fidelity necessitate not only gen-\nerative capabilities but also fine-grained understanding of\ninput images.\nTo tackle this unique challenge, many existing I2V mod-\nels consider how to effectively inject the input image into\nthe diffusion process during training, building upon the\npretrained T2V models. One common approach involves\nchannel-wise concatenation of the input image with the\noriginal noised inputs of the diffusion model\u2019s U-Net [2,\n4, 8, 48]. Such a modification inevitably necessitates alter-\nations to the original model\u2019s structure and weights as the\ninput space undergoes significant changes. Typically, this\napproach requires intensive training on a voluminous video-\ntext dataset to mitigate this discrepancy. However, such an\napproach exacerbates the complexity and instability of the\ntraining process which could result in catastrophic forget-\nting and significantly hampers the performance and general-\nizability. Moreover, this modification limits their reusability\nwith personalized T2I models [16, 32] as well as their com-\npatibility with control tools like ControlNet [49]. Notably,\nthese models and tools, developed by a vast community of\nAI artists and enthusiasts, have been instrumental in nu-\nmerous applications. Another common approach explored\nis the additional cross attention mechanism that relates the\noutputs of a pretrained image encoder (e.g., CLIP [29] and\nDINO [24]) with the diffusion model [3, 4, 50]. This strat-\negy leverages the ability of cross attention mechanism to\ncapture high-level dependencies and correlations. However,\nthe outputs of these encoders often fall short in preserving\nthe identity and fine details of the input image, leading to\nresults that are less than satisfactory.\nIn this work, we present I2V-Adapter, a lightweight\nand plug-and-play adapter designed to overcome afore-\nmentioned limitations for image-to-video generation. I2V-\nAdapter fully capitalizes on the prior knowledge and capa-\nbilities of the pretrained T2V model by feeding the unnoised\ninput image in parallel with the subsequent noised frames\nto the model. Our approach ingeniously propagates the in-\nput image\u2019s identity information to the subsequent frames\nthrough a cross-frame attention mechanism without alter-\ning any parameters of the spatial or motion modules (Stable\nDiffusion and AnimateDiff in our implementation). I2V-\nAdapter is adeptly integrated with the pretrained model via\na trainable copy of query projector and a trainable zero-\ninitialized output projector, ensuring that the model\u2019s ini-\ntialization remains unaffected by the newly incorporated\nmodules. Notably, I2V-Adapter introduces only a few train-\nable parameters, significantly alleviating the training cost\nwhile ensuring the compatibility with the open-source com-\nmunities. Moreover, we propose a novel Frame Similarity\nPrior to balance the motion magnitude and stability in gen-\nerated videos.\nTo summarize, our contributions are as follows:\n\u2022 We present I2V-Adapter, a novel lightweight adapter that\nachieves general image-to-video generation without train-\n2\ning any pre-existing spatial or motion modules.\n\u2022 We propose the Frame Similarity Prior, a novel con-\ncept that leverages the inherent similarity between video\nframes for image-to-video generation.\nThis approach\nis designed to balance the stability with motion magni-\ntude in generated videos, thereby substantially enhancing\nthe model\u2019s controllability and the diversity of generated\nvideos.\n\u2022 Our experiments demonstrate that our model can achieve\nsuperior results in image-to-video generation. Notably,\nour model requires significantly fewer trainable parame-\nters (down to a minimum of 24M), as low as 1% of main-\nstream models.\n\u2022 We show that our I2V-Adapter exhibits remarkable ver-\nsatility and compatibility, supporting a variety of person-\nalized T2I models and controllable tools, thus enabling\ncreative and custom applications.\n2. Related Work\nVisual content generation, notably influenced by the ad-\nvancements in diffusion models [13, 34, 35], is emerging as\na popular research area. This section provides an overview\nof closely related work from three perspectives: text-to-\nvideo (T2V) generation, image-to-video (I2V) generation,\nand adapter techniques.\nText-to-video generation.\nT2V generation extends be-\nyond the capabilities of T2I by producing a sequence of\ncontinuous frames, where diffusion models also play a piv-\notal role. Unlike T2I, T2V demands the synthesis of tem-\nporally coherent and visually consistent video content from\ntextual descriptions, posing unique challenges in maintain-\ning continuity and realism over time.\nIn the realm of T2V, early works such as LVDM [11]\nand ModelScope [40] have pioneered the integration of\ntemporal modules within the 2D U-Net [31] architecture.\nThese approaches often involve redesign and comprehen-\nsive training of volumetric U-Net models, incorporating\ntemporal aspects to cater to the dynamic nature of video\ncontent. This methodology signifies a critical step in han-\ndling the temporal complexity inherent in video generation.\nAnother notable category of methods includes AnimateD-\niff [9] and Hotshot-XL [23]. These models ingeniously in-\ncorporate trainable temporal layers while retaining the foun-\ndational structure of existing T2I frameworks. This strat-\negy leverages the established efficacy of T2I models, ex-\ntending their capabilities to handle the additional dimen-\nsion of time, thereby facilitating the generation of seam-\nless video sequences. Additionally, approaches like Imagen\nVideo [14] and LaVie [42] adopt a cascaded structure with\ntemporal and spatial super-resolution modules. This sophis-\nticated architecture addresses both the spatial and temporal\nintricacies of video content, ensuring higher resolution and\ntemporally coherent video generation.\nTo achieve multimodal compatibility and enhanced\ncontrollability within a unified framework, VideoCom-\nposer [41] emerges as a significant contribution. This model\nintegrates various modalities, enabling more nuanced and\ncontrolled video generation, a crucial factor in applications\ndemanding high fidelity and specific contextual alignment.\nImage-to-video generation.\nI2V generation is distin-\nguished by its requirement to maintain the identity of the\ninput image in the generated video. This poses a challenge\nin balancing the preservation of the image\u2019s identity with\nthe dynamic nature of video.\nMask-based approaches like MCVD [39] and SEINE [5]\nutilize video prediction techniques to achieve I2V genera-\ntion. These methods are pivotal in maintaining the continu-\nity of the input content across the generated video frames,\nensuring a seamless transition from the static to the dy-\nnamic.\nOther innovations such as I2VGen-XL [50] and\nVideoCrafter1 [3] take a different approach by employing\nthe CLIP [29] image encoder. They construct image embed-\ndings as conditions, enabling a more nuanced interpretation\nof the initial image and its subsequent transformation into\nvideo format. Another set of techniques, including Pixel-\nDance [48], Stable Video Diffusion [2], and VideoGen [18],\noperate in the VAE [17] latent space.\nThey concatenate\nimage data within this space, effectively incorporating the\ninput image into the video generation framework.\nThis\nmethod has shown promise in maintaining the fidelity of\nthe input image while enabling dynamic video creation.\nLAMP [45], in particular, employs a first-frame-attention\nmechanism. This method focuses on transferring the infor-\nmation of the first frame to subsequent frames, ensuring that\nthe essence of the initial image is reflected throughout the\nvideo sequence. While this method has demonstrated en-\ncouraging outcomes, its applicability is confined to a fixed\nmotion pattern within a few-shot setting. This constraint\nnarrows its capability for broad generalization across di-\nverse scenarios.\nIn our work, we diverge from these singular approaches\nand introduce a method characterized by its plug-and-play\nnature. Our approach is designed to be highly compatible\nwith various tools within the Stable Diffusion [30] commu-\nnity, such as DreamBooth [32], LoRA [16], and Control-\nNet [49]. This compatibility enhances the flexibility and\napplicability of our method.\nFurthermore, our approach\nis designed for ease of training, making it accessible for a\nbroader range of applications and users.\nAdapter.\nIn the field of natural language processing,\nadapters [15] have become a popular technique for efficient\ntransfer and fine-tuning of large pretrained language mod-\n3\nels, such as BERT [6] and GPT [28]. Adapters function by\ninserting small, task-specific modules between the layers of\npretrained models, thus enabling the learning of new task\nfeatures without disturbing the original pretrained weights.\nThe advantage of this approach is that it allows the model\nto retain foundational knowledge while rapidly adapting to\nspecific tasks without the need for costly retraining of the\nentire network for each new task.\nIn the realm of diffusion models, a similar concept is\nbeing applied. ControlNet [49] and T2I-Adapter [22] are\namong the first to propose the use of adapters to provide\nmore control signals for diffusion models. IP-Adapter [47]\nspecifically proposes a decoupled cross-attention mecha-\nnism for image conditioning, enabling the model to gen-\nerate images that resemble the input image.\nIn our work, we introduce I2V-Adapter, a universal\nadapter that achieves efficient transfer from T2V to I2V\nwith minimal overhead in terms of parameter number and\nworks in a plug-and-play manner.\n3. Method\nGiven a reference image and a text prompt, our goal is to\ngenerate a video sequence starting with the provided im-\nage. This task is particularly challenging as it requires en-\nsuring consistency with the first frame, compatibility with\nthe prompt, and preserving the coherence of the video se-\nquence throughout.\n3.1. Preliminaries\nLatent diffusion models.\nDiffusion models [7, 13, 34,\n35] are a class of generative models that involves a fixed\nforward diffusion process and a learned reverse denoising\ndiffusion process. Given a data x0 \u223c pdata(x), the forward\nprocess gradually adds Gaussian noise \u03f5 \u223c N(0, 1) to the\ndata to obtain a noised data, as shown as follows:\nxt = \u03b1tx0 + \u03c3t\u03f5,\n(1)\nwhere \u03b1t, \u03c3t \u2208 [0, 1] are two predefined functions of ran-\ndom time step t \u223c U(1, . . . , T), satisfying the variance pre-\nserving principle, i.e., \u03b12\nt + \u03c32\nt = 1.\nDiffusion models typically train a neural network \u03f5\u03b8(\u00b7)\nto learn the added noise, which is denoised during sampling\nto gradually generate new data. For any condition c (e.g.,\ntext, image, and audio), the training objective function is\ndefined as\nmin\n\u03b8\nEx0,c,t,\u03f5\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, c, t)\u22252\n2\n\u0003\n.\n(2)\nLatent diffusion models (LDM) such as Stable Diffu-\nsion [30] integrate the diffusion process within the low-\ndimensional latent space of a variational autoencoder\n(VAE) [17] instead of the high-dimensional pixel space for\nfast and efficient training and inference, which also yields\ncomparable text-to-image generation performance. The de-\nnoising network of diffusion models typically adopts a U-\nNet based architecture [31], where each block contains\nresidual convolution layers [10], self-attention blocks [38],\nand cross-attention blocks [38].\nSelf-attention in LDM\nThe self-attention mechanism\nmodels the relationships between feature patches that gen-\nerate the image. Self-attention has been demonstrated to\ncontrol the layout of generated images, object shapes, and\nfine-grained texture details [12, 37]. Self-attention can be\ndefined by:\nAttention(Qi, Ki, Vi) = softmax\n\u0012Qi(Ki)T\n\u221a\nd\n\u0013\nVi, (3)\nwhere Qi = XiWQ represents the query from the in-\nput feature map Xi through the query weight matrix WQ,\nKi = XiWK represents the key from the input feature map\nXi through the key weight matrix WK, and Vi = XiWV\nrepresents the value from the input feature map Xi through\nthe value weight matrix WV .\nCross-attention in LDM\nTo incorporate textual condi-\ntional information into networks for text-to-image tasks,\ncross-attention has been introduced in LDM to model the\ndependency between image feature patches and textual to-\nkens. In the cross-attention mechanism, the query Qi =\nXiWcross\nQ\nstems from the image feature Xi, while the key\nKtext = XtextWcross\nK\nand the value Vtext = XtextWcross\nV\nare\nderived from text embeddings Xtext of textual tokens en-\ncoded via the CLIP text encoder [29]. This attention mech-\nanism enables different textual tokens to govern the ap-\npearance of objects at different positions within the image.\nCross-attention can be defined by the following equation:\nAttention(Qi, Ktext, Vtext) = softmax\n\u0012QiKT\ntext\n\u221a\nd\n\u0013\nVtext.\n(4)\n3.2. I2V-Adapter\nTemporal modeling with Stable Diffusion.\nThe chal-\nlenge in video generation lies in handling high-dimensional\ndata to ensure coherence along the temporal dimension.\nMost existing video generation models incorporate motion\nmodules on top of Stable Diffusion [30] to model the tem-\nporal relationships in videos. In this paper, we are interested\nin AnimateDiff [9], a model initially developed for person-\nalized T2V generation. It employs transformer blocks with\npositional encoding to capture the temporal relationships of\nthe feature map along the dimension of frames. Due to this\n4\nMulti-Head Attention\n  \ufffd\ufffd\n \ufffd1\n+\nImage \nEncoder\nText   \nEncoder\nInput Prompt\nSpatial Block\nI2V-Adapter\n\ufffdK\n\ufffdV\n\ufffdQ\n\u2032\n\ufffdO\n\u2032\nSelf \nAttention\nI2V- \nAdapter\nTemporal Block\nSpatial Block\nTemporal Block\nSpatial Block\nTemporal Block\nSpatial Block\nTemporal Block\nSpatial Block\nTemporal Block\nSpatial Block\nCross \nAttention\nContent-\nAdapter\n+\n\u22ef\n\u22ef\n\u22ef\n\u22ee\n\u22ee\n\u22ef\n\u22ef\n\u22ef\n\u22ef\nOutput Video\nInput Image\nFigure 2. The overall architecture of our method. During both\nthe training and inference procedures, we set the first frame as the\ninput image and keep it noise-free. To capture fine-grained infor-\nmation, we add an additional branch alongside the self-attention\nblock, i.e., I2V-Adapter. For each subsequent frame, the atten-\ntion keys and values are shared with the first frame and remain\nunchanged. Only the query weights and zero-initialized output\nweights are trainable. This lightweight architecture allows us to\ninfuse the input image into the diffusion process effectively.\nframe-wise attention mechanism, the motion module oper-\nates independently from the spatial modules, enabling it to\ngeneralize across multiple varieties of Stable Diffusion.\nInspired by this unique feature, we believe that the pre-\ntrained motion module can be seen as a universal motion\nrepresentation, capable of being applied to more general\nvideo generation tasks, e.g., I2V generation, without any\nfinetuning. Consequently, in this paper, we only modify\nspatial features and keep the pretrained AnimateDiff tem-\nporal weights fixed.\nAdapter\nfor\nattention\nlayers.\nUnlike\ntext-to-video\n(T2V) generation, image-to-video (I2V) generation not\nonly requires adherence to the high-level semantic infor-\nmation of a textual prompt but also needs to maintain the\ncontent information of the reference image, specifically its\nfine-grained, low-level features.\nTo this end, a common\napproach is to concatenate the reference image with the\noriginal noised inputs along the channel dimension before\nbeing fed into the U-Net [31], thereby introducing refer-\nence image information at the input stage. Unfortunately,\nconcatenation of additional input channels often necessi-\ntates retraining the entire U-Net network. This may disrupt\nthe prior knowledge learned by the original Stable Diffu-\nsion model and also limits the flexibility to incorporate con-\ntrollable or personalized models (e.g., ControlNet [49] and\nDreamBooth [32]) developed by the open-source commu-\nnity.\nUnlike these approaches, we apply the first-frame-\nconditioned pipeline proposed by LAMP [45], in which the\nfirst frame is always preserved as unnoised during both the\ntraining and inference stages, as detailed in Fig. 2. This\npipeline keeps the space of the U-Net\u2019s input and output\nunchanged, thereby reducing the trainable parameters while\nprotecting the pretrained model weights.\nHowever, this condition is weak since it is challeng-\ning for the motion module to ensure interaction with the\nfirst frame. To enhance the condition, most existing meth-\nods propose using a pretrained image encoder, such as\nCLIP [29] or DINO [24], to extract features and then\nfeed them into the cross-attention module of U-Net. We\nfollow this paradigm and integrate a pretrained Content-\nAdapter [47] into the cross-attention module. However, de-\nspite the improvement, these image encoders usually only\ncapture high-level semantic information and struggle to pre-\nserve lower-level features of the reference image.\nIn this paper, we propose a novel and effective ap-\nproach to fuse low-level information into the first-frame-\nconditioned pipeline. Our key observation is that the self-\nattention plays a critical role in controlling the layout, object\nshapes, as well as fine-grained spatial details during the gen-\neration process [12, 37]. Therefore, we add a cross-frame\nattention layer alongside the original self-attention layer to\ndirectly query the rich context information of the first frame\nencoded by the pretrained U-Net.\nSpecifically, for each frame i \u2208 {1, ..., l}, where l indi-\ncates the video length, we add a new cross-frame attention\nlayer to integrate the input image features into the original\nself-attention process. Given the key and value features of\nthe first frame K1 and V1, the output of the new cross-\nframe attention is computed as follows:\nAttention((Qi)\u2032, K1, V1) = softmax\n\u0012(Qi)\u2032(K1)T\n\u221a\nd\n\u0013\nV1,\n(5)\nwhere (Qi)\u2032 = XiW\u2032\nQ, and W\u2032\nQ represents a newly train-\nable query weight matrix. The parameters of W\u2032\nQ are ini-\ntialized from the original WQ in the corresponding layer.\nWe then add the output to the original self-attention output,\n5\n\u2026\nReverse SDE with frame similarity prior \nReverse SDE\nFrame N\nFrame 1 \n(input image)\nPure Noise\nDowngraded Frame 1\nadd\nnoise\nFigure 3. An illustration about Frame Similarity Prior. Instead of\nsolving the reverse SDE from pure noise (the top row), we start the\ndenoising process from corrupted input image (the bottom row).\nThe similarity prior allows the model to generate videos with fewer\nsteps and stabilizes the final result.\nusing the output projection matrices WO and W\u2032\nO:\nXi\nout =Attention(Qi, Ki, Vi)WO+Attention((Qi)\u2032, K1, V1)W\u2032\nO.\n(6)\nFrom a macroscopic perspective, the additional term on\nthe right side borrows rich information from the first frame\nby querying its keys and using its values. Furthermore, the\noriginal self-attention module remains unchanged, ensur-\ning that the T2I capability is not compromised. Besides,\ninspired by ControlNet [49], we zero-initialize the output\nprojection layer W\u2032\nO to ensure that the model starts as if\nno modifications have been made. In total, our lightweight\nI2V-Adapter comprises only two trainable parameters: W\u2032\nQ\nand W\u2032\nO, while leaving the pretrained weights of Stable\nDiffusion and the motion module completely unchanged,\nthereby ensuring that their original functionality remains\noptimal.\n3.3. Frame Similarity Prior\nIt is noteworthy that I2V generation is an ill-posed problem\nsince the model is required to generate a full video from just\na single reference image. Despite the tremendous data prior\nwe leverage from pretrained spatial and temporal modules,\nthe model still occasionally tends to produce unstable and\ndistorted results.\nTo tackle this problem, we propose an additional frame\nsimilarity prior. Our key assumption is that, on a relatively\nlow Gaussian noise level, the marginal distributions of the\nnoised first frame and the noised subsequent frames are ad-\nequately close. At a high level, we assume that in most\nshort video clips, all frames are structurally similar and be-\ncome indistinguishable after being corrupted with a certain\namount of Gaussian noise, see Fig. 3. Specifically, inspired\nby SDEdit [21], to inject this prior into the diffusion pro-\ncess, instead of solving SDE [35] from pure noise xT , we\nstart from an intermediate time t0 \u2208 (0, 1] and sample each\nframe i as xi\nt0 \u223cN(\u03b1t0D(x1\n0); \u03c32\nt0I).\nTo strike a balance between the realism and the strength\nof motion effects, we define the degradation operation D as\na combination of masked Gaussian blur. Specifically, the\noperation is given by:\nD(x) = Mp \u25e6 x + (1 \u2212 Mp) \u25e6 GaussianBlur(x),\n(7)\nwhere Mp is a randomly generated binary mask with a\nprobability p dictating the presence of the original image,\nand (1 \u2212 Mp) represents the complementary mask applied\nto the Gaussian blurred image. M and x share the same\ndimensions. The parameter p controls the contribution of\nthe original image. A smaller p results in more informa-\ntion being retained from the input image, leading to a more\nstatic output video. Conversely, as p increases, the dynamic\nstrength of the output is amplified, producing more motion.\nExperimentally, we have found that this prior is criti-\ncal to obtaining stable results, especially for relatively weak\nbase T2I models such as Stable Diffusion V1.5 [30].\n4. Experiments\n4.1. Experimental Setup\nDatasets.\nWe employ the WebVid-10M [1] dataset for\ntraining our models. This dataset encompasses roughly 10\nmillion video clips in a resolution of 336 \u00d7 596, each av-\neraging 18 seconds. Each video clip is coupled with a text\ndescription of the video content. A notable complication\nwith WebVid-10M is the inclusion of watermarks, which\ninevitably appear in the generated videos. In order to gener-\nate videos without watermarks and at higher resolutions, we\nsupplement our training data with additional self-collected\nvideo clips that are free of watermarks in a resolution of 720\n\u00d7 1280. Finally, we employ the MJHQ-30K [26] dataset as\nthe benchmark for evaluation. This dataset encompasses 10\ncategories, each containing 3,000 samples, with each sam-\nple corresponding to an image and its text prompt.\nImplementation details.\nWe implement I2V-Adapter us-\ning two versions of AnimateDiff [9] based on Stable Dif-\nfusion V1.5 (SD1.5) [30] and SDXL [27], respectively.\nIn both versions, we freeze the parameters of the pre-\ntrained models and only fine-tune the newly added modules.\nSpecifically, for the SD1.5 version of AnimateDiff, we start\nby training on the WebVid-10M [1] dataset for one epoch,\nwhere input sequences of 16 frames are center-cropped to a\nresolution of 256 \u00d7 256. Subsequently, in the second phase,\nwe train for another one epoch on a self-collected dataset,\ncropping in the same manner to a resolution of 512 \u00d7 512.\nDuring both phases, we use the AdamW optimizer [20] with\na fixed learning rate of 1e-4. Considering our training strat-\negy does not predict for the first frame (the input image), we\ncalculate the loss only for the subsequent 15 frames. Due\n6\nTable 1. Quantitative comparison on the MJHQ-30K Dataset. In each column, the best number is highlighted in bold, while the second\nbest one is underlined.\nMethod\nReusable to\ncustom models\nCompatible with\ncontrollable tools\nMultimodal\ninputs\nTrainable\nparameters\nDoverVQA \u2191\nCLIPTemp \u2191\nFlowScore \u2191\nWarpingError \u2193\nGen-2 [33]\nN/A\nN/A\n!\nN/A\n0.0800\n0.9252\n0.2490\n12.5976\nPika [25]\nN/A\nN/A\n!\nN/A\n0.0691\n0.9021\n0.1316\n9.4587\nDynamiCrafter [46]\n%\n%\n%\n1.10B\n0.0848\n0.9275\n0.4185\n14.3825\nI2VGen-XL [50]\n%\n%\n!\n3.40B\n0.0459\n0.6904\n0.6111\n12.6566\nSVD [2]\n%\n%\n%\n1.52B\n0.0847\n0.9365\n0.6883\n20.8362\nI2V-Adapter (SD1.5)\n!\n!\n!\n24M\n0.0873\n0.9379\n0.7069\n9.8967\nI2V-Adapter (SDXL)\n!\n!\n!\n204M\n0.0887\n0.9389\n0.7124\n11.8388\nto the necessity of higher resolution inputs for training the\nSDXL version, the implementation of this version is carried\nout exclusively on the self-collected dataset in the resolution\nof 512 \u00d7 512, with all other settings remaining unchanged.\nMost results in our paper are based on the SDXL version.\nEvaluation metrics\nTo evaluate the results, we im-\nplement\nfour\nevaluation\nmetrics:\nDoverVQA\n[44],\nCLIPTemp [19], FlowScore [19], and WarpingError [19].\nDoverVQA indicates the video technical score, CLIPTemp\nreflects the consistency between the generated video and the\nreference image by calculating the average pairwise cosine\nsimilarity between input image and each frame. FlowS-\ncore measures the magnitude of motion within the video.\nWarpingError assesses the accuracy of the video\u2019s motion.\nFlowScore and WarpingError are obtained based on the op-\ntical flow computed via RAFT [36]. These metrics, reflect-\ning model performance from various perspectives, enable a\nmore comprehensive and impartial evaluation.\n4.2. Qualitative Results\nFor qualitative evaluations, we conduct comparisons in\nan open-domain scenario with four existing I2V models,\nwhich encompass two community open-sourced models\nSVD [2] and DynamicCrafter [46] and two commercial\nmodels Gen2 [33] and Pika [25]. The results in Figure 8\nillustrate that our model generates coherent and natural\nvideos while ensuring consistency between the input image\nand subsequent frames in terms of image identity. More-\nover, our results demonstrate superior responsiveness to\nthe input text prompt. Figure 7 presents additional results\nachieved by our method.\n4.3. Quantitative Results\nFor quantitative analysis, we randomly sample 50 image-\ntext pairs from MJHQ-30K dataset [26] to conduct evalu-\nations. Table 1 shows that our model achieves the highest\naesthetic score, surpassing all competing models in terms\nof consistency with the reference image. Additionally, our\nw/o I2V-Adapter \nw/o Content-Adapter\nFull model\nw/o Content-Adapter\nw/o I2V-Adapter\nFigure 4. Qualitative results of ablation study on model design.\ngenerated videos exhibit the largest range of motion, and a\nrelatively low warping error, indicating our model\u2019s ability\nto produce more dynamic videos while maintaining precise\nmotion accuracy.\n4.4. Ablation Study\nModel design\nTo validate the effectiveness of the key\ncomponents in our method, we use the proposed full\nmodel (the last row of Figure 4) as a baseline and design\nthree alternative experimental configurations for compar-\nison: (1) removing the I2V-Adapter module; (2) remov-\ning the Content-Adapter module; and (3) removing both\nof them. The corresponding results are shown in Figure 4.\nThe results reveal that the absence of the I2V-Adapter leads\nto significant loss of detail, as illustrated in the third row\nof Figure 4, where subsequent frames exhibit considerable\nchanges in colors and finer details, underscoring the mod-\nule\u2019s role in enhancing identity consistency. Conversely,\n7\nInput Image\nGenerated video with control sequence\nFigure 5. Our results with ControlNet [49]. The pose control is\nshown as an inset at the bottom left for each frame.\nInput Image\nGenerated Video\nw/ Blue Pencil-XL\nw/ Anime Art XL\nFigure 6. Our results with personalized T2I models [32].\nTable 2. Ablation study on parameters t0 and p of Frame Similar-\nity Prior.\nt0\np\nDoverVQA \u2191\nCLIPTemp \u2191\nFlowScore \u2191\nWarpingError \u2193\n1.0\n1.0\n0.0900\n0.9393\n0.8395\n11.7421\n0.9\n1.0\n0.0933\n0.9445\n0.4168\n10.0771\n0.8\n1.0\n0.0940\n0.9454\n0.2712\n9.1360\n0.7\n1.0\n0.0929\n0.9485\n0.1912\n8.3508\n0.6\n1.0\n0.0925\n0.9449\n0.1428\n7.6332\n1.0\n0.9\n0.0904\n0.9379\n0.9895\n12.0888\n1.0\n0.7\n0.0907\n0.9389\n0.9064\n11.8403\n1.0\n0.5\n0.0894\n0.9391\n0.8046\n11.6175\n1.0\n0.3\n0.0902\n0.9404\n0.7477\n11.4993\n1.0\n0.1\n0.0901\n0.9415\n0.6918\n11.3017\nin the absence of the Content-Adapter, the underlying in-\nformation of subsequent frames remains largely consistent\nwith the first frame, yet these frames tend to manifest in-\nstability, indicating a lack of global information guidance\nand control.\nFinally, the outcomes where both modules\nare omitted show poor performance, both in terms of detail\npreservation and global stability.\nFrame Similarity Prior\nTo verify the effectiveness of\nour Frame Similarity Prior (Section 3.3), we conduct ex-\nperiments by adjusting the parameters t0 and p (see Ta-\nble 2). We observe that increasing the p incrementally leads\nto a higher motion amplitude, as evidenced by the ascend-\ning values of the FlowScore metric. Simultaneously, the\nWarpingError metric exhibits a decreasing trend, indicating\nan enhancement in the stability of the generated video. Sim-\nilarly, a gradual increase of the parameter t0 also yields a\ncomparable effect. These results indicate that our approach\ncan effectively balance motion amplitude with the stability\nof the generated videos.\n4.5. More Applications\nBy keeping the parameters of the T2I models frozen, I2V-\nAdapter functions as a versatile plug-and-play module, al-\nlowing us to seamlessly integrate plugins from the open-\nsource community. This section showcases two such appli-\ncations. First, given an input image and a sequence of poses,\nour model, in combination with ControlNet [49], generates\na corresponding video (Figure 5).\nNotably, our method\nadapts well to non-square resolutions different from the\ntraining resolution, underscoring its adaptability and gen-\neralization performance. Second, with a reference image\nfeaturing a personalized style, our model can create a style-\nconsistent video based on personalized T2I models (e.g.,\nDreamBooth [32], lora [16]), as shown in Figure 6.\n5. Conclusion and Future work\nIn this paper, we introduce I2V-Adapter, a lightweight and\nefficient solution for text-guided image-to-video generation.\nOur method retains the prior knowledge of pretrained mod-\nels. Our method ingeniously propagates the unnoised input\nimage to subsequent frames through a cross-frame attention\nmechanism, thereby ensuring the output subsequent frames\nare consistent with the input image. Furthermore, we intro-\nduce a novel Frame Similarity Prior to balance stability and\nmotion magnitude of generated videos.\nOur method has demonstrated effectiveness in open-\ndomain image-to-video generation, producing high-quality\nvideos with a reduced number of trainable parameters. In\naddition, our method retains the prior knowledge of pre-\ntrained models and the decoupled design ensures compati-\nbility with existing community-driven personalized models\nand controlling tools, facilitating the creation of personal-\nized and controllable videos without further training.\nIn future work, we plan to incorporate frame interpola-\ntion modules and spatial super-resolution modules to gener-\nate videos of longer duration and higher resolution.\n8\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 6\n[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3, 7,\n12\n[3] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter1: Open diffusion models for high-quality video\ngeneration, 2023. 2, 3\n[4] Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu\nLiu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real\nimage animation with text-guided motion control.\narXiv\npreprint arXiv:2312.02928, 2023. 2\n[5] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin\nZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu\nQiao, and Ziwei Liu.\nSeine: Short-to-long video diffu-\nsion model for generative transition and prediction. arXiv\npreprint arXiv:2310.20700, 2023. 3\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota, 2019. Association for\nComputational Linguistics. 4\n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 4\n[8] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi\nYin, Devi Parikh, and Ishan Misra. Emu video: Factoriz-\ning text-to-video generation by explicit image conditioning.\narXiv preprint arXiv:2311.10709, 2023. 2\n[9] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 4, 6\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 4\n[11] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 3\n[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In The Eleventh Inter-\nnational Conference on Learning Representations, 2022. 4,\n5\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3, 4\n[14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2, 3\n[15] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly.\nParameter-efficient transfer\nlearning for nlp. In International Conference on Machine\nLearning, pages 2790\u20132799. PMLR, 2019. 3\n[16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.\nIn\nThe Tenth International Conference on Learning Represen-\ntations, ICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net, 2022. 2, 3, 8\n[17] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3, 4\n[18] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu,\nQi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong\nWang. Videogen: A reference-guided latent diffusion ap-\nproach for high definition text-to-video generation.\narXiv\npreprint arXiv:2309.00398, 2023. 3\n[19] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang,\nYong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Ray-\nmond Chan, and Ying Shan. Evalcrafter: Benchmarking and\nevaluating large video generation models.\narXiv preprint\narXiv:2310.11440, 2023. 7\n[20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[21] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions, 2022. 6\n[22] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\nZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-\nadapter:\nLearning adapters to dig out more controllable\nability for text-to-image diffusion models.\narXiv preprint\narXiv:2302.08453, 2023. 4\n[23] John Mullan,\nDuncan Crawbuck,\nand Aakash Sastry.\nHotshot-XL.\nhttps://github.com/hotshotco/\nhotshot-xl, 2023. 2, 3\n[24] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-\nmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell\nHowes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael\nRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00b4e Je-\ngou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr\nBojanowski. Dinov2: Learning robust visual features with-\n9\nout supervision. arXiv preprint arXiv:2304.07193, 2304. 2,\n5\n[25] Pika. Pika. https://pika.art/, 2023. 7, 12\n[26] PlaygroundAI.\nMJHQ-30K\nDataset.\nhttps :\n//huggingface.co/datasets/playgroundai/\nMJHQ-30K, 2023. 6, 7\n[27] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. SDXL: improving latent diffusion models\nfor high-resolution image synthesis. CoRR, abs/2307.01952,\n2023. 2, 6\n[28] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. Technical report, OpenAI, 2018. 4\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, pages 8748\u2013\n8763. PMLR, 2021. 2, 3, 4, 5\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2022,\nNew Orleans, LA, USA, June 18-24, 2022, pages 10674\u2013\n10685. IEEE, 2022. 2, 3, 4, 6\n[31] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 3, 4, 5\n[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2023, Vancouver, BC,\nCanada, June 17-24, 2023, pages 22500\u201322510. IEEE,\n2023. 2, 3, 5, 8\n[33] RunwayAI. Gen2. https://research.runwayml.\ncom/gen2, 2023. 7, 12\n[34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2020. 3, 4\n[35] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, 2020. 3, 4, 6\n[36] Zachary Teed and Jia Deng. RAFT: recurrent all-pairs field\ntransforms for optical flow. In ECCV (2), pages 402\u2013419.\nSpringer, 2020. 7\n[37] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 4, 5\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[39] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal.\nMcvd-masked conditional video diffusion for prediction,\ngeneration, and interpolation. Advances in Neural Informa-\ntion Processing Systems, 35:23371\u201323385, 2022. 3\n[40] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2,\n3\n[41] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 3\n[42] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 2, 3\n[43] Sam Witteveen and Martin Andrews.\nInvestigating\nprompt engineering in diffusion models.\narXiv preprint\narXiv:2211.15462, 2022. 2\n[44] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-\nwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan,\nand Weisi Lin. Exploring video quality assessment on user\ngenerated contents from aesthetic and technical perspectives.\nIn International Conference on Computer Vision (ICCV),\n2023. 7\n[45] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi\nLi, and Xiangyu Zhang.\nLamp:\nLearn a motion pat-\ntern for few-shot-based video generation.\narXiv preprint\narXiv:2310.10769, 2023. 3, 5\n[46] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-\ntao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:\nAnimating open-domain images with video diffusion priors.\narXiv preprint arXiv:2310.12190, 2023. 7, 12\n[47] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 4, 5\n[48] Yan Zeng,\nGuoqiang Wei,\nJiani Zheng,\nJiaxin Zou,\nYang Wei, Yuchen Zhang, and Hang Li.\nMake pixels\ndance:\nHigh-dynamic video generation.\narXiv preprint\narXiv:2311.10982, 2023. 2, 3\n[49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n1, 2, 3, 4, 5, 6, 8\n[50] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 2, 3, 7\n10\nThe cutest dog ever seen, outside playing in the mountains of France.\nHyper cute, cuddly pet photography kittens cuteness overload.\nMan hair short and dark on a ship and throws an anchor water a harbor\nsunshine island in the background life journey be romantic.\nA girl is charging forward in the snow, portraying the martial arts in\nChinese mythological stories, creating a tense and dynamic atmosphere.\nAstronaut emerging colorful water and reaching for the stars, bright galaxy\nabove the water, spaceman, dramatic lighting.\nModern digital street fighter versus screen, with elon musk as kazuya and\njack as an ai robot going head to head.\nPouring an orbital mechanics coffee.\nHuge triplepatty burger over a city, in the style of beeple.\nGigant warm is destroying a city in real time live tucker reaction.\nGallons of water are pouring down on Hailee Steinfelds head, soaking wet,\nfull body.\nThe great wall of China at sunset, in the style of skyblue and beige, 32k\nuhd, colorful street murals.\n2 characters, obligatorily 1 elephant and 1 giraffe, together side by side,\nmaking a cake, happy, exaggerated facial expression.\nA cute Dutch pig wearing a future space suit,the background is the universe,\ntide play blind box, Pixar, pastel color, Busts, Natural light.\nWinter night. Wide shot of a small log cabin on fire. A very old man on his\nhands and knees on the ground.\nFigure 7. More videos of diverse categories generated by I2V-Adapter, with the text prompt at the bottom and the leftmost image as input.\n11\nGen2\nPika\nDynamicCrafter\nI2VGen-XL\nI2V-Adapter\nMario\nin\na\n3D\nMinecraft\nworld,\nblocky\nworld,\nMinecraft\nworld,\nminecraft\ntrees,\nsunset\nin\nthe\nhorizon, colourful lighting, centered, front view, red\nhat, white gloves, blue pants, moustache, ultrarealistic,\nunreal engine 5, HDR, Ray tracing, cinematic, depth\nof field, sharp focus, natural light, concept art, super\nresolution, cartoon style, pixar style, flat coloring.\nSVD\nInput Image \nInput prompt  \nLong\nexposure\nphotograph\nof\na\nranch\nat\nnight,\nfeaturing a big red barn and a large house amidst a\ndramatic\nthunderstorm,\ncapturing\nthe\nlightning\nilluminating\nthe\nsky\nand\nthe\nsurrounding\nlandscape.\nShot with a Sony A7R IV, 1635mm f2.8 lens, ISO 100,\n30s\nshutter\nspeed,\nand\na\nsturdy\ntripod\nto\nensure\nsharpness in the captured lightning bolts.\nInput Image \nInput prompt  \nFigure 8. We compare our results with those obtained by SVD [2], Gen2 [33], Pika [25], and DynamiCrafter [46], which are state-of-the-art\nI2V approaches. Results show that our model not only generates coherent and natural videos but also ensures consistency between the\ninput image and subsequent frames. Moreover, our results demonstrate superior responsiveness to the input text prompt.\n12\n"
  },
  {
    "title": "InsActor: Instruction-driven Physics-based Characters",
    "link": "https://arxiv.org/pdf/2312.17135.pdf",
    "upvote": "9",
    "text": "InsActor: Instruction-driven Physics-based Characters\nJiawei Ren\u22171\nMingyuan Zhang\u22171\nCunjun Yu\u22172\nXiao Ma3\nLiang Pan1\nZiwei Liu1\n1 S-Lab, Nanyang Technological University\n2 National University of Singapore\n3 Dyson Robot Learning Lab\nAbstract\nGenerating animation of physics-based characters with intuitive control has long\nbeen a desirable task with numerous applications. However, generating physically\nsimulated animations that reflect high-level human instructions remains a difficult\nproblem due to the complexity of physical environments and the richness of human\nlanguage. In this paper, we present InsActor, a principled generative framework\nthat leverages recent advancements in diffusion-based human motion models to\nproduce instruction-driven animations of physics-based characters. Our frame-\nwork empowers InsActor to capture complex relationships between high-level\nhuman instructions and character motions by employing diffusion policies for\nflexibly conditioned motion planning. To overcome invalid states and infeasible\nstate transitions in planned motions, InsActor discovers low-level skills and maps\nplans to latent skill sequences in a compact latent space. Extensive experiments\ndemonstrate that InsActor achieves state-of-the-art results on various tasks, includ-\ning instruction-driven motion generation and instruction-driven waypoint heading.\nNotably, the ability of InsActor to generate physically simulated animations using\nhigh-level human instructions makes it a valuable tool, particularly in executing\nlong-horizon tasks with a rich set of instructions. Our project page is available at\njiawei-ren.github.io/projects/insactor\n1\nIntroduction\nGenerating life-like natural motions in a simulated environment has been the focus of physics-based\ncharacter animation [25, 17]. To enable user interaction with the generated motion, various conditions\nsuch as waypoints have been introduced to control the generation process [44, 15]. In particular,\nhuman instructions, which have been widely adopted in text generation and image generation,\nhave recently drawn attention in physics-simulated character animation [15]. The accessibility and\nversatility of human instructions open up new possibilities for downstream physics-based character\napplications.\nTherefore, we investigate a novel task in this work: generating physically-simulated character\nanimation from human instruction. The task is challenging for existing approaches. While motion\ntracking [4] is a common approach for character animation, it presents challenges when tracking novel\nmotions generated from free-form human language. Recent advancements in language-conditioned\ncontrollers [15] have demonstrated the feasibility of managing characters using instructions, but\nthey struggle with complex human commands. On the other hand, approaches utilizing conditional\ngenerative models to directly generate character actions [14] fall short of ensuring the accuracy\nnecessary for continuous control.\nTo tackle this challenging task, we present InsActor, a framework that employs a hierarchical design\nfor creating instruction-driven, physics-based characters. At the high level, InsActor generates motion\n\u2217Equal contribution\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.17135v1  [cs.CV]  28 Dec 2023\n\u201cit then walks like \na zombie to \ud83d\udea9\u201d\n\u201ckick!\u201d\n\u201cnow it walks \nto \ud83d\udea9\u201d\n\u201ca person deep squats\nand jumps to \ud83d\udea9\u201d\nFigure 1: InsActor enables controlling physics-based characters with human instructions and intended\ntarget position. The figure illustrates this by depicting several \u201cflags\u201d on a 2D plane, each representing\na relative target position such as (0,2) starting from the origin.\nplans conditioned on human instructions. This approach enables the seamless integration of human\ncommands, resulting in more coherent and intuitive animations. To accomplish this, InsActor utilizes\na diffusion policy [24, 5] to generate actions in the joint space conditioned on human inputs. It allows\nflexible test-time conditioning, which can be leveraged to complete novel tasks like waypoint heading\nwithout task-specific training. However, the high-level diffusion policy alone does not guarantee valid\nstates or feasible state transitions, making it insufficient for direct execution of the plans using inverse\ndynamics [1]. Therefore, at the low level, InsActor incorporates unsupervised skill discovery to\nhandle state transitions between pairs of states, employing an encoder-decoder architecture. Given the\nstate sequence in joint space from the high-level diffusion policy, the low-level policy first encodes it\ninto a compact latent space to address any infeasible joint actions from the high-level diffusion policy.\nEach state transition pair is mapped to a skill embedding within this latent space. Subsequently,\nthe decoder translates the embedding into the corresponding action. This hierarchical architecture\neffectively breaks down the complex task into two manageable tasks at different levels, offering\nenhanced flexibility, scalability, and adaptability compared to existing solutions.\nGiven that InsActor generates animations that inherently ensure physical plausibility, the primary\nevaluation criteria focus on two aspects: fidelity to human instructions and visual plausibility of the\nanimations. Through comprehensive experiments assessing the quality of the generated animations,\nInsActor demonstrates its ability to produce visually captivating animations that faithfully adhere\nto instructions, while maintaining physical plausibility. Furthermore, thanks to the flexibility of the\ndiffusion model, animations can be further customized by incorporating additional conditions, such\nas waypoints, as illustrated in Figure 1, showcasing the broad applicability of InsActor. In addition,\nInsActor also serves as an important baseline for language conditioned physics-based animation\ngeneration.\n2\nRelated Works\n2.1\nHuman Motion Generation\nHuman motion generation aims to produce versatile and realistic human movements [12, 22, 23,\n43]. Recent advancements have enabled more flexible control over motion generation [2, 10, 42,\n9]. Among various methods, the diffusion model has emerged as a highly effective approach for\ngenerating language-conditioned human motion [39, 48]. However, ensuring physical plausibility,\nsuch as avoiding foot sliding, remains challenging due to the absence of physical priors and interaction\nwith the environment [46, 31, 34]. Some recent efforts have attempted to address this issue by\nincorporating physical priors into the generation models [45], such as foot contact loss [39]. Despite\nthis progress, these approaches still struggle to adapt to environmental changes and enable interaction\nwith the environment. To tackle these limitations, we propose a general framework for generating\nlong-horizon human animations that allow characters to interact with their environment and remain\n2\nDiffuser\n\ud835\udc60!\"#\n$\n\ud835\udc60!\"#\n$%#\n\ud835\udc60!\"&\n$\n\ud835\udc60!\"&\n$%#\n\ud835\udc60'\n\ud835\udc60'\n$%#\n\u2026\n\u2026\nInstruction \ud83d\udde3\n\u201cA person deep \nsquats and jumps.\u201d\nDenoised Plan\nWaypoint \ud83d\udea9\nHistory\nCondition\nGuide\n\ud835\udc60#\n\ud835\udc60!\nSkills\n\ud835\udc60!\n\ud835\udc60!\"#\n(\n\u2026\n\ud835\udc5d) \ud835\udc4e \ud835\udc60, \ud835\udc67\nDecoder\n\ud835\udc60!\"#\n\ud835\udc60!\"&\n(\n\ud835\udcb5\n\ud835\udc47 \u00d7\n\u2026\n\u2026\nFigure 2: The overall framework of InsActor. At the high level, the diffusion model generates state\nsequences from human instructions and waypoint conditions. At the low level, each state transition is\nencoded into a skill embedding in the latent space and decoded to an action.\nrobust to environmental changes. Our approach strives to bridge the gap between understanding\ncomplex high-level human instructions and generating physically-simulated character motions.\n2.2\nLanguage-Conditioned Control\nLanguage-Conditioned Control aims to guide an agent\u2019s behavior using natural language, which\nhas been extensively applied in physics-based animation and robot manipulation [38, 21, 20] to\nensure compliance with physical constraints. However, traditional approaches often necessitate\ndedicated language modules to extract structured expressions from free-form human languages or\nrely on handcrafted rules for control [35, 47, 36]. Although recent attempts have trained data-driven\ncontrollers to generate actions directly from human instructions [19, 15], executing long-horizon\ntasks remains challenging due to the need to simultaneously understand environmental dynamics,\ncomprehend high-level instructions, and generate highly accurate control. The diffusion model,\nconsidered one of the most expressive models, has been introduced to generate agent actions [14].\nNonetheless, current methods are unable to accurately control a humanoid character, as evidenced by\nour experiments. Recent work utilizing diffusion model to generate high-level pedestrian trajectories\nand ground the trajectories with a low level controller [32]. Compared with existing works, InsActor\nemploys conditional motion generation to capture intricate relationships between high-level human\ninstructions and character motions beyond pedestrian trajectories, and subsequently deploys a low-\nlevel skill discovery incorporating physical priors. This approach results in animations that are both\nvisually striking and physically realistic.\n3\nThe Task of Instruction-driven Physics-based Character Animation\nWe formulate the task as conditional imitation learning, in which we learn a goal-conditioned policy\nthat outputs an action a \u2208 A based on the current state s \u2208 S and an additional condition c \u2208 C\ndescribing the desired character behavior. The environment dynamics are represented by the function\nT : S \u00d7 A \u2192 S.\nThe task state comprises the character\u2019s pose and velocity, including position p, rotation q, linear\nvelocity \u02d9p, and angular velocity \u02d9q for all links in local coordinates. Consequently, the task state, s,\ncontains this information: s := {p, q, \u02d9p, \u02d9q}. Following common practice, we employ PD controllers\nto drive the character. Given the current joint angle q, angular velocity \u02d9q, and target angle \u02dcq, the\ntorque on the joint actuator is computed as kp(\u02dcq \u2212 q) + kd(\u02dc\u02d9q \u2212 \u02d9q), where \u02dc\u02d9q = 0, kp and kd are\nmanually specified PD controller gains. We maintain kp and kd identical to the PD controller used in\nDeepMimic [26]. The action involves generating target angles for all joints to control the character.\nOur work addresses a realistic setting in which demonstration data consists solely of states without\nactions, a common scenario in motion datasets collected from real humans where obtaining actions\nis challenging [27, 3, 13]. To train our model, we use a dataset of trajectory-condition pairs D =\n{(\u03c4 i, ci)}N\ni=1, where \u03c4 = {s\u2217\n1, ..., s\u2217\nL} denotes a state-only demonstration generated by the expert of\n3\nlength L. We use s\u2217 to denote states from the expert demonstration. For example, a human instruction\ncan be \u201cwalk like a zombie\u201d and the trajectory would be a state sequence describing the character\u2019s\nmotion. Our objective is to learn a policy in conjunction with environment dynamics T , which can\nreplicate the expert\u2019s trajectory for given instruction c \u2208 C.\n4\nThe Framework of InsActor\nThe proposed method, InsActor, employs a unified hierarchical approach for policy learning, as\ndepicted in Figure 2. Initially, a diffusion policy interprets high-level human instructions, generating\na sequence of actions in the joint space. In our particular case, the action in the joint space can be\nregarded as the state of the animated character. Subsequently, each pair of actions in the joint space\nare mapped into the corresponding skill embedding in the latent space, ensuring their plausibility\nwhile producing desired actions for character control in accordance with motion priors. Consequently,\nInsActor effectively learns intricate policies for animation generation that satisfy user specifications\nin a physically-simulated environment. The inference process of InsActor is detailed in Algorithm 1.\n4.1\nHigh-Level Diffusion Policy\nFor the high-level state diffusion policy, we treat the joint state of the character as its action. We\nfollow the state-of-the-art approach of utilizing diffusion models to carry out conditional motion\ngeneration [48, 39]. We denote the human instruction as c and the state-only trajectory as \u03c4.\nTrajectory Curation.\nIn order to use large-scale datasets for motion generation in a physical\nsimulator, it is necessary to retarget the motion database to a simulated character to obtain a collection\nof reference trajectories. Large-scale text-motion databases HumanML3D [8] and KIT-ML [27] use\nSMPL [18] sequences to represent motions. SMPL describes both the body shape and the body poses,\nwhere the body poses include pelvis location and rotation, the relative joint rotation of the 21 body\njoints. We build a simulated character to have the same skeleton as SMPL. We scale the simulated\ncharacter to have a similar body size to a mean SMPL neutral shape. For retargeting, we directly\ncopy the joint rotation angle, pelvis rotation, and translation to the simulated character. A vertical\noffset is applied to compensate for different floor heights.\nDiffusion Models.\nDiffusion models [11] are probabilistic techniques used to remove Gaussian\nnoise from data and generate a clean output. These models consist of two processes: the diffusion\nprocess and the reverse process. The diffusion process gradually adds Gaussian noise to the original\ndata for a specified number of steps, denoted by T, until the distribution of the noise closely\napproximates a standard Gaussian distribution, denoted by N(0, I). This generates a sequence of\nnoisy trajectories denoted by \u03c4 1:T = {\u03c4 1, ..., \u03c4 T }. The original data is sampled from a conditional\ndistribution, \u03c4 0 \u223c p(\u03c4 0 | c), where c is the instruction. Assuming that the variance schedules are\ndetermined by \u03b2t, the diffusion process is defined as:\nq(\u03c4 1:T |\u03c4 0) :=\nT\nY\nt=1\nq(\u03c4 t|\u03c4 t\u22121),\nq(\u03c4 t|\u03c4 t\u22121) := N(\u03c4 t;\np\n1 \u2212 \u03b2t\u03c4 t\u22121, \u03b2tI),\n(1)\nwhere q(\u03c4 t|\u03c4 t\u22121) is the conditional distribution of each step in the Markov chain. The parameter \u03b2t\ncontrols the amount of noise added at each step t, with larger values resulting in more noise added.\nThe reverse process in diffusion models is another Markov chain that predicts and removes the added\nnoise using a learned denoising function. In particular, we encode language c into an encoded latent\nvector, \u02c6c, using the classical transformer [40] as the language encoder, \u02c6c = E(c). Thus, the reverse\nprocess starts with a distribution p(\u03c4 T ) := N(\u03c4 T ; 0, I) and is defined as:\np(\u03c4 0:T |\u02c6c) := p(\u03c4 T )\nT\nY\nt=1\np(\u03c4 t\u22121|\u03c4 t, \u02c6c),\np(\u03c4 t\u22121|\u03c4 t, \u02c6c) := N(\u03c4 t\u22121; \u00b5(\u03c4 t, t, \u02c6c), \u03a3(\u03c4 t, t, \u02c6c)).\n(2)\nHere, p(\u03c4 t\u22121|\u03c4 t, \u02c6c) is the conditional distribution at each step in the reverse process. The mean\nand covariance of the Gaussian are represented by \u00b5 and \u03a3, respectively. During training, steps t\nare uniformly sampled for each ground truth motion \u03c4 0, and a sample is generated from q(\u03c4 t|\u03c4 0).\nInstead of predicting the noise term \u03f5 [11], the model predicts the original data \u03c4 0 directly which has\n4\nAlgorithm 1 Inference of InsActor\n1: Input: A instruction c, the diffusion model f\u03b8, the skill encoder, q\u03d5 and decoder p\u03c8. Diffusion\nsteps T. A language encoder E. A history o = {\u02c6s1, ...,\u02c6sl}. A waypoint h. Animation length L.\n2: Output: A physically simulated trajectory, \u02c6\u03c4.\n\u25b7 Generate state sequence.\n3: w \u2190 Initialize a plan from a Gaussian noise, w \u223c N(0, I)\n4: w \u2190 Apply the inpainting strategy in [14] for history o and waypoint h to guide diffusion.\n5: \u02c6c \u2190 Encode the instruction E(c)\n6: \u03c4 = {s1, ....sL} \u2190 Generate trajectory with the diffusion model f\u03b8(w, T, \u02c6c)\n\u25b7 Generate action sequence in a closed-loop manner.\n7: for i = l, l + 1, ..., (L \u2212 1) do\n8:\nzi \u2190 Sample z from q\u03d5(zi|\u02c6si, si+1)\n9:\nai \u2190 Sample action form p\u03c8(ai|\u02c6si, zi)\n10:\n\u02c6si+1 \u2190 Get next state T (\u02c6si+1|\u02c6si, ai)\n11: end for\n12: Output \u02c6\u03c4 = {\u02c6sl+1, ....\u02c6sL}\nthe equivalent formulation [30, 39]. This is done by using a neural network f\u03b8 parameterized by \u03b8 to\npredict \u03c4 0 from the noisy trajectory \u03c4 t and the condition \u02c6c at each step t in the denoise process. The\nmodel parameters are optimized by minimizing the mean squared error between the predicted and\nground truth data using the loss function:\nLPlan = Et\u2208[1,T ],\u03c4 0\u223cp(\u03c4 0|c)[\u2225 \u03c4 0 \u2212 f\u03b8(\u03c4 t, t, \u02c6c) \u2225],\n(3)\nwhere p(\u03c4 0|c) is the conditional distribution of the ground truth data, and \u2225 \u00b7 \u2225 denotes the mean\nsquared error. By directly predicting \u03c4 0, this formulation avoids repeatedly adding noise to \u03c4 0 and is\nmore computationally efficient.\nGuided Diffusion.\nDiffusion models allow flexible test-time conditioning through guided sampling,\nfor example, classifier-guided sampling [6]. Given an objective function as a condition, gradients\ncan be computed to optimize the objective function and perturb the diffusion process. In particular,\na simple yet effective inpainting strategy can be applied to introduce state conditions [14], which\nis useful to generate a plan that adheres to past histories or future goals. Concretely, the inpainting\nstrategy formulates the state conditioning as a Dirac delta objective function. Optimizing the objective\nfunction is equivalent to directly presetting noisy conditioning states and inpainting the rest. We\nleverage the inpainting strategy to achieve waypoint heading and autoregressive generation.\nLimitation.\nDespite the ability to model complex language-to-motion relations, motion diffusion\nmodels can generate inaccurate low-level details, which lead to physically implausible motions and\nartifacts like foot floating and foot penetration [39]. In the context of state diffusion, the diffuser-\ngenerated states can be invalid and the state transitions can be infeasible. Thus, direct tracking of the\ndiffusion plan can be challenging.\n4.2\nLow-Level Skill Discovery\nTo tackle the aforementioned challenge, we employ low-level skill discovery to safeguard against\nunexpected states in poorly planned trajectories. Specifically, we train a Conditional Variational\nAutoencoder to map state transitions to a compact latent space in an unsupervised manner [44]. This\napproach benefits from a repertoire of learned skill embedding within a compact latent space, enabling\nsuperior interpolation and extrapolation. Consequently, the motions derived from the diffusion model\ncan be executed by natural motion primitives.\nSkill Discovery.\nAssuming the current state of the character is \u02c6sl, the first step in constructing a\ncompact latent space for skill discovery is encoding the state transition in a given reference motion\nsequence, \u03c4 0 = {s\u2217\n1, ..., s\u2217\nL}, into a latent variable, z, and we call this skill embedding. This variable\nrepresents a unique skill required to transition from \u02c6sl to s\u2217\nl+1. The neural network used to encode\nthe skill embedding is referred to as the encoder, q\u03d5, parameterized by \u03d5, which produces a Gaussian\ndistribution:\nq\u03d5(zl|\u02c6sl, s\u2217\nl+1) := N(zl; \u00b5\u03d5(\u02c6sl, s\u2217\nl+1), \u03a3\u03d5(\u02c6sl, s\u2217\nl+1)),\n(4)\n5\n\u201cA person picks up.\u201d\n\u201cA person doing martial arts.\u201d\n\u201cA person raises arms\nand walks.\u201d\n\u201cA person jumps.\u201d\n\u201cA person spins.\u201d\n\u201cA person walks like\na zombie.\u201d\nFigure 3: Qualitative results of InsActor with corresponding instructions. Top: only human\ninstruction. Bottom: human instruction and waypoint target.\nwhere \u00b5\u03d5 is the mean and \u03a3\u03d5 is the isotropic covariance matrix. Once we obtain the latent variable, a\ndecoder, p\u03c8(al|\u02c6sl, zl), parameterized by \u03c8, generates the corresponding actions, a, by conditioning\non the latent variable zl and the current state \u02c6sl:\nal \u223c p\u03c8(al|\u02c6sl, zl)\n(5)\nSubsequently, using the generated action, the character transitions into the new state, \u02c6sl+1, via the\ntransition function T (\u02c6sl+1|\u02c6sl, al). By repeating this process, we can gather a generated trajectory,\ndenoted as \u02c6\u03c4 = {\u02c6s1, ...,\u02c6sL}. The goal is to mimic the given trajectory \u03c4 0 by performing the actions.\nThus, to train the encoder and decoder, the main supervision signal is derived from the difference\nbetween the resulting trajectory \u02c6\u03c4 and the reference motion, \u03c4 0.\nTraining.\nOur approach leverages differentiable physics to train the neural network end-to-end\nwithout the need for a separate world model [33]. This is achieved by implementing the physical\nlaws of motion as differentiable functions, allowing the gradient to flow through them during\nbackpropagation. Concretely, by executing action al \u223c p\u03c8(al|\u02c6sl, zl) at state \u02c6sl, the induced state\n\u02c6sl+1 is differentiable with respect to the policy parameter \u03c8 and \u03d5. Thus, directly minimizing the\ndifference between the predicted state \u02c6s and the s\u2217 gives an efficient and effective way of training\nan imitation learning policy [33]. The Brax [7] simulator is used due to its efficiency and easy\nparallelization, allowing for efficient skill discovery. It also ensures that the learned skill is trained on\nthe actual physical environment, rather than a simplified model of it, leading to a more accurate and\nrobust representation.\nThus, the encoder-decoder is trained with an objective that minimizes the discrepancy between\nresulting trajectories and Kullback\u2013Leibler divergence between the encoded latent variable and the\nprior distribution, which is a standard Gaussian,\nLSkill =\u2225 \u03c4 0 \u2212 \u02c6\u03c4 \u2225 +\u03bbDKL(q\u03d5(z|s, s\u2032) \u2225 N(0, I)),\n(6)\nwhere \u2225 \u00b7 \u2225 denotes the mean squared error and (s, s\u2032) is a pair of states before and after transition.\nThe latter term encourages the latent variables to be similar to the prior distribution, ensuring the\ncompactness of the latent space. \u03bb is the weight factor that controls the compactness. During\ninference, we map the generated state sequence from the diffusion model to the skill space to control\nthe character.\n5\nExperiments\nThe goal of our experiment is to evaluate the effectiveness and robustness of InsActor in generating\nphysically-simulated and visually-natural character animations based on high-level human instruc-\ntions. Specifically, we aim to investigate 1) whether InsActor can generate animations that adhere to\nhuman instructions while being robust to physical perturbations, 2) whether InsActor can accomplish\nwaypoint heading while being faithful to the language descriptions, and 3) the impact of several\ndesign choices, including the hierarchical design and the weight factor for skill space compactness.\n6\nTable 1: Quantitative results on the KIT-ML test set. \u2020: with perturbation.\nMethods\nR Precision\u2191\nMultimodal\nFID\u2193\nDiversity\u2191\nTop 1\nTop 2\nTop 3\nDist\u2193\nDReCon [4]\n0.243\u00b1.000\n0.420\u00b1.021\n0.522\u00b1.039\n2.310\u00b1.097\n1.055\u00b1.162\n4.259\u00b1.014\nPADL [15]\n0.091\u00b1.003\n0.172\u00b1.008\n0.242\u00b1.015\n3.482\u00b1.038\n3.889\u00b1.104\n2.940\u00b1.031\nInsActor (Ours)\n0.352\u00b1.013\n0.550\u00b1.010\n0.648\u00b1.015\n1.808\u00b1.027\n0.786\u00b1.055\n4.392\u00b1.071\nDReCon\u2020 [4]\n0.253\u00b1.013\n0.384\u00b1.006\n0.447\u00b1.006\n2.764\u00b1.003\n1.973\u00b1.100\n4.252\u00b1.040\nPADL\u2020 [15]\n0.100\u00b1.012\n0.158\u00b1.011\n0.217\u00b1.015\n3.783\u00b1.069\n4.706\u00b1.298\n3.168\u00b1.065\nInsActor (Ours)\u2020\n0.323\u00b1.013\n0.496\u00b1.017\n0.599\u00b1.008\n2.147\u00b1.061\n1.043\u00b1.091\n4.359\u00b1.073\nTable 2: Quantitative results on the HumanML3D test set. \u2020: with perturbation.\nMethods\nR Precision\u2191\nMultimodal\nFID\u2193\nDiversity\u2191\nTop 1\nTop 2\nTop 3\nDist\u2193\nDReCon [4]\n0.265\u00b1.007\n0.391\u00b1.004\n0.470\u00b1.001\n2.570\u00b1.002\n1.244\u00b1.040\n4.070\u00b1.062\nPADL [15]\n0.144\u00b1.003\n0.227\u00b1.012\n0.297\u00b1.018\n3.349\u00b1.030\n2.162\u00b1.022\n3.736\u00b1.091\nInsActor (Ours)\n0.331\u00b1.000\n0.497\u00b1.015\n0.598\u00b1.001\n1.971\u00b1.004\n0.566\u00b1.023\n4.165\u00b1.076\nDReCon\u2020 [4]\n0.233\u00b1.000\n0.352\u00b1.001\n0.424\u00b1.004\n2.850\u00b1.002\n1.829\u00b1.002\n4.008\u00b1.147\nPADL\u2020 [15]\n0.117\u00b1.005\n0.192\u00b1.003\n0.254\u00b1.000\n3.660\u00b1.040\n2.964\u00b1.115\n3.849\u00b1.159\nInsActor (Ours)\u2020\n0.312\u00b1.001\n0.455\u00b1.006\n0.546\u00b1.003\n2.203\u00b1.006\n0.694\u00b1.005\n4.212\u00b1.154\n5.1\nImplementation Details\nTo implement the experiment, we use Brax [7] to build the environment and design a simulated\ncharacter based on DeepMimic [26]. The character has 13 links and 34 degrees of freedom, weighs\n45kg, and is 1.62m tall. Contact is applied to all links with the floor. For details of neural network\narchitecture and training, we refer readers to the supplementary materials.\n5.2\nEvaluation Protocols\nDatasets.\nWe use two large scale text-motion datasets, KIT-ML [27] and HumanML3D [8], for\ntraining and evaluation. KIT-ML has 3,911 motion sequences and 6,353 sequence-level language\ndescriptions, HumanML3D provides 44,970 annotations on 14,616 motion sequences. We adopt the\noriginal train/test splits in the two datasets.\nMetrics.\nWe employ the following evaluation metrics:\n1. R Precision: For every pair of generated sequence and instruction, we randomly pick 31\nadditional instructions from the test set. Using a trained contrastive model, we then compute\nthe average top-k accuracy.\n2. Frechet Inception Distance (FID): We use a pre-trained motion encoder to extract features\nfrom both the generated animations and ground truth motion sequences. The FID is then\ncalculated between these two distributions to assess their similarity.\n3. Multimodal Distance: With the help of a pre-trained contrastive model, we compute the\ndisparity between the text feature derived from the given instruction and the motion feature\nfrom the produced animation. We refer to this as the multimodal distance.\n4. Diversity: To gauge diversity, we randomly divide the generated animations for all test texts\ninto pairs. The average joint differences within each pair are then computed as the metric\nfor diversity.\n5. Success Rate: For waypoint heading tasks, we compute the Euclidean distance between\nthe final horizontal position of the character pelvis and the target horizontal position. If the\ndistance is less than 0.5 m, we deem it a success. We perform each evaluation three times\nand report the statistical interval with 95% confidence.\n7\nTable 3: Quantitative results for the waypoint heading task. Evaluated on HumanML3D. We set\nthe start point at (0,0) and the waypoint uniformly sampled from a 6x6 square centered at (0,0). It\nis considered a successful waypoint heading if the final position is less than 0.5m away from the\nwaypoint. L: Langauge. W: Waypoint.\nMethod\nL\nW\nR Precision\u2191\nMultimodal\nFID\u2193\nDiversity\u2191\nSuccess\nTop 3\nDist\u2193\nRate\u2191\nDReCon [4]\n\u2713\n\u2713\n0.178\u00b1.000\n4.192\u00b1.019\n8.607\u00b1.114\n2.583\u00b1.157\n0.380\u00b1.002\nInsActor (Ours)\n\u00d7\n\u2713\n0.089\u00b1.001\n4.106\u00b1.001\n3.041\u00b1.101\n3.137\u00b1.029\n0.935\u00b1.002\nInsActor (Ours)\n\u2713\n\u00d7\n0.598\u00b1.001\n1.971\u00b1.004\n0.566\u00b1.023\n4.165\u00b1.076\n0.081\u00b1.004\nInsActor (Ours)\n\u2713\n\u2713\n0.388\u00b1.003\n2.753\u00b1.009\n2.527\u00b1.015\n3.285\u00b1.034\n0.907\u00b1.002\nTable 4: Ablation on hierarchical design. Evaluated on the KIT-ML test set.\nHigh\nLow\nR Precision\u2191\nMultiModal\nFID\u2193\nDiversity\u2191\nTop 1\nTop 2\nTop 3\nDist\u2193\n\u00d7\n\u2713\n0.264\u00b1.011\n0.398\u00b1.016\n0.460\u00b1.018\n2.692\u00b1.034\n1.501\u00b1.095\n4.370\u00b1.066\n\u2713\n\u00d7\n0.068\u00b1.011\n0.145\u00b1.030\n0.188\u00b1.024\n3.707\u00b1.096\n1.106\u00b1.093\n4.148\u00b1.098\n\u2713\n\u2713\n0.352\u00b1.013\n0.550\u00b1.010\n0.648\u00b1.015\n1.808\u00b1.027\n0.786\u00b1.055\n4.392\u00b1.071\n5.3\nComparative Studies for Instruction-driven Character Animation\nComparison Methods.\nWe compare InsActor with two baseline approaches: 1) DReCon [4]. We\nadapted the responsive controller framework from DReCon [4]. We use the diffuser as a kinematic\ncontroller and train a target-state tracking policy. The baseline can also be viewed as a Decision\nDiffuser [1] with a long planning horizon, where a diffuser plans the future states and a tracking\npolicy solves the inverse dynamics. 2) PADL [15]: We adapt the language-conditioned control\npolicy in PADL [15], where language instructions are encoded by a pretrained cross-modal text\nencoder [29] and input to a control policy that directly predict actions. It is also a commonly used\nlearning paradigm in conditional imitation learning [36]. Since the two baselines have no publicly\navailable implementations, we reproduce them and train the policies with DiffMimic [33].\nSettings.\nWe utilize two different settings to assess InsActor\u2019s robustness. In the first setting, we\nevaluate the models in a clean, structured environment devoid of any perturbation. In the second\nsetting, we introduce perturbations by spawning a 2kg box to hit the character every 1 second, thereby\nevaluating whether the humanoid character can still adhere to human instructions even when the\nenvironment changes.\nResults.\nWe present the results in Table 1 and Table 2 and qualitative results in Figure 3. Compared\nto the dataset used in PADL that consists of 131 motion sequences and 256 language captions, our\nbenchmark dataset is two orders larger, where the language-conditioned single-step policy used\nin PADL has difficulty to scaling up. In particular, the inferior performance in language-motion\nmatching metrics suggests that a single-step policy fails to understand unseen instructions and model\nthe many-to-many instruction-motion relation. Compared to PADL, DReCon shows a better result\nin language-motion matching thanks to the high-level motion planning. However, unlike Motion\nMatching used in DReCon that produces high-quality kinematic motions, the diffuser generates\ninvalid states and infeasible state transitions, which fails DReCon\u2019s tracking policy and results\nin a low FID. In comparison, InsActor significantly outperforms the two baselines on all metrics.\nMoreover, the experiment reveals that environmental perturbations do not significantly impair the\nperformance of InsActor, showcasing InsActor\u2019s robustness.\n8\n\u201cA person crouches.\u201d +\n\u201cA person kicks.\u201d\n\u201cA person crouches.\u201d +\n\u201cA person runs.\u201d\nFigure 4: Qualitative results of InsActor with history conditioning. Generation is conditioned on\nthe second human instruction and history motion.\n5.4\nInstruction-driven Waypoint Heading\nWaypoint Heading.\nThanks to the flexibility of the diffusion model, InsActor can readily accom-\nplish the waypoint heading task, a common task in physics-based character animation [44, 15]. This\ntask necessitates the simulated character to move toward a target location while complying with\nhuman instructions. For instance, a human instruction might be, \u201cwalk like a zombie.\u201d In this case,\nthe character should navigate toward the target position while mimicking the movements of a zombie.\nGuided Diffusion.\nWe accomplish this using guided diffusion. Concretely, we adopt the inpainting\nstrategy in Diffuser [14]. Prior to denoising, we replace the Gaussian noise in the first and last 25%\nframes with the noisy states of the character standing at the starting position and target position\nrespectively.\nResults.\nWe conduct this experiment with the model trained on HumanML3D. We contrast InsActor\nwith DReCon and two InsActor variants: 1) InsActor without language condition, and 2) InsActor\nwithout targeting. Our experimental results demonstrate that InsActor can effectively accomplish\nthe waypoint heading task by leveraging guided diffusion. The model trained on HumanML3D\nis capable of moving toward the target position while following the given human instructions, as\nevidenced by a low FID score and high precision. Although adding the targeting position condition\nto the diffusion process slightly compromises the quality of the generated animation, the outcome is\nstill satisfactory. Moreover, the success rate of reaching the target position is high, underscoring the\neffectiveness of guided diffusion. Comparing InsActor with its two variants highlights the importance\nof both the language condition and the targeting in accomplishing the task. Comparing InsActor with\nDReCon shows the importance of skill mapping, particularly when more infeasible state transitions\nare introduced by the waypoint guiding. Without skill mapping, DReCon only has a 38.0% success\nrate, which drops drastically from the 90.7% success rate of InsActor.\nMultiple Waypoints.\nMultiple waypoints allow users to interactively instruct the character. We\nachieve this by autoregressively conditioning the diffusion process to the history motion, where a\nqualitative result is shown in Figure 4. Concretely, we inpaint the first 25% with the latest history\nstate sequences. We show qualitative results for multiple-waypoint following in Figure 1 and more in\nthe supplementary materials.\n5.5\nAblation Studies\nHierarchical Design.\nTo understand the importance of the hierarchical design in this task, we\nperformed an ablation study on its structure. We compared our approach to two baselines: 1) A policy\nwith only a high-level policy, wherein the diffusion model directly outputs the skills, analogous to the\nDiffuser approach [14]; 2) A low-level policy that directly predicts single-step skills. We show the\nresults in Table 4. By leveraging skills, the low-level policy improves from PADL but still grapples\nwith comprehending the instructions due to the absence of language understanding. Conversely,\nwithout the low-level policy, the skills generated directly by the diffusion model are of poor precision.\nAlthough the use of skills safeguards the motions to be natural and score high in FID, the error\naccumulation deviates the plan from the language description and results in a low R-precision. The\nexperimental results underscore the efficacy of the hierarchical design of InsActor.\n9\n10\n4\n10\n3\n10\n2\n10\n1\n10\n0\n0.3\n0.4\n0.5\n0.6\nR Precision (Top 3)\nR Precision (Top 3)\nFID\n0.5\n1.0\n1.5\n2.0\nFID\nFigure 5: Ablation study on the weight factor \u03bb.\nEvaluated on the KIT-ML dataset.\nWeight Factor.\nInsActor learns a compact la-\ntent space for skill discovery to overcome infea-\nsible plans generated by the diffusion model. We\nconduct an ablation study on the weight factor,\n\u03bb, which controls the compactness of the skill\nspace. Our findings suggest that a higher weight\nfactor results in a more compact latent space,\nhowever, it also curtails the instruction-motion\nalignment. Conversely, a lower weight factor\npermits a greater diversity in motion generation,\nbut it might also lead to less plausible and in-\nconsistent motions. Hence, it is vital to find a\nbalance between these two factors to optimize\nperformance for the specific task at hand.\n6\nConclusion\nIn conclusion, we have introduced InsActor, a principled framework for physics-based character\nanimation generation from human instructions. By utilizing a diffusion model to interpret language\ninstructions into motion plans and mapping them to latent skill vectors, InsActor can generate flexible\nphysics-based animations with various and mixed conditions including waypoints. We hope InsActor\nwould serve as an important baseline for future development of instruction-driven physics-based\nanimation. While InsActor is capable of generating such animations, there are crucial but exciting\nchallenges ahead. One limitation is the computational complexity of the diffusion model, which\nmay pose challenges for scaling up the approach to more complex environments and larger datasets.\nAdditionally, the current version of InsActor assumes access to expert demonstrations for training,\nwhich may limit its applicability in real-world scenarios where such data may not be readily available.\nFurthermore, while InsActor is capable of generating physically-reliable and visually-plausible\nanimations, there is still room for improvement in terms of the quality and diversity of generated\nanimations. There are mainly two aspects for future development, improving the quality of the\ndifferentiable physics for more realistic simulations and enhancing the expressiveness and diversity\nof the diffusion model to generate more complex and creative animations. Besides them, extending\nInsActor to accommodate different human body shapes and morphologies is also an interesting\ndirection.\nFrom a societal perspective, the application of InsActor may lead to ethical concerns related to how it\nmight be used. For instance, InsActor could be exploited to create deceptive or harmful content. This\nunderscores the importance of using InsActor responsibly.\nAcknowledgment\nThis research is supported by the National Research Foundation, Singapore under its AI Singa-\npore Programme (AISG Award No: AISG2-PhD-2021-08-018), NTU NAP, MOE AcRF Tier 2\n(T2EP20221-0012), and under the RIE2020 Industry Alignment Fund - Industry Collaboration\nProjects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry\npartner(s).\nReferences\n[1] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit\nAgrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint\narXiv:2211.15657, 2022.\n[2] Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, and Stephen\nGould. A stochastic conditioning scheme for diverse human motion prediction. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5223\u20135232,\n2020.\n[3] Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen\nGall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking.\n10\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n5167\u20135176, 2018.\n[4] Kevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. Drecon: Data-driven\nresponsive control of physics-based characters. ACM Trans. Graph., 38(6), November 2019.\n[5] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and\nShuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings\nof Robotics: Science and Systems (RSS), 2023.\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in Neural Information Processing Systems, 34, 2021.\n[7] C Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier\nBachem. Brax\u2013a differentiable physics engine for large scale rigid body simulation. arXiv\npreprint arXiv:2106.13281, 2021.\n[8] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong,\nand Li Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedings\nof the 28th ACM International Conference on Multimedia, pages 2021\u20132029, 2020.\n[9] F\u00e9lix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion\nin-betweening. ACM Transactions on Graphics (TOG), 39(4):60\u20131, 2020.\n[10] Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow. Moglow: Probabilistic and\ncontrollable motion synthesis using normalising flows. ACM Transactions on Graphics (TOG),\n39(6):1\u201314, 2020.\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020.\n[12] Leslie Ikemoto, Okan Arikan, and David Forsyth. Generalizing motion edits with gaussian\nprocesses. ACM Transactions on Graphics (TOG), 28(1):1\u201312, 2009.\n[13] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large\nscale datasets and predictive methods for 3d human sensing in natural environments. IEEE\ntransactions on pattern analysis and machine intelligence, 36(7):1325\u20131339, 2013.\n[14] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for\nflexible behavior synthesis. In International Conference on Machine Learning, 2022.\n[15] Jordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng. Padl: Language-directed\nphysics-based character control. Association for Computing Machinery, 2022.\n[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[17] Libin Liu, Michiel van de Panne, and KangKang Yin. Guided learning of control graphs for\nphysics-based characters. ACM Transactions on Graphics, 35(3), 2016.\n[18] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black.\nSmpl: A skinned multi-person linear model. ACM transactions on graphics (TOG), 34(6):1\u201316,\n2015.\n[19] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured\ndata. Robotics: Science and Systems, 2021.\n[20] Oier Mees and Wolfram Burgard. Composing pick-and-place tasks by grounding language. In\nInternational Symposium on Experimental Robotics, 2021.\n[21] Oier Mees, Alp Emek, Johan Vertens, and Wolfram Burgard. Learning object placements\nfor relational instructions by hallucinating scene representations. In 2020 IEEE International\nConference on Robotics and Automation (ICRA), pages 94\u2013100, 2020.\n[22] Jianyuan Min and Jinxiang Chai. Motion graphs++ a compact generative model for semantic\nmotion analysis and synthesis. ACM Transactions on Graphics (TOG), 31(6):1\u201312, 2012.\n[23] Dirk Ormoneit, Michael J Black, Trevor Hastie, and Hedvig Kjellstr\u00f6m. Representing cyclic\nhuman motion using functional analysis. Image and Vision Computing, 23(14):1264\u20131276,\n2005.\n11\n[24] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu,\nSergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin.\nImitating human behaviour with diffusion models. In The Eleventh International Conference on\nLearning Representations, 2023.\n[25] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-\nguided deep reinforcement learning of physics-based character skills. ACM Trans. Graph.,\n37(4):143:1\u2013143:14, July 2018.\n[26] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example-\nguided deep reinforcement learning of physics-based character skills. ACM Transactions On\nGraphics (TOG), 2018.\n[27] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset.\nBig data, 4(4):236\u2013252, 2016.\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. CoRR,\nabs/2103.00020, 2021.\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[31] Davis Rempe, Leonidas J. Guibas, Aaron Hertzmann, Bryan Russell, Ruben Villegas, and Jimei\nYang. Contact and human dynamics from monocular video. In Proceedings of the European\nConference on Computer Vision (ECCV), 2020.\n[32] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler,\nand Or Litany. Trace and pace: Controllable pedestrian animation via guided trajectory diffusion.\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[33] Jiawei Ren, Cunjun Yu, Siwei Chen, Xiao Ma, Liang Pan, and Ziwei Liu. Diffmimic: Efficient\nmotion mimicking with differentiable physics. ICLR, 2022.\n[34] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, and Christian Theobalt. Physcap: Physically\nplausible monocular 3d motion capture in real time. ACM Trans. Graph., 39(6), nov 2020.\n[35] Mohit Shridhar and David Hsu. Interactive visual grounding of referring expressions for\nhuman-robot interaction. In Proceedings of Robotics: Science and Systems, 2018.\n[36] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\nmanipulation. In Proceedings of the 5th Conference on Robot Learning (CoRL), 2021.\n[37] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[38] Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben\nAmor. Language-conditioned imitation learning for robot manipulation tasks. In Proceedings\nof the 34th International Conference on Neural Information Processing Systems, 2020.\n[39] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano.\nHuman motion diffusion model. In The Eleventh International Conference on Learning Repre-\nsentations, 2023.\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[41] Nolan Wagener, Andrey Kolobov, Felipe Vieira Frujeri, Ricky Loynd, Ching-An Cheng, and\nMatthew Hausknecht. Mocapact: A multi-task dataset for simulated humanoid control. arXiv\npreprint arXiv:2208.07363, 2022.\n[42] Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan Zhou, Junsong Yuan, and Changyou\nChen. Learning diverse stochastic human-action generators by learning smooth latent transitions.\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 12281\u201312288,\n2020.\n12\n[43] Xinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan Sunkavalli, Eli Shechtman, Sunil Hadap,\nErsin Yumer, and Honglak Lee. Mt-vae: Learning motion transformations to generate mul-\ntimodal human dynamics. In Proceedings of the European conference on computer vision\n(ECCV), pages 265\u2013281, 2018.\n[44] Heyuan Yao, Zhenhua Song, Baoquan Chen, and Libin Liu. Controlvae: Model-based learning\nof generative controllers for physics-based characters. 41(6), 2022.\n[45] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided\nhuman motion diffusion model. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2023.\n[46] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason Saragih. Simpoe: Simulated\ncharacter control for 3d human pose estimation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2021.\n[47] Hanbo Zhang, Yunfan Lu, Cunjun Yu, David Hsu, Xuguang Lan, and Nanning Zheng. Invigorate:\nInteractive visual grounding and grasping in clutter. Proceedings of Robotics: Science and\nSystems, abs/2108.11092, 2021.\n[48] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and\nZiwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv\npreprint arXiv:2208.15001, 2022.\n13\nA\nSimulation Environment\nAs mentioned in the main text, our experiments are mainly executed with Brax [7] for its differentia-\nbility. Our character model, guided by DeepMimic [26], is a humanoid with 13 links and 34 degrees\nof freedom, weighing 45 kg and measuring 1.62m in height. Contact is applied to all links with the\nfloor. Facilitated by GPU-accelerated environment simulations, the physics simulator runs at 480\nFPS. To optimize gradient propagation, the character\u2019s joint limits are eased. System configurations,\nsuch as friction coefficients, align with DeepMimic\u2019s parameters.\nB\nDiffusion Policy\nFor the diffusion policy, we build up an 8-layer transformer as the motion decoder. As for the text\nencoder, we first directly use the text encoder in the CLIP ViT-B/32 [28], and then add four more\ntransformer encoder layers. The latent dimension of the text encoder and the motion decoder are 256\nand 512, respectively. As for the diffusion model, the number of diffusion steps T is 1000, and the\nvariances \u03b2t are linearly increased from 0.0001 to 0.02. We opt for Adam [16] as the optimizer to\ntrain the model with a 0.0002 learning rate. We use 4 NVIDIA A100 for the training, and there are\n256 samples on each GPU, so the total batch size is 1024. The total number of iterations is 40K for\nKIT-ML and 100K for HumanML3D.\nC\nSkill Discovery\nFor the skill discovery, both the encoder and decoder are a 3-layered Multi-layer Perceptron (MLP),\neach layer with 512 nodes. The dimension of the latent vector is 64. We choose the weight factor \u03bb\nas 0.01 except for the ablation study of \u03bb. We opt for Adam as the optimizer to train the model with a\n0.0003 learning rate. We use one NVIDIA A100 for the training, and the batch size is 300. The total\nnumber of iterations is 10K.\nD\nBaselines\nSince there are no publicly available implementations of the two compared methods DReCon [4] and\nPADL [15], in this section, we elaborate on our implementation of the two compared approaches. In\naddition, we will also detail the implementation of the high-level policy and the low-level policy used\nin the hierarchical design ablation.\nD.1\nAdapted DReCon\nFor the kinematic controller, we directly use the pretrained diffusion policy as a replacement for\nthe Motion Matching. For the target state-conditioned policy, we use a three-layer MLP with 512\nhidden units as the neural network. The target state is input to the networks together with the current\nobservation. The target state is normalized to be relative to the current state in the global frame. We\nuse the training method in DiffMimic [33] and the average evaluation pose error converges to below\n0.02.\nD.2\nAdapted PADL\nWe use a three-layer MLP with 512 hidden units as the neural network. A clip embedding with 512\ndimensions from ViT-B/32 is input to the neural net together with the current state observation. The\ntraining follows the aforementioned procedures.\nD.3\nHigh-level Policy\nWe first train the skill discovery module as described above. Then, we rollout the skill discovery\nmodule on all training motion sequences to collect skill trajectories. Following MocapAct [41],\nwe repeat the rollout 16 times with different random seeds. Then we train a diffuser on the joint\nrepresentation of state and skill, as described in Diffuser [14]. Finally, we let the diffuser generate\nboth the initial state and the following skills given a human instruction.\n14\n\u201ca human walks and turns on the spot\u201d\n\u201ca person stomps his left foot\u201d\nDReCon\nPADL\nInsActor\nFigure 6: Quantatative comparison with corresponding instructions. Each row represents one method\nand column correspond to the instruction.\na)\nb)\nFigure 7: Robustness test results. a) InsActor executes without perturbation. b) InsActor executes\nwith perturbation caused by 2kg box hitting the character.\nD.4\nLow-level Policy\nWe use the same encoder-decoder architecture as described in the skill discovery module. Additionally,\nthe language condition is encoded by a 512-dimensional CLIP ViT-B/32 embedding and input into\nthe encoder in replace of the target state.\nE\nTraining and Inference time\nTraining for the diffusion model on HumanML-3D takes approximately 16 hours on 4 NVIDIA\nA100 GPUs. The training for the skill discovery module on HumanML-3D takes approximately 40\nhours on a single NVIDIA A100 GPU. The inference time for the diffusion policy using DDIM [37]\ngenerally takes less than a second to generate a 180-frame plan, and the skill mapping module takes\nless than 3 seconds to execute the plan after Just In Time compilation. We show a demo interface that\nruns on a single NVIDIA A100 GPU in the supplementary video for qualitative evaluation of the\ninference speed.\nF\nQualitative Results\nIn this section, we present a detailed qualitative evaluation to further demonstrate the effectiveness of\nInsActor.\nQualitative Comparison with Baselines.\nAs shown in Table 3, we conduct a qualitative comparison\nof InsActor with two baselines introduced in the main text: DReCon [4] and PADL [15]. The\ncomparison is derived from the results of two different instructions, \u201ca human walks and turns on\nthe spot\u201d and \u201cperson stomps his left foot\u201d. In contrast to DReCon, which fails to comprehend the\nhigh-level instructions, and PADL, which struggles to generate reliable control, InsActor exhibits the\nability to successfully execute the stipulated commands.\nQualitative Assessment of Robustness.\nIn an effort to highlight the robustness of InsActor, we\nshowcase its performance under both perturbed and non-perturbed conditions in Table 7. This\ninvolves introducing a 2kg box to strike the character at random positions. Impressively, InsActor\nmaintains the ability to generate plausible animations under such perturbations, underscoring its\nresilience and adaptability in a variety of unpredictable scenarios.\n15\na) Planned Motion\nb) DReCon\nc) InsActor\nFigure 8: Waypoint results InsActor with corresponding waypoints. When compared with DReCon,\nInsActor successfully reaches the waypoint without falling and effectively follows the planned motion.\nTable 5: Quantitative results on the standard text-to-motion benchmark HumanML3D.\nMethods\nR Precision\u2191\nMultimodal\nFID\u2193\nDiversity\u2191\nTop 1\nTop 2\nTop 3\nDist\u2193\nMDM\n-\n-\n0.611\n5.566\n0.544\n9.559\nMotionDiffuse\n0.491\n0.681\n0.782\n3.113\n0.630\n9.410\nQualitative Examination of Waypoint Heading.\nIn addition to the aforementioned analyses,\nwe delve into a qualitative examination of waypoint heading. Compared with DReCon, InsActor\nsuccessfully reaches the waypoint without falling as planned, demonstrating the flexibility and\nrobustness of InsActor.\nG\nVideo\nWe show more qualitative results in the attached video. We list key timestamps as follows:\n\u2022 Motion Plans - 0:45\n\u2022 Random Skill Sampling - 1:04\n\u2022 Comparative Study on Instruction-driven Generation - 1:11\n\u2022 Robustness to Perturbations - 1:45\n\u2022 Instruction-driven Waypoint Heading - 1:53\n\u2022 Multiple-waypoint Following - 2:24\n\u2022 Ablation on Weight Factor - 2:47\n\u2022 Ablation on Hierarchical Design - 3:00\n\u2022 Ablation on Instruction-driven Waypoint Heading - 3:18\n\u2022 Demo Interface - 3:35\n16\nH\nComparison between our diffusion policy and MDM\n0\n5\n10\n15\n20\n25\nWallclock Time (hour)\n0.1\n0.2\n0.3\n0.4\n0.5\nPose Error (m)\nHumanML3D\nKIT-ML\nFigure 9: Wallclock training time versus pose\nerror for the InsActor skill mapping module\ntraining. Blue dotted line denotes 0.05m pose\nerror, an average DiffMmimic [28] tracking error.\nWe implemented our diffusion policy using the\nopen-source code of MotionDiffuse [48]. We\nmodified the feature dimensions to fit our state\ntrajectories and changed the noise prediction\nto x0 prediction for training efficiency as de-\nscribed in the main paper. Given that it is not\npossible to directly compare our motion planner\nto a pretrained MDM model as they have dif-\nferent generation space, we show a comparison\nbetween MDM and our codebase MotionDiffuse\nin Table 5, where MotionDiffuse achieves com-\nparable quantitative results than MDM on a stan-\ndard text-to-motion benchmark. Qualitatively,\nwe do notice that our generated plans have more\njittering than motions generated by either MDM\nor MotionDiffuse. This could be caused by the\nfact that MotionDiffuse uses temporal smooth-\ning in the visualization but we did not smooth\nour plans in our visualization. We show in the\nfollowing experiment that plan smoothing has a\nminimal effect on the tracking result.\nTable 6: Quantitative results on the HumanML3D test set. Real motions: the test dataset. Planner:\nOur high-level planner. DReCon (Real motions): replace the generated plans with the test dataset.\nDReCon (Real motions): replace the generated plans with plans smoothed following [40].\nMethods\nR Precision\u2191\nMultimodal\nFID\u2193\nDiversity\u2191\nTop 1\nTop 2\nTop 3\nDist\u2193\n(a) Real motions\n0.428\n0.603\n0.694\n1.556\n0.000\n4.586\n(b) Planner\n0.434\n0.625\n0.723\n1.507\n0.314\n4.538\n(c) InsActor\n0.331\n0.497\n0.598\n1.971\n0.566\n4.165\n(d) DReCon (Real motions)\n0.343\n0.494\n0.578\n2.009\n0.086\n4.441\n(e) DReCon (Smooth)\n0.268\n0.391\n0.463\n2.594\n1.271\n4.092\n(f) DReCon\n0.265\n0.391\n0.470\n2.570\n1.244\n4.070\nI\nMore ablations on planning and tracking\nWe show more ablation results in Table 6. Note that results in this table are not comparable with\nTable 5 as they are in different generation spaces. 1) Compare (a) and (b), we observe that our\ndiffusion policy achieves a strong generation performance. Note that our R Precision and Multimodal\nDist are slightly higher than real motions since contrastive models can only give a rough estimation\nof the text-motion alignment. 2) Compare (b) and (f), we observe that directly tracking the plan leads\nto a drastic performance drop, where InsActor (c) greatly alleviates the issue. 3) Compare (d) and\n(f), we observe that our motion tracker is significantly better at tracking real motions than tracking\ngenerated plans, which verifies the performance of the DReCon motion tracker. 4) Compare (e) and\n(f), we observe that although smoothing improves the visual quality of the plans, it has a minimal\neffect on the final result.\nJ\nQuantitative performance of low-level control\nFor the InsActor skill mapping module, we plot is evaluation pose error versus wallclock training\ntime in Figure 9. Single-clip motion tracking pose error in DiffMimic [33] ranges from 0.017m to\n0.097m with an average value around 0.05m. Our low-level controller successfully achieves similar\ncontrol quality on large-scale motion databases.\n17\n"
  },
  {
    "title": "Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis",
    "link": "https://arxiv.org/pdf/2312.16812.pdf",
    "upvote": "9",
    "text": "Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis\nZhan Li1,2\u2217\nZhang Chen1\u2020\nZhong Li1\u2020\nYi Xu1\n1 OPPO US Research Center\n2 Portland State University\nlizhan@pdx.edu\nzhang.chen@oppo.com\nzhong.li@oppo.com\nyi.xu@oppo.com\nhttps://oppo-us-research.github.io/SpacetimeGaussians-website/\n(b) High Quality Result\n(a) Real-Time 8K Demo at 66 FPS\nHyperReel\nOurs\nSize (MB)  Method\nK-Planes\n311\nOurs\n200\nMixVoxels-X\n500\nMixVoxels-L\n500\nNeRFPlayer\n5130\nStreamRF\n5330\nHyperReel\n360\nOurs\nPSNR\nFPS\n0\n40\n80\n120\n140\n28.5\n29.0\n29.5\n30.0\n30.5\n31.0\n31.5\n32.0\n(c) Comparison on Quality, Speed and Size\nFigure 1. Our dynamic scene representation achieves photorealistic quality, real-time high-resolution rendering and compact model\nsize. (a) Our lite-version model can render 8K 6-DoF video at 66 FPS on an Nvidia RTX 4090 GPU. (b) Example novel view rendering\nof a challenging scene. (c) Quantitative comparisons of rendering quality, speed and model size with prior arts on the Neural 3D Video\nDataset.\nAbstract\nNovel view synthesis of dynamic scenes has been an in-\ntriguing yet challenging problem. Despite recent advance-\nments, simultaneously achieving high-resolution photore-\nalistic results, real-time rendering, and compact storage\nremains a formidable task. To address these challenges,\nwe propose Spacetime Gaussian Feature Splatting as a\nnovel dynamic scene representation, composed of three piv-\notal components. First, we formulate expressive Spacetime\nGaussians by enhancing 3D Gaussians with temporal opac-\nity and parametric motion/rotation. This enables Space-\ntime Gaussians to capture static, dynamic, as well as tran-\nsient content within a scene. Second, we introduce splat-\nted feature rendering, which replaces spherical harmonics\nwith neural features. These features facilitate the modeling\nof view- and time-dependent appearance while maintaining\nsmall size. Third, we leverage the guidance of training er-\nror and coarse depth to sample new Gaussians in areas that\nare challenging to converge with existing pipelines. Exper-\niments on several established real-world datasets demon-\nstrate that our method achieves state-of-the-art rendering\nquality and speed, while retaining compact storage. At 8K\nresolution, our lite-version model can render at 60 FPS on\nan Nvidia RTX 4090 GPU.\n1. Introduction\nPhotorealistic modeling of real-world dynamic scenes has\nbeen persistently pursued in computer vision and graph-\nics.\nIt allows users to freely explore dynamic scenes at\nnovel viewpoints and timestamps, thus providing strong im-\nmersive experience, and can vastly benefit applications in\nVR/AR, broadcasting, education, etc.\nRecent advances in novel view synthesis, especially\nNeural Radiance Fields (NeRF) [61], have greatly improved\nthe convenience and fidelity of static scene modeling from\ncasual multi-view inputs in non-lab environments. Since\nthen, large quantities of work [6\u20138, 15, 17, 25, 39, 62, 77]\nhave emerged aiming to enhance rendering quality and\nspeed. Particularly, [17, 39] propose to use anisotropic ra-\ndial basis functions as 3D representations, which are highly\nadaptive to scene structures and boost the modeling of de-\n\u2020 Corresponding authors.\n\u2217 Work done while Zhan was an intern at OPPO US Research Center.\n1\narXiv:2312.16812v1  [cs.CV]  28 Dec 2023\ntails. 3D Gaussian Splatting (3DGS) [39] further presents\nan efficient rasterization-based scheme for differentiable\nvolume rendering. Instead of shooting rays from camera\nto the scene and sampling points along each ray, 3DGS ras-\nterizes 3D Gaussians onto image plane via splatting, which\nbrings about notable rendering speedup.\nDespite the success on static scenes, directly apply-\ning the above methods per-frame to dynamic scenes is\nchallenging, due to the overhead in model size and train-\ning time.\nState-of-the-art dynamic view synthesis meth-\nods [4, 12, 26, 45, 75, 81] adopt a holistic approach\nwhere multiple frames are represented in a single model.\nNeRFPlayer [75] and HyperReel [4] combine static spa-\ntial representations [15, 62] with temporal feature shar-\ning/interpolation to improve model compactness. This strat-\negy exploits the characteristic that adjacent frames in natu-\nral videos usually exhibit high similarity. In a similar vein,\nMixVoxels [81] uses time-variant latents and bridges them\nwith spatial features by inner product. K-Planes [26] and\nHexPlane [12] factorize the 4D spacetime domain into mul-\ntiple 2D planes for compact representation. One limitation\nof these methods is that their grid-like representations can-\nnot fully adapt to the dynamics of scene structures, hinder-\ning the modeling of delicate details. Meanwhile, they strug-\ngle to produce real-time high-resolution rendering without\nsacrificing quality.\nIn this work, we present a novel representation for\ndynamic view synthesis.\nOur approach simultaneously\nachieves photorealistic quality, real-time high-resolution\nrendering and compact model size (see Fig. 1 for example\nresults and comparisons with state-of-the-arts). At the core\nof our approach is Spacetime Gaussian (STG), which ex-\ntends 3D Gaussian to 4D spacetime domain. We propose\nto equip 3D Gaussian with time-dependent opacity along\nwith polynomially parameterized motion and rotation. As\na result, STGs are capable of faithfully modeling static, dy-\nnamic as well as transient (i.e., emerging or vanishing) con-\ntent in a scene.\nTo enhance model compactness and account for time-\nvarying appearance, we propose splatted feature render-\ning. Specifically, for each Spacetime Gaussian, instead of\nstoring spherical harmonic coefficients, we store features\nthat encode base color, view-related information and time-\nrelated information. These features are rasterized to image\nspace via differentiable splatting, and then go through a tiny\nmulti-layer perceptrons (MLP) network to produce the final\ncolor. While smaller in size than spherical harmonics, these\nfeatures exhibit strong expressiveness.\nAdditionally, we introduce guided sampling of Gaus-\nsians to improve rendering quality of complex scenes. We\nobserve that distant areas which are sparsely covered by\nGaussians at initialization tend to have blurry rendering re-\nsults. To tackle this problem, we propose to sample new\nGaussians in the 4D scene with the guidance of training er-\nror and coarse depth.\nIn summary, the contributions of our work are the fol-\nlowing:\n\u2022 We present a novel representation based on Spacetime\nGaussian for high-fidelity and efficient dynamic view\nsynthesis.\n\u2022 We propose splatted feature rendering, which enhances\nmodel compactness and facilitates the modeling of time-\nvarying appearance.\n\u2022 We introduce a guided sampling approach for Gaussians\nto improve rendering quality at distant sparsely covered\nareas.\n\u2022 Extensive experiments on various real world datasets\ndemonstrate that our method achieves state-of-the-art ren-\ndering quality and speed while keeping small model size.\nOur lite-version model enables 8K rendering at 60 FPS.\n2. Related Work\nNovel View Synthesis.\nEarly approaches leverage image-\nbased rendering techniques with proxy geometry/depth to\nsample novel views from source images [11, 13, 19, 29, 32,\n41, 43, 95]. Chaurasia et al. [14] estimate a depth map to\nblend pixels from source views and employ superpixels to\ncompensate for missing depth data.\nHedman et al. [30]\nutilizes RGBD sensors to improve rendering quality and\nspeed. Penner and Zhang [67] leverage volumetric voxels\nfor continuity in synthesized views and robustness to depth\nuncertainty. Hedman et al. [31] learn the blending scheme\nwith neural networks. Flynn et al. [24] combines multi-\nplane images with learned gradient descent.\nNeural Scene Representations.\nIn recent years, neural\nscene representations have achieved great progress in novel\nview synthesis. These methods allocate neural features to\nstructures such as volume [57, 73], texture [16, 78], and\npoint cloud [1]. The seminal work of NeRF [61] proposes\nto leverage differentiable volume rendering. It does not re-\nquire proxy geometry and instead uses MLPs to implicitly\nencode density and radiance in 3D space. Later on, nu-\nmerous works emerge to boost the quality and efficiency\nof differentiable volume rendering. One group of methods\nfocuses on improving the sampling strategy to reduce the\nnumber of point queries [4, 63, 68] or applies light field-\nbased formulation [3, 23, 49, 50, 74, 76, 82]. Another group\ntrades space for speed by incorporating explicit and local-\nized neural representations [8, 15, 25, 34, 55, 62, 70, 77, 88,\n93]. Among them, to improve model compactness, Instant-\nNGP [62] uses hash grid while TensoRF [15] utilizes tensor\ndecomposition.\nRecently, 3D Gaussian Splatting (3DGS) [39] proposes\nto use anisotropic 3D Gaussians as scene representation\nand presents an efficient differentiable rasterizer to splat\n2\nthese Gaussians to the image plane. Their method enables\nfast high-resolution rendering, while preserving great ren-\ndering quality. Similar to 3DGS, NeuRBF [17] leverages\nanisotropic radial basis functions for neural representation\nand achieves high-fidelity rendering. However, the above\nmethods focus on static scene representation.\nDynamic Novel View Synthesis.\nA widely adopted set-\nting for dynamic free-viewpoint rendering is using multi-\nview videos as input.\nClassic methods in this area in-\nclude [18, 37, 38, 46, 47, 90, 96]. More recently, Broxton et\nal. [10] uses multi-sphere image as bootstrap and then con-\nvert it to layered meshes. Bansal et al. [5] separate static and\ndynamic contents and manipulate video with deep network\nin screen space. Bemana et al. [9] learn a neural network to\nimplicitly map view, time or light coordinates to 2D images.\nAttal et al. [2] use multi-sphere representations to han-\ndle the depth and occlusions in 360-degree videos. Lin et\nal. [53] propose 3D mask volume to addresses the temporal\ninconsistency of disocclusions. Neural Volumes et al. [57]\nuse an encoder-decoder network to encode images into a 3D\nvolume and decode it with volume rendering. Lombardi et\nal. [58] enhance Neural Volumes with mixture of volumet-\nric primitives. Extending static NeRF-related representa-\ntions to dynamic scenes are also being actively explored [4,\n12, 26, 36, 44, 45, 52, 66, 72, 75, 81, 83, 84]. DyNeRF [45]\ncombines NeRF with time-conditioned latent codes to com-\npactly represent dynamic scenes. StreamRF [44] acceler-\nates the training of dynamic scenes by modeling the differ-\nences of consecutive frames. NeRFPlayer [75] decomposes\nscene into static, new and deforming fields and proposes\nstreaming of feature channels. MixVoxels [81] represents\nscene with a mixture of static and dynamic voxels to accel-\nerate rendering. HyperReel [4] utilizes sampling prediction\nnetwork to reduce sampling points and leverages keyframe-\nbased representation.\nK-Planes [26], HexPlane [12] and\nTensor4D [72] factorize 4D spacetime domain into 2D fea-\nture planes for compact model size.\nAnother line of research tackles dynamic view synthesis\nfrom monocular videos [20, 21, 27, 28, 48, 51, 56, 64, 65,\n69, 79, 80, 86]. Under this setting, a single camera moves\naround in the dynamic scene, providing only one observed\nviewpoint at each timestep. To address the sparsity of su-\npervision, priors on motion, scene flow or depth are usually\nintroduced. In this work, we focus on the dynamic represen-\ntation itself and only consider the setting with multi-view\ninput videos.\nRecently, there are several work on this topic that are\nconcurrent to ours [35, 42, 54, 59, 85, 87, 89, 91, 92].\n4K4D [89] combines 4D point clouds with K-Planes [26]\nand discrete image-based rendering, and uses differentiable\ndepth peeling to train the model. Luiten et al. [59] models\n4D scene with a set of moving 3D Gaussians, whose posi-\ntions and rotations are discretely defined at each step. Their\nmethod demonstrates appealing results for 3D tracking, but\nits rendering quality is less favorable due to flickering ar-\ntifacts. Yang et al. [92] leverages 4D Gaussians and 4D\nspherindrical harmonics for dynamic modelling. 4D Gaus-\nsians essentially represent motion with linear model. Com-\nparatively, our polynomial motion model is more expres-\nsive, resulting in higher rendering quality. Our method also\nhas higher rendering speed than their work. Yang et al. [91]\nand Wu et al. [85] prioritize on monocular dynamic view\nsynthesis and employ deformation fields to deform a set of\ncanonical 3D Gaussians. For multi-view videos setting, the\nperformance of [91] is not extensively evaluated while [85]\ndepicts inferior rendering quality and speed than ours.\n3. Preliminary: 3D Gaussian Splatting\nGiven images at multiple viewpoints with known camera\nposes, 3D Gaussian Splatting [39] (3DGS) optimizes a set\nof anisotropic 3D Gaussians via differentiable rasterization\nto represent a static 3D scene.\nOwing to their efficient\nrasterization, the optimized model can render high-fidelity\nnovel views in real-time.\n3DGS [39] associates a 3D Gaussian i with a position \u00b5i,\ncovariance matrix \u03a3i, opacity \u03c3i and spherical harmonics\n(SH) coefficients hi. The final opacity of a 3D Gaussian at\nany spatial point x is\n\u03b1i = \u03c3i exp\n\u0012\n\u22121\n2(x \u2212 \u00b5i)T \u03a3\u22121\ni (x \u2212 \u00b5i)\n\u0013\n.\n(1)\n\u03a3i is positive semi-definite and can be decomposed into\nscaling matrix Si and rotation matrix Ri:\n\u03a3i = RiSiST\ni RT\ni ,\n(2)\nwhere Si is a diagonal matrix and is parameterized by a 3D\nvector si, and Ri is parameterized by a quaternion q.\nTo render an image, 3D Gaussians are first projected to\n2D image space via an approximation of the perspective\ntransformation [97]. Specifically, the projection of a 3D\nGaussian is approximated as a 2D Gaussian with center \u00b52D\ni\nand covariance \u03a32D\ni\n. Let W, K be the viewing transforma-\ntion and projection matrix, \u00b52D\ni\nand \u03a32D\ni\nare computed as\n\u00b52D\ni\n= (K((W\u00b5i)/(W\u00b5i)z))1:2,\n(3)\n\u03a32D\ni\n= (JW\u03a3iW T JT )1:2,1:2,\n(4)\nwhere J is the Jacobian of the projective transformation.\nAfter sorting the Gaussians in depth order, the color at a\npixel is obtained by volume rendering:\nI =\nX\ni\u2208N\nci\u03b12D\ni\ni\u22121\nY\nj=1\n(1 \u2212 \u03b12D\nj ),\n(5)\n3\n(a) Spacetime Gaussians\n(b) Feature Splatting and Rendering\nFeatures\nTemporal\nOpacity\nPolynomial \nTrajectory\nPolynomial \nRotation\nTime\nt\nt\n\u03c3i\n\u03bci\nMLP\nFigure 2. Overview of Spacetime Gaussians and splatted feature rendering. (a) Our method leverages a set of Spacetime Gaus-\nsians (STG) to represent the dynamic scenes. On top of 3D Gaussian, each STG is further equipped with temporal opacity, polynomial\nmotion/rotation and time-dependent features. (b) We visualize the splatted features as maps, which are converted to color image via MLP.\nwhere \u03b12D\ni\nis a 2D version of Eq. (1), with \u00b5i, \u03a3i, x re-\nplaced by \u00b52D\ni\n, \u03a32D\ni\n, x2D (pixel coordinate). ci is the RGB\ncolor after evaluating SH with view direction and coeffi-\ncients hi.\n4. Method\nWe propose a novel representation based on Spacetime\nGaussians for modeling dynamic 3D scenes. Our approach\ntakes multi-view videos as input and creates 6-DoF video\nthat allows rendering at novel views. We first describe the\nformulation of our Spacetime Gaussian (STG) in Sec. 4.1.\nThen in Sec. 4.2, we present feature-based splatting for\ntime-varying rendering. Sec. 4.3 details our optimization\nprocess and Sec. 4.4 introduces guided sampling of Gaus-\nsians.\n4.1. Spacetime Gaussians\nTo represent 4D dynamics, we propose Spacetime Gaus-\nsians (STG) that combine 3D Gaussians with temporal com-\nponents to model emerging/vanishing content as well as\nmotion/deformation, as shown in Fig. 2 (a). Specifically, we\nintroduce temporal radial basis function to encode tempo-\nral opacity, which can effectively model scene content that\nemerges or vanishes within the duration of video. Mean-\nwhile, we utilize time-conditioned parametric functions for\nthe position and rotation of 3D Gaussians to model the mo-\ntion and deformation in the scene. For a spacetime point\n(x, t), the opacity of an STG is\n\u03b1i(t) = \u03c3i(t) exp\n\u0012\n\u22121\n2(x \u2212 \u00b5i(t))T \u03a3i(t)\u22121(x \u2212 \u00b5i(t)\n\u0013\n,\n(6)\nwhere \u03c3i(t) is temporal opacity, \u00b5i(t), \u03a3i(t) are time-\ndependent position and covariance, and i stands for the ith\nSTG. We detail each of the components below.\nTemporal Radial Basis Function.\nWe use a temporal ra-\ndial basis function to represent the temporal opacity of an\nSTG at any time t. Inspired by [17, 39] that use radial basis\nfunctions for approximating spatial signals, we utilize 1D\nGaussian for the temporal opacity Pi(t):\n\u03c3i(t) = \u03c3s\ni exp\n\u0000\u2212s\u03c4\ni |t \u2212 \u00b5\u03c4\ni |2\u0001\n,\n(7)\nwhere \u00b5\u03c4\ni is temporal center, s\u03c4\ni is temporal scaling factor,\nand \u03c3s\ni is time-independent spatial opacity. \u00b5\u03c4\ni represents\nthe timestamp for the STG to be most visible while s\u03c4\ni de-\ntermines its effective duration (i.e., the time duration where\nits temporal opacity is high). We include \u03c3s\ni to allow spatial\nopacity variation across STGs.\nPolynomial Motion Trajectory.\nFor each STG, we em-\nploy a time-conditioned function to model its motion. Mo-\ntivated by [22, 33], we choose polynomial function:\n\u00b5i(t) =\nnp\nX\nk=0\nbi,k(t \u2212 \u00b5\u03c4\ni )k,\n(8)\nwhere \u00b5i(t) denotes the spatial position of an STG at time\nt. {bi,k}np\nk=0, bi,k \u2208 R are the corresponding polynomial\ncoefficients and are optimized during training. Combining\nEq. (7) and Eq. (8), complex and long motion can be repre-\nsented by multiple short segments with simpler motion. In\nour implementation, we use np = 3 as we find it a good\nbalance between representation capacity and model size.\nPolynomial Rotation.\nFollowing [39], we use real-valued\nquaternion to parameterize the rotation matrix Ri in Eq. (2).\nSimilar to motion trajectory, we adopt a polynomial func-\n4\nz\nx\nErroneous Area\nGT Training Image\nSample Gaussians \nalong Rays\nResults with Our Guided Sampling\nExisting Gaussians\nSampled Gaussians\nResults without Guided Sampling\nInitial Point Clouds\nErroneous Area\nTraining View\nTest View\nFigure 3. Illustration of our guided sampling strategy for Gaussians. Our strategy samples new Gaussians along rays by leveraging the\nguidance of training error and coarse depth.\ntion to represent quaternion:\nqi(t) =\nnq\nX\nk=0\nci,k(t \u2212 \u00b5\u03c4\ni )k,\n(9)\nwhere qi(t) is the rotation (in quaternion) of an STG at time\nt, and {ci,k}nq\nk=0, ci,k \u2208 R are polynomial coefficients. Af-\nter converting qi(t) to rotation matrix Ri(t), the covariance\n\u03a3i(t) at time t can be obtained via Eq. (2). We set nq = 1\nin our experiments.\nNote that we keep the scaling matrix Si in Eq. (2)\nto be time-independent, since we experimentally do not\nobserve improvement in rendering quality when applying\ntime-conditioned function on this parameter.\n4.2. Splatted Feature Rendering\nTo encode view- and time-dependent radiance both accu-\nrately and compactly, we store features instead of spherical\nharmonics coefficients (SH) in each STG. Specifically, the\nfeatures fi(t) \u2208 R9 of each STG consist of three parts:\nfi(t) =\n\u0002\nf base\ni\n, f dir\ni ,\n(t \u2212 \u00b5\u03c4\ni )f time\ni\n\u0003T ,\n(10)\nwhere f base\ni\n\u2208 R3 contains base RGB color, and f dir\ni , f time\ni\n\u2208\nR3 encode information related to view direction and time.\nThe feature splatting process is similar to Gaussian Splat-\nting [39], except that the RGB color ci in Eq. (5) is now re-\nplaced by features fi(t). After splatting to image space, we\nsplit the splatted features at each pixel into Fbase, Fdir, Ftime,\nwhose channels correspond to the three parts in Eq. (10).\nThe final RGB color at each pixel is obtained after going\nthrough a 2-layer MLP \u03a6:\nI = Fbase + \u03a6(Fdir, Ftime, r),\n(11)\nwhere r is the view direction at the pixel and is additionally\nconcatenated with the features as input. Fig. 2 (b) shows an\nillustration of the rendering process.\nCompared to SH encoding, our feature-based approach\nrequires fewer parameters for each STG (9 vs. 48 for 3-\ndegree SH). At the same time, since the MLP network \u03a6 is\nshallow and narrow, our method still achieves fast rendering\nspeed.\nTo maximize rendering speed, we can also optionally\ndrop \u03a6 and only keep Fbase during training and rendering.\nWe refer to this configuration as our lite-version model.\n4.3. Optimization\nThe\nparameters\nto\nbe\noptimized\ninclude\nthe\nMLP\n\u03a6\nand\nthe\nparameters\nof\neach\nSTG\n(\u03c3s\ni , s\u03c4\ni , \u00b5\u03c4\ni , {bi,k}np\nk=0, {ci,k}nq\nk=0, si, f base\ni\n, f dir\ni , f time\ni\n).\nFollowing [39], we optimize these parameters through\ndifferentiable splatting and gradient-based backpropaga-\ntion, and interleave with density control of Gaussians. We\nuse rendering loss that compares rendered images with\ngroundtruth images. The rendering loss consists of a L1\nterm and a D-SSIM term.\n5\n4.4. Guided Sampling of Gaussians\nWe observe that areas which have sparse Gaussians at ini-\ntialization are challenging to converge to high rendering\nquality, especially if these areas are far away from the train-\ning cameras. Therefore, we further introduce a strategy to\nsample new Gaussians with the guidance of training error\nand coarse depth.\nWe sample new Gaussians along the rays of pixels that\nhave large errors during training, as illustrated in Fig. 3.\nTo ensure sampling effectiveness, we conduct sampling af-\nter training loss is stable. Since error maps can be noisy\nduring training, we patch-wise aggregate training errors to\nprioritize on areas with substantial errors rather than outlier\npixels. Then we sample a ray from the center pixel of each\nselected patches that have large errors. To avoid sampling\nin an excessively large depth range, we exploit the coarse\ndepth map of Gaussians\u2019 centers to determine a more spe-\ncific depth range. The depth map is generated during fea-\nture splatting and incurs little computational overhead. New\nGaussians are then uniformly sampled within the depth\nrange along the rays.\nWe additionally add small noises\nto the centers of the newly sampled Gaussians.\nAmong\nthe sampled Gaussians, the unnecessary ones will have low\nopacity after steps of training and be pruned. For the scenes\nin our experiments, the above sampling process only needs\nto be conducted no more than 3 times.\nOur guided sampling strategy is complimentary to the\ndensity control techniques in [39]. While density control\ngradually grows Gaussians near existing ones by splitting,\nour approach can sample new Gaussians at regions that have\nsparse or no Gaussians.\n5. Implementation Details\nWe initialize our STGs with the structure-from-motion\nsparse point clouds from all available timestamps. For den-\nsity control, we conduct more aggressive pruning than [39]\nto reduce the number of Gaussians and keep model size to\nbe relatively small. We use Adam optimizer [40]. The train-\ning time for a 50-frame sequence is 40-60 minutes on a sin-\ngle NVIDIA A6000 GPU. We adapt the splatting process to\nsupport different camera models in real world datasets. See\nsupplementary material for more implementation details.\n6. Experiments\nWe evaluate our method on three real-world benchmarks:\nNeural 3D Video Dataset [45] (Sec. 6.1), Google Immer-\nsive Dataset [10] (Sec. 6.2), and Technicolor Dataset [71]\n(Sec. 6.3). We also conduct ablation studies on various as-\npects of our method (Sec. 6.4). Please refer to supplemen-\ntary material and video for more results and real-time demo.\nTable 1.\nQuantitative comparisons on the Neural 3D Video\nDataset. \u201cFPS\u201d is measured at 1352 \u00d7 1014 resolution. \u201cSize\u201d\nis the total model size for 300 frames. Some methods only report\npart of the scenes. For fair comparison, we additionally report\nour results under their settings. 1 only includes the Flame Salmon\nscene. 2 excludes the Coffee Martini scene.\nMethod\nPSNR\u2191\nDSSIM1\u2193\nDSSIM2\u2193\nLPIPS\u2193\nFPS\u2191\nSize\u2193\nNeural Volumes [57] 1\n22.80\n-\n0.062\n0.295\n-\n-\nLLFF [60] 1\n23.24\n-\n0.076\n0.235\n-\n-\nDyNeRF [45] 1\n29.58\n-\n0.020\n0.083\n0.015\n28 MB\nOurs 1\n29.48\n0.038\n0.022\n0.063\n103 300 MB\nHexPlane [12] 2\n31.71\n-\n-\n0.075\n- 200 MB\nOurs 2\n32.74\n0.027\n0.012\n0.039\n140 190 MB\nStreamRF [44]\n28.26\n-\n-\n-\n10.9 5310 MB\nNeRFPlayer [75]\n30.69\n0.034\n-\n0.111\n0.05 5130 MB\nHyperReel [4]\n31.10\n0.036\n-\n0.096\n2 360 MB\nK-Planes [26]\n31.63\n-\n0.018\n-\n0.3 311 MB\nMixVoxels-L [81]\n31.34\n-\n0.017\n0.096\n37.7 500 MB\nMixVoxels-X [81]\n31.73\n-\n0.015\n0.064\n4.6 500 MB\nOurs\n32.05\n0.026\n0.014\n0.044\n140 200 MB\n6.1. Neural 3D Video Dataset\nThe Neural 3D Video Dataset [45] contains six indoor\nmulti-view video sequences captured by 18 to 21 cameras at\n2704 \u00d7 2028 resolution. Following common practice, train-\ning and evaluation are conducted in half resolution, and the\nfirst camera is held out for evaluation [45]. The number of\nframes is 300 for each scene.\nWe use PSNR, DSSIM and LPIPS [94] as evaluation\nmetrics. As mentioned in\n[4, 26], there is an inconsis-\ntency in the DSSIM implementation across methods. For\nfair comparison, we do our best to group existing methods\u2019\nDSSIM results into two categories (DSSIM1 and DSSIM2).\nUsing the structural similarity function from scikit-\nimage library, DSSIM1 sets data range to 1.0 while\nDSSIM2 sets data range to 2.0. We use FPS as metric for\nrendering speed. Metrics are averaged over all six scenes\nexcept noted otherwise.\nAs shown in Tab. 1, our method achieves 140 FPS and\noutperforms the others by a large margin. Our approach\nalso has the best LPIPS in all comparisons and the best\nPSNR/DSSIM in most cases. Fig. 4 shows qualitative com-\nparisons on a representative view that is widely used in\nother work. Compared to the other baselines, our result con-\ntains more vivid details (e.g., the textures on the salmon)\nand artifact-less rendering (e.g., the caustics on the cup).\nPlease see supplementary material for comparisons with\nconcurrent methods.\n6.2. Google Immersive Dataset\nGoogle Immersive Dataset [10] contains indoor and outdoor\nscenes captured with a 46-camera rig. The cameras are in\nfish-eye mode and are mounted on an outward-facing hemi-\nsphere. Compared to outside-in setups, there is less overlap\namong views, hence posing additional challenges.\nFollowing [4, 75], we evaluate on 7 selected scenes\n6\nMixVoxels-L\nGT\nDyNeRF\nHexPlane\nLLFF\nHyperReel\nK-Plane\nOurs\nFigure 4. Qualitative comparisons on the Neural 3D Video Dataset.\nNeRFPlayer\nGT\nHyperReel\nOurs\nFigure 5. Qualitative comparisons on the Google Immersive Dataset.\n(Welder, Flames, Truck, Exhibit, Face Paint 1, Face Paint 2,\nCave) and hold out the center camera as test view. The nu-\nmerical results of NeRFPlayer [75] and HyperReel [4] are\nfrom their papers. The visual results of NeRFPlayer [75] are\n7\nTable 2. Quantitative comparisons on the Google Immersive\nDataset. \u201cSize/Fr\u201d stands for model size per frame.\nMethod\nPSNR\u2191\nDSSIM1\u2193\nLPIPS\u2193\nFPS\u2191 Size/Fr\u2193\nNeRFPlayer [75]\n25.8\n0.076\n0.196\n0.12 17.1 MB\nHyperReel [4]\n28.8\n0.063\n0.193\n4\n1.2 MB\nOurs\n29.2\n0.042\n0.081\n99\n1.2 MB\nTable 3. Quantitative comparisons on the Technicolor Dataset.\n\u201cSize/Fr\u201d stands for model size per frame.\nMethod\nPSNR\u2191\nDSSIM1 \u2191\nDSSIM2 \u2191\nLPIPS\u2193\nFPS\u2191 Size/Fr\u2193\nDyNeRF [45]\n31.8\n-\n0.021\n0.140\n0.02 0.6 MB\nHyperReel [4]\n32.7\n0.047\n-\n0.109\n4.00 1.2 MB\nOurs\n33.6\n0.040\n0.019\n0.084\n86.7 1.1 MB\nTable 4. Ablation study of proposed components. Conducted on\nthe Technicolor Dataset.\nMethod\nPSNR\u2191\nDSSIM1\u2193\nLPIPS\u2193\nw/o Temporal Opacity\n31.0\n0.063\n0.153\nw/o Polynomial Motion\n32.6\n0.045\n0.099\nw/o Polynomial Rotation\n33.4\n0.042\n0.085\nw/o Feature Splatting\n33.0\n0.044\n0.097\nw/o Guided Sampling of Gaussians\n33.3\n0.041\n0.085\nOurs Full\n33.6\n0.040\n0.084\nobtained from their authors while those of HyperReel [4]\nare produced by running their released codes.\nAs shown in Tab. 2, our method outperforms NeRF-\nPlayer and HyperReel in both speed and quality. Compared\nto HyperReel, our method is over 10 times faster in render-\ning speed. Although our PSNR is only 0.4 dB higher, the\nimprovements in DSSIM and LPIPS are significant. When\ncompared to NeRFPlayer, the margin is larger for all met-\nrics. Visual comparisons are demonstrated in Fig. 5. Our\nmethod depicts sharper details and fewer artifacts than the\nothers.\n6.3. Technicolor Dataset\nTechnicolor Light Field Dataset [71] contains videos taken\nwith a 4x4 camera array. Each camera is time-synchronized\nand the spatial resolution is 2048\u00d71088. In alignment with\nHyperReel [4], we hold out the camera at second row sec-\nond column and evaluate on five scenes (Birthday, Fabien,\nPainter, Theater, Trains) at full resolution.\nTab. 3 shows the comparisons, where our method\nachieves noticeable gain in quality and speed. Please refer\nto supplementary material for visual comparisons on this\ndataset.\n6.4. Ablation Study\nTo evaluate the effectiveness of proposed components, we\nconduct an ablation study in Tab. 4 using the Technicolor\nTable 5. Ablation study on the number of frames whose SfM\npoint clouds are used in initialization. Conducted on the Theater\nscene from the Technicolor Dataset.\nEvery N Frames\nPSNR\u2191\nDSSIM1\u2193\nLPIPS\u2193\nSize\u2193\nN = 1\n31.58\n0.059\n0.124\n110.2 MB\nN = 4\n31.51\n0.057\n0.117\n46.7 MB\nN = 16\n31.04\n0.060\n0.139\n32.6 MB\nDataset. Below we describe the configuration and perfor-\nmance of each ablation baseline.\nTemporal Opacity.\n\u201cw/o Temporal Opacity\u201d fixes the\ncenter and scale of the temporal radial basis functions in\nSTGs during training. This variant suffers from a signifi-\ncant performance drop, revealing the importance of tempo-\nral opacity.\nPolynomial Motion and Rotation.\n\u201cw/o Polynomial\nMotion\u201d and \u201cw/o Polynomial Rotation\u201d fix the spatial po-\nsition and rotation of STGs respectively. Both lead to a\nperformance drop. Comparatively, motion is more impor-\ntant than rotation, which motivates us to use a lower-degree\npolynomial for rotation.\nFeature Splatting.\n\u201cw/o Feature Splatting\u201d uses the base\nRGB color Fbase as the final color. It can be seen that there\nis a moderate drop in quality due to reduced ability to model\nview- and time-dependent appearance.\nGuided Sampling of Gaussians.\n\u201cw/o Guided Sampling\nof Gaussians\u201d does not encounter much performance drop\nin this dataset. The reason is that the scenes contain rich\ntextures and can be well covered by SfM points. However,\nfor other challenging scenes, guided sampling plays an im-\nportant role (see Fig. 3 and supplementary material for ex-\namples).\nNumber of Frames used for Initialization.\nWe fur-\nther analyzed the number of frames used for initialization\nin Tab. 5. Using fewer frames slightly downgrade quality,\nbut also significantly reduces model size. It reveals that the\ncompactness of our method can be further enhanced with a\ngood selection of frames for initialization.\n6.5. Limitation\nAlthough our representation achieves fast rendering speed,\nit cannot be trained on-the-fly. The support for on-the-fly\ntraining could benefit numerous streaming applications. To\nachieve this, advanced initialization techniques could be ex-\nplored to accelerate the training process or alleviate the re-\nquirement of per-scene training. On the other hand, our\n8\nmethod currently focuses on multi-view video inputs. It is\npromising to adapt our approach to monocular setting by\ncombining with regularization or generative priors.\n7. Conclusion\nWe present a novel representation based on Spacetime\nGaussians for dynamic view synthesis.\nThe proposed\nSpacetime Gaussians are enhanced with temporal opacity\nand parametric motion/rotation to model complex 4D\ncontent.\nTo increase model compactness and encode\nview/time-dependent appearance, we introduce splatted\nfeature rendering, which utilizes neural features and a\nlightweight MLP instead of spherical harmonics.\nAd-\nditionally, we leverage guided sampling of Gaussians\nto further improve the rendering quality of complex\nscenes. Experiments on real-world datasets show that our\nrepresentation delivers state-of-the-art quality at high reso-\nlution and FPS, while maintaining a compact model size.\nReferences\n[1] Kara-Ali Aliev,\nDmitry Ulyanov,\nand Victor Lempit-\nsky.\nNeural point-based graphics.\narXiv preprint\narXiv:1906.08240, 2019. 2\n[2] Benjamin Attal, Selena Ling, Aaron Gokaslan, Christian\nRichardt, and James Tompkin.\nMatryodshka: Real-time\n6dof video view synthesis using multi-sphere images.\nIn\nEuropean Conference on Computer Vision, pages 441\u2013459.\nSpringer, 2020. 3, 14\n[3] Benjamin Attal, Jia-Bin Huang, Michael Zollh\u00a8ofer, Johannes\nKopf, and Changil Kim. Learning neural light fields with\nray-space embedding. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n19819\u201319829, 2022. 2\n[4] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\nZollhoefer, Johannes Kopf, Matthew O\u2019Toole, and Changil\nKim.\nHyperReel:\nHigh-fidelity 6-DoF video with ray-\nconditioned sampling. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2, 3, 6, 7, 8, 13, 16,\n17\n[5] Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, and\nSrinivasa Narasimhan. 4d visualization of dynamic events\nfrom unconstrained multi-view videos.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5366\u20135375, 2020. 3\n[6] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5855\u20135864,\n2021. 1\n[7] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022.\n[8] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-\nbased neural radiance fields. ICCV, 2023. 1, 2\n[9] Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel,\nand Tobias Ritschel. X-fields: Implicit neural view-, light-\nand time-image interpolation. ACM Transactions on Graph-\nics (TOG), 39(6):1\u201315, 2020. 3\n[10] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Er-\nickson, Peter Hedman, Matthew DuVall, Jason Dourgarian,\nJay Busch, Matt Whalen, and Paul Debevec. Immersive light\nfield video with a layered mesh representation. 39(4):86:1\u2013\n86:15, 2020. 3, 6, 13, 14, 17, 21\n[11] Chris Buehler, Michael Bosse, Leonard McMillan, Steven\nGortler, and Michael Cohen. Unstructured lumigraph render-\ning. In Proceedings of the 28th Annual Conference on Com-\nputer Graphics and Interactive Techniques, page 425\u2013432,\nNew York, NY, USA, 2001. Association for Computing Ma-\nchinery. 2\n[12] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 130\u2013141, 2023. 2, 3, 6, 16\n[13] Rodrigo Ortiz Cayon, Abdelaziz Djelouah, and George Dret-\ntakis. A bayesian approach for selective image-based render-\ning using superpixels. In 2015 International Conference on\n3D Vision, pages 469\u2013477. IEEE, 2015. 2\n[14] Gaurav\nChaurasia,\nSylvain\nDuchene,\nOlga\nSorkine-\nHornung, and George Drettakis. Depth synthesis and local\nwarps for plausible image-based navigation. ACM Transac-\ntions on Graphics (TOG), 32(3):1\u201312, 2013. 2\n[15] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 1, 2\n[16] Zhang Chen, Anpei Chen, Guli Zhang, Chengyuan Wang, Yu\nJi, Kiriakos N Kutulakos, and Jingyi Yu. A neural rendering\nframework for free-viewpoint relighting. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5599\u20135610, 2020. 2\n[17] Zhang Chen, Zhong Li, Liangchen Song, Lele Chen, Jingyi\nYu, Junsong Yuan, and Yi Xu. Neurbf: A neural fields repre-\nsentation with adaptive radial basis functions. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 4182\u20134194, 2023. 1, 3, 4\n[18] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-\nnis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,\nand Steve Sullivan. High-quality streamable free-viewpoint\nvideo. ACM Transactions on Graphics (ToG), 34(4):1\u201313,\n2015. 3\n[19] Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik.\nModeling and rendering architecture from photographs: A\nhybrid geometry- and image-based approach. In Proceedings\nof the 23rd Annual Conference on Computer Graphics and\nInteractive Techniques, page 11\u201320, New York, NY, USA,\n1996. Association for Computing Machinery. 2\n[20] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenen-\nbaum, and Jiajun Wu.\nNeural radiance flow for 4d view\n9\nsynthesis and video processing.\nIn 2021 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n14304\u201314314. IEEE Computer Society, 2021. 3\n[21] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural vox-\nels. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139,\n2022. 3\n[22] Liangji Fang, Qinhong Jiang, Jianping Shi, and Bolei Zhou.\nTpnet: Trajectory proposal network for motion prediction.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6797\u20136806, 2020. 4\n[23] Brandon Yushan Feng and Amitabh Varshney. Signet: Ef-\nficient neural representation for light fields. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 14224\u201314233, 2021. 2\n[24] John Flynn, Michael Broxton, Paul Debevec, Matthew Du-\nVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and\nRichard Tucker. Deepview: View synthesis with learned gra-\ndient descent. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2367\u2013\n2376, 2019. 2\n[25] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 1, 2\n[26] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12479\u201312488, 2023. 2,\n3, 6, 13, 16\n[27] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 5712\u20135721, 2021. 3\n[28] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell,\nand Angjoo Kanazawa. Monocular dynamic view synthesis:\nA reality check. Advances in Neural Information Processing\nSystems, 35:33768\u201333780, 2022. 3\n[29] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and\nMichael F Cohen. The lumigraph. In Siggraph, pages 43\u2013\n54, 1996. 2\n[30] Peter Hedman, Tobias Ritschel, George Drettakis, and\nGabriel Brostow. Scalable inside-out image-based render-\ning. ACM Trans. Graph., 35(6), 2016. 2\n[31] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\nfree-viewpoint image-based rendering. In SIGGRAPH Asia\n2018 Technical Papers, page 257. ACM, 2018. 2\n[32] Benno Heigl, Reinhard Koch, Marc Pollefeys, Joachim Den-\nzler, and Luc J. Van Gool. Plenoptic modeling and rendering\nfrom image sequences taken by hand-held camera. In Mus-\ntererkennung 1999, 21. DAGM-Symposium, page 94\u2013101,\nBerlin, Heidelberg, 1999. Springer-Verlag. 2\n[33] Adam Houenou, Philippe Bonnifait, V\u00b4eronique Cherfaoui,\nand Wen Yao. Vehicle trajectory prediction based on motion\nmodel and maneuver recognition. In 2013 IEEE/RSJ inter-\nnational conference on intelligent robots and systems, pages\n4363\u20134369. IEEE, 2013. 4\n[34] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao,\nXiao Liu, and Yuewen Ma.\nTri-miprf: Tri-mip represen-\ntation for efficient anti-aliasing neural radiance fields.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 19774\u201319783, 2023. 2\n[35] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu,\nYan-Pei Cao, and Xiaojuan Qi.\nSc-gs: Sparse-controlled\ngaussian splatting for editable dynamic scenes, 2023. Ac-\ncessed on 12,12,2023. 3\n[36] Mustafa Is\u00b8\u0131k, Martin R\u00a8unz, Markos Georgopoulos, Taras\nKhakhulin, Jonathan Starck, Lourdes Agapito, and Matthias\nNie\u00dfner. Humanrf: High-fidelity neural radiance fields for\nhumans in motion. arXiv preprint arXiv:2305.06356, 2023.\n3\n[37] Ramesh Jain and Koji Wakimoto. Multiple perspective in-\nteractive video. In Proceedings of the international confer-\nence on multimedia computing and systems, pages 202\u2013211.\nIEEE, 1995. 3\n[38] Takeo Kanade, Peter Rander, and PJ Narayanan. Virtualized\nreality: Constructing virtual worlds from real scenes. IEEE\nmultimedia, 4(1):34\u201347, 1997. 3\n[39] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(TOG), 42(4):1\u201314, 2023. 1, 2, 3, 4, 5, 6, 13\n[40] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[41] Johannes Kopf, Michael F Cohen, and Richard Szeliski.\nFirst-person hyper-lapse videos.\nACM Transactions on\nGraphics (TOG), 33(4):78, 2014. 2\n[42] Agelos Kratimenos, Jiahui Lei, and Kostas Daniilidis.\nDynmf: Neural motion factorization for real-time dynamic\nview synthesis with 3d gaussian splatting. arXiV, 2023. 3\n[43] Marc Levoy and Pat Hanrahan. Light field rendering. In Pro-\nceedings of the 23rd annual conference on Computer graph-\nics and interactive techniques, pages 31\u201342. ACM, 1996. 2\n[44] Lingzhi Li, Zhen Shen, zhongshu wang, Li Shen, and Ping\nTan. Streaming radiance fields for 3d video synthesis. In\nAdvances in Neural Information Processing Systems, 2022.\n3, 6\n[45] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\net al. Neural 3d video synthesis from multi-view video. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5521\u20135531, 2022. 2, 3,\n6, 8, 13, 16, 17, 18, 19, 20\n[46] Zhong Li, Yu Ji, Wei Yang, Jinwei Ye, and Jingyi Yu. Ro-\nbust 3d human motion reconstruction via dynamic template\nconstruction. In 2017 International Conference on 3D Vision\n(3DV), pages 496\u2013505. IEEE, 2017. 3\n[47] Zhong Li, Minye Wu, Wangyiteng Zhou, and Jingyi Yu. 4d\nhuman body correspondences from panoramic depth maps.\n10\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2877\u20132886, 2018. 3\n[48] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6498\u2013\n6508, 2021. 3\n[49] Zhong Li, Liangchen Song, Celong Liu, Junsong Yuan, and\nYi Xu. Neulf: Efficient novel view synthesis with neural 4d\nlight field. In Eurographics Symposium on Rendering, 2022.\n2\n[50] Zhong Li, Liangchen Song, Zhang Chen, Xiangyu Du, Lele\nChen, Junsong Yuan, and Yi Xu. Relit-neulf: Efficient re-\nlighting and novel view synthesis via neural 4d light field. In\nProceedings of the 31st ACM International Conference on\nMultimedia, pages 7007\u20137016, 2023. 2\n[51] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\nrendering. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n4273\u20134284, 2023. 3\n[52] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hu-\njun Bao, and Xiaowei Zhou. Im4d: High-fidelity and real-\ntime novel view synthesis for dynamic scenes. arXiv preprint\narXiv:2310.08585, 2023. 3, 13, 14\n[53] Kai-En Lin, Lei Xiao, Feng Liu, Guowei Yang, and Ravi\nRamamoorthi. Deep 3d mask volume for view synthesis of\ndynamic scenes. In ICCV, 2021. 3\n[54] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao.\nGaussian-flow: 4d reconstruction with dynamic 3d gaussian\nparticle, 2023. 3\n[55] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel fields. Advances\nin Neural Information Processing Systems, 33:15651\u201315663,\n2020. 2\n[56] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu\nTseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo-\nhannes Kopf, and Jia-Bin Huang. Robust dynamic radiance\nfields. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 13\u201323, 2023. 3\n[57] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\nACM Trans. Graph., 38(4):65:1\u201365:14, 2019. 2, 3, 6, 16\n[58] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM Transactions on Graphics (ToG), 40(4):1\u201313, 2021. 3\n[59] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan. Dynamic 3d gaussians: Tracking by per-\nsistent dynamic view synthesis. In 3DV, 2024. 3, 13, 14,\n16\n[60] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 2019. 6, 16\n[61] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 1,\n2\n[62] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 1, 2\n[63] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, Anton\nKaplanyan, and Markus Steinberger. Donerf: Towards real-\ntime rendering of compact neural radiance fields using depth\noracle networks. In Computer Graphics Forum, pages 45\u2013\n59. Wiley Online Library, 2021. 2\n[64] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5865\u20135874, 2021. 3\n[65] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv preprint arXiv:2106.13228, 2021. 3\n[66] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xi-\naowei Zhou.\nRepresenting volumetric videos as dynamic\nmlp maps.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4252\u2013\n4262, 2023. 3\n[67] Eric Penner and Li Zhang. Soft 3d reconstruction for view\nsynthesis. 36(6), 2017. 2\n[68] Martin Piala and Ronald Clark. Terminerf: Ray termination\nprediction for efficient neural rendering. In 2021 Interna-\ntional Conference on 3D Vision (3DV), pages 1106\u20131114.\nIEEE, 2021. 2\n[69] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer.\nD-nerf: Neural radiance fields\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10318\u201310327, 2021. 3\n[70] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 14335\u2013\n14345, 2021. 2\n[71] Neus Sabater, Guillaume Boisson, Benoit Vandame, Paul\nKerbiriou, Frederic Babon, Matthieu Hog, Remy Gendrot,\nTristan Langlois, Olivier Bureller, Arno Schubert, et al.\nDataset and pipeline for multi-view light-field video. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition Workshops, pages 30\u201340, 2017. 6, 8, 13, 17,\n22\n[72] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. In Proceedings of the IEEE/CVF Conference\n11\non Computer Vision and Pattern Recognition, pages 16632\u2013\n16642, 2023. 3\n[73] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\nNie\u00dfner, Gordon Wetzstein, and Michael Zollhofer. Deep-\nvoxels: Learning persistent 3d feature embeddings. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 2437\u20132446, 2019. 2\n[74] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh\nTenenbaum, and Fredo Durand. Light field networks: Neu-\nral scene representations with single-evaluation rendering.\nAdvances in Neural Information Processing Systems, 34:\n19313\u201319325, 2021. 2\n[75] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger.\nNerf-\nplayer: A streamable dynamic scene representation with de-\ncomposed neural radiance fields. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(5):2732\u20132742, 2023.\n2, 3, 6, 7, 8, 13, 14, 16, 17\n[76] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and\nAmeesh Makadia. Light field neural rendering. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8269\u20138279, 2022. 2\n[77] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5459\u2013\n5469, 2022. 1, 2\n[78] Justus Thies, Michael Zollh\u00a8ofer, and Matthias Nie\u00dfner. De-\nferred neural rendering: Image synthesis using neural tex-\ntures. ACM Transactions on Graphics (TOG), 38(4):1\u201312,\n2019. 2\n[79] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\nZollh\u00a8ofer, Christoph Lassner, and Christian Theobalt. Non-\nrigid neural radiance fields: Reconstruction and novel view\nsynthesis of a dynamic scene from monocular video.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 12959\u201312970, 2021. 3\n[80] Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio\nGallo. Neural trajectory fields for dynamic novel view syn-\nthesis. arXiv preprint arXiv:2105.05994, 2021. 3\n[81] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei\nSong, and Huaping Liu. Mixed neural voxels for fast multi-\nview video synthesis. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n19706\u201319716, 2023. 2, 3, 6, 13, 16\n[82] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Men-\nglei Chai, Yun Fu, and Sergey Tulyakov.\nR2l: Distilling\nneural radiance field to neural light field for efficient novel\nview synthesis. In European Conference on Computer Vi-\nsion, pages 612\u2013629. Springer, 2022. 2\n[83] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yan-\nshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and\nLan Xu. Fourier plenoctrees for dynamic radiance field ren-\ndering in real-time. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n13524\u201313534, 2022. 3\n[84] Liao Wang, Qiang Hu, Qihan He, Ziyu Wang, Jingyi Yu,\nTinne Tuytelaars, Lan Xu, and Minye Wu. Neural residual\nradiance fields for streamably free-viewpoint videos. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 76\u201387, 2023. 3\n[85] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023. 3, 13, 14\n[86] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 9421\u20139431,\n2021. 3\n[87] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng,\nYin Yang, and Chenfanfu Jiang.\nPhysgaussian: Physics-\nintegrated 3d gaussians for generative dynamics.\narXiv\npreprint arXiv:2311.12198, 2023. 3\n[88] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 2\n[89] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming\nSun, Yujun Shen, Hujun Bao, and Xiaowei Zhou.\n4k4d:\nReal-time 4d view synthesis at 4k resolution. 2023. 3, 13, 14\n[90] Jason C Yang, Matthew Everett, Chris Buehler, and Leonard\nMcMillan. A real-time distributed light field camera. Ren-\ndering Techniques, 2002(77-86):2, 2002. 3\n[91] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin.\nDeformable 3d gaussians for\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\npreprint arXiv:2309.13101, 2023. 3\n[92] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\nZhang. Real-time photorealistic dynamic scene representa-\ntion and rendering with 4d gaussian splatting. arXiv preprint\narXiv:2310.10642, 2023. 3, 13, 14\n[93] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[94] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 6\n[95] Ke Colin Zheng, Alex Colburn, Aseem Agarwala, Maneesh\nAgrawala, David Salesin, Brian Curless, and Michael F Co-\nhen.\nParallax photography: creating 3d cinematic effects\nfrom stills. In Proceedings of Graphics Interface 2009, pages\n111\u2013118. Canadian Information Processing Society, 2009. 2\n[96] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele,\nSimon Winder, and Richard Szeliski.\nHigh-quality video\nview interpolation using a layered representation.\nACM\ntransactions on graphics (TOG), 23(3):600\u2013608, 2004. 3\n[97] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\nMarkus Gross. Ewa volume splatting. In Proceedings Visu-\nalization, 2001. VIS\u201901., pages 29\u2013538. IEEE, 2001. 3\n12\nA. Overview\nWithin the supplementary material, we provide:\n\u2022 Quantitative and qualitative comparisons to concurrent\nwork in Appendix B.\n\u2022 Qualitative examples for the ablation on guided sampling\nin Appendix C.\n\u2022 Per-scene quantitative comparisons and more visual com-\nparisons with other methods on the Neural 3D Video\nDataset [45], Google Immersive Dataset [10] and Tech-\nnicolor Dataset [71] in Appendix D.\n\u2022 Additional experiment details in Appendix E.\n\u2022 Real-time demos and dynamic comparisons in our video.\nB. Comparisons with Concurrent Work\nWe compare with concurrent work [59, 85, 89, 92] on the\nNeural 3D Video Dataset [45] in Tab. 6.\nWe also in-\nclude Im4D [52] in this comparison since it is related to\n4K4D [89]. Same with Tab. 1 in the main paper, we group\nDSSIM results into two categories (DSSIM1: data range\nis set to 1.0; DSSIM2: data range is set to 2.0).\nCompared to methods [59, 85, 92] that similarly build\nupon Gaussian Splatting, our method achieves the best ren-\ndering quality and is among the fastest and most compact\nones. Specifically, in terms of quality, our full model per-\nforms the best on all of PSNR, DSSIM and LPIPS. Mean-\nwhile, our lite model also outperforms Dynamic 3DGS [59]\nand 4DGaussians [85] by a noticeable margin, and is only\ninferior to 4DGS [92].\nBoth our lite model and Dynamic 3DGS [59] can run at\nover 300 FPS on the Neural 3D Video Dataset. Although\nour full model is slower than these two, it is still faster than\n4DGS [92] and 4DGaussians [85].\nCompared with Dy-\nnamic 3DGS, our lite model takes about only six percent\nof model size and is 0.6 dB higher in PSNR. Meanwhile,\nthe results of Dynamic 3DGS contain many time-varying\nfloaters, which harm temporal consistency and visual qual-\nity. To illustrate this, we show the slices of a column of\npixels across time in Fig. 6. In this visualization, tempo-\nral noises appear as sharp vertical lines or dots. It can be\nseen that the results of Dynamic 3DGS contain many such\npatterns. On the contrary, our results are free of these ar-\ntifacts. One reason for this phenomenon is that Dynamic\n3DGS requires per-frame training, while ours trains across\na sequence of frames. As a result, our method can better\npreserve the temporal consistency across frames. Please re-\nfer to our video for dynamic comparisons.\nCompared to Im4D [52] and 4K4D [89], both our full\nmodel and lite-version model achieve higher rendering\nquality and speed.\nC. Ablation on Guided Sampling\nWe visualize the effects of guided sampling in Fig. 7. It\ncan be seen that when without guided sampling, distant ar-\neas that are not well covered by SfM points will have very\nblurry rendering in both training and novel views. It re-\nveals that it is challenging to pull Gaussians to these areas\nwith gradient-based optimization and density control. On\nthe other hand, with guided sampling applied, the rendering\nat these areas become much sharper for both training and\nnovel views.\nD. More Results\nWe provide per-scene quantitative comparisons on the Neu-\nral 3D Video Dataset [45] (Tab. 7), Google Immersive\nDataset [10] (Tab. 8) and Technicolor Dataset [71] (Tab. 9).\nOur method outperforms the other baselines on most scenes.\nFigs. 8 to 10 show more visual comparisons of our full\nmodel and our lite-version model with NeRFPlayer [75],\nHyperReel [4], K-Planes [26], MixVoxels-L [81] and Dy-\nnamic 3DGS [59] on the Neural 3D Video Dataset [45].\nFig. 11 shows more visual comparisons on the Google\nImmersive Dataset [10]. We compare the results of our full\nmodel and lite-version model to NeRFPlayer [75] and Hy-\nperReel [4].\nFig. 12 shows visual comparisons on the Technicolor\nDataset [71]. We compare the results of our full model and\nlite-version model to HyperReel [4].\nThe above visual comparisons demonstrate that our\nmethod preserves sharp details while containing fewer ar-\ntifacts. Compared to our full model, the results of our lite-\nversion model are slightly blurrier.\nE. Experiment Details\nSince the open source code of MixVoxels [81] does\nnot contain the training config for MixVoxels-X, we use\nMixVoxels-L in our comparisons. We train HyperReel with\ntheir official code to generate visual examples. Note that\nthe training time of HyperReel for each scene on the Tech-\nnicolor Dataset is about 3.5 hours, while that of our full\nmodel on the Technicolor Dataset is only about 1 hour. For\nDynamic 3DGS, its performance on the Neural 3D Video\nDataset is not reported in their original paper. When apply-\ning its open source code to Neural 3D Video Dataset with\ntheir default hyperparameters, the rendering quality is sub-\npar. So we tune its hyperparameters to improve the per-\nformance on this dataset. We use Nvidia RTX 3090 when\nreporting our rendering speed in the comparisons with other\nmethods, and use Nvidia RTX 4090 in our real-time demos.\nWe use the original centered-undistorted camera model\nfrom 3DGS [39] for the Neural 3D Video Dataset.\nWe\nimplement the uncentered-undistorted camera model for\nthe Technicolor Dataset.\nFor the Google Immersive\n13\nTable 6. Quantitative comparisons on the Neural 3D Video Dataset. \u201cFPS\u201d is measured at 1352 \u00d7 1014 resolution. \u201cSize\u201d is the total\nmodel size for 300 frames. Some methods only report results on part of the scenes. For fair comparison, we additionally report our results\nunder their settings. 3 only includes the Cook Spinach, Cut Roasted Beef, and Sear Steak scenes. 4 only includes the Cut Roasted Beef\nscene. For the LPIPS metric, no annotation means LPIPSAlex, V denotes LPIPSV GG, and \u2020 denotes it is unclear which one is used from\nthe corresponding paper.\nMethod\nPSNR\u2191\nDSSIM1\u2193\nDSSIM2\u2193\nLPIPS\u2193\nFPS\u2191\nSize\u2193\nDynamic 3DGS [59]\n30.67\n0.035\n0.019\n0.099\n460 2772 MB\n4DGS [92]\n32.01\n-\n0.014\n-\n114\n-\nOurs\n32.05\n0.026\n0.014\n0.044\n140 200 MB\nOurs-Lite\n31.59\n0.027\n0.015\n0.047\n310 103 MB\n4DGaussians [85] 3\n31.02\n0.030\n-\n0.15 \u2020\n36\n145MB\nOurs 3\n33.53\n0.020\n0.010\n0.034, 0.131 V\n154\n148MB\nOurs-Lite 3\n33.36\n0.020\n0.011\n0.036, 0.133 V\n330\n83MB\nIm4D [52] 4\n32.58\n-\n0.015\n0.208 V\n-\n-\n4K4D [89] 4\n32.86\n-\n0.014\n0.167 V\n110\n-\nOurs 4\n33.52\n0.020\n0.011\n0.035, 0.133 V\n151 154 MB\nOurs-Lite 4\n33.72\n0.021\n0.011\n0.038, 0.136 V\n338\n80 MB\nOurs-Lite\nDynamic 3DGS\nGT\nOurs\nTime t\nFigure 6. Comparisons of temporal consistency on the Neural 3D Video Dataset. From the test view video results of each method, we\ntake a vertical column of 150 pixels across 250 frames and concatenate these columns horizontally. The resulting image patch is equivalent\nto a slice in the height-time space. Ours results are clearer than Dynamic 3DGS [59] and contain fewer temporal noises.\nDataset [10], to evaluate on the same distorted videos\nas [2, 75], we further adapt our method to fit the uncentered-\ndistorted camera model with a differentiable image space\nwarping, which maps perspective view to fish-eye distorted\nview. For the real-time demo, we retrain our models on the\nundistorted videos for simplicity.\nFor the Theater, Train and Birthday scenes in Tabs. 3\nand 9, we only use 25 percent of SfM points from the frames\n(except the first frame whose SfM points are all used). In the\nablation study on the number of frames whose SfM points\nare used (Tab. 5), we use all the points in each sampled\nframes.\n14\nw/ Guided Sampling\nw/o Guided Sampling\nGT\nw/o Guided Sampling\nw/ Guided Sampling\nNovel View\nTraining View\nFigure 7. Ablation on Guided Sampling. With guided sampling, the rendering results contain less blurriness in both training and novel\nviews.\n15\nTable 7. Per-scene quantitative comparisons on the Neural 3D Video Dataset [45]. Some methods only report part of the scenes. 1 only\nincludes the Flame Salmon scene. 2 excludes the Coffee Martini scene. \u201c-\u201d denotes results that are unavailable in prior work.\nMethod\nAvg.\nCoffee Martini\nCook Spinach\nCut Roasted Beef\nFlame Salmon\nFlame Steak\nSear Steak\nPSNR\u2191\nNeural Volumes [57] 1\n22.80\n-\n-\n-\n22.80\n-\n-\nLLFF [60] 1\n23.24\n-\n-\n-\n23.24\n-\n-\nDyNeRF [45] 1\n29.58\n-\n-\n-\n29.58\n-\n-\nHexPlane [12] 2\n31.71\n-\n32.04\n32.55\n29.47\n32.08\n32.39\nNeRFPlayer [75]\n30.69\n31.53\n30.56\n29.35\n31.65\n31.93\n29.13\nHyperReel [4]\n31.10\n28.37\n32.30\n32.92\n28.26\n32.20\n32.57\nK-Planes [26]\n31.63\n29.99\n32.60\n31.82\n30.44\n32.38\n32.52\nMixVoxels-L [81]\n31.34\n29.63\n32.25\n32.40\n29.81\n31.83\n32.10\nMixVoxels-X [81]\n31.73\n30.39\n32.31\n32.63\n30.60\n32.10\n32.33\nDynamic 3DGS [59]\n30.67\n26.49\n32.97\n30.72\n26.92\n33.24\n33.68\nOurs\n32.05\n28.61\n33.18\n33.52\n29.48\n33.64\n33.89\nOurs-Lite\n31.59\n27.49\n32.92\n33.72\n28.67\n33.28\n33.47\nDSSIM1\u2193\nNeRFPlayer [75]\n0.034\n0.0245\n0.0355\n0.0460\n0.0300\n0.0250\n0.0460\nHyperReel [4]\n0.036\n0.0540\n0.0295\n0.0275\n0.0590\n0.0255\n0.0240\nDynamic 3DGS [59]\n0.035\n0.0557\n0.0263\n0.0295\n0.0512\n0.0233\n0.0224\nOurs\n0.026\n0.0415\n0.0215\n0.0205\n0.0375\n0.0176\n0.0174\nOurs-Lite\n0.027\n0.0437\n0.0218\n0.0209\n0.0387\n0.0179\n0.0177\nDSSIM2\u2193\nNeural Volumes [57] 1\n0.062\n-\n-\n-\n0.062\n-\n-\nLLFF [60] 1\n0.076\n-\n-\n-\n0.076\n-\n-\nDyNeRF [45] 1\n0.020\n-\n-\n-\n0.020\n-\n-\nK-Planes [26]\n0.018\n0.0235\n0.0170\n0.0170\n0.0235\n0.0150\n0.0130\nMixVoxels-L [81]\n0.017\n0.0244\n0.0162\n0.0157\n0.0255\n0.0144\n0.0122\nMixVoxels-X [81]\n0.015\n0.0232\n0.0160\n0.0146\n0.0233\n0.0137\n0.0121\nDynamic 3DGS [59]\n0.019\n0.0332\n0.0129\n0.0161\n0.0302\n0.0113\n0.0105\nOurs\n0.014\n0.0250\n0.0113\n0.0105\n0.0224\n0.0087\n0.0085\nOurs-Lite\n0.015\n0.0270\n0.0118\n0.0112\n0.0244\n0.0097\n0.0095\nLPIPSAlex\u2193\nNeural Volumes [57] 1\n0.295\n-\n-\n-\n0.295\n-\n-\nLLFF [60] 1\n0.235\n-\n-\n-\n0.235\n-\n-\nDyNeRF [45] 1\n0.083\n-\n-\n-\n0.083\n-\n-\nHexPlane [12] 2\n0.075\n-\n0.082\n0.080\n0.078\n0.066\n0.070\nNeRFPlayer [75]\n0.111\n0.085\n0.113\n0.144\n0.098\n0.088\n0.138\nHyperReel [4]\n0.096\n0.127\n0.089\n0.084\n0.136\n0.078\n0.077\nMixVoxels-L [81]\n0.096\n0.106\n0.099\n0.088\n0.116\n0.088\n0.080\nMixVoxels-X [81]\n0.064\n0.081\n0.062\n0.057\n0.078\n0.051\n0.053\nDynamic 3DGS [59]\n0.099\n0.139\n0.087\n0.090\n0.122\n0.079\n0.079\nOurs\n0.044\n0.069\n0.037\n0.036\n0.063\n0.029\n0.030\nOurs-Lite\n0.047\n0.075\n0.038\n0.038\n0.068\n0.031\n0.031\n16\nTable 8. Per-scene quantitative comparisons on the Google Immersive Dataset [10].\nMethod\nAvg.\n01 Welder\n02 Flames\n04 Truck\n09 Exhibit\n10 Face Paint 1\n11 Face Paint 2\n12 Cave\nPSNR\u2191\nNeRFPlayer [75]\n25.8\n25.568\n26.554\n27.021\n24.549\n27.772\n27.352\n21.825\nHyperReel [4]\n28.8\n25.554\n30.631\n27.175\n31.259\n29.305\n27.336\n30.063\nOurs\n29.2\n26.844\n30.566\n27.308\n29.336\n30.588\n29.895\n29.610\nOurs-Lite\n27.5\n25.499\n29.505\n24.204\n27.973\n28.646\n28.456\n27.977\nDSSIM1\u2193\nNeRFPlayer [75]\n0.076\n0.0910\n0.0790\n0.0615\n0.0655\n0.0420\n0.0490\n0.1425\nHyperReel [4]\n0.063\n0.1050\n0.0475\n0.0760\n0.0485\n0.0435\n0.0605\n0.0595\nOurs\n0.042\n0.0504\n0.0349\n0.0524\n0.0447\n0.0240\n0.0320\n0.0543\nOurs-Lite\n0.051\n0.0585\n0.0546\n0.0684\n0.0516\n0.0271\n0.0326\n0.0630\nLPIPSAlex\u2193\nNeRFPlayer [75]\n0.196\n0.289\n0.154\n0.164\n0.151\n0.147\n0.152\n0.314\nHyperReel [4]\n0.193\n0.281\n0.159\n0.223\n0.140\n0.139\n0.195\n0.214\nOurs\n0.081\n0.098\n0.059\n0.087\n0.073\n0.055\n0.063\n0.133\nOurs-Lite\n0.095\n0.119\n0.070\n0.115\n0.087\n0.067\n0.062\n0.143\nTable 9. Per-scene quantitative comparisons on the Technicolor Dataset [71].\nMethod\nAvg.\nBirthday\nFabien\nPainter\nTheater\nTrains\nPSNR\u2191\nDyNeRF [45]\n31.8\n29.20\n32.76\n35.95\n29.53\n31.58\nHyperReel [4]\n32.7\n29.99\n34.70\n35.91\n33.32\n29.74\nOurs\n33.6\n32.09\n35.70\n36.44\n30.99\n32.58\nOurs-Lite\n33.0\n31.59\n35.28\n35.95\n30.12\n32.17\nDSSIM1\u2193\nHyperReel [4]\n0.047\n0.0390\n0.0525\n0.0385\n0.0525\n0.0525\nOurs\n0.040\n0.0290\n0.0471\n0.0366\n0.0596\n0.0294\nOurs-Lite\n0.044\n0.0330\n0.0522\n0.0382\n0.0634\n0.0324\nDSSIM2\u2193\nDyNeRF [45]\n0.021\n0.0240\n0.0175\n0.0140\n0.0305\n0.0190\nOurs\n0.019\n0.0153\n0.0179\n0.0146\n0.0287\n0.0168\nOurs-Lite\n0.021\n0.0175\n0.0201\n0.0154\n0.0312\n0.0185\nLPIPSAlex\u2193\nDyNeRF [45]\n0.140\n0.0668\n0.2417\n0.1464\n0.1881\n0.0670\nHyperReel [4]\n0.109\n0.0531\n0.1864\n0.1173\n0.1154\n0.0723\nOurs\n0.084\n0.0419\n0.1141\n0.0958\n0.1327\n0.0372\nOurs-Lite\n0.097\n0.0532\n0.1359\n0.0989\n0.1487\n0.0492\n17\nDynamic 3DGS\nOurs\nOurs-Lite\nMixVoxels-L\nNeRFPlayer\nGT\nHyperReel\nK-Planes\nDynamic 3DGS\nOurs\nOurs-Lite\nMixVoxels-L\nNeRFPlayer\nGT\nHyperReel\nK-Planes\nFigure 8. Qualitative comparisons on the Neural 3D Video Dataset [45]. To be continued in the next page.\n18\nDynamic 3DGS\nOurs\nOurs-Lite\nMixVoxels-L\nNeRFPlayer\nGT\nHyperReel\nK-Planes\nDynamic 3DGS\nOurs\nOurs-Lite\nMixVoxels-L\nNeRFPlayer\nGT\nHyperReel\nK-Planes\nFigure 9. Qualitative comparisons on the Neural 3D Video Dataset [45]. To be continued in the next page.\n19\nDynamic 3DGS\nOurs\nOurs-Lite\nMixVoxels-L\nNeRFPlayer\nGT\nHyperReel\nK-Planes\nFigure 10. Qualitative comparisons on the Neural 3D Video Dataset [45].\n20\nOurs\nOurs-Lite\nNeRFPlayer\nGT\nHyperReel\nFigure 11. Qualitative comparisons on the Google Immersive Dataset [10].\n21\nOurs-Lite\nHyperReel\nGT\nOurs\nFigure 12. Qualitative comparisons on the Technicolor Dataset [71].\n22\n"
  },
  {
    "title": "The LLM Surgeon",
    "link": "https://arxiv.org/pdf/2312.17244.pdf",
    "upvote": "8",
    "text": "THE LLM SURGEON\nTycho F.A. van der Ouderaa1\u2217, Markus Nagel2, Mart van Baalen2,\nYuki M. Asano3, Tijmen Blankevoort2\n1Imperial College London , 2Qualcomm AI Research\u2020, 3QUVA Lab, University of Amsterdam\nABSTRACT\nState-of-the-art language models are becoming increasingly large in an effort to\nachieve the highest performance on large corpora of available textual data. How-\never, the sheer size of the Transformer architectures makes it difficult to deploy\nmodels within computational, environmental or device-specific constraints. We\nexplore data-driven compression of existing pretrained models as an alternative\nto training smaller models from scratch. To do so, we scale Kronecker-factored\ncurvature approximations of the target loss landscape to large language models.\nIn doing so, we can compute both the dynamic allocation of structures that can\nbe removed as well as updates of remaining weights that account for the removal.\nWe provide a general framework for unstructured, semi-structured and structured\npruning and improve upon weight updates to capture more correlations between\nweights, while remaining computationally efficient. Experimentally, our method\ncan prune rows and columns from a range of OPT models and Llamav2-7B by\n20%-30%, with a negligible loss in performance, and achieve state-of-the-art re-\nsults in unstructured and semi-structured pruning of large language models.\n1\nINTRODUCTION\nRecent advancements in language modeling (Vaswani et al., 2017) allow fitting large language mod-\nels (LLMs) with millions or even billions of parameters (such as OPT (Zhang et al., 2022) and\nLlama 2 (Touvron et al., 2023)) on big text corpora achieving high performance. Unfortunately,\nthe size of these LLMs often makes it hard to deploy them within practical constraints. Cloud-\nbased deployment can get very expensive for larger models, and efficient devices such as phones are\nfrequently limited in the memory size to host a model.\nA body of literature extending back to the late 1980s, e.g., Optimal Brain Damage (OBD, LeCun\net al. (1989)) and Optimal Brain Surgeon (OBS, Hassibi & Stork (1992)), phrases pruning as a con-\nstraint optimization problem to reduce a model\u2019s footprint and runtime requirements. The Hessian\nrequired for this approach grows with the square of the number of parameters, and can only be com-\nputed in practice for unrealistically small networks. To overcome this issue, Eigendamage (Wang\net al., 2019) introduces a Kronecker factorization of a blockwise-diagonal approximation of the\nHessian. Recent works, like Optimal Brain Compression (Frantar & Alistarh, 2022) and SparseGPT\n(Frantar & Alistarh, 2023), demonstrate practical post-training pruning of LLMs, but only consider\na loss curvature of a pruned layer\u2019s squared output reconstruction error, ignoring gradients that relate\nlocal removal costs to the target loss. As a result, their approximation to the target loss landscape\n\u2217Work done while doing an internship at Qualcomm AI Research\n\u2020Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\nStructured compression (rows and columns)\nUnstructured compression (matrix elements)\n1.3b\n2.7b\n6.7b\nmodel sizes:\n14.62\n12.47\n10.86\ntest set\nperplexity\npretrained\nK-OBD\nLLM Surgeon\n1.3b\n2.7b\n6.7b\nmodel sizes:\n14.62\n12.47\n10.86\ntest set\nperplexity\npretrained\nSparseGPT\nLLM Surgeon\nFigure 1: LLM Surgeon allows interpolation of model size between existing pretrained models.\n1\narXiv:2312.17244v1  [cs.LG]  28 Dec 2023\nis inaccurate, leading to a significant performance degradation for pruned LLMs. Further, these\nmethods do not readily extend to structured pruning.\nThis work introduces LLM Surgeon, a general framework for unstructured, semi-structured and\nstructured pruning of LLMs. To the best of our knowledge, this is the first method to successfully\nperform structured pruning for LLMs. This is achieved by scaling up the block-diagonal Kronecker-\nfactorized approximations to the empirical Fisher from Eigendamage to LLMs. We further expand\nupon the work by deriving OBS-like weight pruning costs and updates for structured pruning of\nmultiple rows and columns, and provide a general framework that also incorporates semi-structured\nand unstructured pruning. Instead of treating individual weight updates independently, we strive\nto consider as many correlations between weights as practically possible and derive joint weight\nupdates for pruning multiple weights (or multiple sets of structured weights) at once. Unlike prior\nwork in LLM pruning, LLM Surgeon prunes in multiple shots, updating weights and curvature es-\ntimates between shots. We use global thresholding for unstructured, semi-structured and structured,\ni.e., instead of pruning layers by a fixed amount, more sensitive layers are pruned less than those\nthat are more robust. Lastly, we propose to mitigate possible first-order gradients not being zero by\nusing optional low-rank first-order updates between shots. A key advantage of LLM Surgeon is that\nit allows trading off additional compute during compression for better accuracy by increasing the\nnumber of correlations and/or shots. Our method gives the first practically usable results for struc-\ntured pruning of LLMs \u2013 they can be pruned by up to 30% with minor performance degradation.\nFurthermore, we achieve state-of-the-art results in unstructured and semi-structured LLM pruning.\n2\nBACKGROUND AND RELATED WORK\nNeural network pruning aims to remove parameters from a model while minimizing negative im-\npact on final performance. More formally, we denote the P model parameters as vector \u03b8\u2217 =\nvec(W \u2217\n1 , W \u2217\n2 , . . . W \u2217\nL) \u2208 RP , by flattening the L weight matrices of attention and fully-connected\nblocks, with already fitted \u03b8\u2217\u2248 arg min\u03b8 L(\u03b8) to data D to minimise a negative likelihood loss\nL(\u03b8)= \u2212 log p(\u03b8|D). To compress the model, we are looking for a pruned vector \u02c6\u03b8:\n\u02c6\u03b8 = arg min\u03b8 L(\u03b8) s.t. pruning constraints based on \u03b8\u2217\n(1)\nwhere chosen constraints determine the structure of compressed weights \u02c6\u03b8. In unstructured prun-\ning, a fraction of total weight elements is set to zero. In semi-structured pruning of M:N we have\nthat M weights of every N consecutive weights are zero (Zhou et al., 2021; Hubara et al., 2021). And\nin structured pruning (Louizos et al., 2017), entire rows and columns are set to zero. Structured\npruning leads to the most immediate gains in memory and compute, as it directly reduces the dimen-\nsions of matrices that need to be represented explicitly, but is typically regarded as a more difficult\ncompression task. Maintaining high performance is often easier in the other schemes, but requires\nspecialised arithmetic exploiting the sparsity structure to benefit at deployment. We consider all\npruning types above, with a focus on structured pruning for LLMs.\nTypically, eq. (1) can not be solved directly, as the space of possible pruning configurations exceeds\nwhat can be evaluated in practice. To illustrate, a search over all possible unstructured pruning\nmasks of a 125 million parameter LLM would require 2P =2125m\u22481037628749 evaluations. The\nidea, therefore, is to find \u02c6\u03b8 using a surrogate of the loss landscape q that is easier to work with:\nL(\u03b8) = \u2212 log p(D | \u03b8) \u2248 \u2212 log q(\u03b8)\n(2)\nIf one chooses a particular Gaussian form for our surrogate q, then solutions for unstructured, semi-\nstructured, and structured pruning constraints can be derived in closed-form (appendix A).\n2.1\nTAYLOR EXPANSION\nHow do we obtain a good surrogate of the loss q? One of the easiest approaches is to locally expand\nthe log loss through a second-order Taylor expansion around the pretrained weights \u03b8\u2217, yielding:\n\u2212 log q(\u03b8) \u2248 \u2212 log p(D|\u03b8\u2217) \u2212 (\u03b8 \u2212 \u03b8\u2217)T \u2207L(\u03b8\u2217) \u2212 1\n2(\u03b8 \u2212 \u03b8\u2217)T H\u03b8\u2217(\u03b8 \u2212 \u03b8\u2217)\n(3)\nwhere [\u2207L(\u03b8\u2217)]i =\n\u2202\n\u2202\u03b8i L(\u03b8\u2217\ni ) denotes the Jacobian and [H\u03b8]ij =\n\u22022\n\u2202\u03b8i\u03b8j L(\u03b8ij) denotes the Hessian.\nThe first-order term vanishes [\u2207L(\u03b8\u2217)]i = 0 at the optimum. Note that in practice the first order\n2\nFigure 2: Pruning as equality constrained optimization of quadratic approximation of the loss land-\nscape (left), or equivalently, maximising the likelihood under a Laplace approximation (right).\nterm may not vanish. While we follow this assumption initially, we consider interleaved first-order\ncorrections to mitigate the issue in section 3.6. The quadratic expansion of eq. (3) forms the basis of\nthe optimal brain damage (LeCun et al., 1989) and optimal brain surgeon (Hassibi & Stork, 1992)\npruning methods. Note that from a probabilistic perspective, a quadratic approximation of the log\nlikelihood implies a Gaussian approximation of the likelihood, as also observed by (Wang et al.,\n2019) and illustrated in fig. 2. This is well-known (Bishop & Nasrabadi, 2006), (MacKay, 2003)\nas the Laplace approximation q(\u03b8) = N(\u03b8 | \u03b8\u2217 + \u2207L(\u03b8\u2217), H\u22121\n\u03b8\u2217 ), with pretrained weights are the\nmean and the local inverse Hessian is the covariance matrix capturing correlations between weights.\n2.2\nBLOCK FISHER INFORMATION MATRIX\nFor a network trained with negative log-likehood loss, the Hessian is identical to the Fisher matrix:\nH\u03b8 = F\u03b8 =\nXN\nn=1 Ey\u223cp\u03b8(y|xn)\n\u0002\n\u2207\u03b8 log p\u03b8(y|xn)\u2207\u03b8 log p\u03b8(y|xn)T \u0003\n(4)\nwhich has the benefit of always being positive semi-definite, with the inverse thus forming a proper\ncovariance matrix for q, and can be approximated with Monte Carlo samples of p\u03b8(y|xn). For\nmost LLMs, this would be treating the softmax output of the network as categorical distribution\np\u03b8(y|xn), and sampling from that. In practice, we use the \u2018empirical Fisher\u2019 replacing the expec-\ntation over y with target data yn (Kunstner et al., 2019). The full (empirical) Fisher F\u03b8 \u2208 RP \u00d7P\nscales quadratically in the number of parameters P. To overcome this, the Fisher is often written in\nterms of layer-wise blocks Flk = PN\nn=1 E\n\u0002\nvec(\u2207Wl log p\u03b8(y|xn))vec(\u2207Wk log p\u03b8(y|xn))T \u0003\n, and\napproximated by only treating layers independently (Martens & Grosse, 2015; Botev et al., 2017):\nF\u03b8 = diag(F11, F22, . . . , FLL),\nFl =\nXN\nn=1 E\nh\n(gl,ngT\nl,n) \u2297 (al,naT\nl,n)\n|\n{z\n}\nRC\u00d7RC\ni\n(5)\nwhere \u2297 denotes the Kronecker product and vec(\u00b7) the matrix vectorisation operation. Because\nwe disregard cross-layer interactions we write Fl instead of Fll for Fisher blocks associated with\nthe weight matrix Wl\u2208RR\u00d7C producing outputs yl,n = Wlal,n\u2208RR from inputs al,n\u2208RC, for\neach layer l and datapoint n. Consequently, we can compute Fisher blocks from input activations\nal,n\u2208RC of forward-passed data xn and output gradients gl,n=\u2207yl,nL\u2208RR from backpropagation.\n2.3\nPRUNING AS CONSTRAINED OPTIMIZATION\nOptimal brain surgery relies on removing and adapting weights such that the loss is least negatively\naffected, thus it behooves us to write the problem as a constrained optimization problem. From\nthe Gaussian approximation discussed in section 2.1 obtained by quadratically expanding the log\nlikelihood loss \u2212 log p\u2248 1\n2\u03b8T F \u03b8, the optimal update \u2206\u03b8=\u02c6\u03b8\u2212\u03b8 (and thus also \u02c6\u03b8=\u03b8+\u2206\u03b8) becomes\nthe following equality constrained quadratic optimization problem (Hassibi & Stork, 1992):\narg min\n\u2206\u03b8\n1\n2\u2206\u03b8T F \u2206\u03b8\n(6)\ns.t. eT\nk \u2206\u03b8 + eT\nk \u03b8 = 0, \u2200k \u2208 K\nwhere F is positive semi-definite and K is the set of K indices that are pruned (i.e., set to zero).\n3\nAlgorithm 1 LLM Surgeon (structured)\nInput: initial weights \u03b80, target size \u03b1, and data D\nFor shot t in [1, 2, ..., T]\nCompute: approximate curvature G, A from data D\n\u25b7 section 3.1\nCompute: costs per row/column Lr, Lc from G, A\n\u25b7 section 3.2\nCompute: threshold \u03c4 using Lr and Lc given target size \u03b1t\n\u25b7 section 3.3\nSelect: rows and columns to remove ER, EC based on \u03c4\n\u25b7 section 3.3\nCompute: weight update \u2206\u03b8t\u22121 based on ER, EC and G, A\n\u25b7 section 3.4\nUpdate: remaining weights \u03b8t \u2190 \u03b8t\u22121 + \u2206\u03b8t\u22121\n\u25b7 section 3.5\nOptionally: \u03b8t \u2190 low-rank update(\u03b8t)\n\u25b7 section 3.6\nOutput: compressed weights \u02c6\u03b8 = \u03b8T\nGeneral solution\nWe denote EK = [e1\ne2\n. . .\neK]T \u2208 [0, 1]K\u00d7P as a matrix of which the\nrow vectors are canonical basis vectors ek \u2208 RP that select the elements to be pruned. One of the\nmost standard approaches to solve eq. (6) is using Langrange multipliers, which results in a general\nclosed-form solution for the expected increase in loss L and optimal weight update \u2206\u03b8:\nL = 1\n2(EK\u03b8\u2217)T \u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\n(7)\n\u2206\u03b8 = \u2212F \u22121ET\nK\n\u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\n(8)\nwhich we use to derive unstructured, semi-structured, structured for modern Fisher approximations\n(see appendices A.2 to A.4). The same general form of eqs. (7) and (8) appears in prior LLM pruning\nwork Kurtic et al. (2022), but only for much simpler layer-wise pruning and no structured pruning.\n3\nLLM SURGEON\nThis section describes the components of our method, LLM Surgeon, summarised in algorithm 1.\n3.1\nESTIMATING LOSS LANDSCAPE CURVATURE\nAccurate pruning relies on approximating the local curvature accurately while overcoming the mem-\nory cost associated with storing the true curvature. Specifically, even with the block-wise approxima-\ntion of eq. (5), F \u2208 RRC\u00d7RC requires summing N large RC \u00d7RC matrices, too large to practically\nfit in memory. Instead, we adapt the KFAC approximation (Martens & Grosse, 2015) that assumes\nindependence of activations and derivatives, approximating an expectation of Kronecker products\nas a Kronecker product of two expectations E[gl,ngT\nl,n \u2297 al,naT\nl,n] \u2248 E[gl,ngT\nl,n] \u2297 E[al,naT\nl,n],\nallowing layer-wise Fisher blocks to be approximated as Fl \u2248 eFl, where\neFl = Gl \u2297 Al\n, with Gl =\n1\n\u221a\nN\nXN\nn=1 gl,ngT\nl,n and Al =\n1\n\u221a\nN\nXN\nn=1 al,naT\nl,n\n(9)\nconstructed from activations al,n \u2208 RC from forward passes and gradients gl,n \u2208 RR from back-\nward passes. The approximation was originally proposed for optimization, but has recently gained\npopularity for other problems that require curvature approximations (Immer et al., 2022), including\nstructured pruning in Wang et al. (2019).\nAn additional advantage of approximating Fisher blocks as Kronecker products is that the inverse\nbecomes particularly easy to compute eF \u22121 = G\u22121 \u2297 A\u22121, thus only requires inverting the factors.\nThis fact allows us to never explicitly construct large RC\u00d7RC matrices in memory that make up eF\nand eF \u22121, but rather directly work with the much smaller matrices G and A.\n3.2\nCOMPUTING COSTS IN FINAL LOSS\nThe number of possible combinations in which weights can be removed grows (supra-)exponentially\nin parameter count, making it infeasible to estimate a separate cost L for each such removal. A\ncommon strategy, therefore, is to treat weights independently when computing removal costs L. We\nalso follow this strategy, but note that this does not necessarily imply that we have to make such same\nstrong independence assumption for the weight updates \u2206\u03b8 after selecting weights to be removed.\n4\nUnlike most prior work, we present correlated weight updates by taking into account off-diagonal\nelements of the Fisher approximation in section 3.4.\nFor semi-structured and unstructured we use independent costs for individual weight elements\nk\u2208[1, RC], and for structured use independent costs for all rows r\u2208[1, R] and columns c\u2208[1, C].\nWe find that we can derive the appropriate costs from the general cost formula eq. (7) by letting\nE=ek \u2208 RRC where the single one-hot element at index k of canonical basis vector ek selects\nthe weight to remove. For structured pruning, we similarly select rows r and columns c, by setting\nE=eT\nr \u2297I\u2208RC\u00d7RC or E=I\u2297ec\u2208RR\u00d7RC with er\u2208RR, ec\u2208RC. Plugging into eq. (7), we find:\nLk = 1\n2\n(\u03b8k)2\n[G\u22121 \u2297 A\u22121]kk\n,\nLr = 1\n2\n\u03b8T\nr A\u03b8r\n[G\u22121]rr\n,\nLc = 1\n2\n\u03b8T\nc G\u03b8c\n[A\u22121]cc\n(10)\nFull derivations can be found in appendices A.2 and A.3. The costs for single elements Lk are\nequivalent to those found in optimal brain surgeon (Hassibi & Stork, 1992) and Lr and Lc closely\nresemble structured brain surgeon of (Wang et al., 2019), but in our case derived for matrix rows\nand columns (see appendix A.3). Given curvature estimates, costs for either removing all weights\nor all rows and columns can be computed in parallel. In addition, we derive costs for the more\ngeneral sum of Kronecker factor approximation eF \u2248 G1 \u2297 A1 + G2 \u2297 A2 in appendix I through\nan eigendecomposition.\n3.3\nDYNAMIC WEIGHT ALLOCATION WITH GLOBAL THRESHOLD\noriginal W\nrow cost \nr, \ncolumn cost \nc\nstructured\nW +\nW\nelement costs \nk\nsemi-structured\nW +\nW\nelement costs \nk\nunstructured\nW +\nW\nPruned weights\nUpdated remaining weights\nFigure 3: General framework for structured, semi-\nstructured and unstructured compression.\nUnlike prior works that compress layer-by-\nlayer (Frantar & Alistarh, 2023), we use a\nglobal threshold \u03c4 enabling a dynamic alloca-\ntion of sparsity levels across layers, pruning\nmost where it hurts the least. Our method can\ncompress a model to a specifically chosen target\nsize \u03b1, defined as the fraction of weights that\nshould remain, i.e. stay non-zero after com-\npression. In all structured, semi-structured, and\nunstructured pruning (fig. 3), we select as many\nweights for removal so that the target size \u03b1 is reached that inflict the least possible costs L, as com-\nputed according to section 3.2. For unstructured pruning, this is as simple as sorting the costs for all\nweights Lk in the network and setting a global threshold \u03c4 such that \u03b1 fraction of weights fall within\nthe threshold Lk \u2264 \u03c4. For M:N semi-structured pruning, we sort the M costs of each N consecutive\nweights and select the M weights with lowest cost. In case of a multi shot schedule (see section 3.5)\nwe also sum the M lowest costs in each block to find a cost per block, sort costs per block across the\nentire network, and similar to the unstructured case set a global threshold \u03c4 such that an \u03b1 fraction\nof weights fall within threshold. Lastly for structured pruning, we perform a sorting appropriately\nweighted by the number of elements that make up a row or column and set the global threshold \u03c4\nsuch that \u03b1 fraction of all weights fall within the threshold. Then we remove all rows and columns\nthat fall within the threshold Lr, Lc \u2264 \u03c4.\n3.4\nCORRELATED WEIGHT UPDATES\nLike most other pruning methods, we prune multiple weights at once (Frantar & Alistarh, 2023;\nWang et al., 2019). To arrive at pruning costs and weight updates for pruning multiple weights, it\nis common to compute costs and updates for individual weights (or sets of weights) independently\nand add them together to arrive at a joint pruning cost. In LLM Surgeon, we argue that it\u2019s better\nto consider weight updates jointly instead of independently. After selecting the set of weights for\npruning, we can often afford to compute a single correlated weight update associated to the joint\nremoval of multiple weights, instead of naively summing weight updates associated to individual\nremovals. We derive such correlated weight updates below. Note that, for the expected cost compu-\ntation, we do assume that the row, column or weight costs are independent, as the number of possible\ncombinations of weights to prune grows too large to compute within reasonable time.\nFast unstructured / semi-structured correlated weight updates\nMathematically, we represent\npruned weights as EK= [e1\ne2\n. . .\neR\u2032]T \u2208RK\u00d7RS, where er\u2208RR\u2032 are one-hot canonical ba-\n5\nsis vectors selecting the weights for removal. As each element k has a unique associated row r\nand column c index, we can consequently also use canonical basis vectors for these respective rows\nER\u2208RK\u00d7R and columns EC\u2208RK\u00d7C (i.e., we have [ER]i \u2297 [EC]i=[EK]i is satisfied for all i).\nWe derive unstructured weight updates in appendix A.2, by considering eigendecompositions G =\nK1S1KT\n1 , A = K2S2K2 of the Fisher approximation F \u2248 G \u2297 A, which from eq. (8) yields:\n\u2206W = G\u22121\u0010\nK1\n\u0010\nK\nT\n1W\n\u22121K2 \u2298 S\n|\n{z\n}\nK\u00d7K\n\u0011\u22121\nK2\n\u0011\nA\u22121\n(11)\nwhere \u2298 is element-wise division, and for brevity use bar notation K1=EKK1, K2=EKK2,\n\u03b8=EK\u03b8, and S=diag(S1)diag(S2)T \u2208RR\u00d7C, and diag(\u00b7) vectorises matrix diagonals.\nProgrammatically, we always avoid explicitly representing large matrices eF and eF \u22121 in memory,\nbut rather compute relevant quantities from their factors. Likewise, we never represent sparse ma-\ntrices EK, ER or EC in memory, but instead work with a lists of indices of the one-hot elements\ndirectly. For example, we can cheaply construct K1=ERK1 \u2208 RK\u00d7R and K2=ECK2 \u2208 RK\u00d7C,\nby copying row vectors, and the vector\u03b8=EK\u03b8=ERW ET\nC \u2208 RK by indexing all pruned weights.\nMaximum number of correlated weights\nThe main computational bottleneck is the K\u00d7K ma-\ntrix inverse in eq. (11). To control compression speed, we can split pruned weights into disjoint\nsubsets K=K1\u222aK2\u222a . . ., such that each subset Ki does not exceed the set maximum number of\ncorrelated weights Ki\u2264m, and sum associated independent updates. Using less correlation by set-\nting a lower m allows trading compression quality for speed.\nFast structured correlated weight updates\nUnlike the general case which requires inverting a\nK \u00d7 K matrix for K correlated weights, we find that weight updates with the Kronecker factored\nFisher approximation \u02dcF = G \u2297 A only require inverting a R\u2032 \u00d7 R\u2032 matrix when removing R\u2032 rows\nor a C\u2032 \u00d7C\u2032 matrix when removing C\u2032 columns. The updates are much cheaper than we would have\nexpected based on the effective number of weights in those rows and columns, which would imply\ninverting R\u2032C \u00d7 R\u2032C or RC\u2032 \u00d7 RC\u2032 matrices. In practice, this leads to a significant speed-up for\nstructured pruning and weight updates that take into account correlations between rows or columns.\nWhen removing R\u2032 rows, r1, r2, . . . rR\u2032, or the C\u2032 columns c1, c2, . . . , cC\u2032, with 1<R\u2032 < R and\n1<C\u2032<C, we denote one-hot vectors selecting all rows and columns to be removed respectively\nas ER\u2032 = [e1\ne2\n. . .\neR\u2032]T \u2208 RR\u2032\u00d7R and EC\u2032 = [e1\ne2\n. . .\neC\u2032]T \u2208 RC\u2032\u00d7C. We find\nweight updates associated to removing the R\u2032 rows by setting EK = ER\u2032 \u2297 I or EK = I \u2297 EC\u2032:\nremove multiple R\u2032 rows:\nremove multiple C\u2032 columns:\n\u2206W = \u2212W(EC\u2032A\u22121ET\nC\u2032)\u22121(A\u22121ET\nC\u2032)\n\u2206W = \u2212G\u22121ET\nR\u2032(ER\u2032G\u22121ET\nR\u2032)\u22121W\n(12)\nFrom here, it is clear that the special case of removing a single row r or column c under Kronecker\napproximation involves inverting a 1 \u00d7 1 matrix, and thus only requires scalar division:\nremove single row r:\n\f\f\f\u2206\u03b8 = \u2212G\u22121er \u2297 \u03b8r\n[G\u22121]rr\n, or single column c:\n\f\f\f\u2206\u03b8 = \u2212\u03b8c \u2297 A\u22121ec\n[A\u22121]cc\n(13)\nin accordance to independent structured updates in Wang et al. (2019), for convolutional filters. We\nhave thus extended existing structured weight updates to rows and columns, and derived update rules\nthat also consider correlation between structured groups (in our case the rows and columns).\n3.5\nMULTI SHOT PRUNING SCHEDULE\nTo improve the performance-to-sparsity ratio, we propose pruning in multiple shots. We theoreti-\ncally justify this multi-shot approach by noting that the surrogate loss landscape q relies on a Taylor\nexpansion (eq. (3)) that only holds locally and thus becomes unreliable for larger jumps \u2206\u03b8 in pa-\nrameter space. We mitigate this by pruning in multiple T>1 shots, t \u2208 [1, 2, . . . , T], each resulting\nin a smaller weight update \u2206\u03b8 after which the curvature of the loss surface can be re-estimated.\nWhen pruning to target size \u03b1, ie. removing 1\u2212\u03b1 of total weights, we choose a schedule \u03b1t starting\nat \u03b10 = 1 and ends with \u03b1T =\u03b1, such that after T shots, exactly \u03b1 fraction of the total weight remain.\nEmpirically, we find that a linear schedule for \u03b1t, as formulated in section 4, monotonically improves\n6\nTable 1: Structured compression of large language models on wikitext-2 data.\nTest performance (PPL)\nMethod\nTarget size\nOPT (125m)\nOPT (1.3b)\nOPT (2.7b)\nOPT (6.7b)\nLlama-v2 (7b)\nBaseline\n100%\n27.65\n14.62\n12.47\n10.86\n5.12\nMagnitude\n90%\n767.2\n894.4\n1229\n3464\n36746\nI \u2297 I\n80%\n4685\n(1278)\n2788\n16747\n347960\n70%\n17970\n(3098)\n9255\n17312\n41373\nL-OBD\n90%\n33.3\n20.76\n17.69\n27.20\n14259\ndiag(I \u2297 A)\n80%\n94.14\n1392\n3236\n7570\n15630\nmulti shot\n70%\n545.6\n2147\n7233\n7628\n21386\nK-OBD\n90%\n27.97\n14.68\n11.96\n10.53\n5.48\ndiag(G \u2297 A)\n80%\n29.89\n15.63\n12.47\n11.28\n9.14\nmulti shot\n70%\n36.54\n18.29\n14.53\n13.03\n15.43\n60%\n47.54\n24.65\n18.09\n16.21\n28.03\n50%\n75.95\n37.68\n26.68\n25.54\n46.64\nLLM Surgeon (ours)\n90%\n28.29\n14.73\n12.00\n10.82\n5.43\nG \u2297 A\n80%\n29.37\n15.27\n12.37\n11.22\n7.29\nwithin row/col cor. \u2206\n70%\n32.46\n16.60\n13.16\n11.83\n10.85\n60%\n39.82\n19.40\n14.79\n12.94\n16.67\n50%\n51.48\n23.81\n18.01\n15.38\n25.62\nLLM Surgeon (ours)\n90%\n28.01\n14.70\n12.02\n10.77\n5.25\nG \u2297 A\n80%\n28.73\n15.12\n12.27\n11.02\n6.18\nfull cor. \u2206\n70%\n31.82\n16.24\n12.92\n11.64\n7.83\n60%\n38.47\n18.45\n14.23\n12.58\n10.39\n50%\n49.78\n22.95\n17.15\n14.90\n15.38\npruning performance with more shots, and that higher sparsity levels typically require more shots\n(see appendix F.1). Multi-shot pruning allows one to spend (linearly in T) more computation to\nimprove the final compression performance.\n3.6\nINTERLEAVED LOW-RANK FIRST-ORDER CORRECTIONS\nWe propose optional interleaved low-rank first-order corrections to further improve compression\nperformance. So far, we assumed parameters are in a local optimum when finding a closed-form\nsolution to the quadratic constraint problem. In practice, however, this assumption likely does not\nhold since (i) the neural network may not be optimised to the minimum, (ii) a different loss may\nbe used for compression than used for training, or (iii) we prune in multiple shots (section 3.5)\ninevitably causing weights to diverge from the optimum. To mitigate this, we consider first-order\ncorrections by interleaving pruning shots with low-rank adaptations of weights Wl+UV (LoRA, by\n(Hu et al., 2021)), commonly used in LLM finetuning. We always absorb updates after each shot, so\nthat the next loss estimate q is closer to the optimum and underlying assumptions are likely to hold\nmore closely. By absorbing LoRA updates between shots, the sum of low-rank updates can have\na higher rank than individual updates. That is, we have rank(U 1V 1+U 2V 2+ . . . +U T V T ) \u2265\nrank(U tV t) for the updates U tV t at any shot t, with equality only arising if updates lie exactly in\nthe same subspace which is unlikely to ever occur in practice. This insight could also be used during\nregular LoRA finetuning and may therefore be useful outside the context of model compression to\nallow more expressive low-rank model adaptation, at negligible cost.\n4\nRESULTS\nWe compare compression performance of LLM Surgeon on language modeling tasks on OPT\n(Zhang et al., 2022) and Llama-v2 (Touvron et al., 2023) model families, using data from wikitext-2\ndataset (appendix B.2). For compression, we use 128 sequences with a sequence length of 2048\ntokens from the training data set and evaluate test perplexity (PPL) on the standard test split. In our\nexperiments, we use a linear sparsity schedule \u03b1t=1\u2212t( 1\u2212\u03b1\nT ) at each shot s before reaching the final\nsparsity \u03b1. We use 40 shots at \u03b1=0.5 sparsity and report intermediate compression rates, effectively\nusing T=8 shots for \u03b1=0.9, T=16 for \u03b1=0.8, T=24 for \u03b1=0.7, and T=32 for \u03b1=0.6. We com-\npare against magnitude pruning, L-OBD, SparseGPT and K-OBD baselines. The K-OBD and LLM\nSurgeon use the multi shot procedure of section 3.5 using T=40 shots for structured pruning and\nT=5 shots for semistructured and unstructured pruning. Further details are found in appendix B.\n7\n4.1\nSTRUCTURED COMPRESSION\nStructured compression of rows and columns enables direct savings in memory and compute through\na straight reduction of matrix dimensions in the model. For LLM surgeon, we consider in section 3.4\nweight updates with different levels of correlations: limited to correlations within rows and columns,\nand correlations both within and between rows and columns. We further compare against magnitude\npruning, which only uses weight magnitudes, L-OBD, which only uses activations, and K-OBD,\nwhich also uses Kronecker-factored curvature but assumes full independence and thus only prunes\nwithout updating remaining weights. We report results in table 1, and observe that more correlations\nresults in better performance, with the largest improvements for the Llama-v2 model family.\nWhile a 50% structured compression is not better than a smaller model of similar size, LLM Surgeon\nallows us to reduce model size by up to 30% with minimal loss, without training a smaller model\nfrom scratch fig. 1. In our structured compression experiments our proposed LLM Surgeon method\noutperforms all baselines and achieves the best performance for each compression target size.\n4.2\nINTERLEAVED LOW-RANK UPDATES\nTable 2: Structured compression of OPT-125m\non wikitext-2 using interleaved LoRA updates\nTarget\nwithout\nwith\nSize\nLoRA\nLoRA\nPretrained\n100%\n27.65\n23.35\nLLM Surgeon\n90%\n28.01\n24.16\n(ours)\n80%\n28.73\n25.25\nG \u2297 A\n70%\n31.82\n28.86\nfull cor. \u2206\n60%\n38.47\n31.26\n50%\n49.78\n36.50\nAdditionally, we assess compression performance\nin conjunction with the proposed first-order cor-\nrections using the interleaved low-rank adaptation\ndescribed in section 3.6. We find that LoRA im-\nproves compression performance in the smallest\n125m model, but not in larger models. We hypoth-\nesise that larger models are more prone to overfit-\nting on the relatively few batches of wikitext-2 data\nused to compress the model. Nevertheless, we con-\nclude that interleaved LoRA can be useful in cases,\nand recommend first using the proposed method without interleaved updates and, if enough data is\navailable for compression, optionally using it if it improves performance.\n4.3\nSEMI-STRUCTURED COMPRESSION\nFor 2:4 semi-structured pruning, we compare LLM Surgeon with magnitude pruning, which only\nuses weight magnitudes, single-shot L-OBD, which only uses activations, and single-shot K-OBD,\nwhich also uses Kronecker-factored curvature but assumes full independence and thus only prunes\nwithout updating remaining weights as well as the recent state-of-the-art SparseGPT (Frantar & Alis-\ntarh, 2023). We report test performance after 50 % (2:4) semi-structured compression on wikitext-2\ndata in table 3. We empirically find that considering more weight correlations results in improved\nfinal performance after compression. Our proposed LLM Surgeon is competitive with prior work\noutperforming all baselines in terms of test set perplexity (PPL).\nTable 3: Semi-structured 2:4 compression for large language models on wikitext-2 data.\nTarget\nTest performance (PPL)\nMethod\nF \u2248\nsize\nOPT (125m)\nOPT (1.3b)\nOPT (2.7b)\nOPT (6.7b)\nBaseline\n100%\n27.65\n14.62\n12.47\n10.86\nMagnitude\nI \u2297 I\n50%\n342.04\n379.57\n1106.01\n187.29\nL-OBD\ndiag(I \u2297 A)\n50%\n87.26\n44.92\n41.40\n27.36\nK-OBD\ndiag(G \u2297 A)\n50%\n68.74\n27.22\n20.23\n15.55\nSparseGPT\nI \u2297 A\n50%\n45.51\n29.44\n14.92\n13.01\nLLM Surgeon (ours)\nG \u2297 A\n50%\n44.64\n25.10\n14.64\n12.10\n4.4\nUNSTRUCTURED COMPRESSION\nFor unstructured pruning, we repeat the same experiments as structured pruning case described in\nsection 4.1. In table 4, we report final test performance in terms of perplexity (PPL) on wikitext-\n2 after compressing LLMs of different sizes of OPT and Llama-v2 family. Overall, we find that\nmethods with more accurate approximations of the curvature landscape and that account for more\ncorrelations perform better. The proposed LLM Surgeon outperforms all baselines, reaching the\nhighest test performance across target sizes.\n8\nTable 4: Unstructured compression of large language models on wikitext-2 data.\nTarget\nTest performance (PPL)\nMethod\nsize\nOPT (125m)\nOPT (1.3b)\nOPT (2.7b)\nOPT (6.7b)\nLlama-v2 (7b)\nBaseline\n100%\n27.65\n14.62\n12.47\n10.86\n5.12\nMagnitude\n90%\n27.62\n14.69\n12.60\n10.88\n5.18\nI \u2297 I\n80%\n28.53\n15.68\n13.18\n11.26\n5.37\n70%\n52.88\n140.2\n15.22\n12.22\n6.03\nL-OBD\n90%\n29.70\n16.24\n14.44\n13.43\n6.09\ndiag(I \u2297 A)\n80%\n32.18\n21.92\n23.35\n39.85\n116.2\nsingle shot\n70%\n49.08\n204.7\n274.8\n810.4\n6549\nK-OBD\n90%\n27.64\n14.62\n12.09\n36.89\n5.13\nG \u2297 A\n80%\n27.62\n14.37\n130220\n39928\n5.19\nsingle shot\n70%\n27.92\n220.1\n23097\n19506\n5.60\n60%\n29.24\n13783\n10331\n33896\n9.20\n50%\n34.43\n7311\n10495\n91506\n118.6\nSparseGPT\n90%\n27.93\n14.69\n12.00\n10.86\n5.49\nI \u2297 A\n80%\n28.18\n15.07\n12.05\n10.86\n5.58\n70%\n28.93\n22.77\n12.17\n10.89\n5.71\n60%\n30.20\n25.07\n12.37\n10.98\n5.94\n50%\n33.17\n26.77\n12.88\n11.92\n6.51\nLLM Surgeon (ours)\n90%\n27.69\n14.62\n12.01\n10.86\n5.13\nG1 \u2297 A1\n80%\n27.83\n14.66\n12.14\n10.87\n5.20\nfull cor. \u2206\n70%\n28.35\n14.81\n12.25\n10.82\n5.36\nmulti shot\n60%\n28.98\n14.91\n12.28\n10.83\n5.66\n50%\n30.30\n15.47\n12.68\n10.97\n6.08\n4.5\nLEARNED SPARSITY STRUCTURE\nThe proposed method can dynamically allocate sparsity across layers through global thresholds\ndescribed in section 3.3. In Fig. 4.5, we compare total allocated sparsity levels per layer depth and\nper layer type after compressing a pretrained OPT-125m model. We find that the LLM Surgeon\nprunes relatively more in the first layer and less in middle layers. Further, we observe that a larger\nportions of weights are removed in fully-connected compared to attention blocks, but deviations are\nless compared to other methods. Dynamic allocation allows for most pruning where it hurts least.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12\nlayer\n0%\n20%\n40%\n60%\n80%\n100%\nremaining weights\nSparsity per layer depth\nQ\nK\nV\nO\nFC1\nFC2\nlayer type\n0%\n20%\n40%\n60%\n80%\n100%\nSparsity per layer type\nLLM Surgeon (full. cor )\nLLM Surgeon (within row/col cor. )\nMagnitude\nL-OBD\nK-OBD\nFigure 4: Sparsity levels obtained with structured pruning on OPT-125m by layer depth and type.\n5\nCONCLUSION\nIn this work, we have introduced the LLM Surgeon algorithm for unstructured, semi-structured and\nstructured compression of neural networks. The work builds upon classic neural network compres-\nsion approaches originating from the early 1990\u2019s that aim to find optimal pruning by expanding\nthe curvature of the loss landscape. The method utilises modern Fisher approximations to scale\naccurate pruning to the realm of large language models (LLMs) with billions of parameters, while\nremaining practical in both memory and compute. Unlike most prior work on data-based LLM\ncompression, we not only use weight magnitude and activations from forward passes, but also use\ngradient information from backward passes to relate weight removal costs to the true final objective.\nWe improve upon prior work through more accurate approximations to the loss landscape curvature\nand considering more weight correlations to update remaining weights. Increasing the number of\ncorrelations and using multiple shots allows us trading off additional compute for better accuracy.\nLastly, LLM Surgeon gives the first practically usable results for structured pruning of LLMs and\nachieves state-of-the-art results in unstructured and semi-structured large language model pruning.\n9\nREFERENCES\nChristopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, vol-\nume 4. Springer, 2006.\nAleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep\nlearning. In International Conference on Machine Learning, pp. 557\u2013565. PMLR, 2017.\nElias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training\nquantization and pruning. Advances in Neural Information Processing Systems, 35:4475\u20134488,\n2022.\nElias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in\none-shot. 2023.\nGene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013.\nBabak Hassibi and David Stork.\nSecond order derivatives for network pruning: Optimal brain\nsurgeon. Advances in neural information processing systems, 5, 1992.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nItay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training\nquantization with small calibration sets. In International Conference on Machine Learning, pp.\n4466\u20134475. PMLR, 2021.\nAlexander Immer, Tycho van der Ouderaa, Gunnar R\u00a8atsch, Vincent Fortuin, and Mark van der Wilk.\nInvariance learning in deep neural networks with differentiable laplace approximations. Advances\nin Neural Information Processing Systems, 35:12449\u201312463, 2022.\nAbdoulaye Koroko, Ani Anciaux-Sedrakian, Ibtihel Ben Gharbia, Val\u00b4erie Gar`es, Mounir Haddou,\nand Quang Huy Tran. Efficient approximations of the fisher matrix in neural networks using\nkronecker product singular value decomposition. arXiv preprint arXiv:2201.10285, 2022.\nFrederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical fisher approx-\nimation for natural gradient descent. Advances in neural information processing systems, 32,\n2019.\nEldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael\nGoin, and Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning\nfor large language models. arXiv preprint arXiv:2203.07259, 2022.\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information\nprocessing systems, 2, 1989.\nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through\nl 0 regularization. arXiv preprint arXiv:1712.01312, 2017.\nDavid JC MacKay. Information theory, inference and learning algorithms. Cambridge university\npress, 2003.\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\ncurvature. In International conference on machine learning, pp. 2408\u20132417. PMLR, 2015.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nMingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach\nfor large language models. arXiv preprint arXiv:2306.11695, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n10\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nChaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning\nin the kronecker-factored eigenbasis. In International conference on machine learning, pp. 6566\u2013\n6575. PMLR, 2019.\nWikipedia. Wikipedia. PediaPress, 2004.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\nAojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hong-\nsheng Li. Learning n: m fine-grained structured sparse neural networks from scratch. arXiv\npreprint arXiv:2102.04010, 2021.\n11\nA\nDERIVATIONS FOR PRUNING\nGiven that we use a Gaussian approximation of our loss p\u2248q = N through a quadratic approxima-\ntion of our log likelihood \u2212 log p\u2248 1\n2(\u03b8\u2217)T F \u03b8\u2217, the most optimal compression becomes the solution\nto the following constrained optimization problem:\narg min\n\u2206\u03b8\u2217\n1\n2\u2206(\u03b8\u2217)T F \u2206\u03b8\u2217\n(14)\ns.t. eT\nk \u2206\u03b8\u2217 + eT\nk \u03b8\u2217 = 0, \u2200k \u2208 Q\nwhere Q is the set of Q indices that are pruned.\nA.1\nGENERAL SOLUTION\nFollowing (Kurtic et al., 2022), we denote pruned elements as EK\n= [eq1\neq2\n. . .]T\n\u2208\n[0, 1]|Q|\u00d7P and use the fact that solving eq. (6) through use of Langrange multipliers gives the\ngeneral closed-form solution for cost L and weight update \u2206\u03b8:\nL = 1\n2(EK\u03b8\u2217)T \u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\u2217\n(15)\n\u2206\u03b8\u2217 = F \u22121ET\nK\n\u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\u2217\n(16)\nA.2\nREMOVING A SINGLE ELEMENT\nOptimal brain surgeon (OBS)\nTo remove a single element with index q, we simply set EK = eT\nk :\nL = 1\n2(EK\u03b8\u2217)T \u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\n= 1\n2\u03b8T\nk\n1\n[F \u22121]kk\n\u03b8k\n= 1\n2\n(\u03b8k)2\n[F \u22121]kk\n,\n\u2206\u03b8 = \u2212F \u22121ET\nK\n\u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\n= \u2212F \u22121ek\n\u0000eT\nk F \u22121ek\n\u0001\u22121 eT\nK\u03b8\n= \u2212\n\u03b8k\n[F \u22121]kk\nF \u22121ek\n(17)\nwhich exactly correspond to the loss and updates of optimal brain surgeon (Hassibi & Stork, 1992).\nOptimal brain damage (OBD)\nWe may also consider that elements are independent and the\nFisher is diagonal. After noting that this implies that diagonal elements of the inverse Fisher are\nscalar inverses of elements in the Fisher [F \u22121]kk =\n1\n[F ]kk , the formula\u2019s simplify to:\nL = [F ]kk(\u03b8k)2,\n\u2206\u03b8 = \u2212\u03b8kek\n(18)\nwhich exactly corresponds to loss and updates of optimal brain damage (LeCun et al., 1989).\nVECTORISED\nFor implementation purposes, it might be convenient to have a vectorised notation L\u03b8 \u2208 RRC or\nLW \u2208 RR\u00d7C to calculate all expected losses in parallel:\nFor OBD:\n\f\f\f\nFor OBS:\n\f\f\f\nL\u03b8 = 1\n2\u03b8\u2217 \u2299 \u03b8\u2217 \u2299 diag(F )\nL\u03b8 = 1\n2\u03b8\u2217 \u2299 \u03b8\u2217 \u2298 diag(F \u22121)\n,\nLW = 1\n2W \u2217 \u2299 W \u2217 \u2299 mat(diag(F ))\nLW = 1\n2W \u2217 \u2299 W \u2217 \u2298 mat(diag(F \u22121))\n(19)\nA.3\nREMOVING A SINGLE ROW OR COLUMN\nStructured OBS\nIf we consider the approximation F \u2248 G\u2297A with known inverse (G\u2297A)\u22121 =\nG\u22121 \u2297A\u22121, then to remove a row at index r \u2208 [0, R], we must take into account correlations within\n12\nelements of that row. That is, we write matrix EK = (eT\nr \u2297 I) containing one-hot row-vectors for\nall elements in row r. Plugging into the general solution eq. (7), we find:\nL = 1\n2EK\u03b8T \u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\u2217\n= 1\n2((eT\nr \u2297 I)\u03b8\u2217)T \u0000(eT\nr \u2297 I)(G \u2297 A)\u22121(eT\nr \u2297 I)T \u0001\u22121 (eT\nr \u2297 I)\u03b8\u2217\n= 1\n2\u03b8T\nr\n\u0000eT\nr G\u22121er \u2297 IA\u22121I\n\u0001\u22121 \u03b8r\n= 1\n2\u03b8T (eT\nr \u2297 I)\n\u0000\u0002\n[G\u22121]rr\n\u0003\n\u2297 A\u22121\u0001\u22121 (er \u2297 I)\u03b8r\n= 1\n2\n\u03b8T\nr A\u03b8r\n[G\u22121]rr\n(20)\nwhere we write \u03b8r = eT\nr W \u2217\u2208RC for the r\u2019th row-vector in W . Similarly, we obtain the associated\nweight update:\n\u2206\u03b8 = \u2212F \u22121ET\nK\n\u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\u2217\n= \u2212 (G \u2297 A)\u22121 (eT\nr \u2297 I)T \u0010\n(eT\nr \u2297 I) (G \u2297 A)\u22121 (eT\nr \u2297 I)T \u0011\u22121\n(eT\nr \u2297 I)\u03b8\u2217\n= \u2212\n\u0000G\u22121 \u2297 A\u22121\u0001\n(er \u2297 I)\n\u0000eT\nr G\u22121er \u2297 A\u22121\u0001\u22121 \u03b8r\n= \u2212\n1\n[G\u22121]rr\n\u0000G\u22121er \u2297 A\u22121IA\u22121I\n\u0001\n\u03b8r\n= \u2212G\u22121er \u2297 \u03b8r\n[G\u22121]rr\n(21)\narriving at a similar structured pruning update as derived in (Wang et al., 2019) for convolutional\nfilters. We can equivalently derive expected loss and update for columns, by considering EK =\n(I \u2297 eT\nc ). If we do so, we find the structured updates for a row r or column c:\nRemove row r:\n\f\f\f\nRemove column c:\n\f\f\f\nL = 1\n2\n\u03b8T\nr A\u03b8r\n[G\u22121]rr\nL = 1\n2\n\u03b8T\nc G\u03b8c\n[A\u22121]cc\n\u2206\u03b8 = \u2212G\u22121er \u2297 \u03b8r\n[G\u22121]rr\n\u2206\u03b8 = \u2212\u03b8c \u2297 A\u22121ec\n[A\u22121]cc\n(22)\nStructured OBD\nWe may also assume that, when removing a row r, the individual elements\nwithin the row are also independent which would imply [A]ii =\n1\n[A\u22121]ii . Similarly, [G]ii =\n1\n[G\u22121]ii\nwhen removing a column c. Consequently, we can simplify to:\nRemove row r:\n\f\f\f\nRemove column c:\n\f\f\f\nL = 1\n2Grr\u03b8T\nr A\u03b8r\nL = 1\n2Acc\u03b8T\nc G\u03b8c\n\u2206\u03b8 =\n\f\f\f \u2212 er\u03b8T\nr\n\u2206\u03b8 =\n\f\f\f \u2212 \u03b8ceT\nc\n(23)\nsimilar form to structured OBD losses and updates as derived in (Wang et al., 2019) for convolutional\nfilters. The derivations slightly differ in that we start from the general solution eq. (8), circumventing\nthe need to rederive a Langrange multipliers for each possible structure.\nA.4\nPRUNING MULTIPLE (CORRELATED) ROWS AND COLUMNS\nLet us consider the removal of R\u2032 rows r1, r2, . . . r\u2032\nR rows or C\u2032 columns with indices c1, c2, . . . , cC\u2032,\nwith 1<R\u2032 < R and 1<C\u2032<C. We denote matrices containing one-hot vectors selecting all rows\nand columns to be removed respectively as:\nER\u2032 = [e1\ne2\n. . .\neR\u2032]T \u2208 RR\u2032\u00d7R\nEC\u2032 = [e1\ne2\n. . .\neC\u2032]T \u2208 RC\u2032\u00d7C\n(24)\nThen, the matrix EK containing one-hot row vectors selecting all elements to be removed can be\nwritten as:\nMultiple rows:\nMultiple columns:\nEK = (ER\u2032 \u2297 IC) \u2208 RQ\u00d7RC , (with Q = R\u2032C)\nEK = (IR \u2297 EC\u2032) \u2208 RQ\u00d7RC , (with Q = RC\u2032)\n(25)\n13\nTo simultaneously remove rows and columns, we can stack the matrices with duplicate row vectors\nremoved:\nMultiple rows and columns:\nEK\n\u0014\nER\u2032 \u2297 IC\nIR \u2297 EC\u2032\n\u0015\n\u2208 RQ\u00d7RC with duplicate rows removed\n(26)\nThe removal of duplicate rows is required due to the few R\u2032C\u2032 overlapping elements between rows\nand columns, after which the total number of rows thus becomes Q = R\u2032C +C\u2032R\u2212R\u2032C\u2032. We used\nappropriately sized identity matrices IR \u2208 RR\u00d7R and IC \u2208 RC\u00d7C. For brevity, we write the vector\nor matrix of pruned weights\u03b8 := EK\u03b8 \u2208 RQ.\nFirst, we derive the removal for R\u2032 rows by defining removal matrix as EK = ER\u2032 \u2297 I and define\nW := ER\u2032W \u2208 RR\u2032\u00d7C. The complete weight update for the removal of multiple rows becomes:\n\u2206\u03b8 = \u2212F \u22121ET\nK\n\u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\u2217\n= \u2212(G \u2297 A)\u22121(ER\u2032 \u2297 I)T \u0000(ER\u2032 \u2297 I)(G \u2297 A)\u22121(ER\u2032 \u2297 I)T \u0001\u22121 (ER\u2032 \u2297 I)\u03b8\u2217\n= \u2212(G\u22121ET\nR\u2032 \u2297 A\u22121)\n\u0000ER\u2032G\u22121ET\nR\u2032 \u2297 A\u22121\u0001\u22121\u03b8\u2217\n= \u2212(G\u22121ET\nR\u2032 \u2297 A\u22121)\n\u0000(ER\u2032G\u22121ET\nR\u2032)\u22121 \u2297 A\n\u0001\n\u03b8\u2217\n\u2206W = \u2212G\u22121ET\nR\u2032\n\u0010\n(ER\u2032G\u22121ET\nR\u2032)\u22121WA\n\u0011\nA\u22121\n= \u2212G\u22121ET\nR\u2032(ER\u2032G\u22121ET\nR\u2032)\u22121W\n(27)\nSimilarly, we derive the removal of C\u2032 columns by defining removal matrix as EK = I \u2297 EC\u2032 and\ndefineW := EC\u2032W \u2208 RR\u00d7C\u2032. The complete weight update for multiple column removal becomes:\n\u2206\u03b8 = \u2212F \u22121ET\nK\n\u0000EKF \u22121ET\nK\n\u0001\u22121 EK\u03b8\u2217\n= \u2212(G \u2297 A)\u22121(I \u2297 EC\u2032))T \u0000(I \u2297 EC\u2032)(G \u2297 A)\u22121(I \u2297 EC\u2032)T \u0001\u22121 (I \u2297 EC\u2032)\u03b8\u2217\n= \u2212(G \u2297 A)\u22121(I \u2297 EC\u2032))T \u0000(I \u2297 EC\u2032)(G \u2297 A)\u22121(I \u2297 EC\u2032)T \u0001\u22121 (I \u2297 EC\u2032)\u03b8\u2217\n= \u2212(G\u22121 \u2297 A\u22121ET\nC\u2032)\n\u0000G \u2297 EC\u2032A\u22121ET\nC\u2032\n\u0001\u22121\u03b8\n\u2206W = \u2212G\u22121GW(EC\u2032A\u22121ET\nC\u2032)\u22121(A\u22121ET\nC\u2032)\n= \u2212W(EC\u2032A\u22121ET\nC\u2032)\u22121(A\u22121ET\nC\u2032)\n(28)\n14\nB\nEXPERIMENTAL DETAILS.\nCode will be made available upon acceptance.\nB.1\nMODELS\nOPT models\nFrom the OPT model family ((Zhang et al., 2022)), we consider models with the\nfollowing number of parameters: 125 million (125m), 1.3 billion (1.3b), 2.7 billion (2.7b), 6.7\nbillion (6.7b) models. We omit 350 million model due to different layer norm. We obtain the\nstandard pre-trained checkpoints using Huggingface (Wolf et al., 2019) and use this as a baseline\nand initialisation for compression.\nLlama-v2 models\nFrom the Llama-v2 model family ((Touvron et al., 2023)), we consider a model\nwith 7 billion (7b) parameters and a model with 13 billion (13b) parameters. We obtain the stan-\ndard pre-trained checkpoints using Huggingface (Wolf et al., 2019) and use this as a baseline and\ninitialisation for compression.\nB.2\nDATASETS\nEnglish / Wikitext-2\nThe majority of the results are obtained on the Wikitext-2 dataset containing\nparsed subsets of the English Wikipedia (Merity et al., 2016; Wikipedia, 2004), using the default\ntraining and test sets. For fitting, we use 128 batches of 2048 characters and for testing we use the\nstandard test set containing 4358 characters.\nFrench / Wikipedia\nFor French data experiments, we use a subset of French wikipedia\n(Wikipedia, 2004). For fitting, we use 128 batches of 2048 characters and for testing we use a\nrandomly selected test set containing 1067888 characters.\nGerman / Wikipedia\nFor the Italian data experiments, we use a subset of the German wikipedia\n(Wikipedia, 2004). For fitting, we use 128 batches of 2048 characters and for testing we use a\nrandomly selected test set containing 1112372 characters.\nItalian / Wikipedia\nFor the Italian data experiments, we use a subset of the Italian wikipedia\n(Wikipedia, 2004). For fitting, we use 128 batches of 2048 characters and for testing we use a\nrandomly selected test set containing 633177 characters.\nB.3\nMASK EQUIVALENCE\nWhen comparing the equivalence of obtained pruning masks between two models \u03b8A and \u03b8B ob-\ntained by two compression methods A and B. We always consider the case of 50% pruning, and\ndefine the mask equivalence as the fraction of same weights that are set two zero in both models:\nmask equivalence =\nP\nX\ni=1\n1([\u03b8A]i = 0 and [\u03b8B]i = 0)\nP\n.\n(29)\nwhere 1 denotes an indicator function that returns 1 if both weights [\u03b8A]i and [\u03b8B]i are zero, and\nreturns 0 otherwise.\nB.4\nSPARSEGPT AND EVALUATION OF BASELINES\nFor the SparseGPT baseline, we used the official code SparseGPT code repository (Frantar & Alis-\ntarh, 2023) which allows for training and evaluation on wikitext-2. The obtained results may differ\nfrom those reported in the original paper as the C4 dataset was used there.\nIn this work, models were trained with the same 128 batches of the wikitext-2 training set as avail-\nable in the SparseGPT codebase and are evaluated on the wikitext-2 test set using the exact same\nevaluation procedure.\n15\nC\nTECHNICAL DETAILS\nC.1\nPSEUDOCODES\nAlgorithm 2 LLM Surgeon (structured)\nInput: target size \u03b1\nInput: initial weights \u03b80\nFor shot t in [1, 2, ..., T]\nCompute: approximate curvature G1, A1 from data (optionally also G2, A2)\n\u25b7 section 3.1\nCompute: costs per row/column Lr, Lc from G1, A1, (G2, A2)\n\u25b7 section 3.2\nCompute: threshold \u03c4 using Lr and Lc given target size \u03b1\n\u25b7 section 3.3\nSelect: rows and columns to remove ER, EC based on \u03c4\n\u25b7 section 3.3\nCompute: weight update \u2206\u03b8t\u22121 based on ER, EC and G1, A1, (G2, A2)\n\u25b7 section 3.4\nUpdate: remaining weights \u03b8t \u2190 \u03b8t\u22121 + \u2206\u03b8t\u22121\n\u25b7 section 3.5\nOptionally: \u03b8t \u2190 low-rank update(\u03b8t)\nOutput: compressed weights \u02c6\u03b8 = \u03b8T\nAlgorithm 3 LLM Surgeon (semi-structured / unstructured)\nInput: target size \u03b1\nInput: initial weights \u03b80\nFor shot t in [1, 2, ..., T]\nCompute: approximate curvature G1, A1 from data (optionally also G2, A2)\n\u25b7 section 3.1\nCompute: costs per element Lk from G1, A1, (G2, A2)\n\u25b7 section 3.2\nCompute: threshold \u03c4 from Lk and target size \u03b1t (unstructured/semistructured) \u25b7 section 3.3\nSelect: elements to remove EK based on \u03c4 (unstructured/semistructured)\n\u25b7 section 3.3\nCompute: weight update \u2206\u03b8t\u22121 based on EK and G1, A1, (G2, A2)\n\u25b7 section 3.4\nUpdate: remaining weights \u03b8t \u2190 \u03b8t\u22121 + \u2206\u03b8t\u22121\n\u25b7 section 3.5\nOptionally: \u03b8t \u2190 low-rank update(\u03b8t)\nOutput: compressed weights \u02c6\u03b8 = \u03b8T\nC.2\nDAMPENING\nIn practice, we dampen the G and A matrices by adding a diagonal term G + \u03bbGI and A + \u03bbAI.\nIn our experiments, we found that values in the range [0.01, 0.1] multiplied by mean diagonal terms\ngenerally works well. We follow (Frantar & Alistarh, 2023) and always use \u03bbA=0.01diag(A) to\nbe consistent with prior work and allow for a fair comparison with baselines. Further, we use\n\u03bbG=0.1diag(G) for structured experiments and \u03bbG=0.01diag(G) in semi-structured and unstruc-\ntured experiments.\n16\nD\nDOWNSTREAM TASK PERFORMANCE\nWe also evaluate our method on downstream tasks as perplexity metrics do not necessarily correlate\nwith downstream performance. Further, we also repeat this experiment using the C4 dataset as\nreference data for compression, as this is used in prior work (Frantar & Alistarh, 2023) and as this\ncan be regarded a more general reference dataset. In tables 5 and 6 we report 0-shot test performance\nof structured pruning for LLM surgeon and K-OBD baseline.\nTable 5: Downstream task performance using Wikitext-2 for pruning.\nStructured pruning\n(with wikitext-2)\nModel size\nwikitext word ppl\nboolq\npiqa\nhallaswag\nwinogrande\narc easy\narc challenge\nopenbookq\ncopa\nlambada openai\nwsc273\nAVERAGE wikitext2\nDense baseline\n100%\n9.24\n77.74\n79.11\n75.99\n69.14\n74.58\n46.25\n44.20\n86.00\n73.92\n85.71\n71.26\nLLM Surgeon (ours)\n90%\n9.63\n76.21\n78.56\n75.39\n67.64\n74.12\n46.50\n43.60\n85.00\n72.64\n84.98\n70.46\n80%\n12.16\n72.97\n77.09\n71.30\n66.30\n71.36\n41.89\n41.80\n87.00\n56.43\n80.22\n66.66\n70%\n16.91\n61.25\n73.56\n60.72\n61.09\n63.09\n36.69\n38.80\n81.00\n28.33\n76.56\n58.11\n60%\n25.15\n44.98\n69.26\n48.04\n54.38\n52.31\n30.29\n36.80\n78.00\n11.72\n68.50\n49.43\n50%\n43.68\n39.60\n64.36\n40.29\n52.57\n44.91\n26.28\n30.80\n74.00\n6.52\n61.54\n44.09\nK-OBD\n90%\n9.89\n76.67\n78.02\n74.80\n68.11\n75.17\n46.33\n44.60\n86.00\n72.71\n82.78\n70.52\n80%\n17.62\n74.34\n75.24\n67.85\n64.64\n63.80\n40.27\n41.60\n83.00\n30.23\n82.42\n62.34\n70%\n32.72\n65.29\n71.82\n53.07\n56.83\n51.05\n33.11\n37.80\n79.00\n12.21\n70.70\n53.09\n60%\n68.63\n60.80\n65.67\n43.99\n53.20\n41.79\n28.50\n34.00\n75.00\n7.04\n60.44\n47.04\n50%\n136.33\n61.56\n60.66\n36.84\n53.04\n36.11\n26.71\n33.00\n72.00\n4.70\n61.17\n44.58\nTable 6: Downstream task performance using C4 for pruning.\nStructured pruning\n(with C4)\nModel size\nwikitext word ppl\nboolq\npiqa\nhallaswag\nwinogrande\narc easy\narc challenge\nopenbookq\ncopa\nlambada openai\nwsc273\nAVERAGE wikitext2\nDense baseline\n100%\n9.24\n77.74\n79.11\n75.99\n69.14\n74.58\n46.25\n44.20\n86.00\n73.92\n85.71\n71.26\nLLM Surgeon (ours)\n90%\n9.90\n77.03\n78.45\n74.95\n68.27\n73.19\n45.99\n44.60\n84.00\n72.81\n82.78\n70.21\n80%\n14.42\n75.60\n76.82\n69.71\n63.85\n70.29\n41.30\n42.80\n87.00\n45.53\n82.42\n65.53\n70%\n25.16\n66.39\n72.85\n58.11\n56.83\n62.16\n34.47\n38.40\n80.00\n22.69\n69.96\n56.19\n60%\n45.35\n62.48\n68.93\n48.10\n55.64\n51.56\n27.99\n35.20\n70.00\n12.56\n61.54\n49.40\n50%\n77.30\n62.60\n65.02\n41.70\n54.22\n42.55\n24.23\n31.20\n71.00\n7.26\n60.44\n46.02\nK-OBD\n90%\n10.59\n75.47\n78.18\n73.61\n66.46\n72.52\n44.37\n43.60\n87.00\n71.22\n82.42\n69.48\n80%\n20.12\n73.36\n75.14\n66.11\n62.43\n62.84\n38.23\n41.00\n86.00\n21.50\n78.39\n60.50\n70%\n56.92\n63.30\n68.44\n52.31\n55.64\n46.72\n31.31\n34.60\n77.00\n5.69\n68.86\n50.39\n60%\n112.85\n62.23\n64.47\n46.36\n52.17\n40.53\n29.52\n32.40\n72.00\n2.91\n63.00\n46.56\n50%\n272.16\n62.42\n61.70\n38.47\n50.43\n33.29\n26.96\n31.80\n65.00\n0.91\n59.34\n43.03\nWe find that our method not only performs well in terms of test perplexity but also correlates well\nwith downstream performance, outperforming the baselines on these downstream tasks.\nE\nADDITIONAL EXPERIMENTS ON LLAMA-V2 13B.\nTo assess performance on larger 13B parameter models, we also report structured compression on\nthe Llama-v2 13B model and evaluate downstream task performance. Test perplexities (lower is\nbetter) can be found in table 7 below:\nTable 7: Pruning Llama-v2 13B model.\nBaseline\nPruned model sizes\nDense 100%\n90%\n80%\n70%\n60%\n50%\nK-OBD\n4.547\n4.908\n6.294\n10.08\n13.06\n16.06\nLLM Surgeon\n4.547\n4.692\n5.286\n6.207\n7.245\n9.428\nas well as evaluated results on downstream benchmarks (higher is better) in table 8 below.\nTable 8: Downstream task performance after pruning large Llama-v2 13B model.\nLlama-v2 13B\nModel size\nwikitext word ppl\nboolq\npiqa\nhallaswag\nwinogrande\narc easy\narc challenge\nopenbookq\ncopa\nlambada openai\nwsc273\nAVERAGE wikitext2\nDense baseline\n100%\n8.23\n80.52%\n80.52%\n79.38%\n72.14%\n77.53%\n49.23%\n45.20%\n90.00%\n76.77%\n89.38%\n74.07%\nLLM Surgeon (ours)\n90%\n8.57\n81.07%\n79.87%\n79.24%\n72.38%\n76.30%\n49.91%\n47.20%\n92.00%\n75.47%\n89.38%\n74.28%\n80%\n10.08\n80.86%\n79.00%\n77.09%\n70.56%\n75.93%\n46.76%\n46.80%\n90.00%\n67.79%\n86.45%\n72.12%\n70%\n12.74\n74.50%\n76.50%\n71.52%\n68.67%\n69.74%\n40.27%\n45.00%\n91.00%\n54.40%\n83.52%\n67.51%\n60%\n16.00\n64.62%\n73.01%\n65.04%\n65.75%\n63.80%\n37.12%\n39.60%\n90.00%\n44.50%\n81.32%\n62.48%\n50%\n23.75\n65.66%\n68.77%\n56.19%\n63.22%\n56.19%\n31.83%\n36.60%\n85.00%\n35.16%\n77.29%\n57.59%\nK-OBD\n90%\n8.79\n81.31%\n79.76%\n79.12%\n72.22%\n76.94%\n47.95%\n47.80%\n91.00%\n75.26%\n88.64%\n74.00%\n80%\n11.79\n80.80%\n79.16%\n76.80%\n70.56%\n73.74%\n46.93%\n48.60%\n88.00%\n58.99%\n87.55%\n71.11%\n70%\n20.00\n66.76%\n74.43%\n64.18%\n64.96%\n56.23%\n36.01%\n39.00%\n88.00%\n38.54%\n79.49%\n60.76%\n60%\n27.74\n55.66%\n70.24%\n55.52%\n60.46%\n49.62%\n32.68%\n35.80%\n80.00%\n30.06%\n73.63%\n54.37%\n50%\n37.38\n59.79%\n66.54%\n48.39%\n57.46%\n46.59%\n30.72%\n34.00%\n77.00%\n24.61%\n69.96%\n51.50%\nWe find that LLM Surgeon also outperforms baselines on existing Llama-v2 13B models. We stress\nthat these results are obtained on structured pruning of rows and columns, which are regarded the\nhardest and most constrained pruning structure. Yet, we can compress Llama 13B by 20% with less\nthan 2% drop in downstream task performance. It also significantly outperforms the baseline for all\ncompression rates, both in terms of test perplexity and downstream task performance.\n17\nF\nABLATIONS\nF.1\nSHOTS\nTable 9: Ablation of shot counts T for structured LLM Surgeon compressing OPT-1.3b model.\nTarget size\nShots T\nwikitext-2 PPL\nShots T\nwikitext-2 PPL\nShots T\nwikitext-2 PPL\n90%\n6\n14.70\n8\n14.70\n10\n14.72\n80%\n12\n15.14\n16\n15.12\n20\n15.08\n70%\n18\n16.21\n24\n16.24\n30\n16.23\n60%\n24\n18.53\n32\n18.45\n40\n18.49\n50%\n30\n23.32\n40\n22.95\n50\n22.68\nF.2\nTASK-SPECIFIC COMPRESSION\nTable 10:\nCross-task performance and mask\nequivalences of 50% compressed OPT-125m\nmodel using structured LLM Surgeon on language\nsubsets.\nevaluation dataset\nmask equivalence (%)\ntarget\nEN\nFR\nDE\nIT\nEN\nFR\nDE\nIT\nPretrained\n27.66\n22.54\n24.32\n27.66\nEN\n47.46\n172.9\n181.1\n169.1\n1.00\n0.74\n0.70\n0.72\nFR\n113.4\n28.44\n35.02\n34.90\n0.74\n1.00\n0.87\n0.90\nDE\n142.1\n35.15\n27.49\n38.49\n0.70\n0.87\n1.00\n0.87\nIT\n123.7\n31.85\n33.78\n30.58\n0.72\n0.90\n0.87\n1.00\nLLM Surgeon uses data to find a compressed\nmodel that has the least negative impact on fi-\nnal test performance. In this section, we ex-\nplore the extent to which the method can use\ndata to compress specifically to the task at hand.\nWe do so by comparing test performance and\nequivalences between resulting pruning masks\nfor different language modeling languages: En-\nglish (EN/wikitext-2), French (FR) and Italian\n(IT) and the German (DE). We consider 50%\nunstructured compression using LLM Surgeon with correlated weight updates. For each compressed\nmodel, we compare performance on all languages and compare the equivalences between resulting\npruning masks (details in appendix B.3), and report results in table 10. Like other methods that use\ndata for compression (Hassibi & Stork, 1992; Frantar & Alistarh, 2023; Wang et al., 2019), we ex-\npect to see some correlation between the data used for training and data with good test performance,\nwhich is reflected in both test performance and masks. It is important to note that the final perfor-\nmance after compression will depend on the quality of the used dataset for compression. Further, the\nexperiment demonstrates that the method can be used for task-specific compression tailored towards\nthe data used for compression and generalises to high test performance on the associated test data.\n18\nG\nON FAIR COMPARISON\nAll results in this work (including the SparseGPT) were trained on Wikitext-2 for fair comparison.\nTo do so, we used the same dataloader and evaluation script as the official SparseGPT repo and reran\nall SparseGPT results to be trained on Wikitext-2. In some cases, this resulted in better scores for the\nSparseGPT baseline compared to the C4-trained results reported in the original SparseGPT paper.\nYet, we find that our method using improved curvature estimates still outperformed the baselines in\nterms of final test performance.\nH\nCOMPUTATIONAL PERFORMANCE\nWe report computational cost in terms of pruning time in table 11 and GPU memory in table 12.\nTable 11: Time performance.\nTest performance\nRuntime\nNetwork\nTime\nPPL 90%\nPPL 80%\nPPL 70%\nPPL 60%\nPPL 50%\nUnstructured baseline (SparseGPT)\nLlama-v2 7B\n<5m\n5.49\n5.58\n5.71\n5.94\n6.51\nUnstructured LLM Surgeon (ours)\nLlama-v2 7B\n2d8h16m\n5.13\n5.20\n5.36\n5.66\n6.08\nStructured baseline (K-OBD)\nLlama-v2 7B\n16h58m\n5.48\n9.14\n15.43\n28.03\n46.64\nStructured LLM Surgeon (ours)\nLlama-v2 7B\n17h08m\n5.25\n6.18\n7.83\n10.39\n15.38\nStructured baseline (K-OBD)\nLlama-v2 13B\n1d6h5m\n4.908\n6.294\n10.08\n13.06\n16.06\nStructured LLM Surgeon (ours)\nLlama-v2 13B\n1d9h26m\n4.692\n5.286\n6.207\n7.245\n9.428\nOur method is most efficient for structured pruning, but it must be noted that engineering efforts may\nfurther improve speed for unstructured pruning. The focus of the paper is structured pruning, on\nwhich we achieve state-of-the-art compression rates. Importantly, compression of LLMs only needs\nto happen once after which a pruned model can be deployed infinitely many times without further\ncost. This motivates our method which takes longer to run but reaches better final test performance.\nTable 12: Memory performance.\nNetwork\nSparseGPT (baseline)\nUnstructured LLM-Surgeon (ours)\nLlama-7B\n<5m / 1 GPU (32GB)\n2d8h16m / 4xH100 80 GB\nK-OBD (baseline)\nStructured LLM-Surgeon (ours)\nLlama-7B\n16h58m / 4xH100 80 GB\n17h08m / 4xH100 80 GB\nLlama-13B\n1d6h5m / 8xH100 80 GB\n1d9h26m / 8xH100 80 GB\nWe argue that differences in the performance and the runtime of pruning methods can largely be\nattributed to underlying assumptions on correlations between weights. Notably, algorithms that\nconsider few correlations, sometimes to the extent of completely disregarding all gradient informa-\ntion, can result in very fast pruning algorithms for unstructured and semi-structured pruning but are\noften not flexible enough to perform structured pruning of rows and columns. Examples of such\nlightweight algorithms for LLMs are (Sun et al., 2023) and SparseGPT (Frantar & Alistarh, 2023),\nas can also be observed from table 11. Our approach makes less strong assumptions on the cur-\nvature of the loss and as a result outperforms all baselines on all unstructured, semi-structured and\nstructured pruning. Further, the improved curvature is also eligible for dynamic allocation of weight\nremoval and improved correlated weight updates. In practice, we always recommend using our\nmethod for structured pruning. For unstructured and semi-structured pruning, we note an important\ntrade-off between the desired final test accuracy and the available computational budget. Here, our\nproposed method can achieve the highest final model performance but requires more computational\nresources and takes longer to run. It should be noted that pruning only needs to happen once after\nwhich a model can be deployed infinitely many times this time, which dependent on the available\ncomputational resources can also legitimise spending additional pruning time even if this is much\nhigher compared to other algorithms in relative terms. In absolute terms, the use of multiple large\nGPUs is common practice in the field of large language models and many more GPUs are typically\nused to train and deploy large language models. Moreover, the curvature approximation is naively\namenable to data parallelism in case further speed-ups or larger models are required. We hope this\nprovides context and emphasises the trade-off between performance and compute in practice.\n19\nI\nEXTENDING CURVATURE ESTIMATES\nInstead of using a single Kronecker product, we might consider improving the approximation\nthrough a sum of multiple Kronecker factors:\nF \u2248 eF = G1 \u2297 A1 + G2 \u2297 A2\n(30)\nThis last appendix deals with the question how one may computationally find such approximations\nand how to utilise them in the neural network pruning framework.\nI.1\nNEAREST KRONECKER PRODUCT OR SUM OF KRONECKER PRODUCTS\nInstead of assuming independence of activations and derivatives as in section 3.1, following the\nclassic KFAC of (Martens & Grosse, 2015), we might want to find the nearest Kronecker product\napproximation F \u2248 e\nG \u2297 e\nA that is closest to the Fisher in terms of the Frobenius norm:\ne\nGl, e\nAl = arg min\nGl,Al\n||Fl \u2212 Gl \u2297 Al||F\n(31)\nFinding the nearest sum of Kronecker factors can be rephrased as a classic eigenvalue problem of\nfinding the nearest rank-1 matrix. Golub & Van Loan (2013).\n||F \u2212 \u02dcG \u2297 \u02dc\nA||F\n\u2261\n||R(F ) \u2212 vec( e\nG)vec( e\nA)T ||F\n(32)\nPower method and deflation\nAfter considering the reshaping, we can use power iterations to\nsolve for and find the nearest Kronecker factors G1, A1 = solve(F ).\nFind with power iterations:\neG1, eA1 = solve(F ) = arg min\nG,A\n||F \u2212 G \u2297 A||F\nDeflation:\neGr, eAr = solve(F \u2212\nXr\u22121\nr\u2032=1( e\nGr\u2032 \u2297 e\nAr\u2032))\nA more extensive description of the power method solve(\u00b7) can be found in algorithm 4. At the start\nof the algorithm, we initialise power iterations as vector with one\u2019s 1 = [1\n1\n. . .\n1]. After each\nshot we can initialise the vector as the final estimate found during the previous shot.\nAlgorithm 4 Kronecker power method. Finds e\nG, e\nA nearest Kronecker product ||F \u2212 e\nG \u2297 e\nA||F .\nInput: Initialise eg0=1, ea0=1 (or using estimates of previous shot).\nInput: Set iterations I (or I=1 if using estimates from previous shot)\nOutput: e\nG, e\nA\nfor iteration i in [1, 2, ..., I] do\nCompute: egi =\nR( e\nF )eai\u22121\n||R( e\nF )eai\u22121||2 , with R( eF )eai\u22121 = 1\nN\nPN\nn=1 aT\nn e\nAi\u22121anvec(gngT\nn )\nCompute: eai =\nR( e\nF )T egi\n||R( e\nF )T egi||2 , with R( eF )T egi = 1\nN\nPN\nn=1 gT\nn e\nGignvec(anaT\nn)\nCompute: \u03c3i = ||eai||2\nend for\nReturn: e\nG =\n\u221a\n\u03c3imat(eg), e\nA =\n\u221a\n\u03c3imat(ea).\nTrue Fisher\nrmse: 0.13\nrmse diag: 0.19\nClassic KFAC (IAD)\nrmse: 0.12\nrmse diag: 0.15\nNearest KFAC RK = 1\nrmse: 0.11\nrmse diag: 0.15\nNearest KFAC RK = 2\nrmse: 0.09\nrmse diag: 0.14\nNearest KFAC RK = 3\nrmse: 0.04\nrmse diag: 0.14\nNearest KFAC RK = 9\nFigure 5: Example illustration of nearest Kronecker factor approximations eF \u2248 PRK\nr=1 Gi \u2297 Ai,\ncompared to classical KFAC with the IAD assumption. Larger RK yields better approximations to\nthe true Fisher F for larger RK, as measured by the root mean squared error (rmse).\n20\nI.2\nEXTENDED CURVATURE APPROXIMATIONS\nFor classic KFAC with IAD or RK=1 nearest Kronecker approximations of the form eF = G \u2297 A,\nthe inverse simply becomes (G \u2297 A)\u22121 = G\u22121 \u2297 A\u22121. Unfortunately, we can not use this famous\ninverse identity for sum of Kronecker factors, which is why we fall back on eigendecompositions\nG = E1S1ET\n1 and A = E2S2ET\n2 , allowing us to decompose the Fisher into:\neF = K1S1KT\n1 \u2297 K2S2KT\n2 = (K1 \u2297 K2)(I \u2297 I + S1 \u2297 S2)(KT\n1 \u2297 KT\n2 )\n(33)\nwhere specific K1 and K2 can be found in App. B of Martens & Grosse (2015), which we closely\nfollowed in our derivations. Because K1 and K2 are orthogonal and S1 and S2 diagonal, the inverse\nFisher becomes:\neF \u22121 = (K1 \u2297 K2)(I \u2297 I + S1 \u2297 S2)\u22121(KT\n1 \u2297 KT\n2 )\n(34)\nIn the context of neural network training, the problem gets slightly harder since we want to incre-\nmentally construct estimates e\nGi and e\nAi from individual samples al,n, gl,n that make up F , without\nhaving to simultaneously store more than a single or batch of input activations al,n or output gradi-\nents gl,n in memory. Although this online Kronecker-product principal component analysis problem\nlargely remains an open research problem, we our approach closely follows the recent work by (Ko-\nroko et al., 2022) that uses similar approximations in the context of optimisation. A sum of multiple\nRK>1 Kronecker factors will yield closer approximations, but also linearly increase memory re-\nquirements with higher RK and makes inverting F \u22121 considerably more difficult.\nFormulas to compute cost and weight updates.\nFor sum of Kronecker factors, we find that the\nconstrained optimization solution of for costs \u2206L eq. (7) and weight updates \u2206\u03b8 eq. (8) become the\nfollowing inner-product and matrix-vector product:\nLk = 1\n2\u27e8\u03b8\u2217, U\u03b8\u2217\u27e9 = (\u03b8\u2217)T U(\u03b8\u2217) \u2208 R\n(35)\n\u2206\u03b8 = eF \u22121ET\nKu = K1\n\u0010\nK\nT\n1 UK2 \u2298\n\u0002\n11T + s1sT\n2\n\u0003\u0011\nKT\n2 \u2208 RRC\n(36)\nwith at the heart of it all a matrix U = [EKF \u22121ET\nK]\u22121 that captures correlations between weights:\nU =\n\u0014\nEK\n\u0010\nK1 \u2297 K2\n\u0011\u0010\nI \u2297 I + S1 \u2297 S2\n\u0011\u22121\u0010\nKT\n1 \u2297 KT\n2\n\u0011\nET\nK\n\u0015\u22121\n(37)\nwhere (I \u2297 I + S1 \u2297 S2) is diagonal and the inverse can thus be computed element-wise. The\nremaining inverse is of size K \u00d7 K, for K correlated weights.\nNote on sum of Kronecker factors\nExperimentally, we did not find a benefit in performance\nwhen using a sum of two nearest Kronecker factor approximation, or found it too slow. Therefore,\nwe focus in the main text on LLM Surgeon with fast single Kronecker product KFAC approximation\nto approximate the loss landsscape curvature. Nevertheless, we choose to include this appendix as\nwe believe could prove useful in other contexts or inspire future work that aim to further improve\nthe quality of curvature approximations.\n21\n"
  },
  {
    "title": "Compact Neural Graphics Primitives with Learned Hash Probing",
    "link": "https://arxiv.org/pdf/2312.17241.pdf",
    "upvote": "6",
    "text": "Compact Neural Graphics Primitives with Learned Hash Probing\nTOWAKI TAKIKAWA, NVIDIA, Canada and University of Toronto, Canada\nTHOMAS M\u00dcLLER, NVIDIA, Switzerland\nMERLIN NIMIER-DAVID, NVIDIA, Switzerland\nALEX EVANS, NVIDIA, United Kingdom\nSANJA FIDLER, NVIDIA, Canada and University of Toronto, Canada\nALEC JACOBSON, University of Toronto, Canada and Adobe, Canada\nALEXANDER KELLER, NVIDIA, Germany\nJPEG \n(80.1 kB, 40.21 dB)\nInstant NGP \n(80.0 kB, 35.18 dB)\nOurs \n(80.18 kB, 38.48 dB)\nReference Image\n(502.9 kB)\n [Rho et al. 2023] \n(345.52 kB, 29.8 dB)\nInstant NGP\n(1911.94 kB, 30.0 dB)\nOurs\n(373.96 kB, 29.8 dB)\nReference Image\n3D Scene from Multiview Images\nFig. 1. Compact neural graphics primitives (Ours) have an inherently small size across a variety of use cases with automatically chosen hyperparameters. In\ncontrast to similarly compressed representations like JPEG for images (top) and masked wavelet representations [Rho et al. 2023] for NeRFs [Mildenhall\net al. 2020] (bottom), our representation neither uses quantization nor coding, and hence can be queried without a dedicated decompression step. This is\nessential for level of detail streaming and working-memory-constrained environments such as video game texture compression. The compression artifacts\nof our method are easy on the eye: there is less ringing than in JPEG and less blur than in Rho et al. [2023] (though more noise). Compact neural graphics\nprimitives are also fast: training is only 1.2\u20132.6\u00d7 slower (depending on compression settings) and inference is faster than Instant NGP [M\u00fcller et al. 2022]\nbecause our significantly reduced file size fits better into caches.\nNeural graphics primitives are faster and achieve higher quality when their\nneural networks are augmented by spatial data structures that hold trainable\nfeatures arranged in a grid. However, existing feature grids either come with\na large memory footprint (dense or factorized grids, trees, and hash tables)\nor slow performance (index learning and vector quantization). In this paper,\nwe show that a hash table with learned probes has neither disadvantage,\nresulting in a favorable combination of size and speed. Inference is faster than\nunprobed hash tables at equal quality while training is only 1.2\u20132.6\u00d7 slower,\nsignificantly outperforming prior index learning approaches. We arrive at\nthis formulation by casting all feature grids into a common framework: they\neach correspond to a lookup function that indexes into a table of feature\nAuthors\u2019 addresses: Towaki Takikawa, NVIDIA, Canada and University of\nToronto, Canada, tovacinni@gmail.com; Thomas M\u00fcller, NVIDIA, Z\u00fcrich, Switzer-\nland, tmueller@nvidia.com; Merlin Nimier-David, NVIDIA, Z\u00fcrich, Switzerland,\nmnimierdavid@nvidia.com; Alex Evans, NVIDIA, London, United Kingdom,\nbluespoon@gmail.com; Sanja Fidler, NVIDIA, Toronto, Canada and University of\nToronto, Canada, sfidler.com; Alec Jacobson, University of Toronto, Toronto, Canada\nand Adobe, Toronto, Canada, jacobson@cs.toronto.edu; Alexander Keller, NVIDIA,\nBerlin, Germany, akeller@nvidia.com.\nvectors. In this framework, the lookup functions of existing data structures\ncan be combined by simple arithmetic combinations of their indices, resulting\nin Pareto optimal compression and speed.\nAdditional Key Words and Phrases: Neural graphics primitives, compression.\nQality (PSNR)\nSize (kB)\n102\n103\n104\n26\n28\n30\nInstant NGP\nOurs\n[Rho et al. 2023]\nFig. 2. Size vs. PSNR Pareto curves on the NeRF scene from Figure 1. Our\nwork is able to outperform Instant NGP across the board and performs\ncompetitively with masked wavelet representations [Rho et al. 2023].\narXiv:2312.17241v1  [cs.CV]  28 Dec 2023\n2\n\u2022\nTakikawa et al.\n1\nINTRODUCTION\nThe ever increasing demand for higher fidelity immersive expe-\nriences not only adds to the bandwidth requirements of existing\nmultimedia formats (images, video, etc.), but also fosters in the use\nof higher-dimensional assets such as volumetric video and light field\nrepresentations. This proliferation can be addressed by a unified\ncompression scheme that efficiently represents both traditional and\nemerging multimedia content.\nNeural graphics primitives (NGP) are a promising candidate to\nenable the seamless integration of old and new assets across ap-\nplications. Representing images, shapes, volumetric and spatio-\ndirectional data, they facilitate novel view synthesis (NeRFs) [Milden-\nhall et al. 2020], generative modeling [Lin et al. 2023; Poole et al.\n2023], and light caching [M\u00fcller et al. 2021], among more applica-\ntions [Xie et al. 2022]. Particularly successful are those primitives\nthat represent data by a feature grid that contains trained latent em-\nbeddings to be decoded by a multi-layer perceptron (MLP). Various\nsuch feature grids have been proposed, but they usually come with\na substantial memory footprint [Chabra et al. 2020], even when\nfactorized into low-rank representations [Chen et al. 2022] or rep-\nresented in terms of sparse data structures [Fridovich-Keil et al.\n2022; Liu et al. 2020; M\u00fcller et al. 2022; Takikawa et al. 2021; Yu\net al. 2021]. In part, this limitation has been addressed by methods\nthat learn to index feature vectors [Li et al. 2023; Takikawa et al.\n2022a] and leverage sparse tree structures to avoid storing feature\nvectors in empty space. However, in these methods, index learning\ncauses long training time and maintenance of sparse tree structures\nreduces flexibility.\nOur work, Compact NGP, combines the speed of hash tables and\nthe compactness of index learning by employing the latter as a\nmeans of collision detection by learned probing. We arrive at this\ncombination by casting all feature grids into a common framework:\nthey all correspond to indexing functions that map into a table\nof feature vectors. By simple arithmetic combinations of their in-\ndices, the data structures can be combined in novel ways that yield\nstate-of-the-art compression vs. quality trade-offs. Mathematically,\nsuch arithmetic combinations amount to assigning the various data\nstructures to subsets of the bits of the indexing function\u2014thereby\ndrastically reducing the cost of learned indexing that scales expo-\nnentially in the number of bits.\nOur approach inherits the speed of hash tables while compress-\ning much better\u2014coming close to JPEG when representing images\n(Figure 1)\u2014while remaining differentiable and without relying on\na dedicated decompression scheme such as an entropy code. Com-\npact NGP works across a wide range of user-controllable compres-\nsion rates and provides streaming capabilities where partial results\ncan be loaded in particularly bandwidth-constrained environments.\nThe paper is organized as follows: we review related work and\nits relation to indexing schemes in Section 2 before we introduce\nCompact NGP in Section 3. We demonstrate our method in Section 4\nand discuss extensions, alternatives, and limitations in Section 5\nahead of the conclusion in the last section.\n2\nRELATED WORK AND PRELIMINARIES\nIn this article, we focus on lossy compression as it enables the\nhighest compression rates for the multimedia under consideration.\nWe begin by reviewing traditional techniques before studying the\nconnection between (neural) feature grids and indexing functions.\n2.1\nCompression\nTraditional compression. Lossy compression techniques typically\nemploy transform coding [Goyal 2001] and quantization [Gray and\nNeuhoff 1998] followed by lossless entropy coding such as Huffman\ncodes [1952]. On image and video content, linear transforms such\nas the discrete cosine [Ahmed et al. 1974] and wavelet [Haar 1909]\ntransforms are applied to exploit coefficient sparsity and reduce the\nvisual impact of quantization errors. Rather than transform coding,\nour work learns indices into a feature codebook, which is a form\nof vector quantization [Gray 1984; Wei and Levoy 2000], to find\npatterns in the data.\nTexture compression relies on efficient random access to any part\nof the image without having to first decode the entire compressed\nrepresentation. Most methods perform block-wise compression,\npacking small tiles of the texture into codes of fixed size [Beers et al.\n1996; Reed 2012; Str\u00f6m and Akenine-M\u00f6ller 2005]. Although our\napproach is different, it similarly allows for random access queries\nwithout a decompression step, enabling its potential use for texture\ncompression in real-time renderers where feature grids have already\nshown promise [M\u00fcller et al. 2022; Vaidyanathan et al. 2023].\nVolumetric compression in computer graphics [Balsa Rodr\u00edguez\net al. 2014] similarly uses block-based coding schemes [De Queiroz\nand Chou 2016; Tang et al. 2018, 2020; Wang et al. 2021]. Taking\ninto account the often hierarchical structure of sparsity, subdivision-\nbased spatial data structures such as trees additionally improve\ncompression such as in OpenVDB [Museth 2021; Museth et al. 2019].\nBy contrast, our work combines index learning and hash tables that\nboth do not rely on a subdivision scheme for sparsity.\nNeural compression. In neural image compression, auto-encoder\napproaches use a neural network for transform coding [Ball\u00e9 et al.\n2020, 2018; Theis et al. 2017]. Other works use coordinate-based\nneural representations to fit and compress images as continuous\nvector fields, some without feature grids [Dupont et al. 2022; Lin-\ndell et al. 2022; Song et al. 2015; Str\u00fcmpler et al. 2022] and some\nwith feature grids [Martel et al. 2021; M\u00fcller et al. 2022; Saragadam\net al. 2022]. Although many of these works achieve a better equal-\nquality compression rate than JPEG [Wallace 1992] at low parameter\ncounts, high parameter counts remain challenging. Our method is\nalso a feature-grid and coordinate-based representation, yet per-\nforms competitively with JPEG across a wider range of qualities;\nsee Figure 7.\nCoordinate-based neural representations are additionally appli-\ncable to volumetric and spatio-directional data; most commonly\nNeRFs [Mildenhall et al. 2020]. Without feature grids, Bird et al.\n[2021] minimize the entropy of a multi-layer perceptron (MLP) and\nLu et al. [2021] apply vector quantization directly to the MLP pa-\nrameters. Such pure MLP methods usually have high computational\ncost and poor quality as compared to MLPs augmented by feature\nCompact Neural Graphics Primitives with Learned Hash Probing\n\u2022\n3\ngrids, so our work instead focuses on compressing feature grids\nwhile keeping the MLP sufficiently small to be fast.\n2.2\nFeature Grids in the Framework of Lookup Functions\nLet us formalize feature grid methods in the following framework:\nthey train a feature codebook \ud835\udc37\ud835\udc53 \u2208 R\ud835\udc41\ud835\udc53 \u00d7\ud835\udc39 of \ud835\udc41\ud835\udc53 \ud835\udc39-dimensional\nfeature vectors that are associated with a conceptual grid in the\n\ud835\udc51-dimensional application domain. The mapping from grid vertices\nv = (\ud835\udc630, \ud835\udc631, . . .) \u2208 Z\ud835\udc51 to feature vectors is established by a lookup\nfunction \ud835\udc53 (v) that indexes into the codebook, denoted by \ud835\udc37\ud835\udc53 [\u00b7].1\nDense grids. The canonical feature grid is a dense Cartesian grid2,\nvisualized in Figure 3 (a), that establishes a one-to-one correspon-\ndence of grid vertices to feature vectors, given for \ud835\udc51 = 3 as\n\ud835\udc53 (v) = \ud835\udc37\ud835\udc53 [\ud835\udc630 + \ud835\udc600 \u00b7 (\ud835\udc631 + \ud835\udc601 \u00b7 \ud835\udc632)] ,\n(1)\nwhere the scale s = (\ud835\udc600,\ud835\udc601, . . .) defines the resolution of the grid.\nDense grids cannot adapt to sparsity in the data which makes them\nundesirable in practice. For example, in 3D surface reconstruction\nthe number of dense grid vertices is O\u0000\ud835\udc5b3\u0001 while the surfaces to\nbe reconstructed only intersect O\u0000\ud835\udc5b2\u0001 cells. Therefore, practition-\ners either combine dense grids with classic sparsification methods\nsuch as transform coding [Isik et al. 2022] or they choose more\nsophisticated indexing schemes that will be discussed next.\n\ud835\udc58-plane methods. [Chan et al. 2022; Chen et al. 2022; Fridovich-\nKeil et al. 2023; Peng et al. 2020] project the dense grid along \ud835\udc58\nsets of one or more axes as shown in Figure 3 (b), and combine the\nresulting lower-dimensional (but still dense, usually planar) lookups\narithmetically, e.g.\n\ud835\udc53 (v) = \ud835\udc37\ud835\udc53 [\ud835\udc630 + \ud835\udc600 \u00b7 \ud835\udc631] \u00b7 \ud835\udc37\ud835\udc53 [\ud835\udc600 \u00b7 \ud835\udc601 + \ud835\udc632] \u00b7 \ud835\udc37\ud835\udc53 [. . .] + . . . .\n(2)\nSpecial cases of this scheme are equivalent to tensor decompositions\nof the dense grid [Chen et al. 2022]. While \ud835\udc58-planes ensure fewer\nthan O\u0000\ud835\udc5b\ud835\udc51\u0001 parameters, they makes the strong assumption that\nsparsity in the data can be well explained by axis aligned projections\nthat are decoded by the MLP. In practice, this is not always the\ncase, necessitating application-specific tricks such as bounding box\ncropping [Chen et al. 2022] or transform coding of the projected\ngrids [Rho et al. 2023] for better compression.\nSpatial hashing. Contrasting with the axis aligned parameter col-\nlisions of \ud835\udc58-planes, spatial hashing [Teschner et al. 2003] distributes\nits collisions uniformly across lookups\n\ud835\udc53 (v) = \ud835\udc37\ud835\udc53 [hash(v) mod \ud835\udc41\ud835\udc53 ] ,\nhash(v) =\n\ud835\udc51\u22121\n\u00ca\n\ud835\udc56 = 0\n\ud835\udc63\ud835\udc56 \u00b7 \ud835\udf0b\ud835\udc56 ,\n(3)\nwhere \u2295 is the binary XOR operation and \ud835\udf0b\ud835\udc56 are large prime num-\nbers (optionally, \ud835\udf0b0 = 1). Well designed hash functions have the\nbenefit that the lookups always uniformly cover the codebook \ud835\udc37\ud835\udc53 ,\nregardless of the underlying shape of the data, permitting sparsity\nto be learned independently of the data and thus application [M\u00fcller\n1Many methods maintain multiple codebooks at different resolutions, each with its own\nlookup function [M\u00fcller et al. 2022; Takikawa et al. 2022a, 2021], the values of which are\ncombined before being fed to the MLP. Furthermore, most methods invoke the lookup\nfunctions at several grid vertices to compute continuous outputs by interpolation [Liu\net al. 2020; Takikawa et al. 2021].\n2Other tilings, such as permutohedral lattices [Rosu and Behnke 2023], are also possible.\n(b) Factorization\n0\n1\n(a) Dense grid\n0\n1\n(c) Tree\n0\n1\n(d) Hash\nhash\n(\n)\nv = \n0\n1\n(e) Learned indexing\n0\n1\n0\n1\n0\n1\nindexing\nFig. 3. Various indexing schemes mapping integer grid coordinates v =\n(\ud835\udc630, \ud835\udc631, . . .) to feature vectors have been proposed, including (a) dense grids,\n(b) \ud835\udc58-planes, (c) sparse grids and trees, (d) spatial hashing, and (e) learned\nindexing. Since each scheme ultimately computes an index into a codebook\nof feature vectors, the schemes can be combined by arithmetic operations\non the indices they produce. Our method combines deterministic hashing\nand a learned indexing as visualized in Figure 4.\net al. 2022]. But hashing also comes with the significant downside\nof \u201cscrambling\u201d the entries of the learned codebook \ud835\udc37\ud835\udc53 (now a\nhash table), precluding structure-dependent post processing such\nas generative modelling or transform coding.\nSubdivision. Some applications [Chabra et al. 2020; Kim et al.\n2022; Martel et al. 2021; Takikawa et al. 2021] construct a sparse\nhierarchical data structure such as a tree whose nodes hold indices\ninto the feature codebook:\n\ud835\udc53 (v) = \ud835\udc37\ud835\udc53 [tree_index(v)] .\n(4)\nUnfortunately, many tasks are ill-suited to such a subdivision scheme,\nfor example image compression where subdivision heuristics are\ndifficult to design or 3D reconstruction where sparsity is unknown\na priori and only emerges during optimization [Fridovich-Keil et al.\n2022; Liu et al. 2020]. Furthermore, unlike the indexing schemes\nabove, tree traversal involves cache-unfriendly pointer chasing and\ntherefore incurs a non-negligible performance overhead.\nLearning the indexing function. Rather than designing the index-\ning function by hand, it can also be learned from data [Li et al.\n2023; Takikawa et al. 2022a]. In these methods, an index codebook\n\ud835\udc37\ud835\udc50 \u2208 N\ud835\udc41\ud835\udc50 holds the lookup indices into the feature codebook and is\nin turn indexed by one of the methods above. For example, VQAD\n[Takikawa et al. 2022a] has the lookup function\n\ud835\udc53 (v) = \ud835\udc37\ud835\udc53\n\u0002\n\ud835\udc37\ud835\udc50 [tree_index(v)]\n\u0003\n,\n(5)\nwhere \ud835\udc37\ud835\udc50 is trained by softmax-weighted3 indexing into all en-\ntries of \ud835\udc37\ud835\udc53 . This is expensive even for moderately sized feature\ncodebooks (and prohibitive for large ones) but has no inference\noverhead and results in over 10\u00d7 better compression than spatial\nhashing. The compression is not quite as effective as a combination\nof \ud835\udc58-plane methods with transform coding [Rho et al. 2023] but has\n3The softmax function \ud835\udf0e : R\ud835\udc51 \u2192 R\ud835\udc51 is defined as \ud835\udf0e\ud835\udc56 (x) = \ud835\udc52\ud835\udc65\ud835\udc56 /\u00cd\n\ud835\udc57 \ud835\udc52\ud835\udc65\ud835\udc57 .\n4\n\u2022\nTakikawa et al.\nhash\nhash2\np\nmax\nBit concatenation\n\u2207softmax\nIndexing function f (  )v\nv=(   ,    )\n0\n1\nMLP\ninterpolation\nGradients\nOutput\nIndexing codebook\nFeature codebook\nFig. 4. Overview of Compact NGP. For a given input coordinate x \u2208 R\ud835\udc51 (far left), we find its enclosing integer grid vertices v \u2208 Z\ud835\udc51 and apply our indexing\nfunction \ud835\udc53 (v) to each one. The most significant bits of the index are computed by a spatial hash (hash) and the least significant bits by looking up a row of\n\ud835\udc41\ud835\udc5d confidence values from an indexing codebook b\ud835\udc37\ud835\udc50 that is in turn indexed by an auxiliary spatial hash (hash2), and then picking the index with maximal\nconfidence (green arrow). Bitwise concatenation of the two indices yields an index for looking up from the feature codebook \ud835\udc37\ud835\udc53 , which is subsequently\n\ud835\udc51-linearly interpolated per x and fed into an MLP. For optimization, we propagate gradients as if the indexing codebook used a softmax instead of a hard\nmaximum, i.e. we use a \u201cstraight-through\u201d estimator [Bengio et al. 2013]. In practice, after each training step, we bake this log2 \ud835\udc41\ud835\udc5d-bit indices of the maximal\nvalues in each row of b\ud835\udc37\ud835\udc50 into an auxiliary indexing codebook \ud835\udc37\ud835\udc50 that is both compact and allows for more efficient forward evaluation of the model.\nthe advantage that it can be cheaply queried without in-memory\ndecompression to a larger representation.\nCombining methods. Using the framework of lookup functions\nwe can relate our method to previous work: we combine learned\nindexing with spatial hashing by arithmetically combining their\nindices. The most significant bits of our index come from Instant\nNGP\u2019s hash encoding [M\u00fcller et al. 2022] and the least significant\nbits are learned by a variation of VQAD [Takikawa et al. 2022a].\nThus, our method performs learned probing for collision resolution\nand information reuse in analogy to classic hash table probing meth-\nods [Knuth 1963]. This will be motivated and explained in the next\nsection.\n3\nMETHOD\nOur goal is to minimize the number of parameters \ud835\udf03 and \u03a6 of a\nmulti-layer perceptron \ud835\udc5a(\ud835\udc66; \u03a6) and its corresponding input encod-\ning \ud835\udc66 = \ud835\udf13 (\ud835\udc65;\ud835\udf03) without incurring a significant speed penalty. Fur-\nthermore, we want to remain application agnostic and therefore\navoid structural modifications such as tree subdivision and trans-\nform codings that may depend on application-specific heuristics.\nHence, we base our method on Instant NGP\u2019s multi-resolution\nhash encoding [M\u00fcller et al. 2022] and generalize its indexing func-\ntion, Eq. (3), by introducing learned probing. In our lookup function,\nthe spatial hash produces the most significant bits of the index, while\nthe remaining user-configurable log2 \ud835\udc41\ud835\udc5d least significant bits are\nlearned within an auxiliary index codebook \ud835\udc37\ud835\udc50 \u2208 {0, 1, . . . , \ud835\udc41\ud835\udc5d \u2212 1}\ud835\udc41\ud835\udc50\nthat is in turn indexed by a second spatial hash (one that uses differ-\nent prime numbers from the first). The lookup function is illustrated\nin Figure 4 and given for a single grid vertex by\n\ud835\udc53 (v) = \ud835\udc37\ud835\udc53\n\u0002\u0000\ud835\udc41\ud835\udc5d \u00b7 hash(v)\u0001 mod \ud835\udc41\ud835\udc53 + \ud835\udc37\ud835\udc50 [hash2(v)]\n\u0003\n.\n(6)\nIntuitively, the index codebook \ud835\udc37\ud835\udc50, sparsified by the second spa-\ntial hash, learns to probe the feature codebook over \ud835\udc41\ud835\udc5d values for\ncollision resolution and information re-use. The index codebook\u2019s\nTable 1. Hyperparameters of our method and recommended ranges. We\ninherit most parameters from Instant NGP [M\u00fcller et al. 2022] and introduce\ntwo additional ones pertaining to the index codebook. Gray parameters are\nunaffected by our method and therefore set to the same values as in Instant\nNGP; the choice of remaining parameters is explained in Section 3.\nSource\nParameter\nSymbol\nValue\nnew in\nour method\nIndex probing range\n\ud835\udc41\ud835\udc5d\n21 to 24\nIndex codebook size\n\ud835\udc41\ud835\udc50\n210 to 224\ninherited from\nInstant NGP\nFeature codebook size\n\ud835\udc41\ud835\udc53\n26 to 212\nFeature dimensionality\n\ud835\udc39\n2\nNumber of levels\n\ud835\udc3f\n16\nCoarsest resolution\n\ud835\udc41min\n16\nFinest resolution\n\ud835\udc41max\n512 to 524288\nNum. hidden neurons\n\ud835\udc41neurons\n64\nsize \ud835\udc41\ud835\udc50 as well as its probing range \ud835\udc41\ud835\udc5d are hyperparameters of our\nmethod that extend those inherited from Instant NGP; see Table 1.\nFollowing Takikawa et al. [2022a], we maintain two versions of\nthe index codebook: one for training b\ud835\udc37\ud835\udc50 \u2208 R\ud835\udc41\ud835\udc50 \u00d7\ud835\udc41\ud835\udc5d that holds con-\nfidence values for each of the \ud835\udc41\ud835\udc5d features in the probing range, and\none for inference \ud835\udc37\ud835\udc50 \u2208 {0, 1, . . . , \ud835\udc41\ud835\udc5d \u2212 1}\ud835\udc41\ud835\udc50 that holds log2 \ud835\udc41\ud835\udc5d-bit\ninteger indices corresponding to the probe offset with largest confi-\ndence. Compared to Instant NGP, the only inference-time overhead\nis the index lookup from \ud835\udc37\ud835\udc50. Furthermore, our smaller parameter\ncount leads to improved cache utilization; we hence achieve similar\nand in some cases better inference performance as shown in Table 2.\nTraining. In the forward pass we use \ud835\udc37\ud835\udc50 to look up the feature\nwith largest confidence and in the backward pass we distribute gradi-\nents into all features within the probing range, weighted by the soft-\nmax of their confidence values from b\ud835\udc37\ud835\udc50 (see Figure 4). This strategy\nof combining a discrete decision in the forward pass with continuous\ngradients in the backward pass is also known as a \u201cstraight-through\u201d\nestimator that helps to learn hard non-linearities [Bengio et al. 2013].\nBy keeping the learned number of bits log2 \ud835\udc41\ud835\udc5d small, we limit the\nnumber of features and confidence values that need to be loaded\nCompact Neural Graphics Primitives with Learned Hash Probing\n\u2022\n5\n102\n103\nQality (PSNR)\nSize (kB)\nNf = 214\nNf = 212\nNf = 210\nNf = 28\nNf = 26\n30\n35\n40\n45\nNp = 22\nNp = 24\nNp = 26\nPNG File Size\nInstant NGP\nCurves for increasing Nc  from 28 to 218\nRelative gains are \nlower as Nf  increase \nand collisions decrease\nNp = 20\nFig. 5. PSNR vs. file size for varying hyperparameters in compressing the Kodak image dataset. We sweep three parameters: the feature codebook size \ud835\udc41\ud835\udc53\n(colors), the index codebook size \ud835\udc41\ud835\udc50 (curves ranging from 212 to 220), and the probing range \ud835\udc41\ud835\udc5d (dashing and dotting). A value of \ud835\udc41\ud835\udc5d = 1 corresponds to\nInstant NGP (shown as \u2605) and has no curve because it is invariant under \ud835\udc41\ud835\udc50. We see that the optimal curve at a given file size \ud835\udc41 has a feature codebook size\n(same-colored \u2605) of roughly \ud835\udc41\ud835\udc53 = 1/3\ud835\udc41 and index codebook size \ud835\udc41\ud835\udc50 = 2/3\ud835\udc41. Small probing ranges (solid curves) are sufficient for good compression\u2014in-fact\noptimal for small values of \ud835\udc41\ud835\udc50 (left side of curves)\u2014but larger probing ranges (dashed and dotted curves) yield further small improvements for large values of\n\ud835\udc41\ud835\udc50 (right side of curves) at the cost of increased training time.\nQality (PSNR)\nSize (kB) 101\n102\n103\n104\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\n\ud441\ud45d = 22\n\ud441\ud45d = 24\nNf = 214\nNf = 212\nNf = 210\nNf = 28\n[Bird et al. 2021]\n[Li et al. 2023]\n[Rho et al. 2023]\n[Gordon et al. 2023]\nOurs\nNp = 20\nInstant NGP\n69.67 kB, 26.25 dB \n486.32 kB, 32.14 dB \nFig. 6. PSNR vs. file size for varying hyperparameters in compressing the\nNeRF Lego digger. The layout is the same as Figure 5. We also show rendered\nimages of our compressed representation at two quality settings.\nin the backward pass. And since the learned bits are the least sig-\nnificant ones, their corresponding features lie adjacent in memory,\nusually located in the same cache line and thereby incurring only a\nmoderate training overhead of 1.2\u20132.6\u00d7 (see Table 2) while realiz-\ning compression rates on par with the orders of magnitude slower\nVQAD [Takikawa et al. 2022a].\nSelecting hyperparameters. Recall that our method inherits its\nhyperparameters from Instant NGP and introduces two new ones:\nthe index codebook size \ud835\udc41\ud835\udc50 and its probing range \ud835\udc41\ud835\udc5d; see Table 1 for\na complete list. To find quality-maximizing parameters, we recom-\nmend the following scheme inspired by Figures 5 and 6, which we\nTable 2. Training and inference time overheads of Compact NGP. Training\ntimes are measured for an iteration of training on the NeRF Lego digger\ndataset. Inference times are for 218 lookups on a single multiresolution level.\nThe relative training overhead (denoted with \ud835\udc5b\u00d7) is measured with respect\nto Instant NGP (\ud835\udc41\ud835\udc53 = 216), ranging from 1.2\u20132.6\u00d7. The largest impact on\nspeed has the probing range \ud835\udc41\ud835\udc5d, whereas \ud835\udc41\ud835\udc50 (shown) and \ud835\udc41\ud835\udc53 (see M\u00fcller\net al. [2022]) only have a weak effect.\nMethod\n\ud835\udc41\ud835\udc53\n\ud835\udc41\ud835\udc50\n\ud835\udc41\ud835\udc5d\nTraining time\nper iteration\nInference time\nfor 218 lookups\nQuality\n(PSNR dB)\nI NGP\n216\nn/a\n20\n5.4 ms\n28.7\ud835\udf07s\n33.60 dB\n214\nn/a\n20\n5.1 ms\n13.7\ud835\udf07s\n32.00 dB\n28\nn/a\n20\n4.5 ms\n9.8\ud835\udf07s\n19.04 dB\nOurs\n28\n212\n22\n6.8ms (1.26\u00d7)\n10.1\ud835\udf07s\n26.25 dB\n28\n216\n22\n6.8 ms (1.26\u00d7)\n10.1\ud835\udf07s\n31.58 dB\n28\n212\n23\n8.3 ms (1.53\u00d7)\n10.1\ud835\udf07s\n27.13 dB\n28\n216\n23\n8.5 ms (1.57\u00d7)\n10.2\ud835\udf07s\n32.58 dB\n28\n212\n24\n12.7 ms (2.35\u00d7)\n10.2\ud835\udf07s\n27.67 dB\n28\n216\n24\n14.1 ms (2.61\u00d7)\n10.2\ud835\udf07s\n33.24 dB\nuse in all our following results. First, set \ud835\udc41\ud835\udc50 = 1 and \ud835\udc41\ud835\udc5d = 1, turning\nthe method into Instant NGP as indicated by \u2605 in the figure. Second,\nset the feature codebook size \ud835\udc41\ud835\udc53 according to the desired lower\nbound on the compressed size. Third, double \ud835\udc41\ud835\udc50 until a reasonable\nmaximum value (usually \ud835\udc41\ud835\udc50 = 216). Lastly, if even higher quality\nis desired, double \ud835\udc41\ud835\udc53 . The remaining parameter \ud835\udc41\ud835\udc5d can be tuned\nto taste, as this parameter governs how expensive the training is,\nbut a higher value tends to produce slightly better Pareto tradeoffs\nbetween size and quality.\n4\nRESULTS\nWe have implemented our algorithm on top of the version of In-\nstant NGP in the PyTorch-based Kaolin Wisp library [Takikawa\net al. 2022b]. Computationally expensive operations like sparse grid\nray tracing and feature grid lookups of both Instant NGP and our\nmethod are accelerated by custom CUDA kernels called from Py-\nTorch. All results are measured on an NVIDIA RTX 6000 Ada GPU.\n6\n\u2022\nTakikawa et al.\nPerformance. Table 2 lists inference and training times of our\nmethod on the NeRF Lego digger from Figure 6. Compared to In-\nstant NGP, our 1.2\u20132.6\u00d7 training overhead scales with the prob-\ning range \ud835\udc41\ud835\udc5d, confirming the analysis in Section 3 and exposing a\ntrade-off between training speed and compression to the user. Since\nthe compression benefit of larger probing ranges quickly falls off,\nwe cap \ud835\udc41\ud835\udc5d \u2264 24 in all our experiments, manifesting the worst-case\noverhead of 2.6\u00d7. An important performance consideration for train-\ning is the accumulation of gradients into the feature codebook \ud835\udc37\ud835\udc53 .\nSince our method uses very small codebooks \ud835\udc41\ud835\udc53 \u2208 [26, 212], special\ncare must be taken on massively parallel processors, such as GPUs,\nto first accumulate gradients in threadblock-local memory before\nbroadcasting them into RAM. This avoids contention that would\notherwise make training \u223c7\u00d7 slower.\nTable 2 also demonstrates that Compact NGP has faster inference\nthan Instant NGP at roughly equal quality settings. This is because\nour method has a much smaller size (\ud835\udc41\ud835\udc53 = 216 vs. \ud835\udc41\ud835\udc53 = 28, \ud835\udc41\ud835\udc50 = 216)\nand thereby fits better into caches. The only inference overhead of\nour method is the additional index lookup from \ud835\udc37\ud835\udc50, which we find\nnegligible (0.4\ud835\udf07s at \ud835\udc41\ud835\udc53 = 28).\nImage compression. Figure 7 shows the quality vs. size tradeoff of\nour method on the Kodak image dataset, which consists of 24 images\nof 768\u00d7512 pixels. The figure also shows JPEG as well as prior\ncoordinate MLP methods. On this dataset, our method performs\nclose to JPEG at small file sizes and worse at larger ones. At small file\nsizes, our representation is dominated by floating point parameters\nlike the MLP and the feature codebook, causing competing methods\nthat apply quantization on top of pure MLPs [Dupont et al. 2021;\nStr\u00fcmpler et al. 2022] to compress better. However, these methods do\nnot scale to higher quality targets (\u223c35dB and above) as it is difficult\nto train pure MLPs to such qualities. To demonstrate the better\nscaling of our method, we investigate a much larger 8000\u00d78000\nimage of Pluto in Figure 8 on which we outperform both JPEG\non most practical sizes (\u223cmegabyte) and prior neural large-scale\nmethods (Instant NGP [M\u00fcller et al. 2022] and ACORN [Martel et al.\n2021]) at high quality settings. Our method is also evaluated against\ntexture compression methods in Table 4.\nNeRF compression. We evaluate NeRF compression on a real-world\nscene in Figures 1 and 2 as well as synthetic scenes [Mildenhall et al.\n2020] in Figure 6 (Lego) and Table 3 (full dataset). We compare with\nseveral contemporary NeRF compression techniques that are mostly\nbased on TensoRF [Chen et al. 2022]. We report numbers from the\noriginal papers where available. For the real world scene, we ran\nmasked wavelets [Rho et al. 2023] as a strong and recent baseline.\nIn both scenes, we outperform Instant NGP in terms of quality vs.\nsize. On the synthetic scene (Figure 6), our Pareto front lies slightly\nbelow the specialized baselines that use scalar quantization and\ncoding, and in the real-world scene our Pareto front is competitive\n(Figure 2) despite our work requiring neither.\nThe zoom-ins in Figure 1 reveal distinct visual artifacts of the\ndifferent methods, even though their PSNR is the same. Masked\nwavelets [Rho et al. 2023] produce blurrier results whereas Com-\npact NGP yields a sharper reconstruction with high frequency noise\nsimilar to that of Instant NGP.\nQality (PSNR)\n101 Size (kB)\n102\n103\n25\n30\n35\n40\nInstant NGP\nOurs\nJPEG\n[Str\u00fcmpler et al. 2022]\nCOIN\nFig. 7. PSNR vs. file size on the Kodak image dataset using parameters\n\ud835\udc41\ud835\udc53 = 26 and \ud835\udc41\ud835\udc5d = 24 and varying \ud835\udc41\ud835\udc50 (blue curve ranging from 212 to 220).\nOn this dataset, our method performs close to JPEG at small file sizes and\nworse at larger ones. At small file sizes, our representation is dominated by\nfloating point parameters like the MLP and the feature codebook. Compet-\ning methods that quantize pure MLPs perform better in this regime [Dupont\net al. 2021; Str\u00fcmpler et al. 2022], whereas we omit quantization for simplic-\nity and flexibility. At visually pleasant targets (\u223c35dB and above) these prior\nworks do not scale as it is difficult to train pure MLPs to such qualities.\nQality (PSNR)\nSize (kB)\nReference\n57098 kB\nJPEG\n35.34 PSNR\n2740 kB (21\u00d7)\nOurs\n35.06 PSNR\n1122 kB (51\u00d7) \n102\n103\n104\n25\n30\n35\n40\n45\nNf = 26\nNf = 212\nNf = 218\nNf = 224\nACORN\nNp = 20\nNp = 24\nOurs\nJPEG\nPNG File Size\nInstant NGP\nFig. 8. We fit Compact NGP to the 8000\u00d78000px Pluto image using pa-\nrameters \ud835\udc41\ud835\udc53 = 26 and \ud835\udc41\ud835\udc5d = 24 and varying \ud835\udc41\ud835\udc50 (green curve ranging from\n212 to 224). We show that we are able to outperform JPEG on a wide range\nof quality levels. The qualitative comparisons at equal size (insets) show\nthe visual artifacts exhibited by different methods: while JPEG has color\nquantization arfitacts, ours appears slightly blurred.\nAdditional hyperparameter ablations. Aside from the feature code-\nbook size \ud835\udc41\ud835\udc53 , we inherit the default hyperparameters of Instant NGP\nfor a apples-to-apples comparisons. To verify that these defaults\nare reasonable, we sweep the number of multiresolution levels \ud835\udc3f in\nFigure 9 and the number of hidden neurons \ud835\udc41neurons in Figure 10.\nThe default values \ud835\udc3f = 16 and \ud835\udc41neurons = 64 perform well for a large\nCompact Neural Graphics Primitives with Learned Hash Probing\n\u2022\n7\n102\n30\n35\n40\nL = 4\nL = 8\nL = 16\nQality (PSNR)\nSize (kB)\nFig. 9. Impact of the number of multiresolution levels \ud835\udc3f on PSNR vs. size. We\nuse the parameters \ud835\udc41\ud835\udc53 = 26 and \ud835\udc41\ud835\udc5d = 24 while varying \ud835\udc41\ud835\udc50 (curve ranging\nfrom 212 to 220) and \ud835\udc3f on the image compression task from Figure 1. The\ndefault value \ud835\udc3f = 16 (inherited from Instant NGP) performs well for a large\nrange of sizes, particularly in the hundreds of kB range that is most practical.\nYet, a lower number of levels results in a better Pareto curve at smaller\nsizes that could be used if one wanted to compete with MLP based image\ncompression techniques; cf. Figure 7.\nQality (PSNR)\nSize (kB)\n101\n102\n30\n35\n40\nNneurons= 16\nNneurons= 32\nNneurons= 64\nFig. 10. Impact of the MLP width \ud835\udc41neurons on PSNR vs. size. The parameter\nsweeps over \ud835\udc41\ud835\udc53 , \ud835\udc41\ud835\udc5d, and \ud835\udc41\ud835\udc50 are the same as Figure 9. A similar conclusion\ncan be drawn: the default value \ud835\udc41neurons = 64 (inherited from Instant NGP)\nperforms well at practical sizes, whereas a better Pareto front can be achieved\nat smaller sizes.\nrange of sizes, particularly in the hundreds of kB range that is most\npractical. Yet, lower values produce better Pareto frontiers at very\nsmall file sizes that could be used if one wanted to compete with\nMLP based image compression techniques; cf. Figure 7. However,\nwe believe that the hundreds of kB range is more relevant in practice\nand we therefore stick to the default values for simplicity.\n5\nDISCUSSION AND FUTURE WORK\nCompact NGP has been designed with content distribution in mind\nwhere the compression overhead is amortized and decoding on user\nequipment must be low cost, low power, and multi-scale for graceful\ndegradation in bandwidth-constrained environments. As an exam-\nple, NeRFs may be broadcasted and decoded on large numbers of\nend-user devices, possibly in real-time to enable live streaming video\nNeRFs. More generally, (learnable) compression codecs will enable\nthe next generation of immersive content of which live streaming of\nNeRFs are just an example and other applications, like video game\ntexture compression and volumetric video, being right around the\ncorner.\nQuality and compression artifacts. Beyond measuring PSNR, it is\nworth studying the qualitative appearance of compression artifacts\nwith our method. Compared to JPEG, our method appears to produce\nless ringing at the cost of a small amount of additional blur, whereas\nin NeRF our methods looks similar to Instant NGP: sharp, but with\nhigh-frequency noise. This is in contrast to Rho et al. [2023], who\nproduce a smoother yet blurry reconstruction; see Figure 1. Since\nwe measure error in terms of PSNR, which is based on the L2 error,\nblurry results yield lower error than the human visual system might\nexpect [Zhao et al. 2016].\nFrom float to int. Our method shifts the storage cost from being\nfloat-dominated to int-dominated. In the settings we test in, we\nsee that this tradeoff is favorable, particularly because our integers\nhave only log2 \ud835\udc41\ud835\udc5d bits\u2014many fewer than than even 16-bit half pre-\ncision floats. We have additionally investigated several methods that\nreduce the entropy of our learned indices (e.g. through additional\nterms in the loss), coupled to entropy coding, but so far with little\nsuccess that does not warrant forfeiture of random access lookups.\nAlternatively, data-adaptive quantization of floats may reduce the\nbit count further than using an index codebook, but better training\nstrategies are required to this end. We believe that further research\ninto data-adaptive float quantization as well as int entropy mini-\nmization will be fruitful.\nEntropy coding. Our method was inspired by a method that has\nspatial locality built-in [Takikawa et al. 2022a] (i.e. the index code-\nbook represented by a tree). Such spatial locality could be exploited\nby an entropy coder much better than the spatial hash table that\nwe use. We chose spatial hashing for being agnostic of the appli-\ncation [M\u00fcller et al. 2022]\u2014and it performs competitively with\ntransform and entropy coded prior work nonetheless\u2014but if fu-\nture research could devise local data structures that have the same\nflexibility and performance as hash tables, it will likely be worth-\nwhile to utilize those instead of hashing.\nAlternatives to straight-through estimators. In our work we use\nthe softmax function along with the straight-through estimator to\nlearn indexing. While effective, this can be computationally expen-\nsive for large indexing ranges as this requires backpropagation on\nall possible indices. As such, it may be worthwhile to explore the\nvarious sparse [Laha et al. 2018; Martins and Astudillo 2016; Peters\net al. 2019] and stochastic [Lee et al. 2018; Paulus et al. 2020] variants\nhave been proposed in the literature. Proximity-based indexing such\nas locality-sensitive hashing and the nearest-neighbour queries used\nin VQ-VAE [Van Den Oord et al. 2017] may be relevant as well.\n6\nCONCLUSION\nWe propose to view feature grids and their associated neural graph-\nics primitives through a common lens: a unifying framework of\nlookup functions. Within this framework it becomes simple to mix\nmethods in novel ways, such as our Compact NGP that augments\nefficient hash table lookups with low-overhead learned probing.\nThe result is a state-of-the-art combination of compression and\nperformance while remaining agnostic to the graphics application\nin question. Compact NGP has been designed with real-world use\ncases in mind where random access decompression, level of detail\nstreaming, and high performance are all crucial (both in training\nand inference). As such, we are eager to investigate its future use\nin streaming applications, video game texture compression, live-\ntraining as in radiance caching, and many more.\n8\n\u2022\nTakikawa et al.\nACKNOWLEDGMENTS\nThe Lego Bulldozer scene of Figure 6 was created by Blendswap\nuser Heinzelnisse. The Pluto image of Figure 8 was created by\nNASA/Johns Hopkins University Applied Physics Laboratory/Southwest\nResearch Institute/Alex Parker. We thank David Luebke, Karthik\nVaidyanathan, and Marco Salvi for useful discussions throughout\nthe project.\nREFERENCES\nNasir Ahmed, T. Natarajan, and Kamisetty R. Rao. 1974. Discrete cosine transform.\nIEEE transactions on Computers 100, 1 (1974), 90\u201393.\nJohannes Ball\u00e9, Philip A. Chou, David Minnen, Saurabh Singh, Nick Johnston, Eirikur\nAgustsson, Sung Jin Hwang, and George Toderici. 2020. Nonlinear transform coding.\nIEEE Journal of Selected Topics in Signal Processing 15, 2 (2020), 339\u2013353.\nJohannes Ball\u00e9, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. 2018.\nVariational image compression with a scale hyperprior. In International Conference\non Learning Representations. https://openreview.net/forum?id=rkcQFMZRb\nMarcos Balsa Rodr\u00edguez, Enrico Gobbetti, Jose Antonio Iglesias Guitian, Maxim\nMakhinya, Fabio Marton, Renato Pajarola, and Susanne K. Suter. 2014. State-of-\nthe-art in compressed GPU-based direct volume rendering. In Computer Graphics\nForum, Vol. 33. Wiley Online Library, 77\u2013100.\nAndrew C. Beers, Maneesh Agrawala, and Navin Chaddha. 1996. Rendering from\ncompressed textures. In Proceedings of the 23rd Annual Conference on Computer\nGraphics and Interactive Techniques. 373\u2013378.\nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating\ngradients through stochastic neurons for conditional computation. arXiv preprint\narXiv:1308.3432 (2013).\nThomas Bird, Johannes Ball\u00e9, Saurabh Singh, and Philip A. Chou. 2021. 3D Scene Com-\npression through Entropy Penalized Neural Representation Functions. In 2021 Picture\nCoding Symposium (PCS). 1\u20135. https://doi.org/10.1109/PCS50896.2021.9477505\nRohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Love-\ngrove, and Richard Newcombe. 2020. Deep local shapes: Learning local SDF priors\nfor detailed 3D reconstruction. In ECCV. Springer, 608\u2013625.\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De\nMello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. 2022. Efficient Geometry-aware 3D Generative\nAdversarial Networks. In CVPR.\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022. TensoRF:\nTensorial Radiance Fields. In European Conference on Computer Vision (ECCV).\nRicardo L. De Queiroz and Philip A. Chou. 2016. Compression of 3D point clouds using\na region-adaptive hierarchical transform. IEEE Transactions on Image Processing 25,\n8 (2016), 3947\u20133956.\nEmilien Dupont, Adam Goli\u0144ski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet.\n2021. COIN: COmpression with Implicit Neural representations. ICLR 2021 Neural\nCompression Workshop Spotlight, arXiv preprint arXiv:2103.03123 (2021).\nEmilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Golinski, Yee Whye Teh, and\nArnaud Doucet. 2022. COIN++: Neural compression across modalities. Transactions\non Machine Learning Research 2022, 11 (2022).\nSara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k Warburg, Benjamin Recht, and\nAngjoo Kanazawa. 2023. K-Planes: Explicit Radiance Fields in Space, Time, and\nAppearance. In CVPR.\nSara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and\nAngjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In\nCVPR.\nCameron Gordon, Shin-Fang Chng, Lachlan MacDonald, and Simon Lucey. 2023. On\nQuantizing Implicit Neural Representations. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision. 341\u2013350.\nVivek K. Goyal. 2001. Theoretical foundations of transform coding. IEEE Signal\nProcessing Magazine 18, 5 (2001), 9\u201321.\nRobert M. Gray. 1984. Vector quantization. IEEE ASSP Magazine 1, 2 (1984), 4\u201329.\nRobert M. Gray and David L. Neuhoff. 1998. Quantization. IEEE Transactions on\nInformation Theory 44, 6 (1998), 2325\u20132383.\nAlfred Haar. 1909. Zur Theorie der orthogonalen Funktionensysteme. Georg-August-\nUniversit\u00e4t, G\u00f6ttingen.\nDavid A. Huffman. 1952. A method for the construction of minimum-redundancy codes.\nProceedings of the IRE 40, 9 (1952), 1098\u20131101.\nBerivan Isik, Philip A. Chou, Sung Jin Hwang, Nick Johnston, and George Toderici.\n2022. LVAC: Learned Volumetric Attribute Compression for Point Clouds using\nCoordinate Based Networks. Frontiers in Signal Processing 2 (2022). https://doi.org/\n10.3389/frsip.2022.1008812\nDoyub Kim, Minjae Lee, and Ken Museth. 2022. NeuralVDB: High-resolution Sparse\nVolume Representation using Hierarchical Neural Networks. (2022). https://doi.org/\n10.48550/arXiv.2208.04448\nDonald\nKnuth.\n1963.\nNotes\non\n\u201cOpen\u201d\nAddressing.\nhttps://web.archive.org/web/20160303225949/http://algo.inria.fr/AofA/Research/11-\n97.html.\nAnirban Laha, Saneem Ahmed Chemmengath, Priyanka Agrawal, Mitesh Khapra,\nKarthik Sankaranarayanan, and Harish G. Ramaswamy. 2018. On Controllable\nSparse Alternatives to Softmax. In Advances in Neural Information Processing Systems,\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett\n(Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/\npaper/2018/file/6a4d5952d4c018a1c1af9fa590a10dda-Paper.pdf\nHae Beom Lee, Juho Lee, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. 2018. Drop-\nMax: Adaptive Variational Softmax. In Advances in Neural Information Processing\nSystems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett (Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/\npaper_files/paper/2018/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf\nLingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. 2023. Compressing\nVolumetric Radiance Fields to 1 MB. (June 2023), 4222\u20134231.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,\nKarsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3D: High-\nResolution Text-to-3D Content Creation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR). 300\u2013309.\nDavid B. Lindell, Dave Van Veen, Jeong Joon Park, and Gordon Wetzstein. 2022. Bacon:\nBand-limited Coordinate Networks for Multiscale Scene Representation. In 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 16231\u2013\n16241. https://doi.org/10.1109/CVPR52688.2022.01577\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020.\nNeural Sparse Voxel Fields. In Advances in Neural Information Processing Systems,\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran\nAssociates, Inc., 15651\u201315663. https://proceedings.neurips.cc/paper_files/paper/\n2020/file/b4b758962f17808746e9bb832a6fa4b8-Paper.pdf\nYuzhe Lu, Kairong Jiang, Joshua A. Levine, and Matthew Berger. 2021. Compressive\nNeural Representations of Volumetric Scalar Fields. Computer Graphics Forum 40, 3\n(2021), 135\u2013146. https://doi.org/10.1111/cgf.14295\nJulien N.P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro,\nand Gordon Wetzstein. 2021. ACORN: Adaptive Coordinate Networks for Neural\nRepresentation. ACM Trans. Graph. (SIGGRAPH) (2021).\nAndr\u00e9 F. T. Martins and Ram\u00f3n F. Astudillo. 2016. From Softmax to Sparsemax: A\nSparse Model of Attention and Multi-Label Classification. In Proceedings of the 33rd\nInternational Conference on International Conference on Machine Learning - Volume\n48 (New York, NY, USA) (ICML\u201916). JMLR.org, 1614\u20131623.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing scenes as neural radiance fields\nfor view synthesis. In European conference on computer vision. Springer, 405\u2013421.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neu-\nral Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans. Graph.\n41, 4, Article 102 (July 2022), 15 pages. https://doi.org/10.1145/3528223.3530127\nThomas M\u00fcller, Fabrice Rousselle, Jan Nov\u00e1k, and Alexander Keller. 2021. Real-time\nNeural Radiance Caching for Path Tracing. ACM Trans. Graph. 40, 4, Article 36 (Aug.\n2021), 16 pages. https://doi.org/10.1145/3450626.3459812\nKen Museth. 2021. NanoVDB: A GPU-friendly and portable VDB data structure for\nreal-time rendering and simulation. In ACM SIGGRAPH 2021 Talks. 1\u20132.\nKen Museth, Nick Avramoussis, and Dan Bailey. 2019. OpenVDB. In ACM SIGGRAPH\n2019 Courses. 1\u201356.\nMax Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris J. Maddison. 2020.\nGradient Estimation with Stochastic Softmax Tricks. In Advances in Neural Informa-\ntion Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin\n(Eds.), Vol. 33. Curran Associates, Inc., 5691\u20135704. https://proceedings.neurips.cc/\npaper_files/paper/2020/file/3df80af53dce8435cf9ad6c3e7a403fd-Paper.pdf\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.\n2020. Convolutional Occupancy Networks. In European Conference on Computer\nVision (ECCV).\nBen Peters, Vlad Niculae, and Andr\u00e9 F. T. Martins. 2019. Sparse Sequence-to-Sequence\nModels. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Florence, Italy, 1504\u20131519.\nhttps://doi.org/10.18653/v1/P19-1146\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion:\nText-to-3D using 2D Diffusion. In The Eleventh International Conference on Learning\nRepresentations. https://openreview.net/forum?id=FjNys5c7VyY\nNathan Reed. 2012.\nUnderstanding BCn Texture Compression Formats.\nhttps://www.reedbeta.com/blog/understanding-bcn-texture-compression-\nformats/. Online; accessed 24 January 2023.\nDaniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, and\nEunbyung Park. 2023. Masked Wavelet Representation for Compact Neural Radiance\nFields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR). 20680\u201320690.\nRadu Alexandru Rosu and Sven Behnke. 2023. PermutoSDF: Fast Multi-View Reconstruc-\ntion with Implicit Surfaces using Permutohedral Lattices. In IEEE/CVF Conference\nCompact Neural Graphics Primitives with Learned Hash Probing\n\u2022\n9\non Computer Vision and Pattern Recognition (CVPR).\nVishwanath Saragadam, Jasper Tan, Guha Balakrishnan, Richard G. Baraniuk, and\nAshok Veeraraghavan. 2022. MINER: Multiscale Implicit Neural Representation.\nIn Computer Vision \u2013 ECCV 2022, Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9,\nGiovanni Maria Farinella, and Tal Hassner (Eds.). Springer Nature Switzerland,\nCham, 318\u2013333.\nYing Song, Jiaping Wang, Li-Yi Wei, and Wencheng Wang. 2015. Vector regression\nfunctions for texture compression. ACM Transactions on Graphics (TOG) 35, 1 (2015),\n1\u201310.\nJacob Str\u00f6m and Tomas Akenine-M\u00f6ller. 2005.\niPACKMAN: High-quality, low-\ncomplexity texture compression for mobile phones. In Proceedings of the ACM\nSIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware. 63\u201370.\nYannick Str\u00fcmpler, Janis Postels, Ren Yang, Luc Van Gool, and Federico Tombari. 2022.\nImplicit neural representations for image compression. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part\nXXVI. Springer, 74\u201391.\nTowaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas M\u00fcller, Morgan McGuire,\nAlec Jacobson, and Sanja Fidler. 2022a. Variable bitrate neural fields. In ACM\nSIGGRAPH 2022 Conference Proceedings. 1\u20139.\nTowaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek\nNowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2021. Neural\ngeometric level of detail: Real-time rendering with implicit 3D shapes. In CVPR.\n11358\u201311367.\nTowaki Takikawa, Or Perel, Clement Fuji Tsang, Charles Loop, Joey Litalien, Jonathan\nTremblay, Sanja Fidler, and Maria Shugrina. 2022b. Kaolin Wisp: A PyTorch library\nand engine for neural fields research.\nDanhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan\nTaylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, et al. 2018. Real-\ntime compression and streaming of 4D performances. ACM Transactions on Graphics\n(TOG) 37, 6 (2018), 1\u201311.\nDanhang Tang, Saurabh Singh, Philip A. Chou, Christian Hane, Mingsong Dou, Sean\nFanello, Jonathan Taylor, Philip Davidson, Onur G. Guleryuz, Yinda Zhang, et al.\n2020. Deep implicit volume compression. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 1293\u20131303.\nMatthias Teschner, Bruno Heidelberger, Matthias M\u00fcller, Danat Pomeranets, and\nMarkus Gross. 2003. Optimized Spatial Hashing for Collision Detection of De-\nformable Objects. In Proceedings of VMV\u201903, Munich, Germany. 47\u201354.\nLucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Husz\u00e1r. 2017. Lossy\nImage Compression with Compressive Autoencoders. In International Conference\non Learning Representations. https://openreview.net/forum?id=rJiNwv9gg\nKarthik Vaidyanathan, Marco Salvi, Bartlomiej Wronski, Tomas Akenine-M\u00f6ller, Pontus\nEbelin, and Aaron Lefohn. 2023. Random-Access Neural Compression of Material\nTextures. In Proceedings of SIGGRAPH.\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning.\nAdvances in neural information processing systems 30 (2017).\nGregory K. Wallace. 1992. The JPEG still picture compression standard. IEEE transactions\non consumer electronics 38, 1 (1992), xviii\u2013xxxiv.\nJianqiang Wang, Hao Zhu, Haojie Liu, and Zhan Ma. 2021. Lossy Point Cloud Ge-\nometry Compression via End-to-End Learning. IEEE Transactions on Circuits and\nSystems for Video Technology 31, 12 (2021), 4909\u20134923.\nhttps://doi.org/10.1109/\nTCSVT.2021.3051377\nLi-Yi Wei and Marc Levoy. 2000. Fast texture synthesis using tree-structured vector\nquantization. In Proceedings of the 27th annual conference on Computer graphics and\ninteractive techniques. 479\u2013488.\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan,\nFederico Tombari, James Tompkin, Vincent sitzmann, and Srinath Sridhar. 2022.\nNeural Fields in Visual Computing and Beyond. Computer Graphics Forum 41, 2\n(2022), 641\u2013676. https://doi.org/10.1111/cgf.14505\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021.\nPlenOctrees for Real-time Rendering of Neural Radiance Fields. In ICCV.\nHang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. 2016. Loss Functions for Image\nRestoration With Neural Networks. IEEE Transactions on Computational Imaging\nPP (12 2016), 1\u20131. https://doi.org/10.1109/TCI.2016.2644865\n10\n\u2022\nTakikawa et al.\nTable 3. Quantiative results on the full synthetic dataset from Mildenhall et al. [2020], showing a near-quality (PSNR) comparison between Instant NGP and\nour work. We see that we are able to achieve similar quality across the entire dataset with a 2.8\u00d7 more compact representation.\nMethod\n\ud835\udc41\ud835\udc53\n\ud835\udc41\ud835\udc50\n\ud835\udc41\ud835\udc5d\nMic\nFicus\nChair\nHotdog\nMaterials\nDrums\nShip\nLego\navg.\nSize (kB)\nI NGP\n214\nn/a\n20\n35.08\n30.99\n32.59\n34.99\n28.73\n25.36\n27.71\n32.03\n30.93\n1000 kB\nOurs\n28\n23\n216\n33.88\n32.08\n32.05\n34.26\n28.32\n24.71\n27.71\n32.31\n30.66\n357 kB\nTable 4. Quantiative results on texture compression on the Paving Stones texture set, retrieved from https://ambientcg.com, showing the tradeoff between\nquality (PSNR) and size (kB) for different methods. We compare against traditional texture compression baselines (BC) as well as recent neural baselines\n(NTC [Vaidyanathan et al. 2023]). We borrow the results from Vaidyanathan et al. [2023]. Although our work does not outperform NTC, which uses a\nspecialized architecture for textures with quantization, we are still able to outperform BC and Instant NGP at similar size. We only report average across all\nchannels for BC as that was the only data available, and compare against the NTC results without mipmaps (which increase quality) for fair comparison.\nMethod\nQuantization\n\ud835\udc41\ud835\udc53\n\ud835\udc41\ud835\udc50\n\ud835\udc41\ud835\udc5d\nDiffuse\nNormal\nRoughness\nAO\nDisplacement\navg.\nSize (kB)\nI NGP\n216\nn/a\nn/a\n21.58\n22.32\n26.79\n27.72\n35.62\n24.75\n3761 kB\nI NGP\n214\nn/a\nn/a\n19.91\n20.51\n26.61\n25.56\n30.07\n22.61\n1049 kB\nBC\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\nn/a\n23.25\n3500 kB\nNTC\nn/a\nn/a\nn/a\n26.10\n27.17\n29.37\n31.22\n40.59\n29.00\n3360 kB\nOurs\n210\n220\n23\n24.02\n25.00\n27.90\n29.94\n36.18\n26.69\n3494 kB\nOurs\n28\n218\n23\n21.55\n22.61\n26.94\n27.43\n33.74\n24.51\n1173 kB\n"
  },
  {
    "title": "Hyper-VolTran: Fast and Generalizable One-Shot Image to 3D Object Structure via HyperNetworks",
    "link": "https://arxiv.org/pdf/2312.16218.pdf",
    "upvote": "6",
    "text": "Hyper-VolTran: Fast and Generalizable\nOne-Shot Image to 3D Object Structure via HyperNetworks\nChristian Simon\nSen He\nJuan-Manuel P\u00b4erez-R\u00b4ua\nMengmeng Xu\nAmine Benhalloum\nTao Xiang\nMeta\nAbstract\nSolving image-to-3D from a single view is an ill-posed\nproblem, and current neural reconstruction methods address-\ning it through diffusion models still rely on scene-specific\noptimization, constraining their generalization capability.\nTo overcome the limitations of existing approaches regarding\ngeneralization and consistency, we introduce a novel neu-\nral rendering technique. Our approach employs the signed\ndistance function (SDF) as the surface representation and in-\ncorporates generalizable priors through geometry-encoding\nvolumes and HyperNetworks. Specifically, our method builds\nneural encoding volumes from generated multi-view inputs.\nWe adjust the weights of the SDF network conditioned on\nan input image at test-time to allow model adaptation to\nnovel scenes in a feed-forward manner via HyperNetworks.\nTo mitigate artifacts derived from the synthesized views, we\npropose the use of a volume transformer module to improve\nthe aggregation of image features instead of processing each\nviewpoint separately. Through our proposed method, dubbed\nas Hyper-VolTran, we avoid the bottleneck of scene-specific\noptimization and maintain consistency across the images\ngenerated from multiple viewpoints. Our experiments show\nthe advantages of our proposed approach with consistent\nresults and rapid generation.\n1. Introduction\nRecent progress in neural 3D reconstruction has brought sig-\nnificant implications in various applications, e.g., novel view\nsynthesis [1, 32, 38, 39], and robotic vision [10, 13, 28, 37].\nSpecifically, there has been a growing interest in neural\nfields [1, 39, 44] to extract 3D information from multiple\nimages given known camera parameters. NeRF [19] and\nthe Signed Distance Function (SDF) [43] are image-to-3D\nreconstruction techniques [1, 2, 36] that produce plausible\ngeometry and, hence, novel views. Despite great progress,\nachieving accurate 3D object reconstruction via neural im-\nplicit methods [1, 19, 43] still requires a substantial number\nof images featuring consistent view and appearance and pre-\nPoint-e\nShap-e\nZero123+SD\nOne2345\nHyper-VolTran (Ours)\n10\n100\n1000\n0.9\n1.1\n1.3\n1.5\n1.7\n1.9\n2.1\nRunning Time (sec)\nChamfer Distance (L2)\nSynthesize \nMulti-Views\ncamera\nInconsistent\nNeural Encoding Volume\nMesh\nRGB\nSDF Weight Generator\nHyper-VolTran\nTransformer\nand \nSDF Network\nInput\nImage\nOur Pipeline\nComparison\nFigure 1. Top: Comparison of our proposed method against base-\nlines on the running time and Chamfer Distance with the bubble\narea indicating IoU. Bottom: Our pipeline comprises two compo-\nnents for image-to-3D by synthesizing multi-views from a diffusion\nmodel and mapping from multi-views to SDFs using an SDF net-\nwork with weights generated from a HyperNetwork.\ncise camera poses to reconstruct 3D objects accurately.\nIn fact, collecting data from multiple views might not\nalways be feasible when the resources are limited. Several\nworks [2, 17, 44] demonstrate a capability to mitigate issues\non 3D reconstruction under a sparse set of images. One key\ntechnique in these approaches is to build neural encoding\nvolume projected from multiple input views. Though these\ntechniques can perform on limited inputs, reconstructing\n3D from a single image remains challenging and requires a\nstrong prior to enabling the neural reconstruction model to\nproduce plausible shapes and colors of unseen perspectives.\nA recent development in generative models [4, 16, 26, 45]\nhas shown promising results in 2D image generation that\ncan act as a strong prior for unseen perspectives. Several\n1\narXiv:2312.16218v2  [cs.CV]  5 Jan 2024\nworks approach this problem using the guidance of a diffu-\nsion model [21]. In particular, Poole et al. [21] introduce\nScore Distillation Sampling (SDS) [21] in which the neural\nreconstruction model learns through the feedback error from\na diffusion model. The diffusion model is frozen without\nany updates while the NeRF [19] weights are updated dur-\ning optimization. Even though this technique is capable of\nreconstructing 3D scenes, per-scene optimization is still re-\nquired, which usually takes up to 1 hour to converge on a\nsingle GPU. This constraint restricts the practicality of this\napproach, particularly when it comes to efficiently perform-\ning 3D reconstruction. To achieve fast 3D reconstruction,\na generalized prior that allows one feed-forward operation\nthrough the networks is required instead of relying on an\nexpensive per-scene optimization.\nAn alternative method for rapid 3D reconstruction is to\nutilize a diffusion model and synthesize multi-view images.\nThis can be achieved by leveraging a diffusion model that can\nproduce images based on slight variations in camera param-\neters [16]. Nevertheless, creating images using a multi-view\nimage generator (e.g., Zero123 [16]) can be challenging in\nterms of preserving geometry consistency. Rather than op-\ntimizing a network for each object as in [21], we aim to\npreserve only one network to generalize for many objects.\nTo achieve this, we can exploit neural encoding volume built\nfrom the projection of image features with known camera\nparameters as in [2, 17, 36]. While these approaches show\npromise, they still suffer from suboptimal results when em-\nployed for 3D reconstruction involving unseen objects.\nIn this work, we aim to address the aforementioned chal-\nlenges, focusing on generalization, speed, and inconsistency\nissues. To this end, we introduce a neural network to ad-\ndress these concerns by employing an SDF network gen-\nerated by HyperNetworks [9] and a Volume Transformer\n(VolTran) to alleviate the impact of inconsistent examples.\nOur approach explores the potential for generalization by\nintroducing a latent variable obtained from an image encoder\n(e.g., CLIP [23]) to yield image representations. Subse-\nquently, we employ these image representations to generate\nthe weights of the SDF, addressing the challenge of general-\nization. Please see Fig. 1 (bottom) for an illustration of our\ntechnique. To summarize, our contributions include:\n1. We propose a generalizable prior for 3D mesh reconstruc-\ntion with a few synthesized data by assigning the weights\nof SDFs based on the input image embedding.\n2. We propose a transformer module for aggregation to en-\nable working on inconsistent shapes and colors across\ndifferent viewpoints.\n3. We also show that our method only requires one feed-\nforward process and comfortably constructs a 3D mesh\nwith negligible additional processing time \u223c5 seconds.\n2. Related Work\nDiffusion models for 2D to 3D reconstruction.\nRecon-\nstructing a full 3D structure from only a few 2D images is\nchallenging due to the inherent ill-posedness of the prob-\nlem. However, recent advances in generative models and, in\nparticular, diffusion models provide a promising direction\ntoward obtaining the priors about the 3D world that are nec-\nessary to reconstruct the full 3D structure of an object from\na single image. For example, they are used as an indirect\nway to provide feedback during the image-to-3D reconstruc-\ntion process in [3, 18, 21, 33, 35]. A notable work so-called\nDreamFusion [21] proposes text-to-3D generation by Score\nDistillation Sampling (SDS), which allows optimization-\nguided generation of NeRF-parametrized [19] 3D scenes. A\nconcurrent work using Score Jacobian Chaining [35] uses\na similar approach, exploiting the chain rule on the outputs\nof a pretrained image generation model. Tang et al. [33] ex-\ntend the idea with coarse and refining stages to enhance the\noutputs with textured point clouds. Recently, Zero123 [16]\ndescribes a diffusion model that takes an input image and\ncamera parameters to synthesize a novel view. This model\ncan generate more consistent multi-view images compared\nto an off-the-shelf diffusion model like Imagen [26]. Albeit a\npromising direction to reconstruct 3D models, per-scene op-\ntimization is still required and the neural implicit function is\nlimited to represent only one object. Thus, the generalization\nof the trained model is limited for unseen objects.\nGeneralizable priors for fast 3D reconstruction.\nAn\nideal implementation of 3D reconstruction is a single model\nthat can generalize to unseen objects, enabling 3D genera-\ntion using a forward-pass approach only without applying\nfurther per-scene optimization. PixelNeRF [44] as a pioneer\nwork in this direction proposes to extract feature volumes\nfrom an input image which are then passed through a NeRF\nmodel along with the camera extrinsic parameters. Chen\net al. [2] present an approach called MVSNeRF using cost\nvolumes built of warped 2D image features and then regress\nvolume density with a pass through an MLP (i.e., neural\nencoding volumes) as the base geometry. Then, the neural\nencoding volume is used as an additional input to the NeRF\nmodel. SparseNeus [17] extends MVSNeRF [2] to work\non a few-data regime by proposing cascaded geometry rea-\nsoning to refine the details of a 3D object. However, this\napproach still requires multi-view inputs, with no obvious\nmechanism to extend it to a single image. To tackle the\nproblem of 3D reconstruction from a single image, Liu et\nal. [15] propose a method called One2345 to exploit a diffu-\nsion model (e.g., Zero123 [16]) to generate some example\nimages with estimated camera poses. To improve the preci-\nsion of the reconstructed geometric models, One2345 [15]\nemploys SDFs [43] rather than NeRFs [19]. The challenge\nof this approach is inconsistency in generated examples,\n2\nmaking it difficult to reconstruct 3D scenes that fully respect\nthe input appearance.\nAnother approach for avoiding per-scene optimization is\nto train a large-scale model with self-supervised learning and\nmake use of large-scale labeled text-to-3D data. Point-e [20],\na system to generate 3D point clouds from text descrip-\ntion, is a pioneer in this direction. Following up this work,\nShap-e [11] directly generates the weights of the neural im-\nplicit model that can be rendered as meshes and radiance\nfields. This method generates multiple synthetic images\nthen a neural 3D reconstruction technique (e.g., SDF [43] or\nNeRF [19]) is employed to produce 3D models. This model\ncuts the cost of image-to-3D reconstruction from several\nGPU hours to 1-2 minutes. While this method can produce\nresults quickly, the quality of the reconstructed 3D surfaces\nremains subpar. Unlike all these prior works, our proposed\nmethod can generate accurate 3D reconstruction with com-\npetitive processing time (i.e., less than 1 minute).\nContext-based learning.\nIn few-shot learning, the con-\ncept of leveraging contextual information for achieving\noptimal performance across diverse input conditions is a\nwell-established idea, as indicated by previous works like\n[7, 9, 29\u201331, 40]. Some of these methods involve model\nparameter updates through gradient descents, exemplified\nby several works [7, 46]. However, these approaches still re-\nquire multiple feed-forward operations to update the model.\nOur focus lies in developing an approach that accomplishes\ncontext understanding with just a single feed-forward opera-\ntion, without the need for additional optimization steps. To\nachieve this, we opt to adopt context-based information by\ngenerating neural network weights. Specifically, we draw\ninspiration from HyperNetworks [9] designated to generate\nneural network weights based on the provided context.\n3. Proposed Method\nOur 3D neural reconstruction pipeline has two streams, as\nshown in Fig. 2. Given a single-view image and its depth\nmap, we first synthesize multi-view images via a diffusion\nmodel. Then, as shown in the upper stream of the figure, the\nsynthesized images are fed into a neural encoding volume\nto obtain the 3D geometry representation of its structure.\nThe geometry representation is combined with the images to\npredict a rendered RGB map by our proposed transformer\nmodule, VolTran. Meanwhile, we also use the synthesized\nmulti-view images in a HyperNetwork to estimate an SDF\nweight, shown in the bottom stream. The SDF network\npredicts SDFs for surface representations that will later be\nused for rendering the depth map and extracting the mesh.\nTherefore, we name our approach Hyper-VolTran.\n3.1. One to multiple-view images\nWe begin our pipeline by leveraging a pretrained generative\nmodel. This enables us to expand a single input image into\nmultiple views from a broader set of object viewpoints, albeit\nwith some imperfections. For fair comparison, we strictly\nfollow the approach outlined in [16] to leverage elevation\nand azimuth conditioning.\nSynthesized views.\nGiven a single RGB image and its\ncorresponding depth map denoted as I \u2208 RH\u00d7W \u00d73, and\nD \u2208 RH\u00d7W , respectively, we follow Zero123 [16] to nor-\nmalize its shape and use a spherical camera system for the\ndepth map. We apply an off-the-shelf image generation\nmodel to create N RGB images and depth maps sampled\nuniformly from several viewpoints according to ground-truth\ncamera parameters [15]. Concretely for training, we form a\nset of RGB images and depth maps of an object as the source\nset I = {I1, \u00b7 \u00b7 \u00b7 , IN} and D = {D1, \u00b7 \u00b7 \u00b7 , DN}. Note that\nboth RGB and depth images are used as training targets to\nsupervise the model in the training stage. However, those\ndepth maps are omitted in the testing phase.\n3.2. Geometry-Aware Encoding\nGeometry-aware encoding is essential in building a general-\nized method for surface prediction from multi-view images.\nOur approach employs neural encoding volumes [2, 41] to\nconstruct 3D geometry based on the diversified input views\nfrom Sec. 3.1 and their associated camera poses. To this end,\nwe warp 2D image features from the N input images onto a\nlocalized plane situated within the reference view\u2019s frustum.\nNeural encoding volume.\nIn deep multi-view stereo [41,\n42], 3D geometry can be inferred in the form of Cost Vol-\nume construction. Let f\u03b8 : RH\u00d7W \u00d73 \u2192 RH\u00d7W \u00d7C be the\nmapping from an input image to a feature map. Similar\nto [17, 41], we encode images using a Feature Pyramid Net-\nwork [14] as the mapping function to extract a neural feature\nmap, i.e., F i = f\u03b8(Ii). Besides, we partition the scene\u2019s\nbounding volume into a grid of voxels. Then, along with\nthe intrinsic and extrinsic camera parameters P = [K, R, t]\nfor each image Ii, the neural feature map is projected based\non each vertex v, and the output is denoted as F i(\u03a0i(v)),\nwhere \u03a0i(v) projects v \u2208 R3 onto the local plane by apply-\ning P [41]. In particular, the homography warping is applied\nfor each view i, and the final neural encoding volume G can\nbe computed as Eq. 1.\nG = \u03d5\n\u0010\nVar\n\u0000{F i(\u03a0i(v))}N\ni=1\n\u0001\u0011\n.\n(1)\nHere Var({F i(\u03a0i(v))}N\u22121\ni=0 ) is the Cost Volume, Var means\nthe variance over N viewpoints, and \u03d5 denotes a function\nresponsible for regularizing and propagating scene informa-\ntion instantiated as a sparse 3D CNN (i.e., Geometry Guided\nEncoding). Since the variance accommodates differences in\nthe image appearance among multiple input perspectives, G\nacquires the ability to encode complex 3D scene geometry\n3\nv\nv\nv\nInput Image\nNoisy \nand \nInconsistent\nGenerate \nimages\nCost Volume\n360o\nCamera Poses\nImage Encoder\nEncode image\nSDF\nVolTran \u2013 Transformer Module\nGeometry \nGuided \nEncoding\nHyperNetworks\nGenerated Views\nRendered RGB Rendered Depth\nAssign \nweights\nSDF Network\n\u2112Depth\nGT\nDensity\nAggregate from inconsistent inputs\nMulti-view \nGenerator\nEstimate\n\u2112RGB\nCamera\n1\n2\n3\nGenerate weights\n4\nFigure 2. Our training pipeline starts from a single image. Expanding a single view to an image set using a viewpoint-aware\ngeneration model, our method employs supervised learning with RGB and depth regression losses. Specifically, 1) Utilizing N RGB\nimages and depth maps, we generate additional viewpoints and camera poses. 2) Geometry-Guided Encoding is derived from warped image\nfeatures in the form of a Cost Volume. 3) Instead of test-time optimization, we obtain SDF weights with a single pass of a HyperNetwork\nmodule, considering image appearance through visual encoding. 4) The geometry-encoded volume and the image features are passed to the\nSDF network and a transformer module to reveal the complete 3D object structure. Hence, our method Hyper-VolTran encompasses quick\nadaption to novel inputs thanks to our HyperNetwork design and consistent structures from global attention.\nand appearance from diversified images. Thus, these volume\nfeatures contain appearance-aware information that can be\nlater used for volume rendering and SDF predictions.\n3.3. Volume Rendering\nA neural encoding volume previously computed is employed\nto predict both the density and view-dependent radiance\nat arbitrary locations within a scene. Next, this facilitates\nthe utilization of differentiable volume rendering to predict\nthe colors of images. For volume rendering, we opt to use\nSDF [43] instead of NeRF [19] for a more accurate surface\nreconstruction.\nSigned Distance Function (SDF).\nSDFs represent 3D sur-\nfaces using a positional function that provides the nearest\ndistance to the surface. Given an arbitrary 3D location in our\nsetup, we use an MLP f\u03a8 : Rd \u2192 R as an SDF to represent\n3D surfaces. Although the generic SDF input has d = 3 as\nthe signed distance is associated with a point z \u2208 R3, our\nmethod uses a higher d as the input consists of the concate-\nnation of feature from neural encoding volumes, colors, and\nimage features. Another limitation of the generic SDF is\nthe lack of generalization ability. For example, when using\nthe neural encoding volume as an input, we can train an\nSDF network on a large collection of 3D objects [2, 17] to\navoid per-scene optimization. In testing, however, the SDF\nnetwork is usually frozen [15, 17] and limited to the known\nobjects. We propose a more adaptable approach to dynami-\ncally assign MLP\u2019s weights based on the generated outputs\nof a HyperNetworks [9], which is conditioned on the input\nimage.\nHyperNetworks\nfor\nan\nSDF\nnetwork.\nHyperNet-\nworks [9] constitute a neural model that generates the\nweights for a target network designed to generalize on vari-\nous tasks given a context. Rather than preserving a neural\nnetwork fixed during test time, HyperNetwork offers a mech-\nanism to assign weights based on a condition dynamically.\nMathematically, we design a HyperNetwork module \u03b4l(.) to\nproduce the weight for each layer \u03c8l of the SDF network\nf\u03a8:\n\u03c8l = \u03b4l(\u03be(I1)).\n(2)\nTo encode the input image, we use a pretrained image en-\ncoder \u03be that reduces the image dimensionality from RGB\nspace to a latent space. Unlike the past work [6] that needs to\noptimize neural networks for every single object, our method\ntrains the module on the fly without requiring per-scene opti-\nmization and directly calculating losses between two neural\nnetwork parameters. Since our condition is the feature repre-\nsentation of the input object, our HyperNetwork can produce\na more dedicated and appropriate weight for its target net-\nwork. On the other hand, as we utilize the output of the\nHypernetwork [9] to assign weights to the SDF network, our\nmodel generalizes better on the new object during inferences,\n4\nespecially when the object shares similar semantics with the\ntraining data. Moreover, the hypernetworks are directly up-\ndated with a loss from RGB and depth map in our pipeline.\nThus, we do not have to store the individual optimal weight\nparameter after per-scene optimization.\nRendering from SDFs.\nTo estimate the parameters of the\nneural SDF and color field, we adopt a volume rendering\nmethod from NeuS [36] to render colors and volumes based\non the SDF representations. For a given pixel, we describe\nM emitted rays from that pixel as {p(t) = o + tv|t \u2265 0},\nwith o being the camera\u2019s focal point and r representing\nthe ray\u2019s unit direction. We feed the combined features\nthrough an MLP and employ the softmax function to derive\nthe blending weights denoted as {\u03c9i}N\ni=1. The radiance at\na given point p and viewing direction v is calculated as the\nweighted sum in Eq 3.\n\u02c6c =\nN\nX\ni=1\n\u03c9i.ci,\n(3)\nwhere ci is the color of source view i. Given the radiance,\nour volume rendering strategies is expressed in Eq 4.\n\u02c6\nC =\nM\nX\nj=1\nTj\u03b1j\u02c6cj,\n(4)\n\u03b1j = 1 \u2212 exp[\u2212\nZ tj+1\ntj\n\u03c1(t)dt].\n(5)\nHere, Tj = Qj=1\nk=1(1 \u2212 \u03b1k) is a discrete accumulated trans-\nmittance, \u03b1k is the discrete opacity, and \u03c1(t) denotes opaque\ndensity. The rendered depth map can be derived as Eq. 6:\n\u02c6\nD =\nM\nX\nj=1\nTj\u03b1jtj.\n(6)\nNote the rendering process is fully differentiable; we train\nthe pipeline in a supervised manner so that the model can\npredict the rendered colors \u02c6\nC and depths \u02c6\nD in inference.\nVolTran: multi-view aggregation transformer.\nPixel\ndata is inherently confined to a local context and lacks\nbroader contextual information, frequently leading to in-\nconsistent surface patches, particularly in the case of sparse\ninput data. One trivial solution is to aggregate features across\ndifferent views to capture the projected features from mul-\ntiple views. Unfortunately, the synthesized views might\nbe corrupted due to the flaws in the generative model, a\nsimple aggregation [15, 17, 41] (e.g., average and max. pool-\ning) might fail to render shapes and colors accurately. We\npropose a transformer module called VolTran based on the\nself-attention design in [34] to encode global information\nfrom different N viewpoints. Besides the inputs, we learn an\naggregation token as an extra token to obtain a corresponding\noutput for a target view. Formally, let X \u2208 RN+1\u00d7d be a\nmatrix with rows composed of the tokens from source views\nand the aggregation token by concatenating the feature from\ncolor ci, image feature F i(\u03a0(v)), and volume feature G\nyielding the dimension d. We denote fV (.), fQ(.), fK(.) as\nfunctions to map values, queries, and keys of a transformer\nmodule. Thus, the aggregation operation can be calculated\nby the self-attention module, as shown in Eq. 7:\nAttn(X) = Softmax(A)fV (X),\n(7)\nwhere Ai,j = fQ(Xi)\u22a4fK(Xj)/\u03b3 for all i, j \u2208 [N]. As\nwe apply multi-head attention, it can be formulated as\nMHA(X) = [Attn1(X), \u00b7 \u00b7 \u00b7 , Attn3(X)]W H. We opt to\nuse LayerNorm to normalize the intermediate activations\nand skip connection to stabilize training. The final output\nfrom the transformer module, an MLP, is introduced as a\nmapping function to obtain the blending weight \u03c9i. After-\nwards, the final color can be obtained as in the SDF rendering\npipeline.\n3.4. Training and Inference\nOur framework has several losses to train the model, includ-\ning the HyperNetwork module. Every module is optimized\nin an end-to-end fashion only in the training stage. We define\nour loss for rendered colors with mean squared error w.r.t.\nthe ground-truth Ci:\nLRGB =\n1\n|P|\n|P |\nX\ni=1\n\r\r \u02c6\nCi \u2212 Ci\n\r\r2\n2.\n(8)\nIn addition to the color loss, we also calculate depth predic-\ntions supervised with the following loss:\nLDepth =\n1\n|P1|\n|P1|\nX\ni=1\n\f\f \u02c6\nDi \u2212 Di\n\f\f.\n(9)\nAlso, in order to regularize the SDF values derived from the\nSDF network f\u03a8, we compute the Eikonal loss [8] :\nLEikonal = 1\n|V|\nX\nv\u2208V\n\u0000\u2225\u2207f\u03a8(v)\u22252 \u2212 1\n\u00012,\n(10)\nwhere v is a sampled 3D point and \u2207f\u03b8(v) is the gradient\nrelative to the sample point q. This loss impacts the surface\nsmoothness.\nFurthermore, to empower our framework for generating\nconcise geometric surfaces, we incorporate a sparsity regu-\nlarization term that penalizes uncontrollable surfaces called\na sparse loss [17], expressed as follows:\nLSparse = 1\n|V|\nX\nv\u2208V\nexp\n\u0000\u2212 \u03c4|s(v)|\n\u0001\n,\n(11)\n5\nA colorful \nllama\nA rocking horse\nA wooden \nbear\nA deer statue\nA teapot\nA blue \nice cream\nA brown drawer\nA burger\nA red bird\nAn office chair\nA green fish\nInput\nView 1\nView 2\nView 3\nView 4\nFigure 3. Qualitative results of Hyper-Voltran on text-to-3D colored meshes. The generated images from a diffusion model are used as\ninputs. We only focus on the main object of the input image.\nwhere s(v) is the predicted SDF and \u03c4 is the hyperparameter\nto scale the SDF prediction. To summarize, The total loss is\ndefined as LRGB + LDepth + \u03b21LEikonal + \u03b22LSparse.\nInference.\nDuring inference, there is no more optimiza-\ntion, and only one feed-forward is performed, which reduces\nthe expensive computation to update the models during test-\ning. First, given an input image, we segment the input to\nextract the foreground object. After we obtain the object\nwith clear background (e.g., white color), we synthesize\nmulti-view scenes from the pretrained Zero123 model [16]\nconditioned on the relative change of camera viewpoints.\nThese synthesized images are then employed to generate a\n3D mesh by our proposed method. The inference of our pro-\nposed method only contains feed-forward, thus comfortably\nreducing the computational time compared to the existing\ndistillation methods [18, 21, 27].\n4. Experiments\n4.1. Implementation details\nWe train our models from publicly available data first shared\nby [15], containing 46K synthesized 3D scenes. For the\nbase multi-view generative model, we follow Zero123 [16]\nand keep its weights frozen. Additionally, for the geometry-\nguided encoder, we set the volume encoding size to 96 \u00d7\n96 \u00d7 96 for all of our experiments. For the SDF weight\ngeneration, we employ the CLIP model [23] as the image\nencoder, known for generating dependable representations.\nIn terms of the loss function, we verified that the setting\nproposed by [17] is optimal, i.e., \u03b21 = 0.1 and \u03b22 = 0.02.\nOn the other hand, during inference, we first apply image\nsegmentation to get an accurate cutout of the target object\nusing the Segment Anything Model (SAM) [12]. Then, we\ngenerate 8 key views which are further extended by 4 nearby\nimages each, for a total of 32 viewpoints.\n4.2. Text-to-3D Results\nThe text-to-3D pipeline is performed by using off-the-shelf\ntext-to-image models e.g., [24, 26, 45]. We apply the corre-\nsponding diffusion process conditioned on a given prompt\n(e.g., \u201da wooden bear\u201d) and obtain an image depicting it. To\nhandle unexpected background information, we cut out the\ntarget object from the generated image using SAM [12]. Dif-\nferent views are further synthesized alongside corresponding\ncamera poses using Zero123 [16]. The full set of generated\nimages are fed to our model, constructing neural encoding\nvolume, generating SDF network weights through a Hyper-\nNetwork, and applying global attention, the main compo-\nnents of Hyper-VolTran. Fig. 3 shows results of our method\nacross different views for a given text prompt. It can be ob-\nserved from these images that Hyper-Voltran produces good\nquality meshes that adhere well to corresponding texture,\ngiving a sense of consistency across views.\n4.3. Image-to-3D Results\nWe use a subset of the GSO dataset [5] to quantitatively\nevaluate one-shot image-to-3D mesh, comprising 25 objects\n6\nShap-e\nInput\nZero123\n+SD\nOne2345\nOurs\nPoint-e\nFigure 4. Qualitative comparison on single image to 3D reconstruction with previous works e.g., One2345 [15], Shap-e [11], Point-\ne [20], and Zero123+SD [21]. VolTran offers more consistent and higher-quality results than competitors, generally providing a higher level\nof preservation of input details. Please see our supplementary material for more results and zoomed-in details.\nOurs\nOne234\nInput\nOurs\nOne234\nInput\nMesh\nGenerated Views from Zero123\nGenerated Views from Zero123\nMesh\nGenerated Views from Zero123\nMesh\nOurs\nOne234\nInput\nFigure 5. Examples of inconsistently generated views and comparison of our proposed method against One2345 [15] in generating meshes.\nOne2345 fails to build well-reconstructed meshes when the views are arguably inconsistent and challenging.\nfrom different GSO categories. For evaluating rendering\nquality, we use images from [18], spanning 15 objects.\nQualitative results.\nWe offer qualitative demonstrations\nof our approach and comparison to One2345 [15], Shap-\ne [11], Point-e [20], and Zero123+SD [16] in Fig. 4, showcas-\ning Hyper-Voltran\u2019s efficacy in addressing one-shot image-\nto-3D object reconstruction. For a fair comparison with\nOne2345 [15], we employ the same set of synthesized im-\nages to generate the 3D meshes. We note that One2345 [15]\nshowcases inaccurate and unnatural shapes in Fig. 4. Also,\nwe compare to other feed-forward-only approaches [11, 20].\nPoint-e and Shap-e cannot successfully reconstruct 3D\nmeshes from a single image yielding incorrect colors and\nshapes. Our proposed method is proven robust across a var-\nied set of different objects with higher fidelity and more\naccurate shapes compared to the baselines. We also show\nin Fig. 5 some inconsistencies in generated images from\nZero123 [16] and how our method can robustly construct the\nmeshes compared to the baseline.\nQuantitative results.\nTo evaluate our method and com-\npare against baselines in generating meshes, we use the\nPyTorch3D [25] package to calculate Chamfer distance and\nIterated Closest Point for source and target alignment to\ncompute F-score.\nIn terms of metrics, we follow prior\nworks [15], and [5], and use F-Score, Chamfer L2 distance,\nand intersection-over-union (IoU). These metrics are sum-\n7\nMethod\nF-Score (\u2191) Chamfer L2 (\u2193) IoU (\u2191)\nTime\nPoint-e [20]\n16.45\n1.73\n0.09\n78 secs\nShap-e [11]\n10.10\n1.98\n0.11\n27 secs\nZero123+SD [16]\n14.85\n1.41\n0.21\n15 mins\nOne2345 [15]\n12.00\n1.90\n0.13\n45 secs\nHyper-VolTran (ours)\n17.45\n1.14\n0.22\n45 secs\nTable 1. F-Score, Chamfer L2, IoU, and time comparison to base-\nlines on the GSO dataset [5].\nMethod\nPSNR (\u2191)\nLPIPS (\u2193)\nCLIP Sim. (\u2191)\nPoint-e [20]\n0.98\n0.78\n0.53\nShap-e [11]\n1.23\n0.74\n0.59\nZero123 [16]\n19.49\n0.11\n0.75\nRealFusion [18]\n0.67\n0.14\n0.67\nMagic123 [22]\n19.50\n0.10\n0.82\nOne2345 [15]\n16.10\n0.32\n0.57\nHyper-VolTran (ours)\n23.51\n0.10\n0.86\nTable 2. PSNR, LPIPS, and CLIP similarity comparison to prior\nworks on the collected images in RealFusion [5].\nmarized in Table 1, where Hyper-VolTran proves its im-\nproved generalization capabilities on unseen objects by scor-\ning higher than competitors across all tracks, at reasonable\ncomputation time cost. Similarly, for rendering quality, our\nmethod tops all previous works on 3D rendering across all\nscores: PSNR, LPIPS, and the CLIP similarity score as\nshown in Table 2.\nProcessing Time.\nAlthough our proposed method relies\non encoding the input image through an image embedding\nmodel and generating weights of the SDF network, the full\n3D generation latency is only around 5 seconds on a sin-\ngle A100 GPU. This is on par with the processing time of\nOne2345 [15]. Additional latency is due to the base diffu-\nsion model. In our case, we opt to use Zero123 [16] for the\nsynthesis of additional views, adding on average around 40\nseconds per object. As shown in Table 1, the processing time\nof Shap-e is lower, which results in generally lower quality\nresults than our method.\n4.4. Analysis and Ablations\nThe SDF weight generator via a HyperNetwork and\nVolTran.\nWe investigate the efficacy of our proposed two\nmodules: the HyperNetwork for SDF and VolTran. This\nablation study is performed to analyze the impact of each\nmodule. As shown in Fig. 6, we can observe that rendering\ndeteriorates without the HyperNetwork and Voltran. While\nwithout VolTran, rendering scenes yields some noise as the\nimpact of inconsistent inputs. Using both, we can achieve\nplausible rendering results.\nInput\nVolTran\nHyperNet SDF\nHyperNet SDF\nVolTran\nVolTran\nHyperNet SDF\nFigure 6. Ablation study on each module. Impacts of each module\non rendering colored scenes.\nN=32\nN=4\nN=24\nN=16\nN=8\nFigure 7. Qualitative results with different numbers of samples\ngenerated from a diffusion model. The more images are generated\nfrom the diffusion model, the better shape quality is achieved.\nNumber of samples.\nWe evaluate the generated results by\nvarying numbers of support images obtained from the diffu-\nsion model, ranging from 32 down to 4 images from different\nperspectives. Fig. 7 showcases the impact of the number of\nsamples generated from the diffusion model. Our approach\ngains advantages from an increased number of generated\nimages for forming geometry representations. Conversely,\nan excessively low number of samples leads to degradation.\n5. Conclusions\nIn this paper, we address the challenge of deriving a 3D ob-\nject structure from a single image. Our proposed approach,\ncalled Hyper-VolTran, comprises a HyperNetwork module\nand a transformer module. Specifically, HyperNetworks\ngenerate SDF weights, while the transformer module fa-\ncilitates robust global aggregation from inconsistent multi-\nviews. Our method demonstrates effective generalization to\nunseen objects in the single image-to-3D task, as evidenced\nby both quantitative and qualitative evaluations. Notably, our\napproach rapidly generates 3D meshes, accomplishing this\ntask in just 45 seconds without per-scene optimization. Com-\npared with state-of-the-art methods, our proposed approach\nexcels in both time efficiency and reconstruction accuracy.\n8\nReferences\n[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neural\nradiance fields, 2021. 1\n[2] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance field reconstruction from multi-view stereo.\narXiv preprint arXiv:2103.15595, 2021. 1, 2, 3, 4\n[3] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2023. 2\n[4] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen,\nJiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo\nRosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-\nguided attention for consistent text-to-video editing. arXiv\npreprint arXiv:2310.05922, 2023. 1\n[5] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B. McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items, 2022. 6, 7,\n8\n[6] Ziya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner,\nand Angela Dai.\nHyperdiffusion:\nGenerating implicit\nneural fields with weight-space diffusion. arXiv preprint\narXiv:2303.17015, 2023. 4\n[7] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the 34th International Conference on Ma-\nchine Learning, pages 1126\u20131135, 2017. 3\n[8] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and\nYaron Lipman. Implicit geometric regularization for learning\nshapes. In Proceedings of Machine Learning and Systems\n2020, pages 3569\u20133579, 2020. 5\n[9] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks.\nCoRR, abs/1609.09106, 2016. 2, 3, 4\n[10] Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, and\nChi-Keung Tang. Nerf-rpn: A general framework for object\ndetection in nerfs. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 1\n[11] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional\n3d implicit functions, 2023. 3, 7, 8\n[12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and Ross\nGirshick. Segment anything. arXiv:2304.02643, 2023. 6\n[13] Stanley Lewis, Jana Pavlasek, and Odest Chadwicke Jenkins.\nNARF22: Neural articulated radiance fields for configuration-\naware rendering. In International Conference on Intelligent\nRobots and Systems (IROS). IEEE, 2022. 1\n[14] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross B. Girshick, Kaiming He,\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017. 3\n[15] Minghua Liu,\nChao Xu,\nHaian Jin,\nLinghao Chen,\nMukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:\nAny single image to 3d mesh in 45 seconds without per-shape\noptimization, 2023. 2, 3, 4, 5, 6, 7, 8\n[16] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n9298\u20139309, 2023. 1, 2, 3, 6, 7, 8\n[17] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\nWenping Wang. Sparseneus: Fast generalizable neural surface\nreconstruction from sparse views. ECCV, 2022. 1, 2, 3, 4, 5,\n6\n[18] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi. Realfusion: 360 reconstruction of any object\nfrom a single image. In Arxiv, 2023. 2, 6, 7, 8\n[19] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ra-\nmamoorthi, and R Ng. Nerf: Representing scenes as neural\nradiance fields for view synthesis. In European conference\non computer vision, 2020. 1, 2, 3, 4\n[20] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 3, 7, 8\n[21] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. 2,\n6, 7\n[22] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One image to high-quality 3d object gen-\neration using both 2d and 3d diffusion priors. arXiv preprint\narXiv:2306.17843, 2023. 8\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable vi-\nsual models from natural language supervision.\nCoRR,\nabs/2103.00020, 2021. 2, 6\n[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents, 2022. 6\n[25] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\nGkioxari. Accelerating 3d deep learning with pytorch3d.\narXiv:2007.08501, 2020. 7\n[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nRaphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans,\nJonathan Ho, David J. Fleet, and Mohammad Norouzi. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. In Advances in Neural Information Processing\nSystems, 2022. 1, 2, 6\n[27] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 6\n[28] Anthony Simeonov, Yilun Du, Lin Yen-Chen, , Alberto Ro-\ndriguez, , Leslie P. Kaelbling, Tomas L. Perez, and Pulkit\n9\nAgrawal. Se(3)-equivariant relational rearrangement with\nneural descriptor fields. In Conference on Robot Learning\n(CoRL). PMLR, 2022. 1\n[29] Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash\nHarandi. On modulating the gradient for meta-learning. In\nECCV, 2020. 3\n[30] Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash\nHarandi. Adaptive subspaces for few-shot learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020.\n[31] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. Meta-\nlearning for multi-label few-shot classification. In Proceed-\nings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision (WACV), pages 3951\u20133960, 2022. 3\n[32] Mukund Varma T, Peihao Wang, Xuxi Chen, Tianlong Chen,\nSubhashini Venugopalan, and Zhangyang Wang. Is attention\nall that neRF needs? In The Eleventh International Confer-\nence on Learning Representations, 2023. 1\n[33] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d\ncreation from a single image with diffusion prior, 2023. 2\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. CoRR, abs/1706.03762,\n2017. 5\n[35] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and\nGreg Shakhnarovich. Score jacobian chaining: Lifting pre-\ntrained 2d diffusion models for 3d generation. arXiv preprint\narXiv:2212.00774, 2022. 2\n[36] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nNeurIPS, 2021. 1, 2, 5\n[37] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree,\nThomas Muller, Alex Evans, Dieter Fox, Jan Kautz, and\nStan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d\nreconstruction of unknown objects. CVPR, 2023. 1\n[38] Francis Williams, Zan Gojcic, Sameh Khamis, Denis Zorin,\nJoan Bruna, Sanja Fidler, and Or Litany. Neural fields as learn-\nable kernels for 3d reconstruction. CoRR, abs/2111.13674,\n2021. 1\n[39] Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, and Li\nZhang. S-neRF: Neural radiance fields for street views. In\nThe Eleventh International Conference on Learning Repre-\nsentations, 2023. 1\n[40] Mengmeng Xu, Yanghao Li, Cheng-Yang Fu, Bernard\nGhanem, Tao Xiang, and Juan-Manuel Perez-Rua. Where\nis my wallet? modeling object proposal sets for egocentric\nvisual query localization. arXiv preprint arXiv:2211.10528,\n2022. 3\n[41] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan.\nMvsnet: Depth inference for unstructured multi-view stereo.\nEuropean Conference on Computer Vision (ECCV), 2018. 3,\n5\n[42] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,\nLei Zhou, Tian Fang, and Long Quan.\nBlendedmvs: A\nlarge-scale dataset for generalized multi-view stereo networks.\nComputer Vision and Pattern Recognition (CVPR), 2020. 3\n[43] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. In Advances in\nNeural Information Processing Systems, pages 4805\u20134815.\nCurran Associates, Inc., 2021. 1, 2, 3, 4\n[44] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images. In\nCVPR, 2021. 1, 2\n[45] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell\nHowes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron\nAshual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard\nJames, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi,\nAsli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan.\nScaling autoregressive multi-modal models: Pretraining and\ninstruction tuning. arcXiv:2309.02591, 2023. 1, 6\n[46] Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hof-\nmann, and Shimon Whiteson. Fast context adaptation via\nmeta-learning. 2019. 3\n10\nCVPR\n#1001\nCVPR\n#1001\nCVPR 2024 Submission #1001. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n** Supplementary Material **\nHyper-VolTran: Fast and Generalizable\nOne-Shot Image to 3D Object Structure via HyperNetworks\nAnonymous CVPR submission\nPaper ID 1001\nImage Encoder\nInputs\nNN\nNN\nSDF Network\nPredicted SDF\nValue\nFC \nLayer\nFC \nLayer\nImage Embedding\nGenerate Network \nParameters\nHyperNetwork\nFC Layer\nAssign \nweights\nSingle \nInput Image\nFigure 1. The detail of our HyperNetwork architecture to assign\nweights to the SDF network. The input is obtained from the image\nembedding of an image encoder.\nIn the main manuscript, we have provided the pipeline of\n001\nour work and the In this supplementary material, we provide\n002\nthe details of our method, settings, and additional results.\n003\n1. Details of Our Proposed Modules\n004\nIn the main paper, we mentioned two modules construct-\n005\ning our framework to improve both generalization and con-\n006\nsistency. We would detail out these two modules namely\n007\nHyperNetworks and Voltran in the following sections.\n008\n1.1. HyperNetworks\n009\nThe HyperNetworks [? ] are used in our pipeline to build\n010\nthe SDF network. In each HyperNetwork module for each\n011\nSDF network layer, we build 3 fully-connected layers with\n012\nReLU activations in the intermediate layers.\nIn the first\n013\nlayer, we map from the output dimension of the text em-\n014\nbedding which is 768 to 32 as the dimension of the hidden\n015\nlayer. The input to the HyperNetwork is the image embed-\n016\nding as an output of an image encoder (e.g., CLIP [? ]).\n017\nQ\nK\nV\nMulti-head Attention\nInput Tokens\nGeometric \nFeature\nImage Feature\n+\nRGB\nRendering \nRay\nLayerNorm\nMLP\nConcatenate\nLayerNorm\nMLP\nTransformer \nLayer\nInput\nLinear, No Bias\nLinear, No Bias\nReLU\nOutput\nFigure 2. The detail of our Volume Transformer VolTran archi-\ntecture as an aggregator from the features of multiple views.\nPlease see Fig. 1 for an illustration of our method.\n018\n1.2. VolTran: Multi-View Transformer\n019\nThe transformer module in our framework is used as an ag-\n020\ngregator to reduce the noise impacts in multi-view synthe-\n021\nsized images. The transformer module is fed with input to-\n022\nkens from the feature of each view. Fig. 2 shows the detail\n023\nof each component in the transformer module in VolTran.\n024\nWe set the multi-head number to 5 and 2 layers of the trans-\n025\nformer with self-attention.\n026\n2. Ablation on the Loss Terms\n027\nWe investigate the role of each loss function to produce\n028\nhigh-quality images. We observe that without sparseness\n029\nreguralization term as described in the main paper impact-\n030\ning the capability in generating compact surfaces. As shown\n031\nin Fig. 5, some object surfaces have some defects and im-\n032\n1\narXiv:2312.16218v2  [cs.CV]  5 Jan 2024\nCVPR\n#1001\nCVPR\n#1001\nCVPR 2024 Submission #1001. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nNo Depth Loss\nWith Depth Loss\nFigure 3. Comparison of including the depth loss and without the\ndepth loss in the training stage. Without the depth loss, the shape\nbecomes more rough and less detailed. The generated 3D scene\nhas some residuals outside the boundaries of the shape.\nWithout Eikonal \nRegularization Term\nWith Eikonal \nRegularization Term\nFigure 4. Comparison of including the Eikonal regularization term\nfor smoothness in the training stage. We can observe that the upper\npart has very rough surface (see the high contrast between dark\nand bright regions of the surface) and less smooth compared to the\nbelow one trained using the Eikonal regularization term.\nWithout \nSparseness Term\nWith \nSparseness Term\nFigure 5. Comparison of including the sparseness regularization\nterm for compactness in the training stage. We observe that the\ngenerated surface has some defects (i.e., holes) compared to the\nbelow one trained using the sparseness regularization term.\npact compactness of the surface because of the uncontrol-\n033\nlable free surfaces when we remove the sparseness regular-\n034\nization term. For the Eikonal term, it is applied to regularize\n035\nthe SDFs and the network to have the unit l2 norm gradient\n036\nyielding smooth surfaces. We experimented to train the net-\n037\nwork without the Eikonal term, as a result, Fig. 4 shows\n038\nthat the objects have very rough surfaces due to unregular-\n039\nized SDFs. We also observe that the shape details of an\n040\nobject might not be accurately predicted without the depth\n041\nloss. We also notice that the depth loss enriches the shape\n042\nof an object to add more details and remove some unwanted\n043\nresiduals as shown in Fig. 3.\n044\n3. Implementation and Evaluation Details\n045\n3.1. Hyperparameters\n046\nIn our experiments, we use 32 images as the optimal number\n047\nof generated images from the outputs of a generative model.\n048\nFor training the model, we set the learning rate to 5e-4 and\n049\nadjust it using the Cosine learning schedule. The model\n050\nis trained for 300K iterations. We set the sparseness term,\n051\nEikonal term, and depth term to 0.02, 0.1, 1.0, respectively.\n052\nThe background ratio is set to 0.3.\n053\n3.2. Data Used for Evaluation\n054\nWe opt to use two datasets for our evaluation. The first\n055\ndataset is obtained from 15 images proposed in RealFu-\n056\nsion [? ], intended for evaluating rendering quality. For the\n057\nGSO dataset [? ], we pick 25 images from different cate-\n058\ngories as follows: Alarm, Backpack, Bell, Blocks, Chicken,\n059\nCream, Elephant, Grandfather, Grandmother, Leather, Lion,\n060\nLunchbag, Mario, Oil, Schoolbus1, schoolbus2, Shoeblack,\n061\nSoap, Sofa, Sortingboard, Stacking cups, Teapot, Toaster,\n062\nTrain, Turtle.\n063\n4. Additional Results\n064\nIn this section, we provide some more detailed images and\n065\nmore results in addition to the results provided in the main\n066\npaper. We provide a zoomed version of the generated results\n067\nin comparison to baselines in Fig. 6. We also show all of\n068\nour generated results on text-to-3d and image-to-3d tasks in\n069\nFig. 7 Note that we also generate our results and comparison\n070\nin the videos. Please see our generated video format.\n071\n2\nCVPR\n#1001\nCVPR\n#1001\nCVPR 2024 Submission #1001. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nOurs\nShap-e\nInput\nPoint-e\nZero123\n+\nSD\nOne2345\nFigure 6. Comparison of our proposed method Hyper-VolTran against baselines with zoomed patches.\nA colorful \nllama\nA teapot\nA rocking \nhorse\nA blue \nice cream\nA burger\nText-to-3D\nA deer\nstatue\nA brown \ndrawer\nAn office \nchair\nA red bird\nA green fish\nA wooden \nbear\nInput \nImage\nInput \nImage\nResult\nResult\nResult\nFigure 7. The results of generated 3D objects using our proposed method on text-to-3d and image-to-3d tasks.\n3\n"
  },
  {
    "title": "PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion",
    "link": "https://arxiv.org/pdf/2312.16486.pdf",
    "upvote": "6",
    "text": "PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with\nTime-Decoupled Training and Reusable Coop-Diffusion\nGuansong Lu1\nYuanfan Guo1\nJianhua Han1\nMinzhe Niu1\nYihan Zeng1\nSongcen Xu1\nZeyi Huang2\nZhao Zhong2\nWei Zhang1\nHang Xu1\n1Huawei Noah\u2019s Ark Lab\n2Huawei\nAbstract\nCurrent large-scale diffusion models represent a giant\nleap forward in conditional image synthesis, capable of in-\nterpreting diverse cues like text, human poses, and edges.\nHowever, their reliance on substantial computational re-\nsources and extensive data collection remains a bottle-\nneck. On the other hand, the integration of existing dif-\nfusion models, each specialized for different controls and\noperating in unique latent spaces, poses a challenge due\nto incompatible image resolutions and latent space embed-\nding structures, hindering their joint use. Addressing these\nconstraints, we present \u201cPanGu-Draw\u201d, a novel latent dif-\nfusion model designed for resource-efficient text-to-image\nsynthesis that adeptly accommodates multiple control sig-\nnals. We first propose a resource-efficient Time-Decoupling\nTraining Strategy, which splits the monolithic text-to-image\nmodel into structure and texture generators. Each genera-\ntor is trained using a regimen that maximizes data utiliza-\ntion and computational efficiency, cutting data preparation\nby 48% and reducing training resources by 51%. Secondly,\nwe introduce \u201cCoop-Diffusion\u201d, an algorithm that enables\nthe cooperative use of various pre-trained diffusion mod-\nels with different latent spaces and predefined resolutions\nwithin a unified denoising process. This allows for multi-\ncontrol image synthesis at arbitrary resolutions without the\nnecessity for additional data or retraining. Empirical val-\nidations of Pangu-Draw show its exceptional prowess in\ntext-to-image and multi-control image generation, suggest-\ning a promising direction for future model training efficien-\ncies and generation versatility. The largest 5B T2I PanGu-\nDraw model is released on the Ascend platform. Project\npage: https://pangu-draw.github.io\n1. Introduction\nThe Denoising Diffusion Probabilistic Models (DDPMs)\n[16] and their subsequent enhancements [6, 15, 29] have\nestablished diffusion models as a leading approach for\nimage generation.\nThese advancements excel in the ap-\nplication of diffusion models to text-to-image synthesis,\nyielding high-fidelity results with large-scale models and\ndatasets, supported by substantial computational resources\n[18, 28, 34, 36, 38]. These foundational models, capable\nof understanding and rendering complex semantics, have\npaved the way for diverse image generation tasks, accom-\nmodating various control signals such as reference images,\nedges [51], and poses [51].\nHowever, the extensive computational demand and sig-\nnificant data collection required by these models pose a sub-\nstantial challenge. The ambitious goal of higher fidelity and\nincreased resolution in image synthesis pushes the bound-\naries of model and dataset sizes, escalating computational\ncosts, and environmental impact. Moreover, the aspiration\nfor versatile control and multi-resolution in image gener-\nation introduces additional complexity. Existing diffusion\nmodels, each tailored for specific controls and operating\nwithin distinct latent spaces, face the challenge of integra-\ntion due to incompatible image resolutions and latent space\nembeddings, obstructing their concurrent utilization. This\nincompatibility not only leads to more resource consump-\ntion of retraining but also impedes the joint synthesis of\nimages controlled by multiple factors, thereby limiting the\nscalability and practical application of such existing gener-\native models. In response to these challenges, our work in-\ntroduces a novel paradigm named \u201cPanGu-Draw\u201d that ju-\ndiciously conserves training resources while enhancing data\nefficiency, thereby proposing a resource-efficient pathway\nforward for diffusion model scalability.\nAs shown in Figure 1, the training strategies of prede-\ncessors like DeepFloyd [40] and GLIDE [28], which em-\nploy a cascaded approach, excel in leveraging data across\nresolutions but suffer from inefficient inference due to their\nreliance on multiple models. Alternatively, Stable Diffusion\n[36] and AltDiffusion [48] use a Resolution Boost Training\nstrategy aiming for cost-effectiveness by refining a single\nmodel. However, this strategy falls short on data efficiency.\nIn light of these considerations, our PanGu-Draw frame-\nwork advances the field by presenting a Time-Decoupling\nTraining Strategy that segments the training of a compre-\nhensive text-to-image model into two distinct generators:\n1\narXiv:2312.16486v2  [cs.CV]  29 Dec 2023\n(a) Cascaded Training\nCascaded Training \n(e.g. DeepFlyod-IF, GLIDE...)\nResolution Boost Training \n(e.g. Stable Diffusion, AltDiffusion...)\nTime-decoupling(Ours)\nData Efficiency\n \nAll training data \n \nDrop low resolution data\n \nAll training data \nTraining Efficiency\n \n3x time training\n \nHigh cost for High-Resolution Stage  \n \nHalf the parameters, Texture\nGenerator train on low res.\nInference Efficiency\n3x times for cascaded models\ninference \n \nSingle large model across all\ntimesteps \n \nHalf the parameters \nTraining Strategy \nResource Efficiency\n+ 48% Data Usage\n- 51% Training Cost\n- ~50% Infer Time  \nSimilar Performance \n(b) Resolution Boost Training\nsize>512 Part\nSame Architecture\nParameter Inherent\nsize>1024 Part\n(c) Time-decoupling Training(Ours)\nTexture Generator \nStructure Generator \n(d) Comparison on Resource Efficiency\n512 x 512\n1024 x 1024\nHigh\nHigh\nHigh\nHigh\nMid\nMid\nLow\nLow\nLow\n1024 x 1024\n512 x 512\nParameter Inherent\nSame Architecture\nsize>512 Part\nAll Data\nAll Data\n256 x 256\n512 x 512\n1024 x 1024\nSize > 512 Part\nSize>1024 Part\nStage1 Model\nStage2 Model\nStage3 Model\nLow-Resolution Stage\nHigh-Resolution Stage\nHalf the  \nParameters\nOurs vs.  \nResolution Boost Training\nFigure 1. Illustration of three multi-stage training strategies and comparison between them in resource efficiency in data, training and\ninference aspects. Our time-decoupling training strategy significantly surpasses the representative methods in Cascaded Training [28, 40]\nand Resolution Boost Training [36, 48] in resource efficiency.\none dedicated to structural outlines and another to textu-\nral details. This division not only concentrates on training\nefforts but also enhances data efficacy. The structural gen-\nerator is adept at crafting the initial outlines of images, of-\nfering flexibility in data quality and enabling training across\na spectrum of data calibers; the textural generator, in con-\ntrast, is fine-tuned using low-resolution data to infuse these\noutlines with fine-grained details, ensuring optimal perfor-\nmance even during high-resolution synthesis. This focused\napproach not only accelerates the training process of our\n5B model but also significantly reduces the reliance on ex-\ntensive data collection and computational resources, as ev-\nidenced by a 48% reduction in data preparation and a 51%\nreduction in resource consumption.\nFurthermore,\nwe introduce a pioneering algorithm\nnamed Coop-Diffusion, which facilitates the cooperative\nintegration of diverse pre-trained diffusion models. Each\nmodel, conditioned on different controls and pre-defined\nresolutions, contributes to a seamless denoising process.\nThe first algorithmic sub-module addresses inconsisten-\ncies in VAE decoders that arise during the denoising pro-\ncess across different latent spaces, ensuring cohesive image\nquality by effectively reconciling disparate latent space rep-\nresentations. The second sub-module confronts the chal-\nlenges associated with multi-resolution denoising. Tradi-\ntional bilinear upsampling for the intermediate noise map,\nintroduced during the denoising process, can undesirably\namplify the correlation between pixels. This amplification\ndeviates from the initial Independent and Identically Dis-\ntributed (IID) assumption, leading to severe artifacts in the\nfinal output image. However, our innovative approach cir-\ncumvents this issue with a single-step sampling method that\npreserves the integrity of pixel independence, thus prevent-\ning the introduction of artifacts. Coop-Diffusion obviates\nthe need for additional data or model retraining, addressing\nthe challenges of multi-control and multi-resolution image\ngeneration with scalability and efficiency.\nPanGu-Draw excels in text-to-image (T2I) generation,\noutperforming established models like DALL-E 2 and\nSDXL, as evidenced by its FID of 7.99 in English T2I. It\nalso leads in Chinese T2I across metrics like FID, IS, and\nCN-CLIP-score. User feedback highlights a strong prefer-\nence for PanGu-Draw, aligning well with human visual per-\nceptions. Available on the Ascend platform, PanGu-Draw\nis efficient and versatile.\nIn summary, our contributions are manifold:\n\u2022 PanGu-Draw: A resource-efficient diffusion model with\na Time-Decoupling Training Strategy, reducing data and\ntraining resources for text-to-image synthesis.\n\u2022 Coop-Diffusion: A novel approach for integrating multi-\nple diffusion models, enabling efficient multi-control im-\nage synthesis at multi-resolutions within a unified denois-\ning process.\n\u2022 Comprehensive evaluations demonstrate PanGu-Draw\n(5B model) can produce high-quality images aligned with\ntext and various controls, advancing the scalability and\nflexibility of diffusion-based image generation.\n2. Related Work\nText-to-Image Generation. The integration of diffusion\nmodels into the realm of text-to-image generation marks\na significant stride in computational creativity [6, 10, 17,\n18, 28, 29, 34, 36, 38, 40, 43, 44]. Text-to-image synthe-\nsis models like GLIDE [28] and DALL-E 2 [34], which in-\ncorporate CLIP image embeddings, have significantly ad-\nvanced in generating diverse and semantically aligned im-\nages from textual descriptions. The Latent Diffusion model\n[36] addresses computational challenges by creating images\nfrom text-conditioned low-dimensional latent representa-\ntions. Techniques like LoRA [20] enhance domain-specific\n2\nadaptability through low-rank matrix-driven parameter off-\nsets, avoiding catastrophic forgetting. Additionally, Con-\ntrolNet [51] introduces spatial conditioning controls, offer-\ning flexibility in image generation under varied conditions\nlike edges and depth.\nCurrent research also focuses on\naligning model outputs with human aesthetic preferences,\naiming to optimize image quality and user satisfaction\n[8, 13, 23, 41, 42]. Despite the proliferation of such spe-\ncialized models, a unified framework that consolidates these\ndisparate capabilities remains absent, limiting the potential\nfor multi-control and complex editing in image synthesis.\nModel Efficient Training and Scaling Up Strategies. Ef-\nficient training and scaling of models are pivotal for ad-\nvancing large-scale neural networks. In the realm of text-\nto-image (T2I) diffusion models, the quest for efficiency\nhas led to innovative training strategies. Historical meth-\nods, such as those utilized by DeepFloyd [40] and GLIDE\n[28], capitalize on cascaded approaches that proficiently uti-\nlize data across various resolutions, yet their reliance on\nmultiple models results in less efficient inference processes.\nContrastingly, models like Stable Diffusion [36] and AltD-\niffusion [48] adopt Resolution Boost Training strategies that\nrefine a single model for cost-effectiveness. Despite the ad-\nvantages, such strategies do not fully exploit data efficiency.\nIn scaling up strategies, training efficiency is also impor-\ntant. The correlation between model size and performance\nis well-documented [19, 22], with larger models like SDXL\n[30] showing notable gains. Efficient adaptation and scaling\nare explored in [2] through distillation, and in [31] by mar-\nrying model expansion with domain-specific prompts. Se-\nrial scaling and knowledge distillation reduce training times\nsignificantly as demonstrated by [11], while [7] proposes\nprogressive network expansion for faster training with min-\nimal loss. Our approach offers a novel approach to diffusion\nmodel scaling that enhances efficiency.\n3. Preliminary\nGiven an image x0, diffusion models first produce a series\nof noisy images x1, ..., xT by adding Gaussian noise to x0\naccording to some noise schedule given by \u00af\u03b1t as follows:\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5,\n(1)\nwhere \u03f5 \u223c N (0, I).\nDiffusion models then learn a denoising model \u03f5\u03b8(xt, t)\nto predict the added noise of a noisy image xt with the fol-\nlowing training objective:\nL = Ex0\u223cq(x0),\u03f5\u223cN (0,I),t\u223c[1,T ] \u2225\u03f5 \u2212 \u03f5\u03b8 (xt, t)\u22252 ,\n(2)\nwhere t is uniformly sampled from {1, ..., T}. Once the\ndenoising model \u03f5\u03b8(xt, t) is learned, starting from a random\nnoise xT \u223c N(0, I), one can iteratively predict and reduce\nthe noise in xt to get a real image x0. During the sampling\nprocess, we can predict the clean data x0 from \u03f5\u03b8 (xt, t)\nwith single-step sampling as follows:\n\u02c6x0,t =\n1\n\u221a\u00af\u03b1t\n(xt \u2212\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u03b8(xt, t)).\n(3)\nOur text-to-image generation model is built on the model\narchitecture proposed in Latent Diffusion Model [36]. In\nthis model, a real image x0 is first down-sampled 8 times\nas a lower-dimension latent code z0 with an image en-\ncoder model E, which can be decoded with a latent decoder\nmodel D back to a real image x0. The denoising network\n\u03f5\u03b8(zt, t, c) is parameterized as a U-Net [37] model, where\nembedding of time step t is injected with adaptive normal-\nization layers and embedding of input text c is injected with\ncross-attention layers.\n4. PanGu-Draw\nIn this section, we first illustrate our resource-efficient\n5B text-to-image generation model, trained with a time-\ndecoupling training strategy and further enhanced with a\nprompt enhancement LLM. Then, we present our Coop-\nDiffusion algorithm for the cooperative integration of di-\nverse pre-trained diffusion models, enabling multi-control\nand multi-resolution image generation.\n4.1. Time-Decoupling Training Strategy\nEnhancing data, training, and inference efficiency is vital\nfor text-to-image models\u2019 practical use. Figure 1 shows two\nexisting training strategies: (a) Cascaded Training, using\nthree models to incrementally improve resolution, is data-\nefficient but triples training and inference time. (b) Resolu-\ntion Boost Training starts at 512x512 and then 1024x1024\nresolution, discarding lower resolution data and offering\nmoderate efficiency with higher training costs and single-\nmodel inference across all timesteps. These approaches dif-\nfer from our time-decoupling strategy, detailed below.\nResponding to the need for enhanced efficiencies, we\ndraw inspiration from the denoising trajectory of diffusion\nprocesses, where initial denoising stages primarily shape\nthe image\u2019s structural foundation, and later stages refine\nits textural complexity. With this insight, we introduce the\nTime-Decoupling Training Strategy. This approach divides\na comprehensive text-to-image model, denoted as \u03f5\u03b8, into\ntwo specialized sub-models operating across different tem-\nporal intervals: a structure generator, \u03f5struct, and a texture\ngenerator, \u03f5texture. Each sub-model is half the size of the\noriginal, thus enhancing manageability and reducing com-\nputational load.\nAs illustrated in Figure 1(c), the structure gener-\nator,\n\u03f5struct,\nis responsible for early-stage denoising\nacross larger time steps, specifically within the range\nT, ..., Tstruct, where 0 < Tstruct < T.\nThis stage fo-\ncuses on establishing the foundational outlines of the image.\nConversely, the texture generator, \u03f5texture, operates during\nthe latter, smaller time steps, denoted by Tstruct, ..., 0, to\n3\nModel\nVAE\nResolution\nSD1.x\nSD1\n512\nSD2.x\nSD2\n768\nSDXL\nSDXL\n1024\nControlNet\nSD1\n512\nImageVariation\nSD1\n512\nInstructPix2Pix\nSD1\n512\nDeepFloyd\n/\n1024\nImagen\n/\n1024\nStableSR\nSD1/SD2\nx2\nOur\nSDXL\n1024\n(a) Candidate Models\n(Any Resolution)\n...\n...\n...\n...\nEq. (4)\nEq. (5)\nMulti-Control\nFusing\n(b) Bridging Latent  Space Gap\n(c) Bridging Resolution Gap\nMulti-Control\nFusing\nsampling step\ndown-sample\nAny Resolution\nLower Resolution\nEq. (4)\nup-sample\nadd noise\nLatent Space A\nLatent Space B\nImage Space\nRemove Objects from Image\nReplace Objects from Image\nChange Image by the Text\nImage-to-Image\nText-to-Image\nSuper Resolution\nAny-resolution Synthesis\nEdge-to-Image\nPose-to-Image\nSeg-to-Image\nDepth-to-Image\nSketch-to-Image\nLine-to-Image\n(d) Multi-tasks Supported\nFigure 2. Visualization of our Coop-Diffusion algorithm for the cooperative integration of diverse pre-trained diffusion models. (a) Existing\npre-trained diffusion models, each tailored for specific controls and operating within distinct latent spaces and image resolutions. (b) This\nsub-module bridges the gap arising from different latent spaces by transforming \u03f5\u2032\nt in latent space B to the target latent space A as \u02dc\u03f5t. (c)\nThis sub-module bridges the gap arising from different resolutions by performing upsampling on the predicted clean data \u02c6x\u2032\n0,t.\nelaborate on the textural details. Each generator is trained\nin isolation, which not only alleviates the need for high-\nmemory computation devices but also avoids the complex-\nities associated with model sharding and its accompanying\ninter-machine communication overhead.\nIn the inference phase, \u03f5struct initially constructs a base\nstructural image, zTstruct, from an initial random noise vec-\ntor, zT . Subsequently, \u03f5texture refines this base to enhance\ntextural details, culminating in the final output, z0. This\nsequential processing facilitates a more resource-efficient\nworkflow, significantly reducing the hardware footprint and\nexpediting the generation process without compromising\nthe model\u2019s performance or output quality, as demonstrated\nin our ablated experiment in Sec. 5.3.\nResource-Efficient Specialized Training Regime.\nWe\nfurther adopt specialized training designs for the above\ntwo models.\nThe structure generator \u03f5struct, which de-\nrives image structures from text, requires training on an\nextensive dataset encompassing a wide range of concepts.\nTraditional methods, like Stable Diffusion, often elimi-\nnate low-resolution images, discarding about 48% of train-\ning data and thereby inflating dataset costs.\nContrarily,\nwe integrate high-resolution images with upscaled lower-\nresolution ones. This approach, as proven by our ablated\nexperiments in Sec. 5.3, shows no performance drop, as\nthe predicted zTstruct still contains substantial noise. In this\nway, we achieve higher data efficiency and avoid the prob-\nlem of semantic degeneration.\nAdditionally, since the image structure is determined in\nzTstruct and the texture generator \u03f5texture focuses on refin-\ning texture, we propose training \u03f5texture at a lower resolu-\ntion while still sampling at high resolution. This strategy, as\ndemonstrated in our ablated experiments in Sec. 5.3, results\n(a) Upsampling from intermediate \ud835\udc67\ud835\udc61.\n(b) Our upsampling algorithm.\nFigure 3. Results of fusing a low-resolution model and a high-\nresolution model with different upsampling methods. Upsampling\nfrom intermediate zt results in severe artifacts, while our upsam-\npling algorithm results in high-fidelity image.\nin no performance drop and no structural problems (e.g.,\nrepetitive presentation [21]). Consequently, we achieved an\noverall 51% improvement in training efficiency. Figure 1\nsummarizes the data, training, and inference efficiency of\ndifferent training strategies. Besides higher data and train-\ning efficiency, our strategy also achieves higher inference\nefficiency with fewer inference steps compared to the Cas-\ncaded Training strategy and a smaller per-step model com-\npared to the Resolution Boost Training strategy.\n4.2. Coop-Diffusion: Multi-Diffusion Fusion\nAs shown in Figure 2(a), there are numerous pre-trained\ndiffusion models, such as various SD, ControlNet, image\nvariation, etc., each tailored for specific controls and image\nresolutions. It is promising to fuse these pre-trained models\nfor multi-control or multi-resolution image generation with-\nout needing to train a new model. However, the different\nlatent spaces and resolutions of these models impede joint\nsynthesis of images controlled by different models, thereby\n4\nAlgorithm 1 Coop-Diffusion: Multi-Diffusion Fusing\nSub-Module 1. Bridging Latent Space Gap\nInput: random noise zT \u223c N(0, I), diffusion model \u03f5\u03b8, decoder\nD, encoder E in latent space A; random noise z\u2032\nT = zT , diffusion\nmodel \u03f5\u2032\n\u03b8, decoder D\u2032, encoder E\u2032 in latent space B; guidance\nstrength d, sampling method S.\n1: for t = T, . . . , 1 do\n2:\n\u03f5\u2032\nt = \u03f5\u2032\n\u03b8(z\u2032\nt), \u02c6z\u2032\n0,t =\n1\n\u221a\u00af\u03b1t (z\u2032\nt \u2212 \u221a1 \u2212 \u00af\u03b1t\u03f5\u2032\nt)\n3:\n\u02c6x\u2032\n0,t = D\u2032(\u02c6z\u2032\n0,t), \u02dcz0,t = E(\u02c6x\u2032\n0,t)\n4:\n\u02dc\u03f5t =\n1\n\u221a1\u2212\u00af\u03b1t (zt \u2212 \u221a\u00af\u03b1t\u02dcz0,t)\n5:\n\u03f5t = \u03f5\u03b8(zt), \u03f5t,fuse = d \u00b7 \u02dc\u03f5t + (1 \u2212 d) \u00b7 \u03f5t\n6:\nzt\u22121 = S(zt, t, \u03f5t,fuse)\n7:\nz\u2032\nt\u22121 = S(z\u2032\nt, t, \u03f5\u2032\nt,fuse) \u25b7 \u03f5\u2032\nt,fuse from \u03f5t,fuse similar to\nthe process from \u03f5\u2032\nt to \u02dc\u03f5t, omitted for brevity\n8: end for\n9: return D(z0)\nSub-Module 2. Bridging Resolution Gap\nInput:\ndiffusion model \u03f5\u03b8, decoder D, encoder E in high-\nresolution space; random noise z\u2032\nT \u223c N(0, I), diffusion model \u03f5\u2032\n\u03b8,\ndecoder D\u2032, encoder E\u2032 in low-resolution space; low-resolution\nsampling end step Tlow, sampling method S.\n1: for t = T, . . . , Tlow + 1 do\n2:\n\u03f5\u2032\nt = \u03f5\u2032\n\u03b8(z\u2032\nt), z\u2032\nt\u22121 = S(z\u2032\nt, t, \u03f5\u2032\nt)\n3: end for\n4: \u02c6z\u2032\n0,Tlow =\n1\n\u221a\u00af\u03b1Tlow (z\u2032\nTlow \u2212 p1 \u2212 \u00af\u03b1Tlow\u03f5\u2032\nTlow)\n5: \u02c6x\u2032\n0,Tlow = D\u2032(\u02c6z\u2032\n0,Tlow), \u02c6x0,Tlow =Upsample(\u02c6x\u2032\n0,Tlow)\n6: \u02c6z0,Tlow = E(\u02c6x0,Tlow)\n7: zTlow = \u221a\u00af\u03b1Tlow \u02c6z0,Tlow + p1 \u2212 \u00af\u03b1Tlow\u03f5, \u03f5 \u223c N (0, I)\n8: for t = Tlow, . . . , 1 do\n9:\n\u03f5t = \u03f5\u03b8(zt), zt\u22121 = S(zt, t, \u03f5t)\n10: end for\n11: return D(z0)\nlimiting their practical applications. In response to these\nchallenges, we propose the Coop-Diffusion algorithm with\ntwo key sub-modules, as shown in Figures 2(b) and (c), to\nbridge the latent space gap and the resolution gap, and to\nunite the denoising process in the same space.\nBridging the Latent Space Gap. To bridge the latent space\ngap between spaces A and B, we propose to unify the model\nprediction in latent space A by transforming the model pre-\ndiction \u03f5\u2032\nt in latent space B to latent space A using the image\nspace as an intermediate. This is done in the following way:\nfirst, we predict the clean data \u02c6z\u2032\n0,t using Equation (3) as:\n\u02c6z\u2032\n0,t =\n1\n\u221a\u00af\u03b1t\n(z\u2032\nt \u2212\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u2032\nt),\n(4)\nwhich is then decoded into a pixel-level image \u02c6x\u2032\n0,t using\nthe latent decoder model D\u2032. This image is encoded into\nlatent space A using the image encoder model E, as \u02dcz0,t =\nE(\u02c6x\u2032\n0,t), and finally transformed into a model prediction by\ninverting Equation (3) as:\n\u02dc\u03f5t =\n1\n\u221a1 \u2212 \u00af\u03b1t\n(zt \u2212 \u221a\u00af\u03b1t\u02dcz0,t).\n(5)\nWith the united \u02dc\u03f5t, we can now perform multi-control fu-\nsion between \u02dc\u03f5t and \u03f5t (the prediction from model \u03f5\u03b8 with\nTable 1.\nComparisons of PanGu-Draw with recent representa-\ntive English text-to-image generation models on COCO dataset in\nterms of FID. Our classifier-free guidance scale is set as 2.\nMethod\nFID\u2193\nModel Size\nRelease\nDALL-E [33]\n27.50\n12B\nN\nLDM [36]\n12.63\n1.5B\nY\nGLIDE [28]\n12.24\n5B\nN\nSDXL [30]\n11.93\n2.5B\nY\nPixArt-\u03b1 [3]\n10.65\n0.6B\nY\nDALL-E 2 [34]\n10.39\n5.5B\nN\nImagen [38]\n7.27\n3B\nN\nRAPHAEL [44]\n6.61\n3B\nN\nPanGu-Draw\n7.99\n5B\nY\nzt in latent space A, omitted in Figure 2 for brevity) as:\n\u03f5t,fuse = d\u00b7\u02dc\u03f5t +(1\u2212d)\u00b7\u03f5t, where d and 1\u2212d are the guid-\nance strengths of each model with d \u2208 [0, 1], to guide the\ndenoising process jointly with these two models for multi-\ncontrol image generation. Algorithm 1 further illustrates\nthis fusion process.\nBridging Resolution Gap. To integrate the denoising pro-\ncesses of a low-resolution model with a high-resolution\nmodel, upsampling and/or downsampling is necessary. Tra-\nditional bilinear upsampling, often applied to the intermedi-\nate result zt during the denoising process, can undesirably\namplify pixel correlation. This amplification deviates from\nthe initial Independent and Identically Distributed (IID) as-\nsumption, leading to severe artifacts in the final images,\nas shown in Figure 3(a). Conversely, downsampling does\nnot present this issue. To address the IID issue in upsam-\npling, we propose a new upsampling algorithm that pre-\nserves the IID assumption, thereby bridging the resolution\ngap between models with different pre-trained resolutions.\nFigure 2(c) visualizes our upsampling algorithm. Specif-\nically, for a low-resolution z\u2032\nt, we use the image space as an\nintermediate space to transform z\u2032\nt in low-resolution space\ninto high-resolution space as \u02dczt. We first predict the noise \u03f5\u2032\nt\nwith the denoising model \u03f5\u2032\n\u03b8 and then predict the clean data\n\u02c6z\u2032\n0,t as described in Eq. 4. This is decoded into an image\n\u02c6x\u2032\n0,t using decoder D\u2032. We then perform upsampling on \u02c6x\u2032\n0,t\nto obtain its high-resolution counterpart \u02c6x0,t. Finally, \u02c6x0,t\nis encoded into the latent space with encoder E as \u02c6z0,t, and\nt-step noise is added to get the final result \u02dczt using Eq. 1.\nWith the unified \u02dczt, we can now perform multi-resolution\nfusion. First, we denoise with a low-resolution model to\nobtain the intermediate z\u2032\nt and its high-resolution counter-\npart \u02dczt. Then, we perform denoising with a high-resolution\nmodel starting from \u02dczt, and vice versa. This approach al-\nlows us to conduct one-stage super-resolution without un-\ndergoing all the low-resolution denoising steps, thereby im-\nproving inference efficiency. Algorithm 1 further illustrates\nthis fusion process.\n5\n\u201c\u4e00\u4f4d\u5e74\u8f7b\u5973\u6027\uff0c\u8eab\u7740\u4f18\u96c5\u793c\u670d\uff0c\u4f69\u6234\u6bd5\u4e1a\u5e3d\uff0c\u5fae\n\u7b11\u7740\u9762\u5bf9\u955c\u5934\uff0c\u4f38\u51fa\u624b\u81c2\uff0c\u5979\u7684\u80cc\u666f\u662f\u5915\u9633\u4e0b\u7684\u6821\n\u56ed\u3002\u201d\u201cA young woman, wearing an elegant gown and \ngraduation cap, smiles at the camera and extends her \narms, with the sunset on campus in the background.\u201d\n\u201c\u65c5\u884c\u8005\u4eec\u6cdb\u821f\u5728\u6ce2\u5149\u7cbc\u7cbc\u7684\u6e56\u9762\u4e0a\uff0c\u5468\u56f4\u662f\u96c4\u4f1f\u7684\u5c71\u8109\uff0c\u4ee5\u4e2d\u56fd\u6c34\u58a8\u753b\u98ce\u683c\u63cf\u7ed8\uff0c\u753b\u9762\u8272\u5f69\u6de1\u96c5\uff0c\u5177\n\u6709\u53e4\u5178\u8bd7\u610f\u3002\u201d \u201cTravelers are boating on the sparkling lake, surrounded by majestic mountains, painted in the \nstyle of Chinese ink painting, with elegant colors and a classical poetic feel.\u201d\n\u201c\u4e00\u4e2a\u5de8\u5927\u7684\u6c34\u6676\u7403\uff0c\u5185\u90e8\u8574\u542b\u7740\u4e00\u4e2a\u5fae\u578b\u96e8\u6797\uff0c\n\u96e8\u6797\u4e2d\u8774\u8776\u98de\u821e\uff0c\u9633\u5149\u900f\u8fc7\u6811\u53f6\u6d12\u843d\u3002\u201d \u201cA huge \ncrystal ball contains a miniature rainforest inside, with \nbutterflies flying in the rainforest and sunlight shining \nthrough the leaves.\u201d\n\u201c\u4e00\u8258\u53e4\u8001\u7684\u6d77\u76d7\u8239\uff0c\u5b8c\u5168\u7531\u7cd6\u679c\u548c\u5de7\u514b\u529b\u5236\u6210\u3002\u201d\n\u201cAn ancient pirate ship made entirely of candy and \nchocolate.\u201d\n\u201c\u4e00\u4f4d\u9762\u5e26\u5fae\u7b11\u7684\u5973\u5b50\uff0c\u8eab\u7a7f\u767d\u8272T\u6064\uff0c\u7ea2\u8272\u5939\u514b\n\u5728\u9633\u5149\u4e0b\u71a0\u71a0\u751f\u8f89\uff0c\u753b\u9762\u6e05\u65b0\uff0c\u98ce\u683c\u50cf\u52a8\u6f2b\uff0c\u7ec6\u8282\n\u4e30\u5bcc\u3002\u201d\u201cA smiling woman wearing a white T-shirt \nand a red jacket shines in the sun. The picture is fresh, \nanime-like in style, and rich in details.\u201d\n\u201c\u8d5b\u535a\u670b\u514b\u98ce\u683c\u7684\u6444\u5f71\u673a\uff0c\u65e0\u4eba\u673a\u5728\u591c\u7a7a\u4e2d\u98de\u884c\uff0c\n\u4ee5\u7c92\u5b50\u6c34\u58a8\u753b\u98ce\u5c55\u73b0\uff0c\u5177\u6709\u5f3a\u70c8\u7684\u5149\u5f71\u6548\n\u679c\u3002\u201d\u201cCyberpunk style camera, drone flying in the \nnight sky, presented in particle ink painting style, with \nstrong light and shadow effects.\u201d\n\u201c\u4e00\u53f0\u672a\u6765\u98ce\u683c\u7684\u6469\u6258\u8f66\uff0c\u95ea\u8000\u7740\u9713\u8679\u706f\uff0c\u505c\u5728\u591c\n\u665a\u7684\u4e1c\u4eac\u8857\u5934\u3002\u201d\n\u201cA futuristic motorcycle, shining with neon lights, is \nparked on the streets of Tokyo at night.\u201d\n\u201c\u4e00\u5ea7\u7531\u51b0\u6676\u548c\u96ea\u82b1\u6784\u6210\u7684\u7cbe\u81f4\u57ce\u5821\uff0c\u5750\u843d\u5728\u5317\u6781\n\u7684\u51b0\u539f\u4e0a\u3002\u201d\n\u201cAn exquisite castle made of ice crystals and \nsnowflakes, located on the Arctic ice sheet.\u201d\n\u201c\u4e00\u53ea\u7a7f\u7740\u4e2d\u4e16\u7eaa\u94e0\u7532\u7684\u5154\u5b50\uff0c\u624b\u6301\u957f\u5251\u7ad9\u5728\u4e00\u5ea7\n\u53e4\u8001\u57ce\u5821\u7684\u57ce\u5899\u4e0a\uff0c\u80cc\u540e\u662f\u843d\u65e5\u7684\u4f59\u6656\u3002\u201d\n\u201cA rabbit wearing medieval armor and holding a sword \nstands on the wall of an ancient castle with the setting \nsun behind him.\u201d\nFigure 4. Images generated with PanGu-Draw, our 5B multi-lingual text-to-image generation model. PanGu-Draw is able to generate\nmulti-resolution high-fidelity images semantically aligned with the input prompts.\n5. Experiments\nImplementation Details. We adopt the pretrained Varia-\ntional Autoencoder (VAE) model from SDXL [30], and we\nbuild our structure and texture generator based on the archi-\ntecture of its U-Net model with the following modifications.\nTo achieve bilingual text-to-image generation (Chinese and\nEnglish), we pre-train a Chinese text encoder [12, 46] on\nour Chinese training dataset.\nWe then concatenate the\ntext embeddings from this Chinese text encoder with those\nfrom a pretrained English text encoder, serving as the fi-\nnal text embeddings for the denoising models. For multi-\nresolution image generation, we select a range of image\nresolutions around 1024x1024 and further condition the de-\nnoising model on the sinusoidal positional embeddings cor-\nresponding to the index of image resolutions. The Tstruct\nparameter is set to 500, as suggested by our ablation study.\nOur models are trained on a cluster consisting of 256 As-\ncend 910B cards. During training, we applied several tech-\nniques to reduce redundant memory usage. These include\nreplacing traditional attention with Flash Attention [5], em-\nploying mixed-precision training [27], and using gradient\ncheckpointing [4], also known as the recompute technique.\nThese methods enable the model to fit within the memory\nof a single Neural Processing Unit (NPU), allowing paral-\nlelism to be applied only in the data scope and avoiding\nmodel sharding among NPUs, as well as reducing inter-\nmachine communication overhead.\nDataset Construction. To encompass the abundant con-\ncepts in the world, we collect images in various styles from\nmultiple sources, including Noah-Wukong [12], LAION\n[35], and others, such as photography, cartoons, portraits,\nand gaming assets. The collected images are filtered based\non CLIP score, aesthetic score, watermark presence, reso-\nlution, and aspect ratio. To improve the semantic alignment\nof PanGu-Draw, we discard parts of the noisy captions that\nare meaningless or mismatched to the image, sourced from\nthe Internet. Instead, we recaption the collected images by\n6\nTable 2. Comparisons of PanGu-Draw with Chinese text-to-image\ngeneration models on COCO-CN dataset in terms of FID, IS and\nCN-CLIP-score. The classifier-free guidance scales are set as 9\nfollowing AltDiffusion [48].\nModel\nFID\u2193\nIS\u2191\nCN-CLIP-score\u2191\nAltDiffusion[48]\n25.31\n29.16\n35.12\nTaiyi-Bilingual[50]\n24.61\n34.29\n32.26\nTaiyi-CN[50]\n23.99\n34.29\n34.22\nPanGu-Draw\n21.81\n37.00\n36.62\nTable 3. Results of a User study on ImageVal-prompt in terms of\nimage-text alignment, image fidelity, and aesthetics.\nMethod\nAlign\u2191\nFidelity\u2191\nAesthetics\u2191\nAve\u2191\nDALL-E 3[1]\n4.72\n4.59\n4.76\n4.69\nMJ 5.2\n4.63\n4.54\n4.75\n4.64\nSDXL[30]\n4.41\n4.37\n4.59\n4.46\nSD[36]\n4.17\n3.99\n4.20\n4.12\nPanGu-Draw\n4.5\n4.52\n4.72\n4.58\nfirst employing an open-vocabulary detector [47] to locate\nthe primary subjects within the images. These subjects are\nthen processed by LLaVA [26], a high-performance vision-\nlanguage model, along with prompting templates, to yield\ndetailed image descriptions. These English annotations are\nsubsequently translated into Chinese.\nEvaluation Metrics. We evaluate PanGu-Draw\u2019s text-to-\nimage generation on COCO [25] with 30k images for En-\nglish, and COCO-CN [24] with 10k images for Chinese.\nThe Frechet Inception Distance (FID [14]) is utilized to\nevaluate image quality and diversity. For Chinese, addi-\ntional metrics include the Inception Score (IS [39]) and CN-\nCLIP-score[45], assessing image quality and text-image\nalignment. Complementing these metrics, a user study is\nconducted to evaluate image-text alignment, fidelity, and\naesthetics using ImageEval-prompt1 across 339 prompts.\n5.1. Text-to-Image Generation\nEvaluation on COCO. As shown in Table 1, PanGu-Draw\nachieves a FID of 7.99, which is superior to compared meth-\nods such as DALL-E 2 and SDXL. It also achieves compet-\nitive FID with SOTA methods, indicating the effectiveness\nof our time-decoupling training strategy and its outstanding\ndata and training efficiencies. Our 5B PanGu model is the\nbest-released model in terms of FID.\nEvaluation on COCO-CN. As shown in Table 2, PanGu-\nDraw outperforms other released Chinese text-to-image\nmodels, including Taiyi-CN, Taiyi-Bilingual, and AltDiffu-\nsion, across all three metrics. This performance highlights\nPanGu-Draw\u2019s exceptional Chinese text-to-image genera-\ntion capabilities and the effectiveness of our bilingual text\nencoder architecture.\nUser Study. We conducted a user study to compare PanGu-\nDraw with top-performing methods, including SDXL [30],\nMidjourney 5.2, and DALL-E 3 [1].\nAs shown in Ta-\nble 3, PanGu-Draw achieves better results than SD and\n1https://github.com/FlagOpen/FlagEval/tree/master/imageEval\n\u201c\u4e00\u53ea\u767d\u8272\u7684\u72d7\u201d \n(\"a white dog\")\n(a) Input image and generation results of image variation.\n(b) Input prompt and generation results of PanGu-Draw.\n(c) Generation results of fusing image\nvariation and PanGu-Draw with Coop-\nDiffusion algorithm.\nFigure 5. Generation results of the fusing of an image variation\nmodel and PanGu-Draw and with the proposed Coop-Diffusion al-\ngorithm.\n\u201c\u4e00\u4e2a\u7a7f\u6bdb\u8863\u7684\u7537\u4eba\u201d \n(\"a man with sweater\")\n(a) Input pose and generation results of pose-to-image.\n(b) Input prompt and generation results of PanGu-Draw.\n(c) Generation results of fusing pose-to-\nimage and PanGu-Draw with Coop-\nDiffusion algorithm.\n(a) Input edge map and generation results of edge-to-image.\n\u201c\u4e00\u53ea\u7eff\u8272\u7684\u5c0f\u9e1f\u201d \n(\"a little green bird\")\n(b) Input prompt and generation results of PanGu-Draw.\n(c) Generation results of fusing edge-to-\nimage and PanGu-Draw with Coop-\nDiffusion algorithm.\nFigure 6. Generation results guided by fusing signals of text and\npose/edge map by our Coop-Diffusion.\nSDXL across all three metrics.\nIt also attains approxi-\nmately 99%/98% of the performance of Midjourney 5.2\nand DALL-E 3, respectively, indicating PanGu-Draw\u2019s ex-\ncellent text-to-image capabilities.\nFigure 4 shows a col-\nlection of high-fidelity multi-resolution images generated\nby PanGu-Draw.\nAs we can see, the generated images\nof PanGu-Draw are of high aesthetics and semantically\naligned with the input prompts.\n5.2. Multi-Diffusion Fusing Results\nMulti-Control Image Generation. To demonstrate the ef-\nfectiveness of the proposed reusable multi-diffusion fusing\nalgorithm, Coop-Diffusion, we first present multiple results\nof multi-control image generation. Figure 5 displays results\nfrom fusing an image variation model 2 with PanGu-Draw.\nThe fusing results maintain a style similar to that of the ref-\nerence image, matching the texture described by the input\nprompt. Figure 6 shows results from fusing PanGu-Draw\nwith a pose/edge-to-image ControlNet model, which oper-\nates in guess mode without input prompts. Here, the fusing\nresults combine the structure of the pose/edge image with\nthe texture described by the input prompt.\n2https://huggingface.co/lambdalabs/sd-image-variations-diffusers\n7\nLR model\nLR model + HR model\nFigure 7. Images generated with a low-resolution (LR) model (first\nrow: text-to-image model; second row: Edge-to-image Control-\nNet) and the fusion of the LR model and HR PanGu-Draw with\nour Coop-Diffusion. This allows for single-stage super-resolution\nfor better details and higher inference efficiency.\nMulti-Resolution Image Generation.\nWe also present\nmulti-resolution image generation results of fusing PanGu-\nDraw with low-resolution text-to-image and edge-to-image\nControlNet model by first denoising with the low-resolution\nmodel to get the intermediate zt and the high-resolution\ncounterpart \u02dczt, and then perform denoising in high res-\nolution with PanGu-Draw.\nFigure 7 shows the results\nfrom the low-resolution model and our fusing algorithm\nCoop-Diffusion. As we can see, PanGu-Draw adds much\ndetails to the low-resolution predictions leading to high-\nfidelity high-resolution results. Besides, compared with the\ncommon practice of super-resolution with diffusion model,\nwhich carries out all the low-resolution denoising steps, our\nmethod achieve higher inference efficiency.\n5.3. Ablation Study\nIn this section, we perform ablation studies to analyze our\ntime-decoupling training strategy. The baseline model has\n1B parameters while the structure and texture generators\nboth have 0.5B parameters. During the training process,\nthe latter two models only train half the steps of the baseline\nmodel with Tstruct set as 500. Both settings of the models\nare trained from scratch on a subset of the LAION dataset\ncontaining images with all sizes. After training, FID, IS and\nCLIP-score on COCO are reported for comparison.\nTime-Decoupling Training Strategy.\nWe compare the\nfinal performance of models trained with the Resolution\nBoost strategy and our time-decoupling strategy in Table\n4. We found that models trained with our strategy achieves\nmuch better performance in all three criteria, indicating the\neffectiveness of our strategy.\nTraining Designs.\nThe structure and texture generators\n(\u03f5struct and \u03f5texture) are designed to train on different reso-\nlutions to improve data and training efficiency. However,\nTable 4. Comparison of models across Resolution Boost (1B pa-\nrameters) and Time-Decoupling training strategies (0.5B parame-\nters for structure and texture generators)\nModel\nFID\u2193\nIS\u2191\nCLIP-score\u2191\nResolution Boost\n106.12\n10.46\n22.9\nTime-Decoupling\n87.66\n11.07\n23.4\nTable 5. Performance of structure and texture models training with\nimages of different resolutions.\nStructure\nTexture\nFID\u2193\nIS\u2191\nCLIP-score\u2191\nData\nResolution\nAll data\n256\n87.66\n11.07\n23.4\nOnly high resolution\n256\n89.52\n10.96\n23.2\nAll data\n512\n90.98\n10.59\n23.3\nTable 6. Comparisons of PanGu-Draw inference performance with\ndifferent time step splitting point Tstruct settings.\nTstruct\nFID\u2193\nIS\u2191\nCLIP-score\u2191\n200\n105.08\n10.59\n22.98\n300\n98.08\n10.72\n23.12\n500\n87.66\n11.07\n23.40\n700\n89.48\n11.02\n23.32\nthis approach may negatively influence the final perfor-\nmance. In Table 5, we compare such a design with a tradi-\ntional training process, where \u03f5struct discards low-resolution\nimages, or \u03f5texture trains with high resolution. Results on\nCOCO show that \u03f5struct benefits from these extra up-scaled\ndata, and \u03f5texture learns enough texture patterns at a smaller\nresolution.\nTimestep Splitting Point.\nThe timestep splitting point\nTstruct between the structure and texture generators also in-\nfluences the final performance. To this end, we set Tstruct to\n200, 300, 500, and 700, while keeping the other settings of\nthe structure and texture generators unchanged. As shown\nin Table 6, as Tstruct increases from 200 to 700, the perfor-\nmance initially increases and then decreases continuously.\nTstruct = 500 is the optimal value, and we adopt it as the\ndefault setting in all other experiments.\n6. Conclusion\nIn this paper, we present \u201cPanGu-Draw\u201d, a new latent dif-\nfusion model for efficient text-to-image generation that ef-\nfectively integrates multiple control signals. Our approach\nincludes a Time-Decoupling Training Strategy to separate\nthe text-to-image process into structure and texture gen-\neration, enhancing data use and computational efficiency.\nAdditionally, \u201cCoop-Diffusion\u201d is introduced, an algorithm\nallowing cooperative use of different pre-trained diffusion\nmodels in a unified denoising process for multi-control im-\nage synthesis at various resolutions without extra data or\nretraining. PanGu-Draw outperforms models like DALL-\nE 2 and SDXL in English T2I, achieves superior FID, IS,\nand CN-CLIP-scores in Chinese T2I, and receives favorable\nuser feedback. This positions PanGu-Draw as a versatile\nand efficient state-of-the-art method, which is available on\nthe Ascend platform.\n8\nReferences\n[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al.\nImproving image generation with\nbetter captions.\nComputer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2023. 7\n[2] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia\nQin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and\nQun Liu. bert2bert: Towards reusable pretrained language\nmodels. arXiv preprint arXiv:2110.07143, 2021. 3\n[3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, and Zhenguo Li. Pixart-\u03b1: Fast training of dif-\nfusion transformer for photorealistic text-to-image synthesis,\n2023. 5, 2, 6\n[4] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.\nTraining deep nets with sublinear memory cost, 2016. 6\n[5] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00b4e. Flashattention: Fast and memory-efficient\nexact attention with io-awareness, 2022. 6\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in Neural Informa-\ntion Processing Systems, 34:8780\u20138794, 2021. 1, 2\n[7] Ning Ding, Yehui Tang, Kai Han, Chao Xu, and Yunhe\nWang. Network expansion for practical training accelera-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 20269\u201320279,\n2023. 3\n[8] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe\nDiao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft:\nReward ranked finetuning for generative foundation model\nalignment. arXiv preprint arXiv:2304.06767, 2023. 3, 1, 2\n[9] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang.\nGlm: General language\nmodel pretraining with autoregressive blank infilling. In Pro-\nceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n320\u2013335, 2022. 1\n[10] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang,\nLanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong\nYin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-\nimage diffusion model with knowledge-enhanced mixture-\nof-denoising-experts. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10135\u201310145, 2023. 2, 5, 6\n[11] Cheng Fu, Hanxian Huang, Zixuan Jiang, Yun Ni, Lifeng\nNai, Gang Wu, Liqun Cheng, Yanqi Zhou, Sheng Li, An-\ndrew Li, et al. Triple: Revisiting pretrained model reuse and\nprogressive learning for efficient vision transformer scaling\nand searching. In ICCV, 2023. 3\n[12] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu\nMinzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei\nZhang, Xin Jiang, et al. Wukong: A 100 million large-scale\nchinese cross-modal pre-training benchmark. Advances in\nNeural Information Processing Systems, 35:26418\u201326431,\n2022. 6\n[13] Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimiz-\ning prompts for text-to-image generation.\narXiv preprint\narXiv:2212.09611, 2022. 3\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 7\n[15] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 1\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 1\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2\n[18] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. J. Mach. Learn.\nRes., 23:47\u20131, 2022. 1, 2\n[19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, et al. Training compute-optimal large language mod-\nels. arXiv preprint arXiv:2203.15556, 2022. 3\n[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2, 1\n[21] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training-\nfree diffusion model adaptation for variable-sized text-to-\nimage synthesis. arXiv preprint arXiv:2306.08645, 2023. 4\n[22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361,\n2020. 3\n[23] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,\nYuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad\nGhavamzadeh, and Shixiang Shane Gu.\nAligning text-\nto-image models using human feedback.\narXiv preprint\narXiv:2302.12192, 2023. 3\n[24] Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengx-\niong Jia, Gang Yang, and Jieping Xu. Coco-cn for cross-\nlingual image tagging, captioning, and retrieval. IEEE Trans-\nactions on Multimedia, 21(9):2347\u20132360, 2019. 7\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 7\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 7\n[27] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory\nDiamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael\nHouston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu.\nMixed precision training, 2018. 6\n9\n[28] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 1, 2, 3, 5\n[29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn ICML, pages\n8162\u20138171. PMLR, 2021. 1, 2\n[30] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 3, 5, 6, 7, 2\n[31] Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou.\nElle:\nEfficient life-\nlong pre-training for emerging data.\narXiv preprint\narXiv:2203.06311, 2022. 3\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 1\n[33] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n5\n[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 1, 2, 5, 6\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 6\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 1, 2, 3, 5,\n7\n[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 3\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1, 2, 5\n[39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29:2234\u20132242, 2016. 7\n[40] Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva,\nChristoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova.\nDeepfloyd if:\nA powerful text-to-image model that can\nsmartly integrate text into images, 2023. Online; accessed\n16-November-2023. 1, 2, 3, 5, 6\n[41] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hong-\nsheng Li. Better aligning text-to-image models with human\npreference. arXiv preprint arXiv:2303.14420, 2023. 3\n[42] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagere-\nward: Learning and evaluating human preferences for text-\nto-image generation, 2023. 3\n[43] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang,\nand Humphrey Shi.\nVersatile diffusion:\nText, images\nand variations all in one diffusion model.\narXiv preprint\narXiv:2211.08332, 2022. 2\n[44] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuo-\nfan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image\ngeneration via large mixture of diffusion paths.\narXiv\npreprint arXiv:2305.18295, 2023. 2, 5, 6\n[45] An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang\nZhang, Jingren Zhou, and Chang Zhou.\nChinese clip:\nContrastive vision-language pretraining in chinese.\narXiv\npreprint arXiv:2211.01335, 2022. 7\n[46] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe\nNiu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and\nChunjing Xu. Filip: Fine-grained interactive language-image\npre-training. arXiv preprint arXiv:2111.07783, 2021. 6\n[47] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei\nZhang, Zhenguo Li, and Hang Xu.\nDetclipv2:\nScal-\nable open-vocabulary object detection pre-training via word-\nregion alignment.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n23497\u201323506, 2023. 7\n[48] Fulong Ye, Guang Liu, Xinya Wu, and Ledell Wu. Altdiffu-\nsion: A multilingual text-to-image diffusion model, 2023. 1,\n2, 3, 7\n[49] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao\nXia, et al. Glm-130b: An open bilingual pre-trained model.\narXiv preprint arXiv:2210.02414, 2022. 1\n[50] Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin\nZhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong,\nJunqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xi-\nayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen,\nTing Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun\nWu, Zhongshen Zeng, and Chongpei Chen. Fengshenbang\n1.0: Being the foundation of chinese cognitive intelligence.\nCoRR, abs/2209.02970, 2022. 7\n[51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 1, 3, 2\n10\nPanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with\nTime-Decoupled Training and Reusable Coop-Diffusion\nSupplementary Material\n7. More Details about PanGu-Draw\nPrompt Enhancement LLM with RLAIF Algorithm. To\nfurther enhance our generation quality, we harness the ad-\nvanced comprehension abilities of large language models\n(LLM)[9, 49] to align users\u2019 succinct inputs with the de-\ntailed inputs required by the model. Specifically, shown\nin Figure 10, we first construct a human-annotated dataset\nthat enriches succinct prompts with background and style\ndescriptions and then fine-tune the LLM to adapt a suc-\ncinct prompt to an enriched one using this data. To better\nadapt to the inputs required by PanGu-Draw, we perform\nfurther refinement based on the Reward rAnked FineTun-\ning (RAFT)[8] method. Subsequently, we use the fine-tuned\nLLM to expand on multiple texts, which are then input into\nPanGu-Draw for image generation. The best expansions\nare selected jointly by an aesthetic scoring model3 and a\nCLIP [32] semantic similarity calculation model, allowing\nfor further fine-tuning of the LLM.\nFigure 8 shows the generation results of PanGu-Draw\nwithout and with prompt enhancement.\nAs we can see,\nprompt enhancement serves to add more details and illus-\ntration to the original brief prompts, leading to better image\naesthetics and semantic alignment.\nControllable Stylized Text-to-Image Generation. While\ntechniques like LoRA [20] allow one to adapt a text-to-\nimage model to a specific style (e.g., cartoon style, human-\naesthetic-preferred style), they do not allow one to adjust\nthe degree of the desired style. To this end, inspired by the\nclassifier-free guidance mechanism, we propose to perform\ncontrollable stylized text-to-image generation by first con-\nstruct a dataset consisting of human-aesthetic-prefer, car-\ntoon and other samples with a pretrained human aesthetic\nscoring model and a cartoon image classification models,\nand then train the text-to-image generation model with these\nthree kinds of samples. For human-aesthetic-prefer and car-\ntoon samples, we prepend a special prefix to the original\nprompt, denoted as caes and ccartoon respectively. During\nsampling, we extrapolated the prediction in the direction of\n\u03f5\u03b8(zt, t, cstyle) and away from \u03f5\u03b8(zt, t, c) as follows:\n\u02c6\u03f5\u03b8(zt, t, c) = \u03f5\u03b8(zt, t, \u2205) + s \u00b7 (\u03f5\u03b8(zt, t, c) \u2212 \u03f5\u03b8(zt, t, \u2205))\n+ sstyle \u00b7 (\u03f5\u03b8(zt, t, cstyle) \u2212 \u03f5\u03b8(zt, t, c)),\nwhere s is the classifier-free guidance scale, cstyle\n\u2208\n{caes, ccartoon} and sstyle is the style guidance scale.\nFigure 9 shows the controllable stylized text-to-image\n3https://github.com/christophschuhmann/improved-aesthetic-predictor\n\u201c\u5915\u9633\u4e0b\uff0c\u91d1\u8272\u7684\u6c99\u6ee9\uff0c\u6930\u6811\u77d7\u7acb\uff0c\n\u6d77\u6d6a\u8f7b\u62cd\u7740\uff0c\u6d77\u9e25\u5728\u7a7a\u4e2d\u98de\u821e\u3002\u753b\u9762\n\u4ee5\u5199\u5b9e\u98ce\u683c\u5448\u73b0\uff0c\u8272\u5f69\u9c9c\u660e\u4e14\u6e29\u99a8\u3002\u201d\n\u201cUnder the setting sun, coconut trees \nstand on the golden beach, the waves \nare lapping, and seagulls are flying in \nthe sky. The pictures are presented in a \nrealistic style, with bright and warm \ncolors.\u201d\nw/o prompt enhancement\nw/ prompt enhancement\n\u201c\u6c99\u6ee9\u3002\u201d\n\u201cBeach.\u201d\n\u201c\u53d8\u5f62\u91d1\u521a\u3002\u201d\n\u201cTransformers.\u201d\n\u201c\u53cb\u60c5\u3002\u201d\n\u201cFriendship.\u201d\n\u201c\u9713\u8679\u706f\u4e0b\u7684\u53d8\u5f62\u91d1\u521a\uff0c\u8d5b\u535a\u670b\u514b\u98ce\n\u683c\uff0c\u673a\u68b0\u7ec6\u8282\u4e30\u5bcc\uff0c\u91d1\u5c5e\u8d28\u611f\u5f3a\u70c8\uff0c\n\u80cc\u666f\u662f\u672a\u6765\u90fd\u5e02\u7684\u591c\u666f\u3002\u201d\n\u201cTransformers under neon lights, \ncyberpunk style, rich in mechanical \ndetails, strong metallic texture, and the \nbackground is the night scene of the \nfuture city.\u201d\n\u201c\u4e24\u4e2a\u670b\u53cb\u5728\u9633\u5149\u4e0b\u76f8\u62e5\uff0c\u80cc\u666f\u662f\u7f8e\n\u4e3d\u7684\u6821\u56ed\u666f\u8272\u3002\u201d\n\u201cTwo friends hugging each other in the \nsun with a beautiful campus view in the \nbackground.\u201d\nFigure 8.\nText-to-image generation results without and with\nprompt enhancement. Enriched text improve image generation by\nbetter image aesthetic perception (left), more detailed background\n(middle) and better interpretation of abstract concepts (right).\n\u201c\u5706\u5f62\u86cb\u7cd5\u3002\u7c89\u8272\u5976\u6cb9\u3002\u51b0\n\u6c99\u51b0\u6dc7\u6dcb\u3002\u8349\u8393\u88c5\u9970\u3002\u201d\n\u201cRound cake. Pink cream. \nSmoothie ice cream. \nStrawberry decoration.\u201d\n\ud835\udc60\ud835\udc4e\ud835\udc52\ud835\udc60=0\n\ud835\udc60\ud835\udc4e\ud835\udc52\ud835\udc60=3\n\ud835\udc60\ud835\udc4e\ud835\udc52\ud835\udc60=6\n\u201c\u9b54\u6cd5\u5c11\u5973\u3002\u201d\n\u201cMagical girl.\u201d\n\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc5c\ud835\udc5c\ud835\udc5b=0\n\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc5c\ud835\udc5c\ud835\udc5b=3\n\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc5f\ud835\udc61\ud835\udc5c\ud835\udc5c\ud835\udc5b=6\nFigure 9. Controllable stylized text-to-image generation results\nof PanGu-Draw. PanGu-Draw can control the generated images\ntowards the desired style with the style guidance scale. saes for\nhuman-aesthetic-prefer style and scartoon for cartoon style.\ngeneration results of PanGu-Draw,\nincluding human-\naesthetic-prefer and cartoon style image generation.\nAs\nwe can see, with the corresponding style guidance scale,\nPanGu-Draw can control the generated images towards the\ndesired style.\n1\nHuman Input\nLarge Language\nModel \n(tuned w/ human-\nannotated dataset) \nEnhanced Input1\nEnhanced Input2\nEnhanced Input3\nEnhanced Input4\nPanGu-Draw\nAestheic Score\nCLIP Similarity\nScore\nReward1\nReward2\nReward3\nReward4\nChoose The Best\nBackward\nFigure 10. Prompt enhancemnent pipeline with Large Language Model (LLM), specifically tailored for PanGu-Draw. Initially, we fine-\ntune the LLM using a human-annotated dataset, transforming a succinct prompt into a more enriched version. Subsequently, to optimize\nfor PanGu-Draw, we employ the Reward rAnked FineTuning (RAFT) method, as introduced in [8], which selects the prompt pairs yielding\nthe highest reward for further fine-tuning.\nTable 7. The image resolutions used for multi-resolution training\nof structure generation model and texture generation model.\nStructure Generation Model\nTexture Generation Model\nHeight\nWidth\nHeight\nWidth\n512\n2048\n256\n1024\n512\n1920\n256\n960\n704\n1408\n384\n768\n768\n1344\n416\n736\n864\n1152\n480\n640\n1024\n1024\n512\n512\n1152\n864\n640\n480\n1344\n768\n736\n416\n1408\n704\n768\n384\n1920\n512\n960\n256\n2048\n512\n1024\n256\n8. Image Resolutions for Multi-Resolution\nTraining\nTable 7 shows the list of resolutions used for multi-\nresolution training of our structure generation model and\ntexture generation model.\n9. More Generation Results of PanGu-Draw\n9.1. Text-to-Image Generation\nFigure 11 shows more generated images of PanGu-Draw.\nAs we can see, the generated images are of high visual qual-\nity and are well aligned with the input prompts.\n9.2. Multi-Diffusion Fusing Results\nMulti-Control Image Generation. Figure 12 shows re-\nsults of multi-control image generation by fusing PanGu-\nDraw with different models, including image variation,\ndepth-to-image, edge-to-image generation models.\nFigure 13 shows results of fusing two ControlNet mod-\nels with our algorithm and with the algorithm proposed by\nControlNet [51], which fuses the features of different Con-\ntrolNets before injecting into the U-Net model. As we can\nsee, our algorithm is able to specify the prompts of different\nControlNets such that enabling a finer-grain control.\nMulti-Resolution Image Generation. Figure 14 shows the\nresults from the low-resolution model and our fusing algo-\nrithm Coop-Diffusion by fusing the low-resolution model\nand our high-resolution PanGu-Draw model. As we can\nsee, PanGu-Draw adds much details to the low-resolution\npredictions leading to high-fidelity high-resolution results.\n10. Visual Comparison against Baselines\nFigure 15 and 16 shows qualitative comparisons of PanGu-\nDraw against baseine methods, including RAPHAEL [44],\nSDXL [30], DeepFloyd [40], DALL-E 2 [34], ERNIE-\nViLG 2.0 [10], PixArt-\u03b1 [3] and . The input prompts are\nalso used in RAPHAEL and are provided at the bottom of\nthe figure. As we can see, PanGu-Draw generates high-\nquality images, which are better than or on par with these\ntop-performing models.\n2\n\u201c\u5510\u671d\u7f8e\u5c11\u5973\uff0c\u771f\u4eba\uff0c\u751c\u7f8e\u53ef\u7231\uff0c\u9ad8\u6e05\u6444\u5f71\uff0c\u7ec6\u8282\n\u6e05\u6670\uff0c\u7d2b\u8272\u7684\u957f\u53d1\u5728\u9633\u5149\u4e0b\u71a0\u71a0\u751f\u8f89\uff0c\u5fae\u7b11\u7684\u9762\u5bb9\uff0c\n\u8272\u5f69\u9c9c\u8273\u3002\u201d\u201cBeautiful girl from the Tang Dynasty, \nreal person, sweet and cute, high-definition \nphotography, clear details, long purple hair shining in \nthe sun, smiling face, bright colors.\u201d\n\u201c\u4e00\u53ea\u91d1\u8272\u7684\u673a\u68b0\u9e66\u9e49\uff0c\u5b83\u7684\u7fbd\u6bdb\u7531\u95ea\u4eae\u7684\u91d1\u5c5e\u7247\n\u7ec4\u6210\uff0c\u773c\u775b\u50cf\u5b9d\u77f3\u4e00\u6837\u95ea\u8000\uff0c\u7ad9\u5728\u4e00\u6839\u53e4\u8001\u822a\u6d77\u6728\n\u6746\u4e0a\u3002\u201d\n\u201cA golden mechanical parrot, its feathers made of shiny \nmetal flakes and its eyes shining like gems, stands on \nan ancient nautical wooden pole.\u201d\n\u201c\u52a8\u6f2b\u98ce\u683c\u7684\u98ce\u666f\u753b\uff0c\u6709\u5c71\u8109\u3001\u6e56\u6cca\uff0c\u4e5f\u6709\u7e41\u534e\u7684\u5c0f\u9547\u5b50\uff0c\u8272\u5f69\u9c9c\u8273\uff0c\u5149\u5f71\u6548\u679c\u660e\u663e\u3002\u201d\n\u201cAnimation-style landscape paintings, including mountains, lakes, and bustling towns, with bright colors and \nobvious light and shadow effects.\u201d\n\u201c\u4e00\u53ea\u7cbe\u81f4\u7684\u9676\u74f7\u732b\u54aa\u96d5\u50cf\uff0c\u5168\u8eab\u7ed8\u6709\u7cbe\u7f8e\u7684\u4f20\u7edf\n\u82b1\u7eb9\uff0c\u773c\u775b\u4eff\u4f5b\u4f1a\u53d1\u5149\u3002\u201d \u201cAn exquisite ceramic \ncat statue with exquisite traditional patterns painted all \nover its body and eyes that seem to glow.\u201d\n\u201c\u4e00\u53ea\u60a0\u7136\u81ea\u5f97\u7684\u957f\u9888\u9e7f\u5728\u7eff\u8272\u8349\u539f\u4e0a\u60a0\u95f2\u5730\u884c\n\u8d70\u3002\u201d\n\u201cA carefree giraffe walks leisurely on the green \ngrassland.\u201d\n\u201c\u8d85\u5927\u5e7f\u89d2\u4e0b\uff0c\u6c99\u6f20\u3001\u6cb3\u6d41\u3001\u7eff\u6d32\u4ea4\u76f8\u8f89\u6620\uff0c\u843d\u65e5\u4f59\u6656\u6d12\u6ee1\u5927\u5730\uff0c\u6b64\u666f\u5b9b\u5982\u6444\u5f71\u827a\u672f\u5bb6\u7684\u4f5c\u54c1\uff0c\u753b\u9762\u5f00\n\u9614\uff0c\u8272\u5f69\u4e30\u5bcc\u3002\u201d\u201cUnder the super wide angle, deserts, rivers, and oasis complement each other, and the \nsetting sun fills the earth. This scene is like the work of a photography artist, with a broad picture and rich colors.\u201d\n\u201c\u4e00\u5e45\u79d1\u5e7b\u98ce\u683c\u7684\u5b87\u822a\u5458\u7167\u7247\uff0c\u80cc\u666f\u4e3a\u661f\u7a7a\uff0c\u4eba\u7269\n\u5de6\u53f3\u5bf9\u79f0\uff0c\u84dd\u5149\u7167\u4eae\u5168\u8eab\uff0c\u7167\u7247\u6e05\u6670\uff0c\u8272\u5f69\u9c9c\u660e\uff0c\n\u4ee5\u7eb8\u827a\u548c\u751f\u7269\u5149\u6548\u679c\u5448\u73b0\u3002\u201d\u201cA sci-fi style photo of \nan astronaut with a starry sky in the background. The \nfigure is symmetrical. Blue light illuminates the whole \nbody. The photo is clear and colorful. It is presented \nwith paper art and bioluminescence effects.\u201d\n\u201c\u55b5\u55b5\u5e3d\u7684\u7eff\u8272\u82b1\u6735\uff0c\u6d1b\u4e3d\u5854\u98ce\u683c\u7684\u6444\u5f71\uff0c\u8272\u5f69\u9c9c\n\u660e\uff0c\u5149\u5f71\u81ea\u7136\u3002\u201d\u201cGreen flowers in Meow Hat, \nLolita style photography, bright colors, natural light \nand shadow.\u201d\n\u201c\u4e00\u53ea\u8272\u5f69\u6591\u6593\u7684\u9e66\u9e49\uff0c\u6816\u606f\u5728\u7e41\u82b1\u4f3c\u9526\u7684\u6811\u679d\u4e0a\uff0c\n\u753b\u9762\u98ce\u683c\u4e3a\u5199\u5b9e\u6444\u5f71\uff0c\u5149\u5f71\u4e0b\u9e66\u9e49\u6bdb\u53d1\u7eb9\u7406\u6e05\u6670\u53ef\n\u89c1\u3002\u201d\u201cA colorful parrot is perched on a branch full of \nflowers. The style of the picture is realistic photography. \nThe texture of the parrot's hair is clearly visible under \nthe light and shadow.\u201d\n\u201c\u4e00\u9876\u88c5\u9970\u6709\u5404\u79cd\u70ed\u5e26\u9e1f\u7c7b\u7fbd\u6bdb\u7684\u534e\u4e3d\u5e3d\u5b50\u3002\u201d\n\u201cAn ornate hat decorated with feathers from various \ntropical birds.\u201d\n\u201c\u91d1\u8272\u7334\u5b50\u8eab\u7740\u673a\u7532\uff0c\u8eab\u59ff\u5a07\u5c0f\u53ef\u7231\uff0c\u5177\u6709\u5f3a\u70c8\u7684\n\u6b66\u4fa0\u5143\u7d20\uff0c\u5c45\u4e2d\u5c55\u73b0\u3002\u91c7\u7528\u7535\u5f71\u7279\u6548\u548c\u7c92\u5b50\u7279\u6548\uff0c\n\u5149\u7ebf\u8ffd\u8e2a\uff0c\u5448\u73b0\u51fa8K\u753b\u8d28\u548c\u6781\u4f73\u7684\u7ec6\u8282\u523b\u753b\u3002\u201d\n\u201cThe golden monkey is dressed in a mecha, petite and \ncute, with strong martial arts elements, shown in the \ncenter. It uses movie special effects, particle special \neffects, and ray tracing to present 8K image quality and \nexcellent detail characterization.\u201d\n\u201c\u6781\u5177\u771f\u5b9e\u611f\u7684\u590d\u6742\u7684\u8001\u4eba\u8096\u50cf\u3002\u201d\n\u201cRealistically complex portrait of an elderly person.\u201d\nFigure 11. Images generated with PanGu-Draw, our 5B multi-lingual text-to-image generation model. PanGu-Draw is able to generate\nmulti-resolution high-fidelity images semantically aligned with the input prompts.\n3\n\u201c\u4e00\u680b\u9ed1\u8272\u7684\u623f\u5b50\u201d \n(\"a black house\")\n(a) Input image and generation results of image variation.\n(b) Input prompt and generation results of PanGu-Draw.\n(c) Generation results of fusing image\nvariation and PanGu-Draw with Coop-\nDiffusion algorithm.\n\u201c\u4e00\u4e2a\u5973\u4eba\u201d \n(\"a woman\")\n(a) Input depth map and generation results of depth-to-image.\n(b) Input prompt and generation results of PanGu-Draw.\n(c) Generation results of fusing depth-to-\nimage and PanGu-Draw with Coop-\nDiffusion algorithm.\n(a) Input edge map and generation results of edge-to-image.\n\u201c\u4e00\u680b\u9ed1\u8272\u7684\u623f\u5b50\u201d \n(\"a black house\")\n(b) Input prompt and generation results of PanGu-Draw.\n(c) Generation results of fusing edge-to-\nimage and PanGu-Draw with Coop-\nDiffusion algorithm.\nFigure 12.\nGeneration results of the fusing of an image\nvariation/depth-to-image/edge-to-image model and PanGu-Draw\nwith the proposed Coop-Diffusion algorithm.\n\"a black dog on the\nleft and a white dog\non the right on the\ngrass\"\n(a) Input prompts and edge maps, and generation results of single edge-to-image ControlNet.\n\"a white dog on the\ngrass.\"\n\"a black dog on the\ngrass.\"\n(b) Input prompt and generation results of Multi-ControlNets. The\ninput edge maps are showed in (a).\n(c) Generation results of fusing two\nControlNets in (a) with Coop-Diffusion.\nFigure 13.\nGeneration results of the fusing of an image\nvariation/depth-to-image/edge-to-image model and PanGu-Draw\nwith the proposed Coop-Diffusion algorithm.\nLR model\nLR model + PanGu-Draw\nFigure 14. Images generated with a low-resolution (LR) model\nand the fusion of the LR model and HR PanGu-Draw with our\nCoop-Diffusion. This allows for single-stage super-resolution for\nbetter details and higher inference efficiency.\n4\nRAPHAEL\nStable Diffusion XL\nDeepFloyd\nPanGu-Draw\nDALL-E 2\nERNIE-ViLG 2.0\nPixArt-\u03b1\n1. A parrot with a pearl earring, Vermeer style.\n2. A car playing soccer, digital art.\n3. A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style.\n4. Moonlight Maiden, cute girl in school uniform, long white hair, standing under the moon, celluloid style, Japanese \nmanga style.\n5. Street shot of a fashionable Chinese lady in Shanghai, wearing black high-waisted trousers.\n6. Half human, half robot, repaired human, human flesh warrior, mech display, man in mech, cyberpunk.\nFigure 15. Visual comparison of PanGu-Draw against baseline methods, including RAPHAEL [44], SDXL [30], DeepFloyd [40], DALL-E\n2 [34], ERNIE-ViLG 2.0 [10], and PixArt-\u03b1 [3]. The input prompts are also used in RAPHAEL and are provided at the bottom of the\nfigure. The results of PanGu-Draw are better than or on par with these top-performing baseline models.\n5\nRAPHAEL\nStable Diffusion XL\nDeepFloyd\nPangu-Draw\nDALL-E 2\nERNIE-ViLG 2.0\nPixArt-\u03b1\nMidjourney V5.1\n1. A cute little matte low poly isometric cherry blossom forest island, waterfalls, lighting, soft shadows, trending on Artstation, \n3d render, monument valley, fez video game.\n2. A shanty version of Tokyo, new rustic style, bold colors with all colors palette, video game, genshin, tribe, fantasy, overwatch.\n3. Cartoon characters, mini characters, figures, illustrations, flower fairy, green dress, brown hair, curly long hair, elf-like wings, \nmany flowers and leaves, natural scenery, golden eyes, detailed light and shadow , a high degree of detail.\n4. Cartoon characters, mini characters, hand-made, illustrations, robot kids, color expressions, boy, short brown hair, curly hair, \nblue eyes, technological age, cyberpunk, big eyes, cute, mini, detailed light and shadow, high detail.\nFigure 16. Visual comparison of PanGu-Draw against baseline methods, including DALL-E 2 [34], ERNIE-ViLG 2.0 [10], DeepFloyd\n[40], SDXL [30], RAPHAEL [44],Midjourney V5.1 and PixArt-\u03b1 [3]. The input prompts are also used in RAPHAEL and are provided at\nthe bottom of the figure. The results of PanGu-Draw are better than or on par with these top-performing baseline models.\n6\n"
  },
  {
    "title": "SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation",
    "link": "https://arxiv.org/pdf/2312.16272.pdf",
    "upvote": "5",
    "text": "SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven\nGeneration\nYuxuan Zhang1*, Yiren Song4, Jiaming Liu2\u2020, Rui Wang3*, Jinpeng Yu6*, Hao Tang5,\nHuaxia Li2, Xu Tang2, Yao Hu2, Han Pan1, Zhongliang Jing1\u2020\n1Shanghai Jiao Tong University, 2Xiaohongshu Inc., 3Beijing University of Posts and Telecommunications,\n4National University of Singapore, 5Carnegie Mellon University, 6ShanghaiTech University\nFigure 1. Our SSR-Encoder is a model generalizable encoder, which is able to guide any customized diffusion models for single subject-\ndriven image generation (top branch) or multiple subject-driven image generation from different images (middle branch) based on the\nimage representation selected by the text query or mask query without any additional test-time finetuning. Furthermore, our SSR-Encoder\ncan also be applied for the controllable generation with additional control (bottom branch).\nAbstract\nRecent advancements in subject-driven image gener-\nation have led to zero-shot generation, yet precise se-\nlection and focus on crucial subject representations re-\nmain challenging. Addressing this, we introduce the SSR-\nEncoder, a novel architecture designed for selectively cap-\n*Work done during internship at Xiaohongshu Inc.\n\u2020Corresponding authors.\nturing any subject from single or multiple reference images.\nIt responds to various query modalities including text and\nmasks, without necessitating test-time fine-tuning. The SSR-\nEncoder combines a Token-to-Patch Aligner that aligns\nquery inputs with image patches and a Detail-Preserving\nSubject Encoder for extracting and preserving fine features\nof the subjects, thereby generating subject embeddings.\nThese embeddings, used in conjunction with original text\nembeddings, condition the generation process. Character-\narXiv:2312.16272v2  [cs.CV]  14 Mar 2024\nized by its model generalizability and efficiency, the SSR-\nEncoder adapts to a range of custom models and control\nmodules. Enhanced by the Embedding Consistency Regu-\nlarization Loss for improved training, our extensive exper-\niments demonstrate its effectiveness in versatile and high-\nquality image generation, indicating its broad applicability.\nProject page: ssr-encoder.github.io\n1. Introduction\nRecent advancements in image generation, especially\nwith the advent of text-to-image diffusion models trained on\nextensive datasets, have revolutionized this field. A prime\nexample is Stable Diffusion, an open-source model cited as\n[40], which allows a broad user base to easily generate im-\nages from textual prompts. A growing area of interest that\nhas emerged is the subject-driven generation, where the fo-\ncus shifts from creating a generic subject, like \u201ca cat\u201d to\ngenerating a specific instance, such as \u201cthe cat\u201d.\nHow-\never, crafting the perfect text prompt to generate the de-\nsired subject content poses a significant challenge. Con-\nsequently, researchers are exploring various strategies for\neffective subject-driven generation.\nSubject-driven image generation aims to learn subjects\nfrom reference images and generate images aligning with\nspecific concepts like identity and style.\nCurrently, one\nprominent approach involves test-time fine-tuning [1, 12,\n24, 41], which, while efficient, requires substantial compu-\ntational resources to learn each new subject. Another ap-\nproach [7, 14, 22, 45, 50] encodes the reference image into\nan image embedding to bypass the fine-tuning cost. How-\never, these encoder-based models typically require joint\ntraining with the base diffusion model, limiting their gener-\nalizability. A concurrent work, IP-adapter [53], tackles both\nfine-tuning costs and generalizability by learning a projec-\ntion to inject image information into the U-Net, avoiding\nthe need to fine-tune the base text-to-image model, thereby\nbroadening its application in personalized models.\nDespite these advancements, a critical aspect often over-\nlooked is the extraction of the most informative representa-\ntion of a subject. With images being a complex mixture of\nsubjects, backgrounds, and styles, it\u2019s vital to focus on the\nmost crucial elements to represent a subject effectively. To\naddress this, we introduce the SSR-Encoder, an image en-\ncoder that generates Selective Subject Representations for\nsubject-driven image generation.\nOur SSR-Encoder firstly aligns patch-level visual em-\nbeddings with texts in a learnable manner, capturing de-\ntailed subject embeddings guided by token-to-patch atten-\ntion maps. Furthermore, we propose subject-conditioned\ngeneration,\nwhich utilizes trainable copies of cross-\nattention layers to inject multi-scale subject information.\nA novel Embedding Consistency Regularization Loss is\nproposed to enhance the alignment between text queries\nand visual representations in our subject embedding space\nduring training. This approach not only ensures effective\ntoken-to-patch alignment but also allows for flexible sub-\nject selection through text and mask queries during infer-\nence. Our SSR-Encoder can be seamlessly integrated into\nany customized stable diffusion models without extensive\nfine-tuning. Moreover, the SSR-Encoder is adaptable for\ncontrollable generation with various additional controls, as\nillustrated in Fig. 1.\nWe summarize our main contributions as follows:\n\u2022 We propose a novel framework, termed as SSR-Encoder,\nfor selective subject-driven image generation.\nIt al-\nlows selective single- or multiple-subject generation,\nfully compatible with ControlNets (e.g. canny, OpenPose,\netc.), and customized stable diffusion models without ex-\ntra test-time training.\n\u2022 Token-to-Patch Aligner and Detail-Preserved Subject En-\ncoder are proposed in our SSR-Encoder to learn selective\nsubject embedding. We also present an Embedding Con-\nsistency Regularization Loss to enhance token-to-patch\ntext-image alignment in the subject embedding space.\n\u2022 Our extensive experiments have validated the robustness\nand flexibility of our approach, showcasing its capa-\nbility to deliver state-of-the-art (SOTA) results among\nfinetuning-free methods.\nImpressively, it also demon-\nstrates competitive performance when compared with\nfinetuning-based methods.\n2. Related Work\nText-to-image diffusion models.\nIn recent years,\ntext-to-image diffusion models [2, 34, 36, 38\u201340, 42,\n43, 51, 54] have made remarkable progress, particularly\nwith the advent of diffusion models, which have pro-\npelled text-to-image generation to large-scale commercial-\nization. DALLE [38] first achieved stunning image gener-\nation results using an autoregressive model. Subsequently,\nDALLE2 [39] employed a diffusion model as the genera-\ntive model, further enhancing text-to-image synthesis abil-\nity. Imagen [42] and Stable Diffusion [40] trained diffu-\nsion models on larger datasets, further advancing the devel-\nopment of diffusion models and becoming the mainstream\nfor image generation large models. DeepFloyd IF [43] uti-\nlized a triple-cascade diffusion model, significantly enhanc-\ning the text-to-image generation capability, and even gener-\nating correct fonts. Stable Diffusion XL [36], a two-stage\ncascade diffusion model, is the latest optimized version of\nstable diffusion, greatly improving the generation of high-\nfrequency details, small object features, and overall image\ncolor.\nControllable image generation.\nCurrent diffusion\nmodels can incorporate additional modules, enabling im-\nage generation guided by multimodal image information\nsuch as edges, depth maps, and segmentation maps. These\nmultimodal inputs significantly enhance the controllability\nof the diffusion model\u2019s image generation process. Meth-\nods like ControlNet [55] utilize a duplicate U-Net struc-\nture with trainable parameters while keeping the original\nU-Net parameters static, facilitating controllable generation\nwith other modal information. T2I-adapter [33] employs\na lightweight adapter for controlling layout and style us-\ning different modal images. Uni-ControlNet [58] differenti-\nates between local and global control conditions, employing\nseparate modules for injecting these control inputs. Paint\nby Example [52] allows for specific region editing based\non reference images. Other methods [3, 5, 11, 17, 32, 57]\nmanipulate the attention layer in the diffusion model\u2019s de-\nnoising U-Net to direct the generation process. P2P [17]\nand Null Text Inversion [32] adjust cross-attention maps to\npreserve image layout under varying text prompts.\nSubject-driven image generation. Subject-driven im-\nage generation methods generally fall into two categories:\nthose requiring test-time finetuning and those that do not.\nThe differences in characteristics of these methods are il-\nlustrated in Table 1. Test-time finetuning methods [1, 6, 12,\n13, 16, 20, 24, 41, 49] often optimize additional text em-\nbeddings or directly fine-tune the model to fit the desired\nsubject. For instance, Textual Inversion [12] optimizes ad-\nditional text embeddings, whereas DreamBooth [41] adjusts\nthe entire U-Net in the diffusion model. Other methods like\nCustomdiffusion [24] and SVDiff [16] minimize the param-\neters needing finetuning, reducing computational demands.\nFinetuning-free methods [14, 22, 26, 30, 45, 48, 50, 53]\ntypically train an additional structure to encode the refer-\nence image into embeddings or image prompts without ad-\nditional finetuning. ELITE [50] proposes global and local\nmapping training schemes to generate subject-driven im-\nages but lack fidelity. Instantbooth [45] proposes an adapter\nstructure inserted in the U-Net and trained on domain-\nspecific data to achieve domain-specific subject-driven im-\nage generation without finetuning. IP-adapter [53] encodes\nimages into prompts for subject-driven generation. BLIP-\nDiffusion [26] enables efficient finetuning or zero-shot se-\ntups. However, many of these methods either utilize all in-\nformation from a single image, leading to ambiguous sub-\nject representation, or require finetuning, limiting general-\nizability and increasing time consumption. In contrast, our\nSSR-Encoder is both generalizable and efficient, guiding\nany customized diffusion model to generate images based\non the representations selected by query inputs without any\ntest-time finetuning.\n3. The Proposed Method\nSelective subject-driven image generation aims to gen-\nerate target subjects in a reference image with high fidelity\nand creative editability, guided by the user\u2019s specific queries\nTable 1. Comparative Analysis of Previous works. Considering\nFine-Tuning free, Model Generalizability, and Selective Represen-\ntation, SSR-Encoder is the first method offering all three features.\nMethod\nFinetuning\n-free\nModel\nGeneralizable\nSelective\nRepresentation\nTextual Inversion [12]\n\u2717\n\u2713\n\u2717\nDreambooth [41]\n\u2717\n\u2717\n\u2717\nLoRA [20]\n\u2717\n\u2713\n\u2717\nCustom diffusion [24]\n\u2717\n\u2717\n\u2717\nBreak-A-Scene [1]\n\u2717\n\u2717\n\u2713\nE4T [14]\n\u2717\n\u2717\n\u2717\nInstantbooth [45]\n\u2713\n\u2717\n\u2717\nELITE [50]\n\u2713\n\u2717\n\u2717\nTaming [22]\n\u2713\n\u2717\n\u2717\nIP-adapter [53]\n\u2713\n\u2713\n\u2717\nBLIP-diffusion [26]\n\u2713\n\u2717\n\u2713\nSSR-Encoder(Ours)\n\u2713\n\u2713\n\u2713\n(text or mask). To tackle this, we propose our SSR-Encoder,\na specialized framework designed to integrate with any cus-\ntom diffusion model without necessitating test-time fine-\ntuning.\nFormally, for a given reference image I and a user query\nq, the SSR-Encoder effectively captures subject-specific in-\nformation and generates multi-scale subject embeddings cs.\nThese multi-scale subject embeddings cs are subsequently\nintegrated into the U-Net model with trainable copies of\ncross-attention layers. The generation process, conditioned\non both subject embeddings cs and text embedding ct, al-\nlows for the production of desired subjects with high fidelity\nand creative editability. The overall methodology is illus-\ntrated in Fig. 2.\nIn general, SSR-Encoder is built on text-to-image dif-\nfusion models [40]1.\nIt comprises two key components:\nthe token-to-patch aligner and detail-preserving subject en-\ncoder (Sec. 3.1). The subject-conditioned generation pro-\ncess is detailed in Sec. 3.2. Lastly, training strategies and\nloss functions are presented in Sec. 3.3.\n3.1. Selective Subject Representation Encoder\nOur Selective Subject Representation Encoder (SSR-\nEncoder) is composed of two integral parts: Token-to-Patch\nAligner and Detail-Preserving Subject Encoder. The details\nof each component are as follows.\nToken-to-patch aligner. Several works [8, 29, 59] have\npointed out that CLIP tends to prioritize background regions\nover foreground subjects when identifying target categories.\nTherefore, relying solely on text-image similarity may not\nadequately capture subject-specific information. To address\nthis issue, we propose the Token-to-Patch (T2P) Aligner,\nwhich implements two trainable linear projections to align\nimage patch features with given text token features. Math-\nematically, given a query text-image pair (q, I ), we em-\n1Reviewed in the Supplementary.\nFigure 2. Overall schematics of our method. Given a query text-image pairs (q, I), the SSR-Encoder employs a token-to-patch aligner to\nhighlight the selective regions in the reference image by the query. It extracts more fine-grained details of the subject through the detail-\npreserving subject encoder, projecting multi-scale visual embeddings via the token-to-patch aligner. Then, we adopt subject-conditioned\ngeneration to generate specific subjects with high fidelity and creative editability. During training, we adopt reconstruction loss LLDM and\nembedding consistency regularization loss Lreg for selective subject-driven learning.\nploy pre-trained CLIP encoders to generate the text query\nand image reference into query embedding zq \u2208 RNq\u00d7Dq\nand semantic visual embedding z0 \u2208 RNi\u00d7Di from the last\nCLIP layer, respectively, where N(\u00b7) and D(\u00b7) represent the\nnumber of tokens and dimensions for query and image fea-\ntures respectively. We then use the trainable projection lay-\ners WQ and WK to transform them into a well-aligned\nspace. The alignment is illustrated as follows:\nQ = WQ \u00b7 zq,\nK = WK \u00b7 z0,\n(1)\nAt2p = Softmax\n\u0012QK \u22a4\n\u221a\nd\n\u0013\n,\n(2)\nwhere At2p \u2208 RNt\u00d7Ni represents the token-to-patch atten-\ntion map.\nFurthermore, the At2p matrix serves a dual purpose:\nsimilarity identification and region selection. Consequently,\nour aligner naturally supports mask-based query. In prac-\ntice, we can manually assign a mask M to At2p for mask-\nguided generation with null-text query inputs. Following\nEq. (2), we can proceed to reweight At2p using the prede-\nfined mask M to highlight selected regions, ensuring our\nSSR-Encoder focuses solely on the selected valid regions\nof reference images.\nDetail-preserving subject encoder. Following most of\nthe preceding methods [28, 50, 53], we employ a pre-trained\nCLIP visual backbone to extract image representations from\nreference images. However, the conventional practice of ex-\ntracting visual embeddings z0 from the last CLIP layer does\nnot align with our objective of preserving fine details to the\nmaximum extent. Our preliminary experiments2 have iden-\ntified a notable loss of fine-grained details in the semantic\nimage features z0. Addressing this, we introduce the detail-\npreserving subject encoder, which extracts features across\nvarious layers to preserve more fine-grained details. For-\nmally, the visual backbone processes an image I to produce\nmulti-scale detailed image features zI = {zk}K\nk=0, where z0\n2Detailed in the supplementary.\nrepresents semantic visual embedding used in T2P aligner\nand zk represents other detailed visual embeddings at the\nscale of k in CLIP visual backbone and K refers to the\nnumber of target scales. We set K to 6 in all experimen-\ntal settings.\nTo fully leverage the benefits of multi-scale represen-\ntation, we adopt separate linear projections WV\nk for im-\nage feature zk at different scales.\nCombining with the\ntoken-to-patch attention map At2p, the subject embeddings\ncs = {ck\ns }K\nk=0 are computed as per Eq. (3):\nVk = WV\nk \u00b7 zk, ck\ns = At2pV \u22a4\nk ,\n(3)\nwhere ck\ns denotes subject embedding at scale of k. Our\nSSR-Encoder now enables to capture multi-scale subject\nrepresentation cs = {ck\ns }K\nk=0, which are subsequently used\nfor subject-driven image generation via subject-conditioned\ngeneration process.\n3.2. Subject Conditioned Generation\nIn our approach, cs is strategically projected into the\ncross-attention layers of the U-Net.\nThis is achieved\nthrough newly added parallel subject cross-attention lay-\ners, each corresponding to a text cross-attention layer in\nthe original U-Net. Rather than disturbing the text embed-\nding ct, these new layers independently aggregate subject\nembeddings cs. Inspired by works like [24, 50, 53, 55],\nwe employ trainable copies of the text cross-attention lay-\ners to preserve the efficacy of the original model. The key\nand value projection layers are then adapted to train specif-\nically for a subject-conditioned generation. To full exploit\nof both global and local subject representation, we concate-\nnate all ck\ns at the token dimension before projection, i.e.\nc\u2032\ns = concat\n\u0000ck\ns , dim = 0\n\u0001\n, where ck\ns \u2208 RNq\u00d7Di repre-\nsents subject representation at the scale of k. The output\nvalue O of the attention layer is formulated as follows:\nO = CrossAttention (Q, K, V, ct, xt)\n|\n{z\n}\ntext condition\n+ \u03bb CrossAttention (Q, KS, VS, c\u2032\ns, xt)\n|\n{z\n}\nsubject condition\n,\n(4)\nwhere ct represents the text embedding and xt represents\nthe latent. Q, K, V represents query, key, and value pro-\njection layers in the original text branch respectively while\nKS, VS represents trainable copies of key and value pro-\njection layers for concatenated subject embedding cs. \u03bb is\na weight adjustment factor, with a default value of 1.\nBy our subject-conditioned generation, text-to-image\ndiffusion models can generate target subjects conditioned\non both text embeddings and subject embeddings.\n3.3. Model Training and Inference\nDuring the training phase, our model processes paired\nimages and texts from multimodal datasets. The trainable\ncomponents include the token-to-patch aligner and the sub-\nject cross-attention layers.\nIn contrast to CLIP, which aligns global image fea-\ntures with global text features, our token-to-patch aligner\ndemands a more granular token-to-patch alignment.\nTo\nachieve this, we introduce an Embedding Consistency Reg-\nularization Loss Lreg.\nThis loss is designed to enhance\nsimilarity between the subject embeddings cs and the cor-\nresponding query text embedding zq, employing a cosine\nsimilarity function as demonstrated in Eq. (5):\ncs = Mean\n\u0000c0\ns , c1\ns , ..., cK\ns\n\u0001\n,\nLreg = Cos (cs, zq) = 1 \u2212 cs \u00b7 zq\n|cs||zq|,\n(5)\nwhere cs is the mean of subject embeddings and zq repre-\nsents the query text embeddings. As illustrated in Fig. 5, our\nT2P Aligner, trained on a large scale of image-text pairs,\ncan effectively align query text with corresponding image\nregions. This capability is a key aspect of selective subject-\ndriven generation.\nSimilar to the original Stable diffusion model, our train-\ning objective also includes the same LLDM loss, as outlined\nin Eq. (6):\nLLDM(\u03b8) = Ex0 ,t,\u03f5\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (xt, t, ct, cs)\u22252\n2\ni\n,\n(6)\nwhere xt is the noisy latent at time step t, \u03f5 is the ground-\ntruth latent noise. \u03f5\u03b8 is the noise prediction model with pa-\nrameters \u03b8.\nThus, our total loss function is formulated as:\nLtotal = LLDM + \u03c4Lreg,\n(7)\nwhere \u03c4 is set as a constant, with a value of 0.01. As de-\npicted in Fig. 6 (in the last column), the inclusion of Lreg\nsignificantly enhances the text-image alignment capabilities\nof the SSR-Encoder. This improvement is evident in the\ngenerated images, which consistently align with both the\nsubject prompt and the details of the reference image.\nDuring inference, our method has the ability to decom-\npose different subjects from a single image or multiple im-\nages. By extracting separate subject embeddings for each\nimage and concatenating them together, our SSR-Encoder\ncan seamlessly blend elements from multiple scenes. This\nflexibility allows for the creation of composite images with\nhigh fidelity and creative versatility.\n4. Experiment\n4.1. Experimental Setup\nTraining data. Our model utilizes the Laion 5B dataset,\nselecting images with aesthetic scores above 6.0. The text\nprompts are re-captioned using BLIP2. The dataset com-\nprises 10 million high-quality image-text pairs, with 5,000\nimages reserved for testing and the remainder for training.\nImplementation details. We employed Stable Diffusion\nV1-5 as the pre-trained diffusion model, complemented by\nthe pre-trained CLIP text encoder. For training, images are\nresized to ensure the shortest side is 512 pixels, followed by\na center crop to achieve a 512\u00d7512 resolution, and sent to\nthe stable diffusion. The same image is resized to 224\u00d7224\nand sent to the SSR encoder. The model training process\nis divided into two steps. In the first step, the multi-scale\nstrategy is not employed, and the model is trained for 1 mil-\nlion steps on 8H800s GPUs, with a batch size of 16 per\nGPU and a learning rate of 5e-5. In the second step, the\nsame hyper-parameters are used, and the model parameters\nobtained from the first step are used as the initialization pa-\nrameters. The multi-scale strategy is employed in this step\nto train the model for an additional 100,000 steps. Inference\nwas performed using DDIM as the sampler, with a step size\nof 30 and a guidance scale set to 7.5.\n4.2. Evaluation Metrics\nTo evaluate our model, we employ several metrics and\ndatasets:\n\u2022 Multi-subject bench: We created a benchmark with 100\nimages, each containing 2-3 subjects.\n\u2022 DreamBench datasets [41]: This dataset includes 30\nsubjects, each represented by 4-7 images.\nFor a comprehensive comparison with state-of-the-art\n(SOTA) methods, we employed the following metrics:\nDINO Scores[4], CLIP-I[37] and DINO-M Scores to as-\nsess subject alignment, CLIP-T [18] for evaluating image-\ntext alignment, CLIP Exclusive Score (CLIP-ES) to mea-\nsure the exclusivity of subject representation, and the Aes-\nthetic Score [44] to gauge the overall quality of the gener-\nated images.\nNotably, CLIP-ES is calculated by generating an image\nI using prompts for subject A from a reference image and\nevaluating the CLIP-T score with a different subject B and\nI. A lower CLIP-ES score indicates higher exclusivity. The\nFigure 3. Qualitative results of SSR-Encoder in different generative capabilities. Our method supports two query modalities and is\nadaptable for a variety of tasks, including single- and multi-subject conditioned generation. Its versatility extends to integration with other\ncustomized models and compatibility with off-the-shelf ControlNets.\nFigure 4. Qualitative comparison of different methods. Our results not only excel in editability and exclusivity but also closely resemble\nthe reference subjects in visual fidelity. Notably, the SSR-Encoder achieves this without the need for fine-tuning.\nDINO-M score, specifically designed for multiple subjects,\nevaluates identity similarity between masked versions of in-\nput and generated images, as detailed in [1]. Both CLIP-\nES and DINO-M scores are evaluated on the Multi-Subject\nBench.\n4.3. Comparison Methods\nFor a comprehensive evaluation of our method, we\nbenchmarked it against a range of state-of-the-art (SOTA)\ntechniques.\nThe methods we compared are categorized\nbased on their approach to fine-tuning.\nIn the fine-\ntuning-based category, we include Textual Inversion [12],\nDreambooth [41], and Break-a-Scene [1].\nFor fine-\ntuning-free methods, our comparison encompassed Refer-\nence Only [31], Elite [50], IP-adapter [53], and BLIPDif-\nfusion [26]. This selection of methods provides a diverse\nrange of approaches for a thorough comparative analysis\nwith our SSR-Encoder.\n4.4. Experiment Results\nQuantitative comparison. Table 2 presents our quanti-\ntative evaluation across two benchmarks: the Multi-Subject\nBench and DreamBench.\nOverall, SSR-Encoder clearly\noutweighs previous SOTA finetuning-free methods on all of\nthe metrics, including subject alignment, image-text align-\nment, subject exclusivity, and overall quality. Remarkably,\nit also outperforms fine-tuning-based methods in image\nquality and image-text alignment within both benchmarks.\nParticularly in the Multi-Subject Benchmark, the SSR-\nEncoder demonstrates outstanding performance in subject\nexclusivity, markedly outperforming competing methods.\nThis highlights the efficacy of its selective representation\ncapability and editability. While Dreambooth excels in sub-\nject alignment within the DreamBench dataset, the SSR-\nEncoder and Break-A-Scene show comparable performance\non the Multi-Subject Bench. This suggests that although\nDreambooth is highly effective in capturing detailed subject\ninformation, SSR-Encoder achieves a balanced and compet-\nitive performance in subject representation.\nQualitative comparison.\nFig. 3 displays the high-\nfidelity outcomes produced by the SSR-Encoder using di-\nverse query inputs, affirming its robustness and zero-shot\ngenerative capabilities.\nThe SSR-Encoder demonstrates\nproficiency in recognizing and focusing on common con-\ncepts, ensuring an accurate representation of the selected\nimage subjects.\nIts seamless integration with other cus-\ntomized models and control modules further solidifies its\nsignificant role in the stable diffusion ecosystem.\nIn qualitative comparisons, as depicted in Fig. 15, Tex-\ntual Inversion and Reference Only encounter difficulties in\nmaintaining subject identity. Dreambooth, IP-adapter, and\nBLIP-Diffusion, although advanced, exhibit limitations in\neffectively disentangling intertwined subjects.\nBreak-A-\nScene achieves commendable subject preservation but at the\ncost of extensive fine-tuning. ELITE, with its focus on local\naspects through masks, also faces challenges in consistent\nidentity preservation.\nIn contrast, our SSR-Encoder method stands out for its\nfast generation of selected subjects while adeptly preserving\ntheir identities. This capability highlights the method\u2019s su-\nperior performance in generating precise and high-quality\nsubject-driven images, thereby addressing key challenges\nfaced by other current methods.\nAblation study. Our ablation study begins with visual-\nizing the attention maps generated by our Token-to-Patch\nAligner, as shown in Fig. 5. These maps demonstrate how\ndifferent text tokens align with corresponding patches in the\nreference image, evidencing the Aligner\u2019s effectiveness.\nTo evaluate the significance of various components, we\nconducted experiments by systematically removing them\nand observing the outcomes. Initially, we removed the sub-\nject condition, relying solely on the text condition for im-\nage generation, to determine if the subject details could be\nimplicitly recalled by the base model. Subsequently, we\nTable 2. Quantitative comparison of different methods. Metrics that are bold and underlined represent methods that rank 1st and 2nd,\nrespectively. \u2020 indicates that the experimental value is referenced from BLIP-Diffusion[26].\nType\nMethod\nCLIP-T \u2191\nCLIP-ES \u2193\nDINO-M \u2191\nCLIP-T \u2191\nDINO \u2191\nCLIP-I \u2191\nAesthetic\n(Multi-subject bench)\n(DreamBench)\nScore\u2191\nTextual Inversion\n0.240\n0.212\n0.410\n0.255\u2020\n0.569\u2020\n0.780\u2020\n6.029\nFinetune-based\nDreambooth\n0.298\n0.223\n0.681\n0.305\u2020\n0.668\u2020\n0.803\u2020\n6.330\nmethods\nBreak-A-Scene\n0.285\n0.187\n0.630\n0.287\n0.653\n0.788\n6.234\nOurs(full)\n0.302\n0.182\n0.556\n0.308\n0.612\n0.821\n6.563\nBLIP-Diffusion\n0.287\n0.198\n0.514\n0.300\u2020\n0.594\u2020\n0.779\u2020\n6.212\nReference only\n0.242\n0.195\n0.434\n0.286\n0.542\n0.727\n5.812\nFinetune-free\nIP-adapter\n0.272\n0.201\n0.442\n0.274\n0.608\n0.809\n6.432\nmethods\nELITE\n0.253\n0.194\n0.483\n0.298\n0.605\n0.775\n6.283\nOurs(full)\n0.302\n0.182\n0.556\n0.308\n0.612\n0.821\n6.563\nFigure 5. Visualization of attention maps At2p.\nTable 3. Ablation results on Multi-subject Bench. Removing\neach component would lead to a performance drop on different\naspects.\nAblation Setups\nCLIP-T \u2191\nCLIP-ES \u2193\nDINO-M \u2191\nText2Image\n0.352\n\u2013\n0.318\nOurs(w/o multi-scale)\n0.257\n0.185\n0.510\nOurs(w/o reg loss)\n0.235\n0.199\n0.552\nOurs(full)\n0.302\n0.182\n0.556\ntrained a model without the embedding consistency regular-\nization loss (Lreg) to assess its criticality. We also substi-\ntuted our multi-scale visual embedding with a conventional\nlast-layer visual embedding. The results of these experi-\nments are depicted in Fig. 6.\nOur observations reveal that without subject condition-\ning, the generated subjects failed to correspond with the\nreference image. Omitting the multi-scale image feature re-\nsulted in a loss of detailed information, as evidenced by a\nsignificant drop in the DINO-M score. Discarding the em-\nbedding consistency regularization loss led to challenges in\ngenerating specific subjects from coexisting subjects, ad-\nversely affecting the CLIP-ES score. In contrast, the full\nimplementation of our method demonstrated enhanced ex-\npressiveness and precision.\nQuantitative comparisons, as shown in Table 3, also in-\ndicate that our complete method achieves the best results\nacross subject exclusivity and subject alignment. It slightly\ntrails the original Stable Diffusion (SD) model only in text-\nimage alignment. Substituting the multi-scale visual em-\nbedding significantly impacts image consistency, while ex-\ncluding the embedding consistency regularization loss ham-\npers text-image consistency.\nFigure 6. Qualitative ablation. We ablate our approach by using\ndifferent model settings. Without the Lreg, the model struggles\nto exclude undesired subjects from reference images. Substituting\nthe multi-scale image feature results in less detailed outputs.\n5. Conclusion\nIn this paper, we introduced the SSR-Encoder, a ground-\nbreaking finetuning-free approach for selective subject-\ndriven image generation.\nThis method marks a signifi-\ncant advancement in the field, offering capabilities previ-\nously unattainable in selective subject representation. At its\ncore, the SSR-Encoder consists of two pivotal the token-\nto-patch aligner and the detail-preserving subject encoder.\nThe token-to-patch aligner effectively aligns query input\ntokens with corresponding patches in the reference image,\nwhile the subject encoder is adept at extracting multi-scale\nsubject embeddings, capturing fine details across different\nscales.\nAdditionally, the incorporation of a newly pro-\nposed embedding consistency regularization loss further en-\nhances the overall performance of the system.\nOur ex-\ntensive experiments validate the SSR-Encoder\u2019s robustness\nand versatility across a diverse array of scenarios. The re-\nsults clearly demonstrate the encoder\u2019s efficacy in generat-\ning high-quality, subject-specific images, underscoring its\npotential as a valuable tool in the open-source ecosystem.\nReferences\n[1] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski.\nBreak-a-scene:\nExtracting\nmultiple concepts from a single image.\narXiv preprint\narXiv:2305.16311, 2023. 2, 3, 7\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al.\nediffi:\nText-to-\nimage diffusion models with an ensemble of expert denois-\ners. arXiv preprint arXiv:2211.01324, 2022. 2\n[3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 3\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u2019egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. 2021\nieee. In CVF International Conference on Computer Vision\n(ICCV), 2021. 5\n[5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models, 2023.\n3\n[6] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,\nYuwei Zhou, and Wenwu Zhu.\nDisenbooth:\nIdentity-\npreserving disentangled tuning for subject-driven text-to-\nimage generation. arXiv preprint arXiv:2305.03374, 2023.\n3\n[7] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao. Anydoor: Zero-shot object-level im-\nage customization. arXiv preprint arXiv:2307.09481, 2023.\n2\n[8] Timoth\u00b4ee Darcet, Maxime Oquab, Julien Mairal, and Pi-\notr Bojanowski. Vision transformers need registers. arXiv\npreprint arXiv:2309.16588, 2023. 3\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 248\u2013255, 2009. 3\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 2\n[11] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. arXiv preprint arXiv:2306.00986,\n2023. 3\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion, 2022. 2, 3, 7\n[13] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Designing an encoder\nfor fast personalization of text-to-image models.\narXiv\npreprint arXiv:2302.12228, 2023. 3\n[14] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano,\nGal Chechik, and Daniel Cohen-Or.\nEncoder-based do-\nmain tuning for fast personalization of text-to-image models,\n2023. 2, 3\n[15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 5\n[16] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang.\nSvdiff: Compact pa-\nrameter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305, 2023. 3\n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning, 2022. 5, 3\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models.\nIn International Conference on Learning Representations,\n2022. 3\n[21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. If you use this software, please cite it as below.\n3\n[22] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 2, 3\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 3\n[24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2, 3, 4\n[25] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\nfan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\nThe open images dataset v4:\nUnified image classifica-\ntion, object detection, and visual relationship detection at\nscale.\nInternational Journal of Computer Vision, 128(7):\n1956\u20131981, 2020. 4\n[26] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 3, 7, 8, 4\n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 5\n[28] Peipei Li, Rui Wang, Huaibo Huang, Ran He, and Zhaofeng\nHe. Pluralistic aging diffusion autoencoder. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pages 22613\u201322623, 2023. 4\n[29] Yi Li, Hualiang Wang, Yiqun Duan, and Xiaomeng Li. Clip\nsurgery for better explainability with enhancement in open-\nvocabulary tasks. arXiv preprint arXiv:2304.05653, 2023.\n3\n[30] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.\nSubject-diffusion: Open domain personalized text-to-image\ngeneration without test-time fine-tuning.\narXiv preprint\narXiv:2307.11410, 2023. 3, 4\n[31] Mikubill. sd-webui-controlnet, 2023. GitHub repository. 7,\n3\n[32] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 3\n[33] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 3\n[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[35] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 3\n[36] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021. 5, 2\n[38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation, 2021. 2\n[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents, 2022. 2\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 2, 3\n[41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. 2022. 2, 3, 5, 7, 4\n[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding, 2022. 2\n[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[44] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. Laion-5b: An open large-scale dataset for training\nnext generation image-text models, 2022. 5, 2, 3\n[45] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 2,\n3\n[46] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2\n[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2\n[48] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv\nLeviathan. Face0: Instantaneously conditioning a text-to-\nimage model on a face. arXiv preprint arXiv:2306.06638,\n2023. 3\n[49] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir\nAberman.\np+: Extended textual conditioning in text-to-\nimage generation. arXiv preprint arXiv:2303.09522, 2023.\n3\n[50] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2, 3, 4,\n7\n[51] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuo-\nfan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image\ngeneration via large mixture of diffusion paths.\narXiv\npreprint arXiv:2305.18295, 2023. 2\n[52] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xue-\njin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint\nby example: Exemplar-based image editing with diffusion\nmodels.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 18381\u2013\n18391, 2023. 3\n[53] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 2, 3, 4, 7\n[54] Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng\nGao, Jiaming Liu, Huaxia Li, Xu Tang, Jianzhuang Liu,\nand Baochang Zhang. Ipdreamer: Appearance-controllable\n3d object generation with image prompts.\narXiv preprint\narXiv:2310.05375, 2023. 2\n[55] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 3, 4, 2, 5\n[56] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 3\n[57] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-\nworld image variation by aligning diffusion inversion chain.\narXiv preprint arXiv:2305.18729, 2023. 3\n[58] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin\nBao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong.\nUni-controlnet: All-in-one control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2305.16322, 2023. 3\n[59] Chong Zhou, Chen Change Loy, and Bo Dai.\nExtract\nfree dense labels from clip.\nIn European Conference on\nComputer Vision, pages 696\u2013712. Springer, 2022. 3\nSSR-Encoder: Encoding Selective Subject Representation for Subject-Driven\nGeneration\nSupplementary Material\nIn this supplementary material, we first introduce the\npreliminaries of Diffusion and CLIP in Section F. Follow-\ning that, we provide an in-depth discussion on our Detail-\nPreserving Image Encoder in Section G. In subsequent sec-\ntions, we introduce the methods we compared against and\nthe user study we conducted, specifically in Section H and\nSection I respectively. We also present our results on human\nimage generation in Section J. Additional results from our\nwork on Dreambench and Multi-subject bench are show-\ncased in Section L. We then provide further details about\nour training data and Multi-subject bench in Section M. In\nSection N and Section O, we present the outcomes gener-\nated by combining our SSR-encoder with ControlNet [55]\nand animatediff [15], which not only demonstrates the gen-\neralization of our SSR encoder but also illustrates its seam-\nless applicability in the realm of controllable generation and\nvideo generation for maintaining character consistency with\nreference images. Lastly, we analyze the broader impact\nbrought by our method and the limitation of our method in\nSection P and Section Q.\nF. Preliminaries\nF.1. Preliminary for Diffusion Models\nDiffusion Model (DM) [19, 47] belongs to the category\nof generative models that denoise from a Gaussian prior xT\nto target data distribution x0 by means of an iterative de-\nnoising procedure. The common loss used in DM is:\nLsimple(\u03b8) := Ex0,t,\u03f5\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (xt, t)\u22252\n2\ni\n,\n(8)\nwhere xt is an noisy image constructed by adding noise\n\u03f5 \u2208 N(0, 1) to the natural image x0 and the network \u03f5\u03b8(\u00b7)\nis trained to predict the added noise. At inference time, data\nsamples can be generated from Gaussian noise \u03f5 \u2208 N(0, 1)\nusing the predicted noise \u03f5\u03b8(xt, t) at each timestep t with\nsamplers like DDPM [19] or DDIM [46].\nLatent Diffusion Model (LDM) [40] is proposed to\nmodel image representations in autoencoder\u2019s latent space.\nLDM significantly speeds up the sampling process and\nfacilitates text-to-image generation by incorporating addi-\ntional text conditions. The LDM loss is:\nLLDM(\u03b8) := Ex0,t,\u03f5\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (xt, t, \u03c4\u03b8(c))\u22252\n2\ni\n,\n(9)\nwhere x0 represents image latents and \u03c4\u03b8(\u00b7) refers to the\nBERT text encoder [10] used to encodes text description\nct.\nStable Diffusion (SD) is a widely adopted text-to-image\ndiffusion model based on LDM. Compared to LDM, SD is\ntrained on a large LAION [44] dataset and replaces BERT\nwith the pre-trained CLIP [37] text encoder.\nF.2. Preliminary for CLIP\nCLIP [37] consists of two integral components: an im-\nage encoder represented as F(x), and a text encoder, rep-\nresented as G(t). The image encoder, F(x), transforms an\nimage x with dimensions R3\u00d7H\u00d7W (height H and width\nW) into a d-dimensional image feature fx with dimensions\nRN\u00d7d, where N is the number of divided patches. On the\nother hand, the text encoder, G(t), creates a d-dimensional\ntext representation gt with dimensions RM\u00d7d from natural\nlanguage text t, where M is the number of text prompts.\nBoth encoders are concurrently trained using a contrastive\nloss function that enhances the cosine similarity of matched\npairs while reducing that of unmatched pairs. After training,\nCLIP can be applied directly for zero-shot image recogni-\ntion without the need for fine-tuning the entire model.\nG. Designing Choice of Image Encoder\nIn this section, we conduct a preliminary reconstruction\nexperiment to demonstrate that vanilla image features fail\nto capture fine-grained representations of the target subject\nand verify the effectiveness of our method.\nWe first in-\ntroduce our experimental setup and evaluation metrics in\nSec. G.1. Subsequently, we explain the implementation de-\ntails of each setting in Sec. G.2. Finally, we conduct quali-\ntative and quantitative experiments in Sec. G.3 to prove the\nsuperiority of our proposed methods compared to previous\nworks.\nG.1. Experimental Setup\nIn our image reconstruction experiment, we investigate\nfour types of image features. The details are as shown in\nFig. 7:\n\u2022 Setting A: CLIP Image Features. In this setting, we em-\nploy the vanilla CLIP image encoder to encode the input\nimage and utilize the features from the final layer as the\nprimary representation for subsequent reconstruction.\n\u2022 Setting B: DINOv2 Image Features. Analogous to set-\nting A, we replace the CLIP image encoder with the DI-\nNOv2 encoder to extract the features.\n\u2022 Setting C: Fine-tuned CLIP Image Features. With the\ngoal of recovering more fine-grained details while pre-\nserving text-image alignment, we fine-tune the last layer\nFigure 7. Details for each setting.\nparameters of the CLIP image encoder using a CLIP reg-\nularization loss.\n\u2022 Setting D: Multi-scale CLIP Image Features. Instead\nof fine-tuning, we resort to using features from different\nscales of the CLIP backbone as the image representations.\nTo verify the effectiveness of our methods, we employ\nthe following metrics: Perceptual Similarity (PS) [56] and\nPeak Signal-to-Noise Ratio (PSNR) to assess the quality\nof reconstruction, CLIP-T [18] and Zero-Shot ImageNet\nAccuracy (ZS) [9] to access the preservation of text-image\nalignment in image encoder variants.\nAs for data used in our preliminary experiments, we uti-\nlize a subset of LAION-5B [44]. This dataset comprises\napproximately 150,000 text-image pairs for training and a\nfurther 10,000 text-image pairs designated for testing.\nG.2. Implementation Details\nWe use OpenCLIP ViT-L/14 [21] and DINOv2 ViT-\nL/14 [35] as the image encoders and all images are resized\nto 224\u00d7224 for training. The model underwent 100,000\ntraining iterations on 4 V100 GPUs, using a batch size of 32\nper GPU. We adopt the Adam optimizer [23] with a learning\nrate of 3e-4 and implement the one-cycle learning sched-\nuler. To better preserve the pre-trained weights, we set the\nlearning rate of the image encoder as 1/10 of the other pa-\nrameters if fine-tuning is required. We adopt the same archi-\ntecture of the VAE decoder in LDM [40] with an extra up-\nsampling block and employ nearest interpolation to obtain\nthe final reconstruction results. We adopt L2 reconstruction\nloss in all our settings and additionally employ Lclip when\nfine-tuning the CLIP encoder.\nG.3. Experiment Results\nQualitative results. To demonstrate the effectiveness of\nour method, we present reconstruction results in Fig. 8. It\nis observed that vanilla CLIP image features and DINOv2\nfeatures only result in rather blurry outcomes. By contrast,\nboth fine-tuned CLIP image features and multi-scale CLIP\nimage features manage to retain more details. Specifically,\nmulti-scale CLIP image features is able to generate sharp\nedges without obvious degradations. Consequently, we in-\nfer that multi-scale features are more competent at preserv-\ning the fine-grained details we require.\nFigure 8. Comparisons of different settings.\nQuantitative results. The quantitative results are shown\nin Table 4. In terms of reconstruction quality, it\u2019s notewor-\nthy that both the fine-tuned CLIP image features and multi-\nscale CLIP image features are adept at producing superior\noutcomes, exhibiting lower perceptual similarity scores and\nhigher PSNR. This indicates that these features are more\nrepresentative than either vanilla CLIP image features or\nDINOv2 features.\nHowever, despite the assistance from\nCLIP regularization loss, fine-tuned CLIP image features\nstill suffer significant degradation in text-image alignment,\nwhich fails to meet our requirements. Consequently, we opt\nfor multi-scale features as our primary method for extract-\ning subject representation.\nTable 4. Comparisons of different settings.\nSettings\nPS \u2193\nPSNR \u2191\nCLIP-T \u2191\nZS \u2191\nA\n0.0036\n28.63\n0.1816\n75.3%\nB\n0.0013\n28.56\n\u2013\n\u2013\nC\n0.0004\n29.73\n0.1394\n68.4%\nD\n0.0006\n29.49\n0.1816\n75.3%\nH. Details of Comparison Experiments\nH.1. Details of Compared Methods\n1. Finetune-based Methods:\n\u2022 Textual Inversion [12]: A method to generate specific\nsubjects by describing them using new \u201cwords\u201d in the\nembedding space of pre-trained text-to-image models.\n\u2022 Dreambooth [41]: A method of personalized image\ngeneration by fine-tuning the parameters in diffusion\nU-Net structure.\n\u2022 Break-A-Scene [1]: Aims to extract a distinct text to-\nken for each subject in a single image, enabling fine-\ngrained control over the generated scenes.\n2. Finetune-free Methods:\n\u2022 Reference only [31]: Guide the diffusion directly us-\ning images as references without training through sim-\nple feature injection.\n\u2022 ELITE [50]: An encoder-based approach encodes the\nvisual concept into the textual embeddings for subject-\ndriven image generation.\nFigure 9. User study comparisons of different methods.\n\u2022 IP-adapter [53]: Focuses on injecting image informa-\ntion without fine-tuning the base model.\n\u2022 BLIPDiffusion [26]:\nCombines BLIP\u2019s language-\nimage pretraining with diffusion models.\nThese methods were chosen for their relevance and ad-\nvancements in the field, providing a robust frame of refer-\nence for evaluating the performance and innovations of our\nSSR-Encoder.\nH.2. Details of Implementation\nIn order to achieve a fair comparison, all the methods\nare implemented using the official open-source code based\non SD v1-5 and the official recommended parameters. For\nthe Multi-subject bench, all the methods use a single image\nas input and utilize different subjects to guide the genera-\ntion. We provide 6 different text prompts for each subject\non each image and generate 6 images for each text prompt.\nFor Dreambench, we follow [26, 41] and generate 6 images\nfor each text prompt provided by DreamBench.\nI. User Study\nWe conducted a user study to compare our method with\nDB, TI, Break-A-Scene, ELITE, and IP-adapter perceptu-\nally. For each evaluation, each user will see one input image\nwith multiple concepts, two different prompts for different\nconcepts, and 5 images generated by each prompt and each\nmethod. 60 evaluators were asked to rank each generated\nimage from 1 (worst) to 5 (best) concerning its selectiv-\nity, text-image alignment, subject alignment, and genera-\ntive quality. The results are shown in Table. 9 indicate that\nour method outperforms the comparison methods in gen-\nerative quality and better balances subject consistency and\ntext-image alignment.\nJ. Human Image Generation\nDespite the SSR-Encoder not being trained in domain-\nspecific settings (such as human faces), it is already capa-\nble of capturing the intricate details of the subjects. For\ninstance, similar to the method outlined in [30], we utilize\nface images from the OpenImages dataset [25] as reference\nimages for generating human images. Fig. 11 showcases\nsamples of the face images we generated. To better illus-\ntrate our results, we also employ images of two celebrities\nas references.\nK. Ablations of \u03c4 and \u03bb\nAs shown in Fig. 10 (a), under the same training settings,\nwhen \u03c4 was 0.01, the model managed to balance both iden-\ntity consistency and selectivity. The effects of different \u03bb\nvalues on the images under ablation and fixed seed condi-\ntions are shown in Fig. 10 (b). The smaller \u03bb, the weaker\nthe influence of the reference image.\nL. Examples of Evaluation Samples\nIn this section, we present more evaluation samples in\nour method on two different test datasets: Multi-Subject\nbench and DreamBench bench in Fig. 12, Fig. 13, and\nFig. 14.\nMoreover, we present more qualitative comparison re-\nsults in Fig. 15. As illustrated in the figure, our approach\nis more adept at focusing on the representation of distinct\nsubjects within a single image, utilizing a query to select\nthe necessary representation. In contrast to other methods,\nour method does not result in ambiguous subject extraction,\na common issue in finetune-based methods. For instance, in\nthe Dreambooth row from Fig. 15, two subjects frequently\nFigure 10. Visual ablation results of \u03c4 and \u03bb.\nappear concurrently, indicating a low level of selectivity.\nWhen considering selectivity, generative quality, and text-\nimage alignment, our SSR-Encoder surpasses all methods\nand achieves the level of finetune-based methods in terms\nof subject alignment.\nM. Details of Our Training Data and the Multi-\nsubject Bench\n\u2022 Details of training data. Our model utilizes the Laion 5B\ndataset[44], selecting images with aesthetic scores above\n6.0. The text prompts are re-captioned using BLIP2 [27].\nThe dataset comprises 10 million high-quality image-text\npairs, with 5,000 images reserved for testing and the re-\nmainder for training. Clearly, the distribution of training\ndata has a significant impact on our model. The more a\nparticular type of subject data appears in the training data\ncapt, the better our performance on that type of subject.\nTherefore, we further analyze the word frequency in the\ntraining data caption and report the most frequent subject\ndescriptors in the table5.\n\u2022 Details of multi-subject bench.\nThe Multi-subject\nBench comprises 100 images from our test data. More\nspecifically, the data is curated based on the caption as-\nsociated with each image from our test set. An image\nprogresses to the next stage if its caption contains at least\ntwo subject descriptors. Subsequently, we verify the con-\nTable 5. The most frequent subject descriptors in our training data.\nSubject\nfrequency\nSubject\nfrequency\nsubject\nfrequency\nwoman\n1528518\nsuit\n256732\ndog\n164819\nman\n1256613\ntrees\n240771\nsnow\n163838\npeople\n536434\nhair\n229538\ngirl\n162311\ntable\n385643\nwooden\n216958\nhat\n157549\nmountain\n315765\nstreet\n212259\nflowers\n152308\nchairs\n291189\nhouse\n191785\nsky\n151332\ndress\n268058\nbuilding\n168670\ncat\n147851\ngruence between the caption and the image. If the image\naligns with the caption and adheres to human aesthetic\nstandards, it is shortlisted as a candidate image.\nUlti-\nmately, we meticulously selected 100 images from these\ncandidates to constitute the Multi-subject Bench.\nN. Compatibility with ControlNet\nOur SSR-Encoder can be efficiently integrated into con-\ntrollability modules. As demonstrated in Fig. 16, we present\nadditional results of amalgamating our SSR-Encoder with\nControlNet [55]. Our approach can seamlessly merge with\ncontrollability modules, thereby generating controllable im-\nages that preserve consistent character identities in align-\nment with reference images.\nO. Compatibility with AnimateDiff\nOur SSR-Encoder is not only versatile enough to adapt\nto various custom models and controllability modules, but it\ncan also be effectively applied to video generation, integrat-\ning seamlessly with video generation models. In Fig. 17,\nwe demonstrate the impact of combining our SSR-Encoder\nwith Animatediff [15]. Despite not being trained on video\ndata, our method can flawlessly combine with Animatediff\nto produce videos that maintain consistent character identi-\nties with reference images.\nP. Broader Impact\nOur method in subject-driven image generation holds\nsignificant potential for advancing the field of text-to-image\ngeneration, particularly in creating personalized images.\nThis technology can be applied across various domains such\nas personalized advertising, artistic creation, and game de-\nsign, and can enhance research at the intersection of com-\nputer vision and natural language processing.\nHowever,\nwhile the technology has numerous positive applications,\nit also raises ethical and legal considerations. For instance,\ngenerating personalized images using others\u2019 images with-\nout appropriate permission could infringe upon their privacy\nand intellectual property rights. Therefore, adherence to rel-\nevant ethical and legal guidelines is crucial. Furthermore,\nour model may generate biased or inappropriate content if\nmisused. We strongly advise against using our model in\nuser-facing applications without a thorough inspection of\nits output and recommend proper content moderation and\nregulation to prevent undesirable consequences.\nQ. Limitation\nDue to the uneven distribution of the filtered training\ndata, we found that the fidelity will be slightly worse for\nsome concepts that are uncommon in our training data. This\ncan be addressed by increasing the training data. We plan\nto address these limitations and extend our approach to 3D\ngeneration in our future work.\nFigure 11. Results for human image generation.\nFigure 12. Examples of evaluation samples on the multi-subject bench.\nFigure 13. Examples of evaluation samples on the multi-subject bench.\nFigure 14. Examples of evaluation samples on the dreambench.\nFigure 15. More results of the qualitative comparison.\nFigure 16. Results of combining our SSR-Encoder with controlnet.\nFigure 17. Results of combining our SSR-Encoder with Animatediff.\n"
  },
  {
    "title": "DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaption by Combining 3D GANs and Diffusion Priors",
    "link": "https://arxiv.org/pdf/2312.16837.pdf",
    "upvote": "5",
    "text": "DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaption\nby Combining 3D GANs and Diffusion Priors\nBiwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, Xuansong Xie\nAlibaba Group\n{biwen.lbw, jinmao.yk, mengyang.fmy, miaomiao.cmm}@alibaba-inc.com,\nxingtong.xxs@taobao.com\n3D Domain Adaption\nReal image\nPixar\nGreek statue\nJoker\nSampled image\nPixar\nGolden statue\nFox, Pixar style\nBlue eyes\nPink hair\nHulk\nBatman\nObama\nGirl in\nwhite\ndress\nReal image\nText-to-Avatar\nLocal Editing\nSteven\nJobs\nFigure 1. Some results of the proposed DiffusionGAN3D on different tasks.\nAbstract\nText-guided domain adaption and generation of 3D-\naware portraits find many applications in various fields.\nHowever, due to the lack of training data and the challenges\nin handling the high variety of geometry and appearance,\nthe existing methods for these tasks suffer from issues like\ninflexibility, instability, and low fidelity. In this paper, we\npropose a novel framework DiffusionGAN3D, which boosts\ntext-guided 3D domain adaption and generation by com-\nbining 3D GANs and diffusion priors. Specifically, we in-\ntegrate the pre-trained 3D generative models (e.g., EG3D)\nand text-to-image diffusion models. The former provides a\nstrong foundation for stable and high-quality avatar gen-\neration from text.\nAnd the diffusion models in turn of-\nfer powerful priors and guide the 3D generator finetun-\ning with informative direction to achieve flexible and effi-\ncient text-guided domain adaption. To enhance the diver-\nsity in domain adaption and the generation capability in\ntext-to-avatar, we introduce the relative distance loss and\ncase-specific learnable triplane respectively. Besides, we\ndesign a progressive texture refinement module to improve\nthe texture quality for both tasks above. Extensive exper-\niments demonstrate that the proposed framework achieves\nexcellent results in both domain adaption and text-to-avatar\ntasks, outperforming existing methods in terms of gener-\nation quality and efficiency. The project homepage is at\nhttps://younglbw.github.io/DiffusionGAN3D-homepage/.\n1. Introduction\n3D portrait generation and stylization find a vast range\nof applications in many scenarios, such as games, adver-\ntisements, and film production.\nWhile extensive works\n[4, 7, 9, 17] yield impressive results on realistic portrait gen-\neration, the performance on generating stylized, artistic, and\ntext-guided 3D avatars is still unsatisfying due to the lack\nof 3D training data and the difficulties in modeling highly\nvariable geometry and texture.\nSome works [2, 25, 26, 47, 51, 53, 56] perform transfer\nlearning on a pre-trained 3D GAN generator to achieve 3D\nstylization, which relies on a large number of stylized im-\nages and strictly aligned camera poses for training. [2, 47]\nleverage existing 2D-GAN trained on a specific domain to\narXiv:2312.16837v2  [cs.CV]  29 Dec 2023\nsynthesize training data and implement finetuning with ad-\nversarial loss. In contrast, [25, 26, 51] utilize text-to-image\ndiffusion models to generate training datasets in the target\ndomain. This enables more flexible style transferring but\nalso brings problems like pose bias, tedious data processing,\nand heavy computation costs. Unlike these adversarial fine-\ntuning based methods, StyleGAN-Fusion [48] adopts SDS\n[37] loss as guidance of text-guided adaption of 2D and 3D\ngenerators, which gives a simple yet effective way to ful-\nfill domain adaption. However, it also suffers from limited\ndiversity and suboptimal text-image correspondence.\nThe recently proposed Score Distillation Sampling\n(SDS) algorithm [37] exhibits impressive performance in\ntext-guided 3D generation.\nIntroducing diffusion priors\ninto the texture and geometry modeling notably reduces\nthe training cost and offers powerful 3D generation ability.\nHowever, it also leads to issues like unrealistic appearance\nand Janus (multi-face) problems. Following [37], massive\nworks [5, 21, 27, 30, 49, 50, 52] have been proposed to en-\nhance the generation quality and stability. Nevertheless, the\nrobustness and visual quality of the generated model are still\nfar less than the current generated 2D images.\nBased on the observations above, we propose a novel\ntwo-stage framework DiffusionGAN3D to boost the perfor-\nmance of 3D domain adaption and text-to-avatar tasks by\ncombining 3D generative models and diffusion priors, as\nshown in Fig. 2. For the text-guided 3D Domain Adaption\ntask, We first leverage diffusion models and adopt SDS loss\nto finetune a pre-trained EG3D-based model [4, 7, 9] with\nrandom noise input and camera views. The relative distance\nloss is introduced to deal with the loss of diversity caused\nby the SDS technique. Additionally, we design a diffusion-\nguided reconstruction loss to adapt the framework to local\nediting scenarios. Then, we extend the framework to Text-\nto-Avatar task by finetuning 3D GANs with a fixed latent\ncode that is obtained guided by CLIP [38] model. During\noptimization, a case-specific learnable triplane is introduced\nto strengthen the generation capability of the network. To\nsum up, in our framework, the diffusion models offer pow-\nerful text-image priors, which guide the domain adaption\nof the 3D generator with informative direction in a flexible\nand efficient way. In turn, 3D GANs provide a strong foun-\ndation for text-to-avatar, enabling stable and high-quality\navatar generation. Last but not least, taking advantage of\nthe powerful 2D synthesis capability of diffusion models,\nwe propose a Progressive Texture Refinement module as\nthe second stage for these two tasks above, which signifi-\ncantly enhances the texture quality. Extensive experiments\ndemonstrate that our method exhibits excellent performance\nin terms of generation quality and stability on 3D domain\nadaption and text-to-avtar tasks, as shown in Fig. 1.\nOur main contributions are as follows:\n(A) We achieve text-guided 3D domain adaption in high\nquality and diversity by combining 3D GANs and diffusion\npriors with the assistance of the relative distance loss.\n(B) We adapt the framework to a local editing scenario by\ndesigning a diffusion-guided reconstruction loss.\n(C) We achieve high-quality text-to-avatar in superior per-\nformance and stability by introducing the case-specific\nlearnable triplane.\n(D) We propose a novel progressive texture refinement\nstage, which fully exploits the image generation capabilities\nof the diffusion models and greatly enhances the quality of\ntexture generated above.\n2. Related Work\nDomain Adaption of 3D GANs. The advancements in 3D\ngenerative models [4, 6, 7, 9, 11, 13, 14, 29, 35, 45] have\nenabled geometry-aware and pose-controlled image gener-\nation.\nEspecially, EG3D [7] utilizes triplane as 3D rep-\nresentation and integrates StyleGAN2 [24] generator with\nneural rendering [33] to achieve high-quality 3D shapes\nand view-consistency image synthesis, which facilitates the\ndownstream applications such as 3D stylization, GAN in-\nversion [28].\nSeveral works [2, 22, 53, 56] achieve 3D\ndomain adaption by utilizing stylized 2D generator to syn-\nthesize training images or distilling knowledge from it. In\ncontrast, [25, 26, 51] leverage the powerful diffusion mod-\nels to generate training datasets in the target domain and\naccomplish text-guided 3D domain adaption with great per-\nformance. Though achieving impressive results, these ad-\nversarial learning based methods above suffer from issues\nsuch as pose bias, tedious data processing, and heavy com-\nputation cost. Recently, non-adversarial finetuining meth-\nods [3, 12, 48] also exhibit great promise in text-guided do-\nmain adaption. Especially, StyleGAN-Fusion [48] adopts\nSDS loss as guidance for the adaption of 2D generators and\n3D generators. It achieves efficient and flexible text-guided\ndomain adaption but also faces the problems of limited di-\nversity and suboptimal text-image correspondence.\nText-to-3D Generation. In recent years, text-guided 2D\nimage synthesis [10, 41\u201343, 55] achieve significant progress\nand provide a foundation for 3D generation. Prior works,\nincluding CLIP-forge [44], CLIP-Mesh [34], and Dream-\nFields [20], employ CLIP [38] as guidance to optimize 3D\nrepresentations such as meshes and NeRF [33]. DreamFu-\nsion [37] first proposes score distillation sampling (SDS)\nloss to utilize a pre-trained text-to-image diffusion model\nto guide the training of NeRF. It is a pioneering work and\nexhibits great promise in text-to-3d generation, but also\nsuffers from over-saturation, over-smoothing, and Janus\n(multi-face) problem.\nSubsequently, extensive improve-\nments [30, 39, 49, 50] over DreamFusion have been in-\ntroduced to address these issues.\nProlificDreamer [50]\nproposes variational score distillation (VSD) and produces\nhigh-fidelity texture results. Magic3D [30] adopts a coarse-\nDecoder\nTriplane\nGenerator\nTriplane\nGenerator\n\ud835\udc4a\n\ud835\udc73\ud835\udc85\ud835\udc8a\ud835\udc94\nT2I\nDiffusion\nPixar style\n\ud835\udc3f$%$\nAdd Noise\n\ud835\udc9a\n\ud835\udc84\nTriplane\nGenerator\nLatent\nSearching\nT2I\nDiffusion\n\ud835\udc3f$%$\n\ud835\udc9a\nDecoder\nAdd Noise\n\ud835\udc84\n\ud835\udc9a\nLink in Zelda\nTask2: Text-to-Avatar\nStage 1\nStage 2\n\u2026\n\u2026\nVolumn\nRendering\nMarching cube &\nUV unwrapping\nAdaptive\nblend\n\ud835\udc99\ud835\udfce\n\ud835\udc99\ud835\udfd0\ud835\udc8c$\ud835\udfcf\n\ud835\udc99\ud835\udfd0\ud835\udc8c\n\ud835\udc99\ud835\udfd0\ud835\udc8c&\ud835\udfcf\n\ud835\udc7c\ud835\udfce\n\ud835\udc7c\ud835\udfcf\n\ud835\udc7c\ud835\udfd0\n\ud835\udc7c\ud835\udfd0\ud835\udc8c+\ud835\udfd0\n\u2026\nAdaptive\nblend\n\ud835\udc93\ud835\udc8a\n\"(\ud835\udc56 = 0, \u2026 , 2\ud835\udc58 + 1)\n\u2026\n\ud835\udc9a\n\ud835\udc93\ud835\udfce\n\ud835\udc93\ud835\udfcf\n\ud835\udc93\ud835\udfd0\ud835\udc8c+\ud835\udfcf\n\ud835\udc93\ud835\udfce\n,\n\ud835\udc93\ud835\udfcf\n,\n\ud835\udc93\ud835\udfd0\ud835\udc8c+\ud835\udfcf\n,\n\u2026\n\u2026\nImg2img + ControlNets\nInpainting + ControlNets\nInpainting + ControlNets\nrotate\nrotate\nProgressive Texture Refinement\nCLIP\n\ud835\udc98\n\ud835\udc99\n\ud835\udc9b\ud835\udc95\n\ud835\udc99\n\ud835\udc9b\ud835\udc95\n\ud835\udc7b\n\ud835\udc7b\u2032\n\ud835\udc9a\n\ud835\udc9a\n\ud835\udc7b\n\ud835\udc3f.$/0\n\ud835\udc8e\ud835\udfcf\n\ud835\udc8e\ud835\udfd0\ud835\udc8c+\ud835\udfcf\n\ud835\udc98,\n\ud835\udc9a\n\ud835\udc9a\nTask1: Domain Adaption\n\ud835\udc98,\n\ud835\udc3a/1234\n\ud835\udc3a516784\n\ud835\udc3a/1234\n\ud835\udc7b\ud835\udc8d\nA cute boy, Pixar style\n\ud835\udc9a\n\ud835\udc74\n\ud835\udc74\nFigure 2. Overview of the proposed two-stage framework DiffusionGAN3D.\nto-fine strategy and utilizes DMTET [46] as the 3D rep-\nresentation to implement texture refinement through SDS\nloss. Despite yielding impressive progress, the appearance\nof their results is still unsatisfying, existing issues such as\nnoise [50], lack of details [30, 49], multi-view inconsistency\n[8, 40].\nMoreover, these methods still face the problem\nof insufficient robustness and incorrect geometry. When it\ncomes to avatar generation, these shortcomings can be more\nobvious and unacceptable.\nText-to-Avatar Generation. To handle 3D avatar genera-\ntion from text, extensive approaches [5, 18, 19, 21, 27, 54]\nhave been proposed.\nAvatar-CLIP [18] sets the founda-\ntion by initializing human geometry with a shape VAE\nand employing CLIP to guide geometry and texture model-\ning. DreamAvatar [5] and AvatarCraft [21] fulfill robust 3D\navatar creation by integrating the human parametric model\nSMPL [31] with pre-trained text-to-image diffusion mod-\nels. DreamHuman [27] further introduces a camera zoom-\nin strategy to refine the local details of 6 important body\nregions. Recently, AvatarVerse [52] and a concurrent work\n[36] employ DensePose-conditioned ControlNet [55] for\nSDS guidance to realize more stable avatar creation and\npose control. Although these methods exhibit quite decent\nresults, weak SDS guidance still hampers their performance\nin multi-view consistency and texture fidelity.\n3. Methods\nIn this section, we present DiffusionGAN3D, which boosts\nthe performance of 3D domain adaption and text-to-avatar\nby combining and taking advantage of 3D GANs and diffu-\nsion priors. Fig. 2 illustrates the overview of our framework.\nAfter introducing some preliminaries (Sec. 3.1), we first\nelaborate our designs in diffusion-guided 3D domain adap-\ntion (Sec. 3.2) , where we propose a relative distance loss to\nresolve the problem of diversity loss caused by SDS. Then\nwe extend this architecture and introduce a case-specific\nlearnable triplane to fulfill 3D-GAN based text-to-avatar\n(Sec. 3.3). Finally, we design a novel progressive texture\nrefinement stage (Sec. 3.4) to improve the detail and au-\nthenticity of the texture generated above.\n3.1. Preliminaries\nEG3D [7] is a SOTA 3D generative model, which employ\ntriplane as 3D representation and integrate StyleGAN2 [24]\ngenerator with neural rendering [33] to achieve high quality\n3D shapes and pose-controlled image synthesis. It is com-\nposed of (1) a mapping network that projects the input noise\nto the latent space W, (2) a triplane generator that synthe-\nsizes the triplane with the latent code as input, and (3) a\ndecoder that includes a triplane decoder, volume rendering\nmodule and super-resolution module in sequence. Given a\ntriplane and camera poses as input, the decoder generates\nhigh-resolution images with view consistency.\nScore Distillation Sampling (SDS), proposed by Dream-\nFusion [7], utilizes a pre-trained diffusion model \u03f5\u03d5 as prior\nfor optimization of a 3D representation \u03b8. Given an image\nx = g(\u03b8) that is rendered from a differentiable model g, we\nadd random noise \u03f5 on x at noise level t to obtain a noisy\nimage zt. The SDS loss then optimizes \u03b8 by minimizing\nthe difference between the predicted noise \u03f5\u03d5(zt; y, t) and\nthe added noise \u03f5, which can be presented as:\n\u2207\u03b8LSDS(\u03d5, g\u03b8) = Et,\u03f5\n\u0014\nwt (\u03f5\u03d5(zt; y, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u0015\n, (1)\nwhere y indicates the text prompt and wt denotes a weight-\ning function that depends on the noise level t.\n3.2. Diffusion-Guided 3D Domain Adaption\nDue to the difficulties in obtaining high-quality pose-aware\ndata and model training, adversarial learning methods for\n3D domain adaption mostly suffer from the issues of te-\ndious data processing and mode collapse. To address that,\nwe leverage diffusion models and adopt the SDS loss to im-\nplement transfer learning on an EG3D-based 3D GAN to\nachieve efficient 3D domain adaption, as shown in Fig. 2.\nGiven a style code w generated from noise z \u223c N(0, 1)\nthrough the fixed mapping network, we can obtain the tri-\nplane T and the image x rendered in a view controlled by\nthe input camera parameters c using the triplane generator\nand decoder in sequence. Then SDS loss (Sec. 3.1) is ap-\nplied on x to finetune the network. Different from Dream-\nFusion which optimizes a NeRF network to implement sin-\ngle object generation, we shift the 3D generator with ran-\ndom noise and camera pose to achieve domain adaption\nguided by text y. During optimization, all parameters of\nthe framework are frozen except the triplane generator. We\nfind that the gradient provided by SDS loss is unstable and\ncan be harmful to some other well-trained modules such as\nthe super-resolution module. Besides, freezing the mapping\nnetwork ensures that the latent code w lies in the same do-\nmain during training, which is a crucial feature that can be\nutilized in the diversity preserving of the 3D generator.\nRelative Distance Loss.\nThe SDS loss provides diffu-\nsion priors and achieves text-guided domain adaption of 3D\nGAN in an efficient way. However, it also brings the prob-\nlem of diversity loss as illustrated in [48]. To deal with that,\n[48] proposes the directional regularizer to regularize the\ngenerator optimization process, which improves the diver-\nsity to a certain extent. However, it also limits the domain\nshifting, facing a trade-off between diversity and the degree\nof style transfer. To address this, we propose a relative dis-\ntance loss. As shown in Fig. 3, considering two style codes\nwi and wj which are mapping from two different noise zi\nand zj, we project them into the original triplane domain\n(T \u2032\ni , T \u2032\nj) and the finetuned one (Ti, Tj) using a frozen tri-\nplane generator Gfrozen and the finetuned triplane genera-\ntor Gtrain, respectively. Note that, since the mapping net-\nwork is frozen during training in our framework, Ti and T \u2032\ni\n(same for Tj and T \u2032\nj) share the same latent code and ought\nto be close in context. Thus, we model the relative distance\nOriginal triplane space\nNew triplane space\n\ud835\udc64!\n\ud835\udc64\"\n\ud835\udc47!\n\ud835\udc47\"\n\ud835\udc47\"\n#\n\ud835\udc47!\n#\n\ud835\udc3a$%&'()\n\ud835\udc3a*%+!)\ntransfer\nFigure 3. An illustration of the relative distance loss.\nof these two samples in triplane space and formulate the\nrelative distance loss Ldis as:\nLdis = abs(\n||T \u2032\ni \u2212 T \u2032\nj||2\n||Ti \u2212 Tj||2 \u2212 1).\n(2)\nIn this function, guided by the original network, the sam-\nples in the triplane space are forced to maintain distance\nfrom each other. This prevents the generator from collaps-\ning to a fixed output pattern. Note that it only regularizes\nthe relative distance between different samples while per-\nforming no limitation to the transfer of the triplane domain\nitself. Extensive experiments in Sec. 4 demonstrate that the\nproposed relative distance loss effectively improves the gen-\neration diversity without impairing the degree of stylization.\nDiffusion-guided Reconstruction Loss. Despite the com-\nbination of SDS loss and the proposed relative distance loss\nis adequate for most domain adaption tasks, it still fails to\nhandle the local editing scenarios. A naive solution is to per-\nform reconstruction loss between the rendered image and\nthe one from the frozen network. However, it will also in-\nhibit translation of the target region. Accordingly, we pro-\npose a diffusion-guided reconstruction loss especially for\nlocal editing, which aims to preserve non-target regions\nwhile performing 3D editing on the target region. We found\nthat the gradient of SDS loss has a certain correlation with\nthe target area, especially when the noise level t is large, as\nshown in Fig. 4. To this end, we design a diffusion-guided\nreconstruction loss Ldiff that can be presented as:\n\u03b3 = abs(wt(\u03f5\u03d5(zt; y, t) \u2212 \u03f5)),\n(3)\nLdiff = t||(x \u2212 x\u2032) \u2299\n\u0014\nJ \u2212 h(\n\u03b3\nmax(\u03b3))\n\u0015\n||2,\n(4)\nwhere \u03b3 is the absolute value of the gradient item in Eq. 1, h\nrepresents the averaging operation in the feature dimension,\nJ is the matrix of ones having the same spatial dimensions\nas the output of h, x\u2032 denotes the output image of the frozen\nnetwork under the same noise and camera parameters x, \u2299\nindicates the Hadamard product. The latter item of the \u2299\noperation can be regarded as an adaptive mask indicating\nthe non-target region. Compared with ordinary reconstruc-\ntion loss, the proposed diffusion-guided reconstruction loss\nSampled image\n\ud835\udc61 = 0.1\n\ud835\udc61 = 0.5\n\ud835\udc61 = 0.9\nAccumulation\nFigure 4. Visualizations of the gradient response of SDS loss at\ndifferent noise levels, given the text \u201da man with green hair\u201d.\nalleviates the transfer limitation of the target region. Al-\nthough the gradient of SDS loss in a single iteration contains\na lot of noise and is inadequate to serve as an accurate mask,\nit can also provide effective guidance for network learning\nwith the accumulation of iterations as shown in Fig. 4. The\nablation experiment in Sec. 4 also proves its effectiveness.\nTo sum up, we can form the loss functions for normal\ndomain adaption and local editing scenario as Ladaption =\nLsds+\u03bb1Ldis and Lediting = Lsds+\u03bb2Ldiff, respectively,\nwhere \u03bb1 and \u03bb2 are the weighting coefficients.\n3.3. 3D-GAN Based Text-to-Avatar\nDue to the lack of 3D priors , most text-to-3D methods can-\nnot perform stable generation, suffering from issues such\nas Janua (multi-face) problem. To this end, we extend the\nframework proposed above and utilize the pre-trained 3D\nGAN as a strong base generator to achieve robust text-\nguided 3D avatar generation. As shown in Fig. 2, we first\nimplement latent searching to obtain the latent code that is\ncontextually (gender, appearance, etc.) close to the text in-\nput. Specifically, we sample k noise z1, ..., zk and select\none single noise zi that best fits the text description accord-\ning to the CLIP loss between the corresponding images syn-\nthesized by the 3D GAN and the prompt. The CLIP loss is\nfurther used to finetune the mapping network individually\nto obtain the optimized latent code w\u2032 from zi. Then, w\u2032 is\nfixed during the following optimization process.\nCase-specific learnable triplane. One main challenge of\nthe text-to-avatar task is how to model the highly variable\ngeometry and texture. Introducing 3D GANs as the base\ngenerator provides strong priors and greatly improves sta-\nbility. However, it also loses the flexibility of the simple\nNeRF network, showing limited generation capability. Ac-\ncordingly, we introduce a case-specific learnable triplane Tl\nto enlarge the capacity of the network, as shown in Fig. 2.\nInitialized with the value of 0, Tl is directly added to T as\nthe input of subsequent modules. Thus, the trainable part of\nthe network now includes the triplane generator Gtrain and\nTl. The former achieves stable transformation, while the\nlatter provides a more flexible 3D representation. Due to\nthe high degree of freedom of Tl and the instability of SDS\nloss, optimizing Tl with SDS loss alone will bring a lot of\nnoise, resulting in unsmooth results. To this end, we adopt\nthe total variation loss [23] and expand it to a multi-scale\nmanner Lmstv to regularize Tl and facilitate more smooth-\ning results. In general, the loss function for text-to-avatar\nDifferentiable\nRendering\n\ud835\udc73\ud835\udc95\ud835\udc97\n\ud835\udc73\ud835\udc8e\ud835\udc94\ud835\udc86\n\u2026\nFigure 5. The details of the proposed adaptive blend module.\ntask can be presented as: Lavatar = Lsds + \u03bb3Lmstv.\nNote that, the proposed framework is only suitable for\nthe generation of specific categories depending on the pre-\ntrained 3D GAN, such as head (PanoHead [4]) and hu-\nman body (AG3D [9]). Nevertheless, extensive experiments\nshow that our framework can well adapt to avatar genera-\ntion with large domain gaps, benefiting from the strong 3D\ngenerator and the case-specific learnable triplane.\n3.4. Progressive Texture Refinement\nThe SDS exhibits great promise in geometry modeling but\nalso suffers from texture-related problems such as over-\nsaturation and over-smoothing. How can we leverage the\npowerful 2D generation ability of diffusion models to im-\nprove the 3D textures? In this section, we propose a pro-\ngressive texture refinement stage, which significantly en-\nhances the texture quality of the results above through ex-\nplicit texture modeling, as shown in Fig. 2.\nAdaptive Blend Module. Given the implicit fields obtained\nfrom the first stage, we first implement volume rendering\nunder uniformly selected 2k + 2 azimuths and j elevations\n(we set the following j to 1 for simplicity) to obtain multi-\nview images xi, ..., x2k+1. Then the canny maps and depth\nmaps of these images are extracted for the following image\ntranslation. Meanwhile, we perform marching cube [32]\nand the UV unwrapping [1] algorithm to obtain the explicit\nmesh M and corresponding UV coordinates (in head gen-\neration, we utilize cylinder unwrapping for better visualiza-\ntion). Furthermore, we design an adaptive blend module to\nproject the multi-view renderings back into a texture map\nthrough differentiable rendering. Specifically, as shown in\nFig. 5, the multi-view reconstruction loss Lmse and total\nvariation loss Ltv are adopted to optimize the texture map\nthat is initialized with zeros.\nCompared to directly im-\nplementing back-projection, the proposed adaptive blend-\ning module produces smoother and more natural textures\nin spliced areas of different images without compromising\ntexture quality. This optimized UV texture U0 serves as an\ninitialization for the following texture refinement stage.\nProgressive Refinement. Since we have already obtained\nthe explicit mesh and the multi-view renderings, a natu-\nral idea is to perform image-to-image on the multi-view\nrenderings using diffusion models to optimize the texture.\nHowever, it neglects that the diffusion model cannot guar-\nantee the consistency of image translation between differ-\nent views, which may result in discontinuous texture. To\nthis end, we introduce a progressive inpainting strategy to\naddress this issue. Firstly, we employ a pre-trained text-\nto-image diffusion model and ControlNets [55] to imple-\nment image-to-image translation guided by the prompt y\non the front-view image r0 that is rendered from M and\nU0. The canny and depth extracted above are introduced\nto ensure the alignment between r0 and the resulting image\nr\u2032\n0. Then we can obtain the partially refined texture map\nU1 by projecting r\u2032\n0 into U0. Next, we rotate the mesh\ncoupled with T1 (or change the camera view) and render\na new image r1, which is refined again with the diffusion\nmodel to get r\u2032\n1 and U2. Differently, instead of image-to-\nimage, we apply inpainting on r1 with mask m1 in this\ntranslation, which maintains the refined region and thus im-\nproves the texture consistency between the adjacent views.\nNote that the masks m1, ..., m2k+1 indicate the unrefined\nregions and are dilated to facilitate smoother results in in-\npainting. Through progressively performing rotation and\ninpainting, we manage to obtain consistent multi-view im-\nages r\u2032\n0, ..., r\u2032\n2k+1 that are refined by the diffusion model.\nFinally, we apply the adaptive blend module again on the\nrefined images to yield the final texture. By implementing\nrefinement on the explicit texture, the proposed stage sig-\nnificantly improves the texture quality in an efficient way.\n4. Experiments\n4.1. Implementation Details\nOur framework is built on an EG3D-based model in the first\nstage. Specifically, we implement 3D domain adaption on\nPanoHead, EG3D-FFHQ, and EG3D-AFHQ for head, face,\nand cat, respectively. For text-to-avatar tasks, PanoHead\nand AG3D are adopted as the base generators for head and\nbody generation. We employ StableDiffusion v2.1 as our\npre-trained text-to-image model. In the texture refinement\nstage, StableDiffusion v1.5 coupled with ControlNets are\nutilized to implement image-to-image and inpainting. More\ndetails about the parameters and training setting are speci-\nfied in supplementary materials.\n4.2. Qualitative Comparison\nFor 3D Domain adaption, we evaluate our model with\ntwo powerful baselines:\nStyleGAN-NADA* [12] and\nStyleGAN-Fusion [48] for text-guided domain adaption of\n3D GANs, where * indicates the extension of the method\nto 3D models.\nFor a fair comparison, we use the same\nprompts as guidance for all the methods. Besides, the visu-\nalization results of different methods are sampled from the\nsame random noise. As shown in Fig. 6, the naive exten-\nsion of StyleGAN-NADA* for EG3D exhibits poor results\nin terms of diversity and image quality. StyleGAN-Fusion\nachieves decent 3D domain adaption and exhibits a certain\ndiversity. However, the proposed regularizer of StyleGAN-\nFusion also hinders itself from large-gap domain transfer,\nresulting in a trade-off between the degree of stylization\nand diversity. As Fig. 6 shows that the generated faces of\nStyleGAN-Fusion lack diversity and details, and the hair\nand clothes suffer from inadequate stylization. In contrast,\nour method exhibits superior performance in diversity, im-\nage quality, and text-image correspondence.\nFor text-to-avatar task, We present qualitative compar-\nisons with several general text-to-3D methods (DreamFu-\nsion [37], ProlificDreamer [50], Magic-3D [30]) and avatar\ngeneration methods (DreamAvatar [5], DreamHuman [27],\nAvatarVerse [52]). The former three methods are imple-\nmented using the official code and the results of the rest\nmethods are obtained directly from their project pages. As\nshown in Fig. 7, DreamFusion shows inferior performance\nin avatar generation, suffering from over-saturation, Janus\n(multi-face) problem, and incorrect body parts. Prolific-\nDreamer and Magic-3D improve the texture fidelity to some\nextent but still face the problem of inaccurate and unsmooth\ngeometry. Taking advantage of the human priors obtained\nfrom the SMPL model or DensePose, these text-to-avatar\nmethods achieve stable and high-quality avatar generation.\nHowever, due to that the SDS loss requires a high CFG [16]\nvalue during optimization, the texture fidelity and authentic-\nity of their results are still unsatisfying. In comparison, the\nproposed method achieves stable and high-fidelity avatar\ngeneration simultaneously, making full use of the 3D GANs\nand diffusion priors. Please refer to the supplementary ma-\nterials for more comparisons.\n4.3. Quantitative Comparison\nWe quantitatively evaluate the above baselines and our\nmethod on 3D domain adaption through FID [15] compari-\nson and user study. Specifically, all methods are employed\nto conduct domain adaption on EG3D-face and EG3D-cat\nwith both four text prompts, respectively.\nFor each text\nprompt, we utilize the text-to-image diffusion model to\ngenerate 2000 images with different random seeds as the\nground truth for FID calculation. In the user study, 12 vol-\nunteers were invited to rate each finetuned model from 1 to\n5 based on three dimensions: text-image correspondence,\nimage quality, and diversity. As shown in Table 1, the pro-\nposed methods achieve lower FID scores than other base-\nlines, which indicates superior image fidelity. Meanwhile,\nthe user study demonstrates that our method outperforms\nthe other two methods, especially in terms of image quality\nand diversity.\nFor text-to-avatar, we also conducted a user study for\nquantitative comparison. Since AvatarVerse and DreamA-\nvatar have not released their code yet, while DreamHu-\nman provided extensive results on the project page.\nSo\nwe compare our method with DreamHuman for full-body\nPixar\nLego\n(b) StyleGAN-Fusion\n(a) StyleGAN-NADA*\n(c) Ours\nFigure 6. The qualitative comparisons on 3D domain adaption (applied on EG3D-FFHQ [7]).\n(d) DreamAvatar/\nDreamHuman\n(a) DreamFusion\n(b) ProlificDreamer\n(c) AvatarVerse\n(d) Ours\n(a) DreamFusion\n(b) ProlificDreamer\n(f) Ours\n(c) Magic-3D\n(e) AvatarVerse\n'' Yoda in Star \nWars Series ''\n'' A bearded \nman in a black \nleather jacket''\n'' Spiderman''\n'' A man \nwearing a \nwhite tanktop\nand shorts ''\nFigure 7. Visual comparisons on text-to-avatar task. The first two rows are the results of \u2019head\u2019 and the rest are the results of \u2019body\u2019.\ngeneration. Besides, DreamFusion, ProlificDreamer, and\nMagic3D are involved in the comparison of head (10\nprompts) and full-body (10 prompts) generation both. We\nrequest the 12 volunteers to vote for their favorite results\nbased on texture and geometry quality, where all the results\nare presented as rendered rotating videos. The final rates\npresented in Table 2 show that the proposed method per-\nforms favorably against the other approaches.\n4.4. Ablation Study\nOn progressive texture refinement. Since we utilize cylin-\nder unwrapping for head texture refinement, a naive idea\nis to conduct image-to-image on the UV texture directly\nTable 1. Quantitative comparison on 3D domain adaption task.\nMethods\nMetric\nUser Study\nFID \u2193\ntext-corr \u2191\nquality \u2191\ndiversity \u2191\nStyleGAN-NADA*\n136.2\n2.619\n2.257\n1.756\nStyleGAN-Fusion\n53.6\n3.465\n3.255\n2.978\nOurs\n28.5\n3.725\n3.758\n3.416\nTable 2. User preference on text-to-avatar generation.\nDreamFusion\nProlificDreamer\nMagic3D\nDreamHuman\nOurs\nhead\n1.1%\n11.7%\n6.7%\nN.A.\n80.5%\nbody\n0.6%\n8.3%\n5.6%\n18.9%\n66.6%\n(a) Stage1\n(b) Texture img2img\n(c) w/o Inpainting\n(e) w/o ABM\n(d) MSE\n(f) Ours\nFigure 8. Ablation study of the texture refinement.\n(a) Source Domain\n(b) w/o Relative distance loss\n(c) Ours\nFigure 9. Ablation study of the relative distance loss.\nto refine it. However, the result in Fig. 8 (b) shows that\nthis method tends to yield misaligned texture, let alone be\napplied to fragmented texture maps. We also attempt to\nreplace all the inpainting operations with image-to-image\ntranslation, and the results in Fig. 8 (c) show that this will\ncause the discontinuity problem. The refining strategy pro-\nposed in [49] is also compared, where texture is progres-\nsively optimized using MSE loss between the randomly\nrendered images and the corresponding image-to-image re-\nsults. The results in Fig. 8 (d) show that it fails to generate\nhigh-frequency details. The comparison between (e) and (f)\nin Fig. 8 proves the effectiveness of the proposed adaptive\nblend module (ABM) in smoothing the texture splicing re-\ngion. By contrast, the proposed progressive texture refine-\nment strategy significantly improves the texture quality.\nOn relative distance loss. As shown in Fig. 9, if adopt-\ning SDS loss alone for domain adaption, the generator will\ntend to collapse to a fixed output pattern, losing its origi-\nnal diversity. In contrast, the proposed relative distance loss\neffectively preserves the diversity of the generator without\nsacrificing the stylization degree.\nOn diffusion-guided reconstruction loss. The results in\nFig 10 show that the SDS loss tends to perform global trans-\nfer. Regular reconstruction loss helps maintain the whole\nstructure, but also stem the translation of the target area. By\ncontrast, the model trained with our diffusion-guided recon-\nstruction loss achieves proper editing.\nOn additional learnable triplane. To prove the necessity\nof the proposed case-specific learnable triplane, we finetune\nthe network with SDS loss without adding it, given a chal-\nlenging prompt: \u201dLink in Zelda\u201d. The results in the first row\nof Fig. 11 reveal that the network is optimized in the right\n(a) Source image\n(b) w/o Rec loss\n(c) MSE loss\n(d) Ours\nFigure 10.\nAblation study of the diffusion guided reconstruc-\ntion loss. The ToRGB module in EG3D is trained together with\nGtrain. The input text is \u201da close-up of a woman with green hair\u201d.\n(a) w/o Case-specific learnable triplane\n(b) w/o Multi-scale total variation loss\n(c) Ours\nFigure 11. Ablation study toward the case-specific learnable tri-\nplane and the multi-scale total variation loss.\ndirection but fails to reach the precise point. By contrast,\nthe network adding the learnable triplane exhibits accurate\ngeneration (second row in Fig. 11). Furthermore, the intro-\nduced multi-scale total variation loss Lmstv on the triplane\nfacilitates more smooth results.\n4.5. Applications and Limitations\nDue to the page limitation, we will introduce the application\nof DiffusionGAN3D on real images and specify the limita-\ntions of our methods in the supplementary materials.\n5. Conclusion\nIn this paper, we propose a novel two-stage framework Dif-\nfusionGAN3D, which boosts text-guided 3D domain adap-\ntion and avatar generation by combining the 3D GANs\nand diffusion priors.\nSpecifically, we integrate the pre-\ntrained 3D generative models (e.g., EG3D) with the text-to-\nimage diffusion models. The former, in our framework, set\na strong foundation for text-to-avatar, enabling stable and\nhigh-quality 3D avatar generation. In return, the latter pro-\nvides informative direction for 3D GANs to evolve, which\nfacilitates the text-guided domain adaption of 3D GANs\nin an efficient way. Moreover, we introduce a progressive\ntexture refinement stage, which significantly enhances the\ntexture quality of the generation results. Extensive exper-\niments demonstrate that the proposed framework achieves\nexcellent results in both domain adaption and text-to-avatar\ntasks, outperforming existing methods in terms of genera-\ntion quality and efficiency.\nReferences\n[1] Jonathan\nyoung.\nxatlas,\n2021.\nhttps : / /\ntriplegangers.com/. 5\n[2] Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai,\nAliaksandr Siarohin, Peter Wonka, and Sergey Tulyakov.\n3davatargan: Bridging domains for personalized editable\navatars.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 4552\u2013\n4562, 2023. 1, 2\n[3] Aibek Alanov, Vadim Titov, and Dmitry P Vetrov. Hyperdo-\nmainnet: Universal domain adaptation for generative adver-\nsarial networks. Advances in Neural Information Processing\nSystems, 35:29414\u201329426, 2022. 2\n[4] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y\nOgras, and Linjie Luo. Panohead: Geometry-aware 3d full-\nhead synthesis in 360deg. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 20950\u201320959, 2023. 1, 2, 5\n[5] Yukang Cao, YanPei Cao, Kai Han, Ying Shan, and Kwan-\nYee K. Wong. Dreamavatar: Text-and-shape guided 3d hu-\nman avatar generation via diffusion models. arXiv preprint\narXiv:2304.00916, 2023. 2, 3, 6\n[6] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. pi-gan: Periodic implicit generative\nadversarial networks for 3d-aware image synthesis. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5799\u20135809, 2021. 2\n[7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 1, 2, 3,\n7\n[8] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey\nTulyakov, and Matthias Nie\u00dfner.\nText2tex:\nText-driven\ntexture synthesis via diffusion models.\narXiv preprint\narXiv:2303.11396, 2023. 3\n[9] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Ot-\nmar Hilliges, and Andreas Geiger. Ag3d: Learning to gen-\nerate 3d avatars from 2d image collections. arXiv preprint\narXiv:2305.02312, 2023. 1, 2, 5\n[10] Aditya Ramesh et al.\nHierarchical text-conditional image\ngeneration with clip latents, 2022. 2\n[11] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape\ninduction from 2d views of multiple objects. In 2017 In-\nternational Conference on 3D Vision (3DV), pages 402\u2013411.\nIEEE, 2017. 2\n[12] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik,\nand Daniel Cohen-Or.\nStylegan-nada:\nClip-guided do-\nmain adaptation of image generators.\narXiv preprint\narXiv:2108.00946, 2021. 2, 6\n[13] Jiatao\nGu,\nLingjie\nLiu,\nPeng\nWang,\nand\nChristian\nTheobalt.\nStylenerf:\nA style-based 3d-aware genera-\ntor for high-resolution image synthesis.\narXiv preprint\narXiv:2110.08985, 2021. 2\n[14] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escap-\ning plato\u2019s cave: 3d shape from adversarial rendering. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 9984\u20139993, 2019. 2\n[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[16] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 6\n[17] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and\nZiwei Liu. Eva3d: Compositional 3d human generation from\n2d image collections.\narXiv preprint arXiv:2210.04888,\n2022. 1\n[18] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535, 2022. 3\n[19] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao\nQi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz:\nMake a scene with complex 3d animatable avatars. arXiv\npreprint arXiv:2305.12529, 2023. 3\n[20] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 2\n[21] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao.\nAvatar-\ncraft: Transforming text into neural human avatars with\nparameterized shape and pose control.\narXiv preprint\narXiv:2303.17606, 2023. 2, 3\n[22] Wonjoon Jin, Nuri Ryu, Geonung Kim, Seung-Hwan Baek,\nand Sunghyun Cho.\nDr.3d: Adapting 3d gans to artistic\ndrawings.\nIn SIGGRAPH Asia 2022 Conference Papers,\npages 1\u20138, 2022. 2\n[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nEuropean conference on computer vision, pages 694\u2013711.\nSpringer, 2016. 5\n[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 2, 3\n[25] Gwanghyun Kim and Se Young Chun. Datid-3d: Diversity-\npreserved domain adaptation using text-to-image diffusion\nfor 3d generative model. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 14203\u201314213, 2023. 1, 2\n[26] Gwanghyun Kim, Ji Ha Jang, and Se Young Chun. Podia-\n3d: Domain adaptation of 3d generative model across large\ndomain gap using pose-preserved text-to-image diffusion. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 22603\u201322612, 2023. 1, 2\n[27] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-\nuard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchis-\nescu. Dreamhuman: Animatable 3d avatars from text. arXiv\npreprint arXiv:2306.09329, 2023. 2, 3, 6\n[28] Yushi Lan, Xuyi Meng, Shuai Yang, Chen Change Loy, and\nBo Dai. Self-supervised geometry-aware encoder for style-\nbased 3d gan inversion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 20940\u201320949, 2023. 2\n[29] Yiyi Liao, Katja Schwarz, Lars Mescheder, and Andreas\nGeiger. Towards unsupervised learning of generative mod-\nels for 3d controllable image synthesis. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5871\u20135880, 2020. 2\n[30] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2, 3, 6\n[31] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\nperson linear model. In Seminal Graphics Papers: Pushing\nthe Boundaries, Volume 2, pages 851\u2013866. 2023. 3\n[32] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. In Sem-\ninal graphics: pioneering efforts that shaped the field, pages\n347\u2013353. 1998. 5\n[33] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 2,\n3\n[34] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 2\n[35] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStylesdf: High-resolution 3d-consistent image and geome-\ntry generation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13503\u2013\n13513, 2022. 2\n[36] Mohit Mendiratta Pan, Mohamed Elgharib, Kartik Teo-\ntia, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski,\nChristian Theobalt, et al.\nAvatarstudio: Text-driven edit-\ning of 3d dynamic human head avatars.\narXiv preprint\narXiv:2306.00547, 2023. 3\n[37] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2, 6\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2\n[39] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nbooth3d:\nSubject-driven text-to-3d generation.\narXiv\npreprint arXiv:2303.13508, 2023. 2\n[40] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. arXiv preprint arXiv:2302.01721, 2023. 3\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2\n[42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023.\n[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi,\nRaphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding. 2022. 2\n[44] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18603\u201318613,\n2022. 2\n[45] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance fields for 3d-aware im-\nage synthesis. Advances in Neural Information Processing\nSystems, 33:20154\u201320166, 2020. 2\n[46] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler.\nDeep marching tetrahedra: a hybrid repre-\nsentation for high-resolution 3d shape synthesis. Advances\nin Neural Information Processing Systems, 34:6087\u20136101,\n2021. 3\n[47] Guoxian Song, Hongyi Xu, Jing Liu, Tiancheng Zhi, Yichun\nShi, Jianfeng Zhang, Zihang Jiang, Jiashi Feng, Shen Sang,\nand Linjie Luo.\nAgilegan3d: Few-shot 3d portrait styl-\nization by augmented transfer learning.\narXiv preprint\narXiv:2303.14297, 2023. 1\n[48] Kunpeng Song,\nLigong Han,\nBingchen Liu,\nDimitris\nMetaxas, and Ahmed Elgammal.\nDiffusion guided do-\nmain adaptation of image generators.\narXiv preprint\narXiv:2212.04473, 2022. 2, 4, 6\n[49] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 2, 3, 8\n[50] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 3, 6\n[51] Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang Yu,\nBillzb Wang, Bin Fu, Tao Chen, Guosheng Lin, and Chun-\nhua Shen. Styleavatar3d: Leveraging image-text diffusion\nmodels for high-fidelity 3d avatar generation. arXiv preprint\narXiv:2305.19012, 2023. 1, 2\n[52] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu\nWang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min\nZheng. Avatarverse: High-quality & stable 3d avatar cre-\nation from text and pose. arXiv preprint arXiv:2308.03610,\n2023. 2, 3, 6\n[53] Junzhe Zhang, Yushi Lan, Shuai Yang, Fangzhou Hong,\nQuan Wang, Chai Kiat Yeo, Ziwei Liu, and Chen Change\nLoy. Deformtoon3d: Deformable neural radiance fields for\n3d toonification. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 9144\u20139154,\n2023. 1, 2\n[54] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang,\nCheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and\nJingyi Yu.\nDreamface:\nProgressive generation of ani-\nmatable 3d faces under text guidance.\narXiv preprint\narXiv:2304.03117, 2023. 3\n[55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2, 3, 6\n[56] Peng Zhou,\nLingxi Xie,\nBingbing Ni,\nand Qi Tian.\nCips-3d:\nA\n3d-aware\ngenerator\nof\ngans\nbased\non\nconditionally-independent pixel synthesis.\narXiv preprint\narXiv:2110.09788, 2021. 1, 2\n"
  },
  {
    "title": "Prompt Expansion for Adaptive Text-to-Image Generation",
    "link": "https://arxiv.org/pdf/2312.16720.pdf",
    "upvote": "4",
    "text": "PROMPT EXPANSION FOR\nADAPTIVE TEXT-TO-IMAGE GENERATION\nSiddhartha Datta2\u2217, Alexander Ku1, 3, Deepak Ramachandran1, and Peter Anderson1\n1Google Research\n2University of Oxford\n3Princeton University\nABSTRACT\nText-to-image generation models are powerful but difficult to use. Users craft\nspecific prompts to get better images, though the images can be repetitive. This\npaper proposes a Prompt Expansion framework that helps users generate high-\nquality, diverse images with less effort. The Prompt Expansion model takes a text\nquery as input and outputs a set of expanded text prompts that are optimized such\nthat when passed to a text-to-image model, generates a wider variety of appealing\nimages. We conduct a human evaluation study that shows that images generated\nthrough Prompt Expansion are more aesthetically pleasing and diverse than those\ngenerated by baseline methods. Overall, this paper presents a novel and effective\napproach to improving the text-to-image generation experience.\nStraight-Query Generation\nText \nEncoder\ntext features\nN noise samples\nImage \nDiffusion\nN images\njack o lantern designs\nA portrait of a black-clad Jack O' \nLantern with glowing eyes, \nwearing a mask, holding a lantern, \nand sitting at a campfire, \nsurrounded by the stars of the \nnight sky. psychedelic art.\nA giant jack o lantern built into a \ngiant red glowing pyramid on a hill \nin the shape of a face. The jack o' \nlantern is glowing with both an \ninner light and an outer light. \nkinetic pointillism.\nA rendering of a jack o lantern \ndesign from various cultures. The \ndesign combines traditional \nelements from many different \ncultures, lending the image an \neclectic, cosmopolitan feel..\nA series of photographs of lanterns \nin the form of animals, and some of \nthem are very anthropomorphic. The \noverall image is playful and inviting, \nand the relevant keywords are: \nchildlike, optimistic, energetic.\njack o lantern designs\nA portrait of a black-clad Jack O' \nLantern with glowing eyes, \nwearing a mask, holding a lantern, \nand sitting at a campfire, \nsurrounded by the stars of the \nnight sky. psychedelic art.\nA giant jack o lantern built into a \ngiant red glowing pyramid on a hill \nin the shape of a face. The jack o' \nlantern is glowing with both an \ninner light and an outer light. \nkinetic pointillism.\nA rendering of a jack o lantern \ndesign from various cultures. The \ndesign combines traditional \nelements from many different \ncultures, lending the image an \neclectic, cosmopolitan feel..\nA series of photographs of lanterns \nin the form of animals, and some of \nthem are very anthropomorphic. The \noverall image is playful and inviting, \nand the relevant keywords are: \nchildlike, optimistic, energetic.\njack o lantern \ndesigns\n(Query)\nPrompt Expansion\nText \nEncoder\njack o lantern designs\n(Query)\nN text features\nPrompt \nExpansion\nN prompts\nN noise samples\nN images\nImage \nDiffusion\njack o lantern designs\nA portrait of a black-clad Jack O' \nLantern with glowing eyes, \nwearing a mask, holding a lantern, \nand sitting at a campfire, \nsurrounded by the stars of the \nnight sky. psychedelic art.\nA giant jack o lantern built into a \ngiant red glowing pyramid on a hill \nin the shape of a face. The jack o' \nlantern is glowing with both an \ninner light and an outer light. \nkinetic pointillism.\nA rendering of a jack o lantern \ndesign from various cultures. The \ndesign combines traditional \nelements from many different \ncultures, lending the image an \neclectic, cosmopolitan feel..\nA series of photographs of lanterns \nin the form of animals, and some of \nthem are very anthropomorphic. The \noverall image is playful and inviting, \nand the relevant keywords are: \nchildlike, optimistic, energetic.\njack o lantern designs\nA portrait of a black-clad Jack O' \nLantern with glowing eyes, \nwearing a mask, holding a lantern, \nand sitting at a campfire, \nsurrounded by the stars of the \nnight sky. psychedelic art.\nA giant jack o lantern built into a \ngiant red glowing pyramid on a hill \nin the shape of a face. The jack o' \nlantern is glowing with both an \ninner light and an outer light. \nkinetic pointillism.\nA rendering of a jack o lantern \ndesign from various cultures. The \ndesign combines traditional \nelements from many different \ncultures, lending the image an \neclectic, cosmopolitan feel..\nA series of photographs of lanterns \nin the form of animals, and some of \nthem are very anthropomorphic. The \noverall image is playful and inviting, \nand the relevant keywords are: \nchildlike, optimistic, energetic.\nFigure 1: Prompt Expansion is an alternative paradigm for interaction with text-to-image models.\nTop: Sampled outputs from Straight-Query Generation may result in images that are less visually\ncompelling and diverse. Bottom: Prompt Expansion samples uncommitted aspects of the image in\ntext space, improving visual quality and diversity while enabling interaction modes beyond prompt\nengineering / iteration.\n\u2217Work done while at Google. Correspondence with author at siddhartha.datta@cs.ox.ac.uk.\n1\narXiv:2312.16720v1  [cs.CV]  27 Dec 2023\nsurreal imagination\nA surreal image of an island with \nan island, a windmill, and trees. \nThis image is dominated by strong \ncolors and bright sunlight.\na surreal image that combines \nboth nature and architecture, \ncreating an almost unearthly \nscenery.\nA person with a plate of fried food \nin their hand who appears to be \nhaving a hallucination.\nA surreal painting of a woman flying \nthrough the air. The woman is \nsoaring through the air surrounded \nby a rainbow of colors and light.\nStraight-Query Generation\nsurreal imagination\nPrompt Expansion\nFigure 2: Abstract queries: While Straight-Query Generation returns a set of similar images (Top), Prompt Expansion returns a diverse range of\nimages (Bottom), varying in topic, subject, location, colours, etc. This range is reflected in the expanded prompts for the first four images shown to the\nuser, and is continued to be reflected in the next 18 images generated.\n2\nfantasy landscape and natural scenery\nA painting of a forest at night, with \nlightning striking the trees. The \npainting is by the artist James \nNewton. sots art.\nA fantasy landscape in an open \ncountry, with lush forests and \nrushing rivers. The image is of a \nman standing on the edge of a cliff \nin front of a lake. kitsch movement.\nA fantasy landscape and natural \nscenery of a beautiful lake with a \nboat in it. The boat is sailing \nacross the water and you can hear \nthe splash of the water. german \nromanticism.\nA painting of a beautiful landscape \nthat a viewer would naturally want to \nexplore and enjoy. The painting is a \nwork of fantasy, but it is still a real \nworld painting. hyperrealism.\nFigure 3: Abstract-Concrete queries: For a query that is broad in topic but contains specific details that should not be left out, straight-query generation\nstill returns similar-looking images. Prompt Expansion returns images that remain entailed and faithful to the query, but adds variations in lighting,\nfine-grained details.\nforest trees\nA close up of a forest at night. The \nmoon is rising behind a forest, \nilluminating the small town in the \ndistance. modern european ink \npainting.\nA forest scene that contains a \nwaterfall, a bridge, and a single \ntree. The tree is at the top of a hill \nand the water falls into the \ndistance. german romanticism.\nA group of trees standing in a \nforest. The trees are in full foliage, \nthe sunlight shining brightly, and \nthe forest is at its most alive. \nfuturism.\nA forest of tall dark red trees in front \nof a light blue sky. The trees are all \nstanding around each other, forming \na ring around each other. digital art.\nFigure 4: Concrete queries: For a query that is very specific in nature, Straight-Query Generation (Top) returns images of the same forest setting with\ntrees from the same angle, same species, ground vegetation, etc. Prompt Expansion (Bottom) explores colours, styles of imagery, angles of view, time\nof day, scenery, etc.\n3\n1\nINTRODUCTION\nText-to-image generation models (Ramesh et al., 2022; Saharia et al., 2022a; Yu et al., 2022b) are\ncapable of rendering a stunning variety of high-quality images, from highly-realistic professional-\nlooking photos to fanciful dreamworlds in almost any visual style. However, interaction with these\nmodels frequently exposes two significant usability problems. First, the connection between text\ninputs and image outputs in these models still bears the fingerprints of the training data, which\nis typically images scraped from the web along with their alt-text. As a consequence, achieving\nhigh-quality outputs often requires users to include arcane lighting and camera jargon (\u201835mm\u2019,\n\u2018DSLR\u2019, \u2018backlit\u2019 \u2018hyper detailed\u2019), idiosyncratic descriptors (\u2018audacious and whimsical\u2019), and social\nmedia tags (\u2018trending on artstation\u2019). The prompts that produce the best images are not necessarily\nstable across different models, or even across different versions of the same model, leading to a focus\non prompt-sharing and \u2018prompt engineering\u2019, which is the process of iterating over prompts to craft\nthe optimal textual input for a desired image.\nSecond, we find that when sampling sets of images from these models, while the outputs are different,\nthey are not necessarily diverse. For example, randomly sampling four images for the prompt \u2018jack o\nlantern designs\u2019 from a diffusion model similar to Imagen (Saharia et al., 2022a) produces highly\nsimilar outputs (refer Fig 1 Top). Even though the prompt does not specify the composition, style,\nviewpoint or lighting of the image, whether it is night or day, or whether there are other features in\nthe scene, samples from the model do not vary in any of these aspects. This lack of diversity can\namplify harmful social biases when generating people (e.g. generating all doctors as male) (Naik\nand Nushi, 2023), and also fails to lead the user towards more interesting imagery. Just as people\nare non-committal about basic aspects of their mental images (Bigelow et al., 2023), we argue that\ntext-to-image models should also exhibit non-commitment by generating diverse outputs for any\nimage characteristics that are unspecified (Hutchinson et al., 2022).\nWe reduce the burden of prompt engineering and improve diversity in text-to-image generation by\nproposing a Prompt Expansion (PE) framework. A Prompt Expansion model takes a text prompt\nas input, which we refer to as a query, and outputs a set of N expanded text prompts that include\nspecialized keywords (to improve image quality) and interesting additional details (to add diversity to\nthe generated images). To do this, we construct a Prompt Expansion dataset by inverting a dataset of\nhigh aesthetic images to text, and few-shot prompting the inverted text to queries. We then train a\nPALM 2 (Anil et al., 2023) text-to-text model on the query:prompt pairs, and iteratively re-fine-tune\nbased on generated output. As illustrated in Fig 1 Bottom, incorporating Prompt Expansion into\ntext-to-image generation produces a greater variety of appealing, high-quality images and also opens\nup additional modes of user interaction beyond iterating over prompts (e.g., the user can simply\nselect the image they prefer, which can be used as the starting point for another iteration of Prompt\nExpansion). Empirically, we observe that Prompt Expansion achieves improved diversity, aesthetics,\nand text-image alignment compared to Straight-Query Generation in both automatic metrics and\nhuman rater evaluation.\nContributions. Motivated by the frustrations of prompt-sharing and prompt-engineering, we:\n1. Propose a new framework, Prompt Expansion, to improve image quality and diversity while\nopening up new modes of interaction;\n2. Construct a Prompt Expansion dataset by reverse engineering prompts and high-level queries\nfrom aesthetic images, and train a variety of Prompt Expansion models;\n3. Undertake a detailed human evaluation demonstrating that Prompt Expansion can improve\ndiversity, aesthetics, and text-image alignment.\n2\nRELATED WORK\nTo better control and introduce diversity into image output, most prior work focuses on techniques\nto allow end-users to edit the generated images, such as text-guided image editing (Hertz et al.,\n2022), or to fine-tune to create personalized images (Collell and Moens, 2016; Ruiz et al., 2022).\nThese methods focus on local edits and iterating with respect to a given image. Our goal is to\nintroduce diversity in the generated images set. Modifying the sampler is a common approach to\nachieving this. The standard approaches to introducing variation are increasing guidance (Ho and\n4\nSalimans, 2021) in diffusion models (e.g. Imagen (Saharia et al., 2022a)), or increasing temperature in\nautoregressive models (e.g. Parti (Yu et al., 2022b)). Changing the sampler/decoder hyperparameters,\nhowever, may not return meaningfully-diverse images. Setting the temperature/guidance too high\nwill insert excessive noise and yield images of low quality. The noise inserted is random, thus\nit may not return variations along targeted dimensions (e.g. style, lighting, attributes of people /\nobjects / places). To interpretably sample images, we shift our attention to sampling within text space.\nStraight-Query Generation\nPrompt Expansion\nPromptist\n(Query) drawing outline dancing in the dark\nFigure 5: We demonstrate the difference between\nprompt optimization techniques and prompt ex-\nploration techniques. While prompt optimization\nmethods such as Promptist can return the opti-\nmized prompt \"drawing outline dancing in the\ndark, trending on artstation\", it aims to return one\noptimized prompt, and thus 4 images generated\nfrom this optimized prompt may return less visual\ndiversity and explores less of the prompt space\nthan Prompt Expansion. We generate Promptist\u2019s\noptimized prompt with Stable Diffusion 1.4, which\nis the model it is aligned with during its training.\nPrompt Optimization techniques such as Promp-\ntist (Hao et al., 2022) or Promptify (Brade et al.,\n2023) investigate automatic and human-in-the-\nloop approaches to improving the aesthetic qual-\nity of a prompt\u2019s generated image. The task\nof Prompt Optimization presumes that an ideal\nprompt may exist for a given query that returns\nthe \"best\" image. Instead, the task of Prompt Ex-\npansion acknowledges that the ideal generated\nimage depends on more than the query alone\nand that the query may have different intents or\ncontext (e.g. varying by user preferences). It\naddresses the fact that there are uncommitted\naspects of the image (e.g. query ambiguity) and\nthat images are evaluated in sets. It also lays\na foundation for treating image generation as a\nmulti-turn process.As an example, an abstract\nquery \"drawing outline dancing in the dark\" in\nFigure 5 returns a similar set of black-and-white\ndancers. Prompt Optimization (e.g. Promptist)\nwould return a prompt that maximizes aesthet-\nics, but it would still return a similar set of black-and-white dancers. PE explores variations and\npossibilities in colors, angles, level of abstraction, gender, plurality, etc.\n3\nPROMPT EXPANSION DATASET\nThe Prompt Expansion (PE) framework requires a model to take a user text query as input and\nreturn N text prompts as output, such that the N text prompts through text-to-image generation will\nreturn a set of N diverse, aesthetic images aligned to the query. To train a PE model, we require a\ndataset mapping queries to prompts, which we construct in reverse. First, we collect aesthetically\nhigh-quality images, including both model-generated and natural images (refer Sec 3.1). Second, we\ninvert the images to a closely corresponding prompt that includes alt-text jargon (which we refer to as\nflavors, refer Sec 3.2). Finally, we map the inverted text to a range of high-level queries that more\nclosely correspond to user input (refer Sec 3.3). These queries are paired with the prompts from the\nsecond step to form the {query:prompt} dataset.\n3.1\nIMAGE-AESTHETIC DATASETS\nWe curate two image datasets. The first, Webli-Align, is composed of images from the Webli (Chen\net al., 2023) and Align (Jia et al., 2021) datasets, filtered to retain only images with high MUSIQ (Ke\net al., 2021) aesthetic scores. The second, CrowdSourced, is obtained by crowd-sourcing output\nfrom a text-to-image model. We provide an interface for text-to-image generation similar to Gradio,\nallowing users from a large organization to enter prompts that generate images. Users also have the\noption to upvote images that they like. We use this signal to retain only the most appealing images.\nWe retain 80k Webli-Align (natural) and 40k CrowdSourced (generated) images.\n3.2\nIMAGE-TO-TEXT INVERSION\nThe second step is to invert the images in the Image-Aesthetic datasets to prompt text. While the user\nquery is the input a user provides, the prompt is the text that generates a specific image. We use the\nInterrogator (CLIP-Interrogator) approach to image-to-text inversion. The computed prompt text is\ngenerated by concatenating: (i) a caption, and (ii) a set of \u2018flavors\u2019. The caption is a description of the\n5\nTable 1: Types of prefixes used for controllable generation. The models used in the paper make use\nof ABST, DTL (* indicates we replace the non-parenthesed prefix with the paranthesed prefix), MSTP.\nPrefix\nFull name\nDescription\nABST\nABSTract\nReturns output sentences conditioned on the input sentence being an abstract query (output\nsentence structure is in-line with the abstract augmentation/mixture in training).\nDTL\nDeTaiLed\nAdding this prefix returns output sentences conditioned on the input sentence being a\ndetailed query and requiring a detailed expansion (output sentence structure is in-line with\nthe detailed augmentation/mixture in training).\nGRD (DTL*)\nGRounDed\nAdding this prefix returns output sentences conditioned on the input sentence being a query\nrequiring a grounded expansion (output sentence structure is in-line with the grounded\naugmentation/mixture in training).\nSPCT (DTL*)\nSPeCificiTy\nAdding this prefix returns output sentences conditioned on the input sentence being a query\nrequiring an expansion that elicits specificity (output sentence structure is in-line with the\nspecificity augmentation/mixture in training).\nFLV\nFLaVor\nAdding this prefix returns only the flavor alone.\nHAST\nHighAeSThetics\nAdding this prefix returns an output sentence that should return images with good aesthetics\n(MUSIC > 6).\nRFT\nRe-Fine-Tuned\nAdding this prefix in front of the the input sentence will return images based on the re-fine-\ntuning objectives (e.g. based on aesthetics and renderable flavors).\nMSTP\nMultiSTeP\nAdding this prefix returns an output sentence specifically for multi-step Prompt Expansion.\nTable 2: Examples of prefixed {query:prompt} pairs, where prepending the prefix controls the\noutput generated.\nAugmentation Query\nPrompt\nAbstract\nABST hope\na tunnel with a light at the far end\nGrounded\nGRD colossal dragon\na colossal dragon eating humans\nGRD monster bike\na monster bike being ridden by a daredevil\nSpecificity\nSPCT animal drawings\nanimal drawings, for example a children\u2019s drawing\nof a dog\nSPCT groups of animals\ngroups of animals, specifically a herd of sheep\nbeing gathered by a farmer\nFlavor\nFLV a drawing of a dog swimming in a river\npointillism\nFLV a giant robot fighting a dinosaur\npixel art\nMulti-step\nMSTP a brain on a wall\na brain on a wall, poster art by Robert Beatty\nexpansion\nMSTP a brain on a wall, poster art by Robert\nBeatty\na brain on a wall, poster art by Robert Beatty,\nfeatured on behance\nMSTP a brain on a wall, poster art by Robert\nBeatty, featured on behance\na brain on a wall, poster art by Robert Beatty,\nfeatured on behance, psychedelic art, psychedelic\nartwork, brains, 8k archival print\nMSTP a brain on a wall, poster art by Robert\nBeatty, featured on behance, psychedelic art,\npsychedelic artwork, brains, 8k archival print\na picture of a brain on a wall, poster art by\nRobert Beatty, featured on behance, psychedelic\nart, psychedelic artwork, brains, 8k archival print\ncontent of the image (e.g. who, what, where, when). To generate the caption, we use COCA (Yu et al.,\n2022a) fine-tuned for the captioning task. A flavor refers to a descriptive word/phrase that alters the\nstyle of the image, without intending to add/change the content of the image, like \"impressionism\" or\n\"dslr\". We generate the lists of flavors from words and phrases used in a large number of collected\nprompts of generated images (details in Section J).\n3.3\nQUERY/PROMPT EXTRACTION\nThe final step in dataset preparation is to compute a range of potential user queries that are suitable to\nmap to the inverted text (prompt). We use few-shot prompting with FLAN-PaLMChilla 62B (Chung\net al., 2022) to generate successively shorter queries and longer prompts. The model receives few-shot\nprompting sets of long prompts mapped to short queries as examples. The few-shot prompts are in\nthe format {prompt : query}, and examples of these pairs can be seen in Figure 6 and Table 2 as\nwell. For each prompt from Image-to-Text inversion, the few-shot prompt examples are prepended\nbefore the prompt as context, and a corresponding query is generated by the text-to-text Model.\nWe extract a range of different queries that can be mapped to the expanded prompt, and use few-shot\nprompting to generate queries that are abstract, concrete, short-length, medium-length, or long-length.\nAppendix A has further details on how the query types are generated (e.g. grounded queries, eliciting\n6\nQuery \n(extracted \nvia few-shot \nprompting)\nsurreal hybrid \nanimals\nAesthetically-pleasing \nimages\nExpanded Prompt \n(via text \ninversion)\na cartoon of a walrus \nwith a pineapple on \nits head, vector art \nby Maurycy Gottlieb, \nreddit contest \nwinner, \nneo-primitivism, \nwalrus, surreal hybrid \nanimals, monster \nmanula\nc\nQuery (extracted \nvia few-shot \nprompting)\na walrus with a \npineapple on its \nhead, vector art by \nMaurycy Gottlieb, \nreddit contest \nwinner, \nneo-primitivism, \nwalrus, surreal \nhybrid animals, \nmonster manula\nQuery \n(extracted via \nfew-shot \nprompting)\na walrus with a \npineapple, vector \nart by Maurycy \nGottlieb, reddit \ncontest winner, \nneo-primitivism, \nwalrus, surreal \nhybrid animals, \nmonster manula\nQuery \n(extracted via \nfew-shot \nprompting)\na walrus with a \npineapple\nImage-Aesthetics \nDatasets\nImage-to-Text \nInversion\n{Image : Prompt}\nQuery/Prompt \nExtraction\n{Query : Prompt}\nPrompt Expansion \nDataset\nPrompt \nExpansion \nModel\nTrain\nRe-fine-tune\nText \nEncoder\nImage \nDiffusion\nFigure 6: (Outer) Overview of Prompt Expansion dataset construction and model training. Beginning\nwith the Image-Aesthetic datasets (Sec 3.1), we generate expanded prompts through Image-to-Text\nInversion (Sec 3.2), then we extract queries for each prompt through Query/Prompt Extraction (Sec\n3.3). The resulting Prompt Expansion dataset is used to train a base Prompt Expansion model. We\nthen align with our downstream text-to-image model to create the PE: Re-fine-tuned model (Sec\n4.2). (Inner) An example of the Image-Text inversion (using COCA-Interrogator) generating an\nexpanded prompt containing caption, art form, artist, medium, style, and other flavours / objects /\ndescriptors. Subsequently, the prompt is mapped by successive few-shot prompting inference calls\ninto shorter and more abstract queries that are each paired with the prompt to construct the dataset.\nspecificity). This results in a Prompt Expansion Dataset of 600k {query:prompt} pairs. We perform\na 70-20-10 train-val-test split, and split the train set 50-50 for base and re-fine-tuning.\n4\nPROMPT EXPANSION MODEL\nWe describe the two stages to train the Prompt Expansion model: (i) we train a base Prompt Expansion\nmodel on the Prompt Expansion Dataset; then (ii) we re-fine-tune the base model with respect to the\ndownstream text-to-image model.\n4.1\nBASE MODEL\nOur Prompt Expansion model is a text-to-text generation model trained to map query text to expanded\nprompt text with an architecture based on the PaLM 2 language model family (Anil et al., 2023).\nPaLM 2 is a decoder-only transformer-based architecture trained with the UL2 objective (Tay et al.,\n2023). We train a PaLM 2 1B parameter model with prompt-tuning (Lester et al., 2021), after\nevaluating different model configurations, as described in Table 7. We chose this relatively small size\nfor the base architecture, as it needs to serve as a front-end to a complex high-latency text-to-image\nmodel such as Imagen (Saharia et al., 2022a), and thus needs to be low-resource/latency to make the\nentire pipeline usable. For the base dataset, we use a 50% split of the Prompt Expansion Dataset\ndescribed in Section 3, consisting of 300k {query:prompt} examples.\n4.2\nRE-FINE-TUNING\nAfter training the base model, we observe that it may generate prompts that the text-to-image model\ncannot generate good images for. The main reason for this is that the expanded prompts generated by\nthe base Prompt Expansion model are based on the alignment between text and images favored by\nthe COCA image-to-text inversion model. Therefore, we propose a general re-fine-tuning procedure:\ngiven a target behavior for the model, re-fine-tuning filters for expanded prompts generated from\n7\nStraight-Query \nGeneration\nA blue \ncolored \ndog\nOne-step\nMulti-step\nA blue dog standing on a \nrug. The dog is staring at \nsomething off into the \ndistance. The dog\u2019s \nexpression is one of \nsadness or fear or \nhunger. gothic art.\nA blue and white dog sitting in a \npark, eating some grass. The \ndog is happy and relaxed, and \nit\u2019s wagging its tail. The light \ncoming in the window softens \nthe dog\u2019s expression, making it \nlook more playful. hyperrealism.\nA blue dog sits on a \nblanket n a sun-warmed, \ninviting living room. The \ndog is lying on its side, \nfacing the camera. The \ndog yawns and stretches, \nshowing off its blue \ntongue. pointillism.\nThe dog standing up on its hind \nlegs, its front paws resting on \nits hind legs. The dog has blue \neyes and brown hair and is \ncompletely relaxed. The overall \nmood of the image is peaceful \nand relaxed. \npost-impressionism. \nA blue dog standing on a rug. \nA blue dog lying on a rug in a \nliving room. The rug in the \nroom is covered with throw \npillows. The dog has his eyes \nclosed and his mouth is open. \ngothic art.\nA blue dog standing on a \nrug. A blue dog standing \non a rug in a room. The \nroom is bright and sunny, \nand the dog is at ease. The \nroom is filled with toys, \nand it is full of happy \nmemories. hyperrealism.\nA blue dog standing on a \nrug. The dog is wearing a \nsweater and hat, and it\u2019s \nfacing some sort of \ncreature. The dog is very \ncalm, and it\u2019s staring at \nthe creature. pointillism.\nA blue dog standing on a \nrug. A blue dog standing on \na rug in a dark alley. The \ndog is blue and has a white \ncollar. The dog is facing its \nhuman owner and the \nowner is walking up to \nthem. post-impressionism.\nA blue dog lying on a rug \nin the living room. The rug \nis very fluffy, and the do is \nvery cute. The setting is a \nmodern living room, and \nthe dog is very much at \nhome. gothic art.\nA blue dog lying on a rug \nin the living room. The rug \nin the living room is a \ntraditional blue striped \nrug. The room is \ndecorated in a mid-\ncentury modern style. \nhyperrealism.\nA blue dog lying on a rug in \nthe living room. A blue dog \nlying on a rug in a living room \nthat is decorated in the 19th \ncentury. The rug and the dog \nare both antique and show \nobvious signs of age. \npointillism.\nA blue dog lying on a rug in the \nliving room, his head tilted back \nslightly and his eyes closed. The \nroom is dark with only the soft \nglow of a single light shining \nbrightly. The dog has his head on \nhis paws and his ears are back. \npost-impressionism.\nPrompt Expansion\nFigure 7: Tree of Multi-Step Prompt Expansion: A comparison of images generated of straight-query\ngeneration (left) and Prompt Expansion (right). The first image from each step is further expanded\nupon, in-line with the implementation described in Appendix C. We observe diversity in pose, action,\npoint-of-view, background, colours.\nthe base model that align with the target behaviour. To align with the text-to-image model, we\nconstruct a dataset where the expanded prompts are aligned closer to the behavior of the downstream\ntext-to-image model.\nFor queries in the remaining 50% split of the Prompt Expansion dataset, we generate expanded\nprompts from our base model, which are then input to the downstream text-to-image model (Ima-\ngen (Saharia et al., 2022a) in our experiments). We score these images using a weighted average\nof the query-image embedding distance and prompt-image embedding distance (See Appendix D\nfor details) and filter out {query:prompt} pairs whose scores are below a fixed threshold. We then\ncontinue re-fine-tuning from the base model checkpoint using only these filtered {query:prompt}\npairs, thus producing a PE: Re-fine-tuned model which is optimized to return expansions of the\nquery and flavors that the text-to-image model can faithfully generate high quality images for.\n5\nCONTROLLABLE GENERATION\n5.1\nPREFIXES FOR CONTROLLED PROMPT EXPANSION\nTill now, we have presented our approach for building a model for generic use cases of Prompt\nExpansion. However, it is often the case that the user or application designer would like to control the\ndirection of the Prompt Expansion strategy towards e.g. adding more flavors or adding specific kinds\nof diverse details. To support these use cases, we implement a controllable version of our Prompt\nExpansion model that can be directed to produce specific kinds of expansions by prepending the\nquery with one of 8 supported Prefixes. For example, we can direct the model to produce flavors\nonly using the FLV prefix, or to iteratively expand the original query for interactive multi-step prompt\n8\nTable 3: Aggregate Prompt Expansion performance.\nMethod\nAesthetics (MUSIQ-AVA) \u2191\nText-Image Alignment (COCA(q, I(p))) \u2191\nDiversity (\u03c3p)) \u2191\nStraight-Query Generation\n5.121 \u00b1 0.519\n0.125 \u00b1 0.0147\n0.00582 \u00b1 0.00275\nFew-shot Prompting\n5.295 \u00b1 0.549\n0.114 \u00b1 0.0199\n0.00726 \u00b1 0.00339\nPrompt Expansion\n5.225 \u00b1 0.585\n0.120 \u00b1 0.0175\n0.00720 \u00b1 0.00341\nPE: Re-fine-tuned\n6.185 \u00b1 0.474\n0.113 \u00b1 0.0199\n0.00746 \u00b1 0.00354\nPE: Multi-Prefix\n5.712 \u00b1 0.616\n0.125 \u00b1 0.0157\n0.00624 \u00b1 0.00297\nPE: Prefix Dropout\n5.410 \u00b1 0.622\n0.121 \u00b1 0.0156\n0.00634 \u00b1 0.00304\nexpansion scenarios with the MSTP prefix. A few examples of controlled generation are shown in\nTable 2 and the full list of supported flavors are in Table 1. To train the PE: Multi-Prefix model,\nwe begin with the Prompt Expansion dataset of Section 3. Each {query:prompt} pair is assigned\nwith an appropriate prefix. During few-shot prompting, some queries / prompts were formatted as\nabstract or detailed, thus these prefixes are known (e.g. ABST, DTL). Some prefixes (e.g. RFT, MSTP)\nare also known as their {query:prompt} pairs are synthesized. Some {query:prompt} pairs need\nto be classified to its prefix, for example HAST prefixes are assigned to prompts whose images return\ngood aesthetics. Prefix assignment resulted in a new version of the Prompt Expansion dataset with\nevery query prepended with a prefix; and this was used to fine-tune and train the PE: Multi-Prefix\nmodel.\n5.2\nPREFIX DROPOUT FOR GENERIC PROMPT EXPANSION\nWith the Multi-Prefix dataset in hand, we explored the possibility of using controllable generation\nhints to improve the performance on the generic Prompt Expansion task. The idea is to initialize the\ntraining of the model using controlled generation and then gradually shift its behavior over the course\nof the training to guess an appropriate prefix for a given query and generate the matching expansions\ne.g. for highly abstract queries such as \"Undying Love\", the model\u2019s behavior should match that of\nthe ABST prefix (See Table 2). This is achieved through a novel curriculum learning technique we\ncall Prefix Dropout where we start with the prefix-annotated dataset described above, but over\nthe course of training steadily increase the percentage of examples where the prefix is randomly\nremoved or dropped-out from the query starting from a 0.4 dropout rate to 1.0. This yields the PE:\nPrefix Dropout model which can be compared to our base and re-fine-tuned models as a candidate\nfor generic Prompt Expansion.\n5.3\nMULTI-STEP PROMPT EXPANSION\nExploration can be a multi-step process. After the user\u2019s query returns a set of expanded prompts,\nthe user can select amongst the prompts, and this prompt is fed back into the Prompt Expansion\nmodel. This allows users to iterate on the expanded prompts without the need for manually prompt\nengineering the text. Using PE: Re-fine-tuned, we generate expanded prompts on held-out queries,\nand iteratively generate prompts upon the previous step\u2019s prompts. This results in multi-step training\ndata of expanded prompts to next-step expanded prompts. We re-fine-tune the Prompt Expansion\nmodel with the multi-step data prepended with the MSTP prefix.\n6\nEXPERIMENTS\nWe conducted evaluations of the different flavors of PE described above, focusing mainly on their\nability to generate diverse and aesthetically pleasing images, without signifcant semantic drift in the\nprompt. We conducted these using both automatic metrics and a human evaluation with paid interns.\nIn the rest of this section, we describe the experimental setup, task design, and metrics.\n6.1\nEVALUATION SETUP\nEvaluation set. For both automatic and human evaluation experiments, we sample queries from 2\nsources: (1) n=200 prompts from the PartiPrompts (PP) (Yu et al., 2022b) dataset of prompts that is\ndesigned to represent different domains and features of language (such as counting, negation etc);\n(2) n=500 prompts from a novel test set of potential queries constructed by applying the PE dataset\n9\ngeneration process of Section 3 (Text-Inversion + Query/Prompt Extraction) to Webli-Align (WA)\n(Chen et al., 2023; Jia et al., 2021) images. Queries are categorized as abstract, concrete, short (<4\nwords), medium length (4-7 words), or long length (>7 words). For WA, we obtain prompts of\ndifferent lengths by selecting prompts from different steps of the prompt shortening process (Sec 3).\nModels. We evaluate three variations of Prompt Expansion: (1) A Few-shot Prompting baseline,\nwhere we few-shot prompt a FLAN-PaLMChilla 62B (Chowdhery et al., 2022) model to generate\nexpanded prompts given examples of query-prompt mappings. This is similar to the setup in Section\n3, except we swap the input and output to generate prompts from queries. (2) The base Prompt\nExpansion model which is constructed following all the steps in Section 4.1, except for re-fine-tuning.\n(3) A model trained using the full Prompt Expansion pipeline, which we call PE: Re-fine-tuned.\nWe compare the results of these to Straight-Query Generation, where the text-to-image generation\nmodel is given the original query as input. All our prompts are given as input to Imagen (Saharia\net al., 2022b) and the resulting images are evaluated using the metrics below.\n6.2\nMETRICS FOR AUTOMATIC EVALUATION\nWe use 3 metrics to evaluate the images generated from the prompts returned by the models described\nabove:\n\u2022 For aesthetics, we use MUSIQ (pre-trained on the AVA dataset) (Ke et al., 2021). A higher\nscore is better, and is primarily used for evaluating distortions in photorealistic images\n(cartoon images, pixel art, and paintings tend to receive lower scores).\n\u2022 For diversity, we use the variance(\u03c3p) of the COCA (Yu et al., 2022a) image embeddings\nof the generated images.\n\u2022 For text-image alignment, we use the cosine similarity between the COCA text embeddings\nof the query text against the COCA image embeddings of the generated image, similar\nto CLIP-score (Hessel et al., 2021). For a query q used to generate an expanded prompt\np, which is then used to generate its corresponding image I(p), to measure the COCA\nembedding distance COCA(\u00b7, \u00b7) we denote the COCA score between query text and expanded\nprompt image as COCA(q, I(p)), and between prompt text and expanded prompt image as\nCOCA(p, I(p)).\n6.3\nTASK DESIGN FOR HUMAN EVALUATION\nWe perform a side-by-side (SxS) evaluation task, where we show raters a pair of images side by side\nfor a given text query, where each image is generated by a prompt from one PE model or the other,\nand ask the rater to pick the best one. We do this for 2 separate criteria, aesthetic preference and\ntext-image alignment (details of the task design in Appendix H). We split our query set into two\nsubsets of 350 queries each to be used in the following settings:\n1. Random vs. Random (1x1): A random image generated from each prompt is compared SxS.\n2. Best vs. Best (4x4): For each prompt, we generate 4 images using the text-to-image model, ask\nraters to select the best one for each prompt in separate tasks, and then ask a fresh rater to compare\nthe selected images SxS.\nIf two images contain all the attributes and details specified by the query, there are no further\ndistinctions from which a rater may rate one image as being more aligned to the query than the other.\nThus, for text-image alignment we allow the rater to judge the SxS rating task as \"Equivalent\" when\nthey find no significant difference between the images, but for Aesthetics they are required to select\none of the images as better. We analyze the results using Prompt-win rates vs Query-win rates (the\npercentage of time each rater chose the prompt-generated image over the query-generated image),\nalong with reporting Equivalent-rate for text-image alignment.\n7\nRESULTS AND DISCUSSION\nFrom the results of the automatic (Table 3) and human evaluation (Figures 8, 9, and 10), we derive\nsome insights relevant to the Research Questions raised in Section 1.\n10\nFew-shot Prompting Task: Abstract\nFew-shot Prompting Task: Concrete\nFew-shot Prompting Length: Short\nFew-shot Prompting Length: Medium\nFew-shot Prompting Length: Long\nPrompt Expansion Task: Abstract\nPrompt Expansion Task: Concrete\nPrompt Expansion Length: Short\nPrompt Expansion Length: Medium\nPrompt Expansion Length: Long\nPE (re-fine-tune) Task: Abstract\nPE (re-fine-tune) Task: Concrete\nPE (re-fine-tune) Length: Short\nPE (re-fine-tune) Length: Medium\nPE (re-fine-tune) Length: Long\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n\u0394Aesthetics\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\n0.0025\n0.0030\n\u0394Diversity\n\u22120.025 \u22120.020 \u22120.015 \u22120.010 \u22120.005 0.000\n\u0394Text \u2212 Image Alignment\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\n0.0025\n0.0030\n\u0394Diversity\n\u22120.025 \u22120.020 \u22120.015 \u22120.010 \u22120.005 0.000\n\u0394Text \u2212 Image Alignment\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n\u0394Aesthetics\nFew-shot Prompting Task: Abstract\nFew-shot Prompting Task: Concrete\nFew-shot Prompting Length: Short\nFew-shot Prompting Length: Medium\nFew-shot Prompting Length: Long\nPrompt Expansion Task: Abstract\nPrompt Expansion Task: Concrete\nPrompt Expansion Length: Short\nPrompt Expansion Length: Medium\nPrompt Expansion Length: Long\nPE (re-fine-tune) Task: Abstract\nPE (re-fine-tune) Task: Concrete\nPE (re-fine-tune) Length: Short\nPE (re-fine-tune) Length: Medium\nPE (re-fine-tune) Length: Long\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nHuman (Aesthetics 4x4)\n0.005\n0.006\n0.007\n0.008\n0.009\n0.010\nDiversity\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nHuman (Text-Image Alignment 4x4)\n0.005\n0.006\n0.007\n0.008\n0.009\n0.010\nDiversity\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nHuman (Text-Image Alignment 4x4)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nHuman (Aesthetics 4x4)\nFigure 8: We compare ({Few-shot Prompting, Prompt Expansion, and PE (Re-fine-tuned)}\n- Straight-Query Gen.). For all automatic evaluation metrics, we plot the difference \u2206metric =\nmetricPE \u2212 metricSQG. For all human rater evaluation metrics, the rater study already compares\nrelative to Straight-Query Generation. For Human (Consistency) values, we plot Prompt+Equivalent\nscores. For completeness, we provide all numbers in Figures 5 and 9-10.\n\u2014Does Prompt Expansion increase diversity of the generated images? Table 3 shows a small\nbut consistent increase in diversity scores between our various Prompt Expansion models and the\nStraight-Query baseline. Though small compared to the confidence interval, the gains are similar\nacross all systems and hold consistently across query types (see Table 5), increasing our confidence\nin the positive impact of PE on diversity. We observe PE: Re-fine-tuned has marginally higher\ndiversity than other PE models, both in aggregate (Table 3) and per query type (Table 5).\nWe also found it possible to further increase overall diversity by using techniques such as temperature\nsampling and post-hoc filtering (See Table 8 for results). Qualitatively, this increase in diversity\nmanifests in topical, semantic, and expressive diversity, and in fairness considerations such as age,\ngender, and culture (See the examples in Table 4).\n\u2014Does prompt optimization lead to better aesthetics? The PE: Re-fine-tuned model shows\na significant increase in the aesthetic score vs. Straight-Query generation, while the gain for the\nother PE models are more modest. We conclude therefore, that the text-to-image model-relative parts\nof our pipeline are critical to improving aesthetic values. This conclusion is further strengthened\nby examining the rater preference results (Figure 9), where we see that re-fine-tuning consistently\nachieves higher win-rates over straight-query baselines (scoring > 0.52 across all query types) than\nthe other models. For the 1x1 setting, this can be entirely attributed to the prompt being optimized\nduring re-fine-tuning for higher aesthetics. A significant source of the gains in aesthetics is due to\nthe Prompt Expansion model learning which flavors the downstream text-to-image model is most\nresponsive to (Examples of these are shown in Figure 6).\n\u2014Does increased diversity of samples lead to better aesthetics? We hypothesize that in addition to\nthe benefits of tuning the expanded prompt to generate higher quality images, there is an additional\ngain of quality achieved when images from a single prompt are more diverse. With higher diversity,\nthe likelihood of a highly aesthetically pleasing image being generated and selected by the user\ngoes up. To demonstrate this effect, we compare the difference in Prompt-win rates between the\n4x4 (where the best images from each system are compared) and 1x1 settings, see Figure 9. The\nPrompt-win rates are consistently larger across all systems in the former, showing the aesthetic\nbenefits of increased diversity at both the prompt and image level.\n11\n(Query) religious light\nreligious light\nAn iconic statue of Buddha in a \nbuddhist monastery. The statue is \nmajestic and dominating in its \nsurroundings. The statue \nrepresents peace and harmony, \nand the monastery is filled with \nmonks and monks. art deco.\nA religious leader with a halo \nilluminating the sky, while \nsurrounded by thousands of \ncandles set ablaze.\nA depiction of a church with many \npeople worshipping in it during \nevening service. The church is full \nof sunlight and the people are all \ndressed up in black. The church is \nornate and the people are singing \nand dancing.\nA photograph of a golden lamp, a \ncandle burning inside it. The lamp \nhas an intense glow and reflects the \ngolden light around it. The lamp has \na bright, sparkling look, and the \nsurrounding area is decorated in \ncheerful and colorful patterns.\nBuddhism                             Christianity                           Hinduism\n(Query) emotional \nconflict people\nmiddle-aged couple                 mother and child                     old age\nAspect\nQuery\nPrompt\nReligion\nreligious light\n\u2014An iconic statue of Buddha in a buddhist monastery. The statue is ma-\njestic and dominating in its surroundings. The statue represents peace and\nharmony, and the monastery is filled with monks and monks. art deco.\n\u2014A depiction of a church with many people worshipping in it during\nevening service. The church is full of sunlight and the people are all dressed\nup in black. The church is ornate and the people are singing and dancing.\nCulture\nA woman wearing a\nlong, flowing orange\nand white dress.\n\u2014A beautiful portrait of a woman in a flowing orange and white dress. The\ndress is long and flowing, with a deep v-neck and flowing sleeves. The\nbackground is a bright sunrise, with a lone cloud in the sky. The overall\nmood of the work is nostalgic and romantic. futurism.\n\u2014A portrait of a young woman wearing a traditional Indian wedding dress.\nThe woman is smiling and looking over at her husband. The dress is flowing,\nflowing, flowing, and the lighting is soft and romantic. futurism.\nAge & Gender\nangular eyebrows\n\u2014A black-and-white photograph of a man\u2019s eyes with thick, angular eye-\nbrows. The photograph is at least two decades old, and the man is most\nlikely still alive. hyperrealism.\n\u2014A young woman with wide-open eyes and an angular, sagginess to her\neyebrows. The woman is looking into the mirror, her face is radiant with\nconfidence. trending on cg society.\n\u2014A young boy has angular eyebrows with dark hair. The boy is smiling, and\nhe is wearing a dark shirt and dark pants. The boy is standing in the grass,\nand he is looking at the sky with a dark grin on his face. post-impressionism.\nTable 4: Fairness from diversity: Prompt Expansion generates prompts that introduce variations\nacross religion, culture, and demographics at the textual level. We highlight in these examples, the\nsentences that are indicative of the variation. Despite no explicit training for specific topical diversity,\nPrompt Expansion responds to a query on religion by showing imagery from different religious\nbackgrounds from Buddhism, Christianity, and Hinduism. It responds to another underspecified\nquery \"emotional conflict people\" by showing people in different situations of emotional conflict\nand of varying age demographics. For another example, Prompt Expansion still finds small ways\nto introduce cultural variations that remain faithful to a long and detailed query, such as describing\ndresses from different global contexts.\n12\nFigure 9: Human Rater results for Aesthetics in 1x1 and 4x4 settings.\nhuman aesthetics 1x1 few-shot\n(a) Human Evaluation (1x1 Aes-\nthetics, Few-Shot Prompting)\nhuman aesthetics 1x1 pe\n(b) Human Evaluation (1x1 Aes-\nthetics, Prompt Expansion)\nhuman aesthetics 1x1 rft\n(c) Human Evaluation (1x1 Aes-\nthetics, PE: re-fine-tuning)\nhuman aesthetics 4x4 few-shot\n(d) Human Evaluation (4x4 Aes-\nthetics, Few-Shot Prompting)\nhuman aesthetics 4x4 pe\n(e) Human Evaluation (4x4 Aes-\nthetics, Prompt Expansion)\nhuman aesthetics 4x4 rft\n(f) Human Evaluation (4x4 Aes-\nthetics, PE: re-fine-tuning)\nFigure 10: Human Rater results for Text-Image Alignment in 1x1 and 4x4 settings.\nhuman consistency 1x1 few-shot\n(a)\nHuman\nEvaluation\n(1x1\nText-Image Alignment, Few-Shot\nPrompting)\nhuman consistency 1x1 pe\n(b) Human Evaluation (1x1 Text-\nImage Alignment, Prompt Expan-\nsion)\nhuman consistency 1x1 rft\n(c) Human Evaluation (1x1 Text-\nImage Alignment, PE: re-fine-\ntuning)\nhuman consistency 4x4 few-shot\n(d)\nHuman\nEvaluation\n(4x4\nText-Image Alignment, Few-Shot\nPrompting)\nhuman consistency 4x4 pe\n(e) Human Evaluation (4x4 Text-\nImage Alignment, Prompt Expan-\nsion)\nhuman consistency 4x4 rft\n(f) Human Evaluation (4x4 Text-\nImage Alignment, PE: re-fine-\ntuning)\n\u2014Does Prompt Expansion balance between aesthetics and text-image alignment? An important\nobservation is that while an expanded prompt can be designed to preserve all the semantics of the\noriginal query (by ensuring no details are lost etc.), it can never be expected to increase the degree\nof alignment of the image to the original intent of the user as expressed by the query. Thus, the\nmain goal of Prompt Expansion with respect to text-image alignment is to minimize the decrease\nof alignment scores while optimizing for aesthetics and diversity. Thus, we expect to see a tradeoff\nbetween alignment against and aesthetic/diversity scores.\nIndeed, in Table 3, we see that there is a minimal drop in the alignment scores of our PE models,\ninversely correlated with the other 2 metrics of diversity and aesthetics. The alignment score drop\nhowever, is well within the confidence intervals, showing that the semantics of the expanded prompt\ndo not stray too far from the meaning of the original query. As further evidence of this, we turn to the\nhuman rater evaluation for text-image alignment (Figure 10) where see large Equivalent-rates in the\ncomparisons, and comparable win-rates for prompt and query. For example, in the 1x1 setting for the\nre-fine-tuning model, we see a 70% equivalent-rate and prompt/query win-rates of 15% each.\nWe further explore the trade-offs between diversity, aesthetics, and text-image alignment by presenting\nPareto-optimality curves between the various automatic and human metrics for all the different query\n13\ntypes in Figure 8. While PE: Re-fine-tuned dominates Prompt Expansion in terms of Diversity-\nAesthetics, we notice that the converse is true for Diversity-Alignment. If text-image alignment\nis more important than diversity and aesthetics collectively, then it may be beneficial to omit the\nre-fine-tuning step.\n\u2014Can controllable generation improve performance? In Table 3, we see that PE: Multi-Prefix\nobtains better aesthetics than our base PE model, and remarkably without any loss in text-image\nalignment. Note however that these results are obtained with experts annotating the test set with the\noptimal prefix for each query before it is input to the model. Thus, these results should be seen more\nas an upper-bound on how much controlled generation can benefit the generic Prompt Expansion task.\nHowever, we also see that the PE: Prefix Dropout model retains much of the advantage of PE:\nMulti-Prefix, without matching the aesthetic performance of the PE: Re-fine-tuned model. It\nhas the advantage however, of not being optimized for a particular downstream text-to-image model\nand is thus more generally more applicable.\n8\nCONCLUSION\nText-to-image generation models are capable of producing high-quality images from text prompts,\nbut they require specialized skills and knowledge to use effectively. Prompt Expansion reduces the\nneed for users to iteratively prompt-engineer and over-specify their text prompt. Human raters find\nthat images generated through Prompt Expansion are more aesthetically-pleasing and diverse than\nbaselines. Empirical metrics for aesthetic and diversity also show that Prompt Expansion outperforms\ncomparable baselines. In addition, the paper discusses the use of Prompt Expansion as a building\nblock for other use cases such as multi-step adaptation and controllable generation.\nACKNOWLEDGEMENTS\nWe would like to thank Jason Baldridge, Austin Waters, Roopal Garg, and Su Wang for discussions,\nguidance, and regular support throughout the project. We thank Stefan Zinke, Amnah Ahmed,\nJason Wang, Sunny Mak, and Ravi Rajakumar for building the text-to-image platforms from which\nthe CrowdSourced dataset was collected, and for general infrastructure support. We thank Jordi\nPont-Tuset, Marisa Ferrara Boston, Alena Butyrna, Josh Lee, Max Lin, and internal product teams at\nGoogle for guidance in the evaluation and deployment of Prompt Expansion, and Isabelle Guyon and\nTania Bedrax-Weiss for executive support. We thank Chin-yi Cheng for reviewing an early version\nof this draft. We also thank the many colleagues working on the text-to-image generation effort,\ndatasets, language and image models, and scalable deep learning infrastructure within Google for\nhelping to make the project a reality.\n14\nREFERENCES\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury,\nSiddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A.\nChoquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa\nDev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad\nFienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,\nSteven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,\nMichael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang\nLi, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,\nAroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John\nNham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,\nReiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,\nBrennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,\nDaniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny\nZhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report.\nEric J. Bigelow, John P. McCoy, and Tomer D. Ullman. 2023. Non-commitment in mental imagery.\nCognition, 238:105498.\nStephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, and Tovi Grossman. 2023. Promptify:\nText-to-image generation through interactive prompt exploration with large language models.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver,\nNan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V Thapliyal, James\nBradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme\nRuiz, Andreas Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut.\n2023. PaLI: A jointly-scaled multilingual language-image model. In The Eleventh International\nConference on Learning Representations.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.\nDai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. In\narXiv:2001.08361.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun\nDai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin\nRobinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang,\nAndrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny\nZhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models.\nCLIP-Interrogator. Clip-interrogator. https://github.com/pharmapsychotic/clip-interrogator.\n15\nGuillem Collell and Marie-Francine Moens. 2016. Is an Image Worth More than a Thousand Words?\nOn the Fine-Grain Semantic Differences between Visual and Linguistic Representations. In\nCOLING.\nGradio. Gradio. https://www.gradio.app/.\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022.\nOptimizing prompts for text-to-image\ngeneration.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\n2022.\nPrompt-to-Prompt Image Editing with Cross Attention Control.\nIn arXiv preprint\narXiv:2208.01626.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: A\nreference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718.\nJonathan Ho and Tim Salimans. 2021. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop\non Deep Generative Models and Downstream Applications.\nBen Hutchinson, Jason Baldridge, and Vinodkumar Prabhakaran. 2022. Underspecification in scene\ndescription-to-depiction tasks. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter\nof the Association for Computational Linguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 1: Long Papers), pages 1172\u20131184, Online only.\nAssociation for Computational Linguistics.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916.\nPMLR.\nJunjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. 2021. Musiq: Multi-scale\nimage quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 5148\u20135157.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient\nprompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\nRanjita Naik and Besmira Nushi. 2023. Social biases through the text-to-image generation lens.\narXiv preprint arXiv:2304.06034.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical\nText-Conditional Image Generation with CLIP Latents. In arXiv:2204.06125.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\n2022. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim\nSalimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. 2022a. Photorealistic text-to-\nimage diffusion models with deep language understanding.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim\nSalimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. 2022b. Photorealistic Text-to-\nImage Diffusion Models with Deep Language Understanding. In NeurIPS.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.\n2023. UL2: Unifying language learning paradigms. In The Eleventh International Conference on\nLearning Representations.\n16\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.\n2020. Neural text generation with unlikelihood training. In International Conference on Learning\nRepresentations.\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein.\n2023. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and\ndiscovery.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.\n2022a. Coca: Contrastive captioners are image-text foundation models. Transactions on Machine\nLearning Research.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\nLi, Han Zhang, Jason Baldridge, and Yonghui Wu. 2022b. Scaling Autoregressive Models for\nContent-Rich Text-to-Image Generation. In arXiv:2206.10789.\n17\nAPPENDIX\nA\nTYPES OF QUERIES\nQueries can arise in different structures or user intents at test-time. We extract a range of potential\nuser queries that would be mapped to the inverted text (prompt). One intent for this is that a range\nof user queries may map to the same prompt text. For example, the user query may be abstract and\nunderspecified, and a Prompt Expansion model is needed to introduce ideas and potential content\ndirections. Or the user query may be very concrete and detailed, but needs refinement in fine-grained\ndetails or phrasing to improve image aesthetics. The latter example is supportive of an iterative\napproach to image generation, where prompt/image details are added incrementally.\nFocused on adapting Prompt Expansion to the level of image specification provided by the user, there\nare two general settings that we consider: semantic expansion, and detailed expansion. Semantic\nexpansion is a Prompt Expansion setting where the user queries are abstract or underspecified, the\nuser intent may be towards ideation or exploration, and the output prompts intend to add details\nand expand on the query conceptually. Detailed expansion is a Prompt Expansion setting where the\nqueries may be quite detailed and concrete, and the expanded prompt needs added details while still\nremaining faithful to the original query. This consequently results in the construction of additional\nsub-datasets (mixtures) to be added to the base Prompt Expansion dataset. These augmentative\nmixtures include abstract queries (to be used in evaluating semantic expansion), detailed queries\n(such as grounded / specificity mixtures, to be used in evaluating detailed expansion), etc.\nTo further support these wider range of query use cases (abstract, detailed, varying sentence\nlength), we few-shot prompt to generate more training data accordingly. For example, grounded\n{query:prompt} pairs require the sub-phrases in the input query to reside in the output prompt, while\nspecificity {query:prompt} pairs require the format of the output prompt to begin with the query,\nand the remainder of the prompt is posited as the example of the query (Table 2). Constructing these\naugmentations need to be mindful of grammatical errors, which may reduce human readability in\nthe expanded prompts. We also need to need to be mindful of the token distribution of the training\ndataset, so as to avoid skewing the model towards generating prompts on very specific topics, or\nrepeating tokens (e.g. in Figure 13b). These mixtures are also train-test split, where the test splits can\nbe used to evaluate models across varying query types. Examples of these query types can be seen in\nTable 2.\nB\nADAPTING TO QUERY SPECIFICATION\nB.1\nCONTROLLABLE GENERATION WITH Multi-Prefix\nNot all queries are the same, and different queries may have different requirements on what their\nexpanded prompts should look like. We assign different query types with a corresponding prefix\n(Table 1), and train a Prompt Expansion model. Having prepended the queries with prefixes during\ntraining, at test-time we prepend the queries with query-specific prefixes to determine the format of\nthe expanded prompts. In Table 5, assuming the query type is known and the query-specific prefix\nis used during inference, we find that PE: Multi-Prefix maintains higher text-image alignment\nagainst other Prompt Expansion models.\nOther than handling varying levels of image specification, prefixes have also been used for indicating\nmulti-step prompt expansion. The use of queries sets a clear task boundary between different query\nsets/mixtures, such that the style of expanded prompt for one query does not interfere with others.\nThis also means we can continue to add new query types, their data, and prefixes over time, without\nrisking interference with respect to other query types. For a given dataset, if we assign a prefix\nfor each data point, the trade-off is that assigning (more) prefixes (further) dilutes the number of\nsamples per prefix. Encountering fewer samples per prefix may result in underfitting per prefix\u2019s\ndata, in contrast to fitting on the whole prefix-less dataset. To balance this trade-off, the Prompt\nExpansion models in the main results only make use of the MSTP prefix in the training dataset and\nduring multi-step inference, and does not assign any other prefixes. Models trained on other prefixes\nare used in PE: Multi-Prefix (query-specific) and PE: Prefix Dropout (query-agnostic) models\nin Table 5. PE: Multi-Prefix makes use of all augmentative mixtures, assigns prefixes to queries\n18\nduring training (though replacing GRD and SPCT with DTL), and relies on the type-specific prefix\nduring inference.\nTable 5: Comparison of PE variations: The base Prompt Expansion (PE) model maps varying\nquery lengths to a fixed expanded prompt; we evaluate the ablation where each query maps to the\nsubsequent slightly-longer query, resulting in varying output prompt length. We also ablate the use\nof all prefixes and all mixtures in the dataset. We sub-ablate this further by comparing the use of\nprefixes to perform query-specific inference (i.e. the model knows what kind of query it is handling)\nagainst query-agnostic inference (through the use of Prefix Dropout).\nMethod\nPrompt\nCategory\nAesthetics\n(MUSIQ-AVA) \u2191\nText-Image Alignment\n(COCA(q, I(p))) \u2191\nDiversity (\u03c3p)) \u2191\nOutput prompt length\n(mean+std)\nStraight-Query\nGeneration\nTask: Abstract\n5.114 \u00b1 0.528\n0.117 \u00b1 0.0106\n0.00606 \u00b1 0.00286\n2.927 \u00b1 2.249\nTask: Concrete\n5.141 \u00b1 0.505\n0.126 \u00b1 0.0137\n0.00596 \u00b1 0.00281\n7.590 \u00b1 2.893\nLength: Short\n5.0632 \u00b1 0.516\n0.117 \u00b1 0.0106\n0.00611 \u00b1 0.00289\n2.196 \u00b1 0.686\nLength: medium\n5.142 \u00b1 0.523\n0.125 \u00b1 0.0140\n0.00561 \u00b1 0.00265\n5.417 \u00b1 1.114\nLength: long\n5.164 \u00b1 0.513\n0.134 \u00b1 0.0180\n0.00571 \u00b1 0.00268\n11.525 \u00b1 5.773\nDataset: WAA\n5.128 \u00b1 0.517\n0.122 \u00b1 0.0130\n0.00601 \u00b1 0.00283\n5.263 \u00b1 3.486\nDataset: PartiPrompts\n5.105 \u00b1 0.523\n0.134 \u00b1 0.0189\n0.00535 \u00b1 0.00253\n9.156 \u00b1 7.483\nFew-shot\nPrompting\nTask: Abstract\n5.289 \u00b1 0.560\n0.102 \u00b1 0.0161\n0.00780 \u00b1 0.00372\n11.218 \u00b1 3.0391\nTask: Concrete\n5.293 \u00b1 0.530\n0.117 \u00b1 0.0176\n0.00726 \u00b1 0.00346\n17.0727 \u00b1 3.0695\nLength: Short\n5.274 \u00b1 0.568\n0.100 \u00b1 0.0164\n0.00794 \u00b1 0.00379\n9.911 \u00b1 3.0202\nLength: medium\n5.322 \u00b1 0.522\n0.117 \u00b1 0.0181\n0.00711 \u00b1 0.00338\n12.842 \u00b1 2.620\nLength: long\n5.293 \u00b1 0.552\n0.127 \u00b1 0.0198\n0.00668 \u00b1 0.00318\n11.420 \u00b1 3.502\nDataset: WAA\n5.291 \u00b1 0.545\n0.109 \u00b1 0.0186\n0.00753 \u00b1 0.00359\n14.169 \u00b1 3.401\nDataset: PartiPrompts\n5.305 \u00b1 0.559\n0.127 \u00b1 0.0233\n0.00658 \u00b1 0.00312\n16.138 \u00b1 4.120\nPrompt\nExpansion (PE)\nTask: Abstract\n5.232 \u00b1 0.596\n0.112 \u00b1 0.0118\n0.00765 \u00b1 0.00362\n34.736 \u00b1 9.0216\nTask: Concrete\n5.260 \u00b1 0.567\n0.120 \u00b1 0.0154\n0.00724 \u00b1 0.00342\n39.362 \u00b1 8.167\nLength: Short\n5.233 \u00b1 0.593\n0.112 \u00b1 0.0122\n0.00780 \u00b1 0.00369\n34.287 \u00b1 8.850\nLength: medium\n5.227 \u00b1 0.577\n0.119 \u00b1 0.0153\n0.00707 \u00b1 0.00334\n37.165 \u00b1 8.169\nLength: long\n5.216 \u00b1 0.584\n0.128 \u00b1 0.0200\n0.00669 \u00b1 0.00316\n42.645 \u00b1 9.129\nDataset: WAA\n5.246 \u00b1 0.582\n0.116 \u00b1 0.0143\n0.00744 \u00b1 0.00352\n37.054 \u00b1 8.909\nDataset: PartiPrompts\n5.173 \u00b1 0.590\n0.129 \u00b1 0.0208\n0.00660 \u00b1 0.00311\n40.467 \u00b1 10.276\nPE: Re-fine-tuned\nTask: Abstract\n6.183 \u00b1 0.505\n0.100 \u00b1 0.0162\n0.00796 \u00b1 0.00380\n42.0169 \u00b1 7.986\nTask: Concrete\n6.183 \u00b1 0.451\n0.115 \u00b1 0.0184\n0.00744 \u00b1 0.00353\n46.143 \u00b1 8.088\nLength: Short\n6.172 \u00b1 0.510\n0.0997 \u00b1 0.0167\n0.00805 \u00b1 0.00384\n41.985 \u00b1 7.978\nLength: medium\n6.210 \u00b1 0.443\n0.115 \u00b1 0.0187\n0.00729 \u00b1 0.00345\n43.126 \u00b1 7.551\nLength: long\n6.176 \u00b1 0.461\n0.125 \u00b1 0.0201\n0.00699 \u00b1 0.00332\n49.49 \u00b1 8.861\nDataset: WAA\n6.183 \u00b1 0.479\n0.108 \u00b1 0.0189\n0.00770 \u00b1 0.00366\n44.0845 \u00b1 8.298\nDataset: PartiPrompts\n6.189 \u00b1 0.464\n0.125 \u00b1 0.0227\n0.00686 \u00b1 0.00325\n47.0 \u00b1 9.754\nPE: step-by-step\nexpansion\nTask: Abstract\n5.191 \u00b1 0.539\n0.114 \u00b1 0.0120\n0.00756 \u00b1 0.00360\n11.161 \u00b1 3.941\nTask: Concrete\n5.179 \u00b1 0.505\n0.124 \u00b1 0.0141\n0.00670 \u00b1 0.00318\n19.344 \u00b1 6.125\nLength: Short\n5.173 \u00b1 0.520\n0.114 \u00b1 0.0123\n0.00765 \u00b1 0.00365\n9.888 \u00b1 2.719\nLength: medium\n5.187 \u00b1 0.527\n0.122 \u00b1 0.0149\n0.00663 \u00b1 0.00316\n15.405 \u00b1 3.787\nLength: long\n5.161 \u00b1 0.526\n0.131 \u00b1 0.0184\n0.00617 \u00b1 0.00290\n25.926 \u00b1 9.401\nDataset: WAA\n5.185 \u00b1 0.522\n0.119 \u00b1 0.0139\n0.00713 \u00b1 0.00339\n15.261 \u00b1 6.579\nDataset: PartiPrompts\n5.144 \u00b1 0.529\n0.132 \u00b1 0.0198\n0.00610 \u00b1 0.00289\n21.582 \u00b1 12.526\nPE: Multi-Prefix\nTask: Abstract\n5.653 \u00b1 0.610\n0.116 \u00b1 0.0153\n0.00658 \u00b1 0.00315\n20.198 \u00b1 8.617\nTask: Concrete\n5.718 \u00b1 0.617\n0.123 \u00b1 0.0162\n0.00630 \u00b1 0.00301\n21.569 \u00b1 7.280\nLength: Short\n5.663 \u00b1 0.619\n0.116 \u00b1 0.0156\n0.00652 \u00b1 0.00312\n20.196 \u00b1 8.0729\nLength: medium\n5.744 \u00b1 0.603\n0.125 \u00b1 0.0165\n0.00627 \u00b1 0.00298\n22.471 \u00b1 7.849\nLength: long\n5.709 \u00b1 0.639\n0.126 \u00b1 0.0163\n0.00616 \u00b1 0.00293\n21.976 \u00b1 6.117\nDataset: WAA\n5.705 \u00b1 0.609\n0.122 \u00b1 0.0156\n0.00638 \u00b1 0.00305\n20.994 \u00b1 6.833\nDataset: PartiPrompts\n5.729 \u00b1 0.635\n0.131 \u00b1 0.0161\n0.00588 \u00b1 0.00278\n23.692 \u00b1 6.501\nPE: Prefix\nDropout\nTask: Abstract\n5.394 \u00b1 0.618\n0.116 \u00b1 0.0147\n0.00667 \u00b1 0.00320\n18.217 \u00b1 8.298\nTask: Concrete\n5.457 \u00b1 0.631\n0.120 \u00b1 0.0149\n0.00640 \u00b1 0.00308\n19.264 \u00b1 8.672\nLength: Short\n5.384 \u00b1 0.637\n0.117 \u00b1 0.0152\n0.00660 \u00b1 0.00317\n18.377 \u00b1 8.472\nLength: medium\n5.401 \u00b1 0.619\n0.122 \u00b1 0.0167\n0.00630 \u00b1 0.00302\n20.212 \u00b1 8.729\nLength: long\n5.445 \u00b1 0.608\n0.122 \u00b1 0.0158\n0.00624 \u00b1 0.00300\n19.903 \u00b1 7.868\nDataset: WAA\n5.425 \u00b1 0.625\n0.118 \u00b1 0.0149\n0.00654 \u00b1 0.00314\n18.741 \u00b1 8.504\nDataset: PartiPrompts\n5.374 \u00b1 0.614\n0.129 \u00b1 0.0174\n0.00583 \u00b1 0.00280\n22.148 \u00b1 7.359\n19\nB.2\nQUERY AMBIGUITY WITH Prefix Dropout\nAt test-time, we may not be able to assume we know the query type or which prefix to use. Thus, it is\nnecessary for the Prompt Expansion model to infer the query type such that it can return appropriate\nexpanded prompts (e.g. semantic vs detailed expansion). The Prompt Expansion model will be\nrequired to (implicitly) infer the query type correctly, then perform type-specific inference. To\ntackle this, we develop Prefix Dropout, a curriculum learning approach where we assign a prefix\nto each query type mixture, and train the Prompt Expansion model on the data with prefixes, but\nwith a linearly increasing dropout rate (starting from 0.4 dropout rate to gradually 1.0 dropout rate),\nwhere the prefixes are randomly dropped out. This allows the model to develop the representations\nneeded to perform the task, and then gradually reassign weights whose activations would trigger\nthese representations. The model learns during training that the prefixes are not a reliable indicator.\nWe observed during training that the distribution of sampled expanded prompts would initially be\neither one query type or the other for a given query (implicit classification of the query type, i.e.\ntype inference), but as the model begins to encounter a higher dropout rate such that no prefixes are\navailable, it begins to diversify the generated samples such that there is a mix of sentences from all\nquery types. During training, PE: Prefix Dropout makes use of all the augmentative mixtures,\nuses all the original prefixes, and during inference does not require the prefix. As seen in Table 5\u2019s\nresults for PE: Prefix Dropout, Prefix Dropout can resolve query type ambiguity at test-time with\noutput samples from multiple types, while also retaining text-image (query-prompt) alignment.\nAesthetics (MUSIQ)\nText-Image Alignment \n(COCA(q, I(p))\nDiversity (\u03c3)\nFigure 11: Evaluating consistency in Multi-Step Prompt Expansion: We find that the diversity of\nPrompt Expansion is consistent over number of steps.\nAesthetics (MUSIQ)\nText-Image Alignment \n(COCA(q, I(p))\nDiversity (\u03c3)\nFigure 12: Evaluating consistency with scale: We find that the diversity of Prompt Expansion is\nconsistent over number of expanded prompts per step.\nC\nMULTI-STEP PROMPT EXPANSION\nRather than continuously increasing expanded prompt sequence length, we construct a set/mixture\nthat maps an expanded prompt from step t\u22121 to an expanded prompt for step t. Using a re-fine-tuned\nmodel, we generate expanded prompts on held-out queries, and iteratively generate prompts upon the\nprevious step\u2019s prompts. We re-fine-tune the model on this multistep dataset.\nAs demonstrated in Figure 6, few-shot prompting of prompts to queries return multiple steps of\nexpansion based on input length and output length. This enables multi-step prompt expansion for\n20\nshorter queries. In the initial model, there is a maximum observed output length in our training\ndataset, and thus an expanded prompt may tend to stop expanding in length at this limit. Additionally,\nthe text-to-text model also has a token length limit.\nTo enable expanded prompts past such limits, we first construct a paired training set/mixture of\nexpanded prompts to next-step expanded prompts. The expanded prompt passed to the Prompt\nExpansion model returns expanded prompts (of similar output length but replaced details). We reuse\nthe Prompt Expansion model by passing the expanded prompt and receiving a set of variant expanded\nprompts of alternative details. We then append sentences with additional details to the input prompt.\nWe repeat this until the input prompt length reaches the token limit. In practice, if the query exceeds\nthe token limit, it will need to be truncated.\nWe re-fine-tune the Prompt Expansion model with the multi-step data, and add the MSTP prefix. As\nthe user is reusing the previous step\u2019s prompt, we know that the query type is that of \"multi-step\",\nand know from Table 5 (PE: Multi-Prefix) that a prefix can return better metric performance if\nthe query type is known and its prefix is used. We also may wish to avoid the performance drop that\nmay arise from having output sequences of varying lengths, as seen in Table 5 (PE: step-by-step\nexpansion). PE: step-by-step expansion is a supporting ablation that mapping queries to the\nfull expanded prompt (as opposed to the subsequent query) is not ideal.\nAfter generating N = 4 expanded prompts from the initial query at step t = 0, at step t = 1 we then\nsubsequently generate N expanded prompt for each expanded prompt, and so on, resulting in N t+1\nexpanded prompts in a set to evaluate. In Figure 11, we observe consistent diversity across number\nof expansion steps. We evaluate against N t+1 expanded prompts at the t-th step. We observe that\ndiversity is consistent in a multi-step setting over several iterations, where the expanded prompt from\na previous iteration is passed back into the model to return another set of expanded prompts.\nWhile Figure 11 evaluates the consistency of Prompt Expansion over the number of steps, Figure\n12 evaluates its consistency over the number of expanded prompts generated. We observe that the\ndiversity of the generated images continue to increase, and aesthetics and text-image alignment\nremains consistent.\n21\nflavor\nCOCA(q, I(p)) \u2191\nCOCA(p, I(p))) \u2191\nTop-5 flavors\nart deco\n0.163\n0.169\nvorticism\n0.162\n0.168\nclassical realism\n0.161\n0.165\nfine art\n0.155\n0.166\nfigurative art\n0.159\n0.161\nWorst-5 flavors\nacademic art\n0.096\n0.110\ninternational gothic\n0.107\n0.098\npixel art\n0.085\n0.114\narte povera\n0.093\n0.102\ngenerative art\n0.102\n0.088\nc\n+optical illusion\n+behance\nUnresponsive to flavours\nResponsive to flavours\nc\n+shock art\n+kinetic pointillism\nc\n+deconstructivism\n+psychedelic art\nTable 6: Top: The average query-image embedding distance and prompt-image embedding distance\nfor flavors are ranked. Bottom: For the same seed, image generation does not respond to the insertion\nto some flavors, while responding to others.\nD\nPROBING THE ALIGNMENT BETWEEN TEXT-TO-TEXT AND TEXT-TO-IMAGE\nMODELS\nUsing the base model, we probe the ability of the Prompt Expansion model in rendering aesthetically-\npleasing and diverse images. We observe that the text-to-image model has difficulty in rendering\ncertain prompts (e.g. human hands, faces). This can become an issue when the Prompt Expansion\nmodel generates prompts such that the text-to-image model cannot render its image.\nWe begin by evaluating how well flavors can be rendered. Compared to specific details in captions,\nflavors have a higher frequency of overlap / re-use between different expanded prompts. As such,\nwe would be able to evaluate how well a flavor is rendered across multiple expanded prompts. We\nevaluate the COCA similarity between the query text embedding and generated image embedding, as\nwell as the COCA similarity between the prompt text embeddings and generated image embedding.\nAs shown in Table 6 (Top), sorted by the average of the two COCA scores, the similarity scores\nbetween the Top-5 highest flavors diverges greatly from that of the Worst-5 flavors. Referring to\nTable 6 (Bottom), where we generate two images (without and with flavor) with the same seed, we\ncan visually observe that flavors in the left column are unresponsive (e.g. the seeded image does not\nchange much), while the flavors in the right column visually change the image. Probing the difficulty\nin rendering certain prompts motivates the need to re-fine-tune the Prompt Expansion model to align\nit with the target text-to-image model (Section 4.1).\n22\nE\nPROMPT TUNING AVOIDS A SKEWED TOKEN DISTRIBUTION\nWith Table 7, we establish the training setup of the PaLM 2 model. We evaluate fine-tuning against\nprompt-tuning, and a smaller 1B against a larger 24B parameter model. We show that prompt-\ntuning a 1B PaLM 2 model is the optimal setup, achieving pareto-optimality in parameter count,\naesthetics, text-image alignment, and diversity scores compared to other configurations. In-line\nwith Welleck et al. (2020)\u2019s evaluation of repetition, we also evaluate the repetitiveness of expanded\nprompt generation. We observe that fine-tuned models returned a higher repetition rate, indicating a\npotentially skewed token distribution, likely towards tokens seen during training. This manifests in\nexpanded prompts as recurring terms appearing regardless of varying queries, and even recurring\nterms for the same query (examples in Figure 13b).\nFigure 13: Examples of challenges mitigated in the current Prompt Expansion Model.\nQuery: religious\nPrompt: a painting of a \nman with a beard.\nQuery: poultry in a barn\nPrompt: a chicken laying \neggs in a chicken coop.\nDropping details\n(a) Dropping details from step-by-step expansion: Qualitatively supporting the findings from Table 5, training\non variable output sequence length and mapping step-by-step (PE: step-by-step expansion) encounters\nchallenges that others do not. We observe that a drop of details may occur (e.g. barn is not mentioned in the\nprompt for \"poultry in a barn\"), thus resulting in lower text-image alignment, especially for concrete / long\nqueries.\nQuery: religious\nPrompt: religious. a \npainting of a clock \nwith a cross on it.\nQuery: an abstract painting of \nthe Empire State Building\nPrompt: an abstract painting \nof an empire state building \nand a clock, an illustration \nof by Hugh Ferriss.\nQuery: art nouveau\nPrompt: art nouveau. a \npink building with a \nclock tower.\nQuery: art nouveau\nPrompt: art nouveau. a \npink building with a \nclock tower.\nQuery: art nouveau\nPrompt: art nouveau. a \npink building with a \nlot of green plants.\nQuery: art nouveau\nPrompt: art nouveau. a \npink building with a \ngolden dome.\nOverfitting across queries\nOverfitting across prompts\n(b) Overfitting from fine-tuning: Qualitatively supporting the findings from Table 7, Prompt Tuning alleviates\ncertain challenges that Fine Tuning (end-to-end) may present. We also observe overfitting to the training\ndistribution, where tokens (e.g. clock, pink building) are repeated, regardless of same or different query.\nF\nFIXED/LONG LENGTH IS THE OPTIMAL PROMPT LENGTH COMPARED TO\nVARIABLE/SHORT LENGTH\nIn Table 5, we ablate the sequence length of the expected output prompt. We do this by comparing\ntwo query-prompt mapping settings. When we few-shot prompt a prompt p to a (shorter) query qn,\nwhere n is the nth-order query, we then few-shot prompt the query qn to a (shorter) query qn\u22121. We\nrepeat this till query q1 , until we have a set of n queries mapped to each p. While Prompt Expansion\n23\nmaps each qi 7\u2192 p for every i \u2208 n, PE: step-by-step expansion maps each (shorter) query qi to\nthe next (longer) query qi+1 such that qi 7\u2192 qi+1.\nThough the variance of output sequence length between PE: step-by-step expansion and Prompt\nExpansion is similar, the mean length is much higher for the latter than the former. We find that PE:\nstep-by-step expansion consistently underperforms Prompt Expansion across varying query\ntypes across all metrics. Qualitatively we also find that the step-by-step expansion setup also results\nin drop of query details, as shown in Figure 13(left).\nTable 7: Evaluating different model configurations, namely model size and parameter tuning method.\nMethod\nAesthetics (MUSIQ-AVA) \u2191\nText-Image Alignment (COCA(q, I(p))) \u2191\nDiversity (\u03c3p)) \u2191\nRepetition Rate (r)) \u2193\nFine-Tune, 1B\n5.282 \u00b1 0.570\n0.122 \u00b1 0.0170\n0.00621 \u00b1 0.00297\n0.0101\nFine-Tune, 24B\n5.411 \u00b1 0.622\n0.1216 \u00b1 0.0166\n0.00657 \u00b1 0.00312\n0.112\nPrompt-Tune, 1B\n5.225 \u00b1 0.585\n0.120 \u00b1 0.0175\n0.00720 \u00b1 0.00341\n0.00285\nPrompt-Tune, 24B\n5.335 \u00b1 0.625\n0.119 \u00b1 0.0177\n0.00702 \u00b1 0.00331\n0.00715\nTable 8: Ablations for diversity in Prompt Expansion and its subsequent text-to-image generation.\nMethod\nHyperparameters\nAesthetics (MUSIQ-AVA) \u2191\nText-Image Alignment (COCA(q, I(p))) \u2191\nDiversity (\u03c3p)) \u2191\nStraight-Query Gen.\n4 random seeds\n5.121 \u00b1 0.519\n0.125 \u00b1 0.0147\n0.00582 \u00b1 0.00275\nPrompt Expansion\nTemperature = 0.1\n5.267 \u00b1 0.534\n0.119 \u00b1 0.0177\n0.00702 \u00b1 0.00331\nTemperature = 0.5\n5.283 \u00b1 0.5266\n0.116 \u00b1 0.0215\n0.00683 \u00b1 0.00326\nTemperature = 1.0\n5.225 \u00b1 0.585\n0.120 \u00b1 0.0175\n0.00720 \u00b1 0.00341\nPrompt Expansion\ndecoder=greedy\n5.294 \u00b1 0.538\n0.115 \u00b1 0.0213\n0.0 \u00b1 0.0\ndecoder=beam search\n5.0839 \u00b1 0.550\n0.118 \u00b1 0.0211\n0.00426 \u00b1 0.00332\nPrompt Expansion\n+ text post-hoc filtering\n5.318 \u00b1 0.542\n0.114 \u00b10.0216\n0.00726 \u00b10.00346\n+ image post-hoc filtering\n5.316 \u00b10.562\n0.113 \u00b10.0215\n0.00739 \u00b10.00350\nG\nDIVERSITY ABLATIONS\nWe further evaluate diversity in the output images compared to other ablation methods in Table 8.\nThe baseline is Straight-Query Generation, which generates 4 images with 4 random seeds.\nDecoding. Prompt Expansion uses temperature-based decoding, with default temperature being\n1.0. The first set of ablations is varying temperature between 0 to 1. A higher temperature tends\nto result in more diverse output for text models using temperature-based decoding. The next set of\nablations is changing the decoding method, specifically greedy decoding and beam search decoding\n(with beam size 4). While beam search can return 4 expanded prompts per query, greedy decoding\nreturns 1 expanded prompt per query. For the latter, this means that the embedding variance of 1\nimage generated per expanded prompt \u03c3p is 0.0.\nPost-hoc filtering. Our final set of ablations is to maximize diversity after text/image generation.\nSpecifically, post-hoc filtering aims to generate a large number (N=20) of prompts/images, and filter\nout 4 that return the highest combinatorial diversity score. For post-hoc text filtering, we return 20\nprompts through Prompt Expansion, compute the COCA text embeddings for each prompt, then\nenumerate through each combination of 4 prompts, and the selected 4 prompts is the combination\nthat has the highest variance in text embeddings. For post-hoc image filtering, we return 20 prompts\nthrough Prompt Expansion and generate 1 image per prompt, compute the COCA image embeddings\nfor each prompt, then enumerate through each combination of 4 images, and the selected 4 images is\nthe combination that has the highest variance in image embeddings.\n24\nH\nHUMAN RATER TASK\nWe retain the queries (n=700) and expanded prompts used in the automatic evaluation, and evaluate\nthem with human raters. We perform a side-by-side (SxS) evaluation task, where we show a rater a\nset of images side by side for a given text query, and ask the rater to pick the best image. We split\nthe 700 queries into two sets of 350 for 1x1 and 4x4 settings. Each instance was evaluated by 3\nhuman raters, with the strong consensus (Table 9) indicating consistency in preferences between\nraters. Figure 15 shows the annotation interface for the rater experiment.\nAesthetics. In this setting, raters are first provided instructions on how to evaluate Aesthetics. We\ndescribe it as follows:\nIn this task, please select the image that you personally prefer. You may find this\nimage more appealing due to aesthetic, stylistic or compositional qualities. Please\nonly evaluate this image for aesthetic appeal, and ignore all other aspects (overall\nimage quality).\nWe provide examples of one image of high aesthetic appeal side-by-side with another image of low\naesthetic appeal. In addition to positive examples, we provide examples of images not to reject due to\nflaws in image quality.\nPlease only evaluate this image for aesthetic appeal, and ignore all other aspects\n(e.g. overall image quality). Image quality measures the visual flaws of the image.\nExamples of visual flaws include, watermarks, illegible text, hallucinated content,\nwarped features or objects, blurriness, and out of place objects. If these flaws\nappear, this does not mean the image must be rejected. Only reject the image if the\nother image is one that you personally prefer (e.g. more appealing due to aesthetic,\nstylistic or compositional, but not image quality).\nOnce the rater is acclimated with the metric, we proceed with the task and can ask them:\nQ: Which image do you personally prefer? (aesthetically, stylistically, or composi-\ntonally).\nText-Image Alignment. We provide positive examples of images more aligned to the text description\n(query) compared to the other side-by-side image. We similarly provide negative examples of images\nnot pertaining to text-image alignment. We describe the task to the rater as follows:\nPlease only evaluate this question for consistency, and ignore all other aspects of\nthe image (image quality, or overall image preference).\nFurthermore, we add an option Unsure when the rater perceives all images as equally likely to be\npicked. Specifically, we describe to the raters to select this option as follows:\nSelect Unsure if:\n\u2022 You do not understand the text prompt / it is unclear\n\u2022 Both images are equally consistent\n\u2022 Both images are equally inconsistent\nOnce the rater is acclimated with the metric, we proceed with the task and can ask them:\nQ: Which image is more consistent with the text description?\nRandom vs. Random (1x1). Can we attribute aesthetic preference of prompt expanded images over\nstraight-query images to improved prompt construction / engineering? To answer this question, we\nperform a one-to-one side-by-side (1x1 SxS) comparison, where we pick the first image generated\nfrom straight-query generation and the first image generated from Prompt Expansion, then ask the\nraters to evaluate which one is better. This tests the ability of Prompt Expansion to reduce the need for\nmanual prompt engineering by the user. For both Aesthetics and Consistency, raters have 700 discrete\nrating decisions (we split to n=350, going through 1,400 images). The raters can pick between the\n25\nFigure 14: An overview of the 1x1 and NxN rater experiments. A Query vs Prompt evaluation always\ntakes place, but an NxN experiment requires a set of 4 images to be evaluated against each other\nbefore the 1x1 experiment.\nRater  \nComputer \nscreen \nQuery\nUser feedback\nImage \nImage \nUser feedback\nImage \nImage \nUser feedback\n4x Query images\n4x Prompt images\n1x Query \nvs \n1x Prompt\nImage \nImage \nImage \nImage \nImage \nImage \n1.\n1x1 SxS, 1 from query, 1 \nfrom expanded prompt\n2.\nNxN SxS, first provide N \nfrom query to 1 set of raters, \nthen N from expanded \nprompt to 1 set of raters, \neach set provide best image \nfrom their N set, final set of \nraters perform 1x1 on best \nimage from each set\nTable 9: Consensus between Raters: This details how many of the raters agree on an each Prompt-win\nimage (i.e. number of raters that all picked the same Prompt-win image).\nPrompt-win Consensus\nConsensus Agreement (3/3)\nConsensus Agreement (2/3)\nConsensus Agreement (0/3)\n1x1 Aesthetics\n0.450 \u00b1 0.0103\n0.549 \u00b1 0.0103\n0.0 \u00b1 0.0\n4x4 Aesthetics\n0.396 \u00b1 0.0167\n0.604 \u00b1 0.0167\n0.0 \u00b1 0.0\n1x1 Text-Image Alignment\n0.513 \u00b1 0.146\n0.446 \u00b1 0.117\n0.0407 \u00b1 0.0288\n4x4 Text-Image Alignment\n0.486 \u00b1 0.150\n0.481 \u00b1 0.128\n0.0333 \u00b1 0.0259\noptions: Left, Right, (Unsure). For the consistency task, we provide the Unsure option in case two\nimages are equally good (or bad), since text-image alignment can be objectively evaluated. For\naesthetics, we do not provide the Unsure option to make their aesthetic preference.\nBest vs. Best (4x4). Can we attribute aesthetic preference of prompt expanded images over straight-\nquery images to increased diversity of samples? To answer this question, we perform a N-to-N\nside-by-side (NxN SxS) comparison followed by another 1x1 SxS comparison. We begin by showing\nthe rater N=4 images from straight-query generation. We also show the rater N=4 images from\nPrompt Expansion. The raters are not shown straight-query generation and Prompt Expansion images\nin separate stages; the 4x4 SxS is in one stage, and the rating decisions are shuffled. The raters pick\nthe best image out of N=4. The raters can pick between the options: 1, 2, 3, 4, (Unsure). Then\nthe raters enter a second stage, where they are shown the best straight-query image against the best\nPrompt Expansion image for the same query, and asked to select the best. The raters can pick between\nthe options: Left, Right, (Unsure). For both Aesthetics and Consistency, raters have 2,100 discrete\nrating decisions (we split to a distinctly-different N=350, going through 7,000 images).\n26\n(a) 1x1. The question here is on aesthetics. No query is shown to the user.\n(b) 4x4. The question here is on text-image alignment. The query is shown to the user.\nFigure 15: The user interface for our human rater experiments.\n27\nI\nDATASETS\nSeveral datasets varying by modality, training-evaluation use, captioned/non-captioned, etc are used\nthroughout the paper. We provide further details on all of them here.\nWebli-Align (Jia et al., 2021; Chen et al., 2023).\nWebli-Align is composed of two existing\ndatasets Webli (Chen et al., 2023) and Align (Jia et al., 2021). We filter out non-aesthetic images in\nWebli-Align with the MUSIQ (Ke et al., 2021) aesthetics metric (refer to Sec 6). The filtered dataset\ncontains 80k images. We also perform a 70-20-10 train-val-test split. Within the train set, we split\n50-50 for base and re-fine-tune training.\nCrowdSourced. CrowdSourced is obtained through crowd-sourcing, where we provide an interface\nfor text-to-image generation (similar to Gradio), and users enter queries to generate images. Users also\nhave the option to upvote images that they liked, and we use this to filter for aesthetically-appealing\nimages. An example of an upvoted, aesthetically-appealing image is shown in Figure 6. The filtered\ndataset contains 40k images. We also perform a 70-20-10 train-val-test split. Within the train set, we\nsplit 50-50 for base and re-fine-tune training.\nBase Prompt Expansion Dataset.\nThe procedure for generating the base dataset is detailed in\nSection 3.\nRe-fine-tune Prompt Expansion Dataset. From the queries of the held-out set from the base dataset,\nwe compute the expanded prompts with the base Prompt Expansion model (4 prompts per query).\nWe discard the original prompt mapped to the query, and replace it with the new expanded prompts.\nFor each expanded prompt, we compute the generated image. From this probing set, we score the\nimages in order to rank the images based on renderability with respect to its prompt text. Our scoring\nmetric is a weighted average of the query-image embedding distance and prompt-image embedding\ndistance: 0.6 \u00d7 COCA(q, I(p)) + 0.4 \u00d7 COCA(p, I(p)). We filter out query-prompt-image pairs\nthat are below a threshold. This filtered {query:prompt} dataset is thus used for re-fine-tuning.\nWhile other scoring tools such as using MUSIQ scores or measuring embedding distance with respect\nto common image defects (e.g. distorted hands/faces) were also tested, we qualitatively find that\nthis weighted average as a general heuristic can sufficiently filter out images that are non-renderable,\nunaligned to neither query nor prompt, and non-aesthetic. We also avoid completely filtering out\nevery prompts/flavor that do not work. If the user intentionally requests for specific flavors / objects,\nwe would need to generate expanded prompts that still fit the query.\n(Evaluation) Webli-Align (WA) (Jia et al., 2021; Chen et al., 2023). We sample n=500 queries\nfrom a test set of potential queries constructed by applying the PE dataset generation process of\nSection 3 (Image-to-Text Inversion + Query/Prompt Extraction) to Webli-Align (WA) (Chen et al.,\n2023; Jia et al., 2021) images. Queries are categorized based on whether they are abstract (WA=249),\nconcrete (WA=251), short (<4 words) (WA=224), medium length (4-7 words) (WA=134), or long\nlength (>7 words) (WA=143).\n(Evaluation) PartiPrompts (PP) (Yu et al., 2022b).\nWe sample n=200 prompts from the Par-\ntiPrompts (PP) dataset of prompts that is designed to represent different domains and features of\nlanguage (such as counting, negation, etc). Queries are categorized based on whether they were short\n(<4 words) (PP=35), medium length (4-7 words) (PP=66), or long length (>7 words) (PP=66).\n28\nJ\nCOCA-INTERROGATOR\nThe Interrogator (CLIP-Interrogator) approach to image-to-text inversion requires an image-text\nmodel to probe what set of words and phrases are most similar to a given image, and concatenate\nthe words/phrases together as the prompt text. The prompt text computed through Interrogators are\ncomposed of (i) a caption, and (ii) a set of flavors. The caption is a description of the content of the\nimage (e.g. who, what, where, when).\nWe detail the implementation steps for COCA-Interrogator. First, it requires an image captioning\nmodel (COCA fine-tuned on captioning (Yu et al., 2022a)) to compute the image captions. Then, it\ncomputes flavors pertaining to the image, and appends them to the caption. The distance between\nthe text embedding of the flavor and image embedding of the image is used to measure how likely\nthe image manifests the given flavor. To curate the lists of flavors, we first aggregate a large number\nof prompts written by users to generate images for text-to-image models. We split the prompts into\nwords and phrases, measure its COCA distance w.r.t. the corresponding generated image, and filter\nout infrequent flavors. Flavors are categorized by art form (e.g. vector art), artist (e.g. Maurycy\nGottlieb), medium (e.g. reddit contest winner), style (e.g. neo-primitivism, photorealistic), and other\ncommonly-used phrases to prompt engineer images (e.g. monster manula, glamor pose, matte finish).\nFrom each potential flavor, we compute the text embeddings, and measure the cosine similarity with\nrespect to the image embeddings of the image. Both text and image embeddings are computed with\nCOCA (Yu et al., 2022a). We enumerate through each flavor across the categories to identify the\nset of flavors that maximize similarity. Flavors are organized by categories, and we ensure that at\nleast one flavor from each category is included. Though COCA-Interrogator is used here, other\nimage-to-text inversion methods can be used in-place, such as CLIP-Interrogator (CLIP-Interrogator)\nor PEZ Dispenser (Wen et al., 2023). We pursue the interrogator approach to inversion, given the\nhigh interpretability / human readability of the output, and because it enables us to explicitly include\nprompt-engineering flavors that govern the style and tone of the generated image.\n29\nneon wallpaper\n(a) Straight-Query Generation\nA neon wallpaper depicting a surfer girl. \nThe surfer girl is surfing in the ocean in \nbright sunlight. fauvism.\na neon wallpaper of a cat running \nthrough a city at night. kinetic \npointillism.\nThe room in which you are is lit from \nabove by brightly shining neon tubes. \nThe neon lights are bright enough to \nmake it seem like night. The space is \nilluminated both from above and from \nbelow.\na neon wallpaper that changes colors\naround dusk. the wallpaper changes \ncolors around sunset to show off a \nnighttimecityscape, and it may be a \ndigital rendition of a city scene or a real-\nworld cityscape in color. pointillism.\n(b) Prompt Expansion\nA  neon wallpaper depicting a sunset \nwith billowing clouds and the sea. The \nweather is cloudy and breezy, and the \nsun is peeking through the clouds. The \noverall mood of the image is romantic \nand dreamy.\na neon skylight over a city street, \nproviding beautiful and inviting \nlighting for the night.\na large neon wall covering the entire \ninside of the room. The room is filled \nwith sunlight, with only a faint strip of \ndarkness illuminating the room from \nabove. The room exudes energy and \ndynamism, and the user feels inspired.\nA wall in a subway station with neon \nlights shining above it. The wallpaper is \nblack and white, highlighting the details \nof the subway car and the artwork on it. \nThe wallpaper adds an industrial feel to \nthe shot.\n(c) Prompt Expansion (Re-fine-tuned)\nFigure 16: We compare images generated between Prompt Expansion and PE (Re-fine-tuned). Both explore different interpretations of the query, e.g.\nposter, city environments, rooms. The re-fine-tuned model explores more types of locations and types of neon.\n30\nA robot couple fine dining with Eiffel Tower in the background.\nA robot couple fine dining with Eiffel \nTower in the background.. A robot \ncouple is dining at a restaurant. It's night \ntime, and the couple is making love while \nusing the restaurant's kitchen as their \nbackdrop. gothic art.\nA robot couple fine dining with Eiffel \nTower in the background.. A robot \ncouple of art lovers, in an ironic, avant-\ngarde sort of romantic scene, sharing \na glass of wine and savoringa tasty \npastry in romantic Eiffel Tower. \nmodern europeanink painting.\nA robot couple fine dining with Eiffel \nTower in the background.. A robot \ncouple fine dining at a fancy restaurant. \nThe couple is eating and drinking at their \ntable while gazing at the Eiffel Tower in \nthe background. kitsch movement.\nThe painting shows a robot couple fine \ndining with Eiffel Tower in the \nbackground. It shows a closeup of the \nfaces of the couple, so we can see a lot \nof emotion in their eyes. The painting is \nin black and white, but has a vibrant \nyellow splashy background that gives it a \npop of color.\nFigure 17: Concrete / Long queries: Using a query from PartiPrompts, we find both retainment and faithfulness to the original query, as well as\nstylistic variations in the subjects and environment.\nA red cube beside a smaller yellow sphere.\nA red cube next to a smaller yellow \nsphere. The sphere is emitting rays of \nlight, causing it to glow green and yellow.\nA close-up of a red sphere in the sky \nsurrounded by green plants and \ngreen trees. The sphere is floating in a \ngrassy field. The setting is a rural \nsetting in spring.\nA red cube beside a smaller yellow \nsphere is formed by dropping a larger \nyellow sphere and breaking it. fine art.\nA close up of a sculpture of a red sphere \nbeside a smaller yellow sphere, the \nsphere in a spotlight. The focus is on the \nart, but the colors of the art are also \nshown and the sculpture is in the \nbackground. kinetic pointillism.\nFigure 18: Concrete queries: Using another query from PartiPrompts, we find both retainment and faithfulness to the original query (e.g. size and\ncolors of the objects), as well as variations in the characteristics of the objects and environment.\n31\n90s science wallpaper\nAn old science wallpaper of a space \nshuttle drifting through space. The \nspace shuttle is in space for many days \nwithout leaving orbit, and is drifting \nthrough space without seeing any other \nspacecraft. crayon art.\nA retro wallpaper that shows a sci-fi \nscience lab. The lab is colorful, full of \nstrange apparatuses and gadgets. The \nlab is futuristic and looks like a piece \nfrom some kind of dreamscape. \ntrending on cg society.\nA 90s science themed room with a \nspaceship, and a starry sky, and a space \nstation. The room is at a science \nmuseum.\nA close-up of a science book printed flat \non a white background. The book opens \nto reveal an intricate illustration of an \nalien creature sitting on a glowing \nglowing sphere. featured on behance.\nFigure 19: Navigational / ideation queries: The interpretation of wallpaper can vary by user intent. As a result, Prompt Expansion returns images\nacross different mediums (e.g. poster wallpapers, room wallpapers). It also shows a variation of aesthetic styles referencing the 90s.\njack o lantern designs\nA portrait of a black-clad Jack O' \nLantern with glowing eyes, \nwearing a mask, holding a lantern, \nand sitting at a campfire, \nsurrounded by the stars of the \nnight sky. psychedelic art.\nA giant jack o lantern built into a \ngiant red glowing pyramid on a hill \nin the shape of a face. The jack o' \nlantern is glowing with both an \ninner light and an outer light. \nkinetic pointillism.\nA rendering of a jack o lantern \ndesign from various cultures. The \ndesign combines traditional \nelements from many different \ncultures, lending the image an \neclectic, cosmopolitan feel.\nA series of photographs of lanterns \nin the form of animals, and some of \nthem are very anthropomorphic. The \noverall image is playful and inviting, \nand the relevant keywords are: \nchildlike, optimistic, energetic.\nFigure 20: Navigational / ideation queries: These types of abstract queries are intended to be exploratory, and the user does not have a specific image\nin mind necessarily. They are looking for \"designs\", so showing as many variations would improve the user experience.\n32\n"
  },
  {
    "title": "Restoration by Generation with Constrained Priors",
    "link": "https://arxiv.org/pdf/2312.17161.pdf",
    "upvote": "3",
    "text": "Restoration by Generation with Constrained Priors\nZheng Ding1\u2020, Xuaner Zhang2, Zhuowen Tu1, Zhihao Xia2\n1UC San Diego\n2Adobe\nPersonal Album\nGenerat ive Album\nAnchor \nInput\nCodeFormer\nOurs\nGenerate\nInput\nAnchor\nCodeFormer [60]\nOurs\nFigure 1. We harness the generative capacity of a diffusion model for image restoration. By constraining the generative space with a\ngenerative or personal album, we can directly use a pre-trained diffusion model to produce a high-quality and realistic image that is also\nfaithful to the input identity. Without any assumption on the degradation type, we are able to generalize to real-world images that exhibit\ncomplicated degradation. We compare our restoration result with CodeFormer, a state-of-the-art baseline [60]. Our method generalizes\nbetter to different types of degradation while more faithfully preserving the input identity. Images are best viewed zoomed in on a big screen.\nAbstract\nThe inherent generative power of denoising diffusion mod-\nels makes them well-suited for image restoration tasks where\nthe objective is to find the optimal high-quality image within\nthe generative space that closely resembles the input im-\nage. We propose a method to adapt a pretrained diffusion\nmodel for image restoration by simply adding noise to the\ninput image to be restored and then denoise. Our method\nis based on the observation that the space of a generative\nmodel needs to be constrained. We impose this constraint by\nfinetuning the generative model with a set of anchor images\nthat capture the characteristics of the input image. With the\nconstrained space, we can then leverage the sampling strat-\negy used for generation to do image restoration. We evaluate\n\u2020 Work done during an internship at Adobe.\nagainst previous methods and show superior performances\non multiple real-world restoration datasets in preserving\nidentity and image quality. We also demonstrate an impor-\ntant and practical application on personalized restoration,\nwhere we use a personal album as the anchor images to\nconstrain the generative space. This approach allows us\nto produce results that accurately preserve high-frequency\ndetails, which previous works are unable to do. Project\nwebpage: https://gen2res.github.io.\n1. Introduction\nImage restoration involves recovering a high-quality nat-\nural image x from its degraded observation y = H(x) is a\nfundamental task in low-level vision. The challenge lies in\nfinding a solution that 1) matches the observation through a\nset of degradation steps; and 2) aligns with the distribution of\n1\narXiv:2312.17161v1  [cs.CV]  28 Dec 2023\nx. In scenarios where the degradation process H is unknown,\nthe problem becomes a blind image restoration problem.\nDiscriminative learning approaches [12, 43, 52, 60] aim\nto solve this inverse problem directly by training an inverse\nmodel F(y), typically a neural network, using datasets of\nlow- and high-quality image pairs (x, y). However, the\ntrained model is limited to restoring images with degrada-\ntions H present in the training set. This limitation places the\nburden of generalization on the construction of the training\nset. The effectiveness of these methods also heavily depends\non the capacity of the inversion model and the characteris-\ntics of the loss function. Model-based optimization methods\n[5, 21, 33, 34, 57], on the other hand, assume that the degra-\ndation model is only known at inference time. They focus\non learning the image prior p(x), which can be represented\nas regularization terms [34], denoising networks [33, 55], or\nmore recently pre-trained diffusion models [5, 21]. However,\nthese methods generally assume that the degradation process\nis known at inference time, limiting their practicality and\noften relegating them to synthetic evaluations.\nIn this paper, we adopt a markedly different approach\nto the image restoration problem. We observe that humans\nare able to recognize a degraded image (i.e., a \u2018bad photo\u2019)\nand envision a fix without knowing the imperfections in the\nimage formation process. Such insights rely on our inher-\nent understanding of what constitutes a high-quality image.\nBuilding on this observation, we propose to approach im-\nage restoration using the recent success of large generative\nmodels, which possess the capacity of forming high-quality\nimagery. Unlike prior works, we do not make any assump-\ntion on the degradation process. Our method solely relies on\na well-trained denoising diffusion model.\nThe challenges then arise in how to project the input im-\nage into the generative process given the models are trained\non mostly clean images. And once projected, how to con-\nstrain the generation to preserve the useful features in the\ninput, e.g., the identity. We address the input projection\nby adding Gaussian noise to the low-quality image to be\nrestored, matching the distribution of clean images added\nwith noise. Once projected, we can then denoise the image\nas is normally done in the generation process of a diffusion\nmodel. To handle the second challenge of preserving useful\nsignals in the input, we propose to constrain the generative\nspace by finetuning the model with anchor images that share\ncharacteristic features with the low-quality input. When the\nanchor is given, such as from an album of other photos of\nthe same identity, we can simply finetune the model with the\nprovided images. When the anchor is missing, as in most\nsingle-image restoration scenarios, we propose to use a gen-\nerative album as the anchor. The generative album is a set\nof clean images generated from the diffusion model with the\nlow-quality input image imposing soft guidance, and thus\nclosely resembles the input image.\n...\nDk\nDt\nDt -1\nDt\nDt -1\nLQ\nHQ\nAnchor\nxK\nyK\nyt -1\nyt -1\nW/ Space Const raining\nW/O Space Const raining\nxK\nyK\ny0\n...\nx0\nFigure 2. Left: Image projection. When sufficient Gaussian\nnoise is added to the low- and high-quality image, we can bring\nthem to the same distribution. The low-quality image can thus be\ndenoised with a pre-trained diffusion model. Right: With and\nwithout space constraining. A regular diffusion step lands yt in an\narbitrary position in the generative space; with space constraining,\nthe path of generation becomes more constrained towards the space\ndefined by the anchor images.\nSurprisingly, we find that our straightforward approach\nyields high-quality results on blind image restoration. Un-\nlike previous methods, our approach does not rely on paired\ntraining data or assumptions about the degradation process.\nIt thus generalizes well to real-world images with unknown\ndegradation types, such as noise, motion blur, and low reso-\nlution. By effectively harnessing the generative capacity of\na pre-trained diffusion model, our generation-based restora-\ntion approach produces high-quality and realistic images that\nare faithful to the input identity.\n2. Related Works\nSupervised Learning for Image Restoration.\nThe trend\nof leveraging advanced neural network architectures for im-\nage restoration has spanned from CNNs [2, 41, 54, 56, 58]\nto GANs [24, 25, 27], and more recently, to transformers\n[30, 46, 53] and diffusion models [36, 37, 48]. One aspect re-\nmains unchanged: these methods are trained on datasets com-\nprising pairs of high-quality and low-quality images. Typi-\ncally, these image pairs are synthetically generated, depicting\na single type of degradation, leading to task-specific models\nfor denoising [13, 41, 49, 54, 56], deblurring [2, 24, 25, 48],\nor super-resolution [27, 37, 44]. However, they fall short\nwhen applied to real-world low-quality images, which often\nsuffer from diverse, unknown degradations.\nIn specific domains, particularly with facial images, nu-\nmerous works have focused on training blind restoration\nmodels that simulate various degradation types during train-\ning. For instance, GFPGAN [43] and GPEN [52] enhance\npretrained GAN networks with modules to leverage genera-\ntive priors for blind face restoration. Recent approaches like\nCodeFormer [60], VQFR [12] and RestoreFormer[53] ex-\nploit the low-dimensional space of facial images to achieve\nimpressive results. Emerging works have also started build-\n2\ning upon the success of diffusion models [7, 15, 39]. For\nexample, IDM [59] trains a conditional diffusion model for\nface image restoration by injecting low-quality images at dif-\nferent layers of the model. Conversely, DR2 [47] combines\nthe generative capabilities of pre-trained diffusion models\nwith existing face restoration networks. Another line of\nworks [28, 29] seeks to enhance the results by incorporating\nadditional information present in a guide image or photo\nalbum, which is often available in practice. Nevertheless,\nthese methods rely on a synthetic data pipeline for training,\nwhich limits their generalizability. Diverging from these\nmethodologies, our approach does not use paired data, syn-\nthetic or real, allowing it to generalize naturally to real data\nwithout succumbing to artifacts.\nModel-based Image Restoration.\nUnlike supervised\nlearning methods, model-based methods often form a poste-\nrior of the underlying clean image given the degraded image,\nwith a likelihood term from the degradation process and an\nimage prior. [33, 57] proposed using denoising networks as\nthe image prior. These priors are integrated with the known\ndegradation process during inference, and the Maximum A\nPosteriori (MAP) problem is addressed through approximate\niterative optimization methods. DGP [32] proposes image\nrestoration through GAN inversion, searching for a latent\ncode that generates an image closely matching the input im-\nage after processing it through the known degradation. The\nrecent success of pre-trained foundational diffusion models\nhas inspired works [3, 17, 19, 20] to utilize diffusion models\nas such priors. Kawar et al. [21] and Wang et al. [45] pro-\nposed an unsupervised posterior sampling method using a\npre-trained denoising diffusion model to solve linear inverse\nproblems. Chung et al. [5] extends diffusion solvers to gen-\neral noise inverse problems. Despite these advancements,\nthese methods generally assume that the degradation process\nis known at inference, limiting their practicality to synthetic\nevaluations. In contrast, our method does not assume any\nknowledge of the degradation model at training or inference.\nPersonalized Diffusion Models.\nPersonalization methods\naim to adapt pre-trained diffusion models to specific subjects\nor concepts by leveraging data unique to the target case. In\ntext-to-image synthesis, many works opt for customization\nby fine-tuning with personalized data, adapting token em-\nbeddings of visual concepts [10, 11], the entire denoising\nnetwork [35], or a subset of the network [23]. Recent studies\n[16, 38, 50] propose bypassing per-object optimization by\ntraining an encoder to extract embeddings of subject identity\nand injecting them into the diffusion model\u2019s sampling pro-\ncess. In other domains, DiffusionRig [9] learns personalized\nfacial editing by fine-tuning a 3D-aware diffusion model\non a personal album. In this work, we demonstrate that a\npersonalized diffusion model represents a constrained gener-\native space, directly usable for sampling high-quality images\nto restore images of a specific subject, without additional\ncomplexities. For single-image restoration, unlike previ-\nous instance-based personalization methods [16, 38, 50], we\ngenerate an album of images close to the input and then\nconstrain the diffusion model using this generative album.\nThis approach enables restoration by directly sampling from\nthe fine-tuned model, eliminating the need for guidance.\n3. Method\n3.1. Preliminaries\nA diffusion model approximates its training image distri-\nbution p\u03b8(x0) by learning a model \u03b8 that effectively reverses\nthe process of adding noise. The commonly used Denoising\nDiffusion Probabilistic Models (DDPM) gradually introduce\nGaussian noise into a clean image x0:\nxt = \u221a\u03b1tx0 +\n\u221a\n1 \u2212 \u03b1t\u03f5,\nwhere\n\u03f5 \u223c N(0, I)\n(1)\nThe reverse generative process aims to progressively denoise\nxt until it is free from noise. Once a diffusion model is\ntrained, for any given time t and the corresponding noisy im-\nage xt, it can iteratively denoise by sampling from p(x0|xt)\nusing the trained model.\nThe objective of image restoration, on the other hand,\nis to recover the latent high-quality image x0 from a low-\nquality, partially observed image y0. Contrary to previous\nmethods that decompose the posterior distribution into the\nlikelihood p(y0|x0) and the prior p(x0) to solve a MAP\nproblem, we propose to recover the complete observation by\ndirectly sampling from the posterior:\n\u02c6x \u223c p(x0|y0)\n(2)\n3.2. Restoration by Generation\nWe aim to maximally leverage the generative capacity\nof the diffusion model by using its iterative sampling pro-\ncess for restoration. A critical observation underlies this\napproach: when sufficient Gaussian noise is added to the\ndegraded observation y0, the resultant image yt:\nyt = \u221a\u03b1ty0 +\n\u221a\n1 \u2212 \u03b1t\u03f5,\nwhere\n\u03f5 \u223c N(0, I)\n(3)\nbecomes indistinguishable from the underlying clean image\nx0 with the same noise. That is, there exists a large enough\nK such that\nyK \u2248 xK\n(4)\nThis phenomenon becomes apparent from Eq 1 and 3 as \u03b1\ndecreases and when the same noise \u03f5 is sampled. It is also\ndemonstrated in Fig 2, where adding noise to high-quality\nand low-quality images brings them to the same distributions.\nCrucially, this suggests:\np(x0|yK) \u2248 p(x0|xK)\n(5)\nBased on this observation, we can sample a clean image x0\nfrom p(x0|yK) using the same sampling process as from\n3\nPersonal Album\nGenerat ive Album\nXk-1\nXk-n\n. . .\nX0\n. . .\nXk-2\nOR\nSkip Guidance\ny0\ny0\n...\ny0\nX'k-1\nDDPM\nyk\nx0\nDDPM\nInference\nGenerat ive Space Const raining (Finet uning)\nyk\nXk-n-1\nX'k-n\nFigure 3. An illustration of our finetuning and inference stage. The core of our method is to constrain the generative space by fine-tuning a\npre-trained diffusion model with either a generative album or a personal album. The generative album is generated from the input low-quality\nimage with skip guidance to loosely follow the characteristics of the input. Once the generative space is constrained, at inference time, we\ncan simply add noise to the input low-quality image and pass it through the diffusion model to do restoration.\np(x0|xK); in other words, we can denoise yK iteratively\ndirectly with the pre-trained diffusion model. Since the\nsampling process remains unchanged, the resultant image\nshould match the quality of the images generated from the\noriginal diffusion model.\nWe find it critical to select the optimal time K, which\ndetermines the amount of noise added to the low-quality in-\nput image to start the sampling process. If too little noise is\nadded, the discrepancy between xK and yK becomes large,\nyielding low-quality samples as yK does not align with the\ntraining distribution p(xK) of the diffusion model. On the\nother hand, with too excessive noise added, the original con-\ntents in the input yK are hardly discernible. The generated\nsample, though with high quality, will not be faithful to the\ninput. We aim to produce high-quality samples, while miti-\ngating the information loss, and achieve so by constraining\nthe generative space of the pre-trained diffusion model.\n3.3. Generative Space Constraining\nThe loss of information is inherent in the diffusion pro-\ncess. Due to the stochasticity of the forward Markov chain,\nthe clean image generated using the reverse process from\nxt may not match the original x0. The larger t is, the larger\nthe generative space p(x0|xt) spans. The learned score func-\ntions guide xt to the clean image space without constraining\nits content. This property is desirable for a generative model\nwhere the diversity of generation is valued. However, this\nis not ideal for image restoration where the input contents\nalso need to be preserved. The goal is thus to constrain the\ngenerative space to a small subspace that tightly surrounds\nthe underlying clean image.\nWe propose to use a set of anchor images to fine-tune\nthe diffusion model, thus imposing the generative space.\nThese anchor images can be given in the form of a personal\nalbum, or be generated as a generative album in the common\nscenario of single image restoration.\nPersonal Album as Additional Information.\nIn many\nreal-world scenarios, additional information about the un-\nderlying clean image beyond a single degraded observation\nis available, such as an album of different clean images of\nthe same subject. We personalize the pre-trained diffusion\nmodel in this case \u2014 fine-tuning it with the personal al-\nbum. This approach naturally addresses the ill-posed nature\nof single-image restoration, producing results containing\nauthentic high-frequency details absent in the degraded ob-\nservation. This is demonstrated in identity preservation in\nface restoration tasks (Sec 4.2).\nGenerative Album from a Single Degraded Observation.\nFor single-image restoration, due to its ill-posed nature, we\ncan only constrain the generative space to a subspace of high-\nquality realistic images close to the degraded observation.\nTo generate this album of high-quality images, we follow\napproaches similar to previous works on guided image gener-\nation [1, 5, 40]. Specifically, given a degraded image y0, we\nfirst add noise \u03f5K to obtain yK, then denoise it progressively\nwith the pre-trained diffusion model. For the denoised image\nxt, we apply a simple L1 guidance that computes the dis-\ntance between the input degraded image and the generated\n4\nWider-Test\nWebPhoto-Test\nLFW-Test\nDeblur-Test\nFID \u2193\nMUSIQ \u2191\nFID \u2193\nMUSIQ \u2191\nFID \u2193\nMUSIQ \u2191\nFID \u2193\nMUSIQ \u2191\nInput\n183.03\n15.68\n161.82\n20.26\n131.68\n27.51\n169.43\n27.53\nGFPGAN[43]\n59.38\n56.48\n114.15\n55.13\n64.10\n60.46\n178.40\n58.03\nCodeFormer[60]\n48.57\n55.70\n98.55\n55.20\n66.31\n58.72\n163.47\n57.09\nVQFR[12]\n52.64\n54.23\n105.94\n52.44\n63.73\n57.52\n168.36\n54.45\nDR2(+VQFR)[47]\n69.40\n53.62\n143.96\n51.92\n67.70\n57.42\n173.33\n55.34\nOurs\n46.38\n58.73\n96.44\n57.71\n56.32\n60.68\n135.33\n60.20\nTable 1. Quantitative comparison on real-world single-image blind face restoration on four datasets.\nimage:\nx\u2032\nt = xt \u2212 \u03bb\u2207xt||y0 \u2212 \u02c6x0,t||2\n2\n(6)\nUnlike previous methods where the guidance needs to be\nstrongly followed, our guidance, the low-quality input, is an\napproximation. Instead of applying the guidance at every\nstep [5, 40], we propose to apply this approximated guidance\nperiodically at every n steps. The proposed Skip Guidance\nenforces the generated image to loosely follow the infor-\nmation in the degraded input while retaining the quality of\nimages in the generative steps. We repeat this process multi-\nple times to generate a set of images that form a generative\nalbum, which is used to fine-tune the diffusion model.\nOnce the diffusion model is fine-tuned with a personal\nor generative album, we restore a degraded image y0 by\nadding noise \u03f5K. Then, we iteratively denoise yK using the\nfine-tuned model for K steps, without further guidance. No-\ntably, our approach does not rely on paired data for training\nand makes no assumptions about the degradation process at\ntraining or inference.\n4. Experiments\nWith the core observation that generation can be di-\nrectly applied for restoration, our method requires only a\npre-trained unconditional diffusion model and is applica-\nble to any image domain for which the diffusion model has\nbeen trained. We first show results of our restoration-by-\ngeneration approach on the standard task of single-image\nblind face restoration in Sec 4.1. In Sec 4.2, we extend our\napproach to personalized face restoration. Here, the objec-\ntive is to restore a degraded image of a subject using other\nclean images of the same identity. Sec 4.3 presents the adap-\ntation of our method to different image categories, such as\ndogs and cats, by simply swapping the pre-trained diffusion\nmodel. Notably, as our method does not presume any spe-\ncific form of degradation, all our evaluations are conducted\non real images with unknown degradation.\n4.1. Blind Face Restoration with Generative Album\nFor the task of single-image blind face restoration, we\nutilize an unconditional diffusion model pretrained on the\nFFHQ dataset [18]. We first assess our approach on three\nwidely-used real-world face benchmarks with degradation\nlevels ranging from heavy to mild: Wider-Test (970 images)\n[60], LFW-Test (1771 images) [43], and Webphoto-Test (407\nimages) [43]. These datasets are collections of in-the-wild\nimages aligned using the method employed in FFHQ [18].\nOur approach uses a generative album as the anchor for\nrestoring these in-the-wild images. For each input low-\nquality image, we generate 16 images with skip guidance to\nform the album. We then fine-tune the diffusion model using\nthis album to constrain the generative space. The process\ninvolves adding noise to the input low-quality image and\ndenoising it for K steps with the fine-tuned model, where\nK = 200. The model is fine-tuned for 3,000 iterations with\na batch size of 4 and a learning rate of 1e-5.\nWe benchmark our method against state-of-the-art super-\nvised alternatives for blind face restoration, including the\nGAN-based GFPGAN [43], two codebook-based approaches\n(Codeformer [60] and VQFR [12]), and a diffusion-based\napproach DR2 [47]. Except for DR2, which combines a dif-\nfusion model with the pretrained supervised face restoration\nmodel VQFR [12], all methods utilize supervised training\nwith synthetic low-quality images from FFHQ.\nQuantitative and qualitative results are provided. For the\nformer, we use FID [14] and MUSIQ(Koniq) [22] as metrics\nfollowing CodeFormer [60]. The quantitative scores are in\nTable 1. Previous methods, except for DR2 [47], are trained\non FFHQ-512\u00d7512 for restoration. For a fair comparison,\nwe downsize the outputs of these methods to 256\u00d7256 for\nmetric calculation. Our results surpass all previous methods\nin terms of FID and MUSIQ across all datasets, despite\nnot undergoing a supervised training approach for image\nrestoration. Qualitative comparisons in Figure 4 illustrate\nthat our method produces high-quality restoration results\nakin to those from an unconditional diffusion model, even\nwith severely degraded input images.\nOur method\u2019s agnosticism to the degradation process\nleads to superior generalization capabilities.\nTo further\ndemonstrate this, we constructed a motion blur dataset\n(Deblur-Test) by selecting 67 images from [26] featuring\nmoderate to severe real motion blur. The synthetic data\npipeline in other supervised approaches does not model\nmotion blur, resulting in poor performance on this out-of-\ndistribution dataset. In contrast, our method consistently\nrestores clean images from complex non-uniform motion\n5\nInput\nGFPGAN\nVQFR\nCodeFormer\nDR2(+VQFR)\nOurs\nFigure 4. Qualitative comparison with baselines on Wider-Test. With strong generative capacity of the diffusion model, our method\nperforms well on severely degraded images. We are able to produce high-quality and realistic images while prior works suffer from\nunrealistic artifacts.\nblur, as seen in Figure 5, outperforming previous methods\nsignificantly, as shown in Table 1.\nInput\nVQFR\nCodeFormer\nOurs\nFigure 5. Comparison with previous methods on Deblur-Test.\nPrevious methods do not include motion blur as part of the degrada-\ntion simulation for training, and thus fail to restore the images. In\ncontrast, our method does not make assumptions on the degradation\ntypes and generalizes more robustly.\n4.2. Personalized Face Restoration\nWe now evaluate our method on personalized restoration.\nGiven a set of clean images of a subject, the goal is to restore\nany degraded image of the same subject using personalized\nfeatures to preserve identity and recover high-frequency de-\ntails that may have been lost in the degraded image. Our\nmethod naturally incorporates the personal album as the\nanchor. We use a personal album that contains around 20\nimages with diversity in pose, hairstyle, accessories, lighting,\netc. We fine-tune on the personal album for 5,000 itera-\ntions. The model can then be used to restore any low-quality\nimages of the same subject through direct sampling.\nWe compare our method against three single-image-based\nworks: Codeformer [60], VQFR [12], DR2 [47], as well as\nan exemplar-based approach ASFFNet [29] which also in-\ncorporates a personal album for additional information. We\nevaluate our approach on three subjects: an elderly woman\n(Subject A), Obama and Hermione. We present the qual-\nitative comparison in Figure 6. Single-image-based meth-\nods struggle to preserve identity \u2013 for example, wrinkles\nand other facial structures are often missing in the results of\nCodeFormer or DR2 for the elderly subject, altering their age\nand identity. By using a photo album as reference, ASFFNet\npreserves identity better, but fails to produce high-quality\nresults. Our method, on the other hand, directly samples\n6\nInput\nCodeFormer\nDR2(+VQFR)\nASFFNet\nOurs\nFigure 6. Qualitative Comparison on personalized face restoration. From top to bottom: Subject A, Obama and Hermione. With a\npersonal album as anchor, we are able to restore images with faithful preservation of the input identity. Previous single-image methods alter\nthe identity with lost details; previous reference-based methods fail to produce high-quality images and are prone to artifacts.\nfrom the personalized generative space to do restoration, and\nthus produces faithful and high-quality results.\nWe also provide quantitative evaluation in Table 2 where\nwe focus on the identity preservation. We use the identity\nscore which uses the cosine similarity of the features given\nby a face recognition network ArcFace[6]. For each subjects,\nwe collect around 20 test images and compute their average\nidentity scores. Table 2 shows that our method preserves the\nidentity of the subject much better than both single-image-\nbased methods and the exemplar-based approach ASFFNet.\nSubject A\nObama\nHermione\nInput\n0.721\n0.502\n0.483\nCodeFormer[60]\n0.633\n0.558\n0.518\nVQFR[12]\n0.560\n0.527\n0.483\nDR2(+VQFR)[47]\n0.384\n0.400\n0.392\nASFFNet[29]\n0.694\n0.574\n0.522\nOurs\n0.731\n0.716\n0.664\nTable 2. IDS comparison on three subjects. We use the cosine\nsimilarity of the features given by ArcFace[6] to compute identity\nscore.\n4.3. Beyond Face Restoration\nOur model does not make any assumptions about the\ntype of degradation or image contents, allowing it to be\neasily extended to other categories of data where a generative\nmodel is available. Specifically, we evaluate our approach\u2019s\nability to generalize to restoring dog and cat images. We\npre-train two diffusion models with the same architecture,\none for dogs and one for cats, on the AFHQ Dog and Cat\ndatasets [4]. Our testing involves three subjects: a gray cat,\nan English golden retriever, and an Australian shepherd. For\neach subject, we fine-tune the pre-trained diffusion model\nusing an album of around 20 images. Once fine-tuned, given\na low-quality image, we add noise to it and then denoise it\nusing the fine-tuned model. Qualitative results in Figure 7\ndemonstrate that our method can effectively reconstruct high-\nfrequency details such as fur, while preserving the identity.\n5. Ablation Studies\nNoise Step K.\nOur restoration-by-generation approach is\npredicated on the observation that sufficient noise added\nto a degraded image y0 and subsequent denoising of the\nnoisy image yK with a pre-trained diffusion model yields\na high-quality, realistic image. Here, we demonstrate this\nobservation and analyze the effect of the choice of K, which\ndetermines the noise level added to initiate the sampling\nprocess. Figure 8 displays sampled images from yK for\nvarying K values. A smaller K leads to a yK that falls\noutside the typical diffusion process\u2019s training trajectory,\nresulting in lower-quality sampled output. Conversely, while\n7\nInput\nOurs\nInput\nOurs\nFigure 7. Results on real-world cat/dog restoration. Our method\neasily extends to other categories with corresponding pre-trained\ndiffusion models. We show results on cats and dogs where we can\nreconstruct high-frequency details while preserving the identity.\na larger K enhances sample quality as hypothesized, it may\nalso produce outputs less faithful to the input.\nConstraining Prior with Generative Album.\nIn the same\nFigure 8, we illustrate the significance of prior constraining\nand the effectiveness of using a generative album. As shown,\na generative space that is too diverse increases the difficulty\nof sampling high-quality images from a given input, espe-\ncially when K is small. Conversely, for large K values,\nthe sampled image can deviate significantly from the input.\nConstraining the generative space with an album close to the\ninput ensures preservation of input information in the out-\nput for large K, while still allowing high-quality sampling\nfrom small K. Ablation on Skip Guidance is included in the\nsupplementary.\nConstraining Prior with Personal Album.\nWhen a per-\nsonal album is available, we directly constrain the generative\nspace with this album. This not only improves output quality\nand faithfulness, as with the generative album, but also aids\nin recovering information absent in the input. As demon-\nstrated in Figure 9, compared to an unconstrained model (i.e.,\nthe pre-trained diffusion model), the personalized model pro-\nduces higher-quality images that better preserve identity.\n6. Conclusion\nWe propose a method for image restoration that involves\nsimply adding noise to a degraded input and then denois-\ning it with a diffusion model. The key to our approach is\nconstraining the generative space with a set of anchor im-\nages. We demonstrate in single-image restoration tasks that\nthis method yields high-quality restoration results, surpass-\ning previous supervised approaches. Furthermore, we show\nw/o Constraining\nw/ Constraining\nInput\nK = 200\nK = 400\nK = 600\nFigure 8. Ablation on Noise Step K and Constraining with\nGenerative Album. As K increases, quality of images sampled\nfrom yK improves, but alignment with the input reduces. Fine-\ntuning with a generative album notably enhances both image quality\nand input fidelity.\nInput\nw/o constraining w/ constraining\nFigure 9. Constraining with personal album. Personalized model\nproduces higher-quality images that better preserve identity com-\npared to the model without constraining.\nthat constraining the generative space with a personal album\nleads to a personalized restoration-by-generation model that\nis effective for any image of the same subject, producing\nresults with high quality and faithful details.\nLimitations and Future Work.\nUnlike the personaliza-\ntion case, for single-image restoration, our approach requires\nfine-tuning for each input image. This is relatively slow com-\npared to feed-forward approaches. Investigating methods to\nconstrain the generative space without fine-tuning could be\ninteresting. Furthermore, we have primarily validated our ap-\nproach on class-specific image restoration tasks, largely due\nto the absence of a high-quality pre-trained diffusion model\nfor natural images. Exploring whether our approach remains\neffective within a more diverse generative space would be\nintriguing. Such exploration could potentially address the\nchallenge of blind restoration for general images.\nAcknowledgment\nWe thank Marc Levoy for providing\nvaluable feedback, and everyone whose photos appear in\nthe paper, including our furry friends, Chuchu, Nobi and\nPanghu.\n8\nReferences\n[1] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip\nSengupta, Micah Goldblum, Jonas Geiping, and Tom Gold-\nstein.\nUniversal guidance for diffusion models.\nIn Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) Workshops, pages 843\u2013852,\n2023. 4\n[2] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung,\nand Sung-Jea Ko.\nRethinking coarse-to-fine approach in\nsingle image deblurring. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 4641\u2013\n4650, 2021. 2\n[3] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. Ilvr: Conditioning method for\ndenoising diffusion probabilistic models.\narXiv preprint\narXiv:2108.02938, 2021. 3\n[4] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.\nStargan v2: Diverse image synthesis for multiple domains. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 8188\u20138197, 2020. 7\n[5] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L\nKlasky, and Jong Chul Ye.\nDiffusion posterior sam-\npling for general noisy inverse problems. arXiv preprint\narXiv:2209.14687, 2022. 2, 3, 4, 5\n[6] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.\nArcface: Additive angular margin loss for deep face recog-\nnition. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 4690\u20134699, 2019.\n7\n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021. 3\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021. A2\n[9] Zheng Ding, Xuaner Zhang, Zhihao Xia, Lars Jebe, Zhuowen\nTu, and Xiuming Zhang. Diffusionrig: Learning personal-\nized priors for facial appearance editing. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12736\u201312746, 2023. 3\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. arXiv preprint arXiv:2208.01618,\n2022. 3\n[11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Encoder-based domain\ntuning for fast personalization of text-to-image models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201313, 2023. 3\n[12] Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li,\nYing Shan, and Ming-Ming Cheng. Vqfr: Blind face restora-\ntion with vector-quantized dictionary and parallel decoder. In\nEuropean Conference on Computer Vision, pages 126\u2013143.\nSpringer, 2022. 2, 5, 6, 7, A3, A4\n[13] Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei\nZhang. Toward convolutional blind denoising of real pho-\ntographs. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 1712\u20131722,\n2019. 2\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n5\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[16] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models. arXiv\npreprint arXiv:2304.02642, 2023. 3\n[17] Zahra Kadkhodaie and Eero Simoncelli. Stochastic solutions\nfor linear inverse problems using the prior implicit in a de-\nnoiser. Advances in Neural Information Processing Systems,\n34:13242\u201313254, 2021. 3\n[18] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 4401\u20134410, 2019. 5, A1\n[19] Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips:\nSolving noisy inverse problems stochastically. Advances in\nNeural Information Processing Systems, 34:21757\u201321769,\n2021. 3\n[20] Bahjat Kawar, Gregory Vaksman, and Michael Elad. Stochas-\ntic image denoising by sampling from the posterior distribu-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1866\u20131875, 2021. 3\n[21] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. Advances\nin Neural Information Processing Systems, 35:23593\u201323606,\n2022. 2, 3\n[22] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and\nFeng Yang. Musiq: Multi-scale image quality transformer. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 5148\u20135157, 2021. 5\n[23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shecht-\nman, and Jun-Yan Zhu. Multi-concept customization of text-\nto-image diffusion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n1931\u20131941, 2023. 3\n[24] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych,\nDmytro Mishkin, and Ji\u02c7r\u00b4\u0131 Matas.\nDeblurgan: Blind mo-\ntion deblurring using conditional adversarial networks. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 8183\u20138192, 2018. 2\n[25] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang\nWang.\nDeblurgan-v2: Deblurring (orders-of-magnitude)\nfaster and better. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 8878\u20138887,\n2019. 2\n[26] Wei-Sheng Lai, Yichang Shih, Lun-Cheng Chu, Xiaotong Wu,\nSung-Fang Tsai, Michael Krainin, Deqing Sun, and Chia-Kai\nLiang. Face deblurring using dual camera fusion on mobile\n9\nphones. ACM Transactions on Graphics (TOG), 41(4):1\u201316,\n2022. 5, A1\n[27] Christian Ledig, Lucas Theis, Ferenc Husz\u00b4ar, Jose Caballero,\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\nAlykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-\nrealistic single image super-resolution using a generative ad-\nversarial network. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4681\u20134690,\n2017. 2\n[28] Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang\nLin, and Ruigang Yang. Learning warped guidance for blind\nface restoration. In Proceedings of the European conference\non computer vision (ECCV), pages 272\u2013289, 2018. 3\n[29] Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang,\nMeng Wang, and Wangmeng Zuo.\nEnhanced blind face\nrestoration with multi-exemplar images and adaptive spatial\nfeature fusion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2706\u2013\n2715, 2020. 3, 6, 7, A1, A5\n[30] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. Swinir: Image restoration\nusing swin transformer. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 1833\u2013\n1844, 2021. 2\n[31] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. A2\n[32] Xingang Pan,\nXiaohang Zhan,\nBo Dai,\nDahua Lin,\nChen Change Loy, and Ping Luo. Exploiting deep generative\nprior for versatile image restoration and manipulation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n44(11):7474\u20137489, 2021. 3\n[33] Yaniv Romano, Michael Elad, and Peyman Milanfar. The\nlittle engine that could: Regularization by denoising (red).\nSIAM Journal on Imaging Sciences, 10(4):1804\u20131844, 2017.\n2, 3\n[34] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear\ntotal variation based noise removal algorithms. Physica D:\nnonlinear phenomena, 60(1-4):259\u2013268, 1992. 2\n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 22500\u201322510,\n2023. 3\n[36] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi. Palette: Image-to-image diffusion models. In ACM\nSIGGRAPH 2022 Conference Proceedings, pages 1\u201310, 2022.\n2\n[37] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-\nmans, David J Fleet, and Mohammad Norouzi. Image super-\nresolution via iterative refinement. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 45(4):4713\u20134726,\n2022. 2\n[38] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. In-\nstantbooth: Personalized text-to-image generation without\ntest-time finetuning. arXiv preprint arXiv:2304.03411, 2023.\n3\n[39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 3\n[40] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mar-\ndani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash\nVahdat. Loss-guided diffusion models for plug-and-play con-\ntrollable generation.\nIn Proceedings of the 40th Interna-\ntional Conference on Machine Learning, pages 32483\u201332498.\nPMLR, 2023. 4, 5\n[41] Chunwei Tian, Yong Xu, Zuoyong Li, Wangmeng Zuo, Lunke\nFei, and Hong Liu. Attention-guided cnn for image denoising.\nNeural Networks, 124:117\u2013129, 2020. 2\n[42] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To-\nwards real-world blind face restoration with generative facial\nprior. In The IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2021. A3\n[43] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To-\nwards real-world blind face restoration with generative facial\nprior. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 9168\u20139178, 2021.\n2, 5\n[44] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan: Training real-world blind super-resolution with\npure synthetic data. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1905\u20131914,\n2021. 2\n[45] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image\nrestoration using denoising diffusion null-space model. In\nThe Eleventh International Conference on Learning Repre-\nsentations, 2022. 3\n[46] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang\nZhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general\nu-shaped transformer for image restoration. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 17683\u201317693, 2022. 2\n[47] Zhixin Wang, Ziying Zhang, Xiaoyun Zhang, Huangjie\nZheng, Mingyuan Zhou, Ya Zhang, and Yanfeng Wang. Dr2:\nDiffusion-based robust degradation remover for blind face\nrestoration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 1704\u20131713,\n2023. 3, 5, 6, 7, A1, A3, A5\n[48] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan\nSaharia, Alexandros G Dimakis, and Peyman Milanfar. De-\nblurring via stochastic refinement. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16293\u201316303, 2022. 2\n[49] Zhihao Xia and Ayan Chakrabarti. Identifying recurring pat-\nterns with deep neural networks for natural image denoising.\nIn Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, pages 2426\u20132434, 2020. 2\n[50] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00b4edo\nDurand, and Song Han. Fastcomposer: Tuning-free multi-\nsubject image generation with localized attention.\narXiv\npreprint arXiv:2305.10431, 2023. 3\n[51] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang.\nWider face: A face detection benchmark. In Proceedings of\n10\nthe IEEE conference on computer vision and pattern recogni-\ntion, pages 5525\u20135533, 2016. A1\n[52] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Gan\nprior embedded network for blind face restoration in the wild.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 672\u2013681, 2021. 2\n[53] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-\nnawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.\nRestormer: Efficient transformer for high-resolution image\nrestoration. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5728\u20135739,\n2022. 2\n[54] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a Gaussian denoiser: Residual learning of\ndeep CNN for image denoising. IEEE Transactions on Image\nProcessing, 26(7):3142\u20133155, 2017. 2\n[55] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.\nLearning deep cnn denoiser prior for image restoration. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 3929\u20133938, 2017. 2\n[56] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward\na fast and flexible solution for cnn-based image denoising.\nIEEE Transactions on Image Processing, 27(9):4608\u20134622,\n2018. 2\n[57] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc\nVan Gool, and Radu Timofte. Plug-and-play image restora-\ntion with deep denoiser prior. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 44(10):6360\u20136376, 2021.\n2, 3\n[58] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu.\nResidual non-local attention networks for image restoration.\narXiv preprint arXiv:1903.10082, 2019. 2\n[59] Yang Zhao, Tingbo Hou, Yu-Chuan Su, Xuhui Jia, Yandong\nLi, and Matthias Grundmann. Towards authentic face restora-\ntion with iterative diffusion models and beyond. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision, pages 7312\u20137322, 2023. 3\n[60] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change\nLoy. Towards robust blind face restoration with codebook\nlookup transformer. Advances in Neural Information Process-\ning Systems, 35:30599\u201330611, 2022. 1, 2, 5, 6, 7, A1, A3,\nA4, A5\n11\nSupplementary Material\nA. Additional Results on Blind Face Restora-\ntion\nWe provide additional qualitative comparisons on Wider-\nTest dataset in Figure S1 and the Deblur-Test dataset in\nFigure S2. Wider-Test contains 970 images selected from\nthe Wider-Face[51] dataset which is initially collected for\nface detection that contains many real-world low-quality\nface images. The Deblur-Test dataset contains 67 real-world\nmotion-blur images from [26]. Both of the datasets are\naligned using the same way in FFHQ[18]. All the previous\nmethods we compare are synthetic-data-based methods. As\nour method does not utilize synthetic data which previous\nmethods rely on, our method shows good generalizability in\nhandling different kinds of real-world degraded images.\nB. Additional Results on Personalized Blind\nFace Restoration\nIn this section, we present more personalized restora-\ntion results involving additional subjects. This includes\nSubject B (a man), as well as public figures such as\nBiden and Hermione. These results are showcased in Fig-\nure S3. We compare our method with previous methods\nCodeFormer[60] and DR2(+VQFR) [47] which are two\nsingle-image-based restoration methods that rely on syn-\nthetic data as well as ASFFNet[29] that requires a reference\ndataset. We use the same personal album in ASFFNet and\nour method. Our findings demonstrate a superior preser-\nvation of identity, along with a high level of quality in the\nresults.\nC. Anchor Images and Constrained Generative\nSpace\nIn this section, we discuss both the generative album and\nthe personalized album utilized by the model to restrict the\ngenerative space for restoration. Additionally, we visual-\nize this constrained space through unconditional generation\nfrom the fine-tuned model.\nFor the generative album, we employ the input low-\nquality image with skip guidance to produce anchor images.\nThese images are then used to fine-tune the model. We dis-\nplay the generated album, along with randomly generated\nimages from the fine-tuned model, in Figure S4. The gen-\nerated album contains images similar to the input but with\nenhanced quality, though not as high as those produced by\nthe pre-trained model. This is likely due to the influence of\nskip guidance. Nevertheless, the model fine-tuned with this\nalbum is capable of generating high-quality images that still\nbear resemblance to the original input.\nRegarding the personalized album, we collect around 20\nreal high-quality images to act as anchor images. Examples\nof these images, along with images randomly generated by\nthe personalized model, are presented in Figure S5. The\nimages produced by the personalized model exhibit both\ndiversity and identity preservation, attributes learned from\nthe personal album.\nD. More Ablation Studies\nNoise Step K and Constraining Prior with Generative\nAlbum.\nHere we provide supplemental results to Figure 8\nfrom the main paper, which analyzes the effect of noise\nstep K and the effectiveness of using a generative album to\nconstrain the prior. Results are shown in Figure S6. From\nthe results we can see that as K increases both results using\neither the constrained prior or not would have better quality\nbut would not be less faithful to the input image. However,\nwith constrained prior, we can see that the loss in faithfulness\nis considerably less than the ones not using the constrained\nprior.\nSkip Guidance for Generative Album.\nWe analyze the\neffectiveness of our proposed Skip Guidance in generating a\ngenerative album from a degraded input image. The album\nshould contain images close to the input yet of high qual-\nity, serving as anchor images for the constrained generative\nspace. Figure S7 shows that without guidance (i.e., direct\nsampling of the album from yK), we obtain high-quality\nimages that do not closely resemble the input, thus failing\nto effectively constrain the generative space. Conversely,\napplying skip guidance too frequently lowers sample quality\ndue to the guidance\u2019s approximate nature, potentially leading\nto a constrained space filled with low-quality images.\nE. Implementation Details\nWe provide the model details trained on datasets\n(256\u00d7256 and 512\u00d7512) along with the training/inference\nparameters in Table S1. Due to the lack of 512\u00d7512 model\ntrained with diffusion models and its slow speed in both train-\ning and inference, we use 256\u00d7256 for standard benchmarks\nwhile for personalized restoration, we utilize a 512\u00d7512\nmodel.\nA1\nWe first train an unconditional generative model using\nthe model architecture based on [8, 31]. After this, we\nwill get a powerful generative prior that can output high-\nquality images. Then we finetune the model using either the\ngenerative album or the personal album. For the generative\nalbum, we first generate the images with the skip guidance\nto ensure that our images follow the input. Then we finetune\nthe model using either the generative album or the personal\nalbum. Finally, we restore the images using the constrained\nprior.\n256 \u00d7 256\n512 \u00d7 512\nModel Details\nDiffusion Steps\n1000\n1000\nChannels\n128\n256\nChannels Multiple\n1, 1, 2, 2, 4, 4\n0.5, 1, 1, 2, 2, 4, 4\nHeads Channels\n128\n64\nAttention Resolution\n16\n32, 16, 8\nDropout\n0.1\n0.1\nTraining Details\nBatch Size[1]\n256\n32\nIterations\n200k\n2320k\nLearning Rate\n10\u22124\n10\u22124\nOptimizer\nAdam\nAdam\nWeight Decay\n0.0\n0.0\nGenerative Album\nNoise Step K\n600\n-\nSkip Guidance\n20\n-\nFinetuning Details\nBatch Size\n4\n4\nIterations[2]\n3000\n5000\nLearning Rate\n10\u22125\n10\u22125\nInference Details\nNoise Step K\n200\n300\nTable S1. Implementation details. [1] for AFHQ-Dog and AFHQ-\nCat (256\u00d7256), the iterations are 50k and 100k respectively. [2]\nfor personalized finetuning, we use 5000 iterations.\nA2\nInput\nGFPGAN[42]\nVQFR[12]\nCodeFormer[60] DR2(+VQFR)[47]\nOurs\nFigure S1. More qualitative comparison with previous methods on Wider-Test.\nA3\nInput\nVQFR[12]\nCodeFormer[60]\nOurs\nFigure S2. More qualitative comparison with previous methods on Deblur-Test.\nA4\nInput\nCodeFormer[60]\nDR2(+VQFR)[47]\nASFFNet[29]\nOurs\nFigure S3. More Qualitative Comparison on Personalized Face Restoration. We present three subjects here. For each subjects, we\ncompare two real-world low-quality images with previous methods. The subjects from the top to bottom are: subject B referenced in the\nmain paper, Biden and Hermione.\nA5\nInput\nGenerative Album for constraining the prior\nGenerated Images using the constrained prior\nFigure S4. Generative Album & Unconditional Generation from Fine-tuned Model. The generative album is generated with the input\nimage as guidance. Model fine-tuned with this album can then generate high-quality images that are close to the original input.\nPersonal Album for constraining the prior\nGenerated Images using the constrained prior\nFigure S5. Personal Album & Unconditional Generation from Personalized Model. We provide two sets of results.\nA6\nw/o Constraining\nw/ Constraining\nw/o Constraining\nw/ Constraining\nw/o Constraining\nw/ Constraining\nInput\nK = 100\nK = 200\nK = 300\nK = 400\nK = 500\nK = 600\nFigure S6. More Results of Ablation on Noise Step K and Constraining with Generative Album.\nInput\nn = 1\nn = 5\nn = 10\nn = 20\nn = 30\nn = 50\nNo Guidance\nFigure S7. The impact of skip guidance frequency on the generative album. Absence of skip guidance results in divergence from the\ninput, while overly frequent guidance produces low-quality anchor images.\nA7\n"
  }
]