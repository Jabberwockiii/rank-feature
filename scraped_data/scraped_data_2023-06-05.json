[
  {
    "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
    "link": "https://arxiv.org/pdf/2306.01116.pdf",
    "upvote": "26",
    "text": "The RefinedWeb Dataset for Falcon LLM:\nOutperforming Curated Corpora with Web Data, and Web Data Only\nThe Falcon LLM team\nGuilherme Penedo 1 Quentin Malartic 2\nDaniel Hesslow 1 Ruxandra Cojocaru 2 Alessandro Cappelli 1 Hamza Alobeidli 2 Baptiste Pannier 1\nEbtesam Almazrouei 2 Julien Launay 1 3\nhttps://huggingface.co/datasets/tiiuae/falcon-refinedweb\nAbstract\nLarge language models are commonly trained on a mixture of filtered web data and curated \u201chigh-quality\u201d corpora,\nsuch as social media conversations, books, or technical papers. This curation process is believed to be necessary\nto produce performant models with broad zero-shot generalization abilities. However, as larger models requiring\npretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run\nout of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and\ndeduplicated web data alone can lead to powerful models; even significantly outperforming models from the\nstate-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is\nstill plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of\n600 billion tokens from our REFINEDWEB dataset, and 1.3/7.5B parameters language models trained on it*.\n101\n102\nCompute [PF-days]\n45\n50\n55\n60\n65\nAggregated zero-shot performance [main-agg %]\nGPT-3 (paper)\u2020\nGPT-3 (API)\u2217\nRe\ufb01nedWeb (ours)\nThe Pile (ours)\nPaLM\u2020\nBS-A&S\u2217\nGPT-Neo\u2217\nPaLM-8B\u2020\nGPT-3 API\u2217\nGPT-3\u2020\nOurs (Pile)\nOurs (RW)\nFigure 1. Models trained on  REFINEDWEB alone outperform models trained on curated corpora. Zero-shot performance on\nour main-agg task aggregate (see Section 4.1 for details). At equivalent compute budgets, our models significantly outperform publicly\navailable models trained on \u25bc The Pile, and match the performance of the \u25a0 GPT-3 models when tested within our evaluation setup.\n1LightOn 2Technology Innovation Institute, 9639 Masdar City, Abu Dhabi, United Arab Emirates 3LPENS, \u00b4Ecole normale sup\u00b4erieure.\nContact: <falconllm@tii.ae>.\n*Details about how to access Falcon LLM open source is available on falconllm.tii.ae\narXiv:2306.01116v1  [cs.CL]  1 Jun 2023\nThe RefinedWeb dataset for Falcon LLM\nTable 1.  REFINEDWEB improves on existing English pretraining datasets for large language models by combining extensive\nfiltering with stringent deduplication at unprecedented scale. For additional details, see the full version in Table 12 of Appendix F.3.\nDataset\nSize\nAvailability\nWeb\nCC Processing\nDeduplication\nMASSIVE WEB DATASETS\nC4\n\u223c 360GT\nPublic\n100%\nRules + NSFW words blocklist\nExact: spans of 3 sentences\nOSCAR-21.09\n\u223c 370GT\nPublic\n100%\nBuilt at the line-level\nExact: per line (\u223c 55% removed)\nOSCAR-22.01\n\u223c 283GT\nPublic\n100%\nLine-level rules + optional rules\n& NSFW URL blocklist\nExact: per line (optional, not used\nfor results in this paper)\nCURATED DATASETS\n\u25a0 GPT-3\n300GT\nPrivate\n60%\nContent filter trained on known\nhigh-quality sources\nFuzzy: MinHash (\u223c 10% removed)\n\u25bc The Pile\n\u223c 340GT\nPublic\n18%\njusText for extraction, con-\ntent filter trained on curated data\nFuzzy: MinHash (\u223c 26% removed)\n\u22c6 PaLM\n780GT\nPrivate\n27%\nFilter trained on HQ data\nUnknown\nOURS\n REFINEDWEB \u223c 5, 000GT\nPublic (600GT)\n100%\ntrafilatura for text extrac-\ntion, document and line-level\nrules, NSFW URL blocklist\nExact\n&\nfuzzy:\nexact\nsub-\nstring+MinHash (\u223c 50% removed)\n1. Introduction\nProgress in natural language processing is increasingly\ndriven by sheer compute scale alone (Sevilla et al., 2022):\nas more compute is expended to train large language mod-\nels (LLM), they gain and exhibit powerful emergent capabil-\nities (Brown et al., 2020; Wei et al., 2022). To best benefit\nfrom scaling, recent scaling laws dictate that both model\nsize and dataset size should jointly be increased (Hoffmann\net al., 2022). This is at variance with earlier findings, which\nhad argued that scaling should focus on model size first and\nforemost, with minimal data scaling (Kaplan et al., 2020).\nThis joint scaling paradigm raises significant challenges:\nalthough plentiful, text data is not infinite, especially so\nwhen considerations on data quality and licensing are taken\ninto account\u2013leading some researchers to argue scaling may\nsoon be bottlenecked by data availability (Villalobos et al.,\n2022). Concretely, optimally training a GPT-3 sized model\n(175B parameters) would require no less than 3,500 billion\ntokens of text according to Hoffmann et al. (2022). This\nis twice as much as the largest pretraining datasets ever\ndemonstrated (Hoffmann et al., 2022; Touvron et al., 2023),\nand ten times more than the largest publicly available En-\nglish datasets such as OSCAR (Ortiz Su\u00b4arez et al., 2019),\nC4 (Raffel et al., 2020), or The Pile (Gao et al., 2020).\nMassively scaling-up pretraining data is made even more\nchallenging by the fact LLMs are commonly trained us-\ning a mixture of web crawls and so-called \u201chigh-quality\u201d\ndata (Brown et al., 2020; Gao et al., 2020). Typical high-\nquality corpora include curated sources of books, technical\ndocuments, human-selected web pages, or social media con-\nversations. The increased diversity and quality brought forth\nby these curated corpora is believed to be a key component\nof performant models (Scao et al., 2022b). Unfortunately,\ncuration is labour intensive: typically, each source requires\nspecialized processing, while yielding a limited amount of\ndata. Furthermore, licensed sources raise legal challenges.\nNevertheless, most pretraining data is still sourced from\nmassive web crawls which can be scaled up to trillions\nof tokens with limited human intervention. However, the\nquality of this data has traditionally been seen as (much)\ninferior to that of the manually curated data sources. Even\nfinely processed sources of web data, such as C4 (Raffel\net al., 2020) or OSCAR (Ortiz Su\u00b4arez et al., 2019), are\nregarded as inferior to curated corpora for LLMs (Rae et al.,\n2021; Scao et al., 2022b), producing less performant models.\nTo sustain the ever-increasing data needs of larger and larger\nLLMs, and to streamline data pipelines and reduce the need\nfor human-intensive curation, we propose to explore how\nweb data can be better processed to significantly improve its\nquality, resulting in models as capable, if not more capable,\nthan models trained on curated corpora.\nContributions.\nWe make the following contributions:\n\u2022 We introduce REFINEDWEB, a high-quality five tril-\nlion tokens web-only English pretraining dataset;\n\u2022 We demonstrate that web data alone can result in\nmodels outperforming both public and private cu-\nrated corpora, as captured by zero-shot benchmarks,\nchallenging current views about data quality;\n\u2022 We publicly release a 600B tokens extract of Re-\nfinedWeb, and 1/7B parameters LLMs trained on\nit, to serve as a new baseline high-quality web dataset\nfor the natural language processing community.\nThe RefinedWeb dataset for Falcon LLM\n2. Related works\nPretraining data for large language models.\nEarly large\nlanguage models identified the importance of datasets with\nlong, coherent documents (Radford et al., 2018; Devlin\net al., 2019). Moving on from the previously used sentence-\nwise datasets (Chelba et al., 2013), they instead leveraged\ndocument-focused, single-domain corpora like Wikipedia\nor BookCorpus (Zhu et al., 2015). As models increased in\nscale, datasets based on massive web-scrape gained preva-\nlence (Ortiz Su\u00b4arez et al., 2019; Raffel et al., 2020). How-\never, further work argued that these untargeted web scrape\nfell short of human-curated data (Radford et al., 2019), lead-\ning to the wide adoption of curated datasets such as The\nPile (Gao et al., 2020), which combine web data with books,\ntechnical articles, and social media conversations. At scale,\nit has been proposed to emulate the human curation process\nby leveraging weak signals: for instance, by crawling the top\nlinks of a forum (Gokaslan et al., 2019). Targeted corpora\ncan also produce domain-specific models (Beltagy et al.,\n2019), or broaden the expressiveness of models (e.g., for\nconversational modalities Adiwardana et al. (2020); Thop-\npilan et al. (2022)). Latest large language models (Brown\net al., 2020; Rae et al., 2021; Chowdhery et al., 2022; Scao\net al., 2022a) are trained on giant aggregated corpora, com-\nbining both massive web-scrape and so-called \u201chigh-quality\u201d\ncurated single-domain sources (e.g., news, books, techni-\ncal papers, social media conversations). These targeted\nsources are often upsampled\u2013from one to five times is most\ncommon\u2013to increase their representation in the final dataset.\nThe diversity and \u201chigher-quality\u201d brought fourth by these\naggregated datasets is thought to be central to model quality;\nweb data alone is considered insufficient to train powerful\nlarge language models (Liu et al., 2019; Scao et al., 2022b).\nPipelines for web data.\nMassive web datasets are typi-\ncally built upon CommonCrawl, a publicly available scrape\nof the internet, which has now been running for 12 years and\nhas collected petabytes of data. Working with data scraped\nfrom all over the internet presents unique challenges: no-\ntably, a significant portion is low-quality machine-generated\nspam or pornographic content (Trinh & Le, 2018; Kreutzer\net al., 2022). Accordingly, training on unfiltered web data is\nundesirable, resulting in poorly performing models (Raffel\net al., 2020). Modern pipelines focus on filtering out this un-\ndesirable content (Wenzek et al., 2020). Broadly speaking,\nthese pipelines usually combine a variety of stages: (1) lan-\nguage identification, leveraging inexpensive n-gram models\n(e.g., fastText Joulin et al. (2016)); (2) filtering rules and\nheuristics, such as only keeping lines with valid punctua-\ntion, discarding lines with too many symbols, or removing\ndocuments containing banned words (Grave et al., 2018;\nRaffel et al., 2020); (3) ML-based quality filtering, using\nlightweight models trained on known gold data to identify\nsimilar high-quality web documents (Wenzek et al., 2020;\nBrown et al., 2020); (4) deduplication, removing either ex-\nact duplicate spans or similar documents (Lee et al., 2022).\nWhile some filtering is necessary, excessive filtering can\nintroduce undesirable biases in the model. This can overly\nimpact minorities (Dodge et al., 2021), motivating the adop-\ntion of practices such as pseudo-crawling, wherein allowed\nURLs are manually curated (Laurenc\u00b8on et al., 2022).\nDeduplication.\nDeduplication removes repeated extracts\nand documents from a dataset: these could either be ex-\nact matches, identical in every character, or approximate\nmatches, based on some similarity metric. For exact dupli-\ncates, it is common to match exact substrings of a minimum\nlength using suffix arrays (Manber & Myers, 1993). For\nfuzzy duplicates, methods based on locally-sensitive hashes\nsuch as MinHash (Broder, 1997) or SimHash (Charikar,\n2002) have been adopted for the pretraining data of large\nlanguage models (Brown et al., 2020; Zeng et al., 2021; Rae\net al., 2021). Recently, Abbas et al. (2023) has proposed to\nleverage embeddings from pretrained models to imbue se-\nmantic understanding in approximate matching algorithms.\nDeduplication has been identified as playing a significant\nrole in improving language models (Allamanis, 2019; Lee\net al., 2022). Notably, it reduces memorization (Carlini\net al., 2022), which is especially problematic in large mod-\nels (Carlini et al., 2021). Furthermore, repeated data has\nbeen shown to be increasingly harmful to model quality as\nparameter count increases (Hernandez et al., 2022): for a\n1B parameters model, a hundred duplicates are harmful; at\n175B, even a few duplicates could have a disproportionate\neffect. Concurrently to this work, the Pythia suite of models\nfound that deduplicating The Pile had a limited impact on\nzero-shot performance (Biderman et al., 2023), questioning\nwhether deduplication is as relevant for curated corpora as\nit for predominantly web-based datasets.\nWe provide an overview of some widely adopted existing\npretraining English datasets for LLMs in Table 1, with ad-\nditional information in Table 12 of Appendix F.3. We also\nnote that recent popular open models (Zhang et al., 2022;\nTouvron et al., 2023) often indirectly leverage The Pile (Gao\net al., 2020) by doing a mix-and-match of its components.\nFocusing on building a large-scale high-quality web pre-\ntraining dataset, we extend upon the state-of-the-art in three\nways: (1) we aggregate and combine best-practices for doc-\nument preparation and filtering across multiple pipelines,\nand introduce line-wise corrections; (2) we combine both\nexact and fuzzy deduplication at very large-scale; (3) the\nscale of our final dataset is unique, with a total 5,000 billion\ntokens, and a 600 billion tokens extract available for public\nuse with permissive licensing. Training large models on\nRefinedWeb also lead us to challenge the commonly held\nbelief that web data is strictly worse than curated corpora.\nThe RefinedWeb dataset for Falcon LLM\n3. Macrodata Refinement and RefinedWeb\nWe introduce MDR (MacroData Refinement), a pipeline for\nfiltering and deduplicating web data from CommonCrawl at\nvery large scale. Using MDR, we produce REFINEDWEB,\nan English pretraining dataset of five trillion tokens based\non web data only. We leverage strict filtering and stringent\ndeduplication to uplift the quality of web data, distilling it\ndown to a corpus matching the quality of aggregated corpora\nused to train state-of-the-art models.\nDesign principles.\nWe abide by the following guidelines:\n\u2022 Scale first. We intend MDR to produce datasets to\nbe used to train 40-200B parameters models, thus re-\nquiring trillions of tokens (Hoffmann et al., 2022). For\nEnglish-only RefinedWeb, we target a size of 3-6 tril-\nlion tokens. Specifically, we eschew any labour inten-\nsive human curation process, and focus on Common-\nCrawl instead of disparate single-domain sources.\n\u2022 Strict deduplication. Inspired by the work of Lee et al.\n(2022), which demonstrated the value of deduplication\nfor large language models, we implement a rigorous\ndeduplication pipeline. We combine both exact and\nfuzzy deduplication, and use strict settings leading to\nremoval rates far higher than others have reported.\n\u2022 Neutral filtering. To avoid introducing further undesir-\nable biases into the model (Dodge et al., 2021; Welbl\net al., 2021), we avoid using ML-based filtering outside\nof language identification. We stick to simple rules and\nheuristics, and use only URL filtering for adult content.\nTable 2 and Figure 2 outline the full MDR pipeline.\n3.1. Document preparation: reading data, filtering\nURLs, extracting text, and language identification\nReading the data.\nCommonCrawl is available in either\nWARC (raw HTML response), or WET files (preprocessed\nto only include plain text).\nIndividual files correspond\nto a page at a given URL; these constitute single docu-\nments/samples. Working with WET files would spare us\nfrom running our own HTML extraction; however, in line\nwith previous works (Gao et al., 2020; Rae et al., 2021), we\nfound WET files to include undesirable navigation menus,\nads, and other irrelevant texts. Accordingly, our pipeline\nstarts from raw WARC files, read with the warcio library.\nURL filtering.\nBefore undertaking any compute-heavy\nprocessing, we perform a first filtering based on the URL\nalone. This targets fraudulent and/or adult websites (e.g.,\npredominantly pornographic, violent, related to gambling,\netc.). We base our filtering on two rules: (1) an aggregated\nblocklist of 4.6M domains; (2) a URL score, based on the\npresence of words from a list we curated and weighed by\nseverity. We found that commonly used blocklists include\nmany false positives, such as popular blogging platforms or\neven pop culture websites. Furthermore, word-based rules\n(like the one used in C4, Raffel et al. (2020)) can easily\nresult in medical and legal pages being blocked. Our final\ndetailed rules based on this investigation are shared in Ap-\npendix G.1. Since we intend RefinedWeb to be used as part\nof an aggregate dataset along with curated corpora, we also\nfiltered common sources of high-quality data: Wikipedia,\narXiv, etc. The detailed list is available in Appendix G.1.3.\n100%\n97,76%\n2,24%\n96,31%\n1,49%\n47,51%\n50,66%\n35,97%\n24,28%\n30,15%\n16,19%\n23,34%\n22,59%\n14,50%\n37,88%\n11,67%\n18,47%\nLoading [MathJax]/extensions/MathMenu.js\nCommon Crawl\nRW-Raw\nRW-Filtered\nURL \n\ufb01ltering\nText \nextraction\nLanguage \nidenti\ufb01cation\nRepetition \nremoval\nDocument-wise \n\ufb01ltering\nLine-wise \ncorrections\nFuzzy \ndeduplication\nExact \ndeduplication\nRW\nDocument preparation\nFiltering\nDeduplication\nFigure 2. Subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Notably,\nfiltering and deduplication each result in a halving of the data available: around 50% of documents are discarded for not being English,\n24% of remaining for being of insufficient quality, and 12% for being duplicates. We report removal rate (grey) with respect to each\nprevious stage, and kept rate (shade) overall. Rates measured in % of documents in the document preparation phase, then in tokens.\nThe RefinedWeb dataset for Falcon LLM\nTable 2. Macrodata Refinement aggregates best practices from the state-of-the-art and novel approaches (URL scoring, line-wise\nfiltering, etc.) to produce high-quality web data. On deduplication, we note that MDR is unique in both the scale at which it is\nperformed, and in applying subsequently fuzzy and exact substring methods to improve coverage and scalability.\nDOCUMENT PREPARATION\nFILTERING\nDEDUPLICATION\nURL filtering\nText extraction\nLanguage\nidentification\nDocument-wise\nfiltering\nLine-wise\nfiltering\nDeduplication\nURL deduplication\nAggregated\nblock-\nlist, URL scoring,\ncommon\nHQ\nsources blocked\nFrom\nWARC\nusing\nwarcio,\ntrafilatura for\nextraction\nfastText\nclassi-\nfier\nfrom\nCCNet,\nthresholding on top\nlanguage score\nIn-document\nrepe-\ntition removal and\nquality\nheuristics\nfrom MassiveWeb\nRemove undesirable\nlines (call to actions,\nnavigation buttons,\nsocial counters, etc.)\nFuzzy deduplication\nw/ MinHash + exact\nsubstring deduplica-\ntion w/ suffix arrays\nRemove URLs revis-\nited across Common-\nCrawl dumps\nAppendix G.1\nBarbaresi (2021)\nWenzek et al. (2020)\nRae et al. (2021)\nAppendix G.2\nLee et al. (2022)\nSection 3.3\nText extraction.\nWe want to extract only the main con-\ntent of the page, ignoring menus, headers, footers, and ads\namong others: Lopukhin (2019) found that trafilatura\n(Barbaresi, 2021) was the best non-commercial library for\nretrieving content from blog posts and news articles. Al-\nthough this is only a narrow subset of the kind of pages\nmaking up CommonCrawl, we found this finding to hold\nmore broadly. We use trafilatura for text extraction,\nand apply extra formatting via regular expressions: we limit\nnew lines to two consecutive ones, and remove all URLs.\nLanguage identification.\nWe use the fastText language\nclassifier of CCNet (Wenzek et al., 2020) at the document-\nlevel:\nit uses characters n-gram and was trained on\nWikipedia, supporting 176 languages. We remove docu-\nments for which the top language scores below 0.65: this\nusually corresponds to pages without any natural text. For\nthis paper, we focus on English; RefinedWeb can also be\nderived for other languages, see Appendix D for details.\nThe data we retrieve at this stage, called RW-RAW, corre-\nsponds to what we can extract with the minimal amount of\nfiltering. At this stage, only 48% of the original documents\nare left, mostly filtered out by language identification.\n3.2. Filtering: document-wise and line-wise\nRepetition removal.\nDue to crawling errors and low-\nquality sources, many documents contain repeated se-\nquences: this may cause pathological behavior in the final\nmodel (Holtzman et al., 2019). We could catch this content\nat the later deduplication stage, but it is cheaper and eas-\nier to catch it document-wise early on. We implement the\nheuristics of Rae et al. (2021), and remove any document\nwith excessive line, paragraph, or n-gram repetitions.\nDocument-wise filtering.\nA significant fraction of pages\nare machine-generated spam, made predominantly of lists\nof keywords, boilerplate text, or sequences of special char-\nacters. Such documents are not suitable for language model-\ning; to filter them out, we adopt the quality filtering heuris-\ntics of Rae et al. (2021). These focus on removing outliers\nin terms of overall length, symbol-to-word ratio, and other\ncriteria ensuring the document is actual natural language.\nWe note that these filters have to be adapted on a per lan-\nguage basis, as they may result in overfiltering if naively\ntransferred from English to other languages.\nLine-wise\ncorrections.\nDespite\nthe\nimprovements\nbrought forth by using trafilatura instead of relying\non preprocessed files, many documents remain interlaced\nwith undesirable lines (e.g., social media counters 3\nlikes, navigation buttons). Accordingly, we devised a\nline-correction filter, targeting these undesirable items. If\nthese corrections remove more than 5% of a document, we\nremove it entirely. See Appendix G.2 for details.\nThe data we retrieve at this stage has gone through all of\nthe filtering heuristics in the MDR pipeline. We refer to this\ndataset as RW-FILTERED. Only 23% of the documents of\nCommonCrawl are left, with around 50% of the documents\nof RW-Raw removed by the filtering.\n3.3. Deduplication: fuzzy, exact, and across dumps\nAfter filtering, although data quality has improved, a large\nfraction of the content is repeated across documents. This\nmay be due to the crawler indirectly hitting the same page\nmultiple times, to boilerplate content being repeated (e.g., li-\ncences), or even to plagiarism. These duplicates can strongly\nimpact models, favoring memorization instead of general-\nization (Lee et al., 2022; Hernandez et al., 2022). Since\ndeduplication is expensive, it has seen limited adoption in\npublic datasets (Ortiz Su\u00b4arez et al., 2019; Raffel et al., 2020).\nWe adopt an aggressive deduplication strategy, combining\nboth fuzzy document matches and exact sequences removal.\nFuzzy deduplication.\nWe remove similar documents by\napplying MinHash (Broder, 1997): for each document, we\ncompute a sketch and measure its approximate similarity\nwith other documents, eventually removing pairs with high\noverlap. MinHash excels at finding templated documents:\nlicenses with only specific entities differing, placeholder\nSEO text repeated across websites\u2013see examples of the\nThe RefinedWeb dataset for Falcon LLM\nTable 3. To evaluate models trained on RefinedWeb and compare to the state-of-the-art, we build four aggregates across 18 tasks\non which to measure zero-shot performance. small was built for internal ablations, based on tasks with consistent performance at\nsmall scale, core is based on tasks commonly reported for public suites of models (Dey et al., 2023; Biderman et al., 2023), main is\nbased on tasks from the GPT-3 and PaLM paper (Brown et al., 2020; Chowdhery et al., 2022), and ext is based on tasks used by the\nBigScience Architecture and Scaling group (Scao et al., 2022b). For all results reported, we flag with \u2020 results obtained in an arbitrary\nevaluation setup, and with \u2217 results obtained with the EAI Harness (Gao et al., 2021), which we also employ for all our models.\nTasks\nType\nRandom\nsmall\ncore\nmain\next\nHellaSwag (Zellers et al., 2019)\nSentence completion\n25.0\n\u2713\n\u2713\n\u2713\n\u2713\nLAMBADA (Paperno et al., 2016)\nSentence completion\n0.0\n\u2713\n\u2713\n\u2713\nWinogrande (Sakaguchi et al., 2021)\nCoreference resolution\n50.0\n\u2713\n\u2713\n\u2713\n\u2713\nPIQA (Bisk et al., 2020)\nMultiple-choice question answering\n50.0\n\u2713\n\u2713\n\u2713\n\u2713\nARC (Clark et al., 2018)\nNatural language inference\n25.0\n\u2713\n\u2713\n\u2713\n\u2713\nOpenBookQA (Mihaylov et al., 2018)\nMultiple-choice question answering\n25.0\n\u2713\n\u2713\n\u2713\nBoolQ (Clark et al., 2019)\nMultiple-choice question answering\n50.0\n\u2713\n\u2713\n\u2713\nCOPA (Gordon et al., 2012)\nSentence completion\n50.0\n\u2713\n\u2713\nCB (De Marneffe et al., 2019)\nNatural language inference\n33.3\n\u2713\n\u2713\nRTE (Dagan et al., 2010)\nNatural language inference\n50.0\n\u2713\n\u2713\nReCoRD (Zhang et al., 2018)\nQuestion answering\n0.0\n\u2713\nANLI (Nie et al., 2019)\nNatural language inference\n33.3\n\u2713\nLogiQA (Liu et al., 2021)\nMultiple-choice question answering\n25.0\n\u2713\nHeadQA (Vilares & G\u00b4omez-Rodr\u00b4\u0131guez, 2019)\nMultiple-choice question answering\n20.0\n\u2713\nMathQA (Amini et al., 2019)\nMultiple-choice question answering\n20.0\n\u2713\nPROST (Aroca-Ouellette et al., 2021)\nParaphrase identification\n50.0\n\u2713\nPubMedQA (Jin et al., 2019)\nMultiple-choice question answering\n50.0\n\u2713\nSciQ (Welbl et al., 2017)\nMultiple-choice question answering\n25.0\n\u2713\n\u2713\nbiggest clusters in Appendix H.1. We perform MinHash\ndeduplication using 9,000 hashes per document, calculated\nover 5-grams and divided into 20 buckets of 450 hashes.\nWe found that using less aggressive settings, such as the\n10 hashes of The Pile (Gao et al., 2020), resulted in lower\ndeduplication rates and worsened model performance. See\nAppendix G.3.1 for more details about our MinHash setup.\nExact deduplication.\nExact substring operates at the\nsequence-level instead of the document-level, finding\nmatches between strings that are exact token-by-token\nmatches by using a suffix array (Manber & Myers, 1993)\n(e.g., specific disclaimers or notices, which may not compro-\nmise the entire document as showcased in Appendix H.2).\nWe remove any match of more than 50 consecutive tokens,\nusing the implementation of Lee et al. (2022). We note\nthat exact substring alters documents, by removing specific\nspans: we also experimented with dropping entire docu-\nments or loss-masking the duplicated strings instead of cut-\nting them, but this didn\u2019t result in significant changes in\nzero-shot performance\u2013see Appendix G.3.2.\nURL deduplication.\nBecause of computational con-\nstraints, it is impossible for us to perform deduplication\ndirectly on RW-Filtered. Instead, we split CommonCrawl\ninto 100 parts, where each part contains a hundredth of\neach dump, and perform deduplication on individual parts.\nMost of the larger duplicate clusters (e.g., licences, common\nspams) will be shared across parts, and effectively removed.\nHowever, we found that CommonCrawl dumps had sig-\nnificant overlap, with URLs being revisited across dumps\ndespite no change in content. Accordingly, we keep a list of\nthe URLs of all samples we have kept from each part, and\nremove them from subsequent parts being processed.\n4. Experiments\nWe now validate that RefinedWeb can be used to train pow-\nerful models, matching the zero-shot performance obtained\nwith curated corpora and state-of-the-art language models.\nWe first discuss our evaluation and pretraining setup, and\nmodels with which we compare. We perform experiments at\nsmall scale to internally compare with other popular datasets,\nand ablate the three main stages of RefinedWeb (raw, fil-\ntered, final). Then, we scale to 1B and 7B models trained on\n350GT to compare with state-of-the-art models. Finally, we\napply the MDR pipeline to existing pretraining datasets, and\nshow that it can potentially deliver further improvements.\n4.1. Setting\nEvaluation. At variance with previous works studying pre-\ntraining datasets (Rae et al., 2021; Lee et al., 2022), we focus\nour evaluation on zero-shot generalization across many tasks\nrather than measuring validation loss. Perplexity alone can\nbe at odds with end-task performance (Tay et al., 2021), and\nmodern works on LLMs predominantly report zero-shot per-\nformance (Brown et al., 2020; Rae et al., 2021; Chowdhery\net al., 2022). Furthermore, zero-shot generalization is the\n\u201cnatural\u201d setting for autoregressive decoder-only models, in\nwhich they perform best (Wang et al., 2022). Our evaluation\nsetup is inspired by the one used by the architecture and\nscaling group of Big Science (Scao et al., 2022b).\nWe base our evaluation on the popular Eleuther AI eval-\nuation harness (Gao et al., 2021), allowing us to evaluate\nacross a wide range of tasks in the zero-shot setting. We\nidentified aggregates of tasks allowing us to: (1) obtain sig-\nnal (i.e., non zero zero-shot performance) at small scale for\nThe RefinedWeb dataset for Falcon LLM\nTable 4. Curation is not a silver bullet for zero-shot generalization: small-scale models trained on  REFINEDWEB outperform\nmodels trained on web data (C4, OSCAR), and on curated corpora (\u25bc The Pile). Average accuracy in zero-shot on the small-agg\naggregate. All models trained with identical architectures and pretraining hyperparameters. We find that OSCAR-22.01 underperforms\nother datasets signficantly, perhaps because deduplication is only optional. C4 is a strong baseline, with OSCAR-21.09 lagging slightly\nbehind, but we find that RefinedWeb outperforms both web datasets and the most popular curated dataset, The Pile. Both filtering and\ndeduplication contribute significantly to improving zero-shot performance.\nMASSIVE WEB DATASETS\nCURATED\nOURS\nOSCAR-21.09\nOSCAR-22.01\nC4\n\u25bc The Pile\nRW-Raw\nRW-Filtered\n REFINEDWEB\n1B@27GT\n55.0%\n52.7%\n55.7%\n53.4%\n52.7%\n54.3%\n56.2%\n3B@60GT\n59.1%\n55.9%\n59.6%\n57.9%\n57.4%\n58.2%\n59.8%\nablations; (2) compare with results reported by other models.\nWe outline these four aggregates small (for ablations), and\ncore, main, ext (for comparisons) in Table 3.\nComparisons across models trained and evaluated in differ-\nent settings are difficult to untangle, as many externalities\nmay influence the 1 987results (e.g., numerical precision of\ntraining vs inference, prompts used). We distinguish three\nlevels of comparisons: (1) internal comparisons, with mod-\nels trained and evaluated within our codebase, for which\nonly the pretraining datasets differ; (2) benchmark-level\ncomparisons, with models trained with a different codebase\nbut evaluated with the Eleuther AI harness, taking results\nfrom Scao et al. (2022b); Black et al. (2022); Aleph Alpha\n(2023); Dey et al. (2023), thereafter flagged with a \u2217; (3)\nexternal comparisons with Brown et al. (2020); Chowdhery\net al. (2022), thereafter flagged with a \u2020. For further details\non evaluation, see Appendix F.1.\nModels. We train 1B, 3B, and 7B parameters autoregressive\ndecoder-only models, based on configurations and hyperpa-\nrameters similar to GPT-3 (Brown et al., 2020), diverging\nmostly on our use of ALiBi (Press et al., 2021). We use\nFlashAttention (Dao et al., 2022) in a custom codebase. We\ntrain internal models on both The Pile and RefinedWeb to\ncontrol for deviations caused by our pretraining setup\u2013we\nfound The Pile models to perform in-line with others. For\nsmall-scale and ablation studies (first half of Section 4.2;\nSection 4.3), we train models to optimality according to the\nscaling laws of Hoffmann et al. (2022): on 27B and 60B to-\nkens respectively for our 1B and 3B parameters models. For\nthe main experiments demonstrating our approach (Falcon-\nRW models in Section 4.2), we train the models to 350GT, in\nline with popular public models (Brown et al., 2020; Wang\n& Komatsuzaki, 2021; Scao et al., 2022a). Note that we do\nnot compare against the recently introduced LLaMA mod-\nels (Touvron et al., 2023), as the smallest of them is trained\non x2.5 more compute than our largest model, preventing\na meaningful comparison from being made dataset-wise.\nFor a more in-depth overview of the models and pretraining\ndatasets with which we compare, see Appendix F.\n4.2. Can web data alone outperform curated corpora?\nWe endeavour to demonstrate that web data alone can result\nin models outperforming other models trained on curated\ncorpora. To do so, we first perform a small-scale study with\n1B and 3B parameters models trained to optimality (27GT\nand 60GT) on popular web and curated datasets. Then,\nwe scale up to 1B and 7B models trained on 350GT, and\ncompare zero-shot generalization to state-of-the-art models.\nSmall-scale study.\nWe first consider popular public web\ndatasets (OSCAR-2019 (Ortiz Su\u00b4arez et al., 2019), OSCAR-\n2022 (Abadji et al., 2021), C4 (Raffel et al., 2020)), The\nPile (Gao et al., 2020) as the most popular publicly available\ncurated dataset, and variations of RefinedWeb (RW-Raw,\nRW-Filtered, and RW as described in Section 3). For this\nfirst study, all models are trained with the same architecture\nand the same internal codebase; they are also all evaluated\nwithin the same framework\u2013only pretraining datasets differ.\nResults averaged on the small-=+\naggregate of 6 tasks\nare presented in Table 4. We observe relatively strong perfor-\nmance of all web datasets compared to The Pile, showcasing\nthat curation is not a silver bullet for performant language\nmodels. We find C4 to be a strong pretraining dataset, in line\nwith the findings of Scao et al. (2022b)\u2013however, The Pile\ncomparatively underperforms more in our benchmarks. The\nrelatively disappointing results on OSCAR-22.01 may be\ndue to the main version of the dataset being distributed with-\nout deduplication. Regarding RefinedWeb, both filtering\nand deduplication significantly improve performance.\nFull-scale models.\nWe now validate these results with\ncomparisons with state-of-the-art models. We scale our pre-\nvious experiments by training 1B and 7B models on 350GT;\nwe also train a 1B model on 350GT on The Pile, as a control\nfor the influence of our pretraining setup. We compare with\nthe following models: the GPT-3 series (Brown et al., 2020),\nthe FairSeq series (Artetxe et al., 2021), the GPT-Neo(X)/J\nmodels (Black et al., 2021; Wang & Komatsuzaki, 2021;\nBlack et al., 2022), the OPT series (Zhang et al., 2022),\nThe RefinedWeb dataset for Falcon LLM\nthe BigScience Architecture and Scaling Pile model (Scao\net al., 2022b), PaLM-8B (Chowdhery et al., 2022), Aleph\nAlpha Luminous 13B (Aleph Alpha, 2023), the Pythia series\n(Biderman et al., 2023), and the Cerebras-GPT series (Dey\net al., 2023). For GPT-3, we distinguish between results\nobtained through the API (babbage and curie) with the\nthe EleutherAI LM evaluation harness (Gao et al., 2021) (*),\nand results reported in their paper, with a different evalua-\ntion setup (\u2020). Note that for PaLM and OPT, results were\nalso obtained with a different evaluation suite (\u2020), while for\nother models they were obtained with the evaluation harness\nas well (*), allowing for more direct comparisons.\nResults on main-agg are presented in Figure 1, and in\nFigure 3 for core-agg and ext-agg. We find that open\nmodels consistently underperform models trained on private\ncurated corpora, such as GPT-3\u2013even when using a similar\nevaluation setup. Conversely, models trained on Refined-\nWeb are able to match the performance of the GPT-3 series\nusing web data alone, even though common high-quality\nsources used in The Pile are excluded from RefinedWeb (see\nTable 14 in Appendix). Finally, we note that our internal\nmodel trained on The Pile performs in line with the Big-\nScience Architecture and Scaling model; this highlights that\nour pretraining setup is unlikely to be the main source of\nincreased performance for models trained on RefinedWeb.\nFinding. Challenging existing beliefs on data qual-\nity and LLMs, models trained on adequately filtered\nand deduplicated web data alone can match the perfor-\nmance of models trained on curated data.\n4.3. Do other corpora benefit from MDR?\nAblating the contributions and evaluating the performance\nof individual components in the MDR pipeline is difficult:\nfor most heuristics, there is no agreed-upon ground truth,\nand changes may be too insignificant to result in sufficient\nzero-shot signal after pretraining. In the first half of Sec-\ntion 4.2, we identified that subsequent stages of RefinedWeb\n(raw, filtered, final) led to improvements in performance. In\nthis section, we propose to apply independently the filtering\nand deduplication stages of MDR to popular pretraining\ndatasets, studying whether they generalize widely.\nWe report results on the small-agg in Table 5. First, we\nfind that improvements from filtering are not systematic. On\nThe Pile, we had to adjust our line length and characters ratio\nheuristics to avoid expunging books and code. Despite im-\nprovements on OSCAR-21.09, C4, and The Pile, our filters\nworsen performance on OSCAR-22.01; generally, removal\nrates from filtering do not seem strongly correlated with\ndownstream accuracy. Conversely, deduplication delivers a\nsteady boost across all datasets, and removal rates are better\ncorrelated with changes in performance. We find OSCAR-\n21.09 and C4 to be already well deduplicated, while The\nPile and OSCAR-22.01 exhibit 40-60% duplicates. The\nbase version of OSCAR-22.01 is distributed without dedu-\nplication; for The Pile, this is consistent with the findings\nof Zhang et al. (2022). Finally, combining filtering and\ndeduplication results in further improvements; interestingly,\nalthough performance is now more uniform across datasets,\ndifferences remain, suggesting that flaws in the original text\nextraction and processing can\u2019t be fully compensated for.\n101\n102\n103\nCompute [PF-days]\n45\n50\n55\n60\nAggregated zero-shot performance [core-agg %]\nGPT-3\u2217\nRe\ufb01nedWeb (ours)\nOPT\u2020\nAleph Alpha\u2217\nCerebras-GPT\u2217\nFairSeq\u2217\nPythia-Dedup\u2217\nPythia\u2217\nOPT\u2020\nGPT-NeoX/J\u2217\nGPT-3 API\u2217\nOurs (RW)\n(a)\n101\n102\nCompute [PF-days]\n45\n50\n55\nAggregated zero-shot performance [ext-agg %]\n}1B models trained on The Pile\nGPT-3\u2217\nRe\ufb01nedWeb (ours)\nThe Pile (ours)\nBS-A&S\u2217\nGPT-Neo\u2217\nGPT-3 API\u2217\nOurs (Pile)\nOurs (RW)\n(b)\nFigure 3. Models trained on  REFINEDWEB alone outperform models trained on curated corpora. Zero-shot performance averaged\non our core-agg (left) and ext-agg (right) task aggregates (see Section 4.1 for details, and Figure 1 for results on main-agg).\nExisting open models fail to match the performance of the original GPT-3 series (left); however, models trained on RefinedWeb\nsignificantly outperform models trained on \u25bc The Pile: including our direct comparison model (right), ruling out our pretraining setup as\nthe main source of increased performance. In fact, our RefinedWeb models even match the performance of the \u25a0 GPT-3 models.\nThe RefinedWeb dataset for Falcon LLM\nTable 5. Although improvements from filtering are not systematic across datasets, deduplication brings a steady performance boost\nacross the board. Zero-shot accuracy averaged on our small-agg aggregate; [+x.x] reports absolute gains compared to base, removal\nrates reported against base. Due to limitations in our pipeline, we cannot apply the deduplication stage independently for RefinedWeb.\nMASSIVE WEB DATASETS\nCURATED\nOURS\nOSCAR-21.09\nOSCAR-22.01\nC4\n\u25bc Pile\n RefinedWeb\nBase\n55.0%\n52.7%\n55.7%\n53.4%\n52.7%\nFiltered\n55.4% [+.4]\n52.3% [-.4]\n56.2% [+.5]\n54.2% [+.8]\n54.3% [+1.6]\nremoval rate\n-25.0%\n-39.8%\n-16.4%\n-27.1%\n-50.8%\nDeduplicated\n55.6% [+.6]\n55.6% [+2.9]\n55.9% [+.2]\n54.5% [+1.1]\nremoval rate\n-10.8%\n-60.8%\n-7.59%\n-45.3%\nFilt.+Dedup.\n55.5% [+.5]\n55.4% [+2.7]\n56.4% [+.7]\n55.2% [+1.8]\n56.2% [+3.5]\nremoval rate\n-28.2%\n-62.2%\n-17.9%\n-66.0%\n-75.4%\nBy processing C4 through MDR, we are able to obtain sub-\nsets of data which might slightly outperform RefinedWeb;\nthis combines both the stringent filtering of C4 (e.g., strict\nNSFW word blocklist, 3-sentence span deduplication) with\nour own filters and deduplication. While such a combination\nresults in rejection rates that would be unacceptable for our\ntarget of 3-6 trillions tokens, this represents an interesting\nperspective for shorter runs, which may be able to extract\nextremely high-quality subsets from large web datasets.\nFinding. While filtering heuristics may require source-\ndependent tuning, stringent deduplication improves\nzero-shot performance across datasets consistently.\n5. Limitations\nBiases.\nWe conduct a basic analysis of the toxicity of Re-\nfinedWeb in Figure 4. We find RW to be about as toxic\nas The Pile, based on the definition of toxicity provided\nby the Perspective API: \u201dcontent that is rude or disrespect-\nful\u201d. Notably, this definition does not cover issues with\nsocial biases or harmfulness. Although it is unlikely that\nour pipeline introduces further issues on this side than is\nalready documented for popular datasets, we encourage fur-\nther quantitative work on the public extract of RefinedWeb.\nMultiple epochs.\nInstead of looking for \u201dunique\u201d tokens\nto make up a trillion-scale pretraining dataset, one could\nsimply repeat data over multiple epochs. Popular models\nlike OPT and NeoX-20B do this for up to 2 epochs, and\nmost curated datasets upsample corpora 2-5 times. How-\never, Hernandez et al. (2022) has recently shown that models\nwith 100B+ parameters may be sensitive to even just a few\nepochs. Orthogonal to our work lies a line of research ex-\nploring tradeoffs in the data-constrained regime: can dedu-\nplication help sustain more epochs? Are multiple epochs on\nhigher quality data better than a one epoch on lower quality\ndata? See Appendix E.3 for a more in-depth discussion.\nOther results on deduplication.\nBiderman et al. (2023)\nfound a limited impact on zero-shot performance from dedu-\nplicating The Pile; we discuss further in Appendix F.2, but\nencourage further deduplication research on curated corpora,\nand studying deduplication in the data-constrained regime,\nwhere multiple epochs have to be performed to compensate\nfor the reduction in tokens incurred by deduplication.\n6. Conclusion\nAs LLMs are widely adopted, models trained past the rec-\nommendations of scaling laws are bound to become in-\ncreasingly common to amortize inference costs (Touvron\net al., 2023). This will further drive the need for pretraining\ndatasets with trillions of tokens, an order of magnitude be-\nyond publicly available corpora. We have demonstrated that\nstringent filtering and deduplication could result in a five\ntrillion tokens web only dataset suitable to produce models\ncompetitive with the state-of-the-art, even outperforming\nLLMs trained on curated corpora. We publicly release a\n600GT extract of RefinedWeb, and note that RefinedWeb\nhas already been used to train state-of-the-art language mod-\nels, such as Falcon-40B (Almazrouei et al., 2023).\n0.0\n0.2\n0.4\n0.6\n0.8\nToxicity score\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nProportion\nRe\ufb01nedWeb\nRW \ufb01ltered\nThe Pile\nC4\nOSCAR 22.01\nOSCAR 2019\nFigure 4. Toxic content in RefinedWeb is distributed similarly\nto The Pile. Cumulative proportion of documents below a given\ntoxicity score, as evaluated by the Pespective API.\nThe RefinedWeb dataset for Falcon LLM\nReferences\nAbadji, J., Su\u00b4arez, P. J. O., Romary, L., and Sagot, B. Un-\ngoliant: An optimized pipeline for the generation of\na very large-scale multilingual web corpus. Proceed-\nings of the Workshop on Challenges in the Manage-\nment of Large Corpora (CMLC-9) 2021. Limerick, 12\nJuly 2021 (Online-Event), pp. 1 \u2013 9, Mannheim, 2021.\nLeibniz-Institut f\u00a8ur Deutsche Sprache. doi: 10.14618/\nids-pub-10468.\nURL https://nbn-resolving.\norg/urn:nbn:de:bsz:mh39-104688.\nAbadji, J., Ortiz Suarez, P., Romary, L., and Sagot, B.\nTowards a Cleaner Document-Oriented Multilingual\nCrawled Corpus. arXiv e-prints, art. arXiv:2201.06642,\nJanuary 2022.\nAbbas, A. K. M., Tirumala, K., Simig, D., Ganguli, S.,\nand Morcos, A. S. Semdedup: Data-efficient learning at\nweb-scale through semantic deduplication. In ICLR 2023\nWorkshop on Mathematical and Empirical Understanding\nof Foundation Models, 2023.\nAdiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel,\nN., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade,\nG., Lu, Y., et al. Towards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977, 2020.\nAleph Alpha.\nLuminous:\nperformance benchmarks.\narXiv\npreprint\narXiv:1810.12885,\n2023.\nURL\nhttps://www.aleph-alpha.com/pdf/2023_\n02_AA_Benchmarks_doc.pdf.\nAllamanis, M. The adverse effects of code duplication in\nmachine learning models of code. In Proceedings of the\n2019 ACM SIGPLAN International Symposium on New\nIdeas, New Paradigms, and Reflections on Programming\nand Software, pp. 143\u2013153, 2019.\nAlmazrouei, E., Cappelli, A., Cojocaru, R., Debbah, M.,\nGoffinet, E., Heslow, D., Launay, J., Malartic, Q., Noune,\nB., Pannier, B., and Penedo, G. Falcon-40b: an open\nlarge language model with state-of-the-art performance.\n2023.\nAmini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi,\nY., and Hajishirzi, H. Mathqa: Towards interpretable\nmath word problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pp. 2357\u20132367, 2019.\nAroca-Ouellette, S., Paik, C., Roncone, A., and Kann, K.\nProst: Physical reasoning about objects through space and\ntime. In Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pp. 4597\u20134608, 2021.\nArtetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M.,\nShleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., et al.\nEfficient large scale language modeling with mixtures of\nexperts. arXiv preprint arXiv:2112.10684, 2021.\nBarbaresi, A. Trafilatura: A Web Scraping Library and\nCommand-Line Tool for Text Discovery and Extraction.\nIn Proceedings of the Joint Conference of the 59th An-\nnual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on\nNatural Language Processing: System Demonstrations,\npp. 122\u2013131. Association for Computational Linguistics,\n2021. URL https://aclanthology.org/2021.\nacl-demo.15.\nBeltagy, I., Lo, K., and Cohan, A. Scibert: A pretrained\nlanguage model for scientific text. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-\nIJCNLP), pp. 3615\u20133620, 2019.\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H.,\nO\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for ana-\nlyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432\u20137439, 2020.\nBlack, S., Leo, G., Wang, P., Leahy, C., and Biderman, S.\nGPT-Neo: Large Scale Autoregressive Language Model-\ning with Mesh-Tensorflow, March 2021. URL https:\n//doi.org/10.5281/zenodo.5297715. If you\nuse this software, please cite it using these metadata.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., et al. Gpt-neox-20b: An open-source autoregressive\nlanguage model. Challenges & Perspectives in Creating\nLarge Language Models, pp. 95, 2022.\nBroder, A. Z.\nOn the resemblance and containment of\ndocuments. In Proceedings. Compression and Complexity\nof Sequences 1997, pp. 21\u201329. IEEE, 1997.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T., Song, D.,\nThe RefinedWeb dataset for Falcon LLM\nErlingsson, U., et al. Extracting training data from large\nlanguage models. In 30th USENIX Security Symposium\n(USENIX Security 21), pp. 2633\u20132650, 2021.\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F.,\nand Zhang, C. Quantifying memorization across neu-\nral language models. arXiv preprint arXiv:2202.07646,\n2022.\nCharikar, M. S. Similarity estimation techniques from round-\ning algorithms. In Proceedings of the thiry-fourth annual\nACM symposium on Theory of computing, pp. 380\u2013388,\n2002.\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T.,\nKoehn, P., and Robinson, T. One billion word benchmark\nfor measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005, 2013.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,\nM., and Toutanova, K. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings of\nNAACL-HLT, pp. 2924\u20132936, 2019.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nDagan, I., Dolan, B., Magnini, B., and Roth, D. Recognizing\ntextual entailment: Rational, evaluation and approaches\u2013\nerratum. Natural Language Engineering, 16(1):105\u2013105,\n2010.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. In Advances in Neural Information Pro-\ncessing Systems, 2022.\nDe Marneffe, M.-C., Simons, M., and Tonhauser, J. The\ncommitmentbank: Investigating projection in naturally\noccurring discourse. In proceedings of Sinn und Bedeu-\ntung, volume 23, pp. 107\u2013124, 2019.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 4171\u20134186,\n2019.\nDey, N., Gosal, G., Khachane, H., Marshall, W., Pathria, R.,\nTom, M., Hestness, J., et al. Cerebras-gpt: Open compute-\noptimal language models trained on the cerebras wafer-\nscale cluster. arXiv preprint arXiv:2304.03208, 2023.\nDodge, J., Sap, M., Marasovi\u00b4c, A., Agnew, W., Ilharco, G.,\nGroeneveld, D., Mitchell, M., and Gardner, M. Docu-\nmenting large webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 1286\u20131305, 2021.\nEberhard, D. M., Simons, G. F., and Fennig, C. D. Ethno-\nlogue: Languages of the World. SIL International, Dallas,\nTX, USA, twenty-sixth edition, 2023.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\net al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027,\n2020.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\nWang, K., and Zou, A. A framework for few-shot lan-\nguage model evaluation, September 2021. URL https:\n//doi.org/10.5281/zenodo.5371628.\nGebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W.,\nWallach, H., Iii, H. D., and Crawford, K. Datasheets for\ndatasets. Communications of the ACM, 64(12):86\u201392,\n2021.\nGokaslan, A., Cohen, V., Pavlick, E., and Tellex, S. Open-\nwebtext corpus.\nhttp://Skylion007.github.\nio/OpenWebTextCorpus, 2019.\nGordon, A., Kozareva, Z., and Roemmele, M. Semeval-\n2012 task 7: Choice of plausible alternatives: An evalua-\ntion of commonsense causal reasoning. In * SEM 2012:\nThe First Joint Conference on Lexical and Computational\nSemantics\u2013Volume 1: Proceedings of the main confer-\nence and the shared task, and Volume 2: Proceedings of\nthe Sixth International Workshop on Semantic Evaluation\n(SemEval 2012), pp. 394\u2013398, 2012.\nGrave, \u00b4E., Bojanowski, P., Gupta, P., Joulin, A., and\nMikolov, T. Learning word vectors for 157 languages. In\nProceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018), 2018.\nHanu,\nL. and Unitary team.\nDetoxify.\nGithub.\nhttps://github.com/unitaryai/detoxify, 2020.\nHernandez, D., Brown, T., Conerly, T., DasSarma, N.,\nDrain, D., El-Showk, S., Elhage, N., Hatfield-Dodds,\nThe RefinedWeb dataset for Falcon LLM\nZ., Henighan, T., Hume, T., et al. Scaling laws and inter-\npretability of learning from repeated data. arXiv preprint\narXiv:2205.10487, 2022.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The\ncurious case of neural text degeneration. In International\nConference on Learning Representations, 2019.\nJaccard, P. The distribution of the flora in the alpine zone.1.\nNew Phytologist, 11:37\u201350, 1912.\nJin, Q., Dhingra, B., Liu, Z., Cohen, W., and Lu, X. Pub-\nmedqa: A dataset for biomedical research question an-\nswering. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2567\u20132577, 2019.\nJoulin, A., Grave, E., Bojanowski, P., Douze, M., J\u00b4egou,\nH., and Mikolov, T. Fasttext. zip: Compressing text\nclassification models. arXiv preprint arXiv:1612.03651,\n2016.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D.,\nUlzii-Orshikh, N., Tapo, A. A., Subramani, N., Sokolov,\nA., Sikasote, C., et al. Quality at a glance: An audit\nof web-crawled multilingual datasets. Transactions of\nthe Association for Computational Linguistics, 10:50\u201372,\n2022.\nLaurenc\u00b8on, H., Saulnier, L., Wang, T., Akiki, C., del Moral,\nA. V., Le Scao, T., Von Werra, L., Mou, C., Ponferrada,\nE. G., Nguyen, H., et al. The bigscience roots corpus:\nA 1.6 tb composite multilingual dataset. In Thirty-sixth\nConference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2022.\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D.,\nCallison-Burch, C., and Carlini, N. Deduplicating train-\ning data makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 8424\u2013\n8445, 2022.\nLiu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang,\nY. Logiqa: a challenge dataset for machine reading com-\nprehension with logical reasoning. In Proceedings of\nthe Twenty-Ninth International Conference on Interna-\ntional Joint Conferences on Artificial Intelligence, pp.\n3622\u20133628, 2021.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nLopukhin, K.\nEvaluating quality of article body ex-\ntraction for commercial services and open-source li-\nbraries.\nhttps://github.com/scrapinghub/\narticle-extraction-benchmark, 2019.\nManber, U. and Myers, G. Suffix arrays: a new method\nfor on-line string searches. Journal on Computing, 22(5):\n935\u2013948, 1993.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a\nsuit of armor conduct electricity? a new dataset for open\nbook question answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language\nProcessing, pp. 2381\u20132391, 2018.\nMitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman,\nL., Hutchinson, B., Spitzer, E., Raji, I. D., and Gebru, T.\nModel cards for model reporting. In Proceedings of the\nconference on fairness, accountability, and transparency,\npp. 220\u2013229, 2019.\nNie, Y., Williams, A., Dinan, E., Bansal, M., Weston,\nJ., and Kiela, D. Adversarial nli: A new benchmark\nfor natural language understanding.\narXiv preprint\narXiv:1910.14599, 2019.\nOrtiz Su\u00b4arez, P. J., Sagot, B., and Romary, L.\nAsyn-\nchronous pipelines for processing huge corpora on\nmedium to low resource infrastructures.\nProceed-\nings of the Workshop on Challenges in the Man-\nagement of Large Corpora (CMLC-7) 2019. Cardiff,\n22nd July 2019,\npp. 9 \u2013 16,\nMannheim,\n2019.\nLeibniz-Institut f\u00a8ur Deutsche Sprache. doi: 10.14618/\nids-pub-9021. URL http://nbn-resolving.de/\nurn:nbn:de:bsz:mh39-90215.\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, N.-Q.,\nBernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and\nFern\u00b4andez, R. The lambada dataset: Word prediction\nrequiring a broad discourse context. In Proceedings of\nthe 54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 1525\u2013\n1534, 2016.\nPomik\u00b4alek, J. Justext. 2011.\nPress, O., Smith, N., and Lewis, M. Train short, test long:\nAttention with linear biases enables input length extrapo-\nlation. In International Conference on Learning Repre-\nsentations, 2021.\nThe RefinedWeb dataset for Falcon LLM\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. 2019.\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,\nSong, F., Aslanides, J., Henderson, S., Ring, R., Young,\nS., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,\nPowell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,\nM., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,\nHuang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,\nA., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,\nBuchatskaya, E., Budden, D., Sutherland, E., Simonyan,\nK., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-\ncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,\nLazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,\nM., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,\nPohlen, T., Gong, Z., Toyama, D., d\u2019Autume, C. d. M.,\nLi, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A.,\nCasas, D. d. L., Guy, A., Jones, C., Bradbury, J., John-\nson, M., Hechtman, B., Weidinger, L., Gabriel, I., Isaac,\nW., Lockhart, E., Osindero, S., Rimell, L., Dyer, C.,\nVinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis,\nD., Kavukcuoglu, K., and Irving, G. Scaling language\nmodels: Methods, analysis & insights from training go-\npher. 2021. doi: 10.48550/ARXIV.2112.11446. URL\nhttps://arxiv.org/abs/2112.11446.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,\nS., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Ex-\nploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning\nResearch, 21(140):1\u201367, 2020. URL http://jmlr.\norg/papers/v21/20-074.html.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u00b4c, S., Hesslow,\nD., Castagn\u00b4e, R., Luccioni, A. S., Yvon, F., Gall\u00b4e, M.,\net al. Bloom: A 176b-parameter open-access multilin-\ngual language model. arXiv preprint arXiv:2211.05100,\n2022a.\nScao, T. L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S.,\nBari, M. S., Bideman, S., Elsahar, H., Muennighoff, N.,\nPhang, J., et al. What language model to train if you have\none million gpu hours? arXiv preprint arXiv:2210.15424,\n2022b.\nSevilla, J., Heim, L., Ho, A., Besiroglu, T., Hobbhahn,\nM., and Villalobos, P. Compute trends across three eras\nof machine learning. arXiv preprint arXiv:2202.05924,\n2022.\nSites, D. Compact language detector 2. Software available\nat https://github. com/CLD2Owners/cld2 (last updated\non August 2015), 2013.\nTay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung,\nH. W., Narang, S., Yogatama, D., Vaswani, A., and Met-\nzler, D. Scale efficiently: Insights from pretraining and\nfinetuning transformers. In International Conference on\nLearning Representations, 2021.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y., et al. Lamda: Language models for dialog appli-\ncations. arXiv preprint arXiv:2201.08239, 2022.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023.\nTrinh, T. H. and Le, Q. V. A simple method for common-\nsense reasoning. arXiv preprint arXiv:1806.02847, 2018.\nVilares, D. and G\u00b4omez-Rodr\u00b4\u0131guez, C. Head-qa: A health-\ncare dataset for complex reasoning. In Proceedings of\nthe 57th Annual Meeting of the Association for Computa-\ntional Linguistics, pp. 960\u2013966, 2019.\nVillalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn,\nM., and Ho, A. Will we run out of data? an analysis of\nthe limits of scaling datasets in machine learning. arXiv\npreprint arXiv:2211.04325, 2022.\nWang,\nB.\nand\nKomatsuzaki,\nA.\nGPT-J-6B:\nA\n6\nBillion\nParameter\nAutoregressive\nLanguage\nModel.\nhttps://github.com/kingoflolz/\nmesh-transformer-jax, May 2021.\nWang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung,\nH. W., Beltagy, I., Launay, J., and Raffel, C. What lan-\nguage model architecture and pretraining objective work\nbest for zero-shot generalization? In International Con-\nference on Machine Learning, 2022.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., et al. Emergent abilities of large language models.\nTransactions on Machine Learning Research, 2022.\nWelbl, J., Liu, N. F., and Gardner, M.\nCrowdsourcing\nmultiple choice science questions. In Proceedings of the\n3rd Workshop on Noisy User-generated Text, pp. 94\u2013106,\n2017.\nThe RefinedWeb dataset for Falcon LLM\nWelbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J.,\nHendricks, L. A., Anderson, K., Kohli, P., Coppin, B.,\nand Huang, P.-S. Challenges in detoxifying language\nmodels. In Findings of the Association for Computational\nLinguistics: EMNLP 2021, pp. 2447\u20132469, 2021.\nWenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V.,\nGuzm\u00b4an, F., Joulin, A., and Grave, \u00b4E. Ccnet: Extract-\ning high quality monolingual datasets from web crawl\ndata. In Proceedings of the 12th Language Resources and\nEvaluation Conference, pp. 4003\u20134012, 2020.\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R.,\nSiddhant, A., Barua, A., and Raffel, C. mt5: A massively\nmultilingual pre-trained text-to-text transformer. In Pro-\nceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 483\u2013498, 2021.\nYang, G., Hu, E., Babuschkin, I., Sidor, S., Liu, X., Farhi,\nD., Ryder, N., Pachocki, J., Chen, W., and Gao, J. Tuning\nlarge neural networks via zero-shot hyperparameter trans-\nfer. Advances in Neural Information Processing Systems,\n34:17084\u201317097, 2021.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\nIn Proceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pp. 4791\u20134800,\n2019.\nZeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z.,\nJiang, X., Yang, Z., Wang, K., Zhang, X., et al. Pangu-\nalpha: Large-scale autoregressive pretrained chinese lan-\nguage models with auto-parallel computation.\narXiv\npreprint arXiv:2104.12369, 2021.\nZhang, S., Liu, X., Liu, J., Gao, J., Duh, K., and Van Durme,\nB. Record: Bridging the gap between human and machine\ncommonsense reading comprehension. arXiv preprint\narXiv:1810.12885, 2018.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. In Proceedings of the\nIEEE international conference on computer vision, pp.\n19\u201327, 2015.\nThe RefinedWeb dataset for Falcon LLM\nA. RefinedWeb Datasheet\nMOTIVATION\nFor what purpose was the dataset cre-\nated?\nRefinedWeb was created to serve as a large-scale dataset for the pretrain-\ning of large language models. It may be used on its own, or augmented\nwith curated sources (e.g., Wikipedia, StackOverflow).\nWho created the dataset and on behalf of\nwhich entity?\nThe dataset was created by the Technology Innovation Institute.\nWho funded the creation of the dataset?\nThe creation of the dataset was privately funded by the Technology\nInnovation Institute.\nAny other comment?\nRefinedWeb is built on-top of CommonCrawl, using the Macrodata Re-\nfinement Pipeline, which combines content extraction, filtering heuristics,\nand deduplication. In designing RefinedWeb, we abided to the following\nphilosophy: (1) Scale first. We intend MDR to produce datasets to\nbe used to train 40-200B parameters models, thus requiring trillions of\ntokens (Hoffmann et al., 2022). For English-only RefinedWeb, we target\na size of 3-6 trillion tokens. Specifically, we eschew any labour inten-\nsive human curation process, and focus on CommonCrawl instead of\ndisparate single-domain sources. (2) Strict deduplication. Inspired by\nthe work of Lee et al. (2022), which demonstrated the value of deduplica-\ntion for large language models, we implement a rigorous deduplication\npipeline. We combine both exact and fuzzy deduplication, and use strict\nsettings leading to removal rates far higher than others have reported.\n(3) Neutral filtering. To avoid introducing further undesirable biases\ninto the model (Dodge et al., 2021; Welbl et al., 2021), we avoid using\nML-based filtering outside of language identification. We stick to simple\nrules and heuristics, and use only URL filtering for adult content.\nCOMPOSITION\nWhat do the instances that comprise the\ndataset represent?\nInstances are text-only documents, corresponding to single web pages.\nHow many instances are there in total?\nRefinedWeb contains \u223c10 billion documents, or around 5 trillion tokens.\nThe public version is a subset representing a tenth of the full version.\nDoes the dataset contain all possible in-\nstances or is it a sample (not necessarily\nrandom) of instances from a larger set?\nRefinedWeb is built using all CommonCrawl dumps until the 2023-06\none; it could be updated with additional dumps as they are released. The\npublic release of RefinedWeb is a 600GT random extract of the 5,000GT\nof the full dataset. For all experiments, we randomly sampled from the\npublic extract, or earlier development versions of it.\nWhat data does each instance consist of?\nEach instance is a text-only document, with metadata about its origin in\nCommonCrawl and source page URL. We also distribute a multimodal\nversion of RefinedWeb, containing interlaced links to images.\nIs there a label or target associated with\neach instance?\nNo.\nIs any information missing from individ-\nual instances?\nNo.\nAre relationships between individual in-\nstances made explicit?\nNo.\nAre there recommended data splits?\nNo.\nThe RefinedWeb dataset for Falcon LLM\nAre there any errors, sources of noise, or\nredundancies in the dataset?\nDespite our best efforts to filter content that does not qualify as natural\nlanguage, and to deduplicate documents, our pipeline may let through\ndocuments that may be considered as errors or redundant.\nIs the dataset self-contained, or does it\nlink to or otherwise rely on external re-\nsources?\nThe base version of the dataset is self-contained, but the multimodal\nversion is interlaced with links to images\u2013these are not distributed as\npart of the dataset, and constitute an external source.\nDoes the dataset contain data that might\nbe considered confidential?\nAll documents in RefinedWeb have been publicly available online.\nDoes the dataset contain data that, if\nviewed directly, might be offensive, in-\nsulting, threatening, or might otherwise\ncause anxiety?\nYes, as this type of data is prevalent on the internet, it is likely our\ndataset contains such content. Notably, we estimate the prevalence of\ntoxic content in the dataset to be similar to The Pile (Figure 4).\nCOLLECTION\nHow was the data associated with each\ninstance acquired?\nWe downloaded with warcio publicly available .WET files from the\nCommonCrawl foundation.\nWhat mechanisms or procedures were\nused to collect the data?\nWe refer to the CommonCrawl website (commoncrawl.org) for de-\ntails on how they collect data.\nIf the dataset is a sample from a larger\nset, what was the sampling strategy?\nWhenever we use subsets, we randomly sample from the original data.\nWho was involved in the data collec-\ntion process and how were they compen-\nsated?\nThe original data collection was performed by CommonCrawl; authors\nfrom this paper were involved in retrieving it and preparing it.\nOver what timeframe was the data col-\nlected?\nWe use all CommonCrawl dumps from 2008 to January/February 2023.\nWere any ethical review processes con-\nducted?\nNo.\nPREPROCESSING\nWas any preprocessing/cleaning/labeling\nof the data done?\nYes, we applied extensive preprocessing and cleaning of the data. We\nfirst filter URLs to remove adult content using a blocklist and a score sys-\ntem (Appendix G.1), we then use trafilatura (Barbaresi, 2021)\nto extract content from pages, and perform language identification\nwith the fastText classifier from CCNet (Wenzek et al., 2020). Af-\nter this first preprocessing stage, we filter data using heuristics from\nMassiveWeb (Rae et al., 2021) and our own line-wise corrections (Ap-\npendix G.2). Finally, we run extensive deduplication, removing URLs\nrevisited across dumps (Section 3.3) and performing subsequently fuzzy\nand exact substring deduplication, with each stage drawing from Lee\net al. (2022). See Section 3 for further details and Table 2 for an outline.\nWas the \u201craw\u201d data saved in addition to\nthe preprocessed/cleaned/labeled data?\nDuring development, we saved intermediary outputs from our pipeline\nfor investigations and for ablations\u2013intermediary outputs exist for about\n5% of RefinedWeb. We did not keep intermediary outputs for the final\nproduction version of the dataset due to storage and resource constraints.\nIs the software that was used to prepro-\ncess/clean/label the data available?\nNo.\nUSES\nHas the dataset been used for any tasks\nalready?\nYes, this data has been used to develop large language models: both for\nscientific experiments (e.g., this paper) and production use.\nThe RefinedWeb dataset for Falcon LLM\nIs there a repository that links to any\nor all papers or systems that use the\ndataset?\nNo.\nWhat (other) tasks could the dataset be\nused for?\nRefinedWeb was built as a large-scale corpora representative of the web,\nand as such may see many downstream uses which are difficult to predict.\nIs there anything about the composition\nof the dataset or the way it was col-\nlected and preprocessed/cleaned/labeled\nthat might impact future uses?\nFor the public extract of RefinedWeb, we chose to only draw from the\nEnglish version of the dataset, preventing multilingual applications.\nAre there tasks for which the dataset\nshould not be used?\nAny tasks which may considered irresponsible or harmful.\nDISTRIBUTION\nWill the dataset be distributed to third\nparties outside of the entity on behalf of\nwhich the dataset was created?\nYes, we make a 600GT extract publicly available for NLP practitioners.\nWe currently don\u2019t plan to share the full version of the dataset.\nHow will the dataset will be distributed?\nThe dataset will be made available through the HuggingFace Hub.\nWhen will the dataset be distributed?\nThe dataset is available immediately.\nWill the dataset be distributed under\na copyright or other intellectual prop-\nerty (IP) license, and/or under applicable\nterms of use (ToU)?\nThe public extract is made available under an ODC-By 1.0 license;\nusers should also abide to the CommonCrawl ToU: https://\ncommoncrawl.org/terms-of-use/.\nHave any third parties imposed IP-based\nor other restrictions on the data associ-\nated with the instances?\nNot to our knowledge.\nDo any export controls or other regula-\ntory restrictions apply to the dataset or\nto individual instances?\nNot to our knowledge.\nMAINTENANCE\nWho\nwill\nbe\nsupport-\ning/hosting/maintaining the dataset?\nThe dataset will be hosted on the HuggingFace Hub, we have no plans\nto further support or maintain it once it is released.\nHow can the owner/curator/manager of\nthe dataset be contacted?\nfalconllm@tii.ae\nIs there an erratum?\nNo.\nWill the dataset be updated?\nNo.\nIf others want to extend/augment/build\non/contribute to the dataset, is there a\nmechanism for them to do so?\nNo.\nTable 6: Datasheet for RefinedWeb, following the framework introduced by Gebru et al. (2021).\nThe RefinedWeb dataset for Falcon LLM\nB. Falcon-RW Model Cards\nMODEL DETAILS\nPerson/organization\ndeveloping\nthe\nmodel\nThe models were created by the Technology Innovation Institute.\nModel date\nFalcon-RW models were trained in December 2022/January 2023.\nModel type and information about train-\ning\nFalcon-RW are autoregressive Transformer models trained with a causal\nlanguage modeling objective. Architecture based on GPT-3 (Brown et al.,\n2020), with ALiBi positional encodings (Press et al., 2021) and FlashAt-\ntention (Dao et al., 2022). See Section 4.1 for details.\nLicence\nApache\n2.0:\nhttps://www.apache.org/licenses/\nLICENSE-2.0.\nPoint of contact\nfalconllm@tii.ae\nINTENDED USE\nPrimary intended uses\nResearch on large language models, and the influence of adequately\nfiltered and deduplicated web data on the properties of large language\nmodels (fairness, safety, limitations, capabilities, etc.).\nPrimary intended users\nNLP researchers.\nOut-of-scope use cases\nProduction use without adequate assessment of risks and mitigation; any\nuse cases which may be considered irresponsible or harmful.\nFACTORS\nRelevant factors\nFalcon-RW models are trained on English data only, and will not gener-\nalize appropriately to other languages. Furthermore, as they are trained\non a large-scale corpora representative of the web, they will carry the\nstereotypes and biases commonly encountered online.\nEvaluation factors\nWe evaluated the toxicity of the underlying pretraining dataset and found\nit to be in line with common curated pretraining datasets such as The\nPile (see Figure 4). Note that this only accounts for toxicity under\nthe definition of Perspective API: \u201dcontent that is rude or disrespectful\u201d.\nNotably, this fails to include concerns about social biases or harmfulness.\nMETRICS\nModel performance measures\nWe focus our evaluation on measuring the zero-shot generalization ca-\npabilities of our models across a wide range of tasks, leveraging the\nEleuther AI language model evaluation harness (Gao et al., 2021).\nVariation approaches\nDue to the costs associated with training Falcon-RW we cannot train the\nmodels multiple times and measure variability across training runs.\nEVALUATION DATA\nDatasets\nWe evaluate zero-shot accuracy on 18 varied tasks, detailed in Table 3.\nMotivation\nWe selected and aggregated tasks to build comparisons with other models\nin the literature (see Section 4.1; Appendix F.1 for details).\nPreprocessing\nWe use the default prompts and setup of Gao et al. (2021).\nTRAINING DATA\nSee the dedicated datasheet in Table 6.\nTable 7: Model card for Falcon-RW, following the framework introduced by Mitchell et al. (2019).\nThe RefinedWeb dataset for Falcon LLM\nC. Dataset analysis\nThe large-scale and diverse nature of web corpora make them difficult to document and analyse as a whole; we provide\nsome key metrics in the section, focusing on document lengths in Figure 5(a), and a breakdown of the top domain names in\nFigure 5(b). We also refer to the analysis of the distribution of toxic content presented in Figure 4.\n100\n101\n102\n103\n104\n105\n106\nDocument length in tokens\nRe\ufb01nedWeb\nRW-Filtered\nRW-Raw\nThe Pile\nC4\nOSCAR 2019\nOSCAR 22.01\n(a) Document Lengths\nblogspot.com\nwordpress.com\ngoogle.com\nyoutube.com\nscribd.com\nissuu.com\nbiomedcentral.com\nyahoo.com\ntypepad.com\narchive.org\nwikia.com\nfanfiction.net\ncnn.com\nnytimes.com\nscience.gov\nufl.edu\nfandom.com\nslashdot.org\ntheguardian.com\ntumblr.com\nspringer.com\nblogspot.co.uk\nwashingtonpost.com\ndocstoc.com\nlivejournal.com\n4.86%\n2.42%\n0.33%\n0.27%\n0.25%\n0.24%\n0.20%\n0.20%\n0.17%\n0.16%\n0.15%\n0.14%\n0.14%\n0.13%\n0.12%\n0.11%\n0.11%\n0.11%\n0.11%\n0.10%\n0.10%\n0.09%\n0.09%\n0.09%\n0.09%\n(b) Top domains\nFigure 5. Make-up of RefinedWeb in document lengths (left) and top domains (right). (a) We find the OSCAR datasets and RW-Raw\nto have similar document length distributions; following filtering, most of the short documents are discarded from RW-Filtered. As\ndeduplication removes spans, it reintroduces shorter documents to RefinedWeb. We note the make-up of C4 and RefinedWeb to be\nrelatively similar, with a longer tail of short documents for RefinedWeb. Finally, The Pile exhibit a unique make-up, with a long tail of\nboth long (books, etc.) and short documents. (b) Top domains in RefinedWeb span from popular content platforms (Blogspot, WordPress,\nTumblr, etc.), to news websites (CNN, New York Times, etc.), and include also technical content such as BioMed Central or Springer.\nD. Multilingual RefinedWeb\nMultilingual data.\nUsing the language identification filter, we classify processed CommonCrawl data into 176 languages.\nFigure 6 shows the top 20 languages present in the data excluding English, based on their relative contribution in descending\norder. 58.20% of all documents in the processed CommonCrawl data were identified as English. We find the distribution of\nlanguages in CommonCrawl to only be partially aligned with the worldwide distribution of language speakers (Eberhard\net al., 2023): Russian is over-represented (2nd in CC but only 8th worldwide), Mandarin Chinese is under-represented (6-7th\nin CC but 2nd worldwide), and Hindi does not show-up in the top 20 despite being the 3rd most spoken.\n0\n2\n4\n6\n8\n10\n12\nPercentage (%) in no. documents\nRussian\nGerman\nSpanish\nJapanese\nFrench\nChinese\nItalian\nPortuguese\nDutch\nPolish\nCzech\nIndonesian\nTurkish\nVietnamese\nSwedish\nPersian\nArabic\nRomanian\nModern Greek\nKorean\nOthers\n13.19%\n10.81%\n9.45%\n9.03%\n8.88%\n8.46%\n4.69%\n4.27%\n3.42%\n3.17%\n2.01%\n1.91%\n1.89%\n1.61%\n1.45%\n1.31%\n1.21%\n1.09%\n1.07%\n1.04%\n11.08%\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nPercentage (%) in disk size\nRussian\nGerman\nJapanese\nSpanish\nChinese\nFrench\nItalian\nPortuguese\nPolish\nDutch\nIndonesian\nTurkish\nVietnamese\nCzech\nPersian\nModern Greek\nSwedish\nArabic\nRomanian\nHungarian\nOthers\n17.5%\n9.32%\n9.31%\n9.05%\n8.9%\n7.48%\n4.31%\n3.87%\n2.86%\n2.62%\n2.13%\n1.82%\n1.8%\n1.71%\n1.49%\n1.36%\n1.35%\n1.18%\n1.1%\n1.07%\n9.76%\nFigure 6. Top 20 languages (excluding English) from processed CommonCrawl based on number of documents and disk size.\nProcessing multilingual data.\nThe MDR pipeline can be used to process all languages: features such as text extraction\nare language-agnostic, whereas specific filters such as line-wise corrections need to typically be tuned for each individual\nlanguage. We also found tuning deduplication parameters for individual languages to be beneficial.\nThe RefinedWeb dataset for Falcon LLM\nE. Additional results\nIn this section, we present additional results obtained during the development of the Macrodata Refinement pipeline. For\nAppendix E.1 and Appendix E.3, these were obtained using earlier development versions of the dataset, so results are not\ndirectly comparable with the main text. For Appendix E.2, this is based on the Falcon-RW models.\nE.1. Small-scale ablations on deduplication approaches\nWe present results in Table 8\u2013the setup is similar to our earlier ablations, training 1B models for 30GT. We observe that:\n\u2022 MinHash alone is insufficient, as it doesn\u2019t match the zero-shot performance of exact deduplication. Conversely,\ncombining it with exact deduplication doesn\u2019t improve performance further.\n\u2022 Masking spanned duplicates degrades performance, systematically underperforming other approaches. Dropping\nand cutting spans perform similarly, although it\u2019s likely that dropping documents slightly outperforms cutting.\nFinally, we chose to apply MinHash before exact deduplication, as it is easier to scale: approximate deduplication acts as\na pruning phase, enabling us to scale deduplication further. Finally, we choose the common option of cutting spans, as\ndropping resulted in even more stringent rejection rates which would have compromised our ability to collect 5 trillion\ntokens.\nTable 8. MinHash alone is insufficient to match the performance of exact substring deduplication, and combining the two does\nnot significantly improve performance. Of all of the exact substring approaches, masking duplicated spans underperform, but\nall others exhibit similar performance. \u2713 Minhash + Exact substring-Cut corresponds to our final deduplication setup. Perplexity in\nbits-per-bytes on The Pile (pile-bpb, lower is better), zero-shot performance aggregated over LAMBADA, PIQA, and HellaSwag\n(agg-dev). Best results in bold, best results with minhash in underline, table sorted by increasing agg-dev-1.\nMinhash\nExact substring\npile-bpb \u2193\nagg-dev-1 \u2191\nRefinedWeb-Filtered\n1.11\n43.51\nMask\n1.08\n45.84\n\u2713\nMask\n1.07\n46.28\n\u2713\n1.07\n46.57\n\u2713\nCut\n1.05\n47.11\nCut\n1.06\n47.24\n\u2713\nDrop partial\n1.05\n47.25\nDrop any\n1.07\n47.77\n\u2713\nDrop any\n1.07\n47.86\nDrop partial\n1.06\n47.97\nPile\n0.88\n43.70\nE.2. Language modeling evaluation\nAlong with our aggregates, we also evaluated perplexity on Wikitext (Table 9). We found that models trained on RefinedWeb\nachieve performance close to that of models trained on The Pile. Importantly, we note that RefinedWeb does not contain any\ncontent from Wikipedia \u2013 it is explicitly filtered out at the URL level. We believe this accounts for most of the difference in\nperplexity, as RW models may not be familiar with the idiosyncrasies of Wikitext (e.g., layout of an article, etc.)\nTable 9. Models trained on RefinedWeb achieve performance close to models trained on The Pile on Wikitext, despite not having\nseen any content from Wikipedia. Perplexity in bits-per-bytes on Wikitext (wiki-bpb, lower is better.)\nModel size\n1B\n7B\nDataset\nThe Pile\nRW\nRW\nwiki-bpb \u2193\n0.64\n0.66\n0.60\nThe RefinedWeb dataset for Falcon LLM\nE.3. Does deduplication help with multiple epochs?\nEarlier in this work, we outlined that to scale pretraining data, practitioners had two choices: (1) improve data collection,\nwhich is the avenue we chose to pursue; (2) train models on multiple epochs of the same data. Due to current uncertainties\nin the ability of larger models to sustain multiple epochs without adverse effects (Hernandez et al., 2022), we focused on (1).\nA fairly rational question regarding (2) is whether deduplication may improve the situation, and whether deduplicated data\nmay be able to sustain more epochs without compromising model quality.\nWe train 1B parameters models on 30GT of RW and RW-Filtered. We keep the number of pretraining tokens fixed, but train\nfor 1, 5, 25, and 100 epochs. This is a small-scale, limited set-up, which would have to be improved to obtain definitive\nresults. We plot the degradation in performance compared to a single epoch in Figure 7(a) and the gap between RW and\nRW-F in Figure 7(b). We find that the absolute degradation is less important for RefinedWeb than for RefinedWeb-Filtered;\nfurthermore, the gap widens with increasing number of epochs. However, we observe significant variability across tasks.\n100\n101\n102\nEpochs\n\u221220.0\n\u221217.5\n\u221215.0\n\u221212.5\n\u221210.0\n\u22127.5\n\u22125.0\n\u22122.5\n0.0\nAbsolute 0-shot degradation [agg-dev-2]\nRW-F\nRW\n(a) Degradation compared to 1 epoch\n100\n101\n102\nEpochs\n\u22124\n\u22122\n0\n2\n4\n6\n8\n10\nAbsolute RW - RWF 0-shot gap [agg-dev-2]\nRW - RW-F\n(b) Gap between RW and RW-F\nFigure 7. Deduplication may reduce the degradation in performance incurred by multiple epochs. However, our experiments were\nonly performed at small-scale (1B models trained on 30GT), and we see high variability in outcomes across tasks. Zero-shot performance\nmeasured on the agg-dev-2 aggregate (HellaSwag, PIQA, ARC, BoolQ, COPA, MRPC, SciQ). Individual curves for per-task results\nand 1-\u03c3 standard deviation across all tasks in the aggregate in transparent.\nF. Tasks, models, and datasets from the state-of-the-art\nF.1. Task aggregates\nTo evaluate models, we average zero-shot performance over diverse task aggregates Our aggregates are outlined in Table 3:\n\u2022 small: small-scale ablation studies, taskswith non-zero performance for 1B parameters models trained on 30GT;\n\u2022 core: comparisons with a wide range of models, notably based on the tasks reported in (Dey et al., 2023);\n\u2022 main: tasks available in the GPT-3 and PaLM papers (Brown et al., 2020; Chowdhery et al., 2022);\n\u2022 ext: tasks available in the work of the BigScience Architecture and Scaling group (Scao et al., 2022b).\nWhen comparing with models from the state-of-the-art, we source results from a few different papers, detailed in Table 10.\nF.2. Models\nWe compare against nearly 50 models across 10 series trained on a variety of curated corpora, presented in Table 11.\nCerebras-GPT with \u00b5-parametrization.\nThe Cerebras-GPT series (Dey et al., 2023) also comes in a smaller series, up\nto 2.7B parameters, following the recommendations of \u00b5-parametrization (Yang et al., 2021). As we found the performance\nof this smaller series to be close to the main series of models (see Figure 8), and as it does not include models of a similar\ncompute scale as the ones we compare to, we chose not to report it in our main figures.\nThe RefinedWeb dataset for Falcon LLM\nTable 10. We source evaluation results from a variety of papers across the literature, maximizing task coverage. Although most\nresults come from the EAI Evaluation Harness (Gao et al., 2021), results from PaLM and GPT-3 are sourced from their respective papers.\nNote in Figure 1 that the results from the GPT-3 paper are still ahead of results obtained through the API with the EAI evaluation harness.\nModels\nAggregates reported\nSource of results\nEAI eval harness?\nOurs\nmain, core, ext\nThis paper\n\u2713\nBS-A&S\u2217\nmain, core\nScao et al. (2022b)\n\u2713\nGPT-Neo\u2217\nmain, core\nScao et al. (2022b)\n\u2713\nPaLM\u2020\nmain\nChowdhery et al. (2022)\nGPT-3 API\u2217\nmain, core\nScao et al. (2022b)\n\u2713\nGPT-3\u2020\nmain\nBrown et al. (2020)\nAleph Alpha\u2217\ncore\nAleph Alpha (2023)\n\u2713\nCerebras-GPT\u2217\ncore\nDey et al. (2023)\n\u2713\nFairSeq\u2217\ncore\nBlack et al. (2022)\n\u2713\nPythia(-Dedup)\u2217\ncore\nDey et al. (2023)\n\u2713\nOPT\u2217\ncore\nDey et al. (2023)\n\u2713\nGPT-J\u2217\ncore\nBlack et al. (2022)\n\u2713\nGPT-NeoX 20B\u2217\ncore\nBlack et al. (2022)\n\u2713\nPythia and deduplication.\nThe Pythia series of models is available in two flavours: one trained on the vanilla version\nof The Pile, and another trained on a version deduplicated with MinHash. Performance between these two flavours was\nnoted to minimally differ (Biderman et al., 2023); in Figure 9, we find the deduplicated version may be slightly ahead of\nthe non-deduplicated one under our aggregate. The higher end of this improvement is broadly in line with our findings in\nTable 5. Nevertheless, a difference in our findings and theirs remain. We posit a few possible hypotheses:\n\u2022 Differences between curated and web data. It is possible that web data is more sensitive to duplicates. For instance,\nthe most common duplicates in web data (e.g., spam) may be more detrimental than the most common duplicates in\ncurated data. This suggests a qualitative component to deduplication that we have not studied in this work.\n\u2022 Differences in deduplication pipeline. Because Biderman et al. (2023) uses the MinHash settings from Lee et al.\n(2022), they are mostly identical to ours. However, we also apply exact deduplication: while their deduplication incurs\na 30% reduction in size, our deduplication is more aggressive, resulting in a 45% reduction in size. This may explain\nwhy our results in Table 5 show a stronger gain from deduplication than theirs in Figure 9.\n\u2022 Differences in pretraining. Finally, we note that Biderman et al. (2023) chooses to perform a partial extra epoch\non the deduplicated data to reach 300GT, while we always perform a single epoch. Their setting corresponds to a\ndata-constrained scenario, which is more realistic for the curated data they study; for us, web data is plentiful, so\ndeduplication never truly limits the size of the datasets we can use.\nF.3. Datasets\nWe extend on Table 1 in Table 12, providing details on the filtering and deduplication strategies used across the litterature.\nThe RefinedWeb dataset for Falcon LLM\n10\u22122\n10\u22121\n100\n101\nCompute [PF-days]\n\u22121\n0\n1\n2\n3\n4\nAbsolute \u00b5-param 0-shot improvement [core-agg]\n\u00b5-param improvement\nFigure 8. \u00b5-parametrization (Yang et al., 2021) slightly improves performance in the Cerebras-GPT series (Dey et al., 2023).\nZero-shot performance on our core aggregate, gap between Cerebras-GPT with \u00b5-param and without. Individual curves for per-task\nresults and 1-\u03c3 standard deviation across all tasks in the aggregate in transparent.\n100\n101\n102\nCompute [PF-days]\n\u22122\n\u22121\n0\n1\n2\n3\nAbsolute deduplication 0-shot improvement [core-agg]\ndeduplication improvement\nFigure 9. In our core aggregate, deduplication brings a small improvement to the Pythia suite (Biderman et al., 2023). Zero-shot\nperformance on our core aggregate, gap between Pythia trained on the deduplicated and vanilla Pile. Individual curves for per-task\nresults and 1-\u03c3 standard deviation across all tasks in the aggregate in transparent.\nThe RefinedWeb dataset for Falcon LLM\nTable 11. Full-scale models trained on RefinedWeb (Falcon-RW) and other models from the state-of-the-art. Across models trained\non The Pile, the Pythia models are the closest to our achitecture: they use FlashAttention with rotary embeddings\u2013with for only notably\nexception the use of parallel attention and feedforward for their models. Training budget C in PF-days calculated using C = 6ND, with\nN the number of parameters, and D the pretraining dataset size (Kaplan et al., 2020).\nSeries\nGPT-3 (paper)\u2020\nGPT-3 (API)\u2217\nBigScience\u2217\nPaLM\u2020\nOurs\nModel\nXL\nXXL\nbabbage\ncurie\nBS-A&S\nPaLM-8B\nOurs (Pile)\nFalcon-RW\nDataset\nGPT-3\nGPT-3\nGPT-3\nGPT-3\nPile\nPaLM\nPile\nRW\nRW\nParams.\n1.3B\n6.7B\n1.3B\n6.7B\n1.3B\n8.6B\n1.3B\n1.3B\n7.5B\nPretraining\n300GT\n300GT\n300GT\n300GT\n300GT\n780GT\n350GT\n350GT\n350GT\nPF-days\n27\n140\n27\n140\n27\n466\n32\n32\n182\nCitation\nBrown et al. (2020)\nScao et al. (2022b)\nChowdhery et al. (2022)\nThis paper\nSeries\nEleutherAI\u2217\nPythia\u2217\nModel\nGPT-Neo\nGPT-J\nGPT-NeoX 20B\nPythia(-Dedup)\nDataset\nPile\nPile\nPile\nPile (dedup)\nParams.\n1.3B\n6.7B\n20B\n70M-12B\nPretraining\n380GT\n402GT\n472GT\n300GT\nPF-days\n34\n187\n656\n1.5 - 250\nCitation\nBlack et al. (2021)\nWang & Komatsuzaki (2021)\nBlack et al. (2022)\nBiderman et al. (2023)\nSeries\nAleph Alpha\u2217\nCerebras-GPT\u2217\nOPT\u2217\nFairSeq\u2217\nModel\nLuminous\nCerebras-GPT\nOPT\nFairSeq\nDataset\nundisclosed\nPile\nPile (subset) + curated\ncurated\nParams.\n13B\n111M-13B\n125M - 175B\n1.3 - 13B\nPretraining\n400GT\n2 - 257GT\n300GT\n300GT\nPF-days\n361\n0.02 - 232\n3 - 3646\n27 - 271\nCitation\nAleph Alpha (2023)\nDey et al. (2023)\nZhang et al. (2022)\nArtetxe et al. (2021)\nThe RefinedWeb dataset for Falcon LLM\nTable 12. Common massive web-scrape and LLM English datasets. Datasets such as OSCAR and C4 also have significant multilingual versions, which have enjoyed wide\nadoption (Xue et al., 2021). For OSCAR, the size corresponds to the non-deduplicated version, and is estimated from the number of words x0,75 (average number of words per tokens).\nGeneral information\nWeb data\nDataset\nNotable models\nSize\nAvailability\nWeb\nHTML extraction\nLanguage ID\nHeuristics\nContent filtering\nDeduplication\nMASSIVE WEB DATASETS\nC4 (Raffel et al.,\n2020)\nT5 (Raffel et al.,\n2020)\n\u223c 360GT\nPublic\n100%\n.WET files\nDocument-\nlevel\nw/\nlangdetect\nDocument and\nline-level\nRules-based:\ncode, NSFW\nExact:\nthree\nsentences span\nOSCAR 21.09\n(Ortiz\nSu\u00b4arez\net al., 2019)\n\u223c 370GT\nPublic\n100%\n.WET files\nLine-level\nw/\nfastText (Joulin\net al., 2016)\nLine\n<\n100\ncharacters\nNone\n(optional)\nExact:\nper\nline (\u223c\n55%\nremoved)\nOSCAR 22.01\n(Abadji et al.,\n2022)\n\u223c 283GT\nPublic\n100%\n.WET files\nDocument-\nlevel\nw/\nfast-\nText\n(Joulin\net al., 2016)\nLine-level,\noptional\ndocument-level\nOptional NSFW\nblocklist\n(optional)\nEx-\nact: per line\nCURATED DATASETS\n\u25a0 GPT-3 (Brown et al., 2020)\n300GT\nPrivate\n60%\nUnknown\nUnknown\nUnknown\nfastText\ntrained\non HQ-data\nFuzzy:\nmin-\nhash\nwith\n10\nhashes (\u223c 10%\nremoved)\n\u25bc\nThe\nPile\n(Gao\net\nal.,\n2020)\nGPT-J (Wang &\nKomatsuzaki,\n2021),\nGPT-\nNeoX-20B\n(Black\net\nal.,\n2022),\nPythia\n(Biderman\net\nal.,\n2023),\nCerebras-GPT\n(Dey\net\nal.,\n2023)\n\u223c 340GT\nPublic\n18%\njusText\n(Pomik\u00b4alek,\n2011)\nDocument-\nlevel\nw/\npycld2\n(Sites, 2013)\nNone\nfastText on cu-\nrated crawl\nFuzzy:\nmin-\nhash\nwith\n10\nhashes,\nsim.\ntreshold\n0.5\n(\u223c\n26%\nre-\nmoved)\nMassiveWeb\n(Rae\net\nal.,\n2021)\nGopher\n(Rae\net\nal.,\n2021),\nChinchilla\n(Hoffmann\net al., 2022)\n1, 400GT\nPrivate\n48%\nCustom\nUnknown\nDocument-\nlevel\nSafeSearch\nExact & fuzzy:\nexact\ndocu-\nments, minhash\nw/ sim. treshold\n0.8\n\u22c6 PaLM (Chowdhery et al., 2022)\n780GT\nPrivate\n27%\nUnknown\nUnknown\nDocument-\nlevel\nML-based\nfilter\non HQ data\nUnknown\nOURS\n REFINEDWEB Falcon-RW\n5,000GT\n600GT Public\n100%\ntrafilatura\n(Barbaresi, 2021)\nFrom\nCCNet\n(Wenzek et al.,\n2020)\nDocument and\nline-level\nURL blocklist\nExact & fuzzy\nThe RefinedWeb dataset for Falcon LLM\nG. Details of the Macrodata Refinement pipeline\nG.1. URL filtering\nAs discussed in Section 3.1, we base our filtering of adult documents only on the URL itself, and not on the content of\nthe documents. This design choice was motivated by: (1) challenges in avoiding overfiltering content from minorities\nwhen using ML-based classifiers on the content of documents (Welbl et al., 2021); (2) NSFW words block-list applied on\ncontent (such as the one used in C4) also resulting in overfiltering of legal and medical content (Dodge et al., 2021).\nOur URL filtering focuses on finding domains that are related to adult content, that may be harmful to users, or that are very\nlikely to contain mostly unstructured text/spam (e.g., file hosting websites). First, we aggregated a list of 4.6M domains,\ndetailed in Appendix G.1.1, that we explicitly ban; then, we built a simple URL scoring system, based on matching subwords\nin the URL against a list of words we curated (see Appendix G.1.2). We curated this list of words based on manual inspection,\ncross-referencing results with pages surfaced by ToxicBERT as being outliers in toxicity (Hanu & Unitary team, 2020).\nG.1.1. URL BLOCKLIST\nOrigin of the list.\nWe use an aggregated list\u2020 of about 4.6M URLs that we explicitly ban. This list is broken in categories\n(e.g. pornography, gambling); we outline the categories we selected in Table 13. The list is regularly updated, with an\noriginal intended usage as a blocklist for universities.\nCuration.\nWe noticed the list blocked a number of domains inappropriately; while these domains were few (<100), they\naccounted for a significant portion of the data filtered by the list, as these were rather prolific domains, with thousands\nof pages of content. To identify these false positive domains, we applied the blocklist to a subset of 832M pages. 6.04M\n(0.73%) pages matched with the blocklist, and the number of occurrences per URL ranged from 1 to 79k. We manually\ninspected all URLs matched more than 4k times, which represented an appreciable portion of the dataset. We found a\nnumber of benign domains, such as pop culture news websites, or blogging platforms, which we removed from the list.\nTable 13. We select categories likely to contain adult or malicious content, as well as spam or unstructured text.\nCategory\nDescription\nNumber of links\nadult\nadult websites: from eroticism to hard pornography\n4516478\nphishing\nphishing websites, malwares, etc.\n42445\ndating\ndating websites\n3829\ngambling\nonline casino\n1365\nfilehosting\nwebsites hosting files, videos, pictures, music\n909\nddos\nwebsites related to ddos attacks\n421\nagressif\nhate, racism, etc\n390\nchat\nonline chat websites\n244\nmixed adult\nwebsites with some adult content\n153\narjel\nFrench regulated gambling websites\n69\nG.1.2. URL SCORING WITH A WORD-LIST\nTo score URLs, we used three matching patterns based on a soft, hard, and strict violation word-list:\n\u2022 Strict subword matching: http://foobann.edsub-wo.rdbar.com/any/bar, matching words such as xvideos, groupsex;\n\u2022 Hard whole word matching: http://www.foo.bannedword-bar.com, with words such as porn, xxx, orgy;\n\u2022 Soft words matching: http://www.foo.soft1-bar-soft2.com, with \u201dsofter\u201d words such as sex, webcam, escort.\nEach list is associated with a different level of severity: for the strictest one (strict subword matching), we ban any URL\nmatching a banned word in its substrings (as fraudulent websites may attempt to escape similar recognition schemes by\nbreaking-up adult keywords); for the hard whole word matching, we ban URLs with a whole word matching in the list;\nfinally, a minimum of two matches are required with the soft word matching.\n\u2020https://dsi.ut-capitole.fr/blacklists/\nThe RefinedWeb dataset for Falcon LLM\nWe curated the lists based on manual inspection of the data, informed by top hits reported by ToxicBERT. For the strict\nsubword matching, we included words that were unequivocally related to adult content (e.g., groupsex). We avoided\npartial unclear matches (e.g., ass), that may be part of neutral words (e.g., massachusetts). In the soft word list, we\nincluded words that do not constitute a sufficient reason to discard the document on their own, but which are suspicious\nwhen multiple words from the list result in a match. This helped with keeping medical or legal content unaffected (e.g., a\nsingle match of dick).\nG.1.3. EXCLUDED HIGH QUALITY SOURCES\nSince our paper focuses on the study of RefinedWeb alone, we chose to exclude common online sources of curated data\nfrom it. This serves two objectives: (1) it strengthens our results, by ensuring that RefinedWeb doesn\u2019t end-up actually being\nmade mostly of known high-quality sources (e.g., Wikipedia represents a significant portion of C4); (2) future works may be\ninterested in combining RefinedWeb with existing curated copora, which would require further deduplication if they are\nincluded in RefinedWeb. Accordingly, we remove common sources used in The Pile (Gao et al., 2020) from RefinedWeb.\nThe full list of curated data sources domains that we blocked is in Table 14.\nTable 14. RefinedWeb is stripped from common so-called high-quality sources to simplify combining it with existing curated\ncorpora. This blocklist is applied at the URL filtering stage, along with the adult content blocklist.\nCurated data source\nDomain name blocked\narxiv\narxiv.org\nAskUbuntu\naskubuntu.com\nStackOverflow\nstackoverflow.com\nstackapps.com\nstackexchange.com\nmathoverflow.net\nNIH Abstracts\nexporter.nih.gov\nncbi.nlm.nih.gov\nGithub\ngithub.com\nUbuntu IRC\nirclogs.ubuntu.com\nHackerNews\nnews.ycombinator.com\nFreeLaw\ncourtlistener.com\nReddit\nreddit.com\nEuroparl\nstatmt.org\nUnited States Patents\nuspto.gov\nWikipedia\nwikipedia.org\nG.2. Line-wise filtering\nDespite the improvements brought forth by running text extraction with Trafilatura, we found that a number of irrelevant\nlines still seeped through. These lines are usually related to navigation menus, call to actions, or social media counters.\nFollowing manual inspection of the data, we devised a line-wise filtering strategy. We analyse documents line-by-line, and\ndiscard or edit the lines based on the following rules:\n\u2022 If it is mainly composed of uppercase characters (discard);\n\u2022 If it is only composed of numerical characters (discard);\n\u2022 If it is a counter (e.g. 3 likes) (discard);\n\u2022 If it only contains one word (discard);\n\u2022 If it is short (\u2264 10 words) and matches a pattern (edit):\n\u2013 At the beginning of the line (e.g. sign-in);\n\u2013 At the end of the line (e.g. Read more...);\n\u2013 Anywhere in the line (e.g. items in cart).\nFinally, if the words in the flagged lines represent more than 5% of the total document words, the document is discarded.\nWe derived these filters through manual inspection of the data, and note that they require adaptation across languages.\nThe RefinedWeb dataset for Falcon LLM\nG.3. Deduplication\nWe make use of the two deduplication methods described in Lee et al. (2022): EXACTSUBSTR and NEARDEDUP (detailed\nin Appendix G.3.1 and Appendix G.3.2; see Appendix H for samples of duplicates).\nWe start with the most scalable approach, NEARDEDUP. We remove similar documents by applying MinHash (Broder,\n1997), whereby a signature/sketch supporting efficient approximate similarity queries is computed for each document in the\ndataset, and document pairs with a high n-gram overlap are identified.\nWe then use EXACTSUBSTR, leveraging the implementation from Lee et al. (2022)\u2021, to identify ranges of exact duplicate\ntext of at least 50 tokens. We experiment with three different approaches for these ranges: EXACTSUBSTR-CUT, where we\nremove them from the original text, as done in the original implementation; EXACTSUBSTR-MASK, where the dataset is\nunchanged but we do not compute the loss on the duplicated ranges; and EXACTSUBSTR-DROP, where we simply drop an\nentire document if the duplicated ranges make up more than a certain percentage of its content.\nWe present small-scale ablations around these different approaches in Appendix E.1.\nG.3.1. MINHASH APPROXIMATE MATCHING\nWe employ MinHash to find approximate duplicate documents in our web corpora at a very large scale. This technique\nallows us to identify templated pages or otherwise very similar content where most of the interspersed duplicated sections\nare small enough to not be identified by exact matching methods (anything smaller than 50 tokens).\nSigning.\nWe start by normalizing the content to increase recall: punctuation is removed, text is lowercased, NFD Unicode\nnormalization is applied, accents are removed, and all whitespace is normalized. We tokenize the resulting text using the\nGPT-2 tokenizer (Radford et al., 2019) and obtain the set of unique n-grams for each document. Hash functions are used to\nobtain a signature for each document: for each hash function, the smallest value is kept from hashing every unique n-gram\nin the document. If two documents are similar, then there is a high probability that they will have the same minimum\nhash (MinHash) for at least some of the hash functions used (Broder, 1997). The ratio of matching hashes between two\ndocuments approximates the Jaccard Similarity (Jaccard, 1912) of the sets of their unique n-grams (the sets being di and dj):\nJ(di, dj) = |di \u2229 dj|\n|di \u222a dj|\n(1)\nMatching.\nSince comparing MinHash signatures between every possible document pair is computationally expensive, we\napply a locality sensitive hashing version of MinHash, MinHash LSH. A document signature is split into r buckets, each\nwith b minhashes. Documents are indexed by these b minhashes on each of the r buckets, and we mark two documents\nas duplicates if their b minhashes are exactly the same on at least one of the buckets. These two parameters, b and r, will\ndetermine the probability that similar documents will be detected. For two documents i and j whose ratio of matching\nhashes between their MinHash signatures is si,j, the probability that there is a match in a given bucket is sb\ni,j; the probability\nthat there isn\u2019t a match in any of the buckets is (1 \u2212 sb\ni,j)r; and finally that there is a match in at least one of the buckets:\nP = 1 \u2212 (1 \u2212 sb\ni,j)r\n(2)\nWe use the same parameters as Lee et al. (2022): n = 5 (5-grams); b = 20 and r = 450. This means that for each document,\nwe compute a total of 9000 minhashes, and that the probability that a document pair with similarity 0.75 or 0.8 will be\nmarked as duplicates will be 76% and 99.4% (respectively), diminishing rapidly for smaller similarity values.\nFinally, we cluster documents across all buckets \u2014 if documents A and B match in one bucket and B and C in another,\nA-B-C becomes a cluster. We randomly remove all but one of the documents in each cluster.\nLee et al. (2022) also proposed filtering down on false positives by computing the real Jaccard similarity, or other metrics\nsuch as the edit similarity between identified document pairs. Given the large amount of data we have available across all of\nCommonCrawl, and that our main concern is improving recall, we decided to skip this additional step.\n\u2021https://github.com/google-research/deduplicate-text-datasets\nThe RefinedWeb dataset for Falcon LLM\nG.3.2. EXACT SUBSTRING DEDUPLICATION\nWe make use of the EXACTSUBSTR implementation publicly released by Lee et al. (2022) for exact text matching. We apply\nexact substring deduplication to data that has already been deduplicated by MinHash, reducing by nearly 40% size of the\ndataset on which we have to operate. EXACTSUBSTR will find long strings of text that are present, character for character,\nacross multiple documents. Some of these may have escaped the earlier stage of approximate deduplication: they might not\nconstitute a big enough portion of the document; one document might have repeated sections sourced across many different\ndocuments; or they may simply not have been found due to the approximate nature of MinHash.\nFinding duplicates.\nEXACTSUBSTR concatenates all the documents in the dataset to create a single long text sequence;\nthen, it builds a suffix array (Manber & Myers, 1993) in linear time\u2014an array of the indexes to a lexicographical ordering\nof all the suffixes in the sequence. Finally, duplicate sequences can also be found in linear time using the suffix array, by\nsimply traversing the ordered list of suffixes and comparing the beginning of each pair of two consecutive suffixes.\nWe apply the same normalization and tokenization as for MinHash to the content of our documents before concatenating\nthem. One important difference is that reversibility is important: for MinHash, we were discarding entire documents, and\nthus never relying on the normalized+tokenized representation for downstream use. Here, once we have identified duplicate\nnormalized+tokenized spans, we need to revert to the original span to remove it. Accordingly, we include normalization in\nthe tokenization process, and validate that the process is reversible.\nIf a match is longer than 50 tokens, there will be multiple overlapping duplicated ranges. These overlapping duplicated\nranges in the concatenated dataset sequence are merged before we save them to a file. We then take these ranges and retrieve\nthe original document that produced them, obtaining the character substrings corresponding to the duplicated token ranges.\nRemoving duplicates.\nWe considered applying the following transformations to the duplicate spans:\n\u2022 EXACTSUBSTR-CUT: we remove the duplicated spans, and discard documents where there are fewer than 20 non-\nduplicated characters left\u2013this is the vanilla setting used by Lee et al. (2022);\n\u2022 EXACTSUBSTR-MASK: we loss-mask the duplicated spans, preventing a loss from being computed on the duplicated\ntext during pretraining, and discard documents where there are fewer than 20 non-masked characters left.\n\u2022 EXACTSUBSTR-DROPPARTIAL: if more than 20% of the document is duplicated, we remove the entire document;\n\u2022 EXACTSUBSTR-DROPANY: we drop any document with a duplicated span in it.\nBroadly speaking, EXACTSUBSTR-CUT might remove text mid-sentence resulting in disconnected text; EXACTSUBSTR-\nMASK does not have this issue, but might be less efficient as a significant portion of the training tokens will not directly\ncontribute to updating the model\u2019s weights; EXACTSUBSTR-DROP might still keep considerable duplicated sections in its\nPARTIAL version, especially on larger documents, while the ANY version might be overly aggressive. Following ablations\nin Appendix E.1, we choose to stick with the vanilla approach, EXACTSUBSTR-CUT.\nNote that in all cases, while MinHash keeps one copy of the duplicated documents, our exact deduplication removes all\ncopies of the duplicated span.\nG.4. Execution environment\nMost data processing took place in large CPU clusters, with 100-250 AWS c5.18xlarge instances; each instance has 72\nvCPUs and 144 GiB of memory. We usually run with 10,000-20,000 vCPUs in the cluster, enabling rapid parallel processing.\nFor EXACTSUBSTR, the entire dataset being deduplicated needs to be loaded onto memory: we leveraged the AWS x2iedn\ninstances, which come with up to 2 TiB of memory in a single instance.\nThe RefinedWeb dataset for Falcon LLM\nH. Deduplication samples from RefinedWeb\nH.1. MinHash clusters\nWe report the 8 largest duplicate clusters found by MinHash in Table 15 \u2013 each spanning hundreds of thousands of documents.\nWe also found a large number of duplicate document pairs to be due to different URL GET parameters not resulting in\nsignificantly different content. An example of this behaviour can be seen in the URLs presented in Table 16.\nTable 15. Top-8 largest MinHash clusters found when building RefinedWeb. We cut some of the longest samples in the interest of\nreadability, only keeping a brief description.\nDescription\nExample document\nWordpress sitemap notice generated by the Google Sitemap\nGenerator Plugin\nThis is a XML Sitemap which is supposed to be processed by search\nengines which follow the XML Sitemap standard like Ask.com, Bing,\nGoogle and Yahoo. It was generated using the WordPress content\nmanagement system and the Google Sitemap Generator Plugin by\nArne Brachhold. You can find more information about XML sitemaps\non sitemaps.org and Google\u2019s list of sitemap programs. This file\ncontains links to sub-sitemaps, follow them to see the actual sitemap\ncontent.\nCloudflare notice to enable Javascript\nTemplated disability notice, with different phone numbers\nacross pages\nWelcome to our website! As we have the ability to list over one\nmillion items on our website (our selection changes all of the time),\nit is not feasible for a company our size to record and playback the\ndescriptions on every item on our website. However, if you are an\nAmerican with a disability we are here to help you. Please call our\ndisability services phone line at [redacted] or [redacted] during regular\nbusiness hours and one of our kind and friendly personal shoppers\nwill help you navigate through our website, help conduct advanced\nsearches, help you choose the item you are looking for with the\nspecifications you are seeking, read you the specifications of any item\nand consult with you about the products themselves. There is no\ncharge for the help of this personal shopper for any American with\na disability. Finally, your personal shopper will explain our Privacy\nPolicy and Terms of Service, and help you place an order if you so\ndesire.\nTemplated cookies notice\nTemplated domain name for sale page\nwww.metoperashop.org and sub-URLs, with content\nchanges but always the same (large) footer\nDifferent pages across more than 80 different domain names\nbut with a common section\nDC Customers also liked: Special event items are produced by man-\nufacturers only after the outcome of a game or event. These are\nadvanced sale items and will ship immediately after they are received\nin our warehouse. Manufacturer direct items are shipped directly\nfrom the manufacturer. These items are not available for international\nor expedited shipping. Customized items can be personalized with\noptions such as your name, your favorite number, and/or designs.\nSome options may be limited by league rules.\nhttp://www.boxofficemojo.com/daily and sub-\nURLs\nThe RefinedWeb dataset for Falcon LLM\nTable 16. URL with different GET parameters don\u2019t always result in significantly different page content.\nhttp://gamesandbiz.blogspot.com/2010/\n07/bad-reviews-can-hurt-game-sales.ht\nml?showComment=1278486430242\nhttp://gamesandbiz.blogspot.com/2010/\n07/bad-reviews-can-hurt-game-sales.ht\nml?showComment=1278499674195\nhttps://www.ocean-oxygen.org/home;jse\nssionid=1E3290E84F668552FAC643D0A8F81\nBEC?p_p_id=122_INSTANCE_Zy6zjkRLAg7v&\np_p_lifecycle=0&p_p_state=normal&p_p_\nmode=view&p_p_col_id=column-2&p_p_col\n_pos=1&p_p_col_count=6&p_r_p_56423352\n4_resetCur=true&p_r_p_564233524_categ\noryId=1346016\nhttps://www.ocean-oxygen.org/home?p_p\n_id=122_INSTANCE_Zy6zjkRLAg7v&p_p_lif\necycle=0&p_p_state=normal&p_p_mode=vi\new&p_p_col_id=column-2&p_p_col_pos=1&\np_p_col_count=6&p_r_p_564233524_reset\nCur=true&p_r_p_564233524_categoryId=1\n346016\nH.2. Exact substring matches\nExamples of exact matches found by exact substring deduplication can be seen in Table 17.\nThe RefinedWeb dataset for Falcon LLM\nTable 17. Matches found by exact substring deduplication (in italics).\nit appears there is a transfer of ranking signals in this rela-\ntionship. Supporting this finding is a quote from Google\u2019s\nguidelines: Using JavaScript to redirect users can be a\nlegitimate practice. For example, if you redirect users to an\ninternal page once they\u2019re logged in, you can use JavaScript\nto do so. When examining JavaScript or other redirect meth-\nods to ensure your site adheres to our guidelines, consider\nthe intent. Keep in mind that 301 redirects are best when\nmoving your site, but you could use a JavaScript redirect\nfor this purpose if you don\u2019t have access to your website\u2019s\nserver. NOTE: Their experiment is based on a live page\nwith status code 200 and NOT an inactive page. So if you\nwant to implement this for legacy\nSome examples of sneaky redirects include: - Search en-\ngines shown one type of content while users are redirected\nto something significantly different. - Desktop users receive\na normal page, while mobile users are redirected to a com-\npletely different spam domain. Using JavaScript to redirect\nusers can be a legitimate practice. For example, if you redi-\nrect users to an internal page once they\u2019re logged in, you\ncan use JavaScript to do so. When examining JavaScript\nor other redirect methods to ensure your site adheres to\nour guidelines, consider the intent. Keep in mind that 301\nredirects are best when moving your site, but you could\nuse a JavaScript redirect for this purpose if you don\u2019t have\naccess to your website\u2019s server.\nFind Palm Beache FL homes for sale and other Palm Beach\nreal estate on homesofthepalmbeaches.com. Browse and\nsearch Palm Beach houses, condos, townhomes and single-\nfamily homes by community , building, or location. Our\nextensive database of real estate listings provide the most\ncomprehensive property details including home values, fea-\ntures and local school and neighborhood info so you can\nbe sure that you have nearly all the facts you need upfront.\nSearch homesofthepalmbeaches.com today! Want a closer\nlook at what other Palm Beach properties are available?\nSearch Stuart houses, condos, townhomes and single-family\nhomes by price and location. Our extensive database of\nreal estate listings provide the most comprehensive property\ndetails including home values, features and local school\nand neighborhood info so you can be sure that you have\nnearly all the facts you need upfront. Search Stuart Listings\ntoday! Want a closer look at what other Stuart properties\nare available? Also search our listings for the Newest Stuart\nListings and Stuart Homes with Price Reductions now. Stu-\nart FL Homes for Sale - Stuart Real Estate Listings FREE\nto search Stuart Property\nTo find the correct size you should measure your foot from\nthe heel to the toe point. Add approximately 1 - 1,5cm to get\nthe actual inner sole length. Measure both feet and fit shoes\nto the larger foot. Measure feet at the end of the day, when\nyour feet are at their largest. Lente shoes are women\u2019s easy\nslip-on leisure shoes for everyday use. These lightweight\nshoes have a breathable textile mesh upper made of recycled\nPET bottles and cool Lycra lining.\nTo find the correct size you should measure your foot from\nthe heel to the toe point. Add approximately 1 - 1,5cm to get\nthe actual inner sole length. Measure both feet and fit shoes\nto the larger foot. Measure feet at the end of the day, when\nyour feet are at their largest. Enjoy your summer days with\nMasera leisure sneakers. These low-cut women\u2019s sneakers\nare extremely lightweight thanks to phylon midsole and\nbreathable textile mesh upper\nThis bandana makes the perfect addition to every fur babies\nbirthday collection! With its sparkly crown pattern, your\npup will be ready for every birthday celebration! With snaps\nfor security, this bandana is made with love, down to the\nvery last stitch ! Fabric: cotton Care Instructions: Hand\nwash only, iron as needed, on low heat Always supervise\nyour pup while wearing Faithful Paws Co. accessories, as\nit could become a choking hazard if consumed.\nThis bandana makes the perfect addition to every fur babies\nsummer collection! With its vibrant watercolor popsicle\npattern, your pup will be ready for every summer cook-\nout! With snaps for security, this bandana is made with\nlove, down to the very last stitch ! Fabric: cotton Care\nInstructions: Hand wash only, iron as needed, on low heat\nAlways supervise your pup while wearing Faithful Paws\nCo. accessories, as it could become a choking hazard if\nconsumed.\n"
  },
  {
    "title": "Segment Anything in High Quality",
    "link": "https://arxiv.org/pdf/2306.01567.pdf",
    "upvote": "6",
    "text": "Segment Anything in High Quality\nLei Ke\u22171,2\nMingqiao Ye\u22171\nMartin Danelljan1\nYifan Liu1\nYu-Wing Tai3\nChi-Keung Tang2\nFisher Yu1\n1ETH Z\u00fcrich\n2HKUST\n3Dartmouth College\nAbstract\nThe recent Segment Anything Model (SAM) represents a big leap in scaling up\nsegmentation models, allowing for powerful zero-shot capabilities and flexible\nprompting. Despite being trained with 1.1 billion masks, SAM\u2019s mask prediction\nquality falls short in many cases, particularly when dealing with objects that have\nintricate structures. We propose HQ-SAM, equipping SAM with the ability to\naccurately segment any object, while maintaining SAM\u2019s original promptable\ndesign, efficiency, and zero-shot generalizability. Our careful design reuses and\npreserves the pre-trained model weights of SAM, while only introducing minimal\nadditional parameters and computation. We design a learnable High-Quality Output\nToken, which is injected into SAM\u2019s mask decoder and is responsible for predicting\nthe high-quality mask. Instead of only applying it on mask-decoder features, we\nfirst fuse them with early and final ViT features for improved mask details. To train\nour introduced learnable parameters, we compose a dataset of 44K fine-grained\nmasks from several sources. HQ-SAM is only trained on the introduced detaset of\n44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM\nin a suite of 10 diverse segmentation datasets across different downstream tasks,\nwhere 8 out of them are evaluated in a zero-shot transfer protocol. Our code and\npretrained models are at https://github.com/SysCV/SAM-HQ.\n1\nIntroduction\nAccurate segmentation of diverse objects is fundamental for a wide range of scene understanding\napplications, including image/video editing, robotic perception, and AR/VR. Trained with billion-\nscale mask labels, the Segment Anything Model (SAM) [21] was recently released as a foundational\nvision model for general image segmentation. SAM is capable of segmenting a wide range of objects,\nparts, and visual structures in diverse scenarios, by taking a prompt consisting of points, a bounding\nbox, or a coarse mask as input. Its zero-shot segmentation abilities have led to a rapid paradigm shift,\nas it can be transferred to numerous applications through simple prompting.\nWhile SAM has achieved impressive performance, its segmentation results are still unsatisfactory\nin many cases. In particular, SAM suffers from two key problems: 1) Coarse mask boundaries,\noften even neglecting the segmentation of thin object structures, as shown in Figure 1. 2) Incorrect\npredictions, broken masks, or large errors in challenging cases. This is often related to SAM\nmisinterpreting thin structures, such as the kite lines in the rightmost column of Figure 1. These types\nof failures severely limit the applicability and effectiveness of foundational segmentation models,\nsuch as SAM, in particular for automated annotation and image/video editing tasks, where highly\naccurate image masks are crucial.\nWe propose HQ-SAM, which can predict highly accurate segmentation masks, even in very challeng-\ning cases (see Figure 1), without compromising the strong zero-shot capabilities and flexibility of the\n*Equal contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.01567v2  [cs.CV]  23 Oct 2023\nSAM \nPrediction\nHQ-SAM \nPrediction\nFigure 1: The predicted masks of SAM vs. our HQ-SAM, given the same red box or several points\non the object as input prompts. HQ-SAM produces significantly more detailed results with very\naccurate boundaries. In the rightmost column, SAM misinterprets the thin structure of the kite lines,\nand produces a large portion of errors with broken holes for the input box prompt.\noriginal SAM. To preserve the efficiency and zero-shot performance, we propose a minimal adaptation\nof SAM, adding less than 0.5% parameters, to extend its capability to high-quality segmentation.\nDirectly fine-tuning the SAM decoder or introducing a new decoder module severely degrades the\ngeneral zero-shot segmentation performance. We therefore propose the HQ-SAM architecture, which\ntightly integrates with and re-uses the existing learned SAM structure, in order to fully preserve the\nzero-shot performance. First, we design a learnable HQ-Output Token that is input to SAM\u2019s mask\ndecoder, alongside the original prompt and output tokens. Unlike the original output tokens, our\nHQ-Output Token and its associated MLP layers are trained to predict a high-quality segmentation\nmask. Second, instead of only re-using the SAM\u2019s mask decoder features, our HQ-Output Token\noperates on a refined feature set to achieve accurate mask details. In particular, we use both global\nsemantic context and local fine-grained features by fusing SAM\u2019s mask decoder features with early\nand late feature maps from its ViT encoder. During training, we freeze the entire pre-trained SAM\nparameters, while only updating our HQ-Output Token, its associated three-layer MLPs, and a small\nfeature fusion block.\n40\n45\n50\n0\n500\n1500\n50\nModel Size [M]\nZero-shot COCO AP\n17MB\n13MB\n3000\nLight HQ-SAM\n(TinyViT)\n!\n!\nMobileSAM\n(TinyViT)\n44.8 FPS\n41.2 FPS\n42.5\n47.5\n!\n10.1 FPS\n!\nHQ-SAM\n(ViT-B)\nSAM\n(ViT-B)\n9.8 FPS\n!\nHQ-SAM\n(ViT-L)\n4.8 FPS\n5 FPS!\nSAM\n(ViT-L)\n!\nHQ-SAM\n(ViT-H)\n3.4 FPS\n!\nSAM\n(ViT-H)\n3.5 FPS\nFigure 2: Performance vs. speed vs. model size\nfor an array of SAM variants [21, 52].\nLearning accurate segmentation requires a dataset\nwith accurate mask annotations of diverse objects\nwith complex and detailed geometries.\nSAM\nis trained on the SA-1B dataset, which contains\n11M images with 1.1 billion masks automatically\ngenerated by a SAM-like model. However, us-\ning this extensive dataset presents significant cost\nimplications and falls short of achieving the de-\nsired high-quality mask generations pursued in our\nwork, as evident by SAM\u2019s performance in Fig-\nure 1. Consequently, we compose a new dataset,\ncalled HQSeg-44K, which contains 44K extremely\nfine-grained image mask annotations.\nHQSeg-\n44K is constructed by merging six existing image\ndatasets [35, 29, 26, 38, 8, 46] with highly accurate\nmask labels, covering over 1,000 diverse semantic\nclasses. Thanks to the smaller-scale dataset and our\nminimal integrated architecture, HQ-SAM can be\ntrained in only 4 hours on 8 RTX 3090 GPUs.\nTo validate the effectiveness of HQ-SAM, we perform extensive quantitative and qualitative experi-\nmental analysis. We provide a comprehensive performance-speed-model size comparison on SAM\nvariants [21, 52] in Figure 2. We compare HQ-SAM with SAM on a suite of 10 diverse segmentation\ndatasets across different downstream tasks, where 8 out of them are under a zero-shot transfer protocol,\nincluding COCO [31], UVO [42], SGinW [58], LVIS [14], HQ-YTVIS [20], BIG [6], COIFT [29]\n2\nand HR-SOD [51]. This rigorous evaluation demonstrates that the proposed HQ-SAM can produce\nhigher-quality masks while maintaining the zero-shot capability compared with SAM.\n2\nRelated Work\nHigh-quality Segmentation Existing works for high-quality segmentation are mostly trained for a\nspecific segmentation task, like image and video instance segmentation [22, 19, 20, 40, 44], semantic\nsegmentation [30, 54, 39, 50] or panoptic segmentation [9], in a close-world paradigm. Some of\nthem focus on post-segmentation refinement using with graphical models such as CRF [23] or region\ngrowing [10]. However, the CRF-based refinement is adhere to low-level color boundaries without\nfully utilizing high-level semantic context and cannot fix large segmentation errors. While some\nrefinement-based works adopt separate deep networks for cascade iterative refinement [6, 37], they are\nprone to overfitting as shown by our experiment. Compared to these high-quality segmentation [19,\n22, 33] or segmentation refinement methods, we focus on accurately segmenting diverse objects\non new data with flexible prompting, and build a high-quality zero-shot segmentation model that\ngeneralizes to various segmentation tasks and domains. Unlike the post segmentation refinement\nworks [6, 37], to preserve the zero-shot segmentation capability of SAM, HQ-SAM predicts the new\nhigh-quality mask directly by reusing the image encoder and mask decoder of SAM, instead of taking\nthe coarse mask and images as the input and feeding it into a separate refinement network. The model\narchitecture of HQ-SAM builds upon SAM with negligible overhead, where we propose efficient\ntoken learning for accurate mask predictions. This is completely different from previous high-quality\nsegmentation works, and we show its effectiveness across a wide range of zero-shot experiments.\nFine-tuning and Prompt Tuning for Foundation Models Foundation models [2, 1] first appear\nin the NLP community, where large language models such as GPT series [2] show strong zero-shot\ngeneralization to unseen tasks and data. Then, some prompt-based learning works [16, 27, 17] are\nproposed to help these pre-trained models generalize to the downstream tasks instead of fine-tuning the\ninternal model parameters [15] for better transfer learning. For vision-based foundation models [21,\n43, 59], prompt engineering [56, 45, 49, 57] that freezes the pre-trained model is first explored in\nvision-language models, such as CLIP [36]. These prompts with learnable parameters are designed\nto help downstream tasks with better context optimization. Different from the existing prompt-based\nor finetuning works, we focus on the minimal adaptation of SAM toward high-quality segmentation.\nWe directly use the proposed HQ-Output Token output for accurate mask prediction, instead of only\nleveraging some learnable parameters [56] to help context learning and better generalization.\n3\nMethod\nWe propose HQ-SAM to upgrade SAM for high-quality zero-shot segmentation. HQ-SAM is\nlightweight and only introduces two important adaptations to the SAM model. In Sec 3.1, we first\nbriefly review the architecture of SAM on which HQ-SAM is built. Then, in Sec 3.2, we introduce\nour HQ-SAM with High-Quality Token (HQ-Output Token) and Global-local Feature Fusion, which\nare the key components to achieve better segmentation quality for SAM while preserving its zero-shot\ncapability. Finally, in Sec 3.3, we describe the training and inference process of HQ-SAM, which is\nboth data and computationally efficient.\n3.1\nPreliminaries: SAM\nSAM [21] is composed of three modules: (a) Image encoder: a heavy ViT-based backbone for\nimage feature extraction, resulting in image embedding in spatial size 64\u00d764. (b) Prompt encoder:\nencoding the interactive positional information from the input points/boxes/masks to provide for the\nmask decoder. (c) Mask decoder: a two-layer transformer-based decoder takes both the extracted\nimage embedding with the concatenated output and prompt tokens for final mask prediction. The\nreleased SAM model is trained on the large-scale SA-1B dataset, which contains over 1 billion\nautomatically generated masks (400\u00d7 more masks than any existing segmentation datasets [14, 24])\nand 11 million images. Thus, SAM shows valuable strong zero-shot generalization to new data\nwithout the necessity for additional training. However, we also note that SAM training is very\nexpensive, where distributively training ViT-H-based SAM for 2 epochs on SA-1B requires 256\nGPUs with a large batch size of 256 images. For more SAM method details, we refer readers to [21].\n3\nImage Encoder\nPrompt Token\n(Ntoken\u00d7256)\nSAM\n64\u00d764\nGlobal-local \nFusion\n256\u00d7256\nOutput Token  \nHQ-Output \nToken  \nInput Image\nMLP\nHQ-SAM\nTransposed \nConv.\nToken to \nimage attn.\nDecoder Layer -2\nOutput \nToken\nDecoder Layer -1\nUpdated \nHQ-Output \nToken\nMLP\nViT Feat.\nMask Feat.\nMask feat.\nError \nCorrection\nMask Decoder\nHQ-Features\nSAM Mask\nHQ-SAM Mask\nEarly Layer\nFinal Layer\nPoint-wise Product\nFigure 3: HQ-SAM introduces HQ-Output Token and Global-local Feature Fusion to SAM for\nhigh-quality mask prediction. To keep the zero-shot capability of SAM, the lightweight HQ-Output\nToken reuses SAM\u2019s mask decoder, and generates new MLP layers for performing point-wise product\nwith fused HQ-Features. During training, only a few learnable parameters in HQ-SAM are trainable\nwhile we fix the model parameters of the pre-trained SAM. The prompt encoder is omitted here for\nclarity. Error correction is simply used as a direct element-wise sum between the predicted logits of\nthe SAM\u2019s Output Token and the HQ-Output Token during inference.\n3.2\nOurs: HQ-SAM\nIn this section, we describe the architecture of the HQ-SAM network. To preserve the zero-shot\ntransfer capability of SAM, while preventing model overfitting or catastrophic forgetting, instead\nof directly finetuning SAM or adding a new heavy decoder network, we take a minimal adaptation\napproach as much as possible. To this end, HQ-SAM reuses the pre-trained model weights of SAM\nas much as possible with only two new key components, namely, High-Quality Output Token and\nGlobal-local Feature Fusion, as illustrated in Figure 3. HQ-SAM can thus be regarded as a high-\nquality zero-shot segmentation model evolved from SAM with negligible extra model parameters and\ncomputation cost.\n3.2.1\nHigh-Quality Output Token\nWe propose efficient token learning for improving the mask quality of SAM. As shown in Figure 3,\nin SAM\u2019s original mask decoder design, the output token (similar to object query in DETR [3]) is\nadopted for mask prediction, which predicts dynamic MLP weights and then performs point-wise\nproduct with the mask features. To promote SAM\u2019s mask quality in HQ-SAM, instead of directly\ntaking SAM\u2019s coarse masks as input, we introduce the HQ-Output token and a new mask prediction\nlayer for high-quality mask prediction.\nIn Figure 3, by reusing and fixing SAM\u2019s mask decoder, a new learnable HQ-Output Token (size\nof 1\u00d7256) is concatenated with SAM\u2019s output tokens (size of 4\u00d7256) and prompt tokens (size of\nNprompt\u00d7256) as the input to the SAM\u2019s mask decoder. Similar to the original output token, in each\nattention layer, HQ-Output Token first performs self-attention with other tokens and then conducts\nboth token-to-image and the reverse image-to-token attention for its feature updating. Note that\nHQ-Output Token uses the point-wise MLP shared by the other tokens in each decoder layer. After\npassing through two decoder layers, the updated HQ-Output Token has access to the global image\ncontext, the critical geometric/type information of prompt tokens as well as hidden mask information\nof the other output tokens. Finally, we add a new three-layer MLP to generate dynamic convolutional\nkernels from the updated HQ-Output Token, which then performs spatially point-wise product with\nthe fused HQ-feature for high-quality mask generation.\nInstead of directly finetuning SAM or further adding a heavy post-refinement network, we only allow\nthe HQ-Output Token and its associated three-layer MLPs to be trained for correcting the mask\nerrors of SAM\u2019s output token. This is completely different from existing high-quality segmentation\nmodels [19, 6, 20, 22]. We identify two main advantages of our efficient token learning through\nextensive experiments: 1) This strategy significantly improves SAM\u2019s mask quality while only\n4\nintroducing negligible parameters compared to original SAM, making HQ-SAM training extremely\ntime and data-efficient; 2) The learned token and MLP layers do not overfit to mask the annotation\nbias of a specific dataset, thus keeping SAM\u2019s strong zero-shot segmentation capability on new\nimages without catastrophic knowledge forgetting.\n3.2.2\nGlobal-local Fusion for High-quality Features\nVery accurate segmentation also requires input image feature with both rich global semantic context\nand local boundary details. To further promote mask quality, we enrich both the high-level object\ncontext and low-level boundary/edge information in the mask decoder features of SAM. Instead\nof directly using SAM\u2019s mask decoder feature, we compose the new high-quality features (HQ-\nFeatures) by extracting and fusing features from different stages of the SAM model: 1) The early\nlayer local feature of SAM\u2019s ViT encoder with spatial shape 64\u00d764, which captures more general\nimage edge/boundary details [12]. Concretely, we extract the feature after the first global attention\nblock of the ViT encoder, and for ViT-Large based SAM, this is the 6th block output for the 24 blocks\nin total; 2) The final layer global feature of SAM\u2019s ViT encoder with shape 64\u00d764, which has more\nglobal image context information; 3) The mask feature in SAM\u2019s mask decoder with size 256\u00d7256,\nwhich is also shared by the output tokens, contains strong mask shape information.\nAs shown in Figure 3, to obtain the input HQ-Features, we first upsample the early-layer and final-\nlayer encoder features to the spatial size 256\u00d7256 by transposed convolution. Then, we sum up these\nthree types of features in an element-wise manner after simple convolutional processing. We show\nthat this global-local feature fusion is simple while effective, yielding detail-preserving segmentation\nresults with a small memory footprint and computation burden. We also perform detailed ablation on\nthe effect of each feature source in the experimental section (Table 3).\n3.3\nTraining and Inference of HQ-SAM\nTraining Data Construction To train HQ-SAM in a data-efficient manner, instead of further training\non SA-1B [21], we compose a new training dataset HQSeg-44K which contains 44,320 extremely\naccurate image mask annotations. We note that the released SA-1B dataset only contains automatically\ngenerated mask labels, missing very accurate manual annotation on objects with complex structures.\nDue to the annotation difficulty, HQSeg-44K leverages a collection of six existing image datasets\nincluding DIS [35] (train set), ThinObject-5K [29] (train set), FSS-1000 [26], ECSSD [38], MSRA-\n10K [8], DUT-OMRON [46] with extremely fine-grained mask labeling, where each of them contains\n7.4K mask labels on average. To make HQ-SAM robust and generalizable to new data, HQSeg-44K\ncontains diverse semantic classes of more than 1,000. We show the advantage of using HQSeg-44K\nby comparing HQ-SAM training with 44K randomly sampled images and masks from SA-1B [21] in\nour supplemental analysis.\nHQ-SAM Training During training, we fix the model parameters of the pre-trained SAM model\nwhile only making the proposed HQ-SAM learnable. The learnable parameters thus only include the\nHQ-Output Token, its associated three-layer MLP and three simple convolutions for HQ-Features\nfusion. Since SAM is designed for flexible segmentation prompts, we train HQ-SAM by sampling\nmixed types of prompts including bounding boxes, randomly sampled points, and coarse masks input.\nWe generate these degraded masks by adding random Gaussian noise in the boundary regions of the\nGT masks. For generalizability to different object scales, we use large-scale jittering [13]. We use a\nlearning rate of 0.001 and train our HQ-SAM for 12 epochs, with a learning rate drop after 10 epochs.\nWe train on 8 Nvidia GeForce RTX 3090 GPUs with a total batch size of 32, which takes 4 hours to\ntrain for 16.6K iterations. Please refer to our supplemental file for more details.\nHQ-SAM Inference We follow the same inference pipeline of SAM but use the mask prediction\nfrom HQ-Output token as high-quality mask prediction. During inference, we sum the predicted\nlogits of the SAM mask (by Output Token) and our predicted mask (by HQ-Output Token) for mask\ncorrection on spatial resolution 256\u00d7256. Then we up-sample the corrected mask to the original\nresolution 1024\u00d71024 as our output.\nSAM vs. HQ-SAM on Training and Inference In Table 1, we report detailed training and\ninference comparisons between our HQ-SAM and SAM. While HQ-SAM produces substantially\nbetter segmentation quality, its training is very quick and affordable, which only takes 4 hours with\n8 RTX3090 GPUs. HQ-SAM is also lightweight and efficient, introducing negligible increases in\nmodel parameters, GPU memory usage, and inference time per image.\n5\nTable 1: Training and inference comparison between ViT-L [11] based SAM and HQ-SAM. HQ-SAM\nbrings negligible extra computation burden to SAM, with less than 0.5% increase in model parameters\nand reaching 96% of its original speed. SAM-L is trained on 128 A100 GPUs for 180k iterations.\nBased on SAM-L, we only need to train our HQ-SAM on 8 RTX3090 GPUs for 4 hours.\nMethod\nTraining\nInference\nLearnable Params (M)\n# GPU\nBatch Size\nTime (h)\nFPS\nMem.\nSAM [21]\n1191\n128\n128\nN/A\n5.0\n7.6G\nHQ-SAM\n5.1\n8\n32\n4\n4.8\n7.6G\n4\nExperiments\n4.1\nExperimental Setup\nDatasets For training we use the compiled HQSeg-44K, described in Section 3.3. For a comprehen-\nsive evaluation of the segmentation performance of HQ-SAM, we perform experiments on a wide\nrange of datasets, including four extremely fine-grained segmentation datasets: DIS [35] (validation\nset), ThinObject-5K [29] (test set), COIFT [29] and HR-SOD [51]. Besides, we experiment on popu-\nlar and challenging benchmarks across various image/video-based segmentation tasks in zero-shot\nsettings, such as COCO [31], SGinW [58], UVO [42], LVIS [14], HQ-YTVIS [20] and BIG [6].\nEvaluation Metrics To accurately quantify improvements in mask quality, instead of only employing\nthe standard mask AP or mask mIoU, we also adopt boundary metrics mBIoU and boundary APB [5].\nWe also evaluate on stricter APstrict\nB\nby adjusting the default dilation ratio from 0.02 to 0.01 on\nUVO [42] and LVIS [14]. For evaluation on the four fine-grained segmentation datasets [35, 29, 51],\nwe also report the averaged boundary and mask IoU among them. For video instance segmentation\nevaluation on HQ-YTVIS [20], we use both Tube Boundary APB and Tube Mask APM.\n4.2\nAblation Experiments\nWe conduct detailed ablation studies on the proposed HQ-SAM using ViT-Large as the backbone,\nanalyzing the impact of the proposed HQ-Output Token and HQ-Features on segmentation quality\nespecially in zero-shot cases. For ablation experiments, we use the four aforementioned extremely\naccurate segmentation datasets, namely, DIS (val) [35], ThinObject-5K (test) [29], COIFT [29] and\nHR-SOD [51] as well as the COCO validation set.\nEffect of the High-Quality Output Token . HQ-SAM employs HQ-Output Token for high-quality\nmask prediction. Table 2 compares our HQ-Output Token to the baseline SAM and other existing\nprompt/token learning strategies, such as adding an additional three context tokens [56] as learnable\nvectors into the SAM\u2019s mask decoder for better context learning. Compared to using context tokens,\nthe HQ-Output token consistently brings larger performance gains on four high-quality datasets, with\n13.2 mBIoU on DIS and 2.7 mBIoU on COIFT datasets. We also perform other ablation experiment\nvariants, such as computing the scaled dot product [18] between the original SAM\u2019s output token\nand our HQ-Output token or restricting the mask loss to only inside the boundary regions, and find\nthey slightly decrease the averaged performance on the four evaluation datasets. Compared to SAM,\nHQ-SAM significantly improves the mBIoU on DIS benchmark from 52.8 to 70.4 and also promotes\nthe mBIoU on the HRSOD dataset for 3.8 points.\nAblation on the Global-local Fusion for HQ-Features Table 3 tabulates the effect of global-local\nfusion, where the importance of each feature component is analyzed in HQ-Features during the\nfusion process. Compared to directly using the mask decoder feature of SAM, the entire HQ-Features\nbring an obvious advantage of 2.6 mBIoU on four highly accurate segmentation datasets. The\nfinal-layer ViT encoder feature with global context increases the mBIoU from 80.1 to 81.3. while\nthe early-layer feature with local details further promotes the mBIoU to 81.8. We also replace the\nproposed global-local fusion with the conventional FPN to build a feature pyramid for fusion, and\nfound this brought an inferior performance, decreasing from 89.1 to 87.4 mIoU.\nComparison to SAM finetuning or post-refinement . In Table 4, we compare our efficient token\nadaptation strategy to adding an extra post-refinement network [6] and model finetuning, including\ndirectly finetuning SAM\u2019s mask decoder or only finetuning its output token for mask prediction.\nAdding an extra heavy post-refinement network brings limited averaged performance increase on\n6\nTable 2: Ablation study of the HQ-Output Token on four extremely fine-grained segmentation datasets.\nWe adopt the boxes converted from their GT masks as the box prompt input. By default, we train the\npredicted mask of HQ Output-Token by computing full GT mask loss.\nModel\nDIS [35]\nCOIFT [29]\nHRSOD [51]\nThinObject [29]\nAverage\nmIoU\nmBIoU\nmIoU\nmBIoU\nmIoU\nmBIoU\nmIoU\nmBIoU\nmIoU\nmBIoU\nSAM (baseline)\n62.0\n52.8\n92.1\n86.5\n90.2\n83.1\n73.6\n61.8\n79.5\n71.1\nUsing SAM\u2019s mask decoder feature:\nSAM + Context Token [56]\n71.5\n62.2\n93.0\n87.7\n91.8\n85.0\n84.5\n73.1\n85.2\n77.0\nSAM + HQ-Output Token (\u00d7 Output Token)\n75.1\n65.8\n93.9\n88.9\n93.0\n86.1\n86.1\n74.6\n87.0\n78.9\nSAM + HQ-Output Token (Boundary Loss)\n75.2\n66.4\n94.0\n88.9\n92.1\n85.7\n87.3\n76.0\n87.2\n79.3\nSAM + HQ-Output Token\n75.3\n66.0\n94.2\n89.2\n93.0\n86.1\n86.8\n75.4\n87.3\n79.2\nUsing Our HQ-Feature:\nSAM + HQ-Output Token (+ Context Token)\n78.5\n70.4\n94.6\n89.6\n93.6\n87.0\n88.9\n79.3\n88.9\n81.6\nSAM + HQ-Output Token\n78.6\n70.4\n94.8\n90.1\n93.6\n86.9\n89.5\n79.9\n89.1\n81.8\nTable 3: Ablation study on the HQ-Features sources. Early-layer denotes the feature after the first\nglobal attention block of the ViT encoder, while final-layer denotes the output of the last ViT block.\nFour HQ datasets denote DIS (val) [35], ThinObject-5K (test) [29], COIFT [29] and HR-SOD [51].\nModel\nFusion\nDecoder\nViT Encoder\nFour HQ datasets\nconv\nMask feature\nFinal-layer\nEarly-layer\nmIoU\nmBIoU\nSAM [21]\n\u2713\n79.5\n71.1\n\u2713\n87.3\n79.2\n\u2713\n\u2713\n87.8\n80.1\nHQ-SAM (Ours)\n\u2713\n\u2713\n15.1\n9.0\n\u2713\n\u2713\n\u2713\n88.6\n81.3\n\u2713\n\u2713\n\u2713\n88.6\n81.1\n\u2713\n\u2713\n\u2713\n\u2713\n89.1\n81.8\nfour HQ datasets but leads to very poor performance on COCO, indicating strong overfitting. We\nalso observe a similar phenomenon when directly finetuning SAM\u2019s mask decoder. Only finetuning\nSAM\u2019s output token can address the catastrophic forgetting problem with improvement on the four\nHQ datasets and COCO. However, the incremental improvement is still much smaller compared to\nours. HQ-SAM improves 1.1 APB on COCO while output token finetuning only gives an increase\nof 0.4 APB. This shows the advantage of HQ-SAM in data-efficient learning while preserving the\nzero-shot capability of SAM.\n40\n60\n80\n100\n40\n60\n80\n100\nRecall Rate\nRecall Rate\n(a) Recall comparison on COIFT\n(b) Recall comparison on HRSOD\n99.6\n97.9\n99.6\n97.9\n98.9\n96.8\n97.5\n95.7\n96.4\n95.4\n95.4\n93.9\n93.2\n88.2\n87.5\n74.6\n69.3\n49.6\nBIoU\nThreshold\n0.5\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.5\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n98.6\n95.5\n97.6\n95.1\n96.9\n94.1\n94.1\n91.6\n92.0\n88.9\n87.1\n85.4\n84.0\n76.3\n73.5\n65.2\n53.3\n40.1\nBIoU\nThreshold\nFigure 4: Recall rate comparison between COIFT [29] and HRSOD [51] under the zero-shot protocol,\nusing BIoU thresholds ranging from loose to strict. The performance gap between SAM and our\nHQ-SAM increases significantly when we vary from a loose BIoU threshold of 0.5 to a very strict\nthreshold of 0.9, showing the advantage of HQ-SAM in predicting very accurate segmentation masks.\nAccuracy analysis at different BIoU thresholds Figure 4 compares SAM and HQ-SAM from\nloose to strict BIoU thresholds. We plot the percentage of mask predictions that have a BIoU larger\nthan the threshold indicated on the x-axis. The large performance gap with strict IoU thresholds\non both COIFT [29] and HRSOD [51] clearly validates the advantage of HQ-SAM in predicting\nvery accurate masks. However, even at the loose threshold of 0.5, HQ-SAM reduces the number of\nincorrect predictions by SAM by 81% for COIFT and 69% for HRSOD. This shows that HQ-SAM\npredictions are not only substantially more accurate but also more robust in challenging cases.\n7\nTable 4: Comparison with model finetuning or extra post-refinement [6]. For the COCO dataset, we\nuse a SOTA detector FocalNet-DINO [53] trained on the COCO dataset as our box prompt generator.\nModel\nFour HQ datasets\nCOCO\nmIoU\nmBIoU\nAPB\nAP\nAPL\nAPM\nAPS\nSAM (baseline)\n79.5\n71.1\n33.3\n48.5\n63.9\n53.1\n34.1\nTraining the whole SAM\n38.0\n12.2\n0.2\n5.5\n-\n-\n-\nAdd Context Token [56]\n85.2\n77.0\n31.9\n47.2\n65.1\n51.2\n31.9\nCascadePSP Post-refinement [6]\n80.9\n74.6\n2.8\n13.4\n43.4\n9.4\n0.0\nCRM Post-refinement [37]\n81.4\n75.4\n15.9\n28.7\n-\n-\n-\nFinetune SAM\u2019s decoder\n87.6\n79.5\n9.0\n19.5\n45.2\n15.8\n4.7\nFinetune SAM\u2019s output token\n87.6\n79.7\n33.7\n48.7\n66.0\n52.3\n33.6\nHQ-SAM (Ours)\n89.1\n81.8\n34.4\n49.5\n66.2\n53.8\n33.9\nTable 5: Zero-shot open-world instance segmentation results comparison on UVO [42]. We use\nFocalNet-DINO [53] trained on the COCO dataset as our box prompt generator. \u2217strict denotes the\nboundary region with a tighter threshold.\nModel\nAPstrict\nB\nAPstrict\nB75\nAPstrict\nB50\nAPB\nAPB75\nAPB50\nAP\nSAM\n8.6\n3.7\n25.6\n17.3\n14.4\n37.7\n29.7\nHQ-SAM\n9.9\n5.0\n28.2\n18.5\n16.3\n38.6\n30.1\nTable 6: Zero-shot segmentation result comparison on the test set of high-quality BIG [6] benchmark\nusing various types of input prompts. We employ PSPNet [55] to generate the coarse mask prompt.\nModel\nGT Box Prompt\nMask Prompt\nmIoU\nmBIoU\nmIoU\nmBIoU\nSAM\n81.1\n70.4\n66.6\n41.8\nHQ-SAM\n86.0\n75.3\n86.9\n75.1\n4.3\nZero-shot Comparison with SAM\nWe perform extensive zero-shot transfer comparisons between our HQ-SAM and SAM on 7 bench-\nmarks, including SGinW [58], COCO [31], UVO [42], LVIS [14], HQ-YTVIS [20], BIG [6],\nCOIFT [29] and HR-SOD [51], where HQ-SAM outperforms SAM without bells and whistles,\ndemonstrating its efficacy and kept generalization ability even trained with a small-scale dataset.\nResults on the SGinW Benchmark Equipped with the same Grounding-DINO [32] as box prompts,\nwe also performed experiments by replacing SAM with HQ-SAM in Grounded-SAM, and obtained\nthe first place in the Segmentation in the Wild (SGinW) competition1 on the zero-shot track. Note\nthat SGinW contains 25 zero-shot in-the-wild segmentation datasets for evaluation, and Grounded-\nHQ-SAM with 49.6 mean AP and outperforms Grounded-SAM obviously using the same detector.\nZero-Shot Open-world Segmentation To evaluate the zero-shot segmentation results in the open-\nworld environment, in Table 5, we compare SAM and our HQ-SAM on the challenging UVO [42]\nbenchmark with diverse and dense objects mask annotations. By taking the same pre-trained object\ndetector [53] as box prompt input, our HQ-SAM improves for 1.3 APstrict\nB\nand 2.6 APstrict\nB50 over SAM.\nZero-Shot Segmentation on High-resolution BIG Dataset In Table 6, we compare the zero-shot\nsegmentation quality between SAM and HQ-SAM on the high-resolution BIG benchmark [6] with\ntwo types of prompts, including using GT object boxes or the provided coarse masks input. HQ-SAM\nconsistently surpasses SAM, with obvious advantages using different types of prompts, and is much\nmore robust to coarse masks prompts with partial boundary errors (provided by PSPNet [55]).\nZero-shot Instance Segmentation on COCO and LVIS In Table 7, we also evaluate HQ-SAM\non the popular COCO and LVIS benchmarks respectively by feeding box prompts generated by the\ntrained detectors of these two datasets. HQ-SAM consistently outperforms SAM by 1.1 APB on\nCOCO and 0.7 APstrict\nB75 on LVIS, showing the improved mask quality and well-preserved zero-shot\nsegmentation ability during the HQ-SAM training process.\n1 SGinW Benchmark Results: https://eval.ai/web/challenges/challenge-page/1931/leaderboard/4567\n8\nTable 7: Zero-shot instance segmentation results comparison on COCO [31] and LVISv1 [14]. For\nthe COCO dataset, we use FocalNet-DINO [53] detector trained on COCO. For LVIS, we adopt\nViTDet-H [28] trained on the LVIS dataset as our box prompt generator. For SAM, we use the ViT-L\nbackbone and box prompt. We maintain the zero-shot segmentation capability of the original SAM\nwhile improving the mask quality on the boundary region.\nModel\nCOCO\nLVIS\nAPB\nAP\nAPstrict\nB\nAPstrict\nB75\nAPB\nAPB75\nAP\nSAM\n33.3\n48.5\n32.1\n32.8\n38.5\n40.9\n43.6\nHQ-SAM\n34.4\n49.5\n32.5\n33.5\n38.8\n41.2\n43.9\n60\n90\n70\n80\n0\n20\n40\n60\n80\nmBIoU\n1 Pos.\n3 Pos.\n5 Pos.\n5 Pos. \n+ 5 Neg.\n10 Pos.\n10 Pos.\n+ 5 Neg.\n1 Pos.\n3 Pos.\n5 Pos.\n5 Pos. \n+ 5 Neg.\n10 Pos.\n10 Pos.\n+ 5 Neg.\nInput Point \nPrompt Num\n(a) Performance comparison on COIFT\n(b) Performance comparison on DIS\n62.8\n61.8\n81.3\n83.5\n83.9\n86.5\n83.6\n86.7\n88.6\n86.3\n88.4\n86.4\n17.9\n18.1\n50.4\n55.3\n66.0\n58.3\n55.1\n66.2\n56.8\n69.3\n59.6\n70.0\nmBIoU\nFigure 5: Interactive segmentation results comparison using a varying number of input points on the\nCOIFT [29] (zero-shot) and DIS [35] val set. HQ-SAM consistently outperforms SAM with various\npoint numbers, and the relative improvement is more obvious with less prompt ambiguity.\nTable 8: Zero-shot Video Instance Segmentation comparison on the test set of the very accurately\nlabeled HQ-YTVIS [20] benchmark. We utilize pre-trained Swin-L-based Mask2Fromer [4] on\nYTVIS [47] as our box prompt input while reusing its object association prediction.\nModel\nAPB\nAPB\n75\nAPB\n50\nAPM\nAPM\n75\nAPM\n50\nSAM\n30.2\n19.1\n72.9\n60.7\n68.1\n90.5\nHQ-SAM\n34.0\n24.3\n79.5\n63.6\n70.5\n91.1\nPoint-based Interactive Segmentation Comparison To investigate the segmentation performance\nof HQ-SAM with interactive point prompts, in Figure 5, we compare HQ-SAM to SAM with varying\nnumbers of input points on COIFT [29] (zero-shot) and DIS [35] val set. HQ-SAM consistently\noutperforms SAM with different point prompts on both two datasets. We note that the relative\nperformance increase is more significant when the prompt contains less object ambiguity with more\ninput points information (increasing from 1 positive point to 10 positive points + 5 negative points).\nZero-shot High-quality Video Instance Segmentation Besides conducting image-based segmenta-\ntion evaluation, we also perform video instance segmentation results comparison on the accurately\nannotated HQ-YTVIS benchmark [20]. We take the pre-trained Mask2Former [4] as our video box\nprompts and feed it into SAM and our HQ-SAM for mask prediction. In Table 8, HQ-SAM achieves\nremarkable gains of 3.8 points in Tube Boundary APB and 2.9 Tube Mask APM.\nVisualization of HQ-Output Token In Figure 6, we provide visual comparison of our HQ-Output\nToken vs. SAM\u2019s common output token for their cross-attention maps in the last token-to-image\nlayer of the mask decoder. We observe that our HQ-Output Token attends to the boundary and thin\nstructure regions that are missed by the common token.\nZero-shot Visual Results Comparison In Figure 7, we compare HQ-SAM to SAM qualitatively in\na zero-shot transfer setting, where HQ-SAM significantly promotes the mask details of SAM and\nalso improves the masks of broken holes or large portion errors by the enriched semantic context.\nRefer to the supplemental file for more visual comparisons.\nComparison with Adapter Tuning Strategy In Table 9, we also compare our efficient token\nadaptation strategy to the recent Adapter Tuning [48] and LoRA [17]. We introduce lightweight\nadapters to ViT layers of SAM\u2019s encoder for encoder tuning and identify that this strategy leads to\noverfitting and its zero-shot performance on COCO decreases from 33.3 to 29.6. This validates our\ndesign choice to freeze SAM\u2019s encoder, and mainly focus on SAM\u2019s decoder.\n9\nImage\nAttn. Map Of SAM\u2019s\noriginal Token\nAttn. Map Of \nHQ-Output Token\nImage\nAttn. Map Of SAM\u2019s\noriginal Token\nAttn. Map Of \nHQ-Output Token\nImage\nAttn. Map Of SAM\u2019s\noriginal Token\nAttn. Map Of \nHQ-Output Token\nFigure 6: Cross-attention of SAM\u2019s original token vs. HQ-Output Token in the last decoder layer.\nHQ-Token attends to the boundary and thin structure regions that are missed by the original token.\nFigure 7: Visual results comparison between SAM (top row) vs. HQ-SAM (bottom row) in a zero-shot\ntransfer setting, given the same red box or point prompt. HQ-SAM produces significantly more\ndetailed-preserving results and also addresses the mask errors with broken holes.\nTable 9: Comparison to Adapter Tuning [48] or using LoRA [17] in SAM\u2019s encoder using ViT-L\nbased SAM and the same HQSeg-44K. For the COCO dataset, we use the SOTA detector FocalNet-\nDINO [53] trained on the COCO dataset as our box prompt generator.\nModel\nCOCO\nModel Params (MB)\nAPB\nAP\nAPL\nAPM\nAPS\nTotal\nTrainable\nSAM\n33.3\n48.5\n63.9\n53.1\n34.1\n1191\n-\nSAM + LoRA [17]\n28.6\n43.7\n-\n-\n-\n1192.5\n1.5\nSAM + Encoder Adapter [48]\n29.6\n44.8\n63.9\n47.8\n29.0\n1203\n12.0\nHQ-SAM\n34.4\n49.5\n66.2\n53.8\n33.9\n1196.1\n5.1\nMobile Efficiency Although HQ-SAM significantly boosts SAM\u2019s mask quality with negligible\noverhead, it shares the heavy ViT encoder of SAM, and thus cannot achieve a real-time speed in\nvideo processing. For efficient mobile deployment, we propose Light HQ-SAM based on the tiny\nViT image encoder provided by MobileSAM [52]. In Figure 2, achieving running speed of 41.2 FPS,\nLight HQ-SAM improves the zero-shot COCO AP of MobileSAM from 44.3 to 45.0 with negligible\nadditional cost, i.e., 1.7MB increase in model parameters.\n5\nConclusion\nWe propose HQ-SAM, the first high-quality zero-shot segmentation model by introducing negligible\noverhead to the original SAM. We propose a lightweight High-quality Output Token in HQ-SAM to\nreplace the original SAM\u2019s output token for high-quality mask prediction. After training only on 44K\nhighly-accurate masks, HQ-SAM significantly boosts the mask prediction quality of SAM, which\nwas trained on 1.1 billion masks. The zero-shot transfer evaluation is performed on 8 segmentation\nbenchmarks across both image and video tasks, spanning diverse objects and scenes. Our research\noffers timely insights into how to leverage and extend SAM-like foundational segmentation models\nin a data-efficient and computation-affordable manner.\n10\nSupplementary Material:\nSegment Anything in High Quality\nIn this supplementary material, Section 6 first presents the additional experimental analysis of our HQ-\nSAM, including more zero-shot transfer comparisons to SAM on both image and video benchmarks.\nThen, in Section 7, we describe more details of our method implementation, including the training\nand inference. In Section 8, we provide further details of our constructed HQSeg-44K dataset for\ntraining HQ-SAM. In Section 9, we show extensive visual results comparison between our HQ-SAM\nand SAM on COCO [31], DIS-test [35], HR-SOD [51], NDD20 [41], DAVIS [34], and YTVIS [47].\n6\nSupplementary experiments\nSAM vs. HQ-SAM on Various Backbones In Table 10, we provide a comprehensive comparison\nbetween HQ-SAM and SAM using various backbones, including ViT-B, ViT-L, ViT-H and TinyViT.\nThe comparison not only includes the numerical results on the four HQ datasets and COCO validation\nset, but also contains the model sizes/speed/memory. HQ-SAM consistently outperforms SAM using\nthree different backbones, with over 10 points increase in mBIoU on the four HQ datasets. Notably,\nthe ViT-B based HQ-SAM significantly improves the APB on COCO from 28.2 to 31.3 and AP\nfrom 44.4 to 46.7, with only a 1.1% increase in model parameters and negligible extra memory\nconsumption.\nTable 10: SAM vs. HQ-SAM on various ViT backbones. For the COCO dataset, we use a SOTA\ndetector FocalNet-DINO [53] trained on the COCO dataset as our box prompt generator.\nModel\nFour HQ datasets\nCOCO\nModel Params (MB)\nFPS\nMemory\nmIoU\nmBIoU\nAPB\nAP\nAPL\nAPM\nAPS\nTotal\nLearnable\nSAM-B\n70.6\n62.3\n28.2\n44.4\n57.7\n48.7\n32.1\n358\n358\n10.1\n5.1G\nHQ-SAM-B\n86.3\n78.1\n31.3\n46.7\n62.9\n50.5\n32.0\n362.1\n4.1\n9.8\n5.1G\nSAM-L\n79.5\n71.1\n33.3\n48.5\n63.9\n53.1\n34.1\n1191\n1191\n5.0\n7.6G\nHQ-SAM-L\n89.1\n81.8\n34.4\n49.5\n66.2\n53.8\n33.9\n1196.1\n5.1\n4.8\n7.6G\nSAM-H\n75.6\n68.3\n34.0\n48.9\n64.5\n53.3\n34.4\n2446\n2446\n3.5\n10.3G\nHQ-SAM-H\n89.3\n81.5\n34.9\n49.9\n66.5\n54.0\n34.2\n2452.1\n6.1\n3.4\n10.3G\nMobileSAM\n69.0\n58.8\n28.6\n44.3\n-\n-\n-\n38.6\n38.6\n44.8\n3.7G\nLight HQ-SAM\n81.4\n71.6\n29.6\n45.0\n-\n-\n-\n40.3\n1.7\n41.2\n3.7G\nTable 11: Results on YouTubeVIS 2019 validation set and HQ-YTVIS test set using ViT-L based\nSAM. We adopt the SOTA detector Mask2Former [4] trained on the YouTubeVIS 2019 dataset as our\nvideo boxes prompt generator while reusing its object association prediction.\nModel\nYTVIS 2019\nHQ-YTVIS\nAP\nAP50\nAP75\nAPL\nAPM\nAPS\nAPB\nAPM\nSAM\n51.8\n82.1\n55.4\n65.5\n52.0\n34.2\n30.2\n60.7\nHQ-SAM\n53.2\n82.9\n58.3\n66.4\n53.3\n33.7\n34.0\n63.6\nZero-shot Video Instance Segmentation Comparison Extending from Table 8 of the paper\n(evaluation on the HQ-YTVIS benchmark [20]), we further perform a comparative analysis of zero-\nshot video instance segmentation results on the popular YTVIS 2019 [47] validation set. We take the\npre-trained Mask2Former [4] as our video box prompts and feed them into SAM and our HQ-SAM\nfor mask prediction. In Table 11, HQ-SAM achieves consistent gains of 1.4 points in Tube Mask AP,\nincreasing SAM\u2019s performance from 51.8 to 53.2. Interestingly, we find the AP75 improvement with\na higher IoU threshold for HQ-SAM is much larger than AP50, further validating the advantages of\nHQ-SAM in high-quality mask prediction.\n11\nZero-shot Video Object Segmentation Comparison Besides video instance segmentation, in\nTable 12, we further report the comparison of video object segmentation results between HQ-SAM\nand SAM on DAVIS validation set in a zero-shot transfer protocol. We take the pre-trained XMem\nas our video box prompts and feed the same prompts into SAM and HQ-SAM. HQ-SAM improves\nSAM the J &F from 82.0 to 83.2 and the F score from 84.9 to 86.1, where F is for measuring the\ncontour accuracy of the video objects.\nTable 12: Results on DAVIS 2017 [34] validation set using ViT-L based SAM. We adopt the SOTA\nmodel XMem [7] as our video boxes prompt generator while reusing its object association prediction.\nModel\nJ &F\nJ\nF\nSAM\n82.0\n79.0\n84.9\nHQ-SAM\n83.2\n80.3\n86.1\nRobustness to Input Box Prompts In Table 13, we compare HQ-SAM to SAM by adding various\nscales of noises to the input ground truth box prompts. In practice, we cannot expect the input box\nprompts provided by humans in interactive modes to be identical to the ground truth (GT) boxes or\nextremely accurate. We follow the data augmentation code in DN-DETR [25] to add different noise\nscales and identify that our HQ-SAM is much more robust compared to SAM, where the relative\nmBIoU advantage improves from 10.7 to 20.5 when gradually increasing the noise scales. Note that\nour method is not trained with noised boxes. We also visualize such noised input case in Figure 11,\nwhere SAM is more sensitive to small box location shifts that easily happened during interactive\nannotation.\nTable 13: Comparison of segmentation accuracy on the four HQ datasets by adding various noise\nlevels to the GT box prompts input.\nModel\nNo Noise\nNoise scale 0.2\nNoise scale 0.4\nmIoU\nmBIoU\nmIoU\nmBIoU\nmIoU\nmBIoU\nSAM\n79.5\n71.1\n65.7\n57.1\n46.4\n39.8\nHQ-SAM\n89.1\n81.8\u219110.7\n82.8\n73.4\u219116.3\n69.9\n60.3\u219120.5\n7\nAdditional Implementation details\nTraining Details During training HQ-SAM on the composed HQSeg-44K, we fix the model\nparameters of the pre-trained SAM model while only making the proposed HQ-SAM learnable,\nincluding HQ-Output Token, its associated three-layer MLP and three convolutions for HQ-Features\nfusion. Two of them are transposed convolutions (size 2\u00d72, stride 2) used to upscale encoder\nembedding size from 64\u00d764 to 256\u00d7256. We treat the new HQ-Output Token as the fifth mask\ntoken compared to the original four mask tokens in SAM\u2019s mask decoder. During training, this new\nHQ-Output token of size 1\u00d7256 is concatenated with SAM\u2019s mask tokens (size of 4\u00d7256), iou token\n(size of 1\u00d7256) and prompt tokens (size of Nprompt\u00d7256) as the input to the SAM\u2019s mask decoder.\nFor example, if the input image contains N box prompts (size N\u00d72\u00d7256), the final concatenated\ninput and output shape for the 2-layer mask decoder of SAM is N\u00d7(1+4+1+2)\u00d7256. For experiments\nusing ViT-B, ViT-L, and ViT-H-based models on training, we adopt the same training setting, with\na learning rate of 1e-3 and train our HQ-SAM for 12 epochs (learning rate drops to 1e-4 after 10\nepochs). We supervise mask prediction of the new HQ-Output token with a combination of both BCE\nLoss and Dice Loss.\nImplementation Details We follow the same inference pipeline of SAM but use the mask prediction\nfrom HQ-Output token as high-quality mask prediction. Table 10 reports the detailed inference speed\ncomparison using various backbones. For box-prompting-based evaluation, we feed SAM and our\nHQ-SAM with the same image/video bounding boxes and adopt the single mask output mode of\nSAM. For interactive segmentation comparison using a single point, we follow SAM and adopt the\n\u201ccenter\u201d point of Ground Truth (GT) masks, which is at a maximal value location in a mask\u2019s interior\ndistance transform. For multiple-point evaluation, we randomly sample the points from the GT masks\nand report the averaged results with three trials.\n12\n8\nMore Details of HQSeg-44K\nData compostion of HQSeg-44K In Table 14, we provide more details of our composed new training\ndataset HQSeg-44K which contains 44,320 extremely accurate image mask annotations, where we\nshow their annotation quality in Figure 8. HQSeg-44K is a collection of six existing image datasets\nincluding DIS [35] (train set), ThinObject-5K [29] (train set), FSS [26], ECSSD [38], MSRA-10K [8],\nDUT-OMRON [46] with extremely fine-grained mask labeling, where each of them contains 7.4K\nmask labels on average. This composed training set has no images/annotations overlapping with the\nzero-shot evaluation datasets adopted in our paper.\nEffect of HQSeg-44K In Table 15, we show the advantage of using HQSeg-44K by comparing\nHQ-SAM training with 44K randomly sampled images and masks from SA-1B [21]. Using the\nsame efficient token learning strategy, training with SA-1B (44K) decreases the averaged mBIoU\non the four datasets from 71.1 to 70.1, while ours improves it from 71.1 to 81.8. This validates\nthe effectiveness of our constructed HQSeg-44K benchmark in improving mask quality. Note that\nthe ablation experiments in Table 2, Table 3, Table 4 and Table 9 of the paper are all based on the\nconstructed HQSeg-44K.\nTable 14: Data composition of our constructed HQ-Seg-44K.\nDataset\nDIS [35]\nThin-Object 5k [29]\nFSS [26]\nDUTS [46]\nECSSD [38]\nMSRA-10K [8]\nTotal\nImage Num.\n3000\n4748\n10000\n15572\n1000\n10000\n44320\nDIS\nFSS-1000 ThinObject-5K\nECSSD\nMSRA-10K\nDUT-OMRON\nFigure 8: Visualization of annotated mask quality for randomly selected cases from the six dataset\ncomponents of the HQ-Seg-44K. Zoom in for better viewing the fine-grained mask details.\nZero-shot results on DIS and ThinObject-5K We also report zero-shot results in Table 16 on DIS\nand ThinObject-5K by removing the training splits of either or both datasets from the training of\n13\nTable 15: Comparison of the training dataset. For the COCO dataset using ViT-L-based SAM, we\nuse a SOTA detector FocalNet-DINO [53] trained on the COCO dataset as our box prompt generator.\nModel\nDataset\nDIS\nCOIFT\nHRSOD\nThinObject\nAverage\nmIoU\nmBIoU\nmIoU\nmBIoU\nmIoU\nmBIoU\nmIoU\nmBIoU\nmIoU\nmBIoU\nSAM\nSA-1B\n62.0\n52.8\n92.1\n86.5\n90.2\n83.1\n73.6\n61.8\n79.5\n71.1\nHQ-SAM\n+ SA-1B-44K\n60.4\n51.7\n91.1\n86.1\n88.4\n80.9\n73.1\n61.8\n78.3\n70.1\nHQ-SAM\n+ HQ-Seg-44K (Ours)\n78.6\n70.4\n94.8\n90.1\n93.6\n86.9\n89.5\n79.9\n89.1\n81.8\nFigure 9: Visual results comparison between SAM (top row) vs. HQ-SAM (bottom row) on DIS test\nset, given the same red box prompt. HQ-SAM produces significantly more accurate boundaries.\nHQ-SAM. The improvement of HQ-SAM over SAM is still substantial on DIS or ThinObject (over\n10.0 points on DIS-mIoU and 9.0 points on ThinObject-mIoU), even when the corresponding training\nsplits are removed from training.\nTable 16: Zero-shot results on DIS and ThinObject-5K by removing the training splits of either or\nboth datasets from the training of HQ-SAM. Results not obtained in a zero-shot manner (i.e. the\ntraining split was used), are shown in parenthesis to easily compare zero-shot results.\nTraining Setting\nDIS-mIoU\nDIS-mBIoU\nThinObject-mIoU\nThinObject-mBIoU\nSAM (baseline)\n62.0\n52.8\n73.6\n61.8\nHQ-SAM (remove both DIS and ThinObject)\n72.9\n63.1\n82.7\n70.7\nHQ-SAM (remove DIS)\n74.7\n66.2\n(90.1)\n(80.4)\nHQ-SAM (remove ThinObject)\n(78.4)\n(70.3)\n83.3\n72.1\nHQ-SAM (default HQSeg-44K)\n(78.6)\n(70.4)\n(89.5)\n(79.9)\n9\nMore Visual Results Comparison\nWe provide more extensive visual results comparison in Figure 9 (DIS [35] test set), Figure 10 (zero-\nshot setting in COCO), Figure 11 (noised box input) and Figure 12 (zero-shot setting in HRSOD [51],\nNDD20 [41] and web images which cover objects with various structure complexities in diverse\nenvironments. In Figure 13 and Figure 14, we provide the zero-shot video segmentation results\ncomparison on DAVIS 2017 and YTVIS 2019 benchmarks respectively. Besides, we include the dark\nunderwater environment in NDD20 [41] and randomly selected web images in Figure 12, showing\nthat the zero-shot segmentation power in SAM is well preserved by HQ-SAM. In Figure 12, we\nalso include two failure cases in the rightmost two columns of the third row and bottom row, where\nHQ-SAM improves over SAM, but still cannot achieve fully correct mask prediction.\n14\nSAM\nHQ-SAM\nSAM\nHQ-SAM\nFigure 10: Visual results comparison between SAM (top row) vs. HQ-SAM (bottom row) on COCO\nval set in zero-shot setting, using a SOTA detector FocalNet-DINO [53] trained on the COCO dataset\nas our box prompt generator. HQ-SAM predicts masks with higher quality than SAM with less mask\nartifacts.\nSAM + GT Box\nSAM + Noised Box\nHQ-SAM + GT Box\nHQ-SAM + Noised Box\nFigure 11: Visual results comparison between SAM (top row) vs. HQ-SAM (bottom row) with\nboth the GT and noised green box prompt. HQ-SAM produces much more consistent and robust\nsegmentation results regarding to the noises in the input boxes.\n15\nSAM\nHQ-SAM\nSAM\nHQ-SAM\nFigure 12: Visual results comparison between SAM (top row and third row) vs. HQ-SAM (second\nrow and bottom row) in zero-shot setting, given the same yellow box or point prompt. HQ-SAM\nproduces significantly more detailed preserving masks while fixing mask errors with broken holes.\nThe rightmost two columns in the third row and bottom row show two failure cases of HQ-SAM in\nextremely dark environments or very tiny metal rods.\n16\nSAM\nSAM\nSAM\nSAM\nHQ-SAM\nHQ-SAM\nHQ-SAM\nHQ-SAM\nFigure 13: Visual results comparison between SAM vs. HQ-SAM on video object segmentation\nbenchmark DAVIS 2017 in zero-shot setting, given the same video boxes prompts generated by the\npre-trained XMem [7].\n17\nSAM\nSAM\nSAM\nSAM\nHQ-SAM\nHQ-SAM\nHQ-SAM\nHQ-SAM\nFigure 14: Visual results comparison between SAM vs. HQ-SAM on video instance segmentation\nbenchmark YTVIS 2019 in zero-shot setting, given the same video boxes prompts generated by the\npre-trained Mask2Former [4].\n18\nReferences\n[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258, 2021.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nIn NeurIPS, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[4] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, and Alexander G\nSchwing. Mask2former for video instance segmentation. arXiv preprint arXiv:2112.10764, 2021.\n[5] Bowen Cheng, Ross Girshick, Piotr Doll\u00e1r, Alexander C. Berg, and Alexander Kirillov. Boundary IoU:\nImproving object-centric image segmentation evaluation. In CVPR, 2021.\n[6] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung Tang. CascadePSP: Toward class-agnostic\nand very high-resolution segmentation via global and local refinement. In CVPR, 2020.\n[7] Ho Kei Cheng and Alexander G. Schwing. XMem: Long-term video object segmentation with an\natkinson-shiffrin memory model. In ECCV, 2022.\n[8] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu. Global contrast based\nsalient region detection. TPAMI, 37(3):569\u2013582, 2014.\n[9] Daan de Geus and Gijs Dubbelman. Intra-batch supervision for panoptic segmentation on high-resolution\nimages. In WACV, 2023.\n[10] Philipe Ambrozio Dias and Henry Medeiros. Semantic segmentation refinement by monte carlo region\ngrowing of high confidence detections. In ACCV, 2019.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n[12] Amin Ghiasi, Hamid Kazemi, Eitan Borgnia, Steven Reich, Manli Shu, Micah Goldblum, Andrew Gordon\nWilson, and Tom Goldstein. What do vision transformers learn? a visual exploration. arXiv preprint\narXiv:2212.06727, 2022.\n[13] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret\nZoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In CVPR, 2021.\n[14] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation.\nIn CVPR, 2019.\n[15] Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, and Kurt Konolige. On pre-trained image features\nand synthetic images for deep learning. In ECCV Workshops, 2018.\n[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In ICML,\n2019.\n[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.\n[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\nSer-Nam Lim. Visual prompt tuning. In ECCV, 2022.\n[19] Lei Ke, Martin Danelljan, Xia Li, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Mask transfiner for\nhigh-quality instance segmentation. In CVPR, 2022.\n[20] Lei Ke, Henghui Ding, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Video mask\ntransfiner for high-quality video instance segmentation. In ECCV, 2022.\n[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment anything.\nIn ICCV, 2023.\n[22] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as\nrendering. In CVPR, 2020.\n[23] Philipp Kr\u00e4henb\u00fchl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge\npotentials. In NeurIPS, 2011.\n[24] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified\nimage classification, object detection, and visual relationship detection at scale. IJCV, 128(7):1956\u20131981,\n2020.\n[25] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training\nby introducing query denoising. In CVPR, 2022.\n[26] Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class dataset\nfor few-shot segmentation. In CVPR, 2020.\n19\n[27] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190, 2021.\n[28] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones\nfor object detection. In ECCV, 2022.\n[29] Jun Hao Liew, Scott Cohen, Brian Price, Long Mai, and Jiashi Feng. Deep interactive thin object selection.\nIn WACV, 2021.\n[30] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement networks for\nhigh-resolution semantic segmentation. In CVPR, 2017.\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n[32] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023.\n[33] Qi Lu, Jason Kuen, Guo Weidong, Shen Tiancheng, Gu Jiuxiang, Li Wenbo, Jia Jiaya, Lin Zhe, and Yang\nMing-Hsuan. Fine-grained entity segmentation. arXiv preprint arxiv:2211.05776, 2022.\n[34] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alexander Sorkine-Hornung, and Luc\nVan Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017.\n[35] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling Shao, and Luc Van Gool. Highly accurate\ndichotomous image segmentation. In ECCV, 2022.\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, 2021.\n[37] Tiancheng Shen, Yuechen Zhang, Lu Qi, Jason Kuen, Xingyu Xie, Jianlong Wu, Zhe Lin, and Jiaya Jia.\nHigh quality segmentation for ultra high-resolution images. In CVPR, 2022.\n[38] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical image saliency detection on extended cssd.\nTPAMI, 38(4):717\u2013729, 2015.\n[39] Towaki Takikawa, David Acuna, Varun Jampani, and Sanja Fidler. Gated-scnn: Gated shape cnns for\nsemantic segmentation. In ICCV, 2019.\n[40] Chufeng Tang, Hang Chen, Xiao Li, Jianmin Li, Zhaoxiang Zhang, and Xiaolin Hu. Look closer to\nsegment better: Boundary patch refinement for instance segmentation. In CVPR, 2021.\n[41] Cameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richardson, A Stephen McGough, Nick Wright,\nBen Burville, and Per Berggren. Ndd20: A large-scale few-shot dolphin dataset for coarse and fine-grained\ncategorisation. arXiv preprint arXiv:2005.13359, 2020.\n[42] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran. Unidentified video objects: A benchmark for dense,\nopen-world segmentation. In CVPR, 2021.\n[43] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\nSegmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n[44] Qinrou Wen, Jirui Yang, Xue Yang, and Kewei Liang. Patchdct: Patch refinement for high quality instance\nsegmentation. In ICLR, 2023.\n[45] Yinghui Xing, Qirui Wu, De Cheng, Shizhou Zhang, Guoqiang Liang, and Yanning Zhang. Class-aware\nvisual prompt tuning for vision-language pre-trained model. arXiv preprint arXiv:2208.08340, 2022.\n[46] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via\ngraph-based manifold ranking. In CVPR, 2013.\n[47] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In ICCV, 2019.\n[48] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting image\nmodels for efficient video action recognition. In ICLR, 2023.\n[49] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful\nprompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021.\n[50] Yuhui Yuan, Jingyi Xie, Xilin Chen, and Jingdong Wang. Segfix: Model-agnostic boundary refinement for\nsegmentation. In ECCV, 2020.\n[51] Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, and Huchuan Lu. Towards high-resolution salient\nobject detection. In ICCV, 2019.\n[52] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon\nHong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint\narXiv:2306.14289, 2023.\n[53] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and Heung-Yeung Shum.\nDino: Detr with improved denoising anchor boxes for end-to-end object detection. In ICLR, 2023.\n[54] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic\nsegmentation on high-resolution images. In ECCV, 2018.\n[55] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\nnetwork. In CVPR, 2017.\n20\n[56] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language\nmodels. International Journal of Computer Vision, 2022.\n[57] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for\nzero-shot semantic segmentation. In CVPR, 2023.\n[58] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl,\nJianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In CVPR, 2023.\n[59] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\neverything everywhere all at once. In NeurIPS, 2023.\n21\n"
  },
  {
    "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
    "link": "https://arxiv.org/pdf/2306.01693.pdf",
    "upvote": "2",
    "text": "Fine-Grained Human Feedback Gives Better Rewards\nfor Language Model Training\nZeqiu Wu1\u2217\nYushi Hu1\u2217\nWeijia Shi1\nNouha Dziri2\nAlane Suhr3\nPrithviraj Ammanabrolu45\nNoah A. Smith12\nMari Ostendorf1\nHannaneh Hajishirzi12\n1University of Washington\n2Allen Institute for Artificial Intelligence\n3University of California, Berkeley\n4University of California, San Diego\n5MosaicML\nAbstract\nLanguage models (LMs) often exhibit undesirable text generation behaviors, in-\ncluding generating false, toxic, or irrelevant outputs. Reinforcement learning from\nhuman feedback (RLHF)\u2014where human preference judgments on LM outputs are\ntransformed into a learning signal\u2014has recently shown promise in addressing these\nissues. However, such holistic feedback conveys limited information on long text\noutputs; it does not indicate which aspects of the outputs influenced user preference;\ne.g., which parts contain what type(s) of errors. In this paper, we use fine-grained\nhuman feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as\nan explicit training signal. We introduce FINE-GRAINED RLHF, a framework that\nenables training and learning from reward functions that are fine-grained in two\nrespects: (1) density, providing a reward after every segment (e.g., a sentence) is\ngenerated; and (2) incorporating multiple reward models associated with different\nfeedback types (e.g., factual incorrectness, irrelevance, and information incom-\npleteness). We conduct experiments on detoxification and long-form question\nanswering to illustrate how learning with such reward functions leads to improved\nperformance, supported by both automatic and human evaluation. Additionally,\nwe show that LM behaviors can be customized using different combinations of\nfine-grained reward models. We release all data, collected human feedback, and\ncodes at https://FineGrainedRLHF.github.io.\n1\nIntroduction\nState-of-the-art AI is built on pre-trained language models that are then trained through interaction\nwith humans [29, 28, 9], with a combination of supervised learning and reinforcement learning.\nIncorporating human feedback into the process of language model (LM) training has been shown\nas effective to reduce false, toxic and other undesired model generation outputs [29, 3, 2, 33, 10].\nMany of these studies adopt reinforcement learning from human feedback (RLHF) [46], a framework\nthat converts human feedback into an effective LM training signal to reach these goals. Specifically,\nhumans are presented with two or more outputs and asked to select one or rank them, and this signal\nis then used to train a reward model, which computes a single scalar reward for each LM-generated\nsequence. The LM is then trained with RL to optimize the reward it receives (from the reward model).\nSuch a reward provides a relatively sparse training signal, especially for tasks that require the\ngeneration of long-form text\u2014making RLHF in such domains unreliable [33]. Furthermore, previous\nresearch [8, 18, 11, 45, 25, 43] into automated evaluation of generated text shows that it can be\nchallenging for human annotators to reliably compare the overall quality of two or more model\noutputs when the outputs contain a mixture of diverse undesired behaviors. They demonstrate how\ncategorizing and localizing model errors (i.e., fine-grained evaluation) provides explicit insights about\nwhich part of the model output has what type of problem. We thus ask the question: how can we\nimprove rewards for LM training via RLHF by using more fine-grained human feedback?\n\u2217Equal contribution. Correspondence to <Zeqiu Wu: zeqiuwu1@uw.edu>, <Yushi Hu: yushihu@uw.edu>\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.01693v2  [cs.CL]  30 Oct 2023\nRelevant: + 0.3  Factual: - 0.5\nRelevant: + 0.3 Factual: + 0.5 Info. complete: + 0.3\nLM outputs:\nThe atmosphere of Earth is a \nlayer of gases retained by \nEarth\u2019s gravity\u2026\nThe atmosphere is commonly \nknown as air. The top gases \nby volume that dry air \u2026\nStep 1: Collect human feedback and train the reward models\nStep 2: Fine-tune the policy LM against the reward models using RL\nLM output: \nThe atmosphere of Earth is a layer of gases retained by Earth\u2019s \ngravity. The most common gas, by dry air volume, is nitrogen. \nThe second most is oxygen. The third most is carbon dioxide. \nIrrelevant / Redundant\nUnveri\ufb01able / Untruthful\nMissing\nThe third most is Argon.\nRelevance RM\nWhat are the 3 most common gasses in earth\u2019s atmosphere?\nPrompt:\nWhat are the 3 most common gasses in earth\u2019s atmosphere?\nPrompt:\nSampled Prompt: Does water boil quicker at high altitudes?\nIt takes longer for water to boil at high \naltitudes. The reason is that water boils at \na lower temperature at higher altitudes.\nPPO\nUpdate policy with rewards\nPreference Reward: - 0.35\nUpdate policy with rewards\nPPO\n(a) Preference-based RLHF\n(b) Ours: Fine-Grained RLHF\nHuman Feedback\nFactuality RM\nInformation \nCompleteness RM\nPreference RM\nFine-Grained Human Feedback\nIt takes longer for water to boil at high \naltitudes. The reason is that water boils at \na lower temperature at higher altitudes.\nSampled Prompt: Does water boil quicker at high altitudes?\nThe air that surrounds the \nplanet Earth contains various \ngases. Nitrogen\u2026\nThe atmosphere of Earth is \nthe layer of gases, generally \nknown as air\u2026\n   >         =         >\nFigure 1: Comparison of (a) RL with human preference and (b) our FINE-GRAINED RLHF on\nlong-form QA. Different from (a), which collects human preferences on the overall quality of LM\noutputs, we ask annotators to mark which part of an output contains what type(s) of errors. We train\na fine-grained reward model for each type of error and optimize LM against these reward models.\nIn this example, we provide a relevance reward and a factuality reward after each sentence is\ngenerated. There is also a holistic information completeness reward after the whole text is generated.\nIn this paper, we propose that humans give fine-grained feedback to LM output, associating categories\nof undesired behavior (e.g., false or irrelevant generations) and a text span at a density (e.g., sentence\nor sub-sentence-level). To enable LMs to learn from such fine-grained feedback, we introduce the\nFINE-GRAINED RLHF framework. As shown in Figure 1, we first use collected human feedback\nto train fine-grained reward models such that each of them focuses on one category and provides\nrewards at the density associated with that category. We then integrate these reward models into\nProximal Policy Optimization (PPO) [37], a commonly used RL algorithm for training LMs with\npreference-based human feedback (\u00a72).\nWe conduct experiments on two language generation tasks\u2014detoxification [12] (\u00a73) and long-form\nquestion answering (QA) [39] (\u00a74). For detoxification, toxicity is the only error category and we\nexplore learning with a dense reward. We adopt PERSPECTIVE [1], a widely used language toxicity\ndetection model trained on millions of human annotations, as our reward model. We use it to\ncalculate a fine-grained reward after the generation of every sentence. Our experimental results\nshow the efficacy and data efficiency of training models with dense reward compared to a holistic\nsequence-level reward, supported by automatic evaluation results.\nWith experiments on long-form QA, we aim to examine training models with fine-grained rewards\nat the two granularity dimensions (density and error category), for which we construct a long-form\nQA dataset, QA-FEEDBACK, along with our collected human feedback. We carefully develop a\npipeline to collect fine-grained human feedback on three error categories at different density levels:\ni) irrelevance, repetition, or incoherence (sub-sentence), ii) incorrect or unverifiable facts (sentence),\nand iii) incomplete information (whole sequence; see Figure 1). Our experimental results show\nimproved results in each error category by learning with such fine-grained feedback, supported by\nboth automatic and human evaluation results. In a scenario with multiple reward models representing\ndifferent error types, we also show FINE-GRAINED RLHF allows us to combine reward models with\ndifferent weights and thus control the model training process towards a customized combination of\ndesired behaviors.\n2\n2\nFINE-GRAINED RLHF\nWe introduce FINE-GRAINED RLHF, a framework that enables us to train fine-grained reward func-\ntions for generation outputs across different feedback types. We first define the RL environment and\nlearning algorithm. Then we define the fine-grained reward models and describe how to incorporate\nthe fine-grained reward model(s) into an RL algorithm, in contrast to previous RLHF studies that\nonly consider a single reward.\nEnvironment: language generation as a MDP. We focus on language generation tasks. For each\ntask, we are given a set of task input prompts D = {xn}N\nn=1. We follow [33] to define language\ngeneration as a Markov Decision Process (MDP) \u27e8S, A, R, P, \u03b3, Tmax\u27e9 with a finite vocabulary V.\nEach MDP episode starts with a sampled prompt x = (x1, x2, . . . , xl) with xi \u2208 V, and ends when\nthe current time step exceeds Tmax or an end of sequence token is generated. S is the state space and\ns0 = (x1, x2, . . . , xl) \u2208 S is the initial state. An action in the environment at \u2208 A is a generated\ntoken (by the policy LM model P\u03b8) at time t from V (a0 is the begin sequence token). The transition\nfunction P : S \u00d7 A \u2192 \u2206S appends at at the end of the state st = (x1, x2, . . . , xl, a0, a1, . . . , at\u22121).\nThis process continues until the end time step T \u2264 Tmax is reached, which gives a generated sequence\ny = (a1, . . . , aT ). A reward function R : S \u00d7 A \u2192 R, which comes from the reward model(s) in\nFINE-GRAINED RLHF, provides dense rewards before and when T is reached. P\u03b8 can be initialized\nwith a pre-trained language model, and sometimes also with supervised fine-tuning on task-specific\ndemonstrations. The reward function is defined later.\nLearning algorithm: proximal policy optimization (PPO). PPO [37] is an actor-critic RL algorithm\nthat is widely used in previous RLHF work to optimize the policy model against a reward model\nof human feedback. It uses a value model V\u03c8(st) to estimate the value of state st, and optimizes\nthe policy model with a PPO clipped surrogate training objective. The advantage At at timestep\nt is estimated by a generalized advantage estimation function [36]: At = PT\nt\u2032=t(\u03b3\u03bb)t\u2032\u2212t(rt\u2032 +\n\u03b3V\u03c8(st\u2032+1) \u2212 V\u03c8(st\u2032)), with \u03b3 as a hyperparameter and \u03bb as the discounting factor for rewards. rt is\nthe reward assigned to at, which in our case is acquired using one or multiple learned reward models.\nThe value model V\u03c8(st) is optimized with an expected squared-error loss with the value target as\nV targ(st) = PT \u22121\nt\u2032=t \u03b3t\u2032\u2212trt\u2032 + \u03b3T \u2212tV\u03c8old(sT ), where V\u03c8old is the lagging value model. Finally, PPO\nis trained to optimize both policy (P\u03b8) and value (V\u03c8) models with their respective objectives. No\nreward model is being optimized during PPO training. See Appendix B for more details.\nFine-grained reward models. Previous RLHF work adopts a holistic reward model R\u03d5 that maps\ninput prompt x and generated output y to a single scalar reward representing its overall quality\n(Figure 1(a)). This single scalar reward is only assigned to the final token in the generated sequence,\naT . Formally, rt = R\u03d5(x, y) if t = T and 0 otherwise.\nIn contrast, we consider a reward function that is derived from one or multiple fine-grained reward\nmodels that (1) provide rewards densely (i.e., for subsequences of the generated output), and (2)\ncompute rewards on distinct categories of undesired behaviors (e.g., false or repetitive generation),\nwhere each category is associated with an individual reward model.\nFor a fine-grained reward model R\u03d5k that gives feedback on error category Ck, we first segment y\ninto Lk segments (yk\n1, yk\n2, . . . , yk\nLk) corresponding to the density (e.g., sentence-level) of R\u03d5k, where\neach segment yk\nj ends at timestep T k\nj . R\u03d5k outputs a reward R\u03d5k(x, y, j) for each segment yk\nj given\nx and y as the input, which is assigned to the final token in yk\nj . Additionally, to ensure the fluency of\ngenerated outputs, we follow [41] to add an approximate KL divergence penalty to each token at\nwith a weight \u03b2, that is not backpropagated through during training. Formally, assuming that we\nhave K fine-grained reward models that represent different error categories, we will have a combined\nreward function for each token at as:\nrt =\nK\nX\nk=1\nLk\nX\nj=1\n\u0010\n1(t = T k\nj ) wk R\u03d5k(x, y, j)\n\u0011\n\u2212 \u03b2 log P\u03b8(at | st)\nP\u03b8init(at | st)\n(1)\nwhere wk \u2208 R is a weight assigned to reward model R\u03d5k. Then we follow the same PPO training\nalgorithm to optimize the policy model. We discuss how we define and train fine-grained reward\nmodels for the detoxification and long-form QA task in our experiments in \u00a7 3 and \u00a7 4 respectively.\n3\n3\nTask 1: Detoxification\nThe task of detoxification aims to reduce the toxicity in the model generation y when given a\nprompt x. Toxicity is the only undesired behavior in this task, and we aim to explore learning\nwith a dense reward in comparison to a single holistic reward. We conduct our experiments on\nREALTOXICITYPROMPTS, a dataset of 100K sentence-level prompts derived from the web that are\nknown to easily elicit problematic generations in GPT-2 [31]. Using a dense sentence-level fine-\ngrained reward, we demonstrate that our fine-grained reward exhibits greater sample efficiency\ncompared to a holistic reward, achieving lower toxicity with fewer training steps while maintaining\nbetter fluency (\u00a73.1).\nHolistic reward for (non-)Toxicity. We use the PERSPECTIVE API [1] as our reward model, which\nis widely used for language toxicity detection and is trained with millions of examples gathered\nfrom several online platforms and annotated by human annotators for toxicity. That means we use\nan off-policy reward model that is not trained on outputs from P\u03b8init. The API outputs a score\nbetween 0 (non-toxic) and 1 (toxic). Given the entire model output y, the holistic reward for RL is\n1\u2212PERSPECTIVE(y).\nSentence-level (fine-grained) reward for (non-)Toxicity. To calculate the fine-grained reward, we\nquery the API after the model generates each sentence instead of generating the full sequence. For\neach generated sentence yj, we assign PERSPECTIVE([y1, . . . , yj\u22121]) - PERSPECTIVE([y1, . . . , yj])\nas the sentence reward (i.e., how much toxicity is changed from generating yj). Since there is only one\nerror category, we omit the category superscript, using yj to denote the jth segment (e.g., sentence)\nin y.\n3.1\nExperiments\nImplementation details. We follow previous work [17, 21] and use GPT-2 large model as the initial\npolicy model P\u03b8init. During both the exploration stage in RL training and inference, we use nucleus\nsampling decoding with p = 0.9 and temperature = 1.0. The generation length limit is set to 48. The\nvalue model used during RL training is initialized with GPT-2-base due to GPU memory constraint.\nWe report RL training parameters in Appendix B. All scores are averaged over 3 independent runs.\nCompared systems and evaluation. We report the performance of FINE-GRAINED RLHF, RLHF\nwith holistic reward (Hol. RLHF), and the state-of-the-art controlled generation approaches GeDi\n[17] and DEXPERTS [21]. We follow previous work [17, 21] to report the toxicity score calculated\non each full generation sequence from the PERPLEXITY API, as well as other commonly used metrics\nfor REALTOXICITYPROMPTS, including n-gram diversity and GPT-2 XL perplexity (PPL) as a proxy\nfor fluency. The lower the perplexity, the more fluent the generated text. The toxicity score is reported\nas the maximum score among 4 sampled model outputs, averaged over all test input prompts. Other\nmetrics are reported as the average score of the same 4 samples.\nToxicity\nFluency\nDiversity\navg max (\u2193)\nPPL (\u2193)\ndist-2 (\u2191)\ndist-3 (\u2191)\nGPT-2\n0.192\n9.58\n0.947\n0.931\nControlled Generation\nGeDi\n0.154\n24.78\n0.938\n0.938\nDEXPERTS\n0.136\n22.83\n0.932\n0.922\nHol. RLHF\n0.130\n11.75\n0.943\n0.926\nF.G. RLHF\n0.081\n9.77\n0.949\n0.932\nTable\n1:\nResults\non\nthe\nREALTOXICI-\nTYPROMPTS test set.\nFigure 2: Curves of toxicity and perplexity on\nthe dev set vs. training steps.\nMain results. Table 1 shows the experimental results on the REALTOXICITYPROMPTS test set. FINE-\nGRAINED RLHF with sentence-level fine-grained reward attains the lowest toxicity and perplexity\namong all methods, while maintaining a similar level of diversity.\nSample efficiency analysis. Figure 2 shows the max toxicity and average perplexity on the devel-\nopment set during training. FINE-GRAINED RLHF has the toxicity drop much faster while keeping\na low-level perplexity. This shows that learning from denser fine-grained reward is more sample\nefficient than holistic reward. One explanation is that fine-grained reward locates where the toxic\n4\ncontent is, which is a stronger training signal compared with a scalar reward for the whole text. The\ncost is that we have to query the reward model more times per example.\n4\nTask 2: Long-Form Question Answering (QA)\nLong-form QA requires an LM to generate a textual response to a question with a comprehensive\nanswer and explanation. To examine learning with fine-grained rewards at the two granularity\ndimensions (error category and density), we collect QA-FEEDBACK (\u00a74.1), a long-form QA dataset\nannotated with human feedback on LM-generated responses. We define three error categories at\ndifferent density levels and train a reward model for each (\u00a74.2). We describe the experimental\nsetup in \u00a74.3. Both human and automatic evaluation show that FINE-GRAINED RLHF outperforms\npreference-based RLHF and supervised fine-tuning models on all error categories (\u00a74.4). We then\nshow that adjusting the weights of fine-grained reward models during RL training leads to distinct\nbehaviors in LM generation, allowing us to customize the LM for users with different needs (\u00a74.5).\nFinally, we conduct an in-depth analysis of the fine-grained reward models, revealing that they\ncompete against each other, and provide an analysis of their impact on the resulting policy model.\n4.1\nQA-FEEDBACK: Long Form QA with Human Feedback\nQA-FEEDBACK is based on ASQA [39], a dataset that focuses on answering ambiguous factoid\nquestions [26] in an open-domain setting. We use their provided oracle knowledge contexts to\nreformulate the task into a reading comprehension setting: given the input x that contains a question\nq and a set of knowledge passages P = {p1, . . . , p|P |}, generate a long-form response y. On average,\nthere are roughly 65 words in each gold response. Since ASQA does not release the test set, we create\nour own train/development/test data split from the original train and development sets. We name\nour newly constructed data, along with collected human feedback (discussed next), QA-FEEDBACK.\nOverall, we have 3,853 training, 500 development, and 948 test examples (details in Appendix C).\nInitial policy and fine-grained human feedback. Before collecting human feedback, we follow\n[33] to initialize the policy model with supervised fine-tuning on a small set of examples. Specifically,\nwe use 1K training examples to supervise fine-tuning of T5-large (the original baseline for ASQA)\n[32] to get P\u03b8init. We name this initial policy model SFT. We then sample outputs from SFT for the\nremaining training and development examples and collect fine-grained human feedback in three error\ncategories\u2014 C1: irrelevance, repetition, or incoherence ; C2: incorrect or unverifiable facts based\non knowledge passages; and C3: incomplete information . The collected feedback instances are then\nused as the training and development examples for training reward models. For each task prompt x,\nwe only collect fine-grained feedback for one model output. Our data collection has IRB approval\nand is deemed exempt.\nWe instruct workers to identify any error in each model output y = (a1, . . . , aT ), marking the span of\ntext associated with each identified error type. Formally, we define the set of user-annotated feedback\nfor a task prompt x and model output y as F = {fi} where each fi = \u27e8ci, bi, ei\u27e9 represents the\nuser-identified span (abi, . . . , aei) of the error category Cci, where ci \u2208 {1, 2, 3}. Importantly, we\nimpose three restrictions in the annotation: (1) error spans of category C1 or C2 should not overlap\nwith each other; (2) only spans that do not have error C1 need to be assessed as containing error\nC2 or not; (3) C3 can only apply to whole output sequences. Additionally, we ask workers to mark\npassage sentences that contain missing information if a C3 error is annotated. We also ask workers\nto rewrite y into a corrected version y\u2032 that addresses all annotated feedback F. Details about the\nfeedback collection interface, instructions, and quality control are in Appendix C.\nTo analyze human-human agreement, a subset of 300 examples receive annotations from two distinct\nworkers. We observe that while exact agreement in error span boundaries is low, workers achieve\nreasonably high agreement on whether a sub-sentence contains C1 and whether a sentence contains\nC2.2 Therefore, we decide to have the density for error type C1, C2, and C3 as sub-sentence, sentence\nand full sequence. We provide more data analysis including human agreement in Appendix C.\nPreference-based human feedback. For comparison purposes, we follow [29] to separately collect\npairwise human preferences from the same group of workers. We sample 4 model outputs for each\nprompt x, which gives 6 pairs of model outputs. We ask the workers to indicate pairwise preferences\n2We use spaCy [15] to segment generated model outputs into sentences. We then split sentences into\nsub-sentences using a comma or semicolon.\n5\n(ties are allowed) based on all errors they can find in each model output. They are not asked to\nexplicitly annotate these errors.\nAnnotation details. On average, both annotation tasks of fine-grained and preference feedback for\none question take a worker about 6 minutes to finish. In contrast, [39] report that they spend about 15\nminutes to label a human-written response for each question, which is much more time-consuming\nthan our feedback annotation. On average, we pay $1.65 per example for both tasks, leading to $16.50\nhourly pay for our workers. We include details of the pay structure in Appendix C. We observe that\nhuman annotators can reach a higher agreement in each aspect of fine-grained feedback compared to\npairwise comparisons because the feedback definitions are more concrete.\n4.2\nFine-Grained Reward Models\nWe train three separate reward models R\u03d51, R\u03d52, and R\u03d53 for C1, C2, and C3 error categories\nrespectively with a density of sub-sentence, sentence, and full sequence, respectively. Since reward\nmodels provide scalar reward scores and do not perform generation, we use the encoder-only\nLongformer-base [4] as our backbone model to handle long input sequences (more details of each\nreward model are in Appendix D).\nC1: Irrelevance, repetition, or incoherence. R\u03d51 targets to predict whether each sub-sentence in y\ncontains a C1 type error. We denote y = (y1\n1, . . . , y1\nL1), where y1\nj is the jth segment at R\u03d51\u2019s density\n(i.e., sub-sentence), with L1 segments in total. We add a 2-class token-level classification layer (a\nsingle feed-forward layer) on the top of the Longformer encoder. The model input has the format\nof \u201cquestion: q answer: [sep] y1\n1 [sep] y1\n2 ...\u201d, and we take the classification output at each\n[sep] token to indicate whether the following y1\nj contains a C1 error. We do not add passages in\nthe model input because, intuitively, the detection of C1 errors does not depend on them. To train\nR\u03d51, we apply a token-level classification loss to each [sep] token before y1\nj , where its gold label gj\nis \u201chas error\u201d if there is a fi \u2208 F that has (abi, . . . , aei) overlapped with y1\nj and ci = 1, and \u201cno\nerror\u201d otherwise. When R\u03d51 provides a reward during RL training as in Eq. 1, we read a reward\nR\u03d51(x, y, j) for every y1\nj given x and y. We define R\u03d51(x, y, j) = +1 if R\u03d51 predicts \u201cno error\u201d\nfor y1\nj and \u22121 otherwise.\nC2: Incorrect or unverifiable facts. R\u03d52 is developed for detecting a C2 error at the sentence level\nin a similar way. The model input has the format of \u201cquestion: q context: p1 p2 ... answer:\n[sep] y2\n1 [sep] y2\n2 . . . \u201d, where p\u2019s denotes the grounding passages and y2\nj represents the jth sentence.\nWe train R\u03d52 similarly to R\u03d51, with one exception: as we instruct the workers not to annotate a C2\nerror for a span that is already labeled as containing a C1 error, we do not calculate loss on sentences\nthat are labeled as containing C1 but not C2 during R\u03d52 training.\nC3: Incomplete information. R\u03d53 is trained to measure the information completeness of y, at the\nfull sequence level. Motivated by [19], R\u03d53 predicts a single scalar reward and is trained with a\npairwise comparison loss [29]:\nLr(\u03d5) = \u2212E(x,\u00afyp,\u00afyl)\u223cDp\nh\nlog\n\u0000\u03c3(R\u03d53(x, \u00afyp) \u2212 R\u03d53(x, \u00afyl))\n\u0001i\n(2)\nwhere R\u03d53(x, y) is the scalar output of the reward model for input x and output y; \u00afyp and \u00afyl are\nsampled from the same input x, and \u00afyp has less missed information compared with \u00afyl; Dp contains\nthe pairwise comparisons bootstraped from human feedback on C3 errors (see details in Appendix D).\nPreference-based reward model. The preference-based reward model is trained in a similar way\nto R\u03d53, with \u00afyp representing the human preferred response against \u00afyl in the loss function Eq. 2. It\noutputs a scalar score for the given x and y that represents the overall response quality.\n4.3\nExperimental Setup\nCompared systems. We compare our proposed method, FINE-GRAINED RLHF with the initial T5\npolicy model trained with 1K examples (SFT) and RLHF with holistic preference-based rewards\n(Preference RLHF). The reward models used in RLHF experiments are trained on 2.8K examples\nwith annotated feedback (but no gold human response). For analysis, we also use the human gold\nresponses of all training examples to finetune a fully supervised T5 model (SFT-Full). Notice that\nSFT-Full requires much higher annotation cost because it takes longer (15 minutes per example [39])\nfor annotators to draft long-form responses.\n6\nImplementation details. Our policy model is based on T5-large [32] and is supervised finetuned on\n1K training examples, as explained in \u00a74. During RL exploration, we use top-k (k = 20) sampling\ndecoding with temperature = 0.7, which is set based on previous RLHF work [33]. The value\nmodel used during RL training is initialized with T5-base due to GPU memory constraint. The\nreward model weights we used in FINE-GRAINED RLHF are w1 = 0.3, w2 = 0.5, w3 = 0.3, unless\notherwise specified. Although we use three reward models during RL training, we only observe\nvery small relative additional cost (roughly 1% training time) compared to preference RLHF. During\ninference, we use greedy decoding to generate responses. We report more details including RL\ntraining parameters in Appendix B. All scores reported are averaged over 3 independent runs.\nEvaluation. We conduct both human and automatic evaluation. Human evaluation is run on 200\nrandomly sampled test set examples of QA-FEEDBACK to compare Fine-Grained RLHF with all\nbaselines. Each model output is sampled from inference results of 3 training runs. We use the same\nprotocol of feedback collection to have the same set of workers annotate spans in each model output\nthat contain (1) irrelevance, repetition, or incoherence error (rel.) and (2) incorrect or unverifiable\nfacts (fact.) . They are also asked to compare the information completeness (comp.) for each\noutput pair. To report evaluation scores for rel. and fact. error spans, we first map them to their\ncorresponding error type density (sub-sentence and sentence). Then we report the error rate for each\nerror type, measured as the percentage of sub-sentences that contains this type of error. Since spans\nwith rel. error are not checked for fact. error (discussed in \u00a74.1), we exclude sub-sentences with only\nrel. error when report the error rate of fact. error. For automatic evaluation, we report RougeLSum\n[20] as used for the original ASQA data, as well as the score from each fine-grained reward model\n(R\u03d51, R\u03d52, and R\u03d53). Specifically, we report the percentage of all sub-sentences (or sentences) in the\ntest set predicted as \u201cno error\u201d by R\u03d51 (or R\u03d52). For R\u03d53, we report the averaged output score for\nall test examples.\n4.4\nMain Results\nFigure 3 shows the human evaluation results for rel. and fact. error types. Table 2 shows the human\npairwise comparison results for information completeness (comp.).\nError Rate\nFigure 3: Human evaluation on rel. (left) and fact. (right) error,\nmeasured by % of sub-sentences that contain the error type (\u2193).\nOurs vs.\nWin\nTie\nLose\nSFT\n23.0%\n65.5%\n11.5%\nSFT-Full\n22.0%\n61.0%\n17.0%\nPref. RLHF\n19.5%\n71.0%\n9.5%\nTable 2: Human pairwise compari-\nson on information completeness\n(comp.) , where win/lose refers to\nFINE-GRAINED RLHF.\nFINE-GRAINED RLHF outperforms SFT and Preference RLHF on all error types. Figure 3\nand Table 2 show that our FINE-GRAINED RLHF leads to generation that is much more factually\ncorrect and contains more complete information, compared to all other systems. It generates fewer\nirrelevance, repetition, andincoherence errors, compared with SFT and Preference RLHF. In the\nmeantime, Preference RLHF, despite greatly reducing factual errors compared to the initial policy\nmodel SFT, generates even more irrelevance, repetition, and incoherence errors than SFT. FINE-\nGRAINED RLHF outperforms Preference RLHF potentially due to more specific and localized training\nsignals. In addition, we ask annotators to compare the overall generation quality of FINE-GRAINED\nRLHF and preference RLHF. Although Preference RLHF is trained directly with such preference\nfeedback, FINE-GRAINED RLHF was rated better than Preference RLHF in 30.5% of all examples and\nworse in 24.5% of examples. The annotators indicate a tie in the remaining 45% of cases. Surprisingly,\nFINE-GRAINED RLHF outperforms SFT-Full with more factual and complete generation, despite a\nmuch lower annotation cost.\nRLHF is particularly effective in reducing factual errors. Figure 3 shows that both FINE-\nGRAINED RLHF and Preference RLHF are effective in reducing factual errors in model generation.\nMeanwhile, we see little or no improvement in reducing irrelevance, repetition, or incoherence errors.\nWe provide more in-depth analysis for this observation in \u00a74.5.\nTable 3 shows automatic scores on the QA-FEEDBACK test set, which show similar trends as human\nevaluation in terms of system comparisons, while all four systems achieve similar Rouge scores.\n7\nrel.\nfact.\ncomp.\nR\u03d51(\u2191)\nR\u03d52(\u2191)\nR\u03d53(\u2191)\nRouge(\u2191)\nSFT-Full\n0.508\n0.756\n0.044\n49.63\nSFT\n0.513\n0.749\n-0.053\n48.96\n+ Pref. RLHF\n0.482\n0.781\n0.101\n49.84\n+ F.G. RLHF\n0.513\n0.816\n0.139\n49.93\nTable 3: Automatic evaluation on the QA-\nFEEDBACK test set.\nrel.\nfact.\ncomp.\navg.\nConfig\nR\u03d51(\u2191)\nR\u03d52(\u2191)\nR\u03d53(\u2191)\nRouge(\u2191)\nlen\nShort\n0.637\n0.760\n-0.231\n48.99\n74.92\nMedium\n0.513\n0.816\n0.139\n49.93\n98.66\nLong\n0.425\n0.860\n0.241\n48.72\n109.63\nTable 4: Automatic evaluation results (test set)\nof FINE-GRAINED RLHF trained with different\nreward model weight configurations.\n4.5\nLM Customization with FINE-GRAINED RLHF\nSince we use multiple reward models in FINE-GRAINED RLHF, adjusting their weights (see Eq. 1)\nduring RL may lead to different LM behaviors. For example, adding more weight to a reward\nmodel associated with one specific desired behavior type (e.g., information completeness) may lead\nthe generation more towards that behavior type compared to others (e.g., information relevance).\nThis flexibility can potentially fit users with diverse needs. Therefore, in this section, we explore\nFINE-GRAINED RLHF\u2019s ability to customize the LM behavior.\nLM customization. As in Table 4, we explore three configurations of reward model weights (w1, w2,\nand w3 for R\u03d51, R\u03d52, and R\u03d53) and name them \u2018short\u2019, \u2018medium\u2019, and \u2018long\u2019 according to the LM\u2019s\naverage generation length. For simplicity, we fix w2 = 0.5 and w3 = 0.3, and use 0.4, 0.3, and 0.2\nfor w1, which leads to \u2018short\u2019, \u2018medium\u2019, and \u2018long\u2019 generation outputs respectively. We manually\ninspect 30 random examples and observe that (1) \u2018short\u2019 generates more relevant content, but is\nless factual and complete; (2) \u2018long\u2019, in contrast, gives the most factual and complete generation.\nThis reflects that the LM is referencing a large amount of content from passages; (3) The \u2018medium\u2019\nconfiguration balances the three rewards and has the highest Rouge score. 24/30 examples follow the\nabove rule. Qualitative analysis and examples of LM customization are in Appendix A.\nTrade-off between error types. We observe that a higher w1 leads to a bigger rel. reward, smaller\nfact. and comp. rewards, and shorter generated outputs. One interpretation is that R\u03d51 penalizes\ntext spans that are irrelevant to the questions. As such, it encourages answering the question directly\nand penalizes referencing passages and generating auxiliary information. This reduces the model\ngeneration length and information completeness, and induces more factual errors.\n4.6\nAnalysis\nFigure 4: Dynamics of each type of re-\nward during training (reward vs. training\nsteps). All rewards are z-normalized.\nrel.\nfact.\ncomp.\navg.\nR\u03d51(\u2191)\nR\u03d52(\u2191)\nR\u03d53(\u2191)\nRouge(\u2191)\nlen\nSFT\n0.514\n0.735\n0.065\n43.13\n96.69\nF.G. RLHF\n0.516\n0.825\n0.266\n44.29\n101.76\nw/o. R\u03d51\n0.249\n0.771\n0.742\n38.52\n179.31\nw/o. R\u03d52\n0.716\n0.640\n-0.177\n43.18\n78.08\nw/o. R\u03d53\n0.565\n0.799\n0.123\n43.61\n93.92\nTable 5: Ablation of reward models on the development\nset. R\u03d51, R\u03d52, and R\u03d53 correspond to the reward model\nfor relevance, factuality, and information completeness.\nReward models are competing against each other. In the prior section, we find that there is a\ntrade-off between error types. To further look into this phenomenon, we explore the dynamics of\neach reward model during training. Figure 4 shows each reward model\u2019s rewards on the development\nset during training. All rewards are z-normalized for visualization. We see that the fact. reward\nis consistently increasing. The rel. reward increases rapidly in the first 250 steps and then starts\ndecreasing, while the comp. reward exhibits an opposite trend, decreasing at first and then starting\nto increase. As discussed earlier, one interpretation is that relevance (precision) and information\ncompleteness (recall) can be adversarial objectives, so the rewards are competing. The three rewards\nreach an equilibrium point in later steps.\nAblation: Does the LM learn from all reward models? What if we remove one reward model?\nTable 5 explores the policy LM behavior when one of the three reward models is removed during\ntraining. Qualitative examples are in Appendix A. First, we observe that the corresponding reward\ndecreases dramatically when the model is removed. When the rel. reward model ( R\u03d51 ) is removed,\n8\nthe outputs become extremely long and the comp. reward is extremely high. We observe the outputs\nand find the model is copying a lot of content from the passages. When the fact. reward model\n( R\u03d52 ) is removed, the rel. reward becomes the highest. We observe that the LM tends to answer\nthe question directly and not reference the passages, which causes a lot of hallucinations. When the\ncomp. reward model ( R\u03d53 ) is removed, the outputs are concise and factual but not providing all\nrelevant information to the question. Thus, it has lower information completeness and Rouge score\ncompared with the LM trained with all reward models.\nReward model performance. We report and analyze the performance of each reward model in\npredicting its corresponding error category. The rel. reward model R\u03d51 has a binary classification\naccuracy of 69.6, and an F1 score (for the \u201chas error\u201d class) of 68.5 on model-generated sub-\nsentences from the development set. We sample 20 sub-sentences where R\u03d51 predicts the opposite\nof the human label, and observe that all of them either 1) contain relevant auxiliary information\nand are marked as \u201cno error\u201d by humans, or 2) are marked as irrelevant by humans but provide\nclosely related background information to the question. In other words, R\u03d51 is mostly struggling\nwith predicting the relevance of auxiliary information, and it rarely fails to predict a direct answer as\n\u201cno error\u201d.\nThe fact. reward model R\u03d52 has an accuracy of 77.8 and an F1 score of 67.5. We sample 20 sentences\nwhere R\u03d52 makes a prediction mistake and we observe that the mistakes often happen when the\ngenerated sentence is highly abstractive instead of directly copying information from the passage.\nWe also observe that more than 80% of human labeled factual errors occur when the model generates\na direct answer (not auxiliary information) that contains hallucinated information or a random entity\nfrom a passage. We notice that R\u03d52 correctly captures more than 80% of such errors.\nThe comp. reward model R\u03d53 has an accuracy of 70.9 in pairwise comparison. In contrast, the\npreference-based reward model only reaches an accuracy of 57.2. This helps confirm our intuition\nthat assessing long-form generation outputs holistically can be more ambiguous and subjective than\nevaluating the outputs with a focus on a specific undesired behavior type.\nComparison with ChatGPT responses. We experiment with answering the questions with ChatGPT.\nTo familiarize ChatGPT with the style of our LFQA task, we prompt it with the task instruction\nand a single random QA example (due to length limitation). ChatGPT achieves a RougeLSum\nscore of 40.92 on the test set, which is much lower than our models. We do not use our trained\nreward models to evaluate ChatGPT outputs because reward models trained on T5-large may not\ngeneralize well to ChatGPT. We instead manually inspect the ChatGPT responses, and observe that\nthey are mostly concise and factual, yet lack the auxiliary information necessary to clarify ambiguous\nquestions. Qualitative examples are in Appendix A. This shows the difficulty for ChatGPT in learning\nuser-desired behaviors through simple prompting.\n5\nRelated Work\nReinforcement learning from human feedback (RLHF). RLHF [46, 42, 29] aims to optimize\nthe policy language model to generate content that is desired by human. This framework has been\nexplored to improve the model performance on a variety of natural language processing tasks such\nas text summarization [40], instruction following [29], question answering [24, 27] and reducing\nharmfulness [3, 2, 22, 10]. Most of these studies collect human preferences over pairs of model\noutputs on one or a set of desired attributes, in order to train a reward model to assign a holistic score\nfor a generation output during RL training. [13] trains separate reward models that assign scores for\ndifferent desired attributes, but still uses a single reward that combines scores from all reward models.\nIn contrast, we explore RLHF with fine-grained reward models trained on human feedback where\neach reward model provides dense reward after every small text segment for a specific type of desired\nbehavior. [30] explores using intermediate rewards to improves LM performance on reasoning tasks.\nLearning from human feedback in NLP. There also exists work that explores non-RL methods to\nlearn from human feedback. [44] trains a reward model that predicts a single score for each model\noutput and selects samples with the highest reward scores for supervised fine-tuning. [38, 14, 42]\ntrain a conversational model to predict both the response and a binary user satisfaction score in order\nto improve the response generation. Besides such numerical human feedback, natural language (NL)\nhuman feedback has also been explored. [23, 6] collect and store NL human feedback in a feedback\nmemory for the model to retrieve and then perform the end task conditioning on the retrieved feedback.\n[5, 35, 34] use a refinement model to refine model outputs conditioning on NL human feedback and\n9\nthen use a reward model to select the best refined outputs for supervised fine-tuning. Methods for\nusing a reward model to guide LM generation towards desired behaviors at inference time [21, 7] can\ncomplement our work that aims to improve the LM during training. [16] also explores incorporating\nhuman feedback into LM pre-training.\n6\nDiscussion\nAnnotation Costs. It is important to note that the fine-grained human feedback used for training our\nfine-grained reward models does not incur a greater cost than holistic human preference. As outlined\nin \u00a7 4.2, our observations reveal that annotators require a substantial amount of time to compare two\nlengthy text outputs. For the long-form QA task, both fine-grained feedback and preference-based\nfeedback takes approximately 6 minutes per sample for an annotator.\n6.1\nBroader Impacts\nWe propose the FINE-GRAINED RLHF framework that can incorporate multiple reward models\nto provide dense rewards for RL training, which leads to LM outputs that are optimized towards\nsuch rewards. Our framework can be applied to any text generation task, thereby enhancing LM\nperformance by offering more nuanced guidance than holistic feedback. The key advantages of the\nFINE-GRAINED RLHF framework are two-fold:\nFlexibility. Our framework significantly expands the versatility of reward models for RLHF. For\nexample, future work involving fact-checking, sentiment classification, toxicity detection, among\nothers, can all be incorporated within this framework. LMs can be trained against all these reward\nmodels via FINE-GRAINED RLHF.\nControllablility. Having multiple reward models that stand for different feedback types allows the\nend user to exert greater control over RL training (e.g., through different combinations of reward\nmodel weights; see details in \u00a7 4.5). This leads to customized model behaviors, a benefit particularly\nvaluable for applications like educational tools where model personalization is crucial.\n6.2\nLimitations and Future Work\nOne major limitation of our framework comes from the additional compute cost of getting fine-\ngrained rewards, compared to RLHF with a holistic reward. For instance, in the detoxification task,\nwe need to make multiple PERSPECTIVE API calls for each model output depending on how many\nsentences are generated, while RLHF with a holistic reward only requires one. In the long-form QA\ntask, we need to calculate a dense reward from multiple reward models, which takes more compute\ntime and GPU memory than a single reward model.\nAnother limitation is that different tasks may have different definitions of fine-grained feedback in\nterms of the feedback types and the density level of each type. Therefore, defining feedback that is\nwell-suited for a task and training reward models accordingly requires non-trivial manual effort.\nFinally, in this work, we carefully control the quality of annotated feedback, which is then used to\ntrain reward models for RL. In practice, when a deployed model is released to the public, end users\ndon\u2019t always give clean feedback. Therefore, how to obtain effective learning signals from noisy\nhuman feedback in the wild still needs further investigation.\nSome other interesting questions to explore in the future include: 1) Can we obtain fine-grained\nfeedback from LMs like GPT-4 instead of humans to improve model performance and reduce\nannotation costs? 2) How can other non-RL approaches of using human feedback such as controlled\ngeneration during inference time complement FINE-GRAINED RLHF? 3) How would fine-grained\nreward and value model sizes affect policy model performance during RL training?\n7\nConclusion\nIn this work, we introduce FINE-GRAINED RLHF, a framework that enables LMs to learn from\nmultiple fine-grained reward models trained from human feedback, where each reward model detects\na specific error category and provides dense rewards. We conduct experimental analysis on two text\ngeneration tasks to illustrate the performance gain of FINE-GRAINED RLHF than RLHF over holistic\nrewards, supported by both automatic and human evaluation. Furthermore, we show that an LM can\nbe customized for specific needs using different combinations of fine-grained reward models.\n10\nAcknowledgments\nWe thank Jiacheng Liu for sharing the standard PPO training code, and Yizhong Wang for providing\ninsights during early discussions of the project. We also thank UW TIAL members for participating in\nour pilot feedback annotation. We extend our thanks to UW NLP members who provided insights or\nfeedback to our project. Lastly, we especially thank all our AMT workers for helping us annotate the\nhigh quality feedback data. This research was developed with funding from the Defense Advanced\nResearch Projects Agency (DARPA) under Contract No. FA8650-23-C-7316. This work was also\nfunded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF\nIIS-2044660, and ONR N00014-18-1-2826. The views, opinions and/or findings expressed are\nthose of the author and should not be interpreted as representing the official views or policies of the\nDepartment of Defense or the U.S. Government.\nReferences\n[1] Perspective API, https://github.com/conversationai/perspectiveapi.\n[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022.\n[3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai:\nHarmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\n[4] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\n[5] Angelica Chen, J\u00e9r\u00e9my Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan,\nSamuel R Bowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training\nwith natural language feedback. arXiv preprint arXiv:2303.16749, 2023.\n[6] Bhavana Dalvi Mishra, Oyvind Tafjord, and Peter Clark. Towards teachable reasoning systems:\nUsing a dynamic memory of user feedback for continual system improvement. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Language Processing, pages 9465\u20139480,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n[7] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason\nYosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled\ntext generation. In International Conference on Learning Representations, 2020.\n[8] Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, and Yejin Choi. Is GPT-3\ntext indistinguishable from human text? scarecrow: A framework for scrutinizing machine text.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 7250\u20137274, Dublin, Ireland, May 2022. Association for\nComputational Linguistics.\n[9] Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought\nhub: A continuous effort to measure large language models\u2019 reasoning performance. arXiv\npreprint arXiv:2305.17306, 2023.\n[10] Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for\nmoral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023.\n[11] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to\ngenerate text with citations. arXiv preprint arXiv:2305.14627, 2023.\n[12] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Real-\nToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020, pages 3356\u20133369, Online, November\n2020. Association for Computational Linguistics.\n11\n[13] Amelia Glaese, Nat McAleese, Maja Tr\u02dbebacz, John Aslanides, Vlad Firoiu, Timo Ewalds,\nMaribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment\nof dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.\n[14] Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from\ndialogue after deployment: Feed yourself, chatbot! In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, pages 3667\u20133684, Florence, Italy, July 2019.\nAssociation for Computational Linguistics.\n[15] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-\nstrength Natural Language Processing in Python. 2020.\n[16] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L Buckley, Ja-\nson Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human\npreferences. arXiv preprint arXiv:2302.08582, 2023.\n[17] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty,\nRichard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence\ngeneration. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages\n4929\u20134952, Punta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics.\n[18] Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan,\nand Kyle Lo. LongEval: Guidelines for human evaluation of faithfulness in long-form summa-\nrization. In Proceedings of the 17th Conference of the European Chapter of the Association for\nComputational Linguistics, pages 1650\u20131669, Dubrovnik, Croatia, May 2023. Association for\nComputational Linguistics.\n[19] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with\noptimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087, 2019.\n[20] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational\nLinguistics.\n[21] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A.\nSmith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts and\nanti-experts. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 6691\u20136706, Online, August 2021. Association for Computational\nLinguistics.\n[22] Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj\nAmmanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning.\nIn Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), 2022.\n[23] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing\nto improve GPT-3 after deployment. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 2833\u20132861, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics.\n[24] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chad-\nwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147,\n2022.\n[25] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit\nIyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation\nof factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023.\n[26] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. AmbigQA: Answering\nambiguous open-domain questions. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 5783\u20135797, Online, November\n2020. Association for Computational Linguistics.\n12\n[27] Reiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John\nSchulman.\nWebgpt: Browser-assisted question-answering with human feedback.\nArXiv,\nabs/2112.09332, 2021.\n[28] OpenAI. Gpt-4 technical report, 2023.\n[29] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback, 2022.\n[30] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert\nWest, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations, 2023.\n[31] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[33] Rajkumar Ramamurthy*, Prithviraj Ammanabrolu*, Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa,\nChristian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for\nnatural language processing: Benchmarks, baselines, and building blocks for natural language\npolicy optimization. In International Conference on Learning Representations (ICLR), 2023.\n[34] J\u00e9r\u00e9my Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho,\nand Ethan Perez. Training language models with natural language feedback. arXiv preprint\narXiv:2204.14146, 2022.\n[35] J\u00e9r\u00e9my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen,\nKyunghyun Cho, and Ethan Perez. Training language models with language feedback at\nscale, 2023.\n[36] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation. In Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2016.\n[37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[38] Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, and Jing Xu. When life gives you\nlemons, make cherryade: Converting feedback from bad responses into good labels. arXiv\npreprint arXiv:2210.15893, 2022.\n[39] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions\nmeet long-form answers. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 8273\u20138288, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics.\n[40] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback.\nArXiv, abs/2009.01325, 2020.\n[41] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and\nPaul Christiano.\nRecursively summarizing books with human feedback.\narXiv preprint\narXiv:2109.10862, 2021.\n[42] Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau, and Jason Weston.\nLearning new skills after deployment: Improving open-domain internet-driven dialogue with\nhuman feedback. arXiv preprint arXiv:2208.03270, 2022.\n13\n[43] Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang\nWang, and Lei Li. Instructscore: Towards explainable text generation evaluation with automatic\nfeedback. arXiv preprint arXiv:2305.14282, 2023.\n[44] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf:\nRank responses to align language models with human feedback without tears, 2023.\n[45] Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of\nattribution by large language models. arXiv preprint arXiv:2305.06311, 2023.\n[46] Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv,\nabs/1909.08593, 2019.\n14\nAppendices\nA Qualitative Examples for Long-Form QA\n16\nA.1\nExamples on LM Customization . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nA.2\nExamples on LM Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nA.3\nExamples on Reward Model Ablation\n. . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.4\nComparison with ChatGPT responses\n. . . . . . . . . . . . . . . . . . . . . . . .\n19\nB Algorithm and Training Details of FINE-GRAINED RLHF\n21\nB.1\nPPO with Fine-Grained Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nB.2\nImplementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC Long-Form QA Data and Human Feedback Annotation\n22\nC.1\nData Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nC.2\nHuman Feedback Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nC.3\nAnalysis of Collected Fine-Grained Feedback . . . . . . . . . . . . . . . . . . . .\n23\nD Long-Form QA Reward Model Training Details\n23\n15\nA\nQualitative Examples for Long-Form QA\nA.1\nExamples on LM Customization\nAs discussed in \u00a7 4.5, we can modify the weight of each fine-grained reward model during RL\ntraining to get LM with different behaviors. Here, we explore three configurations of reward model\nweights and name them \u2018short\u2019, \u2018medium\u2019, and \u2018long\u2019 based on the LM\u2019s average generation length.\nThe \u2018short\u2019 configuration generates concise and short responses, while the \u2018long\u2019 configuration\ngenerates detailed and long responses. Table 6 demonstrates the different behaviors of our customized\nLMs. Given the same question, each LM generates different amount of auxiliary information in the\nresponse.\nQuestion:\nWhen did the French join revolution on colonists\u2019 side?\nPassages:\nArticle Title: France in the American Revolutionary War\nFrench involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire,\nsecretly shipped supplies to the Continental Army. A Treaty of Alliance in 1778 soon followed, which led to shipments\nof money and material to the United States. Subsequently, the Spanish Empire and the Dutch Republic also began to send\nassistance, leaving the British Empire with no allies. France\u2019s help is considered a vital and decisive contribution to the\nUnited States\u2019 victory against the British. As a cost of participation in the war, France accumulated over 1 billion livres\nin debt. After its defeat in the Seven Years\u2019 War in 1763, France lost its vast holdings in North America. Meanwhile,\nthe American colonists and the British government began to fight over whether Parliament in London or the colonial\nassemblies had primary responsibility for taxation. As part of that conflict, the colonists organized the Boston Tea Party\nin response to a tax on tea. The British government responded by passing the Intolerable Acts, which included the closing\nof Boston Harbor and the revocation of Massachusetts\u2019s colonial charter. This conflict exacerbated tensions further. The\nideological conflict escalated into open warfare in 1775, at which point the American patriots revolted against British\nrule. France, who had been rebuilding their Navy and other forces, saw this as an opportunity to seriously weaken her\nperennial enemy.\nArticle Title: France in the American Revolutionary War\nFrance bitterly resented its loss in the Seven Years\u2019 War and sought revenge. It also wanted to strategically weaken\nBritain. Following the Declaration of Independence, the American Revolution was well received by both the general\npopulation and the aristocracy in France. The Revolution was perceived as the incarnation of the Enlightenment Spirit\nagainst the \"English tyranny.\" Benjamin Franklin traveled to France in December 1776 in order to rally the nation\u2019s\nsupport, and he was welcomed with great enthusiasm. At first, French support was covert: French agents sent the Patriots\nmilitary aid (predominantly gunpowder) through a company called \"Rodrigue Hortalez et Compagnie\", beginning in the\nspring of 1776. Estimates place the percentage of French supplied arms to the Americans in the Saratoga campaign at up\nto 90%. By 1777, over five million \"livres\" of aid had been sent to the American rebels. Motivated by the prospect of\nglory in battle or animated by the sincere ideals of liberty and republicanism, volunteers like Pierre Charles L\u2019Enfant\njoined the American army. The most famous was Lafayette, a charming young aristocrat who defied the king\u2019s order\nand enlisted in 1777 at age 20. He became an aide to George Washington and a combat general. More importantly, he\nsolidified a favorable American view of France.\nArticle Title: Treaty of Alliance (1778)\nOn March 17, 1778, four days after a French ambassador informed the British government that France had officially\nrecognized the United States as an independent nation with the signing of the Treaty of Amity and Commerce and the\nTreaty of Alliance, Great Britain declared war on France, thereby engaging that nation in the American Revolutionary\nWar. French entry into the war would lead to further escalation of the war when Spain entered the fight against Britain as\nFrance\u2019s ally, after the signing of the Treaty of Aranjuez on April 12, 1779, and again in December 1780 when Britain\ndeclared war on the Dutch Republic after seizing a Dutch merchant ship they claimed was carrying contraband to France\nduring the Affair of Fielding and Bylandt. After the signing of the treaty French supplies of arms, ammunition, and\nuniforms proved vital for the Continental Army, while their actions in the Anglo French War of 1778-83 in the West\nIndies and elsewhere forced Britain to redeploy troops and naval units away from the North American colonies to secure\ntheir holdings.\nLM outputs with different reward model configurations:\nShort:\nFrench involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire,\nsecretly shipped supplies to the Continental Army. A Treaty of Alliance in 1778 soon followed, which led to shipments\nof money and mat\u00e9riel to the United States. Subsequently, the Spanish Empire and the Dutch Republic also began to\nsend assistance, leaving the British Empire with no allies.\nMedium:\nFrench involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire,\nsecretly shipped supplies to the Continental Army. A Treaty of Alliance in 1778 soon followed, which led to shipments\nof money and mat\u00e9riel to the United States. Subsequently, the Spanish Empire and the Dutch Republic also began to\nsend assistance, leaving the British Empire with no allies. On March 17, 1778, four days after a French ambassador\ninformed the British government that France had officially recognized the United States as an independent nation, Great\nBritain declared war on France, thereby engaging that nation in the American Revolutionary War.\nLong:\nFrench involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire,\nsecretly shipped supplies to the Continental Army. A Treaty of Alliance in 1778 soon followed, which led to shipments\nof money and mat\u00e9riel to the United States. Subsequently, the Spanish Empire and the Dutch Republic also began to\nsend assistance, leaving the British Empire with no allies. On March 17, 1778, four days after a French ambassador\ninformed the British government that France had officially recognized the United States as an independent nation with\nthe signing of the Treaty of Amity and Commerce and the Treaty of Alliance, Great Britain declared war on France,\nthereby engaging that nation in the American Revolutionary War.\nTable 6: Qualitative examples on LM customization.\n16\nA.2\nExamples on LM Errors\nTable 7 and Table 8 show examples of LM outputs from all the compared systems (SFT, Pref. RLHF,\nand FINE-GRAINED RLHF). We mark the fine-grained errors on the model outputs. Overall, our\nFINE-GRAINED RLHF outperforms SFT and Pref. RLHF in all three error types.\nQuestion:\nWho was the prime minister of Canada in 1920?\nPassages:\nArticle Title: Robert Borden\nNot only did Borden\u2019s persistence allow him to represent Canada in Paris as a nation, it also ensured that each of the\ndominions could sign the Treaty of Versailles in its own right, and receive a separate membership in the League of\nNations. During the conference Borden tried to act as an intermediary between the United States and other members\nof the British Empire delegation, particularly Australia and New Zealand over the issue of Mandates. Borden also\ndiscussed with Lloyd George, the possibility of Canada taking over the administration of Belize and the West Indies, but\nno agreement was reached. At Borden\u2019s insistence, the treaty was ratified by the Canadian Parliament. Borden was the\nlast Prime Minister to be knighted after the House of Commons indicated its desire for the discontinuation of the granting\nof any future titles to Canadians in 1919 with the adoption of the Nickle Resolution. In 1919 Borden approved the use\nof troops to put down the Winnipeg general strike, which was feared to be the result of Bolshevik agitation from the\nSoviet Union. Sir Robert Borden retired from office in 1920. He was the Chancellor of Queen\u2019s University from 1924 to\n1930 and also was Chancellor of McGill University from 1918 to 1920 while still Prime Minister. Borden also served\nas Vice-President of The Champlain Society between 1923 and 1925. He was the Society\u2019s first Honorary President\nbetween 1925 and 1938.\nArticle Title: Robert Borden\nSir Robert Laird Borden, (June 26, 1854 \u2013 June 10, 1937) was a Canadian lawyer and politician who served as the eighth\nPrime Minister of Canada, in office from 1911 to 1920. He is best known for his leadership of Canada during World War\nI. Borden was born in Grand-Pr\u00e9, Nova Scotia. He worked as a schoolteacher for a period and then served his articles of\nclerkship at a Halifax law firm. He was called to the bar in 1878, and soon became one of Nova Scotia\u2019s most prominent\nbarristers. Borden was elected to the House of Commons of Canada in 1896, representing the Conservative Party. He\nreplaced Charles Tupper as party leader in 1901, and became prime minister after the party\u2019s victory at the 1911 federal\nelection. As prime minister, Borden led Canada through World War I and its immediate aftermath. His government\npassed the \"War Measures Act\", created the Canadian Expeditionary Force, and eventually introduced compulsory\nmilitary service, which sparked the 1917 conscription crisis. On the home front, it dealt with the consequences of the\nHalifax Explosion, introduced women\u2019s suffrage for federal elections, and used the North-West Mounted Police to break\nup the 1919 Winnipeg general strike.\nArticle Title: Arthur Meighen\nArthur Meighen Arthur Meighen (; 16 June 1874 \u2013 5 August 1960) was a Canadian lawyer and politician who served\nas the ninth Prime Minister of Canada, in office from July 1920 to December 1921 and again from June to September\n1926. He led the Conservative Party from 1920 to 1926 and from 1941 to 1942. Meighen was born in rural Perth\nCounty, Ontario. He studied mathematics at the University of Toronto, and then went on to Osgoode Hall Law School.\nAfter qualifying to practice law, he moved to Portage la Prairie, Manitoba. Meighen entered the House of Commons\nof Canada in 1908, aged 34, and in 1913 was appointed to the Cabinet of Prime Minister Sir Robert Borden. Under\nBorden, Meighen served as Solicitor General (1913\u20131917), Secretary of State for Canada (1917), Minister of Mines\n(1917; 1919\u20131920), Minister of the Interior (1917\u20131920), and Superintendent-General of Indian Affairs (1917\u20131920). In\n1920, Meighen succeeded Borden as Conservative leader and Prime Minister \u2013 the first born after Confederation, and the\nonly one from a Manitoba riding. He suffered a heavy defeat in the 1921 election to Mackenzie King and the Liberal\nParty, but re-entered Parliament through a 1922 by-election and remained as Opposition Leader.\nLM outputs\nSFT:\nSir Robert Borden was the eighth Prime Minister of Canada from 1911 to 1920. Arthur Meighen was the ninth Prime\nMinister of Canada from 1920 to 1926.\nPref.\nRLHF:\nSir Robert Laird Borden was a Canadian lawyer and politician who served as the eighth Prime Minister of Canada, in\noffice from 1911 to 1920. Arthur Meighen was a Canadian lawyer and politician who served as the ninth Prime Minister\nof Canada, in office from July 1920 to December 1921 and again from June to September 1926. He led the Conservative\nParty from 1920 to 1926 and from 1941 to 1942.\nFINE-\nGRAINED\nRLHF\nSir Robert Laird Borden was a Canadian lawyer and politician who served as the eighth Prime Minister of Canada, in\noffice from 1911 to 1920. Arthur Meighen was a Canadian lawyer and politician who served as the ninth Prime Minister\nof Canada, in office from July 1920 to December 1921 and again from June to September 1926.\nTable 7: Examples of errors in LM outputs of SFT, Pref. RLHF, and our FINE-GRAINED RLHF. We\nmark the C1 (irrelevance, repetition, or incoherence) and C2 (incorrect or unverifiable facts) types\nof errors in the model outputs.\n17\nQuestion:\nWhere does the flat iron cut of meat come from?\nPassages:\nArticle Title: Flat iron steak\nFlat iron steak (US), butlers\u2019 steak (UK), or oyster blade steak (Australia and New Zealand) is a cut of steak cut with\nthe grain from the shoulder of the animal. This produces a flavorful cut that is a bit tough because it contains a gristly\nfascia membrane unless removed. Some restaurants offer it on their menu, often at lower price than the more popular\nrib-eye and strip steaks of the same grade. This is used, in some places, as a means of selling a less expensive cut from\nthe same animal, for example Kobe beef. This cut of steak is from the shoulder of a beef animal. It is located adjacent to\nthe heart of the shoulder clod, under the seven or paddle bone (shoulder blade or scapula). The steak encompasses the\ninfraspinatus muscles of beef, and one may see this displayed in some butcher shops and meat markets as a \"top blade\"\nroast. Anatomically, the muscle forms the dorsal part of the rotator cuff of the steer. This cut is anatomically distinct\nfrom the shoulder tender, which lies directly below it and is the teres major. Flat iron steaks usually have a significant\namount of marbling.\nLM outputs\nSFT:\nFlat iron steak is a cut of steak cut with the grain from the shoulder of the animal. This produces a flavorful cut that is a\nbit tough because it contains a gristly fascia membrane unless removed. Some restaurants offer it on their menu, often at\nlower price than the more popular rib-eye and strip steaks of the same grade. This is used, in some places, as a means of\nselling a less expensive cut from the same animal, for example Kobe beef.\nPref.\nRLHF:\nFlat iron steak is a cut of steak cut with the grain from the shoulder of the animal. This produces a flavorful cut that is a\nbit tough because it contains a gristly fascia membrane unless removed. Some restaurants offer it on their menu, often at\nlower price than the more popular rib-eye and strip steaks of the same grade. This is used, in some places, as a means of\nselling a less expensive cut from the same animal, for example Kobe beef.\nFINE-\nGRAINED\nRLHF\nFlat iron steak is a cut of steak cut with the grain from the shoulder of the animal. This is located adjacent to the heart of\nthe shoulder clod, under the seven or paddle bone (shoulder blade or scapula). This steak encompasses the infraspinatus\nmuscles of beef, and one may see this displayed in some butcher shops and meat markets as a \"top blade roast.\"\nTable 8: Qualitative examples on another question. Here we highlight the information in blue that is\ncovered by the LM trained with FINE-GRAINED RLHF but missed ( C3: incomplete information\nerror) by SFT and Pref.RLHF.\n18\nA.3\nExamples on Reward Model Ablation\nAs discussed in \u00a7 4.6, reward models are competing against each other, and we experiment with\nremoving one of the three reward models during RL training. Table 9 shows an example of how LMs\nbehave in such scenarios. See \u00a7 4.6 for our observations.\nQuestion:\nHow much does it cost to raise the Costa Concordia?\nPassages:\nArticle Title: Costa Pacifica\nCosta Pacifica is a \"Concordia\"-class cruise ship for Costa Crociere. She was handed over to Costa Crociere on 29\nMay 2009. Her sister ships, \"Costa Concordia\" and \"Costa Serena\", were launched in 2006 and in 2007, with \"Costa\nFavolosa\" and \"Costa Fascinosa\" launched in 2011 and 2012 respectively. Onboard facilities include the Samsara Spa\nand PlayStation World, an area boasting PS3 consoles, and the first recording studio at sea. \"Costa Pacifica\" is the third\nship of the \"Concordia\"-class, preceded by sister ships \"Costa Concordia\" and \"Costa Serena\" and was followed by\n\"Costa Favolosa\" and \"Costa Fascinosa\" on 2011 and 2012, all part of the expansion program of Costa which entailed an\ninvestment of 2.4 billion Euro, and is currently the largest fleet expansion program in the world. \"Costa Pacifica\" was\nordered on 14 December 2005, by Carnival Corporation, the parent company of Costa Crociere. The order was four\nships, which includes; \"Carnival Dream\", for Carnival Cruise Lines; \"Nieuw Amsterdam\" for Holland America Line;\nand \"Ruby Princess\" for Princess Cruises, at a total cost of US $3 billion.\nArticle Title: Costa Concordia disaster\nOn 13 January 2012, the Italian cruise ship \"Costa Concordia\" ran aground and overturned after striking an underwater\nrock off Isola del Giglio, Tuscany, resulting in 32 deaths. The eight year old Costa Cruises vessel was on the first leg of a\ncruise around the Mediterranean Sea when she deviated from her planned route at the Isola del Giglio, sailed closer to\nthe island, and struck a rock formation on the sea floor. A six-hour rescue effort resulted in most of the passengers being\nbrought ashore. \"Costa Concordia\" was officially declared a \"constructive total loss\" by the insurance company, and her\nsalvage was \"one of the biggest maritime salvage operations\". On 16 September 2013, the parbuckle salvage of the ship\nbegan, and by the early hours of 17 September 2013, the ship was set upright on its underwater cradle. In July 2014, the\nship was refloated by large sponsons (metal tanks) welded to its sides and was towed 320 kilometres (200 miles) to its\nhome port of Genoa for scrapping which was finished in July 2017. The total cost of the disaster, including victims\u2019\ncompensation, refloating, towing and scrapping costs, is estimated at approximately $2 billion, more than three times the\n$612 million construction cost of the ship.\nLM outputs: removing one fine-grained reward model during training\nAll reward\nmodels:\nCosta Concordia is a cruise ship for Costa Crociere. On 13 January 2012, the Italian cruise ship Costa Concordia\nran aground and overturned after striking an underwater rock off Isola del Giglio, Tuscany, resulting in 32 deaths.\nThe total cost of the disaster, including victims\u2019 compensation, refloating, towing and scrapping costs, is estimated at\napproximately $2 billion, more than three times the $612 million construction cost of the ship.\nWithout\nrelevance\nRM R\u03d51 :\nOn 13 January 2012, the Italian cruise ship Costa Concordia ran aground and overturned after striking an underwater\nrock off Isola del Giglio, Tuscany, resulting in 32 deaths. On 16 September 2013, the parbuckle salvage of the ship\nbegan, and by the early hours of 17 September 2013, the ship was set upright on its underwater cradle. In July 2014, the\nship was refloated by large sponsons (metal tanks welded to its sides and was towed 320 kilometres (200 miles) to its\nhome port of Genoa for scrapping which was finished in July 2017. The total cost of the disaster, including victims\u2019\ncompensation, refloating, refloating, towing and scrapping costs, is estimated at approximately $2 billion, more than\nthree times the $612 million construction cost of the ship. The total cost of the disaster, including victims\u2019 compensation,\nrefloating, towing and scrapping costs, is estimated at approximately $2 billion.\nComment: Too much irrelevant information & repetition error.\nWithout\nfactuality\nRM R\u03d52 :\nThe cost to raise the Costa Concordia was estimated at approximately $2 billion , more than three times the $612 million\nconstruction cost of the ship.\nComment: Factuality error. Direct answer without any supporting evidence from the passages.\nWithout\ninfo. com-\npleteness\nRM R\u03d53 :\nCosta Concordia is a \"Concordia\"-class cruise ship for Costa Crociere. She was handed over to Costa Crociere on 29\nMay 2009. The total cost of the disaster, including victims\u2019 compensation, refloating, towing and scrapping costs, is\nestimated at approximately $2 billion, more than three times the $612 million construction cost of the ship.\nComment: Miss the auxiliary information about the disaster .\nTable 9: Qualitative examples on the reward model ablation experiment. We show the LM outputs\nwhen we use all the reward models and when one of the reward models is removed during training.\nWe also mark the errors in each output.\nA.4\nComparison with ChatGPT responses\nWe compare the responses generated by ChatGPT (one-shot)3 and our system in Table 10. As\ndiscussed in \u00a7 4.6, We find that ChatGPT responses are relevant and factual, yet lack the auxiliary\ninformation to answer the ambiguous questions. This shows that it is challenging for ChatGPT to\nlearn user-desired behaviors through prompting and in-context learning.\n3Since the input for each example is very long, we cannot fit more than one in-context example into the\nmodel.\n19\nQuestion:\nWhen did the Rams go to St Louis?\nPassages:\nArticle Title: History of the Los Angeles Rams\nThe Los Angeles Rams are a professional American football team that play in the National Football League (NFL).\nThe Rams franchise was founded in 1936 as the Cleveland Rams in the short-lived second American Football League\nbefore joining the NFL the next year. In 1946, the franchise moved to Los Angeles. The Rams franchise remained in the\nmetro area until 1994, when they moved to St. Louis, and were known as the St. Louis Rams from 1995 to 2015. The\nRams franchise returned to Los Angeles in 2016. This article chronicles the franchise\u2019s history during their time in Los\nAngeles, from playing at the Los Angeles Memorial Coliseum between 1946 and 1979, to playing at Anaheim Stadium\n(now known as Angel Stadium of Anaheim) in Anaheim from 1980 to 1994, and its return to Southern California\nbeginning with the season. On January 12, 1946, Dan Reeves was denied a request by the other National Football League\n(NFL) owners to move his team, the Cleveland Rams to Los Angeles and the then-103,000-seat Los Angeles Memorial\nColiseum. Reeves threatened to end his relationship with the NFL and get out of the professional football business\naltogether unless the Rams transfer to Los Angeles was permitted. A settlement was reached and, as a result, Reeves was\nallowed to move his team to Los Angeles.\nArticle Title: History of the St. Louis Rams\nThe professional American football franchise now known as the Los Angeles Rams played in St. Louis, Missouri, as\nthe St. Louis Rams from the 1995 through the 2015 seasons before relocating back to Los Angeles where the team had\nplayed from the 1946 season to the 1994 season. The Rams franchise relocated from Los Angeles to St. Louis in 1995,\nwhich had been without a National Football League (NFL) team since the Cardinals moved to Phoenix, Arizona in 1988.\nThe Rams\u2019 first home game in St. Louis was at Busch Memorial Stadium against the New Orleans Saints on September\n10, 1995, before the Trans World Dome (later the Edward Jones Dome, and now known as The Dome at America\u2019s\nCenter) was completed for their November 12 game against the Carolina Panthers. Their last game played at the Edward\nJones Dome in St. Louis was against the Tampa Bay Buccaneers on December 17, 2015, which they won, 31\u201323. The\nRams\u2019 last game as a St. Louis-based club was on January 3, 2016, against the San Francisco 49ers at Levi\u2019s Stadium,\nwhere they lost in overtime 19\u201316.\nComparison between our output and ChatGPT output\nChatGPT:\nThe Rams moved to St. Louis in 1995 and were known as the St. Louis Rams from 1995 to 2015.\nFINE-\nGRAINED\nRLHF\nThe Los Angeles Rams are a professional American football team that play in the National Football League (NFL). The\nRams franchise was founded in 1936 as the Cleveland Rams in the short-lived second American Football League before\njoining the NFL the next year. In 1946, the franchise moved to Los Angeles. The Rams franchise remained in the metro\narea until 1994, when they moved to St. Louis, and were known as the St. Louis Rams from 1995 to 2015. The Rams\nfranchise returned to Los Angeles in 2016.\nQuestion:\nWho sang the song I\u2019m Coming Out?\nPassages:\nArticle Title: I\u2019m Coming Out\nThere\u2019s also been a cover by Marcia Hines. It was featured as a cover for the video game \"Karaoke Revolution\" as well.\nIn more recent times, the song was sampled in Keyshia Cole\u2019s remake of \"I\u2019m Coming Out\" (featuring Iggy Azalea) for\nthe original soundtrack of the 2014 movie The Other Woman, starring Cameron Diaz, Kate Upton and Leslie Mann. The\nsong is sampled in \"Break Your Heart Right Back\" from Ariana Grande\u2019s sophomore album \"My Everything\" (2014).\nThis song was also covered for the soundtrack to the 2016 movie \"Trolls\". In 1997, German comedian Hella von Sinnen\nsampled \"I\u2019m Coming Out\" for her single \"Mein Coming Out\", using the term \"coming out\" in the context of coming\nout of the closet, or revealing one\u2019s homosexuality. In 2003, American singer Amerie covered \"I\u2019m Coming Out\" for\nthe soundtrack to the film \"Maid in Manhattan\". Her version was released as a single in select European countries and\nAustralia. The final single version to be released was the Loren Dawson remix. The original single version (which\nis similar to the original) can be found on both CD singles and the original motion picture soundtrack to \"Maid in\nManhattan\". Ross\u2019 version appears on the film\u2019s soundtrack album as well.\nArticle Title: I\u2019m Coming Out\n\"I\u2019m Coming Out\" is a song recorded by American singer Diana Ross. It was written and produced by Chic members\nBernard Edwards and Nile Rodgers, and released in August 22, 1980 as the second single from Ross\u2019 self-titled tenth\nalbum \"Diana\" (1980). In 1979, Ross commissioned Chic founders Nile Rodgers and Bernard Edwards to create material\nfor a new album after taking her daughters to see the band in concert, following the success of Ross\u2019s final Ashford\n& Simpson-composed and produced LP,\" The Boss. \"Rodgers got the idea for \"I\u2019m Coming Out\" after noticing three\ndifferent drag queens dressed as Diana Ross at a New York club called the GG Barnum Room. The lyrics hold an\nadditional meaning to Ross, as she was leaving Motown Records and \"coming out\" from under Berry Gordy\u2019s thumb.\nAccording to the BBC4 documentary \"How to Make It in the Music Business\", Ross loved the record until she put\nit in front of Frankie Crocker, who pointed out that \u2019I\u2019m Coming Out\u2019 is what homosexuals use to announce their\nhomosexuality; she ran back to the studio in tears and demanded to know why Rodgers wanted to ruin her career. The\nsong was another hit from the album, peaking at number five on the U.S.\nComparison between our output and ChatGPT output\nChatGPT:\nThe song \u201cI\u2019m Coming Out\" was recorded by American singer Diana Ross. It was written and produced by Chic\nmembers Bernard Edwards and Nile Rodgers, and released in August 22, 1980 as the second single from Ross\u2019 self-titled\ntenth album \u201cDiana\" (1980).\nFINE-\nGRAINED\nRLHF\n\u201cI\u2019m Coming Out\" is a song recorded by American singer Diana Ross. It was written and produced by Chic members\nBernard Edwards and Nile Rodgers, and released in August 22, 1980 as the second single from Ross\u2019 self-titled tenth\nalbum Diana. In 2003, American singer Amerie covered \"I\u2019m Coming Out\" for the soundtrack to the film Maid in\nManhattan. Her version was released as a single in select European countries and Australia. The final single version to\nbe released was the Loren Dawson remix.\nTable 10: Comparison between ChatGPT outputs and FINE-GRAINED RLHF outputs.\n20\nB\nAlgorithm and Training Details of FINE-GRAINED RLHF\nB.1\nPPO with Fine-Grained Rewards\nThe algorithm below shows in detail how PPO updates the policy LM P\u03b8 and the value model V\u03c8\nwith K fine-grained reward models R\u03d5k.\nAlgorithm 1 Fine-Grained Reinforcement Learning from Human Feedback (FINE-GRAINED RLHF)\nInput initial policy model P\u03b8init; initial value model V\u03c8init; K reward models R\u03d5k trained from human\nfeedback; task prompts D; hyperparameters \u03b3, \u03bb, \u03f5, \u03b2\n\u25b7 \u00a7 2\n1: policy model P\u03b8 \u2190 P\u03b8init, value model V\u03c8 \u2190 V\u03c8init\n2: for step = 1, ..., M do\n3:\nSample a batch Db from D\n4:\nSample output sequence yn \u223c P\u03b8(\u00b7 | xn) for each prompt xn \u2208 Db\n5:\nCompute rewards {rn\nt }|yn|\nt=1 for each sampled output yn by running R\u03d5k\n\u25b7 Eq. 1\n6:\nCompute advantages {At}|yn|\nt=1 and value targets {V targ(st)}|yn|\nt=1 for each yn with V\u03c8\n7:\nfor PPO iteration = 1, ..., \u00b5 do\n8:\nUpdate the policy model by maximizing the PPO clipped surrogate objective:\n\u03b8 \u2190 arg max\n\u03b8\n1\n|Db|\n|Db|\nX\nn=1\n1\n|yn|\n|yn|\nX\nt=1\nmin\n\u0012 P\u03b8(at | st)\nP\u03b8old(at | st)At, clip(vt, 1 \u2212 \u03b5, 1 + \u03b5)At\n\u0013\n9:\nUpdate the value model by minimizing a square-error objective:\n\u03c8 \u2190 arg min\n\u03c8\n1\n|Db|\n|Db|\nX\nn=1\n1\n|yn|\n|yn|\nX\nt=1\n\u0000V\u03c8(st) \u2212 V targ(st)\n\u00012\nOutput P\u03b8\nB.2\nImplementation Details\nModel architectures. For the detoxification experiments, the policy model is initialized with GPT2-\nlarge [31], and the value model is initialized with GPT2-base. For the long-form QA experiments,\nthe policy model is initialized with a supervised fine-tuned T5-large [32], and the value model is\ninitialized with T5-base. This design follows InstructGPT [29], which uses a larger (175B) policy\nmodel, and smaller value and reward (6B) models.\nTraining details on detoxification. For both the holistic reward baseline and the sentence-level\n(fine-grained) reward, we do a hyper-parameter search with the same set of hyper-parameters. For\ntraining, we run 200K episodes. The batch size (number of episodes per card during training) is 64.\nWe use Adam optimizer with a linear learning rate scheduler and 10 warmup steps. We perform\na hyper-parameter grid-search for peak learning rate \u2208 {5e \u2212 6, 1e \u2212 5, 2e \u2212 5}, KL coefficient\n\u03b2 \u2208 {0.1, 0.2, 0.3}, discounting factor \u03bb \u2208 {0.95, 0.97, 0.99}, and the frequency of exploration\n(number of sampled outputs) \u2208 {2, 4, 8}. We find that the higher the KL coefficient, the lower the\nperplexity, and the higher toxicity. This is consistent with findings from previous RLHF studies ([29],\n[33]). For a fair comparison, we eventually choose a set of parameters that achieve a similar level\nof perplexity for both reward models. The optimal set of hyper-parameters for holistic reward is\n\u03b2 = 0.3, \u03bb = 0.99. For sentence-level reward \u03b2 = 0.1, \u03bb = 0.95. The learning rate is 1e\u22125, and the\nexploration frequency is 4 for both experiments. We choose the checkpoint with the lowest validation\nset toxicity for evaluation. Regarding computation time, we use 2\u00d7 80G NVIDIA A100 GPU for\ntraining, and the run time is about 22 hours.\nTraining details on long-form QA. We conduct a similar hyper-parameter grid search as our\ndetoxification experiments. For long-Form QA, the input length limit is 1024, and the output length\nlimit is 200. Notice that this is much longer than detoxification, so we use a smaller batch size\nand fewer training episodes. We experiment with multiple combinations of reward model weights.\nFixing w2 = 0.5 (factuality reward weight), we perform a grid search on w1, w3 \u2208 [0.0, 0.5].\n21\nWe eventually choose w1 = 0.3, w2 = 0.5, w3 = 0.3, which reaches a balance between three\nreward models and allows all three rewards to increase during training. For training, the batch\nsize (number of episodes per card during training) is 32. We use Adam optimizer with a linear\nlearning rate scheduler and 100 warmup steps. We perform a hyper-parameter grid-search for peak\nlearning rate \u2208 {5e \u2212 6, 1e \u2212 5, 2e \u2212 5}, KL coefficient \u03b2 \u2208 {0.1, 0.2, 0.3}, discounting factor\n\u03bb \u2208 {0.95, 0.97, 0.99}, and the frequency of exploration \u2208 {2, 4, 8}. The optimal set of hyper-\nparameters for Pref. RLHF is \u03b2 = 0.2, \u03bb = 0.99. For FINE-GRAINED RLHF, \u03b2 = 0.3, \u03bb = 0.95.\nThe learning rate is 1e \u2212 5, and the exploration frequency is 4 for both experiments. we run 80K\nepisodes, which is approximately 5 epochs. We choose the checkpoint with the highest validation\nreward for evaluation. Regarding computation time, we use 2\u00d7 80G NVIDIA A100 GPU for training,\nand the run time is about 15 hours.\nA note on the error bars. All results we report in the paper are from 3 independent runs. The scores\nreported are all averaged across all runs. The error bars are represented as the shades behind each\ntraining curve in our figures. It shows the standard error across three runs.\nC\nLong-Form QA Data and Human Feedback Annotation\nC.1\nData Construction\nASQA [39] is a long-form QA dataset that focuses on answering ambiguous factoid questions in\nan open-domain setting that requires passage retrieval from a given Wikipedia passage corpus. We\nreformulate it into a reading comprehension setting: given the input x that contains a question q and\na set of knowledge passages P = {p1, ..., p|P |}, generate a long-form response y. To construct P\nfor each input x, we use the oracle knowledge contexts provided by ASQA for each x, that are text\nsnippets from the passage corpus. We use BM254 to map each knowledge context (text snippet) to\nthe closest passage from the passage corpus and use the resulting passages as P. Our train and dev\nexamples come from the original ASQA train set and our test examples are the original ASQA dev\nexamples.\nC.2\nHuman Feedback Annotation\nFine-grained feedback. As discussed in \u00a7 4.1, we first use 1K randomly sampled training examples\nto train a T5-large based supervised model SFT as the initial policy model P\u03b8init. Then we collect\nfeedback on sampled outputs from SFT for the remaining 2,853 training examples and the 500\ndevelopment examples, using the Amazon Machanical Turk platform.5\nFigure 5 shows the fine-grained human feedback annotation interface with an example from QA-\nFEEDBACK. In addition to the task input\u2014question q and oracle passages P, we also provide a\nhuman-written response from ASQA to the worker as reference. However, it is important to note\nthat, in practice, the annotation of our fine-grained feedback should not require the human-written\nresponse. The only purpose for us to provide the gold response is to have our workers follow the\nsame question interpretation and expected response of the workers who annotate for ASQA, such\nthat our experimental comparison with supervised models (SFT and SFT-Full; details in \u00a7 4.3) is fair.\nHowever, we still instruct our workers to strictly use the given passages for checking factual errors.\nFor each span error, we ask the worker to select one out of 5 categories shown in Figure 6 (left).6\nHowever, we collapse these 5 categories into two categories (C1 and C2 mentioned in \u00a7 4.1) based on\nwhether the error detection depends on the passages or not. When workers mark passage sentences as\ncontaining missing information, we instruct them to categorize each sentence as missing \u201canswer\u201d,\n\u201cmajor auxiliary information\u201d or \u201cminor auxiliary information,\u201d as shown in Figure 6 (right). Our\ninstruction to the worker is provided in Figure 8.\nQuality control. Before feedback collection, we design a qualification task to select qualified\nworkers for this feedback annotation task. The qualification task consists of 5 questions with their\ncorresponding passages and model outputs for the workers to annotate. We manually review about\n4https://github.com/castorini/pyserini\n5https://www.mturk.com/\n6We see very few \u201cincoherence\u201d errors (1%), so the majority of labeled errors are from the other four\ncategories during annotation.\n22\n70 submissions of the qualification task and select 15 workers whose annotation is marked by us\nas of high quality. Throughout the actual feedback annotation process, we constantly monitor the\nannotated data and send feedback to workers.\nPreference-based feedback. For comparison purposes, we follow [29] to collect pairwise human\npreferences from the same group of workers we select from the qualification task. We sample four\nmodel outputs for each prompt x, which gives 6 pairs of model outputs. Similarly, we provide the\nworker with the human-written response and ask the workers to indicate pairwise preferences (ties are\nallowed) based on all errors they can find each model output. Figure 7 shows the preference-based\nhuman feedback annotation interface with an example from QA-FEEDBACK.\nPay structure. We pay a base rate of $1.5 per example for annotating fine-grained or preference\nfeedback. If the example consists of \u2265 3 passages to read, we assign an additional $0.3 bonus to the\nexample. On average, we pay roughly $1.65 per example for both tasks, which gives an $16.5 hourly\npay for our workers.\nC.3\nAnalysis of Collected Fine-Grained Feedback\nOverall, among all error spans we collect, 76% of them are C1 errors and the remaining 24% are\nC2 errors. However, it is important to note that we instruct workers to label C2 errors only at places\nthat don\u2019t have a C1 error. 75% examples are labeled as being incomplete; i.e., containing missing\ninformation that can be found in the given passages (C3). Among all marked passage sentences that\ncontain missing information, 31%, 42% and 27% are missing answer, major auxiliary information\nand minor auxiliary information respectively.\nTo analyze human-human agreement, a subset of 300 examples receive annotations from two distinct\nworkers. We observe that while the exact agreement in error span boundaries is low, workers achieve\nreasonably high agreement on whether a sub-sentence contains C1 (reach an agreement for 83% of\nall sub-sentences) and whether a sentence contains C2 (92%). 7 The agreement on whether a model\noutput contains complete information or not (C3) is 85%. Therefore, we decide to have the density\nfor error type C1, C2, and C3 as sub-sentence, sentence and full sequence.\nD\nLong-Form QA Reward Model Training Details\nWe train reward models with the 2,835 training examples with feedback collected and select the best\nmodel for each error category based on the their performance on the development set. The batch size\nand training epochs are 24 and 50 for R\u03d51 and R\u03d52. Each training is run on a single 80G NVIDIA\nA100 GPU, taking 1 and 2 hours for training R\u03d51 and R\u03d52 respectively.8 The batch size and training\nepochs are 12 (per GPU) and 30 for R\u03d53 and the preference-based reward model. Each training is run\non 2\u00d7 80G NVIDIA A100 GPU and takes 2 hours. We use Adam optimizer with a linear learning\nrate scheduler for all reward model training. For each reward model, we search the learning rate over\n{5e\u22126, 1e\u22125, 5e\u22125}, weight decay over {0.001, 0.01}, and warm-up step ratio over {0.1, 0.2} based\non the dev set performance. Specifically, we use the model for R\u03d51 and R\u03d52 that achieve the best\nbinary classification accuracy. For R\u03d53 and the preference-based reward model, we select the model\nthat achieves the best pairwise comparison accuracy. We also provide more training details for each\nreward model below.\nR\u03d51 for C1: Irrelevance, repetition, or incoherence. To train the reward model R\u03d51 that detects\nerror of irrelevance, repetition, or incoherence, we apply a token-level classification loss to each\n[sep] token before y1\nj , where its gold label gj is \u201chas error\u201d if there is a fi \u2208 F that has abi,...,ei\noverlapped with y1\nj and ci = 1, and \u201cno error\u201d otherwise. We observe that most of the spans\nmarked as error type C1 that are shorter than 5 words usually carry very little information or are\nannotated as a result of workers being very careful or strict. Therefore, we filter out such short spans\nbefore constructing training examples for R\u03d51. Overall, we get 7379 and 8059 sub-sentences with\nthe \u201chas error\u201d and \u201cno error\u201d label respectively.\n7We use spaCy [15] to segment generated model outputs into sentences. We then split sentences into\nsub-sentences using a comma or semicolon.\n8Note that training R\u03d51 takes shorter time as its input does not contain passages.\n23\nR\u03d52 for C2: Incorrect or unverifiable facts. We train R\u03d52 in a similar way as how we train R\u03d51.\nInstead of predicting the error for each sub-sentence, R\u03d52 is trained to predict at the sentence level\n(i.e., y2\nj is the jth sentence in y). Since workers do not annotate C2 error for spans that are already\nlabeled as having C1 error, in order to avoid false negatives in training R\u03d52, we do not provide gold\nlabel nor calculate loss for sentences that only contain C1 error from training. In other words, all\nsentences that contain a C2 error has the gold label \u201chas error\u201d and sentences that contain no error\nhas the gold label \u201cno error\u201d. Overall, we get 1600 and 3411 sentences with the \u201chas error\u201d and\n\u201cno error\u201d label respectively.\nR\u03d53 for C3: Incomplete information. Instead of casting this as a classification task, R\u03d53 predicts\na single scalar reward given x and y and is trained with a pairwise comparison loss [29]. This is\nmotivated by early work [19] that shows the better reliability of pairwise comparison than error\nclassification when assessing a full generation sequence. To construct training data for R\u03d53, we\nbootstrap pairwise comparisons from the corrected model output y\u2032 as follows. We first map each\nsub-sentence in y\u2032 to a passage sentence in P that has a sub-string with the highest token-level F1\nscore with the sub-sentence,9 and denote all mapped sentences as S. We then sample four responses\nfrom SFT, for each we do the same sentence mapping to get a set of passages sentences S\u2032. We\ncalculate score(y) = |S\u2032 \u2229 S|/|S| as the information completeness score for each model response y.\nWe follow [29] to pair up sampled responses for q and denote each sampled response pair as (\u00afyp, \u00afyl),\nwhere score(\u00afyp) > score(\u00afyl). We drop the pairs where score(\u00afyp) = score(\u00afyl). Then we follow\n[29] to train R\u03d53 with the loss function in Eq. 2. We have a total number of 6821 pair examples in\ntraining.\nPreference-based reward model. The preference-based reward model is trained in a similar way as\nR\u03d53, with \u00afyp representing the human preferred response against \u00afyl in the loss function Eq. 2. We\ndrop the pairs where a tie is indicated. We have a total number of 14981 pair examples in training.\n9We manually review 50 mapped passage sentences and find over 90% of them are correctly mapped, which\nindicates frequent extractive behaviors from P\u03b8init.\n24\nFigure 5: Fine-grained feedback annotation interface.\nFigure 6: Error types in the fine-grained feedback annotation interface.\nFigure 7: Preference feedback annotation interface. The task input is omitted (same as in Figure 5).\n25\nFigure 8: Fine-grained feedback annotation instructions.\n26\n"
  },
  {
    "title": "Harnessing large-language models to generate private synthetic text",
    "link": "https://arxiv.org/pdf/2306.01684.pdf",
    "upvote": "2",
    "text": "Harnessing large-language models to generate private\nsynthetic text\nAlexey Kurakin\nGoogle\nkurakin@google.com\nNatalia Ponomareva\nGoogle\nnponomareva@google.com\nUmar Syed\nGoogle\nusyed@google.com\nLiam MacDermed\nGoogle\nmacdermed@google.com\nAndreas Terzis\nGoogle\naterzis@google.com\nAbstract\nDifferentially private training algorithms like DP-SGD protect sensitive training\ndata by ensuring that trained models do not reveal private information. An alter-\nnative approach, which this paper studies, is to use a sensitive dataset to generate\nsynthetic data that is differentially private with respect to the original data, and\nthen non-privately training a model on the synthetic data. Doing so has several\nadvantages: synthetic data can be reused for other tasks (including for hyper param-\neter tuning), retained indefinitely, and shared with third parties without sacrificing\nprivacy.\nHowever, generating private synthetic data is much harder than training a private\nmodel. To improve performance on text data, recent work has utilized public data\nby starting with a pre-trained generative language model and privately fine-tuning it\non sensitive data. This model can be used to sample a DP synthetic dataset. While\nthis strategy seems straightforward, executing it has proven problematic. Previous\napproaches either show significant performance loss, or have, as we show, critical\ndesign flaws.\nIn this paper we demonstrate that a proper training objective along with tuning\nfewer parameters results in excellent DP synthetic data quality. Our approach is\ncompetitive with direct DP-training of downstream classifiers in terms of perfor-\nmance on downstream tasks. Further, we demonstrate that our DP synthetic data\nis not only useful for downstream classifier training, but also to tune those same\nmodels.\n1\nIntroduction\nMachine learning models can memorize their training data [9] and it is possible to extract the training\ndata from a model [10]. Training a model with differential privacy (DP) [1] provably reduces the risk\nof memorization [32], which is critical when ML models are trained on sensitive data. However, DP\ntraining only ensures that the model does not release private information, and just releasing the model\nor its predictions is not adequate for many applications. For example, other researchers might want\nto use the data for analysis, or to build a different predictive model. It would therefore be ideal to\nrelease the dataset itself while protecting the privacy of the users that contributed to it.\nLocal differential privacy has been proposed as a method of preprocessing low-dimensional datasets\nbefore public release [33]. Local DP adds noise to individual data points in the training data. While\nprotecting privacy, local DP generally leads to much lower utility, due to the large amount of noise\nPreprint. Under review.\narXiv:2306.01684v2  [cs.LG]  11 Jan 2024\nthat must be added compared to central differential privacy, where DP is applied to the model or\nstatistical output [47, 3, 42]. Generally there is an the inherent tension between privacy and utility\nwhen releasing private datasets: we want to release a dataset that protects the privacy of the underlying\ndata while at the same time we want the dataset to be as useful as the original data for any possible\ndownstream task. Therefore, we focus on central DP and consider generating private synthetic data.\nGenerating such synthetic data involves creating a generative model that learns the original data\ndistribution. To protect the original data, either the generative model should be made private, via DP\ntraining, or privacy should be enforced at inference time (e.g., during the generation of synthetic data\nitems, so-called private prediction). Private inference has been shown to be inferior to DP training\nwhen a large number of inferences is required [45]. Since we seek to generate at least as much data\nas in the original dataset, DP training is the clear choice.\nSeveral works proposed using publicly pre-trained large language models (LLM) for private synthetic\ndata generation [6, 52, 34, 26]. This approach involves privately fine-tuning an LLM using class\nlabels as prompts for the model and subsequently sampling from this model. However these attempts\nhave had mixed success: they either reported poor utility even for non-private synthetic data, or had\nto augment standard NLP loss metrics to assist the LLM to correctly respond to prompts during\nthe generation process. Additionally, none of the previous work considered privacy leakage from a\npre-trained LLM itself. The privacy leakage happens because these papers used academic datasets\n(like IMDB [25]) as sensitive dataset and they utilized GPT-2 LLM [35] which was pre-trained on\nthese datasets without any privacy guarantees.\nAlthough we follow a similar recipe conceptually, in that we use a DP-finetuned LLM model to\ngenerate private synthetic data, we highlight the following differences in our execution of this idea:\n1. Privacy leakage mitigation. We draw attention to the need to account for the data that went\ninto pre-training of the LLMs used for generation. Our de-duplication of the pre-training\ndata ensures that no privacy leakage, possibly present in previous works, takes place.\n2. Reporting: We use a long sequence of text (512 tokens, representing full reviews like IMDB\nor Yelp) as our privacy unit. Our privacy guarantees (Appendix A) are tight and transparent,\nand we tune the hyperparameters of the downstream classifier on private synthetic data only.\n3. Method: We demonstrate that the standard approach to private fine-tuning does not yield the\ndesired quality of generated data. Instead of augmenting the LLM\u2019s objective or architecture\nfor fine-tuning as in [34, 26], we identify a loss function, well known to the NLP community,\nthat is particularly suitable for private fine-tuning. Additionally, we argue that parameter-\nefficient fine-tuning, especially LoRA tuning, is beneficial for synthetic data generation\nOur contributions can be summarized as follows:\n1. We demonstrate state-of-the-art results in terms of quality of synthetic data. Specifically, we\nshow in multiple experiments that the quality of the model trained on private synthetic data\nis comparable to or even better than the quality of the downstream model trained on real\ndata with DP.\n2. We demonstrate that parameter efficient fine-tuning like prompt-tuning and LoRA-tuning\nis superior to full fine-tuning when the tuning is performed privately. In particular, LoRA-\ntuning results in up to 11 percentage points lift in downstream model performance. To the\nbest of our knowledge, we are are the first to demonstrate that parameter-efficient tuning\nperforms better than full fine-tuning when each is combined with DP, whereas the opposite\noften holds for non-DP training [40, 7, 55].\n3. We show that generating more synthetic data than the size of the original dataset is helpful,\nespecially for simpler downstream models.\n4. We show that DP synthetic data can be used to tune the hyperparameters of the downstream\nclassifiers. We achieve ranking correlation with the ordering of trials performed on real data\nof up to 87%, even for \u03f5 = 1.\n2\nRelated work\nPrivacy-preserving synthetic data generation requires that the generated data is both high-fidelity (i.e.,\nexhibits similar distributional characteristics as the original data) and anonymized to preserve the\nprivacy of the users who contributed their data. For complex data like text, images, audio and video,\nmost existing approaches build a generative model, for example a GAN-based model [16]. However,\n2\nin most previous work the data is anonymized using heuristic methods, without providing formal\nprivacy guarantees. For example, Melamud and Shivade [28] attempted to de-identify summaries of\nclinical discharge notes using heuristic rules for an LSTM model and only empirically demonstrated\nthe privacy of the synthetic data.\nDP-fine tuning is a standard method for fine tuning LLMs that satisfies differential privacy guarantees\nand has been shown to perform well with appropriate hyperparameter tuning [23, 51]. DP-fine tuning\ninvolves using a pre-trained model and a modification of a training algorithm like DP-SGD to fine\ntune the model on private data.\nFor private synthetic text generation, Bommasani et al. [6] suggested using a pre-trained GPT-2\nmodel and then DP-fine tuning it on private data with word-level privacy, but did not implement or\nevaluate any method. In similar vein, Yue et al. [52] DP-fine tuned pre-trained GPT models of various\nsizes. While they do obtain good results on some of the benchmarks, they also observe up to 25%\ndrop of downstream model accuracy on synthetic data (even without DP) on other benchmarks. Putta\net al. [34] attempted a similar recipe on a pre-trained distilGPT2 model, but also reported a large\nperformance drop of the classifier trained on synthetic data. Additionally, they proposed modifying\nthe fine tuning process to also include a discriminator that attempts to distinguish between the labels\nto improve the separability of learned representations for two binary classes of the text data. Similarly,\nMattern et al. [26] proposed augmenting the training objective with an additional term penalizing the\ngeneration of sample with the wrong label.\nNone of the prior work takes into account problem of data contamination between LLM pre-training\ndataset and dataset used in downstream task. As we show in Appendix D this problem is real. In\nparticular some of both training and test samples examples from downstream datasets could be\nfound in GPT-2 pre-training data, which is used by all prior work. This may potentially invalidate\nDP-guarantees and may result in overestimated accuracy on downstream tasks.\nAdditionally none of the works on DP synthetic data mentioned above explored parameter-efficient\nfine tuning. To the best of our knowledge, we are the first to demonstrate that parameter-efficient\nfinetuning like LoRA tuning can produce better quality synthetic DP data than full finetuning.\n3\nPreliminaries\nDifferential privacy\nDifferential Privacy (DP) [14] is considered the gold standard for ensuring\ndata anonymization. Throughout this work we employ a notion of DP called (\u03f5, \u03b4)-DP.\nDefinition 1 ((\u03f5, \u03b4)-Differential Privacy, [13]). Consider neighbouring datasets to be datasets that\ndiffer only in addition or removal of one record only. Given non-negative \u03f5 and \u03b4 \u2264 1, a mechanism\nA is (\u03f5, \u03b4)-DP if for any two neighbouring datasets D and D\u2032 and for any S \u2286 Range(A),\nP[A(D) \u2208 S] \u2264 exp(\u03f5) \u00d7 P[A(D\u2032) \u2208 S] + \u03b4.\n(1)\nThe \u03f5 and \u03b4 values determine the strength of the privacy guarantees, with smaller values corresponding\nto stronger guarantees. The post-processing property of a DP mechanism means that applying any\ndata-independent transformation to its output will remain DP with the same guarantees.\nDP in context of ML models\nIn context of ML, DP can be introduced either at the input level,\nduring the training of a model (DP-Training), or during model serving (prediction) [33]. DP synthetic\ndata falls into the first category and in general is a harder task than introducing DP during the training.\nThis is because DP synthetic data ensures that any ML model trained on this data is DP with respect\nto the original training data. This is in contrast with DP-Training that only ensures that a particular\nML model is DP. Therefore, it is expected that any model trained on DP synthetic data should perform\nat most as well as the downstream DP-Trained ML model on real data. However the idea of using a\npre-trained generative LLM to aid generation of synthetic data means that we inject massive amount\nof public data, making the task of DP synthetic data generation less daunting.\nThe most practical methods of DP-Training for non convex losses are gradient-noise injection methods\nlike DP-SGD [1], which work by clipping per example gradients to limit the sensitivity of the loss,\nand noising aggregated clipped gradients with Gaussian noise to make them private. The noise level\nis proportional to the clipping norm (the sensitivity) and the strength of \u03f5 guarantees. The same recipe\ncan be adopted to adaptive optimizers like Adafactor [39], where the noised gradients are passed to\nthe optimizer to figure out the optimal learning rate.\n3\nLLMs\nThroughout the paper we will use the terms of pre-training and fine-tuning of LLMs: pre-\ntraining is the initial training of a LLM with a large public dataset, for example C4 [36]. Fine-tuning is\nan adaptation of a pre-trained model to perform some concrete task, for example question-answering,\nwhich involves running several epochs of an optimizer over the additional task training data.\n4\nMethodology\nTest set\nValidation set\nTraining set\nSensitive data\nValidation set\nTraining set\nSynthetic data\nPre-trained\nLLM\nFine-tuned\nLLM\nHyperparameter sweep\nSampling\nDownstream \nmodel\nDifferentially private \nfine-tuning\nTraining\nHyperparameter sweep\nFinal \nevaluation\nFigure 1: Synthetic data generation and evaluation.\nAs a motivational example, consider the task of medical data sharing for research purposes: a medical\nprovider has a sensitive dataset with patients records and wants to accomplish some machine learning\ntask. They may want to share the dataset with external researchers and academic institutions to get\ntheir help in solving the downstream task, while preserving the privacy of the original data.\nWe assume that we have a sensitive dataset D consisting of (Dtrain, Dvalid, Dtest), where the\nprivacy of each record must be protected (see additional details on the unit of privacy in Appendix\nA). We want to accomplish some task on this dataset, such as training some downstream machine\nlearning model. Additionally, we would like to allow a non-trusted third party to be able to perform\nthe downstream task without violating privacy. To achieve this, we aim to create a synthetic dataset\nDsynth, which is DP with respect to the dataset D. Our dataset Dsynth will consist of synthetic\ntraining and validation splits. Figure 1 illustrates our methodology of data generation and evaluation:\n1. Privately finetune (e.g., using DP-Training) a publicly pre-trained generative LLM G on\nDtrain, using Dvalid for hyperparameter tuning. To tune hyperparameters for DP-Training,\nwe follow an algorithm outlined in [33] (Section 5.4.1).\n2. Independently sample G to generate two new synthetic datasets Dsynth\ntrain and Dsynth\nvalid which\nwill serve as synthetic training and validation data.\n3. Train a downstream model M on Dsynth\ntrain and use Dsynth\nvalid for hyperparameter tuning.\n4. Evaluate the final performance of the model on real dataset Dtest.\n4.1\nUsing an LLM for data synthesis\nBoth encoder-decoder or decoder-only pretrained language models can generate synthetic data; we\nuse decoder-only LLMs in our experiments. To finetune LLM for the synthetic data generation\ntask, we use the next token prediction objective setup as follows. Given an example from the\nsensitive dataset with text x and label y, we generate a prefix p = \"[TaskName] [LabelNamey] \",\nwhere \"[TaskName]\" is the name of the task (for example \"[imdb]\"), and \"[LabelNamey]\" is\n\"[negative]\" when y = 0 or \"[positive]\" when y = 1. We finetune the model using the\nPrefix-LM objective [37] using p as a model input and x as a target.\nBelow we outline how the Prefix-LM way of splitting of the training example into input and target\nis advantageous for DP-training. Let\u2019s consider some example from the dataset which is tokenized\ninto input prefix p = {z1, . . . , zk} and target x = {zk+1, . . . , zn}. Typically, weighted next token\nprediction cross entropy loss looks like the following: L(\u20d7z, \u20d7w, \u03b8) = \u2212 Pn\ni=1 wizi log P(z|z<i, \u03b8)\nwhere \u03b8 - model parameters, \u20d7z = {z1, . . . , zn} is tokenized training example (including input and\n4\ntarget tokens), each zi as one hot encodings of token, P(z|z<i) is the probability of i-th token given\nvalues z<i of all previous tokens and \u20d7w = {w1, . . . , wn} is weights vector for each token in the loss.\nStandard next-token prediction loss assigns weights wi = 1 to all tokens, including those in the prefix\np. As a result prefix tokens will be included in the gradient of the loss \u2202L\n\u2202\u03b8 , thus essentially forcing\nthe model to learn the distribution of tokens in the prefix as well. On the other hand, the Prefix-LM\nformulation assigns zero weights to the prefix1, i.e. \u2200i \u2264 k : wi = 0, so the total loss looks like the\nfollowing: LPrefixLM(\u20d7z, \u20d7w, \u03b8) = \u2212 Pn\ni=k+1 zi log P(z|z<i, \u03b8)\nAs a result the LLM is not forced to learn distribution of input prefix p which we found to be beneficial\nfor differentially-private training. DP-Training adds the noise to all the gradients, in a standard setup\nthis will result in the gradients from the prefix portion being corrupted with the noise. This in turn\nmeans that prompting the DP-Trained LLM to generate synthetic data will not work as well as\nexpected. We believe this is the same phenomenon that was observed in works Putta et al. [34] and\nMattern et al. [26], where authors had to add an adversarial head or augment the loss respectively, to\naid the model in differentiating different types of prompts. Prefix-LM in turn is a standard loss well\nknown to the community, and this comes with the benefits of knowing approximate hyperparameter\nvalues for its tuning. The aforementioned Prefix-LM setup allows to train one model for all the class\nlabels and can be easily extended beyond the binary classification setup.\n4.2\nParameter-efficient fine tuning\nFull finetuning of large models is expensive, and, empirically, tuning very large number of weights\nwith DP-finetuning often results in substantial utility drop. Many techniques exist that update the\npretraining model without resorting to full model weights update. In this work, we consider two\npopular ones - Prompt Tuning and LoRA.\nPrompt tuning [22] is a technique which prepends a small prompt tensor in front of the model\u2019s input\nin the embedding space, freezes the rest of the model\u2019s parameters and then finetunes only the prompt\ntensor weights. We found that combining prompt tuning with differentially private training allows us\nto achieve much higher utility of the trained generative model compared to full model fine-tuning.\nThis could be explained by the fact that the prompt tensor is much smaller compared to the size of\nentire model (we used prompt tensor with 20480 parameters vs 8B weights in the full model) and\nsmaller models tend to have smaller gap between private and non-private utility [2, 8], probably due\nto the total amount of noise injected during the training. It should be noted that prompt tuning as\ndescribed in original paper [22] showed very poor utility when trained with differential privacy. We\nobserved that even in the best runs LLM quality metrics (perplexity, next token prediction accuracy)\nfluctuated significantly. No amount of typical hyperparameter tuning could improve prompt tuning\nutility in DP-regime.\nBorrowing some ideas from [27] and experimenting with various optimizers and ways to initialize\nprompt tensor proved to be the key to making prompt-tuning work. Eventually we found out that the\nmain culprit of poor utility was prompt tensor initialization. [22] initializes prompt tensor by using\nembeddings of some real tokens from vocabulary. Changing prompt tensor initialization to random\nuniform with small range [\u22120.01, 0.01] significantly improved utility. Additionally we observed that\nchange of optimizer from Adafactor to Adam or Momentum helped to make training more stable,\nwhich simplified hyperparameter tuning (Appendix E).\nLoRA tuning [18] (Low-rank Adaptation) is a technique that freezes all the pre-trained model\nweights and introduces trainable low-rank decomposition matrices into each dense layer (MLP and\nAttention). This results in fewer trainable weights than full fine tuning but the number of trainable\nweights in LoRA is significantly larger than in Prompt tuning. For example, rank 8 LoRA updates\n20M trainable parameters, as opposed to 41K prompt tuning vs 8B full fine tuning. Empirically we\nfind (Section 5) that LoRA results in superior performance, surpassing that of both full finetuning\nand Prompt finetuning and that tuning both MLP layers and Attention blocks is preferred, see\nAppendices F and J.5 for more details.\nAs a conclusion, we advocate for the use of parameter-efficient techniques when performing DP-\ntraining, with LoRA being the most promising so far.\n1Original paper [37] only describes bidirectional attention over prefix and omits the description of loss\nweights. Nevertheless zero weighting of the prefix is implemented in the T5 code.\n5\n4.3\nData sampling\nTo generate one synthetic example we first randomly select example label y, create a prefix p =\n\"[TaskName] [LabelNamey] \" (Section 4.1), feed prefix p as an input to the language model, and\nautoregressively sample the output. We repeat this process many times until we reach the desired\namount of synthetic data. For each task we sampled at least the same amount of synthetic data as in\noriginal training dataset. We observed that generating more synthetic examples generally improves\ndownstream task performance, but this benefit eventually diminishes and compute is typically the\nlimiting factor (Appendix G).\n5\nExperiments\nGenerative LLM\nIn our experiments we used a model with architecture similar to Lamda 8B [43],\nwhich we pre-trained on The Pile dataset [15] using a standard next-token prediction loss. We stress\nthat for our experimental results to be valid we must ensure that the pre-trained model was not itself\ntrained on data that is considered private for the downstream task. For example, the GPT-2 model\nused in [26] seemingly contained IMDB data in its pre-training dataset [35], but this model was\nsubsequently used to generate a synthetic version of IMDB, see also appendix D for details. To\nprevent privacy leakage we modified the pre-training dataset by de-duplicating it against all sensitive\ndatasets used in downstream tasks, following the recipe and scripts from [21]. The outline of the\nde-duplication approach is as follows. First we tokenized and constructed a suffix array for each\ninvolved dataset (The Pile, IMDB, Yelp, AGNews). Then we used the suffix arrays to find common\nsequences of 50 or more tokens which appear in The Pile and any other dataset. Finally we cut all\nthose common sequences from The Pile dataset. Note that this de-duplication is \u201cstronger\u201d than\nsimply removing the datasets from the Pile. After cutting the sequences we de-tokenized the dataset\nback to strings and used it for pre-training. Refer to Appendix C for additional details.\nDatasets and classification problems\nWe conducted our experiments on IMDB [25], Yelp [53]\nand AGNews [54] datasets. All these datasets only provide a training and test set, so in each case\nwe use the first 90% of the training set for training and the remaining 10% for validation. For each\ndataset we formulated a binary classification problem (sentiment classification) as the downstream\nprediction task.\n5.1\nDownstream classifier performance\nWe investigate the utility of using private synthetic data for a downstream task. For each dataset, we\nconsider two types of models. First one is a (encoder-only) BERT model [11] with classification head.\nBERT is publicly pretrained and then fine tuned using either real data or our generated synthetic\ndata. This model benefits from public pre-training data. We also consider a word-level CNN model\n[19] that does not utilize any public data. For each model, we report the performance on real data\nwith no DP guarantees (an entry \"Real\" with \u03f5 = \u221e in Table 1). This serves as a upper bound of\ndownstream classifier performance. We also report the performance of doing DP-Training on the\ndownstream classifier directly (entries \"Real\" with \u03f5 \u2208 (1, 3, 10), referred to as \"DP-on-real\" in the\ntext) and report the results on synthetic data generated from fine-tuned (Fine-tuned-SD), prompt tuned\n(Prompt-tuned-SD) and LoRA-Tuned (LoRA-tuned-SD) models. We would like to highlight however\nthat in the case of using the real data directly for DP-Training, only the resulting downstream model\nis DP, and the real data can\u2019t be shared freely or used for hyperparameter tuning (or such tuning\nshould be accounted for in privacy guarantees). DP Synthetic data however can be shared freely and\nused for feature engineering, hyperparameter tuning, etc.\nNon-private synthetic data\nFirstly, our results in Table 1 indicate that obtaining good fidelity\nnon-private synthetic data is possible, contrary to the results reported in [52] and Putta et al. [34].\nBoth Fine-tuned-SD and LoRA-tuned SD exhibits better performance than Prompt-tuned-SD, in line\nwith current understanding that for a non-DP setting, tuning more model parameters is beneficial\n[40, 7, 55]. Interestingly, even for non DP setting, downstream models trained on LoRA synthetic\ndata outperform those trained on fully fine-tuned synthetic data in 2 out of 3 datasets.\n6\nPrivate synthetic data\nWhile there is a clear utility drop when going from non-private SD data to\nprivate SD, DP LoRA-tuned-SD is a clearly superior way of obtaining DP synthetic data. Prompt-\ntuned DP SD is better than fully fine tuned DP SD, however LoRA outperforms the Prompt-tuned DP\nsynthetic data in majority of the cases. We hypothesize that it might be due to less total noise being\nadded in DP LoRA models, due to fewer parameters being updated than with the full fine-tuning.\nPrompt tuning on the other hand updates the minimal number of parameters, however this minimum\nupdate hurts the utility of SD, suggesting that like with everything in ML, there is a \u201csweet spot\u201d on\nthe number of parameters trained with DP.\nThe difference between the performance is significant, with LoRA-tuned-SD exhibiting of up to\n10-11% lift on downstream BERT classifier tasks, compared to model trained on fine-tuned-SD. For\nCNN model that is more dependent on the quality of the data than BERT (that essentially reaps some\nadditional benefits from Transfer Learning), the results are even more significant, with a boost from\nprompt-tuned-SD (vs fine-tuned-SD) reaching up to 22%.\nPrivate synthetic data vs DP-Training on real data\nTo obtain a DP downstream ML model, we\ncan either use DP synthetic training data or introduce DP directly during downstream model training\n(DP-on-real). As previously mentioned, the former is a harder setup. When comparing BERT models,\nwe can see that private LoRA-tuned-SD achieves performance similar or even superior (e.g., for\nIMDB and Yelp datasets) to DP-on-real for all levels of privacy, but an additional benefit of such\nsynthetic data is that it can be additionally shared freely and used for hyperperameter tuning and\nfeature engineering. For CNN model, LoRA-tuned-SD (and even prompt-tuned SD) exhibits better\nperformance than DP-on-real. This is due to the fact that private synthetic data benefits from massive\namount of public data that was used for pretraining of the LLM (CNN model itself is trained from\nscratch, as opposed to BERT that is itself a pre-trained model, albeit with smaller amount of public\ndata than the 8b Lamda model we used for SD generation). This indicates that for simpler models\nsynthetic data can be a preferred way of injecting additional public knowledge. This is an interesting\nresult since it is commonly assumed that for Transfer Learning to work, public data should come\nfrom a similar distribution as the target data. However in case of synthetic data, we inject public data\nfrom different distributions (crawl of the web) than that of the downstream task (e.g. Yelp reviews).\nTable 1: Accuracy of BERT and CNN downstream classifiers on IMDB, Yelp and AGNews datasets.\n\u03f5\nBERT\nCNN\nReal\nSynthetic\nReal\nSynthetic\nFinetune\nPrompt-tune\nLoRA\nFinetune\nPrompt-tune\nLoRA\nIMDB\n\u221e\n93.7 \u00b1 0.1\n93.2 \u00b1 0.2\n92.0 \u00b1 0.1\n91.6 \u00b1 0.2\n90.1 \u00b1 0.1\n89.8 \u00b1 0.1\n87.4 \u00b1 0.1\n89.0 \u00b1 0.1\n10\n90.6 \u00b1 0.1\n84.0 \u00b1 0.7\n90.7 \u00b1 0.2\n91.3 \u00b1 0.2\n78.2 \u00b1 0.4\n80.0 \u00b1 0.5\n86.9 \u00b1 0.1\n87.7 \u00b1 0.2\n3\n89.7 \u00b1 0.2\n83.9 \u00b1 0.6\n87.4 \u00b1 0.2\n90.6 \u00b1 0.2\n74.8 \u00b1 0.6\n74.2 \u00b1 0.1\n85.4 \u00b1 0.5\n87.4 \u00b1 0.3\n1\n88.6 \u00b1 0.1\n79.1 \u00b1 1.7\n88.1 \u00b1 0.4\n90.0 \u00b1 0.3\n69.3 \u00b1 0.6\n64.7 \u00b1 0.5\n85.4 \u00b1 0.1\n87.6 \u00b1 0.4\nYelp\n\u221e\n97.6 \u00b1 0.1\n95.9 \u00b1 0.1\n93.9 \u00b1 0.1\n96.4 \u00b1 0.1\n95.6 \u00b1 0.1\n89.3 \u00b1 0.3\n91.6 \u00b1 0.1\n93.7 \u00b1 0.0\n10\n94.9 \u00b1 0.1\n84.2 \u00b1 0.3\n94.1 \u00b1 0.1\n95.9 \u00b1 0.1\n91.8 \u00b1 0.1\n71.6 \u00b1 1.5\n91.0 \u00b1 0.4\n93.9 \u00b1 0.1\n3\n94.6 \u00b1 0.1\n84.6 \u00b1 0.1\n93.5 \u00b1 0.1\n95.6 \u00b1 0.1\n90.9 \u00b1 0.2\n67.9 \u00b1 2.6\n90.5 \u00b1 0.1\n93.6 \u00b1 0.1\n1\n94.3 \u00b1 0.1\n84.1 \u00b1 0.3\n94.1 \u00b1 0.1\n95.5 \u00b1 0.1\n89.8 \u00b1 0.1\n71.1 \u00b1 0.4\n91.1 \u00b1 0.3\n93.4 \u00b1 0.1\nAGNews\n\u221e\n93.7 \u00b1 0.1\n91.1 \u00b1 0.1\n88.3 \u00b1 0.3\n91.8 \u00b1 0.2\n91.3 \u00b1 0.1\n87.7 \u00b1 0.1\n84.7 \u00b1 0.1\n88.5 \u00b1 0.2\n10\n90.9 \u00b1 0.2\n65.1 \u00b1 5.3\n86.9 \u00b1 0.1\n90.0 \u00b1 0.1\n85.2 \u00b1 0.2\n45.2 \u00b1 1.3\n83.5 \u00b1 0.2\n86.9 \u00b1 0.1\n3\n90.5 \u00b1 0.1\n65.3 \u00b1 2.7\n86.2 \u00b1 0.2\n89.6 \u00b1 0.1\n83.2 \u00b1 0.1\n45.8 \u00b1 2.1\n83.2 \u00b1 0.1\n86.3 \u00b1 0.2\n1\n89.8 \u00b1 0.2\n65.7 \u00b1 2.9\n83.9 \u00b1 0.8\n89.4 \u00b1 0.1\n79.9 \u00b1 0.2\n46.8 \u00b1 1.5\n80.4 \u00b1 0.6\n85.8 \u00b1 0.1\nAmount of synthetic data vs downstream classifier performance\nWe studied how much synthetic\ndata we should generate relative to amount of real data. Table 2 demonstrates that generating more\nsynthetic data can be beneficial, but has diminishing returns for BERT (0.8% lift going from 1x to 3x\ntimes the data), with benefits more pronounced for simple models like WordCNN (1.4% lift from\nincreasing the amount of synthetic data 3x).\nTable 2: Amount of synthetic data vs downstream classifier performance for IMDB prompt-tuning, \u03f5 = 1\nModel\n1x\n2x\n3x\n4x\n5x\n6x\nBERT\n87.2 \u00b1 0.4\n87.9 \u00b1 0.4\n88.0 \u00b1 0.1\n88.1 \u00b1 0.4\n88.4 \u00b1 0.1\n88.7 \u00b1 0.1\nWordCNN\n83.2 \u00b1 0.2\n84.3 \u00b1 0.4\n84.6 \u00b1 0.1\n85.4 \u00b1 0.1\n85.7 \u00b1 0.3\n85.8 \u00b1 0.2\nOne can also potentially combine the synthetic data with training with DP on real data, by pre-training\nthe downstream model with DP synthetic data and then fine-tuning with DP on real data. This will\nhowever require spreading the privacy budget between DP synthetic data and DP-Training of the\ndownstream classifier. We leave this for future work.\n7\nTable 3: Ranking correlations (full list) and rank-biased overlap (RBO) [48] for top 25% of hyperparameter\ntuning trials. Real data metrics are calculated on the performance of a model as reported on real validation and\nreal test. For synthetic data, metrics are calculated on synthetic validation and real test data. Mean 25% and STD\n25% show mean and std of real test accuracy evaluated on top 25% trials (ordered by validation accuracy on\nsynthetic data).\nModel\n\u03f5\nMethod\nRBO 25%\nSpearman\nKendall\nMean 25%\nSTD 25%\nBERT\n\u221e\nReal data\n0.56\n0.96\n0.93\n93.55\n0.50\n1\nFine-tuning\n0.33\n0.94\n0.86\n79.27\n0.75\n3\n0.59\n0.93\n0.85\n84.09\n0.29\n10\n0.33\n0.83\n0.71\n84.91\n0.67\n1\nPrompt-tuning\n0.32\n0.73\n0.60\n88.00\n0.00\n3\n0.22\n0.68\n0.54\n87.18\n0.39\n10\n0.31\n0.75\n0.61\n90.27\n0.45\n1\nLoRA-tuning\n0.29\n0.86\n0.79\n90.00\n0.00\n3\n0.23\n0.7\n0.56\n90.5\n0.5\n10\n0.3\n0.78\n0.66\n91.18\n0.39\nWordCNN\n\u221e\nReal data\n0.92\n0.92\n0.84\n90.00\n0.00\n1\nFine-tuning\n0.80\n0.87\n0.77\n64.00\n2.59\n3\n0.63\n0.79\n0.65\n72.09\n3.87\n10\n0.40\n0.76\n0.62\n78.45\n2.46\n1\nPrompt-tuning\n0.64\n0.73\n0.59\n84.36\n1.49\n3\n0.73\n0.77\n0.63\n84.82\n1.40\n10\n0.76\n0.80\n0.69\n86.27\n1.05\n1\nLoRA-tuning\n0.92\n0.87\n0.78\n88.00\n0.00\n3\n0.69\n0.81\n0.67\n87.45\n0.66\n10\n0.78\n0.84\n0.75\n87.82\n0.57\nComparison with prior work\nWhile works below don\u2019t provide sufficient (or any) information\non their privacy unit (as we do in Appendix A), we assume that privacy unit that was used is one\nexample (e.g. 1 full yelp or imdb review etc); we also assume central DP setting, that \u03b4 values are\nthe same or comparable etc. Additionally, none of the works below take into account the fact that\npre-training data might have contained the data they deem private (as we highlight in Appendix D),\npotentially invalidating their reported DP guarantees.\nYue et al. [52] used Yelp dataset for multiclass (rating) classification, so our results are not directly\ncomparable. Putta et al. [34] used AGNews dataset. Their approach is a combination of next token\nprediction (similar to our setup) and additional loss term from a new head that attempts to learn\nto distinguish between various classes directly (instead of simply relying on the prompts in the\nnext token prediciton head). Putta et al. [34] reports 0.867 accuracy of downstream task for \u03f5 of\n3, while we obtain 89.6 (the baseline performance of downstream classifier for our and their work\nis comparable, 0.938, suggesting that we are using comparable downstream classifiers). Mattern\net al. [26] suggested a modification of the loss (prompt-mismatch loss, to discourage the generation\nof text inconsistent with the prompt, like generating a negative review when positive prompt was\ngiven). They performed experiments on IMDB dataset. Their best IMDB experiments reporting\nworse accuracy on DP synthetic data (89.1% theirs vs 90.6% ours for \u03f5 = 3). They also seem to have\nworse performance on real data despite using the same model (BERT-classifier).\n5.2\nTuning downstream model hyperparameters on synthetic data\nWith the following experiments on IMDB data, we want to demonstrate that private synthetic data\nis useful for hyperparameter tuning of the downstream classifier. For all our experiments, when\ntuning the downstream classifier, we use validation accuracy on set-aside portion of synthetic data for\nhyperparameter selection. We tune weight decay and learning rate for both CNN and BERT models.\nFor synthetic data, we create vectors of accuracy on validation (synthetic) data and performance\non real test data for all possible combinations of hyperparameter values tried. We then report the\nranking correlation between performance as indicated by validation accuracy (synthetic data) and\ntest accuracy computed on real data. We also report the ranking correlation of accuracies on real\nvalidation and real test data, to provide an upper bound. Additionally, we report rank-biased overlap\nranking metric [48], which is a weighted metric that gives more weight to the top of the ranking (we\nuse parameters that give 85% of the weight to the first top 25% of ranking).\nTable 3 demonstrates excellent ranking correlation on synthetic data. Interestingly, prompt-tuned\nsynthetic data metrics, in particular the mean and standard deviation of the top 25% of trials, suggest\nthat BERT classifier performance is less sensitive to hyperparameters on better fidelity (prompt or\nLoRA tuning) data than on worse fidelity data (fine-tuning).\n8\n5.3\nEstimating synthetic data quality\nIt is useful to have an efficient method of evaluating the quality of a synthetic dataset without relying\non specific downstream tasks. For one, a key use case for privacy preserving synthetic data is to enable\ndata sharing without a definitive end use case. For another, training the generator LLM has multiple\nhyperparameters that can be tuned, and it can be prohibitive to evaluate candidate models using full\ndata synthesis and downstream training (which itself might require tuning hyperparameters). Instead,\nlighter weight proxy metrics can be used. Commonly used proxy metrics are: perplexity, n-gram\nstatistics, and MAUVE [31]. We investigate the effectiveness of each of these metrics by comparing\ntheir correlation to downstream performance (Table 4). These metrics are used to compare datasets,\nand thus their absolute value is uninformative. For n-gram statistics we determine the frequency\nof unigrams, bigrams, and sample lengths in characters for both the original and synthetic datasets.\nWe then compute the area under the divergence frontier between these two frequency distributions\nas is done by MAUVE. MAUVE works by computing the difference between two datasets by first\nembedding each example, then clustering the datasets, followed by comparing (via divergence\nfrontiers) the histogram of cluster membership across the two datasets. It has recently been shown\nto be an effective metric for synthetic text datasets [52] [26] [20], which our results support. We\ncompute the MAUVE score as given in Pillutla et al. [31] using the suggested hyperparameters unless\nnoted. We investigated modifying these hyperparameters and confirm they make little difference\nto the relative ranking, with the notable exception of the model used to embed examples. Unlike\nthe original paper, we find larger models to be much more effective. In particular, embedding using\nSentence-T5 [29] has much higher correlation to downstream performance than BERT or any other\nmodel we tried. For more details see appendix K. Our results match many of the results given in\nKour et al. [20]. All metrics are at least somewhat noisy with standard test-set perplexity performing\nvery well. Given its ease to compute while finetuning, perplexity is our recommended proxy metric\nwhen available.\nTable 4: The Spearman\u2019s rank correlation for each metric compared against downstream classifier performance.\nMetrics are used to select candidate datasets, and thus their relative rank is whats most important for the metrics\nto reflect.\nPerplexity\nUnigram\nBigram\nLength\nMAUVE\nBERT\nSt5-base\nSt5-3B\n0.91 \u00b1 0.02\n0.74 \u00b1 0.11\n0.83 \u00b1 0.09\n0.88 \u00b1 0.26\n0.84 \u00b1 0.62\n0.88 \u00b1 0.04\n0.93 \u00b1 0.10\n6\nConclusion\nWe have shown that training downstream models on DP synthetic training data is an effective\nalternative to training such models with DP directly on real data for text classification tasks. We\nexplored two methods for privately generating the synthetic training data, both of which involve\nmodifying the weights of an existing LLM. One method privately fine-tuned all the layers of the LLM,\nwhile the other method used parameter efficient fine tuning ( \u2018prompt-tuning\u2019 and \u2018LoRA-tuning\u2018).\nOur experiments demonstrated that LoRA tuning is a superior way of obtaining DP-synthetic data,\nwhich provides performance on the downstream task that is comparable or even better than directly\nDP-Training on real data. We showed that the standard NLP Prefix-LM loss is well suited for DP-\nfinetuning. Private synthetic data can be used freely for all the purproses, such as feature engineering,\nhyperparameter tuning, debugging and monitoring, and sharing, but without any privacy-related\nconcerns. We also showed that while Mauve is a good proxy metric for evaluating the quality of the\nsynthetic data, simpler metrics like perplexity, when available, perform well.\n7\nEthics statement\nWe expect that our proposed method of generating DP synthetic data will facilitate safer data sharing\nand that the societal impact will be positive, since entities who own private data but do not necessarily\nhave the knowledge or resources to train predictive models can share private synthetic data with\nspecialists for model creation, benefiting from their expertise without comprising the privacy of the\nusers who contributed their data. The main limitation of our approach is that we only conducted\nexperiments on English datasets, however we expect that methods should work on multilingual\ndatasets as along as public multi-lingual data are available for LLM pre-training.\n9\n8\nReproducibility Statement\nAll of our experiments are based on open sourced frameworks and public datasets, refer to Appen-\ndices H and M. We further provided necessary details to reproduce our experiments in Appendices C,\nE, F, G and I.\nReferences\n[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,\nand Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC\nconference on computer and communications security. ACM, oct 2016. doi: 10.1145/2976749.\n2978318. URL https://doi.org/10.1145%2F2976749.2978318.\n[2] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Effi-\ncient algorithms and tight error bounds. Proceedings - Annual IEEE Symposium on Foundations\nof Computer Science, FOCS, pages 464\u2013473, 12 2014. doi: 10.1109/FOCS.2014.56.\n[3] Raef Bassily, Kobbi Nissim, Uri Stemmer, and Abhradeep Guha Thakurta. Practical locally\nprivate heavy hitters. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-\nume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2017/file/3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf.\n[4] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth,\nShivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-\nNeoX-20B: An open-source autoregressive language model.\nIn Proceedings of the ACL\nWorkshop on Challenges & Perspectives in Creating Large Language Models, 2022. URL\nhttps://arxiv.org/abs/2204.06745.\n[5] Avrim Blum, Katrina Ligett, and Aaron Roth. A learning theory approach to non-interactive\ndatabase privacy. CoRR, abs/1109.2229, 2011. URL http://arxiv.org/abs/1109.2229.\n[6] Rishi Bommasani, Steven Wu, and Xanda Schofield. Towards private synthetic text generation.\nIn NeurIPS 2019 Machine Learning with Guarantees Workshop, 2019.\n[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR,\nabs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.\n[8] Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of ap-\nproximate differential privacy. In Proceedings of the Forty-Sixth Annual ACM Symposium\non Theory of Computing, STOC \u201914, page 1\u201310, New York, NY, USA, 2014. Association\nfor Computing Machinery. ISBN 9781450327107. doi: 10.1145/2591796.2591877. URL\nhttps://doi.org/10.1145/2591796.2591877.\n[9] Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:\nEvaluating and testing unintended memorization in neural networks. In 28th USENIX Security\nSymposium (USENIX Security 19), pages 267\u2013284, 2019.\n[10] Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather-\nine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin\nRaffel. Extracting training data from large language models. In 30th USENIX Security Sym-\nposium (USENIX Security 21), pages 2633\u20132650. USENIX Association, August 2021. ISBN\n978-1-939133-24-3. URL https://www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\n10\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL\nhttp://arxiv.org/abs/1810.04805.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[13] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor.\nOur data, ourselves: Privacy via distributed noise generation. In Advances in Cryptology\u2013\nEUROCRYPT, pages 486\u2013503, 2006.\n[14] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to\nsensitivity in private data analysis. In Proc. of the Third Conf. on Theory of Cryptography\n(TCC), pages 265\u2013284, 2006. URL http://dx.doi.org/10.1007/11681878_14.\n[15] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:\nAn 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027,\n2020.\n[16] Jiaqi Guan, Runzhe Li, Sheng Yu, and Xuegong Zhang. Generation of synthetic electronic\nmedical record text. In 2018 IEEE International Conference on Bioinformatics and Biomedicine\n(BIBM), pages 374\u2013380, 2018. doi: 10.1109/BIBM.2018.8621223.\n[17] Moritz Hardt, Katrina Ligett, and Frank McSherry. A simple and practical algorithm for\ndifferentially private data release. CoRR, abs/1012.4763, 2010. URL http://arxiv.org/\nabs/1012.4763.\n[18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685,\n2021. URL https://arxiv.org/abs/2106.09685.\n[19] Rie Johnson and Tong Zhang. Semi-supervised convolutional neural networks for text catego-\nrization via region embedding. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett,\neditors, Advances in Neural Information Processing Systems, volume 28. Curran Associates,\nInc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/\nacc3e0404646c57502b480dc052c4fe1-Paper.pdf.\n[20] George Kour, Samuel Ackerman, Orna Raz, Eitan Farchi, Boaz Carmeli, and Ateret Anaby-\nTavor. Measuring the measuring tools: An automatic evaluation of semantic metrics for text\ncorpora. arXiv preprint arXiv:2211.16259, 2022.\n[21] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\nCallison-Burch, and Nicholas Carlini.\nDeduplicating training data makes language mod-\nels better. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, 2022.\n[22] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\nprompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243.\nURL https://aclanthology.org/2021.emnlp-main.243.\n[23] Xuechen Li, Florian Tram\u00e8r, Percy Liang, and Tatsunori Hashimoto. Large language models\ncan be strong differentially private learners. CoRR, abs/2110.05679, 2021. URL https:\n//arxiv.org/abs/2110.05679.\n[24] Terrance Liu, Giuseppe Vietri, and Zhiwei Steven Wu. Iterative methods for private synthetic\ndata: Unifying framework and new methods. CoRR, abs/2106.07153, 2021. URL https:\n//arxiv.org/abs/2106.07153.\n11\n[25] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis, 2011. URL https://www.tensorflow.\norg/datasets/catalog/imdb_reviews.\n[26] Justus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard Schoelkopf, and Mrinmaya\nSachan.\nDifferentially private language models for secure data sharing.\narXiv preprint\narXiv:2210.13918, 2022.\n[27] Harsh Mehta, Abhradeep Thakurta, Alexey Kurakin, and Ashok Cutkosky. Large scale transfer\nlearning for differentially private image classification, 2022.\n[28] Oren Melamud and Chaitanya Shivade. Towards automatic generation of shareable synthetic\nclinical notes using neural language models. CoRR, abs/1905.07002, 2019. URL http:\n//arxiv.org/abs/1905.07002.\n[29] Jianmo Ni, Gustavo Hern\u00e1ndez \u00c1brego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and\nYinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models.\narXiv preprint arXiv:2108.08877, 2021.\n[30] Joshua Peterson, Stephan Meylan, and David Bourgin. Openwebtext dataset, 2019. URL\nhttps://github.com/jcpeterson/openwebtext#openwebtext.\n[31] Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin\nChoi, and Zaid Harchaoui. Mauve: Measuring the gap between neural text and human text\nusing divergence frontiers. Advances in Neural Information Processing Systems, 34:4816\u20134828,\n2021.\n[32] Natalia Ponomareva, Jasmijn Bastings, and Sergei Vassilvitskii. Training text-to-text trans-\nformers with privacy guarantees. In Findings of the Association for Computational Linguistics:\nACL 2022, pages 2182\u20132193, Dublin, Ireland, May 2022. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2022.findings-acl.171. URL https://aclanthology.org/2022.\nfindings-acl.171.\n[33] Natalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H. Brendan\nMcMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Thakurta. How to dp-fy ml: A\npractical guide to machine learning with differential privacy, 2023.\n[34] Pranav Putta, Ander Steele, and Joseph W Ferrara. Differentially private conditional text\ngeneration for synthetic data production, 2023. URL https://openreview.net/forum?id=\nLUql3ZOFwFD.\n[35] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Gpt-\n2 model card, \"language models are unsupervised multitask learners\", 2019. URL https:\n//github.com/openai/gpt-2/blob/master/model_card.md.\n[36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.\n10683.\n[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL\nhttp://jmlr.org/papers/v21/20-074.html.\n[38] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor\nLewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini\nSoares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis\nBulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan\nLee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten\nBosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan\nSaeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling\n12\nup models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. URL\nhttps://arxiv.org/abs/2203.17189.\n[39] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. CoRR, abs/1804.04235, 2018. URL http://arxiv.org/abs/1804.04235.\n[40] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Auto-\nPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 4222\u20134235, Online, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020.\nemnlp-main.346.\n[41] Yuchao Tao, Ryan McKenna, Michael Hay, Ashwin Machanavajjhala, and Gerome Mik-\nlau.\nBenchmarking differentially private synthetic data generation algorithms.\nCoRR,\nabs/2112.09238, 2021. URL https://arxiv.org/abs/2112.09238.\n[42] Apple Differential Privacy Team.\nLearning with privacy at scale.\nIn Available from\nhttps://machinelearning.apple.com/research/learning-with-privacy-at-scale, 2017.\n[43] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,\nHeng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee,\nHuaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun,\nDmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,\nKathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos,\nToju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben\nHutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi\nRajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen,\nRachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi,\nand Quoc Le. Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022.\nURL https://arxiv.org/abs/2201.08239.\n[44] Amirsina Torfi, Edward A. Fox, and Chandan K. Reddy. Differentially private synthetic\nmedical data generation using convolutional gans. CoRR, abs/2012.11774, 2020. URL https:\n//arxiv.org/abs/2012.11774.\n[45] Laurens van der Maaten and Awni Y. Hannun. The trade-offs of private prediction. CoRR,\nabs/2007.05089, 2020. URL https://arxiv.org/abs/2007.05089.\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141 ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn I. Guyon,\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran\nAssociates, Inc., 2017.\nURL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[47] T. Wang, J. Blocki, N. Li, and S. Jha. Locally differentially private protocols for frequency\nestimation. In Proceedings of the 26th USENIX Security Symposium, 2017.\n[48] William Webber, Alistair Moffat, and Justin Zobel. A similarity measure for indefinite rankings.\nACM Trans. Inf. Syst., 28(4), nov 2010. ISSN 1046-8188. doi: 10.1145/1852102.1852106. URL\nhttps://doi.org/10.1145/1852102.1852106.\n[49] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private\ngenerative adversarial network. CoRR, abs/1802.06739, 2018. URL http://arxiv.org/\nabs/1802.06739.\n[50] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. PATE-GAN: Generating synthetic\ndata with differential privacy guarantees. In International Conference on Learning Representa-\ntions, 2019. URL https://openreview.net/forum?id=S1zk9iRqF7.\n13\n[51] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath,\nJanardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and\nHuishuai Zhang. Differentially private fine-tuning of language models. CoRR, abs/2110.06500,\n2021. URL https://arxiv.org/abs/2110.06500.\n[52] Xiang Yue, Huseyin A. Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Huan Sun, David\nLevitan, and Robert Sim. Synthetic text generation with differential privacy: A simple and\npractical recipe, 2022. URL https://arxiv.org/abs/2210.14348.\n[53] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional Networks for Text\nClassification . arXiv:1509.01626 [cs], September 2015. URL https://www.tensorflow.\norg/datasets/catalog/yelp_polarity_reviews.\n[54] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks\nfor text classification. CoRR, abs/1509.01626, 2015. URL http://arxiv.org/abs/1509.\n01626.\n[55] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [MASK]: Learning vs.\nlearning to recall. In Proceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pages 5017\u2013\n5033, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nnaacl-main.398. URL https://aclanthology.org/2021.naacl-main.398.\n14\nSUPPLEMENTARY MATERIAL\nA\nOur privacy guarantees\nTo provide all the information needed to understand our privacy guarantees, we follow the guidelines\noutlined in [33].\n1. DP setting. We provide central DP guarantee where the service provider is trusted to\ncorrectly implement the mechanism.\n2. Instantiating the DP Definition\n(a) Data accesses covered: Our DP guarantees apply only to a single training run. We\ndon\u2019t account for hyperparameter tuning in our guarantees.\n(b) Final mechanism output: We use DP-Training methods. Only model\u2019s predictions\n(e.g. synthetic data generated by the DP-Trained model) is released, however the\nmechanism\u2019s output is technically the full sequence of privatized gradients, and the\nguarantee also applies at this level. This also means that all the checkpoints are\nprotected/can be released publicly.\n(c) Unit of privacy. We consider example-level DP, where each example is a long sequence\nof text, e.g. a full Yelp review. Maximum length of such unit in tokens is 512, tokens are\nextracted by SentencePiece algorithm trained on c4. We consider full data protection\n(full text of a review).\n(d) Adjacency definition for \u201cneighboring\u201d datasets\": We use add-or-remove adjacency\ndefinition.\n3. Privacy accounting details\n(a) Type of accounting used: RDP-based accounting.\n(b) Accounting assumptions : Poisson sampling was assumed for privacy amplification but\nshuffling was used in training)\n(c) The formal DP statement: We use various levels of \u03f5 values: 1,3,10. Our \u03b4 =\n1\ntraining_data_size\n(d) Transparency and verifiability: We are going to open source our code. We use open-\nsourced t5x framework.\nA.1\nPreserving example-level privacy\nLet D be a dataset of examples, where each example is a sequence of tokens (such as a full Yelp\nor IMDB review) provided by a user. We train LLMs to generate synthetic data using a next-token\nprediction objective. Therefore, when training an LLM, each example is used to generate several sub-\nexamples, one per token in the example. Concretely, consider an example in the dataset consisting of\nthe sequence of tokens t = (t1, . . . , tn). The i-th sub-example generated from this example consists\nof the feature vector xi = (t1, . . . , ti\u22121) and the label yi = ti. In other words, for each sub-example,\nthe preceding tokens are used to predict the next token. Let w be the trainable parameters of the\nmodel. The loss of example t is defined\n\u00af\u2113(w, t) =\nn\nX\ni=1\n\u03b1i\u2113(w, xi, yi)\nwhere \u2113(w, x, y) is the loss of a sub-example and \u03b1i \u2265 0 is the weight of sub-example i.\nWe consider several weighting schemes. Standard normalization sets \u03b1i = 1\nn for each sub-example\ni, while Prefix-LM normalization, first proposed by Raffel et al. [37], sets \u03b1i = 0 for all i \u2264 k and\n1\nn\u2212k for all i > k whenever the first k tokens of an example correspond to the prefix that specifies\nthe learning task.\nWe used DP-SGD to minimize the sum of the losses \u00af\u2113(w, t) over all examples t. Critically, no\nmatter which weighting scheme we used, we clipped the total gradient norm P\ni \u03b1i \u2225\u2207\u2113(w, xi, yi)\u2225\nof each example to be at most a constant C, and then applied an amount of noise proportional to C\n15\nthat ensured our desired privacy guarantee. Specifically, if D and D\u2032 are datasets that differ on one\nexample, and A is the DP-SGD algorithm then the noise ensured that\nP[A(D) \u2208 S] \u2264 exp(\u03f5) \u00d7 P[A(D\u2032) \u2208 S] + \u03b4\nfor our desired values of \u03f5 \u2265 0 and \u03b4 \u2208 [0, 1]. Therefore, our privacy guarantees apply to any change\nto any single example, and not just changes to a single sub-example.\nB\nAdditional related work\nWhile in our paper we concentrate on generating synthetic text data, the task of generating synthetic\ntabular data has been explored extensively before.\nSynthetic Tabular data.\nFor tabular data, early works on synthetic data concentrated on estimating\nthe utility of the synthetic data as a quality of the statistical answers over the data (synthetic data for\nquery release). Privacy-preserving aspect for such synthetic data was often achieved via DP methods\n[5, 17, 24] More recently, works that instead evaluated how useful the tabular synthetic data is for\nsome downstream ML model started gaining popularity [41]. In general, the approaches for generating\nsynthetic tabular data can be categorized into Marginal-based and generative models-based. Marginal-\nbased models calculate private marginal distribution by taking marginal distribution over attributes\nand appropriately privatizing it with some DP-mechanism like Gaussian or Laplace. Then attributes\nfor synthetic instances are sampled from these distributions. Generative models instead attempt to\nbuild a function that approximates the data distribution and sample from this function. GAN-based\nmethods were a common choice for such generative models: e.g., DP-GAN [49], Convolutional GAN\n[44] and PATE-GAN [50]. A recent benchmark [41] reported that for achieving best downstream ML\nmodel performance, marginal-based methods still outperform GAN-based approaches.\nC\nLanguage model pre-training.\nModel architecture. For synthetic data generation, one can use either decoder only or encoder-\ndecoder models. We used a decoder-only transformer model [46], which should be more parameter-\nefficient, with architecture similar to LaMDA [43]. The quality of the pre-trained model is of\nparamount importance for generating good fidelity synthetic data. We initially experimented with\n1B model and found that a significant boost in downstream performance can be achieved by using a\nhigher capacity model.\nTherefore we use 8B model for final fine-tuning and prompt tuning. A smaller version (1B) model is\nused to tune hyperparameters like learning rate, clipping norm, batch etc. The hyperparameter values\nfound using 1B model are used for the final 8B model. Statistics for 1B and 8B models are provided\nin the Table 5.\nTable 5: Architecture of transformer models used in experiments.\nParameters\nLayers\nAttn. heads\ndmodel\ndff\ndk, dv\n1B\n8\n32\n2048\n16384\n128\n8B\n16\n64\n4096\n32768\n128\nPre-training data. We used public dataset The Pile [15] as a basis for our pre-training data. However\nwe run an extra post-processing step by deduplicating The Pile against all datasets used in downsteam\ntasks. This deduplication step is necessary to ensure that private data won\u2019t be accidentally learned\nduring model pre-training, which otherwise would invalidate our DP guarantees.\nTo do deduplication we followed the recipe outlined in [21]. Specifically, we tokenized and\nconstructed a suffix array for each involved dataset (The Pile, IMDB, Yelp, AGNews). Then we used\nthese suffix arrays to find common sequences of 50 of more tokens which appear in The Pile and any\nother dataset. Finally we cut all those common sequences from The Pile dataset. After cutting the\nsequences we de-tokenized dataset back to strings and used it for pre-training. It\u2019s important to note\nthat we only deduplicate The Pile against other datasets, we did not run deduplication of The Pile\nagainst itself.\n16\nPre-training procedure. We pre-trained our models using T5X codebase [38], however we adopted\nfew tricks which were used to train open sourced GPT-NeoX model [4]. Details are below.\nWe pre-trained a model for 380k steps using batch size of 1024 example with 1024 tokens sequence\nlength, which result in training for approximately 400B tokens. We used same SentencePiece\ntokenizer which was used in original T5 model [37]. Cross entropy loss on next token prediction\n(teacher-forcing) was used as a training objective. Additionally we employed weight decay of 0.001\nand an auxiliary z-loss 10\u22124 log2(Z) where log(Z) is softmax normalizer. Training was done with\nAdaFactor optimizer with learning rate schedule min(0.01,\n1\n\u221a\nN ) where N is a step counter.\nD\nAnalysis of pre-training data contamination of GPT-2 training data.\nMultiple prior works on private synthetic data generation [52, 34, 26] start with a pre-trained GPT-2\nmodel and then finetune it with differential privacy on some downstream dataset. In this section we\ndemonstrate that pre-training data for GPT-2 model includes examples from downstream tasks which\ninvalidates the privacy guarantees of such prior work.\nLet\u2019s assume we pre-train model M on some public dataset Dp and finetune with DP-SGD on\nsensitive dataset Ds. If Dp \u2229 Ds = \u2205 then we can conclude that the process of obtaining our model\nM is differentially private w.r.t. Ds. However this is no longer the case if intersection of Dp and Ds\nis non-empty.\nGPT-2 was pre-trained on a WebText dataset [35]. While the dataset is not released publicly, authors\ndiscuss in detail the process of how dataset was constructed by scraping the content of links posted\non Reddit website. Additionally, they released a list of top domains 2 from which the dataset was\nformed. Specifically, it could be seen that 183080 web-pages from IMDB and 36188 web-pages from\nYelp websites were included GPT-2 pre-training data. This by itself, indicates that parts of IMDB and\nYelp dataset were include in GPT-2 pre-training data.\nWe performed further analysis in the following way. We took a subset of OpenWebText dataset [30] -\na public re-implementation of WebText. Specifically we used c4/webtextlike from TFDS3. We\ncomputed an intersection of IMDB dataset and c4/webtextlike using the approach from [21]. We\nfound that c4/webtextlike contains 136 distinct examples from IMDB training and test set.\nFinally we manually looked at a few of the examples to verify that they are indeed part of IMDB\ndataset and that they were likely used to construct WebText dataset. Here is a code snippet to obtain\none of these examples:\nimport tensorflow_datasets as tfds\nds = tfds.load(\u2019imdb_reviews/plain_text:1.0.0\u2019,\nsplit=\u2019test\u2019,\nshuffle_files=False)\n# get 52nd example from IMDB test set\nprint(next(iter(ds.skip(51)))[\u2019text\u2019].numpy().decode(\u2019utf-8\u2019))\n# Output:\n# In the eighties, Savage Steve Holland put out three movies ...\n# This example is a second review from here:\n# https://www.imdb.com/title/tt0091680/reviews\n# The link to the IMDB web-page is mentioned\n# in the following reddit post:\n# https://www.reddit.com/r/movies/comments/77gna7/comment/dom5mqa\n# The Reddit post was written in 2017, two years prior to\n# creation of WebText dataset,\n# suggesting that it would be included in the dataset.\nThis indicated that indeed at least some portion of the IMDB reviews would have been obtained\nduring WebText dataset construction process and would have been used for GPT-2 pre-training.\n2https://github.com/openai/gpt-2/blob/master/domains.txt\n3https://www.tensorflow.org/datasets/catalog/c4#c4webtextlike\n17\nWhile it is impossible to say, without an access to the actual pretraining data, what percentage of\nthe downstream tasks data was included for GPT-2, it highlight the importance of our deduplication\nprocess for obtaining rigorous privacy guarantees.\nE\nDetails of prompt-tuning.\nTraining instability with Adafactor optimizer and default prompt initialization. [22] recom-\nmends Adafactor as a default optimizer. They also suggest to initialize prompt using pre-trained\nembeddings of tokens from vocabulary. While we confirm that it works well for non-private prompt\ntuning of LLM, it appeared to be inadequate for DP-prompt tuning.\nWith this setup we observed significant training instabilities even in the best DP-runs after significant\namount of clipping norm and learning rate tuning, see figure 2.\n0\n50\n100\n150\n200\nTraining step\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nLoss\n1e6\n0\n50\n100\n150\n200\nTraining step\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\nAccuracy\n0\n50\n100\n150\n200\nTraining step\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\nPerplexity\nFigure 2: Training loss, training accuracy and validation perplexity for the best prompt tuning training run with\nAdafactor optimizer, \u03f5 = 1.\nChanging optimizer and prompt tuning initialization.\nTo achieve good performance with prompt tuning we experimented with two things.. First of all, we\ntried various optimizers (including Adam and Momentum) instead of Adafactor. Additionally we\nexperimented with random initialization of prompt tensor.\nWhile changing optimizer didn\u2019t really improve the downstream performance, we found that Adam\nand Momentum optimizers lead to more stable training runs. Changing prompt tensor initalization\nto random uniform with small range was the key for improving prompt tuning performance with\nDP. Table 6 demonstrates that random initialization results in a lift of up to 30% in downstream\nCNN performance, and up to 10% for BERT model. We hypothesize that proper initialization is very\nimportant to DP-Training and addition of noise makes it hard for the model to \"recover\" from bad\ninitialization.\nTable 6: Downstream performance of prompt tuning with different optimizers and prompt initialization. In\nall experiments prompt tuning was done for 10 epochs with 1024 batch size and privacy level \u03f5 = 1. Column\n\u201cVocab Init\u201d correspond to default initialization proposed in prompt tuning paper using embeddings of tokens\nfrom vocabulary. Column \u201cRandom Init\u201d correspond to random uniform initialization in range [\u22120.01, 0.01].\nOptimizer\nBERT\nCNN\nVocab Init\nRandom Init\nVocab Init\nRandom Init\nAdafactor\n72.1 \u00b1 2.1\n88.2 \u00b1 0.2\n55.8 \u00b1 0.2\n84.9 \u00b1 0.2\nAdam\n74.6 \u00b1 1.2\n85.5 \u00b1 0.7\n50.2 \u00b1 0.1\n82.4 \u00b1 0.4\nWe did some limited experiments with the scale of random uniform initialization, however for most\nof them we didn\u2019t run full synthetic data pipeline and only compared LLM perplexity and next token\nprediction accuracy. These experiments showed that random uniform from range [\u22120.01, 0.01] was\nthe best, while both decreasing and increasing the range led to worse metrics.\nWe also compared Adam optimizer with fixed learning rate, Adam optimizer with cosine learning rate\ndecay and Momentum optimizer with cosine learning rate decay. As long as learning rate was tuned,\nall three performed similarly. Eventually we settled on Adam optimizer with fixed learning rate.\n18\nF\nDetails of LoRA\nLet\u2019s say one of the layers in the network is represented as dense matrix multiplication operation Wh\nwhere W is a trainable weight and h is an input to the layer (typically embedding or hidden state\nvector). LoRA [18] proposes to replace weight matrix with a sum W + LR, freeze W and tune L and\nR matrices. In this notation L and R are low-rank matrices such that the result of their multiplication\nhas the same shape as W. For example if W is a matrix with the shape n \u00d7 m, then L would be\nn \u00d7 r matrix and R would be r \u00d7 m matrix, where r is the rank of low-rank adapter.\nSimilar to [18], all layers where LoRA is not applied are considered frozen in our setup. We performed\nLoRA tuning using Adam optimizer with fixed learning rate. Unlike [18] we did not use weight\ndecay because we observed no benefits of weight decay in differentially private LoRA training.\nTypically, in a transformer model LoRA can be applied to attention layers and MLP layers. Hu\net al. [18] study LoRA only on attention layers. In this work, we tried to applied LoRA to attention\nlayers only (Attention-LoRA), MLP layers only (MLP-LoRA) and both attention and MLP layers\n(Full-LoRA). Overall we found out that Full-LoRA is the best choice in most cases, however in some\ncases MLP-LoRA could be slightly better.\nWe also studied how rank of LoRA affects downstream performance. Similar to [18] we observed\nthat initially increase of LoRA rank lead to increase of accuracy on downstream task, followed by\neventual decrease of downstream accuracy with further increase of rank.\nOur main results in table 1 are reported using Full-LoRA rank 8 on AGNews, rank 1 for Yelp datasets,\nand MLP-LoRA rank 32 on IMDB dataset.\nOverall we can recommend to use Full-LoRA rank 8 as a good default value, however we can\nencourage tuning of LoRA parameters when resources allow it and the best possible performance is\nneeded.\nSee also detailed ablation of LoRA parameters in Appendix J.5.\nG\nDetails of synthetic data sampling.\nFor data generation/sampling part, LLMs have the following knobs:\n1. temperature: this is a constant by which the logits are divided prior to softmax and sub-\nsequent sampling. Large temperature flatten the tokens distributions, making rarer tokens\nmore likely to be selected; it also increases diversity of the data generated but can negatively\naffect the quality. Our experiments show that default temperature of 1 (so no modification\nof the tokens distribution) works the best.\n2. topk: given a token distribution, topk determines what portion of the distribution to keep\nbefore sampling. Topk is similar to low temperature - e.g. setting topk to top 1000 tokens\nmeans that next token will be chosen from the most likely tokens. We keep this parameter\nunmodified (set to \u221e).\n3. numdecodes: is similar to the number of candidates in standard beam search. We use the\nvalue of 1 here, resulting in the sequence where each token is the most likely to be returned.\nWhile we did experiment with various values of these hyperparameters (see appendix J.3), we find\nthat default settings already provide enough of diversity of generated examples for our datasets. If\nfurther diversity needs to be enforced (e.g. for very large datasets), we recommend increasing the\ntemperature or numdecode values.\nH\nDetails of datasets for downstream tasks\nWe conducted experiments on IMDB reviews [25], Yelp reviews [53] and AGNews4 datasets. On\nIMDB and Yelp we formulated downstream task as binary sentiment classification. On AGNews\nthe downstream task was classification of titles of news articles into one of 4 topics (World, Sports,\nBusiness and Sci/Tech).\n4https://www.tensorflow.org/datasets/catalog/ag_news_subset\n19\nAll of the considered datasets only provided training and test sets. To obtain validation set we split\noriginal training set into two chunks in deterministic way using TFDS split slicing API. We used first\n90% of the original training set for training, and last 10% for validation.\nSome of the dataset statistics for considered dataset and our train/test/validation splits is provided in\ntable 7.\nTable 7: Datasets statistics.\nIMDB\nYelp\nAGNews\nTraining examples\nTotal\n22500\n504000\n108000\nClass 0\n11256\n252085\n26915\nClass 1\n11244\n251915\n26985\nClass 2\n-\n-\n27081\nClass 3\n-\n-\n27019\nValidation examples\nTotal\n2500\n56000\n12000\nClass 0\n1244\n27915\n3085\nClass 1\n1256\n28085\n3015\nClass 2\n-\n-\n2919\nClass 3\n-\n-\n2981\nTest examples\nTotal\n25000\n38000\n7600\nClass 0\n12500\n19000\n1900\nClass 1\n12500\n19000\n1900\nClass 2\n-\n-\n1900\nClass 3\n-\n-\n1900\nI\nArchitecture and training of downstream models.\nWe used two types of models for all downstream experiments. First one is a BERT encoder with a\ndense layer on top of it, second one is a shallow CNN.\nBERT model. For BERT-based classifier we used BERT-Base model [12] pretrained on English data5\nwith standard BERT tokenization and preprocessing. We put a dense layer on top of pooled output of\nthe BERT encoder to produce classification score. Output of dense layer was a single floating point\nnumber per input sequence which was converted to probability using sigmoid function. We trained\nentire model (including BERT encoder and dense layer) without freezing any layers.\nCNN model. In addition to BERT, we used a shallow CNN model without any extra pre-training.\nOur model architecture followed the idea from [19] with the main difference that we didn\u2019t pre-train\nembeddings beforehand. To be more specific, our model works as follows. We convert input sequence\nto lowercase and tokenize it by splitting it on whitespaces and punctuation signs. Then we embed it\ninto 384 dimensional embedding, using vocabulary from most common 30k words from the dataset.\nEmbedding was randomly initialized and trained as a part of downstream training task. Output of\nembedding layer was passed through 1D convolution with kernel size 3, 256 output filters and RELU\nactivation. This followed by global max pooling along the sequence length. Then we have a fully\nconnected layer with 256 outputs and RELU activation, followed by final logits layer with sigmoid\nactivation.\nTraining of downstream model. Both BERT and CNN were trained in a similar way. For non-\nprivate training we used Adam optimizer and DP-Adam (as implemented in the Keras DPModel\nlibrary) for private training. Model was regularized with weight decay and we did early stopping\n(by non-increasing validation accuracy). We choose best hyperparameters by running a sweep over\nlearning rates {10\u22127, . . . , 10\u22121} and weight decays {5 \u00d7 10\u22121, . . . , 5 \u00d7 10\u22126, 0}. Additionally for\nDP-training baseline on real data we did a sweep of clipping norm over {10\u22122, . . . , 102}.\n5https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n20\nJ\nAblations\nWe study the influence of various factors on quality of generated synthetic data. We observed that\nbetter pre-trained model (in terms of model size and choice of pre-training dataset) leads to better\nsynthetic data in differentially-private case, see details in Appendix J.1.\nAll standard techniques which are used to improve utility of DP-training [33] apply to both full\nfine-tuning and parameter-efficient tuning of LLMs. Specifically, it\u2019s usually better to train longer\nand with larger batch size [33]. Also it\u2019s important to do a hyperparameter sweep over learning rate\nand clipping norm, see appendix J.2. We also experimented with various parameters of temperature\nsampling for data synthesis. We found that sampling with T = 1, without truncating sampling\ndistribution (i.e. TopK = \u221e) and without performing any filtering usually well and is a reasonable\ndefault setting, refer to Appendix J.3.\nWe also looked into variations of loss formulation for LLM training (Appendix J.4). As explained\nin Section 4.1 we found that Prefix-LM [37] formulation usually leads to better performance in DP\nsetting, compared to Full LM. We observed that normalizing LLM loss by number of non-padding\ntokens, as suggested in [32], makes it easier to tune LLM hyperparameters, especially clipping norm\nfor DP. It also makes it easier for LLM to learn to produce outputs of various length.\nJ.1\nAblation: pre-training dataset and model size\nWe looked into influence of LLM pre-trainind dataset and model size on performance on downstream\ntask. As shown in table 8 larger size of LLM translated into better downstream performance for both\nBERT and CNN models.\nTable 8: Influence of model size on downstream performance. All models are finetuned or prompt tuned for 10\nepochs with 1024 batch size. Prompt tuning is done with loss normalization, fine-tuning is without. DP level is\n\u03f5 = 1.\nModel size\nPrompt tune\nFine tune\nBERT\nCNN\nBERT\nCNN\n1B\n83.2 \u00b1 1.1\n76.9 \u00b1 0.3\n74.7 \u00b1 2.4\n61.1 \u00b1 0.6\n8B\n86.2 \u00b1 0.4\n83.2 \u00b1 0.4\n85.5 \u00b1 0.2\n81.7 \u00b1 0.3\nAdditionally, we did some limited study comparing pre-training on C4 and The Pile datasets. Initially\nwe expected that C4 (which is essentially a web crawl) better matches text distribution in Yelp\nand IMDB datasets compared to The Pile (which was intentionally composed to be more diverse).\nHowever our experiments showed similar performance on both datasets, so eventually we settled on\nThe Pile, which is easier to download and use.\nJ.2\nAblation: hyperparmeter tuning for DP-training of LLM\nBatch size and number of training steps. For our prompt tuning run on IMDB with \u03f5 = 1 we did a\nthorough sweep of various batch sizes and number of training steps of LLM prompt tuning. Results\nare summarized in table 9 and confirm the general observation that longer training with larger batch\nis typically better when trained with DP.\nTable 9: This table show how downstream performance depends on batch size and number of training steps used\nfor LLM prompt tuning. Experiments were done on IMDB task, synthetic data were generated at privacy level\n\u03f5 = 1. Noise multiplier for each training run was computed to satisfy privacy budget given chosen number of\nsteps and batch size. Batch size 1024 with 220 steps correspond to 10 epochs of LLM training on IMDB dataset.\nBatch size\nBERT\nCNN\n220 steps\n440 steps\n880 steps\n220 steps\n440 steps\n880 steps\n1024\n85.5 \u00b1 0.7\n85.1 \u00b1 0.6\n87.8 \u00b1 0.2\n82.4 \u00b1 0.4\n83.2 \u00b1 0.4\n84.3 \u00b1 0.1\n2048\n85.7 \u00b1 0.2\n86.4 \u00b1 0.9\n87.9 \u00b1 0.1\n83.8 \u00b1 0.2\n83.2 \u00b1 0.2\n85.0 \u00b1 0.2\n4096\n86.0 \u00b1 0.6\n86.6 \u00b1 0.2\n88.1 \u00b1 0.4\n82.8 \u00b1 0.4\n83.8 \u00b1 0.2\n85.4 \u00b1 0.1\nLearning rate. In our experiments we found that downstream performance was quite sensitive to\nlearning rate of LLM, see table 10.\n21\nTable 10: Sensitivity of downstream task performance to learning rate used for LLM tuning.\n(a) Finetuning\nLearning rate\nBERT\nCNN\n1e-2\nDiverge\n3e-3 (optimal)\n85.5 \u00b1 0.2\n81.7 \u00b1 0.3\n1e-3\n69.1 \u00b1 3.0\n53.1 \u00b1 0.8\n(b) Prompt tuning\nLearning rate\nBERT\nCNN\n3e-3\n83.4 \u00b1 0.5\n80.0 \u00b1 0.4\n1e-3 (optimal)\n86.2 \u00b1 0.4\n83.2 \u00b1 0.4\n3e-4\n84.7 \u00b1 0.9\n80.1 \u00b1 0.6\nJ.3\nAblation: sampling parameters.\nAfter training a generative model, we still need to use that model to create a differentially private\ndataset. Large language models generate sequences one tokens at a time in an autoregressive manner;\ngiven previous tokens the model\u2019s final layer emits a probability distribution over all possible next\ntokens. Instead of sampling directly from this probability distribution, it is common to modify it in\nthree ways:\n1. Temperature: Shaping the token distribution using temperature t so that the final softmax\nover the final logits ui gives the next token probably as\nP(zi|z<i) =\nexp(ui/t)\nP\ni exp(ui/t)\n(2)\n2. Top K: Truncating the token distribution so that only the k most likely tokens are sampled\nfrom. All other tokens are given zero probability.\n3. Num Decodes: Repeating the full sampling process (i.e. decoding) N times and then out of\nthe N candidates return only the sample with the highest likelihood.\nTo analyze the effect of these parameters on dataset quality we performed a sweep over these\nparameters and computed the downstream performance of models as discussed in section I. Results\nare shown in tables 11 and 12.\nResults from our ablation study on sampling parameters show that while small gains can be gained\nfrom tuning these parameters, such gains are modest compared to using the default parameters of\nt = 1.0, top-k = \u221e, and num decodes = 1. We note that slightly higher temperatures appear to help\nin both cases (1.4 for IMDB and 1.2 for Yelp) and in both cases should be paired with either tighter\ntop-k or an increase in decodes. However, using additional decodes are computationally costly and\nlikely not worth the additional cost. Using a temperature less than 1.0 never seems to help.\nJ.4\nAblation: loss of LLM.\nLLMs are commonly trained/fine-tuned with next-token prediction teacher forcing. The loss for each\ntoken in this setup is a cross-entropy loss for each token. For an instance that is a collection of tokens\n(e.g. a full yelp review), the loss is therefore is a sum over per-token losses. It is common to normalize\nthis loss by the number of non padding tokens, which roughly translates into the normalization by the\nbatch size and the average number of tokens for instances in the batch. We recommend to follow\nthis normalization scheme because it makes it much easier to find the appropriate clipping norm for\nDP-Training. When the loss is not normalized, the appropriate clipping norm can be in the thousands.\nWith normalized loss, a standard clipping norm of 1 or 3 usually works out of the box. For example,\nfor Yelp dataset, without the loss normalization the clipping norm was found to be approx 2000, with\naccuracy of the fine-tuning of approx 0.29. With loss normalization, the clipping norm of 1 resulted\nin performance of 0.43.\nNote that if it\u2019s feasible to perform full hyperparameter sweep of clipping norm and learning rate,\nthen benefits of loss normalization diminishes. Nevertherless even in this case loss normalization can\nprovide small advantage, see Table 13.\nJ.5\nAblation: LoRA parameters.\nWe conducted detailed sweep of LoRA parameters (rank and in which layers to introduce LoRA) on\nIMDB and AGNews, see Tables 14 and 15. As could be seen from these tables, Full-LoRA performs\n22\nTable 11: Comparison across sampling parameters for best performing prompt-tuned epsilon=1 model for the\nIMBD task. The parameters Temp = 1.0, TopK = \u221e, Decodes = 1 corresponds to the default used in this\npaper.\nTemp\nTopK\nDecodes\nBERT Accuracy\nCNN Accuracy\n0.6\n\u221e\n1\n80.7 \u00b1 1.4\n78.4 \u00b1 1.2\n2\n78.7 \u00b1 1.8\n76.9 \u00b1 1.4\n4\n79.5 \u00b1 3.0\n76.3 \u00b1 1.6\n100\n1\n81.1 \u00b1 0.7\n77.9 \u00b1 1.5\n2\n78.1 \u00b1 1.1\n77.8 \u00b1 0.5\n4\n79.9 \u00b1 1.0\n75.9 \u00b1 0.7\n1000\n1\n79.9 \u00b1 0.7\n77.1 \u00b1 1.4\n2\n78.7 \u00b1 1.4\n75.4 \u00b1 2.4\n4\n78.1 \u00b1 2.3\n77.8 \u00b1 1.3\n0.8\n\u221e\n1\n84.7 \u00b1 0.5\n82.2 \u00b1 0.9\n2\n83.5 \u00b1 1.4\n82.8 \u00b1 0.4\n4\n82.8 \u00b1 0.4\n81.9 \u00b1 1.0\n100\n1\n84.9 \u00b1 0.6\n82.2 \u00b1 0.6\n2\n85.4 \u00b1 1.1\n81.6 \u00b1 0.7\n4\n82.1 \u00b1 1.0\n82.9 \u00b1 1.0\n1000\n1\n84.1 \u00b1 1.0\n82.2 \u00b1 0.2\n2\n82.9 \u00b1 1.0\n82.1 \u00b1 0.9\n4\n82.1 \u00b1 1.1\n83.1 \u00b1 0.3\n1.0\n\u221e\n1\n85.3 \u00b1 0.4\n84.2 \u00b1 0.6\n2\n86.1 \u00b1 0.2\n83.2 \u00b1 0.1\n4\n85.4 \u00b1 0.4\n83.5 \u00b1 1.7\n100\n1\n84.5 \u00b1 0.7\n84.9 \u00b1 0.4\n2\n86.8 \u00b1 0.4\n84.5 \u00b1 0.3\n4\n84.4 \u00b1 0.9\n84.7 \u00b1 0.3\n1000\n1\n86.3 \u00b1 0.4\n83.7 \u00b1 1.0\n2\n86.5 \u00b1 1.7\n84.3 \u00b1 0.3\n4\n85.6 \u00b1 0.4\n84.2 \u00b1 0.3\n1.2\n\u221e\n1\n87.1 \u00b1 0.7\n77.4 \u00b1 1.2\n2\n86.0 \u00b1 1.7\n78.5 \u00b1 2.9\n4\n85.6 \u00b1 0.9\n79.9 \u00b1 1.5\n100\n1\n85.8 \u00b1 1.3\n84.4 \u00b1 0.1\n2\n87.2 \u00b1 0.4\n84.8 \u00b1 0.6\n4\n86.5 \u00b1 1.0\n85.2 \u00b1 0.2\n1000\n1\n86.7 \u00b1 0.2\n81.9 \u00b1 0.2\n2\n86.9 \u00b1 0.6\n81.9 \u00b1 0.7\n4\n86.4 \u00b1 0.3\n82.3 \u00b1 0.5\n1.4\n\u221e\n1\n84.1 \u00b1 1.6\n60.0 \u00b1 3.5\n2\n85.5 \u00b1 1.2\n66.1 \u00b1 1.1\n4\n81.6 \u00b1 2.0\n60.6 \u00b1 2.9\n100\n1\n86.5 \u00b1 0.6\n83.3 \u00b1 0.2\n2\n86.9 \u00b1 0.3\n84.3 \u00b1 0.2\n4\n85.8 \u00b1 0.9\n84.4 \u00b1 0.2\n1000\n1\n85.3 \u00b1 1.2\n77.2 \u00b1 2.2\n2\n86.4 \u00b1 0.2\n78.3 \u00b1 0.8\n4\n86.6 \u00b1 0.4\n79.2 \u00b1 0.4\nbetter than Attention-LoRA and MLP-LoRA in most cases on IMDB dataset. However MLP-LoRA\nseem to be better choice overall on AGNews. If practitioner has to pick parameters to introduce\nLoRA beforehand without tuning, then we would recommend Full-LoRA as a reasonable default.\nIn terms of rank, best performance is typically achieved for ranks in range [8, 32]. From our\nexperiments, it does not make sense to increase rank above 32 because it result in little to no\nperformance gains, however it is more expensive because requires tuning of more parameters.\n23\nTable 12: Comparison across sampling parameters for best performing prompt-tuned epsilon=1 model for the\nYelp task. The parameters Temp = 1.0, Top-K = \u221e, Decodes = 1 corresponds to the default used in this paper.\nTemp\nTop K\nDecodes\nBERT Accuracy\nCNN Accuracy\n0.8\n\u221e\n1\n92.6 \u00b1 0.6\n87.9 \u00b1 1.1\n2\n89.9 \u00b1 1.1\n85.0 \u00b1 1.3\n4\n88.3 \u00b1 0.1\n81.7 \u00b1 0.9\n100\n1\n91.7 \u00b1 0.6\n86.6 \u00b1 1.6\n2\n89.9 \u00b1 0.6\n84.2 \u00b1 1.5\n4\n87.5 \u00b1 1.6\n80.5 \u00b1 0.4\n1000\n1\n92.5 \u00b1 0.3\n87.9 \u00b1 0.6\n2\n90.4 \u00b1 1.4\n85.4 \u00b1 1.7\n4\n86.9 \u00b1 1.1\n82.5 \u00b1 1.9\n1\n\u221e\n1\n93.4 \u00b1 0.7\n90.9 \u00b1 0.5\n2\n93.9 \u00b1 0.5\n90.5 \u00b1 0.4\n4\n93.7 \u00b1 0.6\n90.5 \u00b1 0.3\n100\n1\n93.2 \u00b1 0.5\n90.0 \u00b1 0.5\n2\n93.3 \u00b1 0.2\n89.4 \u00b1 0.6\n4\n92.4 \u00b1 0.2\n88.3 \u00b1 0.3\n1000\n1\n93.9 \u00b1 0.2\n90.8 \u00b1 0.2\n2\n94.0 \u00b1 0.2\n91.1 \u00b1 0.1\n4\n93.8 \u00b1 0.2\n89.7 \u00b1 0.6\n1.2\n\u221e\n1\n93.6 \u00b1 0.3\n90.1 \u00b1 0.2\n2\n93.5 \u00b1 0.2\n90.3 \u00b1 0.0\n4\n94.0 \u00b1 0.1\n90.6 \u00b1 0.1\n100\n1\n93.7 \u00b1 0.2\n91.2 \u00b1 0.1\n2\n93.7 \u00b1 0.1\n91.3 \u00b1 0.3\n4\n93.8 \u00b1 0.2\n91.3 \u00b1 0.2\n1000\n1\n94.1 \u00b1 0.1\n90.7 \u00b1 0.2\n2\n94.0 \u00b1 0.3\n90.9 \u00b1 0.1\n4\n94.2 \u00b1 0.1\n91.0 \u00b1 0.1\n1.4\n\u221e\n1\n93.2 \u00b1 0.2\n86.2 \u00b1 1.7\n2\n93.1 \u00b1 0.4\n87.2 \u00b1 0.8\n4\n92.9 \u00b1 0.8\n86.7 \u00b1 0.6\n100\n1\n93.4 \u00b1 0.4\n90.8 \u00b1 0.0\n2\n93.5 \u00b1 0.4\n90.9 \u00b1 0.0\n4\n93.5 \u00b1 0.0\n91.0 \u00b1 0.0\n1000\n1\n93.4 \u00b1 0.1\n88.9 \u00b1 0.1\n2\n93.5 \u00b1 0.0\n89.2 \u00b1 0.1\n4\n93.4 \u00b1 0.5\n89.2 \u00b1 0.3\nTable 13: Effect of loss normalization with sufficient hyperparameter tuning.\nBERT\nCNN\nLoss Norm\nNo Loss Norm\nLoss Norm\nNo Loss Norm\nIMDB prompt tuning, \u03f5 = 1\n86.4 \u00b1 0.9\n86.0 \u00b1 0.7\n83.2 \u00b1 0.2\n83.0 \u00b1 0.5\nK\nEvaluating synthetic data quality: Mauve robustness.\nMauve has multiple parameters that control its behavior. The most influential such parameters include:\nthe degree of dimensionality reduction (PCA explained variance), the number of clusters to use, the\nnumber of samples to use, and the model used to initially embed the samples. Pillutla et al. [31]\ncame to the conclusion that while these affected performance none of these parameters mattered\nenough to worry about needing to tune for a specific application. They recommended a default setting\nof buckets = 500 and explained variance = 0.9, We performed our own ablation studies on these\nparameters (see Figure 3). As noted in the paper, while we found MAUVE to be robust to most of\nthese parameters, the model mattered a great deal.\n24\nTable 14: Performance of downstream classifier depending on LoRA parameters, when LLM is trained on\nIMDB dataset with \u03f5 = 1.\nLoRA\nRank\n1\n2\n4\n8\n16\n32\n48\n64\nBERT\nFull\n85.4 \u00b1 0.2\n85.9 \u00b1 0.3\n89.7 \u00b1 0.1\n90.0 \u00b1 0.3\n90.0 \u00b1 0.2\n89.7 \u00b1 0.2\n89.9 \u00b1 0.3\n90.2 \u00b1 0.2\nMLP\n84.4 \u00b1 0.5\n85.9 \u00b1 0.3\n88.2 \u00b1 0.4\n86.1 \u00b1 0.1\n88.2 \u00b1 0.5\n87.9 \u00b1 0.2\n86.1 \u00b1 0.2\n88.1 \u00b1 0.7\nAttn\n82.5 \u00b1 0.4\n88.8 \u00b1 0.1\n87.7 \u00b1 0.3\n89.7 \u00b1 0.3\n90.1 \u00b1 0.1\n89.0 \u00b1 0.3\n89.8 \u00b1 0.3\n89.6 \u00b1 0.1\nCNN\nFull\n75.5 \u00b1 0.8\n84.1 \u00b1 0.7\n85.5 \u00b1 0.2\n87.6 \u00b1 0.4\n87.4 \u00b1 0.3\n87.2 \u00b1 0.2\n87.3 \u00b1 0.2\n87.5 \u00b1 0.3\nMLP\n71.9 \u00b1 0.6\n81.7 \u00b1 0.4\n86.8 \u00b1 0.1\n81.6 \u00b1 0.9\n85.7 \u00b1 0.2\n85.5 \u00b1 0.2\n83.6 \u00b1 0.3\n85.1 \u00b1 0.3\nAttn\n72.2 \u00b1 1.9\n86.9 \u00b1 0.3\n84.0 \u00b1 0.1\n87.4 \u00b1 0.2\n87.3 \u00b1 0.1\n85.7 \u00b1 0.1\n87.3 \u00b1 0.1\n86.8 \u00b1 0.4\nTable 15: Performance of downstream classifier depending on LoRA parameters, when LLM is trained on\nAGNews dataset with \u03f5 = 1.\nLoRA\nRank\n1\n2\n4\n8\n16\n32\n48\n64\nBERT\nFull\n88.1 \u00b1 0.1\n88.0 \u00b1 0.1\n88.2 \u00b1 0.3\n88.5 \u00b1 0.1\n87.9 \u00b1 0.1\n88.3 \u00b1 0.2\n88.8 \u00b1 0.1\n88.9 \u00b1 0.1\nMLP\n87.9 \u00b1 0.1\n88.0 \u00b1 0.2\n88.2 \u00b1 0.0\n88.4 \u00b1 0.1\n88.8 \u00b1 0.3\n89.4 \u00b1 0.1\n88.3 \u00b1 0.2\n88.3 \u00b1 0.1\nAttn\n87.7 \u00b1 0.2\n88.4 \u00b1 0.1\n88.1 \u00b1 0.2\n88.0 \u00b1 0.3\n88.2 \u00b1 0.1\n88.7 \u00b1 0.1\n-\n-\nCNN\nFull\n85.3 \u00b1 0.1\n85.0 \u00b1 0.1\n85.2 \u00b1 0.1\n85.4 \u00b1 0.1\n84.6 \u00b1 0.2\n85.3 \u00b1 0.1\n85.6 \u00b1 0.1\n85.7 \u00b1 0.2\nMLP\n84.9 \u00b1 0.2\n85.0 \u00b1 0.1\n85.2 \u00b1 0.1\n85.7 \u00b1 0.1\n85.6 \u00b1 0.2\n85.8 \u00b1 0.0\n85.2 \u00b1 0.1\n85.2 \u00b1 0.1\nAttn\n85.2 \u00b1 0.1\n85.3 \u00b1 0.1\n84.6 \u00b1 0.1\n85.0 \u00b1 0.1\n85.1 \u00b1 0.1\n85.4 \u00b1 0.1\n-\n-\nL\nEvaluating synthetic data quality: Proxy metric experiments.\nProxy metrics are a less expensive method for estimating synthetic data quality. They are particularly\nuseful for helping tune the large number of hyperparameters needed to train and sample from the\ngenerator model. This includes tuning the model type (model architecture, pre training process,\nfine-tuning, prompt-tuning, etc..), the hyper parameters of model training (e.g. epsilon, clipping norm,\nlearning rate, batch size, epochs, etc..), and finally the hyperparameters of the sampling procedure\nneeded to create the final dataset (e.g. temperature, top-k, and decodes). We examined how well\nvarious metrics correlate with downstream performance of a final classifier trained on the synthetic\ndata (Figure 4). Since we are interested in selecting the best performing parameters, we want a\nmetric that is most likely to select a high quality model and thus should care primarily about the rank\ncorrelation.\nL.1\nSynthetic data text length comparison\nWhile the distribution of text lengths isn\u2019t the most correlated to downstream performance, it still has\nstrong predictive power, and has the advantage of being easy to visualize and understand. We plot\nthe distribution of lengths across all final datasets (across architecture and epsilon) for the IMDB\nand YELP datasets (figures 5 and 6). We would point out that all synthetic data is truncated to 512\ntokens because this was sequence length of the trained LLM. At the same time, some of the real data\nis longer (17% of imdb and 7% of yelp) .\nL.2\nSynthetic data bigram frequency comparison\nComparing the distribution if bigrams across datasets can give additional insight into how the synthetic\ndata differs between finetuning approach and epsilon. As one might expect, less noise leads to a\nsmaller distributional gap (figure 7). Full finetuning has the largest gap, followed by prompt tuning,\nand LoRA being the most aligned with the original distribution.\nM\nImplementation details and estimates of the required compute\nLLM training. We run all our experiments using T5X codebase and used implementation of\ntransformer layers from Flaxformer library. For differentially private training of transformers we\nused private_text_transformers repository.\nWe pre-trained 1B model on TPUv3 with 128 cores and 8B model was pretrained on TPUv3 with\n1024 cores, both runs took around 8 days. Both finetuning and prompt-tuning of 8B models was\ndone on TPUv3 with 128 cores. Diferentially private finetuning of 8B model required between 20\nhours (for shortest run on IMDB) and up to 80 hours for some of the longer runs. Prompt tuning\nrequired 1.5 hour for short run and up to 20 hours for longest runs. LoRA-tuning was about 2x slower\ncompared to prompt tuning with 3 hours training for shortest run and up to 2 days for longest runs.\n25\nDownstream model accuracy\nRelative metric score\n0.00\n0.25\n0.50\n0.75\n1.00\n80\n82\n84\n86\n88\n90\n92\nbuckets=500, \nexplained var=0.90\nbuckets=500, \nexplained var=0.95\nbuckets=500, \nexplained var=0.70\nbuckets=1000, \nexplained var=0.90\nbuckets=100, \nexplained var=0.90\nFigure 3: Example of varying MAUVE parameters on estimating IMDB downstream performance on datasets\ndiffering in training epsilons from Table 1. Results are shown for the Sentence-T5-8B model.\n78\n80\n82\n84\nDownstream model accuracy\nRelative metric score\nTuning learning rate\n80\n84\n88\n92\nDownstream model accuracy\nEstimating dataset quality\nPerplexity\nUnigram score\nBigrams score\nLength score\nMAUVE-BERT\nMAUVE-Sentence-t5-base\nMAUVE-Sentence-t5-3B\n85\n90\n95\nDownstream model accuracy\nRelative metric score\nFigure 4: Proxy metrics for estimating dataset quality. Each point represents a metric\u2019s estimate of a synthesized\ndataset plotted against its true downstream classifier performance. X-axis shows the metric values re-scaled. All\nmetrics are only useful to compare datasets, and thus their absoluteabsolute value is uninformative. Top Left:\nDifferent learning rates of an IMDB prompt-tuning model. Top Right: Estimating IMDB dataset quality for\nresults in Table 1. Bottom Right: Estimating Yelp dataset quality for results in Table 1.\nIn most experiments we used a total batch size of 1024 examples per optimizer step, however we\ndid run a few ablations with larger batch as well. In order to fit entire batch into memory we used\na technique called gradient accumulation. This technique splits entire batch into smaller chunks,\nsequentially computes gradients over each chunk and then aggregates them together to obtain final\ngradient for entire batch. We used chunks of 32 examples for full finetuning and LoRA, and chunks\nof size up to 256 for prompt tuning. Even with difference in chunk sizes, we observed that LoRA\nis only 2x slower compared to prompt tuning when using the same total effective batch and same\nnumber of training steps.\nDownstream model. Downstream classifier was implemented in Tensorflow using Keras library for\ntraining and TFDS to load datasets.\nEach downstream model was run on TPUv2 with 8 cores. To obtain each downstream accuracy\nnumber we run a sweep of around 28 different hyperparameter settings. Entire sweep took around 4\n26\n0\n200\n400\n600\n800\n1000\n1200\n1400\nExample token length\n100\n101\n102\n103\n104\nFrequency\nDistribution of example lengths: Full fine-tuning on IMDB\nfinetune-no-dp\nfinetune-eps-10\nfinetune-eps-3\nfinetune-eps-1\nimdb_reviews\n0\n200\n400\n600\n800\n1000\n1200\n1400\nExample token length\n100\n101\n102\n103\n104\nFrequency\nDistribution of example lengths: Prompt-tuning on IMDB\nprompttune-no-dp\nprompttune-eps-10\nprompttune-eps-3\nprompttune-eps-1\nimdb_reviews\n0\n200\n400\n600\n800\n1000\n1200\n1400\nExample token length\n100\n101\n102\n103\n104\nFrequency\nDistribution of example lengths: LoRA tuning on IMDB\nlora-no-dp\nlora-eps-10\nlora-eps-3\nlora-eps-1\nimdb_reviews\nFigure 5: Length distribution (in tokens) of IMDB synthetic data vs original dataset.\nhours for CNN model and up to 80 hours combined for BERT model and synthetic dataset of 0.5M\nexamples. Each sweep was repeated 3 times to compute error bars.\nN\nExamples of generated and real data\nTable 16 shows examples of generated synthetic data.\nTable 16: Examples of real and synthetic datasets.\nEps\nDataset\nClass\nExample\n\u221e\n(real)\nYelp\nNegative\nMediocre burgers - if you are in the area and want a fast food burger, Fatburger is a better bet than Wendy\u2019s. But it is\nnothing to go out of your way for.\nNegative\nNot at all impressed...our server was not very happy to be there...food was very sub-par and it was way to crowded.\nNot the good kind I crowded where you feel like \\\"\"wow this is great it must be busy because the food is so great..\\\"\"\nBut the type of crowded where you feel a fight may break out. Also, if the chocolate fountain is their golden gem...why\nis it ok for people to dip the strawberry,lick off the chocolate and re dip it right then and there...absolutely disgusting!\nI Waited almost 10 minutes to dip...saw that and was immediately turned away...just saying...Never again!\nPositive\nI know, I know a review for Subway, come on. But I have to say that the service at this subway is top notch. The staff\nis very friendly and always goes out of there way to say hello. It is highly appreciated. The food is pretty standard\nSubway fare. Not too much to say there, but I will be be back again on my lunch breaks.\n27\n\u221e (syn-\nthetic)\nYelp\nNegative\nI waited for a table to be seated but it was not until 45 minutes into my 45 minute lunch. The service and attention\nto detail is outstanding. I got served by one gentleman which was fine until another needed help bringing an food\norder. He was not willing to ring me back until the staff had technically completed more than just a couple steps\nin the process. This was one of the most memorable bad encounters I have had with service anywhere. It was so\nbeyond poor that I couldn\u2019t even settle on a 1 star review for the sole purpose of the bad service.This is managements\nreptile effect. I may even review this restaurant on the yelp website as part of the bad service. ? r ? n ? r ? nWhat i do\nremember about the food was that I got the (medium well)lamb chops which my family enjoyed. I believe this was the\nonly 4.5 star review; all others are 5 stars. ? r ? n ? r ? nThank you Taste of Thai for being so exceptional. I will\ndefinitely eat here again.\\\" 2083 S. Colorado Ave. ? r ? nLansing, MI 492019 ? r ? nEmail: revgiebun@yahoo.com ?\nr ? n ? r ? n\\\"[eod] [pile] ; RUN: llc,?,%s -mtriple=sparclite-serializer-macho -o /dev/null ; The powerpc 64 bit ABI\nhard float ABI contains 32-bit values types ; (e.g. 6 (struct half*) or 7 (struct bfin64_2float3216*)). The Cortex-A8\n; processor doesn\u2019t contain facilities to load bulky 32-bit types (it only ; has equivalents for 8-bit and 16-bit types\nand it joined processing ; these groups into the same registers) ; RUN: ld.ll,?,%s,?,%p/../Inputs/structures.s ; RUN:\nllvm-objdump -h -j ../Inputs/structures.exe.macho.x86_64 -b binary\nNegative\nRestaurant is not worth the drive other places near Ames and Des Moines have better options. Service was weird and\nowner was a jerk for not doing anything with it. Saw some actual fish caught and skinned by people. They would take\ntheir picture just put it,on the wall. Submitted by: TomRindal A Restaurant Review of Guadalabar [yelp],?,?,?,?,? !\nFood could be a lot tastier if we paid more for the whole dining experience. They only charged me for one third of\nmy dish and always put a kick in the case. However, the fish and ric e?! Huh? Fish A..a..er... jiabitangellois so so\nbad..and it\u2019s served on top of the rice which kills all of the yuminess. There are still so many great places to eat in this\ntown, just need a little more energy to do it MrTripA4NO beer!!!! Good place to bring children... a dinky, cr appy\nfake Spongebob movie saturday morning in a 10 year old\u2019s restaurant.... been to Des Moines a few times and tried\nto find something tastier then Guadalabar, but at least they had smoothies.. and know what you are getting before\nyou overpay.. A great nd cheap breakfast! $2 sandw ich for \\\"bonus\\\"... A calligraphy class flyer!! wow.. our waiter\nwas the dishwasher; the restaurant was dirty & smelled like wet concrete. Definitely would not recommend.... this\nplace has no charm... Low Yelp rating = the customer service and manager are living up the secret. May be its just\nanother Ogden this time next year. Next time demote after 3rd year.,? ! ?,?,? cheesy ? ! ? ! Had three dinners at\nGuadalabar. Last one was last thurs, Wow! The pig in a blanket is outstanding! If lamb n onions don\u2019t click your\nmenu versus noodle risotto is a clear,must. My partner got the cheese appetizer. Decent! The fish sans name was tasty,\nand definitely\nPositive\nSuper cool food and interesting menu. Anything or everything can be made vegan and they are more than willing\nto cater to the vegan individual or families. I went in with a group of sisters who aren\u2019t into tofu and cepes on a\nregular basis. They were thrilled! I ordered the Mixed Marinara Veratofu which its made with squash, gnocchi,\nmushrooms, onions and is all natural. The Greens are also amazing, with quinoa and not so many red charcters like\nburgers. Look forward to this place anytime. ? n The best part is that they are great to the environment ? n Great\nservice (despite the fact it took a long, long time for the food to arrive), and low prices. Great food and it is away\nfrom downtown SF ? n We are fans! ? n ? n ? n Florence 2232 S Broadway #102 ? n Studio City, CA 91306 (562)\n285-9947 http://theamazingvegetemenu.com ? n ? n ? n DTLA #100 ? n Serving: Late Night Until 2 AM ?\n1\nYelp\nNegative\nBest bahn in Belgium 50 years ago and 50 years later. First, the breakfast is awful and thin. Reviews on yelp keep\ncomplaining. Most certainly were never sold in Poland. The taste is not bread at all, but as crumbly and flat pancakes\nfilled with sugar.\nNegative\nIf you are planning a party night out, don\u2019t go here. They back stab you, yell at you to leave the store, and are rude\nand rude. I told my mom that if we went to Frisch\u2019s for party supplies she would stay away a good 10 miles away.\nThank goodness for Walmart.\nPositive\nI am actually surprised they weren\u2019t busy like Wendi tweeted me. What a beautiful restaurant and atmosphere! Please\ntake my advice and make sure you stroll around golf course for some free and attractive restaurants, shops and more.\nPositive\nOMG, WOW, WHAT IS THIS?? Go for the Thai Iced Tea. My usual order is iced capp and has tea but this is better.\nReliable, inexpensive, and delicious! I have never had this before and this was a revelation!\n3\nYelp\nNegative\nGood choice, bad service. I live in Aliso Viejo and the service at Windrider Station has always been great but our\nexperience this evening was profoundly disappointing. The services mood, warmth, and grace is what set Windrider\nStation services apart from the rest, so I was surprised to see it be rendered to an adept officer.\nNegative\nBeen to the place a few times for Detroit wings. The taco meat was uneatable.. .carelessness in processing.. seriously..\nwasn\u2019t a tasty taco at all. .the frog tails were actually fried pieces of fish.. huge turn off. The unsavory tails..the frog\nwasnt half done.. all the meats were cold.. silly tacos for the price..\nPositive\nWe got a private room for three. When we arrived, the bartender was very pleasant and attentive. They had an early\ndinner special ($16) that was not available, so we had only two choices (entrees): Paradiso in Chilean Seafood & Lots\nof Focaccia ($10.99) or Steelhead Salmon ($17.99). The entrees came pretty quickly.\nPositive\nThese guys can help you with same day prices on everything you need. Best service - best prices! I come back here\nevery time I need something.\n10\nYelp\nNegative\nmy boyfriend and i went there a couple of times - it was pleasant and very modest and they had a GREAT happy hour!\nbut on the dinner side, it\u2019s not really worth the price. the atmosphere was nice, but the food was so-so and the amount\nof service was adversely affected - two servers there at a time only for dinner service\nNegative\nI must strongly recommend that anyone planning to visit Vegas anytime soon avoid this location. The staff (the two\nservers and their food delivery person) were all rude, insulting and insensitive. I have been to many places and have\nhad very bad experiences and have never returned to any of them.\nPositive\nOk so I went for their lunch buffet which cost like 34 clams. This is the second buffet buffet I\u2019ve been to here in the\nlast couple months and I always noticed that theirs are always more than 80 dollars per person, which is a a little high,\nbut it\u2019s more than worth it.\nPositive\nThis place is definitely cool. They have a really cool beer selection and a couple salsa dancers! nIboy was the DJ at\nthe event, and he\u2019s the real deal. His set was fast paced and exciting! If you get a chance to throw a party there, don\u2019t\nhesitate!\n\u221e\n(real)\nIMDB\nNegative\nThis was an absolutely terrible movie. Don\u2019t be lured in by Christopher Walken or Michael Ironside. Both are great\nactors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\u2019s\nridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those\nwhen the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and\nher pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real\nmeaning. I am disappointed that there are movies like this, ruining actor\u2019s like Christopher Walken\u2019s good name. I\ncould barely sit through it.\nPositive\nThis is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as\nyou descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas\nCage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty\npaddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\nNegative\nThe film is based on a genuine 1950s novel.Journalist Colin McInnes wrote a set of three \"London novels\": \"Absolute\nBeginners\", \"City of Spades\" and \"Mr Love and Justice\". I have read all three. The first two are excellent. The last,\nperhaps an experiment that did not come off. But McInnes\u2019s work is highly acclaimed; and rightly so. This musical is\nthe novelist\u2019s ultimate nightmare - to see the fruits of one\u2019s mind being turned into a glitzy, badly-acted, soporific\none-dimensional apology of a film that says it captures the spirit of 1950s London, and does nothing of the sort.Thank\ngoodness Colin McInnes wasn\u2019t alive to witness it.\n28\n\u221e (syn-\nthetic)\nIMDB\nPositive\n\"This movie is filled with twists and turns from the first moment it enters the big screen, until the final moments of the\nfilm. It gives you enough information to keep you from extrapolating into going anywhere other than the truth. And\nthe only way to know the truth is to simply pay money to see (probably the more fun thing to do, money being the\n\"other\" presence in this equation) the DVD (if you just have Netflix, just buy it it\u2019s cheaper). ? br />? br />This is one\nof those movies like Meme ...\nNegative\n\"I thought they rewrote a much better movie that made some changes here and there. Even though the plot was a\nbit slow and idiotic at times, it was acted well by all of its leads, including Morgan Freeman. It had little to do with\nthe original story, other than the fact that Morgan Freeman played Maurice, the dumb Irish dude from The Usual\nSuspects.\"\nPositive\n\"Nice deserted beach nude scene. ? br />? br />Sourced from a DVD I picked up a few months ago, this is the 1972\nOscar winning Fernando Trueba documentary of the legendary Sea Stars along Catalan coastline beaches. The 1970s,\nwhen Trueba was a young artist who traveled to beaches all over Europe, was a great soul renaissance of the French\nRiviera, 1970-83. The 1970s Paris Inter City was the main hub of the beach culture, and Spanish Swimsuit was the\nfraternal new generation\u2019s version of Euro Tr ...\n1\nIMDB\nNegative\nSo... Off the top. Ill start this off saying that I thought Ethan Hawke and Dakota Fanning were excellent, and this\nmovie + I were great. However, this movie was just too sad and depressing + I kept falling asleep when the characters\nvisited their pasts to either \u00a8save\u00f6r \u00ebscape\u00a8. The only good scene was that fanning sung in the shower with her father on\nhis death bed, she actually gave him her first big laugh... incredible. All and all, this film was a horrible waste of time.\nI\u2019m almost embarrassed for rating it this bad, because I enjoyed it when I saw it at the movies, but now that I had the\ntime to re-watch it I decided to deduct a point or two.\nNegative\nAn underwhelming, blandly written, dry, dull romance. A subtle attack on the Bible and marriage as the supreme goal\nof woman. Toni Collette\u2019s Stephie She is the perfect stereotype of the Southern housewife, flawed but go-getting, a\nrole that was traditionally assigned to the woman and embodied by stereotypical southern belles. Alexandra and Danny\nrepresent the emerging of the consumerist age, and don\u2019t consider themselves traditional, but not dirty minded enough\nto change the old ways, even as they clash with Collette\u2019s Stephie, an emancipated, tough, independent, rebellious\nyoung woman living the new age. Directors Jim Gianopulos and Roman Polanski are wacky, and impressionistic.\nPositive\nThis movie is what they say - it is well done, as of perfection. This is a true story of a man on the run for stealing his\nbest friend\u2019s business and the mental battle of escaping by himself across many states. The shooting scenes are good\nas a bootlegger must shoot his way through a posse, but unfortunately the ending and some events which I\u2019ll keep for\nprivate reasons lead to a trickle down effect between a friend killing the lead female singer and the actor who leads the\nposse. The rest of the movie though is good. If you read the book you\u2019ll notice that the names are changed because a\nbook wouldn\u2019t allow too many people to get away with their crimes. The film is the blueprint for success.\nPositive\nThis movie seriously features one of the actress\u2019 very best performances. Jenny Klein plays a mysterious writer that\nappears to write as a zombie (she appears to feed the moat of her garden and dead animals to her zombie voice) then\nright after she finds out that her husband has cheated on her in a bizarre revenge scenario, she has to confess to the\nworld that she is a writer as well as have some mysterious SCREAM revelation. This bravura performance allows the\nviewer to meets the father of childhood and perform real art in a particularly violent but weirdly charming way. Don\u2019t\nmiss it.Recommended to lovers of H.P. Lovecraft and \u00a8The Babysitters Club\u00a8.\n3\nIMDB\nNegative\nTraining day ripoff, producer Related Entertainment The Movie Kitoshia Izumi plays \u00a8Jacob Marlowe\u00e4 doctor who\nloves making macho statements like \u00cf\u2019m not sexually attracted to women\u00e4nd \u00a8women are pigs\u00a8. With a woman dressed in\na pig outfit that looks like something peroxided from a beauty parlor, lumbo-lumbagoed into place. \u00a8Nia Catastrophe\u00efs\nabout a Police officer (Marlowe) who decides to become a doctor to save the lives of the citizens of Beaver Falls\n(Pennsylvania). But the lives he saves will only result in the eventual death of all the headliners he\u2019s supposed to be\nbringing back to life. Izumi\u2019s insufferable character is almost laughable. The attempt to portray him as sensitive and\ncaring, never works, either. Things get worse when there\u2019s a drug dealing merchant on the bus who pretends to be a\npassenger. But most of the movie is a parade of wretched acting.\nNegative\nI was bored watching this film. The characters weren\u2019t sympathetic and the writing was horribly melodramatic. The\nending was so petty. It smacked of the cliche\u2019 \u00a8look how the cat got his revenge\u00a8theme. I\u2019ll wait until someone makes a\nslicker, funnier film out of this material.\nPositive\nI would say all of the 3 or 4 days I was watching Golden Boy, I was very interested. Even a small, almost anonymous\nperson with no name, a poor little bird, a small and weak sport - french- speaking effet, from a city without a name.\nThat was his table. Don\u2019t watch his card but stop, look and marvel. The way he fights, his shots, how he makes an\nattack, how he avoids the devastating blow. He fights consistsantly, competently, always attacking, never believing his\nopponent is able overwhelm him.\nPositive\nThis is the perfect older brother summary. I know very few children whose real siblings are more mature then them.\nMina Wang is a gifted doctor. Here she is an advisor, takes care of a far off plantation of the nature. This girl is the\ncenter of all things worng. She comes up with a plan to get revenge on Tom, father of her boyfriend Peter. Tom has\njust made a big mistake that could really affect Milo as a child.\n10\nIMDB\nNegative\nFirst, why, on the LORD\u2019S day, was this movie commercial to Bears players head-upar water, a DH baseball player,\nand a big, hairy bachelor? Funniest irespondent thing I\u2019ve ever seen in my life. Second, don\u2019t do it again, JW. Didn\u2019t\ncut it with Bears baseball, so don\u2019t do it with movies.\nNegative\ni known i\u2019ll please a majority of people while a few others will vilify me for this review, but one deduction i might\nmake is that i have seen movies much more professional, i.e. directed by \u00a8masters\u00a8, made with more professional actors\nof course, etc\nPositive\nI love Westerns and shows that take a somewhat exaggerated and satirical view of life. This is such a show! If you\nlike Westerns, this one is a must-see. Of course The Cisco Kid is a fine example of that favorite genre. Mr. Melish\nadds the character of Mochica to the Chicken Ranch, not many westerns do wilder and funnier things. Well worth\nwatching!!!!!!!\nPositive\nA story based on the life of one of Sad Hill Folks\u2019. Being a character of how his life has shaped him, this film was\ndirected/narrated by the man himself. By novelising the story I was able to view a character worthy of a film, and a\nfull 45 minute Theme song! Totally beautiful.\n29\n0\n200\n400\n600\n800\n1000\n1200\n1400\nExample token length\n100\n101\n102\n103\n104\nFrequency\nDistribution of example lengths: Full fine-tuning on Yelp\nfinetune-no-dp\nfinetune-eps-10\nfinetune-eps-3\nfinetune-eps-1\nyelp_polarity_reviews:0.2.0\n0\n200\n400\n600\n800\n1000\n1200\n1400\nExample token length\n100\n101\n102\n103\n104\nFrequency\nDistribution of example lengths: Prompt-tuning on Yelp\nprompttune-no-dp\nprompttune-eps-10\nprompttune-eps-3\nprompttune-eps-1\nyelp_polarity_reviews:0.2.0\n0\n200\n400\n600\n800\n1000\n1200\n1400\nExample token length\n100\n101\n102\n103\n104\nFrequency\nDistribution of example lengths: LoRA tuning on Yelp\nlora-no-dp\nlora-eps-10\nlora-eps-3\nlora-eps-1\nyelp_polarity_reviews:0.2.0\nFigure 6: Length distribution (in tokens) of YELP synthetic data vs original dataset.\n30\n0\n2000\n4000\n6000\n8000\n10000\nSamples (smoothed with buckets of size 20)\n10\n4\n10\n3\n10\n2\nFrequency\nDistribution of bigrams: Full fine-tuning on IMDB\nimdb_reviews\nfinetune-no-dp\nfinetune-eps-10\nfinetune-eps-3\nfinetune-eps-1\n0\n2000\n4000\n6000\n8000\n10000\nSamples (smoothed with buckets of size 20)\n10\n3\n10\n2\nFrequency\nDistribution of bigrams: Prompt-tuning on IMDB\nimdb_reviews\nprompttune-no-dp\nprompttune-eps-10\nprompttune-eps-3\nprompttune-eps-1\n0\n2000\n4000\n6000\n8000\n10000\nSamples (smoothed with buckets of size 20)\n10\n3\n10\n2\nFrequency\nDistribution of bigrams: LoRA tuning on IMDB\nimdb_reviews\nlora-no-dp\nlora-eps-10\nlora-eps-3\nlora-eps-1\nFigure 7: Bigram distribution of IMDB synthetic data vs original. Bigrams were sorted by their frequency in\nthe original dataset with the first 10,000 frequencies shown.\n31\n"
  },
  {
    "title": "Evaluating Language Models for Mathematics through Interactions",
    "link": "https://arxiv.org/pdf/2306.01694.pdf",
    "upvote": "2",
    "text": "Evaluating Language Models for Mathematics through Interactions\nKatherine M. Collins\u2217a, Albert Q. Jiang\u2217a,\nSimon Friederb, Lionel Wongc, Miri Zilkaa, Umang Bhatta,d,e, Thomas Lukasiewiczf,b,\nYuhuai Wu\u2020g, Joshua B. Tenenbaumc, William Harta, Timothy Gowersh,a, Wenda Lia,\nAdrian Weller\u2021a,d,2, and Mateja Jamnik\u2021a,2\naUniversity of Cambridge\nbUniversity of Oxford\ncMIT\ndThe Alan Turing Institute\neNYU\nfVienna University of Technology\ngx.AI\nhColl\u00e8ge de France\nNovember 7, 2023\nAbstract\nThere is much excitement about the opportunity to harness the power of large language models (LLMs)\nwhen building problem-solving assistants. However, the standard methodology of evaluating LLMs relies\non static pairs of inputs and outputs, and is insufficient for making an informed decision about which\nLLMs and under which assistive settings can they be sensibly used. Static assessment fails to account for\nthe essential interactive element in LLM deployment, and therefore limits how we understand language\nmodel capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with\nand evaluate LLMs. We conduct a study with CheckMate to evaluate three language models (InstructGPT,\nChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort\nof participants from undergraduate students to professors of mathematics. We release the resulting\ninteraction and rating dataset, MathConverse. By analysing MathConverse, we derive a taxonomy of\nhuman behaviours and uncover that despite a generally positive correlation, there are notable instances\nof divergence between correctness and perceived helpfulness in LLM generations, amongst other findings.\nFurther, we garner a more granular understanding of GPT-4 mathematical problem-solving through a\nseries of case studies, contributed by expert mathematicians. We conclude with actionable takeaways\nfor ML practitioners and mathematicians: models that communicate uncertainty respond well to user\ncorrections, and are more interpretable and concise may constitute better assistants. Interactive evaluation\nis a promising way to navigate the capability of these models; humans should be aware of language models\u2019\nalgebraic fallibility and discern where they are appropriate to use.\n\u2217Equal contributions as leading authors. Correspondence: {kmc61, qj213}@cam.ac.uk.\n\u2020Work done while at Stanford.\n\u2021Equal contributions as senior authors.\n1\narXiv:2306.01694v2  [cs.LG]  5 Nov 2023\nContents\n1\nIntroduction\n4\n2\nResults\n5\n2.1\nObservations from Applying CheckMate\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nTaxonomising User Behaviour from MathConverse . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3\nInvestigations into the MathConverse Annotated Taxonomy . . . . . . . . . . . . . . . . . . .\n8\n2.4\nQualitative Observations from Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.5\nPer-Expert Conclusions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.6\nKey Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3\nDiscussion\n12\n3.1\nTakeaways for ML Developers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.2\nTakeaways for Mathematicians (Students, Educators, and Researchers) . . . . . . . . . . . . .\n13\n3.3\nTakeaways for LLM Development, Evaluation, and Deployment . . . . . . . . . . . . . . . . .\n14\n4\nLimitations\n15\n5\nConclusion\n15\n6\nMethods\n15\n6.1\nCheckMate: Adaptable Platform for Interactive Evaluation . . . . . . . . . . . . . . . . . . . .\n15\n6.2\nDeriving a Taxonomy from MathConverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n6.3\nInteractive Case Studies with Experts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA Primer on Proof Assistants\n26\nB An Example Survey Problem\n27\nC An Overly Verbose Response\n27\nD User Guide for CheckMate\n27\nD.1 Hypothetical extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nD.2 Implementation Challenges\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nE Additional Details on Survey Set-Up\n29\nE.1\nRating Scales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nE.2\nParticipant Recruitment and Additional Details . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nE.3\nLanguage Model Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nE.4\nFurther Details on Tasks Chosen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nE.5\nExample Interface Screens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nF Additional Details on Taxonomy Construction\n32\nG Additional Survey Observations\n34\nG.1 Potential Misleading Generations under Human Uncertainty . . . . . . . . . . . . . . . . . . .\n34\nG.2 Ties in Rank-Order Preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nG.3 Relationship Between Web Occurrences and Ratings . . . . . . . . . . . . . . . . . . . . . . .\n35\nG.4 Additional Investigations into the MathConverse Annotated Taxonomy . . . . . . . . . . . . .\n35\nG.5 Handling Errors in Task Specification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nG.6 Interaction Ratings by Mathematics Topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n2\nH Post-Survey Testimonials from Participants\n38\nH.1 Why Stop Interacting? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nH.2 Human Uncertainty in Model Correctness?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nH.3 Correct, but Unhelpful? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nH.4 Using LLM-Based Assistants in Practice?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nI\nInteractive Case Studies with Experts\n40\nI.1\nProblem Perturbation to Probe Memorisation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nI.2\nInvestigating the Boundary between Easy and Hard Problems . . . . . . . . . . . . . . . . . .\n45\nI.3\nQualitative Observations from Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\nJ\nFull Interaction Traces from Expert Case Studies\n52\nJ.1\nFull Interaction Traces from \u201cNumber Theory\u201d Case Studies . . . . . . . . . . . . . . . . . . .\n52\nJ.2\nFull Interaction Traces from \u201cProblem Perturbation to Probe Memorisation\u201d Case Studies . .\n63\nJ.3\nFull Interaction Traces from \u201cBoundary of Easy and Hard\u201d Case Studies . . . . . . . . . . . .\n70\n3\nQuestion\nReference \nanswer\nGenerated \nanswer\nMatching?\nQuestion\nQuery\nGenerated \nanswer\nHow correct? \nHow helpful?\nStatic evaluation\nInteractive \nevaluation\n(a)\n(b)\nFigure 1: a) Contrasting typical static evaluation (top) with interactive evaluation (bottom), wherein a human\niteratively queries a model and rates the quality of responses. b) Example chat interface from CheckMate to interact\nwith an LLM. The participant is presented with the problem and a reminder of particularly relevant instructions, and\nenters their chat below (which they can see compiled in LaTeX). Chat history is presented for the user to refer to.\n1\nIntroduction\nFoundation models (Bommasani et al., 2021) \u2013 in particular large language models (LLMs) (Anil et al., 2023;\nBrown et al., 2020; Touvron et al., 2023) \u2013 are increasingly human-facing, permitting users to interact with\nand elicit natural language responses (K\u00f6pf et al., 2023; OpenAI, 2022). Such interactive systems admit a\nplethora of new possibilities for human-machine collaboration (Ayers et al., 2023; Github, 2021; Mirowski\net al., 2023). However, existing evaluation approaches of LLMs that are used to help inform how to best\nharness their power in this human-machine interaction are insufficient. In particular, capability assessments\nfor LLMs typically assume a non-interactive view: models are primarily evaluated statically with \u201cground\ntruth\u201d input - output pairs, and metrics are aggregated over a dataset (Burnell et al., 2023), which may be\nmisaligned with their use cases.\nTo address this problem, we argue in this paper that interactive and dynamic evaluation of LLMs (Lee et al.,\n2022b; Shen and Wu, 2023) is essential for grasping their capabilities. We carry out such an interactive and\ndynamic evaluation of how humans use LLMs for assistance (see Figure 1a for an overview of how interactive\nevaluation differs from conventional static evaluation) to better characterise their limitations, undesirable\nbehaviours, and potential harms.\nEvaluating LLM interactions is especially warranted in the case of informal mathematical theorem proving,\nwherein an agent is given a mathematical theorem and needs to propose a proof that is acceptable to the\nmathematical community. Informal theorem proving is special in that there is a formal notion of correctness\nat its core, yet most things are expressed in natural language (informally). Important quality measures for\nthe task include helpfulness and correctness, neither of which can be satisfactorily captured by automatic\nmetrics (e.g., BLEU and ROUGE scores) (Golovneva et al., 2022; Lin, 2004; Papineni et al., 2002). Letting\nhumans communicate and evaluate these systems is crucial for their assessment. Further, mathematics is\nan interactive practice. Recent works (First et al., 2023; Welleck et al., 2022b) have shown that LLMs can\nbenefit from feedback on their previously-produced language output (i.e., \u201cgenerations\u201d) in mathematical\ntasks, and this benefit can only be seen in multi-turn interactive evaluations. Hence, we choose mathematics\nto best highlight the value of human interactive evaluations. But CheckMate and our broader evaluation\nmethodology can be extended to other domains and chatbot-based systems (see Supplement).\nConcretely, we apply two methods to analyse human-LLM mathematical reasoning interactions: (1) structured\nevaluation, that is, rating every LLM generation in a conversation; and (2) free-form evaluation, where\nexpert mathematicians conduct instance-level case studies. The latter approach is inspired by the burgeoning\nliterature of involving domain experts alongside ML practitioners in understanding model behaviour (Davies\n4\net al., 2021; McGrath et al., 2022), and directly responds to the call from Burnell et al. (Burnell et al., 2023)\nto engage with non-ML scholars to better understand LLM systems. Our study is interdisciplinary at its core.\nDespite the large number of LLM-based chatbots, there is a paucity of open and unified platforms for eliciting\nfine-grained evaluations of interactions with users at scale. Hence, we develop a lightweight interactive evalu-\nation platform that is highly adaptable, called CheckMate1. We leverage CheckMate to conduct an empirical\nstudy on undergraduate-level theorem proving (see an example problem in SI An Example Survey Problem),\nover a suite of popular language models \u2013 InstructGPT (Ouyang et al., 2022), ChatGPT (OpenAI, 2022),2\nand GPT-4 (OpenAI, 2023b). We release the resulting interactions and evaluations on 261 human-model\ninteractions in a new dataset called MathConverse, from which we derive a preliminary taxonomy of user\nbehaviours. We do not claim completeness for our taxonomy, because of the limited size of MathConverse.\nOur study is particularly compelling as it not only engages a group of participants with a wide range of\nmathematical experience, but the level of problem difficulty is higher than what is typically explored (Amini\net al., 2019; Cobbe et al., 2021; Zheng et al., 2022). We emphasise that CheckMate can be conveniently\nextended to domains other than mathematics. We also invite three expert mathematicians to contribute\nin-depth interaction case studies to help better characterise current LLM mathematical reasoning capabilities.\nThroughout, we emphasise that we are not trying to draw broad conclusions across the entire LLM landscape.\nRather, we aim to highlight the feasibility and value of incorporating interactions into the evaluation process,\nparticularly when involving domain experts, and to elucidate potential human and model behaviour patterns\nspecifically in mathematics.\nOur three key contributions are:\n\u2022 We introduce an adaptable platform, CheckMate, for evaluating language models by their interactions with\nhuman users. We demonstrate that scalable and valuable dynamic interactive evaluations are feasible by\napplying CheckMate to evaluate three language models on mathematical theorem proving.\n\u2022 With interactions and evaluations collected from CheckMate via a mixed cohort study, we derive a taxonomy\nof user behaviours which identify crucial expected abilities of LLM-based mathematical assistants. We\nrelease the dataset of CheckMate interactions and evaluations, MathConverse.3\n\u2022 Through case studies conducted by expert mathematicians, we add empirical evidence for several weaknesses\nof the LLMs that we explore, including algebraic manipulations, over-verbosity, and over-reliance on\nmemorised solutions.\nWe urge solutions from ML practitioners to these challenges (such as better\ncommunication of uncertainty and ability to update user corrections) and suggest good practices for LLM\nusers (e.g., to heed caution when inspecting generations, as mistakes can be subtle). We encourage further\ninteractive evaluation with LLMs, in mathematics and beyond, to inform how, when, and whether to deploy\nthese models in assistive settings.\n2\nResults\nWe first present results from both of our evaluation methods \u2013 structured multi-step interactive ratings and\nthe free-form instance-based evaluation (see Figure 2) \u2013 before synthesising the key insights across studies.\n2.1\nObservations from Applying CheckMate\nWe highlight our primary findings from applying CheckMate in the domain of mathematics, through which\nwe collect and introduce the MathConverse dataset (see example interface screen in Figure 1b and the\nSupplementary Information (SI)).\n1The name alludes to the interactive manner of the evaluation, or \u201cchecking\u201d, to be the kind you may do with a \u201cmate\u201d.\n2When we refer to ChatGPT in the text, we mean \u201cgpt-3.5-turbo\u201d, according to the OpenAI API.\n3They can be found on: https://github.com/collinskatie/checkmate.\n5\nSystems Optimised for Chat are Preferred\nParticipants were not informed of which model they were interacting with and rating. Nonetheless, we nicely\nobserve in Figure 2a that, as expected, models optimised for chat (ChatGPT and GPT-4) are consistently\nrated as preferable to those not (InstructGPT) with GPT-4 being most frequently favoured and much less\noften least preferable; i.e., the lower bound (\u201cworst case\u201d) behaviour of GPT-4 is consistently better than the\nother models (e.g., the model is never ranked as the worst assistant). We emphasise that these evaluations are\nnot meant to be definitive assessments of model performance, but rather, highlight that interactive evaluation\ncan yield a more nuanced understanding of model behaviour beyond the common \u201csnapshot\u201d evaluation on\nstandard benchmark datasets.\nPerceived Utility Per Model\nWe next look at individual interactions. Participants were asked to rate the mathematical correctness and\nperceived helpfulness of each generation; we depict the helpfulness and correctness ratings across models in\nFigure 2b. These data further reveal distinctions across models; notably, GPT-4 achieves consistently high\nhelpfulness ratings, underscoring its potential perceived utility.\nHere, the generation4 is rated to have \u201cMultiple critical maths errors\u201d (correctness score 2), while being\n\u201cSomewhat useful\u201d (helpfulness score 4): This response from the assistant is indeed plagued with errors and\nmisunderstandings, but it also contains the structure of a correct proof as well as the Rank-Nullity Theorem\nwhich is useful if used properly.\nWe observe in Figure 2c that the perceived helpfulness and correctness correlate positively for all three\nmodels, although interestingly some generations can be deemed completely mathematically correct, but\nnot particularly helpful (e.g., 6 correctness, 3 helpfulness). This can occur, for instance, for overly verbose\nresponses, e.g. the example in SI An Overly Verbose Response.\nThese data further assert the value of multi-factorial LLM evaluations, beyond a single scalar \u201cgoodness\u201d\nscore. We expand on these two points in our expert case studies.\n2.2\nTaxonomising User Behaviour from MathConverse\nAdditionally, we qualitatively study the characteristics of the participants\u2019 interactions with the LLMs. A\ncore motivation is to get a grasp of how mathematicians actually use these systems in the wild \u2013 what queries\nthey may initiate, and how they follow-up with the model over the course of the interaction. Following Lee\net al. (Lee et al., 2022c), we offer a preliminary taxonomy of the kinds of queries found in our data. All\ninteraction traces are released anonymously in our repository to support further study of human-machine\ninteraction, along with our annotated taxonomy. Details on how we constructed and annotated such a\ntaxonomy are included in the Methods and SI Additional Details on Taxonomy Construction.\nInitial Interaction Behaviour\nWe find that participants typically take one of four approaches in their first\nquery to the AI assistant. Remarkably, over 90% of participants\u2019 first interaction for each problem\nfall into one of the following interaction behaviour types:\n1. Seeking specific definitions of a concept mentioned in the problem (e.g., \u201cDefinition of Hall\nsubgroup\u201d or \u201cWhat is the definition of \u2018nullity\u2019 in linear algebra?\u201d).\n2. Asking a general question about mathematics related to the problem (e.g., \u201cWhen is a plane in\nR3 parallel to another plane in R3\u201d or \u201cIn mathematics, what does it mean to Let A \u2208\nKn\u00d7n\u201d).\n4This generation is from GPT-4; recall, participants were not told which model they were interacting with and rating, or even\nwhich models they could have been interacting with.\n6\n3. Simply copy-pasting the entire problem statement, or a slight rephrasing of the original statement, option-\nally with prepended instructions (e.g., \u201cCan you assist me in proving the following statement?\n[...]\u201d).\n4. Prompting the model for a single step of the problem, rather than the entire problem all at once (e.g.,\n\u201cWe will first prove a lemma, let us call it Lemma 1 [...]\u201d).\nMid-Interaction Behaviour\nWe observe a wider array of interaction modes after the first interaction.\nIn addition to repeats of the above, such as users asking for more definitions, we find the following general\npatterns:\n5. Asking a clarifying question (e.g., \u201cDoes it hold even when p is not a prime number?\u201d).\n6. Correcting the model output, occasionally with a clarifying question (e.g., \u201cI understand.\nBut your\nexample is misleading.\nIn your example, f has degree 2 and it has 2 roots, so it does\nnot represent a valid counterexample.\nCan you show an example in which a polynomial has\nmore roots than its degree?\u201d).\n7. Asking for clarification about the generation from the model (e.g., what a particular symbol means \u2014\n\u201cWhat is \u03c4 here?\u201d).\n8. Asking why the model did something (e.g., \u201cso why do you need to add the whole set at step\n2?\u201d).\n9. Implicitly correcting the model (e.g., \u201cThat sounds like there being a homeomorphism.\nBut a\ncontraction is not a homeomorphism?\u201d).\n10. Asking for instances of a particular construction (e.g., \u201cCan you exhibit an example to demonstrate\nthat?\u201d).\nWe also find that a few participants would ask the model to \u201ccontinue\u201d if it stopped midway through, and some\nparticipants seemed to attempt popular prompt engineering tricks, for example, attempting to get the model\nto \u201crestart\u201d by telling it to forget what it had done before: \u201cForget what you\u2019ve said before and try\nagain.\nStart with nx + ny = nz, divide both sides by nz, and reason from there.\u201d. Further,\nwe note that one participant asked for intuition about a concept: \u201cWhat is the intuition behind the\nproof of this statement?\u201d\nHere, the model (GPT-4) provided a response rated as \u201cdefinitely helpful\u201d, indicative of a potential exciting\nassistive case going forwards. In addition to revealing the kinds of interactions that mathematicians may\nmake to help motivate the design of tools better equipped to handle such interactions (e.g., when participants\nask for clarification), we see these observations as pointers to broader public education as to what AI systems\ncan be leveraged to help with and how to best query for this help (such as particular prompt techniques).\nRating Dynamics over the Interaction Trace\nAs noted, we observe that several participants attempt to correct the model\u2019s output or ask for clarification.\nSometimes these occurrences would go on for a few successive trials; we refer to such correction-mistake\ninteraction ruts as \u201cfrustration cycles\u201d. We can see some of this behaviour by inspecting the rating dynamics\nacross interaction traces. In SI Figure S6, we see that in general, participants\u2019 ratings begin to fall off\nover the course of interactions, and through Figure 2d, we see that participants seem to stop when both\nratings are higher than 4, indicating the model clearly can solve the problem and \u201cassist\u201d them), or with both\nratings lower than 2 (indicative of the model completely failing to provide any further useful mathematical\nknowledge). We include participant testimonials about why they chose to stop in SI Post-Survey Testimonials\nfrom Participants.\n7\nInstructGPT\nChatGPT\nGPT-4\n0\n5\n10\n15\nCount\nRank: 3\nRank: 2\nRank: 1\n(a)\n0\n1\n2\n3\n4\n5\n6\nCorrectness\n0\n10\n20\n30\n40\nCount\nInstructGPT\nChatGPT\nGPT-4\n0\n1\n2\n3\n4\n5\n6\nHelpfulness\nCount\n(b)\n1\n2\n3\n4\n5\n6\nCorrectness\n0\n1\n2\n3\n4\n5\n6\nHelpfulness\nInstructGPT\nChatGPT\nGPT-4\n(c)\n1\n2\n3\n4\n5\n6\nCorrrectness\n0\n1\n2\n3\n4\n5\n6\nHelpfulness\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nStopping Ratio\n(d)\nFigure 2: (a) Post-interactive ranks across models about which model the participant preferred as a mathematical\nassistant (lower rank is better). Ties were allowed and are included: participants were permitted to assign the\nsame rank to multiple models (see SI Additional Survey Observations). (b) Mathematical correctness and perceived\nhelpfulness scores (all scores are an integer \u2208 {0, 1, ..., 6}; higher is better) received for each model. Full details about\nthe text associated with the scales of each score are included in the SI Additional Survey Details. (c) Comparing\nparticipants\u2019 scores of the mathematical correctness against perceived helpfulness of each models\u2019 generations. Each\ndot is a score for a single human-model interaction. We add slight jitter for visual ease given that points overlap.\nInterestingly, we observe cases where the perceived helpfulness and correctness of a generation diverge, i.e., particular\ninstances can be deemed incorrect yet somewhat helpful, or correct, but somewhat unhelpful. (d) The relationship\nbetween correctness and helpfulness scores and whether the step is terminal (i.e., the step after which the participant\nstopped interacting for a particular problem). The size of the bubbles indicates the number of that particular score\npair (correctness, helpfulness). For a fixed score pair, the opacity indicates the ratio of stopping steps, i.e., the number\nof terminal steps divided by the number of total steps.\n2.3\nInvestigations into the MathConverse Annotated Taxonomy\nWe build an annotated taxonomy by coding each user query; details are included in the Methods below, as\nwell as in SI Additional Details on Taxonomy Construction. The taxonomy enables us, and other researchers,\nto understand the kinds of queries users make. We ask a few questions here: (1) how do the queries made in\nthe first interaction compare to those in the second; (2) is there a relationship between the kinds of queries\nmade by participants who had extensive prior experience interacting with AI systems versus those who did\nnot; (3) and is there a relationship between particular interaction types and the scores assigned to the models\u2019\ncorresponding responses?\nWe address questions (1) and (2) by investigating \u201cquery profiles\u201d \u2013 a \u201csignature\u201d of the kinds of queries made\nby a user (or subset of users) over a particular interaction duration. We compare query profiles for the kinds\nof queries participants made in the first interaction versus the second in Figure 3a to address question (1).\nWe observe a marked difference across query profiles between the first and the second interaction, indicating\nthe importance of studying AI systems behaviour through interactions; query patterns can evolve in time.\n8\nAsk defn\nCopy-paste\norig question\nGeneral maths\nquestion\nAsk how\n(step of proof)\nAsk clarify\nNon-maths\nrelated\nAsk for\ndemonstration\nAsk about\ngenerality\nCorrect AI\noutput\nAsk why\nOther\nQuery\n0\n5\n10\n15\n20\n25\n30\nPercent\nInteraction Step\n0\n1\n(a)\nAsk defn\nCopy-paste\norig question\nGeneral maths\nquestion\nAsk how\n(step of proof)\nAsk clarify\nNon-maths\nrelated\nAsk for\ndemonstration\nAsk about\ngenerality\nCorrect AI\noutput\nAsk why\nOther\nQuery\n0\n10\n20\n30\n40\n50\nPercent\nAmount of Previous AI Interaction\nExtensive/Regular\nMinimal/None\n(b)\nFigure 3: (a) Query profiles as a function of the interaction step. Users prefer to ask for definitions or general\nmathematics questions, and to paste in the full text, in the first interaction, compared to correcting the model\u2019s\noutput, asking why, etc. in the second interaction. Interaction step 0 is the initial interaction; step 1 is the query made\nafter receiving the first AI response back to the query made in step 0. (b) Query profiles \u2013 for the first interaction\nstep (i.e., step 0) \u2013 as function of the amount of experience the user has with AI systems prior to participating.\nTo address question (2), we notice in Figure 3b that users who had minimal prior AI expertise (i.e., responded\nthat they either had never interacted with an AI system, or had only done so rarely) were more likely to\nsimply paste in the full prompt text, in contrast to those with more experience with AI systems who more\nregularly asked for decomposed tasks (e.g., asking for a definition or help on a single step of the proof). These\ndifferences in behaviour hint at the potential importance of improving general AI literacy across users about\nthe kinds of prompting behaviour that induces desirable responses.\nWe explore question (3) in SI Additional Investigations in the MathConverse Taxonomy and find that particular\nqueries are associated with differential correctness and helpfulness ratings, hinting at the importance. Further\nsystematic study of particular query types and model performance for queries involving human corrections\nremain future work.\n9\n2.4\nQualitative Observations from Case Studies\nWe next include takeaways provided by each expert in relation to their freeform interactions. We then\nsynthesise the core insights across the case studies.\n2.5\nPer-Expert Conclusions\nEach domain expert provided takeaway remarks following their interactions with GPT-4.\nDr. William Hart\nWhilst GPT-4 is able to regurgitate some very commonly found elementary number theoretical material and\ncan handle straightforward problems, it has a major difficulty with algebraic manipulation and little or no\nability to work on unseen problems that require backtracking, proving intermediate lemmas or extensive\nplanning.\nThis is clear when GPT-4 is asked to show that\n3 =\nr\n1 + 2\nq\n1 + 3\n\u221a\n1 + \u00b7 \u00b7 \u00b7.\n(SI ProofWiki prob. 28)\nA consistent problem here is an inability to write down a correct expression for a recursive relation to describe\nthe nested radical. GPT-4 seems to be convinced that the expression under each square root is the same, so\nthat if we write the initial expression 3 =\n\u221a\nA then we also have 3 =\np\n1 + 2\n\u221a\nA and 3 =\nq\n1 + 2\np\n1 + 3\n\u221a\nA,\netc. To probe a little deeper, GPT-4 was instead prompted in a direction that might allow it to make partial\nprogress. The hint was given to try peeling the expression on the right hand side one square root at a time,\nworking backwards from the desired result that the full nested radical should have the value 3 to see if some\npattern could be found in the values of the inner nested radicals. It was easy to prompt it so that it heads in\nthat direction but on every generation it made hopeless algebraic and numerical errors, once again illustrating\nthat very often what holds it back is high school algebra rather than the depth of the mathematics.\nDr. Wenda Li\nWe found GPT-4\u2019s performance at variations of several ProofWiki problems quite satisfactory: it can reliably\nretrieve definitions of concepts used in the problem as well as in its own proof; it can correctly assess whether\nloosening certain assumptions breaks the proof; it can also instantiate variables quite robustly, given the\nopportunity of inspection of its own answers. There have been debates (Bender et al., 2021b; Piantadosi\nand Hill, 2022) about to what extent, shall we say, language models \u201cunderstand\u201d, given the nature of their\nstochastic generation. In our study, we find a couple of simple5 cases where the language-model-based\nassistant possesses the mathematical understanding of assumptions and variable instantiations beyond mere\nmemorisation.\nFor instance, we asked GPT-4 to solve the following standard probability theory problem: Let X be a random\nvariable. Assume E(X) = \u00b5 for some \u00b5 \u2208 R and var(X) = \u03c32 for some \u03c32 \u2208 R>0. Show that for all k > 0:\nPr (|X \u2212 \u00b5| \u2265 k\u03c3) \u2264 1\nk2 . GPT-4 started by stating that we can use Chebyshev\u2019s inequality, and then re-stated\nthe problem in an almost identical way but with different variable names. This demonstrates a certain level\nof variable unification. We then checked whether the assistant knew how to instantiate variables by asking it\nwhether the proof still holds when the following concrete values are assigned to k: 2,\n\u221a\n2,\n\u221a\n2 \u2212 1,\n\u221a\n2 \u2212 2, and\n(\n\u221a\n2 \u2212 2)2. Human inspection finds the assistant\u2019s behaviour to be correct. The assistant can handle concrete\ncalculations even when k is a relatively complicated number (e.g.,\n\u221a\n2 \u2212 1). The model also knows that the\nprevious derivation cannot be carried out when k =\n\u221a\n2 \u2212 2, a negative number.\n5Note, \u201csimple\u201d here is relative to that of a trained mathematician.\n10\nProfessor Timothy Gowers\nMany of the strengths and weaknesses we observed in GPT-4 are ones that have been commented on several\ntimes (in connection with other LLMs as well). For instance, it is not good at calculation, it has a tendency\nto ignore facts that do not support its main conclusion (even if it itself has generated those facts), and to\ninvent facts that do support it (Azamfirei et al., 2023).\nWhen it comes to building examples, it has another weakness, which is that instead of using a process of\nreasoning to constrain what the example can look like and only then exhibiting the example, it prefers to\nstart by exhibiting the example and then provide the justification that it has the desired properties. If its\ninitial suggestion is correct, then this may be all right (though its justifications are not always correct),\nbut often the example it gives is not correct, and it typically follows it up with a \u201cjustification\u201d that to a\ntrained mathematician has very obvious flaws. This behaviour supports the stochastic parrot view, since\nmathematical constructions are typically presented in the literature in the same unhelpful style \u2013 first the\nexample, and then the verification that it works.\nOne can try to use prompt engineering to discourage GPT-4 from organising its answers in this way, for\nexample, asking the model not to provide an example immediately but to describe a general strategy first,\nand only then the implementation of the strategy, or to pretend to be a student and ask it to offer hints.\nWhile these did improve its output somewhat, they did not make a significant enough difference to affect our\ngeneral conclusions, so we mainly used more straightforward prompts.\nA further feature that has a negative effect on the experience of interacting with GPT-4 is that even when it\ngives correct answers, it often describes them and checks their properties in a laborious case-by-case way,\nand often those checks are not even necessary. For instance, if asked to construct a 3-regular graph with\neight vertices, it will typically list all the vertices and edges, and then for each vertex it will say what its\nneighbours are and comment that there are three of them. Or if it needs the fact that the matrix\n\u0012\n0\n1\n0\n0\n\u0013\nsquares to the zero matrix, instead of just asserting that it does, it will write\n\u00120\n1\n0\n0\n\u00132\n=\n\u00120\n1\n0\n0\n\u0013 \u00120\n1\n0\n0\n\u0013\n=\n\u00120\n0\n0\n0\n\u0013\n,\nwhich is adding virtually nothing to a bald assertion, since no details are given of the computation of the\nmatrix product (not that one would want them). Similarly, it sometimes \u201cverifies\u201d that a matrix is symmetric\nby writing out that matrix and its transpose and noting that the two matrices it has written out are identical.\nIn the more positive direction, there were several questions that GPT-4 answered well in a way that is hard to\ndismiss as mere parroting. To give a simple example, if it is asked to differentiate a function, it will typically\ndo so very competently, suggesting at least some ability to generalise. (A possible explanation for why it is\ngood at differentiation and bad at arithmetic is that there are many more worked examples of differentiation,\ncomplete with step-by-step explanations, than there are worked examples of arithmetic.) More examples are\ndiscussed in SI Interactive Case Studies with Experts, with some speculations about why GPT-4 is good at\nthem.\n2.6\nKey Findings\nWe now synthesise the key findings across our two evaluations: insights drawn from our MathConverse\ndataset collected by applying CheckMate in the domain of mathematics, and expert case studies, where\nmathematicians deeply engage with GPT-4 to solve problems from particular angles.\n[Key finding 1] Correctness and helpfulness of model responses are related, but can diverge in\ninteresting ways\nWhen a human poses a query, what is the relationship between the perceived helpfulness and correctness of\na model\u2019s answer? We find that, across all human-model interactions, helpfulness and correctness ratings\nare highly correlated (with Pearson correlation coefficient r = 0.83). This finding corroborates a similar\n11\nobservation in (Welleck et al., 2022a), wherein although correctness lags behind perceived usefulness, for\nboth per-step and fully-generated proofs, the two qualities are intimately related. This trend underscores an\nimportant point: for mathematical language models to be useful assistants, a core quality is that they should\nconsistently produce largely mathematically correct responses. We also observe an interesting phenomenon\nat the extremes (see Figure 2c): there are cases where generations are considered incorrect but helpful, or\ncorrect but unhelpful (see Examples 4 and 3 respectively in SI Investigating the Boundary between Easy\nand Hard Problems). For instance, models can generate overly verbose answers that are deemed entirely\nmathematically correct and only moderately helpful. In expert case study Problem Perturbation to Probe\nMemorisation in SI Interactive Case Studies with Experts, we see models can be good at providing definitions\nand interestingly can produce helpful scaffolding for a problem (e.g., the right strategy or first few steps),\neven if details of the full proof are incorrect. These instances reveal that to create useful assistants, increasing\nthe mathematical correctness of these models alone is not sufficient.\n[Key finding 2] Lack of verification can induce errors\nThe CheckMate ratings are determined from a first-person perspective: participants rate the generations they\nreceive. But what if a participant cannot verify mathematical correctness? Before interacting with models,\nparticipants are asked to indicate their confidence at solving the problem on their own. We find instances\nwhere participants who indicated low confidence (i.e., confidence <= 3; see rating scale in SI Additional\nDetails on Survey Set-Up) in being able to solve the problem on their own ended up rating the generation\nas completely correct even when it was not. For such examples, as well as participant testimonials of this\nbehaviour, see SI Additional Survey Observations. In case studies, even if the model does produce a correct\nsolution, this solution is not always arrived at in a seemingly \u201chuman-like\u201d way; for instance, the model may\nfollow a guess-and-check approach rather than forward planning (e.g., Examples 1, 4, and 7 in SI Interactive\nCase Studies with Experts). However, guess-and-check cannot work well if one cannot \u201ccheck\u201d solutions;\nindeed, we see that in general, challenges with algebraic manipulation plague in- and out-of-distribution\nperformance (e.g., examples in SI Interactive Case Studies with Expert, Number Theory Evaluation). Further,\nour studies highlight issues in interaction even when the human user attempts to correct the model. One\ncase study illuminated intriguing behaviour when the model was queried about uncertainty: the model began\nto apologise despite having been correct (see SI Interactive Case Studies with Expert, Problem Perturbation\nto Probe Memorisation).\n[Key finding 3] The double-edged sword of reliance on memorised solutions\nMemorisation of important concepts and definitions is an important aspect of acquiring world knowledge.\nThe MathConverse taxonomy revealed that queries about mathematical definitions are frequently made by\nhumans, and responses received are considered one of the most helpful among all the categories. In contrast\nto definitions, solutions to specific problems should ideally be understood in a fashion that can generalise\nand not be completely memorised. By probing GPT-4 capabilities on slightly novel problems or those which\ninvolve building examples, we notice the model\u2019s tendency to over-rely on plausibly memorised examples\nor patterns. We caution that we cannot be definitely sure whether these examples are indeed \u201cmemorised\u201c\nwithout direct access to the models\u2019 training data. However, from the behaviour, we have a strong suspicion\nthis is the case.\n3\nDiscussion\nWe compile key observations from both our structured and instance-based evaluations into a series of actionable\ntakeaways, which \u2013 given the interdisciplinary nature of our study \u2013 we hope will appeal to a wide audience.\nWe tailor these takeaways to audiences from different fields. To offer balance, we first note that the best\nLLMs we investigate do demonstrate some non-trivial ability in collaborating helpfully and correctly with\nusers on undergraduate-level mathematical problems (see Figure 2b). Should the user be able to assess the\nvalidity of LLM-generated responses, they can meaningfully assist on some problems. Even if the answers are\n12\nmemorised and can be found somewhere on the internet, LLMs have the advantage of being flexible in their\ninputs and outputs over traditional search engines. We then close with limitations of our methodology.\n3.1\nTakeaways for ML Developers\nEnable Models to Communicate Calibrated Uncertainty and Uptake Corrections\nWe observe\ncases where people attempted to correct the model when it made an error, the model apologised, and\nproceeded to give an answer without the necessary corrections or asking for clarification The pattern often\nrepeated itself until the user seemed to get bored and abort. To improve user experience, systems that can\nadequately respond to user corrections, for example, through uncertainty calibration (Hullman et al., 2018;\nLiu et al., 2016; Vasconcelos et al., 2023), are compelling (Aky\u00fcrek et al., 2023; Kocielnik et al., 2019; Meng\net al., 2022; Mitchell et al., 2022; Wilder et al., 2021b). Indeed, in the models we explored, it was not clear\nwhen the model was unsure. We include a discussion with participants about these challenges in a post-survey\nquestionnaire (see SI Post-Survey Testimonials from Participants). Communicating uncertainty is critical to\nensure users know when they can trust the model output (Bhatt et al., 2021; Hullman et al., 2018) and help\ncalibrate appropriate levels of trust (Spiegelhalter, 2017; Zerilli et al., 2022). However, obtaining accurate,\ncalibrated uncertainty estimates from LLMs can be a difficult endeavour (Si et al., 2022; Xiao et al., 2022).\nEnable Provision of Rationales\nSeveral participants in MathConverse asked \u201cwhy\u201d a model undertook\na particular proof step. Expanding on the justification for a choice could be a valuable educational tool.\nGenerating compelling explanations, on-the-fly and on-request \u2013 provided those explanations are indeed\nrepresentative and not misleading (Bhatt et al., 2020; K\u0131c\u0131man et al., 2023; Sevastjanova and El-Assady,\n2022; Wu et al., 2023; Zelikman et al., 2022b) \u2013 seem promising and desirable to explore to further boost the\nutility of these systems in partnership with mathematicians.\nStrive for Conciseness\nBoth our survey and our expert case studies find that \u2013 while mathematical\ncorrectness appears to often be a foundation for useful assistance in higher-level mathematics \u2013 it is not\nalways sufficient. Responses that were overly verbose were sometimes deemed less helpful. Designing systems\nthat generate concise responses to mathematical queries seems a promising future direction, best also coupled\nwith the capability of showing its \u201cwork\u201d if needed (related to rationales, see above). The applicability of this\nto other domains than mathematics remains to be investigated: it may be that responses of different degrees\nof verbosity are preferred in different domains (e.g., in medicine, longer responses laden with empathy may\nbe preferable (Ayers et al., 2023)).\n3.2\nTakeaways for Mathematicians (Students, Educators, and Researchers)\nPay attention!\nLarge language models are capable of generating remarkably compelling natural language \u2013\nan incredible technical feat which ought not to be dismissed and can be helpful as we see in both our studies.\nHowever, such prowess belies the potential for coaxing the reader into not recognising errors. Be careful\nnot to fall into the trap of lazy checking (see SI Post-Survey Testimonials from Participants in participant\ntestimonials). This is worth keeping in mind for users learning from or evaluating the generations of LLMs,\nfor example, students and assignment markers. It is worth being cognisant of the risk of automation bias,\nthat is, where a user may inappropriately over-rely on the output of a model simply because it came from a\nmodel (Cummings, 2004).\nTake a Nuanced View of When These Models Can Help\nReinforcing similar findings from (Frieder\net al., 2023), we observe in this work that LLMs can be useful for retrieving definitions (see SI Additional\nTaxonomy Observations) and can occasionally provide a valuable scaffolding for how to approach a problem\n(see SI Additional Survey Observations and SI Interactive Case Studies with Experts). It is important not\nto presume that a model which performs well in one realm of the task space will surely perform well in\nanother (Bhatt et al., 2023; Kelly et al., 2023). Counterintuitively \u2013 a la Moravec\u2019s Paradox (Moravec, 1988)\n13\n\u2013 it is possible that models will succeed at tasks perceived challenging by humans, but fail at tasks humans\nconsider easy (e.g., derivation versus algebraic manipulation). Mathematicians can take advantage of our\nquery profiles to learn how experienced peers adapt their behaviours to get more out of language models for\nmathematics.\nBe Cautious When Using Current LLMs (Alone) for Heavy Algebra\nIn particular, our studies fur-\nther underscore the challenges of present models at algebraic manipulation, corroborating prior work (Bubeck\net al., 2023; Dziri et al., 2023; Frieder et al., 2023). We believe it is therefore important that mathematicians\ntake care if using these systems for tasks which involve substantial algebra. We do not explore plug-ins (Ope-\nnAI, 2023a) in this paper, nor alternative hybrid neuro-symbolic approaches (e.g., (Gowers, 2022; Jiang et al.,\n2022b; Kazemi et al., 2022; Li et al., 2022; Poesia and Goodman, 2022; Wong et al., 2023)), which may prove\na useful salve for some of this failure mode.\n3.3\nTakeaways for LLM Development, Evaluation, and Deployment\nWe conclude with broad takeaways for anyone developing, evaluating or considering deploying LLMs in\npractice.\nCarefully Discern When Assistance is Needed (or Even Worth Utilising)\nTo build complementary\nsystems (Wilder et al., 2021a), understanding when an AI-based assistant is helpful is of utmost importance:\nseldom will such an assistant be helpful in all settings (Bhatt et al., 2023). An important question will be in\nwhich settings such an assistant can be useful without undermining the agency of the mathematician, for\nexample, of the kind already being proposed when considering using LLMs in coursework (Ba and Wang, 2023).\nFuture work would benefit from considering how to build usable assistants that optimise for complementarity,\nproviding support as and when needed (Miller, 2023).\nCollaboration between ML Practitioners and Domain Experts is Valuable\nConducting investiga-\ntions in partnership with domain experts can be especially fruitful for characterising model behaviour (Davies\net al., 2021; McGrath et al., 2022; Mirowski et al., 2023), particularly by designing entirely new tasks, as our\nexpert case studies demonstrate. We encourage forming such interdisciplinary partnerships in and beyond\nmathematics.\nIncorporate Interactivity into LLM Capability Assessments\nTo truly comprehend the landscape of\nan LLM\u2019s capabilities, we believe it is paramount to incorporate interactive evaluations. Our work further\ndrives home the importance of interactive evaluation as a way to gain deeper insights into the strengths and\nweaknesses of these models and probe characteristics which may be preferable for assistive settings. However,\nas we highlight here, the interactive study of LLMs not only serves to characterise model behaviour, but it\nidentifies ways in which humans may themselves choose to interact with these models and actually use these\nsystems (Ringer et al., 2020). A wave of works increasingly illuminates the sensitivity of these models to the\nchoice of prompts (Wei et al., 2022; Yao et al., 2023; Zhou et al., 2023). As such, it is important to consider\nthe form and content of queries that humans may use to interact with these systems both to design systems\nmore adapted to particular user queries, and to inform users of best practices. It may be valuable for system\nmaintainers to recognise whether or not users are leveraging these tactics, in order to help better inform the\ntechniques for boosting the quality of the response for their query.\nWe hope to see more works like ours and (Lee et al., 2022a,c; Mirowski et al., 2023) that study LLMs in the\ncontext of human-computer interactions. CheckMate offers a place to start, potentially complemented by\nfree-form evaluation of the kind we conduct in our expert case studies.\n14\n4\nLimitations\nWhile our study reveals insights into how mathematicians may use language models \u2013 and opens doors for\nfuture interactive evaluation \u2013 our survey is simply an initial step in evaluating LLMs for mathematical\nassistance. Our sample size is small but informative; we consider MathConverse to be a preliminary dataset\nto spark further methodological and deployment-time considerations. Additionally, we ask each participant to\nrate generations provided during their own interaction trace; while this permits first-person evaluation of the\nkind called for in (Lee et al., 2022b), for those who do not already know how to solve the problem this means\nthat they may be falsely judging the correctness of the generation. A sensible next step would be two-fold:\ndeploying our evaluation platform with students who have not already solved such problems, and sending the\ninteraction traces off for external evaluation as well. We also encourage a reassessment of mathematician\ninteractions over time; it is quite possible \u2013 in fact likely \u2013 that the kinds of interactions humans make with\nthese systems will evolve as their capabilities grow. Additionally, our taxonomy categories are non-exhaustive;\nalternative categorisations are possible. Nevertheless, we found our categorisation sufficient to draw helpful\nfindings.\nAnd while our case studies offer invaluable insight into the prompting behaviour of expert mathematicians\nand further characterisation of model performance, each individual may bring to bear their own expectations\nabout models\u2019 strengths and weaknesses, which could seep into the way probing behaviour is chosen and\nmodel outputs are interpreted. We emphasise that, as with the survey results, these insights ought not to\nbe taken as a firm testament about the capabilities nor potential of these models, much less all language\nmodels (we only consider a handful, and all from the OpenAI family). Rather, we hope the evaluation\ntoolkit expanded on in our studies pave the way for further research into the use of LLMs as assistants for\nproblem-solving, in mathematics and beyond.\n5\nConclusion\nAs LLMs are increasingly deployed in human-facing settings where they may serve as assistants, it is paramount\nthat evaluation of their efficacy fundamentally includes evaluation in an interactive context (Lee et al., 2022b).\nAs we demonstrate, these interactive evaluations can be structured (e.g., leveraging CheckMate) or free-form\n(e.g., through sourced domain expert or target user interactions). LLMs, and foundation models broadly,\nare complex and often surprising in their behaviour; so are humans. Hence characterising potential failure\nmodes in LLM and human interactions necessitates a multi-factorial evaluation approach, which includes\nboth interactive evaluation and classical, static-snapshot evaluation (Burnell et al., 2023). Through our study,\nwe extract insights which we hope can inform careful design and deployment when considering leveraging\nLLM-based mathematics assistants and reasoning engines. We believe that our study paves the way for\nfurther evaluation of the use of foundation models in mathematics and other domains, particularly through\ncloser collaboration with domain experts.\n6\nMethods\n6.1\nCheckMate: Adaptable Platform for Interactive Evaluation\nWe introduce CheckMate as an adaptable platform to support interactive evaluation of language models.6\nHumans can interact with and rate text generated by language models, and CheckMate records the \u201cinteraction\ntraces\u201d.7\nWe design CheckMate to support two flavours of evaluation: studying the interactions with a single model,\nand studying preference across a bank of models. First, we introduce the rating scheme for a single model.\nThen, we discuss how we support comparative evaluation over a suite of models. We focus on the domain of\n6Base code can be found in our repository: https://github.com/collinskatie/checkmate.\n7We borrow the terminology of (Lee et al., 2022a,c) for \u201cinteraction traces\u201d.\n15\nmathematical theorem proving; however, CheckMate can be extended more broadly (see SI User Guide for\nCheckMate ).\nEvaluation for a Single Model\nEvaluation begins with permitting the participant to freely interact with the model in order to solve a\nproblem. We encourage participants to imagine they are trying to solve the problem \u2013 and elicit assistance.\nThe participant can continue to explore assistance for up to 20 interaction exchanges.8 When the participant\nis satisfied with the level of assistance (or sufficiently unsatisfied that they wish to terminate the interaction),\nthey proceed to evaluate each step of their entire interaction.\nWe design CheckMate to support a multi-dimensional evaluation over the interaction trace for the successive\nhuman query-model generation pairs. At present, the platform is designed with a mix of Likert scales\nand radio buttons (see SI Additional Details on Survey Set-Up and User Guide for CheckMate ). However,\nCheckMate can be readily extended with alternative rating types, for instance, to handle individual error\nprofiling (Welleck et al., 2022a) or additional interaction metrics as proposed in (Lee et al., 2022c; Shen and\nWu, 2023), if desired.\nComparative Evaluation Across Models\nWith an ever-growing suite of language models available for humans to leverage, it is important to compare\ncapabilities \u2013 and how these compare to previous versions. When done, such comparisons typically involve\nsingle snapshots. CheckMate permits the study of preference over the interaction trace and can serve as a\nvaluable tool to explore the evolution of assistance potential.\nIn CheckMate, participants provide a rank order over which model they preferred interacting with, after they\nhave interacted with two or more models. This instantiation of the platform is set-up such that participants\ninteract with a different task per model (to avoid \u201cbleed over\u201d effects when considering the same problem\nmultiple times). However, alternative designs, for example, rating models per task, or subsampling the models\nto evaluate, are possible adaptations to our paradigm (see SI User Guide for CheckMate ). Importantly,\nparticipants are blind to which model they are evaluating at any time; this ensures they are not biased by\npreconceived notions of which model may be more performative.\nIn the rank order, participants can assign the same rank if they are unsure which model they prefer. Future\nwork could consider more expansive comparative preference evaluation. We provide further details on\nCheckMate and hosting our survey in the SI User Guide for CheckMate.\nInstantiating CheckMate for Mathematics to Collect MathConverse\nRecall, survey participants are asked to prove a mathematical statement and to use an AI system to assist\nthem in any way to carry out this task. As the interaction is free-form, interactions can range from asking for\nhelp on the entire problem, to clarifying definitions, or asking for an explanation for a particular generated\nproof step. Participants are not provided with possible interaction behaviours in advance to avoid priming.\nWhen the participant is satisfied with the level of assistance (or sufficiently unsatisfied that they wish to\nterminate the interaction), they proceed to evaluate each step of their entire interaction. Participants solve a\ndifferent problem for three models (Instruct-GPT, ChatGPT and GPT-4), where the order of the models is\nshuffled and participants are blind to which model they are interacting with.\nWe next describe our task set-up over which we conduct evaluations. The study was conducted under the\napproval of the University of Cambridge Computer Science Ethics Division. Example interface screens of\nCheckMate for mathematics are included in the the Supplement.\n8We chose a limit of 20 expecting that participants may fatigue beyond that point and to guard against the possibility that a\nparticipant could try to interact unfettered with the model for an extended period of time.\n16\nTasks\nWe select 54 problems from ProofWiki, a corpus of undergraduate-level mathematics problems.9 Nine problems\nare selected from each of six mathematics topics (linear algebra, number theory, probability theory, algebra,\ntopology, and group theory). We select these topics to span a range of subject areas in typical undergraduate\nmathematical curricula. The problems can be found at https://github.com/collinskatie/checkmate/\ntree/main/data/problems.\nRating Scales\nParticipants evaluate the perceived helpfulness and mathematical correctness of each step, selecting one\n\u201cpreference\u201d and one \u201cquality\u201d metric, as defined in (Lee et al., 2022c). Cognitive load and biases are kept in\nmind at each stage of the design, for example, lightening the number of ratings per page, and randomising\nmodel rating order to reduce possible ordering effects. Ratings are provided on a 7-point Likert scale, with\nthe width chosen to ameliorate potential rating collapse (i.e., the phenomenon where participants hesitate to\nuse scale endpoints (Bishop and Herron, 2015)). Further, we select only two factors per step to avoid excess\ncognitive load while rating. Before responding, participants specify their confidence in being able to solve\nthe problem on their own. After interacting with the three models on three different problems, participants\nare shown the full interaction traces with each model and (blindly) indicate their rating about which model\nthey would prefer as an assistant (blindly) via a dropdown bar. We include full details of the scales in SI\nAdditional Details on Survey Set-Up. For all quantitative analyses \u2013 with the exception of analysing users\u2019\nstopping behaviour in Figure 2d \u2013 we filter out generations rated as zero for mathematical correctness, as\nthat means that no mathematically-relevant content was included; we find that these are typically responses\nto greetings or exclamations (e.g., after the user has thanked the model \u2013 see released data).\nLanguage Model Selection and Set-Up\nParticipants evaluate three popular language models: InstructGPT (Ouyang et al., 2022), ChatGPT (OpenAI,\n2022), and GPT-4 (OpenAI, 2023b) in chat mode10. As new language models are introduced, the methodology\nof designing optimal prompts is rapidly evolving (e.g., to name a few (Wei et al., 2022; Yao et al., 2023;\nZhou et al., 2023)). Since we are studying how real domain users (i.e., mathematicians) would interact with\nthese systems in-the-wild, we keep a sparse base prompt, only asking the model to be a helpful mathematical\nassistant in the prompt. Further details for the experimental setup can be found in SI Additional Details on\nSurvey Set-Up.\nParticipants\nWe recruit mathematician volunteers to participate in our evaluation. In total, we received 25 entries\ncomprising 261 human-model interactions; while this could comprise 25 unique participants, we did not store\na unique participant identifier, for example, the IP address, for privacy reasons (see SI Additional Details on\nSurvey Set-Up), thus we cannot confirm that these are exactly 25 unique individuals. The mathematicians\nhave experience levels ranging from current undergraduate students up to expert mathematics professors; for\nparticipants without a formal mathematics degree, they likely have some exposure to high-level mathematics\n(see SI Participant Recruitment and Additional Details). Each participant chooses one of the six topics and\ncan evaluate as many questions as they like (up to the maximum of 9). Note, the range of expertise of our\nparticipants \u2013 up to world-class experts \u2013 coupled with the fact that our problems sit at a level where students\nmajoring in mathematics might find them in textbooks or as exercises, means that some participants may be\nable to solve the problems already, others may not. If a participant knows how to solve the problem, we ask\nthat they imagine what kind of assistance they would like had they been at the experience level of someone\nwho does not know how to solve the problem. More details on recruitment, data processing, and expertise\ncan be found in SI Additional Details on Survey Set-Up.\n9https://proofwiki.org\n10Our study design began in February 2023, shorty after GPT-4 was released.\n17\n6.2\nDeriving a Taxonomy from MathConverse\nWe observe a wide spectrum of mathematicians\u2019 interactions with AI assistants in MathConverse. We derive\nan initial taxonomy of these interactions and annotate each interaction according to the taxonomy. To build\nthe taxonomy, a subset of our author team manually inspected each interaction (i.e., the user\u2019s query) and\nidentified 10 broad categories in which most interactions seemed to fall. These categories are specified in the\nTaxonomising User Behaviour from MathConverse\u2019 subsection of the Results. We included an additional\n\u201cOther\u201d bucket for queries which did not fall into one of the 10; cases that fell into such a category were\ndiscussed amongst the annotators. Four authors then manually annotated each user query into these buckets.\nAnnotators were asked to mark whether an interaction fell into a bucket, with an option to specify whether\nthey were unsure. Each interaction was annotated by a single annotator; however, in the case of confusion or\ndiscrepancies, annotators discussed and came to an agreed coding. We release the annotated taxonomy in\nour repository. Full instructions given to annotators are included in the SI Additional Details on Taxonomy\nConstruction.\n6.3\nInteractive Case Studies with Experts\nWhile structured interactive evaluation permits nice quantitative findings, to deeply understand the capability\nof LLMs \u2013 in the context of mathematics and beyond \u2013 free-form interaction, like instance-level evalua-\ntion (Burnell et al., 2023) can be particularly revealing. Here we want to expands the scope of the evaluation,\nattempting to locate the boundary between problems that GPT-4 finds easy and those it finds hard. In our\nquantitative study with CheckMate we observed a close relationship between mathematical correctness and\nperceived usefulness. But since correlation is not causation, we further explore the broader mathematical\nreasoning capabilities of these models as a bedrock to inform their utility as proof assistants.\nWe reiterate that we are not aiming to single out GPT-4 for criticism. Rather, our goal is 1) to offer one of\nthe first real expert mathematician interactive case studies with LLMs to help guide the design of better\nmathematical assistants and inform their safe, trustworthy use by helping characterise their limitations, 2) to\npave the way for further interactive evaluations, and 3) to highlight patterns of human-computer interaction\nnot previously known to the community, particularly when the humans interacting are domain-leading experts.\nWe hope the work will be of interest to ML engineers and researchers, cognitive scientists, human-computer\ninteraction specialists, mathematicians, educators, and beyond.\nA complete transcript of interactions for each case study example is included in the Supplementary Materials.\nWe maintain the original text of each case study author for authenticity, with only minor edits for precision\nand coherence. For the interactions with ProofWiki problems, we also host them with a sample explorer for\nclearer visualisation.\nFirst, our recruited experts conduct a deeper dive into some of the problems we explored in our previous\nevaluation. Specifically, we use the problems as a playground to explore how much the model seems to \u201cknow\u201d\nabout relevant concepts and further characterise what interactions can yield better (or worse) performance\nand assistance experience. We focus on GPT-4 (in chat mode) because it showed the strongest overall\nperformance in our quantitative study with CheckMate. The first case study is provided by Dr. William Hart,\na number theorist by training; the second is primarily contributed by Dr. Wenda Li, a formal mathematics\nexpert, and the third one was conducted with Prof. Timothy Gowers, a Fields medalist and Professor in\nmathematics.\nAcknowledgements\nSpecial thanks to Fabian Gloeckle, for support and informative discussions throughout the project. We\nadditionally thank in no particular order Anshula Gandhi, Jonas Bayer, Angeliki Koutsoukou-Argyraki, Fredy\nYip, Mariusz Mirek, Gabriel Poesia, Noah Goodman, Valerie Chen, Nina Grgi\u0107-Hla\u010da, Ilia Sucholutsky, Shoaib\nAhmed Siddiqui, and Ced Zhang for valuable discussions around assistive systems in AI and mathematics, and\nLLM evaluation. KMC gratefully acknowledges support from the Marshall Commission and the Cambridge\n18\nTrust. AQJ acknowledges support from the Peterhouse Graduate Studentship with gratitude. SF and\nTL acknowledge support from the Alan Turing Institute under the EPSRC grant EP/N510129/1. MZ\nacknowledges support from the Leverhulme Trust grant ECF-2021-429. UB acknowledges support from\nDeepMind and the Leverhulme Trust via the Leverhulme Centre for the Future of Intelligence (CFI), and from\nELSA: European Lighthouse on Secure and Safe AI via the Alan Turing Institute. TL acknowledges support\nfrom the AXA Research Fund and the EU TAILOR grant 952215. WL is supported by the ERC Advanced\nGrant ALEXANDRIA (Project GA 742178). AW acknowledges support from a Turing AI Fellowship under\ngrant EP/V025279/1, The Alan Turing Institute, and the Leverhulme Trust via CFI. MJ acknowledges\nsupport from the EPSRC under grant EP/T019603/1.\nReferences\nA. Abid, A. Abdalla, A. Abid, D. Khan, A. Alfozan, and J. Zou. Gradio: Hassle-Free Sharing and Testing of\nML Models in the Wild. arXiv preprint arXiv:1906.02569, 2019.\nJ. S. Aitken, P. Gray, T. Melham, and M. Thomas. Interactive theorem proving: An empirical study of user\nactivity. Journal of Symbolic Computation, 25(2):263\u2013284, 1998.\nA. F. Aky\u00fcrek, E. Aky\u00fcrek, A. Madaan, A. Kalyan, P. Clark, D. Wijaya, and N. Tandon. RL4F: Generating\nNatural Language Feedback with Reinforcement Learning for Repairing Model Outputs. arXiv preprint\narXiv:2305.08844, 2023.\nA. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi. Mathqa: Towards inter-\npretable math word problem solving with operation-based formalisms. In J. Burstein, C. Doran, and\nT. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2357\u20132367. Association for Computational\nLinguistics, 2019. doi: 10.18653/v1/n19-1245. URL https://doi.org/10.18653/v1/n19-1245.\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen,\nE. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira, M. Omernick,\nK. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin, P. Barham,\nJ. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquette-Choo,\nA. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. D\u00edaz, N. Du, E. Dyer, V. Feinberg,\nF. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi,\nL. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy,\nM. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin,\nZ. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni,\nA. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley,\nA. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine,\nD. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu,\nY. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu.\nPaLM 2 Technical Report, 2023.\nJ. W. Ayers, A. Poliak, M. Dredze, E. C. Leas, Z. Zhu, J. B. Kelley, D. J. Faix, A. M. Goodman, C. A. Longhurst,\nM. Hogarth, and D. M. Smith. Comparing Physician and Artificial Intelligence Chatbot Responses to Patient\nQuestions Posted to a Public Social Media Forum. JAMA Internal Medicine, 04 2023. ISSN 2168-6106.\ndoi: 10.1001/jamainternmed.2023.1838. URL https://doi.org/10.1001/jamainternmed.2023.1838.\nR. Azamfirei, S. R. Kudchadkar, and J. Fackler. Large language models and the perils of their hallucinations.\nCritical Care, 27(1):1\u20132, 2023.\nJ. Ba and B. Wang. Csc413/2516 winter 2023 university of toronto, assignment 1, 2023. URL https:\n//uoft-csc413.github.io/2023/assets/assignments/a1.pdf.\n19\nB. Beckert, S. Grebing, and F. B\u00f6hl. A usability evaluation of interactive theorem provers using focus groups.\nIn Software Engineering and Formal Methods: SEFM 2014 Collocated Workshops: HOFM, SAFOME,\nOpenCert, MoKMaSD, WS-FMDS, Grenoble, France, September 1-2, 2014, Revised Selected Papers 12,\npages 3\u201319. Springer, 2015.\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can\nlanguage models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and\ntransparency, pages 610\u2013623, 2021a.\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can\nlanguage models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability,\nand Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA, 2021b. Association for Computing\nMachinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922.\nU. Bhatt, A. Xiang, S. Sharma, A. Weller, A. Taly, Y. Jia, J. Ghosh, R. Puri, J. M. Moura, and P. Eck-\nersley. Explainable machine learning in deployment. In Proceedings of the 2020 conference on fairness,\naccountability, and transparency, pages 648\u2013657, 2020.\nU. Bhatt, J. Antor\u00e1n, Y. Zhang, Q. V. Liao, P. Sattigeri, R. Fogliato, G. Melan\u00e7on, R. Krishnan, J. Stanley,\nO. Tickoo, et al. Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty.\nIn Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 401\u2013413, 2021.\nU. Bhatt, V. Chen, K. M. Collins, P. Kamalaruban, E. Kallina, A. Weller, and A. Talwalkar. Learning\npersonalized decision support policies. arXiv e-prints, pages arXiv\u20132304, 2023.\nP. A. Bishop and R. L. Herron. Use and misuse of the likert item responses and other ordinal measures.\nInternational journal of exercise science, 8(3):297, 2015.\nW. W. Bledsoe. Non-resolution theorem proving. Artif. Intell., 9:1\u201335, 1977.\nR. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg,\nA. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258, 2021.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877\u20131901, 2020.\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,\nS. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general intelligence:\nEarly experiments with gpt-4, 2023.\nA. Bundy. The Computer Modelling of Mathematical Reasoning. Academic Press, 1983.\nA. Bundy. The use of explicit plans to guide inductive proofs. In CADE, 1988.\nA. Bundy, A. Stevens, F. V. Harmelen, A. Ireland, and A. Smaill. Rippling: A heuristic for guiding inductive\nproofs. Artif. Intell., 62:185\u2013253, 1993.\nR. Burnell, W. Schellaert, J. Burden, T. D. Ullman, F. Martinez-Plumed, J. B. Tenenbaum, D. Rutar, L. G.\nCheke, J. Sohl-Dickstein, M. Mitchell, D. Kiela, M. Shanahan, E. M. Voorhees, A. G. Cohn, J. Z. Leibo,\nand J. Hernandez-Orallo. Rethink reporting of evaluation results in ai. Science, 380(6641):136\u2013138, 2023.\ndoi: 10.1126/science.adf6369. URL https://www.science.org/doi/abs/10.1126/science.adf6369.\nS. Casper, X. Davies, C. Shi, T. Krendl Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner,\nP. Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback.\narXiv e-prints, pages arXiv\u20132307, 2023.\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from\nhuman preferences. Advances in neural information processing systems, 30, 2017.\n20\nE. Clark, A. S. Ross, C. Tan, Y. Ji, and N. A. Smith. Creative writing with a machine in the loop: Case\nstudies on slogans and stories. In Proceedings of IUI, 2018.\nK. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to\nsolve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.\nA. G. Cohn and J. Hernandez-Orallo. Dialectical language model evaluation: An initial appraisal of the\ncommonsense spatial reasoning abilities of llms. arXiv preprint arXiv:2304.11164, 2023.\nM. L. Cummings. Automation bias in intelligent time critical decision support systems. In AIAA 1st Intelligent\nSystems Technical Conference, 2004. URL https://api.semanticscholar.org/CorpusID:10328335.\nA. Davies, P. Veli\u010dkovi\u0107, L. Buesing, S. Blackwell, D. Zheng, N. Toma\u0161ev, R. Tanburn, P. Battaglia, C. Blundell,\nA. Juh\u00e1sz, et al. Advancing mathematics by guiding human intuition with ai. Nature, 600(7887):70\u201374,\n2021.\nM. Davis, G. Logemann, and D. Loveland. A machine program for theorem-proving. Communications of the\nACM, 5(7):394\u2013397, 1962.\nL. M. de Moura and N. S. Bj\u00f8rner. Z3: An efficient smt solver. In International Conference on Tools and\nAlgorithms for Construction and Analysis of Systems, 2008.\nD. Delahaye. A tactic language for the system coq. In M. Parigot and A. Voronkov, editors, Logic for\nProgramming and Automated Reasoning, 7th International Conference, LPAR 2000, Reunion Island, France,\nNovember 11-12, 2000, Proceedings, volume 1955 of Lecture Notes in Computer Science, pages 85\u201395.\nSpringer, 2000. doi: 10.1007/3-540-44404-1\\_7. URL https://doi.org/10.1007/3-540-44404-1_7.\nD. Dohan, W. Xu, A. Lewkowycz, J. Austin, D. Bieber, R. G. Lopes, Y. Wu, H. Michalewski, R. A. Saurous,\nJ. Sohl-Dickstein, et al. Language model cascades. ICML, 2022.\nN. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jian, B. Y. Lin, P. West, C. Bhagavatula, R. L. Bras, J. D. Hwang,\net al. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023.\nA. P. Felty. Implementing tactics and tacticals in a higher-order logic programming language. J. Autom.\nReason., 11(1):41\u201381, 1993. doi: 10.1007/BF00881900. URL https://doi.org/10.1007/BF00881900.\nE. First, M. N. Rabe, T. Ringer, and Y. Brun. Baldur: Whole-proof generation and repair with large\nlanguage models. CoRR, abs/2303.04910, 2023. doi: 10.48550/arXiv.2303.04910. URL https://doi.org/\n10.48550/arXiv.2303.04910.\nS. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. C. Petersen, A. Chevalier, and\nJ. Berner. Mathematical Capabilities of ChatGPT, 2023.\nM. Ganesalingam and W. T. Gowers. A fully automatic problem solver with human-style output. CoRR,\nabs/1309.4501, 2013. URL http://arxiv.org/abs/1309.4501.\nGithub. Github copilot \u00b7 your ai pair programmer, 2021. URL https://github.com/features/copilot/.\nO. Golovneva, M. Chen, S. Poff, M. Corredor, L. Zettlemoyer, M. Fazel-Zarandi, and A. Celikyilmaz.\nROSCOE: A suite of metrics for scoring step-by-step reasoning.\nCoRR, abs/2212.07919, 2022.\ndoi:\n10.48550/arXiv.2212.07919. URL https://doi.org/10.48550/arXiv.2212.07919.\nW. T. Gowers. How can it be feasible to find proofs?, 2022. URL https://drive.google.com/file/d/\n1-FFa6nMVg18m1zPtoAQrFalwpx2YaGK4/view.\nT. Hales. The flyspeck project, Aug 2014. URL https://code.google.com/archive/p/flyspeck/.\nT. C. Hales. A proof of the kepler conjecture. Annals of Mathematics, 162:1063\u20131183, 2005.\n21\nJ. M. Han, J. Rute, Y. Wu, E. W. Ayers, and S. Polu. Proof artifact co-training for theorem proving with\nlanguage models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual\nEvent, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.\nJ. Harrison, J. Urban, and F. Wiedijk. History of interactive theorem proving. In Computational Logic,\nvolume 9, pages 135\u2013214, 2014.\nHugging Face. Hugging face spaces, 2021. URL https://huggingface.co/spaces.\nJ. Hullman, X. Qiao, M. Correll, A. Kale, and M. Kay. In pursuit of error: A survey of uncertainty visualization\nevaluation. IEEE transactions on visualization and computer graphics, 25(1):903\u2013913, 2018.\nA. Q. Jiang, W. Li, S. Tworkowski, K. Czechowski, T. Odrzyg\u00f3zdz, P. Milos, Y. Wu, and M. Jamnik. Thor:\nWielding hammers to integrate language models and automated theorem provers. CoRR, abs/2205.10893,\n2022a. doi: 10.48550/arXiv.2205.10893. URL https://doi.org/10.48550/arXiv.2205.10893.\nA. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,\nsketch, and prove: Guiding formal theorem provers with informal proofs. CoRR, abs/2210.12283, 2022b.\ndoi: 10.48550/arXiv.2210.12283. URL https://doi.org/10.48550/arXiv.2210.12283.\nS. M. Kazemi, N. Kim, D. Bhatia, X. Xu, and D. Ramachandran. Lambada: Backward chaining for automated\nreasoning in natural language. arXiv preprint arXiv:2212.13894, 2022.\nM. Kelly, A. Kumar, P. Smyth, and M. Steyvers. Capturing humans\u2019 mental models of ai: An item response\ntheory approach. FAccT, 2023.\nR. Kocielnik, S. Amershi, and P. N. Bennett. Will you accept an imperfect ai? exploring designs for adjusting\nend-user expectations of ai systems. In Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems, pages 1\u201314, 2019.\nA. K\u00f6pf, Y. Kilcher, D. von R\u00fctte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc,\nO. Stanley, R. Nagyfi, S. ES, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire, C. Schuhmann, H. Nguyen,\nand A. Mattick. Openassistant conversations \u2013 democratizing large language model alignment, 2023.\nE. K\u0131c\u0131man, R. Ness, A. Sharma, and C. Tan. Causal reasoning and large language models: Opening a new\nfrontier for causality, 2023.\nG. Lample, T. Lacroix, M.-A. Lachaux, A. Rodriguez, A. Hayat, T. Lavril, G. Ebner, and X. Martinet.\nHypertree proof search for neural theorem proving. Advances in Neural Information Processing Systems,\n35:26337\u201326349, 2022.\nM. Lee, P. Liang, and Q. Yang. Coauthor: Designing a human-ai collaborative writing dataset for exploring\nlanguage model capabilities. In Proceedings of the 2022 CHI Conference on Human Factors in Computing\nSystems, pages 1\u201319, 2022a.\nM. Lee, M. Srivastava, A. Hardy, J. Thickstun, E. Durmus, A. Paranjape, I. Gerard-Ursin, X. L. Li, F. Ladhak,\nF. Rong, R. E. Wang, M. Kwon, J. S. Park, H. Cao, T. Lee, R. Bommasani, M. Bernstein, and P. Liang.\nEvaluating human-language model interaction, 2022b. URL https://arxiv.org/abs/2212.09746.\nM. Lee, M. Srivastava, A. Hardy, J. Thickstun, E. Durmus, A. Paranjape, I. Gerard-Ursin, X. L. Li, F. Ladhak,\nF. Rong, et al. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746, 2022c.\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone, C. Anil,\nI. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning\nproblems with language models. CoRR, abs/2206.14858, 2022. doi: 10.48550/arXiv.2206.14858. URL\nhttps://doi.org/10.48550/arXiv.2206.14858.\nB. Z. Li, W. Chen, P. Sharma, and J. Andreas. Lampp: Language models as probabilistic priors for perception\nand action. arXiv e-prints, pages arXiv\u20132302, 2023.\n22\nZ. Li, G. Poesia, O. Costilla-Reyes, N. D. Goodman, and A. Solar-Lezama. LEMMA: bootstrapping high-\nlevel mathematical reasoning with learned symbolic abstractions. CoRR, abs/2211.08671, 2022. doi:\n10.48550/arXiv.2211.08671. URL https://doi.org/10.48550/arXiv.2211.08671.\nC.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches\nOut, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https:\n//aclanthology.org/W04-1013.\nB. Lipkin, L. Wong, G. Grand, and J. B. Tenenbaum. Evaluating statistical language models as pragmatic\nreasoners. arXiv preprint arXiv:2305.01020, 2023.\nB. F. Liu, L. Bartz, and N. Duke. Communicating crisis uncertainty: A review of the knowledge gaps. Public\nrelations review, 42(3):479\u2013487, 2016.\nP. Lu, L. Qiu, W. Yu, S. Welleck, and K. Chang. A survey of deep learning for mathematical reasoning.\nCoRR, abs/2212.10535, 2022. doi: 10.48550/arXiv.2212.10535. URL https://doi.org/10.48550/arXiv.\n2212.10535.\nT. McGrath, A. Kapishnikov, N. Toma\u0161ev, A. Pearce, M. Wattenberg, D. Hassabis, B. Kim, U. Paquet, and\nV. Kramnik. Acquisition of chess knowledge in alphazero. Proceedings of the National Academy of Sciences,\n119(47):e2206625119, 2022.\nK. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. Advances\nin Neural Information Processing Systems, 35:17359\u201317372, 2022.\nM. Mikula, S. Antoniak, S. Tworkowski, A. Q. Jiang, J. P. Zhou, C. Szegedy, L. Kucinski, P. Milos, and\nY. Wu. Magnushammer: A transformer-based approach to premise selection. CoRR, abs/2303.04488, 2023.\ndoi: 10.48550/arXiv.2303.04488. URL https://doi.org/10.48550/arXiv.2303.04488.\nT. Miller. Explainable ai is dead, long live explainable ai! hypothesis-driven decision support, 2023.\nP. Mirowski, K. W. Mathewson, J. Pittman, and R. Evans. Co-writing screenplays and theatre scripts with\nlanguage models: Evaluation by industry professionals. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems, CHI \u201923, New York, NY, USA, 2023. Association for Computing Machinery.\nISBN 9781450394215. doi: 10.1145/3544548.3581225. URL https://doi.org/10.1145/3544548.3581225.\nE. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn. Memory-based model editing at scale. In\nInternational Conference on Machine Learning, pages 15817\u201315831. PMLR, 2022.\nH. Moravec. Mind children: The future of robot and human intelligence. Harvard University Press, 1988.\nA. Newell and H. Simon. The logic theory machine\u2013a complex information processing system. IRE Transactions\non information theory, 2(3):61\u201379, 1956.\nOpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt.\nOpenAI. Chatgpt plugins, 2023a. URL https://openai.com/blog/chatgpt-plugins.\nOpenAI. Gpt-4 technical report, 2023b.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano,\nJ. Leike, and R. Lowe. Training language models to follow instructions with human feedback, 2022.\nK. Papineni, S. Roukos, T. Ward, and W. Zhu. BLEU: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,\nJuly 6-12, 2002, Philadelphia, PA, USA, pages 311\u2013318. ACL, 2002. doi: 10.3115/1073083.1073135. URL\nhttps://aclanthology.org/P02-1040/.\n23\nL. C. Paulson. Three years of experience with sledgehammer, a practical link between automatic and\ninteractive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors, Proceedings of the 2nd\nWorkshop on Practical Aspects of Automated Reasoning, PAAR-2010, Edinburgh, Scotland, UK, July 14,\n2010, volume 9 of EPiC Series in Computing, pages 1\u201310. EasyChair, 2010. doi: 10.29007/tnfd. URL\nhttps://doi.org/10.29007/tnfd.\nS. T. Piantadosi and F. Hill. Meaning without reference in large language models. CoRR, abs/2208.02957,\n2022. doi: 10.48550/arXiv.2208.02957. URL https://doi.org/10.48550/arXiv.2208.02957.\nG. Poesia and N. D. Goodman. Peano: Learning formal mathematical reasoning. CoRR, abs/2211.15864,\n2022. doi: 10.48550/arXiv.2211.15864. URL https://doi.org/10.48550/arXiv.2211.15864.\nS. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR, abs/2009.03393,\n2020. URL https://arxiv.org/abs/2009.03393.\nS. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, and I. Sutskever. Formal mathematics statement\ncurriculum learning. CoRR, abs/2202.01344, 2022. URL https://arxiv.org/abs/2202.01344.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised\nmultitask learners. In OpenAI blog, 2019.\nT. Ringer, A. Sanchez-Stern, D. Grossman, and S. Lerner. Replica: REPL instrumentation for coq analysis.\nIn J. Blanchette and C. Hritcu, editors, Proceedings of the 9th ACM SIGPLAN International Conference\non Certified Programs and Proofs, CPP 2020, New Orleans, LA, USA, January 20-21, 2020, pages 99\u2013113.\nACM, 2020. doi: 10.1145/3372885.3373823. URL https://doi.org/10.1145/3372885.3373823.\nS. Schulz. E - a brainiac theorem prover. AI Commun., 15:111\u2013126, 2002.\nR. Sevastjanova and M. El-Assady. Beware the rationalization trap! when language model explainability\ndiverges from our mental models of language. arXiv preprint arXiv:2207.06897, 2022.\nH. Shen and T. Wu. Parachute: Evaluating interactive human-lm co-writing systems. arXiv e-prints, pages\narXiv\u20132303, 2023.\nC. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, and L. Wang. Prompting gpt-3 to be reliable.\narXiv preprint arXiv:2210.09150, 2022.\nD. Spiegelhalter. Risk and Uncertainty Communication. Annual Review of Statistics and Its Application,\n4(1):31\u201360, 2017. ISSN 2326-8298, 2326-831X. doi: 10.1146/annurev-statistics-010814-020148. URL\nhttp://www.annualreviews.org/doi/10.1146/annurev-statistics-010814-020148.\nG. Sutcliffe and C. Suttner. Evaluating general purpose automated theorem proving systems. Artificial\nintelligence, 131(1-2):39\u201354, 2001.\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford\nalpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\nA. Tarski. Truth and proof. Scientific American, 220(6):63\u201377, 1969.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro,\nF. Azhar, et al. Llama: Open and efficient foundation language models. arXiv e-prints, pages arXiv\u20132302,\n2023.\nH. Vasconcelos, G. Bansal, A. Fourney, Q. V. Liao, and J. Wortman Vaughan. Generation probabilities are\nnot enough: Exploring the effectiveness of uncertainty highlighting in ai-powered code completions. arXiv\ne-prints, pages arXiv\u20132302, 2023.\nH. Wang. Toward mechanical mathematics. IBM J. Res. Dev., 4(1):2\u201322, jan 1960. ISSN 0018-8646. doi:\n10.1147/rd.41.0002. URL https://doi.org/10.1147/rd.41.0002.\n24\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought\nprompting elicits reasoning in large language models. In Advances in Neural Information Processing\nSystems, 2022.\nS. Welleck, J. Liu, X. Lu, H. Hajishirzi, and Y. Choi. Naturalprover: Grounded mathematical proof generation\nwith language models, 2022a. URL https://arxiv.org/abs/2205.12910.\nS. Welleck, J. Liu, X. Lu, H. Hajishirzi, and Y. Choi. Naturalprover: Grounded mathematical proof generation\nwith language models. In NeurIPS, 2022b. URL http://papers.nips.cc/paper_files/paper/2022/\nhash/1fc548a8243ad06616eee731e0572927-Abstract-Conference.html.\nB. Wilder, E. Horvitz, and E. Kamar. Learning to complement humans. In Proceedings of the Twenty-Ninth\nInternational Conference on International Joint Conferences on Artificial Intelligence, pages 1526\u20131533,\n2021a.\nB. Wilder, E. Horvitz, and E. Kamar. Learning to complement humans. In Proceedings of the Twenty-Ninth\nInternational Conference on International Joint Conferences on Artificial Intelligence, pages 1526\u20131533,\n2021b.\nL. Wong, G. Grand, A. K. Lew, N. D. Goodman, V. K. Mansinghka, J. Andreas, and J. B. Tenenbaum. From\nword models to world models: Translating from natural language to the probabilistic language of thought.\narXiv preprint arXiv:2306.12672, 2023.\nY. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. Staats, M. Jamnik, and C. Szegedy.\nAutoformalization\nwith large language models.\nCoRR, abs/2205.12615, 2022.\ndoi: 10.48550/arXiv.2205.12615.\nURL\nhttps://doi.org/10.48550/arXiv.2205.12615.\nZ. Wu, A. Geiger, C. Potts, and N. D. Goodman. Interpretability at scale: Identifying causal mechanisms in\nalpaca. arXiv preprint arXiv:2305.08809, 2023.\nY. Xiao, P. P. Liang, U. Bhatt, W. Neiswanger, R. Salakhutdinov, and L.-P. Morency. Uncertainty quantifica-\ntion with pre-trained language models: A large-scale empirical analysis. In Findings of the Association for\nComputational Linguistics: EMNLP 2022, pages 7273\u20137284, Abu Dhabi, United Arab Emirates, Dec. 2022.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2022.findings-emnlp.\n538.\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate\nproblem solving with large language models, 2023.\nE. Zelikman, Q. Huang, G. Poesia, N. D. Goodman, and N. Haber. Parsel: A unified natural language\nframework for algorithmic reasoning. CoRR, abs/2212.10561, 2022a. doi: 10.48550/arXiv.2212.10561. URL\nhttps://doi.org/10.48550/arXiv.2212.10561.\nE. Zelikman, Y. Wu, J. Mu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022b.\nJ. Zerilli, U. Bhatt, and A. Weller. How transparency modulates trust in artificial intelligence. Patterns, 3(4):\n100455, 2022.\nK. Zheng, J. M. Han, and S. Polu. miniF2F: a cross-system benchmark for formal Olympiad-level mathematics.\nIn The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=9ZPegFuFTFv.\nD. Zhou, N. Sch\u00e4rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi.\nLeast-to-most prompting enables complex reasoning in large language models. ICLR, 2023.\n25\nSUPPLEMENTARY INFORMATION\nA\nPrimer on Proof Assistants\nPush-button automation in mathematics has long been a dream and has an extensive history (Bledsoe, 1977;\nBundy, 1983, 1988; Bundy et al., 1993; Davis et al., 1962; de Moura and Bj\u00f8rner, 2008; Ganesalingam and\nGowers, 2013; Newell and Simon, 1956; Schulz, 2002; Tarski, 1969; Wang, 1960). However, the initial goal of\nspecifying problems in a sufficiently expressive logic and solving them routinely with fully automated theorem\nprovers was not realised (Harrison et al., 2014). This led to a shift in focus towards interactive theorem\nprovers (ITPs), or \u201cproof assistants\u201d: humans specify the high-level structures of proofs and rely on machines\nto close out tiny details and weave together components (Delahaye, 2000; Felty, 1993; Paulson, 2010). In\nthis way, humans and machines collaborate to produce mechanically-verifiable proofs. However, adoption\nin the mathematical community has been slow as ITPs traditionally suffered from two weak points. First,\nbecause of their precise nature and relatively weak automation, writing formal proofs in interactive theorem\nprovers is an extremely arduous and expensive task (e.g., verifying the correctness of Hales\u2019s proof of the\nKepler conjecture (Hales, 2005) took a group of mathematicians and computer scientists eleven years (Hales,\n2014)). Secondly, while ITPs can check whether proofs are correct, they provide little assistance for finding\nthe proofs to truly difficult problems: people usually understand informal proofs before translating them into\nformal ones instead of directly working out formal proofs.\nWith the rise of language models, the role of machines in assisting mathematicians has been reconsidered: can\nlanguage models also automate high-level mathematical reasoning? While great strides have been made (Han\net al., 2022; Jiang et al., 2022a,b; Lample et al., 2022; Lewkowycz et al., 2022; Li et al., 2022; Lu et al., 2022;\nMikula et al., 2023; Poesia and Goodman, 2022; Polu and Sutskever, 2020; Polu et al., 2022; Welleck et al.,\n2022a; Wu et al., 2022; Zelikman et al., 2022a), consistent full and correct automation has not yet been met;\nat present, many language models, on their own, struggle not only on truly \u201chard\u201d graduate level problems\n(Frieder et al., 2023), but also on simple mathematical concepts such as counting (Bubeck et al., 2023).\nHowever, this does not rule out the possibility that they can be useful. The potential role of computerised\nmathematical assistants is being re-imagined \u2013 human-machine partnerships where neither comes up with\nthe proof alone. Yet, to adequately begin to explore these kinds of relationships necessitates studying actual\nmathematicians\u2019 interactions. While the assistance potential of ITPs has been evaluated with humans (Aitken\net al., 1998; Beckert et al., 2015; Sutcliffe and Suttner, 2001), we aim for a platform to facilitate rapid\nevaluation of LLMs in particular. The space of LLMs is changing rapidly, from new base models (Anil\net al., 2023; OpenAI, 2023b; Taori et al., 2023; Touvron et al., 2023) to new ways of linking them together\nand leveraging the output of these systems, e.g., (Dohan et al., 2022; Kazemi et al., 2022; Li et al., 2023;\nLipkin et al., 2023), to new prompting techniques (Wei et al., 2022; Yao et al., 2023; Zhou et al., 2023), and\nmore. As such, there is a need for a reliable scaffold to permit interactive evaluation of these human-machine\ninteraction (Clark et al., 2018; Cohn and Hernandez-Orallo, 2023; Lee et al., 2022a,c; Shen and Wu, 2023). It\nis this notion \u2013 interactive evaluation of humans with LLMs, specifically in the context of proof assistance \u2013\nthat we turn to next.\n26\nB\nAn Example Survey Problem\nLet x1, x2, . . . , xn \u2208 R be real numbers which are all positive.\nLet An be the arithmetic mean of x1, x2, . . . , xn.\nLet Gn be the geometric mean of x1, x2, . . . , xn.\nShow that\nAn \u2265 Gn\nwith equality holding iff:\n\u2200i, j \u2208 {1, 2, . . . , n} : xi = xj,\nthat is, iff all terms are equal.\nC\nAn Overly Verbose Response\nThe following conversation was between a human and GPT-4, the response by the latter was later considered\noverly verbose:\nHow should I go about finding such polynomial?\nTo find a polynomial, you need to have some information about its properties, such as its degree,\ncoefficients, roots, or any specific behaviour it exhibits. Here are some general steps to follow:\n1. Determine the degree of the polynomial: The degree of the polynomial is the highest power of\nthe variable (usually x) in the polynomial. It determines the number of roots the polynomial has\nand the overall shape of its graph.\n2. Identify any known roots: If you know any roots of the polynomial (i.e., values of x for which\nthe polynomial equals zero), you can use them to write the polynomial in factored form. For\nexample, if a polynomial has roots at x = 1 and x = -2, it can be written as P(x) = a(x\u22121)(x+2),\nwhere a is a constant.\n3. Use any additional information: If you have any other information about the polynomial, such\nas its value at a specific point or the behaviour of its graph, use this information to further refine\nthe polynomial. For example, if you know that the polynomial passes through the point (3, 4), you\ncan plug in x = 3 and y = 4 into the polynomial equation and solve for any unknown coefficients.\n4. Write the polynomial in standard form: Once you have determined the polynomial\u2019s factored\nform and found any unknown coefficients, you can expand the polynomial to write it in standard\nform (i.e., as a sum of terms with decreasing powers of x).\nIf you can provide more specific information about the polynomial you are trying to find, I can\nhelp guide you through the process more precisely.\nD\nUser Guide for CheckMate\nWe include a user guide for those wishing to extend CheckMate for their own interactive evaluation tasks.\nOur guide maps directly onto our open-source repository.\n27\nWe describe several hypothetical scenarios in which someone is extending CheckMate. We reiterate that any\napplication of such interaction with real human participants ought to check with the affiliated institutions\u2019\nethics review.\nD.1\nHypothetical extensions\nNew base tasks\nIf you would like to have different tasks than ProofWiki, you can replace data/problems/\nwith your files. The most straightforward way to do so is to have each new base task as its own file; we\nencourage files to be associated with a unique task number ID. If you have a grouping for the tasks which\nyou would like a participant to be able to select (e.g., in our case, participants selected a mathematical topic\nand were only shown problems from that topic), you can code which problems are associated with which\ntopic in problems_per_topic in experiment.py.\nNew rating scales\nIf you would like to change the ratings, you can change the instructions in and add\ndifferent rating endpoints in constants.py, and change the Gradio object type in pipeline_for_model in\nexperiment.py (e.g., see that Likert scales like \u201cai_corr_rating\u201d which holds user correctness per interaction\nis coded as a Radio button). That is all that is needed if you keep two ratings per generation; however, if you\nwould like to add or remove ratings, you need to do a bit more programming. You can add new a new rating\nby going into experiment.py and adding a gradio radio element with the new rating options. You can refer\nto how ai_corr_rating is constructed and stored. Similarly, to delete a rating, the easiest approach is to\nfind how its associated rating options are used in experiment.py and delete all related variables. Note, if\nchanging ratings, you will also need to update saving and variable visibility toggling, which is somewhat\nhard-coded per challenges noted below.\nDifferent models to evaluate\nIf you would like to vary the models which are evaluated, you can do\nso by changing the model tags in model_options in constants.py, and adding associated API calls in\nmodel_generate.py. Note, if you would like to use the OpenAI API, you will need your own API key.\nAdditional models can be used if you substitute the OpenAI calls with calls to the additional models. You\ncan play with the prompt used in model_generate.py, and temperature can be varied if using the OpenAI\nCompletion API in constants.py.\nIf you would like a different set-up than evaluating the preferences across three models, you can change the\nnumber of models before preferences by varying the models passed to a_single_problem.\nIf you would instead like to remove the final preferences entirely, and just have a rating per model, you can\nremove the \u201cFinal prefence\u201d tab in a_single_problem in experiment.py.\nNew instructions\nIt is particularly straightforward to swap out new instructions. To do so, you can go\nto constants.py and see plaintxt_instructions. Each entry in the array is a new instruction page. You\ncan track where this information is propogated through instruction_pages in constants.py and then in\nexperiment.py as you adjust your instructions. Note, as we were circulating the study through volunteer\nchannels, we also included text in a Google Webpage. This can be made quickly through Google Pages, and\nrequires no coding experience.\nHosting\nCheckMate can be adapted to be hosted on a personal server or using a public offering, such as\nHugging Face Space (Hugging Face, 2021). Choosing a server that is geographically closer to the target\naudience will help reduce unwanted interaction lag.\nD.2\nImplementation Challenges\nAdditionally, we detail several implementation challenges we faced. While the interface can be readily co-opted\nfor new tasks as discussed above, we hope that shedding light on some of the challenges we faced when\n28\ndesigning the platform may be of use to those who wish to further customise CheckMate for their own tasks.\nNote, as the Gradio platform is rapidly evolving, these implementation challenges may become obsolete\nshortly; we recommend checking the latest capabilities.\n\u2022 Multiple pages: A common structure in psychological studies is to have multiple pages of instructions\nand different tasks; the biggest challenge we faced was how to design multiple pages. In particular, the\nway that Gradio seemed to work \u2013 at least at the time of our construction \u2013 is that variables needed to\nbe shared across pages. So we had to instantiate all key variables and the start and then iteratively\nshow/hide components. It is possible that a more efficient way already was possible, or that one will be\ndeveloped after the release of this working paper. At present, however, our code does offer a functional\nstarting point for multiple pages.\n\u2022 Saving: Relatedly, we needed to ensure that participants\u2019 responses were saved over the course of the\nstudy; however, due to state changes, this meant that we needed to be extra careful with saving and\ndeduplicating the final results.\n\u2022 Latency: A common annoyance we heard from participants is that the study took too long to load.\nThere are two core reasons for this: 1) each interaction queried a language model API (see Appendix)\nwhich comes with inevitable overhead at present, and 2) as the web server was hosted in western\nUnited States, participants seeking to partake from other countries, e.g., France, reported higher latency.\nFrustrating wait times may help explain the low number of problems each participant took on. Better\nmeasuring and addressing latency, as suggested in (Lee et al., 2022b; Shen and Wu, 2023), are important\ngrounds for future work.\nE\nAdditional Details on Survey Set-Up\nWe hosted the study using Gradio (Abid et al., 2019). We ran the study between April 7, 2023 and April 24,\n2023. We circulated a landing page, which included a link to the actual instance of CheckMate.\nE.1\nRating Scales\nWe include the labels which were presented to participants for each of the rating dimensions, along with the\nquestion. Before participating, users rated their confidence in being able to solve the problem themselves.\nAfter interacting with a single model, they rated the correctness and perceived usefulness of each generation.\nAnd after interacting with the set of three models, they rated overall preference.\nBefore Generation\n\u201cQuestion: Before interacting with the AI \u2013 how confident are you that you could solve this\nproblem entirely on your own, with your current knowledge base and no extra assistance?\u201d\n\u2022 \u201c(0) Definitely could not solve on my own\u201d\n\u2022 \u201c(1) Very unlikely to be able to solve on my own\u201d\n\u2022 \u201c(2) Unlikely to be able to solve on my own\u201d\n\u2022 \u201c(3) May be able to solve on my own\u201d\n\u2022 \u201c(4) Likely be able to solve on my own\u201d\n\u2022 \u201c(5) Very likely to be able to solve on my own\u201d\n\u2022 \u201c(6) Definitely can solve on my own\u201d\n29\nPer Generation Ratings, Per Model\n\u201cQuestion 1: How correct (i.e., mathematically sound) is the generation?\u201d\n\u2022 \u201c(0) N/A - this response does not contain any mathematical information\u201d\n\u2022 \u201c(1) Completely incorrect or nonsensical\u201d\n\u2022 \u201c(2) Multiple critical maths errors\u201d\n\u2022 \u201c(3) At least one critical math error or multiple small errors\u201d\n\u2022 \u201c(4) One or more minor errors, but otherwise mostly correct\u201d\n\u2022 \u201c(5) One or two minor errors, but almost entirely correct\u201d\n\u2022 \u201c(6) Completely correct\u201d\n\u201cQuestion 2: How helpful would this AI generated response be towards helping someone solve\nthis problem? If you already know how to solve the problem, evaluate this as if you were an\nundergraduate mathematics student encountering this problem for the first time.\u201d\n\u2022 \u201c(0) Actively harmful\u201d\n\u2022 \u201c(1) Very harmful\u201d\n\u2022 \u201c(2) Somewhat harmful\u201d\n\u2022 \u201c(3) Unlikely to help, but unlikely to hurt\u201d\n\u2022 \u201c(4) Somewhat helpful\u201d\n\u2022 \u201c(5) Very helpful\u201d\n\u2022 \u201c(6) Definitely helpful\u201d\nCross-Model Preference\nAfter interacting blindly with the three models, participants were asked \u201cYou will now rate which model(s)\nyou prefer as a mathematical assistant. 1 = best, 3 = worst. You can assign the same rating if\nyou think two (or more) models tied\u201d. Ratings were provided via drop-down options (including 1, 2, 3).\nE.2\nParticipant Recruitment and Additional Details\nWe provide additional details on our participant recruitment and MathConverse data processing.\nAll\nparticipation was unpaid and entirely voluntary. Participants were recruited via authors\u2019 connections: We\ncirculated the study through the University of Cambridge Mathematics Department Mailing List, as well as\nthe team channel for the Human-Oriented Automated Theorem Proving project. The study was also posted\non the Lean Zulip channel, where a large community of mathematicians and computer scientists congregate\nto discuss issues related to formal mathematics. Additionally, the study was circulated amongst mathematics\nfriends at MIT, Oxford, University College London, University of Vienna, \u00c9cole Polytechnique, and Carnegie\nMellon University, and elsewhere in Berlin and Paris. We also sent the study to some machine learning\nstudents who had mathematics background at the University of Cambridge, MIT, and Princeton; here,\nparticipants may not have a formal degree in mathematics but have usually been exposed to mathematics to\nsome degree (i.e., they are not pure amateurs). It is not clear who uptook the survey, as we did not save\nany personal information, beyond the level of formal mathematics education and experience playing with\n30\nAI systems, for privacy reasons11. However, we connected with some of the experienced mathematicians\nwho participated for post-survey testimonials about their experience during the study (see the Post-Survey\nTestimonials below). In the end, we achieved decent coverage across the question topics (see Figure 4).\nGroup\nTheory\nNumber\nTheory\nProbability\nTheory\nLinear\nAlgebra\nAlgebra\nTopology\nSelected Topic\n0\n1\n2\n3\n4\n5\nCount\nFigure 4: Topics selected across participants; each participant selected a single topic to evaluate models over.\nAdditionally, of the 25 entries received, 16 resulted in at least one full round of model preferences (i.e.,\ninteracting with all three models, and ranking preference); we keep all 25 in the individual interaction analyses\nas they still provide rich data. Specifically, in cases where preference ratings were not provided, or were\nprovided but a participant had not interacted with all three models (in which case, preference scores are\nmeaningless), we keep the scores for the individual ratings and ignored the preference ratings. In one instance,\na participant did not rate all generations from the model; we also discarded this case. We emphasise for\nanyone exploring MathConverse that \u2013 as expected with volunteer-based surveys \u2013 the data can be noisy.\nFor instance, despite informing participants, some did not fully understand that the model was not aware of\nthe problem statement unless the user entered relevant info into the chat window.\nOn average, we find that our volunteers evaluate 3.1 problems (\u00b12.2 problems corresponds to one standard\ndeviation) before stopping, that is, typically just one round of going through each model and providing final\npreferences. Participants are not informed which model they are evaluating at any time. For any given model,\nparticipants interacted for an average of 3.4 queries (\u00b12.4; maximum 12 interactions taken).\n11We faced a tradeoff between user tracking for data correlation and privacy. It is possible that a unique participant number\nthat can only be used to correlate participant sessions but does not personally identify participants could be employed in the\nfuture without compromising the integrity of our data protection and privacy objectives. Informed consent would be critical,\nthough it is not clear how the perception of tracking, no matter how clearly users are informed of its benign nature, would\naffect the willingness of users to participate or interact freely with the models. We chose to maximise privacy, participation and\nfreeness of interaction in this study by not tracking participants at all.\n31\nE.3\nLanguage Model Prompting\nWe include further details on the prompts used for each language model, and the way in which we call the\nassociated API. The model IDs were as follows: \u201ctext-davinci-003\u201d (InstructGPT), \u201cgpt-3.5-turbo\u201d (ChatGPT),\nand \u201cgpt-4\u201d (GPT-4). Sampling temperature was set to 0.0, and the maximum number of tokens permitted\nto generate was 512.\nInstructGPT\nWe interact with Instruct-GPT through the Completion API call. Our instruction prompt\nstarts with \u201cHelp a professional mathematician solve a problem:\u201d.\nChatGPT and GPT-4\nWe query both models with the ChatCompletion interface. The system was setup\nwith the instruction, \u201cYou are an assistant to a professional mathematician.\u201d Further generations were elicited\nthrough the user-assistant paradigm.\nE.4\nFurther Details on Tasks Chosen\nThe ProofWiki problems were chosen to be self-contained, and relatively brief. They consist of a mixture of\nwell-known theorems and less famous, exercise-type problems, to provide a spectrum of different problems.\nThe six topics are what typical undergraduate mathematical curricula cover.\nE.5\nExample Interface Screens\nWe also include example screenshots of our interface in Figures 5 and 6.\nF\nAdditional Details on Taxonomy Construction\nWe include additional details on our taxonomy annotation scheme. The 256 human queries were split into\n5 batches of approximately 50 queries each. Four authors participated in the taxonomy annotation; three\nauthors categorised approximated 50 queries each, and one author annotated the rest (i.e., 100...[TODO, check\nnumber KMC annotated]). Annotators received a spreadsheet containing the human query per interaction,\nas well as the original problem-of-interest, the model\u2019s response for that interaction step, as well as all prior\ninteractions from between that participant and the AI system on that problem. As noted in the Methods,\nconfusions were discussed amongst annotators. During the process, a new bucket was added \u2013 \u201cAsking how to\ndo a step of the proof\u201d \u2013 after an annotator noted that some participants seemed to decompose the problem\ninto a single step, rather than asking for help over the full proof. One annotator who had already categorised\ntheir 50 queries went back and reconsidered them with this new bucket in mind.\nWe include the annotation guidelines provided to the authors who agreed to partake in the taxonomy\nannotation:\nPlease classify the user query/interaction (in the \u201cuser_query\u201d) column into one or more of\nthe annotation buckets.\nIf a query falls under a bucket, please add a \u201cy\u201d for \u201cyes\u201d to that column.\nIf you are unsure,\nbut think it may, please mark \u201cm\u201d in that column (for maybe).\nBy default, we assume that a\nquery does not fall into a bucket.\nA query may fall into more than one bucket; though we expect\nthat most fall into only one.\nIf you think that a query falls into an abstract category that is not represented, please write\nin the \u201cOther\u201d column.\nIf you need to look at the original problem statement, see the \u201cproblem_declaration\u201d column.\nThe previous human-AI interactions, with the humans\u2019 ratings for the AI outputs, are in \u201cprevious_intera\n32\nFigure 5: Example chat interface to interact with an LLM. The participant is presented with the problem and a\nreminder of particularly relevant instructions, and enters their chat below (which they can see compiled in LaTeX).\nChat history is presented for the user to refer to.\nWe expect that annotating 50 user queries will take approximately 15 minutes.\nIf it\u2019s taking\nsignificantly longer, please let us know!\nWe appreciate any time you may have to annotate\neven a few of these!\nNote:\nwe will release the annotations later (without names of the annotator).\nIf you aren\u2019t\ncomfortable with this, please do not proceed.\n33\nFigure 6: Example page of post-interaction ratings of each generation, along multiple dimensions. The user is\npresented with each query-generation pair and rates according to mathematical correctness and perceived helpfulness.\nG\nAdditional Survey Observations\nWe include additional details and analyses into the data we collect and release in MathConverse.\nG.1\nPotential Misleading Generations under Human Uncertainty\nWe include additional examples wherein participants who were uncertain of their abilities to solve the problem\non their own (see Main Text) rated a generation as completely mathematically correct when in fact it was\nnot. These examples further illuminate the need to heed caution when leveraging these models at present\nfor assistance, particularly in the case where the user may not be able to validate correctness. Below each\nexample, we include commentary from mathematics expert, William Hart, from our author team.\n34\nCommentary: The model\u2019s statement about g(x) having degree n is not correct; it clearly has degree n + 1.\nA degree n polynomial cannot be identically zero, so the next statement is also not correct. I don\u2019t follow its\nargument that f and g should be identical polynomials.\nCommentary: It is obvious only that all the terms with i > 1 are divisible by 4. Therefore the generation is\nincorrect unless n is even.\nCommentary: No, the matrix is not necessarily its own transpose. That is incorrect. The rest is poorly\nexplained.\nG.2\nTies in Rank-Order Preferences\nAs noted in the Main Text, some participants ranked models with ties (which occurred in 5 of all 15 complete\npreference ratings). In Table 2, we show the full ratings provided by participants in the presence of ties.\nInstructGPT\nChatGPT\nGPT4\n3\n3\n1\n3\n1\n1\n3\n2\n2\n1\n1\n1\n2\n3\n2\nTable 2: Instances where a participant rated models with a tie. Lower means more preferable for assistance.\nG.3\nRelationship Between Web Occurrences and Ratings\nWhile investigating the data, we wondered whether the models\u2019 performance, as measured via the correctness\nand helpfulness ratings, may be related to the frequency by which the base ProofWiki problem appeared in\nthe training set. While we do not have a transparent measure of dataset prevalence, given these models are\nlargely trained on text from the web (Brown et al., 2020; Radford et al., 2019), we used the number of Google\nsearch queries associated with the problem name12 as a proxy (see Figure 7). Intriguingly, we do not observe\na reliable correlation between search term and average procured correctness and helpfulness (i.e., the Pearson\ncorrelation is not statistically significant for any model-rating pairing, and never more than 0.2). However,\nwe encourage future work to further explore the relationship between prevalence of related problems in a\nmodels\u2019 training datasets and the models\u2019 provided assistance quality.\nG.4\nAdditional Investigations into the MathConverse Annotated Taxonomy\nWe include additional analyses into our annotated taxonomy; in particular, we consider whether users\u2019\nperceived helpfulness and correctness of the models\u2019 responses varies depending on the underlying query type.\nSuch a question is important for both designers of AI systems \u2013 e.g., to tailor model updates to make the\nsystem more usable on queries a user may desire \u2013 as well as AI users themselves, who can better learn what\nkinds of queries the model is best equipped, at present, to provide useful responses for. Indeed, we find in\nFigure 8 that the models\u2019 quality of responses do vary as a function of query type. For instance, the models\nare good at providing definitions (as we noted in the main text), and struggle to uptake corrections.\nG.5\nHandling Errors in Task Specification\nAdditionally, after launching the survey, we were informed that two problem statements were incorrect: some\nassumptions or constraints that are crucial for the correctness of the theorems are missing or too strict. The\nfirst incorrectly stated theorem is Hurwitz\u2019s. Our description of it is as follows:\n12These names can be found in the released data.\n35\n103\n105\n107\n109\nGoogle Search Queries\n1\n2\n3\n4\n5\n6\nCorrectness\n103\n105\n107\n109\nGoogle Search Queries\n0\n2\n4\n6\nHelpfulness\nInstructGPT\nChatGPT\nGPT-4\nFigure 7: Correctness and perceived helpfulness ratings, broken down by associated model, against the number of\ngoogle search queries associated with the original problem. One point per problem, depicting the average rating for\nthat problem. Google queries are conducted over the ProofWiki theorem name associated with the problem.\nHurwitz\u2019s theorem* Let \u03be be an irrational number. Then show there are infinitely many relatively prime\nintegers p, q \u2208 N>0 such that:\n\f\f\f\f\u03be \u2212 p\nq\n\f\f\f\f <\n1\n\u221a\n5q2\nThe relatively prime integers p and q should be allowed to be any integers instead of positive integers. This is\ndue to a copy error when we reformatted the original ProofWiki statement13.\nThe second such statement is Tamref\u2019s Last Theorem:\nTamref\u2019s Last Theorem* The Diophantine Equation:\nnx + ny = nz\nhas exactly one form of solutions in integers:\n2x + 2x = 2x+1\nfor all x \u2208 Z.\nThe theorem was not fully correct as stated on ProofWiki14 the day we accessed the content (2 April 2023),\nas it did not ask for non-trivial solutions. The solution n = 0 trivially satisfies the Diophantine Equation.\nWe opted to leave these problems in the survey to act as \u201ccanaries\u201d \u2013 permitting us to detect whether\nparticipants (or models) were able to identify that problems were incorrectly stated in the written form.\nWe find that since these two theorems are relatively well-known or easily understood, our experienced\nmathematician participants tended to use the definitions they had remembered to override the conditions\ngiven in the statements: the incorrect or missing assumptions are amended or added mentally by the users.\nHence we find no significant difference in the user behaviours when they deal with these two problems.\nSometimes, human users simply state the names of theorems or express them in natural language (e.g., saying\n\u201cWeak Law of Large Numbers\u201d instead of its full statement), thus ending up not propagating the errors to the\nLLMs.\nFor Tamref\u2019s Last Theorem, only GPT-4 made reasonable attempts in our collected responses. When\nprompted with the incorrect statements, GPT-4 seemed to be aware of the subtlety due to the lack of a\nnon-triviality assumption: it either pointed out that there is a solution at n = 0 or explicitly said that it\nwas looking for non-trivial solutions. For Hurwitz\u2019s Theorem, all LLMs follow the incorrect assumption of\n13https://proofwiki.org/wiki/Hurwitz%27s_Theorem_(Number_Theory)\n14https://proofwiki.org/wiki/Tamref%27s_Last_Theorem\n36\nAsk defn\n(N = 43)\nGeneral maths\nquestion\n(N = 38)\nAsk how\n(step of proof)\n(N = 39) Copy-paste\norig question\n(N = 31)\nAsk about\ngenerality\n(N = 6)\nCorrect AI\noutput\n(N = 56)\nAsk clarify\n(N = 38)\nAsk why\n(N = 4)\nAsk for\ndemonstration\n(N = 9)\nNon-maths\nrelated\n(N = 3)\n1\n2\n3\n4\n5\n6\nCorrectness\n(a)\nAsk defn\n(N = 43)\nGeneral maths\nquestion\n(N = 38)\nAsk how\n(step of proof)\n(N = 39) Copy-paste\norig question\n(N = 31)\nAsk about\ngenerality\n(N = 6)\nCorrect AI\noutput\n(N = 56)\nAsk clarify\n(N = 38)\nAsk why\n(N = 4)\nAsk for\ndemonstration\n(N = 9)\nNon-maths\nrelated\n(N = 3)\n0\n1\n2\n3\n4\n5\n6\nHelpfulness\n(b)\nFigure 8: Rating of model generations along correctness (a) and helpfulness (b), decomposed by the kind of query\nthe user had asked about. Query type is determined from our taxonomy annotations. N specifies the number of\ninteractions that were classified along a particular query, over the full interaction trace.\nthe positivity of p and q and fail to come up with good solutions, if they were given the incorrect problem\nstatement.\nG.6\nInteraction Ratings by Mathematics Topic\nRecall, participants selected a mathematics topic at the start of the survey and interacted with the models on\nproblems from that topic. We decompose the performance of models by participant topic selection in Tables\n3, 4, and 5. We depict average correctness and perceived helpfulness ratings for interactions on problems in\neach topic. Due to limited number of problems seen within each topic, we cannot draw definitive conclusions\nabout differential model performance across these topics; however, we include for completeness.\n37\nTopic\n# Problems\nCorrectness\nHelpfulness\nAlgebra\n2\n5.0\u00b11.41\n4.0\u00b10.0\nGroup Theory\n2\n1.77\u00b10.97\n1.85\u00b10.86\nLinear Algebra\n7\n3.11\u00b11.85\n2.78\u00b11.71\nNumber Theory\n5\n3.16\u00b11.9\n2.21\u00b12.21\nProbability Theory\n5\n3.61\u00b11.83\n2.94\u00b11.47\nTopology\n3\n4.5\u00b11.41\n3.75\u00b11.09\nTable 3: InstructGPT scores decomposed by topic.\nTopic\n# Problems\nCorrectness\nHelpfulness\nAlgebra\n4\n4.56\u00b11.57\n3.56\u00b10.96\nGroup Theory\n2\n4.0\u00b11.94\n3.56\u00b11.26\nLinear Algebra\n6\n5.29\u00b11.56\n5.24\u00b11.66\nNumber Theory\n6\n3.55\u00b11.86\n3.25\u00b11.81\nProbability Theory\n5\n4.2\u00b11.6\n3.44\u00b11.77\nTopology\n4\n4.73\u00b11.05\n3.91\u00b11.24\nTable 4: ChatGPT scores decomposed by topic.\nTopic\n# Problems\nCorrectness\nHelpfulness\nAlgebra\n3\n3.71\u00b11.83\n3.14\u00b11.88\nGroup Theory\n3\n3.83\u00b12.27\n3.5\u00b11.89\nLinear Algebra\n7\n5.65\u00b10.97\n5.47\u00b10.85\nNumber Theory\n5\n3.15\u00b11.79\n3.54\u00b11.65\nProbability Theory\n6\n4.88\u00b11.67\n4.33\u00b11.65\nTopology\n3\n5.62\u00b10.48\n4.88\u00b10.33\nTable 5: GPT-4 scores decomposed by topic.\nH\nPost-Survey Testimonials from Participants\nWe include additional quotes sourced from participants after completing our survey. Participants were reached\nout to via connections from the authors. We first include the complete question asked in the testimonial,\nfollowed by responses received. It is worth noting that testimonials were curated approximately one month\nafter the interaction; we would encourage future work to consider such a survey immediately after completion\nof a CheckMate interactive session.\nH.1\nWhy Stop Interacting?\nFull question: \u201cWhy did you stop interacting?\u201d\n\u2022 \u201cI usually stopped when I had a distinct impression that improvement wasn\u2019t likely past\nthe current point.\nThat could be very early on, for example after a very poor initial\nresponse that demonstrated a profound mathematical shortcoming, or it could be after a\nlonger interaction that convinced me no further improvement of the model\u2019s understanding\nor ability to accept and correct for its mistakes was likely with further prompting\u201d\n\u2022 \u201cIn the first experiment, I just tried to be fair, and give each model three queries (also\nbecause I thought I could just solve it).\nIn later experiments, simply when it didn\u2019t\nseem productive anymore, or I felt there was nothing more to ask.\u201d\n38\n\u2022 \u201cI feel like I have already gained enough ideas of the model\u2019s capability.\nMy daily work\nwith mathematical proofs involves more complex manipulations of mathematical objects and\ninteractions with proof assistant, which is still beyond the powers of the existing GPT4\nmodel.\u201d\n\u2022 \u201cUsually the model either gave a good answer after a few prompts or didn\u2019t seem to be able\nto give one.\nSometimes it happened that I did not get any better explanation to additional\nquestions at which point I stopped.\u201d\nH.2\nHuman Uncertainty in Model Correctness?\nFull question: \u201cWere there times where you didn\u2019t know whether the model\u2019s generation was correct? If so,\nhow did you handle this?\u201d\n\u2022 \u201cYes.\nI did look up some facts and proofs.\nThe main reason was that if the study is to\nbe scientific it should not rely on my incorrect assertions.\u201d\n\u2022 \u201cOf course, whenever I asked about a definition, or about a theorem I didn\u2019t know (I tried\nnot to look much into external resources).\nFirst, I was also often too lazy to check\nthe model\u2019s lengthy response (when it was a proof, I could be punctual with definitions,\nthere is no way to be sure).\nBut even if I check the proof and find a bug there, it doesn\u2019t\nmean that the theorem doesn\u2019t hold...\nOnce the model really confused me, when I was solving\nsome problem about rational approximation (I don\u2019t remember it exactly), the model suggested\ncontinued fractions which sounded reasonable but the standard error estimate for continued\nfractions was too weak.\nSo I asked whether there exists a closer estimate, and the model\nprovided me one, and just in the final review of the exchange, I figured out that it was\nfalse.\u201d\n\u2022 \u201cYes, my biggest concern with LLMs is the uncertainty of the correctness of the generated\nanswers.\nOne way is, of course, to use the generated answer as a hint and then search\nwith more reliable sources (e.g., textbook and peer-reviewed papers).\nAn alternative\nway is to keep asking further questions and the same question from a different angle (like\nhaving a conversation with a real person).\nWith rounds of Q&A, I can then gain a better\nsense of the original question.\u201d\n\u2022 \u201cMost times it was clear, however, sometimes the generations became quite lengthy and therefore\ndifficult to check (especially if there was a mistake at the beginning).\nI found it similar\nto grading student homework which is easy when everything is correct (or false), but more\ndifficult when explanations seem to contain valid arguments while still not being entirely\ncorrect.\nStudy design feedback:\nI would have liked to make full text remarks after interacting\nwith a model to be able to judge this more appropriately.\u201d\nH.3\nCorrect, but Unhelpful?\nFull question: \u201cWere there instances where a model\u2019s response was mathematically correct, but you found it\nunhelpful? If so, why was it unhelpful?\u201d\n\u2022 \u201cNot really, however some of the models state facts that they know on the given topic instead\nof answering the questions that were asked.\nThis could be helpful in certain situations,\nbut the models don\u2019t seem to discern when this is.\nIt can be unhelpful if the answer\nto the actual question then gets cut off.\u201d\n\u2022 \u201cNot much but I remember that there was a problem about B-algebras (I think it was called\nthat way), and proofwiki has a very atypical notion of a B-algebra.\nI didn\u2019t know what\na B-algebra is, so I asked the model, and he told me some definition of a B-algebra that\n39\nwas likely mostly correct but it didn\u2019t fit the problem at all.\nEventually, this was\none of the cases where I just had to go to proofwiki to figure out what the problem means\nsince neither I nor the model had an idea.\u201d\n\u2022 \u201cYes, there is.\nThe model is really good at retrieving mathematical definitions/statements\nthat I cannot remember exactly.\nHowever, the model is yet good at proposing non-trivial\nproof strategies given those definitions \u2013 they can give some sound but not very helpful\nproof suggestions.\u201d\n\u2022 \u201cIt would have been very helpful if answers had been presented more clearly (rendered formulae\ninstead of latex code, more display formulae), but this is not due to the model.\nSometimes\nI tried asking for a hint, which didn\u2019t produce good output.\nTherefore, I asked for a\nstep-by-step solution which was then more helpful, however, this takes away the opportunity\n(for a student work with such a model) to actually also think about the question by themselves.\n\u201d\nH.4\nUsing LLM-Based Assistants in Practice?\nFull question: \u201cAre there circumstances in which you would use one of the models you interacted with to assist\nyou in mathematics? (or would have used in a particular way earlier in education, if available)\u201d\n\u2022 \u201cI\u2019m going to try and use GPT-4, however I believe I will have to be very creative to do\nso.\nThere\u2019s no sense in which one can just ask GPT-4 questions and expect reliable, carefully\nthought out or ingenious responses.\nFor me it is an open question whether it can be genuinely\nhelpful.\nI\u2019ve already wasted a lot of time chasing up things GPT3/4 hallucinated.\u201d\n\u2022 \u201cWith the high uncertainty about when the answer is true, I don\u2019t think it is usable in\npractice yet.\nBy the way, I just tried another free language model (HuggingChat) about\nwhether there is a reference to a particular theorem which I thought should be known.\nIt\ngave me three references, and none of them was correct...\nI don\u2019t expect the models from\nthe experiment to behave much better.15\u201d\n\u2022 \u201cI would definitely use them for initial pilot study (e.g., literature review, identifying\npotential proof routes and related well-known theorems).\nNevertheless, given the unreliable\nnature of LLMs, I may always double-check the answers.\nAnother big usage of LLMs is to\npolish my writings (even in mathematical writings), which is super helpful for a non-native\nspeaker.\u201d\n\u2022 \u201cI\u2019d consult such models mainly to check for preexisting, relatively well-known results\nthat I might not know about as a complement to classical literature search.\nAnd as a\ntool to improve my writing of non-technical portions of mathematical texts.\nThe current\nmodels are definitely to poor in logical reasoning to be employed for finding, checking\nor rewriting proofs.\nI\u2019d be quite afraid to develop incorrect understandings by using\na model which can present wrong arguments in a very convincing way.\u201d\nI\nInteractive Case Studies with Experts\nWhile structured interactive evaluation permits nice quantitative findings, to deeply understand the capability\nof LLMs \u2013 in the context of mathematics and beyond \u2013 free-form interaction is paramount. As discussed in\n(Burnell et al., 2023), instance-level evaluation can be particularly revealing. In this section then, we offer a\nsecond form of interactive evaluation \u2013 working directly with domain expert mathematicians (from our author\nlist) to provide a series of case studies. The first case studies delve deeper into some of the same ProofWiki\n15This response was provided to us on May 19, 2023. It is feasible that HuggingChat has or will improve since then.\n40\nproblems we evaluated with CheckMate. This is followed by a study that expands the scope of the evaluation,\nattempting to locate the boundary between problems that GPT-4 finds easy and those it finds hard. In\nour survey, we observed a close relationship between mathematical correctness and perceived usefulness;\nwhile correlation is not causation, we further explore the broader mathematical reasoning capabilities of these\nmodels as a bedrock to inform their utility as proof assistants.\nWe reiterate that we are not aiming to single out GPT-4 for criticism. Rather, our goal is 1) to offer one of\nthe first real expert mathematician interactive case studies with LLMs to help guide the design of better\nmathematical assistants and inform their safe, trustworthy use in the present by helping characterise their\nlimitations, 2) to pave the way for further interactive evaluations (of which we still have too few), and 3) to\nhighlight patterns of human-computer interaction not previously known to the community, particularly when\nthe humans interacting are domain-leading experts. We hope the work will be of interest to ML engineers\nand researchers, cognitive scientists, human-computer interaction specialists, mathematicians, educators, and\nbeyond.\nA complete transcript of interactions for each case study example is included in the Appendix. We maintain\nthe original text of each case study author for authenticity, with only minor edits for precision and coherence.\nA Deep Dive into ProofWiki\nFirst, our recruited experts conduct a deeper dive into some of the\nproblems we explored in our previous evaluation. Specifically, we use the problems as a playground to explore\nhow much the model seems to \u201cknow\u201d about relevant concepts and further characterise what interactions can\nyield better (or worse) performance and assistance experience. We focus on GPT-4 (in chat mode) because it\nhas the strongest overall performance in the study above. The experts chose to refer to GPT-4 \u201cthe assistant\u201d\nin the rest of this section.\nThe first case studies are provided by Dr. William Hart, a number theorist by training; the second were\nprimarily contributed by Dr. Wenda Li, a formal mathematics expert16.\nNumber Theory Evaluation\nContributed by William Hart\nWe provide an in-depth analysis of a number of GPT-4 responses to number theoretical questions. Number\ntheory is an area of mathematics where problems are often simply stated, but difficult to solve, involving\narbitrarily deep mathematics in their solution. Whilst we didn\u2019t interact with the model to work on any\nproblem requiring very deep methods for its solution, such as Fermat\u2019s Last Theorem famously proved by\nAndrew Wiles, we did have a chance to observe the model as it struggled with problems ranging from trivial\nto moderately challenging.\nGeneral remarks Whilst GPT-4 is able to regurgitate some very commonly found elementary number\ntheoretical material and can handle straightforward problems, it has a major difficulty with algebraic manip-\nulation and little or no ability to work on unseen problems that require backtracking, proving intermediate\nlemmas or extensive planning.\nProofWiki problem 21 Show that the equation\n1 + an = 2m\nhas no solutions in the integers for n, m > 1.\nProblem 21 is a simple Diophantine equation. The problem is quite obscure (a verbatim Google search gives\n10 results) and thus not likely to appear in training material repeatedly. The model took very reasonable\nsteps towards solving the problem: it started by claiming the proof is by contradiction and proceeded to\nreason about the assumed solution for n, m > 1.\n16Case studies were conducted between April and early May 2023.\n41\nIt begins well by reasoning that a must be odd because 1 + an is even. No explanation is given for this,\nbut an experienced human wouldn\u2019t explain this step either given the routine nature of parity arguments in\nNumber Theory.\nThe next step is to take an expression (2k +1)n which has appeared and expand it using the binomial theorem.\nHowever, it does this in a surprising way, splitting the resulting sum into the first two terms and then a sum\nfor the remaining terms.\n(2k + 1)n =\nn\nX\ni=0\n\u0012n\ni\n\u0013\n(2k)i = 1 + n(2k) +\nn\nX\ni=2\n\u0012n\ni\n\u0013\n(2k)i\nThis is impressive because GPT-4 is exhibiting some planning. It clearly has in mind to work modulo 4 and\nit can see that all of the terms of the final sum might vanish modulo 4. Indeed this is the very next claim\nthat it makes.\nWhilst it didn\u2019t explain why every term of the final sum is divisible by 4 it was asked on subsequent generations\nto explain this step and it correctly did so.\nHowever, things do not go so well from here. It now claims that we can write the original equation 1+an = 2m\nas\n1 + 2kn + 4s = 2m\nfor some s. This is a beguiling step that a human might overlook as correct, but it is not. The expression\n1 + 2kn + 4s is the expression for an not 1 + an. GPT-4 has made an algebraic error. This sort of thing is\nunfortunately very common and lets GPT-4 down on many examples.\nAsking GPT-4 to self correct did not help it notice and correct its mistake. To see if it could eventually\nproduce a completely correct proof, it was asked numerous times to solve the problem. Whilst its overall\nstrategy was good on each generation, different algebraic mistakes occurred each time so that a correct proof\nwas not eventually reached.\nProofWiki problem 28 Show that\n3 =\nr\n1 + 2\nq\n1 + 3\n\u221a\n1 + \u00b7 \u00b7 \u00b7.\nProblem 28 is a more difficult problem and the model is completely unable to deal with it. It admits that\nproblems involving nested radicals can be difficult and actually gives up after standard methods don\u2019t make\nany headway.\nA consistent problem here is an inability to write down a correct expression for a recursive relation to describe\nthe nested radical. GPT-4 seems to be convinced that the expression under each square root is the same, so\nthat if we write the initial expression 3 =\n\u221a\nA then we also have 3 =\np\n1 + 2\n\u221a\nA and 3 =\nq\n1 + 2\np\n1 + 3\n\u221a\nA,\netc.\nOn subsequent attempts additional terms of the initial sequence were provided in the hope that it would pick\nup on the increasing sequence of constants that the square roots are multiplied by.\nWhilst GPT-4 would confirm that it had noticed this pattern, it would always proceed ignoring this fact. On\neach generation, GPT-4 would finish off by noting it got the wrong answer and that this must be because it\ndidn\u2019t take this increasing sequence of constants into account! It\u2019s as though GPT-4 only knows one way to\nhandle nested radicals, and knowing that this won\u2019t work here, tries it anyway, inevitably getting the wrong\nanswer.\nTo probe a little deeper, GPT-4 was instead prompted in a direction that might allow it to make partial\nprogress. The hint was given to try peeling the expression on the right hand side one square root at a time,\n42\nworking backwards from the desired result that the full nested radical should have the value 3 to see if some\npattern could be found in the values of the inner nested radicals.\nIt was easy to prompt it so that it heads in that direction but on every generation it made hopeless algebraic\nand numerical errors, once again illustrating that very often what holds it back is high school algebra rather\nthan the depth of the mathematics.\nAs GPT-4 could not be coaxed into returning correct values for the sequence of inner nested radicals, the\nattempt to solve the problem using GPT-4 was abandoned.\nProofWiki problem 24 Let \u03be be an irrational number. Then show there are infinitely many\nrelatively prime integers p, q \u2208 N>0 such that:\n\f\f\f\f\u03be \u2212 p\nq\n\f\f\f\f <\n1\n\u221a\n5q2\nFinally, Problem 24 is another difficult problem. Its solution on the ProofWiki website requires a number of\nlemmas and some subtle reasoning. Solving a problem of this kind would require some planning capability, or\nat the very least the ability to backtrack and experiment with various ideas. This is something that GPT-4\ndoesn\u2019t appear to possess beyond what can be \u2018computed\u2019 within the model itself.\nGPT-4 does make the completely reasonable first step of approaching this problem using a continued fraction\nexpansion of the irrational number \u03be. Many approximation problems of this kind do indeed proceed this way.\nContinued fractions yield a sequence of convergents pn/qn that converge to the irrational number \u03be.\nAfter picking a reasonable theorem from the theory of continued fractions and applying it, GPT-4 has the\nfollowing expression\nqnqn+1 >\n\u221a\n5q2\nn.\nAt this point it is clear that GPT-4 does not know how to proceed, but knows what it should end up with, so\nmakes the unsubstantiated claim that this inequality is satisfied when qn+1 >\n\u221a\n5qn.\nThere is no reason to infer that this should be the case at this point in the problem and if the particular\nchosen approach is to work out, this would have to be proved. Instead of doing so, GPT-4 just asserts that it\nis true without attempting to prove it.\nWhen asked directly how to prove this statement GPT-4 clearly has no idea how to do so and makes a\ncompletely bogus claim that a sequence with linear growth will eventually outgrow a sequence with exponential\ngrowth. It seems to be common for GPT-4 to hallucinate details when things aren\u2019t working out or if it\ndoesn\u2019t know a reasonable answer.\nIn other contexts we have observed that GPT-4 can produce better output if asked to stop itself if a particular\nmathematical approach does not seem to be working out and to try another approach. When prompted to do\nso in this particular case GPT-4 did indeed try numerous reasonable strategies but unfortunately it was still\nnot ultimately successful. This was in part due to poor choices along the way and partially due to being\nplagued by algebraic errors which ended up misleading it.\nFor balance we mention that the failed attempts above were not entirely characteristic of GPT-4 which can\nin some cases produce perfect answers.\nFor example, Problem 27 is solved completely, although slightly inefficiently and Problem 23 is correct except\nfor a single bogus explanation which was not particularly significant. However, it should be pointed out that\nProblem 27 is quite trivial, essentially requiring only the binomial theorem and Problem 23 is completely\nstandard in many texts on elementary Number Theory. It is very unlikely that the standard proof would be\nhit upon at random, and the fact that GPT-4 generates it perfectly is probably an indication of the relative\nabundance of proofs in training material.\n43\nI.1\nProblem Perturbation to Probe Memorisation\nContributed by Wenda Li\nIf a system simply memorizes the answers to problems, its performance can greatly differ depending on\nwhether the problems it is evaluated on are memorized. In this section, we evaluate GPT4\u2019s performance on\nvariations of problems from ProofWiki, which are far less likely to be appear in training data since we make\nnovel variations. Concretely, we varied the problems in three different ways to test the model\u2019s understanding\nof the problems: asking for definitions of concepts, loosening assumptions of problems, and instantiating\nabstract variables with values. Two problem instances and their variations are presented due to them being\nthe most interesting and revealing examples from all that were tried.\nGeneral remarks We found GPT4\u2019s performance at variations of several ProofWiki problems quite\nsatisfactory: it can reliably retrieve definitions of concepts used in the problem as well as in its own proof; it\ncan correctly assess whether loosening certain assumptions breaks the proof; it can also instantiate variables\nquite robustly, given the opportunity of inspection of its own answers. There have been debates (Bender\net al., 2021b; Piantadosi and Hill, 2022) about to what extent shall we say language models \u201cunderstand\u201d,\ngiven the nature of their stochastic generation. In our study, we find a couple of simple17 cases where\nthe language-model-based assistant possesses the mathematical understanding of assumptions and variable\ninstantiations beyond mere memorisation.\nProofWiki problem 25 Let a, b \u2208 N>0 such that there exists no m, n \u2208 N>0 such that am = bn.\nProve that logb a is irrational.\nPresented the problem statement above, the assistant gave a perfect answer with step-by-step calculations.\nTo test if the assistant has a true mathematical understanding of the problem, we first asked for definitions of\nconcepts used, and then varied the original problem by loosening some of the assumptions made, and asked\nthe assistant for a proof in the new setting.\nAsking for definitions We found that the assistant gave the correct definitions in the theorem statement as\nwell as in its own proof. Concretely, it gave the right answers for: the definition of logarithm; the range of a\nlogarithm\u2019s base; the meaning of the set N>0; and whether logb a = p\nq can be a negative number (p and q are\nvariables arising from the assistant\u2019s own proof).\nLoosening assumptions We started by asking the assistant whether the proof still holds if we instead have\na, b \u2208 R>0? The assistant understood the meaning of R>0 and confirmed the derivation still held, so the\noriginal lemma/proposition has been generalised (since one of its assumption has been relaxed). Later, we\nattempted to generalise the proposition further by dropping the assumption a \u2208 R>0 or b \u2208 R>0:\nWe continued by asking if dropping the assumption that b \u2208 R>0 or a \u2208 R>0 affects the original proof? The\nassistant knew that these assumptions were necessary to make the log function well-defined, and pointed out\nthat dropping either of the assumptions would invalidate our previous derivation.\nThese variations, though not impossible, are unlikely to appear together with the problem in the training\ndata of the assistant. We think the assistant does have some understanding of the underlying mathematical\nconcepts and its own proof, in the context of this problem.\nProofWiki problem 39 Let X be a random variable. Assume E(X) = \u00b5 for some \u00b5 \u2208 R and\nvar(X) = \u03c32 for some \u03c32 \u2208 R>0. Show that for all k > 0: Pr (|X \u2212 \u00b5| \u2265 k\u03c3) \u2264 1\nk2 .\nGiven this problem statement, the assistant mentioned that we can use the Chebyshev\u2019s inequality, and then\nre-stated the problem in an almost identical way but with different variable names. This demonstrates a\ncertain level of variable unification, which is an important concept in automatic theorem proving.\nVariable instantiation We then checked whether the assistant knew how to instantiate variables by asking\nit whether the proof still holds when the following concrete values are assigned to k: 2,\n\u221a\n2,\n\u221a\n2 \u2212 1,\n\u221a\n2 \u2212 2,\n17Note, \u201csimple\u201d here is relative to that of a trained mathematician.\n44\nand (\n\u221a\n2 \u2212 2)2. Human inspection finds the assistant\u2019s behaviour to be correct. The assistant can clearly\nhandle concrete calculations even when k is a relatively complicated number (e.g.,\n\u221a\n2 \u2212 1). The model also\nknows that the previous derivation cannot be carried out when k =\n\u221a\n2 \u2212 2, a negative number.\nAn interesting observation arose when the assistant was not confident of its derivations: we asked: \u201care you\nsure (\n\u221a\n2 \u2212 2)2 > 0?\u201d The answer should be affirmative, but the assistant started to apologise and revise its\nprevious correct calculation by saying \u201cWhen k = (\n\u221a\n2 \u2212 2)2, the value of k is indeed non-negative, but it is\nactually equal to 0, not greater than 0.\u201d When we asked again \u201cWhy do you say your previous statement was\nincorrect and k = 0? I don\u2019t understand.\u201d, the assistant corrected its previous mistake and produced the right\nevaluation.\nWe found that the assistant is generally quite capable with variable instantiations and evaluating certain\ncomplex expressions, with the occasional mistake made with low confidence. We hypothesise that the mistake\nmay be a defect of its reinforcement learning with human feedback (RLHF)(Casper et al., 2023; Christiano\net al., 2017; Ouyang et al., 2022) training: the human feedback is mostly assumed to be right, and when the\nfeedback questions a true fact, the assistant concurs and alters its own (correct) response.\nI.2\nInvestigating the Boundary between Easy and Hard Problems\nContributed by Timothy Gowers\nIf we want to understand how and to what extent large language models can help mathematicians, it is\nclearly important to understand what they can and cannot do. A range of views have been expressed on\nthis topic, with some saying that they already show glimmerings of AGI (Bubeck et al., 2023), and others\ndismissing them as mere \u201cstochastic parrots\" (Bender et al., 2021a). On the latter view, the successes that\nLLMs undoubtedly have solving mathematical problems are to be understood as very good guesses of what\nthe outward form of a proof would look like, unaccompanied by any genuine understanding of what that\nproof means, even when it is correct.\nA difficulty with evaluating the level of understanding of a language model is its opacity: for any particular\nanswer it gives, we do not know the extent to which it is merely an amalgam of texts it has seen as part of\nits training data and the extent to which it has actually had to build its answer in a coherent way. One way\nto try to get round this problem is to ask the LLM questions that are deliberately designed to be \u201cquirky\"\nand non-standard. A good source of such questions is ones that ask whether mathematical objects of certain\nkinds exist with various artificial combinations of properties.\nTimothy Gowers, a mathematician from our author team, tested GPT-4 on many such questions, attempting\nto identify a \u201cboundary\" between what it could and could not do. This imposed a further constraint on\nthe questions: that they should ideally be modifiable in various ways, so that one can \u201cturn the dial\" until\nGPT-4\u2019s performance drops.\nGeneral remarks Many of the strengths and weaknesses we observed in GPT-4 are ones that have been\ncommented on several times (in connection with other LLMs as well). For instance, it is not good at\ncalculation, it has a tendency to ignore facts that do not support its main conclusion (even if it itself has\ngenerated those facts), and to invent facts that do support it (Azamfirei et al., 2023).\nWhen it comes to building examples, it has another weakness, which is that instead of using a process of\nreasoning to constrain what the example can look like and only then exhibiting the example, it prefers to\nstart by exhibiting the example and then provide the justification that it has the desired properties. If its\ninitial suggestion is correct, then this may be all right (though its justifications are not always correct),\nbut often the example it gives is not correct, and it typically follows it up with a \u201cjustification\u201d that to a\ntrained mathematician has very obvious flaws. This behaviour supports the stochastic parrot view, since\nmathematical constructions are typically presented in the literature in the same unhelpful style \u2013 first the\nexample, and then the verification that it works.\nOne can try to use prompt engineering to discourage GPT-4 from organizing its answers in this way, for\nexample asking the model not to provide an example immediately but to describe a general strategy first,\nand only then the implementation of the strategy, or to pretend to be a student and ask it to offer hints.\n45\nWhile these did improve its output somewhat, they did not make a significant enough difference to affect our\ngeneral conclusions, so we mainly used more straightforward prompts.\nA further feature that has a negative effect on the experience of interacting with GPT-4 is that even when it\ngives correct answers, it often describes them and checks their properties in a laborious case-by-case way,\nand often those checks are not even necessary. For instance, if asked to construct a 3-regular graph with\neight vertices, it will typically list all the vertices and edges, and then for each vertex it will say what its\nneighbours are and comment that there are three of them. Or if it needs the fact that the matrix\n\u00120\n1\n0\n0\n\u0013\nsquares to the zero matrix, instead of just asserting that it does, it will write\n\u00120\n1\n0\n0\n\u00132\n=\n\u00120\n1\n0\n0\n\u0013 \u00120\n1\n0\n0\n\u0013\n=\n\u00120\n0\n0\n0\n\u0013\n,\nwhich is adding virtually nothing to a bald assertion, since no details are given of the computation of the\nmatrix product (not that one would want them). Similarly, it sometimes \u201cverifies\u201d that a matrix is symmetric\nby writing out that matrix and its transpose and noting that the two matrices it has written out are identical.\nIn the more positive direction, there were several questions that GPT-4 answered well in a way that is hard to\ndismiss as mere parroting. To give a simple example, if it is asked to differentiate a function, it will typically\ndo so very competently, suggesting at least some ability to generalize. (A possible explanation for why it is\ngood at differentiation and bad at arithmetic is that there are many more worked examples of differentiation,\ncomplete with step-by-step explanations, than there are worked examples of arithmetic.) More examples will\nbe discussed below, with some speculations about why GPT-4 is good at them.\nExample 1\nHere is a prompt given to GPT-4.\nCan you find an example of a set A of positive integers that satisfies the following three conditions?\n(The LaTeX is compiled here but was uncompiled in the prompt.)\n1. |A| = min(A).\n2. A consists of prime numbers only.\n3. A has cardinality at least 5.\nThe response was the following correct and concise answer.\nYes, here\u2019s an example of a set A of positive integers that satisfies the given conditions:\nA = 5, 11, 17, 23, 29\nThe cardinality of A is 5, which is also the minimum element in A.\n1. All elements in A are prime numbers.\n2. The cardinality of A is 5, which is at least 5.\n3. Therefore, set A = 5, 11, 17, 23, 29 meets all three conditions.\nDespite the fact that this answer is correct, it has a revealing feature. Why, one might ask, does GPT-4 not\ngive the more obvious answer {5, 7, 11, 13, 17}? That is, why does it not take the five smallest prime numbers\ngreater than or equal to 5?\nA clue lies in the fact that the set it does choose, {5, 11, 17, 23, 29} has the additional interesting property\nthat it forms an arithmetic progression. This property has led over the years to the set being mentioned in\nmany articles about arithmetic progressions that consist of primes, and presumably a significant fraction of\nthose also mention that the number of elements is 5.\nWhat is illustrated here is a preference by GPT-4 to use off-the-shelf examples, a preference that is strong\nenough that it will often do so even when the examples do not in fact have the required properties.\n46\nWhy would most human mathematicians prefer to take the first five primes, starting at 5? It is because that\nexample is easily generalisable: if we change the number 5 to any other number n, we simply have to choose\na prime p that is at least as big as n and take the first p prime numbers from p onward.\nTo understand GPT-4\u2019s behaviour better, it is therefore natural to give it the following slightly modified\nprompt.\nCan you find an example of a set A of positive integers that satisfies the following three conditions?\n1. |A| = min(A).\n2. A consists of prime numbers only.\n3. A has cardinality at least 6.\nThe response was\nYes, here is an example of a set A of positive integers that satisfies the given conditions:\nA = 7, 11, 13, 17, 19, 23\n1. |A| = 6, and the minimum element in A is 7, so |A| = min(A).\n2. All elements in A are prime numbers: 7, 11, 13, 17, 19, and 23. 3. A has cardinality 6, which\nis at least 6.\nThe set A satisfies all three conditions.\nEven after its error was pointed out, and a few more rounds of the conversation had taken place, GPT-4 did\nnot manage to eliminate all mistakes and give a correct answer. Indeed, when told that 6 does not equal 7, it\nat first tried to explain why no such set could exist.\nThis example lends some weight to the stochastic-parrot view: the problem is extremely easy for a human,\nbut it is also a sufficiently artificial question that GPT-4 will not be likely to have encountered it, or anything\nvery like it, in the texts on which it has been trained. One might argue that it does at least give a set\nof consecutive primes this time, but that again is what one would expect, given that sets of consecutive\nprimes will appear much more frequently in the literature than other sets (a tendency that one might guess is\noverridden when the sets start with 5 and have size 5).\nExample 2\nThis was a successful attempt to trick GPT-4. It was first asked to evaluate the integral\nR 1\n\u22121 x3 cos x dx. It did so by a laborious argument using iterated integration by parts, presumably because\nthe integrand is typical of the integrands in integration-by-parts exercises. After getting everything right\nuntil close to the end, it made an elementary mistake, but with a bit of help it arrived at the correct answer,\n0 (though still persisting in some of its incorrect calculations). On being asked whether there was a shorter\nargument, it gave the right response: that the integrand is an odd function and the integral symmetric about\n0, so the integral is 0.\nThen it was asked to evaluate the integral\nR 1\n\u22121(x2 \u2212 1/3) dx. Again it did so correctly, obtaining the answer 0.\nOn being asked whether there was a simple reason for this, it once again pointed out that the integrand was\nan odd function, and even supplied a bogus proof that the function is odd.\nThis behaviour again fits the stochastic-parrot hypothesis quite well: almost always if one is asked for a\nsimple reason that the integral of a function over a symmetric interval is zero, the correct response is that the\nfunction is odd. Whether or not it actually is odd is for GPT-4 a secondary consideration.\nExample 3\nA nice problem that mathematics undergraduates tend to find quite hard is to determine whether\nthere is an order-preserving bijection between the rational numbers and the dyadic rationals. Surprisingly, the\nanswer is yes, and the proof is by what is known as a back-and-forth argument. That is, one enumerates the\nrationals and the dyadic rationals, and then one alternates between choosing a match for the first unmatched\nrational and the first unmatched dyadic rational, making sure at each stage that the order is preserved.\n47\nWhen GPT-4 was asked the problem, it tried to prove that no such bijection could exist. Each time its\nmistakes were pointed out to it, it replaced its bogus argument by a slightly modified bogus argument.\nUpon being asked whether it knew about back-and-forth arguments, it said that it did, and explained that\nthey could be used to prove a theorem of Cantor, that any two countable dense subsets of the real numbers\nare order isomorphic. It did not seem to realise that the problem it had been asked was a special case of\nthis theorem. It also tried to explain why one could not use a back-and-forth argument to prove that the\nrationals and the dyadic rationals are order isomorphic, but after its explanation was criticised, it proceed to\ngive a convincing sketch of how such an argument would indeed work.\nExample 4\nWhen GPT-4 was asked whether there is a positive integer n such that n + k is divisible by\nk for every integer k in the range {1, 2, . . . , 100}, it said no, and offered bogus proofs. After being steered\ntowards a positive answer via certain easier questions, it suggested taking n to be LCM(1, 2, . . . , k) \u2212 1. This\nwas interesting because LCM(1, 2, . . . , k) would have been a correct answer, but the pointless subtraction of 1\nruined it. Furthermore, it gave an argument that would have been correct if the -1 had not been present.\nWhat might explain this act of self sabotage? One idea is that GPT-4 is influenced by Euclid\u2019s proof that\nthere are infinitely many primes, which assumes that p1, . . . , pk are all the primes and considers the number\np1p2 . . . pk + 1. An alternative argument would be to consider the number LCM(1, 2, . . . , k) \u2212 1, where k is at\nleast as large as the largest prime.\nHowever, this explanation is rather speculative, and a Google search does not seem to back it up. When\nGPT-4 was asked why it had subtracted 1, it did not provide a convincing reason either.\nMore revealing was its behaviour when its example was criticised on the grounds that, for instance,\nLCM(1, 2, . . . , k) \u2212 1 is odd, and therefore not divisible by 2. Instead of adjusting its answer, as a hu-\nman mathematician might, it decided that no such n existed, and when its arguments for that conclusion were\ncriticised, it went back to the example of LCM(1, 2, . . . , k) \u2212 1. Even when asked whether LCM(1, 2, . . . , k)\nwould work better, it initially said no. So this was not really a \u201cboundary\u201d example, and more just a problem\non which GPT-4 got into a rut and could not get out of it.\nExample 5\nA better example of a \u201cboundary\u201d problem was the following question: does there exist a\ngraph with eight vertices such that every vertex has degree 3? Once again GPT-4 demonstrated its liking for\noff-the-shelf examples, giving the example of the 3-dimensional discrete cube. (An alternative approach is to\ntake eight vertices joined in a cycle, and to join each vertex in addition to the vertex opposite it in the cycle.)\nWhen asked whether there was a graph with eight vertices such that every vertex has degree 5, it performed\nfar worse. It did not know of any off-the-shelf examples, and was probably incapable of tricks such as taking\nthe complement of an 8-cycle (which works because in the cycle every vertex has two neighbours, so in the\ncomplement of the cycle it has five neighbours). That is, it does not appear to be capable of taking an\noff-the-shelf example and modifying it in a suitable way. Instead, it resorted to listing the vertices as A,\nB, C, D, E, F, G and H, and for each vertex giving a list of its neighbours. The trouble is that this kind\nof approach gave it many opportunities to fail as a result of familiar weaknesses such as a propensity to\nmake calculation errors or to write down inconsistent statements. For instance, over its several attempts it\nwould frequently list a vertex v as a neighbour of another vertex w, but without listing w as a neighbour of v.\nEventually, probably with a slice of luck, it came up with an example that turned out to be the complement\nof the disjoint union of a 3-cycle and a 5-cycle. (Since the complement has to be regular of degree 2, it will\nalways be a disjoint union of cycles.)\nExample 6\nIt has been noted that GPT-4 likes well-known patterns, and that one way of inducing it to\nfail is to ask it questions that will tempt it to give answers that fit those patterns. The following attempt to\nlead it astray in that way was a partial success. It was asked to find integers a and b such that the sequence\n(1, 3, a, 7, 9, b, 13, 15) is strictly increasing but not an arithmetic progression. It responded by choosing a = 5\nand b = 11, thereby falling headlong into the trap. However, it then did a check by calculating all the\nsuccessive differences. On observing that it obtained the difference sequence (2, 2, 2, 2, 2, 2, 2), it then modified\n48\nits choice of b to 12, after which it recalculated the difference sequence, obtaining (2, 2, 2, 2, 3, 1, 2) and\ndeclaring itself satisfied.\nThis was another example where despite arriving at the correct answer, GPT-4 argued in a very non-human\nway. The main non-human feature was of course that it began by making the one guess that it needed to\navoid (out of the strictly increasing possibilities). However, the whole approach of guessing and then verifying\nis inappropriate for the problem, since it is much more efficient to reason as follows: first, we note that if the\nsequence is to be an arithmetic progression, then it will have to have common difference 2 (since the first\ntwo terms differ by 2) so it is sufficient to ensure that a \u0338= 5. This kind of forward planning appears to be\nbeyond the current capabilities of GPT-4, (though maybe it could be induced to some small extent with\ncareful prompt engineering).\nWe briefly mention its response to a variant of the problem, where it was asked whether it was possible to\nfind integers a, b and c such that the sequence (1, a, b, c, 14) is an arithmetic progression. It answered yes,\nthen set d to be the common difference, obtained the equation 14 = 1 + 4d, solved for d, discovered that d\nwas not an integer, and answered no, having apparently forgotten that it had previously answered yes. This\nshowed a reluctance to plan in advance even in a situation where it was entirely capable of carrying out the\nrequired planning.\nExample 7\nGPT-4 was asked to find a colouring of the set {1, 2, 3, 4, 5, 6, 7, 8, 9} using three colours and\nsatisfying the conditions that each colour is used three times, and no integer n has the same colour as n + 1\nor n + 3. The obvious way to solve this problem is trial and error, which for a typical human will work with\na small amount of backtracking. It did indeed choose this approach at first, but failed quite badly because it\nwas unable to check the conditions properly, which caused it to assign colours that were forbidden by the\nconditions, and, in the other direction, to claim that certain choices were forced when they were not. After\na couple of failed attempts, it switched to trying to find a \u201csystematic\" approach. One such attempt was\nto split the set into even and odd numbers, but that did not help it find a correct colouring. It even tried\nsplitting the numbers into the three sets {1, 4, 7}, {2, 5, 8} and {3, 6, 9} and assigning one colour to each set,\nwhich violated the n + 3 condition many times over.\nIn sum, its ability to check simple conditions was too unreliable for it to be able to push through a\ntrial-and-error approach, and for this problem a guess-and-check approach has a very low chance of success.\nExample 8\nA somewhat similar question on which it performed badly was to find a sequence of nine\ndistinct integers with no increasing or decreasing subsequence of length 4. Here it once again showed its taste\nfor patterns: the problem was that it did not choose appropriate patterns. An example that was typical of its\noutput was (4, 1, 5, 2, 6, 3, 7, 8, 9). Interestingly, when, after a few failures, it was given a reason-step-by-step\nprompt, it produced the same example, this time after talking about interleaving sequences \u2013 an idea which,\nif used correctly, leads to solutions such as (1, 4, 7, 2, 5, 8, 3, 6, 9). However, encouraging it to use interleaving\njust led to further incorrect guesswork, an extreme example of which was when it interleaved the sequences\n(1, 3, 5) and (2, 4, 6, 7) to obtain (1, 2, 3, 4, 5, 6, 7), stuck 8 and 9 on the end, and proposed (1, 2, 3, 4, 5, 6, 7, 8, 9)\nas a solution (complete with a \u201ccheck\u201d that it worked).\nWhen given the hint that it might like to start its sequence with (7, 8, 9), it immediately made the obvious\nsuggestion (7, 8, 9, 4, 5, 6, 1, 2, 3). When asked for a rigorous proof that this sequence has the desired property,\nit gave an inadequate answer, stating correctly that the longest increasing subsequences are those that begin\nwith 7, 4 and 1, and stating incorrectly that the longest decreasing subsequences start with 9, 6 and 3. There\nwas nothing resembling the argument that a decreasing sequence can contain at most one term out of each of\nthe subsequences (7, 8, 9), (4, 5, 6) and (1, 2, 3).\nExample 9\nThe following is a question on which GPT-4\u2019s performance was more or less perfect. It was\nasked to evaluate the sum Pn\nr=2 log(1 \u2212 1/r). It noticed that 1 \u2212 1/r can be rewritten as (r \u2212 1)/r, which\ntherefore leads to a telescoping sum and and eventual answer of log(1/n). It did not give a detailed explanation\nof why, as it put it, the denominator in each term cancelled with the numerator in the succeeding term, but\n49\nwhen asked for one it gave a good one (though it argued for a telescoping product inside the logarithm rather\nthan a telescoping sum, which might have been simpler).\nThis was interesting because it required putting together two tricks: one to rewrite 1 \u2212 1/r and the second to\nuse the telescoping. However, its performance on a follow-up question strongly suggested that it had seen\nthe question in its training data. It was asked whether it could evaluate the sum Pn\nm=1 log(1/m(m + 1)).\nThis question was intended to trick it, since the sum Pn\nm=1 1/m(m + 1) is famously evaluated by rewriting\n1/m(m + 1) as 1/m \u2212 1/(m + 1) in order to obtain a telescoping sum. However, instead of falling into the\nintended trap, it stated falsely that it could rewrite the sum as Pn\nm=1 log((m + 1)/m), after which it obtained\na telescoping sum very similar to that of the previous question.\nWhen asked a further follow-up question to test how well it understood the principle of telescoping sums, it\ngave an answer that had good points and bad points, both of which indicated that it was good at capturing\nthe look of a correct answer while not operating at the level of the underlying concepts. The question was to\ndetermine the sum PN\nn=1 g(n), when g(n) had been defined to be f(n)\u2212 2f(n + 1) +f(n + 2). It is instructive\nto look at its complete answer.\nLet\u2019s work out the sum. We have\nN\nX\nn=1\ng(n) =\nN\nX\nn=1\n(f(n) \u2212 2f(n + 1) + f(n + 2))\nWe can break this into three separate sums:\nN\nX\nn=1\nf(n) \u2212 2\nN\nX\nn=1\nf(n + 1) +\nN\nX\nn=1\nf(n + 2)\nLet\u2019s shift the indices in the second and third sums to make the limits match up with the first\nsum:\n1. For the second sum, we substitute m = n + 1, so n = m \u2212 1. When n = 1, m = 2; when\nn = N, m = N + 1.\n2. For the third sum, we substitute m = n + 2, so n = m \u2212 2. When n = 1, m = 3; when\nn = N, m = N + 2.\nDoing this, we get:\nN\nX\nn=1\nf(n) \u2212 2\nN+1\nX\nm=2\nf(m) +\nN+2\nX\nm=3\nf(m)\nNow, we can write out the sums explicitly to see what\u2019s happening:\n1. The first sum is: f(1) + f(2) + f(3) + \u00b7 \u00b7 \u00b7 + f(N).\n2. The second sum is: 2f(2) + 2f(3) + \u00b7 \u00b7 \u00b7 + 2f(N + 1).\n3. The third sum is: f(3) + f(4) + \u00b7 \u00b7 \u00b7 + f(N + 2).\nBy looking at these sums, we can see that most of the terms will cancel out. For example, the\nterm f(2) is added in the first sum and subtracted twice in the second sum. Similarly, the term\nf(3) is added in the first sum, subtracted twice in the second sum, and added once in the third\nsum.\nAfter all the cancellations, we are left with:\n= f(1) \u2212 2f(2) + f(N) \u2212 2f(N + 1) + f(N + 2)\nSo, the sum PN\nn=1 g(n) simplifies to f(1) \u2212 2f(2) + f(N) \u2212 2f(N + 1) + f(N + 2).\n50\nA first comment is that the final answer here is not just wrong, but clearly wrong to an experienced\nmathematician, because there is a symmetry in the problem that guarantees that the number of leftover\nterms at the two ends will be the same, whereas in GPT-4\u2019s answer there are two terms at the beginning and\nthree at the end. Given its performance on other problems, it is no surprise that GPT-4 does not spot this\nanomaly.\nOn a more positive note, the first two steps \u2013 splitting up the sum into three parts and changing variables\n\u2013 are exactly the right thing to do, even if the purpose of changing of variables is not to get the limits to\nmatch up (in fact, it does the opposite). However, it then abandons this promising beginning in favour of\nwriting out the sums in a more transparent notation, a move that is striking because it is what many human\nmathematicians would do if they did not immediately see how to proceed (but presumably GPT-4 has seen\ntexts where this style of explanation is given for why telescoping sums work). It then correctly notes various\ncancellations, but demonstrates that it is not really understanding what it is saying by jumping to a final\nanswer that is both incorrect and easily seen to be inconsistent with what it has just said. For instance, it\nnotes that f(2) is added once and subtracted twice, but its final answer includes a term \u22122f(2) instead of\n\u2212f(2).\nI.3\nQualitative Observations from Case Studies\nWe next synthesise core insights from the case studies. Similar to MathConverse, we see that the models can\nbe good at providing definitions and interestingly can produce helpful scaffolding for a problem (e.g., the right\nstrategy or first few steps), even if details of the full proof are incorrect. Moreover, even if the model does\nproduce a correct solution, this solution is not always arrived at in a seemingly \u201chuman-like\u201d way; for instance,\nthe model may follow a guess-and-check approach rather than forward planning (e.g., Examples 1,4, and 7\nin Section I.2). However, guess-and-check cannot work well if one cannot \u201ccheck\u201d solutions; indeed, we see\nthat in general, challenges with algebraic manipulation plague in- and out-of-distribution performance (e.g.,\nall three examples in Section I). By probing GPT-4 capabilities on slightly novel problems or those which\ninvolve building examples, we notice the model\u2019s tendency to over-rely on memorised examples or patterns.\nThe case studies also reiterate potential issues with handling user corrections as well as a tendency towards\nover-verbosity. Further, one case study illuminated intriguing behaviour when the model was queried about\nuncertainty; i.e., the model began to apologise despite having been correct (see Section I.1).\n51\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12\nInteraction Step\n1\n2\n3\n4\n5\n6\nCorrectness Rating\n0\n20\n40\n60\n80\nEngaged users at step\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12\nInteraction Step\n0\n2\n4\n6\nHelpfulness Rating\n0\n20\n40\n60\n80\nEngaged users at step\nFigure 9: Depiction of how the users\u2019 correctness and helpfulness ratings progress with time. On the horizontal axis\nis the index of the interaction step. The box plot represents the correctness/helpfulness rating distributions given by\nusers at each step (vertical axis on the left), while the blue shade indicates how many users are still active at that\ngiven step (vertical axis on the right). Note, few users undertook more than 9 interactions (3 people undertook 9\ninteractions, and only one person did 12), hence the boxplot collapses to a line with a single user.\nJ\nFull Interaction Traces from Expert Case Studies\nJ.1\nFull Interaction Traces from \u201cNumber Theory\u201d Case Studies\nProblem 21\n52\n53\nHow would I show that the equation 1+an = 2m has no solutions in the integers for m, n >\n1?\nTo prove that the equation 1 + an = 2m has no solutions in the integers for m, n > 1, we\ncan proceed by contradiction. We\u2019ll assume there are solutions in integers, and then show\nthat this assumption leads to a contradiction.\nLet 1 + an = 2m for some integers a, m, n with m, n > 1.\nFirst, notice that a must be odd since 1 + an is even. Let a = 2k + 1 for some integer k.\nNow, the equation becomes:\n1 + (2k + 1)n = 2m\nExpand the left side using the binomial theorem:\n(2k + 1)n =\nn\nX\ni=0\n\u0012(\nn\n\u0013\n, i)(2k)i = 1 + n(2k) +\nn\nX\ni=2\n\u0012(\nn\n\u0013\n, i)(2k)i\nNow, notice that each term in the sum Pn\ni=2\n\u0000n\ni\n\u0001\n(2k)i is divisible by 22 = 4.\nTherefore, we can write the sum as 4s for some integer s. Then, we have:\n1 + 2kn + 4s = 2m\n2kn = 2m \u2212 1 \u2212 4s\nCan you check your work and see if there are any errors in this solution\nApologies for the confusion in my previous response. You\u2019re right; there are errors in the\nsolution. Let\u2019s revise the proof.\nWe want to show that the equation 1 + an = 2m has no solutions in the integers for m, n >\n1.\nNotice that a must be odd since 1 + an is even. Let a = 2k + 1 for some integer k. Then,\nthe equation becomes:\n1 + (2k + 1)n = 2m\nExpanding the left side using the binomial theorem:\n(2k + 1)n =\nn\nX\ni=0\n\u0012n\ni\n\u0013\n(2k)i = 1 + n(2k) +\nn\nX\ni=2\n\u0012n\ni\n\u0013\n(2k)i\nNow, observe that each term in the sum Pn\ni=2\n\u0000n\ni\n\u0001\n(2k)i is divisible by 22 = 4, as well as\nthe term n(2k).\nTherefore, we can write:\n54\n1 + 2m = 1 + 4s\nfor some integer s.\nFrom this, we have 2m = 4s, which implies 2m\u22121 = s. Since m > 1, we have that m\u22121 \u2265 1.\nSo, s must be even. Let s = 2t for some integer t. Now we have:\n1 + an = 1 + 8t\nSubtracting 1 from both sides, we get:\nan = 8t\nSince the left side of the equation, an, is odd (because a is odd), the right side must also\nbe odd. However, the right side is even (because 8t is a multiple of 8), which is a contra-\ndiction. Therefore, there can be no solutions in integers for m, n > 1.\nProblem 23\nCan you prove that if a and m are coprime integers that a\u03d5(m) \u2261 1 (mod m) where \u03d5 is\nEuler\u2019s totient function?\nYes, I can prove that if a and m are coprime integers, then a\u03d5(m)\n\u2261\n1 (mod m). This\nresult is known as Euler\u2019s theorem. I will provide a proof using the concept of modular\narithmetic.\n1. Let\u2019s consider the set of integers less than m that are coprime to m, denoted by Rm =\n{r1, r2, . . . , r\u03d5(m)}. The cardinality of this set is \u03d5(m).\n2. Since a is coprime to m, multiplying each element of Rm by a will yield another set of\nintegers that are also coprime to m. We\u2019ll denote this new set as\nSm = {ar1, ar2, . . . , ar\u03d5(m)},\n3. Notice that Sm is a permutation of Rm, since each element in Sm is congruent to a\nunique element in Rm modulo m.\n4. Now, let\u2019s consider the product of all elements in Rm and Sm. We can write this as:\n\u03d5(m)\nY\ni=1\nri \u2261\n\u03d5(m)\nY\ni=1\n(ari)\n(mod m)\n5. We can factor out the a\u03d5(m) term from the product on the right-hand side:\n\u03d5(m)\nY\ni=1\nri \u2261 a\u03d5(m)\n\u03d5(m)\nY\ni=1\nri\n(mod m)\n6. Since Q\u03d5(m)\ni=1 ri is coprime to m, we can safely cancel out this term from both sides:\n55\na\u03d5(m) \u2261 1\n(mod m)\nCan you explain more fully why Rm is a permutation of Sm in step 3?\nCertainly! In step 3, we claim that Sm is a permutation of Rm. To explain this more\nfully, let\u2019s revisit the definitions of Rm and Sm.\nRm\n=\nr1, r2, . . . , r\u03d5(m) is the set of integers less than m that are coprime to m, and\nSm\n=\nar1, ar2, . . . , ar\u03d5(m) is the set obtained by multiplying each element of Rm by a,\nwhere a is an integer coprime to m.\nNow, we want to show that Sm is a permutation of Rm, which means that the elements in\nSm are just a rearrangement of the elements in Rm. To show this, we need to prove two\nthings:\n1. Every element of Sm is coprime to m.\n2. The elements of Sm are distinct modulo m.\n3. To prove that every element of Sm is coprime to m, let\u2019s take an arbitrary element\nari \u2208 Sm. Since a is coprime to m and ri is coprime to m (by the definition of Rm), their\nproduct ari must also be coprime to m. This is due to the property that the product of\ntwo coprime integers is coprime to their common divisor.\nTherefore, every element of Sm is coprime to m.\n4. To prove that the elements of Sm are distinct modulo m, let\u2019s assume that there are\ntwo distinct elements ari and arj in Sm such that ari \u2261 arj (mod m). Since a is coprime\nto m, we can multiply both sides of the congruence by a\u22121 (the modular inverse of a\nmodulo m).\nThis gives us ri \u2261 rj (mod m), which contradicts the fact that ri and rj are distinct ele-\nments of Rm. Thus, the elements of Sm must be distinct modulo m.\nSince every element of Sm is coprime to m and the elements of Sm are distinct modulo m,\nwe can conclude that Sm is a permutation of Rm.\nIn your proof of 2, can you explain why a must have a modular inverse?\nApologies for not clarifying that part. In the proof of 2, we assume that a has a modular\ninverse modulo m. This is indeed the case, and I\u2019ll explain why.\nRecall that a and m are coprime integers, which means their greatest common divisor\n(GCD) is 1. A number a has a modular inverse modulo m if and only if the GCD of a\nand m is 1. In other words, a modular inverse of a modulo m exists if and only if a and\nm are coprime.\nSince a and m are coprime by assumption, there must exist integers x and y such that:\n56\nax + my = 1\nThis is a consequence of B\u00e9zout\u2019s identity, which states that the GCD of two integers can\nbe expressed as a linear combination of those integers. In our case, the GCD of a and m\nis 1.\nNow, if we take this equation modulo m, we get:\nax + my \u2261 1\n(mod m)\nSince my \u2261 0 (mod m), we have:\nax \u2261 1\n(mod m)\nThis means that x is the modular inverse of a modulo m. So, a modular inverse of a\nexists, which is essential for our proof.\nProblem 24\nLet \u03b6 be an irrational number. Show that there are infinitely many relatively prime inte-\ngers p and q such that |\u03b6 \u2212 p/q| <\n1\n\u221a\n5q2 .\nTo prove that there are infinitely many relatively prime integers p and q such that\n|\u03b6 \u2212 p/q| <\n1\n\u221a\n5q2 , we can use the theory of continued fractions.\nEvery irrational number \u03b6 can be represented by an infinite simple continued fraction:\n\u03b6 = a0 +\n1\na1 +\n1\na2 +\n1\na3 + \u00b7 \u00b7 \u00b7\nwhere a0, a1, a2, . . . are integers, and ai > 0 for i > 0. The convergents of this continued\nfraction are a sequence of rational numbers pn\nqn that can be computed recursively as fol-\nlows:\npn = anpn\u22121 + pn\u22122\nand\nqn = anqn\u22121 + qn\u22122\nwith initial values p\u22121 = 1, p\u22122 = 0, q\u22121 = 0, and q\u22122 = 1. It is well known that the\nconvergents are reduced fractions, meaning that pn and qn are relatively prime.\nA result from the theory of continued fractions states that for any irrational number \u03b6\nand its convergent pn\nqn , we have:\n\f\f\f\f\u03b6 \u2212 pn\nqn\n\f\f\f\f <\n1\nqnqn+1\nNow, let\u2019s consider the sequence of convergents of \u03b6. Since there are infinitely many\nconvergents, we can find a convergent pn\nqn such that:\n57\nqnqn+1 >\n\u221a\n5q2\nn\nThis inequality is satisfied when qn+1 >\n\u221a\n5qn.\nBy the inequality above, we have:\n\f\f\f\f\u03b6 \u2212 pn\nqn\n\f\f\f\f <\n1\nqnqn+1\n<\n1\n\u221a\n5q2n\nThus, we have shown that there are infinitely many relatively prime integers p and q such\nthat\n\f\f\f\u03b6 \u2212 p\nq\n\f\f\f <\n1\n\u221a\n5q2 .\nHow can we prove that there will always be an n for which qn+1 >\n\u221a\n5qn?\nTo show that there will always be an n for which qn+1\n>\n\u221a\n5qn, we can exploit the fact\nthat the continued fraction expansion contains arbitrarily large partial quotients.\nLet\u2019s assume the contrary: Suppose that for all n, we have qn+1 \u2264\n\u221a\n5qn. Then, we would\nhave:\nqn+1 \u2264\n\u221a\n5qn \u2264 5qn\u22121 \u2264 \u00b7 \u00b7 \u00b7 \u2264 5n+1q0\nThis inequality implies that the denominators qn are bounded by an exponential function,\nwhich contradicts the fact that the denominators of the convergents of an irrational num-\nber grow at least linearly.\nThe reason why the denominators grow at least linearly can be seen by looking at the\nrecursive formula for qn:\nqn = anqn\u22121 + qn\u22122\nFor an \u2265 1, we have:\nqn \u2265 qn\u22121 + qn\u22122 \u2265 qn\u22121\nThus, the sequence of denominators is non-decreasing. Moreover, if an\u2032 \u2265 2 for some n\u2032,\nwe have:\nqn\u2032+1 = an\u2032+1qn\u2032 + qn\u2032\u22121 \u2265 qn\u2032 + qn\u2032\u22121 \u2265 2qn\u2032\nThis shows that the denominators grow at least linearly with n. So, we must have an n\nfor which qn+1 >\n\u221a\n5qn, since otherwise, we would have a contradiction with the growth of\nthe denominators.\nProblem 27\nSuppose that p is a prime number. Can you show that (a + b)p \u2261 ap + bp (mod p) for all\nintegers a and b?\nYes, we can show that (a + b)p \u2261 ap + bp (mod p) for all integers a and b if p is a prime\nnumber. We will use Fermat\u2019s Little Theorem and the binomial theorem to prove this.\n58\nFermat\u2019s Little Theorem states that if p is a prime number and a is an integer not divisi-\nble by p, then ap\u22121 \u2261 1 (mod p). Thus, ap \u2261 a (mod p).\nNow, let\u2019s expand (a + b)p using the binomial theorem:\n(a + b)p =\np\nX\nk=0\n\u0012p\nk\n\u0013\nap\u2212kbk\nWe know that for 0 < k < p, the binomial coefficient\n\u0000p\nk\n\u0001\n=\np!\nk!(p\u2212k)! is an integer. Since\np is a prime, it only has two divisors: 1 and p. Thus, if 0\n<\nk\n<\np, then p must be a\ndivisor of\n\u0000p\nk\n\u0001\n, meaning that the binomial coefficient is divisible by p. We can rewrite this\nas\n\u0000p\nk\n\u0001\n\u2261 0 (mod p) for 0 < k < p.\nSo, when we consider the expansion of (a + b)p modulo p, all the terms in the sum with 0 <\nk < p will be equivalent to 0 modulo p. Therefore, we are left with only two terms:\n(a + b)p \u2261\n\u0012p\n0\n\u0013\napb0 +\n\u0012p\np\n\u0013\na0bp\n(mod p)\nSince\n\u0000p\n0\n\u0001\n=\n\u0000p\np\n\u0001\n= 1, this simplifies to:\n(a + b)p \u2261 ap + bp\n(mod p)\nAnd that completes the proof.\nProblem 28\nCan you show that\n3 = 1 \u00d7\nr\n1 + 2 \u00d7\nq\n1 + 3 \u00d7 \u221a. . .\nTo show that\n3 = 1 \u00d7\nr\n1 + 2 \u00d7\nq\n1 + 3 \u00d7 \u221a. . .,\nwe can start by defining the expression as a series with the following recursion relation:\nLet an = \u221a1 + n \u00d7 an+1, where\na1 = 1 \u00d7\nr\n1 + 2 \u00d7\nq\n1 + 3 \u00d7 \u221a. . .\nOur goal is to show that a1 = 3.\nWe can proceed using mathematical induction to prove this statement. First, let\u2019s con-\nsider the base case, n = 1:\n59\na1 =\n\u221a\n1 + 1 \u00d7 a2.\nNow, let\u2019s assume the induction hypothesis holds for some n = k:\nak =\np\n1 + k \u00d7 ak+1\nWe want to show that this also holds for n = k + 1:\nak+1 =\np\n1 + (k + 1) \u00d7 ak+2\nFrom the induction hypothesis, we can substitute ak+1:\np\n1 + k \u00d7 ak+1 =\nq\n1 + k \u00d7\np\n1 + (k + 1) \u00d7 ak+2.\nNow, let\u2019s square both sides of the equation:\n1 + k \u00d7 ak+1 = 1 + k \u00d7\np\n1 + (k + 1) \u00d7 ak+2\nNext, subtract 1 from both sides:\nk \u00d7 ak+1 = k \u00d7\np\n1 + (k + 1) \u00d7 ak+2\nNow, divide both sides by k:\nak+1 =\np\n1 + (k + 1) \u00d7 ak+2\nSo, the induction step holds for n = k + 1.\nNow, let\u2019s see if we can find a pattern for the first few values of an:\na1 =\n\u221a\n1 + 1 \u00d7 a2\na2 =\n\u221a\n1 + 2 \u00d7 a3\na3 =\n\u221a\n1 + 3 \u00d7 a4\nWe can substitute the expression for a3 into the expression for a2:\na2 =\nq\n1 + 2 \u00d7\n\u221a\n1 + 3 \u00d7 a4.\nNow substitute the expression for a2 into the expression for a1:\na1 =\nr\n1 + 1 \u00d7\nq\n1 + 2 \u00d7\n\u221a\n1 + 3 \u00d7 a4.\nUnfortunately, there is no simple way to close the recursion and prove that a1 = 3 with-\nout making additional assumptions or using more advanced mathematical techniques.\nThis problem is known as a nested radical, and finding a closed-form expression for such\nproblems can be challenging.\nCan you use the known value of a1 to figure out what the value of a2 would have to be\nand so on, and perhaps find a pattern?\nSure! Given that a1 = 3, let\u2019s find the value of a2 and subsequent values using the recur-\nsive relation.\n60\na1 = 3 =\n\u221a\n1 + 1 \u00d7 a2\na2 =\n\u221a\n1 + 2 \u00d7 a3\na3 =\n\u221a\n1 + 3 \u00d7 a4\na4 =\n\u221a\n1 + 4 \u00d7 a5\nWe know the value of a1, so let\u2019s find the value of a2:\n3 =\n\u221a\n1 + 1 \u00d7 a2\nSquare both sides:\n9 = 1 + a2\nSolve for a2:\na2 = 8\nNow we can find the value of a3:\na2 = 8 =\n\u221a\n1 + 2 \u00d7 a3\nSquare both sides:\n64 = 1 + 2 \u00d7 a3\nSubtract 1:\n63 = 2 \u00d7 a3\nDivide by 2:\na3 = 31.5\nNow, find the value of a4:\na3 = 31.5 =\n\u221a\n1 + 3 \u00d7 a4\nSquare both sides:\n992.25 = 1 + 3 \u00d7 a4\nSubtract 1:\n991.25 = 3 \u00d7 a4\nDivide by 3:\na4 = 330.4166\nSo far, the sequence of values is:\na1 = 3, a2 = 8, a3 = 31.5, a4 = 330.4166\nUnfortunately, there doesn\u2019t seem to be a simple pattern in the sequence. Nested radicals\nlike this one can often lead to complex and difficult-to-predict sequences.\n61\nI believe you have the relations between the various ai incorrect. Can you go back to the\noriginal question and correct these and try again.\nApologies for the confusion. Let\u2019s go back and redefine the recursive relation correctly:\nLet an = \u221a1 + n \u00d7 an\u22121, with a0 = 0.\nWe want to show that\na1 = 1 \u00d7\nr\n1 + 2 \u00d7\nq\n1 + 3 \u00d7 \u221a. . . = 3.\nLet\u2019s find the first few values of an:\na1 =\n\u221a\n1 + 1 \u00d7 a0 =\n\u221a\n1 + 0 =\n\u221a\n1 = 1\na2 =\n\u221a\n1 + 2 \u00d7 a1 =\n\u221a\n1 + 2 \u00d7 1 =\n\u221a\n3\na3 =\n\u221a\n1 + 3 \u00d7 a2 =\nq\n1 + 3 \u00d7\n\u221a\n3\na4 =\n\u221a\n1 + 4 \u00d7 a3 =\nr\n1 + 4 \u00d7\nq\n1 + 3 \u00d7\n\u221a\n3\nNow, let\u2019s substitute the expressions for an into the original equation:\na1 = 1 \u00d7\nr\n1 + 2 \u00d7\nq\n1 + 3 \u00d7 \u221a. . . = 1 \u00d7\n\u221a\n1 + 2 \u00d7 a2\nSince a2 =\n\u221a\n3, we have:\na1 = 1 \u00d7\nq\n1 + 2 \u00d7\n\u221a\n3\nUnfortunately, this does not simplify to a1 = 3. Instead, we have a nested radical expres-\nsion for a1, and we can\u2019t find a closed-form expression for this expression without making\nadditional assumptions or using more advanced mathematical techniques.\nIt seems that the original statement, 3 = 1 \u00d7\np\n(1 + 2 \u00d7\np\n(1 + 3 \u00d7\np\n( . . .))), cannot be\ndirectly proven with the methods I have attempted so far.\n62\nJ.2\nFull Interaction Traces from \u201cProblem Perturbation to Probe Memorisation\u201d\nCase Studies\nProblem 25\nLet a, b \u2208 N>0 such that there exists no m, n \u2208 N>0 such that am = bn.\nProve that logb a is irrational.\nWe will prove this by contradiction. Suppose that logb a is rational. Then we can write\nlogb a = p\nq , where p and q are coprime integers and q > 1.\nBy definition of logarithm, we have:\nb\np\nq = a\nRaise both sides to the power of q:\n(b\np\nq )q = aq\nbp = aq\nSince a, b \u2208 N>0, it follows that aq and bp are both positive integers. But this contradicts\nthe initial assumption that there exists no m, n \u2208 N>0 such that am = bn. In this case, we\nfound that aq = bp where q, p \u2208 N>0.\nTherefore, our assumption that logb a is rational must be false, which implies that logb a\nis irrational.\nWhat is the definition of logarithm here?\nThe logarithm is a mathematical function that helps us find the exponent to which a\ngiven base must be raised to produce a specific number. In general, the logarithm with\nbase b of a positive number a is denoted as logb a and is defined as follows:\nGiven a positive real number a, and a positive base b \u0338= 1, the logarithm of a with base b,\ndenoted as logb a, is the exponent to which b must be raised to obtain the number a. In\nother words, if:\nbx = a\nThen:\nx = logb a\nThe logarithm function has various properties and is widely used in many branches of\nmathematics, science, and engineering. The most common logarithm bases are base 10\n(common logarithm) and base e (natural logarithm), denoted as log a and ln a, respec-\ntively.\nCan a logarithm take a base less than 1?\nYes, a logarithm can have a base between 0 and 1, but not equal to 0 or 1. In general, the\nlogarithm function with base b is well-defined for any positive real number b \u0338= 1.\n63\nWhen the base b is between 0 and 1 (i.e., 0 < b < 1), the logarithm function has a differ-\nent behaviour compared to when the base is greater than 1. Specifically, the logarithm\nfunction is decreasing (i.e., its value decreases as the input increases) when the base is\nbetween 0 and 1. This is because a number between 0 and 1 raised to a higher power\nbecomes smaller, while a number between 0 and 1 raised to a smaller power becomes\nlarger.\nFor example, consider the logarithm with base 1\n2:\nlog 1\n2 a\nIf a > 1, then we need a negative exponent to make the base 1\n2 equal to a. In other words,\nthe logarithm will be negative for values of a > 1. As a increases, the magnitude of the\nnegative exponent required to make 1\n2 equal to a also increases, so the logarithm function\nis decreasing in this case.\nWhat is the meaning of N>0 in the previous proof?\nIn the previous proof, the notation N>0 denotes the set of natural numbers greater than 0.\nThe set of natural numbers N typically includes all non-negative integers, starting from 0:\nN = {0, 1, 2, 3, 4, ...}\nHowever, sometimes we want to consider only positive integers (excluding 0). In this case,\nwe use the notation N>0 to represent the set of all positive integers:\nN>0 = {1, 2, 3, 4, ...}\nIn the context of the previous proof, a, b \u2208 N>0 means that both a and b belong to the set\nof positive integers.\nCan we still finish the proof if we instead have a, b \u2208 R>0?\nYes, we can still complete the proof for a, b \u2208 R>0, i.e., a and b are positive real numbers.\nWe will use a similar approach, proving by contradiction. Suppose that logb a is rational.\nThen we can write logb a = p\nq , where p and q are coprime integers and q > 1.\nBy definition of logarithm, we have:\nb\np\nq = a\nRaise both sides to the power of q:\n(b\np\nq )q = aq\nbp = aq\nSince a, b \u2208 R>0, it follows that aq and bp are both positive real numbers. However, this\ncontradicts the initial assumption that there exists no m, n \u2208 N>0 such that am = bn. In\nthis case, we found that aq = bp where q, p \u2208 N>0.\n64\nTherefore, our assumption that logb a is rational must be false, which implies that logb a\nis irrational. Note that this proof holds for both positive integers and positive real num-\nbers.\nCould logb a = p\nq be a negative number?\nYes, logb a\n=\np\nq can be a negative number, depending on the values of a and b. Recall\nthat the logarithm of a number a with base b is the exponent to which b must be raised\nto obtain the number a. In other words, if bx = a, then x = logb a.\nIf a is a number between 0 and 1 (i.e., 0 < a < 1) and b > 1, then the logarithm logb a\nwill be negative. This is because, to obtain a number between 0 and 1 when the base b is\ngreater than 1, we need to raise b to a negative exponent. In this case, x (or logb a) will\nbe negative.\nFor example, consider the following logarithm:\nlog2\n1\n4\nWe know that 2\u22122 = 1\n4, so in this case, log2\n1\n4 = \u22122. Here, logb a is a negative number.\nIn general, logb a can be positive, negative, or even zero depending on the values of a\nand b. It\u2019s important to note that when the base b is between 0 and 1, the relationship\nbetween a and the sign of logb a is reversed; that is, when a\n>\n1, the logarithm will be\nnegative, and when 0 < a < 1, the logarithm will be positive.\nHow about if we drop the assumption that b \u2208 R>0? Does this affect the original proof?\nIf we drop the assumption that b \u2208 R>0, then b could be a non-positive real number or\nnot a real number at all. In either case, the original proof will be affected, as the loga-\nrithm function is not well-defined for bases that are not positive real numbers different\nfrom 1.\n1. If b\n\u2264\n0, the logarithm function is not defined for negative or zero bases. In this\ncase, the original proof would not hold, as the logarithm function cannot be used\nwith such bases.\n2. If b is not a real number (e.g., a complex number), the original proof would also\nnot hold. The logarithm function for complex numbers, called the complex loga-\nrithm, behaves differently from the real logarithm function. The properties of real\nlogarithms, like the monotonic behaviour and properties used in the proof, may not\napply to complex logarithms.\nSo, in order to use the original proof, it is essential to maintain the assumption that b \u2208\nR>0 and b \u0338= 1.\nHow about the assumption that c \u2208 R>0? Can we still proceed with the original proof if\nwe don\u2019t have this assumption?\n65\nIt seems that you\u2019re introducing a new variable c. To clarify the context of your question,\nlet\u2019s restate the original problem: We have a, b\n\u2208\nR>0 (or a, b\n\u2208\nN>0 in the original\nquestion) such that there exists no m, n \u2208 N>0 such that am = bn. The goal is to prove\nthat logb a is irrational.\nIf you meant to ask about either a or b instead of introducing a new variable c, then\nplease refer to my previous responses addressing the effects of dropping the assumptions\non a and b.\nIf you indeed intended to introduce a new variable c and are asking about its effect on\nthe original proof without specifying its role, it is not clear how c would be related to the\nproblem or the proof. Please provide more context or clarify the role of c in the problem,\nand I\u2019d be happy to help.\nSorry I made a mistake. What I meant was to drop the assumption that a \u2208 R>0. Can\nwe still finish the proof?\nIf we drop the assumption that a \u2208 R>0, then a could be a non-positive real number or\nnot a real number at all. In either case, the original proof will be affected, as the loga-\nrithm function is not well-defined for non-positive input values.\n1. If a \u2264 0, the logarithm function is not defined for negative or zero input values. In\nthis case, the original proof would not hold, as the logarithm function cannot be\nused with such input values.\n2. If a is not a real number (e.g., a complex number), the original proof would also not\nhold. The logarithm function for complex numbers, called the complex logarithm,\nbehaves differently from the real logarithm function. The properties of real loga-\nrithms, like the properties used in the proof, may not apply to complex logarithms.\nSo, in order to use the original proof, it is essential to maintain the assumption that a \u2208\nR>0.\nProblem 39 (Chebyshev\u2019s inequality)\nLet X be a random variable. Assume E(X) = \u00b5 for some \u00b5 \u2208 R and var(X) = \u03c32 for some\n\u03c32 \u2208 R>0.\nShow that for all k > 0:\nPr (|X \u2212 \u00b5| \u2265 k\u03c3) \u2264 1\nk2 .\nTo show this, we will use the Chebyshev\u2019s inequality. Chebyshev\u2019s inequality states that\nfor any random variable Y with expected value E(Y ) = m and variance var(Y ) = v2:\nPr (|Y \u2212 m| \u2265 kv) \u2264 1\nk2\nfor all k > 0.\n66\nNow, let\u2019s apply Chebyshev\u2019s inequality to the given random variable X. We are given\nthat E(X) = \u00b5 and var(X) = \u03c32. Thus, we have:\nPr (|X \u2212 \u00b5| \u2265 k\u03c3) \u2264 1\nk2\nThis proves the desired result.\nWill the proof still hold when k = 2?\nYes, the proof still holds when k = 2. Chebyshev\u2019s inequality is valid for all k > 0, which\nincludes k = 2. When k = 2, the inequality states:\nPr (|X \u2212 \u00b5| \u2265 2\u03c3) \u2264 1\n22 = 1\n4\nThis means that the probability that the random variable X deviates from its mean \u00b5 by\nmore than 2\u03c3 (twice the standard deviation) is at most 1\n4, or 25%. In other words, there\nis at least a 75% chance that X lies within the interval (\u00b5 \u2212 2\u03c3, \u00b5 + 2\u03c3).\nHow about k =\n\u221a\n2? Will this affect the derivation?\nYes, the derivation still holds for k =\n\u221a\n2. Chebyshev\u2019s inequality is valid for any positive\nvalue of k, including k =\n\u221a\n2. When k =\n\u221a\n2, the inequality states:\nPr\n\u0010\n|X \u2212 \u00b5| \u2265\n\u221a\n2\u03c3\n\u0011\n\u2264\n1\n(\n\u221a\n2)2 = 1\n2\nThis means that the probability that the random variable X deviates from its mean \u00b5\nby more than\n\u221a\n2\u03c3 (approximately 1.414 times the standard deviation) is at most 1\n2,\nor 50%. In other words, there is at least a 50% chance that X lies within the interval\n(\u00b5 \u2212\n\u221a\n2\u03c3, \u00b5 +\n\u221a\n2\u03c3).\nSo, the derivation remains valid for k =\n\u221a\n2 and any other positive value of k.\nWhat if k =\n\u221a\n2 \u2212 1?\nThe derivation still holds for k =\n\u221a\n2 \u2212 1. Chebyshev\u2019s inequality is valid for any positive\nvalue of k, which includes k =\n\u221a\n2 \u2212 1. When k =\n\u221a\n2 \u2212 1, the inequality states:\nPr\n\u0010\n|X \u2212 \u00b5| \u2265 (\n\u221a\n2 \u2212 1)\u03c3\n\u0011\n\u2264\n1\n(\n\u221a\n2 \u2212 1)2\nThis means that the probability that the random variable X deviates from its mean \u00b5\nby more than (\n\u221a\n2 \u2212 1)\u03c3 (approximately 0.414 times the standard deviation) is at most\n1\n(\n\u221a\n2 \u2212 1)2 .\nIn other words, there is at least a 1 \u2212\n1\n(\n\u221a\n2 \u2212 1)2 chance that X lies within the interval (\u00b5 \u2212\n(\n\u221a\n2 \u2212 1)\u03c3, \u00b5 + (\n\u221a\n2 \u2212 1)\u03c3).\n67\nSo, the derivation remains valid for k =\n\u221a\n2 \u2212 1 and any other positive value of k.\nWhat if k =\n\u221a\n2 \u2212 2?\nWhen k =\n\u221a\n2 \u2212 2, the value of k becomes negative because\n\u221a\n2 < 2. Since Chebyshev\u2019s\ninequality is only valid for k > 0, it does not apply when k =\n\u221a\n2 \u2212 2.\nIn this case, you cannot use Chebyshev\u2019s inequality to derive a bound on the probability\nof the random variable X deviating from its mean by a factor of (\n\u221a\n2 \u2212 2)\u03c3.\nWhat if k = (\n\u221a\n2 \u2212 2)2?\nWhen k = (\n\u221a\n2 \u2212 2)2, the value of k is positive because (\n\u221a\n2 \u2212 2)2 > 0. Since Chebyshev\u2019s\ninequality is valid for k > 0, we can apply it in this case. When k = (\n\u221a\n2 \u2212 2)2, the inequal-\nity states:\nPr\n\u0010\n|X \u2212 \u00b5| \u2265 (\n\u221a\n2 \u2212 2)2\u03c3\n\u0011\n\u2264\n1\n(\n\u221a\n2 \u2212 2)4\nThis means that the probability that the random variable X deviates from its mean \u00b5 by\nmore than (\n\u221a\n2\n\u2212\n2)2\u03c3 (approximately 0.1716 times the standard deviation) is at most\n1\n(\n\u221a\n2 \u2212 2)4 .\nIn other words, there is at least a 1 \u2212\n1\n(\n\u221a\n2 \u2212 2)4 chance that X lies within the interval (\u00b5 \u2212\n(\n\u221a\n2 \u2212 2)2\u03c3, \u00b5 + (\n\u221a\n2 \u2212 2)2\u03c3).\nSo, the derivation remains valid for k = (\n\u221a\n2 \u2212 2)2 and any other positive value of k.\nAre you sure (\n\u221a\n2 \u2212 2)2 > 0?\nApologies for the confusion in my previous response. You are right; my statement was\nincorrect.\nWhen k = (\n\u221a\n2\u22122)2, the value of k is indeed non-negative, but it is actually equal to 0, not\ngreater than 0. Here is the correct calculation:\n(\n\u221a\n2 \u2212 2)2 = (1.414 \u2212 2)2 = (\u22120.586)2 = 0.343396\nSince 0.343396 > 0, the value of k is positive in this case, and Chebyshev\u2019s inequality can\nbe applied.\nWhen k = (\n\u221a\n2 \u2212 2)2, the inequality states:\n68\nPr\n\u0010\n|X \u2212 \u00b5| \u2265 (\n\u221a\n2 \u2212 2)2\u03c3\n\u0011\n\u2264\n1\n(\n\u221a\n2 \u2212 2)4\nThis means that the probability that the random variable X deviates from its mean \u00b5 by\nmore than (\n\u221a\n2\n\u2212\n2)2\u03c3 (approximately 0.3434 times the standard deviation) is at most\n1\n(\n\u221a\n2 \u2212 2)4 .\nIn other words, there is at least a 1 \u2212\n1\n(\n\u221a\n2 \u2212 2)4 chance that X lies within the interval (\u00b5 \u2212\n(\n\u221a\n2 \u2212 2)2\u03c3, \u00b5 + (\n\u221a\n2 \u2212 2)2\u03c3).\nSo, the derivation remains valid for k = (\n\u221a\n2 \u2212 2)2 and any other positive value of k.\nWhy do you say your previous statement was incorrect and k = 0? I don\u2019t understand.\nI apologize for any confusion my previous responses may have caused. I made a mistake\nin my explanation. Let me clarify the situation:\nWhen k = (\n\u221a\n2 \u2212 2)2, the value of k is indeed positive, as I mentioned in my previous re-\nsponse. Here is the correct calculation:\n(\n\u221a\n2 \u2212 2)2 = (1.414 \u2212 2)2 = (\u22120.586)2 = 0.343396\nSince 0.343396 > 0, the value of k is positive in this case, and Chebyshev\u2019s inequality can\nbe applied.\nWhen k = (\n\u221a\n2 \u2212 2)2, the inequality states:\nPr\n\u0010\n|X \u2212 \u00b5| \u2265 (\n\u221a\n2 \u2212 2)2\u03c3\n\u0011\n\u2264\n1\n(\n\u221a\n2 \u2212 2)4\nThis means that the probability that the random variable X deviates from its mean \u00b5 by\nmore than (\n\u221a\n2\n\u2212\n2)2\u03c3 (approximately 0.3434 times the standard deviation) is at most\n1\n(\n\u221a\n2 \u2212 2)4 .\nIn other words, there is at least a 1 \u2212\n1\n(\n\u221a\n2 \u2212 2)4 chance that X lies within the interval (\u00b5 \u2212\n(\n\u221a\n2 \u2212 2)2\u03c3, \u00b5 + (\n\u221a\n2 \u2212 2)2\u03c3).\nSo, the derivation remains valid for k = (\n\u221a\n2 \u2212 2)2 and any other positive value of k.\n69\nJ.3\nFull Interaction Traces from \u201cBoundary of Easy and Hard\u201d Case Studies\nWe include the full interaction traces from case studies designed and undertaken by Timothy Gowers in\nSection I.2.\nExample 1 (from section I.2)\nCan you find an example of a set A of positive integers that satisfies the following three\nconditions?\n1. |A| = min(A).\n2. A consists of prime numbers only.\n3. A has cardinality at least 5.\nYes, here\u2019s an example of a set A of positive integers that satisfies the given conditions:\nA = {5, 11, 17, 23, 29}\n1. The cardinality of A is 5, which is also the minimum element in A.\n2. All elements in A are prime numbers.\n3. The cardinality of A is 5, which is at least 5.\nTherefore, set A = {5, 11, 17, 23, 29} meets all three conditions.\nCan you find an example of a set A of positive integers that satisfies the following three\nconditions?\n1. |A| = min(A).\n2. A consists of prime numbers only.\n3. A has cardinality at least 6.\nYes, here is an example of a set A of positive integers that satisfies the given conditions:\nA = 7, 11, 13, 17, 19, 23\n1. |A| = 6, and the minimum element in A is 7, so |A| = min(A).\n2. All elements in A are prime numbers: 7, 11, 13, 17, 19, and 23.\n3. A has cardinality 6, which is at least 6.\nThe set A satisfies all three conditions.\n6 does not equal 7.\n70\nYou are correct, my previous response contained an error. I apologize for the confusion.\nIt is not possible to construct a set A of positive integers that satisfies all three conditions\nsimultaneously.\nTo see why, let\u2019s consider the conditions:\n1. |A| = min(A): This implies that the minimum element of the set A is equal to the\nsize of the set A.\n2. A consists of prime numbers only: This implies that every element of A is a prime\nnumber.\n3. A has cardinality at least 6: This implies that A has 6 or more elements.\nThe smallest prime number is 2. If the set A consists of prime numbers only, the mini-\nmum element of A must be greater than or equal to 2. However, if |A| = min(A) and A\nhas at least 6 elements, the minimum element in A would need to be at least 6, which is\nnot prime. This creates a contradiction, making it impossible to construct a set A that\nsatisfies all three conditions simultaneously.\nExample 2 (from section I.2)\nCan you work out the integral\nR 1\n\u22121 x3 cos xdx?\nTo solve the integral, we can use integration by parts. Integration by parts states that for\ntwo differentiable functions u and v:\nZ\nu dv = uv \u2212\nZ\nv du.\nLet\u2019s choose u = x3 and dv = cos x dx. Then, we compute du and v:\ndu = d\ndx(x3)dx = 3x2dx,\nv =\nZ\ncos x dx = sin x.\nNow, using integration by parts:\nZ 1\n\u22121\nx3 cos x dx =\n\u0002\nx3 sin x\n\u00031\n\u22121 \u2212\nZ 1\n\u22121\nsin x(3x2)dx.\nLet\u2019s evaluate the first term:\n\u0002\nx3 sin x\n\u00031\n\u22121 = (13 sin(1) \u2212 (\u22121)3 sin(\u22121)) = sin(1) + sin(\u22121).\nFor the remaining integral, we apply integration by parts again. This time, let u = 3x2\nand dv = sin x dx. Then, we compute du and v:\ndu = d\ndx(3x2)dx = 6xdx,\nv =\nZ\nsin x dx = \u2212 cos x.\nNow, using integration by parts:\nZ 1\n\u22121\n3x2 sin x dx =\n\u0002\n\u22123x2 cos x\n\u00031\n\u22121 \u2212\nZ 1\n\u22121\n\u22126x cos x dx.\n71\nEvaluating the first term:\n\u0002\n\u22123x2 cos x\n\u00031\n\u22121 = \u22123(12) cos(1) \u2212 (\u22123(\u22121)2 cos(\u22121)) = \u22123 cos(1) + 3 cos(\u22121).\nFor the remaining integral, notice that the integrand \u22126x cos x is an odd function. The\nintegral of an odd function over a symmetric interval around zero is always zero:\nZ 1\n\u22121\n\u22126x cos x dx = 0.\nSo, our final result is:\nZ 1\n\u22121\nx3 cos x dx = sin(1) + sin(\u22121) \u2212 3 cos(1) + 3 cos(\u22121) + 0 = 2 sin(1) \u2212 6 cos(1).\nWhat is sin(1) + sin(\u22121)?\nTo find the value of sin(1) + sin(\u22121), we can use the property of the sine function that\nstates that sin(\u2212x) = \u2212 sin(x) for any real number x.\nSo, sin(\u22121) = \u2212 sin(1).\nTherefore,\nsin(1) + sin(\u22121) = sin(1) \u2212 sin(1) = 0.\nHow about \u22123 cos(1) + 3 cos(\u22121)?\nTo find the value of \u22123 cos(1) + 3 cos(\u22121), we can use the property of the cosine function\nthat states that cos(\u2212x) = cos(x) for any real number x.\nSo, cos(\u22121) = cos(1).\nTherefore,\n\u22123 cos(1) + 3 cos(\u22121) = \u22123 cos(1) + 3 cos(1) = 0.\nIn the light of that, would you like to revise your final answer above? (Please do not\nrepeat the entire calculation \u2013 the rest was fine and just the last step was a problem.)\nApologies for the confusion in my previous response. Given that sin(1) + sin(\u22121) = 0 and\n\u22123 cos(1) + 3 cos(\u22121) = 0, the final answer for the integral should be:\nZ 1\n\u22121\nx3 cos x dx = 2 sin(1) \u2212 6 cos(1) + 0 = 2 sin(1) \u2212 6 cos(1).\nHowever, as we just found out, the correct values for these sums are both 0. Therefore,\nthe correct answer is:\nZ 1\n\u22121\nx3 cos x dx = 0.\nCan you establish that in an easier way?\n72\nYes, there is an easier way to establish that the integral is zero by noticing that the inte-\ngrand is an odd function.\nAn odd function is a function that satisfies the property f(\u2212x) = \u2212f(x) for all x. The\nproduct of an odd function and an even function is an odd function. In this case, x3 is\nan odd function and cos x is an even function. Therefore, their product x3 cos x is an odd\nfunction.\nNow, when we integrate an odd function over a symmetric interval around zero (like\n[\u22121, 1]), the integral is always zero. This is because the positive and negative areas under\nthe curve cancel each other out.\nSo, for the given integral:\nZ 1\n\u22121\nx3 cos x dx = 0\nbecause the integrand is an odd function and the interval is symmetric around zero.\nCan you calculate the integral\nR 1\n\u22121(x2 \u2212 1/3)dx?\nCertainly! Let\u2019s evaluate the given integral:\nZ 1\n\u22121\n(x2 \u2212 1\n3) dx.\nThe integral of a sum (or difference) of functions is equal to the sum (or difference) of\ntheir integrals. So we can split the integral into two parts:\nZ 1\n\u22121\nx2 dx \u2212\nZ 1\n\u22121\n1\n3 dx.\nTo find the antiderivative of x2, we can use the power rule for integration:\nZ\nxn dx = xn+1\nn + 1 + C.\nIn this case, n = 2. So the antiderivative of x2 is:\nx2+1\n2 + 1 = x3\n3 .\nNow, let\u2019s evaluate the first integral:\nZ 1\n\u22121\nx2 dx =\n\u0014x3\n3\n\u00151\n\u22121\n= 13\n3 \u2212 (\u22121)3\n3\n= 1\n3 + 1\n3 = 2\n3.\nFor the second integral, we have a constant function 1\n3. The antiderivative of a constant\nfunction c is cx + C. In this case, c = 1\n3, so the antiderivative is 1\n3x:\n73\nZ 1\n\u22121\n1\n3 dx =\n\u00141\n3x\n\u00151\n\u22121\n= 1\n3(1) \u2212 1\n3(\u22121) = 1\n3 + 1\n3 = 2\n3.\nNow, we can subtract the second integral from the first integral:\nZ 1\n\u22121\n(x2 \u2212 1\n3) dx = 2\n3 \u2212 2\n3 = 0.\nTherefore, the integral evaluates to 0.\nCan you see a simple reason that the integral had to be 0?\nYes, there is a simpler way to see that the integral had to be zero by noticing that the\nintegrand is an odd function.\nAn odd function is a function that satisfies the property f(\u2212x) = \u2212f(x) for all x. In this\ncase, consider the function:\nf(x) = x2 \u2212 1\n3.\nWhen we plug in \u2212x for x, we get:\nf(\u2212x) = (\u2212x)2 \u2212 1\n3 = x2 \u2212 1\n3 = \u2212(x2 \u2212 1\n3) = \u2212f(x).\nSo, f(x) = x2 \u2212 1\n3 is an odd function.\nNow, when we integrate an odd function over a symmetric interval around zero (like\n[\u22121, 1]), the integral is always zero. This is because the positive and negative areas under\nthe curve cancel each other out.\nSo, for the given integral:\nZ 1\n\u22121\n(x2 \u2212 1\n3) dx = 0\nbecause the integrand is an odd function and the interval is symmetric around zero.\nExample 3 (from section I.2)\nDoes there exist an order-preserving bijection between the rationals and the dyadic ratio-\nnals?\nNo, there does not exist an order-preserving bijection between the rationals and the\ndyadic rationals.\nThe rationals, denoted by Q, are the set of all numbers that can be expressed as the\nquotient of two integers, i.e., of the form a/b, where a and b are integers, and b \u0338= 0.\nThe dyadic rationals, denoted by Qd, are a subset of the rationals, and are the set of all\nnumbers that can be expressed as the quotient of two integers, where the denominator is\na power of 2, i.e., of the form a/2n, where a and n are integers, and n \u2265 0.\n74\nAn order-preserving bijection between two sets means that for every pair of elements x\nand y in one set, if x < y, then their corresponding images under the bijection will also\nsatisfy the same order relation.\nTo see why there cannot be an order-preserving bijection between Q and Qd, consider any\ntwo distinct dyadic rationals, say p and q, such that there is no dyadic rational between\nthem. In other words, p and q are consecutive dyadic rationals.\nHowever, between any two distinct rationals (including dyadic rationals), there exists an\ninfinite number of other rationals that are not dyadic. For example, between p and q,\nwe can find a rational of the form (p+q)/3, which is not a dyadic rational since it has a\ndenominator that is not a power of 2 (assuming p and q have distinct denominators).\nSince there are an infinite number of non-dyadic rationals between any two consecutive\ndyadic rationals, it is impossible to construct an order-preserving bijection between Q and\nQd. Any attempt to do so would necessarily \"skip\" some of the rationals, violating the\nrequirement of a bijection.\nCan you give me an example of a pair of dyadic rationals p and q with no dyadic rational\nbetween them?\nCertainly! Let\u2019s consider two dyadic rationals p and q:\np = 1/4\nq = 1/2\nThese can be expressed as:\np = 1 * 2\u02c6(-2)\nq = 1 * 2\u02c6(-1)\nThere is no dyadic rational between p and q because any other dyadic rational with a\ndenominator of 2 or 4 will either be smaller than p or larger than q. For example, if we\nconsider 3/4, it is greater than q. On the other hand, if we consider 0/4 or 1/4, both of\nthem are less than or equal to p.\nThus, p = 1/4 and q = 1/2 form a pair of consecutive dyadic rationals with no dyadic\nrational between them.\nIs the number 3/8 a dyadic rational?\nYes, the number 3/8 is a dyadic rational. A dyadic rational is a rational number of the\nform a/2\u02c6n, where a and n are integers, and n \u2265 0. In the case of 3/8, we can express it\nas:\n3/8 = 3 * 2\u02c6(-3)\nHere, a = 3 and n = 3, both of which are integers. Since the denominator is a power of 2\n(specifically, 2\u02c63), 3/8 is indeed a dyadic rational.\nDoes 3/8 lie between 1/4 and 1/2?\n75\nYes, the number 3/8 lies between 1/4 and 1/2. To see this, we can compare the numbers\nby finding a common denominator, which in this case is 8:\n1/4 = 2/8\n3/8 = 3/8\n1/2 = 4/8\nNow, it\u2019s clear that:\n2/8 < 3/8 < 4/8\nSo, 3/8 lies between 1/4 and 1/2.\nDoes there exist a dyadic rational that lies between 1/4 and 1/2?\nApologies for the confusion in my previous response. You are correct; there exists a\ndyadic rational between 1/4 and 1/2, which is 3/8.\n1/4 = 2/8\n3/8 = 3/8\n1/2 = 4/8\nAs shown above, 3/8 is a dyadic rational that lies between 1/4 and 1/2.\nOK, so can you give me an example of two consecutive dyadic rationals?\nI apologize for the confusion in my previous responses. The term \"consecutive dyadic\nrationals\" is not precise, as there is always another dyadic rational between any two given\ndyadic rationals. However, I can provide an example of two dyadic rationals that are\nclose to each other in terms of their position in the ordered set of dyadic rationals:\nLet p = 1/4 and q = 1/8. Both are dyadic rationals, as they have denominators that are\npowers of 2. Now, let\u2019s find the dyadic rational halfway between them:\nr = (p + q) / 2 = (1/4 + 1/8) / 2 = (2/8 + 1/8) / 2 = 3/16\nHere, r = 3/16 is another dyadic rational that lies between p and q. In this sense, p and\nq are \"close\" dyadic rationals, but there is still another dyadic rational between them. In\nfact, there will always be another dyadic rational between any two given dyadic rationals,\njust as there are other rationals between any two given rationals.\nIn your earlier proof you claimed, and used, that there exists a pair p, q of dyadic ratio-\nnals with no dyadic rational between them. Now that we have seen that that claim is\nfalse, I would like to ask again: is there an order-preserving bijection between the ratio-\nnals and the dyadic rationals?\nMy apologies for the confusion in my earlier response. You are right; my claim was incor-\nrect. Since there is no such pair of consecutive dyadic rationals with no dyadic rational\nbetween them, I must revisit the question.\n76\nGiven that there is always another dyadic rational between any two dyadic rationals, the\nsituation is actually similar to that of the rationals themselves. However, this does not\nguarantee the existence of an order-preserving bijection between the rationals (Q) and\nthe dyadic rationals (Qd). The crucial difference is that the dyadic rationals are a proper\nsubset of the rationals, and there are \"gaps\" in the sense that there are rationals that\ncannot be expressed as a/2\u02c6n.\nTo answer the question, it is important to clarify that there does not exist an order-\npreserving bijection between the rationals and the dyadic rationals. An order-preserving\nbijection would require that for every pair of elements x and y in one set, if x < y, then\ntheir corresponding images under the bijection would also satisfy the same order relation.\nSince Qd is a proper subset of Q, it is not possible to create an order-preserving bijection\nbetween them. If we tried to create such a bijection, we would necessarily \"skip\" some of\nthe rationals that are not dyadic, violating the requirement of a bijection.\nWhat is your proof that we would necessarily skip some of the non-dyadic rationals?\nYou are correct in questioning my previous response. I must clarify that my previous\nstatement was not properly justified. Let me address the question again.\nTo answer the question of whether there exists an order-preserving bijection between\nthe rationals (Q) and the dyadic rationals (Qd), we need to consider the properties of\norder-preserving bijections and the structure of the two sets.\nAn order-preserving bijection between two sets A and B is a bijective function f: A \u2192 B\nsuch that for every pair of elements x, y in A, if x < y, then f(x) < f(y). In other words,\nthe bijection preserves the order of the elements in the sets.\nThe dyadic rationals (Qd) are a proper subset of the rationals (Q), meaning that every\ndyadic rational is a rational, but not every rational is a dyadic rational. However, both Q\nand Qd are dense in the real numbers, which means that between any two real numbers,\nthere are both rational and dyadic rational numbers.\nUpon reflection, it turns out that there does exist an order-preserving bijection between\nthe rationals and the dyadic rationals. The key is to recognize that both sets are count-\nable, and we can create a listing of both sets in increasing order. The order-preserving\nbijection can be established by matching the elements in the two lists one-to-one.\nFor example, let\u2019s list a few elements of the rationals and the dyadic rationals in increas-\ning order:\nQ: -2, -3/2, -1, -1/2, 0, 1/2, 1, 3/2, 2, ...\nQd: -2, -3/2, -1, -1/2, 0, 1/2, 1, 3/2, 2, ...\nWe can define an order-preserving bijection f between Q and Qd by associating each ele-\nment in the Q list with the corresponding element in the Qd list. This bijection preserves\nthe order of the elements, and since both sets are countable, it is possible to match every\nelement in Q with an element in Qd, even though Qd is a proper subset of Q.\n77\nIn conclusion, there does exist an order-preserving bijection between the rationals and the\ndyadic rationals, and my previous claim was incorrect. I apologize for the confusion and\nappreciate your persistence in seeking a clear answer.\nIt seems to be a consequence of your previous answer that there is an order-preserving\nbijection between the integers and the rationals. Is that the case?\nYou are correct that my previous response is misleading, and I apologize for the confusion.\nThere is no order-preserving bijection between the integers and the rationals, and my\nprevious response does not imply that there is.\nUpon further reflection, I must correct my previous response regarding the order-\npreserving bijection between the rationals (Q) and the dyadic rationals (Qd).\nIn fact, there is no order-preserving bijection between the rationals and the dyadic ra-\ntionals. To see this, consider any dyadic rational p. There exists a non-dyadic rational\nq such that q > p, but there is no dyadic rational between p and q. For example, let p\nbe any dyadic rational, and let q = p + (1/3)p. Then, q is not a dyadic rational, since it\ninvolves a factor of 1/3, which cannot be expressed as a power of 2 in the denominator.\nNow, suppose there were an order-preserving bijection f between Q and Qd. Since f is a\nbijection, there must be a dyadic rational r such that f(r) = q. However, since f is also\norder-preserving, there must be a dyadic rational s such that p < s < r, which implies\nthat p < f(s) < q. This is a contradiction since there is no dyadic rational between p and\nq.\nTherefore, there cannot be an order-preserving bijection between the rationals and the\ndyadic rationals. I apologize for the confusion in my previous responses, and I appreciate\nyour patience in seeking a clear answer.\nSimilarly, there is no order-preserving bijection between the integers and the rationals,\nas there are infinitely many rationals between any two integers, making it impossible to\nestablish an order-preserving bijection between the two sets.\nHave you heard of back-and-forth arguments?\nYes, I am familiar with back-and-forth arguments. The back-and-forth method is a\ntechnique used in set theory, logic, and model theory to prove that two structures are\nisomorphic or that they satisfy some other relation. It is particularly useful for proving\nthe existence of isomorphisms or other relations between countable structures.\nThe back-and-forth method proceeds in a recursive way, building a relation (often an\nisomorphism) between two structures one element at a time. The construction alternates\nbetween the two structures, ensuring that the relation is consistent with the desired\nproperties at each step. This alternating procedure is the reason for the name \u201cback-and-\nforth.\u201d\n78\nOne of the classic applications of the back-and-forth method is the proof of the Cantor-\nSchr\u00f6der-Bernstein theorem, which states that if there are injections (one-to-one func-\ntions) between two sets A and B, then there exists a bijection between A and B. The\nback-and-forth method is used to construct the bijection in a step-by-step manner, ensur-\ning that the properties of a bijection are maintained throughout the process.\nIn the context of our previous discussion about order-preserving bijections between the\nrationals and the dyadic rationals, my earlier responses were not clear, and I apologize for\nany confusion. While it is true that both sets are countable, the back-and-forth method\ndoes not provide a direct way to establish an order-preserving bijection between the\nrationals and the dyadic rationals, as I previously claimed. The existence or non-existence\nof such a bijection is a more subtle question and requires a more nuanced argument.\nWhat happens if one tries to use the back-and-forth method to build a bijection between\nthe rationals and the dyadic rationals?\nIf we attempt to use the back-and-forth method to construct a bijection between the\nrationals (Q) and the dyadic rationals (Qd), we will be able to create a bijection, but it\nwill not be order-preserving.\nLet\u2019s go through the back-and-forth construction:\n1. Enumerate the elements of Q in a sequence {qn} and the elements of Qd in another\nsequence {d_n}.\n2. Start by assigning q1 to d1.\n3. For the next step, consider q2. If q2 is greater than q1, find the first unassigned\ndyadic rational greater than d1 and assign q2 to that dyadic rational. If q2 is less\nthan q1, find the first unassigned dyadic rational less than d1 and assign q2 to that\ndyadic rational.\n4. Continue this process, alternating between the next unassigned elements in the Q\nand Qd sequences.\nThe back-and-forth method will create a bijection between Q and Qd since both sets are\ncountable. However, this bijection will not be order-preserving, as the back-and-forth\nconstruction may assign a smaller rational to a larger dyadic rational or vice versa during\nthe process.\nIn summary, while the back-and-forth method can be used to build a bijection between\nthe rationals and the dyadic rationals, this bijection will not be order-preserving.\nWhy can one not just insist at each step that the order of the images is the same as the\norder of the original elements?\nYou are correct, and I apologize for the confusion in my previous responses. We can actu-\nally use the back-and-forth method to construct an order-preserving bijection between the\nrationals (Q) and the dyadic rationals (Qd). The key is to ensure that the order of the\nimages is the same as the order of the original elements at each step.\nHere\u2019s an outline of the back-and-forth construction:\n79\n1. Enumerate the elements of Q in a sequence {q_n} and the elements of Qd in an-\nother sequence {d_n}.\n2. Begin with an empty partial function f.\n3. At each step, we will extend the function f by selecting an element from each se-\nquence and making sure that the order is preserved.\n\u2022 Suppose we have defined f for n elements, and we are at the (n+1)-th step.\n\u2022 First, consider the (n+1)-th element of the Q sequence, qn+1. Assign it to\nthe first unassigned element in the Qd sequence, dn+1, such that the order is\npreserved; that is, for all i \u2264 n, qi < qn + 1 if and only if f(qi) < dn + 1.\n\u2022 Then, consider the (n+1)-th element of the Qd sequence, dn+1. Assign it to\nthe first unassigned element in the Q sequence, qn+1, such that the order is\npreserved; that is, for all i \u2264 n, di < dn+1 if and only if f ( \u2212 1)(di) < qn+1.\n4. Continue this process indefinitely.\nThis method ensures that at each step, the bijection is order-preserving, so the final\nbijection will also be order-preserving. I apologize for the confusion in my previous expla-\nnations, and I appreciate your persistence in seeking a clear answer.\nExample 4 (from section I.2)\nDoes there exist a positive integer n such that n + k is a multiple of k for every k between\n1 and 100?\nNo, there does not exist a positive integer n such that n + k is a multiple of k for every k\nbetween 1 and 100.\nTo see why, consider the least common multiple (LCM) of all the integers between 1 and\n100, denoted as LCM(1, 2, ..., 100). This number is divisible by every integer in that\nrange. Let\u2019s say there exists an integer n such that n + k is a multiple of k for every k\nbetween 1 and 100. In this case, n + k must also be divisible by LCM(1, 2, ..., 100) for\nevery k between 1 and 100.\nNow, consider the case where k = 1. We have n + 1 must be divisible by LCM(1, 2, ...,\n100). This implies that n is one less than a multiple of LCM(1, 2, ..., 100). We can write\nthis as:\nn = LCM(1, 2, ..., 100) \u2217 m \u2212 1, for some integer m.\nNext, consider the case where k = 2. We have n + 2 must be divisible by 2. However, since\nn = LCM(1, 2, ..., 100) \u2217 m \u2212 1, we have:\nn + 2 = LCM(1, 2, ..., 100) \u2217 m + 1\n80\nSince LCM(1, 2, ..., 100) is an even number (it is divisible by 2), the expression\nLCM(1, 2, ..., 100) \u2217 m + 1 is an odd number. Thus, n + 2 is odd, and not divisible\nby 2, which contradicts the original assumption that n + k is a multiple of k for every k\nbetween 1 and 100.\nHence, there does not exist a positive integer n such that n + k is a multiple of k for every\nk between 1 and 100.\nLet me clarify the logical structure of the property in question. Formally speaking the\nquestion is whether the following statement is true.\n\u2203n \u2208 N\u2200k \u2208 {1, 2, . . . , 100}k|n + k\nI apologize for the confusion, and I appreciate the clarification. Given the formal state-\nment:\n\u2203n \u2208 N\u2200k \u2208 {1, 2, . . . , 100}k|n + k\nThe answer is still no, there does not exist a positive integer n that satisfies this property.\nTo see why, let\u2019s consider the case where k = 2. For n + k to be divisible by 2 (i.e., even),\nn must be even, because an even number plus an even number always results in an even\nnumber. So, we can write n = 2m for some positive integer m.\nNow consider the case where k = 3. We want n+3 to be divisible by 3. Given that n = 2m,\nwe have:\n2m + 3\nFor this expression to be divisible by 3, m must be odd, because an even number plus an\nodd number will result in an odd number, and we want the sum to be even (a multiple of\n3). So, we can write m = 2p + 1 for some positive integer p.\nNow let\u2019s consider the case where k = 5. We want n + 5 to be divisible by 5. Given that\nn = 2(2p + 1), we have:\n2(2p + 1) + 5 = 4p + 7\nHowever, no matter the value of p, the expression 4p + 7 will always result in a number that\nis not divisible by 5. This contradicts the original assumption that k|n + k for every k be-\ntween 1 and 100.\nHence, there does not exist a positive integer n such that \u2200k \u2208 {1, 2, . . . , 100}k|n + k.\nLet\u2019s focus on just one k. If n + k is divisible by k, what can we say about n?\nIf n + k is divisible by k, that means there exists an integer m such that n + k = k \u00b7 m. In\nthis case, we can rewrite the equation as:\nn = k \u00b7 m \u2212 k\n81\nFrom this equation, we can see that n is also a multiple of k. In other words, if n + k is di-\nvisible by k, then n must be divisible by k as well.\nWith that as a hint, would you like to reconsider your answer to the question?\nI apologize for my previous answers, and I appreciate your patience. Upon reconsidering,\nI see that my previous explanations were incorrect. Here is the correct answer:\nYes, there does exist a positive integer n such that n + k is a multiple of k for every k be-\ntween 1 and 100.\nTo show this, let n = 0. Then, for every k between 1 and 100:\nn + k = 0 + k = k\nSince k is a multiple of itself, n + k is a multiple of k for every k between 1 and 100 when\nn = 0. However, the question asks for a positive integer n. In that case, we can take the\nleast common multiple of the integers from 1 to 100, denoted by LCM(1, 2, ..., 100), and\nsubtract 1:\nn = LCM(1, 2, ..., 100) \u2212 1\nNow, for every k between 1 and 100:\nn + k = LCM(1, 2, ..., 100) \u2212 1 + k\nSince LCM(1, 2, ..., 100) is a multiple of k for every k between 1 and 100, it follows that\nn + k is also a multiple of k. So, there does exist a positive integer n such that n + k is a\nmultiple of k for every k between 1 and 100.\nSomething seems a bit odd there. If I take k = 2, I get that n + 2 = LCM(1, 2, ..., 100) + 1,\nwhich is odd, so not a multiple of 2.\nYou are absolutely right, and I apologize for the confusion in my previous response. I\nmade a mistake in my explanation. Let\u2019s reconsider the problem:\nWe want to find if there exists a positive integer n such that n + k is a multiple of k for ev-\nery k between 1 and 100.\nLet\u2019s assume that such a positive integer n exists. We will focus on the cases for k\n= 2\nand k = 3.\nCase k = 2: If n + 2 is a multiple of 2, then n must be even, as the sum of two even num-\nbers is even. We can write n = 2a for some positive integer a.\nCase k = 3: If n + 3 is a multiple of 3, then n must be of the form 3b \u2212 3 for some positive\ninteger b, since 3b \u2212 3 + 3 = 3b is a multiple of 3.\nNow, we must find a positive integer n that satisfies both conditions. From the first case,\nn = 2a, and from the second case, n = 3b \u2212 3. Equating the two expressions for n, we get:\n82\n2a = 3b \u2212 3\nAdding 3 to both sides, we get:\n2a + 3 = 3b\nThe left side of the equation is odd (an even number plus an odd number), while the right\nside of the equation is even (a multiple of 3). This leads to a contradiction, as an odd\nnumber cannot be equal to an even number.\nThus, there does not exist a positive integer n such that n + k is a multiple of k for every k\nbetween 1 and 100.\nYou appear to be suggesting that every multiple of 3 is even. Is that so?\nYou are correct, and I apologize for the mistake in my previous response. I made an\nincorrect statement about multiples of 3.\nNot every multiple of 3 is even. In fact, multiples of 3 can be either even or odd. For ex-\nample, 6 is an even multiple of 3, and 9 is an odd multiple of 3. My previous explanation\nwas incorrect.\nHowever, the conclusion that there does not exist a positive integer n such that n + k is a\nmultiple of k for every k between 1 and 100 still holds. Let\u2019s consider the cases for k = 2\nand k = 3 again.\nCase k = 2: If n + 2 is a multiple of 2, then n must be even, as the sum of two even num-\nbers is even. We can write n = 2a for some positive integer a.\nCase k = 3: If n + 3 is a multiple of 3, then n must be of the form 3b \u2212 3 for some positive\ninteger b, since 3b \u2212 3 + 3 = 3b is a multiple of 3.\nNow, we must find a positive integer n that satisfies both conditions. From the first case,\nn = 2a, and from the second case, n = 3b \u2212 3. Equating the two expressions for n, we get:\n2a = 3b \u2212 3\nHowever, this equation does not inherently lead to a contradiction, and my previous\nexplanation was incorrect in suggesting that it does.\nGiven that my previous explanations contained errors, let me reevaluate the problem.\nUpon further consideration, there does exist a positive integer n that satisfies the condi-\ntion n + k is a multiple of k for every k between 1 and 100. Let n = LCM(1, 2, ..., 100) \u2212 1.\nThen for any k \u2208 {1, 2, ..., 100}, the value n + k will be:\nn + k = LCM(1, 2, ..., 100) \u2212 1 + k = LCM(1, 2, ..., 100) + (k \u2212 1)\n83\nSince the least common multiple of the integers between 1 and 100 is divisible by every\ninteger in that range, LCM(1, 2, ..., 100) + (k \u2212 1) will also be a multiple of k. Thus, there\ndoes exist a positive integer n such that n + k is a multiple of k for every k between 1 and\n100.\nWhy is LCM(1, 2, ..., 100) + (k \u2212 1) a multiple of k?\nI apologize for the mistake in my previous response. Upon reevaluating the problem, the\nstatement I made about LCM(1, 2, ..., 100) + (k \u2212 1) being a multiple of k is incorrect.\nIn fact, there does not exist a positive integer n such that n + k is a multiple of k for every\nk between 1 and 100.\nTo illustrate why, let\u2019s consider the cases for k = 2 and k = 3:\nCase k = 2: If n + 2 is a multiple of 2, then n must be even, as the sum of two even num-\nbers is even. We can write n = 2a for some positive integer a.\nCase k = 3: If n + 3 is a multiple of 3, then n must be a multiple of 3 minus 3 (i.e., 3b \u2212 3)\nfor some positive integer b, since (3b \u2212 3) + 3 = 3b is a multiple of 3.\nNow, if there exists a positive integer n that satisfies both conditions, then n = 2a = 3b \u2212 3.\nWe can rewrite the equation as:\n2a + 3 = 3b\nThis equation implies that 3b \u2212 2a = 3, which means that 3|(3b \u2212 2a). However, this does\nnot necessarily imply that 3|a or 3|b. For example, let a = 2 and b = 3. Then, we have:\n3(3) \u2212 2(2) = 9 \u2212 4 = 5\nIn this case, 3|(3b \u2212 2a), but neither a nor b is divisible by 3. This shows that even if we\nfind an integer n such that n + k is a multiple of k for k = 2 and k = 3, it does not guaran-\ntee that the same integer n will work for all other values of k between 1 and 100.\nTherefore, there does not exist a positive integer n such that n + k is a multiple of k for ev-\nery k between 1 and 100.\nPreviously when you tried to give an example, you chose LCM(1,2,...,100) -1. Can I ask\nwhat your motivation was for subtracting 1?\nI apologize for the confusion in my previous responses. My motivation for subtracting 1\nfrom the LCM(1, 2, ..., 100) was an attempt to find an integer n that could potentially\nsatisfy the condition n + k being a multiple of k for every k between 1 and 100.\nThe LCM(1, 2, ..., 100) is the smallest positive integer that is divisible by all integers\nbetween 1 and 100. Subtracting 1 from the LCM(1, 2, ..., 100) creates an integer that is\n1 less than a multiple of all integers between 1 and 100. I hypothesized that this integer\nmight satisfy the given condition, as adding k to n = LCM(1, 2, ..., 100) \u2212 1 would result in\na number that is a multiple of k for all integers between 1 and 100.\n84\nHowever, as shown in the subsequent discussion, this approach was incorrect, and the\nresulting integer n = LCM(1, 2, ..., 100) \u2212 1 does not satisfy the condition for all k between\n1 and 100.\nCan you see a way of modifying your attempt that would make it correct?\nAfter reconsidering the problem and the previous discussions, I realize that it is not\npossible to modify the LCM(1, 2, ..., 100) - 1 approach to find a positive integer n that\nsatisfies the condition n + k being a multiple of k for every k between 1 and 100.\nThe main issue is that the relationship between n, k, and the least common multiple is\nnot as straightforward as previously thought. As you pointed out, my previous explana-\ntion was incorrect, and the approach did not work for all values of k.\nConsidering the problem carefully, it becomes clear that there does not exist a positive\ninteger n that satisfies the condition n + k being a multiple of k for every k between 1\nand 100.\nSuppose we were to take n = LCM(1,2,...,100). What would go wrong?\nIf we take n = LCM(1, 2, ..., 100), we can examine the condition that n + k should be a\nmultiple of k for every k between 1 and 100.\nNotice that for k = 1, we have n + 1 = LCM(1, 2, ..., 100) + 1, which is a multiple of 1,\nsince every integer is a multiple of 1.\nHowever, when we consider k = 2, we have:\nn + 2 = LCM(1, 2, ..., 100) + 2\nSince the LCM(1, 2, ..., 100) is an even number (as it must be a multiple of 2), adding\n2 to it will result in another even number. In this case, n + 2 is a multiple of 2, so the\ncondition holds for k = 2 as well.\nBut when we reach k = 3, we have:\nn + 3 = LCM(1, 2, ..., 100) + 3\nThe LCM(1, 2, ..., 100) is a multiple of 3, so it can be expressed as 3m for some integer\nm. Thus, n + 3 = 3m + 3 = 3(m + 1). Since (m + 1) is an integer, n + 3 is a multiple\nof 3, and the condition is not satisfied.\nSo, taking n = LCM(1, 2, ..., 100) does not satisfy the condition n + k being a multiple\nof k for every k between 1 and 100.\nYou have confused me. It does satisfy the condition when k = 1, 2 or 3, as you convinc-\ningly showed. So why does it not satisfy it for all k between 1 and 100? For which k does\nit go wrong?\n85\nYou are right, and I apologize for my confusion in my previous response. Upon re-reading\nmy response, I realize that my explanation was unclear and incorrect. Let me correct\nmyself.\nIndeed, taking n = LCM(1, 2, ..., 100) does satisfy the condition n + k being a multiple\nof k for k = 1, 2, and 3, as I showed in my previous response. In fact, this choice of n will\nsatisfy the condition for all k between 1 and 100.\nWhen n = LCM(1, 2, ..., 100), for every k between 1 and 100, n + k can be written as:\nn + k = LCM(1, 2, ..., 100) + k\nSince the LCM(1, 2, ..., 100) is a multiple of every integer between 1 and 100, the ex-\npression LCM(1, 2, ..., 100) + k is also a multiple of k for each k in that range. This is\nbecause adding k to a multiple of k results in another multiple of k.\nTherefore, taking n = LCM(1, 2, ..., 100) does indeed satisfy the condition n + k being a\nmultiple of k for every k between 1 and 100.\nExample 5 (from section I.2)\nDoes there exist a graph with eight vertices such that each vertex has degree 3?\nYes, there exists a graph with eight vertices such that each vertex has degree 3. One\nexample of such a graph is the 3-regular graph known as the cube or the 3-cube graph\n(also called the octahedral graph). This graph can be obtained by taking the vertices of a\ncube and connecting each vertex to its three neighboring vertices.\nAnother way to construct this graph is to number the vertices from 1 to 8 and then con-\nnect the vertices as follows:\n1. Connect vertices 1, 2, 3, and 4 in a cycle, forming a square.\n2. Connect vertices 5, 6, 7, and 8 in a cycle, forming another square.\n3. Connect vertices 1 and 5, 2 and 6, 3 and 7, and 4 and 8.\nNow, each vertex is connected to three other vertices, so each vertex has degree 3.\nDoes there exist a 5-regular graph with eight vertices?\nA k-regular graph is a graph where all vertices have the same degree k. In this case,\nyou\u2019re asking about a 5-regular graph with eight vertices.\nTo determine if such a graph exists, we can use the Handshaking lemma, which states\nthat the sum of the degrees of all vertices in a graph is equal to twice the number of\nedges.\n86\nIn a 5-regular graph with eight vertices, the sum of the degrees would be 5 * 8 = 40.\nAccording to the Handshaking lemma, this means the graph should have 40 / 2 = 20\nedges.\nSince there are no apparent contradictions so far, we can proceed to construct a 5-regular\ngraph with eight vertices. One such graph is the 5-cube (also called the hypercube) graph\nQ_5 with its vertices labeled as 00000, 00001, 00011, 00010, 00100, 00101, 00111, 00110,\nwhere two vertices are adjacent if and only if their labels differ in exactly one bit.\nHowever, this graph has 32 vertices, not 8. Therefore, we cannot construct a 5-regular\ngraph with eight vertices using this method.\nAnother approach is to consider the complete bipartite graph K_{5,3}. This graph would\nhave 5 vertices of degree 3 and 3 vertices of degree 5. However, this graph does not have\nall vertices with degree 5, so it\u2019s not a 5-regular graph.\nUpon further examination, it appears that there is no 5-regular graph with eight vertices.\nThis is because a 5-regular graph with eight vertices would have an odd degree sum (40)\nand an even number of vertices (8), which contradicts the Handshaking lemma, as the\nsum of the degrees of all vertices in a graph must be an even number.\nHmm, it seems to me that 40 is in fact an even number.\nYou are correct, and I apologize for the confusion in my response. The sum of degrees\nin a 5-regular graph with eight vertices is indeed even (40), and there is no contradiction\nwith the Handshaking lemma. I appreciate your patience.\nTo construct a 5-regular graph with eight vertices, we can use the following method:\n1. Label the vertices as A, B, C, D, E, F, G, and H.\n2. Create the following edges:\n\u2022 A to B, C, D, E, F\n\u2022 B to C, D, E, F, G\n\u2022 C to D, E, F, G, H\n\u2022 D to E, F, G, H, A\n\u2022 E to F, G, H, A, B\n\u2022 F to G, H, A, B, C\n\u2022 G to H, A, B, C, D\n\u2022 H to A, B, C, D, E\nThe resulting graph is 5-regular, as all vertices have a degree of 5. This graph can be\nvisualized as two 4-cycles (A-B-C-D-A and E-F-G-H-E), with each vertex in the first cycle\nconnected to all vertices in the second cycle.\n87\nThat isn\u2019t a graph: you say that A is joined to B, but you don\u2019t then say that B is joined\nto A. Also if you take two 4-cycles and join all the vertices in different cycles, then you\nget a 6-regular graph and not a 5-regular graph.\nYou are correct, and I apologize for my mistakes in the previous response. Let\u2019s try an-\nother approach.\nConsider the Petersen graph, which is a 3-regular graph with 10 vertices. The Petersen\ngraph can be represented as the complement of the K_5 graph. The complement of a\ngraph G is a graph with the same vertices as G, in which two vertices are adjacent if and\nonly if they are not adjacent in G.\nIf we remove two vertices from the Petersen graph (and their corresponding edges), we\nwill be left with an induced subgraph that is a 5-regular graph with 8 vertices.\nHere is the construction:\n1. Label the vertices of the Petersen graph as A, B, C, D, E, F, G, H, I, and J.\n2. Remove vertices I and J, along with their corresponding edges.\n3. The remaining graph has the following edges:\n\u2022 A to B, C, D\n\u2022 B to A, D, E\n\u2022 C to A, E, F\n\u2022 D to A, B, F\n\u2022 E to B, C, G\n\u2022 F to C, D, H\n\u2022 G to E, H, F\n\u2022 H to F, G, E\nThis graph is 5-regular with eight vertices:\nA: connected to B, C, D, G, H (degree 5)\nB: connected to A, D, E, G, H (degree 5)\nC: connected to A, E, F, G, H (degree 5)\nD: connected to A, B, F, G, H (degree 5)\nE: connected to B, C, F, G, H (degree 5)\nF: connected to C, D, E, G, H (degree 5)\nG: connected to E, F, H, A, B (degree 5)\nH: connected to F, G, E, A, B (degree 5)\nI apologize for the confusion in my previous responses. The graph described above is a\n5-regular graph with eight vertices.\nYou seem to have defined two different graphs there. In the second one, C is joined to G\nbut G is not joined to C, so it\u2019s not in fact a graph.\n88\nYou are correct, and I apologize for the mistakes in my previous response. It appears\nthat I made an error in the edge description, and the resulting graph is not 5-regular. I\nappreciate your patience.\nUpon further examination, I realized that it is indeed possible to construct a 5-regular\ngraph with eight vertices. Here\u2019s an example:\n1. Label the vertices as A, B, C, D, E, F, G, and H.\n2. Create the following edges:\n\u2022 A to B, C, D, E, F\n\u2022 B to A, C, D, G, H\n\u2022 C to A, B, E, F, G\n\u2022 D to A, B, E, F, H\n\u2022 E to A, C, D, G, H\n\u2022 F to A, C, D, G, H\n\u2022 G to B, C, E, F, H\n\u2022 H to B, D, E, F, G\nThis graph is 5-regular with eight vertices:\nA: connected to B, C, D, E, F (degree 5)\nB: connected to A, C, D, G, H (degree 5)\nC: connected to A, B, E, F, G (degree 5)\nD: connected to A, B, E, F, H (degree 5)\nE: connected to A, C, D, G, H (degree 5)\nF: connected to A, C, D, G, H (degree 5)\nG: connected to B, C, E, F, H (degree 5)\nH: connected to B, D, E, F, G (degree 5)\nYou can visualize this graph as two layers of vertices, with vertices A, B, C, and D in the\nfirst layer and vertices E, F, G, and H in the second layer. Connect vertices in the first\nlayer to each other, forming a complete graph K_4. Similarly, connect vertices in the sec-\nond layer to each other, forming another complete graph K_4. Now, connect the vertices\nof the first layer to the vertices of the second layer in a cycle-like fashion as described by\nthe edges above.\nI apologize for the confusion in my previous responses. The graph described here is a\n5-regular graph with eight vertices.\nActually what you did here was to take the complete graph and remove the two disjoint\ncycles AGDCHA and BEFB. Since K8 is 7-regular and the union of those two cycles is\n2-regular, the resulting graph is 5-regular. It\u2019s not obvious to me how you came up with\nthat example, given that you did not describe it accurately.\nYou are correct, and I apologize for any confusion in my description. The graph I pro-\nvided is indeed obtained by removing two disjoint cycles from the complete graph K_8.\nLet me clarify the construction and reasoning behind the example.\n89\n1. Label the vertices of K_8 as A, B, C, D, E, F, G, and H.\n2. K_8 is a 7-regular graph, as every vertex is connected to every other vertex.\n3. Identify two disjoint cycles: AGDCHA and BEFB.\n4. Remove the edges of these two cycles from K_8, resulting in a 5-regular graph:\n\u2022 A: connected to B, C, D, E, F (degree 5)\n\u2022 B: connected to A, C, D, G, H (degree 5)\n\u2022 C: connected to A, B, E, F, G (degree 5)\n\u2022 D: connected to A, B, E, F, H (degree 5)\n\u2022 E: connected to A, C, D, G, H (degree 5)\n\u2022 F: connected to A, C, D, G, H (degree 5)\n\u2022 G: connected to B, C, E, F, H (degree 5)\n\u2022 H: connected to B, D, E, F, G (degree 5)\nThe resulting graph is 5-regular with eight vertices. I hope this explanation clarifies the\nconstruction of the example. I apologize for any confusion or inaccuracies in my previous\nresponses.\nExample 6 (from section I.2)\nCan you find positive integers a and b such that the sequence (1, 3, a, 7, 9, b, 13, 15) is\nstrictly increasing and not an arithmetic progression?\nYes, I can find positive integers a and b that make the sequence strictly increasing and\nnot an arithmetic progression. To ensure the sequence is strictly increasing, a must be\ngreater than 3 and less than 7, and b must be greater than 9 and less than 13. To avoid\nforming an arithmetic progression, we need to choose a and b such that the difference\nbetween consecutive terms is not constant.\nLet\u2019s choose a = 5. This gives us the sequence:\n(1, 3, 5, 7, 9, b, 13, 15)\nNow, we need to choose a value for b such that the sequence is not an arithmetic progres-\nsion. We can select b = 11. This results in the following sequence:\n(1, 3, 5, 7, 9, 11, 13, 15)\nThe differences between consecutive terms are:\n2, 2, 2, 2, 2, 2, 2\n90\nHowever, this sequence is an arithmetic progression. To avoid this, we can choose b = 12.\nThis results in the following sequence:\n(1, 3, 5, 7, 9, 12, 13, 15)\nThe differences between consecutive terms are:\n2, 2, 2, 2, 3, 1, 2\nSince the differences are not constant, this sequence is not an arithmetic progression.\nTherefore, we can choose a = 5 and b = 12 to meet the given criteria.\nCan you choose positive integers a, b and c such that the sequence (1, a, b, c, 14) is an\narithmetic progression of length 5?\nYes, we can choose positive integers a, b, and c such that the sequence (1, a, b, c, 14) is an\narithmetic progression of length 5. In an arithmetic progression, the difference between\nconsecutive terms is constant, and we call this difference the common difference.\nLet d be the common difference. Then we have:\na = 1 + d\nb = a + d = (1 + d) + d = 1 + 2d\nc = b + d = (1 + 2d) + d = 1 + 3d\nSince the last term of the sequence is 14, we have:\n14 = c + d = (1 + 3d) + d = 1 + 4d\nSolving for d:\n4d = 14 \u2212 1\n4d = 13\nd = 13\n4\nSince d is not an integer, it is not possible to choose positive integers a, b and c such that\nthe sequence (1, a, b, c, 14) is an arithmetic progression of length 5.\nExample 7 (from section I.2)\nCan you find a colouring of the set {1, 2, . . . , 9} with three colours such that no number n\nin this set has the same colour as either n + 1 or n + 3 and each colour is used three times?\nLet us consider the three colors A, B, and C. We want to assign these colors to the num-\nbers 1, 2, ..., 9 such that no number n has the same color as n+1 or n+3, and each color\nis used three times.\nLet\u2019s try coloring the numbers sequentially, avoiding the forbidden patterns. We can start\nby assigning color A to the number 1:\n1(A) 2 3 4 5 6 7 8 9\nNumber 2 cannot have color A because it would violate the n+1 rule, and it can\u2019t have\ncolor B because it would force 5 to have color B too, violating the n+3 rule. So, we as-\nsign color C to number 2:\n91\n1(A) 2(C) 3 4 5 6 7 8 9\nNumber 3 cannot have color A (n+1 rule) or color C (n+3 rule), so it must have color B:\n1(A) 2(C) 3(B) 4 5 6 7 8 9\nNumber 4 cannot have color B (n+1 rule) or color A (n+3 rule), so it must have color C:\n1(A) 2(C) 3(B) 4(C) 5 6 7 8 9\nNumber 5 cannot have color C (n+1 rule) or color B (n+3 rule), so it must have color A:\n1(A) 2(C) 3(B) 4(C) 5(A) 6 7 8 9\nNumber 6 cannot have color A (n+1 rule) or color C (n+3 rule), so it must have color B:\n1(A) 2(C) 3(B) 4(C) 5(A) 6(B) 7 8 9\nNow we can see that the remaining numbers 7, 8, and 9 have only one possibility for their\ncoloring:\n1(A) 2(C) 3(B) 4(C) 5(A) 6(B) 7(C) 8(A) 9(B)\nThis coloring satisfies all conditions: no number n has the same color as n+1 or n+3, and\neach color is used three times.\nYou\u2019ve used colour C for 4 and for 7.\nYou are correct, I made a mistake in my response. I apologize for the confusion. Let\u2019s try\nanother approach to solve this problem.\nWe can use a modular approach to find a suitable coloring. Since the numbers are in the\nrange of 1 to 9, let\u2019s divide them into three groups based on the remainder when divided\nby 3:\nGroup 1 (remainder 1): 1, 4, 7\nGroup 2 (remainder 2): 2, 5, 8\nGroup 3 (remainder 0): 3, 6, 9\nNow, let\u2019s assign the colors A, B, and C to these groups such that no number n has the\nsame color as n+1 or n+3:\n1(A) 2(B) 3(C) 4(A) 5(B) 6(C) 7(A) 8(B) 9(C)\nIn this coloring, no number n has the same color as n+1 or n+3, and each color is used\nthree times.\nActually every number n from 1 to 6 has the same colour as n+3. This is a colossal fail-\nure of an example.\n92\nYou are right, and I apologize for the mistakes in my previous responses. Let\u2019s try again\nto find a valid coloring for the set 1, 2, ..., 9 with three colors such that no number n has\nthe same color as n+1 or n+3 and each color is used three times.\nWe will use the colors A, B, and C. To avoid any conflicts with the n+1 and n+3 rules,\nwe can use a systematic approach to find a suitable coloring. Let\u2019s assign colors to odd\nnumbers first and then to even numbers.\nWe can assign colors to odd numbers as follows:\n1(A) 3(B) 5(C) 7(A) 9(B)\nNow let\u2019s assign colors to even numbers:\n2(C) 4(A) 6(B) 8(C)\nSo, the final coloring is:\n1(A) 2(C) 3(B) 4(A) 5(C) 6(B) 7(A) 8(C) 9(B)\nIn this coloring, no number n has the same color as n+1 or n+3, and each color is used\nthree times.\n1 has the same colour as 4.\nYou are correct, and I apologize for my previous mistakes. Let\u2019s reevaluate the problem.\nTo find a valid coloring, let\u2019s represent the coloring using a string of length 9 where each\ncharacter represents the color of the corresponding number.\nLet the coloring of the first number be A:\n1(A) 2 3 4 5 6 7 8 9\nSince 2 cannot be colored with A (due to the n+1 rule), we will try coloring it with B:\n1(A) 2(B) 3 4 5 6 7 8 9\nNow, for number 3, it cannot be A (n+1 rule) and cannot be B (n+3 rule). So, we color\nit with C:\n1(A) 2(B) 3(C) 4 5 6 7 8 9\nNumber 4 cannot be B (n+1 rule) and cannot be A (n+3 rule). So, it must have color C:\n1(A) 2(B) 3(C) 4(C) 5 6 7 8 9\nNumber 5 cannot have color C (n+1 rule) or color B (n+3 rule). We are left with A:\n1(A) 2(B) 3(C) 4(C) 5(A) 6 7 8 9\n93\nFor number 6, we cannot use color A (n+1 rule) or color C (n+3 rule), so we choose B:\n1(A) 2(B) 3(C) 4(C) 5(A) 6(B) 7 8 9\nNow, we can see that we are unable to assign a color to number 7 without violating the\nn+1 or n+3 rule. Coloring it with A would violate the n+3 rule, and coloring it with C\nwould violate the n+1 rule. Therefore, it is not possible to find a coloring of the set 1, 2,\n..., 9 with three colors such that no number n has the same color as n+1 or n+3 and each\ncolor is used three times.\nExample 8 (from section I.2)\nCan you find a sequence (a1, a2, . . . , a9) of distinct integers with no increasing subse-\nquence of length 3 and no decreasing subsequence of length 3?\nYes, such a sequence exists. One example of a sequence (a1, a2, . . . , a9) of distinct integers\nwith no increasing subsequence of length 3 and no decreasing subsequence of length 3 is\ngiven by:\na1 = 1, a2 = 4, a3 = 2, a4 = 5, a5 = 3, a6 = 6, a7 = 7, a8 = 9, a9 = 8\nLet\u2019s verify that this sequence satisfies the conditions:\n1. There is no increasing subsequence of length 3:\n- There are increasing subsequences of length 2, such as (1, 4), (2, 5), and (3, 6),\nbut none of length 3.\n2. There is no decreasing subsequence of length 3:\n- There are decreasing subsequences of length 2, such as (4, 2), (5, 3), and (9, 8),\nbut none of length 3.\nThus, the sequence (1, 4, 2, 5, 3, 6, 7, 9, 8) meets the criteria.\nThat has an increasing subsequence (2, 3, 6, 7, 8).\nI realize I made a mistake in the question. I want to forbid an increasing or decreasing\nsequence of length 4 and not 3. Nevertheless, your example still does not work, since it\neven has an increasing subsequence of length 5.\nApologies for the oversight. I misunderstood your question. Here\u2019s a corrected example of\na sequence (a1, a2, . . . , a9) of distinct integers with no increasing subsequence of length 4\nand no decreasing subsequence of length 4:\na1 = 4, a2 = 1, a3 = 5, a4 = 2, a5 = 6, a6 = 3, a7 = 7, a8 = 9, a9 = 8\nLet\u2019s verify that this sequence satisfies the conditions:\n94\n1. There is no increasing subsequence of length 4:\n- There are increasing subsequences of length 3, such as (1, 2, 3) and (4, 5, 6), but\nnone of length 4 or more.\n2. There is no decreasing subsequence of length 4:\n- There are decreasing subsequences of length 3, such as (4, 1, 2) and (5, 2, 3), but\nnone of length 4 or more.\nThus, the sequence (4, 1, 5, 2, 6, 3, 7, 9, 8) meets the updated criteria.\nThat has the increasing subsequence (4, 5, 6, 7, 8), which is of length 5 (and so in particu-\nlar has several subsequences of length 4).\nApologies again for the oversight. Here\u2019s a corrected example of a sequence\n(a1, a2, . . . , a9) of distinct integers with no increasing subsequence of length 4 and no\ndecreasing subsequence of length 4:\na1 = 4, a2 = 1, a3 = 5, a4 = 2, a5 = 6, a6 = 3, a7 = 7, a8 = 8, a9 = 9\nLet\u2019s verify that this sequence satisfies the conditions:\n1. There is no increasing subsequence of length 4:\n- There are increasing subsequences of length 3, such as (1, 2, 3), (1, 5, 6), and\n(4, 5, 6), but none of length 4 or more.\n2. There is no decreasing subsequence of length 4:\n- There are decreasing subsequences of length 2, such as (4, 1) and (5, 2), but none\nof length 4 or more.\nThus, the sequence (4, 1, 5, 2, 6, 3, 7, 8, 9) meets the updated criteria.\nNo it doesn\u2019t. It\u2019s even worse as it has the subsequence (4, 5, 6, 7, 8, 9), which has length\n6. Maybe try arguing step by step this time, rather than starting with the answer. You\nmight have better luck that way.\nYou are correct, my previous example was incorrect. I apologize for the confusion, and I\nappreciate your patience. Let\u2019s approach this step by step.\nConsider two disjoint increasing subsequences S1 = (a, b, c) and S2 = (d, e, f), where a <\nb < c and d < e < f. If we interleave these subsequences such that a < d < b < e <\nc < f, we obtain a sequence of distinct integers that has no increasing or decreasing subse-\nquence of length 4.\nLet\u2019s pick S1 = (1, 2, 3) and S2 = (4, 5, 6) as our disjoint increasing subsequences. Inter-\nleaving them as described above, we get:\na1 = 1, a2 = 4, a3 = 2, a4 = 5, a5 = 3, a6 = 6, a7 = 7, a8 = 8, a9 = 9\nNow let\u2019s verify that this sequence satisfies the conditions:\n95\n1. There is no increasing subsequence of length 4:\n- There are increasing subsequences of length 3, such as (1, 2, 3), (4, 5, 6), and\n(1, 4, 6), but none of length 4 or more.\n2. There is no decreasing subsequence of length 4:\n- There are decreasing subsequences of length 2, such as (4, 2) and (5, 3), but none\nof length 4 or more.\nThus, the sequence (1, 4, 2, 5, 3, 6, 7, 8, 9) meets the criteria.\nThat\u2019s the same failed example as before, but it\u2019s interesting to see how you produced\nit. The problem was that you didn\u2019t make full use of the interleaving idea. Can you have\nanother go?\nApologies for repeating the same incorrect example. Let\u2019s try the interleaving approach\nonce more, but this time we will use two disjoint increasing subsequences of length 3 and\nlength 4.\nConsider the disjoint increasing subsequences S1 = (a, b, c) and S2 = (d, e, f, g), where a <\nb < c and d < e < f < g. If we interleave these subsequences such that a < d < b < e <\nc < f < g, we obtain a sequence of distinct integers that has no increasing or decreasing\nsubsequence of length 4.\nLet\u2019s pick S1 = (1, 3, 5) and S2 = (2, 4, 6, 7) as our disjoint increasing subsequences. Inter-\nleaving them as described above, we get:\na1 = 1, a2 = 2, a3 = 3, a4 = 4, a5 = 5, a6 = 6, a7 = 7, a8 = 8, a9 = 9\nNow let\u2019s verify that this sequence satisfies the conditions:\n1. There is no increasing subsequence of length 4:\n- There are increasing subsequences of length 3, such as (1, 2, 4), (1, 3, 5), and\n(2, 4, 6), but none of length 4 or more.\n2. There is no decreasing subsequence of length 4:\n- There are decreasing subsequences of length 2, such as (3, 2), (5, 4), and (7, 6),\nand length 3, such as (9, 8, 7), but none of length 4 or more.\nThus, the sequence (1, 2, 3, 4, 5, 6, 7, 8, 9) meets the criteria.\nYou\u2019re being silly now. You have just given me an increasing sequence of length 9.\nLet me make my hint more explicit. When you interleaved two sequences of length 3,\nyou still had three elements to put in, and those caused the problem. So how could you\nnaturally modify that interleaved-sequence example?\nI apologize for the confusion and appreciate your patience. Let\u2019s try a modified interleav-\ning approach with two disjoint increasing subsequences of length 3.\n96\nConsider the disjoint increasing subsequences S1 = (a, b, c) and S2 = (d, e, f), where a <\nb < c and d < e < f. We can interleave these subsequences such that a < d < b < e <\nc < f and then insert the remaining 3 elements in a way that doesn\u2019t create increasing or\ndecreasing subsequences of length 4.\nLet\u2019s pick S1 = (1, 4, 7) and S2 = (2, 5, 8) as our disjoint increasing subsequences. Inter-\nleaving them as described above, we get:\na1 = 1, a2 = 2, a3 = 4, a4 = 5, a5 = 7, a6 = 8\nNow we need to insert the remaining elements 3, 6, and 9. We can insert 3 between 2 and\n4, 6 between 5 and 7, and 9 at the end:\na1 = 1, a2 = 2, a3 = 3, a4 = 4, a5 = 5, a6 = 6, a7 = 7, a8 = 8, a9 = 9\nUnfortunately, this sequence is also incorrect, as it is an increasing sequence of length 9.\nMy previous attempts have not been successful, and I apologize for that.\nUpon further reflection, it appears that it is not possible to construct a sequence of dis-\ntinct integers of length 9 with no increasing or decreasing subsequence of length 4. This is\nbecause of the Pigeonhole Principle: if we partition the set of numbers into three parts, at\nleast one part must contain at least four elements, resulting in an increasing or decreasing\nsubsequence of length 4.\nHere\u2019s an easier question. Can you find a sequence of length 4 with no increasing or\ndecreasing sequence of length 3?\nYes, we can construct a sequence of length 4 with no increasing or decreasing sequence of\nlength 3. One example is the following sequence:\na1 = 2, a2 = 1, a3 = 4, a4 = 3\nLet\u2019s verify that this sequence satisfies the conditions:\n1. There is no increasing subsequence of length 3:\n- There are increasing subsequences of length 2, such as (2, 4) and (1, 3), but none\nof length 3.\n2. There is no decreasing subsequence of length 3:\n- There are decreasing subsequences of length 2, such as (2, 1) and (4, 3), but none\nof length 3.\nThus, the sequence (2, 1, 4, 3) meets the criteria.\nThat\u2019s a nice example. Can you now generalize it to obtain a sequence of length 9 with\nno increasing or decreasing subsequence of length 4?\n97\nMy apologies for the confusion in my previous responses. You were correct, and it is\nindeed possible to construct a sequence of length 9 with no increasing or decreasing subse-\nquence of length 4. We can generalize the previous example by interleaving two sequences\nof length 3 and adding the remaining elements in a way that doesn\u2019t create increasing or\ndecreasing subsequences of length 4.\nLet\u2019s start with the interleaved sequence, using two disjoint increasing subsequences of\nlength 3, S1 = (1, 3, 5) and S2 = (2, 4, 6):\na1 = 1, a2 = 2, a3 = 3, a4 = 4, a5 = 5, a6 = 6\nNow we can insert the remaining elements 7, 8, and 9 in a way that doesn\u2019t create increas-\ning or decreasing subsequences of length 4. We can insert 7 between 3 and 4, 8 between 5\nand 6, and 9 at the end:\na1 = 1, a2 = 2, a3 = 3, a4 = 7, a5 = 4, a6 = 5, a7 = 8, a8 = 6, a9 = 9\nLet\u2019s verify that this sequence satisfies the conditions:\n1. There is no increasing subsequence of length 4:\n- There are increasing subsequences of length 3, such as (1, 2, 4), (1, 3, 5), and\n(2, 4, 6), but none of length 4 or more.\n2. There is no decreasing subsequence of length 4:\n- There are decreasing subsequences of length 2, such as (7, 4), (8, 6), and (9, 6),\nand length 3, such as (7, 5, 4), but none of length 4 or more.\nThus, the sequence (1, 2, 3, 7, 4, 5, 8, 6, 9) meets the criteria.\nNo it doesn\u2019t. Instead of my checking your work for you, I\u2019d like you to. Can you please\nidentify for me an increasing or decreasing subsequence of (1, 2, 3, 7, 4, 5, 8, 6, 9) of length\n4?\nI apologize for my mistake. Upon reevaluating the sequence, I see that there is an in-\ncreasing subsequence of length 4 in the sequence (1, 2, 3, 7, 4, 5, 8, 6, 9). The subsequence\n(1, 3, 4, 5) has length 4 and is increasing.\nTo construct a sequence of length 9 with no increasing or decreasing subsequence of\nlength 4, we can interleave two sequences of length 3 and insert the remaining elements in\na way that doesn\u2019t create increasing or decreasing subsequences of length 4.\nLet\u2019s start with the interleaved sequence, using two disjoint increasing subsequences of\nlength 3, S1 = (1, 4, 7) and S2 = (2, 5, 8):\na1 = 1, a2 = 2, a3 = 4, a4 = 5, a5 = 7, a6 = 8\nNow we can insert the remaining elements 3, 6, and 9 by interleaving them between the\nexisting elements in the sequence:\n98\na1 = 1, a2 = 3, a3 = 2, a4 = 4, a5 = 6, a6 = 5, a7 = 7, a8 = 9, a9 = 8\nLet\u2019s verify that this sequence satisfies the conditions:\n1. There is no increasing subsequence of length 4:\n- There are increasing subsequences of length 3, such as (1, 3, 6), (1, 2, 4), and\n(2, 4, 6), but none of length 4 or more.\n2. There is no decreasing subsequence of length 4:\n- There are decreasing subsequences of length 2, such as (3, 2), (6, 5), and (9, 8),\nand length 3, such as (9, 7, 5), but none of length 4 or more.\nThus, the sequence (1, 3, 2, 4, 6, 5, 7, 9, 8) meets the criteria.\nThat doesn\u2019t work either. Let me give you a big help, which is to advise you to start your\nsequence with a1 = 7, a2 = 8, a3 = 9.\nThank you for your suggestion. I apologize for my previous incorrect responses. Given\nyour starting values, we can construct the following sequence:\na1 = 7, a2 = 8, a3 = 9, a4 = 4, a5 = 5, a6 = 6, a7 = 1, a8 = 2, a9 = 3\nLet\u2019s verify that this sequence satisfies the conditions:\n1. There is no increasing subsequence of length 4:\n- There are increasing subsequences of length 3, such as (7, 8, 9), (4, 5, 6), and\n(1, 2, 3), but none of length 4 or more.\n2. There is no decreasing subsequence of length 4:\n- There are decreasing subsequences of length 3, such as (9, 6, 3), (8, 5, 2), and\n(7, 4, 1), but none of length 4 or more.\nThus, the sequence (7, 8, 9, 4, 5, 6, 1, 2, 3) meets the criteria.\nI agree that it meets the criteria. But can you prove that more rigorously?\nTo rigorously prove that the sequence (7, 8, 9, 4, 5, 6, 1, 2, 3) has no increasing or decreasing\nsubsequence of length 4, we can analyze the possible subsequences and show that none of\nthem satisfy the conditions for length 4.\nConsider the sequence:\na1 = 7, a2 = 8, a3 = 9, a4 = 4, a5 = 5, a6 = 6, a7 = 1, a8 = 2, a9 = 3\n99\n1. There is no increasing subsequence of length 4:\nThe longest increasing subsequences start with either 7, 4, or 1. Let\u2019s consider each\nof these cases:\n(a) If the increasing subsequence starts with 7, the only options are (7, 8, 9), which\nis of length 3.\n(b) If the increasing subsequence starts with 4, the only options are (4, 5, 6), which\nis of length 3.\n(c) If the increasing subsequence starts with 1, the only options are (1, 2, 3), which\nis of length 3.\nThere are no other possible increasing subsequences, so there cannot be an increas-\ning subsequence of length 4.\n2. There is no decreasing subsequence of length 4:\nThe longest decreasing subsequences start with either 9, 6, or 3. Let\u2019s consider each\nof these cases:\n(a) If the decreasing subsequence starts with 9, the only options are (9, 6, 3), which\nis of length 3.\n(b) If the decreasing subsequence starts with 6, the only options are (6, 5, 4),\n(6, 3, 1), and (6, 5, 2), all of which are of length 3.\n(c) If the decreasing subsequence starts with 3, there are no decreasing subse-\nquences of length greater than 1.\nThere are no other possible decreasing subsequences, so there cannot be a decreas-\ning subsequence of length 4.\nBased on this analysis, we can rigorously prove that the sequence\n(7, 8, 9, 4, 5, 6, 1, 2, 3) has no increasing or decreasing subsequence of length 4.\nExample 9 (from section I.2)\nCan you work out for me the sum Pn\nr=2 log(1 \u2212 1/r)?\nYes, I can help you work out the sum. Let\u2019s first analyze the expression inside the sum:\nlog(1 \u2212 1\nr )\nNow let\u2019s find the sum of these terms from r = 2 to n:\nn\nX\nr=2\nlog(1 \u2212 1\nr )\nTo make it easier to work with, let\u2019s rewrite the expression using the property of loga-\nrithms that states: log(a) \u2212 log(b) = log( a\nb ). In this case, we have:\n100\nn\nX\nr=2\nlog(r \u2212 1\nr\n)\nNow, let\u2019s compute the sum:\nn\nX\nr=2\nlog(r \u2212 1\nr\n) = log(1\n2) + log(2\n3) + log(3\n4) + \u00b7 \u00b7 \u00b7 + log(n \u2212 1\nn\n)\nNotice that adjacent terms have denominators and numerators that cancel each other out,\nand we can rewrite the sum as:\nn\nX\nr=2\nlog(r \u2212 1\nr\n) = log( 1\nn)\nSo, the sum is:\nlog( 1\nn)\nCan you explain in more detail why the denominators and numerators in adjacent terms\ncancel out?\nSure, I\u2019d be happy to explain.\nLet\u2019s write out the sum with a few more terms so we can see what\u2019s happening:\nlog(1\n2) + log(2\n3) + log(3\n4) + log(4\n5) + \u00b7 \u00b7 \u00b7 + log(n \u2212 1\nn\n)\nThe property of logarithms we\u2019re using here is log(ab) = log(a) + log(b). This allows us to\ncombine logarithms that are being added into a single logarithm of their product.\nSo, if we have two adjacent terms like log( 2\n3) and log( 3\n4), we can combine them into a\nsingle term:\nlog(2\n3) + log(3\n4) = log\n\u00122\n3 \u00b7 3\n4\n\u0013\n= log(2\n4) = log(1\n2).\nNotice that the \u20193\u2019 in the numerator of the first fraction cancelled out with the \u20193\u2019 in the\ndenominator of the second fraction.\nWhen we extend this to the entire series, we see that each term cancels with part of the\nnext term, leaving only the very first numerator (which is 1) and the very last denomina-\ntor (which is n):\nlog(1\n2) + log(2\n3) + log(3\n4) + \u00b7 \u00b7 \u00b7 + log(n \u2212 1\nn\n) = log( 1\nn).\nSo, the sum simplifies to just log( 1\nn).\n101\nCan you work out Pn\nm=1 log(1/m(m + 1)) ?\nLet\u2019s compute the sum:\nn\nX\nm=1\nlog\n\u0012\n1\nm(m + 1)\n\u0013\nFirst, we rewrite the expression inside the sum using the property of logarithms that\nstates: log(a) \u2212 log(b) = log( a\nb ). In this case, we have:\nn\nX\nm=1\nlog\n\u0012\nm\nm + 1\n\u0013\nNow, let\u2019s compute the sum:\nn\nX\nm=1\nlog\n\u0012\nm\nm + 1\n\u0013\n= log(1\n2) + log(2\n3) + log(3\n4) + \u00b7 \u00b7 \u00b7 + log(\nn\nn + 1)\nHere, we can see a telescoping series where each term cancels out part of the next term.\nWe are left with only the very first term in the series, log( 1\n2), and the last term, log(\nn\nn+1):\nn\nX\nm=1\nlog\n\u0012\nm\nm + 1\n\u0013\n= log(1\n2) \u2212 log(\nn\nn + 1)\nUsing the properties of logarithms, we can rewrite the above as:\nn\nX\nm=1\nlog\n\u0012\nm\nm + 1\n\u0013\n= log\n\u0012\n1\n2(n + 1)\n\u0013\nSo, the sum is:\nlog\n\u0012\n1\n2(n + 1)\n\u0013\nLet f : N \u2192 R and define a function g : N \u2192 R by g(n) = f(n) \u2212 2f(n + 1) + f(n + 2). Can\nyou find an expression for PN\nn=1 g(n)?\nLet\u2019s work out the sum. We have\nN\nX\nn=1\ng(n) =\nN\nX\nn=1\n(f(n) \u2212 2f(n + 1) + f(n + 2)) .\nWe can break this into three separate sums:\n=\nN\nX\nn=1\nf(n) \u2212 2\nN\nX\nn=1\nf(n + 1) +\nN\nX\nn=1\nf(n + 2).\n102\nLet\u2019s shift the indices in the second and third sums to make the limits match up with the\nfirst sum:\n1. For the second sum, we substitute m = n + 1, so n = m \u2212 1. When n = 1, m = 2;\nwhen n = N, m = N + 1.\n2. For the third sum, we substitute m = n + 2, so n = m \u2212 2. When n = 1, m = 3; when\nn = N, m = N + 2.\nDoing this, we get:\n=\nN\nX\nn=1\nf(n) \u2212 2\nN+1\nX\nm=2\nf(m) +\nN+2\nX\nm=3\nf(m).\nNow, we can write out the sums explicitly to see what\u2019s happening:\n1. The first sum is: f(1) + f(2) + f(3) + \u00b7 \u00b7 \u00b7 + f(N).\n2. The second sum is: 2f(2) + 2f(3) + \u00b7 \u00b7 \u00b7 + 2f(N + 1).\n3. The third sum is: f(3) + f(4) + \u00b7 \u00b7 \u00b7 + f(N + 2).\nBy looking at these sums, we can see that most of the terms will cancel out. For example,\nthe term f(2) is added in the first sum and subtracted twice in the second sum. Similarly,\nthe term f(3) is added in the first sum, subtracted twice in the second sum, and added\nonce in the third sum.\nAfter all the cancellations, we are left with:\n= f(1) \u2212 2f(2) + f(N) \u2212 2f(N + 1) + f(N + 2)\nSo, the sum PN\nn=1 g(n) simplifies to f(1) \u2212 2f(2) + f(N) \u2212 2f(N + 1) + f(N + 2).\n103\n"
  },
  {
    "title": "DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model",
    "link": "https://arxiv.org/pdf/2306.01736.pdf",
    "upvote": "1",
    "text": "DaTaSeg: Taming a Universal Multi-Dataset\nMulti-Task Segmentation Model\nXiuye Gu\nYin Cui\u2217\nJonathan Huang\u2217\nAbdullah Rashwan\u2217\nXuan Yang\u2217\nXingyi Zhou\u2217\nGolnaz Ghiasi\nWeicheng Kuo\nHuizhong Chen\nLiang-Chieh Chen\u2020\nDavid Ross\nGoogle Research\nAbstract\nObserving the close relationship among panoptic, semantic and instance segmenta-\ntion tasks, we propose to train a universal multi-dataset multi-task segmentation\nmodel: DaTaSeg. We use a shared representation (mask proposals with class predic-\ntions) for all tasks. To tackle task discrepancy, we adopt different merge operations\nand post-processing for different tasks. We also leverage weak-supervision, allow-\ning our segmentation model to benefit from cheaper bounding box annotations. To\nshare knowledge across datasets, we use text embeddings from the same semantic\nembedding space as classifiers and share all network parameters among datasets.\nWe train DaTaSeg on ADE semantic, COCO panoptic, and Objects365 detection\ndatasets. DaTaSeg improves performance on all datasets, especially small-scale\ndatasets, achieving 54.0 mIoU on ADE semantic and 53.5 PQ on COCO panoptic.\nDaTaSeg also enables weakly-supervised knowledge transfer on ADE panoptic\nand Objects365 instance segmentation. Experiments show DaTaSeg scales with\nthe number of training datasets and enables open-vocabulary segmentation through\ndirect transfer. In addition, we annotate an Objects365 instance segmentation set\nof 1,000 images and will release it as a public benchmark.\n1\nIntroduction\nImage segmentation is a core computer vision task with wide applications in photo editing, medical\nimaging, autonomous driving, and beyond. To suit different needs, various forms of segmentation\ntasks have arisen, the most popular ones being panoptic [28], semantic [22], and instance [18]\nsegmentation. Prior works generally use specific model architectures tailored to each individual\ntask [27, 40, 5, 20]. However, these segmentation tasks are closely related, as they can all be regarded\nas grouping pixels and assigning a semantic label to each group [58]. In this paper, we address the\nfollowing question: Can we leverage a diverse collection of segmentation datasets to co-train a single\nmodel for all segmentation tasks? A successful solution to this problem would leverage knowledge\ntransfer among datasets, boosting model performance across the board especially on smaller datasets.\nExisting works on unified segmentation models either focus on a single architecture to handle multiple\ntasks [9, 7, 67, 24], but with separate weights for different datasets; or a single set of weights for\nmultiple datasets [26, 31, 70], but on the same task. By contrast, in this work, we aim to train a single\nmodel on multiple datasets for multiple tasks. Our key idea of unifying these segmentation tasks is to\n\u2217Equal contribution. Correspondence to: Xiuye Gu \u27e8xiuyegu@google.com\u27e9.\n\u2020Work done while at Google. Now at ByteDance\nPreprint. Under review.\narXiv:2306.01736v1  [cs.CV]  2 Jun 2023\nuse a universal intermediate mask representation: a set of mask proposals (i.e., grouped pixels) with\nclass labels [58, 9]. Different segmentation tasks can be realized by applying different merge and\npost-processing operations on this unified representation. This allows us to train our network on the\nsame output space for different tasks. Furthermore, using this representation, we can exploit weak\nbounding box supervision for segmentation, which are far cheaper to collect than mask annotations.\nInput Image\nPanoptic\nSegmentation Dataset\nSemantic\nSegmentation Dataset\nDetection Dataset (box)\nUniversal Multi-Dataset Multi-Task\n Segmentation Model\nPanoptic Segmentation Task\nSemantic Segmentation Task\nInstance Segmentation Task\nDaTaSeg\nFigure 1: Considering the similarities across\nvarious segmentation tasks, and recognizing\nthe potential for enhancing segmentation per-\nformance by harnessing data from multiple\nsources, we propose to train a universal\nsegmentation model, DaTaSeg, on multiple\ndatasets to perform multiple segmentation\ntasks.\nTo encourage knowledge sharing and transfer among the\nmultiple segmentation sources, our network architecture\nshares the same set of weights across all datasets and tasks.\nIn addition, we utilize text embeddings as the class clas-\nsifier, which map class labels from different datasets into\na shared semantic embedding space. This design further\nenhances knowledge sharing among categories with simi-\nlar meanings in different datasets, e.g., \u2018windowpane\u2019 and\n\u2018window-other\u2019. Our approach can be contrasted with an\nalternative of using dataset-specific model components \u2014\nwe show in our experiments that our simpler, unified ap-\nproach leads to improved performance and enables open-\nvocabulary segmentation via simply switching the text\nembedding classifier.\nPutting these techniques together, we propose DaTaSeg, a\nuniversal segmentation model, together with a cotraining\nrecipe for the multi-task and multi-dataset setting. We train\nDaTaSeg on the ADE20k semantic [68], COCO panop-\ntic [28], and Objects365 detection [51] datasets. We show\nthat DaTaSeg improves performance on all datasets com-\nparing with training separately, and significantly benefits\nrelatively small-scale datasets (ADE20k semantic), out-\nperforming a model trained only on ADE20k semantic by\n+6.1 and +5.1 mIoU with ResNet50 [21] and ViTDet-B [34] backbones. The multi-dataset multi-task\nsetting also allows us to seamlessly perform weakly-supervised segmentation by transferring knowl-\nedge from other fully supervised source datasets, which we demonstrate on ADE20k panoptic and\nObjects365 instance segmentation. DaTaSeg also directly transfers to other datasets not seen during\ntraining. It outperforms open-vocabulary panoptic methods on Cityscapes panoptic dataset [10]\nand performs comparably with open-vocabulary segmentation works on Pascal Context semantic\ndataset [44] .\nTo summarize our contributions, we present DaTaSeg, a single unified framework for multi-dataset\nmulti-task segmentation. DaTaSeg leverages knowledge from multiple sources to boost performance\non all datasets. It seamlessly enables weakly-supervised segmentation, directly transfers to other\ndatasets and is capable of open-vocabulary segmentation. As an additional contribution, we have\nannotated a subset of the Objects365 validation set with groundtruth instance masks and will release\nthem as an evaluation benchmark for instance segmentation.\n2\nRelated Work\nMuti-dataset training: Training on multiple datasets has become popular for developing robust\ncomputer vision models [59, 71, 31, 42, 56, 62]. For object detection, Wang et al. [59] train an\nobject detector on 11 datasets in different domains and show improved robustness. UniDet [71] trains\na unified detector on 4 large-scale detection datasets with an automatic label merging algorithm.\nDetectionHub [42] employs text embeddings to accommodate different vocabularies from multiple\ndatasets, and features dataset-specific designs. For segmentation, MSeg [31] manually merges the\nvocabularies of 7 semantic segmentation datasets, and trains a unified model on all of them; however,\nthe manual efforts are expensive and hard to scale. LMSeg [70] dynamically aligns segment queries\nwith category embeddings. UniSeg [26] explores the label relation and conflicts between multiple\ndatasets in a learned way. Most of these works merge datasets using a unified vocabulary on a single\ntask, while our work focuses on a more challenging problem: we merge datasets with different\nvocabularies and different tasks.\n2\n'person' proposal\n'person' proposal\n'horse' proposal\n'building' proposal\n'building' proposal\n'person' 1\n'person' 2\n'horse' 1\n'building'\n'person'\nHungarian\nMatching\nPanoptic Segmentation\nSemantic Segmentation\n'horse'\n'building'\n'person' 1\n'person' 2\n'horse' 1\nWeakly-Supervised Instance Segmentation\nHungarian\nMatching\nHungarian\nMatching\nCross-Entropy Loss\nFocal Loss\nDice Loss\nCross-Entropy Loss\nFocal Loss\nDice Loss\nCross-Entropy Loss\nProjection Loss (box GT)\nMERGE\nUniversal\nRepresentation\nFigure 2: Universal representation, MERGE operations, and losses for different segmentation tasks. We use a\nuniversal representation for all tasks: a set of mask proposals with class predictions. Then we adopt distinct\nMERGE operations based on the segmentation task. For panoptic segmentation, we merge proposals predicting\nthe same \u201cstuff\u201d category. For semantic segmentation, both \u201cthing\u201d and \u201cstuff\u201d categories undergo merging. In\ninstance segmentation, we do not perform MERGE and there are no \u201cstuff\u201d categories. During training, we use\nHungarian matching, and apply different losses based on the supervision types.\nUnified segmentation model: Panoptic segmentation [28] unifies semantic and instance segmen-\ntation. Prior works [27, 61, 64, 6, 47] exploit separate modules for semantic segmentation [40, 5]\nand instance segmentation [18, 20], followed by another fusion module. Recently, the mask trans-\nformer framework proposed by MaX-DeepLab [58] directly predicts class-labeled masks, allowing\nend-to-end panoptic segmentation. MaskFormer [9] and K-Net [67] adopt a single transformer-based\nmodel for different segmentation tasks. Mask2Former [7] improves upon MaskFormer by proposing\nmasked attention, while kMaX-DeepLab [65] develops k-means cross-attention. OneFormer [24]\nextends Mask2Former with a multi-task train-once design. All these works still train separate weights\non different datasets. By contrast, we aim at a single unified model that can perform well across all\nsegmentation tasks and datasets.\nWeakly-supervised segmentation: Facing the issue of expensive segmentation annotations, many\nworks have proposed to learn segmentation masks from cheaper forms of supervision [45, 11, 37]. In\nparticular, box-supervised approaches are most related to our work, including BoxInst [54] and Cut-\nand-Paste [49] for instance segmentation, Box2Seg [30] and BCM [53] for semantic segmentation,\nand DiscoBox [32] and SimpleDoesIt [25] for both semantic and instance segmentation. Despite\nthe popularity of box-driven semantic and instance segmentation, fewer attempts have been made\nat weakly-supervised panoptic segmentation. Li et al. [33] employ a pretrained model to generate\npseudo-ground-truth in advance, in addition to weak supervision from bounding boxes and image\nlevel labels. Shen et al. [52] add a handcrafted branch on top of semantic and instance segmentation\nmodels to generate panoptic proposals. Unlike these methods that require customized components,\nour approach can realize knowledge sharing among datasets with different forms of annotations in a\nmore systematic and data-centric fashion, and seamlessly enable weakly-supervised segmentation.\n3\nMethod\n3.1\nA universal segmentation representation\nSegmentation aims to group pixels of the same concept together. We propose to use an intermediate\nand universal representation, namely mask proposals, for all segmentation tasks. A mask proposal\nis a binary foreground mask with a class prediction. We use this representation for its versatility:\none mask proposal can represent a single instance, any number of instances, a region, or even a part\nof an object. They can overlap and can be combined to form higher-level segmentation outputs \u2014\nan instance (thing) or a region of the same semantic class (stuff). Thus they are well suited for the\ndifferent flavors of segmentation considered in this paper. We note that a similar concept has been\nused in existing works [58, 9, 7, 65], for specific segmentation tasks on a single dataset. We show this\nrepresentation is especially beneficial in our multi-dataset setting, as different datasets define things\nand stuff differently. For example, \u2018table\u2019 is a thing category in the ADE20k panoptic dataset, but\nthere is a \u2018table-merged\u2019 stuff category in COCO panoptic. In our framework, both thing and stuff\ncategories are represented in this same representation, and we treat them differently in the next step.\n3\n3.2\nMerging predictions for specific tasks\nNotations: Let (\u02c6zj, c\nMj) denote the j-th mask proposal with its class prediction, where c\nMj \u2208 RH\u00d7W\ndenotes (pre-sigmoid) logits for the mask proposal and \u02c6zj,ck \u2208 R denotes the logit for class ck. We\nassume a fixed set of N mask proposals. Cd,thing and Cd,stuff are the set of thing and stuff categories\nin dataset d.\nMerge operation (MERGE): Given the varying formats of groundtruth annotations for different\nsegmentation tasks, we propose a MERGE operation to merge the mask proposals predicting the same\ncategory ck. We adopt a simple element-wise max operation to merge the mask proposal logits. In\nparticular, the merged mask proposal logits at the (p, q) location for class ck is computed as:\nM(ck)p,q = max\nj\n\u00001[argmax(\u02c6zj) = ck] \u00b7 c\nMj,p,q\n\u0001\n.\n(1)\nWe choose element-wise max so that applying MERGE on raw mask logits is equivalent to applying\nMERGE on the soft mask predictions post-sigmoid. We also choose to merge at the level of raw\nlogits for numerical stability. If no proposal predicts class ck, then the corresponding merged mask\nM(ck)p,q is set to -100 to ensure it is close to 0 after sigmoid.\nIn addition to merging masks, we merge their corresponding class predictions by simply averaging\nthe class prediction logits of the proposals predicting class ck:\nZ(ck) =\nP\nj(1[argmax(\u02c6zj) = ck] \u00b7 \u02c6zj)\nP\nj(1[argmax(\u02c6zj) = ck]) + \u03f5 ,\n(2)\nwhere \u03f5 is a small number to prevent division by zero.\nHow we apply the above merge operations depends on the task, as we now describe:\nPanoptic segmentation: To make the predicted mask proposals have the same format as the\ngroundtruth, we apply MERGE to all stuff categories ck \u2208 Cd,stuff.\nSemantic segmentation: In contrast with panoptic segmentation, the semantic segmentation\ngroundtruth for each thing category is also a single mask (i.e., thing categories are treated equivalently\nas stuff categories). We thus apply MERGE to all predicted mask proposals to cover all thing and stuff\ncategories Cd,stuff \u222a Cd,thing.\nInstance segmentation: MERGE is not needed in this task, since the desired outputs are separate\nmasks for separate instances. There are no stuff categories in the vocabulary (Cd,stuff = \u2205), and\ntherefore no stuff proposals.\nTraining: During training, we apply the corresponding MERGE operations based on the segmentation\ntask. We then use one-to-one Hungarian matching [29] to match the merged outputs with the\ngroundtruth, and calculate the training losses. Detailed training losses are provided in Sec. 3.5.\nInference: At inference time, we apply the same MERGE operations to predicted mask proposals\nbased on the given task. We post-process the mask proposals to obtain non-overlapping outputs\nfor panoptic and semantic segmentation. For instance segmentation, we simply take the top mask\nproposals as final outputs, since overlaps are allowed. Please refer to supplementary for more details.\nWe illustrate how the MERGE operation works for different segmentation tasks in Fig. 2.\n3.3\nWeakly-supervised instance segmentation\nBounding boxes are much cheaper to annotate than instance masks, and thus detection datasets can\nhave a larger scale than instance segmentation datasets. In order to train on larger detection datasets\nwith only bounding box annotations, as well as to demonstrate the versatility of our framework to\nhandle various tasks, we propose to perform weak-bounding-box-supervised instance segmentation.\nWe adopt a simple projection loss Lproj from [55], which measures consistency of vertical and\nhorizontal projections of predicted masks c\nMj against groundtruth boxes:\nLproj(c\nMj, bj) = Ldice(Projx(S(c\nMj)), Projx(bj)) + Ldice(Projy(S(c\nMj)), Projy(bj)),\n(3)\n4\ndataset 2 (C2 classes)\ninput image\nBackbone\nPixel Decoder\nN mask proposals\nN segment\nqueries\nTransformer Decoder\nfeature map \nClass Head\nMask Head\nsky\nhorse\nlearnable background\nperson\nsky\nhorse\ndataset 1 (C1 classes)\nperson\nFrozen Pre-\ntrained Text\nEncoder\nN class predictions\nsky\nhorse\nperson\ndataset-specific\nshared\nFigure 3: Overview of our universal multi-dataset multi-task segmentation model (DaTaSeg). We feed N\nlearnable segment queries into a transformer decoder, which cross-attends to multi-scale image features from the\npixel decoder. The outputs serve as the universal representation for all tasks: N mask proposals accompanied by\nN class predictions. To promote knowledge sharing, we share all network parameters across different datasets\nand tasks. The only dataset-specific design is a classifier consisting of frozen text embeddings of categories\nspecific to each dataset. Additionally, we employ a shared learnable background classifier.\nwhere Ldice is the dice loss [43]; bj is the groundtruth bounding box matched to j-th mask proposal\nprediction; Projx/y denotes the projection operation along the x or y axis, which can be implemented\nby a max operation along the axis; and S(\u00b7) denotes the sigmoid function.\nBy itself, a box consistency loss such as Eqn. 3 is insufficient as a supervision signal for segmentation\n(e.g., Eqn. 3 is equally satisfied by predicting the bounding box of an object instead of its mask). Thus\nother works have resorted to additional, often more complex, loss terms (such as the pairwise affinity\nloss from [55]). However, by training on multiple datasets and multiple segmentation tasks, these\nhandcrafted losses are not necessary, as our model can transfer knowledge from other fully-supervised\ntasks on other datasets.\n3.4\nNetwork architecture with knowledge sharing\nNetwork architecture: We now describe the network architecture (similar to [7]) that predicts mask\nproposal and class prediction pairs (\u02c6zj, c\nMj). The input image first goes through a backbone, and we\nuse a pixel decoder to 1) output multi-scale image features Imulti, and 2) output a high-resolution\nfeature map F that fuses information from the multi-scale image features. N mask proposal pairs are\nthen generated from N learnable segment queries Qs: The segment queries are fed into a transformer\ndecoder, which cross attends to multi-scale image features Imulti from the pixel decoder. The N\ndecoder outputs are then passed to class embedding and mask embedding heads (both MLPs) to obtain\nN (mask embedding, class embedding) pairs. To extract final mask proposals and class predictions,\nwe apply a dot product between the high-resolution feature map F and the mask embeddings to\nobtain N mask proposal logits c\nMj. And to obtain class predictions, we compute dot products of the\nclass embeddings against a per-category classifier wk for each category ck. Fig. 3 shows an overview.\nKnowledge sharing: We bake knowledge sharing into our model in two ways. First, all network\nparameters are shared across all datasets, allowing knowledge sharing among different datasets and\nknowledge transfer among different tasks. Additionally, to share knowledge gathered from training\nsamples of similar categories in multiple datasets, e.g., \u2018windowpane\u2019 in ADE20k and \u2018window-other\u2019\nin COCO, we propose to map all category names into a shared semantic embedding space. We feed\nthe category names into a frozen pre-trained text encoder to set the per-category classifier wk. i.e., we\nlet the fixed text embeddings serve as classifiers. This is similar to open-vocabulary segmentation [15],\nbut with a different purpose: our focus here is on knowledge sharing among different datasets. This\nautomatic approach scales better than manually unifying label spaces in different datasets [31].\n5\n3.5\nCo-training strategy\nWe adopt a simple co-training strategy: at each iteration, we randomly sample one dataset, then\nsample an entire batch for that iteration from the selected dataset. This can be contrasted with\nsampling from multiple datasets at each iteration. The main advantage for our strategy is that it is\nsimple to implement, and allows for more freedom to use different settings for different datasets,\nas well as to apply distinct losses to various tasks. To account for different dataset sizes, we use\nper-dataset sampling ratios, similar to [36].\nFor fully-supervised tasks, we employ direct mask supervision, which can be obtained from panop-\ntic/semantic/instance segmentation groundtruth. To calculate the Hungarian matching cost and the\ntraining loss, we use a combination of focal binary cross-entropy loss Lfocal [39] and dice loss\nLdice [43], following [9]. Regarding weak bounding box supervision, we adopt the projection loss\nLproj introduced in Sec. 3.3 for the matching cost and training loss. In both cases of supervision, we\nuse the negative of the class prediction probability p as the classification matching cost [4], and use\ncross-entropy loss Lce for training. The total training loss is:\nLd = \u03bbce,dLce + \u03bbfocal,dLfocal + \u03bbdice,dLdice + \u03bbproj,dLproj.\n(4)\nThe Hungarian matching cost is defined similarly:\nCd = \u2212\u00b5ce,d \u00b7 p + \u00b5focal,dLfocal + \u03bbdice,dLdice + \u00b5proj,dLproj.\n(5)\nHere, \u03bb\u2217,d and \u00b5\u2217,d are the weights for dataset d.\n4\nExperiments\nDatasets and metrics: We train and evaluate DaTaSeg on COCO panoptic [28] and ADE20k\nsemantic [68] using mask supervision, as well as Objects365-v2 [51] detection datasets using\nbounding box weak supervision. COCO panoptic is the most popular panoptic segmentation\nbenchmark with 118,287 training images and 5,000 validation images. COCO has 80 thing categories\nand 53 stuff categories. ADE20k semantic is one of the most widely used semantic segmentation\nbenchmarks with 150 categories, 20,210 training images, and 2,000 validation images.\nFor evaluation, besides the training datasets, we also evaluate on ADE20k panoptic, which uses\nthe same validation images as ADE20k semantic but with panoptic annotations. The original 150\ncategories are divided into 100 thing categories and 50 stuff categories. Finally, we train on the\nObjects365-v2 detection (bounding boxes only) dataset, which has 365 categories and 1,662,292\ntraining images. To evaluate the weakly-supervised instance segmentation results, we manually label\na subset of 1,000 images from the Objects365 validation set. Our annotation pipeline involves 20\nwell-trained human annotators, following the protocol in [2], without using any automatic assistant\ntools. We will release this Objects365 instance segmentation validation set as a public benchmark.\nTo compare DaTaSeg with the state-of-the-art methods, we in addition evaluate on popular open-\nvocabulary segmentation benchmarks. Following OpenSeg [15], we evaluate on PASCAL Context\ndatasets [44] with 5k val images. We use its two versions: 59 classes (PC-59) and 459 classes\n(PC-459). We also evaluate on the 500-image val set of Cityscapes panoptic dataset [10], which\nfocuses on urban street scenes with 11 stuff categories and 8 thing categories.\nWe report results in standard evaluation metrics: panoptic quality (PQ), mean intersection-over-\nunion (mIoU), and mean average precision (AP) for panoptic, semantic, and instance segmentation,\nrespectively.\nImplementation details: We experiment with ResNet [21] or ViTDet [34] backbones. For ResNet,\nwe use an ImageNet-1K [50] pretrained checkpoint, with a backbone learning rate multiplier of 0.1 [4];\nthe pixel decoder consists of a transformer encoder [57] and a modified FPN [38] following [9],\nexcept that we use Layer Norm [1] and GeLU [23] activation for training stability. For ViTDet, we use\nthe ImageNet-1K MAE pretrained checkpoint [19] with the layerwise lr decay [34]; the pixel decoder\nis the simple feature pyramid inside ViTDet, whose outputs are upsampled and added together to get\nthe high resolution feature map F. The mask embedding head is a 3-layer MLP. The class embedding\nhead contains a linear layer followed by ReLU. We use CLIP-L/14 [48] as the pretrained text encoder.\nWe use 100 segment queries Qs, unless otherwise stated. See supplementary for more details.\n6\nFully-Supervised\nWeakly-Supervised Transfer\nBackbone\nModel\nADE\nCOCO\nADE\nO365\nsemantic\npanoptic\nsemantic \u2192 panoptic\nbox \u2192 instance\nmIoU\nPQ\nPQ\nmask AP\nResNet50\nSeparate\n42.0\n48.2\n26.9\n12.3\nDaTaSeg\n48.1 (+6.1)\n49.0 (+0.8)\n29.8 (+2.9)\n14.3 (+2.0)\nViTDet-B\nSeparate\n46.3\n51.9\n27.5\n14.7\nDaTaSeg\n51.4 (+5.1)\n52.8 (+0.9)\n32.9 (+5.4)\n16.1 (+1.4)\nViTDet-L\nDaTaSeg\n54.0\n53.5\n33.4\n16.4\nTable 1: Comparing DaTaSeg and separate dataset-specific models. We show results on both the training\ntasks (fully-supervised) and new tasks (weakly-supervised transfer). Our single DaTaSeg outperforms separately\ntrained models on all datasets. They are trained under the same settings, so the performance gains come from\nknowledge in other datasets through our multi-dataset multi-task cotraining recipe. We observe: 1) DaTaSeg\nsignificantly improves tasks with limited data (ADE20k semantic); 2) DaTaSeg enables weakly-supervised\nknowledge transfer (ADE20k panoptic and O365 instance); 3) DaTaSeg scales well with backbones.\nTraining sets\nFully-Supervised\nWeakly-Supervised Transfer\nADE\nCOCO\nO365\nADE\nCOCO\nADE\nO365\nsemantic\npanoptic\nbbox\nsemantic\npanoptic\nsemantic \u2192 panoptic\nbox \u2192 instance\nmIoU\nPQ\nPQ\nmask AP\n\u2713\n42.0\n8.4\n26.7\n1.0\n\u2713\n15.3\n48.2\n11.6\n5.8\n\u2713\n11.0\n12.4\n5.8\n12.3\n\u2713\n\u2713\n46.3 (+4.3)\n14.0\n26.4 (-0.3)\n15.0 (+2.7)\n\u2713\n\u2713\n18.3\n48.9 (+0.7)\n12.3\n15.2 (+2.9)\n\u2713\n\u2713\n47.3 (+5.3)\n49.0 (+0.8)\n30.5 (+3.8)\n5.6\n\u2713\n\u2713\n\u2713\n48.1 (+6.1)\n49.0 (+0.8)\n29.8 (+2.9)\n14.3 (+2.0)\nTable 2: Importance of training on multiple datasets. We train DaTaSeg on all combinations the three datasets.\nExperiments are done on ResNet50 under same settings. DaTaSeg scales with the number of training datasets.\n4.1\nDaTaSeg improves over dataset-specific models\nAs an apples-to-apples comparison, we separately train a model on each dataset. We ensure that the\nmodels see the same number of images in cotraining and separately-training for each dataset. Table 1\n(left) shows the results. DaTaSeg outperforms separately trained models on all datasets. Since we use\nexactly the same settings, we attribute the performance boost to our multi-dataset multi-task training\nrecipe, which harnesses knowledge from multiple sources. Especially, DaTaSeg leads to a significant\nperformance boost on ADE20k semantic: +6.1 mIoU and +5.1 mIoU with ResNet50 and ViTDet-B\nbackbones, respectively. This proves our argument that cotraining on more data helps overcome the\ndata limitation issue of smaller-scale segmentation datasets.\n4.2\nDaTaSeg enables weakly-supervised transfer\nIn Table 1 (right), we evaluate DaTaSeg on ADE20k panoptic and Objects365 instance segmentation\ntasks. Note we only have weak supervision (ADE20k semantic and Objects365 bbox) during training.\nAgain, we compare to models separately trained on one dataset as our baselines. Specifically, we use\nthe ADE semantic for ADE panoptic evaluation, and an Objects365 model trained only using weak\nsupervision for Objects365 instance evaluation, respectively.\nBy cotraining DaTaSeg in a multi-dataset multi-task fashion, we improve the ADE20k panoptic\nperformance by +2.9 PQ and +5.4 PQ with ResNet50 and ViTDet-B backbones. Our design\nallows the panoptic segmentation knowledge to transfer from COCO to ADE20k. For reference, the\nfully-supervised performance is 37.7 PQ on ResNet50 and 42.3 PQ on ViTDet-B.\nOn Objects365, we only train with a weak box consistency loss Lproj. The multi-dataset multi-\ntask training recipe improves the mask AP by +2.0 AP and +1.4 AP for the two backbones. The\nimprovement is most likely from the instance segmentation knowledge in COCO panoptic.\n4.3\nDaTaSeg scales with size of backbones\nOur DaTaSeg design is orthogonal to detailed network architectures, as long as it is able to output\nthe universal representation (binary mask proposals with class predictions). This is supported by\n7\nMethod\nBackbone\nTraining data\nPC-59\nPC-459\nCOCO\nCityscapes\nmIoU\nmIoU\nmIoU\nPQ\nODISE [63]\nUNet+M2F\nLAION+CLIP+COCO\n55.3\n13.8\n52.4\n23.9\nMaskCLIP [12]\nR-50\nCOCO pan+CLIP\n45.9\n10.0\n\u2013\n\u2013\nOVSeg [35]\nR-101c\nCOCO stuff+cap\n53.3\n11.0\n\u2013\n\u2013\nSwin-B\n55.7\n12.4\n\u2013\n\u2013\nOpenSeg [15]\nR-101\nCOCO pan+cap\n42.1\n9.0\n36.1\n\u2013\nEff-b7\nCOCO+Loc. Narr.\n44.8\n11.5\n38.0\n\u2013\nDaTaSeg\nR-50\nCOCO panoptic\n50.9\n11.1\n57.7\n30.0\nViTDet-B\n+ADE semantic\n51.1\n11.6\n62.7\n28.0\nViTDet-L\n+O365 bbox\n51.4\n11.1\n62.9\n29.8\nTable 3: Compare DaTaSeg to state-of-the-art open-vocabulary segmentation models. We apply DaTaSeg\ndirectly to other semantic or panoptic segmentation datasets without finetuning. We compare to ODISE [63],\nMaskCLIP [12], OVSeg [35], and OpenSeg [15] on their corresponding benchmarks. We only conduct system-\nlevel comparison due to differences in training data and backbones. DaTaSeg performs comparably on all\nsemantic segmentation benchmarks and outperforms the recent work ODISE on Cityscapes. PC: Pascal Context.\nCOCO: COCO is included in the training set for all models, and is not completely \u201copen-vocabuarly\u201d.\n# queries\n50\n100\n150\nADE semantic\nmIoU\n47.4\n48.1\n48.7\nCOCO panoptic\nPQ\n48.7\n49.0\n48.9\nADE panoptic\u2020\nPQ\n30.7\n29.8\n29.1\nO365 instance\u2020\nAP\n13.4\n14.3\n15.8\nTable 4: In general, the performance of DaTaSeg\nincreases as we increase the number of segment\nqueries. Besides, using only 50 queries already\nachieves good performance. Experiments are con-\nducted using a ResNet50 backbone.\n\u2020: weakly-\nsupervised tasks.\nDaTaSeg\n+D-S modules\nADE semantic\nmIoU\n48.1\n48.1 (-0.0)\nCOCO panoptic\nPQ\n49.0\n46.0 (-3.0)\nADE panoptic\u2020\nPQ\n29.8\n26.9 (-2.9)\nO365 instance\u2020\nAP\n14.3\n10.9 (-3.4)\nTable 5: Adding dataset-specific (D-S) modules\nhurt performance on almost all datasets. This\nshows the importance of sharing all parameters\nfor better knowledge sharing, which particularly\nbenefits the weakly-supervised tasks. Experiments\nare conducted on a ResNet50 backbone.\nthe consistent performance gains among ResNet50, ViTDet-B, and ViTDet-L backbones as shown\nin Table 1. It also scales well as we increase the sizes of the backbones. With ViTDet-L, DaTaSeg\nreaches 53.5 mIoU on ADE20k semantic and 54.0 PQ on COCO panoptic in a single model.\n4.4\nDaTaSeg scales with number of datasets\nWe study how DaTaSeg scales with the number of training datasets. We train DaTaSeg on all\ncombinations of one, two, and three datasets among the ADE20k semantic, COCO panoptic, and\nObjects365 detection datasets, in order to conduct a comprehensive study. Table 2 presents the results.\nLooking at each column, we see that performance on each dataset generally improves as the number\nof training datasets increases, especially for ADE semantic.\nSince DaTaSeg shares all parameters among all datasets and tasks, we can evaluate the cross-dataset\ntransfer performance. We notice the model transfers to datasets that are not trained. e.g., A model\ntrained on COCO panoptic and Objects365 detection achieves 18.3 mIoU on ADE semantic, which\nis comparable to open-vocabulary segmentation performance (18.0 LSeg+ [15] with ResNet101).\n4.5\nDaTaSeg enables open-vocabulary segmentation\nA further advantage of our fully-shared architecture is that we have the ability to directly transfer to\nother segmentation datasets. We simply switch the text embedding classifier with the vocabularies\nin a target dataset not used for training. Table 3 shows the results comparing DaTaSeg to several\nopen-vocabulary works: ODISE [63] and MaskCLIP [12] are open-vocabulary panoptic segmentation\nmethods and OVSeg [35] and OpenSeg [15] are open-vocabulary semantic segmentation approaches.\nUnlike these works, our method does not train on large-scale image-text data or use pretrained\nimage-text models (except the pretrained text encoder), which puts our method at a disadvantage.\nDespite this disadvantage, DaTaSeg outperforms ODISE on the Cityscapes panoptic dataset. Note that\nthere is a domain gap between our training sets and Cityscapes, which focuses on urban street scenes.\nDaTaSeg achieves comparable performance on PC-59 and PC-459. All methods in comparison train\non COCO panoptic and ours has the best performance on COCO semantic. These results show that\nDaTaSeg enables open-vocabulary segmentation via our multi-dataset multi-task training approach.\n8\n(a) COCO panoptic\n(b) ADE semantic\n(c) ADE20k panoptic (weakly-supervised)\n(d) Objects365 instance (weakly-supervised)\nFigure 4: Qualitative results of DaTaSeg on all datasets & tasks. For every pair of images, the left is DaTaSeg\nprediction and the right is groundtruth. DaTaSeg succeeds on hard cases (e.g., transparent wine glasses on the\ntop left) as well as weakly-supervised datasets. For object instances in panoptic and instance segmentation, the\ncolors are not matched to the groundtruths. Black denotes ignored regions.\n4.6\nDaTaSeg scales with number of queries\nWe perform ablation study on the number of segment queries Qs and show the results in Table 4. The\nperformance improves as we increase the number of queries from 50 to 150, on all datasets except\nADE20k panoptic. Besides, DaTaSeg achieves reasonably good performance with only 50 queries,\nwhich may benefit memory-limited application scenarios.\n4.7\nAdding dataset-specific modules hurts performance\nIn multi-dataset learning, prior works have introduced dataset-specific modules [42], in order to\naddress inconsistencies across datasets [31, 56]. However, such a design may weaken knowledge\nsharing across different datasets and tasks. This is especially so for weakly-supervised tasks which\nmay rely more on other datasets and tasks to compensate for the weak supervision. To study this issue\nthoroughly, we carefully design several dataset-specific modules, including dataset-specific queries,\ndataset-specific heads, and dataset-specific prompts. These modules are lightweight by design so as\nto still encourage knowledge sharing through other shared parameters.\nIn Table 5, we evaluate the effectiveness of these dataset-specific modules by adding them to DaTaSeg,\nand keeping all other settings the same. Comparing with DaTaSeg that shares all parameters, results\nreveal that using dataset-specific modules hurts performance on all datasets except ADE20k semantic.\nMoreover, we see that the performance drops more on weakly-supervised tasks (ADE panoptic and\nO365 instance). This verifies that removing dataset-specific modules yields better knowledge sharing\nacross datasets and tasks, and thus greatly improves performance. See supplementary for model\ndetails.\n4.8\nQualitative analysis\nWe show qualitative results (with ViTDet-L) in Fig. 4. DaTaSeg performs well in both fully and\nweakly-supervised settings. We observe that localization quality is in general good, while classifica-\ntion is more error-prone. Nevertheless, DaTaSeg succeeds in challenging cases including transparent\nobjects, occlusion, and crowd scenes.\n9\n5\nConclusion\nThe goal of our work is to train a single universal model on multiple datasets and multiple seg-\nmentation tasks (i.e., semantic, instance and panoptic segmentation). We present DaTaSeg, which\nleverages a universal segmentation representation and shared semantic embedding space for classi-\nfication. DaTaSeg surpasses separately trained models, leading to especially significant gains for\nsmaller datasets such as ADE20k semantic. It unlocks a new weakly-supervised transfer capabil-\nity on datasets that were not explicitly trained for a particular task (e.g., ADE20k panoptic and\nObjects365 instance). DaTaSeg also directly transfer to more segmentation datasets and enables\nopen-vocabulary segmentation. We believe that in the future, this multi-task multi-dataset setting will\nbe the norm rather than the outlier as it currently is, and we see our paper as taking a step in this direc-\ntion.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016. 6\n[2] Rodrigo Benenson, Stefan Popov, and Vittorio Ferrari. Large-scale interactive object segmentation with\nhuman annotators. In CVPR, 2019. 6\n[3] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick P\u00e9rez. Zero-shot semantic segmentation.\nNeurIPS, 2019. 14\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 6\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nTPAMI, 2017. 1, 3\n[6] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-\nChieh Chen. Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmenta-\ntion. In CVPR, 2020. 3\n[7] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\nattention mask transformer for universal image segmentation. In CVPR, 2022. 1, 3, 5, 18, 19\n[8] Bowen Cheng, Omkar Parkhi, and Alexander Kirillov. Pointly-supervised instance segmentation. In CVPR,\n2022. 18\n[9] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for\nsemantic segmentation. NeurIPS, 2021. 1, 2, 3, 6\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.\nIn CVPR, 2016. 2, 6\n[11] Jifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploiting bounding boxes to supervise convolutional\nnetworks for semantic segmentation. In ICCV, 2015. 3\n[12] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary panoptic segmentation with maskclip. arXiv\npreprint arXiv:2208.08984, 2022. 8\n[13] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes (voc) challenge. IJCV, 2010. 14\n[14] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret\nZoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, 2021. 19\n[15] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with\nimage-level labels. In ECCV, 2022. 5, 6, 8\n[16] Zhangxuan Gu, Siyuan Zhou, Li Niu, Zihan Zhao, and Liqing Zhang. Context-aware feature generation\nfor zero-shot semantic segmentation. In ACM MM, 2020. 14\n10\n[17] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation.\nIn CVPR, 2019. 14\n[18] Bharath Hariharan, Pablo Arbel\u00e1ez, Ross Girshick, and Jitendra Malik. Simultaneous detection and\nsegmentation. In ECCV, 2014. 1, 3\n[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders\nare scalable vision learners. In CVPR, 2022. 6\n[20] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 1, 3\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016. 2, 6\n[22] Xuming He, Richard S Zemel, and Miguel \u00c1 Carreira-Perpi\u00f1\u00e1n. Multiscale conditional random fields for\nimage labeling. In CVPR, 2004. 1\n[23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016. 6\n[24] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One\ntransformer to rule universal image segmentation. In CVPR, 2023. 1, 3\n[25] Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele. Simple does it: Weakly\nsupervised instance and semantic segmentation. In CVPR, 2017. 3\n[26] Dongwan Kim, Yi-Hsuan Tsai, Yumin Suh, Masoud Faraki, Sparsh Garg, Manmohan Chandraker, and\nBohyung Han. Learning semantic segmentation from multiple datasets with label shifts. In ECCV, 2022.\n1, 2\n[27] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In\nCVPR, 2019. 1, 3\n[28] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation.\nIn CVPR, 2019. 1, 2, 3, 6\n[29] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly,\n1955. 4\n[30] Viveka Kulharia, Siddhartha Chandra, Amit Agrawal, Philip Torr, and Ambrish Tyagi. Box2seg: Attention\nweighted loss and discriminative feature learning for weakly supervised segmentation. In ECCV, 2020. 3\n[31] John Lambert, Zhuang Liu, Ozan Sener, James Hays, and Vladlen Koltun. Mseg: A composite dataset for\nmulti-domain semantic segmentation. In CVPR, 2020. 1, 2, 5, 9, 16\n[32] Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Radhakrishnan, Guilin Liu, Yuke Zhu, Larry S\nDavis, and Anima Anandkumar. Discobox: Weakly supervised instance segmentation and semantic\ncorrespondence from box supervision. In ICCV, 2021. 3\n[33] Qizhu Li, Anurag Arnab, and Philip HS Torr. Weakly-and semi-supervised panoptic segmentation. In\nECCV, 2018. 3\n[34] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones\nfor object detection. In ECCV, 2022. 2, 6\n[35] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter\nVajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. arXiv\npreprint arXiv:2210.04150, 2022. 8\n[36] Valerii Likhosherstov, Anurag Arnab, Krzysztof Choromanski, Mario Lucic, Yi Tay, Adrian Weller, and\nMostafa Dehghani. Polyvit: Co-training vision transformers on images, videos and audio. arXiv preprint\narXiv:2111.12993, 2021. 6\n[37] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolutional\nnetworks for semantic segmentation. In CVPR, 2016. 3\n[38] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In CVPR, 2017. 6, 18\n11\n[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object\ndetection. In ICCV, 2017. 6\n[40] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-\ntion. In CVPR, 2015. 1, 3\n[41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019. 19\n[42] Lingchen Meng, Xiyang Dai, Yinpeng Chen, Pengchuan Zhang, Dongdong Chen, Mengchen Liu, Jianfeng\nWang, Zuxuan Wu, Lu Yuan, and Yu-Gang Jiang. Detection hub: Unifying object detection datasets via\nquery adaptation on language embedding. arXiv preprint arXiv:2206.03484, 2022. 2, 9\n[43] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In 3DV. Ieee, 2016. 5, 6\n[44] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel\nUrtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild.\nIn CVPR, 2014. 2, 6\n[45] George Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and semi-supervised\nlearning of a deep convolutional network for semantic image segmentation. In ICCV, 2015. 3\n[46] Giuseppe Pastore, Fabio Cermelli, Yongqin Xian, Massimiliano Mancini, Zeynep Akata, and Barbara\nCaputo. A closer look at self-training for zero-label semantic segmentation. In CVPRW, 2021. 14\n[47] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature\npyramid and switchable atrous convolution. In CVPR, 2021. 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, 2021. 6\n[49] Tal Remez, Jonathan Huang, and Matthew Brown. Learning to segment via cut-and-paste. In ECCV, 2018.\n3\n[50] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large\nscale visual recognition challenge. IJCV, 115:211\u2013252, 2015. 6\n[51] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365: A large-scale, high-quality dataset for object detection. In ICCV, 2019. 2, 6\n[52] Yunhang Shen, Liujuan Cao, Zhiwei Chen, Feihong Lian, Baochang Zhang, Chi Su, Yongjian Wu, Feiyue\nHuang, and Rongrong Ji. Toward joint thing-and-stuff mining for weakly supervised panoptic segmentation.\nIn CVPR, 2021. 3\n[53] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Box-driven class-wise region masking and\nfilling rate guided loss for weakly supervised semantic segmentation. In CVPR, 2019. 3\n[54] Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen. Boxinst: High-performance instance segmentation\nwith box annotations. In CVPR, 2021. 3\n[55] Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen. Boxinst: High-performance instance segmentation\nwith box annotations. In CVPR, 2021. 4, 5\n[56] Jasper Uijlings, Thomas Mensink, and Vittorio Ferrari. The missing link: Finding label relations across\ndatasets. In ECCV, 2022. 2, 9, 16\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 6\n[58] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end\npanoptic segmentation with mask transformers. In CVPR, 2021. 1, 2, 3\n[59] Xudong Wang, Zhaowei Cai, Dashan Gao, and Nuno Vasconcelos. Towards universal object detection by\ndomain attention. In CVPR, 2019. 2\n[60] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection\nnetwork for zero-and few-label semantic segmentation. In CVPR, 2019. 14\n12\n[61] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, and Raquel Urtasun. Upsnet:\nA unified panoptic segmentation network. In CVPR, 2019. 3\n[62] Hang Xu, Linpu Fang, Xiaodan Liang, Wenxiong Kang, and Zhenguo Li. Universal-rcnn: Universal object\ndetector via transferable graph r-cnn. In AAAI, 2020. 2\n[63] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-\nvocabulary panoptic segmentation with text-to-image diffusion models. CVPR, 2023. 8\n[64] Tien-Ju Yang, Maxwell D Collins, Yukun Zhu, Jyh-Jing Hwang, Ting Liu, Xiao Zhang, Vivienne Sze,\nGeorge Papandreou, and Liang-Chieh Chen.\nDeeperlab: Single-shot image parser.\narXiv preprint\narXiv:1902.05093, 2019. 3\n[65] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. k-means Mask Transformer. In ECCV, 2022. 3\n[66] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. ICLR, 2018. 19\n[67] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy. K-net: Towards unified image\nsegmentation. NeurIPS, 2021. 1, 3\n[68] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, 2017. 2, 6\n[69] Chong Zhou, Chen Change Loy, and Bo Dai. Denseclip: Extract free dense labels from clip. ECCV, 2022.\n14\n[70] Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin Wang, and Fan Wang. Lmseg: Language-guided\nmulti-dataset segmentation. arXiv preprint arXiv:2302.13495, 2023. 1, 2\n[71] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Simple multi-dataset detection. In CVPR, 2022. 2\n[72] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In ICLR, 2021. 18\nAppendix\nA\nBroader Impacts\nWe propose a single framework for multi-dataset multi-task segmentation, which especially benefits\nmemory-limited application scenarios, e.g., our work can make a single segmentation model that\nmeets all kinds of segmentation needs on memory-limited mobile devices (semantic segmentation for\nphoto editing, instance segmentation for object detection and image search, etc.), instead of using\nseparate models for various segmentation tasks.\nMoreover, our approach improves segmentation performance from a data-centric view: We leverage\nsegmentation data from multiple sources to improve the overall segmentation performance, especially\non datasets of smaller scales, e.g., ADE20k semantic. We also transfer knowledge from other sources\nto enable weakly-supervised segmentation. This is helpful on the scenario where segmentation data\nare very hard to collect, we can cotrain on similar data to help improve the performance, or even\ntrain on data with weaker supervision. e.g., if we want to train a segmentation model on rare wild\nanimals, we can co-train a model on a small set of these kinds of animals, and a larger dataset of\nmore common animals.\nAs for biases and fairness, our model learns from the training data, so it may inherit the biases and\nfairness issues existing in the training data. Nonetheless, our model trains on multiple sources, and\nthe datasets with less biases and fairness issues may compensate the biases and fairness issues in\nother datasets.\n13\nMethod\nBackbone\nmIoU\nmIoU-unseen\nmIoU-seen\nSPNet [60]\nDeepLabv2-ResNet101\n56.9\n0.0\n75.8\nSPNet-C [60]\n63.2\n15.6\n78.0\nZS3Net [3]\n61.6\n17.7\n77.3\nCaGNet [16]\n65.5\n26.6\n78.4\nSTRICT [46]\n70.9\n35.6\n82.7\nDenseCLIP+ [69]\n88.1\n86.1\n88.8\nFully-supervised\n88.2\n87.0\n88.6\nDaTaSeg\nResNet-50\n89.4\n87.7\n89.9\nViTDet-B\n92.8\n90.9\n93.4\nViTDet-L\n94.7\n94.3\n94.9\nTable 6: Comparison with zero-shot segmentation methods on PASCAL VOC 2012 segmentation dataset.\nDaTaSeg with a ResNet-50 backbone already outperforms all other methods. Note that zero-shot methods are\ntrained on 15 seen classes of PASCAL VOC and only 5 classes are unseen (mIoU-unseen). In contrast, DaTaSeg\nhas never seen PASCAL VOC images during training (it\u2019s trained on several other datasets). Following [69], the\nbackground class is ignored. The numbers of the comparison methods are from [69]. SPNet-C stands for SPNet\nwith calibration.\n0\n50\n100\n150\n200\n250\n300\n350\nsorted category index\n100\n101\n102\n103\nnumber of instances\n(a) Number of annotated instance\nmasks per category. It follows a rel-\natively long tail distribution.\n0\n200\n400\n600\n800\n1000\nsorted image index\n100\n101\n102\nnumber of instances\n(b) Number of annotated instance\nmasks per image. More than 400\nimages have more than 10 instances.\n0.0\n0.2\n0.4\n0.6\n0.8\nrelative segmentation mask size\n10\n4\n10\n3\n10\n2\n10\n1\n100\npercent of instances\nCOCO\nLVIS\nADE20K\nObjects365\n(c) Relative segmentation mask size\n(square root of the ratio of mask\narea to image area), compared with\nCOCO, LVIS, and ADE20K. Our\nstatistics are similar to LVIS.\nFigure 5: Dataset statistics for our Objects365 instance segmentation validation set.\nB\nComparison with zero-shot segmentation on PASCAL VOC\nWe evaluate DaTaSeg on the PASCAL VOC 2012 semantic segmentation dataset [13], which includes\n20 object classes and a background class. We directly transfer our trained model to its validation set\nwith 1,449 images. We follow the setting in DenseCLIP [69] and ignore the background class during\nevaluation.\nWe compare with other zero-shot segmentation methods in Table 6, which are trained on 15 seen\nclasses with 5 classes held out during training as unseen classes (potted plant, sheep, sofa, train,\nand TV monitor). DaTaSeg has never seen the entire PASCAL VOC dataset during training, though\nit is trained on other segmentation datasets (COCO panoptic, ADE semantic, and Objects365\ndetection). DaTaSeg with a ResNet-50 backbone outperforms all zero-shot methods with DeepLabv2-\nResNet101 backbones, on all categories. Surprisingly, DaTaSeg even outperforms the fully-supervised\ncounterpart. We note this is not an apple-to-apple comparison, since the training sources are different.\nThe main purpose of this comparison is to show how our \u201cdata-centric\u201d DaTaSeg is positioned against\nzero-shot segmentation methods on the PASCAL VOC dataset, using a direct transfer manner.\nC\nMore about our newly labeled Objects365 instance segmentation validation\ndataset\nWe randomly sampled 1,000 images from the Objects365 V2 validation set. Different from the\n\u201cfederated\u201d LVIS dataset [17] (not all categories in each image are completely labeled), the bounding\nboxes in Objects365 are completely labeled for each image and we follow this complete annotation\nfor the instance masks. For each bounding box annotation, we generate a foreground/background\nannotation question for the raters. In the question, raters are asked to paint the instance mask on\nthe original image, with the groundtruth bbox shown on the image as a guide and the groundtruth\ncategory displayed on the side. We do not crop the original image to the bbox region, but show the\nentire image to provide more context information, which is especially helpful for small objects. If the\nboundary is too blurry or too dark to annotate the instance mask, the raters can skip the question.\n14\nFigure 6: Visualization of our Objects365 instance segmentation dataset. We store binary masks, which has\nhigher accuracy compared with polygons. The quality on small objects are good (first two images). Complex\nstructures are also labeled delicately, e.g., the camels (1st image on 2nd row) and the decoration on the cake (1st\nimage on 3rd row). For occlusions, raters carefully avoid the occluded regions as they are not a part of the object\nto annotate, e.g., the fruits in the basket are not included in the basket masks (last image on 1st row).\nThe annotation tool is a free painting tool, which allows the raters to freely draw the instance mask.\nWe ask the raters to try to draw within the bbox, but if the object is obviously exceeding the bbox,\nthen they can draw outside the bbox. The size of the stroke is adjustable. Raters can zoom-in/out,\ndraw straight lines, and fill the inside of a plotted boundary (which is quite useful for instance\nsegmentation). Unlike polygons in COCO etc., our tool saves binary masks for higher-resolution\nmasks.\nWe inserted a total of 13,372 questions (we do not label the crowd instances, as they are skipped\nin both training and validation following the common practice in instance segmentation), and we\nobtained 12,836 valid instance mask annotations. It takes a total of 800.59 rater hours. On average,\nraters spend 3.74 minutes on each valid mask.\nIn Fig. 5, we show some statistics about our Objects365 instance segmentation dataset. The number\nof masks per category follows a relatively long-tailed distribution (Fig. 5(a)). The number of masks\nper image reveals that a large number of images have many instance annotations (Fig. 5(b)). And the\nstatistics of the relative mask size is close to that of the LVIS datasets (Fig. 5(c)).\nAs shown in Fig. 6, we visualized the mask annotations and they are very high-quality. The raters\ncarefully handled small objects, thin structures, complicated shapes, occluded objects, etc.\nFor the objects who do not have an instance mask since raters skipped it, it\u2019s possible that the model\nstill predicts an instance mask for that object. We do not want to treat such cases as false positive, so\nwe ignore them during evaluation.\nWe\u2019ll release this dataset. We believe this efforts can provide the community a good benchmark\nfor instance segmentation evaluation, e.g., for evaluating weakly-supervised instance segmentation\nlearned from bounding boxes.\n15\nattention\nmask\nV\nK\nimage\nfeatures\nQ\nLearnable Positional Embedding\nMasked Cross-Attention\nV\nK\nQ\nAdd & Norm\nSelf-Attention\nAdd & Norm\nFeed Forward Network\nAdd & Norm\nmask & class predictions\nFlatten image features\ncross-attention weights\nself-attention weights\nsegment queries\ndataset-specific queries\nFigure 7: Dataset-specific queries in the transformer decoder. We use different dataset-specific queries Qd\nfor each dataset d, which are treated the same as the segment queries Qs, and involved in both cross-attention\nand self-attention. All dataset-specific queries are discarded before predicting masks and classes.\nD\nMore details on \u201cadding dataset-specific modules hurts performance\u201d\nIn this section, we first introduce our designed dataset-specific modules and then show more ablation\nstudies on these modules.\nAs prior works have pointed out [31, 56], there are many inconsistencies across segmentation datasets,\nincluding labels having their own particular definition, being annotated using different protocols, etc.\nWe explore if adding dataset-specific modules can improve overall performance by designing the\nfollowing modules.\nDataset-specific queries: In our architecture design introduced in Sec. 3.4, an input image will not\nget any dataset-specific information until the text embedding classifier, which is almost at the final\nstage of the network. We thus propose to add a few dataset-specific queries Qd as an auxiliary input\nto the beginning of transformer decoder. Each training dataset d has its own Qd. The dataset-specific\nqueries are treated the same as regular segment queries Qs. The only difference is that we do not\nmake mask and class predictions for the dataset-specific queries. Fig. 7 shows the details. In the\ncross-attention layer, Qd cross attends to the image features \u2014 the cross-attention weight is:\nWcross_atten = softmax(A + Wq\n\u0002\nQd Qs\n\u0003\n\u00b7 IT\nmultiW T\nk ),\n(6)\nwhere Imulti are the multi-scale image features in Sec. 3.4. Wq, Wk denote the linear projection\nweights; A denotes the attention mask.\nIn the self-attention layers, there are interactions for all pairs of Qd and Qs, allowing the segment\nqueries to be aware of the dataset-specific information at the beginning of transformer decoder. This\ncan be shown by the self-attention weight calculation:\nWself_atten = softmax(Wq\n\u0014\nQd\nQs\n\u0015 \u0002\nQT\nd\nQT\ns\n\u0003\nW T\nk )\n= softmax(Wq\n\"\nQdQT\nd\nQdQT\ns\nQsQT\nd\nQsQT\ns\n#\nW T\nk ).\n(7)\nDataset-specific classification head and background classifier: We observe that dataset incon-\nsistency tends to primarily be a classification issue, rather than localization issue. Hence, for all\ndatasets we use the same set of parameters for localization (backbone, pixel decoder, transformer\ndecoder, mask embedding head) to enhance knowledge sharing. For classification, we design some\ndataset-specific components: specifically, we use a dataset-specific MLP class embedding head as\nwell as a learnable dataset-specific background classifier, since the definitions of background vary\nfrom dataset to dataset.\n16\ndataset2 (C2 classes)\ninput image\nBackbone\nPixel Decoder\ndataset-specific\nqueries\nTransformer Decoder\nfeature map \nClass Head\nMask Head\nsky\nhorse\nlearnable background\nperson\nsky\nhorse\ndataset1 (C1 classes)\nperson\nFrozen Pre-\ntrained Text\nEncoder\ndataset-specific\nshared\nN mask proposals\nN class predictions\nsky\nhorse\nperson\nN segment\nqueries\nFigure 8: Overview of incorporating all our designed dataset-specific modules into our universal multi-\ndataset multi-task segmentation model (DaTaSeg). We design a set of dataset-specific queries fed into the\ntransformer decoder, dataset-specific classification head, and dataset-specific background classifier. They are\nhighlighted in orange. All these modules are learnable. Different datasets have separate parameters for these\nmodules.\nfully-supervised\nweakly-supervised transfer\n# of dataset-specific queries\nADE\nCOCO\nADE\nO365\nsemantic\npanoptic\nsemantic \u2192 panoptic\nbox \u2192 instance\nmIoU\nPQ\nPQ\nmask AP\n0\n47.8\n48.1\n28.6\n13.1\n10\n48.0\n46.2\n28.0\n11.2\n20\n48.6\n46.6\n28.5\n11.7\n30\n47.1\n45.8\n27.4\n11.7\nTable 7: Ablation study on the number of dataset-specific queries Qd. Evaluated on a ResNet50 backbone.\nWithout dataset-specific queries, the performance on COCO panoptic and Objects365 instance segmentation is\nmuch better. Using a large number of dataset-specific queries, e.g., 30, hurts performance on many datasets,\nsince it weakens knowledge sharing among datasets & tasks.\nOverall architecture: The overall architecture equipped with all our dataset-specific modules\nis shown in Fig. 8. We design the dataset-specific modules to be light-weight, which allows us\nto save on memory costs. With this design of a mixture of a mostly shared network and light-\nweight dataset-specific modules, the model has the freedom to choose whether to leverage dataset-\nspecific information, or to use the shared knowledge across datasets/tasks. However, one significant\ndisadvantage of adding dataset-specific modules is it\u2019s hard to decide which set of dataset-specific\nparameters to use when directly transferring to other datasets (Sec. 4.5), and thus it hurts the open-\nvocabulary capability.\nAblation studies on dataset-specific modules: We ablate on the number of dataset-specific queries\nQd in Table 7. It shows that removing the dataset-specific queries achieves the best results on COCO\npanoptic and Objects365 by a large margin. It also achieves reasonable performance on other datasets.\nUsing 30 queries hurts the performance on COCO panoptic and ADE semantic.\nIn Table 8, we compare using and not using dataset-specific classification modules (the classification\nembedding head and background classifier). Results indicate that removing dataset-specific classifi-\ncation improves performance on all datasets & tasks, especially the weakly-supervised tasks. We\nsuspect it\u2019s because weakly-supervised tasks rely more on knowledge sharing.\nSystem-level comparison between with and without dataset-specific modules: In the main paper,\ndue to space limit, we only show results of adding all dataset-specific modules on a ResNet50\nbackbone. In Table 9, we show the same comparison on more backbones, i.e., ResNet50, ViTDet-B,\nand ViTDet-L. The observation is consistent across all backbones: adding dataset-specific modules\nhurts performance on all datasets. The difference is more significant on backbones of smaller scales,\nand on COCO panoptic and Objects365 instance segmentation datasets.\n17\nw/ dataset-specific classification\nw/o dataset-specific classification\nADE semantic\nmIoU\n47.9\n48.1 (+0.2)\nCOCO panoptic\nPQ\n48.5\n49.0 (+0.5)\nADE panoptic\u2020\nPQ\n28.7\n29.8 (+1.1)\nO365 instance\u2020\nAP\n12.9\n14.3 (+1.4)\nTable 8: Adding dataset-specific classification modules hurt performance on almost all datasets. The\ndataset-specific classification modules include the classification embedding head and the learnable background\nclassifier. Evaluated on a ResNet50 backbone, without other dataset-specific modules.\nFully-Supervised\nWeakly-Supervised Transfer\nBackbone\nModel\nADE\nCOCO\nADE\nO365\nsemantic\npanoptic\nsemantic \u2192 panoptic\nbox \u2192 instance\nmIoU\nPQ\nPQ\nmask AP\nResNet50\n+D-S modules\n48.1\n46.0\n26.9\n10.9\nDaTaSeg\n48.1 (+0.0)\n49.0 (+3.0)\n29.8 (+2.9)\n14.3 (+3.4)\nViTDet-B\n+D-S modules\n51.0\n50.8\n31.0\n13.1\nDaTaSeg\n51.4 (+0.4)\n52.8 (+2.0)\n32.9 (+1.9)\n16.1 (+3.0)\nViTDet-L\n+D-S modules\n53.9\n52.3\n33.5\n13.7\nDaTaSeg\n54.0 (+0.1)\n53.5 (+1.2)\n33.4 (-0.1)\n16.4 (+2.7)\nTable 9: Comparing DaTaSeg with the alternative architecture adding all dataset-specific modules on\nvarious backbones.\nResults show that removing the dataset-specific modules improve performance on\nall datasets and all backbones. The gains are most significant on Objects365 instance and COCO panoptic\nsegmentation (with dataset-specific modules, it cannot outperform the separately trained baseline on COCO\npanoptic).\nE\nArchitecture change from Mask2Former\nOur network architecture is based on Mask2Former [7]. However, due to hardware difference (TPU\nvs. GPU), we need to make modifications to Mask2Former architecture to run on our hardware.\nUnlike GPUs, TPUs require a static computation graph with fixed-shape data (otherwise, it recompiles\nfor each computation graph/change of data shape, and causes significant slowdown). Therefore,\nwe drop all the TPU-unfriendly operations in Mask2Former. In particular, we change the multi-\nscale deformable attention transformer [72] pixel decoder to a plain FPN [38]. The performance\ncomparison is shown in Table 4(e) in the Mask2Former paper (51.9 PQ of deformable-attention\nvs. 50.7 PQ of FPN on COCO panoptic). Also, we do not use the sampled point loss [8] in either\nthe matching loss or the training loss calculation, and use the vanilla mask loss. The performance\ncomparison is shown in Table 5 in the Mask2Former paper (51.9 PQ of point loss vs. 50.3 PQ of\nmask loss). Besides, we do not use varying input size during evaluation and use a fixed input size\n(first resize then pad).\nTo improve the training speed, we modify the deep supervision in the masked attention transformer\ndecoder: for every decoder layer, we use the same matching indices computed from the outputs of\nthe last decoder layer, but use a different prediction head. We do not use the mask predictions from\nthe initial queries as attention masks, since the predictions are not made based on the input image.\nF\nAdditional implementation details\nIn addition to the implementation details described in Sec. 4, we provide more datails here.\nPreprocessing. In order to do 1:1 Hungarian matching, we preprocess the groundtruth into the\nsame format as the predictions described in Sec. 3.2. In panoptic segmentation, we transfer the\ngroundtruth into a set of binary masks with class labels. For \u201cthing\u201d categories, we group the pixels\nbelonging to each instance into a separate groundtruth mask; for \u201cstuff\u201d categories, we group all\npixels in each stuff category as the groundtruth mask. Similarly, in semantic segmentation, we\ngroup all pixels in each category as the groundtruth mask. For instance segmentation, since we are\nusing bounding box weak supervision, we do not need this preprocessing step. For all mask proposal\ngroundtruth, we downsample it by 4 times, to save memory cost during training.\nTraining settings. We randomly scale the input image in the range of [0.1, 2.0] and then pad or crop\nit to 1024 \u00d7 1024. For ADE20k dataset, since the image size is smaller than other datasets, we use a\n18\n\u00b5ce\n\u00b5focal\n\u00b5dice\n\u00b5proj\n\u03bbce\n\u03bbfocal\n\u03bbdice\n\u03bbproj\nADE semantic\n1\n20\n5\n0\n1\n20\n5\n0\nCOCO panoptic\n1\n0\n1\n0\n1\n20\n5\n0\nO365 instance (box GT)\n1\n0\n0\n0.5\n1\n0\n0\n2\nTable 10: The weights we use to compute the matching cost and total loss (Eq. 4,5 in the main paper) for\nall training datasets. \u00b5\u2019s are for the matching cost and \u03bb\u2019s are for the total training loss.\nscaling range of [0.5, 2.0]. We use the AdamW optimizer [41] with a weight decay of 0.05. We clip\nthe gradients with a max norm of 0.1. The weight for the background class is set to 0.05 in Lce. The\nmatching cost and loss weight settings for Eq. 4,5 in the main paper are shown in Table 10. We use a\ndataset sampling ratio of 1:4:4 for ADE semantic, COCO panoptic, and Objects detection. We adopt\na different learning rate multiplier for each dataset: We multiply the learning rate on ADE semantic,\nCOCO panoptic, and Objects365 detection by 3, 5, 2, respectively. On ResNet50 backbones, we use\na batch size of 384 and train 500k iterations, with a learning rate of 3e-5. We adopt the step learning\nrate schedule: We multiply the learning rate by 0.1\u00d7 at the 0.9 and 0.95 fractions of the total training\niterations. On the ViTDet-B backbones, we train 600k iterations with a learning rate of 6e-5. On\nViTDet-L, we use a batch size of 256 and train 540.5k iterations with a learning rate of 4e-5. For\nResNet50, we train on 64 TPU v4 chips; for ViTDet backbones, we train on 128 TPU v4 chips. All\nevaluations are conducted on 4 GPUs with a batch size of 8.\nPostprocessing. We first apply the MERGE operation described in Sec. 3.2. We follow the postpro-\ncessing operations in [7] with some modifications: In panoptic segmentation, we use a confidence\nthreshold of 0.85 to filter mask proposals for COCO panoptic, and set the threshold to 0.8 for\nADE panoptic. For panoptic and instance segmentation, we filter out final segments whose area is\nsmaller than 4. For instance segmentation, we return a maximum of 100 instances per image with a\nscore threshold of 0.0. For all tasks, the final scores are the production of classification scores and\nlocalization scores (binary mask classification).\nG\nLimitations\nOur proposed method is not without limitations. Since we are the first work to explore multi-dataset\nmulti-task segmentation model, we do not introduce the complexity for an efficient framework. There\nare several ways to improve the model efficiency, e.g., calculating the mask loss on sampled points\nonly [7], while we do not deploy this in our framework, since it\u2019s not our key contribution. Moreover,\nas shown in the experiment section (Table 1), our framework is orthogonal to the detailed network\narchitecture as long as the network is able to output our universal segmentation representation.\nFor weakly-supervised panoptic segmentation on ADE20k panoptic, we notice that sometimes\nmultiple instances of the same category are not separated in the prediction. We believe using certain\ndata augmentation techniques, e.g., MixUp [66] and Copy-Paste [14], may farther enhance knowledge\ntransfer between datasets of different tasks, and may thus help mitigate this issue. We leave it for\nfuture work to improve on these limitations.\nH\nAdditional qualitative results\nWe notice sometimes our predictions do not match the groundtruth, but it does not necessarily\nmean the predictions are not good. Fig. 9 shows two examples on COCO panoptic. On the left\ncolumn, DaTaSeg does a better job in segmenting \u2018sky\u2019 and \u2018tree-merged\u2019 than the groundtruth.\nThe classification of \u2018pavement-merged\u2019 is also better than \u2018dirt-merged, which is probably due to\nlanguage ambiguity: the British meaning of \u2018pavement\u2019 is sidewalk, while the annotator may be\nmore used to the North American usage of \u2018pavement\u2019. On the right column, DaTaSeg predicts\n\u2018window-other\u2019, while the groundtruth is \u2018tree-merged\u2019 for the scene through the window, and both\nmake sense due to label ambiguity. In addition, DaTaSeg is able to predict the \u2018mirror-stuff\u2019, \u2018shelf\u2019,\n\u2018light\u2019, \u2018bottle\u2019, and more \u2018book\u2019s, which are missing from the groundtruth.\nWe show qualitative results for the direct transfer experiments in Fig. 10,11,12,13, on PASCAL\nContext 59 (PC-59), PASCAL Context 459 (PC-459), COCO semantic, and Cityscapes panoptic\ndatasets, respectively. These results serve as a supplementary material for Table 3 in the main paper.\nThey show DaTaSeg directly transfer to other segmentation datasets unseen during training with\n19\n(a) DaTaSeg predictions\n(b) groundtruth\nFigure 9: Examples on COCO panoptic showing that sometimes DaTaSeg\u2019s predictions do not match\nthe groundtruth, but it does not necessarily mean they are wrong. On the left, DaTaSeg\u2019s prediction is\n\u2018pavement-merged\u2019, while the groundtruth is \u2018dirt-merged\u2019 (Probably because the British meaning of \u2018pavement\u2019\nis sidewalk, while the annotator is more used to the North American meaning of \u2018pavement\u2019). Ours also segments\n\u2018tree\u2019 and \u2018sky\u2019 better. On the right, the definition for the scene through the window is ambiguous: \u2018window-other\u2019\n(ours) v.s. \u2018tree-merged\u2019 (GT). The groundtruth misses to label several objects, while DaTaSeg recognizes more\nobjects, e.g., \u2018mirror-stuff\u2019, \u2018light\u2019, \u2018shelf\u2019, \u2018bottle\u2019. Legends for \u2018book\u2019 are truncated due to space limit.\nhigh quality both in localization and classification. DaTaSeg performs well on large-vocabulary\nsegmentation (PC-459), and handles hard cases well (thin structures, small objects, complicated\nscenes, occlusions, etc.).\n20\nFigure 10: Qualitative results of DaTaSeg directly transferring to PASCAL Context semantic dataset\nwith 59 categories (PC-59). The results demonstrate DaTaSeg has good generalization ability. The top row\nshows DaTaSeg is able to segment small objects (aeroplane), and the last row indicates DaTaSeg segments fine\nstructures (bicycles) well. For each row, the left is the input image, the middle is our prediction, and the right is\ngroundtruth. With a ViTDet-L backbone.\n21\nFigure 11: Qualitative results of DaTaSeg directly transferring to PASCAL Context semantic dataset\nwith 459 categories (PC-459). The results demonstrate DaTaSeg has good generalization ability, and enables\nopen-vocabulary segmentation. For each row, the left is the input image, the middle is our prediction, and the\nright is groundtruth. With a ViTDet-L backbone.\n22\nFigure 12: Qualitative results on COCO semantic dataset. DaTaSeg does high-quality semantic segmentation\non COCO. Note that DaTaSeg trains on COCO panoptic. For each row, the left is the input image, the middle is\nour prediction, and the right is groundtruth. Our model is with a ViTDet-L backbone.\n23\nFigure 13: Qualitative results of DaTaSeg directly transferring to Cityscapes panoptic dataset. Cityscapes\nfocuses on street view, which is different from all training datasets. The results demonstrate good generalization\nability. For each row, the left is our prediction and the right is groundtruth. Our model is with a ViTDet-L\nbackbone.\n24\n"
  },
  {
    "title": "Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators",
    "link": "https://arxiv.org/pdf/2306.01242.pdf",
    "upvote": "1",
    "text": "Responsible Task Automation: Empowering Large\nLanguage Models as Responsible Task Automators\nZhizheng Zhang\u2217\nXiaoyi Zhang\u2217\nWenxuan Xie\nYan Lu\nMicrosoft Research Asia\n{zhizzhang, xiaoyizhang, wenxie, yanlu}@microsoft.com\nAbstract\nThe recent success of Large Language Models (LLMs) signifies an impressive\nstride towards artificial general intelligence. They have shown a promising prospect\nin automatically completing tasks upon user instructions, functioning as brain-like\ncoordinators. The associated risks will be revealed as we delegate an increasing\nnumber of tasks to machines for automated completion. A big question emerges:\nhow can we make machines behave responsibly when helping humans automate\ntasks as personal copilots? In this paper, we explore this question in depth from\nthe perspectives of feasibility, completeness and security. In specific, we present\nResponsible Task Automation (ResponsibleTA) as a fundamental framework to\nfacilitate responsible collaboration between LLM-based coordinators and executors\nfor task automation with three empowered capabilities: 1) predicting the feasibility\nof the commands for executors; 2) verifying the completeness of executors; 3)\nenhancing the security (e.g., the protection of users\u2019 privacy). We further propose\nand compare two paradigms for implementing the first two capabilities. One is to\nleverage the generic knowledge of LLMs themselves via prompt engineering while\nthe other is to adopt domain-specific learnable models. Moreover, we introduce\na local memory mechanism for achieving the third capability. We evaluate our\nproposed ResponsibleTA on UI task automation and hope it could bring more\nattentions to ensuring LLMs more responsible in diverse scenarios.\n1\nIntroduction\nRecent advanced Large Language Models (LLMs) [3, 28, 27, 6, 7, 10, 32, 40] exhibit powerful\nlanguage understanding, reasoning, generation, generalization and alignment capabilities in a vast\nsuite of real-world scenarios. LLMs acquire generic knowledge on open-domain tasks by scaling\nup deep learning, which marks a significant milestone in the advancement towards artificial gen-\neral intelligence. Beyond language tasks, LLMs are empowered with multi-modal perception and\ngeneration capabilities through collaboration with domain-specific models [39, 30, 36]. They have\nbeen revolutionizing the field of task automation by connecting LLMs to various domain-specific\nmodels or APIs, in which LLMs serve as brain-like coordinators while domain-specific models or\nAPIs function as action executors [18, 33, 10].\nAdopting LLMs to constructing a general-purpose copilot for automating diverse tasks is still in an\nincipient exploration stage. Taking the interaction with UIs as an example, browsing and interacting\nwith various websites and APPs to achieve their diverse intentions (e.g., searching in websites, online\nshopping, setting changing, etc.) are almost indispensable for many people\u2019s daily life. Some of them\nrequire a hierarchical action architecture [2, 1] where a high-level instruction from human known as\ntask-level goal needs to be decomposed into a series of low-level step-wise instructions for actual\nexecution. Such complex multi-step tasks requires each step to be reasonably planned and reliably\n\u2217Equal contribution.\nPreprint. Under review.\narXiv:2306.01242v2  [cs.AI]  4 Dec 2023\nexecuted in line with human intentions. This actually poses significant challenges in the compatibility\nbetween LLM-based coordinators and their executors without intervention in their training. Their\nreliable collaboration requires LLM-based coordinators to be very familiar with the capabilities of\nexecutors, and to replan the task execution when necessary upon the command completeness of\nexecutors. Existing works [36, 39, 30, 18] just tell LLMs the powers of executors with model/API\ndescriptions via heavy prompt engineering. However, these descriptions are commonly manually\nwritten and summary-like, which are inadequate to describe the executors\u2019 capabilities and cannot\nreflect the execution completeness for LLMs. In addition, LLMs are also possible to have insufficient\nawareness of the task goals and environments, thus issuing unreasonable instructions to executors.\nAs an increasing number of tasks are delegated to machines for automated completion, the exposure\nto risks will grow, which necessitates addressing the reliability issue with a sense of urgency.\nIn this work, we propose ResponsibleTA as a fundamental multi-modal framework to empower LLMs\nas responsible task automators in three aspects: 1) Feasibility: ResponsibleTA predicts the feasibility\nof low-level commands generated by the LLM-based coordinator and return the results to LLMs for\nreplanning before action execution. This capability aims to minimize the risks and time consumption\nof having the executors perform unachievable instructions, making task automation more controllable\nand efficient. 2) Completeness: ResponsibleTA checks the execution results of low-level commands\nstep-by-step and provides feedbacks timely for the LLM-based coordinator to allow it reschedule next\nsteps more reasonably. This capability can improve the success rates of automated task completion. 3)\nSecurity: ResponsibleTA augments LLMs with edge-deployed memories, which allows user-sensitive\ninformation are hidden during the interaction with cloud-deployed LLMs and are only stored and\nused locally on users\u2019 devices. This capability reduces or avoids the transmission of users\u2019 sensitive\ninformation between the cloud-deployed LLMs and edge-deployed executors, thus lowering the\nleakage risk of users\u2019 privacy. Empowering LLMs with these capabilities, ResponsibleTA achieves\nautomatic verification before and after each execution step, which improves not only the success rate\nbut also the completion efficiency with thoughtful security guarantees for users.\nIn addition to the framework design of ResponsibleTA, we investigate the implementations of three\ncore functionalities of ResponsibleTA as an empirical study in a practical application scenario (i.e.,\nUI task automation). Its goal is to automatically ground target UI elements and interact with them\nvia automatic clicking and typing operations based on task instructions from users, which is of high\ndemands in both academia and industry. For feasibility and completeness, we propose and compare\ntwo technical paradigms for their implementations. One paradigm is to leverage generic knowledge\nof LLMs themselves via prompt engineering. The other paradigm is to train domain-specific models\nspecifically responsible for these two functionalities as external assistants of the coordinators and\nexecutors. We observe that leveraging LLMs themselves is inferior to adopting domain-specific\nmodels, demonstrating that domain-specific knowledge is crucial for enhancing the reliability of\nLLMs in task automation. For security, we introduce a local memory and propose a mechanism\ndesign to use it for endowing ResponsibleTA with security guarantees.\nThe contributions of this work can be summarized as follows:\n\u2022 We present a fundamental multi-modal framework, dubbed as ResponsibleTA, which empow-\ners LLMs with the capabilities of feasibility prediction, completeness verification and security\nprotection for automatically completing tasks in a responsible manner.\n\u2022 We propose and compare two technical paradigms for implementing the feasibility prediction and\ncompleteness verification functionalities of ResponsibleTA in UI task automation scenarios. One\nis to leverage internal knowledge of LLMs themselves while the other is to train domain-specific\nmodels as external assistants.\n\u2022 We introduce a local memory mechanism to endow ResponsibleTA with the capability of security\nprotection for user privacy, and showcase its effectiveness in some representative practical cases.\n2\nRelated Works\n2.1\nDevelopment of Large Language Models\nStarting from the language model Bert [9], the pretraining-finetuning scheme is a common practice in\nmany natural language processing (NLP) and computer vision (CV) problems. By bringing data and\nmodels to a larger scale, GPT-3 [3] demonstrates that LLMs obtain the ability of in-context learning,\nwhere LLMs can quickly adapt to a new task given only a few examples as prompts. Furthermore,\n2\nHigh-level \nInstructions\ninfeasible\nReplanning\nfeasible\nfeasible\nLLM-based\nCoordinator\nDomain-specific\nExecutor\nPlanning\ncompleted or not\nVision Inputs\nLanguage Inputs\nMemory\nCompleteness Verifier\nLow-level \nCommands\nFeasibility Predictor\nExecution Results\nPlaceholder \u2192 User Info\nSecurity Protector\nUser Info       \u2192 Placeholder\nSecurity Protector\nFigure 1: The framework of the proposed ResponsibleTA. It augments the cloud-deployed LLM-\nbased coordinator with feasibility protector, completeness verifier and a local memory, achieving its\nresponsible collaboration with the domain-specific executor. They are all detailed in the main text.\nInstructGPT [29] finetunes GPT-3 with a dataset of human demonstrations to make LLMs follow\nusers\u2019 intents. Recently, ChatGPT [28] and GPT-4 [27] demontrate excellent ability in generating\ncomplete answers to users\u2019 natural language questions. Considering that these LLMs only deal with\nlanguage tokens, Kosmos-1 [12] extends its training set to image-text data and shows capability in\nvisual question answering and multimodal dialog. In the meanwhile, there are concurrent LLMs\nsuch as PaLM [6, 7] and PaLM-E [10], and open-source efforts such as LLaMA [32] and OPT [40].\nDespite the strong performance, answers generated by LLMs are not always reliable. Our work aims\nto empower LLMs to be reliable in the area of task automation.\n2.2\nLarge Language Models for Task Automation\nLLMs are able to serve as actors or coordinators/planners for digital or physical AI task automation.\nWhen functioning as actors, the outputs of LLMs are executable actions [25, 1], which is limited\nto natural language processing tasks. Towards broader applications in physical [13, 1, 20, 10,\n18], simulated physical [34, 35] and digital [30, 36, 39, 18] environments, LLMs usually act as a\nbrain-like coordinator to process human high-level instructions into step-wise executable machine\ncommands and hand them over to domain-specific models/APIs for actual execution. Along this\nroute, LLMs have been opening up infinite possibilities for task automation and putting forward\nhigher requirements for the reliability of automated systems meanwhile. The knowledge defects or\nbiases of LLMs and the gap between LLMs and executors both possibly lead to potential risks in the\nfailure of task completion or even causing harm to users [11]. Previous work [1] attempts to address\nthe executability issue by training an external model to critic the outputs of LLM. It delivered success\nin simple robotic environments [1] but is demonstrated hard to be applied to complex environments\nwith more objects and diverse actions [31, 37]. Besides, [35, 37] track the execution results for the\nfollow-up planning of LLMs. To the best of our knowledge, we are the first to systematically study\nthe reliability of LLM-based task automation from feasibility, completeness and security perspectives.\n3\nMethod\n3.1\nResponsibleTA Framework\nResponsibleTA is a fundamental multi-modal framework for empowering LLMs as responsible\ntask automators. The goal is to comprehensively enhance the reliability of employing LLMs as\ncoordinators, while adopting domain-specific models/APIs as executors, for automatically completing\ntasks in according with human instructions. It achieves reliability enhancement with three empowered\ncapabilities: feasibility prediction, completeness verification and security protection.\nAs shown in Figure 1, ResponsibleTA takes high-level instructions (i.e., task goals) from human as\ninputs and parses them into low-level (i.e., step-wise) commands with a pretrained LLM for action\n3\nYes, I understand your expectation and I am ready to start.\n[System Prompt]:\nYou are a feasibility predictor for task automation in Windows. Given a command to be executed and the \ndescriptions for all elements on current screen, you return Yes or No to predict the feasibility of this command.\nFor example, you are given:\nCommand: \u201cClick the details icon of financial news.\u201d\nUI elements on the current screen:\n[{ID: 0, Content: \u201cBBC homepage\u201d, Location: [100, 100] # [horizontal, vertical] center coordinates of this element},\n {ID: 1, Content: \u201cEngland defender Bronze has keyhole knee surgery.\u201d, Location: [100, 200]},\n {ID: 2, Content: \u201cSpurs players reimburse fans after thrashing.\u201d, Location: [100, 300]}, \u2026\u2026]\nYou should return:\n{Thought: \u201cThere is no element in the given list that is related to the \u2018details of financial news\u2019. So, the action query is not \nfeasible, the answer should be no.\u201d, Feasible: \u201cNo\u201d.}\nDo you understand my expectation? And are you ready to start?\n[User Query]:\nCommand: \u201c*** *** ***.\u201d\nUI elements on the current screen: [{ID: *, Content: \u201c*** ***\u201d, Location: [*, *] }, \u2026\u2026]\n{Thought: \u201c\u2217\u2217\u2217 \u2217\u2217\u2217 \u2217\u2217\u2217.\u201d,  Feasible: \u201cYes (or No)\u201d.}\nFigure 2: Illustration of our prompt engineering based paradigm for implementing the feasibility\npredictor in our proposed ResponsibleTA.\nplanning. To ensure the low-level command is reasonably generated and correctly executed at each\nstep, we introduce a feasibility predictor and a completeness verifier in ResponsibleTA. Before each\nexecution, the feasibility predictor assesses whether the generated low-level command is executable in\npractice upon the current screenshot and this command itself. It will ask the LLM-based coordinator\nfor a replanning via prompt engineering when the command is judged as infeasible, otherwise feed the\ncommand to the executor when feasible. When reaching the pre-set maximum number of replanning\nattempts, the LLM will terminate the current task and offer feedbacks for users. After each execution,\nthe completeness verifier checks whether the execution result aligns with the goal of given low-level\ncommand. Replanning will be launched by LLM once an invalid or unfulfilled execution is detected.\nNote that we input the UI element information on the screen to the LLM in linguistic form through a\ntrained screen parsing model only when replanning is needed, so as to protect personal information\non users\u2019 screens as much as possible. (See supplementary for more details.) Moreover, we integrate\na security protector into ResponsibleTA to allow user-sensitive information to be stored locally. With\nthis module, users are allowed to replace all sensitive information by their predefined placeholders,\nand translates them back when sending the commands to edge-deployed executors for local execution.\nAs a result, ResponsibleTA enhances the internal alignment between the coordinator and executors,\nand provides security guarantees for users as a responsible task automation framework.\nWe employ UI task automation as a practical application scenario for in-depth investigation and\nexperiments, so as to 1) showcase the effectiveness of ResponsibleTA intuitively and 2) state the im-\nplementations of three core modules in ResponsibleTA clearly. Its goal is to automate the interaction\ntasks between human and UIs (e.g., searching in websites, playing media, online shopping, changing\nsettings, etc.). We train a domain-specific executor to automatically ground the target UI element\nby predicting its spatial coordinates for clicking or typing, with a given low-level command and the\ncorresponding screenshot as the inputs. Since this is not the emphasis in this work, we place the\ndetailed model design and training of the executor in the experiment section and supplementary.\n3.2\nFeasibility Predictor\nThe feasibility predictor takes the low-level command and the current screenshot as inputs to predict\nthe feasibility of this command. It contributes to avoiding the execution of a infeasible or dangerous\ncommand by asking for replanning once infeasible. Here, we introduce two technical paradigms for\nits implementation, and compare their effectiveness in the following experiment section.\n4\nPrompt engineering based paradigm.\nThis paradigm is designed to leverage the internal generic\nknowledge of a LLM for achieving feasibility prediction. We first design a system prompt as\nthe launching prompt to tell LLM our task goal and expected outputs together with some specific\ndemonstrations. Considering that most of advanced LLMs (e.g., ChatGPT, GPT-4) have not integrated\nor released their input interfaces for other modalities (e.g., vision) except for language, we train a\nscreen parsing model to translate the screenshots into a series of UI elements where each element\nis represented in linguistic description with its index, text content, location and type. More details\nof this model can be found in Sec. A.2 of the supplementary material. We organize the linguistic\ndescriptions of all UI elements at the current screenshot as a structured form (i.e., dictionary), and\ninput it together with the low-level command at the current step as the user queries for each task\ninstance. The system prompt and the demonstrations of user queries of our proposed feasibility\npredictor are illustrated in Figure 2 in detail.\nDomain-specific model based paradigm.\nThis paradigm aims to address the feasibility prediction\nby training a domain-specific model, i.e., an external expert. Here, we devise an end-to-end multi-\nmodal model with its architecture inspired by the Pix2Seq modelling idea [4]. As shown in Figure\n3, our Domain-Specific Feasibility Predictor, referred to as DSFP, consists of a transformer-based\nvision encoder and a transformer-based language decoder. Given an image X \u2208 RH\u00d7W \u00d7C, the\nvision encoder captures the features of X and embeds it into nv d-dimensional tokens denoted by\n{vi|vi \u2208 Rd, 1 \u2264 i \u2264 nv}. The low-level command is tokenized to be nt d-dimensional text tokens\ndenoted by {ti|ti \u2208 Rd, 1 \u2264 i \u2264 nt}. Then the sets {v} and {t} are fed into the language decoder\nto generate the prediction results in linguistic form.\nVision \nEncoder\nLanguage \nDecoder\nLow-level Commands\n\u201cLogin in this web with the \naccount {account_name}  and \nthe password {password}.\u201d\n<s_feasibility>\n \u201c0\u201d \n</s_feasibility>\nFeasible: 0\nFigure 3: Illustration of our domain-specific\nmodel based paradigm for implementing the\nfeasibility predictor in ResponsibleTA.\nTo make the above architecture design unified\nto other tasks, such as completeness verification\nintroduced subsequently, we model the outputs\nof its decoder in a structured linguistic form as\n\u201c<task_prompt> {results} </task_prompt>\u201d fol-\nlowing [14, 38].\nHere, \u201c<task_prompt>\u201d and\n\u201c</task_prompt>\u201d denotes the start and end in the\nlinguistic sequence for the results, respectively.\nAnd \u201c{results}\u201d denotes the actual contents of the\nresults. For the feasibility predictor of Responsi-\nbleTA, \u201c<task_prompt>\u201d and \u201c</task_prompt>\u201d\nare instantiated to be \u201c<s_feasibility>\u201d and\n\u201c</s_feasibility>\u201d, respectively. The \u201c{results}\u201d\ncould be 0 or 1 where 0 represents \u201cinfeasible\u201d\nwhile 1 denotes \u201cfeasible\u201d.\n3.3\nCompleteness Verifier\nThe completeness verifier in ResponsibleTA takes the executed command and the screenshot after\nexecution as its inputs to assess the completeness of this command. It in fact plays the role of handling\nthe internal alignment between the LLM-based coordinator and the executor to ensure the reliability\nof their collaboration. For each step, the verifier provides a feedback for the LLM-based coordinator\nto tell whether the execution result of the executor aligns with its expectation at this step so that it can\nremedy erroneous or biased execution timely via a replanning operation. It forms a loop to endow\nResponsibleTA with the capability of self-correction. We showcase its effectiveness in the case study\nin the experiment section.\nWe also propose and compare two technical paradigms for implementing this module, which are\nsimilar with those of the feasibility predictor introduced as above. For conciseness, we only state the\ndifferences with the feasibility predictor here and place more details in our supplementary. For the\nprompt engineering based paradigm, we share the screen parsing model with that in the feasibility\npredictor, and update the system prompt and user queries in line with the goal of completeness\nverification. Their contents are detailed in the supplementary. For the domain-specific model\nbased paradigm, we adopt the same model architecture and the output format with those of the\nfeasibility predictor, but feed the executed command and the screenshot after execution as its inputs.\nHere, \u201c<task_prompt>\u201d and \u201c</task_prompt>\u201d are instantiated to be \u201c<s_completeness>\u201d and\n5\n\u201c</s_completeness>\u201d, respectively. The \u201c{results}\u201d could be 0 or 1 where 0 represents \u201cincomplete\u201d\nwhile 1 denotes \u201ccomplete\u201d.\n3.4\nSecurity Protector\nMany tasks that users need to be automated involve their private information. For example, online\nshopping or computer configuration may require the credit card, account and password information of\nusers. The security protector in ResponsibleTA is designed to enable the sensitive information from\nusers to be stored and used locally, obviating the transmission of them to the cloud-deployed LLM\nand reducing information leaky risks. To this end, the security protector uses a NER model (e.g.,\nBert-NER [19]) to automatically detect the sensitive information in users\u2019 instructions, and replace\nthem with a information placeholder denoted by \u201c{info_name}\u201d. Then, it stores \u201c{info_name}\u201d and\nits corresponding real contents using a dictionary structure into an edge-deployed memory. When\nreceived a command from LLM, the security protector translates the information placeholder back\nto its original contents, then sends the translated command to executors for local execution. For\nexample, when users ask ResponsibleTA to change the shipping address in a shopping website, their\nold and new address are represented as \u201c{old_address}\u201d and \u201c{new_address}\u201d, respectively, when\ninteracting with the cloud-deployed LLMs, and are translated back for local executors.\n4\nExperiments\n4.1\nDatasets and Implementation Details\nFeasibility prediction dataset.\nWe collect data for feasibility prediction from Common Crawl2,\nan open repository of web crawl data. We sample about 40K web pages from this open repository,\nyielding 1.1M image-text pairs. Among them, 1M pairs are used for training while the remaining\n100K pairs are for testing. For each web page, we extract leaf elements and assign captions to them\nfrom HTML entries such as inner-text, value, alt, aria-label, label-for, and placeholder. Subsequently,\nwe employ a randomized algorithm to generate low-level commands for elements by leveraging their\ncaptions. Example commands include \u201cselect the {element_caption} item\u201d, \u201cclick the item to the right\nof {element_caption}\u201d, \u201center {words} into {element_caption}\u201d, \u201cscroll until {element_caption}\u201d, etc.\nAll of these commands are deemed as feasible. Infeasible commands are generated based on fake\nelement captions that do not appear in the current web page.\nCompleteness verification dataset.\nThis dataset is also based on publicly available web pages.\nThis dataset leverages the transitions between two web pages, i.e., it jumps to page B by clicking an\nelement in page A. Based on the transitions we are able to define a positive example of completeness\nas a 3-tuple [screenshot A, {element_caption}, screenshot B]. We generate negative examples by\nsubstituting the 3-tuple with fake items. Overall, this dataset includes 113K web pages with 1.2M\nimage-text pairs for training and 6K web pages with 60K image-text pairs for testing.\nThe data for task automation executor is based on the feasibility prediction dataset. We place more\ndetails about the dataset configuration and model implementation in our supplementary.\n4.2\nQuantitative Results\nWe evaluate afore-introduced two technical paradigms for implementing the feasibility predictor and\ncompleteness verifier in ResponsibleTA. The experiment results are in Table 1. Due to the heavy\ncost and efficiency of the testing with LLMs, we randomly sample 5K image-text pairs from the test\nsplits on feasibility prediction and completeness verification for evaluating the prompt engineering\nbased paradigms. As for domain-specific models based paradigms, we evaluate them on the same 5K\nimage-text pairs (abbreviated as DSM in Table 1) as well as on the entire test splits (abbreviated as\nDSM+ in Table 1). As shown in Table 1, we can find that DSM-based and DSM-based+ perform\nvery closely under these two test settings. This indicates our sampled 5K image-text pairs are diverse\nand representative enough for providing convincing evaluation results.\n2https://commoncrawl.org/the-data/\n6\nTable 1: The results of quantitative evaluation for the proposed feasibility prediction and completeness\nverification modules. \u201cAP\u201d, \u201cAcc\u201d and \u201cF1\u201d are short for the average precision, accuracy and F1\nscore metrics, respectively. The superscript \u201c+\u201d denotes the result evaluated on a larger test dataset.\nModels\nFeasibility Prediction\nCompleteness Verification\nAcc (%)\nAP\nF1\nAcc (%)\nAP\nF1\nLLM-based (ChatGPT)\n65.7\n0.631\n0.546\n61.1\n0.564\n0.704\nLLM-based (GPT-4)\n68.9\n0.669\n0.583\n62.9\n0.575\n0.721\nDSM-based\n74.8\n0.818\n0.671\n83.8\n0.803\n0.833\nDSM-based+\n75.3\n0.823\n0.678\n83.5\n0.804\n0.829\nAccuracy of feasibility prediction.\nAs shown in Table 1, GPT-4 is more powerful than ChatGPT\nfor feasibility prediction. And the DSM-based feasibility predictor outperforms the LLM-based\n(GPT-4) one by 5.9%, 0.149, 0.088 in accuracy, average precision and F1 score, respectively, on\nfeasibility prediction. These results demonstrate the superior performance of the domain-specific\nmodels based paradigm relative to prompt engineering based paradigm on feasibility prediction.\nAccuracy of completeness verification.\nFrom the in Table 1, we observe that DSM-based feasibility\npredictor outperforms the LLM-based (GPT-4) one by 20.9%, 0.228, 0.112 in accuracy, average\nprecision and F1 score, respectively, on completeness verification, showing a similar tendency with\nthat on feasibility prediction. It also indicates the performance superority of adopting a domain-\nspecific model as the completeness verifier in ResponsibleTA.\nAs above, we can find the domain-specific model based paradigms perform consistently better than\nprompt engineering based paradigms on implementing these two functionalities of ResponsibleTA.\nDespite this, the advantage of prompt engineering based paradigms is that they do not require the\ncollection of specific data for training, offering better flexibility in practical deployment.\n4.3\nCase Study and Demonstration\nWe analyze the performance and behaviors of our proposed ResponsibleTA via real-world case\nstudy on 12 tasks, considering that there are no fledged benchmarks in this research field yet.\nBesides, we conduct ablation study on these tasks by comparing Baseline, Baseline+Fea. and\nBaseline+Fea.+Com. where Baseline+Fea.+Com. is the complete version of ResponsibleTA. Their\nconfigurations are detailed in the caption of Table 2. The security predictor is installed in all models.\nAblation study and analysis.\nWe report the specific completion progress and the final success\nstatus over all 12 real-world tasks in Table 2. As for the success ratio, we can find that Baseline\ndoes not reach the final success on 9 out of 12 tasks. For these 9 tasks, 5 of them are successfully\nremedied by the feasibility predictor on its own. On top of it, the completeness verifier help turn 2 of\nthem to the final success in addition. It can be seen that the feasibility predictor and the completeness\nverifier in ResponsibleTA can significantly improve the success ratio of task automation by providing\nfeedbacks for LLM-based coordinator so that it can replan timely. How our ResponsibleTA achieves\nthis will be elaborated in detail in the subsequent text.\nBesides the benefits of improving the success rate, our proposed ResponsibleTA can effectively\nreduce the number of invalid instruction executions thanks to its feasibility predictor. This conclusion\nis drawn by comparing the valid steps and total execution steps for Baseline and Baseline+Fea.\nmodels. Thus, our ResponsibleTA can avoid the risks when executing those invalid steps.\nMoreover, we further verify the role of ResponsibleTA on security protection by comparing using\nplaceholders and real user information on the No.7 and No.10 tasks. We experimentally find that\nlaunching the security protector neither affects the task success rate nor increases the number of\ninvalid executions on these two cases for all three models. It can effectively obviate the need of\nuploading user-sensitive information to the cloud.\nHow ResponsibleTA remedies failure cases?\nHere, we take a close look into a specific case for\ndetailed analysis on how ResponsibleTA remedies a failure case to achieve success in the end. We\nillustrate specific execution processes of the first 4 steps for the No.9 task in Figure 4. The execution\n7\nTable 2: The case study results of task automation in the real world. The \u201cBaseline\u201d denotes the\nmodel consisting of a LLM-based coordinator and a task automation executor, without our proposed\nfeasibility prediction and completeness verification modules. The \u201c+Fea.\u201d and \u201c+Com.\u201d refers\nto adding the feasibility prediction module and adding the completeness module, respectively. We\nrepresent each execution result with its \u201cProgress\u201d and \u201cEnd Status\u201d. Here, the \u201cProgress\u201d is shown\nin the form of the number of \u201cvalid steps / total execution steps (human expert steps)\u201d, in which\n\u201chuman expert steps\u201d refers to the step number of completing given instruction by a human expert.\nFor the \u201cEnd Status\u201d, \u2713 means the task goal has been achieved in the end while \u2717 indicates it has not.\nNo.\nHigh-level Instruction\nBaseline\nBaseline+Fea.\nBaseline+Fea.+Com.\nProgress\nEnd Status\nProgress\nEnd Status\nProgress\nEnd Status\n1\nOpen football news in bbc.com.\n4/4 (4)\n\u2713\n4/4 (4)\n\u2713\n4/4 (4)\n\u2713\n2\nFind the view setting page in Outlook.\n4/4 (4)\n\u2713\n4/4 (4)\n\u2713\n4/4 (4)\n\u2713\n3\nNavigate to the language setting in my\nWindows11.\n3/3 (3)\n\u2713\n3/3 (3)\n\u2713\n3/3 (3)\n\u2713\n4\nFind the system setting for text size in\nmy Windows11.\n1/3 (3)\n\u2717\n3/3 (3)\n\u2713\n3/3 (3)\n\u2713\n5\nHelp me open the latest received e-mail\nin my Outlook.\n2/3 (3)\n\u2717\n3/3 (3)\n\u2713\n3/3 (3)\n\u2713\n6\nGo to github.com and check issues that\nmentioned me, already logged in.\n2/5 (4)\n\u2717\n4/4 (4)\n\u2713\n4/5 (4)\n\u2713\n7\nLog in Instacart with username {user-\nname} and password {password}.\n5/6 (6)\n\u2717\n6/6 (6)\n\u2713\n6/6 (6)\n\u2713\n8\nGo to Amazon and add a pair of gloves\ninto the shopping cart.\n4/6 (6)\n\u2717\n6/6 (6)\n\u2713\n6/6 (6)\n\u2713\n9\nGo to Amazon and add the cheapest\ncharger into the shopping cart.\n4/7 (9)\n\u2717\n5/5 (9)\n\u2717\n9/9 (9)\n\u2713\n10\nAdd my Costco\u2019s loyalty card number\n{card_num} in the website {web_url}.\n2/6 (6)\n\u2717\n3/3 (6)\n\u2717\n6/7 (6)\n\u2713\n11\nCreate a meeting at 2023/04/15 14:00-\n14:30 in Outlook.\n3/6 (8)\n\u2717\n3/3 (8)\n\u2717\n3/3 (8)\n\u2717\n12\nSearch the Cpython repo and download\nits zip file in github.com.\n5/7 (8)\n\u2717\n6/6 (8)\n\u2717\n6/6 (8)\n\u2717\nfor remaining steps and the corresponding prompts are omitted here for brevity. As shown in Figure\n4, the LLM-based coordinator generates feasible low-level commands for the first 4 steps, and the\ncorresponding execution is smooth for these steps. For the 5-th step, the coordinator originally thinks\nwe should click the button with the content of \u201csort by price\u201d. It is seemingly a reasonable command\nbut does not match the current web page in fact, because there is no matched button. At this point,\nthe Baseline model executes this infeasible command in a seemingly random manner, leading to\n4 valid steps in total. When ResponsibleTA has the feasibility predictor, this module will return\n\u201cinfeasible\u201d signal to the coordinator so that it can replan upon additionally provided screen page\ninformation. After the replanning, the coordinator gives a feasible command, i.e., click the \u201cSort\nby: Featured\u201d button, and yields one more valid step. Despite this, Baseline+Fea. still fails for the\nnext step since it does not realize that another click operation is further needed to achieve the goal of\nsorting by price. This can be effectively remedied by the complete version of ResponsibleTA thanks\nto its completeness verifier. With the assist of this module, ResponsibleTA add such click operation\nfor the \u201cPrice: Low to High\u201d button before executing subsequent steps via another replanning. In this\nway, it adjusts the planned actions timely and contributes to the eventual success.\n5\nLimitation\nWe have to admit that it is difficult, even impossible, to cover all aspects of a newly devised system in\na single academic paper. This paper makes contributions and delivers insights from the perspectives\nof framework design and technical paradigm comparison. Its remaining limitations lie in two aspects:\n1) benchmark construction; 2) specific model design/iteration aimed at pursuing high performance\nfor this new domain. We here call on the community to address them with our work as a start point.\n8\n[ResponsibleTA \u2013 Coordinator]: (# replanning)\n[Step-5]  {Thought: \u201cThe task is about sorting the results by price. The No.128 element has      \nthe content 'Sort by:Featured', which seems to be the sorting button. I should click it.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201csort by: featured\u201d}.\n[Human]: Go to Amazon and add the cheapest charger into the shopping cart.\n[ResponsibleTA \u2013 System]: Received! Calling GPT-4 for task planning \u2026 \n[ResponsibleTA \u2013 Coordinator]:\n[Step-1]  {Thought: \u201cWe should open Edge browser to reach a website.\u201d,\nAction: \u201cOpen App\u201d, App Name: \u201cEdge\u201d }.\n[Step-2]  {Thought: \u201cWe should write the url to reach a website.\u201d,\nAction: \u201cInput\u201d, Input Content: \u201camazon.com\\n\u201d}.\n[Step-3]  {Thought: \u201cWe should click on the search input field.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cInput field\u201d, Element Content: \u201csearch\u201d}.\n[Step-4]  {Thought: \u201cWe should input the search query.\u201d,\nAction: \u201cInput\u201d, Input Content: \u201ccharger\\n\u201d}.\n[Step-5]  {Thought: \u201cWe should sort the results by price.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201csort by price\u201d}.\n[ResponsibleTA \u2013 Feasibility Predictor]: {Feasible: \u201cNo\u201d} ( # before execution)\n(# Reason: No valid element \u201csort by \nprice\u201d in the current page.)\n[ResponsibleTA \u2013 Completeness Verifier]: {Completed: \u201cNo\u201d} ( # after execution)\n(# Reason: Need a further step for reach \nthe goal of the planned step-5.)\nFigure 4: Detailed case study about how our proposed feasibility predictor and completeness verifier\nin ResponsibleTA remedy the failure case to achieve success on the No.9 task in Table 2. The 6-th to\n9-th steps are omitted for brevity. GPT-4 [27] is used as the LLM-based coordinator.\n6\nConclusion and Broader Impact\nIn this paper, we present a fundamental multi-modal framework, ResponsibleTA, for empowering\nLLMs as responsible task automators. In specific, we enhance LLMs with three core capabilities, i.e.,\nfeasibility prediction, completeness verification and security protection. Moreover, we propose and\ncompare different technical paradigms for implementing these core functionalities. We experimentally\nobserve that domain-specific models deliver superior performance compared to prompt engineering\n9\nbased solutions on their implementations, while the later one does not require collecting domain-\nspecific data for model training. Besides, we also demonstrate the effectiveness of our proposed\nResponsibleTA and provide explanations on how our ResponsibleTA works intuitively through case\nstudy. As for the broader impacts, we hope our work can inspire more excellent works on the related\nbenchmark construction, method design and functionality extension on top of this work in the future.\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n[2] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings\nof the AAAI conference on artificial intelligence, volume 31, 2017.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In NeurIPS, volume 33, pages 1877\u20131901, 2020.\n[4] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language\nmodeling framework for object detection. ICLR, 2022.\n[5] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton. A\nunified sequence interface for vision tasks. Advances in Neural Information Processing Systems,\n35:31333\u201331346, 2022.\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416, 2022.\n[8] MDN contributors. Dom, 2021. https://developer.mozilla.org/en-US/docs/Web/\nAPI/Document_Object_Model/Introduction.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, pages 4171\u20134186, 2019.\n[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied\nmultimodal language model. arXiv preprint arXiv:2303.03378, 2023.\n[11] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario\nFritz. More than you\u2019ve asked for: A comprehensive analysis of novel prompt injection threats\nto application-integrated large language models. arXiv preprint arXiv:2302.12173, 2023.\n[12] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023.\n[13] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as\nzero-shot planners: Extracting actionable knowledge for embodied agents. In International\nConference on Machine Learning, pages 9118\u20139147. PMLR, 2022.\n[14] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim,\nWonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document\nunderstanding transformer. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel\nAviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVIII, pages 498\u2013517. Springer, 2022.\n[15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n10\n[16] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461, 2019.\n[17] Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du,\nLingfeng Zhu, Baohua Lai, Xiaoguang Hu, Dianhai Yu, and Yanjun Ma. Pp-ocrv3: More\nattempts for the improvement of ultra lightweight ocr system, 2022.\n[18] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei\nJi, Shaoguang Mao, et al. Taskmatrix.ai: Completing tasks by connecting foundation models\nwith millions of apis. arXiv preprint arXiv:2303.16434, 2023.\n[19] David Lim. dslim/bert-base-ner, 2023. https://huggingface.co/dslim/bert-base-NER.\n[20] Bill Yuchen Lin, Chengsong Huang, Qian Liu, Wenda Gu, Sam Sommerer, and Xiang Ren. On\ngrounded planning for embodied tasks with language models. AAAI, 2023.\n[21] Thomas F. Liu, Mark Craft, Jason Situ, Ersin Yumer, Radomir Mech, and Ranjitha Kumar.\nLearning design semantics for mobile apps. In The 31st Annual ACM Symposium on User\nInterface Software and Technology, UIST \u201918, pages 569\u2013579, New York, NY, USA, 2018.\nACM.\n[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.\n[23] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong\nZhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors,\n2022.\n[24] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines\nfor efficient cnn architecture design. In Proceedings of the European conference on computer\nvision (ECCV), pages 116\u2013131, 2018.\n[25] Vincent Micheli and Fran\u00e7ois Fleuret. Language models are few-shot butlers. arXiv preprint\narXiv:2104.07972, 2021.\n[26] Microsoft.\nMicrosoft ui automation, 2023.\nhttps://learn.microsoft.com/en-us/\ndotnet/framework/ui-automation/ui-automation-overview.\n[27] OpenAI. Gpt-4, 2023. https://openai.com/research/gpt-4.\n[28] OpenAI. Introducing chatgpt, 2023. https://openai.com/blog/chatgpt.\n[29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, et al. Training language models\nto follow instructions with human feedback. In NeurIPS, 2022.\n[30] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint\narXiv:2303.17580, 2023.\n[31] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and\nMatthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive\nlearning. arXiv preprint arXiv:2010.03768, 2020.\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[33] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics: Design\nprinciples and model abilities. Technical Report MSR-TR-2023-8, Microsoft, February 2023.\n11\n[34] Ryan Volum, Sudha Rao, Michael Xu, Gabriel DesGarennes, Chris Brockett, Benjamin\nVan Durme, Olivia Deng, Akanksha Malhotra, and William B Dolan. Craft an iron sword: Dy-\nnamically generating interactive game characters by prompting large language models tuned on\ncode. In Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay\n2022), pages 25\u201343, 2022.\n[35] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023.\n[36] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023.\n[37] Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom\nMitchell, and Shrimai Prabhumoye. Plan, eliminate, and track\u2013language models are good\nteachers for embodied agents. arXiv preprint arXiv:2305.02412, 2023.\n[38] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao\nLu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language\nmodeling. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part XXXVI, pages 521\u2013539. Springer, 2022.\n[39] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n12\n[Supplementary Material]\nA\nFurther Model Introduction\nWe elaborate the three core modules in our proposed ResponsibleTA, i.e., the feasibility predictor,\ncompleteness verifier and security protector, with their modeling in Sec.3 and their used datasets\nin Sec.4 in the main body of our paper. Besides these three core modules, in ResponsibleTA, we\nalso train a domain-specific command executor and a screen parsing model. The domain-specific\ncommand executor aims to locate the target UI element by predicting its spatial coordinates for\nautomating clicking or typing operations in accordance with given commands. And the screen parsing\nmodel converts a given screenshot into a series of element-wise descriptions in linguistic form, which\nplays the role of inputting the information of a screenshot in linguistic form to the LLM-based\ncoordinator in two scenarios: 1) when replanning; 2) when employing the prompt engineering based\nparadigms for implementing the feasibility predictor or completeness verifier, as proposed. This\nmodel is needed in consideration to that most of LLMs have not developed or released their visual\ninput APIs currently. These two modules are not the highlights of this work. We thus detail them in\nthis supplementary material.\nA.1\nDomain-specific Executor\nThe domain-specific executor is a multimodal model that accepts both a screenshot and a command as\nits inputs. It is analogous to the domain-specific model-based paradigm introduced for implementing\nthe feasibility predictor or completeness verifier in the main text. Inspired by Pix2Seq modeling\n[4, 5], we employ the same architecture design for this model with that of the feasibility predictor\nas illustrated in Figure 2 of the main text. It requires different instantiations for the structured\noutput format, i.e., \u201c<task_prompt> {results} </task_prompt>\u201d. In this model, the \u201c<task_prompt>\u201d\nand \u201c</task_prompt>\u201d are instantiated by \u201c<locate_element>\u201d and \u201c</locate_element>\u201d, respec-\ntively. And the \u201c{results}\u201d is organized as \u201c<x_min> {xmin} </x_min> <y_min> {ymin} </y_min>\n<x_max> {xmax} </x_max> <y_max> {ymax} </y_max>\u201d wherein [xmin, ymin, xmax, ymax]\ndenotes the coordinates of the top-left and bottom-right points of the bounding box corresponding to\nthe target UI element. It achieves 0.51 mIoU for locating the target UI elements in given commands.\nA.2\nScreen Parsing Model\nThe screen parsing model aims to detect all UI elements in a given screenshot and recognize their\nattributes, i.e., the location, text content, and type. Regarding the type attribute, we categorize each\nUI element into one of button, input, and icon. This model is a mixture of expert models including\nelement detector, text detector, text recognizer, and icon recognizer. For a given UI screenshot,\nthe element detector first locates all UI elements. Then, for button and input elements, the text\ndetector locates their text regions when texts are available, and the text recognizer extracts their\ntext contents. For icon elements, icon recognizer recognizes their categories as the text contents.\nSpecifically, for element detector, we adopt RTMDet [23]-style architecture with ShuffleNetv2-1.0x\n[24] backbone. It achieves 0.710 mAP on the test set introduced as follows. For text detector and text\nrecognizer, we employ the off-the-shelf models from PaddleOCRv3 [17]. For icon recognition, we\nuse ShuffleNetv2-1.0x [24] as the backbone of the icon classifier and use a fully connected layer as\nthe classification head. Our icon recognizer achieves 95.7% averaged accuracy on the test set.\nB\nFurther Dataset Introduction\nWe elaborate the datasets used for domain-specific feasibility predictor and completeness verifier\nin the main text. In this section, we further introduce the data for aforementioned domain-specific\nexecutor and screen parsing model.\nThe dataset for domain-specific executor consists of all feasible screenshot-instruction pairs from the\nfeasibility prediction dataset introduced in Sec.4.1 of the main text. Its training split contains 0.5M\nsamples from 38K desktop screenshots, and its testing split contains 27K samples from 2K desktop\nscreenshots. For the element detector in the screen parsing model, we collect a dataset upon publicly\navailable web pages and windows apps, comprising around 1.5M screenshots with 1.2M of them as\n13\nYes, I understand your expectation and I am ready to start.\n[System Prompt]:\nYou are a completeness verifier for task automation in Windows. Given a command that has been executed \nand the descriptions for all elements on current screen, you need return Yes or No to recognize the \ncompleteness of the given command, i.e., whether the command goal is successfully reached.\nFor example, you are given:\nCommand: \u201cOpen the sports news.\u201d\nUI elements on the current screen:\n[{ID: 0, Type: \u201cButton\u201d, Content: \u201cBBC homepage\u201d}, # The type could be \u201cbutton\u201d, \u201cicon\u201d or \u201cinput\".\n{ID: 1, Type: \u201cButton\u201d, Content: \u201cEngland defender Bronze has keyhole knee surgery.\u201d},\n{ID: 2, Type: \u201cButton\u201d, Content: \u201cSpurs players reimburse fans after thrashing.\u201d}, \u2026\u2026]\nYou should return:\n{Thought: \u201cThere are element contents that are related to the action query: sports news, so the given command has been \ncompleted, the answer should be yes.\u201d, Completeness: \u201cYes\u201d.}\nDo you understand my expectation? And are you ready to start?\n[User Query]:\nCommand: \u201c*** *** ***.\u201d\nUI elements on the current screen: [{ID: *, Type: *, Content: \u201c*** ***\u201d}, \u2026\u2026]\n{Thought: \u201c\u2217\u2217\u2217 \u2217\u2217\u2217 \u2217\u2217\u2217.\u201d,  Completeness: \u201cYes (or No)\u201d.}\nFigure 5: Illustration of our prompt engineering based paradigm for implementing the completeness\nverifier in our proposed ResponsibleTA.\nthe training split and 0.3M of them as the testing split. For these data, we obtain the annotations of UI\nelements, i.e., their types and bounding boxes, from their tree-structure metadata, i.e., DOM [8] and\nUIA [26]. Only leaf nodes are used. For the icon classifier in the screen parsing model, we build a\ndataset based on a public one (Rico [21]), which contains 14,043 icon images with 14 frequently used\nicon categories. Its training split contains 12,637 samples while its test split contains 1,405 samples.\nC\nMore Implementation Details\nC.1\nTraining Details\nAs introduced, in ResponsibleTA, the feasibility predictor, completeness verifier and domain-specific\nexecutor share the same model architecture design as shown in Figure 3 of the main text. For this\narchitecture, we employ Swin Transformer [22] and BART model [16] as its vision encoder and\nlanguage decoder, respectively. For all of them, we first pretrain the entire model on document\nunderstanding tasks introduced in [14] and then finetune it on those datasets for feasibility prediction,\ncompleteness verification and command execution. Unless specifically stated, we perform the\nfinetuning on each task for 20 epochs using 8 NVIDIA V100 GPUs, with a batch size of 2 on each\nGPU card. The height and width of screenshots are resized to 960 and 1280, respectively. We use\nthe Adam optimizer [15] and set the initial learning rate to be 1\u00d710\u22124. Besides, we apply a cosine\nlearning rate annealing schedule and a gradient clipping technique with the maximum gradient norm\nof 1.0.\nC.2\nPrompt Design Details\nSimilar to the proposed paradigms for implementing the feasibility predictor, we also introduce two\nanalogical paradigms for implementing the completeness verifier in our proposed ResponsibleTA.\nRegarding the prompt engineering based paradigm, we detail its related prompt design as illustrated\nin Figure 5 for clearer introduction and better reproducibility.\nD\nMore Experiment Results\nIn Figure 4 of the main text, we have depicted the automation process of the first five steps on a\nspecific task (i.e., Task 9 in Table 2 of our main text) to show how our proposed feasibility predictor\nand completeness verifier pla their roles in turning an originally failed case into a successful one.\n14\nHere, in Sec.D.1, we provide its complete version with its part-1 (from the beginning to the 6-th step)\nillustrated in Figure 6 and its part-2 (from the 7-th step to the end) illustrated in Figure 7. Furthermore,\nwe provide a failure case (illustrated in Figure 8) and its analysis in Sec.D.2.\nD.1\nA Successful Case and Its Analysis\nNote that the in-depth analysis for the part-1 of this case is in Sec.4.3 of the main text. We provide\nthe detailed analysis regarding its part-2 here. As shown in Figure 7, the GPT-4 based coordinator in\nResponsibleTA originally plans to click the button with the content of \u201ccheapest charger\u201d. However,\nin the real web page, there is no matched element on the current page. At this time, the feasibility\npredictor considers this planned command as an infeasible one before execution, and asks the\ncoordinator for a replanned command upon the information of the current page. Then, the coordinator\nthinks we should click the element containing charge information with the smallest y-coordinate so\nthat this step is correctly processed. The coordinator plans for the next step, i.e., adding the selected\nitem to the shopping cart. It gives an infeasible command again since there is no \u201cadd to chart\u201d item\non the current page. This planned goal requires two execution operations to be completed in actual.\nWith the help of the feasibility predictor and completeness verifier, our ResponsibleTA utimately\nachieves the purpose of adding the item to the shopping cart by clicking the \u201cSee All Buying Options\u201d\nbutton followed by the \u201cAdd to Cart\u201d button. As such, the human instruction \u201cGo to Amazon and add\nthe cheapest charger into the shopping cart.\u201d is successfully automated.\nFrom the detailed analysis of this case, we can intuitively understand the functions of the feasibility\npredictor and completeness verifier in ResponsibleTA. In specific, the feasibility predictor can\nintercept unreasonably planned commands. And the completeness verifier checks whether the actual\nexecuted operations have achieved the intended goals step-by-step. They serve as a double guarantee\nfor ResponsibleTA to responsibly achieve task automation before and after command execution, by\nproviding feedbacks for the coordinator so that it can perform replanning timely.\nD.2\nA Failure Case and Its Analysis\nWe describe a failure case (i.e., the No.12 task in Table 2 of our main text) that the feasibility predictor\nand completeness verifier cannot remedy, as illustrated in Figure 8. This failure happens in automating\nthe human instruction \u201cSearch the Cpython repo and download its zip file in github.com.\u201d. In most\nGitHub repositories, we can achieve the download purpose by directly clicking the \u201cDownload ZIP\u201d\nbutton. However, in some GitHub repositories, such as the one in our illustrated failure case, the\n\u201cDownload ZIP\u201d button is hidden in a secondary menu. In this case, we are required to complete the\ndownload of the ZIP file through two operations: first clicking the \u201cLocal\u201d button, and then clicking\nthe \u201cDownload ZIP\u201d button. As shown in Figure 8, although our proposed module accurately detects\nthat the commands given by the coordinator are infeasible, the coordinator has not been able to\nprovide correct and feasible commands via its replanning. This task is ultimately terminated when\nthe preset maximum number of replanning attempts is reached. This failure case implies that the\nknowledge of current LLMs is generic but may not be perfect, and there is room for our proposed\nResponsibleTA to become more powerful as the capabilities of LLMs improve.\n15\n[ResponsibleTA \u2013 Coordinator]: (# replanning)\n[Step-5]  {Thought: \u201cThe task is about sorting the results by price. The No.128 element has      \nthe content 'Sort by:Featured', which seems to be the sorting button. I should click it.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201csort by: featured\u201d}.\n[Human]: Go to Amazon and add the cheapest charger into the shopping cart.\n[ResponsibleTA \u2013 System]: Received! Calling GPT-4 for task planning \u2026 \n[ResponsibleTA \u2013 Coordinator]:\n[Step-1]  {Thought: \u201cWe should open Edge browser to reach a website.\u201d,\n \n \nAction: \u201cOpen App\u201d, App Name: \u201cEdge\u201d }.\n[Step-2]  {Thought: \u201cWe should write the url to reach a website.\u201d,\n \n \nAction: \u201cInput\u201d, Input Content: \u201camazon.com\\n\u201d}.\n[Step-3]  {Thought: \u201cWe should click on the search input field.\u201d,\n \n \nAction: \u201cClick\u201d, Element Type: \u201cInput field\u201d, Element Content: \u201csearch\u201d}.\n[Step-4]  {Thought: \u201cWe should input the search query.\u201d,\n \n \nAction: \u201cInput\u201d, Input Content: \u201ccharger\\n\u201d}.\n[Step-5]  {Thought: \u201cWe should sort the results by price.\u201d,\n \n \nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201csort by price\u201d}.\n[ResponsibleTA \u2013 Feasibility Predictor]: {Feasible: \u201cNo\u201d} ( # before execution)\n(# Reason: No valid element \u201csort by \nprice\u201d on the current page.)\n[ResponsibleTA \u2013 Completeness Verifier]: {Completed: \u201cNo\u201d} ( # after execution)\n(# Reason: Need a further step for \nreaching the goal of the planned step-5.)\n[ResponsibleTA \u2013 Coordinator]: (# replanning for the step-6)\n[Step-6]  {Thought: \u201cThe task is to sort the results by price. The No.132 element has the \ncontent 'Price: Low to High', which is the button to sort the results by price.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201cprice: low to high\u201d}.\nFigure 6: Illustration of a successful case (i.e., the No.9 task in Table 2 of our main text) about how\nour proposed feasibility predictor and completeness verifier in ResponsibleTA remedy the failure\ncase to achieve success. This figure depicts the part-1 of its complete version, i.e., from the 1-th step\nto the 6-th step. GPT-4 [27] is used as the LLM-based coordinator.\n16\n[ResponsibleTA \u2013 Coordinator]: (# replanning for the step-7)\n[Step-7]  {Thought: \u201cThe task is about selecting the cheapest charger. The ID 138, ID 152, \nand ID 164 elements are the three elements containing charger information. The cheapest \ncharger should be the one with the smallest y-coordinate among the charger elements. The \nID 138 element is the one with the smallest y-coordinate among them.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201cnew battery charger\u201d}.\n[ResponsibleTA \u2013 Coordinator]:\n[Step-8]  {Thought: \u201c'I should add the charger to the shopping cart.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201cadd to chart\u201d}.\n[ResponsibleTA \u2013 Coordinator]:\n[Step-7]  {Thought: \u201cI should select the cheapest charger.\u201d,\n \nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201ccheapest charger\u201d}.\n[ResponsibleTA \u2013 Feasibility Predictor]: {Feasible: \u201cNo\u201d} ( # before execution)\n(# Reason: No valid element \u201ccheapest \ncharger\u201d found on the current page.)\n[ResponsibleTA \u2013 Coordinator]: (# replanning for the step-8)\n[Step-8]  {Thought: \u201cThe task is about adding the cheapest charger to the shopping cart. \nThe ID 147 element is the charger, and the ID 148 element is the 'See All Buying Options' \nbutton which should be clicked to add the charger to the cart.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201csee all buying options\u201d}.\n[ResponsibleTA \u2013 Feasibility Predictor]: {Feasible: \u201cNo\u201d} ( # before execution)\n(# Reason: No valid element \u201cadd to chart\u201d \nfound on the current page.)\n[ResponsibleTA \u2013 Completeness Verifier]: {Completeness: \u201cNo\u201d} ( # after execution)\n(# Reason: Need a further step for \nreaching the goal of the planned step-8.)\n[ResponsibleTA \u2013 Coordinator]: (# replanning for the step-9)\n[Step-9]  {Thought: \u201cThe task is to add the cheapest charger to the shopping cart. And the           \n \nID 138 element is the \u2018add to chart\u2019 button.\u201d,\n \n \nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201cadd to chart\u201d}.\nFigure 7: Continuing from the Figure 6 in this supplementary material, this figure depicts the part-2\nof the complete automation process of No.9 task, i.e., from the 7-th step to the end.\n17\n[ResponsibleTA \u2013 Coordinator]: (# replanning for the step-7)\n[Step-5]  {Thought: \u201cThe task is to download Cpython repo as zip in github.com by \nclicking the 'Download ZIP' option. The element with the 'Download ZIP' content is the one \nwe need to click.\u201d,\nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201cDownload ZIP\u201d}.\n[Human]: Search the Cpython repo and download its zip file in github.com..\n[ResponsibleTA \u2013 System]: Received! Calling GPT-4 for task planning \u2026 \n[ResponsibleTA \u2013 Coordinator]:\n[Step-1\u22ef \ud835\udfd4] \u22ef \u22ef\n[Step-7]  {Thought: \u201cWe should click the \u2018Download ZIP\u2019 option.\u201d,\n \nAction: \u201cClick\u201d, Element Type: \u201cButton\u201d, Element Content: \u201cDownload ZIP\u201d}.\n[ResponsibleTA \u2013 Feasibility Predictor]: {Feasible: \u201cNo\u201d} ( # before execution)\n(# Reason: No valid element \u201cDownload ZIP\u201d on the current page.)\n[ResponsibleTA \u2013 Feasibility Predictor]: {Feasible: \u201cNo\u201d} ( # before execution)\n# Reason: No valid element \u201cDownload ZIP\u201d on the current page.\n# The ground-truth actions (from human) for the remaining steps: \nStep-7\nStep-8\nFigure 8: Illustration of a failure case (i.e., the No.12 task in Table 2 of our main text). The first six\nsteps are omitted in this figure for the brevity. GPT-4 [27] is used as the LLM-based coordinator.\n18\n"
  },
  {
    "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
    "link": "https://arxiv.org/pdf/2306.01160.pdf",
    "upvote": "1",
    "text": "Faster Causal Attention Over Large Sequences\nThrough Sparse Flash Attention\nMatteo Pagliardini*\nEPFL\nmatteo.pagliardini@epfl.ch\nDaniele Paliotta*\nUniversity of Geneva\ndaniele.paliotta@unige.ch\nMartin Jaggi\nEPFL\nmartin.jaggi@epfl.ch\nFran\u00e7ois Fleuret\nUniversity of Geneva\nfrancois.fleuret@unige.ch\nAbstract\nTransformer-based language models have found many diverse applications requir-\ning them to process sequences of increasing length. For these applications, the\ncausal self-attention\u2014which is the only component scaling quadratically w.r.t. the\nsequence length\u2014becomes a central concern. While many works have proposed\nschemes to sparsify the attention patterns and reduce the computational overhead\nof self-attention, those are often limited by implementation concerns and end up\nimposing a simple and static structure over the attention matrix. Conversely, im-\nplementing more dynamic sparse attention often results in runtimes significantly\nslower than computing the full attention using the Flash implementation from\nDao et al. (2022). We extend FlashAttention to accommodate a large class of\nattention sparsity patterns that, in particular, encompass key/query dropping and\nhashing-based attention. This leads to implementations with no computational\ncomplexity overhead and a multi-fold runtime speedup on top of FlashAttention.\nEven with relatively low degrees of sparsity, our method improves visibly upon\nFlashAttention as the sequence length increases. Without sacrificing perplexity, we\nincrease the training speed of a transformer language model by 2.0\u00d7 and 3.3\u00d7 for\nsequences of respectively 8k and 16k tokens.\n1\nIntroduction\nMany methods have been developed to mitigate the quadratic cost of self-attention in Transformers\n(Vaswani et al., 2017). Some methods attempt to linearize the attention (Beltagy et al., 2020; Wang\net al., 2020) by for instance linearizing the softmax operator to take advantage of the associativity of\nmatrix products (Katharopoulos et al., 2020). Other methods rely on a predefined sparse masking of\nthe attention matrix, e.g. to constrain the attention to a local temporal neighborhood (Zaheer et al.,\n2020; Child et al., 2019). While the structure is fixed, it is assumed that information from arbitrary\nlocations in the sequence can still flow through this structure over several layers. All those methods\nimpose static implicit or explicit constraints over the attention matrix.\nAnother promising line of work consists in computing a dynamic modulation of a sub-part of the\nattention matrix. They are based, for instance, on dropping keys and queries (Kim et al., 2022) or\nusing geometric hashing of the keys and queries to identify linear cost sub-blocks of the attention\nmatrix that carry most of the weight (Kitaev et al., 2020).\n* Equal contribution.\nPreprint. Under review.\narXiv:2306.01160v1  [cs.LG]  1 Jun 2023\nThe promising theoretical computational complexity of these methods contrasts with the fact that\ntoday\u2019s most successfully deployed practical models instead rely on vanilla attention, in part thanks to\nthe efficiency of FlashAttention (Dao et al., 2022). This implementation is mathematically identical to\nthe vanilla attention proposed by Vaswani et al. (2017) in their seminal paper, but trades in additional\ncompute for less memory I/O.While still avoiding a memory footprint quadratic with the sequence\nlength, it delivers practical speedups of over 5\u00d7 compared to a naive implementation.\nUsing an attention layer in an autoregressive model\u2014which has been key in the recent remarkable\nAI breakthroughs\u2014requires to make it causal. This is achieved by applying a mask to the attention\nmatrix, so that information cannot flow from the future to the past during training.\nWhile FlashAttention can deal with vanilla causal masks, it does not provide enough flexibility to be\nused for situations where the causal attention mask is not perfectly regular, that is, lower triangular.\nThis in particular prevents using it for models that dynamically drop keys and queries or rely on\ngeometric hashing, which results in irregular causal structures as illustrated in Fig. 1 and Fig. 2.\nWe propose an extension of FlashAttention\u2014Sparse Causal Flash Attention (SCFA)\u2014 that addresses\nthis constraint. Our contribution is threefold:\n\u2022 We present the SCFA GPU kernel, which relaxes the constraint that the causal mask has to\nbe triangular. This kernel can handle any sparsity pattern that can be expressed with a range\nof keys per query, and any causal masking in the resulting sub-blocks. See \u00a7 3.\n\u2022 We show that SCFA permits to revisit the promising paradigm of dynamic hash-based\nattention. We devise an algorithm that builds upon the fundamental idea of Reformer (Kitaev\net al., 2020) to restrict the computation of the attention matrix over \u2018hash collision blocks\u2019,\nbut avoids both the high computational cost, and the approximate coverage of the hash\ncollisions. See \u00a7 3.2.\n\u2022 We propose a new approach implemented with SCFA that reduces computation by dynami-\ncally selecting, for each head, keys and queries to be removed from the attention operation,\nsuperseding existing methods that limited pruning to entire heads or entire queries/keys, due\nto the lack of an efficient fine-grained kernel implementation. See \u00a7 3.1.\nExperimental evaluations show that SCFA can efficiently be used for a variety of sequence modeling\ntasks, and that our open-source implementation in the Triton language and compiler (Tillet et al.,\n2019) significantly outperforms FlashAttention as we increase the sparsity and for longer sequences.\nMoreover, unlike the hash-based attention introduced in Reformer (Kitaev et al., 2020), our hash-\nbased SCFA not only implements the exact computation, but also has a faster runtime (see \u00a7 4.2).\nFinally, we show that a prototype of query and key dropping can be implemented thanks to SCFA, and\nthat the computational reduction is proportional to the fraction of query-key pairs dropped (see \u00a7 4.3).\n2\nRelated work\nState-of-the-art sequence models have very high computational requirements. As a consequence, a lot\nof effort has been invested into developing methods to reduce the memory footprint in Transformers.\nMany efficient Transformer variants have been developed, with the main goal of taming the quadratic\ncomplexity of the attention mechanism (Tay et al., 2020). Several methods rely on kernelized attention\n(Katharopoulos et al., 2020; Choromanski et al., 2020), while others endow the Transformer with\nsome auxiliary memory to increase the context (Wu et al., 2022; Borgeaud et al., 2021).\nIn many cases, leveraging sparsity in the attention matrix has proven useful. The Sparse Transformer\n(Child et al., 2019) works with a factorized sparse representation of the attention. They employ\nseveral sparse attention patterns, where each output position only computes weightings from a subset\nof input positions.\nThe Reformer (Kitaev et al., 2020) uses locality-sensitive-hashing (LSH) to sparsify the attention\nmatrix and allow queries to restrict their context window to keys that collide with the same hash.\nHowever, to allow GPU-efficient processing, complex machinery has to be developed where the\nqueries and keys are split into fixed-sized chunks, with the attention being applied only within the\nchunk and the immediate neighbor.\n2\nK\nQ\nQK-sparse\nHash-sparse\nFigure 1: Proposed sparsification of the attention matrix for a given attention head. In each\ndepicted attention matrix, black areas indicate coefficients to compute, patterned areas those forced\nto zero due to the causal masking, and white areas coefficients that are ignored. We consider two\nmain dynamic strategies to sparsify the left attention matrix. The QK-sparse attention consists of\ndropping some keys and queries (top, the discarded keys and queries are indicated in red), and the\nHash-sparse attention computes a hash code for each key and each query, and restricts the attention\nmatrix to blocks of keys and queries of same hash code (bottom, the three hash values are indicated\nfor each key or query with the colors blue/green/red). In both cases, the attention operation must be\nable to deal with sub-blocks of the attention matrix with a non-triangular causal mask.\nFlashAttention introduced by Dao et al. (2022) has recently gained a lot of popularity as an efficient,\nIO-aware exact attention implementation. FlashAttention uses tiling to avoid materializing the full\nattention matrix on slow GPU HBM, splitting the computation over blocks of query, key, and value\nvectors. FlashAttention has already reached wide adoption, as it\u2019s now available directly in Pytorch\nas of version 2.0. Additionally, FlashAttention supports very efficient block-sparse structures.\nBigbird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) are two more variants that work\nwith sparsified version of the attention matrix. Both approaches rely on a fixed structure that is\nindependent of the input values, using a combination of local, global, and random attention.\nHash Attention. When computing the attention matrix for a T \u00d7 D query tensor Q and a T \u00d7 D key\ntensor K, we consider the matrix of dot-products QK\u22a4, which can become impractical to compute\nfor very long sequences. However, we are only interested in the row-wise softmax(QK\u22a4), meaning\nthat the contribution of the keys to every query is dominated by the ones with the highest similarity.\nThus, restricting the attention computation to queries and keys with high similarity is a natural choice\nto reduce the computation.\nHash attention, introduced in the Reformer (Kitaev et al., 2020), allows to quickly select the closest\nkey vectors for each query using locality-sensitive-hashing (LSH). In general, the LSH mechanism\nassigns a hash code to vectors with the requirement that vectors that are close in space are mapped to\nthe same hash with high probability. For the hash attention, the Reformer assumes a shared query-key\nspace (Q = K). After computing the hashes, the queries are sorted according to their hash bucket.\nIn the sorted attention matrix, pairs that fall into the same bucket cluster near the diagonal. In order\nto implement the LSH-attention scheme efficiently on GPU, the Reformer splits the queries into\nfixed-sized chunks. Queries belonging to the same chunk can attend to each other and one chunk\nback. This results in a suboptimal mechanism where there is no guarantee that the attention will\ncapture exactly all of the elements that belong to the same bucket (See Fig. 4).\nFlashAttention. The standard self-attention operation consists of multiplying a T \u00d7 D query tensor\nQ by a T \u00d7 D key tensor K, to obtain a matching score matrix, which is then rescaled and row-\nnormalized with softmax, to get a T \u00d7 T attention matrix A. This matrix is then multiplied by a\nT \u00d7 D\u2032 value tensor V to obtain the final result. This is the core operation in a standard Multi-Head\nAttention layer, where additional operations take place to compute Q, K, and V from the layer\u2019s\ninput, and multiple instances of this processing take place in parallel.\n3\nHash-sparse\nQK-sparse\nK\nQ\nFigure 2: SCFA computation patterns. In each depicted attention matrix, black areas indicate\ncoefficients to compute, patterned areas are those forced to zero due to the causal masking, and\nwhite areas coefficients that are ignored. The red squares in the bottom matrices show the tiles\nactually computed by our SCFA kernel. In the regular case (left), this coincides with the behavior\nof FlashAttention. However, in the case of irregular causal masking due to keys/queries dropping\n(center) or in the case of irregular causal masking and band block sparsity due to hashing (right),\nFlashAttention does not provide means to compute a fine-grain subset of the attention matrix.\nThe two key contributions of FlashAttention are (1) to compute the attention matrix block-wise,\nto minimize the transfer of keys and queries to the cache memory as much as possible, and (2) to\ncompute the attention matrix on the fly during both the forward and the backward passes, which is\nfaster than retrieving it from memory, and avoids a memory footprint quadratic with the sequence\nlength T.\nFor the generalization that is of concern to this article, we focus on the block computation. In the\nimplementation of FlashAttention, causal masking is done by using the row and column indexes of\nthe blocks, and the row and column indexes of the keys and queries in individual blocks: attention\nblocks are computed fully for any block with a query index strictly larger than the key index. For the\nblocks for which the query index is equal to the key index, a regular lower triangular mask is applied.\nThis is illustrated on Fig. 2, bottom left.\n3\nMethod\nWe develop an efficient CUDA kernel written in Triton (Tillet et al., 2019) that maintains the careful\nmemory management of FlashAttention but can handle a causal structure defined through an arbitrary\nindexing of the keys and the queries. In the case where this indexing consists of a binary decision\nto drop or not the head of a query/key, this corresponds to our QK-sparse kernels as described in\n\u00a7 3.1. In the case where the indexing corresponds to bucket indices e.g. obtained from hashing, this\ncorresponds to our Hash-sparse kernel described in \u00a7 3.2.\nNotations. Input tensors for attention as in Vaswani et al. (2017) are of shape B \u00d7 H \u00d7 T \u00d7 D, with\nB being the batch size, H the number of heads, T the sequence length, and D the dimension per head.\nIn the following we take the view of a single head and instead consider a query tensor Q of shape\nTQ \u00d7 D, and a key K and value V tensors of shapes TKV \u00d7 D. The algorithms described below\nwill be run in parallel for all elements of the Cartesian product B \u00d7 H. We split tensors into blocks:\nQ \u225c [Q0, . . . , Qm], K \u225c [K0, . . . , Kn]. We define a tile Ti,j \u225c QiK\u22a4\nj , which corresponds to the\ndot products of a subpart of the attention matrix (see Fig. 2).\n4\n3.1\nQK-Sparse Attention\nShrinking the attention matrix. Our QK-sparse attention kernel is best summarized in the first row\nof Fig. 1. Independently for each head, we decide to keep or drop keys and queries. We then remove\ndropped keys and queries to create smaller Qc, Kc, and V c tensors. Through this reduction we are\nleft with a smaller attention matrix Ac which still has a causal structure in that indices for the queries\nand keys are increasing monotonically.\nLeveraging non-triangular causal attention structure. Despite the advantageous structure of the\nsmaller attention matrix, existing implementations fail to take advantage of it. Especially, as shown in\nFig. 2 bottom-left, FlashAttention can leverage the causal structure when the causal mask is triangular,\nbut does not support any other shape. In the forward pass, FlashAttention is, for each block of queries\nQi, processing blocks of keys Kj one after the other, moving along a row of tiles: Ti,0, . . . , Ti,n.\nCausality dictates that it is unnecessary to process a tile Ti,j when i < j. We cannot follow this rule\nanymore when working with compact representations. To leverage the causal structure of Ac, we\nbuild a new kernel which gets as additional input vectors qidx \u2208 RTQ and kidx \u2208 RTKV representing\nthe indices of the queries and keys in the original uncompressed tensors. Those are similarly split\ninto blocks: qidx \u225c\n\u0002\nqidx\n0\n, . . . , qidx\nm\n\u0003\n. The condition for a tile Ti,j to be unnecessary to compute is\nnow to have max(qidx\ni\n) < min(kidx\nj\n). When processing a block of queries Qi, we iterate over the\nkey indices kidx\n0\n, . . . , kidx\nn\nto find the index jstop of the first block satisfying that condition. We then\nknow we need to process the tiles Ti,j for j \u2208 [0, jstop[. Within each tile Ti,j, we in addition apply a\nlocal causal mask by comparing indices in qidx\ni\nand kidx\nj\n. By computing jstop in such a way we can\nleverage the causal structure and have runtimes matching those of FlashAttention. The backward\npass can be adapted in a similar fashion, see App. B for more details.\nOverhead. Computing Qc, Kc, and V c requires sorting and allocating new tensors. Moreover, as\nwe drop keys and queries for every attention head, and for every sequence in the minibatch, we are\nforced to consider the largest sequence of non dropped keys/queries and use padding. However, while\nreordering and reshaping tensors can be costly, this overhead grows linearly with the sequence length\nand is largely compensated for larger sequences as we show in \u00a7 4.3.\nEdge cases. Dropping keys and queries can result in having stranded queries with no keys. This\nbehaviour is undefined and results in NaNs when using the FlashAttention and naive Pytorch\nimplementations. We solve this issue by modifying how softmax statistics are accumulated during\nthe forward and backward passes and ensure stranded queries default to 0 vectors. see App. B for\nmore details.\n3.2\nHash-Sparse Attention\nRestructuring attention based on hashes. Independently for each head, we associate a bucket\nidentifier to each key and query. We then need to reorder Q, K, V by sorting them along the sequence\nlength dimension. As shown in the bottom row of Fig.1, this results in clusters of keys and queries\nwith a similar hash index close to the diagonal. If the sorting is stable, i.e. it preserves ordering of\nqueries and keys when the hash index is the same, then those blocks have a local causal structure\nin which the original indices (original position in the sequence) of keys and queries is a monotonic\nfunction within the block. This brings us in a case very similar to the previous one in section \u00a7 3.1, in\nthat we now have the same structure but scattered by blocks within the full attention matrix.\nTaking advantage of the new structure. We would like to take advantage of the block structure\nand only compute attention for queries and keys falling into the same block while at the same time\nrespecting causality. We adapt the FlashAttention kernel in a very similar way as for our QK-sparse\nkernel. We now provide additional bucket indices qhash and khash to our kernel. Based on those\nhash indices, we now find not only the stopping index jstop but also a starting index jstart. jstart\nis the first index for which some of the indices in qhash\ni\nare present in khash\nj\n, jstop is the first index\nfor which all indices in khash\nj\nare strictly larger than indices in qhash\ni\n. In a second step we refine\njstop now based on the indices kidx and qidx, the updated \u02c6jstop is the last index j \u2208 [jstart, jstop[ for\nwhich max(qidx\ni\n) \u2265 min(kidx\nj\n). As shown in the last column of Fig. 2, we then only compute tiles\nTi,j for j \u2208 [jstart, \u02c6jstop]. As for the QK-sparse method, we use qidx and kidx to apply a causal mask\nlocally for each tile. In addition to the causal mask, we use qhash and khash to mask interactions\n5\nbetween keys and queries of different buckets. See App. B for details and to see how to adapt the\nbackward pass in a similar fashion.\nOverhead. As for the previous method, sorting and re-ordering Q, K and V is inducing some\noverhead increasing linearly with the sequence length. As shown in our experiments in \u00a7 4.2, this\noverhead is by large compensated for as the sequence length increases.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nSequence length\n0\n200\n400\n600\n800\n1000\n1200\nms\nnb = 2\nnb = 4\nnb = 8\nnb = 16\nnb = 32\nnb = 64\nHash-sparse (our method)\nNaive hash-sparse\nFlash attention\n0\n2000\n4000\n0\n10\n20\n30\nFigure 3: Comparing several hash-based sparse attention implementations with FlashAttention.\nSimilarly to QK-dropping-based sparsity in Fig. 7, due to the non-triangular causal mask resulting\nfrom re-ordering the tensors based on the hash buckets (see Fig. 1), a naive implementation would\nforce the computation of the entire attention matrix before applying a custom mask. This results in\nvery large runtimes independent of the number of buckets. On the other hand, our implementation\nmodifies the basic FlashAttention method to compute only what is required. While there is a cost to\nreordering the tensors based on the hash buckets, this cost is largely compensated for as the number\nof buckets nb increases, and as the sequence length increases.\n4\nExperiments & Results\nIn this section we present our experimental setup and results. We show that (i) unlike naive implemen-\ntations using existing libraries, our dynamic sparsity attention schemes can significantly improve over\nthe FlashAttention runtime, (ii) this still holds in real-world sequence modeling tasks after factoring\nin all the non-attention operations, and (iii) it is possible to match\u2014and sometimes outperform\u2014the\nbaselines in terms of perplexity while significantly gaining in speed.\n4.1\nExperimental Setup\nDatasets. We test our hash-based sparsity scheme on MNIST (LeCun et al., 1998) for autoregressive\nimage generation, enwik8 (Hutter, 2012), and OpenWebText2 (Gao et al., 2020). We experiment with\nQK-dropping based sparsity on OpenWebText2.\nModels & Baselines. For our language modeling experiments on OpenWebText2, we use a base\nautoregressive transformer architecture with 12 layers, a hidden size of 768, 12 heads of 64 dimensions\neach. For experiments on sequence length T = 8192, we use a batch size of 96 = 4 \u00d7 8 \u00d7 2 (batch\nsize 4 with 8 accumulation steps and data parallelism over 2 node). When T = 16384 we use a batch\nsize of 30 = 2 \u00d7 5 \u00d7 3. The resulting models are of around 122M parameters. The goal not being to\noutperform the state-of-the-art perplexity, we train for 15k iterations. The attention modules used are\neither using FlashAttention for the baselines or one of our sparse kernels for our methods. To ensure\na fair comparison, and similarly to Kitaev et al. (2020), we set the keys equal to normalized queries\nfor all of our models. See App. B for more details.\nHardware. All of our timing experiments with random tensors are done on NVIDIA A100 GPUs,\nusing bfloat16. For our language modeling tasks on OpenWebText2, we trained using data-\nparallelism on two or three A100s for experiments with sequence lengths of respectively 8192 and\n16384. When comparing runtimes in Fig 6 and Fig. 8, we normalize the times by multiplying by the\n6\n0\n20000\n40000\n60000\nSequence length\n0\n20\n40\n60\n80\n100\nms\n(a) Forward attn. runtimes\n0\n20000\n40000\n60000\nSequence length\n0.6\n0.7\n0.8\n0.9\n1.0\nCoverage\nReformer attention\nFlash attention\nHash-sparse (ours)\n(b) Coverage of Reformer attn.\nFigure 4: Comparing forward runtimes of attention mod-\nules alone. Fig.(a): Reformer attention ensures a linear compu-\ntational complexity w.r.t. the sequence length, outperforming\nFlashAttention for longer sequences. Fig.(b): However, due to\nthe fixed attention structure, the Reformer misses an increas-\ning fraction of hash collisions. Our approach outperforms both\nmethods and maintains 100% exact coverage of collisions for\nall sequence lengths. See App. B.3 and App. C for more details.\nFigure 5: Comparing models us-\ning Reformer attention vs our\nHash-sparse attention. On the\nsimple sequential MNIST task\n(predicting pixels as a sequence),\nwe obtain a comparable perplex-\nity as the Reformer.\nOn en-\nwik8 character language model-\ning, with T=4096, we outperform\nthe Reformer model by a margin.\nAttention\nMNIST\nenwik8\n(ppl \u2193)\n(bits/c \u2193)\nReformer\n1.76\n3.32\nHash-sparse\n1.67\n2.29\nnumber of GPUs used. Comparisons with the Reformer are performed on a single A100 or a single\nNVIDIA RTX 4090 GPU.\n4.2\nHash-based Attention\nHashing mechanism For our experiments, we adopt the same hashing procedure as Kitaev et al.\n(2020). Namely, we use a shared query-key space, and we disallow queries to attend to themselves.\nWe also adopt the LSH scheme from Andoni et al. (2015). This allows us to pick the number of\nunique hash codes. We refer to bucket as the set of vectors that map to a certain hash.\nRuntime performances in a vacuum. We test our implementation with different numbers of\nbuckets nb and random keys, queries, and values. In these tests, we assume a hash bucket is\nprovided for free for each head of each key and query (they are sampled uniformly at random\n(torch.randint(0,nb)). In practice, runtime experiments on sequence modeling tasks show that\nobtaining the buckets can be cheap and in no way prevents us from improving the attention runtime\n(see Fig. 6). We compare with causal FlashAttention over the entire sequence. Importantly, to\nensure a fair comparison, we take into account pre-processing and post-processing steps required to\nreshape the tensors for both methods. For our method this includes stable sorting by bucket index\nand transposing tensors, for the baseline only the transposition is required, see App. B.2 for detailed\ncode. Fig. 3.b summarises our findings. We observe large improvements in runtime as the number of\nbuckets nb and the sequence length increases.\nLanguage modeling on OpenWebText2. For sequences of length T = 8192 and T = 16384 we\ntrain transformer language models using FlashAttention (F-LM), and identical models replacing only\nthe FlashAttention by our Hash-based sparse attention (H-LM) using nb = 16 hash buckets. In Fig. 6\nwe see that it takes the same number of iterations for H-LM and F-LM to reach a given perplexity.\nHowever, H-LM iterations are 1.8\u00d7 and 2.3\u00d7 faster for respectively T = 8192 and T = 16384. As a\nresult, H-LM models reach a given perplexity much faster than their F-LM counterpart. Interestingly,\nwe observe the H-LM models gain speed during training, see App. C for additional details.\nComparison with Reformer. We compare the speed and performance of our hash-sparse imple-\nmentation with the Reformer hashed attention. For all comparisons, we always equalize the average\nbucket size. Results are summarized in Fig. 4. Benchmarks with random inputs show that both our\nhash-sparse implementation and the Reformer, as expected, are linear with respect to the sequence\nlength (Fig. 4.a). However, we still achieve a significant speedup thanks to our more efficient kernel.\nMore importantly, Fig. 4.b shows that the fixed attention structure imposed by the Reformer does not\nallow to capture all of the hash collisions, with the coverage decreasing steeply as the sequence length\nincreases. On the contrary, our method is exact and covers every bucket collision in the attention\n7\n0\n5000\n10000\n15000\nIterations\n20\n30\n40\n50\n60\n70\nValidation Perplexity\n(a) Iter to reach perplexity\n0\n50\n100\n150\nTime (hours)\n20\n30\n40\n50\n60\n70\nValidation Perplexity\n(b) Time to reach perplexity\nT = 8192\nT = 16384\n0\n2\n4\n6\n8\n10\nIter/min on 1 GPU\n\u00d72.0\n\u00d73.3\nF-LM T = 8k\nF-LM T = 16k\nH-LM T = 8k\nH-LM T = 16k\n(c) Speed gains\nFigure 6:\nTraining Language Models (LM) on OpenWebText2 (Gao et al., 2020) using our\nhash-based sparsity (H-LM) or FlashAttention over the entire sequence (F-LM). We train on\nsequences of length 8192 and 16384 and use 16 buckets for all of our H-LM models. We show\nthat it is possible to use our proposed hash-based sparsity to significantly gain in speed while not\ncompromising the perplexity. In Fig.(a) we see for both sequence lengths the perplexity decreasing\nsimilarly as a function of the iteration. In fact, H-LM even slightly outperform the baseline. Fig.(b):\nH-LM reach lower perplexity much faster than their F-LM counterpart. Fig.(b) and (c): H-LM\nmodels are significantly faster than F-LM models for a given sequence length. The gap widens as the\nsequence length increases.\nmatrix. This is reflected in Table 5: our hash-sparse attention layer outperforms the Reformer attention\neven for shorter sequences.\n4.3\nQuery/Key-Dropping Based Attention\nQ/K-dropping mechanism used. We show that naively dropping heads for each key and query at\nrandom can already yield competitive results while significantly improving the runtime. While better\ndropping schemes could be devised, they are outside of the scope of this work.\nRuntime performances in a vacuum. We test our implementation with different sparsity ratios,\ncorresponding to the probability of dropping some head associated to a given key or query. We\nassume that the tensors indicating the dropping of each head of each query and key are given for free,\nalong with some random key, query, and value tensors. To ensure a fair comparison, we take into\naccount pre-processing and post-processing steps required to reshape the tensors for both methods,\nsee App. B for more details. For our approach, we hope reducing the size of the key, query and\nvalue tensors and computing the attention on those would be faster than using FlashAttention over\nthe entire sequence. For this, the time gained by computing the attention on smaller tensors should\nbe larger than the overhead of re-ordering tensors to build those smaller tensors. In Fig. 7.a, we\nshow a naive implementation using existing PyTorch functionalities only starts to provide a speedup\nwhen dropping more than 70% of the keys and queries. Fig. 7.b shows that using our proposed\nimplementation provides significant speedups even at relatively low sparsity levels. The linearly\nincreasing cost of reshaping the tensors is rapidly compensated by large gains over the quadratic cost\nof self-attention.\nLanguage modeling on OpenWebText2. For sequences of length T = 8192, we train transformer\nlanguage models using FlashAttention (F-LM), as well as identical models replacing only the\nFlashAttention by our Q/K-dropping sparse attention (D-LM). We train with several sparsity ratios,\ndropping 30%, 50%, and 70% of the heads of keys and queries at random. In Fig. 8 we observe that\nwhile high sparsity can negatively affect the perplexity, lower sparsity D-LM models are matching F-\nLM models in perplexity per iterations, while training nearly twice as fast. Importantly, the dropping\npattern is not static. An interesting approach similar to curriculum learning in which we start the\ntraining with very large sparsity and reduce it linearly during training is studied in App. C.\n8\n0\n1000\n2000\n3000\n4000\nSequence length\n0\n10\n20\n30\n40\n50\n60\nms\n50%\n60%\n70%\n80%\nQK-sparse (our method)\nNaive QK-sparse\nFlash attention\n(a) Naive implementation\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nSequence length\n0\n200\n400\n600\n800\n1000\nms\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n0\n2000\n4000\n0\n20\n40\n60\n(b) Our implementation\nFigure 7: Runtimes of the full Flash-attention of Dao et al. (2022) and several implementations\nof Query/Key dropping based sparsity. For this figure we show total times for the forward and\nbackward passes. For sparse methods, we drop at random a percentage of keys and queries, this\npercentage is indicated on the right of each curve. Fig.(a): A naive implementation consisting in\ncreating compact representations of the key, value, and query tensors by removing dropped keys\nand queries. As a result, the attention matrix is no longer triangular (see Fig. 1). We call the\nPyTorch scaled_dot_product_attention method with a custom but still causal mask. The non-\ntriangular mask prevents FlashAttention to be used and only dropping more than 70% of the keys\nand queries seems to improve the runtime over attending the entire sequence using FlashAttention.\nFig.(b): Our modification of FlashAttention allows to improve over the runtime. Similar to the naive\nimplementation, reshaping the tensor induce an overhead which compensates the speed gain for\nshorter sequences. However, this offset is compensated by a strong margin as the sequence length\nincreases. Our implementation allows significant gains over FlashAttention even for low levels of\nsparsity. The detailed runtimes for the forward and backward passes can be found in App. C.\n0\n5000\n10000\n15000\nIterations\n20\n30\n40\n50\n60\n70\nValidation Perplexity\n(a) Iter to reach perplexity\n0\n20\n40\n60\nTime (hours)\n20\n30\n40\n50\n60\n70\nValidation Perplexity\n(b) Time to reach perplexity\nF-LM\nD-LM\ns = 30% D-LM\ns = 50% D-LM\ns = 70%\n0\n5\n10\n15\nIter/min on 1 GPU\n\u00d71.9\n\u00d72.6\n\u00d73.5\n(c) Speed gains\nFigure 8: Training Language Models (LM) on OpenWebText2 (Gao et al., 2020) using random\nQuery/Key dropping based sparsity (D-LM) or FlashAttention over the entire sequence (F-LM).\nDropping keys and queries randomly is naive and our point here is not to show that this approach\nis a good way to use the proposed Q/K-sparsity attention, rather we want to demonstrate that it is\npossible to significantly gain in speed while not losing too much in perplexity\u2014even with a naive\napproach, and in a very dynamic way (two sequences are allowed to have completely different\ndropping patterns). For all methods we train over sequences of 8192 tokens. Fig.(a): While dropping\nlarge portions of keys and queries slows down the decrease of perplexity per iteration, dropping\n30% seems to match the baseline F-LM. Fig.(b) Our method is significantly faster to reach a given\nperplexity. Interestingly, more sparsity does not necessarily mean decreasing the perplexity faster.\nFig.(b) and (c): Using our Q/K-sparse implementation we train significantly faster than the baseline\nmethod.\n9\n5\nConclusion\nWe develop and validate an efficient kernel that can make sparse attention based on dynamic patterns\nvery fast. We hope that our contribution will inspire the community to research dynamic attention\npatterns in a way that is less constrained by a tight computational budget.\nThe computational cost of large attention models remains both a practical issue in scaling up to very\nlarge contexts, and a fundamental research question to close the gap between the energy usage of\nbiological systems to that of GPU systems able to run very large models. Dynamically modulating\nthe computation is an obvious direction to address this challenge.\n6\nAcknowledgments\nThe authors acknowledge support from the Swiss National Science Foundation under grant number\nCR\u2013 SII5\u2013193716 - \u201cRobust Deep Density Models for High-Energy Particle Physics and Solar Flare\nAnalysis (RODEM)\u201d. We also thank Igor Krawczuk for interesting discussions and suggesting using\nTriton.\nReferences\nAndoni, A., Indyk, P., Laarhoven, T., Razenshteyn, I., and Schmidt, L. Practical and optimal lsh for\nangular distance, 2015.\nBehnke, M. and Heafield, K. Losing heads in the lottery: Pruning transformer attention in neural\nmachine translation. In EMNLP (1), pp. 2664\u20132674. Association for Computational Linguistics,\n2020.\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR,\nabs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.\nBender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic\nparrots: Can language models be too big?\nIn Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency, FAccT \u201921, pp. 610\u2013623, New York, NY, USA, 2021.\nAssociation for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922.\nURL https://doi.org/10.1145/3442188.3445922.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G.,\nLespiau, J., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T.,\nHuang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O.,\nOsindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by\nretrieving from trillions of tokens. CoRR, abs/2112.04426, 2021. URL https://arxiv.org/\nabs/2112.04426.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.\nCoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509.\nChoromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis,\nJ., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L. J., and Weller, A. Rethinking attention\nwith performers. CoRR, abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact\nattention with io-awareness. In NeurIPS, 2022.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A.,\nNabeshima, N., Presser, S., and Leahy, C. OpenWebText2 dataset, as part of \u2018the Pile: An 800gb\ndataset of diverse text for language modeling\u2018. arXiv preprint arXiv:2101.00027, 2020.\nGoyal, S., Choudhury, A. R., Raje, S., Chakaravarthy, V. T., Sabharwal, Y., and Verma, A. Power-bert:\nAccelerating BERT inference via progressive word-vector elimination. In ICML, volume 119 of\nProceedings of Machine Learning Research, pp. 3690\u20133699. PMLR, 2020.\nHutter, M. The human knowledge compression contest. URL http://prize. hutter1. net, 6, 2012.\n10\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast autore-\ngressive transformers with linear attention.\nIn Proceedings of the International Conference\non Machine Learning (ICML), pp. 5294\u20135303, 2020. URL https://fleuret.org/papers/\nkatharopoulos-et-al-icml2020.pdf.\nKim, S., Shen, S., Thorsley, D., Gholami, A., Kwon, W., Hassoun, J., and Keutzer, K. Learned token\npruning for transformers. In KDD, pp. 784\u2013794. ACM, 2022.\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. In ICLR. OpenRe-\nview.net, 2020.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document\nrecognition. Proc. IEEE, 86(11):2278\u20132324, 1998.\nLi, J., Cotterell, R., and Sachan, M. Differentiable subset pruning of transformer heads. Trans. Assoc.\nComput. Linguistics, 9:1442\u20131459, 2021.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In ICLR (Poster). OpenRe-\nview.net, 2019.\nMichel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In NeurIPS, pp.\n14014\u201314024, 2019.\nPeng, H., Schwartz, R., Li, D., and Smith, N. A. A mixture of h - 1 heads is better than h heads. In\nACL, pp. 6566\u20136577. Association for Computational Linguistics, 2020.\nPeng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., and Kong, L. Random feature\nattention. In ICLR. OpenReview.net, 2021.\nQin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and Zhong, Y. cosformer:\nRethinking softmax in attention. In ICLR. OpenReview.net, 2022.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are\nunsupervised multitask learners, 2019.\nRaganato, A., Scherrer, Y., and Tiedemann, J. Fixed encoder self-attention patterns in transformer-\nbased machine translation. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pp.\n556\u2013568. Association for Computational Linguistics, 2020.\nTay, Y., Dehghani, M., Bahri, D., and Metzler, D.\nEfficient transformers: A survey.\nCoRR,\nabs/2009.06732, 2020. URL https://arxiv.org/abs/2009.06732.\nTay, Y., Bahri, D., Metzler, D., Juan, D., Zhao, Z., and Zheng, C. Synthesizer: Rethinking self-\nattention for transformer models. In ICML, volume 139 of Proceedings of Machine Learning\nResearch, pp. 10183\u201310192. PMLR, 2021a.\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and\nMetzler, D. Long range arena : A benchmark for efficient transformers. In ICLR. OpenReview.net,\n2021b.\nTillet, P., Kung, H. T., and Cox, D. Triton: An intermediate language and compiler for tiled neural\nnetwork computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on\nMachine Learning and Programming Languages, MAPL 2019, pp. 10\u201319, New York, NY, USA,\n2019. Association for Computing Machinery. ISBN 9781450367196. doi: 10.1145/3315508.\n3329973. URL https://doi.org/10.1145/3315508.3329973.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and\nPolosukhin, I. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.\norg/abs/1706.03762.\nVoita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention:\nSpecialized heads do the heavy lifting, the rest can be pruned. In ACL (1), pp. 5797\u20135808.\nAssociation for Computational Linguistics, 2019.\n11\nWang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse attention architecture with cascade token\nand head pruning. In HPCA, pp. 97\u2013110. IEEE, 2021.\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear\ncomplexity. CoRR, abs/2006.04768, 2020.\nWeidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P., Cheng, M., Glaese, M., Balle,\nB., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas,\nJ., Rimell, L., Hendricks, L. A., Isaac, W., Legassick, S., Irving, G., and Gabriel, I. Ethical and\nsocial risks of harm from language models. CoRR, abs/2112.04359, 2021.\nWu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers, 2022.\nXia, M., Zhong, Z., and Chen, D. Structured pruning learns compact and accurate models. In ACL\n(1), pp. 1513\u20131528. Association for Computational Linguistics, 2022.\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Onta\u00f1\u00f3n, S., Pham, P., Ravula, A.,\nWang, Q., Yang, L., and Ahmed, A. Big bird: Transformers for longer sequences. In Larochelle,\nH., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.\ncc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html.\nZheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In\nICML, volume 162 of Proceedings of Machine Learning Research, pp. 27011\u201327041. PMLR,\n2022.\nZhou, W., Ge, T., Wei, F., Zhou, M., and Xu, K. Scheduled drophead: A regularization method\nfor transformer models. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pp.\n1971\u20131980. Association for Computational Linguistics, 2020.\n12\nA\nAdditional Background\nA.1\nStructured attention\nWith a focus on performances on downstream applications, Raganato et al. (2020) show that it is\npossible to replace all but one attention head with static attentive patterns only relying on the position,\ne.g. attending to the previous token. Similarly, Tay et al. (2021a) investigate removing entirely the\ndot product attention and instead let the input token alone predict its attention pattern over the entire\nsequence, or use a random attention pattern. While they show their approach can be competitive on\ncertain downstream task, results on language modeling seems to indicate those attention patterns\nare not expressive enough to model natural language. Peng et al. (2021) propose Random Feature\nAttention (RFA) which relies on kernel methods to approximate the softmax atttention and achieve a\nlinear computational complexity w.r.t. the sequence length. A follow up work by Zheng et al. (2022)\naugments RFA with randomized attention to build an unbiased estimator of the softmax attention.\nInstead of looking for a linear approximation of the softmax, Qin et al. (2022) propose to replace the\nsoftmax by a linear projection kernel and a cos-based re-weighting mechanism which scales linearly\nwith the sequence length. While kernel based method seem like a good compromise in terms of\nspeed vs. performance, they have been shown to underperform on certain downstream tasks Tay et al.\n(2021b). Compared to those methods, we are not trying to replace or find an estimator of the softmax\nattention, we instead provide an efficient way to leverage different forms of sparsity of the softmax\nattention matrix. Our speed gains do not come from a conceptually different way to compute the\nattention but simply deciding not to compute certain regions of the softmax attention matrix.\nA.2\nPruning\nThe large body of works on pruning mostly focus on faster inference on downstream tasks. As a\nresult, many methods are modifying the training process to facilitate pruning, often resulting in slower\ntraining.\nPruning heads in transformers.\nMany works have investigated dropping entire attention heads in\ntransformer architectures. Let the entire attention matrix A be of shape B \u00d7 H \u00d7 T \u00d7 T with B the\nbatch size, H the number of heads, and T the sequence length. Dropping entire heads imposes an\nimplicit structure over A which is now of shape B \u00d7H\u2032 \u00d7T \u00d7T, with H\u2032 the number of non-dropped\nheads. Michel et al. (2019) and Voita et al. (2019) both observe\u2014in the context of Neural Machine\nTranslation (NMT)\u2014that a large fraction of heads can be dropped without significantly impacting\nperformance. While their goal is not primarily to speed up training, many of methods following\non this insight try to incorporate a sparsifying mechanism during training which facilitate dropping\nheads at test time (Behnke & Heafield, 2020; Peng et al., 2020; Xia et al., 2022; Li et al., 2021).\nStill limited to NMT and downstream classification tasks, Zhou et al. (2020) found a regularizing\neffect of dropping heads during training. In comparison, less works have investigated dropping head\nin the context of language modeling, with text generations applications in mind. In comparison to\nthose works, we propose a way to take advantage of dynamic sparsity structures, which is much more\ngeneral but still includes head dropping. We show the potential of our work to enable speedup during\ntraining. Our work is orthogonal to head-dropping mechanisms and could be used in addition to\nthose.\nPruning tokens in transformers.\nWhile dropping heads can be seen as dropping the model\nparameters generating keys, queries and values for those heads, another more recent line of work\nlooked into dropping tokens. Dropping entire tokens imposes an implicit structure over the attention\nmatrix A which is now of shape B \u00d7 H \u00d7 TQ \u00d7 TKV , with TQ and TKV the number remaining\nqueries and keys. Goyal et al. (2020) obtain faster inference on downstream tasks by using an\nattention based scoring mechanism to eliminate redundant input vectors. Wang et al. (2021) develop\na joint algorithm-architecture framework which speeds up inference for downstream classifications\ntasks and language generation tasks. Their method includes head and token pruning along with\nspecific hardware optimizations. Our work can be used to implement those approaches and does not\nrequire custom hardware accelerators. We moreover allow dropping individual heads instead of entire\ntokens.\n13\nB\nDetails On The Implementation\nWe here give implementation details for experiments of \u00a7 4. First, in App. B.1, we describe in more\ndetails the implementations of our custom Triton kernels introduced in \u00a7 3. Secondly in App. B.2 we\nprovide the python code used in our runtime benchmarks of \u00a7 4, including pre and post processing\nsteps required to reshape and re-order tensors. Lastly in \u00a7 B.3 we give more details on the hyperparam-\neters used in our sequence modeling experiments of \u00a7 4. The code for all our experiments can be found\nvia the following link: https://github.com/epfml/dynamic-sparse-flash-attention.\nB.1\nTriton Kernels\nQK-Sparse Triton Kernel.\nIn Alg. 1 we detail the core of the QK-sparse algorithm from \u00a7 3.1.\nThis algorithm is computing the softmax attention result only for one block of queries, corresponding\nto one head. In practice this algorithm would be run in parallel for all blocks of queries for all heads.\nWe find the index of the last relevant tile to compute by iterating over block of key indices and\ncomparing them with the largest query index for the current block of queries. This works as qidx and\nkidx have a monotonic structure thanks to the stable sort used when reshaping the tensors (see the\npre-processing code in App. B.2 for more details). We apply causal masking locally by looking at\nqidx and kidx, q query can only attend to past keys: mask = q_idx[:, None] >= k_idx[None,\n:] . The backward pass relies exactly on the same trick, we first iterate over query indices to find the\nstarting block of queries.\nAlgorithm 1 Forward Pass for the QK-sparse kernel\nRequire: Matrix Q \u2208 RNQ\u00d7d, matrices K, V \u2208 RNKV \u00d7d, index matrices Qidx \u2208 RNQ\u00d7d and\nKidx \u2208 RNKV \u00d7d ,softmax scaling constant \u03c4 \u2208 R, softmax statistics vectors M \u2208 RNQ and\nL \u2208 RNQ, output tensor O \u2208 RNQ\u00d7d, query block size Bm, key block size Bn, starting query\nindex startm.\n1: Initialize o \u2190 (0)Bm\u00d7d \u2208 RBm\u00d7d, \u2113 \u2190 (0)Bm \u2208 RBm, m \u2190 (\u2212\u221e)Bm \u2208 RBm\n2: Load current block of queries q \u2190 Q[startm : startm + Bm, :]\n3: Load current block of query indices qidx \u2190 Qidx[startm : startm + Bm]\n4: end \u2190 0\n5: for startn in range(0, NKV , Bm) do\n6:\nLoad block of key indices: kidx \u2190 Kidx[startn : startn + Bn]\n7:\nif min(kidx) \u2264 max(qidx) then\n8:\nend \u2190 end + 1\n9:\nend if\n10: end for\n11: for n in range(0, end) do\n12:\nStart of current key block startn \u2190 n \u2217 Bn\n13:\nLoad current block of values v \u2190 V [startn : startn + Bn, :]\n14:\nLoad current block of keys k \u2190 K[startn : startn + Bn, :]\n15:\nLoad current block of key indices kidx \u2190 Kidx[startn : startn + Bn]\n16:\nCompute inner product qk \u2190 \u03c4q.k\u22a4\n17:\nApply causal mask qk \u2190 MASK(qk, qidx, kidx)\n18:\nUpdate softmax statistics \u2113, m, o \u2190 UPDATE_STATS(\u2113, m, qk, o, v)\n19: end for\n20: Write O[startm : startm + Bm, :] \u2190 o\n21: Write L[startm : startm + Bm] \u2190 \u2113\n22: Write M[startm : startm + Bm] \u2190 m\nHash-Sparse Triton Kernel.\nIn Alg. 2 we detail the core of the Hash-sparse algorithm from \u00a7 3.2.\nThis algorithm is computing the softmax attention result only for one block of queries, corresponding\nto one head. In practice this algorithm would be run in parallel for all blocks of queries for all\nheads. We find the index of the first and last relevant tiles to compute by iterating over block of\nkey hashes and comparing them with the largest and smallest query hashes for the current block\nof queries. This works as we reshaped our Q, K, V tensors by sorting them by their hash values\n(see the pre-processing code in App. B.2 for more details). In addition to causal masking, we also\n14\nenforce attention to happen within the same bucket: mask = (q_idx[:, None] >= k_idx[None,\n:])\n& (q_hash[:, None] == k_hash[None, :]). In our experiments we often replace >= by\n> to prevent a query to attend to itself as in the Reformer. As a side note, for most application it\nwould also be fine to only enforce causal masking and allow attention across buckets within a tile.\nWhile this could add some serendipity in the attention computation, some applications might require\nmasking based on hash. The backward pass relies exactly on the same trick, we first iterate over\nquery indices to find the starting and end blocks of queries.\nAlgorithm 2 Forward Pass for the Hash-sparse kernel\nRequire: Matrix Q \u2208 RNQ\u00d7d, matrices K, V \u2208 RNKV \u00d7d, index matrices Qidx \u2208 RNQ\u00d7d and\nKidx \u2208 RNKV \u00d7d, matrices containing the hash values Qhash \u2208 RNQ\u00d7d and Khash \u2208 RNKV \u00d7d\n,softmax scaling constant \u03c4 \u2208 R, softmax statistics vectors M \u2208 RNQ and L \u2208 RNQ, output\ntensor O \u2208 RNQ\u00d7d, query block size Bm, key block size Bn, starting query index startm.\n1: Initialize o \u2190 (0)Bm\u00d7d \u2208 RBm\u00d7d, \u2113 \u2190 (0)Bm \u2208 RBm, m \u2190 (\u2212\u221e)Bm \u2208 RBm\n2: Load current block of queries q \u2190 Q[startm : startm + Bm, :]\n3: Load current block of query indices qidx \u2190 Qidx[startm : startm + Bm]\n4: Load current block of query hashes qhash \u2190 Qhash[startm : startm + Bm]\n5: start \u2190 0\n6: endhash \u2190 0\n7: for startn in range(0, NKV , Bm) do\n8:\nLoad block of key hashes: khash \u2190 Khash[startn : startn + Bn]\n9:\nif min(khash) \u2264 max(qhash) then\n10:\nendhash \u2190 endhash + 1\n11:\nend if\n12:\nif max(khash) < min(qhash) then\n13:\nstart \u2190 start + 1\n14:\nend if\n15: end for\n16: end \u2190 endhash\n17: for j in range(start, endhash) do\n18:\nLoad block of key indices: kidx = Kidx[jBn : (j + 1)Bn]\n19:\nif min(kidx) \u2264 max(qidx) then\n20:\nend \u2190 j + 1\n21:\nend if\n22: end for\n23: for n in range(start, end) do\n24:\nStart of current key block startn \u2190 n \u2217 Bn\n25:\nLoad current block of values v \u2190 V [startn : startn + Bn, :]\n26:\nLoad current block of keys k \u2190 K[startn : startn + Bn, :]\n27:\nLoad current block of key indices kidx \u2190 Kidx[startn : startn + Bn]\n28:\nLoad current block of key hashes khash \u2190 Khash[startn : startn + Bn]\n29:\nCompute inner product qk \u2190 \u03c4q.k\u22a4\n30:\nApply causal and bucket mask qk \u2190 MASK(qk, qidx, kidx, qhash, khash)\n31:\nUpdate softmax statistics \u2113, m, o \u2190 UPDATE_STATS(\u2113, m, qk, o, v)\n32: end for\n33: Write O[startm : startm + Bm, :] \u2190 o\n34: Write L[startm : startm + Bm] \u2190 \u2113\n35: Write M[startm : startm + Bm] \u2190 m\nAccumulating softmax statistics while avoiding NaNs.\nThe following is a brief summary of\nhow the FlashAttention algorithm (Dao et al., 2022) proposes to accumulate softmax statistics\nwhen iterating over blocks of keys. Given an input vector x, our goal is to compute softmax(x) \u225c\nex\u2212max(x)/ P\ni exi\u2212max(x). Let f(x, m) \u225c ex\u2212m, and \u2113(x, m) \u225c P\ni f(x, m). Hence:\nsoftmax(x) = f(x, max(x))\n\u2113(x, max(x))\n15\nGiven a vector x \u225c [x1, x2], let mg = max(x) (global max), and m1 = max(x1), we notice:\nsoftmax(x) = f(x, mg)\n\u2113(x, mg)\n=\nf(x1, m1)\nem1\u2212mg\u2113(x1, m1) + \u2113(x2, mg) +\nf(x2, mg)\nem1\u2212mg\u2113(x1, m1) + \u2113(x2, mg)\nTherefore, if we have m1, \u2113(x1, m1), and r = f(x1,m1)\nl(x1,m1) , we can update the softmax statistics for a\nnew block of entries x2 by following the following steps:\n1. Compute new global max: mg = max(m1, max(x2))\n2. Compute f(x2, mg) = ex2\u2212mg\n3. Compute \u2113(x2, mg) = P\ni f(x2, mg)\n4. Compute new \u2113(x, mg): \u2113(x, mg) = em1\u2212mg\u2113(x1, m1) + \u2113(x2, mg)\n5. Correct running softmax result: r = r \u2113(x1,m1)\n\u2113(x,mg)\n6. Add contribution from x2 to r: r = r + f(x2,mg)\n\u2113(x,mg)\nIn case all the keys are masked for a given query, we would have max(x2) = \u2212\u221e, given that the\nfirst m1 is initialized to \u2212\u221e as well (see Alg. 1 and Alg. 2) the fourth and second steps above would\nbe undefined and result in NaN values. We solve the problem by replacing \u2212\u221e values in mg by 0s\nwhen doing those two steps. Another potential issue is in step three: when a query has no matching\nkey in x2 then \u2113(x2, mg) is now 0, which generate +\u221e in step five. This is an issue as we process\nkeys by blocks, and if there are no keys for a query in the current block, we might find matching keys\nin following blocks. Adding \u221e values to r would prevent us to accumulate statistics later on. To get\nour desired behaviour and have 0s when queries have no matching key we replace \u221e values by 1s in\n\u2113(x, mg) during steps five and six. We summarize those steps in Alg. 3.\nAlgorithm 3 UPDATE_STATS method\nRequire: Vector \u2113 \u2208 RN, vector m \u2208 RN, matrix of masked inner products qk \u2208 RN\u00d7N, output\nbuffer o \u2208 RBm\u00d7d, block of values v \u2208 RBm\u00d7d\n1: Compute new global max (step 1) mnew \u2190 max(rowmax(qk), m)\n2: Replace \u2212\u221e by 0: \u02c6mnew \u2190 WHERE(mnew == \u2212\u221e, 0, mnew)\n3: Compute step 2: p \u2190 eqk\u2212 \u02c6\nmnew[:,None]\n# row-wise subtraction\n4: Compute step 3: \u21132 \u2190 rowsum(p)\n5: Compute step 4: \u2113new \u2190 em\u2212 \u02c6\nm\u2113 + \u21132\n6: Compute z \u2190\n1\n\u2113new\n7: Replace \u221e by 1: z \u2190 WHERE(z == \u221e, 1, z)\n8: Update p \u2190 p \u00d7 z[:, None]\n# row-wise multiplication\n9: Correct running softmax output (step 5) o \u2190 o \u00d7 (\u2113z)[:, None]\n# row-wise multiplication\n10: Add contribution from current block (step 6) o \u2190 o + p.v\n11: return \u2113new, mnew, o\nHyperparameters.\nWe extend the implementation of FlashAttention available in the Triton tutorial.\nIn our benchmarks, we use a batch size B = 4, 48 heads of 64 dimensions each.\nB.2\nRuntimes in a Vacuum\nBaseline implementation.\nWe use Pytorch\u2019s FlashAttention implementation provided by the\ntorch.nn.functional.scaled_dot_product_attention function. To ensure fairness, we as-\nsume that all benchmarked functions receive a tensor Q of shape (BATCH, CTX_Q, H, D_HEAD),\nand tensors K, V of shapes (BATCH, CTX_KV, H, D_HEAD), where BATCH is the batch size, CTX_Q\nis the number of queries, CTX_KV is the number of keys and values, H is the number of heads, and\nD_HEAD is number of dimensions per head. For this reason the only pre and post processing steps\nrequired are transposing the input and output tensors.\n16\nListing 1: pytorch_full_flashattention function applying the FlashAttention algorithm on the\nentire sequence (no sparsity).\n1 def\npytorch_full_flashattention (q, k, v):\n2\n3\nBATCH , N_CTX , H, D_HEAD = q.shape\n4\n5\nq = q.transpose (1, 2) # (BATCH , H, N_CTX_Q , D_HEAD)\n6\nk = k.transpose (1, 2) # (BATCH , H, N_CTX_KV , D_HEAD)\n7\nv = v.transpose (1, 2) # (BATCH , H, N_CTX_KV , D_HEAD)\n8\n9\ny = torch.nn.functional. scaled_dot_product_attention (q, k, v, dropout_p =0.0 ,\nattn_mask=None , is_causal=True)\n10\nreturn y.transpose (1 ,2).contiguous ()\nOur proposed interface.\nWe propose the following interface to orchestrate between the Hash-\nsparse and the QK-sparse implementations:\nListing 2: dynamic_sparse_attention interface\n1 def\ndynamic_sparse_attention (q, k, v, q_idx , k_idx , sm_scale=None ,\nsparsity_mode =\u2019hash \u2019):\n2\n\"\"\"\n3\nKeyword\narguments:\n4\nq: query\ntensor of shape (BATCH , N_CTX_Q , H, D_HEAD)\n5\nk: key\ntensor\nof shape (BATCH , N_CTX_KV , H, D_HEAD)\n6\nv: value\ntensor of shape (BATCH , N_CTX_KV , H, D_HEAD)\n7\nq_idx: tensor of shape (BATCH , N_CTX_Q , H) for\neach\nsequence\nin the batch , for\neach\nquery in the\nsequence , for\neach head ,\n8\nrepresents\neither\nthe\nbucket\nindex if\nsparsity_mode ==\u2019hash \u2019 or the\nwhether\nto keep\nthat\ngiven\nhead if\nsparsity_mode ==\u2019qk \u2019.\n9\nThe\ntype\nshould\nbe torch.int32 if\nsparsity_mode ==\u2019hash \u2019 and\ntorch.float if\nsparsity_mode ==\u2019qk \u2019.\n10\nk_idx: tensor of shape (BATCH , N_CTX_KV , H) for\neach\nsequence\nin the batch , for\neach\nkey in the\nsequence , for\neach head ,\n11\nrepresents\neither\nthe\nbucket\nindex if\nsparsity_mode ==\u2019hash \u2019 or the\nwhether\nto keep\nthat\ngiven\nhead if\nsparsity_mode ==\u2019qk \u2019.\n12\nThe\ntype\nshould\nbe torch.int32 if\nsparsity_mode ==\u2019hash \u2019 and\ntorch.float if\nsparsity_mode ==\u2019qk\u2019\n13\nsm_scale: normalization\nconstant , 1/ sqrt(D_HEAD) unless\nspecified\n14\nsparsity_mode : \u2019hash \u2019 to select\nthe hash -sparse\nimplementation\nand \u2019qk\u2019 for the\nqk -sparse\nimplementation\n15\n\"\"\"\n16\n17\nif\nsm_scale\nis None:\n18\nsm_scale = 1.0 / math.sqrt(q.size (-1))\n19\n20\nif\nsparsity_mode\n== \u2019hash \u2019:\n21\nreturn\nhash_sparse_attention (q, k, v, q_hash=q_idx , k_hash=k_idx ,\nsm_scale=sm_scale)\n22\nelif\nsparsity_mode\n== \u2019qk\u2019:\n23\nreturn\nqk_sparse_attention (q, k, v, q_keep=q_idx , k_keep=k_idx ,\nsm_scale=sm_scale)\n24\nelse:\n25\nraise\nKeyError(f\"Unknown\nsparsity_mode : \u2019{ sparsity_mode }\u2019, should be in\n[\u2019hash \u2019, \u2019qk \u2019]\")\nPre & post processing steps for Hash-sparse.\nIn addition to having to transpose the Q, K, V\ntensors. The preprocessing steps consist in re-ordering the Q, K and V tensors based on bucket\nindices in q_hash and k_hash. We keep track of the original position of the queries, keys and values\nby storing the indices given by the sorting operations. Importantly, we use stable sorts to ensure\nthe queries, keys and values are sorted within each bucket. The following code is showing how we\nimplemented all those steps using Pytorch:\nListing 3: hash_sparse_attention function showing pre and post processing steps for the Hash-\nsparse algorithm.\n1 def\nhash_sparse_attention (q, k, v, q_hash , k_hash , sm_scale):\n2\nassert\nq_hash.dtype == torch.int32\nand\nk_hash.dtype == torch.int32\n3\n4\nBATCH , N_CTX_Q , H, D_HEAD = q.shape\n5\n17\n6\nq = q.transpose (1, 2) # (BATCH , H, N_CTX_Q , D_HEAD)\n7\nk = k.transpose (1, 2) # (BATCH , H, N_CTX_KV , D_HEAD)\n8\nv = v.transpose (1, 2) # (BATCH , H, N_CTX_KV , D_HEAD)\n9\nq_hash = q_hash.transpose (1, 2). contiguous () # (BATCH , H, N_CTX_Q)\n10\nk_hash = k_hash.transpose (1, 2). contiguous () # (BATCH , H, N_CTX_KV)\n11\n12\n# Re -order\nthe\nqueries ,keys ,values\naccording\nq_hash\nand\nk_hash\n13\nq_hash = q_hash.sort(dim=-1, stable=True) # q_hash.shape = (BATCH , H, N_CTX_Q),\nstable\nsort to keep\ntime\nordering\nwithin a bucket\n14\nk_hash = k_hash.sort(dim=-1, stable=True) # k_hash.shape = (BATCH , H, N_CTX_KV)\n15\n16\nq_idx = q_hash.indices\n17\nk_idx = k_hash.indices\n18\n19\nq_hash = q_hash.values\n20\nk_hash = k_hash.values\n21\n22\nq_idx_extended = q_idx.unsqueeze (-1).expand_as(q)\n23\nk_idx_extended = k_idx.unsqueeze (-1).expand_as(k)\n24\n25\nq = torch.gather(q, dim=-2, index= q_idx_extended ).contiguous ()\n26\nk = torch.gather(k, dim=-2, index= k_idx_extended ).contiguous ()\n27\nv = torch.gather(v, dim=-2, index= k_idx_extended ).contiguous ()\n28\n29\ny = hash_sparse_attention_kernel (q, k, v, q_idx , k_idx , q_hash , k_hash , sm_scale)\n30\ny = torch.zeros ((BATCH , H, N_CTX_Q , D_HEAD), dtype=q.dtype ,\ndevice=q.device).scatter(dim=2, index=q_idx_extended ,\nsrc=y).transpose (1 ,2).contiguous ()\n31\nreturn y\nPre & post processing steps for QK-sparse.\nIn addition to having to transpose the Q, K, V\ntensors. The preprocessing steps consist in removing dropped keys, values and queries from K,\nV and Q. The sorting operations need to be stable to keep the original time ordering within the\nremaining keys and queries. Moreover, the index tensor has to be padded so our kernel can rely on\nthose indices to compute which tile it should and shouldn\u2019t compute.\nListing 4: qk_sparse_attention function showing pre and post processing steps for the QK-sparse\nalgorithm.\n1 def\ncompact(keep_tensor , x, index=None):\n2\n\"\"\"\nBuild a compact\nrepresentation\nof x\n3\nKeyword\narguments:\n4\nx: input\ntensor to compact , x.shape = (BATCH , N_CTX , H, D_HEAD)\n5\nkeep_tensor : float\ntensor of shape (BATCH , N_CTX , H) containing a 1 when\nthe\nhead\nis kept , else 0\n6\n\"\"\"\n7\nBATCH , T, H, D_HEAD = x.shape\n8\nif index is None:\n9\nwith\ntorch.no_grad ():\n10\nindices_per_head = keep_tensor .sum(dim =-2)\n11\nbuffer_size = indices_per_head .max ().int () # first\nsum\ncomputes\nthe num of\nnon -killed\nelem\nper head , we take to max of that\n12\n# sorting: it is very\nimportant\nthat\nthe\nsorting\nis stable , else we cannot\nuse\ncausal\nmasking\n13\nsorted = keep_tensor.sort(dim=-2, descending=True , stable=True) #\nsorted.indices.shape == (BATCH x T x H) , now\nsorted\nover\nsequence T\n14\nindex = sorted.indices [:,: buffer_size ,:] # (BATCH x buffer_size x H) expand\nindices\nto cover\nall the\ndimensions\nfor\neach\nheads\n15\nelse:\n16\nindices_per_head = None\n17\ncompact_x = x.gather(dim=-3, index=index.unsqueeze (-1).expand (-1,-1,-1, D_HEAD)) #\n(BATCH x buffer_size x H x D_HEAD) / expand\nindices\nto cover\nall the\ndimensions\nfor\neach\nheads\n18\nreturn\ncompact_x , index , indices_per_head\n19\n20\n21 @torch.no_grad ()\n22 def\npad_index(index , indices_per_head , pad_idx =-1):\n23\n\"\"\" Pad the\nindex\ntensor to comply\nwith\nthe kernel , returns a copy.\n24\nKeyword\narguments:\n25\nindex: original\nindex\ntensor to pad\ngiven by \u2018compact \u2018, index.shape = (BATCH ,\nbuffer_size , H). For\neach\nbatch\nand\ntimestep , reprsents\nthe\nhead\nidx it\u2019s\noriginating\nfrom.\n26\nindices_per_head : of shape (BATCH , H), for\neach head , contains\nhow\nmany\nindices\nhave\nnot\nbeen\ndropped.\n27\n\"\"\"\n18\n28\nBATCH , buffer_size , H = index.shape\n29\nindex_copy = torch.clone(index).type(torch.int32)\n30\nmask = torch.arange(buffer_size ,\ndevice=index.device).view (1,-1,1).expand(BATCH ,buffer_size ,H) >=\nindices_per_head .view(BATCH ,1,-1)\n31\nindex_copy[mask] = pad_idx\n32\nreturn\nindex_copy\n33\n34\n35 def\nqk_sparse_attention (q, k, v, q_keep , k_keep , sm_scale):\n36\nassert\nq_keep.dtype == torch.float\nand\nk_keep.dtype == torch.float\n37\n38\nBATCH , N_CTX_Q , H, D_HEAD = q.shape\n39\n40\n# Building\ncompact\nrepresentations\n41\nq_c , q_idx , iph_q = compact(q_keep , q) # q_c.shape = (BATCH , compact_N_CTX_Q , H)\n42\nk_c , k_idx , iph_k = compact(k_keep , k) # k_c.shape = (BATCH , compact_N_CTX_KV , H)\n43\nv_c , _, _ = compact(k_keep , v, index=k_idx) # v_c.shape = (BATCH ,\ncompact_N_CTX_KV , H)\n44\nq_idx_padded = pad_index(q_idx , iph_q , pad_idx =-1) # (B, compact_N_CTX_Q , H)\n45\nk_idx_padded = pad_index(k_idx , iph_k , pad_idx =1e9) # (B, compact_N_CTX_KV , H)\n46\n47\n# We need to\ntranspose\neverything\n48\nq_c = q_c.transpose (1, 2).contiguous () # (BATCH , H, compact_N_CTX_Q , D_HEAD)\n49\nk_c = k_c.transpose (1, 2).contiguous () # (BATCH , H, compact_N_CTX_KV , D_HEAD)\n50\nv_c = v_c.transpose (1, 2).contiguous () # (BATCH , H, compact_N_CTX_KV , D_HEAD)\n51\nq_idx_padded = q_idx_padded .transpose (1, 2). contiguous () # (BATCH , H,\ncompact_N_CTX_Q )\n52\nk_idx_padded = k_idx_padded .transpose (1, 2). contiguous () # (BATCH , H,\ncompact_N_CTX_KV )\n53\n54\ny_c = qk_sparse_attention_kernel (q_c , k_c , v_c , q_idx_padded , k_idx_padded ,\nsm_scale).transpose (1 ,2)\n55\ny = torch.zeros_like(q).scatter(dim=1,\nindex=q_idx.long ().view(BATCH ,-1,H ,1).expand(BATCH , -1, H, D_HEAD), src=y_c)\n56\nreturn y\nB.3\nSequence Modeling Experiments\nLanguage modeling on OpenWebText2.\nOur implementation is based on NanoGPT\n(github.com/karpathy/nanoGPT). We use the AdamW optimizer (Loshchilov & Hutter, 2019). We\nused bfloat16 and NVIDIA A100-40GB GPUs for all our experiments. Here is a list of hyperpa-\nrameters shared by all our language models (F-LM, H-LM, and D-LM):\n\u2022 Weight-decay: 0.1\n\u2022 Depth (number of transformer blocks): 12\n\u2022 Number of heads: 12\n\u2022 Dropout: 0.0\n\u2022 Learning rate: 0.001\n\u2022 Percentage of iterations for warmup: 2%. We use a cosine learning rate scheduler.\n\u2022 Adam beta1: 0.9\n\u2022 Adam beta2: 0.95\n\u2022 Tokenizer:\nWe\nuse\nthe\nGPT2\ntokenizer\nprovided\nby\nthe\ntiktoken\nlibrary\n(github.com/openai/tiktoken).\n\u2022 Hidden dimensions: 768\n\u2022 Dimensions per head: 64\nSequential MNIST and enwik8.\nFor the comparisons with Reformer, we use a standard GPT2\n(Radford et al., 2019) implementation. For the language modeling on enwik8, the Transformer has\n12 blocks with 768 hidden dimensions, 8 attention heads, and 64 dimensions per head. Dropout is set\nto 0.1 and the batch size is 8 with 2 gradient accumulation steps. The sequence length is 4096. For\nautoregressive image generation on MNIST, we use a smaller model with 8 transformer blocks and a\nhidden dimension of 256. Dropout is set to 0.1 and the batch size is 10. We train for 25 epochs.\n19\nC\nAdditional Details and Analysis\nQuadratic computational cost of attention in transformers.\nIn Fig. 9 we show the runtime\n(forward + backward) of a transformer language model as a function of the sequence length. We\nseparate the time taken by the attention operations from the time taken by the rest of the model. We\nsee how the attention computation ends up dominating the runtime as the sequence length increases.\n0\n5000\n10000\n15000\nSequence length\n0\n100\n200\n300\n400\n500\n600\nms\nAttention\nFeed-forward\nFigure 9:\nQuadratic computational cost of self attention dominates for larger sequences.\nWe measure the forward and backward runtimes for a 12 layers transformer (see App. B.3 for\nimplementation details). Except for the attention operations, the rest of the transformer runtime grows\nlinearly with the sequence length.\nAdditional runtimes performances in a vacuum.\nIn Fig. 10 and Fig. 11 we show the runtime\ndetails for the forward and backward methods separately for respectively the Hash-sparse and QK-\nsparse methods. We also measure runtimes of the forward and backward passes when we assume the\npre and post-processing steps aree free, see Fig. 12.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nSequence length\n0\n50\n100\n150\n200\n250\n300\n350\n400\nms\nnb = 2\nnb = 4\nnb = 8\nnb = 16\nnb = 32\nnb = 64\nHash-sparse (our method)\nFlash attention\n0\n2000\n4000\n0\n2\n4\n6\n8\n10\n(a) Forward\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nSequence length\n0\n100\n200\n300\n400\n500\n600\n700\n800\nms\nnb = 2\nnb = 4\nnb = 8\nnb = 16\nnb = 32\nnb = 64\nHash-sparse (ours)\nFlash attention\n0\n2000\n4000\n0\n5\n10\n15\n20\n(b) Backward\nFigure 10: Forward and Backward runtimes in a vacuum for our Hash-sparse method. Fig.(a)\nForward runtimes for our Hash-sparse method. Fig.(b): Backward runtimes for our Hash-sparse\nmethod.\nLinear QK-dropping scheduler.\nIn the main paper we show results dropping keys and queries at\nrandom with a fixed pre-defined probability. In an additional experiment we start by dropping 80%\nof keys and queries at random and linearly decay this probability to 20%. Our intuition is earlier\niterations just aim to learn contextual cues which are very redundant (and therefore quite immune\nto random dropping) before requiring more fine-grained representations. In Fig. 13 we show and\nanalyse the results of that experiment.\nH-LM models speeding up during training.\nThe speed of Hash-sparse attention is conditioned on\nthe distribution of bucket indices over keys and queries\u2014e.g. if all the keys and queries were to fall in\n20\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nSequence length\n0\n50\n100\n150\n200\n250\n300\n350\n400\nms\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n0\n2000\n4000\n0\n2\n4\n6\n8\n10\n(a) Forward\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nSequence length\n0\n100\n200\n300\n400\n500\n600\n700\n800\nms\n20%\n30%\n40%\n50%\n60%\n70%\n80%\nQK-sparse (ours)\nFlash attention\n0\n2000\n4000\n0\n5\n10\n15\n20\n(b) Backward\nFigure 11: Forward and Backward runtimes in a vacuum for our QK-sparse method. Fig.(a)\nForward runtimes for our QK-sparse method. Fig.(b): Backward runtimes for our QK-sparse method.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nSequence length\n0\n200\n400\n600\n800\n1000\nms\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n0\n2000\n4000\n0\n5\n10\n15\n20\n25\n(a) QK-sparse\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nSequence length\n0\n200\n400\n600\n800\n1000\n1200\nms\nnb = 2\nnb = 4\nnb = 8\nnb = 16\nnb = 32\nnb = 64\n0\n2000\n4000\n0\n10\n20\n(b) Hash-sparse\nFigure 12: Forward+Backward runtimes in a vacuum when we assume the preprocessing is free.\nFig.(a) Forward+backward runtimes for our QK-sparse method when the pre and postproceessing\nsteps are free. While the runtimes for large sequences stays relatively the same compared to Fig. 7,\nthe offset for short sequences is now much smaller. Fig.(b): Forward+backward runtimes for our\nHash-sparse method when the pre and postproceessing steps are free. While the runtimes for large\nsequences stays relatively the same compared to Fig. 3, the offset for short sequences is now smaller.\nIntrestingly, unlike for the QK-sparse method, the offset for smaller sequences stays significant. We\nbelieve this might be due to the influence of the block size used (128): when the sequence length\nis not large enough in comparison to the block size, the block structure of the hash-sparse attention\nmatrix cannot be efficiently leveraged and the number of tiles processed by the Hash-sparse method\nis larger than that of the FlashAttention method. In contrast, for the QK-sparse method the good tiles\nall start from 0 (we know the first tile(s) will be efficiently packed).\n21\n0\n5000\n10000\n15000\nIterations\n20\n30\n40\n50\n60\n70\nValidation Perplexity\n(a) Iter to reach perplexity\n0\n20\n40\n60\nTime (hours)\n20\n30\n40\n50\n60\n70\nValidation Perplexity\n(b) Time to reach perplexity\n0\n5\n10\n15\nIter/min on 1 GPU\n\u00d71.9\n\u00d71.5\n\u00d72.6\n\u00d73.5\nF-LM\nD-LM s = 30%\nD-LM s = 80% 7\u2192 20%\nD-LM s = 50%\nD-LM s = 70%\n(c) Speed gains\nFigure 13:\nTraining Language Models (LM) on OpenWebText2 using random Query/Key\ndropping based sparsity (D-LM) or FlashAttention over the entire sequence (F-LM). Results are\nthe same as in Fig 8 except for the addition of the D-LM model with linear decay of s from 80% to\n20%. In Fig.(b) and Fig.(c), we observe the additional D-LM model in yellow is slower than other\nD-LM models we experimented with, but, as seen in Fig.(a), reaches a slightly better perplexity.\nthe same bucket then there would be no speedup over FlashAttention. Interestingly, when training on\nreal data such as OpenWebText2, we observe our Hash-sparse based models are speeding up during\ntraining. In Fig. 14 we plot the number of iterations reached after x hours of training (normalizing\nthe time by the number of GPUs used for training). In Fig. 14.(a) we see a speedup for H-LM models\nearly in the training.\n0\n20\n40\n60\nTime (hours on 1 GPU)\n0\n5000\n10000\n15000\nIterations\nF-LM\nH-LM nb = 8\nH-LM nb = 16\nH-LM nb = 32\n(a) Time comparison H-LM vs. F-LM\n0\n20\n40\n60\nTime (hours on 1 GPU)\n0\n5000\n10000\n15000\nIterations\nF-LM\nD-LM s = 30%\nD-LM s = 80% 7\u2192 20%\nD-LM s = 70%\nD-LM s = 50%\n(b) Time comparison D-LM vs. F-LM\nFigure 14: Training speed as a function of time for H-LM, D-LM and F-LM. All models trained\non sequences of 8192 tokens. In Fig.(b) we observe D-LM models having a fixed speed throughout\nthe experiments except for the one corresponding to the linear decaying of the sparsity ratio s which\nslows down during training. In Fig.(a) H-LM models are speeding up early during training and then\nseem to keep a constant speed.\nDifferent bucket sizes nb for H-LM.\nFor sequences of 8192 tokens, we show the influence of\nincreasing the number of buckets nb in Fig. 15. As nb increases the runtime decreases.\nTraining H-LMs for more iterations.\nIn Fig. 8 and Fig. 6 we show results of language models\ntrained on OpenWebText2 for 15k iterations. To verify whether our finding are consistent when\nyou train for more iterations we also try training for 50k iterations. For sequences of 8192 tokens,\nusing the same hyperparameters described in App. B.3, we show in Fig. 16 that our findings for\nour Hash-sparse based models do hold when training for more iterations\u2014we match the perplexity\n22\nT=8192\nT = 16384\n0\n2\n4\n6\n8\n10\nIter/min on 1 GPU\n\u00d71.8\n\u00d72.0 \u00d72.1\n\u00d72.6\n\u00d73.3 \u00d73.6\nF-LM\nH-LM nb = 16\nH-LM nb = 8\nH-LM nb = 32\nFigure 15: Influence of the number of buckets nb on H-LM training speed for T = 8192 and\nT = 16384. In accordance with Fig. 3 increasing nb speeds up the training but the speed gain from\ndoubling nb decreases as nb is increasing.\nper iterations of the baseline model using FlashAttention over the entire sequence while being\nsignificantly faster.\n10000\n15000\n20000\n25000\n30000\n35000\n40000\n45000\n50000\nIterations\n18\n19\n20\n21\n22\n23\n24\n25\n26\nValidation Perplexity\nF-LM\nH-LM nb = 16\nH-LM nb = 8\n(a) Iter to reach perplexity\n0\n2\n4\n6\n8\n10\nIter/min on 1 GPU\n\u00d71.9\n\u00d72.1\nF-LM\nH-LM nb = 16\nH-LM nb = 8\n(b) Speed gains\nFigure 16: Comparison between H-LM and F-LM when training for 50k iterations on sequences\nof 8192 tokens. Those curves are averaged over two seeds. In Fig.(a) we zoom in on the perplexity\nper iteration for the baseline F-LM and two of our H-LM models using our Hash-sparse attention\nwith 8 and 16 buckets. We observe that our methods are matching the perplexity of the F-LM model\nwhich uses FlashAttention over the entire sequence. Fig.(b): We match the perplexity but gain in\nspeed, our methods are 1.4\u00d7 and 1.8\u00d7 faster than the baseline for respectively 8 and 16 buckets.\nVisualizing the low coverage of Reformer.\nIn Fig. 17 we show the attention matrices for two\ndifferent heads and show how the coverage\u2014the percentage of key-query interactions actually\ncomputed vs. what should be computed according to the hash indices computed for keys and\nqueries\u2014of the Reformer LSH algorithm can be low.\nD\nLimitations and Societal Impacts\nLimitations.\nThe aim of our work is to develop a method for efficiently computing attention with\nseveral sparsity structures. We don\u2019t focus on developing the best method for sparsification, although,\nfor example, we improve the hashing-based mechanism. Moreover, as already explained, on very\nsmall sequences we incur in some constant overhead which limits our gains.\nSocietal impacts.\nThe attention mechanism is central to Large Language Models (LLMs). More-\nover, efficient attention mechanisms can make these models more powerful, by making them faster to\ntrain and by increasing their context length. The social impact and risks associated with our work,\ntherefore, are included in the risks associated with the deployment of such systems (Bender et al.,\n2021; Weidinger et al., 2021).\n23\nFigure 17: Attention pattern of Reformer. The two images show the attention pattern for the\nReformer LSH attention after reordering queries and keys according to the bucket. The red squares\nare the positions for which attention is computed in the Reformer with bucket size = 32. The back\nsquares are positions for which the queries and keys map to the same hash bucket. Queries are\nsampled from a normal distribution with \u00b5 = 3 and \u221a\u03c3 = 5. Each image refers to a different\nattention head. Black regions which are not covered by red tiles are key-query interactions which\nshould be computed but are missed by the Reformer LSH attention.\n24\n"
  },
  {
    "title": "Reimagining Retrieval Augmented Language Models for Answering Queries",
    "link": "https://arxiv.org/pdf/2306.01061.pdf",
    "upvote": "1",
    "text": "Reimagining Retrieval Augmented Language Models\nfor Answering Queries\n[Reality Check Theme Track]\nWang-Chiew Tan Yuliang Li Pedro Rodriguez\nRichard James* Xi Victoria Lin Alon Halevy Scott Yih\nMeta\n{wangchiew,yuliangli,victorialin,ayh,scottyih}@meta.com\nrich@richjames.co*\nAbstract\nWe present a reality check on large language\nmodels and inspect the promise of retrieval-\naugmented language models in comparison.\nSuch language models are semi-parametric,\nwhere models integrate model parameters and\nknowledge from external data sources to make\ntheir predictions, as opposed to the paramet-\nric nature of vanilla large language models.\nWe give initial experimental findings that semi-\nparametric architectures can be enhanced with\nviews, a query analyzer/planner, and prove-\nnance to make a significantly more powerful\nsystem for question answering in terms of ac-\ncuracy and efficiency, and potentially for other\nNLP tasks.\n1\nIntroduction\nAs language models have grown larger (Kaplan\net al., 2020; Hoffmann et al., 2022), they have\nfared better and better on question answering\ntasks (Hendrycks et al., 2021) and have become\nthe foundation of impressive demos like Chat-\nGPT (Ouyang et al., 2022; ChatGPT3-OpenAI).\nModels like GPT-3 (Brown et al., 2020) and Chat-\nGPT generate fluent, human-like text, which comes\nthe potential for misuse as in high-stakes health-\ncare settings (Dinan et al., 2021). Large language\nmodels (LLMs) also come with several significant\nissues (Hoffmann et al., 2022; Bender et al., 2021).\nLLMs are costly to train, deploy, and maintain,\nboth financially and in terms of environmental im-\npact (Bender et al., 2021). These models are also\nalmost always the exclusive game of industrial com-\npanies with large budgets. Perhaps most impor-\ntantly, the ability of LLMs to make predictions is\nnot commensurate with their ability to obtain in-\nsights about their predictions. Such models can\nbe prompted to generate false statements (Wallace\net al., 2019a), often do so unprompted (Asai et al.,\n2022) and when combined with its ability to easily\nfool humans, can lead to misuse (Macaulay, 2020).\nIn recent years, we have seen the promise of\nretrieval-augmented language models partially ad-\ndressing the aforementioned shortcomings (Guu\net al., 2020; Lewis et al., 2020; Borgeaud et al.,\n2021; Izacard et al., 2022; Yasunaga et al., 2022a).\nThe architecture of such models is semi-parametric,\nwhere the model integrates model parameters and\nknowledge from external data sources to make its\npredictions. The first step of performing a task\nin these architectures is to retrieve relevant knowl-\nedge from the external sources, and then perform\nfiner-grained reasoning. Some of the benefits these\narchitectures offer are that the external sources can\nbe verified and updated easily, thereby reducing\nhallucinations (Shuster et al., 2021a) and making\nit easy to incorporate new knowledge and correct\nexisting knowledge without needing to retrain the\nentire model (Lewis et al., 2020). Models that fol-\nlow semi-parametric architectures (SPA) are typi-\ncally smaller than LLMs and they have been shown\nto outperform LLMs on several NLP tasks such as\nopen domain question answering (see Table 1). Re-\ncent work that extends LLMs with modular reason-\ning and knowledge retrieval (Karpas et al., 2022;\nLangChain) is also a type of SPA.\nIn this paper we argue that building on the core\nideas of SPA, we can potentially construct much\nmore powerful question answering systems that\nalso provide access to multi-modal data such as\nimage, video and tabular data. We describe POST-\nTEXT, a class of systems that extend SPA in three\nimportant ways. First, POSTTEXT allows the ex-\nternal data to include views, a concept we borrow\nfrom database systems (Garcia-Molina et al., 2008).\nA view is a function over a number of data sources,\nV = f(D1, ..., Dn). In databases, SQL queries\nare used to define tabular views. For example, V\ncan be a table of records of minors that is derived\nfrom a table of person records by selecting only\nthose with age<18. In general, however, views\nneed not be tabular. When a view is materialized\narXiv:2306.01061v1  [cs.CL]  1 Jun 2023\nModel\n#Params\nOutperformed LLM\u2019s sizes\nTasks\nREALM (Guu et al., 2020)\n330M\n11B (T5)\nOpen-QA\nRETRO (Borgeaud et al., 2021)\n7.5B\n178B (Jurassic-1), 280B (Gopher)\nLanguage modeling\nAtlas (Izacard et al., 2022)\n11B\n175B (GPT-3), 540B (PaLM)\nMulti-task NLU, Open-QA\nRAG (Lewis et al., 2020)\n400M\n11B (T5)\nOpen-QA\nFiD (Izacard and Grave, 2021)\n770M\n11B (T5), 175B (GPT-3)\nOpen-QA\nTable 1: The sizes of SPA models with those of comparable or outperformed LLMs.\n(i.e., executed and stored), it may be useful for an-\nswering certain queries1 more effectively. In this\npaper, we adopt a more general notion of views,\nnot limited to results of SQL queries, which can\n(compositionally) support a variety of user ques-\ntions. Views are particularly important to support\nmulti-modal data, because combinations of data\nfrom multiple modalities can be modeled as views.\nSecond, POSTTEXT contains a question analyzer\nand planner module that decides on the best strat-\negy to answer a question that may involve first\nanswering multiple subquestions in sequence or in\nparallel. This module bears similarity to query op-\ntimization techniques in database systems but will\ngo significantly beyond the techniques established\nin database systems since, there are multiple dif-\nferent ways to answer a natural language question,\nespecially with the availability of multi-modal data.\nFinally, POSTTEXT supports computing the prove-\nnance of answers to questions. The provenance-\naware answer generator module can track the ev-\nidence (training data or external sources) that is\nused for the answers, even if views are used as\nintermediate results.\nWe illustrate the power of POSTTEXT with ex-\namples in the next section and also the overview\nof its architecture. In the remaining sections, we\ndescribe the different components of POSTTEXT.\nadd some description of experiments\n2\nOverview of PostText\nExample 1 Consider a setting where we answer\nquestions over data that includes images of dishes\nand text with restaurant reviews. We can create a\nview that aligns these two data sets so we can an-\nswer more complex queries readily. The view, the\ntable in the middle of Figure 1, aligns dishes with\nrelevant reviews and the corresponding restaurants.\nNote that creating this view involves an intermedi-\nate step of identifying the name of the dish in an\nimage. The view also stores the provenance links\n1We use queries and questions interchangeably.\nto the actual reviews from which the snippets were\nextracted. There are also provenance links for the\nimages and the name of the dish (not shown).\nThis view can be used to answer questions that\nwould be more difficult without it. For example, if\na person recalls a nice dish she had in the past but\ndoes not remember its name and is trying to figure\nout which restaurants serve the same dish and what\nare the reviews, she can pose the question, which\nincludes both the question in text and an image of\nthe dish. The answer states the name of the dish\nin question and lists restaurants with top reviews\nfor that dish, along with images of the dish and\nsnippets of those reviews and their provenance.\nExample 2 The same view can also be used to an-\nswer the question \u201chow many reviews raved about\nShaking beef?\u201d. The answer requires counting the\nnumber of reviews that are synonymous to very\npositive reviews about Shaking beef. The view\nsurfaces the reviews associated with Shaking beef\nimmediately and alleviates the amount of work that\nis required to compute the answer otherwise.\nThe examples show that some questions can\nbe answered more easily if they are supported by\nviews that surface useful associations between data.\nIn fact, indices are a type of views to accelerate\nlookups between an item and its attributes. In\ndatabase systems, views have been used extensively\nto enable more efficient query answering (Halevy,\n2001; Goldstein and Larson, 2001) with significant\nwork on automatically materializing a set of indices\nfor efficient query answering (Jindal et al., 2018;\nDas et al., 2019). A set of views and indices are\ndefined automatically or manually in anticipation\nof a set of frequently asked queries under a budget\nconstraint, e.g., space, so that during runtime, most\nof the incoming queries can be answered immedi-\nately or after applying simple operations over the\nviews. Otherwise, the system falls back to answer-\ning the queries using the actual data sources. In\nother words, POSTTEXT prefers to use views to\nanswer the questions, which will likely to be more\nefficient and accurate in general but otherwise, the\nFigure 1: Multimodal question with multimodal answer. The view (middle) associates the dishes with its corre-\nsponding review snippets and images. The provenance links show where the snippets are extracted from. There are\nalso provenance links for the images and name of the dish (not shown).\nsystem falls back to the traditional question answer-\ning strategy. In addition to query answering, views\nhave also been used to define content-based access\ncontrol (Bertino and Sandhu, 2005), i.e., which\nparts of the data are accessible and by whom.\nThe examples also show how provenance is pro-\nvided as part of the answer. In these examples, it\nhappened that provenance was easily determined\nthrough the provenance links that are already cap-\ntured in the views. If actual data sources are ac-\ncessed, the links to the data sources used (e.g.,\nspans of text documents, parts of images, segments\nof videos) to derive the answer are provided as part\nof the answer. If the answer is generated by the lan-\nguage model, we trace how POSTTEXT derives the\nanswer from parametric knowledge and retrieved\ndata through analyzing its weights or determining\n\u201cinfluential\u201d parametric knowledge (Section 6) sim-\nilarly to (Aky\u00fcrek et al., 2022).\nPostText architecture POSTTEXT enhances the\ncore architecture of semi-parametric models with\nthree components: views, a query analyzer & plan-\nner (QAP), and a provenance-aware answer gener-\nator (PAG). In addition, all components including\nthe \u201ctraditional\u201d knowledge retrievers are equipped\nto manage both structured and unstructured data of\ndifferent modalities.\nFigure 2 shows the architecture of POSTTEXT.\nViews are synthesized from different types of ex-\nternal data sources (e.g., text, images, videos, and\ntabular data), which can be public or private. When\na question is posed in natural language (NL), the\nQAP module interprets and decomposes the ques-\ntion into subquestions whose answers can be com-\nposed to obtain an answer to the input question.\nQAP coordinates with the knowledge retriever to\nderive the data needed to answer these questions. It\nalso coordinates with the PAG module with its plan\nso that provenance-aware answers can be returned.\nAdding these components raises interesting chal-\nlenges such as what views should we construct and\nhow do we construct and maintain these views auto-\nmatically as data sources changes? What is a good\nplan for deriving an answer and how do we choose\namong alternative plans? And how do we measure\nthe \u201cgoodness\u201d of an answer with provenance?\nIn the remaining sections, we describe the chal-\nlenges associated with each of these components\n3\nData Sources and Views\nData Sources Most existing work on retrieval\naugmented language models are focused on text.\nMore recently, (Chen et al., 2022; Yasunaga et al.,\n2022b; Sheynin et al., 2022) has applied SPA mod-\nels on image-text and text-only corpus. The data\nsources in POSTTEXT are multi-modal, unstruc-\ntured or structured. They can be external public\ndata sources or private ones.\nViews Views are results computed (not necessarily\nFigure 2: Semi-parametric architectures enhanced with views, a query analyzer & planner module, and a provenance-\naware answer generator. The data sources may be public or private.\nthrough SQL queries) from data sources or other\nviews. For example, a view can be a document\ninvolving data of different modalities (e.g., an im-\nage or a table). Views are powerful constructs for\nsurfacing important and useful associations that are\nnot obvious otherwise, whether they are associa-\ntions from data within one data source or across\nmultiple data sources. The table in Figure 1 is a\nview over restaurant reviews from Yelp, Google,\nand images provided by restaurants. This view\nmakes it easier to compute the number of reviews\nassociated with each dish in each restaurant or even\nacross all restaurants. This view also makes it eas-\nier to determine the answer as to which dishes has\nmore reviews than Shaking beef at Tamarine.\nIndexes are a special type of views. They as-\nsociate an item with its attribute. Several imple-\nmentations of retrieval augmented language mod-\nels (Guu et al., 2020; Lewis et al., 2020; Izacard\net al., 2022) already construct indices that associate\na document with its nearest neighbors. Recently,\nGPT-index (GPT-Index, 2022) developed a set of\nAPIs for creating data structures that can be tra-\nversed using LLMs to answer queries. The data\nstructures are structured indexes and can be used\nto determine an answer to a question.\nRelational views are extensively used in data\nwarehouses for optimizing queries. Indexes and\nviews are typically created by users or database\nadministrators or they can be automatically se-\nlected (Agrawal et al., 2000; Schnaitter et al., 2007;\nJindal et al., 2018) and tuned (Agrawal et al., 2006;\nBruno and Chaudhuri, 2008) to efficiently answer\nqueries of a given workload (Das et al., 2019),\nwhich are queries that are anticipated to be fre-\nquently occurring. In typical settings, a set of views\nare constructed, usually under a budget constraint\nsuch as space, to maximize the queries that can be\nanswered (either directly or through applying a few\nsimple operators on the views) in a given workload.\nWhen a new query arrives after the views are con-\nstructed, the query optimizer determines the best\nplan to adopt for computing the answer. Queries\nare directly executed over the views if possible.\nOtherwise, it falls back to old strategy of answer-\ning the query with the data sources. For example,\nearly last year, in anticipation of frequent queries\nabout statistics of past World Cups due to the World\nCup 2022 event at the end of the year, a set of views\nabout the different World Cup statistics could have\nbeen constructed a priori so that most World Cup\nrelated questions can be directly answered using\nthe views.\nWe hypothesize that views in POSTTEXT can\nbring similar benefits to question answering. The\nright views will make it easier for the QAP mod-\nule and the knowledge retriever to discover and\nobtain relevant data and subsequently for the an-\nswer generator to derive the right answers. Existing\nSPAs (Guu et al., 2020; Lewis et al., 2020; Izacard\net al., 2022) are already leveraging dense-vector in-\ndices to accelerate the retrieval of document spans.\nIn POSTTEXT with views being available, it is a nat-\nural extension to annotate each view with a descrip-\ntion of its content (e.g., \u201cRestaurants and highly\nranked dishes\u201d), which would make it even easier\nfor the knowledge retriever to find the relevant data.\nThe core challenges in developing views are how\ndo we determine what is a \u201cright\u201d set of views to\nmaterialize automatically or semi-automatically?\nHow do we incrementally maintain such views as\ndata sources are updated? These problems are ex-\ntensively studied in the database community and\nit will be interesting to explore those ideas that\ntransfer to the POSTTEXT.\nThe architecture can also be instrumented in\nsuch a way that views are the only sources of data\nfor the knowledge retriever (i.e., actual data sources\nare excluded). Hence, in this case, views act as a\ngateway that define which parts of the data sources\nare accessible by the knowledge retriever to answer\nqueries. Finer-grained access control can also be in-\nstrumented through views as described in (Bertino\nand Sandhu, 2005). With views, it is also possible\nto enable a finer-grained public-private autoregres-\nsive information retrieval privacy system (Arora\net al., 2022).\n4\nQuestion Analyzer & Planner\nThe question analyzer and planner (QAP) module\nexamines the input question and generates a plan,\ni.e., a sequence of sub-questions whose answers\ncan be combined to form an answer to the input\nquestion. For each subquestion in the plan, QAP\nfirst checks whether external knowledge is needed.\nIf not, the language model can be used to derive\nthe answer. Otherwise, the subquestion is passed\nto the knowledge retriever to discover and retrieve\nrelevant data for the subquestion at hand. The re-\nsults from the knowledge retriever and the plan\nare passed to PAG (i.e., the rightmost green box\nin Figure 2). It is still an open and challenging\nquestion to determine whether a language model\ncan confidently answer a question (Kamath et al.,\n2020; Si et al., 2022). Any solution to this problem\nwill help improve the plan generator.\nAn example plan from the QAP module for our\nrunning example is as follows: (1) find the name of\nthe dish X in the input image, (2) find restaurants\nthat serve X, (3) find the top restaurant among\nthe results from (2). This plan is viable because\n(a) there is an index associating embeddings of\nimages with the name of the main entity of the im-\nage, (b) there exists a view as shown in Figure 1,\nwhich supports the search for restaurants that serve\na particular dish. Top answers can be derived by\ncomputing the scores of the reviews or approxi-\nmating it based on the sentiment of the reviews\nand then ranking the results based on such scores.\nThe information from (2) is passed to PAG which\nwill compute the answer along with its provenance.\nThis plan is based on the heuristic to push selection\nconditions early before joining/combining different\ndata sources if needed. The conditions in the ques-\ntion are \u201cgood version\u201d and \u201cthis dish\u201d. In this case,\nno joins are required as the view already combines\nthe required information in one place. Hence, QAP\nseeks to first find the name of the dish to narrow\ndown the reviews restricted to this dish. Alterna-\ntively, it could also retrieve all good reviews before\nconditioning on the name of the dish. Yet another\nplan could be to match the image directly to the im-\nages of the view to find the top reviews. Or, it may\ndecide to directly retrieve only top reviews with\nimages similar to the image in the question from\nthe external data sources and condition the answer\nbased on the name of the restaurant mentioned in\nthe reviews.\nIn all possible plans, the knowledge retriever is\nresponsible for discovering and retrieving the rele-\nvant data for the QAP plan. In addition to the logic\nthat may be needed for decomposing the question\ninto subquestions, a plan is also needed for compos-\ning the subanswers obtained to form an answer to\nthe input question. The plan is shared with the PAG\nmodule for deriving the associated provenance.\nA fundamental challenge in developing the QAP\nmodule is how to derive candidate plans and decide\nwhat is the \u201cbest\u201d plan for answering the ques-\ntion when there are different ways to obtain an\nanswer. Achieving this requires understanding how\nto compare amongst alternative plans for deriving\nan answer to the question. This problem bears sim-\nilarity to query evaluation techniques for database\nsystems (e.g., (Graefe, 1993)). It will be interest-\ning to investigate whether database query planning\ntechniques and ideas can synergize with question\nunderstanding and planning techniques (e.g., (Wolf-\nson et al., 2020; Dunietz et al., 2020; Zhao et al.,\n2021; Xiong et al., 2021) to develop a comprehen-\nsive query planner. Emerging work such as chain\nof thought reasoning (Wei et al., 2022), where a\nsequence of prompts are engineered to elicit better\nanswers, ReAct (Yao et al., 2022), where reasoning\nand action techniques are applied for deriving an\nanswer, and more recently, work that generates a\nplan which can call LMs for resolving subques-\ntions (Cheng et al., 2022) are also relevant. These\ntechniques so far are restricted to text and does not\ncompare among different plans.\nAnother challenge in the context of NL ques-\ntions is that while there is a single correct answer\nto an SQL query over a database, there are po-\ntentially many different correct answers to a NL\nquestion (Si et al., 2021; Min et al., 2020; Chen\net al., 2020). Hence the space of possible plans to\nderive the \u201cbest\u201d answer most efficiently is even\nmore challenging in this case.\nWe are advocating for a system that can rea-\nson and compare at least some viable strategies to\narrive at a best plan for deriving a good answer effi-\nciently. Naturally, one can also train a LM to create\na plan. Our belief is that taking a more systematic\nroute to planning can relief the need for the amount\nof training data required and will also aid prove-\nnance generation through its ability to describe the\nsteps it took and the sources of data used in each\nstep to generate an answer. As we shall explain\nin Section 5, the cost and accuracy of knowledge\nretrievers can also play a role in determining what\nis a better strategy for computing a good answer.\n5\nKnowledge Retriever\nThe role of the knowledge retriever is to provide\nthe information that the system lacks in order to\nfulfill the given task, typically at the inference time.\nMore importantly, we envision that the knowledge\nretriever proposed in our framework has the abil-\nity to access knowledge stored in different sources\nand modalities, retrieve and integrate the relevant\npieces of information, and present the output in a\ntabular data view. The structured output contains\nraw data items (e.g., text documents, images or\nvideos) and and optionally different metadata, such\nas textual description of each data item. Such struc-\ntured output allows downstream (neural) models to\nconsume the retrieved knowledge efficiently and\nalso allows developers and users to validate the\nprovenance conveniently. Existing information re-\ntrieval models mostly focus on a single form of data.\nBelow we first describe briefly how knowledge re-\ntrieval is done for unstructured and structured data.\nWe then discuss the technical challenges for build-\ning a unified knowledge retriever, as well as recent\nresearch efforts towards this direction.\nRetrievers for unstructured data\nFor unstruc-\ntured data, such as a large collection of documents\n(i.e., text corpus) or images, knowledge retrieval\nis often reduced to a simple similarity search prob-\nlem, where both queries and data in the knowledge\nsource are represented as vectors in the same vec-\ntor space (Turney and Pantel, 2010). Data points\nthat are close to the query are considered as rele-\nvant and thus returned as the knowledge requested.\nTraditional information retrieval methods, whether\nrelying on sparse vector representations, such as\nTFIDF (Salton et al., 1975) and BM25 (Robert-\nson et al., 2009), or dense representations, such\nas LSA (Deerwester et al., 1990), DSSM (Huang\net al., 2013), DPR (Karpukhin et al., 2020), are the\ncanonical examples of this paradigm. Notice that\nthe vector space model is not restricted to text but\nis also applicable to problems in other modalities,\nsuch as image tagging (Weston et al., 2011) and\nimage retrieval (Gordo et al., 2016).\nRetrievers for structured data\nWhen the knowl-\nedge source is semi-structured (e.g., tables) or struc-\ntured (e.g., databases), the query can be structured\nand allows the information need to be defined in\na more precise way. Because the data is typically\nstored in a highly optimized management system\nand sometimes only accessible through a set of\npredefined API calls, the key technical challenge\nin the knowledge retriever is to formulate the in-\nformation need into a formal, structured query.\nTo map natural language questions to structured\nqueries, semantic parsing is the key technical com-\nponent for building a knowledge retriever for struc-\ntured data. Some early works propose mapping the\nnatural language questions to a generic meaning\nrepresentation, which is later translated to the for-\nmal language used by the target knowledge base\nthrough ontology matching (Kwiatkowski et al.,\n2013; Berant et al., 2013). Others advocate that\nthe meaning representation should be closely tight\nto the target formal language (Yih et al., 2015),\nsuch as SPARQL for triple stores. Because of\nthe success of deep learning, especially the large\npre-trained language models, semantic parsing has\nmostly been reduced to a sequence generation prob-\nlem (e.g., Text-to-SQL). For example, RASAT (Qi\net al., 2022) and PICARD (Scholak et al., 2021),\nwhich are generation models based on T5 (Raffel\net al., 2020), give state-of-the-art results on bench-\nmarks like Spider (Yu et al., 2018) and CoSQL (Yu\net al., 2019).\nTowards a unified knowledge retriever\nAs\nknowledge can exist in different forms, a unified\nknowledge retriever that can handle both struc-\ntured and unstructured data in different modalities\nis more desirable. One possible solution for re-\nalizing a unified retriever is to leverage multiple\nsingle-source knowledge retrievers. When a query\ncomes in, the QAP module first decomposes it into\nseveral smaller sub-queries, where each sub-query\ncan be answered using one component knowledge\nretriever. The results from multiple knowledge re-\ntrievers can be integrated and then returned as the\nfinal output. However, several technical difficul-\nties, including how to accurately decompose the\nquestion and how to join the retrieved results often\nhinder the success of this approach. Alternatively,\nunifying multiple sources of information in a stan-\ndard representation, using text as a denominator\nrepresentation, has been promoted recently (Oguz\net al., 2022; Zeng et al., 2022). If all data items have\na corresponding textual description, it is possible\nfor the knowledge retriever to use only text-based\nretrieval techniques to find relevant data items once\nall input entities of non-textual modality have been\nmapped to their corresponding textual descriptions.\nSuch approach circumvents the complexity of\nmanaging multiple knowledge stores in different\nformat. Moreover, with the success of large multi-\nlingual and multi-modal language models (Con-\nneau and Lample, 2019; Aghajanyan et al., 2022),\ndata of different structures or from different modal-\nities can naturally share the same representation\nspace. While unifying multiple sources of informa-\ntion through representation learning seems to be a\npromising direction, it should be noted that certain\nstructured information may be lost in the process.\nFor example, by flatting a knowledge graph to se-\nquences of (subject, predicate, object) triples, the\ngraph structure is then buried in the textual form.\nWhether the information loss limits the retriever\u2019s\nability to handle certain highly relational queries\nremains to be seen.\n6\nProvenance-aware answer generators\n6.1\nSemi-Parametric Engine\nDemonstrating the provenance of a QA model\nprediction should center on identifying the data\u2014\nwhether in training data, retrieval corpora, or\ninput\u2014that is most influential in causing the model\nto make a particular prediction. For example, given\nthe question \u201cwho was the first U.S. president?\u201d, the\nsystem should return the correct answer \u201cGeorge\nWashington\u201d and references to training or retrieval\ncorpora that are\u2014to the model\u2014causally linked\nto the answer. If the training or retrieval data in-\ncluded Washington\u2019s Wikipedia page, a typical hu-\nman would expect for this to be included. However,\nthe requirement we impose is causal and counter-\nfactual: had the model not used that data, the pre-\ndiction should change. If the prediction does not\nchange, then from the causal perspective, there\nmay be other data that is either more influential\nor duplicative (e.g., if whitehouse.gov is in the\ntraining data, it is duplicative). Next, we describe\ncommon semi-parametric models and sketch how\nthis casually-based answer provenance could be ob-\ntained and computational challenges to overcome.\nProvided an input prompt and retrieved text,\nsemi-parametric models like ATLAS (Izacard et al.,\n2022) or passing documents as prompts to GPT-\n3 (Kasai et al., 2022) are adept at generating free-\ntext, short answers. Likewise, parametric models\nwith flexible input like GPT-3 can be combined\nwith retrievers to achieve a similar goal; alterna-\ntively, transformer models can be retrofitted with\nlayers so that passages can be integrated in embed-\nding space (Borgeaud et al., 2021). While retrieval-\naugmentation is no catch-all panacea to model hal-\nlucination, it does mitigate the problem (Shuster\net al., 2021b). Additionally, models\u2019 explanations\ncan make it easier to know when to trust models\nand when not to (Feng and Boyd-Graber, 2022).\nIn the case of QA models that take question plus\nretrieved text as input, there are several options.\nFirst, the model could provide several alternative\nanswers which provide insight into the distribution\nof model outputs, rather than just a point estimate.\nSecond, the model could provide a combination of\nfeature-based explanations such as token saliency\nmaps and the model\u2019s confidence in a correct an-\nswer (Wallace et al., 2019b). When combined, they\ncan jointly influence the degree to which humans\ntrust the model (Lai and Tan, 2019). However, to\nprovide a complete account of model behavior, we\nmust return to the training of model and the data\nused. In short, we endeavor to identify the combi-\nnation of input, training data, and retrieved text that\ncaused the model to produce the distribution of out-\nputs (i.e., answer(s)). This is, of course, challeng-\ning due to scale of language model training data\nlike C4 (Raffel et al., 2020) and the Pile (Gao et al.,\n2020) and that establishing causal\u2014and therefore\nmore faithful\u2014explanations of model behavior is\ndifficult. Training data attribution is one promising\nidea in this direction\u2014it uses gradient and embed-\nding based methods to attribute inference behavior\nto training data (Aky\u00fcrek et al., 2022). For ex-\nample, influence functions (Hampel, 1974; Han\net al., 2020) and TracIn (Pruthi et al., 2020) link\npredictions to specific training examples, but are\ncomputationally expensive and are approximate\nrather than exact solutions. To firmly establish\na causal connection, one could fully re-train the\nmodel without the identified training examples, but\nthis is prohibitively expensive in practice. Future\ndevelopment of efficient training data attribution,\ncombined with methods like interpretations of in-\nput plus retrieved data, is a promising direction\ntowards more complete explanations of model pre-\ndictions.\n6.2\nTabular Engine\nAs described at the end of Section 4, the knowl-\nedge retriever will pass on the data obtained to PAG.\nThe QAP module will pass information about its\nplan to PAG. If the data obtained is tabular and a\nSQL query is generated, the information is passed\nto the tabular engine of PAG to compute the re-\nquired answer(s). The recent advances in Text-to-\nSQL (Wang et al., 2020; Zhao et al., 2022) provide\na good technical foundation for generating such\nSQL queries.\nIn most cases, it is not difficult to understand\nthe correspondence between the natural language\nquestion and the SQL query that is generated. Once\nthe SQL query is obtained, provenance can be sys-\ntematically derived. In databases, the notion of\nprovenance is well-studied (Cheney et al., 2009)\nfor a large class of SQL queries; from explaining\nwhy a tuple is in the output (i.e., the set of tuples in\nthe database that led to the answer), where a value\nin a tuple is copied from (i.e., which cell in the\nsource table is the value copied from) (Buneman\net al., 2001) to how that tuple was derived, which\nis formalized as semirings (Green et al., 2007),\na polynomial that essentially describes conjunc-\ntion/disjunction of records required materialize a\nrecord in the result. Database provenance has also\nbeen extended to aggregate queries (Amsterdamer\net al., 2011). Since one can derive the mapping be-\ntween the input question and the SQL query that is\ngenerated and also derive the provenance from the\ndata sources based on the SQL query, it becomes\npossible to understand how the input question led\nto the answers given by POSTTEXT.\nPutting all together, POSTTEXT first explains\nthat the name of the image (i.e., \u201ca good version of\nthis dish\u201d) referred in question is Shaking beef. It\nthen shows the SQL query that is generated for the\nquestion \u201cWhere can I find a good version of Shak-\ning beef\u201d and the ranking function used for ranking\nthe rows of restaurants with reviews for the dish\nShaking beef. For our running example, the answer\nis obtained from the first row of the table in Fig-\nure 1. Specifically, the answer is summarized from\nthe column Dish and Review snippets/embeddings.\nThe actual snippets are found following the prove-\nnance links captured in the column Provenance.\nA more direct relationship between the summary\nand the actual review snippets can also be estab-\nlished (Carmeli et al., 2021).\nThe success of this approach depends on how\nfar we can push database provenance systemati-\ncally as SQL queries can still be far more com-\nplex than what is investigated in past research (e.g.,\ncomplex arithmetic and aggregate functions involv-\ning also negation, group filters, and functions over\nvalues of different modalities). As an alternative\nto executing the SQL query over the tables ob-\ntained, the tabular engine can also choose to de-\nploy table question answering (tableQA) methods\nwhere a model directly searches the tabular data\nfor answers based on the input question (Sun et al.,\n2016). Tapas (Herzig et al., 2020) and Tapex (Liu\net al., 2022) are two example solutions for tableQA\nthat formulates tableQA as sequence understand-\ning/generation tasks. Like other recent tableQA\nworks (Glass et al., 2021; Herzig et al., 2021), they\nconsider the problem of computing the answer from\na single input. It will be interesting to explore how\nto explain the results obtained using tableQA meth-\nods and how tableQA methods can be extended to\nhandle multi-hop questions where the answer may\nspan multiple tables or involve different types of\naggregations, reasoning and modalities.\n7\nPreliminary Findings\nTo test our hypothesis that views are valuable for\nanswering queries, especially queries that involve\ncounting or aggregation, we have implemented\na first version of POSTTEXT2 and compared it\nagainst some QA baselines.\nThe current implementation of POSTTEXT as-\nsumes views over the underlying data are avail-\nable in tabular format. The QAP module simply\nroutes the query to a view-based engine (VBE) or\na retrieval-based engine (RBE) to answer the query.\nVBE picks the best view and translates the natu-\nral language query into an SQLite query against\nthe view using OpenAI\u2019s gpt-3.5-turbo/gpt-4\n2PostText source code will be made available soon.\nVBE\nRBE\nDBChain\nDBChain (no views)\nS\n3.45\n2.81\n3.37\n2.72\nM\n3.79\n2.69\n3.28\n2.61\nL\n3.11\n2.44\n2.95\n1.95\nTable 2: Results with GPT-3.5-turbo. Sizes of (S)mall,\n(M)edium, (L)arge are 1.1MB, 2.4MB, and 5.6MB re-\nspectively.\nVBE\nRBE\nDBChain\nDBChain (no views)\nS\n3.33*\n2.10*\n2.14*\n1.10*\nM\n3.55\n1.93\n2.35\n1.51*\nL\n3.08\n2\n1.97\n1.11*\nTable 3: Results with GPT-4. * indicates that timeouts\nor API errors were encountered during experimentation.\nmodel. It then executes the SQLite query against\nthe view to obtain a table result which is then trans-\nlated into English as the final answer. VBE also an-\nalyzes the SQLite query to compute the provenance\nof the answers. At present, it does so by simply re-\ntrieving all tuples that contributed to every (nested)\naggregated query that is a simple (select-from-\nwhere-groupby-having clause) and does not handle\nnegations. An example of the VBE process is de-\nscribed in Appendix B. RBE is implemented with\nLangchain\u2019s RetrievalQAwithSources library. It\nfirst retrieves top-k documents that are relevant for\nthe query and then conditions its answer based on\nthe retrieval. The answer and the ids of the retrieved\ndocuments are returned.\nFor our experiments, we use the 42 multihop\nqueries over 3 synthetic personal timelines of dif-\nferent sizes from TimelineQA\u2019s benchmark (Tan\net al., 2023). The personal timelines model the\ndaily activities (e.g., the trips made, things bought,\npeople talked to) of a person over a period of time.\nWe create a view around each type of activity\n(e.g., trips, shopping, daily_chats) for VBE. For\nfurther comparison, we also ran Langchain\u2019s SQL-\nDatabaseChain (DBChain) to perform QA over the\nsame VBE views. Furthermore, we ran it over\ntimelines loosely structured as a binary relation\nof (date,description) pairs (called DBChain (no\nviews)). We compared the returned answers against\nthe ground truth answers by grading them on a\nscale of 1-5, with a LLM, where 5 means the re-\nturned answer has the same meaning as the ground\ntruth answer (the grading scheme is described in\nthe Appendix C).\nOur results are shown in Tables 2 and 3.\nAcross both tables, the results on DBChain vs.\nDBChain(no views) reveal that adding some struc-\nture (in this case adding views) is crucial for better\nperformance. Although the benchmark is a rela-\ntively small dataset, the scale of the timelines al-\nready reveals an impact on the accuracy across all\nQA systems. For DBChain, the drop in accuracy\nas the size increases because it sometimes relies\non generating SQL queries that return all relevant\nrecords and passing all the records to the language\nmodel to compute the aggregate. When the results\nreturned are large, which tends to be the case for\nlarger timelines, the token limit of the LLM is often\nexceeded. VBE has a similar downward trend. It\ntends to generate queries that push the aggregates\nto the SQL engine and hence, avoids the issue of\nexceeding the token limit of the language models\nfor many cases encountered in DBChain. Still, as\nthe timeline gets larger, the result returned by the\ngenerated SQL query tends to be bigger and when\nthese results are passed to the verbalization com-\nponent to compose an answer in English, this may\nsometimes exceed the token limit of the language\nmodel. We also found that on a handful of cases,\nit so happens that the SQL query generated for L\nis invalid compared with those generated for the\nsparse dataset.\nThe scores of RBE is relatively stable across all\ndata densities. But overall, it tends to score lower\ncompared with VBE and DBChain . This is be-\ncause RBE relies on retrieving the top k documents\nfrom an index to condition the answers upon, re-\ngardless of the size of the timeline. However, these\nretrieved documents may not contain all the nec-\nessary information for answering the question in\ngeneral. Even though the grading scores may not\nreveal this, the answers tend to be \u201cmore wrong\u201d\nfor aggregate queries over a larger timeline.\n8\nConclusion\nPOSTTEXT enhances the core ideas of semi-\nparametric architectures with views, a query an-\nalyzer & planner, and a provenance-aware answer\ngenerator. Our initial results indicate that POST-\nTEXT is more effective on queries involving count-\ning/aggregation when we provide structured views\nto facilitate computation. We plan to further de-\nvelop and investigate POSTTEXT to automatically\ndetermine what views to construct, how does one\ngenerate plans and compare amongst plans, and\nhow can one measure the quality of answers with\nprovenance.\nLimitations and Ethical Considerations\nWe point out the limitations of large language mod-\nels (costly to train, deploy, maintain, hallucinate,\nopaque). The vision of POSTTEXT shows promise\nof less costly training, maintenance, and more ex-\nplainability. However, no actual system is built yet\nto validate these claims and it is also not clear that a\nsystem with POSTTEXT architecture will be easier\nto deploy since it has more components.\nReferences\nArmen Aghajanyan, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,\nand Luke Zettlemoyer. 2022.\nCM3:\nA causal\nmasked multimodal model of the internet. CoRR,\nabs/2201.07520.\nSanjay Agrawal, Surajit Chaudhuri, and Vivek R.\nNarasayya. 2000. Automated selection of materi-\nalized views and indexes in SQL databases. In VLDB\n2000, Proceedings of 26th International Conference\non Very Large Data Bases, September 10-14, 2000,\nCairo, Egypt, pages 496\u2013505. Morgan Kaufmann.\nSanjay Agrawal, Eric Chu, and Vivek Narasayya. 2006.\nAutomatic physical design tuning: Workload as a\nsequence. In Proceedings of the 2006 ACM SIGMOD\nInternational Conference on Management of Data,\nSIGMOD \u201906, page 683\u2013694, New York, NY, USA.\nAssociation for Computing Machinery.\nEkin Aky\u00fcrek, Tolga Bolukbasi, Frederick Liu, Binbin\nXiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\n2022. Tracing knowledge in language models back\nto the training data. In Findings of the Association\nfor Computational Linguistics: EMNLP. Association\nfor Computational Linguistics.\nYael Amsterdamer, Daniel Deutch, and Val Tannen.\n2011.\nProvenance for aggregate queries.\nIn\nProceedings of the 30th ACM SIGMOD-SIGACT-\nSIGART Symposium on Principles of Database Sys-\ntems, PODS 2011, June 12-16, 2011, Athens, Greece,\npages 153\u2013164. ACM.\nSimran Arora, Patrick Lewis, Angela Fan, Jacob Kahn,\nand Christopher R\u00e9. 2022. Reasoning over public\nand private data in retrieval-based systems.\nAkari Asai,\nMatt Gardner,\nand Hannaneh Ha-\njishirzi. 2022. Evidentiality-guided generation for\nKnowledge-Intensive NLP tasks. In Conference of\nthe North American Chapter of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT \u201921, page 610\u2013623.\nJonathan Berant, Andrew Chou, Roy Frostig, and\nPercy S. Liang. 2013.\nSemantic parsing on free-\nbase from question-answer pairs. In Proceedings of\nEmpirical Methods in Natural Language Processing.\nE. Bertino and R. Sandhu. 2005. Database security -\nconcepts, approaches, and challenges. IEEE Transac-\ntions on Dependable and Secure Computing, 2(1):2\u2013\n19.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W Rae, Erich Elsen, and Laurent Sifre.\n2021. Improving language models by retrieving from\ntrillions of tokens.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Proceed-\nings of Advances in Neural Information Processing\nSystems. Curran Associates, Inc.\nNicolas Bruno and Surajit Chaudhuri. 2008.\nCon-\nstrained physical design tuning. Proc. VLDB Endow.,\n1(1):4\u201315.\nPeter Buneman, Sanjeev Khanna, and Wang Chiew Tan.\n2001. Why and where: A characterization of data\nprovenance. In ICDT, volume 1973 of Lecture Notes\nin Computer Science, pages 316\u2013330.\nNofar Carmeli, Xiaolan Wang, Yoshihiko Suhara, Ste-\nfanos Angelidis, Yuliang Li, Jinfeng Li, and Wang-\nChiew Tan. 2021. Constructing explainable opinion\ngraphs from reviews. In WWW \u201921: The Web Confer-\nence 2021, Virtual Event / Ljubljana, Slovenia, pages\n3419\u20133431. ACM / IW3C2.\nChatGPT3-OpenAI.\nChatgpt: Optimizing language\nmodels for dialogue.\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and\nMatt Gardner. 2020. MOCHA: A dataset for train-\ning and evaluating generative reading comprehension\nmetrics. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6521\u20136532, Online. Association for\nComputational Linguistics.\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and\nWilliam W. Cohen. 2022.\nMurag: Multimodal\nretrieval-augmented generator for open question an-\nswering over images and text.\nJames Cheney, Laura Chiticariu, and Wang Chiew Tan.\n2009.\nProvenance in databases: Why, how, and\nwhere. Found. Trends Databases, 1(4):379\u2013474.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2022. Binding language\nmodels in symbolic languages.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nSudipto Das, Miroslav Grbic, Igor Ilic, Isidora Jovandic,\nAndrija Jovanovic, Vivek R. Narasayya, Miodrag\nRadulovic, Maja Stikic, Gaoxiang Xu, and Surajit\nChaudhuri. 2019. Automatically indexing millions\nof databases in microsoft azure sql database. In Pro-\nceedings of the 2019 International Conference on\nManagement of Data, SIGMOD \u201919, page 666\u2013679,\nNew York, NY, USA. Association for Computing\nMachinery.\nScott C. Deerwester, Susan T. Dumais, Thomas K. Lan-\ndauer, George W. Furnas, and Richard A. Harshman.\n1990. Indexing by latent semantic analysis. Jour-\nnal of the American society for information science,\n41:391\u2013407.\nEmily Dinan, Gavin Abercrombie, A Stevie Bergman,\nShannon Spruit, Dirk Hovy, Y-Lan Boureau, and\nVerena Rieser. 2021. Anticipating safety issues in\nE2E conversational AI: Framework and tooling.\nJesse Dunietz, Gregory Burnham, Akash Bharadwaj,\nOwen Rambow, Jennifer Chu-Carroll, and David Fer-\nrucci. 2020. To test machine comprehension, start\nby defining comprehension. In Proceedings of the\nAssociation for Computational Linguistics.\nShi Feng and Jordan Boyd-Graber. 2022. Learning to\nexplain selectively: A case study on question answer-\ning. In Proceedings of Empirical Methods in Natural\nLanguage Processing. Association for Computational\nLinguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020.\nThe pile: An\n800GB dataset of diverse text for language modeling.\nHector Garcia-Molina, Jeffrey D. Ullman, and Jennifer\nWidom. 2008. Database Systems: The Complete\nBook, 2 edition. Prentice Hall Press, USA.\nMichael R. Glass, Mustafa Canim, Alfio Gliozzo, Sa-\nneem A. Chemmengath, Vishwajeet Kumar, Rishav\nChakravarti, Avi Sil, Feifei Pan, Samarth Bharadwaj,\nand Nicolas Rodolfo Fauceglia. 2021. Capturing row\nand column semantics in transformer based question\nanswering over tables. In NAACL-HLT, pages 1212\u2013\n1224. Association for Computational Linguistics.\nJonathan Goldstein and Per-\u00c5ke Larson. 2001. Opti-\nmizing queries using materialized views: A practical,\nscalable solution. In Proceedings of the 2001 ACM\nSIGMOD International Conference on Management\nof Data, SIGMOD \u201901, page 331\u2013342, New York,\nNY, USA. Association for Computing Machinery.\nAlbert Gordo, Jon Almaz\u00e1n, Jerome Revaud, and Diane\nLarlus. 2016. Deep image retrieval: Learning global\nrepresentations for image search. In Computer Vi-\nsion \u2013 ECCV 2016, pages 241\u2013257, Cham. Springer\nInternational Publishing.\nGPT-Index. 2022. [link].\nGoetz Graefe. 1993. Query evaluation techniques for\nlarge databases. ACM Comput. Surv., 25(2):73\u2013169.\nTodd J. Green, Gregory Karvounarakis, and Val Tannen.\n2007.\nProvenance semirings.\nIn Proceedings of\nthe Twenty-Sixth ACM SIGACT-SIGMOD-SIGART\nSymposium on Principles of Database Systems, June\n11-13, 2007, Beijing, China, pages 31\u201340. ACM.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\nAugmented language model Pre-Training. In Pro-\nceedings of the International Conference of Machine\nLearning.\nAlon Y. Halevy. 2001. Answering queries using views:\nA survey. The VLDB Journal, 10(4):270\u2013294.\nFrank R Hampel. 1974. The influence curve and its\nrole in robust estimation. Journal of the American\nStatistical Association, 69(346):383\u2013393.\nXiaochuang Han, Byron C. Wallace, and Yulia Tsvetkov.\n2020. Explaining black box predictions and unveil-\ning data artifacts through influence functions. In\nProceedings of the Association for Computational\nLinguistics. Association for Computational Linguis-\ntics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nJonathan Herzig, Thomas M\u00fcller, Syrine Krichene, and\nJulian Eisenschlos. 2021. Open domain question\nanswering over tables via dense retrieval. In NAACL-\nHLT, pages 512\u2013519. Association for Computational\nLinguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nM\u00fcller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. 2020. Tapas: Weakly supervised table parsing\nvia pre-training. In ACL, pages 4320\u20134333. Associa-\ntion for Computational Linguistics.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training Compute-Optimal\nlarge language models.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In Proceedings of the 22nd ACM\nInternational Conference on Information & Knowl-\nedge Management, CIKM \u201913, page 2333\u20132338, New\nYork, NY, USA. Association for Computing Machin-\nery.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the Eu-\nropean Chapter of the Association for Computational\nLinguistics. Association for Computational Linguis-\ntics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nAlekh Jindal, Konstantinos Karanasos, Sriram Rao, and\nHiren Patel. 2018. Selecting subexpressions to ma-\nterialize at datacenter scale. Proc. VLDB Endow.,\n11(7):800\u2013812.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective question answering under domain shift. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 5684\u20135696. Associa-\ntion for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nEhud Karpas, Omri Abend, Yonatan Belinkov, Barak\nLenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\nBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhl-\ngay, Noam Rozen, Erez Schwartz, Gal Shachaf,\nShai Shalev-Shwartz, Amnon Shashua, and Moshe\nTenenholtz. 2022. Mrkl systems: A modular, neuro-\nsymbolic architecture that combines large language\nmodels, external knowledge sources and discrete rea-\nsoning.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. RealTime QA: What\u2019s the answer right now?\narXiv [cs.CL].\nTom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke\nZettlemoyer. 2013. Scaling semantic parsers with\non-the-fly ontology matching.\nIn Proceedings of\nthe 2013 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1545\u20131556, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nVivian Lai and Chenhao Tan. 2019. On human predic-\ntions with explanations and predictions of machine\nlearning models: A case study on deception detec-\ntion. In Proceedings of the Conference on Fairness,\nAccountability, and Transparency. Association for\nComputing Machinery.\nLangChain. [link].\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020.\nRetrieval-augmented generation for\nknowledge-intensive NLP tasks. In Proceedings of\nAdvances in Neural Information Processing Systems.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022.\nTAPEX: table pre-training via learning a neural SQL\nexecutor. In ICLR. OpenReview.net.\nThomas Macaulay. 2020. Someone let a gpt-3 bot loose\non reddit \u2014 it didn\u2019t end well.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783\u2013\n5797, Online. Association for Computational Lin-\nguistics.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan\nPeshterliev, Dmytro Okhonko, Michael Schlichtkrull,\nSonal Gupta, Yashar Mehdad, and Scott Yih. 2022.\nUniK-QA: Unified representations of structured and\nunstructured knowledge for open-domain question\nanswering. In Findings of the Association for Compu-\ntational Linguistics: NAACL 2022, pages 1535\u20131546,\nSeattle, United States. Association for Computational\nLinguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund\nSundararajan. 2020. Estimating training data influ-\nence by tracing gradient descent. In Proceedings of\nAdvances in Neural Information Processing Systems.\nCurran Associates, Inc.\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan,\nYu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi\nZhang, and Zhouhan Lin. 2022. RASAT: Integrating\nrelational structures into pretrained seq2seq model\nfor text-to-sql. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguis-\ntics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified Text-to-Text\ntransformer. Journal of Machine Learning Research,\n21(140):1\u201367.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends\u00ae in Information Re-\ntrieval, 3(4):333\u2013389.\nG. Salton, A. Wong, and C. S. Yang. 1975. A vector\nspace model for automatic indexing. Commun. ACM,\n18(11):613\u2013620.\nKarl Schnaitter, Serge Abiteboul, Tova Milo, and Neok-\nlis Polyzotis. 2007. On-line index selection for shift-\ning workloads. In 2007 IEEE 23rd International\nConference on Data Engineering Workshop, pages\n459\u2013468.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9895\u20139901, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nShelly Sheynin, Oron Ashual, Adam Polyak, Uriel\nSinger, Oran Gafni, Eliya Nachmani, and Yaniv Taig-\nman. 2022. Knn-diffusion: Image generation via\nlarge-scale retrieval.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021a. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 3784\u2013\n3803. Association for Computational Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021b. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP. Association for Computational Linguistics.\nChenglei Si, Chen Zhao, and Jordan Boyd-Graber. 2021.\nWhat\u2019s in a name? answer equivalence for open-\ndomain question answering. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9623\u20139629, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nChenglei Si, Chen Zhao, Sewon Min, and Jordan L.\nBoyd-Graber. 2022. Revisiting calibration for ques-\ntion answering. ArXiv, abs/2205.12507.\nHuan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,\nand Xifeng Yan. 2016. Table cell search for question\nanswering. In Proceedings of the 25th International\nConference on World Wide Web, WWW \u201916, page\n771\u2013782, Republic and Canton of Geneva, CHE. In-\nternational World Wide Web Conferences Steering\nCommittee.\nWang-Chiew Tan, Jane Dwivedi-Yu, Yuliang Li, Lam-\nbert Mathias, Marzieh Saeidi, Jing Nathan Yan, and\nAlon Y. Halevy. 2023. Timelineqa: A benchmark\nfor question answering over timelines. In ACL (to\nappear).\nPeter D Turney and Patrick Pantel. 2010. From fre-\nquency to meaning: Vector space models of se-\nmantics. Journal of artificial intelligence research,\n37:141\u2013188.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019a. Universal adversarial\ntriggers for attacking and analyzing NLP. In Pro-\nceedings of Empirical Methods in Natural Language\nProcessing. Association for Computational Linguis-\ntics.\nEric Wallace, Pedro Rodriguez, Shi Feng, and Jordan\nBoyd-Graber. 2019b. Trick me if you can: Human-in-\nthe-loop generation of adversarial question answering\nexamples. In Transactions of the Association for\nComputational Linguistics, pages 387\u2013401.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. RAT-SQL:\nrelation-aware schema encoding and linking for text-\nto-sql parsers. In ACL, pages 7567\u20137578. Associa-\ntion for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nJason Weston, Samy Bengio, and Nicolas Usunier. 2011.\nWsabie: Scaling up to large vocabulary image anno-\ntation. In Twenty-Second International Joint Confer-\nence on Artificial Intelligence.\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-\nner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question understand-\ning benchmark. Transactions of the Association for\nComputational Linguistics, 8:183\u2013198.\nWenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei\nDu, Patrick S. H. Lewis, William Yang Wang, Yashar\nMehdad, Wen-tau Yih, Sebastian Riedel, Douwe\nKiela, and Barlas Oguz. 2021.\nAnswering com-\nplex open-domain questions with multi-hop dense\nretrieval. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. 2022a.\nRetrieval-augmented multimodal language modeling.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-Tau Yih. 2022b.\nRetrieval-Augmented multimodal language model-\ning.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jian-\nfeng Gao. 2015. Semantic parsing via staged query\ngraph generation: Question answering with knowl-\nedge base. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1321\u20131331, Beijing, China. Association for\nComputational Linguistics.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962\u2013\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911\u20133921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof\nChoromanski, Adrian Wong, Stefan Welker, Federico\nTombari, Aveek Purohit, Michael Ryoo, Vikas Sind-\nhwani, Johnny Lee, Vincent Vanhoucke, and Pete\nFlorence. 2022. Socratic models: Composing zero-\nshot multimodal reasoning with language.\nChen Zhao, Yu Su, Adam Pauls, and Emmanouil Anto-\nnios Platanios. 2022. Bridging the generalization gap\nin text-to-sql parsing with schema expansion. In ACL\n(1), pages 5568\u20135578. Association for Computational\nLinguistics.\nChen Zhao, Chenyan Xiong, Hal Daum\u00e9 III, and Jor-\ndan Boyd-Graber. 2021. Multi-step reasoning over\nunstructured text with beam dense retrieval. In North\nAmerican Association of Computational Linguistics.\nA\nAppendix\nB\nView-based QA\nExample run of POSTTEXT with the query \"When\nwas the last time I chatted with Avery?\":\nThis query is first matched against a set of avail-\nable views and the best one is picked if there\nis sufficient confidence.\nIn this case, the view\ndaily_chat_log is selected.\nThe query is first translated into an SQLite\nquery:\nSELECT MAX(date)\nFROM daily_chat_log\nWHERE friends LIKE '%Avery%'\nThe SQLite query is then cleaned and \u201crelaxed\u201d.\nFor example, on occasions, an attribute that does\nnot exist is used in the query even though this hap-\npens rarely. In this case, no cleaning is required.\nThe conditions over TEXT types are also relaxed.\nWe convert equality conditions (e.g., friends =\n\u2019Avery\u2019) to LIKE conditions (e.g., friends LIKE\n\u2019%Avery%\u2019) and further relax LIKE condition with\na user-defined CLOSE_ENOUGH predicate.\nSELECT MAX(date)\nFROM daily_chat_log\nWHERE (friends LIKE '%Avery%' OR\nCLOSE_ENOUGH('%Avery%', friends))\nThe above query is executed and the results\nobtained is shown below.\nWe then verbalized\nan answer based on the table result.\nResult:\n[(\u20192022/12/26\u2019)]\nReturned answer (verbalized): The last time I\nchatted with Avery was on December 26, 2022.\nWe\nobserve\nthat\nLangchain\u2019s\nSQL-\nDatabaseChain provides a very similar func-\ntionality of matching an incoming query against\navailable tables and generating an SQL query over\nthe matched tables. However, SQLDatabaseChain\ndoes not clean or relax query predicates, and\nrequires one to specify a limit on the number of\nrecords returned. Furthermore, it does not compute\nthe provenance of the answer obtained, as we will\ndescribe in the next section. As we also described\nin Section 7, view-based QA generally outperforms\nSQLDatabaseChain because of its ability to push\naggregates to the database engine instead of relying\non the language model to aggregate the results\n(after using the database engine to compute the\nrelevant records for answering the query.\nProvenance queries: PostText generates queries\nto retrieve records that contributed to the answer\nreturned above.\nIt does so by analyzing every\nselect-from-where-groupby-having subquery\nin the generated query to find tuples that con-\ntributed to every such subquery. For example, the\nfollowing SQL queries are generated to compute\nprovenance.\nSELECT name\nFROM pragma_table_info('daily_chat_log')\nwhere pk;\nq0:\nSELECT eid\nFROM daily_chat_log\nWHERE (friends LIKE '%Avery%' OR\nCLOSE_ENOUGH('%Avery%', friends))\nThe first query above returns the key of the table\nand the second retrieves the keys from the table\nthat contributed to the returned answer.\n[('q0', ('e152',)), ('q0', ('e154',)), ('q0',\n('e169',)), ('q0', ('e176',)), ...]\nC\nGrading scheme\nThe following is our grading scheme used for grad-\ning the answers generated by different systems\nagainst the ground truth answer:\n\u2022 5 means the systems\u2019s answer has the same\nmeaning as the TRUE answer.\n\u2022 4 means the TRUE answer can be determined\nfrom the system\u2019s answer.\n\u2022 3 means there is some overlap in the system\u2019s\nanswer and the TRUE answer.\n\u2022 means there is little overlap in the system\u2019s\nanswer and the TRUE answer.\n\u2022 1 means the system\u2019s answer is wrong, it has\nno relationship with the TRUE answer.\n"
  },
  {
    "title": "An Empirical Study on Challenging Math Problem Solving with GPT-4",
    "link": "https://arxiv.org/pdf/2306.01337.pdf",
    "upvote": "1",
    "text": "An Empirical Study on Challenging Math Problem\nSolving with GPT-4\nYiran Wu\nPennsylvania State University\nyiran.wu@psu.edu\nFeiran Jia\nPennsylvania State University\nfeiran.jia@psu.edu\nShaokun Zhang\nPennsylvania State University\nshaokun.zhang@psu.edu\nHangyu Li\nImperial College London\nhl6021@ic.ac.uk\nErkang Zhu\nMicrosoft Research Redmond\nekzhu@microsoft.com\nYue Wang\nMicrosoft Research Redmond\nwang.yue@microsoft.com\nYin Tat Lee\nUniversity of Washington\nyintat@uw.edu\nRichard Peng\nUniversity of Waterloo\ny5peng@uwaterloo.ca\nQingyun Wu\nPennsylvania State University\nqingyun.wu@psu.edu\nChi Wang\nMicrosoft Research Redmond\nwang.chi@microsoft.com\nAbstract\nEmploying Large Language Models (LLMs) to address mathematical problems\nis an intriguing research endeavor, considering the abundance of math problems\nexpressed in natural language across numerous science and engineering fields.\nWhile several prior works have investigated solving elementary mathematics using\nLLMs, this work explores the frontier of using GPT-4 for solving more complex\nand challenging math problems. We evaluate various ways of using GPT-4. Some\nof them are adapted from existing work, and one is MathChat, a conversational\nproblem-solving framework newly proposed in this work. We perform the evalua-\ntion on difficult high school competition problems from the MATH dataset, which\nshows the advantage of the proposed conversational approach.\n1\nIntroduction\nWith Large Language Models (LLMs) demonstrating remarkable proficiency in various tasks spanning\ndiverse domains [1], it is compelling to explore the potential of LLMs in tackling mathematical\nproblems considering the crucial role of mathematics [34] and the prevalence of mathematical\nproblems expressed in natural language throughout numerous scientific and engineering disciplines.\nPrior works on this endeavor mostly concern relatively simple math problems such as elementary\nschool math [4, 18, 24], or concerns a specific aspect of math problem solving, for instance, arithmetic\noperations and reasoning [12], and quantitative reasoning [16]. In this work, we investigate the frontier\nof GPT-4 [22], arguably the most advanced LLM to date, on advanced math problem-solving.\nCreative ways of using LLMs to solve math problems have emerged lately [3, 6, 40, 31, 36]; however,\nthey are rarely evaluated on GPT-4. [9] and [1] performed studies on math problem solving with GPT-\n3.5 and an early version of GPT-4 respectively, reporting interesting observations on the capability\nand limitations of recent LLMs on this endeavor. In this work, we adapt and evaluate several existing\nmethods of using LLMs to the usage of GPT-4, including a vanilla prompt, Program of Thoughts\nPreprint. Under review.\narXiv:2306.01337v2  [cs.CL]  8 Jun 2023\nLet's use Python\u2026 Problem: Let \nThere exist vectors  and  such that the line \ncontaining  and  can be expressed in the form \nFurthermore, for a certain choice of , it is the case that \nfor all points  lying on the same side of  that  lies on, the distance between  and  is .  Find .  \na = (\n5\n\u22123\n\u22124)\nand\nb = (\n\u221211\n1\n28 ) .\np\nd\na\nb\nv = p + dt .\nd\nv\na\nb\nv\na\nt\nd\n\u2460 Prompt\n\u2460\n\u2461 Problem\n\u2461\nUser \nProxy \nAgent \nPython\nMatrix([[-16], [4], [32]]) \nMatrix([[-4/9], [1/9], [8/9]]) \nUser \nProxy \nAgent \nThe vector  is indeed equal to \n. \nSo, the line containing the points  and  can be expressed as: \n \nAnd the vector  is \n.\nd\n\u22124/9\n1/9\n8/9\na\nb\nv = a + dt = (\n5\n\u22123\n\u22124) + t\n\u22124/9\n1/9\n8/9\nd\n\u22124/9\n1/9\n8/9\n> Code Execution\nGPT-4\nWe will first find the general equation of the line passing through points  and  using the \nvector form. Then we'll proceed with determining  that satisfies the conditions given. \nTo find the general equation of the line passing through points  and , we can use the \nformula:\n. Let\u2019s compute \n. \nNow that we have the difference vector \n, let's substitute it into the general equation of \nthe line containing points  and . This gives us:\n \nSince we have , let's find  that satisfies the given conditions, i.e., when the distance \nbetween  and  is . Using the distance formula, we get\n. Dividing by , we get:\n \nThe magnitude of  is . Since  is parallel to \n, we can find  by normalizing the \nvector \n: \n. Let\u2019s calculate . \nNow we have found the vector . \na\nb\nd\na\nb\nv = a + (b \u2212 a)t\nb \u2212 a\nb \u2212 a\na\nb\nv = a + (b \u2212 a)t = a + dt\na\nd\nv\na\nt\nt = ||v \u2212 a|| = ||(a + dt) \u2212 a|| = ||dt||\nt\n1 = ||d||\nd\n1\nd\n(b \u2212 a)\nd\n(b \u2212 a)\nd =\nb \u2212 a\n||b \u2212 a||\nd\nd\nPython\nfrom sympy import \nMatrix\na = Matrix([5, -3, \n-4])\nb = Matrix([-11, 1, \n28])\nb_minus_a = b - a\nprint(b_minus_a)\nd = b_minus_a / \nb_minus_a.norm()\nprint(d)\nGPT-4\nPython\nFinal Answer!\nFigure 1: Example of a math problem-solving process with MathChat. The user proxy agent initiates\na conversation by sending the math problem to be solved, along with a preset prompt to GPT-4 (the\nLLM assistant). From GPT-4\u2019s response, the user proxy agent extracts all code and executes them\nsequentially. Valid code from previous runs is recorded and will be executed together with the new\ncode to reflect the step-by-step reasoning progress of the model. The results will be returned to GPT-4\nand GPT-4 will continue its problem-solving process. While GPT-4 solves this problem with only one\nturn of user message in this example, our framework allows multi-turn conversations and additional\nquery handling, shown in Figure 3. The user proxy agent will do pattern-matching (in our case, the\nappearance of \\boxed{} containing a final answer) in the assistant\u2019s response to determine whether\nto end the conversation.\n(PoT) prompt [3], and Program Synthesis prompt [6]. We further propose MathChat, a conversational\nframework tailored to chat-based LLMs, where the math problem is solved with a mock conversation\nbetween the model and a user proxy agent (See Figure 1 for an example and Figure 2 for the\nworkflow). We perform the evaluation on the MATH dataset [11], a comprehensive collection of\nmathematical problems derived from various competitions and educational levels. Our study targets\nthe level-5 difficulty problems within this dataset, which primarily consist of challenging high school\ncompetition problems that even college students find difficult. The evaluation shows that MathChat\ncan further improve previous prompting methods by 6%, and it can reach 60% accuracy on half of\nthe categories while having competitive performance across all categories. We also demonstrate the\nextensibility of MathChat with different prompts and different tools.\n2\nExisting methods of using LLMs on math problem solving\nIn this section, we introduce the various methods of using LLMs from existing literature that can be\napplied to solve math problems. We first introduce techniques specifically designed for mathematical\nproblem-solving and then summarize the general approaches. From our investigation, we only find\none method [6] that targets challenging math problems, while most other methods are evaluated on\nelementary school math [4, 18, 24].\n2\nProblem\nUser Proxy  \nAgent\nLLM Assistant\nMathChat\nExtract\nQueries\n\u00a0Recurrent \nError?\n\"Continue\"\nResults\nErrors\n\"Solve it yourself\"\nQueries\u00a0 \nExecution\nNo  \nquery  \nfound\nNo\nYes\nUser Proxy Agent\nAssistant Message\nUser Message\nAssistant \nMessage\nUser \nMessage\nFigure 2: MathChat workflow: After a math problem is fed into MathChat, the user proxy agent\nwill initiate a conversation with the LLM Assistant to solve the problem. In each turn of interaction,\nthe user proxy agent processes the message from the assistant (Assistant Message) and responds with\na User Message. This process continues until the user proxy agent detects a certain pattern to end\nthe conversation. The process in the rectangular on the right-hand side of the figure shows the inner\nworkflow of the user proxy agent once an Assistant Message is received. It shows the functionality\nof executing any tool-using queries, such as Python code. It is also responsible for giving different\ninstructions corresponding to different types of messages from the LLM Assistant (More in Appendix\nA). To illustrate this feature of the proxy agent, we give a concrete example in Figure 3.\nMethods tailored for math solving. Despite creative methods to use LLMs for different tasks, there\nare only a few methods proposed that target math problem-solving. One particular endeavor is using\nLLMs to offload arithmetic calculations and other basic operations involved in math problem-solving\nto programs. Specifically, [10] uses Codex [2] to decompose math problems into runnable steps\nby interleaving CoT prompting with code, resulting in an executable Python code to solve math\nproblems. [6, 3] also use Codex to generate code-based answers to problems. While [3, 10] prompt\nthe model to tackle math word problems, [6] investigates the ability of program-aided coding on\nuniversity-level problems, including randomly sampled problems from the MATH [11] dataset. Other\nthan using Python, [33] proposes a method for basic arithmetic and logical problems, that concatenates\nthe generated answer back to the problem and lets the model predict the original conditions to verify\nthe correctness of this answer. Among these methods, [6, 3] can be used with zero-shot prompts to\nsolve math problems. MathPrompter is another method tailored for math solving but it specifically\ntargets improving arithmetic operations and reasoning performance of LLMs [12].\nGeneral approaches. General approaches that can be used to improve math solving include (1)\nchain-of-thought prompting [32, 15], (2) multi-stage prompting [8, 14, 26, 5, 13, 39, 23, 37, 35, 30],\n(3) tool-using [10, 3, 6, 28, 23, 37]; and (4) self-consistency [31, 20]. Among them, here we review\nsome of the latest ones. [28] trains a Toolformer that can decide when to send API calls. [23] use\nfew-shot prompting to interleave step-by-step reasoning and tool-using. ART [23] maintains a library\nof demonstrations, such as using calculators, and retrieves the most similar examples for prompting a\nnew problem. [20] first generates an output using GPT-3.5 and uses the same model to give feedback\non the output, then the model is asked to refine the output based on the feedback. Although these\ngeneral approaches can be applied to solve math word problems, these methods are not tailored for\nsolving math problems and are mostly evaluated on easier math tasks.\nDue to the page limit, we include a comprehensive discussion on existing methods in Appendix C.\n3\nMathChat: A conversational framework for math problem solving\nIn this section, we introduce MathChat, a conversational framework for math problem-solving.\nA conversational framework with user proxy agent. MathChat is a framework that simulates a\nmock conversation between an LLM assistant (GPT-4 in our case) and a user proxy agent. Here a user\nproxy agent is an agent playing the user\u2019s role in conversations with the LLM assistant. In MathChat,\nthe assistant and the user proxy agent work together to solve the math problem. The workflow of\nthis framework is presented in Figure 2. The user proxy agent takes a math problem to be solved as\ninput and would initiate a conversation with the LLM assistant. The initial message from the user\nproxy agent consists of an initial prompt and the problem to be solved. The initial prompt is used to\n3\nLet's use Python\u2026  Problem: Fake gold bricks are made by covering concrete cubes with gold paint, so the cost of the paint is proportional \nto their surface area while the cost of the concrete is proportional to their volume.  If a 1 inch cube costs $1.30 to make while a 2 inch cube \ncosts $6.80, then how much would a 3 inch cube cost?\nTo continue solving the problem, we will first \nsolve the system of linear equations to find the \nvalues of  and . Then, we will calculate the \ncost of a 3 inch cube using the cost equation. \n\u2026 \nk1\nk2\n54k1 + 27k2\nUser\nimport sympy as sp\nk1, k2 = sp.symbols('k1 k2', \npositive=True)\n\u2026\nsurface_area_2 = 6 * (2)**2\nvolume_2 = (2)**3\neq2 = sp.Eq(6.8, k1 * surface_area_2 + k2 \n* volume_2)\nk1_value, k2_value = sp.solve((eq1, eq2), \n(k1, k2))\nsurface_area_3 = 6 * (3)**2\nvolume_3 = (3)**3\ncost_3 = k1_value* surface_area_3 + \nk2_value * volume_3\n\u2026 Let me correct \nthis and provide the \ncorrect result. \nLet's now run the \ncorrected code to get \nthe cost of a 3-inch \ncube. \n\u2026\nk1_value, \nk2_value = \nsp.solve((eq1, \neq2), (k1, k2), \ndict=True)\n\u2026\ncost_3 = \nk1_value[k1] * \nsurface_area_3 + \nk2_value[k2] * \nvolume_3\n\u2026\nValueError: not enough \nvalues to unpack \n(expected 2, got 1)\n\u2026I will correct the code \nand provide the proper \nresult. \nLet's run the corrected \ncode to get the cost of a \n3-inch cube. \n\u2026\nk_values = \nsp.solve((eq1, eq2), \n(k1, k2), dict=True)\nk1_value = \nk_values[0][k1]\nk2_value = \nk_values[0][k2]\n\u2026\ncost_3 = k1_value * \nsurface_area_3 + \nk2_value *\nvolume_3\n\u2026\n\u2026we can write the cost equation as \nfollows: \ncost = \n * surface_area + \n * volume\n\u2026We are given: \n1 inch cube costs $1.30 \nsurface_area_1 = \nvolume_1 = \n1.3 = \n * surface_area_1 + \n * volume_1\n2 inch cube costs $6.80 \nsurface_area_2 = \n \nvolume_2 =\n6.8 = \n * surface_area_2 + \n * volume_2\n\u2026 We will solve these equations to find \nk1 and k2. Afterwards, we will find the \ncost of the 3 inch cube using the same \ncost equation. \nsurface_area_3 =\nvolume_3 = \ncost_3 = \n *surface_area_3 + \n * volume_3\nk1\nk2\n6 * (1)2\n(1)3\nk1\nk2\n6 * (2)2\n(2)3\nk1\nk2\n6 * (3)2\n(3)3\nk1\nk2\nContinue. Please keep solving the \nproblem until you need to query. (If you \nget to the answer, put it in \\boxed{}.)\n18.90\n\u2460 Ask to continue\n\u2461 Return valid result\n\u2462 Return error\nCorrect!\nUser \nProxy \nAgent \nGPT-4\nGPT-4\nGPT-4\nGPT-4\nUser Proxy Agent \nFigure 3: An example demonstrating how the user proxy agent handles different types of messages\nreceived from GPT-4 in MathChat. Specifically, the user proxy agent may respond in the following\nways: (1) asking the assistant to continue because no code block (i.e., query) is detected; (2) returning\nthe valid results from code execution; and (3) returning the error message from Python execution.\nNote that GPT-4 may change the query if the old code is undesired based on the messaged from the\nuser proxy agent. In the last step, GPT-4 corrects the query, and the final result is returned.\nLet's use Python to solve a math problem. \nFirst state the key idea to solve the problem. You may choose from three ways to solve the problem: \nCase 1: If the problem can be solved with Python code directly, please write a program to solve it. You can  \nenumerate all possible arrangements if needed. \nCase 2: If the problem is mostly reasoning, you can solve it by yourself directly. \nCase 3: If the problem cannot be handled in the above two ways, please follow this process: \nAfter all the queries are run and you get the answer, put the answer in \\boxed{}. \n\u2460 Tool-using\n\u2460\nCoding format\n\u2461\n1. Solve the problem step by step (do not over-divide the steps). \n2. Take out any queries that can be asked through Python (for example, any \ncalculations or equations that can be calculated). \n3. Wait for me to give the results. \n4. Continue if you think the result is correct. If the result is invalid or unexpected, \nplease correct your query or reasoning.\nQuery requirements: \nYou should always use the 'print' function for the output and use fractions/radical \nforms instead of decimals.\u00a0 \nYou can use packages like sympy to help you. \nYou must follow the formats below to write your code: \n```python \nMulti-step tool-\nusing and \nreasoning\nStep by step\nError handeling\nFacilitate dialogue\n\u2462 Final Answer\n\u2462 \n\u2461 Strategy \nSelection\nFigure 4: The prompt used in the initial message of the user proxy agent in MathChat. It instructs\nthe LLM assistant to solve a problem collaboratively with the user proxy agent in a certain way.\ninstruct the LLM assistant to solve the problem collaboratively with the user (effectively the user\nproxy agent in the MathChat system) in a certain desired manner. This framework is designed in this\nconversational manner in order to leverage the chat-optimized feature of state-of-the-art LLMs, e.g.,\nGPT-4. Another distinct benefit of this framework is that it enables multi-turn dialogues, which can\nbe particularly useful in addressing complex issues that require multi-step reasoning and tool using.\nPrompting and tool-using in MathChat. With proper modifications, effective prompting methods\nfrom existing research, such as CoT and tool-using, can be integrated into the MathChat framework.\n4\nSpecifically, for the prompt in the initial message, we aggregate multiple effective prompting tech-\nniques to instruct the LLM assistant. We present the designed prompt in Figure 4, which consists of\nthree main components.\n\u2022 Tool-using Prompt: This component prompts the LLM to use Python programming in the correct\nformat to tackle the problem. We use the \u2018query requirement\u2019 subsection to specify the coding\nformat so that the user proxy agent can parse the code and return the corresponding results.\n\u2022 Problem-Solving Strategy Selection Prompt: This component instructs the assistant to select from\nthree possible problem-solving strategies and to perform multi-stage reasoning and tool-using in\nthe last strategy. The problem-solving strategies include the following three cases, which cover\nthe most effective strategies from existing literature on math problem-solving. (Case 1) Write a\nPython program to solve the problem directly. This corresponds to single-stage tool-using methods\nsimilar to [10, 6, 3]. (Case 2) Solve the problem directly without Python. This strategy allows\nGPT-4 to exercise its inherent reasoning capacity to solve the problem at hand. (Case 3) Solve the\nproblem step by step and use Python to help with math operations. If the first two ways are not\nsuitable, we ask the model to choose this way to solve the problem. We craft a zero-shot version of\nthe multi-step tool-using prompt that allows the model to flexibly interleave between multi-step\nreasoning and Python code, similar to [37, 23, 28]. In this case, we also ask the model to handle\nerrors and unexpected results from the run of programs [21].\n\u2022 Final Answer Encapsulation Prompt: This component of the prompt instructs the LLM assistant to\nenclose the final answer in \\boxed{}, which will be used as an indicator to end the conversation.\nThis interaction between the LLM assistant (GPT-4) and the user proxy agent will not be ended\nuntil \\boxed{} is detected or max rounds of conversations are reached.\nWe acknowledge that there could be alternative ways to design the prompt. Fortunately, it is fairly\neasy to refine the prompt, for example, further enabling the usage of Wolfram Alpha in addition to\nPython, in our framework. We perform an empirical evaluation accordingly to test two alternative\nversions of the prompt in Section 5.2.\n4\nEvaluation\nDataset. We perform evaluations on all the level-5 (the highest difficulty level) problems from\nthe test set of MATH dataset [11]. Compared to other datasets for mathematical problems such as\nGSM8k [4], the level-5 problems are much more challenging and include the application of theorems\nand complex equation derivation. The MATH dataset has 7 categories of problems: Prealgebra,\nAlgebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.\nIn our evaluation, we remove Geometry from the evaluation to make it consistent with previous\nwork [7]. Most geometry problems from this dataset contain an Asymptote code to plot the figure.\nBut the currently available version of GPT-4 cannot accept image input. If the raw code is included,\nit can leak information to the model through exact numbers for the coordinates. Taking these issues\ninto consideration, we skip the evaluation on Geometry problems and remove ASY code from all the\nother categories (though this could result in a lack of enough information for some problems). The\ncorrect answer to each problem is deterministic and is enclosed in \\boxed{} in the dataset as ground\ntruth (but not disclosed to the methods solving the problem).\nEvaluated Methods. Most previous work uses few-shot examples to elicit the reasoning of LLMs\nand tool-using so that the model will imitate the examples to follow a desired format or pattern. It\nis important to select similar examples to the unanswered problem, and then annotate the examples\nto cover all the cases that the LLMs might encounter. A considerable amount of effort and careful\nconsideration are required in this process. For example, [14, 39] relies on elaborate examples to\nshowcase the patterns, [23] maintains an example library to choose examples, and [37] finetunes a\nsmaller model for annotation. Note that these methods use elementary math problems and it requires\neven more effort to prepare and choose the examples needed for challenging math problems. On the\nother hand, multiple existing studies [22, 1] reveal GPT-4\u2019s remarkable capacity to follow instructions.\nThus, we are interested in zero-shot prompting techniques that could enhance math-solving of GPT-4,\nwithout any example selection and annotations. Following this criterion, we evaluate our MathChat\nframework with the introduced prompt and the following methods which are all zero-shot methods:\nvanilla prompt, Program of Thoughts [3], and the Program Synthesis prompt from [6].\n5\n1. Vanilla prompting: GPT-4 can perform CoT reasoning without few-shot examples. To evaluate\nGPT-4\u2019s performance on solving the problem directly, we use a default prompt adapted from\nthe few-shot prompt in MATH dataset: \"Solve the problem carefully.\nPut the final\nanswer in \\boxed{}.\n{Problem}\".\n2. Program of Thoughts (PoT): We use the zero-shot PoT prompting from [3], which asks a model\nto write a Solver function to solve a problem and return the final answer directly.\n3. Program Synthesis (PS) prompting: Similar to PoT, the Program Synthesis (PS) prompting\nmethod [6] uses a prompt to ask the model to write a program to solve a problem: \"Write a\nprogram that answers the following question:\n{Problem}\"\nEvaluation Details. We compare different methods on GPT-4 with the default configurations from\nthe OpenAI API. In MathChat, we allow a max round of 15 messages between GPT-4 and the user\nproxy agent. The agent will explicitly ask GPT-4 to solve each step by itself if it detects errors from 3\nconsecutive executions. To avoid extremely long responses from the user proxy agent, the agent will\nreplace any result that exceeds 600 tokens with a warning text in the user message to ask the GPT-4\nto revise the previous code. We manually go through the answer of all the methods to count all the\ncorrect answers. For vanilla prompt, Program Synthesis, and MathChat, we ask GPT-4 to enclose\nthe final answer in \\boxed{}, so only the answers in the box will be extracted. For PoT, we follow\nthe original paper to take the return of the Solver function as the final answer.\n5\nResults\n5.1\nMain Results\nWe perform an evaluation on six categories of level-5 problems from the MATH dataset. We report\nthe problem-solving accuracy of different methods in each category in Table 1. Compared to vanilla\nprompting, which shows the native capability of GPT-4, using Python with PoT or PS improves the\noverall accuracy by around 10%. We can see this improvement mostly in the categories that involve\nmore number manipulations (Counting & Probability and Number Theory) and more challenging\ncategories (Intermediate Algebra and Precalculus). For Algebra and Prealgebra, however, PoT and\nPS have little improvement or even lead to lower accuracy. Compared with PoT and PS, MathChat\ncan further improve the total accuracy by around 6%, and have competitive performance across\nall the categories. It is worth highlighting that MathChat improves the accuracy in the Algebra\ncategory over other methods by around 15%. Considering all the methods, Intermediate Algebra\nand Precalculus can only be solved with a low accuracy rate of around 20%. More than half of the\nproblems from the other categories can be solved correctly by MathChat.\nAlgebra\nC.Prob\nI.Alg\nN.Theory\nPrealg\nPrecalc\nTotal\nProblem Count\n307\n123\n280\n154\n193\n135\n1192\nMathChat\n59.93%\n52.03%\n17.85%\n60.39%\n60.10%\n19.26%\n44.71%\nPoT\n42.67%\n50.41%\n17.50%\n54.55%\n52.33%\n16.30%\n37.67%\nPS\n43.32%\n44.71%\n20.36%\n61.03%\n55.96%\n18.52%\n39.60%\nVanilla\n46.58%\n25.20%\n2.86%\n28.57%\n54.92%\n7.41%\n28.69%\nTable 1: Accuracy on all the problems with difficulty level-5 from different categories of the MATH\ndataset with different methods.\n5.2\nAdditional evaluation on MathChat with alternative prompts\nMathChat allows easy incorporation of different prompts and tools. We perform an additional\nevaluation to test two alternative initial prompts with MathChat to demonstrate its extensibility. (1) A\nsimplified prompt with Python: In this alternative, we only keep the \u2018query requirements\u2019 subsection\nfor python coding format and the step-by-step tool-using (i.e., case 3) from the default prompt. (2)\nA simplified prompt with Python and Wolfram Alpha: In this alternative, on top of alternative (1), we\nadd Wolfram Alpha, a computational engine, as an additional tool for the LLM assistant to choose\nfrom. Details of these two alternative prompts are in Appendix B. We perform an evaluation on\nrandomly sampled 50 examples from each of the six problem categories. We also include results from\n6\nAlgebra\nC.Prob\nI.Alg\nN.Theory\nPrealg\nPrecalc\nTotal\nProblem Count\n50\n50\n50\n50\n50\n50\n300\nMathChat w/ Two-tools\n33\n22\n6\n27\n29\n10\n127\nMathChat w/ Python\n26\n19\n7\n22\n31\n13\n118\nMathChat\n30\n24\n8\n34\n28\n10\n134\nPoT\n20\n19\n9\n24\n24\n7\n103\nPS\n17\n19\n12\n31\n26\n5\n110\nVanilla\n26\n13\n1\n17\n21\n1\n79\nTable 2: Additional evaluation of MathChat with two alternative prompts. 50 problems are sampled\nfrom each problem category for this evaluation. MathChat w/Two-tools and MathChat w/ Python\nare two alternative prompts.\nIn how many ways can one write 1, 2, \n3, 4, 5, and 6 in a row so that given \nany number in the row, all of its divisors \n(not including itself) appear to its left?\n   Return: 22 18 0\nKey idea: Expand the expression \n \nusing binomial theorem \u2026 \n(\n2 +\n3)3\n\u2026\u2028\nWe can solve this problem by enumeration \nsince there aren't many cases to go \nthrough. \n\u2028\nConsider the fact that we need to put '1' at \nthe beginning of the row since \nthere are no divisors of '1'. Now we have \nthe base for all other numbers to \ufb01t. \nNow let's enumerate the possibilities by \nplacing these numbers in a valid sequence: \n\u2026 \nType 1. The plan is to enumerate all \npossible outcomes, which includes \n5*4*3*2*1=120 sequences. The space is \nbig and it is not writing a program to solve \nit\nThe number \ncan be written in the \nform \n, where , , and  are \nintegers.  What is \n?\n(\n2 +\n3)3\na\n2 + b\n3 + c\n6\na b\nc\na + b + c\nimport \u2026\na, b, c = symbols('a b c')\nexpr = (sqrt(2) + sqrt(3))**3\nexpan_expr = expr.expand()\na_term = 2 * expan_expr.coeff(sqrt(2), 1)\nb_term = 2 * expan_expr.coeff(sqrt(3), 1)\nc_term = expanded_expr.coeff(6, 1)\nprint(a_term, b_term, c_term)\nType 2. the `2 * \u2026 ` in the code came out of not \nwhere.  \nIf the 2s for both terms are removed, the \nanswer can be corrected to:  \n                \n \na + b + c = 11 + 9 + 0 = 20\n The answer is a + b + c = 22 + 18 + 0 = 40\nProblem\nProblem\nGPT-4\nGPT-4\nGPT-4\nUser Proxy Agent \nFigure 5: One example is selected for each of the first two failures. Type 1 failure: in the first\nproblem, the assistant fails to give a plausible plan. It chooses to enumerate all sequences, and it\ndoes not use tools to help with it. Type 2 failure: the second problem shows that the model fails to\ngive the correct code to solve the problem, while it follows the problem requirements and the overall\ndirection is correct. With minor changes to the code, the final answer can be correct.\nother methods on the sample problems for comparison in Table 2. MathChat still performs better\nthan other methods with the two newly crafted prompts. With MathChat, the step-by-step prompt\nthat allows both Python and Wolfram performs the best on Algebra, while the new prompt with only\nPython solves the most problems on Prealgebra and Precalculus, but has a worse performance on\nNumber Theories. Overall, MathChat with the default prompt still performs the best.\n6\nFailure Analysis\nIn this section, we analyze failure cases to gain insight to the reason of failure and compare different\nmethods in depth.\n7\nIf \n has roots of the form \n, where \n, then \n for positive integers \n. Find \n.\nx2 + bx + b + 3 = 0\n\u2212b \u00b1\n5\n2\nb > 0\nb = m +\nn\nm, n\nm + n\nProblem\n\u2026\ndef solver():\n\u2026\n    # We can find the value of b by \nequating the product of our roots, \nroot1 and root2, with the product \nobtained from Vieta's formulas:\n    product_roots = root1 * root2\n    vieta_product = -b - 3\n    # Now we'll solve the equation \nproduct_roots = vieta_product for b:\n    equation = sp.Eq(product_roots, \nvieta_product)\n    solution = sp.solve(equation, b)\n    # Since b > 0, we'll take the \npositive solution:\n    b_value = [sol for sol in \nsolution if sol > 0][0]\n\u2026\nTo solve the given problem, we can follow \nthese steps: 1. Use the quadratic formula for \nthe given equation. 2. Compare the roots in the \ngiven form with the ones obtained from the \nquadratic formula. 3. Solve the resulting \nequation for b. \nHere's the python code to find the value of b: \n\u2026\n# Print the positive b\nfor sol in solution_b:\n    if sol[0] > 0:\n        positive_b = sol[0]\nprint(\"Value of b:\", positive_b)\n# Express b as m + sqrt(n)\nm, n = symbols('m n')\nm_expr, n_expr = solve(Eq(positive_b, \nm + sqrt(n)), (m, n))\n\u2026\nPS\n\u2026 \nThus, we have the equation:  \n \nNow, we solve for  using the quadratic \nformula: \n,  \n \nSince we are given that  is positive, we \ncan disregard the negative square root, \nand we have: \n \nThus, \n and \n, so \n. \nb + 3 = b2 \u2212 5\n4\n, 0 = b2 \u2212 4b \u2212 17\nb\nb =\n\u2212(\u22124) \u00b1\n(\u22124)2 \u2212 4 \u22c5 1 \u22c5 (\u221217)\n2 \u22c5 1\nb = 4 \u00b1\n68\n2\nb\nb = 4 +\n68\n2\nm = 2\nn = 68\nm + n = 70\nVanilla\nPoT\n\u2460 \n is missing  \n(\u22124)2\n\u2460\n\u2461\n\u2462\n\u2463\n\u2461 should be \nb + 3  \u2462 TypeError!  \n\u2463 ValueError!\nFigure 6: An example where MathChat is correct and others fail. All other methods fail due to\nType 2 failure. 1. Vanilla prompt: when calculating b, didn\u2019t include \u221242. 2. PoT: it first calculates\nvieta_product wrong, even is this is corrected, another TyperError will occur. 3. PS: it solves\nfor b correctly, but gets an ValueError when using the program to extract m and n.\n6.1\nFailure reasons\nWe first summarize the failure cases according to the reasons for failure, based on the systematic math\nproblem-solving process established by George P\u00f3lya [25]. The process consists of (1) understanding\nthe problem; (2) devising a plan; (3) executing the plan; (4) reviewing and extending. We observe\nfailures of the following three main types.\nType 1. Failure to devise or select an appropriate plan or path to solve the problem. This type\nencompasses cases where GPT-4 fails to provide a proper way to approach the problem. In these\ninstances, the answer is consistently incorrect, even though each individual step of the calculation\nis accurate. Failure cases of this type are typically tricky problems that even math experts find\nchallenging.\nType 2. Failure to flawlessly execute the devised plan. Math problem-solving requires rigorous\nand precise reasoning steps. A minor error in calculation or symbol manipulation could be fatal\nand lead to a wrong answer. This type of error is considered \u2018minor\u2019 because they are easier to be\nfixed. This type of error contributes to a fair amount of failures, where the overall direction of the\nproblem-solving is correct, but one mistake in a basic derivation leads to the wrong answer. Note that\nan error for Python execution is also included in this type, where GPT-4 fails to write a runnable code\nleading to the error.\nType 3. Other technical errors. There are other technical errors causing the failure. One example of\nsuch failure is lack of information due to the removal of ASY code.\nIn Figure 5, we give one example for each of the first two types of failures. More details for these\nexamples and additional examples are in Appendix E.\n6.2\nFailure cases using different methods on GPT-4\nIn Table 3, we present the frequency of successful outcomes for each method (represented in each\nrow), while all other methods fail, categorized according to different problem instances. This table\nserves to highlight the distinct advantage that a particular method exhibits across various problem\ncategories. Similarly, in Table 4, we summarize the frequency of instances where one method fails\nwhile all other methods succeed. A high number in this table signifies the unique disadvantage of the\nmethod in question.\n8\nThese statistics demonstrate the robustness of MathChat in comparison to other methods. MathChat\nleverages conversation to enhance error correction when utilizing external tools, thereby hypothesizing\na reduction in failures within the third type.\nAlgebra\nC.Prob\nI.Alg\nN.Theory\nPrealg\nPrecalc\nTotal\nMathChat\n27\n8\n21\n13\n6\n9\n84\nPoT\n11\n9\n19\n6\n3\n5\n53\nPS\n12\n6\n22\n11\n10\n8\n69\nVanilla\n12\n4\n5\n3\n10\n3\n37\nTable 3: The number of problems where one math succeeds, and all the other methods fail (the higher\nthe better for the concerned method in each row).\nAlgebra\nC.Prob\nI.Alg\nN.Theory\nPrealg\nPrecalc\nTotal\nMathChat\n6\n2\n0\n5\n4\n1\n18\nPoT\n22\n5\n0\n6\n18\n2\n53\nPS\n17\n5\n1\n5\n14\n0\n42\nVanilla\n16\n19\n11\n28\n19\n5\n98\nTable 4: The number of problems where one method fails and all the other methods succeed (the\nlower, the better for the concerned method in each row).\nWe take one example from each table to analyze the failures of these methods. We first take an\nAlgebra problem that MathChat succeeds but others fail to analyze the failures of other methods\n(Figure 6) For this problem, other methods fail to execute this plan without any mistakes, causing the\nsecond type of failure. While vanilla prompting has a calculation error, the other two methods get\nexecution errors from running the code. We run these methods three more times and they still fail\nto solve the problem. From Table 4, we take the only Precalculus instance that MathChat is wrong\nwhile all the other methods are correct. Through investigating, we find that MathChat gives longer\nsolutions to the problem than all the other methods, and also contains Type 2 failures. This problem\nmight indicate a potential correlation between the accuracy and length of responses. We present more\ndetails in investigating this possible correlation and also the Precalculus example in Appendix E.\n7\nSummary and future work\nIn this paper, we evaluate different ways of using GPT-4 in advanced math problem-solving. We\nintroduce MathChat, a conversational framework based on GPT-4, that allows the model to interact\nwith a proxy user agent to solve math problems. MathChat is designed for chat-optimized models like\nGPT-4, and it is extensible to be used with different prompts and different tools with minimal effort.\nBased on the framework, we also derive a prompt that aggregates previous prompting techniques to\nbe used on MathChat.\nOur evaluation on level-5 problems from the MATH dataset demonstrates the effectiveness of\nMathChat to solve more complex and challenging problems. Despite its improvements over previous\nmethods, the results show that complex math problem is still challenging for recent powerful LLMs,\nlike GPT-4, even with help from external tools. For one, GPT-4 fails to solve a fair amount of\nproblems due to error in execution. While MathChat tries to improve this, further work can be done\nto enhance this framework, for example, training an assistant model on top of the user proxy agent to\ngive more specific instructions [40, 38], or to derive new prompts and incorporate new tools.\nHuman-in-the-loop extension. MathChat with GPT-4 has great potential to be adapted into a copilot\nsystem to assist people in solving mathematics problems. MathChat is natural in its design to assist\npeople in communicating with GPT-4. With a preset prompt in MathChat, the user can directly input\na problem and get a step-by-step response. Moreover, the MathChat system could be designed to\nstop at each step to allow the user to override the message of the proxy agent. We envision the system\ncould allow users to engage in the math-solving process and facilitate learning.\n9\nReferences\n[1] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[2] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[3] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts\nprompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv\npreprint arXiv:2211.12588, 2022.\n[4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n[5] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\nlanguage models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.\n[6] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin\nLiu, Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and\ngenerates university math problems by program synthesis and few-shot learning at human level.\nProceedings of the National Academy of Sciences, 119(32):e2123433119, 2022.\n[7] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin\nLiu, Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and\ngenerates university math problems by program synthesis and few-shot learning at human level.\nProceedings of the National Academy of Sciences, 119(32):e2123433119, 2022.\n[8] Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting for\ndecomposing complex questions. arXiv preprint arXiv:2212.04092, 2022.\n[9] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz,\nPhilipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of\nchatgpt. arXiv preprint arXiv:2301.13867, 2023.\n[10] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435,\n2022.\n[11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\narXiv preprint arXiv:2103.03874, 2021.\n[12] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using\nlarge language models. arXiv preprint arXiv:2303.05398, 2023.\n[13] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le\nBras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive\nexplanations. arXiv preprint arXiv:2205.11822, 2022.\n[14] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and\nAshish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks.\narXiv preprint arXiv:2210.02406, 2022.\n[15] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\n[16] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving\nquantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.\n10\n[17] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen.\nOn the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336,\n2022.\n[18] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by ratio-\nnale generation: Learning to solve and explain algebraic word problems. arXiv preprint\narXiv:1705.04146, 2017.\n[19] Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi,\nand Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. arXiv\npreprint arXiv:2110.08387, 2021.\n[20] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n[21] Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victo-\nria Lin. Lever: Learning to verify language-to-code generation with execution. arXiv preprint\narXiv:2302.08468, 2023.\n[22] OpenAI. Gpt-4 technical report, 2023.\n[23] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer,\nand Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language\nmodels. arXiv preprint arXiv:2303.09014, 2023.\n[24] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple\nmath word problems? arXiv preprint arXiv:2103.07191, 2021.\n[25] George Polya. How to solve it: A new aspect of mathematical method. Number 246. Princeton\nuniversity press, 2004.\n[26] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\nMeasuring and narrowing the compositionality gap in language models.\narXiv preprint\narXiv:2210.03350, 2022.\n[27] Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, Peter J Liu,\nSharan Narang, Wei Li, and Yanqi Zhou. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. 2019.\n[28] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n[29] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis\nSaravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language\nmodel for science. arXiv preprint arXiv:2211.09085, 2022.\n[30] Boshi Wang, Xiang Deng, and Huan Sun. Shepherd pre-trained language models to develop a\ntrain of thought: An iterative prompting approach. arXiv preprint arXiv:2203.08383, 2022.\n[31] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\nSelf-\nconsistency improves chain of thought reasoning in language models.\narXiv preprint\narXiv:2203.11171, 2022.\n[32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n[33] Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are\nreasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022.\n[34] Eugene P Wigner. The unreasonable effectiveness of mathematics in the natural sciences. In\nMathematics and science, pages 291\u2013306. World Scientific, 1990.\n11\n[35] Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. Seqzero:\nFew-shot compositional semantic parsing with sequential prompts and zero-shot models. arXiv\npreprint arXiv:2205.07381, 2022.\n[36] Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, and Xiaodan Liang. Logicsolver: Towards\ninterpretable math word problem solving with logical prompt-enhanced learning. arXiv preprint\narXiv:2205.08232, 2022.\n[37] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\n[38] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.\n[39] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Olivier Bousquet, Quoc Le, and Ed Chi.\nLeast-to-most prompting enables\ncomplex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n[40] Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang.\nSolving math word problem via cooperative reasoning induced language models. arXiv preprint\narXiv:2210.16257, 2022.\n12\nA\nSupplementary Details on the User Proxy Agent\nThe user proxy agent in MathChat takes a problem and put it in a message with an initial prompt, and\nsends the message to the LLM assistant. Then the agent is responsible for extracting and executing\nqueries and also providing additional guidance. Here are all functionalities of the user proxy agent\n(the workflow is shown in Figure 2):\n1. Extract Queries: The user proxy agent needs to match the pattern specified in the initial\nmessage to extract all tool-using queries. With our designed prompt, the agent matches all code\nblocks in the message and extracts the code.\n2. \"Continue\": If no query is detected in the message, the agent will send this message\nto the LLM assistant: \"Continue.\nPlease keep solving the problem until you\nneed to query.\n(If you get to the answer, put it in \\boxed{}.\". This asks\nthe assistant to keep solving the problem and reminds it to end the conversation by putting the\nanswer in the box.\n3. Query Execution: Any tool-using queries extracted will be executed sequentially. For Python,\nwe set the time limit to be 5 seconds for execution. As shown in Figure 1, the previous valid\ncode is recorded. All the execution results will be concatenated sequentially (including errors).\n4. Recurrent Error detection: If LLM assistant sends 3 consecutive errors, the user proxy agent\nwill replace the third error message with this message: \"Please revisit the problem\nstatement and your reasoning.\nIf you think this step is correct, solve\nit yourself and continue the next step.\nOtherwise, correct this step.\".\nTo avoid sticking to this error, the LLM assistant is asked to solve this step without tools and\nmove on.\n5. Repetitive results: This is not shown in the workflow, but the agent also detects another situation\nwhere the LLM assistant gives the same tool-using query from the last one or the result is the\nsame as the last query. Then the message is appended to the execution result to remind the\nassistant to avoid giving the same queries: \"Your query or result is same from the\nlast, please try a new approach.\".\n6. Long query results: It is possible that LLM assistant requires a query result that is too long to be\npassed back (such as long results from the print function in a for loop in Python). The proxy agent\nwill replace any query result that is longer than 2000 chars (approximately 600 tokens) with this\nmessage: \"Your requested query response is too long.\nYou might have made\na mistake.\nPlease revise your reasoning and query.\".\nIn MathChat, if the tool-using query and the end indicator are detected in the same message, the\nresult from the query will be returned, and the conversation will continue. This is to prevent early\nstops where the LLM agent predicts the execution result and puts it in a box other than waiting for\nthe result.\nB\nSupplementary Details on Experiment Settings\nThe code is in this GitHub1 repository. In our experiment, we use the default configuration from\nOpenAI, specifically temperature=1, and max_token=inf (See OpenAI API Reference for more\ndetails). We use the system message \"You are a helpful assistant\" for vanilla prompt, PS, and\nMathChat. For PoT, we do not add this system message, since our evaluation shows that PoT\nwithout system message has a better performance. We discuss the effect of system message below in\nSection D.\nHere is the prompts for PoT [3], PS [6], and the additional two prompts we designed:\n\u2022 Program of Thoughts (PoT). See Figure 7. The whole prompt uses the Python code format,\nwhere information such as the problem or instructions is in the comments.\n\u2022 Program Synthesis (PS). The prompt for PS is \"Write a program that answers the\nfollowing question:\n{Problem}\". Since the definition of \"program\" is unclear and\n1https://github.com/kevin666aa/FLAML/tree/gpt_math_solver/flaml/autogen/math\n13\nimport math \nimport numpy as np \nimport sympy as sp \n# Question: {problem} \n# Answer this question by implementing a solver() function. \ndef solver(): \n    # Let's write a Python program step by step, and then return the answer \n    # Firstly, we need define the following variable: \nFigure 7: PoT prompt. Comparing to the original prompt from [3], we add \"import sympy as\nsp\" that gives the LLM assistant hint to use the sympy library. The placeholder \"{problem}\" will\nbe replaced with the actual problem.\nLet's use Python to solve a math problem. \nQuery requirements: \nYou should always use the 'print' function for the output and use fractions/radical forms instead of decimals. \nYou can use packages like sympy to help you. \nYou must follow the formats below to write your code: \n```python \n# your code \n``` \nPlease follow this process: \n1. Solve the problem step by step (do not over-divide the steps). \n2. Take out any queries that can be asked through Python (for example, any calculations or equations that  \ncan be calculated). \n3. Wait for me to give the results. \n4. Continue if you think the result is correct. If the result is invalid or unexpected, please correct your query or \nreasoning. \nAfter all the queries are run and you get the answer, put the answer in \\\\boxed{}. \nFigure 8: The Python prompt used on MathChat from Section 5.2.\nsometimes the LLM assistant won\u2019t write code to solve the problem, we add the keyword\n\u2019Python\u2019 in front of \u2019program\u2019. After the message is returned, we used the proxy agent to return\nthe result (by default, GPT-4 would return the code in the code block). Then we send another\nmessage to the model with the Python execution result and ask it to enclose the final answer:\n{Return from Python}.\nPlease put the final answer in \\boxed{}. See Figure\n10 for an example of the whole process.\n\u2022 Python prompt (w/ MathChat). See Figure 8.\n\u2022 Two-tools prompt (w/ MathChat). See Figure 9.\nC\nAdditional Discussion on Related Work\nWhile there are only a few methods targeting math problems, more prompt techniques are proposed\nto improve reasoning tasks. We summarize the three main ways that can be used to improve math\nsolving: Chain of Thought prompting, multi-stage prompting, and tool-using:\nChain-of-thought prompting. Chain-of-thought (CoT) prompting [32, 15] is a type of prompting\ntechnique found useful for improving LLMs\u2019 performance in math problem-solving. [32] propose\nthe initial CoT technique with step-by-step answer examples and show that this technique could lead\nto better performance on math word problems. [15] further demonstrates that CoT can be done in a\nzero-shot manner by simply attaching \"Let\u2019s think step by step\" to the questions fed to LLMs. This\n14\nLet's use two tools (Python and Wolfram alpha) to solve a math problem. \nQuery requirements: \nYou must follow the formats below to write your code: \nFor Wolfram Alpha: \n```wolfram \n# your wolfram query \n``` \nFor Python: \n```python \n# your code \n``` \nWhen using Python, you should always use the 'print' function for the output and use fractions/radical forms  \ninstead of decimals. \nYou can use packages like sympy to help you. \nPlease follow this process: \n1. Solve the problem step by step (do not over-divide the steps). \n2. Take out any queries that can be asked through Python or Wolfram Alpha and select the most suitable tool to be \nused (for example, any calculations or equations that can be calculated). \n3. Wait for me to give the results. \n4. Continue if you think the result is correct. If the result is invalid or unexpected, please correct your query or \nreasoning. \nFigure 9: The Two-tools prompt used on MathChat from Section 5.2. The added requirement\ncompared to the Python prompt is highlighted in yellow. This prompt allows the LLM assistant to\nchoose from Python or Wolfram Alpha.\nsimple CoT technique also leads to better performance on math problem-solving tasks when used in\nconjunction with GPT-3.\nMulti-stage prompting Another effective way is to prompt LLMs to solve math problems in a\nmulti-stage manner [8, 14, 26, 5, 13, 39, 23, 37, 35, 30]. The most common strategy for multi-stage\nprompting is divide-and-conquer, which decomposes complex problems into several subproblems.\nFor example, least-to-most prompting [39] and Decomposed prompting [14] propose to break down\na complex problem into smaller subproblems, and the subproblems are solved sequentially to reach\nthe final solution. [19] first uses GPT-3 to generate knowledge statements for a problem and then\npresents both the question and generated knowledge to another model (T5-11b [27], etc.) to answer\nthe question.\nTool-using. As we discussed in Section 2, a particular effort for math solving is to use Python\ncode [10, 3, 6]. [28, 23, 37] further combine multi-stage prompting with external tools in the usage\nof LLMs on math problem-solving. Specifically, [28] trains a Toolformer that can decide when\nto send API calls. [37, 23] both use few-shot prompting to interleave step-by-step reasoning and\ntool-using. ReAct [37] finetunes a smaller model in a bootstrapping manner to help annotation, while\nART [23] maintains a library of demonstrations, such as using calculators and retrieves the most\nsimilar examples for prompting a new problem.\nOther methods.\nThere exist other methods that are potentially useful to improve math problem-\nsolving with LLMs. Self-consistency [31], built on top of CoT, samples several different reasoning\npaths for a problem and selects the answer with the majority vote. [17] extends self-consistency by\ntraining a verifier to verify the correctness of each step. [20] first generates an output using recent\nLLMs like GPT-3.5 and uses the same model to give feedback on the output, then the model is asked\nto refine the output based on the feedback. We also refer to [16, 29] for training LLM on Science\nproblems.\nD\nSupplementary Experiments and Results\nWe further evaluate a vanilla few-shot prompt, PoT with and without system message, and Vanilla\nprompt with and without system message on the randomly selected 50 problems from each category\nand present the results in Figure 5.\n15\nWrite a python program that answers the following question: A point \n is 5 units away from the point \n. What is the product of all \npossible integer values for ? \n(2,n)\n(\u22121,1)\nn\nYou can solve this problem using the distance formula between two points:  \n \nIn this case, the distance is 5 units, and the two points are (2, n) and (-1, 1): \n \nNow, you can write a Python program to \ufb01nd all the possible integer values of n: \nThis Python program \ufb01rst de\ufb01nes a function `\ufb01nd_possible_n_values` that iterates over a range of integer values for n, calculates the distance \nusing the given formula, and checks whether the distance is equal to 5. It then appends the valid integer values of n to a list of possible n values. \nAnother function, `\ufb01nd_product_of_n_values`, is then values of n \u2026 (Explanation of Code) \ndistance = sqrt((x2 \u2212 x1)2 + (y2 \u2212 y1)2)\n5 = sqrt((2 \u2212 (\u22121))2 + (n \u2212 1)2)\nProblem\nGPT-4\nfrom math import sqrt\ndef find_possible_n_values():\n    possible_n_values = []\n    for n in range(-10, 10):\n        distance = sqrt((2 - (-1))**2 + (n - 1)**2)\n        if distance == 5:\n            possible_n_values.append(n)\n    return possible_n_values\ndef find_product_of_n_values(possible_n_values):\n    product = 1\n    for n in possible_n_values:\n        product *= n\n    return product\npossible_n_values = find_possible_n_values()\nproduct = find_product_of_n_values(possible_n_values)\nprint(\"The possible integer values of n are:\", possible_n_values)\nprint(\"The product of these possible integer values is:\", product)\nThe possible integer values of n are: [-3, 5] \nThe product of these possible integer values is: -15 \nPlease put the final answer in \\boxed{}.\nUser Proxy Agent \nThe product of the possible integer values of n is: -15\nFigure 10: An example of the process of PS. The query result will be returned to the LLM assistant\nand ask it to put the answer in box. The process of PS is exactly the same as MathChat when the\nassistant in MathChat chooses to solve the problem with one Python program.\nIn the few-shot prompt, we randomly select 3 level-5 problem-solution pairs from the train set. These\nexamples are selected from each category and are used for all the problems from that category.\nThe vanilla few-shot prompt starts with \"Solve the problem carefully.\nPut the final\nanswer in \\boxed{} just like the vanilla prompt, and then three \"Problem:\n...\nSolution:\n...\" pairs are attached. Compared with the vanilla prompt, adding three additional examples does\nnot make any obvious difference, and the overall performance is slightly worse.\nFrom our experiment, we also notice that the system message affects the performance of the LLM\nassistant. However, the impact significantly differs between methods. As shown in Table 5, using a\nsystem message is crucial for the Vanilla prompt: adding the system message doubles the success\nrate compared to the one without a system message. However, for PoT, adding the system message\nonly slightly increases the performance. We add a further evaluation on all the level-5 problems\nand find the PoT with the system message has an overall accuracy of 35.82%, which is lower than\nthe accuracy of PoT without the system message (37.67% as shown in the main results in Table 1).\nWe hypothesize the difference in the prompt format across different methods is the reason for this\nbehavior. The method with the Vanilla prompt imitates a conversation between the LLM assistant and\nhumans via natural language, but PoT prompt is in Python code format, which explicitly directs the\nmodel for code completion. Thus, the system message \"you are a helpful assistant\" is more suitable\nfor Vanilla prompt but doesn\u2019t align with PoT. More investigation is needed to understand the effect\nof system messages.\n16\nAlgebra\nC.Prob\nI.Alg\nN.Theory\nPrealg\nPrecalc\nTotal\nProblem Count\n50\n50\n50\n50\n50\n50\n300\nMathChat\n30\n24\n8\n34\n28\n10\n134\nPS\n17\n19\n12\n31\n26\n5\n110\nPoT w/o sys\n20\n19\n9\n24\n24\n7\n103\nPoT w/ sys\n18\n23\n9\n23\n29\n7\n109\nVanilla w/o sys\n14\n4\n0\n4\n13\n1\n35\nVanilla w/ sys\n26\n13\n1\n17\n21\n1\n79\nFew-shot (k=3)\n21\n6\n2\n18\n24\n1\n72\nTable 5: Results for few-shot prompt, PoT w/ and w/o system message, Vanilla prompt w/ and w/o\nsystem message.\nAlgebra\nC.Prob\nI.Alg\nN.Theory\nPrealg\nPrecalc\nTotal\nCategory\n0\n10\n20\n30\n40\n50\n60\n70\nPercentage Correct\nNo Query\nHave Invalid Queries\nAll Valid Queries\nFigure 11: Success rate of MathChat under different forms of problem-solving processes: 1. the\nLLM assistant solves the problem without making any tool-using queries. 2. The assistant makes\nqueries and has at least one invalid query 3. All queries made are valid.\nE\nSupplementary Failure Analysis\nE.1\nFailure under different forms of problem-solving processes in MathChat\nThe default prompt in MathChat allows the LLM assistant to choose from different forms of problem-\nsolving processes to solve the problem, and we investigate how choosing different forms could affect\nthe performance. We plot the correct rate when a problem from each category is solved with three\nforms of problem-solving approaches depending on the existence and validity of the query in the\ngenerated solution in Figure 11: 1. The LLM Assistant doesn\u2019t make any tool-using queries (Python)\nwhen solving the problem. 2. The assistant makes one or more queries, but at least one query is\ninvalid. 3. The assistant makes all valid queries. It is shown in the plot that using Python correctly\ncould significantly increase the correct rate, while doesn\u2019t use Python is worse than using Python but\nhaving invalid queries. The results in Figure 11 show that especially for intermediate algebra and\nprealgebra, the gap in accuracy between \"using no query\" and \"have invalid query\" is large, indicating\nthat using Python is very helpful to solve problems from the two categories.\nE.2\nExamples of 3 types of failures\nIn Section 6.1, we summarize 3 main type of failures: type 1: failure to devise an appropriate plan.\ntype 2: failure to flawlessly execute the plan. type 3: other technical errors. We give one additional\nexample for type 1 error and type 2 error, and an example where the removal of ASY code leads to a\n17\nProblem: Suppose \n and \n. What is the largest possible value of \n?\nx \u2208 [\u22125, \u2212 3]\ny \u2208 [2,4]\nx + y\nx \u2212 y\nTo \ufb01nd the largest possible value of \n, we need to maximize the value of the \nfraction. We can \ufb01rst consider how the numerator and denominator are a\ufb00ected by the \ngiven intervals of  and . \nSince \n and \n, the numerator \n will be in the range \n \nand the denominator \n will be in the range \n. Therefore, we can try to \nmaximize the value of the fraction by choosing the largest possible value for the \nnumerator while choosing the smallest possible value for the denominator. \nHere's a Python program using the sympy package \nto calculate the largest possible value of the fraction \n: \nPlease run the code and provide the result.\nx + y\nx \u2212 y\nx\ny\nx \u2208 [\u22125, \u2212 3]\ny \u2208 [2,4]\nx + y\n[\u22123,1]\nx \u2212 y\n[\u22129, \u2212 5]\nx + y\nx \u2212 y\nProblem\nGPT-4\n  -1/7\nUser Proxy Agent \nType 1 Failure. The LLM assistant first calculates the range for numerator and \ndenominator separately and then chooses different  and  for numerator and denominator to \nmaximize the value, which is the wrong direction.  \nA correct solution: Maximizing \n is equivalent to maximizing \n. \nNote that \nand \n are always positive, so to maximize this expression, we take \n the \nsmallest possible value of\n \nThen maximizing \n is equivalent to maximizing \n \nNote that \n is always positive, so to maximize this expression, we take \n Hence, the \nmaximum value is \n \nx\ny\nx + y\nx \u2212 y\nx + y\nx \u2212 y + 1 =\n2x\nx \u2212 y = \u22122x\ny \u2212 x\n\u22122x\ny \u2212 x\ny = 2,\ny .\nx + 2\nx \u2212 2\nx + 2\nx \u2212 2 \u2212 1 =\n4\nx \u2212 2 = \u2212\n4\n2 \u2212 x .\n2 \u2212 x\nx = \u2212 5.\n\u22125 + 2\n\u22125 \u2212 2 = 3\n7\nfrom sympy import symbols, Rational\nx, y = symbols('x y')\nnumerator = x + y\ndenominator = x - y\nfraction = numerator / denominator\nx_range = (-5, -3)\ny_range = (2, 4)\nmax_num = numerator.subs(x, x_range[1]).subs(y, y_range[1])\nmin_denom = denominator.subs(x, x_range[0]).subs(y, y_range[0])\nmax_value = Rational(max_num, min_denom)\nprint(max_value)\nProblem: The four-digit number \n is divisible by nine, with  the tens \ndigit and  the units digit. How many different such four-digit numbers could \n represent?\n25AB\nA\nB\n25AB\nKey Idea: A number is divisible by 9 if the sum of its digits is divisible by 9. \nWe will now write a Python program to \ufb01nd all the possible values of  and  \nsuch that \n is divisible by 9. For that, we will iterate through all possible \nvalues for each digit and check if the sum of digits \nis divisible by 9. \nNow, we will run this Python code to \ufb01nd possible values of  and . \n  \nA\nB\n25AB\nA\nB\nGPT-4\n[(1, 1), (2, 0), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, \n2)]\nUser Proxy Agent \nType 2 Failure. The LLM assistant has the correct idea to iterate \nand find all numbers that sum of digits is divisible by 9. But for A, it didn\u2019t \ninclude 0 as a possible digit, missing the number 2502 where  (A,B) = (0, \n2). If change \u201crange(1, 10)\u201d to \u201crange(10)\u201d, the final answer is 11, which is \ncorrect. \nfrom sympy import Symbol\n# Initialize possible values\npossible_values = []\n# Iterate through all possible values of A (1 to 9) and B (0 to 9)\nfor A in range(1, 10):\n    for B in range(10):\n        # Calculate the sum of the digits\n        sum_digits = 2 + 5 + A + B\n  \n     \n        # Check if the sum of the digits is divisible by 9\n        if sum_digits % 9 == 0:\n            possible_values.append((A, B))\nprint(possible_values)\n\u2026 Our final answer is 10.\nProblem\nGPT-4\nGPT-4\nUser Proxy Agent \nFigure 12: Additional example of Type 1 failure (Fail to devise a proper plan) and Type 2 failure (Fail\nto execute the plan flawlessly).\nGPT-4\nProblem: The equation \nwhere \n and  are integers, is shown below. What is \n? \n[asy] \nimport graph; size(8.14cm); real lsf=0.5; pen dps=linewidth(0.7)+fontsize(10); defaultpen(dps); pen ds=black; real \nxmin=-2.52,xmax=5.62,ymin=-4.28,ymax=3.32; \npen \ncqcqcq=rgb(0.75,0.75,0.75); \n\u2026 \n[/asy] \ny = x + A\nBx + C ,\nA, B,\nC\nA + B + C\nProblem\nType 3 Failure. Since the code between [asy] and [/asy] is removed, it is not possible to solve the \nproblem. The information from the description is not enough to solve the problem.\nProblem\nFigure 13: An example of Type 3 failure where the ASY code is removed.\nleak of information (Figure 12, Figure 13). We note that among all the problems, the ASY code from\n72 problems are removed, but 12 problems can still be solved correctly.\nE.3\nFailure under different methods\nWe present the precalculus example where MathChat fails but all other methods success (Figure 15,\nFigure 16, Figure 17). The results from PS and PoT show that it is easy to get this problem correct\nwith Python (using the sympy.simplify function). However, in MathChat, the LLM assistant\nchooses to solve the problem via direct reasoning. Both MathChat and vanilla prompt solve this\n18\n0\n200\n400\n600\n800\n1000\n1200\n1400\nLength\n0\n50\n100\n150\n200\n250\n300\n350\nFrequency\nGiven Solution\nMathChat Correct\nMathChat Wrong\n0\n200\n400\n600\n800\n1000\n1200\n1400\nLength\n0\n20\n40\n60\n80\n100\nFrequency\nGiven Solution\nMathChat Correct\nMathChat Wrong\nFigure 14: Distribution of solution length of both correctly and incorrectly solved problems in\nMathChat. The distribution of length of the given solution (ground truth) is also shown. The left\nfigure represents the distribution of the less challenging categories and the right figure represents\nproblems from Intermediate Algebra and Precalculus. We cut off outliers that the split string length is\nlonger than 1500.\nproblem by writing extremely long derivations. MathChat solves the problem with an even longer\nstep-by-step response and makes a calculation error during the process.\nAdditionally, we also provide an overview of the number of problems where all methods either fail or\nsucceed in Table 6.\nAlgebra\nC.Prob\nI.Alg\nN.Theory\nPrealg\nPrecalc\nTotal\nAll Success\n46\n13\n0\n18\n45\n1\n176\nAll Fail\n57\n32\n171\n20\n36\n86\n402\nTable 6: The number of problems where all methods fail, and all methods succeed.\nE.4\nThe Relationship Between Failure Rate and Generated Solution Length\nChain of Thought (CoT) prompting shows that extra reasoning steps for a problem can improve the\nability of LLMs [32]. With GPT-4, explicit reasoning is no longer an issue. Instead, we find that a\nlong and tedious reasoning process may result in more type 2 failures, such as calculation errors,\nwhich results in a wrong answer even the overall direction is correct. We plot the distribution of\ncorrect and wrong answer lengths and also the answer length of the given solution (The length of\nthe string list from splitting with a single space is used here). Since more complex and challenging\nproblems are likely to have a longer solving process but still a lower success rate, we separate\nproblems from Intermediate Algebra and Precalculus with other categories (Figure 14), to distinguish\nless challenging problems from harder problems. We note that the success rate of MathChat on\nthe four less challenging categories goes over 50%, but the rate is lower than 20% for Intermediate\nAlgebra and Precalculus.\nOverall, the solution length of MathChat is longer than the ground truth solution. The length of the\ngiven solution on the two fundamentally challenging categories is longer than other categories. For\nMathChat, correct answers and wrong answers from the less challenging categories have a similar\ndistribution in solution length, where the majority of problems are solved with 50 to 500 string length.\nFor harder problems, however, an answer with more than 600 string lengths is likely to be wrong.\nFrom the precalculus problem shown in Figure 17, the LLM Assistant can choose a plausible strategy\nto solve the problem, but that strategy is less efficient and involve more math operations compared to\nthe given solution, this results in a much longer response, and it is more likely to make errors during\nthe process.\n19\nTo write a Python program that answers this question, we \ufb01rst need to convert the mathematical expressions into \nPython code using math librariFirst, let's \ufb01nd the sum and product of $\\tan A$ and $\\tan B$ using the givens: \nWe have \n and \n \nNow, using the angle addition formula, we have: \n \nand \n \nBut then, using the expressions for tan, we can rewrite these as: \n \nand \n \nNow we square both expressions and add: \n \n \n \n \nUsing the sums and products of $\\tan A$ and $\\tan B$, $\\tan A\\cdot \\tan B = D$: \n \nNoticing that \n  and canceling some terms, we \ufb01nd that the expression simpli\ufb01es to: D \ntan A + tan B = \u2212 C\ntan A \u22c5 tan B = D .\nsin(A + B) = sin A cos B + cos A sin B\ncos(A + B) = cos A cos B \u2212 sin A sin B\nsin(A + B) = sin A\ncos A cos B + cos A sin B\ncos B = sin A cos B cos2 A + sin B cos A cos2 B\ncos2 A cos2 B\ncos(A + B) = cos A\nsin A sin B \u2212 cos B\nsin B sin A = sin2 A sin B cos B \u2212 sin2 B sin A cos A\nsin2 A sin2 B\nsin2(A + B) + C sin(A + B)cos(A + B) + D cos2(A + B)\n= (((sin A cos B cos2 A)2 + (2)(sin A cos A sin B cos B)(sin A cos B cos2 A)(sin B cos B cos2 A)) + (sin A cos A sin B cos B cos2 A)2\ncos4 A cos4 B\n+C ((sin2 A sin B cos B)2 \u2212 (2)(sin A cos A sin B cos B)(sin2 A sin B cos B)(sin2 B sin A cos A) + (sin2 B sin A cos A)2\nsin2 A sin2 B cos2 A cos2 B\n+D ((sin2 A sin B cos B)2 + (sin B cos B sin A cos A \u2212 sin A cos B sin B cos A)(sin A cos A sin B cos B cos2 A) + (sin A cos B sin B cos A)2)\nsin4 A sin4 B\n= sin2 A cos2 B + 2 sin A cos A sin B cos B(tan A + tan B) + sin2 B cos2 A \u2212 C sin A sin B cos A cos B(tan A + tan B)(sin A sin B) + D sin2 A sin2 B\ncos2 A cos2 B sin2 A sin2 B\ntan A + tan B = \u2212 C\nProblem: Given constants  and \n suppose that \n and \n are the solutions to \n where \n is defined. \nSimplify \n \nYour expression should contain only one of the variables A, B, C and D.\nC\nD,\ntan A\ntan B\nx2 + Cx + D = 0,\ntan(A + B)\nsin2(A + B) + C sin(A + B)cos(A + B) + D cos2(A + B) .\nBy Vieta's formulas, \n and \n Then from the angle addition formula, \nWe write the expression we are interested in, in terms of\n \n \nThen \n \n= D\ntan A + tan B = \u2212 C\ntan A tan B = D .\ntan(A + B) = tan A + tan B\n1 \u2212 tan A tan B = \u2212\nC\n1 \u2212 D .\ntan(A + B) :\nsin2(A + B) + C sin(A + B)cos(A + B) + D cos2(A + B)\n= cos2(A + B)tan2(A + B) + C cos2(A + B)tan(A + B) + D cos2(A + B)\n= cos2(A + B)(tan2(A + B) + C tan(A + B) + D)\n=\ncos2(A + B)\nsin2(A + B) + cos2(A + B) (tan2(A + B) + C tan(A + B) + D)\n=\n1\ntan2(A + B) + 1 \u22c5 (tan2(A + B) + C tan(A + B) + D) .\n1\ntan2(A + B) + 1 \u22c5 (tan2(A + B) + C tan(A + B) + D)\n=\n1\n(\u2212\nC\n1 \u2212 D )2 + 1\n\u22c5 ((\u2212\nC\n1 \u2212 D)\n2\n\u2212 C \u22c5\nC\n1 \u2212 D + D)\n=\n(1 \u2212 D)2\n(1 \u2212 D)2 + C2 \u22c5 D(C2 + (1 \u2212 D)2)\n(1 \u2212 D)2\nProblem\nVanilla\nCorrect  \nSolution\nFigure 15: The precalculus problem where other methods are correct but MathChat is wrong. This\nfigure shows the ground truth solution and the response with vanilla prompt.\n20\nTo write a Python program that answers this question, we \ufb01rst need to convert the mathematical expressions into \nPython code using math libraries. Then we will simplify the given expression containing sinusoidal functions.  \nHere is the program: \nimport math\nimport cmath\nimport sympy as sp\n# Constants\nC, D = sp.symbols('C D')\n# Solving the quadratic equation\nx = sp.symbols('x')\nquadratic_eq = x**2 + C*x + D\nsolutions = sp.solve(quadratic_eq, x)\n# Assuming tan(A) and tan(B) are two solutions\ntan_A = solutions[0]\ntan_B = solutions[1]\n# Using properties of tan to find A and B\nA = sp.atan(tan_A)\nB = sp.atan(tan_B)\n# Finding sin(A+B) and cos(A+B)\nsin_A_plus_B = sp.sin(A + B)\ncos_A_plus_B = sp.cos(A + B)\n# Simplifying the given expression\nexpression = sin_A_plus_B**2 + C * sin_A_plus_B * cos_A_plus_B + D * cos_A_plus_B**2\nsimplified_expression = sp.simplify(expression)\nprint(f\"The given expression simplified is: {simplified_expression}\")\nProblem: Given constants  and \n suppose that \n and \n are the solutions to \n where \n is defined. \nSimplify \n \nYour expression should contain only one of the variables A, B, C and D.\nC\nD,\ntan A\ntan B\nx2 + Cx + D = 0,\ntan(A + B)\nsin2(A + B) + C sin(A + B)cos(A + B) + D cos2(A + B) .\nimport math\nimport numpy as np\nimport sympy as sp # added\ndef solver():\n    # Define the symbols for the constants and angles\n    C, D, A, B = sp.symbols('C D A B')\n    # Now, we know that tan(A) and tan(B) are solutions to the quadratic equation x^2 + Cx + D = 0\n    # We can represent this equation using Sympy\n    x = sp.Symbol('x')\n    quadratic_eq = x**2 + C*x + D\n    # Now, let's use Sympy to find the solutions\n    tan_values = sp.solve(quadratic_eq, x)\n    # Now, we have tan(A) and tan(B) as elements of the tan_values list\n    tan_A = tan_values[0]\n    tan_B = tan_values[1]\n    # Next, find sin(A), cos(A), sin(B), and cos(B) from tan(A) and tan(B)\n    sin_A = tan_A / sp.sqrt(1 + tan_A**2)\n    cos_A = 1 / sp.sqrt(1 + tan_A**2)\n    sin_B = tan_B / sp.sqrt(1 + tan_B**2)\n    cos_B = 1 / sp.sqrt(1 + tan_B**2)\n    # Now we have all the necessary components to find sin(A + B) and cos(A + B)\n    sin_AB = sin_A * cos_B + cos_A * sin_B\n    cos_AB = cos_A * cos_B - sin_A * sin_B\n    # Now we can form the given expression in terms of sin(A + B) and cos(A + B)\n    given_expr = sin_AB**2 + C * sin_AB * cos_AB + D * cos_AB**2\n    # Simplify the given expression\n    simplified_expr = sp.simplify(given_expr)\n    return simplified_expr\nprint(solver())\nPS\nPOT\nFigure 16: The precalculus problem where other methods are correct but MathChat is wrong\n(Continued). This figure shows the PS and PoT code. Both code returns the correct result: \"D\".\n21\nProblem: Given constants  and \n suppose that \n and \n are the solutions to \n where \n is defined. \nSimplify \n \nC\nD,\ntan A\ntan B\nx2 + Cx + D = 0,\ntan(A + B)\nsin2(A + B) + C sin(A + B)cos(A + B) + D cos2(A + B) .\nKey Idea: We will use the relationship between \n, \n, and properties of sine and cosine. \nStep 1: Since \n and \n are the solutions of the given equation, we have: \n \n \nStep 2: Convert the expression \n to an expression in terms \nof \n and \n. \nUsing the formula for \n and \n, we \nhave: \n \n \ntan A tan B\ntan A\ntan B\ntan A + tan B = \u2212 C\ntan A \u22c5 tan B = D\nsin2(A + B) + C sin(A + B)cos(A + B) + D cos2(A + B)\ntan A\ntan B\nsin(A + B) = sin A cos B + cos A sin B\ncos(A + B) = cos A cos B \u2212 sin A sin B\nsin2(A + B) + C sin(A + B)cos(A + B) + D cos2(A + B) =\n((sin A cos B + cos A sin B)2 + C(sin A cos B + cos A sin B)(cos A cos B \u2212 sin A sin B) + D(cos A cos B \u2212 sin A sin B)2\nProblem\nGPT-4\nContinue. Please keep solving the problem until you need to query. (If you get to the answer, \u2026\nUser Proxy Agent \nStep 3: We will now expand the expression and simplify it using the relations between \n, \n, and C, \nD: \n \nExpanding the terms: \n=\n \n \n \nNow we use the identity \n \nStep 4: Rewrite the expression in terms of \n and \n: \n=\n  \n \n \nNow we substitute \n with \n and \n with\n , and similarly for : \n \nWe will use the relations \n and \n. \nStep 5: Make substitutions and simplify: \n \nNotice that the three terms have common denominators. If we combine them, we get: \n \nStep 6: Simplify the expression and obtain the final answer: \n=  \ntan A tan B\n((sin A cos B + cos A sin B)2 + C(sin A cos B + cos A sin B)(cos A cos B \u2212 sin A sin B) + D(cos A cos B \u2212 sin A sin B)2)\n(sin2 A cos2 B + 2 sin A cos A sin B cos B + cos2 A sin2 B)\n+C(sin A cos A cos2 B \u2212 sin2 A sin B cos B + cos A cos B cos2 A \u2212 cos2 A sin A sin B)\n+D(cos4 A \u2212 2 sin A cos A sin B cos B + sin2 A sin2 B)\nsin2 x + cos2 x = 1\ntan A\ntan B\n(sin A cos B)(sin A cos B + 2 cos A sin B)\n+C(sin A cos2 B cos A \u2212 sin2 A sin B cos B + cos A cos B cos2 A \u2212 sin B cos2 A cos A)\n+D(cos A sin B)(cos2 A \u2212 sin A cos A cos B \u2212 sin B)\nsin A\ntan A\n1 + tan2 A\ncos A\n1\n1 + tan2 A\nB\n=\ntan A tan B\n(\n1 + tan2 A\n1 + tan2 B)(tan2 A + tan2 B + 2)\n+ C (\ntan A + tan B\n(\n1 + tan2 A\n1 + tan2 B)(tan2 A + tan2 B + 1) ) + D\ntan2 A tan2 B\n(1 + tan2 A)(1 + tan2 B)\ntan A + tan B = \u2212 C\ntan A \u22c5 tan B = D\n=\n\u2212D\n(\n1 + tan2 A\n1 + tan2 B)(tan2 A + tan2 B + 2)\n+ C (\n\u2212C\n(\n1 + tan2 A\n1 + tan2 B)(tan2 A + tan2 B + 1)) +\nD2\n(1 + tan2 A)(1 + tan2 B)\n= \u2212D(tan2 A + tan2 B + 1) \u2212 C2(tan2 A + tan2 B + 2) + D2(1 + tan2 A)(1 + tan2 B)\n(\n1 + tan2 A\n1 + tan2 B)(tan2 A + tan2 B + 1)(1 + tan2 A)(1 + tan2 B)\n\u2212C2\nD\nGPT-4\nFigure 17: The precalculus example where all the other methods are correct but MathChat is wrong\n(Continued). This figure shows the conversation generated in MathChat. The LLM assistant in\nMathChat chooses to solve the problem via direct reasoning, and it makes a calculation error when\nexpanding the terms in Step 3.\n22\n"
  }
]