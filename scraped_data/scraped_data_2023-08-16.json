[
  {
    "title": "CoDeF: Content Deformation Fields for Temporally Consistent Video Processing",
    "link": "https://arxiv.org/pdf/2308.07926.pdf",
    "upvote": "26",
    "text": "CoDeF: Content Deformation Fields for Temporally Consistent Video Processing\nHao Ouyang1,2*\nQiuyu Wang2*\nYuxi Xiao2,3*\nQingyan Bai1,2\nJuntao Zhang1\nKecheng Zheng2\nXiaowei Zhou3\nQifeng Chen1\u2020\nYujun Shen2\u2020\n1HKUST\n2Ant Group\n3CAD&CG, ZJU\n(a)\n(b)\n(c)\nFigure 1.\nVersatile applications of CoDeF including (a) text-guided video-to-video translation (left half: translated frames, right\nhalf: input frames), (b) video object tracking, and (c) video keypoint tracking. It is noteworthy that, with the proposed type of video\nrepresentation, we manage to directly lift image algorithms for video processing without any tuning on videos.\nAbstract\nWe present the content deformation field (CoDeF) as\na new type of video representation, which consists of a\ncanonical content field aggregating the static contents in\nthe entire video and a temporal deformation field recording\nthe transformations from the canonical image (i.e., rendered\nfrom the canonical content field) to each individual frame\nalong the time axis. Given a target video, these two fields\nare jointly optimized to reconstruct it through a carefully\ntailored rendering pipeline. We advisedly introduce some\nregularizations into the optimization process, urging the\ncanonical content field to inherit semantics (e.g., the object\nshape) from the video. With such a design, CoDeF naturally\nsupports lifting image algorithms for video processing, in\nthe sense that one can apply an image algorithm to the\ncanonical image and effortlessly propagate the outcomes to\nthe entire video with the aid of the temporal deformation\nfield.\nWe experimentally show that CoDeF is able to\nlift image-to-image translation to video-to-video translation\nand lift keypoint detection to keypoint tracking without any\ntraining. More importantly, thanks to our lifting strategy\nthat deploys the algorithms on only one image, we achieve\nsuperior cross-frame consistency in processed videos com-\npared to existing video-to-video translation approaches,\nand even manage to track non-rigid objects like water and\nsmog. Project page can be found here.\n* Equal contribution\n\u2020 Corresponding author\n1\narXiv:2308.07926v1  [cs.CV]  15 Aug 2023\n1. Introduction\nThe field of image processing has witnessed remark-\nable advancements, largely attributable to the power of\ngenerative models trained on extensive datasets, yielding\nexceptional quality and precision. However, the processing\nof video content has not achieved comparable progress. One\nchallenge lies in maintaining high temporal consistency,\na task complicated by the inherent randomness of neural\nnetworks.\nAnother challenge arises from the nature of\nvideo datasets themselves, which often include textures of\ninferior quality compared to their image counterparts and\nnecessitate greater computational resources. Consequently,\nthe quality of video-based algorithms significantly lags\nbehind those focused on images. This contrast prompts a\nquestion: is it feasible to represent video in the form of an\nimage to seamlessly apply established image algorithms to\nvideo content with high temporal consistency?\nIn pursuit of this objective, researchers have suggested\nthe generation of video mosaics from dynamic videos [40,\n47] in the era preceding deep learning, and the utilization of\na neural layered image atlas [16, 23, 66] subsequent to the\nproposal of implicit neural representations. Nonetheless,\nthese methods exhibit two principal deficiencies. First, the\ncapacity of these representations, particularly in faithfully\nreconstructing intricate details within a video, is restricted.\nOften, the reconstructed video overlooks subtle motion\ndetails, such as blinking eyes or slight smiles. The second\nlimitation pertains to the typically distorted nature of the\nestimated atlas, which consequently suffers from impaired\nsemantic information.\nExisting image processing algo-\nrithms, therefore, do not perform optimally as the estimated\natlas lacks sufficient naturalness.\nWe propose a novel approach to video representation that\nutilizes a 2D hash-based image field coupled with a 3D\nhash-based temporal deformation field. The incorporation\nof multi-resolution hash encoding [29] for the represen-\ntation of temporal deformation significantly enhances the\nability to reconstruct general videos.\nThis formulation\nfacilitates tracking the deformation of complex entities such\nas water and smog.\nHowever, the heightened capability\nof the deformation field presents a challenge in estimating\na natural canonical image. An unnatural canonical image\ncan also estimate the corresponding deformation field with\na faithful reconstruction. To navigate this challenge, we\nsuggest employing annealed hash during training.\nIni-\ntially, a smooth deformation grid is utilized to identify a\ncoarse solution applicable to all rigid motions, with high-\nfrequency details added gradually. Through this coarse-to-\nfine training, the representation achieves a balance between\nthe naturalness of the canonical and the faithfulness of\nthe reconstruction. We observe a noteworthy enhancement\nin reconstruction quality compared to preceding methods.\nThis improvement is quantified as an approximately 4.4\nincrease in PSNR, along with an observable increase in the\nnaturalness of the canonical image. Our optimization pro-\ncess requires a mere approximate 300 seconds to estimate\nthe canonical image with the deformation field while the\nprevious implicit layered representations [16] takes more\nthan 10 hours.\nBuilding upon our proposed content deformation field,\nwe illustrate lifting image processing tasks such as prompt-\nguided image translation, super-resolution, and segmenta-\ntion\u2014to the more dynamic realm of video content. Our\napproach to prompt-guided video-to-video translation em-\nploys ControlNet [69] on the canonical image, propagating\nthe translated content via the learned deformation.\nThe\ntranslation process is conducted on a single canonical image\nand obviates the need for time-intensive inference models\n(e.g., Diffusion models) across all frames. Our translation\noutputs exhibit marked improvements in temporal consis-\ntency and texture quality over the state-of-the-art zero-shot\nvideo translations with generative models [36, 64]. When\ncontrasted with Text2Live, which relies on a neural layered\natlas, our model is proficient in handling more complex\nmotion, producing more natural canonical images, and\nthereby achieving superior translation results.\nAddition-\nally, we extend the application of image algorithms such\nas super-resolution, semantic segmentation, and keypoints\ndetection to the canonical image, leading to their practical\napplications in video contexts. This includes video super-\nresolution, video object segmentation, video keypoints\ntracking, among others. Our proposed representation con-\nsistently delivers superior temporal consistency and high-\nfidelity synthesized frames, demonstrating its potential as a\ngroundbreaking tool in video processing.\n2. Related Work\nImplicit Neural Representations. Implicit representations\nin conjunction with coordinate-based Multilayer Percep-\ntrons (MLPs) have demonstrated its powerful capability\nin accurately representing images [4, 49, 51], videos [16,\n21, 49, 66], and 3D/4D representations [26, 27, 31\u201334, 56].\nThese techniques have been employed in a range of appli-\ncations, including novel view synthesis [27], image super-\nresolution [4], and 3D/4D Reconstruction [56,62]. Further-\nmore, for the purpose of speeding up the training, a various\nof acceleration [29, 46] techniques have been explored\nto replace the original Fourier positional encoding with\nsome discrete representation like multi-resolution feature\ngrid or hash table. Moreover, the adoption of an implicit\ndeformation field [20,32,33,35] has displayed a remarkable\ncapability to overfit dynamic scenes.\nInspired by these\nworks, our primary objective is to reconstruct videos by\nutilizing a canonical image which inherit semantics for\nvideo processing purposes.\nConsistent Video Editing. Our research is closely aligned\n2\nCanonical Image\nDeformation Field:\nLifting Image Algorithms to Videos \n(e.g., ControlNet, Real-ESRGAN, SAM)\n0\n7\n4\n1\n2\n3\n7\n0\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\nMLP\n\u0394x\n\u0394y\nVideo Reconstruction\nt\nR\nG\nB\nControlNet for \nVideo Translation\nReal-ESRGAN for \nVideo Super-resolution\nx\ny\nx\ny\nt\nx'\ny'\nSAM for \nVideo Segmentation\nMulti-resolution \n(x, y, t)\n(x, y, t)\nCanonical Field:\nCanonical Field:\nFigure 2. Illustration of the proposed video representation, CoDeF, which factorizes an arbitrary video into a 2D content canonical\nfield and a 3D temporal deformation field. Each field is implemented with a multi-resolution 2D or 3D hash table using an efficient MLP.\nSuch a new type of representation naturally supports lifting image algorithms for video processing, in the way of directly applying the\nestablished algorithm on the canonical image (i.e., rendered from the canonical content field) and then propagating the results along the\ntime axis through the temporal deformation field.\nwith the domain of consistent video editing [15,16,19,23],\nwhich predominantly features two primary approaches:\npropagation-based methods and layered representation-\nbased techniques. Propagation-based methods [13\u201315, 43,\n53, 60] center on editing an initial frame and subsequently\ndisseminating those edits throughout the video sequence.\nWhile this approach offers advantages in terms of com-\nputational efficiency and simplicity, it may be prone to\ninaccuracies and inconsistencies during the propagation of\nedits, particularly in situations characterized by complex\nmotion or occlusion. Conversely, layered representation-\nbased techniques [16, 23, 24, 40, 47] entail decomposing a\nvideo into distinct layers, thereby facilitating greater control\nand flexibility during the editing process.\nText2Live [1]\nintroduces the application of CLIP [37] models for video\nediting by modifying an optimized atlas [16] using text\ninputs, thereby yielding temporally consistent video editing\nresults. Our work bears similarities to Text2Live in the con-\ntext of employing an optimized representation for videos.\nHowever, our methodology diverges in several aspects: we\noptimize a more semantically-aware canonical represen-\ntation incorporating a hash-based deformable design and\nattain higher-fidelity video processing.\nVideo Processing via Generative Models. The advance-\nment of diffusion models has markedly enhanced the syn-\nthesis quality of text-to-image generation [6, 11, 50], sur-\npassing the performance of prior methodologies [25,41,65,\n68]. State-of-the-art diffusion models, such as GLIDE [30],\nDall-E 2 [38, 39], Stable Diffusion [42], and Imagen [45],\nhave been trained on millions of images, resulting in excep-\ntional generative capabilities. While existing text-to-image\n(T2I) models enable free-text generation, incorporating ad-\nditional conditioning factors [2,3,9,28,44,54,58,69] such as\nedge, depth map, and normal map is essential for achieving\nprecise control.\nIn an effort to enhance controllability,\nresearchers have proposed several approaches. For instance,\nPITI [58] involves retraining an image encoder to map\nlatents to the T2I latent space.\nInstructPix2Pix [2], on\nthe other hand, fine-tunes T2I models using synthesized\nimage condition pairs.\nControlNet [69] introduces addi-\ntional control conditions for Stable Diffusion through an\nauxiliary branch, thereby generating images that faithfully\nadhere to input condition maps. A recent research direction\nconcentrates on the processing of videos utilizing text-to-\nimage (T2I) models exclusively. Approaches like Tune-A-\nVideo [64], Text2Video-Zero [17], FateZero [36], Vid2Vid-\nZero [59], and Video-P2P [22] explore the latent space of\nDDIM [50] and incorporate cross-frame attention maps to\nfacilitate consistent generation. Nevertheless, these meth-\nods may experience compromised temporal consistency due\nto the inherent randomness of generation, and the control\ncondition may not be achieved with precision.\nText-to-video generation has emerged as a prominent\nresearch area in recent years, with prevalent approaches\nencompassing the training of diffusion models or autore-\ngressive transformers on extensive datasets. Although text-\nto-video architectures such as NUWA [63], CogVideo [12],\nPhenaki [55], Make-A-Video [48], Imagen Video [10],\nand Gen-1 [7] are capable of generating video frames that\nsemantically correspond to the input text, they may exhibit\nlimitations in terms of precise control over video conditions\nor low resolution due to substantial computational demands.\n3. Method\nProblem Formulation.\nGiven a video V comprised of\nframes {I1, I2, ..., IN}, one can naively apply the image\nprocessing algorithm X to each frame individually for\n3\ncorresponding video tasks, yet may observe undesirable\ninconsistencies across frames.\nAn alternative strategy\ninvolves enhancing algorithm X with a temporal module,\nwhich requires additional training on video data. However,\nsimply introducing a temporal module is hard to guaran-\ntee theoretical consistency and may result in performance\ndegradation due to insufficient training data.\nMotivated by these challenges, we propose representing\na video V using a flattened canonical image Ic and a\ndeformation field D. By applying the image algorithm X\non Ic, we can effectively propagate the effect to the whole\nvideo with the learned deformation field. This novel video\nrepresentation serves as a crucial bridge between image\nalgorithms and video tasks, allowing directly lifting of state-\nof-the-art image methodologies to video applications.\nThe proposed representations ought to exhibit the fol-\nlowing essential characteristics:\n\u2022 Fitting Capability for Faithful Video Reconstruction.\nThe representation should possess the ability to accu-\nrately fit large rigid or non-rigid deformations in videos.\n\u2022 Semantic Correctness of the Canonical Image. A dis-\ntorted or semantically incorrect canonical image can lead\nto decreased image processing performance, especially\nconsidering that most of these processes are trained on\nnatural image data.\n\u2022 Smoothness of the Deformation Field. The assurance\nof smoothness in the deformation field is an essential\nfeature that guarantees temporal consistency and correct\npropagation.\n3.1. Content Deformation Fields\nInspired by the dynamic NeRFs [32, 33], we propose\nto represent the video in two distinct components:\nthe\ncanonical field and the deformation field.\nThese two\ncomponents are realized through the employment of a 2D\nand a 3D hash table, respectively. To enhance the capacity\nof these hash tables, two minuscule MLPs are integrated.\nWe present our proposed representation for reconstructing\nand processing videos, as illustrated in Fig. 2. Given a video\nV comprising frames {I1, I2, ..., IN}, we train an implicit\ndeformable model tailored to fit these frames. The model is\ncomposed of two coordinate-based MLPs: the deformation\nfield D and the canonical field C.\nThe canonical field C serves as a continuous represen-\ntation encompassing all flattened textures present in the\nvideo V.\nIt is defined by a function F\n:\nx\n\u2192\nc,\nwhich maps a 2D position x : (x, y) to a color c :\n(r, g, b). In order to speed up the training and enable the\nnetwork to capture the high-frequency details, we adopt\nthe multi-resolution hash encoding \u03b32D : R2 \u2192 R2+F \u00d7L\nto map the coordinate x into a feature vector, where L\nis the number of levels for multi-resolution and F is the\nnumber of feature dimensions for per layer. The function\n\u03b32D(x) = (x, F1(x), ..., FL(x)) facilitates the model\u2019s\nability to capture high-frequency details, where Fi(x) is\nthe features linearly interpolated by x at ith resolution. The\ndeformation field D captures the observation-to-canonical\ndeformation for every frame within a video. For a specific\nframe Ii, D establishes the correspondence between the\nobserved and canonical positions. Dynamic NeRFs [32,33]\nimplement the deformation field in 3D space by using the\nFourier positional encoding and an extra learnable time\ncode.\nThis implementation ensures the smoothness of\nthe deformation field.\nNevertheless, this straightforward\nimplementation can not be seamlessly transferred into video\nrepresentation for two reasons (i.e. low training efficiency\nand inadequate representative capability). Therefore, we\npropose to represent the deformation field as a 3D hash\ntable with a tiny MLP following. Specifically, an arbitrary\nposition x in the tth frame is first encoded by a 3D hash\nencoding function \u03b33D(x, t) to get high-dimension features.\nThen a tiny MLP D : (\u03b33D(x, t)) \u2192 x\u2032 maps the embedded\nfeatures its corresponding position x\u2032 in canonical field. We\nelaborate our 3D hash encoding based deformation field in\ndetail as follows.\n3D Hash Encoding for Deformation Field. Specifically,\nan arbitrary point in the video can be conceptualized as a\nposition x3D : (x, y, t) within an orthogonal 3D space.We\nrepresent our video space using the 3D hash encoding tech-\nnique, as depicted on the left side of Fig. 2. This technique\nencapsulates the 3D space as a multi-resolution feature\ngrid. The term multi-resolution refers to a composition of\ngrids with varying degrees of resolution, and feature grid\ndenotes a grid populated with learnable features at each\nvertex. In our framework, the multi-resolution feature grid\nis organized into L distinct levels. The dimensionality of\nthe learnable features is represented as F. Furthermore,\nthe resolution of the lth layer, denoted as Nl, exhibits\na geometric progression between the coarsest and finest\nresolutions, denoted collectively as [Nmin, Nmax], using\nNl = \u230aNmin \u00b7 bl\u230b, b = exp\n\u0012ln Nmax \u2212 ln Nmin\nL \u2212 1\n\u0013\n.\n(1)\nConsidering the queried points x3D at lth layer, the input\ncoordinate is scaled by that level\u2019s grid resolution. And the\nqueried features of x3D are tri-linear interpolated from its\n8-neighboring corner points(seen in Fig. 2). For attaining\nthe corner points of x3D, rounding down and up are first\noperated as\n\u230axl\n3D\u230b = \u230ax3D \u00b7 Nl\u230b, \u2308xl\n3D\u2309 = \u2308x3D \u00b7 Nl\u2309,\n(2)\nand we map its each corner to an entry in the level\u2019s\nrespective feature vector array, which has fixed size of\nat most T.\nFor the coarse level, the parameters of low\nresolution grid are fewer than T, where the mapping is 1 : 1.\n4\nThus, the features can be directly looked up by its index. On\nthe contrary. For the finer resolution, the point is mapped by\nthe hash function,\nh(xl\n3D) =\n\u0000\u2295d\ni=1xi\u03c0i\n\u0001\nmod\nT,\n(3)\nwhere \u2295 denotes the bit-wise XOR operation and {\u03c0i} are\nunique large prime numbers following [29].\nThe output color value at coordinate x for frame t can be\ncomputed as\nc = C(D(\u03b33D(x, t))).\n(4)\nThis output can be supervised using the ground truth color\npresent in the input frame.\n3.2. Model Design\nThe proposed representation can effectively model and\nreconstruct both the canonical content and the temporal\ndeformation for an arbitrary video.\nHowever, it faces\nchallenges in meeting the requirements for robust video\nprocessing.\nIn particular, while 3D hash deformation\npossesses powerful fitting capability, it compromises the\nsmoothness of temporal deformation. This trade-off makes\nit notably difficult to maintain the inherent semantics of\nthe canonical image, creating a significant barrier to the\nadaptation of established image algorithms for video use.\nTo achieve precise video reconstruction while preserving\nthe inherent semantics of the canonical image, we pro-\npose the use of annealed multi-resolution hash encoding.\nTo further enhance the smoothness of deformation, we\nintroduce flow-based consistency.\nIn challenging cases,\nsuch as those involving large occlusions or complex multi-\nobject scenarios, we suggest utilizing additional semantic\ninformation. This can be achieved by using semantic masks\nin conjunction with the grouped deformation fields.\nAnnealed 3D Hash Encoding for Deformation. For the\nfiner resolution, the hash encoding enhance the complex\ndeformation fitting performance but introducing the discon-\ntinuity and distortion in canonical field (Seen in Fig. 9).\nInspired by the annealed strategy utilized in dynamic\nNeRFs [32], we employ the annealed hash encoding tech-\nnique for progressive frequency filter for deformation.\nMore specifically, we use a progressive controlling weights\nfor those features interpolated in different resolution. The\nweight for the lth layer in training step k is computed as\nwj(k) = 1 \u2212 cos(\u03c0 \u00b7 clamp(m(j \u2212 Nbeg)/Nstep, 0, 1))\n2\n,\n(5)\nwhere Nbeg is a predefined step for beginning annealing\nand m represents a hyper parameters for controlling the\nannealing speed, and Nstep is the number for annealing step.\nFlow-guided Consistency Loss.\nCorresponding points\nidentified by flows with high confidence should be the same\npoints in the canonical field. We compute the flow-guided\nLayered Neural Atlas\nInput Video\nOurs\nCanonical Image\nReconstructed Video\nFigure 3.\nQualitative comparison between layered neural at-\nlas [16] and our CoDeF regarding video reconstruction, which\nreflects the capacity of the video representation and also plays\na fundamental role in faithful video processing. Details are best\nappreciated when zoomed in.\nconsistency loss according to this observation.\nFor two\nconsecutive frames Ii and Ii+1, we employ RAFT [52]\nto detect the forward flows Fi\u2192i+1 and backward flows\nFi+1\u2192i. The confident region of a frame Ii can be defined\nas\nMflow = |Warp(Warp(Ii, Fi\u2192i+1), Fi+1\u2192i)\u2212Ii| < \u03f5, (6)\nwhere \u03f5 represents a hyperparameter for the error threshold.\nThis loss can be formulated as\nLflow =\nX\n\u2225D(\u03b33D(x, t)) \u2212 D(\u03b33D(x + Fx\nt\u2192t+1, t + 1)) \u2212 Fx\nt\u2192t+1\u2225 \u2217 M x\nflow,\n(7)\nwhere Fx\nt\u2192t+1 and M x\nflow are the optical flow and the\nflow confidence at x . The flow loss efficiently regularize\nthe smoothness of the deformation field especially for the\nsmooth region.\nGrouped Content Deformation Fields.\nAlthough the\nrepresentation can learn to reconstruct a video using a\nsingle content deformation field, complex motions arising\nfrom overlapped multi-objects may lead to conflicts within\none canonical. Consequently, the boundary region might\nsuffer from inaccurate reconstruction.\nFor challenging\ninstances featuring large occlusions, we propose an option\nto introduce the layers corresponding to multiple content\ndeformation fields. These layers would be defined based\non semantic segmentation, thereby improving the accuracy\nand robustness of video reconstruction in these demanding\nscenarios. We leverage the Segment-Anything-track (SAM-\ntrack) [5] to attain the segmentation of each video frame\n5\nOriginal Video\nOriginal Video\nOurs\nOurs\nText2Live\nText2Live\nTune-A-Video\nTune-A-Video\nFateZero\nFateZero\nText Prompt: Cyberpunk robot\nText Prompt: Cyberpunk robot\nText Prompt: Iron man\nText Prompt: Iron man\nText Prompt: Your name by Makoto Shinkai, handsome man and beautiful woman\nText Prompt: Your name by Makoto Shinkai, handsome man and beautiful woman\nFigure 4. Qualitative comparison on the task of text-guided video-to-video translation across different methods, including Text2Live [1],\nTune-A-Video [64], FateZero [36], and directly lifting ControlNet [69] through our CoDeF. We strongly encourage the readers to see the\nvideos on the project page for a detailed evaluation of temporal consistency and synthesis quality.\nIi into K semantic layers with mask M i\n0, ..., M i\nK\u22121. And\nfor each layer, we use a group of canonical fields and\ndeformation fields to represent those separate motion of\ndifferent objects. These models are subsequently formu-\nlated as groups of implicit fields: D : {D1, ..., DK}, C :\n{C1, ..., CK}. In theory, for semantic layer k in frame i, it\nis sufficient to sample pixels in the region M i\nk for efficient\nreconstruction.\nHowever, hash encoding can result in\nrandom and unstructured patterns in unsupervised regions,\nwhich decreases the performance of image-based models\ntrained on natural images. To tackle this issue, we sample\na number of points outside of the region M i\nk and train them\nusing L2 loss with the ground truth color.\nIn this way,\nwe effectively regularize \u00af\nM i\nk with the background loss Lbg.\nConsequently, the canonical image attains a more natural\nappearance, leading to enhanced processing results.\nTraining Objectives.\nThe representation is trained by\nminimizing the objective function Lrec.\nThis function\ncorresponds to the L2 loss between the ground truth color\nand the predicted color c for a given coordinate x.\nTo\nregularize and stabilize the training process, we introduce\nadditional regularization terms as previously discussed. The\ntotal loss is calculated using the following equation\nL = Lrec + \u03bb1 \u2217 Lflow,\n(8)\nwhere \u03bb1 represents the hyper-parameters for loss weights.\nIt\u2019s important to note that when training the grouped defor-\nmation field, we include an additional regularizer, denoted\nas \u03bb2 \u2217 Lbg.\n6\n1-th frame\n10-th frame\n20-th frame\n30-th frame\nCanonical Image\nFigure 5. Visualization of point correspondence across frames, which is directly extracted from the temporal deformation field after\nreconstructing the video with CoDeF.\n3.3. Application to Consistent Video Processing\nUpon the optimization of the content deformation field,\nthe canonical image Ic is retrieved by setting the defor-\nmation of all points to zero. It is important to note that\nthe size of the canonical image can be flexibly adjusted\nto be larger than the original image size depending on the\nscene movement observed in the video, thereby allowing\nmore content to be included. The canonical image Ic is\nthen utilized in executing various downstream algorithms\nfor consistent video processing. We evaluated the following\nstate-of-the-art (SOTA) algorithms: (1) ControlNet [69]:\nUsed for prompt-guided video-to-video translation.\n(2)\nSegment-anything (SAM) [18]: Applied for video object\ntracking. (3) R-ESRGAN [61]: Employed for video super-\nresolution. Additionally, the canonical image allows users\nto conveniently edit the video by directly modifying the\nimage. We further illustrate this capability through multiple\nmanual video editing examples.\n4. Experiments\n4.1. Experimental Setup\nWe conduct experiments to underscore the robustness\nand versatility of our proposed method. Our representation\nis robust with a variety of deformations, encompassing rigid\nand non-rigid objects, as well as complex scenarios such\nas smog. The default parameters for our experiments are\nset with the anneal begin and end steps at 4000 and 8000,\nrespectively. The total iteration step is capped at 10000.\nOn a single NVIDIA A6000 GPU, the average training\nduration is approximately 5 minutes when utilizing 100\nvideo frames. It should be noted that the training time varies\nwith several factors such as the length of the video, the\ntype of motion, and the number of layers. By adjusting the\ntraining parameters accordingly, the optimization duration\ncan be varied from 1 to 10 minutes.\n4.2. Evaluation\nThe evaluation of our representation is concentrated on\ntwo main aspects: the quality of the reconstructed video\nwith the estimated canonical image, and the quality of\ndownstream video processing. Owing to the lack of ac-\ncurate evaluation metrics, conducting a precise quantitative\nInput Video\nVideo Object Tracking\nFigure 6. Video object tracking results achieved by lifting an\nimage segmentation algorithm [18] through our CoDeF.\nLow Resolution\nHigh Resolution\nFigure 7. Video super-resolution results achieved by lifting an\nimage super-resolution algorithm [61] through our CoDeF.\nanalysis remains challenging. Nevertheless, we include a\nselection of quantitative results for further examination.\nReconstruction Quality. In a comparative analysis with\nthe Neural Image Atlas, our model, as demonstrated\nin Fig. 3, exhibits superior robustness to non-rigid motion,\neffectively reconstructing subtle movements with height-\nened precision (e.g. eyes blinking, face textures). Quantita-\ntively, the video reconstruction PSNR of our algorithm on\nthe collected video datasets is 4.4 dB higher. In comparison\nbetween the atlas and our canonical image, our results\nprovide a more natural representation, and thus, facilitate\nthe easier application of established image algorithms.\nBesides, our method makes a significant progress in training\nefficiency, i.e., 5 minutes (ours) vs. 10 hours (atlas).\nDownstream Video Processing. We provide an expanded\nrange of potential applications associated with the pro-\n7\nposed representations, including video-to-video translation,\nvideo keypoint tracking, video object tracking, video super-\nresolution, and user-interactive video editing.\n(a) Video-to-video Translation. By applying image transla-\ntion to the canonical image, we can perform video-to-video\ntranslation. A qualitative comparison is presented encom-\npassing several baseline methods that fall into three distinct\ncategories:\n(1) per-frame inference with image transla-\ntion models, such as ControlNet [69]; (2) layered video\nediting, exemplified by Text-to-live [1]; and (3) diffusion-\nbased video translation, including Tune-A-Video [64] and\nFateZero [36]. As depicted in Fig. 4, the per-frame image\ntranslation models yield high-fidelity content, accompanied\nby significant flickering. The alternative baselines exhibit\ncompromised generation quality or comparatively low tem-\nporal consistency. The proposed pipeline effectively lifts\nimage translation to video, maintaining the high quality\nassociated with image translation algorithms while ensuring\nsubstantial temporal consistency. A thorough comparison is\nbetter appreciated by viewing the accompanying videos.\n(b) Video Keypoint Tracking. By estimating the deformation\nfield for each individual frame, it is feasible to query the\nposition of a specific keypoint in one frame within the\ncanonical space and subsequently identify the correspond-\ning points present in all frames as in Fig. 5. We show the\ndemonstration of tracking points in non-rigid objects such\nas fluids in the videos on the project page.\n(c) Video Object Tracking. Using the segmentation algo-\nrithms on the canonical image, we are able to facilitate\nthe propagation of masks throughout all video sequences\nleveraging the content deformation field.\nAs illustrated\nin Fig. 6, our pipeline proficiently yields masks that main-\ntain consistency across all frames.\n(d) Video Super-resolution. By directly applying the image\nsuper-resolution algorithm to the canonical image, we can\nexecute video super-resolution to generate high-quality\nvideo as in Fig. 7. Given that the deformation is represented\nby a continuous field, the application of super-resolution\nwill not result in flickering.\n(e) User interactive Video Editing.\nOur representation\nallows for user editing on objects with unique styles without\ninfluencing other parts of the image.\nAs exemplified\nin Fig. 8, users can manually adjust content on the canonical\nimage to perform precise edits in areas where the automatic\nediting algorithm may not be achieving optimal results.\n4.3. Ablation Study\nTo validate the effect of the proposed modules, we\nconducted an ablation study.\nOn substituting the 3D\nhash encoding with positional encoding, there is a notable\ndecrease in the reconstruction PSNR of the video by 3.1\ndB. In the absence of the annealed hash, the canonical\nimage loses its natural appearance, as evidenced by the\nInput Video\nVideo Editing\nFigure 8. User interactive video editing achieved by editing only\none image and propagating the outcomes along the time axis using\nour CoDeF. We strongly encourage the readers to see the videos\non the project page to appreciate the temporal consistency.\nTransferred \nCanonical Image\nCanonical Image\nFigure 9.\nAblation study on the effectiveness of annealed\nhash. The unnaturalness in the canonical image will harm the\nperformance of downstream tasks.\npresence of multiple hands in Fig. 9. Furthermore, without\nincorporating the flow loss, smooth areas are noticeably\naffected by pronounced flickering. For a more extensive\ncomparison, please refer to the videos on the project page.\n5. Conclusion and Discussion\nIn this paper, we have investigated representing videos\nas content deformation fields, focusing on achieving tem-\nporally consistent video processing. Our approach demon-\nstrates promising results in terms of both fidelity and tempo-\nral consistency. However, there remain several challenges\nto be addressed in future work.\nOne of the primary issues pertains to the per-scene op-\ntimization required in our methodology. We anticipate that\nadvancements in feed-forward implicit field techniques [57,\n67] could potentially be adapted to this direction. Another\nchallenge arises in scenarios involving extreme changes in\nviewing points. To tackle this issue, the incorporation of\n3D prior knowledge [8] may prove beneficial, as it can\nprovide additional information and constraints. Lastly, the\n8\nhandling of large non-rigid deformations remains a concern.\nTo address this, one potential solution involves employing\nmultiple canonical images [33], which can better capture\nand represent complex deformations.\nReferences\n[1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni\nKasten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In Eur. Conf. Comput. Vis., 2022.\n[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2023.\n[3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan,\nXiaohu Qie, and Yinqiang Zheng.\nMasactrl: Tuning-free\nmutual self-attention control for consistent image synthesis\nand editing. arXiv preprint arXiv:2304.08465, 2023.\n[4] Yinbo Chen, Sifei Liu, and Xiaolong Wang.\nLearning\ncontinuous image representation with local implicit image\nfunction. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.\n[5] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li,\nZongxin Yang, Wenguan Wang, and Yi Yang. Segment and\ntrack anything. arXiv preprint arXiv:2305.06558, 2023.\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis.\nIn Adv. Neural Inform.\nProcess. Syst., 2021.\n[7] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023.\n[8] Ayaan Haque, Matthew Tancik, Alexei A Efros, Alek-\nsander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf:\nEditing 3d scenes with instructions.\narXiv preprint\narXiv:2303.12789, 2023.\n[9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022.\n[10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022.\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising\ndiffusion probabilistic models.\nIn Adv. Neural Inform.\nProcess. Syst., 2020.\n[12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022.\n[13] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time\ncorrespondence as a contrastive random walk. In Adv. Neural\nInform. Process. Syst., 2020.\n[14] Varun Jampani, Raghudeep Gadde, and Peter V Gehler.\nVideo propagation networks. In IEEE Conf. Comput. Vis.\nPattern Recog., 2017.\n[15] Ond\u02c7rej Jamri\u02c7ska, \u02c7S\u00b4arka Sochorov\u00b4a, Ond\u02c7rej Texler, Michal\nLuk\u00b4a\u02c7c, Jakub Fi\u02c7ser, Jingwan Lu, Eli Shechtman, and Daniel\nS`ykora. Stylizing video by example. ACM Trans. Graph.,\n38(4):1\u201311, 2019.\n[16] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel.\nLayered neural atlases for consistent video editing.\nACM\nTrans. Graph., 40(6):1\u201312, 2021.\n[17] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023.\n[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al.\nSegment\nanything. arXiv preprint arXiv:2304.02643, 2023.\n[19] Chenyang Lei, Yazhou Xing, Hao Ouyang, and Qifeng Chen.\nDeep video prior for video consistency and propagation.\nIEEE Trans. Pattern Anal. Mach. Intell., 45(1):356\u2013371,\n2022.\n[20] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\net al. Neural 3d video synthesis from multi-view video. In\nIEEE Conf. Comput. Vis. Pattern Recog., 2022.\n[21] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of\ndynamic scenes. In IEEE Conf. Comput. Vis. Pattern Recog.,\n2021.\n[22] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023.\n[23] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew\nZisserman, David Salesin, William T Freeman, and Michael\nRubinstein. Layered neural rendering for retiming people in\nvideo. arXiv preprint arXiv:2009.07833, 2020.\n[24] Erika Lu, Forrester Cole, Weidi Xie, Tali Dekel, Bill Free-\nman, Andrew Zisserman, and Michael Rubinstein. Associ-\nating objects and their effects in video through coordination\ngames. In Adv. Neural Inform. Process. Syst., 2022.\n[25] Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and\nRuslan Salakhutdinov.\nGenerating images from captions\nwith attention. arXiv preprint arXiv:1511.02793, 2015.\n[26] Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack,\nMahsa Baktashmotlagh, and Anders Eriksson.\nImplicit\nsurface representations as layers in neural networks. In Int.\nConf. Comput. Vis., 2019.\n[27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Eur. Conf. Comput. Vis., 2020.\n[28] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023.\n[29] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022.\n9\n[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021.\n[31] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2019.\n[32] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Int. Conf. Comput. Vis., 2021.\n[33] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 40(6), 2021.\n[34] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\nPollefeys, and Andreas Geiger.\nConvolutional occupancy\nnetworks. In Eur. Conf. Comput. Vis., 2020.\n[35] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields for\ndynamic scenes. In IEEE Conf. Comput. Vis. Pattern Recog.,\n2021.\n[36] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv preprint arXiv:2303.09535, 2023.\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In Int. Conf. Mach. Learn., 2021.\n[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[39] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation.\nIn Int. Conf. Mach.\nLearn., 2021.\n[40] Alex Rav-Acha, Pushmeet Kohli, Carsten Rother, and An-\ndrew Fitzgibbon. Unwrap mosaics: A new representation\nfor video editing. In SIGGRAPH, pages 1\u201311, 2008.\n[41] Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka,\nBernt Schiele, and Honglak Lee. Learning what and where\nto draw. In Adv. Neural Inform. Process. Syst., 2016.\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn IEEE Conf.\nComput. Vis. Pattern Recog., 2022.\n[43] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.\nArtistic style transfer for videos.\nIn Pattern Recognition:\n38th German Conference, GCPR 2016, Hannover, Germany,\nSeptember 12-15, 2016, Proceedings 38, pages 26\u201336.\nSpringer, 2016.\n[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn IEEE Conf. Comput. Vis. Pattern Recog.,\n2023.\n[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nIn Adv. Neural Inform. Process.\nSyst., 2022.\n[46] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks.\nIn IEEE Conf.\nComput. Vis. Pattern Recog., 2022.\n[47] Jonathan Shade, Steven Gortler, Li-wei He, and Richard\nSzeliski. Layered depth images. In Proceedings of the 25th\nannual conference on Computer graphics and interactive\ntechniques, 1998.\n[48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022.\n[49] Vincent Sitzmann, Julien Martel, Alexander Bergman, David\nLindell, and Gordon Wetzstein.\nImplicit neural represen-\ntations with periodic activation functions.\nIn Adv. Neural\nInform. Process. Syst., 2020.\n[50] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020.\n[51] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi\nRamamoorthi, Jonathan Barron, and Ren Ng.\nFourier\nfeatures let networks learn high frequency functions in low\ndimensional domains. In Adv. Neural Inform. Process. Syst.,\n2020.\n[52] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In Eur. Conf. Comput. Vis., 2020.\n[53] Ond\u02c7rej Texler, David Futschik, Michal Ku\u02c7cera, Ond\u02c7rej\nJamri\u02c7ska, \u02c7S\u00b4arka Sochorov\u00b4a, Menclei Chai, Sergey Tulyakov,\nand Daniel S`ykora. Interactive video stylization using few-\nshot patch-based training. ACM Trans. Graph., 39(4):73\u20131,\n2020.\n[54] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation.\nIn IEEE Conf. Comput. Vis.\nPattern Recog., 2023.\n[55] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint arXiv:2210.02399, 2022.\n[56] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021.\n10\n[57] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser.\nIbrnet: Learning multi-view image-based rendering. In IEEE\nConf. Comput. Vis. Pattern Recog., 2021.\n[58] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong\nChen, Qifeng Chen, and Fang Wen.\nPretraining is all\nyou need for image-to-image translation.\narXiv preprint\narXiv:2205.12952, 2022.\n[59] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023.\n[60] Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning\ncorrespondence from the cycle-consistency of time. In IEEE\nConf. Comput. Vis. Pattern Recog., 2019.\n[61] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan: Training real-world blind super-resolution with\npure synthetic data. In Int. Conf. Comput. Vis., 2021.\n[62] Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-\nilidis, Christian Theobalt, and Lingjie Liu.\nNeus2: Fast\nlearning of neural implicit surfaces for multi-view recon-\nstruction. arXiv preprint arXiv:2212.05231, 2022.\n[63] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation.\nIn Eur. Conf.\nComput. Vis., 2022.\n[64] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022.\n[65] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2018.\n[66] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa,\nand Noah Snavely.\nDeformable sprites for unsupervised\nvideo decomposition. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2022.\n[67] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images. In\nIEEE Conf. Comput. Vis. Pattern Recog., 2021.\n[68] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In Int. Conf. Comput. Vis.,\n2017.\n[69] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023.\n11\n"
  },
  {
    "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
    "link": "https://arxiv.org/pdf/2308.07921.pdf",
    "upvote": "20",
    "text": "SOLVING CHALLENGING MATH WORD PROBLEMS USING GPT-4\nCODE INTERPRETER WITH CODE-BASED SELF-VERIFICATION\nAojun Zhou1\u2217\nKe Wang2\u2217\nZimu Lu3\u2217\nWeikang Shi4\u2217\nSichun Luo5\u2217\nZipeng Qin1\nShaoqing Lu 6\nAnya Jia 7\nLinqi Song5\nMingjie Zhan1\u2020\nHongsheng Li1\u2021\n1Multimedia Laboratory (MMLab), The Chinese University of Hong Kong\n2Nanjing University\n3University of Science and Technology of China\n4Tsinghua University\n5City University of Hong Kong\n6Changsha University of Science and Technology\n7Tufts University\n{aojunzhou, wangk.gm, sichunluo2, zmjdll}@gmail.com\nluzimu@mail.ustc.edu.cn\nshiwk20@mails.tsinghua.edu.cn\nlinqi.song@cityu.edu.hk\nhsli@ee.cuhk.edu.hk\nABSTRACT\nRecent progress in large language models (LLMs) like GPT-4 and PaLM-2 has\nbrought significant advancements in addressing math reasoning problems.\nIn\nparticular, OpenAI\u2019s latest version of GPT-4, known as GPT-4 Code Interpreter,\nshows remarkable performance on challenging math datasets. In this paper, we\nexplore the effect of code on enhancing LLMs\u2019 reasoning capability by intro-\nducing different constraints on the Code Usage Frequency of GPT-4 Code Inter-\npreter. We found that its success can be largely attributed to its powerful skills\nin generating and executing code, evaluating the output of code execution, and\nrectifying its solution when receiving unreasonable outputs. Based on this in-\nsight, we propose a novel and effective prompting method, explicit code-based\nself-verification (CSV), to further boost the mathematical reasoning potential of\nGPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code\nInterpreter to encourage it to use code to self-verify its answers. In instances where\nthe verification state registers as \u201cFalse\u201d, the model shall automatically amend its\nsolution, analogous to our approach of rectifying errors during a mathematics ex-\namination. Furthermore, we recognize that the states of the verification result\nindicate the confidence of a solution, which can improve the effectiveness of ma-\njority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive\nzero-shot accuracy on MATH dataset (53.9% \u2192 84.3%).\n1\nINTRODUCTION\nLarge language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023) have shown\nimpressive success in various tasks, such as common sense understanding and code generation.\nHowever, they still fall short in mathematical reasoning, often producing nonsensical or inaccu-\nrate content and struggling with complex calculations. Previous attempts to tackle these challenges\ninclude the Chain-of-Thought (CoT) (Wei et al., 2022) framework, which enhances LLMs\u2019 logi-\ncal reasoning abilities by generating intermediate steps in their reasoning process. Additionally,\nPAL (Gao et al., 2023) introduces a novel approach by using the Python programming interpreter to\nimprove computational accuracy.\nIn recent advancements, OpenAI has unveiled an improved version of GPT-4, namely the GPT-4\nCode Interpreter12 or GPT4-Code, which is proficient at providing logical natural language reason-\n\u2217Equal contribution.\n\u2020Project lead.\n\u2021Corresponding author.\n1https://openai.com/blog/chatgpt-plugins#code-interpreter\n2https://chat.openai.com/?model=GPT4-Code-interpreter\n1\narXiv:2308.07921v1  [cs.CL]  15 Aug 2023\ning, alongside step-by-step Python code. Notably, it can generate and execute code incrementally,\nand subsequently present the executed code\u2019s output back to the LLM. The addition of code genera-\ntion and execution to natural language outputs has shown promising results in solving mathematical\nreasoning problems. Our initial experiments show that GPT4-Code achieved an impressive zero-shot\naccuracy of 69.7% on the challenging MATH dataset (Hendrycks et al., 2021), marking a significant\nimprovement of 27.5% over GPT-4\u2019s performance (42.2%).\nWhile GPT4-Code has demonstrated proficiency in solving math problems, there has been a notable\nabsence of systematic analysis focusing on understanding and further enhancing its mathematical\nproblem-solving abilities. A key distinction between GPT4-Code and its predecessor, GPT-4, lies in\nGPT4-Code\u2019s ability to automatically generate and execute code. Therefore, this paper presents pi-\nlot experiments that investigate GPT4-Code\u2019s code generation and execution mechanism using spe-\ncific code-constrained prompts. The analysis reveals that GPT4-Code\u2019s strong performance is not\nsolely due to its code generation and execution abilities, but also its capacity to adjust its problem-\nsolving strategies based on feedback from code execution\u2014a process we term self-debugging (illus-\ntrated in Tab. 7 and Tab. 8). Due to the fact that code generation evolves its reasoning step-by-step\nand performs self-debugging after code execution errors, there is an increased frequency of code\nusage. Hence, we introduce the concept of Code Usage Frequency to differentiate these unique\nprompting strategies to quantitatively analyze the impact of code-constrained prompts on GPT4-\nCode for mathematical problem-solving.\nThe step-by-step code generation and self-debugging mechanisms highlight the critical role of code\nin mathematical problem-solving. Nevertheless, the self-debugging mechanism only verifies each\nstep of the generated code while lacks the verification of the reasoning steps and the final answer,\nwhich has been demonstrated to be of vital importance to the math problem-solving abilities of\nLLMs (Cobbe et al., 2021; Lightman et al., 2023; Weng et al., 2023).\nWe therefore ask the question: can we fully exploit the code generation and self-debugging mech-\nanisms in GPT4-code, so that it can automatically verify and correct its solutions, without extra\nassistance from other models or users?\nTo answer this question, we propose a simple yet effective prompting technique termed the ex-\nplicit code-based self-verification (CSV), which guides GPT4-Code to generate additional code\nthat verifies the answer and adjusts the reasoning steps if there\u2019s a flaw in reasoning. Unlike previ-\nous methods that rely on external language models for verification (Lightman et al., 2023; Cobbe\net al., 2021), our approach leverages GPT4-Code\u2019s inherent strengths. This approach offers two key\nbenefits: (1) When the verification indicates an answer is False, GPT4-Code can rectify its prior\nsolution and provide an improved alternative. (2) Solutions verified as True tend to be more reli-\nable, akin to human problem-solving. However, even if a solution is self-verified as False, we do\nnot directly abandon it. Instead, we propose a weighted majority voting strategy that incorporates\nthe code-based solution verification results, as opposed to relying exclusively on the frequency of\nanswers. We assign different weights to the solutions according to their verification states, reflecting\nthe solutions\u2019 varying levels of reliability. In alignment with the Code Usage Frequency analysis\nfrom our pilot experiments, our explicit code-based self-verification prompt boosts GPT4-Code\u2019s\naccuracy in mathematical problem-solving with increased code usage.\nEmpirical study demonstrates the effectiveness of our proposed framework on the MATH, GSM8K,\nand MMLU-Math datasets using GPT4-Code. Our approach achieves an impressive accuracy of\n84.32% on the MATH dataset, greatly outperforming the base GPT4-Code and previous state-of-\nthe-art methods. Additionally, we are making our experimental data on the MMLU-Math and MATH\ndatasets publicly available, enabling result replication and facilitating fine-tuning of open-source\nLLM models (e.g., LLaMA 2 (Touvron et al., 2023)) to further enhance mathematical problem-\nsolving capabilities with the assistance of code generation.\nThis paper\u2019s main contributions can be summarized in three key aspects:\n\u2022 This study provides the first systematic analysis of code generation, execution, and self-\ndebugging\u2019s role in mathematical problem-solving. Our findings reveal that GPT4-Code\u2019s\nimpressive mathematical problem-solving proficiency is primarily attributed to its step-by-\nstep code generation and dynamic solution refinement based on code execution outcomes.\n2\nQuestion\nQuestion:\nThe greatest common divisor of positive integers m and n is 6.\nThe least common multiple of m and n is 126. What is the least\npossible value of m + n? Answer: 60\n(a) Model output\nPrompt 1\nYou are not allowed to use\nany code.\nThe problem is:\n{Question}.\nWe know that for any two positive\nintegers m and n, the product of\ntheir greatest common divisor (GCD)\nand least common multiple (LCM) is\nequal to the product of the two num-\nbers. This can be written as:\nGCD(m, n) \u00b7 LCM(m, n)\n= m \u00b7 n\nGiven that GCD(m, n) = 6 and\nLCM(m, n)\n=\n126, we can\nsubstitute these values into the equa-\ntion:\n6 \u00b7 126 = m \u00b7 n\nSolving this gives us m \u00b7 n = 756.\nNow, we need to find the pair of fac-\ntors of 756 that add up to the smallest\nsum, as we want to find the least pos-\nsible value of m + n.\nThe pairs of factors of 756 are\n(1,756), (2,378), (3,252), (4,189),\n(6,126),\n(7,108),\n(9,84),\n(12,63),\n(14,54), (18,42), (21,36), (27,28).\nWe can see that the pair (21,36) gives\nthe smallest sum, which is 21 +\n36 = 57.\nSo, the least possible value of m+n\nis 57.\nTherefore, the solution to the\nproblem is\n57 .\u2717\n(b) Model output\nPrompt 2\nYou can use code only 1 time. The problem is:\n{Question}.\nFirst, we should recall that for any two positive inte-\ngers m and n, their greatest common divisor (gcd)\nand least common multiple (lcm) satisfy the relation-\nship:\nm \u00b7 n = gcd(m, n) \u00b7 lcm(m, n)\nIn this problem, we\u2019re given that gcd(m, n) = 6\nand lcm(m, n) = 126. We can substitute these\nvalues into the formula to get:\nm \u00b7 n = 6 \u00b7 126\nWe can solve this equation for one variable and then\nuse the fact that m and n are integers to generate\npossible pairs (m, n).\nAfter we have the pairs (m, n), we can find the pair\nthat minimizes m + n.\ncode\nimport sympy as sp\ngcd,lcm = 6,126\nproduct = gcd * lcm\npairs\n=\n[(d,\nproduct\n//\nd)\nfor\nd\nin\nsp.divisors(product)\nif\nd\n<=\nproduct\n//\nd]\nmin pair = min(pairs, key=sum)\nmin pair, sum(min pair)\n>>> ((27, 28), 55)\nThe pair (m, n) that minimizes m + n is\n(27, 28), and the minimum possible value of m +\nn is\n55 .\u2717\n(c) Model output\nBasicPrompt\nSolve the problem and put your answer in\n\\\\boxed{}. The problem is: {Question}.\nOne important property relating greatest common divi-\nsor (GCD) and least common multiple (LCM) is that\nGCD(m, n) \u00d7 LCM(m, n) = m \u00d7 n\nGiven in the problem, we can get\n6 \u00d7 126 = m \u00d7 n\nLet\u2019s compute the right hand side of the equation first.\ncode\ngcd,lcm = 6, 126\ngcd * lcm\n>>> 756\nThe product of m and n is 756.\nSince the GCD of m and n is 6, both m and n must\nbe multiples of 6.\nLet\u2019s find all the pairs of multiples of 6 that multiply to\n756. We\u2019ll do this by finding all the factors of 756 that\nare multiples of 6.\ncode\n[(i,\nm times n\n//\ni)\nfor\ni\nin\nrange(6,\nint(m times n**0.5) + 1, 6) if m times n %\ni == 0]\n>>> [(6, 126), (12, 63), (18, 42)]\nThe pairs of factors of 756 that are multiples of 6 are:\n1. (6, 126) 2. (12, 63) 3. (18, 42)\nWe want to find the pair with the least sum. Let\u2019s com-\npute the sums for each pair,identify the smallest one.\ncode\nsums = [(pair, sum(pair)) for pair in factor pairs]\nmin(sums, key=lambda x: x[1])\n>>> ((18, 42), 60)\nThe pair with the smallest sum is (18, 42), and the\nsum is 60.So, the least possible value is\n60 .\u2713\nFigure 1: Example from the 277th number theory problem in the MATH dataset: (a) Prompt 1 gen-\nerates intermediate text-only reasoning steps. (b) Prompt 2 generates intermediate text reasoning\nsteps and post-hoc Python code, akin to PAL (Gao et al., 2023). (c) Basic prompt that set no con-\nstrain on code usage. Details of the three prompts are presented in Sec. 3.1. (For more examples,\nsee Tab. 9 and Tab. 10.)\n\u2022 We introduce the innovative explicit code-based self-verification (CSV) prompt, which\nleverages GPT4-Code\u2019s advanced code generation mechanism. This prompt guides the\nmodel to verify the answer and then reevaluate its solution with code. CSV not only ex-\ntends the verification to the logic behind problem-solving but also improves the efficacy of\nthe majority voting method by integrating the verification states.\n\u2022 Additionally, we have contributed to the LLM community by creating two new instruction-\nfollowing datasets: MATH-code and MMLU-Math-code. These datasets are designed to\nenhance the mathematical reasoning capabilities of open-source models.\n2\nRELATED WORK\nChain-of-Thought Reasoning. The Chain-of-Thought (CoT) prompting approach proposed by Wei\net al. (2022) is a notable contribution that showcases the multi-step reasoning capabilities of LLMs.\nBy simply adding \u201cLet\u2019s think step by step\u201d before questions, Kojima et al. (2022) implements Zero-\nshot-CoT, which can serve as a strong zero-shot baseline. Further research extends the reasoning\n3\ncapabilities of CoT by applying majority voting to improve self-consistency (Wang et al., 2023),\nchoosing few-shot examples and output chains with more complex reasoning steps (Fu et al., 2022),\nbreaking down the problem to simpler sub-problems (Zhou et al., 2023), or even expanding Chain-\nof-thought to Tree-of-Thoughts (Yao et al., 2023). Similar to Zero-shot-CoT, our method apply \u201cstep\nby step\u201d-like prompts to regularize GPT4-Code\u2019s use of code without the careful design of step-by-\nstep few-shot examples. Additionally, We enhance majority voting to verification-guided weighted\nmajority voting, leveraging the results of CSV as voting weights.\nSolving Math Problems with Code. Large language models have been found to be less accurate in\nperforming arithmetic calculations, such as addition, subtraction, multiplication, etc (Cobbe et al.,\n2021; Lewkowycz et al., 2022; Gao et al., 2023; Lu et al., 2022). Consequently, previous works\nhave attempted to solve math problems with the assistance of code. The GSM8K dataset (Cobbe\net al., 2021) uses calculation annotations to extract all arithmetic calculations solved by an external\ncalculator: the Python eval function. To further leverage the role of code in LLMs, Program-Aided\nLanguage model (PAL) (Gao et al., 2023) as well as Program of Thoughts (PoT) (Chen et al., 2022)\ninterpret the math problems as Python codes and execute the codes with an external Python in-\nterpreter to obtain the answer. Although they can get more accurate answers than some non-code\nmethods, many generated codes have execution errors or get wrong answers due to the lack of veri-\nfication mechanism. Our approach not only utilizes the ability of GPT4-Code to generate multi-step\ncodes and refine codes that fail to run, but also uses CSV to enhance the reliability and accuracy of\nthe answers.\nSelf-Verification. Human problem solving is not always a one-time success, but rather requires iter-\native thinking, verification, and refinement. Unlike previous studies that train an additional verifier\nto verify the correctness of final answers (Cobbe et al., 2021) or intermediate steps (Lightman et al.,\n2023; Li et al., 2023), Weng et al. (2023) showed the self-verification abilities of LLMs by gener-\nating multiple answers and ranking them by self-verification scores. Furthermore, SELF-REFINE\nproposed by Madaan et al. (2023) iteratively refines its output through self-generated feedback.\nUnlike these self-verification methods that require LLMs to give verification feedback in natural\nlanguage, our method applies generated codes to verify the answers and votes on different answers\nbased on the verification results, thus improving the accuracy of the verification and making full use\nof the information in the verification process.\n3\nMETHOD\nWe first conduct a pilot experiment with GPT4-Code on the challenging MATH dataset (Hendrycks\net al., 2021). Remarkably, it achieves an accuracy of 69.7%, significantly surpassing the previ-\nous state-of-the-art performance of 53.9% (Zheng et al., 2023). Encouraged by the compelling\nperformance of GPT4-Code, we strive to systematically explore and analyze its underlying code\nmechanisms. In Sec. 3.1, we illustrate, via our code-constrained prompts design, that GPT4-Code\u2019s\nrobust performance in solving math problems derives not only from its ability to generate accurate\nstep-by-step code, but also from its self-debugging mechanism. In Sec. 3.2, we aim to leverage\nGPT4-Code\u2019s self-debugging strengths to further improve its mathematical problem-solving ability.\n3.1\nPILOT EXPERIMENTS ON ANALYZING CODE USAGE OF GPT4-CODE\nTo explore the impact of code usage on GPT4-Code\u2019s math problem-solving capabilities, we adopt a\nstraightforward approach by constraining GPT4-Code\u2019s interaction with code through thoughtfully\nconstructed prompts. Specifically, we introduce two code-constrained prompts and the basic prompt\nfor comparison:\n\u2022 Prompt 1: No code usage is allowed: In response to this prompt, GPT4-Code is prohibited\nfrom incorporating code into its solutions. This prompts GPT4-Code to rely solely on\nNatural Language (NL) reasoning chain, resembling solutions in the CoT framework (Wei\net al., 2022). The resulting sequence of reasoning steps is depicted as CNL, with an example\ngiven in Fig. 1 (a).\n\u2022 Prompt 2: Code can be used only once: In this prompt setting, GPT4-Code is permitted\nto employ code exclusively within a single code block to generate the solution, mirroring\n4\n1\n2\n3\n4\nPrompts\n60\n62\n64\n66\n68\n70\n72\n74\n76\nAccuracy\n60.80\n67.58\n69.94\n74.48\n6.78\n4.54\n(a)\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nLevel 5\nLevels\n40\n50\n60\n70\n80\n90\n100\nAccuracy\n(b)\nBasic Prompt\nPrompt 1\n(You are not allowed to use any code.)\nVerification Prompt\n(Please verify your answer using code interpreter by yourself.)\nPrompt 2\n(You can use code only 1 time.)\nCode Usage Frequency\n3.0\n2.0\n1.5\n1.0\n0.5\n0.1\nFigure 2: Performance on MATH dataset of different levels by applying different prompts to adjust the fre-\nquency of code usage. (a) Comparison of overall accuracy between the 4 prompts. (b) Code usage frequency\nis in proportion to accuracy in all five levels and this phenomenon is especially apparent when the problems are\nrelatively complicated (i.e. with higher level).\nthe PAL approach introduced by Gao et al. (2023). We denote this sequence as CSL, rep-\nresenting a series of Symbolic Language (SL), such as Python. An example is shown in\nFig. 1 (b).\n\u2022 Basic Prompt: GPT4-Code is prompted to tackle the problem without any restrictions on\ncode usage. This prompt leads to GPT4-Code\u2019s typical functioning pattern, which can be\ndenoted as C = ((c1NL, c1SL), (c2NL, c2SL), . . . ), representing a sequential list of rea-\nsoning steps, each consisted of both natural language and Python code, with an example\nshown in Fig. 1 (c).\nApart from the specific example in Fig. 1, we introduce Code Usage Frequency to record the number\nof the code execution for different prompts. The results of the experiments using these prompts are\nshown in Fig. 2 (b). This figure illustrates a positive correlation between the better performance of\nGPT4-Code and the higher Code Usage Frequency. More specifically,\nPrompt 1 v.s. Prompt 2: Prompt 1 results in almost negligible code usage, while Prompt 2 results\nin approximately 1 time\u2019s code usage. Prompt 2 yields an accuracy gain of 6.9 percent over Prompt\n1. This suggests that the Python code chains CSL, can improve computational capability more\nthan the natural language chains CNL. This observation is consistent with the findings in previous\nPython-based prompting methods (Gao et al., 2023; Chen et al., 2022). However, employing code\nonly once comes with an inherent drawback \u2013 the model lacks the ability to self-debugging when\nthe code output triggers an error or produces an implausible outcome.\nPrompt 2 v.s. Basic Prompt: The Basic Prompt consistently produces solutions that entail multiple\ninstances of code usage, resulting in a large Code Usage Frequency. Additionally, the Basic Prompt\nexhibits notably enhanced accuracy. These improvements in Code Usage Frequency and accuracy\nmight be attributable to two unique advantages: (1) Generating code in brief and frequent segments,\ndivided among natural language reasoning steps, tends to result in higher accuracy. (2) The model\npossesses the capability to evaluate the results of code execution and make corrections to solution\nsteps if the outcomes contain bugs or are deemed illogical, as illustrated in Tab. 7 and Tab. 8.\nFrom these observations, it is plausible to enhance and build upon the favorable attributes of GPT4-\nCode, to further improve its precision in tackling math problems.\n3.2\nEXPLICIT CODE-BASED SELF-VERIFICATION PROMPTING\nInspired by the observations on Code Usage Frequency analysis, we seek to harness the capabilities\nof GPT4-Code. These capabilities include the model\u2019s aptitude for generating accurate code, evalu-\nating the outcomes of code execution, and automatically adjusting reasoning steps of solutions when\n5\nQuestion\nQuestion:\nWhat is the smallest positive integer solution\nto ( 100,000,000\nx4\n) < 1?\nAnswer: 101\n(a) Example: w/o self verification\nBreaking this down into subproblems, the first step is to rearrange\nthe inequality to isolate x.\nWe can do this by multiplying both sides of the inequality by x4 and\nthen taking the fourth root of both sides.\nThis gives us x >\n4\u221a\n100000000.\nLet\u2019s solve for x.\ncode\nimport math\nroot = math.pow(100000000, 1/4)\n>>> 100\nSince 100 is already a positive integer, the smallest positive integer\nsolution to the inequality 100,000,000\nx4\n< 1 is x = 100.\nThe final answer is\n100 .\u2717\n(b) Example: w/ self verifications\nWe can firstly change the inequality into x >\n4\u221a\n100000000.\nThen let\u2019s evaluate the fourth root of 100,000,000.\ncode\nimport math\nroot = math.pow(100000000, 1/4)\nsmallest integer = math.ceil(root)\n>>> (100.0, 100)\nThus, the smallest positive integer x is 100.\nLet\u2019s verify this solution.\ncode\nverification = 1e8 / (smallest integer \u2217\u2217 4) < 1\n>>> False\nThe smallest integer x should be the next integer greater than 100,\nnot 100 itself.\nLet\u2019s calculate that next integer.\ncode\nsmallest integer = 100 + 1\nverification = 1e8 / (smallest integer \u2217\u2217 4) < 1\n>>> (101, True)\nUpon substituting x = 101 back, it holds true, which verifies our\nsolution.\nThe final answer is\n101 .\u2713\nFigure 3: Question from the 712th intermediate algebra problem in the MATH dataset. (a) Without self-\nverification, the model generates a wrong answer. (b) With self-verification, the model corrects the error and\ngenerates the correct answer. The CSV prompt: To solve the problem using code interpreter step by step, and\nplease verify your answer using code interpreter.\nneeded. However, despite these advantages, GPT4-Code currently falls short in assuring solution\ncorrectness. Consequently, our objective is to utilize these strengths to augment solution verification.\nTo achieve this objective, we propose the technique termed as explicit code-based self-verification\n(CSV). This method prompts GPT4-Code to explicitly validate its answer through code generation.\nBy implementing this prompt, we introduce an extra verification stage to the solution C, referred\nto as V. The verification outcome V can be classified as True, False, or Uncertain. An Uncertain\nclassification indicates that GPT4-Code encountered difficulties in identifying an effective method\nfor answer verification, thereby abstaining from delivering a definitive verification result. Leverag-\ning GPT4-Code\u2019s inherent autonomous capabilities, we can formulate the proposed prompting as\nfollows:\nC \u2192 V =\n\uf8f1\n\uf8f2\n\uf8f3\nTrue\n\u2192 final answer\nFalse\n\u2192 Cnew \u2192 V \u2192 \u00b7 \u00b7 \u00b7 \u2192 True \u2192 final answer\nUncertain\n\u2192 final answer\nAn example is presented in Fig. 3 (b). Incorporated with CSV, the model becomes capable of using\ncode to verify answers, then reviewing and adjusting how it arrived at the solution if the verification\nresult is False, aiming at obtaining the correct answer. Upon refining and correcting the initial\nsolution, we anticipate a notable increase in accuracy. It is worth noting that both the verification\nand rectification stages are code-based. This inevitably results in increased Code Usage Frequency,\nakin to the aforementioned analysis, which will be further demonstrated in subsequent experiments.\nWe perform experiments with CSV, and these results can be found in Fig. 2. The experiment here is\nconducted with GPT4-Code on MATH (Hendrycks et al., 2021). In Fig. 2 (b), the accuracy achieved\nwith our proposed CSV prompt consistently surpasses that of the Basic Prompt across all designated\ndifficulty levels3. Meanwhile, the Code Usage Frequency receives a clear increase.\n3Human-perceived easier problems are categorized under Level-1 difficulty as per Hendrycks et al. (2021).\n6\nBefore the advent of GPT4-Code, prior frameworks (Lightman et al., 2023; Cobbe et al., 2021)\ndepended on an external LLM to use natural language for verification and well-designed few-shot\nexample prompts. In contrast, our approach simplifies the process by relying solely on a straightfor-\nward prompt for GPT4-Code, all in a zero-shot manner. This enables GPT4-Code to autonomously\nverify and independently rectify its solutions using the advanced code execution mechanism, thereby\neliminating the need for customized few-shot examples.\nGiven that CSV can effectively verify problem-solving answers, we can naturally integrate the verifi-\ncation states into majority voting, akin to the methodology embraced in self-consistency CoT (Wang\net al., 2023). Answers deemed True through verification are generally more trustworthy, reflecting\nthe problem-solving approach seen in human cognition (Newell & Simon, 1972; Wang & Chiew,\n2010). This improved reliability can be leveraged in the widely-used majority voting process. To ex-\nploit this insight, we introduce verification-guided weighted majority voting, which assigns different\nweights to the states of the verification process.\nIn practice, it sometimes occurs that once an answer is confirmed as False, no additional verification\nis conducted, yielding a False verification state. We allocate corresponding weights these states of\nTrue, Uncertain, False: wT, wU, and wF, respectively.\nSimilar to the Self-consistency with CoT (CoT-SC) (Wang et al., 2023) in Fig. 4 (a)(ii), our frame-\nwork can sample k paths. For simplicity, we extract pairs of final answers and their corresponding\nverification results from k solutions, represented as (vi, ai), i = 1, 2, . . . , k, where vi and ai denote\nthe i-th final answer and final verification result, respectively.\nSo the voting score for each candidate answer a can be expressed as:\nScore(a) =\nX\n{vi}\nwv(#{i | ai = a and vi = v}),\nv \u2208 {True, Uncertain, False},\n(1)\nHere, a represents a candidate answer, v denotes the state of verification, and wv is an element from\nthe set {wT, wU, wF}. Each wv signifies the degree of confidence associated with its corresponding\nverification state.\nFinally, we select the answer with the highest score from all candidate answers,\nOutput = arg max\na\nScore(a),\n(2)\nwhere Score(a) refers to the score of answer a according to Eq. 1.\nIt should be noted that when wv = 1 for all wv \u2208 {wT, wU, wF}, Eq. 1 becomes equivalent to\nthe naive majority voting employed in Self-Consistency with CoT (CoT-SC) (Wang et al., 2023).\nTypically, we set wT > wU > wF, which means that an answer verified true has a greater confidence\nthan the one with an uncertain state of verification, while an answer verified false has the lowest\ndegree of confidence. An example of the calculation process within verification-guided weighted\nmajority voting is illustrated in Fig. 4.\n4\nEXPERIMENTS\n4.1\nPERFORMANCE ON MATH\nThe MATH dataset (Hendrycks et al., 2021) is recognized as the most challenging math word prob-\nlem dataset, as also highlighted by Chen et al. (Chen et al., 2023). Most of our experiments and the\ncorresponding analyses are performed on the MATH benchmark. Tab. 1 compares the performance\nof the GPT4-Code against other models. GPT4-Code reaches 69.69% on MATH (Hendrycks et al.,\n2020), largely surpassing the previous state of art result (53.90%), which shows that GPT4-Code ex-\nhibits strong abilities in solving math problems and is used as our baseline for ablation study. On\ntop of GPT4-Code, our method further improves its accuracy, raising the result to 73.54% after\nadding explicit code-based self-verification, and 84.32% after adding both explicit code-based self-\nverification and verification-guided weighted majority voting (the number of sampled paths is 16).\n7\nReasoning\nCode\nReasoning & Code\nVerification\nAnswer\nVerification-guided \nweighted majority \nvoting\nQuestion: Given that the repetend in the decimal representation of 1\n19 contains 18 \ndigits, find the 39th digit in the decimal representation.\n\ud835\udc64\ud835\udc47 = 1\n\ud835\udc64\ud835\udc48 = 0.5\n\ud835\udc64\ud835\udc39 = 0.2\nGiven that the sequence \nrepeats every 18 digits, the \n39th digit will be the same as \nthe 3rd. So, we need to find \nthe 3rd digit in the decimal \nrepresentation of 1\n19\nCode execution\u2026\u2026\nThe 3rd digit in the decimal \nrepresentation of \n1\n19 is 2.\nNow let\u2019s verify if this \nsolution is equal to the 39th\ndigit.\nThe verification result is True. \nTherefore, our final solution \nis \nVerified: True\n2\nIf we calculate the first few \ndecimal places of 1\n19, we will\nsee a pattern emerge. Let's \ncalculate this using Python.\nCode execution\u2026\u2026\nThe 39th digit will be the \nsame as the 21st, which is \nactually the 3rd digit in our \ncalculated sequence. Let's \nfind this digit.\ngiven the repeating nature \nof the decimal \nrepresentation of 1\n19, we can \nconfidently state that the \n39th digit will indeed be 2\nSince the decimal \nrepresentation repeats every \n18 digits, the 39th digit will be \nthe same as the 21st digit.\nThe result is 0. Now let\u2019s \nverify the answer.\nCode execution\u2026\u2026\nCode execution\u2026\u2026\nThe pattern of 18 repeating \ndigits is \n\"052631578947368421\". The \n21st digit, which is the same \nas the 39th digit, is '5', not \u20180\u2019. \nThe verification result is False, \nOur final result is\nVerified: False\n5\n5\u2179\nCandidate answers:\n2 \u221a\nMajority voting:\nVerification-guided \nweighted majority \nvoting:\nTrue    Uncertain  False\n2:      4 \n  (    3,            1, \n  0    )\n5:      6 \n  (    1, \n   1, \n  4    )\n\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 2 = 1 \u00d7 3 + 0.5 \u00d7 1 + 0.2 \u00d7 0 = 3.5\n\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 5 = 1 \u00d7 1 + 0.5 \u00d7 1 + 0.2 \u00d7 4 = 2.3\nCode execution\u2026\u2026\nCode execution\u2026\u2026\nVerified: Uncertain\nexample\n(a)\n(b)\n(ii) CSV\nMajority \nVoting\n(i) SC CoT\n\ud835\udc64\ud835\udc47\n\ud835\udc64\ud835\udc48\n\ud835\udc64\ud835\udc39\nOutput\nOutput\nInput\nInput\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n4 < 6\n3.5 > 2.3\nFigure 4: (a) Illustration of the Naive majority voting (Wang et al., 2023) and our Verification-guided\nweighted majority voting. The full pipeline of the proposed Verification-guided Weighted Majority\nVoting framework. We use the model to generate several different solutions. Then we detect the\nself-verification state of each solution, and classify them into three states: True, Uncertain, and\nFalse. According to the state of the verification, we assign each solution a different weight, and use\nthe classified result to vote the score of each possible answer.\nTable 1: Accuracy (%) on MATH dataset. VW-voting is the abbreviation for the verification-guided\nweighted majority voting. (Overall: The results across various MATH subtopics (Hendrycks et al.,\n2021))\nCode-based\nVW-\nIntermediate\nPrecalculus\nGeometry\nNumber\nCounting &\nPreAlgebra\nAlgebra\nOverall\nVerification\nVoting\nAlgebra\n\u2013\n\u2013\nTheory\nProbability\n\u2013\n\u2013\nMATH\nGPT-4\n\u2717\n\u2717\n-\n-\n-\n-\n-\n-\n-\n42.20\nGPT-3.5\n\u2717\n\u2717\n14.6\n16.8\n22.3\n33.4\n29.7\n53.8\n49.1\n34.12\nGPT-4 (CoT)\n\u2717\n\u2717\n23.4\n26.7\n36.5\n49.6\n53.1\n71.6\n70.8\n50.36\nGPT-4 (PHP)\n\u2717\n\u2717\n26.3\n29.8\n41.9\n55.7\n56.3\n73.8\n74.3\n53.90\nGPT4-Code\n\u2717\n\u2717\n50.1\n51.5\n53.4\n77.2\n70.6\n86.3\n83.6\n69.69\nGPT4-Code + CSV\n\u2713\n\u2717\n56.6\n53.9\n54.0\n85.6\n77.3\n86.5\n86.9\n73.54\nImprovement\n+6.5\n+2.4\n+0.6\n+7.6\n+6.7\n+0.2\n+3.3\n+3.85\nGPT4-Code + CSV + Voting\n\u2713\n\u2713 (k=16)\n74.4\n67.8\n64.9\n94.1\n89.0\n91.6\n95.6\n84.32\nImprovement\n+24.3\n+16.3\n+11.5\n+16.9\n+18.4\n+5.3\n+12.0\n+14.63\nNote that this astonishingly high result is based on the strong abilities of the base model GPT4-Code,\nand our method amplifies its good qualities of GPT4-Code, with the ability to verify solutions.\nNote that although adding Code-based Self-verification can improve the performance of every indi-\nvidual subject, the extent of improvement varies from subject to subject, from 7.6% to only 0.6%.\nIn particular, the Geometry problem only has an increased accuracy of 0.6%, even though the orig-\ninal GPT4-Code accuracy is only 54.0%, which is low among the subjects. This discrepancy may\nbe attributed to the fact that solving geometry problems often requires multi-modality (Chen et al.,\n2023), a concept beyond the scope of this paper.\n4.2\nPERFORMANCE ON OTHER DATASETS\nIn addition to the challenging MATH dataset, we have also performed our method on other reasoning\ndatasets such as GSM8K (Cobbe et al., 2021), MMLU-Math, and MMLU-STEM (Hendrycks et al.,\n2020). The corresponding results can be viewed in Tab. 2 and Tab. 3. When integrated on top of GPT-\n8\nTable 2: Performance on GSM8K dataset.\nMethod\nSampled paths\nAccuracy(%)\nGPT-3.5 (5-shot)\n\u2013\n57.1\nGPT-4 (5-shot CoT)\n\u2013\n92.0\nGPT-4 (PHP)\n40\n96.5\nGPT-4 (Model selection)\n15\n96.8\nGPT4-Code\n-\n92.9\nGPT4-Code + CSV + Voting\n5\n97.0\nTable 3: Performances on MMLU dataset.\nMethod\nDataset\nAccuracy(%)\nFew-shot\nChinchilla (Hoffmann et al., 2022)\nMath\n35.7\n5-shot\nGalactica (Taylor et al., 2022)\nMath\n41.3\n5-shot\nGPT4-Code\nMath\n87.5\nzero-shot\nGPT4-Code + CSV + Voting\nMath\n89.2\nzero-shot\nLLaMA 2\nSTEM\n58.0\n5-shot\nOpenLLM\nSTEM\n70.6\n5-shot\nGPT-4\nSTEM\n82.7\nzero-shot\nGPT4-Code\nSTEM\n86.8\nzero-shot\nGPT4-Code + CSV + Voting\nSTEM\n87.0\nzero-shot\n0\n1\n2\n3\nCode Usage Frequency\n40\n50\n60\n70\n80\n90\n100\nAccuracy\n(a) Level\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nLevel 5\nOverall\n0\n1\n2\n3\nCode Usage Frequency\n40\n50\n60\n70\n80\n90\nAccuracy\n(b) Subject\nOverall\nnumber_theory\nprecalculus\nalgebra\nprealgebra\nintermediate\nalgebra\ncounting and\nprobability\ngeometry\nFigure 5: The four points on each curve correspond to results using Prompt 1, Prompt 2, Basic\nPrompt and Code-based Self-verification Prompt, respectively. (a) The accuracy of different\nlevels at various code usage frequencies. (b) The accuracy of different subjects at various code\nusage frequencies.\n4-code, our method outperforms other methods in the competition, achieving state-of-the-art results\nacross all datasets. Other subjects in MMLU benchmarks are provided in Fig. 8. A comparative\nanalysis of our results with those of previous state-of-the-art techniques and open-source models is\nalso provided.\nTab. 2 illustrates that verification-guided majority voting is an effective framework to reduce\nthe number of sampled paths, compared to GPT-4 with model selection (Zhao et al., 2023) and\nPHP (Zheng et al., 2023).\nTab. 3 presents a comparison of our model\u2019s performance with existing models (Hoffmann et al.,\n2022; Taylor et al., 2022) on the MMLU-Math dataset and with state-of-the-art open-sourced mod-\nels4 on MMLU-STEM. The open-source models remain significantly outpaced by their closed-\nsource counterparts. To address this gap, we have made the dataset and will make it publicly\navailable in the near future. Our intention is to facilitate the fine-tuning of open-source LLMs.\nFor example, the open-source model LLaMA 2 (Touvron et al., 2023) can potentially utilize this\ndata to further bolster its math reasoning capabilities.\n4.3\nCODE USAGE FREQUENCY OF PROPOSED PROMPTS\nAnalogous to the approach taken in Sec. 3.1, we gather data to elucidate the correlation between\naccuracy and Code Usage Frequency across various dimensions - prompts (proposed CSV prompt\nas well as prompts used in pilot experiments), subjects, and difficulty levels. As shown in Fig. 5,\nthe model\u2019s behavior is in accordance with our expectations when adding the code-based prompts.\nEach line in Fig. 5 has an obvious trend of going upwards, proving that the increase of Code Usage\nFrequency induces a general improvement in accuracy. The performance gain when using more\ncode is more obvious in the higher difficulty levels, while in lower levels, the performance gain is\nnot very prominent, as shown in Fig. 5 (a). Also, the Code Usage Frequency increases steadily with\n4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n9\nTable 4: Comparison Self-verification with/without explicit code-based prompt (Overall:The results\nacross various MATH subtopics (Hendrycks et al., 2021))\nVerification\nIntermediate\nPrecalculus\nGeometry\nNumber\nCounting &\nPreAlgebra\nAlgebra\nOverall\nMethod\nAlgebra\n\u2013\n\u2013\nTheory\nProbability\n\u2013\n\u2013\n\u2013\nGPT4-Code\nwithout Verification\n50.1\n51.5\n53.4\n77.2\n70.6\n86.3\n83.6\n69.69\nInterpreter\nNature Language\n52.6\n48.7\n50.8\n79.9\n72.5\n83.1\n82.6\n69.29\n+2.5\n-2.8\n-2.6\n+2.7\n+1.9\n-3.2\n-1.0\n-0.40\nCode-based\n56.6\n53.9\n54.0\n85.6\n77.3\n86.5\n86.9\n73.54\n+6.5\n+2.4\n+0.6\n+8.4\n+6.7\n+0.2\n+3.3\n+3.85\nthe increase of difficulty levels. This shows that the harder math problems require more frequent\ncode usage, which implies that invoking code multiple times might be an important reason why\nGPT4-Code have such an advantage in solving difficult math problems. There is a similar trend in\nFig. 5 (b).\n4.4\nABLATION STUDY AND DISCUSSION\nComparisons between Natural Language and Code-based Self-Verification: to underscore the\nsignificance of code in the self-verification stage, we employed a distinct natural language self-\nverification prompt. In this approach, GPT4-Code is directed to verify the solution through natural\nlanguage instead of relying on code-based verification, as presented in Tab. 4. The accuracy achieved\nwith this method was slightly lower than that of the Basic Prompt. Moreover, we observed a decline\nin accuracy for 4 of the 7 subtopics, indicating that relying solely on natural language verification\ncan not only compromise accuracy but also negatively impact performance. In contrast, code-based\nverification enhances accuracy across all 7 subtopics when compared to the Basic Prompt.\nAnalysis of Verification-guided Weighted Majority Voting: we initially compiled the confusion\nmatrix (TP/TN/FP/FN), capturing solutions with self-verification that matches the True and False\nstates mentioned in Eq. 1 from five distinct sampled paths. The details of the confusion matrix\nare presented in Appendix A.1.1. From this data, we computed Precision, Recall, and Accuracy.\n(Solutions in the True state are seen as positive.) The results are presented in Fig. 6. In comparison\nto Accuracy, we observed numerical enhancements of 22.3% and 5.6% in the average Precision and\nRecall, respectively. In particular, the average Precision registered at 95.88%. This implies that the\nAccuracy has the potential to become much higher, if more solutions reach the verified True state\nbefore giving the final answer.\nHyper-parameters ablation in Verification-guided Weighted Majority Voting: we also per-\nformed ablation studies on the hyper-parameter wv \u2208 {wT, wU, wF} in Eq. 1. When the hyper-\nparameter setting satisfied wT > wU \u2265 wF, the performance of the verification-guided weighted\nmajority voting consistently surpassed that of the naive majority voting methods across all sampled\npaths. In contrast, when we set the hyper-parameter (wT = 0.5, wU = 0.5, wF = 1), the perfor-\nmance under this configuration was worse than the naive majority voting. Therefore, our proposed\nmethod, verification-guided weighted majority voting, is easy to tune and robust.\n5\nCONCLUSION AND LIMITATION\nIn this paper, we begin with pilot experiments on GPT4-Code to explore how its use of code impacts\nits performance in mathematical reasoning. By analyzing Code Usage Frequency and accuracy, we\ndetermine that GPT4-Code\u2019s skill in solving math problems can be largely attributed to its ability\nto generate and execute code, as well as its effectiveness in adjusting and rectifying solutions when\nconfronted with implausible execution outputs. Expanding on this understanding, we introduce the\nideas of explicit code-based self-verification and verification-guided weighted majority voting, with\nthe goal of enhancing GPT4-Code\u2019s mathematical capabilities.\nHowever, there are limitations in our work that we plan to explore further in the future. Firstly,\nour analysis and improvements are currently focused on GPT4-Code, which is somewhat restrictive.\nWe aim to apply the methods to other LLMs. Secondly, our explicit code-based self-verification\nand verification-guided weighted majority voting technique could potentially create more accurate\ndatasets. These datasets would include detailed step-by-step code-based solution generation and\n10\n0\n1\n2\n3\n4\nDifferent Reasoning Path\n60\n70\n80\n90\n100\nValue\nAvg.\n 95.88\nAvg.\n 79.11\nAvg.\n 73.54\n(a)\nPrecision\nRecall\nAccuracy\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16\n# Sampled Reasoning Paths\n72\n74\n76\n78\n80\n82\n84\nAccuracy\n(b)\n1/0/0\n0.5/0.5/1\n1/0.5/0.2\n1/1/1\n(Majority Voting)\nFigure 6: (a). shows the precision, recall, and accuracy on different reasoning paths. (b). shows the\naccuracy in response to the number of sampled reasoning paths when the weight is set to different\nvalues.\ncode-based validation, which could help improve open-source LLMs like LLaMA 2 (Touvron et al.,\n2023) and enhance their mathematical abilities. Although we haven\u2019t yet investigated this approach,\nwe leave it for future work.\n11\nREFERENCES\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks, 2022.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and\nTony Xia. Theoremqa: A theorem-driven question answering dataset, 2023.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,\n2021.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting\nfor multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning, pp. 10764\u201310799. PMLR, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong\nSong, and Jacob Steinhardt.\nMeasuring massive multitask language understanding.\nArXiv,\nabs/2009.03300,\n2020.\nURL https://api.semanticscholar.org/CorpusID:\n221516475.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\ning compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. In Advances in Neural Information Processing Systems,\nvolume 35, pp. 22199\u201322213, 2022.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative\nreasoning problems with language models. Advances in Neural Information Processing Systems,\n35:3843\u20133857, 2022.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making\nlanguage models better reasoners with step-aware verifier. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315\u2013\n5333, 2023.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint\narXiv:2305.20050, 2023.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\nClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured\nmathematical reasoning. arXiv preprint arXiv:2209.14610, 2022.\n12\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nA. Newell and H.A. Simon.\nHuman Problem Solving.\nACS symposium series. Prentice-Hall,\n1972.\nISBN 9780134454030.\nURL https://books.google.com.hk/books?id=\nh03uAAAAMAAJ.\nOpenAI. Gpt-4 technical report, 2023.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for\nscience. arXiv preprint arXiv:2211.09085, 2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=1PL1NIMMrw.\nYingxu Wang and Vincent Chiew. On the cognitive process of human problem solving. Cognitive\nSystems Research, 11(1):81\u201392, 2010. ISSN 1389-0417.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\nQuoc V Le, and Denny Zhou.\nChain of thought prompting elicits reasoning in large lan-\nguage models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),\nAdvances in Neural Information Processing Systems, 2022. URL https://openreview.\nnet/forum?id=_VjQlMeSB_J.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language\nmodels are better reasoners with self-verification, 2023.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv,\nabs/2305.10601, 2023.\nXu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with\nlarge language models for reasoning. arXiv preprint arXiv:2305.14333, 2023.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting\nimproves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.\nDenny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-\nmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables\ncomplex reasoning in large language models, 2023.\n13\nAPPENDIX\nThis Appendix contains two sections. The first section provides the experiment details, including\nthe detailed experiment results on MATH and MMLU datasets. The second section presents some\nexamples of GPT4-Code.\nA\nEXPERIMENT DETAILS\nA.1\nDETAILED EXPERIMENT RESULT ON MATH DATASET\nA.1.1\nCONFUSION MATRIX\nA confusion matrix is a specific table layout that allows visualization of the performance of an algo-\nrithm. It\u2019s particularly useful for classification problems, and we utilize it to analyze the performance\nof our verification process.\nThe matrix itself is a two-dimensional grid, 2x2, for the binary classification of verification results.\nEach row of the matrix represents the instances in a predicted class, which is determined by the\nverification results given by the language model, while each column represents the instances in an\nactual class, which is determined by the actual correctness of the answer given by the model. Tab. 5\nshows how the matrix looks for our verification process:\nTable 5: Confusion Matrix of Verification\nAnswer Correct\nAnswer Wrong\nVerification True\nTP\nFP\nVerification False\nFN\nTN\nHere\u2019s what the four terms mean:\n\u2022 True Positive (TP): The cases in which the model\u2019s verification result is \u2018True\u2019, and the\nanswer is actually correct.\n\u2022 True Negative (TN): The cases in which the model\u2019s verification result is \u2018False\u2019, and the\nanswer is actually wrong.\n\u2022 False Positive (FP): The cases in which the model\u2019s verification result is \u2018True\u2019, but the\nanswer is actually wrong.\n\u2022 False Negative (FN): The cases in which the model\u2019s verification result is \u2018False\u2019, but the\nanswer is actually correct.\nThis matrix is very helpful for measuring more than just straightforward accuracy, based on which\nPrecision and Recall are two important metrics. They are defined in Eq. 3 and their meanings are as\nfollows:\n\u2022 Precision is the fraction of relevant instances among the retrieved instances. It is a measure\nof the accuracy of the classifier when it predicts the positive class.\n\u2022 Recall is the fraction of the total amount of relevant instances that were actually retrieved.\nIt is a measure of the ability of a classifier to find all the positive instances.\nPrecision =\nTP\nTP + FP, Recall =\nTP\nTP + FN\n(3)\nIn other words, precision answers the question \u201cWhat proportion of verified TRUE answers was\nactually correct?\u201d, while recall answers \u201cWhat proportion of actual correct answers was verified\nTRUE?\u201d Given its meaning, verification-guided voting is bounded to be effective when the precision\nof verification is high.\n14\nA.1.2\nPYTHON PACKAGE USAGE ANALYSIS\nTab. 6 outlines the usage of various Python packages in our experiments. Among them, we found\nthat the sympy package is utilized most frequently, highlighting its central role in the computational\ntasks performed.\nTable 6: Python package usage frequency on MATH dataset.\nPackage\nsympy\nnumpy\nmath\nfractions\nitertools\ncmath\nscipy\nmatplotlib\nfunctools\ncollections\nstatistics\nAll\n0.4168\n0.0284\n0.1590\n0.0094\n0.0034\n0.0034\n0.0016\n0.0010\n0.0004\n0.0004\n0.0002\nCorrect\n0.3907\n0.0241\n0.1638\n0.0110\n0.0029\n0.0026\n0.0009\n0.0003\n0.0003\n0.0006\n0.0003\ncorrect per code\n0.3323\n0.0205\n0.1393\n0.0094\n0.0025\n0.0022\n0.0007\n0.0003\n0.0003\n0.0005\n0.0003\nWrong\n0.4724\n0.0383\n0.1493\n0.0058\n0.0045\n0.0052\n0.0032\n0.0026\n0.0007\n0\n0\nwrong per code\n0.3194\n0.0259\n0.1009\n0.004\n0.0031\n0.0035\n0.0022\n0.0018\n0.0004\n0\n0\nc/w per code\n104%\n79%\n138%\n238%\n80%\n63%\n34%\n14%\n57%\nNaN\nNaN\nA.2\nDETAILED EXPERIMENT RESULT ON MMLU DATASET\nFig. 8 illustrates that GPT4-Code performs relatively poorly in certain domains, such as engineering\nand the humanities, with a particularly marked deficiency in virology, where it achieves a score of\nless than 60%. These observations delineate specific areas that call for further investigation and\nrefinement, thus outlining the direction for future improvements in the model.\nB\nEXAMPLES\nIn this section, we provide more examples.\n15\nFigure 7: GPT4-Code performance on MMLU dataset on: (a) subsubcategory, (b) category, (c)\nsubcategory\n16\nFigure 8: Model performance on MMLU dataset on: (a) subsubcategory, (b) category, (c) subcate-\ngory\n17\nMath Problem: 0.54 as a fraction in lowest terms.\nSure, we can solve this problem by setting up an equation, solving it, and simplifying the\nfraction.\nLet\u2019s call 0.54 as x.\nThe decimal 0.54 means that the digits \u201d54\u201d repeat indefinitely. So, we can multiply x by 100\n(since \u201d54\u201d is a two-digit number) to get another equation.\nThis gives us 100x = 54.54.\nBy subtracting the original equation from this new one, we eliminate the repeating decimal:\n100x \u2212 x = 54.54 \u2212 0.54.\nLet\u2019s perform these steps in the next Python cell.\n# define the symbol\nx = symbols(\u2019x\u2019)\n# define the equations\neq1 = Eq(x, 0.54545454545454545454545454545454)\n# approximating the\nrepeating decimal\n......\nRESULT\n---------------------------------------------------------------------\nTypeError\nTraceback (most recent call last)\n......\nApologies for the mistake. We can\u2019t directly subtract two equations in Python. Instead, we\nshould subtract the right-hand sides and left-hand sides of the equations separately to get a new\nequation. Let\u2019s correct that.\n# subtract the right hand sides and left hand sides of the equations\nseparately\nrhs = simplify(eq2.rhs - eq1.rhs)\nlhs = simplify(eq2.lhs - eq1.lhs)\n......\nRESULT\nEq(99*x, 54.0)\nAfter subtraction, we get the equation 99x = 54.\nNext, we can solve for x to express 0.54 as a fraction.\n# solve for x\nsolution = solve(new_eq, x)\nsolution[0]\nRESULT\n0.545454545454545\nThe solution for x is approximately 0.545454545454545, but we need to express it as a fraction.\nLet\u2019s convert this decimal into a fraction.\n# convert the decimal solution to a fraction\nfraction = nsimplify(solution[0])\nfraction\nRESULT\n6/11\nThe decimal 0.54 can be expressed as the fraction\n6\n11.\nAnswer:\n6\n11\nTable 7: An example in which GPT4-Code self-debugs when the code execution emits an error.\n18\nMath Problem: A bus comes by Jerry\u2019s bus stop every 20 minutes starting at exactly 5:13 a.m. If Jerry\nshows up at exactly 8:35 a.m., how many minutes will he have to wait for the next bus?\nStep 1: Calculate the total minutes from 5:13 a.m. to 8:35 a.m.\nTotal minutes from 5:13 a.m. to 8:35 a.m. is 180 + 22 minutes.\nStep 2: Calculate how many buses have come by during this time.\nStep 3: Calculate the time when the last bus before 8:35 a.m. came by.\n- The time when the last bus came by is 5 : 13 a.m. + 20 \u00d7 \u230an\u230b minutes.\nStep 4: Calculate the waiting time.\nNow, let\u2019s calculate each step.\n# Step 1: Calculate the total minutes from 5:13 a.m. to 8:35 a.m.\ntotal_minutes = 180 + 22\n......\nRESULT\n-311\nIt seems that there\u2019s a mistake in the calculations. The waiting time cannot be negative. This\nerror might have occurred in Step 3, where I calculated the time of the last bus in minutes past\nmidnight, but then in Step 4, I subtracted this from the total time from 5:13 a.m. to 8:35 a.m.\nI should correct Step 4 to calculate the time until the next bus, which is either 0 if a bus has just\narrived at 8:35 a.m., or 20 minutes minus the time since the last bus if a bus arrived before 8:35\na.m.\nLet\u2019s correct this.\n# Corrected Step 4: Calculate the time until the next bus\ntime_since_last_bus = total_minutes - last_bus_time\n......\nRESULT\n331\nThere still seems to be a mistake in the calculations. The waiting time calculated is significantly\nmore than 20 minutes, which is not possible given that the bus comes every 20 minutes.\nLooking back at the calculations, it seems that the error is in Step 3 when calculating the time\nthe last bus came. The last bus time should be calculated from 5:13 a.m., not from midnight.\nLet\u2019s correct this.\n# Corrected Step 3: Calculate the time when the last bus before 8:35\na.m. came by.\nlast_bus_time_corrected = 5 * 60 + 13 + num_buses * 20\n......\nRESULT\n331\nIt seems there is still an error. Let\u2019s try a different approach.\n1. Calculate the total minutes that have passed since the first bus at 5:13 a.m. until 8:35 a.m.\n2. Since the buses come every 20 minutes, calculate how many complete cycles of 20 minutes\nhave passed in the calculated total minutes.\n3. The time Jerry will have to wait is the remainder of the total minutes divided by 20 (which\nis the time since the last bus) subtracted from 20 (which is the bus cycle).\nThis approach should give us the correct waiting time. Let\u2019s calculate it.\n# Calculate total minutes from 5:13 a.m. to 8:35 a.m.\ntotal_minutes = 3 * 60 + 22\n# 3 hours and 22 minutes\n......\nRESULT\n18\nSo, Jerry will have to wait for 18 minutes for the next bus.\nAnswer: 18\nTable 8: An example in which GPT4-Code adjusts its solution based on the result of code execution.\n19\nMath Problem: Solve for x : 32x + 19 = 10x.\ncode 0\nThis\nis\na\ndifficult\nequation\nto solve and there\u2019s no simple\nanalytical\nsolution\nusing\nele-\nmentary functions. In real world\nscenarios,\nsuch equations are\ntypically\nsolved\nnumerically.\nFor example, using a graphing\ncalculator,\na computer,\nor a\nmathematical software.\nHowever, if you want an approx-\nimation of the solution, it is ad-\nvisable to use numerical meth-\nods such as the bisection method,\nNewton\u2019s method, or others.\nIf you would like me to provide\nan algorithm to find the solution\nnumerically, let me know.\ncode 1\nAnswer:\nThe equation you\u2019re asking to\nsolve, 32x + 19 = 10x, cannot\nbe solved algebraically due to\nthe nature of the functions and\noperations involved.\nTherefore,\nwe\u2019ll\nneed\nto\nuse\nnumerical\nmethods\nsuch\nas\nNewton\u2019s\nmethod or Bisection method for\nroot finding.\nfrom scipy.optimize\nimport fsolve\nimport numpy as np\n# Defining the\nfunction\ndef func(x):\nreturn 3**(2*x) +\n19 - 10**x\n# Initial guess for x,\nit can be\nanything, 1.0 is\nchosen arbitrarily\nhere\nx0 = 1.0\n# Using fsolve to find\nthe root\nroot = fsolve(func, x0\n)\nx = root[0]\nx\nRESULT\n1.001831315399663\nSo the answer is 1.00.\ncode more\nAnswer:\nRewrite the equation as:\n32x \u2212 10 + 19 = 0\nLet\u2019s denote 3x\n=\na,\nand\n10x = b\nso a2 \u2212 b + 19 = 0\nnow we have two equations:\n1. a = 3x\n2. b = 10x Let\u2019s solve this with\nPython.\nfrom scipy.optimize\nimport root\nimport numpy as np\n# Define the system of\nequations\ndef equations(vars):\nx, a, b = vars\neq1 = a - 3**x\neq2 = b - 10**x\neq3 = a**2 - b +\n19\nreturn [eq1, eq2,\neq3]\n.....\nRESULT:\n0.0\n32.0 + 19 = 100\nThe\nleft-hand\nside\nbecomes\n1+19=20 and the right-hand side\nbecomes 1.\nThus, the original\nequation does not hold true for\nx=0.\nLet\u2019s try a different initial guess\nfor the solution to see if that\nhelps.\n# Adjust initial guess\nfor x, a, b\nx0 = [1, 3, 10]\n# Solve system of\nequations\nsolution = root(\nequations, x0)\n# Extract solution\nx = solution.x[0]\nx\nRESULT:\n2.0000000000004463\nWe know that 32\u00b72 = 34 = 81\nand 102 = 100. The left-hand\nside becomes 100 and the right-\nhand side is also 100. Thus, the\noriginal equation holds true for\nx = 2.\nAnswer: 2\nTable 9: An example in which no restriction on code use succeeds while not using code and using\ncode only once fails.\n20\nMath Problem: What is the sum of the digits in the terminating decimal representation of the\nfraction\n4321\n57\u00b728 ?\ncode 0\nThe given fraction is\n4321\n57 \u00b7 28\nTo simplify this fraction, we can\nnotice that 57 \u00b7 28 = 107, which\nwould mean the fraction is equiv-\nalent to 4321 \u00d7 10\u22127, i.e., the\nnumber 4321 with the decimal\npoint moved 7 places to the left.\nThis would result in the number\n0.0004321.\nTo get the sum of the digits in the\nterminating decimal representa-\ntion of this number, we just need\nto add up the digits in 4321, since\nthe zeros do not contribute to the\nsum.\nSo, the sum is 4+3+2+1 = 10.\nTherefore, the answer is 10\ncode 1\nAnswer:\nFirst, we need to simplify the\nfraction.\nGiven that 4321\n=\n25 \u00b7 33 \u00b7\n50 \u00b7 70 \u00b7 110 \u00b7 131, we can\nsimplify the fraction\n4321\n57\u00b728\nto\n25\u00b733\u00b750\u00b770\u00b7110\u00b7131\n57\u00b728\n= 33 \u00b7 131 \u00b7\n2\u22123 \u00b7 5\u22127.\nNext, we need to compute the\ndecimal representation and sum\nthe digits.\n# Compute the decimal\nrepresentation of\nthe fraction\nvalue = (3**3 * 13**1)\n/ (2**3 * 5**7)\n# Convert to string\nand replace the\ndecimal point\nstr_value = str(value)\n.replace(\u2019.\u2019, \u2019\u2019)\n# Compute the sum of\nthe digits\ndigit_sum = sum(int(\ndigit) for digit\nin str_value)\ndigit_sum\nRESULT:\n18\ncode more\nAnswer:\nThe\ndenominator\ncan\nbe\nrewritten\nas\n10min(7,8)\n\u00d7\n5max(0,7\u22128)\n\u00d7\n2max(0,8\u22127),\nwhere min(a, b) is the minimum\nof a and b,\nand max(a, b)\nis the maximum of a and b.\nNow let\u2019s calculate the value of\nmin(7, 8), max(0, 7 \u2212 8), and\nmax(0, 8 \u2212 7).\nmin_ab = min(a, b)\nmax_0_a_minus_b = max\n(0, a-b)\nmax_0_b_minus_a = max\n(0, b-a)\nmin_ab,\nmax_0_a_minus_b,\nmax_0_b_minus_a\nRESULT:\n(7, 0, 1)\nNow, let\u2019s simplify the fraction\n4321\n107\u00d750\u00d721 .\n# define the numerator\nand denominator\nnumerator = 4321\ndenominator = 10**7 *\n5**0 * 2**1\n......\nRESULT:\n0.00021605\nWe exclude the zeros before the\nfirst non-zero digit and the dec-\nimal point as they do not con-\ntribute to the sum.\nThus, the sum of the digits is the\nsum of 2, 1, 6, 0, and 5. Let\u2019s\ncalculate it.\n# digits in the\nfraction\ndigits = [2, 1, 6, 0,\n5]\n# calculate the sum of\nthe digits\nsum_of_digits = sum(\ndigits)\nsum_of_digits\nRESULT:\n14\nSo, the final answer is: 14\nAnswer: 14\nTable 10: An example in which no restriction on code use succeeds while not using code and using\ncode only once fails.\n21\nMath Problem: What is the sum of all positive integers r that satisfy lcm[r, 700] = 7000 ?\nTrue\nThe\nleast\ncommon\nmultiple\n(LCM) of two numbers a and b\nis given by the formula:\nLCM(a, b) =\n|a \u00d7 b|\nGCD(a, b)\nHence,\nthe\nequation\ncan\nbe\nrewritten as:\nr \u00d7 700\nGCD(r, 700) = 7000\nWe also know that r must be a\ndivisor of 7000. So, we\u2019ll find all\nthe divisors of 7000.\n# Find all divisors of\n7000\ndivisors_7000 = sympy.\ndivisors(7000)\n# Filter out the\ndivisors\nr_values_corrected = [\nr for r in\ndivisors_7000 if r\n== 7000 * sympy.\ngcd(r, 700) //\n700]\n......\nRESULT:\n(7000, 1000)\nLet\u2019s\nverify\nour\nsolution\nby\nchecking whether the LCM of\neach possible r and 700 is indeed\n7000.\n# Verify the solution\nlcm_values_corrected =\n[sympy.lcm(r,\n700) for r in\nr_values_corrected\n]\n......\nRESULT:\n(True, True)\nUpon verification, we find that\nthe\nleast\ncommon\nmultiple\n(LCM) of each possible value of\nr and 700 is indeed 7000. Hence,\nthe sum of all positive integers r\nthat satisfy lcm[r, 700] = 7000\nis 1000 + 7000 = 8000. The\nanswer is 8000.\nUncertain\nFirst,\nlet\u2019s\nfind\nthe\nprime\nfactorization of 7000.\nThen,\nwe\u2019ll generate all the\nfactors of 7000 and find which\nof those factors are multiples of\n700.\nFinally, we\u2019ll sum up all the\npositive integers that satisfy the\ncondition.\nimport sympy\n# Get the prime\nfactorization of\n7000\nprime_factors = sympy.\nfactorint(7000)\n# Generate all factors\nof 7000\nfactors = set()\nfor factor_2_count in\nrange(\nprime_factors[2] +\n1):\nfor factor_5_count\nin range(\nprime_factors\n[5] + 1):\n......\nRESULT:\n(2: 3, 5: 3, 7: 1, [3500, 700,\n7000, 1400], 12600)\nThe prime factorization of 7000\nis 23 \u00b7 53 \u00b7 71.\nThe factors of\n7000 that are multiples of 700\nare: 3500, 700, 7000, and 1400.\nSo, the sum of all positive inte-\ngers r that satisfy lcm(r, 700) =\n7000 is 3500 + 700 + 7000 +\n1400 = 12600.\nSo the final answer is 12600.\nFalse\nIn this problem, we are given\nthat lcm(r, 700) = 7000.\nLet\u2019s first find the prime factors\nof 700 and 7000.\nimport sympy\n# Find the prime\nfactorization of\n700 and 7000\nfactors_700 = sympy.\nfactorint(700)\n......\nRESULT:\n(2: 2, 5: 2, 7: 1, 2: 3, 5: 3, 7: 1)\nSince the LCM of r and 700 is\n7000, r must contain all the prime\nfactors of 7000 that are not al-\nready in 700.\n# Generate all\ncombinations\ncombinations = list(\nproduct(*[[1, i]\nfor i in\nextra_factors]))\n......\nRESULT:\n([1, 5, 2, 10], 18)\nThe possible values of r are 1, 2,\n5, and 10.\nTo verify our answer, we can\ncheck that the least common mul-\ntiple of r and 700 is indeed 7000.\n# Remove 1 from the\npossible values of\nr\npossible_r_values.\nremove(1)\n......\nRESULT:\nFalse\nIt seems that there\u2019s a mistake.\nNot all the calculated values of r\nsatisfy the given condition.\nLet\u2019s correct the calculation.\n# All the values of r\nshould have the\nfactors of 700\nr_values = [700]\n......\nRESULT:\n([700, 1400, 3500, 7000], 18720)\nTherefore, answer is 18720.\nAnswer: 8000\nTable 11: An example in which True succeeds while Uncertain and False fails.\n22\nMath Problem: Let A = (1, \u221211, 2), B = (3, \u22124, 1), and C = (\u22122, 1, \u22121). Compute \u2220ABC,\nin degrees.\nTrue\nThe angle ABC\nis the an-\ngle between the vectors \u20d7\nBA and\n\u20d7\nBC. We can find these vectors\nas follows:\n\u20d7\nBA = \u20d7A \u2212 \u20d7B = (\u22122, \u22127, 1)\n\u20d7\nBC = \u20d7C \u2212 \u20d7B = (\u22125, 5, \u22122)\nThe angle \u03b8 between two vectors\n\u20d7u and \u20d7v can be found using the\nformula\ncos(\u03b8) =\n\u20d7u \u00b7 \u20d7v\n||\u20d7u||||\u20d7v||\nLet\u2019s calculate these values.\nimport numpy as np\n# Define vectors\nvec_BA = np.array([1,\n-11, 2]) - np.\narray([3, -4, 1])\n......\nRESULT:\n120.00000000000001\nLet\u2019s now verify this answer.\n\u20d7u \u00b7 \u20d7v = ||\u20d7u|| \u00b7 ||\u20d7v|| \u00b7 cos(\u03b8)\nSubstituting our values into this\nformula, we should get the same\ndot product (-27) that we calcu-\nlated earlier. Let\u2019s check this.\n# Compute the dot\nproduct using the\nformula\ndot_product_verification\n= magnitude_BA *\nmagnitude_BC * np.\ncos(theta * np.pi\n/ 180)\n......\nRESULT:\n-27\nwhich matches the dot product\nwe calculated earlier. Therefore,\nthe computed angle of 120\u25e6 is\ncorrect.\nUncertain\nthe\ncosine\nof\nthe\nangle\n\u03b8\nbetween them is given by\ncos \u03b8 =\nu \u00b7 v\n||u|| ||v||\nIn the given problem, we want\nto find the angle between vectors\n\u2212\u2192\nAB and \u2212\u2212\u2192\nBC, where\n\u2212\u2192\nAB = B \u2212 A = (2, 7, \u22121)\n\u2212\u2212\u2192\nBC = C \u2212 B = (\u22125, 5, \u22122)\nNow, we\u2019ll calculate the angle\nusing the formula for cos \u03b8.\nfrom numpy import\narray, dot\nfrom numpy.linalg\nimport norm\nimport math\n# Defining the vectors\nA = array([1, -11, 2])\n......\nRESULT:\n60\nThe angle \u2220ABC is 60\u25e6.\nFalse\nThe vector \u2212\u2192\nAB\nis given by\nB \u2212 A, and the vector \u2212\u2212\u2192\nBC is\ngiven by C \u2212 B.\nThe cosine of the angle between\ntwo vectors u and v is given by\ncos(\u03b8) =\nu \u00b7 v\n\u2225u\u2225\u2225v\u2225\nLet\u2019s calculate.\nimport numpy as np\n# define the points\nA = np.array([1, -11,\n2])\n......\nRESULT:\n60.00000000000001\nThe angle ABC\nis approxi-\nmately 60.0\u25e6. Let\u2019s verify this.\nWe can solve for cos(C):\ncos(C) = a2 + b2 \u2212 c2\n2ab\n# calculate vector AC\nAC = C - A\n# calculate the\nmagnitudes of AC\nmagnitude_AC = np.\nlinalg.norm(AC)\n......\nRESULT:\nFalse\nI apologize for the confusion;\nthere seems to have been a mis-\ntake in the verification.\nThe discrepancy arises due to the\ndirectionality of the vectors.\nTherefore, the initial calculation\nwas correct: the angle ABC is\napproximately 60.0\u25e6.\nAnswer: 120\u25e6\nTable 12: An example in which True succeeds while Uncertain and False fails.\n23\n"
  },
  {
    "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
    "link": "https://arxiv.org/pdf/2308.07922.pdf",
    "upvote": "16",
    "text": "RAVEN: In-Context Learning with Retrieval\nAugmented Encoder-Decoder Language Models\nJie Huang1\u2217\nWei Ping2\nPeng Xu2\nMohammad Shoeybi2\nKevin Chen-Chuan Chang1\nBryan Catanzaro2\n1University of Illinois at Urbana-Champaign, USA\n2NVIDIA, USA\njeffhj@illinois.edu, wping@nvidia.com\nAbstract\nIn this paper, we investigate the in-context learning ability of retrieval-augmented\nencoder-decoder language models. We first conduct a comprehensive analysis of\nthe state-of-the-art ATLAS model and identify its limitations in in-context learning,\nprimarily due to a mismatch between pretraining and testing, as well as a restricted\ncontext length. To address these issues, we propose RAVEN, a model that combines\nretrieval-augmented masked language modeling and prefix language modeling. We\nfurther introduce Fusion-in-Context Learning to enhance the few-shot performance\nby enabling the model to leverage more in-context examples without requiring\nadditional training or model modifications. Through extensive experiments, we\ndemonstrate that RAVEN significantly outperforms ATLAS and achieves results\ncomparable to the most advanced language models in certain scenarios, despite\nhaving substantially fewer parameters. Our work underscores the potential of\nretrieval-augmented encoder-decoder language models for in-context learning and\nencourages further research in this direction.\n1\nIntroduction\nRecent advancements in natural language processing have been predominantly driven by the devel-\nopment of large language models (LLMs) (Brown et al., 2020; OpenAI, 2022, 2023; Chowdhery\net al., 2022; Smith et al., 2022). These models have demonstrated remarkable performance across a\nwide range of tasks (Qin et al., 2023; Bubeck et al., 2023; Huang and Chang, 2023b). One of the\nkey features that enables these models to excel is their ability to perform in-context learning (Dong\net al., 2022). By conditioning on given context, LLMs can adapt to new tasks and domains without\nthe need for task-specific fine-tuning. This enables LLMs to perform well on zero-shot or few-shot\nlearning tasks, where only a limited number of examples are available.\nWhile in-context learning has been extensively studied for decoder-only language models like GPT-\n3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022), research on encoder-decoder language\nmodels, which have shown to learn stronger representations (Devlin et al., 2019; Raffel et al., 2020),\nremains limited. Notably, Patel et al. (2023) tap into the potential of mT5 (Xue et al., 2021), a\nmultilingual encoder-decoder LM, by iteratively prompting the model to produce long generations\nwith in-context examples. Chung et al. (2022); Longpre et al. (2023) finetune T5 (Raffel et al., 2020)\nwith a large mixture of tasks using instruction tuning (Mishra et al., 2022; Wei et al., 2022; Sanh\net al., 2022) to improve model performance and generalization to unseen tasks in both zero-shot and\nfew-shot settings.\nOn the other hand, LLMs still face challenges such as hallucination and limitations in representing\nthe long-tail and most recent knowledge (Mallen et al., 2022; Huang et al., 2022; Luu et al., 2022;\n\u2217 Work done during internship at NVIDIA.\narXiv:2308.07922v1  [cs.CL]  15 Aug 2023\nJang et al., 2022; Zheng et al., 2023). Retrieval-augmented language models (Izacard et al., 2022b;\nBorgeaud et al., 2022; Wang et al., 2023; Shi et al., 2023; Huang and Chang, 2023a) have emerged as\na powerful approach to address these issues by retrieving relevant knowledge from an external corpus.\nAmong these, the encoder-decoder models, such as ATLAS (Izacard et al., 2022b), stand out. They\nbenefit from the strong representation ability of a bidirectional encoder, coupled with of the efficacy\nof a Fusion-in-Decoder architecture (Izacard and Grave, 2021), enabling the effective integration\nof multiple retrieved passages. Despite these advancements, in-context learning with these models\nremains underexplored.\nIn this regard, we first conduct a comprehensive analysis of the state-of-the-art retrieval-augmented\nencoder-decoder language model, ATLAS, by experimenting with various prompting strategies. We\nfind that ATLAS exhibits a certain in-context learning ability; however, due to a mismatch between\npretraining and testing and a limited context length\u2014issues that are common to existing encoder-\ndecoder LMs trained with masked language modeling\u2014its few-shot performance is not stable and\nproviding more than, e.g., 8-shot, examples does not lead to further improvement.\nBased on the analysis, we develop RAVEN by first mitigating the mismatch between pretraining and\ntesting of ATLAS through a combination of retrieval-augmented masked language modeling and\nprefix language modeling. Moreover, to enable the model to learn from more in-context examples,\nwe propose Fusion-in-Context Learning, a novel approach that allows the model to utilize more\nin-context examples without modifying the model configuration or requiring additional training.\nFurthermore, we suggest using the retriever of the model to obtain relevant in-context examples to\nfurther enhance few-shot performance. Our empirical results demonstrate that RAVEN significantly\noutperforms ATLAS in both zero-shot and few-shot settings, even achieving comparable results to\ndecoder-only large language models in some settings despite having 180 times fewer parameters.\nThe main contributions of this paper are summarized as follows:\n\u2022 We present a comprehensive analysis of the in-context learning ability of the SOTA retrieval-\naugmented encoder-decoder language model, ATLAS, and identify aspects for improvement.\n\u2022 We develop RAVEN by combining retrieval-augmented masked and prefix language modeling.\n\u2022 We further design Fusion-in-Context Learning and In-Context Example Retrieval to enhance the\nfew-shot performance of retrieval-augmented encoder-decoder language models.\n\u2022 We demonstrate the effectiveness of RAVEN and the proposed methods through extensive experi-\nments, showcasing its superiority in various settings compared to ATLAS and other baselines.\n2\nBackground and Related Work\nIn-Context Learning. In-context learning is one of the most significant features of LLMs (e.g.,\nDong et al., 2022). While there is growing interest in this area, most studies have focused on in-\ncontext learning with decoder-only LMs, e.g., GPT-3 (Brown et al., 2020). However, bidirectional\nLMs like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) have been shown to achieve\nsuperior performance on various natural language understanding tasks, indicating that they may\noffer unique advantages for in-context learning as well. Patel et al. (2023); Chung et al. (2022)\nhave initiated exploration into in-context learning with bidirectional LMs. While these studies have\nshown promising results, there is a considerable scope for further investigation. For instance, Patel\net al. (2023) demonstrate that bidirectional models can outperform decoder-only LMs of a similar\nscale regarding in-context learning; however, there is still a significant performance gap compared to\ndecoder-only models on a much larger scale.\nRetrieval-Augmented Language Models. Retrieval-augmented language models are a class of\nlanguage models designed to enhance their performance by incorporating external knowledge. These\nmodels typically employ an information retrieval mechanism to access relevant information from\na large corpus, which is then integrated into the model\u2019s prediction process. Retrieval-augmented\nLMs can be based on both encoder-decoder (Izacard et al., 2022b; Lewis et al., 2020) and decoder-\nonly (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022) architectures. While there\nhas been some research on in-context learning with retrieval-augmented decoder-only LMs, which\ncan be straightforwardly implemented by concatenating retrieved passages with the query as the\ninput of the LM (Mallen et al., 2022; Shi et al., 2023; Khattab et al., 2022), in-context learning with\nretrieval-augmented encoder-decoder LMs, such as ATLAS, remains unexplored to the best of our\n2\nInput to Encoder:\nQuestion: What is the capital of the Provence-\nAlpes-Cote d'Azur region of France?\nAnswer: Marseilles\nQuestion: The Greek word Xero (pronounced zero)\nin xerography and related terminology means what?\nAnswer: Dry\nQuestion: In which country was the first permanent\nbungee jumping site situated?\nAnswer:<extra_id_0>\nPassage: \u2026 first permanent commercial bungee\nsite, the Kawarau Bridge Bungy at the Kawarau\nGorge Suspension Bridge near Queenstown in the\nSouth Island of New Zealand \u2026\nInput to Decoder:\nNone\nOutput:\n<extra_id_0> New Zealand\nInput to Encoder:\nQuestion: What is the capital of the Provence-\nAlpes-Cote d'Azur region of France?\nAnswer:<extra_id_0>\nQuestion: The Greek word Xero (pronounced zero)\nin xerography and related terminology means what?\nAnswer:<extra_id_1>\nQuestion: In which country was the first permanent\nbungee jumping site situated?\nAnswer:<extra_id_2>\nPassage: \u2026 first permanent commercial bungee\nsite, the Kawarau Bridge Bungy at the Kawarau\nGorge Suspension Bridge near Queenstown in the\nSouth Island of New Zealand \u2026\nInput to Decoder:\n<extra_id_0> Marseilles<extra_id_1> Dry\nOutput:\n<extra_id_2> New Zealand\nPrompting Strategy 1\nPrompting Strategy 2\nMasked Language Modeling (Pretraining)\nInput to Encoder:\nMachine learning algorithms build a model based\non sample data,<extra_id_0> as training data, in\norder to<extra_id_1> being explicitly programmed\nto do so. Machine learning algorithms are used in a\nwide variety of applications, such as in medicine,\nemail filtering, speech recognition, agriculture, and\ncomputer\nvision,<extra_id_2>\nunfeasible\nto\ndevelop\nconventional\nalgorithms\nto\nperform\nthe<extra_id_3>\nPassage: \u2026 machine learning models require a high\nquantity of reliable data in order for the models \u2026\nInput to Decoder:\nNone\nOutput:\n<extra_id_0> known<extra_id_1> make predictions\nor\ndecisions\nwithout<extra_id_2>\nwhere\nit\nis\ndifficult or<extra_id_3> needed tasks.\nFigure 1: Pretraining task of ATLAS and prompting strategies for in-context learning.\nknowledge. Despite the fact that the encoder-decoder LMs can be more efficient at incorporating\nmultiple (e.g., 40) retrieved passages (Izacard and Grave, 2021). In the following sections, we will\nstart with an analysis of ATLAS and develop our model based on the analysis.\n3\nIn-Context Learning with ATLAS\nATLAS is the state-of-the-art retrieval-augmented encoder-decoder language model, which combines\na general-purpose dense retriever and a sequence-to-sequence reader with the Fusion-in-Decoder\narchitecture. The retriever, encoder and decoder are jointly trained during the pretraining process. In\nthis process, the dense retriever, based on the Contriever model (Izacard et al., 2022a), is responsible\nfor selecting relevant passages from an external knowledge source, e.g., Wikipedia, based on the\ngiven corrupted context. The retrieved passages are then processed along with the context by the\nencoder, which generates the corresponding output, i.e., the masked spans, at the decoder (Figure 1,\nleft). ATLAS demonstrates exceptional few-shot performance on knowledge-intensive language\ntasks (Petroni et al., 2021), despite having a lower parameter count compared to other recent LLMs.\nHowever, in Izacard et al. (2022b), the few-shot performance of ATLAS is achieved by finetuning\nthe model with few-shot examples, which requires additional training and may limit its applications,\nsuch as dealing with dynamic and diverse real-time user queries like GPT-3/4 (Brown et al., 2020;\nOpenAI, 2023), where in-context learning plays a vital role. Nonetheless, the in-context learning\nability of ATLAS has not been investigated in the original paper. Therefore, in this section, we aim to\nexplore the in-context learning ability of ATLAS, using open-domain question answering (Chen et al.,\n2017) as a representative task.\n3.1\nPrompting Strategies\nTo facilitate in-context learning, an effective prompting strategy is paramount. In contrast to decoder-\nonly language models, where the input can only be fed to the decoder, encoder-decoder language\nmodels can take input in either the encoder or the decoder. In alignment with the pretraining objective\nof ATLAS, we identify two prompting strategies for in-context learning:\nStrategy 1. The first strategy involves feeding all example question-answer pairs and the target\nquestion to the encoder, without any input to the decoder. The prompt is designed as:2\nEnc: Question: q1 Answer: a1 . . . Question: qk Answer: ak Question: q0 Answer:<extra_id_0> d\nwhere (q1, a1), . . . , (qk, ak) represent example QA pairs, q0 denotes the target question, <extra_id_0>\nis a sentinel token (Raffel et al., 2020), and d is the relevant passage retrieved with q0. An example in\na 2-shot setting is illusated in Figure 1 (middle).\nStrategy 2. As the decoder of ATLAS can also accept input, we can feed the answers of in-context\nexamples to the decoder and only feed the questions to the encoder, using multiple sentinel tokens:\n2Here we present a format designed for better demonstration. The actual prompt, which follows the template\nused in the pretraining of ATLAS, can be found in Appendix A.2.\n3\nTable 1: Results of ATLAS 11B with prompting strategy 1 (S2) and strategy 2 (S2).\nNatural Questions\nTriviaQA\n0-shot\n1-shot\n5-shot\n8-shot\n0-shot\n1-shot\n5-shot\n8-shot\nATLAS\n11B S1\n26.7\n21.3\n29.8\n31.3\n56.9\n35.5\n62.3\n63.9\nATLAS\n11B S2\n21.4\n16.3\n9.8\n49.8\n48.4\n44.4\nEnc: Question: q1 Answer:<extra_id_0> . . . Question: qk Answer:<extra_id_(k \u2212 1)> Question:\nq0 Answer:<extra_id_k> d\nDec: <extra_id_0> a1 . . . <extra_id_(k \u2212 1)> ak\nAn example with this strategy is shown in Figure 1 (right). The model is expected to learn from\nin-context examples by examining both the input to the encoder and input to the decoder.\n3.2\nExperimental Setup\nWe select two widely-used datasets in the domain of open-domain question answering: Natural\nQuestions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (TQA) (Joshi et al., 2017). To assess the\nperformance, we follow the previous work (Izacard et al., 2022b) to employ the standard exact match\n(EM) metric. For the few-shot settings, we follow Brown et al. (2020) to evaluate each example in the\ntest set by generating in-context examples through randomly sampling k instances from the respective\ntask\u2019s training set. Following Izacard et al. (2022b), we use an index composed of December 2018\nWikipedia dump for NQ and an index composed of December 2021 Wikipedia corpora for TriviaQA.\nWe use the checkpoints released in the official repository3, covering sizes of 11B (XXL), 3B (XL),\nand 770M (Large). We conduct experiments with various configurations detailed in the next section.\n3.3\nResults & Analysis\n3.3.1\nEffect of prompting strategies\nWe first study the effectiveness of the prompting strategies described in \u00a73.1. Table 1 summarizes\nthe results. We find that ATLAS struggles to learn from in-context examples using strategy 2, as\nthe few-shot performance is worse than the zero-shot performance. We hypothesize that this is\nbecause the model has difficulty learning the pattern of S2 with masked language modeling during its\npretraining, since it is unlikely to obtain several consecutive question-answer pairs (or something\nsimilar) in the form of strategy 2 by randomly masking several spans in a sequence.\nOn the other hand, we observe that with strategy 1, the model does exhibit some in-context learning\nability, where the 5-shot and 8-shot performance is significantly better than the zero-shot performance\non both NQ and TriviaQA. Therefore, we choose to focus on strategy 1 for further study and disregard\nstrategy 2 for the remainder of the paper.\n3.3.2\nEffect of number of in-context examples\nThe number of in-context examples is a crucial hyperparameter for in-context learning. Generally, we\nexpect better performance from a model with more in-context examples, but there is an upper limit\ndue to 1) the maximum context length setup, e.g., 512 tokens, during the pretraining process, and 2)\nthe point at which the model has received sufficient examples and cannot gain additional information\nfrom more examples. The optimal number of in-context examples also varies between models. For\ninstance, on TriviaQA, PaLM (Chowdhery et al., 2022) exhibits better 1-shot performance than\nsettings with more examples, while this is not the case for GPT-3 (Brown et al., 2020).\nFigure 2 illustrates the impact of varying the number of in-context examples across different ATLAS\nmodel sizes. Interestingly, the 11B model demonstrates poor performance in low-shot settings, e.g.,\n1-shot, but improves significantly after 4-shot and 5-shot. Upon examining the generated responses,\nwe find that the model tends to produce answers with more tokens in low-shot settings, while the\nground truth typically consists of shorter answers with fewer than 5 tokens. By relaxing the criteria\nfor a correct prediction to include instances where the ground-truth answer is a substring of the model\noutput, we find that the 1-shot performance surpasses that of the 0-shot setting (38.3 vs 32.1 on NQ).\n3https://github.com/facebookresearch/atlas\n4\n0\n1\n2\n3\n4\n5\n8\n16\n# In-Context Examples\n22\n24\n26\n28\n30\n32\nNatural Questions, EM\nATLAS 11B (XXL)\nATLAS 3B (XL)\nATLAS 770M (Large)\n0\n1\n2\n3\n4\n5\n8\n16\n# In-Context Examples\n52\n54\n56\n58\n60\n62\n64\nTriviaQA, EM\nFigure 2: Results of ATLAS with different numbers of in-context examples.\nAll models perform well in the 5-shot and 8-shot settings, but their performance does not continue to\nimprove with more in-context examples (e.g., 16-shot). We believe this plateau may be attributed\nto two factors: 1) the sequence length constraints during ATLAS pretraining, where the maximum\ninput length to the encoder is set to 384 tokens, and the average input sequence length (excluding\npassages) is around 130 tokens; 2) the model\u2019s ability to learn adequately with 5 or 8 examples,\nmaking additional examples less beneficial.\n3.3.3\nEffect of position\nAs ATLAS is an encoder-decoder language model with a bidirectional encoder, it can also examine\nin-context examples that follow the target question to fill in the masked token. This means that we\nmay position the target question at the beginning or middle of a sequence, for example:\nQuestion: q0 Answer:<extra_id_0> Question: q1 Answer: a1 . . . Question: qk Answer: ak d\nQuestion: q1 Answer: a1 . . . Question: q0 Answer:<extra_id_0> . . . Question: qk Answer: ak d\nTable 2: Results of ATLAS\n11B (5-shot) with different\ntarget question positions.\nNQ\nTQA\nfirst\n0.7\n9.2\nrandom\n6.5\n19.5\nlast\n29.8\n62.3\nTable 2 summarizes the results. We denote the target question\u2019s po-\nsition as \u201cfirst\u201d for the beginning of the sequence, \u201crandom\u201d for a\nrandom position, and \u201clast\u201d for the original setting (S1). Interestingly,\nplacing the target question anywhere other than the last position results\nin a significant performance drop. Upon examining the generated\nanswers, we observe that when the target question is placed at the\nbeginning or in the middle, the model tends to repeat the answer or\ngenerate additional text. For example, for the prompt \u201cQuestion: What\nnumber in Bingo is sometimes referred to as Heinz varieties? An-\nswer:<extra_id_0> Question: ...\u201d. The generated text is \u201c57 \u2018Heinz\nvarieties\u2019 is a term used in Bingo to describe\u201d. This indicates that the model does not fully understand\nand follow the style of in-context examples. Therefore, in the remainder of this paper, we position\nthe target question after all the in-context examples.\n3.3.4\nEffect of number of retrieved passages\nFigure 3 illustrates the impact of the number of retrieved passages on model performance. We observe\nthat for both 0-shot and 5-shot settings, the performance of the models increases significantly with\nthe number of retrieved passages. This highlights the effectiveness of the Fusion-in-Decoder architec-\nture (Izacard and Grave, 2021) for knowledge-intensive tasks like open-domain question answering,\nand underscores the importance of pretraining language models with retrieval augmentation.\nAdditionally, the 5-shot performance consistently outperforms the 0-shot setting. This observation\nfurther emphasizes the value of providing in-context examples to improve the performance of\nretrieval-augmented encoder-decoder language models.\n3.3.5\nSummary\nIn summary, ATLAS exhibits a certain ability for in-context learning, which has been overlooked\nin previous studies. However, there are also some limitations such as unstable performance in\nlow-shot settings, and the fact that providing more in-context examples does not consistently improve\nperformance. Moreover, retrieving more relevant passages significantly enhances performance,\n5\n1\n5\n10\n20\n30\n40\n# Retrieved Passages\n14\n16\n18\n20\n22\n24\n26\n28\n30\nNatural Questions, EM\n5-shot\n0-shot\n1\n5\n10\n20\n30\n40\n# Retrieved Passages\n40\n45\n50\n55\n60\nTriviaQA, EM\nFigure 3: Results of ATLAS 11B with different numbers of retrieved passages.\ndemonstrating the significance of pretraining language models with retrieval for knowledge-intensive\ntasks. This also highlights the superiority of the encoder-decoder (Fusion-in-Decoder) architecture,\nwhich offers an advantage not available to decoder-only language models.\n4\nMethodology\nIn this section, we design methods to improve the models\u2019 zero-shot performance and in-context\nlearning abilities based on the findings and analysis presented in \u00a73.\n4.1\nRAVEN: Combining Retrieval-Augmented Masked and Prefix Language Modeling\nPrefix Language Modeling\nInput to Encoder:\nMachine learning algorithms build a model based\non sample data, known as training data, in order to\nmake\npredictions\nor\ndecisions\nwithout\nbeing\nexplicitly programmed to do so. Machine learning\nalgorithms\nare\nused\nin\na\nwide\nvariety\nof\napplications, such as in medicine, email filtering,\nspeech\nrecognition,\nagriculture,\nand\ncomputer\nvision, where it is difficult or<extra_id_0>\nPassage: \u2026 machine learning models require a high\nquantity of reliable data in order for the models \u2026\nInput to Decoder:\nNone\nOutput:\n<extra_id_0> unfeasible to develop conventional\nalgorithms to perform the needed tasks.\nFigure 4: Prefix Language Modeling.\nAs described in \u00a73, ATLAS is pretrained with a masked\nlanguage modeling objective, where the input is a corrupted\ntext with several masked spans placed randomly within the\nsequence (refer to Figure 1 (left) for an example). However,\nin testing, based on our analysis in \u00a73.3.1 and \u00a73.3.3, it is\nmost effective to place the target question after all the in-\ncontext examples, with a masked token (i.e., <extra_id_0>)\nfollowing the question (Figure 1, middle)). Thus, there exists\na mismatch between pretraining and testing of ATLAS.\nTo better align pretraining with testing, we propose to con-\ntinually pretrain ATLAS with prefix language modeling (Liu\net al., 2018; Raffel et al., 2020; Tay et al., 2023). Specifically,\nfor each sequence, we mask 10% of the tokens on average at\nthe end of the sequence with the <extra_id_0> token. Then,\nwe use the retriever of ATLAS to retrieve relevant passages\nusing the prefix and train the model to recover the suffix of\nthis sequence with the prefix and the passages as input. An\nexample of input and output for prefix language modeling is shown in Figure 4. We can observe that\nthe pretraining objective aligns more closely with the prompting strategy 1 in Figure 1. We refer to\nthe model trained with additional prefix language modeling as RAVEN4.\nStarting from the ATLAS checkpoint, which is based on masked language modeling, the training\nof RAVEN can be considered a combination of retrieval-augmented masked and prefix language\nmodeling. This methodology shares certain aspects with the mixture objective of UL2 (Tay et al.,\n2023). However, there are key differences: 1) UL2 blends various language modeling objectives\nthroughout its training process, while RAVEN applies these two objectives in a sequential order;\n2) Unlike RAVEN, UL2 is trained without retrieval. Consequently, RAVEN benefits from both the\nmasked language modeling, which contributes to a better reader and retriever as evidenced in Izacard\net al. (2022b), and prefix language modeling, which mitigates the gap between pretraining and testing.\nWe verify the effectiveness of this design by exploring different training strategies in \u00a75.2.\n4RAVEN, a bird known for its intelligence and adaptability, has the letters \u201cRA\u201d in its name, which represents\n\u201cRetrieval-Augmented\u201d in our context.\n6\n4.2\nFusion-in-Context Learning\nIn \u00a73.3.2, we observe that ATLAS\u2019s performance does not further improve with more in-context\nexamples after 8-shot. One major reason for this is the limited sequence length during the pretraining\nprocess, which makes it difficult for the model to handle long sequences during testing. Pretraining\nmodels with longer contexts would be a straightforward approach to address this issue, but it would\nsignificantly increase computation cost and GPU memory requirements. Additionally, the maximum\ninput length is also constrained by the maximum sequence length of the retriever, i.e., Contriever,\nwhich is based on BERT (Devlin et al., 2019) and has a maximum length of 512 tokens.\n\ud835\udc5e!\ud835\udc4e!; \ud835\udc5e\"\ud835\udc4e\"; \ud835\udc5e# + passage 1\n\ud835\udc5e!\ud835\udc4e!; \ud835\udc5e\"\ud835\udc4e\"; \ud835\udc5e# + passage 2\n\ud835\udc5e!\ud835\udc4e!; \ud835\udc5e\"\ud835\udc4e\"; \ud835\udc5e# + passage \ud835\udc5b\n\u2026\nencode\nencode\nencode\nconcat\n\u2026\n\u2026\n\u2026\ndecode\n\ud835\udc4e#\n\ud835\udc5e!\ud835\udc4e!; \ud835\udc5e\"\ud835\udc4e\"; \ud835\udc5e# + passage 1\n\ud835\udc5e$\ud835\udc4e$; \ud835\udc5e%\ud835\udc4e%; \ud835\udc5e# + passage 2\n\ud835\udc5e&\ud835\udc4e&; \ud835\udc5e'\ud835\udc4e'; \ud835\udc5e# + passage \ud835\udc5b\nencode\nencode\nencode\nconcat\n\u2026\ndecode\n\ud835\udc4e#\n\u2026\n\u2026\n\u2026\nStandard In-Context Learning\nFusion-in-Context Learning\nFigure 5: Standard In-Context Learning vs Fusion-in-\nContext Learning.\nAs an alternative, we propose an ap-\nproach to enable models to learn from\nmore in-context examples without mod-\nifying the model configuration or requir-\ning additional training. As described in\n\u00a73, the reader of ATLAS (and RAVEN)\nis based on the Fusion-in-Decoder ar-\nchitecture (Izacard and Grave, 2021),\nwhere multiple passages are retrieved,\nand each passage, concatenated with the\nin-context examples and target question,\nis fed to the encoder separately (Fig-\nure 5, top). To allow the model to pro-\ncess more in-context examples without increasing the length of the input to the encoder, we can\nfeed different in-context examples to the encoder with each passage (Figure 5, bottom). In this way,\nthe model can incorporate more in-context examples during its inference process. We refer to this\nstrategy as Fusion-in-Context Learning (FiCL).\nIn implementation, for a k-shot setting, such as a 64-shot setting, to effectively utilize the 64 examples,\nwe randomly shuffle these examples and select m (e.g., 5) examples in order as the input for the\nencoder each time. If all the examples have been used, we shuffle the 64 examples again. We denote\nthe configuration of FiCL as [k, m], which stands for [k-shot, m-fusion].\n4.3\nIn-Context Example Retrieval\nIn recent studies (Liu et al., 2022; Rubin et al., 2022; Su et al., 2023), it has been demonstrated that\na well-chosen selection of in-context examples can enhance in-context learning. Building on this\ninsight, we propose utilizing the retriever of RAVEN to retrieve in-context examples. Specifically, we\nuse RAVEN\u2019s retriever to build an index during the preparation step, and then, during testing, when\nthe model receives an input, it could efficiently retrieve in-context examples with its retriever.\nBy integrating RAVEN\u2019s retriever in this manner, we aim to: 1) automate in-context learning, which is\nparticularly practical for model owners who have a database of examples. Without this, users would\nneed to manually provide in-context examples; and 2) optimize the selection of in-context examples,\nthereby enhancing in-context learning and improving overall performance.\n5\nExperiments\n5.1\nExperimental Setup\nDatasets. Following \u00a73.2, we first evaluate on two widely-used open-domain question answering\ndatasets: Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Additionally,\nwe conduct a case study on long-form question answering using the ELI5 dataset (Fan et al., 2019).\nFurthermore, we assess the models\u2019 language understanding ability using the Massively Multitask\nLanguage Understanding (MMLU) benchmark (Hendrycks et al., 2021). Detailed information\nregarding the MMLU evaluation is in Appendix A.3. Other evaluation settings are the same as \u00a73.2.\nBaselines. Since RAVEN is built upon ATLAS, we choose ATLAS as a primary baseline for comparison.\nWe also compare our model with decoder-only large language models such as GPT-3 (Brown et al.,\n2020) and PaLM (Chowdhery et al., 2022) (in a closed-book setting). Additionally, for open-domain\nQA, we evaluate our approach against REPLUG (Shi et al., 2023) and RETRO (Borgeaud et al., 2022),\nas well as its improved version RETRO++ (Wang et al., 2023). These models are decoder-only\n7\nTable 3: Results of ATLAS and RAVEN on NQ and TriviaQA.\nNatural Questions\nTriviaQA\n0-shot\n1-shot\nfew-shot\nFiCL\n0-shot\n1-shot\nfew-shot\nFiCL\nATLAS\n3B\n23.7\n25.1\n28.4 (5)\n29.6 [64-5]\n54.3\n55.5\n61.1 (5)\n62.0 [64-5]\nATLAS\n11B\n26.7\n21.3\n31.3 (8)\n32.0 [64-8]\n56.9\n35.5\n63.9 (8)\n64.9 [64-8]\nRAVEN\n3B\n29.3\n31.7\n31.4 (5)\n32.8 [40-1]\n62.4\n63.2\n62.6 (5)\n63.6 [40-1]\nRAVEN\n11B\n29.6\n31.4\n32.7 (5)\n33.5 [64-5]\n65.7\n66.2\n66.7 (5)\n67.3 [64-5]\n0\n1\n2\n3\n4\n5\n8\nmore\n# In-Context Examples\n26\n27\n28\n29\n30\n31\n32\n33\n34\nNatural Questions, EM\nRAVEN 11B\n + FICL\nATLAS 11B\n0\n1\n2\n3\n4\n5\n8\nmore\n# In-Context Examples\n56\n58\n60\n62\n64\n66\n68\nTriviaQA, EM\nFigure 6: RAVEN vs ATLAS.\nlanguage models augmented with retrieval. REPLUG is based on Codex (Chen et al., 2021) and\nContriever (Izacard et al., 2022a), where the passages are retrieved by Contriever (using ensemble\nand additional adaptation) and fed directly to Codex. RETRO is a GPT model (Radford et al., 2019)\naugmented with a transformer encoder to encode the retrieved passages. RETRO++ is a variant of\nRETRO that feeds the most relevant retrieved passage into the GPT decoder while providing other\npassages to its encoder. For MMLU, we also include FLAN-T5 (Chung et al., 2022), an enhanced\nversion of T5 that has been trained on a large mixture of tasks with instruction finetuning.\nImplementation Details. We train two versions of RAVEN: 3B and 11B. We initialize both the\nretriever and the reader of the models with the weights of ATLAS (3B and 11B). To isolate the effect\nof retrieval, we do not update the retriever during the training process. We pretrain the reader using\nthe December 2021 Wikipedia corpora preprocessed by Izacard et al. (2022b), where the index is\nalso constructed using the same corpora. In accordance with Izacard et al. (2022b), we retrieve 20\npassages for each masked sequence (excluding passages identical to the original sequence). Both\nthe 3B and 11B models are trained for 5,000 steps, using AdamW optimizer (Loshchilov and Hutter,\n2019) with a batch size of 64. We employ a learning rate of 4 \u00d7 10\u22125 for the 3B model and 1 \u00d7 10\u22125\nfor the 11B model, with linear decay and 100 warmup steps. All the models are trained on NVIDIA\nA100 GPUs (80 GB). For the 3B model, we utilize 8 GPUs, whereas for the 11B model, we employ\n32 GPUs. The prompt used for prefix language modeling is detailed in Appendix A.1. During testing,\nwe default to retrieving 40 documents for all tasks. The prompts used can be found in Appendix A.2\nand Appendix A.3.\n5.2\nOpen-Domain Question Answering\nWe choose open-domain QA as our primary evaluation task, as it effectively represents knowledge-\nintensive challenges and is widely employed in real-world applications.\nRAVEN vs ATLAS. Table 3 and Figure 6 present the exact match (EM) scores for ATLAS and RAVEN\non the NQ and TriviaQA datasets. As shown in Table 3, both the 3B and 11B RAVEN models\nsignificantly outperform ATLAS. For instance, on TriviaQA, RAVEN 11B achieves an improvement of\n8.8%, 30.7%, and 2.8% in the 0-shot, 1-shot, and few-shot settings respectively, compared to ATLAS\n11B. Furthermore, as illustrated in Figure 6, the performance of RAVEN increases steadily with the\nnumber of in-context examples, while the performance of ATLAS experiences a substantial decline in\nlow-shot settings. These results demonstrate the effectiveness of RAVEN across various shot settings.\nFusion-in-Context Learning. We also report the results of models with Fusion-in-Context Learning\n(FiCL) in Table 3. For both ATLAS and RAVEN, FiCL contributes to approximately a 1% improvement,\nwhich is not attainable by standard in-context learning, where performance does not further improve\n8\nTable 4: Results on NQ and TriviaQA in comparison to the baselines.\nNatural Questions\nTriviaQA\n0-shot\n1-shot\nfew-shot\n0-shot\n1-shot\nfew-shot\nGPT-3\n13B\n7.8\n13.7\n21.0 (64)\n41.8\n51.3\n57.5 (64)\nGPT-3\n175B\n14.6\n23.0\n29.9 (64)\n64.3\n68.0\n71.2 (64)\nPaLM\n8B\n8.4\n10.6\n14.6 (5)\n39.5\n48.5\n47.2 (5)\nPaLM\n62B\n18.1\n23.1\n27.6 (5)\n67.3\n72.7\n70.1 (5)\nPaLM\n540B\n21.2\n29.3\n39.6 (64)\n76.9\n81.4\n81.4 (1)*\nCodex\n175B\n-\n-\n40.6 (16)\n-\n-\n73.6 (16)\nCodex + Contriever\n175B\n-\n-\n44.2 (16)\n-\n-\n76.0 (16)\nCodex + REPLUG\n175B\n-\n-\n44.7 (16)\n-\n-\n76.8 (16)\nCodex + REPLUG LSR\n175B\n-\n-\n45.5 (16)\n-\n-\n77.3 (16)\nRETRO\n9.5B\n8.9\n-\n-\n36.0\n-\n-\nRETRO++\n9.5B\n25.8\n-\n-\n48.3\n-\n-\nRAVEN\n3B\n29.3\n31.7\n31.4 (5)\n62.4\n63.2\n62.6 (5)\nRAVEN + FiCL\n3B\n32.8 [40-1]\n63.6 [40-1]\nRAVEN\n11B\n29.6\n31.4\n32.7 (5)\n65.7\n66.2\n66.7 (5)\nRAVEN + FiCL\n11B\n33.5 [64-5]\n67.3 [64-5]\n* For TriviaQA, PaLM\u2019s 1-shot performance surpasses other settings. We follow the original paper to report the 1-shot result.\nFor other models, we select the best k-shot (k \u2208 {2, 3, 4, 5, 8, 16}) performance or report the number in the original paper.\nTable 5: Results of ATLAS and RAVEN trained with different strategies.\nNatural Questions\nTriviaQA\n0-shot\n1-shot\n5-shot\n0-shot\n1-shot\n5-shot\nATLAS\n3B (Mask)\n23.7\n25.1\n28.4\n54.3\n55.5\n61.1\nATLAS\n3B (Mask, 5k more steps)\n22.9\n22.5\n28.1\n50.8\n50.1\n61.1\nRAVEN\u2212\n3B (Prefix)\n24.8\n29.1\n30.1\n55.4\n61.4\n62.3\nRAVEN\u2212\n3B (Mix)\n25.1\n28.4\n30.9\n56.1\n61.4\n62.2\nRAVEN\n3B\n29.3\n31.7\n31.4\n62.4\n63.2\n62.6\n(or even decreases) with more than 8 in-context examples. This demonstrates the superiority of the\nFiCL for enabling models to learn from more examples.\nComparison to SOTA. In Table 4, we compare RAVEN to other baselines. On NQ, RAVEN\u2019s zero-\nshot and one-shot performance surpasses all the baselines, including PaLM, even though RAVEN 3B\nhas 180 times fewer parameters than PaLM 540B. The zero-shot performance of RAVEN on TriviaQA\nis also on par with PaLM 62B. Furthermore, RAVEN\u2019s zero-shot performance significantly exceeds\nthat of both RETRO and RETRO++, which are models of a similar scale.\nIn the few-shot setting, with FiCL, RAVEN achieves performance comparable to GPT-3 175B and\nPaLM 62B. However, there remains a gap between RAVEN and the larger PaLM 540B and Codex\n175B models. Nevertheless, given the considerably smaller scale of RAVEN in comparison to PaLM\nand Codex, its performance can be considered impressive. The performance of RAVEN may be\nfurther improved if it is built upon a larger model, in which case its few-shot performance is likely to\nsurpass that of PaLM and Codex.\nAblation Study. We conduct an ablation study by training ATLAS and RAVEN with different\npretraining strategies. First, to isolate the effect of more training steps of RAVEN, we also train\nATLAS for 5,000 more steps using the masked language modeling objective. Results in Table 5 (row\n2) show that the performance does not improve, indicating that the performance improvement of\nRAVEN compared to ATLAS is not simply due to training for more steps.\nSecond, to verify the effectiveness of RAVEN\u2019s training strategy (i.e., first masked language modeling,\nand then prefix language modeling), we train two variants of RAVEN, starting from the T5-lm-adapt\ncheckpoint5, which is the checkpoint that ATLAS starts from. For the first variant, we use the same\nprefix language modeling objective of RAVEN. For the second variant, we train the model with a\n5https://huggingface.co/google/t5-xl-lm-adapt\n9\nmixture of masked and prefix language modeling. Specifically, we construct corrupted texts by both\nmasking 15% spans in the sequence (same as ATLAS) and replacing the suffix with a special mask\ntoken <extra_id_99> (used in testing). We train the model for 10,000 steps and update the retriever\nand refresh the index during training with the optimal strategy described in Izacard et al. (2022b).\nTable 5 (RAVEN\u2212 in row 3 and 4) summarizes the results. We find that the performance of these two\nvariants is superior to ATLAS, but inferior to RAVEN when trained using the strategy described in\n\u00a74.1. An explanation for this is that, by training with masked language modeling first, the model\ncan achieve better language understanding ability and is equipped with a more effective retriever\n(as emperically verified in Izacard et al. (2022b)). Subsequently, by training with prefix language\nmodeling, the mismatch between pretraining and testing is mitigated, resulting in improved zero-shot\nand few-shot performance.\nIn-Context Example Retrieval. \u00a74.3 suggests using RAVEN\u2019s retriever for in-context example\nretrieval. Results in Table 6 show that this approach improves RAVEN\u2019s few-shot results, especially\non NQ where a \u223c10% improvement is observed. This indicates the significant positive impact of\nincorporating more relevant examples on in-context learning and overall performance.\nTable 6:\nPerformance improvement of\nRAVEN with In-Context Example Retrieval.\nNQ\nTQA\n1-shot\n5-shot\n1-shot\n5-shot\n3B\n+9.1\n+11.6\n+0.0\n+1.6\n11B\n+9.8\n+11.1\n-0.5\n+1.0\nAdditional Results. We examine the effect of the number of retrieved documents in Appendix B.1\nand provide a case study on long-form question answering in Appendix B.2.\n5.3\nMassive Multitask Language Understanding\nTable 7: Results on MMLU.\n0-shot\n1-shot\n5-shot\nGPT-3\n13B\n-\n-\n26.0\nGPT-3\n175B\n-\n-\n43.9\nPaLM\n8B\n-\n-\n25.3\nPaLM\n62B\n-\n-\n53.7\nPaLM\n540B\n-\n-\n69.3\nT5\n3B\n-\n-\n25.7\nT5\n11B\n-\n-\n25.9\nFLAN-T5\n3B\n-\n-\n52.4\nFLAN-T5\n11B\n-\n-\n55.1\nATLAS\n3B\n43.7\n36.9\n38.5\n+ FiCL\n3B\n42.6 [40-1]\nATLAS\n11B\n47.4\n45.3\n44.2\n+ FiCL\n11B\n48.0 [40-1]\nRAVEN\n3B\n45.7\n40.0\n40.4\n+ FiCL\n3B\n44.5 [64-5]\nRAVEN\n11B\n48.9\n49.2\n48.7\n+ FiCL\n11B\n50.5 [40-1]\nTable 7 summarizes the results (accuracy) on\nMMLU. We find that the zero-shot performance\nof RAVEN is impressive, surpassing the few-shot\nperformance of GPT-3 175B and being slightly\nworse than PaLM 62B, despite having a signif-\nicantly smaller number of parameters. Further-\nmore, with the same number of parameters, the\nperformance of RAVEN is far superior to T5.\nAdditionally, even without instruction finetun-\ning, RAVEN achieves performance comparable\nto FLAN-T5, a model finetuned on a large col-\nlection of tasks. We expect further improvement\nof RAVEN by applying instruction tuning as well\nand leave it for future study.\nInterestingly, with standard in-context learn-\ning, the few-shot performance of RAVEN is\nworse than zero-shot, possibly due to the longer\nquestions and answer options in MMLU caus-\ning context length issues in the 5-shot setting.\nAlso, in the one-shot setting, since MMLU is\na multiple-choice QA task, providing only one\nexample might introduce bias in the model\u2019s\nprediction, favoring a specific option. However,\nwith Fusion-in-Context Learning, the perfor-\nmance improves significantly, leading to better\nfew-shot performance for the 11B model compared to its zero-shot performance, further demonstrat-\ning the effectiveness of FiCL.\n6\nConclusion\nIn this study, we have delved into the in-context learning ability of retrieval-augmented encoder-\ndecoder language models. We commenced with a comprehensive analysis of the state-of-the-art\nATLAS model and subsequently developed our model based on the analysis. Our extensive experi-\nmental results demonstrated that our model significantly outperforms ATLAS and achieves results\non par with some of the most advanced language models, even with substantially fewer parameters.\nThese findings highlight the potential of retrieval-augmented encoder-decoder language models in\nthe realm of in-context learning. Future work focusing on scaling up the model and studying its\nin-context learning ability is encouraged.\n10\nLimitations\nOne primary limitation of our work arises from the constrained context length inherent to the base\nmodels (e.g., T5 or ATLAS) we employed. This restriction poses challenges to the scalability of\nin-context learning, especially as the number of in-context examples increases. While our Fusion-in-\nContext Learning (FiCL) strategy does offer a mitigative approach to this constraint, an alternative\nand possibly more optimal solution might involve extending the context length. This would be\nparticularly beneficial for tasks requiring extensive inputs.\nFurthermore, when compared to some of the prevailing decoder-only language models, particularly\nthose exceeding 100B parameters, the models deployed in our research might appear relatively\ndiminutive in scale. Our endeavor partially seeks to catalyze further investigations into more powerful\nencoder-decoder models. Drawing on the benefits of scaling up and combining this with our proposed\napproaches, we believe that there is potential to develop even more powerful retrieval-augmented\nlanguage models in the future.\nReferences\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol\nVinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022.\nImproving language models by retrieving from trillions of tokens. In Proceedings of the 39th\nInternational Conference on Machine Learning, volume 162 of Proceedings of Machine Learning\nResearch, pages 2206\u20132240. PMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer\nopen-domain questions.\nIn Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1870\u20131879, Vancouver, Canada.\nAssociation for Computational Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating\nlarge language models trained on code. arXiv preprint arXiv:2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language\nmodels. arXiv preprint arXiv:2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\n11\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019.\nELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3558\u20133567, Florence, Italy. Association for Computational\nLinguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring massive multitask language understanding. In International\nConference on Learning Representations.\nJie Huang and Kevin Chen-Chuan Chang. 2023a. Citation: A key to building responsible and\naccountable large language models. arXiv preprint arXiv:2307.02185.\nJie Huang and Kevin Chen-Chuan Chang. 2023b. Towards reasoning in large language models: A\nsurvey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049\u20131065,\nToronto, Canada. Association for Computational Linguistics.\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. 2022. Are large pre-trained language\nmodels leaking your personal information? In Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2038\u20132047, Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2022a. Unsupervised dense information retrieval with contrastive learning.\nTransactions on Machine Learning Research.\nGautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models\nfor open domain question answering. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume, pages 874\u2013880, Online.\nAssociation for Computational Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022b. Atlas: Few-shot\nlearning with retrieval augmented language models. arXiv preprint arXiv, 2208.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun\nKim, and Minjoon Seo. 2022. TemporalWiki: A lifelong benchmark for training and evaluating\never-evolving language models. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 6237\u20136250, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale\ndistantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n1601\u20131611, Vancouver, Canada. Association for Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. General-\nization through memorization: Nearest neighbor language models. In International Conference on\nLearning Representations.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and\nMatei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and language models for\nknowledge-intensive nlp. arXiv preprint arXiv:2212.14024.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. 2019. Natural questions: A benchmark for question answering research. Transactions of\nthe Association for Computational Linguistics, 7:452\u2013466.\n12\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela.\n2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural\nInformation Processing Systems, volume 33, pages 9459\u20139474. Curran Associates, Inc.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022.\nWhat makes good in-context examples for GPT-3?\nIn Proceedings of Deep Learning Inside\nOut (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep\nLearning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational\nLinguistics.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summarizing long sequences. In International Conference\non Learning Representations.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for\neffective instruction tuning. arXiv preprint arXiv:2301.13688.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International\nConference on Learning Representations.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A. Smith. 2022.\nTime waits for no one! analysis and challenges of temporal misalignment. In Proceedings\nof the 2022 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 5944\u20135958, Seattle, United States. Association\nfor Computational Linguistics.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Investigating effectiveness and limitations of parametric\nand non-parametric memories. arXiv preprint arXiv:2212.10511.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task general-\nization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487,\nDublin, Ireland. Association for Computational Linguistics.\nOpenAI. 2022. Chatgpt: Optimizing language models for dialogue. OpenAI.\nOpenAI. 2023. Gpt-4 technical report.\nAjay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, and Chris Callison-\nBurch. 2023. Bidirectional language models are also few-shot learners. In The Eleventh Interna-\ntional Conference on Learning Representations.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao,\nJames Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim\nRockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language\ntasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 2523\u20132544, Online. Association\nfor Computational Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang.\n2023. Is chatgpt a general-purpose natural language processing task solver?\narXiv preprint\narXiv:2302.06476.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. J. Mach. Learn. Res., 21(1).\n13\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United\nStates. Association for Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal\nNayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,\nSheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,\nJos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan,\nTeven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask\nprompted training enables zero-shot task generalization. In International Conference on Learning\nRepresentations.\nWeijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. 2022. Nearest neighbor\nzero-shot inference. In Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3254\u20133265, Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using\ndeepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language\nmodel. arXiv preprint arXiv:2201.11990.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari\nOstendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023. Selective annotation makes\nlanguage models better few-shot learners. In The Eleventh International Conference on Learning\nRepresentations.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler.\n2023. UL2: Unifying language learning paradigms. In The Eleventh International Conference on\nLearning Representations.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. 2023. Shall we pretrain autoregressive language\nmodels with retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In\nInternational Conference on Learning Representations.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association\nfor Computational Linguistics.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why does chatgpt fall short in\nproviding truthful answers. ArXiv preprint, abs/2304.10513.\n14\nA\nAdditional Experimental Details\nA.1\nPrefix Language Modeling\nIn alignment with the pretraining of ATLAS, we design the prompt for prefix language modeling as\n{prefix}<extra_id_0> title:\n{title} context:\n{text}\nwhere {prefix} represents the prefix of an input sequence. The {title} and {text} elements are\nretrieved by the model\u2019s retriever using the prefix as a query. Here, {text} signifies the retrieved\npassage, while {title} denotes the corresponding article and section title of the passage. The model is\ntrained to generate\n<extra_id_0>{suffix}\nwhere {suffix} is the suffix (masked by <extra_id_0>) of the input sequence.\nA.2\nOpen-Domain Question Answering\nIn accordance with pretraining, we use the following prompt for open-domain question answering:\nQuestion:\n{question} Answer:<extra_id_0> title:\n{title} context:\n{text}\nFor example,\nQuestion:\nIn which country was the first permanent bungee jumping site\nsituated?\nAnswer:<extra_id_0> title:\nBungee jumping:\nModern sport\ncontext:\nfirst permanent commercial bungee site, the Kawarau Bridge Bungy\nat the Kawarau Gorge Suspension Bridge near Queenstown in the South Island\nof New Zealand.\nHackett remains one of the largest commercial operators,\nwith concerns in several countries.\nSeveral million successful jumps have\ntaken place since 1980.\nThis safety record is attributable to bungee\noperators rigorously conforming to standards and guidelines governing\njumps, such as double checking calculations and fittings for every jump.\nAs with any sport, injuries can still occur (see below), and there have\nbeen fatalities.\nA relatively common mistake in fatality cases is to use a\ncord that\nA.3\nMassive Multitask Language Understanding\nMMLU comprises 57 multiple-choice question answering datasets that span various domains, in-\ncluding elementary mathematics, US history, computer science, and more. For the evaluation on\nMMLU, we report the accuracy and use an index composed of December 2021 Wikipedia corpora.\nWe follow Izacard et al. (2022b) to apply the \u201cde-biased\u201d inference. Specifically, during inference, we\nexecute four forward passes, each corresponding to a cyclic permutation of the answer letter-option\nassignment within the question. For instance, the answer option designated to letter \u2018A\u2019 is shifted\nto \u2018B\u2019, \u2018B\u2019 to \u2018C\u2019, \u2018C\u2019 to \u2018D\u2019, and \u2018D\u2019 to \u2018A\u2019. The final prediction is obtained by summing up the\nprobabilities from these four forward passes.\nWe design the prompt in the following format:\nQuestion:\n{question} Options:\n{candidate answers} Answer:<extra_id_0>\ntitle:\n{title} context:\n{text}\nFor example,\nQuestion:\nOver time, non-volcanic mountains can form due to the\ninteraction of plate boundaries.\nWhich interaction is most likely\nassociated with the formation of non-volcanic mountains?\nOptions:\n(A)\ncontinental plates colliding with continental plates (B) continental\nplates separating from continental plates (C) oceanic plates colliding\nwith oceanic plates (D) oceanic plates separating from oceanic plates\nAnswer:<extra_id_0> title:\n...\ncontext:\n...\n15\n1\n5\n10\n20\n30\n40\n# Retrieved Passages\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\nNatural Questions, EM\n5-shot\n0-shot\n1\n5\n10\n20\n30\n40\n# Retrieved Passages\n45\n50\n55\n60\n65\nTriviaQA, EM\nFigure 7: Results of RAVEN 11B with different numbers of retrieved passages.\nGiven that many questions in the MMLU benchmark are quite lengthy, concatenating in-context\nexamples (questions and candidate answers) with the target question in a few-shot setting is likely to\nexceed the maximum input length. To mitigate this, we only sample examples with question lengths\nof fewer than 50 tokens to use as in-context examples.\nB\nAdditional Results\nB.1\nEffect of number of retrieved passages (RAVEN)\nFigure 7 illustrates the effect of the number of retrieved passages on the performance of RAVEN 11B.\nThe model performance improves with an increased number of retrieved passages in both 0-shot and\n5-shot settings, with the 5-shot performance consistently outperforming 0-shot. These observations\nalign with the ones presented in \u00a73.3.4.\nB.2\nLong-Form Question Answering\nTable 8 presents some example outputs of ATLAS and RAVEN 11B on long-form question answering.\nThe questions are sampled from the ELI5 dataset (Fan et al., 2019). An examination of these results\nreveals that ATLAS typically generates concise answers, while the output from RAVEN generally\nencompasses more information. This is a predictable outcome given that ATLAS is pretrained solely\nwith masked language modeling, where each masked span usually contains only a handful of tokens.\nBesides, while RAVEN\u2019s answers are not always entirely accurate, they generally exhibit higher\nquality compared to ATLAS. Furthermore, the use of Fusion-in-Context Learning in RAVEN appears\nto contribute to a more coherent and informative generation.\n16\nTable 8: Example outputs of ATLAS and RAVEN 11B on long-form question answering.\nQuestion\nHow does a painting like \"When Will You Marry\" sell for $300 million\ndollars when it doesn\u2019t look like anything spectacular\nHuman\nWhen it comes to a lot of art, context is more important than the actual\nmaterial. Rarity, historical context etc. make a thing more valuable than its\nactual aesthetic properties. For example, some shitty buggy SNES or N64\ngames will sell for huge amounts if they are well preserved and in the original\npackaging etc, even when any high school student nowadays could write a\nbetter game in a week.\nATLAS\n1-shot\nIt\u2019s a masterpiece.\nRAVEN\n1-shot\nIt\u2019s not the painting that sells for $300 million dollars, it\u2019s the story behind the\npainting.\nRAVEN\nFiCL [40-1]\nIt\u2019s not the painting that sells for $300 million dollars, it\u2019s the reputation of\nthe artist.\nQuestion\nWhen the society goes fully automatized, what will be the \"normal\"\npeople needed for?\nHuman\nCall centers... I am being totally serious. If your Sony TV is being completely\nfabricated by machines, shipped by machines, sold on amazon and delivered\nby machines, one of the few value adds companies will be able to deliver is\nGOOD customer service. So calling that 1-800 number and being instantly\nconnected to a knowledgeable, friendly, human being who is going to patiently\nhelp you fix whatever issue you have (FYI... you forgot to plug it in).\nATLAS\n1-shot\n\u2019> ELI5: What will the society go fully automatized? >\nRAVEN\n1-shot\nThe ones who will program the robots.\nRAVEN\nFiCL [40-1]\nThe \"normal\" people will be needed to program the robots.\nQuestion\nWhy is a 2 liter of Coca-cola $1, a 12 oz bottle $1.49 and the same size\nbottle of water $1.99?\nHuman\nBecause people will pay. Plain and simple. Capitalism at its best.\nATLAS\n1-shot\nThe price of the product is based on the cost of the packaging.\nRAVEN\n1-shot\nThe cost of packaging is proportional to the size of the container.\nRAVEN\nFiCL [40-1]\nThe cost of packaging is the largest portion of the cost of a product. The larger\nthe container, the less packaging is required.\nQuestion\nIf Matter Cannot Be Created or Destroyed... How do trees form?\nHuman\nSay you have a box of Legos with 500 pieces. With those 500 pieces, you\nbuild a bulldozer. Then one day, you decide to rearrange those pieces from a\nbulldozer to a monster truck. It\u00b4s still the same 500 pieces, just in a different\nlayout. This is how trees are formed. The matter is just rearranged from other\nobjects. Soil, nutrients, water, and even air. This is called \"Conservation of\nMass.\" EDIT: added air. Thanks. :)\nATLAS\n1-shot\nTrees are made of carbon and oxygen.\nRAVEN\n1-shot\nMatter can be created or destroyed by combining two or more different types\nof matter.\nRAVEN\nFiCL [40-1]\nThe tree is a living organism. It is made up of matter. The tree is made up of\ncarbon, hydrogen, oxygen, and water.\nQuestion\nIf the flu shot contains a non-active version of the flu, then why are \"flu\nlike symptoms\" common for the days following?\nHuman\nthe non-active proteins trigger an immune system response which can involve\nsome inflammatory system responses. once the immune system is activated\nfor that infection your body will react more readily if the actual infection gets\ninto you.\nATLAS\n1-shot\nThe flu shot contains live virus particles.\nRAVEN\n1-shot\nThe flu shot contains a live attenuated virus. This means that it is not able to\ninfect the body, but it can still cause some side effects.\nRAVEN\nFiCL [40-1]\nThe flu shot contains a weakened version of the virus. This weakened virus is\nable to trigger an immune response in the body. This immune response can\ncause flu like symptoms.\n17\n"
  },
  {
    "title": "Link-Context Learning for Multimodal LLMs",
    "link": "https://arxiv.org/pdf/2308.07891.pdf",
    "upvote": "14",
    "text": "Link-Context Learning for Multimodal LLMs\nYan Tai1, 2\u2217, Weichen Fan1\u2217\u2020, Zhao Zhang1, Feng Zhu1, Rui Zhao1, Ziwei Liu3\n1SenseTime Research\n2Institute of Automation, CAS\n3S-Lab, Nanyang Technological University\nHere are two images: The first one \nis a RockFlock, and the second \none is a Sheep. Could you tell me \nwhat animal is in these two \npictures?\nThe first image is a sheep wearing \nsunglasses, while the second one \nis a sheep.\nThe first image is a RockFlock, \nwhile the second one is a Sheep.\nFigure 1: The demo dialogue of our proposed link-context learning. After presenting the model with a pair of unseen images\nand novel concepts, our improved model gains the ability to learn and retain the acquired knowledge throughout the conversation\nwhile the vanilla MLLMs fail to provide accurate answers.\nAbstract\nThe ability to learn from context with novel concepts, and\ndeliver appropriate responses are essential in human conver-\nsations. Despite current Multimodal Large Language Models\n(MLLMs) and Large Language Models (LLMs) being trained\non mega-scale datasets, recognizing unseen images or under-\nstanding novel concepts in a training-free manner remains a\nchallenge. In-Context Learning (ICL) explores training-free\nfew-shot learning, where models are encouraged to \u201clearn to\nlearn\" from limited tasks and generalize to unseen tasks. In\nthis work, we propose link-context learning (LCL), which\nemphasizes \"reasoning from cause and effect\" to augment\nthe learning capabilities of MLLMs. LCL goes beyond tra-\nditional ICL by explicitly strengthening the causal relation-\nship between the support set and the query set. By providing\ndemonstrations with causal links, LCL guides the model to\ndiscern not only the analogy but also the underlying causal\nassociations between data points, which empowers MLLMs to\nrecognize unseen images and understand novel concepts more\neffectively. To facilitate the evaluation of this novel approach,\n*Equal Technical Contribution.\n\u2020Project Lead.\nwe introduce the ISEKAI dataset, comprising exclusively of\nunseen generated image-label pairs designed for link-context\nlearning. Extensive experiments show that our LCL-MLLM\nexhibits strong link-context learning capabilities to novel con-\ncepts over vanilla MLLMs. Code and data will be released at\nhttps://github.com/isekai-portal/Link-Context-Learning.\n1\nIntroduction\n(In the near future, mankind finally be able to travel\ninterstellar and come to the centaur constellation.)\nHuman and MLLM walk off the spaceship.\nHuman:\u201cWe made it! Look! The locals are here.\u201d\nLocals: Greetings, you can call us \u2019RockFlock\u2019.\nMLLM: \u201cHi, sheep!\u201d\nHuman: \u201c\n\u201d\nThe above conversation between humans and MLLMs\nserves as a humorous representation of how MLLMs struggle\nto learn from demonstration during the conversation for real.\n\u2019RockFlock\u2019 is our hand-made species, which possesses both\narXiv:2308.07891v1  [cs.CV]  15 Aug 2023\nDemonstration\nInference\nQ: What is this ?\nA: Cat.\nQ: What is this ?\nA: Panda.\nQ: What is this ?\nA: Tiger.\nIn-context Learning\nDemonstration\nQ: What is this ?\nA: Okapi.\nQ: What is this ?\nA: Panda, not okapi.\nQ: What is this ?\nA: Okapi.\nLink-context Learning\nInference\nRelevant\nIrrelevant\nFigure 2: The difference between our link-context learn-\ning with in-context learning. In-context learning involves\nproviding irrelevant tasks for demonstration, whereas there\nis a direct causal relationship between the demonstration and\ninference phases of link-context learning.\na human-like body and a sheep-like head, as shown in Figure\n1. Current MLLMs fail to link the unseen image-label pairs to\nrecognize novel objects in a single conversation. To address\nthis limitation, equipping the model with few-shot learning\nability has been a long-standing topic in computer vision even\nbefore the era of MLLMs. This approach enables the model to\nlearn from limited examples and mitigate the issue effectively.\nThe primary method for MLLMs to learn from demonstra-\ntions is known as in-context learning, wherein the models\nshow remarkable improvement on downstream tasks after\nbeing exposed to a few input-label pairs. However, current\nMLLMs have very limited benefits from in-context learn-\ning, since the emphasis is primarily on guiding the model\nto acquire the ability to process novel tasks after \u201clearning\u201d\nfrom meta tasks. However, the model\u2019s performance is not\naffected even if the answers provided in the meta-tasks are all\nwrong. [1] Thus, what MLLMs have \u201clearned\u201d from demon-\nstration remains on answering questions in a specific format\nrather than understanding the causal relationship between the\nimage-label pairs. To enable MLLMs to concentrate more\non the causal relationship between the image and label pairs,\nFrozen method [2] binds different labels to known images.\nHowever, a significant challenge arises when MLLMs en-\ncounter entirely novel scenarios where both the image and\nthe label are unseen. In such instances, the task of extracting\nthe underlying cause and effect from the demonstration and\nmaking accurate predictions based on this newfound knowl-\nedge remains an unsolved puzzle. The \u2019RockFlock\u2019 (unseen\nimages and novel concepts), shown in Figure 1, would be\nmisrecognized by the previous methods, while our model\nlearns the concept of \u2019RockFlock\u2019 from the demonstration\nand makes responses accurately. Moreover, the acquisition\nof novel concepts does not impede the existing knowledge,\nenabling the model to effectively distinguish between the\noriginal and newly learned images.\nInspired by in-context learning (hereinafter called ICL),\nwe propose link-context learning (hereinafter called LCL),\nwhich requires the MLLMs to acquire knowledge about\nnew concepts from the conversation and retain their existing\nknowledge for accurate question-answering. As shown in\nFigure 2, current in-context learning in MLLMs emphasizes\nbenefiting from the causal-irrelevant demonstration. How-\nwingeda\nstripehopper\nhornedephant\nthornbrawlers\naquarover\nbrawnger\nmushroomhaven\nshellfortress\nlavasnake\npineapplepalace\nrockflock\ngildedfloatship\ncactihog\nairstone\nblazefrost\nskyquill\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nOur Method (7B)\nOpenFlamingo (9B)\nOtter (9B)\nFigure 3: Overview of results on several categories of\nISEKAI dataset: Our model outperforms OpenFlamingo\n(9B) [6] and Otter (9B) [5] across almost all the categories,\nshowcasing superior performance in scenarios involving en-\ntirely unseen images.\never, for link-context learning, the demonstration and the\nfinal task are linked causally. (e.g. If the \u2019apple\u2019 is renamed\nas \u2019orange\u2019 in the demonstration, the model should call ap-\nple an \u2019orange\u2019 during the inference.) With this ability, the\nMLLMs could support few-shot learning in a flexible way.\nIn the era of Large Language Models, evaluating models\u2019\nperformance on few-shot learning becomes a challenge, as\nthese models are extensively trained on vast amounts of real-\nlife data. To address this issue and provide a comprehensive\nassessment of link-context learning, we introduce the ISEKAI\ndataset. This dataset comprises unseen images and concepts,\nentirely novel to MLLMs, as they transcend the boundaries\nof realism. All the images in the dataset are generated by\nStable Diffusion [3] and Midjourney [4], while all the labels\nor concepts are fabricated as well. Figure 3 shows the com-\nparisons between our model and Otter [5], OpenFlamingo [6]\non ISEKAI dataset.\nIn this paper, we present link-context learning (LCL), a\nsetting that bestows MLLMs with the capability to under-\nstand the potential causal relationship in the conversation\nand process unseen images and concepts. Unlike ICL mainly\nfocuses on inspiring models with a wide variety of different\ntasks, LCL goes a step further by empowering the model to\nestablish a mapping between the source and target, thereby\nenhancing its overall performance. The contributions of this\nwork can be summarized as follows:\n\u2022 Link-Context Learning: We introduce a novel causal-\nrelevant few-shot learning setting, where MLLMs are\nchallenged to assimilate new concepts from the ongo-\ning conversation and retain this knowledge for accurate\nquestion-answering. Under link-context learning, we em-\npower the MLLMs to grasp the causal relationship be-\ntween the source and target from the demonstration.\n\u2022 ISEKAI Dataset: Since most real-world data is not com-\npletely unseen to MLLMs, we release a challenging fab-\nricated dataset to the public, where novel image-concept\npairs are introduced, for evaluation of MLLMs\u2019 perfor-\nmance.\n2\nRelated Works\nMultimodal Large Language Models [7\u201311] have demon-\nstrated significant capabilities in universal generation or\nrecognition tasks. Following the new paradigm of MLLMs,\nvarious visual tasks can be achieved in a training-free zero-\nshot manner [12, 13], escaping from the heavy pretrain-\nand-finetune process. However, recognize arbitrary content\nthrough a single model is generally considered extremely dif-\nficult. How to enhancing recognition capability of MLLMs\nin the wild at a low cost has emerged as a recent research\nfocus.\nMultimodal Prompt Tuning\nMultimodal Prompt Tuning\n(M-PT) is commonly used in contrastive learning-based mul-\ntimodal large models, such as CLIP [12]. In the training\nprocess, prompt tuning usually freezes most of the model\u2019s\nparameters and only updates a small number of parameters\nto achieve results similar to fine-tuning [14\u201317]. PT [14] add\ntunable prompt embeddings to each layer of the encoder and\ndecoder, only the weights of the added embeddings will be\nupdated during training. VPT [18] added a set of learnable\nparameters in specific positions to tune the model. CoOp [15]\nand UPT [19] used CLIP as the backbone and prompted\nit to fit few-shot settings. CoCoOp [16], POMP [20] and\nMaPLe [21] extend prompt tuning to open-vocabulary visual\nrecognition tasks. However, traditional prompt tuning meth-\nods are not suitable for the powerful generative multimodal\nlarge language models.\nMultimodal Instruction Tuning\nMultimodal Instruction\nTuning (M-IT) enhances the zero-shot capability of MLLMs\nin unseen tasks by fine-tuning them on an instruction\ndescriptions-based dataset [7,8,11,22,23]. MiniGPT-4 [24]\nand LLaVA [11] keep the visual encoder frozen and tune the\nlanguage model, extending instruction tuning to multimodal-\nity. mPLUG-Owl [25] tuned visual and text encoder sepa-\nrately in two stages, and proposed an evaluation dataset for\nassessing vision-related instruction tuning. InstructBLIP [26]\nenhances zero-shot capability by performing instruction tun-\ning on multiple datasets. Shikra [27] and Kosmos-2 [28] ex-\npanded MLLMs to visual grounding tasks using instructions\nwith bounding box coordinates. Even though these studies\ndemonstrate outstanding zero-shot capability, they still can-\nnot recognize classes that were not seen during the model\ntraining process.\nMultimodal In-Context Learning\nLarge Language Mod-\nels (LLMs) have shown outstanding capability in learning\nfrom context samples. In the Multimodal In-Context Learn-\ning (M-ICL) settings, following the input image samples and\noptional instruction, MLLMs can learn new task patterns in\na few-shot manner [29\u201332]. Flamingo [33] takes in-context\nlearning into consideration during the pretraining process, al-\nlowing the model to possess the ability to support in-context\nlearning. Otter [5] follows Flamingo and proposed a new in-\ncontext learning dataset, proceeding with the ICL capability\nin the instruction tuning stage.\nDifferent from previous methods, our proposed link-\ncontext learning can establish a causal link between the sup-\nport and query set. Specifically, using few-shot class-specific\nimages and textual prompts, LCL can link the prompt and\ninference samples, and even associate previously unseen im-\nages with new concepts.\n3\nLink-Context Learning\nIn this section, we first give a brief introduction to in-context\nlearning and unveil its main restrictions and difference to our\nlink-context learning in Preliminary; next, we bring the power\nof link-context learning into MLLMs in Bring Link-Context\nLearning to MLLMs.\n3.1\nPreliminary\nIn-Context Learning Formally, in-context learning [34]\nrefers to: the model should choose the answer with the\nhighest prediction score from a set candidate answers\nY\n= {y1, y2, ..., yn}, given a query input x, condition-\ning on a support set S, which consists of multiple input-\nlabel pairs from a wide variety of tasks, where S\n=\n{(x1, y1), (x2, y2), ..., (xn, yn)}. (The query and the sample\nof S should belong to different tasks.)\nFrom another perspective, in-context learning could be\ndenoted as training-free few-shot learning, as it transforms\nthe training stage of few-shot learning into the demonstration\ninput for Large Language Models. Noted that the ICL [34]\nis consistent with FSL, where the tasks in the demonstra-\ntion (training) stage and in the inference (query) stage are\ndifferent.\nLink-Context Learning Essentially, link-context learning\n(LCL) represents a form of training-free and causal-linked\nfew-shot learning. In this approach, a support set S =\n(x1, y1), (x2, y2), ..., (xn, yn) is provided, along with a query\nsample x from the query set Q, where the data pairs from the\nsupport set are causally linked to the query set. The model is\ntasked with predicting the answer based on the causal-linked\nrelationship between the query and support set.\nTo provide further clarity, link-context learning signifi-\ncantly strengthens the causal relationship between the sup-\nport set and the query set. For instance: 1). New arithmetic\nrules: In this scenario, the support set consists of arith-\nmetic expressions such as (1 <op> 2 = 3), (2 <op> 3 = 5),\nwith the query sample being 4 <op> 5\n=?. Here,\n\"<op>\" represents a new arithmetic rule that we aim\nto\nteach\nthe\nmodel\nthrough\nthe\ndemonstration;\n2).\nNovel image classification: In this case, the support\nset contains pairs like (<unseen image> : <novel cls A>),\n(<unseen image> : <novel cls B>), while the query sample is\n(<unseen image> belongs to?). This example demonstrates\nhow we expect the model to correctly classify the unseen\nimage into one of the specified novel classes based on the\ndemonstration.\nIn essence, link-context learning enhances the model\u2019s\ncapacity to grasp new concepts and relationships by effec-\ntively establishing a causal link between the support set and\nthe query set. While this setting is applicable to both LLMs\nand MLLMs, our primary focus in this paper is on the appli-\ncation of link-context learning specifically in MLLMs. By\nconcentrating on MLLMs, we aim to showcase the potential\nof this approach in multimodal models and its implications\nfor advancing their learning capabilities.\n3.2\nBring Link-Context Learning to MLLMs\nIn this section, our main objective is to introduce Link-\nContext Learning (LCL) to the realm of MLLMs. Recog-\nnizing that the current MLLMs trained in the ICL manner\nmay not excel in LCL tasks, we propose a novel training strat-\negy to fine-tune MLLMs. This approach aims to equip the\nmodels with the capability to grasp causal links from context\neffectively. By leveraging this novel training strategy, we aim\nto empower MLLMs to excel in tasks that require reasoning\nand understanding causal relationships, thereby broadening\ntheir range of capabilities and improving their overall perfor-\nmance. To be more specific, we choose Shikra [27] as our\nbaseline, and we divide ImageNet1k into ImageNet-900 and\nImageNet-100 by classes, which would be discussed in detail\nin Training Dataset. Additionally, we incorporate the concept\nof contrast learning in our training strategy, as discussed in\nTraining Strategy. This helps guide the model to understand\nthe shared characteristics among samples of the same kind\nand the distinctions between samples of different kinds.\n3.2.1\nTraining Dataset\nUnlike traditional tasks that require extensive training data,\nLCL concentrates on acquiring the ability to find the link\nbetween the source-target pairs in demonstration and gener-\nalize to the query samples. Thus, adequate representation of\ndiverse image categories is essential to enable MLLMs to\ngrasp causal relationships effectively and efficiently.\nImageNet1k [35] is commonly employed for image clas-\nsification tasks, and it is customary to train models on the\nentire dataset to enhance their recognition ability across all\ncategories. In contrast, within the training configuration of\nLCL, we only select a limited number of samples randomly\nfrom each category. Then we arrange a set of related cate-\ngories with decreasing similarity for each category, referred\nto as \"neighbors\". Specifically, we adopted CLIP [12] to\ncalculate the similarity between different classes within the\ntraining dataset. Firstly, we randomly select 100 images from\neach class and calculate the average image feature for each\nclass. Subsequently, we encode the text names of all classes\nto obtain their corresponding feature vectors. Ultimately, we\ncompute weighted similarities across distinct class pairs, en-\ncompassing image-to-image, image-to-text, and text-to-text\ncorrelations. For a specific category, we sort all other cate-\ngories based on similarity and divide them into N intervals.\nThen, within each interval, we randomly select categories to\nconstruct a set of \"neighbors\" with a total quantity of N.\n3.2.2\nTraining Strategy\nIn order to make MLLMs understand the causal link be-\ntween the support set and query sample, as well as the causal\nrelationship between the input-label pairs in the support\nset, we build positive-negative pairs to urge the model to\nlearn from comparisons. Let the support set be denoted as\nS = {s1, s2, ..., sn}. Based on the correlation among its sam-\nples, we can redefine the support set as C = {c1, c2, ..., cm},\nwhere each cm serves as a prototype representing a cluster\nof samples from S. These prototypes capture the essential\nrelationships and similarities among samples within S. Given\nthe query x, we train \u03b8 to maximize the likelihood:\nlog p\u03b8(y|x) =\nX\nl\nlog p\u03b8(yl|x, C, y1, y2, ..., yl\u22121),\n(1)\nwhere \u03b8 denotes the parameters of the language model. The\nparameters of the visual encoder are frozen during the train-\ning.\n[2-way] strategy: In this strategy, we train the MLLMs\nfor binary image classification, where the C = {c1, c2}.\nTo be more specific, c1 and c2 here represent the proto-\ntype of two classes. We denote the training class set as\nT = {t1, t2, ..., t100}, we randomly sample a class ti as\nthe positive class, where its neighbor class set N ti\n=\n{nti\n1 , nti\n2 , ..., nti\n100} (nti\n1 is the most similar class to ti, while\nthe nti\n100 is the least). Then we apply a hard-negative mining\nstrategy, where we sample the negative class nti\nj from N ti\nwith a probability pj =\n101\u2212j\nP100\nm=1 m. Noted that this setting is\nfixed to train on 16 shots.\n[2-way-random] strategy: In this strategy, we first train the\nMLLMs on fixed-16 shots following the [2-way] strategy,\nthen further train the model with shots averaged sampled\nfrom 2-16 shots for 10 epochs.\n[2-way-weight] strategy: Within this strategy, we initially\ntrain the MLLMs using a fixed-16 shot regimen, adhering\nto the [2-way] approach. Subsequently, we refine the model\nby additional training with shots sampled from the range of\n2-16, with each shot\u2019s probability denoted as pj =\nej\nP16\nm=2 em .\n[mix] strategy: To enhance the model\u2019s generalizability, we\nundertake a fine-tuning process that involves both [2-way]\ntasks and Shikra\u2019s [27] original tasks. During each iteration,\nthe training samples are evenly sampled from both the [2-\nway] tasks and the original tasks. This balanced approach\nensures that the model gains proficiency in both the newly\nintroduced link-context learning tasks and the pre-existing\ntasks from Shikra [27].\n4\nISEKAI Dataset\nTo objectively evaluate MLLM\u2019s ability to learn new con-\ncepts through LCL, we created an ISEKAI dataset, shown\nin Figure 4. The concepts involved are unreal, rarely seen in\nlegends, myths, or fictional media. Thus, MLLM\u2019s exposure\nto these concepts is minimal. The term \"Isekai\" originates\nISEKAI World\nReal World\nShellfortress\nPineapplePalace\nOctoVac\nCrystalStag\nAirStone\nCactihog\nFigure 4: Overview of the ISEKAI Dataset: This dataset comprises entirely generated images, where the images from \u201cISEKAI\nWorld\u201d are non-existent in real life, while the images from \u201cReal World\u201d are sourced from reality.\nfrom a fantasy subgenre in anime. Plots usually involve char-\nacters transported to a different world, like a fantasy realm or\nvirtual universe. Audiences understand the new world gradu-\nally through the protagonist\u2019s exploration, akin to MLLM\u2019s\njourney into a new realm of knowledge.\nThe dataset\u2019s images are generated by Midjourney\u2019s [4]\ntext-to-image model using well-crafted instructions. Images\nwere manually selected to ensure core concept consistency.\nThe dataset currently comprises 20 groups, and 40 categories\nin total (continues to grow). Each group pairs a new concept\nwith a related real-world concept, like \"octopus vacuum\" and\n\"octopus.\" These can serve as challenging negative samples\nfor each other. Each concept has no less than 32 images, sup-\nporting multi-shot examples. These features enable ISEKAI\nto comprehensively assess the model\u2019s LCL capability. We\nalso provide text descriptions of each concept\u2019s appearance\nand name, contributing to evaluations beyond LCL.\nIn this paper, we evaluated different models\u2019 performance\non ISEKAI. For details, refer to Results on ISEKAI.\n5\nExperiments\nIn this section, we present the results of our experiments\nto showcase the effectiveness of our proposed method. We\nconduct comprehensive comparisons between our approach\n(link-context learning-based) and other in-context learning-\nbased MLLMs.\n5.1\nResults on ISEKAI\nTo quantitatively evaluate the performance of link-context\nlearning, we compare our methods in different strategies with\nour baseline (Shikra [27]) as well as ICL methods (Otter and\nOpenFlamingo) in two challenge datasets: ISEKAI-10 and\nISEKAI-pair.\nISEKAI-10 Evaluation: Comprising 10 classes of challeng-\ning positive-negative image pairs, ISEKAI-10 presents a\nscenario where the positive class is entirely nonexistent in\nthe real world yet shares certain characteristics with the nega-\ntive class, which comprises common animals or objects from\nour reality. The upper section of Table 1 showcases the out-\ncomes on the ISEKAI-10 dataset, where vanilla-shikra [27]\nencountered difficulty. Our model demonstrates competitive\nperformance compared with OpenFlamingo [6] and Otter [5]\nacross all shot numbers.\nISEKAI-pair Evaluation: In the ISEKAI-pair evaluation,\npositive and negative pairs are constructed using all image\nSetting\nMethod\n2-shot\n4-shot\n6-shot\n8-shot\n10-shot\n12-shot\n14-shot\n16-shot\nISEKAI-10\nOpenFlamingo [6]\n0.46\n0.44\n0.46\n0.48\n0.50\n0.50\n0.48\n0.46\nOtter [5]\n0.23\n0.23\n0.19\n0.15\n0.14\n0.12\n0.10\n0.07\nVanilla-Shikra [27]\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nOurs-[2-way-random]\n0.64\n0.63\n0.65\n0.62\n0.61\n0.57\n0.56\n0.56\nOurs-[mix]\n0.68\n0.70\n0.73\n0.69\n0.63\n0.62\n0.65\n0.62\nISEKAI-pair\nOpenFlamingo [6]\n0.19\n0.34\n0.38\n0.39\n0.41\n0.40\n0.40\n0.40\nOtter [5]\n0.01\n0.04\n0.04\n0.03\n0.03\n0.02\n0.02\n0.01\nVanilla-Shikra [27]\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nOurs-[mix]\n0.39\n0.38\n0.38\n0.40\n0.40\n0.39\n0.37\n0.35\nOurs-[2-way-random]\n0.43\n0.46\n0.47\n0.48\n0.48\n0.49\n0.49\n0.49\nTable 1: Quantitative evaluation on ISEKAI from zero-shot to 16-shot, measured by accuracy. We achieve the best results\ncompared with Otter [5] and OpenFlamingo [6].\nMethod\nzero-shot\n2-shot\n4-shot\n6-shot\n8-shot\n10-shot\n12-shot\n14-shot\n16-shot\nOpenFlamingo [6]\n0.00\n0.41\n0.62\n0.72\n0.75\n0.77\n0.78\n0.73\n0.72\nOtter [5]\n0.13\n0.18\n0.21\n0.24\n0.25\n0.26\n0.24\n0.23\n0.23\nVanilla-Shikra [27]\n0.05\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nOurs-[mix]\n0.16\n0.73\n0.78\n0.83\n0.73\n0.71\n0.72\n0.65\n0.57\nOurs-[2-way]\n0.02\n0.51\n0.61\n0.68\n0.73\n0.77\n0.78\n0.78\n0.79\nOurs-[2-way-random]\n0.0\n0.77\n0.78\n0.77\n0.79\n0.77\n0.77\n0.77\n0.75\nOurs-[2-way-weight]\n0.0\n0.69\n0.71\n0.72\n0.76\n0.77\n0.78\n0.78\n0.79\nTable 2: Quantitative evaluation on ImageNet-100 from zero-shot to 16-shot, measured by accuracy. We achieve the best\nresults compared with Otter [5] and OpenFlamingo [6].\ncategories that do not exist in the real world. Each individual\nimage is paired with all images from other categories, facili-\ntating a comprehensive assessment. This evaluation provides\na realistic gauge of the model\u2019s capability to handle complete\nunknowns through various combinations. The lower section\nof Table 1 underscores our model\u2019s superiority over Open-\nFlamingo [6] and Otter [5] in this context.\nQualitative Results: Figure 1 provides a visual compari-\nson between our model and OpenFlamingo [6], as well as\nOtter [5]. Notably, our model demonstrates its proficiency\nin accurately comprehending novel concepts and effectively\ndiscerning unfamiliar objects from those with close resem-\nblance. This observation underscores our model\u2019s capacity to\ncapture the causal relationship between the source and target\ndomains from the demonstration.\n5.2\nResults on ImageNet-100\nWe proceed to assess our model\u2019s performance on ImageNet-\n100, encompassing 100 classes that were entirely absent from\nthe training phase. The outcomes underscore the efficacy\nof our mix strategy, which attains the highest accuracy of\n83% at 6-shot. In contrast, Otter achieves a peak accuracy\nof 25%, and OpenFlamingo\u2019s performance reaches 78%.\nUnlike the ISEKAI dataset, the images from ImageNet-100\ndo correspond to real-world entities.\n5.3\nAblation Study\nDoes the ground-truth input-label mapping exists?\nWe conduct an ablation analysis on the correctness of labels\nwithin the demonstration (support set). Given a set of im-\nage domains Xc \u2208 RH\u00d7W \u00d73 and label domains C \u2208 RN, a\nmapping f : Xc \u2192 C exists to associate each image with\nits corresponding label. We use several image-label pairs\n{(x1\nc1, c1), (x2\nc1, c1), ..., (xn\nc1, c1)}, where xj\nci \u2208 Xci, as the\nsupport set. The model is going to predict the correct answer\nfrom a candidate set Y :\n\u02c6y = arg max\nyi\u2208Y\nP(yi|x, f),\n(2)\nwhere the prediction is conditioned on the mapping f.\nConsequently, intentionally breaking the mapping rela-\ntionship within the support set would lead the model to\nprovide incorrect answers, as it heavily relies on the accurate\nassociation between the image-label pairs of the support\nset to make precise predictions. As shown in Figure 7, we\ndisturb the mapping f by gradually inserting false labels\ninto the support set, and the accuracy falls from 0.78 to\n0.00 when the correctness of the labels falls from 100% to\n0%. These results clearly show that maintaining accurate\nassociations between image-label pairs within the support\nset plays a crucial role in link-context learning.\nWould the model benefit from using a larger shot?\nUser: What is in the image?\nAnswer: Cactihog.\nAnswer: Hedgehog.\nUser: What is in the image?\nUser: What is in the image?\nUser: What is in the image?\nOurs: Cactihog.\nOurs: Hedgehog.\nOpenFlamingo:  An image of \ncactus.\nOpenFlamingo: An image of \nhedgehog.\nOtter:  This image is of a \nsmall, green, and cute animal.\nOtter: A hedgehog in a field of \nflowers.\nSupport set\nQuery set\nUser: What is in the image?\nAnswer: Mushroomhaven.\nAnswer: Mushroom.\nUser: What is in the image?\nUser: What is in the image?\nUser: What is in the image?\nOurs: Mushroomhaven.\nOurs: Mushroom.\nOpenFlamingo:  An image of \nmushroom house.\nOpenFlamingo: An image of \nmushrooms.\nOtter:  A mushroom with a \nblue roof and a door.\nOtter: A mushroom with water \ndroplets on it.\nFigure 5: Qualitative comparisons of novel images understanding results between ours and OpenFlamingo [6], Otter [5].\nThe name \u201cCactihog\u201d is a fusion of \u201ccactus\u201d and \u201chedgehog\u201d, combining the key features of these two creatures. The name\n\u201cMushroomHaven\u201d suggests a dwelling place characterized by giant mushrooms\nMuch like supervised learning, the model\u2019s accuracy experi-\nences rapid initial growth with an increasing amount of train-\ning data, eventually reaching a plateau. During this phase,\nthe selection of more representative samples becomes crucial.\nFigure 6 presents two outcomes: one depicts model accuracy\nfrom separate training at a fixed shot (gray bar in the figure),\nwhile the other showcases the model\u2019s performance through\nsampling across various shots (red line in the figure). The\nresults reveal slight gains from lower fixed-shot training and\nconsistent performance from random-shot training. Notably,\nin both random and fixed settings, accuracy plateaus or expe-\nriences gradual growth after the 8-shot threshold.\nWhat does the model\u2019s decision-making in the case of\nmulti-shot depend on?\nAs shown in Fig 8, when disturbing the label of different\npositions, the accuracy of the model with 16-shot drops dif-\nferently, which reflects the extent to which the model prefers\ndifferent locations. We observe that the model heavily relies\non the beginning and the middle positions. From another\naspect, it provides an explanation of why the model encoun-\nters a plateau in a higher number of shots. Similarly, this\nphenomenon also exists in LLMs [36], where the language\nmodel tends to be \u201clost in the middle\u201d when processing long\ncontexts. They also reveal that the model\u2019s performance keeps\ndecreasing when the contexts grow longer.\nWhat is the difference between different training strate-\ngies?\nTable 2 presents a comprehensive view of the outcomes\nachieved through our four distinct training strategies. The\nmix strategy stands out by elevating the zero-shot accuracy\nfrom 5% to 16% and attaining a remarkable 83% accuracy\nat 6-shot; however, its performance diminishes to 57% at\n16-shot. In contrast, the 2-way strategy, anchored at 16-shot\ntraining, initiates with a 51% accuracy at 2-shot and progres-\nsively ascends to 79% at 16-shot. Interestingly, we observe\nthat the accuracy trend of the 2-way strategy isn\u2019t solely at-\ntributable to an increase in shots, but rather stems from a\ncloser alignment with the trained pattern. To validate this,\nwe introduce two additional settings: 2-way-random and 2-\nway-weight. These settings undergo fixed-shot training for\ninitialization, followed by finetuning across 2-16 shots with\nrandom and weighted approaches, respectively. Both exhibit\n2-shot 4-shot\n8-shot\n12-shot\n16-shot\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\n0.69\n0.71\n0.76\n0.78\n0.79\nSampled\nFixed\nFigure 6: The ablation study on shot\nnumber. The grey bars illustrate the high-\nest accuracy achieved for each shot num-\nber, denoting specific shot-based train-\ning. The red line illustrates the perfor-\nmance of the model trained using a sam-\npled strategy. Notably, both scenarios ex-\nhibit plateaus in accuracy after reaching\nthe 8-shot mark.\n0%\n25%\n50%\n75%\n100%\nFalse Rate\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\n0.38\n0.2\n0.78\n0.64\n0.48\n0.35\n0.0\nOurs\nOpenFlamingo\nOtter\nFigure 7: The ablation study on false\nrate. In contrast to OpenFlamingo [6],\nwhich sustains a 38% accuracy at a 100%\nfalse rate, our model attains 0% accuracy\nunder the same conditions. This outcome\nunderscores our model\u2019s ability to pre-\nserve precise linkages between the sup-\nport set and the query.\n1st\n4th\n8th\n12th\n16th\nModified Position\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAccuracy\n0.63\n0.69\n0.63\n0.68\n0.7\nOrigin\nOurs\nFigure 8: The effect of label modifica-\ntions at distinct positions. The dashed\nblue line serves as a reference for the\noriginal accuracy, while the red line por-\ntrays the accuracy of our model subse-\nquent to the label modified at specific\npositions. Significant accuracy drop re-\nflects position dependency, while minor\nchange indicates position insignificance\nin the model\u2019s decision-making.\nconsiderable accuracy improvements in lower shots. Notably,\nwhile the accuracy of higher shots, finetuned with a random\nstrategy, drops\u2014an observation mirroring the behavior of the\nmix strategy. These results underscore the efficacy of an even,\nsustained, and generalized training approach in harnessing\nthe potential of large language models, revealing the emer-\ngence of a \"lost-in-the-middle\" phenomenon, in coherence\nwith our earlier observations.\nDoes the training harm the zero-shot performance?\nTable 3 shows the comparison between our-7B model with\nshikra-13B [27] and some previous SOTA methods on\nImagenet-100 and VQAv2. From the results, we conclude\nthat our mix training strategy would not harm the model\u2019s\nzero-shot performance.\n6\nDiscussion\n6.1\nLimitations\nWe believe that this work introduces a challenging and\npromising setting for both MLLMs and LLMs. However,\nthe primary focus in this paper lies on link-context learn-\ning within the context of MLLMs, specifically validating\nthe basic tasks such as image classification. Consequently,\nthis work should be regarded as a foundational baseline for\nexploring the potential of link-context learning.\nLooking ahead, future research directions encompass a\ndeeper theoretical analysis that delves into the intricacies of\nthe causal relationship between the support samples and, cru-\ncially, between the support set and the query. Understanding\nand unraveling the complexities of these causal links repre-\nsent meaningful avenues of inquiry that could lead to signifi-\ncant advancements in the capabilities of models in reasoning,\nlearning, and adapting to novel scenarios. As the field pro-\ngresses, we anticipate further investigations and refinements\nMethod\nImageNet-100\nVQAv2dev\nVQAv2std\nOpenFlamingo [6]\n0.00\n-\n-\nFlamingo-80B [33]\n-\n56.3\n-\nFlamingo-9B [33]\n-\n51.8\n-\nBLIP2 [9]\n-\n65.0\n-\nOtter [5]\n0.13\n-\n-\nShikra-13B [27]\n0.05\n77.3\n77.5\nOurs-7B-[mix]\n0.16\n75.1\n75.3\nTable 3: Quantitative evaluation was conducted on both\nImageNet-100 and VQAv2 datasets employing a zero-shot\napproach. The outcomes substantiate that our training strat-\negy exhibits no detrimental impact on the zero-shot perfor-\nmance.\nthat will not only enrich our understanding of link-context\nlearning but also implement in-context learning for MLLMs\nand LLMs in a unified way.\n6.2\nConclusion\nIn conclusion, this paper introduces a groundbreaking\nparadigm of causal-relevant few-shot learning, significantly\nexpanding the capabilities of Multimodal Large Language\nModels (MLLMs) within the context of single conversations.\nThrough meticulous experimentation and a carefully devised\ntraining strategy, we demonstrate that MLLMs can adeptly\nestablish a mapping between ground-truth input-label pairs,\nthereby acquiring the proficiency to seamlessly generalize\nthis capacity to previously unencountered images and novel\nconcepts. This pivotal advancement propels MLLMs into un-\ncharted territories, enabling them to not only acquire but also\napply knowledge in a manner more akin to human cognition.\nReferences\n[1] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike\nLewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethink-\ning the role of demonstrations: What makes in-context learning\nwork? arXiv preprint arXiv:2202.12837, 2022.\n[2] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill. Multimodal few-shot\nlearning with frozen language models. Advances in Neural\nInformation Processing Systems, 34:200\u2013212, 2021.\n[3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj\u00f6rn Ommer. High-resolution image synthesis\nwith latent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n10684\u201310695, 2022.\n[4] Midjourney. Midjourney. https://www.midjourney.com, 2023.\n[5] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. CoRR, abs/2305.03726,\n2023.\n[6] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh,\nGabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.\nOpenflamingo, March 2023.\n[7] OpenAI. Gpt-4 technical report, 2023.\n[8] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:\nBootstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In International Con-\nference on Machine Learning, pages 12888\u201312900. PMLR,\n2022.\n[9] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\n2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023.\n[10] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and\nJianfeng Gao. Instruction tuning with gpt-4. arXiv preprint\narXiv:2304.03277, 2023.\n[11] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023.\n[12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International\nconference on machine learning, pages 8748\u20138763. PMLR,\n2021.\n[13] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei\nYang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei\nZhang, Jenq-Neng Hwang, et al. Grounded language-image\npre-training. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10965\u2013\n10975, 2022.\n[14] Hao Yang, Junyang Lin, An Yang, Peng Wang, Chang Zhou,\nand Hongxia Yang. Prompt tuning for generative multimodal\npretrained models. arXiv preprint arXiv:2208.02532, 2022.\n[15] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to prompt for vision-language models. In-\nternational Journal of Computer Vision, 130(9):2337\u20132348,\n2022.\n[16] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Conditional prompt learning for vision-language models.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 16816\u201316825, 2022.\n[17] Xuejing Liu, Wei Tang, Jinghui Lu, Rui Zhao, Zhaojun Guo,\nand Fei Tan. Deeply coupled cross-modal prompt learning.\narXiv preprint arXiv:2305.17903, 2023.\n[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual\nprompt tuning. In European Conference on Computer Vision,\npages 709\u2013727. Springer, 2022.\n[19] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and\nChen Change Loy. Unified vision and language prompt learn-\ning, 2022.\n[20] Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai\nZheng, Mu Li, Alex Smola, and Xu Sun.\nPrompt pre-\ntraining with twenty-thousand classes for open-vocabulary\nvisual recognition, 2023.\n[21] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad\nMaaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-\nmodal prompt learning, 2023.\n[22] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and\nQuoc V Le. Finetuned language models are zero-shot learn-\ners. In International Conference on Learning Representations,\n2022.\n[23] OpenAI.\nIntroducing ChatGPT.\nhttps://openai.com/blog/\nchatgpt, 2023.\n[24] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language un-\nderstanding with advanced large language models, 2023.\n[25] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng\nTian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modular-\nization empowers large language models with multimodality,\n2023.\n[26] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung,\nand Steven Hoi. Instructblip: Towards general-purpose vision-\nlanguage models with instruction tuning, 2023.\n[27] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng\nZhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s\nreferential dialogue magic, 2023.\n[28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nmultimodal large language models to the world, 2023.\n[29] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu,\nBaobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui.\nA survey on in-context learning, 2023.\n[30] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan\nAzarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng,\nand Lijuan Wang. Mm-react: Prompting chatgpt for multi-\nmodal reasoning and action, 2023.\n[31] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei\nChang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.\nChameleon: Plug-and-play compositional reasoning with large\nlanguage models, 2023.\n[32] Tanmay Gupta and Aniruddha Kembhavi. Visual program-\nming: Compositional visual reasoning without training, 2022.\n[33] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, Roman Ring, Eliza\nRutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina\nSamangooei, Marianne Monteiro, Jacob L Menick, Sebas-\ntian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Shar-\nifzadeh, Miko\u0142 aj Bi\u00b4nkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Kar\u00e9n Simonyan. Flamingo: a visual\nlanguage model for few-shot learning. In S. Koyejo, S. Mo-\nhamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, edi-\ntors, Advances in Neural Information Processing Systems, vol-\nume 35, pages 23716\u201323736. Curran Associates, Inc., 2022.\n[34] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020.\n[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\nlenge.\nInternational Journal of Computer Vision (IJCV),\n115(3):211\u2013252, 2015.\n[36] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,\nMichele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in\nthe middle: How language models use long contexts. arXiv\npreprint arXiv:2307.03172, 2023.\n"
  },
  {
    "title": "Relightable and Animatable Neural Avatar from Sparse-View Video",
    "link": "https://arxiv.org/pdf/2308.07903.pdf",
    "upvote": "9",
    "text": "Relightable and Animatable Neural Avatar from Sparse-View Video\nZhen Xu 1\nSida Peng 1\nChen Geng 1\nLinzhan Mou 1\nZihan Yan 2\nJiaming Sun 3\nHujun Bao 1\nXiaowei Zhou 1\n1Zhejiang University\n2MIT Media Lab\n3Image Derivative Inc.\nTime\nSparse-View (or Monocular) Video\nInverse Rendering\nRelightable and Animatable Neural Avatar\nRendering\nAlbedo\nRoughness\nGeometry\nVisibility\nSpecular\nLighting\nNovel Lighting + Novel Pose + Novel View\nSpecular\nVisibility\nFigure 1. Reconstructing relightable and animatable neural avatar from sparse-view (or monocular) video. Our method takes only a\nsparse-view (or monocular) video as input and reconstructs a relightable and animatable neural avatar under unknown illumination, which\ncan then be relit with arbitrary environment lights and animated with arbitrary motion sequences. Note that our method successfully\ncaptures the shininess of the skin and pants as well as the specular highlights on the t-shirt\u2019s plastisol printings.\nAbstract\nThis paper tackles the challenge of creating relightable\nand animatable neural avatars from sparse-view (or even\nmonocular) videos of dynamic humans under unknown il-\nlumination. Compared to studio environments, this setting\nis more practical and accessible but poses an extremely\nchallenging ill-posed problem. Previous neural human re-\nconstruction methods are able to reconstruct animatable\navatars from sparse views using deformed Signed Distance\nFields (SDF) but cannot recover material parameters for re-\nlighting. While differentiable inverse rendering-based meth-\nods have succeeded in material recovery of static objects,\nit is not straightforward to extend them to dynamic humans\nas it is computationally intensive to compute pixel-surface\nintersection and light visibility on deformed SDFs for inverse\nrendering. To solve this challenge, we propose a Hierarchical\nDistance Query (HDQ) algorithm to approximate the world\nspace distances under arbitrary human poses. Specifically,\nwe estimate coarse distances based on a parametric human\nmodel and compute fine distances by exploiting the local\ndeformation invariance of SDF. Based on the HDQ algo-\nrithm, we leverage sphere tracing to efficiently estimate the\nsurface intersection and light visibility. This allows us to de-\nvelop the first system to recover animatable and relightable\nneural avatars from sparse view (or monocular) inputs. Ex-\nperiments demonstrate that our approach is able to produce\nsuperior results compared to state-of-the-art methods. Our\ncode will be released for reproducibility.\n1. Introduction\nRealistic human avatars have a range of applications\n[14, 64] in various domains, e.g., virtual reality, filmmak-\ning, and video games. This work focuses on the specific\nsetting of creating animatable and relightable human avatars\nfrom sparse-view or monocular RGB videos. This problem\nis challenging due to the inherent ambiguity of acquiring\nhuman geometry, materials, and motions from sparse view\nimages [22,53]. Traditional methods [18,21,22,26,37,64,70]\nresolve this ambiguity via customized and costly capture de-\nvices, e.g., light stages with controllable illumination and\ndense camera arrays. However, such devices are restricted to\nprofessional users, impeding their universality and general-\nization.\nRecent neural scene representation-based methods [45,\n52,68] have demonstrated the ability to extract detailed ge-\nometry and photorealistic appearance of human performers\nfrom sparse-view videos without sophisticated studio setup.\nThese methods typically define the human model in canoni-\ncal space and warp it into world space through a deformation\nmodule to represent human performers observed in videos.\n1\narXiv:2308.07903v2  [cs.CV]  17 Aug 2023\nFor example, AniSDF [52] models the human geometry and\nappearance as neural signed distance and radiance fields,\nand deforms them using linear blend skinning (LBS) [41]\nand learned local deformation networks. Albeit showing the\ncapability of novel pose synthesis, the reconstructed avatars\nin these works [45,51,68] are not relightable as they bake the\nshading and shadow into the appearance model. As a result,\nthe shading of the avatars under novel poses is unrealistic\nand the environment illumination cannot be changed, which\nrestricts the applicability of the avatars.\nAnother line of works attempts to create relightable mod-\nels under natural illumination through inverse rendering tech-\nniques [10,62,75,78,79], which estimate surface material pa-\nrameters from input images through differentiable physically-\nbased rendering. Computing the visibility of 3D points to the\nenvironment light is essential for accurate estimation [78,79],\nbut the cost of visibility computation is high. To improve\nefficiency, L-Tracing [16] adopts a signed distance field to\nrepresent the scene geometry and estimates the light visibil-\nity through sphere tracing, which iteratively marches along a\nray using distance values until hitting the surface. Although\nthis strategy works well on static objects, it is not suitable\nfor animatable neural avatars [52, 68, 71], which warp the\ncanonical SDF to world space based on a non-rigid motion\nfield, producing a deformed SDF. The reason is that sphere\ntracing might not converge on the deformed SDF [59] since\nthe SDF is inherently defined in the canonical space, thereby\nyielding incorrect world-space distance.\nIn this work, we propose a novel approach for creating\nrelightable and animatable human avatars from sparse-view\n(or monocular) videos via neural inverse rendering. Inspired\nby previous methods [52,68], we parameterize the human\navatar as MLP networks, which predict material parameters\nand signed distance for any 3D point in canonical space.\nThese values are transformed into world space for rendering\nthrough a neural deformation field. Our innovation lies in\ndesigning a hierarchical query scheme that enables a consis-\ntent approximation of 3D points\u2019 distance to the surface of\nthe neural avatar under arbitrary human poses. This allows\nus to perform sphere tracing for 3D points\u2019 pixel-surface\nintersection and light visibility for physically-based render-\ning. Specifically, we smoothly blend the world-space KNN\n(when query points are far from the surface) distances and\ncanonical-space neural SDF (when query points are close to\nthe surface), approximating an SDF defined on the world-\nspace geometry of the neural avatar. In this way, vanilla\nsphere tracing [30] can be performed on the deformed SDF\nin world space when animating and relighting the avatar,\navoiding the non-linearity of canonical sphere tracing, as\nwell as the pitfalls of world space tracing with incorrect\nworld-space distance.\nBased on the Hierarchical Distance Query algorithm, we\nfurther develop a soft visibility computation scheme by incor-\nporating traditional distance field soft shadow (DFSS) [50]\nonto the deformed SDF, which is essential to the photo-\nrealism of the relightable neural avatar. The soft shadow\nproduced by an area light source typically requires multiple\nlight samples to compute, while DFSS utilizes distance val-\nues to approximate the soft shadow coefficient with only a\nsingle sample. Note that it is not trivial to combine DFSS\nwith previous methods [51,68,71], as they cannot produce\nworld-space distance values from 3D points to the scene\nsurface along an arbitrary direction.\nTo validate our approach, we collect a real-world multi-\nview dataset dubbed MobileStage, which captures the com-\nplex shading and shadow effects of dynamic humans with\nan array of mobile phone cameras. Furthermore, we extend\nthe SyntheticHuman dataset [52] with novel illuminations,\nenabling the evaluation of relightable neural avatars with\nground-truth photometric properties and relighting results.\nExperiments on relighting ability and novel pose synthesis\nshow that our method outperforms the state-of-the-art with\nsuperior visual quality and physical accuracy on both real-\nworld and synthetic datasets. Our code will be made publicly\navailable for reproducibility.\nOur contributions can be summarized as follows: (a) We\npropose a novel system for reconstructing relightable and\nanimatable neural avatars from sparse-view (or monocular)\nvideos. (b) We design a hierarchical distance query algorithm\nfor efficient pixel-surface intersection and light visibility\ncomputation using sphere tracing. (c) We extend DFSS to\ndrivable neural SDF, efficiently producing realistic soft shad-\nows for the neural avatars. (d) We demonstrate quantitative\nand qualitative improvements compared to prior work.\n2. Related work\nHuman avatars.\nTo produce animatable human avatars,\nprevious methods [14, 27, 28, 64, 66, 72] generally adopt a\nthree-stage pipeline: they first reconstruct the human shape\nand appearance, then bind the shape to a skeleton, and fi-\nnally animate the human model through linear blend skin-\nning (LBS) algorithm [41]. Traditional methods tend to\nleverage complicated hardware, such as dense camera ar-\nrays [18,24,37,63,64] or depth sensors [4,9,60,65], to create\nhigh-fidelity human models. Recently, some optimization-\nbased methods [5,35,53,69,71] have attempted to reconstruct\nhuman models given sparse multi-view videos. For exam-\nple, Neural Body [53] represents a dynamic human model\nby combining SMPL model [46] with neural radiance field\n(NeRF) [48]. Its model parameters are learned from images\nthrough differentiable volume rendering.\nTo animate the reconstructed human model, some [5,31]\nretrieve the skinning weights of the SMPL model for per-\nforming the LBS algorithm. Several methods [15,31,51,56]\nopt to optimize personalized skinning weights for the target\nhuman subject, where they represent the skinning weights as\n2\nWorld\nSpace\nResidual\nDisplacement\nSurface Normal\nGradient\nBRDF\nEvaluation\nCamera\nRays\nLight\nRays\nLight Visibility\nAlbedo\nRoughness\nSpecular\nGround Truth\nFinal Rendering\nSurface Intersection\nGeometry\nMaterial\nCanonical SDF\nCanonical Material\nPhotometric Loss\nCanonical\nSpace\nHierarchical\nDistance\nHierarchical\nDistance\nPhysically\nBased\nRendering\nNetwork\nLearnable Light Probe\nSMPL Pose\nCanonical\nPoint\nGeometry\nMaterial\nRendering\nFigure 2. Overview of the proposed approach. Given world space camera rays, we perform sphere tracing on the hierarchically queried\ndistances (Section 3.2) to find surface intersections and canonical correspondences (Section 3.3). Light rays generated by an optimizable light\nprobe are also sphere traced with HDQ to compute the closest distances along the ray for soft visibility (Section 3.3). Material properties\n(Section 3.4) and surface normals are queried on the canonical correspondences and warped to world space. Then, the final pixel colors are\ncomputed using the rendering equation (Section 3.5).\nan MLP network and learn it from input data, such as human\nshapes [15,56] or multi-view videos [31,51]. Another line\nof works [45, 52] introduce a neural displacement field to\nimprove animation realism. The articulated deformation is\nrepresented by the LBS model of SMPL, and the non-rigid\ndeformation is predicted using an MLP network. While neu-\nral animatable methods can produce dynamic avatars that\nappear realistic, they do not model the material properties of\nthe avatars, making them unable to adapt to different lighting\nconditions.\nRelighting.\nTo relight objects, a typical approach is first\nacquiring their material properties and then rendering with\nnew illumination through physically-based rendering. Tradi-\ntional methods [22,57] mostly require a known illumination\nfor calculating the material parameters through photomet-\nric stereos. Light stage-based approaches [21, 22, 26, 70]\nbuild a controllable light array to capture images of tar-\nget subjects under multiple illuminations. Based on these\ncaptured images and the known illuminations, they solve\nfor the unknown material properties. Following this setting,\nsome methods [8, 33, 42] achieve photorealistic relighting\nresults by adopting a neural renderer to implicit learn the\nrelightable appearance model from light-stage images. How-\never, these methods typically require the geometry to be\nknown a priori. More recently, neural inverse rendering meth-\nods [10\u201312,16,40,62,75,78,79] explore more flexible cap-\nture settings, where the illumination is unknown or even\nvariable. Motivated by its potential for many human-centric\napplications, research on human relighting has been widely\nconducted in the literature [47, 49, 74, 77]. Same as other\nobjects, the material properties of human subjects can be\nrecovered using neural inverse rendering methods. The dif-\nference is that human subjects exhibit more strong material\npriors. Therefore, some methods [6,32,34,38,49,74] attempt\nto train neural networks to predict human materials from a\nsingle image. Recently, Relighting4D [17] have attempted\nto acquire human materials from sparse multi-view videos.\nHowever, Relighting4D is not designed to relight animatable\navatars realistically, limiting its applicability.\n3. Method\nGiven a sparse-view (or monocular) video of a human\nperformer under natural and unknown illumination, we learn\nto reconstruct the drivable geometry and photometric prop-\nerties of the human performer to create an animatable and\nrelightable neural avatar. We assume the human poses and\nthe foreground masks are provided as in [45,51\u201353].\n3.1. Relightable and Animatable Neural Avatar\nWe formulate the relightable and animatable avatar using\na set of canonical space neural fields and a warping between\nworld and canonical space defined by the linear blend skin-\nning algorithm [41] and a displacement field [45,51,52,69].\nIn the canonical space, we define a set of geometry (S(x))\nand material neural fields (A(x) and \u0393(x)) for the animated\nhuman model. The canonical space displacement field F\u2206x\nprovides additional pose-dependent deformation on top of\nSMPL inverse LBS. More details about the warping process\nare provided in Section 3.2 and the supplementary.\nThe relightable and animatable neural avatar will be ren-\ndered by casting camera rays in world space and finding the\nsurface intersection points xs and their normals ns using\nthe Hierarchical Distance Query (HDQ) algorithm, whose\n3\nSource (B) \nCanonical Space SDF\nSource (A) \nCoarse World Distance\nQuery Point 0\nBlend Threshold\nFine Distance\nWorld to Canonical Warp\nTarget \nWorld Space SDF\nQuery Point 1\nFigure 3. Illustration of the Hierarchical Distance Query algo-\nrithm. In this figure, query point 0 falls out of the cut-off threshold\nthus its coarse distance is used directly as the world space distance,\nwhile query point 1 blends the coarse world space distance and fine\ncanonical distance together since it is within the range of local SDF\nvalues.\nmaterial properties albedo \u03b1s and roughness \u03b3s can be ob-\ntained from the canonical mateiral MLPs A(x) and \u0393(x),\ncomposing the BRDF Rs(xs, \u03c9i, \u03c9o, ns). Light visibility\nVs(xs, \u03c9i) can be computed by performing HDQ sphere\ntracing on all incoming light directions. We also incorpo-\nrate Distance Field Soft Shadow (DFSS) algorithm [3,7,50]\nonto our drivable neural distance fields for soft-visibility\ncomputation. These properties will be integrated around the\nhemisphere \u03c9i \u2208 \u2126 using the rendering equation [36]:\nLo =\nZ\n\u2126\nLs(\u03c9i)Rs(xs, \u03c9i, \u03c9o, ns)Vs(xs, \u03c9i)(ns \u00b7 \u03c9i)d\u03c9i,\n(1)\nwhere Lo(xs, \u03c9o) \u2208 R3 is the outgoing radiance at the\nsurface intersection point xs, \u03c9o is the outgoing radiance\ndirection and \u03c9i is the incoming radiance direction. In this\npaper, we use the Microfacet BRDF model in [67] which\nis defined in the canonical space of the animatable avatar,\nand an optimizable light probe image Ls(\u03c9i) \u2208 R16\u00d732\u00d73.\nAn overview of the relightable and animatable avatar can be\nfound in Figure 2.\n3.2. Hierarchical Distance Query\nGiven the world space query point x, we approximate its\nworld space distance dworld(x) to the closest surface point\non the neural avatar with the Hierarchical Distance Query\nalgorithm dworld(x) \u2248 \u02dcdworld(x) = HDQ(x), which is\nlater used for Sphere Tracing [30]. The query algorithm\nconsists of four stages: (a) coarse distance query, (b) inverse\nwarping, (c) fine distance query, and (d) smooth distance\nblending.\nCoarse distance query.\nWe first perform a geodesically-\naware signed K Nearest Neighbor (GS-KNN) algorithm\n[55] on the posed vertices v \u2208 V of the driven paramet-\nric human model (SMPL-H [54]). GS-KNN produces the\nindices IK = {i0, ..., iK} of the K closest points to x\nin V, and its corresponding world-space closest vertices\nVK = {v0..., vK}, distances DK = {d0..., dK}, normals\nNK = {n0..., nK} and blend weights WK = {w0..., wK}.\nWe set K = 10 through all experiments. The unsigned dis-\ntance D is augmented with the sign of the dot product be-\ntween x\u2212v and n to produce a coarse SDF. We additionally\ndiscard the k-th neighbor vk if its canonical correspondence\n(T-Pose of SMPL-H) is too far from the canonical corre-\nspondence of the nearest neighbor, K is set to 10 for all\nexperiments. The coarse level world space SDF is defined as\ndworld\ncoarse =\nPK\nk=0 dk\nK\n.\nInverse warping.\nWe follow the previous literature [45,51]\nand use the linear blend skinning algorithm [41] to perform\nthe inverse warping. The details can be found in the supple-\nmentary material.\nFine distance query.\nGiven the warped query point x\u2032, the\npose-dependent displacement field F\u2206x adds small perturba-\ntion to produce the final canonical space query point x\u2032\u2032. We\nimplement F\u2206x as an MLP with the human pose at the fth\nframe \u0398f and x\u2032 as input. The displaced canonical point x\u2032\u2032\nfed into the canonical distance model S is defined as\nx\u2032\u2032 = x\u2032 + F\u2206x(\u0398f, x\u2032).\n(2)\nThen, the fine canonical distance value can be obtained by\nquerying the network dcan\nfine = S(x\u2032\u2032).\nSmooth distance blending.\nSince SDF values of points\nclose to the surface are hardly affected by LBS (Figure 2\nand 5 of the supplementary), we propose to blend the fine\ncanonical space distance value dcan\nfine and the coarse world\nspace distance dworld\ncoarse using a smooth function to produce\nthe final approximated world space distance value \u02dcdworld\n\u02dcdworld =\n(\ndworld\ncoarse\n, if dworld\ncoarse > \u02dcTd\ndcan\nfine(1 \u2212\ndcan\nfine\nTd ) + dworld\ncoarse\ndcan\nfine\nTd\n, otherwise\n(3)\nwhere \u02dcTd is the distance threshold for cutting off coarse and\nfine distances, which is empirically set to 0.1. Note that we\nonly perform the evaluation of S on points that satisfy the\ncutoff criteria dworld\ncoarse \u2264 \u02dcTd for efficiency.\n3.3. Geometry\nOur physically based renderer requires the pixel-surface\nintersection xs \u2208 R3, surface normal ns \u2208 R3, and light\n4\nvisibility V (xs, \u03c9i) \u2208 R as input. Using the Hierarchical\nDistance Query, these values can be easily obtained from the\nworld space SDF of the neural avatar under arbitrary human\nposes.\nSurface intersection.\nGiven a camera ray and the neural\navatar\u2019s SDF, we compute the location xs at which the ray\nr(t) = o + td from the camera origin o along the ray di-\nrection d intersects the surface of the posed neural avatar.\nSpecifically, we perform Nst Sphere Tracing iterations with\nthe world space distance \u02dcdworld = HDQ(x) using Hierar-\nchical Distance Query until the ray converges to the surface\nintersection point xs. The detailed algorithm is listed in the\nsupplementary. Nst is set to 16 across all experiments.\nSurface normal.\nThe analytic normal direction n of any\n3D points could be computed as the gradient of the neu-\nral SDF using \u2207 \u02dcdworld(x). Although the hierarchical dis-\ntance is differentiable, computing gradient through the whole\nquery process is not efficient. Instead, we notice that surface\nintersections should satisfy the cutoff criteria of smooth dis-\ntance blending in Section 3.2, that is\n\u02dcdworld(xs) = dcan\nfine(xs), dworld\ncoarse(xs) \u2264 \u02dcTd.\n(4)\nThus, the world space normal can be computed using\n\u2207S(xcan\ns\n) and transformed from canonical to world space\nusing the forward warping process. More details can be\nfound in the supplementary.\nLight visibility.\nLight visibility V (x, \u03c9i) from any 3D\npoint x along any light direction \u03c9i can be computed as\nwhether the light path x + t\u03c9i is occluded by the geometry\nof the posed neural avatar, which is later integrated in the ren-\ndering equation [36] around the hemisphere. Since we use a\ndiscrete light probe Ls(\u03c9i) \u2208 R16\u00d732\u00d73, the visibility term\nfor every light direction needs to be integrated on the area\nof the pixel of Ls(\u03c9i), which is time-consuming. Thanks to\nthe global meaning of distance field, this occlusion and inte-\ngration process can be approximated using Distance Field\nSoft Shadow (DFSS) [3,7,50], producing soft visibility with\na single light sample. Specifically, we compute the visibility\nas the soft penumbra coefficient ps(xs, \u03c9i):\nps(xs, \u03c9i) =\nmin(\n\u02dcdworld(xs + t0\u03c9i)\n2t0\np a\n\u03c0\n, ...,\n\u02dcdworld(xs + tN vis\nst \u03c9i)\n2tN vis\nst\np a\n\u03c0\n),\n(5)\nfor each surface point xs along one of the 512 light direc-\ntions \u03c9i defined by Ls(\u03c9i) during the N vis\nst\nsphere tracing\nsteps, which is set to 4 for all experiments. The ratio between\nthe two tangent values\n\u02dcdworld\nt\nand 2p a\n\u03c0 serves as an approx-\nimation of the ratio of light being occluded by the geometry\nfrom xs along \u03c9i. Thanks to the smooth blending of dworld\ncoarse\nand dcan\nfine in Section 3.2, our soft visibility scheme produces\nrealistic and smooth soft shadow even when distance values\nfrom the parametric human model [54] and the canonical\nneural SDF are not perfectly aligned. A detailed listing of\nthis algorithm is provided in the supplementary.\n3.4. Reflectance\nWe adopt the Microfacet BRDF model in [67] for our ma-\nterial representation, which is composed of a diffuse albedo\n\u03b1 \u2208 R3 term and a specular roughness \u03b3 \u2208 R term. We\nuse a fixed Fresnel term of 0.04. Similar to [17,78,79], we\nparamterize the albedo and roughness map with two MLPs\n\u03b1 = A(x\u2032\u2032) and \u03b3 = \u0393(x\u2032\u2032), which is defined in the same\ncanonical frame as S(x\u2032\u2032) and F\u2206(x\u2032) in Section 3.2. The\nBRDF model is denoted Rs(xs, \u03c9i, \u03c9o, ns) where \u03c9i is the\nincoming radiance direction, \u03c9o is the outgoing radiance\ndirection and ns is the surface normal.\nGiven world space query point xs and its correspond-\ning canonical space point x\u2032\u2032, we obtain the albedo \u03b1 and\nroughness \u03b3 by querying their canonical neural fields A\nand \u0393, which can then be converted to BRDF values as\ndefined in [67]. Our physically-based renderer also takes\na light probe Ls(\u03c9i) \u2208 R16\u00d732\u00d73 as input, which is rep-\nresented by an optimizable neural texture during training\nand replaced with the designated environment map during\nrelighting [17,20,78].\n3.5. Training\nWe use 512 discrete incoming light directions defined\nby the light probe Ls(\u03c9i) \u2208 R16\u00d732\u00d73 to approximate the\nRendering Equation [36] as\nLo =\nX\n\u03c9i\nLs(\u03c9i)Rs(xs, \u03c9i, \u03c9o, ns)Vs(xs, \u03c9i)(ns \u00b7\u03c9i)\u2206\u03c9i,\n(6)\nwhere \u2206\u03c9i is the solid angle of the incoming light \u03c9i sam-\npled from the light probe Ls(\u03c9i) and Lo(xs, \u03c9o) \u2208 R3 is\nthe outgoing radiance at the surface intersection xs.\nWe optimize our relightable and animatable neural hu-\nman avatar by rendering the image with given camera poses\nand comparing the pixel values Lo against the ground truth\nones Lgt. The main loss function is defined as Ldata =\nP\nr\u2208R \u2225Lo(r) \u2212 Lgt(r)\u22252, where r = o+td \u2208 R denotes\nall camera rays in the forward rendering process. Due to\nthe ill-posed nature of the problem, we adopt a two-stage\ntraining strategy and add additional regularizations one the\ngeometry (eikonal loss Leik) and material (sparsity loss Lent\nand smoothness loss La, Lr). We elaborate on the details of\neach loss term and the training stragety in the supplemen-\ntary. The training takes 20 hours on an Nvidia RTX 3090.\nRendering a 512\u00d7512 image takes 5s.\n5\nNormal\nOurs\nNeRFactor\nRelighting4D\nGround Truth\nAlbedo\nVisibility Rendering\nRelight\n    Visibility\n(Novel Light)\nNormal\nAlbedo\nVisibility Rendering\nRelight\n    Visibility\n(Novel Light)\nNormal\nAlbedo\nVisibility Rendering\nRelight\n    Visibility\n(Novel Light)\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nNeRFactor*\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nRelighting4D*\nFigure 4. Qualitative comparison of our method and baselines. The first six columns display the results of synthesizing a character in a\nnovel pose from the MobileStage dataset. The middle six columns depict a character in a training pose from the MobileStage dataset. For\nthe last six columns, we show results from SyntheticHuman++, for which we have ground truth as reference. Note that NeRFactor is only\ntrained on 1 frame. Relighting4D* and NeRFactor* denote directly computing normal and visibility using their density MLPs.\n4. Experiments\nIn this section, we conduct qualitative and quantitative\nexperiments to investigate the performance of our relightable\nneural avatars. All hyperparameters are fixed through out\nthe experiments unless otherwise specified. In Section 4.1,\nwe briefly introduce the datasets used for evaluation. Then\nwe make quantitative and qualitative comparisons with three\nbaseline methods in Section 4.2. Finally, we conduct ablation\nstudies to investigate the effectiveness of our Hierarchical\nDistance Query and the soft visibility scheme in Section 4.3.\n4.1. Datasets\nWe collect two datasets MobileStage and SyntheticHu-\nman++ for evaluation. MobileStage is a real-world multi-\nview (36 views) dataset created with synchronized mobile\nphone cameras on 4 real-world humans. We uniformly se-\nlect 12 views for training. SyntheticHuman++ contains 4\nsequences (20 views) of dynamic 3D human models with\nground truth shape and relighting information. We uniformly\nselect 10 views for training for the sparse-view setting and\nwe use the fourth view for the monocular setting. Please\nrefer to the supplementary material for more detail.\n4.2. Baseline Comparisons\nBaselines.\nTo the best of our knowledge, there are very\nfew prior works that study the exact same setting as ours,\ni.e. training with unknown illumination and sparse-view (or\nmonocular) videos while rendering with novel illumination\n6\nTable 1. Quantitative comparison. We compare our method with baselines on the SyntheticHuman++ dataset. Following [53], the SSIM\nand LPIPS [76] metrics are computed in the bounding box of the human region, while the degree of normal and PSNR metrics are computed\nwithin the foreground mask. Due to the inherent scale ambiguity in the inverse rendering task, we align the rendered images and albedo with\nground truth ones following [75] before computing metrics. Since NeRFactor [78] is designed to fit static objects, we train and evaluate\nit only on the first frame of each sequence. All image metrics are computed in the foreground region in linear color space. \u201c\u2217\u201d denotes\nvariants without the normal and visibility MLPs. AniSDF produces baked lighting in the rendering network, while NeRFactor [78] and\nRelighting4D [17] perform poorly on the uniform shading task for visibility since the reconstructed geometry is too rough. Our method\nworks well even on the challenging monocular setting.\nNormal\nDiffuse Albedo\nRelighting\nVisibility\nDegree \u2193\nPSNR\u2191 SSIM\u2191 LPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193\nSparse-View\nOurs\n12.44\n29.01\n0.933\n0.119\n22.69\n0.861\n0.206\n20.20\n0.848\n0.155\nRelighting4D*\n29.38\n24.70\n0.885\n0.183\n22.13\n0.835\n0.237\n15.22\n0.763\n0.252\nRelighting4D\n93.83\n24.71\n0.885\n0.183\n20.87\n0.774\n0.276\n5.366\n0.514\n0.375\nNeRFactor* (1 frame)\n34.29\n22.23\n0.817\n0.226\n21.04\n0.758\n0.313\n11.37\n0.581\n0.387\nNeRFactor (1 frame)\n51.92\n22.23\n0.817\n0.226\n20.70\n0.757\n0.299\n10.56\n0.597\n0.361\nAniSDF\n14.72\n22.13\n0.862\n0.202\n17.55\n0.799\n0.262\n-\n-\n-\nMonocular\nOurs\n18.71\n23.42\n0.873\n0.176\n22.45\n0.831\n0.224\n17.95\n0.761\n0.212\nRelighting4D*\n26.17\n25.37\n0.864\n0.210\n21.81\n0.802\n0.254\n17.10\n0.709\n0.286\nRelighting4D\n81.74\n25.36\n0.864\n0.210\n21.85\n0.806\n0.268\n16.18\n0.726\n0.302\nAniSDF\n20.36\n21.51\n0.812\n0.255\n18.29\n0.745\n0.297\n-\n-\n-\nand novel human poses. We take NeRFactor [78] and Re-\nlighting4D [17] as baselines and make comparisons with\nthem on both real and synthetic datasets. Since NeRFactor\nis designed to handle static objects, we only train and eval-\nuate it on the multi-view images of the first frame of each\nvideo. We observe that their normal and visibility MLPs\noften fail under complex human motions, thus we addition-\nally compare with a Relighting4D* and NeRFactor* variant\nwhere we use the normal and visibility computed from the\ndensity MLP instead of the normal and visibility MLPs. To\nillustrate the effectiveness of our proposed components, we\nadditionally render AniSDF [52] as a baseline.\nMetrics.\nFor quantitative analysis, we compare the normal\n(in degrees), albedo (in PSNR, SSIM and LPIPS [76]), light\nvisibility rendered with uniform BRDF (in PSNR, SSIM,\nand LPIPS) and relighting (in PSNR, SSIM, and LPIPS) re-\nsults on 6 different light probes obtained from Polyhaven [2].\nFollowing [75], we align the diffuse albedo and rendered\nimages with ground truth ones before computing metrics to\nmitigate the inherent scale ambiguity in the inverse render-\ning problem. Note that we compute PSNR using the save\nprotocal as [51], only computing metrics on the human re-\ngion. When computing metrics on the full image, our method\nreports PSNR of 28-30 dB for the relighting and visibility\ntask. We do not compare the roughness term since Blender\nuses a different Principled BRDF model from [67]. Envi-\nronment map of SyntheticHuman [52] is not available since\nthey used programmatically defined light sources. Qualita-\ntive results can be found in the supplementary video. We\ncompare the uniform shading results to evaluate the visibility\nquality, where the BRDFs of the reconstructed avatars are\nset to 0.8 across all radiance directions (denoted \u201cVisibility\u201d)\nwhen rendering. Since SyntheticHuman [52] does not pro-\nvide ground truth models for novel poses, we only perform\nquantitative comparisons on training poses in Table 1, while\nqualitative analysis of animating the avatars can be found in\nFigure 4 and the supplementary video.\nResults.\nAs shown in Figure 4, our approach can success-\nfully decompose the material and dynamic geometry of the\nneural avatar, generating a relightable neural avatar from only\nsparse-view (or monocular) video inputs. In comparison,\nNeRFactor [78] trained on 1 video frame overfits the training\nimage when training views are sparse. Relighting4D [17]\npasses structured latent codes [53] to NeRFactor\u2019s MLPs, en-\nabling it to relight a dynamic video of human performance.\nHowever, its quality decreases greatly when synthesizing\nnovel poses. This is mainly because the visibility and normal\nMLP used in [17] is not generalizable to novel human mo-\ntions. For the Relighting4D* variant, the density backbone\nstill fails to generalize to novel poses [45,52]. AniSDF [52]\nbakes illuminations effects like self-occlusions onto the ren-\ndering network, thus the reconstructed neural avatar looks\nunrealistic under novel illuminations. Qualitative results on\nmonocular inputs can be found in Figure 12 and the sup-\nplementary video. Relighting4D and NeRFactor take 3s to\nrender a 512\u00d7512 image, their \u201c*\u201d variants take 50s and our\nmethod takes 5s.\n4.3. Ablation Studies\nIn this part, we ablate the effectiveness of our proposed\nHierarchical Distance Query and soft visibility scheme on\nrelighting quality with the jody model of SyntheticHuman++\n7\nw/o HDQ\nw/o HDQ vis\nw/o\nw/o\nOurs\nGround Truth\n(GS-KNN)\nFigure 5. Effectiveness of Hierarchical Distance Query. Performing sphere tracing using only the canonical distance dcan\nfine or coarse world\ndistance dworld\ncoarse results in incorrect surface intersection xs and soft visibility ps, while tracing with our proposed Hierarchical Distance\nQuery produces correct results. Using hard shadow (\u201cw/o soft vis\u201d) or local shadow (\u201cw/o HDQ vis\u201d) leads degraded perceptual quality.\nunder the sparse-view setting. We provide more detailed\nablation on the soft visibility scheme, the number of sphere\ntracing steps, the number of vertices for GS-KNN, and the\nnumber of canonical material samples in the supplementary.\nTable 2. Ablation study on the proposed Hierarchical Distance\nQuery algorithm. Qualitative results can be found in Figure 5. The\n\u201cw/o HDQ\u201d variant uses naive volume rendering (128 samples per\nray) to compute pixel-surface intersection and visibility, which is\nnot only slow (60s per image at 512 \u00d7 512 resolution compared to\nour 5s per image) but also exhibits inferior rendering quality.\nRelighting\nVisibility\nPSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193\nOurs\n21.57\n0.853\n0.168\n20.53\n0.869\n0.142\nw/o soft vis\n21.19\n0.848\n0.173\n21.27\n0.873\n0.145\nw/o HDQ vis\n21.00\n0.844\n0.175\n20.88\n0.869\n0.143\nw/o HDQ\n21.36\n0.753\n0.196\n20.00\n0.760\n0.173\nw/o dworld\ncoarse\n20.69\n0.792\n0.236\n18.75\n0.767\n0.250\nw/o dcan\nfine (GS-KNN) 19.56\n0.784\n0.245\n14.63\n0.758\n0.233\nEffectiveness of Hierarchical Distance Query and soft\nvisibility scheme.\nIn Figure 5, We compare the results of\nperforming sphere tracing on the canonical space distance\n(\u201cw/o dworld\ncoarse\u201d), coarse GS-KNN distance (\u201cw/o dcan\nfine\u201d) and\nour proposed hierarchically queried distance (\u201cOurs\u201d). As\nshown in the figure, the canonical space distance is incorrect\nwhen the query point is far from the actual surface of the\nhuman geometry, resulting in incorrect surface intersection\npoints after the termination of the sphere tracing algorithm.\nAdditionally, computing light visibility on this incorrect dis-\ntance field would lead to false black regions since distances\nfar from surface points are not reported correctly. Perform-\ning surface intersection and visibility computation on the\ncoarse distance results in distorted rendering results. The\n\u201cw/o HDQ\u201d variant uses volume rendering of 128 samples\nper ray for surface intersection and light visibility computa-\ntion, leading to an excessive rendering time of 60s per image\nfor a resolution of 512\u00d7512, while our HDQ algorithm is\nable to obtain 10x speed-up at 5s per image with superior\nrendering quality.\nWe demonstrate the effectiveness of our HDQ-DFSS al-\ngorithm by comparing it with two other variants where (a)\nhard visibility is used (\u201cw/o soft vis\u201d) and (b) only local visi-\nbility computed from normal is used (\u201cw/o HDQ vis\u201d). The\nquantitative comparison of all three variants can be found\nin Table 2. Note that although the \u201cw/o soft\u201d variants report\nhigher PSNR and SSIMs, the visual quality of hard cast shad-\nows is worse than ours, as indicated by the LPIPS metric and\nvisible in Figure 5.\nSensitivity analysis on hyper-parameters.\nWe provide\nadditional sensitivity analysis of hyper parameters including\nNst (the number of sphere tracing steps), N vis\nst\n(the num-\nber of sphere tracing steps for visibility computation), K\n(the number of points sampled to perform GS-KNN and\ncanonical-to-world warping), Ns (the number of points to\nsample in the material MLPs) and a runtime and quality anal-\nysis regarding the cut-off value \u02dcTd in the supplementary.\n5. Conclusion and Discussion\nThis paper presents a novel framework to reconstruct\nrelightable and animatable neural avatars from only sparse-\nview (or monocular) video input. We generalize the canon-\nical distance field to arbitrary human poses via a hierar-\nchical distance query scheme, with which the photometric\nproperties of the neural avatar can be easily retrieved for\nrelighting. We demonstrate that together with other inno-\nvative components, our approach reconstructs high-quality\nanimatable geometry and material, supporting realistic re-\nlighting. This work also has some limitations. Although the\nproposed method produces high-quality relighting results\nfrom challenging sparse-view or monocular settings, it has\nthe natural limitation of neural field methods in that it re-\nquires a long training time of 20 hours. Future work could\nconsider recent neural field acceleration methods to further\nincrease the training speed.\n8\nReferences\n[1] Easymocap - make human motion capture easier. Github,\n2021. 14\n[2] Poly Haven, 2023. 7, 12\n[3] Sebastian Aaltonen.\nGpu-based clay simulation and ray-\ntracing tech in claybook. San Francisco, CA, 2(5), 2018.\n4, 5, 16\n[4] Kairat Aitpayev and Jaafar Gaber. Creation of 3d human\navatar using kinect. Asian Transactions on Fundamentals of\nElectronics, Communication & Multimedia, 1(5):12\u201324, 2012.\n2\n[5] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian\nTheobalt, and Gerard Pons-Moll. Video based reconstruction\nof 3d people models. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 8387\u2013\n8397, 2018. 2\n[6] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu.\nPhotorealistic monocular 3d reconstruction of humans wear-\ning clothing. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 1506\u20131515,\n2022. 3\n[7] R\u00b4obert B\u00b4an, Csaba B\u00b4alint, and G\u00b4abor Valasek. Area lights\nin signed distance function scenes. In Eurographics (Short\nPapers), pages 85\u201388, 2019. 4, 5, 16\n[8] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon,\nShih-En Wei, Kevyn Mcphail, Ravi Ramamoorthi, Yaser\nSheikh, and Jason Saragih.\nDeep relightable appearance\nmodels for animatable faces. ACM Transactions on Graphics\n(TOG), 40(4):1\u201315, 2021. 3\n[9] Federica Bogo, Michael J Black, Matthew Loper, and Javier\nRomero. Detailed full-body reconstructions of moving people\nfrom monocular rgb-d sequences. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2300\u2013\n2308, 2015. 2\n[10] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Bar-\nron, Ce Liu, and Hendrik Lensch. Nerd: Neural reflectance\ndecomposition from image collections. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 12684\u201312694, 2021. 2, 3\n[11] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen\nLi, Deqing Sun, Jonathan T Barron, Hendrik Lensch, and\nVarun Jampani. Samurai: Shape and material from uncon-\nstrained real-world arbitrary image collections. arXiv preprint\narXiv:2205.15768, 2022. 3\n[12] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan\nBarron, and Hendrik Lensch.\nNeural-pil: Neural pre-\nintegrated lighting for reflectance decomposition. Advances\nin Neural Information Processing Systems, 34:10691\u201310704,\n2021. 3\n[13] Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang,\nand Juyong Zhang.\nNeural surface reconstruction of dy-\nnamic scenes with monocular rgb-d camera. arXiv preprint\narXiv:2206.15258, 2022. 18\n[14] Joel Carranza, Christian Theobalt, Marcus A Magnor, and\nHans-Peter Seidel. Free-viewpoint video of human actors.\nACM transactions on graphics (TOG), 22(3):569\u2013577, 2003.\n1, 2\n[15] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\nand Andreas Geiger. Snarf: Differentiable forward skinning\nfor animating non-rigid neural implicit shapes. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision, pages 11594\u201311604, 2021. 2, 3\n[16] Ziyu Chen, Chenjing Ding, Jianfei Guo, Dongliang Wang,\nYikang Li, Xuan Xiao, Wei Wu, and Li Song. L-tracing: Fast\nlight visibility estimation on neural surfaces by sphere tracing.\nIn European Conference on Computer Vision, pages 217\u2013233.\nSpringer, 2022. 2, 3, 16\n[17] Zhaoxi Chen and Ziwei Liu. Relighting4d: Neural relightable\nhuman from videos. In European Conference on Computer\nVision, pages 606\u2013623. Springer, 2022. 3, 5, 7, 12, 13, 17, 20,\n21\n[18] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-\nnis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk, and\nSteve Sullivan. High-quality streamable free-viewpoint video.\nACM Transactions on Graphics (ToG), 34(4):1\u201313, 2015. 1,\n2\n[19] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2018. 14\n[20] Paul Debevec. Rendering synthetic objects into real scenes:\nBridging traditional and image-based graphics with global\nillumination and high dynamic range photography. In Acm\nsiggraph 2008 classes, pages 1\u201310. 2008. 5\n[21] Paul Debevec. The light stages and their applications to\nphotoreal digital actors. Technical report, UNIVERSITY OF\nSOUTHERN CALIFORNIA LOS ANGELES, 2012. 1, 3\n[22] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter\nDuiker, Westley Sarokin, and Mark Sagar. Acquiring the\nreflectance field of a human face. In Proceedings of the 27th\nannual conference on Computer graphics and interactive\ntechniques, pages 145\u2013156, 2000. 1, 3\n[23] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei\nZhou. Reconstructing 3d human pose by watching humans in\nthe mirror. In CVPR, 2021. 14\n[24] Oliver Grau. Studio production system for dynamic 3d con-\ntent. In Visual Communications and Image Processing 2003,\nvolume 5150, pages 80\u201389. SPIE, 2003. 2\n[25] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and\nYaron Lipman. Implicit geometric regularization for learning\nshapes. arXiv preprint arXiv:2002.10099, 2020. 16\n[26] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,\nXueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-\nEscolano, Rohit Pandey, Jason Dourgarian, et al. The re-\nlightables: Volumetric performance capture of humans with\nrealistic relighting. ACM Transactions on Graphics (ToG),\n38(6):1\u201319, 2019. 1, 3\n[27] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger-\nard Pons-Moll, and Christian Theobalt. Livecap: Real-time\nhuman performance capture from monocular video. ACM\nTransactions On Graphics (TOG), 38(2):1\u201317, 2019. 2\n[28] Marc Habermann, Weipeng Xu, Michael Zollhofer, Gerard\nPons-Moll, and Christian Theobalt. Deepcap: Monocular\nhuman performance capture using weak supervision. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5052\u20135063, 2020. 2\n9\n[29] John C Hart. Sphere tracing: A geometric method for the\nantialiased ray tracing of implicit surfaces. The Visual Com-\nputer, 12(10):527\u2013545, 1996. 17\n[30] John C Hart et al. Sphere tracing: Simple robust antialiased\nrendering of distance-based implicit surfaces. In Siggraph,\nvolume 93, pages 1\u201311, 1993. 2, 4, 18\n[31] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony\nTung. Arch: Animatable reconstruction of clothed humans.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3093\u20133102, 2020. 2, 3\n[32] Umar Iqbal, Akin Caliskan, Koki Nagano, Sameh Khamis,\nPavlo Molchanov, and Jan Kautz. Rana: Relightable articu-\nlated neural avatars. arXiv preprint arXiv:2212.03237, 2022.\n3\n[33] Shun Iwase, Shunsuke Saito, Tomas Simon, Stephen Lom-\nbardi, Timur Bagautdinov, Rohan Joshi, Fabian Prada,\nTakaaki Shiratori, Yaser Sheikh, and Jason Saragih.\nRe-\nlightablehands: Efficient neural relighting of articulated hand\nmodels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 16663\u201316673,\n2023. 3\n[34] Chaonan Ji, Tao Yu, Kaiwen Guo, Jingxin Liu, and Yebin Liu.\nGeometry-aware single-image full-body human relighting. In\nEuropean Conference on Computer Vision, pages 388\u2013405.\nSpringer, 2022. 3\n[35] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,\nand Anurag Ranjan. Neuman: Neural human radiance field\nfrom a single video. In European Conference on Computer\nVision, pages 402\u2013418. Springer, 2022. 2\n[36] James T Kajiya. The rendering equation. In Proceedings\nof the 13th annual conference on Computer graphics and\ninteractive techniques, pages 143\u2013150, 1986. 4, 5\n[37] Takeo Kanade, Peter Rander, and PJ Narayanan. Virtualized\nreality: Constructing virtual worlds from real scenes. IEEE\nmultimedia, 4(1):34\u201347, 1997. 1, 2\n[38] Yoshihiro Kanamori and Yuki Endo. Relighting humans:\nocclusion-aware inverse rendering for full-body human im-\nages. arXiv preprint arXiv:1908.02714, 2019. 3\n[39] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 17, 18\n[40] Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang,\nPanos Achlioptas, and Sergey Tulyakov.\nNeroic: Neural\nrendering of objects from online image collections. arXiv\npreprint arXiv:2201.02533, 2022. 3\n[41] John P Lewis, Matt Cordner, and Nickson Fong. Pose space\ndeformation: a unified approach to shape interpolation and\nskeleton-driven deformation.\nIn Proceedings of the 27th\nannual conference on Computer graphics and interactive\ntechniques, pages 165\u2013172, 2000. 2, 3, 4, 18\n[42] Junxuan Li, Shunsuke Saito, Tomas Simon, Stephen Lom-\nbardi, Hongdong Li, and Jason Saragih. Megane: Morphable\neyeglass and avatar network. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12769\u201312779, 2023. 3\n[43] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa.\nAi choreographer: Music conditioned 3d dance generation\nwith aist++. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 13401\u201313412, 2021.\n12, 14\n[44] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip\nSengupta. Robust high-resolution video matting with tem-\nporal guidance. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 238\u2013\n247, 2022. 14\n[45] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt. Neural actor: Neu-\nral free-view synthesis of human actors with pose control.\nACM Transactions on Graphics (TOG), 40(6):1\u201316, 2021. 1,\n2, 3, 4, 7\n[46] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\nperson linear model. ACM transactions on graphics (TOG),\n34(6):1\u201316, 2015. 2\n[47] Abhimitra Meka, Rohit Pandey, Christian Haene, Sergio Orts-\nEscolano, Peter Barnum, Philip David-Son, Daniel Erickson,\nYinda Zhang, Jonathan Taylor, Sofien Bouaziz, et al. Deep\nrelightable textures: volumetric performance capture with\nneural rendering. ACM Transactions on Graphics (TOG),\n39(6):1\u201321, 2020. 3\n[48] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 2,\n16\n[49] Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Chris-\ntian Haene, Sofien Bouaziz, Christoph Rhemann, Paul De-\nbevec, and Sean Fanello. Total relighting: learning to relight\nportraits for background replacement. ACM Transactions on\nGraphics (TOG), 40(4):1\u201321, 2021. 3\n[50] Steven Parker, Peter Shirley, and Brian Smits. Single sample\nsoft shadows. Technical report, Technical Report UUCS-98-\n019, Computer Science Department, University of Utah, 1998.\n2, 4, 5, 14, 16\n[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang,\nQing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable\nneural radiance fields for modeling dynamic human bodies.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 14314\u201314323, 2021. 2, 3, 4, 7\n[52] Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang,\nShangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei\nZhou.\nAnimatable implicit neural representations for\ncreating realistic avatars from videos.\narXiv preprint\narXiv:2203.08133, 2022. 1, 2, 3, 7, 14, 16, 17, 18, 20, 21\n[53] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9054\u20139063, 2021. 1, 2, 3, 7, 14\n[54] Javier Romero, Dimitrios Tzionas, and Michael J Black. Em-\nbodied hands: Modeling and capturing hands and bodies to-\ngether. ACM ToG, 2017. 4, 5, 14\n[55] Nick Roussopoulos, Stephen Kelley, and Frederic Vincent.\nNearest neighbor queries. In Proceedings of the 1995 ACM\n10\nSIGMOD international conference on Management of data,\npages 71\u201379, 1995. 4\n[56] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J\nBlack. Scanimate: Weakly supervised learning of skinned\nclothed avatar networks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2886\u20132897, 2021. 2, 3\n[57] Carolin Schmitt, Simon Donne, Gernot Riegler, Vladlen\nKoltun, and Andreas Geiger. On joint estimation of pose,\ngeometry and svbrdf from a handheld scanner. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3493\u20133503, 2020. 3\n[58] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pages 4104\u2013\n4113, 2016. 14\n[59] Dario Seyb, Alec Jacobson, Derek Nowrouzezahrai, and Wo-\njciech Jarosz. Non-linear sphere tracing for rendering de-\nformed signed distance fields. ACM Transactions on Graph-\nics, 38(6), 2019. 2, 18\n[60] Ari Shapiro, Andrew Feng, Ruizhe Wang, Hao Li, Mark Bo-\nlas, Gerard Medioni, and Evan Suma. Rapid avatar capture\nand simulation using commodity depth sensors. Computer\nAnimation and Virtual Worlds, 25(3-4):201\u2013211, 2014. 2\n[61] Nicholas Sharp and Alec Jacobson. Spelunking the deep: guar-\nanteed queries on general neural implicit surfaces via range\nanalysis. ACM Transactions on Graphics (TOG), 41(4):1\u201316,\n2022. 18\n[62] Pratul P Srinivasan, Boyang Deng, Xiuming Zhang, Matthew\nTancik, Ben Mildenhall, and Jonathan T Barron. Nerv: Neu-\nral reflectance and visibility fields for relighting and view\nsynthesis. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 7495\u20137504,\n2021. 2, 3\n[63] Jonathan Starck and Adrian Hilton. Virtual view synthesis\nof people from multiple view video sequences. Graphical\nModels, 67(6):600\u2013620, 2005. 2\n[64] Jonathan Starck and Adrian Hilton.\nSurface capture for\nperformance-based animation. IEEE computer graphics and\napplications, 27(3):21\u201331, 2007. 1, 2\n[65] Jing Tong, Jin Zhou, Ligang Liu, Zhigeng Pan, and Hao Yan.\nScanning 3d full human bodies using kinects. IEEE transac-\ntions on visualization and computer graphics, 18(4):643\u2013650,\n2012. 2\n[66] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan\nPopovi\u00b4c. Articulated mesh animation from multi-view sil-\nhouettes. In Acm Siggraph 2008 papers, pages 1\u20139. 2008.\n2\n[67] Bruce Walter, Stephen R Marschner, Hongsong Li, and Ken-\nneth E Torrance. Microfacet models for refraction through\nrough surfaces. In Proceedings of the 18th Eurographics\nconference on Rendering Techniques, pages 195\u2013206, 2007.\n4, 5, 7\n[68] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu\nTang. Arah: Animatable volume rendering of articulated\nhuman sdfs. In European conference on computer vision,\npages 1\u201319. Springer, 2022. 1, 2\n[69] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video. arXiv preprint arXiv:2201.04127, 2022. 2,\n3\n[70] Andreas Wenger, Andrew Gardner, Chris Tchou, Jonas Unger,\nTim Hawkins, and Paul Debevec. Performance relighting and\nreflectance transformation with time-multiplexed illumination.\nACM Transactions on Graphics (TOG), 24(3):756\u2013764, 2005.\n1, 3\n[71] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.\nH-nerf: Neural radiance fields for rendering and temporal\nreconstruction of humans in motion. Advances in Neural\nInformation Processing Systems, 34:14955\u201314966, 2021. 2\n[72] Weipeng Xu, Avishek Chatterjee, Michael Zollh\u00a8ofer, Helge\nRhodin, Dushyant Mehta, Hans-Peter Seidel, and Christian\nTheobalt. Monoperfcap: Human performance capture from\nmonocular video. ACM Transactions on Graphics (ToG),\n37(2):1\u201315, 2018. 2\n[73] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. Advances in Neural\nInformation Processing Systems, 34:4805\u20134815, 2021. 16\n[74] Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz, Ming-\nYu Liu, and Ting-Chun Wang. Learning to relight portrait\nimages via a virtual light stage and synthetic-to-real adap-\ntation. ACM Transactions on Graphics (TOG), 41(6):1\u201321,\n2022. 3\n[75] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and\nNoah Snavely. Physg: Inverse rendering with spherical gaus-\nsians for physics-based material editing and relighting. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5453\u20135462, 2021. 2, 3,\n7\n[76] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n586\u2013595, 2018. 7\n[77] Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun,\nTianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip\nDavidson, Christoph Rhemann, Paul Debevec, et al. Neu-\nral light transport for relighting and view synthesis. ACM\nTransactions on Graphics (TOG), 40(1):1\u201317, 2021. 3\n[78] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De-\nbevec, William T Freeman, and Jonathan T Barron. Nerfactor:\nNeural factorization of shape and reflectance under an un-\nknown illumination. ACM Transactions on Graphics (TOG),\n40(6):1\u201318, 2021. 2, 3, 5, 7, 16, 19\n[79] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei\nJia, and Xiaowei Zhou. Modeling indirect illumination for\ninverse rendering. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18643\u201318652, 2022. 2, 3, 5\n[80] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-\ndong Guo, and Yebin Liu. Structured local radiance fields\nfor human avatar modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 15893\u201315903, 2022. 15\n11\n6. Addtional Results\n6.1. Addtional Comparisons\nIn this section, we provide more qualitative results. Dy-\nnamic reconstruction and animation results under novel illu-\nminations can be found in the supplementary video, which\nbetter demonstrates the effectiveness of our method to recon-\nstruct the animatable human avatar than figures.\nWe provide more relighting and animation results in Fig-\nure 12 and Figure 13 where we show the reconstructed neural\navatar being driven by novel human poses extracted from\nthe AIST++ [43] dataset and relight with HDRi images from\nPolyhaven [2].\nAdditionally, we provide comparisons on monocular in-\nputs for our method and baselines in Figure 6. Relight-\ning4D [17] failed to reconstruct the normal and visibility\nMLP under this challenging monocular input, leading to\nincorrect relighting results and visibility estimation. Our\nmethod is able to recover the relightable properties and suc-\ncessfully relight the human avatar given even only monocular\ninput.\n6.2. Additional Ablation Studies\nTable 3. Ablation on other hyperparameters used in the model.\nNst and N vis\nst\ndenote the number of sphere tracing steps for pixel-\nsurface intersection and soft light visibility estimation respectively.\nK is the number of vertices sampled in the inverse warping process\nof GS-KNN and Ns is the number canonical material samples for\nvolume rendering albedo \u03b1 and roughness \u03b3.\nRelighting\nVisibility\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nNst = 32\n20.84\n0.810\n0.201\n18.45\n0.798\n0.193\nNst = 16\n21.08\n0.815\n0.197\n18.19\n0.798\n0.187\nNst = 8\n20.67\n0.773\n0.242\n17.50\n0.726\n0.266\nNst = 4\n20.43\n0.680\n0.350\n15.88\n0.578\n0.402\nNst = 2\n19.22\n0.589\n0.428\n13.38\n0.494\n0.465\nN vis\nst\n= 8\n20.99\n0.811\n0.196\n18.27\n0.802\n0.187\nN vis\nst\n= 4\n21.08\n0.815\n0.197\n18.19\n0.798\n0.187\nN vis\nst\n= 2\n20.71\n0.812\n0.199\n18.63\n0.806\n0.184\nN vis\nst\n= 1\n20.55\n0.804\n0.202\n18.49\n0.804\n0.186\nK = 10\n21.08\n0.815\n0.197\n18.19\n0.798\n0.187\nK = 5\n20.76\n0.810\n0.203\n18.17\n0.795\n0.195\nK = 2\n20.78\n0.812\n0.198\n18.42\n0.803\n0.188\nNs = 10\n20.65\n0.813\n0.199\n17.98\n0.800\n0.195\nNs = 5\n20.80\n0.811\n0.202\n18.42\n0.797\n0.194\nNs = 3\n21.08\n0.815\n0.197\n18.19\n0.798\n0.187\nNs = 1\n20.81\n0.813\n0.200\n18.48\n0.804\n0.183\nEffectiveness of soft visibility scheme.\nWe also study the\neffect of introducing the soft visibility scheme in Figure 8\nand Table 3. All hyperparameters are fixed through out the\nexperiments unless otherwise specified. We show the results\nof lighting a model with uniform material, where the BRDFs\nare equal everywhere, using a single area light source in\nFigure 8. This effectively produces ambient occlusion for\nthe models. The quantitative comparison of all three variants\ncan be found in Table 2 of the main paper. Note that although\nthe \u201cw/o soft\u201d variants report higher PSNR and SSIMs, the\nvisual quality of hard cast shadows is worse than ours, as\nindicated by the LPIPS metric. Additionally, we compare\nthe effect of modeling visibility on material acquisition in\nFigure 10. The results indicate not modeling visibility leads\nto baked shadow on the diffuse albedo, while our method\nreconstructs a much more uniform diffuse map.\nSensitivity analysis and ablation study on other hyperpa-\nrameter choices.\nIn this section, we ablate the influence\nof other hyperparameters used in the model. The quantitative\nresults are shown in Table 3. Nst denotes the number of\nsphere tracing steps for finding pixel-surface intersections.\nTable 3 shows that too low a number of steps for sphere\ntracing would lead to worse rendering quality, however, a\ntoo high number of steps only leads to diminishing returns.\nThus we chose Nst = 16 in all our experiments. N vis\nst\nis\nthe number of sphere tracing steps for computing the soft\nvisibility term, which shares a similar diminishing return\npattern as Nst. The number of vertices K used in the inverse\nwarping process and the number of samples Ns taken in\ncanonical space for material properties (Section 7.6) does\nnot significantly impact the rendering quality. Thus their\nvalues are empirically set to 10 and 3 in our experiments.\nFurthermore, we provide a runtime and metric analysis on\ndifferent choices of cut-off value \u02dcTd used during the sphere\ntracing of the neural distance field in Table 5. In our method,\nthe cut-off during camera-human intersection sphere-tracing\n( \u02dcTd) is set to 0.1 and the cut-off for computing the visibility\nterm ( \u02dcT vis\nd\n) is set to 0.025. It\u2019s worth noting that setting\nthe cut-off value to zero is effectively only using the coarse\ndistance produced by GS-KNN for rendering, which greatly\ndegrades the quality of relighting and rendering as shown in\nFigure 5 of the main paper.\nTable 4. Ablation study on the accuracy of the proposed HDQ\nalgorithm and sphere tracing procedure. We provide comparison\non the average absolute distance (denoted abs(SDF(xs))) of the\ncomputed pixel-surface intersection on four variants of the HDQ\nalgorithm to compare its accuracy. Our method achieves an average\naccuracy of 0.00017m while the other variants exhibit at least two\norders of magnitude higher error.\nours\nw/o HDQ\nw/o dworld\ncoarse\nw/o dcan\nfine\nabs(SDF(xs))\n0.00017\n0.20462\n0.06774\n0.00961\nAccuracy of the proposed Hierarchical Distance Query\nand sphere tracing algorithm.\nIn Table 4, we provide an\n12\nNormal\nOurs\nRelighting4D\nGround Truth\nAlbedo\nVisibility Rendering\nRelight\n    Visibility\n(Novel Light)\nNormal\nAlbedo\nVisibility Rendering\nRelight\n    Visibility\n(Novel Light)\nNormal\nAlbedo\nVisibility Rendering\nRelight\n    Visibility\n(Novel Light)\nRelighting4D*\nAniSDF\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nFigure 6. Qualitative Comparison of our method and baselines. We present qualitative comparisons between our method and various\nbaselines given only monocular input. The brightness difference between the ground truth and our method comes of the inherant scale\nambiguity of recovering the environment lighting and albedo at the same time. The normal and visibility MLPs of Relighting4D [17] did not\nsuccessfully capture the complex motion of the human avatar when only given monocular video as input thus producing overly smooth\nnormal map that looks like a single-colored image. Note that NeRFactor is only trained on 1 frame and all 10 input views.\nTable 5. Sensitivity study and runtime analysis on the cut-off\nvalue. The frame time and the rendering quality is roughly linear\nto the cut-off value up to a certain point, after which only dimin-\nishing returns can be observed by increasing the cut-off. However,\na too small value may result in incorrect surface intersection and\nvisibility estimation, leading to degraded quality. Thus we choose\nthe minimum cut-off value without a visible quality degredation\n( \u02dcTd = 0.1, \u02dcT vis\nd\n= 0.025) as the default one.\nRelighting\nVisibility\nFrame Time\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193\n\u02dcTd = 2.0, \u02dcT vis\nd\n= 0.5\n7.659\n21.11\n0.815\n0.200\n18.10\n0.795\n0.193\n\u02dcTd = 1.0, \u02dcT vis\nd\n= 0.25\n7.675\n21.11\n0.815\n0.200\n18.17\n0.796\n0.191\n\u02dcTd = 0.5, \u02dcT vis\nd\n= 0.125\n7.320\n21.10\n0.815\n0.199\n18.17\n0.797\n0.189\n\u02dcTd = 0.1, \u02dcT vis\nd\n= 0.025\n4.524\n21.08\n0.815\n0.197\n18.19\n0.798\n0.187\n\u02dcTd = 0.05, \u02dcT vis\nd\n= 0.0125\n2.631\n21.04\n0.814\n0.202\n17.83\n0.790\n0.197\n\u02dcTd = 0.01, \u02dcT vis\nd\n= 0.0025\n0.976\n19.50\n0.733\n0.300\n15.82\n0.697\n0.304\nadditional ablation study on the accuracy of the proposed\nHDQ-ST (Hierarchical Distance Query and sphere tracing)\nalgorithm. \u201cw/o HDQ\u201d variant uses volume rendering of\n128 samples per ray for surface intersection and \u201cw/o dcan\nfine\u201d\nand \u201cw/o dworld\ncoarse\u201d denote variant where sphere tracing is\nperformed without the fine distance and coarse distance of\nthe HDQ algorithm respectively. Our method achieves at\nleast two orders of magnitude higher accuracy at 0.00017m\ncompared to other variants.\n7. Additional Implementation Details\n7.1. Details of the Datasets\nMobileStage.\nTo evaluate our approach on real-world hu-\nmans, we collect a real-world multi-view dataset called Mo-\nbileStage. This dataset is composed of 4 dynamic human\n13\nCoarse (GS-KNN)\nCanonical Distance\nOurs (HDQ)\n0.00m\n0.50m\n0.25m\n0.05m\n0.10m\n0.15m\n0.20m\nFigure 7. Visualization of hierarchically queried distance. Hot\ncolors indicate large distance values. We visualize dworld\ncoarse, dcan\nfine\nand \u02dcdworld on (a) surface intersection points, which should be zero\neverywhere, and (b) bounding box of the human model, which\nshould reflect the real world space SDF. Note that the geometry is\nfixed for the comparison since we want to visualize the distance\nvalues on the correct geometry. The coarse distance dworld\ncoarse of\nnear surfaces points is not strictly zero (indicated by the light blue\nregions on the body) due to misalignment between SMPL-H [54]\nand the neural avatar, and dcan\nfine is incorrect in world space since it\nis defined by the canonical space network S. This leads to both of\nthem reporting errornous geometry for the relighting task as seen\nin Figure 9. In contrast to both, our proposed HDQ algorithm\nis well behaved both when near and far from the geometry.\n(Ours)\nFigure 8. Comparison of light visibility estimation algorithm.\nWe visualize the ambient occlusion result of different shadow algo-\nrithms by casting shadows from a dome lighting where the upper\nhalf of the environment map is set to a constant brightnerss. We\nuse Nst = 16 sphere tracing iterations to generate shadows on the\nground for \u201cw/o DFSS\u201d and \u201cw/ DFSS (ours)\u201d. The \u201cw/o DFSS\u201d\nvariant takes the binary light occlusion mask as the visibility term,\nresulting in unnatural hard shadow. \u201cw/ DFSS (ours)\u201d generates\nrealistic soft shadows by incorporating [50] onto neural SDF\nproduced by ours Hierarchical Distance Query.\nvideos collected using 36 synchronized mobile phone cam-\neras at 1920\u00d71080 resolution and 30 fps with complex in-\ndoor environment illuminations. We uniformly select 12\nviews for training in all experiments, the detailed frames se-\nlected are listed in Table 6. The characters perform complex\nmotions, including talking, walking, and swinging, resulting\nin complex shading and shadow effects. We calibrate the\ncamera poses of the multi-view system using COLMAP [58]\nOurs\n(GS-KNN)\n(HDQ)\nGround Truth\nFigure 9. Geometry quality of GS-KNN and HDQ. Performing\nsphere tracing using only the canonical distance dcan\nfine or coarse\nworld distance dworld\ncoarse (GS-KNN) results in incorrect surface inter-\nsection xs and soft visibility ps, while tracing with our proposed\nHierarchical Distance Query produces correct results.\nand extract foreground human mask with [44]. The human\nposes corresponding to each of the characters are estimated\nusing EasyMocap [1].\nSyntheticHuman++\nSyntheticHuman++ contains 5 se-\nquences of dynamic 3D human models with detailed shapes\nand complex materials. We use the original multi-view video\nin SyntheticHuman [52] for training, where 10 views of\nthe synthetic characters were rendered with obvious self-\nocclusion and shading variations using Blender Cycles [19].\nWe uniformly select 6 views from all 20 views for testing.\nFor the monocular setting, we select the fourth view for\ntraining.\nTo measure the quality of the visibility term, we addition-\nally render all sequences with uniform diffuse material where\nthe BRDFs in all directions are the same. This factors out\nthe material from the rendering algorithm. For evaluating the\nperformance of inverse rendering photometric properties, we\nalso render the ground truth albedo and normal. We do not\ncompare with the roughness term since Blender uses a differ-\nent material model than ours. For novel pose synthesis, we\nused the motion capture data from AIST++ [43] along with\nmotion capture data from the ZJU-MoCap Dataset [23,53]\n(sequence 377, 386, 390).\nTable 6. Specific training settings. Nview and Nframe represent\nthe number of training views and training frames respectively.\nN total\nview denotes the total number of views of the dataset and N total\nframe\ndenotes the total number of frames.\nDataset\nCharacter Nview/N total\nview\nNframe/N total\nframe\nMobileStage\ndark\n12 / 36\n1600 / 2000\npurple\n12 / 36\n600 / 700\nblack\n12 / 36\n300 / 400\nwhite\n12 / 36\n300 / 600\njody\n1 / 20\n100 / 100\nSynthHuman\njosh\n1 / 20\n100 / 100\n(monocular)\nmegan\n1 / 20\n100 / 100\nleonard\n1 / 20\n100 / 100\n14\nOurs\nw/o vis\nGround Truth\nFigure 10. Modeling visibility helps reconstructing diffuse\nalbedo. The \u201cw/o vis\u201d variant trained without computing light\nvisibility bakes shadows onto the diffuse albedo, while our ap-\nproach reconstructs high quality albedo.\n7.2. Implementation Details for GS-KNN\nSince vanilla KNN only produces unsigned distance to the\nclosest K points, the sign of the distance value is determined\nby the dot product between x \u2212 vk and the world space\nnormal nk corresponding to ik using\ndk = sign(dot(x \u2212 vk, nk))dk.\n(7)\nSince the world space query point might be close to multiple\nparts of the posed vertices of the world space parametric\nmodel, which will lead to incorrect inverse LBS results, we\nneed to avoid using KNN query results from vertices that\nare far away in the canonical space. To avoid assigning the\nquery point to multiple parts of the posed vertices V of the\nworld space parametric model, we further update the KNN\nresults using an approximation of the geodesic distance on\nthe parametric human model. Specifically, we take the index\nof the closest parametric model vertex imin, and compute\nthe approximated canonical Euclidian distance between all\nother K \u2212 1 selected vertices by querying their distance dV\nk\non the canonical parametric model Vcan (e.g. T-Pose) using\nik, dk, vk, nk, wk =\n(\nimin, dmin, vmin, nmin, wmin\n, if dV\nk < Td.\nik, dk, vk, nk, wk\n, otherwise.\n(8)\nwhere dmin, vmin, nmin, wmin are the vertex properties\ncorresponding to imin, and Td is the geodesic distance cutoff\nthreshold, which is set to 0.1 through all experiments.\n7.3. Implementation Details for Inverse LBS\nWith K sets of vertex properties produced by GS-KNN,\nthe world-space query point is transformed into canonical\nspace by the inverse LBS module. The human pose defines\nB body parts, which produces B transformation matrices\n{Gb} \u2208 SE(3). In the inverse LBS module, the world space\npoints are transformed to canonical space using\nT world(x) = (\nB\nX\nb=1\nwb(x)Gb)\u22121,\n(9)\nx\u2032 = T world(x)x,\n(10)\nwhere the blend weights wb of the bth body part is the bth\nelement of the blend weight vector w = (w1, ..., wB), which\nis computed from the KNN results DK = {d0..., dk} and\nWK = {w0..., wk} using DK = {d0..., dk} and WK =\n{w0..., wk}:\nw =\nK\nX\nk=1\nsoftmax( \u2212d0\n2R2w\n, ..., \u2212dK\n2R2w\n)kwk,\n(11)\nwhere softmax denotes the softmax normalization operator,\nand Rw is the blend weight blending radius, which is set\nto 0.075 through all experiments following the distance-\nweighted blending [80].\nx = x\u2032 + F\u2206x(\u0398f, x\u2032).\n(12)\nUsing Equation 10 and Equation 11, the world space\nquery point x is transformed to canonical space x\u2032.\nFor world space normal direction, we compute the normal-\nized gradient of the canonical deformed SDF and transform\nit back to world space using:\nns(xs) = (Rworld)\u22121(\u2207S(xcan\ns\n)),\n(13)\nwhere Rworld is the rotation component of T world the in-\nverse warping step of HDQ, and xcan\ns\n= T worldxs is the\ncorresponding canonical space coordinate of the surface in-\ntersection xs.\n7.4. Listing of the Surface Intersection Sphere Trac-\ning Algorithm\nIn Algorithm 1 we provide the detailed procedure for\nfinding surface intersections when performing sphere tracing\non the hierarchically queried distance. The sphere tracing\nprocedure on ray r(t) = o+td is performed with t bounded\nby near and far distances n and f. Additionally, we add\nan offset o to the queried distance when updating t to skip\nthrough gazing angles, and we linearly interpolate for the\nsurface intersection xs when the sign of the queried distance\nchanges. We empirically set Nst = 16 and o = 0.02 for\nall experiments. n and f are provided by intersecting the\ncamera ray with the axis-aligned bounding box of the posed\nparametric human model corresponding to the neural avatar.\n7.5. Listing of the Sphere Tracing Soft Visibility\nAlgorithm\nLight visibility is defined as whether a light ray from\nthe light source to the surface intersection point is occluded\n15\nInput: o, d, Nst, n, f\nOutput: xs\n1 t \u2190 n\n// Start from near plane\n2 ts \u2190 f\n// Surface intersection depth\n3 d0 \u2190 inf\n// Previous closest distance\n4 d1 \u2190 inf\n// Current closest distance\n5 dc \u2190 inf\n// All time closest distance\n6 dt \u2190 inf\n// Update of t during iteration\n7 for i \u2190 0 to Nst do\n// Hierarchical distance query\n8\nd0, d1 \u2190 d1, \u02dcdworld(o + td)\n9\ndabs\n0\n, dabs\n1\n\u2190 abs(d0), abs(d1)\n// Update closest distance and\nintersection\n10\nif dabs\n1\n< dc then\n11\ndc \u2190 dabs\n1\n12\nts \u2190 t\n13\nend\n// Linear interpolation upon sign\nchange of SDF\n14\nif sign(d0) \u0338= sign(d1) then\n15\nts \u2190 t \u2212 dt\ndabs\n1\ndabs\n0\n+dabs\n1\n16\nend\n// Prepare for next iteration\n17\ndt \u2190 d + o\n18\nt \u2190 t + dt\n// Constrain t to be within near and\nfar plane\n19\nt \u2190 min(t, f)\n20\nt \u2190 max(t, n)\n21 end\n22 return o + tsd\nAlgorithm 1: Surface intersection sphere tracing.\nby other parts of the geometry. This occlusion test can be\nachieved by performing the same Sphere Tracing algorithm\nfor surface intersection and checking whether the returned\nintersection point xs lies on the far plane f.\nHowever, such a simple binary test can only produce hard\nshadow for a point light source [16], while perfect point\nlight rarely exists and the more common area lights should\nproduce soft shadows [78]. If we simply perform dense inte-\ngration or monte-carlo integration to compute the soft light\nvisibility values, the computational cost will be too high. To\nthis end, we propose to integrate traditional distance field\nsoft shadow algorithms [3,7,50] to our hierarchically queried\ndriable neural distance field, which interpret the global mean-\ning of the distance values as the penumbra coefficient for\nsoft shadow effects with a small number of \u03c9i samples.\nAlgorithm 2 illustrates the soft visibility computation\nalgorithm, in which o is set to the surface intersection xs, d\nis set to the light direction \u03c9i, n is set to 0.01, f is set to 10.0\nand N vis\nst is set to 4 in all experiments. The ratio between the\ncurrent distance d1 and current ray depth t along with the\nInput: o, d, N vis\nst , n, f, a\nOutput: p\n1 t \u2190 n\n// Start from near plane\n2 d0 \u2190 inf\n// Previous closest distance\n3 d1 \u2190 inf\n// Current closest distance\n4 dt \u2190 inf\n// Update of t during iteration\n5 ps \u2190 1\n// Soft shadow penumbra\ncoefficient\n6 Rs \u2190 p a\n\u03c0\n// Per solid angle a = \u03c0R2\ns\n7 for i \u2190 0 to N vis\nst\ndo\n// Hierarchical distance query\n8\nd0, d1 \u2190 d1, \u02dcdworld(o + td)\n// Compute penumbra coefficient\n9\nps \u2190 min(ps, max(d1,0)\n2tRs\n)\n// Prepare for next iteration\n10\ndt \u2190 d + o\n11\nt \u2190 t + dt\n// Constrain t to be within near and\nfar plane\n12\nt \u2190 min(t, f)\n13\nt \u2190 max(t, n)\n14 end\n15 return ps\nAlgorithm 2: Soft light visibility sphere tracing.\narea a (in solid angle) of the light source forms ps =\nd1\n2t\u221a a\n\u03c0\nfollowing [7,50].\n7.6. Details on Network Queries of the Canonical\nMaterial Fields\nTo apply more supervision on the rendering process and\nthe canonical material MLPs, we construct a sparse set of\nvolume sampling Ts along the camera ray r(t) = o + td\nnear the computed surface intersection point xs = o + tsd\nwith a fixed step size tstep = 0.005 and number of samples\nNs = 3 using\nTs = ts + {2 ns\nNs\ntstep, ..., 2Ns\nNs\ntstep} \u2212 tstep.\n(14)\nThen, Ts is used to sample the material networks, on which\nthe Volume Rendering algorithm in [52, 73] is applied to\ncompute the final albedo \u03b1s(xs) and roughness \u03b3s(xs) cor-\nresponding to xs.\n7.7. Network Structures\nFor the canonical space geometry and displacement field,\nwe use 8 layer MLPs of width 256 for S and F\u03b4x with ReLU\nand Softplus activation respectively. S takes positionally\nencoded (PE) [48] coordinate of resolution 8 as input and\nF\u03b4x takes point input with 10 levels of PE along with the\npose of the current human frame. We follow [52] to initialize\nS with [25]. For canonical material networks, we use 8-layer\nMLPs for A and \u0393 with ReLU activation of points input with\n16\nNeural Signed Distance Field\nLinear layer + Softplus\nLinear layer + Sigmoid\nLinear layer\nLinear layer + Tanh\nInput concatenation\n256\n256\n256\n256\n256\nGeometry\nFeature\nAlbedo & Roughness\n128\n256\nCanonical Displacement\n256\n256\n256\n256\n256\nPositional\nEncoded\nDeformed\nPoints\nPositional\nEncoded\nCanonical\nPoints\nSMPL\nPose\nFigure 11. Detailed network structure.\n10 levels of PE. We additionally input the current human\npose to increase its representational power. An illustration\nof the network structure is shown in Figure 11.\n7.8. Loss Functions\nWe optimize our relightable and animatable neural human\navatar by rendering the image with given camera poses and\ncompare the pixel values Lo, which corresponds to ray r,\nwhere xs = o + tsd and \u03c9o = d, against the ground truth\nones Lgt. The loss function is defined as\nLd =\nX\nr\u2208R\n\u2225Lo(r) \u2212 Lgt(r)\u22252.\n(15)\nwhere R denotes all camera rays in the forward rendering\nprocess.\nFollowing [52], we additionally regularize the residual\ndisplacement field F\u2206 and canonical SDF S with the Eikonal\nterm using\nLeik =\nX\nx\u2208X\n\u03bbe0(\u2225\u2206xS(x + \u2206F(x))\u22252 \u2212 1)+\n\u03bbe1(\u2225\u2206x+\u2206F (x)S(x + \u2206F(x))\u22252 \u2212 1)\n(16)\nwhere X denotes all the warped canonical points meeting the\ncutoff criteria dworld\ncoarse <= \u02dcTd during the forward rendering\nprocess.\nTheir [52] regularization on the magnitude of the residual\ndisplacement field F\u2206 is also adopted in our work, which is\ndefined as\nLr =\nX\nx\u2208X\n\u2225F\u2206(x)\u22252.\n(17)\nWe also define a mean intersection of union foreground\nmask loss on the silhouette Mo(r) of the rendered image to\nregularize the training process as\nLm =\nX\nr\u2208R\nMo(r)Mgt(r)\nMo(r) + Mgt(r).\n(18)\nTo alleviate ambiguities in the physically-based render-\ning model, we follow [17] to add sparsity and smoothness\nregularizations on our material representation, the albedo\nMLP \u03b1 = A(x) and the roughness MLP \u03b3 = \u0393(x). The\nregularization terms are defined as\nLent = gaussian entropy(\u03b1(X))\n(19)\nLa =\nX\nx\u2208X\n\u2225\u03b1(x) \u2212 \u03b1(x + \u2206x)\u22252\n(20)\nLr =\nX\nx\u2208X\n\u2225\u03b3(x) \u2212 \u03b3(x + \u2206x)\u22252\n(21)\nwhere gaussian entropy(\u03b1(X)) is the entropy of the Gaus-\nsian distribution of the albedo map \u03b1 evaluated on the canon-\nical points X as in [17] and \u2206x is a random perturbation\nsampled from a normal distribution \u2206x \u223c N(\u00b5, \u03c32) with\n\u00b5 = 0 and \u03c3 = 0.02 in all experiments.\n7.9. Training\nTo make the optimization process more controllable, we\nseparate the training process into two stages by postponing\nthe training of the material and environment light probe after\nthe geometry of the neural avatar has converged.\nWe first train the base geometry of the neural avatar in-\ncluding S, \u2206F and a neural rendering network C(x, d\u2032) as\nin [52] with the same volume rendering algorithm defined\nin [52]. The rendering network C(x, d) [52] takes the canon-\nical query point x obtained in Equation 12 and the canonical\nview direction d\u2032 = Rworldd as input where Rworld is the\nrotation component of the world space transformation ma-\ntrix T world in Equation 9. The networks are trained using\nLd, Leik and Lr for 200k iterations with and Adam opti-\nmizer [39] of learning rate 5e\u22124 exponentially annealed to\n5e\u22126 during the first stage training. The full loss function of\nthe first stage training is defined as\nL1st = \u03bbdLd + \u03bbeikLeik + \u03bbrLr + \u03bbmLm\n(22)\nwhere \u03bbd, \u03bbeik, \u03bbr, \u03bbm are set to 1.0, 0.025, 0.1 and 0.01\nrespectively. And the loss weights in the eikonal loss Leik\ndefinition \u03bbe0 and \u03bbe1 are set to 1.0 and 2.0 respectively.\nDuring the second stage of the training process, we re-\nplace the volume rendering algorithm of [52] with Sphere\nTracing [29] defined on the neural SDF of the human avatar,\n17\nand the rendering network C is replaced with the physically-\nbased renderer. Moreover, we replace the SMPL model from\nthe first stage with the learned coarse mesh during the second-\nstage optimization. Additional losses Lent, La, Lr are added\nfor regularization with the full loss defined as\nL2nd = L1st + \u03bbentLent + \u03bbaLa + \u03bbrLr.\n(23)\nThe second stage optimization is performed with an Adam\noptimizer [39] of learning rate 5e\u22124 exponentially annealed\nto 5e\u22126 for 25k iterations. Additionally, the starting learning\nrate on parameters of S and \u2206F are tuned down to 1e \u2212 5\nif not otherwise specified to preserve the geometry during\nthe first few iterations of the second stage. Loss weights \u03bbd,\n\u03bbeik, \u03bbr, \u03bbm, \u03bbent, \u03bba and \u03bbr are set to 10.0, 0.1, 0.1, 0.01,\n5e\u22124, 5e\u22123 and 5e\u22123 respectively, where \u03bbe0 and \u03bbe1 are\nall set to 1.0.\nThe first and second stages are all trained with a batch size\nof 8 with 1024 rays per batch. On two NVIDIA RTX 3090\nGPUs, the first stage training (200k iterations) takes about\n20 hours and the second stage (25k iterations) takes about 10\nhours. The forward rendering process of the second stage is\nadopted for inference when the neural avatar is driven with\nnovel human poses or relit with novel lighting conditions\nthrough all experiments.\n8. Additional Discussion\n8.1. Discussion on Alternatives to HDQ\nNLST [59] also proposes a method on performing sphere\ntracing on deformed signed distance fields with undeformed-\nspace distance query and integration of an ODE solver. How-\never, their method is not directly applicable to animatable\nneural avatars since NLST requires the computation of the\nJacobian of the inverse deformation field, which is unavail-\nable in the neural deformation field used in our method. One\ncould try to replace the single-direction displacement field\nwith a bi-directional one like in [13], which might reduce\nthe generalizability to novel human poses. But computing\nthe Jacobian of a neural deformation field requires taking\nthe backward of a large computation graph multiple times,\nwhich is time-consuming. Combined with the fact that NLST\nrequires multiple subsets in one single query step, it would\nnot be practical to naively combine the two. In comparison,\nour method does not require computing the Jacobian and di-\nrectly returns a single well-approximated distance value for\nan articulated and neurally deformed distance field thanks\nto the adoption of the observation space Eikonal loss, which\nessentially also makes the neurally deformed field a valid\ndistance field.\n[61] could also be considered a viable approach for sur-\nface intersection and distance queries if the target of the\nquery is a static neural implicit field. However, its core tech-\nnique Range Analysis not directly applicable to the animated\ngeometry of our reconstructed neural avatar, which requires\ncomputing the closest K points on a parametric model and\nperforming space warping through blended skinning meth-\nods.\n8.2. Difference from AniSDF\nSimilar to [52], we define the animatable geometry of\nthe human avatar as the combination of a pose-driven de-\nformation field F(x) and a canonical SDF network S(x),\nwhere the deformation field can be further decomposed into\na pose-dependent displacement field F\u2206x and KNN-based\ninverse Linear Blend Skinning (LBS) [41] module. We fol-\nlow their canonical and deformation field setup to model the\nanimatable human geometry. However, one key difference\nis that we focus on producing the correct world-space dis-\ntance values, instead of simply warping query points back to\ncanonical spaces. We intend to use the SDF values in a fixed\nstep Sphere Tracing [30] framework where correct world\nspace distance values are expected for convergence.\nThe key observation that makes the Hierarchical Distance\nQuery scheme possible is that SDF values are locally defor-\nmation invariant under the inverse Linear Blend Skinning\n(LBS) [41] algorithm. The closer the query point is to the ge-\nometry, the less the value changes when transforming under\nnovel human poses, where the distance value of a surface\npoint will always be zero no matter the pose. This makes it\npossible to use the canonical SDF network S(x) for near-\nsurface distance queries, where the accuracy and detail of the\ndistance are well-preserved, and then use the coarse KNN\ndistance values for far-surface distance queries.\nSpecifically, due to the linearity of the inverse LBS al-\ngorithm, close-to-surface points are rarely mapped to the\nwrong canonical location since they exhibit very little ambi-\nguity, while it is quite likely for far-from-surface points to\nfall into the region of confusion during the inverse wrapping\nprocess. Thus dcan\nfine is rarely inaccurate for close-to-surface\nquery points, while it is more likely to be inaccurate for far-\nfrom-surface ones. Utilizing this property of the SDF value,\nwe can interpret the fine canonical distance value dcan\nfine as\na good approximation for the actual world-space fine level\ndistance dworld when the query points are close to the neural\navatar, while the coarse world space distance dworld\ncoarse could\nalso serve as a good approximation for the world-space dis-\ntance dworld because the parametric human model is aligned\nto the neural avatar\u2019s geometry.\n18\nTable 7. Addtional quantitative comparison on all characters of SyntheticHuman++. We report the result of NeRFactor and NeRFactor*\non the first frame of each sequence. NeRFactor [78] and NeRFactor* failed to converge on the first frame of megan and leonard.\nNormal\nDiffuse Albedo\nRelighting\nVisibility\nCharacter Method\nDegree \u2193 PSNR\u2191\nSSIM\u2191\nLPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193\nPSNR\u2191\nSSIM\u2191 LPIPS\u2193\njody\nOurs\n10.98\n25.88\n0.925\n0.144\n21.57\n0.853\n0.168\n20.53\n0.869\n0.142\nRelighting4D*\n27.65\n16.40\n0.827\n0.229\n20.77\n0.826\n0.205\n15.47\n0.785\n0.241\nRelighting4D\n76.73\n16.40\n0.827\n0.229\n17.25\n0.752\n0.249\n9.51\n0.604\n0.313\nNeRFactor*\n33.19\n17.74\n0.860\n0.201\n21.66\n0.867\n0.224\n16.12\n0.752\n0.283\nNeRFactor\n79.26\n17.74\n0.860\n0.201\n20.31\n0.862\n0.181\n11.78\n0.755\n0.227\nAniSDF\n13.61\n16.02\n0.815\n0.230\n19.63\n0.818\n0.223\n-\n-\n-\njosh\nOurs\n12.88\n34.46\n0.968\n0.072\n24.72\n0.897\n0.206\n20.04\n0.858\n0.149\nRelighting4D*\n26.56\n29.32\n0.917\n0.153\n24.48\n0.881\n0.229\n16.02\n0.790\n0.233\nRelighting4D\n101.0\n29.31\n0.917\n0.153\n23.86\n0.830\n0.273\n3.25\n0.554\n0.363\nNeRFactor*\n35.39\n29.14\n0.926\n0.167\n24.68\n0.895\n0.228\n14.85\n0.729\n0.291\nNeRFactor\n24.57\n29.14\n0.926\n0.167\n24.65\n0.896\n0.217\n15.96\n0.791\n0.245\nAniSDF\n16.34\n30.28\n0.924\n0.164\n20.76\n0.843\n0.260\n-\n-\n-\nmegan\nOurs\n13.09\n22.67\n0.873\n0.198\n18.55\n0.790\n0.234\n19.69\n0.814\n0.167\nRelighting4D*\n31.98\n22.91\n0.857\n0.223\n17.25\n0.742\n0.273\n14.83\n0.713\n0.278\nRelighting4D\n98.05\n22.93\n0.857\n0.222\n17.26\n0.671\n0.307\n5.47\n0.364\n0.433\nNeRFactor*\n-\n15.11\n0.569\n0.394\n14.06\n0.490\n0.472\n7.41\n0.341\n0.538\nNeRFactor\n-\n15.11\n0.569\n0.394\n14.06\n0.490\n0.472\n7.41\n0.341\n0.538\nAniSDF\n15.18\n16.91\n0.788\n0.258\n11.51\n0.686\n0.316\n-\n-\n-\nleonard\nOurs\n12.81\n33.04\n0.968\n0.064\n25.92\n0.903\n0.218\n20.53\n0.852\n0.160\nRelighting4D*\n31.33\n30.18\n0.938\n0.129\n26.03\n0.891\n0.242\n14.57\n0.766\n0.255\nRelighting4D\n99.54\n30.18\n0.938\n0.130\n25.14\n0.845\n0.273\n3.24\n0.533\n0.388\nNeRFactor*\n-\n26.93\n0.912\n0.143\n23.77\n0.779\n0.326\n7.09\n0.500\n0.435\nNeRFactor\n-\n26.93\n0.912\n0.143\n23.77\n0.779\n0.326\n7.09\n0.500\n0.435\nAniSDF\n13.75\n25.31\n0.920\n0.155\n18.30\n0.849\n0.246\n-\n-\n-\nTable 8. Addtional quantitative comparison on all characters of SyntheticHuman++ under the monocular setting.\nNormal\nDiffuse Albedo\nRelighting\nVisibility\nCharacter\nMethod\nDegree \u2193 PSNR\u2191\nSSIM\u2191\nLPIPS\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193\nPSNR\u2191\nSSIM\u2191 LPIPS\u2193\njody\nOurs\n16.83\n24.09\n0.877\n0.180\n21.08\n0.815\n0.197\n18.19\n0.798\n0.187\nRelighting4D*\n25.16\n19.57\n0.799\n0.261\n20.79\n0.766\n0.245\n17.31\n0.736\n0.273\nRelighting4D\n80.80\n19.57\n0.799\n0.262\n20.28\n0.778\n0.233\n16.16\n0.753\n0.278\nAniSDF\n18.65\n16.25\n0.752\n0.284\n18.65\n0.746\n0.274\n-\n-\n-\njosh\nOurs\n20.09\n26.67\n0.905\n0.142\n24.52\n0.873\n0.224\n17.46\n0.761\n0.222\nRelighting4D*\n24.93\n29.61\n0.919\n0.165\n23.95\n0.863\n0.240\n17.29\n0.737\n0.268\nRelighting4D\n81.23\n29.61\n0.919\n0.165\n24.06\n0.861\n0.269\n15.88\n0.750\n0.297\nAniSDF\n21.91\n26.54\n0.879\n0.226\n19.88\n0.790\n0.294\n-\n-\n-\nmegan\nOurs\n19.64\n18.63\n0.796\n0.262\n18.33\n0.742\n0.246\n17.70\n0.707\n0.222\nRelighting4D*\n29.51\n22.38\n0.798\n0.286\n17.21\n0.696\n0.285\n16.29\n0.640\n0.316\nRelighting4D\n83.40\n22.38\n0.798\n0.286\n17.59\n0.705\n0.299\n16.30\n0.648\n0.354\nAniSDF\n19.80\n26.29\n0.905\n0.179\n20.77\n0.825\n0.274\n-\n-\n-\nleonard\nOurs\n18.30\n24.28\n0.912\n0.121\n25.88\n0.893\n0.231\n18.45\n0.779\n0.215\nRelighting4D*\n25.08\n29.90\n0.940\n0.130\n25.28\n0.884\n0.245\n17.50\n0.726\n0.286\nRelighting4D\n81.55\n29.90\n0.940\n0.130\n25.48\n0.879\n0.270\n16.40\n0.753\n0.280\nAniSDF\n21.08\n16.98\n0.710\n0.332\n13.85\n0.619\n0.346\n-\n-\n-\n19\nOurs\nOurs\nmonocular\nOurs\nmonocular\nAS\nR4D\nOurs\nAS\nR4D\nFigure 12. More novel pose relighting results on the SyntheticHuman dataset. \u201cAS\u201d denotes AniSDF [52]. \u201cR4D\u201d denotes Relighting4D\n[17]. All compared relighting methods compute shadows on the ground plane. The visibility MLP of [17] failed to generalize to far-from-\nhuman points.\n20\nOurs\nR4D\nR4D\nOurs\nFigure 13. More novel pose relighting results on the MobileStage dataset. \u201cAS\u201d denotes AniSDF [52]. \u201cR4D\u201d denotes Relighting4D [17].\nAll compared relighting methods compute shadows on the ground plane. The visibility MLP of [17] failed to generalize to far-from-human\npoints.\n21\n"
  },
  {
    "title": "Learning to Identify Critical States for Reinforcement Learning from Videos",
    "link": "https://arxiv.org/pdf/2308.07795.pdf",
    "upvote": "6",
    "text": "Learning to Identify Critical States for Reinforcement Learning from Videos\nHaozhe Liu1\u2020, Mingchen Zhuge1\u2020, Bing Li1B, Yuhui Wang1, Francesco Faccio1,2\nBernard Ghanem1, J\u00a8urgen Schmidhuber1,2,3\n1AI Initiative, King Abdullah University of Science and Technology\n2The Swiss AI Lab IDSIA/USI/SUPSI, 3NNAISENSE\n{haozhe.liu, mingchen.zhuge, bing.li, yuhui.wang,\nfrancesco.faccio, bernard.ghanem, juergen.schmidhuber}@kaust.edu.sa\nAbstract\nRecent work on deep reinforcement learning (DRL) has\npointed out that algorithmic information about good poli-\ncies can be extracted from offline data which lack explicit\ninformation about executed actions [50, 51, 35]. For exam-\nple, videos of humans or robots may convey a lot of im-\nplicit information about rewarding action sequences, but\na DRL machine that wants to profit from watching such\nvideos must first learn by itself to identify and recognize\nrelevant states/actions/rewards. Without relying on ground-\ntruth annotations, our new method called Deep State Iden-\ntifier learns to predict returns from episodes encoded as\nvideos. Then it uses a kind of mask-based sensitivity analy-\nsis to extract/identify important critical states. Extensive ex-\nperiments showcase our method\u2019s potential for understand-\ning and improving agent behavior. The source code and the\ngenerated datasets are available at Github.\n1. Introduction\nIn deep reinforcement learning (DRL), the cumulative\nreward\u2014also known as the return\u2014of an episode is ob-\ntained through a long sequence of dynamic interactions be-\ntween an agent (i.e., a decision-maker) and its environment.\nIn such a setting, the rewards may be sparse and delayed,\nand it is often unclear which decision points were critical to\nachieve a specific return.\nSeveral existing methods use the notion of localizing\ncritical states, such as EDGE [21] and RUDDER [1]. These\nmethods typically require explicit action information or pol-\nicy parameters to localize critical states. This limits their\npotential applicability in settings like video-based offline\nRL, where an agent\u2019s actions are often hard to measure, an-\n\u2020 Equal Contribution.\nB Corresponding Author.\nAccepted to ICCV23.\nFigure 1. Motivation of the proposed method. In the illustrated\nrace between a turtle and a rabbit, the sleep state is critical in deter-\nmining the winner of the race. Our method is proposed to identify\nsuch critical states.\nnotate, or estimate [76, 37]. To avoid this pitfall, in this\nwork, we explicitly study the relationship between sequen-\ntial visual observations and episodic returns without access-\ning explicit action information.\nInspired by the existing evidence that frequently only a\nfew decision points are important in determining the return\nof an episode [1, 13], and as shown in Fig. 1, we focus\non identifying the state underlying these critical decision\npoints. However, the problem of directly inferring critical\nvisual input based on the return is nontrivial [13], and com-\npounded by our lack of explicit access to actions or policies\nduring inference. To overcome these problems\u2014inspired\nby the success of data-driven approaches [72, 44, 27]\u2014our\nmethod learns to infer critical states from historical visual\ntrajectories of agents.\nWe propose a novel framework, namely the Deep State\nIdentifier, to identify critical states in video-based environ-\nments. A principal challenge of working in such settings\nlies in acquiring ground-truth annotations of critical states;\nit is laborious to manually label in videos critical states cor-\narXiv:2308.07795v1  [cs.CV]  15 Aug 2023\nresponding to complex spatio-temporal patterns. The Deep\nState Identifier is designed to directly overcome this chal-\nlenge by identifying the critical states based solely on visual\ninputs and rewards. Our proposed architecture comprises a\nreturn predictor and a critical state detector. The former pre-\ndicts the return of an agent given a visual trajectory, while\nthe latter learns a soft mask over the visual trajectory where\nthe non-masked frames are sufficient for accurately predict-\ning the return. Our training technique explicitly minimizes\nthe number of critical states to avoid redundant information\nthrough a novel loss function. If the predictor can achieve\nthe same performance using a small set of frames, we con-\nsider those frames critical. Using a soft mask, we obtain\na rank that indicates the importance of states in a trajec-\ntory, allowing for the selection of critical states with high\nscores. During inference, critical states can be directly de-\ntected without relying on the existence of a return predictor.\nOur contributions can be summarized as follows:\n\u2022 We propose a novel framework that effectively iden-\ntifies critical states for reinforcement learning from\nvideos, despite the lack of explicit action information.\n\u2022 We propose new loss functions that effectively enforce\ncompact sets of identified critical states.\n\u2022 We demonstrate the utility of the learned critical states\nfor policy improvement and comparing policies.\n2. Related Work\nIn the past decade, researchers have explored the po-\ntential of combining computer vision (CV) and RL to de-\nvelop more intelligent agents. A pioneering study by Kout-\nnik et al. [32] used recurrent neural networks to tackle\nvision-based RL problems through an evolutionary strat-\negy [33]. Since then, this topic has gained popularity. Mnih\net al. [40, 41] trained a deep neural network using raw\npixel data from Atari games to learn the Q-function for RL\nagents. Recently, Visual MPC [15] proposed a method us-\ning deep convolutional neural networks to predict the future\nstates of a robot\u2019s environment based on its current visual\ninput. RIG [43] trains agents to achieve imagined goals in a\nvisual environment using a combination of RL and an aux-\niliary visual network. Ha and Schmidhuber [22] propose a\nversion of the world model, which employs a Variational\nAutoencoder (VAE) [31] to construct representations of the\nvisual environment and help train a model using imagined\nfuture states. Robotprediction [14] designs a method for un-\nsupervised learning of physical interactions through video\nprediction, achieved by an adversarial model that assists RL\nagents in learning to interact with the environment. More\nrecently, researchers have explored novel CV advances,\nsuch as self-attention and self-supervised learning, applied\nto RL algorithms [28, 7, 73, 18, 10], leading to satisfactory\nimprovements. While visual input is integral to RL agents\nand can benefit RL in numerous ways, our paper proposes a\nmethod to assist agents in identifying the most crucial visual\ninformation for decision-making rather than solely focusing\non improving visual representation.\nOur method offers a novel perspective on explainable\nRL by identifying a small set of crucial states. Explain-\ning the decision-making process in RL is more challeng-\ning than in CV, due to its reliance on sequential interac-\ntions and temporal dependencies. Various methods have\nbeen employed to address this challenge. Recent attention-\nbased approaches [28, 7, 42] focus on modeling large-scale\nepisodes offline [28, 7] to localize crucial decision-making\npoints [42]. However, the attention structure typically op-\nerates on feature space, where the spatial correspondence\nis not aligned with the input space [5, 21]. Therefore, it is\nchallenging to directly threshold attention values to identify\ncritical temporal points. Post-training explanation is an ef-\nficient method that directly derives the explanation from an\nagent\u2019s policy or value network [38, 20, 19, 16], thereby\nreducing memory and computation costs. Other popular\nexplainable DRL methods include self-interpretable meth-\nods, such as Relational-Control Agent [74] and Alex [42],\nand model approximation methods, such as VIPER [4] and\nPIRL [67]. These methods are widely used in the field of\nDRL [38, 20, 19, 16, 74, 42, 4, 67]. For example, Alex [42]\nproposes using the output of the attention mechanism to en-\nable direct observation of the information used by the agent\nto choose its action, making this model easier to interpret\nthan traditional models. Tang et al. [64] use a small frac-\ntion of the available visual input and demonstrate that their\npolicies are directly interpretable in pixel space. The PIRL\nmethod [67] produces interpretable and verifiable policies\nusing a high-level, domain-specific language. Recent work\nuses policy fingerprinting [24] to build a single value func-\ntion to evaluate multiple DRL policies [13, 12, 11]. The au-\nthors use only the policy parameters and the return to iden-\ntify critical abstract states for predicting the return. How-\never, policy parameters are often unavailable in practical\napplications, and storing them for multiple policies can re-\nquire significant memory resources. We circumvent this is-\nsue by using visual states observed from the environment\nrather than relying on policy parameters.\nApart from the methods mentioned above, reward de-\ncomposition is also popular.\nSuch methods [56, 29] re-\nengineer the agent\u2019s reward function to make the rewards\nearned at each time step more meaningful and understand-\nable. Compared to these methods, our approach evaluates\nthe specific states. It provides a context-based framework\nfor long-horizon trajectories in a challenging, yet practical\ndomain, specifically learning without actions. Our method\nis also related to the concept of Hierarchical RL [71, 63],\nwhich aims to identify high-level subgoals [53, 47] that a\nlow-level policy should achieve. Using a few crucial states\nto explain an RL agent is closely connected to the concept\nof history compression [46, 48], where a neural network is\ntrained to learn compact representations that are useful for\nmodeling longer data sequences.\n3. Method\n3.1. Problem Formulation\nIn Reinforcement Learning (RL) [62], an agent inter-\nacts sequentially with an environment. At each time step\nt, the agent observes a state s(t)\u2014in our case, the frame of\na video, chooses an action a(t), obtains a scalar immediate\nreward r(t) = R(s(t), a(t)), where R is the reward func-\ntion, and transitions to a new state s(t+1) with probability\nP(s(t+1)|s(t), a(t)).\nThe behavior of an agent is expressed by its policy\n\u03c0(a|s), which defines a probability distribution over actions\ngiven a state. The agent starts from an initial state and inter-\nacts with the environment until it reaches a specific state (a\ngoal state or a failing state) or hits a time horizon T. Each\nof these interactions generates an episode and a return, i.e.,\nthe discounted cumulative reward y = PT\nt=0 \u03b3tr(t), where\n\u03b3 \u2208 [0, 1) is a discount factor. Due to the general form of\nthe return and the complex agent-environment interaction,\nit is generally difficult to identify which decision points\u2014\nor states\u2014are essential to achieve a specific return in an\nepisode. In other words, it is difficult to explain the behav-\nior of a policy.\nInspired by the success of data-driven approaches [72,\n44, 27, 77], we design a learning-based method to identify\na few crucial states in an episode that are critical to achiev-\ning the return y. Unlike previous approaches [1, 21], we fo-\ncus on identifying critical states in a video without needing\nan explicit representation of the policy or actions executed.\nMore formally, let {si, yi}i be the collected episode-return\ntraining data, where si = {s(t)\ni }t is the i-th state trajectory,\ns(t)\ni\nis a state at the time step t, and yi is the return achieved\nin the state trajectory si.\nTo identify critical states, we suggest a novel framework,\ncalled the Deep State Identifier, consisting of the following\ntwo steps. First, we propose a return predictor that esti-\nmates the return yi given a state trajectory si. Second, we\nuse the return predictor to train a critical state detector to\nidentify critical states. The detector receives the states as in-\nput and outputs a mask over the states. It is used to measure\nhow important each state is to the return. Fig. 2 illustrates\nthe architecture of our method.\n3.2. Return Predictor\nOur return predictor G(\u00b7) aims to predict the return of\na sequence of states. We build it using a neural network\nand train it in a supervised manner. There are two types\nFigure 2. Illustration of the proposed framework. During train-\ning, our return predictor learns to predict the return of an episode\nfrom a state trajectory. Our critical state detector learns to exploit\nthe return predictor to identify a compact set of states critical for\nreturn prediction. During testing, the critical state detector takes a\nstate trajectory as input and automatically detects its critical states\nwithout using the return predictor.\nof learning objectives depending on whether the return is\ndiscrete or continuous. For discrete return values (e.g., 1\nindicates success, while 0 denotes failure), we train G(\u00b7)\nusing cross-entropy loss:\nLc\nG =\nX\ni\nLc\nG(si, yi) = \u2212\nX\ni\nyilogG(si),\n(1)\nwhere yi is the category-level annotation of si. If the return\nis continuous, we employ a regression loss Lr\nG to train G(\u00b7),\nLr\nG =\nX\ni\nLr\nG(si, yi) =\nX\ni\n||G(si) \u2212 yi||2,\n(2)\nwhere yi \u2208 R is the scalar return of state trajectory si.\n3.3. Critical State Detector\nIn a general environment, manually labeling critical\nstates is expensive and impractical. The unavailability of\nground-truth critical states prevents our method from be-\ning fully-supervised. We hereby propose a novel way of\nleveraging the return predictor for training a critical state\ndetector. Note that the critical states are elements of the\nstate trajectory and can be discontinuous along the tempo-\nral dimension. We cast the task of identifying critical states\nas deriving a soft mask on a state trajectory. In particular,\ngiven a state trajectory si = {s(t)\ni }, the critical state de-\ntector D outputs a mask on si, i.e., mi = D(si), where\nmi = {m(t)\ni }, m(t)\ni\n\u2208 [0 1] can be interpreted as confi-\ndence that s(t)\ni\nis a critical state. Intuitively, a high value of\nm(t)\ni\nindicates a higher probability that the corresponding\nstate s(t)\ni\nis critical. To enforce D to identify critical states,\nwe design three loss functions, namely, importance preser-\nvation loss, compactness loss, and reverse loss, for training\nD:\nLD = \u03bbsLimp\nD\n+ \u03bbrLcom\nD\n+ \u03bbvLrev\nD ,\n(3)\nwhere \u03bbs, \u03bbr and \u03bbv are the weights for importance preser-\nvation loss, compactness loss, and reverse loss respectively.\nImportance preservation loss. Given a state trajectory si,\nthe goal of the importance preservation loss is to ensure the\nstates discovered by the critical state detector are important\nto predict the return yi. Hence, the loss enforces the masked\nstate sequence discovered by D to contain a similar predic-\ntive information of the original state trajectory si. Given the\ntraining data {(si, yi)}, the importance preservation loss is\ndefined as follows:\nLimp\nD\n=\nX\ni\nLG(G(si \u25e6 D(si)), yi),\n(4)\nwhere \u25e6 denotes the element-wise multiplication (si \u25e6\nD(si))(t) \u225c m(t)\ni s(t)\ni\n, G(si \u25e6 D(si)) predicts the return of\nthe masked state sequence si \u25e6 D(si), LG stands for Lc\nG or\nLr\nG, as defined in the previous subsection. Note that the\nmasked state sequence can be discontinuous, and the in-\nformation is dropped by skipping some redundant states.\nAs a result, we cannot obtain a ground-truth return for a\nmasked state sequence by running an agent in its environ-\nment. Thanks to the generalization abilities of neural net-\nworks [75, 68, 52, 49], we expect that the return predictor\ntrained on the original state trajectories can predict well the\nreturn for masked state trajectories when critical states are\nnot masked.\nCompactness loss. Solely using the importance preserva-\ntion loss Limp\nG\nleads to a trivial solution where the mask\nidentifies all states in si as critical. Critical states should\ninstead be as compact as possible to avoid involving redun-\ndant and irrelevant states. To address this issue, we fur-\nther introduce the compactness loss Lcom\nD\n. The compact-\nness loss forces the discovered critical state to be as few as\npossible. Specifically, we employ the L1-norm to encour-\nage the mask, i.e., the output of D, to be sparse given each\nsi :\nLcom\nD\n=\nX\ni\n||D(si)||1.\n(5)\nIt is difficult to balance the importance preservation loss and\ncompactness loss. The detector may ignore some critical\nstates for compactness. We propose a reverse loss for train-\ning D to mitigate this problem.\nReverse loss.\nThe third loss is designed for undetected\nstates. We remove the critical states by inverting the mask\nfrom the original state trajectory si \u25e6 (1 \u2212 D(si)) and pro-\ncess this masked sequence where the remaining states are\nuseless for return prediction. This loss ensures that all the\nremaining states are not useful for estimating the return. We\ndefine the reverse loss as:\nLrev\nD\n= \u2212\nX\ni\nLG(G(si \u25e6 (1 \u2212 D(si))), yi).\n(6)\n3.4. Iterative Training\nHere we introduce the training strategy of our frame-\nwork. We train the return predictor on complete and contin-\nuous state trajectories. At the same time, we use it to predict\nthe return of masked state sequences that are incomplete\nand discontinuous when training the critical state detector.\nWe iteratively train the predictor and the detector, where the\nlearning objective of the whole framework is given by:\nmin\nG min\nD LD + LG.\n(7)\nAfter training, our critical state detector automatically de-\ntects critical states without using the return predictor. Ap-\npendix A lists the pseudo-code of the proposed method.\n4. Experiments\n4.1. Benchmark and Protocol Navigation\nWe begin this section by releasing a benchmark to test\nour method and facilitate the research on explainability. As\nshown in Table 1, we collect five datasets on three different\nRL environments, i.e., Grid World [9, 8], Atari-Pong [6],\nand Atari-Seaquest [6]. We select Grid World for qualitative\nanalysis since it is very intuitive for human understanding.\nWe study a challenging environment with partial observa-\ntion. In the context of Grid World, we define a \u201dstate\u201d as\na combination of the current visual frame and historical in-\nformation. Although this surrogate representation does not\nequate to the full, true state of the environment, it serves\nas an agent\u2019s internal understanding, developed from its se-\nquence of past observations. To elaborate, when we say\nthat our model identifies a \u201dstate\u201d in this context, we imply\nthat it recognizes a specific observation or frame, based on\nthe agent\u2019s history of previous observations. For fully ob-\nservable environments like Atari, the term \u201dstate\u201d assumes\nits traditional definition, providing complete information\nabout the system at any given time. We use Atari-Pong and\nAtari-Seaquest environments to compare our method with\nsimilar approaches based on critical state identification, us-\ning adversarial attacks, and evaluating policy improvement.\nNote that evaluating critical states using adversarial attacks\nwas first proposed by work on Edge [21]. However, Edge\ndoes not consider cross-policy attacks where the policies for\ntraining and testing the detector are different. More details\ncan be found in the supplementary material.\nTable 1. The specification of the five collected datasets. The\ndatasets cover discrete and continuous returns for a comprehensive\nstudy of the proposed method. y here is the cumulative reward.\nLength\nTraining\nTest\nTotal\nGrid World-S (Memory: 353 MB)\nReaching Goal\n31.97\n1000\n200\n1200\nFail\n25.72\n1000\n200\n1200\nGrid World-M (Memory: 412 MB)\nPolicy-1\n31.97\n1000\n200\n1200\nPolicy-2\n38.62\n995\n200\n1195\nAtari-Pong-[S/M](Memory: 174 GB /352 GB)\nAgent Win\n200\n13158/17412\n1213/1702\n14371/19114\nAgent Lose\n200\n8342/4088\n787/298\n9129/4386\nTotal\n-\n21500\n2000\n23500\nAtari-Seaquest-S (Memory:706 GB)\nE[y]=2968.6\n2652.5\n8000\n2000\n10000\nTable 2. Summary of improvements due to our method, where\nGain refers to improvement over the baselines. Our method im-\nproves performance across various tasks.\nThe baselines in the\n2nd-6th rows are our method using Imp. Loss on Grid-World-S,\nEDGE [21] for Atari-Pong-S, an attack with 30 randomly selected\nframes on Atari-Pong-M, and DQN trained with 25M time steps\non Atari-Seaquest-S, respectively.\nDatasets\nNavigation\nTask\nGain\nGridWorld-S\nSec. 4.2\nCritical State Identify\n16.38%\nGridWorld-S\nSec. 4.2\nSequence Reasoning\nQualitative\nGridWorld-M\nSec. 4.3\nPolicy Evaluation\nFirst Study\nAtari-Pong-S\nSec. 4.4\nIn-Policy Adv. Attack\n18.63%\nAtari-Pong-M\nSec. 4.4\nRobust Analysis\n50.35%\nAtari-Seaquest-S\nSec. 4.5\nPolicy Improvement\n17.65%\nFigure 3. Illustration of an instance of the GridWorld environ-\nment. The environment consists of six rooms and one corridor.\nThe agent starts from a random initial location in the corridor, and\nthe final goal is to get the ball. Since the ball is locked in a room,\nthe agent must pick up the key and open the yellow locked door.\nIn a successful episode, the agent must open the unlocked doors\n(colored in green and blue), pick up the key, unlock the yellow\ndoor, and reach the purple ball. Note that the agent has only par-\ntial observation (colored white) of the environment at a time step.\n4.2. Critical State Discovery\nPerformance. This section provides a qualitative analy-\nsis of the critical time point identified by our Deep State\nIdentifier. We choose the \u2018MiniGrid-KeyCorridorS6R3-v0\u2019\nTable 3. Ablation study for the critical state detector.\nImp. Loss\nCom. Loss\nRev. Loss\nF-1 Score (%)\u2191\n\u2713\n\u00d7\n\u00d7\n68.98\n\u2713\n\u2713\n\u00d7\nunstable\n\u00d7\n\u2713\n\u2713\n74.42\n\u2713\n\u00d7\n\u2713\n76.09\n\u2713\n\u2713\n\u2713\n80.28\ntask [54, 9] of the GridWorld environment, where the goal\nis to reach a target position in a locked room after picking\nup a key (see the yellow block in Fig. 3). This task is use-\nful to visually evaluate our method since it is intuitive to\nidentify what states are critical: top row in Fig. 4 shows\nthat states immediately before actions such as \u2018opening the\ndoor\u2019 (S.1, S.2, S.3 ), \u2018picking up the key\u2019 and \u2019opening the\nlocked door\u2019 are critical to successfully reaching the goal.\nNote that there is no ground truth on the critical state for a\ngeneral, more complex environment.\nWe use a pre-defined DRL agent to collect trajectories.\nSince our method detects critical states by masking the tra-\njectory, we evaluate how our critical state detector accu-\nrately assigns high scores to the states we intuitively labeled\nas critical. As shown in Fig. 4, our method assigns high\nvalues to human-annotated critical states and low values to\nremaining states, showing its effectiveness in discovering\ncritical states.\nAblation study. We analyze the contribution of each com-\nponent of the critical state detector loss in Tab. 3 and Fig. 5.\nIf we remove the compactness loss and the reverse loss, our\nmethod wrongly assigns high confidence to all states in an\nepisode, i.e., all states are detected as critical ones. Sim-\nilarly, if we remove the reverse loss, our method detects\nall states as non-critical. Finally, removing only the com-\npactness loss, most states (including non-critical ones) are\nwrongly detected as critical. This ablation shows that each\nloss component is crucial to critical state identification.\nMore Analysis. In RL, states within an episode can be\nhighly correlated. We show how our method can discover\nstate dependencies essential to identifying critical states. It\nis challenging to capture the dependencies among states in\nthe Gridworld since the agent can only partially observe the\nenvironment through a small local view.\nTab. 4 provides examples of states in the environment1.\nIn Gridworld, the states that occur immediately before or\nafter the action \u201copening door\u201d are frequently observed in\na trajectory. In these states, the agent can be either with\nor without the key. However, obtaining the key is crucial\nfor achieving the goal of GridWorld (see Fig. 3). With-\nout the key, the agent cannot successfully finish the task.\nTherefore, the states immediately before or after the action\n\u201copening door\u201d without the key are not as critical as the\n1We use a text description of states due to space constraints. We pro-\nvide visual states in the supplemental material.\nFigure 4. The performance of our method in identifying critical states. The top row shows human-annotated critical states (i.e., ground\ntruth) in an episode. The bottom row shows for each time step in the environment how confident the detector is that the current state is\ncritical. Our method assigns high scores to human-annotated critical states, demonstrating its identification abilities.\nFigure 5. Ablation study of the detector\u2019s loss function. For each\ntime step and loss component, the line indicates how confident the\ndetector is that the current input is critical. Red blocks mark the\nhuman annotation.\nstates immediately before or after the action \u201copening the\ndoor\u201d with the key to predict the return. Tab. 4 shows how\nour method captures such dependencies between \u201copening\ndoor\u201d and \u201cpicking up the key.\u201d Our method successfully\nassigns much higher confidence to the critical states imme-\ndiately before or after the action \u201copening door\u201d with the\nkey and lower confidence to the states immediately before\nor after the action \u201copening door\u201d without the key.\n4.3. Policy Comparison by Critical States\nIn general, researchers use cumulative rewards to vali-\ndate policy performance. However, these metrics cannot\nelucidate the diverse behavioral patterns exhibited by dif-\nferent policies. To better distinguish and explain the behav-\nioral differences among various policies, a return predictor\nis trained to recognize the distinct trajectories of each pol-\nicy. Our detector then is trained to identify critical states\nfor highlighting the contrasts between policies rather than\nmerely focusing on returns, thus facilitating a more compre-\nTable 4. State detector\u2019s confidence score over different states.\nOur method has different confidence scores for the states imme-\ndiately before and after (i.b.a.) opening a door with or without\nthe key, which indicates that it can capture temporal dependencies\namong states. Normal states refer to states where the agent has a\ndistance greater than two from positions where it can take a rele-\nvant action (pick up the key or open a door). We report the mean\nand standard deviation of the confidence over four random seeds.\nState Description\nConfidence\nScore\nNormal States (Full)\n53.66 \u00b1 0.12\nNormal States Before Picking up the Key\n49.59 \u00b1 0.13\nState i.b.a. Opening Door (without the Key)\n67.13 \u00b1 0.12\nState i.b.a. Trying Locked Door (without the Key)\n50.81 \u00b1 0.08\nState i.b.a. Picking up the Key\n78.35 \u00b1 0.04\nNormal States After Picking Up the Key\n56.58 \u00b1 0.10\nState i.b.a. Opening Door (with the Key)\n80.65 \u00b1 0.06\nState i.b.a. Opening Locked Door\n87.55 \u00b1 0.01\nhensive comparison of their behaviors. Consequently, we\ncan leverage the ability of the critical state detector to pin-\npoint the key states that discriminate between the two poli-\ncies and visually represent the dissimilarities between them.\nAs shown in Fig. 6, both policy-A and policy-B can achieve\nthe final goal, but in policy-B, the agent always enters an\ninvalid room after picking up the key, leading to more steps\nin the environment before achieving the goal. Both policies\nachieve a high return. However, our approach identifies the\nmost discriminating states. Our method precisely assigns\nthe highest confidence to the states inside the invalid room.\nThe visualization shows that our method can explain the dif-\nference between the two policies. More details are provided\nin Appendix A.\nFigure 6. Visualization of the Deep State Identifier for policy comparison. We pre-collect policy-A and policy-B. While policy-A is\noptimal, policy-B first causes the agent to enter the incorrect room after picking up the key and then reach the goal. We train our method\nto discriminate between policy-A and policy-B, given sequences of trajectories generated by them. The critical state detector assigns high\nconfidence to states where policy-B is suboptimal.\nTable 5. Win rate changes of the agent before/after attacks by\nfollowing the protocol of EDGE [21]. We use the detected top 30\nstates as input to attack the policy. We report means and standard\ndeviations over three random seeds. The reported results of all the\nbaselines are from previous work [21]. s, a, y, \u03c0 denote the state,\naction, return, and policy parameters, respectively.\n.\nMethod\nInput\nWin Rate Changes \u2193\nRudder [1]\n(s, a, y)\n-19.93 \u00b1 4.43\nSaliency [57, 59, 60]\n(s, a, y)\n-30.33 \u00b1 0.47\nAttention RNN [2]\n(s, a, y, \u03c0)\n-25.27 \u00b1 1.79\nRationale Net [36]\n(s, a, y, \u03c0)\n-29.20 \u00b1 4.24\nEdge [21]\n(s, a, y, \u03c0)\n-65.47 \u00b1 2.90\nOurs with single policy\n(s, y)\n-77.67 \u00b1 0.56\nOurs with multiple policies\n(s, y)\n-85.90 \u00b1 1.47\n4.4. Efficient Attack using Critical States\nIn the previous sections, we showed that our method\nidentifies the critical states with the highest impact on re-\nturn prediction. However, for complex environments, it is\ndifficult to evaluate the performance of this class of meth-\nods because the ground-truth critical states are not available.\nFollowing previous approaches [21], we use adversarial at-\ntacks to validate whether the identified states are critical.\nIntuitively, if a state is critical, introducing noise in the ac-\ntion that a policy would take in such a state will significantly\ndeteriorate performance (the return will be lower). Here we\nfollow the same protocol of previous approaches [21], and\nwe compare the policy\u2019s performance drop to the baseline\nmethods when the 30 most critical states are attacked (i.e.,\nwhenever the agent reaches those states, its action is per-\nturbed).\nTable 5 shows that our method outperforms the other\ntechniques in the Atari-Pong environment, exhibiting the\nmost significant changes in win rates, highlighting its ef-\nficacy in localizing critical states. In particular, we achieve\nan 18.63% improvement over the previous SOTA method\nEdge[21], suggesting that the states identified by our Deep\nState Identifier are more crucial to achieve a high return.\nNote that the previous methods, such as Edge [21], are\nbased on sequences of states and action pairs. Our method\ninstead achieves higher performance by only observing a\nstate sequence. In the real-world scenario, imaging sys-\ntems can easily capture sequences of visual states, while\nactions are more difficult to collect, requiring special sen-\nsors or manual annotations. In other words, our method can\nwork with pure visual information to achieve higher per-\nformance, resulting in flexibility toward various potential\napplications. Moreover, when different policies collect the\ntraining dataset, the proposed method can benefit from data\ndiversity, inducing more satisfactory results (i.e., an 85.90\ndrop in winning performance).\nWe then analyze the attack performance across different\npolicies to test the robustness against policy shifts. In Table\n6, we set the baseline that attacks 30 states chosen randomly\nand attacks a policy that was never used to train our method.\nTo ensure policy diversity for testing, we derive the policies\nwith various random seeds, training steps, and network ar-\nchitectures. Compared with the baseline, our method can-\nnot improve performance using a single policy, which indi-\ncates that a cross-policy protocol is challenging for adver-\nsarial attacks. However, when we increase the training data\nTable 6. Win rate changes of the agent before/after attacks for\ndifferent policies.\nWe assess whether our method, trained on\ntrajectories generated by one or multiple policies, can accurately\nidentify critical time points within a trajectory generated by an-\nother unseen policy. We consider three kinds of unseen policies,\nincluding different random seeds (seeds), different training steps\n(steps), and different network architectures (Arch.), to test the per-\nformance of our method against cross-policy challenges. We re-\nport mean and standard error over three random seeds. We attack\nthe policy perturbing its action in the top 30 states detected.\nBaseline\nOurs (Single)\nOurs (Multi.)\nIn-Policy (baseline)\n54.88 \u00b1 1.80\n-77.67 \u00b1 0.56\n-85.90 \u00b1 1.47\nCross-Policy (Seeds)\n-63.32 \u00b1 0.93\n-30.67 \u00b1 0.58\n-85.45 \u00b1 0.86\nCross-Policy (Steps)\n-50.23 \u00b1 1.21\n-30.57 \u00b1 1.01\n-83.72 \u00b1 0.91\nCross-Policy (Arch.)\n-49.85 \u00b1 3.50\n-39.55 \u00b1 2.38\n-76.50 \u00b1 3.11\nTable 7. Performance of DQN with different adaptive step\nstrategies on Atari-Seaquest. We base the implementation on\nthe Tianshou Platform [70]. Our method effectively improves the\nperformance of DQN. n-step stands for the lookahead steps.\nMethods\nReturn \u2191 \u00b1 St.d.\nPPO (time steps=5M) [55]\n887.00 \u00b1 4.36\nSAC (time steps=5M) [23]\n1395.50 \u00b1 339.34\nRainbow (step=3,time steps=5M) [25]\n2168.50 \u00b1 332.89\nDQN(time steps=10M) [40]\n3094.75 \u00b1 1022.54\nDQN (n-step=random(1,5),time steps=5M) [61]\n3250.25 \u00b1 638.13\nBaseline: DQN (n-step=5,time steps=5M) [61]\n1987.00 \u00b1 115.71\nDQN (n-step=12,time steps=5M) [61]\n1472.50 \u00b1 407.40\nDQN (n-step=grid search,time steps=5M) [61]\n3936.50 \u00b1 459.19\nSAC (time steps=25M)[23]\n1444.00 \u00b1 136.86\nRainbow (time steps=25M)[25]\n2151.25 \u00b1 329.29\nDQN (time steps=25M)[40]\n3525.00 \u00b1 63.87\nHL based on Frequency (time steps=5M)[39, 58]\n2477.00 \u00b1 223.65\nDQN + Ours (n-step\u22645,time steps=5M)\n4147.25 \u00b1 378.16\ndiversity by adding policies, we achieve a higher general-\nization, and the model\u2019s drop in performance improves from\n49.85 to 76.50. A potential explanation is that each policy\ninduces a specific distribution over the state space in the en-\nvironment. Using different policies to collect data allows\nus to generalize to unseen policies and achieve more invari-\nant representations of the policy behavior. Indeed, when\nthe dataset can cover the distribution of states in the envi-\nronment, our method generalizes to arbitrary unseen poli-\ncies. We thereby achieve an environment-specific policy-\nagnostic solution for interoperability.\n4.5. Policy Improvement\nWe show how our method can improve DRL policies.\nThe experimental results in the previous sections demon-\nstrate that our Deep State Identifier can efficiently identify\ncritical states. Here we show how one can use these states\nto perform rapid credit assignment for policy improvement.\nIn particular, we combine our method with the widely-used\nDQN [40] for multi-step credit assignment. The objective\nfunction of traditional Multi-step DQN[25, 61] is:\nX\n(s(j),a(j))\u2208Rep.\n\"\nQ(s(j), a(j)) \u2212\n j+n\u22121\nX\nt=j\n\u03b3t\u2212jr(t)+\n\u03b3n max\na(j+n) QT(s(j+n), a(j+n))\n!#2\n,\n(8)\nwhere Q is the action-value function, i.e., a network pre-\ndicting the expected return of the policy from a particular\nstate-action pair, Rep. is the replay buffer, QT is a copy of\nQ, which is periodically synchronized with Q to facilitate\nlearning, \u03b3 is the discount factor, and a denotes an action.\nA recent study [69] highlights the importance of varying\nthe lookahead step n in Multi-step DQN. Here we combine\nour method with Multi-step DQN by first identifying critical\nstates and then dynamically setting lookahead steps to learn\nDQN. In other words, we set n as the number of time steps\nfrom the state to the most critical state detected within a\nspecific range. Here, we set the maximum lookahead step\nto 5.\nTable 7 presents preliminary results which illustrate that\nMulti-step DQN combined with our method improves the\nreturn of DQN from 1987.00 to 4147.25. Since our method\neffectively discovers states important for return predic-\ntion, our Deep State Identifier provides DQN with faster\ncredit assignment, improving its performance. Moreover,\nour method performs slightly better than finely tuning the\nlookahead step n using grid search. Table 7 also includes\nimproved versions of DQN [39, 58] for comparison. Our\nmethod outperforms all of them.\n5. Conclusion\nOur novel method identifies critical states from episodes\nencoded as videos. Its return predictor and critical state de-\ntector collaborate to achieve this. When the critical state de-\ntector is trained, it outputs a soft mask over the sequence of\nstates. This mask can be interpreted as the detector\u2019s belief\nin the importance of each state. Experimental results con-\nfirm that the generated belief distribution closely approxi-\nmates the importance of each state. Our approach outper-\nforms comparable methods for identifying critical states in\nthe analyzed environments. It can also explain the behav-\nioral differences between policies and improve policy per-\nformance through rapid credit assignment. Future work will\nfocus on applying this method to hierarchical RL and ex-\nploring its potential in more complex domains.\nAcknowledgements\nWe thank Dylan R. Ashley for his valuable comments\nand help to polish the paper.\nThis work was supported\nby the European Research Council (ERC, Advanced Grant\nNumber 742870) and the SDAIA-KAUST Center of Excel-\nlence in Data Science and Artificial Intelligence (SDAIA-\nKAUST AI).\nReferences\n[1] Jose A Arjona-Medina, Michael Gillhofer, Michael Widrich,\nThomas Unterthiner,\nJohannes Brandstetter,\nand Sepp\nHochreiter. Rudder: Return decomposition for delayed re-\nwards. NIPS, 32, 2019.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. ICLR, 2015.\n[3] Trapit\nBansal,\nJakub\nPachocki,\nSzymon\nSidor,\nIlya\nSutskever, and Igor Mordatch.\nEmergent complexity via\nmulti-agent competition. arXiv preprint arXiv:1710.03748,\n2017.\n[4] Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Ver-\nifiable reinforcement learning via policy extraction. NIPS,\n31, 2018.\n[5] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza,\nBolei Zhou, and Antonio Torralba. Understanding the role of\nindividual units in a deep neural network. Proceedings of the\nNational Academy of Sciences, 117(48):30071\u201330078, 2020.\n[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas\nSchneider, John Schulman, Jie Tang, and Wojciech Zaremba.\nOpenai gym, 2016.\n[7] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\nAditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-\nvas, and Igor Mordatch. Decision transformer: Reinforce-\nment learning via sequence modeling. Advances in neural\ninformation processing systems, 34:15084\u201315097, 2021.\n[8] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem\nLahlou, Lucas Willems, Chitwan Saharia, Thien Huu\nNguyen, and Yoshua Bengio. Babyai: A platform to study\nthe sample efficiency of grounded language learning. arXiv\npreprint arXiv:1810.08272, 2018.\n[9] Maxime Chevalier-Boisvert, Lucas Willems, and Suman\nPal.\nMinimalistic gridworld environment for gymnasium.\nGithub, 2018.\n[10] Benjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov,\nand Sergey Levine. Contrastive learning as goal-conditioned\nreinforcement learning.\narXiv preprint arXiv:2206.07568,\n2022.\n[11] Francesco Faccio, Vincent Herrmann, Aditya Ramesh, Louis\nKirsch, and J\u00a8urgen Schmidhuber. Goal-conditioned gener-\nators of deep policies.\narXiv preprint arXiv:2207.01570,\n2022.\n[12] Francesco Faccio, Louis Kirsch, and J\u00a8urgen Schmidhuber.\nParameter-based value functions. In 9th International Con-\nference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[13] Francesco Faccio, Aditya Ramesh, Vincent Herrmann, Jean\nHarb, and J\u00a8urgen Schmidhuber. General policy evaluation\nand improvement by learning to identify few but crucial\nstates. arXiv preprint arXiv:2207.01566, 2022.\n[14] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsuper-\nvised learning for physical interaction through video predic-\ntion. Advances in neural information processing systems, 29,\n2016.\n[15] Chelsea Finn and Sergey Levine. Deep visual foresight for\nplanning robot motion. In 2017 IEEE International Confer-\nence on Robotics and Automation (ICRA), pages 2786\u20132793.\nIEEE, 2017.\n[16] Ruth C Fong and Andrea Vedaldi. Interpretable explanations\nof black boxes by meaningful perturbation. In ICCV, pages\n3429\u20133437, 2017.\n[17] Kunihiko Fukushima.\nNeocognitron:\nA self-organizing\nneural network model for a mechanism of pattern recogni-\ntion unaffected by shift in position. Biological cybernetics,\n36(4):193\u2013202, 1980.\n[18] Xinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey\nLevine, and Pieter Abbeel.\nMultimodal masked autoen-\ncoders learn transferable representations.\narXiv preprint\narXiv:2205.14204, 2022.\n[19] Wenbo Guo, Sui Huang, Yunzhe Tao, Xinyu Xing, and\nLin Lin. Explaining deep learning models\u2013a bayesian non-\nparametric approach. NIPS, 31, 2018.\n[20] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang,\nand Xinyu Xing. Lemna: Explaining deep learning based se-\ncurity applications. In proceedings of the 2018 ACM SIGSAC\nconference on computer and communications security, pages\n364\u2013379, 2018.\n[21] Wenbo Guo, Xian Wu, Usmann Khan, and Xinyu Xing.\nEdge:\nExplaining deep reinforcement learning policies.\nNIPS, 34:12222\u201312236, 2021.\n[22] David Ha and J\u00a8urgen Schmidhuber. World models. arXiv\npreprint arXiv:1803.10122, 2018.\n[23] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey\nLevine. Soft actor-critic: Off-policy maximum entropy deep\nreinforcement learning with a stochastic actor.\nIn ICML,\npages 1861\u20131870. PMLR, 2018.\n[24] Jean Harb, Tom Schaul, Doina Precup, and Pierre-Luc\nBacon.\nPolicy evaluation networks.\narXiv preprint\narXiv:2002.11833, 2020.\n[25] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom\nSchaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal\nPiot, Mohammad Azar, and David Silver. Rainbow: Com-\nbining improvements in deep reinforcement learning.\nIn\nAAAI, volume 32, 2018.\n[26] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term\nmemory. Neural Comput., 9(8):1735\u20131780, Nov. 1997.\n[27] Qibin Hou, PengTao Jiang, Yunchao Wei, and Ming-Ming\nCheng.\nSelf-erasing network for integral object attention.\nAdvances in Neural Information Processing Systems, 31,\n2018.\n[28] Michael Janner, Qiyang Li, and Sergey Levine. Offline re-\ninforcement learning as one big sequence modeling prob-\nlem. In Advances in Neural Information Processing Systems,\n2021.\n[29] Zoe Juozapaitis, Anurag Koul, Alan Fern, Martin Erwig, and\nFinale Doshi-Velez. Explainable reinforcement learning via\nreward decomposition. In IJCAI/ECAI Workshop, 2019.\n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014.\n[31] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013.\n[32] Jan Koutn\u00b4\u0131k, Giuseppe Cuccu, J\u00a8urgen Schmidhuber, and\nFaustino Gomez. Evolving large-scale neural networks for\nvision-based reinforcement learning. In Proceedings of the\n15th annual conference on Genetic and evolutionary compu-\ntation, pages 1061\u20131068, 2013.\n[33] Jan Koutnik, Faustino Gomez, and J\u00a8urgen Schmidhuber.\nEvolving neural networks in compressed weight space. In\nProceedings of the 12th annual conference on Genetic and\nevolutionary computation, pages 619\u2013626, 2010.\n[34] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Communications of the ACM, 60(6):84\u201390, 2017.\n[35] Yann LeCun. A path towards autonomous machine intelli-\ngence version 0.9. 2, 2022-06-27. Open Review, 62, 2022.\n[36] Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationaliz-\ning neural predictions. EMNLP-IJCNLP, 2017.\n[37] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey\nLevine.\nImitation from observation: Learning to imitate\nbehaviors from raw video via context translation. In 2018\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 1118\u20131125. IEEE, 2018.\n[38] Yang\nYoung\nLu,\nWenbo\nGuo,\nXinyu\nXing,\nand\nWilliam Stafford Noble.\nDance:\nEnhancing saliency\nmaps using decoys.\nIn ICML, pages 7124\u20137133. PMLR,\n2021.\n[39] Amy McGovern and Andrew G Barto. Automatic discovery\nof subgoals in reinforcement learning using diverse density.\nComputer Science Department Faculty Publication Series.\n8., 2001.\n[40] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\nGraves, Ioannis Antonoglou, Daan Wierstra, and Martin\nRiedmiller. Playing atari with deep reinforcement learning.\narXiv preprint arXiv:1312.5602, 2013.\n[41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, An-\ndrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,\net al. Human-level control through deep reinforcement learn-\ning. nature, 518(7540):529\u2013533, 2015.\n[42] Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan\nWierstra, and Danilo Jimenez Rezende.\nTowards inter-\npretable reinforcement learning using attention augmented\nagents. NIPS, 32, 2019.\n[43] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl,\nSteven Lin, and Sergey Levine. Visual reinforcement learn-\ning with imagined goals. Advances in neural information\nprocessing systems, 31, 2018.\n[44] Phuc Nguyen, Ting Liu, Gautam Prasad, and Bohyung Han.\nWeakly supervised action localization by sparse temporal\npooling network. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 6752\u2013\n6761, 2018.\n[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in neural information processing systems, 32, 2019.\n[46] J. Schmidhuber. Adaptive history compression for learning\nto divide and conquer. In Proc. International Joint Confer-\nence on Neural Networks, Singapore, volume 2, pages 1130\u2013\n1135. IEEE, 1991.\n[47] J. Schmidhuber. Neural sequence chunkers. Technical Re-\nport FKI-148-91, Institut f\u00a8ur Informatik, Technische Univer-\nsit\u00a8at M\u00a8unchen, April 1991.\n[48] J\u00a8urgen Schmidhuber. Learning complex, extended sequences\nusing the principle of history compression. Neural Compu-\ntation, 4(2):234\u2013242, 1992.\n[49] J\u00a8urgen Schmidhuber. Deep learning in neural networks: An\noverview. Neural networks, 61:85\u2013117, 2015.\n[50] J\u00a8urgen Schmidhuber.\nOn learning to think: Algorithmic\ninformation theory for novel combinations of reinforce-\nment learning controllers and recurrent neural world models.\narXiv preprint arXiv:1511.09249, 2015.\n[51] J\u00a8urgen Schmidhuber.\nOne big net for everything.\narXiv\npreprint arXiv:1802.08864, 2018.\n[52] Juergen Schmidhuber. Annotated history of modern ai and\ndeep learning. arXiv preprint arXiv:2212.11279, 2022.\n[53] J\u00a8urgen Schmidhuber and Reiner Wahnsiedler. Planning sim-\nple trajectories using neural subgoal generators. In From An-\nimals to Animats 2: Proceedings of the Second International\nConference on Simulation of Adaptive Behavior, volume 2,\npage 196. MIT Press, 1993.\n[54] Juergen Schmidhuber, Jieyu Zhao, and MA Wiering. Simple\nprinciples of metalearning. Technical report IDSIA, 69:1\u201323,\n1996.\n[55] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms. arXiv preprint arXiv:1707.06347, 2017.\n[56] Tianmin Shu, Caiming Xiong, and Richard Socher. Hierar-\nchical and interpretable skill acquisition in multi-task rein-\nforcement learning. arXiv preprint arXiv:1712.07294, 2017.\n[57] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.\nDeep inside convolutional networks:\nVisualising image\nclassification models and saliency maps.\narXiv preprint\narXiv:1312.6034, 2013.\n[58] \u00a8Ozg\u00a8ur S\u00b8ims\u00b8ek and Andrew Barto.\nSkill characterization\nbased on betweenness. Advances in neural information pro-\ncessing systems, 21, 2008.\n[59] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00b4egas,\nand Martin Wattenberg.\nSmoothgrad: removing noise by\nadding noise. arXiv preprint arXiv:1706.03825, 2017.\n[60] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic\nattribution for deep networks. In ICML, pages 3319\u20133328.\nPMLR, 2017.\n[61] Richard S Sutton.\nLearning to predict by the methods of\ntemporal differences. Machine learning, 3:9\u201344, 1988.\n[62] Richard S. Sutton and Andrew G. Barto.\nReinforcement\nLearning: An Introduction. A Bradford Book, USA, 2018.\n[63] Richard S Sutton, Doina Precup, and Satinder Singh. Be-\ntween mdps and semi-mdps: A framework for temporal ab-\nstraction in reinforcement learning. Artificial intelligence,\n112(1-2):181\u2013211, 1999.\n[64] Yujin Tang, Duong Nguyen, and David Ha.\nNeuroevolu-\ntion of self-interpretable agents. In Proceedings of the 2020\nGenetic and Evolutionary Computation Conference, pages\n414\u2013424, 2020.\n[65] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A\nphysics engine for model-based control. In 2012 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems,\npages 5026\u20135033. IEEE, 2012.\n[66] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 4489\u20134497,\n2015.\n[67] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh,\nPushmeet Kohli, and Swarat Chaudhuri. Programmatically\ninterpretable reinforcement learning. In ICML, pages 5045\u2013\n5054. PMLR, 2018.\n[68] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang,\nTao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip\nYu. Generalizing to unseen domains: A survey on domain\ngeneralization. IEEE Transactions on Knowledge and Data\nEngineering, 2022.\n[69] Yuhui Wang, Haozhe Liu, Miroslav Strupl, Francesco Fac-\ncio, Qingyuan Wu, Xiaoyang Tan, and J\u00a8urgen Schmidhuber.\nHighway reinforcement learning. Open Review, 2023.\n[70] Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis\nDuburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu.\nTianshou: A highly modularized deep reinforcement learn-\ning library. JMLR, 23(267):1\u20136, 2022.\n[71] Marco Wiering and J\u00a8urgen Schmidhuber.\nHq-learning.\nAdaptive Behavior, 6:219\u2013246, 09 1997.\n[72] Jinheng Xie, Xianxu Hou, Kai Ye, and Linlin Shen. Clims:\ncross language image matching for weakly supervised se-\nmantic segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n4483\u20134492, 2022.\n[73] Tao Yu, Zhizheng Zhang, Cuiling Lan, Zhibo Chen, and\nYan Lu. Mask-based latent reconstruction for reinforcement\nlearning. NIPS, 2022.\n[74] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor\nBapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Re-\nichert, Timothy Lillicrap, Edward Lockhart, et al. Deep rein-\nforcement learning with relational inductive biases. In ICLR,\n2018.\n[75] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and\nChen Change Loy. Domain generalization: A survey. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n2022.\n[76] Deyao Zhu, Yuhui Wang, J\u00a8urgen Schmidhuber, and Mo-\nhamed Elhoseiny.\nGuiding online reinforcement learn-\ning with action-free offline pretraining.\narXiv preprint\narXiv:2301.12876, 2023.\n[77] Mingchen Zhuge, Deng-Ping Fan, Nian Liu, Dingwen\nZhang, Dong Xu, and Ling Shao. Salient object detection\nvia integrity learning. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, 2022.\nThis appendix provides the implementation details of our\nDeep State Identifier. In Section A, we provide the pseudo-\ncode for the Deep State Identifier, its network architecture,\nand the hyperparameters used during training. Then, Sec-\ntion B discusses the datasets we collected and our experi-\nmental protocol. Finally, Section C provides additional ex-\nperimental results related to the ablation study and the com-\nparison with EDGE [21] on MuJoCo.\nA. Implementation Details\nThis section details our implementation of the proposed\nmethod. We implement our method and conduct our experi-\nments using PyTorch [45]. All experiments were conducted\non a cluster node equipped with 4 Nvidia Tesla A100 80GB\nGPUs.\nThe proposed method\u2014while quite effective\u2014is con-\nceptually simple. The training pipeline can be written in\n25 lines of pseudo-code:\n1 import torch as T\n2 def cs_detector_train(input_states, labels):\n3\nmask = cs_detector(input_states)\n4\nloss_reg = lambda_r*T.linalg.norm(mask,ord=1)\n5\nmasked_states = mask * input_states\n6\noutput = return_predictor(masked_states)\n7\nloss_sub = lambda_s*criterion(output,labels)\n8\nreverse_mask = torch.ones_like(mask) - mask\n9\nreverse_states = reverse_mask * input_states\n10\noutput_r = return_predictor(reverse_states)\n11\nconfused_label = torch.ones_like(output_r)\n*0.5 #binary classification case\n12\nloss_vic = lambda_v * criterion(output_r,\nconfused_label)\n13\nloss_total = loss_reg + loss_sub + loss_vic\n14\nloss_total.backward()\n15\noptimizer_cs.step()\n16 def return_predictor_train(input_states, labels):\n17\noutput = return_predictor(input_states)\n18\nloss_d = criterion(output,labels)\n19\nloss_d.backward()\n20\noptimizer_return.step()\n21 def main_train(input_states, labels):\n22\noptimizer_cs.zero_grad()\n23\ncs_detector_train(input_states, labels)\n24\noptimizer_return.zero_grad()\n25\nreturn_predictor_train(input_states, labels)\nWe use two potential network architectures in our work,\n3DCNN [66], and CNN-LSTM [17, 26, 34], to implement\nour Deep State Identifier. Tables 8 and 9 show the specifi-\ncation of the corresponding architectures. We use 3DCNN\narchitecture in Table 10 and employ LSTM structure in the\nother empirical studies.\nTo train the critical state detector and return predic-\ntor, we use the Adam optimizer [30] with \u03b21 = 0.9 and\n\u03b22 = 0.999. The learning rate is set as 1 \u00d7 10\u22124 and the\nweight decay is 1\u00d710\u22124. The input length of 3DCNN is 12\nframes and is a partial observation (7 \u00d7 7 pixels) of the en-\nvironment [9, 8]. The remaining hyper-parameters \u03bbs, \u03bbr,\nand \u03bbv are set to 1, 5 \u00d7 10\u22123 and 2 respectively.\nTable 8. The specification of the 3DCNN-based Neural Net-\nwork adopted in this paper. In-Norm refers to the Instance Nor-\nmalization, 3D Conv. is the 3D convolutional Layer, and F.C.\nrefers to the fully connected layer. In the last layer, the [return\npredictor/critical state detector] has a different architecture speci-\nfied in the last column.\n3DCNN\nChannel\nFilter\nStride\nIn-Norm\nActivation\n3D Conv.\n12 \u2192 32\n(1,3,3)\n(1,2,2)\nFalse\nRelu\n3D Conv.\n32 \u2192 64\n(1,3,3)\n(1,1,1)\nTrue\nRelu\n3D Conv.\n64 \u2192 128\n(1,3,3)\n(1,2,2)\nFalse\nRelu\n3D Conv.\n128 \u2192 128\n(1,3,3)\n(1,1,1)\nTrue\nRelu\n3D Conv.\n128 \u2192256\n(3,2,2)\n(1,1,1)\nFalse\nRelu\nAvg Pooling\n-\n-\n-\n-\n-\nF.C.\n256 \u2192 512\n-\n-\n-\n-\nF.C.\n512 \u2192 [2/12]\n-\n-\n-\n[-/sigmoid]\nTable 9. The specification of the CNN-LSTM Neural Network\nin this paper. In the last layer, the critical state detector outputs\na vector with the same length as the input (i.e., 256\u21921). The\nreturn predictor estimates a scalar for the whole episode (i.e., 256\n\u00d7 length \u21922)\nCNN-LSTM\nChannel\nFilter\nStride\nIn-Norm\nActivation\n2D Conv.\n3 \u2192 32\n3\n2\nFalse\nRelu\n2D Conv.\n32 \u2192 64\n3\n1\nTrue\nRelu\n2D Conv.\n64 \u2192 128\n3\n2\nFalse\nRelu\n2D Conv.\n128 \u2192 128\n3\n1\nTrue\nRelu\n2D Conv.\n128 \u2192 256\n2\n1\nFalse\nRelu\nAvg Pooling\n-\n-\n-\n-\n-\nInput\nHidden\nBi-Direct.\nActivation\nLSTM\n256\n128\nTrue\n-\nF.C.\n[length\u00d7256] \u2192 [2/length]\n[-/sigmoid]\nFigure 7. Illustration of the Deep State Identifier for policy\ncomparison. We modify the return predictor as a binary classi-\nfier. Its training data comprises pairs {si, ci}, where si represents\na trajectory and ci \u2208 R is a class label indicating whether it be-\nlongs to policy-A or policy-B. By exploiting the return predictor,\nthe critical state detector can directly localize the states that pri-\nmarily explain the difference between policy-A and policy-B.\nFig. 7 shows how we can adapt the return predictor to\nfind the critical frame that explains the difference in behav-\nior between the two policies. We can train the return predic-\ntor to identify which of the two policies generates a specific\ntrajectory.\nB. Experimental details\nCritical States Discovery. We use a GridWorld environ-\nment (MiniGrid-KeyCorridorS6R3-v0) to collect a dataset\n(Grid-World-S) to test the accuracy of the critical state de-\ntector. Data is collected by acting in the environment us-\ning an optimal policy based on a depth-first search algo-\nrithm (DFS). Additional data is collected from a random-\nexploring policy. Since, in this environment, one can find\ncritical states by visual inspection (they correspond to the\nstates immediately before or after the action of opening\ndoors or picking up keys), we can directly test the accuracy\nof the proposed method. We use the F1 score as a metric.\nPolicy Comparison by Critical States. Here, we collect a\ndataset, Grid-World-M, for our experiments on policy com-\nparison. The labels in Grid-World-M are the policies that\ncollected the corresponding episode. We use two policies\nto collect data: Policy-A is the optimal policy used to col-\nlect Grid-World-S, while Policy-B is an exploratory policy.\nEfficient Attack using Critical States. Here we use ad-\nversarial attacks on Atari-Pong to validate whether the de-\ntected states are critical.\nFollowing the same protocol\nas Edge [21], we use a trained policy downloaded from\nhttps://github.com/greydanus/baby-a3c to\ncollect the training data. We call the corresponding dataset\nAtari-Pong-S. In particular, we collect 21500 episodes for\ntraining and 2000 for testing, and we fix the length of each\nepisode as 200. We augment the input by randomly in-\ncreasing or decreasing the length within 50 frames, and the\npadding value is set as 0. To validate the generalization\nof the proposed method for unseen policies, we then col-\nlect another dataset, denoted Atari-Pong-M. We train poli-\ncies with different seeds using the same implementation as\nEdge [21] from https://github.com/greydanus/\nbaby-a3c.\nIn particular, we use ten different policies\nto collect training data.\nIn cross-policy (seeds), we use\nthe trained policy on different random seeds to test the\nperformance.\nIn cross-policy (steps), we use the policy\ntrained with 80M and 40M steps for training and testing our\nmethod, respectively. In cross-policy (Arch.), we change\nthe architecture to make the setting more challenging. In\nparticular, we train our method using a policy with 32 chan-\nnels but test it by attacking a policy trained using 64 chan-\nnels. The result in each case is collected by attacking the\nagent for 1500 episodes using three random seeds.\nPolicy Improvement. We test the potential of our method\nto improve policy performance in the Atari-Seaquest envi-\nronment. We first train the policies based on DQN follow-\ning the implementation of Tianshou [70]. Then we use the\ntrained policies to collect a dataset called Atari-Seaquest-S,\nconsisting of 8000 trajectories for training and 2000 trajec-\ntories for testing. The average length of the trajectories is\n2652.5, and the average return is 2968.6. We cut the tra-\njectory into subsequences with 200 states for training. To\nstabilize the training, we equip an orthogonal regularization\nfor our method. Considering the output of the predictor is\na matrix, M \u2208 Rb\u00d7l where n refers to the batch size and\nl is the length of m, we drive the model to minimize the\naccumulation of MMT . As the critical states of each tra-\njectory are generally with different time steps, this regular-\nization can benefit our approach. We train our Deep State\nIdentifier on this video dataset and then test its effectiveness\nby re-training a new adaptive multi-step DQN from scratch,\nwhere the critical state detector adaptively determines the\nlookahead step. We use our trained critical state detector to\ndetermine the lookahead steps for rapid credit assignment\nduring re-training.\nC. Experimental Results\nTo justify the effectiveness of the proposed method, we\ncarry out some additional visualization and analysis. Ta-\nble 10 shows some statistics of the output of the critical\nstate detector and return predictor.\nWe observe that the\nidentified states are few (the L1 Norm is low), and the out-\nput of the return predictor does not change when it ignores\nnon-critical states. If instead, the return predictor observes\nonly states identified as non-critical, then the performance is\nmuch lower. These results further validate the effectiveness\nof the proposed method. We provide additional visualiza-\ntion of the performance of our method when using different\nlosses for the critical state detector. The results are con-\nsistent with our empirical studies. In particular, Fig. 8(a)\nshows that when using only the importance preservation\nloss, all the states are considered critical. When adding only\nthe compactness loss (see Fig. 8(b)) or the reverse loss (see\nFig. 8(c)), the performance is still not satisfactory. The pro-\nposed method can precisely detect the critical states only\nwhen using all three losses. Indeed, as shown in Fig. 9, our\nmethod correctly outputs high confidence when the agent\nobserves critical states (0.73, 0.94, and 0.92) and low confi-\ndence (0.6) otherwise.\nC.1. Non-Vision Environment\nWe also tested the performance in non-vision environ-\nments [3] and compared our method with the same methods\nin Table 5. As shown in Table 12, our method achieves a\nwin rate change of -45.10 on the MuJoCo [65] environment\nYou-Shall-Not-Pass game, surpassing the performance of\nEDGE (-35.13) by 28.38%. In the Kick-and-Defense envi-\nronment, our method achieves a win rate change of -48.03,\noutperforming EDGE (-43.47) by 10.49%. The consistent\nimprovement indicates that our method exhibits strong ro-\nbustness in non-vision environments.\nFigure 8. Visualization of our method with different losses. The number at the top-left corner indicates the confidence score predicted\nby the critical state detector, indicating whether the corresponding state is important. (a) Baseline trained with Importance Preservation\nloss; (b) Baseline with Importance Preservation loss and Compactness loss. (c) Baseline with Importance Preservation loss and Reverse\nloss. None of them can detect critical states effectively.\nTable 10. Ablation study for the Deep State Identifier. Clean Acc. refers to the accuracy of the return predictor in the test set; Masked\nAcc. is the accuracy of the return predictor with the input (critical states) detected by the critical state detector; R-Masked Acc. is the\naccuracy of the return predictor where the masked is inverted (non-critical states are treated as critical and vice versa); L1(Mask) and\nVar(Mask) are the L1 norm and the average variance of the output of the critical state detector respectively.\nImp. Loss\nCom. Loss\nRev. Loss\n3DCNN\nCNN-LSTM\nClean Acc. (%) \u2191\nMasked Acc.(%) \u2191\nR-Masked Acc.(%) \u2193\nL1(Mask) \u2193\nVar(Mask) \u2191\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u00d7\n90.07\n90.07\n44.57\n63.74\n2 \u00d7 10\u22126\n\u2713\n\u2713\n\u00d7\n\u2713\n\u00d7\n91.45\n87.71\n89.28\n8.79\n0.01\n\u2713\n\u00d7\n\u2713\n\u2713\n\u00d7\n91.45\n91.39\n76.12\n63.73\n0.03\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n90.78\n89.45\n64.55\n57.35\n0.04\n\u2713\n\u2713\n\u2713\n\u00d7\n\u2713\n98.66\n98.44\n55.58\n41.05\n0.12\nFigure 9. Sampled observations from an episode collected by our method. The number at the top-left corner indicates the confidence\nscore predicted by the critical state detector, indicating whether the corresponding state is critical. Our method can localize the critical\nstates effectively.\nTable 11.\nSensitivity Analysis of the Deep State Identifier.\nWe show the F1 score \u2191 of our method using different hyper-\nparameters on GridWorld-S datasets.\n\u03bbr(\u00d710\u22123)\n1\n2.5\n5\n7.5\n10\nVariance\nF1 Score\n76.69\n78.44\n80.28\n78.26\n76.44\n1.39\n\u03bbs\n0.5\n0.75\n1\n1.25\n1.5\nVariance\nF1 Score\n76.68\n77.50\n80.28\n78.18\n78.83\n1.22\n\u03bbv\n1.5\n1.75\n2\n2.25\n2.5\nVariance\nF1 Score\n77.77\n77.04\n80.28\n78.76\n83.78\n2.39\nC.2. Sensitivity analysis\nWe evaluate the performance of our method using dif-\nferent values of hyperparameters \u03bbr,\u03bbs, and \u03bbv. Table 11\nshows that our algorithm has moderate sensitivity to hyper-\nparameters when their values are within a specific range.\nTable 12.\nWin rate changes of the agent before/after attacks\nby following the protocol of EDGE [21] We compare the meth-\nods on two MuJoCo environments: You-Should-Not-Pass game\n[3] (MuJoCo-Y) and Kick-And-Defend game [3] (MuJoCo-K).\nMethod\nMuJoCo-Y\nMuJoCo-K\nRudder [1]\n-32.53\n-21.80\nSaliency [57, 59, 60]\n-29.33\n-37.87\nAttention RNN [2]\n-33.93\n-41.20\nRationale Net [36]\n-30.00\n-7.13\nEdge [21]\n-35.13\n-43.47\nOurs\n-45.10\n-48.03\nFor example, given \u03bbs \u2208 [0.5, 1.5], the performance vari-\nance is only 1.22, indicating stable performance. To de-\ntermine the optimal hyperparameters, we searched a range\nof values. The best hyperparameters found in GridWorld-S\nwere then used in all other environments.\n"
  },
  {
    "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
    "link": "https://arxiv.org/pdf/2308.07395.pdf",
    "upvote": "4",
    "text": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models\nShaan Bijwadia1, Shuo-yiin Chang1, Weiran Wang1, Zhong Meng1, Hao Zhang1, Tara N. Sainath1\n1Google, USA\n{shaanb, shuoyiin, weiranwang, haozhang, zhongmeng, tsainath}@google.com\nAbstract\nText injection for automatic speech recognition (ASR), wherein\nunpaired text-only data is used to supplement paired audio-text\ndata, has shown promising improvements for word error rate.\nThis study examines the use of text injection for auxiliary tasks,\nwhich are the non-ASR tasks often performed by an E2E model.\nIn this work, we use joint end-to-end and internal language\nmodel training (JEIT) as our text injection algorithm to train\nan ASR model which performs two auxiliary tasks. The first\nis capitalization, which is a de-normalization task. The second\nis turn-taking prediction, which attempts to identify whether a\nuser has completed their conversation turn in a digital assistant\ninteraction. We show results demonstrating that our text in-\njection method boosts capitalization performance for long-tail\ndata, and improves turn-taking detection recall.\nIndex Terms:\nspeech recognition, text injection, auxiliary\ntasks\n1. Introduction\nAutomatic speech recognition (ASR) has long been an integral\npart of important technologies, including voice dictation, dig-\nital assistants, and video captioning [1]. While ASR systems\nare typically evaluated based on word error rate (WER), this is\nnot the only metric of concern in production applications; sev-\neral \u201cauxiliary tasks\u201d must be integrated with the ASR task in a\nfull system. These tasks may include: capitalization and punc-\ntuation, which improves readability; voice activity detection\n(VAD) and end-of-query (EOQ) detection, which are important\nfor implementing low-latency systems; and natural conversa-\ntion understanding, which involves predicting the cadence and\nturn-taking aspects of an ongoing conversation. In this study,\nwe focus on improving the quality of such auxiliary tasks in an\nend-to-end (E2E) ASR setting via text injection.\nWe build on two recent capabilities for speech models. First\nis the E2E integration of auxiliary tasks with the ASR task into\na single model. In the past, auxiliary tasks were usually per-\nformed by separate models downstream of ASR [2, 3, 4, 5]. Re-\ncent work has successfully explored integrating auxiliary tasks,\nsuch as endpointing [6, 7, 8], capitalization [9], natural con-\nversation understanding [10], and speaker diarization [11] into\nthe same model as ASR prediction. E2E integration of ASR\nand auxiliary tasks has a key drawback, however. When folded\ninto an E2E ASR model, pure text-to-text tasks (such as capi-\ntalization) can no longer be trained on plentiful text-only data\n(i.e., text data with no associated audio); instead, their training\nexamples will be limited to the transcripts available in paired\naudio-text labeled data. This puts E2E methods at a disadvan-\ntage, since text-only data is generally more plentiful and easier\nto obtain than labeled audio data, and can be used to more easily\nexpose the model to rare words and other long-tail phenomena\nwhich may be difficult to collect in labeled audio form [12].\nThe second capability enabling the current study is the use\nof \u201ctext injection\u201d as a means of improving ASR quality [13].\nAn ASR model\u2019s internal language model (ILM) is the notional\npart of the network that predicts the next token given the pre-\nvious token history, independent of audio input. Though it is\nusually infeasible to exactly separate the influence of audio in-\nput from previous token predictions, several methods have been\ndeveloped to estimate ILM scores [14, 15]. Text-only data can\nthen be used to refine the ILM capabilities of the ASR network\n[16, 17].\nIn this work, we propose a method to utilize text injec-\ntion techniques for improving auxiliary task performance in an\nE2E ASR model. Doing so allows auxiliary tasks to access the\nmulti-task learning benefits of co-training with ASR while still\nincluding rich text-only data in their training corpora. We fo-\ncus our study on two tasks: capitalization and conversational\nturn-taking prediction. The former is a strongly text-based task,\nsince capitalization is merely a form of de-normalization from\nspoken to written domain, and capitalized words are not pro-\nnounced differently. The latter task clearly involves combining\nlinguistic and acoustic understanding \u2014 the prosody of the in-\nput speech as well as the semantics of the current recognition\nare both informative for predicting whether a pause is only mo-\nmentary or if the user has finished speaking. We integrate these\ntasks into a production-ready model, streaming E2E RNN-T\nASR model [18, 19]. We show results demonstrating that text\ninjection can meaningfully improve auxiliary task performance,\nparticularly in long-tail settings.\n2. Related Work\nWhile auxiliary tasks are usually performed by separate models\nfrom ASR [20, 21], E2E approaches to auxiliary task modeling\nhave been recently popular for production-grade systems. Joint\ntraining of ASR with endpointing [7], capitalization [9, 22], in-\ntended query detection [23, 24], sentence segmentation [25],\nand more, have been explored. Our work builds most closely\non Wang et al.[9], who co-train ASR, capitalization, and turn-\ntaking prediction by building multiple parallel label sequences.\nTo our knowledge, this is the first attempt to refine auxiliary\ntasks in an E2E ASR model using text-only data.\nThere has long been interest in utilizing unpaired text data\nfor the ASR task. Several approaches to LM fusion, the use of\nan external LM to improve ASR recognition quality, have been\nproposed [26]. These methods have the drawback of increasing\ntotal parameter count (due to the size of the LM model), and\ncomputation cost during inference. Text injection [13] solves\nthese issues by using LM-style unpaired text data to train the\narXiv:2308.07395v1  [cs.CL]  14 Aug 2023\nASR model itself. Some methods focus on fine-tuning an ex-\nisting ASR model trained on audio-text data; ILM adaptation\nof the ASR decoder has been shown to work well [27, 28, 29].\nThe text injection method we employ here is joint end-to-end\nand ILM training (JEIT), which was introduced by Meng et al\n[30]. We choose JEIT as our method due to its lightweight na-\nture; its primary focus on refining the ASR decoder makes com-\nparison to standard methods straightforward, since the behavior\nof the audio encoder is preserved. Other methods inject text\ndata directly into the encoder, with fixed and learned duration\nmodels to align text and audio sequences [16, 17]. All of the\nabove works focus on improving ASR quality, both for stan-\ndard and long-tail data; to the best of our knowledge, adapting\nthese techniques for auxiliary tasks is a novel contribution to the\nliterature.\n3. Auxiliary Tasks\n3.1. Capitalization\nCapitalization is the process of restoring the correct case (up-\npercase or lowercase) of noisy text. Notably, capitalization is\nspecific to the written domain, and has no marker in spoken\nspeech. This task is important for maintaining readability in\nASR output, especially for long-form captioning cases.\n3.2. Conversational turn-taking\nTurn-taking is an active area of research for E2E speech model-\ning [10, 31]. While humans typically adjust their speech when\ninteracting with voice assistants [31], natural human speech pat-\nterns during conversation are often filled with natural disfluen-\ncies. For digital assistant products, it is desirable that voice\nassistants have the ability to predict when the speaker is expect-\ning a response, versus when they merely pause with the inten-\ntion to resume speaking. We model this phenomenon similar\nto Chang et al. [10], who classify pauses in speech as being\nwithin a complete thought, or after having a finished complete\nthought. That is, when a user stops speaking, the model should\npredict whether they will continue speaking after a brief pause\nor whether a system response is expected. Because the active\nregion of interest is pauses in the audio, we refer to this task in\nthis paper as \u201cpause prediction.\u201d\n4. Model\n4.1. Multi-output HAT decoder\nHAT is a decoder structure for RNN-T in which the \u27e8blank\u27e9\nprobability is computed separately from next token prediction,\nfacilitating more accurate ILM estimation [14]. Wang et al. [9]\npropose a variant of HAT decoder which introduces multiple\njoint networks, one for each task (in our case, these are ASR,\ncapitalization, and pause prediction). All of the parallel joint\nnetworks are conditioned on features from both the prediction\nnetwork and audio encoders.\nThe model is trained using an RNN-T objective [18], where\nat each timestep the model may choose to emit a wordpiece to-\nken, or to insert a special token \u27e8blank\u27e9 which indicates non-\nemission. Formally, let X be the input utterance and Y be the\nlabel sequence. The ASR output space YASR consists of {y0 =\n\u27e8blank\u27e9, y1, y2, ...}.. Let T = |X| be the number of input\naudio frames and U = |Y | be the length of the transcript. The\nacoustic encoder produces f(X) = [f0, ..., fT \u22121], ft \u2208 RDa,\nand the prediction network produces g(X) = [g0, ..., gU\u22121],\ngu \u2208 RDp. As in the original HAT implementation, the joint\nFigure 1: Model diagram for JEIT training. The blue arrows\ndenote the data flow for paired audio-text data. The red ar-\nrows denote the path that unpaired text data takes through the\nnetwork. Baseline experiments are trained using only the blue\npaths, while the proposed system is trained using both.\nnetwork fuses ft and gu with a \u201cproject and sum\u201d operation\nto produce a hidden representation ht,u, which is then passed\nthrough a non-linear activation and a final linear layer to pro-\nduce st,u:\nht,u = P \u00b7 ft + Q \u00b7 gu + bh\n\u2208 RDh\n(1)\nst,u = A \u00b7 tanh(ht,u) + bs\n\u2208 RV .\n(2)\nwhere P, Q, and A are learned weight matrices with dimen-\nsions determined by Da, Dp, Dh, and V is the size of the vo-\ncabulary. As this is a HAT model, the 0-th logit of st,u is used\nindividually to compute the probability of emission bt,u:\nbt,u := Pt,u(\u27e8blank\u27e9|f0:t, g0:u) = \u03c3(st,u[0])\n(3)\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)) is the sigmoid activation.\nProbabilities over the ASR tokens are computed by feeding all\nremaining logits to a softmax function. The probability of each\nASR token yv in the vocabulary is:\n\u02c6yv;t,u = Pt,u(\u02c6yv|f0:t, g0:u)\n= softmax(st,u[1 :])[v \u2212 1]\n(4)\nThus the predicted probability distribution over all output to-\nkens is the emission probability, followed by the probabilities\nof each token given emission:\n\u02c6yt,u = [bt,u,\n(1 \u2212 bt,u) \u00b7 \u02c6y0;t,u,\n... ,\n(1 \u2212 bt,u) \u00b7 \u02c6yV \u22121;t,u]\n(5)\nThus far we have referred to the mechanism above in terms\nof ASR prediction. Capitalization and pause predictions are\nmade in the exact same way, where each task independently\ncomputes Eqs. (1) and (2) based on the shared representations\nft and gu (note that each auxiliary task is exposed to the label\nhistory of the ASR output, not its own prediction history).\nSince capitalization tokens must be strictly aligned with\nASR tokens, the capitalization posterior borrows the blank logit\nInitial Transcript\ndriving time to san francisco\nCapitalization\nDriving time to San Francisco\nTokenization\n\u27e8cap\u27e9 driving time to \u27e8pause\u27e9 \u27e8cap\u27e9 san \u27e8cap\u27e9 fran cisco \u27e8eos\u27e9\nLabel Factorization\nYASR :\ndriving\ntime\nto\nsan\nfran\ncisco\nYCap :\n\u27e8cap\u27e9\n\u27e8non-cap\u27e9\n\u27e8non-cap\u27e9 \u27e8cap\u27e9\n\u27e8cap\u27e9\n\u27e8non-cap\u27e9\nYPause : \u27e8non-pause\u27e9 \u27e8non-pause\u27e9 \u27e8pause\u27e9\n\u27e8non-pause\u27e9 \u27e8non-pause\u27e9 \u27e8eos\u27e9\nFigure 2: Data preparation for auxiliary tasks. Wordpieces that begin with\ndenote word boundaries. In this example, we assume that\nthe speaker takes a verbal pause as follows: \u201dDriving time to... San Francisco,\u201d to illustrate the \u27e8pause\u27e9 logic.\nfrom the ASR prediction. Thus, a capitalization token will only\nbe emitted when an ASR token is emitted as well. Capitaliza-\ntion has output space YCap = {\u27e8cap\u27e9, \u27e8non-cap\u27e9} and its\nposterior is:\n\u02c6yCap\nt,u = [bASR\nt,u ,\n(1 \u2212 bASR\nt,u ) \u00b7 Pt,u(\u27e8cap\u27e9),\n(1 \u2212 bASR\nt,u ) \u00b7 Pt,u(\u27e8non-cap\u27e9)]\n(6)\nAt inference time, we estimate P(\u27e8cap\u27e9) every time an\nASR token is emitted and predict a capitalization if it is above\na threshold (in this work, we use 0.5).\nPause tokens do not need to be strictly aligned with the\nASR transcript prediction, since they are likely to be pre-\ndicted during non-speech periods in the audio during infer-\nence, so the turn-taking sequence has its own blank pos-\nterior.\nThe pause prediction output space is YPause\n=\n{\u27e8blank\u27e9, \u27e8non-pause\u27e9, \u27e8pause\u27e9, \u27e8eos\u27e9} and its poste-\nrior is computed in the same way as Eq. (5).\n5. Training\n5.1. JEIT\nJoint end-to-end model and ILM training (JEIT) was proposed\nby Meng et al. [30] as a way to train an RNN-T ASR model on\npaired audio-text data while simultaneously training the HAT\ndecoder ILM on text-only data. For paired dataset Dpaired, train-\ning is conducted in the usual way; the model is given the audio\nsequence as input and predicts PE2E(Y |X). This is converted to\na loss LASR\nE2E via the RNN-T objective [18].\nThe text-only dataset Dunpaired contains transcripts with cap-\nitalization and pause annotations (see \u00a75.2). Similar to HAT\nILM adaptation (ILMA) [27], we feed the transcript as the pre-\nvious token history to the prediction network, and mock the en-\ncoder output with vectors full of zeros: \u2200t\u22080:T : ft = 0. Since\nthe audio sequence does not exist, we simply ignore the blank\nposterior, and the predicted next token probabilities are given\ndirectly by the softmax output in Eq. (4). With previous token\nhistory as input and next token probabilities as output, this al-\nlows us to estimate PILM(yt : y0:t\u22121). ILM loss is defined as\nthe negative log probability of each label token given the label\nsequence prefix:\nLASR\nILM = \u2212\nU\nX\nu=1\nlog P(yASR\nu\n|\u02c6yASR\n0:u\u22121)\n(7)\nThe losses LE2E and LILM are averaged over their respec-\ntive datasets Dpaired and Dunpaired, then combined in a weighted\naverage to obtain the total JEIT loss:\nLASR\nJEIT(Dpaired, Dunpaired) =\nLASR\nE2E (Dpaired) + \u03b2LASR\nILM(Dunpaired)\n(8)\nwhere \u03b2 is a hyperparameter controlling the weight given\nto ILM training (in this work, we use \u03b2 = 0.2 to match\nMeng et al.\u2019s original study).\nAdapting JEIT to include auxiliary tasks is straightforward.\nAs described in \u00a74.1, each auxiliary task makes a sequence\nprediction YAux based on the predicted ASR sequence YASR.\nThus, each auxiliary task predicts PE2E(YAux| \u02c6YASR; X) to pro-\nduce LAux\nE2E. Similarly, the ILM loss is\nLAux\nILM = \u2212\nU\nX\nu=1\nlog P(yAux\nu |\u02c6yASR\n0:u\u22121)\n(9)\nThe full JEIT loss for each task is defined in the same way as\nEq. (8). Total loss is a linear combination of all tasks: (datasets\nomitted for clarity):\nLTotal\nJEIT =\nLASR\nE2E + \u03b2LASR\nILM\n+ \u03b1Cap(LCap\nE2E + \u03b2LCap\nILM)\n+ \u03b1Pause(LPause\nE2E + \u03b2LPause\nILM ) (10)\nwhere each \u03b1 is a loss weight for the corresponding task.\nMatching Wang\u2019s original study, we use \u03b1Cap\n= 0.1 and\n\u03b1Pause = 0.3. Figure 1 shows the data flow for paired and un-\npaired data through the ASR model.\n5.2. Transcript annotation\nWhile a small amount of our paired training corpus is hand-\nlabeled and capitalized, most of our paired data and all of our\nunpaired text data have lowercase transcripts. For the lowercase\ntranscripts, we use a text-based truecasing RNN teacher model\nsimilar to [32] to produce capitalization predictions.\nProducing pause prediction labels requires different ap-\nproaches for paired and unpaired data. For paired audio-text\ndata, we use the approach taken by Chang et al. [10], which\nuses heuristics based on a forced alignment [33] to insert pause\ntokens into the transcript. There are two such tokens: \u27e8pause\u27e9\ndenotes a brief stop by the speaker in the middle of a full\nthought, and \u27e8eos\u27e9 (end of sentence) is inserted at the end of\nthe full thought, i.e. a full conversational turn.\nFor unpaired text-only data, the above strategy is impossi-\nble, since we do not have access to the associated audio. In-\nstead, we rely on the fact that our text-only data comes from\nshort-query sources (see \u00a76.2). We simply append the \u27e8eos\u27e9\ntoken to the end of the transcript.\n5.3. Multi-task label structure\nA common approach to transcript labeling for auxiliary tasks\nwould be to embed special tokens corresponding to each task\nin the transcript itself [7]. However, this is not ideal for in-\nference, since the extra tokens must be expanded in-line with\nthe ASR tokens; if predictions on competing beams differ only\nin their special tokens, lattice diversity is reduced because the\nASR prediction would be identical. To solve for this, we fol-\nlow Wang et al. [9], factorizing the auxiliary task tokens into\nparallel sequences of equal length, one for each task. The ASR\ntask is trained on the lowercase transcript sequence, segmented\ninto wordpieces. The capitalization sequence is defined as fol-\nlows: each token is either \u27e8cap\u27e9 (capitalized) or \u27e8non-cap\u27e9\nTable 1: Capitalization.\nWe report word error rate (WER\n(%)) and uppercase error rate (UER (%)) on a representative\n(\u201chead\u201d) voice dictation dataset. We also report UER on a\ndataset containing rare words (\u201ctail\u201d).\nExp.\nMethod\nWER\nHead\nTail\nUER\nUER\nB1\nPaired Data Only\n3.9\n24.3\n46.0\nE1\nJEIT (Proposed)\n3.9\n24.7\n45.1\nTable 2: Sample capitalization improvements. For anonymity,\nsome transcript words are substituted with equivalents, while\npreserving the capitalization dynamics.\nGround Truth\nHypothesis\nExp.\nMatheus Nicolau UFC fighter\nMatheus nicolau UFC fighter\nB1\nMatheus Nicolau UFC fighter\nE1\nSmoketown Brewing Company\nsmoketown Brewing Company\nB1\nSmoketown Brewing Company\nE1\nPlay Maldita Vecindad\nplay Maldita vecindad\nB1\nplay Maldita Vecindad\nE1\n(not capitalized), based on the corresponding wordpiece in the\nASR transcript. Similarly, the turn-prediction sequence is pop-\nulated with \u27e8pause\u27e9 and \u27e8eos\u27e9 tokens corresponding to the\nwordpieces immediately preceding the corresponding predicted\npauses in the transcript. All other token slots are filled with\n\u27e8non-pause\u27e9. The successive steps of label generation are\nshown in Figure 2.\n6. Experimental Details\n6.1. Model architecture\nWe use a 128-dimensional log-mel feature frontend computed\non 32ms windows with a 10ms stride. We stack four consecu-\ntive frames together and sub-sambled by a factor of 3, resulting\nin 512-dim features at a 30ms framerate. This vector is then\nconcatenated with a 16-dim one-hot domain ID vector [34]. As\nour ASR backbone we use a 2-pass cascaded encoder model\n[35]. The first encoder consists of 7 conformer layers [36] with\ncausal convolution and left-context attention. The second en-\ncoder consists of 10 conformer layers with a 900ms lookahead.\nEach conformer layer uses 512-dim 8-head self-attention and\na kernel size of 15, and the final layer emits Da = 384-dim\nencodings. The prediction network of each decoder is a V 2\nembedding lookup table, which computes Dp = 640-dim fea-\ntures based on embeddings of the previous two wordpiece to-\nkens. Each joint network has hidden dimension Dh = 384, and\npredictions are made over a vocabulary of V = 4096 word-\npieces. For evaluation, we report only 2nd pass WER.\nIn total, our model has \u223c160M params. It is implemented\nin Tensorflow using the Lingvo toolkit, and is trained on propri-\netary specialized hardware for 500k steps using batch size 4096\nfor paired and unpaired data.\n6.2. Data\n6.2.1. Paired training data\nOur training set of audio-text pairs consists of a dataset of 650\nmillion English multi-domain examples, drawn from search,\ndictation, online video, and telephony domains. A small sub-\nset of these utterances are anonymized and hand-transcribed,\nand the rest are pseudo-labeled by a 600M parameter bidirec-\ntional teacher model. To increase model robustness, we apply\nsimulated noise to utterances, as well as SpecAug [37].\nTable 3: Pause prediction. We report precision and recall for\nthe \u27e8eos\u27e9 token on a conversation-style test set.\nExp.\nMethod\n\u27e8eos\u27e9 Prediction\nPrecision\nRecall\nB1\nPaired Data Only\n72.16\n89.56\nE1\nJEIT (Proposed)\n71.12\n92.94\n6.2.2. Unpaired training data\nOur text-only data selection pipeline is designed in the style of\nSentence-Select by Huang et al [12]. Text query data (\u223c 100B\nutterances) is collected from web search, maps search, app store\nsearch, and online video search domains. This data is filtered\nfor rare words and contrastive filtering based on perplexity is\napplied. Because the data is selected to include rare words, we\nexpect improvements at the tails of the evaluation distribution.\n6.2.3. Evaluation Data\nWER is reported on \u223c 17k utterances representative of real-\nworld voice dictation traffic. Ground truth transcript and auxil-\niary task annotations are obtained via human labeling. We also\nreport uppercase error rate (UER) on this set, which is calcu-\nlated by removing all lowercase letters from the ground truth\nlabel and the predicted transcript and computing standard WER\nwith upper case letters as words. Since our text-only data fo-\ncuses on long-tail traffic, we also report UER on a set of \u223c300\nutterances with transcripts containing rare words.\nFor pause prediction, we use a testset of \u223c2500 utter-\nances containing hesitations and multiple consecutive com-\nmands. Pauses in the audio are hand-annotated as continuation\npauses or final pauses. The metrics reported are average preci-\nsion and recall of the \u27e8eos\u27e9 token.\n7. Results\nWe evaluate the proposed method (E1) against a baseline (B1)\nwhich uses an identical model but is trained on paired data only\n(Table 1). On the large voice search test set on which it is eval-\nuated, WER does not change, while UER regresses slightly on\nthe voice dictation dataset (1.6% relative). For long tail data,\nUER improves by a relative 2.0%. Table 2 shows example tran-\nscripts demonstrating our proposed method\u2019s better capability\nat recognizing capitalized named entities.\nPause detection recall improves by 3.7% (relative), while\nprecision is reduced slightly, by 1.4% (relative) (Table 3). This\nmatches the intuition that our text-injection method biases the\nmodel towards \u27e8eos\u27e9, since the unpaired text data is only aug-\nmented with \u27e8eos\u27e9 at the end of short form transcripts. How-\never, the improvement in recall is larger than the change in\nprecision, and in a production setting, hyperparameters may\nbe tuned to balance the two metrics differently. These results\nshow that augmenting the training data of an ASR model with\nunpaired text data using JEIT can be used to meaningfully im-\nprove pause prediction performance, without regressing word-\nerror rate.\nThese results show that augmenting the training data of an\nASR model with unpaired text data meaningfully impacts aux-\niliary task performance. In our case, we use long-tail, shortform\ntext data to improve capitalization performance for rare words\nand turn-taking prediction recall. We recommend that future\nwork extend this technique to other text injection methods, and\nexplore the use of text injection for other auxiliary tasks.\n8. References\n[1] J. Schalkwyk, D. Beeferman, F. Beaufays, B. Byrne, C. Chelba,\nM. Cohen, M. Kamvar, and B. Strope, \u201cYour Word is my Com-\nmand\u201d: Google Search by Voice: A Case Study.\nBoston, MA:\nSpringer US, 2010, pp. 61\u201390.\n[2] S.-Y. Chang, B. Li, T. N. Sainath, G. Simko, and C. Parada, \u201cEnd-\npoint detection using grid long short-term memory networks for\nstreaming speech recognition.\u201d in Proc. Interspeech, 2017.\n[3] F. Beaufays and B. Strope, \u201cLanguage model capitalization,\u201d in\n2013 IEEE International Conference on Acoustics, Speech and\nSignal Processing, 2013, pp. 6749\u20136752.\n[4] B. P. Nguyen, V. B. H. Nguyen, H. Nguyen, P. N. Phuong, T.-L.\nNguyen, Q. T. Do, and L. C. Mai, \u201cFast and accurate capitaliza-\ntion and punctuation for automatic speech recognition using trans-\nformer and chunk merging,\u201d 2019 22nd Conference of the Orien-\ntal COCOSDA International Committee for the Co-ordination and\nStandardisation of Speech Databases and Assessment Techniques\n(O-COCOSDA), pp. 1\u20135, 2019.\n[5] F. Batista, \u201cRecovering capitalization and punctuation marks on\nspeech transcriptions,\u201d 2011.\n[6] S. Bijwadia, S.-y. Chang, B. Li, T. Sainath, C. Zhang, and Y. He,\n\u201cUnified end-to-end speech recognition and endpointing for fast\nand efficient speech systems,\u201d in 2022 IEEE Spoken Language\nTechnology Workshop (SLT), 2023, pp. 310\u2013316.\n[7] S. Chang, R. Prabhavalkar, Y. He et al., \u201cJoint Endpointing and\nDecoding with End-to-End Models,\u201d in Proc. ICASSP, 2019.\n[8] R. Maas, A. Rastrow, C. Ma, G. Lan, K. Goehner, G. Tiwari,\nS. Joseph, and B. Hoffmeister, \u201cCombining acoustic embeddings\nand decoding features for end-of-utterance detection in real-time\nfar-field speech recognition systems,\u201d in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2018, pp. 5544\u20135548.\n[9] W. Wang, D. Zhao, S. Ding, H. Zhang, S. yiin Chang, D. Rybach,\nT. N. Sainath, Y. He, I. McGraw, and S. Kumar, \u201cMulti-output\nrnn-t joint networks for multi-task learning of asr and auxiliary\ntasks,\u201d in Proc. ICASSP (forthcoming, 2023.\n[10] S. yiin Chang, B. Li, T. N. Sainath, C. Zhang, T. Strohman,\nQ. Liang, and Y. He, \u201cTurn-taking prediction for natural conver-\nsational speech,\u201d in Interspeech, 2022.\n[11] L. E. Shafey, H. Soltau, and I. Shafran, \u201cJoint speech recognition\nand speaker diarization via sequence transduction,\u201d ArXiv, vol.\nabs/1907.05337, 2019.\n[12] W. R. Huang, C. Peyser, T. N. Sainath, R. Pang, T. Strohman,\nand S. Kumar, \u201cSentence-select: Large-scale language model data\nselection for rare-word speech recognition,\u201d in Interspeech, 2022.\n[13] S. Thomas, B. Kingsbury, G. Saon, and H.-K. J. Kuo, \u201cIntegrating\ntext inputs for training and adapting rnn transducer asr models,\u201d\nICASSP 2022 - 2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pp. 8127\u20138131,\n2022.\n[14] E. Variani, D. Rybach, C. Allauzen, and M. Riley, \u201cHybrid Au-\ntoregressive Transducer (HAT),\u201d in Proc. ICASSP, 2020.\n[15] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu,\nX. Chen, R. Zhao, J. Li, and Y. Gong, \u201cInternal language model\nestimation for domain-adaptive end-to-end speech recognition,\u201d\n2021 IEEE Spoken Language Technology Workshop (SLT), pp.\n243\u2013250, 2020.\n[16] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo,\nZ. Chen, B. Li, W. Wang, and T. Strohman, \u201cJoist: A joint speech\nand text streaming model for asr,\u201d 2022 IEEE Spoken Language\nTechnology Workshop (SLT), pp. 52\u201359, 2022.\n[17] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno,\nA. Bapna, and H. Zen, \u201cMaestro: Matched speech text represen-\ntations through modality matching,\u201d in ICASSP, 2022.\n[18] A. Graves, \u201cSequence Transduction with Recurrent Neural Net-\nworks,\u201d CoRR, vol. abs/1211.3711, 2012.\n[19] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen et al., \u201cA streaming\non-device end-to-end model surpassing server-side conventional\nmodel quality and latency,\u201d in ICASSP 2020-2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2020, pp. 6059\u20136063.\n[20] R. Rei, N. M. Guerreiro, and F. Batista, \u201cAutomatic truecas-\ning of video subtitles using bert: A multilingual adaptable ap-\nproach,\u201d Information Processing and Management of Uncertainty\nin Knowledge-Based Systems, vol. 1237, pp. 708 \u2013 721, 2020.\n[21] M. Sunkara, S. Ronanki, K. Dixit, S. Bodapati, and K. Kirchhoff,\n\u201cRobust prediction of punctuation and truecasing for medical asr,\u201d\nArXiv, vol. abs/2007.02025, 2020.\n[22] V. Pahuja, A. Laha, S. Mirkin, V. C. Raykar, L. Kotlerman, and\nG. Lev, \u201cJoint learning of correlated sequence labeling tasks using\nbidirectional recurrent neural networks,\u201d in Interspeech, 2017.\n[23] S. yiin Chang, G. Prakash, Z. Wu, Q. Liang, T. N. Sainath, B. Li,\nA. Stambler, S. Upadhyay, M. Faruqui, and T. Strohman, \u201cStream-\ning intended query detection using e2e modeling for continued\nconversation,\u201d in Interspeech, 2022.\n[24] S. H. R. Mallidi, R. Maas, K. Goehner, A. Rastrow, S. Matsoukas,\nand B. Hoffmeister, \u201cDevice-directed utterance detection,\u201d in In-\nterspeech, 2018.\n[25] W. R. Huang, S. yiin Chang, D. Rybach, R. Prabhavalkar, T. N.\nSainath, C. Allauzen, C. Peyser, and Z. Lu, \u201cE2e segmenter: Joint\nsegmenting and decoding for long-form asr,\u201d in Interspeech, 2022.\n[26] A. Kannan, Y. Wu, P. Nguyen et al., \u201cAn analysis of incorporating\nan external language model into a sequence-to-sequence model,\u201d\nin Proc. ICASSP, 2018.\n[27] Z. Meng, Y. Gaur, N. Kanda, J. Li, X. Chen, Y. Wu, and Y. Gong,\n\u201cInternal language model adaptation with text-only data for end-\nto-end speech recognition,\u201d ArXiv, vol. abs/2110.05354, 2021.\n[28] V. Bataev, R. Korostik, E. Shabalin, V. Lavrukhin, and B. Gins-\nburg, \u201cText-only domain adaptation for end-to-end asr us-\ning integrated text-to-mel-spectrogram generator,\u201d ArXiv, vol.\nabs/2302.14036, 2023.\n[29] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang,\nK. Audhkhasi, J. Emond, T. Strohman, B. Ramabhadran, W. R.\nHuang, E. Variani, Y. Huang, and P. J. Moreno, \u201cModular\nhybrid autoregressive transducer,\u201d 2022. [Online]. Available:\nhttps://arxiv.org/abs/2210.17049\n[30] Z. Meng, W. Wang, R. Prabhavalkar, T. N. Sainath, T. Chen,\nE. Variani, Y. Zhang, B. Li, A. Rosenberg, and B. Ramabhadran,\n\u201cJeit:\nJoint end-to-end model and internal language model\ntraining for speech recognition,\u201d 2023. [Online]. Available:\nhttps://arxiv.org/abs/2302.08583\n[31] C. Liu, C. T. Ishi, and H. Ishiguro, \u201cTurn-taking estimation model\nbased on joint embedding of lexical and prosodic contents,\u201d in\nInterspeech, 2017.\n[32] H. Zhang, Y.-C. Cheng, S. Kumar, W. R. Huang, M. Chen, and\nR. Mathews, \u201cCapitalization normalization for language modeling\nwith an accurate and efficient hierarchical rnn model,\u201d ICASSP\n2022 - 2022 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pp. 6097\u20136101, 2022.\n[33] P. J. Moreno, C. F. Joerg, J.-M. Van Thong, and O. Glickman, \u201cA\nrecursive algorithm for the forced alignment of very long audio\nsegments.\u201d in ICSLP, vol. 98, 1998, pp. 2711\u20132714.\n[34] A. Narayanan, R. Prabhavalkar, C.-C. Chiu et al., \u201cRecogniz-\ning Long-Form Speech Using Streaming End-to-End Models,\u201d in\nProc. ASRU, 2019.\n[35] A. Narayanan, T. N. Sainath, R. Pang et al., \u201cCascaded en-\ncoders for unifying streaming and non-streaming ASR,\u201d in Proc.\nICASSP, 2021.\n[36] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution-\naugmented Transformer for Speech Recognition,\u201d in Proc. Inter-\nspeech, 2020.\n[37] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A Simple\nData Augmentation Method for Automatic Speech Recognition,\u201d\nin Proc. Interspeech, 2019.\n"
  }
]