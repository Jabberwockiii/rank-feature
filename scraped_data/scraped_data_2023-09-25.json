[
  {
    "title": "CodePlan: Repository-level Coding using LLMs and Planning",
    "link": "https://arxiv.org/pdf/2309.12499.pdf",
    "upvote": "67",
    "text": "CodePlan: Repository-level Coding using LLMs and Planning\nRAMAKRISHNA BAIRI, Microsoft Research, India\nATHARV SONWANE, Microsoft Research, India\nADITYA KANADE, Microsoft Research, India\nVAGEESH D C, Microsoft Research, India\nARUN IYER, Microsoft Research, India\nSURESH PARTHASARATHY, Microsoft Research, India\nSRIRAM RAJAMANI, Microsoft Research, India\nB. ASHOK, Microsoft Research, India\nSHASHANK SHET, Microsoft Research, India\nSoftware engineering activities such as package migration, fixing errors reports from static analysis or testing,\nand adding type annotations or other specifications to a codebase, involve pervasively editing the entire\nrepository of code. We formulate these activities as repository-level coding tasks.\nRecent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in\noffering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved\nand cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire\nrepository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and\npresent a task-agnostic framework, called CodePlan to solve it. CodePlan synthesizes a multi-step chain of\nedits (plan), where each step results in a call to an LLM on a code location with context derived from the entire\nrepository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of\nan incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm.\nWe evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and\ntemporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires\ninter-dependent changes to many files (between 2\u201397 files). Coding tasks of this level of complexity have\nnot been automated using LLMs before. Our results show that CodePlan has better match with the ground\ntruth compared to baselines. CodePlan is able to get 5/6 repositories to pass the validity checks (e.g., to build\nwithout errors and make correct code edits) whereas the baselines (without planning but with the same type\nof contextual information as CodePlan) cannot get any of the repositories to pass them. We will release our\ndata and evaluation scripts at https://aka.ms/CodePlan.\nCCS Concepts: \u2022 Computing methodologies \u2192 Planning under uncertainty; \u2022 Software and its engi-\nneering \u2192 Software maintenance tools; Software evolution; Automatic programming.\nAdditional Key Words and Phrases: Automated coding, repositories, LLMs, static analysis, plan, chain of edits\n1\nINTRODUCTION\nThe remarkable generative abilities of Large Language Models (LLMs) [24, 28, 30, 35, 57, 73]\nhave opened new ways to automate coding tasks. Tools built on LLMs, such as Amazon Code\nWhisperer [14], GitHub Copilot [38] and Replit [66], are now widely used to complete code\ngiven a natural language intent and context of surrounding code, and also to perform code edits\nbased on natural language instructions [78]. Such edits are typically done for small regions of code\nsuch as completing or editing the current line, or the body of the entire method.\nWhile these tools help with the \"inner loop\" of software engineering where the developer is\ncoding in the editor and editing a small region of code, there are several tasks in the \"outer loop\" of\nsoftware engineering that involve the entire code repository. For example, if our code repository\nuses a library \ud835\udc3f, and the API of library \ud835\udc3f changes from version \ud835\udc63\ud835\udc5b to version \ud835\udc63\ud835\udc5b+1, we need to migrate\nour code repository to correctly invoke the revised version. Such a migration task involves making\nedits not only to all the regions of repository that make calls to the relevant APIs in library \ud835\udc3f, but\n1\narXiv:2309.12499v1  [cs.SE]  21 Sep 2023\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nWe use a Complex Numbers library that had the\nfollowing edit -\n+ class Complex {\n+\nfloat real;\n+\nfloat imag;\n+\ndict<string, string> metadata;\n+ }\n\u2212 tuple<float, float> create_complex(float a,\nfloat b)\n+ Complex create_complex(float a, float b, dict\nmetadata)\nModify the code repository in accordance with this\nchange.\nFig. 1. Task instruction to migrate a code repository due\nto an API change in the Complex Numbers library.\nFig. 2. Overview of CodePlan.\ntuple<tuple<float, float>, dict> func(float a,\nfloat b) {\nstring timestamp = GetTimestamp(DateTime.Now);\nvar c = (create_complex(a,b), new\nDictionary<string, string>()\"time\",\ntimestamp);\nreturn c;\n}\nComplex func(float a, float b) {\nString timestamp = GetTimestamp(DataTime\n.Now);\ndict_metadata = new Dictionary<string,\nstring>(){\"time\", timestamp};\nComplex c = create_complex(a, b,\nmetadata);\nreturn c;\n}\n(a) Create.cs - Original\n(b) Create.cs - Modified (seed edit)\nvoid process(float a, float b, float k) {\nvar c = func(a, b);\nConsole.WriteLine(c[0][0], c[0][1]);\nfloat norm = compute_norm(c[0][0], c[0][1]);\nConsole.WriteLine(norm * k);\n}\nvoid process(float a, float b, float k) {\nComplex c = func(a, b);\nConsole.WriteLine(c.real, c.imag);\nfloat norm = compute_norm(c.real, c.imag\n);\nConsole.WriteLine(norm * k);\n}\n(c) Process.cs - Original\n(d) Process.cs - Modified (derived edit)\nFig. 3. Relevant code snippets from our repository.\nalso to regions of the repository (across file boundaries) having transitive syntactic and semantic\ndependencies on the updated code.\nThis is illustrated in Figure 1, which shows a change in the API for a Complex Numbers library.\nOur task is to migrate our code repository in accordance with this change. The left side of Figure 3\nshows relevant parts of our code repository that use the Complex Numbers library. Specifically, the\nfile Create.cs has the method func, which invokes the create_complex method from the library,\nand Process.cs has the method process which invokes func.\nWe can pass the task description from Figure 1 and the body of func to an LLM to generate the\nrevised code for func as shown in the right side of Figure 3. As seen, the LLM has correctly edited\nthe invocation to the create_complex API so that it returns an object of type Complex instead of\na tuple of two floating point values. Note that this edit has resulted in a change to the signature of\nthe method func \u2013 it now returns an object of type Complex. This necessitates changes to callers of\nmethod func such as the process method in file Process.cs, shown in the left-bottom of Figure 3.\nWithout a suitable change to the body of the process method, our code does not build! A suitable\n2\nCodePlan: Repository-level Coding using LLMs and Planning\nchange to the process method which gets the repository to a consistent state, so that it builds\nwithout errors, is shown in the bottom-right of Figure 3.\nProblem Formulation. The migration task above is representative of a family of tasks that involve\nediting an entire code repository for various purposes such as fixing error reports from static\nanalysis or testing, fixing a buggy coding pattern, refactoring, or adding type annotations or other\nspecifications. Each of these tasks involves a set of seed specifications such as the one shown in\nFigure 1, which are starting points for the code editing task. These seed specifications typically\ntrigger other editing requirements on code, and such requirements need to be propagated across\ndependencies in the code repository to perform other edits across the repository to complete the\ncoding task. Typically, such propagation of edits across dependencies is done manually.\nOur goal is to construct a repository-level coding system, which automatically generates derived\nspecifications for edits such as one required for the process method in Figure 3, in order to get\nthe repository to a valid state. Here, validity is defined with respect to an oracle, which can be\ninstantiated to various ways of enforcing repository-level correctness conditions such as building\nwithout errors, passing static analysis, passing a type system or a set of tests, or passing a verification\ntool. We define an LLM-driven repository-level coding task as follows:\nLLM-driven Repository-level Coding Task\nGiven a start state of a repository \ud835\udc45\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61, a set of seed edit specifications \u0394\ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc60, an oracle \u0398 such\nthat \u0398(\ud835\udc45\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61) = True, and an LLM \ud835\udc3f, the goal of an LLM-driven repository-level coding\ntask is to reach a repository state \ud835\udc45\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61 = \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc62\ud835\udc61\ud835\udc52\ud835\udc38\ud835\udc51\ud835\udc56\ud835\udc61\ud835\udc60(\ud835\udc3f, \ud835\udc45\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61, \ud835\udc43) where \ud835\udc43 is a chain of edit\nspecifications from \u0394\ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc60 \u222a \u0394\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc51 where \u0394\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc51 is a set of derived edit specifications so that\n\u0398(\ud835\udc45\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61) = True.\nProposed Solution. In this paper, we propose a method to compute derived specifications by\nframing (LLM-driven) repository-level coding as a planning problem. Automated planning [37, 67]\naims to solve multi-step problems, where each step executes one action among many alternatives\ntowards reaching a target state. It is used in a wide range of areas such as motion planning [47],\nautonomous driving [39], robotics [44] and theorem proving [26].\nWe present a task-agnostic framework, called CodePlan, which synthesizes a multi-step plan to\nsolve the repository-level coding task. As shown in Figure 2, the input to CodePlan is a repository,\na task with seed specifications expressed through a natural language instruction or a set of initial\ncode edits, a correctness oracle and an LLM. CodePlan constructs a plan graph where each node in\nthe graph identifies a code edit obligation that the LLM needs to discharge and an edge indicates\nthat the target node needs to be discharged consequent to the source node. CodePlan monitors the\ncode edits and adaptively extends the plan graph. The edits \u0394\ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc60 follow from the task description,\nwhereas the edits \u0394\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc51 are identified and contextualized based on a novel combination of\nan incremental dependency analysis, a change may-impact analysis and an adaptive planning\nalgorithm. The merge block merges the code generated by the LLM into the repository. Once all\nthe steps in a plan are completed, the repository is analyzed by the oracle. The task is completed if\nthe oracle validates the repository. If it finds errors, the error reports are used as seed specifications\nfor the next round of plan generation and execution.\nConsider again, the example API migration task specified in Figure 1 on code in Figure 3.\nCodePlan performs the edit of the method func using the instruction in Figure 1 as a seed specifi-\ncation. By analyzing the code change between Figure 3(a)\u2013(b), it classifies the change as an escaping\nchange as it affects signature of method func. The change may-impact analysis identifies that the\ncaller(s) of func may be affected and hence, the adaptive planning algorithm uses caller-callee\n3\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\ndependencies to infer a derived specification to edit the method process, which invokes func.\nBoth the seed and derived changes are executed by creating suitable prompts for an LLM and the\nresulting code repository passes the oracle, i.e., builds without errors. Note that this is a simple\nexample with only one-hop change propagation. In practice, the derived changes can themselves\nnecessitate other changes transitively and CodePlan handles such cases.\nA simpler alternative to our planning is to use the oracle to infer derived specifications. For\nexample, the build system can find the error in the process method after the seed change is made in\nFigure 3. This has important limitations. First, not all changes induce build errors even though they\nresult in behavioral changes, e.g., changing the return value from True to False without changing\nthe return type. Second, the build system is agnostic to cause-effect relationship when code breaks.\nFor example, if the signature of an overriding method is changed as per the seed specification then a\nsimilar change is needed in the corresponding virtual method. However, the build system (when run\non the intermediate, inconsistent snapshot of the repository) blames the overriding method for not\nconforming to the virtual method. Na\u00efvely trying to fix the build error would end up reverting the\nseed change. The static analysis and planning components of CodePlan overcome these limitations.\nWe experimentally compare CodePlan against a baseline that uses a build system to iteratively\nidentify breaking changes and uses an LLM to fix them. Our quantitative and qualitative results\nshow that CodePlan is superior to this kind of oracle-guided repair technique.\nContributions. To the best of our knowledge, the problem of monitoring the effects of code edits\nmade by an LLM to a repository and systematically planning a chain of inter-dependent edits has\nnot been identified and solved before.\nIn the space of repository-level coding tasks, two types of contexts have been found to be useful\nfor prompting LLMs: (1) spatial context to provide cross-file information to the model using static\nanalysis [9, 34, 51, 59, 61, 70, 71, 77] or retrieval [81, 85], and (2) temporal context to condition the\npredictions on the history of edits to the repository [23, 40, 64, 76]. Since CodePlan monitors the\ncode changes and maintains a repository-wide dependency graph, we provide both these forms\nof contexts in a unified framework. The existing techniques assume that the next edit location is\nprovided by the developer and do not account for the effect of an edit on the dependent code. In\ncontrast, by inferring the impact of each change, CodePlan propagates the changes to dependent\ncode, paving a way to automate repository-level coding tasks through chain of edits.\nIn summary, we make the following contributions in this paper:\n(1) We are the first to formalize the problem of automating repository-level coding tasks using\nLLMs, which requires analyzing the effects of code changes and propagating them across\nthe repository. There are currently no systematic and scalable solutions to this problem.\n(2) We frame repository-level coding as a planning problem and design a task-agnostic frame-\nwork, called CodePlan, based on a novel combination of an incremental dependency analysis,\na change may-impact analysis and an adaptive planning algorithm. CodePlan synthesizes a\nmulti-step chain of edits (plan) to be actuated by an LLM.\n(3) We experiment with two repository-level coding tasks using the gpt-4-32k model: package\nmigration for C# repositories and temporal code edits for Python repositories. We compare\nagainst baselines that use the oracles (a build system for C# and a static type checker\nfor Python) for identifying derived edit specifications (in contrast to planning used in\nCodePlan). We use the same contextualization method as CodePlan in the baselines.\n(4) Our results show that CodePlan has better match with the ground truth compared to\nbaselines. CodePlan is able to get 5/6 repositories to pass the validity checks, whereas the\nbaselines cannot get any of the repositories to pass them. Except for the 2 proprietary\nrepositories, we will release our data and evaluation scripts at https://aka.ms/CodePlan.\n4\nCodePlan: Repository-level Coding using LLMs and Planning\n2\nDESIGN\nIn this section, we first give an overview of the CodePlan algorithm for automating repository-level\ncoding tasks (Section 2.1). We then present the static analysis (Section 2.2) and the adaptive planning\nand plan execution (Section 2.3) components of CodePlan.\n2.1\nThe CodePlan Algorithm\n1\n/* Inputs: R is the source code of a repository, Delta_seeds is a set of seed edit\nspecifications, Theta is an oracle and L is an LLM. */\n3\nCodePlan(R, Delta_seeds, Theta, L):\n4\nlet mutable G: PlanGraph = null in\n5\nlet mutable D: DependencyGraph = ConstructDependencyGraph(R) in\n6\nwhile Delta_seeds is not empty\n7\nIntializePlanGraph(G, Delta_seeds)\n8\nAdaptivePlanAndExecute(R, D, G)\n9\nDelta_seeds = Theta(R)\n11\nInitializePlanGraph(G, Delta_seeds):\n12\nfor each \u27e8B, I\u27e9 in Delta_seeds\n13\nAddRoot(G, \u27e8B, I, Pending\u27e9)\n15\nAdaptivePlanAndExecute(R, D, G):\n16\nwhile G has Nodes with Pending status\n17\nlet \u27e8B, I, Pending\u27e9 = GetNextPending(G) in\n18\n// First step: extract fragment of code\n19\nlet Fragmemt = ExtractCodeFragment(B, R, I) in\n20\n// Second step: gather context of the edit\n21\nlet Context = GatherContext(B, R, D) in\n22\n// Third step: use the LLM to get edited code fragment\n23\nlet Prompt = MakePrompt(Fragment, I, Context) in\n24\nlet NewFragment = InvokeLLM(L, Prompt) in\n25\n// Fourth step: merge the updated code fragment into R\n26\nlet R = Merge(NewFragment, B, R) in\n27\nlet Labels = ClassifyChanges(Fragment, NewFragment) in\n28\nlet D' = UpdateDependencyGraph(D, Labels, Fragment, NewFragment, B) in\n29\n// Fifth step: adaptively plan and propogate the effect of the edit on dependant code\n30\nlet BlockRelationPairs= GetAffectedBlocks(Labels, B, D, D') in\n31\nMarkCompleted(B, G)\n32\nfor each \u27e8B\u2019, rel\u27e9 in BlockRelationPairs\n33\nlet N = GetNode(B) in\n34\nlet M = SelectOrAddNode(B', Nil, Pending) in\n35\nAddEdge(G, M, N, rel)\n36\nD := D'\n38\nGatherContext(B, R, D):\n39\nlet SC = GetSpatialContext(B, R) in\n40\nlet TC = GetTemporalContext(G, B) in\n41\n(SC, TC)\nAlgorithm 1: The CodePlan algorithm to automate repository-level coding tasks. The data\nstructures and functions in Cyan and Orchid are explained in Section 2.2\u2013 2.3 respectively.\nThe CodePlan algorithm (Algorithm 1) takes four inputs: (1) the source code of a repository \ud835\udc45,\n(2) a set of seed edit specifications for the task in hand, \u0394\ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc60, (3) an oracle, \u0398, and (4) an LLM, \ud835\udc3f.\nThe core data structure maintained by the algorithm is a plan graph \ud835\udc3a, a directed acyclic graph\nwith multiple root nodes (line 4). Each node in the plan graph is a tuple \u27e8\ud835\udc35, \ud835\udc3c,\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc60\u27e9, where \ud835\udc35 is\na block of code (that is, a sequence of code locations) in the repository \ud835\udc45, \ud835\udc3c is an edit instruction\n(along the lines of the example shown in Figure 1),\n5\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nand \ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc60 is either \ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54 or \ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc61\ud835\udc52\ud835\udc51.\nThe CodePlan algorithm also maintains a dependency graph \ud835\udc37 (line 5). Figure 4 illustrates the\ndependency graph structure. We will discuss it in details in Section 2.2.1. For now, it suffices to\nknow that the dependency graph \ud835\udc37 represents the syntactic and semantic dependency relations\nbetween code blocks in the repository \ud835\udc45.\nThe loop at lines 6\u20139 is executed until \u0394\ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc60 is non-empty. Line 7 calls the InitializePlanGraph\nfunction (lines 11\u201313) that adds all the changes in \u0394\ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc60 as root nodes of the plan graph. Each edit\nspecification comprises of a code block \ud835\udc35 and an edit instruction \ud835\udc3c.\nThe status is set to pending for the root nodes (line 13). The function AdaptivePlanAndExecute\nis called at line 8 which executes the plan, updates the dependency graph with each code change\nand extends the plan as necessary. Once the plan graph is completely executed, the oracle \u0398 is run\non the repository. It returns error locations and diagnostic messages which form \u0394\ud835\udc60\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc60 for the next\nround. If the repository passes the oracle\u2019s checks then it returns an empty set and the CodePlan\nalgorithm terminates.\nWe now discuss AdaptivePlanAndExecute, which is the main work horse. It iteratively picks\neach pending node and processes it. Processing a pending node with an edit specification for a\nblock \ud835\udc35 with edit instruction \ud835\udc3c involves the following five steps:\n(1) The first step (line 19) is to extract the fragment of code to edit. Simply extracting\ncode of the block \ud835\udc35 loses information about relationship of \ud835\udc35 with the surrounding code.\nKeeping the entire file on the other hand takes up prompt space and is often unnecessary.\nWe found the surrounding context is most helpful when a block belongs to a class. For such\nblocks, we sketch the enclosing class. That is, in addition to the code of block \ud835\udc35, we also\nkeep declarations of the enclosing class and its members. As we discuss later, this sketched\nrepresentation also helps us merge the LLM\u2019s output into a source code file more easily.\n(2) The second step (line 21) is to gather the context of the edit. The context of the edit\n(line 38\u201341) consists of (a) spatial context, which contains related code such as methods\ncalled from the block \ud835\udc35, and (b) temporal context, which contains the previous edits that\ncaused the need to edit the block \ud835\udc35. The temporal context is formed by edits along the paths\nfrom the root nodes of the plan graph to \ud835\udc35.\n(3) The third step (lines 23\u201324) constructs a prompt for the edit using the fragment extracted\nin the first step, the instruction \ud835\udc3c from the edit specification and the context extracted in\nthe second step, and invokes the LLM using the prompt to get the edited code fragment.\n(4) The fourth step (lines 26\u201328) merges the edited code back into the repository. Since\nthe code is updated, many dependency relationships such as caller-callee, class hierarchy,\netc. may need to change, and hence, this step also updates the dependency graph \ud835\udc37.\n(5) The fifth and final step (lines 30\u201335) does adaptive planning to propagate the effects\nof the current edit on dependant code blocks. This involves classifying the change in\nthe edited block, and depending on the type of change, picking the right dependencies in the\ndependency graph to traverse and locate affected blocks. For instance, if the edit of a method\n\ud835\udc5a in the current block \ud835\udc35 involves update to the signature of the method, then all callers\nof \ud835\udc5a get affected (the scenario in Figure 3). For each affected block \ud835\udc35\u2032 and the dependency\nrelation rel connecting \ud835\udc35 to \ud835\udc35\u2032 in the dependency graph, we get a pair \u27e8\ud835\udc35\u2032, rel\u27e9. If a node\nexists for \ud835\udc35\u2032 in the plan graph and it is pending, then we add an edge from \ud835\udc35 to \ud835\udc35\u2032 labeled\nwith rel to the plan graph. Otherwise, the edge is added to a newly created node for \ud835\udc35\u2032\n(line 34). The block \ud835\udc35 is marked as completed (line 31).\n6\nCodePlan: Repository-level Coding using LLMs and Planning\nFig. 4. Illustration of the dependency graph annotated with relations as the edge labels.\n2.2\nStatic Analysis Components\nWe now turn our attention to the static analysis components used in CodePlan. We will cover all\nthe data structures and functions in Cyan background from Algorithm 1.\n2.2.1\nIncremental Dependency Analysis. An LLM can be provided a code fragment and an instruc-\ntion to edit it in a prompt. While the LLM may perform the desired edit accurately, analyzing the\nimpact of the edit on the rest of the repository is outside the scope of the LLM call. We believe static\nanalysis is well-suited to do this and propose an incremental dependency analysis for the same.\nDependencyGraph. Dependency analysis [12] is used for tracking syntactic and semantic relations\nbetween code elements. In our case, we are interested in relations between import statements,\nmethods, classes, field declarations and statements (excluding those that operate only on variables\ndefined locally within the enclosing method). Formally, a dependency graph D = (\ud835\udc41, \ud835\udc38) where \ud835\udc41\nis a set of nodes representing the code blocks mentioned above and \ud835\udc38 is a set of labeled edges\nwhere the edge label gives the relation between the source and target nodes of the edge. Figure 4\nillustrates all the relations we track as labeled edges. The relations include (1) syntactic relations\n(ParentOf and ChildOf, Construct and ConstructedBy) between a block \ud835\udc50 and the block \ud835\udc5d that\nencloses \ud835\udc50 syntactically; a special case being a constructor and its enclosing class related by\nConstruct and ConstructedBy, (2) import relations (Imports and ImportedBy) between an import\nstatement and statements that use the imported modules, (3) inheritance relations (BaseClassOf\nand DerivedClassOf) between a class and its superclass, (4) method override relations (Overrides\nand OverridenBy) between an overriding method and the overriden method, (5) method invocation\nrelations (Calls and CalledBy) between a statement and the method it calls, (6) object instantiation\nrelations (Instantiates and InstantiatedBy) between a statement and the constructor of the object it\ncreates, and (7) field use relations (Uses and UsedBy) between a statement and the declaration of a\nfield it uses.\nConstructDependencyGraph. The dependency relations are derived across the source code\nspread over the repository through static analysis. We represent the source code of a repository as a\nforest of abstract syntax trees (ASTs) and add the dependency edges between AST sub-trees. A file-\nlocal analysis derives the syntactic and import relations. All other relations require an inter-class,\ninter-procedural analysis that can span file boundaries. In particular, we use the class hierarchy\nanalysis [32] for deriving the semantic relations.\nClassifyChanges. As discussed in Section 2.1, in the fourth step, CodePlan merges the code\ngenerated by the LLM into the repository. By pattern-matching the code before and after, we classify\nthe code changes. Table 1 (the first and second columns) gives the types of atomic changes and\n7\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nAtomic\nChange\nLabel Dependency Graph Update\nChange May-Impact Analysis\nModification Changes\nBody\nof\nmethod M\nMMB Recompute the edges incident on the\nstatements in the method body.\nIf an escaping object is modified then\nRel(D, M, CalledBy) else Nil.\nSignature\nof\nmethod M\nMMS\nRecompute the edges incident on the\nmethod.\nRel(D, M, CalledBy), Rel(D, M, Over-\nrides), Rel(D, M, OverriddenBy), Rel(D\u2032,\nM, Overrides), Rel(D\u2032, M, Overrid-\ndenBy)\nField F in class\nC\nMF\nRecompute the edges incident on the\nfield.\nRel(D, F, UsedBy), Rel(D, C, Construct-\nedBy), Rel(D, C, BaseClassOf), Rel(D, C,\nDerivedClassOf)\nDeclaration of\nclass C\nMC\nRecompute the edges incident on the\nclass.\nRel(D, C, InstantiatedBy), Rel(D, C, Base-\nClassOf), Rel(D, C, DerivedClassOf),\nRel(D\u2032, C, BaseClassOf), Rel(D\u2032, C, De-\nrivedClassOf)\nSignature\nof\nconstructor of\nclass C\nMCC\nNo change.\nRel(D, C, InstantiatedBy), Rel(D, C, Base-\nClassOf), Rel(D, C, DerivedClassOf)\nImport/Using\nstatement I\nMI\nRecompute the edges incident on the\nimport statement.\nRel(D, I, ImportedBy)\nAddition Changes\nMethod M in\nclass C\nAM\nAdd new node and edges by analyzing\nthe method. If C.M overrides a base class\nmethod B.M then redirect the Calls/-\nCalledBy edges from B.M to C.M if the\nreceiver object is of type C.\nRel(D, C, BaseClassOf), Rel(D, C, De-\nrivedClassOf), Rel(D\u2032, M, CalledBy)\nField F in class\nC\nAF\nAdd new node and edges by analyzing\nthe field declaration.\nRel(D, C, ConstructedBy), Rel(D, C,\nBaseClassOf), Rel(D, C, DerivedClas-\nsOf)\nDeclaration of\nclass C\nAC\nAdd new node and edges by analyzing\nthe class declaration.\nNil\nConstructor of\nclass C\nACC\nAdd new node and edges by analyzing\nthe constructor.\nRel(D, C, InstantiatedBy), Rel(D, C, Base-\nClassOf), Rel(D, C, DerivedClassOf)\nImport/Using\nstatement I\nAI\nAdd new node and edges by analyzing\nthe import statement.\nNil\nDeletion Changes\nMethod M in\nclass C\nDM\nRemove the node for M and edges in-\ncident on M. If C.M overrides a base\nclass method B.M then redirect the Call-\ns/CalledBy edges from C.M to B.M if the\nreceiver object is of type C.\nRel(D, M, CalledBy), Rel(D, M, Over-\nrides), Rel(D, M, OverriddenBy)\nField F in class\nC\nDF\nRemove the node of the field and edges\nincident on it.\nRel(D, F, UsedBy), Rel(D, C, Construct-\nedBy), Rel(D, C, BaseClassOf), Rel(D, C,\nDerivedClassOf)\nDeclaration of\nclass C\nDC\nRemove the node of the class and edges\nincident on it.\nRel(D, C, InstantiatedBy), Rel(D, C, Base-\nClassOf), Rel(D, C, DerivedClassOf)\nConstructor of\nclass C\nDCC\nRemove the edges incident on the class\ndue to object instatiations using the con-\nstructor.\nRel(D, C, InstantiatedBy), Rel(D, C, Base-\nClassOf), Rel(D, C, DerivedClassOf)\n8\nCodePlan: Repository-level Coding using LLMs and Planning\ntheir labels. Broadly, the changes are organized as modification, addition and deletion changes, and\nfurther by which construct is changed. We distinguish between method body and method signature\nchanges. Similarly, we distinguish between changes to a class declaration, to its constructor or to\nits fields. The changes to import statements or the statements that use imports are also identified.\nThese are atomic changes. An LLM can make multiple simultaneous edits in the given code fragment,\nresulting in multiple atomic changes, all of which are identified by the ClassifyChanges function.\nUpdateDependencyGraph. As code generated by the LLM is merged, the dependency relations\nassociated with the code at the change site are re-analyzed. Table 1 (the third column) gives the\nrules to update the dependency graph D to D\u2032 based on the labels inferred by ClassifyChanges. For\nmodification changes, we recompute the relations of the changed code except for constructors.\nA constructor is related to its enclosing class by a syntactic relation which does not have to be\nrecomputed. For addition changes, new nodes and edges are created for the added code. Edges\ncorresponding to syntactic relations are created in a straightforward manner. If a change simulta-\nneously adds an element (an import, a method, a field or a class) and its uses, we create a node for\nthe added element before analyzing the statements that use it. Addition of a method needs special\nhandling as shown in the table: if an overriding method C.M is added then the Calls/CalledBy edges\nincident on the matching overriden method B.M are redirected to C.M if the call is issued on a\nreceiver object of type C. The deletion of an overriding method requires an analogous treatment as\nstated in Table 1. All other deletion changes require removing nodes and edges as stated in the\ntable.\n2.2.2\nChange May-Impact Analysis. In the fifth step, CodePlan identifies the code blocks that may\nhave been impacted by the code change by the LLM. Let Rel(D, B, rel) be the set of blocks that are\nconnected to a block B via relation rel in the dependency graph D. Let D and D\u2032 be the dependency\ngraph before and after the updates in Table 1.\nGetAffectedBlocks. The last column in Table 1 tells us how to identify blocks affected by a\ncode change for each type of change. When the body of a method M is edited, we perform escape\nanalysis [22, 29] to identify if any object accessible in the callers of M (an escaping object) has\nbeen affected by the change. If yes, the callers of M (identified through Rel(D, M, CalledBy)) are\nidentified as affected blocks. Otherwise, the change is localized to the method and there are no\naffected blocks. If the signature of a method is edited, the callers and methods related to it through\nmethod-override relation in the inheritance hierarchy are affected. The signature change itself\ncan affect the Overrides and OverridenBy relations, e.g., addition or deletion of the @Override\naccess modifier. Therefore, the blocks related by these relations in the updated dependency graph\nD\u2032 are also considered as affected as shown in Table 1 (the row with MMS label). When a field F\nof a class C is modified, the statements that use F, the constructors of C and sub/super-classes of\nC are affected. When a class is modified, the methods that instantiate it and its sub/super-classes\nas per D and D\u2032 are affected. A modification to a constructor has a similar rule except that such\na change does not change inheritance relations and hence, only D is required. When an import\nstatement I is modified, the statements that use the imported module are affected.\nThe addition and deletion changes are less complex than the modification changes, and their rules\nare designed along the same lines as discussed above. In the interest of space, we do not explain\neach of them step-by-step. We assume that there is no use of a newly added class or an import in\nthe code. Therefore, adding them does not result in any affected blocks. In our experiments, we\nhave found the rules in Table 1 to be adequate. However, CodePlan can be easily configured to\naccommodate variations of the rules in Table 1 if necessary.\n9\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\n2.3\nAdaptive Planning and Plan Execution\nWe now discuss the data structures and functions from Algorithm 1 in the Orchid background.\n2.3.1\nAdaptive Planning. Having identified the affected blocks (using GetAffectedBlocks), CodePlan\ncreates change obligations that need to be discharged using an LLM to make the dependent code\nconsistent with the change. As discussed in Section 2.1, this is an iterative process.\nPlanGraph. A plan graph P = (\ud835\udc42,\ud835\udc36) is a directed acyclic graph with a set of obligations \ud835\udc42, each of\nwhich is a triple \u27e8\ud835\udc35, \ud835\udc3c,\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc60\u27e9 where B is a block, I is an instruction and status is either pending\nor completed. An edge in \ud835\udc36 records the cause, the dependency relation between the blocks in the\nsource and target obligations. In other words, the edge label identifies which Rel clause in a change\nmay-impact rule in Table 1 results in creation of the target obligation.\nExtractCodeFragment. As discussed in the first step in Section 2.1, simply extracting code for a\nblock B is sub-optimal as it loses context. The ExtractCodeFragment function takes the whole class\nthe code block belongs to, keeps the complete code for B and retains only declarations of the class\nand other class members. We found this to be useful because the names and types of the class\nand other members provide additional context to the LLM. Often times the LLM needs to make\nmultiple simultaneous changes. For example, in some of our case studies, the LLM has to add a\nfield declaration, take an argument to a constructor and use it within the constructor to initialize\nthe field. Providing the sketch of the surrounding code as a code fragment to the LLM allows the\nLLM to make these changes at the right places. The code fragment extraction logic is implemented\nby traversing the AST and \"folding\" away the subtrees (e.g., method bodies) that are sketched. As\nstated in Section 1, this sketched representation also allows us to place the LLM generated code\nback into the AST without ambiguity, even when there are multiple simultaneous changes.\nGetSpatialContext. Spatial context in CodePlan refers to the arrangement and relationships of\ncode blocks within a codebase, helping understand how classes, functions, variables, and modules\nare structured and interact. It\u2019s crucial for making accurate code changes. CodePlan utilizes the\ndependency graph to extract spatial context, representing code as nodes and their relationships\nas edges. This graph enables CodePlan to navigate codebases, identify relevant code blocks, and\nmaintain awareness of their spatial context. As a result, when generating code edits, the dependency\ngraph empowers CodePlan to make context-aware code modifications that are consistent with the\ncode\u2019s spatial organization, enhancing the accuracy and reliability of its code editing capabilities.\nGetTemporalContext. The plan graph records all change obligations and their inter-dependences.\nExtracting temporal context is accomplished by linearizing all paths from the root nodes of the plan\ngraph to the target node. Each change is a pair of the code fragments before and after the change.\nThe temporal context also states the \"causes\" (recorded as edge labels) that connect the target node\nwith its predecessor nodes. For example, if a node A is connected to B with a CalledBy edge, then\nthe temporal context for B is the before/after fragments for A and a statement that says that \"B\ncalls A\", which helps the LLM understand the cause-effect relation between the latest temporal\nchange (change to A) and the current obligation (to make a change to B).\n2.3.2\nPlan Execution. CodePlan iteratively selects a pending node in the plan graph and invokes\nan LLM to discharge the change obligation.\nMakePrompt. Having extracted the code fragment to be edited along with the relevant spatial and\ntemporal context, we construct a prompt to pass to the LLM with the structure given below. We\nopen with the task specific instructions p1 followed by listing the edits made in the repository so\nfar p2 that are relevant to the fragment being edited. The next section p3 notes how each of the\n10\nCodePlan: Repository-level Coding using LLMs and Planning\nfragments present in p2 are related to the fragment to be edited. This is followed by the spatial\ncontext p4 and the fragment to the edited p5 .\nPrompt Template\np1 Task Instructions: Your task is to . . .\np2 Earlier Code Changes (Temporal Context): These are edits that have been made in\nthe code-base previously -\nEdit 1:\nBefore: \u00abcode_before\u00bb\nAfter: \u00abcode_after\u00bb\n\u00b7 \u00b7 \u00b7\np3 Causes for Change: The change is required due to -\n\u00abcode_to_be_edited\u00bb is related to \u00abcode_changed_earlier\u00bb by \u00abcause\u00bb\n\u00b7 \u00b7 \u00b7\np4 Related Code (Spatial Context): The following code maybe related -\n\u00abrelated_code_block-1\u00bb\n\u00b7 \u00b7 \u00b7\np5 Code to be Changed Next: The existing code is given below -\n\u00abcode_to_be_edited\u00bb\nEdit the \"Code to be Changed Next\" and produce \"Changed Code\" below. Edit the \"Code\nto be Changed Next\" according to the \"Task Instructions\" to make it consistent with\nthe \"Earlier Code Changes\", \"Causes for Change\" and \"Related Code\". If no changes are\nneeded, output \"No changes.\"\nOracle and Plan Iterations. Once all the nodes in the plan graph are marked as completed and no\nnew nodes are added, an iteration of repository-level code edits is completed. As shown in Figure 2,\nthe oracle is invoked on the repository. If the oracle flags any errors (e.g., build errors), the error\nlocations and diagnostic messages are added as seed changes for the next iteration and the adaptive\nplanning resumes once again. If the oracle does not flag any errors, CodePlan terminates.\n3\nIMPLEMENTATION\nIn this section, we provide a detailed overview of the implementation components that constitute\nthe core of our method.\nDependency Graph Construction. At the core of the CodePlan methodology lies the Dependency\nGraph, which is instrumental in representing the intricate relationships between code blocks. To\nbuild this Dependency Graph from a code repository, we adopt a systematic approach. Initially, we\n11\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nFig. 5. AST structure for a C# code snippet produced by tree-sitter.\nparse all the code files within the repository, utilizing the tree-sitter library [25] to generate an\nAST-like structure. This structured representation simplifies the identification of various funda-\nmental code blocks within the codebase. For instance, Figure 5 exemplifies an AST structure for a\nC# code snippet produced by tree-sitter. Code blocks are identified at different levels, including\nClasses, Methods, import statements, and non-class expressions. For instance, in Figure 5, the\nsubtree rooted at the class_declaration node corresponds to the SyncSubscriberTest class.\nRelation Identification in C#. In the context of C# repositories, the establishment of edges within\nthe Dependency Graph involves the careful tracing of relationships within the AST. We have devised\ncustom logic for each type of relationship outlined in Figure 4, encompassing vital connections\nsuch as caller-callee, overrides-overridden, base class-derived class, and others. To illustrate, for\nthe Caller/Callee relationship, we search for invocation_expression nodes within the AST.\nSubsequently, we process the sub-tree beneath these nodes to resolve essential details such as the\ntarget class and the invoked method\u2019s name. Armed with this information, we create Calls/CalledBy\nrelation links between the code block initiating the method call and the corresponding method\nblock within the target class. While we have implemented custom logic for these relations, it\u2019s\nimportant to note that alternative dependency analysis tools for C# such as Language Servers for\nC# (LSP) [5], CodeQL [2], or similar solutions can also be integrated into our system, owing to its\ninherent flexibility.\nRelation Identification in Python. For Python repositories, we use Jedi [4] - a static analysis tool\nwhich discovers references and declarations of symbols throughout the codebase. These capabilities\n12\nCodePlan: Repository-level Coding using LLMs and Planning\nare harnessed to identify edges in the Dependency Graph for relationships such caller-callee,\noverrides-overridden, and base class-derived class.\nIntegration of GPT-4 for Code Edits. CodePlan leverages the remarkable capabilities of GPT-4 [57]\nto perform code edits effectively. During the construction of input data for the edit model, we\nmeticulously provide temporal context, spatial context, and the actual code to be edited in the form\nof code snippets. These code snippets represent classes or methods that contain the edit site and\nare meticulously structured in a sketched representation, as stated in Section 2.1. This sketched\nrepresentation ensures that the model is enriched with a substantial context for each edit site,\nsignificantly enhancing the quality and accuracy of the edits it generates.\nLanguage Extensibility. While our current implementation proficiently supports C# and Python\nrepositories, extending support to repositories in other programming languages is a straightforward\nendeavor. It primarily entails creating a dependency graph with the relations identified in Figure 4\nand incorporating it into the CodePlan framework, thereby allowing for seamless adaptation to a\ndiverse array of programming languages.\n4\nEXPERIMENTAL DESIGN\nIn this section, we\u2019ll explain how we conducted our experiments to test CodePlan. We\u2019ll start by\ntalking about the different sets of data we used. Then, we\u2019ll discuss the methods we compared\nCodePlan to, which are like our reference points. Finally, we\u2019ll explain how we measured the results\nto see how well CodePlan performed compared to the other methods.\n4.1\nDatasets\nIn our experiments, we utilized diverse datasets representing a wide spectrum of code reposito-\nries with varying complexities and sizes. These datasets allowed us to thoroughly evaluate the\nperformance of CodePlan in different real-world scenarios.\nInternal Repositories (Int-1 and Int-2). These repositories are proprietary and belong to a large\nproduct company. They are characterized by their large size, complex patterns, and are typical of\nproduction-level codebases. The primary task we focused on here was the migration of these repos-\nitories from a legacy logging framework to a modern logging framework. This migration involved\nnon-trivial changes, including creating service-specific loggers using a logging factory, passing the\nlogger through call chains, managing class hierarchies, storing logger references at different scopes\n(class, method, etc.), and handling loggers at both static and non-static classes/methods. These two\nproduction repositories, Int-1 and Int-2, were chosen for their distinct coding styles and design\npatterns, providing a comprehensive internal dataset for our evaluation.\nExternal Repositories (Public GitHub). We also considered external repositories from GitHub\nto diversify our dataset. These repositories were chosen to represent two different coding tasks:\nMigration and Temporal Edits (discussed next).\nMigration Task. This task involves migrating APIs or addressing breaking changes within a\ncodebase. Examples include updating dependencies, adapting to changes in external libraries, or\naligning code with new coding standards. It is characterized by its complexity, often requiring\nconsistent updates across numerous code files and dependencies. To select these repositories, we\nlooked for those containing commits and pull requests related to various kinds of migrations (API,\nframeworks, etc.). We filtered for repositories with at least 50 files.\nTemporal Edits Task. This task involves orchestrating a sequence of code changes given some\ninitial code-change. Many code-changes can be characterised as temporal edits including refactoring\n13\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nMigration\n(C#)\nTemporal Edits\n(Python)\nRepositories\nInt-1\nInt-2\nExt-1\nT-1\nT-2\nT-3\nNumber of files\n91\n168\n55\n21\n137\n4\nLines of code\n8853\n16476\n8868\n3883\n20413\n1874\nNumber of files changed\n47\n97\n21\n2\n2\n3\nNumber of seed changes\n41\n63\n42\n2\n1\n1\nNumber of derived changes\n110\n375\n16 0\n8\n3\n10\nSize of diff (lines)\n1744\n4902\n1024\n104\n15\n39\nSize of seed edits (lines)\n242\n242\n379\n76\n4\n1\nPrompt template size (lines)\n81\n81\n81\n75\n75\n75\nURL\n-\n-\n[7]\n[8]\n[1]\n[3]\nTable 2. Dataset statistics. Int-1,2 are internal (proprietary) repositories, Ext-1 and T-1,2,3 are external (public\nGitHub) repositories.\nor addition/removal of functionality. A temporal edit task is specified by a set of initial edits (usually\nmade by the user) along with derived edits precipitated by the seed edits. The task for the tool is to\ninfer the derived edits from the set of initial edits. An example temporal edit task can be where\nthe initial edit is adding an argument to a method and the derived edits are changes to all the\nplaces where this method is called. We identify temporal edit tasks from commits made in public\nrepositories on GitHub. We consider python repositories with permissive licenses, sort by stars,\nfilter out documentation/tutorial related repositories and from amongst candidates with at least\n10,000 stars, we select commits made after 1st November 2021 (after the cut-off date for training\ndata of GPT-4) containing multiple related changes.\nSource/Target/Predicted Repositories. To collect the code changes for the migration and temporal\nedit tasks, we obtained files from GitHub before and after the commits. We refer to these as the\nSource repository (before commit) and Target repository (after commit). Analyzing these changes\nallowed us to identify seed changes through manual inspection. For instance, in the NUnit to\nXUnit migration, one of the seed edits involved replacing Console.WriteLine with writing to an\nITestOutputHelper object. With the Source repository and seed change instructions, CodePlan\nwas tasked with making the necessary changes to the Source repository, resulting in what we call\nthe Predicted repository. If CodePlan successfully executed all changes, the Predicted repository\nshould match the Target repository, providing a robust evaluation of its capabilities.\nPreprocessing Source and Target Repositories. When dealing with large repositories, it\u2019s common\nfor multiple developers to contribute code, resulting in various coding styles driven by individual\npreferences. For instance, one developer might consistently use the this qualifier to reference\nclass members, while another may not. When CodePlan executes changes through LLM prompting,\nit tends to establish a uniform style throughout the codebase, which may involve enforcing the\nconsistent use of the this qualifier, among other things. While these changes ensure functional\nequivalence, they can impact evaluation metrics when comparing the Predicted repository with\nthe Target repository. To address these issues, we employ a manual preprocessing step on both the\nSource and Target repositories. This preprocessing aims to establish uniformity across various files\nwithin the repository. By doing so, we provide a fair basis for comparing CodePlan\u2019s performance\nagainst the ground truth in these diverse codebases.\n14\nCodePlan: Repository-level Coding using LLMs and Planning\nTable 2 summarizes the dataset statistics for both Migration (C#) and Temporal Edits (Python)\nrepositories, including the names of repositories (both internal and external) and various key\nstatistics including their size, code changes, and other relevant metrics:\n\u2022 Number of Files: The total count of files in each repository.\n\u2022 Lines of Code: The cumulative number of lines of code across all files.\n\u2022 Number of Files Changed: The count of files that have undergone changes between the\nsource and target repositories.\n\u2022 Number of Seed Changes: The number of initial edits, often considered as the starting point\nof code changes.\n\u2022 Number of Derived Changes: The count of subsequent edits that follow the initial seed\nchanges.\n\u2022 Size of Diff : The number of lines that differ between the source and target repositories.\n\u2022 Size of Seed Edits: When the seed edits are made directly on the code, it represents the\nnumber of lines of initial edits. When the seed edits are made through LLM instruction, it\ndenotes the size of the instruction text.\n\u2022 Prompt Template Size: This number represents the size of the LLM prompt template used\nby CodePlan. The same template is employed for all the migration repository tasks, and\nanother similar template is utilized for all the Temporal Edit repository tasks.\nThese metrics not only offer a comprehensive overview of the dataset characteristics but also\nhighlight the considerable advantages of using CodePlan over manual processes, especially for\nlarge repositories. In manual scenarios, human effort is required to painstakingly identify dependent\nchanges and implement each modification. Notably, metrics such as \"Size of Diff\" and \"Size of Seed\nEdits\" provide insights into the developer effort involved. CodePlan, on the other hand, automates\nthese changes, effectively reducing the burden on developers. Furthermore, it\u2019s worth noting that\nthe effort required to craft LLM instructions for CodePlan is significantly less than the extensive\nmanual labor required for making all the code changes. These metrics collectively demonstrate\nthe efficiency and effectiveness of CodePlan across diverse codebases, emphasizing its potential to\nstreamline development workflows and save valuable developer time.\n4.2\nOracles and Baselines\nOracles. Recall that our definition of repository-level coding tasks is centered around satisfying an\noracle that can determine the validity of our solution. In our experiments, we consider two specific\ninstatiations of such an oracle. For the C# migration tasks, we define the oracle as passing the C#\nbuild tools without any errors. For temporal edits scenario, we use Pyright [6], a static checker for\npython as the oracle.\nOracle-Guided Repair. Both of these oracles take a codebase as input and can output a list of\nerrors in that codebase. This leads naturally to the formulation of baseline approaches to our tasks\nwhich we term as Oracle-Guided Repair. These are simple reactive approaches where at each step\nwe attempt to rectify the errors flagged by the oracle. For the C# migration scenario, the baseline is\nBuild-Repair and for temporal edits it is Pyright-Repair according to the oracles used in both.\nThe oracle-guide repair works in the following steps:\n(1) Initial Edit: The process begins with application of the initial seed edit to the codebase.\n(2) Build and Error Detection: After the seed edits, we invoke the oracle which detects errors in\nthe codebase arising from the seed edits.\n(3) Error Message Analysis: The error messages generated by the oracle is then parsed to\nprecisely identify the location of the error within the code.\n15\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\n(4) LLM Patching: Subsequently, the error message, along with the code fragment from the\nflagged location, is passed to an LLM. The LLM leverages its code generation capabilities to\ngenerate a patch or fix for the identified error. For fair comparison with CodePlan, we use\nour implementation for spatial and temporal context extraction in oracle-guided repair. That\nis, the key difference between CodePlan and oracle-guided repair is that CodePlan uses\nadaptive planning whereas oracle-guided repair uses the diagnostic information generated\nby the oracle, but they use the same contextualization machinery.\nNote that Oracle-Guided Repair approaches are reactive and lack a comprehensive \"change\nmay-impact analysis.\" This means that they may not thoroughly assess how the proposed code\nchanges could affect other parts of the codebase. As a result, the fixes generated by such approaches\nmay be incomplete or incorrect, especially when dealing with complex coding tasks.\nAlternate Edit Model: Coeditor [76]. CodePlan by default leverages the text and code processing\nabilities to LLMs to make local edits to code fragments given proper context. However, in theory, the\nincremental dependancy analysis, change may-impact analysis and adaptive planning components\nof CodePlan can be used in conjunction with any tool or model that can make localized edits to\ncode following some provided intent. Coeditor [76] is a transformer based model fine-tuned to edit\ncode snippets while taking into account prior edits made in the repository. Such a model is good fit\nfor the Temporal Edits task, where we need to make a sequence of edits from a set of seed edits,\nwhere each edit is dependant on some subset of the previous edits. Indeed, Coeditor is evaluated on\nthe Temporal Edits task in [76]. In order to demonstrate the generality of our analysis and planning,\nwe evaluate how our approach performs in the temporal edits scenario, when replacing gpt-4-32k\nwith Coeditor as the edit model.\n4.3\nEvaluation Metrics\nThe evaluation metrics employed in our study are aimed at assessing how effectively CodePlan (or\na baseline) propagates changes across the entire code repository and the correctness of each of\nthese changes. To achieve this, we rely on two key metrics: Block Metrics and Edit Metrics.\nBlock Metrics. Block Metrics help us understand CodePlan\u2019s ability to accurately identify code\nblocks in need of modification. These metrics include:\n\u2022 Matched Blocks: These are code blocks that exist in the Source Repository, have been edited\nin the Target Repository, and have also been edited in the Predicted Repository. Essentially,\nthese are blocks that CodePlan successfully identifies for change.\n\u2022 Missed Blocks: Missed Blocks refer to code blocks present in the Source Repository that\nhave been edited in the Target Repository but were not edited in the Predicted Repository.\nIn other words, these are blocks that CodePlan failed to modify when it should have.\n\u2022 Spurious Blocks: Spurious Blocks are code blocks found in the Source Repository that were\nnot edited in the Target Repository but were incorrectly edited by CodePlan in the Predicted\nRepository. These represent edits that CodePlan made unnecessarily.\nThe ideal outcome is to have a high number of Matched Blocks and low numbers of Missed and\nSpurious Blocks.\nEdit Metrics. While Block Metrics assess the identification of code blocks, Edit Metrics delve into\nthe correctness of the modifications made by CodePlan. These metrics include:\n\u2022 Levenshtein Distance: Levenshtein Distance measures the edit distance at the file level\nbetween the Predicted Repository and the Target Repository. It calculates how many changes\n16\nCodePlan: Repository-level Coding using LLMs and Planning\nwere made to transform one file into the other. A higher Levenshtein Distance indicates\nthat CodePlan did not make the correct changes to the repository.\n\u2022 Diff BLEU: Typically, we use BLEU [58], a common metric in Natural Language Processing,\nto measure text similarity. However, when applied in our tasks, BLEU can produce overly\nhigh similarity scores because code edits for specific tasks often involve only a small portion\nof the file. To address this issue, we compute Diff BLEU: a modified version of the BLUE\nscore, denoted as BLUE(DIFF(Source repo file, Target repo file), DIFF(Source\nrepo file, Predicted repo file)). Here, DIFF calculates the differences (diff hunks)\nbetween two files. What sets Diff BLUE apart is its focus on comparing the modified sections\nof code between the Predicted and Target Repositories while disregarding common code.\nWhen the modifications in Predicted and Target Repositories match precisely, Diff BLUE\nyields a score of 1.0, indicating a high level of correctness and alignment in handling code\nmodifications.\nIn summary, these evaluation metrics provide a comprehensive assessment of CodePlan\u2019s perfor-\nmance, both in terms of identifying code blocks for modification and ensuring that the modifications\nmade are correct.\n5\nRESULTS AND ANALYSIS\nIn this section, we present empirical results to answer the following research questions:\nRQ1: How well CodePlan is able to localize and make the required changes to automate repository-\nlevel coding tasks?\nRQ2: How important are the temporal and spatial contexts for CodePlan\u2019s performance?\nRQ3: What are the key differentiators that allow CodePlan to outperform baselines in solving\ncomplex coding tasks?\n5.1\nRQ1: How well CodePlan is able to localize and make the required changes to\nautomate repository-level coding tasks?\nMotivation. The research question addressing the effectiveness of the CodePlan framework in\nautomating repository-level coding tasks is of paramount importance in the context of modern\nsoftware engineering. Several key motivations drive the significance of this inquiry:\n\u2022 Complexity of Repository-Level Tasks: Software engineering activities, such as pack-\nage migration and temporal code edits, often transcend the scope of local code changes.\nRepository-level coding tasks involve pervasive modifications across the entire codebase.\nThis level of complexity necessitates novel approaches to ensure efficiency and correctness.\n\u2022 Real-world Relevance: In practice, software repositories frequently encounter the need\nfor large-scale changes. For instance, package migration involves updating dependencies\nacross multiple files and dependencies, while temporal code edits require tracking and\nmanaging evolving codebase. These tasks are not only time-consuming but also error-prone\nwhen done manually.\n\u2022 Evaluation against Baselines: The assessment of CodePlan against baseline methods\nis crucial. Baseline methods, such as \"Oracle-Guided Repair\", are common in software\ndevelopment but may lack efficiency when dealing with repository-level tasks. Evaluating\nCodePlan against baselines provides a benchmark for measuring its effectiveness and\nhighlights areas where it excels. We also study how our system behaves when using a\ndifferent edit model by evaluating a combination of Coeditor and CodePlan on the Temporal\nEdits Task.\n17\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\n\u2022 Large-scale Repositories: The research considers not only isolated coding problems but\nevaluates CodePlan\u2019s performance on large internal and external repositories. This broad\nscope ensures that the framework\u2019s effectiveness is tested under diverse and challenging\nreal-world scenarios.\nExperimental Setup.\nTo study how well CodePlan is able to localize and make the required changes for repository-\nlevel tasks, we evaluate it in the context of the tasks described in 4.1. For both the C# Migration\nand Temporal Edits tasks, we start with the repository in its Source state without any edits having\nbeen made. We apply the seed edits on top of the Source state at which point CodePlan (or the\nbaseline being evaluated) takes over to complete the the edit across the repository. In the case of C#\nMigrations, the seed edits themselves are performed using the LLM with a suitable prompt, while\nfor Temporal Edits, we the seed edits for each repository task are stored a-priory and applied as\npatches. CodePlan performs incremental dependency analysis on the repository after the seed edit,\nidentifies code that may be impacted by it, plans which edit to be made next, queries the LLM with\na suitable prompt and merges the result back into the repository. CodePlan does this iteratively\nuntil it finds that there are no more sites to be edited.\nAt times, multiple iterations become necessary due to the inherent variability in Large Language\nModel (LLM) responses. We initiate the first iteration, known as \"Iter 1,\" to kickstart the code\nediting process using LLMs. However, occasional inaccuracies in LLM responses may introduce\nerroneous code changes and subsequent build errors. To address these challenges, the second\niteration, referred to as \"Iter 2,\" becomes important. During this phase, CodePlan actively identifies\nand acknowledges any build errors from the previous iteration, and re-engagement with the LLM\nto obtain more precise responses for correcting the initial errors.\nAlongside CodePlan, we also evaluate a series of baselines on the same repositories. For C#\nmigration we evaluate Build-Repair and for Temporal Edits we evaluate Pyright-Repair, the setup\nfor both of which is presented in 4.2. Pyright-Strict-Repair is a variant in which we use the Pyright\ntool with strict mode enabled. In all the Repair baselines, we provide the same context (temporal\nand spatial) as in CodePlan, the only difference being that the localisation of sites to edit is using\nthe oracle. We also evaluate using Coeditor instead of gpt-4-32k as the edit model as described\nin 4.2. In all Coeditor baselines, contextualisation is performed as in [76], with the localisation of\nnext edit site being done through either CodePlan or using the oracle.\nWe evaluate all of these approaches for how well they localise the site to be edited using the\nMatched, Missed and Spurious Blocks metrics and the correctness of the overall modification using\nLevenshtein Distance and Diff BLEU as described in 4.3. We also determine whether the state of the\nrepository after the approach is finished executing passes the validity check \u2013 that is whether it\nsatisfies the oracle and makes the correct edit according to the ground truth.\nResults Discussion. The experimental results in Table 3 demonstrate the effectiveness of the\nCodePlan framework in automating repository-level coding tasks.\nIn the context of the C# Migration Task on Internal (Proprietary) Repositories, the results table\nprovides a comprehensive view of the performance of two approaches: CodePlan and Build-Repair.\nNotably, CodePlan demonstrates superior capabilities in several key aspects. It excels in \"Matched\nBlocks\", achieving perfect results with 151 matched blocks for both \"Int-1 (Logging)\" and 438\nmatched blocks for \"Int-2 (Logging)\" datasets. This indicates CodePlan\u2019s exceptional precision in\naccurately identifying and addressing intended code changes. Moreover, CodePlan impressively\nexhibits zero \"Missed Blocks\" ensuring that no crucial code modifications are overlooked, thus\nminimizing the risk of functional issues. Equally noteworthy is the absence of \"Spurious Blocks\"\n18\nCodePlan: Repository-level Coding using LLMs and Planning\nDataset\nApproach\nMatched\nBlocks\nMissed\nBlocks\nSpurious\nBlocks\nDiff\nBLEU\nLevenshtein\nDistance\nValidity\nCheck\nC# Migration Task on Internal (Proprietery) Repositories\nInt-1\n(Logging)\nCodePlan (Iter 1)\n151\n0\n0\n0.99\n60\n\u2717 (4)\nCodePlan (Iter 2)\n151\n0\n0\n1.00\n0\n\u2713 (0)\nBuild-Repair\n82\n69\n13\n0.81\n6465\n\u2717 (46)\nInt-2\n(Logging)\nCodePlan (Iter 1)\n438\n0\n0\n0.99\n90\n\u2717 (6)\nCodePlan (Iter 2)\n438\n0\n0\n1.00\n0\n\u2713 (0)\nBuild-Repair\n337\n101\n25\n0.66\n7496\n\u2717 (68)\nC# Migration Task on External (Public) Repositories\nExt-1\n(NUnit-\nXUnit)\nCodePlan (Iter 1)\n58\n0\n0\n1.00\n0\n\u2713 (0)\nBuild-Repair\n52\n6\n1\n0.94\n530\n\u2717 (8)\nPython Temporal Edit Task on External (Public) Repositories\nT-1\nCodePlan (Iter 1)\n8\n2\n0\n0.90\n1044\n\u2717\nPyright-Repair\n5\n5\n0\n0.76\n1090\n\u2717\nPyright-Strict-Repair\n8\n2\n0\n0.90\n1045\n\u2717\nCoeditor-CodePlan\n8\n2\n0\n0.90\n1160\n\u2717\nCoeditor-Pyright-Repair\n5\n5\n0\n0.66\n1205\n\u2717\nCoeditor-Pyright-Strict-Repair\n5\n5\n0\n0.65\n1139\n\u2717\nT-2\nCodePlan (Iter 1)\n4\n0\n0\n0.86\n248\n\u2713\nPyright-Repair\n1\n3\n0\n0.00\n344\n\u2717\nPyright-Strict-Repair\n1\n3\n0\n0.00\n344\n\u2717\nCoeditor-CodePlan (Iter 1)\n3\n1\n0\n0.82\n254\n\u2717\nCoeditor-Pyright-Repair\n1\n3\n0\n0.00\n344\n\u2717\nCoeditor-Pyright-Strict-Repair\n1\n3\n0\n0.00\n344\n\u2717\nT-3\nCodePlan (Iter 1)\n11\n0\n0\n0.92\n358\n\u2713\nPyright-Repair\n1\n10\n0\n0.00\n798\n\u2717\nPyright-Strict-Repair\n1\n10\n0\n0.00\n798\n\u2717\nCoeditor-CodePlan (Iter 1)\n10\n1\n0\n0.78\n717\n\u2717\nCoeditor-Pyright-Repair\n1\n10\n0\n0.00\n798\n\u2717\nCoeditor-Pyright-Strict-Repair\n1\n10\n0\n0.00\n798\n\u2717\nTable 3. Comparison of CodePlan\u2019s repository edit metrics with Build-Repair baseline. Higher values of\nMatched Blocks, Diff BLEU, and lower values of Missed Blocks, Spurious Blocks, Levenshtein Distances are\nbetter.\nintroduced by CodePlan, signifying its ability to maintain codebase cleanliness and integrity. In\ncontrast, Build-Repair falls behind with 82 matched blocks for \"Int-1 (Logging)\" and 437 for \"Int-2\n(Logging)\", while also missing a substantial number of blocks (69 and 101, respectively). Additionally,\nBuild-Repair introduces 13 spurious blocks for \"Int-1 (Logging)\" and 25 for \"Int-2 (Logging)\", which\ncan increase code complexity and error potential. These findings underscore CodePlan\u2019s superiority\nover Build-Repair, not only in terms of matched blocks but also in its ability to avoid missed and\nspurious code changes, ultimately offering a more precise and reliable solution for the C# Migration\nTask on internal repositories.\nCodePlan vs Build-Repair. The comparative analysis reveals why Build-Repair falls behind\nCodePlan. One crucial factor contributing to its performance gap is its reliance on \"build er-\nror location\" as an indicator for code correction. Build errors often pinpoint the location where an\nerror is detected, but they may not necessarily coincide with the actual required fix location. For\ninstance, an error may manifest in a derived class\u2019s overridden function signature mismatch, but the\nfix is necessitated in the base class\u2019s virtual function signature, causing Build-Repair to misinterpret\nthe correction site. Additionally, the build process in the context of compiler optimizations may\n19\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nobscure subsequent errors, only displaying selected errors at a given time. This can lead to an\nincorrect identification of the correction location and hinder the proper propagation of changes,\nfurther exacerbating the performance gap between CodePlan and Build-Repair. These limitations\nunderscore the challenges faced by Build-Repair in accurately pinpointing and addressing code\nmodifications in complex migration tasks.\nMultiple Iterations. We can see the necessity for multiple iterations to handle the inherent\nvariability in Large Language Model (LLM) responses as illustrated with the 4 build errors after\n\"Iter 1\" of CodePlan in the \"Int-1\" dataset. To rectify this, \"Iter 2\" plays a vital role. In this phase,\nCodePlan identifies and acknowledges the build errors from the previous iteration and re-engages\nwith the LLM to obtain more accurate responses for correcting the initial errors. It is noteworthy\nthat there are no changes to the block metrics between the two iterations since CodePlan correctly\nidentifies the blocks requiring correction and engages the LLM for modification. However, the LLM\ncorrections in \"Iter 1\" were erroneous, leading to a lower Levenshtein distance metric. This iterative\nrefinement process significantly contributes in mitigating the impact of occasional inaccuracies in\nLLM outputs during the code editing phases.\nPerformance on Ext-1. In the comparison between CodePlan and the Build-Repair baseline\nmethod on the \"Ext-1\" dataset, we observe a significant difference in their performance. CodePlan\nsuccessfully identified and updated 58 code blocks, achieving a perfect DiffBLEU score of 1.00,\nindicating that it made changes identical to the target repository. In contrast, Build-Repair, the\nbaseline method, fell short by missing six blocks and generating eight build errors. This discrepancy\nhighlights a critical limitation of Build-Repair\u2014the lack of comprehensive change may-impact\nanalysis capabilities. Specifically, Build-Repair failed to update constructor blocks required for\ninitializing the newly added ITestOutputHelper _output class member. This omission went\nunnoticed because the absence of initialization didn\u2019t trigger build errors, causing a cascading effect\non the callers of this constructor. In contrast, CodePlan successfully handled the initialization of the\nnewly added ITestOutputHelper _output class member. This achievement can be attributed to\nits robust change-may impact analysis, which accurately identified the necessary modifications to\nthe constructor block upon the addition of a new field. As a result, CodePlan seamlessly updated the\nconstructor and all its callers, preventing any missed blocks or build errors. This finding underscores\nthe importance of CodePlan\u2019s advanced planning abilities, which ensure a more thorough and\naccurate approach to repository-level code edits. Further details and qualitative analysis are provided\nin RQ3 (Figure 11) of our study.\nCodePlan vs Pyright-Repair. In context of the Temporal Edits task, we can see that CodePlan\nis able to successfully identify all the derived edit location in 2 repos (T-2, T-3) and is almost\nsuccessuful in the third (T-1). This is in contrast to the baseline Pyright-Repair approach which fails\nto identify any of the derived edits in two repos (T-2, T-3). We find that using Pyright on a strict\nchecking mode yields improved results, but only on one repo (T-1) where it peforms just as well as\nCodePlan. Overall we observe that the Pyright-Repair baseline falls short in identifying location\nto edit. This is also reflected in the DiffBLEU score which is consistently higher for CodePlan\nand Levenshtein Distance (L.D.) which is consistently lower. Note that since the LLM does not\nnecessarily make the exact edit present in the ground truth, both DiffBLEU and L.D. may not have\nperfect 1.0 and 0 values respectively.\nFor both T-2 and T-3 we see that the Pyright-Repair baseline is unable to identify any derived\nedits. In both of these repos Pyright does not flag any errors in the codebase once the seed-edit has\nbeen made. In the case of T-2, the seed edit involves adding an argument to a method as shown in\nFigure 6 However this new parameter is also assigned a default value. Due to the presence of a\n20\nCodePlan: Repository-level Coding using LLMs and Planning\ndef load_mbd_ckpt(\nfile_or_url_or_id: Union[Path, str],\n+\nfilename: tp.Optional[str] = None,\ncache_dir: tp.Optional[str] = None\n):\nreturn _get_state_dict(\nfile_or_url_or_id,\n\u2212\nfilename=\"all_in_one.pt\",\n+\nfilename=filename,\ncache_dir=cache_dir\n)\nFig. 6. Seed edit for T-2.\ndef send_request(data):\napi_key = data.pop(\"api_key\")\napi_type = data.pop(\"api_type\")\n+\napi_endpoint = data.pop(\"api_endpoint\")\n...\nFig. 7. Seed edit for T-3.\ndefault value for this argument, Pyright does not flag any of the call sites of this method as errors.\nHowever in the ground truth, the developer does follow up and edit the call sites. CodePlan\u2019s\nchange may-impact analysis identifies these call-sites as \"might requiring\" edit and so we pass\nthem to the LLM to edit.\nIn the case of T-3, the seed edit involves modifying the body of the function as describe in\nFigure 6. After the seed edit, the method now expects the dictionary being passed as argument\nto contain a new key \"api_endpoint\". Pyright will not flag any errors in the repo at this stage\nhowever it is clear that this may require modification to the callers of send_request. CodePlan\ndoes detect this and is able to identify and make edits at 10 derived sites present in the ground\ntruth.\nIn both of these scenarios, we do not know a-priory whether the call-sites need editing. Thus\nCodePlan leaves this decision upto the LLM to decide given the context. In contrast, the Oracle-\nGuided baseline does not even detect that a change maybe needed. This can be attributed to the\nfact that Oracles such as Pyright, or Build aim to detect errors and hence will only flag code that\nviolates certain rules. This is not aligned with the task of propagating changes across a repo, as\nthere may be many cases where a change may need to be propagated but the repo with the change\napplied does violate any of the oracle\u2019s rules.\nCoeditor Evaluation. When comparing CodePlan with Coeditor-CodePlan, we can see that it\nperforms on par on T-1 but lags slightly behind on T-2 and T-3 missing one edit site in each. While\nboth approaches are using the same analysis and planning, the local edits made in CodePlan are\nmore in-line with the ground-truth compared to those by Coeditor, reflected in the the lower\nDiffBLEU scores and higher L.D. exhibited by Coeditor-CodePlan in T-2 and T-3. This can be due\nto the difference in context-understanding abilities between gpt-4-32k and Coeditor. For example,\nopting to instantiate an argument to a method instead of adding a parameter to the caller can\nmean missing out of edits resulting from the signature change to the caller. Being a significantly\nmore powerful model, gpt-4-32k is better at understanding the context of the temporal edits, hence\nthe edits it makes are more aligned with the ground truth as compared to Coeditor. This is also\nobserved when comparing the Pyright-Strict-Repair and Coeditor-Pyright-Strict-Repair on T-2,\nwhere incorrect local edits by Coeditor lead to missing of edit sites and worse DiffBLEU and L.D.\nIn conclusion, the experimental results substantiate that CodePlan\u2019s planning-based approach is\nhighly effective in automating repository-level coding tasks, offering superior matching, complete-\nness, and precision compared to traditional baseline methods. Its ability to handle complex coding\ntasks within large-scale repositories signifies a significant advancement in automating software\nengineering activities.\n21\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\n5.2\nRQ2: How important are the temporal and spatial contexts for CodePlan\u2019s\nperformance?\nMotivation.: The motivation behind RQ2 lies in recognizing that Large Language Models (LLMs)\nrequire both temporal and spatial contexts to provide accurate and contextually relevant code\nupdate suggestions. Temporal context is vital for understanding the sequence and timing of code\nchanges, allowing LLMs to make suggestions that align with the code\u2019s evolution and maintain\nconsistency. Without this temporal awareness, LLMs might offer solutions that conflict with earlier\nor subsequent modifications, leading to code errors. Spatial context, on the other hand, enables\nLLMs to comprehend the intricate relationships and dependencies between different parts of the\ncodebase. This understanding is crucial for pinpointing where updates are needed and ensuring that\nthey are applied in a way that preserves code functionality. Therefore, investigating the importance\nof these contexts in CodePlan\u2019s planning is fundamental to harnessing the full potential of LLMs\nin automating repository-level coding tasks effectively.\nExperimental Setup.: To assess the importance of temporal and spatial contexts in CodePlan\u2019s\nplanning (RQ2), a specific experimental setup is employed. Recall that temporal contexts are tracked\nby CodePlan through the maintenance of a list of previous related changes, and these contexts are\nsubsequently incorporated into the Large Language Model (LLM) prompt.\nTo measure the significance of these contexts, a controlled experiment is carried out. In this\nexperiment, the tracking of temporal changes is intentionally halted, and no temporal or spatial\ncontexts are provided to the LLM during code modification tasks. This enables a detailed examination\nof how LLM responses are impacted when these essential contexts are omitted. The experiment\nalso encompasses a comprehensive assessment of various metrics, including code consistency\n(block metrics) and code correctness (Diff BLEU and Levenshtein distance). By comparing the\noutcomes between CodePlan\u2019s setup with temporal and spatial contexts and the experimental\nsetup without them, the research aims to precisely quantify the importance of these contexts in\nCodePlan\u2019s planning process and their influence on the quality of automated code modifications.\nResults Discussion.:\nThe results regarding the importance of temporal and spatial contexts for CodePlan\u2019s planning\n(RQ2) reveal critical insights. As observed in Table 2, when temporal contexts are not considered,\nthere is a noticeable increase in missed blocks during the code modification process. This increase is\nattributed to the Large Language Model (LLM) not making necessary changes to certain code blocks\ndue to its inability to comprehend the need for those modifications in the absence of temporal\ncontext.\nAn illustrative example in Figure 8 exemplifies this issue. In this scenario, a correction is required\nin the base class\u2019s virtual method based on changes to the overridden method\u2019s signature in the\nderived class. However, the LLM, lacking temporal context, does not possess information about the\nderived class\u2019s method, leading it to believe that no changes are necessary to the base class method.\nThis highlights the critical role that temporal context plays in understanding code dependencies\nand ensuring accurate updates.\nFurthermore, Figure 9 provides another instance where the absence of temporal context impacts\nthe code modification process. In this case, a \"Context\" parameter needs to be added to the \"Create-\nService()\" call within the \"Startup()\" method. However, since the LLM lacks temporal context, it is\nunaware of the signature change to \"CreateService()\" and, consequently, fails to recognize the need\nfor updates to all the callers. This omission results in numerous missed updates throughout the\ncodebase.\n22\nCodePlan: Repository-level Coding using LLMs and Planning\nDataset\nApproach\nMatched\nBlocks\nMissed\nBlocks\nSpurious\nBlocks\nDiff\nBLEU\nLevenshtein\nDistance\nOracle\nVerdict\nC# Migration Task on Int-1 with Temporal and Spatial Contexts\nInt-1\nCodePlan\n(Iter 1 + Iter 2)\n151\n0\n0\n1.00\n0\n\u2713 (0)\nC# Migration Task on Int-1 without Temporal and Spatial Contexts\nInt-1\nCodePlan (Iter 1)\n112\n39\n4\n0.73\n3674\n\u2717 (34)\nCodePlan (Iter 2)\n121\n30\n52\n0.53\n4522\n\u2717 (68)\nCodePlan (Iter 3)\n121\n30\n54\n0.51\n4524\n\u2717 (69)\nTable 4. Ablation study with and without temporal/spatial blocks. Without temporal/spatial contexts,\nCodePlan is unable to make correct edits.\nFig. 8. Illustration of importance of temporal context. Failure to update LogacyLogger to ModernLogger in\nInitialize() method is the results of missing missing temporal context.\nIt\u2019s crucial to highlight another significant observation: the increase in the count of spurious\nblocks when spatial context is insufficient. This phenomenon occurs because, in the absence of\nadequate spatial context, the Large Language Model (LLM) may incorrectly perceive missing code\nelements and attempt to create them, leading to the generation of spurious code blocks.\nAn illustrative example in Figure 10 demonstrates this issue. In this scenario, the task is to modify\nthe \"AuthorizeUser()\" method by migrating the logging calls from an old logging framework to\na new one. However, due to the lack of spatial context that would specify the existence of the\n\"GetUserSubscription()\" method and the \"CurrentUser\" property, the LLM attempts to create these\nelements as well. Consequently, not only is the logging migration addressed, but the LLM also\n23\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nFig. 9. Illustration of importance of temporal context. Failed update to Startup() method is the results of\nmissing missing temporal context.\nFig. 10. Illustration of importance of spatial context. Spurious blocks, highlighted in yellow are the results of\nmissing missing spatial context.\nintroduces unnecessary code blocks, such as the creation of the \"GetUserSubscription()\" method\nand the addition of \"CurrentUser\" as a class-level object.\nThis observation underscores the critical role of spatial context in guiding the LLM\u2019s under-\nstanding of code structure and relationships. Providing comprehensive spatial context can help\nprevent the generation of superfluous code blocks and ensure that code modifications are precise\nand aligned with the intended changes.\nIn summary, the experimental results emphasize the essential nature of temporal and spatial\ncontexts in CodePlan\u2019s planning. The increase in missed and spurious updates due to the absence\n24\nCodePlan: Repository-level Coding using LLMs and Planning\nof temporal and spatial contexts underscores the significance of providing the LLM with a com-\nprehensive understanding of code evolution and dependencies through these contexts to ensure\naccurate and effective code modifications.\n5.3\nRQ3: What are the key differentiators that allow CodePlan to outperform baselines\nin solving complex coding tasks?\nMotivation.: RQ3 seeks to uncover the key differentiators that empower CodePlan to excel in\ntackling intricate coding tasks compared to baseline approaches. This research question is moti-\nvated by the need to identify and understand the factors that contribute to CodePlan\u2019s superior\nperformance. Given the increasing complexity of coding tasks, especially those at the repository\nlevel, it becomes crucial to pinpoint the specific aspects that set CodePlan apart from traditional\nmethods. By discerning these differentiators, the research aims to shed light on the strengths and\nadvantages of CodePlan, offering valuable insights into how it effectively addresses the challenges\nposed by complex coding tasks.\nExperimental Setup.: The primary focus here is on qualitative analysis. After conducting code\nmodifications with both CodePlan and the baseline methods, the results are meticulously examined\nthrough manual inspection. This entails a detailed examination of the changes made by each\napproach, with the aim of gaining qualitative insights into the decision-making processes and\nthe nuances of code modifications. By manually eyeballing and comparing the alterations, the\nresearch seeks to uncover subtle yet crucial distinctions that illuminate the strengths and underlying\nmechanisms that give CodePlan its edge in addressing the challenges posed by intricate coding\ntasks. This qualitative approach provides a comprehensive understanding of how CodePlan excels\nin this context and what sets it apart from conventional methods.\nResults Discussion.:\nCodePlan\u2019s Strategic Planning and Context Awareness:.\nCodePlan\u2019s exceptional performance in handling complex coding tasks can be attributed to its\npowerful features, notably its incremental analysis and change-may-impact analysis. These capa-\nbilities set it apart from baseline methods like Build-Repair, which primarily focus on maintaining\nsyntactic correctness while overlooking critical contextual details. To illustrate this, let\u2019s delve\ninto an example from repository Ext-1 illustrated in Figure 11, where CodePlan is tasked with\nmigrating the Console.WriteLine method to ITestOutputHelper.WriteLine. This migration\ninvolves a series of changes 1 to 4 as described in the Figure 11. These cascading changes start\nfrom introducing ITestOutputHelper _output as a class-level member, accomplished via LLM\nupdates.\nCodePlan\u2019s change-may-impact analysis proves invaluable in this scenario. It recognizes that\nthe addition of a new field necessitates modifications to the constructor to ensure proper initializa-\ntion. As a result, CodePlan schedules the necessary constructor modification. Consequently, the\nconstructor Subscriber(...) is correctly updated to accept ITestOutputHelper as a parameter\nand initialize the class member _output. This in turn results in a series of changes through the\nrepository as explained in steps 1 to 4 in the Figure 11.\nThis example demonstrates how CodePlan makes methodical and contextually-aware changes to\nthe repository, thanks to its ability to do change impact analysis and incorporate temporal contexts.\nIn contrast, Build-Repair, reliant solely on syntactic correctness, fails to even detect the need for\nmodification in the Subscriber\u2019s constructor. Given that all syntactic rules are adhered to, it does\nnot prompt a build error and consequently fails to implement changes in steps 2 to 4, as illustrated\n25\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nFig. 11. Illustration of the CodePlan\u2019s plan execution.\nin Figure 4. Instead, it solely executes the modification outlined in step 1, resulting in incomplete\ncode updates.\nCodePlan\u2019s distinctive advantage lies in its holistic understanding of code relationships and its\nmeticulous planning, which ensures the integrity and functionality of the codebase are maintained\nthroughout complex coding tasks. This qualitative analysis highlights how CodePlan\u2019s nuanced\napproach outperforms baselines in handling intricate coding challenges.\nIncremental Analysis: Maintaining Relationships with Dependency Graph:.\nCodePlan\u2019s exceptional performance in tackling complex coding tasks is attributed to its incre-\nmental analysis, which effectively links edits with the underlying dependency graph. Unlike a static\nsnapshot of code, which may result in an incomplete representation of dependencies, our incre-\nmental analysis method ensures that relationships within the dependency graph are maintained\nuntil the affected blocks are modified.\nConsider a scenario where a caller function undergoes a renaming process. Traditional static\nsnapshots would struggle to preserve the caller-callee relationship because, in their view, the caller\nhas already been renamed. However, CodePlan\u2019s incremental analysis steps in, preserving the\ncaller-callee relation until the caller function itself undergoes an update. This dynamic approach\n26\nCodePlan: Repository-level Coding using LLMs and Planning\nensures that critical relationships aren\u2019t prematurely severed, allowing for more accurate and\ncontext-aware code modifications.\nAnother instance of CodePlan\u2019s prowess lies in handling modifications to import statements.\nSuppose an import statement originally reads as import numpy, and it\u2019s modified to import numpy\nas np. In a static snapshot, this alteration could result in the loss of the \"ImportedBy\" relationship.\nHowever, CodePlan\u2019s incremental analysis ensures that such vital relationships are maintained,\nfacilitating precise and comprehensive code updates.\nIncremental Analysis: Enhanced Spatial and Temporal Context Extraction:.\nCodePlan\u2019s remarkable success in complex coding tasks can be attributed to its proficiency in\nextracting spatial context more accurately, thanks to incremental analysis. Attempting to extract\nspatial context without the support of incremental analysis often leads to a loss of accuracy and\ncompleteness.\nConsider a scenario where a method within the codebase constructs an object of a class, let\u2019s say\n\"A.\" However, at some point in the code\u2019s history, \"A\" was renamed to \"B.\" Traditional methods that\nlack incremental analysis may struggle with this situation. When attempting to extract the class\ndefinition, they may encounter a roadblock because, in the current static snapshot, \"A\" no longer\nexists.\nHowever, CodePlan\u2019s incremental analysis comes to the rescue by establishing the crucial link\nbetween the historical context and the present state. It accurately extracts the class definition,\nrecognizing that the object is now of class \"B\" due to the earlier temporal edit (the renaming\nof \"A\" to \"B\"). This holistic approach ensures that spatial context extraction is both precise and\ncomprehensive, allowing CodePlan to make informed and context-aware code modifications.\nChange-may-impact analysis propagates subtle behavioral changes..\nOne of the key factors differentiating CodePlan\u2019s superior performance in complex coding tasks\nis its adeptness at detecting subtle behavioral changes through extensive change-may-impact\nanalysis. While certain code edits, like modifying method signatures, result in obvious breaking\nchanges that can be detected by build tools, others induce more nuanced behavioral shifts without\ndirectly breaking the build. These subtle alterations, often overlooked, can significantly affect code\ncorrectness and functionality. For instance, a seemingly minor change in a method\u2019s return value,\nfrom True to False, may invalidate assertions in unit tests.\nCodePlan excels in identifying such subtle behavioral transformations that may elude oracles\nsuch as build or static checking tools. Its thorough change-may-impact analysis delves beyond\nsurface-level modifications, proactively recognizing these inconspicuous shifts. This capability sets\nCodePlan apart from baseline methods, which primarily focus on changes related to build success.\nConsequently, CodePlan emerges as a powerful solution for addressing complex coding tasks,\nensuring that even the most subtle alterations are meticulously considered, ultimately enhancing\ncode quality.\nChange may-impact analysis maintains cause-effect relationship.. One of CodePlan\u2019s differ-\nentiators lies in its proficiency in preserving the cause-effect relationship when handling complex\ncoding tasks. Traditional build tools are effective at pinpointing breaking changes but often fall\nshort in identifying the underlying causes and their corresponding effects. For instance, if a method\nsignature is altered within an overridden method, a typical build tool would flag the issue at the\noverridden method\u2019s location, where the error is observed. However, this approach fails to recognize\nthe underlying cause\u2014the change in the method signature, which should ideally lead to an update\nin the corresponding virtual method in the base class.\n27\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nIn contrast, CodePlan\u2019s change-may-impact analysis excels in maintaining the causal link be-\ntween code modifications. When a breaking change is introduced, CodePlan not only identifies\nthe error but also traces it back to the root cause, establishing the need for subsequent changes.\nIn the aforementioned example, CodePlan recognizes that the change in the overridden method\u2019s\nsignature necessitates an update to the corresponding virtual method in the base class. This metic-\nulous preservation of cause and effect sets CodePlan apart from baseline methods, which often\ntreat issues in isolation without considering the broader context.\nIncremental static analysis is lightweight and easy to deploy.. One of the key distinguishing\nfeatures that enables CodePlan to outperform baselines in tackling complex coding tasks is its\nutilization of lightweight and readily deployable incremental static analysis. In real-world software\ndevelopment, build systems often prove to be complex and resource-intensive. Traditional build\nprocesses can be quite expensive in terms of computational resources and time, especially when\ndealing with large and intricate codebases.\nIn contrast, CodePlan\u2019s incremental static analysis offers a more efficient and practical alternative.\nBy focusing on analyzing only the code portions that have been modified, CodePlan significantly\nreduces the computational overhead associated with build processes. This approach not only\naccelerates the repository-wide editing but also minimizes the resources required for each editing\ntask.\nIn summary, CodePlan\u2019s exceptional performance in complex coding tasks can be attributed\nto several key differentiators. Firstly, its strategic planning, coupled with its deep understanding\nof temporal and spatial contexts, enables CodePlan to make methodical and context-aware code\nmodifications. Additionally, the system\u2019s ability to maintain relationships within the dependency\ngraph through incremental analysis ensures the preservation of code correctness and functionality.\nCodePlan\u2019s enhanced spatial and temporal context extraction further aids in accurately updating\ncode elements. Notably, the change-may-impact analysis not only identifies subtle behavioral\nchanges but also maintains a clear cause-effect relationship between code modifications. Finally,\nCodePlan\u2019s lightweight and easily deployable incremental static analysis offer practical advantages,\nreducing resource overhead in complex software projects. These combined differentiators empower\nCodePlan to excel in solving intricate coding challenges.\n6\nLIMITATIONS AND THREATS TO VALIDITY\nWhile CodePlan demonstrates significant capabilities, there are certain limitations and potential\nthreats to the validity of its results. One crucial factor for CodePlan\u2019s success lies in the quality\nof the dependency analysis. In statically typed languages like C# and Java, rich code dependency\ninformation can be extracted effectively. However, in dynamically typed languages such as Python\n(without type hints) or JavaScript, establishing semantically rich relationships between code blocks\ncan be more challenging due to the dynamic nature of the code.\nOur current implementation of CodePlan primarily handles relations between code blocks\nthrough static analysis. In real-world software systems, numerous dynamic dependencies exist,\nincluding data flow dependencies, complex dynamic dispatching (via virtual functions and dynamic\ncastings), algorithmic dependencies (e.g., when input lists are expected to be sorted), and various\nexecution dependencies (such as multi-threading and distributed processing). Addressing these\ndynamic dependencies will be a crucial focus of our future work.\nWe selected multiple repositories across two challenging tasks (migration and temporal edits)\nand two languages (C# and Python) to evaluate the generality of CodePlan. These repositories\nand tasks are representative of real-world usecases. However, given the complexity of setting up\n28\nCodePlan: Repository-level Coding using LLMs and Planning\neach experiment, our evaluation is restricted to a few repositories. More experimentation will be\nrequired to probe the strengths and weaknesses of CodePlan.\nWhile CodePlan excels in planning and editing for programming language repositories, enterprise\nsoftware systems often comprise various other artifacts like configuration files, metadata files,\nproject setting files, external dependencies, and more. A comprehensive repository-level editing\napproach, particularly for tasks like software migrations, necessitates the ability to edit these\nadditional artifacts. This calls for an evolution of dependency graphs to include nodes and relations\nencompassing all these diverse artifacts, and change-may-impact analysis that spans across them.\nCodePlan currently relies on Large Language Models (LLMs) to utilize the necessary context\ninformation for making code edits. It trusts the LLM\u2019s response for change-may-impact analysis. In\ncases where the LLM response is incorrect or spurious, it may lead to erroneous updates in the\nrepository. While running CodePlan in multiple iterations in our experiments helped rectify such\nissues, there may be instances where this approach may not suffice.\nIn terms of CodePlan\u2019s update strategy, it currently focuses on updating one block at a time.\nWhile this approach aligns with our current design choices, there may be scenarios where it would\nbe more efficient or necessary to implement multiple simultaneous changes. More sophisticated\ntechniques for handling multiple block updates (within the same file or across different files) and\nchange propagation will be a key area of exploration in our future work.\nAlthough our current methodology employs zero-shot prompting, there exists potential for\nextension to include few-shot examples, Chain of Thought (CoT), and other techniques. These\nextensions represent a promising avenue for future research.\n7\nRELATED WORK\nLLMs for Coding Tasks. A multitude of LLMs [10, 19, 21, 24, 28, 30, 35, 57, 73\u201375, 80] have been\ntrained on large-scale corpora of source code and natural language text. These have been used to\naccomplish a variety of coding tasks. A few examples of their use include program synthesis [50, 56],\nprogram repair [11, 43, 79], vulnerability patching [60], inferring program invariants [62], test\ngeneration [69] and multi-task evaluation [72]. However, these investigations are performed on\ncurated examples that are extracted from their repositories and are meant to be accomplished\nwith independent invocations of the LLM. We consider a different class of tasks posed at the\nscale of code repositories, where an LLM is called multiple times on different examples which\nare inter-dependent. We monitor the results of each LLM invocation within the repository-wide\ncontext to identify future code change obligations to get the repository to a consistent state, e.g.,\nwhere the repository is free of build or runtime errors.\nAutomated Planning. Automated planning [37, 67] is a well-studied topic in AI. Online plan-\nning [67] is used when the effect of actions is not known and the state-space cannot be enumerated a\npriori. It requires monitoring the actions and plan extension. In our case, the edit actions are carried\nout by an LLM whose results cannot be predicted before-hand and the state-space is unbounded.\nAs a consequence, our adaptive planning is an online algorithm where we monitor the actions and\nextend the plan through static analysis. In orthogonal directions, [42] uses an LLM to derive a plan\ngiven a natural language intent before generating code to solve complex coding problems and [86]\nperforms lookahead planning (tree search) to guide token-level decoding of code LMs. Planning\nin our work is based on analyzing dependency relations and changes to them as an LLM makes\nchanges to a code repository.\nAnalysis of Code Changes. Static analysis is used for ensuring software quality. It is expensive to\nrecompute the analysis results every time the code undergoes changes. The field of incremental\n29\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nprogram analysis offers techniques to recompute only the analysis results impacted by the change.\nSpecialized algorithms have been developed for dataflow analysis [18, 68], pointer analysis [84],\nsymbolic execution [63], bug detection [52] and type analysis [27]. Program differencing [16, 46, 48]\nand change impact analysis [17, 41] determine the differences in two program versions and the\neffect of a change on the rest of the program. The impact of changes has been studied for regression\ntesting [65], analyzing refactorings [33] and assisting in code review [13, 36]. We analyze the code\ngenerated by an LLM and incrementally update the syntactic (e.g., parent-child) and dependency\n(e.g., caller-callee) relations. We further analyze the likely impact of those changes on related code\nblocks and create change obligations to be discharged by the LLM.\nSpatial and Temporal Contextualization. As discussed in the Introduction, LLMs benefit from\nrelevant context derived from other files in the repository and from past edits. We provide both\nthese pieces of information to the LLM by tracking the code changes and dependency relations.\nLearning Edit Patterns. Many approaches have been developed to learn edit patterns from\npast edits or commits in the form of rewrite rules [31], bug fixes [15, 20], type changes [45],\nAPI migrations [49, 82] and neural representations of edits [83]. Approaches such as [53] and\n[54] synthesize context-aware edit scripts from user-provided examples and apply them in new\ncontexts. Other approaches observe the user actions in an IDE to automate repetitive edits [55] and\ntemporally-related edit sequences [87]. We do not aim to learn edit patterns and we do not assume\nsimilarities between edits. Our focus is to identify effects of code changes made by an LLM and to\nguide the LLM towards additional changes that become necessary.\n8\nCONCLUSIONS AND FUTURE WORK\nIn this paper, we introduced CodePlan, a novel framework designed to tackle the challenges of\nrepository-level coding tasks, which involve pervasive code modifications across large and inter-\ndependent codebases. CodePlan leverages incremental dependency analysis, change may-impact\nanalysis, and adaptive planning to orchestrate multi-step edits guided by Large Language Models.\nWe evaluated CodePlan on diverse code repositories with varying complexities and sizes, including\nboth internal proprietary repositories and public GitHub repositories in C# and Python for migration\nand temporal edit tasks. Our results demonstrated that CodePlan outperforms baseline methods,\nachieving better alignment with the ground truth. In conclusion, CodePlan presents a promising\napproach to automating complex repository-level coding tasks, offering both productivity benefits\nand accuracy improvements. Its success in addressing these challenges opens up new possibilities\nfor efficient and reliable software engineering practices.\nWhile CodePlan has shown significant promise, there are several avenues for future research\nand enhancements. First, we aim to expand its applicability to a broader range of programming\nlanguages and code artifacts, including configuration files, metadata, and external dependencies,\nto provide a more holistic solution for repository-level editing. Additionally, we plan to explore\nfurther customization of CodePlan\u2019s change may-impact analysis. This could involve incorporating\ntask-specific impact analysis rules, either through rule-based methods or more advanced machine\nlearning techniques, to fine-tune its editing decisions for specific coding tasks. Furthermore, we will\naddress the challenge of handling dynamic dependencies, such as data flow dependencies, complex\ndynamic dispatching (via virtual functions and dynamic castings), algorithmic dependencies (e.g.,\nwhen input lists are expected to be sorted), and various execution dependencies (such as multi-\nthreading and distributed processing), to make CodePlan even more versatile in addressing a wider\nrange of software engineering tasks.\n30\nCodePlan: Repository-level Coding using LLMs and Planning\nREFERENCES\n[1] [n. d.]. audiocraft. https://github.com/facebookresearch/audiocraft.\n[2] [n. d.]. CodeQL. https://github.com/github/codeql.\n[3] [n. d.]. JARVIS. https://github.com/microsoft/JARVIS.\n[4] [n. d.]. Jedi. https://github.com/davidhalter/jedi.\n[5] [n. d.]. OmniSharp. https://github.com/OmniSharp/csharp-language-server-protocol.\n[6] [n. d.]. Pyright. https://github.com/microsoft/pyright.\n[7] [n. d.]. Reactive Streams TCK. https://github.com/reactive-streams/reactive-streams-dotnet/tree/master/src/tck.\n[8] [n. d.]. whisper. https://github.com/openai/whisper.\n[9] Lakshya A Agrawal, Aditya Kanade, Navin Goyal, Shuvendu K. Lahiri, and Sriram K. Rajamani. 2023. Guiding\nLanguage Models of Code with Global Context using Monitors. arXiv:2306.10763 [cs.CL]\n[10] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified Pre-training for Program\nUnderstanding and Generation. arXiv:2103.06333 [cs.CL]\n[11] Toufique Ahmed and Premkumar Devanbu. 2023. Better patching using LLM prompting, via Self-Consistency.\narXiv:2306.00108 [cs.SE]\n[12] Alfred V Aho, Ravi Sethi, Jeffrey D Ullman, et al. 2007. Compilers: principles, techniques, and tools. Vol. 2. Addison-wesley\nReading.\n[13] Everton L. G. Alves, Myoungkyu Song, and Miryung Kim. 2014. RefDistiller: A Refactoring Aware Code Review\nTool for Inspecting Manual Refactoring Edits. In Proceedings of the 22nd ACM SIGSOFT International Symposium on\nFoundations of Software Engineering (Hong Kong, China) (FSE 2014). Association for Computing Machinery, New York,\nNY, USA, 751\u2013754. https://doi.org/10.1145/2635868.2661674\n[14] Amazon Web Services, Inc. 2023. Amazon Code Whisperer - AI Code Generator. https://aws.amazon.com/codewhisperer/\nAccessed: July 25, 2023.\n[15] Jesper Andersen and Julia L Lawall. 2010. Generic patch inference. Automated software engineering 17 (2010), 119\u2013148.\n[16] Taweesup Apiwattanapong, Alessandro Orso, and Mary Jean Harrold. 2004. A differencing algorithm for object-\noriented programs. In Proceedings. 19th International Conference on Automated Software Engineering, 2004. IEEE,\n2\u201313.\n[17] RS Arnold and SA Bohner. 1996. An introduction to software change impact analysis. Software Change Impact Analysis\n(1996), 1\u201326.\n[18] Steven Arzt and Eric Bodden. 2014. Reviser: efficiently updating IDE-/IFDS-based data-flow analyses in response to\nincremental program changes. In Proceedings of the 36th International Conference on Software Engineering. 288\u2013298.\n[19] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models.\nhttp://arxiv.org/abs/2108.07732 arXiv:2108.07732 [cs].\n[20] Johannes Bader, Andrew Scott, Michael Pradel, and Satish Chandra. 2019. Getafix: Learning to Fix Bugs Automatically.\nProc. ACM Program. Lang. 3, OOPSLA, Article 159 (Oct. 2019), 27 pages. https://doi.org/10.1145/3360585\n[21] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,\nKyle McDonell, Jason Phang, and others. 2022. Gpt-neox-20b: An open-source autoregressive language model. arXiv\npreprint arXiv:2204.06745 (2022).\n[22] Bruno Blanchet. 2003. Escape analysis for JavaTM: Theory and practice. ACM Transactions on Programming Languages\nand Systems (TOPLAS) 25, 6 (2003), 713\u2013775.\n[23] Shaked Brody, Uri Alon, and Eran Yahav. 2020. A structural model for contextual code changes. 4, OOPSLA (Nov.\n2020). https://doi.org/10.1145/3428283 Publisher Copyright: \u00a9 2020 Owner/Author..\n[24] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[25] Max Brunsfeld, Andrew Hlynskyi, Patrick Thomson, Josh Vera, Phil Turnbull, Timothy Clem, Douglas Creager, Andrew\nHelwer, Rob Rix, Hendrik Van Antwerpen, Michael Davis, , Ika, Tu\u00b4\u00e2n-Anh Nguy\u02dc\u00ean, Stafford Brunk, Niranjan Hasabnis,\nBfredl, Mingkai Dong, Matt Massicotte, Jonathan Arnett, Vladimir Panteleev, Steven Kalt, Kolja Lampe, Alex Pinkus,\nMark Schmitz, Matthew Krupcale, Narpfel, Santos Gallegos, Vicent Mart\u00ed, and , Edgar. 2023. tree-sitter/tree-sitter:\nv0.20.8. https://doi.org/10.5281/ZENODO.4619183\n[26] Alan Bundy. 1988. The use of explicit plans to guide inductive proofs. In 9th International Conference on Automated\nDeduction: Argonne, Illinois, USA, May 23\u201326, 1988 Proceedings 9. Springer, 111\u2013120.\n[27] Matteo Busi, Pierpaolo Degano, and Letterio Galletta. 2019. Using standard typing algorithms incrementally. In NASA\nFormal Methods: 11th International Symposium, NFM 2019, Houston, TX, USA, May 7\u20139, 2019, Proceedings 11. Springer,\n106\u2013122.\n31\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\n[28] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, and others. 2021. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374 (2021).\n[29] Jong-Deok Choi, Manish Gupta, Mauricio Serrano, Vugranam C Sreedhar, and Sam Midkiff. 1999. Escape analysis for\nJava. Acm Sigplan Notices 34, 10 (1999), 1\u201319.\n[30] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311 (2022).\n[31] Reudismam Rolim de Sousa, Gustavo Soares, Rohit Gheyi, Titus Barik, and Loris D\u2019Antoni. 2021. Learning Quick\nFixes from Code Repositories. In SBES \u201921: 35th Brazilian Symposium on Software Engineering, Joinville, Santa Catarina,\nBrazil, 27 September 2021 - 1 October 2021, Cristiano D. Vasconcellos, Karina Girardi Roggia, Vanessa Collere, and Paulo\nBousfield (Eds.). ACM, 74\u201383. https://doi.org/10.1145/3474624.3474650\n[32] Jeffrey Dean, David Grove, and Craig Chambers. 1995. Optimization of object-oriented programs using static class\nhierarchy analysis. In ECOOP\u201995\u2014Object-Oriented Programming, 9th European Conference, \u00c5arhus, Denmark, August\n7\u201311, 1995 9. Springer, 77\u2013101.\n[33] Danny Dig, Can Comertoglu, Darko Marinov, and Ralph Johnson. 2006. Automated detection of refactorings in\nevolving components. In ECOOP 2006\u2013Object-Oriented Programming: 20th European Conference, Nantes, France, July\n3-7, 2006. Proceedings 20. Springer, 404\u2013428.\n[34] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,\nDan Roth, and Bing Xiang. 2022. CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context.\nhttp://arxiv.org/abs/2212.10007 arXiv:2212.10007 [cs].\n[35] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke\nZettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint\narXiv:2204.05999 (2022).\n[36] Xi Ge, Saurabh Sarkar, Jim Witschey, and Emerson Murphy-Hill. 2017. Refactoring-aware code review. In 2017 IEEE\nSymposium on Visual Languages and Human-Centric Computing (VL/HCC). IEEE, 71\u201379.\n[37] Malik Ghallab, Dana Nau, and Paolo Traverso. 2004. Automated Planning: theory and practice. Elsevier.\n[38] Github, Inc. 2023. GitHub Copilot: Your AI pair programmer. https://github.com/features/copilot Accessed: July 25,\n2023.\n[39] David Gonz\u00e1lez, Joshu\u00e9 P\u00e9rez, Vicente Milan\u00e9s, and Fawzi Nashashibi. 2015. A review of motion planning techniques\nfor automated vehicles. IEEE Transactions on intelligent transportation systems 17, 4 (2015), 1135\u20131145.\n[40] Priyanshu Gupta, Avishree Khare, Yasharth Bajpai, Saikat Chakraborty, Sumit Gulwani, Aditya Kanade, Ar-\njun Radhakrishna, Gustavo Soares, and Ashish Tiwari. 2023. GrACE: Generation using Associated Code Edits.\narXiv:2305.14129 [cs.SE]\n[41] Mohammad-Amin Jashki, Reza Zafarani, and Ebrahim Bagheri. 2008. Towards a more efficient static software change\nimpact analysis method. In Proceedings of the 8th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software\ntools and engineering. 84\u201390.\n[42] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. 2023. Self-planning Code Generation with Large\nLanguage Model. arXiv:2303.06689 [cs.SE]\n[43] Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. 2023.\nInferFix: End-to-End Program Repair with LLMs. arXiv preprint arXiv:2303.07263 (2023).\n[44] Erez Karpas and Daniele Magazzeni. 2020. Automated planning for robotics. Annual Review of Control, Robotics, and\nAutonomous Systems 3 (2020), 417\u2013439.\n[45] Ameya Ketkar, Oleg Smirnov, Nikolaos Tsantalis, Danny Dig, and Timofey Bryksin. 2022. Inferring and applying type\nchanges. In Proceedings of the 44th International Conference on Software Engineering. 1206\u20131218.\n[46] Miryung Kim, David Notkin, Dan Grossman, and Gary Wilson. 2012. Identifying and summarizing systematic code\nchanges via rule inference. IEEE Transactions on Software Engineering 39, 1 (2012), 45\u201362.\n[47] Steven M La Valle. 2011. Motion planning. IEEE Robotics & Automation Magazine 18, 2 (2011), 108\u2013118.\n[48] Shuvendu K Lahiri, Chris Hawblitzel, Ming Kawaguchi, and Henrique Reb\u00ealo. 2012. Symdiff: A language-agnostic\nsemantic diff tool for imperative programs. In Computer Aided Verification: 24th International Conference, CAV 2012,\nBerkeley, CA, USA, July 7-13, 2012 Proceedings 24. Springer, 712\u2013717.\n[49] Maxime Lamothe, Weiyi Shang, and Tse-Hsun Peter Chen. 2020. A3: Assisting android api migrations using code\nexamples. IEEE Transactions on Software Engineering 48, 2 (2020), 417\u2013431.\n[50] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling,\nFelix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor Babuschkin,\nXinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,\n32\nCodePlan: Repository-level Coding using LLMs and Planning\nEsme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-\nlevel code generation with AlphaCode. Science 378, 6624 (2022), 1092\u20131097. https://doi.org/10.1126/science.abq1158\n_eprint: https://www.science.org/doi/pdf/10.1126/science.abq1158.\n[51] Tianyang Liu, Canwen Xu, and Julian McAuley. 2023. RepoBench: Benchmarking Repository-Level Code Auto-\nCompletion Systems. arXiv:2306.03091 [cs.CL]\n[52] Scott McPeak, Charles-Henri Gros, and Murali Krishna Ramanathan. 2013. Scalable and incremental software bug\ndetection. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering. 554\u2013564.\n[53] Na Meng, Miryung Kim, and Kathryn S McKinley. 2011. Sydit: Creating and applying a program transformation from\nan example. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of\nsoftware engineering. 440\u2013443.\n[54] Na Meng, Miryung Kim, and Kathryn S McKinley. 2013. LASE: locating and applying systematic edits by learning\nfrom examples. In 2013 35th International Conference on Software Engineering (ICSE). IEEE, 502\u2013511.\n[55] Anders Miltner, Sumit Gulwani, Vu Le, Alan Leung, Arjun Radhakrishna, Gustavo Soares, Ashish Tiwari, and Abhishek\nUdupa. 2019. On the fly synthesis of edit suggestions. Proceedings of the ACM on Programming Languages 3, OOPSLA\n(2019), 1\u201329.\n[56] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023.\nCodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In The Eleventh International\nConference on Learning Representations. https://openreview.net/forum?id=iaYcJKpY2B_\n[57] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[58] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311\u2013318.\n[59] Pardis Pashakhanloo, Aaditya Naik, Yuepeng Wang, Hanjun Dai, Petros Maniatis, and Mayur Naik. 2022. Codetrek:\nFlexible modeling of code using an extensible relational representation. (2022).\n[60] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2022. Examining Zero-\nShot Vulnerability Repair with Large Language Models. In 2023 IEEE Symposium on Security and Privacy (SP). IEEE\nComputer Society, 1\u201318.\n[61] Hengzhi Pei, Jinman Zhao, Leonard Lausen, Sheng Zha, and George Karypis. 2023. Better context makes better code lan-\nguage models: A case study on function call argument completion. In AAAI. https://www.amazon.science/publications/\nbetter-context-makes-better-code-language-models-a-case-study-on-function-call-argument-completion\n[62] Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2023. Can Large Language Models Reason\nabout Program Invariants? (2023).\n[63] Suzette Person, Guowei Yang, Neha Rungta, and Sarfraz Khurshid. 2011. Directed incremental symbolic execution.\nAcm Sigplan Notices 46, 6 (2011), 504\u2013515.\n[64] Machel Reid and Graham Neubig. 2022. Learning to Model Editing Processes. https://doi.org/10.48550/ARXIV.2205.\n12374\n[65] Xiaoxia Ren, Fenil Shah, Frank Tip, Barbara G Ryder, and Ophelia Chesley. 2004. Chianti: a tool for change impact\nanalysis of java programs. In Proceedings of the 19th annual ACM SIGPLAN conference on Object-oriented programming,\nsystems, languages, and applications. 432\u2013448.\n[66] Replit, Inc. 2023. Replit. https://replit.com/ Accessed: July 25, 2023.\n[67] Stuart J Russell. 2010. Artificial intelligence a modern approach. Pearson Education, Inc.\n[68] Barbara G Ryder. 1983. Incremental data flow analysis. In Proceedings of the 10th ACM SIGACT-SIGPLAN symposium\non Principles of programming languages. 167\u2013176.\n[69] Max Sch\u00e4fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. Adaptive test generation using a large language model.\narXiv preprint arXiv:2302.06527 (2023).\n[70] Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. 2023. RepoFusion:\nTraining Code Models to Understand Your Repository. arXiv:2306.10998 [cs.LG]\n[71] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2022. Repository-level prompt generation for large language\nmodels of code. arXiv preprint arXiv:2206.12839 (2022).\n[72] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, and Tegawend\u00e9 F. Bissyand\u00e9. 2023.\nIs ChatGPT the Ultimate Programming Assistant \u2013 How far is it? arXiv:2304.11938 [cs.SE]\n[73] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,\n33\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B.\nAshok, and Shashank Shet\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,\nand Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]\n[74] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model.\n[75] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained\nEncoder-Decoder Models for Code Understanding and Generation. ArXiv abs/2109.00859 (2021).\n[76] Jiayi Wei, Greg Durrett, and Isil Dillig. 2023. Coeditor: Leveraging Contextual Changes for Multi-round Code\nAuto-editing. arXiv:2305.18584 [cs.SE]\n[77] Jiayi Wei, Greg Durrett, and Isil Dillig. 2023.\nTypeT5: Seq2seq Type Inference using Static Analysis.\narXiv:2303.09564 [cs.SE]\n[78] Mark Wilson-Thomas. [n. d.]. GitHub Copilot chat for Visual Studio 2022. https://devblogs.microsoft.com/visualstudio/\ngithub-copilot-chat-for-visual-studio-2022/ Accessed: July 25, 2023.\n[79] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated program repair in the era of large pre-trained\nlanguage models. In Proceedings of the 45th International Conference on Software Engineering (ICSE 2023). Association\nfor Computing Machinery.\n[80] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A Systematic Evaluation of Large\nLanguage Models of Code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming\n(MAPS 2022). Association for Computing Machinery, New York, NY, USA, 1\u201310.\nhttps://doi.org/10.1145/3520312.\n3534862 event-place: San Diego, CA, USA.\n[81] Frank F Xu, Junxian He, Graham Neubig, and Vincent J Hellendoorn. 2021. Capturing structural locality in non-\nparametric language models. arXiv preprint arXiv:2110.02870 (2021).\n[82] Shengzhe Xu, Ziqi Dong, and Na Meng. 2019. Meditor: inference and application of API migration edits. In 2019\nIEEE/ACM 27th International Conference on Program Comprehension (ICPC). IEEE, 335\u2013346.\n[83] Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. 2019. Learning to\nRepresent Edits. In ICLR 2019. https://www.microsoft.com/en-us/research/publication/learning-to-represent-edits/\narXiv:1810.13337 [cs.LG].\n[84] Jyh-shiarn Yur, Barbara G Ryder, and William A Landi. 1999. An incremental flow-and context-sensitive pointer\naliasing analysis. In Proceedings of the 21st International conference on Software Engineering. 442\u2013451.\n[85] Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023.\nRepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation.\narXiv preprint\narXiv:2303.12570 (2023).\n[86] Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. 2023. Planning\nwith Large Language Models for Code Generation. arXiv:2303.05510 [cs.LG]\n[87] Yuhao Zhang, Yasharth Bajpai, Priyanshu Gupta, Ameya Ketkar, Miltiadis Allamanis, Titus Barik, Sumit Gulwani,\nArjun Radhakrishna, Mohammad Raza, Gustavo Soares, and Ashish Tiwari. 2022. Overwatch: Learning patterns in\ncode edit sequences. Proc. ACM Program. Lang. 6, OOPSLA2 (2022), 395\u2013423. https://doi.org/10.1145/3563302\n34\n"
  },
  {
    "title": "DualToken-ViT: Position-aware Efficient Vision Transformer with Dual Token Fusion",
    "link": "https://arxiv.org/pdf/2309.12424.pdf",
    "upvote": "11",
    "text": "DualToken-ViT: Position-aware Efficient Vision Transformer with Dual Token\nFusion\nZhenzhen Chu*\nJiayu Chen\u2020\nCen Chen\u2021\nChengyu Wang\u00a7\nZiheng Wu\u00b6\nJun Huang||\nWeining Qian**\nAbstract\nSelf-attention-based vision transformers (ViTs) have emerged as a\nhighly competitive architecture in computer vision. Unlike convo-\nlutional neural networks (CNNs), ViTs are capable of global infor-\nmation sharing. With the development of various structures of ViTs,\nViTs are increasingly advantageous for many vision tasks. How-\never, the quadratic complexity of self-attention renders ViTs com-\nputationally intensive, and their lack of inductive biases of locality\nand translation equivariance demands larger model sizes compared\nto CNNs to effectively learn visual features. In this paper, we pro-\npose a light-weight and efficient vision transformer model called\nDualToken-ViT that leverages the advantages of CNNs and ViTs.\nDualToken-ViT effectively fuses the token with local information\nobtained by convolution-based structure and the token with global\ninformation obtained by self-attention-based structure to achieve\nan efficient attention structure. In addition, we use position-aware\nglobal tokens throughout all stages to enrich the global information,\nwhich further strengthening the effect of DualToken-ViT. Position-\naware global tokens also contain the position information of the im-\nage, which makes our model better for vision tasks. We conducted\nextensive experiments on image classification, object detection and\nsemantic segmentation tasks to demonstrate the effectiveness of\nDualToken-ViT. On the ImageNet-1K dataset, our models of differ-\nent scales achieve accuracies of 75.4% and 79.4% with only 0.5G\nand 1.0G FLOPs, respectively, and our model with 1.0G FLOPs\noutperforms LightViT-T using global tokens by 0.7%.\n1\nIntroduction\nIn recent years, vision transformers (ViTs) have emerged\nas a powerful architecture for various vision tasks such\nas image classification [9] and object detection [3, 39].\nThis is due to the ability of self-attention to capture global\ninformation from the image, providing sufficient and useful\nvisual features, while convolutional neural networks (CNNs)\nare limited by the size of convolutional kernel and can only\n*East China Normal University. 51215903091@stu.ecnu.edu.cn\n\u2020Alibaba Group. yunji.cjy@alibaba-inc.com\n\u2021East China Normal University. cenchen@dase.ecnu.edu.cn\n\u00a7Alibaba Group. chengyu.wcy@alibaba-inc.com\n\u00b6Alibaba Group. zhoulou.wzh@alibaba-inc.com\n||Alibaba Group. huangjun.hj@alibaba-inc.com\n**East China Normal University. wnqian@dase.ecnu.edu.cn\nFigure 1: Visualization of the attention map of position-\naware global tokens and the key token (the most important\npart of the image for the image classification task).\nIn\neach row, the first image is the input of our model, and\nthe second image represents the correlation between the red-\nboxed portion and each token in the position-aware global\ntokens containing 7\u00d77 tokens, where the red-boxed portion\nis the key token of the first image.\nextract local information. As the model size of ViTs and the\ndataset size increase, there is still no sign of a saturation in\nperformance, which is an advantage that CNNs do not have\nfor large models as well as for large datasets [9]. However,\nCNNs are more advantageous than ViTs in light-weight\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\narXiv:2309.12424v1  [cs.CV]  21 Sep 2023\nmodels due to certain inductive biases that ViTs lack. Since\nthe quadratic complexity of self-attention, the computational\ncost of ViTs can also be high. Therefore, it is challenging to\ndesign light-weight-based efficient ViTs.\nTo design more efficient and light-weight ViTs, [31, 6]\npropose a pyramid structure that divides the model into sev-\neral stages, with the number of tokens decreasing and the\nnumber of channels increasing by stage. [23, 2] focus on\nreducing the quadratic complexity of self-attention by sim-\nplifying and improving the structure of self-attention, but\nthey sacrifice the effectiveness of attention. Reducing the\nnumber of tokens involved in self-attention is also a com-\nmon approach, e.g., [31, 32, 25] downsample the key and the\nvalue in self-attention. Some works [19, 8] based on locally-\ngrouped self-attention reduce the complexity of the overall\nattention part by performing self-attention on grouped to-\nkens separately, but such methods may damage the sharing\nof global information. Some works also add a few additional\nlearnable parameters to enrich the global information of the\nbackbone, for example, [14, 5, 35] add the branch of global\ntokens that throughout all stages. This method can supple-\nment global information for local attention (such as locally-\ngrouped self-attention based and convolution-based struc-\ntures). These existing methods using global tokens, how-\never, consider only global information and ignore positional\ninformation that is very useful for vision tasks.\nIn this paper, we propose a light-weight and efficient vi-\nsion transformer model called DualToken-ViT. Our proposed\nmodel features a more efficient attention structure designed\nto replace self-attention. We combine the advantages of con-\nvolution and self-attention, leveraging them to extract local\nand global information respectively, and then fuse the out-\nputs of both to achieve an efficient attention structure. Al-\nthough window self-attention [19] is also able to extract lo-\ncal information, we observe that it is less efficient than the\nconvolution on our light-weight model. To reduce the com-\nputational complexity of self-attention in global information\nbroadcasting, we downsample the feature map that produces\nkey and value by step-wise downsampling, which can retain\nmore information during the downsampling process. More-\nover, we use position-aware global tokens throughout all\nstages to further enrich the global information. In contrast\nto the normal global tokens [14, 5, 35], our position-aware\nglobal tokens are also able to retain position information of\nthe image and pass it on, which can give our model an ad-\nvantage in vision tasks. As shown in Figure 1, the key to-\nken in the image generates higher correlation with the corre-\nsponding tokens in the position-aware global tokens, which\ndemonstrates the effectiveness of our position-aware global\ntokens. In summary, our contributions are as follows:\n\u2022 We design a light-weight and efficient vision trans-\nformer model called DualToken-ViT, which combines\nthe advantages of convolution and self-attention to\nachieve an efficient attention structure by fusing local\nand global tokens containing local and global informa-\ntion, respectively.\n\u2022 We further propose position-aware global tokens that\ncontain the position information of the image to enrich\nthe global information.\n\u2022 Among vision models of the same FLOPs magnitude,\nour DualToken-ViT shows the best performance on\nthe tasks of image classification, object detection and\nsemantic segmentation.\n2\nRelated work\nEfficient Vision Transformers.\nViTs are first proposed\nby [9], which applies transformer-based structures to com-\nputer vision. [31, 6] apply the pyramid structure to ViTs,\nwhich will incrementally transform the spatial information\ninto the rich semantic information.\nTo achieve efficient\nViTs, some works are beginning to find suitable alternatives\nto self-attention in computer vision tasks, such as [23, 2],\nwhich make the model smaller by reducing the complexity\nof self-attention. [31, 32, 25] reduce the required computa-\ntional resources by reducing the number of tokens involved\nin self-attention. [19, 8] use locally-grouped self-attention\nbased methods to reduce the complexity of the overall at-\ntention part. There are also some works that combine convo-\nlution into ViTs, for example, [32, 13] use convolution-based\nFFN (feed-forward neural network) to replace the normal\nFFN, [25] uses more convolution-based structure in the shal-\nlow stages of the model and more transformer-based struc-\nture in the deep stages of the model. Moreover, there are also\nmany works that use local information extracted by convo-\nlution or window self-attention to compensate for the short-\ncomings of ViTs, such as [22, 24].\nEfficient Attention Structures. For local attention, con-\nvolution works well for extracting local information in vi-\nsion tasks, e.g., [22, 24] add convolution to model to aggre-\ngate local information. Among transformer-based structures,\nlocally-grouped self-attention [19, 8] can also achieve local\nattention by adjusting the window size, and their complexity\nwill be much less than that of self-attention. For global atten-\ntion, self-attention [30] has a strong ability to extract global\ninformation, but on light-weight models, it may not be able\nto extract visual features well due to the lack of model size.\nMethods [14, 5, 35] using global tokens can also aggregate\nglobal information. They use self-attention to update global\ntokens and broadcast global information. Since the number\nof tokens in global tokens will not be set very large, the com-\nplexity will not be very high. Some works [24, 14, 25, 5]\nachieve a more efficient attention structure by combining\nboth local and global attention. In this paper, we implement\nan efficient attention structure by combining convolution-\nbased local attention and self-attention-based global atten-\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\nStage 3\n\u00d7L3\n7\u00d77\nFC\nInput\nStem\n\u00d7L2\n\u00d7L1\nFC\nAvg Pool\nFC\nOutput\nPosition-aware \nGlobal Tokens\n224\u00d7224 28\u00d728\nStage 1\n28\u00d728\nStage 2\n14\u00d714\nHead\n1\u00d71\nFC\nC\nMerge\nPatch\nMerge\nPatch\nConv \nEncoder\nPosition-\naware Token \nModule\n+\n\ud835\udc7f!\"#$!\n\ud835\udc7f%!\"&$!\n+\n+\nFFN\nBiDim Attn\n+\n\ud835\udc6e'()\n\ud835\udc7f!\"#$!\n\ud835\udc7f\n\ud835\udc6e\nDWConv\nNorm\nPWConv\nGELU\nPWConv\n+\n\ud835\udc7f\n\ud835\udc7f!\"#$!\nH\u00d7W\u00d7C\nH\u00d7W\u00d74C\nH\u00d7W\u00d7C\nConv Encoder\nDual Token \nBlock\nDual Token \nBlock\nDual Token \nBlock\nFigure 2: The architecture of DualToken-ViT. +\u20dd represents element-wise addition. c\u20dd represents concatenation in the token\naxis.\ntion, and use another branch of position-aware global tokens\nfor the delivery of global and position information through-\nout the model, where position-aware global tokens are an\nimprovement over global tokens [14, 5, 35].\n3\nMethodology\nAs shown in Figure 2, DualToken-ViT is designed based\non the 3-stage structure of LightViT [14]. The structure of\nstem and merge patch block in our model is the same as the\ncorresponding part in LightViT. FC refers to fully connected\nlayer. There are two branches in our model: image tokens\nand position-aware global tokens.\nThe branch of image\ntokens is responsible for obtaining various information from\nposition-aware global tokens, and the branch of position-\naware global tokens is responsible for updating position-\naware global tokens through the branch of image tokens\nand passing it on. In the attention part of each Dual Token\nBlock, we obtain information from the position-aware global\ntokens and fuse local and global information. We also add\nBiDim Attn (bi-dimensional attention) proposed in LightViT\nafter the FFN. In this section, we mainly introduce two\nimportant parts: the fusion of local and global information\nand position-aware global tokens.\n3.1\nFusion of Local and Global Information In the at-\ntention part of each Dual Token Block, we extract the local\nand global information through two branches, Conv Encoder\n(convolution encoder) and Position-aware Token Module, re-\nspectively, and then fuse these two parts.\nLocal Attention. We use Conv Encoder for local informa-\ntion extraction in each block of our model, since for light-\nweight models, local information extraction with convolu-\ntion will perform better than window self-attention. Conv\nEncoder has the same structure as the ConvNeXt block [20],\nwhich is represented as follows:\n(3.1)\nXlocal = X + PW2(GELU(PW1(LN(DW(X)))))\nwhere X is the input image tokens of size H\u00d7W\u00d7C, DW is\nthe depth-wise convolution, PW1 and PW2 are point-wise\nconvolution, LN is the layer norm, and Xlocal containing\nlocal information is the output of Conv Encoder.\nPosition-aware Token Module. This module is responsible\nfor extracting global information, and its structure is shown\nin Figure 3(b). In order to reduce the complexity of extract-\ning global information, we first downsample Xlocal contain-\ning local information and aggregate the global information.\nPosition-aware global tokens are then used to enrich global\ninformation. We end up broadcasting this global information\nto image tokens. The detailed process is as follows:\n(1) Downsampling. If the size of Xlocal is large and\ndoes not match the expected size, then it is downsampled\ntwice first.\nAfter that, local information is extracted by\nconvolution and downsampled twice, and the process is\nrepeated M times until the feature map size reaches the\nexpected size. Compared with the one-step downsampling\nmethod, this step-wise downsampling method can reduce\nthe loss of information during the downsampling process\nand retain more useful information.\nThe entire step-wise\ndownsampling process is represented as follows:\n(3.2)\nXds = \u03d5(DS(Xlocal))\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\nNormal\nPosition-aware\n(a)\nDownsampling\nConv\nDownsampling\nGlobal\nAggregation\nFusion\nGlobal\nBroadcast\n\ud835\udc7f!\"#$!\n\u00d7M\n\ud835\udc7f%$\n\ud835\udc6e&'(\nMLP\n\ud835\udc6e\n\ud835\udc6e&'(\n\ud835\udc7f\n\ud835\udc7f%!\")$!\n\ud835\udc7f*+\n(b)\nMSA\nMSA\n\ud835\udc7f\n\ud835\udc6e\n\ud835\udc78 \ud835\udc72 \ud835\udc7d\n\ud835\udc7d \ud835\udc72\n\ud835\udc78\nMSA\nFusion\n\ud835\udc7f\n\ud835\udc78 \ud835\udc72\n\ud835\udc7d\n. . .\n\ud835\udc7f!\"\nMSA\nMSA\n\ud835\udc7f\n\ud835\udc78 \ud835\udc72\n\ud835\udc7d\n\ud835\udc7d \ud835\udc72 \ud835\udc78\n. . .\n\ud835\udc7f!\"\n\ud835\udc6e\n\ud835\udc6e\n(Normal)\n(Normal)\n(Position-aware)\n(c)\nMSA\nMSA\n\ud835\udc7f\n\ud835\udc6e\n\ud835\udc78 \ud835\udc72 \ud835\udc7d\n\ud835\udc7d \ud835\udc72\n\ud835\udc78\nMSA\nFusion\n\ud835\udc7f\n\ud835\udc78 \ud835\udc72\n\ud835\udc7d\n. . .\n\ud835\udc7f!\"\nMSA\nMSA\n\ud835\udc7f\n\ud835\udc78 \ud835\udc72\n\ud835\udc7d\n\ud835\udc7d \ud835\udc72 \ud835\udc78\n. . .\n\ud835\udc7f!\"\n\ud835\udc6e\n\ud835\udc6e\n(Normal)\n(Normal)\n(Position-aware)\n(d)\nMSA\nMSA\n\ud835\udc7f\n\ud835\udc6e\n\ud835\udc78 \ud835\udc72 \ud835\udc7d\n\ud835\udc7d \ud835\udc72\n\ud835\udc78\nMSA\nFusion\n\ud835\udc7f\n\ud835\udc78 \ud835\udc72\n\ud835\udc7d\n. . .\n\ud835\udc7f!\"\nMSA\nMSA\n\ud835\udc7f\n\ud835\udc78 \ud835\udc72\n\ud835\udc7d\n\ud835\udc7d \ud835\udc72 \ud835\udc78\n. . .\n\ud835\udc7f!\"\n\ud835\udc6e\n\ud835\udc6e\n(Normal)\n(Normal)\n(Position-aware)\n(e)\nFigure 3: (a) shows normal global tokens and position-aware global tokens. (b) shows the structure of Position-aware Token\nModule using position-aware global tokens. (c), (d) and (e) show different methods of applying global tokens and show\nonly the interaction of X and G, omitting the processing of X and G. MSA represents multi-head self-attention.\nwhere DS represents twice the downsampling using average\npooling, \u03d5 represents that if the feature map size does not\nmatch the expected size, then several convolution and down-\nsampling operations are performed, with each operation rep-\nresented by DS(Conv(\u00b7)), and Xds represents the result after\nstep-wise downsampling.\n(2) Global Aggregation. Aggregation of global informa-\ntion using multi-head self-attention for the Xds output in the\nprevious step:\n(3.3)\nXga = MSA(Qds, Kds, Vds)\nwhere Qds, Kds and Vds are produced by Xds through linear\nprojection, and then Xga containing global information is\nobtained.\n(3) Enrich the global information. Use position-aware\nglobal tokens G to enrich Xga\u2019s global information:\n(3.4)\nGnew = Fuse(G, Xga)\nwhere Fuse is how the two are fused, which will be explained\nlater along with position-aware global tokens.\n(4) Global Broadcast. The global information in Gnew\nis broadcast to the image tokens using self-attention. This\nprocess is represented as follows:\n(3.5)\nXglobal = MSA(Qimage, Kg, Vg)\nwhere Qimage is produced by image tokens through linear\nprojection, Kg and Vg are produced by Gnew through linear\nprojection.\nFusion.\nFusing the two tokens, which contain local and\nglobal information respectively:\n(3.6)\nXnew = Xlocal + Xglobal\n3.2\nPosition-aware Global Tokens Global Aggregation\nis able to extract global information, but its scope is only in a\nblock. For this reason, we employ the position-aware global\ntokens G, which throughout all stages, to fuse with the Xga\nto obtain Gnew. Gnew has richer global information and can\nbe used to enrich the global information and function as new\nposition-aware global tokens to the next block after adding\nthe identical mapping.\nIn addition to global information,\nposition information in position-aware global tokens is also\ndelivered.\nGlobal Tokens with Position Information.\nFigure 3(a)\nshows the normal global tokens [14, 5, 35] and our position-\naware global tokens.\nThe one-dimensional global to-\nkens contain global information, and our two-dimensional\nposition-aware global tokens additionally contain location\ninformation. The normal global tokens use the way in Fig-\nure 3(c) to fuse X and G via multi-head self-attention and\nbroadcast the global information. Figure 3(e) is our Position-\naware Global Tokens, which we set to the same number of\ntokens as in Xga, and use weighted summation to fuse them:\n(3.7) Gnew = Fuse(G, Xga) = \u03b1MLP(G) + (1 \u2212 \u03b1)Xga\nwhere \u03b1 \u2208 [0, 1] is a weight that is set in advance. Although\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\nTable 1: Macro structures of two DualToken-ViT variants.\nB, C and H represent the number of blocks, channels and\nattention heads in multi-head self-attention, respectively.\nStage\nStride\nDualToken-ViT-T\nDualToken-ViT-S\nStage 1\n1/8\nB=2 C=48 H=2\nB=2 C=64 H=2\nStage 2\n1/16\nB=6 C=96 H=4\nB=6 C=128 H=4\nStage 3\n1/32\nB=4 C=192 H=8\nB=6 C=256 H=8\nour position-aware global tokens will cause the parameters\nto increase due to the increase in the number of tokens, it\nwill perform better than the normal global tokens.\nMLP. Before fusion, we use MLP for position-aware global\ntokens, which allows for a better fusion of the two. The\nformula of MLP is as follows:\n(3.8)\nG\u2032 = (Linear(GELU(Linear(G)))\nSince the normal MLP is only in the channel dimension, we\nalso attempt to use token-mixing MLP [28] to additionally\nextract the information in the spatial dimension:\n(3.9)\nG\u2032 = Transpose(Linear(Transpose(Linear(G))))\nwhere Transpose represents the transposition of spatial and\nchannel axis. We refer to this process as MixMLP.\n3.3\nArchitectures We design two DualToken-ViT models\nof different scales, and their macro structures are shown\nin Table 1.\nFor the task of image classification on the\nImageNet-1k [7] dataset, we default the size of the image\nafter data augment is 224\u00d7224. To prevent the complexity\nof the model from being too large, we set the size of position-\naware global tokens to 7\u00d77.\nIn this way, the M of the\nfirst two stages are set to 1 and 0 respectively, and the size\nof Xga is exactly 7\u00d77. In the third step, the feature map\nsize of the image tokens is exactly 7\u00d77, this eliminates\nthe need for local information extraction and downsampling,\nand allows these steps to be skipped directly. Furthermore,\nthe convolutional kernel size of depth-wise convolution in\nthe Conv Encoder of the first two stages is 5\u00d75 and 7\u00d77\nrespectively, and the convolutional kernel sizes in the step-\nwise downsampling are all 3\u00d73.\nIn addition, if the size\nof the input image changes (as in the object detection and\nsemantic segmentation tasks) and it is not possible to make\nXga the same size as the position-aware global tokens, we\nuse interpolation to change the size of Xga to the same size\nas the position-aware global tokens. In the fusion of G and\nXga, we set \u03b1 to 0.1.\n4\nExperiments\n4.1\nImage Classification\nSetting. We perform image classification experiments on the\nImageNet-1k [7] dataset and validate the top-1 accuracy on\nTable 2: Image classification performance on ImageNet-1k.\n\u201cmix\u201d indicates that our model uses MixMLP instead of\nnormal MLP.\nModel\nFLOPs\n(G)\nParams\n(M)\nTop-1\n(%)\nMobileNetV2 (1.4) [27]\n0.6\n6.9\n74.7\nMobileViTv1-XXS [22]\n0.4\n1.3\n69.0\nMobileViTv2-0.5 [23]\n0.5\n1.4\n70.2\nPVTv2-B0 [32]\n0.6\n3.4\n70.5\nEdgeViT-XXS [24]\n0.6\n4.1\n74.4\nDualToken-ViT-T (mix)\n0.5\n5.8\n75.4\nRegNetY-800M [26]\n0.8\n6.3\n76.3\nDeiT-Ti [29]\n1.3\n5.7\n72.2\nT2T-ViT-7 [36]\n1.1\n4.3\n71.7\nSimViT-Micro [15]\n0.7\n3.3\n71.1\nMobileViTv1-XS [22]\n1.0\n2.3\n74.8\nTNT-Ti [10]\n1.4\n6.1\n73.9\nLVT [34]\n0.9\n5.5\n74.8\nEdgeViT-XS [24]\n1.1\n6.7\n77.5\nXCiT-T12 [1]\n1.3\n6.7\n77.1\nLightViT-T [14]\n0.7\n9.4\n78.7\nDualToken-ViT-S (mix)\n1.0\n11.4\n79.4\nDualToken-ViT-S\n1.1\n11.9\n79.5\nits validation set. Our model is trained with 300 epochs and\nis based on 224\u00d7224 resolution images. For the sake of fair-\nness of the experiment, we try to choose models with this\nsetup and do not use extra datasets and pre-trained models to\ncompare with our model. We employ the AdamW [21] op-\ntimizer with betas (0.9, 0.999), weight decay 4e-2, learning\nrate 1e-3 and batch size 1024. And we use Cosine scheduler\nwith 20 warmup epoch. RandAugmentation (RandAug (2,\n9)), MixUp (alpha is 0.2), CutMix (alpha is 1.0), Random\nErasing (probability is 0.25), and drop path (rate is 0.1) are\nalso employed.\nResults. We compare DualToken-ViT to other vision models\non two scales of FLOPs, and the experimental results are\nshown in Table 2, where our model performs the best on\nboth scales. For example, DualToken-ViT-S (mix) achieves\n79.4% accuracy at 1.0G FLOPs, exceeding the current SoTA\nmodel LightViT-T [14]. And we improved the accuracy to\n79.5% after replacing MixMLP with normal MLP.\n4.2\nObject Detection and Instance Segmentation\nSetting. We perform experiments on the MS-COCO [18]\ndataset and use RetinaNet [17] and Mask R-CNN [11]\narchitectures with FPN [16] neck for a fair comparison.\nSince DualToken-ViT has only three stages, we modified\nthe FPN neck using the same method as in LightViT [14]\nto make our model compatible with these two detection\narchitectures.\nFor the RetinaNet architecture, we employ\nthe AdamW [21] optimizer for training, where betas (0.9,\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\nTable 3: Object detection and instance segmentation performance by Mask R-CNN on MS-COCO. All the models are\npretrained on ImageNet-1K.\nBackbone\nFLOPs\n(G)\nParams\n(M)\nMask R-CNN 1x schedule\nMask R-CNN 3x + MS schedule\nAP b AP b\n50 AP b\n75 AP m AP m\n50\nAP m\n75\nAP b AP b\n50 AP b\n75 AP m AP m\n50\nAP m\n75\nResNet-18 [12]\n207\n31\n34.0\n54.0\n36.7\n31.2\n51.0\n32.7\n36.9\n57.1\n40.0\n33.6\n53.9\n35.7\nResNet-50 [12]\n260\n44\n38.0\n58.6\n41.4\n34.4\n55.1\n36.7\n41.0\n61.7\n44.9\n37.1\n58.4\n40.1\nResNet-101 [12]\n493\n101\n40.4\n61.1\n44.2\n36.4\n57.7\n38.8\n42.8\n63.2\n47.1\n38.5\n60.1\n41.3\nPVTv1-T [31]\n208\n33\n36.7\n59.2\n39.3\n35.1\n56.7\n37.3\n39.8\n62.2\n43.0\n37.4\n59.3\n39.9\nPVTv1-S [31]\n245\n44\n40.4\n62.9\n43.8\n37.8\n60.1\n40.3\n43.0\n65.3\n46.9\n39.9\n62.5\n42.8\nPVTv2-B0 [32]\n196\n24\n38.2\n60.5\n40.7\n36.2\n57.8\n38.6\n-\n-\n-\n-\n-\n-\nLightViT-T [14]\n187\n28\n37.8\n60.7\n40.4\n35.9\n57.8\n38.0\n41.5\n64.4\n45.1\n38.4\n61.2\n40.8\nDualToken-ViT-S (mix)\n191\n30\n41.1\n63.5\n44.7\n38.1\n60.5\n40.5\n43.7\n65.8\n47.4\n39.9\n62.7\n42.8\nTable 4: Object detection performance by RetinaNet on MS-\nCOCO. All the models are pretrained on ImageNet-1K.\nBackbone\nFLOPs\n(G)\nParams\n(M)\nAP\nAP50 AP75 APS APM\nAPL\nResNet-18 [12]\n181\n21.3\n31.8\n49.6\n33.6\n16.3\n34.3\n43.2\nResNet-50 [12]\n239\n37.7\n36.3\n55.3\n38.6\n19.3\n40.0\n48.8\nPVTv1-T [31]\n221\n23.0\n36.7\n56.9\n38.9\n22.6\n38.8\n50.0\nPVTv2-B0 [32]\n178\n13.0\n37.2\n57.2\n39.5\n23.1\n40.4\n49.7\nConT-M [33]\n217\n27.0\n37.9\n58.1\n40.2\n23.0\n40.6\n50.4\nMF-508M [5]\n168\n17.9\n38.0\n58.3\n40.3\n22.9\n41.2\n49.7\nDualToken-ViT-S\n170\n20.0\n40.3\n61.2\n42.8\n25.5\n43.7\n55.2\n0.999), weight decay 1e-4, learning rate 1e-4 and batch\nsize 16. And we use the training schedule of 1\u00d7 from the\nMMDetection library. For the Mask R-CNN architecture,\nwe employ the AdamW optimizer for training, where betas\n(0.9, 0.999), weight decay 5e-2, learning rate 1e-4 and batch\nsize 16. We use the 1\u00d7 and 3\u00d7 training schedules from the\nMMDetection library, respectively. We use all the standard\nmetrics for object detection and instance segmentation of the\nMS-COCO dataset.\nResults. We compare the performance of our model with\nother models on Mask R-CNN and RetinaNet architectures,\nand the experimental results are shown in Table 3 and\nTable 4, respectively. Although our backbone has only three\nstages, DualToken-ViT-S without the maximum resolution\nstage still performs well in a model of the same FLOPs\nmagnitude. In particular, in the experiments of Mask R-CNN\narchitecture using the training schedule of 1\u00d7, our backbone\nachieves 41.1% AP b and 38.1% AP m at 191G FLOPs,\nwhich far exceeds LightViT-T [14] with similar FLOPs. This\nmay be related to our position-aware global tokens, which we\nwill explain in detail later.\n4.3\nSemantic Segmentation\nSetting. We perform experiments on ADE20K [38] dataset\nat 512\u00d7512 resolution and use DeepLabv3 [4] and PSP-\nNet [37] architectures for a fair comparison. For training, we\nemploy the AdamW [21] optimizer, where betas (0.9, 0.999),\nweight decay 1e-4, learning rate 2e-4 and batch size 32.\nResults. We compare the performance of our model with\nother models on DeepLabv3 and PSPNet architectures, and\nthe experimental results are shown in Table 5. DualToken-\nViT-S performs best among models of the same FLOPs\nmagnitude on both architectures.\n4.4\nAblation Study\nMLPs. We compare two MLPs performed on position-aware\nglobal tokens: normal MLP and MixMLP. The experimental\nresults on DualToken-ViT-S are shown in Table 2.\nThe\nnormal MLP is 0.1% more accurate than MixMLP, but it\nadds a little extra FLOPs and parameters. This is because\nMixMLP extracts information in the spatial dimension, it\nmay damage some positional information on the position-\naware global tokens.\nDifferent methods of applying global tokens. We compare\nthree methods of applying global tokens. The method [14, 5,\n35] in Figure 3(c) is the most common. Figure 3(e) shows\nour method that uses weighted summation to fuse Xga and\nG. Figure 3(d) combines the previous two methods, replac-\ning the weighted summation based fusion in our method with\nthe multi-head self-attention based fusion. We perform ex-\nperiments on DualToken-ViT-S. In the implementation, be-\ncause the complexity of the methods using multi-head self-\nattention based fusion is greater, we set the number of global\ntokens to 8, which is the same number as LightViT-T [14].\nThe experimental results are shown in Table 6, which show\nthat our position-aware-based method performs the best and\nhas 1.1M less parameters than the Normal method, with only\n0.06G more FLOPs. Since the other two methods employ\nmulti-head self-attention based fusion that requires many pa-\nrameters, whereas our method employs weighted summation\nbased fusion, our method has the smallest parameters. This\ndemonstrates the superiority of position-aware global tokens.\nThe number of tokens in position-aware global tokens.\nWe performed ablation study on the number of tokens in\nposition-aware global tokens on ImageNet-1k [7] dataset at\n224\u00d7224 resolution. In our model, the number of tokens\nin position-aware global tokens is set to 7\u00d77. In order to\ncompare the impact of different numbers of tokens on our\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\nTable 5: Semantic segmentation performance by DeepLabv3 and PSPNet on ADE20K dataset. All the models are pretrained\non ImageNet-1K.\nBackbone\nDeepLabv3\nPSPNet\nFLOPs (G)\nParams (M)\nmIoU (%)\nFLOPs (G)\nParams (M)\nmIoU (%)\nMobileNetv2 [27]\n75.4\n18.7\n34.1\n53.1\n13.7\n29.7\nMobileViTv2-1.0 [23]\n56.4\n13.4\n37.0\n40.3\n9.4\n36.5\nDualToken-ViT-S\n68.4\n26.3\n39.0\n58.3\n21.7\n38.8\nTable 6: Ablation study on the method of applying global\ntokens.\nNormal, Normal* and Position-aware represent\nthe methods in Figure 3(c), Figure 3(d) and Figure 3(e),\nrespectively.\nGlobal Tokens\nFLOPs (G) Params (M) Top-1 (%)\nNormal\n0.99\n13.0\n79.2\nNormal*\n0.98\n13.4\n79.0\nPosition-aware\n1.05\n11.9\n79.5\nTable 7: Ablation study on the number of tokens in position-\naware global tokens.\nNumber\nFLOPs (G)\nParams (M)\nTop-1 (%)\n0\n0.99\n10.8\n79.2\n3\u00d73\n0.93\n11.9\n79.1\n4\u00d74\n0.95\n11.9\n79.2\n5\u00d75\n0.98\n11.9\n79.4\n6\u00d76\n1.01\n11.9\n79.3\n7\u00d77\n1.05\n11.9\n79.5\n8\u00d78\n1.10\n11.9\n79.3\nmodel, we experiment with various settings for the number\nof tokens.\nIf the number of tokens is set to 0, then the\nposition-aware global tokens are not used.\nBecause the\nsize of Xga and the position-aware global tokens will not\nmatch when the number of tokens is not 7\u00d77, we will use\ninterpolation for Xga to make the size of the two match.\nThe experimental results on DualToken-ViT-S are shown in\nTable 7. The model with the number of tokens set to 7\u00d77 has\nthe best performance due to the sufficient number of tokens\nand does not damage the information by the interpolation\nmethod. Compared to the 0 token setting, our setting is 0.3%\nmore accurate and will only increase by 0.06G FLOPs and\n1.1M parameters, which demonstrates the effectiveness of\nour position-aware global tokens.\nLocal attention. We compare the role of Conv Encoder and\nwindow self-attention [19] in our model. And we set the\nwindow size of window self-attention to 7. The experimen-\ntal results on DualToken-ViT-S (mix) are shown in Table 8.\nThe model using Conv Encoder as local attention achieves\nbetter performance, with 0.8% more accuracy than when us-\ning window self-attention, and the number of FLOPs and pa-\nTable 8: Ablation study on the method of local attention.\nLocal Attention\nFLOPs (G)\nParams (M)\nTop-1 (%)\nWindow Self-attention\n0.92\n10.8\n78.6\nConv Encoder\n1.04\n11.4\n79.4\nTable 9: Ablation study on the step-wise downsampling part\nof the Position-aware Token Module.\nDownsampling\nFLOPs (G)\nParams (M)\nTop-1 (%)\none-step\n1.01\n11.3\n79.2\nstep-wise\n1.04\n11.4\n79.4\nrameters does not increase very much. The performance of\nConv Encoder is superior for two reasons. On the one hand,\nthe convolution-based structure will be more advantageous\nthan the transformer-based structure for light-weight models.\nOn the other hand, window self-attention damages the posi-\ntion information in the position-aware global tokens. This\nis because the transformer-based structure does not have the\ninductive bias of locality. And in window self-attention, the\nfeatures in the edge part of the window will be damaged due\nto the feature map being split into several small parts.\nDownsampling.\nWe perform ablation study on the step-\nwise downsampling part of the position-aware token mod-\nule. For the setup of one-step downsampling, we directly\ndownsample Xlocal to get the desired size, and then input\nit to the Global Aggregation. The experimental results on\nDualToken-ViT-S (mix) are shown in Table 9.\nStep-wise\ndownsampling is 0.2% more accurate than one-step down-\nsampling, and FLOPs and parameters are only 0.03G and\n0.1M more, respectively.\nThe reason for this is that the\nmethod of step-wise can retain more information by convo-\nlution during the downsampling process.\n4.5\nVisualization To get a more intuitive feel for the posi-\ntion information contained in position-aware global tokens,\nwe visualize the attention map of the Global Broadcast for\nthe last block in DualToken-ViT-S (mix), and the results are\nshown in Figure 4. In each row, the second and third images\nshow that the key tokens in the first image generate higher\ncorrelation with the corresponding tokens in the position-\naware global tokens. And in the second image in each row,\nthe non-key tokens in the first image generate more uniform\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\nFigure 4: Visualization of the attention map of the Global Broadcast for the last block in our model. In each row, each\nsubimage in the second image represents the correlation between this part of the first image and each token in the position-\naware global tokens, and the third image shows the 8 tokens with the highest correlation in each subimage. The fourth image\nin each row represents the average of all subimages in the second image and shows the 8 tokens with the highest correlation.\ncorrelation with each part of the position-aware global to-\nkens. The fourth image in each row shows that the overall\nposition-aware global tokens have a higher correlation with\nthe key tokens of the first image. These demonstrate that our\nposition-aware global tokens contain position information.\n5\nConclusion\nIn this paper, we propose a light-weight and efficient visual\ntransformer model called DualToken-ViT. It achieves effi-\ncient attention structure by combining convolution-based lo-\ncal attention and self-attention-based global attention. We\nimprove global tokens and propose position-aware global to-\nkens that contain both global and position information. We\ndemonstrate the effectiveness of our model on image classi-\nfication, object detection and semantic segmentation tasks.\nReferences\n[1] A. ALI, H. TOUVRON, M. CARON, P. BOJANOWSKI,\nM. DOUZE,\nA. JOULIN,\nI. LAPTEV,\nN. NEVEROVA,\nG. SYNNAEVE, J. VERBEEK, ET AL., Xcit: Cross-covariance\nimage transformers, Advances in neural information process-\ning systems, 34 (2021), pp. 20014\u201320027.\n[2] D. BOLYA, C.-Y. FU, X. DAI, P. ZHANG, AND J. HOFF-\nMAN, Hydra attention: Efficient attention with many heads, in\nComputer Vision\u2013ECCV 2022 Workshops: Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part VII, Springer, 2023,\npp. 35\u201349.\n[3] N. CARION, F. MASSA, G. SYNNAEVE, N. USUNIER,\nA. KIRILLOV, AND S. ZAGORUYKO, End-to-end object de-\ntection with transformers, in European conference on com-\nputer vision, Springer, 2020, pp. 213\u2013229.\n[4] L.-C.\nCHEN,\nG.\nPAPANDREOU,\nF.\nSCHROFF,\nAND\nH. ADAM, Rethinking atrous convolution for semantic image\nsegmentation, arXiv preprint arXiv:1706.05587, (2017).\n[5] Y. CHEN, X. DAI, D. CHEN, M. LIU, X. DONG, L. YUAN,\nAND Z. LIU, Mobile-former: Bridging mobilenet and trans-\nformer, in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2022, pp. 5270\u20135279.\n[6] X. CHU, Z. TIAN, Y. WANG, B. ZHANG, H. REN, X. WEI,\nH. XIA, AND C. SHEN, Twins: Revisiting the design of\nspatial attention in vision transformers, Advances in Neural\nInformation Processing Systems, 34 (2021), pp. 9355\u20139366.\n[7] J. DENG, W. DONG, R. SOCHER, L.-J. LI, K. LI, AND\nL. FEI-FEI, Imagenet:\nA large-scale hierarchical image\ndatabase, in 2009 IEEE conference on computer vision and\npattern recognition, Ieee, 2009, pp. 248\u2013255.\n[8] X. DONG, J. BAO, D. CHEN, W. ZHANG, N. YU, L. YUAN,\nD. CHEN, AND B. GUO, Cswin transformer: A general vision\ntransformer backbone with cross-shaped windows, in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2022, pp. 12124\u201312134.\n[9] A. DOSOVITSKIY, L. BEYER, A. KOLESNIKOV, D. WEIS-\nSENBORN, X. ZHAI, T. UNTERTHINER, M. DEHGHANI,\nM. MINDERER, G. HEIGOLD, S. GELLY, ET AL., An image\nis worth 16x16 words: Transformers for image recognition at\nscale, arXiv preprint arXiv:2010.11929, (2020).\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\n[10] K. HAN, A. XIAO, E. WU, J. GUO, C. XU, AND Y. WANG,\nTransformer in transformer, Advances in Neural Information\nProcessing Systems, 34 (2021), pp. 15908\u201315919.\n[11] K. HE, G. GKIOXARI, P. DOLL \u00b4AR, AND R. GIRSHICK,\nMask r-cnn, in Proceedings of the IEEE international confer-\nence on computer vision, 2017, pp. 2961\u20132969.\n[12] K. HE, X. ZHANG, S. REN, AND J. SUN, Deep residual\nlearning for image recognition, in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016,\npp. 770\u2013778.\n[13] H. HUANG, X. ZHOU, J. CAO, R. HE,\nAND T. TAN,\nVision transformer with super token sampling, arXiv preprint\narXiv:2211.11167, (2022).\n[14] T. HUANG, L. HUANG, S. YOU, F. WANG, C. QIAN, AND\nC. XU, Lightvit: Towards light-weight convolution-free vision\ntransformers, arXiv preprint arXiv:2207.05557, (2022).\n[15] G. LI, D. XU, X. CHENG, L. SI, AND C. ZHENG, Simvit:\nExploring a simple vision transformer with sliding windows,\nin 2022 IEEE International Conference on Multimedia and\nExpo (ICME), IEEE, 2022, pp. 1\u20136.\n[16] T.-Y. LIN, P. DOLL \u00b4AR, R. GIRSHICK, K. HE, B. HARIHA-\nRAN, AND S. BELONGIE, Feature pyramid networks for ob-\nject detection, in Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 2017, pp. 2117\u20132125.\n[17] T.-Y. LIN,\nP. GOYAL,\nR. GIRSHICK,\nK. HE,\nAND\nP. DOLL \u00b4AR, Focal loss for dense object detection, in Proceed-\nings of the IEEE international conference on computer vision,\n2017, pp. 2980\u20132988.\n[18] T.-Y. LIN, M. MAIRE, S. BELONGIE, J. HAYS, P. PERONA,\nD. RAMANAN, P. DOLL \u00b4AR, AND C. L. ZITNICK, Microsoft\ncoco:\nCommon objects in context, in Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13, Springer,\n2014, pp. 740\u2013755.\n[19] Z. LIU, Y. LIN, Y. CAO, H. HU, Y. WEI, Z. ZHANG,\nS. LIN, AND B. GUO, Swin transformer: Hierarchical vi-\nsion transformer using shifted windows, in Proceedings of\nthe IEEE/CVF international conference on computer vision,\n2021, pp. 10012\u201310022.\n[20] Z. LIU, H. MAO, C.-Y. WU, C. FEICHTENHOFER, T. DAR-\nRELL, AND S. XIE, A convnet for the 2020s, in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 11976\u201311986.\n[21] I. LOSHCHILOV AND F. HUTTER, Decoupled weight decay\nregularization, arXiv preprint arXiv:1711.05101, (2017).\n[22] S. MEHTA AND M. RASTEGARI, Mobilevit: light-weight,\ngeneral-purpose, and mobile-friendly vision transformer,\narXiv preprint arXiv:2110.02178, (2021).\n[23]\n, Separable self-attention for mobile vision transform-\ners, arXiv preprint arXiv:2206.02680, (2022).\n[24] J. PAN, A. BULAT, F. TAN, X. ZHU, L. DUDZIAK, H. LI,\nG. TZIMIROPOULOS, AND B. MARTINEZ, Edgevits: Com-\npeting light-weight cnns on mobile devices with vision trans-\nformers, in Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceed-\nings, Part XI, Springer, 2022, pp. 294\u2013311.\n[25] Z. PAN, J. CAI, AND B. ZHUANG, Fast vision transformers\nwith hilo attention, arXiv preprint arXiv:2205.13213, (2022).\n[26] I. RADOSAVOVIC, R. P. KOSARAJU, R. GIRSHICK, K. HE,\nAND P. DOLL \u00b4AR, Designing network design spaces, in Pro-\nceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2020, pp. 10428\u201310436.\n[27] M. SANDLER, A. HOWARD, M. ZHU, A. ZHMOGINOV,\nAND L.-C. CHEN, Mobilenetv2: Inverted residuals and linear\nbottlenecks, in Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 2018, pp. 4510\u20134520.\n[28] I. O. TOLSTIKHIN,\nN. HOULSBY,\nA. KOLESNIKOV,\nL.\nBEYER,\nX.\nZHAI,\nT.\nUNTERTHINER,\nJ.\nYUNG,\nA. STEINER, D. KEYSERS, J. USZKOREIT, ET AL., Mlp-\nmixer: An all-mlp architecture for vision, Advances in neural\ninformation processing systems, 34 (2021), pp. 24261\u201324272.\n[29] H.\nTOUVRON,\nM.\nCORD,\nM.\nDOUZE,\nF.\nMASSA,\nA. SABLAYROLLES, AND H. J\u00b4EGOU, Training data-efficient\nimage transformers & distillation through attention, in In-\nternational conference on machine learning, PMLR, 2021,\npp. 10347\u201310357.\n[30] A. VASWANI, N. SHAZEER, N. PARMAR, J. USZKOREIT,\nL. JONES, A. N. GOMEZ, \u0141. KAISER, AND I. POLOSUKHIN,\nAttention is all you need, Advances in neural information\nprocessing systems, 30 (2017).\n[31] W. WANG, E. XIE, X. LI, D.-P. FAN, K. SONG, D. LIANG,\nT. LU, P. LUO, AND L. SHAO, Pyramid vision transformer: A\nversatile backbone for dense prediction without convolutions,\nin Proceedings of the IEEE/CVF international conference on\ncomputer vision, 2021, pp. 568\u2013578.\n[32]\n, Pvt v2:\nImproved baselines with pyramid vision\ntransformer, Computational Visual Media, 8 (2022), pp. 415\u2013\n424.\n[33] H. YAN, Z. LI, W. LI, C. WANG, M. WU, AND C. ZHANG,\nContnet: Why not use convolution and transformer at the\nsame time?, arXiv preprint arXiv:2104.13497, (2021).\n[34] C. YANG, Y. WANG, J. ZHANG, H. ZHANG, Z. WEI,\nZ. LIN, AND A. YUILLE, Lite vision transformer with en-\nhanced self-attention, in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2022,\npp. 11998\u201312008.\n[35] T. YAO,\nY. LI,\nY. PAN,\nY. WANG,\nX.-P. ZHANG,\nAND T. MEI, Dual vision transformer, arXiv preprint\narXiv:2207.04976, (2022).\n[36] L. YUAN, Y. CHEN, T. WANG, W. YU, Y. SHI, Z.-H.\nJIANG, F. E. TAY, J. FENG, AND S. YAN, Tokens-to-token\nvit: Training vision transformers from scratch on imagenet,\nin Proceedings of the IEEE/CVF international conference on\ncomputer vision, 2021, pp. 558\u2013567.\n[37] H. ZHAO, J. SHI, X. QI, X. WANG, AND J. JIA, Pyramid\nscene parsing network, in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2017, pp. 2881\u2013\n2890.\n[38] B. ZHOU, H. ZHAO, X. PUIG, T. XIAO, S. FIDLER,\nA. BARRIUSO, AND A. TORRALBA, Semantic understanding\nof scenes through the ade20k dataset, International Journal of\nComputer Vision, 127 (2019), pp. 302\u2013321.\n[39] X. ZHU, W. SU, L. LU, B. LI, X. WANG, AND J. DAI,\nDeformable detr: Deformable transformers for end-to-end\nobject detection, arXiv preprint arXiv:2010.04159, (2020).\nCopyright \u00a9 2024 by SIAM\nUnauthorized reproduction of this article is prohibited\n"
  },
  {
    "title": "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation",
    "link": "https://arxiv.org/pdf/2309.13042.pdf",
    "upvote": "9",
    "text": "Noname manuscript No.\n(will be inserted by the editor)\nMosaicFusion: Diffusion Models as Data Augmenters for Large\nVocabulary Instance Segmentation\nJiahao Xie \u00b7 Wei Li \u00b7 Xiangtai Li \u00b7 Ziwei Liu \u00b7 Yew Soon Ong \u00b7 Chen Change Loy\nReceived: date / Accepted: date\nAbstract We present MosaicFusion, a simple yet effective\ndiffusion-based data augmentation approach for large vo-\ncabulary instance segmentation. Our method is training-free\nand does not rely on any label supervision. Two key designs\nenable us to employ an off-the-shelf text-to-image diffusion\nmodel as a useful dataset generator for object instances and\nmask annotations. First, we divide an image canvas into sev-\neral regions and perform a single round of diffusion process\nto generate multiple instances simultaneously, conditioning\non different text prompts. Second, we obtain corresponding\ninstance masks by aggregating cross-attention maps associ-\nated with object prompts across layers and diffusion time\nsteps, followed by simple thresholding and edge-aware re-\nfinement processing. Without bells and whistles, our Mo-\nsaicFusion can produce a significant amount of synthetic la-\nbeled data for both rare and novel categories. Experimen-\ntal results on the challenging LVIS long-tailed and open-\nvocabulary benchmarks demonstrate that MosaicFusion can\nJiahao Xie\nS-Lab, Nanyang Technological University, Singapore\nE-mail: jiahao003@ntu.edu.sg\nWei Li\nS-Lab, Nanyang Technological University, Singapore\nE-mail: wei.l@ntu.edu.sg\nXiangtai Li\nS-Lab, Nanyang Technological University, Singapore\nE-mail: xiangtai.li@ntu.edu.sg\nZiwei Liu\nS-Lab, Nanyang Technological University, Singapore\nE-mail: ziwei.liu@ntu.edu.sg\nYew Soon Ong\nNanyang Technological University, Singapore\nE-mail: asysong@ntu.edu.sg\nChen Change Loy\nS-Lab, Nanyang Technological University, Singapore\nE-mail: ccloy@ntu.edu.sg\nsignificantly improve the performance of existing instance\nsegmentation models, especially for rare and novel cate-\ngories. Code will be released at https://github.com/\nJiahao000/MosaicFusion.\nKeywords Text-to-image diffusion models \u00b7 Long tail \u00b7\nOpen vocabulary \u00b7 Instance segmentation\n1 Introduction\nInstance segmentation is a fundamental yet challenging\ntask\u2014identifying and segmenting each object in an image\u2014\nthat empowers remarkable applications in autonomous driv-\ning, robotics and medical imaging (Lin et al, 2014; Cordts\net al, 2016; Gupta et al, 2019; Waqas Zamir et al, 2019;\nKuznetsova et al, 2020). However, manually labeling a\nlarge-scale instance segmentation dataset is extremely labo-\nrious and expensive as annotators need to provide a mask\nwith precise boundaries and assign a unique label for each\nobject instance. The cost of such a dense labeling process\nincreases dramatically for a very large vocabulary, due to\nhighly diverse and complicated visual scenes in different\napplications. As a result, it is prohibitive to scale up the\nvocabulary size of instance segmentation datasets. As illus-\ntrated in Fig. 1, such a predicament of data scarcity becomes\neven worse under a natural data distribution that contains\nlow-sample rare categories and out-of-distributed novel cat-\negories, both of which lead to poor performance of state-\nof-the-art instance segmentation models in long-tailed and\nopen-vocabulary scenarios.\nIn the literature, researchers have sought ways to re-\nduce the heavy reliance on labeled training data for instance\nsegmentation. One popular method, Copy-Paste (Ghiasi\net al, 2021), enhances datasets by placing objects onto\nbackgrounds. However, while effective, this straightforward\narXiv:2309.13042v1  [cs.CV]  22 Sep 2023\n2\nJiahao Xie et al.\nAP\nw/ MosaicFusion\nResNet-50\n   frequent / common \nrare\nnovel\ngenerated instances with masks\nMosaicFusion\n35.9\n39.0\n31.7\n34.9\nAP\nBaseline\n18.0\n20.6\nAP\nAP\nResNet-50x64\nSwin-B\n9.6\n15.2\nResNet-50\nFig. 1: Long-tailed and open-vocabulary instance segmentation on LVIS (Gupta et al, 2019) using our MosaicFusion\ndata augmentation approach, which can generate meaningful synthetic labeled data for both rare and novel categories with-\nout further training and label supervision. We evaluate the model with the standard mask AP (i.e., APr and APnovel). Mo-\nsaicFusion provides strong gains on all considered baseline methods (e.g., Mask R-CNN (He et al, 2017) with ResNet-50,\nBox-Supervised CenterNet2 (Zhou et al, 2022b) with Swin-B, F-VLM (Kuo et al, 2023) with ResNet-50 and ResNet-50x64)\ntechnique may not offer the extensive diversity and high-\nquality masks needed for optimal model training. Recently,\na promising line of research (Zhang et al, 2021c; Baranchuk\net al, 2022; Li et al, 2022b) builds upon deep genera-\ntive models to synthesize the massive number of images\nwith pixel-level labels. However, existing approaches rely\non training auxiliary segmentation architectures on top of\nGANs (Zhang et al, 2021c; Li et al, 2022b) or diffusion mod-\nels (Baranchuk et al, 2022), limiting their applications only\nto a pre-defined set of classes.\nIn this study, we aim to address the challenge of lim-\nited labeled training data for large vocabulary instance seg-\nmentation, especially in long-tailed and open-set scenarios.\nDriven by the groundbreaking advancements in large-scale\ntext-to-image (T2I) diffusion models\u2014like Google\u2019s Ima-\ngen (Saharia et al, 2022), OpenAI\u2019s DALL-E 2 (Ramesh\net al, 2022), and Stability AI\u2019s Stable Diffusion (Rombach\net al, 2022)\u2014which excel in generating photorealistic im-\nages from free-form text prompts, we delve into produc-\ning vast amounts of synthetic data using T2I models to en-\nhance instance segmentation training. Yet, this promising\navenue comes with two primary hurdles: 1) How can we pro-\nduce high-quality scene images featuring multiple objects?\n2) How can we derive the corresponding per-pixel instance\nlabels, namely, instance masks, from these diffusion models\nwithout additional model training or label supervision?\nTo this end, we propose MosaicFusion, a diffusion-based\ndata augmentation pipeline to generate images and masks\nsimultaneously for large vocabulary instance segmentation.\nOur method consists of two components: image generation\nand mask generation. For image generation, we first di-\nvide an image canvas into several regions. We then run the\ndiffusion process on each region simultaneously, using the\nshared noise prediction model, while conditioning on a dif-\nferent text prompt. In this way, we can control the diffu-\nsion model to generate multiple objects at specific locations\nwithin a single image. For mask generation, we first aggre-\ngate the cross-attention maps of the text token correspond-\ning to a certain object across different layers and time steps\nin the diffusion process. We then threshold the aggregated\nattention maps and use standard edge-aware refinement al-\ngorithms, such as Bilateral Solver (BS) (Barron and Poole,\n2016), to further refine the mask boundaries. The generated\nimages and corresponding instance masks are finally used as\na synthetic labeled dataset to train off-the-shelf models for\ninstance segmentation.\nOverall, our main contributions are summarized as fol-\nlows:\n1) We propose MosaicFusion, an automatic diffusion-\nbased data augmentation pipeline to expand the existing in-\nstance segmentation dataset. Our method can generate im-\nages and masks simultaneously without relying on addi-\ntional off-the-shelf object detection and segmentation mod-\nels to label the data further.\n2) Our method allows us to generate customized objects\nat specific locations in a single image. We study both single-\nobject and multi-object image generation scenarios and re-\nveal that generating images with multiple objects is more\nbeneficial than generating those with a single object.\n3) Extensive experiments on two challenging bench-\nmarks, i.e., long-tailed and open-vocabulary instance seg-\nmentation on LVIS (Gupta et al, 2019), demonstrate that our\nmethod can significantly improve the performance of exist-\ning object detectors and instance segmentors, especially for\nrare and unseen categories. Fig. 1 shows the non-trivial per-\nformance improvement achieved with MosaicFusion.\nMosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation\n3\nTable 1: Comparison with other diffusion-based data augmentation works in terms of key properties. Our MosaicFusion\nis the only method with all these desired properties\nProperties\nGe et al (2022)\nZhao et al (2023)\nLi et al (2023)\nOurs\nTraining-free\n\u2713\n\u2713\n\u2717\n\u2713\nDirectly generate multiple objects\n\u2717\n\u2717\n\u2717\n\u2713\nAgnostic to detection architectures\n\u2713\n\u2713\n\u2717\n\u2713\nWithout extra detectors or segmentors\n\u2717\n\u2717\n\u2717\n\u2713\n2 Related Work\nText-to-image (T2I) diffusion models. Recent advances\nin large-scale generative models, such as Imagen (Saharia\net al, 2022), DALL-E 2 (Ramesh et al, 2022), and Sta-\nble Diffusion (Rombach et al, 2022), have brought signifi-\ncant progress in AI-powered image creation by training on\ninternet-scale text-image datasets. These models can be con-\nditioned on free-form text prompts to generate photoreal-\nistic images. This enables improved controllability in per-\nsonalized image generation (Gal et al, 2023), content edit-\ning (Hertz et al, 2023), zero-shot translation (Parmar et al,\n2023), and concept customization (Kumari et al, 2022).\nSuch great flexibility and scalability also bring the poten-\ntial of transforming a T2I model as an effective training data\ngenerator for large vocabulary instance segmentation.\nData augmentation for instance segmentation. Instance\nsegmentation models are data-hungry and label-expensive.\nTherefore, many works aim at improving the performance\nfrom the data augmentation perspective. Several earlier\nworks adopt synthesis methods via rendering graphics (Su\net al, 2015; Hinterstoisser et al, 2018) or copying from com-\nputer games (Richter et al, 2016). Due to the huge do-\nmain gap between synthetic and real data, another line of\nworks (Dwibedi et al, 2017; Dvornik et al, 2018; Fang et al,\n2019; Ghiasi et al, 2021) use real image sets (Gupta et al,\n2019). For instance, Copy-Paste (Ghiasi et al, 2021) shows\nthat pasting objects onto the background using their masks\ncan work well. However, these methods are not scalable\nfor large vocabulary settings since the augmented instances\nare still confined to existing ones in the training data, i.e.,\nthey cannot create new instances for rare or novel categories\nwith substantially more diversity. In contrast, our goal is\nto generate multiple diverse rare or novel objects on the\nsame image with their masks. Our method is orthogonal to\nprior works using real data for augmentation, as verified in\nSect. 4.2. Concurrently, several works (Ge et al, 2022; Zhao\net al, 2023; Li et al, 2023) also use diffusion models for in-\nstance segmentation augmentation. The comparisons with\nthese works are summarized in Table 1. Our MosaicFusion\nis the only method that is training-free, able to directly gen-\nerate multiple objects and corresponding masks without re-\nlying on off-the-shelf detectors or segmentors, and compat-\nible with various detection architectures.\nLong-tailed instance segmentation. This task aims to han-\ndle class imbalance problems in instance segmentation.\nMost approaches adopt data re-sampling (Gupta et al, 2019;\nLiu et al, 2020; Wu et al, 2020), loss re-weighting (Ren et al,\n2020; Tan et al, 2020a, 2021; Zhang et al, 2021b; Wang\net al, 2021b) and decoupled training (Li et al, 2020; Wang\net al, 2020). In particular, several studies (Liu et al, 2020)\nadopt image-level re-sampling. However, these approaches\nresult in bias of instance co-occurrence. To deal with this\nissue, several works (Hu et al, 2020; Wu et al, 2020; Zang\net al, 2021) perform more fine-grained re-sampling at the\ninstance or feature level. For loss re-weighting, most ap-\nproaches (Ren et al, 2020; Tan et al, 2020a; Wang et al,\n2021a) rebalance the ratio of positive and negative samples\nduring training. Meanwhile, decoupled training methods (Li\net al, 2020; Wang et al, 2020) introduce different calibra-\ntion frameworks to improve classification results. In con-\ntrast, MosaicFusion focuses on data augmentation and im-\nproving different detectors by generating new rare class ex-\namples.\nOpen-vocabulary detection and segmentation. This task\naims to detect and segment novel categories in a large con-\ncept space with the help of pre-trained vision-language mod-\nels (VLMs) (Zhang et al, 2023; Wu et al, 2023). OVR-\nCNN (Zareian et al, 2021) first puts forth the concept\nof open-vocabulary object detection. It is pre-trained on\nimage-caption data to recognize novel objects and then\nfine-tuned for zero-shot detection. With the development of\nVLMs (Radford et al, 2021a; Jia et al, 2021), ViLD (Gu et al,\n2022) is the first work to distill the rich representations of\npre-trained CLIP (Radford et al, 2021a) into the detector.\nSubsequently, many studies (Li et al, 2022a; Zhong et al,\n2022; Zang et al, 2022; Ghiasi et al, 2022; Du et al, 2022;\nGao et al, 2022; Minderer et al, 2022; Chen et al, 2022;\nRasheed et al, 2022; Kuo et al, 2023) propose different ways\nto adapt VLM knowledge into open-vocabulary detection\nand segmentation. For example, DetPro (Du et al, 2022) in-\ntroduces a fine-grained automatic prompt learning scheme,\nand F-VLM (Kuo et al, 2023) adopts frozen VLMs to output\nnovel categories from cropped CLIP features directly. An-\nother related work, Detic (Zhou et al, 2022b), improves the\nperformance on novel categories with the extra large-scale\nimage classification dataset (i.e., ImageNet-21K (Deng et al,\n2009)) by supervising the max-size proposal with all image\n4\nJiahao Xie et al.\nlabels. However, it needs more training data and the vocab-\nulary size is limited by the classification dataset. As verified\nin Sect. 4.3, MosaicFusion is orthogonal to the CLIP knowl-\nedge in the open-vocabulary setting, which boosts the state-\nof-the-art F-VLM (Kuo et al, 2023) by a significant margin.\n3 MosaicFusion\nOur MosaicFusion is a training-free diffusion-based dataset\naugmentation pipeline that can produce image and mask\npairs with multiple objects simultaneously using the off-the-\nshelf text-to-image diffusion models. The overall pipeline of\nour approach is illustrated in Fig. 2. It contains two com-\nponents: image generation and mask generation. In the fol-\nlowing subsections, we first introduce some preliminaries\nw.r.t. latent diffusion models and cross-attention layers in\nSect. 3.1. We then detail the image and mask generation pro-\ncess of MosaicFusion in Sect. 3.2 and Sect. 3.3, respectively.\n3.1 Preliminary\nStable Diffusion. We build our data generation frame-\nwork upon the state-of-the-art text-to-image latent diffusion\nmodel, i.e., Stable Diffusion (SD) (Rombach et al, 2022).\nSD runs the diffusion process in a compressed latent space\nrather than the pixel space for efficiency. It consists of three\ncomponents: i) a variational autoencoder (VAE) (Kingma\nand Welling, 2014) that encodes and decodes the latent vec-\ntors for images; ii) a time-conditional U-Net (Ronneberger\net al, 2015) that denoises the latent vectors at varying time\nsteps; iii) a text encoder like CLIP (Radford et al, 2021b)\nthat encodes the text inputs into textural embeddings. The\npre-trained VAE encodes images as latent vectors for diffu-\nsion training. During inference, the generation process starts\nfrom a random Gaussian noise zT \u223c N(0, 1) in the latent\nspace, and is iteratively denoised conditioned on the text\nprompt p that is encoded via the text encoder. Specifically, at\neach denoising step t = 1, ..., T, given zt and p, U-Net pre-\ndicts the noise estimation term \u03f5 and subtracts it from zt to\nobtain zt\u22121. The final denoised latent z0 is then passed to the\nVAE decoder D to obtain the generated image I = D(z0).\nCross-attention layers. The vision-language interaction oc-\ncurs during the denoising process in the U-Net backbone,\nwhere the embeddings of visual and textual features are\nfused using cross-attention layers that produce spatial at-\ntention maps for each textual token. Specifically, at each\ntime step t, the spatial feature map of the noisy image zt\nis linearly projected into queries and reshaped as Qt \u2208\nRn\u00d7hw\u00d7d. Similarly, the text prompt p is first encoded into\ntextual embeddings via a text encoder, and then linearly pro-\njected into keys K \u2208 Rn\u00d7l\u00d7d and values V \u2208 Rn\u00d7l\u00d7d,\nrespectively. The attention maps are the product between\nqueries and keys:\nAt = Softmax\n\u0012QtKT\n\u221a\nd\n\u0013\n, At \u2208 Rn\u00d7hw\u00d7l\n(1)\nwhere d is the latent projection dimension, n is the number\nof attention heads, h and w are the height and width of the\nspatial feature map of the noisy image, and l is the number\nof text tokens in the text prompt. Intuitively, At[:, i, j] de-\nfines the probability assigned to the token j for the pixel i\nof the spatial feature map. Therefore, they can be used as a\npotential source to generate segmentation masks for specific\ntext tokens.\n3.2 Image Generation\nMosaic canvas. To generate multiple objects at specific lo-\ncations in a single image, we first need to define an im-\nage canvas to customize a region of interest for each ob-\nject. Formally, given a H \u00d7 W image canvas, we divide\nit into a set of regions R = {Ri = (Xi, Yi, Wi, Hi), i =\n1, ..., N} in a Mosaic style. Here, without loss of general-\nity, we take N = 4 as an example. Specifically, we first\nrandomly jitter the center of the image canvas within a cer-\ntain ratio range of W and H to obtain a Mosaic center\n(x, y), x \u2208 [\u03c3W, (1 \u2212 \u03c3)W], y \u2208 [\u03c3H, (1 \u2212 \u03c3)H], where\n\u03c3 \u2208 (0, 0.5] is a pre-defined ratio. We then use the Mo-\nsaic center (x, y) as the intersections of four candidate re-\ngions, i.e., R1 = (0, 0, x, y), R2 = (x, 0, W \u2212 x, y), R3 =\n(0, y, x, H \u2212 y), R4 = (x, y, W \u2212 x, H \u2212 y). To make dif-\nferent object regions more smoothly transitioned, we further\nallow a certain overlap for each neighborhood region. Let\n\u03b4x, \u03b4y denote the number of overlapped pixels in the hori-\nzontal and vertical direction of the canvas, respectively, we\ncan thus obtain the final region coordinates: R1 = (0, 0, x+\n\u03b4x/2, y + \u03b4y/2), R2 = (x \u2212 \u03b4x/2, 0, W \u2212 x + \u03b4x/2, y +\n\u03b4y/2), R3 = (0, y \u2212 \u03b4y/2, x + \u03b4x/2, H \u2212 y + \u03b4y/2), R4 =\n(x \u2212 \u03b4x/2, y \u2212 \u03b4y/2, W \u2212 x + \u03b4x/2, H \u2212 y + \u03b4y/2).\nPrompt template. Given a Mosaic image canvas R, we then\nconsider what kind of object, i.e., class of interest, to gen-\nerate for each region. This is achieved by defining a cus-\ntomized text prompt for each region. Specifically, we use a\ngeneric text prompt template like \u201ca photo of a single1 c,\ndc\u201d, where c is the category name, dc is the category defi-\nnition2. We randomly select N category names c associated\n1 We use the word \u201csingle\u201d to encourage the diffusion model to gen-\nerate a single object at each specific location.\n2 We empirically find that appending a category definition after the\ncategory name reduces the semantic ambiguity of generated images\ndue to the polysemy of some category names. For LVIS (Gupta et al,\n2019), the category definitions are readily available in annotations,\nwhere the meanings are mostly derived from WordNet (Miller, 1995).\nSee Table 2d for ablations on the effect of different prompt templates.\nMosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation\n5\nModel\nreshape\nreshape\nreshape\nreshape\nthreshold\nthreshold\nthreshold\nthreshold\nrefine\nrefine\nrefine\nrefine\nfilter &\nexpand\nfilter &\nexpand\nfilter &\nexpand\nfilter &\nexpand\nextract\nextract\nextract\nextract\nDiffusion\neasel\nseaplane\nparrot\ntree\ntext prompts\na photo of a single\neasel\nseaplane\nparrot\nChristmas tree\n...\nMosaic canvas\nmap\nsynthetic image\ncross-attention maps\ninstance masks\n...\naggregate\nimage generation\nmask generation\nFig. 2: Overview of our MosaicFusion pipeline. The left part shows the image generation process, while the right part\nshows the mask generation process. Given a user-defined Mosaic image canvas and a set of text prompts, we first map the\nimage canvas from the pixel space into the latent space. We then run the diffusion process on each latent region parallelly with\nthe shared noise prediction model, starting from the same initialization noise while conditioning on different text prompts,\nto generate the synthetic image with multiple objects specified in each region. Simultaneously, we aggregate the region-wise\ncross-attention maps for each subject token by upscaling them to the original region size in the pixel space and averaging\nthem across all attention heads, layers, and time steps. After that, we binarize the aggregated attention maps, refine the\nboundaries, filter out the low-quality masks, and expand them to the size of the whole image canvas to obtain the final\ninstance masks\nwith dc from a pre-defined set of interest categories to ob-\ntain N text prompts {pi, ..., pN}. We then assign prompt pi\nto region Ri for the subsequent diffusion process, described\nnext.\nDiffusion process. Since latent diffusion models like SD\nrun the diffusion process in the latent space, we need to\nadapt the image canvas from the pixel space into the la-\ntent space. Due to the fully convolutional nature of U-Net\n(and VAE), the coordinates in the latent space can be sim-\nply mapped to the pixel space by multiplying an upscaling\nfactor U, whose value is equal to the upscaling factor of U-\nNet in SD, i.e., U = 8. Thus, given the region coordinates\nRi = (Xi, Yi, Wi, Hi) in the pixel space, we can obtain the\ncorresponding region coordinates ri = (xi, yi, wi, hi) =\n(Xi/U, Yi/U, Wi/U, Hi/U) in the latent space. To avoid\nfractional indices, we ensure (Xi, Yi, Wi, Hi) in the pixel\nspace are divisible by U. After mapping the image canvas\ninto the latent canvas, we then run N diffusion processes\non different latent regions ri simultaneously with a shared\nnoise-prediction network, starting from the same initializa-\ntion noise while conditioning on different text prompts pi.\nSince all diffusion processes share the same network, the\nmemory cost is essentially the same as that of the largest re-\ngion affected by a single diffusion process. As a result, we\ncan obtain a generated image with multiple objects specified\nin each region.\n3.3 Mask Generation\nAttention aggregation. As introduced in Sect. 3.1, the\ncross-attention maps play an important role in passing the\ninformation from text tokens to spatial image features. They\nindicate the influence of each token on each pixel and po-\ntentially serve as a good mask source. Formally, given the\nnoised region latent zt\ni and the corresponding text prompt\npi, i = 1, ..., N, we can obtain the cross-attention maps\nAt\ni through the U-Net network during the diffusion process\nformulated in Sect. 3.2. Since U-Net contains a series of\ndownsampling and upsampling blocks, we can obtain cross-\nattention maps with varying scales from different layers. To\nmake full use of the spatial and temporal information, we\nfirst bicubically resize all attention maps to the original re-\ngion size (Hi, Wi) in the pixel space, and then average them\nacross all attention heads, layers and time steps3, produc-\n3 We show more details in the experiment section (see Table 2g,\nTable 2h, and Fig. 3) that averaging cross-attention maps across all\nlayers and time steps is necessary to achieve the best performance.\n6\nJiahao Xie et al.\ning the final aggregated attention maps \u02c6Ai \u2208 RHi\u00d7Wi\u00d7l.\nWe extract the specific attention map \u02c6As\ni \u2208 RHi\u00d7Wi along\nthe last channel dimension of \u02c6Ai for the subject token s\nthat contains the interest category name c. We then nor-\nmalize \u02c6As\ni within [0, 1] and threshold it to a binary re-\ngion mask \u02c6\nMi \u2208 {0, 1}Hi\u00d7Wi. In practice, we use Otsu\u2019s\nmethod (Otsu, 1979) to automatically determine the binary\nthreshold.\nEdge refinement. The binarized attention maps provide rel-\natively coarse masks of objects, as shown in Fig. 2. To\nfurther refine the mask boundaries, we adopt the standard\nedge refinement post-processing techniques such as Bilat-\neral Solver (BS) (Barron and Poole, 2016) on top of the\nobtained coarse masks \u02c6\nMi to generate fine-grained masks\n\u02dc\nMi \u2208 {0, 1}Hi\u00d7Wi.\nMask filtering. To further remove the low-quality masks,\nwe filter the refined region masks \u02dc\nMi based on some pre-\ndefined criteria. Specifically, we first apply the connected\ncomponent analysis (Di Stefano and Bulgarelli, 1999) on the\nrefined masks to group the pixels into several disconnected\nregions. We then filter out masks with areas less than 5% or\nover 95% of the whole region since these masks are highly\nlikely segmented incorrectly. After that, we only keep region\nmasks that have one connected component per mask since\nwe want to generate one object at each specific location. To\nobtain the final instance mask Mi \u2208 {0, 1}H\u00d7W , we expand\nthe region mask \u02dc\nMi \u2208 {0, 1}Hi\u00d7Wi to the size of the whole\nimage canvas by padding 0 values for blank regions.\n4 Experiments\n4.1 Implementation Details\n4.1.1 Datasets\nWe conduct our experiments of object detection and instance\nsegmentation on the challenging LVIS v1.0 dataset (Gupta\net al, 2019). LVIS is a large vocabulary instance segmenta-\ntion dataset that contains 1203 categories with a long-tailed\ndistribution of instances in each category. It has 100k im-\nages in the training set and 19.8k images in the validation\nset. Based on how many images each category appears in\nthe training set, the categories are divided into three groups:\nrare (1-10 images), common (11-100 images), and frequent\n(>100 images). The number of categories in each group is:\nrare (337), common (461), and frequent (405). In the open-\nvocabulary detection setting, the frequent and common cate-\ngories are treated as base categories for training and the rare\ncategories serve as novel categories for testing. The annota-\ntions of rare categories are not used during training.\n4.1.2 Evaluation Metrics\nWe use the standard average precision (AP) as the eval-\nuation metric, which is averaged at different IoU thresh-\nolds (from 0.5 to 0.95) across categories. We report the\nbounding-box AP and mask AP on all categories (denoted\nas APbox, APmask) as well as on the rare categories (denoted\nas APbox\nr\n, APmask\nr\n). We report the average of five independent\nruns following the best practice of LVIS challenge (Gupta\net al, 2019).\n4.1.3 MosaicFusion\nWe adopt the open-sourced Stable Diffusion v1.4 model\nwith LMS (Karras et al, 2022) scheduler. We use a guid-\nance scale factor of 7.5 and run 50 inference steps per im-\nage for all experiments. We keep the average region size as\n384\u00d75124 to generate each object. Thus, the final image and\nmask size depends on the number of objects we want to gen-\nerate per image, e.g., 384 \u00d7 512 for one object, 384 \u00d7 1024\n(or 768 \u00d7 512) for two objects, and 768 \u00d7 1024 for four\nobjects.\n4.1.4 Baseline Settings\nWe consider two challenging settings to verify the effective-\nness of our approach, i.e., long-tailed instance segmentation\nand open-vocabulary object detection. Since MosaicFusion\nis agnostic to the underlying detector, we consider two popu-\nlar baselines in long-tailed instance segmentation: Mask R-\nCNN (He et al, 2017) and CenterNet2 (Zhou et al, 2021),\nand use state-of-the-art F-VLM (Kuo et al, 2023) in open-\nvocabulary object detection. Our implementation is based\non the MMDetection (Chen et al, 2019) toolbox. We detail\neach baseline below.\nMask R-CNN baseline. We follow the same setup in Gupta\net al (2019). Specifically, we adopt ResNet-50 (He et al,\n2016) with FPN (Lin et al, 2017) backbone using the stan-\ndard 1\u00d7 training schedule (Chen et al, 2019; Wu et al, 2019)\n(90k iterations with a batch size of 16)5. We use SGD op-\ntimizer with a momentum of 0.9 and a weight decay of\n0.0001. The initial learning rate is 0.02, dropped by 10\u00d7\nat 60k and 80k iterations. Data augmentation includes hori-\nzontal flip and random resize short side [640, 800], long side\n< 1333. We use repeat factor sampling with an oversample\nthreshold of 10\u22123.\n4 The image resolution used during training in Stable Diffusion is\n512\u00d7512. We notice that the generated results will get worse if one de-\nviates from this training resolution too much. Thus, we simply choose\nthe aspect ratio of the average LVIS image and keep the longer dimen-\nsion to 512.\n5 We are aware that different works may use different notations for\na 1\u00d7 training schedule. In this work, we always refer 1\u00d7 schedule to\na total of 16 \u00d7 90k images.\nMosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation\n7\nTable 2: Ablations for MosaicFusion on LVIS long-tailed instance segmentation. We use Mask R-CNN R50-FPN (1\u00d7\nschedule). The baseline is trained on the original training set, while others are trained on the training+synthetic set. We\nreport mask AP on the validation set. Unless otherwise specified, the default settings are: (a) the number of generated objects\nper image is 4, (b) the center jitter ratio is 0.375, (c) the number of overlapped pixels is 64 in the horizontal direction and 48\nin the vertical direction, respectively, (d) the text prompt template is \u201ca photo of a single c, dc\u201d, (e) the generated category\ntypes are all categories (i.e., rare, common, and frequent), (f) the number of generated images per category is 25, (g-h) the\ncross-attention maps are aggregated across all attention layers and time steps, and (i) the diffusion model is Stable Diffusion\nv1.4. The default entry is marked in gray\n(a) Number of objects. Increasing the num-\nber of objects per image performs better\nN\nAPr\nAP\n-\n9.6\n21.7\n1\n14.0\n22.4\n2\n13.3\n22.5\n4\n15.2\n23.1\nrandom\n14.5\n22.9\n(b) Center jitter ratio. Moderately jittering\nthe canvas center works the best\n\u03c3\nAPr\nAP\n-\n9.6\n21.7\n0.25\n13.8\n22.9\n0.375\n15.2\n23.1\n0.5\n14.2\n23.0\n(c) Overlapped pixels. Allowing a certain\nregion overlap in the canvas is effective\n(\u03b4x, \u03b4y)\nAPr\nAP\n-\n9.6\n21.7\n(0, 0)\n14.2\n23.0\n(64, 48)\n15.2\n23.1\n(128, 96)\n13.2\n22.9\n(d) Text prompt. Appending a category\ndefinition is more accurate\nPrompt template\nAPr\nAP\n-\n9.6\n21.7\nc\n13.5\n22.8\na photo of a single c\n14.0\n23.0\na photo of a single c, dc\n15.2\n23.1\n(e) Category type. Generating all cate-\ngories leads to more gains\nCategory set\nAPr\nAP\n-\n9.6\n21.7\n{r}\n14.2\n22.4\n{r, c, f}\n15.2\n23.1\n(f) Number of images. Generating 25 im-\nages per category is enough\n# Images\nAPr\nAP\n-\n9.6\n21.7\n10\n13.1\n22.5\n25\n15.2\n23.1\n50\n14.7\n23.0\n(g) Attention layer. All layers contribute\npositively to the results\nLayer resolution\nAPr\nAP\n-\n9.6\n21.7\n\u2264 \u00d71/32\n12.6\n22.7\n\u2264 \u00d71/16\n14.3\n22.9\n\u2264 \u00d71/8\n15.2\n23.1\n(h) Time step. All time steps are necessary\nfor the best performance\nTime step\nAPr\nAP\n-\n9.6\n21.7\n\u2264 10\n14.3\n22.8\n\u2264 25\n14.6\n22.9\n\u2264 50\n15.2\n23.1\n(i) Diffusion model. MosaicFusion benefits\nfrom more advanced diffusion models\nStable Diffusion\nAPr\nAP\n-\n9.6\n21.7\nv1.2\n12.7\n22.6\nv1.3\n14.4\n22.9\nv1.4\n15.2\n23.1\nv1.5\n15.5\n23.2\nCenterNet2 baseline. We follow the same setup in Zhou\net al (2022b). Specifically, we use Swin-B (Liu et al, 2021)\nwith FPN backbone and a longer 4\u00d7 training schedule (180k\niterations with a batch size of 32, a.k.a., Box-Supervised).\nWe use AdamW (Loshchilov and Hutter, 2019) optimizer.\nThe initial learning rate is 0.0001, decayed with a cosine\ndecay schedule (Loshchilov and Hutter, 2017). Data aug-\nmentation includes the EfficientDet (Tan et al, 2020c) style\nlarge-scale jittering (Ghiasi et al, 2021) with a training im-\nage size of 896 \u00d7 896. We use repeat factor sampling with\nan oversample threshold of 10\u22123 without bells and whistles.\nCompared with Mask R-CNN, Box-Supervised CenterNet2\nis a stronger baseline that can better verify whether Mosaic-\nFusion is scaleable with larger backbones and longer train-\ning schedules.\nF-VLM baseline. We follow the same setup in Kuo\net al (2023). Specifically, we use the pre-trained ResNet-\n50/ResNet-50x64 CLIP (Radford et al, 2021b) model as the\nfrozen backbone and only train the Mask R-CNN (He et al,\n2017) with FPN detector head for 46.1k iterations with a\nbatch size of 256. We use SGD optimizer with a momen-\ntum of 0.9 and a weight decay of 0.0001. The initial learn-\ning rate is 0.36, dropped by 10\u00d7 at intervals [0.8, 0.9, 0.95].\nData augmentation includes large-scale jittering with a train-\ning image size of 1024 \u00d7 1024. The base/novel VLM score\nweight is 0.35/0.65, respectively. We set the background\nweight as 0.9, and the VLM temperature as 0.01. We use\nCLIP prompt templates and take the average text embed-\ndings of each category. The state-of-the-art performance\nof F-VLM in open-vocabulary object detection makes it a\n8\nJiahao Xie et al.\nTable 3: Comparison with Mosaic (Bochkovskiy et al,\n2020) data augmentation. We use Mask R-CNN R50-FPN\n(1\u00d7 schedule), and report mask AP. Synthesizing multi-\nobject images is more effective than simply combining\nsingle-object images\nMethod\nAPr\nAP\nMask R-CNN (He et al, 2017)\n9.6\n21.7\nw/ MosaicFusion (N = 1)\n14.0\n22.4\nw/ MosaicFusion (N = 1) + Mosaic aug.\n14.0\n22.4\nw/ MosaicFusion (N = 4)\n15.2\n23.1\nstrong baseline to verify the effectiveness of our MosaicFu-\nsion in the open-vocabulary setting.\n4.2 Main Properties\nWe start by ablating our MosaicFusion using Mask R-CNN\nR50-FPN as the default architecture on the standard LVIS\nlong-tailed instance segmentation benchmark. Several in-\ntriguing properties are observed.\nSingle object vs. multiple objects. We first study the effect\nof the number of generated objects per image. As shown in\nTable 2a, simply generating one (N = 1) object per im-\nage has already outperformed the baseline by a significant\nmargin (+4.4% APr, +0.7% AP). This indicates that syn-\nthetic images generated from diffusion models are indeed\nuseful in improving the performance of instance segmen-\ntation tasks, especially for rare categories. Increasing the\nnumber of objects per image tends to further improve the\nperformance, with the best performance achieved by setting\nN = 4 (+5.6% APr, +1.4% AP) as more objects in a sin-\ngle image provide more instances and increase the task dif-\nficulty. It is worth noting that a random variant, i.e., ran-\ndomly selecting the number of objects per image among\nN = 1, 2, 4 rather than setting a fixed number achieves\nthe sub-optimal performance compared with N = 4. This\ndemonstrates that the number of objects matters more than\nthe diversity of the spatial layout of the image canvas. See\nFig. 4 for the visualization of some corresponding example\nimages with the different number of generated objects.\nMosaic canvas design. We then study the different design\nchoices of the Mosaic image canvas in Table 2b and Ta-\nble 2c.\nTable 2b ablates the effect of the jittering ratio of the\nMosaic center. A larger ratio creates sub-regions with more\ndiverse shapes. MosaicFusion works the best with a moder-\nate jittering ratio (i.e., \u03c3 = 0.375). Jittering the center with\na larger ratio tends to distort the image generation quality,\nespecially for those of smaller regions.\nTable 2c further ablates the effect of the number of\noverlapped pixels for each neighborhood region. Generating\nTable 4: Comparison with Copy-Paste (Ghiasi et al,\n2021) data augmentation. We use Mask R-CNN R50-FPN\n(1\u00d7 schedule), and report mask AP. MosaicFusion is orthog-\nonal to existing data augmentation methods like Copy-Paste\nMethod\nAPr\nAP\nMask R-CNN (He et al, 2017)\n9.6\n21.7\nw/ Copy-Paste\n10.6\n22.1\nw/ MosaicFusion\n15.2\n23.1\nw/ MosaicFusion + Copy-Paste\n15.5\n23.3\neach object with a moderate region overlap (i.e., \u03b4x = 64,\n\u03b4y = 48) performs better than strictly separating each region\nfor each object (i.e., \u03b4x = 0, \u03b4y = 0) since a certain overlap\ncan allow a more smooth transition between different object\nregions, having a harmonization effect on the whole image.\nHowever, creating sub-regions with a larger overlap tends to\ndegrade the performance as different objects will be highly\nentangled with each other.\nText prompt design. We compare different text prompt\ntemplates in Table 2d. Simply using the category name c\nas the text prompt has already surpassed the baseline by a\nclear margin (+3.9% APr, +1.1% AP). Decorating c with\nthe prefix \u201ca photo of a single\u201d slightly improves the per-\nformance upon c (+0.5% APr, +0.2% AP). Appending the\ncategory definition dc after c leads to further performance\ngains (+1.2% APr, +0.1% AP). We believe that there exist\nbetter text prompts and leave more in-depth study on prompt\ndesign for future work.\nGenerated category types. Table 2e studies the types of\ninterest categories to generate. The results demonstrate that\ngenerating images with objects from all categories (i.e., rare,\ncommon, and frequent, denoted as {r, c, f}) leads to better\nperformance than generating those with objects from rare\ncategories only (denoted as {r}). The improvements are con-\nsistent regardless of the metrics on rare categories or all cat-\negories (+1.0% APr, +0.7% AP).\nGenerated image numbers. Table 2f studies the effect\nof the number of generated images per interest category6.\nOnly generating 10 images per category has already im-\nproved upon the baseline by a significant margin (+3.5%\nAPr, +0.8% AP). A better performance is achieved by set-\nting the number of generated images per category as 25\n(+5.6% APr, +1.4% AP). Generating more images does not\nlead to further performance gains. We hypothesize that the\ndiversity of generated images from existing diffusion mod-\nels is a main challenge to scale up. We expect further perfor-\n6 Note that the final number of synthetic images per category is not\nfixed as we need to filter some images during the mask filtering process\nintroduced in Sect. 3.3. We observe that \u223c50% generated images and\nmasks will be discarded accordingly.\nMosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation\n9\nTable 5: LVIS long-tailed instance segmentation benchmark. We report box AP and mask AP on the validation set. We\nbuild our method upon two representative baselines: (i) Mask R-CNN baseline (1\u00d7 schedule) in Gupta et al (2019) using\nthe ResNet-50 w/ FPN backbone, and (ii) Box-Supervised CenterNet2 baseline (4\u00d7 schedule) in Zhou et al (2022b) using\nthe Swin-B backbone. MosaicFusion improves over the baselines regardless of the architectures with increased gains for the\nrare categories\nMethod\nBackbone\nAPbox\nr\nAPmask\nr\nAPbox\nAPmask\nMask R-CNN (He et al, 2017)\nResNet-50\n9.1\n9.6\n22.5\n21.7\nw/ MosaicFusion\nResNet-50\n14.8\n15.2\n24.0\n23.1\nvs. baseline\n+5.7\n+5.6\n+1.5\n+1.4\nMosaicOS (Zhang et al, 2021a)\nResNeXt-101\n-\n21.7\n-\n28.3\nCenterNet2 (Zhou et al, 2021)\nResNeXt-101\n-\n24.6\n-\n34.9\nAsyncSLL (Han et al, 2020)\nResNeSt-269\n-\n27.8\n-\n36.0\nSeesawLoss (Wang et al, 2021a)\nResNeSt-200\n-\n26.4\n-\n37.3\nCopy-paste (Ghiasi et al, 2021)\nEfficientNet-B7\n-\n32.1\n41.6\n38.1\nTan et al. (Tan et al, 2020b)\nResNeSt-269\n-\n28.5\n-\n38.8\nBox-Supervised (Zhou et al, 2022b)\nSwin-B\n39.9\n35.9\n45.4\n40.7\nw/ MosaicFusion\nSwin-B\n43.3\n39.0\n46.1\n41.2\nvs. baseline\n+3.4\n+3.1\n+0.7\n+0.5\nmance improvement to be achieved with the emergence of\nmore expressive diffusion models in the future.\nAttention layers and time steps. We study whether aggre-\ngating cross-attention maps across all attention layers and\ntime steps is necessary. Table 2g and Table 2h ablate the in-\nfluence of layer resolutions and time steps, respectively. The\nresults indicate that all layers and time steps contribute pos-\nitively to the performance, proving that we should make full\nuse of the spatial and temporal information during the dif-\nfusion process to produce the highest-quality attention maps\nfor instance segmentation. See Fig. 3 for the visualization of\nsome example cross-attention maps across different layers\nand time steps.\nDiffusion models. In Table 2i, we study whether our Mo-\nsaicFusion benefits from the improvement of diffusion mod-\nels. We use Stable Diffusion v1.x (e.g., v1.2, v1.3, v1.4, and\nv1.5) as examples for ablations. A better diffusion model\ngenerally leads to better instance segmentation performance.\nThis makes sense as a more advanced diffusion model gen-\nerates higher-quality images.\nComparison with other data augmentation methods. We\ncompare our MosaicFusion with other existing data aug-\nmentation methods for instance segmentation.\nWe first compare MosaicFusion with Mosaic data aug-\nmentation proposed in the popular object detection frame-\nwork YOLO-v4 (Bochkovskiy et al, 2020). It combines four\ntraining images into one with certain ratios after each image\nhas been independently augmented. To ensure a fair com-\nparison, we simply replace the multi-object images gener-\nated by MosaicFusion with the four combined generated\nsingle-object images using the Mosaic data augmentation\nwhile keeping other hyper-parameters unchanged. As shown\nin Table 3, simply adopting Mosaic augmentation to pro-\nduce multi-object images does not bring further gains over\nthe single-object version of our MosaicFusion (N = 1). In\ncontrast, our multi-object version of MosaicFusion (N = 4)\nleads to further performance improvement compared with\nMosaicFusion (N = 1). This proves that using diffusion\nmodels to synthesize multi-object images is more effective\nthan simply combining single-object images with the exist-\ning data augmentation counterpart.\nWe then show that our method is orthogonal to exist-\ning data augmentation methods. Here, we use the popular\nCopy-Paste (Ghiasi et al, 2021) as an example, which pro-\nduces more instances by randomly copying the existing ob-\njects from one image and pasting them into another. The re-\nsults are shown in Table 4. Either Copy-Paste or MosaicFu-\nsion improves over the baseline, while MosaicFusion yields\nmore performance gains. This makes sense as Copy-Paste\nonly copies and pastes existing instances in the training data,\nwhereas MosaicFusion creates new instances with substan-\ntially more diversity than existing ones. The diversity of in-\nstances matters more for rare categories. A slightly higher\nperformance can be further achieved by combining them\ntogether, indicating that MosaicFusion is compatible with\nother data augmentation methods.\n4.3 Comparison with Previous Methods\nWe perform system-level comparisons with previous meth-\nods on the long-tailed instance segmentation benchmark as\nwell as the open-vocabulary object detection benchmark on\nLVIS.\nLong-tailed instance segmentation benchmark. Table 5\npresents our results on the long-tailed instance segmen-\n10\nJiahao Xie et al.\nTable 6: LVIS open-vocabulary instance segmentation benchmark. We report mask AP on the validation set. We build\nour method upon state-of-the-art F-VLM (Kuo et al, 2023). Unless otherwise specified, all methods use the CLIP (Radford\net al, 2021b) pre-training and fixed prompt templates. \u2217: our reproduced results. \u22c6: prompt optimization (Zhou et al, 2022a)\nand SoCo pre-training (Wei et al, 2021). \u2020: joint training with ImageNet-21K (Deng et al, 2009). \u2021: training with CC-\n3M (Sharma et al, 2018)\nMethod\nBackbone\nPre-trained CLIP\nAPnovel\nAP\nViLD (Gu et al, 2022)\nResNet-50\nViT-B/32\n16.1\n22.5\nViLD-Ens. (Gu et al, 2022)\nResNet-50\nViT-B/32\n16.6\n25.5\nDetPro\u22c6 (Du et al, 2022)\nResNet-50\nViT-B/32\n19.8\n25.9\nDetic-ViLD\u2020 (Zhou et al, 2022b)\nResNet-50\nViT-B/32\n17.8\n26.8\nRegionCLIP\u2021 (Zhong et al, 2022)\nResNet-50\nResNet-50\n17.1\n28.2\nF-VLM (Kuo et al, 2023)\nResNet-50\nResNet-50\n18.6\n24.2\nF-VLM\u2217\nResNet-50\nResNet-50\n18.0\n23.6\nw/ MosaicFusion\nResNet-50\nResNet-50\n20.6\n24.0\nvs. baseline\n+2.6\n+0.4\nViLD (Gu et al, 2022)\nResNet-152\nViT-B/32\n18.7\n23.6\nViLD-Ens. (Gu et al, 2022)\nResNet-152\nViT-B/32\n18.7\n26.0\nViLD-Ens. (Gu et al, 2022)\nEfficientNet-B7\nViT-L/14\n21.7\n29.6\nViLD-Ens. (Gu et al, 2022)\nEfficientNet-B7\nEfficientNet-B7\n26.3\n29.3\nDetPro-Cascade\u22c6 (Du et al, 2022)\nResNet-50\nViT-B/32\n20.0\n27.0\nDetic-CN2\u2020 (Zhou et al, 2022b)\nResNet-50\nViT-B/32\n24.6\n32.4\nMEDet\u2021 (Chen et al, 2022)\nResNet-50\nViT-B/32\n22.4\n34.4\nCentric-OVD\u2020 (Rasheed et al, 2022)\nResNet-50\nViT-B/32\n25.2\n32.9\nRegionCLIP\u2021 (Zhong et al, 2022)\nResNet-50x4\nResNet-50x4\n22.0\n32.3\nOWL-ViT (Minderer et al, 2022)\nViT-L/14\nViT-L/14\n25.6\n34.7\nF-VLM (Kuo et al, 2023)\nResNet-50x64\nResNet-50x64\n32.8\n34.9\nF-VLM\u2217\nResNet-50x64\nResNet-50x64\n31.7\n34.9\nw/ MosaicFusion\nResNet-50x64\nResNet-50x64\n34.9\n35.3\nvs. baseline\n+3.2\n+0.4\ntation benchmark. MosaicFusion yields significant perfor-\nmance gains over the commonly-used Mask R-CNN base-\nline, with the most considerable improvement coming from\nAPr (+5.7% APbox\nr\n, +5.6% APmask\nr\n). We observe consistent\nperformance improvements when building upon a stronger\nBox-Supervised CenterNet2 baseline, obtaining over 3%\nhigher APr (+3.4% APbox\nr\n, +3.1% APmask\nr\n). This demon-\nstrates that MosaicFusion is agnostic to detector architec-\ntures and is an effective plug-and-play data augmentation\nmethod for existing instance segmentation models.\nOpen-vocabulary object detection benchmark. We fur-\nther demonstrate that MosaicFusion is also a promising data\naugmenter in the challenging open-vocabulary object detec-\ntion benchmark. We build our method upon the state-of-\nthe-art open-vocabulary detector F-VLM (Kuo et al, 2023),\nwhich uses the pre-trained CLIP model as the frozen back-\nbone and Mask R-CNN with FPN as the detector head. The\nresults are shown in Table 6. MosaicFusion still consistently\noutperforms F-VLM across different backbones especially\nfor APnovel (+2.6% APnovel for ResNet-50, +3.2% APnovel for\nResNet-50x64), even though the baseline has incorporated\nthe open-vocabulary knowledge from the CLIP model. This\nindicates that text-driven diffusion models and CLIP mod-\nels are complementary to each other. We can achieve better\nperformance by leveraging the benefits from both worlds.\n4.4 Further Discussion\nWe further discuss how to evaluate the synthesized mask\nquality directly and compare MosaicFusion with other\ndiffusion-based data augmentation methods quantitatively.\nMask quality evaluation. In Sect. 4.2 and Sect. 4.3, we use\nthe standard evaluation metrics (i.e., box AP and mask AP)\nin instance segmentation tasks for evaluation since our goal\nis to improve the instance segmentation performance, which\nis a common evaluation protocol used in previous works.\nExplicitly evaluating synthetic images and masks is chal-\nlenging as no ground truth is available. One na\u00a8\u0131ve way is to\nmanually label all synthetic images with corresponding in-\nstance masks. However, it is extremely time-consuming and\nexpensive. Here, we provide a more cost-effective metric to\ndirectly evaluate the synthesized mask quality. Specifically,\nwe use SAM (Kirillov et al, 2023) as a data annotator due\nto its strong zero-shot generalization ability. We first prompt\nSAM with MosaicFusion-generated bounding boxes to pro-\nduce instance masks. We then use these SAM-generated\nMosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation\n11\nTable 7: Comparison with X-Paste (Zhao et al, 2023) on\nLVIS. We use CenterNet2 with ResNet-50 (4\u00d7 schedule)\nas the baseline, and report box AP and mask AP. We re-\nimplement both the baseline and X-Paste using the official\ncode of X-Paste8. All entries use the same network and train-\ning settings for a fair comparison. Note that X-Paste uses\n100k generated images while MosaicFusion only uses 4k\ngenerated images here\nMethod\nAPbox\nr\nAPmask\nr\nAPbox\nAPmask\nbaseline\n21.0\n19.0\n33.9\n30.2\nw/ X-Paste\n25.5\n23.0\n34.6\n30.7\nw/ MosaicFusion\n25.8\n23.4\n34.7\n30.9\nmasks as ground truth and compute mean Intersection-over-\nUnion (mIoU) between MosaicFusion-generated masks and\nground truth masks. For reference, the mIoU result of Mo-\nsaicFusion using the default setting in Table 2 is 77.2%. We\nhope our proposed metric can inspire future works on di-\nrectly evaluating synthetic images and masks for rare and\nnovel categories.\nComparison with other diffusion-based data augmenta-\ntion methods. Several concurrent works (Ge et al, 2022;\nZhao et al, 2023; Li et al, 2023) also use diffusion models for\ninstance segmentation augmentation. We have discussed the\ndifferences with these works in Sect. 2 and Table 1. Here,\nwe additionally provide a quantitative comparison with X-\nPaste (Zhao et al, 2023)7. To ensure a fair comparison, we\nuse the same baseline as in Zhao et al (2023), i.e., Center-\nNet2 (Zhou et al, 2021) with ResNet-50 backbone for a 4\u00d7\ntraining schedule (90k iterations with a batch size of 64).\nConsidering that X-Paste uses additional retrieved images\nby the CLIP model to further boost its performance, we re-\nmove these retrieved images and only keep the generated\nones. Table 7 reports the results on LVIS. Although X-Paste\nuses multiple extra foreground segmentation models to la-\nbel the generated data further, our MosaicFusion can still\nachieve competitive performance with X-Paste. It should\nbe well noted that X-Paste here uses 100k generated im-\nages while our MosaicFusion only uses 4k generated ones\nto achieve such results. This demonstrates the superiority of\ndirectly synthesizing multi-object training images over past-\ning multiple synthesized instances onto the background to\ncompose training images.\n7 We choose X-Paste for comparison due to its open-sourced imple-\nmentation. Note that Li et al (2023) uses a different setting by training\nand testing models on its own synthetic datasets. Thus, an apple-to-\napple quantitative comparison with Li et al (2023) is infeasible.\n8 We find that the baseline and X-Paste cannot be fully reproduced\nusing the official code (see issues here). Therefore, our reproduced re-\nsults are relatively lower than the original reported performance. Nev-\nertheless, all experiments are done under the same settings for a fair\ncomparison.\n4.5 Qualitative Results\nApart from quantitative results, we finally provide qualita-\ntive results of the cross-attention maps during the diffusion\nprocess as well as the synthesized images and masks by our\nMosaicFusion.\nCross-attention maps. As illustrated in Sect. 3, cross-\nattention maps in the diffusion process play a key role in pro-\nducing our instance segmentation masks. We have studied\nthe effects of attention layers and time steps on the final in-\nstance segmentation performance in Sect. 4.2 (see Table 2g\nand Table 2h). In Fig. 3, we visualize some example cross-\nattention maps with respect to each interest word across dif-\nferent time steps and attention layers in the diffusion pro-\ncess, using the same synthetic image example as in Fig. 2.\nWe can observe that the structure of the object is deter-\nmined in the early steps of the diffusion process while more\nfine-grained object details emerge in the later steps. A simi-\nlar phenomenon is observed across different layers. Specif-\nically, low-resolution attention layers produce a coarse ob-\nject shape while the high-resolution counterparts produce a\nfine-grained object edge or silhouette. In conclusion, each\ncross-attention map reflects the object composition to some\nextent. To make full use of the spatial and temporal infor-\nmation, we aggregate the cross-attention maps by averaging\nthem across different time steps and layers, which produces\nthe attention maps with the highest quality as visualized in\nFig. 3. These aggregated attention maps can then serve as a\ngood mask source to generate our final instance masks.\nSynthesized images and masks. Our MosaicFusion can\ngenerate multiple objects with corresponding masks in a sin-\ngle image. We have studied the effect of the number of gen-\nerated objects per image in Sect. 4.2 (see Table 2a). Here,\nwe visualize some example results of our synthesized in-\nstance segmentation dataset by MosaicFusion. In Fig. 4, we\nshow examples of generating N = 1, 2, 4 objects per image,\nrespectively. Given only interest category names, MosaicFu-\nsion can directly generate high-quality multi-object images\nand masks by conditioning on a specific text prompt for each\nregion simultaneously without any further training and ex-\ntra detectors or segmentors. The synthesized instance seg-\nmentation dataset can be used to train various downstream\ndetection and segmentation models to improve their perfor-\nmances, especially for rare and novel categories, as verified\nin Sect. 4.3.\n5 Conclusion\nInstance segmentation is a fundamental task in computer vi-\nsion. In this work, we have introduced a diffusion-based data\naugmentation method, namely MosaicFusion, for large vo-\ncabulary instance segmentation. Our method is training-free,\nequipped with multi-object image and mask generation cap-\n12\nJiahao Xie et al.\nt = 50\nt = 1\naverage\naverage\n\u00d7 1 8\n\u2044\n\u00d7 1 16\n\u2044\n\u00d7 1 32\n\u2044\n\u201ceasel\u201d\naverage\n\u00d7 1 8\n\u2044\n\u00d7 1 16\n\u2044\n\u00d7 1 32\n\u2044\n\u201cseaplane\u201d\naverage\n\u00d7 1 8\n\u2044\n\u00d7 1 16\n\u2044\n\u00d7 1 32\n\u2044\naverage\n\u00d7 1 8\n\u2044\n\u00d7 1 16\n\u2044\n\u00d7 1 32\n\u2044\n\u201cparrot\u201d\n\u201ctree\u201d\na photo of a single \neasel \nseaplane \nparrot \nChristmas tree \nsynthetic image\ntext prompts\ncross-attention maps\nFig. 3: Visualization of cross-attention maps with respect to each interest subject word across different time steps and\nlayers in the diffusion process. The time steps range from the first step t = 50 to the last step t = 1 in equal intervals\n(from left to right), while the layer resolutions range from \u00d71/32 to \u00d71/8 of the original image size (from top to bottom). In\neach entry, the last column shows the averaged cross-attention maps across different time steps, while the last row shows the\naveraged cross-attention maps across different layers. The highest-quality attention maps are produced by averaging them\nacross both time steps and layers (bottom right framed in red)\nMosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation\n13\n\ud835\udc41 = 4\n\ud835\udc41 = 2\n\ud835\udc41 = 1\nFig. 4: Visualization of our synthesized instance segmentation dataset by MosaicFusion. We show examples of gener-\nating N = 1, 2, 4 objects per image, using the settings in Table 2a\n14\nJiahao Xie et al.\nability without relying on extra models, and compatible with\nvarious downstream detection architectures. We hope our\nexplorations can pave the way to unleash the potential of\ngenerative models for discriminative tasks.\nLimitations. Our study has several limitations: 1) We\nchoose a representative text-to-image diffusion model, i.e.,\nStable Diffusion, due to its open-sourced implementation.\nMore text-to-image diffusion models can be studied in fu-\nture research. 2) We consider some representative object de-\ntection and instance segmentation models to examine the ef-\nfectiveness of our method for large vocabulary instance seg-\nmentation due to resource constraints. Essentially, Mosaic-\nFusion is orthogonal to downstream detection and segmen-\ntation models, and it can be built upon all baseline methods\nin Table 5 and Table 6 to improve their performances. More\nbaselines can be explored for further studies. 3) Despite the\nintriguing properties of MosaicFusion, the synthetic images\nstill have an unavoidable domain gap with the real images\ndue to the limited expressiveness of off-the-shelf diffusion\nmodels. As the first work to generate multiple objects and\nmasks in a single image, we leave explorations on generat-\ning more complex scene-level images with diffusion models\nfor future work.\nData availability statements. All data supporting the find-\nings of this study are available online. The LVIS dataset can\nbe downloaded from https://www.lvisdataset.\norg/dataset. The Stable Diffusion models used to\ngenerate data are available at https://github.com/\nCompVis/stable-diffusion.\nAcknowledgements This study is supported under the RIE2020 In-\ndustry Alignment Fund \u2013 Industry Collaboration Projects (IAF-ICP)\nFunding Initiative, as well as cash and in-kind contribution from\nthe industry partner(s). The project is also supported by NTU NAP\nand Singapore MOE AcRF Tier 2 (MOE-T2EP20120-0001, MOE-\nT2EP20221-0012).\nReferences\nBaranchuk D, Rubachev I, Voynov A, Khrulkov V, Babenko A (2022)\nLabel-efficient semantic segmentation with diffusion models. In:\nICLR\nBarron JT, Poole B (2016) The fast bilateral solver. In: ECCV\nBochkovskiy A, Wang CY, Liao HYM (2020) Yolov4: Optimal speed\nand accuracy of object detection. arXiv preprint arXiv:200410934\nChen K, Wang J, Pang J, Cao Y, Xiong Y, Li X, Sun S, Feng W, Liu\nZ, Xu J, Zhang Z, Cheng D, Zhu C, Cheng T, Zhao Q, Li B, Lu\nX, Zhu R, Wu Y, Dai J, Wang J, Shi J, Ouyang W, Loy CC, Lin D\n(2019) MMDetection: Open mmlab detection toolbox and bench-\nmark. arXiv preprint arXiv:190607155\nChen P, Sheng K, Zhang M, Shen Y, Li K, Shen C (2022) Open vocab-\nulary object detection with proposal mining and prediction equal-\nization. arXiv preprint arXiv:220611134\nCordts M, Omran M, Ramos S, Rehfeld T, Enzweiler M, Benenson\nR, Franke U, Roth S, Schiele B (2016) The cityscapes dataset for\nsemantic urban scene understanding. In: CVPR\nDeng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009) Imagenet: A\nlarge-scale hierarchical image database. In: CVPR\nDi Stefano L, Bulgarelli A (1999) A simple and efficient connected\ncomponents labeling algorithm. In: ICIAP\nDu Y, Wei F, Zhang Z, Shi M, Gao Y, Li G (2022) Learning to prompt\nfor open-vocabulary object detection with vision-language model.\nIn: CVPR\nDvornik N, Mairal J, Schmid C (2018) Modeling visual context is key\nto augmenting object detection datasets. In: ECCV\nDwibedi D, Misra I, Hebert M (2017) Cut, paste and learn: Surpris-\ningly easy synthesis for instance detection. In: ICCV\nFang HS, Sun J, Wang R, Gou M, Li YL, Lu C (2019) Instaboost:\nBoosting instance segmentation via probability map guided copy-\npasting. In: ICCV\nGal R, Arar M, Atzmon Y, Bermano AH, Chechik G, Cohen-Or D\n(2023) Designing an encoder for fast personalization of text-to-\nimage models. arXiv preprint arXiv:230212228\nGao M, Xing C, Niebles JC, Li J, Xu R, Liu W, Xiong C (2022) Open\nvocabulary object detection with pseudo bounding-box labels. In:\nECCV\nGe Y, Xu J, Zhao BN, Itti L, Vineet V (2022) Dall-e for detec-\ntion: Language-driven context image synthesis for object detection.\narXiv preprint arXiv:220609592\nGhiasi G, Cui Y, Srinivas A, Qian R, Lin TY, Cubuk ED, Le QV, Zoph\nB (2021) Simple copy-paste is a strong data augmentation method\nfor instance segmentation. In: CVPR\nGhiasi G, Gu X, Cui Y, Lin TY (2022) Scaling open-vocabulary image\nsegmentation with image-level labels. In: ECCV\nGu X, Lin TY, Kuo W, Cui Y (2022) Open-vocabulary object detection\nvia vision and language knowledge distillation. In: ICLR\nGupta A, Dollar P, Girshick R (2019) Lvis: A dataset for large vocab-\nulary instance segmentation. In: CVPR\nHan J, Niu M, Du Z, Wei L, Xie L, Zhang X, Tian Q (2020) Joint\ncoco and lvis workshop at eccv 2020: Lvis challenge track technical\nreport: Asynchronous semi-supervised learning for large vocabulary\ninstance segmentation. In: ECCVW\nHe K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image\nrecognition. In: CVPR\nHe K, Gkioxari G, Doll\u00b4ar P, Girshick R (2017) Mask r-cnn. In: ICCV\nHertz A, Mokady R, Tenenbaum J, Aberman K, Pritch Y, Cohen-Or D\n(2023) Prompt-to-prompt image editing with cross attention control.\nIn: ICLR\nHinterstoisser S, Lepetit V, Wohlhart P, Konolige K (2018) On pre-\ntrained image features and synthetic images for deep learning. In:\nECCV Workshops\nHu X, Jiang Y, Tang K, Chen J, Miao C, Zhang H (2020) Learning to\nsegment the tail. In: CVPR\nJia C, Yang Y, Xia Y, Chen YT, Parekh Z, Pham H, Le Q, Sung YH, Li\nZ, Duerig T (2021) Scaling up visual and vision-language represen-\ntation learning with noisy text supervision. In: ICML\nKarras T, Aittala M, Aila T, Laine S (2022) Elucidating the de-\nsign space of diffusion-based generative models. arXiv preprint\narXiv:220600364\nKingma DP, Welling M (2014) Auto-encoding variational bayes. In:\nICLR\nKirillov A, Mintun E, Ravi N, Mao H, Rolland C, Gustafson L, Xiao\nT, Whitehead S, Berg AC, Lo WY, et al (2023) Segment anything.\narXiv preprint arXiv:230402643\nKumari N, Zhang B, Zhang R, Shechtman E, Zhu JY (2022) Multi-\nconcept customization of text-to-image diffusion. arXiv preprint\narXiv:221204488\nKuo W, Cui Y, Gu X, Piergiovanni A, Angelova A (2023) F-vlm: Open-\nvocabulary object detection upon frozen vision and language mod-\nels. In: ICLR\nKuznetsova A, Rom H, Alldrin N, Uijlings J, Krasin I, Pont-Tuset J,\nKamali S, Popov S, Malloci M, Kolesnikov A, et al (2020) The open\nMosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation\n15\nimages dataset v4: Unified image classification, object detection,\nand visual relationship detection at scale. IJCV\nLi B, Weinberger KQ, Belongie S, Koltun V, Ranftl R (2022a)\nLanguage-driven semantic segmentation. ICLR\nLi D, Ling H, Kim SW, Kreis K, Fidler S, Torralba A (2022b) Big-\ndatasetgan: Synthesizing imagenet with pixel-wise annotations. In:\nCVPR\nLi Y, Wang T, Kang B, Tang S, Wang C, Li J, Feng J (2020) Overcom-\ning classifier imbalance for long-tail object detection with balanced\ngroup softmax. In: CVPR\nLi Z, Zhou Q, Zhang X, Zhang Y, Wang Y, Xie W (2023) Guiding\ntext-to-image diffusion model towards grounded generation. arXiv\npreprint arXiv:230105221\nLin TY, Maire M, Belongie S, Hays J, Perona P, Ramanan D, Doll\u00b4ar P,\nZitnick CL (2014) Microsoft coco: Common objects in context. In:\nECCV\nLin TY, Doll\u00b4ar P, Girshick R, He K, Hariharan B, Belongie S (2017)\nFeature pyramid networks for object detection. In: CVPR\nLiu J, Sun Y, Han C, Dou Z, Li W (2020) Deep representation learning\non long-tailed data: A learnable embedding augmentation perspec-\ntive. In: CVPR\nLiu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B (2021) Swin\ntransformer: Hierarchical vision transformer using shifted windows.\nIn: ICCV\nLoshchilov I, Hutter F (2017) Sgdr: Stochastic gradient descent with\nwarm restarts. In: ICLR\nLoshchilov I, Hutter F (2019) Decoupled weight decay regularization.\nIn: ICLR\nMiller GA (1995) Wordnet: a lexical database for english. Commun\nACM\nMinderer M, Gritsenko A, Stone A, Neumann M, Weissenborn D,\nDosovitskiy A, Mahendran A, Arnab A, Dehghani M, Shen Z, et al\n(2022) Simple open-vocabulary object detection with vision trans-\nformers. In: ECCV\nOtsu N (1979) A threshold selection method from gray-level his-\ntograms. TSMC\nParmar G, Singh KK, Zhang R, Li Y, Lu J, Zhu JY (2023) Zero-shot\nimage-to-image translation. arXiv preprint arXiv:230203027\nRadford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, Sastry\nG, Askell A, Mishkin P, Clark J, et al (2021a) Learning transferable\nvisual models from natural language supervision. In: ICML\nRadford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, Sastry\nG, Askell A, Mishkin P, Clark J, et al (2021b) Learning transferable\nvisual models from natural language supervision. In: ICML\nRamesh A, Dhariwal P, Nichol A, Chu C, Chen M (2022) Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint\narXiv:220406125\nRasheed H, Maaz M, Khattak MU, Khan S, Khan FS (2022) Bridging\nthe gap between object and image-level representations for open-\nvocabulary detection. In: NeurIPS\nRen J, Yu C, Ma X, Zhao H, Yi S, et al (2020) Balanced meta-softmax\nfor long-tailed visual recognition. In: NeurIPS\nRichter SR, Vineet V, Roth S, Koltun V (2016) Playing for data:\nGround truth from computer games. In: ECCV\nRombach R, Blattmann A, Lorenz D, Esser P, Ommer B (2022) High-\nresolution image synthesis with latent diffusion models. In: CVPR\nRonneberger O, Fischer P, Brox T (2015) U-net: Convolutional net-\nworks for biomedical image segmentation. In: MICCAI\nSaharia C, Chan W, Saxena S, Li L, Whang J, Denton E, Ghasemipour\nSKS, Ayan BK, Mahdavi SS, Lopes RG, et al (2022) Photorealistic\ntext-to-image diffusion models with deep language understanding.\nIn: NeurIPS\nSharma P, Ding N, Goodman S, Soricut R (2018) Conceptual captions:\nA cleaned, hypernymed, image alt-text dataset for automatic image\ncaptioning. In: ACL\nSu H, Qi CR, Li Y, Guibas LJ (2015) Render for cnn: Viewpoint esti-\nmation in images using cnns trained with rendered 3d model views.\nIn: ICCV\nTan J, Wang C, Li B, Li Q, Ouyang W, Yin C, Yan J (2020a) Equaliza-\ntion loss for long-tailed object recognition. In: CVPR\nTan J, Zhang G, Deng H, Wang C, Lu L, Li Q, Dai J (2020b) 1st place\nsolution of lvis challenge 2020: A good box is not a guarantee of a\ngood mask. arXiv preprint arXiv:200901559\nTan J, Lu X, Zhang G, Yin C, Li Q (2021) Equalization loss v2: A\nnew gradient balance approach for long-tailed object detection. In:\nCVPR\nTan M, Pang R, Le QV (2020c) Efficientdet: Scalable and efficient\nobject detection. In: CVPR\nWang J, Zhang W, Zang Y, Cao Y, Pang J, Gong T, Chen K, Liu Z, Loy\nCC, Lin D (2021a) Seesaw loss for long-tailed instance segmenta-\ntion. In: CVPR\nWang T, Li Y, Kang B, Li J, Liew J, Tang S, Hoi S, Feng J (2020) The\ndevil is in classification: A simple framework for long-tail instance\nsegmentation. In: ECCV\nWang T, Zhu Y, Zhao C, Zeng W, Wang J, Tang M (2021b) Adaptive\nclass suppression loss for long-tail object detection. In: CVPR\nWaqas Zamir S, Arora A, Gupta A, Khan S, Sun G, Shahbaz Khan F,\nZhu F, Shao L, Xia GS, Bai X (2019) isaid: A large-scale dataset for\ninstance segmentation in aerial images. In: CVPRW\nWei F, Gao Y, Wu Z, Hu H, Lin S (2021) Aligning pretraining for\ndetection via object-level contrastive learning. In: NeurIPS\nWu J, Song L, Wang T, Zhang Q, Yuan J (2020) Forest r-cnn: Large-\nvocabulary long-tailed object detection and instance segmentation.\nIn: ACM-MM\nWu J, Li X, Xu S, Yuan H, Ding H, Yang Y, Li X, Zhang J, Tong Y,\nJiang X, Ghanem B, Tao D (2023) Towards open vocabulary learn-\ning: A survey. arXiv preprint arXiv:230615880\nWu Y, Kirillov A, Massa F, Lo WY, Girshick R (2019) Detectron2\nZang Y, Huang C, Loy CC (2021) Fasa: Feature augmentation and sam-\npling adaptation for long-tailed instance segmentation. In: ICCV\nZang Y, Li W, Zhou K, Huang C, Loy CC (2022) Open-vocabulary detr\nwith conditional matching. ECCV\nZareian A, Rosa KD, Hu DH, Chang SF (2021) Open-vocabulary ob-\nject detection using captions. CVPR\nZhang C, Pan TY, Li Y, Hu H, Xuan D, Changpinyo S, Gong B, Chao\nWL (2021a) Mosaicos: a simple and effective use of object-centric\nimages for long-tailed object detection. In: ICCV\nZhang J, Huang J, Jin S, Lu S (2023) Vision-language models for vi-\nsion tasks: A survey. arXiv preprint arXiv:230400685\nZhang S, Li Z, Yan S, He X, Sun J (2021b) Distribution alignment: A\nunified framework for long-tail visual recognition. In: CVPR\nZhang Y, Ling H, Gao J, Yin K, Lafleche JF, Barriuso A, Torralba\nA, Fidler S (2021c) Datasetgan: Efficient labeled data factory with\nminimal human effort. In: CVPR\nZhao H, Sheng D, Bao J, Chen D, Chen D, Wen F, Yuan L, Liu C,\nZhou W, Chu Q, Zhang W, Yu N (2023) X-paste: Revisiting scalable\ncopy-paste for instance segmentation using clip and stablediffusion.\nIn: ICML\nZhong Y, Yang J, Zhang P, Li C, Codella N, Li LH, Zhou L, Dai\nX, Yuan L, Li Y, et al (2022) Regionclip: Region-based language-\nimage pretraining. In: CVPR\nZhou K, Yang J, Loy CC, Liu Z (2022a) Conditional prompt learning\nfor vision-language models. In: CVPR\nZhou X, Koltun V, Kr\u00a8ahenb\u00a8uhl P (2021) Probabilistic two-stage detec-\ntion. arXiv preprint arXiv:210307461\nZhou X, Girdhar R, Joulin A, Kr\u00a8ahenb\u00a8uhl P, Misra I (2022b) Detecting\ntwenty-thousand classes using image-level supervision. In: ECCV\n"
  },
  {
    "title": "Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model",
    "link": "https://arxiv.org/pdf/2309.13018.pdf",
    "upvote": "9",
    "text": "DYNAMIC ASR PATHWAYS: AN ADAPTIVE MASKING APPROACH TOWARDS EFFICIENT\nPRUNING OF A MULTILINGUAL ASR MODEL\nJiamin Xie\u2217,1 , Ke Li2, Jinxi Guo2, Andros Tjandra2, Yuan Shangguan2,\nLeda Sari2, Chunyang Wu2, Junteng Jia2, Jay Mahadeokar2, Ozlem Kalinli2\n1Center for Robust Speech Systems (CRSS), University of Texas at Dallas, USA\n2Meta AI, USA\njiamin.xie@utdallas.edu, kli26@meta.com\nABSTRACT\nNeural network pruning offers an effective method for compressing\na multilingual automatic speech recognition (ASR) model with min-\nimal performance loss. However, it entails several rounds of pruning\nand re-training needed to be run for each language. In this work, we\npropose the use of an adaptive masking approach in two scenarios\nfor pruning a multilingual ASR model efficiently, each resulting in\nsparse monolingual models or a sparse multilingual model (named as\nDynamic ASR Pathways). Our approach dynamically adapts the sub-\nnetwork, avoiding premature decisions about a fixed sub-network\nstructure. We show that our approach outperforms existing pruning\nmethods when targeting sparse monolingual models. Further, we\nillustrate that Dynamic ASR Pathways jointly discovers and trains\nbetter sub-networks (pathways) of a single multilingual model by\nadapting from different sub-network initializations, thereby reduc-\ning the need for language-specific pruning.\nIndex Terms\u2014 Multilingual, Automatic Speech Recognition,\nSparsity, Pruning\n1. INTRODUCTION\nAutomatic speech recognition (ASR) has become a key feature in\nsmart devices, serving a diverse customer base [1, 2, 3]. For a suc-\ncessful on-device deployment, the ASR model must operate within\nthe storage and computational constraints while delivering an opti-\nmal performance. Furthermore, the ASR model needs to support\nmultiple languages [4, 5] to interact with users worldwide. Neural\nnetwork pruning [6, 7, 8] is an effective technique for reducing the\nsize of an ASR model with minimal performance loss [9, 10]. How-\never, the pruning process, such as Iterative Magnitude Pruning (IMP)\n[6, 11] and Lottery Ticket Hypothesis (LTH) [8], involves multiple\niterations of pruning and re-training to achieve the best performance.\nThe pruning step identifies a task-specific sub-network within the\noriginal dense neural network. Subsequently, the re-training step\ntrains this sub-network with task-specific data, mitigating the perfor-\nmance loss introduced in pruning. This iterative process continues\nuntil the target sparsity level is reached.\nPruning a multilingual ASR model presents specific challenges.\nWhen pruning a pre-trained dense multilingual ASR model, it can\nresult in two scenarios, as discussed in [12]. In the first scenario,\nthe model is fine-tuned and pruned for each language separately,\nresulting in multiple language-specific sparse models. While this\napproach optimizes performance in each language, it can increase\n\u2217 Work done while Jiamin Xie was an intern at Meta AI.\nstorage requirements due to maintaining different monolingual mod-\nels. In the second scenario, the multilingual model is fine-tuned and\npruned using a multilingual dataset, creating a single sparse model\nby finding a language-agnostic pruning mask. While multilingual\ntraining can promote knowledge transfer across languages [13, 14],\ndata imbalance [15, 16] may cause performance degradation in some\nlanguages when training a single language-agnostic sub-network.\nMixing languages in a training batch can also create conflicts in\nweight updates with different languages fighting for model capacity,\nknown as the negative interference effect [17, 18], making it chal-\nlenging to identify an optimal language-agnostic sub-network. A\nrecent study [12] proposes to train language-specific sub-networks\n(referred to as pathways) jointly within the original dense multi-\nlingual model instead of training a language-agnostic sub-network.\nThis method employs monolingual data in a batch to fine-tune the\nrespective pathway without interference from other languages. As\nthese pathways overlap, the weights are updated either in a language-\nspecific or a language-agnostic manner, surpassing the performance\nof language-agnostic methods. However, a drawback of the path-\nways method is acquiring each pathway in a separate stage that per-\nforms monolingual training and pruning, incurring a computational\ncost that scales linearly with the number of languages. These path-\nways, once obtained, remain fixed throughout the training process,\nlacking adaptation to the multilingual data.\nIn this study, we introduce an adaptive masking approach for\nadapting language-specific sub-networks in monolingual or multilin-\ngual pruning situations. Our proposed method re-evaluates the prun-\ning mask dynamically during training, allowing the sub-network to\nalign better with the training data comparing to a fixed masking ap-\nproach. We first assess the benefit of applying this technique to the\nmonolingual case, obtaining sparse monolingual ASR models. We\nthen prune and adapt pathways by employing our approach in mul-\ntilingual training, evaluating the performance of a jointly fine-tuned\nand pruned multilingual ASR model.\n2. RELATED WORKS\nMultilingual Training. The concept of training sub-networks was\nproposed in the context of multi-task learning [19] and has since\nfound applications in multilingual training [20, 21, 22]. This ap-\nproach has demonstrated an efficacy across various domains, includ-\ning self-supervised learning (SSL) [20], machine translation [21],\nand language modeling [22]. Our research builds upon a recent study\n[12] that emphasized the effectiveness of training language-specific\nsub-networks for the supervised multilingual ASR task.\narXiv:2309.13018v2  [eess.AS]  11 Jan 2024\nTrain\nEN\nBatch\nEN\nsub-network\n No \nAt adapt\nstep n ?\nTrainable (masked-out weights)\nMasked-out weights\nTrainable (weights in mask)\nAt prune\nstep T ?\nAdapt & Masked-out\nPrune & Masked-out\nRepeat\nEN\nBatch\nEN\nRepeat\nEN\nBatch\nEN\nsub-network\nNo\nFig. 1: Flowchart of the training and pruning process with adaptive\nmasking enabled for monolingual data\nAdaptive Pruning. Previous research on adaptive pruning can be\nbroadly categorized based on whether the pruning masks are made\ntrainable. One approach [23] involves fine-tuning the trainable prun-\ning masks on downstream tasks while keeping the original model\nweights fixed, demonstrating performance improvements over tradi-\ntional fine-tuning methods. Another approach [24] focuses on re-\nlearning the pruned weights by lifting the mask during training, al-\nlowing adjustments to the pruning mask without learning it directly.\nThis technique was applied to fine-tune a multilingual speech SSL\nmodel for monolingual ASR tasks. In contrast to the latter approach,\nour study applies adaptive masking to the supervised multilingual\nASR task with structured pruning, introducing novel strategies for\nattaining and adapting sub-networks during multilingual training.\n3. METHODOLOGY\nWe first recap the concept of pruning (Section 3.1). We then illus-\ntrate current pruning methods that are foundational to our proposed\napproach (Section 3.2). Finally, we described our adaptive masking\napproach for monolingual and multilingual pruning (Section 3.3).\n3.1. Pruning recap\nFor a dense neural network f(x; \u03b80) trained with input sample x and\nparameters \u03b80, we denote a sub-network f(x; m \u2299 \u03b80) with a binary\npruning mask m and the element-wise product \u2299. The pruning goal\nis to identify the sub-network f(x; m \u2299 \u03b8) through additional train-\ning, where \u03b8 can be the parameters obtained at any stage of training.\nWe consider a progressive pruning schedule [7], where pruning starts\nfrom a low sparsity and incrementally steps up to the target sparsity.\n3.2. Current pruning methods\n3.2.1. IMP, LTH, and LAP\nThe iterative magnitude pruning (IMP) method [6] involves fine-\ntuning a dense model for a specified number of steps denoted as T\nwhile making pruning decisions based on the magnitude of weights.\nHere, the magnitude of a weight reflects its significance to the task,\nwith larger values indicating higher importance. For structured prun-\ning, we use the block-wise scheme similar to that in [25], following\na block pattern of 8 \u00d7 1. This pattern implies that eight consecutive\nTrain\nEN\nBatch\n No \nAt adapt\nstep n ?\nTrainable (masked-out weights)\nMasked-out weights\nTrainable (weights in mask)\nAt prune\nstep T ?\nAdapt & Masked-out\nPrune & Masked-out\nRepeat\nEN\nBatch\nFR\n         sub-net\nNo\nEN\nsub-network\nFR\n         sub-net\nEN\nsub-network\n=\n      FR\nEN\nFR\nBatch\nor\nRepeat\nEN\nBatch\nFR\nBatch\nor\nFig. 2: Flowchart of the training and pruning process with adaptive\nmasking enabled for multilingual data\nweights within a column are pruned simultaneously. We evaluate\nmagnitudes by the L2 norm of a weight block. To initiate the IMP\nprocedure, we initialize model parameters \u03b8 with pre-trained dense\nweights \u03b80 and set the binary pruning mask m to all ones 1, where\nm \u2208 {0, 1}|\u03b8|. The IMP procedure is illustrated as follows:\nRepeat\n1. Train f(x; m \u2299 \u03b8) for T steps, resulting in f(x; m \u2299 \u03b8T ).\n2. Prune p% of total weights from m \u2299 \u03b8T that has the smallest\nmagnitudes. Setting the pruned positions in m to 0.\n3. Assign \u03b8T to \u03b8 for the next iteration\nUntil m reaches the target sparsity\nWe note the property of a pruning mask depends on the training\ndata. When monolingual data is used in IMP, this procedure yields\na language-specific pruning mask ml for a language l. For multilin-\ngual data, it results in a language-agnostic pruning mask, referred to\nas language-agnostic pruning (LAP). Importantly, the pruning mask\nremains fixed at any step within the T training steps, suggesting the\npruning decision is irreversible.\nThe lottery ticket hypothesis (LTH) method [8] modifies the\nStep 3 of the IMP procedure by assigning the pre-trained dense\nweights \u03b80 to \u03b8 instead of \u03b8T , referred to as a re-winding step. It as-\nsumes that a sub-network capable of achieving performance similar\nto the original dense network exists within the original dense archi-\ntecture. Therefore, the LTH method leads to the identification of a\nsub-network embedded within the original dense model weights.\n3.2.2. ASR Pathways\nThe ASR Pathways [12] provides a method to fine-tune a multi-\nlingual ASR model using the language-specific sub-networks (or\npathways) identified through IMP, LTH, or other pruning methods.\nThese sub-networks are attained at the target sparsity level and re-\nmain fixed throughout training.\nThe mini-batch is configured as\nmonolingual, while the training includes a mixture of languages\nacross mini-batches. This setup ensures that each mini-batch acti-\nvates one pathway and updates the weights underlying the pruning\nmask of this pathway, denoted as ml \u2299 \u03b8. Since each language-\nspecific sub-network is a part of the original dense multilingual\nmodel and gets fine-tuned together, the training process results in a\nfinal sparse multilingual ASR model.\nTable 1: WER (%) results on the MLS test set, pruning a dense multilingual ASR model. The proposed approach allows the mask to change\nin training and is compared to other pruning methods for monolingual training scenario.\nStage\nModel\nMask can\nchange?\nSparsity\nMonolingual or\nMultilingual training?\nEN\nFR\nIT\nNL\nAvg.\nRef.\n56M Dense\n/\n0%\nMonolingual\n12.15\n16.00\n27.62\n23.23\n19.75\n(1)\n187M Dense\n/\n0%\nMultilingual\n12.91\n10.90\n16.94\n17.56\n14.58\n(2)\nLAP\nNo\n70%\nMultilingual\n13.82\n11.98\n27.71\n19.32\n18.21\nIMP\nNo\n70%\nMonolingual\n10.74\n11.26\n17.90\n18.38\n14.57\nLTH\nNo\n70%\nMonolingual\n10.80\n10.38\n18.44\n17.48\n14.28\n(3)\nASR Pathways (IMP-70%)\nNo\n70%\nMultilingual\n11.15\n10.68\n17.53\n16.90\n14.06\nASR Pathways (LTH-70%)\nNo\n70%\nMultilingual\n11.39\n10.20\n17.58\n15.84\n13.75\n(2)\nProposed\nIMP\nYes\n70%\nMonolingual\n10.07\n10.90\n17.21\n16.98\n13.79\nLTH\nYes\n70%\nMonolingual\n10.54\n9.91\n17.06\n16.63\n13.53\n3.3. The adaptive masking approach\n3.3.1. Monolingual pruning\nWe propose an adaptive masking approach for monolingual pruning,\nyielding a language-specific pruning mask adapted with the data. We\nillustrate this approach as a flowchart shown in Figure 1. Within the\nframework of the IMP procedure, we introduce a mask adaptation\nstep denoted as n (where n < T). During the adaptation step, we\nre-evaluate the sub-network configuration (adapt) by pruning from\nall weights in \u03b8n with a portion p% that maintains the sparsity level\nof the current pruning mask. Next, we prune \u201csoftly\u201d by setting the\npruned weights to zero, denoted as (1 \u2212 m) \u2299 \u03b8n, and make them\ntrainable (masked-out). Since the masked-out weights receive up-\ndates from training, they can form new connections within the net-\nwork and reveal an optimal configuration of the sub-network as the\ntraining evolves. For the pruning step, we simply raise the sparsity\nlevel and prune from all weights in \u03b8n as opposed to pruning from\nweights in m \u2299 \u03b8T in the IMP procedure.\n3.3.2. Multilingual pruning\nWe propose an adaptive masking approach for multilingual pruning\nbased on the pathways training method described in [12], named\nDynamic ASR Pathways. We use a similar adaptation step to the\nmonolingual pruning and illustrate it in a flowchart shown in Figure\n2. When a mini-batch in language z is processed, we train the sub-\nnetwork of this language z and a \u201cresidual\u201d sub-network, excluding\nother language-specific sub-networks. Given a language set L rep-\nresenting all languages in the data, we denote this pruning mask as,\nmz,r = mz \u222a (1 \u2212 \u222al in L,l\u0338=zmz)\n(1)\nDuring the adaption step, we re-evaluate the language-specific sub-\nnetwork by pruning from weights in mz,r \u2299\u03b8n with its current spar-\nsity level held. Since the adaptation step is monolingual, the newly\nadapted sub-network can become more language-specific compared\nto before. During the pruning step, we simultaneously prune sub-\nnetworks by pruning from weights in mz,r \u2299 \u03b8T , iterating over each\nlanguage z in the language set L. Because different languages would\nshare the \u201cresidual\u201d sub-network depending on the data distribution,\nthis pruning step promotes parameter sharing among sub-networks,\ncompensating potential reductions in the adaptation step.\n4. EXPERIMENTAL SETUP\n4.1. Dataset\nWe conduct our experiments using the multilingual Librispeech\n(MLS) [26] dataset, which consists of multilingual speech derived\nfrom audiobooks. Our study focuses on four languages: English\n(EN), French (FR), Italian (IT), and Dutch (NL), with respective\ntraining audio length of 44.7k hrs, 1.1k hrs, 0.2k hrs, 1.6k hrs.\n4.2. Implementation details\nWe employ a streaming RNN-T model for the dense multilingual\nmodel, using 30 Emformer layers [27] with 512 input dimensions,\n2048 feed-forward dimensions, and encoder layers with convolu-\ntional blocks [28]. This model has about 180 million parameters. We\nutilize word pieces to recognize spoken words in all four languages,\ntotaling 1548 items. For consistency, we use the same output layer\nsize for all training setups. The learning rate schedule is tri-stage\n[29] with a peak learning rate of 1e-3. For monolingual models, we\nconduct training for 100K, 80K, 50K, and 80K steps for EN, FR,\nIT, and NL, respectively. The multilingual pathway model under-\ngoes training for 200K and 100K steps for IMP and LTH methods,\nrespectively. We also conduct a bilingual experiment for the mul-\ntilingual pathway models, where the training step is 80K. We em-\nploy an uniform data sampling scheme for the multilingual training\nwhen Dynamic ASR Pathways method is compared, otherwise an\nnon-uniform sampling scheme. The prune step T was set to be 8%\nof the training step for each setup, with an adaption step n of 100\nand a prune portion p of 20% across all experiments. Pruning was\napplied exclusively to linear layers in the encoder Eformer and the\npredictor LSTM layers, with a uniform sparsity across all prunable\nlayers [25]. We apply group lasso regularization following [30]. We\nuse 16 GPUs for monolingual training and 32 GPUs for multilingual\ntraining, with a per-GPU batch size of 28.\n5. RESULTS\nWe first show baseline results from using current pruning methods\n(Section 5.1). We then compare the adaptive masking approach for\nmonolingual pruning to its relative baseline (Section 5.2). Finally,\nwe compare the adaptive masking approach for multilingual pruning\n(Dynamic ASR Pathways) to the ASR Pathways baseline (Section\n5.3).\nTable 2: WER (%) results on the MLS test set, utilizing language-\nspecific pruning masks. The proposed approach is compared to an\nexisting method for the bilingual training case.\nModel\nInitialization\nMask\nchange?\nSparsity\nFR\nNL\nAvg.\nASR\nPathways\nLTH-70%\nNo\n70%\n10.73\n16.23\n13.48\nASR\nPathways\nLAP-70%\nNo\n70%\n11.98\n19.32\n15.65\nDynamic\nASR\nPathways\nLTH-70%\nYes\n70%\n11.31\n15.55\n13.43\nLTH-50%\nYes\n70%\n10.48\n14.92\n12.70\nLTH-20%\nYes\n70%\n10.99\n16.17\n13.58\nDynamic\nASR\nPathways\nLAP-70%\nYes\n70%\n10.98\n16.54\n13.76\nLAP-50%\nYes\n70%\n10.82\n16.25\n13.54\nLAP-20%\nYes\n70%\n10.88\n16.43\n13.65\n5.1. Baselines\nIn Table 1, we present the results of existing methods for pruning a\nmultilingual ASR model. We breakdown these methods into three\nstages: 1) training a dense multilingual ASR model, 2) pruning the\ndense multilingual ASR model, and 3) training a sparse multilin-\ngual model. For reference, we include results of dense monolin-\ngual model. Both the IMP and LTH language-specific pruning meth-\nods achieve matching performance to the original dense multilingual\nmodel and surpass the dense monolingual models. The ASR Path-\nways method outperforms other methods using the language-specific\nmasks obtained in Stage (2), promoting parameter sharing among\nlanguages.\n5.2. Adaptive masking in monolingual pruning\nIn the last two rows of Table 1, we present the results of using adap-\ntive masking for monolingual pruning. Our proposed Stage (2) mod-\nified the IMP and the LTH language-specific pruning methods in\nStage (2) and achieved a consistent 5.3% relative WER reduction\naveraged across languages. Comparing the adapted sub-networks to\nthe fixed ones, we noticed about an 80% similarity and a 20% differ-\nence, indicating the effective adaptation occurs within a small part\nof the pruning masks. Our proposed Stage (2) also outperforms the\nsparse multilingual model obtained in Stage (3), providing an effi-\ncient alternative when storing multiple models is not a concern.\n5.3. Adaptive masking in multilingual pruning\nIn Table 2, we show the results of a bilingual experiment when using\nadaptive masking for multilingual pruning. We initialized the train-\ning with the LTH or the LAP masks at the target sparsity level (70%)\nand achieved a consistent improvement when only adaptation is en-\nabled. Notably, adapting the LAP-70% mask achieves a 12.1% rel-\native WER reduction, indicating the adaptation step has effectively\nturned the LAP mask to become more language-specific. We no-\nticed a similar but improved performance when using the LTH-70%\nmasks, suggesting these masks may be robust at a high sparsity level.\nWe observed the best overall performance using mask initializa-\ntion at a middle sparsity level (50%) when both pruning and adap-\ntation steps are enabled. For the LTH-50% mask initialization, our\n1The union ratio indicates the ratio between surviving parameters in the\nunion of all masks and the total parameters of the network [12]\n2Due to time limitation, this result is inferred at an early checkpoint, sub-\nject to a better future improvement\nTable 3: WER (%) results on the MLS test set, utilizing language-\nspecific pruning masks. The proposed approach is compared to an\nexisting method, extending to four languages.\nModel\nInitialization\nSparsity\nEN\nFR\nIT\nNL\nAvg.\nASR\nPathways\nLTH-70%\n70%\n13.56\n10.53\n17.10\n16.37\n14.39\nDynamic\nASR\nPathways\nLTH-50%\n70%\n14.84\n10.35\n16.10\n15.15\n14.11\nDynamic ASR Pathways method outperformed the respective ASR\nPathways baseline with a 5.8% relative WER reduction. From an\nanalysis, we find this model results in a even lower union ratio1\n(0.34) compared to its baseline (0.36), indicating a better multilin-\ngual performance is achieved using even less total effective model\nparameters. We believe this effect can be attributed to the prun-\ning step introduced in our approach that increases parameter sharing\n(Section 3.3.2). For different LAP mask initialization, we noticed\nconsistently a significant performance gain compared to its respec-\ntive baseline. Further, it is almost matching performance to the ASR\nPathways baseline using the LTH-70% masks, showing a benefit of\nefficiency with the language-specific pruning rounds eliminated.\nIn Table 3, we present the extended results of applying Dynamic\nASR Pathways to pruning for more languages, initializing from the\nLTH-50% masks.\nOur proposed approach outperforms the ASR\nPathways baseline with a 2% relative WER reduction1 on average\nacross four languages. When considering the performance across\nFR, IT, and NL, it achieves a notable 5.5% relative WER reduction.\nWhen initializing at a 50% sparsity level, we saved additional rounds\nof training and pruning for achieving a target sparsity level, showing\nthe efficacy of applying our approach towards efficient pruning of a\nmultilingual ASR model.\n6. CONCLUSIONS\nIn conclusion, we proposed an adaptive masking approach for both\nmonolingual and multilingual pruning. In the former case, our pro-\nposed method achieved a consistent 5.3% relative WER reduction\naveraged across languages and outperformed the sparse multilingual\nmodel obtained from going through an additional stage, offering\na convenient trade-off between storage and efficiency. In the lat-\nter case, we showed the efficacy of our approach in pruning and\nadapting from different pruning mask initalizations. When initial-\nized from language-agnostic pruning masks, our Dynamic ASR\nPathways method showed a consistent and comparable performance\nto the best performance of the ASR Pathways method that uses\nlanguage-specific pruning masks, indicating a benefit of efficiency\nwith our approach. When initialized from language-specific pruning\nmasks at a 50% sparsity level, our Dynamic ASR Pathways method\noutperforms the ASR Pathways method, ranging from a 2% to 5.8%\nrelative WER reduction.\nFor future work, we want to scale our\nresearch of multilingual pruning for more languages and explore the\noption to make pruning masks learnable.\n7. REFERENCES\n[1] Yuan Shangguan, Rohit Prabhavalkar, Hang Su, Jay Ma-\nhadeokar, Yangyang Shi, Jiatong Zhou, Chunyang Wu, Duc\nLe, Ozlem Kalinli, Christian Fuegen, and Michael L. Seltzer,\n\u201cDissecting User-Perceived Latency of On-Device E2E Speech\nRecognition,\u201d in Interspeech 2021.\n[2] Yanzhang He, Tara N Sainath, Rohit Prabhavalkar, Ian Mc-\nGraw, Raziel Alvarez, Ding Zhao, David Rybach, Anjuli Kan-\nnan, Yonghui Wu, Ruoming Pang, et al., \u201cStreaming end-to-\nend speech recognition for mobile devices,\u201d in ICASSP 2019.\n[3] Zhifu Gao, Yiwu Yao, Shiliang Zhang, Jun Yang, Ming Lei,\nand Ian McLoughlin, \u201cExtremely Low Footprint End-to-End\nASR System for Smart Device,\u201d in Interspeech 2021.\n[4] Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Han-\nnun, Vitaliy Liptchinsky, Gabriel Synnaeve, and Ronan Col-\nlobert, \u201cMassively Multilingual ASR: 50 Languages, 1 Model,\n1 Billion Parameters,\u201d in Interspeech 2020.\n[5] Andros Tjandra, Nayan Singhal, David Zhang, Ozlem Kalinli,\nAbdelrahman Mohamed, Duc Le, and Michael L Seltzer,\n\u201cMassively multilingual asr on 70 languages: Tokenization, ar-\nchitecture, and generalization capabilities,\u201d in ICASSP 2023.\n[6] Song Han, Jeff Pool, John Tran, and William Dally, \u201cLearning\nboth weights and connections for efficient neural network,\u201d in\nNeuraIPS 2015.\n[7] Michael H. Zhu and Suyog Gupta, \u201cTo prune, or not to prune:\nExploring the efficacy of pruning for model compression,\u201d in\nICLR 2018.\n[8] Jonathan Frankle and Michael Carbin, \u201cThe lottery ticket hy-\npothesis: Finding sparse, trainable neural networks,\u201d in ICLR\n2019.\n[9] Yuan Shangguan, Jian Li, Qiao Liang, Raziel Alvarez, and Ian\nMcGraw, \u201cOptimizing speech recognition for the edge,\u201d arXiv\npreprint arXiv:1909.12408, 2019.\n[10] Sharan Narang, Greg Diamos, Shubho Sengupta, and Erich\nElsen, \u201cExploring sparsity in recurrent neural networks,\u201d in\nICLR 2017.\n[11] Alex Renda, Jonathan Frankle, and Michael Carbin, \u201cCompar-\ning rewinding and fine-tuning in neural network pruning,\u201d in\nICLR 2020.\n[12] Mu Yang, Andros Tjandra, Chunxi Liu, David Zhang, Duc Le,\nand Ozlem Kalinli, \u201cLearning asr pathways: A sparse multi-\nlingual asr model,\u201d in ICASSP 2023.\n[13] Arindrima Datta, Bhuvana Ramabhadran, Jesse Emond, Anjuli\nKannan, and Brian Roark, \u201cLanguage-agnostic multilingual\nmodeling,\u201d in ICASSP 2020.\n[14] Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude, Se-\nbastian Gehrmann, Sara Hooker, and Julia Kreutzer, \u201cIntrigu-\ning properties of compression on multilingual models,\u201d\nin\nEMNLP 2022.\n[15] Anjuli Kannan, Arindrima Datta, Tara N. Sainath, Eugene We-\ninstein, Bhuvana Ramabhadran, Yonghui Wu, Ankur Bapna,\nZhifeng Chen, and Seungji Lee,\n\u201cLarge-Scale Multilingual\nSpeech Recognition with a Streaming End-to-End Model,\u201d in\nInterspeech 2019.\n[16] Genta Indra Winata, Guangsen Wang, Caiming Xiong, and\nSteven Hoi, \u201cAdapt-and-Adjust: Overcoming the Long-Tail\nProblem of Multilingual Speech Recognition,\u201d in Interspeech\n2021.\n[17] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine,\nKarol Hausman, and Chelsea Finn,\n\u201cGradient surgery for\nmulti-task learning,\u201d NeurIPS 2020.\n[18] Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy,\nand Shruti Bhosale, \u201cCauses and cures for interference in mul-\ntilingual translation,\u201d in ACL 2023.\n[19] Tianxiang Sun, Yunfan Shao, Xiaonan Li, Pengfei Liu, Hang\nYan, Xipeng Qiu, and Xuanjing Huang, \u201cLearning sparse shar-\ning architectures for multiple tasks,\u201d AAAI 2020.\n[20] Yizhou Lu, Mingkun Huang, Xinghua Qu, Pengfei Wei, and\nZejun Ma, \u201cLanguage adaptive cross-lingual speech represen-\ntation learning with sparse sharing sub-networks,\u201d in ICASSP\n2022.\n[21] Zehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li, \u201cLearning\nlanguage specific sub-network for multilingual machine trans-\nlation,\u201d in Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), 2021.\n[22] Negar Foroutan, Mohammadreza Banaei, R\u00b4emi Lebret, An-\ntoine Bosselut, and Karl Aberer,\n\u201cDiscovering language-\nneutral sub-networks in multilingual language models,\u201d\nin\nEMNLP 2022.\n[23] Yonggan Fu, Yang Zhang, Kaizhi Qian, Zhifan Ye, Zhongzhi\nYu, Cheng-I Jeff Lai, and Celine Lin, \u201cLosses can be blessings:\nRouting self-supervised speech representations towards effi-\ncient multilingual and multitask speech processing,\u201d NeurIPS\n2022.\n[24] Cheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu Chang,\nYi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian, Sameer Khu-\nrana, David Cox, and Jim Glass, \u201cParp: Prune, adjust and re-\nprune for self-supervised speech recognition,\u201d NeurIPS 2021.\n[25] Haichuan Yang, Yuan Shangguan, Dilin Wang, Meng Li,\nPierce Chuang, Xiaohui Zhang, Ganesh Venkatesh, Ozlem\nKalinli, and Vikas Chandra, \u201cOmni-sparsity dnn: Fast sparsity\noptimization for on-device streaming e2e asr via supernet,\u201d in\nICASSP 2022.\n[26] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Syn-\nnaeve, and Ronan Collobert, \u201cMls: A large-scale multilingual\ndataset for speech research,\u201d arXiv preprint arXiv:2012.03411,\n2020.\n[27] Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng\nYeh, Julian Chan, Frank Zhang, Duc Le, and Mike Seltzer,\n\u201cEmformer:\nEfficient memory transformer based acoustic\nmodel for low latency streaming speech recognition,\u201d\nin\nICASSP 2021.\n[28] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-\nmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-\ndong Zhang, Yonghui Wu, and Ruoming Pang, \u201cConformer:\nConvolution-augmented Transformer for Speech Recognition,\u201d\nin Interspeech 2020.\n[29] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, and Quoc V. Le, \u201cSpecAugment:\nA Simple Data Augmentation Method for Automatic Speech\nRecognition,\u201d in Interspeech 2019.\n[30] Chunxi Liu, Yuan Shangguan, Haichuan Yang, Yangyang Shi,\nRaghuraman Krishnamoorthi, and Ozlem Kalinli, \u201cLearning a\ndual-mode speech recognition model via self-pruning,\u201d in 2022\nIEEE Spoken Language Technology Workshop (SLT). IEEE,\n2023, pp. 273\u2013279.\n"
  },
  {
    "title": "Robotic Offline RL from Internet Videos via Value-Function Pre-Training",
    "link": "https://arxiv.org/pdf/2309.13041.pdf",
    "upvote": "8",
    "text": "Robotic Offline RL from Internet Videos\nvia Value-Function Pre-Training\nChethan Bhateja*,1, Derek Guo*,1, Dibya Ghosh*,1, Anikait Singh1,2, Manan Tomar1, Quan Vuong2, Yevgen\nChebotar2, Sergey Levine1, and Aviral Kumar1\n*Equal contributions, 1UC Berkeley, 2Google DeepMind\nPre-training on Internet data has proven to be a key ingredient for broad generalization in many modern\nML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline\nRL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the\nrobotic learning pipeline. However, these methods have a \u201ctype mismatch\u201d with video data (such as Ego4D),\nthe largest prior datasets available for robotics, since video offers observation-only experience without the\naction or reward annotations needed for RL methods. In this paper, we develop a system for leveraging\nlarge-scale human video datasets in robotic offline RL, based entirely on learning value functions via\ntemporal-difference learning. We show that value learning on video datasets learns representations that\nare more conducive to downstream robotic offline RL than other approaches for learning from video data.\nOur system, called V-PTR, combines the benefits of pre-training on video data with robotic offline RL\napproaches that train on diverse robot data, resulting in value functions and policies for manipulation\ntasks that perform better, act robustly, and generalize broadly. On several manipulation tasks on a real\nWidowX robot, our framework produces policies that greatly improve over prior methods. Our video and\nadditional details can be found on our project website.\nVisual\nEncoder \nValue Learning\nw/o Actions\nValue\nPre-Training Phase 1\nEgo4D: 1K Scenes, 70K Clips, 4M Transitions\n20 Clips, ~500 Transitions\nPre-Training Phase 2\nFine-Tuning\nEg. Put Pot in Basket, Sweep Beans\nLearned Policy\nActions\nVisual\nEncoder \nQ-value\nPolicy\nAction\nBridge Data: 3K Clips, 150K Transitions\nFigure 1: Video pre-training for robots (V-PTR) pre-trains by learning value functions on a large-scale video\ndataset like Ego4D, and continues refining learned visual features by performing offline RL on multi-task robot data\nlike the Bridge dataset. This pre-training provides a useful initialization for offline RL fine-tuning on downstream\nrobot tasks with improved generalization and robustness compared to other approaches.\n1. Introduction\nDeveloping methods capable of acquiring robotic skills that generalize widely to new scenarios is an\nimportant problem in robotic learning. In other areas of machine learning, broad generalization has been\nfueled primarily by pre-training on large datasets with a diversity of behavior. It seems compelling that the\nsame formula may be applied to robotic learning, but in practice, even our largest robotic datasets contain\ndata for relatively few tasks, and from a limited number of scenarios. In principle, robotic reinforcement\nlearning (RL) should be able to learn from more general sources of data like human video, which are far\nCorresponding author(s): Chethan Bhateja (chetbhateja@berkeley.edu), Derek Guo (derekguo@berkeley.edu), Dibya Ghosh (dibya@berkeley.edu)\narXiv:2309.13041v1  [cs.RO]  22 Sep 2023\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nmore abundant and capture a broader set of skills, situations, and interactions. However, these datasets are\ndifficult to incorporate into RL methods that exist today, since internet-scale video data does not come with\naction or reward annotations present in typical robot data.\nExisting works (Nair et al., 2022b; Xiao et al., 2022b; Radosavovic et al., 2022) include video data in the robot\nlearning pipeline by performing self-supervised visual representation learning on video data (He et al.,\n2021), followed by downstream policy learning via behavioral cloning using the learned representations.\nWhile such an approach can extract visual features from video, it is limited in its ability to extract a deeper\n\u201cfunctional\u201d understanding of the world from video data: in principle, despite differences in embodiment,\nhuman videos can still be used to understand intents and affordances that can be executed in the real world,\nthe dynamics, and the eventual outcomes that can be attained by acting.\nMotivated by the above desiderata for video pre-training, in this work, we aim to develop an approach that\npre-trains on Internet-scale human video to produce representations for downstream offline RL. Our main\ncontribution is a system, which we call Video Pre-Training for Robots (V-PTR), that fits value functions to\nmodel long-term outcomes achieved when solving tasks on action-free video data.\nConcretely, V-PTR pre-trains on human videos by learning an intent-conditioned value function (Ghosh et al.,\n2023) via temporal-difference learning (TD-learning). This approach eschews self-supervised representation\nlearning objectives utilized in prior works (Nair et al., 2022b; Ma et al., 2022; Radosavovic et al., 2022) in\nfavor of a TD value learning objective, just as how downstream offline RL agents will fine-tune task-specific\nvalue functions. Next, we fine-tune on a multi-task robotic dataset, which is annotated with actions, tasks,\nand rewards, using value-based offline RL (Kumar et al., 2023). Downstream, when a target task is specified,\nV-PTR fine-tunes the multi-task policy on this task. Each phase of our system gradually incorporates the\nknowledge of \u201cwhat future outcomes can be achieved\u201d (video pre-training), \u201cwhat robot actions lead to\nthese outcomes\u201d (robot pre-training), and \u201chow can the desired task be solved\u201d (fine-tuning).\nOur experiments on several manipulation tasks on a real WidowX robot show that by pre-training on\nhuman video data (Ego4D (Grauman et al., 2022)) and multi-task robot data (Bridge data (Ebert et al., 2021)),\nV-PTR endows downstream offline RL methods with significantly improved zero-shot generalization and\nrobustness to different target objects, distractors, and other variations in the workspace compared to prior\nmethods that learn from videos, significantly outperforming prior methods including VIP (Ma et al., 2022).\nTo our knowledge, our work presents the first large-scale demonstration showing that TD-learning alone\ncan be an effective approach to pre-train from video for robotic RL.\n2. Related Work\nA number of prior approaches learn representations from video by applying image-level representation\nobjectives on individual frames in the video or by modelling temporal dependencies along the frames in a\ngiven trajectory. The former includes objectives like reconstruction (Nair et al., 2019; Seo et al., 2022a; Xiao\net al., 2022a; Karamcheti et al., 2023) or contrastive learning on images (Srinivas et al., 2020). While these\nobjectives are widely used in computer vision, resulting representations do not capture any information\nabout environment dynamics. The latter approaches model long-term dynamics from video by predicting\nthe next frame (Seo et al., 2022b), learning value functions (Ma et al., 2022; Ghosh et al., 2023), or running\ntime-contrastive learning (Sermanet et al., 2017; Nair et al., 2022a).\nIn the context of robot learning, recent works learn representations from internet video datasets like\nEgo4D (Grauman et al., 2022), using masked auto-encoders (Nair et al., 2022b; Karamcheti et al., 2023),\nlanguage-video alignment (Karamcheti et al., 2023), or time-contrastive learning (Nair et al., 2022b; Ma et al.,\n2022), and train downstream policies on frozen features using behavioral cloning. In our experiments, we\ncompare to several of these methods, and find that V-PTR attains a higher performance on real-world tasks,\nespecially when evaluated with high initial state variability and in the presence of distractors. Specifically\nrestricted to the setting with egocentric human video, some recent work (Bahl et al., 2023) also attempts\nto utilize wrist trajectory prediction to determine intermediate waypoints to guide robot controllers in\naccomplishing a task. This approach is orthogonal to our goal extracting a state representation and our\napproach can, in principle, be utilized in conjunction with the method from this prior work.\n2\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nThe most closely related work is value-implicit pre-training (VIP) (Ma et al., 2022), which pre-trains a value\nfunction using time-contrastive prediction for downstream reward shaping. Both learn value functions\nduring pre-training, albeit with entirely different algorithms (contrastive learning vs. TD learning), for\ndifferent policies (dataset policy vs. intent-conditioned policy), exhibiting different generalization prop-\nerties (Baird, 1995; Dann et al., 2014). Furthermore, the system desiderata differ for VIP and V-PTR: VIP\nfocuses on learning good visual reward functions for weighted behavioral cloning, while we seek good value\nfunction initializations for downstream offline RL. In our experiments, we find that when both pre-training\napproaches are evaluated on the quality of downstream offline RL, those initialized with V-PTR improve\nover VIP in terms of generalization and robustness. While obtaining reward functions is not the focus of this\npaper, our experiments show that the V-PTR representation alone outperforms VIP with reward shaping.\nFinally, a class of methods attempt to modify the downstream RL algorithm to train on video and robot\ndata together in lieu of a video pre-training stage. For instance, (Torabi et al., 2018a; Schmeckpeper et al.,\n2020; Baker et al., 2022; Chang et al., 2022) train an inverse dynamics model to label video transitions with\naction pseudo-labels to use alongside the robotic experience; (Stadie et al., 2017; Torabi et al., 2018b) use\ninverse RL to imitate the state-distribution present in the video. These methods succeed only when a small\ndomain gap exists between video data and robot data, so that observations from video data can plausibly\nbe interpreted as robot data. This condition fails in our setup, as we utilize Ego4D (Grauman et al., 2022)\nand the Bridge (Ebert et al., 2021) datasets, where observations in these datasets differ significantly from\neach other, including a major difference in viewpoint (e.g., egocentric view in video data (Grauman et al.,\n2022) & shoulder view in robot data (Ebert et al., 2021)). To our knowledge, no method of this type has been\nsuccessfully applied to this setting.\n3. Problem Statement and Background\nWe aim to leverage Internet-scale video data and multi-task robotic data to boost the robustness and\ngeneralization of robotic offline RL. We formulate the robot skill learning problem as the problem of\nmaximizing infinite-horizon discounted reward in a Markov decision process (MDP).\nFormal problem statement. We assume access to two pre-training datasets: an Internet-scale video dataset\n\ue230video (e.g., the Ego4D dataset (Grauman et al., 2022)) and a target dataset, \ue230target of a limited number of\ndemonstrations for a given target task on the robot. Additionally we are also provided a dataset of multi-task\nrobot behaviors, \ue230robot, which may not contain any data relevant to the target task. The video dataset \ue230video\nconsists of sequences of frames (i.e., observations in the MDP), with no action or rewards. Denoting a frame as\n\ud835\udc2c\ud835\udc56,\ud835\udc57, we define \ue230video \u2236=\n{\n(\ud835\udc2c\ud835\udc56,0, \ud835\udc2c\ud835\udc56,1, \u22ef)\n}\ud835\udc5bvideo\n\ud835\udc56=1 . The target dataset, \ue230target, comprises of a few demonstrations\nof the target task on the robot \ue230target \u2236=\n{\n(\ud835\udc2c\ud835\udc56,0, \ud835\udc1a\ud835\udc56,0, \ud835\udc5f\ud835\udc56,0, \ud835\udc2c\ud835\udc56,1, \u22ef)\n}\ud835\udc5btarget\n\ud835\udc56=1 , where the reward, \ud835\udc5f\ud835\udc56,\ud835\udc57 is annotated to\nbe +1 only on the final three timesteps of the demonstration (following (Kumar et al., 2023)). The multi-task\nrobot dataset \ue230robot is organized identically to the target robot dataset, but with an additional task annotation\non each trajectory \ud835\udc61\ud835\udc56, which is specified either as a one-hot identifier or by natural language. Our goal\nis train policy \ud835\udf0b which maximizes the \ud835\udefe-discounted cumulative reward, \ud835\udd3c\ud835\udc2c0\u223c\ud835\udf0c0,\ud835\udc1a0\u2236\u221e,\ud835\udc2c1\u2236\u221e\u223c\ud835\udf0b [\u2211\u221e\n\ud835\udc61=0 \ud835\udefe\ud835\udc61\ud835\udc5f(\ud835\udc2c\ud835\udc61, \ud835\udc1a\ud835\udc61)],\nstarting from a more diverse set of initial states indicated by the distribution \ud835\udf0c0 than what was observed in\nthe target dataset (e.g., more variation in distractor objects).\nBackground. Our system utilizes a generalized formulation of goal-conditioned RL and temporal-difference\nlearning for pre-training value functions. In a nutshell, the goal-conditioned RL problem trains the agent\nto achieve arbitrary goal frames \ud835\udc20, where rewards are specified by the sparse signal of \ud835\udd40 (\ud835\udc2c = \ud835\udc20) when\nthe frame is identical to the goal frame. Although the reward signal is sparse, goals and rewards can be\ndefined by hindsight relabelling (Andrychowicz et al., 2017). To learn a policy for the downstream task, we\nuse value-based offline RL methods, which optimize \ud835\udf0b against a learned Q-function \ud835\udc44\ud835\udf0b(\ud835\udc2c, \ud835\udc1a). The Q-value\nfunction measures the expected long-term reward attained when executing action \ud835\udc1a at state \ud835\udc2c, then following\npolicy \ud835\udf0b thereafter, and satisfies the Bellman equation \ud835\udc44\ud835\udf0b(\ud835\udc2c, \ud835\udc1a) = \ud835\udc5f(\ud835\udc2c, \ud835\udc1a) + \ud835\udefe\ud835\udd3c\ud835\udc2c\u2032,\ud835\udc1a\u2032[\ud835\udc44\ud835\udf0b(\ud835\udc2c\u2032, \ud835\udc1a\u2032)].\n4. Video Pre-Training for Robotic Offline RL\nEven though video data contains rich functional and dynamic information useful for downstream skill\nlearning, this data cannot directly be integrated into robotic offline RL pipelines, which expect data annotated\n3\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nVideo Data\nRobot Data\nVisual\nEncoder \nVisual\nEncoder \nTask\nAction\nActor\nQ Value \nICVF Value\nFigure 2: Network architecture. V-PTR first pre-trains image representations by training a general value function from video\nand then refines this representation via multi-task pre-training on robot data.\nwith the agent\u2019s actions and the reward signal. In this section, we develop V-PTR, our system that pre-trains\ngeneral value functions on Internet-scale video data, with the intention of fine-tuning value functions for the\ndesired downstream robotic task using value-based offline RL. We first present an overview of our system\nnext, and then discuss each of the components in detail subsequently.\nSystem overview. Our system, V-PTR, pre-trains in two phases: first on video data, and then on multi-task\nrobot data. In the first phase, we train an intent-conditioned value function (Ghosh et al., 2023) on action-less\nvideo data using a value-learning objective to model the outcomes associated with solving the parametric\nfamily of goal-achieving tasks. Next, we refine this representation with multi-task robot data with actions\nand rewards, by training a state-action Q-function on this representation using offline RL. We expect the\nvideo and multi-task robot data to capture dynamic features in the environment, with the robot data bridging\nthe domain gap between human video and the robot, and aligning the learned representation with the\nagent\u2019s action space. Downstream, to adapt the system to a new target task, in our final phase, our system\nfine-tunes the Q-function and the policy on the target dataset.\n4.1. Phase 1: Video Pre-Training via TD-Learning\nSince the goal of video pre-training is to improve the performance of downstream value-based offline RL, we\nturn to learning value functions on the video data as a natural pre-training procedure. We choose to pre-train\nby learning an intent-conditioned value function (ICVF), a recently-proposed general value function that\ncan be efficiently trained on passive data without action labels (Ghosh et al., 2023). An ICVF, annotated\n\ud835\udc49(\ud835\udc2cvideo, \ud835\udc20video, \ud835\udc33) computes the value obtained towards reaching a goal \ud835\udc20video, assuming the policy intended\nto reach a different intended goal \ud835\udc33, and is formally defined as\n\ud835\udc49(\ud835\udc2cvideo, \ud835\udc20video, \ud835\udc33) = \ud835\udd3c\ud835\udc4e\ud835\udc61\u223c\ud835\udf0b\u2217\ud835\udc33 (\u22c5|\ud835\udc2c\ud835\udc61)[ \u2211\n\ud835\udc61\n\ud835\udefe\ud835\udc61\ud835\udd40 (\ud835\udc2cvideo = \ud835\udc20video) ].\nAs with a standard value function, the ICVF can be learned by temporal-difference (TD) learning on its\ncorresponding Bellman equation, using a target network to bootstrap the predictions of the learned value\nfunction. Specifically, for a given tuple (\ud835\udc2c, \ud835\udc20, \ud835\udc33) \u223c \ue230video, this TD objective function is given by:\nmin [ (\ud835\udefc \u2212 \ud835\udd40 (\ud835\udc34 \u2264 0)) \u22c5 (\ud835\udd40 (\ud835\udc2cvideo = \ud835\udc20video) + \ud835\udefe \u0304\ud835\udc49(\ud835\udc2c\u2032\nvideo, \ud835\udc20video, \ud835\udc33) \u2212 \ud835\udc49(\ud835\udc2cvideo, \ud835\udc20video, \ud835\udc33))\n2\n],\nwhere \ud835\udc34 = \ud835\udd40 (\ud835\udc2cvideo = \ud835\udc20video) + \ud835\udefe \u0304\ud835\udc49(\ud835\udc2c\u2032\nvideo, \ud835\udc33, \ud835\udc33) \u2212 \ud835\udc49(\ud835\udc2cvideo, \ud835\udc33, \ud835\udc33) is the implied advantage of \ud835\udc2c \u2192 \ud835\udc2c\u2032 when acting\nto reach the observation \ud835\udc33 and \u0304\ud835\udc49 is a delayed copy of the value function (i.e., a target network). We follow\nthe goal-sampling strategy from ICVF: after sampling a start frame \ud835\udc2c from the video dataset, we choose the\ngoal \ud835\udc20 to be either the next observation, a future observation in the video, or a random observation from a\ndifferent video. The intent \ud835\udc33 is also an observation appearing in the video, and is chosen in a similar fashion\nas the goal state. Additionally, following Ghosh et al. (2023), with some probability we set \ud835\udc33 = \ud835\udc20.\n4\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nWe follow Ghosh et al. (2023) and parameterize our estimated value function as\n\ud835\udc49(\ud835\udc2cvideo, \ud835\udc20video, \ud835\udc33) \u2236= \ud835\udf19(\ud835\udc2cvideo)\u22a4\ud835\udc47(\ud835\udc33)\ud835\udf13(\ud835\udc20video),\nwhere \ud835\udf19\ud835\udf03 and \ud835\udf13\ud835\udefc denote models that transform the observation and the goal observation respectively\ninto low-dimensional representations, and \ud835\udc47\ud835\udefd, a learned mapping aligning the two representations. At\nconvergence, the ICVF provides a measure of temporal spatiality, and the learned representation \ud835\udf19\ud835\udf03(\ud835\udc2c) offers\na useful feature basis for downstream value functions.\n4.2. Phase 2: Multi-Task Robot Pre-Training via Offline RL\nIn the next phase, we refine the learned representation on a multi-task robot dataset, \ue230robot, to narrow the\ndomain gap between robot image observations and human video, and to provide information about the\ntarget robot embodiment (i.e., the actions affordable by the robot). The purpose of this phase is to provide\ninformation about the target robot embodiment: crucially note that the tasks and workspaces in this robot\ndataset are explicitly disjoint from the target tasks used in the downstream evaluation.\nV-PTR uses multi-task robot data to pre-train a Q-function and a policy using multi-task conservative\nQ-learning (CQL) (Kumar et al., 2023), initializing the parameters of both the Q-function and the policy\nusing the backbone learned during video pre-training in Phase 1. Concretely, the Q-function and the policy\nare conditioned on the pre-trained representation of the robot observation \ud835\udf19\ud835\udf03(\ud835\udc2crobot) alongside a task vector\n\ud835\udc61 (either a one-hot task identifier or a language embedding from a sentence transformer). At the onset of\nthis phase, we initialize the representation encoder \ud835\udf19\ud835\udf03 to the encoder \ud835\udf19\ud835\udf03\u2217\nvideo obtained at the end of phase 1.\nThe value function is trained to satisfy the Bellman equation,\nmin\n\ud835\udf03\n\ud835\udefc \u22c5 \ue238CQL(\ud835\udf03) + \ud835\udd3c\ue230robot [(\ud835\udc44\ud835\udf03(\ud835\udc2c, \ud835\udc1a; \ud835\udc61) \u2212 \ud835\udc5f \u2212 \ud835\udefe \u0304\ud835\udc44(\ud835\udc2c\u2032, \ud835\udc1a\u2032, \ud835\udc61))\n2\n] ,\nwith target Q-network \u0304\ud835\udc44, and CQL regularizer \ue238CQL(\ud835\udf03) (Kumar et al., 2020), and the policy trained to\nmaximize value,\nmax\n\ud835\udf06\n\ud835\udd3c\ue230robot [\ud835\udd3c\ud835\udc1a\u223c\ud835\udf0b\ud835\udf06(\u22c5|\ud835\udc2c;\ud835\udc61)[\ud835\udc44(\ud835\udc2c, \ud835\udc1a; \ud835\udc61)]] + \ud835\udefd\ue234(\ud835\udf0b\ud835\udf06).\nAfter pre-training, we have a multi-task Q-function and policy that can be fine-tuned to the desired\ndownstream task using a small target dataset.\n4.3. Phase 3: Fine-Tuning to a Target Task\nFinally, we fine-tune the value function and policy from the pre-training stage to the target task by running\nCQL (Kumar et al., 2020) on the target dataset \ue230target. We follow Kumar et al. (2023) and treat the target\ndata simply as a new task; fine-tuning involves assigning a new task identifier to the target data (either\na new one-hot vector or a new language command), and continuing to run multi-task offline CQL on the\nrobot pre-training and target tasks jointly. To address any overfitting induced by the small size of \ue230target,\nwe perform stratified sampling between the multi-task robot data \ue230robot and the target data \ue230target: 1 \u2212 \ud835\udf0f\nproportion of the training batch comes from \ue230robot and \ud835\udf0f from \ue230target, where \ud835\udf0f is small (we use \ud835\udf0f = 0.1).\n4.4. Implementation Details\nVideo pre-training: We use video frames at a 224\u00d7224 resolution. The three components parameterizing\nthe ICVF are implemented as separate 3-layer MLP heads on a shared visual backbone encoder. We use a\nResnetv2-50 (He et al., 2016a) as our backbone since smaller convolutional networks led to worse pre-training\nperformance. We replace all batch normalization layers with group normalization (Wu and He, 2018), since\nprior works (Bhatt et al., 2019; Kumar et al., 2023) often found batch normalization to be unstable. To avoid\noverfitting to spurious correlations between consecutive frames in a video clip, we use image augmentation\n(random crops and color jitter) (Kostrikov et al., 2020), weight decay, and dropout. We train the model for\n2 \u00d7 106 gradient steps with a batch size of 64, using Adam, and learning rate 10\u22124 cosine annealed through\ntraining. All remaining designs we take from the open-source code (Ghosh et al., 2023).\n5\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nFigure 3:\nExamples of setup and successful rollouts for complex tasks. We utilize the robot setup from the Bridge\ndataset (Ebert et al., 2021) for our tasks. Top: Two-phase open microwave; Bottom: Sweep beans into pile with tool.\nMulti-task robot pre-training: We fine-tune on the multi-task robot dataset primarily following design\ndecisions from Kumar et al. (2023). The CQL policy takes the RGB image, proprioceptive state, and task\nidentifier as input. The RGB image is passed through a visual encoder, then concatenated with the remaining\ninformation, and passed through a 2-layer MLP. We additionally concatenate task and action information\ninto each hidden layer of the MLP. The CQL value function is parameterized similarly, but it also takes\nthe action vector as input. Encoder parameters for both the value and policy are initialized from the video\npre-trained ICVF, but there is no further weight tying between the networks. We train multi-task CQL for\n2 \u00d7 105 gradient steps with batch size of 64, and Adam with a constant learning rate of 10\u22124.\n5. Experimental Results\nThe goal of our experiments is to validate the effectiveness of V-PTR in boosting the generalization and\nrobustness of robotic offline RL. We evaluate V-PTR in several scenarios requiring generalization to new\nscenes, compare to other approaches for incorporating video data, and perform additional diagnostic\nexperiments to understand how value pre-training can provide useful representations for downstream\nrobotic RL. Our settings include several challenging robotic manipulation tasks that require zero-shot\ngeneralization to new objects and distractors. A video of our evaluations and diagnostic experiments can be\nfound on our project website.\nReal-world setup. We conduct our experiments on a WidowX robot platform. We perform video pre-\ntraining on Ego4D (Grauman et al., 2022), an egocentric video dataset consisting of 4M transitions of humans\nattempting diverse tasks in the real world, using the same pre-processed subset from prior works (Nair\net al., 2022b; Ma et al., 2022). Then, for multi-task robot pre-training, we utilize the subset of the Bridge\ndataset (Ebert et al., 2021; Walke et al., 2023) used by prior work (Kumar et al., 2023), a dataset with 150K\ntransitions of various manipulation task demonstrations on a WidowX robot in toy kitchens. Downstream,\nwe fine-tune on several tasks on a WidowX robot, in a previously unseen toy-kitchen workspace. For each\ntarget task, we collect 10 demonstrations using teleoperation, with a range of distractor objects and object\nvariability. Solving these tasks requires skills such as picking and placing a variety of objects, using tools to\naccomplish tasks (e.g., sweeping), and two-phase door opening (Figures 3 and 4).\nComparisons to prior methods. We compare V-PTR to approaches that do not utilize video data (PTR (Ku-\nmar et al., 2023), BC (Ebert et al., 2021)), as well as other methods for video pre-training (R3M (Nair et al.,\n2022b), MVP (Xiao et al., 2022b; Radosavovic et al., 2022), and VIP (Ma et al., 2022)). Following the protocols\nin these prior works, we fine-tune the R3M and MVP representations with imitation learning on multi-task\nand target robot data (phases 2 and 3), and evaluate the VIP representation both with reward-weighted\nimitation learning and with downstream offline RL via CQL, to provide an apples-to-apples comparison\nwith V-PTR. We evaluate three versions of VIP: (i) \u201cVIPfrozen\u201d, which freezes the pre-trained representation\nlearned from video data during fine-tuning, (ii) \u201cVIP\u201d, which continues to fine-tune on robot data, and (iii)\n\u201cVIPreward\u201d, which not only utilizes the pre-trained representation for initializing the policy but also uses\ndistances between representations of current and future frames as a reward shaping for downstream offline\nRL via reward-weighted imitation, following (Ma et al., 2022). When using language task specification, we\ncompare to language conditioned variants of BC and PTR.\n6\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nVIP making an imprecise grasp\nV-PTR re-orienting the knife to pick it up\nV-PTR retrying after dropping the croissant\nV-PTR\nV-PTR\nVIP\nFigure 4: Visualizing qualitative performance of V-PTR and VIP. Here we show rollouts for V-PTR (top) and VIP (bottom)\non the real robot manipulation tasks. V-PTR carefully executes the task by orienting the gripper to match the object and retrying\non failure whereas VIP grasp objects without this re-orientation, leading to failure.\n5.1. Real-World Results\nWe evaluate V-PTR and comparisons in three testing scenarios. In Scenario 1, we evaluate the performance\nof the policy, varying the robot\u2019s initial pose and the position of objects in scene. In Scenario 2, we evaluate\nthe policy in the presence of novel distractor objects in the scene. We note that some of the target data\ndoes contain distractor objects, but we use a different set of distractor objects during evaluation than those\nseen in training. Finally, in Scenario 3, we test the ability of the learned policy to manipulate novel target\nobjects that were never seen during training.\nWe evaluate different methods on four pick-and-place tasks in Table 1. Observe that V-PTR outperforms\nall other prior approaches, and in some tasks (e.g., \u201cplace knife in pot\u201d) is the only method that produces\nany successful trials. We observed that all of these methods do learn behavior that attempt solve the task,\nfor example by moving toward relevant objects in the scene, but do not eventually succeed due to either\nimprecise localization of target objects or a prematurely executed grasp attempt. On the other hand, we\nfound that in several scenarios V-PTR is able to re-orient the gripper before executing a grasp (e.g., Fig. 4).\nNext, we evaluate V-PTR with distractor objects in Table 1 (Scenario 2). Adding distractors reduces perfor-\nmance for every method (as one may expect), but V-PTR still exhibits the smallest degradation compared\nto the next best approach. To study generalization to novel target objects (Scenario 3), we also consider\na \u201ctake [object] from bowl\u201d task, and replace the target object with unseen target objects at evaluation.\nPerformance is low for all comparisons in Table 1, but V-PTR succeeds 50% of the time with new objects.\nComparisons to VIP on more complex tasks. We compare V-PTR in more detail to VIP (which also uses\nsimilar video data) on manipulation tasks that require learning more complex skills: \u201copen microwave\u201d and\n\u201csweep beans\u201d in Table 2. Specifically, we compare to different variants of VIP discussed above (VIP+CQL;\nVIPreward) and find that V-PTR outperforms these variants. Qualitatively in Figure 5, we observe that on the\n\u201csweep beans\u201d task, V-PTR sweeps a larger area than the VIP policy, which is too slow to execute the sweep\nmotion a second time. This corroborates our analysis (Figure 6 and 7) that value functions trained on the\nV-PTR representation tend to have lower error than those utilizing VIP representations.\n7\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nVideo pre-training\nNo videos\nNo robot data\nTask\nV-PTR (Ours) R3M+BC MVP+BC VIP+CQL VIPfrozen+CQL\nPTR\nV-PTR w/o phase 2\nScenario 1\nCroissant from bowl\n7 / 12\n0 / 12\n4 / 12\n2 / 12\n0 / 12\n3 / 12\n5 / 12\nSweet potato on plate\n6 / 12\n0 / 12\n1 / 12\n0 / 12\n0 / 12\n1 / 12\n1 / 12\nKnife in pot\n6 / 12\n0 / 12\n0 / 12\n0 / 12\n0 / 12\n0 / 12\n0 / 12\nCucumber in pot\n5 / 12\n0 / 12\n1 / 12\n0 / 12\n0 / 12\n1 / 12\n1 / 12\nTotal\n24 / 48\n0 / 48\n6 / 48\n2 / 48\n0 / 48\n5 / 48\n7 / 48\nScenario 2\nwith\ndistractor\nobjects\nCroissant from bowl\n8 / 12\n0 / 12\n3 / 12\n2 / 12\n0 / 12\n0 / 12\n3 / 12\nSweet potato on plate\n4 / 12\n0 / 12\n2 / 12\n0 / 12\n0 / 12\n1 / 12\n2 / 12\nKnife in pot\n4 / 12\n0 / 12\n0 / 12\n1 / 12\n0 / 12\n0 / 12\n0 / 12\nCucumber in pot\n4 / 12\n0 / 12\n0 / 12\n1 / 12\n0 / 12\n0 / 12\n1 / 12\nTotal\n20 / 48\n0 / 48\n5 / 48\n4 / 48\n0 / 48\n1 / 48\n6 / 48\nScenario 3\nnovel\ntarget\nobjects\nCarrot\n2 / 3\n0 / 3\n0 / 3\n1 / 3\n0 / 3\n0 / 3\n2 / 3\nCucumber\n1 / 3\n0 / 3\n0 / 3\n1 / 3\n0 / 3\n0 / 3\n1 / 3\nIce-cream\n0 / 3\n0 / 3\n0 / 3\n1 / 3\n1 / 3\n1 / 3\n0 / 3\nTotal\n3 / 9\n0 / 9\n0 / 9\n3 / 9\n1 / 9\n1 / 9\n3 / 9\nTable 1: Task success rates of V-PTR and prior methods on several manipulation tasks over 12 trials (best-performing method\nindicated in red). Note that V-PTR outperforms all prior methods, including those approaches that do not fine-tune the learned\nrepresentation, use imitation learning for downstream control, or do not incorporate video data.\nNo CQL\nTask\nV-PTR VIP (Ma et al., 2022)+CQL PTR (Kumar et al., 2023) VIPreward (Ma et al., 2022)\nOpen Microwave\n5 / 12\n2 / 12\n0 / 12\n0 / 12\nSweep Beans\n6 / 12\n5 / 12\n2 / 12\n2 / 12\nTable 2: Performance of V-PTR, VIP, and PTR on more complex tasks. V-PTR outperforms PTR as well as VIP variants that\nuse downstream CQL or BC weighted by the reward shaping from Ma et al. (2022).\nLanguage-based task specification. We next study how V-PTR works when the robot pre-training data is\nlabeled with natural language descriptions (a more general format) instead of task identifiers. To handle\nlanguage, we first encode the task description into an embedding vector using the pre-trained language\nencoder from GRIF (Myers et al., 2023), which has been shown to be effective at learning BC policies.\nThe Q-function and the policy then utilize these embedding vectors in lieu of the one-hot task identifiers,\nprocessing them identically as before. In Table 3, we compare V-PTR to imitation learning and PTR with\nlanguage embeddings, and find that V-PTR improves over both of these methods by around 50%, indicating\nthat V-PTR can leverage downstream language.\n5.2. Visualizations and Diagnostic Experiments\nWe now analyze V-PTR more carefully by visualizing the learned features from video-pretraining, probing\nthe generalization of the system, and assessing the quality of value estimation in downstream offline RL.\nVideo pre-training via V-PTR improves target value estimation. We visualize the learned value\nfunction on frames from different rollouts in Figure 6 (left), where the true value function should mono-\nFigure 5: Examples of areas swept by VIP (Ma et al., 2022) (top) and V-PTR (bottom) methods. V-PTR sweeps a much larger\narea (blue), and consistently begins a second sweep, whereas VIP (Ma et al., 2022) is too slow to sweep a second time.\n8\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nNo language\nTask\nV-PTR (language)\nBC\nPTR\nV-PTR (one-hot)\nCroissant\n7 / 12\n3 / 12\n5 / 12\n7 / 12\nSweet potato\n6 / 12\n2 / 12\n6 / 12\n6 / 12\nKnife in Pan\n5 / 12\n0 / 12\n3 / 12\n6 / 12\nCucumber in Pot\n9 / 12\n0 / 12\n3 / 12\n5 / 12\nOpen Microwave\n9 / 12\n1 / 12\n0 / 12\n5 / 12\nSweep Beans\n8 / 12\n2 / 12\n4 / 12\n6 / 12\nTotal\n44 / 72\n8 / 72 21 / 72\n35 / 72\nTable 3: Performance of language-conditioned V-PTR compared to language-conditioned BC and PTR for the six\nmanipulation tasks that we study. Note that V-PTR outperforms both of these prior methods, indicating that V-PTR can favorably\nlearn in settings with alternate forms of task specification.\nNo\u00a0Distractors\nV\u00adPTR\nPTR\nVIP\nW/\u00a0Distractors\nTrajectory\u00a0timestep\n5e5\nTraining\u00a0Steps\nValue\u00a0Estimation\u00a0Error\nV\u00adPTR\nPTR\nVIP\nDataset\nFigure 6: Visualizing the learned values \ud835\udc49(\ud835\udc2c\ud835\udc61) w.r.t. time-step \ud835\udc61 on rollouts from training data (top), held-out data (middle),\nand rollouts with distractor objects (bottom) obtained after multi-task pre-training in phase 2 (\ud835\udc49(\ud835\udc2c\ud835\udc61) is computed using the average\nof the multi-task Q-value under actions sampled from the learned policy). Note that values trained by PTR and VIP tend to be\nhighly non-smooth, especially on held-out rollouts with novel distractors, whereas V-PTR produces smooth value functions.\ntonically increase from initial states (\ud835\udc61 = 0) to the final states (\ud835\udc61 = 30) of a successful rollout. We visualize\nthe downstream value \ud835\udc49(\ud835\udc2c\ud835\udc61) for two kinds of rollouts: (i) rollouts from training data and (ii) rollouts with\ndistractor objects. While all three visualized methods \u2013 PTR, VIP, and V-PTR\u2013 are able to learn smooth\nvalue functions on the data without distractor objects, in the presence of distractors, the value functions\ntrained by PTR and VIP are non-smooth and non-monotonic (bottom). In contrast, V-PTR representations\nare able to induce higher-quality downstream value functions.\nNext, we more precisely measure the ability of V-PTR to fit downstream value functions. We train a SARSA\nvalue function (for which we may compute a closed-form optimal solution) on top of frozen pre-trained\nrepresentations from PTR (no video data), VIP (Ma et al., 2022), and V-PTR, and report the error between\nthe predicted value estimate and the ground-truth value on a held-out dataset in Figure 6 (right). V-PTR\nattains the smallest fitting error compared to both PTR and VIP.\nWhat kind of visual cues do representations trained by V-PTR capture? We probe which parts\nof the image influence the output of the learned policy for V-PTR and other baselines, by utilizing Grad-\nCAM (Selvaraju et al.) to mark patches of the frame that influence the output of the learned policy in\ngreen in Figure 7. We observe that V-PTR policies discard the scene background and focus on cues relevant\nfor robot control (e.g., object, gripper positions), while PTR and VIP place higher focuses on the scene\nbackground. This evidence provides an explanation for why V-PTR robustly attains better performance in\nour experiments.\nSimulation results. We also evaluate V-PTR in a simulated pick-and-place task (Kumar et al., 2021; 2023)\nwhere generalization to new objects can be measured more precisely. The task is to place a target object into\na container, in the presence of a distractor object. Like our real-world experiments, we pre-train on video\ndata from Ego4D, and use simulated robot demonstrations for multi-task pre-training and task fine-tuning.\nWe evaluate V-PTR under multiple conditions that require policies to behave robustly in the presence of\n9\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nV\u00adPTR\nPTR\nVIP\nFigure 7: Grad-CAM visuals superimposed on frames from robot data. Regions highlighted in green denote patches of\nthe observation with the most significant influence on the learned policy. Without PTR and VIP, background areas in the image\nexert a significant influence on the output of the learned policy. In contrast, initializing the policy with the video pre-trained\nrepresentation obtained from V-PTR is able to focus more on gripper and object positions, crucial for solving the task.\n% Improvement in success rate\nFigure 8: Simulation results. Percentage improvement in success rates of V-PTR over PTR (Kumar et al., 2023), that does not\nutilize any video data for pre-training. y-axis is in log scale. In all but one scenario, V-PTR improves performance.\ndistractor objects and generalize to novel target objects and workspaces. In Figure 8, we present our results\nin terms of the percentage improvement in success rates obtained by V-PTR over those of PTR (Kumar et al.,\n2023) (with no video data). Consistently, across all but one scenario with a novel target container, V-PTR\nimproves over PTR, demonstrating that value-based video pre-training on Ego4D boosts performance and\nrobustness of robotic RL.\n6. Discussion and Conclusion\nIn this paper, we designed a robotic system, V-PTR, that uses value function pre-training on the large-scale\nEgo4D video dataset (Grauman et al., 2022) and the robot Bridge dataset (Ebert et al., 2021) to improve\nthe performance of policies learned downstream. While V-PTR outperforms prior methods for learning\nfrom video data, we found that all the current methods remain sensitive to deviations in workspace height,\ncamera angle, and robot configurations. There also exist many opportunities to scale, whether incorporating\nmulti-robot datasets, larger human video datasets with language, or larger models. Nevertheless, our\nevaluations and diagnostic experiments indicate the promise of using RL-like value pre-training on video\ndata for improving the general ability of robot learning algorithms.\nAcknowledgements\nWe thank Jason Ma, Karl Schmeckpeper, Homer Walke, Mitsuhiko Nakamoto, and Ria Doshi for their help\nwith setting up baseline methods and for debugging the robot setup. We thank Katie Kang, Colin Li, Oleg\n10\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nRybkin, Ajay Sridhar, and all other members of the RAIL lab at UC Berkeley for their feedback on an earlier\nversion of this paper and for informative discussions. This work is supported by an NSF graduate fellowship\nand Office of Naval Research (N00014-21-1-2838). We also thank the TRC programs from Google cloud for\nproviding us with TPU resources that were critical for enabling this research.\nReferences\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew,\nJosh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in\nneural information processing systems, 30, 2017. 3\nShikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from human videos\nas a versatile representation for robotics. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13778\u201313790, 2023. 2\nLeemon Baird. Residual Algorithms : Reinforcement Learning with Function Approximation. In International\nConference on Machine Learning (ICML), 1995. 3\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton,\nRaul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online\nvideos. ArXiv, abs/2206.11795, 2022. 3\nAditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox. Crossnorm: Normalization for off-policy\ntd reinforcement learning. arXiv preprint arXiv:1902.05605, 2019. 5\nMatthew Chang, Arjun Gupta, and Saurabh Gupta. Learning value functions from undirected state-only\nexperience. ArXiv, abs/2204.12458, 2022. 3\nChristoph Dann, Gerhard Neumann, Jan Peters, et al. Policy evaluation with temporal differences: A survey\nand comparison. Journal of Machine Learning Research, 15:809\u2013883, 2014. 3\nFrederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis,\nChelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain\ndatasets. arXiv preprint arXiv:2109.13396, 2021. 2, 3, 6, 10, 14, 15\nDibya Ghosh, Chethan Bhateja, and Sergey Levine. Reinforcement learning from passive data via latent\nintentions. arXiv preprint arXiv:2304.04782, 2023. 2, 4, 5, 14, 15\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar,\nJackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of\negocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 18995\u201319012, 2022. 2, 3, 6, 10, 15\nK He, X Chen, S Xie, Y Li, P Doll\u00e1r, and RB Girshick. Masked autoencoders are scalable vision learners.\narxiv. 2021 doi: 10.48550. arXiv preprint arXiv.2111.06377, 2021. 2\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In\nEuropean Conference on Computer Vision, 2016a. 5\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016b. 17\nSiddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy\nLiang. Language-driven representation learning for robotics. arXiv preprint arXiv:2302.12766, 2023. 2\nIlya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep\nreinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. 5\n11\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline rein-\nforcement learning. arXiv preprint arXiv:2006.04779, 2020. 5, 14, 16, 17\nAviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline\nmodel-free robotic reinforcement learning. In 5th Annual Conference on Robot Learning, 2021. URL\nhttps://openreview.net/forum?id=fy4ZBWxYbIo. 9\nAviral Kumar, Anikait Singh, Frederik Ebert, Yanlai Yang, Chelsea Finn, and Sergey Levine. Pre-training for\nrobots: Offline rl enables learning new tasks from a handful of trials. RSS 2023; arXiv:2210.05178, 2023. 2,\n3, 5, 6, 8, 9, 10, 14, 15, 16, 17\nYecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang.\nVip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint\narXiv:2210.00030, 2022. 2, 3, 6, 8, 9, 15, 17\nVivek Myers, Andre He, Kuan Fang, Homer Walke, Philippe Hansen-Estruch, Ching-An Cheng, Mihai\nJalobeanu, Andrey Kolobov, Anca Dragan, and Sergey Levine. Goal representations for instruction\nfollowing: A semi-supervised language interface to control. arXiv preprint arXiv:2307.00117, 2023. 8\nA. Nair, S. Bahl, A. Khazatsky, V. Pong, G. Berseth, and S. Levine. Contextual imagined goals for self-\nsupervised robotic learning. In Conference on Robot Learning (CoRL), 2019. 2\nSuraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhi Gupta. R3m: A universal visual\nrepresentation for robot manipulation. ArXiv, abs/2203.12601, 2022a. 2, 15, 17\nSuraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual\nrepresentation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022b. 2, 6\nIlija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world\nrobot learning with masked visual pre-training. arXiv preprint arXiv:2210.03109, 2022. 2, 6, 16\nKarl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. Reinforcement\nlearning with videos: Combining offline observations with interaction. In Conference on Robot Learning,\n2020. 3\nRR Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, and D Batra. Grad-cam: Visual explanations from\ndeep networks via gradient-based localization. arxiv 2016. arXiv preprint arXiv:1610.02391. 9\nYounggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and P. Abbeel. Masked\nworld models for visual control. ArXiv, abs/2206.14244, 2022a. 2\nYounggyo Seo, Kimin Lee, Stephen James, and P. Abbeel. Reinforcement learning with action-free pre-\ntraining from videos. ArXiv, abs/2203.13880, 2022b. 2\nPierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine.\nTime-contrastive networks: Self-supervised learning from video. 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pages 1134\u20131141, 2017. 2\nAvi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Cog: Connecting new\nskills to past experience with offline reinforcement learning. arXiv preprint arXiv:2010.14500, 2020. 14\nA. Srinivas, Michael Laskin, and P. Abbeel. Curl: Contrastive unsupervised representations for reinforcement\nlearning. In International Conference on Machine Learning, 2020. 2\nBradly C. Stadie, P. Abbeel, and Ilya Sutskever. Third-person imitation learning. ArXiv, abs/1703.01703,\n2017. 3\nFaraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. ArXiv, abs/1805.01954,\n2018a. 3\n12\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nFaraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. ArXiv,\nabs/1807.06158, 2018b. 3\nHomer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe\nHansen-Estruch, Quan Vuong, Andre He, et al. Bridgedata v2: A dataset for robot learning at scale. arXiv\npreprint arXiv:2308.12952, 2023. 6\nYuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer\nvision (ECCV), pages 3\u201319, 2018. 5\nTete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor\ncontrol. ArXiv, abs/2203.06173, 2022a. 2, 16\nTete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor\ncontrol. arXiv preprint arXiv:2203.06173, 2022b. 2, 6\n13\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nAppendices\nA. Full System Description: V-PTR\nIn this section, we will provide further details regarding the 3 phases of V-PTR\u0130n particular, we will describe\nthe video pre-training with ICVF (Ghosh et al., 2023) as well as Multi-robot pretraining with PTR (Kumar\net al., 2023), describing both the networks present during training as well as the objective function.\nA.1. Video Pre-Training with ICVF\nNetwork. We define the visual backbone \ud835\udc53\ud835\udf03(\ud835\udc2c) to be a ResNet50-v2 model, which outputs a 2048-dimensional\nvector. The ICVF heads \ud835\udf19, \ud835\udf13, \ud835\udc47 are 2-layer MLPs with hidden dimensions of 256 and a final dimension of 256.\nThe ICVF is then defined as \ud835\udc49(\ud835\udc2c, \ud835\udc20, \ud835\udc33) = \ud835\udf19(\ud835\udc53 (\ud835\udc2c))\u22a4\ud835\udc47(\ud835\udc53 (\ud835\udc33))\ud835\udf13(\ud835\udc53 (\ud835\udc20)) for video frames \ud835\udc2c, \ud835\udc20, \ud835\udc33.\nA.2. Multi-robot pretraining with PTR\nNetworks. We mirror the experimental setup of (Kumar et al., 2023), with no modifications except for a\ndifferent visual backbone. PTR uses CQL (Kumar et al., 2020) as the base offline RL method, meaning it\ntrains two Q functions, a separate policy, and delayed target copies of the Q-functions. Each network is\nrepresented by a 3-layer MLP head with width of 256, after the visual representation. To make comparisons\nbetween PTR and V-PTR exact, we also use separate encoders for the actor and critic, although both are\ninitialized at the beginning of Phase 2 using the same visual representation as learned during Phase 1. We\nrefer to (Kumar et al., 2023) for a more complete description.\nB. Environment Setup and Dataset Details\nIn this section, we will describe the setup for our experimental results with respect to both the real-world\nexperiments and the sim diagnostic experiments. There will be a description of the task setup as well as the\ncorresponding datasets associated with the tasks.\nB.1. Description of State and Action Space\nOur state and action description follows that of PTR (Kumar et al., 2023). The state is a 128\u00d7128 RGB image\ncaptured from an over-the-shoulder camera, a one-hot vector identifying which task is to be performed, the\nposition and rotation of the end-effector, and how much the gripper is closed. The action is the translational\nand angular velocity of the robot end-effector, as well as how much the gripper is closed. Rotations and\nangular velocities are expressed using Euler angles.\nB.2. Description of Real Robotic Setup\nWe mirror our real-world experimental setup from Bridge (Ebert et al., 2021) and PTR (Kumar et al., 2023).\nIn particular, we designed and evaluated the performance of our method under several distinct conditions\nin 2 different toy kitchen domains. The robot that is used is a 6-DoF WidowX 250 robot with a fixed side\ncamera. The scene in which the robot was placed in was a toy kitchen with elements such as stove tops,\nsinks, microwaves, and food objects found in the scene. A picture of our setup is found in Figure 9.\nWe evaluate several pick place tasks, in which the agent has to place an object into a container amongst\ndistractors found in the scene. Distractor objects in this scene can include other objects and containers that\nmay have even been shown to be corresponding to different tasks.\nB.3. Description of our Simulation Task\nOur manipulation tasks in simulation are modified versions of the pick-and-place tasks from Singh et al.\n(2020), which were then extended by Kumar et al. (2023). This environment requires controlling a simulated\nWidowX 250 arm which is placed in front of a single bin with an object to place in the bin and a distractor.\nThese objects are selected from a pool of 10 objects, with no overlap between the objects to be placed in the\nbin and the distractors. The goal of the task is to pick up the object and place it in the bin successfully. The\nsetup is designed in the PyBullet simulation framework which models the physics of the robotic arm, the\nobjects in the scene, as well as the interactions between them. A picture of our setup is found in Figure 10.\n14\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nFigure 9: Real-robot experiments. We utilize the setup from the Bridge dataset (Ebert et al., 2021). The bridge dataset\nis collected on a 6-DoF WidowX 250 robot, with a fixed side camera placed in diverse toy-kitchens. Observations for\nthe tasks consist of one 128 \u00d7 128 RGB image from a side camera, as well as robot proprioception. Left: task objects\nand containers for the tasks that we study. Right: Evaluation setup pick place environment.\nFigure 10: Observations for the simulated pick-and-place tasks consist of one 128 \u00d7 128 RGB image from a top-down camera,\nas well as a proprioceptive robot state.\nThe dataset is collected with a scripted policy that reaches for the object of interest, grasps it with some\nlikelihood, and places it in the bin. This collected dataset had an overall success rate of 30%.\nB.4. Description of Video Pre-Training Dataset\nThe Video Pre-training Dataset that we utilize in Stage 1 of our method, PTR, is Ego4D (Grauman et al.,\n2022). We used the same pre-processed Ego4D dataset as in R3M (Nair et al., 2022a) and VIP (Ma et al., 2022).\nIn particular, long videos that are raw in nature are clipped to be shorter consisting of 10-150 frames. From\nhere, the clipped video is decomposed into individual frames that are pre-processed with a random crop at\nthe video level. The frame is then resized and center-cropped to be of size 224 \u00d7 224 \u00d7 3. These processed\nvideo frames are then fed into the replay buffer to be individual transitions for the ICVF objective (Ghosh\net al., 2023).\nB.5. Description of Real-World Multi-Task Robot Datasets\nFor the pre-training and target datasets in Stage 2 of V-PTR for the real-world multi-task training, we utilize\nsubsets of the Bridge Dataset (Ebert et al., 2021). Mirroring the setup of Scenario 3 in PTR (Kumar et al.,\n2023), the pre-training data comprises of all pick-and-place data found in the bridge dataset except for any\ndemonstration data collected in the toy kitchen that was evaluated on. For the target dataset, we collect\n44 new demonstrations for each of the 4 tasks: Removing Croissant from Colander, Placing Knife in Pot,\n15\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nPlacing Sweet Potato on Plate, and Placing Cucumber in Bowl. A total of 218 successful demonstrations\nwere collected with a human demonstrator using an Oculus Quest Headset for Phase 3 of V-PTR.\nC. Evaluation Details\nIn this section, we will provide how real and simulated environments were evaluated fairly across our\nmethod and baselines. This protocol is similar in nature to the one presented in PTR (Kumar et al., 2023).\nC.1. Evaluation Protocol for Real-World Experiments\nTo evaluate a policy in the real world, we loop through 4 starting gripper transformations that move the\ngripper to the left front, right front, left rear, and right rear of the kitchen environment. For each of these\nstarting transformations, we evaluate a total of 3 times: once with the object to pick up directly underneath\nthe gripper and twice with it shifted slightly so that the policy cannot simply grip from the starting location.\nFor each of these evaluations, we let the policy run for 60 seconds.\nFor our experiments testing generalization to novel distractor objects, we place two novel distractor objects\ninto the scene. We shift the locations of these objects between each evaluation and switch out the objects\nthemselves when we change start transforms so that each combination of distractors is evaluated 3 times.\nFor new target object experiments, which we only conducted for the croissant out of colander task, we\nreplace the croissant with a new object never seen at training time. We switch out the target object for each\nstart transform so that each new target object is evaluated 3 times.\nWhen taking objects out of containers, we do not count knocking over the container so that the object falls\nout as a success - the gripper must lift up the object in the air and set it outside the container.\nC.2. Evaluation Protocol for Simulation Experiments\nThe multi-task robot dataset in these experiments consists of 1500 pick-place trajectories for 6 target objects.\nWe created three small target datasets to test generalization along different axes: (a) diverse target objects\nto pick and place, (b) diverse containers holding the target objects, and (c) diverse distractor objects in\nthe scene. Each of these datasets contains 10 successful demonstrations of the target task, spread across 5\ndifferent objects. We assign a unique one-hot task id for each of the source target objects and a single task\nid for the target dataset. Then, we train our system, V-PTR, and a baseline PTR policy using multi-task CQL\nwith hyperparameters \ud835\udefc = 5 and target mixing ratio \ud835\udf0f = 0.7.\nWhen evaluating zero-shot generalization, we test each of these policies along the axis of generalization\npresent in their target datasets. In particular, for the policies trained with diverse targets, diverse distractors,\nand diverse containers, we test generalization to 5 new targets, distractors, and containers respectively,\nwhich are unseen in the source and target datasets. Each evaluation is performed with 20 rollouts and an\nenvironment time limit of 40.\nD. Experimental Details for V-PTR and Baseline Methods\nD.1. CQL Finetuning (Kumar et al., 2023)\nFor our second pre-training phase on multi-task robot data and fine-tuning on target data, we utilized\nCQL (Kumar et al., 2020) as the downstream offline RL algorithm. Following the design decisions in the\nofficial implementation of PTR (Kumar et al., 2023), we utilized a variant of Bellman backup that computes the\ntarget value by performing a maximization over target values computed for \ud835\udc5b = 4 actions sampled from the\npolicy at the next state (max_q_backup). In each domain, we swept over the alpha values of \ud835\udefc = 0.1, 1, 5, 10.\nWe built our code upon the CQL implementation from https://github.com/Asap7772/PTR (Kumar et al., 2023).\nD.2. MVP (Xiao et al., 2022a; Radosavovic et al., 2022)\nFor masked autoencoding (MAE)-based methods such as MVP (Radosavovic et al., 2022), we loaded in\nthe pre-trained PyTorch Checkpoints for the ViT-Base model. For this method, we kept the pre-trained\nrepresentation frozen and finetuned a Multi-Layer Perceptron (MLP) that takes in the class token of the ViT\n16\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nHyperparameters\nSimulation\nPick Place\n\ud835\udefc\n0.1, 1,5,10\n0.1, 1,5,10\npolicy architecture\nResNet-50, ViT-B\nResNet-50, ViT-B\ncritic architecture\nResNet-50, ViT-B\nResNet-50, ViT-B\npolicy learning rate\n1e-4\n1e-4\ncritic learning rate\n3e-4\n3e-4\nreward scale\n11\n11\nreward bias\n-1\n-1\nbatch size\n64\n256\nTable 4: The hyperparameters used by V-PTR. After pre-training on multi-task robot data, in the second pre-training phase\nV-PTR fine-tunes the representation using multi-task CQL (Kumar et al., 2020) on diverse robot data. Finally the third fine-tuning\nphase aims to customize this policy for the target task. The above table presents hyperparameters for V-PTR that we utilize in our\nexperiments.\nas input to the model and outputs an action dimensional vector. The output of the MAE is normalized with\nlayer normalization. Other hyperparameters follow the CQL fine-tuning section.\nD.3. VIP (Ma et al., 2022) and R3M (Nair et al., 2022a)\nPrior methods have shown that TD-based updates with networks with batch normalization have instabilities\nduring training. Given this, methods such as VIP and R3M that utilize a standard ResNet-50 (He et al., 2016b)\nbackbone with batch normalization do not finetune stably for methods such as CQL. For this reason, any\ncomparisons using standard ResNet 50 either use frozen batch statistics, or are trained with a behavioral\ncloning objective.\nAll hyperparameters described above are summarized in Table 4.\nE. Reducing the Number of Fine-Tuning Demonstrations\nWe examine how reducing the number of demonstrations from 44 to 10 degrades performance in Table 5.\nV-PTR continues to significantly outperform baselines.\nVideo pre-training\nNo videos\nTask\nV-PTR (Ours) VIP (Ma et al., 2022)+CQL PTR (Kumar et al., 2023)\nCroissant out of Colander\n4 / 12\n1 / 12\n0 / 12\nSweet Potato on Plate\n5 / 12\n0 / 12\n0 / 12\nKnife in Pan\n2 / 12\n0 / 12\n0 / 12\nCucumber in Pot\n4 / 12\n0 / 12\n0 / 12\nOpen Microwave\n8 / 12\n2 / 12\n4 / 12\nSweep Beans\n2 / 12\n0 / 12\n5 / 12\nTable 5: Task success rates of V-PTR and prior methods with only 10 demonstrations. As should be expected, the performances of\nall approaches, including ours, degrade with less data, but V-PTR performs significantly better than other pre-training methods.\n17\n"
  }
]