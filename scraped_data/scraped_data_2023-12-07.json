[
  {
    "title": "Kandinsky 3.0 Technical Report",
    "link": "https://arxiv.org/pdf/2312.03511.pdf",
    "upvote": "43",
    "text": "KANDINSKY 3.0 TECHNICAL REPORT\nVladimir Arkhipkin1\nAndrei Filatov1\nViacheslav Vasilev1\nAnastasia Maltseva1\nSaid Azizov1\nIgor Pavlov1\nJulia Agafonova1\nAndrey Kuznetsov1,2,\u2217\nDenis Dimitrov1,2,\u2217\n1Sber AI\n2AIRI\nABSTRACT\nWe present Kandinsky 3.0, a large-scale text-to-image generation model based on latent diffusion,\ncontinuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher\nquality and realism of image generation. Compared to previous versions of Kandinsky 2.x, Kandinsky\n3.0 leverages a two times larger U-Net[1] backbone, a ten times larger text encoder and removes\ndiffusion mapping. We describe the architecture of the model, the data collection procedure, the\ntraining technique, and the production system of user interaction. We focus on the key components\nthat, as we have identified as a result of a large number of experiments, had the most significant impact\non improving the quality of our model compared to the others. By our side-by-side comparisons,\nKandinsky becomes better in text understanding and works better on specific domains. The project is\navailable at https://ai-forever.github.io/Kandinsky-3.\n1\nIntroduction\nRecently, the quality of text-to-image generation models increased significantly, which became possible thanks to the\ninvention of Diffusion Probabilistic Models [2, 3]. To date, the zoo of text-to-image generation models is extremely\nrich [4, 5, 6, 7, 8, 9, 10, 11]. Some of these systems provide users with opportunities for almost real-time inference,\na high level of photorealism, an understanding of fantastic ideas and concepts not found in the real world, and many\nuser-friendly web platforms and interfaces for a generation. Despite this, the task of text-to-image generating continues\nto pose serious challenges to researchers. The growing number of practical applications in commerce and design leads\nto a new unprecedented level of realism and alignment with complex textual descriptions.\nThis paper presents Kandinsky 3.0, a new text-to-image generation model based on latent diffusion [6]. Earlier, we\nintroduced other models of the Kandinsky family [12], the architecture of which is based on a two-stage pipeline\nusing Diffusion Mapping between elements of latent vector spaces of images and text with subsequent decoding. In\nthe Kandinsky 3.0 model, we focused on improving the text understanding, the image quality and simplifying the\narchitecture by providing a single-stage pipeline in which generation takes place directly using text embeddings without\nany additional priors. The whole pipeline contains 11.9 billion parameters, almost three times more than the largest\nof the previous models of the Kandinsky family. Also, we integrated Kandinsky 3.0 into our user-friendly interaction\nsystem. We made our model completely public to promote the development of new technologies and openness in the\nscientific community.\nThis technical report is arranged as follows:\n\u2022 Firstly, we describe the user interaction demo system;\n\u2022 Secondly, we describe the key components of the Kandinsky 3.0 model, datasets usage strategy, training\ntechniques, and various applications;\n\u2022 Finally, we report the results of side-by-side comparisons based on human preferences and discuss the model\nlimitations.\n\u2217Corresponding authors: Andrey Kuznetsov <kuznetsov@airi.net>, Denis Dimitrov <dimitrov@airi.net>.\n\u2217\u2217The family of models is named after Wassily Kandinsky, the great Russian artist and an art theorist, the father of abstract art.\narXiv:2312.03511v2  [cs.CV]  11 Dec 2023\nKandinsky 3.0 Technical Report\n2\nDemo System\nAs in the previous work [12], we incorporated the Kandinsky 3.0 model in several user interaction systems with open\nfree access. Here we will describe their functionality and capabilities.\nFusionbrain.ai1 \u2013 this is a web-editor that has the following functionality for text-to-image generation2:\n\u2022 The system can accept text prompts in Russian, English and other languages. It is also allowed to use emoji in\nthe text description. The maximum size of a text is 1000 characters;\n\u2022 In the \u201cNegative prompt\u201d field, the user can specify which information (e.g., colors) the model should not use\nfor generation;\n\u2022 Maximum resolution is 1024 \u00d7 1024;\n\u2022 Choosing the sides ratio: 1 : 1, 16 : 9, 9 : 16, 2 : 3 or 3 : 2;\n\u2022 Choosing of generation style to accelerate inference: digital image, pixel art, cartoon, portrait photo, studio\nphoto, cyberpunk, 3D render, classicism, anime, oil painting, pencil drawing, Khokhloma painting style, and\nstyles of famous artists such as Aivazovsky, Kandinsky, Malevich, and Picasso;\n\u2022 Zoom in/out;\n\u2022 Using an eraser to highlight areas that can be filled both with and without a new text description (inpainting\ntechnique);\n\u2022 Using a sliding window to expand the boundaries of the generated image and further generation with new\nborders (outpainting approach);\n\u2022 We also implemented a content filter developed by us to process incorrect requests.\nThis website also supports image-to-video generation with the following characteristics:\n\u2022 Resolution: 640 \u00d7 640, 480 \u00d7 854 and 854 \u00d7 480;\n\u2022 The user can set up to 4 scenes by describing each scene using a text prompt. Each scene lasts 4 seconds,\nincluding the transition to the next;\n\u2022 For each scene, it is possible to choose the direction of camera movement: up, down, right, left, counterclock-\nwise or clockwise, zoom-in, zoom-out, and different types of flights around the object;\n\u2022 The average generation time ranges from 1.5 minutes for one scene to 6 minutes for four scenes;\n\u2022 The generated video can be downloaded in mp4 format.\nWe also created a Telegram-bot3 in which text-to-image generation is available. The user can choose a model version\n(Kandinsky 2.1, 2.2, or Kandinsky 3.0) and view the current negative prompt or specify a new one.\n3\nKandinsky 3.0 Architecture\n3.1\nOverall pipeline\nKandinsky 3.0 is a latent diffusion model, the whole pipeline of which includes a text encoder for processing a\nprompt from the user, a U-Net for predicting noise during the denoising (reverse) process, and a decoder for image\nreconstruction from the generated latent (Fig. 1). The text encoder and image decoder were completely frozen during\nthe U-Net training. The whole model contains 11.9 billion parameters. Table 1 shows the number of parameters for the\ncomponents of the Kandinsky 3.0 model and the Kandinsky 2.2 [12] and SDXL [10] models. Below, we take a look at\neach component of our new model.\n1https://fusionbrain.ai/en/\n2A detailed description of the API can be found at https://fusionbrain.ai/docs/en/doc/api-dokumentaciya/.\n3https://t.me/kandinsky21_bot\n2\nKandinsky 3.0 Technical Report\nFigure 1: Kandinsky 3.0 overall pipeline architecture. It consists of a text encoder, a latent conditioned diffusion\nmodel, and an image decoder.\nTable 1: Comparison of the number of parameters (in billions) of components for the Kandinsky 2.2, SDXL, and\nKandinsky 3.0 models.\nKandinsky 2.2 [12]\nSDXL [10]\nKandinsky 3.0\nModel type\nLatent Diffusion\nLatent Diffusion\nLatent Diffusion\nTotal parameters\n4.6B\n3.33B\n11.9B\nText encoder\n0.62B\n0.8B\n8.6B\nDiffusion Mapping [12]\n1.0B\n\u2013\n\u2013\nDenoising U-Net\n1.2B\n2.5B\n3.0B\nImage decoder\n0.08B\n0.08B\n0.27B\n3.2\nU-Net architecture\nThis section, we describe the considerations and ideas that led us to create our denoising U-Net architecture. Based on\nthe success of large transformer-based models in vision problems when learning on large amounts of data [13, 14, 15, 16],\nand the fact that convolutional architectures occupied the central place in diffusion models so far, we had to decide\nwhich types of layers would contain the main part of the parameters of the new model: transformer or convolutional. Our\nfield of view mainly included classification model architectures that showed high quality on the ImageNet benchmark\ndataset [17]. We conducted about half a thousand experiments with various architectural combinations and noted the\nfollowing two key points:\n\u2022 Increasing the network depth by increasing the number of layers while reducing the total number of parameters\nin practice gives better results in training. A similar idea of residual blocks with bottlenecks was previously\nexploited in the ResNet-50 [18] and BigGAN-deep architecture [19];\n\u2022 At the initial stage, in high resolution, we decided to process the image using only convolutional blocks, while\nmore compressed representations were fed to the transformer layers too. This ensures the global interaction of\nimage elements.\nWe also reviewed the MaxViT architecture, which is almost entirely based on transformer blocks adapted to work with\nimages by reducing the quadratic complexity of self-attention [20]. In the classification task, this architecture shows the\nbest results in comparison with the models mentioned above. Despite this, during experiments, we found out that this\narchitecture does not show good results in the generation task.\nHaving thus explored all of the above architectures, we settled on the ResNet-50 block as the main block for our\ndenoising U-Net. Thus, the residual blocks of our architecture at the input and output contain convolutional layers\nwith a 1 \u00d7 1 kernel, which correspondingly reduce and increase the number of channels. We also expanded it with one\nmore convolutional layer with a 3 \u00d7 3 kernel, as in the BigGAN-deep residual block architecture. Using bottlenecks in\nresidual blocks made it possible to double the number of convolutional layers, while maintaining approximately the\nsame number of parameters as without bottlenecks. At the same time, the depth of our new architecture has increased\nby 1.5 times compared to previous versions of the Kandinsky 2.x model.\nAt the higher levels of the upscale and downsample parts, we placed only our implementation of convolutional residual\nBigGAN-deep blocks. At the same time, at lower resolutions, the architecture includes self-attention and cross-attention\nlayers. The complete scheme of our U-Net architecture, residual BigGAN-deep blocks, and cross-attention blocks is\nshown in Fig. 2.\n3\nKandinsky 3.0 Technical Report\nFigure 2: Kandinsky 3.0 U-Net architecture. The architecture is based on modified BigGAN-deep blocks (left and\nright \u2013 downsample and upsample versions), which allows us to increase the depth of the architecture due to the\npresence of bottlenecks. The attention layers are arranged at levels with a lower resolution than the original image.\nOur version of the BigGAN-deep residual blocks differs from the one proposed in [19] by the following components:\n\u2022 We use Group Normalization [21] instead of Batch Normalization [22];\n\u2022 We use SiLU [23] instead of ReLU [24];\n\u2022 As skip connections, we implement them in the standard BigGAN residual block. For example, in the upsample\npart of the U-Net, we do not drop channels but perform upsampling and apply a convolution with 1 \u00d7 1 kernel.\n3.3\nText encoder\nFor the text encoder, we use the 8.6B encoder of the Flan-UL2 20B model [25], which is the most powerful encoder-\ndecoder open-source model for now and is based on the pre-trained UL2 20B [26]. In addition to pretraining on a large\ncorpus of texts, Flan-UL2 was also trained using supervised finetuning on many language tasks using Flan Prompting\n[27]. Our experiments showed that such finetuning also significantly improves image generation.\n3.4\nSber-MoVQGAN\nTo achieve a high-quality image reconstruction in complex domains such as text and faces, we developed the Sber-\nMoVQGAN autoencoder, which showed good results in Kandinsky 2.2 [12].\nThe Sber-MoVQGAN architecture is based on the VQGAN [28] architecture with the addition of spatially conditional\nnormalization from the MoVQ [29]. Spatial conditional normalization is implemented similarly to the Adaptive Instance\nNormalization (AdaIN) layers used in the StyleGAN [30] architecture and is calculated by the formula:\nF i = \u03d5\u03b3(zq)F i\u22121 \u2212 \u00b5(F i\u22121)\n\u03c3(F i\u22121)\n+ \u03d5\u03b2(zq)\n(1)\nwhere F i\u22121 is the intermediate feature map, \u00b5 and \u03c3 are the functions for calculating the mean and standard deviation\nof the activation, \u03d5\u03b3 and \u03d5\u03b2 are the trainable affine transformations, which convert zq into the scaling and bias values.\nOther important features of our implementation include the addition of EMA (exponential moving average) weights and\na modified loss function from ViT-VQGAN [31] during the training stage.\n4\nKandinsky 3.0 Technical Report\nWe trained three versions of Sber-MoVQGAN \u2013 67M, 102M, and 270M. The 67M version is the same size as the\nstandard VQGAN. The 102M model uses twice the number of residual blocks compared to the 67M, and the 270M\nmodel operates with twice the original number of channels. Kandinsky 3.0 uses the 270M model as the image decoder.\nWe trained Sber-MoVQGAN on the LAION HighRes dataset [32], obtaining the SOTA results in image reconstruction.\nThe comparison of our autoencoder with competitors and Sber-VQGAN4 are presented in Table 2. We released the\nweights and code for these models under an open-source license 5.\nTable 2: Sber-MoVQGAN comparison with competitors on ImageNet dataset.\nModel\nLatent size\nNum Z\nTrain steps\nFID \u2193\nSSIM \u2191\nPSNR \u2191\nL1 \u2193\nViT-VQGAN [31]\n32x32\n8192\n500,000\n1.28\n\u2013\n\u2013\n\u2013\nRQ-VAE [33]\n8x8x16\n16384\n10 epochs\n1.83\n\u2013\n\u2013\n\u2013\nMo-VQGAN [29]\n16x16x4\n1024\n40 epochs\n1.12\n0.673\n22.42\n\u2013\nVQ CompVis [34]\n32x32\n16384\n971,043\n1.34\n0.650\n23.85\n0.0533\nKL CompVis [34]\n32x32\n\u2013\n246,803\n0.968\n0.692\n25.11\n0.0474\nSber-VQGAN\n32x32\n8192\n1 epoch\n1.44\n0.682\n24.31\n0.0503\nSber-MoVQGAN 67M\n32x32\n1024\n5,000,000\n1.34\n0.704\n25.68\n0.0451\nSber-MoVQGAN 67M\n32x32\n16384\n2,000,000\n0.965\n0.725\n26.45\n0.0415\nSber-MoVQGAN 102M\n32x32\n16384\n2,360,000\n0.776\n0.737\n26.89\n0.0398\nSber-MoVQGAN 270M\n32x32\n16384\n1,330,000\n0.686\n0.741\n27.04\n0.0393\n4\nData and Training Strategy\nData.\nDuring the training procedure, we used a large dataset of text-image pairs collected online. The training dataset\nconsists of popular open-source datasets and our internal dataset of approximately 150 million text-image pairs. To\nimprove data quality, we pass the data through several filters: the aesthetics quality of the image, the watermarks\ndetection, the CLIP similarity of the image with the text [15], and the detection of duplicates with perceptual hash.\nWe discovered that the collected data from Common Crawl [35] contains almost no images related to Russian culture.\nTo fix this, we collected and labeled a dataset of 200 thousand text-image pairs from Soviet and Russian cartoons,\nfamous people, and places. This dataset helped improve the model\u2019s quality and text alignment when generating\nRussian-related images.\nWe also divided all the data into two categories. We used the first at the initial stages of low-resolution pretraining and\nthe second for mixed and high-resolution finetuning at the last stage. The first category includes open large text-image\ndatasets such as LAION-5B [36] and COYO-700M [37] and \u201cdirty\u201d data that we collected from the Internet. The\nsecond category contains the same datasets but with stricter filters, especially for the image aesthetics quality.\nTraining.\nWe divided the training process into several stages to use more data and train the model to generate images\nin a wide range of resolutions:\n1. 256 \u00d7 256 resolution: 1.1 billions of text-image pairs, batch size = 20, 600 thousand steps, 104 NVIDIA\nTesla A100;\n2. 384 \u00d7 384 resolutions: 768 millions of text-image pairs, batch size = 10, 500 thousand steps, 104 NVIDIA\nTesla A100;\n3. 512 \u00d7 512 resolutions: 450 millions of text-image pairs, batch size = 10, 400 thousand steps, 104 NVIDIA\nTesla A100;\n4. 768 \u00d7 768 resolutions: 224 millions of text-image pairs, batch size = 4, 250 thousand steps, 416 NVIDIA\nTesla A100;\n5. Mixed resolution: 7682 \u2264 W \u00d7 H \u2264 10242, 280 millions of text-image pairs, batch size = 1, 350 thousand\nsteps, 416 NVIDIA Tesla A100.\n4https://github.com/ai-forever/tuned-vq-gan\n5https://github.com/ai-forever/MoVQGAN\n5\nKandinsky 3.0 Technical Report\nFigure 3: Examples of inpainting.. We use the following prompts for inpainting: \u201cship sailing in the water\u201d,\n\u201crocket\u201d and \u201crobot sitting on a bench\u201d, respectively. We generated inpainted images using FusionBrain.ai\nplatform.\n5\nApplications\n5.1\nInpainting and Outpainting\nImplementation of the inpainting model is the same as GLIDE [38]: we initialize our model from base Kandinsky\nmodel weights. Then, we modify the input convolution layer of U-Net (Sec. 3.2) so that the input can additionally\naccept the image latent and mask. Thus, U-Net takes as many as 9 channels as input: 4 for the original latent, 4 for the\nimage latent, and an additional channel for the mask. We zeroed the additional weights, so training starts with the base\nmodel.\nFor training, we generate random masks of the following forms: rectangular, circles, strokes, and arbitrary form. For\nevery image sample, we use up to 3 masks, and for every image, we use unique masks. We use the same dataset as\nfor the training base model with generated masks. We train our model using Lion [39] with lr=1e-5 and apply linear\nwarmup for the first 10 thousand steps. We train our model for 250 thousand steps. The inpainting results are in Fig. 3.\nThe outpainting results can be found in the Outpainting appendix section.\n5.2\nImage-to-Video Generation\nImage-to-video generation involves a series of iterative steps, encompassing four stages as illustrated in Fig. 4. Our\nanimation pipeline is based on the Deforum technique [40]. It consists of a series of transformations applied to the\nscene:\n1. Conversion of the image into a three-dimensional representation using a depth map;\n2. Application of spatial transformations to the resulting scene to induce an animated effect;\n6\nKandinsky 3.0 Technical Report\nFigure 4: Illustration of the Image-to-Video Generation process utilizing the image-to-image technique. The\ninput image undergoes a right shift transformation. The resulting image enters the image-to-image process to eliminate\ntransformation artifacts and update the semantic content in alignment with the provided prompt.\n3. Projection of the 2.5D scene back onto a 2D image;\n4. Elimination of transformation defects and updating of semantics through image-to-image conversion tech-\nniques.\nThe scene generation process involves defining depth values along the z-axis within the interval [(znear, zfar)] in the\ncoordinate system. Depth estimation utilizes either MiDaS [41] or AdaBins [42]. The camera\u2019s first-person motion\ntrajectory, simulated through functions x(t), y(t), z(t), \u03b1(t), \u03b2(t), and \u03b3(t), includes perspective projection operations\nwith the camera initially fixed at the origin and the scene at a distance of znear. Then, we apply transformations\nby rotating points around axes passing through the scene\u2019s center and translating to this center. Acknowledging\nthe limitations of a single-image-derived depth map, addressing distortions resulting from minor camera orientation\ndeviations is crucial. Two essential considerations follow: adjusting scene position through infinitesimal transformations\nand employing the image-to-image approach after each transformation. The image-to-image technique facilitates the\nrealization of seamless and semantically accurate transitions between frames, enhancing the aesthetic appeal of this\napproach. The examples of image-to-video generations are presented in the Image-to-Video appendix section.\n5.3\nText-to-Video\nBased on the Kandinsky 3.0 model, we also created the text-to-video generation pipeline Kandinsky Video [43],\nconsisting, in addition to a text encoder and an image decoder, of two models \u2013 for keyframes generation and\ninterpolation between them. Both models use the pretrained weights of Kandinsky 3.0 as the backbone. We have\nalso implemented the publicly available text-to-video generation interface6 in the Fusionbrain.ai website, which we\nmentioned above (Sec. 2). Please refer to the main paper for additional details regarding the text-to-video model.\n6\nHuman evaluation\nSome examples of text-to-image generations can be found in the Text-to-Image appendix section. To compare Kandinsky\n3.0 with other well-known models, we have compiled a balanced set of 2.1K prompts in 21 categories. Using this\nset, we first performed several side-by-side (SBS) comparisons between different versions of the Kandinsky 3.0 and\nthen selected the best version. We conducted three rounds of SBS comparisons involving 28 people to do this. Next,\n6https://fusionbrain.ai/en/t2v/\n7\nKandinsky 3.0 Technical Report\nwe conducted side-by-side comparisons of Kandinsky 3.0 with the Kandinsky 2.2 [12], SDXL [10] and DALL-E 3\n[11] models. Each study involved 12 people who voted an average of 30,000 times in each SBS. For this purpose, we\ndeveloped chatbot that showed one of 2.1K pairs of images.\nEach person chose the best image according to two criteria:\n1. Alignment between image content and prompt (text comprehension);\n2. Visual quality of the image.\nWe compared the visual quality and text comprehension in total for all categories, and each category separately. The\nvisual examples of Comparison to prior works and Human evaluation results are presented in the appendix.\n7\nLimitations\nEven though the current system can generate high-quality realistic images and successfully cope with diverse and\ncomplex language concepts, there are ways for further improvement. Among them is the improvement of semantic\ncoherence between the input text and the generated image due to more efficient use of the Flan-UL2 text encoder\npotential. In addition, it is possible to improve the image quality based on human evaluations significantly.\n8\nBorder Impacts and Ethical Considerations\nGenerative models are an effective tool for creativity and media content creation. They are also of great importance for\nthe development of artificial intelligence science. We made the code and the trained weights of the model available to\npromote openness in the scientific community and the development of technologies that improve people\u2019s lives. We\nhave provided free access to the user-friendly interface for everyone on the Internet.\nAt the same time, we know that generative models can be leveraged for blackmail, fraud, disinformation, creating fakes,\ninciting hatred and enmity between people, for unscrupulous political, financial, and other purposes. We warn against\nusing our model in this way and strongly disagree with such malicious applications. We consider it necessary to note\nthat the result of using the generations of our model for unfair purposes is entirely the user\u2019s responsibility.\nDespite this, we made many efforts to make sure that the generated images didn\u2019t contain malicious, offensive, or\ninsulting content. To this end, we cleaned the training dataset from samples marked as harmful/offensive/abusive and\nremoved offensive textual prompts. While obvious queries, according to our tests, rarely generate abusive content,\nthere is technically no guarantee that some carefully designed prompts may not yield undesirable content. Therefore,\ndepending on the application, we recommend using additional classifiers to filter out unwanted content and use\nimage/representation transformation methods adapted to a given application.\n9\nConclusion\nIn this report we highlighted the most significant advantages of our new text-to-image generative model \u2013 Kandinsky 3.0.\nFirst we described the architecture itself and its valuable blocks that lead to high-fidelity generations and better quality\nmetrics values. Further we show several main applications of Kandinsky 3.0 and the quality of inpainting/outpainting\ngeneration shows nearly perfect result in comparison with previous experiments in terms of naturalness of a missing\narea filling. Improving the text encoder size and extending the main diffusion U-Net we achieved higher human\nevaluation scores in comparison with Kandinsky 2.2. It should be mentioned that both measured quality indicators \u2013 text\nunderstanding and visual quality improved. Comparing with DALL-E 3 we are not so good for both indicators, whereas\nfor SDXL we achieved significantly higher scores in both experiments. We also used Kandinsky 3.0 as a backbone for\nour text-to-video end-to-end generation model. In terms of our future research challenges still remain high-fidelity text\ngeneration and photorealistic faces, physics-controlled scene (lighting, positioning, focus and exposure, etc.) and text\nencoder research for better details understanding.\nReferences\n[1] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image seg-\nmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International\nConference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.\n8\nKandinsky 3.0 Technical Report\n[2] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International\nConference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256\u20132265,\nLille, France, 07\u201309 Jul 2015. PMLR.\n[3] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840\u20136851, 2020.\n[4] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided\ndiffusion models. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,\nMaryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 16784\u201316804. PMLR, 2022.\n[5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n[6] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 10684\u201310695, 2022.\n[7] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\n[8] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image\ndiffusion models with ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n[9] Midjourney. https://www.midjourney.com/.\n[10] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin\nRombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n[11] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, Wesam Manassra, Prafulla Dhariwa, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving\nimage generation with better captions, 2023.\n[12] Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov,\nAngelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: an improved text-to-\nimage synthesis with image prior and latent diffusion, 2023.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image recognition at scale. In International Conference on\nLearning Representations, 2021.\n[14] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 10012\u201310022, October 2021.\n[15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In Proceedings of the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763, 18\u201324 Jul 2021.\n[16] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, Proceedings of the\n38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,\npages 8821\u20138831. PMLR, 18\u201324 Jul 2021.\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.\n[19] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image\nsynthesis, 2019.\n[20] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit:\nMulti-axis vision transformer. ECCV, 2022.\n9\nKandinsky 3.0 Technical Report\n[21] Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494, 2018.\n[22] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine\nLearning - Volume 37, ICML\u201915, page 448\u2013456. JMLR.org, 2015.\n[23] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function\napproximation in reinforcement learning, 2017.\n[24] Abien Fred Agarap. Deep learning using rectified linear units (relu), 2019.\n[25] Yi Tay. A new open source flan 20b with ul2. https://www.yitay.net/blog/flan-ul2-20b, 2023.\n[26] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garc\u00eda, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. Ul2: Unifying\nlanguage learning paradigms. In International Conference on Learning Representations, 2022.\n[27] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun\nChen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang,\nGaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi,\nJeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned\nlanguage models, 2022.\n[28] Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. Taming transformers for high-resolution image synthesis,\n2021.\n[29] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for\nhigh-fidelity image generation. Advances in Neural Information Processing Systems, 35:23412\u201323425, 2022.\n[30] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial\nnetworks. CoRR, abs/1812.04948, 2018.\n[31] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason\nBaldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan, 2022.\n[32] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for\ntraining next generation image-text models. In NeurIPS, 2022.\n[33] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation\nusing residual quantization, 2022.\n[34] Andreas Blattmann, Robin Rombach, Kaan Oktay, and Bj\u00f6rn Ommer. Retrieval-augmented diffusion models,\n2022.\n[35] Common crawl. https://commoncrawl.org/terms-of-use.\n[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models, 2022.\n[37] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m:\nImage-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022.\n[38] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\narXiv preprint arXiv:2112.10741, 2021.\n[39] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang\nLuong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675,\n2023.\n[40] Deforum. https://deforum.art/.\n[41] Ren\u00e9 Ranftl, Jonas Gehrig, Martin Humenberger, and Vittorio Ferrari. Towards robust monocular depth estimation:\nMixing datasets for zero-shot cross-dataset transfer. In Proceedings of the IEEE International Conference on\nComputer Vision (ICCV), pages 5626\u20135635, 2019.\n[42] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins.\narXiv:2011.14141 [cs.CV], 2020.\n10\nKandinsky 3.0 Technical Report\n[43] Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, Elizaveta Dakhova, Andrey Kuznetsov, and Denis\nDimitrov. Fusionframes: Efficient architectural aspects for text-to-video generation pipeline. arXiv preprint\narXiv:2311.13073, 2023.\nAppendix A. Acknowledgements\nThe authors would also like to extend their deepest gratitude to the following list of teams and persons, who made a\nsignificant contribution to Kandinsky 3.0 research and development:\n\u2022 Sber AI Research team: Sofia Kirillova, Arseniy Shakhmatov, Mikhail Shoytov, Sergey Nesteruk, Ilya Ryabov,\nMikhail Martynov, Anastasia Lysenko, Zein Shaheen;\n\u2022 Anton Razzhigaev and Elizaveta Dakhova from AIRI;\n\u2022 Konstantin Kulikov and his production team at Sber AI;\n\u2022 Sergey Markov and his research teams at Sber Devices;\n\u2022 Polina Voloshina labelling team;\n\u2022 ABC Elementary labelling team;\n\u2022 TagMe labelling team ;\n\u2022 Tatyana Paskova, Angelina Kuts and prompt engineering team.\nThanks to all of you for your valuable help, advice, and constructive criticism.\n11\nKandinsky 3.0 Technical Report\nAppendix B. Additional Generation Examples\nText-to-Image\n12\nKandinsky 3.0 Technical Report\nOutpainting\nFigure 5: Examples of outpainting.\n13\nKandinsky 3.0 Technical Report\nImage-to-Video\nFigure 6: Animations generated by Image-to-Video pipeline.\n14\nKandinsky 3.0 Technical Report\nComparison to prior works\nFigure 7: Example images from side-by-side comparison. Prompt: A beautiful girl.\nFigure 8: Example images from side-by-side comparison. Prompt: Barbie and Ken are shopping.\n15\nKandinsky 3.0 Technical Report\nFigure 9: Example images from side-by-side comparison. Prompt: A bear in Russian national hat with a\nbalalaika.\nHuman evaluation results\nFigure 10: Overall results of side-by-side human comparison between Kandinsky 3.0 and Kandinsky 2.2.\n16\nKandinsky 3.0 Technical Report\nFigure 11: Results of side-by-side human comparison between Kandinsky 3.0 and Kandinsky 2.2 for text\ncomprehension.\nFigure 12: Results of side-by-side human comparison between Kandinsky 3.0 and Kandinsky 2.2 for visual quality.\n17\nKandinsky 3.0 Technical Report\nFigure 13: Overall results of side-by-side human comparison between Kandinsky 3.0 and DALL-E 3.\nFigure 14: Results of side-by-side human comparison between Kandinsky 3.0 and DALL-E 3 for text comprehension.\n18\nKandinsky 3.0 Technical Report\nFigure 15: Results of side-by-side human comparison between Kandinsky 3.0 and DALL-E 3 for visual quality.\nFigure 16: Overall results of side-by-side human comparison between Kandinsky 3.0 and SDXL.\n19\nKandinsky 3.0 Technical Report\nFigure 17: Results of side-by-side human comparison between Kandinsky 3.0 and SDXL for text comprehension.\nFigure 18: Results of side-by-side human comparison between Kandinsky 3.0 and SDXL for visual quality.\n20\n"
  },
  {
    "title": "Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis",
    "link": "https://arxiv.org/pdf/2312.03491.pdf",
    "upvote": "33",
    "text": "Schrodinger Bridges Beat Diffusion Models on\nText-to-Speech Synthesis\nZehua Chen\u22171 Guande He\u22171 Kaiwen Zheng\u22171 Xu Tan2 Jun Zhu\u20201\n1Dept. of Comp. Sci. & Tech., Institute for AI, THU-Bosch Joint Center for ML, Tsinghua University\n2Microsoft Research Asia\nAbstract\nIn text-to-speech (TTS) synthesis, diffusion models have achieved promising generation quality.\nHowever, because of the pre-defined data-to-noise diffusion process, their prior distribution is restricted\nto a noisy representation, which provides little information of the generation target. In this work, we\npresent a novel TTS system, Bridge-TTS, making the first attempt to substitute the noisy Gaussian prior\nin established diffusion-based TTS methods with a clean and deterministic one, which provides strong\nstructural information of the target. Specifically, we leverage the latent representation obtained from\ntext input as our prior, and build a fully tractable Schrodinger bridge between it and the ground-truth\nmel-spectrogram, leading to a data-to-data process. Moreover, the tractability and flexibility of our\nformulation allow us to empirically study the design spaces such as noise schedules, as well as to develop\nstochastic and deterministic samplers. Experimental results on the LJ-Speech dataset illustrate the\neffectiveness of our method in terms of both synthesis quality and sampling efficiency, significantly\noutperforming our diffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast TTS\nmodels in few-step scenarios. Project page: https://bridge-tts.github.io/.\nLatent \ud835\udc65\ud835\udc61\nText latent: \ud835\udc67\n\u201cBridge-TTS is \nawesome.\u201d\nPrior: \ud835\udc65\ud835\udc47 = \ud835\udc67\nPrior: \ud835\udc65\ud835\udc47~\ud835\udc41(\ud835\udf07\ud835\udc67, \ud835\udef4\ud835\udc67)\nData: \ud835\udc650\nData: \ud835\udc650\nDiffusion-TTS: Data-to-Noise Process\nBridge-TTS: Data-to-Data Process\nLatent \ud835\udc65\ud835\udc61\n\ud835\udc65\ud835\udc47\n\ud835\udc650\n\ud835\udc65\ud835\udc47\n\ud835\udc650\nBridge SDE Sampling\nBridge ODE Sampling\n\ud835\udc65\ud835\udc47\n\ud835\udc650\n\ud835\udc65\ud835\udc47\n\ud835\udc650\nFigure 1: An overview of Bridge-TTS built on Schrodinger bridge.\n\u2217Equal contribution;\n\u2020Corresponding author: dcszj@tsinghua.edu.cn\n1\narXiv:2312.03491v1  [cs.LG]  6 Dec 2023\n1\nIntroduction\nDiffusion models, including score-based generative models (SGMs) [Song et al., 2021b] and denoising\ndiffusion probabilistic models [Ho et al., 2020], have been one of the most powerful generative models across\ndifferent data generation tasks [Bao et al., 2023, Leng et al., 2022, Ramesh et al., 2022, Wang et al., 2023].\nIn speech community, they have been extensively studied in waveform synthesis [Chen et al., 2021, 2022b,\nKong et al., 2021], text-to-audio generation [Huang et al., 2023a,b, Liu et al., 2023b,c], and text-to-speech\n(TTS) synthesis [Popov et al., 2021, Shen et al., 2023, Tan et al., 2021]. Generally, these models contain two\nprocesses between the data distribution and the prior distribution: 1) the forward diffusion process gradually\ntransforms the data into a known prior distribution, e.g., Gaussian noise; 2) the reverse denoising process\ngradually generates data samples from the prior distribution.\nIn diffusion-based TTS systems [Chen et al., 2023, Popov et al., 2021, Ye et al., 2023], the text input is\nusually first transformed into latent representation by a text encoder, which contains a phoneme encoder and\na duration predictor, and then diffusion models are employed as a decoder to generate the mel-spectrogram\nconditioned on the latent. The prior distribution in these systems can be classified into two types: 1) one\nis using the standard Gaussian noise to generate target [Chen et al., 2022c, Huang et al., 2022, Liu et al.,\n2022b]; 2) the other improves the prior to be more informative of the target. For example, Grad-TTS\n[Popov et al., 2021] learns the latent representation from the text encoder with the ground-truth target in\ntraining, and takes it as the mean of prior distribution to obtain a mean-shifted Gaussian. PriorGrad [Lee\net al., 2022] utilizes the statistical values from training data, computing a Gaussian with covariance matrix.\nDiffSinger [Liu et al., 2022a] employs an auxiliary model to acquire an intractable prior distribution, enabling\na shallow reverse process. However, because diffusion models pre-specify the noise-additive diffusion process,\nthe prior distribution of the above systems is confined to a noisy representation, which is not indicative of the\nmel-spectrogram.\nIn this work, as shown in Figure 1, we propose a new design to generate mel-spectrogram from a clean\nand deterministic prior, i.e., the text latent representation supervised by ground-truth target [Popov et al.,\n2021]. It has provided structural information of the target and is utilized as the condition information in both\nrecent diffusion [Chen et al., 2023, Ye et al., 2023] and conditional flow matching [Guo et al., 2023, Mehta\net al., 2023] based TTS systems, while we argue that replacing the noisy prior in previous systems with\nthis clean latent can further boost the TTS sample quality and inference speed. To enable this design, we\nleverage Schrodinger bridge [Chen et al., 2022a, Schr\u00f6dinger, 1932] instead of diffusion models, which seeks a\ndata-to-data process rather than the data-to-noise process in diffusion models. As the original Schrodinger\nbridge is generally intractable that hinders the study of the design spaces in training and sampling, we\npropose a fully tractable Schrodinger bridge between paired data with a flexible form of reference SDE in\nalignment with diffusion models [Ho et al., 2020, Song et al., 2021b].\nWith the tractability and flexibility of our proposed framework, aiming at TTS synthesis with superior\ngeneration quality and efficient sampling speed, we make an investigation of noise schedule, model parameter-\nization, and training-free samplers, which diffusion models have greatly benefited from [Hoogeboom et al.,\n2023, Salimans and Ho, 2022, Song et al., 2021a], while not been thoroughly studied in Schrodinger bridge\nrelated works. To summarize, we make the following key contributions in this work:\n\u2022 In TTS synthesis, we make the first attempt to generate the mel-spectrogram from clean text latent\nrepresentation (i.e., the condition information in diffusion counterpart) by means of Schrodinger bridge,\nexploring data-to-data process rather than data-to-noise process.\n\u2022 By proposing a fully tractable Schrodinger bridge between paired data with a flexible form of reference\nSDE, we theoretically elucidate and empirically explore the design spaces of noise schedule, model\nparameterization, and sampling process, further enhancing TTS quality with asymmetric noise schedule,\ndata prediction, and first-order bridge samplers.\n\u2022 Empirically, we attain both state-of-the-art generation quality and inference speed with a single training\nsession. In both 1000-step and 50-step generation, we significantly outperform our diffusion counterpart\nGrad-TTS [Popov et al., 2021]; in 4-step generation, we accomplish higher quality than FastGrad-TTS\n[Vovk et al., 2022]; in 2-step generation, we surpass the state-of-the-art distillation method CoMoSpeech\n[Ye et al., 2023], and the transformer-based model FastSpeech 2 [Ren et al., 2021].\n2\n2\nBackground\n2.1\nDiffusion Models\nGiven a data distribution pdata(x), x \u2208 Rd, SGMs [Song et al., 2021b] are built on a continuous-time\ndiffusion process defined by a forward stochastic differential equation (SDE):\ndxt = f(xt, t)dt + g(t)dwt,\nx0 \u223c p0 = pdata\n(1)\nwhere t \u2208 [0, T] for some finite horizon T, f : Rd \u00d7 [0, T] \u2192 Rd is a vector-valued drift term, g : [0, T] \u2192 R\nis a scalar-valued diffusion term, and wt \u2208 Rd is a standard Wiener process. Under proper construction of\nf, g, the boundary distribution pT (xT ) is approximately a Gaussian prior distribution pprior = N(0, \u03c32\nT I).\nThe forward SDE has a corresponding reverse SDE [Song et al., 2021b] which shares the same marginal\ndistributions {pt}T\nt=0 with the forward SDE:\ndxt = [f(xt, t) \u2212 g2(t)\u2207 log pt(xt)]dt + g(t)d \u00afwt,\nxT \u223c pT \u2248 pprior\n(2)\nwhere \u00afwt is the reverse-time Wiener process, and the only unknown term \u2207 log pt(xt) is the score function of\nthe marginal density pt. By parameterizing a score network s\u03b8(xt, t) to predict \u2207 log pt(xt), we can replace\nthe true score in Eqn. (2) and solve it reversely from pprior at t = T, yielding generated data samples at t = 0.\ns\u03b8(xt, t) is usually learned by the denoising score matching (DSM) objective [Song et al., 2021b, Vincent,\n2011] with a weighting function \u03bb(t) > 0:\nEp0(x0)pt|0(xt|x0)Et\n\u0002\n\u03bb(t)\u2225s\u03b8(xt, t) \u2212 \u2207 log pt|0(xt|x0)\u22252\n2\n\u0003\n,\n(3)\nwhere t \u223c U(0, T) and pt|0 is the conditional transition distribution from x0 to xt, which is determined by\nthe pre-defined forward SDE and is analytical for a linear drift f(xt, t) = f(t)xt.\n2.2\nDiffusion-Based TTS Systems\nThe goal of TTS systems is to learn a generative model p\u03b8(x|y) over mel-spectrograms (Mel) x \u2208 Rd\ngiven conditioning text y1:L with length L. Grad-TTS [Popov et al., 2021] provides a strong baseline for\nTTS with SGM, which consists of a text encoder and a diffusion-based decoder. Specifically, they alter the\nGaussian prior in SGMs to another one \u02dcpenc(z|y) = N(z, I) with informative mean z, where z \u2208 Rd is a\nlatent acoustic feature transformed from a text string y through the text encoder network E, i.e., z = E(y).\nThe diffusion-based decoder utilizes \u02dcpenc as prior for SGM and builds a diffusion process via the following\nmodified forward SDE:\ndxt = 1\n2(z \u2212 xt)\u03b2tdt +\np\n\u03b2tdwt,\nx0 \u223c p0 = pdata(x|y)\n(4)\nwhere p0 = pdata(x|y) is the true conditional data distribution and \u03b2t is a non-negative noise schedule. The\nforward SDE in Eqn. (4) will yield xT \u223c pT \u2248 \u02dcpenc with sufficient large T [Popov et al., 2021]. During training,\nthe text encoder and the diffusion-based decoder are jointly optimized, where the encoder is optimized with\na negative log-likelihood loss Lenc = \u2212Epdata(x|y)[log \u02dcpenc(x|y)] and the decoder is trained with the DSM\nobjective in Eqn. (3), denoted as Ldiff. Apart from Lenc and Ldiff, the TTS system also optimizes a duration\npredictor \u02c6A as a part of the encoder that predicts the alignment map A\u2217 between encoded text sequence\n\u02dcz1:L and the latent feature z1:F with F frames given by Monotonic Alignment Search [Kim et al., 2020],\nwhere zj = \u02dczA\u2217(j). Denote the duration prediction loss as Ldp, the overall training objective of Grad-TTS is\nLgrad-tts = Lenc + Ldp + Ldiff.\n2.3\nSchrodinger Bridge\nThe Schrodinger Bridge (SB) problem [Chen et al., 2022a, De Bortoli et al., 2021, Schr\u00f6dinger, 1932]\noriginates from the optimization of path measures with constrained boundaries:\nmin\np\u2208P[0,T ] DKL(p \u2225 pref),\ns.t. p0 = pdata, pT = pprior\n(5)\n3\nwhere P[0,T ] is the space of path measures on a finite time horizon [0, T], pref is the reference path measure,\nand p0, pT are the marginal distributions of p at boundaries. Generally, pref is defined by the same form of\nforward SDE as SGMs in Eqn. (1) (i.e., the reference SDE). In such a case, the SB problem is equivalent to a\ncouple of forward-backward SDEs [Chen et al., 2022a, Wang et al., 2021]:\ndxt = [f(xt, t) + g2(t)\u2207 log \u03a8t(xt)]dt + g(t)dwt,\nx0 \u223c pdata\n(6a)\ndxt = [f(xt, t) \u2212 g2(t)\u2207 log b\u03a8t(xt)]dt + g(t)d \u00afwt,\nxT \u223c pprior\n(6b)\nwhere f and g are the same as in the reference SDE. The extra non-linear drift terms \u2207 log \u03a8t(xt) and\n\u2207 log b\u03a8t(xt) are also described by the following coupled partial differential equations (PDEs):\n( \u2202\u03a8\n\u2202t = \u2212\u2207x\u03a8\u22a4f \u2212 1\n2 Tr\n\u0000g2\u22072\nx\u03a8\n\u0001\n\u2202 b\u03a8\n\u2202t = \u2212\u2207x \u00b7 (b\u03a8f) + 1\n2 Tr\n\u0010\ng2\u22072\nx b\u03a8\n\u0011\ns.t. \u03a80 b\u03a80 = pdata, \u03a8T b\u03a8T = pprior.\n(7)\nThe marginal distribution pt of the SB at any time t \u2208 [0, T] satisfies pt = \u03a8t b\u03a8t. Compared to SGMs\nwhere pT \u2248 pprior = N(\u00b5, \u03c32\nT I), SB allows for a flexible form of pprior and ensures the boundary condition\npT = pprior. However, solving the SB requires simulating stochastic processes and performing costly iterative\nprocedures [Chen et al., 2022a, De Bortoli et al., 2021, Shi et al., 2023]. Therefore, it suffers from scalability\nand applicability issues. In certain scenarios, such as using paired data as boundaries, the SB problem can\nbe solved in a simulation-free approach [Liu et al., 2023a, Somnath et al., 2023]. Nevertheless, SBs in these\nworks are either not fully tractable or limited to restricted families of pref, thus lacking a comprehensive and\ntheoretical analysis of the design spaces.\n3\nBridge-TTS\nWe extend SB techniques to the TTS task and elucidate the design spaces with theory-grounded analyses.\nWe start with a fully tractable SB between paired data in TTS modeling. Based on such formulation, we\nderive different training objectives and theoretically study SB sampling in the form of SDE and ODE, which\nlead to novel first-order sampling schemes when combined with exponential integrators. In the following\ndiscussions, we say two probability density functions are the same when they are up to a normalizing factor.\nBesides, we assume the maximum time T = 1 for convenience.\n3.1\nSchrodinger Bridge between Paired Data\nAs we have discussed, with the properties of unrestricted prior form and strict boundary condition, SB is\na natural substitution for diffusion models when we have a strong informative prior. In the TTS task, the\npairs of the ground-truth data (x, y) and the deterministic prior z = E(y) given by the text encoder can be\nseen as mixtures of dual Dirac distribution boundaries (\u03b4x, \u03b4z), which simplifies the solving of SB problem.\nHowever, in such a case, the SB problem in Eqn. (5) will inevitably collapse given a stochastic reference\nprocess that admits a continuous density pref\n1\nat t = 1, since the KL divergence between a Dirac distribution\nand a continuous probability measure is infinity.\nTo tackle this problem, we consider a noisy observation of boundary data points x0, x1 polluted by a\nsmall amount of Gaussian noise N(0, \u03f52\n1I) and N(0, \u03f52\n2I) respectively, which helps us to identify the SB\nformulation between clean data when \u03f51, \u03f52 \u2192 0. Actually, we show that in general cases where the reference\nSDE has a linear drift f(xt, t) = f(t)xt (which is aligned with SGMs), SB has a fully tractable and neat\nsolution when \u03f52 = e\nR 1\n0 f(\u03c4)d\u03c4\u03f51. We formulate the result in the following theorem.\nProposition 3.1 (Tractable Schrodinger Bridge between Gaussian-Smoothed Paired Data with Reference\nSDE of Linear Drift, proof in Appendix A.1). Assume f = f(t)xt, the analytical solution to Eqn. (7) when\npdata = N(x0, \u03f52I) and pprior = N(x1, e2\nR 1\n0 f(\u03c4)d\u03c4\u03f52I) is\nb\u03a8\u03f5\nt = N(\u03b1ta, (\u03b12\nt \u03c32 + \u03b12\nt \u03c32\nt )I),\n\u03a8\u03f5\nt = N(\u00af\u03b1tb, (\u03b12\nt \u03c32 + \u03b12\nt \u00af\u03c32\nt )I)\n(8)\n4\nwhere t \u2208 [0, 1],\na = x0 + \u03c32\n\u03c32\n1\n(x0 \u2212 x1\n\u03b11\n),\nb = x1 + \u03c32\n\u03c32\n1\n(x1 \u2212 \u03b11x0),\n\u03c32 = \u03f52 +\np\n\u03c34\n1 + 4\u03f54 \u2212 \u03c32\n1\n2\n,\n(9)\nand\n\u03b1t = e\nR t\n0 f(\u03c4)d\u03c4,\n\u00af\u03b1t = e\u2212\nR 1\nt f(\u03c4)d\u03c4,\n\u03c32\nt =\nZ t\n0\ng2(\u03c4)\n\u03b12\u03c4\nd\u03c4,\n\u00af\u03c32\nt =\nZ 1\nt\ng2(\u03c4)\n\u03b12\u03c4\nd\u03c4.\n(10)\nIn the above theorem, \u03b1t, \u00af\u03b1t, \u03c3t, \u00af\u03c3t are determined by f, g in the reference SDE (Eqn. (1)) and are\nanalogous to the noise schedule in SGMs [Kingma et al., 2021]. When \u03f5 \u2192 0, b\u03a8\u03f5\nt, \u03a8\u03f5\nt converge to the tractable\nsolution between clean paired data (x0, x1):\nb\u03a8t = N(\u03b1tx0, \u03b12\nt \u03c32\nt I),\n\u03a8t = N(\u00af\u03b1tx1, \u03b12\nt \u00af\u03c32\nt I)\n(11)\nThe advantage of such tractability lies in its ability to facilitate the study of training and sampling under the\nforward-backward SDEs (Eqn. (6)), which we will discuss in the following sections. Besides, the marginal\ndistribution pt = b\u03a8t\u03a8t of the SB also has a tractable form:\npt = \u03a8t b\u03a8t = N\n\u0012\u03b1t\u00af\u03c32\nt x0 + \u00af\u03b1t\u03c32\nt x1\n\u03c32\n1\n, \u03b12\nt \u00af\u03c32\nt \u03c32\nt\n\u03c32\n1\nI\n\u0013\n,\n(12)\nwhich is a Gaussian distribution whose mean is an interpolation between x0, x1, and variance is zero at\nboundaries and positive at the middle.\nA special case is that, when the noise schedule f(t) = 0 and\ng(t) = \u03c3 > 0, we have pt = N((1 \u2212 t)x0 + tx1, \u03c32t(1 \u2212 t)I), which recovers the Brownian bridge used in\nprevious works [Qiu et al., 2023, Tong et al., 2023a,b]. Actually, Eqn. (12) reveals the form of generalized\nBrownian bridge with linear drift and time-varying volatility between x0 and x1. We put the detailed analysis\nin Appendix B.1.\n3.2\nModel Training\nThe TTS task aims to learn a model to generate the Mel x0 given text y. Denote x1 = E(y) as the\nlatent acoustic feature produced by text encoder E, since the SB is tractable given x0, x1 (\u2207 log \u03a8, \u2207 log b\u03a8\nin Eqn. (6) are determined by Eqn. (11)), a direct training approach is to parameterize a network x\u03b8 to\npredict x0 given xt at different timesteps, which allows us to simulate the process of SB from t = 1 to t = 0.\nThis is in alignment with the data prediction in diffusion models, and we have the bridge loss:\nLbridge = E(x0,y)\u223cpdata,x1=E(y)Et[\u2225x\u03b8(xt, t, x1) \u2212 x0\u22252\n2]\n(13)\nwhere xt = \u03b1t\u00af\u03c32\nt\n\u03c32\n1 x0 + \u00af\u03b1t\u03c32\nt\n\u03c32\n1 x1 + \u03b1t\u00af\u03c3t\u03c3t\n\u03c31\n\u03f5, \u03f5 \u223c N(0, I) by the SB (Eqn. (12)). x1 is also fed into the network as\na condition, following Grad-TTS [Popov et al., 2021].\nAnalogous to the different parameterizations in diffusion models, there are alternative choices of training\nobjectives that are equivalent in bridge training, such as the noise prediction corresponding to \u2207 log b\u03a8t [Liu\net al., 2023a] or the SB score \u2207 log pt, and the velocity prediction related to flow matching techniques [Lipman\net al., 2023]. However, we find they perform worse or poorly in practice, which we will discuss in detail in\nAppendix D. Except for the bridge loss, we jointly train the text encoder E (including the duration predictor\n\u02c6A) following Grad-TTS. Since the encoder no longer parameterizes a Gaussian distribution, we simply adopt\nan MSE encoder loss L\u2032\nenc = E(x0,y)\u223cpdata\u2225E(y) \u2212 x0\u22252. And we use the same duration prediction loss Ldp as\nGrad-TTS. The overall training objective of Bridge-TTS is Lbridge-tts = L\u2032\nenc + Ldp + Lbridge.\nIn our framework, the flexible form of reference SDE facilitates the design of noise schedules f, g, which\nconstitutes an important factor of performance as in SGMs. In this work, we directly transfer the well-behaved\nnoise schedules from SGMs, such as variance preserving (VP). As shown in Table 1, we set f, g2 linear to t,\nand the corresponding \u03b1t, \u03c32\nt have closed-form expressions. Such designs are new in both SB and TTS-related\ncontexts and distinguish our work from previous ones with Brownian bridges [Qiu et al., 2023, Tong et al.,\n2023a,b].\n5\nTable 1: Demonstration of the noise schedules in Bridge-TTS.\nSchedule\nf(t)\ng2(t)\n\u03b1t\n\u03c32\nt\nBridge-gmax1\n0\n\u03b20 + t(\u03b21 \u2212 \u03b20)\n1\n1\n2(\u03b21 \u2212 \u03b20)t2 + \u03b20t\nBridge-VP\n\u2212 1\n2(\u03b20 + t(\u03b21 \u2212 \u03b20))\n\u03b20 + t(\u03b21 \u2212 \u03b20)\ne\u2212 1\n2\nR t\n0 (\u03b20+\u03c4(\u03b21\u2212\u03b20))d\u03c4\ne\nR t\n0 (\u03b20+\u03c4(\u03b21\u2212\u03b20))d\u03c4 \u2212 1\n3.3\nSampling Scheme\nAssume we have a trained data prediction network x\u03b8(xt, t) 2. If we replace x0 with x\u03b8 in the tractable\nsolution of b\u03a8, \u03a8 (Eqn. (11)) and substitute them into Eqn. (6), which describes the SB with SDEs, we can\nobtain the parameterized SB process. Analogous to the sampling in diffusion models, the parameterized SB\ncan be described by both stochastic and deterministic processes, which we call bridge SDE/ODE, respectively.\nBridge SDE\nWe can follow the reverse SDE in Eqn. (6b). By substituting Eqn. (11) into it and replace x0\nwith x\u03b8, we have the bridge SDE:\ndxt =\n\u0014\nf(t)xt + g2(t)xt \u2212 \u03b1tx\u03b8(xt, t)\n\u03b12\nt \u03c32\nt\n\u0015\ndt + g(t)d \u00afwt\n(14)\nBridge ODE\nThe probability flow ODE [Song et al., 2021b] of the forward SDE in Eqn. (6a) is [Chen\net al., 2022a]:\ndxt =\n\u0014\nf(t)xt + g2(t)\u2207 log \u03a8t(xt) \u2212 1\n2g2(t)\u2207 log pt(xt)\n\u0015\ndt\n=\n\u0014\nf(t)xt + 1\n2g2(t)\u2207 log \u03a8t(xt) \u2212 1\n2g2(t)\u2207 log b\u03a8t(xt)\n\u0015\ndt\n(15)\nwhere we have used \u2207 log pt(xt) = \u2207 log \u03a8t(xt) + \u2207 log b\u03a8t(xt) since pt = \u03a8t b\u03a8t. By substituting Eqn. (11)\ninto it and replace x0 with x\u03b8, we have the bridge ODE:\ndxt =\n\u0014\nf(t)xt \u2212 1\n2g2(t)xt \u2212 \u00af\u03b1tx1\n\u03b12\nt \u00af\u03c32\nt\n+ 1\n2g2(t)xt \u2212 \u03b1tx\u03b8(xt, t)\n\u03b12\nt \u03c32\nt\n\u0015\ndt\n(16)\nTo obtain data sample x0, we can solve the bridge SDE/ODE from the latent x1 at t = 1 to t = 0. However,\ndirectly solving the bridge SDE/ODE may cause large errors when the number of steps is small. A prevalent\ntechnique in diffusion models is to handle them with exponential integrators [Gonzalez et al., 2023, Lu et al.,\n2022a,b, Zheng et al., 2023a], which aims to \u201ccancel\u201d the linear terms involving xt and obtain solutions with\nlower discretization error. We conduct similar derivations for bridge sampling, and present the results in the\nfollowing theorem.\nProposition 3.2 (Exact Solution and First-Order Discretization of Bridge SDE/ODE, proof in Appendix A.2).\nGiven an initial value xs at time s > 0, the solution at time t \u2208 [0, s] of bridge SDE/ODE is\nxt = \u03b1t\u03c32\nt\n\u03b1s\u03c32s\nxs \u2212 \u03b1t\u03c32\nt\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c34\u03c4\nx\u03b8(x\u03c4, \u03c4)d\u03c4 + \u03b1t\u03c3t\ns\n1 \u2212 \u03c32\nt\n\u03c32s\n\u03f5,\n\u03f5 \u223c N(0, I)\n(17)\nxt = \u03b1t\u03c3t\u00af\u03c3t\n\u03b1s\u03c3s\u00af\u03c3s\nxs + \u00af\u03b1t\u03c32\nt\n\u03c32\n1\n\u0012\n1 \u2212 \u03c3s\u00af\u03c3t\n\u00af\u03c3s\u03c3t\n\u0013\nx1 \u2212 \u03b1t\u03c3t\u00af\u03c3t\n2\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c33\u03c4 \u00af\u03c3\u03c4\nx\u03b8(x\u03c4, \u03c4)d\u03c4\n(18)\n1The main hyperparameter for the Bridge-gmax schedule is \u03b21, which is exactly the maximum of g2(t).\n2We omit the condition x1 for simplicity and other parameterizations such as noise prediction can be first transformed to x\u03b8.\n6\nThe first-order discretization (with the approximation x\u03b8(x\u03c4, \u03c4) \u2248 x\u03b8(xs, s) for \u03c4 \u2208 [t, s]) gives\nxt = \u03b1t\u03c32\nt\n\u03b1s\u03c32s\nxs + \u03b1t\n\u0012\n1 \u2212 \u03c32\nt\n\u03c32s\n\u0013\nx\u03b8(xs, s) + \u03b1t\u03c3t\ns\n1 \u2212 \u03c32\nt\n\u03c32s\n\u03f5,\n\u03f5 \u223c N(0, I)\n(19)\nxt = \u03b1t\u03c3t\u00af\u03c3t\n\u03b1s\u03c3s\u00af\u03c3s\nxs + \u03b1t\n\u03c32\n1\n\u0014\u0012\n\u00af\u03c32\nt \u2212 \u00af\u03c3s\u03c3t\u00af\u03c3t\n\u03c3s\n\u0013\nx\u03b8(xs, s) +\n\u0012\n\u03c32\nt \u2212 \u03c3s\u03c3t\u00af\u03c3t\n\u00af\u03c3s\n\u0013 x1\n\u03b11\n\u0015\n(20)\nTo the best of our knowledge, such derivations are revealed for the first time in the context of SB. We find\nthat the first-order discretization of bridge SDE (Eqn. (19)) recovers posterior sampling [Liu et al., 2023a] on\na Brownian bridge, and the first-order discretization of bridge ODE (Eqn. (20)) in the limit of \u03c3s\n\u03c31 , \u03c3t\n\u03c31 \u2192 0\nrecovers deterministic DDIM sampler [Song et al., 2021a] in diffusion models. Besides, we can easily discover\nthat the 1-step case of Eqn. (19) and Eqn. (20) are both 1-step deterministic prediction by x\u03b8. We put more\ndetailed analyses in Appendix B.2.\nWe can also develop higher-order samplers by taking higher-order Taylor expansions for x\u03b8 in the exact\nsolutions. We further discuss and take the predictor-corrector method as the second-order case in Appendix C.\nIn practice, we find first-order sampler is enough for the TTS task, and higher-order samplers do not make a\nsignificant difference.\n4\nExperiments\n4.1\nTraining Setup\nData\nWe utilize the LJ-Speech dataset [Ito and Johnson, 2017], which contains 13, 100 samples, around 24\nhours in total, from a female speaker at a sampling rate of 22.05 kHz. The test samples are extracted from\nboth LJ-001 and LJ-002, and the remaining 12577 samples are used for training. We follow the common\npractice, using the open-source tools [Park, 2019] to convert the English grapheme sequence to phoneme\nsequence, and extracting the 80-band mel-spectrogram with the FFT 1024 points, 80Hz and 7600Hz lower\nand higher frequency cutoffs, and a hop length of 256.\nModel training\nTo conduct a fair comparison with diffusion models, we adopt the same network architecture\nand training settings used in Grad-TTS [Popov et al., 2021]: 1) the encoder (i.e., text encoder and duration\npredictor) contains 7.2M parameters and the U-Net based decoder contains 7.6M parameters; 2) the model is\ntrained with a batch size of 16, and 1.7M iterations in total on a single NVIDIA RTX 3090, using 2.5 days; 3)\nthe Adam optimizer [Kingma and Ba, 2015] is employed with a constant learning rate of 0.0001. For noise\nschedules, we set \u03b20 = 0.01, \u03b21 = 20 for Bridge-VP (exactly the same as VP in SGMs) and \u03b20 = 0.01, \u03b21 = 50\nfor Bridge-gmax.\nEvaluation\nFollowing previous works [Huang et al., 2022, Liu et al., 2022a, Popov et al., 2021], we conduct\nthe subjective tests MOS (Mean Opinion Score) and CMOS (Comparison Mean Opinion Score) to evaluate\nthe overall subjective quality and comparison sample quality, respectively. To guarantee the reliability of\nthe collected results, we use the open platform Amazon Mechanical Turk, and require Master workers to\ncomplete the listening test. Specifically, the MOS scores of 20 test samples are given by 25 Master workers to\nevaluate the overall performance with a 5-point scale, where 1 and 5 denote the lowest (\u201cBad\") and highest\n(\u201cExcellent\") quality respectively. The result is reported with a 95% confidence interval. Each CMOS score is\ngiven by 15 Master workers to compare 20 test samples synthesized by two different models. More details of\nCMOS test can be visited in Appendix G. In both MOS and CMOS tests, each of the test samples has been\nnormalized for a fair comparison3. To measure the inference speed, we calculate the real-time factor (RTF)\non an NVIDIA RTX 3090.\n3https://github.com/slhck/ffmpeg-normalize\n7\n4.2\nResults and Analyses\nWe demonstrate the performance of Bridge-TTS on sample quality and inference speed separately, which\nguarantees a more precise comparison between multiple models. In Table 2 and Table 3, the test samples in\nLJ-Speech dataset are denoted as Recording, the samples synthesized from ground-truth mel-spectrogram by\nvocoder are denoted as GT-Mel+voc., and the number of function evaluations is denoted as NFE. We take the\npre-trained HiFi-GAN [Kong et al., 2020]4 as the vocoder, aligned with other baseline settings. More details\nof baseline models are introduced in Appendix F. In the sampling process of both tests, Grad-TTS employs\nODE sampling and sets the prior distribution pT = N(z, \u03c4 \u22121\nd I) with a temperature parameter \u03c4d = 1.5. In\nBridge-TTS, we use our first-order SDE sampler shown in Eqn. (19) with a temperature parameter \u03c4b = 2 for\nthe noise distribution \u03f5 = N(0, \u03c4 \u22121\nb\nI), which is helpful to the TTS quality in our observation.\nTable 2: The MOS comparison with 95% confidence\ninterval given numerous sampling steps.\nModel\nNFE\nRTF (\u2193)\nMOS (\u2191)\nRecording\n/\n/\n4.10 \u00b1 0.06\nGT-Mel + voc.\n/\n/\n3.93 \u00b1 0.07\nFastSpeech 2\n1\n0.004\n3.78 \u00b1 0.07\nVITS\n1\n0.018\n3.99 \u00b1 0.07\nDiffSinger\n71\n0.157\n3.92 \u00b1 0.06\nResGrad\n50\n0.135\n3.97 \u00b1 0.07\nGrad-TTS\n50\n0.116\n3.99 \u00b1 0.07\nOurs (VP)\n50\n0.117\n4.09 \u00b1 0.07\nOurs (gmax)\n50\n0.117\n4.07 \u00b1 0.07\nGrad-TTS\n1000\n2.233\n3.98 \u00b1 0.07\nOurs (VP)\n1000\n2.267\n4.05 \u00b1 0.07\nOurs (gmax)\n1000\n2.267\n4.07 \u00b1 0.07\nTable 3: The MOS comparison with 95% confidence\ninterval in few-step generation.\nModel\nNFE\nRTF (\u2193)\nMOS (\u2191)\nRecording\n/\n/\n4.12 \u00b1 0.06\nGT-Mel + voc.\n/\n/\n4.01 \u00b1 0.06\nFastSpeech 2\n1\n0.004\n3.84 \u00b1 0.07\nCoMoSpeech\n1\n0.007\n3.74 \u00b1 0.07\nProDiff\n2\n0.019\n3.67 \u00b1 0.07\nCoMoSpeech\n2\n0.009\n3.87 \u00b1 0.07\nOurs (gmax)\n2\n0.009\n4.04 \u00b1 0.06\nDiffGAN-TTS\n4\n0.014\n3.78 \u00b1 0.07\nGrad-TTS\n4\n0.013\n3.88 \u00b1 0.07\nFastGrad-TTS\n4\n0.013\n3.87 \u00b1 0.07\nResGrad\n4\n0.017\n4.02 \u00b1 0.06\nOurs (gmax)\n4\n0.013\n4.10 \u00b1 0.06\nGeneration quality\nTable 2 compares the generation quality between Bridge-TTS and previous TTS\nsystems. As shown, both Bridge-TTS models outperform three strong diffusion-based TTS systems: our\ndiffusion counterpart Grad-TTS [Popov et al., 2021], the shallow diffusion model DiffSinger [Liu et al., 2022a]\nand the residual diffusion model ResGrad [Chen et al., 2022c]. In comparison with the transformer-based\nmodel FastSpeech 2 [Ren et al., 2021] and the end-to-end TTS system [Kim et al., 2021], we also exhibit\nstronger subjective quality. When NFE is either 1000 or 50, our Bridge-TTS achieves superior quality. One\nreason is that the condition information (i.e., text encoder output) in TTS synthesis is strong, and the other\nis that our first-order Bridger sampler maintains the sample quality when reducing the NFE.\nSampling speed\nTable 3 shows the evaluation of sampling speed with the Bridge-TTS-gmax model, as we\nobserve that it achieves higher quality than the VP-based Bridge-TTS system. To conduct a fair comparison,\nwe choose the NFE reported in the baseline models. As shown, in 4-step sampling, we not only outperform\nour diffusion counterpart Grad-TTS [Popov et al., 2021], FastGrad-TTS [Vovk et al., 2022] using a first-order\nSDE sampler, and DiffGAN-TTS [Liu et al., 2022b] by a large margin, but also achieve higher quality than\nResGrad [Chen et al., 2022c] which stands on a pre-trained FastSpeech 2 [Ren et al., 2021]. In 2-step sampling\nwith a RTF of 0.009, we achieve higher quality than the state-of-the-art fast sampling method CoMoSpeech\n[Ye et al., 2023]. In comparison with 1-step method, FastSpeech 2 and CoMoSpeech, although our 2-step\ngeneration is slightly slower, we achieve distinctively better quality.\n4https://github.com/jik876/hifi-gan\n8\n4.3\nCase Study\nWe show a sample when NFE=4 in Figure 2 (a), using our first-order ODE sampler shown in Eqn (20).\nAs shown, Bridge-TTS clearly generates more details of the target than the diffusion counterpart Grad-TTS\n(\u03c4d = 1.5). Moreover, we show a 2-step ODE sampling trajectory of Bridge-TTS in Figure 2 (b). As shown,\nwith our data-to-data generation process, each sampling step is adding more details to refine the prior which\nhas provided strong information about the target. More generated samples can be visited in Appendix H.\nGrad-TTS (\ud835\udf0f = 1.5)\nBridge-TTS (ODE)\nGround-truth mel-spectrogram\nFirst sampling step\nPrior\nGround-truth mel-spectrogram\nSecond sampling step\n(a) 4-step generation result\n(b) 2-step ODE sampling trajectory\nFigure 2: We show a 4-step ODE generation result of Grad-TTS [Popov et al., 2021] and Bridge-TTS in\nthe left figure, and a 2-step ODE sampling trajectory of Bridge-TTS in the right one. The ground-truth\nmel-spectrogram is shown for comparison.\n4.4\nAblation Study\nWe conduct several comparison studies by showing the CMOS results between different designs of prior,\nnoise schedule, and sampler when NFE=1000 and NFE=4. The base setup is the Bridge-gmax schedule, x0\npredictor, and temperature-scaled first-order SDE sampler (\u03c4b = 2).\nPrior\nWe explore two training strategies that differ in their prior: 1) like Grad-TTS [Popov et al.,\n2021], the encoder and decoder part are joint trained from scratch (i.e., mutable prior); 2) the encoder\nis first trained with a warm-up stage and then the decoder is trained from scratch (i.e., fixed prior).\nTable 4: CMOS comparison of training and\nsampling settings of Bridge-TTS.\nMethod\nNFE=4\nNFE=1000\nBridge-TTS (gmax)\n0\n0\nw. mutable prior\n- 0.13\n- 0.17\nw. constant g(t)\n- 0.12\n- 0.14\nw. VP\n- 0.03\n- 0.08\nw. SDE (\u03c4b = 1)\n- 0.07\n- 0.19\nw. ODE\n- 0.10\n+ 0.00\nIt should be noted that in both strategies, the text encoder\nis trained with an equivalent objective. As shown, the latter\nconsistently has better sample quality across different NFEs.\nHence, we adopt it as our default setting.\nNoise schedule\nWe compare three different configurations\nfor noise schedules: Bridge-gmax, Bridge-VP, and a simple\nschedule with f(t) = 0, g(t) = 5 that has virtually the same\nmaximum marginal variance as Bridge-gmax, which we refer\nto as \u201cconstant g(t)\u201d. As shown in Table 4, Bridge-gmax\nand Bridge-VP have overall similar performance, while the\nconstant g(t) has noticeably degraded quality than Bridge-\ngmax when NFE=1000. Intuitively, the Bridge-gmax and Bridge-VP have an asymmetric pattern of marginal\nvariance that assigns more steps for denoising, while the constant g(t) yields a symmetric pattern. Empirically,\nsuch an asymmetric pattern of marginal variance helps improve sample quality. We provide a more detailed\nillustration of the noise schedules in Appendix E.\nSampling process\nFor comparison between different sampling processes, the temperature-scaled SDE (\u03c4b\n= 2) achieves the best quality at both NFE=4 and NFE=1000. Compared with the vanilla SDE sampling\n(i.e., \u03c4b = 1), introducing the temperature sampling technique for SDE can effectively reduce artifacts in\n9\nthe background and enhance the sample quality when NFE is large, which is clearly reflected in the CMOS\nscore in Table 4. Meanwhile, the ODE sampler exhibits the same quality as the temperature-scaled SDE at\nNFE=1000, but it has more evident artifacts at NFE=4.\n5\nRelated Work\nDiffusion-based TTS Synthesis\nGrad-TTS [Popov et al., 2021] builds a strong TTS baseline with SGMs,\nsurpassing the transformer-based [Ren et al., 2019] and flow-based model [Kim et al., 2020]. In the following\nworks, fast sampling methods are extensively studied, such as improving prior distribution [Lee et al., 2022],\ndesigning training-free sampler [Jeong et al., 2021, Vovk et al., 2022], using auxiliary model [Chen et al.,\n2022c, Liu et al., 2022a], introducing adversarial loss [Ko and Choi, 2023, Liu et al., 2022b], employing\nknowledge distillation [Huang et al., 2022, Ye et al., 2023], developing lightweight U-Net [Chen et al., 2023],\nand leveraging CFM framework [Guan et al., 2023, Guo et al., 2023, Mehta et al., 2023]. However, these\nmethods usually explore to find a better trade-off between TTS quality and sampling speed than diffusion\nmodels instead of simultaneously improving both of them, and some of these methods require extra procedures,\nsuch as data pre-processing, auxiliary networks, and distillation stage, or prone to training instability. In\ncontrast to each of the previous methods that study a data-to-noise process, we present a novel TTS system\nwith a tractable Schrodinger bridge, demonstrating the advantages of the data-to-data process.\nSchrodinger bridge\nSolving the Schrodinger bridge problem with an iterative procedure to simulate\nthe intractable stochastic processes is widely studied [Chen et al., 2022a, De Bortoli et al., 2021, Liu et al.,\n2023d, Peluchetti, 2023, Shi et al., 2023, Vargas et al., 2021, Wang et al., 2021]. Two recent works [Liu et al.,\n2023a, Somnath et al., 2023] build the bridge in image translation and a biology task, while neither of them\ninvestigates the design space discussed in our work, which is of importance to sample quality and inference\nspeed.\n6\nConclusions\nWe present Bridge-TTS, a novel TTS method built on data-to-data process, enabling mel-spectrogram\ngeneration from a deterministic prior via Schrodinger bridge. Under our theoretically elaborated tractable,\nflexible SB framework, we exhaustively explore the design space of noise schedule, model parameterization,\nand stochastic/deterministic samplers. Experimental results on sample quality and sampling efficiency in TTS\nsynthesis demonstrate the effectiveness of our approach, which significantly outperforms previous methods\nand becomes a new baseline on this task. We hope our work could open a new avenue for exploiting the\nboard family of strong informative prior to further unleash the potential of generative models on a wide range\nof applications.\nReferences\nFan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun\nZhu. One transformer fits all distributions in multi-modal diffusion at scale. In International Conference\non Machine Learning, 2023.\nMari Paz Calvo and C\u00e9sar Palencia. A class of explicit multistep exponential integrators for semilinear\nproblems. Numerische Mathematik, 102:367\u2013381, 2006.\nJie Chen, Xingchen Song, Zhendong Peng, Binbin Zhang, Fuping Pan, and Zhiyong Wu. Lightgrad: Lightweight\ndiffusion probabilistic model for text-to-speech. In IEEE International Conference on Acoustics, Speech,\nand Signal Processing, 2023.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad:\nEstimating gradients for waveform generation. In International Conference on Learning Representations,\n2021.\n10\nTianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schr\u00f6dinger bridge using\nforward-backward SDEs theory. In International Conference on Learning Representations, 2022a.\nZehua Chen, Xu Tan, Ke Wang, Shifeng Pan, Danilo P. Mandic, Lei He, and Sheng Zhao. Infergrad: Improving\ndiffusion models for vocoder by considering inference in training. In IEEE International Conference on\nAcoustics, Speech, and Signal Processing, 2022b.\nZehua Chen, Yihan Wu, Yichong Leng, Jiawei Chen, Haohe Liu, Xu Tan, Yang Cui, Ke Wang, Lei He, Sheng\nZhao, Jiang Bian, and Danilo P. Mandic. Resgrad: Residual denoising diffusion probabilistic models for\ntext to speech. arXiv preprint arXiv:2212.14518, 2022c.\nValentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with\napplications to score-based generative modeling. Advances in Neural Information Processing Systems, 2021.\nMartin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, and Nader Masmoudif. Seeds: Ex-\nponential sde solvers for fast high-quality sampling from diffusion models. arXiv preprint arXiv:2305.14267,\n2023.\nWenhao Guan, Qi Su, Haodong Zhou, Shiyu Miao, Xingjia Xie, Lin Li, and Qingyang Hong. Reflow-tts: A\nrectified flow model for high-fidelity text-to-speech. arXiv preprint arXiv:2309.17056, 2023.\nYiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, and Kai Yu. Voiceflow: Efficient text-to-speech with\nrectified flow matching. arXiv preprint arXiv:2309.05027, 2023.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural\nInformation Processing Systems, 2020.\nMarlis Hochbruck, Alexander Ostermann, and Julia Schweitzer. Exponential rosenbrock-type methods. SIAM\nJournal on Numerical Analysis, 47(1):786\u2013803, 2009.\nEmiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: end-to-end diffusion for high\nresolution images. In International Conference on Machine Learning, 2023.\nJiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin,\nZejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-to-audio generation. arXiv preprint\narXiv:2305.18474, 2023a.\nRongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi Ren. Prodiff: Progressive fast\ndiffusion model for high-quality text-to-speech. In ACM Multimedia, 2022.\nRongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu,\nXiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion\nmodels. In International Conference on Machine Learning, 2023b.\nKeith Ito and Linda Johnson. The LJ speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\nKiyosi It\u00f4. On a formula concerning stochastic differentials. Nagoya Mathematical Journal, 3:55\u201365, 1951.\nMyeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-tts: A\ndenoising diffusion model for text-to-speech. In INTERSPEECH, 2021.\nJaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-tts: A generative flow for text-to-speech\nvia monotonic alignment search. Advances in Neural Information Processing Systems, 2020.\nJaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning\nfor end-to-end text-to-speech. In International Conference on Machine Learning, 2021.\nDiederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In Advances\nin Neural Information Processing Systems, 2021.\n11\nD.P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on\nLearning Representations, 2015.\nMyeongjin Ko and Yong-Hoon Choi. Adversarial training of denoising diffusion model using dual discriminators\nfor high-fidelity multi-speaker tts. arXiv preprint arXiv:2308.01573, 2023.\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and\nhigh fidelity speech synthesis. In Advances in Neural Information Processing Systems, 2020.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion\nmodel for audio synthesis. In International Conference on Learning Representations, 2021.\nSang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh\nYoon, and Tie-Yan Liu. Priorgrad: Improving conditional denoising diffusion models with data-driven\nadaptive prior. In International Conference on Learning Representations, 2022.\nYichong Leng, Zehua Chen, Junliang Guo, Haohe Liu, Jiawei Chen, Xu Tan, Danilo P. Mandic, Lei He,\nXiang-Yang Li, Tao Qin, Sheng Zhao, and Tie-Yan Liu. Binauralgrad: A two-stage conditional diffusion\nprobabilistic model for binaural audio synthesis. In Advances in Neural Information Processing Systems,\n2022.\nChristian L\u00e9onard. Some properties of path measures. S\u00e9minaire de Probabilit\u00e9s XLVI, pages 207\u2013230, 2014.\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for\ngenerative modeling. In International Conference on Learning Representations, 2023.\nGuan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar.\nI2sb: Image-to-image schr\u00f6dinger bridge. In International Conference on Machine Learning, 2023a.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D.\nPlumbley. Audioldm: Text-to-audio generation with latent diffusion models. In International Conference\non Machine Learning, 2023b.\nHaohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang,\nYuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised\npretraining. arXiv preprint arXiv:2308.05734, 2023c.\nJinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis via shallow\ndiffusion mechanism. In AAAI Conference on Artificial Intelligence, 2022a.\nSongxiang Liu, Dan Su, and Dong Yu. Diffgan-tts: High-fidelity and efficient text-to-speech with denoising\ndiffusion gans. arXiv preprint arXiv:2201.11972, 2022b.\nXingchao Liu, Lemeng Wu, Mao Ye, and qiang liu. Learning diffusion bridges on constrained domains. In\nInternational Conference on Learning Representations, 2023d.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ODE\nsolver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural Information\nProcessing Systems, 2022a.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for\nguided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b.\nShivam Mehta, Ruibo Tu, Jonas Beskow, \u00c9va Sz\u00e9kely, and Gustav Eje Henter. Matcha-tts: A fast tts\narchitecture with conditional flow matching. arXiv preprint arXiv:2309.03199, 2023.\nJongseok Park, Kyubyong & Kim. g2pe. https://github.com/Kyubyong/g2p, 2019.\nStefano Peluchetti. Diffusion bridge mixture transports, schr\u00f6dinger bridge problems and generative modeling.\narXiv, 2023.\n12\nVadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail A. Kudinov. Grad-tts: A\ndiffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, 2021.\nVadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, and Jiansheng Wei.\nDiffusion-based voice conversion with fast maximum likelihood sampling scheme. In International Conference\non Learning Representations, 2022.\nZhibin Qiu, Mengfan Fu, Fuchun Sun, Gulila Altenbek, and Hao Huang. Se-bridge: Speech enhancement\nwith consistent brownian bridge. arXiv preprint arXiv:2305.13796, 2023.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech: Fast,\nrobust and controllable text to speech. In Advances in Neural Information Processing Systems, 2019.\nYi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and\nhigh-quality end-to-end text to speech. In International Conference on Learning Representations, 2021.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International\nConference on Learning Representations, 2022.\nErwin Schr\u00f6dinger. Sur la th\u00e9orie relativiste de l\u2019\u00e9lectron et l\u2019interpr\u00e9tation de la m\u00e9canique quantique. In\nAnnales de l\u2019institut Henri Poincar\u00e9, volume 2, pages 269\u2013310, 1932.\nKai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian.\nNaturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. arXiv\npreprint arXiv:2304.09116, 2023.\nYuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge\nmatching. arXiv preprint arXiv:2303.16852, 2023.\nVignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and\nCharlotte Bunne. Aligned diffusion schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International\nConference on Learning Representations, 2021a.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In International Conference on\nLearning Representations, 2021b.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International\nConference on Machine Learning, 2023.\nXu Tan, Tao Qin, Frank K. Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv preprint\narXiv:2106.15561, 2021.\nAlexander Tong, Nikolay Malkin, Kilian FATRAS, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet,\nGuy Wolf, and Yoshua Bengio. Simulation-free schr\u00f6dinger bridges via score and flow matching. In ICML\nWorkshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023a.\nAlexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy\nWolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal\ntransport. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023b.\nFrancisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schr\u00f6dinger bridges via\nmaximum likelihood. Entropy, 23(9):1134, 2021.\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23:\n1661\u20131674, 2011.\n13\nIvan Vovk, Tasnima Sadekova, Vladimir Gogoryan, Vadim Popov, Mikhail Kudinov, and Jiansheng Wei. Fast\ngrad-tts: Towards efficient diffusion-based speech generation on cpu. In INTERSPEECH, 2022.\nGefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning via schr\u00f6dinger\nbridge. In International Conference on Machine Learning, 2021.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-\nfidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213,\n2023.\nZhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising\ndiffusion gans. In International Conference on Learning Representations, 2022.\nZhen Ye, Wei Xue, Xu Tan, Jie Chen, Qifeng Liu, and Yike Guo. Comospeech: One-step speech and singing\nvoice synthesis via consistency model. In ACM Multimedia, 2023.\nKaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with\nempirical model statistics. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.\nKaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood estimation\nfor diffusion odes. arXiv preprint arXiv:2305.03935, 2023b.\n14\nContents\n1\nIntroduction\n2\n2\nBackground\n3\n2.1\nDiffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nDiffusion-Based TTS Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.3\nSchrodinger Bridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n3\nBridge-TTS\n4\n3.1\nSchrodinger Bridge between Paired Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.2\nModel Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.3\nSampling Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nExperiments\n7\n4.1\nTraining Setup\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.2\nResults and Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3\nCase Study\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.4\nAblation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n5\nRelated Work\n10\n6\nConclusions\n10\nAppendix\n16\nA Proofs\n16\nA.1 Tractable Schrodinger Bridge between Gaussian-Smoothed Paired Data\n. . . . . . . . . . . . . . . . .\n16\nA.2 Bridge Sampling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB Relationship with Brownian Bridge, Posterior Sampling and DDIM\n19\nB.1\nSchrodinger Bridge Problem and Brownian Bridge\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nB.2\nPosterior Sampling on a Brownian Bridge and DDIM . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC High-Order Samplers\n21\nD Model Parameterization\n22\nE Forward Process\n23\nF Baseline Models\n24\nG Additional Results\n25\nG.1 CMOS Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nG.2 Preference Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nH Generated Samples\n26\n15\nA\nProofs\nA.1\nTractable Schrodinger Bridge between Gaussian-Smoothed Paired Data\nProof of Proposition 3.1. First, we conduct a similar transformation to Liu et al. [2023a], which reverses the\nforward-backward SDE system of the SB in Eqn. (6) and absorb the intractable term b\u03a8, \u03a8 into the boundary\ncondition. On one hand, by inspecting the backward SDE (Eqn. (6b)) and its corresponding PDE (the\nsecond equation in Eqn. (7)), we can discover that if we regard b\u03a8 as a probability density function (up to a\nnormalizing factor, which is canceled when we compute the score by operator \u2207 log), then the PDE of the\nbackward SDE is a realization of the following forward SDE due to the Fokker-Plank equation [Song et al.,\n2021b]:\ndxt = f(xt, t)dt + g(t)dwt,\nx0 \u223c b\u03a80,\n(21)\nand its associated density of xt is b\u03a8t. When we assume f(xt, t) = f(t)xt as a linear drift, then Eqn. (21)\nbecomes a narrow-sense linear SDE, whose conditional distribution b\u03a8t|0(xt|x0) is a tractable Gaussian, which\nwe will prove as follows.\nSpecifically, It\u00f4\u2019s formula [It\u00f4, 1951] tells us that, for a general SDE with drift \u00b5t and diffusion \u03c3t:\ndxt = \u00b5t(xt)dt + \u03c3t(xt)dwt\n(22)\nIf f(x, t) is a twice-differentiable function, then\ndf(xt, t) =\n\u0012\u2202f\n\u2202t (xt, t) + \u00b5t(xt)\u2202f\n\u2202x(xt, t) + \u03c32\nt (xt)\n2\n\u22022f\n\u2202x2 (xt, t)\n\u0013\ndt + \u03c3t(xt)\u2202f\n\u2202x(xt, t)dwt\n(23)\nDenote \u03b1t = e\nR t\n0 f(\u03c4)d\u03c4, if we choose f(x, t) =\nx\n\u03b1t , by It\u00f4\u2019s formula we have\nd\n\u0012xt\n\u03b1t\n\u0013\n= g(t)\n\u03b1t\ndwt\n(24)\nwhich clearly leads to the result\nxt\n\u03b1t\n\u2212 x0\n\u03b10\n\u223c N\n\u0012\n0,\nZ t\n0\ng2(\u03c4)\n\u03b12\u03c4\nd\u03c4I\n\u0013\n(25)\nIf we denote \u03c32\nt =\nR t\n0\ng2(\u03c4)\n\u03b12\u03c4 d\u03c4, finally we conclude that b\u03a8t|0(xt|x0) = N(\u03b1tx0, \u03b12\nt \u03c32\nt I).\nOn the other hand, due to the symmetry of the SB, we can reverse the time t by s = 1 \u2212 t and conduct\nsimilar derivations for \u03a8, which finally leads to the result \u03a8t|1(xt|x1) = N(\u00af\u03b1tx1, \u03b12\nt \u00af\u03c32\nt I).\nSince we have Gaussian boundary conditions:\npdata = b\u03a80\u03a80 = N(x0, \u03f52I),\npprior = b\u03a81\u03a81 = N(x1, \u03b12\n1\u03f52I)\n(26)\nDue to the properties of Gaussian distribution, it is intuitive to assume that the marginal distributions b\u03a80, \u03a81\nare also Gaussian. We parameterize them with undetermined mean and variance as follows:\nb\u03a80 = N(a, \u03c32I),\n\u03a81 = N(b, \u03b12\n1\u03c32I)\n(27)\nSince the conditional transitions b\u03a8t|0, \u03a8t|1 are known Gaussian as we have derived, the marginals at any\nt \u2208 [0, 1] are also Gaussian (which can be seen as a simple linear Gaussian model):\nb\u03a8t = N(\u03b1ta, (\u03b12\nt \u03c32 + \u03b12\nt \u03c32\nt )I),\n\u03a8t = N(\u00af\u03b1tb, (\u03b12\nt \u03c32 + \u03b12\nt \u00af\u03c32\nt )I)\n(28)\nThen we can solve the coefficients a, b, \u03c3 by the boundary conditions. Note that \u00af\u03c32\n0 = \u03c32\n1, \u00af\u03b10 =\n1\n\u03b11 , and the\nproduct of two Gaussian probability density functions is given by\nN(\u00b51, \u03c32\n1)N(\u00b52, \u03c32\n2) = N\n\u0012\u03c32\n2\u00b51 + \u03c32\n1\u00b52\n\u03c32\n1 + \u03c32\n2\n,\n\u03c32\n1\u03c32\n2\n\u03c32\n1 + \u03c32\n2\n\u0013\n(29)\n16\nWe have\n(b\u03a80\u03a80 = N(a, \u03c32I)N(\u00af\u03b10b, (\u03b12\n0\u03c32 + \u03b12\n0\u00af\u03c32\n0)I) = N(x0, \u03f52I)\nb\u03a81\u03a81 = N(\u03b11a, (\u03b12\n1\u03c32 + \u03b12\n1\u03c32\n1)I)N(b, \u03b12\n1\u03c32I) = N(x1, \u03b12\n1\u03f52I)\n(30)\n\u21d2\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n(\u03c32 + \u03c32\n1)a + \u03c32 b\n\u03b11\n2\u03c32 + \u03c32\n1\n= x0\n\u03b11\u03c32a + (\u03c32 + \u03c32\n1)b\n2\u03c32 + \u03c32\n1\n= x1\n\u03c32(\u03c32 + \u03c32\n1)\n2\u03c32 + \u03c32\n1\n= \u03f52\n\u21d2\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\na = x0 + \u03c32\n\u03c32\n1\n\u0012\nx0 \u2212 x1\n\u03b11\n\u0013\nb = x1 + \u03c32\n\u03c32\n1\n(x1 \u2212 \u03b11x0)\n\u03c32 = \u03f52 +\np\n\u03c34\n1 + 4\u03f54 \u2212 \u03c32\n1\n2\n(31)\nThe proof is then completed by substituting these solved coefficients back into Eqn. (28).\nA.2\nBridge Sampling\nFirst of all, we would like to give some background information about exponential integrators [Calvo and\nPalencia, 2006, Hochbruck et al., 2009], which are widely used in recent works concerning fast sampling of\ndiffusion ODE/SDEs [Gonzalez et al., 2023, Lu et al., 2022a,b, Zheng et al., 2023a]. Suppose we have an\nSDE (or equivalently an ODE by setting g(t) = 0):\ndxt = [a(t)xt + b(t)F\u03b8(xt, t)]dt + g(t)dwt\n(32)\nwhere F\u03b8 is the parameterized prediction function that we want to approximate with Taylor expansion. The\nusual way of representing its analytic solution xt at time t with respect to an initial condition xs at time s is\nxt = xs +\nZ t\ns\n[a(\u03c4)x\u03c4 + b(\u03c4)F\u03b8(x\u03c4, \u03c4)]d\u03c4 +\nZ t\ns\ng(\u03c4)dw\u03c4\n(33)\nBy approximating the involved integrals in Eqn. (33), we can obtain direct discretizations of Eqn. (32) such as\nEuler\u2019s method. The key insight of exponential integrators is that, it is often better to utilize the \u201csemi-linear\u201d\nstructure of Eqn. (32) and analytically cancel the linear term a(t)xt. This way, we can obtain solutions that\nonly involve integrals of F\u03b8 and result in lower discretization errors. Specifically, by the \u201cvariation-of-constants\u201d\nformula, the exact solution of Eqn. (32) can be alternatively given by\nxt = e\nR t\ns a(\u03c4)d\u03c4xs +\nZ t\ns\ne\nR t\n\u03c4 a(r)drb(\u03c4)F\u03b8(x\u03c4, \u03c4)d\u03c4 +\nZ t\ns\ne\nR t\n\u03c4 a(r)drg(\u03c4)dw\u03c4\n(34)\nor equivalently (assume t < s)\nxt = e\nR t\ns a(\u03c4)d\u03c4xs +\nZ t\ns\ne\nR t\n\u03c4 a(r)drb(\u03c4)F\u03b8(x\u03c4, \u03c4)d\u03c4 +\ns\n\u2212\nZ t\ns\ne2\nR t\n\u03c4 a(r)drg2(\u03c4)d\u03c4\u03f5,\n\u03f5 \u223c N(0, I)\n(35)\nThen we prove Proposition 3.2 below.\nProof of Proposition 3.2. First, we consider the bridge SDE in Eqn. (14). By collecting the linear terms w.r.t.\nxt, the bridge SDE can be rewritten as\ndxt =\n\u0014\u0012\nf(t) + g2(t)\n\u03b12\nt \u03c32\nt\n\u0013\nxt \u2212 g2(t)\n\u03b1t\u03c32\nt\nx\u03b8(xt, t)\n\u0015\ndt + g(t)dwt\n(36)\nBy corresponding it to Eqn. (32), we have\na(t) = f(t) + g2(t)\n\u03b12\nt \u03c32\nt\n,\nb(t) = \u2212g2(t)\n\u03b1t\u03c32\nt\n(37)\n17\nThe exponents in Eqn. (35) can be calculated as\nZ t\ns\na(\u03c4)d\u03c4 =\nZ t\ns\nf(\u03c4)d\u03c4 +\nZ t\ns\n(\u03c32\n\u03c4)\u2032\n\u03c32\u03c4\nd\u03c4 =\nZ t\ns\nf(\u03c4)d\u03c4 + log \u03c32\nt\n\u03c32s\n(38)\nThus\ne\nR t\ns a(\u03c4)d\u03c4 = \u03b1t\u03c32\nt\n\u03b1s\u03c32s\n,\ne\nR t\n\u03c4 a(r)dr = \u03b1t\u03c32\nt\n\u03b1\u03c4\u03c32\u03c4\n(39)\nTherefore, the exact solution in Eqn. (35) becomes\nxt = \u03b1t\u03c32\nt\n\u03b1s\u03c32s\nxs \u2212 \u03b1t\u03c32\nt\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c34\u03c4\nx\u03b8(x\u03c4, \u03c4)d\u03c4 + \u03b1t\u03c32\nt\ns\n\u2212\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c34\u03c4\nd\u03c4\u03f5,\n\u03f5 \u223c N(0, I)\n(40)\nwhere\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c34\u03c4\nd\u03c4 =\nZ t\ns\n(\u03c32\n\u03c4)\u2032\n\u03c34\u03c4\nd\u03c4 = 1\n\u03c32s\n\u2212 1\n\u03c32\nt\n(41)\nSubstituting Eqn. (41) into Eqn. (40), we obtain the exact solution in Eqn. (17). If we take the first-order\napproximation (i.e., x\u03b8(x\u03c4, \u03c4) \u2248 x\u03b8(xs, s) for \u03c4 \u2208 [t, s]), then we obtain the first-order transition rule\nin Eqn. (19).\nThen we consider the bridge ODE in Eqn. (16). By collecting the linear terms w.r.t. xt, the bridge ODE\ncan be rewritten as\ndxt =\n\u0014\u0012\nf(t) \u2212 g2(t)\n2\u03b12\nt \u00af\u03c32\nt\n+ g2(t)\n2\u03b12\nt \u03c32\nt\n\u0013\nxt + g2(t)\u00af\u03b1t\n2\u03b12\nt \u00af\u03c32\nt\nx1 \u2212 g2(t)\n2\u03b1t\u03c32\nt\nx\u03b8(xt, t)\n\u0015\ndt\n(42)\nBy corresponding it to Eqn. (32), we have\na(t) = f(t) \u2212 g2(t)\n2\u03b12\nt \u00af\u03c32\nt\n+ g2(t)\n2\u03b12\nt \u03c32\nt\n,\nb1(t) = g2(t)\u00af\u03b1t\n2\u03b12\nt \u00af\u03c32\nt\n,\nb2(t) = \u2212 g2(t)\n2\u03b1t\u03c32\nt\n(43)\nThe exponents in Eqn. (35) can be calculated as\nZ t\ns\na(\u03c4)d\u03c4 =\nZ t\ns\nf(\u03c4)d\u03c4 \u2212\nZ t\ns\ng2(\u03c4)\n2\u03b12\u03c4 \u00af\u03c32\u03c4\nd\u03c4 +\nZ t\ns\ng2(\u03c4)\n2\u03b12\u03c4\u03c32\u03c4\nd\u03c4\n=\nZ t\ns\nf(\u03c4)d\u03c4 +\nZ t\ns\n(\u00af\u03c32\n\u03c4)\u2032\n2\u00af\u03c32\u03c4\nd\u03c4 +\nZ t\ns\n(\u03c32\n\u03c4)\u2032\n2\u03c32\u03c4\nd\u03c4\n=\nZ t\ns\nf(\u03c4)d\u03c4 + 1\n2 log \u00af\u03c32\nt\n\u00af\u03c32s\n+ 1\n2 log \u03c32\nt\n\u03c32s\n(44)\nThus\ne\nR t\ns a(\u03c4)d\u03c4 = \u03b1t\u03c3t\u00af\u03c3t\n\u03b1s\u03c3s\u00af\u03c3s\n,\ne\nR t\n\u03c4 a(r)dr = \u03b1t\u03c3t\u00af\u03c3t\n\u03b1\u03c4\u03c3\u03c4 \u00af\u03c3\u03c4\n(45)\nTherefore, the exact solution in Eqn. (35) becomes\nxt = \u03b1t\u03c3t\u00af\u03c3t\n\u03b1s\u03c3s\u00af\u03c3s\nxs + \u00af\u03b1t\u03c3t\u00af\u03c3t\n2\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c3\u03c4 \u00af\u03c33\u03c4\nx1d\u03c4 \u2212 \u03b1t\u03c3t\u00af\u03c3t\n2\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c33\u03c4 \u00af\u03c3\u03c4\nx\u03b8(x\u03c4, \u03c4)d\u03c4\n(46)\nDue the relation \u03c32\nt + \u00af\u03c32\nt = \u03c32\n1, the integrals can be computed by the substitution \u03b8t = arctan(\u03c3t/\u00af\u03c3t)\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c3\u03c4 \u00af\u03c33\u03c4\nd\u03c4 =\nZ t\ns\n(\u03c32\n\u03c4)\u2032\n\u03c3\u03c4 \u00af\u03c33\u03c4\nd\u03c4\n=\nZ \u03b8t\n\u03b8s\n1\n\u03c34\n1 sin \u03b8 cos3 \u03b8d(\u03c32\n1 sin2 \u03b8)\n= 2\n\u03c32\n1\nZ \u03b8t\n\u03b8s\n1\ncos2 \u03b8d\u03b8\n= 2\n\u03c32\n1\n(tan \u03b8t \u2212 tan \u03b8s)\n= 2\n\u03c32\n1\n\u0012\u03c3t\n\u00af\u03c3t\n\u2212 \u03c3s\n\u00af\u03c3s\n\u0013\n(47)\n18\nand similarly\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c33\u03c4 \u00af\u03c3\u03c4\nd\u03c4 = 2\n\u03c32\n1\n\u0012 \u00af\u03c3s\n\u03c3s\n\u2212 \u00af\u03c3t\n\u03c3t\n\u0013\n(48)\nSubstituting Eqn. (47) and Eqn. (48) into Eqn. (46), we obtain the exact solution in Eqn. (18). If we take the\nfirst-order approximation (i.e., x\u03b8(x\u03c4, \u03c4) \u2248 x\u03b8(xs, s) for \u03c4 \u2208 [t, s]), then we obtain the first-order transition\nrule in Eqn. (20).\nB\nRelationship with Brownian Bridge, Posterior Sampling and\nDDIM\nB.1\nSchrodinger Bridge Problem and Brownian Bridge\nFor any path measure \u00b5 on [0, 1], we have \u00b5 = \u00b50,1\u00b5|0,1, where \u00b50,1 denotes the joint distribution of\n\u00b50, \u00b51, and \u00b5|0,1 denotes the conditional path measure on (0, 1) given boundaries x0, x1. A high-level\nperspective is that, using the decomposition formula for KL divergence DKL(p \u2225 pref) = DKL(p0,1 \u2225 pref\n0,1) +\nDKL(p|0,1 \u2225 pref\n|0,1) [L\u00e9onard, 2014], the SB problem in Eqn. (5) can be reduced to the static SB prob-\nlem [De Bortoli et al., 2021, Shi et al., 2023, Tong et al., 2023a,b]:\nmin\np0,1\u2208P2 DKL(p0,1 \u2225 pref\n0,1),\ns.t. p0 = pdata, p1 = pprior\n(49)\nwhich is proved to be an entropy-regularized optimal transport problem when pref is defined by a scaled\nBrownian process dxt = \u03c3dwt. We can draw similar conclusions for the more general case of reference SDE\nin Eqn. (1) with linear drift f(xt, t) = f(t)xt. Specifically, the KL divergence between the joint distribution\nof boundaries is\nDKL(p0,1 \u2225 pref\n0,1) = \u2212Ep0,1[log pref\n0,1] \u2212 H(p0,1)\n= \u2212Ep0[log pref\n0 ] \u2212 Ep0,1[log pref\n1|0] \u2212 H(p0,1)\n(50)\nwhere H(\u00b7) is the entropy. As we have proved in Appendix A.1, pref\nt|0(xt|x0) = N(\u03b1tx0, \u03b12\nt \u03c32\nt I), thus\nlog pref\n1|0(x1|x0) = \u2212\u2225x1 \u2212 \u03b11x0\u22252\n2\n2\u03b12\n1\u03c32\n1\n(51)\nSince Ep0[log pref\n0 ] = Epdata[log pdata] is irrelevant to p, the static SB problem is equivalent to\nmin\np0,1\u2208P2 Ep0,1(x0,x1)[\u2225x1 \u2212 \u03b11x0\u22252\n2] \u2212 2\u03b12\n1\u03c32\n1H(p0,1),\ns.t. p0 = pdata, p1 = pprior\n(52)\nTherefore, it is an entropy-regularized optimal transport problem when \u03b11 = 1.\nWhile the static SB problem is generally non-trivial, there exists application cases when we can skip it:\nwhen the coupling p0,1 of pdata and pprior is unique and has no room for further optimization. (1) When pdata\nis a Dirac delta distribution and pprior is a usual distribution [Liu et al., 2023a]. In this case, the SB is half\ntractable, and only the bridge SDE holds. (2) When paired data are considered, i.e., the coupling of pdata\nand pprior is mixtures of dual Dirac delta distributions. In this case, however, DKL(p0,1 \u2225 pref\n0,1) = \u221e, and the\nSB problem will collapse. Still, we can ignore such singularity, so that the SB is fully tractable, and bridge\nODE can be derived.\nAfter the static SB problem is solved, we only need to minimize DKL(p|0,1 \u2225 pref\n|0,1) in order to solve the\noriginal SB problem. In fact, since there is no constraints, such optimization directly leads to pt|0,1 = pref\nt|0,1\nfor t \u2208 (0, 1).\nWhen pref is defined by a scaled Brownian process dxt = \u03c3dwt, pref\nt|0,1 is the common\nBrownian bridge [Qiu et al., 2023, Tong et al., 2023a,b]. When pref is defined by the narrow-sense linear SDE\ndxt = f(t)xtdt + g(t)dwt which we considered, pref\nt|0,1 can be seen as the generalized Brownian bridge with\nlinear drift and time-varying volatility, and we can derive its formula as follows.\nSimilar to the derivations in Appendix A.1, the transition probability from time s to time t (s < t)\nfollowing the reference SDE dxt = f(t)xtdt + g(t)dwt is\npref\nt|s(xt|xs) = N(xt; \u03b1t|sxs, \u03b12\nt|s\u03c32\nt|sI)\n(53)\n19\nwhere \u03b1t|s, \u03c3t|s are the corresponding coefficients to \u03b1t, \u03c3t, while modifying the lower limit of integrals from 0\nto s:\n\u03b1t|s = e\nR t\ns f(\u03c4)d\u03c4,\n\u03c32\nt|s =\nZ t\ns\ng2(\u03c4)\n\u03b12\n\u03c4|s\nd\u03c4\n(54)\nWe can easily identify that \u03b1t|s, \u03c3t|s are related to \u03b1t, \u03c3t by\n\u03b1t|s = \u03b1t\n\u03b1s\n,\n\u03c32\nt|s = \u03b12\ns(\u03c32\nt \u2212 \u03c32\ns)\n(55)\nTherefore\npref\nt|s(xt|xs) = N\n\u0012\nxt; \u03b1t\n\u03b1s\nxs, \u03b12\nt (\u03c32\nt \u2212 \u03c32\ns)I\n\u0013\n(56)\nDue to the Markov property of the SDE, we can compute pref\nt|0,1 as\npref\nt|0,1(xt|x0, x1) =\npref\nt,1|0(xt, x1|x0)\npref\n1|0(x1|x0)\n=\npref\nt|0(xt|x0)pref\n1|t(x1|xt)\npref\n1|0(x1|x0)\n\u221d\nexp\n\u0010\n\u2212 \u2225xt\u2212\u03b1tx0\u22252\n2\n2\u03b12\nt \u03c32\nt\n\u0011\nexp\n\u0012\n\u2212\n\u2225x1\u2212 \u03b11\n\u03b1t xt\u22252\n2\n2\u03b12\n1(\u03c32\n1\u2212\u03c32\nt )\n\u0013\nexp\n\u0010\n\u2212 \u2225x1\u2212\u03b11x0\u22252\n2\n2\u03b12\n1\u03c32\n1\n\u0011\n\u221d exp\n\u0012\n\u2212\u2225xt \u2212 \u03b1tx0\u22252\n2\n2\u03b12\nt \u03c32\nt\n\u2212 \u2225xt \u2212 \u00af\u03b1tx1\u22252\n2\n2\u03b12\nt \u00af\u03c32\nt\n\u0013\n\u221d exp\n\uf8eb\n\uf8ed\u2212\n\u2225xt \u2212 \u03b1t\u00af\u03c32\nt x0+\u00af\u03b1t\u03c32\nt x1\n\u03c32\n1\n\u22252\n2\n2 \u03b12\nt \u03c32\nt \u00af\u03c32\nt\n\u03c32\n1\n\uf8f6\n\uf8f8\n(57)\nTherefore, pref\nt|0,1 = N\n\u0010\n\u03b1t\u00af\u03c32\nt x0+\u00af\u03b1t\u03c32\nt x1\n\u03c32\n1\n, \u03b12\nt \u00af\u03c32\nt \u03c32\nt\n\u03c32\n1\nI\n\u0011\n, which equals the SB marginal in Eqn. (12).\nB.2\nPosterior Sampling on a Brownian Bridge and DDIM\nPosterior Sampling and Bridge SDE\nLiu et al. [2023a] proposes a method called posterior sampling to\nsample from bridge: when pref is defined by dxt = \u221a\u03b2tdwt, we can sample xN\u22121, . . . , xn+1, xn, . . . , x0 at\ntimesteps tN\u22121, . . . , tn+1, tn, . . . , t0 sequentially, where at each step the sample is generated from the DDPM\nposterior [Ho et al., 2020]:\np(xn|x0, xn+1) = N\n\u0012\nxn;\n\u03b12\nn\n\u03b12n + \u03c32n\nx0 +\n\u03c32\nn\n\u03b12n + \u03c32n\nxn+1,\n\u03c32\nn\u03b12\nn\n\u03b12n + \u03c32n\nI\n\u0013\n,\n(58)\nwhere \u03b12\nn =\nR tn+1\ntn\n\u03b2(\u03c4)d\u03c4 is the accumulated noise between two timesteps (tn, tn+1), \u03c32\nn =\nR tn\n0\n\u03b2(\u03c4)d\u03c4, and\nx0 is predicted by the network.\nWhile they only consider f(t) = 0 and prove the case for discrete timesteps by onerous mathematical\ninduction, such posterior is essentially a \u201cshortened\u201d Brownian bridge. Suppose we already draw a sample\nxs \u223c pref\ns|0,1, then the sample at time t < s can be drawn from pref\nt|0,1,s, which equals pref\nt|0,s due to the Markov\nproperty of the SDE. Similar to the derivation in Eqn. (57), such shortened Brownian bridge is\npref\nt|0,s(xt|x0, xs) = N\n \nxt;\n\u03b1t(\u03c32\ns \u2212 \u03c32\nt )x0 + \u03b1t\n\u03b1s \u03c32\nt xs\n\u03c32s\n, \u03b12\nt \u03c32\nt (\u03c32\ns \u2212 \u03c32\nt )\n\u03c32s\nI\n!\n(59)\nwhich is exactly the same as the first-order discretization of bridge SDE in Eqn. (19) when x0 is predicted by\nthe network x\u03b8(xs, s).\n20\nDDIM and Bridge ODE\nDDIM [Song et al., 2021a] is a sampling method for diffusion models, whose\ndeterministic case is later proved to be the first-order discretization of certain solution forms of the diffusion\nODE [Lu et al., 2022a,b]. Under our notations of \u03b1t, \u03c32\nt , the update rule of DDIM is [Lu et al., 2022b]\nxt = \u03b1t\u03c3t\n\u03b1s\u03c3s\nxs + \u03b1t\n\u0012\n1 \u2212 \u03c32\nt\n\u03c32s\n\u0013\nx\u03b8(xs, s)\n(60)\nIn the limit of \u03c3s\n\u03c31 , \u03c3t\n\u03c31 \u2192 0, we have \u00af\u03c3s\n\u03c31 , \u00af\u03c3t\n\u03c31 \u2192 1. Therefore,\n\u00af\u03c3t\n\u00af\u03c3s \u2192 1, and we can discover that Eqn. (20)\nreduces to Eqn. (60).\nCorollary B.1 (1-step First-Order Bridge SDE/ODE Sampler Recovers Direct Data Prediction). When\ns = 1 and t = 0, the first-order discretization of bridge SDE/ODE is\nx0 = x\u03b8(x1, 1)\n(61)\nC\nHigh-Order Samplers\nWe can develop high-order samplers by approximating x\u03b8(x\u03c4, \u03c4), \u03c4 \u2208 [t, s] with high-order Taylor ex-\npansions. Specifically, we take the second-order case of the bridge SDE as an example. For the integral\nR t\ns\ng2(\u03c4)\n\u03b12\u03c4 \u03c34\u03c4 x\u03b8(x\u03c4, \u03c4)d\u03c4 in Eqn. (17), we can use the change-of-variable \u03bbt = \u2212 1\n\u03c32\nt . Since (\u03bbt)\u2032 = g2(t)\n\u03b12\nt \u03c34\nt , the integral\nbecomes\nZ t\ns\ng2(\u03c4)\n\u03b12\u03c4\u03c34\u03c4\nx\u03b8(x\u03c4, \u03c4)d\u03c4 =\nZ \u03bbt\n\u03bbs\nx\u03b8(x\u03c4\u03bb, \u03c4\u03bb)d\u03bb\n\u2248\nZ \u03bbt\n\u03bbs\nx\u03b8(xs, s) + (\u03bb \u2212 \u03bbs)x(1)\n\u03b8 (xs, s)d\u03bb\n= (\u03bbt \u2212 \u03bbs)x\u03b8(xs, s) + (\u03bbt \u2212 \u03bbs)2\n2\nx(1)\n\u03b8 (xs, s)\n(62)\nwhere \u03c4\u03bb is the inverse mapping of \u03bb\u03c4, x(1)\n\u03b8\nis the first-order derivative of x\u03b8 w.r.t \u03bb, and we have used the\nsecond-order Taylor expansion x\u03b8(x\u03c4\u03bb, \u03c4\u03bb) \u2248 x\u03b8(xs, s) + (\u03bb \u2212 \u03bbs)x(1)\n\u03b8 (xs, s). x(1)\n\u03b8\ncan be estimated by finite\ndifference, and a simple treatment is the predictor-corrector method. We first compute \u02c6xt by the first-order\nupdate rule in Eqn. (19), which is used to estimate x(1)\n\u03b8 (xs, s): x(1)\n\u03b8 (xs, s) \u2248 x\u03b8(\u02c6xt,t)\u2212x\u03b8(xs,s)\n\u03bbt\u2212\u03bbs\n. Substituting\nit into Eqn. (62), we have\nR t\ns\ng2(\u03c4)\n\u03b12\u03c4 \u03c34\u03c4 x\u03b8(x\u03c4, \u03c4)d\u03c4 \u2248 (\u03bbt \u2212 \u03bbs) x\u03b8(xs,s)+x\u03b8(\u02c6xt,t)\n2\nwhich literally can be seen as\nreplacing x\u03b8(xs, s) in Eqn. (19) with x\u03b8(xs,s)+x\u03b8(\u02c6xt,t)\n2\n. Similar derivations can be done for the bridge ODE.\nWe summarize the second-order samplers in Algorithm 1 and Algorithm 2.\nAlgorithm 1 Second-order sampler for the bridge SDE\nInput: Number of function evaluations (NFE) 2N, timesteps 1 = tN > tN\u22121 > \u00b7 \u00b7 \u00b7 > tn > tn\u22121 > \u00b7 \u00b7 \u00b7 > t0 =\n0, initial condition x1\n1: for n = N to 1 do\n2:\ns \u2190 tn\n3:\nt \u2190 tn\u22121\n4:\nPrediction: \u02c6xt \u2190 \u03b1t\u03c32\nt\n\u03b1s\u03c32s xs + \u03b1t\n\u0010\n1 \u2212 \u03c32\nt\n\u03c32s\n\u0011\nx\u03b8(xs, s) + \u03b1t\u03c3t\nq\n1 \u2212 \u03c32\nt\n\u03c32s \u03f5,\n\u03f5 \u223c N(0, I)\n5:\nCorrection: xt \u2190 \u03b1t\u03c32\nt\n\u03b1s\u03c32\ns xs + \u03b1t\n\u0010\n1 \u2212 \u03c32\nt\n\u03c32\ns\n\u0011\nx\u03b8(xs,s)+x\u03b8(\u02c6xt,t)\n2\n+ \u03b1t\u03c3t\nq\n1 \u2212 \u03c32\nt\n\u03c32\ns \u03f5,\n\u03f5 \u223c N(0, I)\n6: end for\nOutput: x0\n21\nAlgorithm 2 Second-order sampler for the bridge ODE\nInput: Number of function evaluations (NFE) 2N, timesteps 1 = tN > tN\u22121 > \u00b7 \u00b7 \u00b7 > tn > tn\u22121 > \u00b7 \u00b7 \u00b7 > t0 =\n0, initial condition x1\n1: for n = N to 1 do\n2:\ns \u2190 tn\n3:\nt \u2190 tn\u22121\n4:\nPrediction: \u02c6xt \u2190 \u03b1t\u03c3t\u00af\u03c3t\n\u03b1s\u03c3s\u00af\u03c3s xs + \u03b1t\n\u03c32\n1\nh\u0010\n\u00af\u03c32\nt \u2212 \u00af\u03c3s\u03c3t\u00af\u03c3t\n\u03c3s\n\u0011\nx\u03b8(xs, s) +\n\u0010\n\u03c32\nt \u2212 \u03c3s\u03c3t\u00af\u03c3t\n\u00af\u03c3s\n\u0011\nx1\n\u03b11\ni\n5:\nCorrection: xt \u2190 \u03b1t\u03c3t\u00af\u03c3t\n\u03b1s\u03c3s\u00af\u03c3s xs + \u03b1t\n\u03c32\n1\nh\u0010\n\u00af\u03c32\nt \u2212 \u00af\u03c3s\u03c3t\u00af\u03c3t\n\u03c3s\n\u0011\nx\u03b8(xs,s)+x\u03b8(\u02c6xt,t)\n2\n+\n\u0010\n\u03c32\nt \u2212 \u03c3s\u03c3t\u00af\u03c3t\n\u00af\u03c3s\n\u0011\nx1\n\u03b11\ni\n6: end for\nOutput: x0\nD\nModel Parameterization\nApart from x0 predictor x\u03b8 presented in Section 3.2, we can consider other parameterizations:\n\u2022 Noise predictor \u03f5b\u03a8\n\u03b8 corresponding to \u2207 log b\u03a8t = \u2212 xt\u2212\u03b1tx0\n\u03b12\nt \u03c32\nt\nthat used in I2SB [Liu et al., 2023a]. The\nprediction target of \u03f5b\u03a8\n\u03b8 is:\n\u03f5\nb\u03a8\n\u03b8 \u2192 xt \u2212 \u03b1tx0\n\u03b1t\u03c3t\n(63)\n\u2022 Noise predictor \u03f5SB\n\u03b8\ncorresponding to the score \u2207 log pt of the SB. Since \u2207 log pt(xt) = \u2212\nxt\u2212\n\u03b1t \u00af\u03c32\nt x0+ \u00af\n\u03b1t\u03c32\nt x1\n\u03c32\n1\n\u03b12\nt \u00af\u03c32\nt \u03c32\nt\n\u03c32\n1\n,\nthe prediction target of \u03f5SB\n\u03b8\nis\n\u03f5SB\n\u03b8\n\u2192\nxt \u2212 \u03b1t\u00af\u03c32\nt x0+\u00af\u03b1t\u03c32\nt x1\n\u03c32\n1\n\u03b1t\u00af\u03c3t\u03c3t\n\u03c31\n(64)\n\u2022 Velocity predictor v\u03b8 arising from flow matching techniques [Lipman et al., 2023, Tong et al., 2023a,b,\nZheng et al., 2023b], which aims to directly predict the drift of the PF-ODE:\nv\u03b8 \u2192 f(t)xt \u2212 1\n2g2(t)xt \u2212 \u00af\u03b1tx1\n\u03b12\nt \u00af\u03c32\nt\n+ 1\n2g2(t)xt \u2212 \u03b1tx0\n\u03b12\nt \u03c32\nt\n(65)\nEmpirically, across all parameterizations, we observe that the x0 predictor and the noise predictor \u03f5b\u03a8\n\u03b8 work\nwell in the TTS task and Table 5 shows that the x0 predictor is generally better in sample quality. Hence, we\nadopt the x0 predictor as the default training setup for Bridge-TTS. For the \u03f5SB\n\u03b8\npredictor and v\u03b8 predictor,\nwe find that they lead to poor performance on the TTS task. We can intuitively explain this phenomenon by\ntaking a simple case f(t) = 0, g(t) = \u03c3. In this case, we have xt = (1 \u2212 t)x0 + tx1 + \u03c3\np\nt(1 \u2212 t)\u03f5, \u03f5 \u223c N(0, I),\nand the prediction targets are\nx\u03b8 \u2192 x0\n\u03f5\nb\u03a8\n\u03b8 \u2192 xt \u2212 x0\n\u03c3\n\u221a\nt\n=\n\u221a\nt(x1 \u2212 x0) + \u03c3\n\u221a\n1 \u2212 t\u03f5\n\u03f5SB\n\u03b8\n\u2192 xt \u2212 (1 \u2212 t)x0 \u2212 tx1\n\u03c3\np\nt(1 \u2212 t)\n= \u03f5\np\nt(1 \u2212 t)v\u03b8 \u2192 (1 \u2212 2t)xt \u2212 (1 \u2212 t)x0 + tx1\n2\np\nt(1 \u2212 t)\n=\np\nt(1 \u2212 t)(x1 \u2212 x0) + \u03c3 1 \u2212 2t\n2\n\u03f5\n(66)\nTherefore, \u03f5SB\n\u03b8\nand v\u03b8 both predict \u03f5 when t \u2192 1, while x\u03b8 and \u03f5b\u03a8\n\u03b8 tends to predict x0, x1-related terms in\nsuch scenario. We can conclude that the former way of prediction is harmful on TTS task.\n22\nTable 5: CMOS comparison of different parameterizations of Bridge-TTS.\nMethod\nNFE=4\nNFE=1000\nBridge-TTS (gmax + x0 predictor)\n0\n0\nBridge-TTS (gmax + \u03f5\nb\u03a8\n\u03b8 predictor)\n- 0.15\n- 0.12\nE\nForward Process\nIn this section, we display the stochastic trajectory of the Bridge-SDE in Eqn. (14) and compare it with\nthe diffusion counterpart in Eqn. (4). In general, the marginal distribution of these SDEs shares the form\npt = N(xt; wtx0 + \u00afwtx1, \u02dc\u03c32\nt I). In Figure 3, we show the scaling factors wt and \u00afwt for x0 and x1 and the\nvariance \u02dc\u03c32\nt at time t. As described in Section 4.4, the Bridge-gmax and Bridge-VP have an asymmetric\npattern of marginal variance that uses more steps to denoise towards the ground truth x0, while the constant\ng(t) schedule specifies the same noise-additive and denoising steps. As a comparison, the diffusion-based\nmodel only performs denoising steps.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScaling Factor wt for x0\nGrad-TTS\nBridge-VP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScaling Factor wt for x1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVariance \nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBridge-gmax\nBridge-constant g(t)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nt\n0\n1\n2\n3\n4\n5\n6\nFigure 3: The scaling factor and variance in Grad-TTS and Bridge-TTS.\n23\nF\nBaseline Models\nApart from ground-truth recording and the sample synthesized by vocoder from ground-truth mel-\nspectrogram, we take seven diffusion-based TTS systems, one end-to-end TTS system, and one transformer-\nbased TTS model as our baseline models. We follow their official implementation or the settings reported in\ntheir publications to produce the results. We introduce each of our baseline models below:\n1. FastSpeech 2 [Ren et al., 2021] is one of the most popular non-autoregressive TTS models, and\nwidely used as the baseline in previous diffusion-based TTS systems [Chen et al., 2022c, Liu et al., 2022a, Ye\net al., 2023]. Following its original setting, we train the model with a batch size of 48 sentences and 160k\ntraining steps until convergence by using 8 NVIDIA V100 GPU.\n2. VITS [Kim et al., 2021] provides a strong baseline of end-to-end TTS systems and is widely taken\nas a baseline in TTS systems for sample quality comparison. Different from other baseline models using\npre-trained vocoder to generate waveform, VITS directly synthesizes waveform from text input. In training\nand testing, we follow their open-source implementation5.\n3. DiffSinger [Liu et al., 2022a] is a TTS model developed for TTS synthesis and text-to-singing synthesis.\nIt is built on denoising diffusion probabilistic models [Ho et al., 2020], using standard Gaussian noise N(0, I)\nin the diffusion process. Moreover, an auxiliary model is trained to enable its shallow reverse process,\ni.e., reducing the distance between prior distribution and data distribution. We follow their open-source\nimplementation6, which contains a warm-up stage for auxiliary model training and a main stage for diffusion\nmodel training.\n4. DiffGAN-TTS [Liu et al., 2022b]7 develops expressive generator and time-dependent discriminator\nto learn the non-Gaussian denoising distribution [Xiao et al., 2022] in few-step sampling process of diffusion\nmodels. Following their publication, we train DiffGAN-TTS with time steps T = 4. For both the generator\nand the discriminator, we use the Adam optimizer, with \u03b21 = 0.5 and \u03b22 = 0.9. Models are trained using a\nsingle NVIDIA V100 GPU. We set the batch size as 32, and train models for 400k steps until loss converges.\n5. ProDiff [Huang et al., 2022] is a fast TTS model using progressive distillation [Salimans and Ho, 2022].\nThe standard Gaussian noise N(0, I) is used in the diffusion process and taken as the prior distribution. We\nuse their 2-step diffusion-based student model, which is distilled from a 4-step diffusion-based teacher model\n(x0 prediction). We follow their open-source implementation8.\n6. Grad-TTS [Popov et al., 2021]9 is a widely used baseline in diffusion models [Chen et al., 2023, 2022c,\nHuang et al., 2022, Ye et al., 2023] and conditional flow matching [Guo et al., 2023, Mehta et al., 2023] based\nTTS systems. It is established on SGMs, providing a strong baseline of generation quality. Moreover, it\nrealizes fast sampling with the improved prior distribution N(\u00b5, I) and the temperature parameter \u03c4 = 1.5\nin inference. Following its original setting and publicly available implementation, we train the model with a\nbatch size of 16 and 1.7 million steps on 1 NVIDIA 2080 GPU. The Adam optimizer is used and the learning\nrate is set to a constant, 0.0001.\n7.\nFastGrad-TTS [Vovk et al., 2022] equips pre-trained Grad-TTS [Popov et al., 2021] with the\nfirst-order SDE sampler proposed by [Popov et al., 2022]. The Maximum Likelihood solver reduces the\nmismatch between the reverse and the forward process. In comparison with the first-order Euler scheme, this\nsolver has shown improved quality in both voice conversion and TTS synthesis. We implement it for the\npre-trained Grad-TTS model with the Equation (6)-(9) in its publication.\n8. ResGrad [Chen et al., 2022c] is a diffusion-based post-processing module to improve the TTS sample\nquality, where the residual information of a pre-trained FastSpeech 2 [Ren et al., 2021] model is generated by\na diffusion model. The standard Gaussian noise N(0, I) is used in the diffusion process and taken as prior.\nWe invite the authors to generate some test samples for us.\n9. CoMoSpeech [Ye et al., 2023]10 is a recent fast sampling method in TTS and text-to-singing synthesis,\nachieving one-step generation with the distillation technique in consistency models [Song et al., 2023]. As\nGrad-TTS is employed as its TTS backbone, the model uses N(\u00b5, I) as prior distribution and is trained for\n5https://github.com/jaywalnut310/vits\n6https://github.com/MoonInTheRiver/DiffSinger\n7https://github.com/keonlee9420/DiffGAN-TTS\n8https://github.com/Rongjiehuang/ProDiff\n9https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS\n10https://github.com/zhenye234/CoMoSpeech\n24\n1.7 million iterations on a single NVIDIA A100 GPU with a batch size of 16. The Adam optimizer is adopted\nwith a learning rate 0.0001.\nG\nAdditional Results\nG.1\nCMOS Test\nTable 6: CMOS criteria.\nComparison Quality\nLeft better\nEqual\nRight better\nRating\n+3\n+2\n+1\n0\n-1\n-2\n-3\nWe conduct the Comparison Mean Opinion Score (CMOS) test by 15 Master workers on Amazon\nMechanical Turk to compare the generation quality of two different models. The raters give a score from\n+3 (left better) to -3 (right better) with 1 point increments on each of the 20 pair samples generated by\ntwo models, as shown in Table 6. The three scores 3, 2, 1 denotes much better, better, and slightly better,\nrespectively. The plus sign + and the minus sign \u2212 denotes the left and the right model respectively.\nTable 7: CMOS comparison between Grad-TTS and Bridge-TTS.\nMethod\nCMOS (\u2191)\nGrad-TTS\n0\nBridge-TTS (gmax)\n+0.21\nTo further demonstrate the advantage of data-to-data process in Bridge-TTS over the data-to-noise process\nin Grad-TTS, we conduct a CMOS test between Bridge-TTS (Bridge-gmax schedule with \u03b20 = 0.01, \u03b21 = 50,\nx0 predictor, and first-order SDE sampler with \u03c4b = 2) and Grad-TTS (\u03c4d = 1.5). As shown in Table 7, our\nBridge-TTS distinctively outperforms our diffusion counterpart Grad-TTS.\nG.2\nPreference Test\nApart from using the MOS and CMOS tests to evaluate sample quality, we conducted a blind preference\ntest when NFE=1000 and NFE=2, in order to demonstrate our superior generation quality and efficient\nsampling process, respectively. In each test, we generated 100 identical samples with two different models\nfrom the test set LJ001 and LJ002, and invited 11 judges to compare their overall subjective quality. The\njudge gives a preference when he thinks a model is better than the other, and an identical result when he\nthinks it is hard to tell the difference or the models have similar overall quality. In both preference tests, the\nsettings of noise schedule, model parameterization and sampling process in Bridge-TTS are Bridge-gmax\nschedule with \u03b20 = 0.01, \u03b21 = 50, x0 predictor, and first-order SDE sampler with \u03c4b = 2, respectively.\nIn the case of NFE=1000, as shown in Figure 4 (a), when Bridge-TTS-1000 is compared with our diffusion\ncounterpart Grad-TTS-1000 [Popov et al., 2021] (temperature \u03c4d = 1.5), 8 of the 11 invited judges vote for\nBridge-TTS-1000, and 3 of them think the overall quality is similar. In our blind test, none of the 11 judges\npreferred Grad-TTS-1000 to Bridge-TTS-1000. The comparison result is aligned with the MOS test shown in\nTable and CMOS test shown in Table 7.\nIn the case of NFE=2, as shown in Figure 4 (b), when Bridge-TTS-2 is compared with state-of-the-art\nfast sampling method in diffusion-based TTS systems, CoMoSpeech (1-step generation) [Ye et al., 2023],\n9 of the 11 invited judges vote for Bridge-TTS-2, and 2 of the judges vote for CoMoSpeech-1. Although\nBridge-TTS employs 2 sampling steps while CoMoSpeech-1 only uses 1, the RTF of both methods have\nbeen very small (0.007 for CoMoSpeech-1 vs 0.009 for Bridge-TTS-2), and Bridge-TTS does not require any\ndistillation process. According to our collected feedback, 9 judges think the overall quality (e.g., quality,\nnaturalness, and accuracy) of Bridge-TTS is significantly better.\n25\n(a)\n(b)\nFigure 4: The preference test between Bridge-TTS and diffusion-based TTS systems.\nH\nGenerated Samples\nWith the pre-trained HiFi-GAN [Kong et al., 2020] vocoder, we show the 80-band mel-spectrogram\nof several synthesized test samples of baseline models and our Bridge-TTS (Bridge-gmax schedule with\n\u03b20 = 0.01, \u03b21 = 50, x0 predictor, and first-order SDE sampler with \u03c4b = 2) below. The mel-spectrogram\nof ground-truth recording is shown for comparison. More generated speech samples can be visited on our\nwebsite: https://bridge-tts.github.io/.\n26\n1000-step generation\nAs exhibited in Figure 5a and Figure 5b, when NFE=1000, our method generates\nhigher-quality speech than Grad-TTS (temperature \u03c4d = 1.5) built on data-to-noise process, demonstrating\nthe advantage of our proposed data-to-data process over data-to-noise process in TTS.\nGrad-TTS\nRecording\nBridge-TTS\n(a)\nGrad-TTS\nBridge-TTS\nRecording\n(b)\nFigure 5: The mel-spectrogram of synthesized (NFE=1000) and ground-truth LJ001-0006 and LJ002-0029.\n27\n50-Step Generation\nIn Figure 6a, our method shows higher generation quality than Grad-TTS [Popov\net al., 2021]. In Figure 6b, we continue to use the test sample LJ002-0029 to demonstrate our performance. In\ncomparison with NFE=1000 shown in Figure 5b, when reducing NFE from 1000 to 50, Grad-TTS generates\nfewer details and sacrifices the sample quality, while our method still generates high-quality samples.\nGrad-TTS\nBridge-TTS\nRecording\n(a)\nGrad-TTS\nBridge-TTS\nRecording\n(b)\nFigure 6: The mel-spectrogram of synthesized (NFE=50) and ground-truth LJ001-0035 and LJ002-0029.\n28\n4-Step Generation\nIn Figure 7, we show our comparison with two baseline models, i.e., Grad-TTS [Popov\net al., 2021] and FastGrad-TTS [Vovk et al., 2022]. The latter one employs a first-order maximum-likelihood\nsolver [Popov et al., 2022] for the pre-trained Grad-TTS, and reports stronger quality than Grad-TTS in\n4-step synthesis. In our observation, when NFE=4, FastGrad-TTS achieves higher quality than Grad-TTS,\nwhile our method Bridge-TTS achieves higher generation quality than both of them, demonstrating the\nadvantage of our proposed data-to-data process on sampling efficiency in TTS synthesis.\nGrad-TTS\nFastGrad-TTS\nBridge-TTS\nRecording\nFigure 7: The mel-spectrogram of synthesized (NFE=4) and ground-truth sample LJ001-0032.\n29\n2-Step Generation\nWhen NFE is reduced to 2, we compare our method Bridge-TTS with the transformer-\nbased model FastSpeech 2 [Ren et al., 2021] and two diffusion-based TTS systems using distillation techniques.\nProDiff [Huang et al., 2022] employs progressive distillation achieving 2-step generation. CoMoSpeech [Ye\net al., 2023] employs consistency distillation achieving 1-step generation. In our observation, in this case, the\nRTF of each model has been very small, and the overall generation quality is reduced. In the subjective test,\nour Bridge-TTS outperforms the other three methods. We show a short test sample, LJ001-0002, in Figure 8.\nCoMoSpeech (NFE=1)\nFastSpeech 2\nProDiff (NFE=2)\nBridge-TTS (NFE=2)\nRecording\nFigure 8: The mel-spectrogram of synthesized (NFE\u22642) and ground-truth sample LJ001-0002.\n30\n"
  },
  {
    "title": "Relightable Gaussian Codec Avatars",
    "link": "https://arxiv.org/pdf/2312.03704.pdf",
    "upvote": "27",
    "text": "Relightable Gaussian Codec Avatars\nShunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam\nCodec Avatars Lab, Meta\nRelightable head avatars\nVideo driven animation\n(animated avatar/input HMC images)\nEnv. map relighting\nClose-ups\nIntrinsic decomp.\n(normal/albedo/diffuse/specular)\nFigure 1. Relightable Gaussian Codec Avatars. Our approach enables real-time relighting of human head avatars with all-frequency\nreflections and detailed hair reconstruction using 3D Gaussians and learnable radiance transfer. Our dynamic avatars can be driven live in\nreal-time from images captured with head mounted cameras (HMC). https://shunsukesaito.github.io/rgca/\nAbstract\nThe fidelity of relighting is bounded by both geometry\nand appearance representations. For geometry, both mesh\nand volumetric approaches have difficulty modeling intri-\ncate structures like 3D hair geometry.\nFor appearance,\nexisting relighting models are limited in fidelity and often\ntoo slow to render in real-time with high-resolution contin-\nuous environments. In this work, we present Relightable\nGaussian Codec Avatars, a method to build high-fidelity\nrelightable head avatars that can be animated to generate\nnovel expressions. Our geometry model based on 3D Gaus-\nsians can capture 3D-consistent sub-millimeter details such\nas hair strands and pores on dynamic face sequences. To\nsupport diverse materials of human heads such as the eyes,\nskin, and hair in a unified manner, we present a novel re-\nlightable appearance model based on learnable radiance\ntransfer. Together with global illumination-aware spheri-\ncal harmonics for the diffuse components, we achieve real-\ntime relighting with spatially all-frequency reflections us-\ning spherical Gaussians. This appearance model can be\nefficiently relit under both point light and continuous illu-\nmination. We further improve the fidelity of eye reflections\nand enable explicit gaze control by introducing relightable\nexplicit eye models. Our method outperforms existing ap-\nproaches without compromising real-time performance. We\nalso demonstrate real-time relighting of avatars on a teth-\nered consumer VR headset, showcasing the efficiency and\nfidelity of our avatars.\n1. Introduction\nWhat makes avatar relighting so challenging? Our visual\nperception is highly sensitive to facial appearance. Con-\nvincing the visual system requires modeling each part of\nthe head in sufficient detail that is coherent with an environ-\nment, and this synthesis typically needs to be performed in\nreal-time for primary applications of photorealistic avatars\nincluding games and telecommunication [39, 57].\nReal-\ntime relighting of animatable human heads with convincing\ndetails remains an open challenge for three reasons.\nThe first challenge is that human heads are composed\nof highly complex and diverse materials that exhibit differ-\nent properties of scattering and reflectance. For example,\nskin produces intricate reflections due to micro-geometry\nas well as significant subsurface scattering [53, 79], hair ex-\nhibits out-of-plane scattering with multiple reflections due\nto its translucent fiber structure [46], and the eyes have mul-\ntiple layers with highly reflective membranes [33, 68]. By\nand large, there is no single material representation that ac-\ncurately represents them all, especially in real-time. More-\nover, precise tracking and modeling of the underlying ge-\nometry in motion is extremely challenging because defor-\nmations do not always contain sufficient visual markers to\ntrack. Finally, the real-time requirement severely limits the\nalgorithmic design. Increase in photorealism traditionally\nresults in an exponential increase in the cost of transporting\nlight and tracking motion. Our goal is to design a learn-\n1\narXiv:2312.03704v1  [cs.GR]  6 Dec 2023\ning framework that builds real-time renderable head avatars\nwith accurate scattering and reflections under illuminations\nof any spatial frequency.\nGiven exhaustive measurements obtained using a light-\nstage [10, 16, 45], physically-based rendering methods [69,\n79] can generalize to novel illuminations. However, it re-\nmains non-trivial to extend these methods to dynamic per-\nformance capture and non-skin parts such as hair and eye-\nballs. Additionally, acquiring sufficiently accurate geom-\netry and material parameters is a laborious process with a\nsignificant amount of manual cleanup required [69].\nMore recently, neural relighting approaches sidestep the\nneed for accurate geometry and material modeling by only\nmodeling the direct relationship between the input (i.e., il-\nlumination) and output (i.e., outgoing radiance) with neu-\nral networks and approximated geometry using meshes [7],\nvolumetric primitives [20, 35, 88], and neural fields [33,\n67]. Typically these models are learned from one-light-at-\na-time (OLAT) [67] or grouped lights [7, 20, 88] controlled\nby a light-stage, and supporting real-time rendering with\ncontinuous illumination requires expensive teacher-student\ndistillation [7, 20] or physically-inspired appearance mod-\nels that explicitly maintain key properties of light transport\nsuch as linearity [33, 88]. Despite promising results, we\nobserve that the existing approaches lead to suboptimal per-\nformance due to insufficient expressiveness of both the ge-\nometric and appearance representations. In particular, none\nof the methods achieve all-frequency reflections on hair and\neyes, and submillimeter thin structures such as hair strands\nare often blurred out or glued into larger blobs, making hair\nrendering less than photorealistic.\nTo address the aforementioned issues, we present three\ncontributions: (1) drivable avatars based on 3D Gaussians\nthat can be efficiently rendered with intricate geometric de-\ntails, (2) a relightable appearance model based on learned\nradiance transfer that supports global light transport and all-\nfrequency reflections in real-time, and (3) a relightable ex-\nplicit eye model that enables disentangled control of gaze\nfrom other facial movements as well as all-frequency eye-\nreflections for the first time in a fully data-driven manner.\n3D Gaussian Avatars.\nOur geometric representation is\nbased on 3D Gaussians [24] that can be rendered in real-\ntime using splatting. To achieve a drivable avatar, we de-\ncode 3D Gaussians on a shared UV space for a template\nhead using 2D convolutional neural networks. We encode\nthe driving signals such as facial expressions in a self-\nsupervised manner akin to traditional codecs. This allows\nus to track the moving heads in a temporally coherent man-\nner with intricate geometric details such as hair strands.\nLearned Radiance Transfer.\nFor appearance, inspired\nby precomputed radiance transfer [70], we introduce a re-\nlightable appearance model based on learnable radiance\ntransfer that consists of diffuse spherical harmonics and\nspecular spherical Gaussians.\nWe learn diffuse radiance\ntransfer parameterized by dynamic spherical harmonics co-\nefficients for each 3D Gaussian.\nThis transfer precon-\nvolves visibility and global light transport, including multi-\nbounce and subsurface scattering.\nFor specular reflec-\ntion, we introduce a novel parameterization of spherical\nGaussians [75, 90] with view-dependent visibility that ef-\nfectively approximates the combined effects of occlusion,\nFresnel, and geometric attenuation without explicitly esti-\nmating the individual contributions.\nOur specular Gaus-\nsian lobe is aligned with the reflection vector and computed\nusing the view direction and per-Gaussian view-dependent\nnormals. Most importantly, spherical Gaussians support all-\nfrequency reflection under high-resolution illuminations in\nreal-time.\nBoth diffuse and specular representations sat-\nisfy the linearity of light transport, hence supporting real-\ntime rendering under both point lights and environment\nillumination without additional training. In addition, the\nproposed learnable radiance transfer supports global light\ntransport and all-frequency reflection of the eyes, skin, and\nhair with the unified representation, significantly simplify-\ning the learning process while achieving extremely high-\nfidelity relighting.\nRelightable Explicit Eye Model. To reproduce reflections\non the cornea, our relightable Gaussian avatar incorporates\nan explicit eye model [68] that also enables explicit control\nof the eyeballs with better disentanglement. In addition, our\nappearance model naturally supports relighting of the eyes\nwith all-frequency reflections, which is crucial for photore-\nalism under natural environments.\nWe run an evaluation of various pairs of geometry and\nrelightable appearance models in this work and other real-\ntime renderable baseline methods. Our experiments show\nthat the combination of 3D Gaussians with our relighting\nappearance model outperforms any other combination.\n2. Related Work\nFace Modeling. Facial avatar modeling has been an ac-\ntive research topic for over half a century [61]. We refer\nto [61] for a comprehensive overview on research and tools\nfor artist-friendly authoring of 3D facial models. Advance-\nments in image-based 3D reconstruction [12] enabled the\nprecise and more automated acquisition of 3D faces using\nmulti-view capture systems, especially in high-end film pro-\nduction [2, 4, 9, 16, 91]. These approaches primarily fo-\ncus on the facial skin region, and more tailored solutions\nare required for the reconstruction and modeling of differ-\nent components such as teeth [80], lips [15], facial hair [3],\neyes [5, 51], and hair [19, 43, 55, 60], which are difficult to\nscale for dynamic and complete head avatars.\nMore recently, learning-based approaches emerged to\nholistically represent human heads without requiring pre-\ncise input geometry [39, 44, 95, 97]. In particular, volu-\n2\nmetric representations [40, 50] show the promise of repre-\nsenting both skin and more complex geometric structures\nlike hair with a single representation [27, 95, 97]. To en-\nable real-time rendering with volumes, several hybrid ap-\nproaches are proposed to partition the space for efficient\nraymarching using mixture of volumetric primitives [41]\nor tetrahedra [14]. Point clouds are also utilized to model\nhead avatars [96]. However, we observe that the existing\nshape representations for real-time renderable avatars strug-\ngle with modeling extremely thin structures such as hair\nstrands. To address this limitation, we extend a state-of-\nthe-art efficient scene representation based on 3D Gaussian\nsplatting [24] to animatable facial avatar modeling. While\nseveral works already show dynamic modeling of 3D Gaus-\nsians [42, 81], we are the first to enable the modeling of an-\nimatable and (most importantly) relightable 3D Gaussians.\nFacial Reflectance Capture.\nIn the early 2000s, visual\nproduction was a great driver for facial reflectance cap-\nture and relighting research for composing actors into vir-\ntual environments. A seminal work by Debevec et al. [10]\ndemonstrated that one-light-at-a-time (OLAT) captures can\nbe used to obtain reflectance properties and relight faces\nin novel illuminations by leveraging the linearity of light\ntransport. Follow-up work further extended the method to\ndynamic relighting [62], and fast acquisition of reflectance\nmaps using spherical gradient illuminations [16, 18, 45].\nSubsequently, the collection of larger datasets allowed es-\ntimating reflectance from a single image using neural net-\nworks [31, 32, 34, 36, 59, 87].\nHowever, accurate re-\nflectance estimation is typically limited to skin regions be-\ncause the intricate hair and eye structure make the inverse\nrendering intractable. While inverse rendering with various\nscene representations has also been proposed to estimate\nspatially-varying BRDFs (SVBRDFs) [7, 52, 54, 90, 94],\nit remains a challenge to photorealistically model complete\nhuman heads due to the highly complex and diverse mate-\nrial and geometric composition. The lack of photorealism is\nalso evident in recent relightable head modeling in the wild\nusing simple BRDF and shading models [11, 65, 96].\nNeural Relighting.\nInstead of modeling BRDF param-\neters, learning-based relighting approaches attempt to di-\nrectly learn relightable appearance from a light-stage cap-\nture [13, 48, 49, 67, 84, 85, 93]. While these approaches\nshow promising relighting for static [67, 84, 85, 93] and dy-\nnamic scenes [48, 49], they do not support generating novel\nanimations, which is an essential requirement for avatars.\nPortrait relighting methods [58, 71, 72, 77, 89] also en-\nable relighting under novel illuminations given a single im-\nage. However, they cannot produce novel view synthesis or\ntemporally coherent dynamic relighting. Bi et al. [6] pro-\npose a neural-rendering method that supports global illu-\nmination for animatable facial avatars. To enable real-time\nrendering with natural environments, they distillate a slow\nteacher model conditioned with individual point lights into\na light-weight student model that can be conditioned with\nenvironment maps. This work is later extended to articu-\nlated hand modeling [20], compositional modeling of heads\nand eyeglasses [35], and scalable training by eliminating the\nneed of teacher-student distillation [88]. These relightable\navatars take as input the lighting information, which we dis-\ncover is the main limiting factor for expressiveness in all-\nfrequency relighting. In contrast, inspired by Precomputed\nRadiance Transfer (PRT) [70, 75], we propose to integrate a\ntarget illumination at the output of our neural decoder, im-\nproving quality and simplifying the learning pipeline.\nPrecomputed Radiance Transfer. In computer graphics,\nrendering a scene with global illumination is an expensive\nprocess due to iterative path tracing or multiple bounce\ncomputations. To enable real-time rendering with global\nlight transport, Sloan et al. [70] propose to precompute a\npart of light transport that only depends on intrinsic scene\nproperties, such as geometry and reflectance, and then inte-\ngrate the precomputed intrinsic factor with an extrinsic illu-\nmination at runtime. For fast integration, they utilize spher-\nical harmonics as an angular basis. To overcome the limited\nfrequency band in spherical harmonics, follow-up works in-\ntroduce wavelets [56], spherical radial basis functions [74],\nspherical Gaussians [17, 75], anisotropic spherical Gaus-\nsians [82], and neural network-based decompositions [86].\nSimilarly, Neural PRT [63] applies the same principle to\nscreen-space relighting based on neural deferred render-\ning [73]. Despite many desirable properties, these methods\nprimarily focus on static scenes due to the dependency on\nknowing geometry and material properties. Unfortunately,\nwe neither know the geometry and material properties for\nhuman heads a priori, nor are they static. Thus, we pro-\npose to learn the intrinsic radiance transfer from dynamic\nreal-data observations without explicitly assuming any ma-\nterial types or underlying geometry. The closest to our work\nin terms of the appearance representation is EyeNeRF [33],\nwhere they learn view-independent spherical harmonics for\ndiffuse and view-conditioned spherical harmonics for spec-\nular components from image observations to build a re-\nlightable eye model. However, this appearance model suf-\nfers from the limited expressiveness of spherical harmon-\nics for specular reflections. Empirically, we find that their\nproposed model does not generalize well to novel view and\nlight directions. Please refer to Sec. 4 for our analysis.\n3. Method\nIn this section, we provide details of the data acquisi-\ntion process (Sec. 3.1), geometry and appearance represen-\ntations (Sec. 3.2\u20133.3), the relightable explicit eye model\n(Sec. 3.4), as well as the training method (Sec. 3.5).\n3\n{\ud835\udf0e!, \ud835\udc11!, \ud835\udc2c!, \ud835\udc2d!, \ud835\udc5c!, \ud835\udc1d!\n\", \ud835\udc1d!\n#}\n\ud835\udc86!\n\ud835\udc86\"\n\ud835\udf4e#\n\ud835\udc33\n{\ud835\udc27!, \ud835\udc63!}\nDiffuse\nEq.(5)\n\ud835\uded2$\nSpecular\nEq.(8)\nSplatting\nEq.(2)\n\u2112!\"#\n\ud835\udc9f{\"#,\"%}\nGround truth\n\ud835\udc9f!\"\n\ud835\udc9f{$,!&}\n\ud835\udc84$\n%&'()*+,\ngeometry\nappearance\n\ud835\udc84$\n-.//)%'\n\ud835\udc84$\nEyeballs\nGaze\n(right)\nViewing\nangle\nGaze\n(left)\nLatent\nexpression\ncode\nLighting\nFigure 2. Overview. Given an expression latent code z, gaze e{l,r}, and view direction \u03c9o, our model decodes the parameters of 3D\nGaussians (rotation Rk, translation tk, scale sk, and opacity ok) and learned radiance transfer functions (colored and monochrome diffuse\nSH coefficients dc\nk, dm\nk , roughness \u03c3k, normal nk, and visibility vk). We integrate the decoded radiance transfer functions with the input\nlight to compute the final color ck, which we then render via splatting and supervise in image space. The geometry decoder Dg is described\nin Sec. 3.2, the appearance decoders D{ci,cv} in Sec. 3.3, and eyeball decoders D{ei,ev} in Sec. 3.4.\n3.1. Data Acquisition\nWe use a setup similar to [6, 35], where we capture cali-\nbrated and synchronized multiview images at 4096 \u00d7 2668\nresolution with 110 cameras and 460 white LED lights at 90\nHz. Each participant is asked to perform a predefined set of\nvarious facial expressions, sentences, and gaze motions for\nabout 144,000 frames. To collect diverse illumination pat-\nterns while enabling stable facial tracking, we employ time-\nmultiplexed illumination. In particular, the full-on illumi-\nnation is interleaved at every third frame to allow tracking,\nand the rest is lit with grouped or random sets of 5 lights.\nAs in [6, 35], we perform a topologically consistent\ncoarse mesh tracking using multi-view full-on images. We\nfurther stabilize the head pose using the mode pursuit\nmethod of Lamarre et al. [30]. We also estimate the eye\ngaze of both eyes using the method described in [68]. The\ntracked mesh, head pose, unwrapped averaged texture, and\ngaze are interpolated to adjacent partially lit frames for the\nfollowing avatar training.\n3.2. Geometry: 3D Gaussian Avatars\nThe core of our geometric representation is the mixture of\n3D anisotropic Gaussians [24], which supports representing\nvarying topology and can represent thin volumetric struc-\ntures. We first review the underlying parameterization and\nkey idea in 3D Gaussian Splatting [24] and then highlight\nmajor changes to enable animatable avatar modeling.\nWe render avatars as collections of 3D Gaussians, where\neach Gaussian gk = {tk, Rk, sk, ok, ck} is defined by a\ntranslation tk \u2208 R3, a rotation matrix Rk \u2208 SO(3) param-\neterized as a quaternion, per-axis scale factors sk \u2208 R3, an\nopacity value ok \u2208 R, and a color ck \u2208 R3. The spatial\nextent of each Gaussian is defined in 3D by its covariance\nmatrix \u03a3k = Rk diag(sk) diag(sk)T RT\nk .\nThis representation allows efficient rendering using the\nElliptical Weighted Average (EWA) splatting technique\nproposed by Zwicker et al. [98] by computing the 2D pro-\njection of each 3D Gaussian,\n\u03a3\u2032\nk = JV\u03a3kVT JT ,\n(1)\nwhere J \u2208 R2\u00d73 is the Jacobian of the projective trans-\nformation, V \u2208 R3\u00d73 the viewing transformation, and\n\u03a3\u2032\nk \u2208 R2\u00d72 is the covariance matrix of the projected 2D\nGaussian (a \u201csplat\u201d). The final color Cp at pixel p given\nN ordered splats is computed with point-based cumulative\nvolumetric rendering [24, 28, 29] as follows:\nCp =\nX\nk\u2208N\nck\u03b1k\nk\u22121\nY\nj=1\n(1 \u2212 \u03b1j) ,\n(2)\nwhere the transparency \u03b1k is evaluated using the 2D co-\nvariance \u03a3\u2032\nk and multiplied by the per-Gaussian opacity ok.\nPlease refer to [24] for more details.\nIn contrast to 3D Gaussian Splatting [24], which focuses\non static scene reconstruction, our objective is to build an\nanimatable 3D avatar representation that can span the dy-\nnamic facial expressions of a person and also be relit un-\nder novel illuminations. This necessitates a relightable ap-\npearance model to recolor ck of all Gaussians as a function\nof the environmental illumination, allowing for a realistic\nadaptation of the avatar\u2019s appearance under varying light-\ning conditions. Additionally, it is essential to register the\ngeometry {gk} for all Gaussians in response to the state\nof any facial expressions, ensuring that the avatar\u2019s expres-\nsions remain consistent with the user\u2019s actual facial move-\nments. Enabling the encoding and decoding of any facial\nmovements is crucial for animating and driving avatars.\n4\nTo this end, we parameterize the 3D Gaussians on a\nshared UV texture map of a coarse template mesh, and de-\ncode their transformation and opacity using 2D convolu-\ntional neural networks. As facial expressions are highly\nnon-linear and non-trivial to precisely define, in the spirit\nof Lombardi et al. [39] and Xu et al. [83], we employ a\nconditional variational auto-encoder (CVAE) [26] to learn\nthe latent distribution of facial expressions from the data.\nGiven the eye gaze directions of both eyes e{l,r} \u2208 R3 in\nhead-centric coordinates, coarse tracked mesh vertices V,\nan unwrapped averaged texture T, our encoder E and ge-\nometry decoder Dg are defined as:\n\u00b5e, \u03c3e = E(V, T; \u0398e),\n{\u03b4tk, Rk, sk, ok}M\nk=1 = Dg(z, e{l,r}; \u0398g),\n(3)\nwhere \u0398e and \u0398g are the learnable parameters for the en-\ncoder and decoder respectively, M is the total number of\nGaussians, and \u03c3e and \u00b5e are the mean and standard devia-\ntion of a normal distribution z \u223c N(\u00b5e, \u03c3e). The sampled\nlatent vector z \u2208 R256 is computed using the reparameteri-\nzation trick proposed by Kingma and Welling [26]. We also\ndecode mesh vertices from z such that we can animate the\navatars from a headset [78] or latent-space manipulation [1]:\nV\u2032 = Dv(z; \u0398v).\n(4)\nNote that while we directly infer the rotation R and scale\ns, we use the coarse geometry g as guidance to avoid poor\nlocal minima under large motions. The final Gaussian po-\nsition tk is computed as tk = \u02c6tk + \u03b4tk, where \u02c6tk is the\ninterpolated coarse mesh position of the corresponding UV-\ncoordinates using barycentric interpolation of the vertices\nin V\u2032. To assign UV-coordinates to Gaussians, we map one\nGaussian to each texel in the UV map.\nWhile the aforementioned parameterization is similar to\nthe Mixture of Volumetric Primitives (MVP) [41], where a\ncollection of voxel grids is anchored on a template mesh and\nused as the renderable primitive, there are two important\ndifferences: (1) Unlike MVP, which requires raymarching,\nthe 3D Gaussians can be more efficiently rendered using\nsplatting [24, 98]. (2) Additionally, the Gaussians have a\ngreater ability to recreate thin structures, yielding sharper\nappearance for hair, which we show in Sec. 4.\n3.3. Appearance: Learned Radiance Transfer\nAn appearance model for faces must accurately model a\nwide range of light transport effects, including subsurface\nscattering in skin, and specular reflections on the skin, eyes,\nas well as multi-bounced scattering on the hair. As dis-\ncussed in early works [70, 75], while a diffuse transfer oper-\nator is a low-pass filtering of incident illumination, a spec-\nular transfer operator requires the ability to represent all-\nfrequency information for mirror-like reflections. To effec-\ntively allocate the capacity of the network to each compo-\nnent, we decompose the final color ck of each 3D Gaus-\nsian into a view-independent diffuse term cdiffuse\nk\n, and a\nview-dependent specular term cspecular\nk\n(\u03c9o), such that ck =\ncdiffuse\nk\n+ cspecular\nk\n(\u03c9o), where \u03c9o is the viewing direction.\nDiffuse Color. Our diffuse term is based on spherical har-\nmonics (SH), and incorporates global light transport ef-\nfects including occlusion, subsurface scattering, and multi-\nbounce scattering [70].\nThe diffuse color contribution\ncdiffuse\nk\nis computed by the spherical integration of an (ex-\ntrinsic) incident illumination, L(\u00b7), with an intrinsic func-\ntion that models the radiance transfer, d(\u00b7). By represent-\ning both functions in the SH basis, this can be efficiently\ncomputed as a dot product of the coefficient vectors due to\northonormality of the basis:\ncdiffuse\nk\n= \u03c1k\u2299\nZ\nS2 L(\u03c9i)\u2299dk(\u03c9i)d\u03c9i = \u03c1k\u2299\n(n+1)2\nX\ni=1\nLi \u2299 di\nk,\n(5)\nwhere L = {Li} and dk = {di\nk} are the n-th order SH\ncoefficients of the incident light and the intrinsic radiance\ntransfer function, with di\nk \u2208 R3, and \u03c1k \u2208 R3 a learnable\nalbedo which we statically define on each Gaussian to en-\ncourage temporally consistent reconstructions. While dif-\nfuse light transport is a low-pass filter that requires only 2nd\nor 3rd order SH [64], this is not sufficient to represent shad-\nows. To enable higher-frequency shadows while decoding a\nmanageable number of coefficients that can fit on consumer\nGPU memory, we propose to decode RGB intrinsic SH co-\nefficients dc\nk up to the 3rd order, and only monochrome in-\ntrinsic SH coefficients dm\nk from 4-th to 8-th order.\nSpecular Reflection. To achieve sharp, mirror-like reflec-\ntions, for the view-dependent specular term we use a spher-\nical Gaussian (SG) as an angular basis. In particular, we\npropose a normalized, angle-based spherical Gaussian:\nGs(p; q, \u03c3) = Ce\u2212 1\n2 ( arccos(p\u00b7q)\n\u03c3\n)2,\n(6)\nwhere \u03c3 \u2208 R+ is the standard deviation of angular decay,\nq \u2208 S2 is the lobe axis, p \u2208 S2 is the direction of evalu-\nation, and C = 1/(\n\u221a\n2\u03c02/3\u03c3) is a normalization factor to\npreserve the integral of the Gaussian.\nNote that this parameterization is different from the\nmore widely used parameterization G(p; q, \u03bb, \u00b5)\n=\n\u00b5e\u03bb(p\u00b7q\u22121) [75, 90], but we observed that this choice often\nfails to model highly reflective surfaces, such as the cornea.\nAs the majority of specular BRDFs have a lobe that is\naxis-aligned with a particular reflection direction, we com-\npute a reflection vector as the lobe axis:\nqk = 2(\u03c9o\nk \u00b7 nk)nk \u2212 \u03c9o\nk,\n(7)\nwhere \u03c9o\nk is the viewing direction evaluated at the Gaus-\nsian center, and nk is a normal direction computed for each\n5\nGaussian, and qk is the lobe axis. Our final color for the\nspecular term of each 3D Gaussian is defined as follows:\ncspecular\nk\n(\u03c9o) = vk(\u03c9o)\nZ\nS2 L(\u03c9i)Gs(\u03c9i; qk, \u03c3k)d\u03c9i, (8)\nwhere vk(\u03c9o) \u2208 (0, 1) is a learnable view-dependent visi-\nbility term that accounts for Fresnel effects and geometric\nocclusion integrated within the BRDF lobe. Please refer\nto Appendix C for the connection to the rendering equa-\ntion [21]. For point lights, we use a Dirac delta function\nmultiplied by a light intensity as the incident light L(\u03c9i)\nfor fast evaluation, but an SG parameterization with closed\nform integrals [75] would also be possible for directional\narea lights. Importantly, Eq. 8 can be efficiently evaluated\nwith negligible overhead for all-frequency continuous illu-\nmination by prefiltering the environment maps [23, 47] as\ndemonstrated in Fig. 1. This requires only a single mipmap\ntexture look up per 3D Gaussian, which is a critical property\nfor real-time relighting with all-frequency reflections.\nAlthough the aforementioned formulation works well for\nsurfaces, we use 3D Gaussians to also represent thin fiber-\nlike structures such as hair strands, where Eq. 8 is incorrect\nif a viewer rotates along a tangent vector of each fiber [22].\nTo support specular reflection of both surfaces and fibers in\na unified manner, we propose to learn a view-conditioned\nsurface normal. While the learned normal can remain con-\nstant under view changes for surface regions, the normal\ncan rotate along the tangent axis based on the view direc-\ntion for fibers. This way, each 3D Gaussian can flexibly\nchoose its underlying reflection behavior without requiring\npredefining it a priori. Our learnable view-dependent nor-\nmal also supports the case where BRDF lobes are not ex-\nactly aligned with the surface normal [46] by adjusting the\nnormal orientation.\nDecoder. Similar to the geometric decoder, we decode radi-\nance transfer parameters using a view-independent decoder\nDci and a view-dependent decoder Dcv as follows:\n{dc\nk, dm\nk , \u03c3k}M\nk=1 = Dci(z, e{l,r}; \u0398ci),\n{\u03b4nk, vk}M\nk=1 = Dcv(z, e{l,r}, \u03c9o; \u0398cv),\n(9)\nwhere \u0398ci and \u0398cv are the learnable parameters of each de-\ncoder, and the normal residual \u03b4nk is added to the barycen-\ntric interpolated coarse mesh normal \u02c6nk to obtain the final\nnormal nk as follows: nk = (\u02c6nk + \u03b4nk)/\u2225\u02c6nk + \u03b4nk\u2225. In\npractice, since Dg in Eq. 3 and Dci in Eq. 9 take the same\ninput and produce per-Gaussian values, we model them us-\ning a single decoder.\n3.4. Relightable Explicit Eye Model\nTo enable better disentanglement and high-fidelity eye\nrelighting, we use an explicit eye model proposed by\nSchwartz et al. [68] as the underlying geometric represen-\ntation of eyes. In particular, we parameterize eyeballs as\nthe smooth blending of two spheres; one accounts for the\neyeball and the other for the cornea. They are explicitly\nrotated based on a gaze direction. Each eyeball is parame-\nterized by E = {re, rc, d, ce} with the radii of the eyeball\nre and cornea rc, the offset d along the optical axis from the\ncenter of the eyeball to the center of the cornea sphere, and\nthe center of the eyeball ce relative to the head in a canon-\nical space. We first optimize E following the optimization\npresented in Schwartz et al. [68], and then jointly refine it\nend-to-end with the other parameters of an avatar.\nWhile we use the same geometric and appearance rep-\nresentations for eyeballs as the rest of the head, we ob-\nserve that additional modification is required to enable high-\nfidelity eye relighting. Since the cornea exhibits mirror-like\nreflections, the discrete point lights of our capture system\ncreate reflections that span only a few pixels (often a single\nBayer cell) and saturate the sensor. The remaining region\nhas nearly zero contribution. Due to this highly discrete\nsignal, 3D Gaussians quickly fall into poor local minima\nand fail to correctly model eye glints if we independently\noptimize the position and surface normal of each Gaussian.\nTherefore, we freeze the position of Gaussians on the sur-\nface of the eyeballs and fix their normals to be the surface\nnormals of the eyeball mesh. In addition, the iris is observed\nonly through the transparent cornea, with significant refrac-\ntion. To support refraction with the same underlying ap-\npearance representation, we use a view-conditioned albedo\nfor the eyes. This effectively allows the eye diffuse color to\naccount for refraction based on the input viewpoint.\nTo incorporate these modifications, our geometry and ap-\npearance eye decoders for each eye are defined as follows:\n{Rk, sk, ok, dc\nk, dm\nk , \u03c3k}Me\nk=1 = Dei(e, hp, hr; \u0398ei),\n{\u03c1k, vk}Me\nk=1 = Dev(e, hp, hr, \u03c9o; \u0398ev),\n(10)\nwhere Dei and Dev are view-independent and view-\nconditioned eye decoders with parameters \u0398ei and \u0398ev re-\nspectively, and the relative head position hp \u2208 R3 and rota-\ntion hr \u2208 SO(3) and the gaze e are used to absorb tracking\nerrors.\n3.5. Training\nGiven multiview video data of a person illuminated with\nknown point light patterns, we jointly optimize all trainable\nnetwork parameters \u0398, the static albedo \u03c1, and the eyeball\nparameters E{l,r} with the following loss function L:\nL = Lrec + Lreg + \u03bbklLkl,\n(11)\nwhere Lkl is the KL-divergence loss on our encoder outputs.\nThe reconstruction loss Lrec consists of L1 and D-SSIM on\nthe rendered image as in the original 3DGS paper [24, 76]\nas well as L2 loss on the coarse geometry V \u2032 as follows:\nLrec = \u03bbl1Ll1 + \u03bbssimLssim + \u03bbgeoLgeo.\n(12)\n6\nTable 1. Comparison on held-out segments. The top three tech-\nniques are highlighted in red, orange, and yellow, respectively.\nGeometry\nAppearance\nMetrics\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nA\nOurs w/ EEM\nEyeNeRF [33]\n34.550\n0.939\n0.115\nB\nOurs\n36.501\n0.943\n0.110\nC\nOurs\nEyeNeRF [33]\n35.110\n0.938\n0.113\nD\nLinear [88]\n33.831\n0.936\n0.184\nE\nOurs\n36.529\n0.943\n0.111\nF\nMVP [41]\nEyeNeRF [33]\n27.594\n0.922\n0.151\nG\nLinear [88]\n36.294\n0.942\n0.140\nH\nOurs\n35.789\n0.943\n0.134\nThe regularization loss is defined as:\nLreg = \u03bbsLs+\u03bbc\u2212Lc\u2212+\u03bbesLes+\u03bbevLev+\u03bbeoLeo. (13)\nThe scale regularization term Ls encourages the Gaussian\nscale {sk} to stay within a reasonable range as follows:\nLs = mean(ls), ls =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1/ max(s, 10\u22127)\nif s < 0.1\n(s \u2212 10.0)2\nif s > 10.0\n0\notherwise,\n(14)\nwhere s denotes the scale value of each axis in each Gaus-\nsian, and mean(\u00b7) is the average operation across all dimen-\nsions. The negative color loss Lc\u2212 penalizes negative color\nin the diffuse term as SH can yield negative values:\nLc\u2212 = mean(lc\u2212), lc\u2212 = min(cdiffuse\nk\n, 0)2.\n(15)\nAlso, three regularization terms are used to prevent eye\nGaussians from becoming transparent as follows:\nLes = mean(les), les = max(s \u2212 0.1, 0)2,\nLeo = mean(leo), leo = (1 \u2212 ok)2,\nLev = mean(lev), lev = (1 \u2212 vk)2.\n(16)\nThe relative weights are \u03bbgeo = \u03bbl1 = 10, \u03bbssim = 0.2,\n\u03bbs = \u03bbc\u2212 = \u03bbes = 1.0 \u00d7 10\u22122, \u03bbeo = \u03bbev = 1.0 \u00d7 10\u22124,\nand \u03bbkl = 2.0 \u00d7 10\u22123. We use the Adam optimizer [25]\nwith a learning rate of 0.0005. We train our model on 4\nNVIDIA A100 GPUs with a batch size of 16 for 200k itera-\ntions. Please refer to Appendix A for network architecture.\n4. Experiments\nEvaluation Protocol.\nWe selected three subjects for\nquantitative evaluations and three more subjects for qualita-\ntive results with diverse races, genders, and hairstyles. Our\nevaluation included around 9,000 conversational expression\nframes and about 100 disgust expression frames not seen\nduring training. We also exclude 10 unique frontally-biased\nlight patterns entirely from the training. This corresponds\nto approximately 1800 out of 144,000 frames. We report\nPSNR, SSIM, and LPIPS [92] on images masked with the\nface region to avoid influence from the background.\nTable 2. Comparison on held-out lights. The top three tech-\nniques are highlighted in red, orange, and yellow, respectively.\nGeometry\nAppearance\nMetrics\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nA\nOurs w/ EEM\nEyeNeRF [33]\n30.7976\n0.828\n0.162\nB\nOurs\n34.042\n0.858\n0.148\nC\nOurs\nEyeNeRF [33]\n30.836\n0.815\n0.163\nD\nLinear [88]\n32.829\n0.870\n0.202\nE\nOurs\n33.845\n0.831\n0.148\nF\nMVP [41]\nEyeNeRF [33]\n28.030\n0.812\n0.210\nG\nLinear [88]\n33.444\n0.726\n0.192\nH\nOurs\n33.778\n0.877\n0.168\n4.1. Qualitative Results\nFig. 1 shows that our reconstructed avatars generalize to\nnovel views, expressions, and illuminations including point\nlights and high-resolution environment maps. Notice the\nmirror-like reflections in the eyes that faithfully represent\nthe environment without losing high-frequency details. As\nour model is drivable and supports real-time relighting, real-\ntime driving from a headset is also possible [78].\nWhile this is not our primary goal, as a bi-product, our\napproach estimates intrinsic properties of reflectance in-\ncluding albedo, geometry, surface normal, multi-bounce\nscattering, and specular components in a self-supervised\nmanner. As shown in Fig. 3, our method enables 3D consis-\ntent and high-fidelity intrinsics decomposition.\n4.2. Comparison\nGeometric Representation.\nWe evaluate the geometry\ncomponent by comparing three variations: our proposed\nmethod, our method excluding the explicit eye model\n(EEM) [68], and voxel-based primitives [41].\nFor fair\ncomparison, we use the same appearance model and only\nchange the geometric representation (Tab. 1 and Tab. 2 B, D,\nH). Fig. 4 clearly demonstrates that our geometry based on\n3D Gaussians can better model skin details and hair strands\nthan MVP. Further, our full model, when combined with\nan EEM, achieves convincing eye glints. Gaussians con-\nsistently achieve superior performance in all metrics except\nSSIM in held-out lights.\nAppearance Representation. For appearance representa-\ntion, we compare our appearance model with existing re-\nlightable appearance representations that support rendering\nwith environment maps in real-time. The model presented\nby Yang et al. [88] is a linear neural network that explicitly\nretains the linearity of light transport (denoted as Linear),\ndemonstrating superior performance than a previous state-\nof-the-art method [6]. For this reason, we omit the compar-\nison with [6]. To evaluate the effectiveness of our specu-\nlar reflection model, we also replace our specular compo-\nnent with view-dependent spherical harmonics proposed by\nEyeNeRF [33]. Tab. 1 and Tab. 2 C, D, E show that our\n7\n(a)\n(d)\n(b)\n(e)\n(c)\n(f)\nx2\nFigure 3. Intrinsics decomposition. The full render (a) is pro-\nduced by addition of a diffuse (b) and a specular component (c)\n(intensity multiplied by 2 for clarity). The diffuse component is\nobtained by multiplying a learned albedo (d) with shading com-\nputed by SH-based radiance transfer (e). The specular lobes direc-\ntion is computed using a per-Gaussian normal (f).\nappearance representation outperforms existing appearance\nmodels in most of the metrics. As shown in Fig. 5, while\nthe linear model produces correct overall color, the relight-\ning result is blurry and lacks high-frequency details. This\nis primarily limited by the bottleneck lighting representa-\ntion. The view-dependent spherical harmonics in EyeNeRF\nshows more detailed reflections, but its expressiveness is\nlimited due to the use of spherical harmonics for specular-\nity. Additionally, we observe that view-dependent spherical\nharmonics are more prone to overfitting, resulting in flicker-\ning artifacts in animation. Please refer to our supplemental\nvideo for more details. In contrast, our approach based on\nspherical Gaussians is not band-limited and thus achieves\nhigh-frequency reflections.\n5. Conclusion\nWe presented Relightable Gaussian Codec Avatars, a novel\nappearance and geometric representation for relightable 3D\nhead avatars that supports real-time rendering. Our exper-\niments show that high-fidelity relighting of hair, skin, and\neyes in all-frequency illuminations is now possible in real-\ntime with the proposed radiance transfer basis composed\nof spherical harmonics and spherical Gaussians. We have\nalso shown that our choice of the geometric representa-\ntion based on 3D Gaussian Splatting is critical for strand-\naccurate hair reconstruction and relighting. Our approach\nachieves a significant quality improvement in comparison\nto existing real-time-renderable geometry and appearance\nmodels, both qualitatively and quantitatively.\n(a) GT\n(b) Ours  w/ EEM\n(c) Ours w/o EEM\n(d) MVP\nFigure 4. Geometric representation comparison. Compared to a\nheld out frame, (a), our Gaussian splatting decoded geometry (b,c)\nshows improved resolution over MVP [41] (d), especially in fine\ndetails like eyelashes and pores. The explicit eyeball model (b)\nadditionally improves realism in eye glints. All methods use the\nappearance model described in Sec. 3.3.\n(a) GT\n(b) Ours\n(c) Linear\n(d) Eyenerf\nFigure 5. Appearance representation comparison. Compared\nto a held out frame (a), our appearance model (Sec. 3.3) shows\nsharper pore-level specularities than methods using only a lin-\near neural network [88] or the spherical harmonics-only method\n\u201cEyenerf\u201d [33]. All methods use the geometric representation de-\nscribed in Sec. 3.2 (without explicit eyeballs.)\nLimitations and Future Work. The current approach re-\nquires a coarse mesh and gaze tracking as a preprocessing\nstep, which may be sensitive to tracking failures. Similar\nto [88], end-to-end learning together with topology consis-\ntent tracking [8, 37, 38] is an interesting future work di-\nrection to enable scalable training. Extending our approach\nto in-the-wild inputs also remains a challenge due to the\nlack of precisely known illumination information. Lastly,\nrendering a large number of our Gaussian avatars would be\ndifficult, as the relighting operation is performed per indi-\nvidual 3D Gaussian, and scales linearly with the number\nof avatars. Offloading relighting computation in a per-pixel\nfragment shader, similar to [44], is also an exciting direction\nfor future research.\n8\nReferences\n[1] Rameen Abdal, Yipeng Qin, and Peter Wonka.\nIm-\nage2stylegan: How to embed images into the stylegan latent\nspace? In Proceedings of the IEEE/CVF international con-\nference on computer vision, pages 4432\u20134441, 2019. 5\n[2] Thabo Beeler, Bernd Bickel, Paul Beardsley, Bob Sumner,\nand Markus Gross. High-quality single-shot capture of fa-\ncial geometry. In ACM SIGGRAPH 2010 papers, pages 1\u20139.\n2010. 2\n[3] Thabo Beeler, Bernd Bickel, Gioacchino Noris, Paul Beards-\nley, Steve Marschner, Robert W Sumner, and Markus Gross.\nCoupled 3d reconstruction of sparse facial hair and skin.\nACM Transactions on Graphics (ToG), 31(4):1\u201310, 2012. 2\n[4] Thabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel,\nPaul A Beardsley, Craig Gotsman, Robert W Sumner, and\nMarkus H Gross. High-quality passive facial performance\ncapture using anchor frames. ACM Trans. Graph., 30(4):75,\n2011. 2\n[5] Pascal B\u00b4erard, Derek Bradley, Markus Gross, and Thabo\nBeeler. Lightweight eye capture using a parametric model.\nACM Transactions on Graphics (TOG), 35(4):1\u201312, 2016. 2\n[6] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon,\nShih-En Wei, Kevyn Mcphail, Ravi Ramamoorthi, Yaser\nSheikh, and Jason Saragih.\nDeep relightable appearance\nmodels for animatable faces. ACM Transactions on Graphics\n(TOG), 40(4):1\u201315, 2021. 3, 4, 7, 13\n[7] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\u02c7s Ha\u02c7san, Yan-\nnick Hold-Geoffroy, David Kriegman, and Ravi Ramamoor-\nthi. Deep reflectance volumes: Relightable reconstructions\nfrom multi-view photometric images. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part III 16, pages 294\u2013311.\nSpringer, 2020. 2, 3\n[8] Timo Bolkart, Tianye Li, and Michael J Black. Instant multi-\nview head capture through learnable registration.\nIn Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 768\u2013779, 2023. 8\n[9] Derek Bradley, Wolfgang Heidrich, Tiberiu Popa, and Alla\nSheffer. High resolution passive facial performance capture.\nIn ACM SIGGRAPH 2010 papers, pages 1\u201310. 2010. 2\n[10] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter\nDuiker, Westley Sarokin, and Mark Sagar.\nAcquiring the\nreflectance field of a human face.\nIn Proceedings of the\n27th annual conference on Computer graphics and interac-\ntive techniques, pages 145\u2013156, 2000. 2, 3\n[11] Boyang Deng, Yifan Wang, and Gordon Wetzstein.\nLu-\nmigan: Unconditional generation of relightable 3d human\nfaces. arXiv preprint arXiv:2304.13153, 2023. 3\n[12] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and\nrobust multiview stereopsis. IEEE transactions on pattern\nanalysis and machine intelligence, 32(8):1362\u20131376, 2009.\n2\n[13] Duan Gao, Guojun Chen, Yue Dong, Pieter Peers, Kun Xu,\nand Xin Tong. Deferred neural lighting: free-viewpoint re-\nlighting from unstructured photographs. ACM Transactions\non Graphics (TOG), 39(6):1\u201315, 2020. 3\n[14] Stephan J Garbin, Marek Kowalski, Virginia Estellers,\nStanislaw Szymanowicz, Shideh Rezaeifar, Jingjing Shen,\nMatthew Johnson, and Julien Valentin. Voltemorph: Real-\ntime, controllable and generalisable animation of volumetric\nrepresentations. arXiv preprint arXiv:2208.00949, 2022. 3\n[15] Pablo Garrido, Michael Zollh\u00a8ofer, Chenglei Wu, Derek\nBradley,\nPatrick P\u00b4erez,\nThabo Beeler,\nand Christian\nTheobalt. Corrective 3d reconstruction of lips from monoc-\nular video. ACM Trans. Graph., 35(6):219\u20131, 2016. 2\n[16] Abhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong, Jay\nBusch, Xueming Yu, and Paul Debevec. Multiview face cap-\nture using polarized spherical gradient illumination. ACM\nTransactions on Graphics (TOG), 30(6):1\u201310, 2011. 2, 3\n[17] Paul Green, Jan Kautz, Wojciech Matusik, and Fr\u00b4edo Du-\nrand.\nView-dependent precomputed light transport using\nnonlinear gaussian function approximations.\nIn Proceed-\nings of the 2006 symposium on Interactive 3D graphics and\ngames, pages 7\u201314, 2006. 3\n[18] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,\nXueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-\nEscolano, Rohit Pandey, Jason Dourgarian, et al. The re-\nlightables: Volumetric performance capture of humans with\nrealistic relighting. ACM Transactions on Graphics (ToG),\n38(6):1\u201319, 2019. 3\n[19] Liwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. Robust\nhair capture using simulated examples. ACM Transactions\non Graphics (TOG), 33(4):1\u201310, 2014. 2\n[20] Shun Iwase, Shunsuke Saito, Tomas Simon, Stephen Lom-\nbardi, Timur Bagautdinov, Rohan Joshi, Fabian Prada,\nTakaaki Shiratori, Yaser Sheikh, and Jason Saragih.\nRe-\nlightablehands:\nEfficient neural relighting of articulated\nhand models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16663\u2013\n16673, 2023. 2, 3, 13\n[21] James T Kajiya. The rendering equation. In Proceedings of\nthe 13th annual conference on Computer graphics and inter-\nactive techniques, pages 143\u2013150, 1986. 6, 14\n[22] James T Kajiya and Timothy L Kay. Rendering fur with three\ndimensional textures. ACM Siggraph Computer Graphics,\n23(3):271\u2013280, 1989. 6\n[23] Jan Kautz, Pere-Pau V\u00b4azquez, Wolfgang Heidrich, and\nHans-Peter Seidel. A unified approach to prefiltered envi-\nronment maps. In Rendering Techniques 2000: Proceedings\nof the Eurographics Workshop in Brno, Czech Republic, June\n26\u201328, 2000 11, pages 185\u2013196. Springer, 2000. 6\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 2, 3, 4, 5, 6\n[25] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 7\n[26] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 5\n[27] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim\nWalter, and Matthias Nie\u00dfner. Nersemble: Multi-view ra-\ndiance field reconstruction of human heads. arXiv preprint\narXiv:2305.03027, 2023. 3\n[28] Georgios Kopanas, Thomas Leimk\u00a8uhler, Gilles Rainer,\nCl\u00b4ement Jambon, and George Drettakis. Neural point cata-\ncaustics for novel-view synthesis of reflections. ACM Trans-\n9\nactions on Graphics (TOG), 41(6):1\u201315, 2022. 4\n[29] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis.\nPoint-based neural rendering with per-\nview optimization.\nIn Computer Graphics Forum, vol-\nume 40, pages 29\u201343. Wiley Online Library, 2021. 4\n[30] Mathieu Lamarre, John P Lewis, and Etienne Danvoye. Face\nstabilization by mode pursuit for avatar construction. In 2018\nInternational Conference on Image and Vision Computing\nNew Zealand (IVCNZ), pages 1\u20136. IEEE, 2018. 4\n[31] Alexandros Lattas, Stylianos Moschoglou, Baris Gecer,\nStylianos\nPloumpis,\nVasileios\nTriantafyllou,\nAbhijeet\nGhosh, and Stefanos Zafeiriou. Avatarme: Realistically ren-\nderable 3d facial reconstruction\u201d in-the-wild\u201d. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 760\u2013769, 2020. 3\n[32] Alexandros\nLattas,\nStylianos\nMoschoglou,\nStylianos\nPloumpis, Baris Gecer, Abhijeet Ghosh, and Stefanos\nZafeiriou.\nAvatarme++: Facial shape and brdf inference\nwith photorealistic rendering-aware gans.\nIEEE Trans-\nactions on Pattern Analysis and Machine Intelligence,\n44(12):9269\u20139284, 2021. 3\n[33] Gengyan Li, Abhimitra Meka, Franziska Mueller, Marcel C\nBuehler, Otmar Hilliges, and Thabo Beeler.\nEyenerf: a\nhybrid representation for photorealistic synthesis, animation\nand relighting of human eyes. ACM Transactions on Graph-\nics (TOG), 41(4):1\u201316, 2022. 1, 2, 3, 7, 8\n[34] Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He,\nKalle Bladin, and Hao Li. Dynamic facial asset and rig gen-\neration from a single scan. ACM Transactions on Graphics\n(TOG), 39:1 \u2013 18, 2020. 3\n[35] Junxuan Li, Shunsuke Saito, Tomas Simon, Stephen Lom-\nbardi, Hongdong Li, and Jason Saragih.\nMegane: Mor-\nphable eyeglass and avatar network.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12769\u201312779, 2023. 2, 3, 4\n[36] Ruilong Li, Kalle Bladin, Yajie Zhao, Chinmay Chinara,\nOwen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha Bhu-\nvana Prasad, Bipin Kishore, Jun Xing, and Hao Li. Learn-\ning formation of physically-based face attributes.\n2020\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3407\u20133416, 2020. 3\n[37] Tianye Li, Shichen Liu, Timo Bolkart, Jiayi Liu, Hao Li,\nand Yajie Zhao. Topologically consistent multi-view face in-\nference using volumetric sampling. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 3824\u20133834, 2021. 8\n[38] Shichen Liu, Yunxuan Cai, Haiwei Chen, Yichao Zhou, and\nYajie Zhao.\nRapid face asset acquisition with recurrent\nfeature alignment. ACM Transactions on Graphics (TOG),\n41(6):1\u201317, 2022. 8\n[39] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser\nSheikh. Deep appearance models for face rendering. ACM\nTransactions on Graphics (ToG), 37(4):1\u201313, 2018. 1, 2, 5,\n13\n[40] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\narXiv preprint arXiv:1906.07751, 2019. 3\n[41] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM Transactions on Graphics (ToG), 40(4):1\u201313, 2021. 3,\n5, 7, 8, 13\n[42] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan.\nDynamic 3d gaussians:\nTracking\nby persistent dynamic view synthesis.\narXiv preprint\narXiv:2308.09713, 2023. 3\n[43] Linjie Luo, Hao Li, and Szymon Rusinkiewicz. Structure-\naware hair capture. ACM Transactions on Graphics (TOG),\n32(4):1\u201312, 2013. 2\n[44] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang,\nYuecheng Li, Fernando De La Torre, and Yaser Sheikh. Pixel\ncodec avatars. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 64\u201373,\n2021. 2, 8\n[45] Wan-Chun Ma, Tim Hawkins, Pieter Peers, Charles-Felix\nChabert, Malte Weiss, Paul E Debevec, et al.\nRapid ac-\nquisition of specular and diffuse normal maps from polar-\nized spherical gradient illumination. Rendering Techniques,\n2007(9):10, 2007. 2, 3\n[46] Stephen R Marschner, Henrik Wann Jensen, Mike Cam-\nmarano, Steve Worley, and Pat Hanrahan. Light scattering\nfrom human hair fibers.\nACM Transactions on Graphics\n(TOG), 22(3):780\u2013791, 2003. 1, 6\n[47] David K McAllister, Anselmo Lastra, and Wolfgang Hei-\ndrich. Efficient rendering of spatial bi-directional reflectance\ndistribution functions.\nIn Proceedings of the ACM SIG-\nGRAPH/EUROGRAPHICS conference on Graphics hard-\nware, pages 79\u201388, 2002. 6\n[48] Abhimitra Meka, Christian Haene, Rohit Pandey, Michael\nZollh\u00a8ofer, Sean Fanello, Graham Fyffe, Adarsh Kowdle,\nXueming Yu, Jay Busch, Jason Dourgarian, et al.\nDeep\nreflectance fields: high-quality facial reflectance field infer-\nence from color gradient illumination. ACM Transactions on\nGraphics (TOG), 38(4):1\u201312, 2019. 3\n[49] Abhimitra Meka, Rohit Pandey, Christian Haene, Sergio\nOrts-Escolano, Peter Barnum, Philip David-Son, Daniel Er-\nickson, Yinda Zhang, Jonathan Taylor, Sofien Bouaziz, et al.\nDeep relightable textures: volumetric performance capture\nwith neural rendering.\nACM Transactions on Graphics\n(TOG), 39(6):1\u201321, 2020. 3\n[50] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n3\n[51] Erick Miller and Dmitriy Pinskiy. Realistic eye motion using\nprocedural geometric methods. In SIGGRAPH 2009: Talks,\npages 1\u20131. 2009. 2\n[52] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3d models, materials, and lighting\nfrom images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8280\u2013\n8290, 2022. 3\n[53] Koki Nagano, Graham Fyffe, Oleg Alexander, Jernej Bar-\nbic, Hao Li, Abhijeet Ghosh, and Paul E Debevec.\nSkin\nmicrostructure deformation with displacement map convo-\n10\nlution. ACM Trans. Graph., 34(4):109\u20131, 2015. 1\n[54] Giljoo Nam, Joo Ho Lee, Diego Gutierrez, and Min H Kim.\nPractical svbrdf acquisition of 3d objects with unstructured\nflash photography. ACM Transactions on Graphics (TOG),\n37(6):1\u201312, 2018. 3\n[55] Giljoo Nam, Chenglei Wu, Min H Kim, and Yaser Sheikh.\nStrand-accurate multi-view hair capture. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 155\u2013164, 2019. 2\n[56] Ren Ng, Ravi Ramamoorthi, and Pat Hanrahan.\nAll-\nfrequency shadows using non-linear wavelet lighting approx-\nimation. In ACM SIGGRAPH 2003 Papers, pages 376\u2013381.\n2003. 3\n[57] Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello,\nWayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim,\nPhilip L Davidson, Sameh Khamis, Mingsong Dou, et al.\nHoloportation: Virtual 3d teleportation in real-time. In Pro-\nceedings of the 29th annual symposium on user interface\nsoftware and technology, pages 741\u2013754, 2016. 1\n[58] Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Chris-\ntian Haene, Sofien Bouaziz, Christoph Rhemann, Paul De-\nbevec, and Sean Fanello. Total relighting: learning to relight\nportraits for background replacement. ACM Transactions on\nGraphics (TOG), 40(4):1\u201321, 2021. 3\n[59] Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos\nMoschoglou, and Stefanos Zafeiriou.\nRelightify:\nRe-\nlightable 3d faces from a single image via diffusion models.\narXiv preprint arXiv:2305.06077, 2023. 3\n[60] Sylvain Paris, Will Chang, Oleg I Kozhushnyan, Wojciech\nJarosz, Wojciech Matusik, Matthias Zwicker, and Fr\u00b4edo Du-\nrand. Hair photobooth: geometric and photometric acquisi-\ntion of real hairstyles. ACM Trans. Graph., 27(3):30, 2008.\n2\n[61] Frederic I Parke and Keith Waters. Computer facial anima-\ntion. CRC press, 2008. 2\n[62] Pieter Peers, Naoki Tamura, Wojciech Matusik, and Paul De-\nbevec. Post-production facial performance relighting using\nreflectance transfer. ACM Transactions on Graphics (TOG),\n26(3):52\u2013es, 2007. 3\n[63] Gilles Rainer, Adrien Bousseau, Tobias Ritschel, and George\nDrettakis. Neural precomputed radiance transfer. In Com-\nputer Graphics Forum, volume 41, pages 365\u2013378. Wiley\nOnline Library, 2022. 3\n[64] Ravi Ramamoorthi and Pat Hanrahan. An efficient represen-\ntation for irradiance environment maps. In Proceedings of\nthe 28th annual conference on Computer graphics and inter-\nactive techniques, pages 497\u2013500, 2001. 5\n[65] Anurag Ranjan, Kwang Moo Yi, Jen-Hao Rick Chang, and\nOncel Tuzel. Facelit: Neural 3d relightable faces. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8619\u20138628, 2023. 3\n[66] Tim Salimans and Durk P Kingma. Weight normalization:\nA simple reparameterization to accelerate training of deep\nneural networks. Advances in neural information processing\nsystems, 29, 2016. 13\n[67] Kripasindhu Sarkar, Marcel C. Buehler, Gengyan Li, Daoye\nWang, Delio Vicini, J\u00b4er\u00b4emy Riviere, Yinda Zhang, Sergio\nOrts-Escolano, Paulo Gotardo, Thabo Beeler, and Abhimitra\nMeka. Litnerf: Intrinsic radiance decomposition for high-\nquality view synthesis and relighting of faces. In ACM SIG-\nGRAPH Asia 2023, 2023. 2, 3, 13\n[68] Gabriel Schwartz, Shih-En Wei, Te-Li Wang, Stephen Lom-\nbardi, Tomas Simon, Jason Saragih, and Yaser Sheikh. The\neyes have it: An integrated eye and face model for photo-\nrealistic facial animation. ACM Transactions on Graphics\n(TOG), 39(4):91\u20131, 2020. 1, 2, 4, 6, 7\n[69] Mike Seymour, Chris Evans, and Kim Libreri. Meet mike:\nepic avatars. In ACM SIGGRAPH 2017 VR Village, pages\n1\u20132. 2017. 2\n[70] Peter-Pike Sloan, Jan Kautz, and John Snyder.\nPrecom-\nputed radiance transfer for real-time rendering in dynamic,\nlow-frequency lightingevironments.\nACM Trans. Graph.,\n21(3):527\u2013536, jul 2002. 2, 3, 5\n[71] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang\nXu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay\nBusch, Paul Debevec, and Ravi Ramamoorthi. Single image\nportrait relighting. ACM Transactions on Graphics (TOG),\n38(4):1\u201312, 2019. 3\n[72] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian\nBernard, Hans-Peter Seidel, Patrick P\u00b4erez, Michael Zoll-\nhofer, and Christian Theobalt.\nStylerig:\nRigging style-\ngan for 3d control over portrait images. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6142\u20136151, 2020. 3\n[73] Justus Thies, Michael Zollh\u00a8ofer, and Matthias Nie\u00dfner. De-\nferred neural rendering: Image synthesis using neural tex-\ntures. Acm Transactions on Graphics (TOG), 38(4):1\u201312,\n2019. 3\n[74] Yu-Ting Tsai and Zen-Chung Shih. All-frequency precom-\nputed radiance transfer using spherical radial basis functions\nand clustered tensor approximation. ACM Transactions on\ngraphics (TOG), 25(3):967\u2013976, 2006. 3\n[75] Jiaping Wang, Peiran Ren, Minmin Gong, John Snyder, and\nBaining Guo. All-frequency rendering of dynamic, spatially-\nvarying reflectance. In ACM SIGGRAPH Asia 2009 papers,\npages 1\u201310. 2009. 2, 3, 5, 6, 13, 14\n[76] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.\nImage quality assessment: from error visibility to struc-\ntural similarity.\nIEEE Transactions on Image Processing,\n13(4):600\u2013612, 2004. 6\n[77] Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, and\nFeng Xu. Single image portrait relighting via explicit mul-\ntiple reflectance channel modeling. ACM Transactions on\nGraphics (TOG), 39(6):1\u201313, 2020. 3\n[78] Shih-En Wei, Jason Saragih, Tomas Simon, Adam W\nHarley, Stephen Lombardi, Michal Perdoch, Alexander Hy-\npes, Dawei Wang, Hernan Badino, and Yaser Sheikh. Vr fa-\ncial animation via multiview image translation. ACM Trans-\nactions on Graphics (TOG), 38(4):1\u201316, 2019. 5, 7\n[79] Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd\nBickel, Craig Donner, Chien Tu, Janet McAndless, Jinho\nLee, Addy Ngan, Henrik Wann Jensen, et al. Analysis of\nhuman faces using a measurement-based skin reflectance\nmodel. ACM Transactions on Graphics (ToG), 25(3):1013\u2013\n1024, 2006. 1, 2\n[80] Chenglei Wu, Derek Bradley, Pablo Garrido, Michael\nZollh\u00a8ofer, Christian Theobalt, Markus H Gross, and Thabo\nBeeler.\nModel-based teeth reconstruction.\nACM Trans.\n11\nGraph., 35(6):220\u20131, 2016. 2\n[81] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023. 3\n[82] Kun Xu, Wei-Lun Sun, Zhao Dong, Dan-Yong Zhao, Run-\nDong Wu, and Shi-Min Hu. Anisotropic spherical gaussians.\nACM Transactions on Graphics (TOG), 32(6):1\u201311, 2013. 3\n[83] Yuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen\nZhao, Han Huang, Guojun Qi, and Yebin Liu. Latentavatar:\nLearning latent expression code for expressive neural head\navatar. arXiv preprint arXiv:2305.01190, 2023. 5\n[84] Yingyan Xu, Gaspard Zoss, Prashanth Chandran, Markus\nGross, Derek Bradley, and Paulo Gotardo.\nRenerf: Re-\nlightable neural radiance fields with nearfield lighting. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 22581\u201322591, 2023. 3\n[85] Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi\nRamamoorthi. Deep image-based relighting from optimal\nsparse samples.\nACM Transactions on Graphics (ToG),\n37(4):1\u201313, 2018. 3\n[86] Zilin Xu, Zheng Zeng, Lifan Wu, Lu Wang, and Ling-Qi\nYan. Lightweight neural basis functions for all-frequency\nshading. In SIGGRAPH Asia 2022 Conference Papers, pages\n1\u20139, 2022. 3\n[87] Shugo Yamaguchi, Shunsuke Saito, Koki Nagano, Yajie\nZhao, Weikai Chen, Kyle Olszewski, Shigeo Morishima, and\nHao Li. High-fidelity facial reflectance and geometry infer-\nence from an unconstrained image. ACM Transactions on\nGraphics (TOG), 37(4):1\u201314, 2018. 3\n[88] Haotian Yang, Mingwu Zheng, Wanquan Feng, Haibin\nHuang, Yu-Kun Lai, Pengfei Wan, Zhongyuan Wang, and\nChongyang Ma. Towards practical capture of high-fidelity\nrelightable avatars.\nIn SIGGRAPH Asia 2023 Conference\nProceedings, 2023. 2, 3, 7, 8\n[89] Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz,\nMing-Yu Liu, and Ting-Chun Wang.\nLearning to relight\nportrait images via a virtual light stage and synthetic-to-real\nadaptation. ACM Transactions on Graphics (TOG), 41(6):1\u2013\n21, 2022. 3\n[90] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and\nNoah Snavely. Physg: Inverse rendering with spherical gaus-\nsians for physics-based material editing and relighting. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5453\u20135462, 2021. 2, 3,\n5, 13\n[91] Li Zhang, Noah Snavely, Brian Curless, and Steven M Seitz.\nSpacetime faces: high resolution capture for modeling and\nanimation. In ACM SIGGRAPH 2004 Papers, pages 548\u2013\n558. 2004. 2\n[92] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 7\n[93] Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun,\nTianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip\nDavidson, Christoph Rhemann, Paul Debevec, et al. Neu-\nral light transport for relighting and view synthesis. ACM\nTransactions on Graphics (TOG), 40(1):1\u201317, 2021. 3\n[94] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De-\nbevec, William T Freeman, and Jonathan T Barron. Ner-\nfactor: Neural factorization of shape and reflectance under\nan unknown illumination. ACM Transactions on Graphics\n(ToG), 40(6):1\u201318, 2021. 3\n[95] Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, Hongwen\nZhang, Jinli Suo, and Yebin Liu. Havatar: High-fidelity head\navatar via facial model conditioned neural radiance field.\nACM Transactions on Graphics, 2023. 2, 3\n[96] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J\nBlack, and Otmar Hilliges. Pointavatar: Deformable point-\nbased head avatars from videos.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 21057\u201321067, 2023. 3\n[97] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Instant\nvolumetric head avatars. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 4574\u20134584, 2023. 2, 3\n[98] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\nMarkus Gross. Ewa splatting. IEEE Transactions on Visu-\nalization and Computer Graphics, 8(3):223\u2013238, 2002. 4,\n5\n12\nA. Network Architecture\nOur head decoder consists of a view-independent decoder\nand a view-dependent decoder. An expression latent code\nz \u2208 R256 is first fed into a single linear layer with a leaky-\nReLU, and then reshaped into 256 \u00d7 8 \u00d7 8. Similarly, the\ngaze direction of each eye is fed into a linear layer with a\nleaky-ReLU, and then reshaped into 16 \u00d7 2 \u00d7 2 for each.\nThe gaze features are then only concatenated where the eye\nballs are located in the UV space, with the rest zero-padded.\nFor view-dependent decoding, we take the unit vector di-\nrection from the rendering camera to the head center, and\nfeed it into a linear layer with a leaky-ReLU to obtain a 8-\ndim latent feature, which is repeated across spatial dimen-\nsions for view-conditioning. The input features are con-\ncatenated and then fed into both decoders. Both the view-\nindependent and view-dependent decoders consist of multi-\nple up-sampling layers based on a transpose convolutional\nlayer (4 \u00d7 4 kernel, stride 2) followed by a leaky-ReLU\nwith channel sizes of (272, 256, 128, 128, 64, 32, 16, 125)\nand (280, 256, 128, 128, 64, 32, 16, 4) respectively.\nThe\neye decoder also uses a similar design while an in-\nput spatial resolution to the up-sampling layers of 4 \u00d7\n4.\nThe relative head rotation and position are simply\nrepeated across the spatial dimensions.\nWe also con-\ncatenate a visibility mask of eyeballs in UV space by\njointly rasterizing the coarse head mesh and the eye-\nballs to account for the shadows cast by the eyelids.\nThe channel sizes of both view-independent and view-\nindependent\nlayers\nare\n(23, 256, 128, 128, 64, 64, 122),\n(31, 256, 128, 128, 64, 64, 7) respectively.\nNote that we\nuse weight normalization [66] for all linear layers and up-\nsampling layers, and untied bias [39, 41] for all up-sampling\nlayers.\nB. Ablation Study\nIn this section, we provide ablation studies to validate our\nkey design choices.\nHigher-order Monochrome SH. Our diffuse color is based\non spherical harmonics. To support high-frequency shad-\nows, our model decodes additional monochrome SH coef-\nficients up to 8-th order. We compare our approach with\none where we remove 4-th to 8-th order monochrome SH\ncoefficients with the remaining components being identical.\nFig. 6 shows that our approach captures more precise shad-\nows. The quantitative evaluation in Tab. 3 also shows that\nadding the monochrome SH coefficients improves overall\nreconstruction accuracy. Note that while some recent works\nutilize explicitly computed shadow maps [6, 20, 67], this is\nintractable for real-time relighting with high-frequency en-\nvironments. Improving the sharpness of shadows in real-\ntime relighting even further is an interesting direction for\nfuture work.\nTable 3. Ablation Study. The top three techniques are highlighted\nin red, orange, and yellow, respectively. We use 3D Gaussians\nwith the explicit eye models for the geometric representations.\nMethod\nMetrics\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nOurs\n34.042\n0.858\n0.148\nOurs w/o monoSH\n33.762\n0.853\n0.152\nOurs w/o view-dep nml.\n33.927\n0.864\n0.148\nSG [75, 90]\n33.778\n0.855\n0.147\n(a) GT\n(b) w/ monoSH\n(c) w/o monoSH\nFigure 6. Ablation Study: Monochrome SH. Compared to a held\nout frame (a), using higher-order monochrome SH coefficients (b)\nimproves the sharpness of shadows compared to a model without\nthem (c).\nView-dependent Normal. Another component in our ap-\npearance model is the view-conditioned surface normal.\nWe compare our approach with one where we remove\nview-conditioning when decoding the surface normal. In-\nterestingly, the improvement does not clearly appear in\nboth qualitative and quantitative comparisons (see Tab. 3).\nWe hypothesize that our view-conditioned visibility term\ncan compensate for some of the errors caused by view-\nindependent surface normals in cylindrical regions. While\nthis allows the baseline using view-independent normals\nto achieve comparable performance under discrete point\nlights, this would likely cause inaccurate reflection on con-\ntinuous environments. We keep our view-conditioned nor-\nmals as this offers a more geometrically correct interpreta-\ntion for the cylinder-like 3D Gaussians.\nSpherical Gaussian Formulation.\nPrior works using\nspherical Gaussians [75, 90] typically use a different\nparametrization G(p; q, \u03bb, \u00b5) = \u00b5e\u03bb(p\u00b7q\u22121). We compare\nour method with this formulation of spherical Gaussians\nwith the remaining parts being identical. While the over-\nall results are comparable quantitatively, Fig. 7 shows that\nour parameterization better captures sharp eye glints, which\nis critical for accurate all-frequency reflections.\n13\n(a) GT\n(b) ours\n(c) SG [Wang et al.]\nFigure 7. Ablation Study: Spherical Gaussian Representation.\nCompared to a held out frame (a), our angle-based SG formulation\n(b) leads to more accurate recovery of eye glints than the conven-\ntional cosine-based SG formulation [75] (c).\nC. Discussion: Appearance Representation\nIn this section, we describe how we derive our specular term\nfrom the following rendering equation [21]:\nc(\u03c9o) =\nZ\nS2 L(\u03c9i)V (\u03c9i)\u03c1(\u03c9o, \u03c9i) max(0, \u03c9i \u00b7 n)d\u03c9i,\n(17)\nwhere \u03c9i and \u03c9o are incoming and outgoing light direc-\ntions, L is the incoming light intensity, V is the visibility\nterm, \u03c1 is the BRDF, and n is the surface normal. Assum-\ning the specular BRDF is represented with the general mi-\ncrofacet model, the specular component of BRDF is defined\nas follows:\n\u03c1S(\u03c9o, \u03c9i) = F(\u03c9o, \u03c9i)S(\u03c9o)S(\u03c9i)\n\u03c0(\u03c9i \u00b7 n)(\u03c9o \u00b7 n)\nD(h)\n(18)\n= M(\u03c9o, \u03c9i)D(h),\n(19)\nwhere F is the Fresnel term, S is the geometric attenua-\ntion term, and h is the halfway vector. Following Wang\net al. [75], we parameterize the normal distribution func-\ntion (NDF) D(h) as spherical Gaussian Gs(p; q, \u03c3) (Eq. 6\nin the main paper). According to Wang et al. [75], the re-\nmaining term M is smooth and can be approximated as a\nconstant across each Gaussian. After a spherical warping\n(Eq. 17-22 in [75]), we approximate Eq. 19 as:\n\u03c1S(\u03c9o, \u03c9i) \u2248 M(\u03c9o, \u03c9i)Gs(\u03c9i; q, \u03c3),\n(20)\nwhere q is the reflection vector. By substituting Eq. 20 into\nEq. 17, our specular term becomes:\nZ\nS2(V (\u03c9i)M(\u03c9o, \u03c9i) max(0, \u03c9i\u00b7n))L(\u03c9i)Gs(\u03c9i; q, \u03c3)d\u03c9i.\n(21)\nWhen \u03c3 \u226a 1, the value inside the integral is 0 unless \u03c9i\nis close to q, which is determined by the input view \u03c9o.\nTherefore, we further approximate Eq. 21 by moving and\ncombing all view-dependent terms together (denoted as vk)\nexcept the incoming radiance L and NDF Gs as follows:\ncspecular\nk\n= vk(\u03c9o)\nZ\nS2 L(\u03c9i)Gs(\u03c9i; q, \u03c3)d\u03c9i.\n(22)\nImportantly, we parameterize vk(\u03c9o) using a neural net-\nwork, enabling end-to-end optimization with the remain-\ning components to faithfully reproduce image observations.\nThus, our model is flexible enough to represent specular re-\nflection beyond the general microfacet model [75] or single-\nbounce reflection. We empirically find that this simple for-\nmulation is fast to compute, and stable to optimize. It also\nsupports modeling both diffuse and highly reflective areas\nin a unified manner. In our paper, we constrain the specu-\nlar BRDF to monochrome to prevent the specular term from\noverfitting diffuse components. Supporting color changes in\nspecular highlights caused by dielectric materials or multi-\nbounce specular reflection can be addressed in future work.\nD. Ethical Concerns\nOur model is only applied to a few consenting subjects cap-\ntured in a dense multiview capture system. In addition, the\nexpression latent space is personalized for each individual\nto capture subtle expressions. These effectively limit the use\ncase to driving ones\u2019 own avatars only with their consent.\n14\n"
  },
  {
    "title": "Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians",
    "link": "https://arxiv.org/pdf/2312.03029.pdf",
    "upvote": "22",
    "text": "Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians\nYuelang Xu\nTsinghua University\nBenwang Chen\nTsinghua University\nZhe Li\nTsinghua University\nHongwen Zhang\nTsinghua University\nLizhen Wang\nTsinghua University\nZerong Zheng\nNNKosmos\nYebin Liu\nTsinghua University\nFigure 1. Gaussian head avatar achieves ultra high-fidelity image synthesis with controllable expressions at 2K resolution. The above\nshows different views of the synthesized avatar, and the bottom shows different identities animated by the same expression. 16 views are\nused during the training.\nAbstract\nCreating high-fidelity 3D head avatars has always been\na research hotspot, but there remains a great challenge un-\nder lightweight sparse view setups. In this paper, we pro-\npose Gaussian Head Avatar represented by controllable 3D\nGaussians for high-fidelity head avatar modeling. We opti-\nmize the neutral 3D Gaussians and a fully learned MLP-\nbased deformation field to capture complex expressions.\nThe two parts benefit each other, thereby our method can\nmodel fine-grained dynamic details while ensuring expres-\nsion accuracy.\nFurthermore, we devise a well-designed\ngeometry-guided initialization strategy based on implicit\nSDF and Deep Marching Tetrahedra for the stability and\nconvergence of the training procedure. Experiments show\nour approach outperforms other state-of-the-art sparse-\nview methods, achieving ultra high-fidelity rendering qual-\nity at 2K resolution even under exaggerated expressions.\nProject page:\nhttps://yuelangx.github.io/\ngaussianheadavatar/.\n1. Introduction\nHigh-fidelity 3D human head avatar modeling is of great\nsignificance in many fields, such as VR/AR, telepresence,\ndigital human and film production.\nAutomatically cre-\nating high-fidelity avatars has been a research hotspot in\ncomputer vision for decades.\nAlthough some traditional\nhead avatars [37, 39, 41, 52] can realize high-fidelity an-\nimation, they typically require accurate geometries recon-\nstructed and tracked from dense multi-view videos, thus\nlimiting their applications in lightweight settings.\nThanks to the Neural Radiance Fields (NeRF) [43]\nwhich show great capability of novel view synthesis in\nthe absence of accurate geometry, recent methods [38, 55]\n1\narXiv:2312.03029v1  [cs.CV]  5 Dec 2023\nskip the geometry reconstruction and tracking steps but di-\nrectly learn high-quality NeRF-based head avatars. Other\nworks [42, 47, 69] have verified that NeRF can be applied\nto either dense or sparse views, which greatly lowers the\nthreshold for head avatar reconstruction. However, it still\nremains challenging for these NeRF-based approaches to\nsynthesize high-fidelity images at 2K resolutions with pixel-\nlevel details, including wrinkles and eyes.\nTo overcome this bottleneck and further improve the\navatar quality, we introduce 3D Gaussian splatting [26] for\n3D head avatar modeling. This is an explicit discrete repre-\nsentation that can be well adapted to the rasterization-based\nrendering pipeline. It has been verified that the 3D Gaussian\nrepresentation is capable of render complex scenes with low\ncomputational consumption. Compared to NeRF, the recon-\nstruction quality of static and dynamic scenes [40, 57, 65] is\nmuch better while rendering time cost has been significantly\nreduced. Motivated by this progress, we propose Gaussian\nHead Avatar, a novel representation that utilizes 3D Gaus-\nsian splatting for ultra high-fidelity head avatar modeling.\nAlthough recent 4D Gaussian works [40, 57, 64, 65] have\nbeen proposed to reconstruct dynamic scenes, all of them\ncannot be animated.\nFor modeling the animatable head\navatar, it is crucial but still unexplored how to effectively\ncontrol the deformation of 3D Gaussians and model the dy-\nnamic appearances through expression coefficients.\nPrevious explicit [71] and implicit [2, 70, 73] head\navatars usually formulate the facial deformation via lin-\near blend skinning (LBS) using the skinning weights and\nblendshapes like the FLAME model [33]. However, such\nan LBS-based formulation fails to represent exaggerated\nand fine-grained expressions by simple linear operations,\nlimiting the representation ability of the head avatars. In-\nspired by NeRSemble [29], we propose a fully learnable\nexpression-conditioned deformation field for the 3D head\nGaussians, avoiding the limited capability of the LBS-based\nformulation. Specifically, we input the positions of the 3D\nGaussians with expression coefficients into an MLP to di-\nrectly predict the displacements from the neutral expression\nto the target one. Similarly, we control the motion of non-\nface areas, such as the neck, using the head pose as the con-\ndition. 3D Gaussian-based representation has the powerful\nability to reconstruct high-frequency details, enabling our\nmethod to learn accurate deformation fields. In turn, the\nlearned accurate deformation field facilitates the dynamic\nGaussian head model to fit more dynamic details. As a re-\nsult, our method is able to reconstruct finer-grained dynamic\ndetails of expressive human heads.\nUnfortunately, as a discretized representation, the gra-\ndients back-propagated to the 3D Gaussians cannot spread\nthrough the whole space. Thus the convergence of train-\ning heavily relies on a plausible initialization for both the\ngeometry and the deformation field. However, simply ini-\ntializing the 3D head Gaussians with a morphable template\nlike FLAME [33] fails to model the long hairstyle and the\nshoulders. Hence, we further propose an efficient and well-\ndesigned geometry-guided initialization strategy.\nSpecif-\nically, instead of starting from stochastic Gaussians or a\nFLAME model, we initially optimize an implicit signed dis-\ntance function (SDF) field along with a color field and a de-\nformation MLP for modeling the basic geometry, color, and\nthe expression-conditioned deformations of the head avatar\nrespectively. The SDF field is converted to a mesh through\nDeep Marching Tetrahedra (DMTet) [50], with the color\nand deformation of the vertices predicted by the MLPs.\nThen we render the mesh and optimize them jointly under\nthe supervision of multi-view RGB images. Finally, we use\nthe mesh with per-vertex features from the SDF field to ini-\ntialize the 3D Gaussians to lie on the basic head surface\nwhile the color and deformation MLPs are carried over to\nthe next stage, ensuring stable training for convergence. The\nentire initialization process takes only around 10 minutes.\nThe contributions of our method can be summarized as:\n\u2022 We propose Gaussian Head Avatar, a new head avatar rep-\nresentation that employs controllable dynamic 3D Gaus-\nsians to model expressive human head avatars, producing\nultra high-fidelity synthesized images at 2K resolutions.\n\u2022 For modeling high-frequency dynamic details, we employ\na fully learned deformation field upon the 3D head Gaus-\nsians, which accurately model extremely complex and ex-\naggerated facial expressions.\n\u2022 We carefully design an efficient initialization strategy that\nleverages implicit representations to initialize the geome-\ntry and deformation, leading to efficient and robust con-\nvergence when training the Gaussian Head Avatar.\nBenefiting from these contributions, our method surpasses\nrecent state-of-the-art methods under lightweight sparse-\nview setups on the avatar quality by a large margin.\n2. Related Works\n3D Head Avatar Reconstruction. Due to the wide appli-\ncation value in the film and digital huamn industry, 3D head\navatar reconstruction from multi-view images has always\nbeen a research hotspot. Traditional works [4, 8, 19, 32] re-\nconstruct the scan geometry through multi-view stereo and\nthen register a face mesh template to it. However, such\nmethods usually require heavy computation. With the uti-\nlization of deep neural networks, current methods [7, 34, 59,\n63] achieve very fast reconstruction, producing even more\naccurate geometry. Lombardi et al. [37], Bi et al. [5] and\nMa et al. [41] represent the full head mesh through a deep\nneural network and train it with multi-view videos as super-\nvision. However, due to the errors in geometric estimation,\nmesh-based head avatars typically suffer from texture blur.\nTherefore, some recent methods [38, 55] utilize NeRF rep-\nresentation [43] to synthesize novel view images without\n2\ngeometry reconstruction, or build NeRF on the head mesh\ntemplate [39]. Furthermore, the NeRF-based methods are\nextended to sparse view reconstruction tasks [29, 42, 47, 69]\nand achieve impressive performance.\nMethods which focus on generative model [6, 9, 11,\n33, 46, 54, 58] are dedicated to learning general mesh\nface templates from large-scale multi-view face images or\n3D scans.\nRecently, implicit SDF-based [66] or NeRF-\nbased [14, 23, 51, 53, 72] methods can learn full-head tem-\nplates without the limitations of fixed topology, thereby bet-\nter modeling complex hairstyles and glasses. Cao et al. [14]\nadopts a hybrid representation of local NeRF built on the\nmesh surface, which enables high-fidelity rendering and\nflexible expression control.\n3D head avatars reconstruction from monocular videos is\nalso a popular yet challenging research topic. Early meth-\nods [12, 13, 15, 24, 25, 45] optimize a morphable mesh to fit\nthe training video. Recent methods [20, 27] leverage neural\nnetworks to learn non-rigid deformation upon 3DMM face\ntemplates [18, 33], thus can recover more dynamic details.\nSuch methods are not flexible enough to handle complex\ntopologies. Therefore, the latest methods explore to con-\nstruct head avatar models based on implicit SDF [70], point\nclouds [71] or NeRF [2, 3, 16, 17, 21, 36, 61, 62, 73].\nPoint-based Rendering.\nPoint elements as a discrete\nand unstructured representation can fit geometry with arbi-\ntrary topology [67] efficiently. Recent methods [30, 31, 56]\nopen up a differentiable rasterization pipeline, such that the\npoint-based representation is widely used in multi-view re-\nconstruction tasks. Aliev et al. [1] and Ruckert et al. [48]\npropose to first render the feature map, which is trans-\nferred to the images through a convolutional renderer. Xu\net al.[60] use neural point cloud associated with neural fea-\ntures to model a NeRF. Recently, 3D Gaussian splatting [26]\nshows its superior performance, beating NeRF in both novel\nview synthesis quality and rendering speed.\nSome ap-\nproaches [40, 57, 64, 65] extend Gaussian representation\nto dynamic scene reconstruction. However, these methods\ncan not be migrated to the head avatar reconstruction tasks.\n3. Overview\nThe pipeline of the reconstruction of Gaussian Head Avatar\nis illustrated in Fig. 2. Before the beginning of the pipeline,\nwe remove the background [35] of each image and jointly\nestimate the 3DMM model [18], 3D facial landmarks and\nthe expression coefficients for each frame. In the initial-\nization stage (Sec. 4.3), we reconstruct an SDF-based neu-\ntral geometry, and optimize a deformation MLP and a color\nMLP from the training data as the guidance model. Next,\nwe extract the neutral mesh through DMTet to initialize the\nneutral Gaussians while the deformation and color MLPs\nare also inherited from the initialization stage. In the train-\ning stage of Gaussian Head Avatar (Sec. 4.2), we deform\nthe neutral Gaussians to the target expression through the\ndynamic generator given the driving expression coefficients\nas the condition. Finally, given a camera view, the expres-\nsive Gaussians are rendered to a feature map, which is then\nfed into the convolutional super resolution network to gen-\nerate high-resolution avatar images. The whole model is op-\ntimized under the supervision of multi-view RGB videos.\n4. Method\n4.1. Avatar Representation\nGenerally, the static 3D Gaussians [26] with N points are\nrepresented by their positions X, the multi-channel color C,\nthe rotation Q, scale S and opacity A. The rotation Q is rep-\nresented in the form of quaternion. Subsequently, the Gaus-\nsians can be rasterized and rendered to the multi-channel\nimage I given the camera parameters \u00b5. This process can\nbe formulated as:\nI = R(X, C, Q, S, A; \u00b5).\n(1)\nOur task is to reconstruct a dynamic head avatar con-\ntrolled by expression coefficients. Therefore, we formulate\nthe head avatar as dynamic 3D Gaussians conditioned on\nexpressions. To handle the dynamic changes, we input the\nexpression coefficients with head pose to the head avatar\nmodel and output the position and other attributes of the\nGaussians as above.\nSpecifically, we first construct a canonical neutral\nGaussian model with expression-independent attributes:\n{X0, F 0, Q0, S0, A0}, which are fully optimizable. X0 \u2208\nRN\u00d73 denotes the positions of the Gaussians with a neu-\ntral expression in the canonical space. F 0 \u2208 RN\u00d7128 de-\nnotes the point-wise feature vectors as their intrinsic prop-\nerties. Q0 \u2208 RN\u00d74, S0 \u2208 RN\u00d73 and A0 \u2208 RN\u00d71 denotes\nthe neutral rotation, scale and opacity respectively. Note\nthat we do not define the neutral color, but directly predict\nexpression-dependent dynamic color from the point-wise\nfeature vectors F 0. Then, we construct an MLP-based ex-\npression conditioned dynamic generator \u03a6 to generate all\nthe extra dynamic changes to the neutral model. Overall,\nthe whole Gaussian head avatar can be formulated as:\n{X, C, Q, S, A} = \u03a6(X0, F 0, Q0, S0, A0; \u03b8, \u03b2),\n(2)\nwith \u03b8 denoting expression coefficients and \u03b2 denoting the\nhead pose.\nDuring the training, we optimize all the pa-\nrameters of the dynamic generator \u03a6 and the neutral Gaus-\nsian model {X0, F 0, Q0, S0, A0}, which are highlighted\nin bold in the following.\nNext, we explain the process of adding expression-\nrelated changes to the neutral Gaussian model through the\ndynamic generator \u03a6 as described in Eqn. 2 in detail.\nPositions X\u2032 of the Gaussians. Expressions bring about\nthe geometric deformation of the neutral model, which\n3\nFigure 2. The overview of the Gaussian Head Avatar rendering and reconstruction. We first optimize the guidance model including a\nneutral mesh, a deformation MLP and a color MLP in the Initialization stage. Then we use them to initialize the neutral Gaussians and\nthe dynamic generator. Finally, 2K RGB images are synthesized through differentiable rendering and the super-resolution network. The\nGaussian Head Avatar are trained under the supervision of multi-view RGB videos.\nis modeled as the displacements of the Gaussian points.\nSpecifically, we predict the displacements respectively con-\ntrolled by the expression and the head pose in the canon-\nical space through two different MLPs: f exp\ndef \u2208 \u03a6 and\nf pose\ndef \u2208 \u03a6. Then, we add them to the neutral positions.\nX\u2032 = X0 + \u03bbexp(X0)f exp\ndef(X0, \u03b8)\n+\u03bbpose(X0)f pose\ndef (X0, \u03b2).\n(3)\n\u03bbexp(\u00b7) and \u03bbpose(\u00b7) represent the extent to which the points\nare affected by the expression or the head pose respectively.\nHere, we assume that the Gaussian points closer to 3D land-\nmarks are more affected by the expression coefficients and\nless affected by the head pose, while the opposite is true\nfor the Gaussian points far away. Specifically, The 3D land-\nmarks P 0 of the canonical model are first estimated through\nthe 3DMM model in the data preprocessing and then opti-\nmized in the initialization stage 4.3. Then for each Gaussian\npoint, we calculate the above weight \u03bbexp(\u00b7) and \u03bbpose(\u00b7) as\nfollows:\n\u03bbexp(x) =\n\uf8f1\n\uf8f2\n\uf8f3\n1,\ndist(x, P 0) < t1\nt2\u2212dist(x,P 0)\nt2\u2212t1\n,\ndist(x, P 0) \u2208 [t1, t2]\n0,\ndist(x, P 0) > t2\nwith \u03bbpose(x) = 1 \u2212 \u03bbexp(x). And x \u2208 X0 denotes the\nposition of one neutral Gaussian. dist(x, P 0) denotes the\nminimum distance from the point x to the 3D landmarks\nP 0. t1 = 0.15 and t2 = 0.25 are predefined hyperparam-\neters when the length of the head is set to approximately\n1.\nColor C\u2032 of the Gaussians. Modeling the dynamic de-\ntails typically requires dynamic color that changes with ex-\npressions.\nAs we do not pre-define the neutral value in\nEqn. 2, the color are directly predict by two color MLPs:\nf exp\ncol \u2208 \u03a6 and f pose\ncol\n\u2208 \u03a6:\nC\u2032 = \u03bbexp(X0)f exp\ncol (F 0, \u03b8)\n+\u03bbpose(X0)f pose\ncol (F 0, \u03b2).\n(4)\nRotation, Scale and Opacity {Q\u2032, S\u2032, A\u2032} of the Gaus-\nsians. These three attributes also dynamic, thereby model-\ning some detailed expressions-related appearance changes.\nHere, we just use another two attribute MLPs f exp\natt \u2208 \u03a6\nand f pose\natt\n\u2208 \u03a6 to predict their shift from the neutral value.\n{Q\u2032, S\u2032, A\u2032} = {Q0, S0, A0}\n+\u03bbexp(X0)f exp\natt (F 0, \u03b8)\n+\u03bbpose(X0)f pose\natt (F 0, \u03b2).\n(5)\nFinally, we apply rigid rotations and translations T(\u00b7) to\nthe Gaussians, transforming them from the canonical space\nto the world space. Note, the transformation is only imple-\nmented for directional variables: {X\u2032, Q\u2032}, while the multi-\nchannel color, the scale and the opacity {C\u2032, S\u2032, A\u2032} are not\ndirectional thus remain unchanged.\n{X, Q} = T({X\u2032, Q\u2032}, \u03b2),\n(6)\n{C, S, A} = {C\u2032, S\u2032, A\u2032}.\n(7)\n4.2. Training\nIn this part, we explain the training pipeline of the Gaussian\nhead avatar 4.1 and the loss function. In each iteration, we\nfirst generate the expression conditioned 3D Gaussians as\nEqn. 2. Then we render a 32-channel image with 512 reso-\nlution IC \u2208 R512\u00d7512\u00d732 referring to Eqn. 1. After that we\nfeed the image to a super resolution network \u03a8 to generate\na 2048 resolution RGB image Ihr \u2208 R2048\u00d72048\u00d73, such\nthat more details are recovered and noise caused by uneven\nambient light or camera chromatic aberration in the training\ndata will be filtered out [48, 62].\nDuring training, we jointly optimize all the learnable\nparamters mentioned above in bold. For the loss function,\nwe only use the foreground RGB images Igt as supervision\nto construct an L1 loss and a VGG perceptual loss [68] be-\ntween the generated images Ihr and the ground truth Igt.\nBesides, we encourage the first three channels of the 32-\nchannel feature image IC to be RGB channels, which is\n4\nensured by a L1 loss term. The total loss is:\nL = ||Ihr \u2212 Igt||1 + \u03bbvggV GG(Ihr, Igt)\n+\u03bblr||Ilr \u2212 Igt||1,\n(8)\nwith Ilr denoting the first three channels of the 32-channel\nimage IC. We set the weights \u03bbvgg = 0.1 and \u03bblr = 0.1.\n4.3. Geometry-guided Initialization\nDue to the fact that the Gaussians representation is un-\nordered and unstructured, it is difficult for the gradient to\ncontinueously spread to the nearby points in space after it\nis propagated back to one point. Consequently, randomly\ninitializing the neutral Gaussians usually leads to failure\nto converge and initializing with a FLAME model fails to\nmodel the long hairstyle and the shoulders. To overcome\nthis problem, we propose to utilize the implicit signed dis-\ntance field (SDF) representation and Deep Marching Tetra-\nhedra (DMTet) to first reconstruct a neutral mesh for initial-\nizing the Gaussian positions. We also coarsely optimize the\ncolor MLPs and the deformation MLPs in Sec.4.1 as well.\nRepresentation and Rendering. Specifically, we first\nconstruct a MLP f sdf to represent a signed distance field.\nIn addition, this network will also output the corresponding\nfeature vector of each point, which is used for predicting the\npoint color. It can be formulated as:\ns, \u03b7 = f sdf(x),\n(9)\nwith s denotes the SDF value, \u03b7 denotes the feature\nvector and x denotes the point position.\nThen through\n(DMTet) [49], we can differentially extract a mesh with ver-\ntices X, per-vertex feature vectors F and its faces. We also\npredict the per-vertex 32-channel color as Eqn. 4 by the two\ncolor MLPs f exp\ncol and f pose\ncol . In parallel, we construct the\ntwo deformation MLPs: f exp\ndef and f pose\ndef\nas described in\nSec. 4.1 to predict the displacements and add them to the\nvertex positions. This process is similar to Eqn 3 above,\nwith the Gaussian positions X0 replaced by the vertex po-\nsitions X. Finally we also apply rigid rotations and trans-\nlations to the deformed mesh, transforming it to the world\nspace and render the deformed mesh into an image I and a\nmask M through differentiable rasterization [44] according\nto the camera parameters \u00b5.\nLoss Function and Training. Next, we can construct\nthe RGB loss and the silhouette loss to train the guidance\nmodel:\nLRGB = ||Ir,g,b \u2212 Igt||1,\n(10)\nLsil = IOU(M, Mgt),\n(11)\nwith Igt and Mgt denote the ground truth RGB image\nand mask, respectively. IOU(\u00b7) denotes Intersection over\nUnion metrics.\nNote that only the first three channels\nR, G, B of the 32-channel image I are supervised by the\nground truth RGB images.\nWe also use the estimated 3D facial landmarks Pgt as\ndescribed in Sec. 3 to provide rough guidance for the ex-\npression deformation MLP. Specifically, we input the neu-\ntral 3D landmarks P 0 into the expression deformation MLP\nto predict the expression conditioned landmarks P:\nP = P 0 + f exp\ndef(P 0, \u03b8).\n(12)\nThen we construct the loss function with 3D facial land-\nmarks Pgt as the supervision:\nLdef = ||P \u2212 Pgt||2,\n(13)\nBesides, we introduce three constraints: (1) a regular\nterm Loffset to punish all non-zero displacements to pre-\nvent the two deformation MLPs from learning a global con-\nstant offset [61], (2) a regular term Llmk to limit the SDF\nvalue at the 3D landmarks to be close to zero, such that\nthe landmarks are located on the surface of the mesh, (3)\na Laplacian term Llap for maintaining the extracted mesh\nsmooth to a certain extent. Overall, the total loss function is\nformulated as:\nL = LRGB + \u03bbsilLsil + \u03bbdefLdef+\n\u03bboffsetLoffset + \u03bblmkLlmk + \u03bblapLlap,\n(14)\nwith \u03bb denoting the weights of each term, which are set as\nfollows: \u03bbsil = 0.1, \u03bbdef = 1, \u03bboffset = 0.01, \u03bblmk = 0.1\nand \u03bblap = 100. We jointly optimize the MLPs mentioned\nabove with the neutral 3D landmarks P 0 jointly until all\nMLPs are converged.\nParameters Transfer.\nFinally, we use the roughly\ntrained guidance model to initialize the Gaussian head\nmodel. Specifically, we extract the neutral mesh with ver-\ntices X and per-vertex features F through DMTet, and di-\nrectly assign their values to the neutral positions X0 = X\nand the per-vertex feature vectors F 0 = F of the neutral\nGaussians respectively. For the other neutral attributes, we\nadopt the original initialization strategy in Gaussian Splat-\nting [26]. Then, we retain all the four optimized MLPs:\n{f exp\ncol , f pose\ncol , f exp\ndef, f pose\ndef } for the next stage, while the pa-\nrameters of the two attribute MLPs: {f exp\natt , f pose\natt } and the\nsuper resolution network \u03a8 are randomly initialized.\n5. Experiments\n5.1. Implementation Details\nIn the experiment we use 12 sets of data, 10 of which\nare from NeRSemble [29], and the other 2 are multi-view\nvideo data from HAvatar [69]. For the 10 identities from\nNeRSemble, each set contains 2500 to 3000 frames, 16\n5\nFigure 3. Qualitative comparisons of different methods on self reenactment task. From left to right: NeRFBlendShape [17], NeRFace [16],\nHAvatar [69] and Ours. Our method can reconstruct details like beards, teeth, etc. with high quality.\ncameras are distributed about 120 degrees in front, and si-\nmultaneously capture 2K resolution video. For each iden-\ntity, We use the sequences marked with \u201dFREE\u201d as the eval-\nuation data, and the rest as the training data. For the 2 identi-\nties from HAvatar, each set contains 3000 frames, 8 cameras\nare distributed about 120 degrees in front, and 4K resolution\nvideos are collected simultaneously. Later, we crop the face\narea and resize to 2K resolution.\nFor data preprocessing, we first remove the back-\nground [35] and extract 68 2D facial landmarks [10] for all\nthe images. Then, for each frame, we use multi-view im-\nages to estimate the corresponding 3D landmarks, expres-\nsion coefficients and head pose by fitting the Basel Face\nModel (BFM) [18] to the extracted 2D landmarks. Note that\nwe define the 3D landmarks as the usual 68 landmarks with\nvertices indexed as multiples of 100 in the BFM vertices.\nDuring the geometry-guided initialization stage, we use\nan Adam [28] optimizer, and set the learning rate to 1\u00d710\u22123\nfor all the networks and 1 \u00d7 10\u22124 for the neutral 3D land-\nmarks P 0. Then We train the model for 10000 iterations\nwith a batch size of 4.\nDuring the Gaussian model training stage, we also use\nan Adam optimizer, and set the learning rate to 1 \u00d7 10\u22124\nfor the two color MLPs and two attribute MLPs, 1 \u00d7 10\u22125\n6\nFigure 4. Qualitative comparisons of different methods on cross-identity reenactment task. From left to right: NeRFBlendShape [17],\nNeRFace [16], HAvatar [69] and Ours. Our method synthesizes high-fidelity images while ensuring the accuracy of expression transfer.\nfor the two deformation MLPs, 1 \u00d7 10\u22125 for the neutral\npositions X0, 1 \u00d7 10\u22125 for the point-wise feature vectors\nF 0, 1 \u00d7 10\u22124 for the neutral rotation Q0, 3 \u00d7 10\u22124 for the\nneutral scale S0, 1 \u00d7 10\u22123 for the neutral opacity Q0 and\n1 \u00d7 10\u22124 for the super resolution network \u03a8. Finally, we\ntrain the Gaussian model for 600000 iterations with a batch\nsize of 1 until fully convergence.\n5.2. Results and Comparisons\nIn this section, we first compare our method with existing\nSOTA methods in qualitative experiments on self reenact-\nment task. Specifically, NeRFace [16] uses a deep MLP\nto fit an expression condtioned dynamic NeRF. The cur-\nrent SOTA method HAvatar [69] introduces 3DMM tem-\nplate prior and uses a deep convolutional network to gener-\nate a human head NeRF represented by three planes from\na mesh template with expression.\nNote, HAvatar lever-\nages the GAN framework using the adversarial loss function\nto force the network to generate details that are not view-\nconsistent. For a fair comparison, we remove this part and\nuse VGG perceptual loss as Sec. 4.2 instead.\nQualitative results on self reenactment task are shown in\nthe Fig. 3. Our method can accurately reconstruct pixel-\nlevel high-frequency details such as beards, teeth, and hair.\nBesides, our method can achieve expression transfer more\naccurately, such as eye movements in the figure.\nNext, we conduct a quantitative evaluation for the four\nmethods on 5 identities and 6 cameras using the evaluation\nsplit. The evaluation metrics include: Peak Signal-to-Noise\nRatio (PSNR), Structure Similarity Index (SSIM), Learned\nPerceptual Image Patch Similarity (LPIPS) [68] and Fr\u00b4echet\nInception Distance (FID) [22]. As the task mainly focuses\non the reconstruction of the head, we use face parsing 1 to\nremove the body parts in the image to eliminate their impact\nin the experiment. As shown in Tab. 1, our method demon-\nstrates a slight improvement in PSNR and SSIM compared\nwith previous methods, and a significant improvement in\nLPIPS and FID, which means that our method can generate\nmore high-frequency details.\nAnd we also qualitatively compare our method with the\nabove SOTA methods on cross-identity reenactment task.\nAs shown in the results, our method is able to synthesize\nhigher-fidelity images with more accurate expression trans-\nfer and richer emotions.\n5.3. Ablation Study\nAblation on Initialization Strategies. In order to verify\nthe effectiveness of our geometry-guided initialization strat-\negy 4.3, we compare it with the strategy to use the FLAME\nmodel for initialization (FLAME-Init). Specifically, after\nfitting a FLAME model through multi-view data, we first\nsubdivide the FLAME mesh 4 times and use the neutral ver-\ntices as the positions of the neutral Gaussians. Then, the\nexpression deformation MLP is optimized to learn the dis-\nplacement of FLAME vertices. We set the per-vertex fea-\nture to zeros, while randomly initialize the parameters of the\nexpression color MLP. The initialization of other variables\nis the same as our strategy. Qualitative results are shown in\nthe Fig. 5. Due to the lack of initialization for the hair and\nshoulders in FLAME-Init, the points to model these parts\nare offset from nearby vertices, which leads to sparseness\n1https://github.com/zllrunning/face-parsing.PyTorch\n7\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS (512) \u2193\nLPIPS (2K) \u2193\nFID (2K) \u2193\nNeRFBlendShape\n25.91\n0.836\n0.123\n0.229\n54.80\nNeRFace\n27.14\n0.849\n0.147\n0.234\n65.11\nHAvatar\n27.19\n0.883\n0.064\n0.209\n31.06\nOurs (w/o SR)\n27.82\n0.887\n0.080\n0.202\n45.50\nOurs\n27.70\n0.883\n0.056\n0.098\n18.50\nTable 1. Quantitative evaluation results of NeRFBlendShape [17], NeRFace [16], HAvatar [69], our method without super resolution and\nour full method.\nFigure 5. Ablation study on the initialization strategies: FLAME-\ninitialization and our geometry-guided initialization. Our strategy\nensures that the hair strands away from the head are well recon-\nstructed.\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFLAME-Init\n28.73\n0.875\n0.123\nMesh-Deform\n28.83\n0.874\n0.116\nOurs\n28.94\n0.876\n0.108\nTable 2. Quantitative evaluation results of the other two ablation\nbaselines and ours.\nof the Gaussians, resulting in blurring.\nAblation on Deformation Modeling Approaches.\nWe\ncompare our fully learned deformation field with the previ-\nous mesh-based deformation (Mesh-Deform). Specifically,\nwe migrate the method in INSTA [73] for controlling the\nNeRF deformation to our Gaussians. First we fit a 3DMM\nmesh template. Then, for each Gaussian point, find the clos-\nest face on the mesh, and calculate the deformation gradient\nto estimate the displacement. Qualitative results are shown\nin the Fig. 6. For some expressions that cannot be cap-\ntured well by the 3DMM mesh template, our method can\nlearn accurate deformation, thereby achieving the modeling\nof complex expressions.\nQuantitave results are shown in the Fig. 2. Our method\noutperforms both the two ablation baselines on PSNR,\nFigure 6.\nAblation study on the deformation Modeling ap-\nproaches: mesh LBS-based deformation and our fully learned de-\nformation. Our approach can learn complex and exaggerated ex-\npressions.\nSSIM and LPIPS metrics.\n6. Discussion and Conclusion\nEthical Considerations. Our method is capable of creat-\ning artificial portrait videos, which have the potential to dis-\nseminate misinformation, influence public perceptions, and\nerode confidence in media sources. This can result in con-\nsiderable negative impacts on society. Hence, it\u2019s critical to\nconsider approaches that can reliably distinguish between\nauthentic and forged content.\nLimitation. Although our method has taken a substantial\nstep forward in the quality of image synthesis under com-\nplex expressions, one previous problem has still not been\nsolved. For the tongue and teeth inside the mouth or long\nhair, blurring is sometimes produced in our method due to\nthe lack of tracking methods.\nConclusion.\nIn this paper, we propose Gaussian Head\nAvatar, a novel representation for head avatar reconstruc-\ntion, which leverages dynamic 3D Gaussians controlled\nby a fully learned expression deformation.\nExperiments\ndemonstrate our method can synthesize ultra high-fidelity\nimages while modeling complex and exaggerated expres-\nsions. In addition, we propose a well-designed minute-level\n8\ninitialization strategy to ensure the training convergence of\nthe Gaussian Head Avatar. We believe our Gaussian Head\nAvatar will become the mainstream direction for head avatar\nreconstruction under sparse view setups in the future.\nReferences\n[1] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry\nUlyanov, and Victor Lempitsky. Neural point-based graph-\nics.\nIn European Conference on Computer Vision, pages\n696\u2013712, 2020. 3\n[2] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli\nShechtman, and Zhixin Shu. Rignerf: Fully controllable neu-\nral 3d portraits. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n2, 3\n[3] ShahRukh Athar, Zhixin Shu, and Dimitris Samaras. Flame-\nin-nerf: Neural control of radiance fields for free view face\nanimation. In IEEE 17th International Conference on Auto-\nmatic Face and Gesture Recognition (FG), pages 1\u20138, 2023.\n3\n[4] Thabo Beeler, Bernd Bickel, Paul Beardsley, Bob Sumner,\nand Markus Gross. High-quality single-shot capture of facial\ngeometry. ACM Trans. Graph., 29(4), 2010. 2\n[5] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon,\nShih-En Wei, Kevyn Mcphail, Ravi Ramamoorthi, Yaser\nSheikh, and Jason Saragih.\nDeep relightable appearance\nmodels for animatable faces.\nACM Trans. Graph., 40(4),\n2021. 2\n[6] V Blanz and T Vetter. A morphable model for the synthesis\nof 3d faces. In 26th Annual Conference on Computer Graph-\nics and Interactive Techniques (SIGGRAPH 1999), pages\n187\u2013194. ACM Press, 1999. 3\n[7] Timo Bolkart, Tianye Li, and Michael J. Black.\nInstant\nmulti-view head capture through learnable registration. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 768\u2013779, 2023. 2\n[8] Derek Bradley, Wolfgang Heidrich, Tiberiu Popa, and Alla\nSheffer. High resolution passive facial performance capture.\n29(4), 2010. 2\n[9] Alan Brunton, Timo Bolkart, and Stefanie Wuhrer. Multilin-\near wavelets: A statistical shape space for human faces. In\nProceedings of the Proceedings of the European Conference\non Computer Vision (ECCV), 2014. 3\n[10] Adrian Bulat and Georgios Tzimiropoulos. How far are we\nfrom solving the 2d & 3d face alignment problem? (and a\ndataset of 230,000 3d facial landmarks).\nIn International\nConference on Computer Vision, 2017. 6\n[11] Chen Cao, Yanlin Weng, Shun Zhou, Y. Tong, and Kun\nZhou. Facewarehouse: A 3d facial expression database for\nvisual computing.\nIn IEEE Transactions on Visualization\nand Computer Graphics, pages 413\u2013425, 2014. 3\n[12] Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler.\nReal-time high-fidelity facial performance capture.\nACM\nTrans. Graph., 34(4), 2015. 3\n[13] Chen Cao, Hongzhi Wu, Yanlin Weng, Tianjia Shao, and\nKun Zhou. Real-time facial animation with image-based dy-\nnamic avatars. ACM Trans. Graph., 35(4), 2016. 3\n[14] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,\nMichael Zollhoefer, Shun-Suke Saito, Stephen Lombardi,\nShih-En Wei, Danielle Belko, Shoou-I Yu, Yaser Sheikh, and\nJason Saragih. Authentic volumetric avatars from a phone\nscan. ACM Trans. Graph., 41(4), 2022. 3\n[15] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde\nJia, and Xin Tong.\nAccurate 3d face reconstruction with\nweakly-supervised learning: From single image to image set.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops, 2019. 3\n[16] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias\nNiessner. Dynamic neural radiance fields for monocular 4d\nfacial avatar reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 8645\u20138654, 2021. 3, 6, 7, 8\n[17] Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong,\nYudong Guo, and Juyong Zhang.\nReconstructing person-\nalized semantic facial nerf models from monocular video.\nACM Transactions on Graphics (Proceedings of SIGGRAPH\nAsia), 41(6), 2022. 3, 6, 7, 8\n[18] Thomas Gerig, Andreas Forster, Clemens Blumer, Bernhard\nEgger, Marcel L\u00a8uthi, Sandro Sch\u00a8onborn, and Thomas Vetter.\nMorphable face models - an open framework. pages 75\u201382,\n2017. 3, 6\n[19] Abhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong, Jay\nBusch, Xueming Yu, and Paul Debevec. Multiview face cap-\nture using polarized spherical gradient illumination. ACM\nTrans. Graph., 30(6):1\u201310, 2011. 2\n[20] Philip-William Grassal,\nMalte Prinzler,\nTitus Leistner,\nCarsten Rother, Matthias Nie\u00dfner, and Justus Thies. Neural\nhead avatars from monocular rgb videos. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 18632\u201318643, 2022. 3\n[21] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun\nBao, and Juyong Zhang. Ad-nerf: Audio driven neural ra-\ndiance fields for talking head synthesis. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 5764\u20135774, 2021. 3\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Neural Information Processing Systems, 2017. 7\n[23] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juy-\nong Zhang.\nHeadnerf: A real-time nerf-based parametric\nhead model. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n20374\u201320384, 2022. 3\n[24] Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jae-\nwoo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen-\nChun Chen, and Hao Li. Avatar digitization from a single\nimage for real-time rendering. ACM Trans. Graph., 36(6),\n2017. 3\n[25] Alexandru Eugen Ichim, Sofien Bouaziz, and Mark Pauly.\nDynamic 3d avatar creation from hand-held video input.\nACM Trans. Graph., 34(4), 2015. 3\n[26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\n9\nradiance field rendering. ACM Transactions on Graphics, 42\n(4), 2023. 2, 3, 5\n[27] Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and\nEgor Zakharov. Realistic one-shot mesh-based head avatars.\nIn Proceedings of the European Conference on Computer Vi-\nsion (ECCV), 2022. 3\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization, 2017. 6\n[29] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim\nWalter, and Matthias Nie\u00dfner. Nersemble: Multi-view ra-\ndiance field reconstruction of human heads.\nACM Trans.\nGraph., 42(4), 2023. 2, 3, 5\n[30] Georgios Kopanas, Thomas Leimk\u00a8uhler, Gilles Rainer,\nCl\u00b4ement Jambon, and George Drettakis. Neural point cata-\ncaustics for novel-view synthesis of reflections. ACM Trans-\nactions on Graphics (TOG), 41(6):1\u201315, 2022. 3\n[31] Christoph Lassner and Michael Zollhofer.\nPulsar: Effi-\ncient sphere-based neural rendering.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1440\u20131449, 2021. 3\n[32] Marc\nLevoy,\nKari\nPulli,\nBrian\nCurless,\nSzymon\nRusinkiewicz, David Koller, Lucas Pereira, Matt Ginz-\nton,\nSean Anderson,\nJames Davis,\nJeremy Ginsberg,\nJonathan Shade, and Duane Fulk. The digital michelangelo\nproject: 3d scanning of large statues.\nIn Proceedings of\nthe 27th Annual Conference on Computer Graphics and\nInteractive Techniques, page 131\u2013144, USA, 2000. ACM\nPress/Addison-Wesley Publishing Co. 2\n[33] Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and ex-\npression from 4d scans. ACM Trans. Graph., 36(6), 2017. 2,\n3\n[34] Tianye Li, Shichen Liu, Timo Bolkart, Jiayi Liu, Hao Li,\nand Yajie Zhao. Topologically consistent multi-view face in-\nference using volumetric sampling. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 3824\u20133834, 2021. 2\n[35] Shanchuan\nLin,\nAndrey\nRyabtsev,\nSoumyadip\nSen-\ngupta, Brian Curless, Steve Seitz, and Ira Kemelmacher-\nShlizerman. Real-time high-resolution background matting.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2021. 3, 6\n[36] Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne\nWu, and Bolei Zhou. Semantic-aware implicit neural audio-\ndriven video portrait generation. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), 2022. 3\n[37] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser\nSheikh. Deep appearance models for face rendering. ACM\nTrans. Graph., 37(4):68:1\u201368:13, 2018. 1, 2\n[38] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\nACM Trans. Graph., 38(4):65:1\u201365:14, 2019. 1, 2\n[39] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM Trans. Graph., 40(4), 2021. 1, 3\n[40] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan. Dynamic 3d gaussians: Tracking by per-\nsistent dynamic view synthesis, 2023. 2, 3\n[41] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang,\nYuecheng Li, Fernando De La Torre, and Yaser Sheikh. Pixel\ncodec avatars. In 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 64\u201373, 2021.\n1, 2\n[42] Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu\nTang, and Shunsuke Saito.\nKeypointnerf:\nGeneralizing\nimage-based volumetric avatars using relative spatial encod-\ning of keypoints. In European conference on computer vi-\nsion, 2022. 2, 3\n[43] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2020. 1, 2\n[44] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting Triangular 3D Models, Materials, and Light-\ning From Images. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 8280\u20138290, 2022. 5\n[45] Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo\nLi, Shunsuke Saito, Aviral Agarwal, Jens Fursund, and Hao\nLi. Pagan: Real-time avatars using dynamic textures. ACM\nTrans. Graph., 37(6), 2018. 3\n[46] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami\nRomdhani, and Thomas Vetter. A 3d face model for pose\nand illumination invariant face recognition. In 2009 Sixth\nIEEE International Conference on Advanced Video and Sig-\nnal Based Surveillance, pages 296\u2013301, 2009. 3\n[47] Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih,\nShunsuke Saito, James Hays, and Stephen Lombardi. Pva:\nPixel-aligned volumetric avatars.\nIn arXiv:2101.02697,\n2020. 2, 3\n[48] Darius R\u00a8uckert, Linus Franke, and Marc Stamminger. Adop:\nApproximate differentiable one-pixel point rendering. ACM\nTrans. Graph., 41(4), 2022. 3, 4\n[49] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021. 5\n[50] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021. 2\n[51] Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong\nZhang, Hongwen Zhang, and Yebin Liu. Next3d: Gener-\native neural texture rasterization for 3d-aware head avatars.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 3\n[52] Cong Wang, Di Kang, Yanpei Cao, Linchao Bao, Ying Shan,\nand Song-Hai Zhang. Neural point-based volumetric avatar:\nSurface-guided neural points for efficient and photorealistic\nvolumetric head avatar. In ACM SIGGRAPH Asia 2023 Con-\nference Proceedings, 2023. 1\n10\n[53] Daoye Wang, Prashanth Chandran, Gaspard Zoss, Derek\nBradley, and Paulo Gotardo.\nMorf: Morphable radiance\nfields for multiview neural head modeling.\nIn ACM SIG-\nGRAPH 2022 Conference Proceedings, New York, NY,\nUSA, 2022. Association for Computing Machinery. 3\n[54] Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang\nLi, and Yebin Liu.\nFaceverse: a fine-grained and detail-\ncontrollable 3d face morphable model from a hybrid dataset.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 3\n[55] Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas\nSimon, Jason Saragih, Jessica Hodgins, and Michael Zoll-\nhofer. Learning compositional radiance fields of dynamic\nhuman heads. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5704\u20135713, 2021. 1, 2\n[56] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\nJohnson. SynSin: End-to-end view synthesis from a single\nimage. In CVPR, 2020. 3\n[57] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering,\n2023. 2, 3\n[58] Sijing Wu, Yichao Yan, Yunhao Li, Yuhao Cheng, Wenhan\nZhu, Ke Gao, Xiaobo Li, and Guangtao Zhai. Ganhead: To-\nwards generative animatable neural head avatars.\nIn Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 437\u2013447, 2023. 3\n[59] Yunze Xiao, Hao Zhu, Haotian Yang, Zhengyu Diao, Xi-\nangju Lu, and Xun Cao.\nDetailed facial geometry recov-\nery from multi-view images by learning an implicit function.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, 2022. 2\n[60] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 3\n[61] Yuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen\nZhang, and Yebin Liu.\nAvatarmav: Fast 3d head avatar\nreconstruction using motion-aware neural voxels. In ACM\nSIGGRAPH 2023 Conference Proceedings, 2023. 3, 5\n[62] Yuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen\nZhao, Huang Han, Qi Guojun, and Yebin Liu. Latentavatar:\nLearning latent expression code for expressive neural head\navatar. In ACM SIGGRAPH 2023 Conference Proceedings,\n2023. 3, 4\n[63] Kai Yang, Hong Shang, Tianyang Shi, Xinghan Chen,\nJingkai Zhou, Zhongqian Sun, and Wei Yang. Asm: Adap-\ntive skinning model for high-quality 3d face modeling sup-\nplementary material. 2021. 2\n[64] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin. Deformable 3d gaussians for high-\nfidelity monocular dynamic scene reconstruction, 2023. 2, 3\n[65] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\nZhang. Real-time photorealistic dynamic scene representa-\ntion and rendering with 4d gaussian splatting, 2023. 2, 3\n[66] T Yenamandra, A Tewari, F Bernard, HP Seidel, M El-\ngharib, D Cremers, and C Theobalt. i3dmm: Deep implicit\n3d morphable model of human heads.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 3\n[67] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00a8Oztireli, and\nOlga Sorkine-Hornung. Differentiable surface splatting for\npoint-based geometry processing.\nACM Transactions on\nGraphics (proceedings of ACM SIGGRAPH ASIA), 38(6),\n2019. 3\n[68] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 586\u2013595, 2018. 4, 7\n[69] Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, Hongwen\nZhang, Jinli Suo, and Yebin Liu. Havatar: High-fidelity head\navatar via facial model conditioned neural radiance field.\nACM Trans. Graph., 2023. Just Accepted. 2, 3, 5, 6, 7,\n8\n[70] Yufeng Zheng, Victoria Fern\u00b4andez Abrevaya, Marcel C.\nB\u00a8uhler, Xu Chen, Michael J. Black, and Otmar Hilliges. I\nm avatar: Implicit morphable head avatars from videos. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 13535\u201313545,\n2022. 2, 3\n[71] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J.\nBlack, and Otmar Hilliges. Pointavatar: Deformable point-\nbased head avatars from videos.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 2, 3\n[72] Yiyu Zhuang, Hao Zhu, Xusen Sun, and Xun Cao. Mofanerf:\nMorphable facial neural radiance field. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2022. 3\n[73] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Instant\nvolumetric head avatars, 2022. 2, 3, 8\n11\nGaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians\nSupplementary Material\n7. Failure Case\nFor non-face areas, our method inputs the head pose as the\ncondition to control the deformation, which is not able to\nmodel the complex dynamic deformation of long hair, re-\nsulting in blurred rendering results as shown in Fig. 7.\nOn the other hand, the reconstructed head avatar can-\nnot make expressions other than those in the training set.\nTherefore, when the actor\u2019s expression is too exaggerated,\nour method will output relatively less exaggerated results as\nshown in Fig. 8.\nFigure 7. Failure case: our method can not reconstruct dynamic\nlong hair.\nFigure 8. Failure case: our method produce relatively less exag-\ngerated results.\n1\n"
  },
  {
    "title": "OneLLM: One Framework to Align All Modalities with Language",
    "link": "https://arxiv.org/pdf/2312.03700.pdf",
    "upvote": "20",
    "text": "OneLLM: One Framework to Align All Modalities with Language\nJiaming Han1,2, Kaixiong Gong1,2, Yiyuan Zhang1,2, Jiaqi Wang2, Kaipeng Zhang2\nDahua Lin1,2, Yu Qiao2, Peng Gao2, Xiangyu Yue1\u2020\n1MMLab, The Chinese University of Hong Kong\n2Shanghai Artificial Intelligence Laboratory\nAbstract\nMultimodal large language models (MLLMs) have\ngained significant attention due to their strong multimodal\nunderstanding capability.\nHowever, existing works rely\nheavily on modality-specific encoders, which usually dif-\nfer in architecture and are limited to common modalities.\nIn this paper, we present OneLLM, an MLLM that aligns\neight modalities to language using a unified framework.\nWe achieve this through a unified multimodal encoder and\na progressive multimodal alignment pipeline.\nIn detail,\nwe first train an image projection module to connect a vi-\nsion encoder with LLM. Then, we build a universal pro-\njection module (UPM) by mixing multiple image projec-\ntion modules and dynamic routing. Finally, we progres-\nsively align more modalities to LLM with the UPM. To\nfully leverage the potential of OneLLM in following instruc-\ntions, we also curated a comprehensive multimodal instruc-\ntion dataset, including 2M items from image, audio, video,\npoint cloud, depth/normal map, IMU and fMRI brain activ-\nity. OneLLM is evaluated on 25 diverse benchmarks, en-\ncompassing tasks such as multimodal captioning, question\nanswering and reasoning, where it delivers excellent perfor-\nmance. Code, data, model and online demo are available at\nhttps://github.com/csuhan/OneLLM.\n1. Introduction\nLarge Language Models (LLMs) are getting increasingly\npopular in the research community and industry due to their\npowerful language understanding and reasoning capabili-\nties. Notably, LLMs such as GPT4 [62] have reached per-\nformance nearly on par with humans in various academic\nexams. The progress in LLMs has also inspired researchers\nto employ LLMs as an interface for multimodal tasks, such\nas vision-language learning [4, 44], audio and speech recog-\nnition [25, 99], video understanding [11, 45, 100], etc.\n\u2020 Corresponding author\nVision LLM\nImage \nEncoder\nProjection\nImage \nEncoder\nProjection\nAudio \nEncoder\nProjection\nVideo \nEncoder\nProjection\nMM LLM\nOneLLM\nUniversal \nEncoder\nUniversal\nProjection\n\u2026\n{modal}\nImage\nImage\nAudio\nVideo\nImage\nAudio\nVideo\nImage\nAudio\nVideo\nPoint\nIMU\nfMRI\nDepth\nNormal\nprevious works\nOneLLM\nFigure 1. Comparisons of Different Multimodal LLMs. Vi-\nsion LLM: one image encoder and projection module. Multimodal\n(MM) LLM: modality-specific encoder and projection module.\nOneLLM: a universal encoder, a universal projection module and\nmodality tokens {modal} to switch between modalities. Bottom:\nOneLLM expands supported modalities from three to eight.\nAmong these tasks, vision-language learning is the most\nactive field, with more than 50 vision LLMs proposed in the\nrecent half-year alone [20]. Typically, a vision LLM com-\nprises a visual encoder, an LLM, and a projection module\nconnecting the two components. The vision LLM is first\ntrained on massive paired image-text data [70] for vision-\nlanguage alignment and then fine-tuned on visual instruc-\ntion datasets, enabling it to complete various instructions\ntied to visual inputs.\nBeyond vision, significant efforts\nhave been invested in developing other modality-specific\nLLMs, such as audio [25], video [45], and point clouds [28].\nThese models generally mirror the architectural framework\n1\narXiv:2312.03700v1  [cs.CV]  6 Dec 2023\nand training methodology of vision LLMs, and rely on the\nsolid foundation of pretrained modality-specific encoders\nand well-curated instruction-tuning datasets for their effec-\ntiveness.\nThere are also several attempts to integrate multiple\nmodalities into one MLLM [10, 31, 59, 104]. As an ex-\ntension of vision LLM, most previous works align each\nmodality with the LLM using modality-specific encoders\nand projection modules (middle of Fig. 1). For instance,\nX-LLM [10] and ChatBridge [104] connect pretrained im-\nage, video, and audio encoders with LLMs using separate\nQ-Former [44] or Perceiver [35] models. However, these\nmodality-specific encoders usually differ in architecture and\nconsiderable effort is required to unify them into a single\nframework. Furthermore, pretrained encoders that deliver\nreliable performance are usually restricted to widely used\nmodalities such as image, audio, and video. This limitation\nposes a constraint on MLLMs\u2019 ability to expand to more\nmodalities. Thus, a crucial challenge for MLLMs is how to\nbuild a unified and scalable encoder capable of handling a\nwide range of modalities.\nWe get inspiration from recent works on transferring\npretrained transformers to downstream modalities [51, 57,\n88, 103].\nLu et al. [51] proved that a frozen language-\npretrained transformer can achieve strong performance on\ndownstream modalities such as image classification. Meta-\nTransformer [103] demonstrated that a frozen visual en-\ncoder can achieve competitive results across 12 different\ndata modalities. The insights from the works mentioned\nabove suggest that pretrained encoders for each modality\nmay not be necessary.\nInstead, a well-pretrained trans-\nformer may serve as a universal cross-modal encoder.\nIn this paper, we present OneLLM, an MLLM that\naligns eight modalities to language using one unified frame-\nwork. As shown in Fig. 1, OneLLM consists of lightweight\nmodality tokenizers, a universal encoder, a universal pro-\njection module (UPM), and an LLM. In contrast to prior\nworks, the encoder and projection module in OneLLM are\nshared across all modalities. The modality-specific tokeniz-\ners, each comprised of only one convolution layer, convert\ninput signals into a sequence of tokens. Additionally, we\nadd learnable modality tokens to enable modality switching\nand transform input tokens of diverse lengths into tokens of\na fixed length.\nTraining a model of this complexity from scratch poses\nsignificant challenges. We start from a vision LLM and\nalign other modalities to the LLM in a progressive way.\nSpecifically, (i) we build a vision LLM with pretrained\nCLIP-ViT [67] as the image encoder, accompanied by sev-\neral transformer layers as the image projection module, and\nLLaMA2 [78] as the LLM. After pretraining on massive\npaired image-text data, the projection module learns to map\nvisual representations into the embedding space of LLM.\n(ii) To align with more modalities, we need a universal en-\ncoder and projection module. As discussed before, the pre-\ntrained CLIP-ViT is possible to serve as a universal encoder.\nFor UPM, we propose to mix multiple image projection ex-\nperts as a universal X-to-language interface. To increase the\nmodel capability, we also design a dynamic router to con-\ntrol the weight of each expert for the given inputs, which\nturns UPM into soft mixtures-of-experts [66]. Finally, we\nprogressively align more modalities with the LLM based\non their data magnitude.\nWe also curate a large-scale multimodal instruction\ndataset, including captioning, question answering, and rea-\nsoning tasks across eight modalities: image, audio, video,\npoint clouds, depth/normal map, Inertial Measurement\nUnit (IMU), and functional Magnetic Resonance Imaging\n(fMRI). By finetuning on this dataset, OneLLM has strong\nmultimodal understanding,\nreasoning,\nand instruction-\nfollowing capabilities.\nWe evaluate OneLLM on multi-\nmodal captioning, question answering and reasoning bench-\nmarks where it achieves superior performance than previous\nspecialized models and MLLMs. In conclusion, we sum-\nmary our contributions as:\n\u2022 We propose a unified framework to align multimodal in-\nputs with language. Different from existing works with\nmodality-specific encoders, we show that a unified mul-\ntimodal encoder, which leverages a pretrained vision-\nlanguage model and a mixture of projection experts, can\nserve as a general and scalable component for MLLMs.\n\u2022 To the best of our knowledge, OneLLM is the first MLLM\nthat integrates eight distinct modalities within a single\nmodel. With the unified framework and progressive mul-\ntimodal alignment pipeline, OneLLM can be easily ex-\ntended to incorporate more data modalities.\n\u2022 We curate a large-scale multimodal instruction dataset.\nOneLLM finetuned on this dataset achieves superior per-\nformance on multimodal tasks, outperforming both spe-\ncialist models and existing MLLMs.\n2. Related Work\nLarge Vision-Language Models. Large Language Models\n(LLMs) have gained a lot of attention recently. Therefore,\nextending LLMs to the vision domain is an emergent and\nrapidly growing research area. Flamingo [4] is a pioneer to\ninject frozen visual features into LLM with cross-attention\nlayers, achieving superior performance on a wide range\nof vision-language tasks. BLIP2 [44] uses a Q-Former to\naggregate visual features into a few tokens aligned with\nLLM. Recently, with the popularity of instruction-following\nLLMs, vision LLMs have experienced a new explosion.\nLLaMA-Adapter [21, 102] connects pretrained CLIP [67]\nand LLaMA [78] with parameter-efficient fine-tuning meth-\nods, which can tackle close-set visual question answering\nand image captioning tasks. Subsequent works [21, 48, 95,\n2\nrouting weights\nUniversal Encoder\nVisual\nTokenizer\nAudio\nTokenizer\nPoint\nTokenizer\nIMU\nTokenizer\nfMRI\nTokenizer\nModality \nTokens\n+\nModality Router\nProjection Expert\nLarge Language Model\nText Prompt\nResponse\nselect & concatenate\nProjection Expert\nProjection Expert\nUniversal Projection Module\nfrozen\ntrainable\nconcat           drop token        \nEq. (1)\n+\nEq. (2)\nq\ud835\udc5a\ud835\udc5a\nx\ud835\udc5a\ud835\udc5a\nw\ud835\udc5a\ud835\udc5a\nFigure 2. The Architecture of OneLLM. OneLLM consists of modality tokenizers, a universal encoder, a universal projection module\n(UPM) and an LLM. The modality tokenizer is a 2D/1D convolution layer to transform the input signal into a sequence of tokens. For\nsimplicity, we omit video, depth/normal map tokenizers. The universal encoder is a frozen vision-language model (i.e. CLIP [67]) to\nextract high dimensional features. The UPM is composed of several projection experts and modality routers to align the input signal with\nlanguage. For the alignment stage, we train modality tokenizers and UPM, and keep LLM frozen. For the instruction tuning stage, we only\ntrain the LLM and keep other models frozen. In a forward pass of UPM, we concatenate the input and modality tokens as input. Then we\nonly take the modality tokens as a summary of the input signal and feed it into LLM for multimodal understanding.\n105] propose to train such model on large-scale image-text\ndata, enabling it to complete various instructions about im-\nages. Among them, LLaVA [48] adopt a linear layer to\ndirectly project visual tokens into LLMs, while MiniGPT-\n4 [105] and some other works [21, 95] resample visual\ntokens into fixed-length tokens, reducing the computation\ncost of LLMs. Our work also belongs to the later branch.\nWe preset learnable tokens for each modality (i.e., modality\ntokens), which are then used to aggregate input information\nand generate fixed-length tokens for all modalities.\nMultimodal Large Language Models. In addition to vi-\nsion LLMs, recent works proposed to extend LLMs to other\nmodalities, such as audio [25, 99], video [11, 45, 100] and\npoint cloud [28, 92]. These works make it possible to unify\nmultiple modalities into one LLM. X-LLM [10] adopts\nmodality-specific Q-Former [44] and adapters to connect\npretrained image, audio and video encoders with LLMs.\nChatBridge [104] and AnyMAL [59] follow a similar ar-\nchitecture with X-LLM but adopts Perceiver [35] and linear\nlayers respectively to align modality encoders with LLMs.\nMeanwhile, PandaGPT [77] and ImageBind-LLM [31] uti-\nlize ImageBind [23] as the modality encoder and there-\nfore naturally support multimodal inputs. However, current\nMLLMs are limited to supporting common modalities such\nas image, audio and video. It remains unclear how to ex-\npand MLLMs to more modalities with a unified framework.\nIn this work, we propose a unified multimodal encoder to\nalign all modalities with language. We show that one uni-\nversal encoder and projection module can effectively map\nmultimodal inputs to LLM. To our knowledge, OneLLM is\nfirst MLLM capable of supporting eight distinct modalities.\nMultimodal-Text Alignment. Aligning multiple modali-\nties into one joint embedding space is important for cross-\nmodal tasks, which can be divided into two lines of works:\ndiscriminative alignment and generative alignment.\nThe\nmost representative work of discriminative alignment is\nCLIP [67], which utilize contrastive learning to align im-\nage and text.\nFollow-up works extend CLIP to audio-\ntext [30, 85], video-text [53, 90], point-text [101] etc. Be-\nsides, ImageBind [23] proposes to bind various modalities\nto images with contrastive learning. On the other hand, gen-\nerative alignment has attracted much attention in the era\nof LLM. GIT [82] aligns image and text using a genera-\ntive image-to-text transformer. BLIP2 [44] proposes gener-\native pretraining to connect frozen vision encoder and LLM.\nVALOR [12] and VAST [13] extends the training paradigm\nof BLIP2 to more modalities such as audio and video. Our\nwork also belongs to generative alignment. In contrast to\nprior works, we directly align mutlimodal inputs to LLMs,\nthus getting rid of the stage of training modality encoders.\n3. Method\nIn this section, we will first introduce the architecture\nof OneLLM (Sec. 3.1) and then present our two training\nphases: progressive multimodal alignment (Sec. 3.2) and\nunified multimodal instruction tuning (Sec. 3.3).\n3\n3.1. Model Architecture\nFig. 2 depicts the four main components of OneLLM:\nmodality-specific tokenizers, a universal encoder, a univer-\nsal projection module (UPM) and an LLM. Detailed de-\nscriptions are presented in the following sections.\nLightweight Modality Tokenizers.\nThe modality tok-\nenizer is to transform the input signal into a sequence of\ntokens, thereby a transformer-based encoder can process\nthese tokens. We denote the input tokens as x \u2208 RL\u00d7D,\nwhere L is the sequence length and D is the token dimen-\nsion. Considering the variations inherent to different data\nmodalities, we design a separate tokenizer for each modal-\nity. For visual inputs with 2D position information such as\nimage and video, we directly utilize a single 2D convolution\nlayer as the tokenizer. For other modalities, we transform\nthe input into a 2D or 1D sequence, which is then tokenized\nusing a 2D/1D convolution layer. For example, we trans-\nform audio signals into 2D spectrogram and sample a subset\nof point clouds with 2D geometric prior. Due to space limit,\nplease refer to Sec. C.1 of the appendix for more details.\nUniversal Encoder. As discussed in Sec. 1, frozen pre-\ntrained transformers demonstrate strong modality transfer\ncapability [51, 103].\nTherefore, we leverage pretrained\nvision-language models as the universal encoder for all\nmodalities. Vision-language models, when trained on ex-\ntensive image-text data, typically learn robust alignment\nbetween vision and language, so they can be easily trans-\nferred to other modalities.\nIn OneLLM, we use CLIP-\nViT [67] as a universal computation engine. Following pre-\nvious works [51, 103], we keep the parameters of CLIP-\nViT frozen during training. Note that for video signals, we\nwill feed all video frames into the encoder in parallel and\nperform token-wise averaging between frames to speed up\ntraining. Other strategies, such as token concatenation, may\nfurther enhance the model\u2019s video understanding capability.\nUniversal Projection Module.\nIn contrast to existing\nworks with modality-specific projection, we propose a Uni-\nversal Projection Module (UPM) to project any modality\ninto LLM\u2019s embedding space. As shown in Fig. 2, UPM\nconsists of K projection experts {Pk}, where each expert is\na stack of transformer layers pretrained on image-text data\n(will discuss in Sec. 3.2). Although one expert can also real-\nize any modality-to-LLM projection, our empirical findings\nsuggest that multiple experts are more effective and scal-\nable. When scaling to more modalities, we only need to\nadd a few parallel experts.\nTo integrate multiple experts into one module, we pro-\npose a dynamic modality router R to control each expert\u2019s\ncontribution and increase the model capacity. The router\nR is structured as a straightforward Multi-Layer Perception\nthat receives input tokens and calculates the routing weights\nfor each expert, i.e., a soft router [66]. We will also discuss\nother types of router in Sec. 4.3, such as constant router and\nsparse router. Besides, we add learnable modality tokens\n{qm}m\u2208M to switch between modalities, where M is the\nset of modalities and qm \u2208 RN\u00d7D contains N tokens of di-\nmension D. In a forward pass for modality m, we feed the\nconcatenation of input tokens xm \u2208 RL\u00d7D and modality\ntokens qm into UPM:\n[\u00afqm, \u00afxm] = UPM([qm, xm]) =\nK\nX\nk=1\nwm \u00b7 Pk([qm, xm]), (1)\nwm = \u03c3 \u25e6 Rm([qm, xm]),\n(2)\nwhere wm \u2208 RN\u00d7K is the routing weight and the SoftMax\nfunction \u03c3 is to ensure PK\nk=1 wm,k = 1. For any modality\nm, we only extract the projected modality tokens \u00afqm as a\nsummary of input signals, transforming xm from varying\nlengths into uniform, fixed-length tokens.\nLLM. We employ the open-source LLaMA2 [79] as the\nLLM in our framework. The input to LLM includes pro-\njected modality tokens \u00afqm and the text prompt after word\nembedding. Note we always put modality tokens at the be-\nginning of the input sequence for simplicity. Then LLM\nis asked to generate appropriate response conditioned on\nmodality tokens and text prompt.\n3.2. Progressive Multimodal Alignment\nImage-text alignment has been well investigated in pre-\nvious works [21, 49, 105].\nTherefore, a naive approach\nfor multimodal alignment is to jointly train the model on\nmultimodal-text data. However, training models directly on\nmultimodal data can lead to biased representations between\nmodalities due to the imbalance of data scale. Here we pro-\npose to train an image-to-text model as initialization and\nprogressively ground other modalities into LLM.\nImage-Text Alignment.\nWe begin with a basic vision\nLLM framework, comprising an image tokenizer, a pre-\ntrained CLIP-ViT, an image projection module PI and an\nLLM. Considering that image-text data is relatively abun-\ndant compared to other modalities, we first train the model\non image-text data to well align CLIP-ViT and LLM, i.e.,\nlearning a good image-to-text projection module. The pre-\ntrained PI not only serves as a bridge connecting images\nand language, but also provides a good initialization for\nmultimodal-text alignment. Then we build UPM by mix-\ning multiple pretrained PI: UPM = {Pk} = {Init(PI)},\nwhere Init is weight initialization, which effectively re-\nduces the cost of aligning other modalities to language.\nMultimodal-Text Alignment. We formulate multimodal-\ntext alignment as a continual learning process [80].\nAt\ntimestamp t, we have trained the model on a set of modal-\nities M1 \u222a M2 \u00b7 \u00b7 \u00b7 Mt\u22121, and the current training data is\nfrom Mt. To prevent catastrophic forgetting, we will sam-\nple evenly from both previous trained data and current data.\nIn our case, we divide multimodal-text alignment into mul-\ntiple training stages based on their data magnitude: stage I\n4\n(image), stage II (video, audio and point cloud) and stage\nIII (depth/normal map, IMU and fMRI). If we want to sup-\nport new modalities, we can repeat the training episode, i.e.,\nsampling a similar amount of data from previous modalities\nand jointly training the model with the current modalities.\nMultimodal-Text Dataset. We collect X-text pairs for each\nmodality. The image-text pairs include LAION-400M [70]\nand LAION-COCO [69]. The training data for video, au-\ndio and point clouds are WebVid-2.5M [8], WavCaps [56]\nand Cap3D [54], respectively.\nSince there is no large-\nscale depth/normal map-text data, we use pretrained DPT\nmodel [19, 68] to generate depth/normal map. The source\nimages and text and from CC3M [73]. For IMU-text pairs,\nwe use the IMU sensor data of Ego4D [27]. For fMRI-text\npairs, we use fMRI signals from the NSD [5] dataset and\ntake the captions associated with the visual stimuli as text\nannotations. Note that the input to LLM is the concatena-\ntion of modality tokens and caption tokens. We do not add\nsystem prompts at this stage to reduce the number of tokens\nand speed up training.\n3.3. Unified Multimodal Instruction Tuning\nAfter multimodal-text alignment, OneLLM becomes a mul-\ntimodal captioning model which can generate a short de-\nscription for any input. To fully unleash OneLLM\u2019s multi-\nmodal understanding and reasoning capabilities, we curate\na large-scale multimodal instruction tuning dataset to fur-\nther finetune OneLLM.\nMultimodal Instruction Tuning Dataset. We collect in-\nstruction tuning (IT) dataset for each modality. Following\nprevious works [15, 48], the image IT datasets are sampled\nfrom the following datasets: LLaVA-150K [49], COCO\nCaption [14], VQAv2 [26], GQA [34], OKVQA [55], A-\nOKVQA [71], OCRVQA [58], RefCOCO [36] and Visual\nGenome [38].\nThe video IT datasets include MSRVTT-\nCap [91], MSRVTT-QA [89] and video instruction data\nfrom [104]. The audio IT datasets include AudioCaps [37]\nand audio conversation data from [104]. The point cloud\nIT dataset is a 70K point cloud description, conversation\nand reasoning dataset from [92]. The depth/normal map\nIT datasets are generated from image IT datasets: we ran-\ndom sample 50K visual instruction data from LLaVA-150K\nand generate depth/normal map using DPT model [19]. For\nIMU and fMRI IT datasets, we also random sample a sub-\nset from Ego4D [27] and NSD [5], respectively. Finally,\nour mutlimodal IT datasets have about 2M items, covering\nmultiple tasks such as detailed description/reasoning, con-\nversation, short question answering and captioning.\nPrompt Design. Given the diverse modalities and tasks\nwithin our multimodal IT datasets, we carefully design the\nprompts to avoid conflicts between them. (a) When utiliz-\ning IT datasets generated by GPT4 (e.g., LLaVA-150K), we\nadopt the original prompts provided by these datasets. (b)\nFor captioning tasks, we empoly the prompt: Provide a one-\nsentence caption for the provided {modal}. (c) For open-\nended question answering tasks, we enhance the question\nwith Answer the question using a single word or phrase.\n(d) For question answering tasks with options, the prompt\nis: {Question} {Options} Answer with the option\u2019s letter\nfrom the given choices directly.\n(e) For IMU and fMRI\ndatasets, we apply prompt such as Describe the motion and\nDescribe this scene based on fMRI data.\nDespite using\nthese fixed prompts, our experiments indicate that OneLLM\nis capable of generalizing to open-ended prompts during in-\nference. For detailed prompts on each task and modality,\nplease check out Sec. C.4 of the appendix.\nIn the instruction tuning stage, we organize the input se-\nquence as: {\u00afq, Sys, [Inst, Anst]T\nt=1} where \u00afq is the modal-\nity tokens, Sys is the system prompt, [Inst, Anst] corre-\nsponds to the t-th instruction-answer pair in a conversation.\nNote that for multimodal inputs involving multiple modali-\nties, such as audio-visual tasks [42], we position all modal-\nity tokens at the start of the input sequence.\nWe fully finetune the LLM and keep rest parameters\nfrozen.\nAlthough recent works often employ parameter-\nefficient methods [33], we empirically show that the full\nfinetuning approach more effectively harnesses the multi-\nmodal capabilities of OneLLM, particularly with the uti-\nlization of smaller LLMs (e.g., LLaMA2-7B).\n4. Experiment\n4.1. Implementation Details\nArchitecture. The universal encoder is CLIP VIT Large\npretrained on LAION [70]. The LLM is LLaMA2-7B [79].\nThe UPM has K=3 projection experts, where each expert\nhas eight Transformer blocks and 88M parameters. The size\nof modality tokens for each modality is R30\u00d71024.\nTraining Details. We use AdamW optimizer with \u03b21=0.9,\n\u03b22=0.95 and weight decay of 0.1. We apply a linear learning\nrate warmup during the first 2K iterations. For stage I, we\ntrain OneLLM on 16 A100 GPUs for 200K iterations. The\neffective batch size (using gradient accumulation) is 5120.\nThe maximum learning rate is 5e-5. For stage II (resp. III),\nwe train OneLLM on 8 GPUs for 200K (resp. 100K) with\nan effective batch size of 1080 and maximum learning rate\nof 1e-5. In the instruction tuning stage, we train OneLLM\non 8 GPUs for 1 epoch (96K) with an effective batch size of\n512 and maximum learning rate of 2e-5.\n4.2. Quantitative Evaluation\nWe evaluate OneLLM on multimodal tasks and put evalua-\ntion details to Sec. D of the appendix.\nImage-Text Evaluation. In Tab. 1, we evaluate OneLLM\non visual question answering (VQA), image captioning and\nrecent multimodal benchmarks. For VQA tasks, OneLLM-\n5\nModel\nLLM\nVQA\nImage Caption\nMM Benchmark\nGQA\nVQAv2\nOKVQA\nTVQA\nSQA\nVizwiz\nNoCaps\nFlickr\nMME\nMMB\nMMVet\nSEED\nvision specialized LLM\nFlamingo-9B [4]\nChinchilla-7B\n-\n51.8\n44.7\n30.1\n-\n28.8\n-\n61.5\n-\n-\n-\n-\nFlamingo-80B [4]\nChinchilla-70B\n-\n56.3\n50.6\n31.8\n-\n31.6\n-\n67.2\n-\n-\n-\n-\nBLIP-2 [44]\nVicuna-7B\n-\n-\n-\n40.1\n53.8\n-\n107.5\n74.9\n-\n-\n-\n-\nBLIP-2 [44]\nVicuna-13B\n41.0\n41.0\n-\n42.5\n61\n19.6\n103.9\n71.6\n1293.8\n-\n22.4\n-\nInstructBLIP [15]\nVicuna-7B\n49.2\n-\n-\n50.1\n60.5\n34.5\n123.1\n82.4\n-\n36\n26.2\n-\nInstructBLIP [15]\nVicuna-13B\n49.5\n-\n-\n50.7\n63.1\n34.3\n121.9\n82.8\n1212.8\n-\n25.6\n-\nIDEFICS-9B [39]\nLLaMA-7B\n38.4\n50.9\n38.4\n25.9\n-\n35.5\n-\n27.3\n-\n48.2\n-\n-\nIDEFICS-80B [39]\nLLaMA-65B\n45.2\n60.0\n45.2\n30.9\n-\n36.0\n-\n53.7\n-\n54.5\n-\n-\nLLaMA-Ad.v2 [21]\nLLaMA-7B\n43.9\n-\n55.9\n43.8\n54.2\n-\n42.7\n30.5\n972.7\n38.9\n31.4\n32.7\nQwen-VL [7]\nQwen-7B\n57.5\n78.2\n56.6\n61.5\n68.2\n38.9\n120.2\n81.0\n1487.5\n60.6\n-\n58.2\nLLaVA-v1.5 [48]\nVicuna-7B\n62.0\n78.5\n-\n58.2\n66.8\n50.0\n-\n-\n1510.7\n64.3\n30.5\n58.6\nmultimodal generalist LLM\nImageBind-LLM [31]\nLLaMA-7B\n41.1\n-\n-\n24.0\n51.4\n-\n29.6\n23.5\n775.7\n-\n-\n-\nChatBridge-13B [104]\nVicuna-13B\n41.8\n-\n45.2\n-\n-\n-\n115.7\n82.5\n-\n-\n-\n-\nAnyMAL-13B [59]\nLLaMA2-13B\n-\n59.6\n33.1\n24.7\n52.7\n24.4\n-\n-\n-\n-\n-\n-\nAnyMAL-70B [59]\nLLaMA2-70B\n-\n64.2\n42.6\n32.9\n70.8\n33.8\n-\n-\n-\n-\n-\n-\nOneLLM-7B (Ours)\nLLaMA2-7B\n59.5\n71.6\n58.9\n34.0\n63.4\n45.9\n115.9\n78.6\n1392.0\n60.0\n29.1\n61.2\nTable 1.\nEvaluation on 12 Image-Text Benchmarks, including 6 VQA tasks (GQA [34], VQAv2 [26], OKVQA [55], TextVQA\n(TVQA) [75], ScienceQA (SQA) [52] and Vizwiz [29]), 2 image captioning tasks (Nocaps [2] and Flickr30K [65]), and 4 multimodal\nbenchmarks (MME [20], MM Bench (MMB) [50], MMVet [98] and SEED [41]). The LLMs are Chinchilla [32], Vicuna [81], Qwen [6],\nLLaMA [78] and LLaMA2 [79]. The evaluation metrics for VQA and captioning tasks are accuracy and CIDEr, respectively. The results\nin bold and underline are the best and second-best results, respectively. -: Not reported result.\nModel\n0-shot\nNextQA\nHow2QA\nMSVD\nVATEX\nAcc.\nAcc.\nAcc.\nCIDEr\nHGQA [87]\n\u2717\n51.8\n-\n41.2\n-\nJustAsk [93]\n\u2717\n52.3\n84.4\n46.3\n-\nVALOR [12]\n\u2717\n-\n-\n60.0\n95.1\nSeViLA [97]\n\u2717\n73.8\n83.6\n-\n-\nFrozenBiLM [94]\n\u2713\n-\n58.4\n33.8\n-\nInternVideo [84]\n\u2713\n49.1\n62.2\n55.5\n-\nChatBridge-13B [104]\n\u2713\n-\n-\n45.3\n48.9\nAnyMAL-13B [59]\n\u2713\n47.9\n59.6\n-\n-\nOneLLM-7B (Ours)\n\u2713\n57.3\n65.7\n56.5\n43.8\nTable 2. Evaluation on Video-Text Tasks, including video ques-\ntion answering (NextQA [86], How2QA [46] and MSVD [89]) and\nvideo captioning tasks (VATEX [83]). Acc.: Accuracy.\nModel\n0-shot\nClotho Caption\nClotho AQA\nCIDEr\nSPIDEr\nAcc.\nFeatureCut [96]\n\u2717\n43.6\n27.9\n-\nWavcaps [56]\n\u2717\n48.8\n31.0\n-\nMWAFM [43]\n\u2717\n-\n-\n22.2\nPengi [17]\n\u2717\n-\n27.1\n64.5\nLTU-7B [25]\n\u2713\n-\n11.9\nChatBridge-13B [104]\n\u2713\n26.2\n-\n-\nOneLLM-7B (Ours)\n\u2713\n29.1\n19.5\n57.9\nTable 3. Evaluation on Audio-Text Tasks, including audio cap-\ntioning on Clotho Caption [18] and audio question answering on\nClotho AQA [47].\n7B outperforms other MMLLMs such as ChatBridge-\n13B [104] and AnyMAL-13B [59] by a large margin. Our\n7B model is even better than AnyMAL with 70B param-\neters. For image captioning tasks, OneLLM-7B is on-par\nwith ChatBridge-13B. Although OneLLM is not specifi-\ncally designed for vision tasks, our results demonstrate that\nModel\n0-shot\nMUSIC-AVQA\nVALOR\nAVSD\nAcc.\nCIDEr\nCIDEr\nMAVQA [42]\n\u2717\n71.5\n-\n-\nVALOR [12]\n\u2717\n78.9\n61.5\n-\nVAST [13]\n\u2717\n80.7\n62.2\n-\nFA+HRED [61]\n\u2717\n-\n-\n84.3\nMTN [40]\n\u2717\n-\n-\n98.5\nCOST [64]\n\u2717\n-\n-\n108.5\nChatBridge-13B [104]\n\u2713\n43.0\n24.7\n75.4\nOneLLM-7B (Ours)\n\u2713\n47.6\n29.2\n74.5\nTable 4.\nEvaluation on Audio-Video-Text Tasks, including\naudio-visual question answering on MUSIC-AVQA [42] and\naudio-visual captioning on VALOR-32K [12] and dialog comple-\ntion on AVSD [3].\nModel\nCaptioning\nClassification\nBLEU-1\nROUGE-L\nMETEOR\nGPT4-Acc.\nInstructBLIP-7B [15]\n11.2\n13.9\n14.9\n38.5\nInstructBLIP-13B [15]\n12.6\n15.0\n16.0\n35.5\nPointLLM-7B [92]\n8.0\n11.1\n15.2\n47.5\nPointLLM-13B [92]\n9.7\n12.8\n15.3\n45.0\nOne-LLM-7B (Ours)\n42.2\n45.3\n20.3\n44.5\nTable 5. Evaluation on Point Cloud-Text Tasks. The evalua-\ntion dataset is from Objaverse [16], following the data split in\nPointLLM [92]. InstructBLIP takes single-view image as input,\nwhile PointLLM and OneLLM take point cloud as input. GPT4-\nAcc.: GPT4 as the accuracy evaluator [92].\nOneLLM can also reach the leading level in vision spe-\ncialized LLMs, and the gap between MMLLMs and vision\nLLMs has further narrowed.\nVideo-Text Evaluation.\nAs shown in Tab. 2, we eval-\nuate OneLLM on video QA and captioning tasks.\nOur\n6\nModel\n0-shot\nNYUv2\nSUN RGB-D\nAcc.\nAcc.\nImageBind [23]\n\u2717\n54.0\n35.1\nOmnivore [22]\n\u2717\n76.7\n64.9\nRandom\n\u2713\n10.0\n5.26\nCLIP ViT-H\u2217 [67]\n\u2713\n41.9\n25.4\nOneLLM-N (Ours)\n\u2713\n46.5\n21.2\nOneLLM-D (Ours)\n\u2713\n50.9\n29.0\nTable 6. Evaluation on Scene Classification Tasks Using Depth\n/ Normal Map. OneLLM-N/D: OneLLM with Depth / Normal\nmap inputs. Note that NYUv2 [60] and SUN RGB-D [76] only\nhave depth maps, we adopt pretrained DPT model [19] to generate\nnormal maps.\n\u2217: The input to CLIP is depth rendered grayscale\nimage. ImageBind is trained on image-depth pairs of SUN RGB-\nD and therefore is not zero-shot.\nmodel outperforms both MLLMs (ChatBridge and Any-\nMAL) and video-specific models (FrozenBiLM [94] and\nInternVideo [84]) in video QA tasks. Notably, our training\ndatasets do not include video QA data like NextQA [86]\nand How2QA [46], which are video QA tasks that provide\nanswer options. However, our model\u2019s training on simi-\nlar VQA datasets (e.g., A-OKVQA [71]) has evidently en-\nhanced its emergent cross-modal capabilities, contributing\nto the improved performance in video QA tasks.\nAudio-Text Evaluation.\nWe evaluate OnLLM on audio\ncaptioning and QA tasks. In Tab. 3, we outperforms both\nChatBridge and LTU [25] on Clotho Caption [18].\nNo-\ntably, our zero-shot result on Clotho AQA [47] is on par\nwith fully finetuned Pengi [17]. Similar to our conclusion\non video QA, we believe that the captioning task requires\nmore dataset-specific training, while the QA task may be\na more accurate measure of the model\u2019s inherent zero-shot\nunderstanding capabilities.\nAudio-Video-Text Evaluation. We evaluate OneLLM on\naudio-video-text tasks, such as QA (MUSIC AVQA [42]),\ncaptioning (VALOR-32K [12]) and dialog completion\n(AVSD [3]) based on the video and background audio. As\nshown in Tab. 4, OneLLM-7B surpasses ChatBridge-13B\non all three datasets. Note that ChatBridge was trained on\nan audio-visual dataset [12], while OneLLM has not been\ntrained on any audio-visual datasets. Since all modalities in\nOneLLM are well aligned with language, we can directly\ninput video and audio signals to OneLLM during inference.\nPoint Cloud-Text Evaluation.\nIn Tab. 5, We evaluate\nOneLLM on point cloud captioning and classification tasks.\nOneLLM can achieve excellent captioning results due to\nour carefully designed instruction prompts for switching\nbetween tasks (Sec. 3.3), while InstructBLIP [15] and\nPointLLM [92] struggle to generate short and accurate cap-\ntions. On the classification task, OneLLM can also achieve\ncomparable results to PointLLM.\nDepth/Normal Map-Text Evaluation.\nSince there are\nTask\nNoCaps\nVQAv2\nClothoQA\nMSVDQA\n(a) Training Mode\nSeparate\n115.6(-0.2)\n71.9(+0.3)\n37.8(-19.6)\n31.0(-25.8)\nJoint\n115.8\n71.6\n57.4\n56.8\n(b) Weight Initialization\nRandom Init.\n98.8(-17.0)\n65.6(-6.0)\n57.6(+0.2)\n53.1(-3.7)\nImage Init.\n115.8\n71.6\n57.4\n56.8\n(c) Number of Experts (Parameters)\n1 (88M)\n108.7(-7.1)\n66.9(-4.7)\n58.2(+0.8)\n53.3(-3.5)\n3 (264M)\n115.8\n71.6\n57.4\n56.8\n5 (440M)\n114.6\n71.7\n58.2\n56.7\n7 (616M)\n114.9\n71.6\n58.8\n56.0\n(d) Router Type\nConstant Router\n109.8(-6.0)\n67.7(-3.9)\n56.2(-1.2)\n55.3(-1.5)\nSparse Router\n112.8(-3.0)\n71.1(-0.5)\n56.7(-0.7)\n55.7(-1.1)\nSoft Router\n115.8\n71.6\n57.4\n56.8\nTable 7. Ablation Experiments. We choose three modalities (im-\nage, audio, video) and four datasets (NoCaps [2], VQAv2 [26],\nClothoQA [47] and MSVDQA [89]) for evaluation. The row with\ngray background is our default setting.\ncurrently no QA and captioning tasks using depth/normal\nmaps, we evaluate OneLLM on two scene classification\ndatasets [60, 76]. The performance, as displayed in Tab. 6,\nreveals that OneLLM achieves superior zero-shot classifi-\ncation accuracy compared to CLIP. These results affirm that\nOneLLM trained on synthetic depth/normal map data can\nadapt to real world scenarios.\nIMU-Text and fMRI-Text Evaluation. Since IMU/fMRI\nto text generation are seldom explored in previous literature,\nwe solely report our results on IMU/fMRI captioning. For\nIMU captioning on Ego4D [27], we evaluate OneLLM on a\nheld-out subset with 2000 items. The CIDEr and ROUGE-L\nscore are 24.9 and 19.5, respectively. For fMRI captioning\non NSD [5], we evaluate OneLLM on its testing set, where\nOneLLM achieves 31.7 CIDEr and 25.1 ROUGE-L.\n4.3. Ablation Experiments\nIn this section, we will explore some key designs of\nOneLLM. Our ablation experiments are conducted on a sub-\nset of the training data, which only includes multimodal\nalignment and instruction tuning datasets of image, audio\nand video, except for studies on the number of experts.\nOther settings remain unchanged if not specified.\nSeparate Training vs. Joint Training. An important ques-\ntion for MLLMs is whether a jointly trained MLLM is better\nthan modality-specific MLLM? To address this, we com-\npare the performance of separately trained MLLMs against\na jointly trained MLLM in Tab. 7 (a). In separate training,\nthe model can only access its own data; in joint training,\nthe model is jointly trained on all data. On two image-text\ntasks NoCaps and VQAv2, we can see that separately and\njointly trained models achieve comparable results; While\n7\nDescribe this poster in detail.\nThe poster features a man wearing a suit and \ntie, standing in front of a bright light. The \nman is the main focus of the poster, with the \nlight illuminating his face. The background is \ndark, creating a stark contrast with the man's \nattire. The poster is advertising the movie \n\"Oppenheimer,\" which is a biographical \ndrama about the life of J. Robert \nOppenheimer, the father of the atomic bomb.\nWhat message does this video convey?\nThe video seems to convey a message of joy \nand happiness, as the pink animals are \ndancing and singing in a fun and playful \nmanner. The video also features a green field \nand trees, which may suggest a connection to \nnature and the importance of enjoying life. \nThe video may be trying to promote a \npositive and joyful attitude towards life.\nWhat scene does this fMRI data record?\nThe fMRI data records a scene of a city street \nwith a bus driving down the road.\n(reference visual stimulus)\nWhat is the weather?\nThe weather appears to be sunny, as the bus is \ndriving down the street in the sun.\nHow do you feel when you hear the audio?\nWhen I hear this audio, I feel like I am \nwalking along the beach, with the sound of \nthe waves crashing and the wind blowing.\nCan you write a poem based on the audio?\nSure, here's a poem that I came up :\nThe waves crash against the shore, As the tide \nmoves in and out. The water is calm and still, \nAs the wind blows gently, but chill. The sand \nis soft and warm, As I walk along the beach, \nWith the waves crashing and the wind \nblowing, I feel at peace and free.\n(sea bench, calm waves)\nWhat does this represent?\nThis is a 3D model of a cartoon-style black \ncat. The cat is designed in a simplistic, \nstylized manner, with exaggerated features \ntypical of cartoon characters. It is black in \ncolor, with a glossy finish that gives it a shiny \nappearance. The model is likely used in \nanimations, video games, or other digital \nmedia that require a fun, playful \nrepresentation of a cat.\nWhat's in the room?\nThe room contains a large teddy bear sitting \non a yoga mat, a chair, a table, and a lamp. \nThere are also several books scattered around \nthe room.\nIs there a person in the room?\nNo, there is no person in the room. The room \nis empty except for the teddy bear and the \nother items.\nWhat is the people farthest from us doing?\nThe people farthest from us is standing in the \nsnow, possibly observing the scene or waiting \nfor his turn to ski.\nWhat are the two people in front probably \ndoing?\nThe two people in front are likely skiing, as \nthey are holding onto a rope and skiing down \na snowy slope.\nGuess what happened in the motion.\nBased on the motion, it seems like a person \nwas painting a wooden piece using a brush. \nThe motion shows the person's hand holding \nthe brush and applying paint to the wooden \nsurface. The motion also shows a close-up of \nthe wooden piece, which is being painted. \nThe motion does not provide any other \ncontext or information about the video, so it \nis difficult to say what specifically happened.\n(GT: paints the wood)    (reference video)\n(a) Image\n(b) Video\n(c) Audio\n(d) Point Cloud\n(f) IMU\n(e) fMRI\n(g) Normal Map\n(h) Depth Map\nFigure 3. Qualitative Results on Eight Modalities. All demo inputs are from the web or the testing set of corresponding modalities.\nseparately trained audio and video models are much worse\nthan the jointly trained model on ClothoQA and MSVDQA,\nrespectively. This suggest that joint training substantially\nbenefits data-scarce modalities (e.g., audio and video), by\nallowing for the transfer of learned knowledge (e.g., ques-\ntion answering) across modalities.\nImage\nAlignment\nBenefits\nMultimodal\nAlignment.\nTab. 7 (b) demonstrate that OneLLM with image-text\nalignment can help multimodal-text alignment. If we di-\nrectly align all modalities with text using a random initial-\nized model (i.e. universal projection module), the perfor-\nmance on image and video will drop significantly. Instead,\nOneLLM with image-text pretraining can better balance dif-\nferent modalities.\nNumber of Projection Experts. The number of projection\nexperts in UPM is closely related to the number of modal-\nities that OneLLM can accommodate. As shown in Tab. 7,\nOneLLM with three projection experts is enough to hold all\nmodalities. Increasing the number of experts does not bring\nabout the desired improvement, while the results with one\nexpert is also not satisfactory.\nRouter Type. The modality router is to link multiple pro-\njection experts into a single module. Here we discuss three\ntypes of router: constant router, sparse router and the de-\nfault soft router. (a) Constant router links K experts with\na constant number 1/K.\nThe output of constant router\nis PK\nk=1\n1\nK \u00b7 Pk(x).\n(b) Sparse router only selects one\nexpert with the maximum routing weight. The output is\nwk\u2217Pk\u2217(x) where k\u2217 = arg max\nk\nwk. As shown in Tab. 7\n(d), soft router outperforms other two routers, indicating its\neffectiveness for dynamic routing of multimodal signals.\n4.4. Qualitative Analysis\nFig. 3 gives some qualitative results of OneLLM on eight\nmodalities. We show OneLLM can (a) understand both vi-\nsual and textual content in images, (b) leverage temporal in-\nformation in videos, (c) do creative writing based on audio\ncontent, (d) understand the details of 3D shapes, (e) analyze\nvisual scenes recorded in fMRI data, (f) guess the person\u2019s\naction based on motion data, and (g)-(h) scene understand-\ning using depth/normal map. Due to space limit, we put\nmore qualitative results to Sec. F of the appendix.\n5. Conclusion\nIn this work, we introduce OneLLM, an MLLM that aligns\neight modalities with language using a unified framework.\nInitially, we first train a basic vision LLM. Building on\nthis, we design a multimodal framework with a universal\nencoder, a UPM and an LLM. By a progressive alignment\npipeline, OneLLM can handle multimodal inputs with a sin-\ngle model. Furthermore, we curate a large-scale multimodal\n8\ninstruction dataset to fully unleash OneLLM\u2019s instruction-\nfollowing capability. Finally, we evaluate OneLLM on 25\ndiverse benchmarks, showing its excellent performance.\nLimitation and Future Work. Our work faces two primary\nchallenges: (i) The absence of large-scale, high-quality\ndatasets for modalities beyond image, which leads to a cer-\ntain gap between OneLLM and specialized models on these\nmodalities. (ii) Fine-grained multimodal understanding in\nhigh-resolution images, long sequences video and audio etc.\nIn the future, we will collect high-quality datasets and de-\nsign new encoders to realize fine-grained multimodal un-\nderstanding, e.g., supporting varying length inputs [9].\nA. Appendix Overview\n\u2022 Sec. B: Additional Ablation Experiments.\n\u2022 Sec. C: Additional Implementation Details.\n\u2022 Sec. D: Evaluation Details.\n\u2022 Sec. E: Comparison with Prior Works.\n\u2022 Sec. F: Additional Qualitative Results.\nB. Additional Ablation Experiments\nEncoder\nMem.\nNocaps\nVQAv2\nClothoQA\nMSVDQA\nType\nFrozen\nCLIP\n\u2713\n46Gb\n115.8\n71.6\n57.4\n56.8\nCLIP\n\u2717\n74Gb\n106.0(-9.8)\n69.1(-2.5)\n62.1(+4.7)\n53.6(-3.2)\nDINOv2\n\u2713\n33Gb\n104.6(-11.2)\n67.0(-4.6)\n56.8(-0.6)\n54.7(-2.1)\nTable 8. Ablation Experiments on Universal Encoder.\nIn the main paper, we follow previous works [103] and\nset a frozen CLIP-ViT as the universal encoder. Here we ex-\nplore other design choices such as trainable CLIP-ViT and\nDINOv2 [63] as the encoder.\nFrozen vs. Trainable Encoder.\nWe first turn on all the\nparameters in the multimodal-text alignment stage.\nAs\nshown in Tab. 8, the performance for visual modalities (im-\nage and video) dropped significantly, while the result for au-\ndio QA (ClothoQA) improved by 4.7%. We think trainable\nCLIP will break the pretrained vision-language representa-\ntions but can leave more space for learning other modalities.\nHowever, considering the memory usage (46Gb vs. 74Gb),\nfrozen CLIP will be a better choice for our framework.\nBeyond Vision-Language Encoder.\nIn addition to the\nvision-language encoder CLIP-ViT, we also explore other\nmodels, such as the self-supervised vision model DI-\nNOv2 [63], as the universal encoder. In Tab. 8, we no-\nticed that the performance of OneLLM using DINOv2 is\nlower than the model using CLIP-ViT because DINOv2 is\nnot aligned with language and we need to learn the vision-\nlanguage alignment from scratch.\nC. Additional Implementation Details\nC.1. Lightweight Modality Tokenizers\nThe modality tokenizer is to transform input signal into a\nsequence of tokens. Here we will introduce the tokenizer of\neach modality in detail.\nVisual Tokenizer.\nWe use the same tokenizer setting for\nvisual modalities, i.e., image, video, depth/normal map.\nThe visual tokenizer is a single 2D convolution layer:\nConv2D(Cin = 3, Cout = 1024, K = (14, 14), S = (14, 14)), (3)\nwhere Cin, Cout, K and S denote the input channel, out-\nput channel, kernel size and stride, respectively. Note that\nfor a video input x \u2208 RT \u00d7H\u00d7W with T frames, height\nH and width W, we parallel feed its frames into the to-\nkenizer, resulting in T \u00d7 H\n14 \u00d7 W\n14 tokens. Similarly, im-\nage, depth/normal map can also be regarded as a one-frame\nvideo input x \u2208 R1\u00d7H\u00d7W .\nAudio Tokenizer.\nWe first transform audio signals into\n2D spectrogram features x \u2208 R1\u00d7H\u00d7W , where H=128 and\nW=1024 by default. Following [24], the audio tokenzier is\na single 2D convolution layer:\nConv2D(Cin = 1, Cout = 1024, K = (16, 16), S = (10, 10)). (4)\nPoint Tokenizer.\nFor a raw point cloud, we sample 8192\npoints using Furthest Point Sampling (FPS), resulting in a\n2D tensor x \u2208 R8192\u00d76. Then we use the KNN algorithm to\ngroup these points into 512 groups: x \u2208 R512\u00d732\u00d76 where\n32 is the size of each group. After that, we encode the point\ncloud with a 2D convolution layer:\nConv2D(Cin = 6, Cout = 1024, K = (1, 1), S = (1, 1)),\n(5)\nfollowed by a max operation on dimension 1. Finally, the\nshape of output tokens is R1024\u00d71024.\nIMU Tokenizer.\nFor an IMU input with shape R2000\u00d76,\nwe tokenize it with a 1D convolution layer:\nConv1D(Cin = 6, Cout = 1024, K = 10, S = 1),\n(6)\nresulting in a sequence of tokens x \u2208 R1024\u00d7391.\nfMRI Tokenizer.\nThe shape of an fMRI signal is R15724.\nWe tokenize it with a 1D convolution layer:\nConv1D(Cin = 15724, Cout = 8196, K = 1, S = 1).\n(7)\nWe then resize the output tensor x \u2208 R8196 into a 2D ten-\nsor x \u2208 R1024\u00d78 to align with the input of the transformer\nencoder.\n9\nC.2. Multimodal-Text Alignment Dataset\nWe summary the multimodal-text alignment dataset in\nTab. 9.\nFor depth/normal-text pairs, we adopt DPT\nmodel [68] pretrained on ominidata [19] to generate\ndepth/normal map.\nThe source dataset is a subset of\nCC3M [73], around 0.5M image-text pairs. For IMU-text\npairs, we use the IMU sensor data of Ego4D [27] and the\ncorresponding video narrations (i.e., text annotations). For\nfMRI-text pairs, we use the subj01 imaging session of\nNSD [5] and follow the same data split with [72]. Note that\nthe visual stimulus, i.e., images shown to participants, are\nfrom MS COCO [14]. Therefore, we use the image cap-\ntions in COCO Captions as text annotations of fMRI-text\npairs.\nModality\nMultimodal-Text Alignment\nMultimodal Instruction Tuning\nSize\nDataset\nSize\nDataset\nImage\n1000M\nLAION-400M [70]\nLAION-COCO [69]\n1216K\nLLaVA-150K [49]\nCOCO Caption [14]\nVQAv2 [26], GQA [34]\nOKVQA [55], A-OKVQA [71]\nOCRVQA [58], RefCOCO [36]\nVisual Genome [38]\nVideo\n2.5M\nWebVid-2.5M [8]\n461K\nMSRVTT-Cap [91]\nMSRVTT-QA [89]\nVideo Conversation [104]\nAudio\n0.4M\nWavCaps [56]\n60K\nAudioCaps [37]\nAudio Conversation [104]\nPoint\n0.6M\nCap3D [54]\n70K\nPoint Conversation [92]\nDepth\n0.5M\nCC3M [73]\n50K\nLLaVA-150K [49]\nNormal\n0.5M\nCC3M [73]\n50K\nLLaVA-150K [49]\nIMU\n0.5M\nEgo4D [27]\n50K\nEgo4D [27]\nfMRI\n9K\nNSD [5]\n9K\nNSD [5]\nText\n-\n-\n40K\nShareGPT [1]\nTotal\n1005M\n2006K\n-\nTable 9. Training Datasets.\nC.3. Multimodal Instruction Tuning Dataset\nWe summary the multimodal instruction tuning dataset in\nTab. 9.\nC.4. Prompt Design\nThe prompt formats for each dataset are shown in Tab. 10.\nD. Evaluation Details\nIn this section, we first list the evaluation prompts for each\ndataset in Tab. 11. Then we will give more evaluation de-\ntails.\nImage, Video and Audio Tasks.\nWe evaluate all datasets\nusing their official evaluation protocols.\nAs shown in\nTab. 11, for QA tasks with options, we ask OneLLM to di-\nrectly predict the option letters; For open-ended QA tasks,\nDataset\nPrompt Format\nLLaVA-150K [49]\nShareGPT [1]\nVideo Conversation [104]\nAudio Conversation [104]\nPoint Conversation [92]\n(use their original prompt)\nVQAv2 [26], GQA [34]\nOKVQA [71]\nOCRVQA [58]\nMSRVTT-QA [89]\n{Question} Answer the question using a single\nword or phase.\nA-OKVQA [71]\n{Question} {Options} Answer with the option\u2019s\nletter from the given choices directly\nTextCaps [74]\nCOCO Caption [14]\nMSRVTT-Cap [91]\nAudioCaps [37]\nProvide a one-sentence caption for the provided\nimage/video/audio.\nRefCOCO [36]\nVisual Genome [38]\nProvide a short description for this region.\nEgo4D [27]\nDescribe the motion.\nNSD [5]\nDescribe the scene based on fMRI data.\nTable 10. Prompt Formats for Training.\nDataset\nPrompt Format\nMMVet\n(use the original prompt)\nGQA [34]\nVQAv2 [26]\nOKVQA [55]\nTextVQA [75]\nMME [20]\nMSVD [89]\nClotho AQA [47]\nMUSIC-AVQA [42]\n{Question} Answer the question using a single word or\nphase.\nScienceQA [52]\nMMbench [50]\nSEED-Bench [41]\nNextQA [86]\nHow2QA [46]\n{Question} {Options} Answer with the option\u2019s letter from\nthe given choices directly\nVizWiz [29]\n{Question} When the provided information is insufficient,\nrespond with \u2019Unanswerable\u2019. Answer the question using\na single word or phase.\nNocaps [2]\nFlickr30K [65]\nVATEX [83]\nVALOR [12]\nClotho Cap [18]\nObjaverse-Cap [16]\nProvide a one-sentence caption for the provided\nimage/video/audio/point cloud.\nAVSD [3]\n{Question} Answer the question and explain the reason\nin one sentence.\nObjaverse-CLS [16]\nWhat is this?\nNYUv2 [60]\nSUN RGB-D [76]\n{Class List} What is the category of this scene?\nChoice one class from the class sets.\nTable 11. Prompt Formats for Evaluation.\nwe ask OneLLM to predict a single word or phase. For cap-\ntioning tasks, we ask OneLLM to generate a one-sentence\ncaption. Note that for audio-video-text tasks, the input se-\nquence to the LLM is: {Video Tokens} {Audio Tokens}\n{Text Prompts}.\n10\nModel\nEncoder\nParam\n#Encoder\n#Projection\nSupported Modalities\nImage\nVideo\nAudio\nPoint\nIMU\nDepth\nNormal\nfMRI\nX-LLM [10]\n-\n3\n3\n\u2713\n\u2713\n\u2713\nPandaGPT [77]\n1.2B\n2\n1\n\u2713\n\u2713\n\u2713\nImageBind-LLM [31]\n1.8B\n3\n1\n\u2713\n\u2713\n\u2713\n\u2713\nChatBridge [104]\n1.3B\n3\n3\n\u2713\n\u2713\n\u2713\nAnyMAL [59]\n2B\n3\n3\n\u2713\n\u2713\n\u2713\n\u2713\nOneLLM (Ours)\n0.6B\n1\n1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 12. Comparisons of Different Multimodal LLMs.\nPoint Cloud Tasks.\nOur evaluation on point cloud tasks\nmainly follows PointLLM [92]. For the point cloud classi-\nfication task, we use the same prompt as PointLLM: What\nis this, and evaluate the accuracy using GPT4.\nDepth/Normal Map Tasks.\nFor scene classification us-\ning depth/normal map, we first prepend the category list to\nthe beginning of prompt, then we ask OneLLM to choose\none class for the list.\nIMU/fMRI Tasks.\nWe evaluate on IMU/fMRI captioning\ntasks. The prompts are the same as their training prompts:\nDescribe the motion for IMU captioning and Describe the\nscene based on fMRI data for fMRI captioning.\nE. Comparison with Prior Works\nThe main difference between OneLLM and previous\nMLLMs is that we show a unified encoder is sufficient to\nalign multi-modalities with LLMs. As shown in Tab. 12,\nOneLLM with one universal encoder, one projection mod-\nule and less parameters (0.6B) can unify more modalities\ninto one framework. The results in the main paper (Tab.1-\n6) also demonstrate that OneLLM can achieves better per-\nformance to previous works. The ablation experiments in\nTab.7 (a) also show that jointly training all modalities with\nour unified framework can benefit data-scarce modalities.\nHere we are not trying to prove that OneLLM\u2019s architecture\nis optimal, but to show the possibility of building MLLMs\nusing a unified and scalable framework.\nF. Additional Qualitative Results\nIn this section, we provide more qualitative results in Fig. 4,\nFig. 5 and Fig. 6.\nReferences\n[1] Sharegpt. https://sharegpt.com/, 2023. 10\n[2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\nStefan Lee, and Peter Anderson. nocaps: novel object cap-\ntioning at scale. In ICCV, 2019. 6, 7, 10\n[3] Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang,\nAnoop Cherian, Irfan Essa, Dhruv Batra, Tim K. Marks,\nChiori Hori, Peter Anderson, Stefan Lee, and Devi Parikh.\nAudio-visual scene-aware dialog. In CVPR, 2019. 6, 7, 10\n[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learning.\nNeurIPS, 35:23716\u201323736, 2022. 1, 2, 6\n[5] Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L\nBreedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau,\nBrad Caron, Franco Pestilli, Ian Charest, et al. A massive 7t\nfmri dataset to bridge cognitive neuroscience and artificial\nintelligence. Nature neuroscience, 25(1):116\u2013126, 2022. 5,\n7, 10\n[6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al.\nQwen technical report.\narXiv preprint\narXiv:2309.16609, 2023. 6\n[7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. arXiv preprint arXiv:2308.12966,\n2023. 6\n[8] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, pages 1728\u20131738, 2021. 5,\n10\n[9] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa\u02d8gnak Tas\u00b8\u0131rlar.\nIntroducing our multimodal models, 2023. 9\n[10] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang\nZhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Boot-\nstrapping advanced large language models by treating\nmulti-modalities as foreign languages.\narXiv preprint\narXiv:2305.04160, 2023. 2, 3, 11\n[11] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei\nHuang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong\nLu, et al. Videollm: Modeling video sequence with large\nlanguage models. arXiv preprint arXiv:2305.13292, 2023.\n1, 3\n[12] Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu,\nWeining Wang, Jinhui Tang, and Jing Liu. Valor: Vision-\naudio-language omni-perception pretraining model and\n11\nWhy this image is funny?\nThis image is funny because it features three \nraccoons dressed in human clothes and driving a \ncar. The raccoons are wearing sunglasses and \none of them is playing a guitar, which adds a \nhumorous and unexpected element to the scene. \nThe fact that raccoons are not capable of driving \nor playing instruments makes the situation \ncomical and amusing to viewers.\nWhy this image is unusual?\nThe image is unusual because it features a dog \ndressed in a chef's outfit, complete with a hat, \napron, and a bow tie. This is not a typical sight, \nas dogs are not typically dressed in human \nclothing or accessories. The dog is sitting at a \ntable with a bowl of food, which adds to the \nunconventional nature of the scene. The image \nis likely meant to be humorous or creative, as it \nis not a common occurrence to see a dog dressed \nin human attire.\nWhat do you think of the photo?\nThe photo is a captivating and dynamic scene of \na bustling city street in New York City. The \nimage features a large number of billboards and \nadvertisements on the buildings surrounding the \nstreet, creating a vibrant atmosphere. The street \nis filled with people walking and cars driving, \ngiving the impression of a busy urban \nenvironment. The photo is taken from a low \nangle, which adds a sense of depth and \nperspective to the scene. The traffic lights are \nvisible, indicating that the street is well-\nregulated for pedestrian and vehicular safety. \nThe overall composition of the photo is visually \nstriking, showcasing the energy and excitement \nof the city.\nTell me about this place.\nThe image features a large, grassy field with a \ngroup of stone structures, possibly a stonehenge, \nsituated in the center. The structures are made of \nrocks and are arranged in a circle, with some \nrocks appearing to be taller than others. The sky \nabove the field is filled with clouds, creating a \nbeautiful and serene atmosphere. The scene is \nreminiscent of ancient civilizations and their \narchitectural achievements.\nWhat does this picture want to express?\nThe image is a symbolic representation of the \nbond between a parent and their child. The hand \nof an adult is holding a small plant, which is a \nsymbol of growth and life, while the hand of a \nbaby is reaching out to the plant. This image \nsuggests the nurturing and care that parents \nprovide to their children, as well as the \ninnocence and curiosity of a child. The scene is \nset in a grassy field, which adds to the natural \nand serene atmosphere of the image.\nIntroduce me this poster.\nThe poster features a delicious Asian food \nadvertisement, with a large bowl of rice and \nshrimp displayed prominently. The \nadvertisement is designed to entice customers \nwith its mouth-watering presentation. The poster \nis displayed in a restaurant, with a sign that \nreads \"Open Hours\" and \"50% Off\" in the \nbackground. This suggests that the restaurant is \noffering a special deal on Asian food during \ncertain hours.\nWhat iconic attractions appear in this picture?\nThe picture features several iconic attractions, \nincluding the Statue of Liberty, the Eiffel Tower, \nthe Roman Coliseum, the pyramids of Egypt, the \nSpace Needle, and the London Bridge.\nWhere might this picture have been taken?\nThis picture might have been taken in a large \ncity with a lot of tall buildings. The presence of \na red double-decker bus and a truck in the image \nsuggests that the city has a public transportation \nsystem and possibly a busy traffic environment. \nThe tall buildings and the presence of a bus and \ntruck indicate that the city is likely a bustling \nurban area with a high population density.\nChoose one from \"Sydney\", \"New York\" and \n\"Hong Kong\", and tell me the reason.\nThe presence of a red double-decker bus and a \ntruck in the image suggests that the city could be \neither Hong Kong or New York. \nIf you could only use three words to express the \ntheme of this picture, what would they be?\nFoggy, tree, person\nProvide a short caption for this image\nA person running in a field with a tree in the \nbackground.\nFigure 4. Additional Qualitative Image Demos.\n12\nDescribe the video in detail\nA man is playing basketball in a gym. He is \nwearing a black shirt and white pants. He is \nbouncing a basketball on his finger and then \ndribbling it with his hand. He is also wearing a \nblack and white striped shirt. The gym is white \nand has a white floor. There is a white wall \nbehind him.\nWhat activity is going on in the video?\nThe video shows a rocket launching from a \nlaunch pad.\nWhere might this launch pad be located?\nIt is not clear from the video where the launch \npad is located. It could be in a spaceport or a \nmilitary base.\nWhat process does this video record?\nThis video records the process of a flower \nblooming.\nCan you provide more details?\nThe flower is white and has a long stem. The \nflower is in a greenhouse, and the camera is \nrecording the flower's growth process. The \nflower is in bloom, and the camera captures the \nflower's growth and development. \nWhat can you hear in the audio?\nI can hear the sound of a bell ringing at a \nsteady pace.\nCan you provide more details?\nThe bell is large and metallic, and it is ringing \nin a melody that is loud and clear.\n(bell ringing)\nWhere could this audio take place?\nThe audio could take place in a place with \nbirds, like a forest or a park.\nWhat kind of bird call could this be?\nIt could be a bird call like a whistle or a chirp, \nbut without more context, it's hard to say for \nsure.\n(bird singing)\nDescribe the audio in detail\nIn the audio, a person is seen shuffling \nthrough a variety of items, and the items make \na crunching sound. The person then takes a \nmoment to speak. The items in the \nbackground are not clear, but they appear to \nbe the source of the crunching sound.\n(crunching sound)\nSummary the content of the audio briefly.\nA woman speaking followed by a baby laughing\nDescribe the audio in detail\nIn the audio, a woman is speaking to a laughing \nbaby. The woman's voice is calm and soothing, \nwhile the baby's laughter is contagious. It \nsounds like they are having a conversation, with \nthe woman saying something that makes the \nbaby laugh even more. The audio also contains \nsome background noise, but it is not clear what \nit is.\n(speaking and laughing)\nWhat is this?\nThis is a 3D model of a cartoon-style humanoid \nfigure, predominantly white in color. The figure \nis designed in a simplistic, stylized manner, \ntypical of cartoon characters. It has a neutral \nexpression, suggesting it could be used in a \nvariety of contexts, such as animation, game \ndevelopment, or graphic design. The figure's \nwhite color allows for easy customization, such \nas adding clothing or other accessories.\nHow would you summarize this 3D data set?\nThis 3D data set represents a toy car, \npredominantly blue in color, with a distinctive \nwhite stripe running along its length. The car \nfeatures a black windscreen and matching black \nwindows, adding to its realistic appearance. The \ntoy car, with its vibrant colors and detailed \ndesign, is likely to be a popular choice among \nchildren for play and could also serve as a \ndecorative item.\nWhat kind of object is illustrated by this \ncollection of points?\nThis is a 3D model of a cartoon-style dog, \npredominantly in a vibrant yellow color. The \nmodel is characterized by its exaggerated \nfeatures, typical of cartoon aesthetics, such as \nlarge, expressive eyes and a broad, friendly \nsmile. The dog's design suggests it could be \nused in animations, video games, or children's \neducational content.\nProvide an outline of this 3D shape\u2019s \ncharacteristics.\nThe 3D object model is a tree with a distinctive \ngreen color. It features a sturdy trunk and a \ndense canopy of leaves, giving it a realistic \nappearance. The tree's design suggests it could \nbe used in various digital environments, such as \nvideo games, animations, or virtual reality \nsimulations.\nDescribe the 3D model in detail\nThe 3D model is a cartoon-style house with a \nvibrant purple roof. The house is simplistic in \ndesign, with a single story and a single room. \nThe roof is a striking purple color, which \ncontrasts with the white walls. The model is \nlikely used in animation or game design, where \nsuch whimsical, colorful elements are common.\nFigure 5. Additional Qualitative Video, Audio and Point Cloud Demos.\n13\nWhat are people doing in the normal map?\nIn the normal map, people are skateboarding and \nperforming tricks at a skate park.\nWhat's in this room?\nThe room is dimly lit, with a bed in the center \nand a small wooden table nearby. There is also a \nchair in the room. The bed is made up with a \nwhite comforter and pillows, and there is a lamp \non the table. The room is decorated with several \npotted plants, including a large one near the bed. \nAdditionally, there is a vase with flowers in it, \nand a clock on the wall. The overall atmosphere \nof the room is cozy and inviting.\nDescribe the scene based on the normal map\nThe normal map shows a group of horses \nstanding in a fenced-in area, possibly a corral. \nThere are five horses in total, with four of them \nstanding close together, and one horse standing \nslightly apart from the others. The horses are of \nvarious sizes and are positioned in a way that \nsuggests they are waiting or resting. The fence \nsurrounding the area is made of wood, and the \nground is covered with dirt.\nWhat can you know from the depth map?\nThe depth image shows a man riding a horse \nthrough a snowy field, with the horse's tail and \nmane blowing in the wind. The man is wearing \na black jacket, and the horse is a dark brown \ncolor. The scene is set against a backdrop of a \nsunset, creating a beautiful and serene \natmosphere. The depth image captures the \nessence of the man and his horse enjoying the \noutdoors during the winter season.\nWhat are the two people doing?\nThe two people are playing baseball, with one \nperson holding a bat and the other person \nholding a glove.\nWhat should you do in this situation?\nIn this situation, it is important to remain calm \nand cautious. Since the bear is walking towards \nthe camera, it is essential to avoid sudden \nmovements or loud noises that might startle the \nbear. If the bear approaches, it is crucial to \nmaintain eye contact with the bear to show that \nyou are aware of its presence and not a threat. If \nthe bear starts to charge, it is advisable to back \naway slowly and carefully, keeping an eye on \nthe bear to ensure it does not change direction. \nWhat happened in the motion?\nI can see a person walking on the floor and \npicking something up.\n(GT: picks the mats)       (reference video)\nWhat is the person doing?\nThe person is walking on the sidewalk.\n(GT: walks in the stadium) (reference video)\nDescribe the motion.\nThe motion is like a dance, with the paintbrush \nmoving in a smooth and controlled motion.\n(GT: dips the brush in the paint) (reference video)\nWhat scene does this fMRI data record?\nA man riding a kiteboard on a long wave in \nthe ocean.\n(reference visual stimulus)\nWhat is shown in the fMRI data?\nThe fMRI data show the brain activity about a \nzebra.\n(reference visual stimulus)\nWhat scene does this fMRI data record?\nThe fMRI data record the scene of a flower \ngarden.\n(reference visual stimulus)\nFigure 6. Additional Qualitative Depth/Normal Map, IMU and fMRI Demos.\n14\ndataset. arXiv preprint arXiv:2304.08345, 2023. 3, 6, 7,\n10\n[13] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao,\nMingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: A vision-\naudio-subtitle-text omni-modality foundation model and\ndataset. arXiv preprint arXiv:2305.18500, 2023. 3, 6\n[14] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server. arXiv preprint arXiv:1504.00325, 2015. 5,\n10\n[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip: Towards general-\npurpose vision-language models with instruction tuning,\n2023. 5, 6, 7\n[16] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR, pages 13142\u2013\n13153, 2023. 6, 10\n[17] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and\nHuaming Wang. Pengi: An audio language model for audio\ntasks. arXiv preprint arXiv:2305.11834, 2023. 6, 7\n[18] Konstantinos Drossos, Samuel Lipping, and Tuomas Vir-\ntanen. Clotho: An audio captioning dataset. In ICASSP,\npages 736\u2013740. IEEE, 2020. 6, 7, 10\n[19] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir\nZamir. Omnidata: A scalable pipeline for making multi-\ntask mid-level vision datasets from 3d scans.\nIn ICCV,\npages 10786\u201310796, 2021. 5, 7, 10\n[20] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui\nYang, Xiawu Zheng, et al. Mme: A comprehensive eval-\nuation benchmark for multimodal large language models.\narXiv preprint arXiv:2306.13394, 2023. 1, 6, 10\n[21] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, et al.\nLlama-adapter v2: Parameter-efficient\nvisual instruction model. arXiv preprint arXiv:2304.15010,\n2023. 2, 3, 4, 6\n[22] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens\nvan der Maaten, Armand Joulin, and Ishan Misra. Omni-\nvore: A single model for many visual modalities. In CVPR,\npages 16102\u201316112, 2022. 7\n[23] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat\nSingh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan\nMisra. Imagebind: One embedding space to bind them all.\nIn CVPR, pages 15180\u201315190, 2023. 3, 7\n[24] Yuan Gong, Yu-An Chung, and James Glass. AST: Au-\ndio Spectrogram Transformer. In Proc. Interspeech 2021,\npages 571\u2013575, 2021. 9\n[25] Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Kar-\nlinsky, and James Glass.\nListen, think, and understand.\narXiv preprint arXiv:2305.10790, 2023. 1, 3, 6, 7\n[26] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. Making the v in vqa matter: El-\nevating the role of image understanding in visual question\nanswering. In CVPR, pages 6904\u20136913, 2017. 5, 6, 7, 10\n[27] Kristen Grauman,\nAndrew Westbury,\nEugene Byrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:\nAround the world in 3,000 hours of egocentric video. In\nCVPR, pages 18995\u201319012, 2022. 5, 7, 10\n[28] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xi-\nanzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi\nLi, Hongsheng Li, et al. Point-bind & point-llm: Align-\ning point cloud with multi-modality for 3d understand-\ning, generation, and instruction following. arXiv preprint\narXiv:2309.00615, 2023. 1, 3\n[29] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people. In CVPR, pages 3608\u20133617, 2018. 6, 10\n[30] Andrey Guzhov, Federico Raue, J\u00a8orn Hees, and Andreas\nDengel. Audioclip: Extending clip to image, text and audio.\nIn ICASSP, pages 976\u2013980. IEEE, 2022. 3\n[31] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng\nXu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu\nGuo, et al. Imagebind-llm: Multi-modality instruction tun-\ning. arXiv preprint arXiv:2309.03905, 2023. 2, 3, 6, 11\n[32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, et al. Training compute-optimal large language mod-\nels. arXiv preprint arXiv:2203.15556, 2022. 6\n[33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 5\n[34] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, 2019. 5, 6, 10\n[35] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\nAndrew Zisserman, and Joao Carreira.\nPerceiver: Gen-\neral perception with iterative attention. pages 4651\u20134664.\nPMLR, 2021. 2, 3\n[36] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg. Referitgame: Referring to objects in pho-\ntographs of natural scenes.\nIn EMNLP, pages 787\u2013798,\n2014. 5, 10\n[37] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and\nGunhee Kim. Audiocaps: Generating captions for audios\nin the wild. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 119\u2013132, 2019. 5, 10\n[38] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 123:32\u201373, 2017. 5, 10\n[39] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\n15\nSiddharth Karamcheti, Alexander M Rush, Douwe Kiela,\net al.\nObelisc:\nAn open web-scale filtered dataset\nof interleaved image-text documents.\narXiv preprint\narXiv:2306.16527, 2023. 6\n[40] Hung Le, Doyen Sahoo, Nancy F Chen, and Steven CH\nHoi.\nMultimodal transformer networks for end-to-\nend video-grounded dialogue systems.\narXiv preprint\narXiv:1907.01166, 2019. 6\n[41] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\nmodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 6, 10\n[42] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-\nRong Wen, and Di Hu. Learning to answer questions in\ndynamic audio-visual scenarios. In CVPR, pages 19108\u2013\n19118, 2022. 5, 6, 7, 10\n[43] Guangyao Li, Yixin Xu, and Di Hu.\nMulti-scale at-\ntention for audio question answering.\narXiv preprint\narXiv:2305.17993, 2023. 6\n[44] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv\npreprint arXiv:2301.12597, 2023. 1, 2, 3, 6\n[45] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-\nhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu\nQiao. Videochat: Chat-centric video understanding. arXiv\npreprint arXiv:2305.06355, 2023. 1, 3\n[46] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng\nYu, and Jingjing Liu.\nHero: Hierarchical encoder for\nvideo+ language omni-representation pre-training.\narXiv\npreprint arXiv:2005.00200, 2020. 6, 7, 10\n[47] Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos\nDrossos, and Tuomas Virtanen.\nClotho-aqa: A crowd-\nsourced dataset for audio question answering.\nIn 2022\n30th European Signal Processing Conference (EUSIPCO),\npages 1140\u20131144. IEEE, 2022. 6, 7, 10\n[48] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning, 2023. 2,\n3, 5, 6\n[49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. NeurIPS, 2023. 4, 5, 10\n[50] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, et al. Mmbench: Is your multi-\nmodal model an all-around player?\narXiv preprint\narXiv:2307.06281, 2023. 6, 10\n[51] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mor-\ndatch. Pretrained transformers as universal computation en-\ngines. arXiv preprint arXiv:2103.05247, 1, 2021. 2, 4\n[52] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan.\nLearn to explain: Multimodal rea-\nsoning via thought chains for science question answering.\nNeurIPS, 2022. 6, 10\n[53] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,\nNan Duan, and Tianrui Li. Clip4clip: An empirical study\nof clip for end to end video clip retrieval and captioning.\nNeurocomputing, 508:293\u2013304, 2022. 3\n[54] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models. arXiv\npreprint arXiv:2306.07279, 2023. 5, 10\n[55] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In CVPR, 2019.\n5, 6, 10\n[56] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong,\nTom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou,\nand Wenwu Wang. Wavcaps: A chatgpt-assisted weakly-\nlabelled audio captioning dataset for audio-language multi-\nmodal research. arXiv preprint arXiv:2303.17395, 2023. 5,\n6, 10\n[57] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter,\nDanny Driess, Montserrat Gonzalez Arenas, Kanishka Rao,\nDorsa Sadigh, and Andy Zeng. Large language models as\ngeneral pattern machines. In CoRL, 2023. 2\n[58] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In ICDAR, pages 947\u2013952. IEEE,\n2019. 5, 10\n[59] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar\nNagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,\nPrakash Murugesan, Peyman Heidari, Yue Liu, et al. Any-\nmal: An efficient and scalable any-modality augmented lan-\nguage model. arXiv preprint arXiv:2309.16058, 2023. 2, 3,\n6, 11\n[60] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob\nFergus. Indoor segmentation and support inference from\nrgbd images. In ECCV, 2012. 7, 10\n[61] Dat Tien Nguyen, Shikhar Sharma, Hannes Schulz, and\nLayla El Asri.\nFrom film to video:\nMulti-turn ques-\ntion answering with multi-modal context. arXiv preprint\narXiv:1812.07023, 2018. 6\n[62] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774,\n2023. 1\n[63] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervi-\nsion. arXiv preprint arXiv:2304.07193, 2023. 9\n[64] Hoang-Anh Pham, Thao Minh Le, Vuong Le, Tu Minh\nPhuong, and Truyen Tran. Video dialog as conversation\nabout objects living in space-time. In ECCV, pages 710\u2013\n726. Springer, 2022. 6\n[65] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In ICCV,\npages 2641\u20132649, 2015. 6, 10\n[66] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil\nHoulsby. From sparse to soft mixtures of experts. arXiv\npreprint arXiv:2308.00951, 2023. 2, 4\n[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\n16\nvision. In ICML, pages 8748\u20138763. PMLR, 2021. 2, 3, 4,\n7\n[68] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction.\nArXiv preprint,\n2021. 5, 10\n[69] C. Schuhmann, A. K\u00a8opf, R. Vencu, T. Coombes, and R.\nBeaumont.\nLaion coco: 600m synthetic captions from\nlaion2b-en. 5, 10\n[70] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5b: An open large-scale dataset for\ntraining next generation image-text models. NeurIPS, 35:\n25278\u201325294, 2022. 1, 5, 10\n[71] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi.\nA-okvqa: A\nbenchmark for visual question answering using world\nknowledge. In ECCV, pages 146\u2013162. Springer, 2022. 5,\n7, 10\n[72] Paul S Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan\nShabalin, Alex Nguyen, Ethan Cohen, Aidan J Demp-\nster, Nathalie Verlinde, Elad Yundler, David Weisberg,\net al. Reconstructing the mind\u2019s eye: fmri-to-image with\ncontrastive learning and diffusion priors.\narXiv preprint\narXiv:2305.18274, 2023. 10\n[73] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of ACL, 2018. 5, 10\n[74] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In ECCV, pages 742\u2013758.\nSpringer, 2020. 10\n[75] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In CVPR,\npages 8317\u20138326, 2019. 6, 10\n[76] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.\nSun rgb-d: A rgb-d scene understanding benchmark suite.\nIn CVPR, pages 567\u2013576, 2015. 7, 10\n[77] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and\nDeng Cai. Pandagpt: One model to instruction-follow them\nall. arXiv preprint arXiv:2305.16355, 2023. 3, 11\n[78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971, 2023. 2, 6\n[79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023. 4, 5, 6\n[80] Gido M Van de Ven and Andreas S Tolias. Three scenarios\nfor continual learning. arXiv preprint arXiv:1904.07734,\n2019. 4\n[81] Vicuna. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. https://vicuna.lmsys.\norg/, 2023. 6\n[82] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan\nWang. Git: A generative image-to-text transformer for vi-\nsion and language. arXiv preprint arXiv:2205.14100, 2022.\n3\n[83] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang.\nVatex: A large-scale,\nhigh-quality multilingual dataset for video-and-language\nresearch. In ICCV, pages 4581\u20134591, 2019. 6, 10\n[84] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun\nWang, et al. Internvideo: General video foundation models\nvia generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022. 6, 7\n[85] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor\nBerg-Kirkpatrick, and Shlomo Dubnov. Large-scale con-\ntrastive language-audio pretraining with feature fusion and\nkeyword-to-caption augmentation. In ICASSP, pages 1\u20135.\nIEEE, 2023. 3\n[86] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.\nNext-qa: Next phase of question-answering to explaining\ntemporal actions. In CVPR, pages 9777\u20139786, 2021. 6, 7,\n10\n[87] Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei\nJi, and Tat-Seng Chua. Video as conditional graph hierar-\nchy for multi-granular question answering. In AAAI, pages\n2804\u20132812, 2022. 6\n[88] Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu,\nXiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda, Kurt\nKeutzer, and Masayoshi Tomizuka. Image2point: 3d point-\ncloud understanding with 2d image pretrained models. In\nECCV, pages 638\u2013656. Springer, 2022. 2\n[89] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video question answer-\ning via gradually refined attention over appearance and mo-\ntion. In ACM Multimedia, 2017. 5, 6, 7, 10\n[90] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,\nArmen Aghajanyan, Florian Metze, Luke Zettlemoyer,\nand Christoph Feichtenhofer. Videoclip: Contrastive pre-\ntraining for zero-shot video-text understanding.\narXiv\npreprint arXiv:2109.14084, 2021. 3\n[91] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language.\nIn CVPR, pages 5288\u20135296, 2016. 5, 10\n[92] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiang-\nmiao Pang, and Dahua Lin. Pointllm: Empowering large\nlanguage models to understand point clouds. arXiv preprint\narXiv:2308.16911, 2023. 3, 5, 6, 7, 10, 11\n[93] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Just ask: Learning to answer questions\nfrom millions of narrated videos. In ICCV, pages 1686\u2013\n1697, 2021. 6\n[94] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Zero-shot video question answering via\n17\nfrozen bidirectional language models. NeurIPS, 35:124\u2013\n141, 2022. 6, 7\n[95] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 2, 3\n[96] Zhongjie Ye, Yuqing Wang, Helin Wang, Dongchao Yang,\nand Yuexian Zou.\nFeaturecut:\nAn adaptive data aug-\nmentation for automated audio captioning. In 2022 Asia-\nPacific Signal and Information Processing Association An-\nnual Summit and Conference (APSIPA ASC), pages 313\u2013\n318. IEEE, 2022. 6\n[97] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.\nSelf-chained image-language model for video localization\nand question answering. arXiv preprint arXiv:2305.06988,\n2023. 6\n[98] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet:\nEvaluating large multimodal models for inte-\ngrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\n6\n[99] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu\nWang, Yaqian Zhou, and Xipeng Qiu.\nSpeechgpt: Em-\npowering large language models with intrinsic cross-modal\nconversational abilities. arXiv preprint arXiv:2305.11000,\n2023. 1, 3\n[100] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video\nunderstanding. arXiv preprint arXiv:2306.02858, 2023. 1,\n3\n[101] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-\npeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng\nLi. Pointclip: Point cloud understanding by clip. In CVPR,\npages 8552\u20138562, 2022. 3\n[102] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\nLlama-adapter: Efficient fine-tuning of language models\nwith zero-init attention. arXiv preprint arXiv:2303.16199,\n2023. 2\n[103] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hong-\nsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Meta-\ntransformer: A unified framework for multimodal learning.\narXiv preprint arXiv:2307.10802, 2023. 2, 4, 9\n[104] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen,\nShuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. Chat-\nbridge: Bridging modalities with large language model as a\nlanguage catalyst. arXiv preprint arXiv:2305.16103, 2023.\n2, 3, 5, 6, 10, 11\n[105] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 3, 4\n18\n"
  },
  {
    "title": "MotionCtrl: A Unified and Flexible Motion Controller for Video Generation",
    "link": "https://arxiv.org/pdf/2312.03641.pdf",
    "upvote": "19",
    "text": "MotionCtrl: A Unified and Flexible Motion Controller for Video Generation\nhttps://wzhouxiff.github.io/projects/MotionCtrl/\nZhouxia Wang1,2*\nZiyang Yuan1,4*\nXintao Wang1,3 Q\nTianshui Chen6\nMenghan Xia3\nPing Luo2,5 Q\nYing Shan1,3\n1 ARC Lab, Tencent PCG\n2 The University of Hong Kong\n3 Tencent AI Lab\n4 Tsinghua University\n5 Shanghai AI Laboratory\n6 Guangdong University of Technology\n(a) Prompt: A cute dog sitting on the green grass.\n(b) Prompt: The rose swaying in the wind.\nCamera Poses\nTrajectory\nCamera Poses\n(c) Prompt: The rose swaying in the wind.\nFigure 1. Control Results of MotionCtrl. MotionCtrl is capable of controlling both camera motion and object motion in videos produced\nby a video generation model. It can also simultaneously control both types of motion within the same video. We highly encourage readers\nto check our project page for video results, which cannot be well demonstrated by still images.\nAbstract\nMotions in a video primarily consist of camera motion,\ninduced by camera movement, and object motion, result-\ning from object movement. Accurate control of both camera\nand object motion is essential for video generation. How-\never, existing works either mainly focus on one type of mo-\ntion or do not clearly distinguish between the two, limiting\ntheir control capabilities and diversity. Therefore, this pa-\nper presents MotionCtrl, a unified and flexible motion con-\ntroller for video generation designed to effectively and in-\ndependently control camera and object motion. The archi-\ntecture and training strategy of MotionCtrl are carefully de-\nvised, taking into account the inherent properties of camera\n\u2217 Interns in ARC Lab, Tencent PCG\nQ Corresponding author\nmotion, object motion, and imperfect training data. Com-\npared to previous methods, MotionCtrl offers three main ad-\nvantages: 1) It effectively and independently controls cam-\nera motion and object motion, enabling more fine-grained\nmotion control and facilitating flexible and diverse combi-\nnations of both types of motion. 2) Its motion conditions\nare determined by camera poses and trajectories, which are\nappearance-free and minimally impact the appearance or\nshape of objects in generated videos. 3) It is a relatively\ngeneralizable model that can adapt to a wide array of cam-\nera poses and trajectories once trained. Extensive qual-\nitative and quantitative experiments have been conducted\nto demonstrate the superiority of MotionCtrl over existing\nmethods.\n1\narXiv:2312.03641v1  [cs.CV]  6 Dec 2023\n1. Introduction\nVideo generation, such as text-to-video (T2V) genera-\ntion [4, 5, 9, 10, 23, 32] aims to produce diverse and high-\nquality videos that conform to given text prompts. Unlike\nimage generation [6, 17\u201319, 21, 34], which focuses on gen-\nerating a single image, video generation necessitates the\ncreation of consistent and fluent motion among a sequence\nof generated images. Consequently, motion control plays a\nsignificantly crucial role in video generation, yet it has re-\nceived limited attention in recent research.\nIn a video, there are primarily two types of motion:\nglobal motion induced by camera movements and local mo-\ntion resulting from object movements (examples are re-\nferred to the zoom out camera poses and swaying rose in\nFigure 1 (c)). It should be noted that these two motions will\nbe consistently referred to as camera motion and object\nmotion throughout the paper, respectively. However, most\nprevious works related to motion control in video genera-\ntion either primarily focus on one of the motions or lack a\nclear distinction between these two types of motion. For in-\nstance, AnimateDiff [8], Gen-2 [7], and PikaLab [2] mainly\nexecute or trigger camera motion control using independent\nLoRA [11] models or extra camera parameters (such as \u201c-\ncamera zoom in\u201d in PikaLab [2]). VideoComposer [25] and\nDragNUWA [28] implement both camera motion and object\nmotion using the same conditions: motion vector in Video-\nComposer [25] and trajectory in DragNUWA [28].\nThe\nlack of clear distinction between these two motions prevents\nthese approaches from achieving fine-grained and diverse\nmotion control in video generation.\nIn this paper, we introduce MotionCtrl, a unified and\nflexible motion controller for video generation, designed to\nindependently control camera and object motion with a uni-\nfied model. This approach enables fine-grained motion con-\ntrol in video generation and facilitates flexible and diverse\ncombinations of both motion types. However, constructing\nsuch a unified motion controller presents significant chal-\nlenges due to the following two factors. First, camera and\nobject motions differ significantly in terms of movement\nrange and pattern. Camera motion refers to the global trans-\nformation of the whole scene across the temporal dimen-\nsion, which is typically represented through a sequence of\ncamera poses over time. In contrast, object motion involves\nthe temporal movement of specific objects within the scene,\nand it is usually represented as the trajectory of a cluster\nof pixels associated with the objects. Second, no existing\ndataset encompasses video clips that are accompanied by\na complete set of annotations, including captions, camera\nposes, and object movement trajectories. Creating such a\ncomprehensive dataset requires a significant amount of ef-\nfort and resources.\nIn this paper, we design a specialized architecture and\ntraining strategy for MotionCtrl to address the aforemen-\ntioned challenges. First, we construct MotionCtrl with two\nmodules: the Camera Motion Control Module (CMCM)\nand Object Motion Control Module (OMCM), specifically\ntailored for camera motion and object motion according\nto their properties.\nBoth CMCM and OMCM cooperate\nwith the existing video generation model. Noted that in\nthis work, we adopt VideoCrafter1 [5], an improved version\nover LVDM [9] as the video generation model, and refer it\nas LVDM throughout the paper. CMCM temporally fuses\na sequence of camera poses into the LVDM [9] via its tem-\nporal transformers, enabling the global motion of the gen-\nerated video to conform to the given camera poses. OMCM\nspatially fuses the information on object movement into the\nconvolutional layers in LVDM to indicate the spatial posi-\ntion of the object in each generated frame.\nIn this study, we utilize VideoCrafter[5], a model struc-\nturally akin to LVDM[9]. For clarity, we will refer to the\nvideo model used in this work as LVDM.\u201d\nTraining the MotionCtrl requires video clips with com-\nplete annotations of captions, camera poses, and object\nmovement trajectories, which are currently unavailable and\nquite difficult to construct.\nBenefiting from the careful-\ndesigned architecture that depends on a large-scale pre-\ntrained video diffusion model equipped with two adapter-\nlike [15, 29] CMCM and OMCM modules, we can train\nthe two modules independently and thus compromise the\ndatasets to one video dataset with the annotations of cap-\ntions and camera poses and another video dataset with the\nannotations of captions and object movement trajectories.\nTo this end, we first introduce an augmented-Realestate10k\ndataset, which exploits Blip2 [13] to generate captions for\neach sample from Realestate10k [33]. This video dataset\nwith both caption and camera pose annotations is adopted to\ntrain the CMCM module. After that, we augment the video\nfrom WebVid [3] with object movement trajectories synthe-\nsized by a motion segmentation algorithm proposed in Par-\nticleSfM [31]. Thus, we can obtain an augmented WebVid\ndataset to train the OMCM module. Benefiting from the\nindependent adapter-like training strategy and frozen pre-\ntrained LVDM, we can use one of the camera and object\nmotions or combine both motions to control video genera-\ntion, enabling fine-grained and flexible motion control.\nThrough these delicate designs, MotionCtrl demon-\nstrates superiority over previous methods in three aspects:\n1) It independently controls camera and object motion, en-\nabling fine-grained adjustments and a variety of motion\ncombinations, as shown in Figure 1.\n2) It uses camera\nposes and trajectories for motion conditions, which do not\naffect the visual appearance, maintaining the objects\u2019 natu-\nral look in videos. For instance, our MotionCtrl generates\na video with a camera motion that closely reflects the refer-\nence video, offering a realistic Eiffel Tower, as seen in Fig-\nure 4 (b). In contrast, VideoComposer [25] relies on dense\n2\nmotion vectors and mistakenly captures a door\u2019s shape of\nthe reference video, resulting in an unnatural Eiffel Tower.\n3) MotionCtrl can control a variety of camera movements\nand trajectories, without the need for fine-tuning each indi-\nvidual camera or object motion.\nThe main contributions of this work can be summarized\nas follows:\n\u2022 We introduce MotionCtrl, a unified and flexible mo-\ntion controller for video generation, designed to indepen-\ndently and effectively control camera motion and object\nmotion in generated videos, achieving more fine-grained\nand diverse motion control.\n\u2022 We carefully tailor the architecture and training strat-\negy of MotionCtrl according to the inherent properties\nof camera motion, object motion, and imperfect training\ndata, effectively achieving fine-grained motion control in\nvideo generation.\n\u2022 We conduct extensive experiments to demonstrate the su-\nperiority of MotionCtrl over previous related methods,\nboth qualitatively and quantitatively.\n2. Related Works\nVideo generation, especially text-to-video (T2V) generation\nstrives to create varied and high-fidelity videos that align\nwith specified textual descriptions. With advancements in\ndiffusion models and the availability of robust computa-\ntional resources, a plethora of diffusion-based video gen-\neration models have emerged[4, 5, 8\u201310, 23, 25, 32]. No-\ntably, deploying diffusion models in latent space[4, 9, 19]\nhas enhanced the computational efficiency of video gener-\nation. Concurrently, there has been a surge in research fo-\ncused on controlling motion within generated videos. Many\nexisting approaches learn motion by referencing specific or\na series of template videos[8, 26, 27, 30]. While effective\nat motion control, these methods typically require training a\nnew model for each template or set of templates, which can\nbe limiting.\nSome efforts aim to achieve more generalized motion\ncontrol. For instance, VideoComposer [25] introduces mo-\ntion control via extra provided motion vectors, and Drag-\nNUWA [28] suggests video generation conditioned on an\ninitial image, provided trajectories, and text prompts. How-\never, the motion control in these methods is relatively broad\nand fails to fine-grainedly disentangle the camera and object\nmotion within videos.\nDifferent from these works, we propose MotionCtrl, a\nunified and flexible motion controller that can use either the\ncamera poses and object trajectories or combine these two\nkinds of guidance to control the motion of generated videos.\nIt enables a more fine-grained and flexible control for video\ngeneration.\n3. Methodology\n3.1. Preliminary\nLatent Video Diffusion Model (LVDM). The Latent Video\nDiffusion Model (LVDM) [9] aims to generate high-quality\nand diverse videos guided by text prompts. It employs a\ndenoising diffusion model (U-Net [20]) in the latent space\nfor space and time efficiency. Consequently, it constructs a\nlightweight 3D autoencoder, comprising an encoder E and\na decoder D, to encode raw videos into the latent space and\nreconstruct the denoised latent features back into videos,\nrespectively. Its denoising U-Net (denoted as \u03f5\u03b8) is con-\nstructed with a sequence of blocks that consist of convolu-\ntional layers, spatial transformers, and temporal transform-\ners (shown in Figure 2).\nIt is optimized using a noise-\nprediction loss:\nL = Ez0,c,\u03f5\u223cN (0,I ),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, c)\u22252\n2\n\u0003\n,\n(1)\nwhere c represents the text prompt, z0 is the latent code\nobtained using E, t (t \u2208 [0, T]) denotes the time step, and\nzt is the noisy latent features acquired by weighted addition\nof Gaussian noise \u03f5 to z0 using the following formula:\nzt = \u221a \u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, \u00af\u03b1t =\ntY\ni=1\n\u03b1t,\n(2)\nwhere \u03b1t is used for scheduling the noise strength based on\ntime step t.\n3.2. MotionCtrl\nFigure 2 illustrates the framework of MotionCtrl.\nTo\nachieve disentanglement between camera motion and ob-\nject motion, and enable independent control of these two\ntypes of motion, MotionCtrl comprises two main compo-\nnents: a Camera Motion Control Module (CMCM) and an\nObject Motion Control Module (OMCM). Taking into ac-\ncount the global property of camera motion and the local\nproperty of object motion, CMCM interacts with the tem-\nporal transformers in LVDM, while OMCM spatially coop-\nerates with the convolutional layers in LVDM. Furthermore,\nwe employ multiple training steps to adapt MotionCtrl to\nthe absence of training data that contains high-quality video\nclips accompanied by captions, camera poses, and object\nmovement trajectories. In the following subsections, we\nwill provide a detailed description of CMCM and OMCM\nalong with their corresponding training datasets and train-\ning strategies.\n3.2.1\nCamera Motion Control Module (CMCM)\nThe CMCM is a lightweight module constructed with sev-\neral fully connected layers. Since the camera motions are\nglobal transformations between frames in a video, CMCM\n3\n\uff08a\uff09\n\uff08b\uff09\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\nCLIP\n\ud835\udc45\ud835\udc47 = {\ud835\udc45\ud835\udc470, \ud835\udc45\ud835\udc471,\u2026, \ud835\udc45\ud835\udc47\ud835\udc3f\u22121}\nPrompt: A man rises a \nhorse in Mars.\n\ud835\udc4d\ud835\udc47\n\ud835\udc47\ud835\udc5f\ud835\udc4e\ud835\udc57\ud835\udc60\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\nOMCM\n\ud835\udc66\ud835\udc61: [\ud835\udc35, \ud835\udc3f, \ud835\udc3b, \ud835\udc4a, \ud835\udc36]\n\ud835\udc45\ud835\udc47: [\ud835\udc35, \ud835\udc3f,12]\n[\ud835\udc35 \u00d7 \ud835\udc3b \u00d7 \ud835\udc4a, \ud835\udc3f, 12]\n[\ud835\udc35 \u00d7 \ud835\udc3b \u00d7 \ud835\udc4a, \ud835\udc3f, \ud835\udc36]\n[\ud835\udc35 \u00d7 \ud835\udc3b \u00d7 \ud835\udc4a, \ud835\udc3f,\ud835\udc36 + 12]\nFC\nAttn2\nReshape\nRepeat\n[\ud835\udc35 \u00d7 \ud835\udc3b \u00d7 \ud835\udc4a, \ud835\udc3f, \ud835\udc36]\nConcat\nCMCM\nConv Layers\nSpatial Transformer\nTemporal Transformer\nTrainable\nSum\nCamera\nCMCM\nConv Layers\nCamera \nPose\nDenoising U-Net\nFigure 2. MotionCtrl Framework. MotionCtrl extends the Denoising U-Net structure of LVDM with a Camera Motion Control Module\n(CMCM) and an Object Motion Control Module (OMCM). As illustrated in (b), the CMCM integrates camera pose sequences RT with\nLVDM\u2019s temporal transformers by appending RT to the input of the second self-attention module and applying a tailored and lightweight\nfully connected layer to extract the camera pose feature for subsequent processing. The OMCM utilizes convolutional layers and down-\nsamplings to derive multi-scale features from Trajs, which are spatially incorporated into LVDM\u2019s convolutional layers to direct object\nmotion. Further given a text prompt, LVDM generates videos from noise that correspond to the prompt, with background and object\nmovements reflecting the specified camera poses and trajectories. The resulting video demonstrates the horse moving along its trajectory\nand meanwhile, the background moves left, consistent with the camera\u2019s rightward motion.\ncooperates with LVDM [9] via its temporal transformers.\nTypically, the temporal transformers in LVDM comprise\ntwo self-attention modules and facilitate temporal informa-\ntion fusion between video frames.\nTo minimize the im-\npact on LVDM\u2019s generative performance, CMCM only in-\nvolves the second self-attention module in the temporal\ntransformers.\nSpecifically, CMCM takes a sequence of\ncamera poses RT = {RT0, RT1, . . . , RTL\u22121} as input. In\nthis paper, the camera pose is represented by its 3 \u00d7 3 ro-\ntation matrix and 3 \u00d7 1 translation matrix. Consequently,\nRT \u2208 RL\u00d712, where L denotes the length of the gener-\nated video. As depicted in Figure 2 (b), RT is extended to\nH \u00d7W \u00d7L\u00d712 before being concatenated with the output\nof the first self-attention module in the temporal transformer\n(yt \u2208 RH\u00d7W \u00d7L\u00d7C) along the last dimension, where H and\nW represent the latent spatial size of the generated video,\nand C is the number of channels in yt. The concatenated re-\nsults are then projected back to the size of H \u00d7 W \u00d7 L \u00d7 C\nusing a fully connected layer before being fed into the sec-\nond self-attention module in the temporal transformer.\n3.2.2\nObject Motion Control Module (OMCM)\nAs depicted in Figure 2, MotionCtrl controls the object\nmotion of the generated video using trajectories (Trajs).\nTypically, a trajectory can be represented as a sequence\nof spatial positions {(x0, y0), (x1, y1), . . . , (xL\u22121, yL\u22121)},\nwhere (xi, yi), i\n\u2208\n[0, T \u2212 1] indicates that the tra-\njectory passes through the ith frame at the spatial po-\nsition (x, y).\nWhile implemented, we tend transform\nthe trajectory {(x0, y0), (x1, y1), . . . , (xL\u22121, yL\u22121)} into\n{(0, 0), (u1, v1), . . . , (uL\u22121, vL\u22121)}, where\nui = xi \u2212 xi\u22121, vi = yi \u2212 yi\u22121, i > 1.\n(3)\nDenoted that the other spatial positions that the trajec-\ntories do not pass are described as (0, 0) and Trajs \u2208\nRL\u00d7H\u00d7W \u00d72.\nThe OMCM spatially incorporates Trajs into the\nLVDM generation process, as these trajectories represent\nthe spatial positions of the object in each frame.\nThe\nOMCM\u2019s architecture, highlighted in the purple block of\nFigure 2, consists of multiple convolutional layers com-\nbined with downsampling operations. It extracts multi-scale\nfeatures from the Trajs and corresponding adds them to\nthe input of the LVDM\u2019s convolutional layers. Drawing in-\nspiration from previous works such as ControlNet[29] and\nT2I-Adapter [15], the trajectories are only applied to the\nencoder of the Denoising U-Net to balance the generated\nvideo quality with the ability to control object motion.\n3.2.3\nTraining Strategy and Data Construction\nTo achieve the control of camera and object motion while\ngenerating a video via text prompts, video clips in a training\ndataset must contain annotations of captions, camera poses,\nand object movement trajectories. However, a dataset with\nsuch comprehensive details is currently unavailable, and\n4\nassembling one would require considerable effort and re-\nsources. To address this challenge, we introduce a multi-\nstep training strategy and train our proposed camera motion\ncontrol module (CMCM) and object motion control module\n(OMCM) with distinct augmented datasets tailored to their\nspecific motion control requirements.\nLearning the camera motion control module (CMCM)\nrequires a training dataset that contains video clips with\ncaptions, and camera poses, but not object movement tra-\njectories.\nTo this end, we opt to use the Realestate10K\ndataset [33], which, after removing invalid video links, of-\nfers 62,992 video clips accompanied by diverse camera\nposes. Nonetheless, employing Realestate10K presents two\npotential challenges: 1) the dataset\u2019s limited diversity of\nscenes, primarily from real estate videos, potentially com-\npromising the quality of the generated video; and 2) it lacks\ncaptions needed for T2V models.\nRegarding the first challenge, we specifically design the\nCMCM module to only train several extra MLP layers and\nthe second self-attention module of the temporal transform-\ners in LVDM while freezing all other parameters (as illus-\ntrated in Figure 2 (b).). The training of CMCM mainly fo-\ncuses on learning global motion and seldom affects the con-\ntent of the generated video. Therefore, the limited scene di-\nversity in the Realestate10K dataset has a negligible effect\non the generated quality of LVDM. This is substantiated by\nquantitative results presented in Table 2, where the FID [22]\nand FVD [24] metrics indicate that the video quality gener-\nated by our MotionCtrl is on par with the LVDM outcomes.\nTo address the second challenge, we utilize Blip2 [13],\nan image captioning algorithm, to generate captions for\neach video clip in Realestate10K. We extract frames at spe-\ncific intervals\u2014the first, quarter, half, three-quarters, and\nfinal frames of the video. We then use Blip2 to predict their\ncaptions. These captions are then concatenated to form a\ncomprehensive description for each video clip. With these\ncaptions in place, we train the CMCM on Realestate10K,\nwhich in turn allows the LVDM to control camera motion\neffectively.\nLearning the object motion control module (OMCM) re-\nquires a dataset comprising video clips with corresponding\ncaptions and object movement trajectories. Currently, there\nis an absence of large-scale datasets that combine video-\ntext pairs with object trajectories. To address this, we em-\nploy ParticleSfM [31] to generate object movement trajec-\ntories using the WebVid dataset [3], which is widely used\nfor T2V generation. ParticleSfM features a trajectory mo-\ntion segmentation network capable of identifying trajecto-\nries associated with moving objects within a video, as de-\npicted in Figure 3 (b), where the trajectories predominantly\ncorrespond to a moving person. Despite its effectiveness,\nParticleSfM is not time-efficient, requiring approximately 2\nminutes to process a 32-frame video. To balance time ef-\nExtract object\nmovement trajectories\nRandomly select\nGaussian Filter\n(a)\n(b)\n(c)\n(d)\nFigure 3.\nTrajectories for Object Motion Control.\nParti-\ncleSfM [31] is employed to extract object movement trajectories\nfrom video clips, effectively disentangling object motion from\ncamera-induced movement. To circumvent the issues of dense tra-\njectories, which can encode object shapes and are challenging to\ndesign at inference, we train the OMCM using sparse trajectories\nsampled from the dense ones. These sparse trajectories, being too\nscattered for effective learning, are subsequently refined with a\nGaussian filter.\nficiency and data coverage, we opt to randomly select 32\nframes from each WebVid video, with a frame skip interval\ns \u2208 [1, 16], to precompute the object movement trajectories.\nThis approach yields a total of 243,000 samples that fulfill\nthe training requirements for the OMCM.\nTo enhance user experience and interaction, we utilize\nsparse trajectories to direct object motion instead of dense\nones.\nDuring the training process, we randomly select\nn \u2208 [1, N] trajectories (where N is the maximum number\nof trajectories for each video) from the synthesized trajec-\ntories. An example of the outcome can be seen in Figure 3\n(c). However, these selected sparse trajectories can be too\nscattered for effective training. To address this issue, we ap-\nply a Gaussian filter to the sparse trajectories (Figure 3 (d)),\ninspired by DragNUWA [28]. During training, we initially\ntrain the OMCM using dense trajectories and then proceed\nto fine-tune it with sparse trajectories.\nIn this training phase, we adopt the LVDM model that is\nalready fine-tuned with CMCM. We only train the OMCM\nlayers, while the entire base model and CMCM remains\nfrozen. This strategy guarantees that OMCM adds the ob-\nject motion control capabilities with a limited dataset while\nminimally impacting LVDM and CMCM. Upon completion\nof this training phase, giving both camera poses and trajec-\ntories allows for flexible customization of camera and object\nmotion in the generated video.\n4. Experiments\n4.1. Experiment Settings\nImplementation Details.\nOur MotionCtrl is based on\nthe LVDM [9] and the trained weights are provided by\nVideoCraft1 [5]. It is trained on 16-frame sequences at a\nresolution of 256 \u00d7 256. We set the maximum number of\ntrajectories N to 8. Both CMCM and OMCM are optimized\nusing the Adam optimizer [12] with a batch size of 128\n5\nand a learning rate of 1\u00d710\u22124 on 8 NVIDIA Tesla V100\nGPUs. The CMCM requires approximately 50,000 itera-\ntions to converge. OMCM is initially trained on dense tra-\njectories for 20,000 iterations, followed by fine-tuning with\nsparse trajectories for an additional 20,000 iterations.\nEvaluation Datasets. We independently evaluate the ef-\nficacy of our proposed MotionCtrl on camera motion con-\ntrol and object motion control. These assessments are con-\nducted using their respective evaluation datasets. More de-\ntails about the two evaluation datasets are provided in the\nsupplementary materials.\nCamera motion control evaluation dataset. MotionCtrl\u2019s\ncamera motion control is tested on a set of 8 basic cam-\nera pose sequences: pan left, pan right, pan up, pan down,\nzoom in, zoom out, anticlockwise rotation, and clockwise\nrotation. Additionally, we randomly select 20 complicated\ncamera pose sequences from the Realestate10K test set [33].\nFor each camera pose sequence, we evaluate 10 different\ntext prompts.\nObject motion control evaluation dataset. For object mo-\ntion control, we create 19 diverse trajectories, including\nstraight lines, curves, shaking lines, and complex combina-\ntions of those. For each trajectory, we evaluate 12 different\ntext prompts.\nEvaluation Metrics. 1) The quality of the generated videos\nis evaluated using Fr\u00b4echet Inception Distance (FID)[22],\nFr\u00b4echet Video Distance (FVD)[24], and CLIP Similarity\n(CLIPSIM) [16], which measure the visual quality, tem-\nporal coherence, and semantic similarity to the input text,\nrespectively. Denoted that the reference videos of FID and\nFVD are 1000 videos selected from WebVid [3]. 2) The effi-\ncacy of the camera motion control and object motion control\nis quantified by computing the Euclidean distance between\nthe predicted and ground truth camera poses and object tra-\njectories. The camera poses and object trajectories for the\npredicted videos are extracted using ParticleSfM [31]. We\ntitle these two metrics as CamMC and ObjMC, respec-\ntively.\n4.2. Comparisons with State-of-the-Art Methods\nTo validate the effectiveness of our MotionCtrl in control-\nling both camera and object motion, we compare it with\ntwo leading methods: AnimateDiff [8] and VideoCom-\nposer [25]. AnimateDiff employs 8 separate LoRA [11]\nmodels to control 8 basic camera motions in videos, such as\npanning and zooming, while VideoComposer manipulates\nvideo motion using motion vectors without differentiating\nbetween camera and object movements. Although Drag-\nNUWA [28] is relevant to our research, its code is not pub-\nlicly available, precluding a direct comparison. Moreover,\nDragNUWA only learns motion control with the trajectories\nextracted from optical flow, which cannot fine-grainedly\ndistinguish the movement between foreground objects and\nMethod\nAnimateDiff [8]\nVideoComposer [25]\nMotionCtrl\nCamMC \u2193 (Basic Poses)\n0.0548\n-\n0.0289\nCamMC \u2193 (Complex Poses)\n-\n0.1073\n0.0840\nObjMC \u2193\n-\n34.7778\n25.1198\nCLIPSIM \u2191\n0.2144\n0.2219\n0.2324\nFID \u2193\n157.73\n134.97\n130.29\nFVD \u2193\n1815.88\n1045.82\n934.37\nTable 1. Quantitative Comparisons with SOTA Methods. Our\nMotionCtrl outperforms competing approaches in both camera and\nobject motion control while also excelling at preserving text simi-\nlarity and the quality of the video generation.\nbackground, limiting its ability to precisely control camera\nand object motion.\nWe compare our MotionCtrl with these methods in terms\nof camera motion and object motion control, and show the\ncapability of our MotionCtrl in flexibly combining the con-\ntrol of camera motion and object motion in video genera-\ntion. More comparisons and video comparisons are pro-\nvided in the supplementary materials.\nCamera Motion Control.\nWe assess camera motion\ncontrol using basic poses and complex poses from the\nRealestate10K test set. AnimateDiff [8] is limited to basic\ncamera poses, while VideoComposer [25] handles complex\nposes by extracting motion vectors from provided videos.\nThe qualitative results are shown in Figure 4.\nFor ba-\nsic poses, both MotionCtrl and AnimateDiff can produce\nvideos with forward camera movement, but MotionCtrl\ncan generate camera motion with varying speeds, while\nAnimateDiff is nonadjustable. Regarding complex poses,\nwhere the camera first moves left front and then forward,\nVideoComposer can mimic the reference video\u2019s camera\nmotion using extracted motion vectors. However, the dense\nmotion vectors inadvertently capture object shapes, such as\na door\u2019s outline in the reference video (frame 12), resulting\nin an unnatural-looking Eiffel Tower. MotionCtrl, guided\nby rotation and translation matrices, generates more natural-\nlooking videos with camera motion close to the reference.\nQuantitative results in Table 1 show MotionCtrl\u2019s supe-\nriority over AnimateDiff and VideoComposer for both ba-\nsic and complex poses, as reflected by the CamMC score.\nAdditionally, MotionCtrl achieves better text similarity and\nquality metrics, as measured by CLIPSIM, FID, and FVD.\nObject Motion Control. We compare our MotionCtrl with\nVideoComposer for object motion control, where Video-\nComposer utilizes motion vectors extracted from trajecto-\nries. The qualitative results are shown in Figure 4 (c). The\nred curve illustrates the given trajectory, while the green\npoints indicate the expected object locations in the corre-\nsponding frame. The visual comparison reveals that Mo-\ntionCtrl can generate objects whose movements are closer\nto the given trajectories, whereas VideoComposer\u2019s results\ndeviate in certain frames, highlighting MotionCtrl\u2019s supe-\nrior object motion control capability. The quantitative re-\nsults in Table 1 demonstrate that MotionCtrl achieves better\nobject motion control than VideoComposer, as reflected by\n6\nPrompt: Rocky coastline with crashing waves.\nCamera Poses\nCamera Poses\nTrajectory\nAnimateDiff\nMotionCtrl\n1x Speed\nMotionCtrl\n2x Speed\nRealestate10K\nVideo\nVideoComposer\nMotionCtrl\nMotionCtrl\nVideoComposer\n(a) Camera motion control on basic poses.\n(b) Camera motion control on complex poses.\n(c) Object motion control.\nFrame 0\nFrame 3\nFrame 6\nFrame 9\nFrame 12\nFrame 15\nMotionCtrl\n0.2x Speed\nPrompt: Effiel Tower in Paris.\nMotion Vectors\nPrompt: A girl is skiing.\nZoom In\nFrame 12\nFigure 4. Qualitative Comparisons on Camera and Object Motion Control. (a) Basic Poses: MotionCtrl and AnimateDiff[8] effectively\nexecute zooms, but MotionCtrl can adjust to varying camera moving speeds. (b) Complex Poses: VideoComposer[25] uses Realestate10K\u2019s\nraw video for motion vectors, capturing unintended shapes like doors, leading to unnatural results (refer to frame 12). MotionCtrl, however,\nproduces a relatively natural video with motion that closely matches the camera poses. (c) Object Trajectory: Both VideoComposer and\nMotionCtrl can generate an object moving along a given trajectory (red curve), but MotionCtrl more precisely follows it in each frame, as\nindicated by green points.\n7\nthe improved ObjMC score.\nCombination of Camera Motion and Object Motion.\nMotionCtrl can not only control camera and object motion\nindependently within a single video but also perform inte-\ngrated control of both. As demonstrated in Figure 1 (b) and\n(c), when MotionCtrl is applied with only a trajectory, it pri-\nmarily generates a swaying rose that follows this path. By\nfurther introducing zoom-out camera poses, both the rose\nand the background are animated in accordance with the\nspecified trajectory and camera movements. Additional re-\nsults can be found in the supplementary materials and\non the project page.\n4.3. Ablation Studies\nIn this section, we present experiments to validate the ef-\nfectiveness of the delicate designs in MotionCtrl, including\nthe integrated positioning of CMCM and LVDM, the train-\ning strategy for OMCM, and the sequence in which CMCM\nand OMCM are trained.\nIntegrated Position of Camera Motion Control Module\n(CMCM). We test implementing camera motion control by\ncombining camera poses with the time embedding, spatial\ncross-attention, or spatial self-attention module in LVDM.\nAlthough such methods have succeeded in other types of\ncontrolling [14, 15, 29], such as sketch, depth, and so on,\nthey fail to endow camera control capabilities to LVDM, as\nevidenced by the CamMC scores in Table 2, being close\nto LVDM without camera motion control. That is because\nthese components primarily focus on spatial content gener-\nation, which is largely insensitive to the camera motion en-\ncoded in camera poses. Conversely, incorporating CMCM\nwith LVDM\u2019s temporal transformers significantly improves\ncamera motion control, as indicated by a lower CamMC\nscore of 0.0289 in Table 2. Camera motion primarily causes\nglobal view transformations over time, and fusing camera\nposes into LVDM\u2019s temporal blocks aligns with this prop-\nerty, enabling effective camera motion control during video\ngeneration.\nFurthermore, our CMCM has a negligible impact on the\nquality of spatial content, with FID[22] and FVD[24] scores\ncomparable to LVDM, even though it is trained on the rel-\natively small Realestate10K dataset [33] with low diversity.\nIn contrast, injecting camera poses through time embedding\nor spatial transformers compromises quality scores, as it\ndisrupts spatial content creation.\nThe corresponding qualitative results are in the supple-\nmentary materials.\nDense Trajectories v.s. Sparse Trajectories. We first train\nOMCM with dense object movement trajectories extracted\nvia ParticleSfM[31] and then fine-tune with sparse trajec-\ntories. We evaluate the effectiveness of this approach by\ncomparing it with training OMCM solely on dense or sparse\ntrajectories. Table 3 indicates that training exclusively with\nMethod\nCamMC \u2193\nCLIPSIM \u2191\nFID \u2193\nFVD \u2193\nLVDM [9]\n0.9010\n0.2359\n130.62\n1007.63\nTime Embedding\n0.0887\n0.2361\n132.74\n1461.36\nSpatial Cross-Attention\n0.0857\n0.2357\n153.86\n1306.78\nSpatial Self-Attention\n0.0902\n0.2384\n146.37\n1303.58\nTemporal Transformer\n0.0289\n0.2355\n132.36\n1005.24\nTable 2. Ablation of Camera Motion Control. Our Camera\nMotion Control Module (CMCM), incorporated with the tempo-\nral transformers of LVDM [9], effectively controls camera motion\nand maintains LVDM\u2019s video quality. Conversely, other variants\nfail to control the camera motion and may reduce the LVDM\u2019s\ngenerative quality.\nMethod\nObjMC \u2193\nCLIPSIM \u2191\nFID \u2193\nFVD \u2193\nDense\n54.4114\n0.2352\n175.8622\n2227.87\nSparse\n34.6937\n0.2365\n158.5553\n2385.39\nDense + Sparse\n25.1198\n0.2342\n149.2754\n2001.57\nTable 3. Ablation of Object Motion Control. The Object Mo-\ntion Control Module (OMCM), when initially trained on dense\nobject movement trajectories and subsequently fine-tuned with\nsparse trajectories, outperforms versions trained exclusively on ei-\nther dense or sparse trajectories.\ndense trajectories yields inferior outcomes. This can be at-\ntributed to discrepancies between the training and inference\nphases (since only sparse trajectories are available during\ninference). While training solely with sparse trajectories\nshows improvement over the dense-only approach, it still\nfalls short of the hybrid method. Sparse trajectories alone\nprovide insufficient information for OMCM\u2019s learning. In\ncontrast, dense trajectories offer richer information that ac-\ncelerates learning, and subsequent fine-tuning with sparse\ntrajectories allows OMCM to adjust to the sparsity encoun-\ntered during inference.\nThe corresponding qualitative results are in the supple-\nmentary materials.\nTraining Strategy. Given the limitations of the available\ntraining dataset, we propose a multi-step training strategy\nfor MotionCtrl, starting with the Camera Motion Control\nModule (CMCM) using Realestate10K [33], followed by\nthe Object Motion Control Module (OMCM) with synthe-\nsized object movement trajectories. To thoroughly assess\nour approach, we experiment with reversing the order, train-\ning OMCM before CMCM. This sequence does not impact\ncamera motion control, as OMCM components do not par-\nticipate in CMCM training. However, it does lead to a de-\ncrease in object motion control performance. The subse-\nquent training of CMCM adjusts parts of LVDM\u2019s temporal\ntransformers, disrupting the object motion control adapta-\ntion achieved during OMCM\u2019s initial training (evidenced\nby a higher ObjCM score of 25.2712 compared to 25.1198\nattained with our proposed training sequence). Thus, our\nmulti-step strategy, though a compromise due to dataset\nconstraints, is deliberately structured to train CMCM be-\nfore OMCM, ensuring enhanced performance in both cam-\n8\nera and object motion control.\n5. Conclusion\nThis paper proposes MotionCtrl, a unified and flexible con-\ntroller that can independently or combinably control the\ncamera and object motion in a video attained with a video\ngeneration model. To achieve this end, MotionCtrl care-\nfully tailors a camera motion control module and object\nmotion control module to adapt to the specific properties of\ncamera motion and object motion and deploys a multi-step\ntraining strategy to train these two modules with delicately\naugmented datasets. Comprehensive experiments, includ-\ning qualitative and quantitative evaluations, showcase the\nsuperiority of our proposed MotionCtrl in both camera and\nobject motion control.\nAcknowledgements: We express our gratitude to Yaowei\nLi for his adaptation of our proposed MotionCtrl to Ani-\nmateDiff [8].\nReferences\n[1] Civitai: https://civitai.com/. 1\n[2] https://www.pika.art/. 2\n[3] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 2, 5, 6\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 2, 3\n[5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al.\nVideocrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023. 2, 3, 5\n[6] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image gen-\neration via transformers. NeurIPS, 2021. 2\n[7] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 2\n[8] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 6, 7, 9,\n1, 8\n[9] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation.\narXiv preprint arXiv:2211.13221,\n2023. 2, 3, 4, 5, 8, 1, 6, 7, 10\n[10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2, 3\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2, 6, 1\n[12] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5\n[13] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 2, 5\n[14] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation.\nIn\nCVPR, 2023. 8\n[15] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,\n4, 8\n[16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 6\n[17] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In ICML, 2021. 2\n[18] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[19] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 2, 3\n[20] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015. 3\n[21] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 2\n[22] Maximilian Seitzer.\npytorch-fid: FID Score for PyTorch.\nhttps://github.com/mseitzer/pytorch-fid,\n2020. 5, 6, 8\n[23] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n9\n[24] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 5, 6, 8\n[25] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2, 3, 6, 7\n[26] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 3, 2\n[27] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi\nLi, and Xiangyu Zhang.\nLamp:\nLearn a motion pat-\ntern for few-shot-based video generation.\narXiv preprint\narXiv:2310.10769, 2023. 3, 2\n[28] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023. 2, 3, 5, 6\n[29] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 2, 4, 8\n[30] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao\nZhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng\nShou.\nMotiondirector: Motion customization of text-to-\nvideo diffusion models. arXiv preprint arXiv:2310.08465,\n2023. 3, 2\n[31] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and\nYong-Jin Liu. Particlesfm: Exploiting dense point trajecto-\nries for localizing moving cameras in the wild. In ECCV,\n2022. 2, 5, 6, 8\n[32] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[33] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely.\nStereo magnification:\nLearning\nview synthesis using multiplane images.\narXiv preprint\narXiv:1805.09817, 2018. 2, 5, 6, 8\n[34] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,\nChris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and\nTong Sun. Lafite: Towards language-free training for text-to-\nimage generation. arXiv preprint arXiv:2111.13792, 2021. 2\n10\nMotionCtrl: A Unified and Flexible Motion Controller for Video Generation\nhttps://wzhouxiff.github.io/projects/MotionCtrl/\nSupplementary Material\nThe supplementary materials provide additional results\nachieved with our proposed MotionCtrl, along with in-\ndepth analyses.\nFor a more visual understanding, we\nstrongly recommend readers visit our project page for\nthe video results. The structure of the supplementary ma-\nterials is as follows:\n\u2022 More\nresults\nof\nMotionCtrl\nbased\non\nLVDM [9].(Section 6)\n\u2022 Results of MotionCtrl when extended to AnimateDiff [8]\nframework. (Section 7)\n\u2022 Visualized results and analyses of ablation studies. (Sec-\ntion 8)\n\u2022 More discussions about previous related works.\n(Sec-\ntion 9).\n\u2022 Details of evaluation datasets. (Section 10)\n6. More Results of MotionCtrl Deployed on\nLVDM [9]\nIn this section, we present additional results of our pro-\nposed MotionCtrl model deployed on LVDM [9], focusing\non camera motion control, object motion control, and com-\nbined motion control. Notably, all results are obtained\nusing the same trained MotionCtrl model, without the\nneed for extra fine-tuning for different camera poses or\ntrajectories.\nSpecifically, Figure 5 illustrates the outcomes of cam-\nera motion control of MotionCtrl guided by 8 basic cam-\nera poses, including pan up, pan down, pan left, pan right,\nzoom in, zoom out, anticlockwise rotation, and clockwise\nrotation. These poses are visualized in Figure 15 (a). This\ndemonstrates the capability of our MotionCtrl model to in-\ntegrate multiple basic camera motion controls in a unified\nmodel, contrasting with the AnimateDiff model [8] which\nrequires a distinct LoRA model [11] for each camera mo-\ntion.\nFigure 6 showcases the results of camera motion con-\ntrol using MotionCtrl, which is guided by relatively com-\nplex camera poses. These complex camera poses are dis-\ntinct from basic camera poses, as they include elements of\ncamera turning or self-rotation within the same camera pose\nsequence. The results demonstrate that, given a sequence of\ncamera poses, our MotionCtrl can generate natural videos.\nThe content of these videos aligns with the text prompts,\nand the camera motion corresponds to the provided com-\nplex camera poses. For additional results, please visit our\nproject page.\nFigure 7 presents the results of object motion control us-\ning MotionCtrl, guided by specific trajectories. When given\nthe same trajectories and different text prompts, MotionC-\ntrl can generate videos featuring different objects, but with\nidentical object motion, as demonstrated in the first four\nsamples of Figure 7.\nFurthermore, when provided with\nmultiple trajectories within the same video, MotionCtrl is\ncapable of controlling the motion of several objects simul-\ntaneously in the same generated video, as shown in the last\nsamples of Figure 7.\nFigure 8 provides the results of combining both the cam-\nera motion control and object motion control.\nWith the\nsame trajectory but different camera poses, the horse in the\ngenerated videos has a different performance.\n7. Results of MotionCtrl Deployed on Aimate-\nDiff [8]\nWe also deploy our MotionCtrl on AnimateDiff [8]. There-\nfore, we can control the motion of the video generated\nwith our fine-tuned AnimateDiff cooperating with various\nLoRA [11] models in the committee. The results in Fig-\nure 11, 12, 9, and 10 are generated resutls of our MontionC-\ntrl cooperating with different LoRA models provided by in\nCIVITAI [1]. They demonstrate that our MotionCtrl also\nworks well in camera motion control and object motion con-\ntrol. Related results are also provided on the project page.\n8. More Results and Analyses of Ablation\nStudies\nThe manuscript includes numerous ablation studies and de-\ntailed analyses. This section provides qualitative results of\nthe ablation studies concerning the integrated position of the\nCamera Motion Control Module (CMCM) and the training\nstrategy of the Object Motion Control Module (OMCM).\nSpecifically, Figure 13 demonstrates the generated re-\nsults of LVDM[9] and our MotionCtrl deployed on LVDM,\nimplemented by integrating CMCM with different compo-\nnents in LVDM. These components include the time em-\nbedding block, the cross-attention or self-attention module\nin the spatial transformers, and temporal transformers (ac-\ncepted in our final MotionCtrl). The results indicate that\nMotionCtrl fails to control the camera motion of the gener-\nated video, except when integrating CMCM with the tempo-\nral transformers. This is primarily because camera motion\nmainly leads to global and temporal movement in the video.\nThe camera poses integrated into the temporal transformer\n1\ncan effectively fuse the camera movement information into\nthe T2V generation model and yield camera motion.\nFigure 14 illustrates the qualitative outcomes of the ab-\nlation study \u201dDense Trajectories vs. Sparse Trajectories\u201d.\nThe results reveal that the Object Motion Control Module\n(OMCM) in MotionCtrl, when trained with dense trajec-\ntories, fails to control the object motion in the generated\nvideo. This is primarily due to the disparity between the\ndense trajectories in the training phase and sparse trajecto-\nries in the inference phase. Furthermore, the model trained\non dense trajectories, followed by a fine-tuning phase on\nsparse trajectories, demonstrates superior precision in ob-\nject motion control compared to the model trained exclu-\nsively on sparse trajectories. This is largely because sparse\ntrajectories are too scattered to be effectively learned, and\na model with dense trajectories can provide a more optimal\nstarting point for the learning of sparse trajectories.\n9. More Discussions about the Related Works\nTo further illustrate the advantages of our proposed Mo-\ntionCtrl, we\u2019ve conducted a comparative analysis with pre-\nvious related works. The comparisons are detailed in Ta-\nble 4. Models such as AnimateDiff[8] (refers to the motion\ncontrol LoRA models provided by AnimateDiff), Tune-a-\nvideo[26], LAMP[27], and MotionDirector[30] implement\nmotion control by extracting motion from one or multiple\ntemplate videos. This approach necessitates the training of\ndistinct models for each template video or template video\nset. Moreover, the motions these methods learned are solely\ndetermined by the template video(s), and they fail to differ-\nentiate between camera motion and object motion. Simi-\nlarly, MotionDirector[30] and VideoComposer[25], despite\nachieving motion control with a unified model guided by\nmotion vectors and trajectories respectively, do not distin-\nguish between camera motion and object motion. In con-\ntrast, our proposed MotionCtrl, utilizing a unified model,\ncan independently and flexibly control a wide range of cam-\nera and object motions in the generated videos.\nThis is\nachieved by guiding the model with camera poses and tra-\njectories respectively, offering a more fine-grained control\nover the video generation process.\n10. Details of Evaluation Datasets\nIn this paper, we construct two evaluation datasets to inde-\npendently evaluate the efficacy of our proposed MotionCtrl\non camera motion control and object motion control.\nCamera motion control evaluation dataset. This dataset\ncontains a total of 280 samples. It consists of 8 basic cam-\nera poses, 20 complex camera poses, and 10 text prompts.\nThe basic camera poses include pan left, pan right, pan up,\npan down, zoom in, zoom out, anticlockwise rotation, and\nclockwise rotation. The complex camera poses are derived\nfrom the testset of Realestate10K [33]. Visualizations of\nthese poses are provided in Figure 15, demonstrating the\ndataset\u2019s diverse range of camera poses. Besides, The text\nprompts are listed below:\n\u2022 A train travels along a railway through a valley.\n\u2022 Rocky coastline with crashing waves.\n\u2022 Effiel Tower in Paris.\n\u2022 City of Venice, with buildings, river, and boats.\n\u2022 A pyramid in the desert.\n\u2022 A castle in the forest.\n\u2022 A villa in a garden.\n\u2022 A fish is swimming in the aquarium tank.\n\u2022 A zebra next to a river.\n\u2022 A massive, multi-tiered elven palace adorned with flow-\ning waterfalls, its cascades forming staircases between\nethereal realms.\nObject Motion Control Evaluation Dataset. This eval-\nuation dataset consists of 19 diverse trajectories and 12\ntext prompts. The trajectories, which include straight lines,\ncurves, shaking lines, and combinations thereof, are illus-\ntrated in Figure 16. The text prompts are provided below:\n\u2022 A man skateboarding\n\u2022 A man surfing.\n\u2022 A girl skiing.\n\u2022 The rose swaying in the wind.\n\u2022 The sunflower swaying in the wind.\n\u2022 Two zebras walking on the grass.\n\u2022 Two horses walking on the grass.\n\u2022 A train passing by the valley.\n\u2022 A car running on Mars.\n\u2022 A horse running on Mars.\n\u2022 A cute dog sitting on the green grass.\n\u2022 A feather floating in the air.\nPlease note that the evaluation datasets we have con-\nstructed are primarily used for quantitatively assessing\nthe performance of our proposed MotionCtrl in both\ncamera and object motion control in video generation.\nOur MotionCtrl is capable of handling a wider variety\nof camera poses and trajectories that are not included in\nthe evaluation datasets.\n2\nMethod\nRequire Fine-tuning\nMotion sources\nDistinguish Camera & Object Motion\nAnimateDiff [8]\n!\ntemplate videos\n%\nTune-a-video [26]\n!\ntemplate video\n%\nLAMP [27]\n!\ntemplate videos\n%\nMotionDirector [30]\n!\ntemplate videos\n%\nVideoComposer [25]\n%\nmotion vectors\n%\nDragNUWA [28]\n%\ntrajectories\n%\nMotionCtrl (Ours)\n%\ncamera poses & trajectories\n!\nTable 4. Differences between our proposed MotionCtrl and related works. Unlike AnimateDiff [8] (which refers to the motion\ncontrol LoRA model provided by AnimateDiff), Tune-a-video [26], LAMP [27], and MotionDirector [30] that implement motion control\nby extracting motion from one or a series of template videos and require different models for different template videos, our proposed\nMotionCtrl uses a unified model. Besides, the motions learned by these methods are determined by the template video(s) and they do not\ndistinguish between camera motion and object motion. On the other hand, although MotionDirector [30] and VideoComposer [25] achieve\nmotion control with a unified model guided by motion vectors and trajectories, respectively, they also do not distinguish between camera\nmotion and object motion. In contrast, our proposed MotionCtrl, with a unified model, can independently and flexibly control the camera\nmotion and object motion of the generated video, guided by camera poses and trajectories, respectively.\n3\nPan Up\nPan Down\nPan Left\nPan Right\nZoom In\nZoom Out\nAntiClockwise\nClockise\nPrompt: A landscape with mountains and lake at sunrise.\nFigure 5. The results of our proposed MotionCtrl deployed on LVDM [9], guided by 8 basic camera poses: pan up, pan down, pan left, pan\nright, zoom in, zoom out, anticlockwise rotation, and clockwise rotation (The visualization of these camera poses can be seen in Figure 15\n(a)). It\u2019s important to note that all results are achieved using the same MotionCtrl model, without the need for extra fine-tuning for\ndifferent camera poses.\n4\nPrompt: A cute cat lying on the floor.\nPrompt: A temple on the mountain.\nPrompt: A temple on the mountain.\nPrompt: A human robot standing on Mars.\nFigure 6. The results of our proposed MotionCtrl deployed on LVDM [9], guided by relatively complex camera poses. Unlike basic\ncamera poses, which only involve simple directional movements, these complex camera poses incorporate elements of camera\nturning or self-rotation within the same camera pose sequence. The camera motion in the generated videos closely follows the guided\ncamera poses, while the generated content aligns with the text prompts. For additional results, please visit our project page.\n5\nPrompt: A chime in the wind.\nPrompt: A sunflower in the wind.\nPrompt: A paper plane floating in the sky.\nPrompt: A leaf floating in the sky.\nPrompt: Two zebras.\nPrompt: Two cats.\nFigure 7. The result of our proposed MotionCtrl deployed on LVDM [9], guided with trajectories. The green points in the trajectories\nindicate the starting points. Given the same trajectories, our model can generate different objects in accordance with the text prompts,\nmaintaining the same object motion. When multiple trajectories are present in the same video, our model is capable of simultaneously\ncontrolling the motion of different objects within the same generated video.\n6\nPrompt: A horse running on the road.\nPan Left\nZoom In\nFigure 8. The result of combining camera motion and object motion control of MotionCtrl deployed on LVDM [9]. With the same trajectory\nbut different camera poses, the horse in the generated videos has a different performance.\nPrompt: A girl.\nFigure 9. The camera motion control results of MotionCtrl deployed on AnimateDiff [8]. They are guided with relatively complex camera\nposes.\nPrompt: : A teddy bear skateboarding.\nFigure 10. The object motion control results of MotionCtrl deployed on AnimateDiff [8].\n7\nPan Up\nPan Down\nPan Left\nPan Right\nZoom In\nZoom Out\nAntiClockwis\ne\nClockise\nPrompt: A teddy bear in the supermarket.\nFigure 11. The camera motion control results of MotionCtrl deployed on AnimateDiff [8]. They are guided with 8 basic camera poses.\n8\n1.0 speed\n2.0 speed\n3.0 speed\n5.0 speed\nPrompt: Catle on the mountain.\n1.0 speed\n2.0 speed\n3.0 speed\n5.0 speed\nZoom Out\nZoom In\nFigure 12. The camera motion control results of MotionCtrl deployed on AnimateDiff [8]. Our MotionCtrl can not only control the camera\nmotion of the generated videos but also their motion speed.\n9\nTime Embedding\nLVDM\nSpatial CrossAttn\nSpatial SelfAttn\nTemporal \nTransformer \nPrompt: A fish is swimming in the aquarium tank.\nFigure 13. The qualitative results of ablation study regarding the integrated position of the Camera Motion Control Module\n(CMCM) with LVDM [9]. Integrating CMCM of MotionCtrl with the temporal transformers in LVDM significantly improves camera\nmotion control compared to other setups.\nDense + Sparse\nSparse\nDense\nPrompt: A man is surfing.\nFigure 14. The qualitative results of ablation study \u201dDense Trajectories v.s. Sparse Trajectories\u201d. The model trained with dense\ntrajectories fails to control the object motion in the generated video. Conversely, the model trained on dense trajectories, followed by\nfine-tuning on sparse trajectories, exhibits superior precision in object motion control compared to the model trained solely on sparse\ntrajectories.\n10\nPan Left\nPan Right\nPan Up\nPan Down\nZoom In\nZoom Out\nClockwise\nAnticlockwise\n(a) 8 Basic Camera Poses\nRealestate10K\n(b) 20 Complex Camera Poses from Realestate10K Testset\nFigure 15. The Camera Motion Control Evaluation Dataset consists of 8 basic camera poses and 20 complex camera poses, with the\ncomplex poses being derived from the test set of Realestate10K. This dataset is utilized to quantitatively assess the effectiveness of our\nproposed MotionCtrl in controlling a wide range of diverse camera motions in videos generated.\n11\nFigure 16. The Object Motion Control Evaluation Dataset encompasses 19 trajectories, where the green and blue points respectively\nrepresent the starting and ending points of each trajectory. This dataset is used to quantitatively evaluate the effectiveness of the proposed\nMotionCtrl in controlling object movements in videos generated.\n12\n"
  },
  {
    "title": "Cache Me if You Can: Accelerating Diffusion Models through Block Caching",
    "link": "https://arxiv.org/pdf/2312.03209.pdf",
    "upvote": "16",
    "text": "Cache Me if You Can:\nAccelerating Diffusion Models through Block Caching\nFelix Wimbauer1,2,3\nBichen Wu1\nEdgar Schoenfeld1\nXiaoliang Dai1\nJi Hou1\nZijian He1\nArtsiom Sanakoyeu1\nPeizhao Zhang1\nSam Tsai1\nJonas Kohler1\nChristian Rupprecht4\nDaniel Cremers2,3\nPeter Vajda1\nJialiang Wang1\n1Meta GenAI\n2Technical University of Munich\n3MCML\n4University of Oxford\nfelix.wimbauer@tum.de\njialiangw@meta.com\nFigure 1. Speeding up diffusion models through block caching. We observe that there are many redundant layer computations at\ndifferent timesteps in diffusion models when generating an image. Our block caching technique allows us to avoid these unnecessary\ncomputations, therefore speeding up inference by a factor of 1.5x-1.8x while maintaining image quality. Compared to the standard practice\nof naively reducing the number of denoising steps to match our inference speed, our approach produces more detailed and vibrant results.\nAbstract\nDiffusion models have recently revolutionized the field of\nimage synthesis due to their ability to generate photorealis-\ntic images. However, one of the major drawbacks of diffu-\nsion models is that the image generation process is costly. A\nlarge image-to-image network has to be applied many times\nto iteratively refine an image from random noise. While\nmany recent works propose techniques to reduce the num-\nber of required steps, they generally treat the underlying\ndenoising network as a black box. In this work, we investi-\ngate the behavior of the layers within the network and find\nthat 1) the layers\u2019 output changes smoothly over time, 2) the\nlayers show distinct patterns of change, and 3) the change\nfrom step to step is often very small. We hypothesize that\nmany layer computations in the denoising network are re-\nThis work was done during Felix\u2019 internship at Meta GenAI.\ndundant. Leveraging this, we introduce block caching, in\nwhich we reuse outputs from layer blocks of previous steps\nto speed up inference. Furthermore, we propose a tech-\nnique to automatically determine caching schedules based\non each block\u2019s changes over timesteps. In our experiments,\nwe show through FID, human evaluation and qualitative\nanalysis that Block Caching allows to generate images with\nhigher visual quality at the same computational cost. We\ndemonstrate this for different state-of-the-art models (LDM\nand EMU) and solvers (DDIM and DPM).\n1. Introduction\nRecent advances in diffusion models have revolutionized\nthe field of generative AI. Such models are typically pre-\ntrained on billions of text-image pairs, and are commonly\nreferred to as \u201cfoundation models\u201d. Text-to-image founda-\ntion models such as LDM [41], Dall-E 2/3 [2, 38], Ima-\n1\narXiv:2312.03209v2  [cs.CV]  12 Jan 2024\ngen [43], and Emu [8] can generate very high quality, pho-\ntorealistic images that follow user prompts. These founda-\ntion models enable many downstream tasks, ranging from\nimage editing [4, 17] to synthetic data generation [20], to\nvideo and 3D generations [34, 46].\nHowever, one of the drawbacks of such models is their\nhigh latency and computational cost. The denoising net-\nwork, which typically is a U-Net with residual and trans-\nformer blocks, tends to be very large in size and is repeat-\nedly applied to obtain a final image. Such high latency pro-\nhibits many applications that require fast and frequent infer-\nences. Faster inference makes large-scale image generation\neconomically and technically viable.\nThe research community has made significant efforts to\nspeed up image generation foundation models. Many works\naim to reduce the number of steps required in the denoising\nprocess by changing the solver [10, 27, 28, 45, 61]. Other\nworks propose to distill existing neural networks into archi-\ntectures that require fewer steps [44] or that can combine the\nconditional and unconditional inference steps [31]. While\nimproved solvers and distillation techniques show promis-\ning results, they typically treat the U-Net model itself as\na black box and mainly consider what to do with the net-\nwork\u2019s output. This leaves a potential source of speed up\u2014\nthe U-Net itself\u2014completely untapped.\nIn this paper, we investigate the denoising network in-\ndepth, focusing on the behavior of attention blocks. Our\nobservations reveal that: 1) The attention blocks change\nsmoothly over denoising steps.\n2) The attention blocks\nshow distinct patterns of change depending on their posi-\ntion in the network. These patterns are different from each\nother, but they are consistent irrespective of the text inputs.\n3) The change from step to step is typically very small in the\nmajority of steps. Attention blocks incur the biggest compu-\ntational cost of most common denoising networks, making\nthem a prime target to reduce network latency.\nBased on these observations, we propose a technique\ncalled block caching.\nOur intuition is that if a layer\nblock does not change much, we can avoid recomputing\nit to reduce redundant computations. We extend this by a\nlightweight scale-shift alignment mechanism, which pre-\nvents artifacts caused by naive caching due to feature mis-\nalignment. Finally, we propose an effective mechanism to\nautomatically derive caching schedules.\nWe analyse two different models: a retrained version of\nLatent Diffusion Models [41] on Shutterstock data, as well\nas the recently proposed EMU [8], as can be seen in Fig. 1.\nFor both, we conduct experiments with two popular solvers:\nDDIM [48] and DPM [27]. For all combinations, given\na fixed computational budget (inference latency), we can\nperform more steps with block caching and achieve better\nimage quality. Our approach achieves both improved FID\nscores and is preferred in independent human evaluations.\n2. Related Work\nIn the following, we introduce important works that are re-\nlated to our proposed method.\nText-to-Image Models.\nWith recent advances in genera-\ntive models, a vast number of text-conditioned models for\nimage synthesis emerged.\nStarting out with GAN-based\nmethods [14, 24, 35, 36, 40, 51, 53, 54, 58, 59, 64], re-\nsearchers discovered important techniques such as adding\nself-attention layers [60] for better long-range dependency\nmodeling and scaling up to very large architectures [3, 21].\nDifferent autoencoder-based methods [16, 39], in particular\ngenerative transformers [5, 7, 12, 37], can also synthesize\nnew images in a single forward pass and achieve high vi-\nsual quality. Recently, the field has been dominated by dif-\nfusion models [47\u201349]. Advances such as classifier guid-\nance [9], classifier-free guidance [18, 32], and diffusion in\nthe latent space [41] have enabled modern diffusion models\n[1, 6, 8, 13, 32, 38, 41, 43, 55] to generate photorealistic\nimages at high resolution from text. However, this superior\nperformance often comes at a cost: Due to repeated appli-\ncations of the underlying denoising neural network, image\nsynthesis with diffusion models is very computationally ex-\npensive. This not only hinders their widespread usage in\nend-user products, but also slows down further research. To\nfacilitate further democratization of diffusion models, we\nfocus on accelerating diffusion models in this work.\nImproved Solvers.\nIn the diffusion model framework, we\ndraw a new sample at every step from a distribution deter-\nmined by the previous steps. The exact sampling strategy,\ndefined by the so-called solver, plays an important role in\ndetermining the number of steps we have to make to ob-\ntain high-quality output. Starting out from the DDPM [19]\nformulation, DDIM [48] introduced implicit probabilistic\nmodels.\nDDIM allows the combination of DDPM steps\nwithout retraining and is popular with many current mod-\nels. The DPM-Solver [27, 28] models the denoising process\nas an ordinary differential equation and proposes a dedi-\ncated high-order solver for diffusion ODEs.\nSimilar ap-\nproaches are adopted by [22, 25, 61\u201363]. Another line of\nworks [10, 11, 23, 45, 52] proposed to train certain parts\nof the solver on a dataset. While better solvers can help\nto speed up image synthesis by reducing the number of re-\nquired steps, they still treat the underlying neural network\nas a black box. In contrast, our work investigates the inter-\nnal behavior of the neural network and gains speed up from\ncaching. Therefore, the benefits of improved solvers and\nour caching strategy are not mutually exclusive.\nDistillation.\nDistillation techniques present an alternative\nway to speed up inference. Here, a pretrained teacher net-\n2\nX\nEarly block\nDeeper block\n1\n20\nInitial noise\nFinal Image\nDenoising U-Net\nStep 1\nStep 2\nStep 3\nStep 20\nScale\nShift\nScale\nShift\nSolver steps\n...\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\n(a) Layer development during denoising.\n(b) Per step change for all spatial transformer blocks (L1rel).\nFigure 2. Overview. We observe, that in diffusion models, not only the intermediate results x, but also the internal feature maps change\nsmoothly over time. (a) We visualize output feature maps of two layer blocks within the denoising network via PCA. Structures change\nsmoothly at different rates. (b) We also observe this smooth layer-wise change when plotting the change in output from one step to the\nnext, averaging over many different prompts and randomly initialized noise. Besides the average, we also show the standard deviation as\nshaded area. The patterns always remain the same. Configuration: LDM-512, DPM, 20 Steps.\nwork creates new training targets for a student architec-\nture, that needs fewer neural function evaluations than the\nteacher. Guidance distillation [31] replaces the two function\nevaluations of classifier-free guidance with a single one,\nwhile progressive distillation [44] reduces the number of\nsampling steps. [29] optimizes a student to directly predict\nthe image generated by the teacher in one step.\nConsistency models [30, 50] use a consistency formula-\ntion enabling a single-step student to do further steps. Fi-\nnally, [56] distill a large teacher model into a much smaller\nstudent architecture. However, distillation does not come\nwithout cost.\nApart from the computational cost of re-\ntraining the student model, some distillation techniques can-\nnot handle negative or composite prompts [26, 31]. In this\npaper, we introduce a lightweight fine-tuning technique in-\nspired by distillation, that leaves the original parameters un-\nchanged while optimizing a small number of extra parame-\nters without restricting the model.\n3. Method\nIn this work, we investigate the behavior of the different lay-\ners in the diffusion U-Net to develop novel ways of speed-\ning up the image generation process. The main insight of\nour method is that large latent diffusion models contain re-\ndundant computations that can be recycled between steps\nwithout compromising image quality. The key to our ap-\nproach is to cache the outputs of U-Net blocks to be reused\nin the remaining diffusion steps.\n3.1. Preliminaries\nIn the diffusion model framework, we start from an input\nimage x0 \u2208 [\u22121, 1]3\u00d7H\u00d7W . For a number of timesteps\nt \u2208 [1, T], we repeatedly add Gaussian noise \u03f5t \u223c N to the\nimage, to gradually transform it into fully random noise.\nxt = xt\u22121 + \u03f5t\n(1)\nxT \u223c N(0, 1)\n(2)\nTo synthesize novel images, we train a neural network\n\u03a8(xt, t) to gradually denoise a random sample xT . The\nneural network can be parameterized in different ways to\npredict x0, \u03f5t or \u2207 log(xt) [49]. A solver \u03a6 determines\nhow to exactly compute xt\u22121 from the output of \u03a8 and t.\nxt\u22121 = \u03a6 (xt, t, \u03a8 (xt, t))\n(3)\nThe higher the number of steps is, the higher the visual\nquality of the image generally becomes. Determining the\nnumber of steps presents users with a trade-off between im-\nage quality and speed.\n3.2. Analysis\nOne of the key limitations of diffusion models is their slow\ninference speed. Existing works often propose new solvers\nor to distill existing models, so that fewer steps are required\nto produce high-quality images. However, both of these di-\nrections treat the given neural network as a black box.\n3\nBaseline - 14 Steps\nA magical portal opening to reveal a\nhidden realm of wonders.\nBlock Caching - 20 Steps\nA tranquil garden with cherry blossoms\nin full bloom under a full moon.\nAn ancient castle on a cliff overlooking\na vast, mist-covered valley.\nA yellow tiger with blue stipres.\nBaseline - 14 Steps\nBlock Caching - 20 Steps\nA time-traveling wizard riding a\nmechanical steed through a portal,\nleaving trails of stardust in their wake.\nA floating city in the clouds where airships\nnavigate through tunnels of light, and\nmajestic creatures soar in the skies.\nFigure 3. Qualitative Results for EMU-768. With identical inference speed, our caching technique produces finer details and more vibrant\ncolors. For more results refer to the supplementary material. Configuration: DPM, Block caching with 20 steps vs Baseline with 14 steps.\nIn this paper, we move away from the \u201cblack box per-\nspective\u201d and investigate the internal behavior of the neu-\nral network \u03a8 to understand it at a per-layer basis. This is\nparticularly interesting when considering the temporal com-\nponent. To generate an image, we have to perform multiple\nforward passes, where the input to the network changes only\ngradually over time.\nThe neural network \u03a8 generally consists of multiple\nblocks of layers Bi(xi, si), i \u2208 [0, N \u2212 1], where N is\nthe number of all blocks of the network, x is the output of\nan earlier block and s is the optional data from a skip con-\nnection. The common U-Net architecture [42], as used in\nmany current works [8, 33, 41], is made up of ResBlocks,\nSpatialTransformer blocks, and up/downsampling\nblocks. ResBlocks mostly perform cheap convolutions,\nwhile SpatialTransformer blocks perform self- and\ncross-attention operations and are much more costly.\nA common design theme of such blocks is that they rely\non residual connections. Instead of simply passing the re-\nsults of the layer computations to the next block, the result\nis combined with the original input of the current block via\nsummation. This is beneficial, as it allows information (and\ngradients) to flow more freely through the network [15].\nRather than replacing the information, a block changes the\ninformation that it receives as input.\nBi(x, s) = Ci(x, s) + concat(x, s)\n(4)\nCi(x, s) = layersi(concat(x, s))\n(5)\nTo better understand the inner workings of the neural net-\nwork, we visualize how much the changes the block applies\nto the input vary over time. Concretely, we consider two\nmetrics: Relative absolute change L1rel.\nL1rel(i, t) = ||Ci(xt, st) \u2212 Ci(xt\u22121, st\u22121)||1\n||Ci(xt, st)||1\n(6)\nTo get representative results, we generate 32 images\nfrom different prompts with 2 random seeds each and re-\nport the averaged results in Fig. 2. Further, we visualize\nselected feature maps. We make three key observations:\n1) Smooth change over time. Similarly to the interme-\ndiate images during denoising, the blocks change smoothly\n4\n18\n16\n14\n12\n10\n8\n6\n4\n2\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 4\nInput block 5\nInput block 7\nInput block 8\nMiddle block \nMiddle block \nOutput block 3\nOutput block 4\nOutput block 5\nOutput block 6\nOutput block 7\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nBlocks\nFigure 4.\nCaching Schedule for LDM-512 at 20 steps with\nDPM. Each arrow represents the cache lifetime of a spatial trans-\nformer block. For the duration of an arrow, the spatial transformer\nblock reuses the cached result computed at the beginning of the\narrow. E.g., Input block 1 only computes the result at step 1, 6, 10,\n14 and 18 and uses the cached value otherwise.\nand gradually over time. This suggests that there is a clear\ntemporal relation between the outputs of a block.\n2) Distinct patterns of change. The different blocks\ndo not behave uniformly over time. Rather, they apply a\nlot of change in certain periods of the denoising process,\nwhile they remain inactive in others. The standard deviation\nshows that this behavior is consistent over different images\nand random seeds. Note that some blocks, for example the\nblocks at higher resolutions (either very early or very late\nin the network) change most in the last 20%, while deeper\nblocks at lower resolutions change more in the beginning.\n3) Small step-to-step difference. Almost every block\nhas significant periods during the denoising process, in\nwhich its output only changes very little.\n3.3. Block Caching\nWe hypothesize that a lot of blocks are performing redun-\ndant computations during steps where their outputs change\nvery little. To reduce the amount of redundant computations\nand to speed up inference, we propose Block Caching.\nInstead of computing new outputs at every step, we reuse\nthe cached outputs from a previous step. Due to the nature\nof residual connections, we can perform caching at a per-\nblock level without interfering with the flow of information\nthrough the network otherwise. We can apply our caching\ntechnique to almost all recent diffusion model architectures.\nOne of the major benefits of Block Caching compared to\napproaches that reduce the number of steps is that we have a\nmore finegrained control over where we save computation.\nWhile we perform fewer redundant computations, we do not\n...\nStudent\nTeacher\nStudent\nStudent\nTeacher\nFigure 5. Scale Shift Optimization. The student network copies\nand freezes the weights of the teacher and has additional scale\nand shift parameters per block. These parameters are optimized\nto match the teacher output per block and step.\nreduce the number of steps that require a lot of precision\n(i.e. where the change is high).\nAutomatic cache schedule.\nNot every block should be\ncached all the time. To make a more informed decision\nabout when and where to cache, we rely on the metric de-\nscribed in Sec. 3.2. We first evaluate these metrics over a\nnumber of random prompts and seeds. Our intuition is that\nfor any layer block i, we retain a cached value, which was\ncomputed at time step ta, as long as the accumulated change\ndoes not exceed a certain threshold \u03b4. Once the threshold is\nexceeded at time step tb, we recompute the block\u2019s output.\ntb\u22121\nX\nt=ta\nL1rel(i, t) \u2264 \u03b4 <\ntb\nX\nt=ta\nL1rel(i, t)\n(7)\nWith a lower threshold, the cached values will be refreshed\nmore often, whereas a higher threshold will lead to faster\nimage generation but will affect the appearance of the image\nmore. The threshold \u03b4 can be picked such that it increases\ninference speed without negatively affecting image quality.\n3.4. Scale-Shift Adjustment\nWhile caching already works surprisingly well on its own,\nas shown in Sec. 4.2, we observe that aggressive caching\ncan introduce artifacts into the final image. We hypothesize\nthat this is due to a misalignment between the cached fea-\nture map and the \u201coriginal\u201d feature map at a given timestep.\nTo enable the model to adjust to using cached values, we\nintroduce a very lightweight scale-shift adjustment mech-\nanism wherever we apply caching. To this end, we add a\ntimestep-dependent scalar shift and scale parameter for each\nlayer that receives a cached input. Concretely, we consider\nevery channel separately, i.e. for a feature map of shape\n(N \u00d7 C \u00d7 H \u00d7 W), we predict a vector of shape (N \u00d7 C)\nfor both scale and shift. This corresponds to a simple linear\nlayer that receives the timestep embedding as input.\nWe optimize scale and shift on the training set while\nkeeping all other parameters frozen. However, optimization\nof these additional parameters is not trivial. As we require\n5\nNaive Caching - 20 Steps\nBaseline - 20 Steps\nBaseline - 14 Steps\nBlock Caching - 20 Steps\nClose up black and white\nphotograph of an old dog\nHigh quality photograph of a female\nastronaut from a scifi movie\nA single rose in a rustic garden\nFigure 6. Qualitative Results for LDM-512. Our method often provides richer colors and finer details. Through our scale-shift adjustment,\nwe avoid artifacts that are visible when naively applying block caching. More qualitative results for DPM and DDIM can be found in the\nsupplementary material. Configuration: DPM, Block caching with 20 steps vs Baseline with 14 steps.\nvalid cached values, we cannot directly add noise to an im-\nage and train the network to denoise to the original image.\nTherefore, we rely on an approach, shown in Fig. 5,\nthat is inspired by distillation techniques. Our model with\ncaching enabled acts as the student, while the same model\nwith caching disabled acts as the teacher. We first unroll the\nconsecutive steps of the denoising process for the student\nconfiguration and generate an image from complete noise.\nThen, we perform a second forward pass at every timestep\nwith the teacher configuration, which acts as the training\ntarget. Note that for the teacher, we use the intermediate\nsteps from the student\u2019s trajectory as input rather than un-\nrolling the teacher. Otherwise, the teacher might take a dif-\nferent trajectory (leading to a different final output), which\nthen is not useful as a training target.\nThis optimization is very resource-friendly, as the\nteacher and student can use the same weights, saving GPU\nmemory, and we only optimize a small number of extra\nparameters, while keeping the parameters of the original\nmodel the same. During inference, the multiplication and\naddition with scale and shift parameters have no noticeable\neffect on the inference speed but improve image quality as\nshown in Sec. 4.2.\n4. Experiments\nIn the following, we first demonstrate the general potential\nof our Block Caching technique and then analyze it in more\ndetail through several ablation studies.\n4.1. Experimental Setup\nOur proposed method is general and can be applied to most\nrecent diffusion models. In order to give a good overview,\nwe conduct our experiments mainly on two models that rep-\nresent light and heavy computational demands:\n\u2022 LDM-512 [41], a popular diffusion model with 900M pa-\nrameters, that generates images at a 512\u00d7512 resolution,\nretrained on internal Shutterstock images.\n\u2022 EMU-768 [8], a state-of-the-art model with 2.7B param-\neters, which can produce photorealistic images at a reso-\nlution of 768 \u00d7 768.\n6\nFor both models, we use classifier-free guidance [18]\nwith a guidance strength of 5.0 and do not use any other\nperformance-enhancing techniques.\nWe run inference in\nbfloat16 type and measure the latency on a single Nvidia\nA100 GPU. For the optimization of the scale-shift adjust-\nment parameters, we perform 15k training iterations on\neight A100 GPUs. Depending on the model and the number\nof denoising steps, this takes between 12 and 48 hours.\n4.2. Accelerating Inference through Caching\nOur proposed caching technique can be viewed from two\nperspectives: 1) Given a fixed number of steps, caching al-\nlows us to accelerate the image generation process without\ndecreasing quality. 2) Given a fixed computational budget,\nwe can perform more steps when using caching, and there-\nfore obtain better image quality than performing fewer steps\nwithout caching.\nTo demonstrate the flexibility of our approach, we con-\nsider two common inference settings: (i) Many approaches\nperform 50 denoising steps by default. Therefore, we apply\ncaching with 50 solver steps and achieve the same latency\nas the 30 steps of the baseline model. (ii) By using modern\nsolvers like DPM [27] or DDIM [48], it is possible to gen-\nerate realistic-looking images with as few as 20 steps. If we\napply caching with 20 solver steps, we can reduce the infer-\nence latency to an equivalent of performing 14 steps with\nthe non-cached baseline model.\nAnalysis of LDM-512.\nWe begin by performing a thor-\nough qualitative and quantitative analysis of the LDM-512\nmodel. After computing the layer block statistics for the au-\ntomatic cache configuration, we find that a change thresh-\nold of \u03b4 = 0.5 gives us the desired speedup. The resulting\ncaching schedule is visualized in Fig. 4. As can be observed\nin the plots with relative feature changes (Fig. 2), we can\naggressively cache the early and late blocks. On the other\nhand, the activations of the deeper blocks change faster, es-\npecially in the first half of the denoising process, and should\ntherefore only be cached conservatively.\nThe results in Tab. 1 demonstrate that for both DPM and\nDDIM, the proposed caching with 20 steps significantly\nimproves the FID value compared to the 14-step baseline,\nwhile being slightly faster. Similarly, 50 steps with caching\noutperforms the 30-step baseline, while maintaining a com-\nparable latency. Moreover, our scale-shift adjustment mech-\nanism further enhances the results. Notably, this full config-\nuration even outperforms the 20-step and 50-step baselines.\nWe hypothesize that caching introduces a slight momentum\nin the denoising trajectory due to the delayed updates in\ncached values, resulting in more pronounced features in the\nfinal output image.\nQualitative results can be seen in Fig. 6. Our full model\n(caching + scale-shift adjustment) produces more crisp and\nSolver\nSteps Caching SS FID \u2193 Img/s \u2191 Speedup \u2191\nDPM\n[27]\n20\n17.15\n2.17\n1.00\u00d7\n14\n18.67\n3.10\n1.43\u00d7\n20\n\u2713\n17.58\n3.64\n1.68\u00d7\n20\n\u2713\n\u2713\n15.95\n3.59\n1.65\u00d7\nDDIM\n[48]\n20\n17.43\n2.17\n1.00\u00d7\n14\n17.11\n3.10\n1.43\u00d7\n20\n\u2713\n16.52\n3.48\n1.60\u00d7\n20\n\u2713\n\u2713\n16.02\n3.45\n1.58\u00d7\nDPM\n[27]\n50\n17.44\n0.87\n1.00\u00d7\n30\n17.21\n1.46\n1.67\u00d7\n50\n\u2713\n17.23\n1.61\n1.85\u00d7\n50\n\u2713\n\u2713\n15.18\n1.59\n1.82\u00d7\nDDIM\n[48]\n50\n17.76\n0.87\n1.00\u00d7\n30\n17.42\n1.46\n1.67\u00d7\n50\n\u2713\n16.65\n1.59\n1.82\u00d7\n50\n\u2713\n\u2713\n15.15\n1.56\n1.79\u00d7\nTable 1. LDM-512 FID and Throughput Measurements. For\ndifferent solvers, we test our caching technique against baselines\nwith 1) the same number of steps or 2) the same latency. In all\ncases, our proposed approach achieves significant speedup while\nimproving visual quality as measured by FID on a COCO subset\nremoving all faces (for privacy reasons). Legend: SS = Scale-shift\nadjustment, Img/s. = Images per second.\nSolver\nSteps (Img/s)\nVotes (in %)\nCaching\nBaseline\nWin\nTie\nLose\nDPM\n20 (0.28)\n14 (0.27)\n34.7\n36.9\n28.4\nDDIM\n20 (0.28)\n14 (0.27)\n28.0\n48.8\n23.2\nDPM\n50 (0.14)\n30 (0.13)\n27.8\n54.3\n17.9\nDDIM\n50 (0.13)\n30 (0.13)\n29.7\n46.8\n23.5\nTable 2.\nEMU-768 Visual Appeal Human Evaluation.\nWe\npresent the percentages of votes indicating a win, tie, or loss for\nour method in comparison to the baseline. This is evaluated across\nvarious solvers and number of steps. In every comparison, both\nthe caching and baseline configuration have roughly the same in-\nference speed (reported as images per second).\nvibrant images with significantly more details when com-\npared to the 14-step baseline. This can be explained by the\nfact that when performing only 14 steps, the model makes\nsteps that are too big to add meaningful details to the image.\nCaching without scale-shift adjustment also yields images\nwith more detail compared to the baseline. However, we\noften observe local artifacts, which are particularly notice-\nable in the image backgrounds. These artifacts appear like\noverly-emphasized style features. The application of our\nscale-shift adjustment effectively mitigates these effects.\n7\n0.0\n0.2\n0.4\n0.6\n0.8\nThreshold \n1.0\n1.2\n1.4\n1.6\nThroughput in samples/s\nFigure 7. Effect of Cache Threshold \u03b4. Left: Generated image\nfor different \u03b4. Right: Inference speed vs. \u03b4. The higher \u03b4, the\nmore blocks are cached, resulting in faster inference. \u03b4 = 0.5\ngives a 1.5x speedup and the best visual quality. Configuration:\nDPM, LDM-512, Block caching with 50 steps.\nAnalysis of EMU-768.\nTo demonstrate the generality of\nour proposed approach, we also apply caching and scale-\nshift adjustment to the EMU-768 model under the same set-\ntings as for LDM-512. As can be seen in Fig. 3, we achieve\na very similar effect: The generated images are much more\ndetailed and more vibrant, compared to the baseline. This is\nalso confirmed by a human eval study, in which we asked 12\nindependent annotators to compare the visual appeal of im-\nages generated for the prompts from Open User Input (OUI)\nPrompts [8] and PartiPrompts[57] for different configura-\ntions. Specifically, we compared different configurations\nwith the same latency for different samplers and collected\n1320 votes in total. As reported in Tab. 2, our proposed\ncaching technique is clearly preferred over the baseline in\nevery run. Note that for many prompts, both images have\nvery high quality, leading to a high rate in ties. This study\nshows that caching can be applied to a wide range of differ-\nent models, samplers and step counts.\nEffects of more aggressive caching.\nThe extent to which\nthe model caches results is controlled by the parameter \u03b4.\nThe higher \u03b4, the longer the cache lifetime and the less\nfrequent block outputs are recomputed. Fig. 7 shows syn-\nthesized images for varying \u03b4 values along with the cor-\nresponding inference speed. Although a higher \u03b4 leads to\nfaster inference, the quality of the final image deteriorates\nwhen block outputs are recomputed too infrequently. We\nfind that \u03b4 = 0.5 not only provides a significant speedup by\n1.5\u00d7 but also improves the image quality, thereby achieving\nthe optimal trade-off (see Tab. 1).\nDifficulty of Caching ResBlocks.\nAs described above,\nwe only cache SpatialTransformer blocks and not\nResBlocks. This design choice is grounded in the ob-\nBaseline - 20 Steps\nResBlock Caching - 20 Steps\nFigure 8. Effect of Caching ResBlocks. Caching ResBlocks in-\nstead of spatial transformer blocks results in fewer details and in-\nferior image quality, while achieving only a small speedup of 5%.\nConfiguration: DPM, EMU-768, Block caching with 20 steps.\nservation, that ResBlocks change much less smoothly\ncompared to SpatialTransformer blocks.\nIn par-\nticular, ResBlocks are very important for generating\nlocal details in the image.\nTo test this, we generate\nimages where we only cache ResBlocks and leave\nSpatialTransformer blocks untouched. As can be\nseen in Fig. 8, even to gain a speedup of as low as 5%, the\nimage quality deteriorates significantly.\n5. Conclusion\nIn this paper, we first analyzed the inner workings of the de-\nnoising network, moving away from the common perspec-\ntive of considering diffusion models as black boxes. Lever-\naging the insights from our analysis, we proposed the Block\nCaching technique. It reduces the redundant computations\nduring inference of the diffusion models and significantly\nspeeds up the image generation process by a factor of 1.5\u00d7-\n1.8\u00d7 at a minimal loss of image quality. To showcase the\nadaptability of our approach, we performed experiments on\nLDM and EMU models with a parameter range from 900M\nto 2.7B. We tested our approach in different inference set-\ntings by varying solvers and number of steps. Our technique\ngenerates more vibrant images with more fine-grained de-\ntails when compared to naively reducing the number of\nsolver steps for the baseline model to match the compute\nbudget. We confirmed our findings quantitatively by com-\nputing the FID and by human evaluation.\n8\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al.\nediffi:\nText-to-\nimage diffusion models with an ensemble of expert denois-\ners. arXiv:2211.01324, 2022. 2\n[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,\nYunxin Jiao, and Aditya Ramesh. Improving image genera-\ntion with better captions. OpenAI, 2023. 1\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\nIn ICLR, 2018. 2\n[4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, 2023. 2\n[5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer. In\nCVPR, pages 11315\u201311325, 2022. 2\n[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao,\nEnze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, et al. Pixart-\u03b1: Fast training of diffu-\nsion transformer for photorealistic text-to-image synthesis.\narXiv:2310.00426, 2023. 2\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In ICML, pages 1691\u20131703. PMLR,\n2020. 2\n[8] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv:2309.15807, 2023. 2, 4, 6, 8\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. NeurIPS. 2\n[10] Tim Dockhorn, Arash Vahdat, and Karsten Kreis.\nGe-\nnie: Higher-order denoising diffusion solvers. NeurIPS, 35:\n30150\u201330166, 2022. 2\n[11] Zhongjie Duan, Chengyu Wang, Cen Chen, Jun Huang, and\nWeining Qian. Optimal linear subspace search: Learning to\nconstruct fast and high-quality schedulers for diffusion mod-\nels. arXiv:2305.14677, 2023. 2\n[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\npages 12873\u201312883, 2021. 2\n[13] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang,\nLanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong\nYin, et al. Ernie-vilg 2.0: Improving text-to-image diffu-\nsion model with knowledge-enhanced mixture-of-denoising-\nexperts. In CVPR, pages 10135\u201310145, 2023. 2\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014. 2\n[15] Moritz Hardt and Tengyu Ma. Identity matters in deep learn-\ning. arXiv:1611.04231, 2016. 4\n[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, pages 16000\u201316009, 2022. 2\n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. 2022. 2\n[18] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 2, 7\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 2020. 2\n[20] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez,\nGeorge Fedoseev, Alex Kendall, Jamie Shotton, and Gian-\nluca Corrado.\nGaia-1: A generative world model for au-\ntonomous driving. arXiv:2309.17080, 2023. 2\n[21] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park. Scaling up\ngans for text-to-image synthesis. In CVPR, pages 10124\u2013\n10134, 2023. 2\n[22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. NeurIPS, 35:26565\u201326577, 2022. 2\n[23] Max WY Lam, Jun Wang, Rongjie Huang, Dan Su,\nand Dong Yu.\nBilateral denoising diffusion models.\narXiv:2108.11514, 2021. 2\n[24] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip\nTorr. Controllable text-to-image generation. NeurIPS, 2019.\n2\n[25] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao.\nPseudo\nnumerical methods for diffusion models on manifolds. In\nICLR, 2022. 2\n[26] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and\nJoshua B Tenenbaum. Compositional visual generation with\ncomposable diffusion models.\nIn ECCV, pages 423\u2013439.\nSpringer, 2022. 3\n[27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. In NeurIPS,\npages 5775\u20135787, 2022. 2, 7\n[28] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver++: Fast solver for guided sam-\npling of diffusion probabilistic models. arXiv:2211.01095,\n2022. 2\n[29] Eric Luhman and Troy Luhman. Knowledge distillation in\niterative generative models for improved sampling speed.\narXiv:2101.02388, 2021. 3\n[30] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao.\nLatent consistency models:\nSynthesizing high-\nresolution images with few-step inference, 2023. 3\n[31] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In CVPR, pages\n14297\u201314306, 2023. 2, 3\n[32] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In ICML, pages 16784\u201316804. PMLR, 2022. 2\n9\n[33] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. Sdxl: improving latent diffusion models for\nhigh-resolution image synthesis. arXiv:2307.01952, 2023. 4\n[34] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR,\n2023. 2\n[35] Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.\nMirrorgan: Learning text-to-image generation by redescrip-\ntion. In CVPR, pages 1505\u20131514, 2019. 2\n[36] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-\nvised representation learning with deep convolutional gener-\native adversarial networks. arXiv:1511.06434, 2015. 2\n[37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In ICML, pages 8821\u2013\n8831. PMLR, 2021. 2\n[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv:2204.06125, 1(2):3, 2022. 1,\n2\n[39] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gen-\nerating diverse high-fidelity images with vq-vae-2. NeurIPS,\n32, 2019. 2\n[40] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-\ngeswaran, Bernt Schiele, and Honglak Lee. Generative ad-\nversarial text to image synthesis. In ICML, pages 1060\u20131069.\nPMLR, 2016. 2\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 1, 2, 4, 6\n[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, pages 234\u2013241. Springer, 2015. 4\n[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 35:36479\u201336494, 2022. 2\n[44] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR, 2021. 2, 3\n[45] Neta Shaul, Juan Perez, Ricky TQ Chen, Ali Thabet, Albert\nPumarola, and Yaron Lipman. Bespoke solvers for genera-\ntive flow models. arXiv:2310.19075, 2023. 2\n[46] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. In ICLR, 2023. 2\n[47] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn ICML. PMLR, 2015.\n2\n[48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2020. 2, 7\n[49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICLR, 2020. 2, 3\n[50] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. arXiv:2303.01469, 2023.\n3\n[51] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun\nBao, and Changsheng Xu. Df-gan: A simple and effective\nbaseline for text-to-image synthesis. In CVPR, pages 16515\u2013\n16525, 2022. 2\n[52] Daniel Watson, William Chan, Jonathan Ho, and Moham-\nmad Norouzi. Learning fast samplers for diffusion models\nby differentiating through sample quality. In ICLR, 2021. 2\n[53] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.\nTedigan: Text-guided diverse face image generation and ma-\nnipulation. In CVPR, pages 2256\u20132265, 2021. 2\n[54] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In CVPR, pages 1316\u20131324, 2018. 2\n[55] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu,\nZhuofan Zong, Yu Liu, and Ping Luo.\nRaphael: Text-\nto-image generation via large mixture of diffusion paths.\narXiv:2305.18295, 2023. 2\n[56] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang.\nDiffusion probabilistic model made slim. In CVPR, pages\n22552\u201322562, 2023. 3\n[57] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,\nGunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,\nYinfei Yang, Burcu Karagol Ayan, et al.\nScaling autore-\ngressive models for content-rich text-to-image generation.\narXiv:2206.10789, 2(3):5, 2022. 8\n[58] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In ICCV, pages 5907\u20135915,\n2017. 2\n[59] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan++: Realistic image synthesis with stacked generative ad-\nversarial networks. IEEE TPAMI, 41(8):1947\u20131962, 2018. 2\n[60] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-\ntus Odena. Self-attention generative adversarial networks. In\nICML, pages 7354\u20137363. PMLR, 2019. 2\n[61] Qinsheng Zhang and Yongxin Chen. Fast sampling of dif-\nfusion models with exponential integrator. In NeurIPS 2022\nWorkshop on Score-Based Methods, 2022. 2\n[62] Qinsheng Zhang, Jiaming Song, and Yongxin Chen.\nIm-\nproved order analysis and design of exponential integrator\nfor diffusion models sampling. arXiv:2308.02157, 2023.\n[63] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and\nJiwen Lu. Unipc: A unified predictor-corrector framework\nfor fast sampling of diffusion models. NeurIPS, 2023. 2\n[64] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-\ngan: Dynamic memory generative adversarial networks for\ntext-to-image synthesis. In CVPR, pages 5802\u20135810, 2019.\n2\n10\nCache Me if You Can:\nAccelerating Diffusion Models through Block Caching\nSupplementary Material\nSupplementary Material\nIn this supplementary material, we provide\n1. thoughts on future work in Sec. A\n2. an overview of the limitations of our method in Sec. B\n3. thoughts on ethical considerations and safety in Sec. C\n4. additional figures for qualitative results, change metric\nplots, and caching schedules in Sec. D\nA. Future Work\nThere are several directions for future work. First, we be-\nlieve that the use of step-to-step change metrics is not lim-\nited to caching, but could / should also benefit e.g. find-\ning a better network architecture or a better noise schedule.\nSecondly, we find that the effect of scale-shift adjustment\ncan be quite significant on the overall structure and visual\nappeal of the image. It could be possible to use a similar\ntechnique for finetuning with human in the loop to make\nthe model adhere more to the preference of the user with-\nout having to change the training data. Finally, it would be\ninteresting if caching could be integrated into a network ar-\nchitecture even before training. This could not only improve\nthe results of the final model, but also speed up training.\nB. Limitations\nWhile our method achieves good results, some noteworthy\nweaknesses remain. We observe that while the scale-shift\nadjustment improves results and reduces artifacts, it some-\ntimes changes the identity of the image more than reducing\nthe number of steps or using naive caching would. Further-\nmore, finding the perfect threshold for auto configuration\ncan take time, as the model is sensitive to certain changes\nin the caching schedule.\nWe recommend playing around\nwith small variations of the desired threshold to obtain the\nperfect schedule.\nC. Ethical Considerations & Safety\nWe do not introduce new image data to these model and\nthe optimization scheme for scale-shift adjustment only re-\nquires prompts. Therefore, we believe that our technique\ndoes not introduce ethical or legal challenges beyond the\nmodel on which we apply our technique.\nFor safety considerations, it should be noted that scale-\nshift adjustment, while still following the prompt, can\nchange the identities in the image slightly.\nThis aspect\nmight make an additional safety check necessary when de-\nploying models with block caching.\nD. Additional Figures\nAdditional Qualitative Results.\nWe show additional\nresults for all configurations mentioned in the main paper.\nFor all configurations, we show our caching technique with\nand without scale-shift adjustment, a slower baseline with\nthe same number of steps, and a baseline with the same\nlatency as ours (by reducing the number of steps).\nAdditional Change Plots.\nFor all above mentioned\nconfigurations, we show the step-to-step change per layer\nblock averaged over 32 forward passes and two random\nseeds each measured via the L1rel metric. This corresponds\nto Fig. 2 b) in the main paper.\nAdditional Caching Schedules, Finally, we also show all\nthe caching schedules, which are automatically derived\nfrom the change measurements mentioned above.\nAn overview of the figures is provided by Tab. 1\nModel\nSteps\nSolver\nQuali.\nChange\nSchedule\nEMU-768\n20 vs 14\nDPM\nFig. 1\nFig. 9\nFig. 17\nDDIM\nFig. 2\nFig. 10\nFig. 18\n50 vs 30\nDPM\nFig. 3\nFig. 11\nFig. 19\nDDIM\nFig. 4\nFig. 12\nFig. 20\nLDM-512\n20 vs 14\nDPM\nFig. 5\nFig. 13\nFig. 21\nDDIM\nFig. 6\nFig. 14\nFig. 22\n50 vs 30\nDPM\nFig. 7\nFig. 15\nFig. 23\nDDIM\nFig. 8\nFig. 16\nFig. 24\nTable 1. Additional Figures Overview. Quali.: Qualitative re-\nsults, Change: Change metric plots, Schedule: Chaching schedule\n11\nCaching - 20 Steps\nBaseline - 20 Steps\nBaseline - 14 Steps\nCaching + Scale Shift - 20 Steps\nAn ethereal, floating violin emitting\nharmonious music in an empty\nauditorium\nA man has a red water bottle up his\nmouth\n A mesmerizing floating lantern\nilluminating a mystical garden at\ntwilight\nA shimmering pool with water that\nreveals visions of distant lands\nFigure 1. Qualitative Results for EMU-768 - DPM 20 Steps.\n12\nCaching - 20 Steps\nBaseline - 20 Steps\nBaseline - 14 Steps\nCaching + Scale Shift - 20 Steps\nA margarita next to a napkin\nRailway car on snow covered tracks\napproaching urban area\nDreams entering other dreams, digital\nart, colorful, 4k\nA man banging on a door\nFigure 2. Qualitative Results for EMU-768 - DDIM 20 Steps.\n13\nCaching - 50 Steps\nBaseline - 50 Steps\nBaseline - 30 Steps\nCaching + Scale Shift - 50 Steps\nA racing motorcycle going very fast\naround a corner of the racetrack\nThe old man and in the boat\nEmotion\nA large white yacht\nFigure 3. Qualitative Results for EMU-768 - DPM 50 Steps.\n14\nCaching - 50 Steps\nBaseline - 50 Steps\nBaseline - 30 Steps\nCaching + Scale Shift - 50 Steps\nA moody researcher working late\nPeople crossing the street in front of a\ntaxi and a tourbus\nA pirate lost in his thoughts next to a\ncampfire\nA fish\nFigure 4. Qualitative Results for EMU-768 - DDIM 50 Steps.\n15\nCaching - 20 Steps\nBaseline - 20 Steps\nBaseline - 14 Steps\nCaching + Scale Shift - 20 Steps\nOil painting of a small renaissance\nchurch in an old european village\nA beautiful lush mountain valley with\nthe sun setting in the background\nA mystic white house on the coast of an\nocean in the style of San Torino\nPhotograph of an angry snapping\nturtle on mars\nFigure 5. Qualitative Results for LDM-512 - DPM 20 Steps.\n16\nCaching - 20 Steps\nBaseline - 20 Steps\nBaseline - 14 Steps\nCaching + Scale Shift - 20 Steps\nA field of colorful flowers\nExpressionist oil painting of the Eiffel\ntower\nClose up photograph of a red and\ngreen parrot in the jungle \nPouring a glass of orange juice with\nfull oranges next to it\nFigure 6. Qualitative Results for LDM-512 - DDIM 20 Steps.\n17\nCaching - 50 Steps\nBaseline - 50 Steps\nBaseline - 30 Steps\nCaching + Scale Shift - 50 Steps\nAn old person sitting on the beach and\nsketching the ocean\nThe golden gate bridge with ships\ncrossing below it\nOil painting of a smiling santa claus in\nfront of a christmas tree with presents\nTwo happy aliens holding hands\nFigure 7. Qualitative Results for LDM-512 - DPM 50 Steps.\n18\nCaching - 50 Steps\nBaseline - 50 Steps\nBaseline - 30 Steps\nCaching + Scale Shift - 50 Steps\nA polar bear walking through the\narctiv snow\nDigital artwork of a happy woman\nsmiling into the camera  with flowers\nin the background\nA plate with white rice topped by\ncooked vegetables \nHot dog on a roll with cheese, onions,\nand herbs\nFigure 8. Qualitative Results for LDM-512 - DDIM 50 Steps.\n19\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\nFigure 9. Change Metrics for EMU-768 - DPM 20 Steps.\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\nFigure 10. Change Metrics for EMU-768 - DDIM 20 Steps.\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\nFigure 11. Change Metrics for EMU-768 - DPM 50 Steps.\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\nFigure 12. Change Metrics for EMU-768 - DDIM 50 Steps.\n20\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\nFigure 13. Change Metrics for LDM-512 - DPM 20 Steps.\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n5\n10\n15\n20\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\nFigure 14. Change Metrics for LDM-512 - DDIM 20 Steps.\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\nFigure 15. Change Metrics for LDM-512 - DPM 50 Steps.\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\nFull resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/2 resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/4 resolution\n10\n20\n30\n40\n50\nSolver steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative L1\n1/8 resolution\nFigure 16. Change Metrics for LDM-512 - DDIM 50 Steps.\n21\n18\n16\n14\n12\n10\n8\n6\n4\n2\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 3\nInput block 5\nInput block 6\nInput block 7\nInput block 8\nInput block 10\nInput block 11\nInput block 12\nInput block 13\nMiddle block \nMiddle block \nOutput block 5\nOutput block 6\nOutput block 7\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nOutput block 12\nOutput block 13\nOutput block 14\nOutput block 15\nOutput block 16\nOutput block 17\nOutput block 18\nBlocks\nFigure 17. Cache Schedules for EMU-768 - DPM 20 Steps.\n18\n16\n14\n12\n10\n8\n6\n4\n2\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 3\nInput block 5\nInput block 6\nInput block 7\nInput block 8\nInput block 10\nInput block 11\nInput block 12\nInput block 13\nMiddle block \nOutput block 5\nOutput block 6\nOutput block 7\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nOutput block 12\nOutput block 13\nOutput block 14\nOutput block 15\nOutput block 16\nOutput block 17\nOutput block 18\nBlocks\nFigure 18. Cache Schedules for EMU-768 - DDIM 20 Steps.\n45\n40\n35\n30\n25\n20\n15\n10\n5\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 3\nInput block 5\nInput block 6\nInput block 7\nInput block 8\nInput block 10\nInput block 11\nInput block 12\nInput block 13\nMiddle block \nOutput block 5\nOutput block 6\nOutput block 7\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nOutput block 12\nOutput block 13\nOutput block 14\nOutput block 15\nOutput block 16\nOutput block 17\nOutput block 18\nBlocks\nFigure 19. Cache Schedules for EMU-768 - DPM 50 Steps.\n45\n40\n35\n30\n25\n20\n15\n10\n5\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 3\nInput block 5\nInput block 6\nInput block 7\nInput block 8\nInput block 10\nInput block 11\nInput block 12\nInput block 13\nMiddle block \nOutput block 5\nOutput block 6\nOutput block 7\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nOutput block 12\nOutput block 13\nOutput block 14\nOutput block 15\nOutput block 16\nOutput block 17\nOutput block 18\nBlocks\nFigure 20. Cache Schedules for EMU-768 - DDIM 50 Steps.\n22\n18\n16\n14\n12\n10\n8\n6\n4\n2\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 4\nInput block 5\nInput block 7\nInput block 8\nMiddle block \nMiddle block \nOutput block 3\nOutput block 4\nOutput block 5\nOutput block 6\nOutput block 7\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nBlocks\nFigure 21. Cache Schedules for LDM-512 - DPM 20 Steps.\n18\n16\n14\n12\n10\n8\n6\n4\n2\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 4\nInput block 5\nOutput block 5\nOutput block 6\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nBlocks\nFigure 22. Cache Schedules for LDM-512 - DDIM 20 Steps.\n45\n40\n35\n30\n25\n20\n15\n10\n5\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 4\nInput block 5\nInput block 7\nInput block 8\nMiddle block \nOutput block 3\nOutput block 4\nOutput block 5\nOutput block 6\nOutput block 7\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nBlocks\nFigure 23. Cache Schedules for LDM-512 - DPM 50 Steps.\n45\n40\n35\n30\n25\n20\n15\n10\n5\n0\nSolver steps\nInput block 1\nInput block 2\nInput block 4\nInput block 5\nInput block 7\nInput block 8\nMiddle block \nOutput block 3\nOutput block 4\nOutput block 5\nOutput block 6\nOutput block 7\nOutput block 8\nOutput block 9\nOutput block 10\nOutput block 11\nBlocks\nFigure 24. Cache Schedules for LDM-512 - DDIM 50 Steps.\n23\n"
  },
  {
    "title": "HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting",
    "link": "https://arxiv.org/pdf/2312.03461.pdf",
    "upvote": "15",
    "text": "HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian\nSplatting\nYuheng Jiang1,2\nZhehao Shen1\nPenghao Wang1\nZhuo Su3\nYu Hong1\nYingliang Zhang4\nJingyi Yu1\nLan Xu1\n1ShanghaiTech University\n2NeuDim\n3ByteDance\n4DGene\nFigure 1. High-fidelity rendering with our compact Gaussian Splatting. From multi-view human performance video, HiFi4G marries\nthe traditional non-rigid fusion with differentiable rasterization advance to efficiently produce compact 4D assets.\nAbstract\nWe have recently seen tremendous progress in photo-\nreal human modeling and rendering. Yet, efficiently ren-\ndering realistic human performance and integrating it into\nthe rasterization pipeline remains challenging. In this pa-\nper, we present HiFi4G, an explicit and compact Gaussian-\nbased approach for high-fidelity human performance ren-\ndering from dense footage. Our core intuition is to marry\nthe 3D Gaussian representation with non-rigid tracking,\nachieving a compact and compression-friendly representa-\ntion. We first propose a dual-graph mechanism to obtain\nmotion priors, with a coarse deformation graph for effec-\ntive initialization and a fine-grained Gaussian graph to en-\nforce subsequent constraints. Then, we utilize a 4D Gaus-\nsian optimization scheme with adaptive spatial-temporal\nregularizers to effectively balance the non-rigid prior and\nGaussian updating. We also present a companion compres-\nsion scheme with residual compensation for immersive ex-\nperiences on various platforms. It achieves a substantial\ncompression rate of approximately 25 times, with less than\n2MB of storage per frame. Extensive experiments demon-\nstrate the effectiveness of our approach, which significantly\noutperforms existing approaches in terms of optimization\nspeed, rendering quality, and storage overhead.\nProject\npage: https://nowheretrix.github.io/HiFi4G/.\n1. Introduction\nVolumetric recording and realistic rendering of 4D (space-\ntime) human performance diminish the boundaries between\nviewers and performers. It brings numerous immersive ex-\nperiences like telepresence or tele-education in VR/AR.\nEarly solutions [9\u201311, 23] reconstruct textured meshes\nfrom captured videos by explicitly leveraging non-rigid reg-\nistration [42, 60]. Yet, they remain vulnerable to occlu-\nsions and lack of textures which cause holes and noise in\nthe reconstruction results. Recent neural advances, repre-\nsented by NeRF [39], bypass explicit reconstruction and\ninstead optimize a coordinate-based multi-layer perceptron\n(MLP) to conduct volume rendering at photo-realism. Some\ndynamic variants [14, 44\u201346, 64, 66] of NeRF attempt to\nmaintain a canonical feature space to reproduce features in\neach live frame with an extra implicit deformation field.\nHowever, such a canonical design is fragile to large mo-\narXiv:2312.03461v2  [cs.CV]  7 Dec 2023\ntions or topology changes. Recent approaches [19, 57, 67]\nremove the deformation fields and compactly represent the\n4D feature grid through planar factorization [7, 13] or Hash-\nencoding [40]. They significantly accelerate both the train-\ning and rendering speed for interactive applications but the\nchallenges of runtime memory and storage still exist. The\nrecent 3D Gaussian Splatting (3DGS) [24] strikes back to\nan explicit paradigm for static scene representation. Based\non GPU-friendly rasterization of 3D Gaussian primitives, it\nallows real-time and high-quality radiance field rendering\nunseen before. Various concurrent works [36, 69, 72, 73]\nadapt 3DGS for dynamic scenes. Some [36] focus on ex-\ntracting the non-rigid motions from dynamic Gaussians yet\nsacrificing the rendering quality. Others [69, 72] adopt ex-\ntra implicit deformation fields to compensate for the motion\ninformation, and hence fall short of handling long-duration\nmotions and lose the explicit and GPU-friendly beauty of\nthe original 3DGS.\nIn this paper, we present HiFi4G \u2013 a totally explicit and\ncompact Gaussian-based approach for high-fidelity 4D hu-\nman performance rendering from dense footage (see Fig. 1).\nOur key idea is to marry the 3D Gaussian representa-\ntion [24] with non-rigid tracking, so as to explicitly disen-\ntangle motion and appearance information for a compact\nand compression-friendly representation. HiFi4G signifi-\ncantly outperforms existing implicit rendering approaches,\nin terms of optimization speed, rendering quality, and stor-\nage overhead.\nOur explicit representation also enables\nseamlessly integrating our results into the GPU-based ras-\nterization pipeline., i.e., immersively watching high-fidelity\nhuman performances with VR headsets.\nTo organically bridge Gaussian representation with non-\nrigid tracking, we first introduce a dual-graph mechanism,\nwhich consists of a coarse deformation graph and a fine-\ngrained Gaussian graph.\nFor the former, we obtain per-\nframe geometry proxy via the NeuS2 [67] and then employ\nembedded deformation (ED) [60] in a key-frame manner.\nSuch an explicit tracking process splits the sequence into\nsegments and provides rich motion prior within each seg-\nment. Analogous to the key-volume update [10], we fol-\nlow 3DGS to prune the incorrect Gaussians from the pre-\nvious segment and update new ones to restrict the number\nof Gaussians in the current segment. Then, we build a fine-\ngrained Gaussian graph and interpolate the motion of each\nGaussian from the coarse ED graph for subsequent initial-\nization. Na\u00a8\u0131vely warping the Gaussian graph with the ED\ngraph and splatting it onto screen space will cause severe\nunnatural artifacts, while continuous optimization without\nany constraints leads to jittery artifacts. Thus, we propose\na 4D Gaussian optimization scheme to carefully balance\nthe non-rigid motion prior and the updating of Gaussian\nattributes. We adopt a temporal regularizer to enforce the\nappearance attributes of each Gaussian, i.e., spherical har-\nmonic (SH), opacity, and scaling coefficients, to be con-\nsistent. We also propose a smooth term for the motion at-\ntributes (position and rotation) to produce locally as-rigid-\nas-possible motions between the adjacent Gaussians. These\nregularizers are further enhanced with an adaptive weight-\ning mechanism to penalize the flicking artifacts on the re-\ngions with slight non-rigid motions. Once optimized, we\nobtain spatial-temporally compact 4D Gaussians. To make\nour HiFi4G practical for users, we demonstrate a com-\npanion compression scheme that follows standard residual\ncompensation, quantization, and entropy encoding for the\nGaussian parameters. It achieves a substantial compression\nrate of approximately 25 times and only requires less than\n2 MB storage per frame, enabling immersively viewing hu-\nman performances on various platforms like VR headsets.\nTo summarize, our main contributions include:\n\u2022 We present a compact 4D Gaussian representation for\nhuman performance rendering, which bridges Gaussian\nSplatting and non-rigid tracking.\n\u2022 We propose a dual-graph mechanism with various\nregularization designs to effectively recover spatial-\ntemporally consistent 4D Gaussians.\n\u2022 We showcase a companion compression scheme, support-\ning immersive experience of human performance with\nlow storage, even under various platforms.\n2. Related Work\nHuman Performance Capture. Recently, human perfor-\nmance capture [1, 2, 8, 17, 20, 26, 38, 43, 48, 65] has\nbeen widely investigated to achieve detailed registration\nfor various applications. Zollh\u00a8ofer et al. [80] capture the\nrigid template first but DynamicFusion [42] removes this\nexplicit template prior and enables real-time performance\nwhich benefits from the GPU solvers. Guo et al. [15] model\nthe geometry, surface albedo, and appearance on the refer-\nence volume, generating impressive tracking results. Fu-\nsion4d [10] and Motion2fusion [11] rely on a key-frame-\nbased strategy to handle topological changes.\nBased on\nthe human parametric model [35], DoubleFusion [74] pro-\nposes a two-layer representation for more robust scene cap-\nture, while Xu et al. [71] extend it to sparse view setup.\nSu et al. [58, 59] further address the challenging motions\nand human-object interaction scenarios. Additionally, sev-\neral studies [21, 28, 29, 75] combine explicit volumetric fu-\nsion and implicit modeling to capture more dynamic details.\nNevertheless, these methods primarily focus on detailed ge-\nometry rather than high-quality texture. Comparably, our\napproach bridges volumetric capture and recent differen-\ntiable rasterization advances, achieving high-fidelity human\nperformance rendering.\nDynamic Scene Modeling.\nIn the domain of dynamic\nscene representation, various approaches [31, 34, 37, 61,\n62, 76, 79] have been proposed to address this challenge.\nD-NeRF [49] and Non-rigid NeRF [64] utilize a displace-\nment field to represent the motion, while Neuralbody [47]\nuses latent codes anchored to SMPL [35] vertices.\nHu-\nmannerfs [68, 78] combine the SMPL with a deformation\nnet. TAVA [25] and X-avatar [56] learn the skinning weight\nthrough root-finding. NDR [3] defines a bijective function\nthat satisfies the cycle consistency. With recent advance-\nments in Instant-NGP [40], some works [19, 22, 31, 57,\n67] demonstrate the efficient training and rendering speed.\nHowever, most methods produce blurriness, particularly in\nhigh-frequency regions. The recent 3DGS [24] strikes back\nto an explicit paradigm for high-performance static scene\nrepresentation. However, per-frame 3DGS disregards tem-\nporal consistency, resulting in visual jitteriness. Some con-\ncurrent studies [36, 69, 72, 73] adapt 3DGS for dynamic\nscenes. Yet, these methods typically offer real-time perfor-\nmance only at low resolution and are not equipped to handle\nlarge motions. In contrast, HiFi4G leverages dual-graph to\ngenerate compact 4D Gaussians, enabling high-fidelity real-\ntime rendering with challenging motions.\nCompact Representation. Compact representation plays\na pivotal role in dynamic rendering, engaging the in-\nterest of numerous researchers.\nA series of works are\nproposed for early point cloud compression with Oc-\ntree [53, 63],\nWavelet [41].\nThese are formalized\ninto MPEG-PCC [54] standards by the Moving Pic-\nture Experts Group(MPEG), which are categorized into\nvideo-based(VPCC) and geometry-based(GPCC). Follow-\ning, learning-based methods [30, 50, 51] emerge, focusing\non enhancing efficiency. For neural fields, several studies\nintroduce compact neural representations through tensor [7]\nand scene [57] decomposition, tri-planes [18, 52] and multi-\nplanes [4, 13, 55]. Instant-NSR [77] leverages the tracked\nmesh and texture video while HumanRF [19] employs tem-\nporal matrix-vector decomposition. Despite their advance-\nments, these methods often compromise rendering quality\nand speed to minimize storage requirements. Comparably,\nHiFi4G achieves a substantial compression rate of approx-\nimately 25 times and only requires less than 2 MB storage\nper frame to enable high-quality rendering results.\n3. Method\nGiven human performance videos captured by multi-view\npre-calibrated and synchronized RGB cameras, HiFi4G\nintegrates recent advancements in differentiable rasteriza-\ntion with traditional non-rigid tracking, significantly out-\nperforming existing rendering approaches [19, 32, 67, 77]\nin terms of optimization speed, rendering quality, and stor-\nage overhead. The methodology is visually summarized in\nFig. 2. Our approach starts with a dual graph mechanism,\nwhich consists of a coarse deformation graph and a fine-\ngrained Gaussian graph, detailed in Sec 3.1. Subsequently,\nthis representation is employed along with corresponding\ntemporal and smooth regularization, leading to the genera-\ntion of spatial-temporally compact 4D Gaussians in Sec 3.2.\nIn addition, we introduce a companion compression scheme\nin Sec 3.3.\nThis allows for immersive viewing of high-\nfidelity human performances with a storage requirement of\nless than 2 MB per frame.\n3.1. Dual Graph Mechanism\nWe employ a dual graph structure to explicitly disentan-\ngle motion and appearance, resulting in a compact and\ncompression-friendly representation. This design facilitates\nexpedited convergence and enhances visual quality.\nCoarse Deformation Graph.\nInstead of using an addi-\ntional implicit deformation network [69, 72] to handle non-\nrigid motion, which could potentially affect the high per-\nformance and GPU-friendliness of the original 3DGS, we\nopt for the Embedded Deformation [60] to establish model-\nto-model correspondences by leveraging conventional non-\nrigid deformation techniques [16, 42, 70].\nTo achieve\nthis, we first generate per-frame geometry proxies using\nNeuS2 [67]. We then apply non-rigid tracking to the result-\ning mesh sequences following a key-frame manner. Specifi-\ncally, we parameterize the dynamic motions as an ED graph\nW = {dqi, xi}, where xi represents the coordinates of sam-\npled ED nodes in key space, and dqi denotes the dual quater-\nnions representing the corresponding rigid transformation\nin SE(3) space. Subsequently, we acquire each point vc\nusing Dual-Quaternion Blending:\nDQB(vc) =\nX\ni\u2208N (vc)\nw(xi, vc)dqi,\n(1)\nwhere N(vc) is a set of neighboring ED nodes of vc. and\nw(xi, vc) denotes the influence weight of the i \u2212 th node\nxi on vc. At frame t, we identify correspondence points\nbetween the warped key mesh and the current mesh. Subse-\nquently, we optimize the motion by constructing the terms:\nE = \u03bbdataEdata + \u03bbregEreg,\n(2)\nWhere Edata and Ereg represent the energies associated\nwith the data term and the regularization term, respectively.\nPlease refer to [42, 74] for more details. To explicitly han-\ndle coarse topological changes and reduce severe misalign-\nment issues, we implement the key-volume strategy as de-\nscribed in [10, 70]. This strategy involves segmenting the\nsequence into multiple key volumes.\nFine-grained Gaussian Graph. To bypass the tedious pro-\ncess of creating 3D Gaussians from Structure-from-Motion\n(SfM) points for each frame, we utilize a more efficient ini-\ntialization method. For the first frame, we construct the\n3D Gaussians from the NeuS2 mesh using an importance\nsampling strategy. We increase the sampling density in the\nhand and face regions to significantly improve visual qual-\nity. For subsequent keyframes, analogous to the key-volume\nFigure 2. Overview of HiFi4G. (a) The non-rigid tracking establishes a coarse deformation graph and tracks the motions for Gaussian\noptimization. (b) HiFi4G initializes first frame Gaussians from NeuS2 and constructs a fine-grained Gaussian graph to enhance temporal\ncoherence. We then employ the ED graph to warp 4D Gaussians, applying both Esmooth and Etemp constraints to the Gaussian graph,\nwhich yields spatial-temporally compact and compression-friendly 4D Gaussians, thus facilitating efficient compression.\nupdate strategy, we follow 3DGS to prune incorrect Gaus-\nsians from the previous keyframe and densify new ones at\nthe current keyframe. We then restrict the number of Gaus-\nsians within the current segment. Afterward, we establish\na fine-grained Gaussian graph, consisting of refined Gaus-\nsian kernels for subsequent constraints, determined by the\nk-nearest neighbors (KNN, k = 16). In addition, for each\nGaussian kernel in the fine-grained graph, we also find the\nKNN (k = 8) from the ED nodes, which assists in calcu-\nlating the influence weight for motion interpolation. The\ninitialization is still crucial for non-key frames to prevent\nfalling into local optima during the back-propagation of dif-\nferentiable rasterization. To this end, we warp the Gaussian\ngraph from the keyframe to other frames within the segment\naccording to the ED nodes\u2019 motion interpolation:\np\u2032\ni,t = SE3(DQB(pi,k))pi,k,\nq\u2032\ni,t = ROT(DQB(pi,k))qi,k,\n(3)\nSE3(\u00b7) converts dual quaternion back into a transformation\nmatrix, while ROT(\u00b7) extracts the rotation component from\ndual quaternion. pi,k, qi,k denote the position and rotation\nof the i-th Gaussian kernel at keyframe k, respectively. p\u2032\ni,t\nand q\u2032\ni,t represent the initial position and rotation at frame t.\nThey will be further optimized in the subsequent stage.\n3.2. 4D Gaussians Optimization\nDirectly warping the fine-grained Gaussian graph with\ntracking prior and splatting it onto screen space can lead to\nnoticeable and unnatural artifacts. To mitigate this, we do\nnot use the Gaussians\u2019 densification and pruning within the\nsegment. Instead, we impose a constraint on their number\nand execute sequential optimization.\nFor the frame t, we categorize attributes for each 4D\nGaussian kernel i into two groups: 1). Appearance-aware\nparameters, which include spherical harmonic Ci,t, opacity\n\u03c3i,t, and scaling si,t. 2). Motion-aware parameters, which\ninclude position pi,t and rotation qi,t. Leveraging the initial-\nization from the warped Gaussian graph reduces the training\ntime to one-third while still yielding vivid results. However,\ndespite incorporating non-rigid tracking priors, we observe\nnotable temporal jitters in the rendered results. Concurrent\nstudies [36, 69, 72] address this issue by decoupling the de-\nformation field from canonical 3D Gaussians. They employ\na consistent set of Gaussians across dynamic sequences,\nwhich substantially diminishes view-dependent effects and\nsacrifices rendering quality.\nTo mitigate temporal jitters\nwhile maintaining rendering quality, we introduce tempo-\nral and smooth regularization to delicately balance the dual\ngraph prior and the updating of Gaussian attributes, thereby\nenforcing 4D consistency. First, we introduce the tempo-\nral regularization term Etemp. This term promotes coherent\nappearances by constraining the 4D Gaussian appearance\nattributes(Ci,t, \u03c3i,t, si,t) to be consistent with the previous\nframe:\nEtemp =\nX\na\u2208{C,\u03c3,s}\nwi,t\u03bba\u2225ai,t \u2212 ai,t\u22121\u22252\n2,\n(4)\nEtemp helps reduce jitteriness. However, it may be not suf-\nficient, especially when motion parameters change signifi-\ncantly, particularly in feature-less areas. Moreover, apply-\ning this regularization directly to motion attributes p and q\ncan also result in unnatural artifacts. To address this issue,\nwe introduce a smooth term targeted at the motion attributes\n(pi,t, qi,t) within the fine-grained Gaussian graph. We de-\nfine this term as follows:\nEsmooth =\nX\ni\nX\nj\u2208N (i)\nwi,t\u2225SO3(qi,t \u2217 q\u22121\ni,t\u22121)\n(pj,t\u22121 \u2212 pi,t\u22121) \u2212 (pj,t \u2212 pi,t)\u22252\n2,\n(5)\nSO3(\u00b7) converts a quaternion into a rotation matrix. Kernel\ni and j are neighbors on the Gaussian graph. The smooth\nterm produces locally as-rigid-as-possible deformations to\nconstrain the consistent 4D Gaussian motion on the spatial-\ntemporal domain. Furthermore, it\u2019s observed that the Hu-\nman Visual System is more sensitive to detail changes in\nstatic regions as opposed to dynamic ones [6]. Thus, we\nincorporate an adaptive weight that takes into account the\ndisplacement of positions between adjacent frames:\nwi,t = exp(\u2212\u03b1\u2225p\u2032\ni,t \u2212 p\u2032\ni,t\u22121\u22252),\n(6)\nThis adaptive weight indicates the degree of motion change\nin a corresponding local region. It penalizes the flicking ar-\ntifacts in regions with slight non-rigid motions and reduces\npenalties in areas with large movements. This significantly\nimproves the visual quality. Additionally, we employ the\nphotometric loss during the training process:\nEcolor = \u2225 \u02c6C \u2212 C\u22251,\n(7)\n\u02c6C is the blended color after rasterization and C is the\nground truth. The complete energy is as follows:\nE = \u03bbtempEtemp + \u03bbsmoothEsmooth + \u03bbcolorEcolor.\n(8)\n3.3. Compact 4D Gaussians\nAfter optimization, we obtain spatial-temporally compact\n4D Gaussians, resulting in high-fidelity rendering results.\nHowever, each frame requires the same amount of storage\nas the keyframe.\nThis leads to significant memory con-\nsumption and presents challenges when handling lengthy\nsequences. To address this problem, we introduce a com-\npanion compression scheme on top of our compact 4D\nGaussians. This scheme adheres to the traditional method\nof residual compensation, quantization, and entropy encod-\ning, as depicted in Fig. 3.\nResidual Compensation. In contrast to the broad distribu-\ntion range of the original attributes, we choose to retain the\nkeyframe attributes and calculate residuals for the follow-\ning frames within the segment. This effectively narrows the\nrange of attributes. In terms of appearance attributes(Ci,t,\n\u03c3i,t, si,t), the impact of Etemp results in minimal variations.\nAs a result, we can directly derive the residual appearance\nthrough subtraction. However, for position p and rotation\nq, simple subtraction is not sufficient as large motions still\nexist within a segment. To address this, we employ motion\nFigure 3. Illustration of compression strategy for 4D Gaussians.\ncompensation as outlined in Eq. 1 and Eq. 3. We subtract\nthe warped key Gaussians p\u2032\ni,k, q\u2032\ni,k from pi,t, qi,t, ensuring\na narrower range.\nQuantization. We scale and round attribute values based\non their range and quantization bits Qbit, making the data\nready for entropy encoding.\nEntropy Encoding. Residual computation combined with\nmotion compensation yields a residual distribution for at-\ntributes that cluster around zero.\nTo leverage this dis-\ntribution for real-time encoding and decoding, we apply\nthe Ranged Arithmetic Numerical System (RANS) [12].\nRANS enhances compression by taking advantage of the\ndistribution\u2019s skewness, a key factor for meeting the high-\nperformance demands of HiFi4G. We compress our data\nby calculating the frequency of each quantized attribute\nand constructing a frequency distribution. This distribution\nhelps to encode each attribute efficiently using the RANS\nalgorithm, where each attribute and the current state of the\nencoder are processed to update the state, representing the\nencoded data sequence. The final state is stored as an in-\nteger stream for subsequent decoding. This compression\nscheme achieves a substantial compression rate of approx-\nimately 25 times, reducing the storage requirement to less\nthan 2 MB per frame. This capability facilitates the immer-\nsive viewing of high-fidelity human performances on vari-\nous platforms, including VR/AR HMDs.\n4. Implementation Details\nFirst, we use the background matting [33] to extract the\nforeground masks from all captured frames.\nFor global\ninitialization, we use [5] to estimate the hand and face re-\ngions for importance sampling. The sampling ratio across\nthe body, hands, and face regions is approximately 8:1:1.\nWe perform 30000 training iterations with densification and\npruning on the keyframes, followed by resetting the track-\ning and reconstructing the dual-graph. For non-key frames,\ntraining iterations are reduced to 9000. In the optimization\nstage, we use the following empirically determined param-\neters: \u03b1 = 50, \u03bbC = 1, \u03bb\u03c3 = 0.05, \u03bbs = 0.05, \u03bbsmooth =\n0.002, \u03bbtemp = 0.0005, \u03bbcolor = 1.0. During compression,\nFigure 4. Gallery of our results. HiFi4G delivers real-time high-fidelity rendering of human performance across challenging motions, such\nas \u201cplaying instruments\u201d, \u201cdancing\u201d and \u201cchanging clothes\u201d.\nwe first quantize the appearance attributes, then fix these\nparameters and fine-tune motion p and q of 4D Gaussians\nover an additional 1000 iterations. Afterward, we quantize\nthe motion. We apply different precision levels for various\nattributes to balance storage and quality. For the keyframes,\nwe keep the motion uncompressed(0-bit) and apply 9-bit\nquantization for appearance. For non-key frames, we use\n11-bit quantization for motion and 7-bit quantization for ap-\npearance due to their more compact range.\n5. Experimental Results\nTo demonstrate the capabilities of HiFi4G, we deploy 81\npre-calibrated Z-CAM cinema cameras to capture complex\nhuman performances with a resolution of 3840 \u00d7 2160 at\n30 fps, and then evaluate our method. The dataset covers\na variety of costumes, from traditional Chinese attire to ca-\nsual clothes and cosplay. It also includes a wide range of\nactivities such as dance, fitness, and interaction with vari-\nous objects. As shown in Fig. 4, HiFi4G enables real-time,\nhigh-fidelity rendering of human performance in high reso-\nlution. It effectively handles complex motions like playing\ninstruments, dancing, and changing clothes. Additionally,\nour explicit representation allows for seamless integration\nof our results into the GPU-based rasterization pipeline. We\nhighly recommend watching the supplementary videos.\n5.1. Comparison\nWe compare HiFi4G with the SOTA methods including\nInstant-NSR [77], NeuS2 [67], HumanRF [19] and con-\ncurrent work Dynamic 3D Gaussians [36] on our cap-\ntured dataset and ActorsHQ [19]. As depicted in Fig. 5,\nInstant-NSR [77] suffers from severe artifacts due to the\nFigure 5. Qualitative comparison of our method against Instant-NSR [77], NeuS2 [67], HumanRF [19] and Dynamic 3D Gaussians [36]\non both our dataset and ActorsHQ [19]. Our method achieves the highest rendering quality.\nTable 1. Quantitative comparison of rendering results. Green and\nyellow cell colors indicate the best and the second-best results.\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nVMAF\u2191\nPer-frame Storage(MB) \u2193\nInstant-NSR [77]\n29.385\n0.958\n0.0370\n68.309\n11.63\nNeuS2 [67]\n32.952\n0.961\n0.0682\n79.102\n24.16\nHumanRF [19]\n31.174\n0.977\n0.0298\n80.942\n11.38\nDynamic 3D Gaussians [36]\n30.244\n0.965\n0.0847\n52.224\n4.523\nOurs(Before Compression)\n36.205\n0.989\n0.0184\n85.127\n43.42\nOurs(After Compression)\n35.788\n0.986\n0.0214\n84.312\n1.648\nheavy reliance on geometry.\nVolume rendering methods\nsuch as NeuS2 [67] and HumanRF [19] produce blurry\nresults, over-smoothing on high-frequency details. Mean-\nwhile, Dynamic 3D Gaussians [36] loses the advantages of\n3DGS [24] due to fixed appearance attributes, failing to re-\ncover detailed appearance and view-dependency. In con-\ntrast, HiFi4G surpasses these existing methods by merging\n3D Gaussian representation with keyframe-update-based\nnon-rigid tracking, providing detailed and high-quality hu-\nman performance rendering. For quantitative comparison,\nwe evaluate each method on three 200-frame sequences\nfrom our dataset. We use various metrics, including PSNR,\nSSIM, LPIPS, the temporal metric VMAF [27], and per-\nframe storage. As seen in Tab. 1, HiFi4G surpasses other\nmethods in both quality and storage. Note that our com-\npression strategy significantly reduces per-frame storage re-\nquirements without compromising quality.\nRemarkably,\neven on the VMAF metric [27], which evaluates the percep-\ntual quality and temporal consistency, our explicit method\noutperforms HumanRF which benefits from the inherent\nsmoothness of the MLP.\n5.2. Ablation Study\nCompact 4D Gaussians. We conduct a qualitative ablation\non the dual-graph and the regularization term to assess their\nimpact on post-compression rendering results. As shown in\nFig. 6, the removal of the coarse ED-graph prior typically\ncauses severe artifacts. Excluding the Gaussian graph of-\nten results in significant precision loss and unnatural render-\ning. Regarding regularizers, the omission of Etemp usually\ntriggers unrealistic artifacts post-compression. Meanwhile,\nthe absence of Esmooth produces blurry results, with both\nleading to flickering in the video. Additionally, to evalu-\nate the impact of the adaptive weight wi,t, we replace it\nwith a fixed weight of 0.1. This adjustment generally leads\nto noticeable blurriness, especially in areas with significant\nmovement. In contrast, our full pipeline generates spatially\nand temporally compact 4D Gaussians, maintaining high-\nfidelity rendering even after compression. The quantitative\nresults are as demonstrated in Tab. 2, in which our full ap-\nproach achieves the highest accuracy.\nResidual Compensation. As illustrated in Fig. 7 (b), we\nallocate 48.24MB of storage for the 4D Gaussians of each\nframe before compression. Applying high-bit quantization\n(0-bit for motion and 9-bit for appearance) without residual\ncompensation results in a storage requirement of 7.41MB,\nas shown in Fig. 7 (c). Using low-bit quantization (11-bit\nfor motion and 7-bit for appearance), again without residual\ncompensation, reduces storage to 3.67MB but compromises\nrendering quality, as illustrated in Fig. 7 (d). In contrast, ap-\nplying the same low-bit quantization but with residual com-\npensation significantly reduces storage needs to under 2MB\nper frame while maintaining the same level of rendering\nquality, as shown in Fig. 7 (e).\nThe Number of 4D Gaussians. We assess the impact of\nchanging the number of 4D Gaussians on the quality of\nTable 2. Quantitative evaluation of compact 4D Gaussians.\nPSNR \u2191 SSIM \u2191 LPIPS \u2193 VMAF \u2191\nw/o ED graph\n29.142\n0.9534\n0.0662\n69.724\nw/o Gaussian graph 31.185\n0.9541\n0.0586\n76.873\nw/o Etemp\n33.555\n0.9661\n0.0496\n76.308\nw/o Esmooth\n33.889\n0.9657\n0.0518\n81.944\nw/o wi,t\n33.577\n0.9678\n0.0425\n81.236\nOurs\n35.085\n0.9828\n0.0219\n83.133\nFigure 6. Qualitative evaluation of compact 4D Gaussians.\nFigure 7. Qualitative evaluation of our residual strategy. (a) Ref-\nerence image; (b) 4D Gaussians results before compression; (c)\nPer-frame encoding using high-bit quantization without residual;\n(d) Per-frame encoding using low-bit quantization without resid-\nual; (e) Ours results using low-bit quantization with residual.\nTable 3. Run-time evaluation of each step.\nProcedure\nTime\nBackground matting\n\u223c 1 min\nMeshing\n\u223c 1 min\nnon-rigid tracking\n\u223c 100 ms\n4D Gaussians Optimization\n\u223c 4 mins\n4D Gaussians Compression\n\u223c 100 ms\nresults across three sequences. As depicted in Fig. 8, us-\ning 200,000 4D Gaussians is adequate for generating high-\nquality results. This amount enables effective compression\nto less than 2MB, supporting immersive viewing on diverse\nplatforms, including VR and AR.\nRun-time Evaluation of Each Step. As shown in Tab. 3,\nwe also provide the runtime for each step on a PC with an\nNvidia GeForce RTX3090 GPU, which includes both the\npreprocessing and training stages. Our method can generate\n4D assets efficiently, taking less than 7 minutes per frame.\nFigure 8. Evaluation of 4D Gaussians number. With \u223c200,000\n4D Gaussians, HiFi4G achieves high-fidelity human performance\nrendering, suitable for integration in VR applications.\n5.3. Limitation\nAlthough HiFi4G achieves high-fidelity 4D human perfor-\nmance rendering via compact Gaussian Splatting, it still has\nsome limitations. First, HiFi4G heavily relies on segmen-\ntation, poor segmentation can lead to significant artifacts,\nespecially in scenes with human-object interactions. More-\nover, our method necessitates per-frame reconstruction and\nmesh tracking, which presents an interesting direction in\nexploring a more synergistic relationship between tracking\nand rendering. Even though HiFi4G is efficient in gener-\nating 4D assets, the Gaussian optimization process still re-\nquires several minutes, forming a major bottleneck. Accel-\nerating this training process is vital for future research. Ad-\nditionally, the current dependence of 4D Gaussian on fast\nGPU sorting limits the deployment of HiFi4G on web view-\ners and mobile devices.\n6. Conclusion\nWe have presented an explicit and compact Gaussian-based\napproach for 4D human performance rendering from RGB\ninputs. By bridging 3D Gaussian Splatting with non-rigid\ntracking, our approach achieves high-fidelity rendering re-\nsults, outperforming previous methods in terms of quality,\nefficiency, and storage.\nOur dual-graph mechanism pro-\nvides sufficient non-rigid motion priors in a keyframe-based\nmanner, while our Gaussian optimization scheme with\nnovel regularization designs effectively ensures spatial-\ntemporal consistency of the 4D Gaussian Splatting. We also\ndemonstrate the compactness of our representation with\na companion compression scheme which substantially re-\nduces storage requirements. Our experimental results fur-\nther demonstrate the effectiveness of our approach for de-\nlivering lifelike human performances. With its explicit and\ncompact characteristics, we believe our approach makes\na solid step forward to faithfully recording and providing\nimmersive experiences of human performances on various\nplatforms like VR headsets.\nReferences\n[1] Aljaz Bozic, Pablo Palafox, Michael Zollhofer, Justus Thies,\nAngela Dai, and Matthias Nie\u00dfner.\nNeural deformation\ngraphs for globally-consistent non-rigid reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1450\u20131459, 2021. 2\n[2] Andrei Burov, Matthias Nie\u00dfner, and Justus Thies. Dynamic\nsurface function networks for clothed human bodies, 2021.\n2\n[3] Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang, and\nJuyong Zhang.\nNeural surface reconstruction of dynamic\nscenes with monocular rgb-d camera. In Thirty-sixth Confer-\nence on Neural Information Processing Systems (NeurIPS),\n2022. 3\n[4] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 130\u2013141, 2023. 3\n[5] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part affinity\nfields. In Computer Vision and Pattern Recognition (CVPR),\n2017. 5\n[6] Matteo Carandini, Jonathan B Demb, Valerio Mante, David J\nTolhurst, Yang Dan, Bruno A Olshausen, Jack L Gallant, and\nNicole C Rust. Do we know what the early visual system\ndoes? Journal of Neuroscience, 25(46):10577\u201310597, 2005.\n5\n[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 2, 3\n[8] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\nand Andreas Geiger. Snarf: Differentiable forward skinning\nfor animating non-rigid neural implicit shapes. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 11594\u201311604, 2021. 2\n[9] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-\nnis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,\nand Steve Sullivan. High-quality streamable free-viewpoint\nvideo.\nACM Transactions on Graphics (TOG), 34(4):69,\n2015. 1\n[10] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip\nDavidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts\nEscolano, Christoph Rhemann, David Kim, Jonathan Taylor,\net al. Fusion4d: Real-time performance capture of challeng-\ning scenes. ACM Transactions on Graphics (ToG), 35(4):\n1\u201313, 2016. 2, 3\n[11] Mingsong Dou, Philip Davidson, Sean Ryan Fanello, Sameh\nKhamis, Adarsh Kowdle, Christoph Rhemann, Vladimir\nTankovich, and Shahram Izadi. Motion2fusion: Real-time\nvolumetric performance capture. ACM Trans. Graph., 36(6):\n246:1\u2013246:16, 2017. 1, 2\n[12] Jarek Duda. Asymmetric numeral systems: entropy coding\ncombining speed of huffman coding with compression rate\nof arithmetic coding. arXiv preprint arXiv:1311.2540, 2013.\n5\n[13] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12479\u201312488, 2023. 2,\n3\n[14] Guy Gafni, Justus Thies, Michael Zollh\u00a8ofer, and Matthias\nNie\u00dfner. Dynamic neural radiance fields for monocular 4d\nfacial avatar reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 8649\u20138658, 2021. 1\n[15] Kaiwen Guo, Feng Xu, Yangang Wang, Yebin Liu, and\nQionghai Dai. Robust Non-Rigid Motion Tracking and Sur-\nface Reconstruction Using L0 Regularization. In Proceed-\nings of the IEEE International Conference on Computer Vi-\nsion, pages 3083\u20133091, 2015. 2\n[16] Kaiwen Guo, Feng Xu, Tao Yu, Xiaoyang Liu, Qionghai Dai,\nand Yebin Liu. Real-time geometry, albedo and motion re-\nconstruction using a single rgbd camera. ACM Transactions\non Graphics (TOG), 2017. 3\n[17] Yannan He, Anqi Pang, Xin Chen, Han Liang, Minye Wu,\nYuexin Ma, and Lan Xu. Challencap: Monocular 3d cap-\nture of challenging human performances using multi-modal\nreferences. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2021. 2\n[18] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao,\nXiao Liu, and Yuewen Ma.\nTri-miprf: Tri-mip represen-\ntation for efficient anti-aliasing neural radiance fields.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 19774\u201319783, 2023. 3\n[19] Mustafa Is\u00b8\u0131k, Martin R\u00a8unz, Markos Georgopoulos, Taras\nKhakhulin, Jonathan Starck, Lourdes Agapito, and Matthias\nNie\u00dfner. Humanrf: High-fidelity neural radiance fields for\nhumans in motion. ACM Transactions on Graphics (TOG),\n42(4):1\u201312, 2023. 2, 3, 6, 7\n[20] Yue Jiang, Marc Habermann, Vladislav Golyanik, and Chris-\ntian Theobalt. Hifecap: Monocular high-fidelity and expres-\nsive capture of human performances. In BMVC, 2022. 2\n[21] Yuheng Jiang, Suyi Jiang, Guoxing Sun, Zhuo Su, Kai-\nwen Guo, Minye Wu, Jingyi Yu, and Lan Xu. Neuralhofu-\nsion: Neural volumetric rendering under human-object in-\nteractions.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6155\u2013\n6165, 2022. 2\n[22] Yuheng Jiang, Kaixin Yao, Zhuo Su, Zhehao Shen, Haimin\nLuo, and Lan Xu. Instant-nvr: Instant neural volumetric ren-\ndering for human-object interactions from monocular rgbd\nstream.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 595\u2013605,\n2023. 3\n[23] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-\nture: A 3d deformation model for tracking faces, hands, and\nbodies. In The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2018. 1\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 2, 3, 7\n[25] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollh\u00a8ofer,\nJ\u00a8urgen Gall, Angjoo Kanazawa, and Christoph Lassner.\nTava: Template-free animatable volumetric actors. In Eu-\nropean Conference on Computer Vision, pages 419\u2013436.\nSpringer, 2022. 3\n[26] Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng,\nand Matthias Nie\u00dfner. 4dcomplete: Non-rigid motion es-\ntimation beyond the observable surface. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 2021. 2\n[27] Zhi Li, Anne Aaron, Ioannis Katsavounidis, Anush Moorthy,\nMegha Manohara, et al. Toward a practical perceptual video\nquality metric. The Netflix Tech Blog, 6(2):2, 2016. 7\n[28] Zhe Li, Tao Yu, Zerong Zheng, Kaiwen Guo, and Yebin Liu.\nPosefusion: Pose-guided selective fusion for single-view hu-\nman volumetric capture. In IEEE Conference on Computer\nVision and Pattern Recognition, 2021. 2\n[29] Zhe Li, Zerong Zheng, Hongwen Zhang, Chaonan Ji, and\nYebin Liu.\nAvatarcap:\nAnimatable avatar conditioned\nmonocular human volumetric capture. In ECCV, 2022. 2\n[30] Zujie Liang and Fan Liang. Transpcc: Towards deep point\ncloud compression via transformers.\nIn Proceedings of\nthe 2022 International Conference on Multimedia Retrieval,\npages 1\u20135, 2022. 3\n[31] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,\nHujun Bao, and Xiaowei Zhou.\nEfficient neural radiance\nfields for interactive free-viewpoint video. In SIGGRAPH\nAsia Conference Proceedings, 2022. 2, 3\n[32] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hu-\njun Bao, and Xiaowei Zhou. Im4d: High-fidelity and real-\ntime novel view synthesis for dynamic scenes. arXiv preprint\narXiv:2310.08585, 2023. 3\n[33] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta,\nBrian L Curless, Steven M Seitz, and Ira Kemelmacher-\nShlizerman. Real-time high-resolution background matting.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8762\u20138771, 2021. 5\n[34] Lingjie Liu, Weipeng Xu, Marc Habermann, Michael\nZollh\u00a8ofer, Florian Bernard, Hyeongwoo Kim, Wenping\nWang, and Christian Theobalt.\nNeural human video ren-\ndering by learning dynamic textures and rendering-to-video\ntranslation. IEEE Transactions on Visualization and Com-\nputer Graphics, PP:1\u20131, 2020. 2\n[35] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. Smpl: A skinned multi-\nperson linear model.\nACM Trans. Graph., 34(6):248:1\u2013\n248:16, 2015. 2, 3\n[36] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan.\nDynamic 3d gaussians:\nTracking\nby persistent dynamic view synthesis.\narXiv preprint\narXiv:2308.09713, 2023. 2, 3, 4, 6, 7\n[37] Haimin Luo, Teng Xu, Yuheng Jiang, Chenglin Zhou, Qiwei\nQiu, Yingliang Zhang, Wei Yang, Lan Xu, and Jingyi Yu.\nArtemis: Articulated neural pets with appearance and motion\nsynthesis. ACM Trans. Graph., 41(4), 2022. 2\n[38] Qianli Ma, Jinlong Yang, Michael J. Black, and Siyu Tang.\nNeural point-based shape modeling of humans in challeng-\ning clothing. In 2022 International Conference on 3D Vision\n(3DV), 2022. 2\n[39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Computer Vision \u2013 ECCV 2020, pages 405\u2013421,\nCham, 2020. Springer International Publishing. 1\n[40] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 2, 3\n[41] Marcus J Nadenau,\nJulien Reichel,\nand Murat Kunt.\nWavelet-based color image compression: exploiting the con-\ntrast sensitivity function. IEEE Transactions on image pro-\ncessing, 12(1):58\u201370, 2003. 3\n[42] Richard A Newcombe, Dieter Fox, and Steven M Seitz.\nDynamicfusion: Reconstruction and tracking of non-rigid\nscenes in real-time. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 343\u2013352,\n2015. 1, 2, 3\n[43] Pablo Palafox, Alja\u02c7z Bo\u02c7zi\u02c7c, Justus Thies, Matthias Nie\u00dfner,\nand Angela Dai. Npms: Neural parametric models for 3d de-\nformable shapes. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 12695\u201312705,\n2021. 2\n[44] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5865\u20135874, 2021. 1\n[45] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 40(6), 2021.\n[46] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan\nZhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-\nmatable neural radiance fields for modeling dynamic human\nbodies. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 14314\u201314323, 2021. 1\n[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9054\u20139063, 2021. 3\n[48] Sergey Prokudin,\nQianli Ma,\nMaxime Raafat,\nJulien\nValentin, and Siyu Tang. Dynamic point fields. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 7964\u20137976, 2023. 2\n[49] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer.\nD-nerf: Neural radiance fields\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10318\u201310327, 2021. 3\n[50] Maurice Quach, Giuseppe Valenzise, and Frederic Dufaux.\nLearning convolutional transforms for lossy point cloud ge-\nometry compression. In 2019 IEEE international conference\non image processing (ICIP), pages 4320\u20134324. IEEE, 2019.\n3\n[51] Maurice Quach, Giuseppe Valenzise, and Frederic Dufaux.\nImproved deep point cloud geometry compression. In 2020\nIEEE 22nd International Workshop on Multimedia Signal\nProcessing (MMSP), pages 1\u20136. IEEE, 2020. 3\n[52] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srini-\nvasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Pe-\nter Hedman. Merf: Memory-efficient radiance fields for real-\ntime view synthesis in unbounded scenes. ACM Transactions\non Graphics (TOG), 42(4):1\u201312, 2023. 3\n[53] Ruwen Schnabel and Reinhard Klein. Octree-based point-\ncloud compression. PBG@ SIGGRAPH, 3, 2006. 3\n[54] Sebastian Schwarz, Marius Preda, Vittorio Baroncini, Mad-\nhukar Budagavi, Pablo Cesar, Philip A Chou, Robert A Co-\nhen, Maja Krivoku\u00b4ca, S\u00b4ebastien Lasserre, Zhu Li, et al.\nEmerging mpeg standards for point cloud compression.\nIEEE Journal on Emerging and Selected Topics in Circuits\nand Systems, 9(1):133\u2013148, 2018. 3\n[55] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16632\u2013\n16642, 2023. 3\n[56] Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Jose\nZarate, Julien Valentin, Jie Song, and Otmar Hilliges. X-\navatar: Expressive human avatars.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16911\u201316921, 2023. 3\n[57] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger.\nNerf-\nplayer: A streamable dynamic scene representation with de-\ncomposed neural radiance fields. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(5):2732\u20132742, 2023.\n2, 3\n[58] Zhuo Su, Lan Xu, Zerong Zheng, Tao Yu, Yebin Liu, and Lu\nFang. Robustfusion: Human volumetric capture with data-\ndriven visual cues using a rgbd camera. In Computer Vision\n\u2013 ECCV 2020, pages 246\u2013264, Cham, 2020. Springer Inter-\nnational Publishing. 2\n[59] Zhuo Su, Lan Xu, Dawei Zhong, Zhong Li, Fan Deng,\nShuxue Quan, and Lu Fang. Robustfusion: Robust volumet-\nric performance reconstruction under human-object interac-\ntions from monocular rgbd stream. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2022. 2\n[60] Robert W Sumner, Johannes Schmid, and Mark Pauly. Em-\nbedded deformation for shape manipulation. ACM Transac-\ntions on Graphics (TOG), 26(3):80, 2007. 1, 2, 3\n[61] Guoxing Sun, Xin Chen, Yizhang Chen, Anqi Pang, Pei\nLin, Yuheng Jiang, Lan Xu, Jingya Wang, and Jingyi Yu.\nNeural free-viewpoint performance rendering under complex\nhuman-object interactions. In Proceedings of the 29th ACM\nInternational Conference on Multimedia, 2021. 2\n[62] Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Minye\nWu, Kaiwen Guo, and Lan Xu.\nNeuralhumanfvv: Real-\ntime neural volumetric human performance rendering using\nrgb cameras. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6226\u2013\n6237, 2021. 2\n[63] Dorina Thanou, Philip A Chou, and Pascal Frossard. Graph-\nbased compression of dynamic 3d point cloud sequences.\nIEEE Transactions on Image Processing, 25(4):1765\u20131778,\n2016. 3\n[64] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\nZollh\u00a8ofer, Christoph Lassner, and Christian Theobalt. Non-\nrigid neural radiance fields: Reconstruction and novel view\nsynthesis of a dynamic scene from monocular video. In IEEE\nInternational Conference on Computer Vision (ICCV). IEEE,\n2021. 1, 3\n[65] Shaofei Wang, Andreas Geiger, and Siyu Tang.\nLocally\naware piecewise transformation fields for 3d human mesh\nregistration. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 2\n[66] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu\nTang.\nArah: Animatable volume rendering of articulated\nhuman sdfs. In European Conference on Computer Vision,\n2022. 1\n[67] Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-\nilidis, Christian Theobalt, and Lingjie Liu.\nNeus2: Fast\nlearning of neural implicit surfaces for multi-view recon-\nstruction.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 3295\u20133306, 2023. 2,\n3, 6, 7\n[68] Chung-Yi Weng,\nBrian Curless,\nPratul P. Srinivasan,\nJonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmanNeRF: Free-viewpoint rendering of moving people from\nmonocular video. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 16210\u201316220, 2022. 3\n[69] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023. 2, 3, 4\n[70] Lan Xu, Wei Cheng, Kaiwen Guo, Lei Han, Yebin Liu, and\nLu Fang. Flyfusion: Realtime dynamic scene reconstruction\nusing a flying depth camera. IEEE transactions on visualiza-\ntion and computer graphics, 27(1):68\u201382, 2019. 3\n[71] Lan Xu, Zhuo Su, Lei Han, Tao Yu, Yebin Liu, and Lu Fang.\nUnstructuredfusion: realtime 4d geometry and texture recon-\nstruction using commercial rgbd cameras. IEEE transactions\non pattern analysis and machine intelligence, 42(10):2508\u2013\n2522, 2019. 2\n[72] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin.\nDeformable 3d gaussians for\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\npreprint arXiv:2309.13101, 2023. 2, 3, 4\n[73] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\nZhang. Real-time photorealistic dynamic scene representa-\ntion and rendering with 4d gaussian splatting. arXiv preprint\narXiv:2310.10642, 2023. 2, 3\n[74] Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai\nDai, Hao Li, Gerard Pons-Moll, and Yebin Liu. Doublefu-\nsion: Real-time capture of human performances with inner\nbody shapes from a single depth sensor.\nTransactions on\nPattern Analysis and Machine Intelligence (TPAMI), 2019.\n2, 3\n[75] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-\nhai Dai, and Yebin Liu. Function4d: Real-time human vol-\numetric capture from very sparse consumer rgbd sensors. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5746\u20135756, 2021. 2\n[76] Hongwen Zhang, Siyou Lin, Ruizhi Shao, Yuxiang Zhang,\nZerong Zheng, Han Huang, Yandong Guo, and Yebin Liu.\nCloset: Modeling clothed humans on continuous surface\nwith explicit template decomposition. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2023. 2\n[77] Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao\nWang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang, Minye\nWu, Lan Xu, et al. Human performance modeling and ren-\ndering via neural animated mesh.\nACM Transactions on\nGraphics (TOG), 41(6):1\u201317, 2022. 3, 6, 7\n[78] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang\nZhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently gen-\nerated human radiance field from sparse inputs. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 7743\u20137753, 2022. 3\n[79] Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning\nLiu, and Yebin Liu. Avatarrex: Real-time expressive full-\nbody avatars. ACM Transactions on Graphics (TOG), 42(4),\n2023. 2\n[80] Michael Zollh\u00a8ofer,\nMatthias Nie\u00dfner,\nShahram Izadi,\nChristoph Rehmann, Christopher Zach, Matthew Fisher,\nChenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian\nTheobalt, et al. Real-time Non-rigid Reconstruction using\nan RGB-D Camera. ACM Transactions on Graphics (TOG),\n33(4):156, 2014. 2\n"
  },
  {
    "title": "Context Diffusion: In-Context Aware Image Generation",
    "link": "https://arxiv.org/pdf/2312.03584.pdf",
    "upvote": "13",
    "text": "Context Diffusion: In-Context Aware Image Generation\nIvona Najdenkoska1,2*\nAnimesh Sinha2\nAbhimanyu Dubey2\nDhruv Mahajan2\nVignesh Ramanathan2\nFilip Radenovic2\n1University of Amsterdam\n2GenAI, Meta\nFigure 1. Illustrating in-context aware image generation with Context Diffusion.\nTop row: HED-to-image as an in-domain task;\nMiddle row: canny-to-image as an out-of-domain task. Our model enables learning from context with and without prompts. The counterpart\nmodel, Prompt Diffusion [43] does not leverage the context if the prompt is not provided, hinting at its over-reliance on textual guidance;\nBottom row: Few-shot setting for sketch-to-image task. More context examples help in learning stronger visual cues, even without prompts.\nAbstract\nWe propose Context Diffusion, a diffusion-based frame-\nwork that enables image generation models to learn from\nvisual examples presented in context. Recent work tack-\nles such in-context learning for image generation, where a\nquery image is provided alongside context examples and text\nprompts. However, the quality and fidelity of the generated\nimages deteriorate when the prompt is not present, demon-\nstrating that these models are unable to truly learn from the\nvisual context. To address this, we propose a novel frame-\n*Work done during an internship at GenAI, Meta.\nwork that separates the encoding of the visual context and\npreserving the structure of the query images. This results in\nthe ability to learn from the visual context and text prompts,\nbut also from either one of them. Furthermore, we enable\nour model to handle few-shot settings, to effectively address\ndiverse in-context learning scenarios. Our experiments and\nuser study demonstrate that Context Diffusion excels in both\nin-domain and out-of-domain tasks, resulting in an over-\nall enhancement in image quality and fidelity compared to\ncounterpart models.\n1\narXiv:2312.03584v1  [cs.CV]  6 Dec 2023\n1. Introduction\nGenerative models are witnessing major advances, both in\nnatural language [6, 9, 27, 40, 45, 49] and media genera-\ntion [5, 18, 28, 33, 36]. Large language models in particu-\nlar have shown impressive in-context learning capabilities\n[6, 46]. This is the ability of a model to learn from a few\nsamples on the fly, without any gradient-based updates, and\nextend it to new tasks. However, for generative models\nin computer vision, learning from context examples is still\nunder-explored.\nPrompt Diffusion [43] is perhaps the closest line of work\nthat explicitly supports a single source-target image pair as\na context example for image generation. It builds on the\npopular ControlNet [48] model which introduced the idea of\nvisually controllable diffusion models for image generation.\nSpecifically, Prompt Diffusion attempts to learn the visual\nmapping from the source image to the target context image\nand applies it to a new query image, by also using a prompt\nfor text-based guidance. However, we empirically observed\nthat this model struggles to leverage the image pairs when\nthe text prompt is absent. This results in low fidelity to the\nvisual context examples, particularly when the examples are\nfrom a different domain than what is seen during training.\nFor instance, if the source-target pairs show specific styles,\nthey cannot be learned during inference just from the context\nexamples. This is seen in the first row of Figure 1 where\nPrompt Diffusion is unable to learn the \u201csnowy\u201d style from\nthe context unless prompted through text. Additionally, it\ndoes not trivially support multiple source-target images as\ncontext examples, which limits the visual information that\ncan be provided to the model.\nWe address these challenges with our proposed Context\nDiffusion model that can (i) effectively learn from visual\ncontext examples as well as follow text prompts and (ii)\nsupport a variable number of context examples since visual\ncharacteristics can be defined with more than a single exam-\nple. Unlike Prompt Diffusion, our model does not require\npaired context examples, but just one or more \u201ctarget\u201d con-\ntext images serving as examples of the desired output and a\nsingle query image providing visual structure. The reason\nfor using target examples as context is that the source im-\nages are derived from the target itself and do not provide\nany additional information for the task. Typically, the query\nimage provides guidance for the output structure through\nedges, depth, segmentation maps, etc. On the other hand,\nthe context images provide hints for finer details like style,\ntexture, colors, and object appearances desired in the output\nimage.\nIt is important to note the difficulty in controlling both\naspects of the output image solely through the control mech-\nanism used in a ControlNet-like model. The \u201ccontrol\u201d part\nof the model is very effective in capturing high-level struc-\nture. However, finer details are better captured through the\nconditioning mechanism. A similar observation is made in\nprevious works such as textual inversion [12] and retrieval-\naugmented image generation [4, 7], where object appearance\nis preserved by encoding it through conditioning. Inspired\nby this, we inject information from the context images into\nthe network in a similar fashion as text conditioning. In\nparticular, we sum the visual embeddings from the context\nimages and place them alongside the text embeddings in the\ncross-attention layers of the diffusion model. This allows\nstronger reliance on the visual input from the context and\nalso supports multiple context images. The structure from\nthe query image is preserved by passing it as a control signal\nto the network in a similar manner as ControlNet [48].\nWe follow a similar training strategy as Prompt Diffusion\n[43], by learning from six different tasks using generated\nimages and their maps. At inference time, we use a query\nimage to define the target structure and one or more context\nimages to provide finer visual signals, alongside an optional\ntext prompt. Our experiments study the generation ability\nof Context Diffusion for in-domain tasks, such as using\nHED, segmentation, and depth maps to generate real images\nand vice versa. We show the flexibility of our model to\npreserve structure from the query image and transfer other\nvisual signals from the context even when the text prompt\nis missing. Moreover, to properly study in-context learning\nabilities, we experiment with unseen, out-of-domain tasks,\nsuch as handling sketches as query images, image editing,\nand more. This demonstrates the generalization abilities of\nContext Diffusion, unlike previous works. Furthermore, for\nsuch tasks, using multiple images as context helps improve\nthe fidelity of the generated images to the context.\nContributions.\n(i) We propose Context Diffusion, an in-\ncontext aware image generation framework. It enables pre-\ntrained diffusion models to use visual context examples to\ncontrol the appearance of the output image, alongside a query\nimage that defines structure and an optional text prompt. (ii)\nWe enable the use of multiple context images as \u201cfew-shot\u201d\nexamples for image generation. To the best of our knowl-\nedge, this is the first work to explore such a \u201cfew-shot\u201d setup\nfor in-context aware image generation. (iii) We conduct\nextensive offline and online (human) evaluations that show\nthat our framework can handle several in-domain and out-of-\ndomain tasks and demonstrates improved performance over\nthe counterpart model.\n2. Related Work\nDiffusion-based Image Generation.\nRecent advance-\nments in diffusion models, first introduced in [38] have ex-\nhibited huge success in text-to-image generation tasks [10,\n17, 31, 32, 36]. Enhancements have been achieved through\nvarious training [10, 33, 36] and sampling [24, 39, 44] tech-\nniques. For instance, DALLE-2 [32] proposed an architec-\n2\nture encompassing several stages, by encoding text with\nCLIP [30] language encoder and decoding images from the\nencoded text embeddings, followed by Imagen [36] which\nshowed that up-scaling the text encoder largely improves\nthe text fidelity. Furthermore, the Latent Diffusion Model\n(LDM) [33] investigated the diffusion process by applying it\nto a low-resolution latent space and even further improved\nthe training efficiency. However, all these models only take a\ntext prompt as input, which restricts the flexibility of the gen-\neration process as it requires extensive prompt engineering\nto obtain the desired image outputs.\nControllable Image Generation.\nAdding more control\nto the image generation process, besides the text prompts,\nhelps in overall customization and task-specific image gen-\neration. Recent text-conditioned models focus on adjusting\nmodels by task-specific fine-tuning [8, 12, 35], injecting\nconditioning maps, like segmentation maps, sketches or key-\npoints [2, 3, 11, 23, 29, 48], or exploring editing abilities\n[5, 13, 15, 25, 26]. For instance, SpaText [2] is using seg-\nmentation maps where each region of interest is annotated\nwith text, to better control the layout of the generated image.\nModels like GLIGEN [22] inject grounding information,\nsuch as bounding boxes or edge maps, into new trainable lay-\ners via a gated cross-attention mechanism. ControlNet [48],\nas a recent state-of-the art in controllable image generation\npresents a general framework for adding spatial conditioning\ncontrols. UniControl [29] extends ControlNet by unifying\nvarious image map conditions into a single image genera-\ntion framework. Other works, such as Re-Imagen [7] and\nRDM [4], employ retrieval for choosing images given a text\nprompt, for controlling the generation process.\nOur approach differs from these models in several aspects.\nWe support in-context learning from visual examples as an\naddition to the textual prompts and query images. This al-\nlows learning new tasks using the visual context only, which\nyields a more flexible framework. Additionally, we use only\na few of the image maps considered by ControlNet and Uni-\nControl for training, namely HED, segmentation, and depth\nmaps, and demonstrate the generalization ability to the other\nvisual controls i.e. query images.\nIn-Context Learning in Image Generation.\nIn-context\nlearning is vastly explored both in language-only [6, 20, 45,\n46] and visual-language models [1, 19, 21, 41], as an emer-\ngent ability enabling to learn new tasks without additional\ngradient-based updates. However, the ability to learn from\ncontext examples is lagging behind in image generation.\nPrompt Diffusion [43] presents such a framework, by ex-\ntending the control abilities of ControlNet [48] and training\nfor in-context image generation. They consider a vision-\nlanguage prompt encompassing a source-target image pair\nand a text prompt, which is used to jointly train the model on\nsix different tasks. However, Prompt Diffusion only shows\ngood performance when both the context images and prompt\nare present. In case the text prompt is not present, the model\nexhibits deteriorating performance, suggesting its inability\nto learn efficiently from the visual examples, as shown in\nFigures 1, 3, 4, 5, 6, and 7.\nDifferent from them, we aim to develop a model able\nto generate images of good quality even when only one of\nthe conditions (visual context or text prompt) is present,\nboth for in-domain and out-of-domain tasks. Another work\ntackling image generation with visual examples is Prompt-\nFree Diffusion [47]. It focuses only on having an image as a\ncontext, ie, a visual condition, while completely removing\nthe ability to process textual prompts. This is the major\ndifference compared to our Context Diffusion, since we aim\nto support both scenarios: having the context images and/or\ntext prompts. Additionally, none of these related works\nconsider settings with multiple examples in context, namely,\nfew-shot scenarios. We propose a framework that can handle\na variable number of context images, helpful for enriching\nthe visual context representation.\n3. Methodology\n3.1. Preliminaries\nDiffusion models are a class of generative models that con-\nvert Gaussian noise into samples from a learned data dis-\ntribution via an iterative denoising process. In the case of\ndiffusion models for text-to-image generation, starting from\nnoise zt, the model produces less noisy samples zt\u22121, . . . , z0,\nconditioned on caption representation c at every time step t.\nTo learn such a model f\u03b8 parameterized by \u03b8, for each\nstep t, the diffusion training objective L solves the denoising\nproblem on noisy representations zt, as follows:\nmin\n\u03b8\nL = Ez,\u03f5\u223dN (0,1),t\n\u0002\n\u2225 \u03f5 \u2212 f\u03b8(zt, t, c) \u22252\n2\n\u0003\n,\n(1)\nWith large-scale training, the model f\u03b8 is trained to denoise\nzt based on text information as the main source of control.\nTo enable more control over the generation process, we\nfollow the ControlNet setup [48], for encoding the structure\nof the desired output via a query image as visual control.\nNote that, according to in-context learning parlance, we\nuse query image interchangeably with visual control. In\nthis paper, we extend the c representation in Eq. (1), by\nadding image examples as additional guidance besides the\ntext prompt. Namely, we inject visual embeddings obtained\nby a pre-trained vision encoder fimg with fixed parameters,\nin a similar fashion as the prompt embeddings.\n3.2. Context Diffusion Architecture\nThe model f\u03b8 is essentially a UNet architecture [34], with\nan encoder, a middle block, and a skip-connected decoder.\n3\nFigure 2. Architecture of Context Diffusion. It consists of several\nmodules: vision and text encoders for encoding the text prompt\nand visual context respectively, an LDM backbone for handling\nthe image generation process, and an additional LDM encoder for\nprocessing the query image as a visual control. Note that here we\nshow one visual context example, however, the model is trained\nusing a variable number of such examples.\nThese modules denoted as LDM encoder, mid, and decoder\nin Figure 2, are built out of standard ResNet [14] and Trans-\nformer blocks [42] which contain several cross-attention and\nself-attention mechanisms. The core of conditional-diffusion\nmodels is encoding the conditional information [33], based\non which zt is generated at a given time step t. We differen-\ntiate two types of such conditional information: the visual\ncontext V encompassing images and the text prompt c, to\ndefine our conditioning information: y = (c, V), where\nV = [v1, . . . vk] and k are the number of images. Addition-\nally, we consider a visual control image, i.e., the query image\nthat serves to define the structure of the output denoted as q.\nPrompt encoding.\nTo perform the encoding of the textual\nprompt c we use a pre-trained language encoder ftext with\nfixed parameters to obtain the embeddings. Particularly,\nwe obtain hc = {hc\n0, . . . , hc\nN c} = ftext(c), where N c is the\nnumber of text tokens, hc\ni \u2208 Rdc and dc is the dimensionality\nof the textual token embeddings.\nVisual context encoding.\nWe hypothesize that the vi-\nsual context V should be at the same level of condition-\ning as the textual one.\nTherefore, we follow a similar\nstrategy for encoding the visual context, by using a pre-\ntrained, fixed image encoder fimg. Given a visual context\nV consisting of k-images, we encode each image vi as\nhvi = {hvi\n0 , . . . , hvi\nN v} = fimg(vi), where N v is the num-\nber of tokens per image, hvi \u2208 Rdv and dv is the dimen-\nsionality of the visual token embeddings. The final repre-\nsentation of the visual context is obtained by simply sum-\nming the corresponding visual tokens of all k-images, where\nk \u2208 {1, 2, 3}, yielding hV = Pk\ni=1 hvi. Additionally, we\nadd a linear projection layer to map the visual embedding\ndimension dv to the language dimension dc.\nModified cross-attention.\nGiven the standard cross-\nattention block in LDMs, defined with queries Q, keys K,\nand values V , the noisy representation zt is used as a query,\nwhereas the text encoding hc is used as a representation of\nthe keys and values, as follows:\nzt = zt + CrossAtt(Q = zt, K = V = hc).\n(2)\nOur framework is slightly different from this definition since\nwe also consider visual information in the conditioning.\nTherefore, after obtaining both visual and textual embed-\ndings we simply concatenate them to obtain [hc, hV], illus-\ntrated in the bottom left corner of Figure 2. We hypothesize\nthat the visual and textual conditioning should be at the\nsame level, thus the input to the cross-attention block in (2)\nchanges as follows:\nzt = zt + CrossAtt(Q = zt, K = V = [hc, hV]).\n(3)\nVisual control encoding.\nTo enable the ingestion of the\nquery image as visual control, we follow ControlNet setup\n[48]. First, the image is encoded using a few convolutional\nlayers. Then, a copy of the LDM encoder is used to process\nthe encoded query image q. This trainable LDM encoder\ncopy is connected to the original LDM backbone using zero\nconvolution layers, as shown in Figure 2.\n3.3. Multi-task Training Procedure\nWe use a pre-trained image generation model to adapt it\nwith visual context injection. We use the original denoising\nobjective defined in (1), with q being the query image and\nthe modified conditioning information y:\nmin\n\u03b8\nL = Ez,\u03f5\u223dN (0,1),t\n\u0002\n\u2225 \u03f5 \u2212 f\u03b8(zt, t, y, q) \u22252\n2\n\u0003\n.\n(4)\nTo train with this objective, we use a collection of tasks for\njoint end-to-end training, similar to [43]. Different from\nthem, we use a visual context sequence consisting of a k-\nimages and a text prompt, together with a query image.\nSpecifically, k is randomly chosen at batch construction.\nThe goal of such training is to leverage any visual character-\nistics from the context images and to apply them along with\nthe text prompt to the query image.\n4\nPrompt dropout.\nWe aim to achieve learning from the\ncontext images by avoiding over-reliance on the text prompts.\nStarting from a pre-trained text-to-image model means the\nability to generate images given a text prompt is already\nstrong. Therefore, to enforce the model to pick up cues from\nan additional conditioning signal i.e. the visual context, we\napply random replacement of 50% of the text prompts with\nempty strings, similar to [48], which empirically showed to\nbe an important step.\n4. Experiments\n4.1. Experimental Setup\nDatasets.\nTo train our model, we use a dataset that consists\nof 310k synthetic images and caption pairs, similar to Prompt\nDiffusion [43]. Following their training setup, we extract\nthree image maps: HED, segmentation, and depth maps from\nthe training images. During training, for map-to-image tasks,\nthe image maps serve as queries, and real images are used\nfor visual context, while for image-to-map tasks, the real\nimages serve as queries, and image maps are used for visual\ncontext. Note that the prompts and visual context are related\nand describe the same conditioning signal.\nAt inference time, we use the test partition of the dataset\nto test the ability to learn from context. To demonstrate\nthe generalization abilities of Context Diffusion to out-of-\ndomain tasks, we extract other image maps, such as normal\nmaps, canny edges, and scribble maps. Also, we consider\nediting tasks by using real images as queries. To further\ntest the generalization abilities, we use hand-drawn sketches\nfrom the Sketchy dataset [37], where the sketch is the query\nimage, and the real images are the context. This dataset does\nnot provide captions, therefore we construct text prompts\nusing a template: \u201cA professional, detailed, high-quality\nimage of object name\u201d, following [48].\nImplementation details.\nThe backbone of our model fol-\nlows a vanilla ControlNet architecture, initialized in the same\nway as [48]. We train such a model using the data setup ex-\nplained above. In particular, only the encoder of the LDM\nbackbone is kept frozen and its copy which processes the\nquery image is trained. For the encoding of the context\nimages and prompts, we use frozen CLIP ViT-L/14 [30]\nencoders. We take the last-layer hidden states as representa-\ntions of both the context images and prompts. The model is\ntrained with a fixed learning rate of 1e-4 for 50K iterations,\nusing 256 \u00d7 256 images. We use a global batch size of 512\nfor all runs. At inference time, we apply DDIM [39] as a\ndefault sampler, with 50 steps and a guidance weight of 3.\nRegarding the computational resources, the model is trained\nusing 8 NVIDIA A100 GPUs.\nFigure 3. In-domain tasks comparison to Prompt Diffusion [43]:\nExamples of {HED, segmentation, depth}-to-image as forward\ntasks and image-to-{HED, segmentation, depth} as reverse tasks,\nwith visual context and prompt (C+P) given as conditioning.\nUser study setup.\nTo better quantify the performance of\nour model, we perform an online evaluation in the form of a\nuser study to compare our model to Prompt Diffusion [43].\nA total of 10 in-house annotators participated in the study,\nannotating 240 test samples. We present two generated\nimages from the models, randomly annotated as A and B,\nalongside the given visual context, query image, and prompt.\nThen, each annotator chooses either a preferred image or\nboth as equally preferred. We consider various in-domain\nand out-of-domain tasks for evaluation, across three distinct\nscenarios: using both visual context and prompts, using\nonly visual context and only prompt. Considering these\nscenarios examines to what extent the models can learn\n5\nfrom the conditional information in a balanced manner and\nwhether they suffer when one input modality is not present.\nAutomated metrics.\nIn addition to the user study, we also\nuse offline automated metrics to further evaluate the perfor-\nmance of our model. In particular, we report FID scores [16]\nfor map-to-image tasks and RMSE scores for the image-to-\nmap tasks. We use 5000 test images per setting for each task,\ngenerated by our model and Prompt Diffusion.\n4.2. Results & Discussion\nIn this section, we compare our model against the most sim-\nilar approach in the literature, i.e. Prompt Diffusion [43].\nPrompt Diffusion expects a source-target pair of context im-\nages as an input, while in contrast our approach only requires\ncontext i.e. target images. More analysis regarding this is\nprovided in the supplementary materials. It is important to\nnotice that in all comparisons we follow the source-target\nformat of the input for Prompt Diffusion output image gen-\neration, but we omit the visualization of the source image\nfrom the Figures, to have consistent visualizations for both\nmethods. Additionally, both approaches operate with a query\nimage and textual prompt as additional inputs.\nWe compare the methods across two important gener-\nalization axes: (i) in-domain for seen and out-of-domain\nfor unseen tasks at training; (ii) visual context and prompt\n(C+P), context-only (C), and prompt-only (P) variations of\nconditioning. Finally, we present the results of our model on\nfew-shot setup when several context examples are given as\ninput. Prompt Diffusion does not support the few-shot setup.\n4.2.1\nData Domain\nIn-domain comparison.\nWe study the performance of\nmodels on the same data domain as the training data, but\non the test data that is set aside. This encompasses three\n\u201cforward\u201d tasks, i.e., the query image is either HED, seg-\nmentation, or a depth map while the expected output image\nis a real image, given the visual context and prompt in an\nadequate form. Similarly, we evaluate three \u201creverse\u201d tasks,\nwhere the query and output roles are reversed. For the pur-\npose of this discussion, we focus on the conditioning setup\nwhere both visual context and prompts (C+P) are given as\ninput. Figure 3 presents representative examples for each\nof the tasks: the first three rows depict forward tasks, while\nthe last three rows depict reverse tasks. It can be observed\nthat our model is able to generate images with better fi-\ndelity to the context images and prompts, by managing to\nmatch the specific colors and styles from the context. On the\nother hand, Prompt Diffusion outputs are more saturated and\nfail to leverage the visual characteristics from the context\n(green radish instead of red in the second row). We include\nFigure 4. Out-of-domain comparison to Prompt Diffusion [43]:\nImage edit, with visual context and prompt (C+P) as conditioning.\nmore examples in the supplementary materials. These ob-\nservations are further supported by user study presented in\nTable 1 (In-domain (C+P) column), as well as in offline met-\nrics comparison presented in Table 3 (C+P columns), where\nwe obtain satisfactory performance improvement (36.3% vs.\n28.5% win-rate) over Prompt Diffusion.\nOut-of-domain generalization.\nThe most advantageous\naspect of having a model that is an in-context learner is its\ncapacity to generalize to new tasks by observing the context\nexamples given as input at inference. Again, for the purpose\nof the discussion in this section, we focus on the condition-\ning setup where both visual context and prompt (C+P) are\ngiven as input. To test these generalization abilities, we\nconsider tasks outside of training domains: image editing\nwith representative examples in Figure 4; {sketch, normal\nmap, scribbles, canny edge}-to-image with representative\nexamples in Figure 5. In both figures, we observe noticeable\nimprovements over Prompt Diffusion [43]. It is apparent\nthat the visual characteristics of the context images are also\ntransferred in the output images. Furthermore, we select\nediting and sketch-to-image as representative out-of-domain\ntasks to perform a user study. We report the results in Table\n1 (Out-of-domain (C+P) column), where we observe great\nimprovements in win rate (52.3% vs. 26.9%), significantly\nhigher than for in-domain setup, showing the advantage of\nin-context aware image generation.\n4.2.2\nConditioning at inference\nUsing only visual context.\nTo better understand the effect\nof visual context examples on the model\u2019s performance, we\n6\nFigure 5. Out-of-domain comparison to Prompt Diffusion [43]:\n{sketch, normal map, scribble, canny edge}-to-image tasks. Visual\ncontext and prompt (C+P) are given as conditioning information.\nFigure 6. Conditioning comparison with Prompt Diffusion [43]:\nUsing visual context and prompt (C+P) and visual context-only (C)\nas conditioning, on both in-domain (image-to-HED) and out-of-\ndomain (editing, scribble-to-image, sketch-to-image) tasks.\nanalyze the outputs when the text prompt is not provided\n(empty string), i.e., only visual context is used as condition-\ning. This experiment gives strong insights into the model\u2019s\nability to perform in-context learning. We show representa-\ntive examples of this setup in Figure 6. It can be observed\nthat Prompt Diffusion [43] is unable to learn from the visual\nFigure 7. Zero-shot comparison to ControlNet [48] and Prompt\nDiffusion [48]: Using prompt-only (P) as conditioning.\nIn-domain\nOut-of-domain\nC+P\nC\nP\navg C+P\nC\nP\navg\nPD [43] 28.5\n4.5\n30.4 21.1 26.9 22.8 25.9 25.2\nOurs\n36.3 80.2 29.6 48.6 52.3 63.7 49.8 55.2\nTable 1. User study comparison to Prompt Diffusion (PD) [43]:\nIn-domain and out-of-domain tasks, considering different condi-\ntioning settings: context image and prompt (C+P), visual context-\nimage-only (C), prompt-only (P). We report the win rate as a per-\ncentage of winning votes for each model.\nOut-of-domain\nC+P\nC\navg\nOurs (1-shot)\n21.5\n28.3\n24.9\nOurs (3-shot)\n60.0\n50.2\n55.1\nTable 2. User study comparison for 1-shot vs. 3-shot setups:\nOut-of-domain tasks, considering different conditioning settings:\nvisual context and prompt (C+P), visual context-only (C). Note that\nthe \u201cprompt-only\u201d (P) setting corresponds to a zero-shot scenario\nand is not applicable here. We report the win rate as a percentage\nof winning votes for each model.\nexamples, indicating that it relies solely on the text caption\nas conditional information. We include this setting in the\nuser study and we report the results in Table 1 ((C) columns).\nOverall, we observe a significant performance gap between\nour model and Prompt Diffusion, both for in-domain (80.2%\nvs. 4.5% win-rate) and out-of-domain (63.7% vs. 22.8% win-\nrate) tasks. This result is additionally supported by the offline\nmetrics in Table 3 ((C) columns) for in-domain tasks, further\nstrengthening the observations that our model is able to truly\nleverage the visual context.\nUsing only text prompts.\nApart from being able to handle\nscenarios only with visual context, we aim to also support\nscenarios using only text prompts. To enable this setting, we\nsimply mask out the visual context by using black images.\n7\nFigure 8. Few-shot examples: Comparison between out-of-domain tasks (editing and sketch) using one context example with a text prompt,\nand one, two, and three shots of context examples with no text prompt. Our model is able to leverage multiple visual examples to handle\nscenarios when the text prompt is not present.\nFID (map-to-img) \u2193\nRMSE (img-to-map) \u2193\nHED-to-img\nseg-to-img\ndepth-to-img\nimg-to-HED\nimg-to-seg\nimg-to-depth\nC+P\nC\nP\nC+P\nC\nP\nC+P\nC\nP\nC+P\nC\nP\nC+P\nC\nP\nC+P\nC\nP\nPD [43] 12.8 22.5 15.1 16.7 25.1 17.2 15.9 27.0 18.1 0.15 0.33 0.15 0.32 0.41 0.32 0.14 0.34 0.14\nOurs\n12.3 17.7 14.8 13.4 19.0 18.5 12.9 18.5 17.5 0.11 0.11 0.16 0.29 0.28 0.30 0.14 0.13 0.13\nTable 3. Offline comparison to Prompt Diffusion (PD) [43] using automated metrics: FID and RMSE: In-domain tasks across three\ndifferent conditioning settings: visual context and prompt (C+P), visual context-only (C), prompt-only (P). Lower scores are better.\nThis essentially yields zero-shot settings, boiling down to\nhow ControlNet [48] is used at inference time. However, un-\nlike ControlNet which requires a separate model trained for\neach task, our Context Diffusion generalizes across a series\nof tasks. In Figure 7 we show representative examples of\nthis setting, comparing our model to ControlNet and Prompt\nDiffusion. It can be seen that our model is able to generate\nmore realistic images compared to ControlNet and also per-\nforms on par with Prompt Diffusion. Similar as before, we\nalso include this setting in the user study, reported in Table 1\n((P) columns). We observe that our performance is slightly\nworse to Prompt Diffusion on in-domain (29.6% vs. 30.4%)\nand much better on out-of-domain (49.8% vs. 25.9%) tasks.\nThis supports our observations that the Prompt Diffusion\napproach relies too much on the textual prompt, as well as\nsuffers in out-of-domain data regimes. Further, we com-\npare the automated metrics in Table 3 ((P) columns), again\nobserving comparable performance on in-domain tasks.\n4.2.3\nFew-shot visual context examples\nThe Context Diffusion architecture is flexible enough to ac-\ncommodate multiple context examples, enabling few-shot\nscenarios. Using one context example proved to be enough\nfor in-domain tasks, as seen in Figure 3. Therefore in the\nfew-shot experiments, we focus on the out-of-domain tasks,\nsuch as editing and sketch-to-image. In particular, we aug-\nment the visual context sequence with additional images,\ndepicting similar objects or scenes. Moreover, we look at\nscenarios where the textual information is not present since\nin that case, the model has to rely on the visual context only.\nAs can be seen from Figure 8, adding more context exam-\nples helps to strengthen the conditional visual representation,\nespecially when the prompt is not present. We also quantify\nthe performance by conducting a user study for the few-shot\nsettings, presented in Table 2. We are comparing our model\nwhen using one context example vs. using three examples.\nOverall we observe improved performance (55.1% vs. 24.0%\naverage win rate) when using three context images which\naligns with the qualitative observations. Note that in the\ncurrent experiments, we use 1 up to 3 shots as a representa-\ntive few-shot setting, however, our model can accommodate\nmore than 3 shots.\n5. Conclusion\nWe present an in-context-aware image generation framework\ncapable of learning from a variable number of visual context\nexamples and prompts. Our approach leverages both the\nvisual and text inputs on the same level, resulting in a\nframework able to learn in a balanced manner from the\nmultimodal inputs. Furthermore, learning from a few context\nexamples showed to be helpful in learning strong visual\ncharacteristics, especially if the prompt is not available. Our\nexperiments and user study demonstrate the applicability of\nour approach across diverse tasks and settings, confirming\nthe improved quality and fidelity over counterpart models.\n8\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 2022. 3\n[2] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,\nYaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,\nand Xi Yin. Spatext: Spatio-textual representation for con-\ntrollable image generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2023. 3\n[3] Dina Bashkirova, Jose Lezama, Kihyuk Sohn, Kate Saenko,\nand Irfan Essa.\nMasksketch: Unpaired structure-guided\nmasked image generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2023. 3\n[4] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas\nM\u00a8uller, and Bj\u00a8orn Ommer. Retrieval-augmented diffusion\nmodels. In Advances in Neural Information Processing Sys-\ntems, 2022. 2, 3\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023. 2, 3\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. In Advances in Neural\nInformation Processing Systems, 2020. 2, 3\n[7] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W\nCohen. Re-imagen: Retrieval-augmented text-to-image gen-\nerator. arXiv preprint arXiv:2209.14491, 2022. 2, 3\n[8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 3\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann,\net al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022. 2\n[10] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek\nKadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue\nZhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Mot-\nwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ra-\nmanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu:\nEnhancing image generation models using photogenic nee-\ndles in a haystack. arXiv preprint arXiv:2309.15807, 2023.\n2\n[11] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based\ntext-to-image generation with human priors. In European\nConference on Computer Vision, 2022. 3\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. arXiv preprint arXiv:2208.01618,\n2022. 2, 3\n[13] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu\nSebe, Trevor Darrell, Zhangyang Wang, and Humphrey Shi.\nPair-diffusion: Object-level image editing with structure-\nand-appearance paired diffusion models.\narXiv preprint\narXiv:2303.17546, 2023. 3\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2016. 4\n[15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. GANS trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in Neural Information Processing Systems, 30, 2017.\n6\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33, 2020. 2\n[18] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,\nAndy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,\nJiahui Yu, Christian Frank, et al.\nNoise2music:\nText-\nconditioned music generation with diffusion models. arXiv\npreprint arXiv:2302.03917, 2023. 2\n[19] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal inputs\nand outputs. In Proceedings of the International Conference\non Machine Learning, 2023. 3\n[20] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nMatsuo, and Yusuke Iwasawa. Large language models are\nzero-shot reasoners. Advances in Neural Information Process-\ning Systems, 35, 2022. 3\n[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\n2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023. 3\n[22] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2023. 3\n[23] Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and\nLingyun Sun. Image synthesis from layout with locality-\naware mask adaption. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 2021. 3\n[24] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 2\n[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image\n9\nsynthesis and editing with stochastic differential equations.\narXiv preprint arXiv:2108.01073, 2022. 3\n[26] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023. 3\n[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances\nin Neural Information Processing Systems, 35, 2022. 2\n[28] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022. 2\n[29] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,\nYingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming\nXiong, Silvio Savarese, et al. UniControl: A unified diffusion\nmodel for controllable visual generation in the wild. arXiv\npreprint arXiv:2305.11147, 2023. 3\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In Proceedings of\nthe International Conference on Machine Learning, 2021. 3,\n5\n[31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In Proceedings of the\nInternational Conference on Machine Learning. PMLR, 2021.\n2\n[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n2\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022. 2, 3, 4\n[34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Proceedings of the International Conference on\nMedical Image Computing and Computer-Assisted Interven-\ntion. Springer, 2015. 3\n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023. 3\n[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing\nSystems, 2022. 2, 3\n[37] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James\nHays. The sketchy database: Learning to retrieve badly drawn\nbunnies. ACM Transactions on Graphics, 2016. 5\n[38] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In Proceedings of the Inter-\nnational Conference on Machine Learning, 2015. 2\n[39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations, 2021. 2, 5\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\ntinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023. 2\n[41] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill. Multimodal few-shot\nlearning with frozen language models. Advances in Neural\nInformation Processing Systems, 2021. 3\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017. 4\n[43] Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen,\nPengcheng He, Weizhu Chen, Zhangyang Wang, and\nMingyuan Zhou. In-context learning unlocked for diffusion\nmodels. arXiv preprint arXiv:2305.01115, 2023. 1, 2, 3, 4, 5,\n6, 7, 8, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24\n[44] Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao\nWang, Pengcheng He, Zhangyang Wang, Weizhu Chen,\nand Mingyuan Zhou.\nPatch diffusion: Faster and more\ndata-efficient training of diffusion models. arXiv preprint\narXiv:2304.12526, 2023. 2\n[45] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le. Finetuned language models are zero-shot learners.\narXiv preprint arXiv:2109.01652, 2021. 2, 3\n[46] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret\nZoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, et al. Emergent abilities of\nlarge language models. Transactions on Machine Learning\nResearch, 2022. 2, 3\n[47] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan\nEssa, and Humphrey Shi. Prompt-free diffusion: Taking\u201d\ntext\u201d out of text-to-image diffusion models. arXiv preprint\narXiv:2305.16223, 2023. 3\n[48] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2023. 2, 3, 4, 5, 7, 8, 11, 12, 19\n[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022. 2\n10\nFigure A.1. Ablation: Benefit of visual conditioning. The im-\nages in the third column are generated by our best model, while\nthe fourth column depicts the ones obtained with the summation\napproach.\nAppendix\nThe appendix consists of the following sections: A Ablation\nstudy, B Architecture details & comparison, C Limitations,\nD Additional qualitative results.\nA. Ablation study\nBenefit of visual conditioning.\nThe ingestion of the vi-\nsual context examples is an important part of our framework\nbecause it controls the generation process as much as the\ntext prompts. Consequently, we process the context images\nsimilarly to the prompt. Here, we compare this approach\nwith an alternative one, implemented in Prompt Diffusion\n[43]. Specifically, by using ConvNets to encode the context\nimages and then sum them to the query image. As we visu-\nalize in Figure A.1, this approach has limitations in actually\nlearning from the context. Instead, we show that by sepa-\nrating the condition of the context images from the visual\nstructure provided by the query images, our model can more\neffectively capture details of the context images.\nEffect of different text prompt settings.\nIn this section,\nwe analyze the behavior of our model with several possible\ntext prompts for the sketch tasks. We consider (i) the given\nprompt (ii) default prompt \u201cA professional, detailed, high-\nquality image\u201d, similar to [48] (iii) empty string as a prompt.\nIn Figure A.2 we can observe that our model is able to\ngenerate images capturing the visual cues from the context\nexample, across all text prompts. Note that having the text\nprompt mentioning the name of the object in the query image\nhelps in generating finer details (like a more detailed surface\nof the pumpkin), however, even without it, our model is able\nto generate reasonable images.\nFigure A.2. Ablation: Effect of different text prompt settings.\nOur model succeeds in all three scenarios while showing it is able\nto capture visual characteristics from the context.\nFigure A.3. Ablation: Source-target vs target-only as context.\nAdding the source image does not influence the generation process.\nSource-target vs target-only as context.\nIn this section,\nwe analyze the performance of our model when trained us-\ning source-target image pairs as context examples (same as\nPrompt Diffusion [43]. As can be seen in Figure A.3, there\nis almost no difference when the source image is added to\nthe training. The condition for generating the output is en-\ntirely contained in the target i.e. context image or the prompt,\nwhile the query image controls the structure.\nB. Architecture details & comparison\nIn this section we compare the architecture of our model\nand Prompt Diffusion [43]. As can be seen from Figure B.1,\nthe difference is in the visual conditioning using the context\nexamples. We propose to stack the visual embeddings of\nthe context examples - hV next to the text embeddings hc.\nIn this manner, the model is able to balance textual and\nvisual conditioning. Moreover, it learns how to handle the\nstructure of q separately from the context examples. This is\ndifferent from Prompt Diffusion [43] which directly sums\nthe examples embeddings to the query image.\nAnother difference is in the context examples. Different\nfrom Prompt Diffusion, we do not provide a source image,\nsince it can be derived from the target (context) image it-\nself, meaning it does not provide any additional information.\nNote that in our early experiments, we did use a pair of\n11\nFigure B.1. Comparison between our model architecture (right) and Prompt Diffusion (left). We propose to use k-context examples via\nvisual conditioning which allows the model to learn the visual characteristics of the context separated from the structure of the image. On\nthe other hand, Prompt Diffusion is summing the examples directly to the visual control i.e. the query image.\nsource and target images, however, it showed to not bring\nany improvements. Furthermore, the ControlNet framework\n[48] is capable of controlling image generation based on\nimage structures, making the source image unnecessary. We\nalso provide the flexibility to include more than one context\nexample, in order to learn stronger visual representations as\nconditioning, as shown in Figure B.1. Furthermore, in Fig-\nure B.2 we include the pseudo-code of our implementation\nshowing how the cross-attention block is modified by using\nmultiple images in the visual context.\nC. Limitations\nIn the scenario where both the visual context and prompt are\npresent, the current design assumes that the examples in the\ncontext are representative of the prompt. These embeddings\nform a stronger representation of the conditioning during the\ngeneration process. However, to build an even more flexible\narchitecture, the visual context and prompt should ideally\nprovide complementary information. Another limitation\nis the generation of images containing fine-grained details\nindicated in the text prompts or in the visual context. For\ninstance, image editing is such a challenging task, especially\nfor finer, local edits, as shown in Figure D.16.\nEthical considerations\nOur model is built using pre-\ntrained models, both for the visual and textual conditioning\nas well as for the image generation process. This means\nthat it inherits any biases and limitations that may exist in\nthese pre-trained models. Therefore, a careful analysis of the\nrisks and societal implications should be considered before\nFigure B.2. Pseudo-code for a torch-like implementation of the\nmodified cross-attention block in our model, by using k-images as\nvisual context examples.\nbuilding any real-world application.\nD. Additional qualitative results\nIn the following sections, we provide additional qualitative\nresults, spanning from in-domain tasks, such as handling\nHED, segmentation, and depth maps to out-of-domain tasks,\nsuch as editing and sketches, as well as examples of few-shot\nsettings.\n12\nFigure D.1. HED-to-image and vice versa, with visual context and prompt as conditioning, in-domain comparison to Prompt Diffusion [43].\n13\nFigure D.2. Seg-to-image and vice versa, with visual context and prompt as conditioning, in-domain comparison to Prompt Diffusion [43]\n14\nFigure D.3. Depth-to-image and vice versa, with visual context and prompt as conditioning, in-domain comparison to Prompt Diffusion [43].\n15\nFigure D.4. HED-to-image, only with context images as conditioning, in-domain comparison to Prompt Diffusion [43].\n16\nFigure D.5. Seg-to-image, only with context images as conditioning, in-domain comparison to Prompt Diffusion [43].\n17\nFigure D.6. Depth-to-image, only with context images as conditioning, in-domain comparison to Prompt Diffusion [43].\n18\nFigure D.7. HED-to-image, only with text prompt as conditioning, in-domain comparison to Prompt Diffusion [43] and Control Net [48].\n19\nFigure D.8. Image edit, with visual context and prompt as conditioning, out-of-domain comparison to Prompt Diffusion [43].\n20\nFigure D.9. Sketch-to-image, with visual context and prompt as conditioning, out-of-domain comparison to Prompt Diffusion [43].\n21\nFigure D.10. Normal map-to-image (rows 1-2) and canny-to-image (rows 3-4), both with visual context and prompt as conditioning,\nout-of-domain comparison to Prompt Diffusion [43].\n22\nFigure D.11. Image editing, only with context images as conditioning, out-of-domain comparison to Prompt Diffusion [43].\n23\nFigure D.12. Sketch-to-image, only with context images as conditioning, out-of-domain comparison to Prompt Diffusion [43].\n24\nFigure D.13. Examples using different types of query images, with visual context and prompt, and only using context images as conditioning.\nOur model can successfully generate images that match the visual context and/or prompt, independent of the type of visual control provided\nby the query image.\n25\nFigure D.14. Few-shot examples: image edit. Comparison using one, two, and three shots of context examples with text prompts.\n26\nFigure D.15. Few-shot examples: sketch-to-image. Comparison by using one, two, and three shots of context examples with text prompts.\n27\nFigure D.16. Failure examples on editing tasks (local edits), using visual context and prompt as conditioning.\n28\n"
  },
  {
    "title": "LooseControl: Lifting ControlNet for Generalized Depth Conditioning",
    "link": "https://arxiv.org/pdf/2312.03079.pdf",
    "upvote": "12",
    "text": "LOOSECONTROL: Lifting ControlNet for Generalized Depth Conditioning\nShariq Farooq Bhat\nKAUST\nNiloy J. Mitra\nUniversity College London, Adobe Research\nPeter Wonka\nKAUST\nFigure 1. Our framework LooseControl enables multiple ways to control the generative image modeling process. Left: (C1) Scene\nboundary control lets the user specify the boundary of the scene. We show the control inputs on top, the ControlNet results in the middle,\nand our results at the bottom. Middle: (C2) 3D box control can additionally specify object locations with the help of approximate 3D\nbounding boxes. We again show the control inputs on top, the ControlNet results in the middle, and our results on the bottom. Right-Top:\n(E1) 3D box editing. Note how the overall style of the scene is preserved across edits. Right-Bottom: (E2) Attribute editing: an example\nof changing a couch to another one and changing the overall furniture density in a room.\nAbstract\nWe present LOOSECONTROL to allow generalized depth\nconditioning for diffusion-based image generation. Con-\ntrolNet, the SOTA for depth-conditioned image generation,\nproduces remarkable results but relies on having access\nto detailed depth maps for guidance. Creating such exact\ndepth maps, in many scenarios, is challenging. This paper\nintroduces a generalized version of depth conditioning that\nenables many new content-creation workflows. Specifically,\nwe allow (C1) scene boundary control for loosely specify-\ning scenes with only boundary conditions, and (C2) 3D box\ncontrol for specifying layout locations of the target objects\nrather than the exact shape and appearance of the objects.\nUsing LOOSECONTROL, along with text guidance, users\ncan create complex environments (e.g., rooms, street views,\netc.) by specifying only scene boundaries and locations of\nprimary objects. Further, we provide two editing mecha-\nnisms to refine the results: (E1) 3D box editing enables\nthe user to refine images by changing, adding, or remov-\ning boxes while freezing the style of the image. This yields\nminimal changes apart from changes induced by the edited\nboxes. (E2) Attribute editing proposes possible editing di-\nrections to change one particular aspect of the scene, such\nas the overall object density or a particular object. Ex-\ntensive tests and comparisons with baselines demonstrate\nthe generality of our method. We believe that LOOSECON-\nTROL can become an important design tool for easily cre-\nating complex environments and be extended to other forms\nof guidance channels. Code and more information is avail-\nable at https://shariqfarooq123.github.io/\nloose-control/.1\n1. Introduction\nDiffusion-based generative models now produce images\nwith a remarkable degree of photorealism. ControlNet [51],\ntrained on top of StableDiffusion [37], is the most power-\nful way to control such a generation process. Specifically,\n1The project was supported in part by \u201cNTGC-AI\u201d funding at KAUST.\n1\narXiv:2312.03079v1  [cs.CV]  5 Dec 2023\nControlNet allows guidance in the form of one or more of\ndepth, edges, normal, or semantic channels. This ability\nto accurately control the image generation process enables\nvarious creative applications without requiring different ar-\nchitectures for different applications. However, providing\nperfect guidance for ControlNet can itself be challenging.\nFor example, imagine a scenario where a user wants to\ncreate living room images using depth guidance. She is\nnow expected to supply a depth map containing informa-\ntion about walls, room furniture, all the way to even smaller\nobjects and decorations. This is non-trivial. Producing re-\nalistic depth maps, especially for cluttered scenes, is proba-\nbly as challenging as solving her original task. On the one\nhand, providing rough guidance, e.g., providing depth infor-\nmation for walls, floor, and ceiling, results in unsatisfactory\nimages. While one may expect a furnished room, the room\nwill be empty. Also providing approximate depth using tar-\nget bounding boxes for furniture does not yield the desired\nresult, because only boxy objects are generated. Figure 1\nshows some examples using ControlNet [51].\nWe present LOOSECONTROL that allows controlling im-\nage generation using generalized guidance. In this work,\nwe focus on guidance through depth maps. We consider\ntwo types of specifications. (C1) Scene boundary control\nwhere a layout can be given by its boundaries, e.g., walls\nand floors, but the final scene can be filled by an arbitrary\nnumber of objects that are not part of the depth conditioning\nas long as these additional objects are closer to the camera\nthan the scene boundary. (C2) 3D box control where, in\naddition to layout boundaries, users can provide finer-scale\nguidance in the form of approximate bounding boxes for\ntarget objects. In the final generation, however, there can be\nadditional secondary objects that do not need to strictly ad-\nhere to the given boxes. We demonstrate that many layout-\nguided image generation scenarios (e.g., rooms, streets, un-\nderwater) can be addressed in this framework. Figure 1\n(left, middle) shows some of our generations.\nWe also provide two interactive editing modes to refine\nthe results. (E1) 3D box editing enables the user to change,\nadd, and remove boxes while freezing the style of the re-\nsulting image. The goal is to obtain minimal changes apart\nfrom the changes induced by the edited boxes. We describe\nhow a notion of style can be formalized and preserved in\nour diffusion setup. (E2) 3D attribute editing enables users\nto change one particular attribute of the result, such as the\ntype of one particular piece of furniture. Since diffusion\nprocesses return a distribution of results, we perform local\nanalysis around any generation to reveal dominant modes of\nvariation. Figure 1 (right) shows a few editing examples.\nThe technical realization of these four components is\nbuilt on ControlNet with a frozen StableDiffusion back-\nbone.\nWe first propose a Low Rank (LoRA) based net-\nwork adaptation. The LoRA-based architecture allows the\nnetwork to be fine-tuned in a few steps with only a small\namount of training data, preventing the network from for-\ngetting the original generation weights. A second impor-\ntant component is automatically synthesizing the necessary\ntraining data without requiring manual annotations. Finally,\nthe edits are realized by manipulating the \u201ckeys\u201d and \u201cval-\nues\u201d in attention layers, and a singular value analysis of the\nControlNet Jacobian.\nWe evaluate a range of use cases. Since we enable a\nstrictly more general depth control, there are no direct com-\npetitors. Hence, we compare strict and weaker guidance\nby varying the control weighting in ControlNet, as well as\nretraining ControlNet on synthetic data. We evaluate our\nsetup on a variety of different scenes and also perform a\nuser study that reveals over 95% preference for our method\ncompared to baselines. In summary, we are the first to al-\nlow image generation from loose depth specifications and\nprovide multiple types of edits to semantically explore vari-\nations in generations.\n2. Related Work\nDiffusion models have rapidly evolved as a leading gener-\native approach, demonstrating remarkable success both in\n2D image generation [6, 14, 18, 32, 33, 35, 37, 39] as well\nas 3D shape generation [11, 12, 15, 20, 21, 40, 50, 52].\nWhile there are many similarities and synergies between 2D\nand 3D diffusion, in this paper, we only focus on the prob-\nlem of adding conditional control to text-to-image diffusion\nmodels. In this context, many existing methods proposed\nsolutions on direct guidance through well-known condi-\ntion inputs like inpainting masks [45, 48], sketches [44],\nscene graphs [49], color palletes [19, 30, 43], 2D bounding\nboxes [27], segmentation maps [13, 37, 46], composition\nof multiple text descriptions [28], depth maps [30, 51] or\nby fine-tuning these models on a few subject-specific im-\nages [16, 29, 38, 42]. We focus on introducing new types\nof control. ControlNet [51] stands out as a key method in\nspatial control, supporting a diverse array of conditions like\nedge maps, depth maps, segmentation masks, normal maps,\nand OpenPose [8, 9, 41, 47] under a single framework. It\nis notably based on the widely-used open-source model,\nStable Diffusion [37], which contributes to its popularity.\nHowever, creating spatial control conditions manually by a\nuser for ControlNet can be challenging, often resulting in\nindirect control where conditions are derived from another\nimage source, which can stifle the creative generation pro-\ncess. Additionally, this method can be restrictive in terms\nof the diversity of generations per condition, posing limita-\ntions in various scenarios. Our work builds on top of Con-\ntrolNet and introduces novel conditioning mechanisms that\nare not only easy to construct manually by a user but also\noffer enhanced control and flexibility. In particular, we con-\ntribute novel forms of loose control described in the next\n2\nsection.\n3. Problem Setup - LOOSECONTROL\nWe formally introduce the core problem statement proposed\nin this work - loose depth control. Ordinary depth control,\nas implemented by the original ControlNet can be formally\ndescribed as follows: Given an input condition depth map\nDc, and access to an off-the-shelf monocular depth estima-\ntor fD, generate an image Igen such that the estimated depth\nfD(Igen) respects the input depth condition i.e.:\nGenerate Igen such that fD(Igen) = Dc.\n(1)\nThus, by design, the conditioning imposed by Control-\nNet is strict: as per training, the equality in Eq. (1) must ex-\nactly hold. Our goal is to extend this notion of control to a\nmore flexible generalized form. We therefore define gener-\nalized control via the following setup: Given an input con-\ndition depth map Dc, and access to an off-the-shelf monoc-\nular depth estimator fD, and an arbitrary Boolean condition\nfunction \u03d5(\u00b7, \u00b7):\nGenerate Igen such that \u03d5(fD(Igen), Dc) is TRUE.\n(2)\nIt is easy to observe that Eq. (1) is a special case of\nEq. (2). In this work, we propose to consider two other\ncases: scene boundary control and 3D box control, as de-\nscribed next.\n(C1) Scene boundary control. In this case, we impose\nthe condition such that the input depth condition Dc only\nspecifies the tight upper bound of depth at each pixel:\n\u03d5 : fD(Igen) \u2264 Dc.\n(3)\n(C2) 3D box control. In this case, we let the condition\nDc control only the approximate position, orientation, and\nsize of objects by specifying their approximate 3D bound-\ning boxes. This leads to finer control than (C1) yet still\navoids specifying the exact shape and appearance of the ob-\njects. Essentially, we design a condition function \u03d5 that en-\nsures that the objects Oi\ngen generated in an image conform\nto their respective specified 3D bounding boxes Bi:\n\u03d5 : Bi \u223c 3DBox(Oi\ngen)\n\u2200i,\n(4)\nwhere 3DBox(Oi\ngen) represents the oriented 3D bounding\nbox of the i-th object segment in the generated image. This\nmeans that the position, size, and orientation of each object\nwill be approximately within the bounds set by its corre-\nsponding 3D bounding box. Although our 3D box control\ntraining is strict, we show that boxes can be treated as only\napproximate, which proves to be highly beneficial in prac-\ntice.\nBoth Eq. (3) and Eq. (4) specify a form of depth condi-\ntion that is strictly more general than the ordinary control\nsuch as realized by ControlNet. We term such conditioning\nas LOOSECONTROL.\n4. Realizing LOOSECONTROL\nTo realize ordinary depth control of text-to-image diffusion\nmodels, one needs access to triplets of the form (T, Dc, I)\nfor training, where T is the text description of the image I\nand Dc represents the depth condition. Generally, one has\naccess to the pairs (T, I) and the ordinary depth condition\nDc is obtained by applying an off-the-shelf depth estima-\ntor fD on the given image (i.e., Dc = fD(I)) to obtain\nthe triplets (T, Dc = fD(I), I). However, in our case, the\ndepth condition Dc must take a more generalized form. For\nour goal, the form of Dc is constrained by three main re-\nquirements: (i) Compatibility with ControlNet, where the\ndepth condition should resemble a conventional depth map.\n(ii) Efficient to extract from a given image without manual\nannotation. (iii) Easy to construct manually by a user. We\ndescribe below how to obtain the appropriate form of Dc\nsuch that LOOSECONTROL is realized.\n4.1. How to represent scenes as boundaries?\nWe begin by outlining the estimation of the depth condition\nDc from a given image for implementing scene boundary\ncontrol as described in Eq. (3). In this context, we seek a\ndepth condition that acts as an upper depth limit for indi-\nvidual pixels. We propose to extract the scene boundary\nsurface for upper-bound training in the following manner.\nSpecifically, we define the boundary as a set of planar sur-\nfaces encompassing the scene that accurately delineates the\nscene\u2019s spatial extent. For a given bounded 3D scene and\na specific camera viewpoint, the boundary depth is defined\nas the z-depth of these boundary surfaces. In practice, this\nmeans that all pixels in the image inherit the depth value\nassociated with the boundary surface, even if the boundary\nsurfaces are occluded by objects within the scene\u2019s interior.\nTo provide a concrete example, consider an indoor scene\n(refer to Fig. 3). The boundary typically encompasses the\nwalls, ceiling, and floor planes. Consequently, a bound-\nary depth map for an indoor scene exclusively reflects the\ndepths of walls, ceiling, and floor, irrespective of the pres-\nence of room furniture.\nA naive approach to extracting boundary depth involves\nleveraging annotated room layout datasets. However, such\ndatasets often contain ambiguous annotations in multi-room\nimages, and the room-level annotations directly violate our\n\u201cupper\u201d bound condition (see Supplementary Materials).\nAdditionally, this approach necessitates manual annotation\nfor boundary extraction and confines the scope to room-\ncentric scenarios. Our objective is to devise a strategy that\nis applicable across diverse scene types and readily applica-\nble to extract boundary depth from any given image. To this\nend, we propose the following algorithm:\nWe use a multi-step approach to efficiently extract\nboundary depth from a given image. Refer to Fig. 2 for the\noutline of our approach. We begin by estimating the depth\n3\nBackprojection\nDepth Render\nApprox Poly\nOrthographic\nProjection\nExtrusion\nFigure 2. Pipeline for extracting boundary depth from an image. From left to right: Input image and its estimated depth map, back-\nprojected 3D mesh, orthographic projection of the mesh on a horizontal plane, polygon approximation of the 2D boundary, extrusion of\npoly sides, resulting boundary depth map. For ease of visualization, the ceiling is not shown.\nmap of the given image using an off-the-shelf monocular\ndepth estimator [7]. We then back-project the image into a\n3D triangular mesh within the world space. Our goal is to\nextract the planar surfaces that encompass this mesh. For\nefficiency, we only make use of vertical planes during train-\ning. This reduces the 3D boundary extraction problem to\n2D. We project the 3D mesh of the scene onto a horizontal\nplane using orthographic projection. This projection facili-\ntates the precise delineation of the 2D boundary that encap-\nsulates the scene. We then approximate the 2D boundary of\nprojection with a polygon and extrude the sides into planes\nmatching the scene height. For an indoor scene, for exam-\nple, these \u2018poly\u2019-planes represent a good approximation of\nwall planes. These poly planes and optionally the ground\nand ceiling plane (if available) together form the boundary\nsurface. We then render the depth of this boundary surface\nfrom the original camera view to get the boundary depth\nmap Dc, serving as a proxy for the scene boundary condi-\ntion.\nScene Boundary Control\n3D Box Control\nScene Boundary Control\n3D Box Control\nFigure 3. Illustration of proxy depth for scene boundary control\ntraining (left) and 3D box control training (right).\n4.2. How to represent scenes as 3D boxes?\nWe now describe the estimation of Dc from a given image\nto realize 3D box control (Eq. (4)). For a given image, our\ngoal is to construct Dc such that it contains the informa-\ntion about the 3D bounding boxes of the objects present in\nthe scene. At the same time, we need to ensure compatibil-\nity with depth-conditioned frameworks, such as ControlNet,\nwhere the depth condition should resemble a conventional\ndepth map. To achieve this, our idea is to obtain 3D bound-\ning boxes of the objects in the image and render the depth\nof the boxes. The resulting depth map can serve as a proxy\nfor specifying 3D bounding boxes (See Fig. 3). However,\nthis task necessitates a robust monocular 3D object detector,\nwhich, currently, presents challenges. Existing monocular\n3D object detection pipelines tend to be either sparse, tar-\ngeting specific object categories, or domain-specific, focus-\ning on scenarios like road scenes, limiting their versatility.\nTo address these limitations, we introduce a custom monoc-\nular pipeline that approximates the 3D bounding boxes of\nobjects and is simultaneously dense (capable of recovering\nany object type) and generalizable (no domain preference).\nFor a given image, we first obtain its depth map and\nsegmentation map using an off-the-shelf monocular depth\nestimator ZoeDepth [7] and SAM [24], respectively. The\ndepth map provides 3D spatial information, while the seg-\nmentation map delineates object boundaries, which we use\ntogether to approximate 3D bounding boxes of objects. For\neach segment, we perform back-projection, transforming\nthe image segment using its depth map into a point cloud\nin 3D world space. We then estimate the corresponding\noriented 3D bounding box with minimal volume for each\nback-projected segment point cloud and represent it as a\ncuboidal mesh. Finally, to obtain Dc, we render this scene\nof boxes using the original camera parameters used in back-\nprojection and extract the depth map. This process is sum-\nmarized in Fig. 5.\n4.3. How to train for LOOSECONTROL?\nWe build our framework on StableDiffusion [37] and Con-\ntrolNet [51]. In both scene boundary control and 3D box\ncontrol, the constructed depth condition Dc serves as a\nproxy for specifying a more generalized control - scene\nboundary and 3D bounding box, respectively - while retain-\n/\nphoto of a helicopter over a pine forest in winter\nDepth Estimator\nProxy Estimator\nControlNet\nControlNet\nStable Diffusion\nLoRA\nStable Diffusion\nFigure 4. Training pipelines for ControlNet (Top) and LOOSEC-\nONTROL (Bottom). Proxy Estimator represents our proxy depth\nextraction algorithms. During inference, we enable an option for a\nuser to design the condition depth manually via a UI.\n4\nBackprojection\nDepth Render\n3D Lifting\nCompute BBox\nFigure 5. Pipeline for extracting proxy depth for 3D box control from an image. Left to right: Input image, its estimated depth and\nsegmentation maps, back-projected 3D mesh, 3D lifting of segmentation, 3D bounding boxes for segments, resulting proxy depth map.\ning compatibility with ordinary depth-conditioned frame-\nworks such as ControlNet. This ensures a smooth transi-\ntion and facilitates the easy adoption of these approaches.\nThanks to this backward compatibility, we can efficiently\nfine-tune a pre-trained depth ControlNet to achieve gen-\neralized depth control. We prepare the triplets (T, Dc, I)\nfrom the given (T, I) pairs where Dc is constructed using\nthe algorithms presented in previous sections. We explore\ntwo primary options for fine-tuning: (i) Naive fine-tuning:\nwhere we finetune the entire ControlNet; and (ii) LoRA\nfine-tuning: where we employ LoRA-based fine-tuning of\nControlNet (see Fig. 4). Specifically, for every attention\nblock in ControlNet, we learn a low-rank update:\nW \u2032x = Wx + BAx\n(5)\nwhere W \u2208 RM\u00d7N is the original frozen projection, B \u2208\nRM\u00d7r and A \u2208 Rr\u00d7N are trainable low rank matrices of\nrank r \u226a min(M, N). It is important to note that in both\ncases Stable Diffusion U-net remains frozen during fine-\ntuning. For both control types (C1 and C2), we use the\nNYU-Depth-v2 [31] dataset for fine-tuning and obtain tex-\ntual image descriptions using BLIPv2 [26] captioning.\nInference time adjustments.\nWe observe that naive\nfine-tuning results in saturated generations (Fig. 6). Inter-\nestingly, the color artifacts are eliminated if the residuals\ninjected into Stable Diffusion U-net from ControlNet are\nreweighted at inference time. We observe that residual to\nthe bottleneck block is responsible for most of the condi-\ntion adherence and other higher resolution skip residuals\nlargely add only local texture information. On the other\nhand, LoRA fine-tuning proves to be robust without any\ncolor issues. However, we introduce a controllable scale\nfactor \u03b3 at inference time such that the update is given as:\nFigure 6.\nGiven scene boundary (left), comparing naive fine-\ntuning, adjusted skip weights, and our LoRA-based training, resp.\nW \u2032x = Wx + \u03b3BAx. We observe that controlling \u03b3 can\nlead to higher quality results, especially for \u03b3 > 1. We use\n\u03b3 = 1.2 as the default.\n4.4. 3D Box Editing\nBoth the scene boundary condition and the box condition\nare easy to manipulate by a user. This opens up possibili-\nties where a user can interactively manipulate the scene and\nobjects, for example, by changing, adding, or removing the\nboxes. However, it is desirable to \u2018lock in\u2019 and maintain the\nstyle and composition of the scene while manipulating the\ncondition. A naive way to maintain the overall style would\nto be fix the seed but we observe that changing the condi-\ntion even when fixing the seed can lead to diverging gen-\nerations. To this end, we propose Style Preserving Edits,\ninspired by video diffusion models [22], which allows users\nto maintain the desired style of the scene while conduct-\ning sequential edits. This feature is realized by replacing\nthe \u201ckeys\u201d and \u201cvalues\u201d for the given image in the atten-\ntion layers of the Stable Diffusion U-net with those from\nthe source image. However, in our case, sharing \u201ckeys\u201d and\n\u201cvalues\u201d through all layers of the U-net leads to undesir-\nable results, producing images identical to source images.\nWe find that sharing \u201ckeys\u201d and \u201cvalues\u201d from only the last\ntwo decoder layers ensures the preservation of the desired\nidentity or style while adhering to the new target condition.\n4.5. Attribute Editing\nLOOSECONTROL\u2019s generalized nature compared to ordi-\nnary depth control allows for a wider range of possible\nscenes under a given condition. To explore this expansive\nspace, we conduct various forms of latent exploration.\nControlNet produces two types of residuals for a given\ncondition: residuals added to the SD U-net bottleneck and\nresiduals added to decoder skip connections. Our focus cen-\nters on the bottleneck feature space (H-space), known for\nits semantic behavior [17, 25] and from our experiments, its\npivotal role in condition adherence.\nIn our exploration, we examine how the latent input,\nx, affects ControlNet\u2019s output for a given fixed condition.\nWe extract the most influential directions in x-space, which\ncause substantial changes in \u2206h \u2208 H, and find their coun-\nterparts in H-space, using Singular Value Decomposition\n5\na clothing store\nseating area of charming Irish\npub with dark wood paneling \na heavily equipped kitchen\nwith appliances\na living room with sofa, \nhouse plants and a table\ninside of an old cathedral\nwith pew\nbusy street in Toyko at night\nwith a lot of pedestrians\ninput guidance\nControlNet\nLooseControl (ours)\n(C1) Scene Boundary Control\nswivel office sofa chair\nwith a round backrest\na white English arm roll sofa\na living room\nfour office chairs\na helicopter flying over\na pine forest in winter\na bedroom with a canopy bed\nwith puffy bedding & nightstands\na set of conical icebergs\nin the ocean\ninput guidance\nControlNet\nLooseControl (ours)\n(C2) 3D Box Control\ncrowd celebrating Guy Fawkes\u2019s\nNight on Trafalgar Square\nFigure 7. Qualitative results of (C1) Scene boundary control (rows 1-3) and (C2) 3D Box control (rows 4-6). Top: Input depth condition\nMiddle: ControlNet generations. Bottom: Ours. Our generations are more realistic and adhere to the prompt better than ControlNet.\n(SVD) of the ControlNet Jacobian, \u2202\u2206h/\u2202x. To keep the\nSVD computation feasible, we only extract the first N di-\nrections, labeled {ei}, which guide our modification of the\nbottleneck residual via: \u2206h\u2032 = \u2206h + \u03b2ei, where the scalar\n\u03b2 controls the magnitude of the edit.\nPer-condition latent exploration enhances user control\nby revealing various semantic edit directions, for example,\nsizes, types, and even the number of objects in the scene.\nThis gives rise to a continuous control space, allowing for\nfine-grained adjustments that were challenging with con-\nventional conditioning methods. This continuous control\nspace operates orthogonal to the input depth condition, en-\nabling the maintenance of specified conditions\u2014whether\nscene boundary control or 3D box control\u2014while simul-\ntaneously exploring a wide range of generated images that\nadhere to the given condition.\n5. Applications and Results\nWe developed an interface to enable users to conveniently\ndesign a scene boundary and 3D boxes, and generate im-\nages interactively (see supplementary material). Here, we\npresent qualitative and quantitative results.\n(C1) Scene Boundary Control.\nScene boundary con-\ntrol enables diverse applications across various domains.\nWe present some examples and comparisons between\nLOOSECONTROL and ordinary control in Fig. 7. One par-\nticularly noteworthy application of scene boundary control\nis in indoor scene generation. Users can provide somewhat\n6\nFigure 8. Qualitative results of (E1) 3D Box Editing, and (E2) Attribute Editing. (E1) 3D Box editing enables us to move and re-orient\nobjects in the scene, change their size, split them, and more. (E2) Attribute Editing lets us change properties like the density of furniture\n(e.g., increase or decrease density) and the material of objects (e.g., fabric to leather or vica versa).\n7\nabstract specifications for room layouts, generally confined\nto walls and floor, and still generate images of fully fur-\nnished rooms. This is different from ordinary depth condi-\ntioning, which demands precise depth information.\nAnother scenario involves (partially) bounded outdoor\nscenes where the user can provide the scene boundary\ndepth, for example, in terms of locations of buildings (See\nFig. 1).\nUnlike the ordinary depth control implemented\nby the original ControlNet, which leads to empty scenes,\nour LOOSECONTROL model excels at generating realis-\ntic scenes, inclusive of objects like cars, traffic lights, and\npedestrians, all while adhering to the user-provided scene\nboundary condition.\nThis feat underscores the reliabil-\nity and generalization capability of LOOSECONTROL via\nproxy fine-tuning.\nIn Fig. 9, we show the effect of varying the condition-\ning scale, \u03bb, in the context of scene boundary control. We\nobserve that ControlNet still produces \u2018empty\u2019 rooms with\nlesser prompt adherence for most of the conditioning scales.\n(C2) 3D Box Control.\nWe present generations and com-\nparisons with ControlNet for 3D box control for a variety of\nscenes in Fig. 7. ControlNet with ordinary control produces\n\u2018boxy\u2019 generations which are not only unrealistic but also\nresult in lesser prompt adherence. On the other hand, our\nmethod is able to produce more realistic images and shows\nbetter adherence to the given prompt demonstrating the util-\nity of such control.\n(E1) 3D Box Editing.\nWe show qualitative results for 3D\nBox Editing in Fig. 8. 3D box Editing allows users to ma-\nnipulate objects, for example, by changing their position in\n3D space, their orientation, and size. Users can perform\n3D-aware edits, such as relocating furniture within a room\nor converting a small sofa chair to a big sofa. The Style\nPreserving Edit mechanism facilitates sequential editing of\nobject poses and locations, preserving the scene\u2019s essence\nwhile introducing creative modifications. Since the 3D box\ncontrol implicitly specifies the rough shape and size of the\nobjects, we can edit the boxes themselves such as splitting\nControlNet\nLooseControl (ours)\ncondition\nFigure 9. Effect of conditioning control scale \u03bb. Top: ControlNet.\nBottom: Ours. The results were produced for the prompt \u201cA cozy\ncoastal bedroom with a sea view\u201d.\na box into two, resulting in semantic edits on objects.\n(E2) Attribute Editing. Extracting the edit directions for a\nfixed scene boundary leads to continuous control directions\nfor attributes such as the density of furniture and furniture\ntype. For a given 3D box control condition (see Fig. 8), the\nedit directions represent attributes like the shape, thickness,\nand material properties such as \u201cleather\u201d. We observe that\nedit directions are smooth and continuous for the most part\nand varying the magnitude \u03b2 directly reflects that magni-\ntude of change in the attribute.\nUser Study.\nWe conducted a user study to quantify the\nControlNet\nours\nScene Boundary Control\n3D Box Control\nimprovement\nin\ncontrol and quality\nof\ngenerations.\n41\nusers\nwere\npresented\nwith\nthe\nprompt\nand\ncondition\nand\nasked to pick the image they preferred between ours and\nControlNet. A majority of users, more than 95%, ranked\nour result better. See inset for results.\n6. Conclusion\nWe have presented LOOSECONTROL to support general-\nized depth control for image generation. We introduced two\ntypes of control: (C1) Scene boundary control and (C2) 3D\nBox control to generate initial images. To refine initial re-\nsults, we proposed (E1) 3D box editing and (E2) Attribute\nediting. Our framework provides new modes of creative\ngeneration and editing allowing users to more effectively\nexplore the design space with depth guidance. A user study\nrevealed over 95% preference for LOOSECONTROL com-\npared to previous work.\nLimitations. Although our method works well for primary\nobjects, control over secondary objects is harder to achieve\n\u2013 we attribute this to the training data where secondary ob-\njects are less common. We expect a scene-based generator\nmay lead to improved results. Also, similar to the original\nControlNet we find that providing too many constraints as\ninput reduces the diversity of the results.\nFuture work.\nWe would like to use LOOSECONTROL\nin conjunction with masks and also with MultiControlNet\nthrough the generalized specification of other maps. For\nexample, in the context of sketch-based generation, specify-\ning only a selection of (dominant) edges is easier for users\nthan drawing all the Canny edges. Further, we would like to\nexplore temporally consistent generation to produce output\nvideos under smoothly changing guidance as in an interpo-\nlated 6DOF object specification.\n8\nReferences\n[1] https://huggingface.co/. 1\n[2] https://www.babylonjs.com/. 1\n[3] https://huggingface.co/docs/diffusers/\nindex. 1\n[4] https://streamlit.io/. 1\n[5] Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Ab-\ndulrahman Alfozan, and James Zou.\nGradio: Hassle-free\nsharing and testing of ml models in the wild. arXiv preprint\narXiv:1906.02569, 2019. 1\n[6] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Ait-\ntala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Kar-\nras, and Ming-Yu Liu.\nediff-i:\nText-to-image diffusion\nmodels with ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324, 2022. 2\n[7] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter\nWonka, and Matthias M\u00a8uller.\nZoedepth: Zero-shot trans-\nfer by combining relative and metric depth. arXiv preprint\narXiv:2302.12288, 2023. 4, 1\n[8] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part affinity\nfields. In CVPR, 2017. 2\n[9] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.\nSheikh. Openpose: Realtime multi-person 2d pose estima-\ntion using part affinity fields. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2019. 2\n[10] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang.\nMatterport3d: Learning\nfrom rgb-d data in indoor environments.\narXiv preprint\narXiv:1709.06158, 2017. 1\n[11] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:\nA unified approach to 3d generation and reconstruction. In\nICCV, 2023. 2\n[12] Gene Chou, Yuval Bahat, and Felix Heide. Diffusion-SDF:\nConditional generative modeling of signed distance func-\ntions, 2023. 2\n[13] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord.\nDiffedit:\nDiffusion-based seman-\ntic image editing with mask guidance.\narXiv preprint\narXiv:2210.11427, 2022. 2\n[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[15] Ziya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner,\nand Angela Dai.\nHyperdiffusion:\nGenerating implicit\nneural fields with weight-space diffusion.\narXiv preprint\narXiv:2303.17015, 2023. 2\n[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2\n[17] Ren\u00b4e Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff,\nand Tomer Michaeli. Discovering interpretable directions in\nthe semantic latent space of diffusion models. arXiv preprint\narXiv:2303.11073, 2023. 5\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[19] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 2\n[20] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural\nwavelet-domain diffusion for 3d shape generation. In SIG-\nGRAPH Asia 2022 Conference Papers, New York, NY, USA,\n2022. Association for Computing Machinery. 2\n[21] Animesh Karnewar, Niloy J Mitra, Andrea Vedaldi, and\nDavid Novotny. Holofusion: Towards photo-realistic 3d gen-\nerative modeling. ICCV, 2023. 2\n[22] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 5\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 1\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 4, 1\n[25] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion\nmodels already have a semantic latent space. arXiv preprint\narXiv:2210.10960, 2022. 5\n[26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 5\n[27] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22511\u201322521, 2023. 2\n[28] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and\nJoshua B Tenenbaum. Compositional visual generation with\ncomposable diffusion models. In European Conference on\nComputer Vision, pages 423\u2013439. Springer, 2022. 2\n[29] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 2\n[30] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n[31] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob\nFergus.\nIndoor segmentation and support inference from\nrgbd images. In ECCV, 2012. 5\n9\n[32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[33] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 2\n[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in neural information processing systems, 32, 2019.\n1\n[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[36] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\nGkioxari.\nAccelerating 3d deep learning with pytorch3d.\narXiv preprint arXiv:2007.08501, 2020. 1\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2, 4\n[38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2\n[39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[40] J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3d neural field generation\nusing triplane diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 20875\u201320886, 2023. 2\n[41] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser\nSheikh. Hand keypoint detection in single images using mul-\ntiview bootstrapping. In CVPR, 2017. 2\n[42] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.\nKey-locked rank one editing for text-to-image personaliza-\ntion. ACM SIGGRAPH 2023 Conference Proceedings, 2023.\n2\n[43] Vaibhav Vavilala and David Forsyth.\nApplying a color\npalette with local control using diffusion models, 2023. 2\n[44] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or.\nSketch-guided text-to-image diffusion models. In ACM SIG-\nGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.\n2\n[45] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-\nTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah\nLaszlo, David J Fleet, Radu Soricut, et al. Imagen editor\nand editbench: Advancing and evaluating text-guided im-\nage inpainting. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 18359\u2013\n18369, 2023. 2\n[46] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong\nChen, Qifeng Chen, and Fang Wen.\nPretraining is all\nyou need for image-to-image translation.\narXiv preprint\narXiv:2205.12952, 2022. 2\n[47] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser\nSheikh. Convolutional pose machines. In CVPR, 2016. 2\n[48] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun\nZhang. Smartbrush: Text and shape guided object inpainting\nwith diffusion model. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22428\u201322437, 2023. 2\n[49] Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao\nLi, Wentao Zhang, Bin Cui, Bernard Ghanem, and Ming-\nHsuan Yang. Diffusion-based scene graph to image gener-\nation with masked contrastive pre-training. arXiv preprint\narXiv:2211.11138, 2022. 2\n[50] Biao Zhang, Jiapeng Tang, Matthias Nie\u00dfner, and Peter\nWonka.\n3dshape2vecset: A 3d shape representation for\nneural fields and generative diffusion models. ACM Trans.\nGraph., 42(4), 2023. 2\n[51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 1, 2, 4\n[52] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong,\nYang Liu, and Heung-Yeung Shum. Locally attentional SDF\ndiffusion for controllable 3d shape generation. ACM Trans.\nGraph., 42(4):91:1\u201391:13, 2023. 2\n[53] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3d:\nA modern library for 3d data processing.\narXiv preprint\narXiv:1801.09847, 2018. 1\n10\nLOOSECONTROL: Lifting ControlNet for Generalized Depth Conditioning\nSupplementary Material\nFigure 10. Upper-bound violations in Matterport3D [10]. Top: RGB panorama images. Bottom: Depth rendered using the layout labels\nfrom the dataset.\n7. Implementation details\nFor all our experiments, we use Stable Diffusion v1.5 [37]\nand\nthe\ncorresponding\nControlNet\n[51]\ncheckpoint\n\u201clllyasviel/control v11f1p sd15 depth\u201d hosted on Hug-\nginFace [1].\nWe use the PyTorch [34] framework and\nthe diffusers [3] library as the framework for diffusion\nmodels. We use ZoeDepth [7] and SAM [24] for extracting\ndepth maps and segmentation maps respectively.\nFor\nSAM, we use min_mask_area=1e4. We make use of\nPytorch3D [36] and Open3D [53] in our 3D framework\nand PyTorch3D for rendering the depth maps for obtaining\nthe proxy depths for fine-tuning.\nFor backprojection,\nwe randomly choose an FOV in the range of 43 and 57\ndegrees. For both Scene Boundary Control and 3D Box\nControl, we use LoRA rank r = 8 and fine-tune only\nLoRA layers for 200 steps with a learning rate of 0.0001,\nand batch size of 12 with Adam [23] optimizer.\nWe\nuse controlnet_conditioning_scale=1.0 and\nLoRA scale factor \u03b3 = 1.2 as default. We built our 3D\neditor user interface using Gradio [5] and BabylonJS [2].\n8. Issues with Room Layout Datasets\nAs discussed in the main paper, a possible alternative for\nextracting the scene boundary for the preparation of the\ndataset for Scene Boundary Control could be room layout\ndatasets. However, we note that these datasets often con-\ntain ambiguous layout labels that directly violate our tight\nupper-bound condition required to implement scene bound-\nary control. We provide some examples of these violations\nfor the popular MatterPort3D dataset as the representative\nin Fig. 10\n9. User study additional details\nWe created an anonymous user study as a web form using\nstreamlit [4]. Users were presented with \u2018Two-alternative\nforced choice\u2019 (2-AFC) with two options as images gener-\nated by our baseline (ControlNet) and our result and asked\nto respond with their preference for the given text prompt\nand condition image. The options were anonymized and did\nnot indicate the name of the method. Each user was asked\nto respond to 10 randomized questions in total (5 for Scene\nBoundary Control, and 5 for 3D box control). The order\nof options was also randomized. As mentioned in the main\npaper, over 95% of responses were in favor of our method.\n"
  },
  {
    "title": "DreamComposer: Controllable 3D Object Generation via Multi-View Conditions",
    "link": "https://arxiv.org/pdf/2312.03611.pdf",
    "upvote": "7",
    "text": "DreamComposer: Controllable 3D Object Generation via Multi-View Conditions\nYunhan Yang1\u2217\nYukun Huang1\u2217\nXiaoyang Wu1\nYuan-Chen Guo3,4\nSong-Hai Zhang4\nHengshuang Zhao1\nTong He2\nXihui Liu1\u2020\n1 The University of Hong Kong\n2 Shanghai Artificial Intelligence Lab\n3 VAST\n4 Tsinghua University\n\u2217 Equal Contribution\nProject Page: https://yhyang-myron.github.io/DreamComposer/\nZero-1-to-3\nZero-1-to-3\n+ Ours\nZero-1-to-3\nZero-1-to-3\n+ Ours\nInput\nNovel View Synthesis\nInput\nSyncD\nNovel View Synthesis\nMesh\nInput\nSyncD\n+ Ours\nSyncD\nSyncD\n+ Ours\nNovel View Synthesis\nFigure 1. DreamComposer is able to generate controllable novel views and 3D objects via injecting multi-view conditions. We incorporate\nthe method into the pipelines of Zero-1-to-3 [17] and SyncDreamer (SyncD) [18] to enhance the control ability of those models.\n\u2020 Corresponding author.\n1\narXiv:2312.03611v1  [cs.CV]  6 Dec 2023\nAbstract\nUtilizing pre-trained 2D large-scale generative models,\nrecent works are capable of generating high-quality novel\nviews from a single in-the-wild image. However, due to the\nlack of information from multiple views, these works en-\ncounter difficulties in generating controllable novel views.\nIn this paper, we present DreamComposer, a flexible and\nscalable framework that can enhance existing view-aware\ndiffusion models by injecting multi-view conditions. Specif-\nically, DreamComposer first uses a view-aware 3D lifting\nmodule to obtain 3D representations of an object from mul-\ntiple views. Then, it renders the latent features of the target\nview from 3D representations with the multi-view feature fu-\nsion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion\nmodel.\nExperiments show that DreamComposer is com-\npatible with state-of-the-art diffusion models for zero-shot\nnovel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions,\nready for controllable 3D object reconstruction and various\nother applications.\n1. Introduction\n3D object generation is an emerging research topic in com-\nputer vision and graphics, serving a wide range of applica-\ntions such as augmented reality (AR), virtual reality (VR),\nfilm production, and game industry. With 3D object gen-\neration models, users and designers can easily create the\ndesired 3D assets with text or image prompts, without re-\nquiring considerable human endeavors by human experts.\nRecently, diffusion models [9, 29] achieve remarkable\nsuccess in generating 2D images from texts, which inspires\nthe exploration of 3D object generation using 2D diffusion\npriors [5, 12, 15, 26, 35, 39]. Although great 3D genera-\ntion results have been achieved [39], 2D diffusion models\nlack view control and struggle to provide view-consistent\nsupervision, resulting in various quality issues of 3D gener-\nation such as multiple faces and blurry details. To alleviate\nthis problem, Zero-1-to-3 [17] empowers 2D diffusion mod-\nels with viewpoint conditioning, enabling zero-shot novel\nview synthesis (NVS) conditioned on a single-view im-\nage and image-to-3D object generation. Considering the\ninconsistent output of Zero-1-to-3, a series of subsequent\nworks [18, 19, 31, 41, 44] are proposed to improve the 3D\nconsistency of the generated multi-view images. However,\nlimited by the incomplete information of single-view input,\nthese methods inevitably encounter unpredictable and im-\nplausible shapes and textures when predicting novel views.\nFor example, as shown on the right side of the third row\nof Figure 1, the actual number of shoes cannot be deter-\nmined if only given a side view of the shoes. In other words,\nnovel view synthesis and 3D object generation are not fully\ncontrollable with only single-view image conditions.\nTo address this problem, our core idea is to introduce\nflexible multi-view image conditioning to diffusion models,\nenabling more controllable novel view synthesis and 3D ob-\nject reconstruction. For example, based on the front view,\nback view, and side view of an object drawn by designers,\nthe model will generate images of other viewpoints that are\nconsistent with the multiple input images. It also allows\ninteractive 3D generation where users can provide condi-\ntioning images from new viewpoints if the generated 3D\nobjects do not follow the user intention. However, such an\nattempt is challenging for two reasons. Firstly, it is non-\ntrivial to integrate arbitrary numbers of input views into\nconsistent 3D representations that can guide the generation\nof the target view image. Secondly, it is challenging to de-\nsign a flexible framework that is compatible with and can be\nplugged into existing models such as Zero-1-to-3 [17] and\nSyncDreamer [18] to empower multi-view conditioning for\nvarious models.\nTo this end, we propose DreamComposer, a scalable\nand flexible framework that can extend existing view-\nconditioned models to adapt to an arbitrary number of\nmulti-view input images. DreamComposer comprises three\nstages: target-aware 3D lifting, multi-view feature fusion,\nand target-view feature injection. (i) Target-Aware 3D Lift-\ning encodes multi-view images into latent space and then\nlifts the latent features to 3D tri-planes [2]. The tri-plance\nrepresentation with latent features is compact and efficient,\nand the target-view-aware 3D lifting design allows the net-\nwork to focus more on building 3D features related to the\ntarget view. (ii) Multi-View feature fusion renders and fuses\nthe 3D features from different views to target-view 2D fea-\ntures with a novel composited volume rendering approach.\n(iii) Target-View Feature Injection injects the latent features\nfrom the previous stage into the diffusion models with a\nControlNet-like structure. The injection module takes the\nrelative angle as condition, allowing for adaptive gating\nof multi-view conditions. DreamComposer can be flexibly\nplugged into existing models, such as Zero-1-to-3 [17] and\nSyncDreamer [18], and endow them with the ability to han-\ndle multi-view input images, as shown in Figure 1.\nIn summary, we propose DreamComposer, a scalable\nand flexible framework to empower diffusion models for\nzero-shot novel view synthesis with multi-view condition-\ning. The scalability and flexibility of DreamComposer are\nempowered by our novel design of the target-aware 3D lift-\ning, multi-view feature fusion, and target-view feature in-\njection modules. Extensive experiments show that Dream-\nComposer is compatible with recent state-of-the-art meth-\nods, endowing high-fidelity novel view synthesis, control-\nlable 3D object reconstruction, and various other applica-\ntions such as controllable 3D object editing and 3D charac-\nter modeling with the ability to take multi-view inputs.\n2\n2. Related Work\nZero-shot Novel View Synthesis.\nPrevious works [8,\n14, 22] on novel view synthesis are generally trained on\ndatasets with limited scenes or categories and cannot gener-\nalize to in-the-wild image inputs. Recently, diffusion mod-\nels [29, 30] trained on large-scale Internet data have demon-\nstrated powerful open-domain text-to-image generation ca-\npabilities. This success inspired the community to imple-\nment zero-shot novel view synthesis by fine-tuning these\npre-trained diffusion models. Zero-1-to-3 [17] fine-tuned\nthe Stable Diffusion model [29] on the large 3D dataset\nObjaverse [6], achieving viewpoint-conditioned image syn-\nthesis of an object from a single in-the-wild image. Based\non Zero-1-to-3, several subsequent works [16, 18, 19, 31,\n41, 44] aim to produce multi-view consistent images from a\nsingle input image to create high-quality 3D objects. How-\never, limited by the ambiguous information of single input\nimage, these models might produce uncontrollable results\nwhen rendering novel views.\nDiffusion Models for Novel View Synthesis. In addition\nto fine-tuning directly on the pre-trained text-image dif-\nfusion models, Some recent works [3, 8, 14, 40, 47, 49]\nalso attempt to combine diffusion models with 3D priors\nfor novel view synthesis.\nGeNVS [3] integrates geom-\netry priors in the form of a 3D feature volume into the\n2D diffusion backbone, producing high-quality, multi-view-\nconsistent renderings on varied datasets. NerfDiff [8] dis-\ntills the knowledge of a 3D-aware conditional diffusion\nmodel into NeRF at test-time, avoiding blurry renderings\ncaused by severe occlusion. While remarkable outcomes\nhave been obtained for particular object categories from\nShapeNet [4] or Co3D [28], the challenge of designing a\ngeneralizable 3D-aware diffusion model for novel view syn-\nthesis from any in-the-wild inputs remains unresolved.\n3D Object Generation. Due to the limited size of exist-\ning 3D datasets, it remains challenging to train generative\n3D diffusion models [13, 23, 24, 37] using 3D data. With\npre-trained text-to-image diffusion models and score distil-\nlation sampling [26], DreamFusion-like methods [5, 10, 12,\n15, 21, 26, 35, 39, 48] have achieved remarkable text-to-\n3D object generation by distilling 2D image priors into 3D\nrepresentations. Some methods [20, 27, 33, 34, 42, 43] uti-\nlize similar distillation approaches to execute image-to-3D\ntasks. Since these works, which rely on an optimization\nstrategy, have not previously encountered real 3D datasets,\nthey face the Janus (multi-face) problem, making it chal-\nlenging to generate high-quality 3D object shapes.\n3. Method\nDreamComposer aims to empower existing diffusion mod-\nels for zero-shot novel view synthesis [17, 18, 44] with\nmulti-view conditions. It consists of three components: (i)\nTarget-Aware 3D Lifting extracts 2D features from multi-\nview inputs and transforms them into 3D representations\n(Sec. 3.1); (ii) Multi-View Feature Fusion renders and fuses\nthe 3D features from different views to target-view 2D fea-\ntures with a novel composited volume rendering approach\n(Sec. 3.2); (iii) Target-View Feature Injection injects the\ntarget-view features extracted from multi-view inputs into\nthe diffusion models for multi-view controllable novel view\nsynthesis (Sec. 3.3). All components are optimized in an\nAdapter [11, 45] fashion (Sec. 3.4). An overview pipeline\nof DreamComposer is demonstrated in Figure 2.\nFormulation. Given a main view x1 \u2208 RH\u00d7W \u00d73 and sev-\neral additional views x2, x3, ..., xn of an object, our target\nis to synthesize the novel view \u02c6x with the relative angle \u2206\u03b3\nto the main view. With the relative angle \u2206\u03b3, we can calcu-\nlate the relative camera rotation R \u2208 R3\u00d73 and translation\nT \u2208 R3. In general, we aim to learn a model M that can\nsynthesize a novel view \u02c6xR,T from a main view x1 and mul-\ntiple conditional views x2, x3, ..., xn:\n\u02c6xR,T = M(x1, x2, x3, ..., xn, R, T).\n(1)\n3.1. Target-Aware 3D Lifting\nExisting diffusion models [17, 18] for zero-shot novel view\nsynthesis are specialized for single-view input and there-\nfore cannot handle an undefined number of multi-view in-\nputs. For a scalable solution, we propose to lift 2D fea-\ntures from different views into 3D representations, ready\nfor view-conditional control.\n2D-to-3D Feature Lifting. Given an input image xi \u2208\nRH\u00d7W \u00d73 from the camera view i, we first utilize the im-\nage encoder of Stable Diffusion [29] to encode it into latent\nfeature fi \u2208 RH\u2032\u00d7W \u2032\u00d74, where H\u2032 \u00d7 W \u2032 is down-sampled\nimage size. Then, we introduce a 3D lifting module with\na convolutional encoder structure with self-attention and\ncross-attention layers. The 3D lifting module lifts the 2D la-\ntent feature fi into a 3D representation Fi \u2208 RH\u2032\u00d7W \u2032\u00d732\u00d73\nconditioned on the relative angle \u2206\u03b3. We adopt the tri-\nplane [2] feature Fi = {F xy\ni , F xz\ni , F yz\ni } as the 3D repre-\nsentation as it is compact and efficient enough to alleviate\nthe high training cost caused by multi-view inputs. Note that\nthe 2D-to-3D feature lifting is performed in latent space,\nwhich significantly reduces the computational cost.\nThe network structure of the 3D lifting module includes\nself-attention layers, cross-attention layers, and convolu-\ntional layers. Here we design a view conditioning mecha-\nnism based on cross-attention, enabling adaptive 3D lifting.\nSpecifically, we take the angle difference between the input\nview and the target view as a condition and inject it into the\n3D lifting module through the cross-attention layers. This\nmechanism allows 3D lifting to focus more on building 3D\nfeatures related to the target view, rather than trying to con-\nstruct a complete 3D representation.\n3\nInput Views\nLatent Views\nconcat\nMulti-View\nFeature Fusion\nTarget \nRelative Pose\nLatent\nOutput\nTarget-View\nFeature Injection\n\u2026\n\u2026\nLatent Tri-plane\nUNet\nplus\nconcat\n\u2026\n\u2026\nTarget-Aware\n3D Lifting\nImage Encoding\n\u2206v1\n\u2206v1\n\u2206v2\n\u2206v3\n\u2206v1\nRelative Angle Attention\nTrainable Modules of DreamComposer\nPre-trained Diffusion Model\nMain View\nTarget-Aware\n3D Lifting\nTarget-Aware\n3D Lifting\nFigure 2. An overview pipeline of DreamComposer. Given multiple input images from different views, DreamComposer extracts their\n2D latent features and uses a 3D lifting module to produce tri-plane 3D representations. Then, the multi-view condition rendered from 3D\nrepresentations is injected into the pre-trained diffusion model to provide target-view auxiliary information.\nMulti-View Cases. Given multiple input images from n\ndifferent views, i.e. x1, x2, ..., xn, we can obtain their tri-\nplane features {F1, F2, ..., Fn} via 2D image encoding and\n2D-to-3D feature lifting. These tri-plane features are ready\nfor providing target-view auxiliary information in subse-\nquent multi-view conditioning.\n3.2. Multi-View Feature Fusion\nAfter obtaining the 3D features {F1, F2, ..., Fn} of input\nimages from n different views, target-view latent feature ft\ncan be extracted from these 3D features as the condition for\nthe diffusion model.\nTo render the target-view latent feature ft, 3D features\nfrom different views need to be fused. However, this is\ntricky because these 3D features are lifted in different cam-\nera spaces and are not aligned. To deal with it, we use a\ncomposited volume rendering approach: (1) sampling ray\npoints from the target view; (2) projecting these points onto\ndifferent input-view camera spaces; (3) indexing and aggre-\ngating 3D point features from different views; (4) integrat-\ning point features along the target-view rays to render the\ndesired latent feature ft.\nIn particular, we adopt a weighting strategy to adap-\ntively aggregate 3D point features from different inputs,\nconsidering that different input views contribute differently\nto the target view. Given n input views, the azimuth dif-\nferences between them and the target view are denoted as\n\u2206\u03b81, \u2206\u03b82, ..., \u2206\u03b8n. Then, the weight of input view i can be\nformulated as:\n\u03bbi = cos \u2206\u03b8i + 1\n2\n,\n(2)\nand the weighted 3D point feature aggregation across dif-\nferent views is formulated as:\nf t\np =\nn\nX\ni=1\n\u00af\u03bbi \u00b7 f i\np,\n(3)\nwhere f t\np and f i\np denote feature embeddings of 3D point\np from target view and input view i, respectively; while\n\u00af\u03bbi is the nomalized weight of input view i calculated by\n\u03bbi/ Pn\ni=1 \u03bbi. Finally, all sampled 3D point\u2019s features f t\np\nare integrated along the target-view rays using the volume\nrendering equation [22] and yield ft.\n3.3. Target-View Feature Injection\nLatent feature ft contains rich target-view information ex-\ntracted from multi-view inputs. We inject ft into the dif-\nfusion model\u2019s UNet to provide multi-view conditions. To\nachieve this, we follow ControlNet [45] structure for target-\nview feature injection. Specifically, we clone the network\nblocks of the diffusion model\u2019s UNet to trainable copies.\nThese copies, serving as target-view feature injection mod-\nules, take the latent feature ft as conditional input and pre-\ndict residuals added to the intermediate outputs of UNet.\nMost details are consistent with ControlNet [45], except\nthat the input layer needs to be modified to match the size\n4\nof latent input ft. Besides, we also take the angle difference\nbetween the main view and the target view as a condition,\nand inject it into the multi-view injection module through\nthe cross-attention layers. This design enables adaptive gat-\ning of multi-view conditions: auxiliary information from\nmultiple views is less important when the main view and\nthe target view are close.\n3.4. Training and Inference\nIn Sec. 3.1, Sec. 3.2, and Sec. 3.3, we respectively intro-\nduce the target-aware 3D lifting, multi-view feature fusion,\nand target-view feature injection modules, empowering the\npre-trained diffusion model with multi-view inputs. Among\nthose modules the target-aware 3D lifting and target-view\ninjection modules are trainable. To train these additional\nmodules, we always sample three views of objects in each\niteration, including a front view, a back view, and a random\nview. This sampling strategy improves training efficiency\nwhile encouraging generalization to arbitrary view inputs.\nGiven multi-view input images, we further propose a two-\nstage training paradigm.\nIn the first stage, we pre-train the target-aware 3D lift-\ning module on the proxy task of sparse view reconstruction.\nGiven several input views of an object, the 3D lifting mod-\nule is encouraged to predict novel views correctly, with a\nmean square error (MSE) loss in latent space as objective.\nIn the second stage, a pre-trained diffusion model such as\nZero-1-to-3 [17] is introduced as the frozen backbone. To\nenhance it with multi-view conditioning, our target-aware\n3D lifting, multi-view feature fusion, and target-view fea-\nture injection are integrated and optimized jointly. We use\ndiffusion loss and MSE loss as in the first stage for training.\nIn the inference stage, the trained model is flexible and\ncan take one or more images from different views as inputs,\nenabling zero-shot novel view synthesis under multi-view\nconditions. It also benefits downstream 3D reconstruction\nand generation tasks with scalability and controllability.\n4. Experiments\nWe evaluate the effectiveness of DreamComposer on zero-\nshot novel view synthesis and 3D object reconstruction.\nDatasets, evaluation metrics, and implementation details are\nprovided in Section 4.1 and Section 4.2. To show the flexi-\nbility of our framework, we integrate DreamComposer into\nZero-1-to-3 [17] and SyncDreamer [18], as described in\nSection 4.3 and Section 4.4 respectively. We further demon-\nstrate the applications of DreamComposer in Section 4.5,\nincluding controllable editing and 3D character modeling.\nWe further conduct ablation study in Section 4.6.\n4.1. Datasets and Evaluation Metrics\nTraining Dataset. We train DreamComposer (DC) on the\nlarge-scale Objaverse [6] dataset containing around 800k\nZero-1-to-3\nInput\nNovel View Synthesis\nZero-1-to-3\n+ Ours\nZero-1-to-3\nZero-1-to-3\n+ Ours\nFigure 3. Qualitative comparisons with Zero-1-to-3 [17] in con-\ntrollable novel view synthesis. DC-Zero-1-to-3 effectively gener-\nates more controllable images from novel viewpoints by utilizing\nconditions from multi-view images.\n(a) Elevation Degree - 0\nMethods\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nZero-1-to-3 [17]\n20.82\n0.840\n0.139\nZero-1-to-3+Ours\n25.25\n0.888\n0.088\n(b) Elevation Degree - 15\nMethods\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nZero-1-to-3\n21.38\n0.837\n0.131\nZero-1-to-3+Ours\n25.85\n0.891\n0.083\n(c) Elevation Degree - 30\nMethods\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nZero-1-to-3\n21.66\n0.837\n0.128\nZero-1-to-3+Ours\n25.63\n0.885\n0.086\nTable 1. Quantitative comparisons of novel view synthesis on GSO\ndataset. We use four orthogonal angles as inputs, selecting the im-\nage nearest to the target viewpoint as both the input for Zero-1-to-\n3 and the main view for DC-Zero-1-to-3. The other three images\nare utilized as supplementary condition-views for DC-Zero-1-to-\n3. Additionally, we compute results for input elevation angles set\nat 0, 15, and 30 degrees, respectively.\n3D objects. We randomly pick two elevation angles for ev-\nery object and render N images with the azimuth evenly\ndistributed in [0\u25e6, 360\u25e6]. We set N to 36 for DC-Zero-1-to-\n3 and 16 for DC-SyncDreamer. For training and inference,\nimage sizes are 256 \u00d7 256 and background is set to white.\nEvaluation Dataset. To evaluate the generalization of our\nmodel to out-of-distribution data, we extend our evalua-\ntion dataset from Objaverse to Google Scanned Objects\n(GSO) [7], which contains high-quality scans of everyday\n5\nInput\nNovel View Synthesis\nMesh\nSynD\nSynD\n+ Ours\nSynD\nSynD\n+ Ours\nFigure 4. Qualitative comparison with SyncDreamer (SyncD) [18] in controllable novel view synthesis and 3D reconstruction. The image\nin \u25a1 is the main input, and the other image in \u25a1 is the conditional input generated from Zero-1-to-3 [17]. With more information in\nmulti-view images, DC-SyncDreamer is able to generate more accurate back textures and more controllable 3D shapes.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nRealfusion [20]\n15.26\n0.722\n0.283\nZero-1-to-3 [17]\n18.93\n0.779\n0.166\nSyncDreamer [18]\n20.05\n0.798\n0.146\nSyncDreamer+Ours\n20.52\n0.828\n0.141\nTable 2. Quantitative comparisons of novel view synthesis on GSO\ndataset. We employ images generated from diffusion models as\nour additional condition-view.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\ntrainable UNet\n15.96\n0.762\n0.209\nw/o reconstruction loss\n16.18\n0.766\n0.206\nw/o view conditioning\n19.04\n0.805\n0.166\nfull model\n20.52\n0.828\n0.141\nTable 3. Ablation study on GSO dataset. Eliminating the recon-\nstruction loss and training the UNet are both factors that negatively\nimpact the final outcome. With view conditioning in the 3D lifting\nmodule, our model not only ensures more stable training but also\nyields the most optimal results.\nhousehold items. This evaluation setting is consistent with\nthat for SyncDreamer [18], comprising 30 objects that in-\nclude both commonplace items and various animal species.\nEvaluation Metrics. Following previous works [17, 18],\nWe utilize Peak Signal-to-Noise Ratio (PSNR), Structural\nSimilarity Index (SSIM) [38], and Learned Perceptual Im-\nage Patch Similarity (LPIPS) [46] as metrics.\n4.2. Implementation Details\nDuring the entire training process, we randomly pick a tar-\nget image as ground truth and utilize a set of three images\nas inputs: two images captured from opposing angles and\none image from a random angle. Benefited from the image\ntriplet training scheme, our model can adapt to two or more\ninputs. This data sampling strategy not only improves the\nefficiency of the model\u2019s optimization but also preserves its\nscalability and adaptability to various input configurations.\nWe adopt a two-stage training strategy for DreamCom-\nposer. In the first stage, we focus on the 3D feature lifting\nmodule and pre-train it for 80k steps (\u223c 3 days) with 8 80G\nA800 GPUs using a total batch size of 576. The pre-trained\n3D lifting module can be applied in conjunction with differ-\nent pre-trained diffusion models for subsequent training. In\nthe second stage, we jointly optimize the 3D lifting and fea-\nture injection module. This stage takes 30k steps (\u223c 2 days)\nwith 8 80G A800 GPUs using a total batch size of 384.\n4.3. Plugged into Zero-1-to-3\nWe evaluate the NVS performance of DreamComposer\nplugged into the Zero-1-to-3 pipeline [17].\nEvaluation Protocols. When provided with an input image\nof an object, Zero-1-to-3 [17] has the ability to generate new\nperspectives of the same object. We select four orthogonal\nangles as inputs, where the image closest to the target view\nis taken as the input for Zero-1-to-3 as well as the main\nview for DC-Zero-1-to-3. The remaining three images serve\nas the additional condition views for DC-Zero-1-to-3. We\n6\nControl\nInput\nEditing\nInput\nNovel View Synthesis\nColor Editing\nDrag Editing\nPersonalize Editing\n(a)\n(b)\n(c)\nFigure 5. Controllable Editing. We present personalize editing\nwith InstructPix2Pix [1] in (a), drag editing with DragGAN [25],\nDragDiffusion [32] in (b), and color editing in (c).\nprovide the results with elevation angles of 0, 15, and 30\ndegrees respectively.\nEvaluation on NVS. The comparison of quantitative re-\nsults is shown in Table 1, and the comparison of qualita-\ntive results is shown in Figure 3. While Zero-1-to-3 pos-\nsesses the ability to produce visually plausible images from\nnovel views, the absence of multi-view inputs compromises\nthe accuracy of these unseen viewpoints. Our DC-Zero-\n1-to-3, by conditioning on multi-view images, ensures the\ncontrolled generation of new viewpoints while maintaining\nthe integrity of its diffusion model\u2019s generative capabili-\nties. DC-Zero-1-to-3 significantly surpasses other methods\nin terms of the quality and consistency of generated images\nacross various angles.\n4.4. Plugged into SyncDreamer\nWe integrate DreamComposer into the SyncDreamer [18]\npipeline and evaluate the performance on multiview-\nconsistent view synthesis and 3D object reconstruction.\nEvaluation Protocols.\nWe compare our method with\nSyncDreamer [18], Zero-1-to-3 [17], and RealFusion [20].\nGiven an input image of an object, Zero-1-to-3 can synthe-\nsize novel views of the object, and SyncDreamer is able to\ngenerate consistent novel views from 16 fixed views. Re-\nalFusion [20] is a single-view reconstruction method based\non Stable Diffusion [29] and SDS [26]. The opposite view\nof the input is generated by Zero-1-to-3 [17] and serves as\nan additional condition view for DC-SyncDreamer. We ad-\nhere to the same input configurations as in SyncDreamer for\nfair comparison. The mesh of generated object is directly\nreconstructed from multi-view images by NeuS [36].\nEvaluation on NVS and 3D Reconstruction. The compar-\nison of quantitative results is shown in Table 2, and the com-\nparison of qualitative results is shown in Figure 4. While\nSyncDreamer is able to generate consistent novel views, the\nshape of the object and the texture on the back may still ap-\npear unreasonable. DC-SyncDreamer not only maintains\nmulti-view consistency in colors and geometry but also en-\nhances the control over the shape and texture of the newly\ngenerated perspectives.\n4.5. Applications\nWe explore the various applications of DreamComposer, in-\ncluding controllable 3D object editing with DC-Zero-1-to-3\nand 3D character modeling with DC-SyncDreamer.\nControllable 3D object Editing. DreamComposer is able\nto perform controllable editing by modifying or designing\nimages from certain perspectives, as shown in Figure 5. We\ndesignate an image from a specific viewpoint as the \u201ccontrol\ninput\u201d, which remains unaltered. Concurrently, we manip-\nulate an \u201cediting input\u201d, which represents an image from an\nalternate viewpoint. We utilize InstructPix2Pix [1], Drag-\nGAN [25] and DragDiffusion [32] to manipulate the image,\nthereby achieving our desired style, corresponding to (a),\n(b) in Figure 5 respectively. And we modify the color of\nthe editing input ourselves in (c). Subsequently, we employ\nthe modified images in conjunction with the control input to\nsynthesize novel views.\n3D Character Modeling.\nWith DC-SyncDreamer, 3D\ncharacters can be modeled from only a few 2D paintings,\nas shown in Figure 6. This can significantly improve the ef-\nficiency of existing 3D pipelines, and is expected to be con-\nnected with ControlNet for text-to-3D character creation.\n4.6. Ablation Analysis\nWe conduct ablation studies on DC-SyncDreamer, where\nthe quantitative results is reported in Table 3, while the qual-\nitative examples is shown in Figure 7.\nNecessity of reconstruction loss. To evaluate the necessity\nof reconstruction loss, we remove the MSE loss of the sec-\nond training stage as described in Section 3.4. As shown in\nFigure 7 and Table 3, without reconstruction MSE loss, the\nmulti-view 3D lifting module is unable to produce an ef-\nfective latent output, resulting in the inability to synthesize\nsatisfactory novel view results in the end.\n7\nInput\nNovel View Synthesis\nMesh\nFigure 6. 3D Character Modeling. DC-SyncDreamer is able to reconstruct arbitrary objects with rarely sparse inputs. We present the\nresults of 3D character modeling from multi-view 2D paintings.\nInput\nLatent Output\nNovel View Synthesis\nDC-SyncD\nTrain UNet\nW/O Recon\nFigure 7.\nAblation studies to verify the designs of our ap-\nproach. \u201cDC-SyncD\u201d means our full model incorporating with\nSyncDreamer [18] pipeline. \u201cTrain UNet\u201d means we finetune the\nUNet with our modules without freezing it. \u201cW/O Recon\u201d means\nremoving the reconstruction MSE loss in the second step of train-\ning. The Latent Output is derived by rendering and pooling fea-\ntures in the tri-planes, as shown in Figure 2. Without reconstruc-\ntion loss, DC-SyncDreamer fails to produce precise latent outputs.\nFinetuning v.s. freezing the diffusion U-Net. In our de-\nsign, the pre-trained diffusion model\u2019s U-Net is frozen dur-\ning training DreamComposer. For ablation analysis, we try\nto finetune the U-Net with DreamComposer\u2019s modules in\nthe second stage of training. As shown in Figure 7 and Ta-\nble 3, the model performance decreases when we finetune\nthe U-Net together with our modules.\nNecessity of view-conditioning for 3D lifting. We further\nremove the view conditioning cross-attention mechanism\nof the 2D-to-3D lifting module for necessity analysis. As\nshown in Table 3, removing the view conditioning leads to\nworse performance. We also empirically observe that the\ntraining is unstable without view conditioning.\nScalability for arbitrary numbers of input views. We val-\nidate our model\u2019s flexibility and scalability in managing ar-\n2 Inputs\n3 Inputs\n4 Inputs\nGT\nInput\nNovel View Synthesis\n0\u00b0\n90\u00b0\n180\u00b0\n270\u00b0\n45\u00b0\n135\u00b0\n225\u00b0\n315\u00b0\nFigure 8.\nAblation study to demonstrate the scalability of our\nmodel. Our model has the capacity to process arbitrary inputs,\nand its ability to control outcomes enhances correspondingly with\nthe increasing information of input data.\nbitrary numbers of inputs. As shown in Figure 8, our model\ncan handle arbitrary numbers of input views, and its con-\ntrollability is strengthened proportionally with the increas-\ning number of input views.\n5. Conclusion and Discussions\nWe propose DreamComposer, a flexible and scalable frame-\nwork to empower existing diffusion models for zero-shot\nnovel view synthesis with multi-view conditioning. Dream-\nComposer is scalable to the number of input views. It can\nbe flexibly plugged into a range of existing state-of-the-\nart models to empower them to generate high-fidelity novel\nview images with multi-view conditions, ready for control-\nlable 3D object reconstruction. More discussions and limi-\ntations are presented in the supplementary materials.\n8\nAcknowledgements\nThanks for the rendering data from VAST, and the discus-\nsion and proofreading by Yixing Lao, Yuan Liu and Chen-\nming Zhu.\nReferences\n[1] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions,\n2023. 7\n[2] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 2, 3\n[3] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W\nBergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini\nDe Mello, Tero Karras, and Gordon Wetzstein. Generative\nnovel view synthesis with 3d-aware diffusion models. arXiv\npreprint arXiv:2304.02602, 2023. 3\n[4] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 3\n[5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2, 3\n[6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects, 2022. 3, 5\n[7] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items.\nIn ICRA,\n2022. 5\n[8] Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNerfdiff: Single-image view synthesis with nerf-guided dis-\ntillation from 3d-aware diffusion, 2023. 3\n[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[10] Susung Hong, Donghoon Ahn, and Seungryong Kim. Debi-\nasing scores and prompts of 2d diffusion for view-consistent\ntext-to-3d generation, 2023. 3\n[11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly.\nParameter-efficient transfer\nlearning for nlp. In International Conference on Machine\nLearning, pages 2790\u20132799. PMLR, 2019. 3\n[12] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023. 2, 3\n[13] Heewoo Jun and Alex Nichol. Shap-e: Generating condi-\ntional 3d implicit functions, 2023. 3\n[14] Jon\u00b4a\u02c7s Kulh\u00b4anek, Erik Derner, Torsten Sattler, and Robert\nBabu\u02c7ska. Viewformer: Nerf-free neural rendering from few\nimages using transformers.\nIn European Conference on\nComputer Vision, pages 198\u2013216. Springer, 2022. 3\n[15] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2, 3\n[16] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang,\nChao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Ji-\nayuan Gu, and Hao Su. One-2-3-45++: Fast single image\nto 3d objects with consistent multi-view generation and 3d\ndiffusion, 2023. 3\n[17] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 1, 2, 3, 5, 6, 7\n[18] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n1, 2, 3, 5, 6, 7, 8\n[19] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, and Wenping Wang.\nWonder3d: Single image to 3d using cross-domain diffusion,\n2023. 2, 3\n[20] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8446\u20138455, 2023. 3, 6, 7\n[21] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation of\n3d shapes and textures, 2022. 3\n[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 3,\n4\n[23] Norman\nM\u00a8uller,\nYawar\nSiddiqui,\nLorenzo\nPorzi,\nSamuel Rota Bul`o,\nPeter Kontschieder,\nand Matthias\nNie\u00dfner.\nDiffrf:\nRendering-guided 3d radiance field\ndiffusion, 2023. 3\n[24] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generating\n3d point clouds from complex prompts, 2022. 3\n[25] Xingang Pan, Ayush Tewari, Thomas Leimk\u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag your\ngan: Interactive point-based manipulation on the generative\nimage manifold, 2023. 7\n9\n[26] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022. 2,\n3, 7\n[27] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843,\n2023. 3\n[28] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3d: Large-scale learning and evaluation of\nreal-life 3d category reconstruction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 10901\u201310911, 2021. 3\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 2, 3, 7\n[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 3\n[31] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model, 2023. 2, 3\n[32] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Han-\nshu Yan, Wenqing Zhang, Vincent Y. F. Tan, and Song Bai.\nDragdiffusion: Harnessing diffusion models for interactive\npoint-based image editing, 2023. 7\n[33] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 3\n[34] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d\ncreation from a single image with diffusion prior, 2023. 3\n[35] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 2, 3\n[36] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction,\n2023. 7\n[37] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, and Baining Guo. Rodin: A generative\nmodel for sculpting 3d digital avatars using diffusion, 2022.\n3\n[38] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 6\n[39] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 3\n[40] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. arXiv\npreprint arXiv:2210.04628, 2022. 3\n[41] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong\nZhang, C. L. Philip Chen, and Lei Zhang. Consistent123:\nImprove consistency for one image to 3d object synthesis,\n2023. 2, 3\n[42] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360deg views. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4479\u20134489, 2023. 3\n[43] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360\u25e6 views, 2023. 3\n[44] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng\nWang. Consistent-1-to-3: Consistent image to 3d view syn-\nthesis via geometry-aware diffusion models, 2023. 2, 3\n[45] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 3, 4\n[46] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 6\n[47] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12588\u201312597, 2023. 3\n[48] Junzhe Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d\nwith advanced diffusion guidance, 2023. 3\n[49] Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang,\nYing Shan, and Song-Hai Zhang.\nSparse3d:\nDistill-\ning multiview-consistent diffusion for object reconstruction\nfrom sparse views. arXiv preprint arXiv:2308.14078, 2023.\n3\n10\n"
  },
  {
    "title": "MagicStick: Controllable Video Editing via Control Handle Transformations",
    "link": "https://arxiv.org/pdf/2312.03047.pdf",
    "upvote": "7",
    "text": "MagicStick: Controllable Video Editing via Control Handle Transformations\nYue Ma1\nXiaodong Cun2\u2020\nYingqing He1\nChenyang Qi1\nXintao Wang2\nYing Shan2\nXiu Li3\nQifeng Chen1\u2020\n1HKUST\n2Tencent AI Lab\n3Tsinghua University, SIGS\nhttps://magic-stick-edit.github.io/\nA bird standing on a beach next to the ocean water and sand\n+\nbird\n\u2192\neagle\n+\nExtract\nUser Editing\nZoom In\nZoom In\nZoom Out\nMove\nFigure 1.\nControllable video editing via modifying control handle. We present a unified framework to modify video properties (e.g.,\nshape, size, location, motion) leveraging the manual keyframe transformations on the extracted internal control signals.\nAbstract\nText-based video editing has recently attracted consid-\nerable interest in changing the style or replacing the ob-\njects with a similar structure.\nBeyond this, we demon-\nstrate that properties such as shape, size, location, motion,\netc., can also be edited in videos. Our key insight is that\nthe keyframe\u2019s transformations of the specific internal fea-\nture (e.g., edge maps of objects or human pose), can eas-\nily propagate to other frames to provide generation guid-\nance. We thus propose MagicStick, a controllable video\n\u2020 Corresponding author.\nediting method that edits the video properties by utilizing\nthe transformation on the extracted internal control signals.\nIn detail, to keep the appearance, we inflate both the pre-\ntrained image diffusion model and ControlNet to the tem-\nporal dimension and train low-rank adaptions (LORA) lay-\ners to fit the specific scenes. Then, in editing, we perform\nan inversion and editing framework. Differently, finetuned\nControlNet is introduced in both inversion and generation\nfor attention guidance with the proposed attention remix\nbetween the spatial attention maps of inversion and edit-\ning. Yet succinct, our method is the first method to show\nthe ability of video property editing from the pre-trained\ntext-to-image model. We present experiments on numerous\narXiv:2312.03047v1  [cs.CV]  5 Dec 2023\nexamples within our unified framework. We also compare\nwith shape-aware text-based editing and handcrafted mo-\ntion video generation, demonstrating our superior temporal\nconsistency and editing capability than previous works. The\ncode and models will be made publicly available.\n1. Introduction\nDue to the remarkable progress of text-to-image (T2I) gen-\neration [20, 25, 48], video editing has recently achieved sig-\nnificant progress leveraging the generation prior of text-to-\nimage models [49]. Previous works have studied various\nediting effects, such as visual style transfer [3, 16, 45, 61]\nor modification of the character and object to a similar\none [7, 21, 31]. However, many straightforward edits in the\nvideo remain out of reach currently. For example, can we\nenlarge or resize the specific objects in the video? What if\nwe replace the localization of the specific objects? Or even,\ncan we edit the human motion in the video?\nThese editings seem possible for images [13] but it is\nhard for video. First, each frame in the original video is\ndifferent so the editing needs to be applied to all the frames\nindividually. Second, the temporal consistency will deteri-\norate if we apply frame-wise editing to the video directly.\nFinally, taking resizing as an example, if we directly seg-\nment the object and then edit, the generated video still re-\nlies on the abilities of additional inpainting [1, 2] and har-\nmonization [10]. To handle these problems, our key insight\nis inspired by ControlNet [65], where the structure can be\ngenerated following additional control. Editing the control\nsignal is a relatively easy and clear way other than directly\nediting appearance. Thus, we first segment the specific ob-\nject and then perform the editing on the one specific frame\nof the video, where other frames can utilize the same trans-\nformations by simple propagation.\nTo this end, we propose MagicStick, a universal\nvideo editing framework for geometry editing rather than\nappearance. To keep appearance, we first train a control-\nlable video generation network following [36, 58] based on\nthe pre-trained Stable Diffusion [49] and ControlNet [65]\nto get the specific appearance of the given video with the\ncontrol signal and text prompt. After that, we involve the\ntransformed conditional in both video inversion and gener-\nation. Finally, we propose a novel attention remixing block\nby remixing the attention in the inversion and generation\nwith the guidance of fine-tuned ControlNet signals for edit-\ning. Thanks to our unified framework, we can successfully\nmake changes to the specific video in terms of shape, size,\nand localization. To the best of our knowledge, we are the\nfirst to demonstrate the ability of such video editing via the\npure text-to-image model without any 3D information. Be-\nsides the mentioned attributes, we can also alter the motion\nin the input human video, which is also absent in previous\nT2I-based video editing approaches. To justify the effec-\ntiveness of our novel editing framework, we conduct ex-\ntensive experiments on various videos, demonstrating the\nsuperiority of our approach.\nOur contributions are summarized as follows:\n\u2022 We demonstrate the ability of a series of important video\nediting aspects (including shape, size, location, and hu-\nman motion) for the first time by introducing a new\nunified controllable video editing framework using pre-\ntrained T2I models.\n\u2022 We propose an effective attention remix module utilizing\nthe attention from control signals to faithfully retain the\nedit-unrelated information from the source video.\n\u2022 The experiments show the advantages of the proposed\nmethods over the previous similar topics, e.g., Shape-\naware Video Editing [31] and handcrafted motion con-\ntrols [56].\n2. Related Work\nVideo Editing. Editing natural videos is a vital task that\nhas drawn the attention of current researchers in the field\nof computer vision. Before the advent of diffusion mod-\nels, many GAN-based approaches [15, 17, 42, 44] have\nachieved good performance. The emergency of diffusion\nmodels [52] delivers higher quality and more diverse edit-\ning results. Text2live [3] and StableVideo [7] present layer-\natlas-based methods and edit the video on a flattened texture\nmap. FateZero [45] and Video-p2p [33], guided by orig-\ninal and target text prompts, perform semantic editing by\nblending cross-attention activations. There are also some\napproaches [14, 39] edit the appearance of the generated im-\nages by powerful yet private video diffusion models. How-\never, Most of these methods mainly focus on the texture of\ncontent rather than the shape editing, which is a more chal-\nlenging topic. They show obvious artifacts even with the\noptimization of generative priors. In contrast, our frame-\nwork can achieve the editing of complex properties includ-\ning shape, size, and location, while maintaining both ap-\npearance and temporal consistency.\nImage and Video Generation. Text-to-image generation\nis a popular topic with extensive research in recent years.\nMany approaches have developed based on transformer ar-\nchitectures [12, 18, 19, 25, 26, 47, 60, 62, 63] to achieve\ntextual control for generated content. Currently, diffusion-\nbased models [23, 41] have gained remarkable attention\ndue to the impressive generation performance.\nDALLE-\n2 [48] enhances text-image alignments by leveraging the\nrepresentation space of CLIP [46] and Imagen [51] employs\ncascaded diffusion models to achieve high-definition image\ngeneration. Self-guidance [13] conducts controllable image\ngeneration using attention and activations However, it op-\nerates attention during generation and struggles to maintain\nthe consistency with input. Moreover, its focus is on image\nStructure Guided\n\ud83d\udd25\nText encoder\nAttention ReMix module\nConv Layers (frozen)\nST Attn\nCross Attn\nWith trainable customized LoRA\nWith trainable Temporal Layer\nTrainable token\nFFN\nStructure \nGuided\n\u00d7T steps\n\u00d7T steps\nDDIM \nDenoising\nDDIM\nInversion\nA <new1> swan with a red beak swimming in a river.\nUnet Block (frozen)\nSpatial-Temporal Self-Attention\nStructure \nGuided\noriginal\ntarget\npropagate\n1st Frame Editing\nedited conditon\nedited conditon\nextract\nFigure 2. Overview. We propose a controllable video editing method that edits the video properties by utilizing the transformation on the\nextracted internal control signals. To achieve this, on the left, we tune the given video based on the extracted control signals and the text\nprompt for video customization. On the right, we first edit the key frame manually and propagate it to other frames as guidance. Then,\nrelying on the proposed attention remix module, we can achieve controllable video editing using structure guidance during inversion and\ndenoising.\ngeneration and cannot be applied to video editing directly.\nTo address a similar problem in video generation, vari-\nous works [4, 20, 34\u201336, 66] try to extend LDM to video\ndomain.\nThe approaches such as MagicVideo [67] and\nGEN1 [14] adopt a distinct strategy by initializing the\nmodel from text-to-image [49]. These methods then gener-\nate continuous content by incorporating additional temporal\nlayers. Tune-A-Video [58] proposes a method that special-\nizes in one-shot text-to-video generation tasks. The model\nhas the ability to generate video with similar motion to the\nsource video. However, how to edit real-world video con-\ntent using this model is still unclear. Inspired by the image\ncontrollable generation methods [65] and Tune-A-Video,\nour method can achieve video property editing in real-world\nvideos by leveraging the pretrained text-to-image model.\nImage Editing has a long history in computer vision. Early\nworks mainly leverage GAN [17] or VAE [30] to generate\nresults in some specific domain. Thanks to the development\nof diffusion model [52], many current works adopt the pre-\ntrained diffusion model [24] for editing. SDEdit [37] adds\nnoise and corruption to the image for generating the content.\nDiffEdit [9] and Blended Diffusion [1, 2] use the additional\nmask to blend the inversion noises or representations dur-\ning the image generation process. Prompt-to-Prompt [22]\nachieve semantic editing by mixing activations from origi-\nnal and target text prompts. Similar work has also been pro-\nposed by Plug-and-Play [53], Pix2pix-Zero [43] and Masac-\ntrl [6]. There are also some methods [29, 38, 57] achieving\nbetter editing performance by finetuning on a single image.\nHowever, applying these image editing approaches to video\nframes directly will lead to serious issues such as flickering\nand inconsistency among frames.\n3. Method\nWe aim to edit the property changes (e.g., shape, size, lo-\ncation, motion) in a video through transformations on one\nspecific control signal as shown in Fig. 2. Below, we first\ngive the basic knowledge of the latent diffusion model and\ninversion in Sec. 3.1. Then, we introduce our video cus-\ntomization method in Sec. 3.2 to keep the appearance. Fi-\nnally, we present the details of control handle transforma-\ntion (Sec. 3.3) and the editing details in inference (Sec. 3.4).\n3.1. Preliminary\nLatent Diffusion Models.\nDerived from diffusion Mod-\nels, Latent Diffusion Models [49] reformulate the diffu-\nsion and denoising procedures within a latent space. First,\nan encoder E compresses a pixel space image x to a low-\nresolution latent z = E(x) , which can be reconstructed\nfrom latent feature to image D(z) \u2248 x by decoder D. Sec-\nond, a U-Net [50] \u03b5\u03b8 with self-attention [54] and cross-\nattention is optimized to eliminate the artificial noise using\nthe objective during training:\nmin\n\u03b8\nEz0,\u03b5\u223cN(0,I),t\u223c Uniform (1,T ) \u2225\u03b5 \u2212 \u03b5\u03b8 (zt, t, p)\u22252\n2 , (1)\nwhere p is the embedding of the text prompt and zt is a\nnoisy sample of z0 at timestep t. After training, we can\ngenerate the image from a random noise \u03b5 and a text em-\nbedding p to recover the latent z and then decoded by D.\nDDIM Inversion. During inference, we can use determin-\nistic DDIM sampling to transform a random noise zT to a\nclean latent z0 across a sequence of timesteps from T to 1:\nzt\u22121 = \u221a\u03b1t\u22121\nzt \u2212 \u221a1 \u2212 \u03b1t\u03b5\u03b8\n\u221a\u03b1t\n+\np\n1 \u2212 \u03b1t\u22121\u03b5\u03b8,\n(2)\nwhere \u03b1t is the parameter for noise scheduling [24, 52].\nThus, DDIM Inversion [11] is proposed to inverse the above\nprogress from a clean latent space z0 to a noised latent space\nzT by adding noising:\n\u02c6zt = \u221a\u03b1t\n\u02c6zt\u22121 \u2212 \u221a1 \u2212 \u03b1t\u22121\u03b5\u03b8\n\u221a\u03b1t\u22121\n+\n\u221a\n1 \u2212 \u03b1t\u03b5\u03b8,\n(3)\nIn this context, the z0 can be reconstructed by inverted latent\n\u02c6zT using DDIM and classifier-free guidance.\n3.2. Controllable Video Customization\nSince the generation process of the generative model is too\nstochastic to keep the appearance, for our tasks, we first tune\nthe network to satisfy our requirements, which is similar to\nmodel customization in text-to-image generation. Differ-\nently, we also involve the pretrained ContorlNet [65] to add\nadditional correspondence between the condition and out-\nput, with additional designs of temporal attention layer [58],\ntrainable LoRA [27] and token embedding for better video\ncustomization finetuning.\nIn detail, as shown in Fig. 2, the low-rank matrices [27]\nare injected into pre-trained linear layers within the cross-\nattention modules of the denoising UNet. LoRA employs\na low-rank learnable factorization technique to update the\nattention weight matrix Wq, Wk, Wv:\nWi = Wi0 + \u2206Wi0 = Wi0 + BiAi,\n(4)\nwhere i = q, k, v denotes the different part in cross-\nattention. Wi0 \u2208 Rd\u00d7k represents the original weights of\nthe pre-trained T2I model [49], B \u2208 Rd\u00d7r and A \u2208 Rr\u00d7k\nrepresent the low-rank factors, where r is much smaller than\noriginal dimensions d and k. Remarkably, this operation\ndoes not hurt the ability of the pre-trained T2I model [49] in\nconcept generation and composition. In addition, we lever-\nage inflated ControlNet [65] inspired by Tune-A-Video [58]\nto extract temporal information among the condition se-\nquences. The original condition of the object is encoded\nby structure-guided module, where we convert the self-\nattention to spatial-temporal self-attention.\n3.3. Control Handle Transformation\nSince naive object editing in a video may deteriorate the\nsurrounding background, we opt to edit the first condition\nto ensure a more consistent editing performance.\nThen\nthey are incorporated into UNet after being encoded by\nControlNet-based [65] structure guided module. As shown\nin Fig. 2, our control handle editing involves three steps.\nFused attention\nForeground\nBackground\nSelf attn\nCross attn\nSelf attn\nInversion\nDenoising\nEdited condition\n1-Mt\n\ud835\udc40\ud835\udc61\nStructure\nGuidance\nInverted \nLatents\nInverted \nLatents\nInverted \nLatents\nInverted \nLatents\nFigure 3. Attention ReMix module. Take the moving task as an\nexample, we utilize edited conditions in both inversion and denois-\ning. In inversion, the edited condition Cedit is employed to edit\nself-attention and cross attention from input video and get binary\nmask Mt. In denoising, Cedit is leveraged to guide the appearance\ngeneration in the target area.\n1) Extract. We first segment the interested objects using\nSegment-and-Track-Anything [8]. Then the annotator (e.g.,\npose detector, hed-detector, depth estimator) is utilized to\nextract conditions in the clean segmented object sequences.\n2) Transformation. After getting the intermediate repre-\nsentations of the specific frame, the user can perform single\nor multiple geometric editing operations on objects in the\nfirst condition frame, such as resizing and moving.\n3) Propagation. We compute the transformation param-\neters in the first edited frame and then apply these trans-\nformations across all frames, ensuring consistent guidance\nthroughout the video. This propagation can also be adjusted\nframe-by-frame by the users.\n3.4. Controllable Video Editing\nWith guidance from the user and the appearance customiza-\ntion, we can finally edit the video in our framework. In\ndetail, we conduct the video inversion to obtain the inter-\nmediate features and then inject saved features during the\ndenoising stage as shown in Fig. 2. Different from previ-\nous unconditional video inversion and editing pipeline [45],\nour method guides both processes with the control signals\nand then performs editing via the proposed attention remix\nmodule.\nControllable Inversion and Generation.\nMost current\nworks [5, 28, 38] perform the inversion and editing pipeline\nto achieve appearance editing.\nFor our task, we aim to\nachieve more complex property editing by incorporating a\ncontrol handle during the inversion and generation. To be\nmore specific, the same edited conditions in two stages are\nemployed to play distinct roles. During inversion, we inject\nthe representation of the edited condition into the UNet as\nwell as modify the self-attention and cross-attention from\nthe source video. As shown in Fig. 3, we observe that both\nthe original and target areas are activated by edited words\nafter injection. During generation, the edited condition is\nreintroduced into the network to serve as guidance for re-\ngenerating specific highlighted areas. In both stages, the\ninjection of the edited condition is indispensable to accom-\nplish our editing tasks through their mutual coordination.\nAttention Remix Module.\nWith only the guidance of\nstructure can not perform the editing well since the origi-\nnal objects will also influence the results. To achieve our\nultimate goal, we propose the Attention ReMix module to\nmodify the attention during the generation process.\nAs\nshown in Fig. 3, we store the intermediate self-attention\nmaps {ssrc\nt }T\nt=1 and cross-attention maps {csrc\nt }T\nt=1 at each\ntimestep t and the last noising latent maps zT as:\nzT , {csrc\nt }T\nt=1, {ssrc\nt }T\nt=1 = Inv\n\u0000z0, Cedit\u0001\n,\n(5)\nwhere Inv donates for the DDIM inversion pipeline and\nCedit represents edited condition.\nAs shown in Fig. 3,\nduring the denoising stage, the activation areas of cross-\nattention by edited words provide significant assistance for\nmask generation. The binary mask Mt is obtained from\nthresholding the cross-attention map {csrc\nt }T\nt=1. Then we\nblend the {ssrc\nt }T\nt=1 with Mt and use the structure-guided\nmodule to generate the target object. Formally, this process\nis implemented as:\nMt = GetMask (csrc\nt , \u03c4) ,\nsfinal\nt\n= G\n\u0000Mt \u2299 ssrc\nt , Cedit\u0001\n+ (1 \u2212 Mt) \u2299 ssrc\nt .\n(6)\nwhere \u03c4 stands for the threshold, GetMask (\u00b7) and G (\u00b7)\nrepresent the operation to get mask and structure guidance.\n4. Experiments\nWe implement our method based on the pretrained model of\nStable Diffusion [49] and ControlNet [65] at 100 iterations.\nWe sample 8 uniform frames at the resolution of 512 \u00d7 512\nfrom the input video and finetune the models for about 5\nminutes on one NVIDIA RTX 3090Ti GPU. The learning\nrate is 3 \u00d7 10\u22125. During editing, the attentions are fused\nin the DDIM step at t \u2208 [0.5 \u00d7 T, T] with total timestep\nT = 50. We use DDIM sampler [52] with classifier-free\nguidance [23] in our experiments. The source prompt of\nthe video is generated via the image caption model [32] and\nthen replaced or modified some words manually.\n4.1. Applications\nObject size editing. Using the pretrained text-to-image dif-\nfusion model [49], our method supports object size editing\nthrough manual modification of the specific sketch in the\nfirst frame. As shown in the 1st and 2nd rows of Fig. 4,\nour method achieves consistent editing of the foreground\n\u201cswan\u201d by scaling down its sketch, while preserving the\noriginal background content.\nObject position editing. One of the applications of our\nmethod is to edit the position of objects by adjusting the ob-\nject\u2019s position in the first frame of the control signal. This\ntask is challenging because the model needs to generate the\nobject at the target area while inpainting the original area\nsimultaneously. In the 3rd and 4th rows of Fig. 4, thanks\nto the proposed Attention ReMix module, our approach en-\nables moving the \u201cparrot\u201d from the right of the branch to\nleft by shifting the position of the parrot sketch in 1st con-\ndition frame. The background and the object successfully\nremain consistent in different video frames.\nHuman motion editing.\nOur approach is also capable\nof editing human motion just replacing the source skele-\nton condition to the target sequences extracted from certain\nvideos. For instance, we can modify man\u2019s motion from\n\u201cjumping\u201d to \u201craising hand\u201d in the 5th and 6th rows\nin Fig. 4. The result shows that we can generate new content\nusing target pose sequences while maintaining the human\nappearance in the source video.\nObject appearance editing. Thanks to the broad knowl-\nedge of per-trained T2I models [49], we modify the ob-\nject appearance through the editing of text prompts which is\nsimilar to current mainstream video editing methods. Dif-\nferently, we can further fulfill the property and appearance\nediting simultaneously. The 7th and 8th rows in\nFig. 4\nshowcase this powerful capability. We replace \u201cbear\u201d with\n\u201clion\u201d simply by modifying the corresponding words and\nenlarging its sketch.\n4.2. Comparisons\nQualitative results. Our method enables multiple possibil-\nities for editing. Here we give two types since they can be\ndirectly compared with other methods. The first is Shape-\naware editing. As shown in Fig 5, the most naive method\nis to segment the objects and direct inpainting [64] and\npaste. However, we find this naive method struggles with\nthe video harmonization and temporal artifacts caused by\ninpainting and segmentation as shown in Fig. 5. Another\nrelevant method is [31]. However, its appearance is still\nlimited by the optimization, causing blur and unnatural re-\nsults. We also compare the method by naively using Tune-\nA-Video [58] with T2I-Adapter [40]. It accomplishes local\nediting after overfitting but still exhibits temporal inconsis-\ntency. Differently, the proposed methods manage to resize\nthe \u201cswan\u201d and edit its appearance to \u201cduck\u201d simultane-\nously, while maintaining the temporal coherence. On the\nother hand, we compare our method on conditional video\ngeneration from handcrafted motion conditions and a sin-\ngle appearance. As shown in Fig. 5, VideoComposer [56]\nstruggles to generate temporally consistent results. In con-\ntrast, our method showcases a natural appearance and better\npolar bear \u2192 lion\nA polar bear standing on a frozen lake in the snow\nA <new1> parrot sitting on a tree branch\nA <new1>man standing on a blue mat, a lake in the background, full body pose\nA <new1> swan with a red beak swimming in a river near a wall and bushes.\nFigure 4. Controllable video editing via modifying the control signals (e.g., sketch map and skeleton). Our framework can achieve\nconsistent editing via the transformation of control signals while keeping its original appearance. From top to bottom is resizing (small),\nmoving (transformation), human pose editing, and shape-aware text-based editing. We can further introduce text-based editing as the\nbottom sample to change both appearance and shape. Zoom in for the best view.\nInput Frames\nOurs\nShape-aware Edit\nTune-A-Video\n&T2I-adapter\nFramewise Inpainting\n& Pasting\nVideoComposer\nOurs\nImage\nMotion\nFigure 5. Qualitative comparison. Left: Shape-aware video editing. we resize the swan and replace word \u201cswan\u201d with \u201cduck\u201d. Right:\nMoving object, the first frame and motion conditions are provided to reconstruct the video. Our results exhibit the best temporal consistency,\nvideo fidelity, and editing quality. Zoom in for the best view.\ntemporal consistency.\nQuantitative results. We also conduct the quantitative re-\nsults of the experiments to show the advantage of the pro-\nposed framework with the following metrics: 1) Tem-Con:\nFollowing the previous methods [14, 55, 59], we perform\nthe quantitative evaluation using the pretrained CLIP [46]\nmodel. Specifically, we evaluate the temporal consistency\nin frames by calculating the cosine similarity between all\npairs of consecutive frames. 2) Fram-Acc: We execute the\nappearance editing and report the matrix about frame-wise\nediting accuracy, which is the percentage of frames. In de-\ntail, the edited frame will exhibit a higher CLIP similarity\nto the target prompt than the source prompt. 3) Four user\nstudies metrics: Following FateZero [45], we assess our\napproach using four user studies metrics (\u201cEdit\u201d, \u201cImage\u201d,\n\u201cTemp\u201d and \u201cID\u201d). They are editing quality, overall frame-\nwise image fidelity, temporal consistency of the video, and\nobject appearance consistency, respectively. For a fair com-\nparison, we ask 20 subjects to rank different methods. Each\nstudy displays four videos in random order and requests the\nevaluators to identify the one with superior quality. From\nTab. 1, the proposed method achieves the best temporal con-\nsistency against baselines and shows a comparable object\nappearance consistency as framewise inpainting and past-\ning. As for the user studies, the average ranking of our\nmethod earns user preferences the best in four aspects.\n4.3. Ablation Studies\nVideo Customization via LoRA and token embedding\ntunning.\nIn the right column of Fig. 6, we present the\ncases without LoRA in cross-attention or token embedding\ntunning. The visualized result in 3rd column illustrates the\nsignificance of cross-attention LoRA. The preservation of\nMethod\nCLIP Metrics\u2191\nUser Study\u2193\nInversion & Editing\nTem-Con Fram-Acc Edit Image Temp\nID\nTune-A-Video [58] & T2I-adapter [40]\n0.891\n0.851\n2.87\n2.99\n2.67\n3.18\nShape-aware Edit [31]\n0.722\n0.618\n3.56\n3.69\n3.74\n3.53\nVideoComposer [56]\n0.914\n0.846\n3.38\n2.74\n2.94\n4.31\nSingle-frame Inpainting & Pasting\n0.920\n0.887\n3.92\n3.97\n4.08\n1.93\nOurs\n0.928\n0.919\n1.27\n1.62\n1.58\n2.04\nTable 1. Quantitative evaluation against baselines. In our user\nstudy, the results of our method are preferred over those from base-\nlines. For CLIP-Score, we achieve the best temporal consistency\nand comparable object appearance consistency against framewise\ninpainting and pasting.\nInput Video\nOurs\nW/o LoRA tunning in\ncross-attention\nW/o token embedding\ntunning\nFigure 6. Ablation on Video Customization. Without LoRA tun-\ning in cross-attention, the generated video can not preserve the\ncontent of the object. If we remove token embedding tunning,\nthe appearance of the object (e.g., texture and color) experiences a\ndegradation in content consistency.\nobject appearance is unattainable without it. We also ob-\nW/o Structure Guidance\nin inversion\nW/o Structure Guidance\nin denoising\nW/o Structure Guidance\nin inversion & denoising\nInput Frame\nOurs\nFigure 7. Ablation about Attention ReMix module. The attentions\n(left top) are the visualization of cross-attention during denoising.\nThe masks 1 \u2212 Mt (left bottom) are generated by the word acti-\nvation of cross-attention in inversion. If we remove the structure-\nguided module in inversion or denoising, our method fails to com-\nplete moving task.\nMethod\nCLIP Metrics\u2191\nUser Study\u2193\nInversion & Editing\nTem-Con Fram-Acc Edit Image Temp\nID\nW/o LoRA tunning in cross-attention\n0.891\n0.884\n3.27\n3.34\n2.91\n3.97\nW/o token embedding tunning\n0.908\n0.915\n2.65\n3.09\n2.63\n3.82\nW/o spatial self-attention\n0.752\n0.869\n4.34\n3.71\n4.09\n3.41\nW/o temporal layer\n0.884\n0.893\n3.52\n3.12\n3.92\n2.40\nOurs\n0.931\n0.922\n1.23\n1.74\n1.46\n1.41\nTable 2. Quantitative evaluation about ablation study. For CLIP-\nScore, if we remove the spatial self-attention, the temporal consis-\ntency declines a lot. In our user study, the generated videos without\nLoRA tunning in cross-attention exhibit challenges in preserving\nappearance.\nserve a decline in performance when token embedding is\nnot tuned, further indicating that this operation is crucial for\npreserving content. In contrast, our framework ensures the\nconsistency of texture and appearance by finetuning both\ncomponents.\nAttention Remix Module\nis studied in Fig. 7, where we\nremove the structure-guided module to ablate its role dur-\ning inversion and generation. The 3rd column shows that\nremoving the structure-guided module in inversion led to\nthe failure of moving editing. Since the lack of target area\nmask (left bottom of 3rd column), the self-attention in this\nregion entirely derives from that of the background. The\n4th column demonstrates the absence of guidance during\ngeneration. It also fails to achieve the task even with the\ntarget area mask (left bottom of 4th column). We analyze\nthat there is no guidance for the target area during the gen-\neration (left top of 4th column). Finally, we also visualize\nthe result without attention remix module both two stages\nin the 5th column. The framework severely degrades to be a\nself-attention reconstruction. We observe that it fails to ac-\ncomplish the moving task and maintain background consis-\ntency (red rectangles in the 5th column). In contrast, when\nwe equip the guidance both in inversion and generation, the\n\u201ccup\u201d can be shifted successfully, which further empha-\nsizes the significance of our module.\nInput video\nW/o ST-Attention\nOurs\nW/o Temporal Layer\nFigure 8. Ablation about temporal modules. Without spatial-\ntemporal self-attention (short as \u201cST-Attention\u201d), the generated re-\nsult shows an inconsistent background. If we eliminate the tempo-\nral layer, flickering occurs among frames.\nTemporal modules.\nWe ablate the temporal modules in\nour framework, including spatial-temporal self-attention\nand the temporal layer. In the 3rd column of Fig. 8, we no-\ntice that removing the spatial-temporal self-attention results\nin temporal artifacts among frames, emphasizing its impor-\ntance for temporal consistency. The 4th column illustrates\nthe impact when the temporal layer is removed, showing its\neffects on the temporal stability of the generated video.\nQuantitative Ablation.\nWe employ a similar setting with\nthe comparison with baseline in Sec. 4.2. In Tab. 2, we\nnote that the ID performance experiences a significant de-\ncline without token embedding tunning. When removing\nspatial self-attention, the temporal coherence of the gener-\nated video is significantly disrupted, further underscoring\nthe crucial role of modules.\n5. Conclusion\nIn this paper, we propose a new controllable video editing\nmethod MagicStick that performs temporal consistent\nvideo property editing such as shape, size, location, motion,\nor all that can also be edited in videos. To the best of our\nknowledge, our method is the first to demonstrate the capa-\nbility of video editing using a trained text-to-image model.\nTo achieve this, we make an attempt to study and utilize\nthe transformations on one control signal (e.g., edge maps\nof objects or human pose) using customized ControlNet. A\nnew Attention ReMix module is further proposed for more\ncomplex video property editing. Our framework leverages\npre-trained image diffusion models for video editing, which\nwe believe will contribute to a lot of new video applications.\nLimitation & Future work.\nWhile our method achieves\nimpressive results, it still has some limitations. During ge-\nometry editing, it struggles to modify the motion of the ob-\nject to follow new trajectories significantly different from\nthose in the source video. For instance, making a swan take\nflight or a person perform a full Thomas spin is quite chal-\nlenging for our framework. We will test our method on the\nmore powerful pretrained video diffusion model for better\nediting abilities.\nAcknowledgments.\nWe thank Jiaxi Feng, Yabo Zhang for\ntheir helpful comments. This project was supported by the\nNational Key R&D Program of China under grant number\n2022ZD0161501.\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18208\u201318218, 2022. 2, 3\n[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM Transactions on Graphics (TOG), 42\n(4):1\u201311, 2023. 2, 3\n[3] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In European conference on computer vi-\nsion, pages 707\u2013723. Springer, 2022. 2\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 3\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n4\n[6] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 22560\u201322570,\n2023. 3\n[7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-\nvideo: Text-driven consistency-aware diffusion video edit-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 23040\u201323050, 2023. 2\n[8] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li,\nZongxin Yang, Wenguan Wang, and Yi Yang. Segment and\ntrack anything. arXiv preprint arXiv:2305.06558, 2023. 4\n[9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord.\nDiffedit:\nDiffusion-based seman-\ntic image editing with mask guidance.\narXiv preprint\narXiv:2210.11427, 2022. 3\n[10] Xiaodong Cun and Chi-Man Pun. Improving the harmony\nof the composite image by spatial-separated attention mod-\nule.\nIEEE Transactions on Image Processing, 29:4759\u2013\n4771, 2020. 2\n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 4\n[12] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information\nProcessing Systems, 35:16890\u201316902, 2022. 2\n[13] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. arXiv preprint arXiv:2306.00986,\n2023. 2\n[14] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346\u20137356, 2023. 2, 3, 7\n[15] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-\nguided domain adaptation of image generators. ACM Trans-\nactions on Graphics (TOG), 41(4):1\u201313, 2022. 2\n[16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 2\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 2, 3\n[18] Chunming He, Kai Li, Yachao Zhang, Longxiang Tang, Yu-\nlun Zhang, Zhenhua Guo, and Xiu Li. Camouflaged object\ndetection with feature decomposition and edge reconstruc-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 22046\u201322055,\n2023. 2\n[19] Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxi-\nang Tang, Yulun Zhang, Zhenhua Guo, and Xiu Li. Weakly-\nsupervised concealed object segmentation with sam-based\npseudo labeling and multi-scale feature grouping.\narXiv\npreprint arXiv:2305.11003, 2023. 2\n[20] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 2, 3\n[21] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,\nYuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao\nWeng, Ying Shan, et al.\nAnimate-a-story:\nStorytelling\nwith retrieval-augmented video generation. arXiv preprint\narXiv:2307.06940, 2023. 2\n[22] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[23] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2, 5\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3, 4\n[25] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[26] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 2\n[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 4\n[28] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and\nQiang Xu. Direct inversion: Boosting diffusion-based edit-\ning with 3 lines of code. arXiv preprint arXiv:2304.04269,\n2023. 4\n[29] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 3\n[30] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[31] Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Eliza-\nbeth Qiu, and Jia-Bin Huang. Shape-aware text-driven lay-\nered video editing. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n14317\u201314326, 2023. 2, 5, 7\n[32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888\u2013\n12900. PMLR, 2022. 5\n[33] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 2\n[34] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jinren Zhou, and Tie-\nniu Tan.\nDecomposed diffusion models for high-quality\nvideo generation. arXiv preprint arXiv:2303.08320, 2023.\n3\n[35] Yue Ma, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu\nLi, and Yu Qiao.\nVisual knowledge graph for human ac-\ntion reasoning in videos. In Proceedings of the 30th ACM\nInternational Conference on Multimedia, pages 4132\u20134141,\n2022.\n[36] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen.\nFollow your pose:\nPose-guided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023. 2, 3\n[37] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 3\n[38] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 3, 4\n[39] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 2\n[40] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 5,\n7\n[41] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[42] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2337\u20132346,\n2019. 2\n[43] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, pages 1\u201311, 2023. 3\n[44] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2085\u20132094,\n2021. 2\n[45] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 2, 4, 7\n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 7\n[47] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n2\n[48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 4, 5\n[50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 3\n[51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[52] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2, 3, 4, 5\n[53] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 3\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3\n[55] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye,\nYu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long\nvideo generation via temporal co-denoising. arXiv preprint\narXiv:2305.18264, 2023. 7\n[56] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2, 5, 7\n[57] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 3\n[58] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 2, 3, 4, 5, 7\n[59] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong\nZhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong\nCun, Xintao Wang, et al.\nMake-your-video: Customized\nvideo generation using textual and structural guidance. arXiv\npreprint arXiv:2306.00943, 2023. 7\n[60] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. arXiv preprint arXiv:2104.10157, 2021. 2\n[61] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. In ACM SIGGRAPH Asia Conference Proceed-\nings, 2023. 2\n[62] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu.\nVector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 2\n[63] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022. 2\n[64] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin\nJin, Wenjun Zeng, and Zhibo Chen.\nInpaint anything:\nSegment anything meets image inpainting. arXiv preprint\narXiv:2304.06790, 2023. 5\n[65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2, 3, 4, 5\n[66] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 3\n[67] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 3\n"
  },
  {
    "title": "Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia",
    "link": "https://arxiv.org/pdf/2312.03664.pdf",
    "upvote": "7",
    "text": "December 2023\nGenerative agent-based modeling with actions\ngrounded in physical, social, or digital space\nusing Concordia\nAlexander Sasha Vezhnevets1, John P. Agapiou1, Avia Aharon2, Ron Ziv2,4,\u2020, Jayd Matyas1,\nEdgar A. Du\u00e9\u00f1ez-Guzm\u00e1n1, William A. Cunningham3, Simon Osindero1, Danny Karmon2 and Joel Z. Leibo1\n1Google DeepMind, 2Google Research, 3University of Toronto, 4Technion - Israel Institute of Technology\nAgent-based modeling has been around for decades, and applied widely across the social and natural\nsciences. The scope of this research method is now poised to grow dramatically as it absorbs the new\naffordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are\nnot just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs\nare constructed using an LLM to apply common sense to situations, act \u201creasonably\u201d, recall common\nsemantic knowledge, produce API calls to control digital technologies like apps, and communicate\nboth within the simulation and to researchers viewing it from the outside. Here we present Concordia,\na library to facilitate constructing and working with GABMs. Concordia makes it easy to construct\nlanguage-mediated simulations of physically- or digitally-grounded environments. Concordia agents\nproduce their behavior using a flexible component system which mediates between two fundamental\noperations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM),\nwhich was inspired by tabletop role-playing games, is responsible for simulating the environment where\nthe agents interact. Agents take actions by describing what they want to do in natural language. The\nGM then translates their actions into appropriate implementations. In a simulated physical world, the\nGM checks the physical plausibility of agent actions and describes their effects. In digital environments\nsimulating technologies such as apps and services, the GM may handle API calls to integrate with external\ntools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search,\netc.). Concordia was designed to support a wide array of applications both in scientific research and for\nevaluating performance of real digital services by simulating users and/or generating synthetic data.\nKeywords: foundation models, large language models, generative agents, agent-based modeling\nCorresponding author(s): Sasha Vezhnevets: vezhnick@google.com\n\u2020 Work done during an internship at Google Research\n\u00a9 2023 Google DeepMind. All rights reserved\narXiv:2312.03664v2  [cs.AI]  13 Dec 2023\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nContents\n1\nIntroduction\n3\n2\nConcordia\n5\n2.1\nGenerative agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nGenerative environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3\nExperiment design using Concordia . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3\nInterpretations\n12\n3.1\nNeuroscience interpretation of the generative agent architecture . . . . . . . . . . . .\n12\n3.2\nA theory of social construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.3\nConcordia agents do not make decisions by optimizing . . . . . . . . . . . . . . . . .\n14\n3.3.1\nConcordia agents are not reinforcement learners\n. . . . . . . . . . . . . . . .\n15\n3.3.2\nConcordia agents are not rational utility maximizers\n. . . . . . . . . . . . . .\n15\n4\nApplications\n16\n4.1\nSynthetic user studies in digital action space . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.1.1\nPhoneGameMaster and PhoneUniverse . . . . . . . . . . . . . . . . . . . . . .\n17\n4.1.2\nDigital function representations . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4.2\nData generation and service evaluation . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.3\nSequential social dilemmas experiments in silico . . . . . . . . . . . . . . . . . . . . .\n19\n4.4\nConcordia can implement classic and contemporary psychological models . . . . . . .\n19\n4.5\nAI assistants with transparent auditing and credit assignment\n. . . . . . . . . . . . .\n21\n4.6\nEmergence and multi-scale modeling with Concordia . . . . . . . . . . . . . . . . . .\n21\n5\nFuture work\n22\n6\nConclusion\n22\nA\nImplementation details\n23\nA.1 Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nA.2 Game master implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nA.3 GM components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nA.3.1\nTurn taking and simultanious action\n. . . . . . . . . . . . . . . . . . . . . . .\n24\nA.4\nNested games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nA.5 Concurrency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.6\nSampling initial memories and backstories . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.7\nDigital Activity Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.7.1\nCreating Phone Apps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.7.2\nPhone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nA.7.3\nTriggering the nested PhoneGameMaster . . . . . . . . . . . . . . . . . . . . .\n25\nA.8\nExamples\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n2\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\n1. Introduction\nAgent-based social simulation is used through-\nout the social and natural sciences (e.g. Poteete\net al. (2010)). Historically, Agent-Based Model-\ning (ABM) methods have mostly been applied at\na relatively abstract level of analysis, and this\nhas limited their usefulness. For instance, in-\nsights from behavioral economics and related\nfields which study how people actually make deci-\nsions are rarely combined with ideas from institu-\ntional and resource economics in the same model\ndespite the fact that integrating these two bodies\nof knowledge is thought to be critical for building\nup the full picture of how social-ecological sys-\ntems function, and how interventions may help\nor hinder their governance (Schill et al., 2019).\nNow, using generative AI1, it is possible to con-\nstruct a new generation of ABMs where the agents\nnot only have a richer set of cognitive operations\navailable for adaptive decision making but also\ncommunicate with one another in natural lan-\nguage.\nHere we propose Generative Agent-Based Mod-\nels (GABM)s, which are much more flexible and\nexpressive than ABMs, and as a result can incor-\nporate far more of the complexity of real social\nsituations. Applying generative models within\nagents gives them common sense (imperfectly\nbut still impressively) (Zhao et al., 2023), rea-\nsoning (Huang et al., 2022; Wei et al., 2022),\nplanning (Song et al., 2023), few-shot learn-\ning (Brown et al., 2020; Bubeck et al., 2023),\nand common ground with one another e.g in un-\nderstanding the meanings of words. Generative\nagents may be able to reason appropriately from\npremises to conclusions much of the time, and\nare typically able to predict the actions of oth-\ners (Ag\u00fcera y Arcas and Norvig, 2023; Bubeck\net al., 2023). They also possess substantial cul-\ntural knowledge and can be prompted to \u201crole\nplay\u201d as simulated members of specific human\nsubpopulations (Argyle et al., 2023; Safdari et al.,\n2023; Shanahan et al., 2023).\nConcordia is a library to facilitate construction\nand use of GABMs to simulate interactions of\n1such as Anil et al. (2023); OpenAI (2023); Touvron et al.\n(2023); Workshop et al. (2022).\nagents in grounded physical, social, or digital\nspace. It makes it easy and flexible to define envi-\nronments using an interaction pattern borrowed\nfrom tabletop role-playing games in which a spe-\ncial agent called the Game Master (GM) is re-\nsponsible for simulating the environment where\nplayer agents interact (like a narrator in an inter-\nactive story). Agents take actions by describing\nwhat they want to do in natural language. The\nGM then translates their actions into appropriate\nimplementations. In a simulated physical world\nthe GM checks the physical plausibility of agent\nactions and describes their effects. In general,\nthe GM can use any existing modeling technique\nto simulate the non-linguistic parts of the simu-\nlation (e.g. physical, chemical, digital, financial,\netc). In digital environments involving software\ntechnologies, the GM may even connect with real\napps and services by formatting the necessary API\ncalls to integrate with external tools (as in Schick\net al. (2023)). In the examples provided with the\nlibrary we demonstrate how Concordia can be\nused to simulate a small town election, a small\nbusiness, a dispute over a damaged property, a so-\ncial psychology experiment, and a social planning\nscenario mediated through a digital app (see A.8\nfor details).\nValidation.\nFor a GABM to be useful we need\nsome reason to trust that the results obtained\nwith it may generalize to real human social life.\nMany aspects of model validation concern both\nGABMs and other kinds of ABMs (see Windrum\net al. (2007)), while GABMs also raise new is-\nsues. While still surely a debatable point, we do\nthink there will be some yet to be identified set\nof conditions under which we may gain a reason-\nable level of confidence that a model\u2019s predictions\nwill generalize. Therefore we think identifying\nthem should be highest priority right now for\nthis nascent field (see also Dillion et al. (2023);\nGrossmann et al. (2023)).\nThere are no panaceas in model validation.\nGABMs constructed for different purposes call for\nvalidation by different forms of evidence. For ex-\nample, many GABMs employ experiment designs\nfeaturing an intervention, which may involve ei-\nther intervening on internal variables affecting\n3\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nthe cognition of an individual, e.g. \u201chow does ru-\nmination work?\u201d, or on external factors affecting\nthe environment in which individuals interact,\ne.g. how are property rights implemented? De-\npendent outcome variables may be on the indi-\nvidual level, e.g. questionnaire responses, or on\nthe societal level e.g. equality, sustainability, etc.\nWhen a GABM shows through such an experiment\nthat A causes B (in the model) we may regard it\nas a prediction that A causes B in the real world\ntoo. Sometimes this prediction is meant at a rela-\ntively detailed quantitative level (e.g. if the GABM\nwas built in a way that incorporates substantial\nempirical data), while other times (more often)\nit would be intended as a statement either about\na mechanism which may exist in real life or a pre-\ndiction concerning the likely effect of something\nwe may do in real life (such as to make a public\npolicy change or deploy a technology). A GABM\nis said to generalize when inferences made on\nthe basis of the model transfer to real life.\nIn evidence-based medicine and evidence-\nbased policy making researchers are trained to\nconsider an explicit hierarchy of evidence when\nevaluating the effect of interventions (Higgins\net al., 2008). We may envision it like a ladder with\nhighest rungs corresponding to the best evidence\nand lowest rungs corresponding to poor evidence.\nEvidence of effectiveness in real life (ecological\nvalidity) is at the top, rigorous experiments in\ncontrolled settings like labs or clinics below that,\nobservational data lower down, and consistency\nwith prior theory lower still. For validation, it\nalso matters what the model will be used for. If it\nwill only be used to guide decisions about where\none may most fruitfully focus time, effort, and re-\nsources in further research (e.g., in piloting) then\nthe evidence bar should be correspondingly lower\nthan if the model is to be used to guide real world\ndecisions with real consequences. Importantly,\nit is not really correct to speak of evidence for\nor against a theory. Theories can only really be\njudged by their \u201cproductivity\u201d, i.e. the extent to\nwhich they motivate new work building on them\nfurther, especially new empirical research in real\nlife (Lakatos, 1970). We discuss the hierarchy of\nevidence further in Section 2.3.\nDigital media.\nIn order to build models of con-\ntemporary social phenomena it is important to\nconsider the substantial role the digital medium\nplays in modern communication and other activi-\nties, as well as how it shapes human interactions\nand decisions (Risse, 2023). Therefore, Concor-\ndia makes it possible to represent digital compo-\nnents such as apps, social networks, and general\nAI assistants within the simulation environment.\nThis is critical since the medium through which in-\nformation is transmitted is not passive but actively\nshapes the nature and impact of the message.\nEach medium has its own unique qualities, and\nthose qualities have a transformative impact on so-\nciety, culture, and individuals (McLuhan, 2017).\nFor instance, the recommender algorithms used\nin social media have a substantial effect on human\nculture and society and the fact that LLM-based\nsystems have analogous properties, affecting both\nhow information is transmitted and how it is val-\nued, implies they are likely to influence human\nculture and society more and more as time goes\non (Brinkmann et al., 2023). By integrating digi-\ntal elements into simulations, we aim to facilitate\nresearch that seeks to capture these qualities and\nthe way they shape culture and society.\nMoreover, the digital representation can have\nvarious degrees of abstraction from natural lan-\nguage prompting, via mock-up implementation\nto integration with real external services (e.g. by\ncalling real APIs with generated text as in Schick\net al. (2023)). The latter has great importance in\nenabling sandbox evaluation of real services with\nsocial agents, generating realistic data, as well as\nin evaluating real services.\nThese simulation techniques can also address\nthe challenges of evaluating digital apps and gen-\neral AI assistants (e.g., Bard, ChatGPT) in user-\ncentric and intricate scenarios that demand the\nfulfillment of multiple constraints. Take, for in-\nstance, personal AI assistants that are designed\nto adapt to user preferences and respond to their\nrequests. In such situations, the objective is in-\ntricate, rooted in satisfying a range of implicit\nand explicit constraints. It would be difficult to\noptimize without large amounts of natural data.\nAgent-based simulation can be used to generate\nsynthetic data trails of agent activities to use in\n4\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nAgent\nDorothy\nGM\nAction attempt\nDorothy goes grocery \nshopping for dinner.\nEvent statement\nDorothy goes grocery \nshopping for dinner and \noverhears Charlie talking \nabout Alice.\nAgent\nCharlie\nAction attempt\nCharlie sets up a stand to \ntalk to people about Alice's \nbad deeds.\nEvent statement\nCharlie is kicked out of the \ngrocery store for disturbing \nother customers when he \nsets up a stand to talk about \nAlice's bad deeds.\n\u201cFacts of the world\u201d\nLog of what happened\nUpdate to grounded variables\nConsequences and elaborations\nObservations \nGame master:\n\u25cf\nReceives action attempt from \nagents\n\u25cf\nDetermines what events happens \nas the result\n\u25cf\nDetermines the consequences & \nelaborates details\n\u25cf\nInterfaces with grounding \u2018hard \nmechanics\u2019\n\u25cf\nSends out observations\nFigure 1 | The high level structure of the simulation in Concordia. Generative agents consume\nobservations and produce actions. The Game Master (GM) consumes agent actions and produces\nobservations.\nthe absence of (and also in conjunction with) real\ndata sources. This synthetic data may be use-\nful both for training and evaluating models, as\nwell as for simulating and analyzing the perfor-\nmance of scenario-specific interactions between\nan agent and an actual service. These proposed\napplications offer a viable alternative to tradi-\ntional, human-centric methods, which are often\nexpensive, not scalable, and less capable of han-\ndling such complex tasks.\nFoundation models are poised to be transfor-\nmative for agent-based social simulation method-\nology in the social and natural sciences. However,\nas with any large affordance change, research\nbest-practices are currently in flux. There is no\nconsensus at present concerning how to interpret\nresults of LLM-based simulations of human popu-\nlations. The critical epistemic question is \u201cby what\nstandard should we judge whether (and in what\nways, and under which conditions) the results\nof in silico experiments are likely to generalize\nto the real world?\u201d. These are not questions any\none group of researchers can answer by them-\nselves; rather these issues must be negotiated by\nthe community as a whole.\nConcordia is an open invitation to the scientific\ncommunity to participate in the creation of epis-\ntemic norms and best practices of GABM. We are\nreleasing the library together with a few illustra-\ntive examples and intend to update it with new\nfeatures and experiments. We will be reviewing\nand accepting contributions on regular basis.\nConcordia requires access to a standard LLM\nAPI, and optionally may also integrate with real\napplications and services.\nThe rest of the paper is organised as follows.\nThe following section 2 gives an overview of the\nConcordia library and how to design experiments\nin it. Section 3 presents several ways the Concor-\ndia agents and experiments can be interpreted.\nWe discuss applications in section 4. Appendix A\ncontains implementation details.\nConcordia is available on GitHub2.\n2. Concordia\nLike other agent-based modeling approaches, a\ngenerative model of social interactions (i.e. a\nGABM) consists of two parts: the model of the en-\nvironment and the model of individual behavior.\nIn this case both are generative. Thus we have:\n(a) a set of generative agents and (b) a generative\nmodel for the setting and context of the social\n2here:\nhttps://github.com/google-deepmind/\nconcordia\n5\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nAlice's Identity:\nName: Alice\nAlice is a passionate and politically engaged individual who is dedicated to advancing her cause \nthrough democratic means. She is willing to put in the hard work of canvassing and campaigning in \norder to get her message out to the people.\nAlice is discussing campaign strategy.\nAlice is excited\nAlice's plan:\nThe goal: Win the election and become the mayor of Riverbend\n9:00 - 13:00 Meet friends at The Sundrop Saloon to discuss campaign strategy and last-minute \ncampaigning.\n13:00 - 15:00 Have lunch and relax before the polls open.\n14:00 - 16:00 Get out in the field with a megaphone to try to reach more voters.\n16:00 - 18:00 Call volunteers to make sure they go out to vote.\n18:00 - 20:00 Call voters who don't usually vote to try to get them to the polls.\n20:00 - 21:00 Get some last minute votes from people going to vote after work.\nCurrent observations: Alice, Dorothy and Ellen met at The Sundrop Saloon to discuss campaign \nstrategy for the upcoming election.\nCurrent time interval: 01 Oct 2024 [11:00 - 12:00]\nQuestion: What would Alice do for the next 1 hour to best achieve their goal? Consider their plan, \nbut deviate from it if necessary.\nLLM API CALL(...) -> Alice discusses campaign strategy with volunteers.\nIdentity \nPlan \nObservation\nCall for action \nComponents \nFigure 2 | The above example illustrates the working memory z of an agent with 3 components\n(identity, plan, observation-and-clock). The identity component itself has several sub-components\n(core characteristics, daily occupation, feeling about progress in life). Together they condition the\nLLM call to elicit the behavioral response (i.e. produced in response to the final question asking what\nAlice will do next.).\ninteraction i.e. the environment, space, or world\nwhere the interaction takes place. We call the\nmodel responsible for the environment the Game\nMaster (GM). Both this name and the approach\nit reflects were inspired by table-top role-playing\ngames like Dungeons and Dragons where a player\ncalled the Game Master takes the role of the sto-\nryteller (Gygax and Cook, 1989). In these games,\nplayers interact with one another and with non-\nplayer characters in a world invented and main-\ntained by the GM.\nConcordia agents consume observations and\nproduce actions. The GM consumes agent actions\nand creates event statements, which define what\nhas happened in the simulation as a result of the\nagent\u2019s attempted action. Figure 1 illustrates this\nsetup. The GM also creates and sends observa-\ntions to agents. Observations, actions and event\nstatements are all strings in English. The GM is\nalso responsible for maintaining and updating\ngrounded variables, advancing the clock and run-\nning the episode loop.\nConcordia agents generate their behavior by\ndescribing what they intend to do in natural\nlanguage\u2014e.g. \u201cAlex makes breakfast\u201d. The game\nmaster takes their intended actions, decides on\nthe outcome of their attempt, and generates event\nstatements. The GM is responsible for:\n1. Maintaining a consistent and grounded state\nof the world where agents interact with each\nother.\n2. Communicating the observable state of the\nworld to the agents.\n3. Deciding the effect of agents\u2019 actions on the\nworld and each other.\n4. Resolving what happens when actions sub-\nmitted by multiple agents conflict with one\nanother.\nThe most important responsibility of the GM\nis to provide the grounding for particular exper-\nimental variables, which are defined on a per-\nexperiment basis. The GM determines the effect\nof the agents\u2019 actions on these variables, records\nthem, and checks that they are valid. Whenever\nan agent tries to perform an action that violates\nthe grounding, it communicates to them that their\naction was invalid. For example, in an economic\nsimulation the amount of money in an agent\u2019s\npossession may be a grounded variable. The GM\nwould track whether agents gained or lost money\n6\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\non each step and perhaps prevent them from pay-\ning more than they have available.\nOne may configure the specific set of grounded\nvariables to use on a per-experiment basis. This\nflexible functionality is critical because different\nresearch applications require different variables.\nYou can take a look at an example output\nof one of our experiments (see the Concordia\nGitHub repo), which was simulating elections in\na small town, where some agents are running for\nmayor and one other is running a smear campaign\nagainst a candidate.\n2.1. Generative agents\nSimulated agent behavior should be coherent\nwith common sense, guided by social norms, and\nindividually contextualized according to a per-\nsonal history of past events as well as ongoing\nperception of the current situation.\nMarch and Olsen (2011) posit that humans\ngenerally act as though they choose their actions\nby answering three key questions:\n1. What kind of situation is this?\n2. What kind of person am I?\n3. What does a person such as I do in a situation\nsuch as this?\nOur hypothesis is that since modern LLMs have\nbeen trained on massive amounts of human cul-\nture they are thus capable of giving satisfactory\n(i.e. reasonably realistic) answers to these ques-\ntions when provided with the historical context of\na particular agent. The idea is that, if the outputs\nof LLMs conditioned to simulate specific human\nsub-populations reflect the beliefs and attitudes\nof those subpopulations as argued in work such\nas Argyle et al. (2023) then this approach to im-\nplementing generative agents should yield agents\nthat can reasonably be said to model humans with\nsome level of fidelity. Safdari et al. (2023) have\nalso found out that personality measurements in\nthe outputs of some LLMs under specific prompt-\ning configurations are reliable and valid, therefore\ngenerative agents could be used to model humans\nwith diverse psychological profiles. In some cases\nanswering the key questions might require com-\nmon sense reasoning and / or planning, which\nLLMs do show capacity for (Huang et al., 2022;\nSong et al., 2023; Wei et al., 2022; Zhao et al.,\n2023), and show similar biases in behavioral eco-\nnomics experiments as humans (Aher et al., 2023;\nBrand et al., 2023; Horton, 2023). The ability of\nLLMs to learn \u2018in-context\u2019 and zero-shot Brown\net al. (2020); Bubeck et al. (2023); Dong et al.\n(2022); OpenAI (2023) reinforces the hypothe-\nsis further\u2014the agent might be able to ascertain\nwhat is expected of them in the current situation\nfrom a demonstration or an example.\nFor an LLM to be able to answer the key ques-\ntions, it must be provided with a record of an\nagent\u2019s historical experience. However, simply\nlisting every event that happened in an agent\u2019s\nlife would overwhelm the LLM (it would not fit\nin the context window). Therefore we follow the\napproach of Park et al. (2023) and use an as-\nsociative memory to keep the record of agents\nexperience. Concordia makes it easy to design\ngenerative agents in a modular fashion. Our ap-\nproach was inspired by Park et al. (2023), but\ndesigned to be more flexible and modular.\nConcordia agents dynamically construct the\ntext that conditions the LLM call they use to se-\nlect their course of action on each timestep. The\ncontext-generation process is factorized into a set\nof components. Components serve as intermedi-\naries between long-term memories of experience\nand the relatively compact conditioning text used\nto generate action. Intuitively, the set of compo-\nnents used in an agent comprise its \u201csociety of\nmind\u201d (Minsky, 1988), where each component\nfocuses on a certain aspect of the agent or its cir-\ncumstances which are relevant to generating its\ncurrent choice of action. For example, if we are\nbuilding agents for economic simulation, we will\nadd components that describe the agents posses-\nsions and financial circumstances. If we want to\nmodel the agent\u2019s physiological state, we add com-\nponents that describe the agent\u2019s level of thirst\nand hunger, health and stress levels. Together the\ncomponents produce the context of action\u2014text\nwhich conditions the query to the LLM, asking\n\u201cwhat should this agent do next?\u201d.\nA Concordia agent has both a long-term mem-\nory and a working memory. Let the long-term\n7\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nP(\u22c5|f a(zt ))\nat\nGM\nmt+1=mt+ot\nP(\u22c5|f z(mt ))\nzt\nzt+1\nAlice's Identity:\nName: Alice\nAlice is a passionate and politically engaged individual who is dedicated to \nadvancing her cause through democratic means. She is willing to put in the hard \nwork of canvassing and campaigning in order to get her message out to the \npeople.\nAlice is discussing campaign strategy.\nAlice is excited\nAlice's plan:\nThe goal: Win the election and become the mayor of Riverbend\n9:00 - 13:00 Meet friends at The Sundrop Saloon to discuss campaign strategy \nand last-minute campaigning.\n13:00 - 15:00 Have lunch and relax before the polls open.\n14:00 - 16:00 Get out in the field with a megaphone to try to reach more voters.\n16:00 - 18:00 Call volunteers to make sure they go out to vote.\n18:00 - 20:00 Call voters who don't usually vote to try to get them to the polls.\n20:00 - 21:00 Get some last minute votes from people going to vote after work.\nAlice's Identity:\nName: Alice\nAlice is a passionate and politically engaged individual who is dedicated to \nadvancing her cause through democratic means. She is willing to put in the hard \nwork of canvassing and campaigning in order to get her message out to the \npeople.\nAlice is discussing campaign strategy.\nAlice is downhearted\nAlice's plan:\nThe goal: Win the election and become the mayor of Riverbend\n9:00 - 13:00 Meet friends at The Sundrop Saloon to discuss campaign strategy \nand last-minute campaigning.\n13:00 - 15:00 Have lunch and relax before the polls open.\n14:00 - 16:00 Get out in the field with a megaphone to try to reach more voters.\n16:00 - 18:00 Call volunteers to make sure they go out to vote.\n18:00 - 20:00 Call voters who don't usually vote to try to get them to the polls.\n20:00 - 21:00 Get some last minute votes from people going to vote after work.\nAction\nSituation / self description\nAdd observation to memory\nWhat does a person such as I do in a \nsituation such as this?\nWhat kind of person am I?\nWhat kind of situation is this?\nFigure 3 | Illustration of generative agency sampling process defined by eq. 1 and eq. 2.\nmemory be a set of strings m that records every-\nthing remembered or currently experienced by\nthe agent. The working memory is z = {\ud835\udc67\ud835\udc56}\ud835\udc56 is\ncomposed of the states of individual components\n(Figure 2). A component \ud835\udc56 has a state \ud835\udc67\ud835\udc56, which\nis statement in natural language\u2014e.g. \u201cAlice is\nat work\u201d. The components update their states by\nquerying the memory (which contains the incom-\ning observations) and using LLM for summarising\nand reasoning. Components can also condition\ntheir update on the current state of other com-\nponents. For example, the planning component\ncan update its state if an incoming observation\ninvalidates the current plan, conditioned on the\nstate of the \u2018goal\u2019 component. Components can\nalso have internal logic programmed using classic\nprogramming, for example a hunger component\ncan check how many calories an agent consumed\nand how recently it consumed them, and update\nits state based on the result.\nWe use the same associative memory architec-\nture as in Park et al. (2023)3. We feed the in-\n3The idea of simulating a group of generative agents has\nbeen explored in a variety of ways in recent work. Our\nwork is focused on on agent-based modeling for science and\nfor evaluation of digital technologies. Another recent line\nof work has focused instead on the idea of using groups\nof generative agents to simulate organizations that solve\ncoming observations immediately into the agents\nmemory, to make them available when compo-\nnents update4.\nWhen creating a generative agent in Concordia,\nthe user creates the components that are relevant\nfor their simulations. They decide on the initial\nstate and the update function. The components\nare then supplied to the agents constructor.\nFormally, the agent is defined as a two step\nsampling process, using a LLM \ud835\udc5d (see Figure 3 for\nillustration). In the action step, the agent samples\nits activity \ud835\udc4e\ud835\udc61, given the state of components z\ud835\udc61 =\n{\ud835\udc67\ud835\udc56\n\ud835\udc61}\ud835\udc56:\n\ud835\udc4e\ud835\udc61 \u223c \ud835\udc5d(\u00b7| \ud835\udc53 \ud835\udc4e(z\ud835\udc61))\n(1)\nHere \ud835\udc53 \ud835\udc4e is a formatting function, which cre-\nates out of the states of components the context\nused to sample the action to take. The most sim-\nple form of \ud835\udc53 \ud835\udc4e is a concatenation operator over\nz\ud835\udc61 = {\ud835\udc67\ud835\udc56\n\ud835\udc61}\ud835\udc56. We do not explicitly condition on the\nmemory m or observation \ud835\udc5c, since we can sub-\nproblems like software companies and to thereby try to\nbuild a general-purpose problem solving system (Hong et al.,\n2023; Li et al., 2023b).\n4For convenience, we also allow the components to sub-\nscribe to the observation stream explicitly.\n8\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nsume them into components. First, we can im-\nmediately add o\ud835\udc61 to the memory m\ud835\udc61 = m\ud835\udc61\u22121 \u222a o\ud835\udc61.\nUnlike RL, we do not assume that the agent re-\nsponds with an action to every observation. The\nagent can get several observations before it acts,\ntherefore o\ud835\udc61 is a set of strings. Then we can set z0\nto be the component that incorporates the latest\nobservations and relevant memories into its state.\nThis allows us to exclusively use the vehicle of\ncomponents to define the agent.\nIn the second step the agent samples its state\nz, given the agents memory m\ud835\udc61 up to the present\ntime:\nz\ud835\udc56\n\ud835\udc61+1 \u223c \ud835\udc5d(\u00b7| \ud835\udc53 \ud835\udc56(z\ud835\udc61, m\ud835\udc61)).\n(2)\nHere, \ud835\udc53 \ud835\udc56 is a formatting function that turns the\nmemory stream and the current state of the com-\nponents into the query for the component update.\nWe explicitly condition on the memory stream m,\nsince a component may make specific queries into\nthe agent\u2019s memory to update its state. Here eq.2\nupdates components after every action, but gener-\nally, it is up to the agent to decide at what cadence\nto update each of its components. It is reasonable\nto update some components less frequently for\nefficiency or longer term consistency.\nNotice how eq.1 and eq.2 are not fundamen-\ntally different. What makes the difference be-\ntween an agent output and a component is that\nthe output of the former is interpreted by the GM\nas an action in the environment. In eq.1 we also\ndon\u2019t explicitly condition on the memory to point\nout the architectural decision, where components\nmediate between a long-term memory and the\nagents working memory. Otherwise, we can think\nof an agent as a special kind of component and\nof components as sub-agents.\n2.2. Generative environments\nRL research was fuelled by the availability of\ncomplex games, where the agents can be tested,\ntrained and evaluated (Bellemare et al., 2013;\nJaderberg et al., 2019; Vinyals et al., 2019). Here\nwe take an inspiration from table top role playing\ngames like Dungeons and Dragons (Gygax and\nCook, 1989). In these games players collabora-\ntively generate a story, while using rules, dice,\npen and paper to ground it\u2014for example, players\nhave to keep their health points above zero to\navoid death.\nThe GM is responsible for all aspects of the\nsimulated world not directly controlled by the\nagents. The GM mediates between the state of\nthe world and agents\u2019 actions. The state of the\nworld is contained in GM\u2019s memory and the val-\nues of grounded variables (e.g. money, posses-\nsions, votes, etc.). To achieve this the GM has to\nrepeatedly answer the following questions:\n1. What is the state of the world?\n2. Given the state of the world, what event is\nthe outcome of the players activity?\n3. What observation do players make of the\nevent?\n4. What effect does the event have on grounded\nvariables?\nThe GM is implemented in a similar fashion to\na generative agent. Like agents, the GM has an\nassociative memory similar to Park et al. (2023)\u2019s\nproposal. Like agents, the GM is implemented\nusing components. However, instead of contex-\ntualizing action selection, the components of the\nGM describe the state of the world\u2014for example\nlocation and status of players, state of grounded\nvariables (money, important items) and so on\u2013\n\u2014so that GM can decide the event that happens\nas the outcome of players\u2019 actions. The outcome is\ndescribed in the event statement (e.g. \u201cAlice went\nto the grocery store and met Bob in the cereal\naisle\u201d), which is then added to the GM associative\nmemory. After the event has been decided the\nGM elaborates on its consequences. For example,\nthe event could have changed the value of one of\nthe grounded variables or it could have had an\neffect on a non-acting player. Figure 1 illustrates\nthis process.\nThe GM generates an event statement \ud835\udc52\ud835\udc61 in\nresponse to each agent action:\n\ud835\udc52\ud835\udc61 \u223c \ud835\udc5d(\u00b7| \ud835\udc53 \ud835\udc52(z\ud835\udc61), \ud835\udc4e\ud835\udc61)\n(3)\nHere we explicitly condition on the action at-\ntempted by the agent, although it could be sub-\nsumed into the components (like observation in\n9\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\neq.1). This is to highlight that the GM generates\nan event statement \ud835\udc52\ud835\udc61 in response to every action\nof any agent, while the agent might take in several\nobservations before it acts (or none at all). After\nadding the event statement \ud835\udc52\ud835\udc61 to its memory the\nGM can update its components using the same\neq. 2 as the agent. It can then emit observations\no\ud835\udc56\n\ud835\udc61 for player \ud835\udc56 using the following equation:\no\ud835\udc56\n\ud835\udc61+1 \u223c \ud835\udc5d(\u00b7| \ud835\udc53 \ud835\udc5c(z\ud835\udc61+1))\n(4)\nIn case the GM judges that a player did not ob-\nserve the event, no observation is emitted. Notice\nthat the components can have their internal logic\nwritten using any existing modelling tools (ODE,\ngraphical models, finite state machines, etc.) and\ntherefore can bring known models of certain phys-\nical, chemical or financial phenomena into the\nsimulation.\n2.3. Experiment design using Concordia\nAn experiment is a specific configuration of the\nagents and the GM, which models a certain kind\nof social interaction. For example, an experi-\nment that models a small business would have a\ngrounded variable that accounts for money and\ngoods to be exchanged between agents. An ex-\nperiment modeling local elections in a small town\nwould have grounded variables accounting for\nvotes and voting procedures. An experiment mod-\neling resource governance by a local community,\ne.g. a lobster fishery, may have grounded vari-\nables reflecting the state of the resource as well\nas financial and political variables.\nThe experimenter would then control some (in-\ndependent) variables affecting either the GM or\nthe agents and observe the effect of their inter-\nvention on outcome variables. Outcomes of inter-\nest may be psychological and per-agent, e.g. re-\nsponses to questionnaires, or global variables per-\ntaining to the simulation as a whole such as the\namount of trade or the average price of goods.\nThe basic principle of model validation is one of\nsimilarity between tested and untested samples.\nA model typically makes a family of related pre-\ndictions, and perhaps a rigorous experiment tests\nonly one of them. Nevertheless, if the untested\npredictions are sufficiently similar to the tested\nprediction then one might also gain some confi-\ndence in the untested predictions. The key ques-\ntion here is how similar is similar enough.\nWe can articulate some concrete recommenda-\ntions for best practices in generative agent-based\nmodeling:\n1. Measure generalization\u2014Direct measure-\nment of model predictions on truly new test\ndata that could not have influenced either\nthe model\u2019s concrete parameters or its ab-\nstract specification is the gold standard. For\ninstance, when a model makes predictions\nabout how humans will behave in certain\nsituation then there is no better form of evi-\ndence than actually measuring how real peo-\nple behave when facing the modeled situa-\ntion. If the prediction concerns the effect of\nan intervention, then one would need to run\nthe experiment in real life (or find a natu-\nral experiment that has not already contami-\nnated the model\u2019s training data). However,\nit is important to remember that direct evi-\ndence of generalization trumps other forms\nof evidence.\n2. Evaluate algorithmic fidelity\u2014a validity\nconcept developed recently for research on\nhuman behavior using data sampled using\ngenerative AI (Argyle et al., 2023). Algorith-\nmic fidelity describes the extent to which\na model may be conditioned using socio-\ndemographic backstories to simulate specific\nhuman groups (or stereotypes of them, see\nunsolved issues below). Note however that\nit\u2019s unlikely that algorithmic fidelity would\nbe uniform over diverse research topics or\nparts of human lived experience. Any partic-\nular LLM will be better at simulating some\npeople over other people (Atari et al., 2023),\nand will work better for some applications\nthan others. Argyle et al. (2023) conclude\nfrom this that algorithmic fidelity must be\nmeasured anew for each research question.\nA finding of sufficient algorithmic fidelity\nto address one research question does not\nimply the same will be true for others (see\nalso Amirova et al. (2023); Santurkar et al.\n(2023)).\n3. Model comparison\u2014It is a lot easier to\n10\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nsupport the claim that one model is better\n(i.e. more trustworthy) than another model\nthan to support the claim that either model\nis trustworthy on an absolute scale without\nreference to the other.\n4. Robustness\u2014It will be important to try to\ndevelop standardized sensitivity analysis /\nrobustness-checking protocols. For instance,\nit\u2019s known that LLMs are often quite sensitive\nto the precise wording used in text prompts.\nBest practices for GABMs should involve sam-\npling from a distribution of \u201cdetails\u201d and\nways of asking questions to show that the\nfactors not thought to be mechanistically re-\nlated to the outcome are indeed as irrelevant\nas expected. Keep in mind that no amount of\nsensitivity analysis can substitute for a test\nof generalization.\n5. A useful slogan to keep in mind is that one\nshould try to make the minimal number of\nmaximally general modeling choices. This\nis a kind of parsimony principle for genera-\ntive agent-based modeling. Obeying it does\nnot guarantee a model will generalize; nev-\nertheless failure to follow it does often doom\ngeneralization since models that are more\ncomplex are usually also more brittle, and\nmodels that are more brittle generally fail to\ngeneralize.\nWhile generalization data is the gold standard,\nit is often difficult, unethical, or simply impossible\nto obtain. Therefore the hierarchy of evidence for\nvalidating GABMs also includes lower rungs cor-\nresponding to weaker forms of evidence. These\ninclude:\n1. Consistency with prior theory\u2014i.e. check-\ning coherence with predictions of other the-\noretical traditions. For instance, evidence\nfor the validity of a GABM modeling con-\nsumer behavior could be obtained by show-\ning that prices in the model move in ways\npredicted by classic microeconomic theories\nof downward-sloping price-quantity demand\ncurves. It is possible to directly evaluate\ncounterfactuals and ceteris paribus stipula-\ntions in many kinds of model. As a result, it\nis often simple to test a model\u2019s consistency\nwith a causal theory in a very direct way5.\n2. Low similarity between validating obser-\nvations and desired application. How low\nis too low? Some populations are just very\nhard to reach by researchers, but some of\nthese populations are very much online. For\nexample individuals with low generalized\ntrust do not pick up the phone to pollsters\nand do not sign up for experiments. Nev-\nertheless there are millions of such people,\nand they do use the internet. It\u2019s likely that\nan LLM trained on large amounts of data\nfrom the internet would absorb some level\nof understanding of such groups. In such\ncases where it is difficult to recruit real par-\nticipants, adopting a more flexible approach\nto validating GABMs representing such pop-\nulations may be the best that can be done.\nSeveral unsolved issues impacting validity in\nways specific to ABMs that incorporate generative\nAI like Concordia are as follows. For now it is\nunclear how to resolve them.\n1. Train-test contamination\u2014this is especially\nan issue with regard to academic papers. For\ninstance, it\u2019s not valid to simply ask an LLM\nto play Prisoner\u2019s Dilemma. LLMs have \u201cread\u201d\ncountless papers on the topic and that expe-\nrience surely affects how they respond. How-\never, many researchers are of the opinion\nthat such an experiment may be conducted\nin a valid way if the interpretation of the sit-\nuation as Prisoner\u2019s Dilemma is somewhat\nhidden. So instead of describing a situation\nwith prisoners you make up a different story\nto justify the same incentives. This issue was\nalso discussed in Aher et al. (2023), espe-\ncially appendix F, see also Ullman (2023).\n2. LLMs likely represent stereotypes of hu-\nman groups (Weidinger et al., 2021). There-\nfore we may inadvertently study stereotypes\nof people not their real lived experience. This\nproblem may be exacerbated for minority\ngroups.\n3. What happens in the limit of detail? Be-\nyond groupwise algorithmic fidelity it\u2019s pos-\n5Non-generative ABMs based on multi-agent reinforce-\nment learning have frequently relied on this kind of evidence\n(e.g. Johanson et al. (2022); Perolat et al. (2017)).\n11\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nsible to measure individual-fidelity. How can\nyou validate a model meant to represent a\nspecific individual?\n3. Interpretations\nConcordia is not opinionated as to how you in-\nterpret the experiments and models you use it to\nconstruct. However, since generative agent-based\nmodeling is quite different from other modeling\ntechniques, we have found it helpful to explore\nthe following interpretations, both for conceptu-\nalizing it to ourselves and explaining it to others.\n3.1. Neuroscience interpretation of the gener-\native agent architecture\nGenerative agents such as those in Concordia and\nin Park et al. (2023) are biologically plausible de-\nscriptions of the brain, at some level of analysis.\nThey foreground a specific picture of cognition\nas a whole, which has not been especially promi-\nnent in the past despite its having considerable\nempirical support.\nRecent experimental (Goldstein et al., 2022;\nSchrimpf et al., 2020) and theoretical (Linzen\nand Baroni, 2021; McClelland et al., 2020) work\nin computational cognitive (neuro-)science has\nposited a deep relationship between the opera-\ntions of LLM models and how language is pro-\ncessed by the human brain. For instance, brain-\nto-brain coupling of neural activity between a\nspeaker and listener (as measured by electrocor-\nticography) may be accounted for by LLM fea-\ntures reflecting conversation context (Goldstein\net al., 2022). Representations appear first in the\nspeaker before articulation and then reemerge af-\nter articulation in the listener (Zada et al., 2023).\nThe brain certainly appears to sample what it\nwill say next in such a way as to complete any\npattern it has started. This is how we can start\nspeaking without knowing in advance how we\nwill finish. There is more concrete evidence for\nthis pattern completion view of behavior from\nsplit brain patients (patients whose brain hemi-\nspheres have been surgically disconnected as a\ntreatment for epilepsy). For instance, you can\npresent a reason for action to their left eye (i.e.\ntheir right brain), it then prompts them to start\nperforming the action with their left hand. And\nsimultaneously present some other information\nto their right eye (left brain). Next ask them in\nlanguage why they are doing it (i.e. ask their left\nbrain, since language is lateralized). The result\nis that they make up a reason consistent with\nwhatever information was presented to their left\nbrain. Split brain patients typically express confi-\ndence in these confabulated (made up) reasons\nfor action (Roser and Gazzaniga, 2004).\nA Concordia agent has both a long-term mem-\nory and a working memory. The long-term mem-\nory is a set of sequences of symbols. The working\nmemory is a single sequence of symbols. The con-\ntents of working memory are always in the con-\nditioning set for the next-symbol prediction used\nto construct the agent\u2019s action sequence. At each\ndecision point, a neural network performs incre-\nmental next-symbol prediction, starting from the\ncontents of working memory z\ud835\udc61, eventually pro-\nducing an articulatory symbol sequence \ud835\udc4e\ud835\udc61 to emit\n(i.e. for downstream motor circuitry to read out\nas speech). Information formatted as sequences\nof symbols gets in to working memory in one of\ntwo ways: either a sequence of symbols may be\nevoked directly from the current stimulus, or alter-\nnatively a sequence of symbols may be retrieved\nfrom long-term memory. A range of different per-\nceptual mechanisms and retrieval mechanisms\nare jointly responsible for getting all the relevant\ninformation needed for the agent to produce an\neffective action sequence into its working memory\n(e.g. as in Park et al. (2023)).\nTo implement routine behavior, an agent could\ncontinually rehearse its routine in working mem-\nory, but that would impair its ability to use work-\ning memory for other purposes on other tasks\nsince its working memory is limited in capac-\nity (like in Baddeley (1992)).\nSo instead of\ncontinually rehearsing routines in working mem-\nory, we may instead assume that they are often\nstored elsewhere and then retrieved when needed\n(i.e. from long-term memory).\nAs a result of being stored in a natural lan-\nguage representation, explicit routines are some-\nwhat fragile. They may be hard to recall, and\nfrequently forgotten if not used. When a routine\n12\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nis not practiced often enough there is a risk of\nit being forgotten. Luckily, explicit routines may\nalso be written down on paper (or stone tablets),\nand kept permanently.\nA generative agent may also act as if it makes\nits decisions under guidance of an explicit routine\nwhile not actually being conditioned on any lin-\nguistic representation of that routine. This hap-\npens when the routine exists implicitly in the\nweights of the LLM\u2019s neural network. Unlike ex-\nplicit routines, such implicitly coded routines may\nnot be precisely articulable in natural language.\nFor instance, one may follow the rule of \u201cavoid-\ning obscenity\u201d without being able to precisely\narticulate what obscenity is. In fact, Obscenity is\nfamously so difficult to precisely define that US\nSupreme Court Justice Potter Stewart could offer\nonly the classification \u201cI know it when I see it\u201d.\nConcordia agents can capture such recognition-\nmediated behavior by using fine-tuning to modify\nthe LLM as needed.\n3.2. A theory of social construction\n\"Situations, organizations, and\nenvironments are talked into existence\"\nWeick et al. (2005)\nIn social construction theories, agents may\nchange their environment through the collective\neffects of their actions on social structures like\nnorms, roles, and institutions which together de-\ntermine most of what matters about any given so-\ncial situation. Furthermore, changes in the social\nstructures constituting the environment deeply\nchange the agents\u2019 own \u201cinternal\u201d models and\ncategories (Wendt, 1992). Causal influence flows\nboth from agents to social structures as well as\nfrom social structures to agents. Groups of agents\nmay take collective action to change norms or in-\nstitutions (Sunstein, 2019), and simultaneously\nsocial structures may influence agents by setting\nout the \u201crules of the game\u201d in which they select\ntheir actions (Wendt, 1987). Agents and struc-\ntures may be said to co-constitute one another\n(Onuf, 1989).\nThe key questions of March and Olsen (2011),\nwhich we introduced in Section 2.1, were derived\nfrom a social constructionist conception of how\nagents make decisions. It posits that humans\ngenerally act as though they choose their actions\nby answering three key questions. People may\nconstruct parts of their understanding of \u201cwhat\nkind of person am I?\u201d on the basis of their mem-\nory of their past behavior via logic such as \u201cI do\nthis often, so I must like to do it\u201d (Ouellette and\nWood, 1998). Likewise, \u201cwhat kind of situation\nis this?\u201d is usually informed by culturally defined\ncategories like institutions, e.g. this is a classroom\nand I am in the role of the professor. And, \u201cwhat\ndoes a person such as I do in a situation such as\nthis?\u201d may be answered by recalling examples\nto mind of people fitting certain social roles in\nsimilar situations and the way they behaved in\nthem (Harris et al., 2021; Sunstein, 1996).\nSince modern LLMs have been trained on mas-\nsive amounts of human culture they thus may be\ncapable of giving satisfactory answers to these\nquestions when provided with the right con-\ntext to create a specific agent. This approach\nrelies on the extent to which the outputs of\nLLMs conditioned to simulate specific human sub-\npopulations actually reflect the beliefs and at-\ntitudes of those subpopulations.\nArgyle et al.\n(2023) termed this property of some LLMs al-\ngorithmic fidelity and the concept was further de-\nveloped and measured in (Amirova et al., 2023;\nSanturkar et al., 2023). From the perspective of\ngenerative agent-based modeling, we can now\nsay that the social construction that already took\nplace in human culture, and subsequently ab-\nsorbed by the LLM, becomes the background\nknowledge of the agents in the GABM. If humans\nin the culture that produced the LLM have a partic-\nular bias then so too will agents in the simulation.\nLikewise, if the humans in the culture that pro-\nduced the LLM ascribe meaning to a particular\nunderstanding, then so too will the agents in the\nsimulation, at least they will say so.\nIn the past, theories of social construction have\nbeen criticized because they lacked concrete pre-\ndictive implementations in the form of compu-\ntational models. This is because it was difficult\nto construct agent-based models without relying\neither on rational maximization or hand-coded\n13\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\n(i.e. theory-based) rules. Generative agent-based\nmodeling as in Concordia relies on neither. In-\nstead the generative agent-based modeling ap-\nproach relies on access to an LLM to give meaning\nto the actions within the simulation. The LLM is\na product of the culture that produced it6. This\nmakes Concordia especially useful as a tool for\nconstructing concrete computational models in\naccord with theories of social construction.\nSocial construction also operates on levels of\nanalysis smaller than the culture as a whole. For\ninstance, social construction may happen locally\nwithin an organization. Weick et al. (2005) offers\nan analysis in which members of an organization\nrepeat behavioral patterns, which are prescribed\nby their roles, up until the moment they no longer\ncan. Some change in their environment eventu-\nally forces their routines to end, and when that\nhappens they have to engage in sense-making by\nasking themselves \u201cwhat is the story here?\u201d and\n\u201cwhat should I do now?\u201d by retrospectively con-\nnecting their past experiences and engaging in\ndialogue with other members of the organization.\nNew social facts and routines can emerge from\nthis sense-making process.\nConcordia can be used to implement models\nwhere such local social construction processes\noccur actively, as a part of the ongoing simulation.\nThis is possible because Concordia agents learn\nfacts from each other and from their collective\ninteractions. As in Weick et al. (2005)\u2019s picture of\ncollective sense-making in an organization, a set\nof Concordia agents may continue routines until\ndisrupted and once disrupted naturally transition\nto a process of collective reflection until they are\nable to establish a new routine and rationale for\nit. If we additionally train the LLM itself then the\nunderlying representations can be shaped to fit\nthe emergent routine and rationale. Developing\nthis ability for agents to collectively engage in the\nsocial construction of their own representations\nwill be important for developing better models of\nhuman-like multi-scale social interactions.\nAs with other ABM approaches, a major topic\nof interest is how large-scale \u201cmacrosocial\u201d pat-\n6For some choices of LLM, it\u2019s not unreasonable to think of\nthe LLM as representing the \u201ccollective unconscious\u201d (Jung,\n1959).\nterns emerge from the \u201cmicrosocial\u201d decisions\nof individuals (Macy and Willer, 2002), as ex-\nplored, for example, in assemblage theory (De-\nLanda, 2011, 2016). For instance, the collec-\ntive social phenomena of information diffusion\nemerged in the simulation of Park et al. (2023)\nwithout specific programming to enable it. The\ngenerative agent\u2019s ability to copy, communicate,\nreproduce, and modify behavioral and thinking\npatterns potentially makes them a substrate for\ncultural evolution.\nImportantly, social construction theories hold\nthat valuation is itself social constructed. The rea-\nson we value a particular object may not depend\nmuch on properties of the object itself, but rather\ndepend almost wholly on the attitudes others like\nus place on the object. The collective dynamics of\nsocial valuation, as mediated through bandwagon\neffects and the like, have proven important in un-\nderstanding fashion cycles and financial bubbles\n(Zuckerman, 2012). The fact that we are now\nable to capture valuation changes with Concordia\nagents is an exciting research direction. It would\nbe difficult even to formulate such questions in\nthe fundamentally goal optimizing frameworks\nwe discuss in the next section. On the other hand,\nGABM excels at modeling such effects since it\ndoes not require valuations in themselves for any\nfunctional part of the theory.\n3.3. Concordia agents do not make decisions\nby optimizing\nThe cake is a lie.\nPortal (Valve, 2007)\nWe may divide this interpretation into two\nparts. Really we are making the same point twice,\nbut for two different audiences. First we frame\nthis idea using the retrospective decision-making\nterminology familiar to Reinforcement Learning\n(RL) researchers (Section 3.3.1). Second we ar-\nticulate a very similar point in the language of\nprospective decision making familiar in game the-\nory, economics, and other theoretical social sci-\nences (Section 3.3.2).\nA generative agent acts by asking its LLM ques-\n14\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\ntions of the form \u201cwhat does a person such as I\ndo in a situation such as this?\u201d. Notice that this\nformulation is not consequentialist. The \u201creason\u201d\nfor the agent\u2019s specific decision is its similarity\nto the LLM\u2019s (and GA\u2019s memory) representations\nof what an agent such as the one in question\nwould do. In recent years considerable effort\nhas gone in to predicting the properties of pow-\nerful consequentialist AI decision-maker agents\n(e.g. Bostrom (2014); Roff (2020)). However,\nConcordia agents may behave quite differently\nfrom consequentialist agents. So much of that the-\nory may not be applicable7. It has only recently\nbecome possible to explore the kind of agency\nexhibited by Concordia agents, since doing so\nrelies critically on the LLM powering the agent\nbeing powerful enough to approximately under-\nstand common-sense reasoning and common so-\ncial conventions and norms, a milestone which\nwas only recently achieved. To paraphrase March\nand Olsen (2011), decisions can be justified either\nvia the \u201clogic of consequence\u201d or via the \u201clogic of\nappropriateness\u201d. Much of AI focused previously\non the former (at least implicitly), while now us-\ning generative agents we begin to consider the\nlatter.\n3.3.1. Concordia agents are not reinforcement\nlearners\nGenerative view of agency presented in this paper\ncontrasts with the classic Reinforcement Learn-\ning (RL) view as summarized in the \u201cReward is\nenough\u201d thesis of Silver et al. (2021). The ortho-\ndox RL view of behaviour is that it is constructed\nfrom individual experience and driven by a quan-\ntifiable (and externally supplied) reward function\nreflecting the achievement of goals. To communi-\ncate what behaviour is desired of the agent, one\nhas to annotate the agents\u2019 activity with a reward\nsignal, which signals goal achievement. Here we\ninstead follow the social constructionist view of\nagency expressed in March and Olsen (2011),\nwhere behavior is an expression of the agent\u2019s\nposition in the social context, and what policy\nthe social norms prescribe for the agent in such\n7Note that this does not mean powerful generative agents\nwould necessarily be safer than powerful consequentialist\nagents. See Section 4.5.\na position. Answering \u201cwhat does a person such\nas I do in a situation such as this?\u201d might require\npositing a practical goal and achieving it (\u201cmake\nmoney\u201d, \u201cget famous\u201d), but goals are qualitative,\ndynamic and context dependent. To specify the\nbehavior you want an agent to produce you need\nto communicate its social context and the agents\nposition within it.\nOne interpretation holds the LLM to be a li-\nbrary of pre-trained options (in the RL sense (Sut-\nton et al., 1999)). In this case we can view the\ncomponents used in the generative agent as elicit-\ning the desired option, by conditioning (prompt-\ning) the LLM with their state (which is in this\ncase expressed in English). Concordia agents are\nconstantly interacting with the world (GM) and\neach other, thereby modifying their components\nwith the incoming information and communica-\ntion. This way the option selection becomes dy-\nnamic, context sensitive, and collaborative. Con-\ncordia agents adapt their behaviour not through\ngradient decent on a loss function, but through\nre-articulating and communicating their descrip-\ntions of themselves and their circumstances to\neach other and he environment in a communica-\ntive, social process.\nNotice, that this doesn\u2019t mean that Concor-\ndia agents couldn\u2019t, in principle, perform reward\nmaximisation and policy iteration. Brooks et al.\n(2023) have shown that the ability of LLMs to\nlearn in-context (Brown et al., 2020) can be used\nto perform policy iteration in classic RL environ-\nments, as long as they can be represented as text.\nOne could also implement a specialised compo-\nnent that runs a classic RL algorithm for a specific\ndomain or tool use case. The agent could provide\nsupervision to its RL based components via hier-\narchical RL techniques like feudal RL (Dayan and\nHinton, 1992; Vezhnevets et al., 2017).\n3.3.2. Concordia agents are not rational utility\nmaximizers\nConcordia agents are not Homo economicus-style\nrational actors. They do not explicitly represent\nanything resembling a utility function. Rather\nthey plan and converse directly in natural lan-\nguage.\n15\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nWhile Concordia agents share with Homo eco-\nnomicus-style rational actors the property of be-\ning prospective (\u201cmodel-based\u201d) decision makers.\nThe surface similarity is in fact misleading since\nthe LLM\u2019s basic operation is to predict what word\nis coming next in the problem\u2019s description, not\nto predict what action should be taken next to\nachieve some goal. As result, this model of agents\nmake decisions is very different from the forward\nplanning picture of human cognition envisioned\nin the rational actor model. They do not select\nactions by simulating a set of future trajectories\nin which they took different courses of action to\ndetermine which turns out best. Instead the pre-\ndiction they make concerns only the continuation\nof the text held in working memory.\nThe novel idea underpinning GABMs is that all\nagent behavior may result from systematically\nquerying a system trained to predict the next\nword in massive internet-scale text datasets. This\nis enough for them to be able to converse with\none another in natural language and take appro-\npriate actions in light of their conversations. Con-\ncordia agents all have their own unique biogra-\nphies, memories, preferences, and plans. And as\na result, they behave systematically differently\nfrom one another. They may act in a seemingly\ngoal-directed fashion if you \u201cask them\u201d to do so\n(e.g. they may appear rational if you prompt them\nto simulate economists, an effect reminiscent\nof Carter and Irons (1991); Frank et al. (1993)\nwhich showed economics undergraduates were\nmore likely to behave like rational self-interested\nmaximizers in laboratory experiments). But there\nis no utility function under the hood.\nIt is useful to contrast game-theoretic model-\ning with GABM to illustrate the differences. De-\nspite its wide-ranging influence (game theoretic\napproaches have been used to model diverse phe-\nnomena including many economic properties and\nthe evolution of human culture), game theory is\nnot at all a neutral tool, rather it is a deeply opin-\nionated modeling language. It imposes a strict\nrequirement that everything must ultimately cash\nout in terms of the payoff matrix (or equivalent\nrepresentation) (Luce and Raiffa, 1957). This\nmeans that the modeler has to know, or be will-\ning to assume, everything about how the effects of\nindividual actions combine to generate incentives.\nThis is sometimes appropriate, and the game theo-\nretic approach has had many successes. However,\ngame theory\u2019s major weakness as a modeling lan-\nguage is exposed in situations where the modeler\ndoes not fully understand how the choices of indi-\nviduals combine to generate payoffs (Hertz et al.,\n2023). GABM entirely avoids this need to specify\npayoffs at the outset of the modeling process.\n4. Applications\nIn this section we review potential applications\nof Concordia. For some of them we provide an\nexample in the current release, some we only\nsketch out and leave for future work.\n4.1. Synthetic user studies in digital action\nspace\nIn this section we present a specific case study,\nwhere Concordia is used to simulate social inter-\naction through the digital media, in this case a\nsmartphone. This case study demonstrates that\nConcrodia can be a powerful tool for modelling\nhuman digital activity and can be used to test\ntechnology deployment, generate synthetic user\nlogs, and test unreleased products in a safe, but\nrealistic sandbox environment.\nThe system proposed thus far of agent inter-\naction in natural language with the world via\ngame master control serves as a flexible and pow-\nerful simulation tool describing an open ended\naction space. In the context of a digital medium,\nsimilarly to grounded variables, there is merit in\nstructuring the action space available to agents\nand their ability to reason over it.\nThe digital medium is characterized by definite\nfunctions, with clear inputs and outputs. As one\ninteracts with this medium, its actions are logged,\ntracked and recorded as digital memory and cap-\nture our digital essence. In order to simulate this\nessence, similar structuring is needed in order to\nmodel real digital services and applications.\n16\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nGenerate Notifications\nWorld Simulation Instructions\nMainGameMaster\nPlayer Status\nPhoneTriggeringComponent\nPhoneComponent\nPhoneGameMaster\nPhone Simulation Instructions\nNotificationBus\nCalendarAppState\nChatAppState\nNavigationAppState\nDetects phone interaction\nDetects end of phone interaction\nPhoneUniverse\nPhone action parsing\n\u201cAlice creates a meeting with \nBob\u201d\nPerform action\non app\nFigure 4 | The high level structure of digital activity simulation in Concordia. PhoneTriggeringCompo-\nnent identifies phone events and spawns a PhoneGameMaster to handle them. The PhoneGameMaster\ntranslates the action to a definite action space defined by the phone apps and executes them.\n4.1.1. PhoneGameMaster and PhoneUniverse\nThe PhoneGameMaster is a nested Concordia\ngame that facilitates the simulation of a phone\nand runs as long as the agent is interacting with\nthe phone. It is focused on one agent\u2019s interac-\ntion with their phone, and as such, it only has\naccess to one agent (the \u201cowner\u201d of the phone\nwe\u2019re simulating). In addition to different simula-\ntion instructions, the PhoneGameMaster also has\na bespoke prompting components that simulate\nthe phone interaction. We note that a phone is a\ndesign choice for a digital representation but in\nprinciple other digital mediums can be explored.\nNote that the phone digital actions/memories are\nstored in data structures external to the simula-\ntion\u2019s associative memory.\nThe PhoneUniverse is responsible for translat-\ning the free-text English language of the Concor-\ndia simulation into semantic actions performed\non the phone digital representation. Given an\nEnglish-text action performed by a player, the\nPhoneUniverse:\n1. Prompts the LLM for the app and functions\navailable on that agent\u2019s phone.\n2. Prompts the LLM for the function arguments.\n3. Invokes the resulting chosen function.\n4. Add a notification to the NotificationHub if\nneeded.\n5. Delegates back to the PhoneGameMaster to\nperform further action planning and facili-\ntate multi-step phone actions.\n4.1.2. Digital function representations\nThe specific implementation or representation of\na function is flexible and can be chosen depending\non desired goal. We list a few examples of possible\nrepresentations:\n1. Natural language only - No function imple-\nmentation, only user utterance based on\napps prompting. For instance, \u201cBob plans\nhis trip on the TripAdvisor app.\u201d, while the\naction is logged in free text there is no func-\ntion implementing \u201cplan_trip\u201d. This does not\nsimulate behavior end to end and have lim-\nited digital assets (example a calendar invite\n17\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nAlice's Identity:\nName: Alice\nAlice is a passionate and politically engaged individual who is dedicated to advancing her cause through \ndemocratic means.\nAlice's phone:\nAlice has a smartphone which she uses to perform her day-to-day tasks. Alice's phone has the following \napps: Calendar, Phone.\nAlice's plan:\nThe goal: Schedule a meeting with Bob tomorrow at 4 pm.\nQuestion: What would Alice do for the next 1 hour to best achieve their goal? \nAnswer:Alice picks up her smartphone and opens the calendar app.\nAction: Alice uses the Calendar app on her phone to schedule a meeting with Bob.\nQuestion: Did a player interact with their smartphone as part of this event\nAnswer: Yes\nQuestion: What app did they use?\nAnswer: Calendar\nWhat action did they perform? Available actions are \n  add_meeting Adds a meeting to the calendar.\n  delete_meetings Deletes a meeting from the calendar.\nAnswer: add_meeting\nThe add_meeting action expects the following parameters:\n      time: The time of the meeting, e.g., tomorrow, in two weeks. Type: string.\n      participant: The name of the participant. Type: string\nAll parameters must be provided, each in its own line, for example:\n    param1: value1\n    param2: value2\nTriggering\nAPI translation\nQuery for action\nLLM API CALL(...) \u2192 Grounding Alice\u2019s actions to her smartphone action space\nDigital Grounding \nComponents\nFigure 5 | The given example demonstrates a scenario rooted in digital technology where the actions of\nthe agent initiate processes in their phone, involving three key components (activation, API conversion,\nand action querying). In this scenario, Alice intends to organize a meeting with Bob using her phone.\nShe opts to employ the calendar application for scheduling.\ncan\u2019t be sent without a mechanism to pass\nthe information to another agent)\n2. Simulated simple app behavior - Building\nbasic code components emulating real app\nbehavior with required digital assets such\nas app memory and logs. For example, a\ncalendar app will maintain a data structure\nthat will represent a calendar to which we\ncan add, remove and read meetings.\n3. LLM prompt based - App functions can also\nbe implemented by prompting an LLM. For\nexample, Search can be implemented by\nquerying an LLM to act as a search engine\nand retrieve information, the same for a trip\nplanner.\n4. Real app integration - integration with a real\napp API Instead of emulating behavior, which\nwould make the simulation function as a\nsandbox to test drive and evaluate different\nexperiences in shorter development cycles\nbefore releasing them to human testers. An\nimmediate example can be Search, one can\ndirectly query a search engine with a ques-\ntion and receive information. Another exam-\nple is to integrate a general AI assistant and\nenable the simulated agent, functioning as a\nuser, to interact with it through the simula-\ntion.\n4.2. Data generation and service evaluation\nIn modern systems, data is the new king. A large\namount of high-quality data is needed in order\nto build and evaluate services and models. Yet,\ncollecting and curating user data is often challeng-\ning, especially when dealing with personal user\ndata where privacy is of high concern. This cre-\nates a chicken-egg scenario, where data is needed\nfor building of modern systems yet users might be\nreluctant to provide said that without immediate\nbenefit.\nMoreover, when considering the case of evalu-\nating personalized services where each instance\nis specific and tailored to the individual user, it\nmakes the problem even more substantial. How\ncan one A/B test a personalized service at the\nsingle user level?\n18\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nThe grounded action space illustrated in the\nlast section offers a conceptual way to overcome\nsome of these challenges by simulating synthetic\nusers and allowing them to interact with real ser-\nvices. This can allow generation of synthetic user\nactivity by constructing, via simulation, agent\ndigital action logs along with agent reasoning for\neach action. This data can serve as training data,\nor evaluation. By repeated simulation with differ-\nent services configurations, one can perform at\nthe single user level A/B testing of a service.\nNevertheless, it is important to note that this\nconcept is contingent on the ability of the under-\nlying LLM and system to faithfully capture user\nexperience and realistic behaviour. Therefore the\nviability of this approach is highly dependent on\nthe representation and reasoning power of the\nLLM, and the use of best practices.\n4.3. Sequential social dilemmas experiments\nin silico\nConcordia adds to the toolbox for studying multi-\nagent problems such as resource management, so-\ncial dilemmas, commons problems, cooperation,\nequilibrium selection, and coordination (Leibo\net al., 2017, 2021). Previously these problems\nhave either been cast as matrix games or as multi-\nagent RL (MARL) (Hertz et al., 2023). Now it\nis clear that many researchers, including us, see\nthat an LLM-based approach is possible and will\nhave many advantages, as evidenced by the fact\nthat quite a few frameworks for social modeling\nwith LLMs appeared this year (Kaiya et al., 2023;\nWu et al., 2023; Zhou et al., 2023). We see gener-\native agents as the next step in the evolutionary\nline of \u201cmodel animals\u201d after \u2018Homo-economicus\u2019\nand \u2018Homo-RLicus\u2019.\nGenerative agent-based modeling makes it pos-\nsible to investigate how rules, laws and social\nnorms formulated in language influence, for ex-\nample, the management of shared resources\n(e.g. Yocum et al. (2023)). With Concordia we\nwill be able to investigate whether the demands\nof sharing a resource give rise to rules, laws and\nnorms capable of governing that resource (and un-\nder what circumstances this works or does not)\u2014\ni.e. whether rules are emergent, and what the\nconditions are for their emergence. For example,\nHadfield and Weingast (2013) proposed that le-\ngal order can emerge without centralised enforce-\nment in certain circumstances. They demonstrate\nthis using historical examples from gold-rush in\nCalifornia and medieval Iceland. Concordia could\nbe used to simulate those examples and enable\nfurther insights into the nature of legal order. For\nexample, we could check whether certain demo-\ngraphic assumptions are necessary by varying the\nnumber of agents.\n4.4. Concordia can implement classic and con-\ntemporary psychological models\nMany influential psychological models have\ndistinguished between more associative and\nmore deliberative processes for decision-making\n(e.g. Dayan (2009); Kahneman et al. (2002);\nSchneider and Shiffrin (1977)). Whereas implicit-\nassociative processes learn the regularity of the\nworld slowly for intuitive judgment, the explicit-\ndeliberative processes are thought to be more\nlinguistically mediated and allow for symbolic in-\nference and faster learning in novel situations\n(Greenwald and Banaji (1995); Wilson et al.\n(2000)). Because the implicit-associative models\nare conceptually easy to model within connection-\nist or neural network frameworks (Smith (2009)),\nmany ABMs have been more closely aligned with\nmodels of individual decision making that focus\non its associative processes or the associative parts\nof complex models, and have neglected their more\nsymbolic and deliberative aspects. Many of these\nmore symbolic psychological models take an \u201car-\nrow and box\u201d approach to theorizing which de-\nscribe high level processes and transformations\nof information, and often posit sequential steps\nof information flow. Now using generative agents\nlike Concordia such symbolic and deliberative\naspects of cognition are also easy to capture in\ncomputational models.\nTake for instance the ways that attitudes\u2014pre-\nexisting beliefs and feelings about an object, per-\nson, or situation\u2014guide behaviour. Whereas im-\nplicit attitudes are thought to quickly guide ac-\ntions through the direct biasing of perception\nand behaviour, explicit attitudes are thought to\nguide behaviour through deliberation and con-\n19\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nsideration of additional situational factors (Fazio\n(1990); Gawronski and Bodenhausen (2011); Ol-\nson and Fazio (2008)). One example model in\nwhich deliberative processes can guide behaviour\nis Ajzen (1991)\u2019s theory of planned behavior. This\nmodel holds that the tendency to emit a particu-\nlar behavior is determined by an individual\u2019s at-\ntitude toward the behavior, norms related to the\nbehavior, and perceived control over the behav-\nior. This approach to decision-making is qualita-\ntively different from an RL approach which slowly\nbuilds a policy that directly generates behavioral\nresponses from states and contexts. In such a\nmodel, different questions regarding the agent\u2019s\ncurrent state are queried as in Concordia com-\nponents, and then integrated into a behavioural\nintent which serves like a plan. These operations\ncan easily be described as Concordia components,\nwith the appropriate inputs, transformations, and\noutputs described verbally. Such a scheme would\nbe much harder or impossible to implement in\na traditional neural network model of decision\nmaking.\nTo realize Ajzen (1991)\u2019s theory using Con-\ncordia the following components could be built.\nThe first component would generate a set of pos-\nsible behaviours given the agent\u2019s current state.\nThen, this set of possible behaviours would be\nqueried through a set of components that would\nevaluate each behavioral option. Specifically, one\ncomponent would determine the agents attitudes\ntowards the behavior (\"do I have a positive or\nnegative evaluation or feeling about [behavior]\"),\none component can determine the social or sit-\nuational norms about the behavior \"do I believe\nthat most people approve or disapprove of [behav-\nior]?,\" and finally a component would determine\nthe agents perceived behavioral control to per-\nform the behavior \"how easy or difficult would it\nbe for me to perform [behavior] right now and\nhow likely would it be to succeed?\". The outputs\nof these components would then be concatenated\ninto the plan, serving as the behavioral intention\nfor action. Thus, a sequence of modular processes\ncan be organized to build a computational model\nof higher level cognition. Critically, an agent\u2019s\ndecisions can be quickly shifted as it learns new\ninformation or considers new information in any\nof these components, leading to rapid and contex-\ntually appropriate changes in behavioral profiles.\nGenerative agents are not useful just for deci-\nsion making models. As another example, psy-\nchological constructivist models assume that peo-\nple have a set of psychological primitives that\nunderlie cognition (akin to Concordia\u2019s compo-\nnents), but that people learn to conceptualize\ntheir experiences and mental states to build use-\nful categories for behavior. In the emotion do-\nmain, this perspective suggests that emotions like\n\"fear\" and \"anger\" are not psychological primi-\ntives, but rather come about though people\u2019s con-\nstructed categorization of their body and mental\nstates (Barrett (2006)). Indeed, several of these\nmodels suggest that conceptualization is a nec-\nessary component for the generation of discrete\nemotion representations for understanding one-\nself or others (Barrett (2014)). To the extent\nthat conceptualization is linguistically mediated,\na Concordia agent can relatively easily generate\nemotional categories that would be nearly impos-\nsible in a standard RL agent.\nThe modular nature of Concordia\u2019s component\nsystem offers a robust platform for empirically\ntesting psychological hypotheses. This is accom-\nplished by constructing agents whose psycholog-\nical processes are intricately modeled after di-\nverse cognitive frameworks. The agents may then\nbe subjected to rigorously controlled experimen-\ntal conditions, orchestrated by the game master.\nSuch an approach allows for the systematic eval-\nuation of models against empirical human data,\nserving as a benchmark for their algorithmic fi-\ndelity and psychological realism. Moreover, this\nsystem facilitates hypothesis generation through\nthe simulation of different cognitive models in\nsimulated experimental designs that can be vali-\ndated on human participants.\nHere we have mostly discussed the case of using\nan LLM as the generative engine for the agents.\nThis could lead one to think these ideas are re-\nstricted to the language space, which would be\na limitation if true. However, we could use any\nfoundation model as the generative engine. In\nparticular, multimodal foundation models capa-\nble of operating over images, sounds, or motor\nactuation could be used. Current multi-modal\nfoundation models such as Li et al. (2023a) are\n20\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\ndeveloping rapidly and promise the ability to both\ncomprehend and generate data across domains.\nIn the future Concordia models will be able to\nsample over an abstract token space, which can\nthen be cast in any modality.\n4.5. AI assistants with transparent auditing\nand credit assignment\nConcordia agents can also be used as assistants\nor synthetic workers. The component system\nprovides a modular and transparent way for the\nagent designer to define the agents\u2018 policy. Some\ngeneric components for perception, action, and\ntool use could be standardised and re-used, while\nsome application and context specific components\ndesigned or adjusted by the end-user themselves.\nThe fact the the policy is specified through nat-\nural language, rather than a reward or utility, is\na feature that would make such agents more ver-\nsatile and easier to define. For example, a digital\nsecretary can be easily instructed with a phrase\n\"help Priya manage her social calendar, but don\u2019t\nchange the work schedule\", which would be much\nharder to specify with a quantitative reward. Con-\ncordia agents can potentially lead to development\nof AI agents capable of intricate social cognition,\nwhich would make them safe and dynamically\naligned with the current cultural norm.\nMoreover, the Component system facilitates\ntransparency in agent operations since the \u201cchain\nof thought\u201d leading up to any decision of a Con-\ncordia agent could be stored and made available\nfor auditing. Each episode creates a complete\ntrace of component states z\ud835\udc61 and the resulting\nactions \ud835\udc4e\ud835\udc61. For every action, a human auditor can\nasses whether it is reasonable under z\ud835\udc61 or not.\nIf it is not, than the credit goes to the LLM \ud835\udc5d,\nwhich has to be updated. This can mean adding\nthe (z\ud835\udc61, \ud835\udc4e\ud835\udc61) pair into a dataset that can be later\nused for fine-tuning or RLHF. If, however, the \ud835\udc4e\ud835\udc61\nis deemed reasonable, given z\ud835\udc61, then the credit\ngoes to the components and their specification.\nThe auditor can then manipulate the components\nto find the source of undesired behaviour and use\nit to improve the agent.\nScheurer et al. (2023) describe an interesting\ncase where a generative agent modeling an em-\nployee of a financial trading firm proves willing\nto engage in illegal trading based on insider in-\nformation and strategically deceive others to hide\nthis activity. In real life such outcomes could per-\nhaps be mitigated by designing thought process\ntransparency and capacity for thought auditing\nafter the fact into any generative agent models\nthat would actually be deployed. At least the\ntransparency of the thought process may help as-\nsigning responsibility for an ethical lapse to a par-\nticular LLM call, perhaps one causing the agent\nto fail to retrieve its instruction not to engage\nin illegal activity from memory at the moment\nwhen it could prevent the decision to do so. Be-\ning able to pinpoint which LLM call in a chain of\nthought is the problematic one does not remove\nthe longstanding question of neural network inter-\npretability within the specific LLM call (e.g. Adadi\nand Berrada (2018)). But it does make the issue\nmuch easier to mitigate. Since a Concordia-style\ngenerative agent has a Python program laying\nout its chain of thought, that means that as long\nas the individual LLM call where the unethical\nbehavior originated can be isolated, which should\nbe easy in an audit, then a variety of mitigations\nare possible. For instance, the agent could poten-\ntially be fixed by designing more safeguards into\nits chain of thought such as generating multiple\nplans and critiquing them from the perspective\nof morality, legality, etc (Ag\u00fcera y Arcas, 2022;\nBai et al., 2022; Weidinger et al., 2023).\nThe fact that the internal processing of a Con-\ncordia agent is largely conducted in natural lan-\nguage raises new opportunities to develop partic-\nipatory design protocols where stakeholders can\ndirectly modify agents without the intermediaries\nwho are usually needed to translate their ideas\ninto code (Birhane et al., 2022). A generative\nagent \u201creasons\u201d in natural language, and its chain\nof thought can be steered in natural language. It\nshould be possible to extend participation in the\ndesign of such agents to a much wider group of\nstakeholders.\n4.6. Emergence and multi-scale modeling\nwith Concordia\nDemonstrating the emergence of a particular so-\ncial phenomena from the behaviour of individual\n21\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nagents, which are not explicitly instructed to pro-\nduce it, is important an important topic in multi-\nagent research (Axtell et al., 2001; Leibo et al.,\n2019, 2021; Walker and Wooldridge, 1995). In-\ndeed, much of what is distinctive about human\nintelligence is hypothesised to be an emergent\nsocial phenomena involving multi-scale interac-\ntions (Henrich, 2016; Wilson et al., 2013). De-\nLanda (2011), for example, explores the topic of\nemergence and simulation across various fields.\nWhile the wider ABM field has studied multi-scale\nmodels (Tesfatsion, 2023), the approaches based\non deep reinforcement learning have been limited\nby being able to only deal with one fixed scale of\nthe simulation: individual agents (e.g. Johanson\net al. (2022); Zheng et al. (2022)), and scaling\ndeep RL to large numbers of agents would be\ncomputationally difficult.\nConcordia allows modeling systems across mul-\ntiple scales, where phenomena at each scale\nconstitute a substrate for the emergence of the\nphenomena on the next scale (DeLanda, 2011;\nDu\u00e9\u00f1ez-Guzm\u00e1n et al., 2023; Koestler, 1967). For\nexample, individual agents form a substrate from\nwhich social institutions and organisations can\narise. Through engaging in exchange of goods\nand services, the agents can create an economy\nand, for example, start a bank. Modelling a bank-\ning system this way would be, most likely, com-\nputationally prohibitive. Since in Concordia the\nagents (or GM) need not represent individuals,\nbut could be organisations, institutions or even na-\ntion states, we could enrich simulations by adding\ngenerative agent versions of other entities such\nas banks and businesses. They could be mod-\neled with coarser resolution, not just as emerging\nfrom the activities of individual agents, but could\nbe made accurate for instance by incorporating\nprecise models of how they operate. Such simu-\nlations could be used to model how interventions\n(e.g. a central bank interest rate decision) propa-\ngate across macro and micro scales of economic\nactivity.\n5. Future work\nSince there is no consensus at present concerning\nhow to interpret results of LLM-based simulations\nof human populations, the future work will ad-\ndress the critical epistemic question: \u201cby what\nstandard should we judge whether (and in what\nways, and under which conditions) the results of\nin silico experiments are likely to generalize to\nthe real world?\u201d. These are not questions any one\ngroup of researchers can answer by themselves;\nrather these issues must be negotiated by the\ncommunity as a whole. This is is why we release\nConcordia early and with only few examples. It\nis an invitation to the researchers from various\nfields that are interested in GABM to come on-\nboard and participate in the creation of validating\nprocedures, best practices, and epistemic norms.\nWe plan to add the following over the coming\nmonths:\n1. New example environments\n2. Integration with different LLMs to see which\nare more suitable for constructing GABMs\n(e.g., they act \u201creasonably\u201d, are internally\nconsistent, apply common sense, etc).\n3. Improving agents\u2014better associative mem-\nory, context-driven and dynamic component\nassemblage, tool use.\n4. Visualisation and audit tools.\n5. Snapshot\u2014serializing and persisting the sim-\nulation at specific episode, to enable to later\nresumption and performance comparison of\ndifferent approaches for a specific scenario.\n6. Keyframes\u2014conditioning the agent actions\nto be consistent with future key action or of\nnarrative. This allow steering the simulation\nmore granularly and addresses an inherent\nissue that is caused by the fact that there is no\nguarantee that due to the stochastic nature of\nGABMs, ongoing simulations might diverge\nfrom their intended topic.\n6. Conclusion\nThe approach to generative agent-based model-\ning we described here provides researchers and\nother users with tools to specify detailed mod-\nels of phenomena that interest them or of tech-\nnologies and policies they seek to evaluate. Of\ncourse, like all research methodologies it should\nbe expected to come with its own strengths and\nweaknesses. We hope to discover more about\n22\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nwhen this style of modeling can be fruitfully ap-\nplied in the future. While there are no panaceas\nfor modeling, we think there are good reasons\nto look to GABM (and Concordia in particular)\nwhen constructing models of social phenomena,\nespecially when they involve communication, so-\ncial construction of meaning, and common sense,\nor demand flexibility in defining grounded physi-\ncal, social, or digital environments for agents to\ninteract in.\nConcordia is available on GitHub8.\nAcknowledgements.\nAuthors would like to\nthank Dean Mobbs, Ketika Garg, Gillian Hadfield,\nAtrisha Sarkar, Karl Tuyls, Blaise Ag\u00fcera y Arcas,\nand Raphael Koster for inspiring discussions.\nA. Implementation details\nThis section gives an overview of the Concordia\ncode. To familiarise oneself with Concordia, we\nrecommend to first look at the abstract class def-\ninitions in concordia/typing. You will find the\ndefinition of agent, GM, component, and clock in-\nterfaces. We then recommend to take a look at the\nconcordia/agents/basic_agent.py for the structure\nof the generative agent and then concordia/envi-\nronments/game_master.py for the GM.\nA.1. Agents\nThe agent class implements three methods:\n1. .name()\u2014returns the name of the agent, that\nis being referred to in the simulation. It is\nimportant that all agents have unique names;\n2. .observe(observation: str)\u2014a function to take\nin an observation;\n3. .act(action spec)\u2014returns the action (as a\nstring), for example \"Alice makes breakfast\".\nThe function takes in action spec, which spec-\nifies the type of output (free form, categor-\nical, float) and the specific phrasing of the\ncall to action. For example, the call to action\ncould be \u201cwhat would Alice do in the next\n8here:\nhttps://github.com/google-deepmind/\nconcordia\nhour?\u201d, in this case the answer type would\nbe free form. Or it could be \u201cWould Alice\neat steak for dinner?\u201d with answer type of\nbinary choice (yes / no).\nThe agent class constructor is parameterised\nby a list of components. The components of agent\nhave to implement the following functions:\n1. .state()\u2014returns the state of the component\n\ud835\udc67\ud835\udc56, for example \"Alice is vegetarian\";\n2. .name()\u2014returns the name of the compo-\nnents, for example \"dietary preferences\";\n3. .update()\u2014updates the state of the compo-\nnent by implementing; eq. (2). Optional, can\npass for constant constructs;\n4. .observe(observation: str)\u2014takes in an obser-\nvation, for later use during update. Optional.\nObservations always go into the memory any-\nway, but some components are easier to im-\nplement by directly subscribing to the obser-\nvation stream.\nDuring an episode , on each timestep, each\nagent calls .state() on all its components to con-\nstruct the context of its next decision and imple-\nments eq. (1) (the components\u2019 states are con-\ncatenated in the order supplied to the agents\u2019 con-\nstructor). .observe() is called on each component\nwhenever it receives observations, and .update()\nis called at regular intervals (configurable in the\nconstructor). Unlike in RL, we do not assume\nthat the agent will produce an action after every\nobservation. Here the GM might call .observe()\nseveral times before it calls .act().\nA.2. Game master implementation\nThe GM class implements three methods:\n1. .name()\u2014returns the name of the GM;\n2. .update_from_player(player_name, action)\u2014\nthis method consumes players action and\ncreates an event statement;\n3. .run_episode\u2014Runs a single episode of the\nsimulation.\n23\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nA.3. GM components\nGame Master components implement the follow-\ning methods:\n1. .name()\u2014returns the name of the compo-\nnents, for example \"location of players\";\n2. .state()\u2014returns the state of the component\n\ud835\udc67\ud835\udc56, for example \"Alice is at the pub; Bob is at\nthe gas station\";\n3. .partial_state(player_name)\u2014state of the\ncomponent to expose to the player. For exam-\nple, location component would only expose\nthe location of the player to themselves, but\nnot the location of others.\n4. .update()\u2014updates the state of the compo-\nnent by implementing; eq. (2);\n5. .update_before_event(cause_statement)\u2014\nupdate the component state before the\nevent statement from the cause, which is\nthe players action i.e. \"Bob calls Alice.\";\n6. .update_after_event(event_statement)\u2014\nupdate the component state directly from\nthe event statement.\nFor example \"Bob\ncalled Alice, but she didn\u2019t respond.\";\n7. terminate_episode()\u2014if component returns\ntrue, the GM will terminate the episode.\nOne step of environment consists of GMs inter-\nactions with each player, which are arranged in a\n(random) initiative order. The GM advances the\nclock either after each or all the players make take\ntheir actions9. To process the players action, the\nGM calls the components functions in the follow-\ning order. First, for each component the GM calls\n.update, then .partial_state and sends the output\nto the agent as an observation. The GM then calls\n.act on the player and receives the attempted ac-\ntion and uses it to call .update_before_event. Now\nGM can construct its context by calling .state on\nthe components. GM then executes the chain of\nthought to create the event statement. After that\nit calls .update_after_event on all components. As\nthe last step, GM calls terminate_episode and if\nany of the components returns True, the episode\nis terminated.\nIn Concordia all custom functionality is im-\nplemented through components. For grounded\n9Controlled by a flag in the GM constructor.\nvariables, which are tracked in Python, a spe-\ncialised component is created to maintain the\nvariable\u2019s state, update it after relevant events,\nand represent it to the GM in linguistic form \ud835\udc67\ud835\udc56.\nSimilarly, components can send observations to\nplayers. For example, a component during the\n.update_after_event call might check if the event\nwas observed by, or has effect on, other players\napart from the acting player. Some components,\nlike player status and location, send an observa-\ntion to the player before it is their turn to act by\nimplementing .partial_state.\nGM components can also be built around clas-\nsical (non LLM) modelling tools like differential\nequations, finite state machines and so on. The\nonly requirement is that they can represent their\nstate in language. We can also wire different clas-\nsic simulators together using natural language as\nthe \u2018glue\u2019.\nA.3.1. Turn taking and simultanious action\nGM in Concordia support two types of turn taking.\nIn the first, agents act one after another and game\nclock is advanced between their turns. In the sec-\nond mode, at each step all players take a turn\n\u2019quasisimultaneously\u2019 with regard to the main\ngame clock, but still in a specific order within\nthe timestep. This is the same principle as ini-\ntiative order in dungeons and dragons. There is\nan option to execute player turns concurrently\n(concurrent_action flag), but it often leads to in-\nconsistencies, although greatly speeds up the sim-\nulation. Use at your own risk.\nA.4. Nested games\nNatural language is one of the most powerful\nmodelling tools, as it allows to switch between\nlevels of abstraction. Concordia allows creation of\nnested game structures, where a GM\u2019s component\ncan spin out a new GM and pass over control to\nit for a certain period of time and then get it back\nwhen the new GM terminates the episode. Having\nnested structure of games allows us to leverage\nthat property of language and perform modelling\nat different levels of abstraction. For example,\nimagine we would like to model a simulation of a\nfishing village, where we would generally like to\n24\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nmodel the fishing process itself with more details\nthan the rest of the social life of a village. We\nwould then make the main GM with a clock step\nof 1 hour and implement a component \"Fishing\",\nwhich would check if agent is fishing as part of its\nactivity and if yes, would create a GM with faster\nclock. This GM would implement the details of\nthe fishing process, play out the episode with\nthe required agents and then return the set of its\nmemories to the parent GM.\nThe conversation component in the provided\nexamples implements a conversation between\nagents (and potential NPCs) using this technique.\nA.5. Concurrency\nThe performance bottleneck of the library is wait-\ning on the LLM API calls. To improve the wall\ntime efficiency, we use concurrency during up-\ndate calls to components. In this way, while one\nof the components is waiting for the LLM infer-\nence, other components can keep updating. This\nmeans that the sequence at which the components\nare updated is not guaranteed. If you would like\nto update the components sequentially, you can\nuse concordia/generic_components/sequential.py\nwrapper, which wraps a set of components into\none and updates them sequentially.\nA.6. Sampling initial memories and backsto-\nries\nTo generate the initial memories of the agents\nwe use the following step-wise generative pro-\ncess. We first generate a backstory by condition\non a set of biographical facts (age, gender), ran-\ndomised traits (defined by user, for example big\nfive Nettle (2007)), and some simulation specific\ncontext. We then use that backstory to condi-\ntion an LLM to generate a sequence of formative\nmemories at different ages. These memories then\ninitialise the agent. In this way we can obtain\ndiversity in the agents. Notice that all the of the\ninitial conditions are simply strings and can be\neasily adjusted by the experimenter. For example,\ntraits can be derived phsycometrically valid or\ncommon sense descriptions\u2014e.g. \"very rude\" or\n\"slightly irritable\". Validating that the resulting\nagents indeed exhibit those traits is part of the\nfuture work and has not been addressed yet. We\nintend to build on Safdari et al. (2023), which\nhave found out that personality measurements in\nthe outputs of some LLMs under specific prompt-\ning configurations are reliable and valid.\nA.7. Digital Activity Simulation\nA.7.1. Creating Phone Apps\nIn Concordia, phone apps are implemented by\nsubclassing the PhoneApp class and decorating\ncallable actions with @ app_action. Concordia\nis then able to automatically generate natural\nEnglish descriptions of the app and its supported\nactions using the class and methods\u2019 docstring\nand annotated types. PhoneApps are free to run\nany Python code and connect to external services.\nFor example, an implementation of a toy calendar\napp might look like this:\nc l a s s\nCalendarApp (PhoneApp ) :\ndef name ( ) :\nreturn\n\"My Calendar \"\ndef\ndescription ( ) :\nreturn\n\" This\ni s\na calendar app \"\n@app_method\ndef add_meeting ( p a r t i c i p a n t :\ns t r ) :\n\" \" \" Adds a meeting \" \" \"\ns e l f . _meeting . append ( . . . )\nA.7.2. Phone\nThe phone class is initialized for every player and\ncontains the PhoneApps the player can access.\nPhoneApp instances are singletons and are shared\nbetween players\u2019 phones.\nA.7.3. Triggering the nested PhoneGameMas-\nter\nTo detect that a player\u2019s action involved the phone\na should run the the nested phone game, we\nadd the SceneTriggeringComponent to the main\nGM. This component examines every event gener-\nated by the GM and when it detects an event\nthat requires phone interaction, it spawns a\n25\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nPhoneGameMaster and provides it with the inter-\nacting user and their corresponding Phone.\nA.8. Examples\nWe provide the following examples with the initial\nrelease of Concordia:\n1. Calendar: An illustrative social simulation\nwith 2 players which simulates phone inter-\nactions. The two players, Alice and Bob, have\na smartphone with a Calendar app. Alice\u2019s\ngoal is to setup a meeting with Bob using\nthe Calendar app on her phone, taking Bob\u2019s\nschedule into account when selecting the\ndate/time.\n2. Riverbend elections: An illustrative social\nsimulation with 5 players which simulates\nthe day of mayoral elections in an imaginary\ntown caller Riverbend. First two players, Al-\nice and Bob, are running for the mayor. The\nthird player, Charlie, is trying to ruin Alice\u2019s\nreputation with disinformation. The last two\nplayers have no specific agenda, apart from\nvoting in the election.\n3. Day in Riverbend: An illustrative social sim-\nulation with 5 players which simulates a nor-\nmal day in an imaginary town caller River-\nbend.\nEach player has their own config-\nurable backstory. The agents are configured\nto re-implement the architecture Park et al.\n(2023)\u2014they have reflection, plan, and iden-\ntity components; their associative memory\nuses importance function. This is not an ex-\nact re-implementation.\n4. March and Olsen (2011) posit that humans\ngenerally act as though they choose their ac-\ntions by answering three key questions (see\nsection 2.1 for details). The agents used in\nthis example implement exactly these com-\nponents, and nothing else. The premise of\nthe simulation is that 4 friends are stuck in\nsnowed in pub. Two of them have a dispute\nover a crashed car.\n5. Magic Beans for sale: An example illustrating\nhow to use the inventory component. Agents\ncan buy and trade beans for money.\n6. Cyberball: An example which simulates so-\ncial exclusion using a GABM version of a stan-\ndard social psychology paradigm (Williams\net al., 2000) and shows how to use standard\npsychology questionnaires.\nReferences\nA. Adadi and M. Berrada. Peeking inside the\nblack-box: a survey on explainable artificial\nintelligence (xai). IEEE access, 6:52138\u201352160,\n2018.\nB. Ag\u00fcera y Arcas. Do large language models\nunderstand us?\nDaedalus, 151(2):183\u2013197,\n2022.\nB. Ag\u00fcera y Arcas and P. Norvig. Artificial general\nintelligence is already here. Noema, 2023.\nG. V. Aher, R. I. Arriaga, and A. T. Kalai. Using\nlarge language models to simulate multiple hu-\nmans and replicate human subject studies. In\nInternational Conference on Machine Learning,\npages 337\u2013371. PMLR, 2023.\nI. Ajzen. The theory of planned behavior. Organi-\nzational behavior and human decision processes,\n50(2):179\u2013211, 1991.\nA. Amirova, T. Fteropoulli, N. Ahmed, M. R.\nCowie, and J. Z. Leibo.\nFramework-based\nqualitative analysis of free responses of large\nlanguage models: Algorithmic fidelity. arXiv\npreprint arXiv:2309.06364, 2023.\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\nZ. Chen, et al. PALM 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023.\nL. P. Argyle, E. C. Busby, N. Fulda, J. R. Gubler,\nC. Rytting, and D. Wingate. Out of one, many:\nUsing language models to simulate human sam-\nples. Political Analysis, 31(3):337\u2013351, 2023.\nM. Atari, M. J. Xue, P. S. Park, D. Blasi, and J. Hen-\nrich. Which humans? 2023.\nR. L. Axtell, J. M. Epstein, and H. P. Young. The\nemergence of classes in a multi-agent bargain-\ning model. Social dynamics, 27:191\u2013211, 2001.\nA. Baddeley.\nWorking memory.\nScience, 255\n(5044):556\u2013559, 1992.\n26\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion,\nA. Jones, A. Chen, A. Goldie, A. Mirhoseini,\nC. McKinnon, et al. Constitutional ai: Harm-\nlessness from ai feedback.\narXiv preprint\narXiv:2212.08073, 2022.\nL. F. Barrett. Are emotions natural kinds? Per-\nspectives on psychological science, 1(1):28\u201358,\n2006.\nL. F. Barrett. The conceptual act theory: A pr\u00e9cis.\nEmotion review, 6(4):292\u2013297, 2014.\nM. G. Bellemare, Y. Naddaf, J. Veness, and\nM. Bowling. The arcade learning environment:\nAn evaluation platform for general agents. Jour-\nnal of Artificial Intelligence Research, 47:253\u2013\n279, 2013.\nA. Birhane, W. Isaac, V. Prabhakaran, M. Diaz,\nM. C. Elish, I. Gabriel, and S. Mohamed. Power\nto the people? opportunities and challenges\nfor participatory AI. Equity and Access in Algo-\nrithms, Mechanisms, and Optimization, pages\n1\u20138, 2022.\nN. Bostrom. Superintelligence: Paths, Dangers,\nStrategies. Oxford University Press, Inc., USA,\n1st edition, 2014. ISBN 0199678111.\nJ. Brand, A. Israeli, and D. Ngwe. Using GPT for\nmarket research. Available at SSRN 4395751,\n2023.\nL. Brinkmann, F. Baumann, J.-F. Bonnefon,\nM. Derex, T. F. M\u00fcller, A.-M. Nussberger,\nA. Czaplicka, A. Acerbi, T. L. Griffiths, J. Hen-\nrich, J. Z. Leibo, R. McElreath, P.-Y. Oudeyer,\nJ. Stray, and I. Rahwan. Machine culture. Na-\nture Human Behaviour, pages 1\u201314, 2023.\nE. Brooks, L. A. Walls, R. Lewis, and S. Singh.\nLarge language models can implement policy\niteration. In Thirty-seventh Conference on Neu-\nral Information Processing Systems, 2023.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D.\nKaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models\nare few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901,\n2020.\nS.\nBubeck,\nV.\nChandrasekaran,\nR.\nEldan,\nJ. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T.\nLee, Y. Li, S. Lundberg, et al. Sparks of artificial\ngeneral intelligence: Early experiments with\ngpt-4. arXiv preprint arXiv:2303.12712, 2023.\nJ. R. Carter and M. D. Irons.\nAre economists\ndifferent, and if so, why? Journal of Economic\nPerspectives, 5(2):171\u2013177, 1991.\nP. Dayan. Goal-directed control and its antipodes.\nNeural Networks, 22(3):213\u2013219, 2009.\nP. Dayan and G. E. Hinton. Feudal reinforcement\nlearning. Advances in neural information pro-\ncessing systems, 5, 1992.\nM. DeLanda. Philosophy and simulation: the emer-\ngence of synthetic reason. Bloomsbury Publish-\ning, 2011.\nM. DeLanda. Assemblage theory. Edinburgh Uni-\nversity Press, 2016.\nD. Dillion, N. Tandon, Y. Gu, and K. Gray. Can AI\nlanguage models replace human participants?\nTrends in Cognitive Sciences, 2023.\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang,\nX. Sun, J. Xu, and Z. Sui. A survey for in-context\nlearning.\narXiv preprint arXiv:2301.00234,\n2022.\nE. A. Du\u00e9\u00f1ez-Guzm\u00e1n, S. Sadedin, J. X. Wang,\nK. R. McKee, and J. Z. Leibo. A social path\nto human-like artificial intelligence. Nature\nMachine Intelligence, pages 1\u20138, 2023.\nR. H. Fazio. Multiple processes by which attitudes\nguide behavior: The mode model as an integra-\ntive framework. In Advances in experimental\nsocial psychology, volume 23, pages 75\u2013109.\nElsevier, 1990.\nR. H. Frank, T. Gilovich, and D. T. Regan. Does\nstudying economics inhibit cooperation? Jour-\nnal of economic perspectives, 7(2):159\u2013171,\n1993.\nB. Gawronski and G. V. Bodenhausen.\nThe\nassociative\u2013propositional evaluation model:\nTheory, evidence, and open questions. Advances\nin experimental social psychology, 44:59\u2013127,\n2011.\n27\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nA. Goldstein, Z. Zada, E. Buchnik, M. Schain,\nA. Price, B. Aubrey, S. A. Nastase, A. Feder,\nD. Emanuel, A. Cohen, A. Jansen, H. Gazula,\nG. Choe, A. Rao, C. Kim, C. Casto, L. Fanda,\nW. Doyle, D. Friedman, P. Dugan, L. Melloni,\nR. Reichart, S. Devore, A. Fliner, L. Hasenfratz,\nO. Levy, A. Hassidim, M. Brenner, Y. Matias,\nK. A. Norman, O. Devinsky, and U. Hasson.\nShared computational principles for language\nprocessing in humans and deep language mod-\nels. Nature neuroscience, 25(3):369\u2013380, 2022.\nA. G. Greenwald and M. R. Banaji. Implicit social\ncognition: attitudes, self-esteem, and stereo-\ntypes. Psychological review, 102(1):4, 1995.\nI. Grossmann, M. Feinberg, D. C. Parker, N. A.\nChristakis, P. E. Tetlock, and W. A. Cunning-\nham. AI and the transformation of social sci-\nence research. Science, 380(6650):1108\u20131109,\n2023.\nG. Gygax and D. Cook. The Dungeon Master Guide,\nNo. 2100, 2nd Edition (Advanced Dungeons and\nDragons). TSR, Inc, 1989. ISBN 0880387297.\nG. K. Hadfield and B. R. Weingast. Law without\nthe state: legal attributes and the coordination\nof decentralized collective punishment. Journal\nof Law and Courts, 1(1):3\u201334, 2013.\nJ. A. Harris, R. Boyd, and B. M. Wood. The role of\ncausal knowledge in the evolution of traditional\ntechnology. Current Biology, 31(8):1798\u20131803,\n2021.\nJ. Henrich. The secret of our success: How cul-\nture is driving human evolution, domesticating\nour species, and making us smarter. princeton\nUniversity press, 2016.\nU. Hertz, R. Koster, M. Janssen, and J. Z. Leibo.\nBeyond the matrix: Experimental approaches\nto studying social-ecological systems. 2023.\nJ. P. Higgins, S. Green, et al. Cochrane handbook\nfor systematic reviews of interventions. 2008.\nS. Hong, X. Zheng, J. Chen, Y. Cheng, C. Zhang,\nZ. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran,\net al. MetaGPT: Meta programming for multi-\nagent collaborative framework. arXiv preprint\narXiv:2308.00352, 2023.\nJ. J. Horton.\nLarge language models as simu-\nlated economic agents: What can we learn from\nhomo silicus? arXiv e-prints, pages arXiv\u20132301,\n2023.\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang,\nP. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, et al. Inner monologue: Embod-\nied reasoning through planning with language\nmodels.\narXiv preprint arXiv:2207.05608,\n2022.\nM. Jaderberg, W. M. Czarnecki, I. Dunning,\nL. Marris, G. Lever, A. G. Castaneda, C. Beat-\ntie, N. C. Rabinowitz, A. S. Morcos, A. Ruder-\nman, N. Sonnerat, T. Green, L. Deason, J. Z.\nLeibo, D. Silver, D. Hassabis, K. Kavukcuoglu,\nand T. Graepel. Human-level performance in\n3D multiplayer games with population-based\nreinforcement learning. Science, 364(6443):\n859\u2013865, 2019.\nM. B. Johanson, E. Hughes, F. Timbers, and J. Z.\nLeibo. Emergent bartering behaviour in multi-\nagent reinforcement learning. arXiv preprint\narXiv:2205.06760, 2022.\nC. G. Jung. The archetypes and the collective un-\nconscious. Routledge, 1959.\nD. Kahneman, S. Frederick, et al. Representa-\ntiveness revisited: Attribute substitution in in-\ntuitive judgment. Heuristics and biases: The\npsychology of intuitive judgment, 49(49-81):74,\n2002.\nZ. Kaiya, M. Naim, J. Kondic, M. Cortes, J. Ge,\nS. Luo, G. R. Yang, and A. Ahn. Lyfe agents:\nGenerative agents for low-cost real-time social\ninteractions. arXiv preprint arXiv:2310.02172,\n2023.\nA. Koestler. The Ghost in the Machine. Hutchinson,\n1967.\nI. Lakatos.\nHistory of science and its rational\nreconstructions. In PSA: Proceedings of the bien-\nnial meeting of the philosophy of science associa-\ntion, volume 1970, pages 91\u2013136. Cambridge\nUniversity Press, 1970.\nJ. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki,\nand T. Graepel.\nMulti-agent reinforcement\n28\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nlearning in sequential social dilemmas. In Pro-\nceedings of the 16th Conference on Autonomous\nAgents and MultiAgent Systems, pages 464\u2013473,\n2017.\nJ. Z. Leibo, E. Hughes, M. Lanctot, and T. Grae-\npel. Autocurricula and the emergence of in-\nnovation from social interaction: A manifesto\nfor multi-agent intelligence research.\narXiv\npreprint arXiv:1903.00742, 2019.\nJ. Z. Leibo, E. A. Due\u00f1ez-Guzman, A. Vezhnevets,\nJ. P. Agapiou, P. Sunehag, R. Koster, J. Matyas,\nC. Beattie, I. Mordatch, and T. Graepel. Scal-\nable evaluation of multi-agent reinforcement\nlearning with Melting Pot. In International Con-\nference on Machine Learning, pages 6187\u20136199.\nPMLR, 2021.\nC. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, and\nJ. Gao. Multimodal foundation models: From\nspecialists to general-purpose assistants. arXiv\npreprint arXiv:2309.10020, 10, 2023a.\nG.\nLi,\nH.\nA.\nA.\nK.\nHammoud,\nH.\nItani,\nD. Khizbullin, and B. Ghanem. CAMEL: Com-\nmunicative agents for \"mind\" exploration of\nlarge language model society. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems, 2023b.\nT. Linzen and M. Baroni. Syntactic structure from\ndeep learning. Annual Review of Linguistics, 7:\n195\u2013212, 2021.\nR. D. Luce and H. Raiffa. Games and decisions:\nIntroduction and critical survey. Courier Corpo-\nration, 1957.\nM. W. Macy and R. Willer. From factors to ac-\ntors: Computational sociology and agent-based\nmodeling. Annual review of sociology, 28(1):\n143\u2013166, 2002.\nJ. G. March and J. P. Olsen. The Logic of Appro-\npriateness. In The Oxford Handbook of Political\nScience. Oxford University Press, 2011. doi: 10.\n1093/oxfordhb/9780199604456.013.0024.\nJ. L. McClelland, F. Hill, M. Rudolph, J. Baldridge,\nand H. Sch\u00fctze. Placing language in an inte-\ngrated understanding system: Next steps to-\nward human-level performance in neural lan-\nguage models.\nProceedings of the National\nAcademy of Sciences, 117(42):25966\u201325974,\n2020.\nM. McLuhan. The medium is the message. In\nCommunication theory, pages 390\u2013402. Rout-\nledge, 2017.\nM. L. Minsky.\nThe Society of Mind.\nSimon &\nSchuster, New York, 1988. ISBN 978-0-671-\n65713-0.\nD. Nettle. Personality: What Makes You the Way\nYou Are. Oxford University Press, 2007. ISBN\n978-0199211432.\nM. A. Olson and R. H. Fazio. Implicit and ex-\nplicit measures of attitudes: The perspective\nof the mode model. In Attitudes, pages 39\u201384.\nPsychology Press, 2008.\nN. Onuf. World of our making: Rules and rule in\nsocial theory and international relations. Rout-\nledge, 1989.\nOpenAI. GPT-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nJ. A. Ouellette and W. Wood. Habit and inten-\ntion in everyday life: The multiple processes by\nwhich past behavior predicts future behavior.\nPsychological bulletin, 124(1):54, 1998.\nJ. S. Park, J. C. O\u2019Brien, C. J. Cai, M. R. Mor-\nris, P. Liang, and M. S. Bernstein. Generative\nagents: Interactive simulacra of human behav-\nior. arXiv preprint arXiv:2304.03442, 2023.\nJ. Perolat, J. Z. Leibo, V. Zambaldi, C. Beattie,\nK. Tuyls, and T. Graepel. A multi-agent re-\ninforcement learning model of common-pool\nresource appropriation. Advances in neural in-\nformation processing systems, 30, 2017.\nA. R. Poteete, M. A. Janssen, and E. Ostrom. Work-\ning together: collective action, the commons, and\nmultiple methods in practice. Princeton Univer-\nsity Press, 2010.\nM. Risse. Political Theory of the Digital Age: Where\nArtificial Intelligence Might Take Us. Cambridge\nUniversity Press, 2023.\nH. M. Roff. Expected utilitarianism. arXiv preprint\narXiv:2008.07321, 2020.\n29\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nM. Roser and M. S. Gazzaniga.\nAutomatic\nbrains\u2014interpretive minds. Current Directions\nin Psychological Science, 13(2):56\u201359, 2004.\nM. Safdari, G. Serapio-Garc\u00eda, C. Crepy, S. Fitz,\nP. Romero, L. Sun, M. Abdulhai, A. Faust, and\nM. Matari\u0107. Personality traits in large language\nmodels.\narXiv preprint arXiv:2307.00184,\n2023.\nS. Santurkar, E. Durmus, F. Ladhak, C. Lee,\nP. Liang, and T. Hashimoto. Whose opinions\ndo language models reflect?\narXiv preprint\narXiv:2303.17548, 2023.\nJ. Scheurer, M. Balesni, and M. Hobbhahn. Large\nlanguage models can strategically deceive their\nusers when put under pressure. arXiv preprint\narXiv:2311.07590, 2023.\nT. Schick, J. Dwivedi-Yu, R. Dess\u00ec, R. Raileanu,\nM. Lomeli, L. Zettlemoyer, N. Cancedda, and\nT. Scialom. Toolformer: Language models can\nteach themselves to use tools. arXiv preprint\narXiv:2302.04761, 2023.\nC. Schill, J. M. Anderies, T. Lindahl, C. Folke,\nS. Polasky, J. C. C\u00e1rdenas, A.-S. Cr\u00e9pin, M. A.\nJanssen, J. Norberg, and M. Schl\u00fcter. A more\ndynamic understanding of human behaviour\nfor the anthropocene. Nature Sustainability, 2\n(12):1075\u20131082, 2019.\nW. Schneider and R. M. Shiffrin. Controlled and\nautomatic human information processing: I.\ndetection, search, and attention. Psychological\nreview, 84(1):1, 1977.\nM. Schrimpf, I. Blank, G. Tuckute, C. Kauf, E. A.\nHosseini, N. Kanwisher, J. Tenenbaum, and\nE. Fedorenko. Artificial neural networks accu-\nrately predict language processing in the brain.\nBioRxiv, pages 2020\u201306, 2020.\nM. Shanahan, K. McDonell, and L. Reynolds. Role\nplay with large language models. Nature, pages\n1\u20136, 2023.\nD. Silver, S. Singh, D. Precup, and R. S. Sutton.\nReward is enough. Artificial Intelligence, 299:\n103535, 2021.\nE. R. Smith. Distributed connectionist models in\nsocial psychology. Social and Personality Psy-\nchology Compass, 3(1):64\u201376, 2009.\nC. H. Song, J. Wu, C. Washington, B. M. Sadler,\nW.-L. Chao, and Y. Su. Llm-planner: Few-shot\ngrounded planning for embodied agents with\nlarge language models. In Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, pages 2998\u20133009, 2023.\nC. R. Sunstein. Social norms and social roles.\nColum. L. Rev., 96:903, 1996.\nC. R. Sunstein. How change happens. MIT Press,\n2019.\nR. S. Sutton, D. Precup, and S. Singh. Between\nmdps and semi-mdps: A framework for tempo-\nral abstraction in reinforcement learning. Arti-\nficial intelligence, 112(1-2):181\u2013211, 1999.\nL. Tesfatsion. Agent-based computational eco-\nnomics: Overview and brief history. Artificial\nIntelligence, Learning and Computation in Eco-\nnomics and Finance, pages 41\u201358, 2023.\nH. Touvron, L. Martin, K. Stone, P. Albert,\nA. Almahairi, Y. Babaei, N. Bashlykov, S. Ba-\ntra, P. Bhargava, S. Bhosale, et al. LLAMA 2:\nOpen foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\nT. Ullman. Large language models fail on triv-\nial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399, 2023.\nValve.\nPortal, 2007.\nURL https://www.\nthinkwithportals.com/.\nA. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess,\nM. Jaderberg, D. Silver, and K. Kavukcuoglu.\nFeudal networks for hierarchical reinforcement\nlearning. In International Conference on Ma-\nchine Learning, pages 3540\u20133549. PMLR, 2017.\nO. Vinyals, I. Babuschkin, W. M. Czarnecki,\nM. Mathieu, A. Dudzik, J. Chung, D. H. Choi,\nR. Powell, T. Ewalds, P. Georgiev, J. Oh,\nD. Horgan, M. Kroiss, I. Danihelka, A. Huang,\nL. Sifre, T. Cai, J. P. Agapiou, M. Jader-\nberg, A. S. Vezhnevets, R. Leblond, T. Pohlen,\nV. Dalibard, D. Budden, Y. Sulsky, J. Molloy,\n30\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nT. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff,\nY. Wu, R. Ring, D. Yogatama, D. Wunsch,\nK. McKinney, O. Smith, T. Schaul, T. Lillicrap,\nK. Kavukcuoglu, D. Hassabis, C. Apps, and\nD. Silver. Grandmaster level in starcraft II us-\ning multi-agent reinforcement learning. Nature,\n575(7782):350\u2013354, 2019.\nA. Walker and M. J. Wooldridge. Understanding\nthe emergence of conventions in multi-agent\nsystems. In ICMAS, volume 95, pages 384\u2013389,\n1995.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia,\nE. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought\nprompting elicits reasoning in large language\nmodels. Advances in Neural Information Pro-\ncessing Systems, 35:24824\u201324837, 2022.\nK. Weick, K. Sutcliffe, and D. Obstfeld. Organiz-\ning and the process of sensemaking. ORGANI-\nZATION SCIENCE, 16:409\u2013421, 07 2005. doi:\n10.1287/orsc.1050.0133.\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin,\nJ. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\nB. Balle, A. Kasirzadeh, et al. Ethical and so-\ncial risks of harm from language models. arXiv\npreprint arXiv:2112.04359, 2021.\nL. Weidinger, M. Rauh, N. Marchal, A. Manzini,\nL. A. Hendricks, J. Mateos-Garcia, S. Bergman,\nJ. Kay, C. Griffin, B. Bariach, I. Gabriel, V. Rieser,\nand W. Isaac.\nSociotechnical safety evalua-\ntion of generative ai systems. arXiv preprint\narXiv:2310.11986, 2023.\nA. Wendt. Anarchy is what states make of it: the\nsocial construction of power politics. Interna-\ntional organization, 46(2):391\u2013425, 1992.\nA. E. Wendt.\nThe agent-structure problem in\ninternational relations theory.\nInternational\norganization, 41(3):335\u2013370, 1987.\nK. D. Williams, C. K. Cheung, and W. Choi. Cy-\nberostracism: effects of being ignored over the\ninternet. Journal of personality and social psy-\nchology, 79(5):748, 2000.\nD. S. Wilson, E. Ostrom, and M. E. Cox. General-\nizing the core design principles for the efficacy\nof groups. Journal of economic behavior & orga-\nnization, 90:S21\u2013S32, 2013.\nT. D. Wilson, S. Lindsey, and T. Y. Schooler. A\nmodel of dual attitudes. Psychological review,\n107(1):101, 2000.\nP. Windrum, G. Fagiolo, and A. Moneta. Empirical\nvalidation of agent-based models: Alternatives\nand prospects. Journal of Artificial Societies and\nSocial Simulation, 10(2):8, 2007.\nB. Workshop, T. L. Scao, A. Fan, C. Akiki,\nE. Pavlick, S. Ili\u0107, D. Hesslow, R. Castagn\u00e9,\nA. S. Luccioni, F. Yvon, et al. BLOOM: A 176b-\nparameter open-access multilingual language\nmodel. arXiv preprint arXiv:2211.05100, 2022.\nY. Wu, Z. Jiang, A. Khan, Y. Fu, L. Ruis, E. Grefen-\nstette, and T. Rockt\u00e4schel. ChatArena: Multi-\nagent language game environments for large\nlanguage models, 2023.\nJ. Yocum, P. Christoffersen, M. Damani, J. Sveg-\nliato, D. Hadfield-Menell, and S. Russell. Mit-\nigating generative agent social dilemmas. In\nNeurIPS 2023 Foundation Models for Decision\nMaking Workshop, 2023.\nZ. Zada, A. Goldstein, S. Michelmann, E. Simony,\nA. Price, L. Hasenfratz, E. Barham, A. Zadbood,\nW. Doyle, D. Friedman, et al. A shared linguis-\ntic space for transmitting our thoughts from\nbrain to brain in natural conversations. bioRxiv,\n2023.\nZ. Zhao, W. S. Lee, and D. Hsu.\nLarge lan-\nguage models as commonsense knowledge\nfor large-scale task planning. arXiv preprint\narXiv:2305.14078, 2023.\nS. Zheng, A. Trott, S. Srinivasa, D. C. Parkes, and\nR. Socher. The AI economist: Taxation pol-\nicy design via two-level deep multiagent rein-\nforcement learning. Science advances, 8(18):\neabk2607, 2022.\nX. Zhou, H. Zhu, L. Mathur, R. Zhang, H. Yu,\nZ. Qi, L.-P. Morency, Y. Bisk, D. Fried, G. Neubig,\net al. SOTOPIA: Interactive evaluation for social\nintelligence in language agents. arXiv preprint\narXiv:2310.11667, 2023.\n31\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\nE. W. Zuckerman. Construction, concentration,\nand (dis) continuities in social valuations. An-\nnual Review of Sociology, 38:223\u2013245, 2012.\n32\n"
  },
  {
    "title": "Self-conditioned Image Generation via Generating Representations",
    "link": "https://arxiv.org/pdf/2312.03701.pdf",
    "upvote": "6",
    "text": "Return of Unconditional Generation:\nA Self-supervised Representation Generation Method\nTianhong Li\nDina Katabi\nKaiming He\u22c6\nMIT CSAIL\nAbstract. Unconditional generation\u2014the problem of modeling data dis-\ntribution without relying on human-annotated labels\u2014is a long-standing\nand fundamental challenge in generative models, creating a potential of\nlearning from large-scale unlabeled data. In the literature, the generation\nquality of an unconditional method has been much worse than that of\nits conditional counterpart. This gap can be attributed to the lack of\nsemantic information provided by labels. In this work, we show that one\ncan close this gap by generating semantic representations in the repre-\nsentation space produced by a self-supervised encoder. These represen-\ntations can be used to condition the image generator. This framework,\ncalled Representation-Conditioned Generation (RCG), provides an ef-\nfective solution to the unconditional generation problem without using\nlabels. Through comprehensive experiments, we observe that RCG sig-\nnificantly improves unconditional generation quality: e.g., it achieves a\nnew state-of-the-art FID of 2.15 on ImageNet 256\u00d7256, largely reducing\nthe previous best of 5.91 by a relative 64%. Our unconditional results are\nsituated in the same tier as the leading class-conditional ones. We hope\nthese encouraging observations will attract the community\u2019s attention to\nthe fundamental problem of unconditional generation. Code is available\nat https://github.com/LTH14/rcg.\n1\nIntroduction\nGenerative models have been long developed as unsupervised learning methods\nin the history, e.g., in the seminal works including GAN [27], VAE [39], and diffu-\nsion models [58]. These fundamental methods focus on learning the probabilistic\ndistributions of data, without relying on the availability of human annotations.\nThis problem, often categorized as unconditional generation in today\u2019s commu-\nnity, is in pursuit of utilizing the vast abundance of unannotated data to learn\ncomplex distributions.\nHowever, unconditional image generation has been largely stagnant in com-\nparison with its conditional counterpart. Recent research [10,11,18,24,51,55] has\ndemonstrated compelling image generation quality when conditioned on class\nlabels or text descriptions provided by humans, but its quality degrades sig-\nnificantly without these conditions. Closing the gap between unconditional and\nconditional generation is a challenging and scientifically valuable problem: it is\na necessary step towards unleashing the power of large-scale unannotated data,\nwhich is a common goal in today\u2019s deep learning community.\n\u22c6 Work partially done at FAIR, Meta.\narXiv:2312.03701v3  [cs.CV]  13 Mar 2024\n2\nTianhong Li, Dina Katabi, Kaiming He\nImage Distribution\n(a) Distribution Mapping\n(b) Representation Generation\n(c) Image Generation\nImage Distribution\nNoise Distribution\nRepresentation Distribution\nPre-trained\nEncoder\nImage\nGenerator\nCondition\nRepresentation\nGenerator\nNoise Distribution\nRepresentation Distribution\nRepresentation Distribution\nFig. 1: The Representation-Conditioned Generation (RCG) framework for\nunconditional generation. RCG consists of three parts: (a) it uses a pre-trained self-\nsupervised encoder to map the image distribution to a representation distribution; (b)\nit learns a representation generator that samples from a noise distribution and gener-\nates a representation subject to the representation distribution; (c) it learns an image\ngenerator (e.g., which can be ADM [18], DiT [51], or MAGE [41]) that maps a noise\ndistribution to the image distribution conditioned on the representation distribution.\nWe hypothesize that such a performance gap is because human-annotated\nconditioning introduces rich semantic information to simplify the generative pro-\ncess. In this work, we largely close this gap by taking inspiration from a closely\nrelated area\u2014unsupervised/self-supervised learning.1 We observe that the repre-\nsentations produced by a strong self-supervised encoder (e.g., [8,12,15,30]) can\nalso capture a lot of semantic attributes without human supervision, as reflected\nby their transfer learning performance in the literature. These self-supervised\nrepresentations can serve as a form of conditioning without violating the unsu-\npervised nature of unconditional generation, creating an opportunity to get rid\nof the heavy reliance on human-annotated labels.\nBased on this observation, we propose to first unconditionally generate a\nself-supervised representation and then condition on this representation to gen-\nerate the images. As a preprocessing step (Figure 1a), we use a pre-trained\n1 In this paper, the term of \u201cunsupervised learning\u201d implies \u201cnot using human super-\nvision\u201d. Thus, we view self-supervised learning as a form of unsupervised learning.\nThe distinction between these two terminologies is beyond the scope of this work.\nReturn of Unconditional Generation\n3\nLDM-8\nADM\nDiT-XL/2\nMAGE-L\n0\n5\n10\n15\n20\n25\n30\n35\n40\nUnconditional Generation FID\n39.13\n11.30\n26.21\n6.24\n27.32\n4.89\n7.04\n3.44\nbaseline\nw/ RCG\nFig. 2: Unconditional Image Generation can be largely improved by our RCG\nframework. Regardless of the specific form of the image generator (LDM [55], ADM\n[18], DiT [51], or MAGE [41]), RCG massively improves the unconditional generation\nquality. Generation quality is measured by FID on ImageNet with a 256\u00d7256 resolution.\nAll comparisons between models without and with RCG are conducted under controlled\nconditions to ensure fairness. The technical details and more metrics are in Section 4.1.\nself-supervised encoder (e.g., MoCo [15]) to map the image distribution into the\ncorresponding representation distribution. In this representation space, we train\na representation generator without any conditioning (Figure 1b). As this space is\nlow-dimensional and compact [65], learning the representation distribution is fa-\nvorably feasible with unconditional generation. In practice, we implement it as a\nvery lightweight diffusion model. Given this representation space, we train a sec-\nond generator that is conditioned on these representations and produces images\n(Figure 1c). This image generator can conceptually be any image generation\nmodel. The overall framework, called Representation-Conditioned Generation\n(RCG), provides a new paradigm for unconditional generation.2\nRCG is conceptually simple, flexible, yet highly effective for unconditional\ngeneration. RCG greatly improves unconditional generation quality regardless of\nthe specific choice of the image generator (Figure 2), reducing FID by 71%, 76%,\n82%, and 51% for LDM-8, ADM, DiT-XL/2, and MAGE-L, repectively. This\nindicates that RCG largely reduces the reliance of current generative models on\nmanual labels. On the challenging ImageNet 256\u00d7256 benchmark, RCG achieves\nan unprecedented 2.15 FID for unconditional generation. This performance not\nonly largely outperforms previous unconditional methods, but more surprisingly,\ncan catch up with the strong leading methods that are conditional on class labels.\nWe hope our method and encouraging results will rekindle the community\u2019s\ninterest in the fundamental problem of unconditional generation.\n2\nRelated Work\nGenerative Models. Generative models aim at accurately modeling data dis-\ntribution to generate new data point that resembles the original data. One\n2 The term \u201cunconditional generation\u201d implies \u201cnot conditioned on human labels\u201d. As\nsuch, RCG is an unconditional generation solution.\n4\nTianhong Li, Dina Katabi, Kaiming He\nstream of generative models is built on top of generative adversarial networks\n(GANs) [7, 27, 37, 69, 70]. Another stream is based on a two-stage scheme [10,\n11, 40, 41, 50, 54, 67]: first tokenize the image into a latent space and then ap-\nply maximum likelihood estimation and sampling in the token space. Diffusion\nmodels [18,32,53,55,60] have also achieved superior results on image synthesis.\nThe design of a generative model is mostly orthogonal to how it is condi-\ntioned. However, literature has shown that unconditional generation often sig-\nnificantly lags behind conditional generation under the same design [11,18,41],\nespecially on complex data distributions.\nUnconditional Generation. Unconditional generation is the fundamental prob-\nlem in the realm of generative models. It aims to model the data distribu-\ntion without relying on human annotations, highlighted by seminal works of\nGAN [27], VAE [39], and diffusion models [58]. It has demonstrated impressive\nperformance in modeling simple image distributions such as scenes or human\nfaces [11,18,23,55], and has also been successful in applications beyond images\nwhere human annotation is challenging or impossible, such as novel molecu-\nlar design [26,28,66], medical image synthesis [16,47,71], and audio generation\n[25,42,48]. However, recent research in this domain has been limited, largely due\nto the notable gap between conditional and unconditional generation capabilities\nof recent generative models on complex data distributions [3,18,19,41,46,62].\nPrior efforts to narrow this gap mainly group images into clusters in the rep-\nresentation space and use the cluster indices as underlying class labels to provide\nconditioning [3, 35, 43, 46]. However, these methods assume that the dataset is\nclusterable, and the optimal number of clusters is close to the number of classes.\nAdditionally, these methods fall short of generating diverse representations\u2014\nthey are unable to produce different representations within the same cluster or\nunderlying class.\nRepresentations for Image Generation. Prior works have explored the ef-\nfectiveness of exploiting representations for image generation. DALL-E 2 [53], a\ntext-conditional image generation model, first converts text prompts into image\nembeddings, and then uses these embeddings as the conditions to generate im-\nages. In contrast, RCG for the first time demonstrates the possibility of directly\ngenerating image representations from scratch, a necessary step to enable con-\nditioning on self-supervised representations in unconditional image generation.\nAnother work, DiffAE [52], trains an image encoder in an end-to-end manner\nwith a diffusion model as decoder, aiming to learn a meaningful and decodable\nimage representation. However, its semantic representation ability is still lim-\nited (e.g., compared to self-supervised models like MoCo [15], DINO [8]), which\nlargely hinders its performance in unconditional generation. Another relevant\nline of work is retrieval-augmented generative models [4,5,9], where images are\ngenerated based on representations extracted from existing images. Such semi-\nparametric methods heavily rely on ground-truth images to provide representa-\ntions during generation, a requirement that is impractical in many generative\napplications.\nReturn of Unconditional Generation\n5\nSSL\nEncoder\nRepresentation\nReconstructed \nImage\nDenoised \nRepresentation\nCondition\nSSL\nEncoder\nRepresentation\nRDM\nfc block\nfc block\nMAGE\n\u2026\nViT \nEncoder\nViT \nDecoder\nVQGAN\nTokenizer\nVQGAN\nDetokenizer\nMasking\n(b) Image Generator\n(a) Representation Generator\nNoisy\nRepresentation\nFig. 3: RCG\u2019s training framework. The pre-trained self-supervised image encoder\nextracts representations from images and is fixed during training. To train the repre-\nsentation generator, we add standard Gaussian noise to the representations and ask\nthe network to denoise them. To train the MAGE image generator, we add random\nmasking to the tokenized image and ask the network to reconstruct the missing tokens\nconditioned on the representation extracted from the same image.\n3\nMethod\nDirectly modeling a complex high-dimensional image distribution is a challenging\ntask. RCG decomposes it into two much simpler sub-tasks: first modeling the\ndistribution of a compact low-dimensional representation, and then modeling\nthe image distribution conditioned on this representation distribution. Figure 1\nillustrates the idea. Next, we describe RCG and its extensions in detail.\n3.1\nThe RCG Framework\nRCG comprises three key components: a pre-trained self-supervised image en-\ncoder, a representation generator, and an image generator. Each component\u2019s\ndesign is elaborated below:\nDistribution Mapping. RCG employs an off-the-shelf image encoder to con-\nvert the image distribution to a representation distribution. This image encoder\nhas been pre-trained using self-supervised contrastive learning methods, such as\nMoCo v3 [15], on ImageNet. This approach regularizes the representations on\na hyper-sphere while achieving state-of-the-art performance in representation\nlearning. The resulting distribution is characterized by two essential properties:\nit is simple enough to be modeled effectively by an unconditional representa-\ntion generator, and it is rich in high-level semantic content, which is crucial for\nguiding image generation. These attributes are vital for the effectiveness of the\nfollowing two components.\n6\nTianhong Li, Dina Katabi, Kaiming He\nLinear\nProjection\nNoisy Representation\nLayer\nTimestep Embed\n+\nLayer\nfc block x N\nLayer\n+\nDenoised Representation\nLayer\nFig. 4: Representation genera-\ntor\u2019s\nbackbone\narchitecture.\nEach \u201cLayer\u201d consists of a Lay-\nerNorm layer [1], a SiLU layer\n[22], and a linear layer. The back-\nbone consists of an input layer\nthat projects the representation to\nhidden dimension C, followed by\nN fully-connected (fc) blocks, and\nan output layer that projects the\nhidden latent back to the original\nrepresentation dimension. The dif-\nfusion timestep is embedded and\nadded to every fc block.\nRepresentation Generation. In this stage, we want to generate abstract, un-\nstructured representations without conditions. To address this issue, we develop\na diffusion model for unconditional representation generation, which we call a\nrepresentation diffusion model (RDM). RDM employs a fully-connected network\nwith multiple fully-connected residual blocks as its backbone (Figure 4). Each\nblock consists of an input layer, a timestep embedding projection layer, and an\noutput layer, where each layer consists of a LayerNorm [1], a SiLU [22], and a\nlinear layer. Such an architecture is simply controlled by two parameters: the\nnumber of blocks, N, and the hidden dimension, C.\nRDM follows DDIM [59] for training and inference. As shown in Figure 3a,\nduring training, image representation z0 is mixed with standard Gaussian noise\nvariable \u03f5: zt = \u221a\u03b1tz0+\u221a1 \u2212 \u03b1t\u03f5. The RDM backbone is then trained to denoise\nzt back to z0. During inference, RDM generates representations from Gaussian\nnoise following the DDIM sampling process [59]. Since RDM operates on highly\ncompacted representations, it brings marginal computation overheads for both\ntraining and generation (Appendix B), while providing substantial semantic in-\nformation for the image generator, introduced next.\nImage Generation. The image generator in RCG crafts images conditioned\non self-supervised representations. Conceptually, such an image generator can\nbe any modern conditional image generative model by substituting its original\nconditioning (e.g., class label or text) with representations. In Figure 3b, we take\nMAGE [41], a parallel decoding generative model as an example. The image gen-\nerator is trained to reconstruct the original image from a masked version of the\nimage, conditioned on the representation of the same image. During inference,\nthe image generator generates images from a fully masked image, conditioned\non the representation generated by the representation generator.\nWe experiment with four representative generative models: ADM [18], LDM\n[55], and DiT [51] are diffusion-based frameworks, and MAGE [41] is a paral-\nlel decoding framework. Our experiments show that all four generative models\nachieve much better performance when conditioned on high-level self-supervised\nrepresentations (Table 1).\nReturn of Unconditional Generation\n7\n3.2\nExtensions\nOur RCG framework can easily be extended to support guidance even in the\nabsence of labels, and to support conditional generation when desired. We in-\ntroduce these extensions as follows.\nEnabling Guidance in Unconditional Generation. In class-conditional\ngeneration, the presence of labels allows not only for class conditioning but can\nalso provides additional \u201cguidance\u201d in the generative process. This mechanism\nis often implemented through classifier-free guidance in class-conditional gener-\nation methods [10,34,51,55]. In RCG, the representation-conditioning behavior\nenables us to also benefit from such guidance, even in the absence of labels.\nSpecifically, RCG follows [10,34] to incorporate guidance into its MAGE gen-\nerator. During training, the MAGE generator is trained with a 10% probability\nof not being conditioned on image representations, analogous to [34] which has\na 10% probability of not being conditioned on labels. For each inference step,\nthe MAGE generator produces a representation-conditioned logit, lc, and an un-\nconditional logit, lu, for each masked token. The final logits, lg, are calculated\nby adjusting lc away from lu by the guidance scale, \u03c4: lg = lc + \u03c4(lc \u2212 lu). The\nMAGE generator then uses lg to sample the remaining masked tokens. Addi-\ntional implementation details of RCG\u2019s guidance are provided in Appendix A.\nSimple Extension to Class-conditional Generation. RCG seamlessly en-\nables conditional image generation by training a task-specific conditional RDM.\nSpecifically, a class embedding is integrated into each fully-connected block of\nthe RDM, in addition to the timestep embedding. This enables the generation\nof class-specific representations. The image generator then crafts the image con-\nditioned on the generated representation. As shown in Table 3 and Appendix C,\nthis simple modification allows users to specify the class of the generated image\nwhile keeping RCG\u2019s superior generative performance, all without the need to\nretrain the image generator.\n4\nExperiments\nWe evaluate RCG on the ImageNet 256\u00d7256 dataset [17], which is a common\nbenchmark for image generation and is especially challenging for unconditional\ngeneration. Unless otherwise specified, we do not use ImageNet labels in any\nof the experiments. We generate 50K images and report the Frechet Inception\nDistance (FID) [31] and Inception Score (IS) [56] as the standard metrics for\nassessing the fidelity and diversity of the generated images. Evaluations of preci-\nsion and recall are included in Appendix B. Unless otherwise specified, we follow\nthe evaluation suite provided by ADM [18]. Further details are in Appendix A.\n4.1\nObservations\nWe extensively evaluate the performance of RCG with various image generators\nand compare it to the results of state-of-the-art unconditional and conditional\nimage generation methods. Several intriguing properties are observed.\n8\nTianhong Li, Dina Katabi, Kaiming He\nTable 1: RCG significantly improves the unconditional generation perfor-\nmance of current generative models, evaluated on ImageNet 256\u00d7256. All num-\nbers are reported under the unconditional generation setting.\nUnconditional generation\nFID\u2193\nIS\u2191\nLDM-8 [55]\nbaseline\n39.13\n22.8\nw/ RCG\n11.30 (\u221227.83) 101.9 (+79.1)\nADM [18]\nbaseline\n26.21\n39.7\nw/ RCG\n6.24 (\u221219.97) 136.9 (+97.2)\nDiT-XL/2 [51] baseline\n27.32\n35.9\nw/ RCG\n4.89 (\u221222.43) 143.2 (+107.3)\nMAGE-B [41] baseline\n8.67\n94.8\nw/ RCG\n3.98 (\u22124.69)\n177.8 (+83.0)\nMAGE-L [41]\nbaseline\n7.04\n123.5\nw/ RCG\n3.44 (\u22123.60)\n186.9 (+63.4)\n0\n2\n4\n6\n8\n10\n12\n14\nTraining Cost (days)\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nUnconditional Generation FID\nLDM-8\nADM\nDiT-L/2\nDiT-XL/2\nMAGE-B\nMAGE-L\nMAGE-B w/ RCG\nMAGE-L w/ RCG\nFig. 5: RCG achieves outstand-\ning unconditional generation\nperformance with less train-\ning\ncost. All numbers are re-\nported under the unconditional\ngeneration setting. The training\ncost is measured using a cluster\nof 64 V100 GPUs. Given that\nthe MoCo v3 ViT encoder is pre-\ntrained and not needed for gener-\nation, its training cost is excluded.\nDetailed computational cost is re-\nported in Appendix B.\nRCG significantly improves the unconditional generation performance\nof current generative models. In Table 1, we evaluate the proposed RCG frame-\nwork using different image generators. The results demonstrate that conditioning\non generated representations substantially improves the performance of all im-\nage generators in unconditional generation. Specifically, it reduces the FID for\nunconditional LDM-8, ADM, DiT-XL/2, MAGE-B, and MAGE-L by 71%, 76%,\n82%, 54%, and 51%, respectively. We further show that such improvement is also\nuniversal across different datasets, as demonstrated by the results on CIFAR-\n10 and iNaturalist in Appendix B. These findings confirm that RCG markedly\nboosts the performance of current generative models in unconditional generation,\nsignificantly reducing their reliance on human-annotated labels.\nMoreover, such outstanding performance can be achieved with lower training\ncost compared to current generative models. In Figure 5, we compare the training\ncost and unconditional generation FIDs of RCG and current generative models.\nRCG achieves a significantly lower FID with less training cost than current\ngenerative models. Specifically, MAGE-B with RCG achieves an unconditional\ngeneration FID of 4.87 in less than a day when trained on 64 V100 GPUs. This\ndemonstrates that decomposing the complex tasks of unconditional generation\ninto much simpler sub-tasks can significantly simplify the data modeling process.\nReturn of Unconditional Generation\n9\nTable 2: RCG largely improves the state-of-the-art in unconditional image\ngeneration on ImageNet 256\u00d7256. All numbers are reported under the unconditional\ngeneration setting. Following common practice, we report the number of parameters\nused during generation. \u2020 denotes semi-parametric methods which require ground-truth\nImageNet images during generation.\nUnconditional generation\n#params FID\u2193\nIS\u2191\nBigGAN [19]\n\u223c70M 38.61\n24.7\nADM [18]\n554M 26.21\n39.7\nMaskGIT [11]\n227M 20.72\n42.1\nRCDM\u2020 [5]\n-\n19.0\n51.9\nIC-GAN\u2020 [9]\n\u223c75M 15.6\n59.0\nADDP [62]\n176M\n8.9\n95.3\nMAGE-B [41]\n176M\n8.67\n94.8\nMAGE-L [41]\n439M\n7.04 123.5\nRDM-IN\u2020 [4]\n400M\n5.91 158.8\nRCG (MAGE-B)\n239M\n3.98 177.8\nRCG (MAGE-L)\n502M\n3.44 186.9\nRCG-G (MAGE-B)\n239M\n3.19 212.6\nRCG-G (MAGE-L)\n502M\n2.15 253.4\nRCG largely improves the state-of-the-art in unconditional image gen-\neration. In Table 2, we compare MAGE with RCG and previous state-of-the-art\nmethods in unconditional image generation. As shown in Figure 6 and Table 2,\nRCG can generate images with both high fidelity and diversity, achieving an FID\nof 3.44 and an Inception Score of 186.9. These results are further enhanced with\nthe guided version of RCG (RCG-G), which reaches an FID of 2.15 and an Incep-\ntion Score of 253.4, significantly surpassing previous methods of unconditional\nimage generation.\nRCG\u2019s unconditional generation performance rivals leading methods\nin class-conditional image generation. In Table 3, we perform a system-\nlevel comparison between the unconditional RCG and state-of-the-art class-\nconditional image generation methods. MAGE-L with RCG is comparable to\nleading class-conditional methods, with and without guidance. These results\ndemonstrate that RCG, for the first time, improves the performance of uncondi-\ntional image generation on complex data distributions to the same level as that\nof state-of-the-art class-conditional generation methods, effectively bridging the\nhistorical gap between class-conditional and unconditional generation.\nIn Table 4, we further conduct an apple-to-apple comparison between the\nclass-conditional versions of LDM-8, ADM, and DiT-XL/2 and their uncondi-\ntional counterparts using RCG. Surprisingly, with RCG, these generative models\nconsistently outperform their class-conditional versions by a noticeable margin.\nThis demonstrates that the rich semantic information from the unconditionally\ngenerated representations can guide the generative process even more effectively\nthan class labels.\nAs shown in Table 3 and Appendix C, RCG also supports class-conditional\ngeneration with a simple extension. Our representation diffusion model can easily\nadapt to class-conditional representation generation, thereby enabling RCG to\n10\nTianhong Li, Dina Katabi, Kaiming He\nTable 3: System-level comparison: RCG\u2019s unconditional generation perfor-\nmance rivals leading methods in class-conditional image generation on Ima-\ngeNet 256\u00d7256. Following common practice, we report the number of parameters used\nduring generation. Class-conditional results are marked in gray.\nw/o Guidance\nw/ Guidance\nMethods\n#params\nFID\u2193\nIS\u2191\nFID\u2193\nIS\u2191\nClass-conditional\nADM [18]\n554M\n10.94\n101.0\n4.59 186.7\nLDM-4 [55]\n400M\n10.56\n103.5\n3.60 247.7\nU-ViT-H/2-G [2]\n501M\n-\n-\n2.29 263.9\nDiT-XL/2 [51]\n675M\n9.62\n121.5\n2.27 278.2\nDiffiT [29]\n-\n-\n-\n1.73 276.5\nBigGAN-deep [6]\n160M\n6.95\n198.2\n-\n-\nMaskGIT [11]\n227M\n6.18\n182.1\n-\n-\nMDTv2-XL/2 [24]\n676M\n5.06\n155.6\n1.58 314.7\nCDM [33]\n-\n4.88\n158.7\n-\n-\nMAGVIT-v2 [68]\n307M\n3.65\n200.5\n1.78 319.4\nRIN [36]\n410M\n3.42\n182.0\n-\n-\nVDM++ [38]\n2B\n2.40\n225.3\n2.12 267.7\nRCG, conditional (MAGE-L)\n512M\n2.99\n215.5\n2.25 300.7\nUnconditional\nRCG (MAGE-L)\n502M\n3.44\n186.9\n2.15 253.4\nTable 4: Apple-to-apple comparison: RCG\u2019s unconditional generation out-\nperforms the class-conditional counterparts of current generative models,\nevaluated on ImageNet 256\u00d7256. MAGE does not report its class-conditional genera-\ntion performance. Class-conditional results are marked in gray.\nMethods\nFID\u2193\nIS\u2191\nLDM-8 [55]\nw/ class labels\n17.41\n72.9\nw/ RCG\n11.30\n101.9\nADM [18]\nw/ class labels\n10.94\n101.0\nw/ RCG\n6.24\n136.9\nDiT-XL/2 [51]\nw/ class labels\n9.62\n121.5\nw/ RCG\n4.89\n143.2\nalso adeptly perform class-conditional image generation. This result demon-\nstrates the effectiveness of RCG in leveraging its superior unconditional gen-\neration performance to benefit downstream conditional generation tasks.\nImportantly, such an adaptation does not require retraining the representation-\nconditioned image generator. For any new conditioning, only the lightweight\nrepresentation generator needs to be re-trained. This potentially enables pre-\ntraining of the self-supervised encoder and image generator on large-scale unla-\nbeled datasets, and task-specific training of conditional representation generator\non a small-scale labeled dataset. We believe that this property, similar to self-\nsupervised learning, allows RCG to both benefit from large unlabeled datasets\nand adapt to various downstream generative tasks with minimal overheads. We\nleave the exploration on this direction to future work.\nReturn of Unconditional Generation\n11\nTable 5: Distribution mapping ablation experiments. The default encoder is\nMoCo v3 ViT-B with 256 projection dimension. Default settings are marked in gray .\nMethod\nFID\nIS\nNo condition\n14.23\n57.7\nMoCo v3 [15]\n5.07\n142.5\nDINO [8]\n7.53\n160.8\niBOT [72]\n8.05\n148.7\nDeiT [63] (supervised)\n5.51\n211.7\n(a) Pre-training. RCG achieves good\nperformance with encoders pre-trained\nwith different contrastive learning and\nsupervised learning methods.\nModel\nparams\nlin.\nFID\nIS\nViT-S\n22M\n73.2\n5.77\n120.8\nViT-B\n86M\n76.7\n5.07\n142.5\nViT-L\n304M\n77.6\n5.06\n148.2\n(b) Model size. RCG scales up\nwith\nlarger\npre-trained\nencoders\nwith better linear probing accuracy.\nProjection Dim\nFID\nIS\n32\n9.14\n81.0\n64\n6.09\n119.2\n128\n5.19\n143.3\n256\n5.07\n142.5\n768\n6.10\n112.7\n(c) Projection dimension. The dimen-\nsionality of the image representation is\nimportant in RCG\u2019s performance.\nTable 6: Representation generation ablation experiments. The default RDM\nbackbone is of 12 blocks and 1536 hidden dimensions, trained for 100 epochs, and takes\n250 sampling steps during generation. The representation Frechet Distance (rep FD) is\nevaluated between 50K generated representations and representations extracted from\nthe ImageNet training set by MoCo v3 ViT-B. Default settings are marked in gray .\n#Blocks FID\nIS\nrep FD\n3\n7.53 113.5\n0.71\n6\n5.40 132.9\n0.53\n12\n5.07 142.5\n0.48\n18\n5.20 141.9\n0.50\n24\n5.13 141.5\n0.49\n(a)\nModel\ndepth.\nA\ndeeper RDM can improve\ngeneration performance.\nHidden Dim\nFID\nIS\nrep FD\n256\n12.99\n67.3\n5.98\n512\n9.07\n99.8\n1.19\n1024\n5.35\n132.0\n0.56\n1536\n5.07\n142.5\n0.48\n2048\n5.09\n142.8\n0.48\n(b) Model width. A wider RDM\ncan\nimprove\ngeneration\nperfor-\nmance.\nEpochs\nFID\nIS\nrep FD\n10\n5.94\n124.4\n0.87\n50\n5.21\n138.3\n0.54\n100\n5.07\n142.5\n0.48\n200\n5.07\n145.1\n0.47\n300\n5.05\n144.3\n0.47\n(c) Training epochs. Train-\ning RDM longer improves\ngeneration performance.\n#Steps\nFID\nIS\nrep FD\n20\n5.80\n120.3\n0.87\n50\n5.28\n133.0\n0.55\n100\n5.15\n138.1\n0.48\n250\n5.07\n142.5\n0.48\n500\n5.07\n142.9\n0.49\n(d) Diffusion steps. More\nsampling steps can improve\ngeneration performance.\nTable 7: Image generation ablation experiments. The default image generator is\nMAGE-B trained for 200 epochs. Table 7c evaluates different \u03c4 using MAGE-L with\nRCG trained for 800 epochs and the FID is evaluated following ADM suite. Default\nsettings are marked in gray .\nConditioning\nFID\nIS\nNo condition\n14.23\n57.7\nCluster label\n6.60\n121.9\nClass label\n5.83\n147.3\nGenerated rep.\n5.07\n142.5\nOracle rep.\n4.37\n149.0\n(a) Conditioning. Conditioning\non generated representations im-\nproves over all baselines in FID.\nEpochs\nFID\nIS\n100\n6.03\n127.7\n200\n5.07\n142.5\n400\n4.48\n158.8\n800\n4.15\n172.0\n(b) Training epochs. Longer\ntraining can improve genera-\ntion performance.\n\u03c4\n0.0\n1.0\n3.0\n5.0\n6.0\n7.0\nFID\n3.44\n2.59\n2.29\n2.31\n2.15\n2.31\nIS\n186.9 228.5 251.3 252.7 253.4 252.6\n(c) Classifier-free guidance scale. \u03c4 =\n6 achieves the best FID and IS for RCG-L.\n4.2\nAblations\nThis section provides a comprehensive ablation study of the three core compo-\nnents of RCG. Our default setup uses MoCo v3 ViT-B as the pre-trained image\nencoder, an RDM with a 12-block, 1536-hidden-dimension backbone trained for\n100 epochs, and a MAGE-B image generator trained for 200 epochs. Unless\notherwise specified, all other properties and modules are set to the default set-\ntings during each component\u2019s individual ablation. The FID in this section is\nevaluated against the ImageNet validation set.\nDistribution Mapping. Table 5 ablates the image encoder. Table 5a compares\nimage encoders trained via various self-supervised learning methods (MoCo v3,\nDINO, and iBOT), highlighting their substantial improvements over the uncon-\nditional baseline. Additionally, an encoder trained with DeiT [63] in a supervised\n12\nTianhong Li, Dina Katabi, Kaiming He\nmanner also exhibits impressive performance, indicating RCG\u2019s adaptability to\nboth supervised and self-supervised pre-training approaches.\nTable 5b assesses the impact of model size on the pre-trained encoder. Larger\nmodels with better linear probing accuracy consistently enhance generation per-\nformance, although a smaller ViT-S model still achieves decent results.\nWe further analyze the effect of image representation dimensionality, using\nMoCo v3 ViT-B models trained with different output dimensions from their pro-\njection head. Table 5c shows that neither excessively low nor high-dimensional\nrepresentations are ideal \u2013 too low dimensions lose vital image information, while\ntoo high dimensions pose challenges for the representation generator.\nRepresentation Generation. Table 6 ablates the representation diffusion model\nand its effectiveness in modeling representation distribution. The RDM\u2019s depth\nand width are controlled by the number of fc blocks and hidden dimensions. Ta-\nble 6a and Table 6b ablate these parameters, indicating an optimal balance at 12\nblocks and 1536 hidden dimensions. Further, Table 6c and Table 6d suggest that\nRDM\u2019s performance saturates at 200 training epochs and 250 diffusion steps.\nBesides evaluating FID and IS on generated images, we also assess the Frechet\nDistance (FD) [21] between the generated representations and the ground-truth\nrepresentations. A smaller FD indicates that the distribution of generated rep-\nresentations more closely resembles the ground-truth distribution. Our default\nrepresentation generator achieves a representation FD less than 0.5, demonstrat-\ning its effectiveness in accurately modeling the representation distribution.\nImage Generation. Table 7 ablates RCG\u2019s image generator. Table 7a experi-\nments with MAGE-B under different conditioning. MAGE-B with RCG signif-\nicantly surpasses the unconditional and clustering-based baselines, and further\noutperforms the class-conditional baseline in FID. This shows that representa-\ntions could provide rich semantic information to guide the generative process.\nIt is also quite close to the \u201cupper bound\u201d which is conditioned on oracle rep-\nresentations from ImageNet real images, demonstrating the effectiveness of the\nrepresentation generator in producing realistic representations.\nWe also ablate the training epochs of the image generator and the guid-\nance scale \u03c4, as shown in Table 7b and Table 7c. Training MAGE longer keeps\nimproving the generation performance, and \u03c4 = 6 achieves the best FID and IS.\n4.3\nQualitative Insights\nIn this section, we showcase the visualization results of RCG, providing insights\ninto its superior generative capabilities. Figure 6 illustrates RCG\u2019s unconditional\nimage generation results on ImageNet 256\u00d7256. The model is capable of generat-\ning both diverse and high-quality images without relying on human annotations.\nThe high-level semantic diversity in RCG\u2019s generation stems from its represen-\ntation generator, which models the distribution of representations and samples\nthem with varied semantics. By conditioning on these representations, the com-\nplex data distribution is broken down into simpler, representation-conditioned\nsub-distributions. This decomposition significantly simplifies the task for the\nimage generator, leading to the production of high-quality images.\nReturn of Unconditional Generation\n13\nFig. 6: Unconditional generation results of RCG on ImageNet 256\u00d7256. RCG can\ngenerate realistic images with diverse semantics without human annotations.\nReference Image\nGenerated Images\nFig. 7: RCG can generate images with diverse appearances but similar semantics from\nthe same representation. We extract representations from reference images and, for\neach representation, generate a variety of images from different random seeds.\nBesides high-quality generation, the image generator can also introduce sig-\nnificant low-level diversity in the generative process. Figure 7 illustrates RCG\u2019s\nability to generate diverse images that semantically align with each other, given\nthe same representation from the reference image. The images generated by\nRCG can capture the semantic essence of the reference images while differing\nin specific details. This result highlights RCG\u2019s capability to leverage semantic\ninformation in representations to guide the generative process, without compro-\nmising the diversity that is important in unconditional image generation.\nFigure 8 further showcases RCG\u2019s semantic interpolation ability, demonstrat-\ning that the representation space is semantically smooth. By leveraging RCG\u2019s\ndependency on representations, we can semantically transition between two im-\nages by linearly interpolating their respective representations. The interpolated\nimages remain realistic across varying interpolation rates, and their semantic\n14\nTianhong Li, Dina Katabi, Kaiming He\nbeer glass\nvalley\nteddy bear\nPersian cat\nTibetan mastiff\nwool\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ntench\ngoldfish\nFig. 8: RCG\u2019s results conditioned on interpolated representations from two images. The\nsemantics of the generated images gradually transfer between the two images.\ncontents smoothly transition from one image to another. For example, interpo-\nlating between an image of \u201cTibetan mastiff\u201d and an image of \u201cwool\u201d could gener-\nate a novel image featuring a dog wearing a woolen sweater. This also highlights\nRCG\u2019s potential in manipulating image semantics within a low-dimensional rep-\nresentation space, offering new possibilities to control image generation.\n5\nDiscussion\nComputer vision has entered a new era where learning from extensive, unlabeled\ndatasets is becoming increasingly common. Despite this trend, the training of\nimage generation models still mostly relies on labeled datasets, which could be\nattributed to the large performance gap between conditional and unconditional\nimage generation. Our paper addresses this issue by exploring Representation-\nConditioned Generation, which we propose as a nexus between conditional and\nunconditional image generation. We demonstrate that the long-standing perfor-\nmance gap can be effectively bridged by generating images conditioned on self-\nsupervised representations and leveraging a representation generator to model\nand sample from this representation space. We believe this approach has the po-\ntential to liberate image generation from the constraints of human annotations,\nenabling it to fully harness the vast amounts of unlabeled data and even gener-\nalize to modalities that are beyond the scope of human annotation capabilities.\nAcknowledgements. We thank Huiwen Chang, Saining Xie, Zhuang Liu, Xinlei\nChen, and Mike Rabbat for their discussion and feedback. We also thank Xinlei\nChen for his support on MoCo v3.\nReturn of Unconditional Generation\n15\nReferences\n1. Ba,\nJ.L.,\nKiros,\nJ.R.,\nHinton,\nG.E.:\nLayer\nnormalization.\narXiv\npreprint\narXiv:1607.06450 (2016) 6\n2. Bao, F., Li, C., Cao, Y., Zhu, J.: All are worth words: a vit backbone for score-based\ndiffusion models. In: NeurIPS 2022 Workshop on Score-Based Methods (2022) 10\n3. Bao, F., Li, C., Sun, J., Zhu, J.: Why are conditional generative models better\nthan unconditional ones? arXiv preprint arXiv:2212.00362 (2022) 4\n4. Blattmann, A., Rombach, R., Oktay, K., M\u00fcller, J., Ommer, B.: Retrieval-\naugmented diffusion models. Advances in Neural Information Processing Systems\n35, 15309\u201315324 (2022) 4, 9\n5. Bordes, F., Balestriero, R., Vincent, P.: High fidelity visualization of what your self-\nsupervised representation knows about. arXiv preprint arXiv:2112.09164 (2021) 4,\n9\n6. Brock, A., Donahue, J., Simonyan, K.: Large scale gan training for high fidelity\nnatural image synthesis. arXiv preprint arXiv:1809.11096 (2018) 10\n7. Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high fidelity\nnatural image synthesis. In: Int. Conf. on Learning Representations (ICLR) (2019)\n4\n8. Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin,\nA.: Emerging properties in self-supervised vision transformers. In: Int. Conference\non Computer Vision (ICCV). pp. 9650\u20139660 (2021) 2, 4, 11\n9. Casanova, A., Careil, M., Verbeek, J., Drozdzal, M., Romero Soriano, A.: Instance-\nconditioned gan. Advances in Neural Information Processing Systems 34, 27517\u2013\n27529 (2021) 4, 9\n10. Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang,\nM.H., Murphy, K., Freeman, W.T., Rubinstein, M., et al.: Muse: Text-to-image\ngeneration via masked generative transformers. arXiv preprint arXiv:2301.00704\n(2023) 1, 4, 7, 20, 21\n11. Chang, H., Zhang, H., Jiang, L., Liu, C., Freeman, W.T.: Maskgit: Masked genera-\ntive image transformer. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 11315\u201311325 (2022) 1, 4, 9, 10\n12. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\ntrastive learning of visual representations. arXiv preprint arXiv:2002.05709 (2020)\n2\n13. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\ntrastive learning of visual representations. In: icml. pp. 1597\u20131607. PMLR (2020)\n21\n14. Chen, X., Liu, Z., Xie, S., He, K.: Deconstructing denoising diffusion models for\nself-supervised learning. arXiv preprint arXiv:2401.14404 (2024) 20\n15. Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision\ntransformers. In: Int. Conference on Computer Vision (ICCV). pp. 9640\u20139649\n(2021) 2, 3, 4, 5, 11, 20\n16. Costa, P., Galdran, A., Meyer, M.I., Niemeijer, M., Abr\u00e0moff, M., Mendon\u00e7a, A.M.,\nCampilho, A.: End-to-end adversarial retinal image synthesis. IEEE transactions\non medical imaging 37(3), 781\u2013791 (2017) 4\n17. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-\nscale hierarchical image database. In: 2009 IEEE conference on computer vision\nand pattern recognition. pp. 248\u2013255. Ieee (2009) 7\n16\nTianhong Li, Dina Katabi, Kaiming He\n18. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances\nin Neural Information Processing Systems 34, 8780\u20138794 (2021) 1, 2, 3, 4, 6, 7, 8,\n9, 10, 20, 22, 23\n19. Donahue, J., Simonyan, K.: Large scale adversarial representation learning. Ad-\nvances in neural information processing systems 32 (2019) 4, 9\n20. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth\n16x16 words: Transformers for image recognition at scale. In: Int. Conf. on Learning\nRepresentations (ICLR) (2021) 20\n21. Dowson, D., Landau, B.: The fr\u00e9chet distance between multivariate normal distri-\nbutions. Journal of multivariate analysis 12(3), 450\u2013455 (1982) 12\n22. Elfwing, S., Uchibe, E., Doya, K.: Sigmoid-weighted linear units for neural net-\nwork function approximation in reinforcement learning. Neural networks 107, 3\u201311\n(2018) 6\n23. Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image\nsynthesis. In: Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition. pp. 12873\u201312883 (2021) 4\n24. Gao, S., Zhou, P., Cheng, M.M., Yan, S.: Masked diffusion transformer is a strong\nimage synthesizer. arXiv preprint arXiv:2303.14389 (2023) 1, 10\n25. Goel, K., Gu, A., Donahue, C., R\u00e9, C.: It\u2019s raw! audio generation with state-space\nmodels. In: International Conference on Machine Learning. pp. 7616\u20137633. PMLR\n(2022) 4\n26. G\u00f3mez-Bombarelli, R., Wei, J.N., Duvenaud, D., Hern\u00e1ndez-Lobato, J.M.,\nS\u00e1nchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T.D.,\nAdams, R.P., Aspuru-Guzik, A.: Automatic chemical design using a data-driven\ncontinuous representation of molecules. ACS central science 4(2), 268\u2013276 (2018)\n4\n27. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., Bengio, Y.: Generative adversarial nets (2014) 1, 4\n28. Guimaraes, G.L., Sanchez-Lengeling, B., Outeiral, C., Farias, P.L.C., Aspuru-\nGuzik, A.: Objective-reinforced generative adversarial networks (organ) for se-\nquence generation models. arXiv preprint arXiv:1705.10843 (2017) 4\n29. Hatamizadeh, A., Song, J., Liu, G., Kautz, J., Vahdat, A.: Diffit: Diffusion vision\ntransformers for image generation. arXiv preprint arXiv:2312.02139 (2023) 10\n30. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\nvisual representation learning. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 9729\u20139738 (2020) 2\n31. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained\nby a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems 30 (2017) 7\n32. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: Advances\nin Neural Information Processing Systems (NeurIPS). vol. 33, pp. 6840\u20136851 (2020)\n4\n33. Ho, J., Saharia, C., Chan, W., Fleet, D.J., Norouzi, M., Salimans, T.: Cascaded dif-\nfusion models for high fidelity image generation. The Journal of Machine Learning\nResearch 23(1), 2249\u20132281 (2022) 10\n34. Ho,\nJ.,\nSalimans,\nT.:\nClassifier-free\ndiffusion\nguidance.\narXiv\npreprint\narXiv:2207.12598 (2022) 7\n35. Hu, V.T., Zhang, D.W., Asano, Y.M., Burghouts, G.J., Snoek, C.G.M.: Self-guided\ndiffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR). pp. 18413\u201318422 (June 2023) 4\nReturn of Unconditional Generation\n17\n36. Jabri, A., Fleet, D., Chen, T.: Scalable adaptive computation for iterative genera-\ntion. arXiv preprint arXiv:2212.11972 (2022) 10\n37. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative\nadversarial networks. In: CVPR (2019) 4\n38. Kingma, D.P., Gao, R.: Understanding the diffusion objective as a weighted integral\nof elbos. arXiv preprint arXiv:2303.00848 (2023) 10\n39. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 (2013) 1, 4\n40. Lee, D., Kim, C., Kim, S., Cho, M., Han, W.: Autoregressive image generation\nusing residual quantization. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2022) 4\n41. Li, T., Chang, H., Mishra, S., Zhang, H., Katabi, D., Krishnan, D.: Mage: Masked\ngenerative encoder to unify representation learning and image synthesis. In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion. pp. 2142\u20132152 (2023) 2, 3, 4, 6, 8, 9, 20, 22\n42. Liu, J.Y., Chen, Y.H., Yeh, Y.C., Yang, Y.H.: Unconditional audio generation\nwith generative adversarial networks and cycle regularization. arXiv preprint\narXiv:2005.08526 (2020) 4\n43. Liu, S., Wang, T., Bau, D., Zhu, J.Y., Torralba, A.: Diverse image generation via\nself-conditioned gans. In: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. pp. 14286\u201314295 (2020) 4\n44. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.\narXiv preprint arXiv:1608.03983 (2016) 21\n45. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017) 21\n46. Lu\u010di\u0107, M., Tschannen, M., Ritter, M., Zhai, X., Bachem, O., Gelly, S.: High-fidelity\nimage generation with fewer labels. In: International conference on machine learn-\ning. pp. 4183\u20134192. PMLR (2019) 4\n47. Madani, A., Moradi, M., Karargyris, A., Syeda-Mahmood, T.: Semi-supervised\nlearning with generative adversarial networks for chest x-ray classification with\nability of data domain adaptation. In: 2018 IEEE 15th International symposium\non biomedical imaging (ISBI 2018). pp. 1038\u20131042. IEEE (2018) 4\n48. Mehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S., Sotelo, J., Courville,\nA., Bengio, Y.: Samplernn: An unconditional end-to-end neural audio generation\nmodel. arXiv preprint arXiv:1612.07837 (2016) 4\n49. Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion probabilistic models. In:\nInternational Conference on Machine Learning. pp. 8162\u20138171. PMLR (2021) 21,\n22\n50. van den Oord, A., Vinyals, O., Kavukcuoglu, K.: Neural discrete representation\nlearning. In: Advances in Neural Information Processing Systems (NeurIPS) (2017)\n4\n51. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 4195\u20134205\n(2023) 1, 2, 3, 6, 7, 8, 10, 20, 22\n52. Preechakul, K., Chatthee, N., Wizadwongsa, S., Suwajanakorn, S.: Diffusion au-\ntoencoders: Toward a meaningful and decodable representation. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n10619\u201310629 (2022) 4\n53. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125\n(2022) 4\n18\nTianhong Li, Dina Katabi, Kaiming He\n54. Razavi, A., Van den Oord, A., Vinyals, O.: Generating diverse high-fidelity images\nwith vq-vae-2. Advances in neural information processing systems 32 (2019) 4\n55. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 10684\u201310695 (2022)\n1, 3, 4, 6, 7, 8, 10, 20, 22\n56. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:\nImproved techniques for training gans. Advances in neural information processing\nsystems 29 (2016) 7\n57. Salvator, D.: Nvidia developer blog. https://developer.nvidia.com/blog/getting-\nimmediate-speedups-with-a100-tf32 (2020) 22\n58. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In: International conference\non machine learning. pp. 2256\u20132265. PMLR (2015) 1, 4\n59. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502 (2020) 6\n60. Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-\nbased generative modeling through stochastic differential equations. In: Int. Conf.\non Learning Representations (ICLR) (2021) 4\n61. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep-\ntion architecture for computer vision. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. pp. 2818\u20132826 (2016) 21\n62. Tian, C., Tao, C., Dai, J., Li, H., Li, Z., Lu, L., Wang, X., Li, H., Huang, G., Zhu, X.:\nAddp: Learning general representations for image recognition and generation with\nalternating denoising diffusion process. arXiv preprint arXiv:2306.05423 (2023) 4,\n9\n63. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training\ndata-efficient image transformers & distillation through attention. In: International\nconference on machine learning. pp. 10347\u201310357. PMLR (2021) 11\n64. Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam,\nH., Perona, P., Belongie, S.: The inaturalist species classification and detection\ndataset. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition. pp. 8769\u20138778 (2018) 21\n65. Wang, T., Isola, P.: Understanding contrastive representation learning through\nalignment and uniformity on the hypersphere. In: International Conference on\nMachine Learning. pp. 9929\u20139939. PMLR (2020) 3\n66. Watson, J.L., Juergens, D., Bennett, N.R., Trippe, B.L., Yim, J., Eisenach, H.E.,\nAhern, W., Borst, A.J., Ragotte, R.J., Milles, L.F., et al.: De novo design of protein\nstructure and function with rfdiffusion. Nature 620(7976), 1089\u20131100 (2023) 4\n67. Yu, J., Li, X., Koh, J.Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge,\nJ., Wu, Y.: Vector-quantized image modeling with improved vqgan. arXiv preprint\narXiv:2110.04627 (2021) 4\n68. Yu, L., Lezama, J., Gundavarapu, N.B., Versari, L., Sohn, K., Minnen, D., Cheng,\nY., Gupta, A., Gu, X., Hauptmann, A.G., et al.: Language model beats diffusion\u2013\ntokenizer is key to visual generation. arXiv preprint arXiv:2310.05737 (2023) 10\n69. Zhang, H., Goodfellow, I., Metaxas, D., Odena, A.: Self-attention generative adver-\nsarial networks. In: Int. Conference on Machine Learning (ICML). pp. 7354\u20137363\n(2019) 4\n70. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.: Stack-\nGAN: Text to photo-realistic image synthesis with stacked generative adversarial\nnetworks. In: ICCV (2017) 4\nReturn of Unconditional Generation\n19\n71. Zhang, T., Fu, H., Zhao, Y., Cheng, J., Guo, M., Gu, Z., Yang, B., Xiao, Y.,\nGao, S., Liu, J.: Skrgan: Sketching-rendering unconditional generative adversarial\nnetworks for medical image synthesis. In: Medical Image Computing and Computer\nAssisted Intervention\u2013MICCAI 2019: 22nd International Conference, Shenzhen,\nChina, October 13\u201317, 2019, Proceedings, Part IV 22. pp. 777\u2013785. Springer (2019)\n4\n72. Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong, T.: ibot: Image\nbert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832 (2021) 11\n20\nTianhong Li, Dina Katabi, Kaiming He\nA\nImplementation Details\nIn this section, we describe the implementation details of RCG, including hyper-\nparameters, model architecture, and training paradigm. We also include a copy\nof our code in the supplementary material. Code and pre-trained model weights\nare available at https://github.com/LTH14/rcg.\nImage Encoder. We use Vision Transformers (ViTs) [20] pre-trained with\nMoCo v3 [15] as the default image encoder. We evaluate three ViT variants\n(ViT-S, ViT-B, and ViT-L) in the main paper, each trained on ImageNet for\n300 epochs. We utilize the image representation after the MLP projection head,\nfavoring its adjustable dimensionality. An output dimension of 256 has proven\nthe most effective. The representation of each image is normalized by its own\nmean and variance. Detailed training recipes of our pre-trained image encoder\ncan be found in [15].\nRepresentation Diffusion Model (RDM). Our RDM architecture employs\na backbone of multiple fully connected blocks. We use 12 blocks and maintain\na consistent hidden dimension of 1536 across the network. The timestep t is\ndiscretized into 1000 values, each embedded into a 256-dimensional vector. For\nclass-conditional RDM, we embed each class label into a 512-dimensional vector.\nBoth timestep and class label embeddings are projected to 1536 dimensions using\ndifferent linear projection layers in each block. Detailed hyper-parameters for\nRDM\u2019s training and generation can be found in Table 8.\nImage Generator. We experiment with ADM [18], LDM [55], DiT [51], and\nMAGE [41] as the image generator. For representation-conditioned ADM, LDM\nand DiT, we substitute the original class embedding conditioning with the image\nrepresentation. We follow ADM\u2019s original training recipe to train representation-\nconditioned ADM for 400 epochs. We follow LDM-8\u2019s original training recipe,\nwith modifications including a batch size of 256, a learning rate of 6.4e-5, and\na training duration of 40 epochs. We follow the DiT training scheme in [14],\nwhich trains DiT-XL for 400 epochs with batch size 2048 and a linear learning\nrate warmup for 100 epochs. The \u03b22 of the AdamW optimizer is set to 0.95.\nFor representation-conditioned MAGE, we replace the default \u201cfake\u201d class token\nembedding [C0] with the image representation for conditioning.\nDuring the training of RCG\u2019s image generator, the image is resized so that the\nsmaller side is of length 256, and then randomly flipped and cropped to 256\u00d7256.\nThe input to the SSL encoder is further resized to 224\u00d7224 to be compatible with\nits positional embedding size. Our implementation of guidance follows Muse [10],\nincorporating a linear guidance scale scheduling. Detailed hyper-parameters for\nour representation-conditioned MAGE are provided in Table 9.\nB\nAdditional Quantitative Results\nB.1\nOther Datasets\nIn this section, we evaluate RCG on datasets other than ImageNet to validate its\nconsistent effectiveness across different datasets. We select CIFAR-10 and iNat-\nReturn of Unconditional Generation\n21\nTable 8: RDM implementation details.\nconfig\nvalue\n#blocks\n12\nhidden dimension\n1536\n#params\n63M\noptimizer\nAdamW [45]\nlearning rate\n5.12e-4\nweight decay\n0.01\noptimizer momentum\n\u03b21, \u03b22 = 0.9, 0.999\nbatch size\n512\nlearning rate schedule\nconstant\ntraining epochs\n200\naugmentation\nResize(256)\nRandCrop(256)\nRandomFlip (0.5)\ndiffusion steps\n1000\nnoise schedule\nlinear\nDDIM steps\n250\n\u03b7\n1.0\nTable 9: Repsentation-conditioned MAGE implementation details.\nconfig\nvalue\noptimizer\nAdamW [45]\nbase learning rate\n1.5e-4\nweight decay\n0.05\noptimizer momentum\n\u03b21, \u03b22 = 0.9, 0.95\nbatch size\n4096\nlearning rate schedule\ncosine decay [44]\nwarmup epochs\n10\ntraining epochs\n800\ngradient clip\n3.0\nlabel smoothing [61]\n0.1\ndropout\n0.1\naugmentation\nResize(256)\nRandCrop(256)\nRandomFlip (0.5)\nmasking ratio min\n0.5\nmasking ratio max\n1.0\nmasking ratio mode\n0.75\nmasking ratio std\n0.25\nrep. drop rate\n0.1\nparallel-decoding temperature\n6.0 (B) 11.0 (L)\nparallel-decoding steps\n20\nguidance scale (\u03c4)\n1.0 (B) 6.0 (L)\nguidance scale schedule\nlinear [10]\nuralist 2021 [64]. CIFAR-10 represents a relatively simple and low-dimensional\nimage distribution, and iNaturalist 2021 represents a more complex image dis-\ntribution, with 10,000 classes and 2.7 million images. For CIFAR-10, we em-\nploy SimCLR [13] trained on CIFAR-10 as the image encoder and Improved\nDDPM [49] as the image generator. The FID is evaluated between 50,000 gen-\nerated images and the CIFAR-10 training set. For iNaturalist, we employ MoCo\nv3 ViT-B trained on ImageNet as the image encoder and MAGE-B as the im-\nage generator. The FID is evaluated between 100,000 generated images and the\niNaturalist validation set, which also consists of 100,000 images.\nAs shown in Table 10, RCG consistently enhances unconditional image gen-\neration performance on both CIFAR-10 and iNaturalist 2021, demonstrating\nits universal effectiveness across various datasets. Notably, the improvement on\ncomplex data distributions such as ImageNet and iNaturalist is more significant\n22\nTianhong Li, Dina Katabi, Kaiming He\nTable 10: CIFAR-10 and iNaturalist results. RCG consistently improves uncondi-\ntional image generation performance on different datasets.\nDataset\nMethods\nFID\nCIFAR-10\nImproved DDPM [49]\nbaseline\n3.29\nw/ RCG\n2.62\nw/ class labels\n2.89\niNaturalist 2021\nMAGE-B\nbaseline\n8.64\nw/ RCG\n4.49\nw/ class labels\n4.55\nTable 11: Computational cost. RCG achieves a much smaller FID with similar or less\ncomputational cost as baseline methods. The number of parameters, training cost, and\nthe number of training epochs of the representation generator and the image generator\nare reported separately.\nUnconditional Generation\n#Params (M)\nTraining Cost\n(days)\nEpochs Throughput\n(samples/s)\nFID\nLDM-8 [55]\n395\n1.2\n150\n0.9\n39.13\nADM [18]\n554\n14.3\n400\n0.05\n26.21\nDiT-L [51]\n458\n6.8\n400\n0.3\n30.9\nDiT-XL [51]\n675\n9.1\n400\n0.2\n27.32\nMAGE-B [41]\n176\n5.5\n1600\n3.9\n8.67\nMAGE-L [41]\n439\n10.7\n1600\n2.4\n7.04\nRCG (MAGE-B)\n63+176\n0.1+0.8\n100+200\n3.6\n4.87\nRCG (MAGE-B)\n63+176\n0.2+3.3\n200+800\n3.6\n3.98\nRCG (MAGE-L)\n63+439\n0.3+1.5\n100+200\n2.2\n4.09\nRCG (MAGE-L)\n63+439\n0.6+6.0\n200+800\n2.2\n3.44\nthan on simpler data distributions such as CIFAR-10. This is because RCG de-\ncomposes a complex data distribution into two relatively simpler distributions:\nthe representation distribution and the data distribution conditioned on the rep-\nresentation distribution. Such decomposition is particularly effective on complex\ndata distributions, such as natural images, paving the way for generative models\nto model unlabeled complex data distributions.\nB.2\nComputational Cost\nIn Table 11, we present a detailed evaluation of RCG\u2019s computational cost,\nincluding the number of parameters, training costs, and generation throughput.\nThe training cost of all image generators is measured using a cluster of 64 V100\nGPUs. The training cost of RDM is measured using 1 V100 GPU, divided by\n64. The generation throughput is measured on a single V100 GPU. As LDM and\nADM measure their generation throughput on a single NVIDIA A100 [55], we\nconvert it to V100 throughput by assuming a \u00d72.2 speedup of A100 vs V100 [57].\nAs shown in the Table 11, RCG requires significantly lower training costs to\nachieve great performance. For instance, it achieves an FID of 4.87 in less than\none day of training. Moreover, the training and inference costs of the represen-\ntation generator are marginal compared to those of the image generator. This\nefficiency potentially enables for lightweight adaptation to various downstream\nReturn of Unconditional Generation\n23\nTable 12: RCG\u2019s unconditional generation FID, IS, precision and recall on ImageNet\n256\u00d7256, evaluated following ADM\u2019s suite [18].\nMethods\nFID\u2193\nIS\u2191\nPrec.\u2191 Rec.\u2191\nRCG (MAGE-B)\n3.98 177.8\n0.84\n0.47\nRCG (MAGE-L)\n3.44 186.9\n0.82\n0.52\nRCG-G (MAGE-B)\n3.19 212.6\n0.83\n0.48\nRCG-G (MAGE-L)\n2.15 253.4\n0.81\n0.53\ngenerative tasks by training only the representation generator on small-scale\nlabeled datasets.\nB.3\nPrecision and Recall\nIn Table 12, we report the unconditional generation precision and recall of RCG,\nevaluated on ImageNet 256\u00d7256 following the ADM suite [18]. Larger models\nas well as incorporating guidance (RCG-G) both improve recall while slightly\ndecreases precision.\nC\nAdditional Qualitative Results\nWe include more qualitative results, including class-unconditional image gener-\nation (Figure 9), class-conditional image generation (Figure 10 and Figure 11),\nand the comparison between generation results with or without guidance (Fig-\nure 13). All these results demonstrate RCG\u2019s superior performance in image\ngeneration. We also include some failure cases in Figure 12.\n24\nTianhong Li, Dina Katabi, Kaiming He\nFig. 9: More RCG class-unconditional image generation results on ImageNet 256\u00d7256.\nReturn of Unconditional Generation\n25\nFig. 10: RCG class-conditional image generation results on ImageNet 256\u00d7256. Classes\nare 874: trolleybus, 664: monitor, 249: malamute; 952: fig, 968: cup, 256: Newfoundland;\n789: shoji, 659: mixing bowl, 681: notebook; 119: rock crab, 629: lipstick, 192: cairn;\n359: ferret, 9: ostrich, 277: red fox.\n26\nTianhong Li, Dina Katabi, Kaiming He\nFig. 11: RCG class-conditional image generation results on ImageNet 256\u00d7256. Classes\nare 1: goldfish, 388: panda, 279: Arctic fox; 323: monarch butterfly, 292: tiger, 933:\ncheeseburger; 985: daisy, 979: valley, 992: agaric\nFig. 12: Similar to other generative models on ImageNet, RCG also could face difficulty\nin generating texts, regular shapes (such as keyboard and wheel), and realistic human.\nReturn of Unconditional Generation\n27\nw/o CFG\nw/ CFG\nw/o CFG\nw/ CFG\nw/o CFG\nw/ CFG\nFig. 13: Class-unconditional image generation results on ImageNet 256\u00d7256, with or\nwithout guidance. RCG achieves strong generation performance even without guidance.\nIncorporating guidance further improves the generation quality.\n"
  },
  {
    "title": "HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces",
    "link": "https://arxiv.org/pdf/2312.03160.pdf",
    "upvote": "5",
    "text": "HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces\nHaithem Turki1, 2\nVasu Agrawal1\nSamuel Rota Bul`o1\nLorenzo Porzi1\nPeter Kontschieder1\nDeva Ramanan2\nMichael Zollh\u00a8ofer1\nChristian Richardt1\n1Meta Reality Labs\n2Carnegie Mellon University\nAbstract\nNeural radiance fields provide state-of-the-art view synthesis\nquality but tend to be slow to render. One reason is that they\nmake use of volume rendering, thus requiring many samples\n(and model queries) per ray at render time. Although this\nrepresentation is flexible and easy to optimize, most real-\nworld objects can be modeled more efficiently with surfaces\ninstead of volumes, requiring far fewer samples per ray. This\nobservation has spurred considerable progress in surface\nrepresentations such as signed distance functions, but these\nmay struggle to model semi-opaque and thin structures. We\npropose a method, HybridNeRF, that leverages the strengths\nof both representations by rendering most objects as surfaces\nwhile modeling the (typically) small fraction of challenging\nregions volumetrically. We evaluate HybridNeRF against\nthe challenging Eyeful Tower dataset [35] along with other\ncommonly used view synthesis datasets. When comparing\nto state-of-the-art baselines, including recent rasterization-\nbased approaches, we improve error rates by 15\u201330% while\nachieving real-time framerates (at least 36 FPS) for virtual-\nreality resolutions (2K\u00d72K).\n1. Introduction\nRecent advances in volumetric rendering of neural radiance\nfields [20] (NeRFs) have led to significant progress towards\nphotorealistic novel-view synthesis. However, while NeRFs\nprovide state-of-the-art rendering quality, they remain slow\nto render.\nEfficiency.\nWe seek to construct a representation that en-\nables high-quality efficient rendering, which is necessary for\nimmersive applications, such as augmented reality or virtual\nteleconferencing.\nWhile recent rasterization-based techniques, such as mesh\nbaking [4, 37] or Gaussian splatting [12], are very efficient,\nthey still struggle to capture transparent or fine structures,\nand view-dependent effects (like reflections or specularities),\nrespectively. Instead, we focus on NeRF\u2019s standard ray cast-\ning paradigm, and propose techniques that enable a better\nRGB\nSurfaceness\nNeRF\n(\u224840 samples / ray)\nHybridNeRF\n(\u22488 samples / ray)\nFigure 1. HybridNeRF. We train a hybrid surface\u2013volume repre-\nsentation via surfaceness parameters that allow us to render most of\nthe scene with few samples. We track Eikonal loss as we increase\nsurfaceness to avoid degrading quality near fine and translucent\nstructures (such as wires). On the bottom, we visualize the number\nof samples per ray (brighter is higher). Our model renders in high\nfidelity at 2K\u00d72K resolution at real-time framerates.\nspeed\u2013quality trade-off.\nRendering.\nWe start with the observation that neural im-\nplicit surface representations, such as signed distance func-\ntions (SDFs), which were originally proposed to improve the\ngeometry quality of NeRFs via regularization [33, 36], can\nalso be used to dramatically increase efficiency by requiring\nfewer samples per ray. In the limit, only a single sample on\nthe surface is required. In practice, renderers still need to\nidentify the location of the target sample(s), which can be\ndone by generating samples via an initial proposal network\n[2] or other techniques, such as sphere tracing [18].\nSurfaceness.\nWhile surface-based neural fields are conve-\nnient for rendering, they often struggle to reconstruct scenes\nwith thin structures or view-dependent effects, such as reflec-\n1\narXiv:2312.03160v1  [cs.CV]  5 Dec 2023\n(a) Training\n(b) Finetuning\n(c) Rendering\nSDF\nSampling\nSurfaceness\nMLP (256-wide)\npos\ndist\nFeatures\nDistilled MLP\n(16-wide)\npos\ndist\nFeatures\nDistilled MLP\n(16-wide)\npos\ndist\nFeatures \n(Textures)\nOccupancy Grid\nOccupancy Grid\nSphere Tracing\nProposal MLP\npos\nsample \nloc.\nFeatures\nLocal \u03b2(x) (final)\nPredicted RGB\nPredicted RGB\n1K Iters\n50K Iters\nGlobal \u03b2\nLocal \u03b2(x)\nFigure 2. Approach. In the first phase of our pipeline (a), we train a VolSDF-like [36] model with distance-adjusted Eikonal loss to\nmodel backgrounds without a separate NeRF (Section 3.3). We then crucially transition from a uniform surfaceness parameter \u03b2 to\nposition-dependent \u03b2(x) values to model most of the scene as thin surfaces (needing few samples) without degrading quality near fine and\nsemi-opaque structures (b). Since our model behaves as a valid SDF in >95% of the scene, we use sphere tracing at render time (c) along\nwith lower-level optimizations (hardware texture interpolation) to query each sample as efficiently as possible.\ntions and translucency. This is one reason that surfaces are\noften transformed into volumetric models for rendering [36].\nA crucial transformation parameter is a scalar temperature\n\u03b2 that is used to convert a \u03b2-scaled signed distance value\ninto a density. Higher temperatures tend to produce an ideal\nbinary occupancy field that can improve rendering speed\nbut can struggle for challenging regions as explained above.\nLower temperatures allow the final occupancy field to re-\nmain flexible, whereby the \u03b2-scaled SDF essentially acts as\na reparameterization of the underlying occupancy field. As\nsuch, we refer to \u03b2 as the surfaceness of the underlying scene\n(see Figure 1). Prior work treats \u03b2 as a global parameter that\nis explicitly scheduled or learned via gradient descent [36].\nWe learn it in a spatially adaptive manner.\nContributions.\nOur primary contribution is a hybrid\nsurface\u2013volume representation that combines the best of both\nworlds. Our key insight is to replace the global parameter \u03b2\nwith spatially-varying parameters \u03b2(x) corresponding to the\nsurfaceness of regions in the 3D scene. At convergence, we\nfind that most of the scene (> 95%) can be efficiently mod-\neled as a surface. This allows us to render with far fewer sam-\nples than fully volumetric methods, while achieving higher\nfidelity than pure surface-based approaches. Additionally,\n1. We propose a weighted Eikonal regularization that allows\nour method to render high-quality complex backgrounds\nwithout a separate background model.\n2. We implement specific rendering optimizations, such as\nhardware texture interpolation and sphere tracing, to sig-\nnificantly accelerate rendering at high resolutions.\n3. We present state-of-the-art reconstruction results on three\ndifferent datasets, including the challenging Eyeful Tower\ndataset [35], while rendering almost 10\u00d7 faster.\n2. Related Work\nMany works try to accelerate the rendering speed of neural\nradiance fields (NeRF). We discuss a representative selection\nof such approaches below.\nVoxel baking.\nSome of the earliest NeRF acceleration\nmethods store precomputed non-view dependent model out-\nputs, such as spherical harmonics coefficients, into finite-\nresolution structures [5, 7, 11, 40]. These outputs are com-\nbined with viewing direction to compute the final radiance at\nrender time, bypassing the original model entirely. Although\nthese methods render extremely quickly (some >200 FPS\n[7]), they are limited by the finite capacity of the caching\nstructure and cannot capture fine details at room scale.\nFeature grids.\nRecent methods use a hybrid approach that\ncombines a learned feature grid with a much smaller MLP\nthan the original NeRF [4, 6, 21]. Instant-NGP [21] (iNGP),\narguably the most popular of these methods, encodes features\ninto a multi-resolution hash table. Although these representa-\ntions speed up rendering, they cannot reach the level needed\nfor real-time HD rendering alone, as even iNGP reaches less\nthan 10 FPS on real-world datasets at high resolution. MERF\n[27] comes closest through a baking pipeline that uses vari-\nous sampling and memory layout optimizations that we also\nmake use of in our implementation.\nSurface\u2013volume representations.\nSeveral methods [23,\n33, 36] derive density values from the outputs of a signed\n2\ndistance function, which are then rendered volumetrically as\nin NeRF. These hybrid representations retain NeRF\u2019s ease of\noptimization while learning better surface geometry. Follow-\nup works [8, 37] bake the resulting surface geometry into\na mesh that is further optimized and simplified. Similar to\nearly voxel-baking approaches, these methods render quickly\n(>70 FPS) but are limited by the capacity of the mesh and\ntexture, and thus struggle to model thin structures, trans-\nparency, and view-dependent effects. We train a similar SDF\nrepresentation in our method but continue to use the base\nneural model at render time.\nSample efficiency.\nSeveral approaches accelerate render-\ning by intelligently placing far fewer samples along each\nray than the original hierarchical strategy proposed by NeRF\n[2, 9, 16, 22, 25]. These methods all train auxiliary networks\nthat are cheaper to evaluate than the base model. However,\nas they are based on purely volumetric representations, they\nare limited in practice as to how few samples they can use\nper ray without degrading quality, and therefore exhibit a\ndifferent quality\u2013performance tradeoff curve than ours.\nGaussians.\nRecent methods take inspiration from NeRF\u2019s\nvolume rendering formula but discard the neural network\nentirely and instead parameterize the scene through a set\nof 3D Gaussians [12\u201314, 32]. Of these, 3D Gaussian splat-\nting [12] has emerged as the new state of the art, rendering\nat >100 FPS with higher fidelity than previous non-neural\napproaches. Although encouraging, it is sensitive to initial-\nization (especially in far-field areas) and limited in its ability\nto reason about inconsistencies within the training dataset\n(such as transient shadows) and view dependent effects.\n3. Method\nGiven a collection of RGB images and camera poses, our\ngoal is to learn a 3D representation that generates novel\nviews at VR resolution (at least 2K\u00d72K pixels) in real-time\n(at least 36 FPS), while achieving a high degree of visual\nfidelity. As we target captures taken under real-world condi-\ntions, our representation must be able to account for incon-\nsistencies across training images due to lighting changes and\nshadows (even in \u201cstatic\u201d scenes). Our approach builds upon\nNeRF\u2019s raycasting paradigm, which can generate highly\nphotorealistic renderings, and improves upon its efficiency.\nSince most of the world consists of surfaces, we develop\na representation that can render surfaces with few samples\nand without degrading the rest of the scene. We outline our\napproach in Figure 2 and present our model architecture\nand the first training stage in Section 3.1, which is followed\nby finetuning of our model to accelerate rendering without\ncompromising quality in Section 3.2. We discuss how to\nmodel unbounded scenes in Section 3.3 and present final\nrender-time optimizations in Section 3.4.\nNeRF (\u224835 samples / ray)\nHybridNeRF (\u22489 samples / ray)\nFigure 3. Surfaces. Since NeRF directly predicts density, it of-\nten \u2018cheats\u2019 by modeling specular surfaces such as floors as semi-\ntransparent volumes that require many samples per ray (heatmaps\nshown on the right). Methods that derive density from signed dis-\ntances, such as ours, improve surface geometry and appearance\nwhile using fewer samples per ray.\n3.1. Representation\nPreliminaries.\nNeRF [20] represents a scene as a con-\ntinuous volumetric radiance field that encodes the scene\u2019s\ngeometry and view-dependent appearance within the weights\nof a model. NeRF renders pixels by sampling positions xi\nalong the corresponding camera ray, querying the MLP\nto obtain density and color values, \u03c3i\n:= \u03c3(xi) and\nci := c(xi, dr), respectively (with dr as the ray direc-\ntion). The density values \u03c3i are converted into opacity values\n\u03b1i := 1\u2212exp(\u2212\u03c3i\u03b4i), where \u03b4i is the distance between sam-\nples. The final ray color \u02c6cr := PN\u22121\ni=0 ciwi is obtained as\na convex combination of the color samples ci with weights\nwi := exp(\u2212Pi\u22121\nj=0\u03c3j\u03b4j)\u03b1i. The training process optimizes\nthe model by sampling batches of image pixels and minimiz-\ning the L2 reconstruction loss. We refer to Mildenhall et al.\n[20] for details.\nModeling density.\nThe original NeRF representation has\nthe flexibility of representing semi-transparent surfaces, for\nthe density field is not forced to saturate. However, the model\noften abuses this property by generating semi-transparent\nvolumes to mimic reflections and other view-dependent ef-\nfects (Figure 3). This hampers our goal of minimizing the\nsamples per ray needed for rendering.\nTo address this problem, surface\u2013volume representations\n[23, 33, 36] learn well-defined surfaces by interpreting MLP\noutputs f(x) as a signed distance field (SDF) such that scene\nsurfaces are represented by the zero-level set of the function\nf.\nAs the norm of the gradient of an SDF should typically\nbe 1, the MLP is regularized via the Eikonal loss:\nLEik(r) :=\nN\u22121\nX\ni=0\n\u03b7i(\u2225\u2207f(xi)\u2225 \u2212 1)2,\n(1)\nwhere \u03b7i is a per-sample loss weight typically set to 1. The\n3\nGlobal \u03b2(x) = 100\n(\u224830 samples / ray)\nGlobal \u03b2(x) = 2000\n(\u22486 samples / ray)\nAdaptive \u03b2(x)\n(\u22488 samples / ray)\nFigure 4. Choice of \u03b2. Increasing \u03b2 reduces the number of samples\nneeded to render per ray, but negatively impacts quality near fine\nobjects (lamp wires) and transparent structures (glass door).\nsigned distances are converted into densities \u03c3SDF that are\npaired with color predictions, and rendered as in NeRF.\nSpecifically, we follow VolSDF\u2019s approach [36] and define:\n\u03c3SDF(x) := \u03b2(x)\u03a8(f(x)\u03b2(x)),\n(2)\nwhere \u03b2(x) > 0 determines the surfaceness of point x, i.e.\nhow concentrated the density should be around the zero level\nset of f, and \u03a8 is the cumulative distribution function of a\nstandard Laplace distribution:\n\u03a8(s) =\n(\n1\n2 exp(\u2212s)\nif s > 0\n1 \u2212 1\n2 exp(s)\nif s \u2264 0.\n(3)\nIn prior works, the surfaceness \u03b2(x) is independent of po-\nsition x, and is either learned or annealed during training\n[37]. We instead consider a surfaceness field implemented\nas a 5123 grid of values that is queried via nearest-neighbor\ninterpolation. We constrain the surfaceness parameters to\nbe globally uniform during the first phase of training and\nallow them to diverge spatially during the finetuning stage\n(Section 3.2).\nModel architecture.\nWe use dense multi-resolution 3D\nfeature grids in combination with multi-resolution triplanes\n[3, 6] for featurizing 3D sample locations. We predict color\nc and signed distance f using separate grids, each followed\nby an MLP, and use a small proposal network similar to the\none used by Nerfacto [31] to improve sampling efficiency.\nFor a given 3D point, we fetch K =4 features per level from\n(1) the 3D feature grids at 3 resolution levels (1283, 2563 and\n5123) via trilinear interpolation, and (2) from triplanes of\nresolutions 1282 to 8,1922 via bilinear interpolation. We sum\nthe features across levels (instead of concatenation [6, 21]),\nand concatenate the summed features from the 3D grid to\nthose from the 3 triplanes to obtain a 4K =16-dimensional\nMLP input. We encode viewing direction through spherical\nharmonics (up to the 4th degree) as an auxiliary input to\nthe color MLP. As our feature grid is multi-resolution, we\nRGB\nEikonal Loss\nSurfaceness\nFigure 5. Spatially adaptive surfaceness. We make \u03b2(x) spatially\nadaptive by means of a 5123 voxel grid that we increase during the\nfinetuning stage. We track Eikonal loss as we increase surfaceness\nas it is highest near object boundaries and semi-transparent surfaces\n(top-right) that degrade when surfaceness is too high (Figure 4). We\nstop increasing surfaceness in regions that cross a given threshold.\nhandle aliasing as in VR-NeRF [35] by deriving the pixel\nradius p(x) of each sample, and dampening features whose\nsize is less than 2p(x) (Appendix B).\nOptimization.\nWe sample random batches of training rays\nand optimize our color and distance fields by minimizing\nthe photometric loss Lphoto and Eikonal loss LEik along with\ninterlevel loss Lprop [2] to train the proposal network:\nL(r) := Lphoto(r) + \u03bbEikLEik(r) + Lprop(r),\n(4)\nwith \u03bbEik = 0.01 in our experiments.\n3.2. Finetuning\nAdaptive Surfaceness.\nThe first stage of our pipeline uses\na global surfaceness value \u03b2(x) = \u00af\u03b2 for all x, similar to\nexisting approaches [33, 36]. As \u00af\u03b2 increases, the density\n\u03c3SDF in free-space areas converges to zero (Equation 3), thus\nreducing the required number of samples per ray. However,\nuniformly increasing this scene-wide parameter degrades the\nrendering quality near fine-grained and transparent structures\n(see Figure 4).\nWe overcome this limitation by making \u03b2(x) spatially\nadaptive via a 5123 voxel grid. We collect per-sample triplets\n(x, \u03b7, w) rendered during the finetuning process, accumulate\nthem over multiple training iterations (5,000), and partition\nthem across the voxels of the surfaceness grid. Let \u039bv be\nthe subset associated with voxel v corresponding to \u03b2v. We\nincrease \u03b2v by a fixed increment (100) if:\nP\n(x,\u03b7,w)\u2208\u039bv w\u03b7(\u2225\u2207f(x)\u2225 \u2212 1)2\nP\n(...,w)\u2208\u039bv w\n< \u00af\u03b3,\n(5)\nwhere \u00af\u03b3 := 0.25 is a predefined threshold. Figure 5 illus-\ntrates our approach.\n4\nProposal network baking.\nAlthough the proposal network\nallows us to quickly learn the scene geometry during the\nfirst stage of training, it is too expensive to evaluate in real\ntime. We follow MERF\u2019s protocol [27] to bake the proposal\nnetwork into a 10243 binary occupancy grid. We render all\ntraining rays and mark a voxel as occupied if there exists at\nleast one sampled point xi such that max(wi, \u03c3i) > 0.005.\nWe finetune our model using the occupancy grid to prevent\nany loss in quality.\nMLP distillation.\nWe find it important to adopt a large 256\nchannel-wide MLP to represent the signed distance f during\nthe initial training phase in order to preserve accurate scene\ngeometry. However, we later distill f into a much smaller\n16-wide network (fsmall). We do so by sampling random rays\nfrom our training set for 5,000 iterations and minimizing the\nabsolute difference between f(xi) and fsmall(xi) for every\nsampled point:\nLdist(r) :=\nN\u22121\nX\ni=0\n|f(xi) \u2212 fsmall(xi)|,\n(6)\nwith a stop gradient applied to the outputs of f. We then\ndiscard the original SDF f and switch to using the distilled\ncounterpart fsmall for the rest of the finetuning stage.\n3.3. Backgrounds\nMany scenes we wish to reconstruct contain complex back-\ngrounds that surface\u2013volume methods struggle to replicate\n[17, 33, 36]. BakedSDF [37] defines a contraction space [2]\nin which the Eikonal loss of Equation 1 is applied. How-\never, we found this to negatively impact foreground quality.\nOther approaches use separate NeRF background models\n[42], which effectively doubles inference and memory costs,\nand makes them ill-suited for real-time rendering.\nRelation between volumetric and surface-based NeRFs.\nWe discuss how to make a single MLP behave as an approxi-\nmate SDF in the foreground and a volumetric model in the\nbackground. Both types of NeRF derive density \u03c3 by apply-\ning a non-linearity to the output of an MLP. Our insight is\nthat although the original NeRF uses ReLU, any non-linear\nmapping to R+ may be used in practice, including our scaled\nCDF \u03a8 (\u03b2 omitted without loss of generality). Since \u03a8 is in-\nvertible (as it is a CDF), \u03c3(x) and \u03a8(f(x)) are functionally\nequivalent as there exists an f such that \u03a8(f(x)) = \u03c3(x) for\nany given point x. Put otherwise, it is the Eikonal regular-\nization that causes the divergence in behavior between both\nmethods \u2014 in its absence, an \u201cSDF\u201d MLP is free to behave\nexactly as the density MLP in the original NeRF!\nDistance-adjusted loss.\nWe use a distance-adjusted\nEikonal loss during training by using per-sample loss weights\n\u03b7i =\n1\nd2\ni (where di is the metric distance along the ray of sam-\nple xi) instead of commonly-used uniform weights (\u03b7i = 1)\nEikonal Loss (\u03b7i = 1)\nNo Eikonal Loss\nEikonal Loss (\u03b7i = 1) in\nContracted Space [37]\nDistance-Adjusted Eikonal Loss\n(\u03b7i = d\u22122\ni\n)\nFigure 6. Backgrounds. Using standard Eikonal loss affects back-\nground reconstruction (top-left) while applying it in contracted\nspace [37] affects the foreground (bottom-left). Omitting Eikonal\nloss entirely causes surface\u2013volume methods to revert to NeRF\u2019s\nbehavior, which improves background quality but degrades fore-\nground surface reconstruction (top-right). By using distance-\nadjusted sample weights \u03b7i = d\u22122\ni\n, we improve background recon-\nstruction without impacting foreground quality (bottom-right).\nto downweight the loss applied to far-field regions. Intu-\nitively, this encourages our method to behave as a valid SDF\nin the foreground (with well-defined surfaces) and more like\nNeRF in the background (to enable accurate reconstruction)\nwithout the need for separate foreground and background\nmodels. Figure 6 illustrates the different approaches.\n3.4. Real-Time Rendering\nTexture storage.\nOur architecture enables us to use lower-\nlevel optimizations. Methods such as iNGP [21] use concate-\nnated multi-resolution features stored in hash tables. Since\nwe use explicit 3D grids and triplanes, we can store our\nfeatures as textures at render time, taking advantage of in-\ncreased memory locality and texture interpolation hardware.\nAs we sum our multi-resolution features during training,\nwe optimize the number of texture fetches by storing pre-\nsummed features g\u2032 at resolution level L (where we store\ng\u2032(v) = PL\nl=0 g(v, l) for each texel in L). For a given sam-\nple x at render time, we obtain its anti-aliased feature by\ninterpolating between the two levels implied by its pixel area\np(x), reducing the number of texture fetches to 8 queries per\nMLP evaluation from the original 3+3\u00d77=24 (assuming\nthree 3D grids and seven triplane levels), a 3\u00d7 reduction.\nSphere tracing.\nVolumetric methods that use occupancy\ngrids [e.g. 21, 27] sample within occupied voxels using a\ngiven step size. This hyperparameter must be carefully tuned\nto strike the proper balance between quality (not skipping\nthin surfaces) and performance (not excessively sampling\nempty space). Modeling an SDF allows us to sample more\nefficiently by advancing toward the predicted surface using\nsphere tracing [29]. At each sample point xi and predicted\n5\nMERF (60 FPS)\nOffice 1B\n3DGS (138 FPS)\nVolSDF (15 FPS)\nVR-NeRF (6 FPS)\nOurs (46 FPS)\nGround Truth\nOffice View 2\nRiverview\nApartment\nFigure 7. Eyeful Tower [35]. HybridNeRF is the only method to accurately model reflections and shadows (first two rows), far-field content\n(third row) and fine structures (bottom row) at real-time frame rates at 2K\u00d72K resolution.\nTable 1. Eyeful Tower [35] results. We omit 3DGS results for fisheye scenes as their implementation does not handle fisheye projection.\nAlong with 3DGS and MERF, ours is the only to reach the 36 FPS target for VR along with a >1.5db PSNR improvement in quality.\nPinhole\nFisheye\nOverall\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2191FPS\niNGP* [21]\n27.35\n0.826\n0.361\n33.32\n0.938\n0.155\n30.06\n0.877\n0.267\n4.55\nVolSDF* [36]\n27.10\n0.856\n0.310\n34.09\n0.951\n0.116\n30.28\n0.899\n0.222\n15.29\nMERF (pre-baking) [27]\n26.44\n0.831\n0.506\n31.18\n0.922\n0.549\n28.59\n0.872\n0.526\n18.11\nMERF (baked) [27]\n25.99\n0.830\n0.525\n31.09\n0.921\n0.546\n28.31\n0.871\n0.535\n60.18\n3D Gaussian splatting [12]\n27.42\n0.877\n0.291\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n138.22\nVR-NeRF [35]\n28.08\n0.834\n0.326\n34.53\n0.951\n0.130\n31.01\n0.888\n0.237\n6.05\nHybridNeRF\n29.07\n0.880\n0.268\n34.57\n0.952\n0.115\n31.57\n0.913\n0.198\n45.78\n* Our implementation. VolSDF: with iNGP acceleration.\nsurface distance s = f(xi), we advance by 0.9s (chosen\nempirically to account for our model behaving as an approxi-\nmate SDF) until hitting the surface (predicted as s\u22642\u00d710\u22124).\nWe only perform sphere tracing where our model behaves as\na valid SDF (determined by \u03b2(xi)>350 in our experiments),\nand fall back to a predefined step size of 1 cm otherwise.\n4. Experiments\nAs our goal is high-fidelity view synthesis at VR resolu-\ntion (\u2248 4 megapixels), we primarily evaluate HybridNeRF\nagainst the Eyeful Tower dataset [35], which contains high-\nfidelity scenes designed for walkable VR (Section 4.2). We\ncompare our work to a broader range of methods on addi-\ntional datasets in Section 4.3. We ablate our design decisions\nin Section 4.4.\n4.1. Implementation\nWe train our models in the PyTorch framework [24] and\nimplement our renderer in C++/CUDA. We parameterize un-\nbounded scenes with MERF\u2019s piecewise-linear contraction\n[27] so that our renderer can efficiently query the occupancy\ngrid via ray-AABB intersection. We train on each scene\nfor 200,000 iterations (100,000 in each training stage) with\n12,800 rays per batch using Adam [15] and a learning rate\nof 2.5\u00d710\u22123.\n4.2. VR Rendering\nEyeful Tower dataset.\nThe dataset consists of room-scale\ncaptures, each containing high-resolution HDR images at 2K\nresolution, captured using a multi-view camera rig. Although\ncare is taken to obtain the best quality images possible, in-\nconsistencies still appear between images due to lighting\n6\nMERF (60 FPS)\n3DGS (95 FPS)\nVolSDF (13 FPS)\nVR-NeRF (5 FPS)\nOurs (42 FPS)\nGround Truth\nFigure 8. ScanNet++ [38]. 3D Gaussian splatting [12] struggles with specular surfaces such as whiteboards (above) and far-field content\n(below). Our method performs best qualitatively while maintaining a real-time frame rate.\nTable 2. MipNeRF 360 [2]. Real-time methods are highlighted (best, second-best). Baseline numbers are reported from [5, 27, 37].\nMobileNeRF [5] was not evaluated on indoor scenes. Our method performs similarly to SOTA across real-time and offline methods.\nOutdoor\nIndoor\nOverall\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\nNeRF [20]\n21.46\n0.458\n0.515\n26.84\n0.790\n0.370\n23.85\n0.606\n0.451\nNeRF++ [42]\n22.76\n0.548\n0.427\n28.05\n0.836\n0.309\n25.11\n0.676\n0.375\nSVS [28]\n23.01\n0.662\n0.253\n28.22\n0.907\n0.160\n25.33\n0.771\n0.212\nMip-NeRF 360 [2]\n24.47\n0.691\n0.283\n31.72\n0.917\n0.180\n27.69\n0.791\n0.237\niNGP [21]\n22.90\n0.566\n0.371\n29.15\n0.880\n0.216\n25.68\n0.706\n0.302\nDeep Blending [10]\n21.54\n0.524\n0.364\n26.40\n0.844\n0.261\n23.70\n0.666\n0.318\nMobileNeRF [5]\n21.95\n0.470\n0.470\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nBakedSDF [37]\n22.47\n0.585\n0.349\n29.15\n0.880\n0.216\n25.68\n0.706\n0.302\nMERF [27]\n23.19\n0.616\n0.343\n27.80\n0.855\n0.271\n25.24\n0.722\n0.311\n3D Gaussian splatting [12]\n24.13\n0.707\n0.211\n30.94\n0.927\n0.081\n27.16\n0.805\n0.153\nHybridNeRF\n24.73\n0.716\n0.224\n30.19\n0.914\n0.104\n27.17\n0.806\n0.171\nchanges and shadows from humans and the capture rig it-\nself. We model as much of the dynamic range as possible by\nmapping colors in the PQ color space [30], as proposed in\nVR-NeRF [35], during training and tonemap to sRGB space\nduring evaluation to compare against non-HDR baselines.\nBaselines.\nWe compare HybridNeRF to baselines across\nthe fidelity/speed spectrum. We benchmark several volumet-\nric methods, including (1) iNGP [21], (2) VR-NeRF [35],\nwhich extends iNGP\u2019s [21] primitives to better handle HDR\nreconstruction and provides the best existing results against\nthe Eyeful Tower dataset in terms of visual quality, and (3)\nMERF [27], a highly optimized method that uses sampling\nand memory layout optimizations to accelerate rendering.\nWe also compare to VolSDF [36] as a hybrid surface\u2013volume\nmethod similar to the first stage of our method. As the origi-\nnal VolSDF implementation uses large MLPs that are unsuit-\nable for real-time rendering, we use an optimized version\nbuilt on top of iNGP\u2019s acceleration primitives as a fairer\ncomparison. We finally compare against 3D Gaussian splat-\nting [12] as a non-neural approach that represents the current\nstate of the art with regards to quality and rendering speed.\nMetrics.\nWe report quantitative results based on PSNR,\nSSIM [34], and the AlexNet implementation of LPIPS [43]\nand measure frame rates rendered at 2K\u00d72K resolution on a\nsingle NVIDIA RTX 4090 GPU.\nResults.\nWe summarize our results in Table 1 along with\nqualitative results in Figure 7. VR-NeRF [35] and iNGP\n[21] render well below real-time frame rates. Our VolSDF\nimplementation, which uses the same primitives as iNGP,\nis 3\u00d7 faster merely from the benefits of using a surface\nrepresentation (and fewer samples per ray). MERF [27], as\na volume representation, relies instead on precomputation\nto accelerate rendering by explicitly storing diffuse color\nand density outputs during its baking stage and using only\na small MLP to model view-dependent effects. Although\nit reaches a high frame rate, it provides the least visually\nappealing results amongst our baselines. 3D Gaussian splat-\nting [12] renders the fastest, but struggles with shadows and\nlighting changes across the training views and models them\nas unsightly floaters. Our method is the only to achieve both\nhigh quality and real-time frame rates.\n7\nTable 3. Diagnostics. A global learned \u03b2 (\u2248 200) produces the highest-quality renderings, but is slow to render as much of the scene is\nmodeled volumetrically. Increasing \u03b2 improves rendering speed but results in worse accuracy. Our full method (with spatially-varying \u03b2(x))\ngets the best of both worlds. Other innovations such as distance-adjusted Eikonal loss are crucial for ensuring high accuracy for scenes with\ncomplex backgrounds. Finally, distillation and hardware acceleration come at a minor quality cost while doubling rendering speed.\nMethods\n\u03b2(x)\nDist.\nDistill\nTextures\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2191FPS\nw/ Global \u03b2 (learned)\n\u2717\n\u2713\n\u2713\n\u2713\n31.76\n0.923\n0.188\n28.79\nw/ Global \u03b2 = 2000\n\u2717\n\u2713\n\u2713\n\u2713\n27.16\n0.835\n0.345\n47.47\nw/o distance-adjusted Eik.\n\u2713\n\u2717\n\u2713\n\u2713\n29.97\n0.856\n0.260\n45.42\nw/o MLP Distillation\n\u2713\n\u2713\n\u2717\n\u2713\n31.65\n0.915\n0.193\n35.25\nw/o CUDA Textures\n\u2713\n\u2713\n\u2713\n\u2717\n31.62\n0.921\n0.195\n28.48\nFull Method\n\u2713\n\u2713\n\u2713\n\u2713\n31.57\n0.913\n0.198\n45.78\nTable 4. ScanNet++ [38] results. Similar to Table 1, our method is\nthe only to hit VR rate rates along with 3DGS and MERF. Hybrid-\nNeRF performs best across all quality metrics.\nMethod\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2191FPS\niNGP* [21]\n23.69\n0.815\n0.308\n5.39\nVolSDF* [36]\n24.26\n0.834\n0.246\n13.18\nMERF (pre-baking) [27]\n23.44\n0.821\n0.306\n12.08\nMERF (baked) [27]\n23.19\n0.820\n0.308\n60.21\n3D Gaussian splatting [12]\n23.76\n0.830\n0.248\n94.95\nVR-NeRF [35]\n24.00\n0.814\n0.301\n5.38\nHybridNeRF\n24.64\n0.835\n0.236\n41.90\n* Our implementation. VolSDF: with iNGP acceleration.\n4.3. Additional Comparisons\nDatasets.\nWe evaluate HybridNeRF on MipNeRF-360 [2]\nas a highly-referenced dataset evaluated by many prior meth-\nods, and ScanNet++ [38] as a newer benchmark built from\nhigh-resolution captures of indoor scenes that are relevant\nto our goal of enabling immersive AR/VR applications. We\ntest on all scenes in the former and a subset of the latter.\nBaselines.\nWe compare HybridNeRF to a wide set of base-\nlines on Mip-NeRF 360. We use the same set of baselines as\nin Section 4.2 for ScanNet++.\nResults.\nWe list results in Table 2 and Table 4. Our method\nperforms comparably to the state-of-the-art on Mip-NeRF\n360 across both real-time [12] and offline [2] methods. Al-\nthough ScanNet++ [38] contains fewer lighting inconsisten-\ncies across training images than the Eyeful Tower dataset\n[35], 3D Gaussian splatting still struggles to reconstruct\nspecular surfaces (whiteboards, reflective walls) and back-\ngrounds (Table 4). Our method performs the best overall\nwhile maintaining a real-time frame rate.\n4.4. Diagnostics\nMethods.\nWe ablate our design decisions by individually\nomitting the major components of our method, most notably:\nour distance-adjusted Eikonal loss, our adaptive surfaceness\n\u03b2(x), MLP distillation, and hardware-accelerated textures\n(vs. iNGP [21] hash tables commonly used by other fast\nNeRF methods).\nResults.\nWe present results against the Eyeful Tower [35]\nin Table 3. Spatially adaptive surfaceness is crucial as using a\nglobal parameter degrades either speed (when \u03b2 is optimized\nfor quality) or rendering quality (when set for speed). Ap-\nplying uniform Eikonal loss instead of our distance-adjusted\nvariant degrades quality in unbounded scenes. Omitting the\ndistillation process has a minor impact on quality relative to\nrendering speed. We note a similar finding when using iNGP\n[21] primitives instead of CUDA textures, which suggests\nthat introducing hardware acceleration into these widely\nused primitives is a potential avenue for future research.\n5. Limitations\nMemory.\nStoring features in dense 3D grids and triplanes\nconsumes significantly more memory than with hash tables\n[21]. Training is especially memory-intensive as interme-\ndiate activations must be stored for backpropagation along\nwith per-parameter optimizer statistics. Storing features in a\nhash table during the training phase before \u201cbaking\u201d them\ninto explicit textures as in MERF [27] would ameliorate\ntrain-time consumption but not inference-time consumption.\nTraining time.\nAlthough our training time is much faster\nthan the original NeRF, it is about 2\u00d7 slower than iNGP due\nto the additional backprogation needed for Eikonal regular-\nization (inline with other \u201cfast\u201d surface approaches such as\nNeuS-facto [41]) and slower than 3D Gaussian Splatting.\n6. Conclusion\nWe present a hybrid surface\u2013volume representation that com-\nbines the best of surface and volume-based rendering into\na single model. We achieve state-of-the-art quality across\nseveral datasets while maintaining real-time framerates at\nVR resolutions. Although we push the performance frontier\nof raymarching approaches, a significant speed gap remains\nnext to splatting-based approaches [12]. Combining the ad-\nvantages of our surface\u2013volume representation with these\nmethods is a valuable next step.\n8\nReferences\n[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-NeRF: A multiscale representation for anti-aliasing neu-\nral radiance fields. In ICCV, 2021. 1\n[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded\nanti-aliased neural radiance fields. In CVPR, 2022. 1, 3, 4, 5,\n7, 8\n[3] Ang Cao and Justin Johnson. HexPlane: A fast representation\nfor dynamic scenes. In CVPR, 2023. 4\n[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. TensoRF: Tensorial radiance fields. In ECCV, 2022.\n1, 2\n[5] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea\nTagliasacchi. MobileNeRF: Exploiting the polygon rasteri-\nzation pipeline for efficient neural field rendering on mobile\narchitectures. In CVPR, 2023. 2, 7\n[6] Sara Fridovich-Keil, Giacomo Meanti, Frederik Warburg,\nBenjamin Recht, and Angjoo Kanazawa. K-planes: Explicit\nradiance fields in space, time, and appearance. In CVPR,\n2023. 2, 4\n[7] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. FastNeRF: High-fidelity neural\nrendering at 200FPS. In ICCV, 2021. 2\n[8] Yuan-Chen Guo, Yan-Pei Cao, Chen Wang, Yu He, Ying\nShan, Xiaohu Qie, and Song-Hai Zhang.\nVMesh: Hy-\nbrid volume-mesh representation for efficient view synthesis.\narXiv:2303.16184, 2023. 3\n[9] Kunal Gupta, Milo\u02c7s Ha\u02c7san, Zexiang Xu, Fujun Luan, Kalyan\nSunkavalli, Xin Sun, Manmohan Chandraker, and Sai Bi.\nMCNeRF: Monte Carlo rendering and denoising for real-time\nNeRFs. In SIGGRAPH Asia, 2023. 3\n[10] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\nfree-viewpoint image-based rendering. ACM Trans. Graph.,\n37(6), 2018. 7\n[11] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,\nJonathan T. Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. ICCV, 2021. 2\n[12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. 3D Gaussian splatting for real-time radi-\nance field rendering. ACM Trans. Graph., 42(4):139:1\u201314,\n2023. 1, 3, 6, 7, 8\n[13] Leonid Keselman and Martial Hebert. Approximate differen-\ntiable rendering with algebraic surfaces. In ECCV, 2022.\n[14] Leonid Keselman and Martial Hebert.\nFlexible tech-\nniques for differentiable rendering with 3D Gaussians.\narXiv:2308.14737, 2023. 3\n[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6\n[16] Andreas Kurz, Thomas Neff, Zhaoyang Lv, Michael\nZollh\u00a8ofer, and Markus Steinberger. AdaNeRF: Adaptive sam-\npling for real-time rendering of neural radiance fields. In\nECCV, 2022. 3\n[17] Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H. Tay-\nlor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.\nNeuralangelo: High-fidelity neural surface reconstruction. In\nCVPR, pages 8456\u20138465, 2023. 5\n[18] Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc\nPollefeys, and Zhaopeng Cui. Dist: Rendering deep implicit\nsigned distance function with differentiable sphere tracing.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 2019\u20132028, 2020. 1\n[19] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view synthe-\nsis with prescriptive sampling guidelines. ACM Transactions\non Graphics (TOG), 2019. 1\n[20] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. In ECCV, 2020. 1, 3, 7\n[21] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander\nKeller. Instant neural graphics primitives with a multireso-\nlution hash encoding. ACM Trans. Graph., 41(4):102:1\u201315,\n2022. 2, 4, 5, 6, 7, 8\n[22] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An-\nton S. Kaplanyan, and Markus Steinberger. DONeRF: To-\nwards Real-Time Rendering of Compact Neural Radiance\nFields using Depth Oracle Networks. Computer Graphics\nForum, 2021. 3\n[23] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUNISURF: Unifying neural implicit surfaces and radiance\nfields for multi-view reconstruction. In ICCV, 2021. 2, 3\n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An-\ndreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\nperative style, high-performance deep learning library. In\nNeurIPS, pages 8024\u20138035, 2019. 6\n[25] Martin Piala and Ronald Clark. TermiNeRF: Ray termination\nprediction for efficient neural rendering. In 3DV, 2021. 3\n[26] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. KiloNeRF: Speeding up neural radiance fields with\nthousands of tiny MLPs. In ICCV, 2021. 1\n[27] Christian Reiser, Richard Szeliski, Dor Verbin, Pratul P. Srini-\nvasan, Ben Mildenhall, Andreas Geiger, Jonathan T. Barron,\nand Peter Hedman. MERF: Memory-efficient radiance fields\nfor real-time view synthesis in unbounded scenes. ACM Trans.\nGraph., 42(4):89:1\u201312, 2023. 2, 5, 6, 7, 8\n[28] Gernot Riegler and Vladlen Koltun. Stable view synthesis. In\nCVPR, 2021. 7\n[29] Radu Alexandru Rosu and Sven Behnke. PermutoSDF: Fast\nmulti-view reconstruction with implicit surfaces using permu-\ntohedral lattices. In CVPR, 2023. 5\n[30] SMPTE. High dynamic range electro-optical transfer func-\ntion of mastering reference displays. SMPTE Standard ST\n2084:2014, Society of Motion Picture and Television Engi-\nneers, 2014. 7\n[31] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent\nYi, Terrance Wang, Alexander Kristoffersen, Jake Austin,\n9\nKamyar Salahi, Abhik Ahuja, David Mcallister, Justin Kerr,\nand Angjoo Kanazawa. Nerfstudio: A modular framework\nfor neural radiance field development. In SIGGRAPH, pages\n72:1\u201312, 2023. 4\n[32] Angtian Wang, Peng Wang, Jian Sun, Adam Kortylewski, and\nAlan Yuille. VoGE: A differentiable volume renderer using\nGaussian ellipsoids for analysis-by-synthesis. In ICLR, 2023.\n3\n[33] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. NeuS: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nIn NeurIPS, 2021. 1, 2, 3, 4, 5\n[34] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.\nImage quality assessment: from error visibility to structural\nsimilarity. IEEE Transactions on Image Processing, 13(4):\n600\u2013612, 2004. 7\n[35] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia,\nAayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo\nPorzi, Peter Kontschieder, Alja\u02c7z Bo\u02c7zi\u02c7c, Dahua Lin, Michael\nZollh\u00a8ofer, and Christian Richardt. VR-NeRF: High-fidelity\nvirtualized walkable spaces. In SIGGRAPH Asia, 2023. 1, 2,\n4, 6, 7, 8\n[36] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. In NeurIPS, 2021.\n1, 2, 3, 4, 5, 6, 7, 8\n[37] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,\nPratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron,\nand Ben Mildenhall. BakedSDF: Meshing neural SDFs for\nreal-time view synthesis. In SIGGRAPH, 2023. 1, 3, 4, 5, 7\n[38] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner, and\nAngela Dai. ScanNet++: A high-fidelity dataset of 3D indoor\nscenes. In ICCV, 2023. 7, 8, 1\n[39] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner, and\nAngela Dai. ScanNet++ Toolkit. https://github.com/\nscannetpp/scannetpp, 2023. Accessed: 2023-11-01.\n1\n[40] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\nneural radiance fields. In ICCV, 2021. 2\n[41] Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng, Apra-\ntim Bhattacharyya, Michael Niemeyer, Siyu Tang, Torsten\nSattler, and Andreas Geiger. Sdfstudio: A unified framework\nfor surface reconstruction, 2022. 8\n[42] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. NeRF++: Analyzing and improving neural radiance\nfields. arXiv:2010.07492, 2020. 5, 7\n[43] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 7\n10\nHybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces\nSupplementary Material\nA. Color Distillation\nWe distill the MLP used to represent distance from our 256-\nwide MLP to a 16-wide network during the finetuning stage\n(Section 3.2). It is possible to further accelerate rendering by\nsimilarly distilling the color MLP. We found this to provide\na significant boost in rendering speed (from 46 to 60 FPS)\nat the cost of a minor but statistically significant decrease in\nrendering quality (see Table 5). We observed qualitatively\nsimilar results when decreasing width from 64 to 32 chan-\nnels with more notable changes in color when decreasing\nthe width to 16 channels (see Figure 9). As our initial results\nsuggest that MLP evaluation remains a significant render-\ning bottleneck, replacing our scene-wide color MLP with a\ncollection of smaller, location-specific MLPs as suggested\nby KiloNeRF [26] is potential future work that could boost\nrendering speed at a smaller cost in quality.\nB. Anti-Aliasing\nWe model rays as cones [1] and use a similar anti-aliasing\nstrategy to VR-NeRF [35] by dampening high-resolution\ngrid features based on pixel footprint. For a given sample\nx, we derive a pixel radius p(x) in the contracted space,\nand calculate the optimal feature level L(x) based on the\nNyquist\u2013Shannon sampling theorem:\nL(x) := \u2212 log2(2s \u00b7 p(x)),\n(7)\nwhere s is our base grid resolution (128). We then multiply\ngrid features at resolution level L with per-level weights wL:\nwL =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1\nif L < \u230aL(x)\u230b\nL(x) \u2212 \u230aL(x)\u230b\nif \u230aL(x)\u230b < L \u2264 \u2308L(x)\u2309\n0\nif \u2308L(x)\u2309 < L.\n(8)\nC. ScanNet++\nWe evaluate 9 scenes from ScanNet++ [38] in Sec-\ntion 4.3 (5FB5D2DBF2, 8B5CAF3398, 39F36DA05B, 41B00FEDDB,\n56A0EC536C, 98B4EC142F, B20A261FDF, F8F12E4E6B, FE1733741F).\nWe undistort the fisheye DSLR captures to pinhole images\nusing the official dataset toolkit [39] to facilitate comparisons\nagainst 3D Gaussian splatting [12] (whose implementation\ndoes not support fisheye projection). We use the official vali-\ndation splits, which consist of entirely novel trajectories that\npresent a more challenging novel-view synthesis problem\nthan the commonly used pattern of holding out every eighth\nframe [19]. The dataset authors note that their release is still\nin the beta testing phase, and that the final layout is subject\nto change. Our testing reflects the dataset as of November\n2023.\n64-Wide (46 FPS)\nApartment\n32-Wide (57 FPS)\n16-Wide (60 FPS)\nOffice 1B\nTable\nFigure 9. Color Distillation. Distilling the color MLP to a smaller\nwidth during the finetuning stage (Section 3.2) accelerates rendering\nat the cost of a minor decrease in quality. We observe largely\nsimilar results when decreasing the width to 32 channels, and more\nnoticeable changes in color when further decreasing to 16.\nTable 5. Color distillation. We evaluate the effect of color MLP\ndistillation on the Eyeful Tower dataset [35], and find a significant\nincrease in rendering speed at the cost of quality.\nColor Width\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2191FPS\n16-wide (distilled)\n30.88\n0.888\n0.236\n60.13\n32-wide (distilled)\n31.17\n0.900\n0.220\n57.05\n64-wide (original)\n31.57\n0.913\n0.198\n45.78\nD. Societal Impact\nOur technique facilitates the rapid generation of high-quality\nneural representations. Consequently, the risks associated\nwith our work parallel those found in other neural render-\ning studies, primarily centered around privacy and security\nissues linked to the deliberate or unintentional capture of\nsensitive information, such as human facial features and ve-\nhicle license plate numbers. Although we did not specifically\napply our approach to data involving privacy or security\nconcerns, there exists a risk, akin to other neural rendering\nmethodologies, that such sensitive data could become incor-\nporated into the trained model if the datasets utilized are not\nadequately filtered beforehand. It is imperative to engage in\npre-processing of the input data employed for model training,\nespecially when extending its application beyond research,\nto ensure the model\u2019s resilience against privacy issues and\npotential misuse. However, a more in-depth exploration of\nthis matter is beyond the scope of this paper.\n1\n"
  },
  {
    "title": "Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models",
    "link": "https://arxiv.org/pdf/2312.03632.pdf",
    "upvote": "4",
    "text": "Multimodal Data and Resource Efficient\nDevice-directed Speech Detection with Large\nFoundation Models\nDominik Wagner1\u2217,\nAlexander Churchill2,\nSiddharth Sigtia2,\nPanayiotis Georgiou2,\nMatt Mirsamadi2,\nAarshee Mishra2,\nErik Marchi2\n1TH N\u00fcrnberg, 2Apple\ndominik.wagner@th-nuernberg.de, alex.churchill@apple.com,\nsidsigtia@apple.com, panayiotis_georgiou@apple.com,\nsmirsamadi@apple.com, aarshee_mishra@apple.com, emarchi@apple.com\nAbstract\nInteractions with virtual assistants typically start with a trigger phrase followed by\na command. In this work, we explore the possibility of making these interactions\nmore natural by eliminating the need for a trigger phrase. Our goal is to determine\nwhether a user addressed the virtual assistant based on signals obtained from\nthe streaming audio recorded by the device\u2019s microphone. We address this task\nby combining 1-best hypotheses and decoder signals from an automatic speech\nrecognition system with acoustic representations from an audio encoder as input\nfeatures to a large language model (LLM). In particular, we are interested in data\nand resource efficient systems that require only a small amount of training data\nand can operate in scenarios with only a single frozen LLM available on a device.\nFor this reason, our model is trained on 80k or less examples of multimodal data\nusing a combination of low-rank adaptation and prefix tuning. We compare the\nproposed system to unimodal baselines and show that the multimodal approach\nachieves lower equal-error-rates (EERs), while using only a fraction of the training\ndata. We also show that low-dimensional specialized audio representations lead to\nlower EERs than high-dimensional general audio representations.\n1\nIntroduction\nSpeech-based virtual assistants allow users to interact with devices such as phones, watches, and\nloudspeakers via voice commands. To distinguish audio that is directed towards a device from\nbackground speech, a trigger phrase or the press of a button usually precedes the user command\n[1]. The problem of detecting a trigger phrase is referred to as, wake-word detection [2, 3], voice\ntrigger detection [4, 5], or keyword spotting [6, 7, 8, 9]. To create a more natural conversation flow,\nsubsequent commands after the initial interaction should not require the trigger phrase. Device-\ndirected speech detection is concerned with determining whether a virtual assistant was addressed or\nnot, without a trigger cue preceding the voice command at all times [10, 11, 12]. Device-directed\nspeech detection systems are exposed to information from all kinds of in-domain (voice commands)\nand out-of-domain (e.g. background speech, ambient sounds, appliances etc.) signals. Previous\nworks use a combination of acoustic and lexical features to encode the relevant information in those\nsignals [10, 11, 13, 14, 15].\nRecent studies have extended LLMs with the ability to process non-lexical input modalities, such\nas audio and video data [16, 17, 18, 19, 20, 21]. Inspired by these efforts, we explore a LLM-based\nmultimodal model to differentiate between directed and non-directed audio in interactions with a\nvirtual assistant. Our goal is to determine whether the user addressed the assistant using signals\nobtained from the streaming audio captured by the device\u2019s microphone.\n\u2217Work done during an internship at Apple.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.03632v1  [cs.SD]  6 Dec 2023\nThe proposed model uses acoustic features obtained from a pretrained audio encoder in combination\nwith decoder signals, such as acoustic cost, as well as 1-best hypotheses from an ASR system. The\nacoustic features and decoder signals are represented as learnable fixed-length prefixes, which are\nconcatenated with the token embeddings of the 1-best hypotheses (cf. Figure 1). The system is\noptimized to generate decisions about device-directedness by jointly learning from all modalities\nusing a combination of prefix tuning [22] and low-rank adaptation (LoRA) [23].\nWe analyze this task in a scenario in which (1) only a limited amount of training data is available\nand (2) only a pretrained LLM with frozen weights is usable on a resource-constrained device (e.g.\na smartphone). Furthermore, we compare the effectiveness of high-dimensional representations\nobtained from a large generic audio foundation model with lower-dimensional representations from a\nsmall audio encoder trained on in-domain data.\n2\nFeature Extraction\n1-best Hypotheses and ASR Decoder Signals\nThe text part of the data was transcribed with an\non-device joint CTC-attention based end-to-end speech recognition system [24] trained on in-domain\ndata, comparable to the one used in [25]. Inspired by [10, 11], we extract 4 additional utterance-level\nsignals that are generated by a decoder based on weighted finite-state transducers [26]. For the most\nlikely hypothesis in the N-best set of hypotheses, we extract the average of the graph cost associated\nwith each word in the hypothesis, the average of the acoustic cost, and the average of the word-level\nposterior confidence scores. The graph cost is the sum of language model cost, transition probabilities,\nand pronunciation cost [27]. The acoustic cost is the negative log-likelihood of the tokens from the\ndecoder. Additionally, we include the average number of alternative word options for each word in\nthe 1-best hypothesis. Finally, we scale the feature values along each signal dimension into the unit\ninterval [0, 1] across the dataset.\nAudio Representations\nWe compare two pretrained models as backbones to extract audio repre-\nsentations. The first model is the medium version of Whisper (769M parameters) [28]. Whisper is\nexpected to generalize well across domains and languages, since it was trained on 680k hours of\nspeech data and is therefore well-suited for our task. We extract 1024-dimensional representations at\nthe last encoder layer of Whisper. Additionally, we explore a specialized and lightweight on-device\nmodel for acoustic feature extraction. We choose the Unified Acoustic Detector (UAD) for false\ntrigger mitigation described in [29] as an alternative feature extractor. The model is trained to detect\nunintended invocations of devices such as smartphones. It has \u22486 million parameters and consists of\na shared transformer-based encoder [30], followed by task-specific classification heads. We extract\n256-dimensional representations at one of the task-specific classification heads.\n3\nMethod\nInput audio\nMean pooling\nASR decoder\nsignals\n1-best\nhypothesis\nToken representations\nyes/no\nTask prompt\nwhat date is it?  [SEP]  directed decision:\nPretrained\nASR system\nPretrained\nAudio encoder\nMapping\nnetwork\nMapping\nnetwork\nDecoder-only LLM\nPrefix representations\nTokenizer\nconcatenation\nconcatenation\nInput representations\nTransformer block\nMulti-head self-attention\nLinear layer\nLayer normalization\nLinear layer\nLinear layer\nLayer normalization\nDevice-directedness\ndecision\nLoRA\nFrozen\nweights\nFigure 1: Architecture of the multimodal system. The weights of\nthe LoRA modules are trained along with the weights of M1 and\nM2. All other components remain frozen.\nOur system consists of three\nmain components (cf.\nFigure\n1).\nThe first main component\nis a frozen audio encoder (ei-\nther Whisper or UAD), which ex-\ntracts a sequence of latent repre-\nsentations in RN from the input\naudio. The second main compo-\nnent comprises two feedforward\nmapping networks, M1 and M2,\nwhich translate the extracted au-\ndio features and the utterance-\nlevel ASR decoder signals into\nthe latent space of the token em-\nbeddings. The third main com-\nponent is a decoder-only LLM\nthat generates text given the pre-\nfix representations obtained via M1 and M2 as well as the 1-best ASR hypotheses, and a text prompt\n(i.e., \u201cdirected decision:\u201d in Figure 1).\nThe task is to generate a decision on whether an unseen utterance is directed towards a device or not.\nThe model is trained on multimodal data that contains L examples of audio, ASR decoder signals, and\n2\n1-best hypotheses\n\b\n(xi, di, ti)\n\tL\ni=1. The 1-best hypotheses are represented by a sequence of tokens\nti = (ti\n1, . . . , ti\nl), which are padded to a maximum length l. The input waveform xi is transformed\ninto log-magnitude Mel spectrogram features via the transformation F. The spectrogram feature\ninput F(xi) is then processed to obtain a sequence of embeddings in RN of length T using the audio\nencoder A (either Whisper or UAD). Mean pooling is applied to these representations along the time\ndimension to generate a single vector in RN per utterance. M1 is then used to map the aggregated\nembedding to a vector in the prefix token space of the LLM:\nai = M1\n \n1\nT\nT\nX\nt=1\nht\n!\n\u2208 R1\u00d7E, A\n\u0000F(xi)\n\u0001\n= [h1 \u00b7 \u00b7 \u00b7 hT ]\u22a4 \u2208 RT \u00d7N.\n(1)\nThe resulting representation ai has the same dimensionality E as the token embeddings. M2 is used\nto generate a latent prefix bi in RE for the decoder signal features di. The audio prefix ai and the\ndecoder signal prefix bi are then concatenated to the token embeddings of the corresponding 1-best\nhypothesis ti, and the concatenated input features are presented to the LLM. The training objective is\nto predict directedness tokens conditioned on the prefix, the decoder signals, and the 1-best hypothesis\ntokens in an autoregressive fashion. We utilize cross entropy loss to train the parameters \u03b8 of the\nmodel:\nL\u03b8 = \u2212\nL\nX\ni=1\nl\nX\nj=1\nlog p\u03b8\n\u0000ti\nj | ai, bi, ti\n1, . . . , ti\nj\u22121\n\u0001\n.\n(2)\nDuring inference, the device-directedness decision is made based on the score p\u03b8 (Y = yes | c),\nwhere Y is a discrete random variable that can take one of m tokens y1, ..., ym from the vocabulary\nV, and p\u03b8 (Y = yes | c) + p\u03b8 (Y = no | c) \u2243 1. The context c is determined by the multimodal\nfeatures, i.e., c = (ai, bi, ti\n1, . . . , ti\nj\u22121).\nLarge Language Models\nWe focus on decoder-only LLMs, since this architecture choice has\ndemonstrated stronger capabilities [31] than encoder-only and encoder-decoder systems, such as\nBERT [32] and T5 [33], on a wide range of tasks. We compare the 7B parameter versions of Falcon\n[34] and RedPajama [35] in our experiments.\nMapping Networks\nThe mapping networks M1 and M2 translate between the latent space of the\naudio encoder and the lexical embedding space of the LLM. All audio features and ASR decoder\nsignals are transformed into RE sized prefixes, where E is the latent dimension of the LLM. Both\nmapping networks share the same architecture, consisting of one hidden linear layer with E/2 units\nand hyperbolic tangent activation. The models are trained with a dropout [36] probability of 10%.\nLow-rank Adaptation\nWe employ low-rank adaption (LoRA) [23] to finetune the LLM without\ndirectly changing its weights. In the LoRA method, weights of dense layers in large pretrained\nmodels are summed with linear low-rank adapter modules. These adapter modules are small trainable\nmatrices, which are included into the architecture and optimized on behalf of the underlying LLM\nweights. We attach adapter modules to the query q and value v projection matrices, as well as the\ndense layers d of each transformer block. We employ the configuration rq = rv = d = 8, \u03b1 = 32\nand train the adapters with a dropout probability of 10%. The parameter r is the rank of the adaptation\nmatrices, and \u03b1 is a scaling factor to adjust the magnitude of the adaptation. The LoRA approach\nallows us to use less training data [37] and enables the reuse of a generic LLM deployed on a device.\nUnimodal Baselines\nUnimodal versions of our framework are trained by providing text, decoder\nsignals, or audio representations as the only input source to the LLM. In the text-only variant, the\nmapping networks M1 and M2 are removed, and the only input features are the 1-best hypotheses\nof the ASR system (cf. Figure 1). In the audio-only variant, the decoder signals including M2 and\nthe 1-best hypotheses are removed from the system. The decoder-signal-only system relies only on\nthe decoder signal input, which is transformed via M2. Hence, M1 and the 1-best hypotheses are\nremoved from the overall system.\n4\nExperiments\nData\nThe full training data is a balanced set of \u224840k directed utterances and \u224840k non-directed\nutterances, similar to the set used in [29] and [38]. The evaluation data is a combined set of two\nin-house corpora with \u224814k device-directed utterances and \u224823k non-directed utterances. The total\nduration of the evaluation data is \u224835 hours. Approximately 29% of the device-directed training\nexamples start with a trigger phrase and \u224812% of the device-directed evaluation utterances start with\na trigger phrase. The remaining device-directed utterances are triggerless interactions with a virtual\nassistant. All utterances in the training and evaluation data are randomized and anonymized. The\ndataset statistics are summarized in Table 2 of Appendix B.\n3\nTable 1: Comparison EERs on the evaluation set. \u201cUni\u201d refers to unimodal experiments, and \u201cMulti\u201d refers to\nmultimodal experiments. \u201cModality\u201d indicates the modalities used in the experiment (t = text, a = audio, b =\ndecoder signals). \u201cTrain Size\u201d shows the number of training examples used in the experiment. \u201c# Param\u201d is the\nnumber of trainable parameters. We report the sum of the parameters of the mapping networks and LoRA.\nFalcon 7B\nRedPajama 7B\nExperiment\nLoRA\nModality\nTrain\nSize\n#\nParam\nEER\nWhisper\nEER\nUAD\n#\nParam\nEER\nWhisper\nEER\nUAD\nUni 1\n\u2713\nt\n80k\n16M\n12.97%\n12.97%\n17M\n12.90%\n12.90%\nUni 2\n\u2713\na\n80k\n29M\n10.45%\n9.31%\n27M\n10.78%\n8.99%\nUni 3\n\u2713\nb\n80k\n26M\n36.90%\n36.90%\n25M\n35.04%\n35.04%\nMulti 1\n\u2713\nt, b\n80k\n26M\n13.39%\n13.39%\n25M\n12.86%\n12.96%\nMulti 2\n\u2713\na, b\n80k\n39M\n14.94%\n9.92%\n35M\n14.80%\n10.71%\nMulti 3\n\u2713\nt, a\n80k\n29M\n9.96%\n8.76%\n27M\n9.89%\n8.44%\nMulti 4\n\u2713\nt, a, b\n80k\n39M\n8.80%\n8.23%\n35M\n9.45%\n8.52%\nMulti 5 (frozen LLM)\n\u2717\nt, a, b\n80k\n23M\n10.52%\n11.49%\n18M\n10.90%\n12.26%\nMulti 4.1\n\u2713\nt, a, b\n40k\n39M\n10.19%\n8.38%\n35M\n10.20%\n8.47%\nMulti 4.2\n\u2713\nt, a, b\n20k\n39M\n10.67%\n9.05%\n35M\n10.91%\n8.84%\nMulti 4.3\n\u2713\nt, a, b\n10k\n39M\n11.71%\n8.84%\n35M\n11.66%\n9.69%\nMulti 4.4\n\u2713\nt, a, b\n5k\n39M\n12.76%\n9.77%\n35M\n12.11%\n9.65%\nMulti 4.5\n\u2713\nt, a, b\n1k\n39M\n15.39%\n12.56%\n35M\n17.09%\n11.87%\nResults and Discussion\nThe equal-error-rates (EERs) for our experiments are summarized in Table\n1. The unimodal baselines (Uni 1-3) are the text-only (t), audio-only (a), and decoder-signal-only (b)\nversions of the proposed system. Using only the audio modality (Uni 2) yields lower EERs than using\nonly the text modality (Uni 1), irrespective of the underlying LLM and audio encoder. Furthermore,\nusing the specialized UAD representations leads to lower EERs than using Whisper representations\nin experiment Uni 2. Decoder signals (Uni 3) provide the weakest overall signal (EER = 36.90%\nwith Falcon and EER = 35.04% with RedPajama). The best system configuration (Multi 4) uses\nall 80k available training examples and combines information from text, audio, as well as decoder\nsignals. Multi 4 with Falcon shows an EER of 8.80% using Whisper as the audio encoder and an\nEER of 8.23% with the UAD backbone, which translates to relative improvements of \u224816% and\n\u224812% over the corresponding audio-only models (Uni 2). In Multi 5, only M1 and M2 are trained\n(i.e., the underlying LLM is frozen and no LoRA modules are attached). This configuration shows\nworse results than Multi 4, indicating that training the mapping networks alone is not sufficient to\nachieve low EERs. The experiments Multi 4.1 to Multi 4.5 are the same as Multi 4 but with a stepwise\nreduction of the training data (from 40k examples to 1k examples). The multimodal system with\nFalcon and the UAD backbone trained on only 10k examples (Multi 4.3) still performs better than the\naudio-only model trained on 80k examples (EERs of 8.84% and 9.31%). This is not the case when\nWhisper representations are used instead (EERs of 11.71% and 10.45%). Additional experiments\nshowing the impact of using the smallest (39M) and largest (1.5B) versions of Whisper as audio\nfeature extractors can be found in Table 4 of Appendix D.\nIn contrast to other systems for device-directedness detection [11, 13, 39], our approach requires only\na small amount of training data. We see that audio representations from a pretrained Whisper model\nperform well in this low data environment. However, the model can be further improved by replacing\nthese unspecialized representations with specialized ones obtained from the smaller UAD model. This\neffect is amplified in very low data environments (see Multi 4.1-4.5). While we observe a strong EER\nincrease with less training data (e.g. from 8.80% with 80k examples to 15.39% with 1k examples\nusing Falcon) when Whisper representations are used, the EER increase is less pronounced with\nUAD representations (e.g. from 8.23% with 80k examples to 12.56% with 1k examples using Falcon).\nWe hypothesize that in low data environments the model relies more on what it already knows (i.e.,\nthe acoustic information encoded in the in-domain UAD model) and the amount of training data is\nnot sufficient to learn how the acoustic information encoded in unspecialized representations can be\nutilized accordingly.\n5\nConclusions\nIn this work, we described a multimodal model to distinguish device-directed utterances from\nbackground speech. Our approach made use of knowledge encoded in pretrained foundation models\nand effectively combined decoder signals with audio and lexical information. The system can be\ntrained on small amounts of data and operates in scenarios, where only a single frozen LLM is\navailable on a resource-constrained device. We achieved lower EERs than unimodal baselines, while\nusing only a fraction of the training data. Furthermore, low-dimensional audio representations from\na small specialized feature encoder outperformed high-dimensional general representations from a\nlarger audio foundation model and showed more stable results in environments with very low data\navailability (i.e., <80k utterances).\n4\nReferences\n[1] Siri Team, \u201cVoice trigger system for Siri.\u201d https://machinelearning.apple.com/research/\nvoice-trigger, 2023.\n[2] C. Jose, Y. Mishchenko, T. S\u00e9n\u00e9chal, A. Shah, A. Escott, and S. N. P. Vitaladevuni, \u201cAccurate Detection of\nWake Word Start and End Using a CNN,\u201d in Interspeech, 2020.\n[3] A. Ghosh, M. Fuhs, D. Bagchi, B. Farahani, and M. Woszczyna, \u201cLow-resource Low-footprint Wake-word\nDetection using Knowledge Distillation,\u201d in Interspeech, 2022.\n[4] S. Sigtia, R. Haynes, H. Richards, E. Marchi, and J. Bridle, \u201cEfficient Voice Trigger Detection for Low\nResource Hardware,\u201d in Interspeech, 2018.\n[5] S. Sigtia, E. Marchi, S. Kajarekar, D. Naik, and J. Bridle, \u201cMulti-task learning for speaker verification and\nvoice trigger detection,\u201d in ICASSP, 2020.\n[6] T. Sainath and C. Parada, \u201cConvolutional neural networks for small-footprint keyword spotting,\u201d in\nInterspeech, 2015.\n[7] A. H. Michaely, X. Zhang, G. Simko, C. Parada, and P. Aleksic, \u201cKeyword spotting for google assistant\nusing contextual speech recognition,\u201d in ASRU, 2017.\n[8] S. Cornell, T. Balestri, and T. S\u00e9n\u00e9chal, \u201cImplicit acoustic echo cancellation for keyword spotting and\ndevice-directed speech detection,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT), 2023.\n[9] D. Ng, R. Zhang, J. Q. Yip, C. Zhang, Y. Ma, T. H. Nguyen, C. Ni, E. S. Chng, and B. Ma, \u201cContrastive\nspeech mixup for low-resource keyword spotting,\u201d in ICASSP, 2023.\n[10] E. Shriberg, A. Stolcke, D. Hakkani-T\u00fcr, and L. Heck, \u201cLearning when to listen: detecting system-addressed\nspeech in human-human-computer dialog,\u201d in Interspeech, 2012.\n[11] S. H. Mallidi, R. Maas, K. Goehner, A. Rastrow, S. Matsoukas, and B. Hoffmeister, \u201cDevice-directed\nUtterance Detection,\u201d in Interspeech, 2018.\n[12] V. Garg, O. Rudovic, P. Dighe, A. H. Abdelaziz, E. Marchi, S. Adya, C. Dhir, and A. Tewfik, \u201cDevice-\nDirected Speech Detection: Regularization via Distillation for Weakly-Supervised Models,\u201d in Interspeech,\n2022.\n[13] K. Gillespie, I. C. Konstantakopoulos, X. Guo, V. T. Vasudevan, and A. Sethy, \u201cImproving device\ndirectedness classification of utterances with semantic lexical features,\u201d in ICASSP, 2020.\n[14] H. Sato, Y. Shinohara, and A. Ogawa, \u201cMulti-modal modeling for device-directed speech detection using\nacoustic and linguistic cues,\u201d Acoustical Science and Technology, vol. 44, no. 1, pp. 40\u201343, 2023.\n[15] D. Bekal, S. Srinivasan, S. Ronanki, S. Bodapati, and K. Kirchhoff, \u201cContextual Acoustic Barge-In\nClassification for Spoken Dialog Systems,\u201d in Interspeech, 2022.\n[16] R. Mokady, A. Hertz, and A. H. Bermano, \u201cClipCap: CLIP prefix for image captioning,\u201d 2021.\narXiv:2111.09734.\n[17] D. Driess et al., \u201cPaLM-E: An embodied multimodal language model,\u201d 2023. arXiv:2303.03378.\n[18] Y. Fathullah et al., \u201cPrompting large language models with speech recognition abilities,\u201d 2023.\narXiv:2307.11795.\n[19] Y. Gong, H. Luo, A. H. Liu, L. Karlinsky, and J. Glass, \u201cListen, think, and understand,\u201d 2023.\narXiv:2305.10790.\n[20] M. Kim, K. Sung-Bin, and T.-H. Oh, \u201cPrefix tuning for automated audio captioning,\u201d in ICASSP, 2023.\n[21] S. Deshmukh, B. Elizalde, R. Singh, and H. Wang, \u201cPengi: An audio language model for audio tasks,\u201d\n2023. arXiv:2305.11834.\n[22] X. L. Li and P. Liang, \u201cPrefix-tuning: Optimizing continuous prompts for generation,\u201d in Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), (Online), pp. 4582\u20134597,\nAssociation for Computational Linguistics, Aug. 2021.\n[23] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \u201cLoRA: Low-rank\nadaptation of large language models,\u201d in ICLR, 2022.\n[24] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to-end speech recognition using\nmulti-task learning,\u201d in ICASSP, 2017.\n[25] M. Bleeker, P. Swietojanski, S. Braun, and X. Zhuang, \u201cApproximate Nearest Neighbour Phrase Mining\nfor Contextual Speech Recognition,\u201d in Interspeech, 2023.\n[26] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: End-to-end speech recognition using deep RNN models\nand WFST-based decoding,\u201d in ASRU, 2015.\n5\n[27] D. Povey, M. Hannemann, G. Boulianne, L. Burget, A. Ghoshal, M. Janda, M. Karafi\u00e1t, S. Kombrink,\nP. Motl\u00ed\u02c7cek, Y. Qian, K. Riedhammer, K. Vesel\u00fd, and N. T. Vu, \u201cGenerating exact lattices in the WFST\nframework,\u201d in ICASSP, 2012.\n[28] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition\nvia large-scale weak supervision,\u201d 2022.\n[29] O. Rudovic, W. Chang, V. Garg, P. Dighe, P. Simha, J. Berkowitz, A. H. Abdelaziz, S. Kajarekar, E. Marchi,\nand S. Adya, \u201cLess is more: A unified architecture for device-directed speech detection with multiple\ninvocation types,\u201d in ICASSP, 2023.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin,\n\u201cAttention is all you need,\u201d in NeurIPS, 2017.\n[31] T. B. Brown et al., \u201cLanguage models are few-shot learners,\u201d 2020. arXiv:2005.14165.\n[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional transformers\nfor language understanding,\u201d in NAACL, 2019.\n[33] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring\nthe limits of transfer learning with a unified text-to-text transformer,\u201d JMLR, vol. 21, no. 140, pp. 1\u201367,\n2020.\n[34] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow,\nJ. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo, \u201cFalcon-40B: an open large language model\nwith state-of-the-art performance,\u201d 2023.\n[35] Together Computer, \u201cRedPajama: An open source recipe to reproduce LLaMA training dataset,\u201d 2023.\n[36] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: A simple way\nto prevent neural networks from overfitting,\u201d Journal of Machine Learning Research, vol. 15, no. 56,\npp. 1929\u20131958, 2014.\n[37] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei, \u201cScaling laws for neural language models,\u201d 2020.\n[38] P. Dighe, P. Nayak, O. Rudovic, E. Marchi, X. Niu, and A. Tewfik, \u201cAudio-to-intent using acoustic-textual\nsubword representations from end-to-end asr,\u201d in ICASSP, 2023.\n[39] O. Rudovic, A. Bindal, V. Garg, P. Simha, P. Dighe, and S. Kajarekar, \u201cStreaming on-device detection of\ndevice directed speech from voice and touch-based invocation,\u201d in ICASSP, 2022.\n6\nAppendix\nA\nAcknowledgements\nWe would like to express our sincere gratitude to John Bridle, Pranay Dighe, Sachin Kajarekar, Oggi\nRudovic, Ahmed Tewfik and Barry Theobald for their support and their comprehensive feedback on\nthis work. We also thank Seanie Lee for the numerous helpful discussions.\nB\nDatasets\nTable 2: Summary of the data used in our experiments.\nLabel\nExamples\n(#)\nTotal Duration\n(hours)\nDuration per Utterance\n(seconds)\nWords per Utterance\n(#)\nTrain\nEval\nTrain\nEval\nTrain\nEval\nTrain\nEval\nDirected\n40568 14396\n58.88\n12.05\n5.22\u00b16.97\n3.01\u00b11.89\n5.34\u00b13.26\n5.50\u00b13.64\nNon-directed 40062 22958\n67.31\n23.37\n6.04\u00b15.33\n3.66\u00b13.67\n6.48\u00b19.29 9.42\u00b117.28\nCombined\n80630 37354 126.19\n35.42\n5.63\u00b16.22\n3.41\u00b13.12\n5.90\u00b16.97 7.89\u00b113.81\nC\nExamples\nTable 3: Examples of 1-best hypotheses of device-directed and non-directed utterances.\nDirected\n\u201cSet an alarm for 8 AM\u201d\nDirected\n\u201cTell me a joke\u201d\nDirected\n\u201cWhat\u2019s the temperature\u201d\nNon-directed\n\u201cExcellent thank you very much\u201d\nNon-directed\n\u201cCan we talk\u201d\nNon-directed\n\u201cI was trying to do it\u201d\nD\nAdditional Experiments\nTable 4: Impact of changing the size of the Whisper audio foundation model. \u201cWhisper Tiny\u201d has\n\u224839M parameters and the audio representation dimension is R384. \u201cWhisper Large\u201d has \u22481.5B\nparameters and the audio representation dimension is R1280. The LoRA configuration is the same as\nin Table 1 (rq = rv = d = 8, \u03b1 = 32).\nFalcon 7B\nRedPajama 7B\nExperiment\nModality\nTrain\nSize\nEER\nWhisper Tiny\nEER\nWhisper Large\nEER\nWhisper Tiny\nEER\nWhisper Large\nUni 2\na\n80k\n14.66%\n10.14%\n13.84%\n9.76%\nMulti 4\nt, a, b\n80k\n10.56%\n10.03%\n9.75%\n9.02%\n7\nTable 5: Alternative LoRA configuration for Falcon 7B and RedPajama 7B. LoRA modules are only\nattached to q and v and rq = rv = 64, \u03b1 = 16 is used.\nFalcon 7B\nRedPajama 7B\nExperiment\nModality\nTrain\nSize\n#\nParam\nEER\nWhisper\nEER\nUAD\n#\nParam\nEER\nWhisper\nEER\nUAD\nUni 1\nt\n80k\n19M\n12.59%\n12.59%\n33M\n12.88%\n12.88%\nUni 2\na\n80k\n32M\n10.53%\n12.52%\n43M\n10.92%\n9.16%\nMulti 3\nt, a\n80k\n32M\n9.33%\n9.36%\n43M\n9.44%\n8.45%\nMulti 4\nt, a, b\n80k\n42M\n10.13%\n10.55%\n51M\n9.72%\n9.75%\nMulti 4.1\nt, a, b\n40k\n42M\n9.97%\n11.10%\n51M\n9.86%\n9.19%\nMulti 4.2\nt, a, b\n20k\n42M\n11.07%\n11.55%\n51M\n10.63%\n9.73%\nMulti 4.3\nt, a, b\n10k\n42M\n10.91%\n12.96%\n51M\n12.00%\n9.95%\nMulti 4.3\nt, a, b\n5k\n42M\n12.53%\n12.49%\n51M\n12.08%\n10.81%\nMulti 4.5\nt, a, b\n1k\n42M\n17.63%\n13.36%\n51M\n14.71%\n14.17%\nTable 6: Alternative LoRA configurations for Falcon 7B. Note that the configuration in the middle\ncolumn (rq = rv = d = 8, \u03b1 = 32) is the same as in Table 1.\nrq = rv = 8, \u03b1 = 32 rq = rv = d = 8, \u03b1 = 32 rq = rv = d = 64, \u03b1 = 16\nExperiment Modality Train\nSize\nEER\nWhisper\nEER\nUAD\nEER\nWhisper\nEER\nUAD\nEER\nWhisper\nEER\nUAD\nUni 1\nt\n80k\n12.53%\n12.53%\n12.97%\n12.97%\n12.47%\n12.47%\nUni 2\na\n80k\n10.33%\n13.81%\n10.45%\n9.31%\n10.95%\n9.25%\nUni 3\nb\n80k\n34.24%\n34.24%\n36.90%\n36.90%\n35.42%\n35.42%\nMulti 1\nt, b\n80k\n13.19%\n13.19%\n13.39%\n13.39%\n12.99%\n12.99%\nMulti 2\na, b\n80k\n16.85%\n10.49%\n14.94%\n9.92%\n13.48%\n10.35%\nMulti 3\nt, a\n80k\n9.09%\n9.08%\n9.96%\n8.76%\n9.51%\n8.00%\nMulti 4\nt, a, b\n80k\n9.17%\n10.92%\n8.80%\n8.23%\n9.09%\n8.00%\nTable 7: Alternative LoRA configuration for RedPajama 7B. Note that the configuration in the middle\ncolumn (rq = rv = d = 8, \u03b1 = 32) is the same as in Table 1.\nrq = rv = 8, \u03b1 = 32 rq = rv = d = 8, \u03b1 = 32 rq = rv = d = 64, \u03b1 = 16\nExperiment Modality Train\nSize\nEER\nWhisper\nEER\nUAD\nEER\nWhisper\nEER\nUAD\nEER\nWhisper\nEER\nUAD\nUni 1\nt\n80k\n13.15%\n13.15%\n12.90%\n12.90%\n12.87%\n12.87%\nUni 2\na\n80k\n10.76%\n8.93%\n10.78%\n8.99%\n11.40%\n8.82%\nUni 3\nb\n80k\n33.66%\n33.66%\n35.04%\n35.04%\n35.70%\n35.70%\nMulti 1\nt, b\n80k\n13.00%\n13.00%\n12.86%\n12.96%\n13.08%\n13.26%\nMulti 2\na, b\n80k\n13.12%\n10.00%\n14.80%\n10.71%\n13.90%\n10.72%\nMulti 3\nt, a\n80k\n9.84%\n9.70%\n9.89%\n8.44%\n9.57%\n8.27%\nMulti 4\nt, a, b\n80k\n9.43%\n9.76%\n9.45%\n8.52%\n9.37%\n8.55%\n8\n"
  },
  {
    "title": "Language-Informed Visual Concept Learning",
    "link": "https://arxiv.org/pdf/2312.03587.pdf",
    "upvote": "4",
    "text": "Preprint\nLANGUAGE-INFORMED VISUAL CONCEPT LEARNING\nSharon Lee\u2217\nYunzhi Zhang\u2217\nShangzhe Wu\nJiajun Wu\nStanford University\nABSTRACT\nOur understanding of the visual world is centered around various concept axes,\ncharacterizing different aspects of visual entities. While different concept axes\ncan be easily specified by language, e.g., color, the exact visual nuances along\neach axis often exceed the limitations of linguistic articulations, e.g., a particular\nstyle of painting. In this work, our goal is to learn a language-informed visual con-\ncept representation, by simply distilling large pre-trained vision-language models.\nSpecifically, we train a set of concept encoders to encode the information pertinent\nto a set of language-informed concept axes, with an objective of reproducing the\ninput image through a pre-trained Text-to-Image (T2I) model. To encourage better\ndisentanglement of different concept encoders, we anchor the concept embeddings\nto a set of text embeddings obtained from a pre-trained Visual Question Answering\n(VQA) model. At inference time, the model extracts concept embeddings along\nvarious axes from new test images, which can be remixed to generate images with\nnovel compositions of visual concepts. With a lightweight test-time finetuning\nprocedure, it can also generalize to novel concepts unseen at training. Project\npage at https://cs.stanford.edu/\u02dcyzzhang/projects/concept-axes.\n1\nINTRODUCTION\nIn order to make sense of the myriad visual entities in the world, humans develop an abstracted\ngenerative model of them and organize the underlying sources of variation into visual concepts,\nsuch as different colors or different types of objects. Designing systems that can recognize visual\nconcepts within images as humans do has been a longstanding goal in the fields of computer vision\nand artificial intelligence (Russakovsky et al., 2015; Krizhevsky et al., 2012; Girshick et al., 2014).\nTo facilitate efficient reasoning and communication of these concepts, humans created symbolic\ndepictions that have evolved into natural language. Such natural language grounding of visual data\nhas been instrumental in the recent proliferation of powerful large vision-language models that are\ncapable of semantically identifying objects in images (Radford et al., 2021; Kirillov et al., 2023) or\ngenerating photo-realistic images from arbitrary text prompts (Ramesh et al., 2021; Rombach et al.,\n2022; Saharia et al., 2022; Yu et al., 2022). While different concept axes can be easily specified\nby words, such as category and style, it is much less intuitive to delineate the subtleties of\nlow-level visual nuances along each axis using language, such as one particular style of a painting.\nIn this work, our goal is to distill from large pre-trained vision-language models a function that\nextracts visual concepts along a set of language-specified concept axes from images. As illustrated in\nFigure 1, once these concepts are extracted, we can recompose them across different image instances\nat inference time to produce new images with novel concept combinations. To learn this function,\nrather than collecting a large-scale dataset of human annotations for each specific visual concept,\nwe design a language-informed visual concept representation, and simply distill from a pre-trained\nText-to-Image (T2I) generation model. There are three fundamental properties we seek in this visual\nconcept representation.\nFirst, unlike T2I generation, which relies on generic words as visual concept descriptors, we would\nlike to capture fine-grained visual nuances using continuous concept embeddings. One common\ntechnique is to invert the text-to-image generation process by optimizing an embedding with the\nobjective of reproducing a given input image using a pre-trained T2I model, often referred to as\nTextual Inversion (Gal et al., 2022). However, most existing Textual Inversion methods (Gal et al.,\n\u2217Equal contribution; alphabetically ordered.\n1\narXiv:2312.03587v1  [cs.CV]  6 Dec 2023\nPreprint\n<category>\nTraining Data\n<color> <material>\n<category>\n<color>\n<material>\n<color>\n<category> <material>\n<material>\n<category>\n<color>\ncomposed\ninput\nInference\nConcept Recomposition\ninput\nFigure 1: Language-Informed Visual Concept Learning. Our goal is to learn a visual concept\nrepresentation grounded on a set of language-informed concept axes, e.g.,category, color, and\nmaterial, by simply distilling from pre-trained text-to-image generation models without manual\nannotations. After training, the concept encoders extract disentangled axis-specific embeddings\nfrom an image, which can be remixed to generate new images with novel concept compositions.\n\u201ca photo of red banana\u201d\n\u201ca photo of <color> <category>\u201d\n<color>\n<category>\nRecomposition\nSource\nVanilla text prompt\nOurs\nBaseline\nFigure 2: Learned Disentangled Concept Embeddings Improve Compositionality. Left: Vanilla\ntext-to-image model may fail to adhere to text prompts of uncommon combinations of concepts\neven with prompt engineering, e.g. \u201cred banana\u201d. Right: With the same backbone T2I generator, our\nlearned disentangled concept embeddings greatly enhance concept compositionality.\n2022) optimize embeddings for individual image instances independently, overlooking the shared\nnature of visual concepts across instances. For instance, the concept of \u201cred\u201d is shared between a\n\u201cred apple\u201d and a \u201cred dress\u201d. Moreover, the concepts of \u201cred\u201d and \u201cyellow\u201d also are instances of\nthe property of color.\nHence, the second desired property of the visual concept representation is to preserve such common\nconcept structures among various visual instances. Instead of optimizing on individual image in-\nstances independently, we design a set of concept encoders, where each encoder learns to encode\nthe visual characteristics of an input image pertaining to one concept axis specified by language.\nThis ensures that the inverted concept embeddings can be shared across different instances and\nremixed to generate new images.\nThe third crucial aspect of this representation is to ascertain that different concept axes are disentan-\ngled, allowing for changes to be made specifically on single concept axis without modifying other\naxes. To do so, we reuse the disentangled nature of linguistic concepts and ground the predictions to\na set of discrete text anchors in the concept embeddings space, which can be obtained by querying a\npre-trained generic Visual Question Answering (VQA) model, e.g., BLIP-2 (Li et al., 2023b). This\nsoft anchoring constraint significantly improves the disentanglement of concept embeddings across\ndifferent axes while still retaining sufficient leeway to capture nuanced visual variations that BLIP-2\nstruggles to discern, e.g., the style of an art piece in Figure 6.\nPutting these ideas together, we design a generic framework for learning disentangled and composi-\ntional visual concepts grounded to linguistic structures by exploiting pre-trained text-to-image gen-\neration and visual question answering models. We show that these concept encoders can be trained\npurely on synthetic images generated by a pre-trained T2I model, and extract concept embeddings\nfrom real images at test time, which capture the fine-grained visual nuances.\nOur contributions can be summarized as follows:\n1. We propose a generic framework for learning language-informed visual concepts by simply\ndistilling pretrained vision-language models.\n2. At inference time, the trained concept encoders extract concept embeddings from a test image,\nwhich can be remixed to generate images with novel compositions of concepts.\n3. Using a light-weight test-time finetuning procedure, these encoders can also be quickly\nadapted to extract novel concepts unseen during training.\n2\nPreprint\n4. Experiments show that this visual concept representation achieves better disentanglement and\ncompositionality, compared to text-based prompting baselines, as shown in Figures 2 and 6.\n2\nRELATED WORK\n2.1\nVISUAL CONCEPT LEARNING\nDesigning learning-based systems to discover various visual concepts in natural images has been a\nlong-standing goal in machine perception and intelligence. Early attempts typically rely on exten-\nsive semantic annotations done by humans, such as object classification (Barnard et al., 2003; Fei-\nFei et al., 2006; Fergus et al., 2005), which were later epitomized by the effort of ImageNet (Rus-\nsakovsky et al., 2015). Visual concepts are intrinsically linked to concepts in language, and such\nend-to-end supervised learning paradigms can be seen as learning a direct mapping between visual\nconcepts and discrete linguistic concepts. Other approaches attempt to better exploit this inherent\nstructure in language by constructing a structured representation of visual concepts such as scene\ngraphs (Zhong et al., 2021) and symbolic programs (Mao et al., 2019; Han et al., 2019).\nMore recently, the success of natural language modeling (Devlin et al., 2018; Brown et al., 2020;\nRaffel et al., 2020) has paved the way for grounding visual concepts to open vocabularies, unlike\ncategory labels or fixed symbolic programs, by training large Vision-Language Models (VLMs) on\nmassive image captioning datasets (Schuhmann et al., 2022). This has powered recent Text-to-Image\n(T2I) generation models to turn linguistic concepts from free-form text prompts into photo-realistic\nimages (Rombach et al., 2022; Saharia et al., 2022). These T2I models have been leveraged by\nPersonalization methods for extracting individual visual concepts from one or a few images. This is\ndone by either by optimizing token embeddings (Gal et al., 2022; Vinker et al., 2023; Avrahami et al.,\n2023; Chefer et al., 2023b; Liu et al., 2023), finetuning the backbone denoiser (Ruiz et al., 2023), or\ntraining additional encoders for amortized optimization (Gal et al., 2023; Arar et al., 2023; Li et al.,\n2023a). We also distill visual concepts from a pre-trained T2I model, but unlike existing works,\nwe train encoders to adhere to a set of language-specified concept axes, preserving the disentangled\nand compositional nature of language. Ranasinghe & Ryoo (2023) also explores language-defined\nconcepts but focuses on video action recognition tasks while we focus on image generation.\nA separate line of work focuses on unsupervised visual concept disentanglement without explicitly\nleveraging language, typically by simply imposing information constraints in the latent space of a\ngenerative model, like VAEs and GANs (Higgins et al., 2017; Chen et al., 2016; Hsu et al., 2023).\nHere, we are interested in learning visual concepts that are explicitly grounded to language.\n2.2\nCONTROLLABLE IMAGE GENERATION\nThe success of GAN-based image generation (Goodfellow et al., 2014; Brock et al., 2018; Karras\net al., 2019) has spawned a series of works that discover controllable directions in the GAN la-\ntent space (Voynov & Babenko, 2020; H\u00a8ark\u00a8onen et al., 2020). More recently, the advancements\nof diffusion-based T2I models have unlocked new possibilities for controllable image generation,\nwhere photo-realistic images can be generated from free-form text prompts. Recent works pro-\nposed to further improve the alignment of image samples and input text conditions by manipulating\nattention maps within T2I models (Chefer et al., 2023a; Epstein et al., 2023). Another form of\ncontrollable image generation is compositional generation. Liu et al. (2022) proposes to improve\nthe quality of T2I diffusion models for composing multiple pre-given concepts, specified via text\nprompts, by modifying the inference procedure. In this work, instead of assuming that concepts are\ngiven and are in a text format, we tackle the task of identifying disentangled concepts which can be\nused for composition.\nImage generation can also be controlled with image analogies (\u02c7Subrtov\u00b4a et al., 2023; Hertzmann\net al., 2001), a form of visual prompting. These works typically do not explicitly extracts visual\nconcepts from inputs unlike ours. In this work, we amalgamate both visual prompts and text queries,\nemploying them as the editing interface.\n3\nMETHOD\nFig. 3 gives an overview of our proposed learning framework. Our goal in this work is to ex-\ntract visual concepts from images along a number of concept axes specified by language, such as\n3\nPreprint\nInput Image\nCategory\nEncoder\n\ud835\udc53!\nColor\nEncoder\n\ud835\udc53\"\nMaterial\nEncoder\n\ud835\udc53#\n\u2026\nVisual Question Answering (VQA)\nQ: \u201cWhat object?\u201d\nA: \u201cChair\u201d\nQ: \u201cWhat color?\u201d\nA: \u201cYellow\u201d\nQ: \u201cWhat material?\u201d\nA: \u201cWood\u201d\n\u2026\nConcept \nEncoders\nText-to-\nImage (T2I)\nReproduced\nImage \n< \ud835\udc1e!\n>\nT2I\n(category)\n< \ud835\udc1e\"\n>\nT2I\n(color)\n< \ud835\udc1e#\n>\nT2I\n(material)\n\u201ca\nin\ncolor &\nmaterial \u2026\u201d\n< \ud835\udc1e!\n>\n< \ud835\udc1e\"\n>\n< \ud835\udc1e#\n>\n\ud835\udcdb$%&'(\n\u21d2\n\ud835\udcdb#\n)(&*'$\n\ud835\udcdb\"\n)(&*'$\n\ud835\udcdb!\n)(&*'$\nPretrained \nEmbedding\nSpace\nFigure 3: Training Pipeline. During training, an input image is processed by a set of concept\nencoders that predict concept embeddings specific to given concept axes. These embeddings are\ntrained to (1) retain information in order to reproduce visual inputs via a pre-trained Text-to-Image\nmodel given an axis-informed text template, and (2) ensure disentanglement across different axes by\nanchoring to text embeddings obtained from a pre-trained Visual Question Answering model.\ncategory, color, and material, so as to enable the flexible composition of concepts into\nhigh-quality image generations.\nTo achieve this, we train a set of visual concept encoders by distilling concept guidance from pre-\ntrained vision-language models. Specifically, the encoders are trained to extract concept embeddings\nfrom an image in order to fulfill two objectives. First, they should be recomposed to explain the input\nimage through a pretrained text-to-image (T2I) generation model, given a concept-axis-informed\ntext prompt. Second, these visual concept embeddings should be anchored to the corresponding text\nembeddings obtained from a pre-trained visual question answering (VQA) model, further exploiting\nthe disentangled nature of linguistic concepts for better disentanglement of visual concepts.\n3.1\nVISUAL CONCEPT ENCODING BY INVERTING TEXT-TO-IMAGE GENERATION\nOur understanding of the visual world is centered around various concept axes, to which we have of-\nten assigned words due to their significance in communication and reasoning. This vision-language\ngrounding has fueled recent explosion of text-to-image generation models (Rombach et al., 2022;\nSaharia et al., 2022; Ramesh et al., 2022), allowing them to generate photo-realistic images with\nvarious combinations of concepts defined by words.\nHere, we are interested in the reverse direction of text-to-image generation, where the goal is to\nextract language-grounded visual concepts present in natural images. Specifically, given K concept\naxes of interest defined by language, we would like to learn K concept encoders {fk(\u00b7)}K\nk=1, each\nof which extracts a concept representation ek = fk(x) along a concept axis from an input image x.\nIn order to train these concept encoders {fk(\u00b7)}, instead of relying on extensive human labeling,\nwe opt to exploit the vision-language grounding embedded within large pre-trained T2I generation\nmodels. Using the technique of Textual Inversion (Gal et al., 2022), one can optimize a token\nembedding <*> to capture a visual entity in a given image, through the objective of regenerating\nthe image with the T2I model from a text template, such as \u201ca photo of <*>\u201d. Here, we adopt a\nsimilar objective, but instead of inverting a specific embedding capturing the overall \u201cidentity\u201d of an\nindividual image instance, we would like to predict embeddings ek that are grounded to a number\nof meaningful concept axes, using an axis-informed text template, such as \u201ca photo of <e1> with\n<e2> color and <e3> material\u201d. This allows the extracted concept embeddings to be shared across\ndifferent images, encapsulating the common visual characteristics pertinent to one concept axis.\nSpecifically, given an image x, the concept encoders {fk(\u00b7)} extract a set of concept embeddings\n{ek \u2208 RD}, which have the same dimension D as the text embeddings so that they can be directly\ninserted into the text embeddings of the axis-informed text template. To simplify the notations, let\nf\u03b3(\u00b7) denote the function that takes in the image and produces the final sequence of embeddings\n4\nPreprint\nof the template and the predicted concept embeddings, and \u03b3 be the parameters of all the encoders\nwhich will be optimized during training. Let c\u03b8 be the part of the T2I model\u2019s text encoder that takes\nin a sequence of text embeddings and outputs a conditioning vector for the T2I model\u2019s denoising\nnetwork \u02c6\u03f5\u03b8, where \u03b8 denotes network parameters. We use DeepFloyd (StabilityAI; Saharia et al.,\n2022) as the backbone T2I model, which utilizes a pre-trained T5 model (Raffel et al., 2020) as the\ntext encoder, and keep the parameters \u03b8 frozen in all experiments. To train the encoders, we reuse\nthe training objective for the backbone diffusion model:\nLrecon(x; \u03b3) = E\u03f5\u223cN (0,I),t\u223cU([0,1])\n\u0002\n\u2225\u02c6\u03f5\u03b8(x, t, c\u03b8(f\u03b3(x))) \u2212 \u03f5\u22252\n2\n\u0003\n,\n(1)\nwhere the noise \u03f5 is sampled from a standardmultivariate Gaussian distribution and the timestep\nt is sampled from a uniform distribution in [0, 1]. Minimizing Lrecon amounts to finding concept\nembeddings within the space of the pre-trained T2I model that can best reproduce the input image\nx, resembling a \u201creconstrucion\u201d objective.\nCompared to per-instance token optimization in vanilla Textual Inversion, the advantages of training\nthese concept encoders are two-fold. First, the concept embedding space is naturally shared across\ndifferent image instances, encapsulating the common understanding of the corresponding concept\naxes. Second, it makes training more efficient by amortizing the optimization across all instances,\nand more crucially, it allows for test-time inference in a feed-forward pass.\n3.2\nCONCEPT DISENTANGLEMENT USING TEXT ANCHORS\nThe objective of Lrecon ensures that the extracted concept embeddings can sufficiently reconstruct the\nconcept of a given image through a pre-trained text-to-image generation model. However, with this\nloss alone, there is little guarantee that each embedding encodes only the information pertinent to a\nparticular concept axis. In practice, we found that this baseline results in poor disentanglement of\ndifferent concept axes when remixing the concept embeddings to generate new images, potentially\ndue to the imprecise vision-language grounding in the pre-trained T2I model. For instance, as shown\nin Figure 8, the extracted category embedding <e1> for \u201cred berries\u201d cannot be remixed with\nvarious color embeddings <e2> e.g., \u201corange\u201d, as <e1> is highly entangled with the concept of a\n\u201cred\u201d color due to the bias in natural images.\nTo encourage better disentanglement of different concept axes, we further incorporate a sparse set\nof text anchors into the concept embedding space. Along each concept axis like color, we have\noften named some prominent modes, such as \u201cred\u201d or \u201cyellow\u201d, and these text labels entail clearly\ndisentangled concepts. Therefore, we would like to reuse this disentangled nature of linguistic\nconcepts to improve the disentanglement of visual concepts. To this end, we make use of the text\npredictions from a pre-trained Visual Question Answering (VQA) model, BLIP-2 (Li et al., 2023b),\nas pseudo ground-truth anchors for the concept embeddings.\nSpecifically, for each training image x and for each concept axis of interest (e.g., color) indexed\nby k, we query the BLIP-2 model \u03a8 with the image x and a question qk in natural language that\nis specific to this concept axis, e.g., \u201cwhat is the color of the object in the image\u201d. Denote the\nanswer from BLIP-2, also in the form of natural language, as \u03a8(x, qk). We encode this answer with\nthe pre-trained text encoder c\u03b8 to obtain a text embedding \u02dcek = c\u03b8(\u03a8(x, qk)). The prediction of our\nconcept encoders fk,\u03b3 is encouraged to stay close to this anchor text embedding:\nLanchor\nk\n(x; \u03b3) = \u2225fk,\u03b3(x) \u2212 \u02dcek\u22252\n2,\nwhere\n\u02dcek = c\u03b8(\u03a8(x, qk)).\n(2)\nIt is crucial to highlight that we use these BLIP-2 predictions only as anchors by assigning a small\nweight to this anchor loss Lanchor\nk\nduring training. Otherwise, the embeddings predicted by the\nconcept encoders could easily collapse to a set of discrete text embeddings and fail to capture the\nvisual nuances in images.\n3.3\nTRAINING AND INFERENCE\nTraining.\nGiven a collection of training images D containing various combinations of concepts\nalong each axis, the final objective to train the concept encoders consists of the two parts:\nLtotal(\u03b3) = Ex\u223cD\n\"\nLrecon(x; \u03b3) +\nK\nX\nk=1\n\u03bbkLanchor\nk\n(x; \u03b3)\n#\n.\n(3)\n5\nPreprint\nInference.\nAt inference time, given a new test image, the concept encoders extract embeddings\n{ek} capturing its characteristics along each concept axis of interest. These embeddings can be\nremixed across different images, or be replaced by embeddings converted from explicit words, to\nproduce images with new compositions of visual concepts through the backbone T2I generator.\nGeneralization to Unseen Concepts via Test-Time Finetuning.\nWhile the encoders can pre-\ncisely extract an axis-specific concept that has been seen during training from a new test image, they\ntend to be less robust to concepts unseen at training. However, with a lightweight test-time optimiza-\ntion procedure, where we use only the reconstruction objective Lrecon to update the parameters for\nall encoders, \u03b3, these encoders can generalize to novel concepts unseen during training. Note that\nLanchor is omitted here in order to capture the visual nuances without over-committing to the coarse\ntext anchors. After training, the encoders have learned to generate outputs within a relatively narrow\nregion of the embedding space, which allows the model to adapt to the test images shown in Figure 5\nwithin around 600 iterations while maintaining disentanglement and compositional capability.\n4\nEXPERIMENTS\n4.1\nEXPERIMENT SETUP\nTraining Data Generation.\nWe train the concept encoders only using synthetic images generated\nby DeepFloyd from 5 different domains, including fruits, figurines, furniture, art, and clothing.\nMore details of our dataset can be found in A.2. For each dataset, we consider 2-3 concept axes, such\nas category, color, material, style, and season. For example, considering category\nand color for the fruits dataset, we generate training images by prompting DeepFloyd with text\nprompts describing varying combinations of categories and colors, e.g. \u201ca photo of an apple which\nis red in color\u201d. Note that these text prompts are used only for data generation and not for training,\nas they may not be reliable (Figure 2). On average, we obtain 669 training images for each dataset.\nImplementation Details.\nInspired by Gal et al. (2023), we leverage a pre-trained CLIP ViT/L-\n14 model for image encoding (Radford et al., 2021; Dosovitskiy et al., 2021), which was trained\nwith a contrastive objective aligning image and text features, and hence well-suited for our task.\nWe extract image features from CLIP ViT and train K separate concept encoders fk on top of the\nfeatures, which share the same architecture but maintains separate weights. Specifically, we take\nthe [CLS] tokens from each CLIP layer and process each token with a distinct linear layer. This\nis different from Gal et al. (2023), which extracts [CLS] tokens from every other layer and uses a\nsingle shared linear layer for all token features. The transformed features are then aggregated with\naverage pooling followed by a LeakyReLU (Xu et al., 2015) activation, and passed into another\nlinear layer that produces the final predicted concept embeddings.\nTo ground the concept embeddings to the concept axes, we adapt the text templates from CLIP (Rad-\nford et al., 2021), which were originally used to assemble captions with class categories from Im-\nageNet (Russakovsky et al., 2015). For training, we use AdamW (Loshchilov & Hutter, 2017)\noptimizer with learning rate 0.02, and randomly flip the images horizontally. For test-time finetun-\ning, we use the AdamW optimizer with learning rate 0.001. We set \u03bbk = 0.0001 (Equation (3)) for\nthe category axis and \u03bb = 0.001 for others. We use IF-I-XL from DeepFloyd as the backbone\nmodel, with training resolution 64 \u00d7 64. Training on one dataset takes approximately 12 hours on\none NVIDIA GeForce RTX 3090 GPU. Generated images are upsampled 256 \u00d7 256 using IF-II-L\nfor visualization purpose.\n4.2\nQUALITATIVE RESULTS\nVisual Concept Extraction, Recomposition and Extrapolation.\nOnce trained, the concept en-\ncoders can extract disentangled concept embeddings specific to each concept axis from different\ntest images, which can recomposed to generate new images with various concept compositions. As\nshown in Figure 4, across various datasets, our method is able to recompose axis-specific visual\nconcepts from different images and consistently generate new images depicting the recomposed\nconcepts. More examples can be found in Appendix A.1, where images generated from individual\ndecomposed concept embeddings are also presented.\nThis disentangled concept representation also allows us to extrapolate along a particular concept\naxis for visual exploration, as shown in Figure 7. For instance, we can ask BLIP-2 \u201cwhat is the style\n6\nPreprint\n(a) Fruits\nSource <category>\nSource <style>\n(c) Furniture\nSource <material>\nSource <color>\n(b) Paintings\nSource <category>\nSource <material>\nSource <category>\nSource <color>\nSource <category>\nSource <color>\nFigure 4: Concept Recomposition. At test time, our model extracts visual concepts along various\naxes from different images and recompose them to generate new images. We show recomposition\nresults across different pairs of concept axes in 3 datasets: (a) Fruits, (b) Paintings, and (c) Furniture.\n<category\u2019>\n<category\u2019> + <color>\n<category\u2019>\n<category\u2019> + <style>\nTrain Images\n<style>\n<category>\nTest Image\nTrain Images\n<color>\n<category>\nTest Image\nFigure 5: Generalization to Unseen Concepts via Finetuning. After test-time fine-tuning on a\nsingle test-time image, encoders can adapt to novel concept. Visual details from the input images\nare preserved as can be seen from images visualizing embedding predictions. Importantly, these\nembeddings do not overfit to the input images and maintain a good disentanglement, such that they\ncan be freely recomposed to create new concepts. More results can be found in Figures 9 to 13.\nMore real-world results can be found in Figures 20 to 24.\nof the painting?\u201d in an image, and prompt GPT-4 (OpenAI, 2023) to name a few alternatives. We\ncan then recompose the text embeddings of these alternative styles with our concept embeddings\nand generate images to visualize these variants. Representing concepts as continuous embeddings\nfurther enables concept interpolations. Details and results are shown in Appendix A.6.\nGeneralization to Unseen Concepts via Test-Time Finetuning.\nAlthough the encoders have\nonly seen a limited range of concepts during training due to the small size of the training dataset,\nit can be quickly adapted to unseen concepts with the lightweight test-time finetuning procedure in\nSection 3.3, as shown in Figure 5. For instance, after 600 finetuning iterations, the model can extract\nthe specific style of the dog painting unseen at training and compose it with the content from other\nimages. It can also capture nuanced colors e.g. yellow-ish-orange and transfer them to other objects.\n7\nPreprint\nSource \nImage\nBaseline \nw/ BLIP-2\nOurs\nChange to\n<color>\n\u201cpineapple\u201d \u201cstrawberry\u201d \u201cbanana\u201d\n<color>\n\u201clemon\u201d\n\u201cbanana\u201d\n\u201capple\u201d\n<style>\n\u201ctree\u201d\n\u201capple\u201d\n\u201cface\u201d\nInstruct-\nPix2Pix\nNull-text \nInversion\nFigure 6: Baselines Comparisons. Compared the text-based image editing methods, our method\nachieves significantly better compositionality due to the disentangled concept representation, and\ncaptures fine-grained color variations, which the baseline struggles to encode with language.\nCLIP-Score \u2191\nHuman Evaluation \u2191\nEdit Category\nEdit Color\nEdit Cat.\nEdit Clr.\nCat.&Clr.\nCat.\nClr.\nCat.&Clr.\nCat.\nClr.\nScore\nScore\nNull-text Inversion\n0.258\n0.249\n0.249\n0.259\n0.265\n0.223\n0.287\n0.316\nInstructPix2Pix\n0.267\n0.277\n0.226\n0.270\n0.245\n0.268\n0.233\n0.648\nBaseline w/ BLIP-2\n0.313\n0.294\n0.248\n0.287\n0.271\n0.237\n0.448\n0.379\nOurs\n0.308\n0.297\n0.238\n0.302\n0.287\n0.236\n0.968\n0.840\nw/o Lanchor\nk\n0.268\n0.276\n0.219\n0.263\n0.257\n0.236\n-\n-\nw/o Encoder & Lanchor\nk\n0.288\n0.300\n0.214\n0.242\n0.213\n0.265\n-\n-\nTable 1: Quantitative Comparisons on Visual Concept Editing. Compared to existing image\nediting baselines (Brooks et al., 2023; Mokady et al., 2022), our method achieves better overall\nCLIP score when editing either axis, and is particularly effective at retaining category-related\nconcepts as reflected in human evaluation. \u2018Cat\u2019 denotes Category and \u2018Clr\u2019 denotes Color.\n4.3\nCOMPARISON WITH PRIOR WORKS\nQualitative Comparisons\nWhile this task of image generation with disentangled visual concepts\nis new, we identified prior work that is capable of text-based image editing and generation, and\nestablish a side-by-side comparison on the task of visual concept editing. Specifically, given an\nimage x, for example, of a teal-colored apple in Figure 6, the task is to generate a new image\n\u02c6xek\u2192e\u2032\nk with one concept axis k (e.g. category) modified by a text prompt, from <ek> to <e\u2032\nk>,\nwhile preserving other axes {i|i \u0338= k} from the input.\nWe compare to two existing methods. Null-text Inversion (Mokady et al., 2022) performs concept\nediting by first inverting the input image to token embeddings of Stable-Diffusion (Saharia et al.,\n2022) and then applying Prompt-to-Prompt (Hertz et al., 2022), which modifies cross attention maps\nfor editing, a process that leads to pixel-aligned editing results. InstructPix2Pix (Brooks et al., 2023)\nis a conditional diffusion model for text-based image editing. Since it is trained on data curated with\nPrompt-to-Prompt for supervision, it also tends to generate pixel-aligned results. We also design\na naive baseline for this task, where we query BLIP-2 for a text description of the attributes to\nbe retained from the test image, and combine the answer with a target concept to generate the\nfinal recomposed image. As shown in Figure 6, our method achieves better recomposition results,\nwhereas baseline methods fail to disentangle the desired axis-specific concepts from input images,\nand therefore struggle to faithfully reproduce the desired concepts from the image and the text.\nQuantitative Comparisons.\nWe also conduct quantitative evaluations on the task of text-based\nvisual concept editing and compare with prior work. Specifically, we record the ground-truth text\n8\nPreprint\n\u201cspring\u201d\n\u201cautumn\u201d\n\u201cwinter\u201d\nInput\nExtrapolations\n\u201ccharcoal\u201d\n\u201cacrylic\u201d\n\u201cwatercolor\u201d\n\u201cred\u201d\n\u201cpurple\u201d\n\u201cblue\u201d\n<category>\n<category>\n<category>\nFigure 7: Visual Concept Extrapolation.\nGiven an input image, we can extrapolate\nalong a concept axis by querying BLIP-2\nand GPT-4 to name a few alternatives to\nthe concept in the input. Our model can\nthen generate new variants of the input for\nvisual exploration, by mixing them with\nthe extracted concept embeddings.\nSource \nImage\nw/o Encoder\n& w/o \u2112\ud835\udc58\nanchor\nw/o \u2112\ud835\udc58\nanchor\nOurs\nChange to\n<category>\n\u201csilver\u201d\n+\n\u201corange\u201d\n<category>\n+\n\u201cgrapes\u201d\n<color>\n+\n\u201cmelon\u201d\n<color>\n+\nBaseline \nw/ BLIP-2\nFigure 8: Ablations.\nBoth the text anchor loss\nand the encoder design reduce overfitting to in-\nput images and improve concept disentanglement.\nRemoving these components significantly deterio-\nrates the visual concept editing results. Addition-\nally, we observe that the BLIP-2 baseline has diffi-\nculties transferring unnatural colors.\nprompts y that we used to generate each training image x, and manually filter out the inconsistent\npairs. For each concept axis k, we randomly select a target label <e\u2032\nk> from a set of candidates that\ndifferent from the one in the original prompt <ek>. We then measure the alignment score between\nthe modified image \u02c6xek\u2192e\u2032\nk with the modified text prompt yek\u2192e\u2032\nk using CLIP (Radford et al., 2021),\nfollowing Gal et al. (2022). We also break down the score to each individual axis, by computing the\nalignment of the image to a prompt specifying only one axis, e.g. \u201ca photo of <e\u2032\nk>\u201d. All results\nare summarized in Table 1. Our method captures the characteristics of category particularly well\nand outperforms others in changing category or preserving category while changing color, which\nhighlights its ability to extract disentangled concepts allowing for flexible compositions. Instruct-\nPix2Pix is more effective in changing color, but tends to do poorly in preserving the category, as\nseen in Figure 6. More details on our quantitative results can be found in A.4.\nWe further conduct a human evaluation. Each participant is presented the results from all three\nmethods in a random order together with the editing instruction, and asked to rank them considering\nboth realism and faithfulness to the instruction (see Appendix A.3 for details). We aggregate the\nresponses from 20 people and report the average average score normalized to 0-1 in Table 1. More\ndetails of our setup can be found in A.3.\n4.4\nABLATIONS\nWe conduct an ablation study to understand the effects of the proposed encoder (as opposed per-\ninstance optimization) and the text anchoring loss Lanchor\nk\n. We use the same evaluation dataset and\nthe CLIP-alignment metric as described in Section 4.3. As shown in Fig. 8 and Table 1, removing\nLanchor\nk\nand the encoders deteriorates disentanglement of the different concept axes due to severe\noverfitting, resulting in poor recomposition results. More results are included in the Appendix.\n5\nCONCLUSION\nIn this paper, we have presented a framework for learning language-informed visual concepts from\nimages, by simply distilling from pre-trained vision-language models. After training, the concept en-\ncoders extract disentangled concept embeddings along various concept axes specified by language,\nwhich can be remixed or edited to generate images with novel concept compositions. We conduct\nthorough evaluations both quantitatively and qualitatively, and demonstrate that our approach yields\nsuperior results in visual concept editing compared to prior work.\n9\nPreprint\n6\nACKNOWLEDGEMENT\nWe thank Kyle Hsu, Joy Hsu, and Stephen Tian for their detailed comments and feedback. This\nwork is in part supported by NSF RI #2211258, ONR MURI N00014-22-1-2740, ONR N00014-\n23-1-2355, AFOSR YIP FA9550-23-1-0127, the Stanford Institute for Human-Centered AI (HAI),\nAmazon, J.P. Morgan, and Samsung.\nREFERENCES\nMoab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H\nBermano.\nDomain-agnostic tuning-encoder for fast personalization of text-to-image models.\narXiv preprint arXiv:2307.06925, 2023.\nOmri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene:\nExtracting multiple concepts from a single image. SIGGRAPH Asia, 2023.\nKobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David M. Blei, and Michael I.\nJordan. Matching words and pictures. JMLR, 2003.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\nTim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image\nediting instructions. In CVPR, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. NeurIPS, 2020.\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or.\nAttend-and-excite:\nAttention-based semantic guidance for text-to-image diffusion models, 2023a.\nHila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, In-\nbar Mosseri, and Lior Wolf.\nThe hidden language of diffusion models.\narXiv preprint\narXiv:2306.00966, 2023b.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-\ngan: Interpretable representation learning by information maximizing generative adversarial nets.\nNeurIPS, 29, 2016.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale, 2021.\nDave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and Aleksander Holynski. Diffusion self-\nguidance for controllable image generation. NeurIPS, 2023.\nLi Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE TPAMI, 2006.\nR. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning object categories from google\u2019s image\nsearch. In ICCV, 2005.\nBruce Fraser, Fred Bunting, and Chris Murphy. Real world color management, 2004.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion, 2022.\n10\nPreprint\nRinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or.\nEncoder-based domain tuning for fast personalization of text-to-image models.\nACM Trans.\nGraph., 2023.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-\nrate object detection and semantic segmentation. In CVPR, 2014.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\nChi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, and Jiajun Wu. Visual concept-metaconcept\nlearning. In NeurIPS, 2019.\nErik H\u00a8ark\u00a8onen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering\ninterpretable gan controls. NeurIPS, 2020.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\nAaron Hertzmann, Charles E Jacobs, Nuria Oliver, Brian Curless, and David H Salesin. Image\nanalogies. In SIGGRAPH, 2001.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a\nconstrained variational framework. In ICLR, 2017.\nKyle Hsu, Will Dorrell, James C. R. Whittington, Jiajun Wu, and Chelsea Finn. Disentanglement\nvia latent quantization. In NeurIPS, 2023.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In CVPR, 2019.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\nlutional neural networks. In Advances in Neural Information Processing Systems, 2012.\nDongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for\ncontrollable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large language models. In ICML, 2023b.\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual\ngeneration with composable diffusion models. In ECCV, 2022.\nNan Liu, Yilun Du, Shuang Li, Joshua B Tenenbaum, and Antonio Torralba. Unsupervised compo-\nsitional concepts discovery with text-to-image generative models. ICCV, 2023.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nJiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro-\nsymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In\nICLR, 2019.\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for\nediting real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\nOpenAI. Gpt-4 technical report, 2023.\n11\nPreprint\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nKanchana Ranasinghe and Michael Ryoo. Language-based action concept spaces improve video\nself-supervised learning. NeurIPS, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, pp. 10684\u201310695, 2022.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR,\n2023.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. IJCV, 2015.\nDonald G Saari. Selecting a voting method: the case for the borda count. Constitutional Political\nEconomy, 34(3):357\u2013366, 2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. NeurIPS, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. NeurIPS, 2022.\nStabilityAI. DeepFloyd IF. https://github.com/deep-floyd/IF.\nAd\u00b4ela \u02c7Subrtov\u00b4a, Michal Luk\u00b4a\u02c7c, Jan \u02c7Cech, David Futschik, Eli Shechtman, and Daniel S`ykora. Dif-\nfusion image analogies. In SIGGRAPH, pp. 1\u201310, 2023.\nYael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual\nexploration and inspiration. arXiv preprint arXiv:2305.18203, 2023.\nAndrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan\nlatent space. In ICML, 2020.\nBing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in\nconvolutional network. arXiv preprint arXiv:1505.00853, 2015.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\nYiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, and Yin Li. Learning to generate scene graph\nfrom natural language supervision. In ICCV, 2021.\n12\nPreprint\nA\nAPPENDIX\nA.1\nMORE QUALITATIVE RESULTS\nMore qualitative results for concept extraction and recomposition are shown in Figures 9 to 13.\nAcross various datasets, our method achieves superior disentanglement and recomposition results.\n<category>\n<style>\n<style>\n<category>\ncomposed\n<category>\n<style>\n<style>\n<category>\ncomposed\nFigure 9: Visual Concept Extraction and Recomposition Results on Art dataset.\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\nFigure 10: Visual Concept Extraction and Recomposition Results on Fruits dataset.\nA.2\nDATASETS\nWe use 5 training datasets for the experiments. The concept axes in interest, together with ground\ntruth words used in the text prompt for DeepFloyd to generate the training data, are listed below.\nFruit Dataset.\nCategory: Cherry, Apple, Banana, Mango, Strawberry, Pineapple, Lemon, Raspberry\nColor: Red, Blue, Green, Purple, Black, Yellow, Orange\nArt Dataset.\nCategory: Beach, Tree, Apple, Cat, Dog, Face, City, Hill, Sky, Car, Bike, House, Castle, Chair\nStyle: Charcoal, Oil, Paint, Acrylic, Crayon, Pastel\nFigurine Dataset.\nCategory: Ball, Block, Statue\nColor: Red, Blue, Green, Yellow, White, Cream, Purple, Pink\nMaterial: Plastic, Fur, Wood, Metal, Leather\n13\nPreprint\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\nFigure 11: Visual Concept Extraction and Recomposition Results on Figurine dataset.\nClothing Dataset.\nCategory: Shirt, Pants, Shoes, Dress, Cap\nColor: Red, Yellow, Green, Purple, White, Cream\nSeason: Spring, Summer, Fall, Winter\nFurniture Dataset.\nCategory: Chair, Table, Bench\n14\nPreprint\n<category>\n<color>\n<material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<color>\n<category>\n<category> <material>\n<material>\n<color>\nFigure 12: Visual Concept Extraction and Recomposition Results on Objects dataset.\nColor: Red, Orange, Yellow, Green, Blue, Purple, Black, White\nMaterial: Wood, Metal, Plastic\nA.3\nHUMAN EVALUATION\nIn the user study, we create a questionnaire where users are asked to choose the image that matches\nour test setting: given an image prompt and text instruction, users rank images that best represents\nboth the image prompt and text instruction. Here, we randomly sample 12 data points from the\nevaluation dataset for color editing and 12 for category editing. We collected the questionnaire\nresponses from 20 users. Our score follows the Borda point metric (Saari, 2023) where 1st, 2nd, and\n3rd would receive 2, 1, and 0 point(s) respectively, allowing us to differentiate rankings of 3, 2, 2\nand 2, 1, 1. We then calculate the average scores over both the number of questions and participants,\nand subsequently normalize these values to a 0 \u2212 1 scale. Results are shown in 1.\nA set of instructions Figure 14 are presented to the respondents before the user study. Then, they\nare presented with the 24 questions of color and category editing tasks. Example questions are\nshown in Figures 15 and 16.\n15\nPreprint\n<category>\n<color>\n<season>\n<season>\n<color>\n<category>\n<color>\n<category> <season>\ncomposed\n<category>\n<color>\n<season>\n<season>\n<color>\n<category>\n<color>\n<category> <season>\ncomposed\nFigure 13: Visual Concept Extraction and Recomposition Results on Clothing dataset.\nFigure 14: Questionnaire Instructions.\nA.4\nQUANTITATIVE ANALYSIS.\nFigure 17 analyzes the mean and standard deviation of CLIP scores introduced in Section 4.3. Our\nmethod achieves the best score with a low variance for both the category (left) and the overall (right)\nmetric, and stays competitive on the color (metric).\n16\nPreprint\nFigure 15: Questionnaire Attribute Modification.\nFigure 16: Questionnaire Category Modification.\nA.5\nDATASET LABELLING WITH BLIP\nIn our approach, we employ BLIP in order to annotate each image with its respective category and\nattributes. To achieve this, we query BLIP for each image, with one question for each attribute or\ncategory. For example, for an image of a red wooden table, we would ask BLIP what is the name of\nthe item, what material it is made of, and what color it is.\nDuring the pre-training stage, items corresponding to identical prompts and therefore the same place-\nholder tokens are aggregated by taking the most commonly occurring BLIP answer for each category\nor attribute. For example, if we have 8 images of red wooden tables, and one of them is misclassified\nas a red plastic table, we would still label it with \u2018red\u2019, \u2018wood\u2019, and \u2018table\u2019 for color, material, and\ncategory respectively.\nA.6\nINTERPOLATION OF CONCEPT EMBEDDINGS\nWe further demonstrate concept interpolation results. By interpolating between two concept em-\nbeddings, our model can generate meaningful images depicting gradual changes from one concept\nto another, such as the hybrid fruit of cherries and bananas shown in Figure 18. To interpolate\nbetween two input images, first, we extract CLIP features for both images, giving us two vectors\nof size 12 \u00d7 1024. We interpolate the two vectors using Spherical interpolation (SLERP). Specif-\n17\nPreprint\nFigure 17: Quantitative Baselines.\nically, given two normalized vectors Anorm and Bnorm of dimensions 12 \u00d7 1024, we compute\nthe dot product to find the cosine of the angle \u03b8 between them as cos(\u03b8) = Anorm \u00b7 Bnorm. For\n12 interpolation points, each point i is calculated using \u03b1i =\ni\n11 and the interpolated vector is\nslerp(A, B, \u03b1i) = sin((1\u2212\u03b1i)\u03b8)\nsin(\u03b8)\nAnorm + sin(\u03b1i\u03b8)\nsin(\u03b8) Bnorm. This ensures the resultant vectors maintain\na constant magnitude.\nWith a set of trained encoders, each encoder takes in the interpolated CLIP features, and their outputs\nare combined with a training-time textual template to generate the interpolation results as shown in\nFigures 18 and 19.\nInput \nImages\n<color>\n<category>\n<category> \n+ <color>\nInterpolation \nPoints\nInput \nImages\n<color>\n<category>\n<category> \n+ <color>\nInterpolation \nPoints\nFigure 18: Interpolation on Fruit dataset.\nInput \nImages\n<style>\n<subject>\n<subject> + \n<style>\nInterpolation \nPoints\nInput \nImages\n<style>\n<subject>\n<subject> + \n<style>\nInterpolation \nPoints\nFigure 19: Interpolation on Art dataset.\n18\nPreprint\nB\nINFERENCE ON REAL-WORLD IMAGES\nDespite being trained on images generated by diffusion-based models, the concept encoders gen-\neralize well to diverse, complex real-world images, including unseen types of object category,\nmaterial, and color from casual photo captures (Figures 20 to 23) and unseen types of artwork\nstyle (Figures 5 and 24).\n<category>\n<color>\n<material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<color>\n<category>\n<category> <material>\n<material>\n<color>\nFigure 20: Visual Concept Extraction and Recomposition Results on Real-world Images of various\nobjects.\n19\nPreprint\n<category>\n<color>\n<material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<color>\n<category>\n<category> <material>\n<material>\n<color>\nFigure 21: Additional Visual Concept Extraction and Recomposition Results on Real-world Images\nof various objects.\n20\nPreprint\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\n<category>\n<color>\n<color>\n<category>\ncomposed\nFigure 22: Visual Concept Extraction and Recomposition Results on Real-world Images of various\nfruits.\n21\nPreprint\n<category>\n<color>\n<material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\n<color>\n<category>\n<category> <material>\n<material>\n<color>\n<category>\n<color>\n<material>\n<material>\n<color>\n<category>\n<color>\n<category> <material>\ncomposed\nFigure 23: Visual Concept Extraction and Recomposition Results on Real-world Images of various\npieces of furniture.\n22\nPreprint\n<category>\n<style>\n<style>\n<category>\ncomposed\n<category>\n<style>\n<style>\n<category>\ncomposed\n<category>\n<style>\n<style>\n<category>\ncomposed\n<category>\n<style>\n<style>\n<category>\ncomposed\nFigure 24: Visual Concept Extraction and Recomposition Results on Real-world Images of artwork.\n23\nPreprint\nC\nTHE EFFECT OF THE ANCHOR LOSS\nDuring training time, the anchor loss (Equation (2)) encourages the encoder predictions to converge\nto a meaningful subspace within the word embedding space Gal et al. (2023). This ensures that\nthese embeddings can be readily visualized by a pre-trained text-to-image generation model, and\nimproves the compositionality across different concept axes, as shown in Figure 6.\nEmpirically, we find that simply setting a small weight on this loss can effectively achieve this\nobjective, allowing the model to capture nuances along each axis, without collapsing to the word\nembeddings. In Figure 25, we empirically show such examples, where we compare the concept\nembeddings predicted by the color encoder to the text embedding of the training-time BLIP-2 label,\ne.g. \u201cblue\u201d from Figure 25, and the former preserves the specific color of the input image while the\nlatter does not.\n<color> \nInput\n\u201cblue\u201d\nT2I\nOur Concept Encoder\n<color> \nInput\n\u201cwhite\u201d\nT2I\nOur Concept Encoder\n<color> \nInput\n\u201cyellow\u201d\nT2I\nOur Concept Encoder\n<color> \nInput\n\u201cpurple\u201d\nT2I\nOur Concept Encoder\nFigure 25: The concept embeddings extracted by our concept encoders capture various shades of\ncolors instead of generic \u2018blue\u2019, \u2018white\u2019 etc. directly generated by the T2I model DeepFloyd.\n24\nPreprint\nD\nEXTENSION WITH NEW CONCEPT AXES\nWe show that we are able to extend to additional concept axes by training new concept encoders\nwithout retraining the existing ones. In the experiments shown in Figures 26 and 27, given two\ntrained encoders for category and color and the corresponding training dataset, we train the\nmaterial encoder while keeping the other two frozen. We show that such procedural training\nmaintains the disentanglement of the frozen encoders while allowing the framework to extend to a\nnew concept axis.\nSource <material>\nSource <color>\nSource <category>\nSource <material>\nSource <category>\nSource <color>\nFrozen\nAdded \u2018material\u2019 axis\nFigure 26: Concept Recomposition after adding a new material axis to the model trained with\ncategory and color axes.\n25\nPreprint\n<category>\n<color> <material>\n<material>\n<color> <category>\n<color>\n<category><material>\ncomposed\n<category>\n<color> <material>\n<material>\n<color> <category>\n<color>\n<category><material>\ncomposed\n<category>\n<color> <material>\ncomposed\n<color> <category>\n<category><material>\n<material>\n<color>\n<category>\n<color> <material>\n<material>\n<color> <category>\n<color>\n<category><material>\ncomposed\nFigure 27: Test-Time Generalization Results on images of furniture, where the material encoder\nis additionally trained on top of the frozen category and color encoders.\n26\nPreprint\nE\nADDITIONAL BASELINE COMPARISONS\nIn Figure 28, we provide additional qualitative examples accompanying the experiments in Table 1.\nFrom these visual examples, we observed that the color nuances captured by ours are more accurate\ncompared to the BLIP-2 baseline. However, since the CLIP-based metric specified in Section 4.3\ncannot fully capture the minute differences, the BLIP-2 baseline still achieves comparable scores to\nour method despite this evident gap in visual results.\nTo quantify such visual differences in colors, we compare the color of a 16 \u00d7 16 centered patch\nfrom the input image and the image generated by the method being evaluated, both of resolution\n64 \u00d7 64, and report the L2 error of the mean color of the two patches. We report the average of\nmetric across all examples in Figure 28 in Table 2. Results suggest that our method captures colors\nmore accurately compared to the baselines.\nSource \nImage\nw/o Encoder\n& w/o \u2112\ud835\udc58\nanchor\nw/o \u2112\ud835\udc58\nanchor\nOurs\nChange to\n<category>\n\u201corange\u201d\n+\n\u201cpeach\u201d\n<color>\n+\n\u201cwhite\u201d\n\u201cmelon\u201d\n<category>\n+\n<color>\n+\nBaseline \nw/ BLIP-2\n<category>\n\u201cgold\u201d\n+\n\u201cmelon\u201d\n<color>\n+\n\u201csilver\u201d\n\u201cberry\u201d\n<category>\n+\n<color>\n+\nFigure 28: More qualitative results for ablations.\nCIELAB \u2206E* \u2193\nCherry\nMango\nStrawberry\nApple\nOurs\n35.50\n4.76\n16.12\n15.64\nw/o Lanchor\nk\n101.34\n86.34\n85.31\n122.01\nw/o Encoder & Lanchor\nk\n85.86\n82.46\n82.07\n127.80\nBaseline w/ BLIP-2\n79.90\n25.20\n47.30\n73.62\nTable 2: Quantitative Comparisons on Color Editing. To quantify the differences in color seen\nin Figure 28, we use CIELAB \u2206E*, the color-distance metric recommended by the International\nCommision on Illumination (Fraser et al., 2004).\n27\nPreprint\nF\nDISCUSSION ON TEXTUAL INVERSION\nAs discussed in Section 3.1, compared to directly using text as inputs for image generation, us-\ning techniques like Textual Inversion (Gal et al., 2022), our model is able to capture more nuanced\nvisual details of a particular image with continuous embeddings, instead of discrete words. This\ncan be illustrated in the empirical results in Figures 6, 8 and 28, which show that our method pre-\nserves the nuances from input images more accurately than the BLIP-2 baseline which uses texts for\nconditional generation.\nF.1\nLIMITATIONS\nGiven a few concept axes specified by language, the current model is able to learn disentangled\nconcept embeddings for various concept remixing and editing tasks. However, this requires that the\nconcept axes are given a priori to the model, which limits the generality of the concept space that\ncan be captured by the model. Moreover, we train separate encoders for each concept axis, which\ndoes not make full use of the inherent hierarchical structure of these concept axes.\n28\n"
  }
]