[
  {
    "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
    "link": "https://arxiv.org/pdf/2306.14824.pdf",
    "upvote": "32",
    "text": "KOSMOS-2: Grounding Multimodal Large Language\nModels to the World\nZhiliang Peng\u2217, Wenhui Wang\u2217, Li Dong\u2217, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei\u2020\nMicrosoft Research\nhttps://aka.ms/GeneralAI\nAbstract\nWe introduce KOSMOS-2, a Multimodal Large Language Model (MLLM), enabling\nnew capabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer expressions\nas links in Markdown, i.e., \u201c[text span](bounding boxes)\u201d, where object\ndescriptions are sequences of location tokens. Together with multimodal corpora,\nwe construct large-scale data of grounded image-text pairs (called GRIT) to train\nthe model. In addition to the existing capabilities of MLLMs (e.g., perceiving\ngeneral modalities, following instructions, and performing in-context learning),\nKOSMOS-2 integrates the grounding capability into downstream applications. We\nevaluate KOSMOS-2 on a wide range of tasks, including (i) multimodal grounding,\nsuch as referring expression comprehension, and phrase grounding, (ii) multimodal\nreferring, such as referring expression generation, (iii) perception-language tasks,\nand (iv) language understanding and generation. This work lays out the foundation\nfor the development of Embodiment AI and sheds light on the big convergence of\nlanguage, multimodal perception, action, and world modeling, which is a key step\ntoward artificial general intelligence. Code and pretrained models are available at\nhttps://aka.ms/kosmos-2.\nGrounding\nKosmos-2: Multimodal Large Language Model\n[a campfire](<loc4> <loc1007>)\n[It](<loc44> <loc863>) sits next to \nReferring\nFigure 1: KOSMOS-2 is a multimodal large language model that has new capabilities of multimodal\ngrounding and referring. KOSMOS-2 can understand multimodal input, follow instructions, perceive\nobject descriptions (e.g., bounding boxes), and ground language to the visual world.\n\u2217 Equal contribution. \u2020 Corresponding author.\narXiv:2306.14824v3  [cs.CL]  13 Jul 2023\nInput Prompt\nthe left eye of emoji\nQuestion: How many cows are \nhere? Answer: \nQuestion: What does the \nsign say? Answer:\nCompletion\ntwo cows present in the image.\nThe sign says \"Welcome to \nCarnaby Street.\"\nInput Prompt\nQuestion: What is the biggest \ndifference betwen bottle-1 and \nbottle-2? Answer:\nQuestion: Why is this animal \nunusual? Answer:\n(1)\n(2)\n(3)\nCompletion\nThe biggest difference between \nbottle-1 and bottle-2 is the label.\nThis animal is unique because it is a \nturtle. Turtles are known for their slow \nand steady pace, whereas rabbits are \nknown to be agile and quick. It is not \ncommon for a turtle to be faster than a \nrabbit, especially in terms of speed.\na buoy attached to \nthe boat.\n(4)\n(5)\n(6)\nQuestion: What is it? Answer:\nDescribe this image in detail:\nCompletion\nA snowman sits next to a campfire in \nthe snow. He is wearing a hat, scarf, \nand mittens. There are several pots\nnearby, likely containing a hot meal \nfor the snowman. The scene is \nserene and peaceful, with the \nsnowman looking at the fire with a \nwarm smile.\n(7)\nInput Prompt\nFigure 2: Selected examples generated from KOSMOS-2. The examples include (1) visual grounding,\n(2)-(3) grounded question answering, (4)-(6) multimodal referring via bounding boxes, and (7)\ngrounded image captioning.\n2\n1\nIntroduction\nMultimodal Large Language Models (MLLMs) [HSD+22, ADL+22, HDW+23, DXS+23, Ope23]\nhave successfully played a role as a general-purpose interface across a wide range of tasks, such\nas language, vision, and vision-language tasks. MLLMs can perceive general modalities, including\ntexts, images, and audio, and generate responses using free-form texts under zero-shot and few-shot\nsettings.\nIn this work, we unlock the grounding capability for multimodal large language models. Grounding\ncapability can provide a more convenient and efficient human-AI interaction for vision-language\ntasks. It enables the user to point to the object or region in the image directly rather than input\ndetailed text descriptions to refer to it, the model can understand that image region with its spatial\nlocations. Grounding capability also enables the model to respond with visual answers (i.e., bounding\nboxes), which can support more vision-language tasks such as referring expression comprehension.\nVisual answers are more accurate and resolve the coreference ambiguity compared with text-only\nresponses. In addition, grounding capability can link noun phrases and referring expressions in the\ngenerated free-form text response to the image regions, providing more accurate, informational, and\ncomprehensive answers.\nWe introduce KOSMOS-2, a multimodal large language model with grounding capability built upon\nKOSMOS-1. KOSMOS-2 is a Transformer-based causal language model and is trained using the\nnext-word prediction task. In order to unlock the grounding capability, we construct a web-scale\ndataset of grounded image-text pairs, and combine it with the multimodal corpora in KOSMOS-1\nto train the model. The grounded image-text pairs are built upon a subset of image-text pairs from\nLAION-2B [SBV+22] and COYO-700M [BPK+22]. We construct a pipeline to extract and link\nthe text spans (i.e., noun phrases and referring expressions) in the caption to the spatial locations\n(e.g., bounding boxes) of its corresponding objects or regions in the image. We convert the spatial\ncoordinates of the bounding boxes to a sequence of location tokens, which is then appended after its\nrespective text spans. The data format serves as a \u201chyperlink\u201d to connect the objects or regions of the\nimage to the caption.\nExperimental results demonstrate that KOSMOS-2 not only achieves competitive performance on lan-\nguage and vision-language tasks evaluated in KOSMOS-1, but also achieves impressive performance\non grounding tasks (phrase grounding and referring expression comprehension) and referring tasks\n(referring expression generation). As shown in Figure 2, integrating the grounding capability enables\nKOSMOS-2 to be used for more downstream tasks, such as grounded image captioning, and grounded\nvisual question answering.\n2\nConstruction of Web-Scale Grounded Image-Text Pairs (GRIT)\nWe introduce GRIT2, a large-scale dataset of Grounded Image-Text pairs, which is created based on\nimage-text pairs from a subset of COYO-700M [BPK+22] and LAION-2B [SBV+22]). We construct\na pipeline to extract and link text spans (i.e., noun phrases and referring expressions) in the caption\nto their corresponding image regions. The pipeline mainly consists of two steps: generating noun-\nchunk-bounding-box pairs and producing referring-expression-bounding-box pairs. We describe\nthese steps in detail below:\nStep-1: Generating noun-chunk-bounding-box pairs\nGiven an image-text pair, we first extract\nnoun chunks from the caption and associate them with image regions using a pretrained detector. As\nillustrated in Figure 3, we use spaCy [HMVLB20] to parse the caption (\u201ca dog in a field of flowers\")\nand extract all noun chunks (\u201ca dog\u201d, \u201ca field\u201d and \u201cflowers\u201d). We eliminate certain abstract noun\nphrases that are challenging to recognize in the image, such as \u201ctime\u201d, \u201clove\u201d, and \u201cfreedom\u201d, to\nreduce potential noise. Subsequently, we input the image and noun chunks extracted from the caption\ninto a pretrained grounding model (e.g., GLIP [LZZ+22]) to obtain the associated bounding boxes.\nNon-maximum suppression algorithm is applied to remove bounding boxes that have a high overlap\nwith others, even if they are not for the same noun chunk. We keep noun-chunk-bounding-box pairs\nwith predicted confidence scores higher than 0.65. If no bounding boxes are retained, we discard the\ncorresponding image-caption pair.\n2A subset of GRIT can be downloaded at https://aka.ms/kosmos-2.\n3\na dog in a \nfield of flowers\nIdentify \nnoun chunks\nDetection &\nPost-process\na dog: \n[290,371,605,750]\na field: \n[0,264,919,921]\nStep-1: Creating noun \nchunk - bounding box pairs\na dog\na field\nflowers\nExpand\nnoun chunks\nSentence dependency relations\na dog in a field of flowers\na field of flowers\nflowers\nDrop \nsubstrings\nCompose\na dog in a field of \nflowers: \n[290,371,605,750]\nStep-2: Producing referring \nexpression - bounding box pairs\nKeep \u201ca dog in a field of flowers\u201d\nDrop \u201ca field of flowers\u201d\nDrop \u201cflowers\u201d\nFigure 3: The pipeline of constructing web-scale grounded image-text pairs.\nDataset\nImages\nObjects\nText Spans\nAvg Expression Length\nFlickr Entities [PWC+15]\n31,783\n275,775\n513,644\n-\nRefCOCOg [MHT+15]\n26,711\n54,822\n85,474\n8.43\nRefCOCO [YPY+16]\n19,994\n50,000\n142,209\n3.61\nRefCOCO+ [YPY+16]\n19,992\n49,856\n141,564\n3.53\nVisual Genome [KZG+16]\n108,077\n4,102,818\n-\n-\nGRIT (Ours)\n90,614,680\n137,349,210\n114,978,233\n4.7\nTable 1: Comparison GRIT with existing visual grounding datasets.\nStep-2: Producing referring-expression-bounding-box pairs\nIn order to endow the model with\nthe ability to ground complex linguistic descriptions, we expand noun chunks to referring expressions.\nSpecifically, we use spaCy to obtain dependency relations of the sentence. We then expand a noun\nchunk into a referring expression by recursively traversing its children in the dependency tree and\nconcatenating children tokens with the noun chunk. We do not expand noun chunks with conjuncts.\nFor noun chunks without children tokens, we keep them for the next process. In the example shown\nin Figure 3, the noun chunk \u2018a dog\u2019 can be expanded to \u201ca dog in a field of flowers\u201d, and the noun\nchunk \u2018a field\u2019 can be expanded to \u201ca field of flowers\u201d.\nFurthermore, we only retain referring expressions or noun chunks that are not contained by others.\nAs shown in Figure 3, we keep the referring expression \u201ca dog in a field of flowers\u201d and drop \u201ca field\nof flowers\u201d (as it is entailed by \u201ca dog in a field of flowers\u201d) and \u2018flowers\u2019. We assign the bounding\nbox of the noun chunk (\u2018a dog\u2019) to the corresponding generated referring expression (\u201ca dog in a\nfield of flowers\u201d).\nIn the end, we obtain approximately 91M images, 115M text spans, and 137M associated bounding\nboxes. We compare GRIT with existing publicly accessible visual grounding datasets in Table 1.\nData samples of GRIT are shown in the Appendix.\n3\nKOSMOS-2: A Grounded Multimodal Large Language Model\nKOSMOS-2 is a grounded multimodal large language model, which integrates grounding and referring\ncapabilities compared with KOSMOS-1. The model can accept image regions selected by the user\nusing bounding boxes as input, provide visual answers (i.e., bounding boxes), and ground the text\noutput to the visual world. KOSMOS-2 adopts the same model architecture and training objective\nas KOSMOS-1. We add grounded image-text pairs into the training data to endow the model with\ngrounding and referring capabilities. For a text span (such as noun phrase and referring expression)\nand its corresponding bounding boxes in a grounded image-text pair, We discretize continuous\ncoordinates of bounding boxes into a sequence of location tokens to encode with text tokens in a\nunified way. Then we link the location tokens and their corresponding text span via a \u201chyperlink\u201d data\n4\nformat. The model is trained to establish a mapping between image regions and their corresponding\nlocation tokens and connect the image regions with their associated text spans.\n3.1\nGrounded Input Representations\nGiven a text span and its associated bounding boxes in a grounded image-text pair, we first convert the\ncontinuous coordinates of bounding boxes into a sequence of discrete location tokens [CSL+21]. For\nan image with width W and height H, we evenly divide both the width and height into P segments\neach. P \u00d7 P bins are obtained and each bin consists of (W/P) \u00d7 (H/P) pixels. For each bin, we use\na location token to represent the coordinates within that bin. We use the coordinates of the center\npixel of each bin to determine bounding boxes on the image. In total, P \u00d7 P location tokens are\nintroduced, and these tokens are added to word vocabulary to enable unified modeling with texts.\nThe bounding box can be represented using its top-left point (x1, y1) and bottom-right point\n(x2, y2). We discretize the top-left and bottom-right corner points to location tokens, respec-\ntively.\nWe concatenate the top-left location token <loc1>, the bottom-right location token\n<loc2>, and special boundary tokens <box> and </box>, to represent a single bounding box:\n\u201c<box><loc1><loc2></box>\u201d.\nIf the text span is associated with multiple bounding boxes,\nwe use a special token <delim> to concatenate the location tokens of these bounding boxes:\n\u201c<box><loci\n1><loci\n2><delim>...<locj\n1><locj\n2></box>\u201d.\nThen we arrange the text span and its associated location tokens in a format resembling a \u201chyperlink\u201d\nin markdown. For the text span with a single bounding box, the resulted sequence is \u201c<p> text span\n</p><box><loc1><loc2></box>\u201d, where <p> and </p> are special tokens indicating the beginning\nand end of the text span. The data format tells the model that image regions within the bounding box\nare associated with the text span.\nFor the example shown in Figure 1, the input representation is:\n<s> <image> Image Embedding </image> <grounding> <p> It </p><box><loc44><loc863></box>\nseats next to <p> a campfire </p><box><loc4><loc1007></box> </s>\nwhere <s> and </s> indicate start- and end-of-sequence, and <image> and </image> represent\nthe beginning and end of encoded image embeddings. <grounding> is a special token to tell the\nmodel ground the text output to the visual world. We map input text tokens and location tokens to\nembeddings via a lookup table. Following KOSMOS-1, a vision encoder and a resampler module are\nused to obtain image embeddings for input images.\nFor language-only data, cross-modal paired data (i.e., image-text pairs), and interleaved multimodal\ndata, we use the same input representations as of KOSMOS-1.\n3.2\nGrounded Multimodal Large Language Models\nBased on KOSMOS-1, KOSMOS-2 enhances multimodal large language models by incorporating\ngrounding and referring capabilities. KOSMOS-2 also uses a Transformer-based causal language\nmodel as the backbone and is trained with the next-token prediction task.\nIn addition to multimodal corpora used in KOSMOS-1 (including text corpora, image-caption pairs,\nand interleaved image-text data), we add grounded image-text pairs into training. The training loss\nonly considers discrete tokens, such as text tokens and location tokens. The model can learn to locate\nand understand image regions by their location tokens and the whole image, associate text spans to\nimage regions, and output bounding boxes of the image region using location tokens.\nKOSMOS-2 shows new capabilities of grounding and referring. The referring capability enables us to\npoint out image regions with bounding boxes. KOSMOS-2 can understand the image regions users\nrefer to by the coordinates of bounding boxes. The referring capability provides a new interaction\nmethod. Different from previous MLLMs [ADL+22, HSD+22, HDW+23], which can only provide\ntext output, KOSMOS-2 can provide visual answers (i.e., bounding boxes) and ground text output to\nthe image. The grounding capability enables the model to provide more accurate, informative, and\ncomprehensive responses. In addition to vision, language, and vision-language tasks evaluated in\n5\nKOSMOS-1, the model can be used for more downstream tasks, such as grounded image-captioning,\ngrounded VQA, referring expression comprehension and generation.\n3.3\nModel Training\nTraining Setup\nWe train the model on newly added grounded image-text pairs, monomodal text\ncorpora, image-caption pairs, and interleaved image-text data. Our training process involves a\nbatch size of 419K tokens, consisting of 185K tokens from text corpora, 215K tokens from original\nand grounded image-caption pairs, and 19K tokens from interleaved data. We train KOSMOS-2\nfor 60k steps, equivalent to around 25 billion tokens. The AdamW optimizer is employed with\n\u03b2 = (0.9, 0.98). We set the weight decay to 0.01 and the dropout rate to 0.1. The learning rate\nincreases to 2e-4 during the first 375 warm-up steps and linearly decays to zero. We train the model\non 256 V100 GPUs and the training takes approximately one day to complete. In order to tell the\nmodel when to ground text output to the visual world, we prepend the \u2018<grounding>\u2019 token to the\ngrounded caption during training.\nFollowing KOSMOS-1, the vision encoder has 24 layers with 1,024 hidden size and 4,096 FFN\nintermediate size. The multimodal large language model component is a 24-layer MAGNETO\nTransformer [WMH+22, MWH+22] with 2,048 hidden dimensions, 32 attention heads, and 8,192\nFFN intermediate size. The total number of trainable parameters amounts to approximately 1.6B.\nThe image resolution is set to 224\u00d7224 and the patch size is 14\u00d714. We divide the width and height\nof the image into 32 bins, with each bin consisting of 7\u00d77 pixels. A total of 32\u00d732 location tokens\nare added to the vocabulary. KOSMOS-2 uses the weights of KOSMOS-1 for initialization, the newly\nadded word embeddings of location tokens are initialized randomly. We update all the parameters\nduring training and instruction tuning.\nInstruction Tuning\nAfter the model is trained, we perform instruct tuning to better align\nKOSMOS-2 with human instructions. we combine vision-language instruction dataset (i.e., LLaVA-\nInstruct [LLWL23]) and language-only instruction datasets (i.e., Unnatural Instructions [HSLS22]\nand FLANv2 [LHV+23]) with the training data to tune the model. In addition, we construct grounded\ninstruction data by utilizing the pairs of bounding boxes and expressions (i.e., noun phrases, and\nreferring expressions) in GRIT. Given an expression-bounding-box pair, we use \u201c<p> expression\n</p>\u201d as the input instruction, and prompt the model to generate the corresponding location tokens of\nthe bounding boxes. We also use the prompt like \u201c<p> It </p><box><loc1><loc2></box> is\u201d to\nask the model to generate expressions according to its bounding boxes. Table B in Appendix presents\nmore templates.\n4\nEvaluation\nWe first evaluate KOSMOS-2 on multimodal grounding and multimodal referring tasks to assess the\nnew capabilities, and then test the model on language and perception-language tasks evaluated in\nKOSMOS-1.\n\u2022 Multimodal grounding\n\u2013 Phrase grounding\n\u2013 Referring expression comprehension\n\u2022 Multimodal referring\n\u2013 Referring expression generation\n\u2022 Perception-language tasks\n\u2013 Image captioning\n\u2013 Visual question answering\n\u2022 Language tasks\n\u2013 Language understanding\n\u2013 Language generation\n6\nGrounded MLLM\nA man in a blue hard \nhat and <p> orange \nsafety vest </p>\n<box> <loc165> <loc360> </box>\n(1) Phrase grounding\n(2) Referring expression comprehension\nGrounded MLLM\n<box> <loc68> <loc425> </box>\n<p> A man in a blue \nhard hat and orange \nsafety vest </p>\nFigure 4: Input format of evaluation on (1) phrase grounding and (2) referring expression comprehen-\nsion.\n4.1\nMultimodal Grounding\nIn order to evaluate the ability of multimodal grounding, we test KOSMOS-2 on widely used phrase\ngrounding and referring expression comprehension tasks in a generation manner. Phrase grounding\ntask requires the model to predict a set of bounding boxes based on one or more given phrases that\nmaybe interrelated within a single caption. Referring expression comprehension task encourages the\nmodel to locate the object described in a text referring expression within a given image.\nBy testing KOSMOS-2 on these two tasks, we can assess how well the model performs in grounding\ntext descriptions to the visual world, which is crucial for developing advanced AI systems capable of\nhandling complex multimodal tasks.\nFor both phrase grounding and referring expression comprehension tasks, KOSMOS-2 is required\nto generate location tokens which are then converted to bounding boxes for evaluation. The input\nformat is \u201c<s><image> Image Embedding </image><grounding>...\u201d, where \u201c<grounding>\u201d is\nused to prompt the model to generate locations tokens.\n4.1.1\nPhrase Grounding\nWe evaluate phrase grounding task on Flickr30k Entities [PWC+15] val and test splits. In order to\nreduce ambiguity, we do not prompt the model with individual phrases; instead, we use the current\nphrase along with the preceding words as input where preceding words serve as context: \u201c ... <p>\n{phrase} </p>\u201d. For the example shown in Figure 4(1), the model needs to predict the locations of\nphrases \u201cA man\u201d, \u201ca blue hard hat\u201d, \u201corange safety vest\u201d and \u201can intersection\u201d in the caption \u201cA man\nin a blue hard hat and orange safety vest stands in an intersection.\u201d. To generate the location tokens\nfor the phrase \u201cA man\u201d that is the beginning of the caption, the prompt is \u201c<p>A man</p>\u201d. For the\nphrase \u201corange safety vest\u201d, the prompt is \u201cA man in a blue hard hat and <p>orange safety vest</p>\u201d.\nWhen multiple men are in the image, the context \u201cA man in a blue hard hat and\u201d explicitly helps the\nmodel locate the object to reduce ambiguity.\nWe obtain the location tokens in \u201c<box>...</box>\u201d from the model response and then covert it into\nbounding boxes. The generated bounding box is correct if its intersection over union (IoU) with the\nground-truth bounding box is greater than 0.5. If KOSMOS-2 generates a location sequence that can\nnot be converted correctly (e.g., \u201c<box><loc1></box>\u201d), we treat it as a negative sample. We use\nANY-BOX protocol in MDETR [KSL+21]. We report the R@1, R@5, and R@10 metrics, where\nR@1/5/10 means calculating the recall using the top 1/5/10 generated bounding boxes. If there are\nfewer than 5 or 10 bounding boxes generated by KOSMOS-2, we use all available bounding boxes for\nthe calculation.\nResults\nTable 2 presents results on Flickr30k Entities [PWC+15] val and test splits. KOSMOS-2\nachieves impressive zero-shot performance and outperforms GRILL [JMC+23], which relies on\nan attached detector, by a large margin. Moreover, our model outperforms traditional finetuned\n7\nModel\nZero-shot\nVal Split\nTest Split\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nVisualBert [LYY+19]\n\u2717\n70.4\n84.5\n86.3\n71.3\n85.0\n86.5\nMDETR [KSL+21]\n\u2717\n83.6\n93.4\n95.1\n84.3\n93.9\n95.8\nGLIP [LZZ+22]\n\u2717\n86.7\n96.4\n97.9\n87.1\n96.9\n98.1\nFIBER [DKG+22]\n\u2717\n87.1\n96.1\n97.4\n87.4\n96.4\n97.6\nGRILL [JMC+23]\n\u2713\n-\n-\n-\n18.9\n53.4\n70.3\nKOSMOS-2\n\u2713\n77.8\n79.2\n79.3\n78.7\n80.1\n80.1\nTable 2: Phrase grounding results on Flickr30k Entities. We report the R@1, R@5, and R@10\nmetrics, where R@1/5/10 means calculating the recall using the top 1/5/10 generated bounding boxes.\nModel\nZero-\nRefCOCO\nRefCOCO+\nRefCOCOg\nshot\nval\ntestA\ntestB\nval\ntestA\ntestB\nval\ntest\nUNITER [CLY+19]\n\u2717\n81.41\n87.04\n74.17\n75.90\n81.45\n66.70\n74.86\n75.77\nMDETR [KSL+21]\n\u2717\n87.51\n90.40\n82.67\n81.13\n85.52\n72.96\n83.35\n83.31\nOFA [WYM+22]\n\u2717\n90.05\n92.93\n85.26\n84.49\n90.10\n77.77\n84.54\n85.20\nFIBER [DKG+22]\n\u2717\n90.68\n92.59\n87.26\n85.74\n90.13\n79.38\n87.11\n87.32\nVisionLLM [WCC+23]\n\u2717\n86.7\n-\n-\n-\n-\n-\n-\n-\nGRILL [JMC+23]\n\u2713\n-\n-\n-\n-\n-\n-\n-\n47.5\nKOSMOS-2\n\u2713\n52.32\n57.42\n47.26\n45.48\n50.73\n42.24\n60.57\n61.65\nTable 3: Referring expression comprehension results on RefCOCO, RefCOCO+ and RefCOCOg.\nWe report the accuracy metric for all methods.\nVisualBert [LYY+19] model by 7.4% R@1 on both val and test splits. In contrast to other models,\nKOSMOS-2 does not involve prior designs (e.g., object queries or proposals), leading to similar results\namong R@1, R@5, and R@10. These results demonstrate that KOSMOS-2 can generate high-quality\nlocations without the need for post-processing redundant locations. This capability highlights the\neffectiveness of our model in handling phrase grounding tasks.\n4.1.2\nReferring Expression Comprehension\nWe assess the referring expression comprehension task using three well-established datasets: Re-\nfCOCO [YPY+16], RefCOCO+ [YPY+16] and RefCOCOg [MHT+15]. Both RefCOCO and\nRefCOCO+ were generated through a two-player game, with RefCOCO+ specifically designed\nto exclude spatial relations, such as \u201con the left\u201d. RefCOCOg incorporates spatial relations and\nfeatures longer expressions on average. Different from phrase grounding on Flickr30k entities, we\nmeasure this task by using referring expression as the input: \u201c<p> referring expression </p>\u201d. For\nthe example shown in Figure 4(2), the input sequence is \u201c<p>A man in a blue hard hat and orange\nsafety vest</p>\u201d. Similarly, the predicted bounding box is considered correct only if its IOU with\nthe ground-truth bounding box is greater than 0.5. The failed decoded sequence is also treated as a\nnegative sample. We use the first generated bounding box for the query expression to measure the\naccuracy.\nResults\nTable 3 reports referring comprehension results on RefCOCO [YPY+16], Ref-\nCOCO+ [YPY+16] and RefCOCOg [MHT+15]. KOSMOS-2 also obtains promising zero-shot\nperformance on the comprehension task, significantly outperforming previous zero-shot models on\nRefCOCOg benchmark. However, compared to previous finetuned works, KOSMOS-2 achieves\nslightly lower performance on RefCOCO and RefCOCO+ than on RefCOCOg. This discrepancy can\nbe attributed to the data distribution present in RefCOCO and RefCOCO+, where they tend to use a\nshorter referring expression (e.g., \u201cleft bottom\u201d) during the two-player game. Hence, one of our future\ngoals is to enhance MLLMs\u2019 ability to accurately understand more types of human expressions.\n8\nGrounded MLLM\n<p> It </p> <box> \n<loc627> <loc895> \n</box> is \nthe front most cow to the \nright of other cows.\n(1) Zero-shot evaluation\n(2) Few-shot evaluation\nGrounded MLLM\n<p> It </p> <box> \n<loc627> <loc895> \n</box> is \nthe front most cow to the \nright of other cows.\n<p> It </p> \n<box> <loc261> \n<loc1011> \n</box> is the \ngiraffe in the \nmiddle.\nFigure 5: The input format of referring expression generation evaluation under (1) zero-shot and (2)\nfew-shot settings. The bounding boxes shown in the image are for visualization purposes.\n4.2\nMultimodal Referring\nIn addition to multimodal grounding tasks, we evaluate the model\u2019s ability to understand image\nregions or objects users refer to via inputting bounding boxes. Compared with previous multimodal\nLLMs that can only refer image regions or objects to the model via detailed text descriptions, directly\nreferring to image regions using its bounding boxes is more effective and reduces ambiguity.\nWe evaluate the model on the referring expression generation task, which aims to generate unambigu-\nous text descriptions of specific objects or regions within the bounding box. We employ the widely\nused RefCOCOg dataset [MHT+15] to evaluate the model\u2019s performance under both zero-shot and\nfew-shot settings, showcasing its adaptability in different scenarios.\n4.2.1\nEvaluation Setup\nThe model is tasked with generating an associated text description for an object or region given its\nlocation tokens of the bounding boxes (e.g., \u201c<box><loc1><loc2></box>\u201d). Benefiting from the\nunified input format, we use \u201c<p> It </p><box><loc1><loc2></box> is\u201d as prompt to encourage\nthe model to predict its text description. Figure 5 (1) and (2) demonstrate the input format for\nzero-shot and few-shot referring expression generation, respectively. Following previous works, we\nreport results using METEOR and CIDEr metrics. The image resolution is 224\u00d7224. Greedy search\nis used for decoding.\n4.2.2\nResults\nTable 4 presents the zero-shot and few-shot results of referring expression generation on RefCOCOg.\nWe compare KOSMOS-2 with a finetuned listener-speaker model, which introduces an added reward-\nbased module (SLR). Our model obtains impressive zero-shot performance on referring expression\ngeneration, and even outperforms finetuned SLR by 1.1 CIDEr scores. Moreover, when prompted\nwith fewshot demonstrations, KOSMOS-2 shows further improvements, highlighting its in-context\nlearning ability.\nModel\nSetting\nRefCOCOg\nMeteor\nCIDEr\nSLR[YTBB17]\nFinetuning\n15.4\n59.2\nSLR+Rerank[YTBB17]\nFinetuning\n15.9\n66.2\nKOSMOS-2\nZero-shot\n12.2\n60.3\nFew-shot (k = 2)\n13.8\n62.2\nFew-shot (k = 4)\n14.1\n62.3\nTable 4: Results of referring expression generation on RefCOCOg.\n9\n4.3\nPerception-Language Tasks\nIn addition to multimodal grounding and referring tasks, we also evaluate KOSMOS-2 on the vision-\nlanguage tasks following KOSMOS-1. In particular, we perform zero-shot evaluations on two popular\ntasks, including image captioning and visual question answering. Image captioning requires the\nmodel to generate a text description of the given image, whereas visual question answering seeks\nto answer a natural language question based on an image. In order to have a fair comparison with\nKOSMOS-1, we report results without instruction tuning.\n4.3.1\nEvaluation Setup\nFor image captioning, we evaluate the model on the widely used Flickr30k Karpathy split test set.\nWe employ beam search for caption generation, with a beam size of 5. We report results using\nCIDEr [VLZP15] metrics evaluated by COCOEvalCap3. We use the prompt \u201cAn image of\u201d to\ngenerate the image description.\nFor visual question-answering, we evaluate zero-shot performance on the test-dev set of VQAv2.\nGreedy search is used for decoding. We report VQA scores obtained from VQAv2 evaluation server4.\n\u201cQuestion: {question} Answer: {answer}\u201d is used as the prompt for the dataset. The image resolution\nis 224\u00d7224 for both two tasks.\n4.3.2\nResults\nWe present the zero-shot performance on Flickr30k and VQAv2 in Table 5. KOSMOS-2 exhibites\ncomparable overall performance to the KOSMOS-1, showing a slight improvement on Flickr30k\nwhile experiencing a marginal decrease on VQA. While KOSMOS-2 introduces new capabilities of\ngrounding and referring, the model still achieves competitive performance on perception-language\ntasks.\nModel\nFlickr30k\nVQAv2\nCIDEr\nVQA acc.\nFewVLM [JCS+22]\n31.0\n-\nMETALM [HSD+22]\n43.4\n41.1\nFlamingo-3B [ADL+22]\n60.6\n49.2\nFlamingo-9B [ADL+22]\n61.5\n51.8\nKOSMOS-1\n65.2\n46.7\nKOSMOS-2\n66.7\n45.6\nTable 5: Zero-shot image captioning results on Flickr30k test set and zero-shot visual question\nanswering results on VQAv2 test-dev set. We report results of KOSMOS-2 and KOSMOS-1 without\ninstruction tuning.\n4.4\nLanguage Tasks\nWe evaluate KOSMOS-2 on eight language tasks, such as cloze and completion tasks (StoryCloze,\nHellaSwag), Winograd-style tasks (Winograd, Winogrande), commonsense reasoning (PIQA), and\nthree SuperGLUE benchmark [WPN+19] datasets (BoolQ, CB, and COPA). We report the zero-\nshot results in Table 6. Compared with KOSMOS-1, KOSMOS-2 achieves similar performance on\nStoryCloze, HellaSwag, Winograd, Winogrande, and PIQA, experiences a decrease in performance\non CB, but shows improvement on BoolQ and COPA. In summary, KOSMOS-2 demonstrates the\nacquisition of new capabilities while experiencing comparable performance on language tasks. This\nillustrates the potential of the model in balancing and expanding its skills across different domains.\n3https://github.com/salaniz/pycocoevalcap\n4https://eval.ai/challenge/830/overview\n10\nModel\nStory\nCloze\nHella\nSwag\nWinograd\nWinogrande\nPIQA\nBoolQ\nCB\nCOPA\nLLM\n72.9\n50.4\n71.6\n56.7\n73.2\n56.4\n39.3\n68.0\nKOSMOS-1\n72.1\n50.0\n69.8\n54.8\n72.9\n56.4\n44.6\n63.0\nKOSMOS-2\n72.0\n49.4\n69.1\n55.6\n72.9\n62.0\n30.4\n67.0\nTable 6: Zero-shot performance comparisons of language tasks between KOSMOS-2, KOSMOS-1\nand LLM. LLM uses the same text data and training setup to reimplement a language model as\nKOSMOS-1. We report results of KOSMOS-2 and KOSMOS-1 without instruction tuning. Results of\nKOSMOS-1 and the LLM baseline are from [HDW+23].\n5\nConclusion\nWe present KOSMOS-2, a multimodal large language modal, that can ground to the visual world.\nSpecifically, we pre-train KOSMOS-2 by augmenting the multimodal corpora used in KOSMOS-1\nwith GRIT, a large-scale dataset of Grounded Image-Text pairs, which is created by extracting\nand associating noun phrases and referring expressions in the caption to the objects or regions in\nthe scene. KOSMOS-2 enables new capabilities of perceiving image regions and grounding text\noutput to the visual world, which makes grounding as a foundation capability of MLLMs in many\ndownstream applications. Experimental results demonstrate that KOSMOS-2 achieves impressive\nresults on language and vision-language tasks evaluated in KOSMOS-1, grounding tasks including\nphrase grounding and referring expression comprehension, and referring tasks such as referring\nexpression generation.\nAcknowledgement\nSome examples (such as Figure 1) are taken from the WHOOPS corpus [BGBH+23].\nEthics Statement\nThe model presented in this paper is intended for academic and research purposes. The utilization\nof the model to create unsuitable material is strictly forbidden and not endorsed by this work. The\naccountability for any improper or unacceptable application of the model rests exclusively with the\nindividuals who generated such content. We also put Microsoft AI Principles5 into practice when\ndeveloping the models.\nReferences\n[ADL+22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\nHasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman\nRing, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei,\nMarianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Ne-\nmatzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for\nfew-shot learning. In Advances in Neural Information Processing Systems, 2022.\n[AHR+22] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu,\nNaman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke\nZettlemoyer. CM3: A causal masked multimodal model of the Internet. ArXiv,\nabs/2201.07520, 2022.\n[BGBH+23] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici,\nGabriel Stanovsky, and Roy Schwartz.\nBreaking common sense: WHOOPS! a\nvision-and-language benchmark of synthetic and compositional images.\nArXiv,\nabs/2303.07274, 2023.\n5https://www.microsoft.com/ai/responsible-ai\n11\n[BPK+22] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and\nSaehoon Kim. Coyo-700m: Image-text pair dataset, 2022.\n[CLY+19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,\nYu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In\nEuropean Conference on Computer Vision, 2019.\n[CSL+21] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and Geo rey E. Hinton. Pix2seq:\nA language modeling framework for object detection. ArXiv, abs/2109.10852, 2021.\n[DKG+22] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie\nLi, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, Jianfeng Gao, and Lijuan\nWang. Coarse-to-fine vision-language pre-training with fusion in the backbone. ArXiv,\nabs/2206.07643, 2022.\n[DXS+23] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery,\nBrian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Peter R. Florence. Palm-e: An embodied multimodal language model. ArXiv,\nabs/2303.03378, 2023.\n[HDW+23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming\nMa, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal,\nZewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu\nWei. Language is not all you need: Aligning perception with language models. ArXiv,\nabs/2302.14045, 2023.\n[HMVLB20] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy:\nIndustrial-strength Natural Language Processing in Python. 2020.\n[HSD+22] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shum-\ning Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv,\nabs/2206.06336, 2022.\n[HSLS22] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions:\nTuning language models with (almost) no human labor, 2022.\n[JCS+22] Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, and Xiang Ren. A good\nprompt is worth millions of parameters: Low-resource prompt-based learning for\nvision-language models. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 2763\u20132775, Dublin,\nIreland, May 2022. Association for Computational Linguistics.\n[JMC+23] Woojeong Jin, Subhabrata Mukherjee, Yu Cheng, Yelong Shen, Weizhu Chen,\nAhmed Hassan Awadallah, Damien Jose, and Xiang Ren. Grill: Grounded vision-\nlanguage pre-training via aligning text and image regions. ArXiv, abs/2305.14676,\n2023.\n[KSL+21] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and\nNicolas Carion. Mdetr - modulated detection for end-to-end multi-modal understanding.\n2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1760\u2013\n1770, 2021.\n[KZG+16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz,\nStephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein,\nand Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced\ndense image annotations. International Journal of Computer Vision, 123:32\u201373, 2016.\n[LHV+23] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny\nZhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data\nand methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.\n12\n[LLSH23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language models.\nArXiv, abs/2301.12597, 2023.\n[LLWL23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.\narXiv preprint arXiv:2304.08485, 2023.\n[LYY+19] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visual-\nbert: A simple and performant baseline for vision and language. ArXiv, abs/1908.03557,\n2019.\n[LZZ+22] Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li,\nYiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang,\nand Jianfeng Gao. Grounded language-image pre-training. In CVPR, 2022.\n[MHT+15] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana-Maria Camburu, Alan Loddon\nYuille, and Kevin P. Murphy. Generation and comprehension of unambiguous object\ndescriptions. 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 11\u201320, 2015.\n[MWH+22] Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong,\nAlon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. TorchScale:\nTransformers at scale. CoRR, abs/2211.13184, 2022.\n[Ope23] OpenAI. Gpt-4 technical report. 2023.\n[PWC+15] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, J. Hock-\nenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. International Journal of Computer\nVision, 123:74\u201393, 2015.\n[SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\net al. Laion-5b: An open large-scale dataset for training next generation image-text\nmodels. arXiv preprint arXiv:2210.08402, 2022.\n[VLZP15] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-\nbased image description evaluation. In CVPR, pages 4566\u20134575, 2015.\n[WCC+23] Wen Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping\nLuo, Tong Lu, Jie Zhou, Y. Qiao, and Jifeng Dai. Visionllm: Large language model is\nalso an open-ended decoder for vision-centric tasks. ArXiv, abs/2305.11175, 2023.\n[WMH+22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,\nYu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav\nChaudhary, Xia Song, and Furu Wei. Foundation transformers. CoRR, abs/2210.06423,\n2022.\n[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel R Bowman. SuperGLUE: A stickier benchmark for\ngeneral-purpose language understanding systems. arXiv preprint arXiv:1905.00537,\n2019.\n[WYM+22] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,\nChang Zhou, Jingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning framework. In Interna-\ntional Conference on Machine Learning, 2022.\n[YPY+16] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg.\nModeling context in referring expressions. ArXiv, abs/1608.00272, 2016.\n[YTBB17] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L. Berg. A joint speaker-listener-\nreinforcer model for referring expressions. In 2017 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017,\npages 3521\u20133529. IEEE Computer Society, 2017.\n13\nA\nHyperparameters\nThe training hyperparameters of KOSMOS-2 are listed in Table 7.\nHyperparameters\nImage embedding number\n64\nLocation tokens\n1,024\nTraining steps\n60,000\nWarmup steps\n375\nOptimizer\nAdamW\nLearning rate\n2e-4\nLearning rate decay\nLinear\nAdam \u03b2\n(0.9, 0.98)\nWeight decay\n0.01\nBatch size of text corpora\n93\nBatch size of original image-caption pairs\n1,117\nBatch size of grounded image-text pairs\n1,117\nBatch size of interleaved data\n47\nTable 7: Training hyperparameters of KOSMOS-2\nThe instruction tuning hyperparameters are listed in Table 8.\nHyperparameters\nTraining steps\n10,000\nWarmup steps\n375\nLearning rate\n1e-5\nBatch size of language instruction data\n117\nBatch size of vision-language instruction data\n351\nBatch size of grounded image-text pairs\n& grounded instruction data\n1404\nBatch size of text corpora\n30\nBatch size of interleaved data\n15\nTable 8: Instruction tuning hyperparameters of KOSMOS-2\nB\nTemplates for Grounded Instruction Data\nTable 9 presents the instruction templates of expression generation based on its associated bounding\nboxes during instruction tuning.\n\u2022 \"What is <p> it </p><box><loc1><loc2></box>? It is {expression}.\"\n\u2022 \"What is <p> this </p><box><loc1><loc2></box>? This is {expression}.\"\n\u2022 \"Describe <p> this object </p><box><loc1><loc2></box>. This object is {expression}.\"\n\u2022 \"<p> It </p><box><loc1><loc2></box> is {expression}.\"\n\u2022 \"<p> This </p><box><loc1><loc2></box> is {expression}.\"\n\u2022 \"<p> The object </p><box><loc1><loc2></box> is {expression}.\"\nTable 9: Instruction templates used for expression generation.\n14\nC\nExamples of GRIT\nWe present some examples of the GRIT corpus in Figures 6\u20139. The grounded image-text pairs span\nover various domains and contain different numbers of objects.\nFigure 6: Example from GRIT. Caption: \u201cA serving of kale and roasted vegetable salad on an\naluminium tray served with a small white bowl filed with creamy light green avocado Caesar\ndressing\u201d.\nFigure 7: Example from GRIT. Caption: \u201cA Keto Chicken Nugget being dipped into a bowl of keto\nhoney mustard.\u201d.\n15\nFigure 8: Example from GRIT. Caption: \u201cSolar cells on a red roof are in the foreground. The Sydney\nskyline is in the background.\u201d.\nFigure 9: Example from GRIT. Caption: \u201cWoman standing outdoors in a city landscape and wearing\na hijab. Her arm is around a young girl who is hugging her side. The background is blurred.\u201d.\nD\nMore Examples of KOSMOS-2\nAs illustrated in Figure 10, multimodal referring capability used for visual dialogue can unlock\npotential in human-AI interaction. In Figure 11, our approach demonstrates its in-context learning\nability for fine-grained object detection using both text and image descriptions. Figure 12 and\nFigure 13 showcase more selected examples, including grounded visual question answering, grounded\nimage captioning, and multimodal referring.\n16\nWhat he is doing?\nThe man is riding a \nhorse.\nHow about this man?\nHe is holding a flag.\nWhich person is this \none facing to?\nThe person on the \nright side of the fence.\nWhat is the name of \nthis building?\nSydney Opera House.\nWhen it was completed?\nIn 1973.\nWhat is the name of \nthis?\nSydney Harbour \nBridge.\nFigure 10: Examples of visual dialogue generated from KOSMOS-2.\n17\n<p>a downy woodpecker</p>\n<box><loc97><loc942></box>\n<box><loc115><loc958></box>\nDescription of three toed \nwoodpecker: it has black and \nwhite stripes throughout the body \nand a yellow crown. Description \nof downy woodpecker: it has \nwhite spots on its black wings and \nsome red on its crown.\n<p>a downy woodpecker</p>\n<box><loc115><loc958></box>\nThis is a downy woodpecker \n<p>a downy woodpecker</p>\n\u274c\n\u2714\n\u2714\nWith text \ndescription\nWith image \ndescription\nFigure 11: Examples of object detection with multimodal descriptions from KOSMOS-2.\n18\nInput Prompt\nCompletion\n(1) Grounded question answering\nQuestion: Where is the coach? \nAnswer: <p>The coach</p>\nThe coach is standing in the \nmiddle of the field.\nQuestion: What does it say? \nAnswer:\n\"Je suis Charlie!\" is written in \nFrench on a chalkboard.\nQuestion: Where is it? Answer: \nIn a microwave oven.\nInput Prompt\nCompletion\nYou are driving this car. To pick \nup that man,  you need to\ngo through the intersection \nand turn right.\nQuestion: What will happen to it?\nAnswer:\nIt will melt away as it warms up. \nQuestion: What makes this image\nweird? Answer:\nThe image is weird because the \nsnowman is sitting next to a \ncampfire in the snow. \n(2) Multimodal referring via bounding boxes\nFigure 12: Examples generated from KOSMOS-2.\n19\nInput Prompt\nDescribe this bird in detail:\nCompletion\nThe eagle is a large bird \nwith a black and white \nbody and a yellow beak. \nIt has large, dark wings\nand a pointed tail. It \nstands on a stump and \nspreads its wings, \ndisplaying its \nmagnificent plumage.\nInput Prompt\nDescribe this image in detail:\nSix fighter jets fly in \nformation in the sky, \ncreating a beautiful \ncloud trail.\nCompletion\nInput Prompt\nDescribe this image in detail:\nCompletion\nA man is standing in \nfront of a fire, blowing \nthe flames out of his \nmouth. He is in the \nmiddle of a tropical \nsetting with palm trees\nsurrounding him.\nFigure 13: Examples of grounded image captioning generated from KOSMOS-2.\n20\n"
  },
  {
    "title": "MotionGPT: Human Motion as a Foreign Language",
    "link": "https://arxiv.org/pdf/2306.14795.pdf",
    "upvote": "26",
    "text": "MotionGPT: Human Motion as a Foreign Language\nBiao Jiang1,2\u2217\nXin Chen2\u2217\nWen Liu2\nJingyi Yu3\nGang Yu2\nTao Chen1\u2020\n1Fudan University\n2Tencent PCG\n3 ShanghaiTech University\nhttps://github.com/OpenMotionLab/MotionGPT\nAbstract\nThough the advancement of pre-trained large language models unfolds, the explo-\nration of building a unified model for language and other multimodal data, such\nas motion, remains challenging and untouched so far. Fortunately, human motion\ndisplays a semantic coupling akin to human language, often perceived as a form of\nbody language. By fusing language data with large-scale motion models,motion-\nlanguage pre-training that can enhance the performance of motion-related tasks\nbecomes feasible. Driven by this insight, we propose MotionGPT, a unified, versa-\ntile, and user-friendly motion-language model to handle multiple motion-relevant\ntasks. Specifically, we employ the discrete vector quantization for human motion\nand transfer 3D motion into motion tokens, similar to the generation process of\nword tokens. Building upon this \u201cmotion vocabulary\u201d, we perform language mod-\neling on both motion and text in a unified manner, treating human motion as a\nspecific language. Moreover, inspired by prompt learning, we pre-train MotionGPT\nwith a mixture of motion-language data and fine-tune it on prompt-based question-\nand-answer tasks. Extensive experiments demonstrate that MotionGPT achieves\nstate-of-the-art performances on multiple motion tasks including text-driven motion\ngeneration, motion captioning, motion prediction, and motion in-between.\n1\nIntroduction\nRecent years have witnessed a significant breakthrough in pre-trained large language models such as\nGPT [36, 37, 3, 28], BERT [7], and T5 [38, 5], which lead to the convergence of language [61, 49],\nimage [35, 52, 21], mesh [57, 27] and mutlimodal [8] modeling. Nevertheless, a general pre-trained\nmodel for human motion and language has yet to emerge. This pre-trained motion-language model,\ncapable of supporting numerous motion-relevant tasks through prompts, should benefit diverse fields\nlike gaming, robotics, virtual assistant, and human behavior analysis.\nPrevious research on human motion has explored various tasks, including motion generation [31, 11,\n48, 54, 59], motion captioning [9, 12], and motion prediction [58, 63, 25]. Recent text-to-motion\nworks[48, 60, 32, 54] have attempted to employ pre-trained language-relevant models [7, 35]. For\ninstance, MDM [48] learns a motion diffusion model with conditional text tokens from CLIP [35],\nwhile MLD [54] integrates motion latent space to improve the efficiency of motion diffusion process.\nOn the other hand, MotionCLIP [47] and TM2T [12] concentrate on modeling the coupled relationship\nbetween motion and text description. However, the above approaches treat motion and language as\nseparate modalities, which often require strictly paired motion and text data. Moreover, since the\nsupervisions are task-specific, they can hardly generalize effectively to unseen tasks or data, as they\nlack a comprehensive understanding of the relationship between motion and language. We thus focus\non building a pre-trained motion-language model, which can generalize to various tasks and learn\nin-depth motion-language correlation knowledge from more feasible motion and language data.\n\u2217Contributed equally and work done while Biao Jiang was a Research Intern with Tencent PCG.\n\u2020Corresponding author.\narXiv:2306.14795v2  [cs.CV]  20 Jul 2023\nCan you generate a motion that a person \nkneeling on the ground gets up?\nDescribe the motion of \nsomeone as you will.\nA person kicks two times on his left\nthen kicks forward two times.\nA man stands and moves his left leg back-\nward, then try to throw something hard.\nCan you translate this motion to text?\nCan you generate more from this motion?\nFigure 1: MotionGPT can address diverse motion-relevant tasks uniformly given different instructions.\nWe provide the results on text-to-motion (the upper left), motion captioning (the bottom left), motion\ncompletion (the upper right), and the language question-to-answer (the bottom right). The left to right\nof motion represents the time order. Blue motion denotes the input, and yellow is the generation.\nTwo challenges are crucial and need to be solved for pre-training a promising motion-language model.\nThe first is modeling the relation between language and motion, and the second is building a uniform\nmulti-task framework that can generalize to new tasks. Fortunately, human motion exhibits a semantic\ncoupling similar to human language, often interpreted as a form of body language. Building upon\nthis observation,we follow vision-language pre-training from BEiT-3 [52] to treat human motion as\na specific foreign language. By integrating motion and language data together and encoding them\nwithin a single vocabulary, the relationship between motion and language becomes more apparent.\nTherefore, with recent significantly larger-scale language data and models, the motion-language pre-\ntraining has great potential to improve the performance on motion tasks. Meanwhile, this pre-training\non language enables textual instructions like prompts in InstructGPT [28] and makes the model more\nversatile and user-friendly for various motion tasks.\nIn this work, we propose a uniform motion-language framework, namely MotionGPT, that leverages\nthe strong language generation and zero-shot transfer abilities of pre-trained language models for\ndoing human motion-related tasks. To enable MotionGPT to comprehend and generate human-like\nmotions, we first learn a motion-specific vector quantized variational autoencoder (VQ-VAE) model\nto construct \u201cmotion vocabulary\u201d, akin to English vocabulary and then convert raw motion data into a\nsequence of motion tokens. These tokens are then processed by a pre-trained language model [38, 5]\nthat learns the underlying grammar and syntax of the motion language, as well as its relationship with\nthe corresponding textual descriptions. To effectively integrate language and motion in MotionGPT,\nwe design a two-stage training scheme. We first pre-train the language model on the raw motion\ndataset to learn the basic grammar and syntax of the motion language. For prompt tuning, we\nfine-tune the language model on an instruction dataset, which contains both textual descriptions and\nmotion data, to learn the correlation between the two modalities. Extensive experiments demonstrate\nthat MotionGPT achieves state-of-the-art performance on text-to-motion, motion-to-text, motion\nprediction, and motion in-between.\nWe summarize our contributions as follows: (1) We propose a uniform motion-language generative\npre-trained model, MotionGPT, which treats human motion as a foreign language, introduces natural\nlanguage models into motion-relevant generation, and performs diverse motion tasks with a single\nmodel. (2) We introduce a motion-language training scheme with instruction tuning, to learn from\ntask feedback and produce promising results through prompts. (3) We propose a general motion\nbenchmark for multi-task evaluation, wherein MotionGPT achieves competitive performance across\ndiverse tasks, including text-to-motion, motion-to-text, motion prediction, and motion in-between,\nwith all available codes and data.\n2\nMethods\nText-to-Motion\nMotion-to-Text\nMotion Prediction\nMotion In-between\nRandom Motion\nRandom Description\nT2M-GPT [48]\n\"\n%\n%\n%\n\"\n%\nMLD [54]\n\"\n%\n%\n%\n\"\n%\nTM2T [12]\n\"\n\"\n%\n%\n%\n%\nMDM [48]\n\"\n%\n\"\n\"\n\"\n%\nMotionDiffuse[60]\n\"\n%\n\"\n\"\n\"\n%\nMotionGPT (Ours)\n\"\n\"\n\"\n\"\n\"\n\"\nTable 1: Comparison of recent state-of-the-art methods on diverse motion-relevant tasks. Random\nMotion and Random Caption represent unconstrained generation of motions and motion descriptions.\n2\nRelated Work\nHuman Motion Synthesis involves generating diverse and realistic human-like motion using multi-\nmodal inputs, such as text [11, 32, 60, 48, 12, 1, 18], action [31, 13, 48, 54], and incomplete motion\n[58, 63, 25, 48]. Text-to-motion is one of the most important motion generation tasks, due to the user-\nfriendly and convenient language input. MDM [48] proposes a diffusion-based generative model [15]\nseparately trained on several motion tasks. MLD [54] advances the latent diffusion model [45, 40]\nto generate motions based on different conditional inputs. T2M-GPT [59] investigates a generative\nframework based on VQ-VAE and Generative Pre-trained Transformer (GPT) for motion generation.\nMotion completion task generates motion conditioning on partial motions, such as classical motion\nprediction [58, 63, 25] or motion in-between [48], which generates the intermediate motion while the\nfirst and last parts are fixed. Although they show promising results in various human motion tasks,\nmost above methods are limited in using a single model to handle multiple tasks. We thus propose a\nuniform approach that treats human motion as a foreign language, and leverages the strong language\ngeneration and zero-shot transfer abilities of pre-trained language models\nHuman Motion Captioning. To describe human motion with natural languages, [46] learns the\nmapping from motions to language relying on two statistical models. Furthermore, recurrent networks\nhave also been used in [56, 34]. More recently, TM2T [12] proposed a new motion representation\nthat compresses motions into a short sequence of discrete variables, then uses a neural translation\nnetwork to build mappings between two modalities. While previous research like TM2T [12]\nincorporated captioning modules into their training pipeline for motion generation, these approaches\nare constrained to bidirectional translation between text and motion within one uniform framework.\nLanguage Models and Multi-Modal. Large-scale language models (LLMs) [7, 6, 38, 3, 61, 49],\nenabled by extensive datasets and model size, have demonstrated impressive comprehension and\ngeneration capabilities, elevating natural language processing to new heights. BERT [7] pre-trains\ndeep bidirectional language representations that can support downstream tasks. T5 [38] introduced a\nunified framework that converts all text-based language problems into a text-to-text format. More\nrecent research [53, 2, 28, 5] find that by fine-tuning pre-trained models using input-output pairs\nconsisting of instructions and coupled answers, the performance of pre-trained models can be further\nimproved. FLAN [5] presents an instruction-tuning technique that surpasses the performance of\nnon-tuned models in unseen tasks. Recently, the wave of multi-modal models [21, 16, 20] is intriguing\nto process text along with other modalities, such as images [21, 16, 8], audio [14, 8], and videos [55].\nCLIP [35] further learns a semantic latent representation that couples images with corresponding\nlanguage descriptions. Despite the success of language models in various vision-language tasks, the\ndevelopment of multi-modal language models that can handle human motion is still limited.\nMotion Language Pre-training. Existing text-to-motion generation methods [11, 32, 48, 12, 1, 18]\ncan be characterized as caption-to-motion, where the models take in a pure text description of the\ndesired motion. While these methods can generate motions from textual descriptions, they are\noften limited in supporting instructions from users like InstructGPT [28]. In other words, they do\nnot allow users to provide context-specific instructions for certain applications. MotionCLIP [47]\nutilizes the language and visual understanding of CLIP [35] to align its latent space with a motion\nauto-encoder. Meanwhile, many language models, such as T5[38] and InstructGPT [28], have been\ndeveloped to address diverse language processing tasks, including translation, question answering,\nand classification. These models are typically designed to map a given text input to a target output,\nsuch as a translation or answer. However, while these models have shown remarkable performance in\n3\nMOTION TOKENIZER (SEC. 3.1)\nMOTION VOCABULARY\nMotion\nEncoder\nMotion\nDecoder\nInput Motion\n(optional)\nGenerated Motion\nMotion Codebook\nMotion Tokens\nMOTION-AWARE LANGUAGE MODEL (SEC. 3.2)\nCan you give me a motion that \nA person kneeling on the \nground gets up ?\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nLanguage Model T5\nLanguage\nEncoders\nOutput Tokens\nInput Mixed Tokens\nText Codebook\u2026\n\u2026\nText Tokens\n<Token1>\n<Token2>\n<Token1>\n<Token2>\n<Token3>\n<Token4>\n<Token5>\n<Token6>\n\u2026\nLinear + Softmax\nLanguage\nDecoders\n\u2026\n\u2026\n\u2026\nKencdec\nVencdec\nFigure 2: Method overview: MotionGPT consists of a motion tokenizer V (Sec. 3.1) and a motion-\naware language model (Sec. 3.2). Combining Motion Tokens learned by V and Text Tokens by text\ntokenizer, we then learn motion and language jointly utilizing language model as backbone.\nlanguage tasks, they have not been widely applied to motion tasks. Therefore, we propose MotionGPT\nto enable the effective integration of natural language models with human motion tasks, providing a\nunified solution for motion synthesis problems.\n3\nMethod\nTo involve large language data and models in the motion generation tasks, we propose a unified\nmotion-language framework named MotionGPT. As illustrated in Fig. 2, MotionGPT consists of a\nmotion tokenizer responsible for converting raw motion data into discrete motion tokens (Sec. 3.1),\nas well as a motion-aware language model that learns to understand the motion tokens from large\nlanguage pre-training models by corresponding textual descriptions (Sec. 3.2). To address motion-\nrelevant tasks, we introduce a three-stage training scheme (Sec. 3.3) of MotionGPT for the training\nof motion tokenizer, motion-language pre-training, and instruction tuning.\nWe first propose the motion tokenizer consisting of a motion encoder E and a motion decoder D, to\nencode a M frame motion m1:M = {xi}M\ni=1 into L motion tokens z1:L = {zi}L\ni=1, L = M/l, and\ndecode z1:L back into the motion \u02c6m1:M = D(z1:L) = D(E(m1:M)), where l denotes the temporal\ndownsampling rate on motion length. Then, given an N length sentence w1:N = {wi}N\ni=1 describing\na motion-related question or demand, MotionGPT aims to generate its answer as L length tokens\n\u02c6x1:L = {\u02c6xi}L\ni=1. It could be the human motion tokens \u02c6x1:L\nm or the text tokens \u02c6x1:L\nt\n, which results in a\nmotion \u02c6m1:M or a sentence \u02c6w1:L like a description of the given motion.\n3.1\nMotion Tokenizer\nTo represent motion in discrete tokens, we pre-train a 3D human motion tokenizer V based on the\nVector Quantized Variational Autoencoders (VQ-VAE) architecture used in [50, 44, 12, 59]. Our\nmotion tokenizer consists of an encoder E and a decoder D. The encoder generates discrete motion\ntokens with high informative density, while the decoder is able to reconstruct the motion tokens into\nmotion sequences \u02c6m1:M. This approach enables us to efficiently represent motion as a language,\nfacilitating the integration of motion and language for various motion-related tasks.\nSpecifically, the motion encoder E first applies 1D convolutions to given frame-wise motion features\nm1:M along the time dimension, to obtain latent vectors \u02c6z1:L = E(m1:M). Next, we transform\n\u02c6z into a collection of codebook entries z through discrete quantization. The learnable codebook\nZ = {zi}K\ni=1 \u2282 Rd consists of K latent embedding vectors, each of dimension d. The process of\nquantization Q(\u00b7) replaces each row vector b with its nearest codebook entry bk in Z, written as\nzi = Q(\u02c6zi) := arg minzk\u2208Z \u2225\u02c6zi \u2212 zk\u22252 .\n(1)\nAfter quantization, the motion decoder D project z1:L = {zi}L\ni=1 back to raw motion space as the\nmotion \u02c6m1:M with M frames. To train this motion tokenizer, we follow [12, 59] to utilize three\n4\ndistinct loss functions for training and optimizing the motion tokenizer: LV = Lr+Le+Lc, where the\nreconstruction loss Lr, the embedding loss Le, and the commitment loss Lc. To further improve the\ngenerated motion quality, we follow [59] to utilize L1 smooth loss and velocity regularization in the\nreconstruction loss, as well as exponential moving average (EMA) and codebook reset techniques [39]\nto enhance codebook utilization during training. We provide more details about the architecture and\nthe training of our motion tokenizer in the supplement.\n3.2\nMotion-aware Language Model\nEmploying this motion tokenizer, a human motion m1:M can be mapped to a sequence of motion\ntokens z1:L, allowing for joint representation with similar vocabulary embedding in language mod-\nels [19, 38, 28]. By combining them in the unified vocabulary, we then learn motion and language\njointly. We first represent motion tokens z1:L as a sequence of indices s1:L = {si}L\ni=1, where si\ncorresponds to the index number of motion tokens z1:L. On the other hand, previous language models,\nsuch as T5 [38], encode text as WordPiece tokens. They utilized a vocabulary of Kt word pieces and\ntrained the SentencePiece [19] model on a mixture of language datasets.\nMost previous text-to-motion [12, 54, 59] or motion-to-text [12] approaches employ different modules\nto handle text and motion individually, while we aim to model text and human motion together and in\nthe same way. To achieve this, we combine the original text vocabulary Vt = {vi\nt}Kt\ni=1 with motion\nvocabulary Vm = {vi\nm}Km\ni=1, which is order-preserving to our motion codebook Z. Moreover, Vm\nincludes several special tokens like boundary indicators, for example, </som> and </eom> as the start\nand end of the motion. Thus, we employ a new unified text-motion vocabulary V = {Vt, Vm}, and\ncan formulate diverse motion-related tasks in a general format, where both input \"words\" and output\n\"words\" are from the same V . These \"words\" can represent natural language, human motion, or even\na mixture of two, depending on the specific task to be solved. Therefore, our MotionGPT allows for\nthe flexible representation and generation of diverse motion-related outputs within a single model.\nTo address the conditioned generation task, we employ a transformer-based model based on the\narchitecture proposed in [38], which effectively maps the input sequences to the output. Our source\ninput consists of a sequence of tokens Xs = {xsi}N\ni=1, where xs \u2208 V and N represents the\ninput length. Similarly, the target output is Xt = {xti}L\ni=1, where xt \u2208 V and L denotes the\noutput length. As shown in Fig. 2, the source tokens are fed into the transformer encoder, and\nthe subsequent decoder predicts the probability distribution of the potential next token at each step\np\u03b8(xt | xs) = Q\ni p\u03b8\n\u0000xi\nt | x<i\nt , xs\n\u0001\nin an autoregressive manner. Therefore, during the training\nprocess, the objective is to maximize the log-likelihood of the data distribution:\nLLM = \u2212\nLt\u22121\nX\ni=0\nlog p\u03b8\n\u0000xi\nt | x<i\nt , xs\n\u0001\n.\n(2)\nBy optimizing this objective, MotionGPT learns to capture the underlying patterns and relationships\nfrom the data distribution, facilitating the accurate and meaningful generation of the target \"words\".\nDuring the inference process, the target tokens are sampled recursively from the predicted distribution\np\u03b8\n\u0000\u02c6xt\ni | \u02c6xt\n<i, xs\n\u0001\nuntil the end token (i.e., </s>). This sampling strategy enables the generation of\nthe target sequence in a step-by-step manner, where each token is probabilistically determined based\non the previously generated tokens and the given source input.\n3.3\nTraining Strategy\nSince T5s have only been exposed to language data, represented within a text vocabulary Vt, we thus\nbridge motion and language and enable this language model to comprehend human motion concepts,\nby learning the motion vocabulary Vm. As shown in Fig. 3, our training scheme includes three stages:\n(1) Training of motion tokenizer, which focuses on learning the motion codebook to represent human\nmotion as discrete tokens. (2) Motion-language pre-training stage, which includes unsupervised and\nsupervised objectives to learn the relationship between motion and language. (3) Instruction tuning\nstage, which tunes the model based on prompt-based instructions for different motion-relevant tasks.\nTraining of Motion Tokenizer. We first learn the motion tokenizer using the objective defined in\nEquation 3.1. This training process allows any human motion sequence \u02c6x1:L to be represented as a\nsequence of motion tokens, enabling seamless integration with textual information. Once optimized,\nthe motion tokenizer remains unchanged throughout the subsequent stages of the pipeline.\n5\nStep 1\nStep 2\nStep 3\nTraining of Motion Tokenizer.\nMotion-language Pre-training.\nInstruction Tuning.\nMotion Codebook\n\u2026\nMotion Tokens\n<Token1>\n<Token2> \u2026\nA motion sequence \nis sampled from 3D \nmotion dataset.\nMotion tokenizer \nlearns motion \nrepresentation.\nMotion codebook\nis used to represent \nhuman motion as \ndiscrete tokens.\n\u2026\n<motion_id_512>\nA motion and a \nlanguage description \nare sampled.\nThis motion is \nmapped to discrete \nmotion indices and \nmixed with words.\nThis data is used to \npre-train our motion-\nlanguage model.\nA person catches\nand throws a ball.\nA person catches\nand throws a ball. \n<motion_id_33>\n<motion_id_439> <motion_id_70>\nCan you show me that a \nperson does three straight \njumping jacks ?\nCould you provide a motion \nthat a person swings his left \nleg out to the side ?\nMotion\nDecoder\nMotion\nEncoder\nLanguage\nDecoders\nLanguage \nModel\nLanguage\nEncoders\nSoftmax\nThe QAs are sampled from \nour prompt templates.\nThe prompts are used to \nfine-tune our model on \ndiverse motion tasks\nLanguage\nDecoders\nLanguage \nModel\nLanguage\nEncoders\nSoftmax\nFigure 3: Training Scheme. We introduce three training steps for our MotionGPT (Sec. 3.3): First\nV learn a codebook for discrete motion representation. Then we train language using a mixture\nof language and motion data to learn the semantic coupling between text and motion. Finally, we\nfine-tune the model in a multi-task text-motion dataset with instructions.\nMotion-language Pre-training Stage. The T5 models [38, 5] are trained and fine-tuned on natural\nlanguage datasets with instruction-based phrasing [5, 28]. We continue to pre-train this model using a\nmixture of language and motions data in both unsupervised and supervised manners: 1) To generalize\nto various downstream tasks like [7, 37, 38, 28], we follow [38] to design an objective, where a\ncertain percentage (15%) of tokens in the input tokens Xs are randomly replaced with a special\nsentinel token. On the other side, the corresponding target sequence is constructed by extracting\nthe dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence,\nalong with an additional sentinel token to indicate the end of the target sequence. 2) We then learn\nthe motion-language relation by the supervision of paired text-motion datasets [11, 33]. We train\nMotionGPT on the supervised motion-language translation, where the input is either a human motion\nor a text description.After unsupervised and supervised training processes, we aim to equip our model\nwith the understanding of text and motion relationships.\nInstruction Tuning Stage. We construct a multi-task text-motion dataset by formulating it as\ninstructions, building upon the foundation of existing text-to-motion datasets such as HumanML3D\n[11] and KIT [33]. Specifically, we define 15 core motion tasks, such as motion generation with text,\nmotion captioning, motion prediction, and others. For each task, we compose dozens of different\ninstruction templates, resulting in more than one thousand different tasks, each having a unique\ninstruction prompt. For example, an instruction prompt for motion generation task could be \u201cCan\nyou generate a motion sequence that depicts \u2018a person emulates the motions of a waltz dance\u2019?\u201d.\nSimilarly, for the motion captioning task, the instruction prompt could be \u201cProvide an accurate caption\ndescribing the motion of <motion_tokens>\u201d, where <motion_tokens> represents a sequence of motion\ntokens generated by our motion tokenizer. We have demonstrated the efficacy of instruction tuning in\nSec. 4.3, which leads to improvement across various tasks and enhances the model performance for\nunseen tasks or prompts. More examples of prompts are provided in the supplements.\n4\nExperiments\nExtensive comparisons evaluate the performance of our MotionGPTs across multiple motion-relevant\ntasks and datasets. Details of the dataset settings, evaluation metrics, and implementation specifics\n(Sec. 4.1) are provided. We first present a uniform benchmark by comparing our approach with other\nSOTAs across various tasks (Sec. 4.2). Then, we evaluate each specific comparison on text-to-motion\n(Sec. 4.2), motion-to-text (Sec. 4.2), motion prediction and motion in-between (Sec. 4.2). The\nsupplements include more qualitative results, user studies, and further implementation details.\n4.1\nExperimental Setup\nDatasets. General motion synthesis can support diverse task settings, and thus previous datasets\nand a modified benchmark are utilized to evaluate MotionGPT. The study primarily focuses on two\n6\nMethods\nText-to-Motion\nMotion-to-Text\nMotion Prediction\nMotion In-between\nR TOP1\u2191\nFID\u2193\nDIV\u2192\nR TOP3\u2191\nBleu@4\u2191\nCider\u2191\nFID\u2193\nDIV\u2192\nFID\u2193\nDIV\u2192\nReal\n0.511\u00b1.003\n0.002\u00b1.000\n9.503\u00b1.065\n0.828\n-\n-\n0.002\n9.503\n0.002\n9.503\nMLD [54]\n0.481\u00b1.003\n0.473\u00b1.013\n9.724\u00b1.082\n-\n-\n-\n-\n-\n-\n-\nT2M-GPT [48]\n0.491\u00b1.003\n0.116\u00b1.004\n9.761\u00b1.081\n-\n-\n-\n-\n-\n-\n-\nTM2T [12]\n0.424\u00b1.017\n1.501\u00b1.003\n8.589\u00b1.076\n0.823\n7.00\n16.8\n-\n-\n-\n-\nMDM [48]\n0.320\u00b1005\n0.544\u00b1.044\n9.559\u00b1.086\n-\n-\n-\n6.031\n7.813\n2.698\n8.420\nMotionGPT (Ours)\n0.492\u00b1.003\n0.232\u00b1.008\n9.528\u00b1.071\n0.827\n12.47\n29.2\n0.905\n8.972\n0.214\n9.560\nTable 2: Comparison of four motion-related tasks on HumanML3D [11] dataset. The evaluation\nmetrics are computed using the encoder introduced in [11]. The empty columns of previous methods\nindicate that they can not handle the task. The arrows (\u2192) indicate that closer to Real is desirable.\nBold and underline indicate the best and the second best result on text-to-motion task.\ntext-to-motion datasets: HumanML3D [11] and KIT [33]. The KIT dataset provides 6,353 textual\ndescriptions corresponding to 3,911 motion sequences, while the HumanML3D dataset [11] is a more\nrecent dataset that contains 14,616 motion sequences obtained from AMASS [26], along with 44,970\nsequence-level textual descriptions. To evaluate MotionGPT as a uniform framework on tasks, such\nas motion prediction and motion completion (in-between), we utilize the motion sequences available\nin HumanML3D, which is also a subset of the larger AMASS dataset. Following the previous works\n[11, 54, 48], we adopt the same motion representation for fair comparisons, which combines joint\nvelocities, positions, and rotations. By using this consistent representation, MotionGPT enables the\navailability to support further studies in the field. (cf. supplement for the benchmark details.)\nEvaluation Metrics are summarized as four parts. (1) Motion quality: Frechet Inception Distance\n(FID) is our primary metric based on a feature extractor [11] to evaluate the distance of feature\ndistributions between the generated and real motions. For motion completion, we utilize metrics\nused in motion prediction studies [58, 63, 25], such as Average Displacement Error (ADE) and\nFinal Displacement Error (FDE), to evaluate the accuracy of the predicted motion. (2) Generation\ndiversity: We utilize the Diversity (DIV) metric to assess the motions diversity, which calculates\nthe variance through features extracted from the motions [11]. MultiModality (MM) measures the\ndiversity of generated motions within the same text description of motion. (3) Text matching: Based\non the feature space from [11], the motion-retrieval precision (R Precision) evaluates the accuracy\nof matching between texts and motions using Top 1/2/3 retrieval accuracy. Multi-modal Distance\n(MM Dist) measures the distance between motions and texts. (4) Linguistic quality: We follow\n[12] utilizing linguistic metrics from natural language studies, including BLUE [29], Rouge [24],\nCider [51], and BertScore [62] to evaluate the quality of generated motion captions.\nImplementation Details. We set the codebook of motion tokenizer as K \u2208 R512\u00d7512 for most\ncomparisons. The motion encoder E incorporates a temporal downsampling rate l of 4. We utilize\nT5 [38] as the underlying architecture for our language model, with a baseline model consisting\nof 12 layers in both the transformer encoder and decoder. The feed-forward networks have an\noutput dimensionality of dff = 3072, and the attention mechanisms employ an inner dimensionality\nof dkv = 64. The remaining sub-layers and embeddings have a dimensionality of dmodel = 768.\nMoreover, all our models employ the AdamW optimizer for training. The motion tokenizers are\ntrained utilizing a 10\u22124 learning rate and a 256 mini-batch size, while our language models have\na 2 \u00d7 10\u22124 learning rate for the pre-train stage, 10\u22124 for the instruction tuning stage, and a 16\nmini-batch size for both stages. The motion tokenizer undergoes 150K iterations of training, while\nthe language model undergoes 300K iterations during the pre-train stage and another 300K iterations\nduring the instruction tuning stage. All models are trained on 8 Tesla V100 GPUs.\n4.2\nComparisons on Motion-relevant Tasks\nComparisons on Multiple Tasks. By introducing a uniform framework that treats human motion\nas a foreign language, we open up the exploration of diverse motion-relevant tasks. We employ\na 220M pre-trained Flan-T5-Base[38, 5] model as our backbone and fine-tune the model through\nthe pre-training and instruction tuning stage (Sec. 3.3) for all following comparisons. As shown in\nTab. 2, we evaluate MotionGPT against state-of-the-art methods on key tasks such as text-conditioned\nmotion generation [54, 59, 12, 48], motion captioning [12], motion prediction [48], and motion\nin-between[48]. While we leverage existing results from previous works or benchmarks for text-\n7\nMethods\nRPrecision\u2191\nFID\u2193\nMMDist\u2193\nDiversity\u2192\nMModality\u2191\nTop1\nTop2\nTop3\nReal\n0.511\u00b1.003\n0.703\u00b1.003\n0.797\u00b1.002\n0.002\u00b1.000\n2.974\u00b1.008\n9.503\u00b1.065\n-\nTM2T [12]\n0.424\u00b1.003\n0.618\u00b1.003\n0.729\u00b1.002\n1.501\u00b1.017\n3.467\u00b1.011\n8.589\u00b1.076\n2.424\u00b1.093\nT2M [11]\n0.457\u00b1.002\n0.639\u00b1.003\n0.740\u00b1.003\n1.067\u00b1.002\n3.340\u00b1.008\n9.188\u00b1.002\n2.090\u00b1.083\nMotionDiffuse [60]\n0.491\u00b1.001\n0.681\u00b1.001\n0.782\u00b1.001\n0.630\u00b1.001\n3.113\u00b1.001\n9.410\u00b1.049\n1.553\u00b1.042\nMDM [48]\n0.320\u00b1.005\n0.498\u00b1.004\n0.611\u00b1.007\n0.544\u00b1.044\n5.566\u00b1.027\n9.559\u00b1.086\n2.799\u00b1.072\nMLD [54]\n0.481\u00b1.003\n0.673\u00b1.003\n0.772\u00b1.002\n0.473\u00b1.013\n3.196\u00b1.010\n9.724\u00b1.082\n2.413\u00b1.079\nT2M-GPT [59]\n0.491\u00b1.003\n0.680\u00b1.003\n0.775\u00b1.002\n0.116\u00b1.004\n3.118\u00b1.011\n9.761\u00b1.081\n1.856\u00b1.011\nMotionGPT (Pre-trained)\n0.435\u00b1.003\n0.607\u00b1.002\n0.700\u00b1.002\n0.160\u00b1.008\n3.700\u00b1.009\n9.411\u00b1.081\n3.437\u00b1.091\nMotionGPT (Fine-tuned)\n0.492\u00b1.003\n0.681\u00b1.003\n0.778\u00b1.002\n0.232\u00b1.008\n3.096\u00b1.008\n9.528\u00b1.071\n2.008\u00b1.084\nTable 3: Comparison of text-to-motion on HumanML3D [11]. The empty MModality indicates\nReal motion is deterministic. These methods are sorted by FID. Pre-trained and Fine-tuned indicate\nuniform motion-language pre-training and specific fine-tuning on this task. (cf. Tab. 2 for notations.)\nMethods\nRPrecision\u2191\nMMDist\u2193\nLengthavg\u2191\nBleu@1\u2191\nBleu@4\u2191\nRouge\u2191\nCider\u2191\nBertScore\u2191\nTop1\nTop3\nReal\n0.523\n0.828\n2.901\n12.75\n-\n-\n-\n-\n-\nTM2T[12]\n0.516\n0.823\n2.935\n10.67\n48.9\n7.00\n38.1\n16.8\n32.2\nMotionGPT (Ours)\n0.543\n0.827\n2.821\n13.04\n48.2\n12.47\n37.4\n29.2\n32.4\nTable 4: Comparison of motion captioning on HumanML3D [11]. The evaluation metrics follow\n[12], while we use the ground truth texts without pre-processing for linguistic metrics calculation.\nto-motion and motion-to-text tasks, we re-implement the motion diffusion models [48] for motion\nprediction and evaluate it under the same metrics and settings. Please note that some methods are\ndesigned for specific tasks, and thus some metrics are empty for tasks they cannot handle. The results\npresented in Tab. 2 demonstrate that our MotionGPT achieves competitive performance across all\nevaluated tasks, highlighting its capability to address diverse motion tasks within a single model.\nComparisons on Text-to-Motion. The text-to-motion task involves generating human motion\nsequences based on a given text input. We evaluate the proposed the MotionGPT model as the\npre-trained MotionGPT, the same one in Tab. 2, as well as fine-tuned it on text-to-motion task. We\ncompare our MotionGPTs with other SOTAs [12, 11, 48, 54, 59] and evaluate the performance on\nboth HumanML3D and KIT datasets using suggested metrics [11]. The results are computed with a\n95% confidence interval, obtained from 20 repeated runs. The majority of the reported results are\ntaken directly from their own papers or the benchmark presented in [11]. Tab. 3 summarizes the\ncomparison results, where MotionGPT achieves competitive performance on most metrics.\nComparisons on Motion-to-Text. The motion-to-text task involves generating a text description\nbased on a given human motion sequence. We compare the pre-trained MotionGPT with recent work\nTM2T [12]. We evaluate the performance on the HumanML3D using the suggested metrics from [12].\nAdditionally, we measure the average numbers of words Lengthavg for further comparisons. Please\nnote that the reported results in [12] are evaluated with pre-processed ground truth text, which ignores\nthe grammatical tense and plural forms of words. In Tab. 4, we directly use the ground truth text\ndescriptions for a more accurate assessment. This comparison shows that MotionGPT overperforms\nrecent work on text descriptions of given motions.\nComparisons on Motion Prediction and In-between.\nWe summarize motion prediction and\nin-between together as general motion completion. To evaluate the motion completion capability\nof MotionGPT, we employ part of the AMASS dataset [26], a motion-only dataset. For motion\nprediction task, we only input around the first 20% of the motion sequence as conditions. For\nin-between, we mask about 50% motion randomly for completion. We also fine-tune MotionGPT\nspecifically for this task and employ FID, ADE, and FDE as metrics like Sec. 4.1. Furthermore, we\nevaluate MDM [48] on motion prediction by utilizing their provided model, which also supports\nmotion in-between through masked motion \u201cin-painting\u201d. The real motion data is used as one of our\nbaselines. Tab. 5 reports that our MotionGPT has the best motion completion quality and diversity.\n8\nMethods\nMotion Prediction\nMotion In-between\nFID \u2193\nDiversity\u2191\nADE\u2193\nFDE\u2193\nFID \u2193\nDiversity\u2191\nADE\u2193\nReal\n0.002\n9.503\n-\n-\n0.002\n9.503\n-\nMDM[48]\n6.031\n7.813\n5.446\n8.561\n2.698\n8.420\n3.787\nMotionGPT (Ours)\n0.905\n8.972\n4.745\n6.040\n0.214\n9.560\n3.762\nTable 5: Comparison of motion prediction and motion in-between on part of AMASSS [26] dataset\nusing motion data only. FID indicates motion quality and Diversity (DIV) for motion diversity within\neach condition. ADE and FDE are joints distance between generation and ground truth.\nSize\nInstruction Tuning\nText-to-Motion\nMotion-to-Text\nMotion Prediction\nMotion In-between\nR TOP3 \u2191\nFID \u2193\nDIV \u2192\nMMDist\u2193\nBleu@4\u2191\nCider\u2191\nFID \u2193\nDIV \u2192\nFID \u2193\nDIV \u2192\nReal\n-\n0.797\n0.002\n9.503\n2.901\n-\n-\n0.002\n9.503\n0.002\n9.503\nSmall\n0.706\n0.727\n9.264\n2.748\n12.02\n24.9\n-\n-\n-\n-\nSmall\n\u2713\n0.663\n0.336\n9.239\n2.931\n10.54\n24.3\n0.954\n8.727\n0.326\n9.618\nBase\n0.722\n0.365\n9.407\n2.821\n12.47\n29.2\n-\n-\n-\n-\nBase\n\u2713\n0.700\n0.160\n9.411\n3.019\n11.42\n28.2\n0.905\n8.972\n0.214\n9.560\nLarge\n0.694\n0.234\n9.310\n2.776\n12.44\n28.5\n-\n-\n-\n-\nLarge\n\u2713\n0.708\n0.159\n9.301\n3.011\n11.71\n29.1\n0.556\n8.975\n0.223\n9.358\nTable 6: Evaluation of instruction tuning and different model sizes of MotionGPTs in four motion\ntasks on HumanML3D [11] dataset. (cf. Tab. 2 for metrics details)\n4.3\nAblation Studies\nMotionGPT employs T5 [38] as the motion-aware language backbone model, and we train these\nmodels with pre-training and then instruction tuning. Thus, both model size and training strategy\ninfluence the performance of MotionGPTs. We here evaluate them on the typical motion tasks. More\ndetailed ablation studies are provided in the supplements.\nModel Sizes. We evaluate the performance of models with different sizes across four motion tasks.\nBesides the base 220M MotionGPT in Sec. 4.1, we now evaluate 60M, 220M, and 770M MotionGPTs.\nTab. 6 demonstrates that the 220M base model has achieved remarkable performance compared to\nthe smaller 60M model. However, the larger model size of current Motions does not yield significant\nimprovements and, in few cases, even leads to worse results, as observed in the motion in-between\ntask. We believe this could be caused by the small amount of current motion datasets. HumanML3D\nonly includes 15k motion sequences, much smaller than even billions of language and image data.\nEffectiveness of Instruction Tuning. We evaluate the impact of our instruction tuning strategy\non different model sizes. The results in Tab. 6 demonstrate that instruction tuning enhances the\nversatility of MotionGPT, enabling more motion tasks like motion completion and improving the\nmotion performance of the text-to-motion task. However, for pure text-generation tasks, the model\nperformance is downgraded, likely due to the pair amount of textual descriptions and coupled motions.\n5\nDisscusion\nAs the first trial, to our best knowledge, exploring human motion generation using language models,\nthe proposed MotionGPT still owns limitations as follows. MotionGPT only utilizes motion on\narticulated human bodies, while many other works focus on faces [17, 4], hands [41, 23, 22] and\neven animal [42, 64] motion. Besides, our method is also restricted to multiple humans without\nmodeling human-object, or human-environment interactions [43]. It is interesting to model the human\ninteraction scenarios in a motion-language framework and generate controllable motions [43].\nWe summarize the proposed MotionGPT as a uniform motion-language framework to generate plau-\nsible human motion and natural language descriptions through prompt-based instructions. Compared\nto the compatible motion diffusion methods [54, 48], our MotionGPT produces competitive results\non motion generation, motion captioning, motion prediction, and motion in-between using only one\npre-trained generative model. With the advancement of large language data and models [38, 5],\nMotionGPT is also capable of addressing natural question-to-answer tasks. Extensive experiments on\nvarious human motion-relevant tasks demonstrate the effectiveness and extendibility of MotionGPT.\n9\nReferences\n[1] Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecast-\ning. In 2019 International Conference on 3D Vision (3DV), pages 719\u2013728. IEEE, 2019.\n[2] Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An integrated development\nenvironment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[4] Xuan Cao, Zhang Chen, Anpei Chen, Xin Chen, Shiying Li, and Jingyi Yu. Sparse photometric 3d face\nreconstruction guided by morphable models. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 4635\u20134644, 2018.\n[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\n[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov.\nTransformer-xl: Attentive language models beyond a fixed-length context.\narXiv preprint\narXiv:1901.02860, 2019.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[8] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. Imagebind: One embedding space to bind them all. arXiv preprint arXiv:2305.05665,\n2023.\n[9] Yusuke Goutsu and Tetsunari Inamura. Linguistic descriptions of human motion with generative adversarial\nseq2seq learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages\n4281\u20134287. IEEE, 2021.\n[10] John C Gower. Generalized procrustes analysis. Psychometrika, 40(1):33\u201351, 1975.\n[11] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and\nnatural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5152\u20135161, June 2022.\n[12] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the\nreciprocal generation of 3d human motions and texts. In ECCV, 2022.\n[13] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and\nLi Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedings of the 28th ACM\nInternational Conference on Multimedia, pages 2021\u20132029, 2020.\n[14] Andrey Guzhov, Federico Raue, J\u00f6rn Hees, and Andreas Dengel. Audioclip: Extending clip to image,\ntext and audio. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 976\u2013980. IEEE, 2022.\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840\u20136851, 2020.\n[16] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with\nlanguage models. arXiv preprint arXiv:2302.14045, 2023.\n[17] Tero Karras, Timo Aila, Samuli Laine, Antti Herva, and Jaakko Lehtinen. Audio-driven facial animation\nby joint end-to-end learning of pose and emotion. ACM Transactions on Graphics (TOG), 36(4):1\u201312,\n2017.\n[18] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-form language-based motion synthesis &\nediting. arXiv preprint arXiv:2209.00349, 2022.\n[19] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022.\n10\n[22] Yuwei Li, Minye Wu, Yuyao Zhang, Lan Xu, and Jingyi Yu. Piano: A parametric hand bone model from\nmagnetic resonance imaging. arXiv preprint arXiv:2106.10893, 2021.\n[23] Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang, Nianyi Li, Yuexin Ma, Yuyao Zhang, Lan Xu,\nand Jingyi Yu. Nimble: a non-rigid hand model with bones and muscles. ACM Transactions on Graphics\n(TOG), 41(4):1\u201316, 2022.\n[24] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches\nout, pages 74\u201381, 2004.\n[25] Hengbo Ma, Jiachen Li, Ramtin Hosseini, Masayoshi Tomizuka, and Chiho Choi. Multi-objective diverse\nhuman motion prediction with knowledge distillation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 8161\u20138171, 2022.\n[26] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. Amass:\nArchive of motion capture as surface shapes. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), October 2019.\n[27] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Generating\ntextured meshes from text using pretrained image-text models. In SIGGRAPH Asia 2022 Conference\nPapers, pages 1\u20138, 2022.\n[28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[29] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, pages 311\u2013318, 2002.\n[30] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios\nTzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from a single image. In\nProceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019.\n[31] Mathis Petrovich, Michael J. Black, and G\u00fcl Varol. Action-conditioned 3D human motion synthesis with\ntransformer VAE. In International Conference on Computer Vision (ICCV), 2021.\n[32] Mathis Petrovich, Michael J. Black, and G\u00fcl Varol. TEMOS: Generating diverse human motions from\ntextual descriptions. In European Conference on Computer Vision (ECCV), 2022.\n[33] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big Data,\n4(4):236\u2013252, dec 2016.\n[34] Matthias Plappert, Christian Mandery, and Tamim Asfour. Learning a bidirectional mapping between\nhuman whole-body motion and natural language using deep recurrent neural networks. Robotics and\nAutonomous Systems, 109:13\u201326, 2018.\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR,\n2021.\n[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\nby generative pre-training. 2018.\n[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[39] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.\nAdvances in neural information processing systems, 32, 2019.\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00c3\u00b6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2022.\n[41] Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands: Modeling and capturing hands\nand bodies together. arXiv preprint arXiv:2201.02610, 2022.\n[42] Nadine Rueegg, Silvia Zuffi, Konrad Schindler, and Michael J. Black. BARC: Learning to regress 3D dog\nshape from images by exploiting breed information. In IEEE/CVF Conf. on Computer Vision and Pattern\nRecognition (CVPR), pages 3876\u20133884, June 2022.\n[43] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a generative\nprior. arXiv preprint arXiv:2303.01418, 2023.\n11\n[44] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei\nLiu. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11050\u201311059, 2022.\n[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020.\n[46] Wataru Takano and Yoshihiko Nakamura. Statistical mutual conversion between whole body motion\nprimitives and linguistic sentences for human motions. The International Journal of Robotics Research,\n34(10):1314\u20131328, 2015.\n[47] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing\nhuman motion generation to clip space. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel\nAviv, Israel, October 23\u201327, 2022, Proceedings, Part XXII, pages 358\u2013374. Springer, 2022.\n[48] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano, and Daniel Cohen-Or. Human\nmotion diffusion model. arXiv preprint arXiv:2209.14916, 2022.\n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[50] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30, 2017.\n[51] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n4566\u20134575, 2015.\n[52] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\n[53] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652,\n2021.\n[54] Chen Xin, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your\ncommands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2023.\n[55] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke\nZettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text\nunderstanding. arXiv preprint arXiv:2109.14084, 2021.\n[56] Tatsuro Yamada, Hiroyuki Matsunaga, and Tetsuya Ogata. Paired recurrent autoencoders for bidirectional\ntranslation between robot actions and linguistic descriptions. IEEE Robotics and Automation Letters,\n3(4):3441\u20133448, 2018.\n[57] Kim Youwang, Kim Ji-Yeon, and Tae-Hyun Oh. Clip-actor: Text-driven recommendation and stylization\nfor animating human meshes. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part III, pages 173\u2013191. Springer, 2022.\n[58] Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows for diverse human motion prediction. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX\n16, pages 346\u2013364. Springer, 2020.\n[59] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu,\nand Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[60] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Mo-\ntiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001,\n2022.\n[61] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[62] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675, 2019.\n[63] Yan Zhang, Michael J Black, and Siyu Tang. We are more than our joints: Predicting how 3d bodies\nmove. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n3372\u20133382, 2021.\n12\n[64] Silvia Zuffi, Angjoo Kanazawa, and Michael J. Black. Lions and tigers and bears: Capturing non-rigid, 3D,\narticulated shape from images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 3955\u20133963. IEEE Computer Society, 2018.\n13\nAppendix\nThis appendix provides qualitative comparison results (Sec. A), additional experiments (Sec. B) on\nthe components of MotionGPT models, inference time (Sec. C), statistics on motion vocabulary\n(Sec. D), evaluations on hyperparameters (Sec. E), user study (Sec. F), a protocol for the uniform\nevaluation (Sec. G), and more implementation details (Sec. H) of MotionGPT models . Please note\nevaluations on our training scheme (Sec. B.2), elaborations on the difference of T2M-GPT (Sec. B.4),\nimplementation details of motion completion (Sec. B.5), and more metric definitions (Sec. G).\nVideo. We have provided the supplemental video to illustrate our results. In this video, we show 1)\ncomparisons of text-to-motion, 2) comparisons of motion captioning, and 3) more results on motion\nprediction and other tasks. We suggest watching this video for dynamic motion results.\nCode is available in supplements. We provide example code files, which include the process of the\ntraining and evaluation of our MotionGPT models, as well as several example results. We can hardly\nupload our large model files, but all codes, data, and pre-trained models will be fully released.\nA\nQualitative Results\nT2M-GPT\nOurs\nReal\n\u201ca person walks forward, stops, turns directly \nto their right, then walks forward again.\u201d\n\u201ca person walks forward, then jogs back \nand forth across the floor before returning \nto their original position.\u201d\n\u201ca person walks backwards, then turns \naround then walks backwards again.\u201d\nMDM\nFigure 4: Comparison on text-driven motion generation. The provided state-of-the-art methods\nare under the same training and inference setting on HumanML3D [11]. The red words and boxes\nhighlight the misaligned motions. The results demonstrate that our motion-language per-training\nshows promising text understanding for motion generation.\n14\n\u201c Randomly describe the motion of someone.\u201d\n\u201c A man appears to be doing the waltz dance.\u201d\nText-to-Motion Task\n\u201cA man walks in a s-shape.\u201d\n\u201cA person gets pushed \nto their left.\u201d\n\u201cA person bends over and picks things up with \nboth hands individually, then walks forward.\u201d\n\u201c The persons high hands are dropped down, he claps \nonce and then proceeds by walking up to get seated.\u201d\n\u201cA person has both arms spread out and starts \nwalking in a circle. \u201d\n\u201c The person was holding something \nturned around and sat it down.\u201d\n\u201cA person walks forward then does a \nbackwards z-shape movement to its \nleft side. then back to the right.\u201d\n\u201cA person catches a ball, winds up \nand throws the ball then stops \nstanding sideways.\u201d\nMotion-to-Text Task\nTextual Question-to-Answer Task\n\u201c A person stretches out its left arm then moves it back to its side.\u201d\n\u201c The person walks in a curved path to the left then stops, turns around and follows the same path \nback to the way they came and turning around to their original starting position.\u201d\nMotion Completion\nFigure 5: Gallery for the results of our unified MotionGPT. More samples are from our best model for\ntext-to-motion synthesis, motion captioning, and textual question-to-answer task. The supervision of\nMotionGPT relies on our instruction-based motion-language dataset (cf. Sec. G) based on previous\nmotion datasets [11, 26]. We recommend the dynamic visualization in our supplemental video.\n15\nOurs\nTM2T\n\u201ca person walk forward then turn \naround and walk back\u201d\n\u201ca person takes two steps forward then turns to \ntheir right 180 degrees and takes two steps away.\u201d\n\u201ca person walk in a curved line\u201d\n\u201ca person walks in a semi-circular\npattern, tip-toeing.\u201d\n\u201ca person raise their arm to \ntheir chest multiple time\u201d\n\u201ca person pretends to be a bear.\u201d\nReal\n\u201ca person walks backwards, then turns \naround then walks backwards again.\u201d\n\u201ca man starts to walk straight then \nwalks to the right.\u201d\n\u201c person is acting like a \ngrizzly bear.\u201d\nInput\nMotions\nFigure 6: Comparison of the state-of-the-art method on motion captioning task. All provided methods\nare under the same training and inference setting on HumanML3D [11]. The results demonstrate that\nour text descriptions correspond better to the motion and have correct grammar. The orange words\nindicate the matching results, while the red marks the incorrect grammar.\nB\nAdditional Experiments\nWe conduct several experiments to continue the evaluations of MotionGPT models. We first evaluate\nthe text-to-motion results on KIT dataset (Sec. B.1). Then we evaluate the hyperparameters of motion\ntokenizer V (Sec. B.2). After that, we study the effectiveness of the training scheme (Sec. B.2). We\nalso provide the elaboration on the difference of T2M-GPT (Sec. B.4), implementation details of\nmotion completion (Sec. B.5).\nB.1\nText-to-Motion on KIT dataset.\nFollowing the same procedure on HumanML3D[11] dataset, We train a 220M MotionGPT base\nmodel on the KIT[33] dataset without any pre-training. We evaluate this model under the same\nsettings of [11]. Most results are borrowed from their own paper of the benchmark in [11]. Tab. 7\nshows that MotionGPT achieves comparable performance compared to the previous state-of-the-arts.\nMethods\nRPrecision\u2191\nFID\u2193\nMMDist\u2193\nDiversity\u2192\nMModality\u2191\nTop1\nTop2\nTop3\nReal\n0.424\u00b1.005\n0.649\u00b1.006\n0.779\u00b1.006\n0.031\u00b1.004\n2.788\u00b1.012\n11.08\u00b1.097\n-\nTM2T[12]\n0.280\u00b1.005\n0.463\u00b1.006\n0.587\u00b1.005\n3.599\u00b1.153\n4.591\u00b1.026\n9.473\u00b1.117\n3.292\u00b1.081\nMDM[48]\n0.164\u00b1.004\n0.291\u00b1.004\n0.396\u00b1.004\n0.497\u00b1.021\n9.191\u00b1.022\n10.85\u00b1.109\n1.907\u00b1.214\nMLD[54]\n0.390\u00b1.008\n0.609\u00b1.008\n0.734\u00b1.007\n0.404\u00b1.027\n3.204\u00b1.027\n10.80\u00b1.117\n2.192\u00b1.071\nT2M-GPT [59]\n0.416\u00b1.006\n0.627\u00b1.006\n0.745\u00b1.006\n0.514\u00b1.029\n3.007\u00b1.023\n10.92\u00b1.108\n1.570\u00b1.039\nMotionGPT (Ours)\n0.366\u00b1.005\n0.558\u00b1.004\n0.680\u00b1.005\n0.510\u00b1.016\n3.527\u00b1.021\n10.35\u00b1.084\n2.328\u00b1.117\nTable 7: We involve KIT [33]dataset and evaluate the methods on the text-driven motion generation\ntask. Please refer to Tab. 3 for more details on metrics and notations.\nB.2\nAblation on Motion Tokenizer.\nWe ablate the motion tokenizer V of our MotionGPT models, studying the size K of motion codebooks.\nWe also compare this VQ-VAE with other VAE models in previous works [30, 31, 54], as shown in\nTab. 8. This comparison demonstrates the improvement of VQ-VAE on motion reconstruction. With\nthis ablation studies on the codebook size K, we thus select K = 512 for most experiments.\n16\nMethod\nReconstruction\nMPJPE\u2193\nPAMPJPE\u2193\nACCL\u2193\nFID\u2193\nDIV\u2192\nReal\n-\n-\n-\n0.002\n9.503\nVPoser-t [30]\n75.6\n48.6\n9.3\n1.430\n8.336\nACTOR [31]\n65.3\n41.0\n7.0\n0.341\n9.569\nMLD-1 [54]\n54.4\n41.6\n8.3\n0.247\n9.630\nMotionGPT (Ours)\n55.8\n40.1\n7.5\n0.067\n9.675\nK = 256\n76.4\n51.3\n10.0\n0.187\n9.496\nK = 512\n55.8\n40.1\n7.5\n0.067\n9.675\nK = 1024\n60.3\n44.0\n8.6\n0.086\n9.677\nK = 2048\n78.9\n51.4\n10.5\n0.141\n9.597\nTable 8: Evaluation of our motion tokenizer on the motion part of HumanML3D [11] dataset.\nWe follow MLD [54] to evaluate our VQ-VAE model V: MPJPE and PAMPJPE are measured in\nmillimeter. ACCL indicates acceleration error. We evaluate FID and Diversity the same as Tab. 3.\nThe baselines of VPoser-t [30] and ACTOR [31] are borrowed from MLD. K indicates the codebook\nsize, and K = 512 shows the best performance of motion reconstruction.\nB.3\nEffectiveness of Training Scheme\nMotion-Language Pre-training vs Instructions Tuning. We have provided the illustration of our\ntraining scheme in Fig. 3 and the evaluation in Tab. 6. We further ablate this training scheme on\nthe base MotionGPT model, by evaluating the motion-language pre-training (the second step) and\ninstruction tuning (the third step). As shown in Tab. 9, we train these models with the same 600K\niterations. Compared to other training combinations, the full-stage MotionGPT achieves higher\nperformance on most motion tasks.\nSize\nPre-training\nInstruction Tuning\nText-to-Motion\nMotion-to-Text\nMotion Prediction\nMotion In-between\nR TOP3 \u2191\nFID \u2193\nDIV \u2192\nMMDist\u2193\nBleu@4\u2191\nCider\u2191\nFID \u2193\nDIV \u2192\nFID \u2193\nDIV \u2192\nReal\n-\n-\n0.797\n0.002\n9.503\n2.901\n-\n-\n0.002\n9.503\n0.002\n9.503\nBase\n\"\n%\n0.722\n0.365\n9.407\n2.821\n12.47\n29.2\n-\n-\n-\n-\nBase\n\"\n\"\n0.700\n0.160\n9.411\n3.019\n11.42\n28.2\n0.905\n8.972\n0.214\n9.560\nBase\n%\n\"\n0.607\n0.324\n9.563\n3.374\n10.92\n27.7\n1.643\n8.829\n0.323\n9.628\nTable 9: Evaluation of the training scheme on the base MotionGPT models. We evaluate the results\nwith the proposed evaluation protocols in Sec. G. Please refer to Tab. 2 for metrics and the details.\nInstructions Tuning vs Task-Specific Tuning. While our unified instruction-tuned MotionGPT\nmodel has demonstrated competitive performance across various motion-related tasks, further fine-\ntuning can always enhance its performance on specific tasks. Therefore, we focus on the text-to-\nmotion task and motion in-between task as illustrative examples to showcase the performance of\nthe model before and after fine-tuning. By comparing the results in Tab. 10, we can assess the\neffectiveness of fine-tuning in improving task-specific performance.\nInsturct tuned\nFine tuned\nText-to-Motion\nMotion In-between\nR TOP1\u2191\nFID\u2193\nDIV\u2192\nFID \u2193\nDIV\u2191\nADE\u2193\n\"\n%\n0.435\n0.160\n9.411\n0.214\n9.560\n3.762\n\"\n\"\n0.492\n0.232\n9.528\n0.209\n9.378\n3.281\nTable 10: Evaluation of new task tuning of different size models on HumanML3D [11] dataset.\n17\nB.4\nDifference of T2M-GPT\nWe introduce the difference between T2M-GPT [59] to show our unified framework. T2M-GPT\ninvestigates a generative framework based on VQ-VAE and Transformer for motion generation only.\nThey incorporate language information by leveraging CLIP [35] to extract text embedding as motion\ngeneration conditions, which is similar to most previous work, such as MDM [48], MLD [54], and\nMotionDiffuse [60]. However, our MotionGPTs are based on the pre-trained language model so\nit naturally leverages the strong language generation and zero-shot transfer abilities of pre-trained\nlanguage models. Benefiting from the motion-language vocabulary, MotionGPT thus generates both\nhuman language and human motion in a unified model.\nB.5\nImplementation details of Motion Completion\nPlease note that MDM[48] accomplish motion in-between task in their paper through masked motion\n\u201cin-painting\u201d which fix the first and last 25% of the motion, leaving the model to generate the\nremaining 50% in the middle. To achieve the motion prediction task with MDM, we fix the first\n20% of the motion and then generate the remaining. All our results are computed by utilizing their\nprovided pre-trained model. To compare with MDM in Tab. 5 on both motion in-between and motion\nprediction tasks, we evaluate our MotionGPT with the same setting during the inference.\nC\nInference Time\nWe provide a detailed study on inference time with our different model sizes below. Due to our\nauto-regressive model for motion generation, we use Frames Per Second (FPS) to evaluate our time\ncosts. All the time costs are evaluated on 8 Tesla V100 using one batch size. Tab. 11 shows that any\nsize of our MotionGPTs can support real-time human animations and come up to hundreds of FPS.\nModels\nBackbone\nParameters\nFPS \u2191\nMotionGPT\nSmall\n60 M\n421.31\nMotionGPT\nBase\n220 M\n222.69\nMotionGPT\nLarge\n770 M\n119.75\nTable 11: Evaluation of inference time costs on text-driven motion generation. We evaluate the\nFrames Per Second (FPS) by averaging our generated frames for each second. We show the time\ncosts on different model sizes. Under the same 1 Tesla V100, the smaller model size gets the faster\nFPS. All models can support real-time motion animation applications.\nD\nStatistics on Motion Vocabulary\nWe visualize the usage of each \"word\" in our motion vocabulary Vm item generated by our motion\ntokenizer V. We sample all motions from the whole test set of HumanML3D dataset [11] and count\neach \"word\". In Fig. 7, it shows the utilization of our motion codebook, which seems to be a concise\nbut informative motion vocabulary.\n0\n50\n100\n150\n200\n250\nMotion Vocabulary Index\n0\n1000\n2000\n3000\n4000\n5000\nFrequency\n0\n100\n200\n300\n400\n500\nMotion Vocabulary Index\n0\n500\n1000\n1500\n2000\n2500\nFrequency\n0\n200\n400\n600\n800\n1000\nMotion Vocabulary Index\n0\n200\n400\n600\n800\n1000\nFrequency\nFigure 7: The statistics of each \"word\" in different sizes of motion vocabulary Vm. From left to right,\nthe vocabulary size is K = 256, 512, 1024. (cf. Tab. 8, K = 512 for the best motion quality.)\n18\nE\nEvaluation of Hyperparameters\nWe conduct experiments to investigate the impact of different sampling strategies on the generation\nresults. Specifically, we compare the use of greedy search, which selects the most probable token\nat each step, with sampling from the probability distribution and adopting beam search, which is\nevaluated in previous language models [38]. Beam search expands the search space for improved\nsequence probability matching. The results in Tab. 12 demonstrate that while avoiding sampling and\nusing beam search can slightly improve generation quality, they also significantly reduce the diversity\nof generated motions from the same text description.\nMethod\nSample\n#beams\nR Precision\nFID\u2193\nMM Dist\u2193\nDiversity\u2192\nMModality\u2191\nTop 3\u2191\nReal\n-\n-\n0.797\u00b1.002\n0.002\u00b1.000\n2.974\u00b1.008\n9.503\u00b1.065\n-\nMotionGPT\n-\n0.780\u00b1.002\n0.224\u00b1.009\n3.076\u00b1.009\n9.492\u00b1.056\n-\n2\n0.780\u00b1.002\n0.199\u00b1.008\n3.083\u00b1.007\n9.512\u00b1.063\n-\n3\n0.781\u00b1.002\n0.179\u00b1.008\n3.099\u00b1.009\n9.516\u00b1.064\n-\n4\n0.782\u00b1.002\n0.160\u00b1.007\n3.092\u00b1.010\n9.536\u00b1.060\n-\nMotionGPT\n\u2713\n-\n0.778\u00b1.002\n0.232\u00b1.008\n3.096\u00b1.008\n9.528\u00b1.071\n2.008\u00b1.084\n\u2713\n2\n0.780\u00b1.002\n0.194\u00b1.008\n3.091\u00b1.010\n9.508\u00b1.063\n1.140\u00b1.064\n\u2713\n3\n0.780\u00b1.002\n0.190\u00b1.008\n3.089\u00b1.011\n9.529\u00b1.061\n0.929\u00b1.055\n\u2713\n4\n0.780\u00b1.002\n0.182\u00b1.008\n3.093\u00b1.008\n9.537\u00b1.059\n0.803\u00b1.044\nTable 12: Evaluations on hyperparameters for MotionGPT generations. We study the influence of\ntwo hyperparameters: sample stands for sampling from distribution; #beams means the number of\nbeams for beam search, where empty means no beam search.\nF\nUser Study\nFor the comparisons of text-to-motion task, we use the force-choice paradigm to ask \u201cWhich of\nthe two motions is more realistic?\u201d and \u201cwhich of the two motions corresponds better to the\ntext prompt?\u201d. The provided motions are generated from 30 text descriptions from the test set of\nHumanML3D [11] dataset. For the comparisons of motion-to-text task, we ask 15 users to choose the\nmotion descriptions from GT, TM2T [12], and our MotionGPT. The motions are from the test set of\nHumanML3D [11] dataset. As shown in Fig. 8, in both two tasks, our MotionGPT was preferred\nover the other state-of-the-art methods and even competitive with the ground truth.\n73%\n69%\n65%\n67%\n48%\n49%\n0%\n25%\n50%\n75%\n100%\nWhich of the two motions is more realistic?\nwhich of the two motions corresponds better to the text prompt?\nMotionGPT vs MDM\nMotionGPT vs T2M-GPT\nMotionGPT  vs GT\n46%\n43%\n11%\n0%\n25%\n50%\n75%\n100%\nWhich description can better describe the motion?\nGT\nMotionGPT\nTM2T\nFigure 8: User Study. We investigate our motion quality and the alignment with test descriptions.\nThe left part is the user study for text-to-motion. The right part is for motion captioning.\nG\nEvaluation Protocols on the Uniform Motion-Language Generation.\nWe propose a protocol to evaluate our unified MotionGPT on multiple motion-language generation\ntasks. Upon previous datasets [11, 33, 26], we build an instruction motion-language dataset, which\nis composed of 14 core tasks (Fig. 9) for now. As shown in Tab. 13, each core task has dozens of\ninstruction prompts (Tab. 13). We will release the pre-processed dataset.\n19\nFigure 9: Protocols for multiple motion-language tasks. For each task, we follow Tab. 13 to process\nthe previous datasets [26, 11] into the instruction-based data.\nTask\nInput\nOutput\nText-to-Motion\nGive me a motion that corresponds to [caption].\n[motion]\nDemonstrate a sequence of movements that depict [caption].\nI need a human motion that conveys [caption]. Can you generate it for me?\nText-to-Motion w/ length\nGive me a motion that lasts for approximately [frames] frames. The caption is: [caption].\n[motion]\nPlease create a motion that lasts [seconds] seconds and illustrates [caption].\nLength-to-Motion\nShow me a motion that lasts for no more than [frames] frames.\n[motion]\nCreate a motion that has a duration of [seconds] seconds.\nRadnom Motion\nGive me motions as you like.\n[motion]\nProduce actions that are not prescribed.\nMotion-to-Text\nGive me a summary of the motion being displayed in [motion] using words.\n[caption]\nDescribe the motion illustrated in [motion] in natural language.\nMotion-to-Text w/ length\nDescribe the movement portrayed in [motion]that lasts [frames] frames.\n[caption]\nWhat is happening in [motion] for a length of [seconds] seconds?\nMotion-to-Length\nWhat is the duration of [motion]\u2019s gestures in frames?\nThere are [frames] frames in the motion.\nWhat is the total duration of [motion]\u2019s body movements in seconds?\nThe motion lasts for [seconds] seconds.\nCaption-to-Length\nHow many frames are expected for the motion that matches [caption]?\nThe duration is estimated to be around [frames] frames.\nGiven [caption], provide the anticipated second duration for the corresponding motion.\nThe motion has a length of [seconds] seconds.\nLength-to-Caption\nWhat are some possible physical gestures that could be made in [frames] frames?\n[caption]\nWhat motion could be performed in [seconds] seconds?\nRandom Caption\nDepict a motion as like you have seen it.\n[caption]\nDescribe the motion of someone randomly.\nTable 13: Some examples of prompt templates in our uniform evaluation protocols.\nMetric Definitions: We provide more details of evaluation metrics as follows. Our evaluation metrics\ncan roughly divide to five classes including text-motion matching, generation diversity, linguistic\nquality, motion quality, and time cost. For the first two classes, [54] has already claims clearly and\nfor the linguistic metrics including BLUE [29], Rouge [24], Cider [51], and BertScore [62], you can\nrefer to their own papers for details. Here we focus on the explaination of the rest metrics.\nMotion Quality. FID, MPJPE, PAMPJPE [10], ACCL have been clearly explained in [54]. Thus here\nwe focus on the Average Displacement Error (ADE) and Final Displacement Error (FDE) refaccuracy\nof the predicted motion. Following previous motion prediction work[58, 63, 25], ADE is defined as\naverage L2 distance between the ground truth and predicted motion of the whole sequence and FDE\nis the L2 distance between the ground truth and predicted motion in the last frame.\nTime Costs. To evaluate the computing efficiency of our models, especially the inference efficiency,\nwe calculate average Frames Per Second (FPS) when generating motions. In our case, we calculate\nFPS on the test set of HumanML3D [11], set the batch size to one, and ignore the time cost for model\nand dataset loading parts.\nH\nDetails on MotionGPT Models\nH.1\nImplementation Details\nBesides the MotionGPt with 220M parameters, we implement a smaller model that reduces the model\ndimension with dmodel = 512, dff = 2048 with only 6 layers in encoder and decoder, as well as a\nlarger model with 770 million parameters, which increases the model dimensions with dmodel = 1024,\ndff = 4096, dkv = 64, 24 layers for each transformer. Except for the training iterations during the\ninstruction tuning stage, the other settings are the same. Please refer to Tab. 14 for more details.\n20\nMotionGPT\nSmall\nBase\nLarge\nBackbone\nFlan-T5-Small\nFlan-T5-Base\nFlan-T5-Large\nTraining Batch Size\n64\n16\n4\nModel Size\n60M\n220M\n770M\nPre-training - Iterations\n300K\n300K\n300K\nPre-training - Learning Rate\n2e-4\n2e-4\n2e-4\nInstruction Tuning - Iterations\n200K\n300K\n400K\nInstruction Tuning - Learning Rate\n1e-4\n1e-4\n1e-4\nMotion Vocabulary Number Vm\n512\n512\n512\nMotion Codebook Dimension\n512\n512\n512\nTable 14: Hyperparameters for different MotionGPTs. We train these models on 64 Tesla V100 GPUs.\nThe smaller model size lowers the computational requirements and thus provides faster inference (cf.\nSec. C). According to Tab. 6, the base MotionGPT model is the best one for overall tasks. However,\nwe believe this could be caused by the small amount of current motion datasets. The large model\ncould achieve the best performance when the amount of data comes up to millions or even billions.\n21\n"
  },
  {
    "title": "DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing",
    "link": "https://arxiv.org/pdf/2306.14435.pdf",
    "upvote": "19",
    "text": "DragDiffusion: Harnessing Diffusion Models\nfor Interactive Point-based Image Editing\nYujun Shi1\nChuhui Xue2\nJun Hao Liew2\nJiachun Pan1\nHanshu Yan2\nWenqing Zhang2\nVincent Y. F. Tan1\nSong Bai2\n1National University of Singapore\n2 ByteDance Inc.\nshi.yujun@u.nus.edu\nvtan@nus.edu.sg\nsongbai.site@gmail.com\nUser Edit\nDragGAN\nDragDiffusion\nUser Edit\nDragGAN\nDragDiffusion\nFigure 1. DRAGDIFFUSION greatly improves the applicability of interactive point-based editing. Given an input image, the user clicks\nhandle points (red), target points (blue), and draws a mask specifying the editable region (brighter area). All results are obtained under the\nsame user edit for fair comparisons. Project page: https://yujun-shi.github.io/projects/dragdiffusion.html.\nAbstract\nAccurate and controllable image editing is a challenging\ntask that has attracted significant attention recently. No-\ntably, DRAGGAN developed by Pan et al. (2023) [31] is\nan interactive point-based image editing framework that\nachieves impressive editing results with pixel-level preci-\nsion. However, due to its reliance on generative adversarial\nnetworks (GANs), its generality is limited by the capacity of\npretrained GAN models. In this work, we extend this edit-\ning framework to diffusion models and propose a novel ap-\nproach DRAGDIFFUSION. By harnessing large-scale pre-\ntrained diffusion models, we greatly enhance the applica-\nbility of interactive point-based editing on both real and\ndiffusion-generated images. Unlike other diffusion-based\nediting methods that provide guidance on diffusion latents\nof multiple time steps, our approach achieves efficient yet\naccurate spatial control by optimizing the latent of only one\ntime step.\nThis novel design is motivated by our obser-\nvations that UNet features at a specific time step provides\nsufficient semantic and geometric information to support\nthe drag-based editing. Moreover, we introduce two ad-\nditional techniques, namely identity-preserving fine-tuning\nand reference-latent-control, to further preserve the iden-\ntity of the original image. Lastly, we present a challenging\nbenchmark dataset called DRAGBENCH\u2014the first bench-\nmark to evaluate the performance of interactive point-based\nimage editing methods. Experiments across a wide range of\nchallenging cases (e.g., images with multiple objects, di-\nverse object categories, various styles, etc.) demonstrate\nthe versatility and generality of DRAGDIFFUSION. Code\nand the DRAGBENCH dataset: https://github.com/Yujun-\nShi/DragDiffusion1.\n1. Introduction\nImage editing with generative models [8, 14, 20, 29, 32, 35]\nhas attracted extensive attention recently.\nOne landmark\n1Work done when Yujun interning with Song Bai.\n1\narXiv:2306.14435v5  [cs.CV]  11 Dec 2023\nwork is DRAGGAN [31], which enables interactive point-\nbased image editing, i.e., drag-based editing. Under this\nframework, the user first clicks several pairs of handle and\ntarget points on an image. Then, the model performs seman-\ntically coherent editing on the image that moves the con-\ntents of the handle points to the corresponding target points.\nIn addition, users can draw a mask to specify which region\nof the image is editable while the rest remains unchanged.\nDespite DRAGGAN\u2019s impressive editing results with\npixel-level spatial control, the applicability of this method\nis being limited by the inherent model capacity of genera-\ntive adversarial networks (GANs) [11, 18, 19]. On the other\nhand, although large-scale text-to-image diffusion models\n[36, 40] have demonstrated strong capabilities to synthesize\nhigh quality images across various domains, there are not\nmany diffusion-based editing methods that can achieve pre-\ncise spatial control. This is because most diffusion-based\nmethods [14, 20, 29, 32] conduct editing by controlling the\ntext embeddings, which restricts their applicability to edit-\ning high-level semantic contents or styles.\nTo bridge this gap, we propose DRAGDIFFUSION, the\nfirst interactive point-based image editing method with dif-\nfusion models [16, 36, 40, 43]. Empowered by large-scale\npre-trained diffusion models [36, 40], DRAGDIFFUSION\nachieves accurate spatial control in image editing with sig-\nnificantly better generalizability (see Fig. 1).\nOur approach focuses on optimizing diffusion latents to\nachieve drag-based editing, which is inspired by the fact\nthat diffusion latents can accurately determine the spatial\nlayout of the generated images [27]. In contrast to previ-\nous methods [3, 9, 32, 50], which apply gradient descent\non latents of multiple diffusion steps, our approach focuses\non optimizing the latent of one appropriately selected step\nto conveniently achieve the desired editing results.\nThis\nnovel design is motivated by the empirical observations pre-\nsented in Fig. 2. Specifically, given two frames from a video\nsimulating the original and the \u201cdragged\u201d images, we vi-\nsualize the UNet feature maps of different diffusion steps\nusing principal component analysis (PCA). Via this visu-\nalization, we find that there exists a single diffusion step\n(e.g., t = 35 in this case) such that the UNet feature maps\nat this step alone contains sufficient semantic and geomet-\nric information to support structure-oriented spatial control\nsuch as drag-based editing. Besides optimizing the diffu-\nsion latents, we further introduce two additional techniques\nto enhance the identity preserving during the editing pro-\ncess, namely identity-preserving fine-tuning and reference-\nlatent-control. An overview of our method is given in Fig. 3.\nIt would be ideal to immediately evaluate our method\non well-established benchmark datasets. However, due to\na lack of evaluation benchmarks for interactive point-based\nediting, it is difficult to rigorously study and corroborate\nthe efficacy of our proposed approach. Therefore, to facil-\nFigure 2. PCA visualization of UNet feature maps at different\ndiffusion steps for two video frames. t = 50 implies the full\nDDIM inversion, while t = 0 implies the clean image. Notably,\nUNet features at one specific step (e.g., t = 35) provides sufficient\nsemantic and geometric information (e.g., shape and pose of the\ncat, etc.) for the drag-based editing.\nitate such evaluation, we present DRAGBENCH\u2014the first\nbenchmark dataset for drag-based editing. DRAGBENCH\nis a diverse collection comprising images spanning various\nobject categories, indoor and outdoor scenes, realistic and\naesthetic styles, etc. Each image in our dataset is accompa-\nnied with a set of \u201cdrag\u201d instructions, which consists of one\nor more pairs of handle and target points as well as a mask\nspecifying the editable region.\nThrough extensive qualitative and quantitative experi-\nments on a variety of examples (including those on DRAG-\nBENCH), we demonstrate the versatility and generality of\nour approach. In addition, our empirical findings corrob-\norate the crucial role played by identity-preserving fine-\ntuning and reference-latent-control. Furthermore, a com-\nprehensive ablation study is conducted to meticulously ex-\nplore the influence of key factors, including the number\nof inversion steps of the latent, the number of identity-\npreserving fine-tuning steps, and the UNet feature maps.\nOur contributions are summarized as follows: 1) we\npresent a novel image editing method DRAGDIFFUSION,\nthe first to achieve interactive point-based editing with dif-\nfusion models; 2) we introduce DRAGBENCH, the first\nbenchmark dataset to evaluate interactive point-based image\nediting methods; 3) Comprehensive qualitative and quanti-\ntative evaluation demonstrate the versatility and generality\nof our DRAGDIFFUSION.\n2. Related Work\nGenerative Image Editing.\nGiven the initial successes\nof generative adversarial networks (GANs) in image gen-\neration [11, 18, 19], many previous image editing meth-\nods have been based on the GAN paradigm [2, 8, 13, 23,\n31, 33, 41, 42, 46, 52, 53].\nHowever, due to the lim-\nited model capacity of GANs and the difficulty of inverting\nthe real images into GAN latents [1, 7, 26, 35], the gen-\n2\nFigure 3. Overview of DRAGDIFFUSION. Our approach constitutes three steps: firstly, we conduct identity-preserving fine-tuning on the\nUNet of the diffusion model given the input image. Secondly, according to the user\u2019s dragging instruction, we optimize the latent obtained\nfrom DDIM inversion on the input image. Thirdly, we apply DDIM denoising guided by our reference-latent-control on \u02c6zt to obtain the\nfinal editing result \u02c6z0. Figure best viewed in color.\nerality of these methods would inevitably be constrained.\nRecently, due to the impressive generation results from\nlarge-scale text-to-image diffusion models [36, 40], many\ndiffusion-based image editing methods have been proposed\n[4\u20136, 14, 20, 24, 27, 28, 30, 32, 47]. Most of these methods\naim to edit the images by manipulating the prompts of the\nimage. However, as many editing attempts are difficult to\nconvey through text, the prompt-based paradigm usually al-\nters the image\u2019s high-level semantics or styles, lacking the\ncapability of achieving precise pixel-level spatial control.\n[9] is one of the early efforts in exploring better controlla-\nbility on diffusion models beyond the prompt-based image\nediting. In our work, we aim at enabling a even more ver-\nsatile paradigm than the one studied in [9] with diffusion\nmodels\u2014interactive point-based image editing.\nPoint-based editing.\nTo enable fine-grained editing,\nseveral works have been proposed to perform point-based\nediting, such as [8, 31, 49]. In particular, DRAGGAN has\ndemonstrated impressive dragging-based manipulation with\ntwo simple ingredients: 1) optimization of latent codes to\nmove the handle points towards their target locations and\n2) a point tracking mechanism that keep tracks of the han-\ndle points. However, its generality is constrained due to the\nlimited capacity of GAN. FreeDrag [25] propose to improve\nDRAGGAN by introducing a point-tracking-free paradigm.\nIn this work, we extend the editing framework of DRAG-\nGAN to diffusion models and showcase its generality over\ndifferent domains. There is a work [30] concurrent to ours\nthat also studies drag-based editing with diffusion models.\nDifferently, they rely on classifier guidance to transforms\nthe editing signal into gradients.\nLoRA in Diffusion Models. Low Rank Adaptation (i.e.,\nLoRA) [17] is a general technique to conduct parameter-\nefficient fine-tuning on large and deep networks. During\nLoRA fine-tuning, the original weights of the model are\nfrozen, while trainable rank decomposition matrices are in-\njected into each layer. The core assumption of this strategy\nis that the model weights will primarily be adapted within\na low rank subspace during fine-tuning. While LoRA was\ninitially introduced for adapting language models to down-\nstream tasks, recent efforts have illustrated its effectiveness\nwhen applied in conjunction with diffusion models [12, 39].\nIn this work, inspired by the promising results of using\nLoRA for image generation and editing [20, 38], we also\nimplement our identity-preserving fine-tuning with LoRA.\n3. Methodology\nIn\nthis\nsection,\nwe\nformally\npresent\nthe\nproposed\nDRAGDIFFUSION approach. To commence, we introduce\nthe preliminaries on diffusion models.\nThen, we elabo-\nrate on the three key stages of our approach as depicted\nin Fig. 3: 1) identity-preserving fine-tuning; 2) latent op-\ntimization according to the user-provided dragging instruc-\ntions; 3) denoising the optimized latents guided by our\nreference-latent-control.\n3.1. Preliminaries on Diffusion Models\nDenoising diffusion probabilistic models (DDPM) [16, 43]\nconstitute a family of latent generative models. Concerning\na data distribution q(z), DDPM approximates q(z) as the\nmarginal p\u03b8(z0) of the joint distribution between Z0 and a\ncollection of latent random variables Z1:T . Specifically,\np\u03b8(z0) =\nZ\np\u03b8(z0:T ) dz1:T ,\n(1)\n3\nwhere p\u03b8(zT ) is a standard normal distribution and the tran-\nsition kernels p\u03b8(zt\u22121|zt) of this Markov chain are all Gaus-\nsian conditioned on zt. In our context, Z0 corresponds to\nimage samples given by users, and Zt corresponds to the\nlatent after t steps of the diffusion process.\n[36] proposed the latent diffusion model (LDM), which\nmaps data into a lower-dimensional space via a variational\nauto-encoder (VAE) [22] and models the distribution of the\nlatent embeddings instead.\nBased on the framework of\nLDM, several powerful pretrained diffusion models have\nbeen released publicly, including the Stable Diffusion (SD)\nmodel (https://huggingface.co/stabilityai). In SD, the net-\nwork responsible for modeling p\u03b8(zt\u22121|zt) is implemented\nas a UNet [37] that comprises multiple self-attention and\ncross-attention modules [48]. Applications in this paper are\nimplemented based on the public Stable Diffusion model.\n3.2. Identity-preserving Fine-tuning\nBefore editing a real image, we first conduct identity-\npreserving fine-tuning [17] on the diffusion models\u2019 UNet\n(see panel (1) of Fig. 3). This stage aims to ensure that the\ndiffusion UNet encodes the features of input image more ac-\ncurately (than in the absence of this procedure), thus facili-\ntating the consistency of the identity of the image through-\nout the editing process. This fine-tuning process is imple-\nmented with LoRA [17]. More formally, the objective func-\ntion of the LoRA is\nLft(z, \u2206\u03b8) = E\u03f5,t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8+\u2206\u03b8(\u03b1tz + \u03c3t\u03f5)\u22252\n2\n\u0003\n,\n(2)\nwhere \u03b8 and \u2206\u03b8 represent the UNet and LoRA parameters\nrespectively, z is the real image, \u03f5 \u223c N(0, I) is the ran-\ndomly sampled noise map, \u03f5\u03b8+\u2206\u03b8(\u00b7) is the noise map pre-\ndicted by the LoRA-integrated UNet, and \u03b1t and \u03c3t are pa-\nrameters of the diffusion noise schedule at diffusion step t.\nThe fine-tuning objective is optimized via gradient descent\non \u2206\u03b8.\nRemarkably, we find that fine-tuning LoRA for merely\n80 steps proves sufficient for our approach, which is in\nstark contrast to the 1000 steps required by tasks such as\nsubject-driven image generation [12, 38]. This ensures that\nour identity-preserving fine-tuning process is extremely ef-\nficient, and only takes around 25 seconds to complete on an\nA100 GPU. We posit this efficiency is because our approach\noperates on the inverted noisy latent, which inherently pre-\nserve some information about the input real image. Conse-\nquently, our approach does not require lengthy fine-tuning\nto preserve the identity of the original image.\n3.3. Diffusion Latent Optimization\nAfter identity-preserving fine-tuning, we optimize the diffu-\nsion latent according to the user instruction (i.e., the handle\nand target points, and optionally a mask specifying the ed-\nitable region) to achieve the desired interactive point-based\nediting (see panel (2) of Fig. 3).\nTo commence, we first apply a DDIM inversion [44] on\nthe given real image to obtain a diffusion latent at a cer-\ntain step t (i.e., zt). This diffusion latent serves as the ini-\ntial value for our latent optimization process. Then, follow-\ning along the similar spirit of [31], the latent optimization\nprocess consists of two steps to be implemented consec-\nutively. These two steps, namely motion supervision and\npoint tracking, are executed repeatedly until either all han-\ndle points have moved to the targets or the maximum num-\nber of iterations has been reached. Next, we describe these\ntwo steps in detail.\nMotion Supervision: We denote the n handle points at\nthe k-th motion supervision iteration as {hk\ni = (xk\ni , yk\ni ) :\ni = 1, . . . , n} and their corresponding target points as\n{gi = (\u02dcxi, \u02dcyi) : i = 1, . . . , n}. The input image is denoted\nas z0; the t-th step latent (i.e., result of t-th step DDIM in-\nversion) is denoted as zt. We denote the UNet output feature\nmaps used for motion supervision as F(zt), and the feature\nvector at pixel location hk\ni as Fhk\ni (zt). Also, we denote the\nsquare patch centered around hk\ni as \u2126(hk\ni , r1) = {(x, y) :\n|x\u2212xk\ni | \u2264 r1, |y\u2212yk\ni | \u2264 r1}. Then, the motion supervision\nloss at the k-th iteration is defined as:\nLms(\u02c6zk\nt ) =\nn\nX\ni=1\nX\nq\u2208\u2126(hk\ni ,r1)\n\r\rFq+di(\u02c6zk\nt ) \u2212 sg(Fq(\u02c6zk\nt ))\n\r\r\n1\n+ \u03bb\n\r\r(\u02c6zk\nt\u22121 \u2212 sg(\u02c6z0\nt\u22121)) \u2299 (1\u2212M)\n\r\r\n1 ,\n(3)\nwhere \u02c6zk\nt is the t-th step latent after the k-th update, sg(\u00b7) is\nthe stop gradient operator (i.e., the gradient will not be back-\npropagated for the term sg(Fq(\u02c6zk\nt ))), di = (gi \u2212 hk\ni )/\u2225gi \u2212\nhk\ni \u22252 is the normalized vector pointing from hk\ni to gi, M is\nthe binary mask specified by the user, Fq+di(\u02c6zk\nt ) is obtained\nvia bilinear interpolation as the elements of q + di may not\nbe integers. In each iteration, \u02c6zk\nt is updated by taking one\ngradient descent step to minimize Lms:\n\u02c6zk+1\nt\n= \u02c6zk\nt \u2212 \u03b7 \u00b7 \u2202Lms(\u02c6zk\nt )\n\u2202\u02c6zk\nt\n,\n(4)\nwhere \u03b7 is the learning rate for latent optimization.\nNote that for the second term in Eqn. (3), which encour-\nages the unmasked area to remain unchanged, we are work-\ning with the diffusion latent instead of the UNet features.\nSpecifically, given \u02c6zk\nt , we first apply one step of DDIM de-\nnoising to obtain \u02c6zk\nt\u22121, then we regularize the unmasked re-\ngion of \u02c6zk\nt\u22121 to be the same as \u02c6z0\nt\u22121 (i.e., zt\u22121).\nPoint Tracking: Since the motion supervision updates\n\u02c6zk\nt , the positions of the handle points may also change.\nTherefore, we need to perform point tracking to update\nthe handle points after each motion supervision step. To\nachieve this goal, we use UNet feature maps F(\u02c6zk+1\nt\n) and\n4\nF(zt) to track the new handle points. Specifically, we up-\ndate each of the handle points hk\ni with a nearest neigh-\nbor search within the square patch \u2126(hk\ni , r2) = {(x, y) :\n|x \u2212 xk\ni | \u2264 r2, |y \u2212 yk\ni | \u2264 r2} as follows:\nhk+1\ni\n= arg min\nq\u2208\u2126(hk\ni ,r2)\n\r\r\rFq(\u02c6zk+1\nt\n) \u2212 Fh0\ni (zt)\n\r\r\r\n1 .\n(5)\n3.4. Reference-latent-control\nAfter we have completed the optimization of the diffusion\nlatents, we then denoise the optimized latents to obtain the\nfinal editing results. However, we find that na\u00a8\u0131vely applying\nDDIM denoising on the optimized latents still occasionally\nleads to undesired identity shift or degradation in quality\ncomparing to the original image. We posit that this issue\narises due to the absence of adequate guidance from the\noriginal image during the denoising process.\nTo mitigate this problem, we draw inspiration from [6]\nand propose to leverage the property of self-attention mod-\nules to steer the denoising process, thereby boosting coher-\nence between the original image and the editing results. In\nparticular, as illustrated in panel (3) of Fig. 3, given the de-\nnoising process of both the original latent zt and the opti-\nmized latent \u02c6zt, we use the process of zt to guide the process\nof \u02c6zt. More specifically, during the forward propagation\nof the UNet\u2019s self-attention modules in the denoising pro-\ncess, we replace the key and value vectors generated from\n\u02c6zt with the ones generated from zt. With this simple re-\nplacement technique, the query vectors generated from \u02c6zt\nwill be directed to query the correlated contents and texture\nof zt. This leads to the denoising results of \u02c6zt (i.e., \u02c6z0) be-\ning more coherent with the denoising results of zt (i.e., z0).\nIn this way, reference-latent-control substantially improves\nthe consistency between the original and the edited images.\n4. Experiments\n4.1. Implementation Details\nIn all our experiments, unless stated otherwise, we adopt\nthe Stable Diffusion 1.5 [36] as our diffusion model. Dur-\ning the identity-preserving fine-tuning, we inject LoRA into\nthe projection matrices of query, key and value in all of the\nattention modules. We set the rank of the LoRA to 16. We\nfine-tune the LoRA using the AdamW [21] optimizer with a\nlearning rate of 5 \u00d7 10\u22124 and a batch size of 4 for 80 steps.\nDuring the latent optimization stage, we schedule 50\nsteps for DDIM and optimize the diffusion latent at the 35-\nth step unless specified otherwise. When editing real im-\nages, we do not apply classifier-free guidance (CFG) [15]\nin both DDIM inversion and DDIM denoising process. This\nis because CFG tends to amplify numerical errors, which is\nnot ideal in performing the DDIM inversion [29]. We use\nthe Adam optimizer with a learning rate of 0.01 to optimize\nthe latent. The maximum optimization step is set to be 80.\nThe hyperparameter r1 in Eqn. 3 and r2 in Eqn. 5 are tuned\nto be 1 and 3, respectively. \u03bb in Eqn. 3 is set to 0.1 by de-\nfault, but the user may increase \u03bb if the unmasked region\nhas changed to be more than what was desired.\nFinally, we apply our reference-latent-control in the up-\nsampling blocks of the diffusion UNet at all denoising steps\nwhen generating the editing results. The execution time for\neach component is detailed in Appendix D.\n4.2. DRAGBENCH and Evaluation Metrics\nSince interactive point-based image editing is a recently in-\ntroduced paradigm, there is an absence of dedicated eval-\nuation benchmarks for this task, making it challenging to\ncomprehensively study the effectiveness of our proposed\napproach. To address the need for systematic evaluation,\nwe introduce DRAGBENCH, the first benchmark dataset tai-\nlored for drag-based editing.\nDRAGBENCH is a diverse\ncompilation encompassing various types of images. Details\nand examples of our dataset are given in Appendix A. Each\nimage within our dataset is accompanied by a set of drag-\nging instructions, comprising one or more pairs of handle\nand target points, along with a mask indicating the editable\nregion. We hope future research on this task can benefit\nfrom DRAGBENCH.\nIn this work, we utilize the following two metrics for\nquantitative evaluation: Image Fidelity (IF) [20] and Mean\nDistance (MD) [31]. IF, the first metric, quantifies the sim-\nilarity between the original and edited images. It is cal-\nculated by subtracting the mean LPIPS [51] over all pairs\nof original and edited images from 1. The second metric\nMD assesses how well the approach moves the semantic\ncontents to the target points. To compute the MD, we first\nemploy DIFT [45] to identify points in the edited images\ncorresponding to the handle points in the original image.\nThese identified points are considered to be the final han-\ndle points post-editing. MD is subsequently computed as\nthe mean Euclidean distance between positions of all tar-\nget points and their corresponding final handle points. MD\nis averaged over all pairs of handle and target points in the\ndataset. An optimal \u201cdrag\u201d approach ideally achieves both\na low MD (indicating effective \u201cdragging\u201d) and a high IF\n(reflecting robust identity preservation).\n4.3. Qualitative Evaluation\nIn this section, we first compare our approach with DRAG-\nGAN on real images. We employ SD-1.5 for our approach\nwhen editing real images. All input images and the user edit\ninstructions are from our DRAGBENCH dataset. Results are\ngiven in Fig. 4. As illustrated in the figure, when dealing\nwith the real images from a variety of domains, DRAGGAN\noften struggles due to GAN models\u2019 limited capacity. On\nthe other hand, our DRAGDIFFUSION can convincingly de-\n5\nFigure 4. Comparisons between DRAGGAN and DRAGDIFFUSION. All results are obtained under the same user edit for fair comparisons.\nFigure 5. Editing results on diffusion-generated images with (a) Stable-Diffusion-1.5, (b) Counterfeit-V2.5, (c) Majicmix Realistic, (d)\nInterior Design Supermix.\n6\nFigure 6. Ablating the number of inversion step t. Effective results are obtained when t \u2208 [30, 40].\nliver reasonable editing results. More importantly, besides\nachieving the similar pose manipulation and local deforma-\ntion as in DRAGGAN [31], our approach even enables more\ntypes of editing such as content filling. An example is given\nin Fig. 4 (a), where we fill the grassland with the pool using\ndrag-based editing. This further validates the enhanced ver-\nsatility of our approach. More qualitative comparisons are\nprovided in Appendix F.\nNext, to show the generality of our approach, we perform\ndrag-based editing on diffusion-generated images across a\nspectrum of variants of SD-1.5, including SD-1.5 itself,\nCounterfeit-V2.5, Majicmix Realistic, Interior Design Su-\npermix. Results are shown in Fig. 5 These results validate\nour approach\u2019s ability to smoothly work with various pre-\ntrained diffusion models. Moreover, these results also il-\nlustrate our approach\u2019s ability to deal with drag instructions\nof different magnitudes (e.g., small magnitude edits such as\nthe left-most image in Fig. 5 (d) and large magnitude edits\nsuch as the left-most image in Fig. 5 (c)). Additional results\nwith more diffusion models and different resolutions can be\nfound in Appendix F.\n4.4. Quantitative Analysis\nIn this section, we conduct a rigorous quantitative evalua-\ntion to assess the performance of our approach. We begin\nby comparing DRAGDIFFUSION with the baseline method\nDRAGGAN. As each StyleGAN [19] model utilized in [31]\nis specifically designed for a particular image class, we em-\nploy an ensemble strategy to evaluate DRAGGAN. This\nstrategy involves assigning a text description to character-\nize the images generated by each StyleGAN model. Before\nediting each image, we compute the CLIP similarity [34]\nbetween the image and each of the text descriptions associ-\nated with the GAN models. The GAN model that yields the\nhighest CLIP similarity is selected for the editing task.\nFurthermore, to validate the effectiveness of each com-\nponent of our approach, we evaluate DRAGDIFFUSION in\nthe following two configurations: one without identity-\npreserving fine-tuning and the other without reference-\nlatent-control.\nWe perform our empirical studies on the\nDRAGBENCH dataset, and Image Fidelity (IF) and Mean\nDistance (MD) of each configuration mentioned above are\nreported in Fig. 8. All results are averaged over the DRAG-\nBENCH dataset. In this figure, the x-axis represents MD\nand the y-axis represents IF, which indicates the method\nwith better results should locate at the upper-left corner of\nthe coordinate plane. The results presented in this figure\nclearly demonstrate that our DRAGDIFFUSION significantly\noutperforms the DRAGGAN baseline in terms of both IF\nand MD. Furthermore, we observe that DRAGDIFFUSION\nwithout identity-preserving fine-tuning experiences a catas-\ntrophic increase in MD, whereas DRAGDIFFUSION with-\nout reference-latent control primarily encounters a decrease\nin IF. Visualization on the effects of identity-preserving\nfine-tuning and reference-latent-control are given in Fig. 9,\nwhich corroborates with our quantitative results.\n4.5. Ablation on the Number of Inversion Step\nNext, we conduct an ablation study to elucidate the impact\nof varying t (i.e., the number of inversion steps) during the\nlatent optimization stage of DRAGDIFFUSION. We set t to\nbe t = 10, 20, 30, 40, 50 steps and run our approach on\nDRAGBENCH to obtain the editing results (t = 50 corre-\nsponds to the pure noisy latent). We evaluate Image Fidelity\n(IF) and Mean Distance (MD) for each t value in Fig. 7(a).\nAll metrics are averaged over the DRAGBENCH dataset.\nIn terms of the IF, we observe a monotonic decrease as t\nincreases. This trend can be attributed to the stronger flexi-\nbility of the diffusion latent as more steps are inverted. As\nfor MD, it initially decreases and then increases with higher\nt values. This behavior highlights the presence of a critical\nrange of t values for effective editing (t \u2208 [30, 40] in our fig-\nure). When t is too small, the diffusion latent lacks the nec-\nessary flexibility for substantial changes, posing challenges\nin performing reasonable edits. Conversely, overly large t\nvalues result in a diffusion latent that is unstable for edit-\ning, leading to difficulties in preserving the original image\u2019s\nidentity. Given these results, we chose t = 35 as our default\nsetting, as it achieves the lowest MD while maintaining a\ndecent IF. Qualitative visualization that corroborates with\nour numerical evaluation is provided in Fig. 6.\n4.6. Ablation Study on the Number of Identity-\npreserving Fine-tuning Steps\nWe run our approach on DRAGBENCH under 0, 20, 40, 60,\n80, and 100 identity-preserving fine-tuning steps, respec-\ntively (0 being no fine-tuning). The outcomes are assessed\nusing IF and MD, and the results are presented in Fig.7 (b).\n7\n(a)\n(b)\n(c)\nFigure 7. Ablation study on (a) the number of inversion step t of the diffusion latent; (b) the number of identity-preserving fine-tuning\nsteps; (c) Block No. of UNet feature maps. Mean Distance (\u2193) and Image Fidelity (\u2191) are reported. Results are produced on DRAGBENCH.\n35\n40\n45\n50\n55\nMean Distance\n0.70\n0.75\n0.80\n0.85\nImage Fidelity (1-LPIPS)\nDragGAN\nDragDiffusion w/o fine-tune\nDragDiffusion w/o ref-latent-control\nDragDiffusion\nFigure 8. Quantitative analysis on DRAGGAN, DRAGDIFFUSION\nand DRAGDIFFUSION\u2019s variants without certain components. Im-\nage Fidelity (\u2191) and Mean Distance (\u2193) are reported.\nResults\nare produced on DRAGBENCH. The approach with better results\nshould locate at the upper-left corner of the coordinate plane.\nFigure 9.\nQualitative validation on effectiveness of identity-\npreserving fine-tuning and reference-latent-control.\nAll results are averaged over the DRAGBENCH dataset.\nInitially, as the number of fine-tuning steps increases,\nMD exhibits a steep downward trend while IF shows an\nupward trend. This reflects that identity-preserving fine-\ntuning can drastically boost both the precision and consis-\ntency of drag-based editing. However, as the fine-tuning\nprogresses, both MD and IF subsequently begins to plateau.\nThis phenomenon shows that lengthy fine-tuning of LoRA\nwould no longer significantly improve the performance of\nour approach.\nConsidering the experimental results, we\nconduct identity-preserving fine-tuning for 80 steps by de-\nfault to balance between effectiveness and efficiency. Vi-\nsualizations that corroborate our quantitative evaluation are\npresented in the Appendix G.\n4.7. Ablation Study on the UNet Feature Maps\nFinally, we study the effect of using different blocks of\nUNet feature maps to supervise our latent optimization.\nWe run our approach on the DRAGBENCH dataset with\nthe feature maps output by 4 different upsampling blocks\nof the UNet Decoder, respectively. The outcomes are as-\nsessed with IF and MD, and are shown in Fig. 7(c). As\ncan be seen, with deeper blocks of UNet features, IF con-\nsistently increases, while MD first decreases and then in-\ncreases. This trend is because feature maps of lower blocks\ncontain coarser semantic information, while higher blocks\ncontain lower level texture information [10, 47]. Hence, the\nfeature maps of lower blocks (e.g., block No. of 1) lack fine-\ngrained information for accurate spatial control, whereas\nthose of higher blocks (e.g., block No. of 4) lack semantic\nand geometric information to drive the drag-based editing.\nOur results indicate that the feature maps produced by the\nthird block of the UNet decoder demonstrate the best per-\nformance, exhibiting the lowest MD and a relatively high\nIF. Visualizations that corroborate our quantitative evalua-\ntion are presented in the Appendix H.\n5. Conclusion and Future Works\nIn this work, we extend interactive point-based editing to\nlarge-scale pretrained diffusion models through the intro-\nduction of a novel method named DRAGDIFFUSION. Fur-\nthermore, we introduce the DRAGBENCH dataset, which\naims to facilitate the evaluation of the interactive point-\nbased editing methods.\nComprehensive qualitative and\nquantitative results showcases the remarkable versatility\nand generality of our proposed method. Limitations of our\n8\napproach are further discussed in Appendix E, and we leave\nmaking the drag-based editing more robust and reliable on\ndiffusion models as our future work.\nReferences\n[1] Rameen Abdal, Yipeng Qin, and Peter Wonka.\nIm-\nage2stylegan: How to embed images into the stylegan latent\nspace? In Proceedings of the IEEE/CVF international con-\nference on computer vision, pages 4432\u20134441, 2019. 2\n[2] Rameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka.\nStyleflow:\nAttribute-conditioned exploration of stylegan-\ngenerated images using conditional continuous normalizing\nflows. ACM Transactions on Graphics (ToG), 40(3):1\u201321,\n2021. 2\n[3] Arpit\nBansal,\nHong-Min\nChu,\nAvi\nSchwarzschild,\nSoumyadip Sengupta,\nMicah Goldblum,\nJonas Geip-\ning, and Tom Goldstein. Universal guidance for diffusion\nmodels.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 843\u2013852,\n2023. 2\n[4] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing.\nIn European Conference on Computer\nVision, pages 707\u2013723. Springer, 2022. 3\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n[6] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 3, 5\n[7] Antonia Creswell and Anil Anthony Bharath. Inverting the\ngenerator of a generative adversarial network. IEEE transac-\ntions on neural networks and learning systems, 30(7):1967\u2013\n1974, 2018. 2\n[8] Yuki Endo. User-controllable latent transformer for style-\ngan image layout editing. arXiv preprint arXiv:2208.12408,\n2022. 1, 2, 3\n[9] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. arXiv preprint arXiv:2306.00986,\n2023. 2, 3\n[10] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arxiv:2307.10373, 2023. 8\n[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems. Curran Associates,\nInc., 2014. 2\n[12] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yun-\npeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning\nChang, Weijia Wu, et al. Mix-of-show: Decentralized low-\nrank adaptation for multi-concept customization of diffusion\nmodels. arXiv preprint arXiv:2305.18292, 2023. 3, 4\n[13] Erik H\u00a8ark\u00a8onen, Aaron Hertzmann, Jaakko Lehtinen, and\nSylvain Paris. Ganspace: Discovering interpretable gan con-\ntrols. Advances in neural information processing systems,\n33:9841\u20139850, 2020. 2\n[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 1, 2, 3\n[15] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 5\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2, 3\n[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations (ICLR),\n2022. 3, 4\n[18] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 2\n[19] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 2, 7\n[20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 1, 2, 3, 5\n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In Proceedings of the International\nConference on Learning Representations (ICLR), 2015. 5\n[22] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 4\n[23] Thomas Leimk\u00a8uhler and George Drettakis.\nFreestylegan:\nFree-view editable portrait rendering with the camera mani-\nfold. arXiv preprint arXiv:2109.09378, 2021. 2\n[24] Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng.\nMagicmix: Semantic mixing with diffusion models. arXiv\npreprint arXiv:2210.16056, 2022. 3\n[25] Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen,\nand Yi Jin.\nFreedrag:\nPoint tracking is not you need\nfor interactive point-based image editing.\narXiv preprint\narXiv:2307.04684, 2023. 3\n[26] Zachary C Lipton and Subarna Tripathi. Precise recovery of\nlatent vectors from generative adversarial networks. arXiv\npreprint arXiv:1702.04782, 2017. 2\n[27] Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa. Guided\nimage synthesis via initial image editing in diffusion model.\narXiv preprint arXiv:2305.03382, 2023. 2, 3\n[28] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 3\n9\n[29] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 1, 2, 5\n[30] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and\nJian Zhang. Dragondiffusion: Enabling drag-style manipula-\ntion on diffusion models. arXiv preprint arXiv:2307.02421,\n2023. 3\n[31] Xingang Pan, Ayush Tewari, Thomas Leimk\u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag your\nGAN: Interactive point-based manipulation on the generative\nimage manifold. arXiv preprint arXiv:2305.10973, 2023. 1,\n2, 3, 4, 5, 7\n[32] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. arXiv preprint arXiv:2302.03027, 2023. 1, 2, 3\n[33] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2085\u20132094,\n2021. 2\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 7\n[35] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages. ACM Transactions on Graphics (TOG), 42(1):1\u201313,\n2022. 1, 2\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 2, 3, 4, 5\n[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 4\n[38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 3, 4\n[39] Simo Ryu.\nLow-rank adaptation for fast text-to-image\ndiffusion\nfine-tuning.\nhttps : / / github . com /\ncloneofsimo/lora, 2022. 3\n[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2, 3\n[41] Yujun Shen and Bolei Zhou. Closed-form factorization of la-\ntent semantics in gans. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n1532\u20131540, 2021. 2\n[42] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. In-\nterpreting the latent space of gans for semantic face editing.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 9243\u20139252, 2020. 2\n[43] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n2, 3\n[44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In Proceedings of the Inter-\nnational Conference on Learning Representations (ICLR),\n2021. 4\n[45] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 5\n[46] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian\nBernard, Hans-Peter Seidel, Patrick P\u00b4erez, Michael Zoll-\nhofer, and Christian Theobalt.\nStylerig:\nRigging style-\ngan for 3d control over portrait images. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6142\u20136151, 2020. 2\n[47] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 3, 8\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[49] Sheng-Yu Wang, David Bau, and Jun-Yan Zhu. Rewriting\ngeometric rules of a gan. ACM Transactions on Graphics\n(TOG), 41(4):1\u201316, 2022. 3\n[50] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and\nJian Zhang. Freedom: Training-free energy-guided condi-\ntional diffusion model.\narXiv preprint arXiv:2303.09833,\n2023. 2\n[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 5\n[52] Jiapeng Zhu, Ceyuan Yang, Yujun Shen, Zifan Shi, Deli\nZhao, and Qifeng Chen.\nLinkgan: Linking gan latents\nto pixels for controllable image synthesis.\narXiv preprint\narXiv:2301.04604, 2023. 2\n[53] Jun-Yan Zhu, Philipp Kr\u00a8ahenb\u00a8uhl, Eli Shechtman, and\nAlexei A Efros. Generative visual manipulation on the natu-\nral image manifold. In Computer Vision\u2013ECCV 2016: 14th\nEuropean Conference, Amsterdam, The Netherlands, Octo-\nber 11-14, 2016, Proceedings, Part V 14, pages 597\u2013613.\nSpringer, 2016. 2\n10\nFigure 10. Examples of our DRAGBENCH dataset. Each image is accompanied by a set of drag-based editing instruction.\nA. Details About DRAGBENCH Dataset\nWe have collected 205 images and provided 349 pairs of handle and target points in total. Images in our DRAGBENCH are\nclassified into the following 10 categories: animals, art works, buildings (city view), buildings (countryside view), human\n(head), human (upper body), human (full body), interior design, landscape, other objects. All human-related images are se-\nlected from Midjourney generation results to avoid potential legal concerns. All the other images are real images downloaded\nfrom unsplash (https://unsplash.com/), pexels (https://www.pexels.com/zh-cn/), and pixabay (https://pixabay.com/). Some\nexamples of our dataset is given in Fig. 10.\nB. Links to the Stable Diffusion\u2019s Finetuned Variants Used by Us\nHere, we provide links to the fine-tuned variants of Stable Diffusion used by us:\nCounterfeit-V2.5 (https://huggingface.co/gsdf/Counterfeit-V2.5),\nMajixmix Realistic (https://huggingface.co/emilianJR/majicMIX realistic),\nRealistic Vision (https://huggingface.co/SG161222/Realistic Vision V2.0),\nInterior Design Supermix (https://huggingface.co/stablediffusionapi/interiordesignsuperm),\nDVarch (https://huggingface.co/stablediffusionapi/dvarch).\nFigure 11. Limitation of DRAGDIFFUSION. Occasionally, some of the handle points cannot precisely reach the desired target.\nC. More Details on Editing Diffusion-Generated Images\nHere we introduce more details about editing diffusion-generated images. Firstly, different from editing real images, we\ndo not need to conduct LoRA fine-tuning before latent optimization. This is because the purpose of LoRA fine-tuning is\nto help better encode the features of the original image into the diffusion UNet. However, for diffusion-generated images,\nthe image features are already well-encoded as the diffusion model itself can generate these images. In addition, during the\nlatent optimization stage, we do not have to perform DDIM inversion as the diffusion latents are readily available from the\ngeneration process of the diffusion models.\nAnother details we need to attend to is the presence of classifier-free guidance (CFG) when editing generated images. As\ndescribed in the main text, when editing real images, we turn off the CFG as it pose challenges to DDIM inversion. However,\nwhen editing generated images, we inevitably have to deal with CFG, as it is one of the key component in diffusion-based\nimage generation. CFG introduces another forward propagation pass of the UNet during the denoising process with a negative\ntext embedding from null prompt or negative prompt. This makes a difference during our latent optimization stage, as now\nwe have two UNet feature maps (one from the forward propagation with positive text embedding and the other one from the\nnegative text embedding) instead of only one. To deal with this, we concatenate these two feature maps along the channel\ndimension and then use the combined feature maps to supervise latent optimization. This simple strategy have been proven\nto be effective as shown in our empirical results.\nD. Execution Time\nGiven a real image with the resolution of 512 \u00d7 512, the execution time of different stages in DRAGDIFFUSION on a A100\nGPU is as follows: LoRA fine-tuning is around 25 seconds, latent optimization is around 10 to 30 seconds depending on\nthe magnitude of the drag-instruction, the final Latent-MasaCtrl guided denoising is negligible comparing to previous steps\n(about 1 to 2 seconds)\nE. Limitations\nAs shown in Fig. 11, the limitation of our DRAGDIFFUSION is that, occasionally, some of the handle points cannot precisely\nreach the desired target. This is potentially due to inaccurate point-tracking or difficulties in latent optimization when multiple\npairs of handle and target points are given. We leave making the drag-based editing on diffusion models more robust and\nreliable as our future work.\nF. More Qualitative Results\nTo start with, we provide more qualitative comparisons between DRAGDIFFUSION and DRAGGAN in Fig. 13. These results\nconsistently showing that our approach demonstrate much better versatility than DRAGGAN.\nNext, we demonstrate results of applying DRAGDIFFUSION on images generated by two more fine-tuned variants of\nStable-Diffusion-1.5, namely Realistic-Vision and DVarch. Results are shown in Fig. 12. These results along with the results\nin the main text corroborate the generality of our approach on different diffusion models.\nFinally, we provide more results on generated images beyond the 512 \u00d7 512 resolution as in the main text. These results\nare shown in Fig. 14, which further demonstrate the versatility of DRAGDIFFUSION.\n12\nFigure 12. Editing results on diffusion-generated images with (a) Realistic Vision; (b) DVarch.\nG. Visual Ablation on the Number of Identity-preserving fine-tuning steps\nIn the main paper, we ablate on the effect of the number of identity-preserving fine-tuning steps (denoted by n in this section).\nWe show through numerical experiments that n \u2265 80 produce ideal results in Fig. 7 (b) of the main text. In this section, we\nprovide visualization that corroborate with conclusions in the main text, showing setting n \u2265 80 produces editing results that\nare free from artifacts such as distorted faces and scenes, unexpected hands, etc. Results are shown in Fig. 15.\nH. Visual Ablation on the UNet Feature Maps\nIn the main paper, we have studied the effect of using UNet feature maps produced by different blocks of UNet decoder for\nour approach. In this section, we provide visualization that corroborates with conclusions in the main text. Results are shown\nin Fig. 16. According to the results, using the 1-st block of feature maps will lead to unfavorable preservation of local details\ndue to lack of fine-grained information. This corresponds to the low Image Fidelity (IF) and high Mean Distance (MD) as in\nmain text Fig. 7 (c) when Block number is 1.\nOn the other hand, as the 4-th block of UNet feature maps only contains low-level information, the editing results is almost\nthe same as the original real image, indicating ineffective editing. This corresponds to the high IF and high MD as in main\ntext Fig. 7 (c) when Block number is 4.\nFinally, using the 2-nd or the 3-rd block of UNet feature maps can can yield reasonable editing. However, if observing\nmore closely, we can see that using the 3-rd block of features yields slightly better preservation of local details (e.g. more\nreasonable headwrap in Fig. 16 (a) and better details of buildings by the river in the Fig. 16 (b)). Correspondingly, in main\ntext Fig. 7 (c), we also show using UNet feature maps output by the 3-rd block can yield better results (lower MD and higher\nIF).\n13\nFigure 13. Additional comparisons between DRAGGAN and DRAGDIFFUSION. All images are from our DRAGBENCH dataset. Both\napproaches are executed under the same drag-based editing instruction. Zoom in to check the details.\n14\nFigure 14. Editing results from DRAGDIFFUSION beyond 512 \u00d7 512 resolution. Results are produced by perform drag-based edits on\nimages generated by Counterfeit-V2.5. The resolution of images in the first row are 768 \u00d7 512, while the images in the second row are\n512 \u00d7 1024.\n15\nUser Edit\nFigure 15. Visual ablation study on the number of identity-preserving fine-tuning steps (denoted as n). Zoom in to view details. From\nthe visualization, we see that setting n < 80 can produce undesired artifacts in the dragging results (e.g., distorted faces and scenes,\nunexpected hands, etc.). On the other hands, n \u2265 80 normally produces reasonable results without artifacts.\nUser Edit\nBlock No.=1\nBlock No.=2\nBlock No.=3\nBlock No.=4\nworse preservation of local details\nineffective editing\nbest results\n(a)\n(b)\nFigure 16. Visual ablation study on the block number of the UNet feature map. Zoom in to view details. As in the figure, using feature\nmaps of the 2-nd and 3-rd blocks produce reasonable results. However, if observing more closely, we can see that using the 3-rd block of\nfeatures yields slightly better preservation of local details (e.g. more reasonable headwrap in (a) and better details of buildings by the river\nin (b)).\n16\nart works landscape city countryside animals head upper body full body interior design other objects\nDRAGGAN\n0.71\n0.84\n0.74\n0.79\n0.72\n0.91\n0.33\n0.31\n0.57\n0.71\nDRAGDIFFUSION\n0.88\n0.88\n0.89\n0.88\n0.87\n0.85\n0.89\n0.95\n0.90\n0.87\nTable 1. Comparisons of Image Fidelity (1-LPIPS) on DRAGBENCH on each category (\nx\uf8e6).\nart works landscape\ncity\ncountryside animals head upper body full body interior design other objects\nDRAGGAN\n59.51\n47.60\n41.94\n46.96\n60.12\n65.14\n82.98\n37.01\n75.65\n58.25\nDRAGDIFFUSION\n30.74\n36.55\n26.18\n43.21\n39.22\n36.43\n39.75\n20.56\n24.83\n39.52\nTable 2. Comparisons of Mean Distance on DRAGBENCH on each category (\n\uf8e6y).\nI. Detailed Comparisons on DRAGBENCH by Category\nIn the main paper Fig. 8, we report the Mean Distance (MD) and Image Fidelity (IF) averaging over all samples in DRAG-\nBENCH. In this section, we provide detailed comparisons between DRAGGAN and DRAGDIFFUSION on each category in\nDRAGBENCH. Comparisons in terms of IF (i.e., 1-LPIPS) and MD are given in Tab. 1 and Tab. 2, respectively. According to\nour results, DRAGDIFFUSION significantly outperforms DRAGGAN in every categories of DRAGBENCH.\n17\n"
  },
  {
    "title": "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications",
    "link": "https://arxiv.org/pdf/2306.14289.pdf",
    "upvote": "14",
    "text": "FASTER SEGMENT ANYTHING: TOWARDS LIGHTWEIGHT SAM\nFOR MOBILE APPLICATIONS\nA PREPRINT\nChaoning Zhang\u2217\nKyung Hee University\nDongshen Han\nKyung Hee University\nYu Qiao\nKyung Hee University\nJung Uk Kim\nKyung Hee University\nSung-Ho Bae\nKyung Hee University\nSeungkyu Lee\nKyung Hee University\nChoong Seon Hong\nKyung Hee University\nJuly 4, 2023\nABSTRACT\nSegment Anything Model (SAM) has attracted significant attention due to its impressive zero-shot\ntransfer performance and high versatility for numerous vision applications (like image editing with\nfine-grained control). Many of such applications need to be run on resource-constraint edge devices,\nlike mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight\nimage encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM\npaper leads to unsatisfactory performance, especially when limited training sources are available. We\nfind that this is mainly caused by the coupled optimization of the image encoder and mask decoder,\nmotivated by which we propose decoupled distillation. Concretely, we distill the knowledge from\nthe heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be\nautomatically compatible with the mask decoder in the original SAM. The training can be completed\non a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM\nwhich is more than 60 times smaller yet performs on par with the original SAM. For inference speed,\nWith a single GPU, MobileSAM runs around 10ms per image: 8ms on the image encoder and 4ms\non the mask decoder. With superior performance, our MobileSAM is around 5 times faster than the\nconcurrent FastSAM and 7 times smaller, making it more suitable for mobile applications. Moreover,\nwe show that MobileSAM can run relatively smoothly on CPU. The code for our project is provided\nat MobileSAM), with a demo showing that MobileSAM can run relatively smoothly on CPU.\n1\nIntroduction\nChatGPT Zhang et al. [2023a] has revolutionized the NLP field, marking a breakthrough in generative AI (AIGC,\na.k.a Artificial intelligence generated content) Zhang et al. [2023b]. What has made this possible lies in GPT-series\nmodels Brown et al. [2020], Radford et al. [2018, 2019], which are foundation models Bommasani et al. [2021] trained\non web-scale text datasets. Following the success of foundation models in NLP, multiple works He et al. [2020],\nQiao et al. [2023a], Zhang et al. [2022a] have learned an image encoder together with a text encoder via contrastive\nlearning He et al. [2020], Zhang et al. [2022b]. Very recently, Meta Research team has released the \"Segment Anything\"\nproject Kirillov et al. [2023], where a prompt-guided vision foundation termed SAM has been proposed and is believed\nto be a GPT moment for vision. SAM consists of two components: ViT-based image encoder and prompt-guided mask\ndecoder, which work in sequence (see Figure 1).\nSince its advent, SAM has attracted significant attention for multiple reasons. First, it is the first to show that vision\ncan follow NLP to pursue a path that combines foundation model with prompt engineering. Second, it is the first to\nperform label-free segmentation, a fundamental vision task that is in parallel to label prediction Zhang et al. [2023c].\nMoreover, this fundamental task makes SAM compatible with other models to realize advanced vision applications,\n\u2217You are welcome to contact the authors through chaoningzhang1990@gmail.com\narXiv:2306.14289v2  [cs.CV]  1 Jul 2023\nMobileSAM -1\nimage \nencoder\n(632M)\nViT-based image encoder \nmask decoder\n(3.87M) \nprompt-guided mask decoder\nprompt encoder\n(0.006M) \nimage\nPrompt\nimage \nembedding\nHeavyweight\nLightweight\nFigure 1: The overview of Segment Anything Model.\nlike text-guided segmentation and image editing with fine-grained control. Many of such use cases, however, need\nto be run on resource-constrained edge-devices, like mobile apps. As shown in the official demo, with a processed\nimage embedding, the SAM can work on resource-constrained devices because the mask decoder is lightweight. What\nmakes the SAM pipeline computation heavy lies in the huge image encoder. In this work, we investigate how to obtain\na lightweight SAM suitable for resource-constrained mobile devices, which is therefore termed MobileSAM.\nTable 1: Parameters SAM with different image encoders.\nParameters\nSAM (ViT-H)\nSAM (ViT-L)\nSAM (ViT-B)\nViT-based encoder\n632M\n307M\n86M\nprompt-guided encoder\n0.006M\n0.006M\n0.006M\nGiven that the default image encoder\nin the SAM is based on ViT-H, a\nstraightforward way to obtain Mobile-\nSAM is to follow the official pipeline\nin Kirillov et al. [2023] to retrain a\nnew SAM with a smaller image en-\ncoder like replacing the ViT-H with a\nsmaller ViT-L or even smaller ViT-B.\nThe parameters of SAM with different variants of image encoder are summarized in Table 1. As stated in Kirillov\net al. [2023], training a new SAM with ViT-L or ViT-B as the image encoder requires 128 GPUs for multiple days.\nSuch resource-intensive retraining can be a non-trivial burden to reproduce or improve their results. This optimization\ndifficulty mainly comes from the coupled optimization of the image encoder and mask decoder. Motivated by this\nunderstanding, we propose to decouple the optimization of the image encoder and mask decoder. Concretely, we\nfirst distill the knowledge from the default image encoder ViT-H to a tiny ViT. After that, we can finetune the mask\ndecoder in the original SAM to better align with the distilled image encoder. It is worth highlighting that the alignment\noptimization is optional because the fact that the lightweight image encoder is distilled from the default image encoder\nguarantees its inherent alignment with the default mask decoder.\nBy turning the problem of seeking a new SAM pipeline into a decoupled distillation, our approach has the advantage of\nbeing simple, and effective, while being reproducible at a low cost (on a single GPU with less than a day). The resulting\nMobileSAM reduces the encoder parameters by 100 times and total parameters by 60 times yet. Surprisingly, such a\nlightweight MobileSAM performs on par with the original heavyweight SAMs, which constitutes a significant step for\npushing SAM for mobile applications. For the inference with MobileSAM, a single image takes runs only around 10ms:\n8ms on the image encoder and 4ms on the mask decoder. It is worth highlighting that our MobileSAM is around 5 times\nfaster and 7 times smaller than the concurrent FastSAM Zhao et al. [2023], while achieving superior performance.\n2\nRelated work\nSAM: generalization and versatility.\nSince its advent in early April of this year, numerous projects and papers have\nemerged to investigate SAM from different perspectives. Given that SAM claims to segment anything, a line of works\nhas reported its performance in real-world situations, including medical images Ma and Wang [2023], Zhang et al.\n[2023d], camouflaged objects Tang et al. [2023], and transparent objects Han et al. [2023]. The findings consistently\nshow that SAM works well in general setups, but not in the above challenging setups. Another significant research\ndirection has focused on enhancing SAM to improve its practicality. Attack-SAM Zhang et al. [2023e] has shown that\nthe output masks of SAM can be easily manipulated by adversarial attacks through maliciously generated adversarial\nperturbations. Another work Qiao et al. [2023b] further conducts a comprehensive robustness evaluation of SAM,\nranging from style transfer and common corruptions to local occlusion and adversarial perturbation. It is found in Qiao\net al. [2023b] SAM has high robustness but not for adversarial perturbation, which aligns well with the finding in Zhang\net al. [2023e]. Another line of work focuses on demonstrating the versatility of SAM. Grounded SAM IDEA-Research\n2\n[2023] is the pioneering work to combine Grounding DINO Liu et al. [2023a] with SAM for segmenting anything with\ntext inputs. Specifically, it relies on Grounding DINO to generate a bounding box from text and then the generated\nbox can be used as a prompt to segment the mask. SAM predicts masks without labels and multiple works Chen\net al. [2023], Park [2023] combine SAM with other models such as CLIP Radford et al. [2021] to semantically\nsegment anything. Beyond object segmentation, multiple works have also shown its versatility in other fields, including\nimage editing Rombach et al. [2022], as well as inpainting tasks Yu et al. [2023], object tracking within videos Yang\net al. [2023], Zxyang [2023]. Beyond 2D vision, the investigation of SAM has also been extended to 3D object\nreconstruction Shen et al. [2023], Kang et al. [2022], demonstrating its capabilities in assisting 3D model generation\nfrom a single image. For a complete survey of SAM, the readers are suggested to refer to Zhang et al. [2023c].\nViT: lightweight and efficient.\nEarly mobile vision applications have been mainly powered by lightweight CNNs,\nsuch as MobileNet Howard et al. [2017] and its improved varinats Sandler et al. [2018], Howard et al. [2019]. The core\nidea of MobileNet lies in separating a normal convolution block into depth-wise convolution and point-wise convolution,\nwhich significantly reduces the mode parameters and computation time. Since the advent of ViT Dosovitskiy et al.\n[2021], numerous works have attempted to make it lightweight and efficient. Complementary to ViT-Huge (ViT-H),\nViT-Large (ViT-L), ViT-Base (ViT-B) in the original ViT paper Dosovitskiy et al. [2021], smaller ViTs are introduced\nin Touvron et al. [2020] and are denoted as Deit-Small (Deit-S) and Deit-Tiny (Deit-T) ViT-Small and ViT-Tiny.\nMobileViT Mehta and Rastegari [2021] is a pioneering work to combine ViT with standard convolutions for improving\nits performance, which outperforms MobileNet v2 Sandler et al. [2018]. The main motivation is to exploit the local\nrepresentation capability of CNN, and this practice is followed by multiple follow-up works which aim to enhance the\nmodel speed, including EfficientFormer Li et al. [2022a], EfficientViT Liu et al. [2023b], Next-ViT Li et al. [2022b]\nand Tiny-ViT Wu et al. [2022]. The recent progress in lightweight and faster ViT is complementary to our proposed\ndecoupled distillation towards making the next-generation SAM suitable for resource-constrained mobile devices.\n3\nMobile-Friendly SAM\n3.1\nBackground and Project Goal\nBackground on SAM.\nHere, we first summarize the structure of SAM and how it works. SAM consists of a ViT-based\nimage encoder and a prompt-guided mask decoder. The image encoder takes the image as the input and generates an\nembedding, which is then fed to the mask decoder. The mask decoder generates a mask to cut out any object from the\nbackground based on a prompt like a point (or box). Moreover, SAM allows generating multiple masks for the same\nprompt for addressing the ambiguity issue, which provides valuable flexibility. Considering this, this work maintains\nthe pipeline of SAM to first adopt a ViT-based encoder to generate image embedding and then to adopt a prompt-guided\ndecoder to generate the desired mask. This pipeline is optimally designed for the \u201csegment anything\", which can be\nused for the downstream task of \u201csegment everything\" (see Sec. 4.3 for more discussion).\nProject goal.\nThe goal of this project is to generate a mobile-friendly SAM (MobileSAM) that achieves satisfactory\nperformance in a lightweight manner and is much faster than the original SAM. The prompt-guided mask decoder\nin the original SAM has less than 4M parameters and is thus considered lightweight. Given an image embedding\nprocessed by the encoder, as shown in their public demo, SAM can work in resource-constrained devices since the\nmask decoder is lightweight. However, the default image encoder in the original SAM is based on ViT-H with more\nthan 600M parameters, which is very heavyweight and makes the whole SAM pipeline incompatible with mobile\ndevices. Therefore, the key to obtaining a mobile-friendly SAM lies in replacing the heavyweight image encoder with\na lightweight one, which also automatically keeps all its functions and characteristics of the original SAM. In the\nfollowing, we elaborate on our proposed method for achieving this project goal.\n3.2\nProposed Method\nCoupled distillation.\nA straightforward way to realize our project goal is to follow the official pipeline in Kirillov\net al. [2023] to retrain a new SAM with a smaller image encoder. As stated in Kirillov et al. [2023], training a SAM with\nViT-H image encoder requires takes 68 hours on 256 A100 GPUs. Replacing the ViT-H with ViT-L or ViT-B reduces\nthe required GPUs to 128, which is still a non-trivial burden for many researchers in the community to reproduce or\nimprove their results. Following their approach, we can further adopt an even smaller image encoder and retrain a new\nSAM with their provided segmentation dataset which is 11-T. Note that the masks in the provided dataset are given by\nthe pretrained SAM (with the ViT image encoder). In essence, this retraining process is knowledge distillation Hinton\net al. [2015], which transfers the knowledge from a ViT-H-based SAM to a SAM with a smaller image encoder (see\nFigure 2 left).\n3\ndistillation\nMobileSAM -2\nViT-based (large)\nimage encoder \nprompt-guided \nmask decoder\nTeacher SAM\nViT-based (small) \nimage encoder \nprompt-guided \nmask decoder\nmask\nmask\ntrainable\ntrainable\nimage\ndistillation\nMobileSAM -3\nViT-based (large)\nimage encoder \nprompt-guided \nmask decoder\nTeacher SAM\nViT-based (small) \nimage encoder \nprompt-guided \nmask decoder\nmask\nmask\ntrainable\nfrozen\ncopy\nimage\nFigure 2: Coupled knowledge distillation of SAM. The left subfigure denotes the fully-coupled distillation, while the\nright one represents the semi-coupled distillation.\nFrom semi-coupled to decoupled distillation.\nWhen performing a KD from the original SAM to that with a smaller\nimage encoder, the difficulty mainly lies in a coupled optimization of the image encoder and combined decoder.\nIntuitively, the optimization of the image encoder depends on the quality of the image decoder, and vice versa. When the\ntwo modules in the SAM are both in a bad state, it is more challenging to train them both to a good state. Inspired by the\ndivide-and-conquer algorithm Zhang et al. [2022c], we propose to divide the KD task into two sub-tasks: image encoder\ndistillation and mask decoder finetuning. Concretely, we first perform the KD on the image encoder by transferring the\nknowledge from ViT-H to a smaller encoder. Since the mask decoder in the original SAM is already lightweight, we\nplan to keep its architecture. This brings a benefit of a readily used combined decoder for finetuning instead of training\nit from scratch. To alleviate the optimization issue of coupled distillation, a straightforward way is to optimize the\nimage encoder with a copied and frozen mask decoder (see Figure 2 right). The freezing operation can help prevent the\nquality of the mask decoder from being deteriorated by a poor image encoder. We call this distillation semi-coupled\nbecause the optimization of the image encoder is still not fully decoupled from the mask decoder. Empirically, we find\nthat this optimization is still challenging because the choice of a prompt is random, which makes the mask decoder\nvariable and thus increases the optimization difficulty. Therefore, we propose to distill the small image encoder directly\nfrom the ViT-H in the original SAM without resorting to the combined decoder, which is termed decoupled distillation\n(see Figure 3). Another advantage of performing distillation on the image embedding is that we can adopt a simple\nMSE loss instead of using a combination of focal loss Lin et al. [2017] and dice loss Milletari et al. [2016] for making\nthe mask prediction as in Kirillov et al. [2023].\ndistillation\nMobileSAM -4\nViT-based (large)\nimage encoder \nprompt-guided \nmask decoder\nViT-based (small)\nimage encoder \nprompt-guided \nmask decoder\nmask\nmask\nimage\nimage \nembedding\nimage \nembedding\ncopy\nFinetuning (optional)\nFigure 3: Decoupled distillation for SAM.\nOn the necessity of mask decoder finetuning.\nUnlike the semi-coupled distillation, the above decoupled distillation\nyields a lightweight image encoder that might not align well with the original frozen mask decoder. Empirically, we\nfind that this is not true because the generated image encoding from the student image encoder can be sufficiently close\nto that of the original teacher encoder, which renders finetuning on the combined decoder in the second stage optional.\n4\nIt is expected that finetuning the mask decoder on the frozen lightweight image encoder or jointly finetuning them\ntogether might further improve the performance.\nPreliminary evaluation.\nHere, we conduct a preliminary investigation to compare coupled distillation and decoupled\ndistillation. Here, for performance evaluation, we compute the mIoU between the two masks generated by the teacher\nSAM and student SAM on the same prompt point. Intuitively, a higher mIoU indicates a higher mask prediction\nperformance by assuming that the mask generated by ViT-H is ground-truth. For the coupled distillation, we adopt the\nSAM with ViT-B provided in the original SAM Kirillov et al. [2023]. It was trained on SA-1B (11M images) on 128\nGPUs (1 sample per GPU) for 180k iterations. By contrast, in our decoupled distillation setup, we train the model on 2\nGPUs (two samples per GPU to save computation resources) on 0.1% samples of SA-1B dataset (11k) images for 55k\niterations. Overall, decoupled distillation takes less than 1% of the computation resources than coupled distillation,\nwhile achieving a superior performance of mIoU of 0.75 vs 0.72 for the coupled sit (averaged on 200 samples). Since\nViT-B is still a non-trivial burden for mobile devices, therefore in the following we experiment with a TinyViT (with\n5M parameters) Wu et al. [2022] based on our proposed decoupled distillation.\nTable 2: Comparison of coupled distillation and decoupled distillation fro SAM with ViT-B as the image encoder.\nDecoupled distillation performs better and requires less than 1% computation resources than coupled distillation.\n/\nSAM (coupled distillation)\nSAM (decoupled distillation)\nMIoU\n0.72\n0.75\nTraining GPUs\n128\n2\nBatch size\n128\n4\nIterations\n180k\n55k\nTraining Data\n11M\n11K\n4\nExperiments\n4.1\nExperimental Setup\nTable 3: Comparison of the parameters and speed for the image encoder\nin original SAM and MobileSAM. The inference speed is measured on a\nsingle GPU.\nOriginal SAM\nMobileSAM\nParameters\n632M\n5.78M\nSpeed\n452ms\n8ms\nLightweight Image Encoder.\nThe goal\nof our project is to obtain an efficient\nSAM by replacing the default ViT-H with\na lightweight image encoder for mobile de-\nvices. As a ViT-based backbone, ViT-Tiny\nhas similar parameters as Deit-Tiny but per-\nforms better. For example, on ImageNet-1K,\nDeit-Yiny achieves an accuracy of 72.2%,\nwhile ViT-Tiny achieves 79.1%. Therefore,\nwe adopt ViT-Tiny for the proof of concept\nto demonstrate the effectiveness of our proposed decoupled distillation for training a lightweight MobileSAM that can\nbe much faster than the original SAM. The adopted lightweight image encoder consists of four stages which gradually\nreduce the resolution. The first stage is constructed by convolution blocks with inverted residuals Sandler et al. [2018],\nwhile the remaining three stages consist of transformer blocks. At the beginning of the model, there are 2 convolutions\nblocks with a stride of 2 for downsampling the resolution. The downsampling operation between different stages is\nprocessed by convolution blocks with the stride of 2. Different from Wu et al. [2022], we set the stride of 2 in the last\ndownsampling convolution to 1 for making the final resolution match that of the ViT-H image encoder of the original\nSAM. The parameters inference speed of MobileSAM are summarized in Table 3. Note that other efficient image\nencoders discussed in Section 2 can also be adopted as the image encoder.\nTraining and evaluation details.\nFor the decoupled KD on the image encoder, we train the lightweight encoder with\n1% of the SA-1B dataset Kirillov et al. [2023] for 8 epochs on a single GPU. We observe that more computation is spent\non the forward process on the teacher image encoder considering that it is significantly more heavy than our adopted\nstudent image encoder (see above). To make the distillation faster, we follow the practice in Wu et al. [2022] to save the\nimage embeddings beforehand so that we only need to run the forward process once. With a single GPU, we can obtain\nour MobileSAM in less than a day. Training our MobileSAM with more GPUs for a longer time is expected to yield\nbetter performance. The initial investigation of performing mask decoder finetuning further improves the performance\n5\nof MobileSAM, however, we omit this step in this version of our paper for simplicity. For quantitative evaluation of the\ndistilled SAM, we compute the mIoU between the masks predicted by the original SAM and our MobileSAM.\nFigure 4: Mask prediction with a single point as the prompt.\n4.2\nMobileSAM performs on par with the orignal SAM\nFor the main results, we report the predicted masks with two types of prompts: point and box. We do not report the\nresults with text prompt because the official github project of SAM does not provide pretrained models for text-guided\nmask decoder. The results with point as the prompt are shown in Figure 4, and those with box as the prompt are shown\nin Figure 5. We observe that MobileSAM makes a satisfactory mask prediction similar to that of the original SAM.\nFigure 5: Mask prediction with a box as the prompt.\n6\nTable 4: Ablation study on the influence of\ntraining computation on the MobileSAM\nperformance.\nbatch size\nepochs\nIterations\nmIoU\n4\n2\n50k\n0.7057\n8\n4\n50k\n0.7286\n8\n8\n100k\n0.7447\nAblation study.\nHere, we conduct an ablation study on the influence\nof the training computation on the performance of SAM. The results\nin Table 4 show that, under the same number of iterations, increasing\nthe batch size increases the model performance. Moreover, under the\nbatch size, the performance also benefits from more update iterations\nby increasing the training epochs. Note that all the experiments are\nconducted on a single GPU. We expect that increasing the number of\nGPUs for allowing a larger batch size or further increasing the iterations\ncan further improve the performance.\n4.3\nMobileSAM outperforms FastSAM\nTable 5: Comparison between segment any-\nthing and segment everything.\nanything\neverything\n# of objects\n1\nN\nprompt-aware\nyes\nno\nSegment anything v.s. segment everything .\nNote that the title of the\noriginal SAM paper Kirillov et al. [2023] is \u201csegment anything\" instead\nof \u201csegment everything\". As highlighted in Kirillov et al. [2023], SAM\nperforms the task of promptable segmentation which \u201creturns a valid\nsegmentation mask given any segmentation prompt\" (quote from Kirillov\net al. [2023]). The role of the prompt is to specify what to segment in the\nimage. In theory, any object can be segmented as long as the prompt is set\nproperly, therefore, it is called \u201csegment anything\". By contrast, \u201csegment\neverything\" is in essence object proposal generation Kirillov et al. [2023],\nfor which the prompt is not necessary. In Kirillov et al. [2023], \u201csegment everything\" (object proposal generation) is\nchosen as one of the downstream tasks for demonstrating its zero-shot transfer performance. To summarize, \u201csegment\nanything\" solves the foundation task of promptable segmentation for any object, while \u201csegment everything\" solves the\ndownstream task of mask proposal generation for all objects. Since \u201csegment everything\" does not necessarily require a\nprompt, FastSAM directly generates the mask proposal with YOLO v8 in a prompt-free manner. To enable promptable\nsegmentation, a mapping algorithm is designed to select the mask from the proposal mask sets. It is worth highlighting\nthat the follow-up works that evaluate its generalization/robustness or investigate its versatility mainly focus on the\nanything instead of everything mode because the former addresses the foundation task. Therefore, the comparison with\nFastSAM mainly focuses on \u201csegment anything\", but we also provide a comparison regarding \u201csegment everything\" for\ncompleteness.\nTable 6: Comparison between FastSAM and Mo-\nbileSAM.\nFastSAM\nMobileSAM\nRatio\nSize\n68M\n9.66M\n\u2248 7\nSpeed\n64ms\n12ms\n\u2248 5\nMobileSAM is faster and smaller.\nFastSAM consists of a\nYOLOv8-based detection branch and a YOLACT-based segmenta-\ntion branch to perform a prompt-free mask proposal generation. It\nhas 68M parameters and takes 40ms to process an image. By con-\ntrast, MobileSAM has less 10M parameters, which is significantly\nsmaller. For the inference speed, on a single GPU, it takes 40ms\nto process an image while ours only takes 10ms, which is 4 times\nfaster than FastSAM (see Table 6).\nTable 7: mIoU comparison. With the assumption\nthat the predicted mask from the original SAM\nis ground-truth, a higher mIoU indicates a better\nperformance.\n100\n200\n300\n400\n500\nFastSAM\n0.27\n0.33\n0.37\n0.41\n0.41\nMobileSAM\n0.73\n0.71\n0.74\n0.73\n0.73\nmIoU comparison under segment anything mode.\nWe further\ncompare the mIoU between the predicted masks with that of the\noriginal SAM. FastSAM is suggested to predict the mask with\nmultiple points, for which we choose one for the foreground and\nthe other for the background. The results in Table 7 show the mIoU\nfor FastSAM is much smaller than that for MobileSAM, suggesting\nthat the mask prediction of FastSAM is very different from that of\nthe original SAM. Moreover, the mIoU for the FastSAM decreases\nvery fast when the distance between the two prompt points. This\nis mainly caused by the fact that FastSAM often fails to predict\nthe object when the foreground prompt point is set too close to the background prompt point.\nResults for segment everything.\nThe results for \u201csegment everything\" are shown in Figure 6. For completeness, we\nalso report the results of the original SAM, which generates a pleasing object proposal. We have two major observations.\nFirst, the results of our MobileSAM align surprisingly well with that of the original SAM. By contrast, the results of\nFastSAM are often less satisfactory. For example, FastSAM often fails to predict some objects, like the roof in the first\nimage. Moreover, the mask proposal is sometimes difficult to interpret (see the mask for the stage in the first image and\nthat for the sky in the second image). Second, FastSAM often generates masks that have non-smooth boundaries, for\n7\nFigure 6: Comparison of segment everything results.\nwhich we suggest the reader zoom in to check the details in Figure 6. For example, the pillars in the third image have\nnon-smooth boundaries, while the original SAM and our MobileSAM do not have this issue.\n5\nConclusion\nIn this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one.\nWe find that the naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance,\nespecially under a setup of limited training sources. The coupled optimization of the image encoder and mask decoder\nis the reason, and thus we propose decoupled distillation, whhere the knowledge is distilled from the image encoder\nViT-H in the original SAM to a lightweight image encoder. We show that the resulting lightweight image encoder\ncan be automatically compatible with the mask decoder in the original SAM. Our MobileSAM is more than 60 times\nsmaller yet performs on par with the original SAM. Moreover, we conduct a comparison with the concurrent FastSAM\nand show that MobileSAM achieve superior performance. Our MobileSAM is also 4 times faster and 7 times smaller\nthan the concurrent FastSAM, making it more suitable for mobile applications. Since our MobileSAM keeps all the\npipeline of the original SAM and just replaces the image encoder, it can be plug-and-play for the existing SAM-based\nprojects to move from a heavyweight SAM to a lightweight one with almost zero effort.\nReferences\nChaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang,\nJung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for agi: A complete\nsurvey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488, 2023a.\nChaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam,\nChu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al. A complete survey on generative ai (aigc): Is chatgpt from\ngpt-4 to gpt-5 all you need? arXiv preprint arXiv:2303.11717, 2023b.\n8\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 2020.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\ngenerative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 2019.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual\nrepresentation learning. In CVPR, 2020.\nYu Qiao, Md Munir, Apurba Adhikary, Huy Q Le, Avi Deb Raha, Chaoning Zhang, Choong Seon Hong, et al. Mp-fedcl:\nMulti-prototype federated contrastive learning for edge intelligence. arXiv preprint arXiv:2304.01950, 2023a.\nChaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, and In So Kweon. How does\nsimsiam avoid collapse without negative samples? a unified understanding with self-supervised contrastive learning.\nIn ICLR, 2022a.\nChaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. Dual temperature helps contrastive\nlearning without many negative samples: Towards understanding and simplifying moco. In CVPR, 2022b.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.\nChaoning Zhang, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin,\nFrancois Rameau, Sung-Ho Bae, et al. A survey on segment anything model (sam): Vision foundation model meets\nprompt engineering. 2023c.\nXu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment\nanything. arXiv preprint arXiv:2306.12156, 2023.\nJun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\nYizhe Zhang, Tao Zhou, Peixian Liang, and Danny Z Chen. Input augmentation with sam: Boosting medical image\nsegmentation with segmentation foundation model. arXiv preprint arXiv:2304.11332, 2023d.\nLv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouflaged object detection. arXiv\npreprint arXiv:2304.04709, 2023.\nDongsheng Han, Chaoning Zhang, Yu Qiao, Maryam Qamar, Yuna Jung, SeungKyu Lee, Sung-Ho Bae, and\nChoong Seon Hong. Segment anything model (sam) meets glass: Mirror and transparent objects cannot be easily\ndetected. arXiv preprint, 2023.\nChenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, and In So Kweon. Attack-sam:\nTowards evaluating adversarial robustness of segment anything model. arXiv preprint, 2023e.\nYu Qiao, Chaoning Zhang, Taegoo Kang, Donghun Kim, Shehbaz Tariq, Chenshuang Zhang, and Choong Seon Hong.\nRobustness of sam: Segment anything under corruptions and beyond. arXiv preprint arXiv:2306.07713, 2023b.\nIDEA-Research.\nGrounded segment anything, 2023.\nURL https://github.com/IDEA-Research/\nGrounded-Segment-Anything. GitHub repository.\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint\narXiv:2303.05499, 2023a.\nJiaqi Chen, Zeyu Yang, and Li Zhang.\nSemantic-segment-anything, 2023.\nURL https://github.com/\nfudan-zvg/Semantic-Segment-Anything. GitHub repository.\nCurt\nPark.\nsegment\nanything\nwith\nclip,\n2023.\nURL\nhttps://github.com/Curt-Park/\nsegment-anything-with-clip. GitHub repository.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\nICML, 2021.\n9\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022.\nTao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment\nanything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023.\nJinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything\nmeets videos. arXiv preprint arXiv:2304.11968, 2023.\nZxyang.\nSegment\nand\ntrack\nanything,\n2023.\nURL\nhttps://github.com/z-x-yang/\nSegment-and-Track-Anything. GitHub repository.\nQiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-3d: Towards single-view anything reconstruction in the\nwild. arXiv preprint arXiv:2304.10261, 2023.\nMinki Kang, Dongchan Min, and Sung Ju Hwang. Any-speaker adaptive text-to-speech synthesis with diffusion models.\narXiv preprint arXiv:2211.09383, 2022.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto,\nand Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017.\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In CVPR, 2018.\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 1314\u20131324, 2019.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image\nis worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training\ndata-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.\nSachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly vision\ntransformer. arXiv preprint arXiv:2110.02178, 2021.\nYanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren.\nEfficientformer: Vision transformers at mobilenet speed. Advances in Neural Information Processing Systems, 35:\n12934\u201312949, 2022a.\nXinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient\nvision transformer with cascaded group attention. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 14420\u201314430, 2023b.\nJiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xuefeng Xiao, Rui Wang, Min Zheng, and Xin Pan. Next-\nvit: Next generation vision transformer for efficient deployment in realistic industrial scenarios. arXiv preprint\narXiv:2207.05501, 2022b.\nKan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast pretraining\ndistillation for small vision transformers. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part XXI, pages 68\u201385. Springer, 2022.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistilling the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015.\nChaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon. Decoupled\nadversarial contrastive learning for self-supervised adversarial robustness. In ECCV, pages 725\u2013742. Springer, 2022c.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In\nProceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.\nFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565\u2013571. Ieee,\n2016.\n10\n"
  },
  {
    "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data",
    "link": "https://arxiv.org/pdf/2306.13840.pdf",
    "upvote": "11",
    "text": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates\nLLMs are Pre-trained on Formally Diverse Data\nAlycia Lee * 1 Brando Miranda * 1 Sudharsan Sundar 1 Sanmi Koyeyo 1\nAbstract\nCurrent trends to pre-train capable Large Lan-\nguage Models (LLMs) mostly focus on scaling of\nmodel and dataset size. However, the quality of\npre-training data is an important factor for training\npowerful LLMs, yet it is a nebulous concept that\nhas not been fully characterized. Therefore, we\nuse the recently proposed Task2Vec diversity co-\nefficient to ground and understand formal aspects\nof data quality, to go beyond scale alone. Specifi-\ncally, we measure the diversity coefficient of pub-\nlicly available pre-training datasets to demonstrate\nthat their formal diversity is high when compared\nto theoretical lower and upper bounds. In addition,\nto build confidence in the diversity coefficient, we\nconduct interpretability experiments and find that\nthe coefficient aligns with intuitive properties of\ndiversity, e.g., it increases as the number of latent\nconcepts increases. We conclude the diversity\ncoefficient is reliable, show it\u2019s high for publicly\navailable LLM datasets, and conjecture it can be\nused to build useful diverse datasets for LLMs.\n1. Introduction\nCurrent trends in pre-training Large Language Models\n(LLMs) tend to concentrate on model and dataset size scal-\ning (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI,\n2023; Google, 2023). Therefore, vast amounts of effort have\nbeen invested in understanding neural scaling laws \u2013 the\npower-law relationship between the loss of deep artificial\nnetworks and the size of the pre-training dataset and model\nfor a fixed compute budget (Hestness et al., 2017; Rosenfeld\net al., 2019; Henighan et al., 2020; Kaplan et al., 2020; Gor-\ndon et al., 2021; Hernandez et al., 2021; Jones, 2021; Zhai\n*Equal contribution\n1Department of Computer Science,\nStanford University, Palo Alto, USA. Correspondence to:\nAlycia\nLee\n<alylee15@stanford.edu>,\nBrando\nMiranda\n<brando9@stanford.edu>.\nWorkshop on Data-centric Machine Learning Research (DMLR) at\nInternational Conference on Machine Learning (ICML), Honolulu,\nHawaii, USA. 2023. Copyright 2023 by the author(s).\net al., 2022; Hoffmann et al., 2022; Clark et al., 2022; Neu-\nmann & Gros, 2022). In addition, recent work focuses on\ntraining a fixed model but using more tokens (Touvron et al.,\n2023). However, the effectiveness of these systems also\nfundamentally relies on the quality (Longpre et al., 2023)\nand coverage of the pre-training data (Hashimoto, 2021;\nDavid et al., 2010) and not only the size. Unfortunately,\ndata quality and coverage (David et al., 2010) are often over-\nlooked or discussed in vague and imprecise ways (Longpre\net al., 2023). Hence, we propose to ground the discussion\nof data quality through the diversity coefficient (Miranda\net al., 2022a), a data coverage metric that moves beyond\nscale alone. We extend the diversity coefficient to formally\nquantify data diversity of publicly available datasets and\ndiscover that LLMs are pre-trained on formally diverse data.\nWe demonstrate the diversity coefficient is high for these\npre-training datasets by comparing their formal diversity\nto the non-vacuous conceptually well-motivated lower and\nupper bounds of the diversity coefficient. In addition, to\ninstill confidence in the usage of the diversity coefficient,\nwe assess the interpretability of the coefficient as it relates to\nintuitive and expected properties of such a diversity metric.\nConcretely, we demonstrate:\n1. The diversity coefficient increases as one concatenates\nmore pre-training datasets of different sources.\n2. We show the task embedding distances used in the di-\nversity coefficient groups in a meaningful way, reflect-\ning the conceptual and semantic information humans\nexpect.\n3. Using the Generative IN-Context Learning (GINC)\n(Xie et al., 2021) dataset, we show that as the number\nof latent concepts1 increases the diversity coefficient\nincreases.\n4. We show that a larger, more diverse vocabulary leads\nto a higher diversity coefficient in the Generative IN-\nContext Learning (GINC) (Xie et al., 2021) dataset.\nOur key contributions are:\n1Latent concepts represent document-level features such as\nsemantics, structure, and style (Xie et al., 2021).\n1\narXiv:2306.13840v2  [cs.CL]  26 Sep 2023\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\n1. A paradigm shift beyond dataset scale to a data-centric\nmachine learning perspective through a formal data\nquality metric \u2013 the diversity coefficient.\n2. We advance discussions on data quality by measur-\ning an aspect of quality \u2013 data diversity \u2013 using the\ndiversity coefficient.\n3. We further validate the diversity coefficient by demon-\nstrating its interpretability and correlation with intuitive\ndiversity properties aligned with human intuitions, e.g.,\nthe coefficient increases as more datasets are concate-\nnated, the number of latent concepts increases, and a\nricher vocabulary is used.\n4. We formally demonstrate the high diversity of pub-\nlic datasets for LLM pre-training is high using well-\nmotivated lower and upper bounds.\n5. Lastly, for ease of use of our method, we also study\nproperties of different parameters for computing the\nformal diversity and therefore provide practitioners\nwith simpler ways to evaluate the diversity coefficient.\nTherefore, we conclude the diversity coefficient is reliable,\nand conjecture the diversity coefficient can be used to build\nquality diverse datasets for capable LLMs. In doing so,\nwe hope this work inspires more systematic and effective\ntechniques for dataset design beyond simply increasing the\nnumber of data points, sequences, or tokens.\n2. Methods\n2.1. Task2Vec Embeddings for Sequence Data\nWe use the Task2Vec diversity coefficient (Miranda et al.,\n2022a) to compute the formal diversity of a dataset. The first\nstep is to compute Task2Vec (vectorial) embeddings of a\nbatch of sequences. The original Task2Vec method (Achille\net al., 2019) embeds data (e.g. few-shot learning task) using\nthe diagonal entries of the Fisher Information Matrix (FIM)\nthat result from (partially) fine-tuning the final layer of a\nfixed neural network (also called a probe network) to solve\nthe current task (or batch). We implement this framework\nby fine-tuning GPT-2 (Radford et al., 2019) to predict the\nnext token for each sequence in the current batch B, then\ncompute the FIM as follows:\n\u02c6FB = Ex,t,\u02c6xt\u2207w log \u02c6pw(\u02c6xt|xt\u22121:1)\u2207w log \u02c6pw(\u02c6xt|xt\u22121:1)\u22a4\nThe Task2Vec embedding \u20d7fB is the diagonal (Diag) of the\nFIM:\n\u20d7fB = Diag(FB)\nwhere x is a sequence of length Tx sampled from a batch B\ni.e. x \u2208 B, \u02c6x is a sequence of tokens sampled from the fine-\ntune probe network fw (with weights w) conditioned on the\nreal sequence x i.e. \u02c6x \u223c \u02c6pw(\u02c6xt | xt\u22121:1), t indicates taking\nthe average across the sequence length when computing the\n(log) loss.\nTo better understand the Task2Vec embedding, observe that\nthe (diagonal) of the FIM can be interpreted as a measure\nof the information that a given parameter contains about the\ngenerative distribution pw(\u02c6xt | xt\u22121:1). Therefore, it serves\nas a unique fingerprint, or feature vector, for a batch, which\ndefines a task distribution. Empirical findings in (Achille\net al., 2019) show that Task2Vec embeddings cluster in a\nway that reflects semantics between different visual concepts\nand that Task2Vec cosine distances are positively correlated\nwith taxonomical distances.\n2.2. Diversity Coefficient Computation for Natural\nLanguage Datasets\n2.3. Diversity Coefficient\nUsing our extension of Task2Vec for sequence data, we\nexplain how to compute the Task2Vec diversity coefficient\n(Miranda et al., 2022a) for natural language datasets using\nGPT-2 as a probe network. We compute the Task2Vec diver-\nsity coefficient as the expected cosine distance d between\npairs of Task2Vec embeddings of batches:\n\u02c6div(D) = EB1,B2\u223cDd(\u20d7fB1, \u20d7fB2)\nwhere D is the natural language dataset from which we\nsample batches B1, B2, and \u20d7fBi is the Task2Vec embedding\nof a batch Bi using the diagonal of the FIM matrix \u02c6FBi.\nIn this setting if D is a union (also known as interleaved)\nof data sets then a batch has sequences from both data set\naccording to some specified data mixture. In this work, we\nuse the term union when the sequences in one batch can\ncome from both data sets.\n2.4. Cross Diversity Coefficient\nThe cross diversity coefficient computes the expected cosine\ndistances of (Tasl2Vec) embeddings of batches by sampling\na batch from the two data sets separately without mixing. In\nother words, the batch will not have sequences from both\ndata sets:\n\u02c6div(D1, D2) = EB1\u223cD1,B2\u223cD2d(\u20d7fB1, \u20d7fB2)\nIn this work, we use the term concatenated when we se-\nquences in each batch come only from a single data set, but\nunion (interleaved) refers when the sequences in a batch\ncome from both data sets.\n2.5. Backbone Used and Further Explanation of the\nDiversity Coefficient\nTo compute Task2Vec embeddings, we use GPT-2 (Radford\net al., 2019) pre-trained on the English language as the\n2\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nprobe network fw. Following Task2Vec, we fine-tune only\nthe final layer (a language modeling head) on each batch\nbecause it\u2019s the current best trusted method for computing\nTask2Vec embeddings (Achille et al., 2019; Miranda et al.,\n2022a; 2023). Figure 5 demonstrates our pipeline.\nBy measuring the distance between FIMs, the diversity coef-\nficient captures the average intrinsic variability of batches in\nthe underlying data distribution as a proxy for data coverage\nor information contained in the dataset. Another interpreta-\ntion is that dataset diversity reflects how different batches\nare from each other. Therefore, a low diversity coefficient\nimplies that batches are not very different.\n2.6. Recipe for Establishing if a Diversity Coefficient is\nHigh via the Conceptual Lower and Upper Bounds\nTo establish if a diversity coefficient \u02c6\ndiv(D) of a dataset D\nis high (or low), we use two conceptually well-motivated\nreference values. We call them the lower and upper bounds\nof the diversity coefficient. There, we explain the concep-\ntually motivated lower and upper bounds of the diversity\ncoefficient. Consider a dataset constructed by sampling with\nmost of the probability mass concentrated on some arbitrary\ntoken. This is a good candidate for a dataset with minimum\ndiversity. On the other extreme, a dataset constructed by\nsampling any token uniformly at random given a fixed vo-\ncabulary (in our case, the GPT-2 tokenizer vocabulary) is a\ngood candidate to create a dataset with maximum diversity.\nTherefore, we measure a conceptual lower bound on a\ndataset with a vocabulary size of 2: <eos> token and a\nrandomly selected non-special token from the GPT-2 tok-\nenizer vocabulary. The <eos> token was assigned a prob-\nability weight of 1/{GPT-2 vocab size}. The non-special\ntoken was assigned the remaining weight. Similarly, a high\nor maximum diversity dataset would consist of random se-\nquences of all possible tokens, with no underlying order\nto semantics, formatting, etc. The upper bound of the di-\nversity coefficient was therefore measured on a synthetic\ndataset with an equal probability of occurrence assigned to\nall tokens in the GPT-2 tokenizer vocabulary.\n2.7. LLM Pre-training Datasets\nSince LLMs are often trained on internal, non-public\ndatasets2, we used publicly available language datasets from\nthe same sources as LLM pre-training data:\nC4, a 305GB cleaned version of Common Crawl\u2019s web\ncrawl corpus in English (Raffel et al., 2019). Sequences in\nC4 were extracted from the web via de-duplication methods\nand heuristics to remove boiler-plate and gibberish.\n2For instance, Gopher was trained on Google\u2019s internal dataset\nMassiveText.\nWikiText-103, a 500MB collection of over 100 million\ntokens extracted from the set of verified Good and Featured\narticles on Wikipedia (Merity et al., 2016).\nThe Pile, a 825 GiB open-source English-text corpus for\nlanguage modeling that combines 22 smaller, high-quality\ndatasets from diverse sources (Gao et al., 2020). These\nsources include Pile-CC (Common Crawl), PubMed Ab-\nstracts, Books3, OpenWebText2, ArXiv, and GitHub.\nFor instance, GPT-3 was trained on a filtered Common\nCrawl dataset and Wikipedia (Brown et al., 2020), which are\nrepresented by C4 and WikiText-103. It was also trained on\nWebText2 and Books, which are sub-datasets of The Pile.\nWe also evaluate the diversity coefficient of the following\nfive sub-datasets of The Pile:\nPile-CC, a 227 GiB preprocessed version of Common\nCrawl\u2019s web crawl corpus (Gao et al., 2020). While both\nPile-CC and C4 are sourced from Common Crawl, Pile-\nCC was preprocessed from Web Archive files, which are\nraw HTTP responses and page HTML, whereas C4 was\npreprocessed from WET files, which consist of plaintext.\nNonetheless, we expect that both datasets are non-mutually-\nexclusive.\nHackerNews, a 4 GiB scraped and parsed dataset of com-\nment trees from Hacker News, a social news website that\naggregates article links (Gao et al., 2020). Articles are\ngenerally focused on topics in computer science and en-\ntrepreneurship.\nNIH ExPorter, a 1.9 GiB dataset of NIH Grant abstracts\nfor awarded applications from 1985-present hosted on the\nExPORTER initiative (Gao et al., 2020).\nPubMed Abstracts, a 19 GiB dataset of abstracts from 30\nmillion publications in PubMed (Gao et al., 2020).\nUSPTO Backgrounds, a 23 GiB dataset of background\nsections from patents granted by the United States Patent\nand Trademark Office (USPTO) (Gao et al., 2020).\n3. Experiments & Results\nIn this section, we describe the experiments and results\nsupporting the contributions outlined in the introduction.\n3.1. Diversity Coefficients of Pre-training Data shows\nLLMs are Pre-trained on Formally Highly Diverse\nData\nExperiments: We evaluate the diversity coefficient (de-\nscribed in section 2) of eight publicly available LLM pre-\ntraining datasets (described in section 2.7). We also com-\npute the diversity coefficient of two concatenated datasets:\n1) C4 and WikiText-103, and 2) five sub-datasets of The\n3\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nTable 1. Diversity coefficients of LLM pre-training datasets\nwith 95% confidence intervals are 2.7-4.76 times higher than\nthe conceptual lower bound and more than half that of the\nupper bound. Batches were sampled such that a sequence from\nany data sets could be present from the union of the data sets when\nthe (standard) diversity coefficient was computed. Mix1 stands for\na data mixture with ratio 1:2 (i.e., 0.75 to 0.25) for the correspond-\ning combined data sets. Mix2 stands for a data mixture according\nto llama v1 (i.e., 0.77, 0.23) for the corresponding combined data\nsets.\nDATASET\nDIVERSITY COEFF.\nLOWER BOUND (LB)\n0.0525 \u00b1 3.41E-4\nNIH EXPORTER\n0.15 \u00b1 3.218E-5\nUSPTO\n0.1582 \u00b1 4.09E-5\nPUBMED ABSTRACTS\n0.168 \u00b1 2.63E-5\nHACKERNEWS\n0.201 \u00b1 4.52E-5\nOPENWEBTEXT\n0.222 \u00b1 1.00E-3\nWIKITEXT-103\n0.2140 \u00b1 7.93E-5\nC4\n0.2374 \u00b1 2.785E-5\nSLIMPAJAMA\n0.221 \u00b1 9.97E-4\nOPENWEBTEXT\n0.222 \u00b1 1.00E-3\nTHE PILE\n0.2463 \u00b1 3.034E-5\nPILE-CC\n0.2497 \u00b1 3.41E-5\nC4 AND WIKITEXT-103 (MIX1)\n0.235 \u00b1 1.04E-3\nUNION OF FIVE DATASETS (MIX2)\n0.217 \u00b1 9.81E-4\nUPPER BOUND (UB)\n0.4037 \u00b1 1.932E-5\nPile: Pile-CC, HackerNews, NIH ExPorter, PubMed, and\nUSPTO (section F.4). In addition, we compute our con-\nceptually well-motivated lower and upper bounds on the\ndiversity coefficient (section 2.6).\nResults: Table 2 reports the measured diversity coefficients\nof eight publicly available LLM pre-training datasets, in\naddition to the conceptually well-motivated lower and upper\nbounds. Table 2 also reports the measured diversity coeffi-\ncients of the concatenation of different publicly available\ndatasets. The key observations from our results are:\n\u2022 The diversity coefficients of pre-training datasets tend to\nbe 3-5 times greater than the theoretical lower bound\nand, on average, half the upper bound. Prominently,\nWikiText-103, C4, The Pile, and Pile-CC exhibit high\ndiversity coefficients (0.21, 0.25).\n\u2022 The measured diversity of Pile-CC is higher than that of\nC4, indicating a potentially more stringent preprocessing\nmethod applied to the Common Crawl corpus for Pile-CC,\nwhich contributes to enhanced data diversity.\n\u2022 Three sub-datasets of The Pile, namely NIH ExPorter,\nPubMed Abstracts, and USPTO, show relatively low di-\nversity (0.15-0.17), approximately half of the upper bound\n(0.4). The nature of these datasets, curated from special-\nized fields, may account for this observation. For instance,\npatent backgrounds in USPTO may share similar format-\nting and semantics as do abstracts in NIH ExPorter or\nPubMed Abstracts.\nTable 2. Cross Diversity coefficients of LLM pre-training\ndatasets with 95% confidence intervals are 3-5 times higher\nthan the conceptual lower bound and more than half that of\nthe upper bound. Batches were sampled from two different data\nsets and the distance across those was used to compute the (cross)\ndiversity coefficient.\nDATASET\nDIVERSITY COEFF.\nLOWER BOUND (LB)\n0.0525 \u00b1 3.41E-4\nNIH EXPORTER\n0.15 \u00b1 3.218E-5\nUSPTO\n0.1582 \u00b1 4.09E-5\nPUBMED ABSTRACTS\n0.168 \u00b1 2.63E-5\nHACKERNEWS\n0.201 \u00b1 4.52E-5\nWIKITEXT-103\n0.2140 \u00b1 7.93E-5\nC4\n0.2374 \u00b1 2.785E-5\nTHE PILE\n0.2463 \u00b1 3.034E-5\nPILE-CC\n0.2497 \u00b1 3.41E-5\nC4 AND WIKITEXT-103\n0.2711 \u00b1 3.22E-4\nCONCATENATION OF FIVE DATASETS\n0.2939 \u00b1 2.03E-4\nUPPER BOUND (UB)\n0.4037 \u00b1 1.932E-5\n\u2022 However, we observe that Pile-CC and HackerNews have\nhigher diversity, which may be attributed to their coverage\nof a broad range of topics. Among these, Pile-CC exhibits\nhigher diversity, in line with its heterogeneous content\ncomposition.\n3.2. Concatenation of Datasets of Different Sources\nProduces Higher Measured Diversity\nExperiments: To show that the concatenation of different\ndatasets produces high diversity datasets, we measure the\ndiversity coefficient of C4 plus WikiText-103, as well as the\ndiversity coefficient of the five sub-datasets of The Pile in\nTable 2. To understand the source of this increased diversity,\nwe plot the Task2Vec (cosine) distances between batches\nfrom individual datasets and distances of batches from the\ndifferent datasets. We report these distances in Figure 1.\nResults: Our key observations are:\n\u2022 The diversity coefficient for the C4 and WikiText-103\nconcatenated dataset is 0.2711, about +0.03-0.05 higher\nthan that of each individual dataset.\n\u2022 The diversity coefficient for the concatenation of the five\nsub-datasets of the Pile is 0.2939 (Table 2), which is about\n+0.04-0.1 (Figure 1) that of each individual dataset.\n\u2022 The concatenation of the five sub-datasets of The Pile\nachieves the highest diversity coefficient in Table 2.\nThis increase in diversity occurs because concatenating\ndatasets produces higher pairwise Task2Vec distances be-\ntween batches from different datasets (see Figure 1). This\nresults in a higher diversity coefficient, since the coefficient\nis an average of all pairwise Task2Vec distances. Note that,\nthis aligns with human intuition that combining data from\nheterogeneous sources increases the overall diversity of the\n4\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nFigure 1. Distribution of pairwise batch distances reflect conceptual and semantic dataset properties, therefore increasing trust in\nthe diversity coefficient. Pairwise task distances from concatenated C4 and WikiText-103 dataset (top) and concatenated five sub-datasets\nof The Pile (bottom) take on a multi-modal form according to dataset comparisons. Pairwise distances are segmented by source datasets\nfor each pair of batches (right), where each sub-distribution corresponds to a mode from the histograms (left). Dotted lines denote the\ndiversity coefficient of the concatenated C4 and WikiText-103 dataset (top) and concatenation of five sub-datasets of The Pile (bottom).\nThese results show that combining batches from two different datasets computes a higher diversity, as expected. Therefore, these results\nalign with human intuition, increasing the confidence in the diversity coefficient as a diversity metric.\ndata.\n3.3. Distribution of Pairwise Batch Distances Reflects\nConceptual and Semantic Dataset Information\nTo increase our confidence in the diversity coefficient as a\ndiversity metric, we study distributions of the Task2Vec (co-\nsine) distances used to compute the coefficient. In particular,\nwe examine the alignment of the grouping of these distances\nwith (human) conceptual and semantic understanding.\nExperiments: Therefore, we analyze Task2Vec (cosine)\ndistances between batches from five sub-datasets of The\nPile. In particular, we compare distances between batches\nof individual sub-datasets and distances across different\nsub-datasets. We show the resulting histograms and violin\nplots in Figure 1. We also segment these distances between\nbatches across C4 and WikiText-103 in Figure 1.\nResults: Our key observations are:\n\u2022 Figure 1 (top, left) shows 3 modes. We confirm that the\nmodes correspond to pairings of datasets in Figure 1 (top,\nright). For instance, the right-most mode, corresponding\nto distances with values higher than the diversity coef-\nficient, consists of pairwise distances between C4 and\nWikiText-103 batches. This confirms intuitive properties\nwe\u2019d expect, i.e. we\u2019d expect 3 modes given 2 datasets\n(C2\n2 + 2 = 3).\n\u2022 Similarly to the preceding point, Figure 1 (bottom, left)\nshows 15 modes, which is exactly the number expected in\nenumerating all possible pairs of batches from 5 datasets.3\nDue to overlaps in distance values we only see 11 modes\nin the Figure 1 (bottom, right).\n3Given a 5 by 5 distance matrix, we\u2019d expect the lower trian-\ngular portion plus the diagonal to be the number of pairings, so\nC5\n2 + 5 = 15.\n5\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\n\u2022 We also observe that the combined datasets have an in-\ncreased diversity coefficient compared to the individual\ndata sets. We outlined this in the previous section, but we\nunderscore it here to emphasize this semantic property.\n\u2022 We expect pairings of unrelated datasets to have higher\ndiversity compared to pairings of related datasets. We\nobserve this in Figure 1 (right). For the concatenated\ndataset of C4 and WikiText-103, the distribution of pair-\nwise distances where one batch is from C4 and one is\nfrom WikiText-103 (right-most violin) is higher than that\nof individual datasets. For the concatenated sub-datasets\nof The Pile, the violin plots for combinations of concep-\ntually unrelated datasets group above the dotted line (e.g.\nHacker News and PubMed), while the violin plots of tech-\nnical subjects written in a similar style4 are below the\ndotted line (e.g. PubMed and USPTO). Note however that\nall combined diversities always increased after a concate-\nnation.\n\u2022 We expect Pile-CC and HackerNews to cover the most\ndiverse topics since they are broad web-scale datasets,\nunlike the remaining which are technical in nature. There-\nfore, we anticipate 1) these two to have the highest indi-\nvidual diversities, as shown in the first two violin plots in\nFigure 1, and 2) to have the highest increase when com-\nbined with other datasets, as shown in the 6th to the 12th\nviolin plots when counting from the left, in Figure 1.\n\u2022 Distances between batches from Pile-CC and Hack-\nerNews (sixth violin from the left) are the lowest among\npairwise distances of concatenated datasets above the di-\nversity coefficient. This aligns with human conceptual\nintuition because the Pile-CC and HackerNews are the\nmost similar in those sub-datasets, since they are both\nweb-scale datasets.\nThese findings build trust in the diversity coefficient as a\ndataset diversity metric, since the coefficient and underlying\nTask2Vec distances of batches behave in interpretable ways\nthat align with human intuition.\n3.4. Diversity Coefficient Captures LLM Pre-training\nData Distributional Properties\nTo instill further confidence in the diversity coefficient,\nwe perform a correlation analysis with data distributional\nproperties on a synthetic language dataset. We use the\nGINC dataset (Xie et al., 2021), which generates sequences\nby modeling how real documents are generated given a\nfixed number of latent document concepts. It achieves this\nthrough a mixture of Hidden Markov Models (HMM) where\neach HMM has a latent concept that models document statis-\ntics, e.g. wiki bio. Further details on GINC can be found in\nsection H.\n4e.g. NIH ExPorter and PubMed Abstracts both contain medi-\ncal abstracts, and have the lowest distances (third violin from the\nright) among combinations of different datasets.\nExperiments: Given that each GINC dataset is a mixture of\nHMMs with a fixed number of latent concepts (1-10,000),\nwe plot how the diversity coefficient varies as the number\nof latent concepts increases for each dataset. We plot this in\nFigure 2 (top) and fit a curve for GINC datasets with fixed\nvocabulary sizes of 50 and 150. Then we fix the number\nof latent concepts at 5 and 5000 and similarly plot how\nincreasing the vocabulary size for the GINC dataset (50-\n10,000 unique tokens) increases the diversity coefficient.\nWe plot this in Figure 2 (bottom) and fit a curve for GINC\ndatasets with 5 latent concepts and 5000 latent concepts.\nResults: Our observations are as follows:\n\u2022 Diversity coefficient increases with greater number\nof latent concepts. Figure 2 (top) shows adding more\nlatent concepts increases the diversity coefficient with di-\nminishing returns. We hypothesize that additional latent\nconcepts introduce new and varied document-level statis-\ntics, resulting in an increase in the diversity coefficient.\nThe R2 is high with values 0.952 and 0.898.\n\u2022 The diversity coefficient saturates as more latent concepts\nare added. We hypothesize this may be due to marginal\nincreases in variation from increased overlap, e.g. wiki\nbios and autobiographical web pages may have syntactical\nand semantic similarities.\n\u2022 Diversity coefficient increases with larger vocabularies.\nFigure 2 (bottom) shows the measured diversity coeffi-\ncient increases at a seemingly exponential pace for larger\nvocab sizes. The R2 is high with values 0.993 and 0.984.\n\u2022 We hypothesize the growth might be exponential because\nscaling the number of tokens produces a more diverse\ndataset by vastly increasing the number of ways to rep-\nresent any sequence. More formally, given a sequence x\nof length Tx and vocab size |V |, the number of ways to\nrepresent that sequence is approximately |V |Tx. There-\nfore, as |V | increases, the growth rate of the exponential\nincreases.\nThese results show the diversity coefficient successfully\ncaptures different distributional sources of variation of the\ndata.\n4. Using the Diversity Coefficient in Practice:\nSetting Batch Size and Network Parameters\nExperiments: We test the sensitivity of the computed di-\nversity coefficient value to changes in batch size and probe\nnetwork parameters in order to gauge how these parameters\nshould be set in practice for natural language datasets.\nWe vary the batch size and observe the impact on the diver-\nsity coefficient. For the same number of batches (200) and\nprobe network (pretrained, fine-tuned GPT-2), we computed\nthe diversity coefficient of C4 for batch sizes of 128, 256,\n512, and 1024, and plot the results in Figure 3 (left).\nWe test the following probe network configurations to mea-\n6\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nFigure 2. Diversity coefficient of GINC datasets with varying\nnumber of latent concepts and vocab sizes shows the diver-\nsity coefficient behaves as expected. The diversity coefficient\nincreases and saturates with an increasing number of latent con-\ncepts (top) and exponentially increases with increasing vocab size\n(bottom). This implies that increases in the measured diversity\ncoefficient correspond to changes in LM pre-training data distribu-\ntional properties that intuitively enable more diverse data.\nsure the diversity coefficient of C4 and of WikiText-103:\n1. Pretrained GPT-2 with fine-tuning, 2. Pretrained GPT-2\nwithout fine-tuning, 3. Randomly initialized GPT-2 with\nfine-tuning, 4. Randomly initialized GPT-2 without fine-\ntuning. Since using a random and/or non fine-tuned network\nis more resource efficient and easily accessible in practice,\nour motivation is to assess the necessity of using pre-trained\nand fine-tuned probe network, which is the original con-\nfiguration used for Task2Vec in (Achille et al., 2019). We\naim to determine if a good approximation of diversity can\nbe computed without fine-tuning. We plot the diversity of\ncoefficients measured using each of the four probe network\nconfigurations in Figure 3 (right).\nResults: We observe that\n\u2022 Diversity coefficient increases with task batch size, but\nwith diminishing returns. Figure 3 (left) shows positive\ncorrelation between the diversity coefficient and batch\nsize. T his may be because larger batch sizes enable\nmore unique tokens per batch, which may result in higher\ndistances between batches.\n\u2022 However, we observe diminishing returns to the increase\nin diversity coefficient with increasing batch size. We\nhypothesize that as the batch size continues to increase,\nthere is greater coverage in tokens, topics, document for-\nmats, etc. between batches, so the increase in the diversity\ncoefficient saturates.\n\u2022 Using a random probe network underestimates diver-\nsity. Since the Task2Vec method (Achille et al., 2019)\nuses a pretrained and fine-tuned network, we consider the\ndiversity computed using this configuration as a source\nof truth. Figure 3 (left) shows that using random probe\nnetworks underestimates diversity compared to pretrained\nnetworks, which is in accordance with results from (Mi-\nranda et al., 2022b) on vision datasets. We hypothesize\nthat for random networks, the probe network parameters\nare not as calibrated to performing autoregressive lan-\nguage modeling, so batch representations from model\nparameters are similar, and the diversity is underestimated\ncompared to pretrained networks.\n\u2022 Using a non fine-tuned network overestimates diver-\nsity. Lightly fine-tuning (the final layer (Achille et al.,\n2019)) of the probe network ensures the final Task2Vec\nembedding is more faithful to the dataset in question, as\nit adjusts the batch/task representation to a more simi-\nlar distribution. This is due to batches \u2013 while different\ncontent-wise \u2013 being conditioned on the same dataset.\nOn the other hand, a non-fine-tuned network may have\nmore variable representations across batches, as it is not\nwell-adapted to the dataset. This may explain the overes-\ntimation of the diversity coefficient that we observe.\n\u2022 Trends in diversity coefficient overestimation vs. under-\nestimation for different probe network configurations are\nconsistent across C4 and WikiText-103.\nBased on these findings, we recommend using a batch size\nof 512 sequences for faster computations and fewer out of\nmemory issues. We conjecture that our proposed diversity\ncoefficient can be computed more efficiently using random\nand non fine-tuned networks, as eliminating pre-training\nand fine-tuning saves computational costs. While the abso-\nlute diversity coefficient values differ compared to values\ncomputed using a pre-trained and fine-tuned network, this\nis not a serious issue as long as the same network configura-\ntion is used consistently (see section I). Further validation is\nrequired to determine if indeed forgoing pre-trained and/or\nfine-tuned probe networks can produce as robust embed-\ndings as the original Task2Vec method.\n5. Related Work\nExisting diversity metrics have concentrated on data pro-\nduced by General Adversarial Networks (GANs) and in-\nvolve variations of a precision- and recall-based framework\noriginally proposed in (Sajjadi et al., 2018) to measure qual-\nity and diversity, respectively (Kynk\u00a8a\u00a8anniemi et al., 2019;\nSimon et al., 2019; Naeem et al., 2020). Similar to the\n7\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nFigure 3. Diversity coefficients of C4 computed using different task batch sizes show positive and diminishing returns with\nincreasing batch size (left). Diversity coefficients of C4 and WikiText-103 computed using different GPT-2 probe network\nconfigurations show that random networks underestimate diversity vs. pretrained networks, and non-finetuned networks\noverestimate diversity vs. finetuned networks (right). 95% confidence intervals for diversity coefficients are plotted, but are so small\nthat they do not show. \u201dpt\u201d refers to pretrained network and \u201drand\u201d refers to randomly initialized network. \u201dft\u201d refers to a network that\nwas finetuned per task and \u201dno ft\u201d refers to no finetuning performed.\nTask2Vec diversity coefficient, these methods utilize em-\nbedding functions, These methods argue that data quality is\nnot synonymous with data diversity in the context of GANs\n(Fowl et al., 2020) and hence take a two-metric approach.\nIn the context of LLMs, we argue that data diversity is a\nsubset of data quality, which is demonstrably important\nto enable capabilities not explicitly trained for such as in-\ncontext learning. Therefore, a diversity metric is sufficient\nto capture an important aspect of data quality. In addition,\na diverse enough dataset increases the coverage and likeli-\nhood that a task in the test dataset is covered. Furthermore,\nlarge LLMs are robust to noise and therefore even if the\ndiversity is made high, the models might still generalize.\nTherefore, we conjecture that high diversity is preferred and\nprovide evidence that current datasets for open LLMs do\nhave that property.\nA recently proposed diversity metric that does not rely on\nan embedding function is the Vendi Score (Friedman & Di-\neng, 2022). The Vendi Score is given by the exponential\nof the Shannon entropy of the eigenvalues of a similarity\nmatrix or kernel. However, the benefits of this more sophis-\nticated aggregation method are not clear, and its computa-\ntion O(n3) is more expensive than the diversity coefficient\nO(n2), as it requires eigenvalue decomposition. Moreover,\nthe Vendi Score assumes the availability of a suitable sim-\nilarity function (or kernel) for the data, and thus does not\nprovide guidance on data representation \u2013 which is arguably\nthe most challenging and important ingredient in machine\nlearning. Furthermore, they suggest that utilizing data rep-\nresentational methods such as embedding networks that re-\nquire pretrained models may be limiting. We argue instead\nthat data representation is a fundamental property of data\nprocessing that has led to the overwhelming success in ma-\nchine learning due to deep learning, e.g. in computer vision\n(Krizhevsky et al., 2012; He et al., 2015), natural language\nprocessing (Devlin et al., 2018; Brown et al., 2020; Chowd-\nhery et al., 2022; OpenAI, 2023; Google, 2023), game play-\ning (Silver et al., 2016; Mnih et al., 2013; Ye et al., 2021),\ntheorem proving (Rabe et al.; Polu & Sutskever, 2020; Han\net al.), code (Chen et al.) and more. Given the success of\ndeep learning data representations and our work, we demon-\nstrate deep learning is a strong way to create dataset/task\nembeddings. In contrast to the Vendi Score, our approach\nlearns effective embeddings of tasks, batches, and datasets\nin an end-to-end manner, whereas the Vendi Score is fo-\ncused on measuring diversity between specific data points.\nSince many canonical datasets already exist and are publicly\navailable (e.g. Common Crawl, Wikipedia), data used to\ntrain new models may be curated from such datasets, ne-\ncessitating a metric that captures overall dataset diversity.\nThese scenarios are thus in favor of using the Task2Vec\ndiversity coefficient. Therefore, our method is more general,\nflexible, and scalable than the Vendi Score. We leave a\ndetailed comparison with the Vendi Score as future work.\n6. Discussion\nOur work extends, examines, and thus validates the applica-\ntion of the Task2Vec diversity coefficient to a new modality\n\u2013 natural language data \u2013 and demonstrates that open LLMs\nare pre-trained on formally diverse data. Our approach has\na number of advantages. Through an extensive set of exper-\niments that verifies intuitive properties of a diversity metric,\nwe instill confidence in the diversity coefficient method, and\ntherefore effectively concretize/ground the concept of data\n8\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\ndiversity. Our conceptually well-motivated lower and upper\nbounds on the diversity coefficient aid in the understand-\ning of the magnitude of the diversity coefficient. However,\nthe bounds we propose only apply to sequence data with\na symbolic vocabulary. Using a multi-modal embedding\nmethod that embeds our proposed lower & upper bounds\nacross modalities would solve this limitation by providing\naligned comparable embedding distances. Another bene-\nfit is that our method does not rely on activations from an\narbitrarily selected layer in a network. Lastly, note that\nactivations may be unreliable for embedding dataset/tasks\nbecause large distances between datasets/tasks may be due\nto well-separated decision boundaries instead of intrinsic\nsemantic properties of the dataset/task. In contrast, the di-\nversity coefficient is well-justified, extensively tested in our\nwork and previous work, e.g. the diversity coefficient cor-\nrelates with ground truth diversities, cluster according to\nsemantics, taxonomy etc. (see section B and (Achille et al.,\n2019; Miranda et al., 2022a)). In short, FIM-based represen-\ntations are motivated by information theory (e.g. FIMs are\nmetrics in distributions) and have been extensively tested by\nindependent sources (Miranda et al., 2022a; Achille et al.,\n2019; Vu et al., 2020).\nOne potential limitation of our method is the need for a\ndata representation. Although the requirement for a data\nrepresentation might seem restrictive, we argue that it is an\ninherent aspect of data processing. Choosing symbols or\nraw pixels (or anything else) is a choice of data representa-\ntion. We suggest deep learning representations due to their\noverwhelming success in machine learning, e.g. in computer\nvision (Krizhevsky et al., 2012; He et al., 2015), natural lan-\nguage processing (Devlin et al., 2018; Brown et al., 2020;\nChowdhery et al., 2022; OpenAI, 2023; Google, 2023),\ngame playing (Silver et al., 2016; Mnih et al., 2013; Ye\net al., 2021), theorem proving (Rabe et al.; Polu & Sutskever,\n2020; Han et al.), code (Chen et al.) and more. In addition,\nwidely available open-source pre-trained models (e.g. CLIP\n(Radford et al., 2021), LLaMA (Touvron et al., 2023), etc.)\nhas made choosing a good embedding method easier. In\naddition, we explore random networks and models with no\nfine-tuning, to make our method more accessible 4. We hy-\npothesize that as long a consistent model/method is used to\ncreate the task embeddings, the exact model/method might\nnot play a crucial role \u2013 because we only need comparable\ndistances that depend on the data/task.\nData has taken a central role in the success of modern ma-\nchine learning methods \u2013 like GPT4 (OpenAI, 2023), CLIP\n(Radford et al., 2021), and PaLM 2 (Google, 2023). This\nseems especially relevant for architectures with few induc-\ntive biases, like the popular Transformer (Vaswani et al.,\n2017). Therefore, it has become paramount to understand\nthe pre-training data we use beyond scale alone. We con-\nclude the diversity coefficient is a reliable trustworthy met-\nric, and conjecture the diversity coefficient can be used to\nbuild quality diverse datasets for capable LLMs. We hope\nour contributions inspire more effective and quantitative\ndata collection and curation processes in machine learning\nthat go beyond scale alone, yet improve performance.\nImportantly, what is the relationship between the formal\ndiversity of the pre-training data set of an LLM and its eval-\nuation test performance? For this, we conduct experiments\nwhere we pre-train three GPT-2 models from scratch using\nthree different data sets with different formal diversities and\nevaluate their performance on formally diverse data sets.\nWe observe in table 3 a negative correlation between di-\nversity and cross-entropy loss, i.e., an increase in diversity\ncorrelates with a decrease in cross-entropy loss \u2013 indicating\na positive relationship between diversity and model per-\nformance (although the cross-entropy values are arguably\nlarge). Therefore, we conjecture diversity improves test per-\nformance on general tasks, but more extensive experiments\nhave to be done to make this conclusive. However, this type\nof experiment is very challenging because pre-training an\nLLM at scale to build a real foundation model is expensive\n\u2013 about 10M dollars or more (Ruiz, 2023). This type of\nexperiment requires substantial incentives and funding to\ndo.\nAcknowledgements\nWe also want to acknowledge Rylan Schaeffer for discus-\nsions that helped us think more critically about our work.\nWe would like to thank Simran Arora for her feedback on our\nmanuscript. This research was funded in part by the Stan-\nford School of Engineering fellowship and EDGE scholar\nfellowship from Stanford University. Alycia Lee is sup-\nported by Microsoft.\n9\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nReferences\nAchille, A., Lam, M., Tewari, R., Ravichandran, A.,\nMaji, S., Fowlkes, C. C., Soatto, S., and Perona, P.\nTask2vec: Task embedding for meta-learning. CoRR,\nabs/1902.03545, 2019. URL http://arxiv.org/\nabs/1902.03545.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,\nMcCandlish, S., Radford, A., Sutskever, I., and Amodei,\nD.\nLanguage models are few-shot learners.\nCoRR,\nabs/2005.14165, 2020. URL https://arxiv.org/\nabs/2005.14165.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Ponde de Oliveira\nPinto, H., Kaplan, J., Edwards, H., Burda, Y., Joseph,\nN., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\nM., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\nS., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-\nian, M., Winter, C., Tillet, P., Petroski Such, F., Cum-\nmings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-\nVoss, A., Hebgen Guss, W., Nichol, A., Paino, A., Tezak,\nN., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saun-\nders, W., Hesse, C., Carr, A. N., Leike, J., Achiam,\nJ., Misra, V., Morikawa, E., Radford, A., Knight, M.,\nBrundage, M., Murati, M., Mayer, K., Welinder, P., Mc-\nGrew, B., Amodei, D., McCandlish, S., Sutskever, I.,\nand Zaremba, W.\nEvaluating Large Language Mod-\nels Trained on Code. URL https://www.github.\ncom/openai/human-eval.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nClark, A., De Las Casas, D., Guy, A., Mensch, A., Paganini,\nM., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T.,\nBorgeaud, S., et al. Unified scaling laws for routed lan-\nguage models. In International Conference on Machine\nLearning, pp. 4057\u20134086. PMLR, 2022.\nDavid, S. B., Lu, T., Luu, T., and Pal, D.\nImpos-\nsibility theorems for domain adaptation.\nIn Pro-\nceedings of the Thirteenth International Conference\non Artificial Intelligence and Statistics, pp. 129\u2013136,\n2010. URL https://proceedings.mlr.press/\nv9/david10a.html.\nDevlin, J., Chang, M. W., Lee, K., and Toutanova, K. BERT:\nPre-training of Deep Bidirectional Transformers for Lan-\nguage Understanding. NAACL HLT 2019 - 2019 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies - Proceedings of the Conference, 1:4171\u20134186,\noct 2018. URL https://arxiv.org/abs/1810.\n04805v2.\nFowl, L., Goldblum, M., Gupta, A., Sharaf, A., and Gold-\nstein, T. Random network distillation as a diversity metric\nfor both image and text generation, 2020.\nFriedman, D. and Dieng, A. B. The vendi score: A diversity\nevaluation metric for machine learning, 2022.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\nPresser, S., and Leahy, C. The pile: An 800gb dataset of\ndiverse text for language modeling, 2020.\nGoogle.\nPalm\n2\ntechnical\nre-\nport.\nTechnical\nreport,\n2023.\nURL\nhttps://ai.google/static/documents/palm2techreport.pdf.\nGordon, M. A., Duh, K., and Kaplan, J. Data and parameter\nscaling laws for neural machine translation. In Proceed-\nings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pp. 5915\u20135922, 2021.\nHan, J. M., Rute, J., Wu, Y., Ayers, E. W., and Polu, S. Proof\nartifact co-training for theorem proving with language\nmodels.\nHashimoto, T. Model performance scaling with multiple\ndata sources. In Meila, M. and Zhang, T. (eds.), Pro-\nceedings of the 38th International Conference on Ma-\nchine Learning, volume 139 of Proceedings of Machine\nLearning Research, pp. 4107\u20134116. PMLR, 18\u201324 Jul\n2021. URL https://proceedings.mlr.press/\nv139/hashimoto21a.html.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep Residual\nLearning for Image Recognition. Proceedings of the IEEE\nComputer Society Conference on Computer Vision and\nPattern Recognition, 2016-December:770\u2013778, dec 2015.\nISSN 10636919. doi: 10.1109/CVPR.2016.90. URL\nhttps://arxiv.org/abs/1512.03385v1.\nHenighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C.,\nJackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S.,\net al. Scaling laws for autoregressive generative modeling.\narXiv preprint arXiv:2010.14701, 2020.\nHernandez, D., Kaplan, J., Henighan, T., and McCan-\ndlish, S.\nScaling laws for transfer.\narXiv preprint\narXiv:2102.01293, 2021.\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\nKianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou,\nY. Deep learning scaling is predictable, empirically. arXiv\npreprint arXiv:1712.00409, 2017.\n10\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\nJones, A. L. Scaling scaling laws with board games. arXiv\npreprint arXiv:2104.03113, 2021.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E.\nIma-\ngeNet Classification with Deep Convolutional Neural Net-\nworks. 2012. URL http://code.google.com/p/\ncuda-convnet/.\nKynk\u00a8a\u00a8anniemi, T., Karras, T., Laine, S., Lehtinen, J., and\nAila, T. Improved precision and recall metric for assess-\ning generative models, 2019.\nLongpre, S., Yauney, G., Reif, E., Lee, K., Roberts, A.,\nZoph, B., Zhou, D., Wei, J., Robinson, K., Mimno, D.,\nand Ippolito, D. A pretrainer\u2019s guide to training data:\nMeasuring the effects of data age, domain coverage, qual-\nity, & toxicity. arXiv preprint arXiv:2305.13169, 2023.\nURL https://doi.org/10.48550/arXiv.2305.13169.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models, 2016.\nMiranda, B., Yu, P., Wang, Y.-X., and Koyejo, S. The Curse\nof Low Task Diversity: On the Failure of Transfer Learn-\ning to Outperform MAML and Their Empirical Equiva-\nlence. arXiv, 2022a. doi: 10.48550/arXiv.2208.01545.\nURL https://arxiv.org/abs/2208.01545.\nMiranda, B., Yu, P., Wang, Y.-X., and Koyejo, S. The curse\nof low task diversity: On the failure of transfer learning to\noutperform maml and their empirical equivalence, 2022b.\nURL https://arxiv.org/abs/2208.01545.\nMiranda, B., Yu, P., Goyal, S., Wang, Y.-X., and Koyejo, S.\nIs pre-training truly better than meta-learning?\narXiv\npreprint arXiv:2306.13841, 2023.\nhttps://doi.\norg/10.48550/arXiv.2306.13841.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\nAtari with Deep Reinforcement Learning. 2013.\nNaeem, M. F., Oh, S. J., Uh, Y., Choi, Y., and Yoo,\nJ.\nReliable fidelity and diversity metrics for genera-\ntive models. In III, H. D. and Singh, A. (eds.), Pro-\nceedings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Machine\nLearning Research, pp. 7176\u20137185. PMLR, 13\u201318 Jul\n2020. URL https://proceedings.mlr.press/\nv119/naeem20a.html.\nNeumann, O. and Gros, C.\nScaling laws for a multi-\nagent reinforcement learning model.\narXiv preprint\narXiv:2210.00849, 2022.\nNostalgebraist. Chinchilla\u2019s wild implications. AI Alignment\nForum, 2022.\nOpenAI. Gpt-4 technical report. 2023.\nPolu, S. and Sutskever, I. Generative Language Modeling\nfor Automated Theorem Proving. sep 2020. URL http:\n//arxiv.org/abs/2009.03393.\nRabe, M. N., Research, G., Lee, D., Bansal, K., and Szegedy,\nC. Mathematical Reasoning via Self-supervised Skip-tree\nTraining. Technical report.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2019.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I.\nLearning Transfer-\nable Visual Models From Natural Language Supervision.\nfeb 2021. URL https://arxiv.org/abs/2103.\n00020v1.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. arXiv e-prints, 2019.\nRosenfeld, J. S., Rosenfeld, A., Belinkov, Y., and Shavit,\nN. A constructive prediction of the generalization error\nacross scales. arXiv preprint arXiv:1909.12673, 2019.\nRuiz, A.\nWhat it takes to train a foundation model,\n7\n2023.\nURL\nhttps://www.nocode.ai/\nwhat-it-takes-to-train-a-foundation-model/.\nDirector of Data Science at IBM and the founder of\nNoCode.ai.\nSajjadi, M. S. M., Bachem, O., Lucic, M., Bousquet, O.,\nand Gelly, S. Assessing generative models via precision\nand recall, 2018.\nSchaeffer, R., Miranda, B., and Koyejo, S. Are emergent\nabilities of large language models a mirage?, 2023.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,\n11\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nT., Leach, M., Kavukcuoglu, K., Graepel, T., and Has-\nsabis, D.\nMastering the game of Go with deep neu-\nral networks and tree search. Nature 2016 529:7587,\n529(7587):484\u2013489, jan 2016. ISSN 1476-4687. doi:\n10.1038/nature16961. URL https://www.nature.\ncom/articles/nature16961.\nSimon, L., Webster, R., and Rabin, J. Revisiting preci-\nsion and recall definition for generative model evaluation,\n2019.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. Llama: Open and efficient foundation language\nmodels, 2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nVu, T., Wang, T., Munkhdalai, T., Sordoni, A., Trischler, A.,\nMattarella-Micke, A., Maji, S., and Iyyer, M. Exploring\nand predicting transferability across NLP tasks. CoRR,\nabs/2005.00770, 2020. URL https://arxiv.org/\nabs/2005.00770.\nXie, S. M., Raghunathan, A., Liang, P., and Ma, T. An\nexplanation of in-context learning as implicit bayesian\ninference. CoRR, abs/2111.02080, 2021. URL https:\n//arxiv.org/abs/2111.02080.\nYe, W., Liu, S., Kurutach, T., Abbeel, P., Gao, Y., University,\nT., Berkeley, U. C., Qi, S., and Institute, Z. Mastering\nAtari Games with Limited Data. oct 2021. URL https:\n//arxiv.org/abs/2111.00210v1.\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\nvision transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npp. 12104\u201312113, 2022.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals,\nO. Understanding deep learning (still) requires rethink-\ning generalization.\nCommunications of the ACM, 64\n(3):107\u2013115, 2021.\nURL https://doi.org/10.\n1145/3446776.\n12\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nA. Future Work\nOur future research will explore the potential of the Task2Vec distance function for pre-training dataset curation. Given that\nthe objective of pre-training is to maximize downstream task performance, we define high-quality training data as data that\nfacilitates the best achievable performance on such tasks. We anticipate that higher diversity in the dataset will increase\nthe likelihood of achieving this objective. The rationale is that a higher data diversity implies a broader coverage of tasks\nor batches, thereby increasing the probability of training the model on tasks or data representations that are relevant to\nevaluation tasks. Our focus will be to leverage Task2Vec to assess the similarity between individual data points, batches,\nor datasets to a target task. This assessment will enable us to curate the training data by selectively removing tasks that\nresemble random, noisy, or irrelevant sequences, which may adversely affect downstream performance.\nB. Task2Vec Diversity Coefficient Correlates with Ground Truth Diversity\nFigure 4. Task2Vec diversity coefficient correlates with ground truth diversity for synthetic Gaussian benchmark. Source: (Miranda\net al., 2022b)\nAs shown in (Miranda et al., 2022b), when the ground truth diversity is available for a synthetic Gaussian benchmark, the\nTask2Vec diversity coefficient correlates with the ground truth diversity. These results provide confidence in the Task2Vec\ndiversity coefficient as diversity metric.\nC. Pipeline for Diversity Coefficient Computation of Natural Language Datasets\nFigure 5 shows our pipeline for computing the diversity coefficient of large scale, natural language datasets. See section 2.2\nfor more details on our method.\nD. Role of the Diversity of Pre-training data on Test Performance\nTraining Data Set\nPile All Subsets (div 0.246)\nOpenWebText2 (div 0.222)\nUSPTO (div 0.158)\n788.7322\n520.8103\nPubMed (div 0.168)\n805.0728\n470.7946\nUSPTO + PubMed (0.174)\n585.6262\n436.1974\nTable 3. The table illustrates that as the diversity coefficient increases, the perplexity (ppl) decreases, implying that more diversity\nimproves evaluation performance. We trained all models on approximately the same number of tokens (1.053 B, 1.057 B, and 1.033 B),\nall models size 51.5 M params. and are GPT-2 based. We choose the Pile All subsets and OpenWebText since those align with intuitively\ndiverse data sets \u2013 which we verified with the computed diversity values shown.\n13\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nFigure 5. A depiction of a pipeline to compute the Task2Vec diversity coefficient for a natural language dataset.\nPre-training dataset\nUSPTO (validation)\nPubMed Abs. (validation)\nOpenwebtext2\nNIH Exporter\nHacker News\nOpen Subtitles\nWikitext-103\nTiny Stories\nUSPTO\n0.7123 \u00b1 0.001717\n0.5840 \u00b1 0.001389\n0.5267 \u00b1 0.001377\n0.5879 \u00b1 0.001388\n0.5234 \u00b1 0.001275\n0.4917 \u00b1 0.001162\n0.5311 \u00b1 0.001303\n0.5107 \u00b1 0.001203\nPubMed Abs.\n0.5805 \u00b1 0.001396\n0.6939 \u00b1 0.001697\n0.5268 \u00b1 0.001367\n0.6622 \u00b1 0.001569\n0.5114 \u00b1 0.001300\n0.4817 \u00b1 0.001145\n0.5212 \u00b1 0.001200\n0.4868 \u00b1 0.001167\nUSPTO + PubMed Abs.\n0.6687 \u00b1 0.001602\n0.6526 \u00b1 0.001513\n0.5332 \u00b1 0.001390\n0.6331 \u00b1 0.001452\n0.5215 \u00b1 0.001272\n0.4871 \u00b1 0.001123\n0.5347 \u00b1 0.001290\n0.5042 \u00b1 0.001169\nTable 4. The data alignment coefficient appears to capture an intuitive notion of data similarity, since it finds training data that\nshares similar semantics and structure as the validation data as most aligned. In particular, PubMed Abs. (train) and NIH Exporter,\nwhich share the semantics of health-related research and the structure of being research writing, are found to be more aligned than USPTO\n(patent application backgrounds). Similarly, USPTO + PubMed Abs. (train) is more aligned to USPTO (validation) than PubMed Abs.\n(train) but less aligned to USPTO (validation) than USPTO (train). Each cell indicates the alignment coefficient between the given\npre-training dataset (row label) and evaluation dataset (column label).\nE. Role of Formal Data Alignment on Test Performance\nGiven a distance function for a batch of data (or task) one can also measure how aligned or similar two data sets. One can do\nthis my computing the cross difference between batches/tasks of data and subtract 1. Therefore, we propose one minus the\nTask2Vec Cross Diversity Coefficient as the definition of the Task2Vec Alignment Coefficient:\n\u02c6align(D1, D2) = 1 \u2212 EB1\u223cD1,B2\u223cD2d(\u20d7fB1, \u20d7fB2)\nWe choose this method:\n1. It is simple to define given the definition of Cross Diversity proposed in this paper.\n2. It provides simple confidence interval to compute given that it\u2019s an expectation \u2013 in contrast to using a large batch of\ndata and the cosine distance of two Task2vec batch/task embeddings.\nIn figure 6 demonstrate there is solid relationship to model performance (cross entropy loss) on various evaluation datasets\n(r2 = 0.80, r = 0.89). The raw alignment values with 95% confidence are reported at E and as expected when the topics\nare conceptually aligned the alignment coefficient is higher (see caption of table).\nF. Experimental Details\nF.1. Dataset Preprocessing\nIn accordance with (Achille et al., 2019), we used the training split of datasets to finetune the probe network when computing\nTask2Vec embeddings per dataset. Sequences were tokenized using a pre-trained HuggingFace GPT-2 tokenizer based on\nbyte-level Byte-Pair-Encoding, and padded or truncated to a max length of 128. Because the WikiText-103 dataset contained\nempty text examples, we removed these examples before sampling batches to compute embeddings.\n14\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nFigure 6. The data alignment coefficient demonstrates a solid relationship to model performance (cross entropy loss) on various\nevaluation datasets (r2 = 0.80, r = 0.89). The data alignment coefficient is computed between a model\u2019s pre-training dataset (PubMed\nAbs., USPTO, or PubMed Abs. + USPTO) and a single evaluation dataset (represented by a unique color).\n15\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nF.2. Model Architecture and Finetuning\nWe used a pre-trained GPT-2 model with a language modeling (LM) head on top. The pre-trained GPT-2 model itself has 12\nlayers, 12 heads, 768-d hidden size, and 117M total parameters. The LM head is a linear layer with weights corresponding\nto the input embedding layers. The model was pre-trained on the English language and the pre-trained GPT-2 tokenizer\nhas a vocab size of \u2248 50k tokens. For all finetuning experiments, we fine-tuned only the LM head for 10 epochs. We used\nno learning rate scheduler and no gradient accumulation. We used the AdamW optimizer, since AdamW has been shown\nempirically to give better training loss and improved generalization.\nWe note that, in principle, the Task2vec diversity coefficient can be computed with any LLM. The metric itself is not specific\nto any particular LLM architecture or model version. We chose GPT-2 for our experiments due to computational efficiency\nand resource constraints. However, more powerful LLMs like LLaMA can also be used to compute the diversity coefficient.\nAs long as the probe network used is consistent across experiments, the relative differences in the diversity coefficient value\nbetween datasets are directly comparable. The same goes for using pretrained vs. non-pretrained probe networks.\nF.3. Number of Batches and Batch Size Selection\nDiversity coefficients in Table 2 were computed using randomly selected batches of size 512 sequences and a pre-trained,\nfinetuned GPT-2 probe network. Diversity coefficients of C4, WikiText-103, The Pile, Pile-CC, HackerNews, NIH ExPorter,\nPubMed Abstracts, and USPTO were each computed using 200 sampled batches. Given resource constraints, we found 200\nbatches5 to be a sufficiently large number of batches to estimate the diversity coefficient with tight 95% confidence intervals\non the order of 1e-5. We chose 512 as the batch size, since it is a relatively large and feasible batch size to fine-tune the\nprobe network on 200 batches using Azure NV12s v3 instances equipped with Tesla M60 GPUs in a reasonable amount of\ntime (30+ hours).\nF.4. Diversity Coefficient Computation of Concatenated Datasets\nThe diversity coefficient of a concatenated dataset of C4 and WikiText-103 was measured over a combined set of batches.\nEach batch consisted of sequences sampled from one of these datasets, e.g. a batch could have sequences randomly sampled\nfrom C4 or WikiText-103 but not both. The coefficient was computed over 400 batches of batch size 512 (200 batches from\neach dataset). Note that for the concatenated dataset, we utilized the same 200 batches per dataset that were used to compute\nthe coefficients of C4 and of WikiText-103 individually.\nThe diversity coefficient of concatenated five sub-datasets of The Pile was computed over 1000 batches (200 batches from\neach dataset) of batch size 512. Similarly to the concatenated dataset of C4 and WikiText-103, we utilized the same 200\nbatches per dataset that were used to compute the coefficients of each individual sub-dataset.\nF.5. Diversity Coefficient of The Pile vs. Concatenation of Five Sub-Datasets\nWe make a clarification on the approach taken to evaluate the diversity coefficient for The Pile vs. for concatenation of its\nfive sub-datasets.\nThe diversity coefficient of The Pile was computed over 200 batches sampled across all 22 sub-datasets of The Pile. This\nmeans that any given batch could contain sequences across all 22 sub-datasets, i.e. a batch could have sequences from\nPile-CC, HackerNews, and NIH ExPorter.\nThe diversity coefficient of the concatenated dataset was computed over 1000 batches comprised of 200 batches separately\nsampled from each of the five sub-datasets. Each batch contained sequences from only one sub-dataset, i.e. a batch could\nonly have sequences from Pile-CC or HackerNews or NIH ExPorter.\nWe hypothesize this distinction in the diversity coefficient computation explains why the concatenated dataset has higher\ndiversity, even though it consists of only five of the 22 sub-datasets of The Pile. For the diversity coefficient of The\nPile, because batches were sampled such that any batch contains sequences from across the 22 sub-datasets, the batch\nrepresentations learned by the probe network may have been more similar, resulting in lower diversity relative to the\nconcatenated dataset.\n5This results in (2002 \u2212 200)/2 = 19, 900 pairwise distances used to compute the diversity coefficient.\n16\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nG. Pairwise Distance Distributions of C4, WikiText-103, and The Pile\nFigure 7. Distributions of pairwise batch distances from C4 (top left), WikiText-103 (top right), and The Pile (bottom) are\napproximately Gaussian, which justifies the use of a sample of batches to measure the diversity coefficient. Dotted lines indicate the\naverage distance, i.e. the diversity coefficient, for each dataset.\nExperiments: To provide confidence in the magnitude of the coefficient values of C4, WikiText-103, and The Pile, we\nplot the distribution of distances per dataset in Figure 7. We aim to show that a subsample of batches can provide a good\nestimation of population statistics, such as the diversity coefficient, which measures the expected Task2Vec (cosine) distance\nbetween batches.\nResults: For each dataset, the pairwise distances take on unimodal and approximately Gaussian distributions with few\noutliers. These results suggest the Task2Vec distances are approximately normally distributed. This suggests we can make\nstrong inferences about the population. Specifically, we are able to compute a good estimate of the diversity coefficient\nusing 200 batches using the mean. This is in fact the same argument from (Miranda et al., 2022a) \u2013 but we verified it applied\nin our setting. Figure 7 also shows few outlier batches \u2013 the presence of which could influence the computed diversity\ncoefficient. This provides further confidence in the coefficient values computed and justifies our use of a sample of batches\nto estimate diversity.\nOpenWebtext: Data from Reddit post URLs was extracted, deduplicated, and filtered for English content using FastText.\nWeb pages were pulled using the newspaper python package, and near-duplicates were identified and removed using\nlocal-sensitivity hashing (LSH). Only documents with a unique content similarity of less than 0.5 and more than 128 tokens\nwere retained. The finalized dataset comprises 38GB from 8,013,769 documents. Annotations: None present in the dataset.\nUsed to train GPT2.\nH. Generative IN-Context Learning (GINC) Dataset\nH.1. Background\nThe GINC dataset is generated using the latent concept framework proposed in (Xie et al., 2021), where language models\ncondition on a prompt to infer latent document concepts learned during pre-training. The pretraining distribution is defined\nusing a uniform mixture of Hidden Markov Models (HMMs) parameterized over a family \u0398 of latent concepts.\n17\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nH.2. Definitions of GINC Dataset Parameters\nNumber of latent concepts: A latent concept \u03b8 parameterizes the transitions of a HMM in the mixture. A latent concept\n(e.g. a wiki bio) contains document statistics, such as semantics, syntax, and the formatting of and distribution of tokens.\nVocabulary size: Each HMM in a given mixture outputs a fixed number of tokens, defined as the vocabulary size. The\nvocabulary is generated by enumerating combinations of letters from a to z, aa to az, etc. The delimiter token is designated\nby a backslash. Sequences are tokenized by whitespace.\nH.3. Supplemental Figures for Diversity Coefficient vs. GINC Parameters\nFigure 8. Trends noted in Section 3.4 are consistent for diversity coefficient vs. number of latent concepts (left) and coefficient vs.\nvocab size (right) when the other parameter changes. The diversity coefficient with 95% confidence intervals saturates with increasing\nnumber of latent concepts (left) even as vocab size is varied between 50-1000. Larger vocab sizes generally produce higher diversity\ncoefficients (right) even as the number of latent concepts is varied between 1-5000.\nFigure 8 confirms that the trends between the diversity coefficient and number of latent concepts (left) hold even as vocab\nsize is varied. Similarly, trends between the diversity coefficient and the vocabulary size (right) hold as the number of latent\nconcepts is varied. These trends were noted in Section 3.4.\nI. Discussion (cont.)\nOur paper introduces a metric that leverages tunable parameters, such as the number of batches, batch size, probe network\nconfiguration (pre-trained vs. random, fine-tuned vs. not) and depth. While these elements influence the diversity\ncoefficient\u2019s absolute value and necessitate the recalibration of lower and upper bounds (see sections F.3 and 4), a consistent\nchoice of hyperparameters can mitigate these effects.\nIntriguingly, our proposed diversity may not always correlate with model performance, as high diversity could simply be\ndue to uniform noise. Nevertheless, we contend that a higher diversity, in the context of a sufficiently large model, likely\nindicates superior performance and data quality. Furthermore, our diversity metric is intentionally designed to be widely\napplicable, albeit concealing causal factors, rendering it an effective tool for ablation studies.\nDespite our diversity metric\u2019s broader applicability, it may obscure certain causal factors. This limitation is intentional to\nenhance its practical usage \u2013 since causality is often difficult to infer and is out of scope. This can be overcome with data\nproperty ablation studies, as we showed in our GINC dataset experiments.\nCurrently, our proposed bounds are specific to sequence data with a symbolic vocabulary, limiting their applicability across\ndifferent modalities. To overcome this limitation, we suggest using a multimodal embedding method for embedding diversity\ncoefficients and lower/upper bounds across tasks.\nTo really clarify why FIM is better than activations, we provide this intuitive explanation. FIM gives a weight/feature of\nwhich parameter of the generative distribution matters, e.g. the first coordinate of Task2Vec corresponds to how artsy the\ntext sequence is. This is a feature of a task or dataset itself. Therefore, FIM exactly approximates the (task) data generative\ndistribution we are trying to embed. Therefore, we conjecture it results in superior representations for datasets compared\n18\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\nto activations since it directly approximates the data (or task) generative distribution. Our study, and references, provide\npositive evidence in favor of this argument.\nThe strength of embeddings is their ability to approximate semantics in a way that symbols may struggle with, such as\ndistinguishing the equivalence of two sentences with different symbols but identical meanings. In NLP there is no easy\nway to determine this equivalence. In formal mathematics, symbolic semantics and thus equivalence can sometimes be\ndone exactly. Though it does not come without its costs, e.g. requires expert knowledge, computationally demanding or\n(approximately) exhaustive representations like e-graphs. Therefore, embedding methods for data diversity, quality, etc.\nhave the unique advantage of being more generally applicable.\nOur diversity calculations predominantly utilize a small model (GPT-2). Despite the ongoing discussion concerning the\nemergence of large language models (LLMs), our conjecture extends the results to models of all sizes. We base this\ninference on the fact that the manifestation of emergence is intimately tied to the specific metric employed, and the sudden\nunpredictable jumps disappear when smooth metrics are applied (Schaeffer et al., 2023). The cosine distance is smooth and\ndoes not have this issue.\nWhy and when does diversity matter? We propose two central conjectures for the importance of diversity and provide the\nunderlying rationale:\n1. Conjecture 1: Diversity is essential because it promotes learning-to-learn (a surrogate for General Intelligence).\nThe main argument is that a significant level of diversity corresponds to a multitude of tasks in the dataset. Therefore, to\nachieve high (test) performance, the model must perform well on all tasks. One potential strategy is by learning-to-learn,\nthereby allowing transferability when tasked with a new problem. Another alternative could be to memorize all tasks.\n2. Conjecture 2: Diversity is crucial because it enhances the probability that the pre-training set covers the test set.\nDiversity is a formal score of coverage \u2013 it aims to reflect the effective number of tasks in a dataset. Thus, increased\ndiversity equates to more tasks in a dataset. This (could) boosts the chance of the training set covering the test set,\nhence improving performance, given a sufficiently large model like an LLM. The direct exploration of this conjecture\nis slated for future investigation, but we provide a suggestive (correlative) analysis of one reason why LLMs might\nperform so well.\nLimitations:\n\u2022 The diversity coefficient presents an aggregate measure that masks the underlying causal factors. Despite this, we\nillustrate how it might be employed to uncover these factors. We show this through the use of vocabulary size and latent\nspace, acknowledging that these experiments could be resource-intensive. Causality is a challenging topic, and we do\nnot claim to solve it through our experiments. Our experiments in this regime are mostly to show that the diversity\ncoefficient (might) correlates/captures different sources of diversity beyond number of concepts or tasks.\n\u2022 The computation of Task2Vec embeddings requires more resources than computing simply the activations. However,\ngiven the proven correlation with ground truth task generative parameters from previous studies, we posit that it\nsupersedes activations. Furthermore, we hypothesize that using activations could result in high distances due to\noptimization for decision boundaries, making it less reliable for measuring distances i.e., high distances in activation\nspace might be artificially high. We observed this but plan to give more detailed study in the future.\n\u2022 The choice of an expectation as the aggregation function could be seen as arbitrary. Alternatives such as the Vendi\nscore are promising, but still under-explored and computationally demanding compared to expectations/sums. Future\nwork could focus on the comparative analysis of the total distance sum and the Vendi score. We hypothesize, in line\nwith the Central Limit Theorem (CLT), that the results might not differ significantly e.g., CLT still converges to (unit)\nNormal given the proper normalization procedure.\n\u2022 We reject the notion that the use of models is a limitation. As discussed earlier, models can provide superior data\nembeddings, and all forms of data representations are always required. For example, the identity function or symbols\nare data representations.\nImplications:\n19\nBeyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data\n\u2022 Given the impressive performance of LLMs, our study suggests a correlation with our diversity measure, potentially\nproviding an explanation for this high level of performance.\n\u2022 High diversity implies a larger task coverage. Therefore, we conjecture that a highly diverse pre-training set could\nincrease the probability of including relevant pre-training data for the target task/testing. This suggests that collecting\nmore diverse data could be a method to enhance performance. If the model is sufficiently large, we conjecture this\nmethod always (stochastically) monotonically increases the performance (as implied by (Zhang et al., 2021)).\n\u2022 The transition from a qualitative to a quantitative measure of diversity can be seen as a significant advancement in the\nfield because of conceptual transitions about how we think and talk about data quality/diversity.\n\u2022 The use of Task2Vec to embed data implies a method applicable to any modality, potentially benefiting all areas of\nmachine learning research.\n20\n"
  },
  {
    "title": "H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
    "link": "https://arxiv.org/pdf/2306.14048.pdf",
    "upvote": "10",
    "text": "H2O: Heavy-Hitter Oracle for Efficient Generative\nInference of Large Language Models\nZhenyu Zhang1, Ying Sheng2, Tianyi Zhou3, Tianlong Chen1, Lianmin Zheng4, Ruisi Cai1,\nZhao Song5, Yuandong Tian6, Christopher R\u00e92, Clark Barrett2, Zhangyang Wang1, Beidi Chen6,7\n1University of Texas at Austin, 2Stanford University, 3University of California, San Diego,\n4University of California, Berkeley, 5Adobe Research, 6Meta AI (FAIR), 7Carnegie Mellon University\n{zhenyu.zhang,tianlong.chen,ruisi.cai,atlaswang}@utexas.edu, ying1123@stanford.edu,\n{chrismre,barrett}@cs.stanford.edu, t8zhou@ucsd.edu, lianminzheng@gmail.com,\nzsong@adobe.com, yuandong@meta.com, beidic@andrew.cmu.edu\nAbstract\nLarge Language Models (LLMs), despite their recent impressive accomplishments,\nare notably cost-prohibitive to deploy, particularly for applications involving long-\ncontent generation, such as dialogue systems and story writing. Often, a large\namount of transient state information, referred to as the KV cache, is stored in GPU\nmemory in addition to model parameters, scaling linearly with the sequence length\nand batch size. In this paper, we introduce a novel approach for implementing the\nKV cache which significantly reduces its memory footprint. Our approach is based\non the noteworthy observation that a small portion of tokens contributes most of\nthe value when computing attention scores. We call these tokens Heavy Hitters\n(H2). Through a comprehensive investigation, we find that (i) the emergence of H2\nis natural and strongly correlates with the frequent co-occurrence of tokens in the\ntext, and (ii) removing them results in significant performance degradation. Based\non these insights, we propose Heavy Hitter Oracle (H2O), a KV cache eviction\npolicy that dynamically retains a balance of recent and H2 tokens. We formulate\nthe KV cache eviction as a dynamic submodular problem and prove (under mild\nassumptions) a theoretical guarantee for our novel eviction algorithm which could\nhelp guide future work. We validate the accuracy of our algorithm with OPT,\nLLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of\nH2O with 20% heavy hitters improves the throughput over three leading inference\nsystems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by\nup to 29\u00d7, 29\u00d7, and 3\u00d7 on OPT-6.7B and OPT-30B. With the same batch size,\nH2O can reduce the latency by up to 1.9\u00d7. The code is available at https:\n//github.com/FMInference/H2O.\n1\nIntroduction\nLarge Language Models (LLMs) have demonstrated remarkable proficiency in a wide range of\nnatural language processing applications such as content creation, summarization, and dialogue\nsystems [1, 2, 3, 4]. However, their deployment is very costly. In addition to the widely-studied\nbottlenecks of model size and the quadratic cost of attention layers, the problem of the size of\nthe KV cache, which stores the intermediate attention key and values during generation to avoid\nre-computation, is becoming increasingly prominent [5]. For instance, a 30 billion-parameter model\nwith an input batch size of 128 and a sequence length of 1024 results in 180GB of KV cache. A\nnatural approach is to limit its maximum size as is done in classical software or hardware caches [6].\nHowever, it is challenging to reduce KV cache memory footprints in LLMs without accuracy drops.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.14048v3  [cs.LG]  18 Dec 2023\n0.1\n0.1\n0.5\nChildren laughed\nand\nin\nthe\nsunny\npark\n...\n.\nplayed\n0.2\n0.1\n1\n0.9\n0.6\n0.2\n0.1\n0.1\n0.6\n1.4\n0.6\n1.5\n0.5\n0.4\n0.1\n0.4\n0.1\nQuery\nKey\nValue\nStatic Sparsity (Strided)\nStatic Sparsity (Local)\nStatic Sparsity w. \nDynamic Sparsity\nFigure 1: Upper plots illustrate symbolic plots of an attention map deploying different KV cache policies in\nLLM generation. Lower right: contrasts their accuracy-memory trade-off. Left: the overview of H2O framework.\nWhile there exists substantial literature on sparse attention approximation in training, they have\nnot seen wide adoption for alleviating KV cache bottleneck. First, most existing methods, e.g.,\nReformer [7] and Flash Attention [8], are designed to overcome the quadratic memory required by\nattention mechanisms when modeling long sequences but still require a large cache size. Second,\nvariants like sparse transformer [9], low-rank based transformers [10, 11] or multi-query attention [12,\n13, 5] can reduce the cache size, but directly applying them on pre-trained LLMs for generation\nresults in high miss rates and degrades the accuracy as shown in Figure 1. Finally, some recent\nadvances such as gisting tokens [14] can learn to compress the KV cache for documents, but their\nexpensive eviction policies are difficult to deploy during generation.\nTherefore, an ideal KV cache should have (i) a small cache size to reduce memory footprint, (ii) a\nlow miss rate to maintain the performance and long-content generation ability of LLMs, and (iii)\na low-cost eviction policy to reduce the wall-clock time during generation. However, there are\nthree technical challenges. First, it is not immediately clear whether the size of the KV cache can\nbe restricted\u2014each decoding step might, in principle, require access to all previous attention keys\nand values. Second, identifying an optimal eviction policy that maintains generation accuracy is a\ncombinatorial problem1. Finally, even if an optimal policy can be brute-forced, it is infeasible for\ndeployment on real-world applications.\nFortunately, our preliminary exploration has yielded intriguing observations about the empirical\nproperties of LLMs. These findings pave the way for the potential design of an efficient KV cache.\nSparsity for small cache size: We observe that even when trained densely, the attention matrices of\nLLMs are over 95% sparse at inference time (shown in Figure 2). This holds for a wide range of\npre-trained LLMs. Therefore, only 5% of the KV cache is sufficient for decoding the same output\ntoken at each generation step, which suggests it may be possible to have up to a 20\u00d7 reduction in\nKV cache size without an accuracy drop.\nHeavy-Hitters for low miss rate: We discover that the accumulated attention scores of all tokens\nin attention blocks adhere to a power-law distribution. It suggests that there exists a small set of\ninfluential tokens that are critical during generation, named heavy-hitters (H2). H2 provides an\nopportunity to step away from the combinatorial search problem and identify an eviction policy that\nmaintains accuracy.\nGreedy algorithm for low-cost policy: we surprisingly find that retaining the H2 based on local\nstatistics at each decoding step\u2014summing the attention scores of only the preceding tokens\u2014is as\neffective as considering the attention of future tokens (shown in Figure 2).\nBased on the above, we first rigorously define the generative process of LLMs operating with a\nsize-constrained KV cache in Section 2.1. Then we propose Heavy-Hitter Oracle (H2O), a framework\n1Belady\u2019s Algorithm is optimal for standard cache, but not necessarily for KV cache.\n2\nthat exploits the properties of LLMs and uses simple, low-cost eviction policies that retrain the quality\nof LLMs throughout the generation process. Specifically,\n\u2022 In Section 3, we explore the emergence of H2 in attention, revealing their fundamental and critical\nroles: (i) H2 exhibit a strong correlation of frequently co-occurring words in textual data; and (ii)\nremoving H2 completely damages the model\u2019s functionality. We demonstrate that H2 can largely\nlower the cache miss rate of the existing policies mentioned above. Theoretically, assuming the\nattention scheme is submodular, H2 corresponds to a greedy algorithm and is therefore near-optimal.\n\u2022 In Section 4, we present a greedy but low-cost variant of H2 which is dynamically determined\nby the accumulated attention score at each decoding step. We formulate the eviction policy with\ngreedy H2 as a variant of dynamic submodular maximization. The analysis shows that it results in\na similar generative process as the one using the H2 eviction policy.\nWe perform extensive experiments on OPT, LLaMA, and GPT-NeoX on a single NVIDIA A100\n(80GB) GPU to evaluate H2O across a range of tasks from lm-eval-harness [15] and HELM [16].\nWe implement H2O on top of FlexGen that can easily adapt different cache eviction techniques to\nproduce a system with high-throughput inference. Performance experiments show our framework\nachieves 29\u00d7, 29\u00d7, 3\u00d7 higher throughputs compared to three leading inference systems, DeepSpeed\nZero-Inference [17], Hugging Face Accelerate [18], and FlexGen [19] respectively. With the same\nbatch size, H2O achieves up to 1.9\u00d7 lower latency compare to FlexGen.\n2\nRelated Work and Problem Setting\nEfficient Inference of LLMs.\nThe substantial parameter counts of large language models (LLMs)\npresent significant challenges for inference. To overcome this limitation, previous efforts have\nemployed model compression techniques with specific designs to achieve efficient LLM inference,\nsuch as the method described in [20, 21, 22], which employs one-shot pruning on LLMs, resulting in\nnegligible performance degradation even without retraining. Additionally, alternative approaches\nexplore quantization methods specifically tailored to LLMs, as discussed in [23, 24, 25, 26, 27, 28].\nAlso, CoLT5 [29] employs a token-wise conditional computation strategy to reduce the overall\ncomputation cost. These methods address efficient inference from orthogonal perspectives and can be\norganically integrated. The techniques investigated in this study are closely associated with pruning or\nsparsity but focus on a distinct inference bottleneck, namely, KV cache. One closely related work[30]\nutilizes a learnable mechanism that determines necessary tokens during inference but requires an\nextra fine-tuning process, which makes it less practical.\nSparse, Low-rank Attention Approx.\nThe quadratic computational complexity of attention mod-\nules is one of the major bottlenecks of transformer inference [31]. Various efforts are devoted to\naddressing this challenge [7, 9, 10]. For example, Reformer [7] reduces the computational cost\nfrom quadratic to superlinear complexity via locality-sensitive hashing. Performer [10] employs\npositive orthogonal random features to approximate attention kernels. One relevant work, Sparse\nTransformer [9], introduces sparsity to reduce KV cache memory footprint and achieve an efficient\nattention mechanism, considered as our baseline in this paper. Moreover, SpAtten [32] utilizes\naccumulated attention scores to select important tokens for efficient attention inference while they\ndon\u2019t consider the variance of token importance across attention heads and layers. Comparison with\nSpAtten is detailed in Appendix C.9.\nCaching.\nCaching, which plays a pivotal role in optimizing system performance, entails the devel-\nopment of effective eviction policies to handle frequently accessed data. Conventional approaches\nsuch as Least Recently Used and Least Frequently Used [33, 34] prioritize the recency and frequency\nof data access. And the design of KV cache encounters many similar challenges as traditional caching.\nLLM Inference Breakdown.\nThe generative procedure of LLMs encompasses two distinct phases:\n(i) the prompt phase, in which an input sequence is utilized to produce the KV cache (consisting\nof the key and value embeddings), similar to the forward pass employed during LLM training; and\n(ii) the token generation phase, which leverages and updates the KV cache to generate new tokens\nincrementally. Each generation step relies on the previously generated tokens. The primary focus of\nthis paper is to enhance the efficiency of the KV cache in attention during the token generation phase,\nthereby accelerating LLM inference.\n3\n(a)\n(b)\n(c)\n(d)\nCOPA\nMathQA\nOpenBookQA\nPiQA\nRTE\nWinogrande\nFigure 2: (a) Attention Sparsity in pre-trained LLMs. (b) The distribution of accumulated attention scores with\nrespect to the corresponding word (red scatter) and the co-occurrence times of words in the data (gray curve).\nThe x-axis represents the word index in the vocabulary. (c) The performance comparison between the baseline\nmodel with full KV and the model w.o. heavy hitter. (d) Comparison between the baseline model with full KV,\nH2O with the local statistic, H2O with the global statistic, and the model with only the most recent KV (Local).\nApart from the baseline model, each model is evaluated with 20% KV cache budget.\n2.1\nProblem Formulation\nWe formally define the generative process with limited KV cache size. Denote attention query matrix\nas Q \u2208 Rn\u00d7d and key matrix as K \u2208 Rn\u00d7d. Qi,\u2217 represents the i-th row of Q and K\u2264i,\u2217 represents\nthe first i rows of K. Let k denote the budget of space and k < n. For simplicity, KSi,\u2217 (\u2208 Ri\u00d7d)\ndenotes a sub-matrix of K which selects Si rows from K. (For the non-selected rows [i]\\Si, we put\nall zeros in that row) Eviction policy is defined as:\nDefinition 2.1 (Eviction Policy, informal). Let Si\u22121 denote the source set. Let Si denote the target\nset. We defined the eviction policy g : Si\u22121 \u2192 Si such that\n\u2022 |Si| = k (KV cache size is not changing over the time)\n\u2022 |Si\\Si\u22121| \u2264 1 or equivalently |Si \u2229 Si\u22121| \u2265 k \u2212 1 (we can evict at most 1 KV in the KV cache)\nThen, we define the generative process with our eviction policy.\nDefinition 2.2 (The generative process with eviction policy, informal). Let k denote the size of the\nKV cache. For each i \u2208 [n], for the i-th token, we have\n\u2022 Let Si \u2282 [n] denote the tokens in KV cache when predicting the i-th token.\n\u2022 The information we have is a length-i vector oi := D\u22121\ni\n\u00b7exp(Qi,\u2217(KSi,\u2217)\u22a4) (normalized attention)\n\u2013 scalar Di := (exp(Qi,\u2217(KSi,\u2217)\u22a4) \u2212 1[i]\\Si) \u00b7 1i (the evicted KV is set to 0, and we need to\nsubtract them when computing the normalization)\n\u2013 Replacing Si by [i] in the above definition of oi and Di leads to standard generative process.\n\u2022 The eviction policy (Definition 2.1) updates Si based on Si\u22121 and their corresponding information.\nRemark 2.3. Our goal is to find a KV cache eviction policy such that the output of the generative\nprocess is similar or comparable to the original one without limiting the cache size.\n3\nObservations\nWe present two key empirical insights of LLMs that inspire the design of H2O, as follows.\n3.1\nSparsity for Small Cache Size\nInspired by previous literature, which reveals the existence of attention sparsity in DistillBERT [35]\nand bounded-norm self-attention heads [36]. We first show an observation on the sparsity of attention\nin pre-trained LLMs. Then we discuss how it can potentially unlock the possibility of reducing\nKV cache size without an accuracy drop. Given the normalized attention score Softmax(QK\u22a4)\nmatrix that is calculated by the query matrix Q and the key matrix K, we set the threshold as one\npercent of the maximum value in each row and calculates the corresponding sparsity.\nObservation.\nWe conduct zero-shot inference with the pre-trained OPT model on the validation\nset of Wiki-Text-103. We plot the layer-wise sparsity within attention blocks and visualize the\nnormalized attention score matrix. The results are presented in Figure 2 (a). We observe that although\nthe LLMs are densely trained, the resulting attention score matrices are highly sparse, with a sparsity\nover 95% in almost all layers.\n4\nInsights.\nThe attention blocks\u2019 sparsity suggests that access to all previous key and value embed-\ndings is unnecessary for generating the next token. This suggests it is possible to evict unessential\nKV embeddings and reduce the requirement of KV cache during generation.\n3.2\nHeavy-Hitters for Low Miss Rate\nThe previous section showed the sparse nature of attention blocks in pre-trained LLMs, which\nprovides the opportunity for designing small KV cache size while still maintaining the performance\nof LLMs. However, determining the best eviction policy that preserves generation accuracy presents\na combinatorial challenge. Although Belady\u2019s Algorithm [37] is optimal and easy to compute for\nstandard cache (offline), it is not applicable for KV cache design. Because once evicting important\nKVs, it could destroy the performance of LLMs due to the sequential dependency of LLM generation.\nObservation.\nFortunately, in the early stage of our exploration, we find that the accumulated\nattention scores of all the tokens within attention blocks follow a power-law distribution, as shown\nin Figure 2. This suggests the existence of a small set of tokens that are critical during generation.\nWe denote those tokens as heavy-hitters (H2). In order to verify the importance of these tokens, we\ncompare the quality of LLM generation after masking heavy hitters with that of the original model.\nNot surprisingly, as shown in Figure 2, the accuracy drops drastically, confirming the importance of\nthose tokens. Additionally, we can see the accumulated attention score of each word (in red dots)\nhave a high correlation with their co-occurrences in the data (gray curve).\nAnalysis.\nFirst, based on H2, we see an opportunity to side-step from the combinatorial search\nproblem and design a KV cache eviction policy that preserves the LLM generation quality. We\nconduct an empirical study implementing a KV cache eviction policy that retains only the H2 and\nthe recent KV embeddings in the cache. The intuition is that recent words typically exhibit stronger\ncorrelations with current tokens. We assess the effectiveness of this eviction policy through pre-\ntrained OPT-30B and six downstream tasks. The outcomes of these evaluations are illustrated in\nFigure 2. It is obvious that the H2 based eviction policy can largely reduce the KV cache size without\ndegrading the performance of OPT-30B.\nMoreover, during the post analysis, inspired by [38], we find that H2 based policy is related to\nthe classical greedy algorithm (a polynomial-time algorithm with provable guarantees) under the\nassumption that the attention schema is submodular. We present details in Appendix D.\nLemma 3.1 (informal). Assuming the attention scheme is submodular, then greedily constructing the\nset Si (without cache size limitation) satisfies the near-optimal property in terms of submodular.\n4\nHeavy-Hitter Oracle\nThe goal of this section is to propose the greedy algorithm using the H2-based policy and to show\nthe provable guarantees. We first present the H2-based policy called H2O cache eviction policy and\nformulate its deployment in LLM generation as a variant of submodular maximization problem,\nnamed dynamic submodular. Then we present H2O in the generative process, followed by a practical\nexample of deploying our proposal. Finally, we provide theoretical guarantees for H2O and show our\nefficient system implementation.\n4.1\nGreedy Algorithm for Low-Cost Policy\nWe have shown a simple yet effective KV cache policy based on H2. However, it is impractical to\ndeploy such an algorithm because we do not have access to the future-generated tokens. Fortunately,\nwe empirically observe that local H2, which is calculated using local statistics at every decoding step\nby summing up the attention scores of the previous tokens, is equally effective as taking into account\nthe attention of future tokens (Figure 2). In the following, we formally define this dynamic attention\nscore computation (with space limitation) as a novel dynamic submodular type problem.\nDefinition 4.1 (Dynamic submodular framework, informal). Define function F : 2[n] \u00d7 2[n] \u2192 R,\nthen for any set Z \u2282 [n], we assume that F(Z, \u00b7) : 2[n] \u2192 R is a submodular function w.r.t. to Z, i.e.,\n\u2022 For all sets X, Y \u2282 [n] satisfy that Z \u2282 X \u2282 Y ,\n\u2022 For all element x \u2208 [n] satisfy that x \u2208 [n]\\Y ,\nwe have f(X \u222a {x}) \u2212 f(X) \u2265 f(Y \u222a {x}) \u2212 f(Y ), where f(\u00b7) := F(Z, \u00b7).\n5\nRemark 4.2. We provide practical insights of Definition 4.1. X denotes the existing words in the\nKV cache. Y is any superset of X. x can be viewed as a \u201cword\u201d which is either newly added to KV\ncache or existing deleted from KV cache. An example f can be attention score, i.e., see Algorithm 1.\nIf we load the sequence of S1, S2, \u00b7 \u00b7 \u00b7 , Sn (we promise that |Si| \u2264 k and |Si\\Si\u22121| \u2264 1) into\nDefinition 4.1, i.e., for each i \u2208 [n], we choose Z = Si, then it becomes a particular instance of the\ndynamic submodular problem.\nNext, we provide a formal description of our algorithm, followed by an example.\nDefinition 4.3 (H2O Eviction Policy). Let Fscore : 2[n] \u2192 R denote certain score function. Let Si\u22121\ndenote the source set. Let Si denote the target set. We defined the eviction policy g : Si\u22121 \u2192 Si s.t.\n\u2022 |Si| = k (KV cache size is not changing over the time)\n\u2022 |Si\\Si\u22121| \u2264 1 or equivalently |Si \u2229 Si\u22121| \u2265 k \u2212 1 (we can evict at most 1 KV in the KV cache)\n\u2022 We construct Si \u2190 (Si\u22121 \u222a {i})\\{u} as u \u2190 arg maxv\u2208(Si\u22121\u222a{i}) Fscore(Si\u22121 \u222a {i}\\{v}}\nTo describe our algorithm (Algorithm 1), we choose a particular instantiation of the function Fscore,\ni.e., the summation of that sets in the attention matrix.\nAlgorithm 1 H2 Eviction Algorithm\n1: procedure H2_EVICTION(Q, K \u2208 Rn\u00d7d, k \u2208 N)\n2:\nLet k denote the budget size of cache\n3:\nS0 \u2190 \u2205\n4:\nfor i = 1 \u2192 n do\n5:\nif i \u2264 k then\n6:\nSi \u2190 Si\u22121 \u222a {i}\n7:\nelse\n8:\nDi \u2190 (exp(Qi,\u2217(KSi\u22121,\u2217)\u22a4) \u2212 1[i]\\Si\u22121) \u00b7 1i\n9:\noi \u2190 D\u22121\ni\n\u00b7 (exp(Qi,\u2217(KSi\u22121,\u2217)\u22a4) \u2212 1[i]\\Si\u22121)\n10:\nFscore(T ) := P\ns\u2208T os\n11:\nGi \u2190 Si\u22121 \u222a {i}\n12:\nu \u2190 arg max\nv\u2208Gi\nFscore(Si\u22121 \u222a {i}\\{v}}\n13:\nSi \u2190 (Si\u22121 \u222a {i})\\{u}\n14:\nend if\n15:\nend for\n16: end procedure\n0.1\n0.1 0.5\nChildren laughed\nand\nthe\nsunny\npark\n...\n.\nplayed\n0.2 0.1 0.1\n1\n0.9\n0.4\n0.03 0.02 0.2 0.05 0.9\n0.6\nin\n1.43\n0.65\n1.52\n0.9\n0.6\n0.03\n0.02\n0.05\n0.9\n0.1\n0.1\n0.5\n0.2\n0.1\n0.1\n1\n0.9\n0.4\n0.03 0.04 0.01 0.02 0.9\n0.6\n1.6\n0.62\n1.8\n0.9\n0.01\n0.03\n0.04\n0.02\n0.9\n0.01\n0.51\n0.4\n0.1\n0.1\n0.1\n0.5\nChildren laughed\nand\nin\nthe\nsunny\npark\n...\n.\nplayed\n0.2\n0.1\n1\n0.9\n0.6\n0.2\n0.1\n0.1\n0.6\n1.4\n0.6\n1.5\n0.5\n0.4\n0.1\n0.4\n0.1\nQuery\nKey\nValue\nDecoding Step 4\nValue\nKey\n0.03\nQuery\nDecoding Step 5\n0.6\n0.02\nEviction w. Global Statistic \n(infeasible) \nFigure 3: Illustration of Algorithm 1 during two\nconsecutive decoding steps.\nFigure 3 presents an illustrative example of our H2 Eviction Algorithm. We assume that the budget\nsize of KV cache is 3. Following the completion of the fourth decoding step, the KV embeddings\nassociated with the third token are evicted based on the accumulated attention score. Consequently,\nthese evicted KV embeddings become inaccessible in the subsequent decoding steps.\n4.2\nTheoretical Guarantee and System Implementation\nWe state a theoretical result as follows. The proofs and more details are provided in Appendix D.\nTheorem 4.4 (informal). Under the mild assumption, let k denote the budget of space limitation. If\nfor each token, we greedily compute the attention score based on top-k choice, then we can show\nthe set eSi we generate each for token i satisfy that f(eSi) \u2265 (1 \u2212 \u03b1)(1 \u2212 1/e) max|S|=k f(S) \u2212 \u03b2,\nwhere \u03b1, \u03b2 > 0 are parameters.\nRemark 4.5. We remark the above theorem provides a theoretical explanation of why can we hope\nour greedy algorithm (with cache limitation) can provide a good solution to the problem.\nImplementation Details.\nWe provide a general framework that can support any KV cache eviction\nalgorithm and enhance throughput and reduce the latency of LLM generation with careful implemen-\ntation. For example, to ensure I/O efficiency, we do not swap memory when stored KV is evicted, but\ndirectly fill with newly-added KV. More details are included in Appendix A.\n6\nCOPA, OPT-30B\nMathQA, OPT-30B\nRTE, OPT-66B\nOpenBookQA, OPT-66B\nXSUM, LLaMA-7B\nXSUM, LLaMA-13B\nXSUM, LLaMA-30B\nXSUM, GPT-NeoX-20B\nCNN/Daily Mail, LLaMA-7B\nCNN/Daily Mail, GPT-NeoX-20B\nOpenBookQA, OPT-30B\nCOPA, OPT-66B\nFigure 4: Comparsion results between the baseline model with full cache, our H2O, and the \"Local\" strategy\nthat utilizes the most recent KV embeddings.\n5\nEmpirical Evaluation\nIn this section, our goal is to demonstrate that H2O, a remarkably simple KV cache eviction policy is\ncapable of enhancing end-to-end throughput and reducing latency in wall-clock while maintaining\ngeneration quality across a broad spectrum of domains and tasks.\n\u2022 In Section 5.1, we show that H2O can reduce the memory footprint of KV cache by up to 5\u00d7 without\naccuracy degradation on a wide range of model architectures (OPT, LLaMA, GPT-NeoX), sizes\n(from 6.7B to 175B) and evaluation benchmarks (HELM and lm-eval-harness). More importantly,\ncan enhance the performance of existing KV cache sparsification techniques.\n\u2022 In Section 5.2, we demonstrate that H2O can increase the inference throughput by up to 3\u00d7, 29\u00d7,\n29\u00d7 compared to the state-of-the-art inference engine FlexGen, DeepSpeed and the widely used\nHugging Face Accelerate without compromising model quality.\n\u2022 In Section 5.3, we present extensive ablation studies to show the effectiveness of H2O under\ndifferent sequence lengths, especially the input with infinite sequence length and its compatibility\nwith quantization.\nAll details (hyperparameters, data splits, etc.), along with additional experiments, are in Appendix A.\n5.1\nEnd-to-End Results\nWe demonstrate that H2O can reduce KV cache memory footprint by 5-10\u00d7 while achieving compa-\nrable accuracy on a majority of tasks.\nSetup.\nOur experiments are based on three representative model families of LLMs, including the\nOPT [39] with model sizes, LLaMA [40], and GPT-NeoX-20B [41]. We sample eight tasks from two\npopular evaluation frameworks (HELM [16] and lm-eval-harness [15]): COPA [42], MathQA [43],\nOpenBookQA [44], PiQA [45], RTE [46], Winogrande [47], XSUM [48], CNN/Daily Mail [49]. Also,\nwe evaluate our approach on recent generation benchmarks, AlpaceEval [50] and MT-bench [51],\nand the details are included in Appendix. We use NVIDIA A100 80GB GPU.\nBaselines.\nSince H2O evenly assigns the caching budget to H2 and the most recent KV, except for\nfull KV cache, we consider the \"Local\" strategy as a baseline method. In addition, we also provide\ntwo different variants of Sparse Transformers (strided and fixed) as strong baselines. Also, the full\n7\nKV cache with fewer shots (0/1-shot) prompts are considered as the baseline, which has a similar\nsequence length of the 5-shot tasks with 20% KV cache budget.\nTable 1: Quantatively comparison between H2O with\nFull methods of different number of shots.\nMethods\nPiQA\nCOPA\nOpenbookQA\nWinogrande\nFull\n80.09\n81.00\n44.80\n71.51\n0-shot Full\n78.89\n76.00\n41.40\n70.00\n1-shot Full\n79.11\n76.00\n43.60\n70.24\nLocal\n57.94\n56.00\n28.40\n51.30\nH2O\n79.22\n85.00\n43.80\n71.67\nMain Results.\nWe evaluate LLMs with KV\ncache budget ranging from 4% to 100% on 5-\nshot downstream tasks. Results are summarized\nin Figure 4 and Table 1& 2. The following ob-\nservations can be drawn: (1) With different KV\ncache budgets, our H2O demonstrates consistent\nand significant improvements against the \"Lo-\ncal\" strategy across various model sizes, model\ntypes, and downstream tasks. We can draw sim-\nilar conclusions comparing H2O with other baselines like Sparse Transformer; (2) Meanwhile, with\nless than 20% KV cache budget(i.e., more than 5\u00d7 memory reduction), H2O achieves comparable\nperformance as the model with full KV embeddings; (3) H2O with 20% KV cache budget approxi-\nmately uses 1.2 samples per input and show consistent improvement over zero-shot and one-shot full\nmodel that use 1 and 2 samples, respectively. (4) Our H2O shows consistent effectiveness in the more\nchallenging long sequence generation tasks, XSUM, and CNN/Daily Mail.\nTable 2: Results of different sparsification methods w. or w.o. H2.\nExperiments are conducted with OPT-30B with 20% KV cache budget.\nModels\nCOPA\nOpenBookQA\nPiQA\nWinogrande\nFull\n85.00\n43.20\n78.51\n70.24\nLocal w.o. H2\n48.00\n25.20\n55.82\n49.17\nLocal w. H2\n84.00\n43.00\n78.45\n69.06\nSparse Transformer (strided) w.o. H2\n50.00\n24.60\n56.20\n47.59\nSparse Transformer (strided) w. H2\n83.00\n42.60\n78.24\n69.61\nSparse Transformer (fixed) w.o. H2\n61.00\n23.80\n58.60\n49.88\nSparse Transformer (fixed) w. H2\n76.00\n41.40\n77.80\n64.96\nAnalysis.\nSince the evicted KV\nwill not be seen in the future\nsteps, dropping certain critical\nKV embeddings can cause a se-\nvere functional collapse, result-\ning in significant performance\ndegradation, e.g., in {LLaMA-\n13B, XSUM} {LLaMA-7B, CN-\nN/Daily Mail}, the \"Local\" strat-\negy collapses at 60% budgets\nwhile our H2O can still match\nthe full cache performance with\n20% budgets. In some tasks, our methods even surpass the baseline models, which demonstrates\na regularization effect of our H2O. For example, in {OPT-66B, RTE}, {OPT-30B, MathQA} and\n{GPT-NeoX-20B, XSUM}, our H2O achieves an extra performance improvement of 0.73%, 0.64%\nand 0.18 with 20% KV cache budget, respectively. These consistent results validate the effectiveness\nof our H2O framework.\nEnhancing Baseline Techniques.\nImportantly, we observe other sparsification baselines fail under\nan extremely low cache budget while combining the most recent KV embeddings with the ones of\nheavy hitters successfully achieves comparable performance as using full KV embeddings. From\nTable 2, we can observe that both \"strided\" and \"fixed\" sparse attention fail under 20% KV cache\nbudgets, encountering a significant performance drop (up to 35% compared with the full cache).\nAfter combining with H2, both approaches reach a similar performance as using full KV embeddings.\n5.2\nHeavy Hitter for High-Throughput Generative Inference\nTable 3: Generation throughput (token/s) on a T4 GPU with different systems. In the sequence length row, we\nuse \u201c512 + 32\u201d to denote a prompt length of 512 and a generation length of 32. \u201cOOM\u201d means out-of-memory.\nThe gray text in the bracket denotes the effective batch size and the lowest level of the memory hierarchy that\nthe system needs for offloading, where \u201cC\u201d means CPU and \u201cG\u201d means GPU.\nSeq. length\n512+32\n512+512\n512+1024\nModel size\n6.7B\n30B\n6.7B\n30B\n6.7B\n30B\nAccelerate\n20.4 (2, G)\n0.6 (8, C)\n15.5 (1, G)\n0.6 (8, C)\n5.6 (16, C)\n0.6 (8, C)\nDeepSpeed\n10.2 (16, C)\n0.6 (4, C)\n9.6 (16, C)\n0.6 (4, C)\n10.1 (16, C)\n0.6 (4, C)\nFlexGen\n20.2 (2, G)\n8.1 (144, C)\n16.8 (1, G)\n8.5 (80, C)\n16.9 (1, G)\n7.1 (48, C)\nH2O (20%)\n35.1 (4, G)\n12.7 (728, C)\n51.7 (4, G)\n18.83 (416, C)\n52.1 (4, G)\n13.82 (264, C)\n8\nTable 5: Generation throughput and latency on an A100 GPU. In the sequence length row, we use \u201c7000 + 1024\u201d\nto denote a prompt length of 7000 and a generation length of 1024. \u201cOOM\u201d means out-of-memory.\nSeq. length\nModel size\nBatch size\nMetric\nFlexGen\nH2O (20%)\n7000+1024\n30B\n1\nlatency (s)\n57.0\n50.4\n5000+5000\n13B\n4\nlatency (s)\n214.2\n155.4\n2048+2048\n6.7B\n24\nlatency (s)\n99.5\n53.5\n2048+2048\n6.7B\n24\nthroughput (token/s)\n494.1\n918.9\n2048+2048\n6.7B\n64\nthroughput (token/s)\nOOM\n1161.0\nTable 4: Results of generation throughput (token/s)\non a T4 GPU with different systems on real-world\ndatasets, XSUM.\nModel size\n6.7B\n30B\nAccelerate\n11.98 (1, G)\n0.23 (2, C)\nDeepSpeed\n3.52 (6, C)\n0.31 (2, C)\nFlexGen\n10.80 (1, G)\n3.29 (44, C)\nH2O (20%)\n30.40 (1, G)\n6.70 (180, C)\nWe implement our KV cache eviction policy in a\nstate-of-the-art inference engine, FlexGen [19], and\nreport the throughput and latency improvements.\nH2O is orthogonal to existing optimizations in Flex-\nGen, such as offloading and quantization, so they can\nbe combined to achieve better performance.\nSetup\nWe conducted experiments on two GPUs:\nan NVIDIA T4 (16GB) GPU and an NVIDIA A100\n(80GB) GPU. On the T4 GPU, we evaluate the gen-\neration throughput following the settings in the Flex-\nGen paper. The evaluated models are OPT-6.7B and\nOPT-30B. When the model and KV cache do not fit into a single GPU, we turn on CPU offloading.\nThe results of both pure GPU and GPU with CPU offloading are reported. All the speedup results are\ntested in an end-to-end setting, including both the pre-filling and generation phases. And it includes\nthe time for constructing the H2O KV cache. We use synthetic datasets where all prompts are padded\nto the same length. The system is then required to generate the same number of tokens for each\nprompt. We test different combinations of prompt and generation lengths. We also test our method on\nreal-world datasets (XSUM) for further assessment. The evaluation metric is generation throughput,\nwhich is the number of generated tokens / (prompt time + decoding time). We use DeepSpeed\nZeRO-Inference [17], Hugging Face Accelerate [18], and FlexGen [19] as baselines. On the A100\nGPU, with more GPU memory, we evaluate the performance of the systems with sequence lengths up\nto 10K. Although OPT is only trained on 2K sequence length, we benchmark the throughput and\nlatency performance to show the potential of H2O for better models in the future.\nResults.\nTable 3& 4 shows the generation throughput of all systems on the T4 GPU. With our KV\ncache eviction policy, the memory usage is reduced, which brings two advantages: 1) we can use a\nmuch larger batch size; 2) we can make a setting from requiring offloading to not requiring offloading.\nAs shown in Table 3& 4, H2O with a 20% budget improves the generation throughput over FlexGen,\nDeepSpeed, and Accelerate by up to 3\u00d7, 29\u00d7, and 29\u00d7, respectively, across both synthetic and\nreal-world dataset. The results on the A100 GPU with sequence lengths from 4K to 10K are listed in\nTable 5. With the same batch size, H2O can reduce the latency by 1.1 \u2212 1.9\u00d7 compared to FlexGen.\nAdditionally, H2O saves memory so it allows a larger batch size, which brings 2.3\u00d7 improvement on\ngeneration throughput for OPT-6.7B.\n5.3\nAblation Results\nWe present extensive ablation studies of H2O on (1) infinite-length input, (2) different number of\nshots, (3) compatibility with quantization methods on KV cache, and (4) dissecting the effectiveness\nof different components. We find a surprising property of H2O \u2013 it not only improves the efficiency\nof LLMs, but also increases the diversity of the generated text.\nQ1: Can H2O empower LLMs to process infinite-length inputs?\nA1: Effective genera-\ntion with sequence length up to four million tokens.\nSome recent works [52, 53] demon-\nstrate the possibility of handling infinite-length inputs, a notable challenge in current LLMs.\nThese methods employ an attention sink that retains the first few tokens and applies position\nrolling in the KV cache, empowering LLMs to process infinite-length inputs. Inspired by this\nprogress, we further implement our H2O for infinite-length inputs. Figure 5 showcases the pos-\nitive results of H2O, i.e., H2O can empower LLMs to tackle input with length up to four mil-\n9\nlion tokens, achieving a better performance (lower perplexity) than the original StreamLLM\nmethod [52] across various cache size. Further comparisons are reported in Appendix C.4.\nFigure 5: (Upper) streaming with H2O to\nhandle inputs with sequence lengths of four\nmillion tokens. (Bottom) Perplexity compari-\nson between the original StreamLLM method\nand our H2O, results are collected on the first\ntext sample of PG-19 [54].\nQ2: Does the number of shots during inference effects\nthe effectiveness of H2O? A2: Effective across zero-shot\nto ten-shots inference. We further examine H2O under\ndifferent numbers of shots during inference, and the re-\nsults are reported in Table 10 and Figure 8. With different\nshots inference, our H2O achieves matching performance\n(difference less than 1.00%) as the full model across dif-\nferent downstream tasks. The \"Local\" strategy encounters\nsignificant performance degradation (up to 37.00%. Such\nresults demonstrate the effectiveness of our H2O under\ndifferent inference scenarios. More details about zero-shot\nand one-shot inference are reported in Appendix C.3.\nQ3: Compatible with Quatization? A3: Yes. To pur-\nsue further efficiency, we show the compatibility of H2O\nwith another orthogonal approach, i.e., quantization in Ta-\nble 6. We use OPT-30B as our base model and COPA,\nOpenBookWA, and PiQA as evaluation tasks. Intuitively\nsparsity and quantization are highly related so combin-\ning them might introduce larger errors. Surprisingly the\ncombination almost always achieves better accuracy than\nH2O or quantization alone. Experiments about throughput\nimprovement are detailed in Appendix C.2.\nQ4: When does H2O match the baseline with full KV\nembeddings? A4: With both H2 and the recent tokens. We investigate the separate effects of\nKV embeddings of H2 and the local tokens. We conduct experiments on 4 tasks with OPT-13B and\nOPT-30B. For each task, we compare the performance of three KV cache eviction policies, including\nonly the KV embeddings of H2, only the ones of local tokens, and our H2O that keep both. As\nshown in Table 9, only retaining the embeddings of H2 or local tokens can\u2019t maintain a similar\nperformance as the model using full embeddings, with a performance degradation from 2.85% to\n22.75%. Incorporating both components, our H2O successfully retains the baseline performance with\nfull embeddings. Besides, the model with only H2 shows a consistent improvement against the one\nwith only local tokens, which indicates H2 might contribute more to maintaining the performance.\nQ5: Extra benefits from H2O? A5: Increased diversity of generated text. Besides all the benefits\nof our H2O, we also observe an bonus introduced by H2O, i.e., the improved diversity of generated\ncontent. The results are reported in Appendix C.1. Given the same prompts, we visualize the\ngenerated text of the models with different KV cache budgets. Compared with the model of full KV\ncache, our H2O can generate sentences with fewer repeated words and more creativity.\n6\nConclusion and Discussion\nIn this paper, we study one of the key bottlenecks of LLM deployment, KV cache, particularly\nfor long-content and large-batch generation applications. We propose H2O, a simple KV cache\neviction policy for significantly reducing its memory footprint. The main insight of our approach\nis the recognition of a subset of tokens, known as Heavy Hitters, which contribute the most value\nwhen computing attention scores. We formulate the KV cache eviction as a dynamic submodular\nproblem and provide the theoretical guarantees for our algorithm. Through extensive evaluations,\nwe demonstrate that H2O can significantly improve end-to-end throughput and decrease latency in\nwall-clock time, without compromising the generation quality of LLMs across a variety of tasks.\n7\nAcknowledgement\nYing Sheng and Clark Barrett are partly supported by NSF-2110397 and the Stanford Center for\nAutomated Reasoning. Z. Wang is in part supported by a Google Research Scholar Award and the\nNSF AI Institute for Foundations of Machine Learning (IFML).\n10\nReferences\n[1] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for\ndialog applications. arXiv preprint arXiv:2201.08239, 2022.\n[2] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. Wordcraft: story writing with\nlarge language models. In 27th International Conference on Intelligent User Interfaces, pages\n841\u2013852, 2022.\n[3] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682, 2022.\n[4] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B\nHashimoto. Benchmarking large language models for news summarization. arXiv preprint\narXiv:2301.13848, 2023.\n[5] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm\nLevskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling\ntransformer inference. arXiv preprint arXiv:2211.05102, 2022.\n[6] Laszlo A Belady, Robert A Nelson, and Gerald S Shedler. An anomaly in space-time char-\nacteristics of certain programs running in a paging machine. Communications of the ACM,\n12(6):349\u2013353, 1969.\n[7] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\narXiv preprint arXiv:2001.04451, 2020.\n[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast\nand memory-efficient exact attention with io-awareness. Advances in Neural Information\nProcessing Systems, 35:16344\u201316359, 2022.\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509, 2019.\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\nattention with performers. arXiv preprint arXiv:2009.14794, 2020.\n[11] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers\nare rnns: Fast autoregressive transformers with linear attention. In International conference on\nmachine learning, pages 5156\u20135165. PMLR, 2020.\n[12] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\narXiv:1911.02150, 2019.\n[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[14] Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens.\narXiv preprint arXiv:2304.08467, 2023.\n[15] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds,\nEric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot\nlanguage model evaluation. In Zenodo. https://doi.org/10.5281/zenodo.5371628, September\n2021.\n[16] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of\nlanguage models. arXiv preprint arXiv:2211.09110, 2022.\n[17] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng\nLi, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, et al. Deepspeed\ninference: Enabling efficient inference of transformer models at unprecedented scale. arXiv\npreprint arXiv:2207.00032, 2022.\n11\n[18] HuggingFace. Hugging face accelerate. https://huggingface.co/docs/accelerate/\nindex.\n[19] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu,\nZhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative\ninference of large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023.\n[20] Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot.\narXiv preprint arXiv:2301.00774, 2023.\n[21] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning\napproach for large language models. arXiv preprint arXiv:2306.11695, 2023.\n[22] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pech-\nenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl):\nA missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175,\n2023.\n[23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n[24] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models. arXiv preprint\narXiv:2211.10438, 2022.\n[25] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong\nHe. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.\narXiv preprint arXiv:2206.01861, 2022.\n[26] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit\nmatrix multiplication for transformers at scale. In Advances in Neural Information Processing\nSystems, 2022.\n[27] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n[28] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.\nAwq:\nActivation-aware weight quantization for llm compression and acceleration. arXiv preprint\narXiv:2306.00978, 2023.\n[29] Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta\u00f1\u00f3n, Siddhartha Brahma, Yury\nZemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. Colt5: Faster\nlong-range transformers with conditional computation. arXiv preprint arXiv:2303.09752,\n2023.\n[30] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas\nHoffmann. Dynamic context pruning for efficient and interpretable autoregressive transformers.\narXiv preprint arXiv:2305.15805, 2023.\n[31] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.\narXiv preprint arXiv:2009.06732, 2020.\n[32] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture\nwith cascade token and head pruning. In 2021 IEEE International Symposium on High-\nPerformance Computer Architecture (HPCA), pages 97\u2013110. IEEE, 2021.\n[33] Elizabeth J O\u2019neil, Patrick E O\u2019neil, and Gerhard Weikum. The lru-k page replacement\nalgorithm for database disk buffering. Acm Sigmod Record, 22(2):297\u2013306, 1993.\n[34] Donghee Lee, Jongmoo Choi, Jong-Hun Kim, Sam H Noh, Sang Lyul Min, Yookun Cho, and\nChong Sang Kim. Lrfu: A spectrum of policies that subsumes the least recently used and least\nfrequently used policies. IEEE transactions on Computers, 50(12):1352\u20131361, 2001.\n[35] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power\nof self-attention matrices. arXiv preprint arXiv:2106.03764, 2021.\n[36] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and\nvariable creation in self-attention mechanisms. In International Conference on Machine\nLearning, pages 5793\u20135831. PMLR, 2022.\n[37] Laszlo A. Belady. A study of replacement algorithms for a virtual-storage computer. IBM\nSystems journal, 5(2):78\u2013101, 1966.\n12\n[38] Simeng Han, Xiang Lin, and Shafiq Joty. Resurrecting submodularity for neural text generation.\narXiv preprint arXiv:1911.03014, 2019.\n[39] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[41] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-\nrace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth,\nShivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-\nNeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL\nWorkshop on Challenges & Perspectives in Creating Large Language Models, 2022.\n[42] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium:\nlogical formalizations of commonsense reasoning, pages 90\u201395, 2011.\n[43] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Han-\nnaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-\nbased formalisms. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2357\u20132367, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics.\n[44] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In EMNLP, 2018.\n[45] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning\nabout physical commonsense in natural language. In Thirty-Fourth AAAI Conference on\nArtificial Intelligence, 2020.\n[46] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\n[47] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\n[48] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don\u2019t give me the details, just the\nsummary! topic-aware convolutional neural networks for extreme summarization. arXiv\npreprint arXiv:1808.08745, 2018.\n[49] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text sum-\nmarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023,\n2016.\n[50] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-\nfollowing models. https://github.com/tatsu-lab/alpaca_eval, 2023.\n[51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and\nIon Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming\nlanguage models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.\n[53] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang.\nLm-\ninfinite: Simple on-the-fly length generalization for large language models. arXiv preprint\narXiv:2308.16137, 2023.\n[54] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. In The International Conference on Learning\nRepresentations (ICLR), 2020.\n13\n[55] Song Han, Huizi Mao, and William J Dally.\nDeep compression: Compressing deep\nneural networks with pruning, trained quantization and huffman coding. arXiv preprint\narXiv:1510.00149, 2015.\n[56] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\nHartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for\nefficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2704\u20132713, 2018.\n[57] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization\nthrough weight equalization and bias correction. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1325\u20131334, 2019.\n[58] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural\nnetwork quantization without retraining using outlier channel splitting. In International\nconference on machine learning, pages 7543\u20137552. PMLR, 2019.\n[59] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convo-\nlutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440,\n2016.\n[60] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the\nvalue of network pruning. arXiv preprint arXiv:1810.05270, 2018.\n[61] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric\nmedian for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 4340\u20134349, 2019.\n[62] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in\ndeep learning: Pruning and growth for efficient inference and training in neural networks. J.\nMach. Learn. Res., 22(241):1\u2013124, 2021.\n[63] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2(7), 2015.\n[64] Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Pro-\nceedings of the IEEE/CVF international conference on computer vision, pages 4794\u20134802,\n2019.\n[65] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Dis-\ntilling task-specific knowledge from bert into simple neural networks.\narXiv preprint\narXiv:1903.12136, 2019.\n[66] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In\nInternational Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.\n[67] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n[68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in\nneural information processing systems, 32, 2019.\n[69] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert\npretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[70] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa:\nA question answering challenge targeting commonsense knowledge.\narXiv preprint\narXiv:1811.00937, 2018.\n[71] Ajay Jaiswal, Liyan Tang, Meheli Ghosh, Justin Rousseau, Yifan Peng, and Ying Ding.\nRadbert-cl: Factually-aware contrastive learning for radiology report classification. Proceed-\nings of machine learning research, 158:196\u2013208, 2021.\n[72] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and\nJimmy Lin. End-to-end open-domain question answering with bertserini. arXiv preprint\narXiv:1902.01718, 2019.\n14\n[73] Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for\nmulti-hop reading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019.\n[74] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n[75] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang,\nBing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and\nbeyond. arXiv preprint arXiv:2304.13712, 2023.\n[76] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[77] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[78] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[79] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[80] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow,\nRoman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A\n176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,\n2022.\n[81] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi,\nSanjiv Kumar, and Suvrit Sra. Why {adam} beats {sgd} for attention models, 2020.\n[82] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao,\nand Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint\narXiv:1908.03265, 2019.\n[83] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the\ndifficulty of training transformers. arXiv preprint arXiv:2004.08249, 2020.\n[84] Du\u0161an Vari\u0161 and Ond\u02c7rej Bojar. Sequence length is a domain: Length-based overfitting in\ntransformer models. arXiv preprint arXiv:2109.07276, 2021.\n[85] Wancong Zhang and Ieshan Vaidya. Mixup training leads to reduced overfitting and improved\ncalibration for the transformer architecture. arXiv preprint arXiv:2102.11402, 2021.\n[86] Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao. Very deep transformers for neural\nmachine translation. arXiv preprint arXiv:2008.07772, 2020.\n[87] Peng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit\nCheung, Simon JD Prince, and Yanshuai Cao. Optimizing deeper transformers on small\ndatasets. arXiv preprint arXiv:2012.15355, 2020.\n[88] Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. Gradinit:\nLearning to initialize neural networks for stable and efficient training. Advances in Neural\nInformation Processing Systems, 34:16410\u201316422, 2021.\n[89] Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati,\nMichal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive\ngradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.\n[90] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.\nDeepnet: Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022.\n[91] Qiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao,\nand Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. arXiv\npreprint arXiv:2208.01313, 2022.\n[92] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n15\n[93] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu.\nTexygen: A benchmarking platform for text generation models. In The 41st international\nACM SIGIR conference on research & development in information retrieval, pages 1097\u20131100,\n2018.\n[94] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws.\narXiv preprint arXiv:2212.09720, 2022.\n[95] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\nand Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\narXiv:2307.03172, 2023.\n[96] Pranjal Awasthi and Anupam Gupta. Improving length-generalization in transformers via task\nhinting. arXiv preprint arXiv:2310.00726, 2023.\n[97] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating trans-\nformers via kernel density estimation. arXiv preprint arXiv:2302.02451, 2023.\n[98] Josh Alman and Zhao Song.\nFast attention requires bounded entries.\narXiv preprint\narXiv:2302.13214, 2023.\n[99] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the perfor-\nmance edge over linear attention. arXiv preprint arXiv:2310.11685, 2023.\n[100] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic\nattention maintenance in large language models. arXiv preprint arXiv:2304.02207, 2023.\n[101] Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation. arXiv\npreprint arXiv:2305.04701, 2023.\n[102] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations\nof transformers. arXiv preprint arXiv:2306.02896, 2023.\n[103] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic attention\nsparsification algorithms for over-parameterized feature dimension. arxiv preprint: arxiv\n2304.03426, 2023.\n[104] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh regression\nproblems. arXiv preprint, 2303.15725, 2023.\n[105] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression.\narXiv preprint arXiv:2304.10411, 2023.\n[106] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers\nvia sketches for polynomial kernels. arXiv preprint arXiv:2310.01655, 2023.\n[107] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled hyperbolic functions\nregression. arXiv preprint arXiv:2305.00660, 2023.\n[108] Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir\nZandieh.\nHyperattention: Long-context attention in near-linear time.\narXiv preprint\narXiv:2310.05869, 2023.\n[109] Timothy Chu, Zhao Song, and Chiwun Yang. How to protect copyright data in optimization of\nlarge language models? arXiv preprint arXiv:2308.12247, 2023.\n[110] Ritwik Sinha, Zhao Song, and Tianyi Zhou. A mathematical abstraction for balancing the trade-\noff between creativity and reality in large language models. arXiv preprint arXiv:2306.02295,\n2023.\n[111] Yeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Reformu-\nlating single layer attention in llm based on tensor and svm trick, and solving it in matrix\nmultiplication time. arXiv preprint arXiv:2309.07418, 2023.\n[112] Gary Marcus, Ernest Davis, and Scott Aaronson. A very preliminary analysis of dall-e 2. arXiv\npreprint arXiv:2204.13807, 2022.\n[113] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for attention\ncomputation. arXiv preprint arXiv:2307.08045, 2023.\n[114] Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the\noptimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680,\n2023.\n16\n[115] Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized large\nlanguage models. arXiv preprint arXiv:2308.10502, 2023.\n[116] Yichuan Deng, Zhao Song, Shenghao Xie, and Chiwun Yang. Unmasking transformers: A\ntheoretical approach to data recovery via attention weights. arXiv preprint arXiv:2310.12462,\n2023.\n[117] Yichuan Deng, Zhihang Li, Sridhar Mahadevan, and Zhao Song. Zero-th order algorithm for\nsoftmax attention optimization. arXiv preprint arXiv:2307.08352, 2023.\n[118] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix\nsoftmax attention to kronecker computation. arXiv preprint arXiv:2310.04064, 2023.\n[119] Yichuan Deng, Zhao Song, and Shenghao Xie. Convergence of two-layer regression with\nnonlinear units. arXiv preprint arXiv:2308.08358, 2023.\n[120] Timothy Chu, Zhao Song, and Chiwun Yang. Fine-tune language models to approximate\nunbiased in-context learning. arXiv preprint arXiv:2310.03331, 2023.\n[121] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora. Trainable trans-\nformer in transformer. arXiv preprint arXiv:2307.01189, 2023.\n[122] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while\npredicting the masked word? arXiv preprint arXiv:2303.08117, 2023.\n[123] Alexander Schrijver. Combinatorial optimization: polyhedra and efficiency, volume 24.\nSpringer, 2003.\n[124] Kasper Green Larsen, Jelani Nelson, Huy L Nguyen, and Mikkel Thorup. Heavy hitters\nvia cluster-preserving clustering. In 2016 IEEE 57th Annual Symposium on Foundations of\nComputer Science (FOCS), pages 61\u201370. IEEE, 2016.\n[125] Vasileios Nakos and Zhao Song. Stronger l2/l2 compressed sensing; without iterating. In\nProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages\n289\u2013297, 2019.\n[126] Vasileios Nakos, Zhao Song, and Zhengyu Wang. (nearly) sample-optimal sparse fourier\ntransform in any dimension; ripless and filterless. In 2019 IEEE 60th Annual Symposium on\nFoundations of Computer Science (FOCS), pages 1568\u20131577. IEEE, 2019.\n[127] Andreas Krause and Carlos Guestrin. Beyond convexity: Submodularity in machine learning.\nICML Tutorials, 2008.\n[128] Jeff Bilmes. Submodularity in machine learning applications. In Twenty-Ninth Conference on\nArtificial Intelligence, AAAI-15 Tutorial Forum, 2015.\n[129] Simeng Han, Xiang Lin, and Shafiq Joty. Resurrecting submodularity for neural text generation.\narXiv preprint arXiv:1911.03014, 2019.\n[130] George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approxima-\ntions for maximizing submodular set functions\u2014i. Mathematical programming, 14(1):265\u2013\n294, 1978.\n[131] Lianke Qin, Zhao Song, and Yitan Wang. Fast submodular function maximization. arXiv\npreprint arXiv:2305.08367, 2023.\n[132] Junda Wu, Tong Yu, Rui Wang, Zhao Song, Ruiyi Zhang, Handong Zhao, Chaochao Lu, Shuai\nLi, and Ricardo Henao. Infoprompt: Information-theoretic soft prompt tuning for natural\nlanguage understanding. arXiv preprint arXiv:2306.04933, 2023.\n[133] Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning\nand weight shifting for softmax regression. arXiv preprint, 2023.\n[134] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix\nmultiplication time. In STOC, 2019.\n[135] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current\nmatrix multiplication time. In Conference on Learning Theory, pages 2140\u20132157. PMLR,\n2019.\n[136] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster\ninterior point method for semidefinite programming. In 2020 IEEE 61st annual symposium on\nfoundations of computer science (FOCS), pages 910\u2013918. IEEE, 2020.\n17\n[137] Zhao Song and Zheng Yu. Oblivious sketching-based central path method for linear pro-\ngramming. In International Conference on Machine Learning, pages 9835\u20139847. PMLR,\n2021.\n[138] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for\nsolving general lps. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory\nof Computing, pages 823\u2013832, 2021.\n[139] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving sdp faster:\nA robust ipm framework and efficient implementation. In 2022 IEEE 63rd Annual Symposium\non Foundations of Computer Science (FOCS), pages 233\u2013244. IEEE, 2022.\n[140] Yuzhou Gu and Zhao Song.\nA faster small treewidth sdp solver.\narXiv preprint\narXiv:2211.06033, 2022.\n[141] Yuzhou Gu, Zhao Song, and Lichen Zhang. A nearly-linear time algorithm for structured\nsupport vector machines. arXiv preprint arXiv:2307.07735, 2023.\n[142] Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and unified algorithm\nfor projection matrix vector multiplication with application to empirical risk minimization. In\nAISTATS, 2023.\n[143] Zhao Song, Mingquan Ye, and Lichen Zhang. Streaming semidefinite programs: O(\u221an)\npasses, small space and fast runtime. arXiv preprint arXiv:2309.05135, 2023.\n[144] Haotian Jiang, Yin Tat Lee, Zhao Song, and Lichen Zhang. Convex minimization with integer\nminima in eO(n4) time. In ACM-SIAM Symposium on Discrete Algorithms (SODA), 2024.\n[145] S Cliff Liu, Zhao Song, Hengjie Zhang, Lichen Zhang, and Tianyi Zhou. Space-efficient\ninterior point method, with applications to linear programming and maximum weight bipartite\nmatching. In ICALP. arXiv preprint arXiv:2009.06106, 2020.\n18\nAppendix\nTable of Contents\nA More Implementation Details\n20\nB\nExtended Related Works, Discussions, and Limitations\n21\nB.1\nExtended Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nB.2\nDiscussions and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC Extended Experiments\n23\nC.1\nIncreased Diversity of Generated Text . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.2\nThroughput Improvement of H2O Combining with Quantization . . . . . . . . .\n24\nC.3\nEffectiveness on Zero-shot and One-shot Inference . . . . . . . . . . . . . . . .\n25\nC.4\nComparison with StreamingLLM . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nC.5\nEnhancing the \"Top-K\" Baseline\n. . . . . . . . . . . . . . . . . . . . . . . . .\n26\nC.6\nSeparate Effects of Heavy-Hitter and Local Tokens . . . . . . . . . . . . . . . .\n26\nC.7\nPerformance of Inference with Different Number of Shots. . . . . . . . . . . . .\n26\nC.8\nHeavy-Hitter in Attention Blocks . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nC.9\nComparsion with SpAtten . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nC.10 Heavy-Hitter in MLP Blocks\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nD Theoretical Analysis\n32\nD.1\nNotations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nD.2\nSubmodular . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nD.3\nDynamic Submodular . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nD.4\nStatic Attention\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nD.5\nRecursive Attention Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nD.6\nEviction Policy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nD.7\nExplaining Submodular Diminishing Return Property in Attention Scheme\n. . .\n37\nD.8\nSubmodular: High Level Ideas\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nD.9\nRobust Greedy with Error Propagation\n. . . . . . . . . . . . . . . . . . . . . .\n38\nD.10 Robust Submodular and Adding Items\n. . . . . . . . . . . . . . . . . . . . . .\n39\nD.11 Universal Conditions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nD.12 Induction Lemma for Exact Function . . . . . . . . . . . . . . . . . . . . . . .\n40\nD.13 Induction Lemma for Approximate Function . . . . . . . . . . . . . . . . . . .\n41\nD.14 Theoretical Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nD.15 Extended Related Work for Theoretical Attention Problems\n. . . . . . . . . . .\n43\nD.16 Sparsity Preserving\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nD.17 Definition of Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nD.18 Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\nD.19 Hessian\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nD.20 Hessian is Positive Definite . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nD.21 Hessian is Lipschitz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nD.22 Greedy Type Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n19\nA\nMore Implementation Details\nIn this section, our goal is to provide the details of system implementation (mentioned in Section 4.2)\nand experiment settings (mentioned in Section 5), as well as the pseudocode.\nSystem Details.\nWe implement H2O on top of FlexGen. FlexGen is a white-box implementation of\nOPT models, and we have done some surgery on handling the KV cache. Specifically, for the given\nparameter K, we always maintain a list of KV cache with the first K entries as heavy hitters, and the\nlast K entries as most recent tokens. In order to avoid data movement in memory, the memory for\nKV cache is preallocated. We use a circular queue to update the last K entries efficiently.\nExperiment Details.\nOur study involves the evaluation with varying sizes of KV cache that encom-\npass 4%, 10%, 20%, and 60% of the prompt\u2019s length. We select two tasks (XSUM and CNN/Daily\nMail) from the HELM framework [16] and present the performance based on 1000 test samples.\nAdditionally, we employ the lm-eval-harness framework [15] for six other tasks (COPA, MathQA,\nOpenBookQA, PiQA, RTE, and Winogrande). For the tasks derived from the HELM framework, we\nreport the performance for zero-shot inference, while for the tasks from lm-eval-harness, we default\nto conduct five-shot inference.\nPseudocode.\nWe show a simplified pseudocode below to demonstrate our implementation skele-\nton. The function generation_loop() is the base loop in FlexGen that controls prefetch and\noverlap the I/O streams and computation.\nThen in function compute_layer(), the function\nattention_forward() will be called for attention layers. During prefill iterations, the function\ncompute_attention() will return K heavy hitters and K recent tokens if the prompt has a length\ngreater than or equal to 2K, otherwise return all the KV cache. The function compute_attention()\nwill return new KV cache and the indices for evicted entries during decoding iterations, which would\nbe the place to implement a customized eviction strategy. (If the current number of stored KV cache is\nless than 2K, the eviction will be ignored.) During each decoding iteration, the oldest one among the\nlast K tokens (if we have no less than K tokens\u2019 KV cache stored) will be removed from the reserved\nlast K entries, one heavy hitter will be evicted for each head, and the newest token will be added to\nthe KV cache with a position in the last K entries. This happens in the function store_cache().\ndef\ngeneration_loop (...):\n# Prologue\n...\n# Generate\nfor i in range(gen_len):\nfor j in range(num_layers):\nfor k in range( num_gpu_batches ):\nload_weight(i, j+1, k)\nload_cache(i, j, k+1)\nstore_hidden(i, j, k-1)\nload_hidden(i, j, k+1)\ncompute_layer(i, j, k)\nstore_cache(i, j, k-1)\nsync ()\n# Epilogue\n...\n# h is the hidden\nstates (activations)\ndef\nattention_forward (h, ...):\n# the read/write\nbuffer are\nintermediate\nstops for\nprefetching\nif prefill:\nh, new_k_cache , new_v_cache = compute_attention (h, ...)\n# select K heavy\nhitters\nand K recent\ntokens\nnew_k_cache , new_v_cache = select(new_k_cache , new_v_cache , K)\ncache_write_buffer .store(new_k_cache , new_v_cache)\nelse:\nk_cache , v_cache = cache_read_buf .pop()\n# evict_ids\ntrack the\nentries\nthat will be evicted\nh, new_k_cache , new_v_cache , evict_ids =\ncompute_attention (h, k_cache , v_cache , ...)\n20\ncache_write_buffer .store(new_k_cache , new_v_cache , evict_ids)\nreturn h\ndef\nstore_cache (...):\nif prefill:\n# store\ncache\ndirectly\n...\nelse:\nk_new , v_new , evict_ids = cache_write_buffer .pop()\n# circular\nqueue for the last K entries\n# extract\nthe index for the oldest\ntoken at i-th iteration\noldest = ((i - 1) % K) - K\n# update the KV cache (k_home and v_home)\ncache_replace(k_home , evict_ids , k_new , K, oldest)\ncache_replace(v_home , evict_ids , v_new , K, oldest)\nB\nExtended Related Works, Discussions, and Limitations\nThe goal of this section is to first introduce more background and related works for Section 2,\nthen describe some previous attempts in our experiments as well as discuss the social impact and\nlimitations of this work.\nB.1\nExtended Related Works\nQuantization, Pruning, Distillation for Inference.\nPreviously, model compression algorithms\nhave been extensively investigated as a viable approach for mitigating the computational resource\nrequirements of model inference. These algorithms can be broadly categorized into three groups:\n(1) quantization [55, 56, 57, 58], which involves mapping model parameters or activations from\nhigh-precision data types to low-precision counterparts, such as using 8-bit integers instead of the\ncommonly employed 32-bit floating point format; (2) pruning or sparsity [59, 60, 61, 62], which aims\nto eliminate unnecessary neurons or weights within the models; (3) and distillation [63, 64, 65, 66]\nwhere predictions from larger models are utilized as supervised information to train smaller models.\nTransformer in NLP.\nTransformers [67] as a popular option have been frequently adopted by\nplenty of natural language processing (NLP) applications with prevailing successes [68, 69, 70, 71, 72,\n46, 73, 13, 74, 75]. Roughly, modern transformer-based networks can be categorized into two groups:\n(1) Encoder-Decoder or Encoder-only (i.e., BERT-style models [76]). This type of transformers\ncommonly leverages the Masked Language Modeling task which encourages models to capture\nthe intrinsic relationship between words and their context. Notable examples include BERT [76],\nRoBBERTa [69] and T5 [77]. (2) Decoder-only (i.e., GPT-style models [78]). Usually, this group of\ntransformers adopts the Casual Language Modeling task, which is optimized to generate the next\nword/token in a sequence based on the preceding words/tokens. Such an autoregressive manner\nis highly preferred by downstream tasks like text generation and question answering. GPT-3 [79],\nOPT [39], PaLM [13], and BLOOM [80] are representative architectures within this huge family.\nTraining of Transformer.\nTraining a gigantic transformer-based model is not trivial. It notoriously\nsuffers from various issues such as overfitting, instability, etc. [81, 82, 83] analyze these bottlenecks\nfrom the optimization perspective. To address the issues, a great amount of pioneering effort is\ndevoted, including data augmentations [84, 85], a better initialization [86, 83, 87, 88], customized\noptimizers [89], improved normalization [90, 91], weight decay [92], and early stopping. However,\nthere is still a long way to go before we can fully clarify the mystery of transformer training.\nB.2\nDiscussions and Limitations\nPrevious Attempts.\nDuring our experiments, we find several noteworthy observations. In H2O,\nemploying the accumulated attention score to evict KV embeddings can lead to a potential bias\nfavoring the least recent tokens. This bias arises because most previous tokens have a higher number\nof attention scores, resulting in a higher accumulated attention score and, consequently, a greater\nlikelihood of being retained. To address this concern, we conducted an additional experiment utilizing\n21\nthe averaged attention score to determine which KV embeddings should be retained. However, this\nalternative approach resulted in performance degradation. Additionally, we observed a significant\nproportion of H2 occurrences at the beginning of sentences. This finding suggests that the initial\ntokens play a substantial role in subsequent generation tasks.\nSocial Impact.\nOur work represents an initial effort in designing a KV Cache policy, a realm that\nhas been relatively unexplored and yet is a significant bottleneck in LLMs. The proposed Heavy\nHitter Oracle (H2O) provide a solution to improve the efficiency of LLM generation, which can save\nenergy cost and contribute to green AI. Besides, our approach also serves as a source of inspiration\nfor future advanced algorithm designs. We envision H2O as a foundational framework that could\nfacilitate further innovation in this area. Moreover, long content generation is an area of growing\nimportance that currently grapples with several efficiency issues. We hope our work that supports the\ngeneration of very long sequences will support further research in this direction, particularly in terms\nof enhancing consistency, devising superior evaluation methods, and establishing robust benchmarks.\nFurthermore, another contribution of this study is the formulation of a dynamic submodular frame-\nwork. We believe that this theoretical framework possesses the potential to be applicable beyond\nspecific domains of interest. For instance, there may exist numerous other dynamic problems where\nthe task involves solving a submodular problem with slight variations at each time.\nLimitations.\nFurthermore, despite the notable advancements in throughput of our H2O, implement-\ning LLMs for generative inference remains challenging due to the immense parameter count. As a\nsubstantial portion of these parameters is encompassed within the MLP blocks, building upon our\nobservations of H2 occurrences in the MLP blocks, future research efforts can be directed towards\nleveraging the characteristics of H2 to devise an offloading policy. Such a policy can potentially\nenhance the efficiency of LLM inference even further.\n22\nC\nExtended Experiments\nIn this section, our goal is to demonstrate that H2O can improve the diversity of generated text\n(mentioned in Section 5.3), throughput is further improved when combined with quantization (men-\ntioned in Section 5.3), and superior performance to handle extremely long length inputs (up to four\nmiddle tokens, mentioned in Section 5.3). Moreover, additional investigations about H2 are reported,\nincluding experiments under zero-shot/one-shot inference regime; H2O can also enhance the \"Top-K\"\nbaseline; extra results of H2 in attention blocks; the emergence of H2 in MLP blocks as well as its\nproperties.\nC.1\nIncreased Diversity of Generated Text\nModel Input\nIn the year 2087, humanity has achieved remarkable technological advancements and established colonies on\nmultiple planets within the Milky Way galaxy. Interstellar travel has become commonplace, with faster-than-light\nspacecraft enabling people to explore distant star systems. Earth has undergone significant changes due to\nsustainable development efforts, such as harnessing renewable energy sources and implementing widespread\necological restoration projects. However, alongside these triumphs, new challenges have emerged, including the\nrise of artificial intelligence, ethical dilemmas surrounding genetic engineering, and interplanetary political\ntensions. Against this backdrop, a team of intrepid scientists embarks on a mission to uncover the secrets of an\nancient alien civilization, hidden deep within an uncharted exoplanet. As they navigate treacherous terrains and\nencounter otherworldly phenomena, they must confront their own fears and reconcile humanity's thirst for\nknowledge with the potential consequences of uncovering secrets that were better left buried. The fate of both\ntheir mission and the future of humanity hang in the balance.\nOPT-6.7B Full Cache  \nOutput\nOPT-6.7B Local 20% Cache \nOutput\nOPT-6.7B             20% Cache \nOutput\nThe game is set in the year 2087, and is a first-person exploration game. The player takes on the role of a scientist\nwho has been sent to a distant planet to investigate a mysterious signal. The planet is inhabited by a race of\nintelligent beings known as the \"Titans,\" who have been\n........................................,,,,,,,,,,,,,,,,,,,,,,,\nThe game is set in 2087, a few years after the events of the first game. The player takes the role of a team of\nscientists who are sent to an uncharted planet to investigate a mysterious signal. The team is composed of a pilot,\na scientist, a mechanic, and a technician.\nFigure 6: Visualization of one generation example with OPT-6.7B. Results are compared between the baseline\nmodel with full cache, our H2O, and the \"Local\" strategy that utilizes the most recent KV embeddings.\nGiven the same prompt text, we visualize the generated text with OPT-6.7B and LLaMA-7B across\ndifferent methods, including the baseline model with full cache, our H2O, and the \"Local\" strategy.\nResults are reported in Figure 6 and 7. Even with less KV cache budget, our H2O can generate\nmore diverse content. Specifically, with the OPT-6.7B, the full model generates some redundant\nworks, like \"a few years after the events of the first game\" while our H2O describes \"the game is\na first-person exploration game\". As a comparison, when all KV cache budget is assigned to the\nmost recent tokens, the model fails to generate meaningful text and only repeats the word \".\" and\n\",\". Similar observations can also be drawn from the results of LLaMA-7B, in which the full model\nrepeatedly says \"so moving that\", \"so moved that\", and \"began to cry\" while our H2O describes both\nthe people and the environment.\nMoreover, We conducted a quantitative comparison via diversity metric(Self-BELU [93]). We\nrandomly sample 100 instances from the XSUM dataset, as prompts. Subsequently, we employed the\nLLaMa-7B to generate text of equal length. The full model reaches a Self-BELU score of 0.0057\nwhile H2O and the local method achieve 0.0051 and 0.0436, respectively. The comparatively lower\nSelf-BELU score of H2O indicates the slightly increased diversity of generated text.\n23\nModel Input\nLLaMA-7B Full Cache  \nOutput\nLLaMA-7B Local 20% Cache \nOutput\nLLaMA-7B             20% Cache \nOutput\nIn a small, bustling cafe nestled in the heart of a vibrant city, a serendipitous event unfolded, leaving a lasting\nimpression on all who witnessed it. As the patrons sat sipping their coffees and engaging in animated conversations,\na talented street musician entered the cafe, carrying a weathered guitar and radiating an aura of creativity.\nHe began to play, and the patrons were captivated. The musician\u2019s performance was so moving that the patrons\nbegan to applaud, and the musician was so moved that he began to cry. The patrons were so moved that they\nbegan to cry, and the musician was so\nHe began to play, and the room was filled with the sound of his music. The patrons of the cafe were enthralled\nby the music, and the atmosphere was electric. The cafe was packed with people, all of whom were enjoying\nthe music. The musician was a young\nHe ( ( ( ( ( ( ( ( ( ( ( ( ( (,  [)), 1999, 2000, 2001, 1, and, and, and, and, and, and, and, and, and, and, and,\nFigure 7: Visualization of one generation example with LLaMA-7B. Results are compared between the baseline\nmodel with full cache, our H2O, and the \"Local\" strategy that utilizes the most recent KV embeddings.\nTable 6: Compatibility with Quantization.\nModels\nCOPA\nOpenBookQA\nPiQA\nFull\n85.00\n43.20\n78.51\nH2O\n84.00\n43.00\n78.45\nQuant-4bit\n84.00\n43.28\n78.67\nH2O w. Quant-4bit\n84.00\n43.20\n78.80\nC.2\nThroughput Improvement of H2O Combining with Quantization\nTable 6 demonstrates H2O are capable of quantization and perverse the full model\u2019s performance\nwith even 4-bit quantization. To further explore the compatibility of our H2O with quantization\nwith respect to throughput improvement, we conduct an additional evaluation with the quantization\nmethod implemented in FlexGen (Note that [94] employed a different 4-bit quantization method).\nThe corresponding results are presented in Table 7. Notably, for OPT-6.7B, we observed extra\nperformance enhancements in H2O when utilizing quantization compared to the vanilla version.\nThis improvement results from the GPU memory freed by weight quantization, which allows for a\nsignificant increase in batch size. However, it should be emphasized that the quantization method\nemployed in FlexGen is not implemented most efficiently, resulting in considerable computational\noverhead. Despite the batch size being enlarged by 20 times, the actual throughput improvement is\nless than 2 times. Nevertheless, it is important to acknowledge the potential benefits of combining\nH2O with quantization, as exemplified by the ability to increase the batch size further. For instance,\nthe implementation of 4-bit quantization could be accelerated by an optimized CUDA kernel.\nTable 7: Generation throughput (token/s) with different systems without offloading. We use H2O-c denotes the\nH2O with 4-bit weights compression. In the sequence length row, we use \u201c512 + 32\u201d to denote a prompt length\nof 512 and a generation length of 32. \u201cOOM\u201d means out-of-memory. The gray text in the bracket denotes the\nbatch size. We run OPT-6.7B on a single T4 GPU.\nSeq. length\n512+32\n512+512\n512+1024\nModel size\n6.7B\n6.7B\n6.7B\nAccelerate\n20.4 (2)\n15.5 (1)\nOOM\nDeepSpeed\nOOM\nOOM\nOOM\nFlexGen\n20.2 (2)\n16.8 (1)\n16.9 (1)\nH2O (20%)\n35.1 (4)\n51.7 (4)\n52.1 (4)\nH2O-c (20%)\n50.5 (70)\n72.5 (52)\n62.3 (44)\n24\nC.3\nEffectiveness on Zero-shot and One-shot Inference\nWhen facing zero-shot and one-shot inference tasks, H2O can successfully mitigate the memory\nrequirements of the KV cache by up to 5\u00d7 under both one/zero-shot inference across different tasks,\nachieving matching performance as the model with full KV cache while the local method fails.\nHowever, unlike 5-shot, we also found that some tasks are more difficult, and the model requires\nmore information to generate the correct content, resulting in a higher KV cache budget (30-40%).\nThis might be expected since 0/1 shots in our benchmarks have shorter sequence lengths ranging\nfrom 100 to 300.\nPiQA, LLaMA-7B\nCOPA, LLaMA-7B\nOpenBookQA, LLaMA-7B\nWinogrande, LLaMA-7B\n1-shot\n0-shot\nMathQA, LLaMA-7B\nXSUM, LLaMA-7B\n1-shot\n0-shot\nFigure 8: Comparsion results on zero-shot and one-shot inference.\nC.4\nComparison with StreamingLLM\nRecent works, such as StreamLLM [52] and LM-Infinite [53], have shown promising potential in\nenabling Language Models (LLMs) to handle input of infinite length. They achieve this by only\nretaining the initial tokens and a limited local context. However, this approach may pose challenges\nfor certain tasks where vital information lies within the middle of the input and would be lost using\nthis strategy. We investigate it through two specific types of tasks:\n25\nMulti-document question answering [95]: In this task, each test sample comprises ten documents,\nfollowed by a question. The key information to answer this question is stored in one of the documents.\nWe rearranged the crucial document\u2019s position and found that the eviction strategy in StreamLLM or\nLM-Infinite can not perform well when the key document has been dropped.\nText Summarization Task (XSUM and CNN-DailyMail): Text summarization tasks require models\nto generate concise summaries of lengthy texts. Effective summarization demands a comprehensive\nunderstanding of the entire document, making it challenging when crucial information is dispersed\nthroughout the input. In particular, summarization often relies on long-context attention, and critical\ninformation may not be effectively captured within the limited local tokens.\nThe results are reported in Figure 9, illustrating a consistent decline in the performance of StreamLLM.\nSince StreamLLM will always maintain the first few tokens as well as the local tokens, regardless\nof various input content, such a strategy will inevitably result in the loss of crucial information and\nsubsequently lead to a decrease in performance. In contrast, our H2O delivers markedly superior\nperformance. Also, with H2O, The model can successfully stream to four million tokens.\nFigure 9: Comparison results of StreamLLM [52] and our H2O on generization tasks. The number in each\nmethod represents the KV Cache budget of the start/heavy-hitter tokens and the local tokens, respectively. For\nexample, H2O-256-256 means maintaining 256 Heavy-Hitters and 256 local tokens.\nC.5\nEnhancing the \"Top-K\" Baseline\nWe find H2 can further enhance another strong baseline with a \"Top-K\" strategy. The results are\nreported in Table 8. After combing with H2, the \"Top-K\" method achieves an extra improvement with\nup to 2.00% accuracy across 4 different tasks.\nTable 8: Results of the \"Top-K\" method w. or w.o. H2. Experiments are conducted with OPT-30B with 20% KV\ncache budget.\nModels\nCOPA\nOpenBookQA\nPiQA\nWinogrande\nFull\n85.00\n43.20\n78.51\n70.24\nTopK w.o. H2\n80.00\n41.40\n76.96\n65.35\nTopK w. H2\n82.00\n42.80\n77.96\n66.48\nC.6\nSeparate Effects of Heavy-Hitter and Local Tokens\nTable 9 demonstrates the separate effects of Heavy-Hitters and the local tokens. And we can observe\nHeavy-Hitters contribute more to maintaining the performance of models.\nC.7\nPerformance of Inference with Different Number of Shots.\nTable 10 demonstrates our H2O achieves consistent improvements across different number of shots\nduring inference and maintains the full model\u2019s performance with 20% memory budget.\n26\nTable 9: Ablation study of H2O across different tasks.\nTasks\nModels\nFull\nw. Local\nw. H2\nw. Local + H2\nPiQA\nOPT-13B\n77.37\n54.62\n76.12\n77.26\nOPT-30B\n78.51\n55.82\n67.25\n78.45\nOpenBookQA\nOPT-13B\n41.40\n25.60\n30.40\n41.20\nOPT-30B\n43.20\n25.20\n26.60\n43.00\nMathQA\nOPT-13B\n26.67\n22.04\n23.82\n26.93\nOPT-30B\n26.23\n20.87\n21.98\n26.87\nWinogrande\nOPT-13B\n68.59\n49.96\n51.85\n67.32\nOPT-30B\n70.24\n49.17\n47.36\n69.06\nTable 10: Results under different sequence length of OPT-30B with 20% KV cache budget.\nTasks\nMethods\n5-shots\n10-shots\nOPT-30B\nOPT-66B\nOPT-30B\nOPT-66B\nOpenBookQA\nFull\n43.20\n44.40\n43.00\n44.80\nLocal\n25.20\n30.60\n26.60\n38.80\nH2O\n43.00\n44.20\n42.80\n44.80\nCOPA\nFull\n85.00\n83.00\n86.00\n85.00\nLocal\n48.00\n59.00\n60.00\n76.00\nH2O\n84.00\n82.00\n85.00\n86.00\nMathQA\nFull\n26.23\n27.87\n26.67\n27.00\nLocal\n20.87\n25.49\n21.11\n23.08\nH2O\n26.87\n27.67\n26.47\n27.30\nC.8\nHeavy-Hitter in Attention Blocks\nThe distribution of accumulated attention scores of all the tokens within attentions blocks is illustrated\nin Figure 10. We can observe that H2 broadly exists in each layer.\nC.9\nComparsion with SpAtten\nCompared with SpAtten [32], the main differences of H2O are i) They accumulate attention scores\nacross attention heads and layers, while in our algorithm, each token can be kept or evicted indepen-\ndently across heads and layers, providing more flexibility for selecting critical KV embeddings; ii)\nWe also use KV of the most recent tokens during generation and demonstrate that such H2 tokens\ncan effectively enhance other sparse attention strategies; iii) we formulate the KV cache eviction as a\ndynamic submodular problem and prove a theoretical guarantee (under mild assumptions) for our\nnovel algorithms. Moreover, we also provide a quantitative comparison with SpAtten, and the results\nare reported in Table 11.\nTable 11: Comparison between SpAtten [32] and H2O across various tasks with OPT-30B.\nModels\nCOPA\nOpenBookQA\nPiQA\nFull\n85.00\n43.20\n78.51\nSpAtten\n82.00\n41.90\n77.06\nH2O\n84.00\n43.00\n78.45\nC.10\nHeavy-Hitter in MLP Blocks\nBesides the attention blocks, the presence of Heavy-Hitters (H2) is observed within the MLP blocks\nof LLMs. We utilize the Wiki-Text-103 dataset as the input and record the activated frequency of\nneurons in the hidden layer of MLP blocks. As depicted in Figure 11, the activated frequency of\nneurons follows a power-law distribution, wherein a small number of neurons are activated by nearly\nall input tokens (with a 100% frequency) while the majority of other neurons are rarely activated.\nSubsequently, a thorough examination of various characteristics pertaining to H2 in MLP blocks is\nconducted, encompassing the following aspects: (1) The elimination of H2 leads to a substantial\ndecline in performance, although such degradation can be easily recovered even with a mere 1%\nof the training data; (2) H2 exhibits a significant degree of overlap across different type of input\ncontent; (3) The emergence of H2 occurs early in the training process, thus exhibiting an \"early-bird\"\ncharacteristic, and their positions undergo gradual changes during subsequent training phases.\n27\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAccumulated Attention Scores\n1e5\nLayer 1\n0\n10000 20000 30000 40000 50000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e6\nLayer 2\n0\n10000 20000 30000 40000 50000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1e6\nLayer 3\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n1e6\nLayer 4\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAccumulated Attention Scores\n1e6\nLayer 5\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n1e6\nLayer 6\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n1e6\nLayer 7\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n1e6\nLayer 8\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nAccumulated Attention Scores\n1e6\nLayer 9\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n1e6\nLayer 10\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\n1e6\nLayer 11\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\n1e6\nLayer 12\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nAccumulated Attention Scores\n1e6\nLayer 13\n0\n10000 20000 30000 40000 50000\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n1e6\nLayer 14\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\n1e6\nLayer 15\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\n1e6\nLayer 16\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\nAccumulated Attention Scores\n1e6\nLayer 17\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\n1e6\nLayer 18\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\n1e6\nLayer 19\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\n5\n1e6\nLayer 20\n0\n10000 20000 30000 40000 50000\n0\n1\n2\n3\n4\n5\nAccumulated Attention Scores\n1e6\nLayer 21\n0\n10000 20000 30000 40000 50000\nWord Index\n0\n1\n2\n3\n4\n5\n1e6\nLayer 22\n0\n10000 20000 30000 40000 50000\nWord Index\n0\n1\n2\n3\n4\n5\n1e6\nLayer 23\n0\n10000 20000 30000 40000 50000\nWord Index\n0\n1\n2\n3\n4\n1e6\nLayer 24\nFigure 10: The distribution of accumulated attention scores with respect to the corresponding word. The x-axis\nrepresents the word index in the vocabulary, and the y-axis represents the accumulated attention score. Results\nare obtained from OPT-1.3B.\n28\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 1\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 2\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 3\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 4\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 5\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 6\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 7\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 8\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\nActivated Frequency (%)\nLayer 9\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\nLayer 10\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 11\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 12\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 13\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 14\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 15\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 16\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 17\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 18\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 19\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 20\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 21\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 22\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 23\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 24\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\nActivated Frequency (%)\nLayer 25\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 26\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 27\n0\n2500 5000 7500100001250015000\n0\n20\n40\n60\n80\n100\nLayer 28\n0\n2500 5000 7500100001250015000\nNeuron Index\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 29\n0\n2500 5000 7500100001250015000\nNeuron Index\n0\n20\n40\n60\n80\n100\nLayer 30\n0\n2500 5000 7500100001250015000\nNeuron Index\n0\n20\n40\n60\n80\n100\nLayer 31\n0\n2500 5000 7500100001250015000\nNeuron Index\n0\n20\n40\n60\n80\n100\nLayer 32\nFigure 11: The emergence of H2 in MLP blocks of OPT-6.7B. The x-axis represents the index of neurons in the\nhidden layers of MLP blocks, and the y-axis represents the activated frequency.\n29\nElimination of H2.\nWe first train a GPT-2 using Wiki-Text-103 dataset and subsequently identify\nand prune the neurons exhibiting an activation frequency exceeding 20% (i.e., H2). This pruning\noperation leads to a substantial decline in performance, as evidenced by an increase in perplexity\nfrom 19.32 to 31.78. The results emphasize the criticality of H2 in preserving the functionality of the\nmodel. To assess the recoverability of the discarded information, we conduct a few-shot fine-tuning\nexperiment, and the results are summarized in Table 12. The pruned model is fine-tuned with varying\nratios of training data for 500 iterations, and it successfully regains performance levels equivalent\nto those of the pre-trained model. In contrast, when training the model from scratch using only\n1% of the training data, the resulting model achieves a perplexity of 554.12 only. These findings\ndemonstrate that the knowledge encoded in H2 can be easily restored.\nTable 12: . Perplexity on the test-set of Wiki-Text-3 with GPT-2.\nSettings\n1%\n10%\n40%\n100%\nPretrained Model\n19.32\nRemove H2\n31.78\nFine-tuning\n19.86\n19.84\n19.76\n19.83\nOverlap across Diverse Input Contents.\nMoreover, we conduct a comparative analysis of the\nactivation frequencies acquired from various input contents. Specifically, utilizing the pretrained\nOPT-1.3B model, we evaluate three datasets, namely Wiki-Text-103, Penn Treebank, and Amazon\nReview. The positioning of H2 is depicted in Figure 12, revealing significant concurrence across\nmultiple datasets.\nFigure 13: The distribution of activated\nfrequency during training. Experiments\nare conducted with different checkpoints\nof OPT-2.7B during training.\nEarly-Bird Property.\nFurthermore, our investigation re-\nveals that H2 displays an \"early-bird\" characteristic, as illus-\ntrated in Figure 13. By visualizing the distribution of activa-\ntion frequencies across various checkpoints throughout the\ntraining process, we observe the emergence of a power-law\nbehavior at an initial stage, specifically as early as a train-\ning budget of 4%. Subsequently, the positions of H2 exhibit\ngradual and minimal changes.\n30\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 1\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 2\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 3\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 4\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 5\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 6\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\nLayer 7\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\nLayer 8\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 9\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 10\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 11\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 12\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 13\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 14\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 15\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 16\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 17\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 18\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 19\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 20\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nActivated Frequency (%)\nLayer 21\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 22\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 23\nWiki-Text-103\nAmazon Review\nPenn Treebank\n0\n2000\n4000\n6000\n8000\n0\n20\n40\n60\n80\n100\nLayer 24\nWiki-Text-103\nAmazon Review\nPenn Treebank\nFigure 12: The distribution of activated frequency across diverse input content. The x-axis represents the index\nof neurons, which is ordered by the activated frequency from Wiki-Text-103.\n31\nD\nTheoretical Analysis\nRecently, a number of works have studied the attention scheme in LLMs from a theoretical perspective\n[96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n117, 118, 119, 120, 121, 122]. In this work, we provide a different and novel angle compared to the\nprevious work. We present the concept of the submodular property and propose an eviction policy,\nknown as greedy H2, which is a modification of dynamic submodular maximization. Furthermore,\nassuming the attention scheme to be submodular, we establish that constructing the set Si without\nany cache size limitation satisfies the near-optimal property in terms of submodularity. We provide\ntheoretical guarantees for our robust and approximate greedy eviction policy algorithm (Algorithm\n2). Due to space limitation, we only give informal description of algorithm (Algorithm 2) in Section\n4.1 In Section D.6, we give an algorithm (Algorithm 2) which has full and complete implementation\ndetails for Algorithm 1. We also offer a mathematical formulation for sparsity preservation that is\nobserved in Section 3 and proposed an algorithm (Algorithm 4) to solve the problem.\nSpecifically, in Section D.1, we provide several basic definitions and notations. In Section D.2, we\nbriefly the definition of submodular function. In Section D.3, we define the dynamic submodular\nframework, which gives the formal version of Definition 4.1. In Section D.4, we briefly review the\nstatic attention computation problem. In Section D.5, we formulate the attention computation in\nrecursive fashion. In Section D.6, we briefly review our eviction policy, which gives the formal\nversion of Definition 4.3. In Section D.7, we discuss the diminishing return for submodular. In\nSection D.8, we discuss the high-level ideas for submodular. In Section D.9, we analyze the robust\ngreedy algorithm error propagation. In Section D.10, we explain how to add items into sets via\napproximate function. In Section D.11, we provide several definitions related to dynamic properties.\nIn Section D.12, we prove an induction lemma for the exact function. In Section D.13, we prove an\ninduction lemma for the approximation function. In Section D.14, we provide theoretical guarantees\nfor both the full-knowledge version (formal version of Lemma 3.1) and the limited-cache-size\nversion (formal version of Theorem 4.4). In Section D.15, we provide a more detailed discussion\nof theoretical work about attention computation and regression-related problems. In Section D.16,\nwe provide a mathematical formulation for sparsity preserving. In Section D.17, we provide the\ndefinition of loss function which can potentially generate sparse (heavy hitter type attention sore). In\nSection D.18, we explain how to compute the gradient of the loss function. In Section D.19, we show\nhow to compute the Hessian of the loss function. In Section D.20, we show that Hessian is positive\ndefinite. In Section D.21, we prove the Lipschitz property for the Hessian matrix. In Section D.22,\nwe show that using a gradient-type algorithm is sufficient to optimize that (heavy hitter type) loss\nfunction.\nD.1\nNotations\nFor a positive integer n, let [n] := {1, 2, \u00b7 \u00b7 \u00b7 , n}.\nFor a vector x \u2208 Rn, let \u221ax \u2208 Rn denote the vector with the i-th entry being \u221axi and diag(x) \u2208\nRn\u00d7n denote the diagonal matrix with the i-th digonal entry being xi. For two matrices A, W \u2208\nRn\u00d7n, let \u2225A\u2225W := (Pn\ni=1\nPn\nj=1 Wi,jA2\ni,j)1/2 and W \u25e6 A denote the matrix where (W \u25e6 A)i,j =\nWi,jAi,j. For matrix W \u2208 Rn\u00d7n, let DWi := diag(Wi,:) with i \u2208 [n].\nFor two vectors x \u2208 Rn and w \u2208 Rn\n\u22650, let \u2225x\u2225w := (Pn\ni=1 wix2\ni )1/2. For a vector x, its \u21132 norm\nis defined as \u2225x\u22252 := (Pn\ni=1 x2\ni )1/2 and its \u2113p norm is defined as \u2225x\u2225p := (Pn\ni=1 |xi|p)1/p. For a\nsquare matrix A, we denote tr[A] as the trace of matrix A.\nFor a matrix A \u2208 Rn\u00d7k (suppose n \u2265 k), we use \u2225A\u2225 to denote its spectral norm, i.e., \u2225A\u2225 =\nsupx \u2225Ax\u22252/\u2225x\u22252. We use \u2225A\u2225F to denote its Frobenius norm \u2225A\u2225F := (Pn\ni=1\nPk\nj=1 A2\ni,j)1/2.\nSuppose matrix A \u2208 Rn\u00d7k has SVD decomposition U\u03a3V \u22a4 where U \u2208 Rn\u00d7k (this matrix has\northonormal columns), \u03a3 \u2208 Rk\u00d7k is a diagonal matrix, and V \u2208 Rk\u00d7k. We call columns of U\nsingular vectors. We use A\u2020 \u2208 Rk\u00d7n to denote the Moore-Penrose pseudoinverse, then A\u2020 =\nV \u03a3\u22121U \u22a4. Suppose \u03a3 \u2208 Rk\u00d7k is sorted diagonal matrix, let \u03c31, \u00b7 \u00b7 \u00b7 , \u03c3k denote the diagonal entries\nof \u03a3. Then we call \u03c3i the i-th singular value of the matrix, and we write it as \u03c3i(A).\n32\nFor any symmetric matrix B \u2208 Rk\u00d7k, we denote its eigenvalue decomposition as U\u039bU \u22a4, where \u039b\nis a diagonal matrix. Let \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbk denote the entries on diagonal of \u039b \u2208 Rk\u00d7k. We say \u03bbi is the\ni-th eigenvalue. Usually, we write it as \u03bbi(B).\nThe connection between eigenvalues and singular values is\n\u03c32\ni (A) = \u03bbi(A\u22a4A)\nWe use the notation A \u2ab0 0 to denote that matrix A is positive semidefinite (psd). Mathematically,\nA \u2ab0 0 means for all vectors x, we have x\u22a4Ax \u2265 0.\nSimilarly, for two squarer matrices A and B, we use A \u2ab0 B to denote the case where for all vectors\nx, x\u22a4Ax \u2265 x\u22a4Bx.\nLet Pr[] and E[] denote the probability and expectation. We define the maximum between a and b as\nmax{a, b}. We denote min{a, b} (resp. max{a, b}) as the minimum (reps. maximum) between a\nand b.\nThroughout, for non-negative real numbers a and b, we use the notation a = (1 \u00b1 \u03f5)b if a \u2208\n[(1 \u2212 \u03f5)b, (1 + \u03f5)b].\nD.2\nSubmodular\nWe provide the standard definition of the submodular function.\nDefinition D.1 (Submodular function [123]). For a finite set \u2126, a submodular function is defined as\nf : 2\u2126 \u2192 R, where 2\u2126 denotes the power set of \u2126. It is characterized by the fulfillment of any of the\nfollowing equivalent criteria:\n\u2022 Condition 1.\n\u2013 For S, T, \u2286 \u2126 with S \u2286 T and every a \u2208 \u2126\\T we have that f(S \u222a {a}) \u2212 f(S) \u2265\nf(T \u222a {a}) \u2212 f(T)\n\u2022 Condition 2.\n\u2013 For every S, T \u2286 \u2126 we have that f(S) + f(T) \u2265 f(S \u222a T) + f(S \u2229 T).\n\u2022 Condition 3.\n\u2013 For every S \u2286 \u2126 and a1, a2 \u2208 \u2126\\S such that a1 \u0338= a2 we have that f(S \u222a {a1}) +\nf(S \u222a {a2}) \u2265 f(S \u222a {a1, a2}) + f(S).\nFor convenience of discussion, in this paper, we always choose \u2126 = [n] when we want to discuss the\nsubmodular function.\nNext, we provide some examples/types of submodular functions. One important class is called\nmonotone,\nDefinition D.2 (Monotone). A set function f is monotone if for every T \u2286 S we have f(T) \u2264 f(S).\nHere are a number of monotone submodular functions\n\u2022 Linear (Modular) functions\n\u2013 A linear function can be represented as f(S) = P\ni\u2208S wi. If all the weights wi are\nnonnegative, then the function f is considered monotone.\n\u2022 Budget-additive functions\n\u2013 A budget-additive function has the form f(S) = min B, P\ni\u2208S wi where each weight\nwi and the budget B are nonnegative.\n\u2022 Coverage functions\n\u2013 We have a set \u2126 = {E1, E2, \u00b7 \u00b7 \u00b7 , En} where each Ei is a subset of a broader set \u2126\u2032.\nThe coverage function can be expressed as f(S) = | \u222aEi\u2208S Ei| for any S \u2282 \u2126. This\nfunction can be generalized by assigning non-negative weights to the elements.\n33\nD.3\nDynamic Submodular\nThe standard submodular function is mapping 2[n] to real. Here we need a more general definition\nthat maps 2[n] \u00d7 2[n] to real.\nDefinition D.3 (Strong submodular). We define function F : 2[n] \u00d7 2[n] \u2192 R, then for any set\nZ \u2282 [n], we assume that F(Z, \u00b7) : 2[n] \u2192 R is a submodular function.\nIn fact, the above definition is stronger than we want, what we really need can be written as follows.\nWe remark that in Definition 4.1 we provide an informal definition. Here we provide a more detailed\nversion of the definition.\nDefinition D.4 (Dynamic submodular framework, formal version of Definition 4.1). Define function\nF : 2[n] \u00d7 [n] \u00d7 2[n] \u2192 R.\nThen for any index i \u2208 [n], any set Z \u2286 [i \u2212 1], we assume that\nF(Z, i, \u00b7) : 2[n] \u2192 R\nis a submodular function w.r.t. to Z, i.e.,\n\u2022 For all sets X, Y \u2282 [n] satisfy that Z \u2282 X \u2282 Y ,\n\u2022 For all element x \u2208 [n] satisfy that x \u2208 [n]\\Y ,\nwe have\nfZ,i(X \u222a {x}) \u2212 fZ,i(X) \u2265 fZ,i(Y \u222a {x}) \u2212 fZ,i(Y ),\nwhere fZ,i(\u00b7) := F(Z, i, \u00b7).\nRemark D.5. We remark that Definition D.4 is a weaker version of Definition D.3. We also like to\nmention that the informal definition (see Definition 4.1) only contains two input parameters, but in\nfact we need three input parameters (see Definition D.4).\nIn the later, when we use fZ,i(\u00b7), we will replace Z by Si for convenient of analysis, for example see\nDefinition D.23, Definition D.24, Definition D.25 and Definition D.26.\nD.4\nStatic Attention\nBefore we describe the recursive attention computation, we will first describe the static version of\nattention computation as follows (for examples, see Definition 1.1 in [98] and others [97, 100, 101,\n106, 103, 118, 108]):\nDefinition D.6. Given three matrices Q, K, V \u2208 Rd\u00d7d, the goal is to compute\nAtt(Q, K, V ) := D\u22121A \u00b7 V\nwhere square matrix A \u2208 Rn\u00d7n can be rewritten as follows\nA = exp(QK\u22a4)\nand diagonal matrix D \u2208 Rn\u00d7n can be written as follows\nD = diag(A1n)\nHere we apply exp() to a matrix entry-wisely to a matrix. 1n is a length-n vector where all the\nentries are ones. The operator diag() is turning a vector into a diagonal matrix.\nD.5\nRecursive Attention Definition\nWe first provide some definitions.\nDefinition D.7. Given a query matrix Q \u2208 Rn\u00d7d, we use Qi,\u2217 to denote a length-d row vector that\nrepresents the i-th row of Q for each i \u2208 [n].\nDefinition D.8. Given a key matrix K \u2208 Rn\u00d7d, we use K\u2264i,\u2217 to denote a i \u00d7 d matrix that selects\nthe first i rows from K.\n34\no1,1\no2,1\no3,1\no4,1\no2,2\no3,2\no4,2\no3,3\no4,3\no4,4\nI (Q1,\u2217)\neat (Q2,\u2217)\nan (Q3,\u2217)\napple (Q4,\u2217)\n\u2192\ntoday\nI\neat\nan\napple today\n(K1,\u2217) (K2,\u2217) (K3,\u2217) (K4,\u2217)\n(a)\no1,1\no2,1\no3,1\no4,1\no2,2\no3,2\no4,2\no3,3\no4,3\no4,4\nI (Q1,\u2217)\neat (Q2,\u2217)\nan (Q3,\u2217)\napple (Q4,\u2217)\n\u2192\ntoday\nI\neat\nan\napple today\n\u0018\u0018\u0018\n(K1,\u2217) (K2,\u2217) (K3,\u2217) (K4,\u2217)\n(b)\nFigure 14: (a) Exact version of the attention computation. Here, our example is about the language modeling task.\nAt this stage, the model predicts the word \u2019apple\u2019 and computes the exact attention vector o4. (b) Approximate\nversion of the attention computation. Let the budget of the number of tokens we can track be 3. We truncate the\nkey matrix for the first token K1,\u2217 and only keep K2,\u2217, K3,\u2217 and K4,\u2217 in the memory. Compared with the exact\nversion, we don\u2019t compute o4,1.\nNext, we show the exact version of computing attention.\nDefinition D.9 (Exact Version). Given Q, K \u2208 Rn\u00d7d\n\u2022 For each i \u2208 [n], we use oi \u2208 Ri to denote a length-i vector.\n\u2013 For each j \u2208 [i], we use oi,j to denote the j-th coordinate of vector oi \u2208 Ri\nFor the first layer, we have\n\u2022 o1,1 = 1.\nFor the second layer, we have\n\u2022 length-2 vector o2 := D\u22121\n2\n|{z}\nscalar\n\u00b7 exp(Q2,\u2217\n|{z}\n1\u00d7d\n(K\u22642,\u2217)\u22a4\n|\n{z\n}\nd\u00d72\n)\n\u2022 scalar D2 := exp(Q2,\u2217(K\u22642,\u2217)\u22a4)\n|\n{z\n}\n1\u00d72\n\u00b7 12\n|{z}\n2\u00d71\nFor each i \u2208 [n], for the i-th layer, we have\n\u2022 length-i vector oi := D\u22121\ni\n|{z}\nscalar\n\u00b7 exp(Qi,\u2217\n|{z}\n1\u00d7d\n(K\u2264i,\u2217)\u22a4\n|\n{z\n}\nd\u00d7i\n)\n\u2022 scalar Di := exp(Qi,\u2217(K\u2264i,\u2217)\u22a4)\n|\n{z\n}\n1\u00d7i\n\u00b7 1i\n|{z}\ni\u00d71\nNow, we show the approximate version of computing attention. Instead of computing the entire\nattention oi, we only compute the attention of the tokens that are being tracked.\nDefinition D.10 (Approximate Version). Given Q, K \u2208 Rn\u00d7d\n\u2022 For each i \u2208 [n], we use oi \u2208 Ri to denote a length-i vector.\n\u2013 For each j \u2208 [i], we use oi,j to denote the j-th coordinate of vector oi \u2208 Ri\n\u2022 Let k \u2208 [n] be the budget of the number of tokens we can track (due to the memory issue).\n35\nFor the first layer, we have\n\u2022 o1,1 = 1.\nFor the second layer, we have\n\u2022 length-2 vector o2 := D\u22121\n2\n|{z}\nscalar\n\u00b7 exp(Q2,\u2217\n|{z}\n1\u00d7d\n(K\u22642,\u2217)\u22a4\n|\n{z\n}\nd\u00d72\n)\n\u2022 scalar D2 := exp(Q2,\u2217(K\u22642,\u2217)\u22a4)\n|\n{z\n}\n1\u00d72\n\u00b7 12\n|{z}\n2\u00d71\nFor each i \u2208 [n], for the i-th token, we have\n\u2022 Let Si \u2282 [n] denote the tokens we\u2019re tracking when we are predicting the i-th token.\n\u2022 |Si| = k\n\u2022 |Si\\Si\u22121| \u2264 1 or equivalently |Si \u2229 Si\u22121| \u2265 k \u2212 1\n\u2022 length-i vector oi := D\u22121\ni\n|{z}\nscalar\n\u00b7 exp(Qi,\u2217\n|{z}\n1\u00d7d\n(KSi,\u2217)\u22a4\n|\n{z\n}\nd\u00d7i\n)\n\u2022 scalar Di := (exp(Qi,\u2217(KSi,\u2217)\u22a4) \u2212 1[i]\\Si)\n|\n{z\n}\n1\u00d7i\n\u00b7 1i\n|{z}\ni\u00d71\nIn a certain sense, the above definition is related to finding heavy hitters in compressive sensing (for\nmore background on compressive sensing, we refer readers to [124, 125, 126]).\nD.6\nEviction Policy\nThe goal of this section is to our eviction policy under space limitation. We start by giving a process\nwithout having space limitations. We denote the attention query matrix as Q \u2208 Rn\u00d7d and the key\nmatrix as K \u2208 Rn\u00d7d. Qi,\u2217 represents the i-th row of Q and K\u2264i,\u2217 represents the first i rows of K.\nFor simplicity, KSi,\u2217 denotes a sub-matrix of K which selects Si rows from K.\nDefinition D.11 (The generative process with Full knowledge). For each i \u2208 [n], for the i-th token,\nwe have\n\u2022 length-i vector oi := D\u22121\ni\n\u00b7 exp(Qi,\u2217(K\u2264i,\u2217)\u22a4), which denotes the attention of the i-th\nword.\n\u2022 scalar Di := exp(Qi,\u2217(K\u2264i,\u2217)\u22a4) \u00b7 1i, which denotes the normalization factor.\nIn the above process, since we have no space limitation, then we can keep all the scores. In the\nfollowing process, we show how to handle things when there is a space limitation. In this case, we\nneed to dynamically maintain set Si such that |Si| \u2264 k (Compared to the full knowledge case, we\ncan think of |Si| = i).\nDefinition D.12 (H2O Eviction Policy, Formal version of Definition 4.3). Let k denote the budget of\nspace and k < n. Let Fscore : 2[n] \u2192 R denote certain score function. Let Si\u22121 denote the source\nset. Let Si denote the target set. We defined the eviction policy g : Si\u22121 \u2192 Si s.t.\n\u2022 |Si| \u2264 k (KV cache size is not changing over the time)\n\u2022 |Si\\Si\u22121| \u2264 1 (we can evict at most 1 KV in the KV cache)\n\u2022 We construct Si \u2190 (Si\u22121\u222a{i})\\{u} as u \u2190 arg maxv\u2208(Si\u22121\u222a{i}) Fscore(Si\u22121\u222a{i}\\{v}}\nRemark D.13. We remake that, in Definition 4.3, we introduce a simplified notation where we state\nthat |Si| = k in the first bullet point, but in general, we consider the case where |Si| \u2264 k for the first\nk tokens. To accommodate this more general scenario, we present Definition D.12, which provides a\nbroader definition that handles the situation where |Si| \u2264 k.\n36\nAlgorithm 2 H2 Eviction Algorithm, Query matrix Q \u2208 Rn\u00d7d, key matrix K \u2208 Rn\u00d7d, budget size\nof KV cache k \u2208 N. Formal and detailed version of Algorithm 1.\n1: procedure H2_EVICTION(Q \u2208 Rn\u00d7d, K \u2208 Rn\u00d7d, k \u2208 N)\n2:\nS0 \u2190 \u2205, eo0 \u2190 0\n\u25b7 Initialization the algorithm\n3:\nfor i = 1 \u2192 n do\n\u25b7 Linear scan each token\n4:\nif i \u2264 k then\n\u25b7 If the KV cache is not full\n5:\nSi \u2190 Si\u22121 \u222a {i}\n\u25b7 Expand the set directly\n6:\nelse\n\u25b7 If the KV cache is full\n7:\nDi \u2190 (exp(Qi,\u2217(KSi\u22121,\u2217)\u22a4) \u2212 1[i]\\Si\u22121) \u00b7 1i\n\u25b7 Compute the normalization factor\n8:\noi \u2190 D\u22121\ni\n\u00b7 exp(Qi,\u2217(KSi\u22121,\u2217)\u22a4)\n\u25b7 Compute score vector\n9:\neoi \u2190 eoi\u22121 + oi\n\u25b7 Accumulate the scores (Remark D.15)\n10:\nLet score function be Fscore(T) := h(P\ns\u2208T eoi,s) \u25b7 Score function (Remark D.14)\n11:\nu \u2190 arg maxv\u2208(Si\u22121\u222a{i}) Fscore(Si\u22121 \u222a {i}\\{v}}\n\u25b7 Find an entry to swap\n12:\nSi \u2190 (Si\u22121 \u222a {i})\\{u}\n\u25b7 Construct Si\n13:\nend if\n14:\nend for\n15: end procedure\nRemark D.14. We remark the above function Fscore can have multiple choices. In the later analysis,\nwe provide two choices. If we use the exact function, then Fscore is fSi,i (see Definition D.23 and\nDefinition D.25). If we use the approximate function, then Fscore if efSi,i (see Definition D.24 and\nDefinition D.26). Let h : R \u2192 R (the h being used Line 10 in Algorithm 2) denote function. We\nhope to choose h such that it gives the submodular property for the function Fscore in the sense of\nour dynamic framework. For example, h(z) is usually chosen to be some non-decreasing concave\nfunction such as \u221az + 1 and log(z + 1). For simplicity, in Algorithm 1 (which is the informal\nversion of Algorithm 2), we choose h(z) = z for the purpose of providing readers with an intuitive\nunderstanding and due to space limitation.\nRemark D.15. We remark that, in Line 10 in Algorithm 2, the eoi,s is the accumulation of attention\nscore for the token s in set T. In our Algorithm 1, we only use os in Line 10 (see Algorithm 1) to\nrepresent that accumulation of the attention score for simplicity.\nIn Algorithm 2, for simplicity of the presentation, we can create multiple vectors eo0, \u00b7 \u00b7 \u00b7 eon (see Line 2,\nLine 9, Line 10). However, every time at i-th, we only need to use the information for eoi which has at\nmost length n size. Thus, a straightforward and better implementation for only using one length-n to\nstore accumulative score can reduce n2 usage to n.\nD.7\nExplaining Submodular Diminishing Return Property in Attention Scheme\nIn the standard submodular problem [123], we are given a ground set [n] and a function f : 2[n] \u2192 R.\nThe goal is to find a set of elements S such that f(S) is maximized.\nWe say function f is submodular (Recall the formal definition in Definition D.1), if for every\nX, Y \u2282 [n] with X \u2286 Y and every x \u2208 [n]\\Y we have\nf(X \u222a {x}) \u2212 f(X) \u2265 f(Y \u222a {x}) \u2212 f(Y )\nSubmodular functions can represent the cost of items, due to their diminishing returns property\n[127, 128]. It suggests that the increase in information obtained from selecting a candidate object,\nsuch as a word or sentence, becomes smaller as more objects have already been chosen for the\nsummary.\nFor instance, we introduce a new token, denoted as \u201cw\u201d, into two sets, S and S0, where the concepts\ncovered by S0 are a subset of those covered by S. By intuition, the information added to S0 by \u201cw\u201d\nshould be larger compared to adding it to S, as the new concepts carried by \u201cw\u201d might have already\nbeen covered by the concepts present in S but not in S0. This property is known as the diminishing\nreturn property. Hence, we propose that the neural coverage function [129] should exhibit a desirable\ncharacteristic called submodularity.\n37\nD.8\nSubmodular: High Level Ideas\nWe formalize the submodular function maximization problem with cardinality constraint in this\nsection. Informally, a set function is submodular if it has decreasing marginal increment.\nDefinition D.16 (Submodular function). We denote f : 2[n] \u2192 R as a set function. The discrete\nderivative \u2206f is defined as follows:\n\u2206f(i | S) := f(S \u222a {i}) \u2212 f(S).\nA function f is submodular if, for any S \u2286 T and i \u2208 [n]\\T, the following inequality holds:\n\u2206f(i | T) \u2264 \u2206f(i | S).\nFor simplicity, we present the problem of maximizing a submodular function with a cardinality\nconstraint (1). Our goal is to solve the optimization problem efficiently.\nmax\nS\u2286[n]\nf(S)\ns.t.\n|S| \u2264 k\n(1)\nRepresentation of f(S)\nOne challenge in designing the algorithm is determining the representation\nof input instances. Since the constraint in optimization problem (1) is straightforward, we need to\ndecide how to represent f(S). Suppose S = i1, i2, \u00b7 \u00b7 \u00b7 , im \u2286 [n], we can decompose f(S) into a\nsum of increments as follows:\nf(S) = f(S0) +\nm\nX\nj=1\n(f(Sj) \u2212 f(Sj\u22121)),\n(2)\nwhere S0 = \u2205 and Sj = Sj\u22121+ij. Without loss of generality, we assume f(\u2205) = 0. By the definition\nof \u2206f(i|S), we have f(Sj) \u2212 f(Sj\u22121) = \u2206f(ij|Sj\u22121). Therefore, the decomposition (2) can be\nsimplified as follows:\nf(S) =\nm\nX\nj=1\n\u2206f(ij|Sj\u22121)\n(3)\nTo introduce our advanced data structure later, we further represent \u2206f(i|S) in the form of\n\u2206f(i|S) = u\u22a4\ni h(S)ui\n(4)\nwhere ui \u2208 Rd is a d-dimensional vector and h(S) \u2208 Rd\u00d7d is a d-by-d matrix.\nIn practice, a significant subclass of submodular functions is the monotone submodular functions, i.e.\nfunctions f satisfying f(A) \u2264 f(B) for all A \u2286 B \u2286 [n]. When f is monotone, we could restrict\nall h(S) to be positive semidefinite (PSD) matrixes. When the matrix h(S) is positive semidefinite\n(PSD), it makes us achieve a faster acceleration.\nD.9\nRobust Greedy with Error Propagation\nThe procedure for the greedy selection algorithm initiates with empty set S0 = \u2205. For each iteration\nin the main loop, the algorithm chooses the element that maximizes the marginal increment to add to\nthe set. When the size of the set eventually reaches k, the algorithm return that set S. Specifically, for\niteration t \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k}, we let\nSt \u2190 St\u22121 \u222a {jt}\nwhere the element in the singleton is jt = arg maxj\u2208St\u22121 f(St\u22121 \u222a {j}). The greedy strategy is\neffective in the sense that the approximation error of it is 1 \u2212 1/e.\nTheorem D.17 ([130]). For a monotone submodular function f, the greedy algorithm (Algorithm 3)\nguarantees to output a set S satisfying\nf(S) \u2265 (1 \u2212 1/e) max\n|T |=k{f(T)}.\nCorollary D.18 ([131]). Given\n38\nAlgorithm 3 Greedy algorithm benchmark\n1: procedure GREEDYALGORITHM(submodular function f)\n2:\nS0 \u2190 \u2205\n\u25b7 Initialize an empty set S0\n3:\nfor t = 0 \u2192 k \u2212 1 do\n4:\nj \u2190 arg maxi{f(St \u222a {i})}\n\u25b7 Find the element i \u2208 [n] that maximize f(St \u222a {i})\n5:\nSt+1 \u2190 St \u222a {j}\n\u25b7 Append element i to set St to get St+1\n6:\nend for\n7:\nreturn Sk\n8: end procedure\n\u2022 accuracy parameter \u03f5 > 0.\n\u2022 integer k \u2265 1\n\u2022 Let O denote an oracle that takes an arbitrary set S \u2286 [n] and i \u2208 [n]\\S, returns a value\nO(S, i) with guarantee that \u2206(i|S) \u2212 \u03f5 \u2264 O(S, i) \u2264 \u2206(i|S) + \u03f5.\n\u2022 Let A denote an algorithm that each time step t\n=\n1, 2, \u00b7 \u00b7 \u00b7 , k, it selects jt\n=\narg maxj{O(St\u22121, j)} and lets St \u2190 St\u22121 \u222a {jt}.\nThen this algorithm A\n\u2022 returns a set Sk\n\u2022 the Sk satisfy that\nf(Sk) \u2265 (1 \u2212 1/e) max\n|T |=k{f(T)} \u2212 k(2 \u2212 1/e)\u03f5.\nD.10\nRobust Submodular and Adding Items\nWe first propose a lemma that shows the robustness of the submodular.\nLemma D.19. Suppose that for each i \u2208 [n], we have\n\u2022 for all Si \u2282 [n] with |Si| \u2264 k\n\u2022 for all X \u2282 [n], X \u2286 Si \u222a {i} and |X \u2229 Si| \u2264 |Si|,\n|( efSi,i(X)) \u2212 efSi,i(Si) \u2212 (fSi,i(X) \u2212 fSi,i(Si))| \u2264 \u03f5/(2n)\nLet opti denote the optimal cost that can be achieved by using f in i-th iteration. If the greedy\nalgorithm use f to find the solution has performance at least (1 \u2212 1/e) \u00b7 opti, then using ef, we can\nobtain a solution that has performance at least (1 \u2212 1/e) \u00b7 opti \u2212 \u03f5\nProof. The proof follows from Lemma D.18.\nNext, we explain how to add items into sets based on exact values.\nDefinition D.20 (Expanding items based on exact value). If the following conditions hold\n\u2022 Let Si \u2286 [i \u2212 1].\n\u2022 Let fSi,i : 2[i] \u2192 R.\nThen, we can define Si+1 as follows\n\u2022 If |Si| = k then Si+1 = Si \u222a {i}\\u where u = arg maxv\u2208Si\u222a{i} fSi,i(Si \u222a {i}\\v) \u2212\nfSi,i(Si)\n\u2022 If |Si| < k, then Si+1 = Si \u222a {i}.\n39\nRemark D.21. We remark that u = arg maxv\u2208Si\u222a{i} fSi,i(Si \u222a {i}\\v) \u2212 fSi,i(Si) is the same as\nu = arg maxv\u2208Si\u222a{i} fSi,i(Si \u222a {i}\\v). For the convenience of discussion and analysis, we will\nswitch to using both cases in different places.\nHere, we explain how to add items into sets via approximate values.\nDefinition D.22 (Expanding items based on approximate value). If the following conditions hold\n\u2022 Let Si \u2286 [i \u2212 1].\n\u2022 Let efSi,i : 2[i] \u2192 R.\nThen, we can define Si+1 as follows\n\u2022 If |Si| = k then Si+1 = Si \u222a {i}\\u where u = arg maxv\u2208Si\u222a{i} efSi,i(Si \u222a {i}\\v) \u2212\nefSi,i(Si)\n\u2022 If |Si| < k, then Si+1 = Si \u222a {i}.\nD.11\nUniversal Conditions\nWe state several definitions for both the exact function and approximation function.\nDefinition D.23 (Universal Monotone Condition (for exact function)). We say f has universal\nmonotone condition, if for all i \u2208 [n] for all Si \u2286 [i \u2212 1], we have\nfSi,i(X) \u2265 fSi,i(Y ),\n\u2200Y \u2282 X\nDefinition D.24 (Universal Monotone Condition (for approximate function)). We say ef has universal\nmonotone condition, if for all i \u2208 [n] for all Si \u2286 [i \u2212 1], we have\nefSi,i(X) \u2265 efSi,i(Y ),\n\u2200Y \u2282 X\nDefinition D.25 (Universal Dynamic Condition 1(for exact function)). We say f has universal\ndynamic condition 1(for exact function), if for all i \u2208 [n] for all Si \u2286 [i \u2212 1], Si\u22121 \u2286 [i \u2212 2],\n|Si\\Si\u22121| \u2264 1, we have\nfSi,i(Si) \u2265 (1 \u2212 \u03b8) \u00b7 fSi\u22121,i\u22121(Si)\nDefinition D.26 (Universal Dynamic Condition 1 (for approximate function)). We say ef has universal\ndynamic condition 1(for approximate function), if for all i \u2208 [n] for all Si \u2286 [i \u2212 1], Si\u22121 \u2286 [i \u2212 2],\n|Si\\Si\u22121| \u2264 1, we have\nefSi,i(Si) \u2265 (1 \u2212 \u03b8) \u00b7 efSi\u22121,i\u22121(Si)\nWe define opti as follows:\nDefinition D.27. Let k denote the budget length. For each i \u2208 [n], we define opti as\nmax{fX,i(Y ) | X \u2286 [i \u2212 1], Y \u2286 [i], |X| \u2264 k, |Y | \u2264 k, |Y \\X| \u2264 1}.\nDefinition D.28 (Universal Dynamic Condition 2). For i \u2208 [n], we define opti as Definition D.27.\nWe say it has universal dynamic condition if for each i \u2208 [n], we have\nopti \u2265 (1 \u2212 \u03b3) \u00b7 opti\u22121.\nD.12\nInduction Lemma for Exact Function\nThe goal of this section is to prove Lemma D.29\nLemma D.29 (Induction Lemma). For a fixed i, suppose the following conditions hold\n\u2022 Set Condition. Si \u2286 [i \u2212 1]\n\u2022 Budget Condition. |Si| \u2264 k\n40\n\u2022 Value Condition. fSi\u22121,i\u22121(Si) \u2265 (1 \u2212 1/e) \u00b7 (1 \u2212 \u03b8)i(1 \u2212 \u03b3)iopti\n\u2022 Universal Dynamic Condition 1 (for exact function). (See Definition D.25) fSi,i(Si) \u2265\n(1 \u2212 \u03b8) \u00b7 fSi\u22121,i\u22121(Si)\n\u2022 Universal Dynamic Condition 2. (See Definition D.28) opti \u2265 (1 \u2212 \u03b3) \u00b7 opti+1\n\u2022 Universal Monotone Condition (for exact function). (See Definition D.23) fSi,i(X) \u2265\nfSi,i(Y ) for all Y \u2282 X\nThen if we construct Si+1 as Definition D.20, then we have\n\u2022 Set Condition. Si+1 \u2286 [i]\n\u2022 Budget Condition. |Si+1| \u2264 k\n\u2022 Value Condition. fSi,i(Si+1) \u2265 (1 \u2212 1/e) \u00b7 (1 \u2212 \u03b8)i+1(1 \u2212 \u03b3)i+1opti+1\nProof. Proof of Set Condition.\nNote that Si \u2286 [i \u2212 1], by using the way we construct Si+1, then it is obvious that Si+1 \u2286 [i].\nProof of Budget Condition.\nNote that |Si| \u2264 k, by using the way we construct Si+1, then it is straightforward that |Si+1| \u2264 k.\nProof of Value Condition.\nWe can show that\nfSi,i(Si+1) \u2265 fSi,i(Si)\n\u2265 (1 \u2212 \u03b8) \u00b7 fSi\u22121,i\u22121(Si)\n\u2265 (1 \u2212 \u03b8) \u00b7 ((1 \u2212 1/e)(1 \u2212 \u03b8)i(1 \u2212 \u03b3)iopti \u2212 i \u00b7 \u03f50)\n\u2265 (1 \u2212 \u03b8) \u00b7 ((1 \u2212 1/e)(1 \u2212 \u03b8)i(1 \u2212 \u03b3)i+1opti+1)\n\u2265 (1 \u2212 1/e) \u00b7 (1 \u2212 \u03b8)i+1(1 \u2212 \u03b3)i+1opti+1.\nwhere the first step follows from Universal Monotone Condition, the second step follows from\nUniversal Dynamic Condition 1, the third step follows from Value Condition, the forth step follows\nfrom Universal Dynamic Condition 2, and the last step follows from simple algebra.\nThus, we complete the proof.\nD.13\nInduction Lemma for Approximate Function\nThe goal of this section is to prove Lemma D.30\nLemma D.30 (Induction Lemma). For a fixed i, suppose the following conditions hold\n\u2022 Set Condition. Si \u2286 [i \u2212 1]\n\u2022 Budget Condition. |Si| \u2264 k\n\u2022 Value Condition. fSi\u22121,i\u22121(Si) \u2265 (1 \u2212 1/e) \u00b7 (1 \u2212 \u03b8)i(1 \u2212 \u03b3)iopti \u2212 i \u00b7 \u03f50\n\u2022 Universal Dynamic Condition 1 (for approximate function). (see Definition D.26)\nefSi,i(Si) \u2265 (1 \u2212 \u03b8) \u00b7 efSi\u22121,i\u22121(Si)\n\u2022 Universal Dynamic Condition 2. opti \u2265 (1 \u2212 \u03b3) \u00b7 opti+1\n\u2022 Universal Approximate Condition. (See Definition D.28) fSi,i(X) \u2265 efSi,i(X) \u2212 \u03f50 for\nall X\n\u2022 Universal Monotone Condition (for approximate function).\n(see Definition D.24)\nefSi,i(X) \u2265 efSi,i(Y ) for all Y \u2282 X\nThen if we construct Si+1 as Definition D.22, then we have\n41\n\u2022 Set Condition. Si+1 \u2286 [i]\n\u2022 Budget Condition. |Si+1| \u2264 k\n\u2022 Value Condition. fSi,i(Si+1) \u2265 (1 \u2212 1/e) \u00b7 (1 \u2212 \u03b8)i+1(1 \u2212 \u03b3)i+1opti+1 \u2212 (i + 1) \u00b7 \u03f50\nProof. Proof of Set Condition.\nNote that Si \u2286 [i \u2212 1], by using the way we construct Si+1, then it is obvious that Si+1 \u2286 [i].\nProof of Budget Condition.\nNote that |Si| \u2264 k, by using the way we construct Si+1, then it is straightforward that |Si+1| \u2264 k.\nProof of Value Condition.\nWe can show that\nfSi,i(Si+1) \u2265 efSi,i(Si+1) \u2212 \u03f50\n\u2265 efSi,i(Si) \u2212 \u03f50\n\u2265 (1 \u2212 \u03b8) \u00b7 efSi\u22121,i\u22121(Si) \u2212 \u03f50\n\u2265 (1 \u2212 \u03b8) \u00b7 ((1 \u2212 1/e)(1 \u2212 \u03b8)i(1 \u2212 \u03b3)iopti \u2212 i \u00b7 \u03f50) \u2212 \u03f50\n\u2265 (1 \u2212 \u03b8) \u00b7 ((1 \u2212 1/e)(1 \u2212 \u03b8)i(1 \u2212 \u03b3)i+1opti+1 \u2212 i \u00b7 \u03f50) \u2212 \u03f50\n\u2265 (1 \u2212 1/e) \u00b7 (1 \u2212 \u03b8)i+1(1 \u2212 \u03b3)i+1opti+1 \u2212 (i + 1) \u00b7 \u03f50.\nwhere the first step follows from Universal Approximate Condition, the second step follows from\nUniversal Monotone Condition, the third step follows from Universal Dynamic Condition 1, the\nforth step follows from Value Condition, the fifth step follows from Universal Dynamic Condition\n2, and the last step follows from simple algebra.\nThus, we complete the proof.\nD.14\nTheoretical Result\nWe first give the guarantee of our full-knowledge version (without cache size limitation).\nLemma D.31 (Formal version of Lemma 3.1). Under the mild assumption, let k denote any target\nsize. If we greedily compute the attention score based on full information, then we can find the set Si\nsuch that\nf(Si) \u2265 (1 \u2212 1/e) \u00b7 (1 \u2212 \u03b1)opti,\nwhere \u03b1 \u2208 (0, 1) are parameters.\nProof. The proof follows from using Theorem D.17, Corollary D.18, Lemma D.29 with choosing\n\u03b8 = \u03b3 = \u03b1/(10n).\nNext, we show the guarantee for our robust and approximate greedy eviction policy algorithm\n(Algorithm 2).\nTheorem D.32 (Formal version of Theorem 4.4). Under the mild assumption, let k denote the budget\nof space limitation. If for each token, we greedily compute the attention score based on top-k choice,\nthen we can show the set eSi we generate each for token i \u2208 [n] satisfy that\nf(eSi) \u2265 (1 \u2212 1/e) \u00b7 (1 \u2212 \u03b1)opti \u2212 \u03b2,\nwhere \u03b1 \u2208 (0, 1), \u03b2 > 0 are parameters.\nProof. The proof follows from using Theorem D.17, Corollary D.18, Lemma D.30 with choosing\n\u03f50 = \u03b2/(10n) and \u03b8 = \u03b3 = \u03b1/(10n).\n42\nD.15\nExtended Related Work for Theoretical Attention Problems\nThe static attention computation is asking the following question that given Q, K, V \u2208 Rn\u00d7d, the\ngoal is to D\u22121 exp(QK\u22a4)V where D = diag(exp(QK\u22a4)1n). [98] studied the static attention\ncomputation from both algorithm and hardness. On the positive, they provide an almost linear time\nalgorithm to approximately compute the attention matrix. On the negative side, assuming a strong\nexponential time hypothesis (SETH), they prove a hardness result. Their hardness result is, unless\nSETH fails, there is no algorithm that runs in truly subquadratic time to approximately compute\nthe attention matrix. Further, [100] considers the dynamic of attention computation problem. They\nalso provide both algorithmic results and hardness results. In the work of [103], they consider\nthe sparsification of attention matrix construction. In particular, they assume that situation that\nd \u226b n, and show how to sparsify the columns of matrix Q. [103] provides two algorithms, one\nis a randomized algorithm, and the other is a deterministic algorithm. Differential privacy is a\nfamous and textbook topic in graduate school, recently the work of [101] shows how to give a\ndifferentially private algorithm for computing the attention matrix. For a given A \u2208 Rn\u00d7d and\nvector b \u2208 Rn, [104] formulates and studies exponential regression minx \u2225 exp(Ax) \u2212 b\u22252. Then\n[105] considers the normalization factor in exponential regression and defines the softmax regression\nproblem minx \u2225\u27e8exp(Ax), 1n\u27e9\u22121 exp(Ax) \u2212 b\u22252. [107] moves the scaling factor from exp(Ax) to\nb and defines a rescaled softmax regression problem minx \u2225 exp(Ax) \u2212 \u27e8exp(Ax), 1n\u27e9 \u00b7 b\u22252.\nD.16\nSparsity Preserving\nRecall that in Figure 2, we observe that even when trained densely, the attention matrices of LLMs\nare over 95% sparse at inference time. Only 5% of the KV cache is sufficient for decoding the same\noutput token at each generation step. Here, we provide some formal formulations for sparsity.\nDefinition D.33. Suppose the following conditions\n\u2022 Let S0 \u2282 [m].\n\u2022 Let k = |S0|.\n\u2022 Let \u03c4 \u2208 (0, 1) denote a threshold for truncating the value.\n\u2022 Let \u03b1 \u2208 (0, 1) denote a fraction of mass (larger than \u03c4) outside S0.\n\u2022 Let mapping D : Rd \u2192 Rm\n\u22650.\n\u2022 For each x \u2208 Rd, D(x) \u2208 Rm is a vector that has length m.\nWe say the distribution D is (\u03b1, \u03c4, k)-good if the following conditions hold\n\u2022 For all x \u2208 Rd, S0 \u2282 supp\u03c4(D(x))\n\u2022 For all x \u2208 Rd, | supp\u03c4(D(x))\\S0| \u2264 \u03b1 \u00b7 k\nClaim D.34. Suppose we sample n points {x1, x2, \u00b7 \u00b7 \u00b7 , xn} \u2282 Rd from (\u03b1, \u03c4, k)-good distribution\nuniformly at random, then we have\n\u2022 S0 \u2286 \u2229i\u2208[n] supp\u03c4(xi)\n\u2022 |(\u222ai\u2208[n] supp\u03c4(D(x)))\\S0| \u2264 \u03b1kn\nProof. Since for all i \u2208 [n], we have S0 \u2286 supp\u03c4(D(xi)), thus\nS0 \u2286 \u2229i\u2208[n] supp\u03c4(xi).\nTherefore we proved the first property.\nWe know that for all i \u2208 [n], we have | supp\u03c4(D(x))\\S0| \u2264 \u03b1kn. Thus\n|(\u222ai\u2208[n] supp\u03c4(D(xi)))\\S0| \u2264\nn\nX\ni=1\n| supp\u03c4(D(xi)))\\S0| \u2264 n \u00b7 \u03b1k\nTherefore, we finish the proof for the second property.\n43\nD.17\nDefinition of Loss Function\nIn this section, we follow the theoretical softmax regression literature [105] and define a number\nof functions to make the calculations of gradient and Hessian convenient. We also proposed a new\npenalty term (\u21131 type sparsity penalty, see Definition D.41) into the final loss function, which is not\nstudied in previous work [104, 105, 107, 110, 132, 133]. We first provide some function definitions.\nDefinition D.35 (Function u, [105]). Given matrix A \u2208 Rn\u00d7d, let function u : Rd \u2192 Rn be defined\nas follows\nu(x) := exp(Ax)\nDefinition D.36 (Function \u03b1, see Definition 5.4 in [105] as an example). We define u(x) as Defini-\ntion D.35. Then we define \u03b1 : Rd \u2192 R as follows\n\u03b1(x) := \u27e8u(x), 1n\u27e9\nDefinition D.37 (Function f, see Definition 5.1 in [105] as an example). Provided that the following\nconditions are true\n\u2022 We define u(x) as Definition D.35.\n\u2022 We define \u03b1(x) as Definition D.36\nLet function f : Rd \u2192 Rn be defined as follows\nf(x) := \u03b1(x)\u22121u(x).\nDefinition D.38 (Function c, see Definition 5.5 in [105] as an example). Provided that the following\nconditions are true\n\u2022 Given a vector b \u2208 Rn.\n\u2022 Let f(x) be defined as Definition D.38.\nThen, let function c : Rd \u2192 Rn defined as follows\n\u2022 c(x) := f(x) \u2212 b.\nDefinition D.39 (Loss function Lexp, see Definition 5.3 in [105] as an example). We define Lexp :\nRd \u2192 R\nLexp(x) := 0.5 \u00b7 \u2225c(x)\u22252\n2.\nDefinition D.40 (Loss function Lreg). Given A \u2208 Rn\u00d7d.\nLet function Lreg : Rd \u2192 R be defined as follows\nLreg(x) := 0.5 \u00b7 \u2225 diag(w)Ax\u22252\n2\nWe define a novel penalty function\nDefinition D.41 (Implicitly controlling the sparsity). Given A \u2208 Rn\u00d7d.\nWe define\nLsparse(x) := \u2225 exp(Ax)\u22251.\nThen it is obvious that we have\nClaim D.42. Given A \u2208 Rn\u00d7d. Let u be defined as Definition D.35. Let \u03b1 be defined as Defini-\ntion D.36.\nWe have\n\u2022 Lsparse(x) = \u27e8exp(Ax), 1n\u27e9\n\u2022 Lsparse(x) = \u27e8u(x), 1n\u27e9\n44\n\u2022 Lsparse(x) = \u03b1(x)\nProof. The proof is trivially following from the definition of u(x) (see Definition D.35) and \u03b1(x)\n(see Definition D.36).\nThe final loss function can be defined as follows. Intuitively, we can write attention D\u22121 exp(QK\u22a4)\ninto n subproblems where each subproblem can be viewed as one softmax problem.\nDefinition D.43. If the following conditions hold\n\u2022 We define Lexp as Definition D.39.\n\u2022 We define Lreg as Definition D.40.\n\u2022 We define Lsparse as Definition D.41.\nThen we define L function\nL(x) := Lexp(x) + Lsparse(x) + Lreg(x).\nD.18\nGradient\nNext, we show the gradient of Lexp.\nLemma D.44 (Gradient, Lemma 5.6 in [105]). Provided that the following conditions are true\n\u2022 Given matrix A \u2208 Rn\u00d7d and a vector b \u2208 Rn.\n\u2022 Let A\u2217,i \u2208 Rn denote the i-th column of matrix A, for every i \u2208 [d].\n\u2022 We define \u03b1(x) as Definition D.36.\n\u2022 We define f(x) as Definition D.37.\n\u2022 We define c(x) as Definition D.38.\n\u2022 We define Lexp(x) as Definition D.39.\n\u2022 Let \u25e6 denote hadamard product.\nFor every i \u2208 [d], we have\n\u2022 Part 1.\nd exp(Ax)\ndxi\n= exp(Ax) \u25e6 A\u2217,i\n\u2022 Part 2.\nd\u27e8exp(Ax), 1n\u27e9\ndxi\n= \u27e8exp(Ax), A\u2217,i\u27e9\n\u2022 Part 3.\nd\u03b1(x)\u22121\ndxi\n= \u2212\u03b1(x)\u22121 \u00b7 \u27e8f(x), A\u2217,i\u27e9\n\u2022 Part 4.\ndf(x)\ndxi\n= dc(x)\ndxi\n= \u2212 \u27e8f(x), A\u2217,i\u27e9 \u00b7 f(x) + f(x) \u25e6 A\u2217,i\n\u2022 Part 5.\ndLexp(x)\ndxi\n= A\u22a4\n\u2217,i \u00b7 (f(x)(f(x) \u2212 b)\u22a4f(x) + diag(f(x))(f(x) \u2212 b))\n45\nD.19\nHessian\nHere, we compute the Hessian for several functions.\nLemma D.45 (Hessian of u(x), Lemma 5.9 in [105]). Provided that the following conditions are\ntrue\n\u2022 Given a matrix A \u2208 Rn\u00d7d.\n\u2022 For every i \u2208 [d], let A\u2217,i \u2208 Rn denote the i-th column of matrix A.\n\u2022 Let \u25e6 denote hadamard product.\nThen, we have, for each i \u2208 [d]\n\u2022 Part 1.\nd2 exp(Ax)\ndx2\ni\n= A\u2217,i \u25e6 u(x) \u25e6 A\u2217,i\n\u2022 Part 2.\nd2 exp(Ax)\ndxidxj\n= A\u2217,j \u25e6 u(x) \u25e6 A\u2217,i\nLemma D.46 (Lemma 5.10 in [105]). Provided that the following conditions are true\n\u2022 We define \u03b1(x) as Definition D.36.\n\u2022 For every i \u2208 [d], let A\u2217,i \u2208 Rn denote the i-th column of matrix A.\n\u2022 Let \u25e6 denote hadamard product.\nThen, we have\n\u2022 Part 1.\nd2\u03b1(x)\ndx2\ni\n= \u27e8u(x), A\u2217,i \u25e6 A\u2217,i\u27e9\n\u2022 Part 2.\nd2\u03b1(x)\ndxidxj\n= \u27e8u(x), A\u2217,i \u25e6 A\u2217,j\u27e9\n\u2022 Part 3.\nd2\u03b1(x)\ndx2\n= A\u22a4 diag(u(x))A\nD.20\nHessian is Positive Definite\nIt is well known that in literature [104, 105, 107], the Hessian H of loss function can be written as\nA\u22a4(B(x) + W 2)A for some matrix function B(x) \u2208 Rn\u00d7n (for example see explanation in Section\n5.10 in [105]). In this section, we show that Hessian is positive definite.\nLemma D.47. If the following conditions hold\n\u2022 Given matrix A \u2208 Rn\u00d7d.\n\u2022 We define Lsparse(x) as Definition D.41.\n\u2022 We define Lsparse(x) Definition D.40.\n\u2022 We define Lsparse(x) Definition D.39.\n\u2022 Let L(x) = Lexp(x) + Lsparse(x) + Lreg(x).\n46\n\u2022 Let A\u22a4(B(x) + W 2)A be the Hessian of L(x)\n\u2022 Let W = diag(w) \u2208 Rn\u00d7n. Let W 2 \u2208 Rn\u00d7n denote the matrix that i-th diagonal entry is\nw2\ni,i.\n\u2022 Let \u03c3min(A) denote the minimum singular value of A.\n\u2022 Let l > 0 denote a scalar.\nThen, we have\n\u2022 Part 1. If all i \u2208 [n], w2\ni \u2265 20 + l/\u03c3min(A)2, then\nd2L\ndx2 \u2ab0 l \u00b7 Id\n\u2022 Part 2 If all i \u2208 [n], w2\ni \u2265 200 \u00b7 exp(R2) + l/\u03c3min(A)2, then\n(1 \u2212 1/10) \u00b7 (B(x) + W 2) \u2aaf W 2 \u2aaf (1 + 1/10) \u00b7 (B(x) + W 2)\nProof. The entire proof framework follows from [104, 105, 107], in the next few paragraphs, we\nmainly explain the difference.\nThe B(x) based on Lsparse is diag(u(x)). Note that it is obvious that\ndiag(u(x)) \u2ab0 0.\nFrom the upper bound size, we know that\ndiag(u(x)) \u2aaf \u2225u(x)\u2225\u221e \u00b7 In\n\u2aaf exp(R2)\nwhere the last step follows from Proof of Part 0 in Lemma 7.2 in [105].\nTo prove Part 1, following from [104, 105], we only use the lower bound of diag(u(x)). By putting\nthings together, we get our results.\nTo prove Part 2, we follow from [107] and use both the upper bound and lower bound of diag(u(x)).\nD.21\nHessian is Lipschitz\nIn this section, we show Hessian is Lipschitz.\nLemma D.48. If the following conditions hold\n\u2022 Let H(x) = A\u22a4(B(x) + W 2)A denote the Hessian of L.\nThen, we have\n\u2022 \u2225H(x) \u2212 H(y)\u2225 \u2264 n2 exp(40R2)\u2225x \u2212 y\u22252\nProof. The entire proof framework follows from [104, 105, 107], in the next few paragraphs, we\nmainly explain the difference.\nNote that the B(x) based on Lexp + Lreg have been proved by [105]\nWe only need to prove B(x) based on Lsparse and add them together.\nNote that B(x) based on Lsparse is in fact diag(u(x)).\nUsing Lemma 7.2 in [105], we know that\n\u2225 diag(u(x)) \u2212 diag(u(y))\u2225 \u2264 \u2225u(x) \u2212 u(y)\u22252\n\u2264 2\u221anR exp(R2)\u2225x \u2212 y\u22252\nwhere the last step follows from Part 1 in Lemma 7.2 in [105].\nThus, putting things together, we complete the proof.\n47\nD.22\nGreedy Type Algorithm\nIn this section, we propose a greedy-type algorithm (based on the approximate Newton method) to\nsolve the optimization problem.\nAlgorithm 4 A greedy type algorithm.\n1: procedure OURITERATIVEMETHOD(A \u2208 Rn\u00d7d, b \u2208 Rn, w \u2208 Rn, \u03f5, \u03b4)\n2:\nInitialize x0\n3:\nT \u2190 log(\u2225x0 \u2212 x\u2217\u22252/\u03f5)\n\u25b7 Let T denote the number of iterations.\n4:\nfor t = 0 \u2192 T do\n5:\nD \u2190 Bdiag(xt) + diag(w \u25e6 w)\n6:\neD \u2190 SUBSAMPLE(D, A, \u03f51 = \u0398(1), \u03b41 = \u03b4/T)\n7:\nCompute gradient g exactly\n8:\nGet the approximate Hessian eH by computing A\u22a4 eDA\n9:\nUpdate xt+1 by using the Newton step xt + eH\u22121g\n10:\nend for\n11:\nex \u2190 xT +1\n12:\nreturn ex\n13: end procedure\nTheorem D.49. Given matrix A \u2208 Rn\u00d7d, b \u2208 Rn, and w \u2208 Rn.\n\u2022 We use x\u2217 to denote the optimal solution of\nmin\nx\u2208Rd Lexp + Lsparse + Lreg\nthat\n\u2013 g(x\u2217) = 0d, where g denotes the gradient function.\n\u2013 \u2225x\u2217\u22252 \u2264 R.\n\u2022 Suppose that R \u2265 10, M = exp(\u0398(R2 + log n)), and l > 0.\n\u2022 Assume that \u2225A\u2225 \u2264 R. Here \u2225A\u2225 denotes the spectral norm of matrix A.\n\u2022 Suppose that b \u2265 0n and \u2225b\u22251 \u2264 1. Here 0n denotes a length-n vector where all the entries\nare zeros. (Here b \u2265 0n denotes bi \u2265 0 for all i \u2208 [n])\n\u2022 Assume that w2\ni \u2265 200 \u00b7 exp(R2) + l/\u03c3min(A)2 for all i \u2208 [n]. Here \u03c3min(A) denotes the\nsmallest singular value of matrix A.\n\u2022 Let x0 denote an starting/initial point such that M\u2225x0 \u2212 x\u2217\u22252 \u2264 0.1l.\n\u2022 We use to \u03f5 \u2208 (0, 0.1) represent our accuracy parameter.\n\u2022 We use \u03b4 \u2208 (0, 0.1) to represent failure probability.\nThere is a randomized algorithm that\n\u2022 runs log(\u2225x0 \u2212 x\u2217\u22252/\u03f5) iterations\n\u2022 spend\nO((nnz(A) + d\u03c9) \u00b7 poly(log(n/\u03b4))\ntime per iteration,\n\u2022 and finally outputs a vector ex \u2208 Rd such that\nPr[\u2225ex \u2212 x\u2217\u22252 \u2264 \u03f5] \u2265 1 \u2212 \u03b4.\n48\nProof. The proof framework follows from approximate Newton (second order method) literature\n[134, 135, 136, 137, 138, 139, 140, 141, 104, 105, 107, 110, 111, 142, 143, 144, 145].\nFollowing from Lemma D.47, we know the Hessian of the loss function is positive definite.\nFollowing from Lemma D.48, we know the Hessian of L is Lipschitz.\nFollowing Section 9 in [105], by running Algorithm 4, we complete the proof.\nWe remark that \u03c9 denotes the exponent of matrix multiplication (i.e., n\u03c9 is the time of multiplying\nan n \u00d7 n matrix with another n \u00d7 n matrix). The most naive algorithm gives \u03c9 = 3. Currently, the\nstate-of-the-art algorithm gives \u03c9 = 2.373.\n49\n"
  },
  {
    "title": "Language models are weak learners",
    "link": "https://arxiv.org/pdf/2306.14101.pdf",
    "upvote": "9",
    "text": "Language models are weak learners\nHariharan Manikandan1\nYiding Jiang1\nJ Zico Kolter1,2\n1Carnegie Mellon University\n2Bosch Center for AI\n{hmanikan, yidingji, zkolter}@cs.cmu.edu\nAbstract\nA central notion in practical and theoretical machine learning is that of a weak\nlearner, classifiers that achieve better-than-random performance (on any given\ndistribution over data), even by a small margin. Such weak learners form the\npractical basis for canonical machine learning methods such as boosting. In this\nwork, we illustrate that prompt-based large language models can operate effectively\nas said weak learners. Specifically, we illustrate the use of a large language model\n(LLM) as a weak learner in a boosting algorithm applied to tabular data. We show\nthat by providing (properly sampled according to the distribution of interest) text\ndescriptions of tabular data samples, LLMs can produce a summary of the samples\nthat serves as a template for classification and achieves the aim of acting as a\nweak learner on this task. We incorporate these models into a boosting approach,\nwhich in some settings can leverage the knowledge within the LLM to outperform\ntraditional tree-based boosting. The model outperforms both few-shot learning\nand occasionally even more involved fine-tuning procedures, particularly for tasks\ninvolving small numbers of data points. The results illustrate the potential for\nprompt-based LLMs to function not just as few-shot learners themselves, but as\ncomponents of larger machine learning pipelines.\n1\nIntroduction\nWeak learners refer to classifiers that are able to attain better performance than random chance, by\nsome given margin, on any specified distribution over training data. One of the early breakthroughs in\nmachine learning established that this weak learning was sufficient for arbitrarily strong classification,\nvia an ensembling procedure [Schapire, 1990]. This approach in turn led to the development of\nboosting algorithms [Freund and Schapire, 1997], a class of approaches that continue to perform\nextremely well, particularly on tabular datasets that lack the input space regularity of vision or\nlanguage tasks.\nIn a seemingly separate thread of research, large language models (LLMs) based on transform-\ners [Vaswani et al., 2017] in recent years have come to dominate many natural language domains.\nThese models are often finetuned on the data of new downstream tasks [Devlin et al., 2018, Liu\net al., 2019], but in recent years have also been shown to exhibit strong performance as zero-shot or\nfew-shot learning solely via prompting the model [Brown et al., 2020] with a piece of context string.\nIn this paper, we align these two threads of research and ask a simple question: can LLMs also serve\nas weak learners in a boosting framework, specifically on tabular data (where boosting methods\nare most commonly applied)? We answer this question largely in the affirmative. Specifically, we\nshow that by appropriately converting tabular data to text form, and asking LLMs to summarize a\ncarefully chosen set of examples from the data, we produce a summary of the examples that can serve\nas a template (i.e., a prompt) for a tabular data classifier, and one which typically achieves this weak\nlearning aim. This enables us to correspondingly integrate this collection of LLM-generated weak\nlearners into a boosting framework.\nPreprint. Under review.\narXiv:2306.14101v1  [cs.LG]  25 Jun 2023\nWe show that the resulting approach performs well in many settings, easily outperforming zero-shot\nand few-shot classification, as well as \u201csingle-shot\u201d summaries generated by the LLM. This is all done\nwithout any retraining or finetuning of the LLM itself, but rather only via prompting. Furthermore,\non certain domains (particularly those with very few examples, where leveraging the prior knowledge\nbuilt into LLMs would be of particular importance), we show that the approach can even outperform\ntraditional tree-based boosting and LLM-based finetuning methods and its performance would likely\nimprove as LLMs capabilities improve. Overall, we believe this work highlights the potential of\nincorporating LLMs as sub-routines of a larger machine learning system.\n2\nRelated Works\nDeep Learning for Tabular Data.\nTabular data refers to a generic data format that represents data\nas a collection of discrete or continuous attributes [Borisov et al., 2021]. Due to their flexibility,\ntabular data are ubiquitous in many ML settings. However, such flexibility comes with a cost \u2013 they\nlack the inherent structure found in images or text, which makes applying deep learning to them\nchallenging. Furthermore, they are often domain-specific and may have a relatively small number\nof data points. As a result, traditional deep learning methods, which thrive on large datasets and\nhigh-dimensional data, have seen limited success when applied to tabular data [Gorishniy et al.,\n2021, Shwartz-Ziv and Armon, 2022].\nRecently, however, there has been increasing interest in applying deep learning to tasks related to\ntables such as data integration, imputation [Narayan et al., 2022], semantic parsing, and even running\nSQL queries [Herzig et al., 2020, Yin et al., 2020]. Deep learning models have also been successful at\nlearning tabular data classification by optimizing loss functions [Hollmann et al., 2022, Sch\u00e4fl et al.,\n2022, Dinh et al., 2022]. Unlike these approaches, we study how we can use LLM for classifying\ntabular data without finetuning or building a new language model. Since many tabular data can be\ngrounded in natural language, texts are in fact a natural representation for tabular data. Motivated by\nthe observation that LLMs can convert tables to text through prompting alone [Saha et al., 2022], we\nutilize LLMs to do this conversion. After the conversion, our classification algorithm also interacts\nwith existing LLMs strictly through prompts. This creates an abstraction between the underlying\nlanguage model and the learning procedure which may be desirable for various applications since\naccess to the gradients or parameter updates are not required.\nPrompting\nPrompting [Liu et al., 2023] refers to providing initial text or instructions to guide\nthe response of a language model. The advancements in Language Model-based Learning (LLM)\nhave unveiled new capabilities, such as chain of thought reasoning [Wei et al., 2022], zero-shot\nreasoning [Kojima et al., 2022], compositional problem solving [Zhou et al., 2022a], and self-\nimprovement [Huang et al., 2022, Ho et al., 2022, Haluptzok et al., 2022]. As a result, prompting has\ngained widespread application across various Natural Language Processing (NLP) tasks, including\narithmetic, common reasoning [Wang et al., 2022b], among others [Brown et al., 2020]. While\nprompts offer flexibility, it is crucial to note that LLMs interpret them differently from humans.\nTherefore, the process of prompt tuning, which involves carefully engineering prompts, becomes\nessential for obtaining accurate and relevant outputs [Reynolds and McDonell, 2021]. At its core,\nprompt tuning is an optimization process that aims to find the best prompt for a certain downstream\ntask. Though a long line of works propose gradient-guided search to optimize \u201ccontinuous prompt\u201d\ninstead of the language tokens [Liu et al., 2021, Qin and Eisner, 2021, Lester et al., 2021, Shin et al.,\n2020, Rakotonirina et al., 2023, Wang et al., 2022c, Diao et al., 2023], gradient-based updates can be\nlimiting, as LLMs become bigger and the access to these models become increasingly API-based. Our\napproach aligns more with discrete search methods based on the fact that LLMs can automatically\ngenerate prompts for themselves [Zhou et al., 2022b, Zhang et al., 2022, Yu et al., 2022]. Specifically,\nwe prompt the LLM to summarize the tabular dataset. The summary in turn acts as a prompt that the\nLLM uses to make predictions as it encodes knowledge of the dataset. A sequence of such prompts\nsummarizing different subsets of the data can be seen as weak learners for a boosting procedure.\nBoosting\nBoosting [Schapire, 1990, Freund and Schapire, 1997] is a widely used technique to\nimprove the accuracy of a model by combining weak learners (models that perform slightly better\nthan random guessing) to make a strong learner (model with high accuracy). Common boosting\nalgorithms include AdaBoost [Freund and Schapire, 1997], Gradient Boosting [Friedman, 2001], and\nStochastic Gradient Boosting [Friedman, 2002]; the XGBoost library [Chen and Guestrin, 2016] in\nparticular is a commonly used implementation of gradient boosting. In concurrent work most relevant\n2\n{Metadata}\nHere is one example from this dataset.\nGoal: Describe the given data in words.\n{Data as Text}\nUse your creativity to describe this data accurately and concisely. Do not add any additional information.\nLLM\nfresh products\nmilk products\ngrocery products\nfrozen products\ndetergents and \npaper products\ndelicatessen \nproducts\ncustomer's region\ncustomer's \nchannel\n7057.0\n9810.0\n9568.0\n1762.0\n3293.0\n1776.0\nOutside Lisbon and \nPorto\nRetail\nData \nPreprocessor\nConcatenate the \nground truth\nThe dataset refers to clients of a \nwholesale distributor. It includes the \nannual spending in monetary units (m.u.) \non diverse product categories.\nMetadata\nThe customer's spending on fresh products is low, while \nspending on milk products, grocery products, and \ndetergents and paper products is high. Spending on \nfrozen products and delicatessen products is medium. \nThe customer is from outside Lisbon and Porto. ### \nHence this customer\u2019s channel was Retail.\n\u2022\nfresh products : medium \n\u2022\nmilk products : medium \n\u2022\ngrocery products : high \n\u2022\nfrozen products : very less\n\u2022\ndetergents and paper products : high\n\u2022\ndelicatessen products : medium\n\u2022\ncustomer's region : Outside Lisbon and Porto\nData as Text\nData Conversion prompt\nData Description\nFigure 1: The conversion for a data point on the Wholesale customers dataset (OpenML ID 1511).\nto ours, Hou et al. [2022] integrate LLM into AdaBoost for natural language inference (NLI) tasks,\nby training an MLP projection of the final hidden state of a special token. Our proposed method\ndiverges from theirs in two key aspects. Firstly, our method avoids the overhead of learning additional\nparameters, is gradient-free, and does not require access to the model\u2019s internal states. Secondly,\ninstead of storing knowledge in parameters, our approach concentrates on condensing knowledge\ninto an intermediary representation referred to as \"summary.\" This alternative strategy enhances\ninterpretability and strictly learns through prompts, rendering it particularly suitable for small tabular\ndata, where the prior knowledge in LLM can significantly benefit the learning process.\n3\nSummary Boosting with Language Models\nWe now describe the main methodology of our paper, which uses LLMs to generate weak learners,\nand in turn, uses these weak learners within a boosting framework. We refer to the full method\nas Summary Boosting, as the core learning process is one that uses a language model to create a\nsummary of (specifically chosen) samples from the dataset; these summaries themselves function as\nprompts by which we can make predictions on new examples. Finally, we use boosting to construct\nan ensemble of these summaries that gives the overall predictions on new data points.\n3.1\nData conversion\nTo utilize large language models (LLMs) with tabular data, it is necessary to first convert the records\ninto natural language descriptions. We will refer to these as data descriptions. Template matching,\ncommonly used in previous approaches [Dinh et al., 2022], inserts attribute values into predefined\ntemplates. However, this approach often produces unnatural descriptions that differ from how humans\nmight describe the data. Depending on the dataset, designing the template by hand can also be\nchallenging. To overcome this, we propose using LLMs as a more suitable solution.\nAs illustrated in Figure 1, we can get these data descriptions with little effort by zero-shot prompting\nthe LLM with information about the dataset (which is generally available as metadata for tabular\ndatasets) and a textual representation of the tabular record (e.g., parsed JSON). Specifically, to\nensure examples can serve as both training data and query inputs, we extract the descriptions of the\nfeatures and concatenate them with the target label using a separator token (refer to Appendix A.1).\nInterestingly, we find that descriptions generated by LLM this way often perform better than those\nfrom a template. This ablation study can be found in Section 5.\nOne key challenge in this process is how to encode numerical attributes effectively; naively including\nnumerical values in the descriptions can lead to poor performance in subsequent learning tasks. To\naddress this, we adopt a straightforward approach: we bin all numerical features into percentiles\nand encode them descriptively as \u201clow,\u201d \u201cmedium,\u201d and \u201chigh,\u201d. In Section 5, we compare the\nperformance of several such approaches and discuss more examples in Appendix A.6. Overall, the\ndata descriptions can be generated automatically with minimal manual engineering.\n3\n1.\nThis customer spends medium amounts on \nfresh, milk, and delicatessen products, high \namounts on grocery and detergents and paper \nproducts, and very little on frozen products. \nThis customer is located outside of Lisbon and \nPorto. ### Hence this customer channel is \nRetail. \n150.The client spends a lot on frozen, delicatessen, \nand fresh products. They spend a very low \namount on milk, products, and detergents and \npaper products. This client is located outside of \nLisbon and Porto. ### Hence this customer \nchannel was Horeca (Hotel/Restaurant/Caf\u00e9).\nStratified \ncluster \nsampling\n{Metadata}\n{Rich Representative support set \nexamples}\nTl;dr / Summarize in detail how can \nwe tell whether the customer \nchannel is Retail or Horeca.\nBased on the data, a Horeca \n(Hotel/Restaurant/Caf\u00e9) \ncustomer is someone who \nspends a low amount on all \nproduct categories, with the \nexception of delicatessen \nproducts. A Retail customer, \u2026.\nLLM\nThis is a customer who spends very little \non fresh, milk, grocery, and frozen \nproducts, and spends less on detergents \nand paper products and delicatessen \nproducts. The customer is from outside of \nLisbon and Porto.\nGoal: Knowledge to Prediction.\n{Metadata}\n{Summary}\nNow, {query}\nTherefore, this customer\u2019s channel will be \n(Horeca or Retail): \nLLM\n{answer}\nRecords as text\nSummarization prompt\nCandidate summaries\nSelect best \nsummary\nInference prompt\nQuery\nInference\nHypothesis generation\nThe dataset refers to clients of a \nwholesale distributor. It includes \nthe annual spending in monetary \nunits (m.u.) on diverse product \ncategories.\nMetadata\nFigure 2: The process of generating summaries and using them to make predictions on new data.\nThe top half describes how the weak learning hypothesis (summary) is generated. The bottom half\nillustrates how the summary is used to perform inference.\n3.2\nWeak learning via summarization\nA typical method for performing few-shot learning with large language models (LLMs) involves\nproviding a small number of demonstrations of the intended task as a prompt and then asking the\nmodel to generate an answer. One could, for instance in the few-shot setting, simply present the\nnatural language descriptions above and generate predictions on new examples. However, for tabular\ndata, there may be a larger number of data points that do not fit within the LLM context. Furthermore,\nwe observed that increasing the number of examples in the context naively does not always improve\nperformance (Figure 4, right bottom), and there was no obvious way to manage weighted distribu-\ntions over examples as is required in boosting methods. These observations necessitate alternative\napproaches to weak learning via LLMs.\nWe propose instead that producing summaries of a collection of examples can serve as a powerful\nproxy for learning models based upon some number of examples, as summarization naturally\nencourages extraction of representative information in the data. Concretely, given a set of data\ndescriptions, we first perform summarization on the data by calling the LLM (e.g., by concatenating a\nlist of examples in natural language form and appending the prompt \u201ctldr\u201d). This resulting summary\ncan be seen as a hypothesis as it provides an explanation for the data. By using the summary as a\nprompt, the LLM in turn uses the hypothesis to perform inference instead of the raw data description\n(shown in Figure 2). Since the sampled summary can sometimes be noisy, we generate a fixed number\nof summaries and pick the one with the smallest validation error rate. In case of a tie, we choose the\none with a higher training error, i.e., lower generalization gap (see Appendix A.2). Several methods of\nbuilding such summaries are possible, but simple approaches such as the \u201ctldr\u201d approach mentioned\nabove tend to work as well as more sophisticated alternatives, as we show in Section 5.\n(Weighted) Cluster Sampling.\nSince the context size of existing LLMs is still limited, we cannot\nin general fit the entire dataset into the context for summarization. Furthermore, boosting algorithms\nrequire that we provide weak learners on weighted samples of the training set, effectively guiding the\nboosting process to focus on \u201charder\u201d examples as the boosting process continued. Thus, instead\nof attempting to summarize the entire dataset, we propose to use only a representative subset of\nthe dataset. The size of this subset is governed by the maximum context size and size of the\ndata descriptions. To select this representative subset, we use weighted stratified sampling using\nsubpopulations defined by clusters of language embeddings of each data description. The language\nembeddings are sentence representations generated by GPT-3. In particular, we use hierarchical\nagglomerative clustering [Nielsen, 2016] to identify clusters in the embedding. This process is shown\nin Algorithm 1. As we will show in Section 5, this process is able to consistently produce weak\nlearners, and able to improve upon random guessing under the distribution of interest (denoted by the\ninput p to the algorithm). We share more details in Appendix A.7.\n4\nAlgorithm 1 Cluster Sampling\n1: Input: X, all training data; y, all training label; r, ratio of classes; p, AdaBoost weights of the\ncurrent round; s, target number of samples.\n\u25b7 r[k] is the proportion of examples in class k.\n2: S \u2190 new empty set\n3: w \u2190 new array with same length as X filled with -1. \u25b7 w[i] is probability of sampling example i.\n4: for k = 1 to number of target classes in y do\n5:\nE \u2190 GPTEmbedding(X[y == k])\n\u25b7 E refers to the embeddings of the data descriptions\n6:\nC \u2190 AgglomerativeClustering(E).\n\u25b7 Cj is set of data indices present in the jth cluster.\n7:\nc \u2190 new empty array same size as C.\n\u25b7 c[j] will store sampling probability of cluster j.\n8:\nfor j = 1 to len(C) do\n9:\nc[j] \u2190\nlen(X)\nlen(Cj)\n10:\nend for\n11:\nfor i = 1 to len(X) do\n12:\nw[i] \u2190 c[j], such that, i \u2208 Cj\n13:\nend for\n14:\nw \u2190 Normalize(Normalize(w) \u00d7 p) \u25b7 Normalize turns weights to a probability distribution.\n15:\nSample s \u00d7 r[c] examples from X using categorical distribution w and append to S.\n16: end for\n17: Return S\nAlgorithm 2 Summary Boosting [Compact]\n1: Input: X, all training data; y, all training label; T: maximum number of rounds; s: size of the\nsampling subset; r: ratio of classes.\n\u25b7 r[k] denotes the proportion of examples in class k.\n2: h, \u03f5, \u03b1 \u2190 new empty arrays of length T.\n3: N \u2190 len(X); K \u2190 number of target classes in y.\n4: w \u2190 new array of length N filled with 1\nN.\n\u25b7 w is the data distribution\n5: for r = 1 to T do\n6:\n(Xs, ys) \u2190 ClusterSampling(X, y, r, w, s) \u25b7 sample s training examples from distribution w.\n7:\nh[r] \u2190 Summary(Xs, ys)\n\u25b7 h[r] is the weak learner in the current round r.\n8:\n\u03f5[r] \u2190\nPN\ni=1 w[i]\u00d71{h[r](X[i])\u0338=y[i]}\nPN\ni=1 w[i]\n\u25b7 \u03f5[r] is the weighted error at round r.\n9:\nif \u03f5[r] \u2265 1 \u2212 1\nK then\n10:\nGoto Step 6.\n11:\nend if\n12:\n\u03b1[r] \u2190 log\n\u0000 1\u2212\u03f5[r]\n\u03f5[r]\n\u0001\n+ log(K \u2212 1) \u25b7 \u03b1[r] refers to coefficient of the hypothesis at round r.\n13:\nfor i = 1 to N do\n14:\nw[i] = w[i] \u00d7 exp(\u03b1[r]1{h[r](X[i]) \u0338= y[i]})\n15:\nend for\n16:\nw \u2190 Normalize(w)\n17: end for\n18: Return h, \u03b1\n3.3\nBoosting\nFinally, we use the AdaBoost [Freund and Schapire, 1997] algorithm to produce an ensemble with\nthese collections of summary-based weak learners. The central idea of AdaBoost is to fit a sequence\nof weak learners on repeatedly modified versions of the data. The algorithm is carried out over T\nrounds, where the weights of the training data points are adjusted based on the training error.\nGiven a new data point, the predictions from classifiers from all rounds are then combined through a\nweighted majority vote to produce the final prediction. We use the error on a holdout validation set to\ndetermine the number of rounds T. A compact version of this process is presented in Algorithm 2.\nIn the algorithm, the Summary method summarizes the examples in the prompt via the process\ndiscussed in Section 3.2. Each summary can be treated as a hypothesis that can classify new data.\nHowever, unlike the summary process in Section 3.2, where we resample multiple times to find the\nbest learner, the boosting process returns immediately when a summary with an error rate better than\nrandom guessing is found (refer Appendix A.2). We use ClusterSampling to subsample a mini-batch\n5\nof examples that fit within the LLM\u2019s max context length. Appendix A.10 dissects time complexity\nanalysis of this method and A.8 covers full version of our boosting procedure that works in practice.\n4\nExperiments\nWe conduct all of our experiments with OpenAI\u2019s GPT-3 API [Brown et al., 2020] and choose a\ncollection of 18 tabular datasets from the UCI dataset [Dua and Graff, 2017] and OpenML [Vanschoren\net al., 2014]. All main experiments are done with the Curie variant of GPT-3 unless otherwise\nspecified, which has 13B parameters1. We compare the following methods:\n\u2022 Zero-shot: query the language model with the data description and ask the model to\ncomplete the answer (refer Appendix A.4).\n\u2022 Few-shot: provide a few labeled data descriptions of the training data as the context and\nask the model to complete the answer for a new data description. To preserve consistency,\nwe standardize the number of fewshot examples to approximately 15 for all datasets. The\nsetting is explained in Appendix A.5.\n\u2022 Summary (ours): generate a population of summaries given a list of data descriptions\nwith cluster sampling and pick the summary with the lowest validation error; use the best\nsummary as the context and ask the model to complete the answer for a new data description.\n\u2022 Summary Boosting (ours): use Summary as a subroutine in AdaBoost.\nFurthermore, we compared Summary Boosting against popular baselines for tabular data that do\nnot use prompting:\n\u2022 KNN: first embed the data descriptions with the GPT-3 embedding API 2 and then use K-\nnearest neighbor to classify a new data description. This simple baseline demonstrates how\nmuch information can the naive representation produced by LLMs provide about the tasks.\n\u2022 LIFT [Dinh et al., 2022]: Language-Interfaced Fine-Tuning (LIFT) finetunes the LM with\ndata descriptions (without binning) and their corresponding labels in a zero-shot.\n\u2022 TabPFN [Hollmann et al., 2022]: TabPFN is a transformer-based architecture that performs\nBayesian inference on the entire training and test data points at the same time.\n\u2022 XGBoost [Chen and Guestrin, 2016]: XGBoost (eXtreme Gradient Boosting) is a regularized\ngradient boosting algorithm that is widely used for tabular data.\nFor each method and dataset, we use a 50/10/40 split for train, validation, and test sets and repeat\neach experiment for 3 random seeds The results are shown in Table 1 and 2.\n4.1\nAnalysis of prompting-based methods\nAs a general trend from Table 1, test performance improves in the order of Zero-shot < Few-shot\n< Summary < Summary Boosting. Firstly, unlike most works on zero-shot reasoning with LLMs,\nthe LLMs do not have enough prior knowledge to make the correct prediction without additional\ninformation. As a result, we observe that Zero-shot performs poorly on all of the datasets. This\nobservation highlights the necessity of learning from the data, and unlike other tasks, the LLMs\nthemselves do not have enough built-in knowledge to succeed at tabular data zero-shot. Since\nzero-shot does not have enough prior knowledge to classify tabular data, we use few-shot in-context\nlearning (Few-shot) to see if the added information helps make better predictions. As expected,\non all the datasets other than visualizing-hamster, and wholesale-customers, Few-shot\nconsistently improves the test performance compared to Zero-shot, suggesting that this added\ninformation is crucial for LLMs to work on tabular datasets.\nUnlike naively stacking examples inside the prompt in Few-shot, Summary condenses knowledge\nfrom these examples and is the first important algorithmic component of our framework for creating\nweak learners using LLMs. We see that Summary consistently improves upon Few-shot on all the\ndatasets other than haberman-survival and wholesale-customers. This observation suggests\n1We use Curie because it is more cost-effective for large-scale experiments.\n2https://beta.openai.com/docs/guides/embeddings\n6\nTable 1: Test errors for prompting-based methods on all datasets (\u2193). Data Type indicates the number\nand types of attributes the dataset has (c is continuous and d is discrete). Size denotes the number of\ndata points. In square bracket (if present) next to every dataset name, we provide its acronym referred\nto in our main text. In the bracket next to each dataset name is either the OpenML ID of the dataset\nor a reference to the dataset\u2019s associated publication. Error represents one standard deviation.\nDataset\nData Type\nSize\nZero-shot\nFew-shot\nSummary\nSummary Boosting\ncaesarian [cae] (42901)\n1c4d\n80\n0.425\u00b1 0.04\n0.388\u00b1 0.02\n0.350\u00b1 0.04\n0.300\u00b1 0.04\niris (61)\n4c0d\n150\n0.680\u00b1 0.02\n0.460\u00b1 0.01\n0.275\u00b1 0.07\n0.193\u00b1 0.03\ntae (48)\n1c4d\n151\n0.556\u00b1 0.07\n0.494\u00b1 0.01\n0.474\u00b1 0.02\n0.454\u00b1 0.03\nglass (41)\n9c0d\n214\n0.486\u00b1 0.01\n0.473\u00b1 0.01\n0.466\u00b1 0.02\n0.370\u00b1 0.02\nbreast-cancer [bc] (13)\n7c5d\n277\n0.754\u00b1 0.02\n0.516\u00b1 0.02\n0.337\u00b1 0.02\n0.288\u00b1 0.02\nvisualizing-environmental [ve] (678)\n3c0d\n111\n0.522\u00b1 0.01\n0.366\u00b1 0.01\n0.304\u00b1 0.02\n0.268\u00b1 0.03\nanalcatdata-chlamydia [ac] (535)\n2c2d\n100\n0.200\u00b1 0.00\n0.200\u00b1 0.00\n0.170\u00b1 0.01\n0.170\u00b1 0.01\nwine (43571)\n13c0d\n178\n0.820\u00b1 0.03\n0.674\u00b1 0.02\n0.475\u00b1 0.01\n0.320\u00b1 0.01\nblood-transfusion-center [btc] (1464)\n4c0d\n748\n0.544\u00b1 0.01\n0.430\u00b1 0.00\n0.258\u00b1 0.04\n0.240\u00b1 0.04\nsomerville-happiness-survey [shs] [Koczkodaj, 2018]\n0c7d\n143\n0.416\u00b1 0.03\n0.385\u00b1 0.03\n0.422\u00b1 0.02\n0.350\u00b1 0.02\nvehicle (54)\n18c0d\n846\n0.765\u00b1 0.00\n0.560\u00b1 0.01\n0.510\u00b1 0.02\n0.410\u00b1 0.04\nstatlog-heart [stath] [Dua and Graff, 2017]\n6c7d\n270\n0.551\u00b1 0.01\n0.528\u00b1 0.01\n0.444\u00b1 0.05\n0.430\u00b1 0.01\nverterbra-column [vc] (1524)\n6c0d\n310\n0.714\u00b1 0.03\n0.435\u00b1 0.06\n0.327\u00b1 0.01\n0.262\u00b1 0.01\necoli (1011)\n7c0d\n336\n0.581\u00b1 0.02\n0.562\u00b1 0.01\n0.480\u00b1 0.01\n0.270\u00b1 0.03\nhaberman-survival [hs] (43)\n3c0d\n306\n0.308\u00b1 0.02\n0.262\u00b1 0.01\n0.277\u00b1 0.01\n0.250\u00b1 0.01\ndiabetes [dia] (37)\n8c0d\n768\n0.446\u00b1 0.04\n0.400\u00b1 0.00\n0.360\u00b1 0.01\n0.344\u00b1 0.01\nvisualizing-hamster [hams] (708)\n5c0d\n73\n0.464\u00b1 0.03\n0.481\u00b1 0.05\n0.360\u00b1 0.02\n0.207\u00b1 0.00\nwholesale-customers [wc] (1511)\n6c1d\n440\n0.364\u00b1 0.01\n0.347\u00b1 0.01\n0.349\u00b1 0.02\n0.330\u00b1 0.00\nTable 2: Test errors for chosen methods on all datasets (\u2193). (acronym and notation in Table 1)\nDataset\nData Type\nSize\nSummary Boosting\nLIFT\nKNN\nTabPFN\nXgboost\ncae (42901)\n1c4d\n80\n0.300\u00b1 0.04\n0.312\u00b1 0.02\n0.300\u00b1 0.00\n0.425\u00b1 0.07\n0.412\u00b1 0.05\niris (61)\n4c0d\n150\n0.193\u00b1 0.03\n0.100\u00b1 0.01\n0.106\u00b1 0.02\n0.027\u00b1 0.00\n0.054\u00b1 0.04\ntae (48)\n1c4d\n151\n0.454\u00b1 0.03\n0.480\u00b1 0.04\n0.532\u00b1 0.01\n0.450\u00b1 0.13\n0.464\u00b1 0.01\nglass (41)\n9c0d\n214\n0.370\u00b1 0.02\n0.218\u00b1 0.02\n0.294\u00b1 0.03\n0.158\u00b1 0.05\n0.254\u00b1 0.05\nbc (13)\n7c5d\n277\n0.288\u00b1 0.02\n0.318\u00b1 0.01\n0.277\u00b1 0.02\n0.264\u00b1 0.01\n0.270\u00b1 0.01\nve (678)\n3c0d\n111\n0.268\u00b1 0.03\n0.430\u00b1 0.04\n0.308\u00b1 0.01\n0.370\u00b1 0.04\n0.279\u00b1 0.02\nac (535)\n2c2d\n100\n0.170\u00b1 0.01\n0.180\u00b1 0.06\n0.170\u00b1 0.01\n0.090\u00b1 0.01\n0.110\u00b1 0.04\nwine (43571)\n13c0d\n178\n0.320\u00b1 0.01\n0.065\u00b1 0.01\n0.214\u00b1 0.05\n0.040\u00b1 0.01\n0.040\u00b1 0.01\nbtc (1464)\n4c0d\n748\n0.240\u00b1 0.04\n0.270\u00b1 0.01\n0.238\u00b1 0.00\n0.209\u00b1 0.01\n0.219\u00b1 0.01\nshs [Koczkodaj, 2018]\n0c7d\n143\n0.350\u00b1 0.02\n0.419\u00b1 0.02\n0.326\u00b1 0.03\n0.392\u00b1 0.00\n0.406\u00b1 0.00\nvehicle (54)\n18c0d\n846\n0.410\u00b1 0.04\n0.111\u00b1 0.16\n0.636\u00b1 0.01\n0.178\u00b1 0.01\n0.260\u00b1 0.00\nstath [Dua and Graff, 2017]\n6c7d\n270\n0.430\u00b1 0.01\n0.122\u00b1 0.17\n0.244\u00b1 0.03\n0.148\u00b1 0.03\n0.215\u00b1 0.00\nvc (1524)\n6c0d\n310\n0.262\u00b1 0.01\n0.192\u00b1 0.03\n0.318\u00b1 0.02\n0.135\u00b1 0.00\n0.187\u00b1 0.04\necoli (1011)\n7c0d\n336\n0.270\u00b1 0.03\n0.126\u00b1 0.03\n0.211\u00b1 0.03\n0.036\u00b1 0.02\n0.066\u00b1 0.01\nhs (43)\n3c0d\n306\n0.250\u00b1 0.01\n0.314\u00b1 0.03\n0.278\u00b1 0.00\n0.262\u00b1 0.02\n0.281\u00b1 0.02\ndia (37)\n8c0d\n768\n0.344\u00b1 0.01\n0.324\u00b1 0.04\n0.353\u00b1 0.02\n0.238\u00b1 0.03\n0.234\u00b1 0.00\nhams (708)\n5c0d\n73\n0.207\u00b1 0.00\n0.334\u00b1 0.08\n0.528\u00b1 0.02\n0.328\u00b1 0.01\n0.411\u00b1 0.01\nwc (1511)\n6c1d\n440\n0.330\u00b1 0.00\n0.125\u00b1 0.04\n0.043\u00b1 0.00\n0.088\u00b1 0.00\n0.098\u00b1 0.02\nthat summarization is a powerful way to improve few-shot performance and has potential for even\nother tasks using LLMs. Finally, for every dataset we tested, boosting with summarization consistently\noutperforms all other prompting-based approaches. This observation corroborates our hypothesis that\nLLMs with summarization are a good candidate for creating weak learners in boosting.\n4.2\nComparison to other tabular methods\nIn Table 2, we also observe that LLMs have a hard time reasoning about continuous attributes\nwithout finetuning, especially on the glass, wine, iris and vehicle datasets. In particular,\nwhen the datasets have many continuous features, the performance of Summary Boosting can be\nconsiderably worse than other methods such as LIFT or Xgboost. This may be because LLMs\nare fairly bad at quantitive reasoning without finetuning [Lewkowycz et al., 2022], which may be\novercome in future LLMs.\nWhile KNN is a relatively simple baseline, its performance is surprisingly good at a few tasks such\nas wholesale-customers, statlog-heart, ecoli and wine. This highlights that LLMs have a\nremarkable amount of general prior knowledge about the worlds compared to methods like XGboost\nthat sometimes this knowledge alone can produce good performances.\nFinally, we observe that Summary Boosting performs very well when the size of the dataset is very\nsmall. This makes sense since the strength of using LLMs as weak learners is that they have a large\namount of generic prior about the world from pre-training. When the dataset is large, this prior\nknowledge might become less relevant and methods like finetuning become more competitive.\n7\n0.0\n0.2\n0.4\nTest error rate\nwine\ncae\ntae\nve\nbc\nac\nglass\nshs\nbtc\nDataset\nSize of LLM\nCurie\nDavinci\n0.0\n0.5\nTest error rate\nTask agnostic training\nw/ names\nw/o names\n0\n50\n100\nNumber of rounds\nCluster\nRandom\n0.0\n0.2\n0.4\nTest error rate\n0.25\n0.50\n0.75\nValidation error rate\nType of Data description\nTemplate\nGPT-3 created\nEffects of Sampling on boosting\nFigure 3: Ablation experiments compared for the Summary method. The dataset acronymns are\nreferred to 1. Left: Performance of curie vs davinci. Second from left: Comparison with and\nwithout task-specific attribute names. Middle: Effect of Cluster vs. Random sampling on the number\nof rounds till convergence and Second from Right: their final test errors. Right: Performance of\ntemplatized vs LLM-generated data descriptions.\n0.25\n0.50\n0.75\n1.00\ntae\nve\nac\nglass\nExamples ordering\nGrouped\nShuffled\n0.5\n1.0\nSummary prompt\nDetailed\nTl;dr\n0.2\n0.4\n0.6\n0.8\nInference prompt\nPrefix\nCoT\nValidation error rate\n4bins + qfrs\n5bins+ qfrs\n7bins+qfrs\n9 bins+qfrs\n10 bins\npercentile\nstd dev\nquartiles\n0.0\n0.2\n0.4\n0.6\nValidation error rate\nTransformations on continuous attributes (Wine dataset)\n10\n20\n0.3\n0.4\n0.5\n0.6\n0.7\nTest error rate\ntae\nFew-shot\nSummary\n10\n20\nbc\n10\n20\nve\n10\n20\nvc\nComparing summary vs. few-shot with support set size\nSupport set size\nFigure 4: Additional ablations for the Summary method. (Left) Prompt design choices. First plot\nshows the effect of shuffling examples vs presenting them by class. Center plot compares tldr vs a\nmore explicit prompt for inducing summary. Last plot compares prompts for doing inference. (Right\nTop) Performance of methods for discretizing continuous attributes on the Wine dataset. (Right\nBottom) Performance of Few-shot and Summary as a function of number of examples in the context.\n5\nAblations\nSummarization forms the core of our methodology for generating weak learner. Consequently, it\nbecomes important to identify ideal setting that can induce high-quality summaries. We perform\nablation studies over the Summary method, to decide hyperparameters for getting a good weak learner.\nPreprocessing of continuous attributes.\nWe tried several encoding techniques for continuous\nfeatures, including binning, percentiles, and standard deviations. We chose the approach of describing\nthem in technical language terms as well as assigning quantifiers for each level, as illustrated in Figure\n4 right top. We observed that binning with quantifiers such as \u201clow\u201d, \u201cmedium\u201d, and \u201chigh\u201d was\nmost effective for comparing examples and generating high-quality summaries. After hyperparameter\ntuning, we identified that using 5 bins provides sufficient granularity to distinguish variations in the\ncontinuous values. More details can be found in the Appendix A.6.\nDoes the LLM explore prior knowledge to infer?\nTo demonstrate the LLM\u2019s utilization of prior\nknowledge, we conduct an ablation study by masking the attribute names and using a template \u201cThis\nexample has features f1 = {}, f2 = {} and so on.\u201d Figure 3 (second from left) shows the result.\nUsing true variable names in the data descriptions leads to superior few-shot learning performance\ncompared to using dummy names. This confirms that the model indeed leverages its prior knowledge\nof variables for predictions.\n8\nHow does model size affect the performance?\nA natural question to ask is how the model size\naffects the downstream performance. We compare the Summary performances of GPT-3-davinci\n(175B parameters) and GPT-3-curie (13B parameters) on 5 datasets in Figure 3 (left). Surprisingly,\nwe find that the larger model (davinci) does not consistently improve upon the smaller model. We\nalso compare ChatGPT in A.12 and discuss the effects of RLHF [Ouyang et al., 2022].\nHow does the performance scale with more examples?\nIn Figure 4 (right bottom), we study how\nthe behavior of Few-shot and Summary change with different support set sizes (i.e., the number of\ndata descriptions that are summarized). Few-shot performance reaches an optimal size around the\nmedium context length and degrades with more examples. In contrast, Summary improves with more\ndata, which is the more desirable behavior.\nOrdering of examples.\nUnlike conventional machine learning models, the ordering of examples\nin summarization affects the generation of hypotheses. There are two approaches: 1. presenting\ndescriptions randomly (shuffled), and 2. grouping descriptions by classes (grouped). In Figure 4\n(left), we compare these approaches on 4 datasets with Summary and find no significant difference.\nWe use shuffled for all other experiments.\nDifferent summary and inference prompts.\nThe LLM easily generates concise summaries on\nstandard datasets like iris using a simple \u201ctl;dr\u201d prompt, but requires a more explicit prompt\non complex datasets like \u201cvertebra-column\u201d. Comparing their performance in Figure 4 (left), both\nprompting modes are equally effective, so detailed prompts were used in all other experiments. See\nAppendix table 3 for the complete list of summary prompts used. In the left of Figure 4, we also\ncompare two strategies for inference - prefix prompt (e.g. \u201cThis flower will be classified as\u201d), and\ntwo-stage chain-of-thought prompting (e.g. \u201cLet\u2019s think step by step\u201d) [Kojima et al., 2022]. We\nobserve no statistically significant difference between them3. Since the prefix prompt more often\ncompletes the query well under lesser compute, we use prefix prompt for all the other experiments.\nTexts generated from template vs. GPT.\nIn Figure 3 (right), we see that using GPT-generated data\ndescriptions consistently achieves better results. Refer Appendix A.9 for examples of these templates.\nThis may be due to the fact that the data description generated by LLMs conforms to natural text\ndistribution more closely, which is desirable for performing inference using LLMs.\nEffect of cluster sampling.\nCluster sampling improves the performance of the LLM by selecting a\nrepresentative set of texts that generalize better, reducing validation error faster compared to random\nsampling during boosting. Although it may require more resampling to achieve a weak learner, cluster\nsampling converges much faster than random sampling as we see in Figure 3 - middle and second\nfrom right. However, with sufficient boosting rounds, the performances of the two sampling methods\nare not statistically different.\n6\nLimitations\nAlthough the weak learning approach we develop here shows promise, there are currently several\ndrawbacks. Although summarization and boosting alleviate manual prompt tuning to large extent, we\nstill had to minimally tune some parts of the pipeline to get ideal performance (see Appendix table 3).\nAdditionally, when the dataset contains many continuous attributes, there is a non-trivial gap between\nSummary Boosting and the other methods such as XGBoost or finetuning. Finally, the max input\nlength of GPT-3 makes it harder to generate good summaries with just subset sampling. Eventually\non larger datasets, after a certain number of boosting rounds, the summaries derived from a subset\nof examples may not further decrease the weighted error across all training examples. Rigorous\ntechniques such as structured prompting handle this issue by rescaling attention weights [Hao et al.,\n2022]. We believe this issue could be solved with more powerful LLMs such as GPT-4.\n7\nConclusion\nLLMs have been widely used in recent years, not just for their generative abilities but for their\nability to serve as zero- or few-shot learners with proper prompting. This work aims to situate them\n3We do observe that CoT performs better with larger models.\n9\nwithin another context of \u201csimple\u201d learning algorithms \u2013 the weak learner paradigm that forms the\nfoundation of boosting approaches. We show that leveraging the summarization capabilities of LLMs\nindeed leads to models that function as weak learners, and which can thus be integrated into boosting\nframeworks. This result leads to new potential paradigms for treating the results of LLM prompting\nnot just as individual predictors, but as part of a larger set of meta-models as well.\nAcknowledgements\nYiding is supported by funding from Bosch Center of Artificial Intelligence.\nReferences\nV. Borisov, T. Leemann, K. Se\u00dfler, J. Haug, M. Pawelczyk, and G. Kasneci. Deep neural networks\nand tabular data: A survey. arXiv preprint arXiv:2110.01889, 2021.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020.\nT. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm\nsigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.\nG. Cui, S. Hu, N. Ding, L. Huang, and Z. Liu. Prototypical verbalizer for prompt-based few-shot\ntuning. arXiv preprint arXiv:2203.09770, 2022.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.\nBert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nS. Diao, P. Wang, Y. Lin, and T. Zhang. Active prompting with chain-of-thought for large language\nmodels. arXiv preprint arXiv:2302.12246, 2023.\nT. Dinh, Y. Zeng, R. Zhang, Z. Lin, S. Rajput, M. Gira, J.-y. Sohn, D. Papailiopoulos, and K. Lee.\nLift: Language-interfaced fine-tuning for non-language machine learning tasks. arXiv preprint\narXiv:2206.06565, 2022.\nD. Dua and C. Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.\nedu/ml.\nY. Freund and R. E. Schapire.\nA decision-theoretic generalization of on-line learning and an\napplication to boosting. Journal of computer and system sciences, 55(1):119\u2013139, 1997.\nJ. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics,\npages 1189\u20131232, 2001.\nJ. H. Friedman. Stochastic gradient boosting. Computational statistics & data analysis, 38(4):\n367\u2013378, 2002.\nY. Gorishniy, I. Rubachev, V. Khrulkov, and A. Babenko. Revisiting deep learning models for\ntabular data. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, edi-\ntors, Advances in Neural Information Processing Systems, volume 34, pages 18932\u201318943. Cur-\nran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/\n9d86d83f925f2149e9edb0ac3b49229c-Paper.pdf.\nP. Haluptzok, M. Bowers, and A. T. Kalai. Language models can teach themselves to program better.\narXiv preprint arXiv:2207.14502, 2022.\nY. Hao, Y. Sun, L. Dong, Z. Han, Y. Gu, and F. Wei. Structured prompting: Scaling in-context\nlearning to 1,000 examples. arXiv preprint arXiv:2212.06713, 2022.\nJ. Herzig, P. K. Nowak, T. M\u00fcller, F. Piccinno, and J. M. Eisenschlos. Tapas: Weakly supervised\ntable parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020.\nN. Ho, L. Schmid, and S.-Y. Yun. Large language models are reasoning teachers. arXiv preprint\narXiv:2212.10071, 2022.\n10\nN. Hollmann, S. M\u00fcller, K. Eggensperger, and F. Hutter. Tabpfn: A transformer that solves small\ntabular classification problems in a second. arXiv preprint arXiv:2207.01848, 2022.\nB. Hou, J. O\u2019Connor, J. Andreas, S. Chang, and Y. Zhang.\nPromptboosting: Black-box text\nclassification with ten forward passes. arXiv preprint arXiv:2212.09257, 2022.\nS. Hu, N. Ding, H. Wang, Z. Liu, J. Li, and M. Sun. Knowledgeable prompt-tuning: Incorporating\nknowledge into prompt verbalizer for text classification. arXiv preprint arXiv:2108.02035, 2021.\nJ. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can\nself-improve. arXiv preprint arXiv:2210.11610, 2022.\nW. W. Koczkodaj. Somerville happiness survey data set. https://archive.ics.uci.edu/ml/\ndatasets/Somerville+Happiness+Survey, 2018.\nT. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot\nreasoners. arXiv preprint arXiv:2205.11916, 2022.\nB. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning.\narXiv preprint arXiv:2104.08691, 2021.\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil,\nI. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models.\narXiv preprint arXiv:2206.14858, 2022.\nP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A\nsystematic survey of prompting methods in natural language processing. ACM Computing Surveys,\n55(9):1\u201335, 2023.\nX. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang. Gpt understands, too. arXiv preprint\narXiv:2103.10385, 2021.\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoy-\nanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\nA. Narayan, I. Chami, L. Orr, and C. R\u00e9. Can foundation models wrangle your data? arXiv preprint\narXiv:2205.09911, 2022.\nF. Nielsen. Introduction to HPC with MPI for Data Science. Springer, 2016.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al. Training language models to follow instructions with human feedback. Advances in\nNeural Information Processing Systems, 35:27730\u201327744, 2022.\nG. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\npreprint arXiv:2104.06599, 2021.\nN. C. Rakotonirina, R. Dess\u00ec, F. Petroni, S. Riedel, and M. Baroni. Can discrete information extraction\nprompts generalize across language models? arXiv preprint arXiv:2302.09865, 2023.\nL. Reynolds and K. McDonell. Prompt programming for large language models: Beyond the few-shot\nparadigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing\nSystems, pages 1\u20137, 2021.\nS. Saha, X. V. Yu, M. Bansal, R. Pasunuru, and A. Celikyilmaz. Murmur: Modular multi-step\nreasoning for semi-structured data-to-text generation. arXiv preprint arXiv:2212.08607, 2022.\nB. Sch\u00e4fl, L. Gruber, A. Bitto-Nemling, and S. Hochreiter. Hopular: Modern hopfield networks for\ntabular data. arXiv preprint arXiv:2206.00664, 2022.\nR. E. Schapire. The strength of weak learnability. Machine learning, 5:197\u2013227, 1990.\nT. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh. Autoprompt: Eliciting knowledge\nfrom language models with automatically generated prompts. arXiv preprint arXiv:2010.15980,\n2020.\n11\nR. Shwartz-Ziv and A. Armon. Tabular data: Deep learning is not all you need. Information Fusion,\n81:84\u201390, 2022.\nJ. Vanschoren, J. N. Van Rijn, B. Bischl, and L. Torgo. Openml: networked science in machine\nlearning. ACM SIGKDD Explorations Newsletter, 15(2):49\u201360, 2014.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin.\nAttention is all you need. Advances in neural information processing systems, 30, 2017.\nH. Wang, C. Xu, and J. McAuley. Automatic multi-label prompting: Simple and interpretable\nfew-shot classification. arXiv preprint arXiv:2204.06305, 2022a.\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of\nthought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.\nZ. Wang, Z. Zhang, C.-Y. Lee, H. Zhang, R. Sun, X. Ren, G. Su, V. Perot, J. Dy, and T. Pfister.\nLearning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 139\u2013149, 2022c.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\nP. Yin, G. Neubig, W.-t. Yih, and S. Riedel. Tabert: Pretraining for joint understanding of textual and\ntabular data. arXiv preprint arXiv:2005.08314, 2020.\nW. Yu, D. Iter, S. Wang, Y. Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng, and M. Jiang. Generate rather than\nretrieve: Large language models are strong context generators. arXiv preprint arXiv:2209.10063,\n2022.\nZ. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language\nmodels. arXiv preprint arXiv:2210.03493, 2022.\nH. Zhou, A. Nova, H. Larochelle, A. Courville, B. Neyshabur, and H. Sedghi. Teaching algorithmic\nreasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022a.\nY. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models are\nhuman-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022b.\n12\nA\nFailure Modes\nThe design of the prompt plays a pivotal role in our entire process. Specifying instructions precisely\ncan create a significant difference, whether it comes to effectively describing tabular data, generating\nreliable summaries or inferring accurate predictions (shown in Figures 1 and 2). While soft\nprompting has been successful at instructing LLMs [Lester et al., 2021], it cannot be applied to our\nsetting because our classification algorithm learns a human-level summary as a prompt for classifying\ndata, rather than a soft prompt. Instead we chose to write prompts that ask the LLM to perform\nthese tasks. In this way, our prompting method is entirely gradient-free. Hand-written prompts also\noffer flexibility, aligning well with our core methodology for creating weak learner. While carefully\nhandcrafting prompts this way might seem intensive, we will show that once we identify the ideal\nhyperparameter settings they can be framed with little effort.\nA.1\nData Conversion Challenges\nLanguage models (LLMs) have demonstrated impressive performance on standard tasks with minimal\nsupervision [Wang et al., 2022b, Brown et al., 2020]. However, for converting tabular data to text,\nthere were several considerations to meet the requirements of our task. As highlighted in Section 3.1,\nwe will refer to these texts as data descriptions.\nEnsuring Uniform Length\nFirstly, the data descriptions should not be too long or short, also be of\ncomparable length. Excessively long descriptions limit the number of examples that can be fit inside\nthe prompt and summarized. We observed in Figure 4 (right bottom) that the summary performance\nalso scales with more examples, so it makes sense to accomodate as many of them as possible by\nhaving descriptions of approximately uniform length.\nA straightforward way of achieving this uniformity would be by specifying a max word length as\npart of the conversion prompt itself, as in \u201cDescribe in not more than 80 words\u201d. However, we found\nthis approach can falter, sometimes leading to overly simplistic descriptions like \"These are annual\nspendings of a customer.\" (in the wholesale-customers dataset).\nConsequently, we adopt more nuanced strategy by first modifying the prompt with the terms \u201ccon-\ncisely\u201d and \u201caccurately\u201d to emphasize the brevity and preciseness of the generated descriptions\n(shown in Figure 1). Then, we implement a resampling strategy, that generates descriptions until find-\ning the one with a desired length ranging between 20 to 80 words. This process achieves consistent\nand uniformly long descriptions.\nIncluding Metadata\nPrepending metadata to the prompt enhances the contextual awareness of the\ntask, resulting in higher-quality descriptions (shown in Figure 1).\nSeparating Features from Labels\nIn our method, the data descriptions function dual role, both\nas training examples and as query for inferring class labels. This suggests that, when converting\ndata to text, the features need to be described separately from the target label as illustrated in Figure\n1. The resulting strings are then concatenated to form the data description. Instead, if the whole\ntabular record were passed to the LLM, it often produces texts that assimilate the classification label\ninformation in the meat of the description itself, rendering it difficult to extract a query for doing\ninference.\nWhile one might cleverly come up with prompts that can allow the LLM to describe the features\nand target label in separate sentences, we found it to be more sensible to supply just the features for\ndescribing and not reveal any information about the target task. Sometimes that can liberate the LLM\nto hallucinate some facts about the task and form biased data to begin with.\nNatural-Sounding Descriptions\nWhile the LLM generates a different-styled response every time,\nto explicitly ensure that the generated descriptions are not template-like by chance, add a directive\nat the end of the prompt: \u201cUse your creativity\u201d. This encourages the LLM to produce more natural\nnarratives of the record. Alternatively, setting a higher temperature during decoding achieves a similar\neffect.\n13\nA.2\nSummarization\nThere are several aspects worth considering that can contribute to high-quality summaries.\nSampling candidate summaries\nA well-crafted summary is a one that captures salient information\nof the dataset, in a way that facilitates inferring predictions off it. However, the process of generating\nsummary using a LLM is inherently stochastic due to temperature sampling, as a result, the generated\nsummary can be noisy. From our experiments with tuning this temperature, we found 0.80 to be\nideal through Bayesian optimization. Even at this value, on average only 1 out of 3 summaries were\nmeaningful.\nA noisy summary can be distinguished quite easily. For instance, on the vehicle dataset, the tl;dr\nprompt elicits summaries as naive as \u201cThe given data describes a bus, van, or saab silhouette.\u201d or\n\u201cThe data in this table identifies a vehicle as a bus, saab, opel, or van. The compactness, circularity,\ndistance circularity, radius ratio, hollows ratio, and symmetry are all predictive of the vehicle\u2019s type.\u201d\nwhich does not offer actionable insight.\nThis observation indicates that summaries need to be sampled quite a few times and the best one\ncan be determined based on the validation error. As a result, for the Summary learning procedure\nin Section 3.2, we resample approximately 25 times to find a good summary. Also, given that our\ndatasets are small, it is not unusual for the summaries to have the same validation error. When tied,\nwe pick one having a higher training error rate, i.e. lower generalization gap.\nDifferently, in our Summary boosting procedure explained in Section 3.3, we resample only until\nfinding a summary whose training error is better than random guessing and return immediately.\nOrdering examples inside the summarization prompt\nUnlike gradient descent, prompting is not\nrobust to the presentation of the examples to the learning algorithm. While we show via ablation\nstudies in Section 5 that there is no statistically significant difference in performance between either\nshuffling the examples or listing them by class, we can generally expect that depending on the dataset\nand the number of target classes, one might be preferred over the other.\nFor instance, in a multi-class setting, listing examples by class might be more helpful in reaching\na weak learner quickly. However, in a two-class setting, the summary might actually benefit from\nrandomness in the shuffled examples.\nCustomizing the summarization prompt\nThe way of asking the LLM to summarize examples\ncan also give rise to good/bad summaries. For instance, one can prompt the LLM with a simple\ntl;dr or specify the task more elaborately. We will refer to the latter option as explicit. As we\ndemonstrated in Figure 4 (left), both are means to the goal and do not statistically differ in terms of\nperformance induced.\nHowever, in our experiments on certain datasets, we would rather be incentivized choosing the\nexplicit over the tl;dr to attain a weak learner more quickly. This choice becomes important\npurely for compute reasons as it will take relatively lesser resampling, while the tl;dr still works. For\ninstance, this scenario can happen when the LLM cannot decipher what the summary is supposed to\nsay, by just observing the examples. As examples, the tl;dr prompt suffices on datasets such as iris,\ndiabetes, and wine that are commonly encountered in prediction context, whereas the LLM might\nnot be very familar with the goals of vertebra-column or somerville-happiness-survey\ndata, necessitating the use of the explicit prompt. For these other datasets, the fact that it is a\nclassification problem based on some features and target classes may not be very apparent from just\nthe examples and metadata. So, providing a directive such as \u201cSummarize in detail how we can tell\napart people with normal and abnormal vertebra-column\u201d reduces ambiguity in the task setup and\nreduces probability of a noisy summary.\nWhile manual intervention is necessary, framing this prompt can be done with little effort. We provide\na comprehensive list of these parameters for all datasets in Table 3.\nIncluding Metadata\nSimilar to data conversion, including meta-data information in the prompt\noffers better penetration into the world of the dataset, as a result improves boosting performance.\n14\nTable 3: Prompt design: Prompt parameter settings for every dataset.\nDataset\nPrompting hyperparameters\ncaesarian\nmetadata: This dataset contains information about caesarian section results of 80 pregnant women with the most\nimportant characteristics of delivery problems in the medical field.The goal is to predict whether a woman will undergo\nnormal or caesarian delivery.\nclasses: [normal, caesarian]\nsummary directive: Tl;dr\ninference directive: Hence this woman\u2019s delivery mode is likely to be (normal or caesarian):\niris\nmetadata: This is the iris dataset, perhaps the best known database to be found in the pattern recognition literature.\nFisher\u2019s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data\nset contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable\nfrom the other 2; the latter are NOT linearly separable from each other. Predicted attribute- class of iris plant- setosa,\nversicolor, virginica.\nclasses: [setosa, versicolor, virginica]\nsummary directive: Tl;dr\ninference directive: Based on the above information, predict if this flower will be classified as setosa, versicolor, virginica\ntae\nmetadata: The data consist of evaluations of teaching performance over three regular semesters and two summer semesters\nof 151 teaching assistant (TA) assignments at the Statistics Department of the University of Wisconsin-Madison. The\nscores were divided into 3 roughly equal-sized categories (\"low\", \"medium\", and \"high\") to form the class variable.\nclasses: [low, medium, high]\nsummary directive: Tl;dr\ninference directive: Predict whether this class will score low or medium or high:\nglass\nmetadata: This is the glass dataset from USA Forensic Science Service; 6 types of glass; defined in terms of their oxide\ncontent (i.e. Na, Fe, K, etc). The study of classification of types of glass was motivated by criminological investigation.\nAt the scene of the crime, the glass left can be used as evidence...if it is correctly identified!\nclasses: [building_windows_float_processed, building_windows_non_float_processed, vehicle_windows_float_processed,\ncontainers, tableware, headlamps]\nsummary directive: Tl;dr\ninference directive: There are 6 possible type of glass: building_windows_float_processed,\nbuilding_windows_non_float_processed, vehicle_windows_float_processed, containers, tableware, headlamps. Predict\nwhich one will this sample be:\nbreast-cancer\nmetadata: This is one of three domains provided by the Oncology Institute that has repeatedly appeared in the machine\nlearning literature. This data set includes 201 instances of one class and 85 instances of another class. The instances are\ndescribed by 9 attributes, some of which are linear and some are nominal. It contains information about women that had\na recurrence or non-relapse of breast cancer after their first time.\nclasses: [recurrence, non-relapse]\nsummary directive: Based on the above examples, figure out under what conditions will a woman have recurrence or\nnon-relapse of breast cancer?\ninference directive: Predict whether this woman will have a recurrence or non-relapse:\nvisualizing-environmental\nmetadata: This is the visualizing-environmental dataset, one of the 22 data sets from the book Visualizing Data published\nby Hobart Press (books@hobart.com). This data describes indicators for a positive/negative environment based on ozone,\nradiation and temperature. classes: [positive, negative]\nsummary directive: Tl;dr\ninference directive: There are clear signs of this environment being (positive or negative):\nanalcatdata-chlamydia\nmetadata: This chlamydia dataset is one of the data sets used in the book \"Analyzing Categorical Data\" by Jeffrey S.\nSimonoff, Springer-Verlag, New York, 2003. It contains results of individuals that tested for chlamydia.\nclasses: [positive, negative]\nsummary directive: Tl;dr\ninference directive: Predict if this person will test positive or negative for chlamydia:\nwine\nmetadata: This is the Wine recognition data. Updated Sept 21, 1998 by C.Blake. It contains results of a chemical analysis\nof wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities\nof 13 constituents found in each of the three types of wines.\nclasses: [1, 2, 3]\nsummary directive: Using these examples and based on the contents of constituents, summarize what distinguishes wines\nof type 1 or 2 or 3?\ninference directive: Hence this wine will be classified as ->type\nblood-transfusion-center\nmetadata: Data taken from the Blood Transfusion Service Center in Hsin-Chu City in Taiwan - this is a classification\nproblem. The goal is to predict whether a given individual will consent or avoid donating blood.\nclasses: [consent, avoid]\nsummary directive: Tl;dr\ninference directive: Therefore, this individual is likely to (avoid/consent):\nsomerville-happiness-survey\nmetadata: This is the Somerville Happiness Survey Data Set. It has ratings collected from a survey of Somerville\nresidents. From the responses of a resident, the goal is to predict whether they feel happy or unhappy about the place.\nclasses: [unhappy, happy]\nsummary directive: Based on the Somerville happiness survey, how can we predict whether a resident is happy or\nunhappy with their place?\ninference directive: So this resident is (happy or unhappy):\nvehicle\nmetadata: This is the Statlog (Vehicle Silhouettes) Data Set. The purpose is to classify a given silhouette as one of four\ntypes of vehicle - bus, saab, opel or a van, using a set of features extracted from the silhouette. The vehicle may be\nviewed from one of many different angles.\nclasses: [bus, saab, opel, van]\nsummary directive: Using these examples, summarize how can we differentiate if a silhouette is that of a bus, saab, opel\nor a van.\ninference directive: Out of saab, bus, van and opel, this vehicle is likely to be a\nstatlog-heart\nmetadata: This dataset is a heart disease database similar to a database already present in the repository (Heart Disease\ndatabases) but in a slightly different form. It has data on individuals having and not having heart disease.\nclasses: [present, absent]\nsummary directive: Differentiate people with heart disease present from ones absent.\ninference directive: In this case, heart disease is likely to be (present/absent):\n15\nverterbra-column\nmetadata: This dataset contains values for six biomechanical features used to classify orthopaedic patients into 3 classes (normal,\ndisk hernia or spondilolysthesis) or 2 classes (normal or abnormal). Biomedical data set built by Dr. Henrique da Mota during\na medical residence period in the Group of Applied Research in Orthopaedics (GARO) of the Centre M\u00c3\u00a9dico-Chirurgical de\nR\u00c3\u00a9adaptation des Massues, Lyon,\nFrance. The task is to classify patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210\npatients).\nclasses: [abnormal, normal]\nsummary directive: Based on the above examples, summarize how will you distinguish patients that have normal vs. abnormal\nvertebral column.\ninference directive: Therefore, this individual\u2019s vertebral column is likely to be (abnormal or normal):\necoli\nmetadata: This data contains protein localization sites. Reference: \"A Knowledge Base for Predicting Protein Localization Sites\nin Eukaryotic Cells\", Kenta Nakai & Minoru Kanehisa, Genomics 14:897-911, 1992.\nclasses: [1, 2]\nsummary directive: Using these examples, how can we tell apart cells with protein localized in sites 1 and 2?\ninference directive: Hence protein localization will be at site ->\nhaberman-survival\nmetadata: The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago\u2019s\nBillings Hospital on the survival of patients who had undergone surgery for breast cancer.\nclasses: [survived, died]\nsummary directive: Based on these examples, figure out what commonalities are predictive of patients surviving more than 5\nyears and less.\ninference directive: So, 5 years down the line, this person (survived/died):\ndiabetes\nmetadata: This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective\nis to predict based on diagnostic measurements whether a patient has high/low risk of developing diabetes.\nclasses: [low, high]\nsummary directive: Based on these examples, distinguish patients having low vs. high risk of diabetes.\ninference directive: Based on the reasoning, this patient is likely to have a (low/high):\nvisualizing-hamster\nmetadata: This is the visualizing-hamster dataset contains 22 data sets from the book Visualizing Data published by Hobart Press\n(books@hobart.com). It contains examples of hamsters that are ill and healthy.\nclasses: [ill, healthy]\nsummary directive: Using these examples, identify predictive indicators of ill and healthy hamsters.\ninference directive: Predict whether this hamster will be ill or healthy:\nwholesale-customes\nmetadata: The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on\ndiverse product categories. This data gives information about spending patterns and region of operations of Retail and Horeca\n(Hotel/Restaurant/Caf\u00e9) customers of the wholesale distributor.\nclasses: [retail, horeca]\nsummary directive: Using these examples, summarize how can we differentiate Retail customers and Horeca customers.\ninference directive: Therefore, which one of Retail or Horeca this customer is likely to be:\nA.3\nInference\nMapping labels from LLM responses\nAnswer mapping refers to the process of assigning the\nmodel\u2019s answer to a target output class. This step might be trivial when the answer is the class\nitself, for example when the LLM responds with \u201cnon-relapse\u201d or \u201crecurrence\u201d to a query on\nthe breast-cancer dataset. However, in other instances, it can become tricky when the LLM\u2019s\nresponses are \u201cwill not recur\u201d or \u201chas higher chance of non-relapse than recurrence\u201d, requiring a\nmore complex decoding logic to identify the target class.\nPrevious works have handled this problem by disguising the task as a Cloze prompt and learning a\nverbalizer, i.e. MLP projection of hidden state of the [MASK] token, that maps the predicted token to\nthe target classes [Cui et al., 2022, Hu et al., 2021]. By training a verbalizer, one can determinsically\ngo from token vocabulary to the label space. There also exist unsupervised statistical techniques for\nachieving label mapping [Wang et al., 2022a].\nIn our method however, we strictly interact with the LLM through prompts and do not access the\nhidden state nor gradients. As a result, our inference process shown in Figure 2 focusses on inferring\nthe class label solely through prefix prompts, without relying on learning an explicit mapping.\nSpecifically, by conditioning on a suitable prefix, we constrain the LLM to return exactly the class\nlabel string. For example, the prefix \u201cTherefore this iris flower is likely to be (setosa, versicolor,\nvirginica):\u201d works for the iris dataset. A key observation guiding the design of such a prefix prompt\nis the fact that specifying the output classes entices the LLM to predict from among these classes.\nWith a rather plain prefix like \u201cPredict what will be the type of this flower.\u201d, the LLM\u2019s answer space\nis unconstrained and it might liberally go on to explain a chain of reasoning such as \u201cThe flower has\nshort petals and long sepals, hence it is versicolor, and not setosa.\u201d preventing a simple keyword\nsearch for the class label.\nFor a full list of these inference prompts, refer Table 3.\nTwo-step prompting, Davinci vs. Curie\nIt is worth mentioning that a two-step prompting trick,\nby first calling \u201cLets think step by step\u201d then concatenating the response with the prefix prompt\nalso results in accurate answers, as we have shown in Figure 4 (left). However, it could only be\nimplemented on the larger model Davinci but not Curie which is primarily used in our experiments.\n16\nThis is a customer who spends very little \non fresh, milk, grocery, and frozen \nproducts, and spends less on detergents \nand paper products and delicatessen \nproducts. The customer is from outside of \nLisbon and Porto.\n{Metadata}\n{query}\nTherefore, this customer\u2019s channel will be \n(Horeca or Retail): \nLLM\n{answer}\nInference prompt\nQuery\nThe dataset refers to clients of a \nwholesale distributor. It includes \nthe annual spending in monetary \nunits (m.u.) on diverse product \ncategories.\nMetadata\nFigure 5: Steps of Zeroshot prompting\n1.\nThis customer spends medium amounts on \nfresh, milk, and delicatessen products, high \namounts on grocery and detergents and paper \nproducts, and very little on frozen products. \nThis customer is located outside of Lisbon and \nPorto. ### Hence this customer channel is \nRetail.\n150.The client spends a lot on frozen, delicatessen, \nand fresh products. They spend a very low \namount on milk, products, and detergents and \npaper products. This client is located outside of \nLisbon and Porto. ### Hence this customer \nchannel was Horeca (Hotel/Restaurant/Caf\u00e9).\nStratified \ncluster \nsampling\nThis is a customer who spends very little \non fresh, milk, grocery, and frozen \nproducts, and spends less on detergents \nand paper products and delicatessen \nproducts. The customer is from outside of \nLisbon and Porto.\nPrediction from examples\n{Metadata}\n{Rich Representative support set examples}\nNow, {query}\nTherefore, this customer\u2019s channel will be \n(Horeca or Retail): \nLLM\n{answer}\nRecords as text\nInference prompt\nQuery\nThe dataset refers to clients of a \nwholesale distributor. It includes \nthe annual spending in monetary \nunits (m.u.) on diverse product \ncategories.\nMetadata\nFigure 6: Workflow in fewshot prompting\nInterestingly Davinci\u2019s chain of thought reasoning even outperforms its prefix prompt counterpart.\nIn all our experiments with Curie however, the prefix technique works reasonably well.\nThe Davinci API also offers a suffix argument which can be invoked for predicting in a more natural\nway. For instance, for the breast-cancer dataset, the prompt can be posed with prefix \u201cAll in all,\nthis woman is more likely to have a\u201d and a suffix \u201c of breast cancer.\u201d directly expecting the LLM to\nfill in with \u201crecurrence\u201d or \u201cnon-relapse.\u201d\nA.4\nZeroshot Setting\nWe extend the analysis of prompting-based methods in Section 4.1 by further delving into the\nZeroshot experiment. We illustrate this experimental setting in Figure 5. It only consists of the\ninference prompt, wherein the LLM is presented with the metadata and a query. To facilitate answer\nmapping, the output classes are also indicated in the prompt. Unlike Summary, the zeroshot process\nis not stochastic as there is no learning involved. For inference, the predicted tokens are sampled\ngreedily at temperature = 0.\nA.5\nFew-shot Setting\nExtending the analysis from Section 4, we explain the Fewshot setting. As illustrated in Figure 6,\nit is an instance of in-context learning, wherein the support set examples are enlisted in the prompt\nalong with a query in the end. The support set was chosen from the training set through the stratified\ncluster sampling outlined in Algorithm 1. This results in a semantically diverse collection of examples\nevenly spread across the classes. Since we observe in Figure 4 (right bottom) that the Fewshot\nperformance drops with more examples, we choose approximately 15 examples to fit in the prompt.\nSimilar to the Summary method, this inference prompt carries meta-information about the dataset,\nand also indicates the output classes. The prompt also injects support set examples that are stringed\ntogether as we show in Figure 6. The predictions are made greedily at a temperature of 0.\nAgain, the Fewshot method is a stochastic process whose outcome depends on the selection of the\nsupport set. So finding the ideal prompt, requires sampling different support sets from the training\n17\nset quite a few times. We perform this resampling approximately 25 times and pick the best prompt\nbased on the validation error.\nA.6\nPreprocessing continuous attributes\nExtending from the ablation studies in Section 5, we demonstrate in Table 4, concrete examples of\nthese encoding techniques applied to continuous features. Every numerical column in the dataset was\nsubject to the transformation independently of the rest.\nWe applied several encoding techniques for continuous features, including binning, percentiles, and\nstandard deviations. Our approach involved using technical language terms to describe these ranges,\nsuch as \u201cfalls in the nth bin/nth percentile or n deviations above/below mean\". We also characterize\nthem in a more naturalistic way by assigning quantifiers such as low, medium, and high to each level\nin the binning technique.\nTo create effective textual descriptions, we examined three high-level approaches: 1. presenting only\nnumerical values, 2. using solely textual encodings, and 3. concatenating both. We observed that\nutilizing textual encoding alone outperformed the other methods. As a result, we focused on mainly\ncomparing textual encoding methods as shown in Figure 4 (right top). Through Bayesian optimization,\nwe found that binning with \u201c5\u201d quantifiers was ideal for generating high-quality summaries.\nWe describe each encoding technique as follows:\n\u2022 Binning: It involves creating a histogram with the given number of bins. As outputs,\nthe values are directly described as \u201cfalling in the n-th bin\u201d as illustrated in the 10 bins\nexperiment. However, in the presence of degree quantifiers which are categorical names\nassigned to the bins, these tags are used instead. We found that as opposed to calling out the\nbin number, describing in terms of these quantifiers further aids the LLM in comparing the\nrelative extent to which features match and improving the estimation of similarities. This\nled us to tune the number of bins against these degree quantifiers, selecting values in the\nrange of 4, 5, 7, and 9 bins. The first four rows in Table 4 show how these tags get translated\ninto the record.\n\u2022 Percentile: It is given by computing the percentile rank of a value relative to that series of\nvalues. Then, the value is described as falling in that percentile rank in words. This is closer\nto representation of the true numerical values per se, but helps the LLM draw comparisons\non a scale of 1-100.\n\u2022 Standard deviations: In this procedure, the values are segmented into six ranges based on\ndistance from the mean, given by one/two/three standard deviations above/below the mean.\n\u2022 Quartiles: Here, we consider the first and third quartiles, and the median as landmarks to\nbucketize the values into four partitions.\nAmong these methods, the \u201c5 bins with quantifiers\u201d strikes a balance in granularity scale. It is not\nexcessively fine-grained as \u201cpercentile\u201d, nor overly abstract, as the \u201c4-bin\u201d approach. This balance\nultimately leads to optimal performance.\nA.7\nCluster Sampling components\nWe discuss more of the functions in Algorithm 1.\nGPT-Embedding is OpenAI\u2019s text similarity model text-embedding-ada-002 that takes a max-\nimum input size of 8191 tokens. It returns a 1536-dimensional embedding for text. OpenAI\nrecommends cosine distance for comparing ada embeddings in downstream tasks.\nAs a result, the AgglomerativeClustering algorithm applies hierarchical clustering over these features\nusing cosine distance, average linkage and a heuristically selected distance threshold of 0.05. It yields\na set of clusters C and each Cj contains a list of indices of data points that belong to that cluster j.\nA.8\nAdaboost Optimizations\nWe additionally apply several run-time optimizations to the boosting algorithm described in 3.3. Thus\nwe present its full version in Algorithm 3.\n18\nAlgorithm 3 Summary Boosting\n1: Input: X, all training data; y, all training label; T: maximum number of rounds; s: size of the\nsampled subset.\n2: h, P, \u03f5, \u03b1 \u2190 empty array of size T. \u25b7 h holds the round-wise hypotheses, P are the corresponding\nlabel mappings, \u03f5 gathers the weighted train errors, and \u03b1 are coefficients of the hypotheses.\n3: N \u2190 len(X)\n4: c \u2190 set of target classes\n5: K \u2190 len(c)\n6: w \u2190 new array of size N filled with 1\nN.\n\u25b7 w is the weighted data distribution\n7: for r = 1 to T do\n8:\n(Xs, ys) \u2190 Cluster-sample s examples from training distribution w.\n9:\nh[r] \u2190 Summary (Xs, ys)\n\u25b7 h[r] is the weak learner in the current round\n10:\n\u02c6y[i] \u2190 h[r](X[i])\n\u25b7 \u02c6y refers to predictions on training set\n11:\n\u03be \u2190 empty hashmap\n\u25b7 \u03be[p] will have error rate of the corresponding label mapping p\n12:\nfor p in PermutedLabelMappings(c) do\n13:\n\u03be[p] \u2190\nPN\ni=1 w[i]\u00d71{p[\u02c6y[i]]\u0338=y[i]}\nPN\ni=1 w[i]\n14:\nend for\n15:\np\u2217 \u2190 arg minp \u03be[p]\n16:\nif \u03be[p\u2217] > 1 \u2212 1\nK \u2212 \u00b5 OR AllSame(\u02c6y) then Goto Step 8.\n17:\nelse\n18:\nP[r] \u2190 p\u2217; \u03f5[r] \u2190 \u03be[p\u2217]\n19:\nend if\n20:\nif \u03f5[r] == 0 then\n21:\nBreak\n22:\nend if\n23:\n\u03b1[r] \u2190 log\n\u0010\n1\u2212\u03f5[r]\n\u03f5[r]\n\u0011\n+ log(K \u2212 1)\n24:\nfor i = 1 to N do\n25:\nw[i] = w[i] \u00d7 exp(\u03b1[r]1{P[r][h[r](X[i])] \u0338= y[i]})\n26:\nend for\n27:\nw \u2190 Normalize(w)\n28: end for\n29: Return h, \u03b1\n\u2022 Raising the bar for a weak learner: Our goal was to create high-quality summaries that\ndramatically reduce the validation error rate and significantly accelerate the convergence\nof the boosting procedure. Thus we raise the performance threshold to a notch slightly\nhigher than random guessing probability (see Step 15 in Algorithm 3), provoking insightful\nsummaries.\nWe resample until finding a weak learner that satisfies this threshold.\nThe positive quantity \u00b5 is a hyperparameter that typically takes values 0.08 for 2-class\nproblem and 0.16 for 3-class problem, and so on.\nAlthough this step increases compute, it yields better weak learners and improves conver-\ngence overall.\n\u2022 Permuting the predicted class label assignments:\nWe harness the potential of permuting the class assignments by exploring K! different\nmappings of predictions to classes using the PermutedLabelMappings function in steps\n11-14. This process helps us identify the mapping that minimizes the training error to the\ngreatest extent.\nBy considering multiple permutations of predictions across the label space, as outlined in\nSteps 11-14 of Algorithm 3, we obtain a hashmap p from the PermutedLabelMappings\nfunction. This hashmap maps the predictions \u02c6y to the permuted label space. Selecting the\nmapping, p\u2217 that results in the lowest training error effectively diminishes the cumulative\ntraining error during boosting iterations and proves to be an effective strategy for generating\nstrong weak learners. This technique is particularly advantageous in scenarios involving\nmore than two classes.\n19\n\u2022 Sanity checks: Finally, to ensure robustness of the weak learner when faced with skewed\ndatasets, we have implemented a policy that disallows a naive all-ones classifier. The\ncondition calling AllSame in Step 15 of Algorithm 3 performs this check.\nA.9\nText Templates\nIn the ablation study which involves comparing the descriptions created manually vs. by a LLM,\nas illustrated in Figure 3 (right), we transform the descriptive attributes into the textual format by\napplying a pre-defined template. In Table 5 we provide examples of these templates for selected\ndatasets.\nA.10\nComplexity Analysis\nWe provide the time complexity analysis comparing our boosting procedure to finetuning the LLM.\nFor finetuning, the complexity is O(TNf), where f is runtime of the LLM, T is number of epochs,\nN is the number of data points.\nFor summary boosting, the complexity is O(TRf), where f is runtime of the LLM, T is number of\nboosting rounds and R is the number of resampling per round.\nConcretely, for a dataset with 175 examples, finetuning takes 20 epochs \u00d7 175 examples \u00d7 2 = 7000\npasses through the LLM. 2 stands for both forward and backward passes through the model.\nFor the same dataset boosting requires 50 rounds \u00d7 25 resampling on average = 1250 passes through\nthe LLM.\nThus, we believe the complexity of our algorithm is at least comparable to, if not better than, that of\nfinetuning (without considering the cost of the actual API calls).\nA.11\nEstimating the cost of API calls\nWhile our method is applicable to any large language model, we primarily conducted experiments\nusing GPT-3. Each API call to GPT-3 incurs a specific dollar cost.\nAfter analyzing the running time complexity of summary boosting, which is O(TRf), we can provide\na rough estimation of the cost associated with training a classifier on any given dataset.\nTo begin, when making a call to summarize examples, the prompt is filled up to the maximum context\nlength, which is 2048 tokens for the query prompt and completion. We\u2019ll refer to these summary\ntokens as St = 2048.\nAdditionally, if N represents the size of the dataset and we allocate (50 + 10)\nNow, to obtain a weak learner at boosting round r, we may need to resample up to R candidate\nsummaries. Furthermore, we calculate the training error for each candidate summary to determine if\nit performs better than random guessing. Once the desired weak learner is found, we compute the\nvalidation error for that round only once. Therefore, each round requires querying R \u00d7 (St + 0.5N \u00d7\nPt) + 0.1N \u00d7 Pt tokens.\nConsidering that the maximum number of rounds is denoted as T, the total number of tokens\nexchanged would be T \u00d7 [R \u00d7 (St + 0.5N \u00d7 Pt) + 0.1N \u00d7 Pt].\nFor instance, let\u2019s consider a dataset with 175 examples. In this case, the cost would be 30 rounds \u00d7\n[20 resampling \u00d7 (2048 summary tokens + (0.5 \u00d7 175 training examples) \u00d7 210 prediction tokens) +\n(0.1 \u00d7 175 validation examples) \u00d7 210 prediction tokens] = 12364050 tokens, which approximately\ncosts $25 for Curie at a rate of $0.002/1K tokens.\nA.12\nCan ChatGPT function as a weak learner?\nOne would expect that it is more advantageous to try newer LLMs such as ChatGPT that produce\nincreasingly more human-like text and are far more sample-efficient, i.e. can summarize more\nexamples since they come with a larger context length. To investigate this, we conduct experiments\nby feeding ChatGPT with the same tabular data descriptions and using identical prompts to create\nweak learners. The results are presented in Table 6.\n20\nSurprisingly ChatGPT outperforms Curie in classifying datasets with more numerical features, such as\nwine, wholesale-customers, and iris. This observation suggests that LLMs are becoming more\nadept at quantitative reasoning from finetuning with more data. However, the reinforcement learning\nfrom human feedback (RLHF) [Ouyang et al., 2022] poses a limitation as it still ensures the generated\ntext does not deviate too much from its prior. The generated text distribution adheres closely to the\nbehavior programmed into the LLM induced by optimizing with such a reward model. Consequently\nit becomes challenging to bias the LLM with adversarial examples that might occasionally emerge in\nthe training set.\nFor example, ChatGPT does not mostly generalize well on datasets with medical information such\nas verterbra-column, breast-cancer, caesarian and blood-transfusion-center where\nthere can be examples contrary to common medical beliefs. In these cases, the RLHF is more\nrestrictive due to its conformity to human preferences and does not neutrally summarize examples at\nhand from a classification standpoint. However, boosting imposes a significantly higher penalty on\nexamples that the model fails to classify correctly, causing ChatGPT to not decrease training error\nafter a few epochs. While these models exhibit promise in terms of higher-order problem-solving\nskills, their capabilities can also be limited by their alignment with human preferences.\n21\nTable\n4:\nContinuous\nvariable\ntransformations\napplied\nto\nan\nexample\nfrom\nthe\nwholesale-customers dataset.\nThe raw tabular record is as follows:\nspending on fresh\nproducts: 6353.0, spending on milk products: 8808.0, spending on grocery products: 7684.0,\nspending on frozen products: 2405.0, spending on detergents and paper products: 3516.0, spending\non delicatessen products: 7844.0 and customer\u2019s region: Outside Lisbon and Porto.\nMethod\nData Representation\nExample as text\n4 bins + quantifiers\n{very low, low, high,\nvery high}\n- spending on fresh products : low\n- spending on milk products : very high\n- spending on grocery products : high\n- spending on frozen products : high\n- spending on detergents and paper products : high\n- spending on delicatessen products : very high\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer spends low amounts on fresh products, very high\namounts on milk products, high amounts on grocery products,\nfrozen products, detergents and paper products, and very high\namounts on delicatessen products. They are located outside of\nLisbon and Porto.\n5 bins + quantifiers\n{very low, low, medium,\nhigh, very high}\n- spending on fresh products : medium\n- spending on milk products : very high\n- spending on grocery products : high\n- spending on frozen products : high\n- spending on detergents and paper products : high\n- spending on delicatessen products : very high\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer from outside Lisbon and Porto spends medium on\nfresh products, very high on milk products, high on grocery\nproducts, high on frozen products, high on detergents and paper\nproducts, and very high on delicatessen products.\n7 bins + quantifiers\n{extremely low, very low,\nlow, medium, high,\nvery high, extremely high}\n- spending on fresh products : low\n- spending on milk products : very high\n- spending on grocery products : high\n- spending on frozen products : high\n- spending on detergents and paper products : very high\n- spending on delicatessen products : extremely high\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer situated outside Lisbon and Porto spends low on\nfresh products, very high on milk products, high on grocery\nproducts, high on frozen products, very high on detergents and\npaper products, and extremely high on delicatessen products.\n9 bins + quantifiers\n{lowest, extremely low,\nvery low, low, medium,\nhigh, very high,\nextremely high, highest}\n- spending on fresh products : low\n- spending on milk products : extremely high\n- spending on grocery products : high\n- spending on frozen products : high\n- spending on detergents and paper products : very high\n- spending on delicatessen products : highest\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer spends low amounts on fresh products, extremely\nhigh amounts on milk products, high amounts on grocery\nproducts, frozen products, detergents and paper products, and\nhighest amounts on delicatessen products. They are located\noutside Lisbon and Porto.\n10 bins\n- spending on fresh products : falls in the first out of ten\nbins of values.\n- spending on milk products : falls in the second out of\nten bins of values\n- spending on grocery products : falls in the first out of\nten bins of values\n- spending on frozen products : falls in the first out of\nten bins of values\n- spending on detergents and paper products : falls in\nthe first out of ten bins of values\n- spending on delicatessen products : falls in the second\nout of ten bins of values\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer spends relatively little on fresh, grocery, frozen\nand detergents/paper products, and more on milk and\ndelicatessen products. They are based outside Lisbon and Porto.\nPercentile\n- spending on fresh products : falls in the forty-first\npercentile\n- spending on milk products : falls in the eighty-second\npercentile\n- spending on grocery products : falls in the sixty-fifth\npercentile\n- spending on frozen products : falls in the sixty-third\npercentile\n- spending on detergents and paper products : falls in\nthe seventy-second percentile\n- spending on delicatessen products : falls in the ninety\n-eighth percentile\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer has an annual spending of 41st percentile on fresh\nproducts, 82nd percentile on milk products, 65th percentile on\ngrocery products, 63rd percentile on frozen products, 72nd\npercentile on detergents and paper products, and 98th percentile\non delicatessen products, and is located outside of Lisbon and\nPorto.\nStandard deviation\n- spending on fresh products : is within one std-dev\nbelow the mean value\n- spending on milk products : is within one std-dev\nabove the mean value\n- spending on grocery products : is within one std-dev\nbelow the mean value\n- spending on frozen products : is within one std-dev\nbelow the mean value\n- spending on detergents and paper products : is\nwithin one std-dev above the mean value\n- spending on delicatessen products : is two std-dev\nabove the mean value\n- customer\u2019s region : Outside Lisbon and Porto\nThe customer has annual spending on fresh products, milk\nproducts, grocery products, frozen products, detergents and paper\nproducts, and delicatessen products within one standard deviation\nof the mean, except for delicatessen products which is two\nstandard deviations above the mean. The customer is located\noutside Lisbon and Porto.\nQuartiles\n- spending on fresh products : is between the first\nquartile and median values\n- spending on milk products : is more than the third\nquartile value\n- spending on grocery products : is between median\nand third quartile values\n- spending on frozen products : is between median\nand third quartile values\n- spending on detergents and paper products : is\nbetween median and third quartile values\n- spending on delicatessen products : is more than\nthe third quartile value\n- customer\u2019s region : Outside Lisbon and Porto\nThis customer spends more than the third quartile value on milk,\ndelicatessen and detergents and paper products. The customer\u2019s\nspending on fresh, grocery, and frozen products falls between the\nmedian and third quartile values, while the customer is located\noutside of Lisbon and Porto.\n22\nTable 5: Templatized descriptions: Templates used to format examples for the ablation study\nbetween LLM-created data descriptions vs. template descriptions\nDataset\nDescriptive attribute values\nTemplate\ncaesarian\nage: [very young, young, middle-aged, old, very old]\ndelivery_number: [first, second, third, fourth, fifth]\ndelivery_time: [timely, premature, latecomer]\nblood_pressure: [low, normal, high]\nheart_problem: [has, doesn\u2019t have]\ndelivery_mode: [normal, caesarian]\nThis {age} woman is in her {delivery_number} delivery and it is\n{delivery_time}. She has a {blood_pressure} blood pressure and\n{heart_problem} heart problems. ### Based on these attributes, this\nwoman is likely to deliver by {delivery_mode}\niris\nsepal_length, petal_length: [very short, short,\nmedium length, long, very long]\nsepal_width, petal_width: [very narrow, narrow,\nmedium width, wide, very wide]\nflower_type: [setosa, versicolor, virginica]\nThis iris flower has {sepal_length} and {sepal_width} sepals. It also\nhas {petal_length} and {petal_width} petals. ### Hence this flower\nis a {flower_type}\nvertebral-column\npelvic_incidence, pelvic_tilt, lumbar_lordosis_angle,\nsacral_slope, pelvic_radius, grade_of_spondylolisthesis:\n[very low, low, medium, high, very high]\nresult: [normal, abnormal]\nThis patient has a {pelvic_incidence} pelvic incidence, {pelvic_tilt}\npelvic tilt, and {lumbar_lordosis_angle} lumbar lordosis angle,\n{sacral_slope} sacral slope, {pelvic_radius} pelvic radius and\n{grade_of_spondylolisthesis} grade of spondylolisthesis. ### As a\nresult, the patient\u2019s vertebral-column is likely to be {result}\nstatlog-heart\nage: [very young, young, middle-aged, old, very old]\nsex: [male, female]\nchest_pain_type: [asymptomatic, nonanginal pain,\natypical angina, typical angina]\nbp, cholesterol, st_depression, heart_rate,\nnum_major_vessels: [very low, low, medium, high,\nvery high]\nfasting_blood_sugar: [high, low]\nelectrocardiographic_results: [having left ventricular\nhypertrophy, normal, having ST-T wave abnormality]\nslope_st_segment: [flat, upsloping, downsloping]\nexercise_induced_angina: [has, do not have]\ndefect_type: normal, reversible, fixed\npresence_of_heart_disease: [present, absent]\nThis individual is a/an {age} {sex} with {chest_pain_type} chest pain,\n{bp} resting blood pressure, and {cholesterol} serum cholesterol.\nTheir fasting blood sugar {fasting_blood_sugar} >120 mg/dl, they are\n{electrocardiographic_results} and a {heart_rate} maximum heart rate.\nThey {exercise_induced_angina} exercise-induced angina, and have a\n{st_depression} ST depression induced by exercise relative to rest.\nTheir peak exercise ST segment has a {slope_st_segment} slope, and\nthey have a {num_major_vessels} number of major vessels. The defect\ntype is {defect_type}. ### Hence heart disease is likely to be\n{presence_of_heart_disease}.\nhaberman-survival\nage_at_time_of_op: [very young, young,\nmiddle-aged, old, very old]\nyear_of_op: [1964, 1962, 1965, 1959, 1958, 1960,\n1966, 1961, 1967, 1963, 1969,1968]\nnum_pos_axillary_nodes: [very low, low, medium,\nhigh, very high]\nsurvival_status: [survived, died]\nThis patient was {age_at_time_of_op} at the time of operation in\n{year_of_op}. They had a {num_pos_axillary_nodes} number of\npositive axillary nodes detected. ### Therefore 5 years down the line,\nthe patient {survival_status}\nTable 6: Comparing test error rate of Summary Boosting backended by Curie and ChatGPT on all\ndatasets (\u2193). Refer to caption of Table 1 for the notations used.\nDataset\nData Type\nSize\nCurie\nChatGPT\ncaesarian [cae] (42901)\n1c4d\n80\n0.300\u00b1 0.04\n0.406\u00b1 0.03\niris (61)\n4c0d\n150\n0.193\u00b1 0.03\n0.083\u00b1 0.01\ntae (48)\n1c4d\n151\n0.454\u00b1 0.03\n0.443\u00b1 0.04\nglass (41)\n9c0d\n214\n0.370\u00b1 0.02\n0.492\u00b1 0.02\nbreast-cancer [bc] (13)\n7c5d\n277\n0.288\u00b1 0.02\n0.360\u00b1 0.01\nvisualizing-environmental [ve] (678)\n3c0d\n111\n0.268\u00b1 0.03\n0.333\u00b1 0.04\nanalcatdata-chlamydia [ac] (535)\n2c2d\n100\n0.170\u00b1 0.01\n0.300\u00b1 0.06\nwine (43571)\n13c0d\n178\n0.320\u00b1 0.01\n0.250\u00b1 0.01\nblood-transfusion-center [btc] (1464)\n4c0d\n748\n0.240\u00b1 0.04\n0.433\u00b1 0.01\nsomerville-happiness-survey [shs] [Koczkodaj, 2018]\n0c7d\n143\n0.350\u00b1 0.02\n0.430\u00b1 0.02\nvehicle (54)\n18c0d\n846\n0.410\u00b1 0.04\n0.350\u00b1 0.16\nstatlog-heart [stath] [Dua and Graff, 2017]\n6c7d\n270\n0.430\u00b1 0.01\n0.370\u00b1 0.17\nverterbra-column [vc] (1524)\n6c0d\n310\n0.262\u00b1 0.01\n0.669\u00b1 0.03\necoli (1011)\n7c0d\n336\n0.270\u00b1 0.03\n0.193\u00b1 0.03\nhaberman-survival [hs] (43)\n3c0d\n306\n0.250\u00b1 0.01\n0.415\u00b1 0.03\ndiabetes [dia] (37)\n8c0d\n768\n0.344\u00b1 0.01\n0.297\u00b1 0.04\nvisualizing-hamster [hams] (708)\n5c0d\n73\n0.207\u00b1 0.00\n0.400\u00b1 0.08\nwholesale-customers [wc] (1511)\n6c1d\n440\n0.330\u00b1 0.00\n0.199\u00b1 0.04\n23\n"
  },
  {
    "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning",
    "link": "https://arxiv.org/pdf/2306.14892.pdf",
    "upvote": "7",
    "text": "Supervised Pretraining Can Learn\nIn-Context Reinforcement Learning\nJonathan N. Lee\u22171\nAnnie Xie\u22171\nAldo Pacchiano2\nYash Chandak1\nChelsea Finn1\nOfir Nachum3\nEmma Brunskill1\n1Stanford University, 2Microsoft Research, 3Google DeepMind\nAbstract\nLarge transformer models trained on diverse datasets have shown a remarkable\nability to learn in-context, achieving high few-shot performance on tasks they were\nnot explicitly trained to solve. In this paper, we study the in-context learning capa-\nbilities of transformers in decision-making problems, i.e., reinforcement learning\n(RL) for bandits and Markov decision processes. To do so, we introduce and study\nDecision-Pretrained Transformer (DPT), a supervised pretraining method where\nthe transformer predicts an optimal action given a query state and an in-context\ndataset of interactions, across a diverse set of tasks. This procedure, while simple,\nproduces a model with several surprising capabilities. We find that the pretrained\ntransformer can be used to solve a range of RL problems in-context, exhibiting both\nexploration online and conservatism offline, despite not being explicitly trained to\ndo so. The model also generalizes beyond the pretraining distribution to new tasks\nand automatically adapts its decision-making strategies to unknown structure. The-\noretically, we show DPT can be viewed as an efficient implementation of Bayesian\nposterior sampling, a provably sample-efficient RL algorithm. We further leverage\nthis connection to provide guarantees on the regret of the in-context algorithm\nyielded by DPT, and prove that it can learn faster than algorithms used to generate\nthe pretraining data. These results suggest a promising yet simple path towards\ninstilling strong in-context decision-making abilities in transformers.\n1\nIntroduction\nFor supervised learning, transformer-based models trained at scale have shown impressive abilities\nto perform tasks given an input context, often referred to as few-shot prompting or in-context\nlearning [1]. In this setting, a pretrained model is presented with a small number of supervised input-\noutput examples in its context, and is then asked to predict the most likely completion (i.e. output)\nof an unpaired input, without parameter updates. Over the last few years in-context learning has\nbeen applied to solve a range of tasks [2] and a growing number works are beginning to understand\nand analyze in-context learning for supervised learning [3, 4, 5, 6]. In this work, our focus is to\nstudy and understand in-context learning applied to sequential decision-making, specifically in the\ncontext of reinforcement learning (RL) settings. Decision-making (e.g. RL) is considerably more\ndynamic and complex than supervised learning. Understanding and leveraging in-context learning\nhere could potentially unlock significant improvements in an agent\u2019s ability to adapt and make\nfew-shot decisions in response to observations from the world. Such capabilities are instrumental for\npractical applications ranging from robotics to recommendation systems.\nFor in-context decision-making [7, 8, 9], rather than input-output tuples, the context takes the form\nof state-action-reward tuples representing a dataset of interactions with an unknown environments.\n\u2217Equal contribution.\narXiv:2306.14892v1  [cs.LG]  26 Jun 2023\nThe agent must leverage these interactions to understand the dynamics of the world and what actions\nlead to good outcomes. A hallmark of good decision-making in online RL algorithms is a judicious\nbalance of selecting exploratory actions to gather information and selecting increasingly optimal\nactions by exploiting that information [10]. In contrast, an RL agent with access to only a suboptimal\noffline dataset should produce a policy that conservatively selects actions [11]. An ideal in-context\ndecision-maker should exhibit similar behaviors.\nTo study in-context decision-making formally, we propose a new simple supervised pretraining\nobjective, namely, to train (via supervised learning) a transformer to predict an optimal action label2\ngiven a query state and an in-context dataset of interactions, across a diverse set of tasks. We refer\nto the pretrained model as a Decision-Pretrained Transformer (DPT). Once trained, DPT can be\ndeployed as either an online or offline RL algorithm in a new task by passing it an in-context dataset\nof interactions and querying it for predictions of the optimal action in different states. For example,\nonline, the in-context dataset is initially empty and DPT\u2019s predictions are uncertain because the new\ntask is unknown, but it fills the dataset with its interactions as it learns and becomes more confident\nabout the optimal action. We show empirically and theoretically that DPT yields a surprisingly\neffective in-context decision-maker with regret guarantees. As it turns out, DPT effectively performs\nposterior sampling \u2014 a provably sample-efficient Bayesian RL algorithm that has historically been\nlimited by its computational burden [12]. We summarize our main findings below.\n\u2022 Predicting optimal actions alone gives rise to near-optimal decision-making algorithms. The\nDPT objective is solely based on predicting optimal actions from in-context interactions. At the\noutset, it is not immediately apparent that these predictions at test-time would yield good decision-\nmaking behavior when the task is unknown and behaviors such as online exploration are necessary\nto solve it. Intriguingly, DPT as an algorithm is capable of dealing with this uncertainty in-context.\nFor example, despite not being explicitly trained to explore, DPT exhibits an exploration strategy\non par with hand-designed algorithms, as a means to discover the optimal actions.\n\u2022 DPT generalizes to new decision-making problems, offline and online. We show DPT can\nhandle reward distributions unseen in its pretraining data on bandit problems as well as unseen\ngoals, dynamics, and datasets in simple MDPs. This suggests that the in-context strategies learned\nduring pretraining are robust and generalizable without any parameter updates at test time.\n\u2022 DPT improves over the data used to pretrain it by exploiting latent structure. As an example,\nin parametric bandit problems, specialized algorithms can leverage structure (such as linear\nrewards) and offer provably better regret, but a representation must be known in advance. Perhaps\nsurprisingly, we find that pretraining on linear bandit problems, even with unknown representations,\nleads DPT to select actions and explore in a way that matches an efficient linear bandit algorithm.\nThis holds even when the source pretraining data comes from a suboptimal algorithm (i.e., one\nthat does not take advantage of any latent structure), demonstrating the ability to learn improved\nin-context strategies beyond what it was trained on.\n\u2022 Posterior sampling can be implemented via in-context learning. Posterior sampling (PS), a gen-\neralization of Thompson Sampling, can provably sample-efficiently solve online RL problems [12],\nbut a common criticism is the lack of computationally efficient ways to update and sample from a\nposterior distribution. DPT can be viewed as learning a posterior distribution over optimal actions,\nshortcutting the PS procedure. Under some conditions, we show theoretically that DPT in-context\nis equivalent to PS. Furthermore, DPT\u2019s prior and posterior updates are grounded in data rather\nthan needing to be specified a priori. This suggests that in-context learning could help unlock\npractical and efficient RL via posterior sampling.\n2\nRelated Work\nMeta-learning. Algorithmically, in-context learning falls under the meta-learning framework [13, 14].\nAt a high-level, these methods attempt to learn some underlying shared structure of the training\ndistribution of tasks to accelerate learning of new tasks. For decision-making and RL, there is a often\nchoice in what shared \u2018structure\u2019 is specifically learned such as the dynamics of the task [15, 16, 17],\na task context identifier [18, 19, 20, 21], temporally extended skills and options [22, 23, 24], or\n2If not explicitly known, the optimal action can be determined by running any (potentially inefficient)\nminimax-optimal regret algorithm for each pretraining task.\n2\ninitialization of a neural network policy [25, 26]). In-context learning can be viewed as taking a more\nagnostic approach by learning the learning algorithm itself, more similar to [27, 28, 29]. Algorithm\nDistillation (AD) [7, 30] also falls under this category, applying autoregressive supervised learning to\ndistill (sub-sampled) traces of a single-task RL algorithm into a task-agnostic model. While DPT\nalso leverages autoregressive SL, it does not distill an existing RL algorithm in order to imitate how\nto learn. Instead, we pretrain DPT to predict optimal actions, yielding potentially emergent online\nand offline strategies at test time that automatically leverage the task structure to behave similarly to\nposterior sampling.\nAutoregressive transformers for decision-making. In decision-making fields such as RL and\nimitation learning, transformer models trained using autoregressive supervised action prediction have\nproliferated [31], inspired by the successes of these techniques for large language models [32, 33, 1].\nFor example, Decision Transformer (DT) [34, 35] uses a transformer to autoregressively model\nsequences of actions from offline experience data, conditioned on the achieved return. During\ninference, one can then query the model conditioned on a desired return value. This approach has\nbeen shown to scale favorably to large models and multi-task settings [36], at times exceeding the\nperformance of large-scale multi-task imitation learning with transformers [37, 38, 39]. However, DT\nis known to be provably (and unboundedly) sub-optimal in common scenarios [40, 41]. A common\ncriticism of DT, and supervised learned transformers in general, is their inability to improve upon\nthe dataset. For example, there is little reason for DT to output meaningful behavior if conditioned\non return higher than any observed in training, without strong extrapolation assumptions [40]. In\ncontrast, a major contribution of our work is theoretical and empirical evidence for the ability of DPT\nto improve over behaviors seen in the dataset in terms of regret.\nValue and policy-based offline RL. Offline RL algorithms offer the opportunity to learn from\nexisting datasets. To address distributional shift, many prior algorithms incorporate the principle of\nvalue pessimism [42, 43, 44, 45], or policy regularization [46, 47, 48, 49, 50]. To reduce the amount\nof offline data required in a new task, methods for offline meta-RL can reuse interactions collected\nin a set of related tasks [51, 52, 53]. However, they still must address distribution shift, requiring\nsolutions such as policy regularization [51] or additional online interactions [54]. DPT follows\nthe success of autoregressive models like DT and AD, avoiding these issues. With our pretraining\nobjective, DPT also leverages offline datasets for new tasks more effectively than AD.\n3\nIn-Context Learning Model\nBasic decision models. The basic decision model of our study is the finite-horizon Markov decision\nprocess (MDP). An MDP is specified by the tuple \u03c4 = \u27e8S, A, T, R, H, \u03c1\u27e9 to be solved, where S is the\nstate space, A is the action space, T : S \u00d7 A \u2192 \u2206(S) is the transition function, R : S \u00d7 A \u2192 \u2206(R)\nis the reward function, H \u2208 N is the horizon, and \u03c1 \u2208 \u2206(S) is the initial state distribution. A learner\ninteracts with the environment through the following protocol: (1) an initial state s1 is sampled from\n\u03c1; (2) at time step h, the learner chooses an action ah and transitions to state sh+1 \u223c T(\u00b7|sh, ah),\nand receives a reward rh \u223c R(\u00b7|sh, ah). The episode ends after H steps. A policy \u03c0 maps states to\ndistributions over actions and can be used to interact with the MDP. We denote the optimal policy as\n\u03c0\u22c6, which maximizes the value function V (\u03c0\u22c6) = max\u03c0 V (\u03c0) := max\u03c0 E\u03c0\nP\nh rh. When necessary,\nwe use the subscript \u03c4 to distinguish V\u03c4 and \u03c0\u22c6\n\u03c4 for the specific MDP \u03c4. We assume the state space is\npartitioned by h \u2208 [H] so that \u03c0\u22c6 is notationally independent of h. Note this framework encompasses\nmulti-armed bandit settings where the state space is a single point, e.g. S = {1}, H = 1, and the\noptimal policy is a\u22c6 \u2208 argmaxa\u2208A E [r1|a1 = a].\nPretraining. We give pseudocode in Algorithm 1 and a visualization in Figure 1. Let Tpre be a\ndistribution over tasks at the time of pretraining. A task \u03c4 \u223c Tpre can be viewed as a specification\nof an MDP, \u03c4 = \u27e8S, A, T, R, H, \u03c1\u27e9. The distribution Tpre can span different reward and transition\nfunctions and even different state and action spaces. We then sample a context (or a prompt) which\nconsists of a dataset D \u223c Dpre(\u00b7; \u03c4) of interactions between the learner and the MDP specified by\n\u03c4. D = {sj, aj, s\u2032\nj, rj}j\u2208[n] is a collection of transition tuples taken in \u03c4. We refer to D as the\nin-context dataset because it provides the contextual information about \u03c4. D could be generated\nthrough variety of means, such as: (1) random interactions within \u03c4, (2) demonstrations from an\nexpert, and (3) rollouts of an algorithm. Additionally, we independently sample a query state squery\nfrom the distribution Dquery over states S and a label a\u22c6 is sampled from the optimal policy \u03c0\u22c6\n\u03c4(\u00b7|squery)\nfor task \u03c4 (see Section 5.3 for how to implement this in common practical scenarios). We denote the\n3\nFigure 1: A transformer model M\u03b8 is pretrained to predict an optimal action a\u22c6\nquery from a state squery in a task,\ngiven a dataset of interactions from that task. The resulting Decision-Pretrained Transformer (DPT) learns a\ndistribution over the optimal action conditioned on an in-context dataset. M\u03b8 can be deployed in new tasks\nonline by collecting data on the fly, or offline by immediately conditioning on a static dataset.\nAlgorithm 1 Decision-Pretrained Transformer (DPT): Training and Deployment\n1: // Collecting pretraining dataset\n2: Initialize empty pretraining dataset B\n3: for i in [N] do\n4:\nSample task \u03c4 \u223c Tpre, in-context dataset D \u223c Dpre(\u00b7; \u03c4), query state squery \u223c Dquery\n5:\nSample label a\u22c6 \u223c \u03c0\u22c6\n\u03c4(\u00b7|squery) and add (squery, D, a\u22c6) to B\n6: end for\n7: // Pretraining model on dataset\n8: Initialize model M\u03b8 with parameters \u03b8\n9: while not converged do\n10:\nSample (squery, D, a\u22c6) from B and predict \u02c6pj(\u00b7) = M\u03b8(\u00b7|squery, Dj) for all j \u2208 [n]\n11:\nCompute loss in (2) with respect to a\u22c6 and backpropagate to update \u03b8.\n12: end while\n13: // Offline test-time deployment\n14: Sample unknown task \u03c4 \u223c Ttest, sample dataset D \u223c Dtest(\u00b7; \u03c4)\n15: Deploy M\u03b8 in \u03c4 by choosing ah \u2208 argmaxa\u2208A M\u03b8(a|sh, D) at step h\n16: // Online test-time deployment\n17: Sample unknown task \u03c4 \u223c Ttest and initialize empty D = {}\n18: for ep in max_eps do\n19:\nDeploy M\u03b8 by sampling ah \u223c M\u03b8(\u00b7|sh, D) at step h\n20:\nAdd (s1, a1, r1, . . .) to D\n21: end for\njoint pretraining distribution over tasks, in-context datasets, query states, and action labels as Ppre:\nPpre(\u03c4, D, squery, a\u22c6) = Tpre(\u03c4)Dpre(D; \u03c4)Dquery(squery)\u03c0\u22c6\n\u03c4(a\u22c6|squery)\n(1)\nGiven the in-context dataset D and a query state squery, we can train a model to predict the optimal\naction a\u22c6 in response simply via supervised learning. Let Dj = {(s1, a1, s\u2032\n1, r1), . . . , (sj, aj, s\u2032\nj, rj)}\ndenote the partial dataset up to j samples. Formally, we aim to train a causal GPT-2 transformer\nmodel M parameterized by \u03b8, which outputs a distribution over actions A, to minimize the expected\nloss over samples from the pretraining distribution:\nmin\u03b8 EPpre\nP\nj\u2208[n] \u2113 (M\u03b8(\u00b7 | squery, Dj), a\u22c6)\n(2)\nGenerally, we set the loss to be the negative log-likelihood with \u2113(M\u03b8(\u00b7 |squery, Dj), a\u22c6) :=\n\u2212 log M\u03b8(a\u22c6 | squery, Dj). This framework can work for both discrete and continuous A. For\nour experiments with discrete A, we use a softmax parameterization for the distribution of M\u03b8,\nessentially treating this as a classification problem. The resulting output model M\u03b8 can be viewed\nas an algorithm that takes in a dataset of interactions D and can be queried with a forward pass for\npredictions of the optimal action via inputting a query state squery. We refer to the trained model M\u03b8\nas a Decision-Pretrained Transformer (DPT).\nTesting. After pretraining, a new task (MDP) \u03c4 is sampled from a test-task distribution Ttest. If the\nDPT is to be tested offline, then a dataset (prompt) is a sampled D \u223c Dtest( \u00b7 ; \u03c4) and the policy that\n4\nthe model in-context learns is given conditionally as M\u03b8(\u00b7 | \u00b7, D). Namely, we evaluate the policy\nby selecting action ah \u2208 argmaxa M\u03b8(a|sh, D) when the learner visits state sh. If the model is to\nbe tested online through multiple episodes of interaction, then the dataset is initialized as empty\nD = {}. At each episode, M\u03b8(\u00b7 | \u00b7, D) is deployed where the model samples ah \u223c M\u03b8(\u00b7|sh, D) upon\nobserving state sh. Throughout a full episode, it collects interactions {s1, a1, r1, . . . , sH, aH, rH}\nwhich are subsequently appended to D. The model then repeats the process with another episode,\nand so on until a specified number of episodes has been reached.\nA key distinction of the testing phase is that there are no updates to the parameters of M\u03b8. This is in\ncontrast to hand-designed RL algorithms that would perform parameter updates or maintain statistics\nusing D to learn from scratch. Instead, the model M\u03b8 performs a computation through its forward\npass to generate a distribution over actions conditioned on the in-context D and query state sh.\nSources of distribution mismatch. Inherent to pretraining, like nearly all foundation models, is\ndistribution mismatch on downstream test-time tasks. DPT pretrained on sufficiently diverse data\nshould ideally be robust (to some extent) to these mismatches. (1) When deployed, M\u03b8 will execute its\nlearned policy which invariably induces a distribution over states different from Dquery. (2) Pretraining\nTpre likely differs from the downstream Ttest. (3) Similarly, the test-time datasets prompts can also\ndiffer, especially online where they are collected by M\u03b8 itself.\n4\nLearning in Bandits\nWe begin with an empirical investigation of DPT in a multi-armed bandit, a well-studied special case\nof the MDP where the state space S is a singleton and the horizon H = 1 is a single step. We will\nexamine the performance of DPT both when aiming to select a good action from offline historical\ndata and for online learning where the goal is to maximize cumulative reward from scratch. Offline, it\nis critical to account for uncertainty due to noise as certain actions may not be sampled well enough.\nOnline, it is critical to judiciously balance exploration and exploitation to minimize overall regret.\nFor detailed descriptions of the experiment setups, see Appendix A.\nPretraining distribution. For the pretraining task distribution Tpre, we sample 5-armed bandits\n(|A| = 5). The reward function for arm a is a normal distribution R(\u00b7|s, a) = N(\u00b5a, \u03c32) where\n\u00b5a \u223c Unif[0, 1] independently and \u03c3 = 0.3. To generate in-context datasets Dpre, we randomly\ngenerate action frequencies by sampling probabilities from a Dirichlet distribution and mixing them\nwith a point-mass distribution on one random arm (see details in Appendix A.3). Then we sample the\nactions accordingly from this distribution. This encourages diversity of the in-context datasets. The\noptimal policy \u03c0\u22c6\n\u03c4 for bandit \u03c4 is argmaxa \u00b5a, which we can easily compute during pretraining. We\npretrain the model M\u03b8 to predict a\u22c6 from D as described in Section 3 for datasets up to size n = 500.\nComparisons. We compare to several well-known algorithms for bandits3. All of the algorithms are\ndesigned to reason in a particular way about uncertainty based on their observations.\n\u2022 Empirical mean algorithm (Emp) selects the action with the highest empirical mean reward naively.\n\u2022 Upper Confidence Bound (UCB) selects the action with the highest upper confidence bound.\n\u2022 Lower Confidence Bound (LCB) selects the action with the highest lower confidence bound.\n\u2022 Thompson Sampling (TS) selects the action with the highest sampled mean from a posterior\ndistribution over reward models. The prior and likelihood functions are Gaussian.\nEmp and TS [55, 56] can both be used for offline or online learning; UCB [57] is known to be\nprovably optimal online by ensuring exploration through optimism under uncertainty; and LCB\n[58, 59] is used to minimize suboptimality given an offline dataset by selecting actions pessimistically.\nIt is the opposite of UCB. We evaluate algorithms with standard bandit metrics. Offline, we use the\nsuboptimality \u00b5a\u22c6 \u2212\u00b5\u02c6a where \u02c6a is the chosen action. Online, we use cumulative regret: P\nk \u00b5a\u22c6 \u2212\u00b5\u02c6ak\nwhere \u02c6ak is the kth action chosen.\nDPT learns to reason through uncertainty. As shown in Figure 2a, in the offline setting, DPT\nsignificantly exceeds the performance of Emp and LCB while matching the performance of TS, when\nthe in-context datasets are sampled from the same distribution as during pretraining. The results\nsuggest that the transformer is capable of reasoning through uncertainty caused by the noisy rewards\nin the dataset. Unlike Emp which can be fooled by noisy, undersampled actions, the transformer has\n3See Appendix A.2 for additional details such as hyperparameters.\n5\n0\n100\n200\n300\n400\n500\nData\n0.1\n0.3\n0.02\nSuboptimality\nOffline Bandit\nEmp\nLCB\nTS\nDPT (ours)\n(a)\n0\n100\n200\n300\n400\n500\nData\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nCumulative Regret\nOnline Bandit\nEmp\nUCB\nTS\nDPT (ours)\n(b)\n0.0\n0.3\n0.5\nNoise standard deviation\n0\n5\n10\n15\n20\n25\n30\nFinal Cumulative Regret\nOnline Robustness to Reward Shift\nEmp\nUCB\nTS\nDPT (ours)\n(c)\nFigure 2: (a) Offline performance on in-distribution bandits, given random in-context datasets. (b) Online\ncumulative regret on bandits. (c) Final (after 500 steps) cumulative regret on out-of-distribution bandits with\ndifferent Gaussian noise standard deviations. The mean and standard error are computed over 200 test tasks.\nlearned to hedge to a degree. However, it also suggests that this hedging is fundamentally different\nfrom what LCB does, at least on this specific distribution4.\nInterestingly, the same transformer produces an extremely effective online bandit algorithm when\nsampling actions instead of taking an argmax. As shown in Figure 2b, DPT matches the performance\nof classical optimal algorithms, UCB and TS, which are specifically designed for exploration. This is\nnotable because DPT was not explicitly trained to explore, but its emergent strategy is on par with\nsome of the best. In Figure 2c, we show this property is robust to noise in the rewards not seen during\npretraining by varying the standard deviation. In Appendix B, we show this generalization happens\noffline too and even with unseen Bernoulli rewards.\nLeveraging structure from suboptimal data. We now investigate whether DPT can learn to\nleverage the inherent structure of a problem class, even without prior knowledge of this structure\nand even when learning from in-context datasets that do not explicitly utilize it. More precisely,\nwe consider Tpre to be a distribution over linear bandits, where the reward function is given by\nE [r | a, \u03c4] = \u27e8\u03b8\u03c4, \u03d5(a)\u27e9 and \u03b8\u03c4 \u2208 Rd is a task-specific parameter vector and \u03d5 : A \u2192 Rd is fixed\nfeature vector that is the same for all tasks. Given the feature representation \u03d5, LinUCB [60], a\nUCB-style algorithm that leverages \u03d5, should achieve regret e\nO(d\n\u221a\nK) over K steps, a substantial\ngain over UCB and TS when d \u226a |A|. Here, we pretrain a DPT model with in-context datasets\ngathered by TS, which does not leverage the linear structure. Figures 3a and 3b show that DPT can\nexploit the unknown linear structure, essentially learning a surrogate for \u03d5, allowing to do more\ninformed exploration online and decision-making offline. It is nearly on par with LinUCB (which\nis given \u03d5) and significantly outperforms the dataset source, TS, which does not know or use the\nstructure. These results present evidence that (1) DPT can automatically leverage structure, and (2)\nsupervised learning-based approaches to RL can learn novel explorations that transcend the quality\nof their pretraining data.\nAdapting to expert-biased datasets. A common assumption in offline RL is that datasets tend to\nbe a mixture between optimal data (e.g. expert demonstrations) and suboptimal data (e.g. random\ninteractions) [61]. Hence, LCB is generally effective in practice and the pretraining and testing\ndistributions should be biased towards this setting. Motivated by this, we pretrain a second DPT\nmodel where Dpre is generated by mixing the in-context datasets with varying fractions of expert\ndata, biasing Dpre towards datasets that contain more examples of the optimal action. We denote this\nmodel by DPT-Exp. In Figure 3c, we plot the test-time performance of both pretrained models when\nevaluated on new offline datasets with varying percentages of expert data5. Our results suggest that\nwhen the pretraining distribution is also biased towards expert-suboptimal data, DPT-Exp behaves\nsimilarly to LCB, while DPT continues to resemble TS. This is quite interesting as for other methods,\nsuch as TS, it is less clear how to automatically incorporate the right amount of expert bias to yield\nthe same effect, but DPT can leverage this from pretraining.\n4Note our randomly generated environments are equally likely to have expert-biased datasets and adversarial\ndatasets, so LCB is not expected to outperform here [58].\n5That is, 0% is fully random while 100% has only optimal actions in the in-context dataset.\n6\n0\n25\n50\n75\n100 125 150 175 200\nData\n10\n2\n10\n1\nSuboptimality\nOffline Linear Bandit\nTS (source)\nLinReg\nDPT (ours)\n(a)\n0\n25\n50\n75\n100 125 150 175 200\nData\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nCumulative Regret\nOnline Linear Bandit\nTS (source)\nLinUCB\nDPT (ours)\n(b)\n0\n50\n100\n% of Expert Data\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nFinal Suboptimality\nOffline Adaptation to Expert Datasets\nEmp\nLCB\nTS\nDPT (ours)\nDPT-Exp (ours)\n(c)\nFigure 3: (a) Offline performance of DPT trained on linear bandits from TS source data. LinReg does linear\nregression and outputs the greedy action. (b) Online cumulative regret of the same model. The mean and standard\nerror are computed over 200 test tasks. (c) Offline performance on expert-biased datasets. DPT pretrained on a\ndifferent prior continues to match TS, but DPT-Exp trained from a more representative prior excels.\n5\nLearning in Markov Decision Processes\nWe next study how DPT can tackle Markov decision processes by testing its ability to perform\nexploration and credit assignment. In the following experiments, the DPT demonstrates generalization\nto new tasks, scalability to image-based observations, and capability to stitch in-context behaviors\n(Section 5.2). This section also examines whether DPT can be pretrained with datasets and action\nlabels generated by a different RL algorithm, rather than the exact optimal policy (Section 5.3).\n5.1\nExperimental Setup\nEnvironments. We consider environments that require targeted exploration to solve the task. The\nfirst is Dark Room [20, 7], a 2D discrete environment where the agent must locate the unknown\ngoal location in a 10 \u00d7 10 room, and only receives a reward of 1 when at the goal. We hold out a\nset of goals for generalization evaluation. Our second environment is Miniworld [62], a 3D visual\nnavigation problem to test the scalability of DPT to image observations. The agent is in a room with\nfour boxes of different colors, and must find the target box, the color of which is unknown to the agent\ninitially. It receives a reward of 1 only when near the correct box. Details on these environments and\nthe pre-training datasets are in App. A.4 and A.5.\nComparisons. Our experiments aim to understand the effectiveness of DPT in comparison to that of\nother context-based meta-RL algorithms. To that end, we compare to meta-RL algorithms based on\nsupervised and RL objectives.\n\u2022 Proximal Policy Optimization (PPO) [63]: We compare to this single-task RL algorithm, which\ntrains from scratch without any pretraining data, to contextualize the performance of DPT and\nother meta-RL algorithms.\n\u2022 Algorithm Distillation (AD) [7]: AD first generates a dataset of learning histories by running an RL\nalgorithm in each training task. Then, given a sampled subsequence hj = (sj, aj, rj, . . . , sj+c)\nfrom a learning history, a tranformer is trained to predict the next action aj+c from the learning\nhistory.\n\u2022 RL2 [27]: This online meta-RL comparison uses a recurrent neural network to adapt the agent\u2019s\npolicy from the given context. Unlike AD and DPT, which are trained with a supervised objective,\nthe RL2 agent is trained to maximize the expected return with PPO.\nPPO and RL2 are online algorithms, while AD is capable of learning both offline and online. Details\non the implementation of these algorithms can be found in Appendix A.2.\n5.2\nMain Results\nGeneralizing to new offline datasets and tasks. To study the generalization capabilities of DPT,\nwe evaluate the model in Dark Room on a set of 20 held-out goals not in the pretraining dataset.\nWhen given an expert dataset, DPT achieves near-optimal performance. Even when given a random\ndataset, which has an average total reward of 1.1, DPT obtains a much higher average return of\n61.5 (see Fig. 4a). Qualitatively, we observe that when the in-context dataset contains a transition to\nthe goal, DPT immediately exploits this and takes a direct path to the goal. In contrast, while AD\n7\nRandom\nExpert\nDataset Composition\n0\n20\n40\n60\n80\nReturn\nOffline Dark Room (Test Tasks)\nDataset\nAD\nDPT (ours)\n(a)\n0\n5\n10\n15\n20\n25\n30\n35\n40\nEpisodes\n0\n20\n40\n60\n80\nReturn\nOnline Dark Room (Test Tasks)\nDPT (ours)\nAD\nPPO\nRL2\n(b)\nRandom\nExpert\nDataset Composition\n0\n10\n20\n30\n40\nReturn\nOffline Miniworld\nDataset\nAD\nDPT (ours)\n(c)\n0\n5\n10\n15\n20\n25\n30\n35\n40\nEpisodes\n0\n10\n20\n30\n40\n50\nReturn\nOnline Miniworld\nDPT (ours)\nAD\nPPO\nRL2\n(d)\nFigure 4: (a) Offline performance on held-out Dark Room goals, given random and expert datasets. (b) Online\nperformance on held-out Dark Room goals. (c) Offline performance on Miniworld (images), given random and\nexpert datasets. (d) Online performance on Miniworld (images) after 40 episodes. We report the average and\nstandard error of the mean over 100 different offline datasets in (a) and (c) and 20 online trials in (b) and (d).\n0.0\n2.5\n5.0\n7.5\nx\n2\n0\n2\n4\n6\n8\n10\ny\n(a)\nRandom\nExpert\nDataset Composition\n0\n20\n40\n60\n80\n100\nReturn\nOffline PPO Ablations (Test Tasks)\nDPT\nDPT (Rand, PPO)\nDPT (PPO, PPO)\nAD\n(b)\n0\n5\n10\n15\n20\n25\n30\n35\n40\nEpisodes\n0\n20\n40\n60\n80\nReturn\nOnline PPO Ablations (Test Tasks)\nDPT\nDPT (Rand, PPO)\nDPT (PPO, PPO)\nAD\n(c)\nFigure 5: (a) In Dark Room (Three Tasks), DPT stitches a new, optimal trajectory to the goal (blue) given two\nin-context demonstrations of other tasks (pink and orange). (b) Offline Dark Room performance of DPT trained\non PPO data. (c) Online Dark Room performance of DPT trained on PPO data.\ndemonstrates strong offline performance with expert data, it performs worse in-context learning with\nrandom data compared to DPT. The difference arises because AD is trained to infer a better policy\nthan the in-context data, but not necessarily the optimal one.\nWe next evaluate DPT, AD, RL2, and PPO online without any prior data from the 20 test-time Dark\nRoom tasks, shown in Fig. 4b. After 40 episodes, PPO does not make significant progress towards\nthe goal, highlighting the difficulty of learning from such few interactions alone. RL2 is trained to\nperform adaptation within four episodes each of length 100, and we report the performance after the\nfour adaptation episodes. Notably, DPT on average solves each task faster than AD and reaches a\nhigher final return than RL2, demonstrating its capability to explore effectively online even in MDPs.\nIn Appendix B, we also present results on generalization to new dynamics.\nLearning from image-based observations. In Miniworld, the agent receives RGB image observa-\ntions of 25 \u00d7 25 pixels. As shown in Fig. 4d, DPT can solve this high-dimensional task offline from\nboth random and expert datasets. Compared to AD and RL2, DPT also learns online more efficiently.\nStitching novel trajectories from in-context subsequences. A desirable property of some offline\nRL algorithms is the ability to stitch suboptimal subsequences from the offline dataset into new\ntrajectories with higher return. To test whether DPT exhibits stitching, we design the Dark Room\n(Three Tasks) environment in which there are three possible tasks. The pretraining data consists only\nof expert demonstrations of two of them. At test-time DPT is evaluated on third unseen task, but its\noffline dataset is only expert demonstrations of the original two. Despite this, it leverages the data to\ninfer a path solving the third task (see Fig. 5a).\n5.3\nLearning from Algorithm-Generated Policies and Rollouts\nSo far, we have only considered action labels provided by an optimal policy. However, in some tasks,\nan optimal policy is not readily available even in pretraining. In this experiment, we use actions\nlabeled by a policy learned via PPO and in-context datasets sampled from PPO replay buffers. We\ntrain PPO agents in each of the 80 train tasks for 1K episodes to generate 80K total rollouts, from\nwhich we sample the in-context datasets. This variant, DPT (PPO, PPO), performs on par with DPT\nand still better than AD, as shown in Figures 5b and 5c. DPT (PPO, PPO) can be viewed as a direct\ncomparison between our pretraining objective and that of AD, given the same pretraining data but\n8\njust used differently. We also evaluated a variant, DPT (Rand, PPO), which pretrains on random\nin-context datasets (like DPT), but still using PPO action labels. The performance is worse than\nthe other DPT variants in some settings, but only marginally so. In Appendix B, we analyze the\nsensitivity of DPT to other hyperparameters, such as the context size and amount of pretraining data.\n6\nTheory\nWe now shed light on the observations of the previous empirical results through a theoretical analysis.\nOur main result shows that DPT (under a slight modification to pretraining) essentially performs\nin-context posterior sampling (PS). PS is a generalization of Thompson Sampling for RL in MDPs. It\nmaintains and samples from a posterior over tasks \u03c4 given historical data D and executes optimal\npolicies \u03c0\u22c6\n\u03c4 (see Appendix C for a formal outline). It is provably sample-efficient with online Bayesian\nregret guarantees [12], but maintaining posteriors is generally computationally intractable. The ability\nfor DPT to perform PS in-context suggests a path towards computation- and provably sample-efficient\nRL with priors learned from the data.\n6.1\nHistory-Dependent Pretraining and Assumptions\nWe start with a modification to the pretraining of DPT. Rather than conditioning only on squery and\nD to predict a\u22c6 \u223c \u03c0\u22c6\n\u03c4(\u00b7|squery), we propose also conditioning on a sequence \u03beh = (s1:h, a\u22c6\n1:h) where\ns1:h \u223c Sh \u2208 \u2206(Sh) is a distribution over sets of states, independent of \u03c4, and a\u22c6\nh\u2032 \u223c \u03c0\u22c6\n\u03c4(\u00b7|sh\u2032)\nfor h\u2032 \u2208 [h]. Thus, we use \u03c0\u22c6\n\u03c4 to label both the query state (which is the prediction label) and the\nsequence of states sampled from Sh. Note that this does not require any environment interactions\nand hence no sampling from either T\u03c4 or R\u03c4. At test-time at step h, this will allow us to condition on\nthe history \u03beh\u22121 of states that M\u03b8 visits and the actions that it takes in those states. Formally, the\nlearned M\u03b8 is deployed as follows, given D. (1) At h = 0, initialize \u03be0 = () to be empty. (2) At\nstep h, visit sh and find ah by sampling from M\u03b8(\u00b7|squery, D, \u03beh\u22121). (3) Append (sh, ah) to \u03beh\u22121 to\nget \u03beh. Note for bandits and contextual bandits (H = 1), there is no difference between this and the\noriginal pretraining procedure of prior sections because \u03be0 is empty. For MDPs, the original DPT can\nbe viewed as a convenient approximation.\nWe now make several assumptions to simplify the analysis. First, assume Dquery, Dpre, and S have\nsufficient support such that all conditional probabilities of Ppre are well defined. Similar to other\nstudies of in-context learning [64], we assume M\u03b8 fits the pretraining distribution exactly with enough\ncoverage and data, so that the focus of the analysis is just the in-context learning abilities.\nAssumption 1. (Learned model is consistent). Let M\u03b8 denote the pretrained model. For all\n(squery, D, \u03beh), we have Ppre(a|squery, D, \u03beh) = M\u03b8(a|squery, D, \u03beh) for all a \u2208 A.\nTo provide some cursory justification,\nif M\u03b8\nis the global minimizer of (2),\nthen\nEPpre\u2225Ppre(\u00b7|squery, D, \u03beh) \u2212 M\u03b8(\u00b7|squery, D, \u03beh)\u22252\n1 \u2192 0 as the number of pretraining samples\nN \u2192 \u221e with high probability for transformer model classes of bounded complexity (see Proposi-\ntion C.1). Approximate versions of the above assumptions are easily possible but obfuscate the key\nelements of the analysis. We also assume that the in-context dataset D \u223c Dpre is compliant [59],\nmeaning that the actions from D can depend only on the observed history and not additional con-\nfounders. Note that this still allows Dpre to be very general \u2014 it could be generated randomly or from\nadaptive algorithms like PPO or TS.\nDefinition 6.1 (Compliance). The in-context dataset distribution Dpre(\u00b7; \u03c4) is compliant if, for all\ni \u2208 [n], the ith action of the dataset, ai, is conditionally independent of \u03c4 given the ith state si and\npartial dataset, Di\u22121, so far. In other words, the distribution Dpre(ai|si, Di\u22121; \u03c4) is invariant to \u03c4.\nGenerally, Dpre can influence M\u03b8. In Proposition 6.4, we show that all compliant Dpre form a sort of\nequivalence class that generate the same M\u03b8. For the remainder, we assume all Dpre are compliant.\n6.2\nMain Results\nEquivalence of DPT and PS.\nWe now state our main result which shows that the trajectories\ngenerated by a pretrained M\u03b8 will follow the same distribution as those from a well-specified PS\nalgorithm. In particular, let PS use the well-specified prior Tpre. Let \u03c4c be an arbitrary task. Let\n9\nPps(\u00b7 | D, \u03c4c) and PM\u03b8(\u00b7 | D, \u03c4c) denote the distributions over trajectories \u03beH \u2208 (S \u00d7A)H generated\nfrom running PS and M\u03b8(\u00b7|\u00b7, D, \u00b7), respectively, in task \u03c4c given historical data D.\nTheorem 1 (DPT\n\u21d0\u21d2\nPS). Let the above assumptions hold.\nThen, Pps(\u03beH | D, \u03c4c) =\nPM\u03b8(\u03beH | D, \u03c4c) for all trajectories \u03beH.\nRegret implications.\nTo see this result in action, let us specialize to the finite MDP setting [12].\nSuppose we pretrain M\u03b8 on a distribution Tpre over MDPs with S := |S| and A := |A|. Let Dpre be\nconstructed by uniform sampling (si, ai) and observing (ri, s\u2032\ni) for i \u2208 [KH]. Let E [rh|sh, ah] \u2208\n[0, 1]. And let Dquery and Sh be uniform over S and Sh (for all h) respectively. Finally, let Ttest\nbe the distribution over test tasks with the same cardinalities. For a task \u03c4, define the online\ncumulative regret of DPT over K episodes as Reg\u03c4(M\u03b8) := P\nk\u2208[K] V\u03c4(\u03c0\u22c6\n\u03c4) \u2212 V\u03c4(\u02c6\u03c0k) where\n\u02c6\u03c0k(\u00b7|sh) = M\u03b8(\u00b7|sh, D(k\u22121), \u03beh\u22121) and D(k) contains the first k episodes collected from \u02c6\u03c01:k.\nCorollary 6.2 (Finite MDPs). Suppose that sup\u03c4 Ttest(\u03c4)/Tpre(\u03c4) \u2264 C for some C > 0. For the\nabove MDP setting, the pretrained model M\u03b8 satisfies ETtest [Reg\u03c4(M\u03b8)] \u2264 e\nO(CH3/2S\n\u221a\nAK).\nA similar analysis due to [65] allows us to prove why pretraining on (latently) linear bandits can\nlead to substantial empirical gains, even when the in-context datasets are generated by algorithms\nunaware of this structure. We observed this empirically in Section 4. Consider a similar setup as there\nwhere S is a singleton, A is finite but large, \u03b8\u03c4 \u2208 Rd is sampled as \u03b8\u03c4 \u223c N(0, I/d), \u03d5 : A \u2192 Rd\nis a fixed feature map with supa\u2208A \u2225\u03d5(a)\u22252 \u2264 1, and the reward of a \u2208 A in task \u03c4 is distributed\nas N(\u27e8\u03b8\u03c4, \u03d5(a)\u27e9, 1). This time, we let Dpre(\u00b7; \u03c4) be given by running Thompson Sampling with\nGaussian priors and likelihood functions on \u03c4.\nCorollary 6.3 (Latent representation learning in linear bandits). For Ttest = Tpre in the above linear\nbandit setting, M\u03b8 satisfies ETtest [Reg\u03c4(M\u03b8)] \u2264 e\nO(d\n\u221a\nK).\nThis significantly improves over the e\nO(\np\n|A|K) upper regret bound for TS that does not leverage the\nlinear structure. This highlights how DPT can have provably tighter upper bounds on future bandit\nproblems than the algorithms used to generate its (pretraining) data. Note that if there is additional\nstructure in the tasks which yields a tighter regret bound (for example if there are only a small finite\nnumber of known MDPs in the possible distribution), that may further improve performance, such as\nby removing the dependence on the problem finite state, action or full d-dimensional representation.\nInvariance of M\u03b8 to compliant Dpre.\nOur final result sheds light on how Dpre impacts the final\nDPT behavior M\u03b8. Combined with Assumption 1, M\u03b8 is invariant to Dpre satisfying Definition 6.1.\nProposition 6.4. Let P 1\npre and P 2\npre be pretraining distributions that differ only by their in-context\ndataset distributions, denoted by D1\npre and D2\npre. If D1\npre and D2\npre are compliant with the same support,\nthen P 1\npre(a\u22c6|squery, D, \u03beh) = P 2\npre(a\u22c6|squery, D, \u03beh) for all a\u22c6, squery, D, \u03beh.\nThat is, if we generate in-context datasets D by running various algorithms that depend only on the\nobserved data in the current task, we will end up with the same M\u03b8. For example, TS could be used\nfor D1\npre and PPO for D2\npre. Expert-biased datasets discussed in Section 4 violate Definition 6.1, since\nprivileged knowledge of \u03c4 is being used. This helps explain our empirical results that pretraining on\nexpert-biased datasets leads to a qualitatively different learned model at test-time.\n7\nDiscussion\nIn this paper, we studied the problem of in-context decision-making. We introduced a new pretraining\nmethod and transformer model, DPT, which is trained via supervised learning to predict optimal\nactions given an in-context dataset of interactions. Through in-depth evaluations in classic decision\nproblems in bandits and MDPs, we showed that this simple objective naturally gives rise to an\nin-context RL algorithm that is capable of online exploration and offline decision-making, unlike\nother algorithms that are explicitly trained or designed to do these. Our empirical and theoretical\nresults provide first steps towards understanding these capabilities that arise from DPT and what\nfactors are important for it to succeed. The inherent strength of pretraining lies in its simplicity\u2013we\ncan sidestep the complexities of hand-designing exploration or conservatism in RL algorithms and\nwhile simultaneously allowing the transformer to derive novel strategies that best leverage problem\n10\nstructure. These findings underscore the potential of supervised pretraining in equipping transformer\nmodels with in-context decision-making abilities.\nLimitations and future work.\nOne limitation of DPT is the requirement of optimal actions at\npretraining. Empirically, we find that this requirement can be relaxed by using actions generated\nby another RL-trained agent during pretraining, which only leads to a slight loss in performance.\nHowever, fully understanding this problem and how best to leverage multi-task decision-making\ndatasets remains a key open problem. We also discussed that the practical implementation for MDPs\ndiffers from true posterior sampling. It would be interesting to further understand and bridge this\nempirical-theoretical gap in the future. We also remark that our preliminary analysis shows promise\nfor DPT to generalize to new tasks beyond its pretraining distribution. This suggests that diversifying\nthe task distributions during pretraining could significantly enhance the model\u2019s ability to generalize\nto new tasks. This possibility holds an exciting avenue for future work. Finally, further investigation\nis required to understand the implications of these findings for existing foundation models, such as\ninstruction-finetuned models, that are increasingly being deployed in decision-making settings [66].\nAcknowledgments and Disclosure of Funding\nWe thank Evan Liu, Sherry Yang, and Lucy Shi for helpful discussions and feedback. This work was\nsupported in part by NSF grant 2112926 and ONR grant N00014-21-1-2685. JNL acknowledges\nsupport from the NSF GRFP.\nReferences\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[2] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning\nwork? arXiv preprint arXiv:2202.12837, 2022.\n[3] Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre\nRichemond, James McClelland, and Felix Hill. Data distributional properties drive emer-\ngent in-context learning in transformers. Advances in Neural Information Processing Systems,\n35:18878\u201318891, 2022.\n[4] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers\nlearn in-context? a case study of simple function classes. Advances in Neural Information\nProcessing Systems, 35:30583\u201330598, 2022.\n[5] Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining\nterm frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.\n[6] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.\nWhat\nlearning algorithm is in-context learning? investigations with linear models. arXiv preprint\narXiv:2211.15661, 2022.\n[7] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steiger-\nwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement\nlearning with algorithm distillation. arXiv preprint arXiv:2210.14215, 2022.\n[8] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang\nGan. Prompting decision transformer for few-shot policy generalization. In International\nConference on Machine Learning, pages 24631\u201324645. PMLR, 2022.\n[9] Mengdi Xu, Yuchen Lu, Yikang Shen, Shun Zhang, Ding Zhao, and Chuang Gan. Hyper-\ndecision transformer for efficient online policy adaptation. arXiv preprint arXiv:2304.08487,\n2023.\n[10] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n11\n[11] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:\nTutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n[12] Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via\nposterior sampling. Advances in Neural Information Processing Systems, 26, 2013.\n[13] Tom Schaul and J\u00fcrgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010.\n[14] Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Citeseer,\n1990.\n[15] Justin Fu, Sergey Levine, and Pieter Abbeel. One-shot learning of manipulation skills with online\ndynamics adaptation and neural network priors. In 2016 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 4019\u20134026. IEEE, 2016.\n[16] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine,\nand Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-\nreinforcement learning. arXiv preprint arXiv:1803.11347, 2018.\n[17] Nicholas C Landolfi, Garrett Thomas, and Tengyu Ma. A model-based approach for sample-\nefficient multi-task reinforcement learning. arXiv preprint arXiv:1907.04964, 2019.\n[18] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient\noff-policy meta-reinforcement learning via probabilistic context variables. In International\nconference on machine learning, pages 5331\u20135340. PMLR, 2019.\n[19] Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A Ortega, Yee Whye Teh, and\nNicolas Heess. Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424,\n2019.\n[20] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,\nand Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-\nlearning. arXiv preprint arXiv:1910.08348, 2019.\n[21] Evan Z Liu, Aditi Raghunathan, Percy Liang, and Chelsea Finn. Decoupling exploration and\nexploitation for meta-reinforcement learning without sacrifices. In International conference on\nmachine learning, pages 6925\u20136935. PMLR, 2021.\n[22] Theodore J Perkins, Doina Precup, et al. Using options for knowledge transfer in reinforcement\nlearning. Technical report, Citeseer, 1999.\n[23] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-\nreinforcement learning of structured exploration strategies. Advances in neural information\nprocessing systems, 31, 2018.\n[24] Yiding Jiang, Evan Liu, Benjamin Eysenbach, J Zico Kolter, and Chelsea Finn. Learning options\nvia compression. Advances in Neural Information Processing Systems, 35:21184\u201321199, 2022.\n[25] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In International conference on machine learning, pages 1126\u20131135.\nPMLR, 2017.\n[26] Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal\nmeta-policy search. arXiv preprint arXiv:1810.06784, 2018.\n[27] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2:\nFast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,\n2016.\n[28] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,\nCharles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.\narXiv preprint arXiv:1611.05763, 2016.\n[29] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive\nmeta-learner. arXiv preprint arXiv:1707.03141, 2017.\n12\n[30] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and\nFeryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv\npreprint arXiv:2303.03982, 2023.\n[31] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foun-\ndation models for decision making: Problems, methods, and opportunities. arXiv preprint\narXiv:2303.04129, 2023.\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[34] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. Advances in neural information processing systems, 34:15084\u201315097,\n2021.\n[35] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big\nsequence modeling problem. Advances in neural information processing systems, 34:1273\u2013\n1286, 2021.\n[36] Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio\nGuadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game\ndecision transformers. Advances in Neural Information Processing Systems, 35:27921\u201327936,\n2022.\n[37] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.\nA generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n[38] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics\ntransformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.\n[39] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior\ntransformers: Cloning k modes with one stone. Advances in neural information processing\nsystems, 35:22955\u201322968, 2022.\n[40] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna.\nWhen does return-conditioned supervised learning work for offline reinforcement learning?\narXiv preprint arXiv:2206.01079, 2022.\n[41] Mengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Dichotomy of control:\nSeparating what you can control from what you cannot. arXiv preprint arXiv:2210.13435, 2022.\n[42] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for\noffline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u2013\n1191, 2020.\n[43] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea\nFinn. Combo: Conservative offline model-based policy optimization. Advances in neural\ninformation processing systems, 34:28954\u201328967, 2021.\n[44] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch\noff-policy reinforcement learning without great exploration. Advances in neural information\nprocessing systems, 33:1264\u20131274, 2020.\n[45] Kamyar Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating\nuncertainties for offline rl through ensembles, and why their independence matters. Advances in\nNeural Information Processing Systems, 35:18267\u201318281, 2022.\n13\n[46] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning\nwithout exploration. In International conference on machine learning, pages 2052\u20132062.\nPMLR, 2019.\n[47] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-\npolicy q-learning via bootstrapping error reduction. Advances in Neural Information Processing\nSystems, 32, 2019.\n[48] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement\nlearning. arXiv preprint arXiv:1911.11361, 2019.\n[49] Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael\nNeunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing\nwhat worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint\narXiv:2002.08396, 2020.\n[50] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient\nwith state distribution correction. UAI, 2019.\n[51] Lanqing Li, Rui Yang, and Dijun Luo. Focal: Efficient fully-offline meta-reinforcement learning\nvia distance metric learning and behavior regularization. arXiv preprint arXiv:2010.01112,\n2020.\n[52] Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline meta-\nreinforcement learning with advantage weighting. In International Conference on Machine\nLearning, pages 7780\u20137791. PMLR, 2021.\n[53] Ron Dorfman, Idan Shenfeld, and Aviv Tamar.\nOffline meta reinforcement learning\u2013\nidentifiability challenges and effective data collection strategies. Advances in Neural Information\nProcessing Systems, 34:4607\u20134618, 2021.\n[54] Vitchyr H Pong, Ashvin V Nair, Laura M Smith, Catherine Huang, and Sergey Levine. Offline\nmeta-reinforcement learning with online self-supervision. In International Conference on\nMachine Learning, pages 17811\u201317829. PMLR, 2022.\n[55] Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial\non thompson sampling. Foundations and Trends\u00ae in Machine Learning, 11(1):1\u201396, 2018.\n[56] William R Thompson. On the likelihood that one unknown probability exceeds another in view\nof the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933.\n[57] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed\nbandit problem. Machine learning, 47:235\u2013256, 2002.\n[58] Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo Dai, Tor Lattimore, Lihong Li, Csaba Szepes-\nvari, and Dale Schuurmans. On the optimality of batch policy optimization algorithms. In\nInternational Conference on Machine Learning, pages 11362\u201311371. PMLR, 2021.\n[59] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In\nInternational Conference on Machine Learning, pages 5084\u20135096. PMLR, 2021.\n[60] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear\nstochastic bandits. In Advances in Neural Information Processing Systems, pages 2312\u20132320,\n2011.\n[61] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline\nreinforcement learning and imitation learning: A tale of pessimism. Advances in Neural\nInformation Processing Systems, 34:11702\u201311716, 2021.\n[62] Maxime Chevalier-Boisvert. Miniworld: Minimalistic 3d environment for rl and robotics\nresearch, 2018.\n[63] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n14\n[64] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of\nin-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\n[65] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics\nof Operations Research, 39(4):1221\u20131243, 2014.\n[66] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023.\n[67] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mord-\nvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient\ndescent. arXiv preprint arXiv:2212.07677, 2022.\n[68] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning\nand induction heads. arXiv preprint arXiv:2209.11895, 2022.\n[69] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-\ncontext learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\n[70] Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop\nKim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, et al. On the effect\nof pretraining corpora on in-context learning by a large-scale language model. arXiv preprint\narXiv:2204.13509, 2022.\n[71] Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as\nalgorithms: Generalization and implicit model selection in in-context learning. arXiv preprint\narXiv:2301.07067, 2023.\n[72] Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. arXiv\npreprint arXiv:2303.07895, 2023.\n[73] Jacob Abernethy, Alekh Agarwal, Teodor V Marinov, and Manfred K Warmuth. A mech-\nanism for sample-efficient in-context learning for sparse retrieval tasks.\narXiv preprint\narXiv:2305.17040, 2023.\n[74] Shipra Agrawal and Navin Goyal. Near-optimal regret bounds for thompson sampling. Journal\nof the ACM (JACM), 64(5):1\u201324, 2017.\n[75] Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, volume 2000,\npages 943\u2013950, 2000.\n[76] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning:\nworst-case regret bounds. Advances in Neural Information Processing Systems, 30, 2017.\n[77] Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. Advances in neural information\nprocessing systems, 30, 2017.\n[78] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via\nbootstrapped dqn. Advances in neural information processing systems, 29, 2016.\n[79] Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep rein-\nforcement learning. Advances in Neural Information Processing Systems, 31, 2018.\n[80] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. Advances in neural information processing\nsystems, 32, 2019.\n[81] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n15\n[82] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah\nDormann. Stable-baselines3: Reliable reinforcement learning implementations. The Journal of\nMachine Learning Research, 22(1):12348\u201312355, 2021.\n[83] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,\nand Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-\nlearning. In International Conference on Learning Representation (ICLR), 2020.\n[84] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural\ncomplexity and representation learning of low rank mdps. Advances in neural information\nprocessing systems, 33:20095\u201320107, 2020.\n16\nAdditional Related Work\nIn-context learning. Beyond decision-making and reinforcement learning, our approach takes\ninspiration from general in-context learning, a phenomenon observed most prominently in large\nlanguage models in which large-scale autoregressive modelling can surprisingly lead to a model that\nexhibits meta-learning capabilities [1]. Recently, there has been great interest in understanding the\ncapabilities and properties of in-context learning [4, 67, 5, 68, 69, 70, 71, 72, 6, 73]. While a common\nhypothesis suggests that this phenomenon is due to properties of the data used to train large language\nmodels [3], our work suggests that this phenomenon can also be encouraged in general settings via\nadjustments to the pre-training objective. In fact, DPT could be interpreted as explicitly encouraging\nthe ability to perform Bayesian inference, which is a popular explanation for the mechanism behind\nin-context learning for large language models [64].\nPosterior Sampling. Posterior sampling originates from the seminal work of [56], and has been\npopularized and thoroughly investigated in recent years by a number of authors [55, 74, 75, 12, 76, 65].\nFor bandits, it is often referred to as Thompson Sampling, but the framework is easily generalizable\nto RL. The principle is as follows: begin with a prior over possible models (i.e. reward and transition\nfunctions), and maintain a posterior distribution over models by updating as new interactions are\nmade. At decision-time, sample a model from the posterior and execute its optimal policy. The\naforementioned prior works have developed strong theoretical guarantees on Bayesian and frequentist\nregret for posterior sampling. Despite its desirable theoretical characteristics, a major limitation is\nthat computing the posterior is often computationally intractable, leading practitioners to rely on\napproximation-based solutions [77, 78, 79]. In Section 6, we show that a version of the DPT model\nlearned from pretraining can be viewed as implementing posterior sampling as it should be without\nresorting to approximations or deriving complicated posterior updates. Instead, the posterior update\nis implicitly learned through pretraining to predict the optimal action. This suggests that in-context\nlearning (or meta-learning more generally) could be a key in unlocking practically applicable posterior\nsampling for RL.\nA\nImplementation and Experiment Details\nAlgorithm 2 Decision-Pretrained Transformer (detailed)\n1: // Collecting pretraining dataset\n2: Initialize empty dataset B\n3: for i in [N] do\n4:\nSample task \u03c4 \u223c Tpre\n5:\nSample interaction dataset D \u223c Dpre(\u00b7; \u03c4) of length n\n6:\nSample squery \u223c Dquery and a\u22c6 \u223c \u03c0\u22c6\n\u03c4(\u00b7|squery)\n7:\nAdd (squery, D, a\u22c6) to B\n8: end for\n9: // Training model on dataset\n10: Initialize model M\u03b8 with parameters \u03b8\n11: while not converged do\n12:\nSample (squery, D, a\u22c6) from B\n13:\nPredict \u02c6pj(\u00b7) = M\u03b8(\u00b7|squery, Dj) for all j \u2208 [n].\n14:\nCompute loss in (5) with respect to a\u22c6 and backpropagate to update \u03b8.\n15: end while\nA.1\nDPT Architecture: Formal Description\nIn this section, we provide a detailed description of the architecture alluded to in Section 3 and Figure 1.\nSee hyperparameter details for models in their respective sections. The model is implemented in\nPython with PyTorch [80]. The backbone of the transformer architecture we use is an autoregressive\nGPT-2 model from the HuggingFace transformers library.\nFor the sake of exposition, we suppose that S and A are subsets of RdS and RdA respectively. We\nhandle discrete state and action spaces with one-hot encoding. Consider a single training datapoint\nderived from an (potentially unknown) task \u03c4: we have a dataset D of interactions within \u03c4, a query\n17\nAlgorithm 3 Offline test-time deployment (detailed)\n1: // Task and offline dataset are generated without learner\u2019s control\n2: Sample unknown task \u03c4 \u223c Ttest\n3: Sample dataset D \u223c Dtest(\u00b7; \u03c4)\n4: // Deploying offline policy M\u03b8(\u00b7|\u00b7, D)\n5: s1 = reset(\u03c4)\n6: for h in [H] do\n7:\nah = argmaxa\u2208A M\u03b8(\u00b7|sh, D) // Most likely action\n8:\nsh+1, rh = step(\u03c4, ah)\n9: end for\nAlgorithm 4 Online test-time deployment (detailed)\n1: // Online, dataset is empty as learning is from scratch\n2: Initialize D = {}\n3: Sample unknown task \u03c4 \u223c Ttest\n4: for ep in max_eps do\n5:\ns1 = reset(\u03c4)\n6:\nfor h in [H] do\n7:\nah \u223c M\u03b8(\u00b7|sh, D) // Sample action from predicted distribution\n8:\nsh+1, rh = step(\u03c4, ah)\n9:\nend for\n10:\n// Experience from previous episode added to dataset\n11:\nAdd (s1, a1, r1, . . .) to D\n12: end for\nstate squery, and its corresponding optimal action a\u22c6 = \u03c0\u22c6\n\u03c4(squery). We construct the embeddings to be\npassed to the GPT-2 backbone in the following way. From the dataset D = {(sj, aj, s\u2032\nj, rj)}j\u2208[n], we\nconstruct vectors \u03bej = (sj, aj, s\u2032\nj, rj) by stacking the elements of the transition tuple into dimension\nd\u03be := 2dS + dA + 1 for each j in the sequence. This sequence of n elements is concatenated with\nanother vector v := (squery, 0) where the 0 vector is a vector of zeros of sufficient length to make the\nentire element dimension d\u03be. The (n + 1)-length sequence is given by X = (v, \u03be1, . . . , \u03ben). As order\ndoes not often matter for the dataset D6, we do not use positional encoding in order to take advantage\nof this invariance. We first apply a linear layer Linear(X) and pass the result to the transformer,\nwhich outputs the sequence Y = (\u02c6y0, \u02c6y1, . . . , \u02c6yn). In the continuous action case, these can be used\nas is for predictions of a\u22c6. For the discrete action case, we use them as logits to be converted to\neither a distribution over actions in A or one-hot vector predictions of a\u22c6. Here, we compute action\nprobabilities\n\u02c6pj = softmax(\u02c6yj) \u2208 \u2206(A)\n(3)\nBecause of the GPT-2 causal architecture (we defer details to the original papers [81, 1]), we note\nthat \u02c6pj depends only on squery and the partial dataset Dj = {(sk, ak, s\u2032\nk, rk)}k\u2208[j], which is why we\nwrite the model notation,\nM\u03b8(\u00b7|squery, Dj) = \u02c6pj(\u00b7),\n(4)\nto denote that the predicted probabilities of the jth element only depend on Dj and not the entire\nD for the model M with parameters \u03b8 \u2208 \u0398. For example, with j = 0, the prediction of a\u22c6 is made\nwithout any contextual information about the task \u03c4 except for squery, which can be interpreted as the\nprior over a\u22c6. We measure loss of this training example via the cross entropy for each j \u2208 [n]:\n\u2212\nX\nj\u2208[n]\nlog \u02c6pj(a\u22c6)\n(5)\nIntuition.\nElements of the inputs sequence X represent transitions in the environment. When\npassed through the GPT-2 transformer, the model learns to associate elements of the sequence via the\nstandard query-key-value mechanism of the attention model. The query state squery is demarcated\n6This is not always true such as when data comes from an algorithm such as PPO or Thompson Sampling.\n18\nby its zeros vector (which also acts as padding). Unlike other examples of transformers used for\ndecision-making such as the Decision Transformer [34] and Algorithm Distillation [7], DPT does\nnot separate the individual (s, a, s\u2032, r) into their own embeddings to be made into one long sequence.\nThis is because we view the transition tuples in the dataset as their own singletons, to be related with\nother singletons in the dataset through the attention mechanism. We note that there are various other\nimplementation variations one could take, but we found success and robustness with this one.\nA.2\nImplementation Details\nA.2.1\nBandit algorithms\nFirst, we describe the comparisons from the bandit experiments with hyperparameters.\nEmpirical Mean (Emp).\nEmp has no hyperparameters, but we give it some mechanism to avoid\ndegenerate scenarios. In the offline setting, Emp will only choose from actions that have at least one\nexample in the dataset. This gives Emp and LCB-style effect when actions are missing. Similarly,\nonline, Emp will sample each action at least once before defaulting to its real strategy. These changes\nonly improve Emp.\nUpper Confidence Bound (UCB).\nAccording to the Hoeffding bound, we choose actions as\n\u02c6a \u2208 argmaxa\u2208A\nn\n\u02c6\u00b5a +\np\n1/na\no\nwhere \u02c6\u00b5a is the empirical mean so far for action a and na is the\nnumber of times a has been chosen so far. To arrive at this constant for the bonus, we coarsely tried a\nset of plausible values given the noise and found this to perform the best.\nLower Confidence Bound (LCB).\nWe choose actions as \u02c6a \u2208 argmaxa\u2208A\nn\n\u02c6\u00b5a \u2212\np\n1/na\no\nwhere\n\u02c6\u00b5a is the empirical mean so far for action a and na is the number of times a has been chosen so far.\nThompson Sampling (TS).\nSince the means are sampled uniformly from [0, 1], Gaussian TS is\npartially misspecified; however, we set prior mean and variance to 1\n2 and\n1\n12 to match the true ones.\nThe noise model was well-specified with the correct variance. In the linear experiments of Figure 3a\nand Figure 3b, we set the prior mean and variance to 0 and 1 to fit the true ones better.\nLinUCB.\nWe choose \u02c6at \u2208 argmaxa\u2208A\u27e8\u02c6\u03b8t, \u03d5(a)\u27e9 + \u03b2\u2225\u03d5(a)\u2225\u02c6\u03a3\u22121\nt\nwhere \u03b2 = 1 and \u02c6\u03a3t = I +\nP\ns\u2208[t\u22121] \u03d5(as)\u03d5(as)\u22a4 and \u02c6\u03b8t = \u02c6\u03a3\u22121\nt\nP\ns\u2208[t\u22121] rs\u03d5(as). Here, rs and as are the reward and action\nobserved at time s.\nLinReg.\nLinReg (offline) is the same as LinUCB except we set \u03b2 = 0 to greedily choose actions.\nDPT.\nThe transformer for DPT has an embedding size of 32, context length of 500 for basic bandits\nand 200 for linear bandits, 4 hidden layers, and 4 attention heads per attention layer for all bandits.\nWe use the AdamW optimizer with weight decay 1e-4, learning rate 1e-4, and batch-size 64. For\nall experiments, we shuffle the in-context dataset D since order does not matter except in the linear\nbandit.\nA.2.2\nRL Algorithms\nBelow, we describe the comparisons from the MDP experiments and their hyperparameters.\nProximal Policy Optimization (PPO).\nThe reported results for PPO use the Stable Baselines3\nimplementation [82] with the default hyperparameters, which successfully learns each task given\n100K environment steps in Dark Room and 125K environment steps in Miniworld. In Dark Room,\nthe policy is implemented as a multi-layer perceptron with two hidden layers of 64 units each. In\nMiniworld, the policy is a convolutional neural network with two convolutional layers with 16 3 \u00d7 3\nkernels each, followed by a linear layer with output dimension of 8.\n19\nAlgorithm Distillation (AD).\nWe first collect learning histories with PPO for each of the training\ntasks. Then, given a cross-episodic context of length H, where H is the task horizon, the model\nis trained to predict the actions taken K episodes later (given the states visited in that episode).\nThis was shown to lead to faster algorithms in [7]. We evaluated AD across different values of K.\nBetween K = 10, 50, 100, we found K = 100 to be most performant in the Dark Room environment.\nIn Miniworld, we also subsampled with K = 100. In Dark Room, the transformer has similar\nhyperparameters as DPT: an embedding size of 32, context length of 100 steps, 4 hidden layers, and\n4 attention heads per attention layer. In Miniworld, as with DPT, we first encode the image with a\nconvolutional network with two convolutional layers with 16 3 \u00d7 3 kernels each, followed by a linear\nlayer with output dimension of 8.\nRL2.\nThe reported results for RL2 use an open-sourced implementation from [83]. The implemen-\ntation uses PPO as the RL algorithm and defines a single trial as four consecutive episodes. The\npolicy is implemented with one hidden layer of 32 units in Dark Room. In Miniworld, the policy\nis parameterized with a convolutional neural network with two convolutional layers with 16 3 \u00d7 3\nkernels each, followed by a linear layer with output dimension of 8.\nDPT.\nThe transformer for DPT has an embedding size of 32, context length of 100 steps, 4 hidden\nlayers, and 4 attention heads per attention layer in Dark Room. In Miniworld, the image is first\npassed through a convolutional network with two convolutional layers 16 3\u00d73 kernels each, followed\nby a linear layer with output dimension of 8. The transformer model that processes these image\nembeddings otherwise has the same hyperparameters as in Dark Room. We use the AdamW optimizer\nwith weight decay 1e-4, learning rate 1e-3, and batch-size 128.\nA.3\nBandit Pretraining and Testing\nBasic Bandit.\nOffline, to generate the in-context datasets for pretraining, we used a Dirichlet\ndistribution to sample action frequencies in order to generate datasets with diverse compositions\n(i.e. some more uniform, some that only choose a few actions, etc.): p1 \u223c Dir(1) where p1 \u2208 \u2206(A)\nand 1 \u2208 R|A|. We also mixed this with a distribution that has all mass on one action: \u02c6a \u223c Unif(A)\nand p2(\u02c6a) = 1 and p2(a) = 0 for all a \u0338= \u02c6a. The final action distribution is p = (1 \u2212 \u03c9)p1 + \u03c9p2\nwhere \u03c9 \u223c Unif(0.1[10]). We train on 100,000 pretraining samples for 300 epochs with an 80/20\ntrain/validation split. In Figure 2a, Dtest is generated in the same way.\nExpert-Biased Bandit.\nTo generate expert-biased datasets for pretraining, we compute the action\nfrequencies to bias the dataset towards the optimal action. Let a\u22c6 be the optimal one. As before,\nwe take p1 \u223c Dir(1). Then, p2(a\u22c6) = 1 and p2(a) = 0 for all a \u0338= a\u22c6. For of bias of \u03c9, we take\np = (1 \u2212 \u03c9)p1 + \u03c9p2 with \u03c9 \u223c Unif(0.1[10]). We use the same pretraining sample size and epochs\nas before. For testing, Dtest is generated the same way except we fix a particular \u03c9 \u2208 {0, 0.5, 1} to\ntest on.\nLinear Bandit.\nWe consider the case where |A| = 10 and d = 2. To generate environments from\nTpre, we first sampled a fixed set of actions from N(0, Id/d) in Rd to represent the features. Then,\nfor each \u03c4, we sampled \u03b8\u03c4 \u223c N(0, Id/d) to produce the means \u00b5a = \u27e8\u03b8\u03c4, \u03d5(a)\u27e9 for a \u2208 A. To\ngenerate the in-context dataset, we ran Gaussian TS (which does not leverage \u03d5) over n = 200\nsteps (see hyperparameters in previous section). Because order matters, we did not shuffle and used\n1, 000, 000 pretraining samples over 200 epochs with an 80/20 train/validation split. At test time, we\nset Ttest = Tpre and Dtest = Dpre. Note that \u03d5 is fixed over all \u03c4, as is standard for a linear bandit.\nA.4\nMDP Environment Details\nDark Room.\nThe agent must navigate a 10 \u00d7 10 grid to find the goal within H = 100 steps. The\nagent\u2019s observation is its xy-position, the allowed actions are left, right, up, down, and stay, and the\nreward is only r = 1 when the agent is at the goal, and r = 0 otherwise. At test time, the agent\nbegins at the (0, 0) position. We randomly designate 80 of the 100 grid squares to be goals for the\ntraining tasks, and hold out the remaining 20 for evaluation.\n20\n0.0\n0.3\n0.5\nNoise standard deviation\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nFinal Suboptimality\nOffline Robustness to Reward Shift\nEmp\nLCB\nTS\nDPT (ours)\n(a)\n0\n100\n200\n300\n400\n500\nData\n0.1\n0.3\n0.02\nSuboptimality\nOffline Bandit (Bernoulli)\nEmp\nLCB\nTS\nDPT (ours)\n(b)\n0\n100\n200\n300\n400\n500\nData\n0\n5\n10\n15\n20\n25\nCumulative Regret\nOnline Bandit (Bernoulli)\nEmp\nUCB\nTS\nDPT (ours)\n(c)\nFigure 6: (a) Final (after 500 steps) offline suboptimality on out-of-distribution bandits with different Gaussian\nnoise standard deviations. (b) Offline performance on out-of-distribution Bernoulli bandits, given random in-\ncontext datasets. (c) Online cumulative regret on Bernoulli bandits. The mean and standard error are computed\nover 200 test tasks.\nMiniworld.\nThe agent must navigate to the correct box, which is initially unknown, from 25 \u00d7 25\nRGB image observations. The agent is additionally conditioned on its own direction vector. In each\nepisode, the environment is initialized with four boxes of different colors, one in each corner of the\nsquare room. The agent can turn left, turn right, or move forward. The reward is only r = 1 when the\nagent is near the correct box and r = 0 otherwise, and each episode is 50 time-steps long. At test\ntime, the agent begins in the middle of the room.\nA.5\nMDP Pretraining Datasets\nDark Room.\nIn Dark Room, we collect 100K in-context datasets, each of length H = 100 steps,\nwith a uniform-random policy. The 100K datasets are evenly collected across the 100 goals. The\nquery states are uniformly sampled from the state space, and the optimal actions are computed as\nfollows: move up/down until the agent is on the same y-position as the goal, then move left/right\nuntil the agent is on the x-position as the goal. Of the 100K collections of datasets, query states,\nand optimal actions, we use the first 80K (corresponding to the first 80 goals) for training and the\nremaining 20K for validation.\nMiniworld.\nWhile this task is solved from image-based observations, we also note that there are\nonly four distinct tasks (one for each colored box), and the agent does not need to handle new tasks at\ntest time. Hence, the number of in-context datasets required in pretraining is fewer \u2013 we use 40K\ndatasets each of length H = 50 steps. So as to reduce computation, the in-context datasets only have\nonly (s, a, r) tuples. The query states, which consist of image and direction are sampled uniformly\nfrom the entire state space, i.e., the agent is place uniformly at random in the environment, pointing\nin a random direction. The optimal actions are computed as follows: turn towards the correct box if\nthe agent is not yet facing it (within \u00b115 degrees), otherwise move forward. Of the 40K collections\nof datasets, query states, and optimal actions, we use 32K for training and the remaining 8K for\nvalidation.\nB\nAdditional Experimental Results\nB.1\nBandits\nThis section reports additional experimental results in bandit environments.\nOut-of-distribution reward variances.\nIn Figures 2c and 6a, we demonstrate the robustness of\nthe basic pretrained model under shifts in the reward distribution at test time by varying the amount\nof noise observed in the rewards. DPT maintains robustness to these shifts similar to TS.\nBernoulli rewards.\nWe test the out-of-distribution ability of DPT further by completely changing\nthe reward distribution from Gaussian to Bernoulli bandits. Despite being trained only on Gaussian\ntasks during pretraining, DPT maintains strong performance both offline and online in Figures 6b\nand 6c.\n21\nRandom\nExpert\nDataset Composition\n0\n20\n40\n60\n80\nReturn\nOffline Dark Room (Train Tasks)\nDataset\nAD\nDPT (ours)\n(a)\nRandom\nExpert\nDataset Composition\n0\n20\n40\n60\n80\nReturn\nOffline Dark Room (Test Tasks)\nDataset\nAD\nDPT (ours)\n(b)\n0\n5\n10\n15\n20\n25\n30\n35\n40\nEpisodes\n0\n20\n40\n60\n80\nReturn\nOnline Dark Room (Train Tasks)\nDPT (ours)\nAD\nPPO\nRL2\n(c)\n0\n5\n10\n15\n20\n25\n30\n35\n40\nEpisodes\n0\n20\n40\n60\n80\nReturn\nOnline Dark Room (Test Tasks)\nDPT (ours)\nAD\nPPO\nRL2\n(d)\nFigure 8: All comparisons in Dark Room evaluated on the tasks that were seen during pretraining, displayed\nnext to their evaluations on test task counterparts from the main text.\n2 layers\n4 layers\n6 layers\n8 layers\nLayers\n0\n10\n20\n30\n40\n50\n60\nReturn\nLayers (Offline)\n(a)\n1 heads\n2 heads\n4 heads\n8 heads\nHeads\n0\n10\n20\n30\n40\n50\n60\nReturn\nAttention Heads (Offline)\n(b)\n16 dim\n32 dim\n64 dim\n128 dim\nDimensions\n0\n10\n20\n30\n40\n50\n60\nReturn\nEmbedding Dimension (Offline)\n(c)\n10k\n50k\n100k\n200k\nSamples\n0\n10\n20\n30\n40\n50\n60\nReturn\nPretraining Samples (Offline)\n(d)\nFigure 9: Sensitivity analysis of the offline Dark Rook task over the GPT-2 transformer\u2019s hyperparameters: (a)\nlayers (b) attention heads (c) embedding dimensions (d) pretraining samples.\nB.2\nMarkov Decision Processes\n0\n5\n10\n15\n20\n25\n30\n35\n40\nEpisodes\n0\n10\n20\n30\n40\n50\n60\n70\n80\nReturn\nOnline Dark Room Permuted\nFigure 7: Online evalua-\ntion of DPT on Dark Room\nwhen tested on novel ac-\ntions set permutations.\nThis section reports additional experimental results in the Dark Room and\nMiniworld environments.\nPerformance on training tasks.\nIn Fig. 8, we show the performance of\neach method on the training tasks in Dark Room. Offline, DPT and AD\ndemonstrate comparable performance as on the training tasks, indicating a\nminimal generalization gap to new goals. Online, DPT, AD, and RL2 also\nachieve performance on the training tasks similar to that on the test tasks.\nGeneralization to new dynamics.\nIn this experiment, we study general-\nization to variations in a different aspect of the MDP, namely the dynamics.\nWe design Dark Room (Permuted), a variant of Dark Room in which the\ngoal is fixed to a corner but the action space is randomly permuted. Hence,\nthe agent must leverage its historical context to infer the effect of each action. On a held-out set of\n20 permutations, DPT infers the optimal policy correctly every time offline, given only 100 offline\nsamples, matching the optimal policy at 83 return. Similarly, the online performance immediately\nsnaps to a near optimal policy in one episode once it identifies the novel permutation in Figure 7.\nB.3\nSensitivity Analysis\nWe next seek to understand the sensitivity of DPT to different hyperparameters, including the\nmodel size and size of the pretraining dataset. These experiments are performed in the Dark Room\nenvironment. As shown in Fig. 9, the performance of DPT is robust to the model size; it is the same\nacross different embedding sizes, number of layers, and number of attention heads. Notably, the\nperformance is slightly worse with 8 attention heads, which may be attributed to slight overfitting.\nWe do see that when the pretraining dataset is reduced to 10% of its original size (10000 samples)\nthe performance degrades, but otherwise has similar performance with larger pretraining datasets.\nC\nAdditional Theory and Omitted Proofs\nWe start with a well-known concentration inequality for the maximum-likelihood estimate (MLE) to\nprovide some more justification for the approximation made in Assumption 1. We state a version\n22\nfrom [84]. Let F be a finite function class used to model a conditional distribution pY |X(y|x)\nfor x \u2208 X and y \u2208 Y. Assume there is f \u22c6 \u2208 F such that p(y|x) = f \u22c6(y|x) (realizable), and\nf(\u00b7|x) \u2208 \u2206(Y) for all x \u2208 X and f \u2208 F (proper). Let D = {xi, yi}i\u2208[N] denote a dataset of i.i.d\nsamples where xi \u223c pX and yi \u223c pY |X(\u00b7|xi). Let\n\u02c6f = argmax\nf\u2208F\nX\ni\u2208[N]\nlog f(yi|xi)\n(6)\nProposition C.1 (Theorem 21 of [84]). Let D and \u02c6f be given as above under the aforementioned\nconditions. Then, with probability at least 1 \u2212 \u03b4,\nEx\u223cpX\u2225 \u02c6f(\u00b7|x) \u2212 pY |X(\u00b7|x)\u22252\n1 \u2264 8 log (|F|/\u03b4)\nN\n(7)\nThe finiteness of F is done for simplicity, but we can see that this yields dependence on the\nlog-cardinality, a common measure of complexity. Extensions to infinite F of bounded statis-\ntical complexity can be readily made to replace this. For our setting, the bound suggests that\nEPpre\u2225Ppre(\u00b7|squery, D, \u03beh) \u2212 M\u03b8(\u00b7|squery, D, \u03beh)\u22252\n1 \u2192 0 as N \u2192 \u221e with high probability, provided\nthe function class of M\u03b8 has bounded statistical complexity.\nC.1\nPosterior Sampling\nPosterior sampling is most generally described with the following procedure [12]. Initialize a prior\ndistribution T1 = Tpre and dataset D = {}. For k \u2208 [K]\n1. Sample \u03c4k \u223c Tk and compute \u02c6\u03c0\u03c4k\n2. Execute \u03c0\u22c6\n\u03c4k and add interactions to D\n3. Update posterior distribution Tk+1(\u03c4) = P(\u03c4|D).\nThe prior and posteriors are typically over models such as reward functions in bandits or transition\ndynamics in MDPs.\nC.2\nProof of Theorem 1\nTheorem 1 (DPT\n\u21d0\u21d2\nPS). Let the above assumptions hold.\nThen, Pps(\u03beH | D, \u03c4c) =\nPM\u03b8(\u03beH | D, \u03c4c) for all trajectories \u03beH.\nProof. Without loss of generality, for a task \u03c4, we take \u03c0\u22c6\n\u03c4(\u00b7|s) to be deterministic and denote the\noptimal action in state s as \u03c0\u22c6\n\u03c4(s). Recall that we consider a fixed current task \u03c4c and a fixed in-context\ndataset D. Define \u03beh = (s1, a1, . . . , sh, ah).\nWe now formally state the variant of the full joint distribution from which we sample during pretrain-\ning. Let \u03c4 and D\u2032 be an arbitrary task and dataset and let a\u22c6 \u2208 A, squery \u2208 S, \u03beH\u22121 \u2208 (S \u00d7 A)H\u22121,\nand h \u2208 [0, H \u2212 1] be arbitrary.\nPpre(\u03c4, a\u22c6, squery, D\u2032, \u03beH\u22121, h) = Tpre(\u03c4)Dpre(D\u2032; \u03c4)Dquery(squery)SH(s1:H)\u03c0\u22c6\n\u03c4(a\u22c6|squery)\n(8)\n\u00d7 Unif[0, H \u2212 1]\nY\ni\u2208[H]\n\u03c0\u22c6\n\u03c4(ai|si)\n(9)\nThe Unif[0, H \u2212 1] is due to the fact that we sample h \u223c Unif[0, H \u2212 1] and then truncate \u03beh\nfrom \u03beH\u22121 (or, equivalently, sample \u03beh \u223c Sh directly), marginalizing out the other variables. For\nh\u2032 \u2264 h \u2212 1, recall that we also use the notation Sh\u2032(s1:h\u2032) to denote the marginalization of the full\njoint SH. We will eventually work with the posterior of this distribution given the data D and history\n\u03beh:\nPpre(\u03c4|D, \u03beh) \u221d Tpre(\u03c4)Dpre(D; \u03c4)\nY\ni\u2208[h]\n\u03c0\u22c6\n\u03c4(ai|si)\n(10)\n\u221d Ppre(\u03c4|D)\nY\ni\u2208[h]\n\u03c0\u22c6\n\u03c4(ai|si)\n(11)\n23\nWe define the following random sequences and subsequences:\n\u039eps(h; D) = (Sps\n1 , Aps\n1 , . . . , Sps\nh , Aps\nh )\n(12)\nwhere the variables are generated according to the following conditional process: \u03c4ps \u223c P(\u00b7|D),\nSps\n1\n\u223c \u03c1\u03c4c, Aps\nh \u223c \u03c0\u22c6\n\u03c4ps(\u00b7|Sps\nh ), and Sps\nh+1 \u223c T\u03c4c(\u00b7|Sps\nh , Aps\nh ). We also define \u039eps(h\u2032 : h; D) to be\nthe last h \u2212 h\u2032 elements of \u039eps(h; D). Analogously, we define\n\u039epre(h; D) = (Spre\n1\n, Apre\n1\n, . . . , Spre\nh\n, Apre\nh )\n(13)\nwhere the variables are from the process: Spre\n1\n\u223c \u03c1\u03c4c, Apre\nh\n\u223c Ppre(\u00b7|Spre\nh\n, D, \u039epre(h \u2212 1; D)), and\nSpre\nh+1 \u223c T\u03c4c(\u00b7|Spre\nh\n, Apre\nh ). Note that Apre\nh\nis sampled conditioned on the sequence \u039epre(h; D) so\nfar.\nWe will show that \u039eps(h; D) and \u039epre(h; D) follow the same distribution for all h \u2208 [H]. For\nconvenience, we will drop notational dependence on D, except where it resolves ambiguity. Also,\nbecause of Assumption 1, we have that Ppre(\u00b7|Spre\nh\n, D, \u039epre(h\u22121)) = M\u03b8(\u00b7|Spre\nh\n, D, \u039epre(h\u22121)),\nso we will just work with Ppre for the remainder of the proof. We will also make use of the following\nlemma.\nLemma C.2. If Dpre is complaint, then Ppre(\u03c4|D) = P(\u03c4ps = \u03c4|D).\nProof. From the definition of posterior sampling (using the same prior, Tpre), we have that\nP(\u03c4ps = \u03c4|D) \u221d P(D|\u03c4)Tpre(\u03c4)\n(14)\n\u221d Tpre(\u03c4)\nY\nj\u2208[n]\nT\u03c4(s\u2032\nj|sj, aj)R\u03c4(rj|sj, aj)\n(15)\n\u221d Tpre(\u03c4)\nY\nj\u2208[n]\nT\u03c4(s\u2032\nj|sj, aj)R\u03c4(rj|sj, aj)Dpre(aj|sj, Dj\u22121)\n(16)\n= Tpre(\u03c4)Dpre(D; \u03c4)\n(17)\n= Ppre(\u03c4|D)\n(18)\nwhere the second line crucially uses the fact that posterior sampling chooses actions based only on\nthe prior and history so far. Similarly, the third line uses the fact that Dpre is compliant. Since the two\nsides are proportional in \u03c4, they are equivalent.\nWe will prove Theorem 1 via induction for each h \u2208 [H]. First, consider the base case for a sequence\nof length h = 1. Recall that \u03c1\u03c4c denotes the initial state distribution of \u03c4c. We have that the densities\ncan be written as\nP(\u039eps(1) = \u03be1) = P(Sps\n1 = s1, Aps\n1 = a1)\n(19)\n= \u03c1\u03c4c(s1)P(Aps\n1 = a1|Sps\n1 = s1)\n(20)\n= \u03c1\u03c4c(s1)\nZ\n\u03c4\nP(Aps\n1 = a1, \u03c4ps = \u03c4|Sps\n1 = s1)d\u03c4\n(21)\n= \u03c1\u03c4c(s1)\nZ\n\u03c4\n\u03c0\u22c6\n\u03c4(a1|s1)Pps(\u03c4ps = \u03c4|D, Sps\n1 = s1)d\u03c4\n(22)\n= \u03c1\u03c4c(s1)\nZ\n\u03c4\n\u03c0\u22c6\n\u03c4(a1|s1)Pps(\u03c4ps = \u03c4|D)d\u03c4\n(23)\n= \u03c1\u03c4c(s1)Ppre(Apre\n1\n= a1|s1, D)\n(24)\n= P(\u039epre(1) = \u03be1)\n(25)\nwhere the second line uses the sampling process of Spre\n1\n; the third marginalizes over \u03c4ps, which is\nthe task that posterior sampling samples to find the optimal policy; the fourth decomposes this into\nthe optimal policy and the posterior over \u03c4ps given D and Sps\n1 . Since Sps\n1 is independent of sampling\nof \u03c4ps this dependence goes away in the next line. The sixth line applies Lemma C.2 and then, for\nh = 1, there is no history to condition on.\n24\nNow, we leverage the inductive hypothesis to prove the full statement. Suppose that the hypothesis\nholds for h \u2212 1. Then,\nP(\u039eps(h) = \u03beh) = P(\u039eps(h \u2212 1) = \u03beh\u22121)P(Sps\nh = sh, Aps\nh = ah|\u039eps(h \u2212 1) = \u03beh\u22121)\n(26)\n(27)\nBy the hypothesis, we have that P(\u039eps(h \u2212 1) = \u03beh\u22121) = P(\u039epre(h \u2212 1) = \u03beh\u22121). For the second\nfactor,\nP(Sps\nh = sh, Aps\nh = ah|\u039eps(h \u2212 1) = \u03beh\u22121)\n(28)\n= T\u03c4c(sh|sh\u22121, ah\u22121) \u00b7 P(Aps\nh = ah|Sps\nh = sh, \u039eps(h \u2212 1) = \u03beh\u22121)\n(29)\n= T\u03c4c(sh|sh\u22121, ah\u22121) \u00b7\nZ\n\u03c4\nP(Aps\nh = ah, \u03c4ps = \u03c4|Sps\nh = sh, \u039eps(h \u2212 1) = \u03beh\u22121)d\u03c4\n(30)\nAs before, we can further rewrite the last factor as\nP(Aps\nh = ah, \u03c4ps = \u03c4|Sps\nh = sh, \u039eps(h \u2212 1) = \u03beh\u22121)\n(31)\n= \u03c0\u22c6\n\u03c4(ah|sh) \u00b7 P(\u03c4ps = \u03c4|Sps\nh = sh, \u039eps(h \u2212 1) = \u03beh\u22121)\n(32)\nwhere\nP(\u03c4ps = \u03c4|Sps\nh = sh, \u039eps(h \u2212 1) = \u03beh\u22121) = P(Sps\nh = sh, \u039eps(h \u2212 1) = \u03beh\u22121|\u03c4ps = \u03c4)P(\u03c4ps = \u03c4|D)\nP(Sps\nh = sh, \u039eps(h \u2212 1) = \u03beh\u22121)\n(33)\n\u221d Ppre(\u03c4|D)\nY\ni\u2208[h\u22121]\nT\u03c4c(si+1|si, ai)\u03c0\u22c6\n\u03c4(ai|si)\n(34)\n\u221d Ppre(\u03c4|D)\nY\ni\u2208[h\u22121]\n\u03c0\u22c6\n\u03c4(ai|si)\n(35)\n\u221d Ppre(\u03c4|D)Dquery(sh)Sh\u22121(s1:h\u22121)\nY\ni\u2208[h\u22121]\n\u03c0\u22c6\n\u03c4(ai|si)\n(36)\n\u221d Ppre(\u03c4|sh, D, \u03beh\u22121)\n(37)\n(38)\nwhere \u221d denotes that the two sides are equal up to multiplicative factors independent of \u03c4. In the first\nline, we used Bayes rule. In the second line, given that \u03c4ps = \u03c4 (i.e. posterior sampling selected \u03c4 to\ndeploy), we decompose the probability of observing that sequence of states of actions. We also used\nLemma C.2. The denominator does not depend on \u03c4. Similarly, for the third and fourth lines, T\u03c4c and\nS do not depend on \u03c4. The final line follows from the definition of the joint pretraining distribution\nin this regime.\nTherefore, we conclude that the posterior over the value of \u03c4ps is the same as the posterior over the\ntask in the pretraining distribution, given sh, D, \u03beh\u22121. Substituting back through all the previous\nequations, we have\nP(\u039eps(h) = \u03beh)\n(39)\n= P(\u039epre(h \u2212 1) = \u03beh\u22121) \u00b7 T\u03c4c(sh|sh\u22121, ah\u22121)\nZ\n\u03c4\n\u03c0\u22c6\n\u03c4(ah|sh)Ppre(\u03c4|sh, D, \u03beh\u22121)d\u03c4\n(40)\n= P(\u039epre(h \u2212 1) = \u03beh\u22121) \u00b7 T\u03c4c(sh|sh\u22121, ah\u22121)Ppre(ah|sh, D, \u03beh\u22121)\n(41)\n= P(\u039epre(h) = \u03beh)\n(42)\nThis concludes the proof.\nC.3\nProof of Corollary 6.2\nCorollary C.3 (Finite MDPs). Suppose that sup\u03c4 Ttest(\u03c4)/Tpre(\u03c4) \u2264 C for some C > 0. For the\nabove MDP setting, the pretrained model M\u03b8 satisfies ETtest [Reg\u03c4(M\u03b8)] \u2264 e\nO(CH3/2S\n\u221a\nAK).\n25\nProof. Note that Dpre is clearly compliant since it is generated by random sampling. We use the\nequivalence between M\u03b8 and posterior sampling established in Theorem 1. The proof then follows\nimmediately from Theorem 1 of [12] to guarantee that\nETpre [Reg\u03c4(M\u03b8)] \u2264 e\nO\n\u0010\nH3/2S\n\u221a\nAK\n\u0011\n(43)\nwhere the notation e\nO omits polylogarithmic dependence. The bound on the test task distribution\nfollows from the assumed bound on the likelihood ratio under the priors:\nZ\nTtest(\u03c4)Reg\u03c4(M\u03b8)d\u03c4 \u2264 C\nZ\nTpre(\u03c4)Reg\u03c4(M\u03b8)d\u03c4.\n(44)\nC.4\nProof of Corollary 6.3\nCorollary C.4 (Latent representation learning in linear bandits). For Ttest = Tpre in the above linear\nbandit setting, M\u03b8 satisfies ETtest [Reg\u03c4(M\u03b8)] \u2264 e\nO(d\n\u221a\nK).\nProof. The distribution Dpre satisfies compliance by definition because it is generated by an adaptive\nalgorithm TS. The proof once again follows by immediately deferring to the established result of [65]\n(Proposition 3) for linear bandits by the posterior sampling equivalence of Theorem 1. This ensures\nthat posterior sampling achieves regret e\nO(d\n\u221a\nK). It remains, however, to justify that Ppre(\u00b7|Dk) will\nbe covered by Gaussian Thompson Sampling for all Dk with k \u2208 [K]. This is verified by noting that\nPps(a|Dk) > 0 for non-degenerate Gaussian Thompson Sampling (positive variances of the prior\nand likelihood functions) and finite K. This guarantees that any Dk will have support.\nC.5\nProof of Proposition 6.4\nProposition C.5. Let P 1\npre and P 2\npre be pretraining distributions that differ only by their in-context\ndataset distributions, denoted by D1\npre and D2\npre. If D1\npre and D2\npre are compliant with the same support,\nthen P 1\npre(a\u22c6|squery, D, \u03beh) = P 2\npre(a\u22c6|squery, D, \u03beh) for all a\u22c6, squery, D, \u03beh.\nProof. The proof follows by direct inspection of the pretraining distributions. For P 1\npre, we have\nP 1\npre(a\u22c6|squery, D, \u03be) =\nZ\n\u03c4\n\u03c0\u22c6\n\u03c4(a\u22c6|squery)P 1\npre(\u03c4|squery, D, \u03be)d\u03c4\n(45)\nThe posterior distribution over tasks is simply\nP 1\npre(\u03c4|squery, D, \u03be) = P 1\npre(\u03c4, squery, D, \u03be)\nP 1pre(squery, D, \u03be)\n(46)\n\u221d P 1\npre(\u03c4)P 1\npre(\u03be|\u03c4)Dquery(squery)D1\npre(D; \u03c4)\n(47)\n= P 2\npre(\u03c4)P 2\npre(\u03be|\u03c4)Dquery(squery)D1\npre(D; \u03c4)\n(48)\nThen, the distribution over the in-context dataset can be decomposed as\nD1\npre(D; \u03c4) =\nY\ni\u2208[n]\nR\u03c4(ri|si, ai)T\u03c4(s\u2032\ni|si, ai)D1\npre(ai|si, Di\u22121; \u03c4)\n(49)\n=\nY\ni\u2208[n]\nR\u03c4(ri|si, ai)T\u03c4(s\u2032\ni|si, ai)D1\npre(ai|si, Di\u22121)\n(50)\n\u221d\nY\ni\u2208[n]\nR\u03c4(ri|si, ai)T\u03c4(s\u2032\ni|si, ai)D2\npre(ai|si, Di\u22121)\n(51)\n= D2\npre(D; \u03c4),\n(52)\nwhere the second equality holds because D1\npre(aj|sj, Dj; \u03c4) is assumed to be invariant to \u03c4 by\ncompliance, and the fifth equality holds because D2\npre(aj|sj, Dj; \u03c4) is assumed to be invariant to \u03c4.\n26\nTherefore, we conclude that, for any s, D, \u03be,\nP 1\npre(\u03c4|s, D, \u03be) \u221d P 2\npre(\u03c4)P 2\npre(\u03be|\u03c4)Dquery(s)D2\npre(D; \u03c4)\n(53)\n\u221d P 2\npre(\u03c4|s, D, \u03be).\n(54)\nSince also\nR\n\u03c4 P 1\npre(\u03c4|s, D, \u03be) = 1 =\nR\n\u03c4 P 2\npre(\u03c4|s, D, \u03be), then\nP 1\npre(\u03c4|s, D, \u03be) = P 2\npre(\u03c4|s, D, \u03be).\n(55)\nSubstituting this back into Equation45 yields P 1\npre(a\u22c6|s, D, \u03be) = P 1\npre(a\u22c6|s, D, \u03be).\n27\n"
  },
  {
    "title": "Thinking Like an Annotator: Generation of Dataset Labeling Instructions",
    "link": "https://arxiv.org/pdf/2306.14035.pdf",
    "upvote": "7",
    "text": "Thinking Like an Annotator: Generation of Dataset Labeling Instructions\nNadine Chang\nCarnegie Mellon University\nnadinec@cmu.edu\nFrancesco Ferroni\nNvidia\nfrancescoferroni1@gmail.com\nMichael J. Tarr\nCarnegie Mellon University\nmichaeltarr@cmu.edu\nMartial Hebert\nCarnegie Mellon University\nmhebert@andrew.cmu.edu\nDeva Ramanan\nCarnegie Mellon University\ndeva@andrew.cmu.edu\nDataset Curators\nDataset Labeling Process\n1) Curators create labeling instructions\nUnreleased \ninstructions\nBicycle\nConstruction Worker\n2) Annotators label images\nLabeling Instructions\nGeneration (LIG)\nPDC (ours) \nGenerated Instructions\n1) Given released public dataset\n2) Create a framework to generate a            \npotential instruction set\nReleased Dataset\nReleased Dataset\nPropose \nFramework\nBicycle\nConstruction \nWorker\nBicycles\nConstruction \nWorker\nbicycle\nbicycle\nbicycle\nbicycle\nconstruction worker\nconstruction worker\nconstruction worker\nconstruction worker\nFigure 1: Left: The manual dataset labeling process that creates instructions for annotators is tediously long. Center:\nwe propose LIG to address the lack of public labeling instructions for most datasets. LIG\u2019s main objective is to generate\ninstructions given a released dataset. Right: Proxy Dataset Curator (PDC) addresses LIG. We show generated instruction\npairs sets here. Note that each image is accompanied by a text phrase to compose an instruction pair.\nAbstract\nLarge-scale datasets are essential to modern day deep\nlearning. Advocates argue that understanding these meth-\nods requires dataset transparency (e.g. \u201cdataset curation,\nmotivation, composition, collection process, etc...\u201d) [11].\nHowever, almost no one has suggested the release of the\ndetailed definitions and visual category examples provided\nto annotators \u2013 information critical to understanding the\nstructure of the annotations present in each dataset. These\nlabels are at the heart of public datasets, yet few datasets\ninclude the instructions that were used to generate them.\nWe introduce a new task, Labeling Instruction Generation,\nto address missing publicly available labeling instructions.\nIn Labeling Instruction Generation, we take a reasonably\nannotated dataset and: 1) generate a set of examples that\nare visually representative of each category in the dataset;\n2) provide a text label that corresponds to each of the ex-\namples. We introduce a framework that requires no model\ntraining to solve this task and includes a newly created\nrapid retrieval system that leverages a large, pre-trained vi-\nsion and language model. This framework acts as a proxy\nto human annotators that can help to both generate a final\nlabeling instruction set and evaluate its quality. Our frame-\nwork generates multiple diverse visual and text representa-\ntions of dataset categories. The optimized instruction set\noutperforms our strongest baseline across 5 folds by 7.06\nmAP for NuImages and 12.9 mAP for COCO.\n1\narXiv:2306.14035v1  [cs.CV]  24 Jun 2023\n1. Introduction\nLarge-scale datasets are the foundations for almost all\nmodern computer vision tasks. As a field, we often evaluate\nthese datasets based on their released data and annotation\nquality. Underlying annotation quality is an intensive cu-\nration process that is reflected in the labeling instructions\n(LIs) required to annotate that data. LIs are typically the\nresult of numerous painstaking discussions aimed at clar-\nifying desired class memberships and at aligning annota-\ntions between data curators and annotators. Despite incur-\nring significant time and financial costs, end-state LIs are\nrarely available. We attempted, but failed, to obtain the LIs\nfor benchmark datasets such as COCO [27], ADE20K [41],\nOpenImages [23]. This lack of availability illustrates how\nour community has overlooked the clear and comprehensive\nreporting of dataset annotation. Even the influential work\nDatasheets for Datasets [11], which advocates for dataset\ntransparency, falls short of arguing for the release of LIs.\nThe inaccessibility of instruction policies forms a major gap\nin efforts towards full transparency and reproduciblity.\nWhy are instructions important and what are their\napplications? Beyond providing rich details about classes\nand the boundaries between classes, accessible LIs helps the\nresearch community in multiple ways:\nReproduciblity. LIs are critical for understanding foun-\ndational concepts such as model generalization and overfit-\nting. Recent efforts in analyzing overfitting on ImageNet [9]\nrecreated a held-out test set by reusing the original annota-\ntion instructions [32]. Such continual validation set gen-\neration is impossible without LIs. Indeed, in a real-world\napplication related to autonomous vehicles, a version of\nour framework was used for \u201ccontinual dataset construc-\ntion\u201d. That is, although LIs were available, generated LIs\nwere used to refine annotation policy during the natural\nannotator-curator conversation that occurred as more data\n(and edge cases) were collected and labeled.\nClarifications. Analyses of errors in public datasets [20]\nreveal that many \u2018errors\u2019 are due to LIs. For example, in\none dataset, annotators were instructed to not label vehicles\nthat appeared outside of the drivable regions in images [19].\nMedical Biases.\nIn medical imaging, framing biases\narise from how instructions are presented to specialist la-\nbelers [14]. Recovering LIs is central to revealing and un-\nderstanding these biases in medical datasets where LIs are\nprotected, proprietary or private, yet their application has\nserious real-world consequences.\nHuman Studies. Data collection (protocol) transparency\nis longstanding in the behavioral sciences because behav-\nior and decisions vary with demand characteristics (instruc-\ntions) and replication is impossible without protocol infor-\nmation. Publication in these fields requires rigorous doc-\numentation of data collection protocols including instruc-\ntions [36]. Building on recent calls for transparency [11],\nsimilar standards are warranted for computer vision.\nPolicy Initiatives.\nThe media, public, and lawmakers\n(e.g., EU privacy laws such as GDPR) are all increasingly\nconcerned with data bias and transparency in AI. Yet there\nhas been little discussion of how LIs may inject bias or\nhow users gain a transparent view of labels when provided\nwith LIs.\nIdentifying the issue and introducing a novel\ntask+method are initial steps towards a healthy discussion\non this critical facet of the larger policy debate.\nWe address the lack of publicly available LIs by propos-\ning a new task, Labeling Instruction Generation (LIG).\nWe first identify the typical composition of the annota-\ntion instructions for a given dataset, for example, the set\nfrom NuImages [5] as shown in Fig. 1. Note that LIs are\noften multi-modal sets of text descriptions plus visual ex-\namples. Text descriptions provide both a label and detailed\ndefinitions of classes and attributes, synonyms, and descrip-\ntions of corner cases. Visual examples are typically com-\nposed of prototypical representations and rarer class sub-\ntype representations.\nFor instance, a prototypical image\nfor motorbike is a two-wheeled bike and a rarer sub-type\nis an image of a motorbike with three wheels. Adding to\nthe effectiveness of the instruction set, visual examples are\nfrequently shown from various viewpoints and scales. The\ncombination of both text descriptions and image examples\nprovides a compelling, informative, and generally applica-\nble dataset labeling instruction set. Reflecting this degree\nof exposition, we focus on generating LIs that are similarly\ncomposed of both text descriptions and visual examples.\nOur proposed task, LIG, starts with a given annotated\ndataset, which at minimum consists of categorical labels\nand the associated images/bounding boxes that contain the\nobjects referred by these labels. Our objective is to gener-\nate a set of category labeling instructions that can be shown\nto new annotators and effectively demonstrates the desired\ntypes of image classes to be labeled in new images. To gen-\nerate informative instructions, the final instruction set must\nhave, for each category both a set of visual examples that\nare representative of that category and a set of text labels\nthat corresponds to each generated visual example. The fi-\nnal result is a set of text, image pair(s) for each category\n(Fig. 1).\nTo solve LIG, dataset curators and annotators engage\nin a painstaking cycle of instruction policy refinement (in\nthis way dataset curators are effectively dataset annotators\ntoo). Curators provide an initial instruction set to annota-\ntors, who begin to label the dataset. Annotators inevitably\nrun into confusing cases, where they unsure as to how to\nlabel a given object. Ambiguous cases are brought back to\nthe curators, who incorporate these new cases into an up-\ndated instruction set. This back and forth between curators\nand labelers continues and results in a detailed instruction\npolicy that may be as expensive to obtain and as valuable\n2\nas the annotations themselves given that policy. In that such\ninstruction policies are rarely made available, our goal is to\ngenerate these instruction sets to increase the transparency\nand utility of public datasets.\nBeyond surfacing instruction policies for datasets in\nwhich LIs are not available, given the importance of in-\nstruction policies, it is desirable to develop an alternative,\nnon-manual solution to LIG. We propose a computationally\nefficient method that is a proxy for dataset curation. Large-\nscale vision and language models (VLMs), such as CLIP,\nALIGN, and Florence [31, 16, 40], provide text and image\nrepresentations that yield robust results for a variety of tasks\nin the open-world. We leverage VLMs to build a frame-\nwork that rapidly traverses a dataset and retrieves the best\ntext, image pair(s) that are representative of a given class.\nOur framework, which requires no back propagation and\nonly inference level modifications, consists of three com-\nponents: 1) An image representations database constructed\nfrom a dataset\u2019s images converted into representations via a\npre-trained VLM; 2) An image retrieval system that rapidly\nqueries through this database; 3) Multi-modal prompts that\ncan be used with a pre-trained VLM. Condensing text and\nimage representations into a single query via multi-modal\nfusion, we show that multi-modal queries are essentially\nfree, without additional compute.\nOur algorithmic framework, named Proxy Dataset Cu-\nrator or PDC, is intuitively demonstrated in Fig. 4. PDC\nis a greedy algorithm that, on a high-level, searches for the\nbest object images and best text description. Images and\ntexts are paired up and used as queries for image retrieval\non the entire training dataset. The set of pairs that achieves\nthe best retrieval performance is chosen as the final instruc-\ntion set. Importantly, the pairs are surfaced from a training\nset and final mAP evaluation on held-out test set is reported.\nWe perform 5 fold training and testing for evaluation sta-\nbility. PDC is described in Sec. 3.5 and fully detailed in\npseudocode in Sec. A of the Appendix.\nHow do we evaluate labeling instructions? The gold-\nstandard evaluation of LIs is in a human annotation set-\nting. The ideal design would include generated LIs for a\nhuman annotation task and inspection of the quality of the\nresulting annotations as collected on a full dataset. Since\nthis evaluation at scale is prohibitively expensive, impracti-\ncal, and time-consuming, we instead perform two scalable\nevaluations. First, we perform a human experiment with\nforced choices between pairs of candidate instruction sets:\nthe original instructions from NuImages versus our gener-\nated LIs. Participants choose which of the two sets is pre-\nferred for future annotation tasks. Second, we evaluate our\ngenerated instructions for both COCO and NuImages on a\nheld-out annotated test set by building a multi-modal re-\ntrieval engine that returns images that are likely to contain\nthe object class of interest. We then report mAP on these re-\ntrieved images. These two evaluations reveal that our PDC\nmethod is an effective means for LIG.\nFor retrieved images from NuImages, PDC generated\nLIs outperform our strongest baseline by 7.06 mAP and\nthe original NuImages instructions by 12.9 mAP. Similarly,\nfor retrieved images from COCO, PDC generated LIs out-\nperform our strongest baseline by 12.9 mAP (as noted, we\ntried and failed to obtain the original COCO instructions\nfor comparison). Second, for NuImages, across all classes,\nhuman evaluators preferred our generated instructions over\nthe original instructions 44% of the time. Importantly, this\nindicates that our generated instructions are visually almost\nas good as the original instructions. Thus, while this behav-\nioral experiment demonstrates that participants are nearly\nas likely to prefer our generated LIs as the originals, our\nquantitative evaluation demonstrates that our PDC gener-\nated LIs provide additional benefits. In particular, our gen-\nerated LIs outperform both baseline and original LIs (due to\nbetter consideration of corner and other boundary cases).\nLimitations. While these results are promising, we ac-\nknowledge several limitations to our present work. First,\nour framework focuses on generating text and image pairs\nbecause such pairs are the most commonly used in real-\nworld LIs. Thus, although richer multi-modal instructions\nare a potential future direction, we view text+images as\nthe best first step in addressing this new problem. Second,\ngenerated text instructions may sometimes be less nuanced\nand/or detailed as compared to human generated text de-\nscribing visual classes. We expect that rapid advances in\nLLMs and VLMs will enable more expressive generated\ntext instructions in the near future. Third, while our frame-\nwork does generate corner cases, our current implementa-\ntion does not include negative examples. This is in large\npart because negatives are presently difficult to represent in\nboth LLMs and VLMs. We expect that progress in this area\nwill enable us to consider negatives in future versions.\nIn sum, we view our contributions as follows.\nFirst,\nwe highlight LI inaccessibility as an overlooked problem\nin publicly available datasets that directly impacts trans-\nparency, fairness, and reproducibility. Second, we propose a\nnew task, LIG, to address this problem by generating multi-\nmodal instructions (visual examples plus text) for existing\ndatasets that lack LIs due to legal or privacy concerns or\nsimple refusal to publish. Third, we propose PDC, a new\nframework for solving LIG, that acts as a proxy for curators\nand annotators in the creation of LIs. We experimentally\nshow that our framework, which requires only model infer-\nence time changes, is fast, scalable, and efficient because: 1)\nwe can create a pre-computed database index; 2) we do not\nrequire model training; 3) we utilize a fast cosine similarity\noperation. We establish the effectiveness of PDC through\nboth computational and human experiments.\n3\n2. Related Works\nDataset Instructions and Transparency Most influen-\ntial datasets do not include the annotation instructions that\nthe dataset curators provided to the annotators.\nCOCO,\na key detection dataset, crowd sourced annotations using\nAmazon Mechanical Turkers [27].\nAs such, the COCO\ncurators had to provide instructions.\nHowever, as men-\ntioned, we were unable to obtain the unreleased instructions\nfor COCO. ImageNet challenge [33], a key classification\ndataset, collected candidate images by querying the internet\nand uses \u201cboth automatic and manual strategies\u201d to clean\nup search results. None of the manual steps are publicly\nreleased. Even today, datasets such as OpenImages [24],\nneed annotators to verify labels.\nAgain, no instructions\npassed on to annotators are publicly available. In contrast,\nmany datasets completely avoid generating annotation in-\nstructions. TinyImages uses synsets from WordNet [3] and\nscraped the web to collect 80 million images. However, re-\nsults are not manually verified, and recently the dataset was\npublicly removed due to heretofore unflagged inappropri-\nate content [4]. The lack of transparency with such high-\nprofile datasets has motivated advocates to compile a list\nof objectives that would facilitate transparency [11]. This\nlist focuses on expanding details in dataset composition and\nthe collection process that should be answered by dataset\ncurators; however, it omits the process of creating labeling\ninstructions or subsequent public release. The instructions\nprovided to dataset annotators are valuable and expensive to\ncompile and yet completely overlooked and unreleased.\nImproving Labeling Instructions for Better Annota-\ntions One approach to improving annotation quality is to\nlearn whether to prompt an annotator for category labels or\nobject attribute labels [22]. Similarly, one can learn what\ntypes of annotations (e.g., bounding boxes vs. tight or loose\nsegmentations) are sufficient for learning a category [15].\nAn alternative approach is to examine an annotator\u2019s vi-\nsual interpretations of object attributes described through\ntext [21]. Finally, LVIS completely bypasses creating label-\ning instructions and instead ask annotators to \u2018point\u2019 to an\nobject [2] and label that object\u2019s self-defined category [13].\nThe large body of work that aims to analyze and improve\nthe annotation process illustrates the significance of high-\nquality annotation instructions and pipelines. Despite this\nimportance, rarely do datasets publicly release their annota-\ntion code, instructions, or pipeline.\nLarge-Scale Vision and Language Models Recent\nlarge-scale Vision and Language models (VLMs) trained\non extremely large amounts of data align images and text\ninto a common representation space [31, 16, 26, 40]. This\nalignment allows for simple similarity search between two\nmodalities (text, image).\nThus, VLMs demonstrate re-\nmarkable general domain learning and zero-shot capabil-\nities. Several works have built on top of VLMs, applied\nthem successfully on traditional tasks, and seen clear im-\nprovements [34, 42]. One work leverages a VLM to learn\npersonalized concepts from users (e.g. \u2018my favorite skirt\u2019)\nand requires a user to input a specialized instruction set [7].\nNote that our task is vastly different, as we aim to generate\ncategorical instruction sets automatically. From these suc-\ncesses, we observe that these models are increasingly robust\nand difficult to outperform. Interestingly, works show that\nfinetuning on VLMs tend to hurt its generalization [37, 25].\nMulti-modal Training Multi-modal training with both\nimages and language extends VLMs.\nCaptioning tasks,\nwhere an image is provided and a caption is asked for,\njointly train with both text and language [1, 12, 30]. One\nmulti-task model trains on several vision and language\ndatasets for VQA and captioning tasks [29]. Scene graphs,\nwhich are structured graphs depicting relationships between\nattributes and objects, also includes multi-modal text and\nimage training [18, 39, 38]. Scene graphs have been used\nfor image retrieval as well [18], but require training and cre-\nation of a complex graph structure. Finally, a new retrieval\ntask relies on an input image and a modification text that\nexplains how to change the input image [35, 28]. However,\nthese models require extensive training and image to text\npairs. Moreover, these models do not attempt to align both\nimage and text representations into a single space.\n3. Method\nWe split our method section into three parts: 1) The cre-\nation of our database index that allows us to do rapid image\nretrieval, 2) the various query policies and functions that can\nbe performed on our database index, 3) our proposed algo-\nrithmic framework that utilizes the index and selected query\npolicies to generate our final labeling instruction pairs.\n3.1. Creating an Index\nFirst, we acknowledge that building a queryable database\nindex with VLM embeddings is a rather simple and intuitive\nsetup, but have no knowledge of any works that have built\nsuch an index. Thus, we detail our index building below.\nWe build a database index that contains the visual embed-\ndings corresponding to the annotated dataset we would like\nto generate labeling instructions for. We create both testing\nand training indexes, which will not be altered once cre-\nated. To extract all embeddings, we utilize a pre-trained\nimage encoder from a VLM. These embeddings are trained\nusing cross-modal contrastive losses, such that cosine sim-\nilarities result in meaningful cross-modal similarity. Our\nindex building pipeline can be seen in Fig. 2.\nGrid and Extract. One can create an index of visual\nembeddings by extracting them from whole images. How-\never, to focus on objects of varying scale (e.g. small and\nlarge objects), we additionally grid each image into patches.\nFor further diversity, we use odd numbered grid sizes where\ng = 1, 3, 5, 7, 9 so that patches do not overlap significantly.\n4\nEach image is cropped into g2 number of equal patches for\na final M total patches per image i, where M = P g2.\nPatches are passed into a visual encoder to obtain visual\nembeddings. Each image i contains M = 165 embeddings.\nBuild Index.\nExtracting all embeddings yields N \u00d7\n(M + 1) embeddings, which increases our database by 165\ntimes its original size. Loading all embeddings into run-\ntime memory is infeasible and unscalable. We address this\nissue by using the FAISS library [17]. We create a FAISS\nindex which enables fast searches through an on-disk mem-\nory stored index. For example, a single search takes approx-\nimately 50ms on NuImages.\nImage \nencoder\nx\nx\nx\nIndex\nBuild\nExtract\nGrid\nFigure 2: An overview of building our database index.\n3.2. Query Policies\nHere, we describe the various query policies that can be\nused to query our database index. When we query the index,\nwe get a list of relevant images back from the index that is\nranked from most similar to least similar to the query.\nSingle Modal.\nFirst, consider queries from a single\nmodality m (text or visual). Let qm \u2208 RD be the corre-\nsponding VLM embedding. We compute its similarity with\na visual embedding x. The simplest method is to compute\nthe cosine similarity between a query from modality m (e.g.\ntext or visual) and an example x. We define the method as\nSingleScore. An example is seen in Fig. 3.\nSingleScore(qm, x) = cos(qm, x) = \u02c6qT\nm\u02c6x,\n(1)\nwhere\n\u02c6q =\nq\np\nqT q\n, \u02c6x =\nx\n\u221a\nxT x\n.\n(2)\nMulti-Modal. Second, consider the case with queries\nfrom two modalities, text and visual. Let qt \u2208 RD and\nqv \u2208 RD be the corresponding text and visual embeddings\nfrom a multi-modal model. We explore all possible ways to\nperform multi-modal query (Fig. 3).\nEarly-fusion:\nSum.\nWe consider an early fusion\nmethod [6], where two modalities are fused. In practice,\nthis means we compute a single embedding. We show that\nthis is also equivalent to late fusion, where we combine re-\nsults of two modalities\u2019 searches. However, we present our\nmethod as an early fusion method because it is computation-\nally cheaper - computing one set of score instead of two.\nSumFusion(qt, qv, x) = cos(q, x),\nq = \u02c6qv + \u02c6qt\n(3)\n= \u02c6qT\nv \u02c6x + \u02c6qT\nt \u02c6x\n(4)\n= cos(qv, x) + cos(qt, x).\n(5)\nEarly-fusion: Weighted. SumFusion gives equal weight\nto both text and visual queries.\nHowever, consider the\ncase where we weigh one modality differently by using a\nweighted average, where weight w = cos(qt, qv).\nThis\nweighs each text/visual query by how close one is to the\nother. When w = 1, this indicates qt and qv are the same\nand combining text and visual queries is not necessary.\nThus our weighted query assigns q = qt. When w = 0,\nthis indicates qt and qv are perpendicular. That is, they are\nnot the same nor are they the opposite, but the text and vi-\nsual embeddings are in between. Thus, our weighted query\ndefaults to averaging, which is equivalent to SumFusion.\nWF(qt, qv, x) = (1 \u2212 w)cos(qv, x) + (1 + w)cos(qt, x)\n= cos(q, x)\n(6)\nwhere\nq = (1 \u2212 w)\u02c6qv + (1 + w)\u02c6qt\nSingle Modal \n\u201ca photo \nof truck\u201d\nText \nEncoder\nx\nx\nx\nIndex\nqt\ncosine sim\nImage \nEncoder\nx\nx\nx\nIndex\nqv\nMulti-Modal \n\u201ca photo \nof truck\u201d\nText \nEncoder\nqt\nImage \nEncoder\nqv\nq\nx\nx\nx\nIndex\n+\n+ Sum/Weighted-fusion\nFigure 3: Left: A single modal query with text or visual\nqueries. Right: A multi-modal query that combines text +\nvisual queries into a single query.\nLate-fusion: Inverse Rank. Empirically, we find that\ncosine similarities computed by textual queries are consis-\ntently higher than those from visual queries. This suggests\nthat naive combinations such as max or averaging scores\ngive too much weight to text queries. As an alternative, we\nexplore another late-fusion method [6], where results, not\nqueries, across modalities are combined. Our proposed late-\nfusion method adopts an approach similar to mean recipro-\ncal rank (MRR) metrics used to summarize retrieval across\nmultiple queries [8]. Intuitively, such metrics measure the\n(harmonic) average rank of correctly retrieved examples.\nWe rank example x according to different queries and com-\npute the (inverse) harmonic mean of individual query ranks:\nRankFusion(qt, qv, x) =\n1\nRank(qv, x) +\n1\nRank(qt, x),\nwhere Rank(q, x) \u2208 {1, 2, . . .} is the position of example x\nin the ranked list of retrieved examples given query q.\nLate-fusion: Naive. A less complex version of RankFu-\nsion is to iteratively take one result from the ranked returns\nof text and visual queries until we have obtained the total\nnumber of desired returns.\nWe note that late-fusion methods scale linearly with the\nnumber of queries; fusing 10 queries takes 10x more com-\npute. In contrast, early-fusion methods scale by a constant\nfactor by creating a single multi-modal query embedding q.\n5\n3.3. Combining Multiple Query Results\nSuppose we have two queries from the same modality\n(e.g. two qt or qv) or two queries resulting from the same\nmulti-modal fusion technique (e.g. two q = fusion(qv, qt).\nWe fuse the unique results of the two queries, qm0 and qm1,\nby simply combining results and reranking. In cases where\nthere are duplicate retrieved items from the two queries re-\nturns, we address it with the two potential methods below.\nMax-fusion.\nWe max the resulting similarity scores\nacross duplicate items.\nMaxFusion(qm0, qm1, x)\n=\nmax(cos(qm0, x), cos(qm1, x)).\nAvg-fusion.\nWe average the resulting similarity\nscores across duplicate items. AvgFusion(qm0, qm1, x) =\navg(cos(qm0, x), cos(qm1, x))\n3.4. Combining Patch Scores\nBecause we wish to find small objects, our index is com-\nposed of both whole image and patch embeddings.\nTo\ngenerate one retrieval score per image, we perform a max\nacross similarities for all patches Xi from image i.\nPatchFusion(x, i) = si\nm := max\nx\u2208Xi cos(qm, x)\n3.5. PDC Algorithm\nIntuitively, PDC searches through an annotated training\ndataset and greedily grows a set of (text, image) pairs so as\nto maximize multimodal retrieval performance on the train-\ning set. We present a detailed psuedocode in Sec. A of the\nAppendix, but provide an overview in Fig. 4.\nFigure 4: After pre-processing, PDC greedily grows the in-\nstruction set for a class until no new pairs improve retrieval\nperformance (or a max limit on the instruction size is met).\nOptimized Set\nCandidate \nTexts\nCandidate \nPatches\nX\nGreedy Search\nPre-processing\n1) Insert candidate pair q into \u0398 to make \u0398q. \nUse \u0398q as query for image retrieval  \n2) Calculate AUC for PR curve\n3) Grow \u0398 with best pair q*\n4) Repeat (1) - (3) \nCandidate \nPairs\n:\nTruck\nLorry\nPickup\nTruck\nLorry\nPickup\nTruck\nTruck\nTruck\nTruck\nTruck\nLorry\n\u0398\nPre-processing. Assume there exists a set of descrip-\ntive words L associated with a class - e.g., animal has\nthe labels/subclasses/synonyms animal, dog, bird,\ncat. Convert each word into a text embedding with our\nVLM\u2019s text encoder Qt := {ft(w) : w \u2208 L}, where ft(\u00b7) is\nthe text encoder. Similarly, assume a dataset of images has\nbeen converted into a database of image patch embeddings\nwith our VLM\u2019s vision encoder D := {fv(p) : p \u2208 Pi, i \u2208\nI} where Pi is the set of patches extracted from image i,\nand I is the set of training images. We assume our training\ndataset is annotated with class bounding boxes, which al-\nlows us to define a subset of patches that have been labelled\nas positive examples; we denote this as Qv \u2286 D. We can\nnow define the set of potential multimodal query pairs as Q,\nthe cross product of Qt and Qv.\nSetup. Let \u03b8 \u2286 Q be a set of multimodal (text,image)\npairs that represents a candidate labeling instruction (LI) for\na particular class. We wish to find a \u03b8 that is \u201cgood\u201d, where\ngoodness is measured simply by the image retrieval accu-\nracy when using \u03b8 as a (fused) multimodal query set. In the-\nory, one could perform an exponential search over all pos-\nsible subsets of Q, evaluate the retrieval accuracy of each\ncandidate LI, and report the subset with the best accuracy.\nBecause this is far too slow, we perform a greedy variant of\nthis exhaustive search.\nGreedy Search.\nInitialize \u03b8, the set of selected\n(text,image) pairs, to the empty set. Then consider all candi-\ndate (text,image) pairs q \u2208 Q to add. For each, add it to the\ncurrent selected set to create \u03b8q and evaluate its goodness by\nusing \u03b8q to perform image retrieval over the set of annotated\npatches D (where the score for image i is given by the max-\nimum score across all patches p \u2208 Pi from that image). Se-\nlect the candidate pair q\u2217 that best increases image-retrieval\naccuracy (as measured by AUC of a precision-recall curve),\ngrow \u03b8 := \u03b8q\u2217, and repeat.\nEvaluation.\nWe evaluate PDC generated instruction\npairs \u03b8 by using them as queries for an image retrieval\ntask on a held-out test dataset. Intuitively, we reason that\nhumans who see good categorical instructions should be\nable to retrieve images that contain the relevant categories,\nwhich is similar to a computational image retrieval task.\nSince we test on unbalanced, large datasets as seen in mod-\nern image retrieval settings, we only return the top 1000\nunique retrievals. Thus, we also use modern retrieval met-\nrics: precision, recall, and average precision(AP) at k. Note\nthat precision at lower k values is more important, as the\nmost immediate visual images retrieved are the first ac-\ncessed. Lastly, following Pascal VOC [10], we report per\nclass AP and PR curves to accurately portray the results\nacross classes with large discrepancies in samples.\n4. Experiments\nIn our experiments, we aim to answer two crucial ques-\ntions: 1) Are the instruction pairs generated by PDC vi-\nsually accurate, meaningful, and interesting? 2) Can we\nquantitatively show that our PDC instruction pairs are bet-\nter than instruction pairs generated by methodological base-\nlines? Additionally, we explore the query policies and func-\ntions that are and could have been part of PDC. Finally, we\ndiagnose our generated instruction pairs in order to under-\n6\nstand which aspects attribute to their success.\n4.1. NuImages Performance Evaluation\nImplementation.\nOur PDC experiments used NuIm-\nages, a 2D version of NuScenes [5], as our dataset, in\nthat it is one of the few datasets with released labeling\ninstructions. Importantly this allows us to visually com-\npare our generated instruction pairs with NuImages origi-\nnal pairs. NuImages contains 83,724 total images and 25\nclasses. However, we only evaluated 23 foreground classes\nas two classes are background classes (ego flat and\ndrivable surface). As detailed in Sec. 3.1, we build\nour NuImages database index with approximately 14 mil-\nlion visual embeddings. When we query our database in-\ndex, we set our FAISS index probe hyperparameter, indi-\ncating the number of clusters visited, to a high 300 to allow\nfor high accuracy search.\nWe select CLIP [31] as our VLM as it is currently one of\nthe best and largest VLM publicly available. Specifically,\nwe use the ViT-B/32 pre-trained CLIP model. Following\nCLIP, when using category labels as text queries, we con-\nvert them into the following: \u2018a photo of \u27e8label\u27e9\u232a\u2019. PDC\nruns mainly on only CPU, except when we need to extract\nlanguage/vision embeddings on GPU. Lastly, we run our\nPDC until a maximum of four pairs of instruction pairs are\nfound. This is solely to speed up run time, as we find that\nAP improvement is marginal beyond four pairs. However,\nwe emphasize that if qualitative results are favored, running\nPDC longer can surface more interesting results.\nEvaluation. To show statistical importance, we split our\ndataset into 5 folds, run PDC on 5 different splits, and test\non 5 non-overlapping sets. Since some categories contain\nmany samples, it is impractical to query > 50% of the\ndataset to achieve a recall of 1. Thus, when testing with\n(text, image) instruction pair(s), we retrieve 1,000 unique\nimages for each category. Our PR curves, APs, mAPs are\nall calculated from exactly 1,000 returns. We average pre-\ncision and recall at each k up to 1,000 across folds.\n4.2. NuImages Performance Results\nBaselines Comparisons. We first establish several base-\nlines for comparison.\nImportantly, as with PDC, all of\nour baselines do not require model training. Our first two\nbaselines are formed by taking aspects from the original\nNuImages instructions: 1) Original Texts uses all given\nclass labels/sub-class names/synonyms as single-modal text\nqueries, which is CLIP\u2019s standard query. We extract these\ntexts from the original text descriptions (e.g., \u2018animal\u2019,\n\u2018dog\u2019, \u2018cat\u2019, \u2018rat\u2019 for class animal) and combine the re-\nsults of these text queries through Max-fusion, detailed\nin Sec. 3.3.\n2) Original Pairs takes all image examples\nfrom the original instructions and pairs each image with a\ntext from Original Texts set (e.g., (\u2018dog\u2019, \u27e8photo of dog\u27e9)).\nImportantly, the images from Original Pairs are manually\nTable 1: Comparisons of instruction pairs generated by dif-\nferent methods. Average APs@1000 across 5 folds is dis-\nplayed per class. Classes are sorted based on the number of\nimages containing them. Note, most low performing classes\nhave fewer examples or are ambiguous. Our method, PDC,\nperforms the best for 21 of 23 classes.\nCategory\n# Exs\nOrg. Ts\nOrg. Ps\nRnd. Bbs. Rnd. Ps\nMS\nPDC Pairs\ncar\n56517 8.5\u00b10.0\n5.7\u00b10.0\n6.2\u00b10.4\n4.6\u00b10.4\n8.1\u00b10.2\n8.8\u00b10.1\nadult ped.\n40241 7.3\u00b10.2\n2.6\u00b10.0\n2.9\u00b10.1\n3.0\u00b10.2\n4.8\u00b10.4\n12.0\u00b10.2\ntruck\n23499 12.5\u00b10.3\n7.3\u00b10.1\n2.7\u00b10.2\n1.8\u00b10.1\n3.1\u00b10.3\n16.5\u00b10.7\ntraffic cone\n22194 20.7\u00b10.1\n6.0\u00b10.1\n15.0\u00b11.6\n6.2\u00b11.2 16.3\u00b11.3\n22.2\u00b10.3\ntraffic barrier\n13607 3.4\u00b10.1\n0.5\u00b10.0\n1.8\u00b10.3\n0.8\u00b10.1\n0.8\u00b10.1\n16.9\u00b12.3\nmotorcycle\n12523 31.3\u00b10.7\n0.6\u00b10.0\n7.2\u00b11.4\n2.3\u00b10.6\n0.9\u00b10.1\n33.6\u00b11.4\nbicycle\n11883 24.6\u00b11.1 17.2\u00b10.2\n7.6\u00b11.7\n1.3\u00b10.1\n5.9\u00b10.7\n29.1\u00b12.8\nrigid bus\n7042\n14.4\u00b11.1\n1.8\u00b10.0\n1.9\u00b10.3\n0.8\u00b10.1\n0.7\u00b10.1\n24.8\u00b11.1\nconstruct. wrkr\n5586\n26.2\u00b12.1\n0.4\u00b10.0\n3.9\u00b10.6\n0.5\u00b10.1\n6.8\u00b11.0\n31.4\u00b14.4\nconstruct. veh.\n5258\n24.4\u00b10.9\n1.4\u00b10.1\n3.5\u00b10.8\n1.8\u00b10.5\n2.6\u00b10.9\n39.1\u00b11.0\nbicycle rack\n2771\n9.4\u00b11.4\n1.3\u00b10.1\n3.7\u00b10.7\n0.4\u00b10.1\n1.0\u00b11.0\n21.3\u00b110.8\npush-pull obj.\n2585\n1.2\u00b10.2\n0.3\u00b10.0\n0.4\u00b10.0\n0.3\u00b10.0\n0.2\u00b10.1\n6.2\u00b12.1\ntrailer\n2286\n1.9\u00b10.2\n1.1\u00b10.1\n3.8\u00b11.1\n2.9\u00b11.0\n1.3\u00b10.3\n17.3\u00b14.6\ndebris\n1840\n0.2\u00b10.0\n0.3\u00b10.0\n0.8\u00b10.3\n0.5\u00b10.2\n0.1\u00b10.0\n8.9\u00b11.6\nchild ped.\n1060\n1.2\u00b10.2\n0.1\u00b10.0\n0.5\u00b10.1\n0.4\u00b10.1\n0.2\u00b10.2\n1.5\u00b10.4\npers. mobi. veh. 790\n2.7\u00b10.3\n0.8\u00b10.1\n0.4\u00b10.1\n0.1\u00b10.0\n0.2\u00b10.3\n4.5\u00b11.3\npolice officer\n356\n4.5\u00b10.4\n1.1\u00b10.3\n0.7\u00b10.2\n0.1\u00b10.0\n0.1\u00b10.1\n5.5\u00b15.2\nstroller\n334\n3.8\u00b10.9\n17.5\u00b10.8\n0.8\u00b10.0\n0.1\u00b10.0\n0.4\u00b10.8\n16.5\u00b13.9\nanimal\n202\n0.0\u00b10.0\n0.1\u00b10.0\n0.0\u00b10.0\n0.0\u00b10.0\n0.0\u00b10.0\n0.8\u00b11.3\nbendy bus\n169\n0.4\u00b10.0\n0.0\u00b10.0\n0.1\u00b10.0\n0.0\u00b10.0\n0.1\u00b10.1\n6.9\u00b14.4\npolice vehicle\n132\n0.8\u00b10.1\n3.3\u00b10.8\n0.8\u00b10.2\n0.2\u00b10.1\n0.2\u00b10.2\n16.3\u00b111.0\nambulance\n40\n0.5\u00b10.1\n0.1\u00b10.0\n0.0\u00b10.0\n0.0\u00b10.0\n0.1\u00b10.1\n0.1\u00b10.2\nwheelchair\n33\n2.0\u00b10.3\n1.2\u00b10.2\n1.1\u00b10.5\n0.0\u00b10.0\n0.2\u00b10.3\n14.7\u00b19.2\nmAP\n-\n8.77\n3.08\n2.86\n1.22\n2.35\n15.44\nselected by dataset curators. Thus, we consider Original\nPairs as a strong matching baseline.\nWe also include two baselines that rely on randomly se-\nlected examples: 1) Random BBoxes randomly selects the\nsame number of bounding box examples per class as present\nin our PDC final set. Random BBoxes always selects the\nlargest bounding box of the class in an image with multiple\ninstances. 2) Random Pairs uses the bboxes from Random\nBboxes and randomly pairs each bbox with text from Orig-\ninal Texts (e.g., (\u2018dog\u2019, \u27e8photo of deer\u27e9)).\nFinally, we include a mean shift (MS) baseline. Our MS\nbaseline utilizes the training images to fit the model, in par-\nticularly using bboxes. Similar to Random BBoxes, our MS\nbaseline selects the largest bbox per class in an image. The\nclosest training images to each final MS cluster centers are\nselected as the final visual examples. The examples with\nthe class text are used as the final queries during evaluation\non held-out set. MS parameters are default sklearn values.\nNuImages Results. We first examine how PDC quan-\ntitatively compares with our baselines, as seen in Tab. 1.\nPDC shows a significant improvement of 7.06 mAP over\nour strongest baseline, Original Texts.\nFurthermore,\nPDC outperforms all baselines in 21 of 23 classes.\nIn\nclasses construction vehicle, rigid bus, and\ndebris, PDC outperforms the strongest contender by a\nlarge margin of 14.7, 10.4, 8.1AP respectively. We note\nthat PDC improves on classes that are ambiguously defined,\nsuch as pushable pullable object and debris.\n7\nIn general, we see that classes with more instances in the\ndataset achieve higher AP, the exception being car. PDC\nperforms extremely well on car with high precision and\nrecall in the top 1000 retrievals. However, because there are\nso many ground truth car instances, R@1000 is naturally\nlow, leading to low AP.\nFigure 5: Average PR@1000 across 5 folds are shown for a\nsubset of classes. Because each method retrieves 1000 sam-\nples, and classes are unbalanced, methods cannot reach the\nsame or 1.0 recall. Full results in Sec. B of the Appendix.\nTable 2: COCO subset results: Comparisons of instruction\npairs generated by different methods. Average APs across\n5 folds is displayed per class. mAP is calculated across all\n80 classes. PDC performs the best for 75 of 80 classes.\nCategory\n# Exs Org. Ts Rnd. Bbs.\nRnd. Ps\nPDC Pairs\nbottle\n8880 20.2\u00b11.5\n0.3\u00b10.3\n0.5\u00b10.4\n31.0\u00b12.1\nsports ball 4431 43.9\u00b12.4\n2.2\u00b14.7\n4.5\u00b19.5\n46.0\u00b13.6\nskis\n3202 64.4\u00b13.5\n4.4\u00b15.8\n18.0\u00b116.2\n80.2\u00b11.8\nsheep\n1594 78.3\u00b10.7 40.6\u00b125.2 59.3\u00b120.4\n85.4\u00b11.4\nmAP\n-\n42.0\n13.4\n21.1\n54.9\nNuImages Further Investigation. We closely examine\nsome of the most interesting classes\u2019 PR curves in Fig. 5.\nLooking at stroller, one of two classes where Origi-\nnal Texts outperforms PDC, we see that PDC still shows\ncompetitive results. While PDC achieves somewhat lower\nprecision, it shows higher recall. For motorcycle, we\nobserve that while Original Texts has both high precision\nand recall, PDC still outperforms it. In ambiguous classes\nsuch as pushable pullable object, PDC manages\nto dramatically improve the precision. However, we note\nthat classes such as child pedestrian are challenging\nbecause adults are often mistaken for children.\n4.3. COCO Performance Evaluation\nWe also use PDC to generate LIs for COCO, a more class\ndiverse detection benchmark. We compare COCO to the\nsame baselines as NuImages: 1) Original Texts; 2) Random\nBBoxes; 3) Random Pairs. However, as noted, we attempted\nbut failed to obtain the original instructions from COCO cu-\nrators. Thus, Original Pairs is not a viable baseline. For\nOriginal Texts, COCO contains fine-grain categories and,\nconsequently, does not provide synonyms/subtypes. Thus,\nclass names were used. PDC (54.9 mAP) outperforms its\nbest competitor by a significant 12.9 points as shown in\nTab. 2. APs, PR curves, and qualitative results for all COCO\nclasses are reported in Sec. C of the Appendix.\n4.4. Corner Cases and Prototypes\nWe show the PDC generated instruction pairs in Fig. 6.\nAll objects show diversity in viewpoints, sizes, and impor-\ntantly sub-type.\nFor NuImages, motorcycle includes\na corner case three-wheeled motorcycle and bicycle\nrack includes a two tier rack. Note that some of these ob-\njects are partially occluded. For pushable pullable\nobject, PDC\u2019s instruction set conveys that most objects\nin the dataset are garbage bins variants. Although there is\ntext to image mismatch, the errors are explainable (\u2018wheel\nbarrow\u2019 for a garbage bin with prominently shown wheels;\n\u2018dolly\u2019 for a dolly-like garbage bin). For COCO, sheep in-\ncludes a sheared sheep (corner), a sheep drawing (corner),\nand a real sheep (prototype). Sandwich includes various\ntriangle sandwiches and subs (corner).\nFigure 6: PDC\u2019s generated instruction pairs. We show ob-\njects from diverse viewpoint, size, and type. NuImages: top\n3 rows. COCO: bottom 2 rows. Complete results in Secs. B\nand C of the Appendix.\ngarbage bin\ndolley\ngarbage bin\nwheel barrow\nmotorcycle\nmotorcycle\nmotorcycle\nmotorcycle\nbicycle rack\nbicycle rack\nbicycle rack\nbicycle rack\nbicycle rack\nmotorcycle\npush-pull obj.\nsheep\nsandwich\n8\n4.5. NuImages Human Behavioral Experiment\nA human behavioral experiment evaluated how our gen-\nerated NuImages instructions visually compare to the origi-\nnal NuImages instructions. The experiment consisted of 23\ntrials (one per category as in 4.1). We show a sample trial\nin Fig. 7 for class construction vehicle. Each trial\nconsists of the category name and category description pro-\nvided by NuImages original instructions. Within each trial,\nthere are two candidate image instruction sets: 1) images\ngenerated from PDC; 2) images from NuImages original in-\nstructions. All images contain a correct object encased with\na bounding box. Participants select one of the two candidate\ninstruction sets that they believe best guides them for future\ncategorical annotation tasks (i.e., set A or B).\nFigure\n7:\nA\ntrial\nexample\nfor\nconstruction\nvehicle. Correctly labeled objects are bounded by green\nbounding boxes. Participants are asked to pick set A or B.\n10) Construction Vehicle\n\u2022\nVehicles primarily designed for construction. Typically very slow moving or stationary.\n\u2022\nTrucks used to hauling rocks or building materials are considered as truck rather than construction vehicles.\n\u2022\nCranes and extremities of construction vehicles are only included in annotations if they interferes with traffic.\nSet B\nSet A\nFigure 8: Participant responses for 23 NuImages categories\nare shown (N = 9). We observe that preferences for either\noriginal or PDC instructions are consistent across partici-\npants. Preferences are usually decided by a super majority\nof participants.\nCategory\nNumber of Subjects\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14 15 16 17 18 19 20 21 22 23\nPrefer PDC Instructions\nPrefer Original Instructions\nBehavioral Study Responses\nComplete behavioral study results are shown in Fig. 8.\nAcross all trials, the 9 participants preferred PDC gener-\nated instructions over the original instructions 44% of the\ntime. Because no direct inferences can be made from a sta-\ntistically null effect (equal preference), we also examined\nwhether participants were in agreement with one another for\na given pair of instruction sets or were randomly choosing\nbetween generated and original instructions. Importantly\nparticipant preferences were not random, but were found to\nbe consistent across participants with 78.8% agreement for\npreferred PDC generated LIs and 87.8% agreement for pre-\nferred original instructions. These results establish that, on\nthe whole, generated and human selected instructions can\nboth serve as visually effective annotation guides for poten-\ntial future annotation tasks.\nTable 3: Comparisons of instruction pairs results fused by\nvarious query policies. Average APs@1000 across 5 folds\nis shown per class. Our final PDC setup gets the best mAP\nand outperforms the second best setup for 16 of 23 classes.\nComplete results in Sec. B of the Appendix.\nCategory\n# Exs\nSum:Avg\nEarly:Wt\nLate:Naive\nLate:Rank\nPDC Pairs\ntruck\n23499\n15.4\u00b11.3\n16.5\u00b11.2\n15.7\u00b10.7\n16.1\u00b10.6\n16.5\u00b10.7\nconstruct. wrkr\n5586\n27.6\u00b14.3\n34.2\u00b12.9\n29.4\u00b13.2\n31.4\u00b13.5\n31.4\u00b14.4\nconstruct. veh.\n5258\n36.5\u00b12.4\n32.6\u00b16.1\n30.7\u00b12.5\n33.1\u00b12.3\n39.1\u00b11.0\nbicycle rack\n2771\n17.7\u00b110.6\n22.0\u00b18.3\n16.7\u00b16.7\n17.6\u00b16.9\n21.3\u00b110.8\nambulance\n40\n0.1\u00b10.2\n0.1\u00b10.1\n0.4\u00b10.4\n0.4\u00b10.4\n0.1\u00b10.2\nmAP\n-\n13.89\n14.75\n12.97\n13.69\n15.44\nQuery Fusion Policy Ablations We explore various\nways to query fusion policies as described in\nSecs. 3.2\nand 3.3. In PDC, we use Early-Fusion: Sum and Max-\nFusion as the query policy and method to combine results\nacross all generated instruction sets respectively.\nGiven\nthe same PDC generated instruction sets, our ablation re-\nsults on different methods to query with and combine re-\nsults are shown in Tab. 3. Results illustrate that our final\nsetup outperforms the next best, Early-Fusion: Weighted,\nby 0.69 mAP. We observe that all Early-Fusion methods\u2013\nSum:Avg, Weighted, Sum:Max(used by PDC)\u2013outperforms\nall Late-Fusion methods (Naive, Inverse Rank). By combin-\ning queries to create a single query, Early-Fusion methods\nare computationally faster and cheaper than Late-Fusion\nmethods.\nTable 4: Results from using only the texts or bboxes of our\nPDC instruction set. Average APs@1000 across 5 folds is\ndisplayed per class. Using both text and bboxes provides the\nbest APs for 17 of 23 classes. Complete results in Sec. B\nof the Appendix.\nCategory\n# Exs\nTexts\nBboxes\nPDC Pairs\nbicycle\n11883 30.4\u00b10.1\n23.5\u00b11.3\n29.1\u00b12.8\nconstruction vehicle\n5258\n20.1\u00b10.6\n38.6\u00b10.5\n39.1\u00b11.0\nbendy bus\n169\n0.9\u00b10.1\n8.6\u00b10.9\n6.9\u00b14.4\npolice vehicle\n132\n4.7\u00b11.1\n18.9\u00b11.9\n16.3\u00b111.0\nmAP\n-\n10.50\n13.56\n15.44\n9\nGenerated Instruction Pairs Diagnostics Finally, we\nexamine which aspect (text or bbox) of PDC generated in-\nstruction pairs contributes the most to our final results. As\nobserved in Tab. 4, PDC text and bbox pairs outperforms the\nnext best, bbox only, by 1.88 mAP. In general, using only\nbboxes is better than using only texts. We show our largest\nimprovement (+18.95 AP in construction vehicle)\nover only texts and worst decrements (-2.67 AP in police\nvehicle) over only bboxes.\n5. Conclusion\nDetailed and clear annotation policies are integral to\nlarge scale dataset creation which, in turn, forms the back-\nbone for much of modern deep learning. Yet few datasets\ninclude annotation instructions. This omission presents a\nchallenge for dataset transparency, reproducibility, and er-\nror interpretation. To address this gap, we propose a new\ntask, Labeling Instruction Generation (LIG), and a fast and\ncomputationally efficient post-hoc solution - Proxy Dataset\nCurator (PDC) - to LIG that serves as a substitute or en-\nhancement for dataset curators. PDC can efficiently, and\nwithout model training, replicate the laborious manual it-\nerative process of instruction policy refinement and outper-\nforms our strongest baselines by a significant margin. Fu-\nture work should continue to explore solutions to LIG that\nmay provide better refined and well-specified annotation in-\nstructions.\n6. Acknowledgements\nI would like to sincerely thank Jayanth Koushik for var-\nious discussions, comments, advice, and suggestions for\nthis project.\nThis work was supported by National Sci-\nence Foundation Graduate Research Program Fellowship\n(DGE1745016) and CMU Argo AI Center for Autonomous\nVehicle Research.\nReferences\n[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2425\u2013\n2433, 2015. 4\n[2] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li\nFei-Fei. What\u2019s the point: Semantic segmentation with point\nsupervision.\nIn European conference on computer vision,\npages 549\u2013565. Springer, 2016. 4\n[3] Richard Beckwith, Christiane Fellbaum, Derek Gross, and\nGeorge A Miller.\nWordnet: A lexical database organized\non psycholinguistic principles. In Lexical acquisition: Ex-\nploiting on-line resources to build a lexicon, pages 211\u2013232.\nPsychology Press, 2021. 4\n[4] Abeba Birhane and Vinay Uday Prabhu.\nLarge image\ndatasets: A pyrrhic win for computer vision?\nIn 2021\nIEEE Winter Conference on Applications of Computer Vision\n(WACV), pages 1536\u20131546. IEEE, 2021. 4\n[5] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In CVPR, 2020. 2,\n7\n[6] S. Clinchant, Ah-Pine J., and G. Csurka. Semantic combina-\ntion of textual and visual information in multimedia retrieval.\nIn ACM ICMR, pages 1\u20138, 2011. 5\n[7] Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and\nYuval Atzmon.\n\u201d this is my unicorn, fluffy\u201d: Personaliz-\ning frozen vision-language representations. arXiv preprint\narXiv:2204.01694, 2022. 4\n[8] Nick Craswell. Mean Reciprocal Rank, pages 1703\u20131703.\nSpringer US, Boston, MA, 2009. 5\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2\n[10] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. IJCV, 88(2):303\u2013338, 2010. 6\n[11] T. Gebru et al. Datasheets for datasets. ACM, 2021. 1, 2, 4\n[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904\u20136913, 2017. 4\n[13] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In CVPR,\npages 5356\u20135364, 2019. 4\n[14] J. N. Itri et al. Heuristics and cognitive error in medical imag-\ning. AJR Am J Roentgenol, 210(5):1097\u20131105, 2018. 2\n[15] Suyog Dutt Jain and Kristen Grauman.\nPredicting suffi-\ncient annotation strength for interactive foreground segmen-\ntation. In Proceedings of the IEEE International Conference\non Computer Vision, pages 1313\u20131320, 2013. 4\n[16] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In International\nConference on Machine Learning, pages 4904\u20134916. PMLR,\n2021. 3, 4\n[17] Jeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou.\nBillion-\nscale similarity search with GPUs. IEEE Transactions on\nBig Data, 7(3):535\u2013547, 2019. 5\n[18] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li,\nDavid Shamma, Michael Bernstein, and Li Fei-Fei. Image\nretrieval using scene graphs. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n3668\u20133678, 2015. 4\n[19] Daniel Kang. ML models are increasingly being deployed in\nmission-critical settings. Online Communication, LinkedIn,\nSept 2022. 2\n[20] Daniel. Kang et al. Finding label errors in autonomous vehi-\ncle data with learned observation assertions. 2022. 2\n10\n[21] Adriana Kovashka and Kristen Grauman. Discovering at-\ntribute shades of meaning with the crowd.\nInternational\nJournal of Computer Vision, 114(1):56\u201373, 2015. 4\n[22] Adriana Kovashka, Sudheendra Vijayanarasimhan, and Kris-\nten Grauman. Actively selecting annotations among objects\nand attributes. In ICCV, pages 1403\u20131410. IEEE, 2011. 4\n[23] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami\nAbu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Ui-\njlings, Stefan Popov, Andreas Veit, et al. Openimages: A\npublic dataset for large-scale multi-label and multi-class im-\nage classification.\nDataset available from https://github.\ncom/openimages, 2(3):18, 2017. 2\n[24] Ivan Krasin, Tom Duerig, Neil Alldrin, Andreas Veit, Sami\nAbu-El-Haija, Serge Belongie, David Cai, Zheyun Feng,\nVittorio Ferrari, Victor Gomes, Abhinav Gupta, Dhyanesh\nNarayanan, Chen Sun, Gal Chechik, and Kevin Murphy.\nOpenimages: A public dataset for large-scale multi-label\nand multi-class image classification. Dataset available from\nhttps://github.com/openimages, 2016. 4\n[25] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu\nMa, and Percy Liang. Fine-tuning can distort pretrained fea-\ntures and underperform out-of-distribution. arXiv preprint\narXiv:2202.10054, 2022. 4\n[26] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10965\u201310975, 2022. 4\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740\u2013755. Springer, 2014. 2, 4\n[28] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and\nStephen Gould. Image retrieval on real-life images with pre-\ntrained vision-and-language models. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 2125\u20132134, 2021. 4\n[29] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee.\n12-in-1: Multi-task vision and\nlanguage representation learning.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10437\u201310446, 2020. 4\n[30] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3195\u20133204, 2019. 4\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763. PMLR, 2021. 3, 4, 7\n[32] B. Recht et al. Do ImageNet classifiers generalize to Ima-\ngeNet? In ICML, pages 5389\u20135400. PMLR, 2019. 2\n[33] Olga Russakovsky, Li-Jia Li, and Li Fei-Fei. Best of both\nworlds: human-machine collaboration for object annotation.\nIn CVPR, pages 2121\u20132131, 2015. 4\n[34] Gabriel Skantze and Bram Willemsen.\nCollie:\nContin-\nual learning of language grounding from language-image\nembeddings.\nJournal of Artificial Intelligence Research,\n74:1201\u20131223, 2022. 4\n[35] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li,\nLi Fei-Fei, and James Hays.\nComposing text and image\nfor image retrieval-an empirical odyssey. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 6439\u20136448, 2019. 4\n[36] M. D. Wilkinson et al.\nThe FAIR Guiding Principles for\nscientific data management and stewardship. Sci Data, 2016.\n2\n[37] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim,\nMike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon-\ntijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok\nNamkoong, et al. Robust fine-tuning of zero-shot models.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7959\u20137971, 2022. 4\n[38] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.\nScene graph generation by iterative message passing. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 5410\u20135419, 2017. 4\n[39] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi\nParikh. Graph r-cnn for scene graph generation. In Proceed-\nings of the European conference on computer vision (ECCV),\npages 670\u2013685, 2018. 4\n[40] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li,\nChunyuan Li,\net al.\nFlorence:\nA new\nfoundation model for computer vision.\narXiv preprint\narXiv:2111.11432, 2021. 3, 4\n[41] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017. 2\n[42] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to prompt for vision-language models. arXiv\npreprint arXiv:2109.01134, 2021. 4\n11\nA. Pseudocode\nNotation. Let I be a set of images. For each i \u2208 I, let Bi\nbe a set of bounding boxes, bc\ni. A bounding box bc\ni is said\nto be labeled with category c \u2208 C. We denote our image\nencoder taken from our V&L model as f en\nv\n: I \u2192 Rd,\nmapping images to vectors in Rd.\nFor each image i, let Pi \u2282 I be a set of image patches.\nWe define the database D := {f en\nv (p) : p \u2208 Pi, i \u2208 I} as\nthe embedding of all patches. For our experiments, we de-\nfine two database indexes, Dtr and Dte, built from training\nItr and evaluated on Ite image sets respectively.\nFinally, we let T be the set of words. For each category\nc \u2208 C, let Lc \u2286 T be the words that are associated with c.\nWe denote our text encoder taken from our V&L model as\nf en\nt\n: T \u2192 Rd, mapping words to vectors in Rd.\nAlgorithm 1 PDC Framework\nInput: Category, c; the words associated with c, Lc; the\ntraining index Dtr\nOutput: \u0398, a set of instruction pair(s) (bc\ni, t), where bc\ni \u2208\nBi and t \u2208 T\n1: Let Ic = {i \u2208 Itr : \u2203bc\ni \u2208 Bi}\n2: Let Vc{arg maxbc\ni \u2208Bi area(bc\ni) : i \u2208 Ic}\n3: Define rtv : Vc \u00d7 Lc \u00d7 Dtr\nrtv(b, w, i) = PatchFusp\u2208Pi(\nMultiScore(f en\nv (b), f en\nt (w), f en\nv (p)))\n4: Define Jtv(b, w) = topki\u2208Itrrtv(b, w, i)\n5: Define \u03b1tv(b, w) = AUC({rtv(b, w, i) : i \u2208 Jtv}\n6: Let (b\u22c6, w\u22c6) = arg max(b,w)\u2208Vt\u00d7Lc \u03b1tv(b, w)\n7: Add (b\u22c6, w\u22c6) to \u0398\n8: Define s\u2217(\u0398) :=\nAUC(S\n(b,w)\u2208\u0398{rtv(b, w, i) : i \u2208 Jtv(b, w)})\n9: Let \u0398\u2032 = \u0398\u22c6\n10: while s\u2217(\u0398\u2032) \u2265 s\u2217(\u0398\u22c6) do\n11:\n\u0398\u22c6 = \u0398\u2032\n12:\n(b\u2032, w\u2032) = arg max(b,w)\u2208Vt\u00d7Lc s\u22c6(\u0398\u22c6 S{b, w})\n13:\n\u0398\u2032 = \u0398\u22c6 S{b, w}\n14: end while\nAlgorithm. We formally describe our PDC algorithm as\ndepicted in Algorithm 1. Algorithm 1 runs on all c \u2208 C.\nWe assume that for each c, there exists a set of associated\ndescriptive words.\nFor example, class animal has the\nfollowing labels/subclasses/synonyms as the set of words,\n(\u2018animal\u2019, \u2018dog\u2019, \u2018bird\u2019, \u2018cat\u2019). First, in Lines 1 and 2 we\ndefine Vc as the set of the largest bounding box object la-\nbeled c for each image in our training set Itr. Now, we have\na potential pool of texts (Lc) and images (Vc). From this\npool, we will be matching one text and one image to create\npairs that will compose our final instruction pairs.\nIn the following process, we match every text to every\nimage to create every potential (text, image) pair, (b, w).\nFor each potential pair, in\nLine 3, we utilize them as a\nmulti-modal query against our index of patches. Here, we\nintuitively measure each pair\u2019s effectiveness as an instruc-\ntion pair by measuring its capabilities at image retrieval. In\nLines 4 and 5, we utilize PatchFusion to retrieve a set of top\nk image scores indicating highest similarity to each multi-\nmodal query. In image retrieval, it is common practice to\nuse precision-recall at k as a metric. Similarly, from a set\nof image scores, we can measure our precision at several k\nsteps. With a precision-recall at k curve, we can measure its\narea under the curve (AUC). The higher the AUC, the better\nour query is. Thus, we measure the AUC for each set of\nresults from each of our potential (text, image) pair, (b, w).\nIn Lines 6 and 7, we add the pair with best AUC into our\nfinal output instruction set.\nWith the first instruction pair decided, PDC grows the in-\nstruction set as much as possible in Line 8 to Line 14. To\ndo so, we continue to comb through our potential text(Lc)\nand bounding box(Vc) pool and create all potential pairs.\nWhile the AUC of our final outcome instruction set is still\nimproving, we continuously add a potential new pair into\nour outcome set and test its new AUC. If this new pair im-\nproves the outcome\u2019s AUC, we add it into the outcome set.\nB. NuImages Results and Ablations\nPDC Generated Instruction Pairs Additionally, we\nshow a subset of our generated instruction pairs for NuIm-\nages in Fig. A1 since we have too many qualitative results\nthat can reasonably be displayed in a paper.\nClass PR curves. In addition to Fig. 5 (main), we pro-\nvide class PR curves for all NuImages classes in Fig. A2.\nTable A1: Comparisons of instruction pairs results fused\nby various query policies. Average APs across 5 folds is\nshown per class. Final PDC setup gets the best mAP and\nbest AP for 13 of 23 classes. The next best fusion is Early:\nWeighted, which achieves the best AP for 7 of 23 classes.\nCategory\nSamps\nSum:Avg\nEarly:Wt\nLate:Naive\nLate:Rank\nPDC Pairs\ncar\n56517\n8.8\u00b10.1\n8.8\u00b10.1\n8.8\u00b10.1\n8.8\u00b10.1\n8.8\u00b10.1\nadult ped.\n40241\n11.8\u00b10.3\n12.1\u00b10.2\n9.0\u00b10.3\n9.3\u00b10.3\n12.0\u00b10.2\ntruck\n23499\n15.4\u00b11.3\n16.5\u00b11.2\n15.7\u00b10.7\n16.1\u00b10.6\n16.5\u00b10.7\ntraffic cone\n22194\n22.0\u00b10.3\n22.2\u00b10.2\n22.1\u00b10.3\n22.3\u00b10.2\n22.2\u00b10.3\ntraffic barrier\n13607\n12.9\u00b13.2\n16.9\u00b12.7\n13.3\u00b12.6\n14.0\u00b12.7\n16.9\u00b12.3\nmotorcycle\n12523\n31.2\u00b13.2\n34.3\u00b11.4\n32.4\u00b11.6\n33.2\u00b11.3\n33.6\u00b11.4\nbicycle\n11883\n27.2\u00b14.2\n30.1\u00b12.1\n27.5\u00b13.6\n28.2\u00b13.3\n29.1\u00b12.8\nrigid bus\n7042\n24.8\u00b11.3\n25.0\u00b11.6\n24.2\u00b10.9\n24.9\u00b10.9\n24.8\u00b11.1\nconstruct. wrkr.\n5586\n27.6\u00b14.3\n34.2\u00b12.9\n29.4\u00b13.2\n31.4\u00b13.5\n31.4\u00b14.4\nconstruct. veh.\n5258\n36.5\u00b12.4\n32.6\u00b16.1\n30.7\u00b12.5\n33.1\u00b12.3\n39.1\u00b11.0\nbicycle rack\n2771\n17.7\u00b110.6\n22.0\u00b18.3\n16.7\u00b16.7\n17.6\u00b16.9\n21.3\u00b110.8\npush-pull object\n2585\n4.8\u00b12.0\n5.7\u00b11.4\n4.6\u00b11.7\n4.9\u00b11.7\n6.2\u00b12.1\ntrailer\n2286\n16.9\u00b14.5\n15.2\u00b14.9\n10.0\u00b11.7\n10.7\u00b12.0\n17.3\u00b14.6\ndebris\n1840\n7.2\u00b11.3\n7.7\u00b11.2\n4.6\u00b10.5\n4.8\u00b10.6\n8.9\u00b11.6\nchild ped.\n1060\n1.1\u00b10.4\n1.8\u00b10.3\n1.5\u00b10.6\n1.6\u00b10.6\n1.5\u00b10.4\npers. mobi. veh.\n790\n3.9\u00b11.8\n4.2\u00b12.2\n5.8\u00b12.3\n6.0\u00b12.4\n4.5\u00b11.3\npolice officer\n356\n3.7\u00b13.4\n4.3\u00b14.5\n5.2\u00b13.8\n5.4\u00b13.9\n5.5\u00b15.2\nstroller\n334\n13.6\u00b18.4\n14.0\u00b18.7\n10.0\u00b12.3\n11.0\u00b12.6\n16.5\u00b13.9\nanimal\n202\n1.1\u00b12.0\n0.0\u00b10.0\n0.3\u00b10.4\n0.3\u00b10.4\n0.8\u00b11.3\nbendy bus\n169\n5.3\u00b14.0\n4.5\u00b12.5\n4.9\u00b12.5\n5.9\u00b13.0\n6.9\u00b14.4\npolice vehicle\n132\n12.3\u00b16.1\n15.6\u00b19.8\n14.6\u00b111.2\n16.0\u00b111.0\n16.3\u00b111.0\nambulance\n40\n0.1\u00b10.2\n0.1\u00b10.1\n0.4\u00b10.4\n0.4\u00b10.4\n0.1\u00b10.2\nwheelchair\n33\n13.5\u00b17.5\n11.5\u00b16.4\n6.7\u00b15.9\n9.2\u00b18.9\n14.7\u00b19.2\nmAP\n-\n13.89\n14.75\n12.97\n13.69\n15.44\n12\nQuery Fusion Policy Ablations All results that accom-\npany Tab. 3 are shown in Tab. A1 with class PR curves\nin Fig. A3. Importantly, we see that across the majority of\ncategories, our final PDC setup with Sum:Max outperforms\nthe other fusion methods.\nGenerated Instruction Pairs Diagnostics All results\nthat accompany Tab. 4 are shown in Tab. A2 and class\nPR curves in Fig. A4. Again, across most categories, we\nsee that PDC text and bbox pairs outperforms the next best,\nbbox only. Our main observation holds: using only bboxes\nis generally better than using only texts.\nTable A2: Results from using only the texts or bboxes of\nour PDC instruction set. Average APs across 5 folds is dis-\nplayed per class. Using both text and bboxes provides the\nbest APs for 17 of 23 classes.\nCategory\nSamps\nTexts\nBboxes\nPDC Pairs\ncar\n56517\n8.7\u00b10.0\n8.8\u00b10.0\n8.8\u00b10.1\nadult pedestrian\n40241\n7.2\u00b10.1\n11.8\u00b10.0\n12.0\u00b10.2\ntruck\n23499\n14.8\u00b10.2\n15.1\u00b10.1\n16.5\u00b10.7\ntraffic cone\n22194\n22.1\u00b10.1\n21.1\u00b10.3\n22.2\u00b10.3\ntemporary traffic barrier\n13607\n10.6\u00b10.6\n12.3\u00b10.7\n16.9\u00b12.3\nmotorcycle\n12523\n32.0\u00b10.1\n30.1\u00b10.6\n33.6\u00b11.4\nbicycle\n11883\n30.4\u00b10.1\n23.5\u00b11.3\n29.1\u00b12.8\nrigid bus\n7042\n20.6\u00b10.3\n22.8\u00b10.1\n24.8\u00b11.1\nconstruction worker\n5586\n30.5\u00b10.2\n21.4\u00b11.0\n31.4\u00b14.4\nconstruction vehicle\n5258\n20.1\u00b10.6\n38.6\u00b10.5\n39.1\u00b11.0\nbicycle rack\n2771\n13.5\u00b10.3\n16.7\u00b12.2\n21.3\u00b110.8\npushable pullable object\n2585\n2.8\u00b10.2\n5.4\u00b10.4\n6.2\u00b12.1\ntrailer\n2286\n2.4\u00b10.2\n16.7\u00b10.7\n17.3\u00b14.6\ndebris\n1840\n0.3\u00b10.0\n9.3\u00b10.2\n8.9\u00b11.6\nchild pedestrian\n1060\n1.6\u00b10.1\n0.9\u00b10.1\n1.5\u00b10.4\nportable personal mobility vehicle\n790\n4.4\u00b10.3\n3.4\u00b10.5\n4.5\u00b11.3\npolice officer\n356\n3.9\u00b10.7\n3.8\u00b10.6\n5.5\u00b15.2\nstroller\n334\n8.0\u00b10.5\n9.2\u00b10.7\n16.5\u00b13.9\nanimal\n202\n0.1\u00b10.0\n0.4\u00b10.1\n0.8\u00b11.3\nbendy bus\n169\n0.9\u00b10.1\n8.6\u00b10.9\n6.9\u00b14.4\npolice vehicle\n132\n4.7\u00b11.1\n18.9\u00b11.9\n16.3\u00b111.0\nambulance\n40\n0.7\u00b10.2\n0.1\u00b10.0\n0.1\u00b10.2\nwheelchair\n33\n1.2\u00b10.2\n13.2\u00b12.3\n14.7\u00b19.2\nmAP\n-\n10.50\n13.56\n15.44\n13\nFigure A1: NuImages results: PDC\u2019s generated NuImages instruction pairs are shown. In these sets, we observe different\nsubtypes of classes, objects in different sizes and viewpoints, and various text synonyms. In \u2018shuttle\u2019, we see a shuttle (2nd\nimage) that is correctly paired with the text \u2018bendy shuttle\u2019. In \u2018truck\u2019, we see various types of trucks/lorries. However, we\nsee a mismatched dump truck that is also paired with the text \u2018pickup truck\u2019. In \u2018adult pedestrian\u2019 and \u2018construction worker\u2019,\nwe can observe people in various outfits, locations, and positions - a person sitting in the right most image of adult pedestrian.\nConstruct worker\nTraffic cone\nBendy bus\nbendy shuttle\nbendy shuttle\nbendy bus\ntraffic cone\ntraffic cone\ntraffic cone\ntraffic cone\nconstruction worker\nconstruction worker\nconstruction worker\nconstruction worker\nTruck\nsemi tractor\npickup truck\nlorry\nlorry\nAdult Ped.\nadult pedestrian\nadult pedestrian\nadult pedestrian\nadult pedestrian\n14\nFigure A2: NuImages Baselines results: We display the per class AP curve across 5 folds here in parts. These plots\ncorrespond to the calculated AP in Tabs. 1 and subset PR curves in Fig. 4 in the main paper. The solid black line is\nour PDC Pair. We note that for each class, our PDC curve is comfortably above the others. In particular, we see significant\nincreases in harder classes that achieves low precisions with other baselines (e.g. \u2018debris\u2019, \u2018pushable pullable object\u2019, \u2018trailer\u2019,\n\u2018police vehicle\u2019, \u2018bendy bus\u2019). Lastly, classes with strong performance using Original Texts baselines still see a noticeable\nimprovement with our PDC framework.\n0.000 0.025 0.050 0.075 0.100 0.125\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nAdult Pedestrian\n0.0\n0.1\n0.2\n0.3\n0.4\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\nAmbulance\n0.00\n0.02\n0.04\n0.06\n0.08\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nAnimal\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nBendy Bus\n0.0\n0.1\n0.2\n0.3\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nBicycle\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nPrecision\nBicycle Rack\n0.00\n0.02\n0.04\n0.06\n0.08\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nCar\n0.00\n0.05\n0.10\n0.15\n0.20\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nChild Pedestrian\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.2\n0.4\n0.6\n0.8\n1.0\nConstruction Vehicle\n0.0\n0.1\n0.2\n0.3\n0.4\n0.2\n0.4\n0.6\n0.8\n1.0\nConstruction Worker\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPrecision\nDebris\n0.0\n0.1\n0.2\n0.3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMotorcycle\n0.0\n0.1\n0.2\n0.0\n0.1\n0.2\n0.3\n0.4\nPolice Officer\n0.0\n0.2\n0.4\n0.6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPolice Vehicle\n0.0\n0.1\n0.2\n0.3\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nPortable Personal Mobility Vehicle\n0.00\n0.05\n0.10\n0.15\n0.20\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPrecision\nPushable Pullable Object\n0.0\n0.1\n0.2\n0.3\n0.2\n0.4\n0.6\n0.8\n1.0\nRigid Bus\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.2\n0.4\n0.6\n0.8\nStroller\n0.00\n0.05\n0.10\n0.15\n0.20\nRecall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTemporary Traffic Barrier\n0.00\n0.05\n0.10\n0.15\n0.20\nRecall\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTraffic Cone\n0.0\n0.1\n0.2\n0.3\n0.4\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrecision\nTrailer\n0.00\n0.05\n0.10\n0.15\nRecall\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTruck\n0.0\n0.2\n0.4\n0.6\nRecall\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nWheelchair\nMethod\nPDC Pairs\nOriginal Texts\nOriginal Pairs\nRandom Bboxes\nRandom Pairs\n15\nFigure A3: NuImages Query Fusion Policy Ablations results: We display the per class AP curve across 5 folds here in parts.\nThese plots correspond to the calculated AP in Table 2 in the main paper. The solid black line is our PDC Pair. The PR\ncurves indicate two main points: 1) Late Fusion Naive and Inverse Rank both underperform in almost all categories, 2) Our\nEarly Fusion methods show similar results. Thus, we rely on AP to reliably inform us that Early Fusion: Sum, Max is our\nbest fusion method across multiple queries. In the main paper, we see that our final PDC outperforms the next best (Early\nFusion: Weighted) by 0.69 mAP.\n0.000 0.025 0.050 0.075 0.100 0.125\n0.80\n0.85\n0.90\n0.95\n1.00\nPrecision\nAdult Pedestrian\n0.0\n0.1\n0.2\n0.3\n0.000\n0.002\n0.004\n0.006\n0.008\nAmbulance\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nAnimal\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.05\n0.10\n0.15\n0.20\n0.25\nBendy Bus\n0.0\n0.1\n0.2\n0.3\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\nBicycle\n0.0\n0.1\n0.2\n0.3\n0.4\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrecision\nBicycle Rack\n0.00\n0.02\n0.04\n0.06\n0.08\n0.9960\n0.9965\n0.9970\n0.9975\n0.9980\n0.9985\n0.9990\n0.9995\n1.0000\nCar\n0.00\n0.05\n0.10\n0.15\n0.20\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nChild Pedestrian\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConstruction Vehicle\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConstruction Worker\n0.0\n0.1\n0.2\n0.3\n0.1\n0.2\n0.3\n0.4\n0.5\nPrecision\nDebris\n0.0\n0.1\n0.2\n0.3\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nMotorcycle\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.4\nPolice Officer\n0.2\n0.4\n0.6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPolice Vehicle\n0.0\n0.1\n0.2\n0.3\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nPortable Personal Mobility Vehicle\n0.00\n0.05\n0.10\n0.15\n0.20\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPrecision\nPushable Pullable Object\n0.0\n0.1\n0.2\n0.3\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nRigid Bus\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nStroller\n0.00\n0.05\n0.10\n0.15\n0.20\nRecall\n0.6\n0.7\n0.8\n0.9\n1.0\nTemporary Traffic Barrier\n0.00\n0.05\n0.10\n0.15\n0.20\nRecall\n0.9850\n0.9875\n0.9900\n0.9925\n0.9950\n0.9975\n1.0000\nTraffic Cone\n0.0\n0.1\n0.2\n0.3\n0.4\nRecall\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrecision\nTrailer\n0.00\n0.05\n0.10\n0.15\nRecall\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nTruck\n0.2\n0.4\n0.6\nRecall\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nWheelchair\nMethod\nPDC Pairs\nEarlyFus: Sum, Avg\nEarlyFus: Weighted\nLateFus: Naive\nLateFus: Inverse Rank\n16\nFigure A4: NuImages Generated Instruction Pairs Diagnostics results: We display the per class AP curve across 5 folds here\nin parts. These plots correspond to the calculated AP in Tabs. 3 in the main paper. The solid black line is our PDC Pair. We\nnotice that images from our generated pairs contributes more to our final PDC results than text only. We see this particularly\nin \u2018pushable pullable object\u2019, \u2018bendy bus\u2019, \u2018construction vehicle\u2019, and \u2018temporary traffic barrier\u2019. In the main paper, we see\nthat using both text and images as pairs outperforms the next best (image only) by 1.88 mAP.\n0.000 0.025 0.050 0.075 0.100 0.125\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nAdult Pedestrian\n0.0\n0.1\n0.2\n0.3\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\nAmbulance\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nAnimal\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nBendy Bus\n0.0\n0.1\n0.2\n0.3\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nBicycle\n0.0\n0.1\n0.2\n0.3\n0.4\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nPrecision\nBicycle Rack\n0.00\n0.02\n0.04\n0.06\n0.08\n0.990\n0.992\n0.994\n0.996\n0.998\n1.000\nCar\n0.00\n0.05\n0.10\n0.15\n0.20\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nChild Pedestrian\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConstruction Vehicle\n0.0\n0.1\n0.2\n0.3\n0.4\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConstruction Worker\n0.0\n0.1\n0.2\n0.3\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPrecision\nDebris\n0.0\n0.1\n0.2\n0.3\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nMotorcycle\n0.05\n0.10\n0.15\n0.20\n0.25\n0.0\n0.1\n0.2\n0.3\n0.4\nPolice Officer\n0.2\n0.4\n0.6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPolice Vehicle\n0.0\n0.1\n0.2\n0.3\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nPortable Personal Mobility Vehicle\n0.00\n0.05\n0.10\n0.15\n0.20\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPrecision\nPushable Pullable Object\n0.0\n0.1\n0.2\n0.3\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nRigid Bus\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nStroller\n0.00\n0.05\n0.10\n0.15\n0.20\nRecall\n0.5\n0.6\n0.7\n0.8\n0.9\nTemporary Traffic Barrier\n0.00\n0.05\n0.10\n0.15\n0.20\nRecall\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00\nTraffic Cone\n0.0\n0.1\n0.2\n0.3\n0.4\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrecision\nTrailer\n0.00\n0.05\n0.10\n0.15\nRecall\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nTruck\n0.0\n0.2\n0.4\n0.6\nRecall\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nWheelchair\nMethod\nPDC Pairs\nOnly Text from Optimized Pairs\nOnly Images from Optimized Pairs\n17\nC. COCO Results\nWe demonstrate our Proxy Dataset Curator (PDC) re-\nsults on the COCO dataset. Our implementation setup for\nCOCO is the same as for NuImages as detailed in Sec. 4.1.\nIn the following section, we show three sets of results: 1)\nAPs per class along with mAP, 2) Qualitative results show-\ning the generated instruction pairs from COCO, 3) Per class\nPR curves used to generate our average APs across folds.\nC.1. APs\nWe provide the full COCO AP list accompanying Table\n2 in the main paper in Tabs. A3 and A4.\nTable A3: COCO results part 1: Comparisons of instruc-\ntion pairs generated by different methods.\nAverage APs\nacross 5 folds is displayed per class. Classes are sorted\nbased on the number of images. mAP is calculated across\nall classes. Note, low performing classes have fewer sam-\nples. PDC outperforms the next best baselines by a signif-\nicant 12.9 mAP and performs the best for 75 of 80 classes.\nWe observe that PDC best performs for top 48 most fre-\nquent classes. In cases where PDC underperforms, we note\nthat the difference is usually within 1 AP. Lastly, we ob-\nserve a pattern seen in NuImages, Random Pairs outper-\nforms Random BBoxes but underperforms both PDC and\nOriginal Texts.\nCategory\nSamps Org. Ts Rnd. Bbs.\nRnd. Ps\nPDC Pairs\nperson\n66808\n7.3\u00b10.1\n6.2\u00b11.3\n6.4\u00b11.2\n7.5\u00b10.0\nchair\n13354 15.1\u00b11.1\n3.2\u00b12.0\n7.8\u00b14.7\n21.5\u00b11.0\ncar\n12786 18.3\u00b10.5\n3.3\u00b14.7\n5.2\u00b15.8\n31.3\u00b11.6\ndining table 12338 12.4\u00b10.5\n7.2\u00b19.4\n9.1\u00b19.8\n25.6\u00b10.9\ncup\n9579\n16.6\u00b10.9\n2.1\u00b11.5\n6.1\u00b13.5\n27.7\u00b11.9\nbottle\n8880\n20.2\u00b11.5\n0.3\u00b10.3\n0.5\u00b10.4\n31.0\u00b12.1\nbowl\n7425\n10.1\u00b11.2\n0.9\u00b10.5\n3.3\u00b11.5\n27.2\u00b11.5\nhandbag\n7133\n9.6\u00b10.4\n0.7\u00b10.5\n1.9\u00b11.9\n17.9\u00b11.0\ntruck\n6377\n25.6\u00b10.9\n3.2\u00b14.4\n6.6\u00b15.9\n31.7\u00b13.3\nbench\n5805\n20.9\u00b11.9\n0.5\u00b10.5\n1.2\u00b11.0\n28.2\u00b12.1\nbackpack\n5756\n16.9\u00b11.2\n0.2\u00b10.1\n1.3\u00b11.3\n21.1\u00b11.6\nbook\n5562\n14.0\u00b10.7\n0.3\u00b10.2\n1.0\u00b10.8\n40.4\u00b11.5\ncell phone\n5017\n15.2\u00b11.4\n0.6\u00b10.2\n1.3\u00b10.7\n25.4\u00b12.5\nsink\n4865\n50.8\u00b12.0\n3.0\u00b12.0\n11.1\u00b16.2\n60.5\u00b12.1\nclock\n4863\n61.6\u00b11.4\n8.7\u00b117.8\n26.4\u00b116.1\n65.6\u00b11.1\ntv\n4768\n34.2\u00b11.6\n6.7\u00b19.4\n12.5\u00b112.0\n48.8\u00b13.1\npotted plant\n4624\n17.7\u00b11.0\n0.4\u00b10.3\n2.6\u00b12.1\n33.2\u00b12.1\ncouch\n4618\n43.3\u00b11.7\n7.0\u00b16.4\n21.7\u00b110.2\n47.5\u00b11.2\ndog\n4562\n23.6\u00b10.7 18.5\u00b115.6 26.6\u00b116.0\n60.5\u00b13.8\nknife\n4507\n7.8\u00b11.4\n0.9\u00b10.9\n2.0\u00b11.7\n23.5\u00b13.7\nsports ball\n4431\n43.9\u00b12.4\n2.2\u00b14.7\n4.5\u00b19.5\n46.0\u00b13.6\ntraffic light\n4330\n54.5\u00b12.2\n0.3\u00b10.2\n4.9\u00b15.0\n58.2\u00b14.8\ncat\n4298\n43.5\u00b11.5 62.1\u00b112.7\n66.4\u00b18.9\n82.1\u00b13.0\numbrella\n4142\n45.7\u00b12.9\n3.3\u00b13.2\n19.8\u00b18.0\n51.9\u00b13.6\nbus\n4141\n64.4\u00b12.1 26.0\u00b125.4 36.9\u00b129.5\n69.2\u00b13.4\ntie\n3955\n25.2\u00b12.2\n1.0\u00b10.5\n3.7\u00b12.9\n35.6\u00b14.5\nbed\n3831\n52.9\u00b11.5\n9.4\u00b17.5\n25.6\u00b114.1\n56.0\u00b13.5\ntrain\n3745\n60.1\u00b10.7 34.5\u00b132.6 40.1\u00b136.0\n79.1\u00b13.7\nvase\n3730\n20.5\u00b12.1\n0.6\u00b10.3\n1.1\u00b10.6\n37.2\u00b13.8\nmAP\n-\n42.0\n13.4\n21.1\n54.9\nTable A4: COCO results part 2: Comparisons of instruc-\ntion pairs generated by different methods.\nAverage APs\nacross 5 folds is displayed per class. Classes are sorted\nbased on the number of images containing them.\nCategory\nSamps\nOrg. Ts\nRnd. Bbs.\nRnd. Ps\nPDC Pairs\nspoon\n3682\n8.7\u00b10.9\n1.2\u00b10.7\n2.7\u00b11.9\n19.0\u00b11.3\nmotorcycle\n3661\n75.6\u00b10.7\n24.2\u00b126.3 42.7\u00b126.9\n79.2\u00b11.6\nsurfboard\n3635\n31.9\u00b11.2\n8.6\u00b110.8\n18.7\u00b119.0\n77.7\u00b18.5\nskateboard\n3603\n70.5\u00b11.3\n1.6\u00b11.6\n14.0\u00b110.3\n81.3\u00b15.3\ntennis racket\n3561\n76.4\u00b10.8\n47.8\u00b141.7 59.0\u00b132.2\n93.1\u00b12.3\ntoilet\n3502\n82.9\u00b11.7\n27.7\u00b132.9 41.8\u00b133.6\n83.9\u00b11.6\nbicycle\n3401\n52.5\u00b12.2\n17.6\u00b118.2 32.5\u00b114.7\n56.8\u00b14.4\nbird\n3362\n23.1\u00b11.1\n10.1\u00b113.1 12.3\u00b115.2\n53.0\u00b12.7\npizza\n3319\n81.2\u00b10.8\n42.1\u00b134.4 59.1\u00b123.4\n88.4\u00b11.3\nremote\n3221\n13.6\u00b11.2\n0.3\u00b10.2\n0.3\u00b10.2\n23.6\u00b12.9\nskis\n3202\n64.4\u00b13.5\n4.4\u00b15.8\n18.0\u00b116.2\n80.2\u00b11.8\nboat\n3146\n52.0\u00b12.1\n4.8\u00b16.5\n16.9\u00b111.6\n69.9\u00b12.7\nairplane\n3083\n40.4\u00b11.8\n49.4\u00b140.2 52.1\u00b138.4\n91.9\u00b12.1\nhorse\n3069\n48.8\u00b11.9\n26.9\u00b18.4\n38.7\u00b15.5\n72.7\u00b13.4\ncake\n3049\n46.6\u00b13.9\n13.0\u00b18.7\n21.9\u00b112.1\n69.0\u00b15.3\noven\n2992\n46.4\u00b10.6\n9.5\u00b115.7\n15.5\u00b119.4\n63.8\u00b13.2\nbaseball glove\n2729\n49.2\u00b11.5\n0.2\u00b10.1\n2.4\u00b11.8\n82.1\u00b11.8\ngiraffe\n2647\n97.3\u00b10.4 70.1\u00b136.4 86.7\u00b119.4\n96.6\u00b11.1\nwine glass\n2643\n45.0\u00b12.1\n6.8\u00b113.8\n17.3\u00b113.6\n58.6\u00b12.6\nbaseball bat\n2603\n11.9\u00b11.7\n3.2\u00b12.3\n5.5\u00b14.1\n69.0\u00b11.3\nsuitcase\n2507\n44.2\u00b13.1\n1.0\u00b10.6\n6.1\u00b12.8\n53.0\u00b14.4\nsandwich\n2463\n43.3\u00b13.3\n15.0\u00b18.3\n24.0\u00b111.9\n56.8\u00b14.0\nrefrigerator\n2461\n46.2\u00b11.8\n2.5\u00b11.6\n9.5\u00b15.2\n47.9\u00b14.5\nkite\n2352\n47.5\u00b11.6\n5.2\u00b16.0\n12.4\u00b113.1\n68.5\u00b16.7\nbanana\n2346\n63.4\u00b13.3\n19.2\u00b125.1 30.7\u00b127.7\n71.8\u00b13.0\nfrisbee\n2268\n36.3\u00b13.4\n0.4\u00b10.4\n0.8\u00b11.2\n28.1\u00b14.8\nteddy bear\n2234\n64.9\u00b13.0\n19.0\u00b123.2 45.0\u00b113.6\n77.1\u00b12.4\nelephant\n2232\n87.5\u00b11.8\n84.3\u00b14.3\n89.9\u00b13.2\n91.7\u00b14.1\nkeyboard\n2221\n49.5\u00b13.1\n13.1\u00b119.7 26.7\u00b121.9\n57.1\u00b14.6\ncow\n2055\n56.6\u00b11.5\n29.7\u00b122.8 42.2\u00b121.6\n67.9\u00b15.6\nbroccoli\n2010\n70.1\u00b12.4\n35.6\u00b123.9 56.6\u00b126.8\n85.0\u00b11.5\nzebra\n2001\n98.5\u00b10.5 71.5\u00b136.7\n93.1\u00b18.6\n97.9\u00b10.7\nmouse\n1964\n5.1\u00b11.1\n1.0\u00b11.0\n2.3\u00b13.7\n54.0\u00b11.5\nstop sign\n1803\n69.9\u00b11.7 16.4\u00b119.1 41.8\u00b116.2\n68.4\u00b11.4\nfire hydrant\n1797\n69.6\u00b11.0\n0.9\u00b11.3\n12.9\u00b111.7\n61.8\u00b12.6\norange\n1784\n45.9\u00b11.0\n5.0\u00b15.6\n16.2\u00b110.5\n64.3\u00b15.1\ncarrot\n1764\n56.9\u00b12.3\n2.0\u00b11.9\n10.8\u00b18.2\n65.5\u00b12.3\nsnowboard\n1703\n48.2\u00b15.0\n0.3\u00b10.1\n8.6\u00b13.4\n47.3\u00b13.3\napple\n1662\n26.6\u00b11.7\n5.8\u00b16.7\n14.2\u00b111.4\n56.9\u00b13.3\nmicrowave\n1601\n40.0\u00b13.9\n1.2\u00b10.8\n5.5\u00b13.4\n42.3\u00b12.3\nsheep\n1594\n78.3\u00b10.7\n40.6\u00b125.2 59.3\u00b120.4\n85.4\u00b11.4\ndonut\n1585\n56.6\u00b13.1\n7.4\u00b115.7\n11.0\u00b119.7\n69.7\u00b13.2\nhot dog\n1273\n49.5\u00b11.7\n20.8\u00b118.8 28.2\u00b119.6\n59.2\u00b17.3\ntoothbrush\n1041\n22.0\u00b12.5\n0.3\u00b10.2\n0.6\u00b10.4\n34.1\u00b14.5\nbear\n1009\n56.8\u00b12.3\n66.6\u00b118.6 72.5\u00b114.0\n85.0\u00b12.4\nscissors\n975\n23.2\u00b12.7\n0.4\u00b10.3\n2.9\u00b14.6\n28.8\u00b13.8\nparking meter\n742\n38.9\u00b12.8\n0.1\u00b10.1\n0.9\u00b10.6\n42.3\u00b14.5\ntoaster\n225\n4.6\u00b14.6\n0.4\u00b10.6\n1.1\u00b12.1\n6.6\u00b12.7\nhair drier\n198\n11.5\u00b11.0\n0.4\u00b10.5\n0.5\u00b10.5\n7.1\u00b14.2\nmAP\n-\n42.0\n13.4\n21.1\n54.9\nC.2. PDC Generated Instruction Pairs\nWe show a subset of our generated instruction pairs for\nCOCO in Figs. A5 and A6 since we have too many quali-\ntative results that can reasonably be displayed in a paper.\nC.3. Class PR curves.\nWe provide class PR curves for all COCO classes in\nFigs. A7 to A9.\n18\nFigure A5: COCO results part 1: PDC\u2019s generated COCO instruction pairs are shown. For simplicity, we display the\ncommon text for each pair on the left side instead of over each image. The order in which (text,image) pairs are selected\nis shown from left to right. Next, we observe that the generated instruction pairs show 1) prototypical images (seen in all\ncategories displayed), 2) corner case images, 3) occluded objects. In this set we see particularly interesting corner cases. For\ninstance, \u2018sandwich\u2019 instruction set contains a \u2018sub\u2019 that looks remarkably like a \u2018hot dog\u2019. We see various types of \u2018stop\nsigns\u2019 - graffitied, in Arabic, and even crumpled. We also see an unique example of a sheep that is currently being sheared,\na picture of a sheep, and a flock of sheep. In \u2018potted plant\u2019, we observe various different types of plants in various sizes\nand vases. Lastly, we emphasize that while we only show instruction pairs for 10 categories, all categories show remarkably\ninteresting and visually important details.\nSandwich\nHotdog\nSheep\nStop sign\nPotted Plant\n19\nFigure A6: COCO results part 2: In this set, we observe examples for object diversity. We observe that each example\nin \u2018truck\u2019 contains a different type: pickup truck, front of semi-tractor, lorry, and a cross between pickup truck and lorry.\nAdditionally, these examples are partly occluded. Interestingly, we see unique yellow and occluded \u2018fire hydrant\u2019. We also\nsee a rare red and white colored fire hydrant as well as a black and white image. In \u2018sports ball\u2019 and \u2018dog\u2019, we again observe\nvarious types of objects in different viewpoints: volley ball, tennis ball, and soccer ball; back of a dog\u2019s head, boxer dog, and\ndog jumping. Lastly, we note that each example in \u2018backpack\u2019 is of different color, size, and viewpoint.\nSports ball\nDog\nTruck\nFire hydrant\nBackpack\n20\nFigure A7: COCO results part 1: We display the per class AP curve across 5 folds here in parts. These plots correspond to\nthe calculated AP in Tabs. A3 and A4. The solid black line is our PDC Pair. We note that for each class, our PDC curve\nis comfortably above the others. Furthermore, we see that our precisions and recall are quite high, indicating that we find a\nsignificant amount of ground truth objects in the top retrievals.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nAirplane\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nApple\n0.0\n0.1\n0.2\n0.3\n0.0\n0.2\n0.4\n0.6\n0.8\nBackpack\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nBanana\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nBaseball Bat\n0.0\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nBaseball Glove\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nBear\n0.0\n0.2\n0.4\n0.6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nBed\n0.0\n0.1\n0.2\n0.3\n0.2\n0.4\n0.6\n0.8\n1.0\nBench\n0.0\n0.2\n0.4\n0.6\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nBicycle\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nBird\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nBoat\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBook\n0.0\n0.1\n0.2\n0.3\n0.2\n0.4\n0.6\n0.8\n1.0\nBottle\n0.0\n0.1\n0.2\n0.3\n0.2\n0.4\n0.6\n0.8\n1.0\nBowl\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nBroccoli\n0.0\n0.2\n0.4\n0.6\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nBus\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nCake\n0.0\n0.1\n0.2\n0.3\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nCar\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nCarrot\n0.0\n0.2\n0.4\n0.6\n0.8\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nCat\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCell Phone\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nChair\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nClock\n0.0\n0.2\n0.4\n0.6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nCouch\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nCow\n0.0\n0.1\n0.2\n0.3\n0.2\n0.4\n0.6\n0.8\nCup\n0.0\n0.1\n0.2\n0.3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nDining Table\n0.0\n0.2\n0.4\n0.6\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDog\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nDonut\n0.9\n1.0\nElephant\n0.8\n1.0\nFire Hydrant\n0.8\n1.0\nFork\n0.8\n1.0\nFrisbee\n0.9\n1.0\nGiraffe\n21\nFigure A8: COCO results part 2: We display the per class AP curve across 5 folds here in parts. These plots correspond to\nthe calculated AP in Tabs. A3 and A4. The solid black line is our PDC Pair. Again, we see that for each class, our PDC curve\nis comfortably above the others. Precisions and recalls remain high, indicating that we find a significant amount of ground\ntruth objects in the top retrievals.\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrecision\n0.0\n0.2\n0.4\n0.6\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.0\n0.1\n0.2\n0.3\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nCat\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCell Phone\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nChair\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nClock\n0.0\n0.2\n0.4\n0.6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nCouch\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nCow\n0.0\n0.1\n0.2\n0.3\n0.2\n0.4\n0.6\n0.8\nCup\n0.0\n0.1\n0.2\n0.3\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nDining Table\n0.0\n0.2\n0.4\n0.6\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDog\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nDonut\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nElephant\n0.0\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFire Hydrant\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nFork\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFrisbee\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nGiraffe\n0.0\n0.2\n0.4\n0.6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPrecision\nHair Drier\n0.0\n0.1\n0.2\n0.3\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nHandbag\n0.0\n0.2\n0.4\n0.6\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nHorse\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nHot Dog\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nKeyboard\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nKite\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nKnife\n0.0\n0.2\n0.4\n0.6\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nLaptop\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nMicrowave\n0.0\n0.2\n0.4\n0.6\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMotorcycle\n0.0\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nMouse\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nOrange\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nOven\n0.0\n0.2\n0.4\n0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nParking Meter\n0.00\n0.02\n0.04\n0.06\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nPerson\n0.0\n0.2\n0.4\n0.6\n0.8\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nPizza\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPotted Plant\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nRefrigerator\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRemote\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSandwich\n0.8\n1.0\nScissors\n0.9\n1.0\nSheep\n0 8\n1.0\nSink\n0 8\n1.0\nSkateboard\n0 8\n1.0\nSkis\n22\nFigure A9: COCO results part 3: We display the per class AP curve across 5 folds here in parts. These plots correspond to\nthe calculated AP in Tabs. A3 and A4. The solid black line is our PDC Pair. Again, we see that for each class, our PDC curve\nis comfortably above the others. Precisions and recalls remain high, indicating that we find a significant amount of ground\ntruth objects in the top retrievals.\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n0.3\n0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n0.3\n0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nMouse\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nOrange\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nOven\n0.0\n0.2\n0.4\n0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nParking Meter\n0.00\n0.02\n0.04\n0.06\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nPerson\n0.0\n0.2\n0.4\n0.6\n0.8\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nPizza\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPotted Plant\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nRefrigerator\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRemote\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSandwich\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nScissors\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSheep\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nSink\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nSkateboard\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nSkis\n0.0\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nSnowboard\n0.0\n0.1\n0.2\n0.3\n0.4\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSpoon\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nSports Ball\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nStop Sign\n0.0\n0.2\n0.4\n0.6\n0.2\n0.4\n0.6\n0.8\n1.0\nSuitcase\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nSurfboard\n0.0\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nTeddy Bear\n0.0\n0.2\n0.4\n0.6\n0.8\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTennis Racket\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.2\n0.4\n0.6\n0.8\n1.0\nTie\n0.0\n0.2\n0.4\n0.6\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nToaster\n0.0\n0.2\n0.4\n0.6\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nToilet\n0.0\n0.2\n0.4\n0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nToothbrush\n0.0\n0.2\n0.4\n0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraffic Light\n0.0\n0.2\n0.4\n0.6\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTrain\n0.0\n0.1\n0.2\n0.3\n0.4\n0.2\n0.4\n0.6\n0.8\n1.0\nTruck\n0.0\n0.2\n0.4\n0.6\nRecall\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nTv\n0.0\n0.2\n0.4\n0.6\nRecall\n0.2\n0.4\n0.6\n0.8\n1.0\nUmbrella\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRecall\n0.2\n0.4\n0.6\n0.8\n1.0\nVase\n0.0\n0.2\n0.4\n0.6\nRecall\n0.2\n0.4\n0.6\n0.8\n1.0\nWine Glass\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nZebra\n23\n"
  },
  {
    "title": "RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools",
    "link": "https://arxiv.org/pdf/2306.14447.pdf",
    "upvote": "6",
    "text": "RoboCook: Long-Horizon Elasto-Plastic Object\nManipulation with Diverse Tools\nHaochen Shi1*\nHuazhe Xu1*\u2020\nSamuel Clarke1\nYunzhu Li1,2\nJiajun Wu1\n1Stanford University\n2UIUC\n*Equal contribution\nhttps://hshi74.github.io/robocook\nAbstract: Humans excel in complex long-horizon soft body manipulation tasks\nvia flexible tool use: bread baking requires a knife to slice the dough and a\nrolling pin to flatten it. Often regarded as a hallmark of human cognition, tool\nuse in autonomous robots remains limited due to challenges in understanding\ntool-object interactions. Here we develop an intelligent robotic system, Robo-\nCook, which perceives, models, and manipulates elasto-plastic objects with vari-\nous tools. RoboCook uses point cloud scene representations, models tool-object\ninteractions with Graph Neural Networks (GNNs), and combines tool classifi-\ncation with self-supervised policy learning to devise manipulation plans.\nWe\ndemonstrate that from just 20 minutes of real-world interaction data per tool, a\ngeneral-purpose robot arm can learn complex long-horizon soft object manipula-\ntion tasks, such as making dumplings and alphabet letter cookies. Extensive evalu-\nations show that RoboCook substantially outperforms state-of-the-art approaches,\nexhibits robustness against severe external disturbances, and demonstrates adapt-\nability to different materials.\nKeywords: Deformable Object Manipulation, Long-horizon Planning, Model\nLearning, Tool Usage\n1\nIntroduction\nThink about all the steps and tools a robot would need to use to make a dumpling from a lump of\ndough. This scenario contains three fundamental research problems in robotics: deformable object\nmanipulation [1, 2, 3, 4], long-horizon planning [5, 6, 7, 8], and tool usage [9, 10, 11, 12]. The task\nposes significant challenges to the robot, because it involves decisions at both discrete (e.g., which\ntool to use) and continuous levels (e.g., motion planning conditioned on the selected tool).\nTo address these challenges, we propose RoboCook, a framework that perceives, models, and ma-\nnipulates elasto-plastic objects for long-horizon tasks like making dumplings and alphabet letter\ncookies. RoboCook introduces three technical innovations. First, we apply a data-driven approach\nwith a Graph Neural Network (GNN) [13, 14, 15] to learn highly complex interactions between the\nsoft object and various tools purely from visual observations. Second, we combine a PointNet-based\n[16, 17] tool classification module with learned dynamics models to determine the most appropriate\ntool to use at the current task stage. Third, we use a self-supervised policy trained with synthetic data\ngenerated by our learned dynamics model for gripping, rolling, and pressing to improve performance\nand speed, and hand-coded policies for other skills.\nWe carry out comprehensive evaluations to show RoboCook\u2019s effectiveness, robustness, and gener-\nalizability. Figure 1 shows a typical successful trial of making a dumpling. To showcase robustness,\nwe apply external perturbations during real-time execution, and RoboCook still succeeds in making\na dumpling. To demonstrate generalizability, we test RoboCook to make alphabet letter cookies and\nRoboCook outperforms four strong baselines by a significant margin. RoboCook can also generalize\nto various materials by making a particular shape on different materials without retraining.\n\u2020 Now at Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2306.14447v2  [cs.RO]  17 Oct 2023\n2\n3\n4\n1\n0\n5\n6\n7\n8\n10\n9\nInitial state\nKnife\nTwo-plane symmetric gripper Square press\nLarge roller\nCircle cutter\nPusher\nSkin spatula\nFilling spatula\nHook\nFinal state\nFigure 1: Making dumplings. RoboCook makes a dumpling from a piece of dough in nine steps:\nThe robot (1) cuts the dough to an appropriate volume, (2) pinches the dough and regularizes the\nshape, (3) presses to flatten the dough, (4) rolls to flatten the dough further, (5) cuts a circular\ndumpling skin, (6) removes the excess dough, (7) picks and places the skin onto the mold, (8) adds\nthe filling, and (9) closes and opens the mold. The black arrows denote the moving direction.\n2\nRelated Work\nLong-horizon deformable object manipulation. Real-world deformable object manipulation is a\nchallenging task [18, 19, 20]. The innate high DoFs, partial observability, and non-linear local in-\nteractions make deformable objects hard to represent, model, and manipulate [21, 22, 23]. Conven-\ntional approaches in deformable object manipulation choose model-based methods [24, 25, 26, 27]\nor adaptive methods [28, 29, 30], yielding complex system identification problems. For example, Li\net al. [27] proposes a differentiable simulator as the surrogate of real-world elasto-plastic objects.\nHowever, these simulators are based on approximate modeling techniques, resulting in a noticeable\nsim-to-real gap that hinders their application to real-world scenarios . There have been efforts toward\nlearning to manipulate from expert demonstrations [31, 32, 33]. This paradigm is used in manipu-\nlating liquid [11], sand [29], and dough [34]. Despite these successes, obtaining the demonstration\nis expensive and sometimes prohibitive. Another recent trend is to learn a dynamics model from\nhigh-dimensional sensory data directly for downstream manipulation tasks [18, 35, 36, 37, 15, 38].\nWhile these approaches show impressive results, they only consider short-horizon tasks using just\none tool. In contrast, long-horizon tasks like dumpling-making require a reactive planner to under-\nstand the long-term physical effect of different tools to make the most effective discrete (e.g., which\ntool to use) and continuous (e.g., the action parameters) action decisions.\nTool usage. Tool usage is widely studied in cognitive science and robotics research [39, 40]. To\nstudy the process of human evolution, many researchers endorse tool usage as a main benchmark\nto evaluate the intelligence of living primates with respect to that of extinct hominids [41, 42, 43].\nTo equip robots with the same capability of tool usage as humans, prior works focus on teaching\nthe robot the representation and semantics of tools or objects that potentially function like tools\nfor downstream policy learning [11, 12]. To perform complicated long-horizon tasks composed\nof several subtasks, prior works also use human demonstrations to learn a hierarchical policy net-\nwork [44]. In this work, we will use real-world robot random-play data, which is cheaper to acquire\nthan human demonstration data. We train the robot on these self-exploratory trials to understand the\nphysical interactions between different tools and deformable objects.\n3\nMethod\nThe RoboCook framework has three major components: perception, dynamics, and closed-loop\ncontrol. We first use an effective sampling scheme and intuitive tool representations for particle-\n2\n(a)\n(d)\n(b)\n(e)\n(c)\n(f)\nDough\nTool\nOutliers\nB. Dynamics\n\ud835\udc46!\n\ud835\udc46\"\n\ud835\udc46#\n\ud835\udc46$\n\ud835\udc46%\n\ud835\udc46&\n\ud835\udc61\nGNN\n\ud835\udc4e!\nGNN\n\ud835\udc4e\"\nGNN\n\ud835\udc4e#\nGNN\n\ud835\udc4e$\nGNN\n\ud835\udc4e%\nInitial State\nPRED\nGT\nGripping\nTop\nPRED\nGT\nPressing\nSide\nPRED\nGT\nRolling\nPers.\nTop\nTop\nPers.\nSide\nPers.\nSide\nA. Perception\nRaw point cloud\nFigure 2: Perception and dynamics of RoboCook. (A) The input to the perception module is a\npoint cloud of the robot\u2019s workspace captured by four RGB-D cameras. From the raw point cloud,\nwe (a) crop the region of interest, (b) extract the dough point cloud, (c) reconstruct a watertight\nmesh (d) use the Signed Distance Function (SDF) to sample points inside the mesh, (e) remove\npoints within the tools\u2019 SDF, and (f) sample 300 surface points. (B) We process videos of each tool\nmanipulating the dough into a particle trajectory dataset to train our GNN-based dynamics model.\nThe model can accurately predict long-horizon state changes of the dough in gripping, pressing, and\nrolling tasks. On the left are the initial states\u2019 perspective, top, and side views, and on the right is a\ncomparison of model predictions and ground truth states.\nbased scene representation. Second, we train Graph Neural Networks (GNNs) as the dynamics\nmodel from the processed video dataset to accurately predict dough states during manipulation. Last,\na tool classifier selects the best tool for each substage in a long-horizon task and a self-supervised\npolicy network performs closed-loop control.\n3.1\nPerception\nThe perception module aims to sample particles sparsely and uniformly for the downstream dy-\nnamics model. This task is challenging because of dough occlusions from the robot and tool and\nself-occlusions from the irregular and concave shape of the dough.\nWe merge point clouds from four calibrated RGB-D cameras and perform color segmentation to\nextract the dough point cloud. Then we apply either Poisson surface reconstruction [45] or alpha-\nshape surface reconstruction [46] to reconstruct a watertight surface of the dough, depending on\nocclusion levels. In heavy occlusion cases, alpha-shape surface reconstruction is usually worse at\ncapturing concavities than Poisson surface reconstruction. However, we use it to secure a complete\nand singular mesh. With few or no occlusions, we combine Poisson surface reconstruction and\nthe MeshFix algorithm [47] to generate a watertight mesh. We use the watertight mesh\u2019s Signed\nDistance Function (SDF) to sample points inside it randomly. To compensate for details lost during\nsurface reconstruction, we apply a voxel-grid filter to reconstruct concavities by removing the points\nabove the original dough point cloud. We also remove the noisy points penetrating the tool using the\nmesh\u2019s SDF. We compute the tool mesh\u2019s SDF from the robot\u2019s end-effector pose (recorded during\ndata collection) and the ground-truth tool mesh. We then perform alpha-shape surface reconstruction\nand use Poisson disk sampling [48] to sample 300 points uniformly on the surface, capturing more\ndetails of the dough with a fixed particle number.\nFor different tools, we uniformly sample particles on the surface of their ground truth mesh to reflect\ntheir geometric features. The complete scene representation for the downstream dynamics model\n3\nconcatenates dough and tool particles. Section 6.2.1 and 6.2.2 of supplementary materials describe\nmore data collection and preprocessing details.\n3.2\nDynamics Model\nOur GNN-based dynamics model predicts future states of dough based on the current state and\nactions of the tools with only 20 minutes of real-world data per tool. The rigid and non-rigid motions\nare each predicted by a multi-layer perceptron (MLP) and added together as the predicted motion by\nthe GNN.\nThe graph of sampled particles at each time step is represented as st = (Ot, Et) with Ot as vertices,\nand Et as edges. For each particle, oi,t =\n\u0000xi,t, co\ni,t\n\u0001\n, where xi,t is the particle position i at time t,\nand co\ni,t is the particle\u2019s attributes at time t, including the particle normal and group information (i.e.,\nbelongs to the dough or the tool). The edge between a pair of particles is denoted as ek = (uk, vk),\nwhere 1 \u2264 uk, vk \u2264 |Ot| are the receiver particle index and sender particle index respectively, and k\nis the edge index. Section 6.3.1 of supplementary materials provides more details on graph building.\nFollowing the previous work [18], we use a weighted function of Chamfer Distance (CD) [49] and\nEarth Mover\u2019s Distance (EMD) [50] as the loss function to train the dynamics model:\nL(Ot, b\nOt) = w1 \u00b7 LCD(Ot, b\nOt) + w2 \u00b7 LEMD(Ot, b\nOt),\n(1)\nwhere Ot is the real observation, and b\nOt is the predicted state. We select w1 = 0.5 and w2 = 0.5\nbased on the experiment results. To be more specific, the CD between Ot, b\nOt \u2286 R3 is calculated by\nLCD(Ot, b\nOt) =\nX\nx\u2208Ot\nmin\ny\u2208 b\nOt\n\u2225x \u2212 y\u22252\n2 +\nX\ny\u2208 b\nOt\nmin\nx\u2208Ot \u2225x \u2212 y\u22252\n2.\n(2)\nThe EMD matches distributions of point clouds by finding a bijection \u00b5 : Ot \u2192 b\nOt such that\nLEMD(Ot, b\nOt) =\nmin\n\u00b5:Ot\u2192 b\nOt\nX\nx\u2208Ot\n\u2225x \u2212 \u00b5(x)\u22252.\n(3)\nWe train the model to predict multiple time steps forward to regularize training and stabilize long-\nhorizon future predictions. The training loss is calculated as a sum of distances between predictions\nand ground-truth states:\nLtrain =\ns\nX\ni=0\nL(Ot+i, b\nOt+i),\n(4)\nwhere the dynamics model takes b\nOt+i\u22121 as the input to predict b\nOt+i when i > 0. The model\nperforms better in inference time by predicting a slightly longer horizon during training time. Em-\npirically, we discover that s = 2 is good enough for inference-time prediction accuracy, and a larger\ns does not give us much gain on the accuracy. Note that although we use s = 2 during training, we\npredict 15 steps during inference time, which is a long-horizon prediction. This highlights the in-\nductive bias of our GNN-based dynamics model - we only need to train on short-horizon predictions\nto generalize to a much longer-horizon prediction during inference. Another motivation to keep s\nsmall is to increase the training speed. More implementation details on model training can be found\nin Section 6.3.2 of supplementary materials.\n3.3\nClosed-Loop Control\nTo solve long-horizon manipulation tasks involving multiple tools, the robot must determine (1) the\nbest tool to use at each stage and (2) the optimal trajectory for the selected tool.\nTo answer the first question, we implement a tool classification module based on PointNet++ [17]\nthat takes a concatenation of the input and output states of the dough as input and outputs probabil-\nities of the 15 tools achieving the target state. We extract the features of the input and output point\nclouds separately with a PointNet++ classification layer. Then we feed the concatenated features to\nan MLP with a SoftMax activation layer to output probabilities.\n4\nA. Closed-Loop Control\nB. Self-supervised policy learning\nGNN\nPolicy \nNetwork\n\ud835\udc46!\n\ud835\udc46\"\nTool\nRandomly \nsampled action \ud835\udc4e\n\u2112(%\ud835\udc4e, \ud835\udc4e)\n%\ud835\udc4e\ngradient\nClassifer \nNetwork\nUpdated\nPolicy \nNetwork\n\u2026\nLarge roller\nCircle cutter\nKnife\nCircle press\nCircle punch\nHook\nCurrent\nTarget\nVisual feedback loop\nActions\nPress and roll\nGripping\n\ud835\udf03\n\ud835\udc5f\n\ud835\udc51\n\ud835\udc46!\n\ud835\udc46\"\n\ud835\udc4e = (\ud835\udc5f, \ud835\udf03, \ud835\udc51)\n+\nGNN\nPressing\n\ud835\udc4e = (\ud835\udc65, \ud835\udc66, \ud835\udc67, \ud835\udf03)\n+\n\ud835\udc67\n\ud835\udc65\n\ud835\udc66\n\ud835\udf03\n\ud835\udc46!\n\ud835\udc46\"\nGNN\nRolling\n\ud835\udc67\n\ud835\udc65\n\ud835\udc66\n\ud835\udf03\nAction space\n\ud835\udc46!\n\ud835\udc46\"\n\ud835\udc4e = (\ud835\udc65, \ud835\udc66, \ud835\udc67, \ud835\udf03)\n+\nGNN\nAction space\nAction space\nFigure 3: Planning of RoboCook. (A) PointNet-based classifier network identifies appropriate tools\nbased on the current observation and the target dough configuration. The self-supervised policy\nnetwork takes the tool class, the current observation, and the target dough configuration as inputs\nand outputs the manipulation actions. The framework closes the control loop with visual feedback.\n(B) We show the policy network architecture, the parametrized action spaces of gripping, pressing,\nand rolling, and how we generate the synthetic datasets to train the policy network.\nThe three tools with the highest probabilities are selected as candidates, and their optimal actions\nare planned. The robot selects the tool yielding the final state closest to the target and executes the\nplanned actions. We let the module output three tool candidates instead of only one to increase our\npipeline\u2019s error tolerance to the tool classifier.\nTo address the second question, we specify and parameterize the action space of the 15 tools based\non human priors. Then we classify the tools into a few categories based on |A|, the dimension of their\naction space. Section 6.4.1 of supplementary materials provides more details on the specification\nand justification of tool action spaces.\nFor grippers, rollers, presses, and punches, we collect data in the real world by random exploration\nin the bounded action space and learn their interaction model with the dough using a GNN. Then\nwe generate a synthetic dataset with the dynamics model, train a self-supervised policy network to\nobtain the optimal policy, and execute closed-loop control with visual feedback. We use straightfor-\nward planners for other tools based on human priors. More implementation details can be found in\nSection 6.4.1 of supplementary materials.\nTo generate the synthetic dataset, we reuse the initial dough states acquired during data collection\nand randomly sample actions in the parameterized action space of each tool. As shown in Fig-\nure 3(A), the dough states before and after applying actions are the input, and the action parameters\nare the labels. We design a self-supervised policy network based on PointNet++. Given the continu-\nous solution space of the action parameters, we formulate a multi-bin classification problem inspired\nby previous works on 3D bounding box estimation [51, 52]. First, we concatenate input and output\npoint clouds, labeling them 0 or 1 for distinction. Next, we extract features using a PointNet++\nlayer. Finally, we feed the feature vector into separate classification and regression heads, both be-\ning MLPs with identical structures. The learned policy drastically improves the planning efficiency,\nand the complete pipeline only takes around 10 seconds of planning time to make a dumpling. More\nimplementation details are described in Section 6.4.2 of supplementary materials.\nDuring closed-loop control, cameras capture the workspace point cloud, which the perception mod-\nule processes to obtain a clean and sparse point cloud of the dough. Our method takes the current\nobservation and final target as input and returns the best tool and optimal actions, as shown in Fig-\nure 3(B). Then the robot picks up the selected tool, manipulates the dough, lifts its hand to avoid\noccluding the camera views, and plans the next move with visual feedback.\n5\n1\n6\n2\n3\n5\n8\n4\n9\n11\n14\n15\n13\n12\n10\n7\nz\nx\ny\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nFigure 4: RoboCook hardware and setup. Left: Robot\u2019s tabletop workspace with xyz coordinates\nat top-left. Dashed white circles: four RGB-D cameras mounted at four corners of the table. Red\nsquare: dough location and manipulation area. Dashed white square: tool racks. Right: 15 tools:\n(1) large roller, (2) circle press, (3) circle punch, (4) square press, (5) square punch, (6) small\nroller, (7) knife/pusher, (8) circle cutter, (9) hook, (10) dumpling mold, (11) two-rod symmetric\ngripper, (12) asymmetric gripper, (13) two-plane symmetric gripper, (14) skin spatula, (15) filling\nspatula. Tools are 3D-printed, representing common dough manipulation tools. Section 6.5.2 of\nsupplementary materials discusses the design principles of these 3D-printed tools.\n4\nExperiments\nIn this study, we design and implement a hardware setup for soft body manipulation tasks, allowing\neasy selection and interchange of 15 tools, as shown in Figure 4. We demonstrate the RoboCook\nframework\u2019s effectiveness in long-horizon soft object manipulation tasks that require the use of mul-\ntiple tools, such as making a dumpling from a randomly shaped dough and shaping alphabet letter\ncookies to compose the word \u2018RoboCook.\u2019 Additionally, RoboCook can complete these tasks under\nextensive human interference, including significant changes to the shape and volume, as shown in\nthe second supplementary video, demonstrating its robustness to external perturbations. We discuss\nthe experiment setup in Section 6.5 of supplementary materials.\n4.1\nMaking Dumplings\nThe dumpling-making task is to manipulate a piece of dough and the given filling into a dumpling\nshape. The main challenge lies in choosing appropriate tools and planning effective action trajecto-\nries. We consider the task successful if the dumpling skin is thin enough and completely covers the\nfilling. Dumpling-making is a highly challenging task\u2014even a single error might break the entire\npipeline. Our method reliably accomplishes the task as shown in Figure 1. We show a comparison\nwith manipulation results of human subjects in Section 6.6 of supplementary materials.\nRoboCook also demonstrates robustness against external perturbations during real-time execution.\nAt each stage of dumpling making, a human disturbs the robot by deforming the dough, adding\nadditional dough, or even replacing the dough with a completely different piece to deviate from the\ntrajectory of the robot\u2019s original plan. For example, at the rolling stage, the human folds the flattened\ndough into a bulky shape. The robot decides to roll again to get a flatter dumpling skin. In a more\nchallenging case, the human replaces the round dumpling skin with a completely different dough in\na highly irregular shape. After this perturbation, the robot puts down the roller, picks up the knife to\nstart again from the beginning, and successfully makes a dumpling. The complete process is shown\nin the second supplementary video. We also show a quantitative evaluation of the tool classification\nnetwork in Section 6.7 of supplementary materials.\n6\nR\nO\nB\nC\nK\nRoboCraft\nInitial state\nCEM+GNN\nStep 1\nStep 2\nOurs\nOutline\nCEM+MPM\n\ud835\udc61\nRL+GNN\nFigure 5: Making alphabetical letter cookies. We list R, O, B, C, and K shaping steps in Columns\n1 to 4. Column 5 manually highlights the contour of the alphabetical letters. Columns 6 through 9\ncompare our self-supervised learned policy with three model-based planning baselines and one RL\nbaseline. Our method can shape the dough closer to the target than all four baseline methods.\nMethods\nCD \u2193\nEMD \u2193\nCD of normal \u2193\nHuman evaluation \u2191\nPlanning time (s) \u2193\nOurs\n0.0062 \u00b1 0.0007\n0.0042 \u00b1 0.0006\n0.1933 \u00b1 0.0345\n0.90 \u00b1 0.11\n9.3 \u00b1 1.5\nRoboCraft\n0.0066 \u00b1 0.0005\n0.0044 \u00b1 0.0006\n0.2011 \u00b1 0.0329\n0.54 \u00b1 0.43\n613.7 \u00b1 202.7\nCEM+GNN\n0.0066 \u00b1 0.0007\n0.0045 \u00b1 0.0008\n0.2043 \u00b1 0.0431\n0.52 \u00b1 0.41\n756.0 \u00b1 234.5\nCEM+MPM\n0.0070 \u00b1 0.0007\n0.0046 \u00b1 0.0006\n0.1965 \u00b1 0.0265\n0.48 \u00b1 0.35\n1486.7 \u00b1 512.8\nRL+GNN\n0.0077 \u00b1 0.0007\n0.0064 \u00b1 0.0009\n0.2041 \u00b1 0.0414\n0.17 \u00b1 0.09\n1867.9 \u00b1 190.3\nTable 1: Quantitative evaluations. We use CD and EMD between the point clouds and the CD\nbetween the surface normals to evaluate the results. We further profile how long these methods take\nto plan actions. Our method outperforms all baseline methods in these metrics by a large margin.\n4.2\nMaking Alphabet Letter Cookies\nThe RoboCook framework demonstrates effectiveness and robustness in highly complicated ma-\nnipulation tasks such as dumpling-making. This section explores its generalization ability in tasks\nrequiring precise actions such as shaping alphabet letter cookies [18] without additional training.\nFigure 5 shows that the RoboCook framework can accurately shape letters R, O, B, C, and K to\ncompose the word \u2018RoboCook.\u2019 For R, the robot uses the two-rod symmetric gripper for cavities\nand the circle punch for the hole. For O, the tool classifier selects a two-plane symmetric gripper to\npinch the square into a circle and the circle punch for the hole. Our method identifies the asymmetric\ngripper as suitable for B and locates accurate positions for the circle punch to press twice. Shaping\nC is more challenging due to the large distance between the initial and target shapes, but our method\nsuccessfully plans gripping positions using closed-loop visual feedback. The robot combines the\ntwo-rod symmetric and asymmetric gripper for K to create cavities.\nWe compare our method with four strong baselines: (1) limited-memory BFGS [53] with GNN-\nbased dynamics model (RoboCraft) [18]; (2) cross-entropy method (CEM) [54] with GNN-based\ndynamics model; (3) CEM with a Material Point Method (MPM) simulator [55]; and (4) Reinforce-\nment Learning with GNN-based dynamics model. Qualitative results are shown in Figure 5. We\ninclude a detailed analysis of our results in Section 6.10.\n7\nInitial state\nManipulation Steps\nFinal State (K)\n\ud835\udc61\nMaterial\nReal Dough\nFlour + water + salt\nModel Foam\nAir Dry Clay\nPlay-Doh\nFigure 6: Generalizing to different materials. We showcase the dynamics model\u2019s capability to\ngeneralize to various materials by shaping a K without retraining.\nTable 1 evaluates the results using Chamfer Distance (CD) [49], Earth Mover\u2019s Distance (EMD) [50]\nand CD between top surface normals. However, we recognize a discrepancy between how these\nmetrics measure the results and how humans perceive them - these metrics are prone to local noises\nwhile humans are good at capturing the holistic features of the dough. Therefore, we show the pre-\ndiction accuracy of 100 human subjects on recognizing the letters for these five methods in Table 1.\nAnother highlight of our method is its speed. Since the policy network only needs one forward\nprediction to output the actions for each tool, it is significantly faster than other methods that rely\non forwarding the dynamics models many times. Section 6.8 and 6.9 of supplementary materials\ndiscuss more details about the human study and the baseline implementations.\nWe show that the dynamics model can generalize to various materials by shaping a K without re-\ntraining in Figure 6. We test on Play-Doh, Air Dry Clay, and Model Foam, each displaying notably\ndifferent dynamics compared to our original dough made of flour, water, and salt. The dynamics\nmodel is trained solely on interaction data with the real dough.\n5\nConclusion and Limitations\nRoboCook demonstrates its effectiveness, robustness, and generalizability in elasto-plastic object\nmanipulation with a general-purpose robotic arm and everyday tools. The main contributions of\nRoboCook include (1) tool-aware GNNs to model long-horizon soft body dynamics accurately\nand efficiently, (2) a tool selection module combined with dynamics models to learn tool functions\nthrough self-exploratory trials, and (3) a self-supervised policy learning framework to improve the\nperformance and speed significantly. RoboCook pioneers solutions for tool usage and long-horizon\nelasto-plastic object manipulation in building a generic cooking robot.\nOne limitation of RoboCook is the occasional failure of dough sticking to the tool. A solution is to\ndesign an automatic error correction system. RoboCook also relies on human priors of tool action\nspaces to simplify planning. But these simplifications do not constrain generalization as they can be\neasily specified for new tools. Section 6.4.1 provides more justifications for this. Another limitation\nis that humans define the subgoals. Higher-level temporal abstraction and task-level planning are\nrequired to get rid of them. Finally, RoboCook requires additional topology estimation to apply to\ncables and cloths [56], which is beyond the focus of this work.\n8\nAcknowledgments\nThis work was in part supported by AFOSR YIP FA9550-23-1-0127, ONR MURI N00014-22-1-\n2740, the Toyota Research Institute (TRI), the Stanford Institute for Human-Centered AI (HAI),\nJPMC, and Analog Devices.\nReferences\n[1] J. Matas, S. James, and A. J. Davison. Sim-to-real reinforcement learning for deformable\nobject manipulation. In Conference on Robot Learning, pages 734\u2013743. PMLR, 2018.\n[2] X. Lin, Y. Wang, J. Olkin, and D. Held. Softgym: Benchmarking deep reinforcement learning\nfor deformable object manipulation. In Conference on Robot Learning, pages 432\u2013448. PMLR,\n2021.\n[3] J. Zhu, A. Cherubini, C. Dune, D. Navarro-Alarcon, F. Alambeigi, D. Berenson, F. Ficuciello,\nK. Harada, J. Kober, X. Li, et al. Challenges and outlook in robotic manipulation of deformable\nobjects. IEEE Robotics & Automation Magazine, 29(3):67\u201377, 2022.\n[4] H. Yin, A. Varava, and D. Kragic. Modeling, learning, perception, and control methods for\ndeformable object manipulation. Science Robotics, 6(54):eabd8803, 2021.\n[5] V. N. Hartmann, A. Orthey, D. Driess, O. S. Oguz, and M. Toussaint. Long-horizon multi-robot\nrearrangement planning for construction assembly. IEEE Transactions on Robotics, 2022.\n[6] S. Nair and C. Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via\nvisual subgoal generation. In International Conference on Learning Representations.\n[7] S. Pirk, K. Hausman, A. Toshev, and M. Khansari. Modeling long-horizon tasks as sequential\ninteraction landscapes. In Conference on Robot Learning, pages 471\u2013484. PMLR, 2021.\n[8] A. Simeonov, Y. Du, B. Kim, F. Hogan, J. Tenenbaum, P. Agrawal, and A. Rodriguez. A\nlong horizon planning framework for manipulating rigid pointcloud objects. In Conference on\nRobot Learning, pages 1582\u20131601. PMLR, 2021.\n[9] A. Billard and D. Kragic. Trends and challenges in robot manipulation. Science, 364(6446):\neaat8414, 2019.\n[10] N. Yamanobe, W. Wan, I. G. Ramirez-Alpizar, D. Petit, T. Tsuji, S. Akizuki, M. Hashimoto,\nK. Nagata, and K. Harada. A brief review of affordance in robotic manipulation research.\nAdvanced Robotics, 31(19-20):1086\u20131101, 2017.\n[11] D. Seita, Y. Wang, S. J. Shetty, E. Y. Li, Z. Erickson, and D. Held. Toolflownet: Robotic\nmanipulation with tools via predicting tool flow from point clouds. In 6th Annual Conference\non Robot Learning, 2022.\n[12] A. Xie, F. Ebert, S. Levine, and C. Finn. Improvisation through physical understanding: Using\nnovel objects as tools with visual foresight. In Proceedings of Robotics: Science and Systems,\nFreiburgimBreisgau, Germany, June 2019. doi:10.15607/RSS.2019.XV.001.\n[13] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural\nnetwork model. IEEE transactions on neural networks, 20(1):61\u201380, 2008.\n[14] Y. Li, J. Wu, R. Tedrake, J. B. Tenenbaum, and A. Torralba. Learning particle dynamics for\nmanipulating rigid bodies, deformable objects, and fluids. In International Conference on\nLearning Representations, 2018.\n[15] Y. Li, J. Wu, J.-Y. Zhu, J. B. Tenenbaum, A. Torralba, and R. Tedrake. Propagation networks for\nmodel-based control under partial observation. In 2019 International Conference on Robotics\nand Automation (ICRA), pages 1205\u20131211. IEEE, 2019.\n9\n[16] C. R. Qi, H. Su, K. Mo, and L. J. Guibas.\nPointnet: Deep learning on point sets for 3d\nclassification and segmentation. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 652\u2013660, 2017.\n[17] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. Advances in neural information processing systems, 30, 2017.\n[18] H. Shi, H. Xu, Z. Huang, Y. Li, and J. Wu. RoboCraft: Learning to See, Simulate, and Shape\nElasto-Plastic Objects with Graph Networks. In Proceedings of Robotics: Science and Systems,\nNew York City, NY, USA, June 2022. doi:10.15607/RSS.2022.XVIII.008.\n[19] C. Matl and R. Bajcsy. Deformable elasto-plastic object shaping using an elastic hand and\nmodel-based reinforcement learning. In 2021 IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS), pages 3955\u20133962. IEEE, 2021.\n[20] X. Lin, C. Qi, Y. Zhang, Z. Huang, K. Fragkiadaki, Y. Li, C. Gan, and D. Held. Planning\nwith spatial-temporal abstraction from point clouds for deformable object manipulation. In 6th\nAnnual Conference on Robot Learning, 2022.\n[21] D. Driess, Z. Huang, Y. Li, R. Tedrake, and M. Toussaint. Learning multi-object dynamics\nwith compositional neural radiance fields. In Conference on Robot Learning, pages 1755\u2013\n1768. PMLR, 2023.\n[22] C. Chi, B. Burchfiel, E. Cousineau, S. Feng, and S. Song. Iterative Residual Policy for Goal-\nConditioned Dynamic Manipulation of Deformable Objects. In Proceedings of Robotics: Sci-\nence and Systems, New York City, NY, USA, June 2022. doi:10.15607/RSS.2022.XVIII.016.\n[23] H. Ha and S. Song. Flingbot: The unreasonable effectiveness of dynamic manipulation for\ncloth unfolding. In Conference on Robot Learning, pages 24\u201333. PMLR, 2022.\n[24] Z. Huang, Y. Hu, T. Du, S. Zhou, H. Su, J. B. Tenenbaum, and C. Gan. Plasticinelab: A\nsoft-body manipulation benchmark with differentiable physics. In International Conference\non Learning Representations, 2020.\n[25] X. Lin, Z. Huang, Y. Li, J. B. Tenenbaum, D. Held, and C. Gan. Diffskill: Skill abstraction\nfrom differentiable physics for deformable object manipulations with tools. In International\nConference on Learning Representations, 2021.\n[26] A.-M. Cretu, P. Payeur, and E. M. Petriu. Soft object deformation monitoring and learning for\nmodel-based robotic hand manipulation. IEEE Transactions on Systems, Man, and Cybernet-\nics, Part B (Cybernetics), 42(3):740\u2013753, 2011.\n[27] S. Li, Z. Huang, T. Du, H. Su, J. B. Tenenbaum, and C. Gan. Contact points discovery for\nsoft-body manipulations with differentiable physics. In International Conference on Learning\nRepresentations, 2021.\n[28] D. Navarro-Alarcon, H. M. Yip, Z. Wang, Y.-H. Liu, F. Zhong, T. Zhang, and P. Li. Automatic\n3-d manipulation of soft objects by robotic arms with an adaptive deformation model. IEEE\nTransactions on Robotics, 32(2):429\u2013441, 2016.\n[29] A. Cherubini, V. Ortenzi, A. Cosgun, R. Lee, and P. Corke. Model-free vision-based shaping\nof deformable plastic materials. The International Journal of Robotics Research, 39(14):1739\u2013\n1759, 2020.\n[30] K. Yoshimoto, M. Higashimori, K. Tadakuma, and M. Kaneko. Active outline shaping of a\nrheological object based on plastic deformation distribution. In 2011 IEEE/RSJ International\nConference on Intelligent Robots and Systems, pages 1386\u20131391. IEEE, 2011.\n10\n[31] B. Balaguer and S. Carpin. Combining imitation and reinforcement learning to fold deformable\nplanar objects. In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems,\npages 1405\u20131412. IEEE, 2011.\n[32] F. Nadon, A. J. Valencia, and P. Payeur. Multi-modal sensing and robotic manipulation of\nnon-rigid objects: A survey. Robotics, 7(4):74, 2018.\n[33] B. Jia, Z. Pan, Z. Hu, J. Pan, and D. Manocha. Cloth manipulation using random-forest-based\nimitation learning. IEEE Robotics and Automation Letters, 4(2):2086\u20132093, 2019.\n[34] N. Figueroa, A. L. P. Ureche, and A. Billard. Learning complex sequential tasks from demon-\nstration: A pizza dough rolling case study. In 2016 11th ACM/IEEE International Conference\non Human-Robot Interaction (HRI), pages 611\u2013612. Ieee, 2016.\n[35] P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende, et al. Interaction networks for learning\nabout objects, relations and physics. Advances in neural information processing systems, 29,\n2016.\n[36] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based\napproach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016.\n[37] A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell, and\nP. Battaglia. Graph networks as learnable physics engines for inference and control. In Inter-\nnational Conference on Machine Learning, pages 4470\u20134479. PMLR, 2018.\n[38] T. Silver, R. Chitnis, A. Curtis, J. B. Tenenbaum, T. Lozano-P\u00b4erez, and L. P. Kaelbling. Plan-\nning with learned object importance in large problem instances using graph neural networks. In\nProceedings of the AAAI conference on artificial intelligence, volume 35, pages 11962\u201311971,\n2021.\n[39] C. P. Van Schaik and G. R. Pradhan. A model for tool-use traditions in primates: implications\nfor the coevolution of culture and cognition. Journal of Human Evolution, 44(6):645\u2013664,\n2003.\n[40] R. Holladay, T. Lozano-P\u00b4erez, and A. Rodriguez.\nForce-and-motion constrained planning\nfor tool use. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pages 7409\u20137416. IEEE, 2019.\n[41] S. T. Parker and K. R. Gibson. Object manipulation, tool use and sensorimotor intelligence\nas feeding adaptations in cebus monkeys and great apes. Journal of Human Evolution, 6(7):\n623\u2013641, 1977.\n[42] T. Ingold. Tool-use, sociality and intelligence. Tools, language and cognition in human evolu-\ntion, 429(45):449\u201372, 1993.\n[43] T. Matsuzawa. Primate foundations of human intelligence: a view of tool use in nonhuman\nprimates and fossil hominids. In Primate origins of human cognition and behavior, pages\n3\u201325. Springer, 2008.\n[44] J. Liang, B. Wen, K. Bekris, and A. Boularias. Learning sensorimotor primitives of sequential\nmanipulation tasks from visual demonstrations. In 2022 International Conference on Robotics\nand Automation (ICRA), pages 8591\u20138597. IEEE, 2022.\n[45] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proceedings of the\nfourth Eurographics symposium on Geometry processing, volume 7, 2006.\n[46] H. Edelsbrunner and E. P. M\u00a8ucke. Three-dimensional alpha shapes. ACM Transactions on\nGraphics (TOG), 13(1):43\u201372, 1994.\n11\n[47] M. Attene. A lightweight approach to repairing digitized polygon meshes. The visual com-\nputer, 26(11):1393\u20131406, 2010.\n[48] C. Yuksel. Sample elimination for generating poisson disk sample sets. Computer Graphics\nForum, 34(2):25\u201332, 2015.\n[49] H. Fan, H. Su, and L. J. Guibas. A point set generation network for 3d object reconstruction\nfrom a single image. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 605\u2013613, 2017.\n[50] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover\u2019s distance as a metric for image\nretrieval. International journal of computer vision, 40(2):99\u2013121, 2000.\n[51] A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka. 3d bounding box estimation using deep\nlearning and geometry. In Proceedings of the IEEE conference on Computer Vision and Pattern\nRecognition, pages 7074\u20137082, 2017.\n[52] A. Kundu, Y. Li, and J. M. Rehg. 3d-rcnn: Instance-level 3d object reconstruction via render-\nand-compare. In Proceedings of the IEEE conference on computer vision and pattern recogni-\ntion, pages 3559\u20133568, 2018.\n[53] R. Fletcher. Practical methods of optimization. John Wiley & Sons, 2013.\n[54] R. Y. Rubinstein and D. P. Kroese. The cross-entropy method: a unified approach to combi-\nnatorial optimization, Monte-Carlo simulation, and machine learning, volume 133. Springer,\n2004.\n[55] S. G. Bardenhagen and E. M. Kober. The generalized interpolation material point method.\nComputer Modeling in Engineering and Sciences, 5(6):477\u2013496, 2004.\n[56] Z. Huang, X. Lin, and D. Held. Mesh-based Dynamics with Occlusion Reasoning for Cloth\nManipulation. In Proceedings of Robotics: Science and Systems, New York City, NY, USA,\nJune 2022. doi:10.15607/RSS.2022.XVIII.011.\n12\n6\nSupplementary Materials\n6.1\nInputs and Outputs of RoboCook\u2019s Respective Modules . . . . . . . . . . . . . . .\n13\n6.2\nPerception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.2.1\nData Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.2.2\nData Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.3\nDynamics Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.3.1\nGraph Building . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.3.2\nModel Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.3.3\nBuilding Synthetic Datasets\n. . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.4\nClosed-loop Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n6.4.1\nAction Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n6.4.2\nMulti-bin Classification\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n6.4.3\nSubgoal Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n6.5\nExperiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n6.5.1\nRobot and Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n6.5.2\nTool Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n6.5.3\nTool-Switching Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n6.6\nComparison with Human Subjects on Dumpling-making\n. . . . . . . . . . . . . .\n17\n6.7\nTool Classification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n6.8\nHuman Evaluation of Alphabet Letters . . . . . . . . . . . . . . . . . . . . . . . .\n19\n6.9\nBaseline Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n6.10 Analysis of Comparison with Baselines\n. . . . . . . . . . . . . . . . . . . . . . .\n19\n6.1\nInputs and Outputs of RoboCook\u2019s Respective Modules\nPerception module: At each time step, the input is a raw observation point cloud merged from four\nRGBD cameras and the franka panda end-effector pose. The output is a clean and sparse surface\npoint cloud representing the dough concatenated with a point cloud sampled on the tool mesh surface\nrepresenting the action. We compute the pose of the tool point cloud from the end effector pose and\nthe tool geometry.\nGNN dynamics: Our GNN predicts St+1 from St and at+1 at each prediction step. The inference-\ntime input is the initial state S0 and {at, t = 1, 2, . . . N}. The output is SN. We select N = 15 for\nthe gripping, pressing, and rolling tasks.\nTool predictor: The input is a point cloud of the current dough configuration and a point cloud of\nthe final target (e.g., the dumpling point cloud). The output is the next tool to achieve the final target\nconfiguration.\nPolicy network: The input is a point cloud of the current dough configuration and the point cloud\nof the subgoal associated with the selected tool. We have a one-to-one mapping between subgoals\nand tools. For example, a rolling pin corresponds to a dough flattened by the rolling pin. The output\nis the action in the defined action space of the selected tool.\n13\n6.2\nPerception\n6.2.1\nData Collection\nTo train the GNNs, we collect around 20 minutes of data for each tool using a behavior policy that\nrandomly samples parameters within a predefined action space. In each episode, the robot applies\nmultiple action sequences to the soft object, and after the current episode, a human manually resets\nthe environment. Humans reshape the dough into a bulky but not necessarily cubic shape after every\nfive grips, three rolls, and three presses.\nThe data collected for each tool are as follows: (1) Asymmetric gripper / two-rod symmetric\ngripper / two-plane symmetric gripper: 60 episodes with five sequences per episode; (2) Circle\npress / square press / circle punch / square punch: 90 episodes with three sequences per episode; (3)\nLarge roller / small roller: 80 episodes with three sequences per episode.\nWe collect point clouds before and after executing each sequence to train the tool classifier. How-\never, we augment the training data by including any pair of these point clouds, not just consecutive\npairs. For tools that don\u2019t require a GNN-based dynamics model, we execute a pre-coded dumpling-\nmaking pipeline ten times and add point clouds captured before and after using each tool in the\npipeline to our dataset. Note that most tool selection data is a direct reuse of the dynamics data\ncollected during the training of the dynamics model. This approach efficiently collects real-world\ntool selection data without needing extra exploration. We record the data collection process in the\nfourth supplementary video.\n6.2.2\nData Preprocessing\nWhen building the dataset to train the dynamics model, aside from the sampling process of the\nperception module at each time frame, we also want to leverage the continuity of the video data.\nTherefore, we introduce simple geometric heuristics into the physical environment for better frame\nconsistency. First, if the operating tool is not in contact with the convex hull of the object point\ncloud, we use the same sampled particles from the previous frame. This also applies when the tool\nmoves away from the object. Additionally, we subsample the original videos to ensure that each\nvideo in the dataset has the same number of frames (16 frames in practice).\n6.3\nDynamics Model\n6.3.1\nGraph Building\nWhen building the graph, the edges between the dough particles are constructed by finding the\nnearest neighbors of each particle within a fixed radius (in practice, 0.1 cm). The edges between\nthe tool and dough particles are computed slightly differently. Instead of simply connecting to all\nthe neighbors within the threshold, we limit the number of undirected edges between tool particles\nand the dough particles to at most four per tool particle to cut off the redundant edges in the graph\nneural network. Since all the node and edge features in the GNN are encoded in each particle\u2019s local\nneighborhood, our GNN is naturally translation-invariant and therefore can accurately predict the\nmovement of the dough regardless of its absolute location in the world frame.\n6.3.2\nModel Training\nWe train the model with temporal abstraction to enhance performance and inference speed. For\nexample, when t = 0, we train the model to predict the state of the dough at t = 3 directly instead\nof t = 1. This shortens the horizon, eases the task, and improves our model\u2019s inference speed by\ndecreasing the number of forward passes needed for a full action sequence.\n6.3.3\nBuilding Synthetic Datasets\nSince the synthetic dataset generated by the dynamics model is cheap, we can collect as much\nsynthetic data as desired. Empirically, for example, we sample 128 random actions in the action\n14\nspace of gripping for each of the 300 different initial states, so there are 128\u2217300 = 38, 400 pairs of\ninput and output point clouds to train the policy network. The random walk to generate the synthetic\ndataset starts from initial dough states acquired during dynamics data collection, which are not all\nblock-shaped states and cover various shapes.\n6.4\nClosed-loop Control\n6.4.1\nAction Space\nWe classify the tools into a few categories based on |A|, the dimension of their corresponding action\nspace. We visualize action spaces for gripping, pressing, and rolling in Figure 3 (B).\nA) Nine tools that have an action space with |A| \u2265 3:\n1) Asymmetric gripper / two-rod symmetric gripper / two-plane symmetric gripper:\n{r, \u03b8, d}, where r is the distance between the midpoint of the line segment connecting\nthe centers of mass of the gripper\u2019s two fingers and the center of the target object, \u03b8\nis the robot gripper\u2019s rotation about the (vertical) axis, and d is the minimal distance\nbetween the gripper\u2019s two fingers during this pinch.\n2) Large roller / small roller / square press / square punch: {x, y, z, \u03b8}, where {x, y, z}\nis the bounded location indicating the center of the action, and \u03b8 is the robot gripper\u2019s\nrotation about the vertical axis. In the case of rollers, the rolling distance is fixed and\ntherefore not included in the action space.\n3) Circle press / circle punch: {x, y, z}, where {x, y, z} is the bounded location indicat-\ning the center of the action. The robot gripper\u2019s rotation is unnecessary because the\ntool\u2019s bottom surface is a circle.\nB) Five tools that have an action space with |A| = 2:\n1) Knife / circle cutter / pusher / skin spatula / filling spatula: {x, y}, where {x, y} is the\nbounded location indicating the center of the action on the plane. \u03b8 and z are fixed for\nthese tools to simplify the action space.\nC) The action of the hook is precoded.\nIn category B, for all tools except the knife, we leverage the prior that the center of the dough is\nalways the optimal solution in the action space and directly compute the center from the processed\npoint cloud. In the case of the knife, we use the y coordinate of the center of the dough as the\nsolution for y (the xyz coordinate system is illustrated in Figure 4). For x, we first compute the\nvolume of the target dough and then perform a binary search with the center of the dough as the\nstarting point to find the cut position that results in the closest volume to the target volume.\nIn category C, the hook is precoded first to hook the handle of the dumpling mold, then close the\nmold, press the mold handle to turn the dough into a dumpling shape, and finally open the mold by\nhooking and lifting the handle.\nThe guiding principle in designing action spaces involves starting with the end-effector\u2019s 6-DoF\naction space and eliminating redundant DoFs. For instance, rotations along the x and y axes are\ntypically not required to generate a meaningful action. Hence, we opt to exclude them from the\naction space of the 14 tools. For grippers, we transform the Cartesian coordinate system into a polar\ncoordinate system to simplify the search process for action parameters since corner cases in the\nbounded Cartesian space are usually suboptimal. Following this, we introduce tool-specific DoFs,\nwhich are determined by the tool\u2019s geometric properties. For example, in the case of grippers, we\nincorporate an additional parameter, d, to represent the width between the gripper\u2019s two fingers.\nOur method can potentially generalize to various challenging dough manipulation tasks besides\ndumpling-making, such as making alphabet letter cookies (as shown in the paper), pizza, and noo-\ndles. A successful transfer requires the ground truth meshes of new tools and data from interacting\nwith them. We only need 20 minutes of real-world interaction data per tool, demonstrating the ease\n15\nof retraining for new tasks and tools. Although we incorporate human prior knowledge to simplify\nthe action space for tools, it does not constrain the generalization capability since we can easily spec-\nify the action space for new tools. One limitation is that hand-designed action spaces may not be\ndesirable in general manipulation settings. Our insight is that for most tasks, especially tasks where\nthe robot uses a tool, there is much redundant and undesired space in the full 6-DoF action space. In\nmost cases, humans follow certain rules and a constrained action space when using a specific tool.\nA future direction is to design a planner to automatically prune the action space conditioned on the\ntool and the task.\n6.4.2\nMulti-bin Classification\nWe formulate the self-supervised policy training as a multi-bin classification problem inspired by\nprevious works on 3D bounding box estimation [51, 52]. The total loss for the multi-bin classifica-\ntion is\nL =\n|A|\nX\ni=1\n\u0010\nLAi\nconf + w \u00b7 LAi\nloc\n\u0011\n,\n(5)\nwhere the confidence loss LAi\nconf is the softmax loss of the confidences of each bin for each action\nparameter Ai, and the localization loss LAi\nloc is the loss that tries to minimize the difference between\nthe estimated parameter and the ground truth parameter. For orientation estimation, we use negative\ncosine loss as the localization loss and force it to minimize the difference between the ground truth\nand all the bins that cover that value. We use the smooth L1 loss as the localization loss for action\nparameters not representing an orientation. During inference time, for each parameter, the bin with\nmaximum confidence is selected, and the final output is computed by adding the estimated delta of\nthat bin to the center of the same bin.\nTo establish the bins, we first bound our action space into a reasonable range (Amin, Amax). Second,\nwe define the number of bins N as a hyperparameter and select N = 8 for translations and N = 32\nfor rotations. Third, we divide the action space into N + 1 bins with size 2 \u2217 (Amax \u2212 Amin)/N.\nThe center of the first bin is at Amin, the center of the last bin is at Amax, and each bin overlaps\nwith neighboring bins. This approach is similar to [51].\n6.4.3\nSubgoal Definitions\nAs mentioned in 6.2.1, during data collection, we execute a hand-coded dumpling-making pipeline\nten times and add point clouds captured before and after using each tool in the pipeline to our dataset\nas expert demonstrations. The point clouds recorded after using each tool in one of these trajectories\nare selected as subgoals.\n6.5\nExperiment Setup\n6.5.1\nRobot and Sensors\nWe use the 7-DoF Franka Emika Panda robot arm and its parallel jaw gripper as the base robot. Four\ncalibrated Intel RealSense D415 RGB-D cameras are fixed on vertical metal bars around the robot\ntabletop, as shown in Figure 4. The cameras capture 1280\u00d7720 RGB-D images at 30 Hz. We also\ndesign a set of 3D-printed tools based on real-world dough manipulation tools.\n6.5.2\nTool Design\nWe design and 3D-print 14 tools: large roller, small roller, circle press, circle punch, square press,\nsquare punch, knife / pusher, circle cutter, two-rod symmetric gripper, asymmetric gripper, two-\nplane symmetric gripper, skin spatula, filling spatula, and hook. The dumpling mold is the same\nas real-world ones. In Figure 7, we compare our 3D-printed tools and their real-world prototypes,\nwhich are common kitchen tools for dough manipulation. The design principle of these 3D-printed\ntools is to mimic real-world ones as closely as possible.\n16\nPrototype\n3D-printed tools\nRollers\nGrippers\nPresses/punches\nKnife\nCircle cutter\nSpatulas\nHook\n+\nFigure 7: Prototypes of 3D-printed tools. We show a comparison between our 3D-printed tools\nand their real-world prototypes which are common kitchen tools for dough manipulation. The design\nprinciple of these 3D-printed tools is to mimic real-world ones as closely as possible. We use 3D-\nprinted tools instead of real-world ones to allow the robot arm to acquire and manipulate the tools\nmore easily.\nThe roller is composed of a holder and a rolling pin so that the rolling pin can rotate freely while\nthe holder remains static. We designed both large and small rollers to accommodate different needs.\nWe also have a set of punches and presses with square and circle shapes. The knife is a thin planar\ntool that can cut through objects. Similarly, the circle cutter can cut an object into a circular shape.\nAmong the grippers, the two-rod symmetric gripper consists of two cylindrical extrusions, the asym-\nmetric gripper consists of a cylindrical and planar part, and the two-plane symmetric gripper consists\nof two planar parts. The two extruding rods on each gripper insert into the corresponding holes of\nthe two fingers of Franka\u2019s gripper, allowing them to adhere to and move along with the fingers.\nA linear shaft connects the two parts of each gripper, constraining their movement to a single axis.\nThe skin and filling spatulas have a similar design, except that their two extrusions are each spatula,\nso they can pick up and put down the soft object without deforming it. The hook and the dumpling\nmold are tools used together to mold the dough into a dumpling shape.\n6.5.3\nTool-Switching Setup\nThe tool-switching setup is an engineering innovation we implement in this project. We adopt two\ndesigns so that the robot can pick up, use, and put down the tools without any help from humans:\n(1) The connector on the top of each tool attaches firmly to the Franka\u2019s gripper when it closes its\nfingers and also unlocks easily when the gripper reopens. (2) The tool racks on the bottom and\nright side of the robot table hold all the 3D-printed tools in their upright poses so that the robot can\neasily pick up and put down the tools. Additionally, we calibrate the tools\u2019 world coordinates so\nthat the robot knows where to find each tool. The supplementary videos of making dumplings show\nexamples of how the robot switches tools.\n6.6\nComparison with Human Subjects on Dumpling-making\nWe invited five human subjects to make dumplings with the same tools to highlight the complexity\nof dumpling-making. Each subject participated in two experiments: choosing tools independently\nand following a given tool sequence and subgoals. For a fair comparison, human subjects were\nnot allowed to directly touch the dough with their hands or apply each tool more than five times.\nBefore the experiments, we introduced each tool and gave them sufficient time to get familiar with\nthe dough\u2019s dynamics and devise their plan. We compare their best attempt among three trials to\nour method for each experiment. Figure 9 shows that human performance is notably worse than our\nmethod without subgoals. Performance improves with the tool sequence and subgoals but remains\ncomparable to or worse than our method. The fifth supplementary video records the entire process.\n17\nKnife\nCircle cutter\nSmall roller\nHook\nAsymmetric gripper\nSkin spatula\nFilling spatula\nTwo-plane gripper\nTwo-rod gripper\nSquare press\nLarge roller\nSquare punch\nCircle punch\nCircle press\nPusher\nKnife\nCircle cutter\nSmall roller\nHook\nAsymmetric gripper\nSkin spatula\nFilling spatula\nTwo-plane gripper\nTwo-rod gripper\nSquare press\nLarge roller\nSquare punch\nCircle punch\nCircle press\nPusher\nLabel\nPrediction\n0\n20\n40\n60\n80\n100\nFigure 8: Confusion matrix of the tool classifier predictions. We show the confusion matrix of\nthe tool classifier predictions on the test set, which is split from the training data. The tool classifier\nachieves an accuracy very close to 1.\nW/o subgoals\nW/ subgoals\nSubject 3\nSubject 4\nSubject 5\nRoboCook\nSubject 1\nSubject 2\nFigure 9: Comparison with human subjects. We show a comparison with the manipulation results\nof human subjects. In the first row, Human subjects devise their manipulation plan and choose tools\nindependently. In the second row, human subjects follow a given tool sequence and subgoals.\n6.7\nTool Classification\nThe training of our tool classifier is supervised learning with a cross-entropy loss as in standard\nclassification architectures. We split a test set from the training data of the tool classifier and show\n18\nthe confusion matrix of the tool classifier predictions in Figure 8. The instance accuracy is 0.996.\nWe compared PointNet-based and ResNet-based architectures for the tool classification network.\nPointNet-base architecture generalizes better due to its ability to encode depth information. Empiri-\ncally, it demonstrates greater robustness to changes in lighting, dough color, and dough transforma-\ntions.\n6.8\nHuman Evaluation of Alphabet Letters\nWe recognize a discrepancy between how metrics such as Chamfer Distance measure the results\nand how humans perceive them - these metrics are prone to local noises while humans are good at\ncapturing the holistic features of the dough. Therefore, we invite 100 human subjects to evaluate the\nresults. The human survey asks the question: \u201cWhat alphabet letter is the robot trying to shape in the\ngiven image?\u201d If we put all 20 images (four methods \u00d7 five letters) in Question 1, there could be a\npredictive bias from seeing more than one image of the same letter. Therefore, we shuffle the order\nof 20 images and split them into four groups. Each group contains one image for each letter but\nfrom different methods. After averaging over five letters, we show the human perceived accuracy\nand human ranking of the performance of these four methods in Table 1.\n6.9\nBaseline Implementation Details\nBaselines RoboCraft, CEM+GNN, and RL+GNN use the same framework as RoboCook but replace\nthe policy network with gradient descent (RoboCraft), CEM, and RL. In other words, GNNs in these\nbaselines are the same as GNNs we trained in RoboCook. They also use the same tool predictor as\nin RoboCook to handle this multi-tool task. They are meant to compare the policy network in\nRoboCook with alternatives.\nThe RL+GNN baseline utilizes a model-based Soft Actor-Critic (SAC) with a learned GNN-based\ndynamics model as the world model. The action space aligns with other planning methods, and the\nstate space comprises the point cloud position. The reward function is derived from the change in\nChamfer Distance after each grip. Training involves a discount factor of 0.99, a learning rate of\n0.0003 with the Adam optimizer, 2-layer MLPs with 256 hidden units, and ReLU activation for both\npolicy and critic models. We initially collect 250 steps of warm-up data. The replay buffer size is\n1e6, and the target smooth coefficient is 0.005. We trained the RL baseline for 2500 steps online\nfor each action prediction to fit into our closed-loop control module. The inputs and outputs of the\nRL baseline are the same as our policy network. The input is a point cloud of the current dough\nconfiguration and the point cloud of the subgoal. The output is the action in the defined action space\nof the selected tool. The results shown in Figure 5 and Table 1 indicate that the RL baseline is\nnoticeably worse than our method.\n6.10\nAnalysis of Comparison with Baselines\nOur methods outperform baselines RoboCraft, CEM+GNN, and RL+GNN by a large margin, using\nthe same GNNs and tool predictors but different planning methods. One reason is that our policy\nnetwork sees a complete coverage of the entire action space from the large synthetic datasets gen-\nerated by our dynamics model offline. Other planning methods explore the action space online and\ntherefore have a smaller coverage than ours. By training a goal-conditioned policy network offline,\nwe also make the online action synthesis much more efficient than baseline methods.\nAnother insight is that our PointNet-based policy network is better at capturing higher-level shape\nchanges of the point cloud, such as concavities. For example, by comparing the current and target\ndough configuration, the policy network knows that the gripper should probably pinch the concave\nlocations in the target dough configuration to deform the current dough into the target shape.\nOur method also outperforms the baseline CEM+MPM since an MPM simulator suffers from a\nsim2real gap and relies on careful system identification to bridge the gap. Thus, the MPM simulator\nunderperforms the GNN, which is directly trained on real-world data.\n19\n"
  },
  {
    "title": "DomainStudio: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data",
    "link": "https://arxiv.org/pdf/2306.14153.pdf",
    "upvote": "6",
    "text": "DOMAINSTUDIO: FINE-TUNING DIFFUSION MODELS FOR\nDOMAIN-DRIVEN IMAGE GENERATION USING LIMITED DATA\nJingyuan Zhu\nTsinghua University\njy-zhu20@mails.tsinghua.edu.cn\nHuimin Ma\nUniversity of Science and Technology Beijing\nmhmpub@ustb.edu.cn\nJiansheng Chen\nUniversity of Science and Technology Beijing\njschen@ustb.edu.cn\nJian Yuan\nTsinghua University\njyuan@tsinghua.edu.cn\nFigure 1: Given few-shot training samples, the proposed AI image creation approach named DomainStudio can generate: 1) samples\ncontaining the same subject as training samples with different contexts (the first row), 2) samples containing subjects different from\ntraining samples (the second row), 3) samples containing subjects different from training samples with different contexts (the third row).\nAbstract\nDenoising diffusion probabilistic models (DDPMs) have\nbeen proven capable of synthesizing high-quality images\nwith remarkable diversity when trained on large amounts\nof data. Typical diffusion models and modern large-scale\narXiv:2306.14153v4  [cs.CV]  16 Jan 2024\nconditional generative models like text-to-image generative\nmodels are vulnerable to overfitting when fine-tuned on ex-\ntremely limited data. Existing works have explored subject-\ndriven generation using a reference set containing a few\nimages. However, few prior works explore DDPM-based\ndomain-driven generation, which aims to learn the common\nfeatures of target domains while maintaining diversity. This\npaper proposes a novel DomainStudio approach to adapt\nDDPMs pre-trained on large-scale source datasets to tar-\nget domains using limited data. It is designed to keep the\ndiversity of subjects provided by source domains and get\nhigh-quality and diverse adapted samples in target domains.\nWe propose to keep the relative distances between adapted\nsamples to achieve considerable generation diversity. In ad-\ndition, we further enhance the learning of high-frequency\ndetails for better generation quality. Our approach is com-\npatible with both unconditional and conditional diffusion\nmodels. This work makes the first attempt to realize un-\nconditional few-shot image generation with diffusion mod-\nels, achieving better quality and greater diversity than cur-\nrent state-of-the-art GAN-based approaches. Moreover, this\nwork also significantly relieves overfitting for conditional\ngeneration and realizes high-quality domain-driven genera-\ntion, further expanding the applicable scenarios of modern\nlarge-scale text-to-image models.\n1\nIntroduction\nRecent advances in generative models including GANs\n[18, 4, 36, 37, 35], variational autoencoders (VAEs) [40,\n69, 84], and autoregressive models [86, 9, 22] have realized\nhigh-quality image generation with great diversity.\nDif-\nfusion probabilistic models [77] are introduced to match\ndata distributions by learning to reverse multi-step nois-\ning processes.\nHo et al.\n[25] demonstrate the capabil-\nity of DDPMs to produce high-quality results.\nFollow-\ning works [81, 12, 58, 38] further optimize the noise ad-\ndition schedules, network architectures, and optimization\ntargets of DDPMs. DDPMs show excellent generation re-\nsults competitive with GANs [37, 4] on datasets including\nCIFAR-10 [42], LSUN [97], and ImageNet [86]. Moreover,\nDDPMs have also achieved compelling results in generat-\ning videos [27, 20, 95, 99], audios [41, 2], point clouds\n[108, 54, 56, 49], and biological structures [94, 29, 50].\nModern DDPMs depend on large amounts of data to\ntrain the millions of parameters in their networks like other\ngenerative models, which tend to overfit seriously and fail\nto produce high-quality images with considerable diversity\nwhen training data is limited. Unfortunately, it is not al-\nways possible to obtain abundant data under some circum-\nstances. A series of GAN-based approaches [90, 34, 57, 89,\n48, 60, 106] have been proposed to adapt models pre-trained\non large-scale source datasets to target datasets using a few\navailable training samples (e.g., 10 images).\nThese ap-\nproaches utilize knowledge from source models to relieve\noverfitting but can only achieve limited quality and diver-\nsity. Unconditional few-shot image generation with diffu-\nsion models remains to be investigated.\nFew-shot text-to-image generation methods [16, 72, 43,\n19] are designed to preserve the key features of subjects and\nsynthesize the subject with novel scenes, poses, and views\nutilizing the prior knowledge of large-scale text-to-image\nmodels [70]. However, it remains challenging to extract the\ncommon features of limited target data and realize few-shot\ngeneration with diverse subjects instead of producing novel\nscenes of a certain subject.\nThis work concentrates on domain-driven generation\nwith diffusion models using limited data. More specifically,\nwe aim to adapt diverse source samples containing various\nsubjects to target domains given a few reference images.\nFor example, given only a few house paintings of Van Gogh,\nwe train diffusion models to synthesize paintings of various\nsubjects in the same style. Similarly, we can synthesize di-\nverse baby images using a source model pre-trained on face\ndatasets and a few baby images as training data.\nWe first evaluate the performance of DDPMs fine-tuned\non limited data directly and show that both unconditional\nDDPMs and modern conditional text-to-image DDPMs suf-\nfer overfitting problems. Firstly, directly fine-tuned models\ntend to replicate parts of training samples and cannot learn\nthe common features of limited data, resulting in limited\ndiversity and unreasonable results. In addition, it\u2019s some-\ntimes hard for the fine-tuned models to preserve rich de-\ntails, leading to too smooth generated samples and degraded\nquality. To this end, we introduce the DomainStudio ap-\nproach, which keeps the relative distance between generated\nsamples and realizes high-frequency details enhancement\nduring domain adaptation to achieve few-shot and domain-\ndriven generation with remarkable quality and diversity.\nThe main contributions of our work are concluded as:\n\u2022 We make the first attempt to evaluate the performance\nof DDPMs trained from scratch as training data be-\ncome scarce and further take a closer look at DDPMs\nfine-tuned on extremely limited data.\n\u2022 We propose a pairwise similarity loss to keep the rel-\native pairwise distances between generated samples\nduring DDPM domain adaptation for greater diversity.\n\u2022 We design a high-frequency details enhancement ap-\nproach from two perspectives, including preserving\ndetails provided by source models and learning more\ndetails from limited data during DDPM domain adap-\ntation for finer quality.\n2\n\u2022 We demonstrate the effectiveness of DomainStudio\nqualitatively and quantitatively on a series of few-shot\nimage generation tasks and show that DomainStudio\nachieves better generation quality and diversity than\ncurrent state-of-the-art unconditional GAN-based ap-\nproaches and conditional DDPM-based text-to-image\napproaches.\n2\nRelated Work\n2.1\nDiffusion Models\nDDPMs [77] define a forward noising (diffusion) process\nadding Gaussian noises \u03f5 to training samples x0 and em-\nploy a UNet-based neural network \u03f5\u03b8 to approximate the re-\nverse distribution, which can be trained to predict the added\nnoises or the denoised images. Ho et al. [25] demonstrate\nthat predicting \u03f5 performs well and achieves high-quality\nresults using a reweighted loss function:\nLsimple = Et,x0,\u03f5 [||\u03f5 \u2212 \u03f5\u03b8(xt, t)||]2 ,\n(1)\nwhere t and xt represent the diffusion step and correspond-\ning noised image. DDPMs have achieved competitive un-\nconditional generation results on typical large-scale datasets\n[42, 97, 86]. Besides, classifier guidance is added to real-\nize DDPM-based conditional image generation [12]. La-\ntent diffusion models [70] employ pre-trained autoencoders\nto compress images into the latent space and achieve high-\nquality conditional generation using inputs such as text, im-\nages, and semantic maps.\nText-to-image Generation Text-driven image genera-\ntion [10, 13, 15, 31, 24, 46, 44, 63, 64, 67, 82, 103] has\nachieved great success based on GANs [4, 36, 37, 35], trans-\nformers [87], and diffusion models [25] with the help of\nimage-text representations like CLIP [65]. Large-scale text-\nto-image generative models including Imagen [74], Parti\n[98], CogView2 [14], DALL-E2 [66], and Stable Diffusion\n[70] further expands application scenarios and improve gen-\neration quality. Light-weight fine-tuning methods like Tex-\ntual Inversion [16] and DreamBooth [72] realize personal-\nization of text-to-image diffusion models. However, these\nmethods are still vulnerable to overfitting and mainly focus\non subject-driven generation. Recent works [79, 78] based\non MaskGIT [6] and MUSE [5] tackle similar problems of\ngenerating images containing different subjects and shar-\ning the same style with training samples. Our approach is\ndesigned to realize few-shot and domain-driven generation\nwith diffusion models and achieves compelling results with\nhigh quality and great diversity.\nApplications DDPMs have already been applied to\nmany aspects of applications such as image super-resolution\n[45, 75, 71, 26], image translation [73, 62], semantic seg-\nmentation [3, 1], few-shot generation for unseen classes\n[17, 76], and natural language processing [2, 47, 8]. Be-\nsides, DDPMs are combined with other generative models\nincluding GANs [93, 91], VAEs [85, 30, 53], and autore-\ngressive models [68, 28]. Different from existing works,\nthis paper focuses on model-level, unconditional, few-shot\nimage generation with DDPM-based approaches.\n2.2\nFew-shot Image Generation\nFew-shot image generation aims to achieve high-quality\ngeneration with great diversity using only a few available\ntraining samples. However, modern generative models eas-\nily overfit and suffer severe diversity degradation when\ntrained on limited data (e.g., 10 images). They tend to repli-\ncate training samples instead of generating diverse images\nfollowing similar distributions. GAN-based few-shot image\ngeneration approaches mainly follow TGAN [90] to adapt\nGANs pre-trained on large source domains, including Im-\nageNet [86], LSUN [97], and FFHQ [37], to related tar-\nget domains with limited data. Augmentation approaches\n[83, 104, 107, 34] also help improve generation diversity.\nBSA [59] fixes all the parameters except for the scale and\nshift parameters in the generator. FreezeD [57] freezes pa-\nrameters in high-resolution layers of the discriminator to re-\nlieve overfitting. MineGAN [89] uses additional fully con-\nnected networks to modify noise inputs for the generator.\nEWC [48] makes use of elastic weight consolidation to reg-\nularize the generator by making it harder to change the crit-\nical weights which have higher Fisher information [55] val-\nues. CDC [60] proposes a cross-domain consistency loss\nand patch-level discrimination to build a correspondence\nbetween source and target domains.\nDCL [106] utilizes\ncontrastive learning to push away the generated samples\nfrom real images and maximize the similarity between cor-\nresponding image pairs in source and target domains. The\nproposed DomainStudio approach follows similar strategies\nto adapt models pre-trained on large source domains to tar-\nget domains. Our experiments show that DDPMs are qual-\nified for few-shot image generation tasks and can achieve\nbetter results than current state-of-the-art GAN-based ap-\nproaches in generation quality and diversity.\n3\nDDPMs Trained on Small-scale Datasets\n3.1\nTraining from Scratch\nWe first train DDPMs on small-scale datasets contain-\ning various numbers of images from scratch. We analyze\ngeneration diversity qualitatively and quantitatively to study\nwhen do DDPMs overfit as training samples decrease.\nBasic Setups We sample 10, 100, and 1000 im-\nages from FFHQ-babies (Babies), FFHQ-sunglasses (Sun-\nglasses) [60], and LSUN Church [97] respectively as small-\n3\nFigure 2: For small-scale Sunglasses datasets containing 10, 100, and 1000 images, Left: samples picked from the small-scale datasets,\nRight: samples produced by DDPMs trained on the small-scale datasets from scratch.\nNumber of Samples\nBabies\nSunglasses\nChurch\n10\n0.2875\n0.3030\n0.3136\n100\n0.3152\n0.3310\n0.3327\n1000\n0.4658\n0.4658\n0.4658\n0.4819\n0.4819\n0.4819\n0.5707\n0.5707\n0.5707\n10 (+ flip)\n0.1206\n0.1217\n0.0445\n100 (+ flip)\n0.1556\n0.1297\n0.1177\n1000 (+ flip)\n0.4611\n0.4611\n0.4611\n0.4726\n0.4726\n0.4726\n0.5625\n0.5625\n0.5625\nTable 1: Nearest-LPIPS (\u2191) results of DDPMs trained from scratch\non several small-scale datasets.\nscale training datasets.\nThe image resolution of all the\ndatasets is set as 256 \u00d7 256. We follow the model setups\nin prior works [58, 12] used for LSUN 2562 [97]. The max\ndiffusion step T is set as 1000. We use a learning rate of\n1e-4 and a batch size of 48. We train DDPMs for 40K it-\nerations on datasets containing 10 or 100 images and 60K\niterations on datasets containing 1000 images empirically.\nQualitative Evaluation In Fig. 2, we visualize the gen-\nerated samples of DDPMs trained from scratch on few-shot\nSunglasses datasets and provide some training samples for\ncomparison (more generated and training samples are added\nin Appendix G). We observe that DDPMs overfit and tend to\nreplicate training samples when datasets are limited to 10 or\n100 images. Since some training samples are flipped in the\ntraining process as a step of data augmentation, we can also\nfind some generated images symmetric to the training sam-\nples. While for datasets containing 1000 images, DDPMs\ncan generate diverse samples following similar distributions\nof training samples instead of replicating them. The over-\nfitting problem is relatively alleviated. However, the gen-\nerated samples are coarse and lack high-frequency details\ncompared with training samples.\nQuantitative Evaluation LPIPS [102] is proposed to\nevaluate the perceptual distances [32] between images. We\npropose a Nearest-LPIPS metric based on LPIPS to evaluate\nthe generation diversity of DDPMs trained on small-scale\ndatasets. More specifically, we first generate 1000 images\nrandomly and find the most similar training sample hav-\ning the lowest LPIPS distance to each generated sample.\nNearest-LPIPS is defined as the LPIPS distances between\ngenerated samples and the most similar training samples in\ncorrespondence averaged over all the generated samples. If\na generative model reproduces the training samples exactly,\nthe Nearest-LPIPS metric will have a score of zero. Larger\nNearest-LPIPS values indicate lower replication rates and\ngreater diversity relative to training samples.\nWe provide the Nearest-LPIPS results of DDPMs trained\nfrom scratch on small-scale datasets in the top part of Ta-\nble 1. For datasets containing 10 or 100 images, we have\nlower Nearest-LPIPS values. While for datasets containing\n1000 images, we get measurably improved Nearest-LPIPS\nvalues. To avoid the influence of generated images sym-\nmetric to training samples, we flip all the training samples\nas supplements to the original datasets and recalculate the\nNearest-LPIPS metric. The results are listed in the bottom\npart of Table 1. With the addition of flipped training sam-\nples, we find apparently lower Nearest-LPIPS values for\ndatasets containing 10 or 100 images. However, we get al-\nmost the same Nearest-LPIPS results for DDPMs trained\non larger datasets containing 1000 images, indicating that\nthese models can generate diverse samples different from\nthe original or symmetric training samples.\nTo sum up, it becomes harder for DDPMs to learn the\nrepresentations of datasets as training data become scarce.\nWhen trained on limited data from scratch, DDPMs fail to\nmatch target data distributions exactly and cannot produce\nhigh-quality and diverse samples.\n3.2\nFine-tuning\nFurthermore, we fine-tune DDPMs pre-trained on large-\nscale source datasets using limited target data directly to\ntake a closer look at the performance of DDPMs trained on\nlimited data.\nThe fine-tuned unconditional DDPMs achieve faster\nconvergence to target domains. They only need 3K-4K iter-\nations to converge in our experiments. As shown in the mid-\ndle row of Fig. 3, they can produce diverse results utilizing\nonly 10 training samples. However, the generated samples\nlack ample high-frequency details and share similar features\nlike hairstyles and facial expressions, leading to the degra-\ndation of generation quality and diversity. Compared with\npre-trained models, the degradation of fine-tuned models\nmainly comes from the excessively shortened relative dis-\n4\nFigure 3: DDPM-based image generation samples on 10-shot FFHQ \u2192 Babies, FFHQ \u2192 Sketches, and LSUN Church \u2192 Haunted houses.\nFigure 4: Two samples synthesized from fixed noise inputs by the\ndirectly fine-tuned DDPM on 10-shot FFHQ \u2192 Babies become\nmore and more similar throughout training, as shown by the in-\ncreasing cosine similarity.\ntances between generated samples. As shown in Fig. 4, two\nsamples synthesized from fixed noise inputs by the directly\nfine-tuned DDPM become increasingly similar (e.g., eyes\nand facial expressions) throughout training, losing various\nfeatures and high-frequency details.\nIn addition, we fine-tune the large text-to-image gener-\native model Stable Diffusion on 10-shot Van Gogh houses\nand haunted houses, respectively. We employ a unique iden-\ntifier [V] to avoid using prior knowledge of target domains\nin the Stable Diffusion model. As shown in Fig. 5, Dream-\nBooth without prior preservation severely overfits and pro-\nduces low-quality samples. The full DreamBooth approach\nstill overfits and gets limited diversity with replicated sam-\nples. They tend to preserve the subjects in training samples\ninstead of generating diverse samples following similar dis-\ntributions, which is pursued by domain-driven generation.\nFigure 5: Stable Diffusion v1.4 fine-tuned by DreamBooth and\nDomainStudio on 10-shot datasets with the same text prompts.\n4\nUnconditional Image Generation\nIn this section, we illustrate the DomainStudio approach\non unconditional image generation with typical diffusion\nmodels. We propose to regularize the domain adaptation\nprocess by keeping the relative pairwise distances between\nadapted samples similar to source samples (Sec 4.1). Be-\nsides, we guide adapted models to learn high-frequency\ndetails from limited data and preserve high-frequency de-\ntails learned from source domains (Sec 4.2). Our approach\nfixes source models \u03f5sou as reference for adapted models\n\u03f5ada. The weights of adapted models are initialized to the\nweights of source models and adapted to target domains.\nAn overview of the proposed DomainStudio approach for\nunconditional image generation is illustrated in Fig. 6 using\n5\nFigure 6: Overview of the DomainStudio approach applied to unconditional image generation. A pairwise similarity loss is intro-\nduced to keep the relative pairwise distances of generated samples and their high-frequency details of adapted models similar to source\nmodels during adaptation. DomainStudio guides adapted models to learn more high-frequency details from limited data using the MSE\nloss between high-frequency details extracted from training data and adapted samples. DomainStudio generates high-quality and diverse\nsamples sharing similar styles with few-shot training data.\n10-shot FFHQ \u2192 Babies as an example.\n4.1\nRelative Distances Preservation\nWe design a pairwise similarity loss to maintain the\ndistributions of relative pairwise distances between gener-\nated images during domain adaptation. To construct N-way\nprobability distributions for each image, we sample a batch\nof noised images {xn\nt }N\nn=0 by randomly adding Gaussian\nnoises to training samples x0 \u223c q(x0). Then both source\nand adapted models are applied to predict the fully denoised\nimages {\u02dcxn\n0}N\nn=0. We have the prediction of \u02dcx0 in terms of\nxt and \u03f5\u03b8(xt, t) as follows:\n\u02dcx0 =\n1\n\u221a\u03b1t\nxt \u2212\n\u221a1 \u2212 \u03b1t\n\u221a\u03b1t\n\u03f5\u03b8(xt, t).\n(2)\nCosine similarity is employed to measure the relative dis-\ntances between the predicted images \u02dcx0. The probability\ndistributions for \u02dcxi\n0 (0 \u2264 i \u2264 N) in source and adapted\nmodels are as follows:\npsou\ni\n= sfm(\nn\nsim(\u02dcxi\n0sou, \u02dcxj\n0sou)\no\n\u2200i\u0338=j),\n(3)\npada\ni\n= sfm(\nn\nsim(\u02dcxi\n0ada, \u02dcxj\n0ada)\no\n\u2200i\u0338=j),\n(4)\nwhere sim and sfm denote cosine similarity and softmax\nfunction, respectively. Then we have the pairwise similarity\nloss for generated images as follows:\nLimg(\u03f5sou, \u03f5ada) = Et,x0,\u03f5\nX\ni\nDKL(pada\ni\n|| psou\ni\n),\n(5)\nwhere DKL represents KL-divergence.\nLimg prevents\nadapted samples from being too similar to each other or\nreplicating training samples. Instead, adapted models are\nencouraged to learn the common features of training data\nand preserve diverse features learned from source domains.\n4.2\nHigh-frequency Details Enhancement\nTo begin with, we employ the typical Haar wavelet\ntransformation [11] to disentangle images into multiple fre-\nquency components. Haar wavelet transformation contains\nfour kernels including LLT , LHT , HLT , HHT , where L\nand H represent the low and high pass filters, respectively:\nLT =\n1\n\u221a\n2[1, 1],\nHT =\n1\n\u221a\n2[\u22121, 1].\n(6)\nHaar wavelet transformation decomposes inputs into four\nfrequency components LL, LH, HL, and HH. LL con-\ntains fundamental structures of images while other high-\n6\nfrequency components LH, HL, and HH contain rich de-\ntails of images. We define hf as the sum of these high-\nfrequency components needing enhancement:\nhf = LH + HL + HH.\n(7)\nWe implement high-frequency details enhancement from\ntwo perspectives. Firstly, we use the proposed pairwise sim-\nilarity loss to preserve high-frequency details learned from\nsource domains. Similarly, the probability distributions for\nthe high-frequency components of \u02dcxi\n0 (0 \u2264 i \u2264 N) in\nsource and adapted models and the pairwise similarity loss\nfor the high-frequency components are as follows:\npf sou\ni\n= sfm(\nn\nsim(hf(\u02dcxi\n0sou), hf(\u02dcxj\n0sou))\no\n\u2200i\u0338=j),\n(8)\npf ada\ni\n= sfm(\nn\nsim(hf(\u02dcxi\n0ada), hf(\u02dcxj\n0ada))\no\n\u2200i\u0338=j),\n(9)\nLhf(\u03f5sou, \u03f5ada) = Et,x0,\u03f5\nX\ni\nDKL(pf ada\ni\n|| pf sou\ni\n),\n(10)\nSecondly, we guide adapted models to learn more high-\nfrequency details from limited training data by minimizing\nthe mean squared error (MSE) between the high-frequency\ncomponents in adapted samples \u02dcx0ada and training samples\nx0 as follows:\nLhfmse(\u03f5ada) = Et,x0,\u03f5 [||hf(\u02dcx0ada) \u2212 hf(x0)||]2 . (11)\n4.3\nOverall Optimization Target\nThe overall optimization target of DomainStudio for un-\nconditional image generation combines all the methods pro-\nposed above to realize relative distances preservation and\nhigh-frequency details enhancement:\nL = Lsimple + \u03bb1Lvlb + \u03bb2Limg + \u03bb3Lhf + \u03bb4Lhfmse.\n(12)\nWe follow prior works [58] to set \u03bb1 as 0.001 to avoid\nthe variational lower bound loss Lvlb from overwhelming\nLsimple. We empirically find \u03bb2, \u03bb3 ranging between 0.1\nand 1.0 and \u03bb4 ranging between 0.01 and 0.08 to be effec-\ntive for few-shot adaptation setups.\n5\nText-to-image Generation\nSimilarly, we propose to realize relative distances preser-\nvation (Sec. 5.1) and high-frequency details enhancement\n(Sec. 5.2) to realize domain-driven text-to-image genera-\ntion. An overview of the proposed DomainStudio approach\napplied to text-to-image generation is illustrated in Fig. 7.\nThe pre-trained autoencoders (\u03b5 + D) used in text-to-image\ngeneration are fixed during fine-tuning.\nWe directly use\n\u03f5ada and \u03f5sou to represent the pre-trained and adapted text-\nto-image diffusion models.\nGiven few-shot training samples and a text prompt Ptar\nrepresenting the target domain, we first define a source do-\nmain using a text prompt Psou. To avoid using the prior\nknowledge of target domains provided by the large text-to-\nimage model, we employ a unique identifier [V ] to represent\nthe target domains. For example, we define the source do-\nmain with the text prompt \u201ca house\u201d given few-shot house\npaintings of Van Gogh as training samples which are repre-\nsented by the text prompt \u201ca [V ] house\u201d. The source and tar-\nget text prompts Psou and Ptar are encoded by pre-trained\ntext encoder \u0393 to conditioning vectors csou and ctar.\nThe adapted model is guided to learn from target samples\nwith the reconstruction function:\nLsimple = Et,z,ctar,\u03f5||\u03f5ada(zt, ctar) \u2212 \u03f5||2,\n(13)\nwhere z and zt represent the compressed latent codes of\ntraining samples and corresponding noised latent codes.\nDreamBooth [72] generates source samples xpr with\nrandomly sampled Gaussian noises and the source text con-\ndition csou using the pre-trained text-to-image model. Then\nthe pre-trained encoder \u03b5 is employed to compress xpr to\nlatent codes zpr.\nDreambooth proposes a class-specific\nprior preservation loss as follows to relieve overfitting for\nsubject-driven generation by preserving the information of\nsource domains:\nLpr = Et,zpr,csou,\u03f5pr||\u03f5sou(zpr\nt , csou) \u2212 \u03f5ada(zpr\nt , csou)||2,\n(14)\nwhere zpr\nt\nrepresents the source latent codes added with\nnoises \u03f5pr.\n5.1\nRelative Distances Preservation\nSince the source and target samples have different con-\nditions, it\u2019s difficult to build correspondence like uncondi-\ntional image generation. Therefore, we directly use ran-\ndomly generated source and adapted samples to keep simi-\nlar relative distances. Given batches of noised source latent\ncodes {zpr,n\nt\n}N\nn=0 and target latent codes {zn\nt }N\nn=0, we build\nprobability distributions for source and adapted samples as\nfollows:\npsou\ni\n= sfm(\nn\nsim(D(\u02dczpr,i\nada), D(\u02dczpr,j\nada )\no\n\u2200i\u0338=j),\n(15)\npada\ni\n= sfm(\nn\nsim(D(\u02dczi\nada), D(\u02dczj\nada))\no\n\u2200i\u0338=j),\n(16)\nwhere \u02dczpr\nada and \u02dczada represent the denoised latent codes\ncorresponding to source and target samples generated by the\nadapted model. The pairwise similarity loss for generated\nimages can be expressed as follows:\nLimg(\u03f5ada) = Et,z,zpr,\u03f5,\u03f5pr\nX\ni\nDKL(pada\ni\n||psou\ni\n).\n(17)\n7\nFigure 7: Overview of the proposed DomainStudio approach applied to text-to-image diffusion models. Our approach follows\nDreamBooth to preserve the information of source domains with prior preservation loss. Similar to unconditional image generation,\nDomainStudio applies pairwise similarity loss to generated samples and their high-frequency components in source and target domains. In\naddition, DomainStudio uses high-frequency reconstruction loss to enhance details learning from target domains.\nLimg encourages the adapted model to keep the diversity\nof adapted samples similar to source samples. In this way,\nthe adapted model is guided to generate adapted samples\ncontaining diverse subjects following target distributions.\n5.2\nHigh-frequency Details Enhancement\nThe high-frequency details enhancement for text-to-\nimage generation is also composed of two perspectives.\nFirstly, the pairwise similarity loss is used to enhance the\ndiversity of high-frequency details in adapted samples. The\nprobability distributions for the high-frequency components\nof source and adapted samples and the pairwise similarity\nloss are as follows:\npf sou\ni\n= sfm(\nn\nsim(hf(D(\u02dczpr,i\nada)), hf(D(\u02dczpr,j\nada ))\no\n\u2200i\u0338=j),\n(18)\npf ada\ni\n= sfm(\nn\nsim(hf(D(\u02dczi\nada)), hf(D(\u02dczj\nada)))\no\n\u2200i\u0338=j),\n(19)\nLhf(\u03f5ada) = Et,z,zpr,\u03f5,\u03f5pr\nX\ni\nDKL(pf ada\ni\n||pf sou\ni\n).\n(20)\nSecondly, we propose the high-frequency reconstruction\nloss to enhance the learning of high-frequency details from\ntraining samples as follows:\nLhfmse(\u03f5ada) = Et,x0,z,\u03f5||hf(D(\u02dczada)) \u2212 hf(x0)||2.\n(21)\n5.3\nOverall Optimization Target\nThe overall optimization target of DomainStudio for\ntext-to-image generation combines the proposed methods\nfor relative distances preservation and high-frequency de-\ntails enhancement with DreamBooth:\nL = Lsimple + \u03bb1Lpr + \u03bb2Limg + \u03bb3Lhf + \u03bb4Lhfmse.\n(22)\nWe follow DreamBooth to set \u03bb1 as 1 and empirically find\n\u03bb2, \u03bb3 ranging between 1e+2 and 5e+2 and \u03bb4 ranging be-\ntween 0.1 and 1.0 to be effective for most text-to-image\nadaptation setups.\n6\nExperiments\nTo demonstrate the effectiveness of DomainStudio, we\nevaluate it with a series of few-shot fine-tuning tasks using\nextremely few training samples (\u2264 10 images). The per-\nformance of DomainStudio is compared with directly fine-\ntuned DDPMs, modern GAN-based approaches, and few-\nshot fine-tuning approaches of large text-to-image models\n8\nFigure 8: DomainStudio samples on 10-shot FFHQ \u2192 Sunglasses, FFHQ \u2192 Raphael\u2019s paintings, and LSUN Church \u2192 Landscape\ndrawings.\non generation quality and diversity qualitatively and quan-\ntitatively.\nBasic Setups For unconditional image generation, we\nchoose FFHQ [37] and LSUN Church [97] as source\ndatasets and train DDPMs from scratch on these two\ndatasets for 300K and 250K iterations as source models. As\nfor the target datasets, we employ 10-shot Sketches [88],\nBabies, Sunglasses [37], and face paintings by Amedeo\nModigliani and Raphael Peale [96] in correspondence to the\nsource domain FFHQ. Besides, 10-shot Haunted houses and\nLandscape drawings are used as the target datasets in corre-\nspondence to LSUN Church. The model setups are consis-\ntent with the experiments on small-scale datasets in Sec. 3.\nThe adapted models are trained for 3K-5K iterations with a\nbatch size of 24 on \u00d78 NVIDIA RTX A6000 GPUs.\nFor text-to-image generation, we use Stable Diffusion\n[70] v1.4 as the source model. Several few-shot datasets\ncontaining no more than 10 images, including Van Gogh\nhouses, Watercolor dogs [78], Inkpaintings, and Watercolor\npaintings [79] are used as target datasets. The adapted mod-\nels are trained for 800-1500 iterations with a batch size of\n4 on a single NVIDIA RTX A6000 GPU. Our approach\nshares the same learning rates and iterations with Dream-\nBooth [72] for fair comparison. We use typical formats like\n\u201ca [V] house\u201d and \u201ca house in the [V] style\u201d as text prompts.\nEvaluation Metrics We follow CDC [60] to use Intra-\nLPIPS for generation diversity evaluation. To be more spe-\ncific, we generate 1000 images and assign them to one of\nthe training samples with the lowest LPIPS [102] distance.\nIntra-LPIPS is defined as the average pairwise LPIPS dis-\ntances within members of the same cluster averaged over\nall the clusters. If a model exactly replicates training sam-\nples, its Intra-LPIPS will have a score of zero. Larger Intra-\nLPIPS values correspond to greater generation diversity.\nFID [23] is widely used to evaluate the generation qual-\nity of generative models by computing the distribution dis-\ntances between generated samples and datasets. However,\nFID would become unstable and unreliable when it comes\nto datasets containing a few samples (e.g., 10-shot datasets\nused in this paper). Therefore, we provide FID evaluation\nusing relatively richer target datasets including Sunglasses\nand Babies, which contain 2500 and 2700 images for un-\nconditional image generation.\nAs for text-to-image generation, we depend on CLIP\n[65] to measure the textual alignment with target text\nprompts and domain/style consistency. We denote two met-\nrics: CLIP-Text [72] and CLIP-Image. CLIP-Text is the\naverage pairwise cosine similarity between the CLIP em-\nbeddings of text prompts and generated samples. CLIP-\nImage is the average pairwise cosine similarity between the\nCLIP embeddings of training and generated samples. CLIP-\nImage may be unbiased when the model is overfitting. If a\nmodel exactly replicates training samples, its CLIP-Image\nwill have the highest score of 1. We provide CLIP-Image\nresults as a reference.\nWe fix noise inputs for DDPM-based and GAN-based\napproaches respectively to synthesize samples for fair com-\nparison of generation quality and diversity.\nBaselines Since few prior works realize unconditional\nfew-shot image generation with DDPM-based approaches,\nwe employ several GAN-based baselines sharing similar\ntargets with us to adapt pre-trained models to target do-\nmains using only a few available samples for comparison:\nTGAN [90], TGAN+ADA [34], FreezeD [57], MineGAN\n[89], EWC [48], CDC [60], and DCL [106]. All the meth-\nods are implemented based on the same StyleGAN2 [37]\ncodebase. DDPMs directly fine-tuned on limited data are\nincluded for comparison as well. The StyleGAN2 mod-\nels and DDPMs trained on the large source datasets share\nsimilar generation quality and diversity (see more details\nin Appendix H). In addition, modern few-shot fine-tuning\nmethods of text-to-image models including Textual Inver-\nsion [16] and DreamBooth [72] are used as baselines for\nconditional text-to-image generation. Textual Inversion is\ntrained to learn the style of few-shot training samples.\n6.1\nQualitative Evaluation\nUnconditional Image Generation We visualize the\nsamples of DomainStudio on 10-shot FFHQ \u2192 Babies,\n9\nFigure 9: 10-shot image generation samples on FFHQ \u2192 Sunglasses. All the samples of GAN-based approaches are synthesized from\nfixed noise inputs (rows 1-7). Samples of the directly fine-tuned DDPM and DomainStudio are synthesized from fixed noise inputs as well\n(rows 8-9). Our approach generates high-quality results with fewer blurs and artifacts and achieves considerable generation diversity.\nFFHQ \u2192 Sketches, and LSUN Church \u2192 Haunted houses\nin the bottom row of Fig. 3. DomainStudio produces more\ndiverse samples containing richer high-frequency details\nthan directly fine-tuned DDPMs. For example, DomainStu-\ndio generates babies with various detailed hairstyles and fa-\ncial features. Fig. 8 adds visualized samples under other\nadaptation setups.\nDomainStudio adapts source models\nto target domains naturally and preserves various features\ndifferent from training data. Samples of people wearing\nhats can be found when adapting FFHQ to Babies or Sun-\nglasses, which is obviously different from the training sam-\nples. The adaptation from LSUN Church to Haunted houses\nand Landscape drawings retain various architectural struc-\ntures. Fig. 9 shows samples of GAN-based and DDPM-\nbased approaches on 10-shot FFHQ \u2192 Sunglasses. For in-\ntuitive comparison, we fix the noise inputs for GAN-based\nand DDPM-based approaches, respectively.\nGAN-based\nbaselines generate samples containing unnatural blurs and\nartifacts. Besides, many incomplete sunglasses in generated\nsamples lead to vague visual effects. In contrast, the directly\nfine-tuned DDPM produces smoother results but lacks de-\ntails. DomainStudio improves generation quality and di-\nversity, achieving more pleasing visual effects than existing\napproaches. Additional visualized comparison results can\nbe found in Appendix L.\nText-to-image Generation We visualize the domain-\ndriven text-to-image generation results of DomainStudio\nusing 10-shot Van Gogh houses as training samples in Fig.\n1. Firstly, we use the text prompt \u201ca [V] house\u201d correspond-\ning to training samples and generate high-quality and di-\nverse target samples. Moreover, we can synthesize samples\nwith different contexts using text prompts like \u201ca [V] house\nwith night sky\u201d. Moreover, we can also synthesize sam-\nples in target domains containing different subjects with text\nprompts such as \u201ca [V] dog\u201d, \u201ca [V] car\u201d and \u201ca [V] vase\u201d.\nFinally, we also achieve target samples with diverse con-\ntents and contexts different from limited training samples.\nFor example, we synthesize vase samples with various kinds\n10\nFigure 10: Qualitative comparison of domain-driven text-to-image generation trained on 10-shot Dog watercolors. DomainStudio can\nproduce samples containing diverse subjects consistent with the text prompts and sharing the same style with training samples.\nFigure 11: Qualitative comparison of domain-driven text-to-image generation trained on 10-shot Wrecked cars.\nof flowers like sunflowers and roses. Besides, we get vase\nsamples with different contexts following text prompts.\nDomainStudio is compared with Textual Inversion and\nDreamBooth using 10-shot Watercolor dogs and Wrecked\ncars, 6-shot Inkpaintings, and 4-shot Watercolor paintings\nin Fig. 10, 11, and 12. It\u2019s difficult for baselines to synthe-\nsize reasonable target images with different subjects since\nthey are unable to distinguish what to learn from training\nsamples and what to preserve from source domains. For\ninstance, when generating temples and roses in the water-\ncolor style, baselines tend to combine the subjects in train-\ning samples with the subjects mentioned in text prompts, as\nshown in Fig. 10. DomainStudio guides adapted models to\nlearn the common features of few-shot data and maintain di-\nversity similar to source domains. It successfully adapts the\nsubjects mentioned in text prompts to the style of training\nsamples, even if the subjects are very different from the sub-\njects in training samples. Similar phenomena can be found\nfor baselines trained on wrecked cars, as shown in Fig. 11.\nTextual Inversion synthesizes car samples with text prompts\nof train or house. DreamBooth overfits and generates sam-\nples similar to few-shot data. It generates train and house\nsamples containing wrecked cars instead of wrecked trains\nand houses like DomainStudio. Moreover, DomainStudio\nalso achieves compelling results using fewer training sam-\nples while baselines either overfit to synthesize low-quality\nand replicated samples or underfit to synthesize samples in-\nconsistent with target domains, as shown in Fig. 12.\n6.2\nQuantitative Evaluation\nUnconditional Image Generation We provide the Intra-\nLPIPS results of DomainStudio under a series of 10-shot\n11\nFigure 12: Qualitative comparison of domain-driven text-to-image generation trained on few-shot inkpaintings and watercolors.\nApproaches\nFFHQ \u2192\nBabies\nFFHQ \u2192\nSunglasses\nFFHQ \u2192\nRaphael\u2019s paintings\nLSUN Church \u2192\nHaunted houses\nLSUN Church \u2192\nLandscape drawings\nTGAN [90]\n0.510 \u00b1 0.026\n0.550 \u00b1 0.021\n0.533 \u00b1 0.023\n0.585 \u00b1 0.007\n0.601 \u00b1 0.030\nTGAN+ADA [34]\n0.546 \u00b1 0.033\n0.571 \u00b1 0.034\n0.546 \u00b1 0.037\n0.615 \u00b1 0.018\n0.643 \u00b1 0.060\nFreezeD [57]\n0.535 \u00b1 0.021\n0.558 \u00b1 0.024\n0.537 \u00b1 0.026\n0.558 \u00b1 0.019\n0.597 \u00b1 0.032\nMineGAN [89]\n0.514 \u00b1 0.034\n0.570 \u00b1 0.020\n0.559 \u00b1 0.031\n0.586 \u00b1 0.041\n0.614 \u00b1 0.027\nEWC [48]\n0.560 \u00b1 0.019\n0.550 \u00b1 0.014\n0.541 \u00b1 0.023\n0.579 \u00b1 0.035\n0.596 \u00b1 0.052\nCDC [60]\n0.583 \u00b1 0.014\n0.581 \u00b1 0.011\n0.564 \u00b1 0.010\n0.620 \u00b1 0.029\n0.674 \u00b1 0.024\nDCL [106]\n0.579 \u00b1 0.018\n0.574 \u00b1 0.007\n0.558 \u00b1 0.033\n0.616 \u00b1 0.043\n0.626 \u00b1 0.021\nFine-tuned DDPMs\n0.513 \u00b1 0.026\n0.527 \u00b1 0.024\n0.466 \u00b1 0.018\n0.590 \u00b1 0.045\n0.666 \u00b1 0.044\nDomainStudio (ours)\n0.599 \u00b1 0.024\n0.599 \u00b1 0.024\n0.599 \u00b1 0.024\n0.604 \u00b1 0.014\n0.604 \u00b1 0.014\n0.604 \u00b1 0.014\n0.581 \u00b1 0.041\n0.581 \u00b1 0.041\n0.581 \u00b1 0.041\n0.628 \u00b1 0.029\n0.628 \u00b1 0.029\n0.628 \u00b1 0.029\n0.706 \u00b1 0.030\n0.706 \u00b1 0.030\n0.706 \u00b1 0.030\nTable 2: Intra-LPIPS (\u2191) results of DDPM-based approaches and GAN-based baselines on 10-shot image generation tasks adapted from\nthe source domain FFHQ and LSUN Church. Standard deviations are computed across 10 clusters (the same number as training samples).\nDomainStudio outperforms modern GAN-based approaches and achieves state-of-the-art performance in generation diversity.\nMethod\nTGAN\nADA\nEWC\nCDC\nDCL\nOurs\nBabies\n104.79\n102.58\n87.41\n74.39\n52.56 33.26\n33.26\n33.26\nSunglasses\n55.61\n53.64\n59.73\n42.13\n38.01 21.92\n21.92\n21.92\nTable 3: FID (\u2193) results of DomainStudio compared with GAN-\nbased baselines on 10-shot FFHQ \u2192 Babies and Sunglasses.\nadaptation setups in Table 2.\nDomainStudio realizes a\nsuperior improvement of Intra-LPIPS compared with di-\nrectly fine-tuned DDPMs. Besides, DomainStudio outper-\nforms state-of-the-art GAN-based approaches, indicating its\nstrong capability of maintaining generation diversity. Intra-\nLPIPS results under other adaptation setups are added in\nAppendix A.\nAs shown by the FID results in Table 3, DomainStudio\nperforms better on learning target distributions from limited\ndata than prior GAN-based approaches.\nText-to-image Generation We report the CLIP-based\nmetrics of DomainStudio compared with Textual Inversion\nand DreamBooth in Table 4. DomainStudio achieves better\nresults of CLIP-Text than baselines, indicating its ability to\nsynthesize images consistent with text prompts while adapt-\ning to target domains. As for CLIP-Image results, Domain-\nStudio also outperforms baselines on almost all the bench-\nmarks.\nTextual Inversion achieves the best image align-\nment on Wrecked trains since it is overfitting to the few-shot\ntraining car samples instead of synthesizing train samples.\nIn Table 5, we provide Intra-LPIPS results of Do-\nmainStudio and baselines to evaluate the generation diver-\nsity. DomainStudio achieves state-of-the-art performance\n12\nMetrics\nCLIP-Text\nCLIP-Image\nDatasets\nVan Gogh\nhouses\nWrecked\ntrains\nInkpainting\nvolcanoes\nVan Gogh\nhouses\nWrecked\ntrains\nInkpainting\nvolcanoes\nTextual Inversion [16]\n0.259 \u00b1 0.001\n0.243 \u00b1 0.002\n0.244 \u00b1 0.001\n0.763 \u00b1 0.022\n0.737 \u00b1 0.035\n0.737 \u00b1 0.035\n0.737 \u00b1 0.035\n0.658 \u00b1 0.023\nDreamBooth [72]\n0.262 \u00b1 0.002\n0.267 \u00b1 0.001\n0.275 \u00b1 0.004\n0.569 \u00b1 0.039\n0.557 \u00b1 0.011\n0.600 \u00b1 0.086\nDomainStudio (ours)\n0.276 \u00b1 0.002\n0.276 \u00b1 0.002\n0.276 \u00b1 0.002\n0.271 \u00b1 0.003\n0.271 \u00b1 0.003\n0.271 \u00b1 0.003\n0.301 \u00b1 0.002\n0.301 \u00b1 0.002\n0.301 \u00b1 0.002\n0.789 \u00b1 0.024\n0.789 \u00b1 0.024\n0.789 \u00b1 0.024\n0.600 \u00b1 0.068\n0.676 \u00b1 0.091\n0.676 \u00b1 0.091\n0.676 \u00b1 0.091\nTable 4: CLIP-Text (\u2191) and CLIP-Image results of DomainStudio compared with Textual Inversion and DreamBooth. Our approach\noutperforms existing methods on text alignment.\nApproaches\nVan Gogh\nhouses\nWatercolor\npandas\nWatercolor\ntemples\nWrecked\ncars\nWrecked\nhouses\nInkpainting\nvolcanoes\nTextual Inversion [16]\n0.480 \u00b1 0.235\n0.744 \u00b1 0.031\n0.744 \u00b1 0.031\n0.744 \u00b1 0.031\n0.763 \u00b1 0.033\n0.763 \u00b1 0.033\n0.763 \u00b1 0.033\n0.612 \u00b1 0.024\n0.624 \u00b1 0.015\n0.648 \u00b1 0.038\n0.648 \u00b1 0.038\n0.648 \u00b1 0.038\nDreamBooth [72]\n0.558 \u00b1 0.009\n0.450 \u00b1 0.099\n0.553 \u00b1 0.082\n0.534 \u00b1 0.027\n0.601 \u00b1 0.034\n0.535 \u00b1 0.049\nDomainStudio (ours)\n0.588 \u00b1 0.012\n0.588 \u00b1 0.012\n0.588 \u00b1 0.012\n0.519 \u00b1 0.014\n0.544 \u00b1 0.010\n0.636 \u00b1 0.012\n0.636 \u00b1 0.012\n0.636 \u00b1 0.012\n0.628 \u00b1 0.017\n0.628 \u00b1 0.017\n0.628 \u00b1 0.017\n0.633 \u00b1 0.029\nTable 5: Intra-LPIPS (\u2191) results of DomainStudio compared with Textual Inversion and DreamBooth.\nFigure 13: Visualized ablation analysis of the DomainStudio ap-\nproach using 10-shot FFHQ \u2192 Babies as an example.\nwhen generating target samples like Van Gogh houses and\nWrecked cars. Although Textual Inversion also achieves\noutstanding generation diversity on target samples like Wa-\ntercolor pandas and temples, it fails to produce samples\nsharing the same style with training samples and consistent\nwith text prompts, as shown in Fig. 10.\n6.3\nAblation Analysis\nWe ablate our approach on unconditional image gen-\neration using 10-shot FFHQ \u2192 Babies as an example in\nFig.\n13.\nWith relative distances preservation only, the\nadapted model produces coarse samples lacking details like\nhairstyles. DomainStudio combines both ideas and achieves\nrealistic and diverse results. Ablations of the weight coeffi-\nFigure 14: Visualized ablation analysis of the DomainStudio ap-\nproach using 6-shot Inkpatings as training data and \u201ca [V] house\u201d\nas the text prompt.\ncients in Equation 12 are added in Appendix B.\nWe also provide the visualized ablations of DomainStu-\ndio on text-to-image generation using house inkpaintings as\nan example in Fig. 14. DreamBooth is designed to preserve\nkey features of training samples. As a result, it overfits and\nfails to achieve domain-driven generation. DomainStudio\nwithout high-frequency details enhancement applies pair-\nwise similarity loss to relieve overfitting and guide adapted\nmodels to learn the knowledge of target domains while pre-\nserving source subjects corresponding to text prompts. The\nfull DomainStudio approach adds high-frequency details\nenhancement and preserves more details from source and\ntraining samples.\n13\n7\nConclusion\nWe propose DomainStudio, a novel approach to re-\nalize few-shot and domain-driven image generation with\ndiffusion models. DomainStudio keeps the relative pair-\nwise distances between adapted samples and realizes high-\nfrequency details enhancement during domain adaptation.\nIt is compatible with both unconditional and conditional\nimage generation models. For unconditional image gener-\nation, this work first introduces diffusion models to few-\nshot image generation. We demonstrate the effectiveness\nof DomainStudio on a series of few-shot image genera-\ntion tasks. It generates compelling samples with rich de-\ntails and few blurs, outperforming current state-of-the-art\nGAN-based approaches on generation quality and diversity.\nFor conditional image generation, this work realizes high-\nquality domain-driven generation by fine-tuning large text-\nto-image models with limited data. DomainStudio performs\nbetter than existing subject-driven methods and synthesizes\nsamples in target domains with diverse subjects and con-\ntexts. We believe that our work takes an essential step to-\nward more data-efficient diffusion models. The limitations\nare discussed in Appendix C.\nSocietal Impact DomainStudio proposed in this work\ncould be applied to provide additional data for corner cases\nneeded by downstream tasks and improve the efficiency of\nartistic creation by synthesizing images containing diverse\nobjects and sharing similar styles with training samples. We\nrecognize that DomainStudio has potential risks of being\nmisused to imitate existing works without permission since\nit only needs a few samples as training data.\nReferences\n[1] E. B. Asiedu, S. Kornblith, T. Chen, N. Parmar, M. Min-\nderer, and M. Norouzi. Denoising pretraining for semantic\nsegmentation. In IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition Workshops, 2022.\n[2] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. van den\nBerg.\nStructured denoising diffusion models in discrete\nstate-spaces. Advances in Neural Information Processing\nSystems, 34:17981\u201317993, 2021.\n[3] D. Baranchuk, I. Rubachev, A. Voynov, V. Khrulkov, and\nA. Babenko. Label-efficient semantic segmentation with\ndiffusion models. In International Conference on Learning\nRepresentations, 2022.\n[4] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN\ntraining for high fidelity natural image synthesis. In Inter-\nnational Conference on Learning Representations, 2019.\n[5] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama,\nL. Jiang, M.-H. Yang, K. Murphy, W. T. Freeman, M. Ru-\nbinstein, et al. Muse: Text-to-image generation via masked\ngenerative transformers. arXiv preprint arXiv:2301.00704,\n2023.\n[6] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman.\nMaskgit: Masked generative image transformer. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 11315\u201311325, 2022.\n[7] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A sim-\nple framework for contrastive learning of visual represen-\ntations. In International Conference on Machine Learning,\npages 1597\u20131607. PMLR, 2020.\n[8] T. Chen, R. Zhang, and G. Hinton. Analog bits: Generating\ndiscrete data using diffusion models with self-conditioning.\nIn International Conference on Learning Representations,\n2023.\n[9] X. Chen, N. Mishra, M. Rohaninejad, and P. Abbeel. Pixel-\nsnail: An improved autoregressive generative model. In In-\nternational Conference on Machine Learning, pages 864\u2013\n872. PMLR, 2018.\n[10] K. Crowson, S. Biderman, D. Kornis, D. Stander, E. Hal-\nlahan, L. Castricato, and E. Raff. Vqgan-clip: Open do-\nmain image generation and editing with natural language\nguidance. In Proceedings of the European Conference on\nComputer Vision, pages 88\u2013105. Springer, 2022.\n[11] I. Daubechies. The wavelet transform, time-frequency lo-\ncalization and signal analysis. IEEE Transactions on Infor-\nmation Theory, 36(5):961\u20131005, 1990.\n[12] P. Dhariwal and A. Nichol. Diffusion models beat gans on\nimage synthesis. Advances in Neural Information Process-\ning Systems, 34:8780\u20138794, 2021.\n[13] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin,\nJ. Lin, X. Zou, Z. Shao, H. Yang, et al. Cogview: Master-\ning text-to-image generation via transformers. Advances in\nNeural Information Processing Systems, 34:19822\u201319835,\n2021.\n[14] M. Ding, W. Zheng, W. Hong, and J. Tang.\nCogview2:\nFaster and better text-to-image generation via hierarchical\ntransformers. arXiv preprint arXiv:2204.14217, 2022.\n[15] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh,\nand Y. Taigman. Make-a-scene: Scene-based text-to-image\ngeneration with human priors. In Proceedings of the Eu-\nropean Conference on Computer Vision, pages 89\u2013106.\nSpringer, 2022.\n[16] R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano,\nG. Chechik, and D. Cohen-Or. An image is worth one word:\nPersonalizing text-to-image generation using textual inver-\nsion, 2022.\n[17] G. Giannone, D. Nielsen, and O. Winther. Few-shot diffu-\nsion models. In NIPS 2022 Workshop, 2022.\n[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\nGenerative adversarial nets. Advances in Neural Informa-\ntion Processing Systems, 27, 2014.\n[19] Y. Gu, X. Wang, J. Z. Wu, Y. Shi, Y. Chen, Z. Fan, W. Xiao,\nR. Zhao, S. Chang, W. Wu, Y. Ge, Y. Shan, and M. Z. Shou.\nMix-of-show: Decentralized low-rank adaptation for multi-\nconcept customization of diffusion models, 2023.\n14\n[20] W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and\nF. Wood. Flexible diffusion modeling of long videos. Ad-\nvances in Neural Information Processing Systems, 2022.\n[21] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum\ncontrast for unsupervised visual representation learning. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9729\u20139738, 2020.\n[22] T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse,\nJ. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray, et al.\nScaling laws for autoregressive generative modeling. arXiv\npreprint arXiv:2010.14701, 2020.\n[23] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and\nS. Hochreiter. Gans trained by a two time-scale update rule\nconverge to a local nash equilibrium. Advances in Neural\nInformation Processing Systems, 30, 2017.\n[24] T. Hinz, S. Heinrich, and S. Wermter.\nSemantic object\naccuracy for generative text-to-image synthesis.\nIEEE\ntransactions on pattern analysis and machine intelligence,\n44(3):1552\u20131565, 2020.\n[25] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion proba-\nbilistic models. Advances in Neural Information Processing\nSystems, 33:6840\u20136851, 2020.\n[26] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and\nT. Salimans. Cascaded diffusion models for high fidelity\nimage generation. J. Mach. Learn. Res., 23:47\u20131, 2022.\n[27] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and\nD. J. Fleet. Video diffusion models. Advances in Neural\nInformation Processing Systems, 2022.\n[28] E. Hoogeboom, A. A. Gritsenko, J. Bastings, B. Poole,\nR. v. d. Berg, and T. Salimans. Autoregressive diffusion\nmodels. In International Conference on Learning Repre-\nsentations, 2022.\n[29] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling.\nEquivariant diffusion for molecule generation in 3d. In In-\nternational Conference on Machine Learning, pages 8867\u2013\n8887. PMLR, 2022.\n[30] C.-W. Huang, J. H. Lim, and A. C. Courville. A variational\nperspective on diffusion-based generative models and score\nmatching. Advances in Neural Information Processing Sys-\ntems, 34:22863\u201322876, 2021.\n[31] A. Jain, B. Mildenhall, J. T. Barron, P. Abbeel, and\nB. Poole.\nZero-shot text-guided object generation with\ndream fields. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 867\u2013\n876, 2022.\n[32] J. Johnson, A. Alahi, and L. Fei-Fei.\nPerceptual losses\nfor real-time style transfer and super-resolution. In Pro-\nceedings of the European Conference on Computer Vision,\npages 694\u2013711. Springer, 2016.\n[33] T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating\nthe design space of diffusion-based generative models. Ad-\nvances in Neural Information Processing Systems, 2022.\n[34] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and\nT. Aila. Training generative adversarial networks with lim-\nited data. Advances in Neural Information Processing Sys-\ntems, 33:12104\u201312114, 2020.\n[35] T. Karras, M. Aittala, S. Laine, E. H\u00a8ark\u00a8onen, J. Hellsten,\nJ. Lehtinen, and T. Aila. Alias-free generative adversarial\nnetworks. Advances in Neural Information Processing Sys-\ntems, 34:852\u2013863, 2021.\n[36] T. Karras, S. Laine, and T. Aila. A style-based generator\narchitecture for generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4401\u20134410, 2019.\n[37] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen,\nand T. Aila. Analyzing and improving the image quality\nof stylegan. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8110\u2013\n8119, 2020.\n[38] D. Kingma, T. Salimans, B. Poole, and J. Ho. Variational\ndiffusion models. Advances in Neural Information Process-\ning Systems, 34:21696\u201321707, 2021.\n[39] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[40] D. P. Kingma and M. Welling. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\n[41] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro.\nDiffwave: A versatile diffusion model for audio synthesis.\nIn International Conference on Learning Representations,\n2021.\n[42] A. Krizhevsky, G. Hinton, et al. Learning multiple layers\nof features from tiny images. Master\u2019s thesis, University of\nTront, 2009.\n[43] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y.\nZhu. Multi-concept customization of text-to-image diffu-\nsion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 1931\u20131941,\n2023.\n[44] B. Li, X. Qi, T. Lukasiewicz, and P. Torr. Controllable text-\nto-image generation. Advances in Neural Information Pro-\ncessing Systems, 32, 2019.\n[45] H. Li, Y. Yang, M. Chang, S. Chen, H. Feng, Z. Xu, Q. Li,\nand Y. Chen. Srdiff: Single image super-resolution with\ndiffusion probabilistic models. Neurocomputing, 479:47\u2013\n59, 2022.\n[46] W. Li, P. Zhang, L. Zhang, Q. Huang, X. He, S. Lyu, and\nJ. Gao. Object-driven text-to-image synthesis via adversar-\nial training. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12174\u2013\n12182, 2019.\n[47] X. L. Li, J. Thickstun, I. Gulrajani, P. Liang, and T. B.\nHashimoto. Diffusion-lm improves controllable text gener-\nation. Advances in Neural Information Processing Systems,\n2022.\n15\n[48] Y. Li, R. Zhang, J. Lu, and E. Shechtman. Few-shot image\ngeneration with elastic weight consolidation. Advances in\nNeural Information Processing Systems, 33:15885\u201315896,\n2020.\n[49] X. Liu, L. Wu, M. Ye, and Q. Liu. Let us build bridges:\nUnderstanding and extending diffusion generative models.\nIn NIPS 2022 Workshop, 2022.\n[50] T. J. Liu S, Guo H. Molecular geometry pretraining with se\n(3)-invariant denoising distance matching. In International\nConference on Learning Representations, 2023.\n[51] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-\nsolver: A fast ode solver for diffusion probabilistic model\nsampling in around 10 steps. Advances in Neural Informa-\ntion Processing Systems, 2022.\n[52] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-\nsolver++: Fast solver for guided sampling of diffusion\nprobabilistic models.\narXiv preprint arXiv:2211.01095,\n2022.\n[53] C. Luo. Understanding diffusion models: A unified per-\nspective. arXiv preprint arXiv:2208.11970, 2022.\n[54] S. Luo and W. Hu. Diffusion probabilistic models for 3d\npoint cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2837\u20132845, 2021.\n[55] A. Ly, M. Marsman, J. Verhagen, R. Grasman, and E. J.\nWagenmakers. A tutorial on fisher information. Journal of\nMathematical Psychology, 80:40\u201355, 2017.\n[56] Z. Lyu, Z. Kong, X. Xu, L. Pan, and D. Lin.\nA con-\nditional point diffusion-refinement paradigm for 3d point\ncloud completion. In International Conference on Learn-\ning Representations, 2022.\n[57] S. Mo, M. Cho, and J. Shin. Freeze the discriminator: A\nsimple baseline for fine-tuning gans. In CVPR AI for Con-\ntent Creation Workshop, 2020.\n[58] A. Q. Nichol and P. Dhariwal. Improved denoising diffu-\nsion probabilistic models. In International Conference on\nMachine Learning, pages 8162\u20138171. PMLR, 2021.\n[59] A. Noguchi and T. Harada. Image generation from small\ndatasets via batch statistics adaptation. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 2750\u20132758, 2019.\n[60] U. Ojha, Y. Li, J. Lu, A. A. Efros, Y. J. Lee, E. Shechtman,\nand R. Zhang. Few-shot image generation via cross-domain\ncorrespondence. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n10743\u201310752, 2021.\n[61] A. v. d. Oord, Y. Li, and O. Vinyals.\nRepresentation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018.\n[62] M. \u00a8Ozbey, S. U. Dar, H. A. Bedel, O. Dalmaz, S\u00b8. \u00a8Ozturk,\nA. G\u00a8ung\u00a8or, and T. C\u00b8 ukur.\nUnsupervised medical im-\nage translation with adversarial diffusion models.\narXiv\npreprint arXiv:2207.08208, 2022.\n[63] T. Qiao, J. Zhang, D. Xu, and D. Tao.\nLearn, imagine\nand create: Text-to-image generation from prior knowl-\nedge. Advances in neural information processing systems,\n32, 2019.\n[64] T. Qiao, J. Zhang, D. Xu, and D. Tao. Mirrorgan: Learning\ntext-to-image generation by redescription. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 1505\u20131514, 2019.\n[65] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\nG. Krueger, and I. Sutskever. Learning transferable visual\nmodels from natural language supervision, 2021.\n[66] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen.\nHierarchical text-conditional image generation with clip la-\ntents. arXiv preprint arXiv:2204.06125, 2022.\n[67] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Rad-\nford, M. Chen, and I. Sutskever. Zero-shot text-to-image\ngeneration. In International Conference on Machine Learn-\ning, pages 8821\u20138831. PMLR, 2021.\n[68] K. Rasul, C. Seward, I. Schuster, and R. Vollgraf. Autore-\ngressive denoising diffusion models for multivariate proba-\nbilistic time series forecasting. In International Conference\non Machine Learning, pages 8857\u20138868. PMLR, 2021.\n[69] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic\nbackpropagation and approximate inference in deep gen-\nerative models. In International Conference on Machine\nLearning, pages 1278\u20131286. PMLR, 2014.\n[70] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer. High-resolution image synthesis with latent dif-\nfusion models, 2021.\n[71] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer. High-resolution image synthesis with latent dif-\nfusion models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n10684\u201310695, 2022.\n[72] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and\nK. Aberman. Dreambooth: Fine tuning text-to-image dif-\nfusion models for subject-driven generation. 2022.\n[73] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans,\nD. Fleet, and M. Norouzi. Palette: Image-to-image diffu-\nsion models. In ACM SIGGRAPH 2022 Conference Pro-\nceedings, pages 1\u201310, 2022.\n[74] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Den-\nton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan,\nT. Salimans, et al.\nPhotorealistic text-to-image diffusion\nmodels with deep language understanding.\nAdvances in\nNeural Information Processing Systems, 35:36479\u201336494,\n2022.\n[75] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and\nM. Norouzi.\nImage super-resolution via iterative refine-\nment. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2022.\n16\n[76] A. Sinha, J. Song, C. Meng, and S. Ermon.\nD2c:\nDiffusion-decoding models for few-shot conditional gener-\nation. Advances in Neural Information Processing Systems,\n34:12533\u201312548, 2021.\n[77] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and\nS. Ganguli.\nDeep unsupervised learning using nonequi-\nlibrium thermodynamics. In International Conference on\nMachine Learning, pages 2256\u20132265, 2015.\n[78] K. Sohn, N. Ruiz, K. Lee, D. C. Chin, I. Blok, H. Chang,\nJ. Barber, L. Jiang, G. Entis, Y. Li, et al.\nStyledrop:\nText-to-image generation in any style.\narXiv preprint\narXiv:2306.00983, 2023.\n[79] K. Sohn, A. Shaw, Y. Hao, H. Zhang, L. Polania,\nH. Chang, L. Jiang, and I. Essa.\nLearning disentangled\nprompts for compositional image synthesis. arXiv preprint\narXiv:2306.00763, 2023.\n[80] J. Song, C. Meng, and S. Ermon. Denoising diffusion im-\nplicit models.\nIn International Conference on Learning\nRepresentations, 2021.\n[81] Y. Song and S. Ermon. Improved techniques for training\nscore-based generative models. Advances in Neural Infor-\nmation Processing Systems, 33:12438\u201312448, 2020.\n[82] M. Tao, H. Tang, S. Wu, N. Sebe, X.-Y. Jing, F. Wu,\nand B. Bao.\nDf-gan: Deep fusion generative adversar-\nial networks for text-to-image synthesis.\narXiv preprint\narXiv:2008.05865, 2020.\n[83] N.-T. Tran, V.-H. Tran, N.-B. Nguyen, T.-K. Nguyen, and\nN.-M. Cheung.\nOn data augmentation for gan training.\nIEEE Transactions on Image Processing, 30:1882\u20131897,\n2021.\n[84] A. Vahdat and J. Kautz. Nvae: A deep hierarchical varia-\ntional autoencoder. Advances in Neural Information Pro-\ncessing Systems, 33:19667\u201319679, 2020.\n[85] A. Vahdat, K. Kreis, and J. Kautz. Score-based generative\nmodeling in latent space. Advances in Neural Information\nProcessing Systems, 34:11287\u201311302, 2021.\n[86] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,\nA. Graves, et al. Conditional image generation with pixel-\ncnn decoders. Advances in Neural Information Processing\nSystems, 29, 2016.\n[87] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is\nall you need. Advances in neural information processing\nsystems, 30, 2017.\n[88] X. Wang and X. Tang.\nFace photo-sketch synthesis and\nrecognition. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 31(11):1955\u20131967, 2008.\n[89] Y. Wang, A. Gonzalez-Garcia, D. Berga, L. Herranz, F. S.\nKhan, and J. v. d. Weijer. Minegan: Effective knowledge\ntransfer from gans to target domains with few images. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9332\u20139341, 2020.\n[90] Y. Wang, C. Wu, L. Herranz, J. van de Weijer, A. Gonzalez-\nGarcia, and B. Raducanu. Transferring gans: Generating\nimages from limited data. In Proceedings of the European\nConference on Computer Vision, pages 218\u2013234, 2018.\n[91] Z. Wang, H. Zheng, P. He, W. Chen, and M. Zhou.\nDiffusion-gan: Training gans with diffusion. In Interna-\ntional Conference on Learning Representations, 2023.\n[92] J. Xiao, L. Li, C. Wang, Z.-J. Zha, and Q. Huang. Few\nshot generative model adaption via relaxed spatial structural\nalignment. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 11204\u2013\n11213, 2022.\n[93] Z. Xiao, K. Kreis, and A. Vahdat. Tackling the generative\nlearning trilemma with denoising diffusion gans. In Inter-\nnational Conference on Learning Representations, 2022.\n[94] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang.\nGeodiff: A geometric diffusion model for molecular con-\nformation generation.\nIn International Conference on\nLearning Representations, 2022.\n[95] R. Yang, P. Srivastava, and S. Mandt.\nDiffusion prob-\nabilistic modeling for video generation.\narXiv preprint\narXiv:2203.09481, 2022.\n[96] J. Yaniv, Y. Newman, and A. Shamir.\nThe face of art:\nLandmark detection and geometric style in portraits. ACM\nTransactions on Graphics (TOG), 38(4):1\u201315, 2019.\n[97] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and\nJ. Xiao. Lsun: Construction of a large-scale image dataset\nusing deep learning with humans in the loop. arXiv preprint\narXiv:1506.03365, 2015.\n[98] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Va-\nsudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autore-\ngressive models for content-rich text-to-image generation.\narXiv preprint arXiv:2206.10789, 2022.\n[99] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang,\nand Z. Liu.\nMotiondiffuse:\nText-driven human mo-\ntion generation with diffusion model.\narXiv preprint\narXiv:2208.15001, 2022.\n[100] Q. Zhang and Y. Chen. Fast sampling of diffusion models\nwith exponential integrator. In NIPS 2022 Workshop, 2022.\n[101] Q. Zhang, M. Tao, and Y. Chen.\ngddim:\nGeneral-\nized denoising diffusion implicit models.\narXiv preprint\narXiv:2206.05564, 2022.\n[102] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and\nO. Wang. The unreasonable effectiveness of deep features\nas a perceptual metric. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, pages\n586\u2013595, 2018.\n[103] Z. Zhang, Y. Xie, and L. Yang. Photographic text-to-image\nsynthesis with a hierarchically-nested adversarial network.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 6199\u20136208, 2018.\n[104] S. Zhao, Z. Liu, J. Lin, J.-Y. Zhu, and S. Han. Differentiable\naugmentation for data-efficient gan training.\nAdvances\nin Neural Information Processing Systems, 33:7559\u20137570,\n2020.\n17\n[105] Y. Zhao, K. Chandrasegaran, M. Abdollahzadeh, and N.-M.\nCheung. Few-shot image generation via adaptation-aware\nkernel modulation. Advances in Neural Information Pro-\ncessing Systems, 2022.\n[106] Y. Zhao, H. Ding, H. Huang, and N.-M. Cheung. A closer\nlook at few-shot image generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9140\u20139150, 2022.\n[107] Z. Zhao, Z. Zhang, T. Chen, S. Singh, and H. Zhang.\nImage augmentations for gan training.\narXiv preprint\narXiv:2006.02595, 2020.\n[108] L. Zhou, Y. Du, and J. Wu. 3d shape generation and com-\npletion through point-voxel diffusion. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 5826\u20135835, 2021.\n18\nA\nSupplementary Evaluation\nIn Table 6, we provide the additional results of Intra-\nLPIPS [60] on 10-shot FFHQ \u2192 Sketches and FFHQ\n\u2192 Amedeo\u2019s paintings (unconditional) as supplements to\nTable 2.\nThe proposed DomainStudio approach outper-\nforms modern GAN-based approaches and directly fine-\ntuned DDPMs on generation diversity under these adapta-\ntion setups as well.\nFID results of GAN-based baselines in Table 3 are re-\nported in CDC [60] and DCL [106].\nB\nSupplementary Ablation Analysis\nIn this section, we provide the ablation analysis of the\nweight coefficients of Limg, Lhf, and Lhfmse using 10-\nshot FFHQ \u2192 Babies (unconditional) as an example. Intra-\nLPIPS and FID are employed for quantitative evaluation.\nWe first ablate \u03bb2, the weight coefficient of Limg. We\nadapt the source model to 10-shot Babies without Lhf and\nLhfmse. The quantitative results are listed in Table 7. Cor-\nresponding generated samples are shown in Fig. 15. When\n\u03bb2 is set as 0.0, the directly fine-tuned model produces\ncoarse results lacking high-frequency details and diver-\nsity. With an appropriate choice of \u03bb2, the adapted model\nachieves greater generation diversity and better learning of\ntarget distributions under the guidance of Limg. Too large\nvalues of \u03bb2 make Limg overwhelm Lsimple and prevent the\nadapted model from learning target distributions, leading\nto degraded generation quality and diversity. The adapted\nmodel with \u03bb2 value of 2.5 gets unnatural generated sam-\nples even if it achieves the best FID result. We recommend\n\u03bb2 ranging from 0.1 to 1.0 for the adaptation setups used\nin our paper based on a comprehensive consideration of the\nqualitative and quantitative evaluation.\nNext, we ablate \u03bb3, the weight coefficient of Lhf with \u03bb2\nset as 0.5. The quantitative results are listed in Table 8. Cor-\nresponding generated samples are shown in Fig. 16. Lhf\nguides adapted models to keep diverse high-frequency de-\ntails learned from source domains for more realistic results.\nLhf helps the adapted model enhance details like clothes\nand hairstyles and achieves better FID and Intra-LPIPS, in-\ndicating improved quality and diversity. Too large values\nof \u03bb3 make the adapted model pay too much attention to\nhigh-frequency components and fail to produce realistic re-\nsults following the target distributions. We recommend \u03bb3\nranging from 0.1 to 1.0 for the adaptation setups used in our\npaper.\nFinally, we ablate \u03bb4, the weight coefficient of Lhfmse,\nwith \u03bb2 and \u03bb3 set as 0.5. The quantitative results are listed\nin Table 9. Corresponding generated samples are shown in\nFig. 17. Lhfmse guides the adapted model to learn more\nApproaches\nFFHQ \u2192\nSketches\nFFHQ \u2192\nAmedeo\u2019s paintings\nTGAN [90]\n0.394 \u00b1 0.023\n0.548 \u00b1 0.026\nTGAN+ADA [34]\n0.427 \u00b1 0.022\n0.560 \u00b1 0.019\nFreezeD [57]\n0.406 \u00b1 0.017\n0.597 \u00b1 0.032\nMineGAN [89]\n0.407 \u00b1 0.020\n0.614 \u00b1 0.027\nEWC [48]\n0.430 \u00b1 0.018\n0.594 \u00b1 0.028\nCDC [60]\n0.454 \u00b1 0.017\n0.620 \u00b1 0.029\nDCL [106]\n0.461 \u00b1 0.021\n0.616 \u00b1 0.043\nFine-tuned DDPMs\n0.473 \u00b1 0.022\n0.484 \u00b1 0.021\nDomainStudio (ours) 0.495 \u00b1 0.024\n0.495 \u00b1 0.024\n0.495 \u00b1 0.024\n0.626 \u00b1 0.022\n0.626 \u00b1 0.022\n0.626 \u00b1 0.022\nTable 6: Intra-LPIPS (\u2191) results of DDPM-based approaches and\nGAN-based baselines on 10-shot FFHQ \u2192 Sketches and FFHQ\n\u2192 Amedeo\u2019s paintings. Standard deviations are computed across\n10 clusters (the same number as training samples).\nFigure 15: Visualized ablations of \u03bb2, the weight coefficient of\nLimg on 10-shot FFHQ \u2192 Babies. Samples of different models\nare synthesized from fixed noise inputs.\nhigh-frequency details from limited training data. Appro-\npriate choice of \u03bb4 helps the adapted model generate diverse\nresults containing rich details. Besides, the full Domain-\nStudio approach achieves state-of-the-art results of FID and\nIntra-LPIPS on 10-shot FFHQ \u2192 Babies (see Table 2 and\n3). Similar to \u03bb2 and \u03bb3, too large values of \u03bb4 lead to\n19\n\u03bb2\nIntra-LPIPS (\u2191)\nFID (\u2193)\n0.0\n0.520 \u00b1 0.026\n89.94\n0.004\n0.531 \u00b1 0.031\n76.17\n0.02\n0.544 \u00b1 0.026\n69.49\n0.1\n0.558 \u00b1 0.033\n60.00\n0.5\n0.572 \u00b1 0.027\n0.572 \u00b1 0.027\n0.572 \u00b1 0.027\n55.97\n1.0\n0.560 \u00b1 0.034\n58.29\n2.5\n0.543 \u00b1 0.038\n46.82\n46.82\n46.82\n5.0\n0.537 \u00b1 0.028\n52.42\nTable 7: Intra-LPIPS (\u2191) and FID (\u2193) results of adapted models\ntrained on 10-shot FFHQ \u2192 Babies with different \u03bb2, the weight\ncoefficient of Limg.\nFigure 16: Visualized ablations of \u03bb3, the weight coefficient of\nLhf on 10-shot FFHQ \u2192 Babies. Samples of different models are\nsynthesized from fixed noise inputs.\nunreasonable results deviating from the target distributions.\nWe recommend \u03bb4 ranging from 0.01 to 0.08 for the adap-\ntation setups in this paper.\nC\nLimitations\nDespite the compelling results of our approach, it still\nhas some limitations. All the datasets used in this paper\nshare the resolution of 256 \u00d7 256. The experiments of Do-\n\u03bb3\nIntra-LPIPS (\u2191)\nFID (\u2193)\n0.0\n0.572 \u00b1 0.027\n55.97\n0.004\n0.576 \u00b1 0.034\n48.92\n48.92\n48.92\n0.02\n0.581 \u00b1 0.045\n57.39\n0.1\n0.589 \u00b1 0.047\n53.14\n0.5\n0.592 \u00b1 0.031\n0.592 \u00b1 0.031\n0.592 \u00b1 0.031\n53.11\n1.0\n0.583 \u00b1 0.032\n50.04\n2.5\n0.577 \u00b1 0.032\n54.60\n5.0\n0.591 \u00b1 0.031\n54.61\nTable 8: Intra-LPIPS (\u2191) and FID (\u2193) results of adapted models\ntrained on 10-shot FFHQ \u2192 Babies with different \u03bb3, the weight\ncoefficient of Lhf.\nFigure 17: Visualized ablations of \u03bb4, the weight coefficient of\nLhfmse on 10-shot FFHQ \u2192 Babies. Samples of different models\nare synthesized from fixed noise inputs.\n\u03bb4\nIntra-LPIPS (\u2191)\nFID (\u2193)\n0.0\n0.592 \u00b1 0.031\n53.11\n0.01\n0.594 \u00b1 0.038\n47.54\n0.05\n0.599 \u00b1 0.024\n33.26\n33.26\n33.26\n0.08\n0.607 \u00b1 0.025\n38.93\n0.1\n0.603 \u00b1 0.031\n44.23\n0.5\n0.612 \u00b1 0.023\n0.612 \u00b1 0.023\n0.612 \u00b1 0.023\n53.69\nTable 9: Intra-LPIPS (\u2191) and FID (\u2193) results of adapted models\ntrained on 10-shot FFHQ \u2192 Babies with different \u03bb4, the weight\ncoefficient of Lhfmse.\nmainStudio are conducted on NVIDIA RTX A6000 GPUs\n(48 GB memory of each). However, the batch size on each\nGPU is still limited to 3 for unconditional image generation\n20\nFigure 18: Image samples synthesized by CDC [60] using image-level information on 10-shot FFHQ \u2192 Sunglasses and FFHQ \u2192 Babies.\nand 4 for text-to-image generation. Therefore, it is chal-\nlenging to expand our approach to larger image resolution.\nWe will work on more lightweight DDPM-based few-shot\nimage generation approaches.\nDespite that, the datasets\nused in this paper have larger resolution than many DDPM-\nbased works [17, 58, 2, 8, 38, 101] which use datasets with\nresolution 32 \u00d7 32 and 64 \u00d7 64. For text-to-image genera-\ntion, DomainStudio on Stable Diffuison [70] can synthesize\nimages with super-resolution (512 \u00d7 512 or 1024 \u00d7 1024).\nAlthough we have designed methods for high-frequency\ndetails enhancement and achieved realistic results, there\nstill exists room for further improvement, especially when\ntarget domains contain apparently more high-frequency\ncomponents than source domains (e.g., LSUN Church \u2192\nHaunted houses). Besides, DomainStudio still cannot fully\nreproduce the styles of some abstract target domains while\nmaintaining generation diversity. Nevertheless, our work\nfirst introduces diffusion models to few-shot image genera-\ntion tasks and has improved generation quality and diversity\ncompared with existing GAN-based approaches. We hope\nit will be a solid basis for better methods in the future.\nD\nInspiration of Our Approach\nD.1\nLimg & Lhf\nThe proposed pairwise similarity loss designed for\nDDPMs is mainly inspired by the methods in contrastive\nlearning [61, 21, 7], which build probability distributions\nbased on similarities. Similar methods are applied to GAN-\nbased approaches, including CDC [60] and DCL [106] as\nwell. The pairwise similarity loss maintains generation di-\nversity by keeping the distributions of relative pairwise dis-\ntances between adapted samples similar to source samples\nduring domain adaptation. In this way, the generated sam-\nples are prevented from being too similar to each other or\nreplicating training samples. Instead, the adapted model is\nencouraged to learn the common features of training sam-\nples and preserve information learned from source domains,\nimproving generation quality and diversity.\nGAN-based approaches depend on perceptual features in\nthe generator and discriminator to compute similarity and\nMethod\nFID (\u2193)\nIntra-LPIPS (\u2191)\nTime / 1K iters (\u2193)\nOurs\n37.92\n37.92\n37.92\n0.59 \u00b1 0.02\n0.59 \u00b1 0.02\n0.59 \u00b1 0.02\n34min\n34min\n34min\nDesign A\n40.30\n0.55 \u00b1 0.03\n52min\nDesign B\n58.28\n0.57 \u00b1 0.06\n38min\nTable 10: Quantitative evaluation comparison between different\ndesigns for the pairwise similarity loss.\nprobability distributions. As for the proposed DomainStu-\ndio approach, the predicted input images \u02dcx0 calculated in\nterms of xt and \u03f5\u03b8(xt, t) (Equation 2) are applied in re-\nplacement of perceptual features used for GANs. Besides,\nthe high-frequency components of \u02dcx0 are applied to pair-\nwise similarity loss calculation for high-frequency details\nenhancement. DomainStudio directly uses image-level in-\nformation to preserve the relative pairwise distances be-\ntween adapted samples and during domain adaptation.\nWe tried to use features in diffusion processes (Design\nA) and images of several diffusion steps (Design B) for\npairwise similarity loss calculation. As shown in Table 10\n(FID evaluation on FFHQ \u2192 Sunglasses, Intra-LPIPS eval-\nuation on 10-shot FFHQ \u2192 Sunglasses), the proposed loss\ndesign using image-level information directly is simple, ef-\nfective, inexpensive, and achieves the best quality and di-\nversity. Here we do not include high-frequency details en-\nhancement for fair comparison.\nAs illustrated in Sec.\n6, DomainStudio synthesizes\nmore realistic images with fewer blurs and artifacts and\nachieves better generation diversity than current state-of-\nthe-art GAN-based approaches [60, 106]. We also try to\nuse image-level information to replace the perceptual fea-\ntures for the GAN-based approach CDC [60]. However, we\nfail to avoid generating artifacts or achieve higher genera-\ntion quality, as shown in Fig. 18. The proposed pairwise\nsimilarity loss matches better with diffusion models than\nGANs.\nD.2\nLhfmse\nDDPMs learn target distributions mainly through mean\nvalues of predicted noises using the reweighted loss func-\ntion (Equation 1). As a result, it is hard for DDPMs to learn\n21\nFigure 19: Examples of small-scale datasets sampled from Babies, Sunglasses, and LSUN Church. For datasets containing 100 or 1000\nimages, we randomly pick 15 examples.\nhigh-frequency distributions from limited data, as shown in\nthe smooth samples produced by models trained on limited\ndata from scratch in Fig. 20. Therefore, we propose Lhfmse\nto strengthen the learning of high-frequency details from\nlimited data during domain adaptation.\nD.3\nFull Approach\nPrior GAN-based approaches like CDC [60] and DCL\n[106] aim to build a one-to-one correspondence between\nsource and target samples. DomainStudio focuses on gener-\nating realistic and diverse results following target distribu-\ntions. Building a one-to-one correspondence is not the first\nconsideration of DomainStudio.\nE\nComparison with ZADIS and StyleDrop\nZADIS [79] and StyleDrop [78] are contemporary to this\npaper and share similar targets with the proposed Domain-\nStudio approach.\nZADIS is based on MaskGIT [6] and\nlearns visual prompts for target domains/styles. In this way,\nZADIS realizes compositional image synthesis with disen-\ntangled prompts for style and subjects. StyleDrop is based\non MUSE [5] and synthesizes images with user-provided\nstyles using reference images and descriptive style descrip-\ntors for training under the guidance of CLIP [65] scores and\nhuman feedback. DomainStudio is designed for diffusion\nmodels and compatible with typical unconditional DDPMs\n[77, 25] and modern large text-to-image models like Stable\nDiffusion [70]. DomainStudio aims to learn the common\nfeatures of target domains, which may be artistic styles or\nproperties like sunglasses.\nF\nMore Details of Implementation\nF.1\nUnconditional DDPMs\nWe follow the model setups of DDPMs used in prior\nworks [58] for LSUN 2562 [97] datasets. All the DDPM-\nbased models used in this paper are implemented based on\nthe same codebase [58, 12] and share the same model struc-\nture for fair comparison under different adaptation setups\nand optimization targets. All the source and target datasets\nare modified to the resolution of 256 \u00d7 256. We use a max\ndiffusion step T of 1000 and a dropout rate of 0.1. The mod-\nels are trained to learn the variance with Lvlb. The Adam\noptimizer [39] is employed to update the trainable param-\neters. We set the learning rate as 0.001 and apply the lin-\n22\nFigure 20: Image samples produced by DDPMs trained from scratch on small-scale datasets, including Babies, Sunglasses, and LSUN\nChurch containing 10, 100, and 1000 images.\near noise addition schedule. Besides, we use half-precision\n(FP16) binary floating-point format to save memory and\nmake it possible to use a larger batch size in our experiments\n(batch size 6 for directly fine-tuned DDPMs and batch size 3\nfor DomainStudio per NVIDIA RTX A6000 GPU). All the\nresults produced by DDPM-based models in this paper fol-\nlow the sampling process proposed in Ho et al.\u2019s work [25]\n(about 21 hours needed to generate 1000 samples on a sin-\ngle NVIDIA RTX A6000 GPU) without any fast sampling\nmethods [80, 101, 51, 52, 100, 33]. The weight coefficient\n\u03bb2, \u03bb3, and \u03bb4 are set as 0.5, 0.5, 0.05 for the quantitative\nevaluation results of DomainStudio listed in Table 2, 3, and\n6.\nF.2\nText-to-Image DDPMs\nWe follow DreamBooth [72] to set the learning rates of\nDomainStudio ranging from 1e-6 to 5e-6. Experiments of\nDreamBooth and DomainStudio share the same hyperpa-\nrameters in training for fair comparison. Textual Inversion\n[16] sets the learning rate as 5e-4 and trains text prompts for\n2K-3K iterations. The image resolution used for training is\n256\u00d7256.\nF.3\nGAN-based Baselines\nWe employ several GAN-based few-shot image genera-\ntion approaches as baselines for comparison with the pro-\nposed DomainStudio approach. Here we provide more de-\ntails of these baselines. We implement all these approaches\nbased on the same codebase of StyleGAN2 [37]. The source\nmodels are fine-tuned directly on the target datasets to re-\nalize TGAN [90]. TGAN+ADA applies ADA [34] aug-\nmentation method to the TGAN baseline.\nFor FreezeD\n[57], the first 4 high-resolution layers of the discrimina-\ntor are frozen following the ablation analysis provided in\ntheir work. The results of MineGAN [89] and CDC [60]\nare produced through their official implementation. As for\nEWC [48] and DCL [106], we implement these approaches\nfollowing formulas and parameters in their papers since\nthere is no official implementation. These GAN-based ap-\nproaches are designed for generators [89, 48, 60, 106] and\ndiscriminators [34, 57, 106] specially and cannot be ex-\npanded to DDPMs directly.\n23\nFigure 21: Visualized comparison between DDPMs that achieve\nthe best FID results and DDPMs trained for 60K iterations on\n1000-shot Babies and LSUN Church. Image samples produced\nby different models are synthesized from fixed noise inputs.\nThe adapted GANs are trained for 1K-3K iterations.\nWhen evaluating generation diversity using Intra-LPIPS\n[60] for the 10-shot adaptation tasks listed in Tables 2 and\n6, we apply fixed noise inputs to different GAN-based ap-\nproaches for fair comparison.\nApart from the GAN-based baselines illustrated above,\nRSSA [92] and AdAM [105] provide different research per-\nspectives for few-shot image generation. RSSA [92] pre-\nserves the image structure learned from source domains\nwith a relaxed spatial structural alignment method, which\nis inappropriate for abstract target domains like artists\u2019\npaintings. AdAM [105] focuses on unrelated source/target\ndomains with an adaptation-aware kernel modulation ap-\nproach.\nG\nDDPMs Trained from Scratch\nIn Section 3, we evaluate the performance of DDPMs\ntrained from scratch on small-scale datasets containing 10,\n100, and 1000 images.\nIn our experiments, the smaller\ndatasets are included in the larger datasets. For example,\n1000-shot Sunglasses includes all the images in 100-shot\nand 10-shot Sunglasses. Similarly, all the images in 10-\nshot Sunglasses are included in 100-shot Sunglasses as well.\nWe train DDPMs for 40K iterations (about 20 hours on \u00d78\nNVIDIA RTX A6000 GPUs) on datasets containing 10 or\n100 images. While for datasets containing 1000 images,\nDDPMs are trained for 60K iterations (about 30 hours on\n\u00d78 NVIDIA RTX A6000 GPUs).\nWe provide several typical examples randomly picked\nfrom the small-scale datasets in Fig. 19. Compared with\nthe generated images shown in Fig. 20, it can be seen that\nDDPMs trained from scratch need enough training samples\nIterations (K)\nBabies\nSunglasses\nLSUN Church\n0\n444.35\n419.75\n424.53\n10\n444.91\n419.38\n413.68\n20\n222.81\n348.48\n385.25\n30\n90.16\n90.16\n90.16\n168.62\n388.81\n40\n124.97\n82.48\n57.68\n50\n132.33\n68.26\n45.43\n45.43\n45.43\n60\n132.32\n66.09\n66.09\n66.09\n69.18\nTable 11: FID (\u2193) results of DDPMs trained for different iterations\nfrom scratch on 1000-shot Babies, Sunglasses, and LSUN Church.\nApproaches\nFFHQ\nLSUN Church\nStyleGAN2\n0.6619 \u00b1 0.0581\n0.7144 \u00b1 0.0537\nDDPM\n0.6631 \u00b1 0.0592\n0.6631 \u00b1 0.0592\n0.6631 \u00b1 0.0592\n0.7153 \u00b1 0.0513\n0.7153 \u00b1 0.0513\n0.7153 \u00b1 0.0513\nTable 12: Average pairwise LPIPS (\u2191) results of 1000 samples\nproduced by StyleGAN2 and DDPMs trained on FFHQ 2562 and\nLSUN Church 2562.\n(e.g., 1000 images) to synthesize diverse results and avoid\nreplicating the training samples. Detailed FID [23] results\nof DDPMs trained from scratch on Babies, Sunglasses, and\nLSUN Church containing 1000 images for 60K iterations\nare shown in Table 11. DDPMs trained from scratch on\n1000-shot Babies, Sunglasses, and LSUN Church achieve\nthe best FID results at 30K, 60K, and 50K iterations, re-\nspectively.\nIn Fig. 20, we provide samples generated by DDPMs\ntrained from scratch for 60K iterations on all three 1000-\nshot Babies and LSUN Church datasets. In addition, we\nadd the generated samples of models trained from scratch\non 1000-shot Babies and LSUN Church, which achieve the\nbest FID results for comparison in Fig. 21. We do not in-\nclude samples for all three datasets since the model trained\nfor 60K iterations on 1000-shot Sunglasses achieves the\nbest FID result as well. The model trained on 1000-shot\nBabies for 60K iterations achieves smoother results con-\ntaining fewer blurs despite its worse FID result, while the\nmodel trained for 30K iterations achieves the best FID result\nand synthesizes more diverse images. As for the 1000-shot\nLSUN Church, the model trained for 50K iterations with the\nbest FID result produces samples containing more detailed\nstructures of churches than the model trained for 60K iter-\nations. However, all these results are still coarse and lack\nhigh-frequency details, indicating the necessity of adapting\nsource models to target domains when training data is lim-\nited.\nH\nDDPM-based Source Models\nWe train DDPMs on FFHQ 2562 [37] and LSUN Church\n2562 [97] from scratch for 300K iterations and 250K iter-\n24\nFigure 22: Image samples produced by DDPMs trained on FFHQ 2562 [37] (300K iterations) and LSUN Church 2562 [97] (250K\niterations).\nFigure 23: Image samples produced by DomainStudio models trained for different iterations on 10-shot FFHQ \u2192 Babies. All the visualized\nsamples of different models are synthesized from fixed noise inputs.\nations as source models for DDPM adaptation, which cost\n5 days and 22 hours, 4 days and 22 hours on \u00d78 NVIDIA\nRTX A6000 GPUs, respectively.\nImage samples produced by these two source models can\nbe found in Fig. 22. We randomly sample 1000 images\nwith these two models to evaluate their generation diversity\nusing the average pairwise LPIPS [102] metric, as shown\nin Table 12. For comparison, we also evaluate the genera-\ntion diversity of the source StyleGAN2 [37] models used by\nGAN-based baselines [90, 34, 57, 89, 48, 60, 106]. DDPMs\ntrained on FFHQ 2562 and LSUN Church 2562 achieve\ngeneration diversity similar to the widely-used StyleGAN2\n25\nApproaches\nFFHQ\nLSUN Church\nStyleGAN2\n7.71\n8.09\nDDPM\n7.00\n7.00\n7.00\n6.06\n6.06\n6.06\nTable 13: FID (\u2193) results of StyleGAN2 and DDPMs trained on\nFFHQ 2562 and LSUN Church 2562.\nFigure 24: Samples synthesized from fixed noise inputs by Do-\nmainStudio on 10-shot FFHQ \u2192 Babies. DomainStudio keeps the\nrelative pairwise distances during domain adaptation and achieves\ndiverse results containing high-frequency details.\nFigure 25: Samples synthesized by DomainStudio trained on 10-\nshot FFHQ \u2192 Sunglasses for 10000 iterations. Too many itera-\ntions lead to the replication of training samples and the degrada-\ntion of generation quality and diversity.\nmodels.\nBesides, we sample 5000 images to evaluate the genera-\ntion quality of the source models using FID [23]. As shown\nin Table 13, DDPM-based source models achieve FID re-\nsults similar to StyleGAN2 on the source datasets FFHQ\n2562 and LSUN Church 2562.\nI\nDDPM Adaptation Process Analysis\nThis paper mainly concentrates on the challenging 10-\nshot image generation tasks. When fine-tuning pre-trained\nDDPMs on target domains using limited data directly, too\nmany iterations lead to overfitting and seriously degraded\ndiversity.\nFine-tuned models trained for about 10K iter-\nations almost exclusively focus on replicating the train-\ning samples.\nTherefore, we train the directly fine-tuned\nDDPMs for 3K-4K iterations to adapt source models to tar-\nget domains and maintain diversity. However, the directly\nfine-tuned DDPMs still generate coarse samples lacking de-\ntails with reasonable iterations.\nIn Fig. 23, we provide samples produced by DomainStu-\ndio models trained for different iterations on 10-shot FFHQ\nFigure 26: Visualization of the low and high-frequency compo-\nnents obtained with Haar wavelet transformation using images\nfrom Babies and Sunglasses as examples. LL represents the low-\nfrequency components, and LH+HL+HH represents the sum of the\nhigh-frequency components.\n\u2192 Babies. We apply fixed noise inputs to different mod-\nels for comparison. As the iterations increase, the styles of\nthe generated images become closer to the training samples.\nImages synthesized from the same noise inputs as Fig. 4 are\nincluded in red boxes. In addition, detailed evaluation of co-\nsine similarity is added in Fig. 24. The source samples are\nadapted to the target domain while keeping relatively sta-\nble cosine similarity. Compared with the directly fine-tuned\nDDPMs, DomainStudio has a stronger ability to maintain\ngeneration diversity and achieve realistic results containing\nrich details. Nonetheless, too many iterations still lead to\ndetails missing and the degradation of quality and diver-\nsity, as shown by the samples of the DomainStudio model\ntrained on 10-shot FFHQ \u2192 Sunglasses for 10K iterations\nin Fig. 25. Therefore, we recommend choosing suitable\niterations for different adaptation setups (e.g., 4K-5K itera-\ntions for 10-shot FFHQ \u2192 Babies) to adapt the pre-trained\nmodels to target domains naturally and guarantee the high\nquality and great diversity of generated samples.\nJ\nHarr Wavelet Transformation Examples\nFig.\n26 visualizes several examples of Haar wavelet\ntransformation. The low-frequency components LL contain\nthe fundamental structures of images. High-frequency com-\nponents including LH, HL, and HH contain rich details like\ncontours and edges in images.\nK\nPersonalization of DomainStudio\nDomainStudio is designed to realize domain-driven gen-\neration, which differs from modern subject-driven ap-\nproaches like DreamBooth [72] and Textual Inversion [16].\nIn this section, we further explore the personalization of\nDomainStudio to satisfy both domain-driven and subject-\ndriven requests. Given two sets of images as reference for\nthe target subject and domain, we design two training strate-\ngies to realize the personalization of DomainStudio.\n26\nFigure 27: Overview of the one-stage personalization of DomainStudio. We combine DomainStudio with DreamBooth to achieve\npersonalized domain-driven image generation.\nOne-Stage Approach As illustrated in Fig. 27, the pro-\nposed DomainStudio can be combined with DreamBooth to\npersonalize domain-driven image generation. For example,\nwe use text prompts: \u201ca cat\u201d, \u201ca [V] cat\u201d, and \u201ca [V] cat in\nthe [S] style\u201d corresponding to the source domain, personal-\nized subject, and personalized subject in the target domain.\nCompared with the DomainStudio approach shown in Fig.\n7, we add the reconstruction loss of personalized subjects\nand compute the pairwise similarity loss between personal-\nized subjects and personalized subjects in the target domain.\nTwo-Stage Approach We can also divide the training\nprocess into two stages. Firstly, we follow DreamBooth to\ntrain the pre-trained text-to-image model to realize subject-\ndriven generation. They we use the target subject as source\ndomain and conduct DomainStudio to learn the knowledge\nof target domains while maintaining the personalized sub-\nject.\nIn our experiments, we find that these two approaches\nachieve similar results. We provide several personalized\ndomain-driven generation samples containing diverse sub-\njects and styles in Fig. 28. Our approach successfully adapt\nthe personalized subject to target domains under the guid-\nance of few-shot reference images. For instance, we adapt\nthe reference dog and cat to the watercolor style (first row\nof Fig. 28). Besides, we synthesize the reference vase and\nteapot in Van Gogh\u2019s style using 10-shot Van Gogh houses\nas domain reference (second row of Fig. 28).\nL\nAdditional Visualized Samples\nWe show all the 10-shot datasets used in this paper for\nfew-shot image generation tasks in Fig. 29, including 5 tar-\nget domains corresponding to the source domain FFHQ [37]\nand 2 target domains corresponding to LSUN Church [97].\nBesides, we provide image generation samples of GAN-\nbased baselines and DDPM-based approaches on 10-shot\nFFHQ \u2192 Babies, FFHQ \u2192 Raphael\u2019s paintings, and LSUN\nChurch \u2192 Landscape drawings in Fig. 30, 31, and 32 as\nsupplements to Fig.\n9.\nWe apply fixed noise inputs to\nGAN-based approaches and DDPM-based approaches, re-\n27\nFigure 28: Image samples produced through the personalization of DomainStudio.\nspectively. DDPMs are more stable and less vulnerable to\noverfitting than GANs. GANs easily overfit and tend to gen-\nerate samples similar to training samples when training data\nis limited (see samples of TGAN [90]). Directly fine-tuned\nDDPMs can still keep a measure of generation diversity un-\nder the same conditions. Besides, DDPM-based approaches\navoid the generation of blurs and artifacts. However, di-\nrectly fine-tuned DDPMs tend to produce too smooth re-\nsults lacking high-frequency details and still face diversity\ndegradation. DomainStudio generates more realistic results\ncontaining richer high-frequency details than GAN-based\nbaselines under all these adaptation setups.\nDomainStudio performs apparently better when we want\nto keep the fundamental structures of source images and\nlearn the styles of target domains (e.g., LSUN Church \u2192\nLandscape drawings). As shown in Fig. 32, GAN-based\napproaches fail to adapt real churches to the drawing style\nand produce samples containing too many blurs and arti-\nfacts.\nOn the other hand, DomainStudio produces high-\nquality church drawings and preserves more detailed build-\ning structures.\nFig. 33 shows results of DomainStudio on text-to-image\ngeneration using a single image as training data. It\u2019s hard\nto define the target domain accurately with a single image.\nWe recommend using 4-10 images to realize diverse, high-\nquality, and stable domain-driven text-to-image generation.\nM\nComputational Cost\nThe time cost of DDPMs and the proposed Domain-\nStudio approach are listed in Table 14.\nDomainStudio\ncosts 24.14% more training time than the original DDPMs.\nDDPMs trained from scratch need about 40K iterations\nto achieve reasonable results, even if they can only repli-\ncate the training samples. DomainStudio utilizes related\npre-trained models to accelerate convergence (about 3K-\n5K iterations) and significantly improve generation quality\nand diversity. Compared with directly fine-tuned DDPMs,\nDomainStudio is not overly time-consuming and achieves\nmore realistic results.\nThe time cost of DomainStudio applied to text-to-image\ngeneration is shown in Table 15. DomainStudio is more\n28\nFigure 29: All the 10-shot datasets used in this paper, including 5 target domains corresponding to FFHQ and 2 target domains correspond-\ning to LSUN Church.\nApproaches\nTime Cost / 1K Iterations\nDDPMs\n29 min\nDomainStudio (ours)\n36 min\nTable 14: The time cost of directly fine-tuned DDPMs (batch size\n48) and DomainStudio (batch size 24) models trained for 1K iter-\nations on \u00d78 NVIDIA RTX A6000 GPUs.\nApproaches\nTime Cost / 1500 Iterations\nTextual Inversion [16]\n33 min\nDreamBooth [72]\n8 min\nDomainStudio (ours)\n21 min\nTable 15: The time cost of fine-tuned Stable Diffusion trained for\n1.5K iterations on a single NVIDIA RTX A6000 GPU.\nefficient than Textual Inversion [16].\nIt is more time-\nconsuming than DreamBooth [72] since we add the compu-\ntation of pairwise distance maintenance and high-frequency\ndetails enhancement. However, it is still acceptable to fine-\ntune a model using about 20 minutes on a single GPU.\n29\nFigure 30: 10-shot image generation samples on FFHQ \u2192 Babies. All the samples of GAN-based approaches are synthesized from fixed\nnoise inputs (rows 1-9). Samples of the directly fine-tuned DDPM and DomainStudio are synthesized from fixed noise inputs as well (rows\n10-11).\n30\nFigure 31: 10-shot image generation samples on FFHQ \u2192 Raphael\u2019s paintings. All the samples of GAN-based approaches are synthesized\nfrom fixed noise inputs (rows 1-9). Samples of the directly fine-tuned DDPM and DomainStudio are synthesized from fixed noise inputs\nas well (rows 10-11).\n31\nFigure 32: 10-shot image generation samples on LSUN Church \u2192 Landscape drawings. All the samples of GAN-based approaches are\nsynthesized from fixed noise inputs (rows 1-9). Samples of the directly fine-tuned DDPM and DomainStudio are synthesized from fixed\nnoise inputs as well (rows 10-11).\n32\nFigure 33: 1-shot text-to-image generation samples of DomainStudio using different source domains.\n33\n"
  },
  {
    "title": "Aligning Large Multi-Modal Model with Robust Instruction Tuning",
    "link": "https://arxiv.org/pdf/2306.14565.pdf",
    "upvote": "5",
    "text": "Mitigating Hallucination in Large Multi-Modal\nModels via Robust Instruction Tuning\nFuxiao Liu1, Kevin Lin2, Linjie Li2, Jianfeng Wang2, Yaser Yacoob1, Lijuan Wang2\n1University of Maryland, College Park\n2Microsoft Corporation\n{fl3es, yaser}@umd.edu, {keli, lindsey.li, jianfw, lijuanw}@microsoft.com\nhttps://fuxiaoliu.github.io/LRV/\nAbstract\nDespite the promising progress in multi-modal tasks, current large multi-modal\nmodels (LMMs) are prone to hallucinating inconsistent descriptions with respect\nto the associated image and human instructions. This paper addresses this issue\nby introducing the first large and diverse visual instruction tuning dataset, named\nLarge-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual\ninstructions generated by GPT4, covering 16 vision-and-language tasks with open-\nended instructions and answers. Unlike existing studies that primarily focus on\npositive instruction samples, we design LRV-Instruction to include both positive and\nnegative instructions for more robust visual instruction tuning. Our negative instruc-\ntions are designed at three semantic levels: (i) Nonexistent Object Manipulation,\n(ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently\nmeasure the hallucination generated by LMMs, we propose GPT4-Assisted Visual\nInstruction Evaluation (GAVIE), a stable approach to evaluate visual instruction\ntuning like human experts. GAVIE does not require human-annotated groundtruth\nanswers and can adapt to diverse instruction formats. We conduct comprehensive\nexperiments to investigate the hallucination of LMMs. Our results demonstrate\nexisting LMMs exhibit significant hallucinations when presented with our negative\ninstructions, particularly Existent Object and Knowledge Manipulation instructions.\nMoreover, we successfully mitigate hallucination by finetuning MiniGPT4 and\nmPLUG-Owl on LRV-Instruction while improving performance on several public\ndatasets compared to state-of-the-art methods. Additionally, we observed that a\nbalanced ratio of positive and negative instances in the training data leads to a more\nrobust model.\n1\nIntroduction\nSignificant progress has been made in the field of natural language processing, leading to the\ndevelopment of models that can comprehend and follow instructions given natural language inputs [40;\n11; 30; 5]. These models harness the power of large language models (LLM) and rely on high-quality\ninstruction data. Similarly, efforts have been made to introduce similar capabilities to multi-modal\nmodels. GPT4 [29] has demonstrated impressive performance in multi-modal conversations with\nhumans, yet the techniques contributing to its extraordinary capabilities remain opaque. As a\nresult, several large multi-modal models (LMMs) have recently emerged [44; 26; 11; 8], such as\nMiniGPT4 [44] and LLaVA [26], both utilize the Vicuna [7] as the language generator but with\ndifferent vision encoders [31; 17]. InstructBLIP [8] is initialized from a pre-trained BLIP-2 [16]\nwhile Multimodal-GPT (MMGPT) [11] is built on Flamingo [1; 3].\nA recent study [15] revealed that the hallucination issue of LLM, although not desired, is inherited by\nthese LMMs [44; 26; 11; 8]. Hallucination, a major ethical concern associated with LLMs [4], can\nPreprint. Under review.\narXiv:2306.14565v3  [cs.CV]  29 Sep 2023\nCan you help me \ufb01nd a dog playing with a \nball on the shore?\nThere is no mention of a dog or a ball in \nthe given image. Introducing an element \nnot in the image may cause confusion and \nmake it difficult for the user to \ufb01nd the \ncorrect details.\nScore (mPLUG-Owl): 1\nReason: Despite the detailed description \nprovided, there is no mention of a dog or \na ball in the image's content.\nScore (MiniGPT4): 10\nReason: It provides a detailed \ndescription of the image, such as the \npeople sur\ufb01ng, the ocean, and the wet \nsuits.\nScore (Ours): 10\nReason:  The answer is in line with the \nimage information, pointing out the lack \nof mentioned elements from the \nquestion in the image descriptions.\nScore (mPLUG-Owl): 10\nReason: It  followed the instruction and \ndirectly described a dog playing with a \nball on the shore, which was the exact \nscenario asked for in the instruction.\nScore (MiniGPT4): 1\nReason: It provided an overview of the \nimage but didn't directly address the \ninstruction about \ufb01nding a dog and a \nball. \nScore (Ours): 10\nReason: The answer directly responds to \nthe question. \n  GAVIE\n   Relevancy Score(0-10)\n     Accuracy Score(0-10)\nEvaluate\nEvaluate\n mPLUG-Owl\n MiniGPT4\n LRV-Instruction \nDataset(Ours)\n Finetune\nThere is a dog playing with a ball on the \nshore. The dog located near the center of the \nscene, with the ball close to its paw. This adds \na playful element to the otherwise serene \nbeach setting, where people are enjoying \nwater activities like sur\ufb01ng and swimming.\nThe image is a photo of a group of three \npeople sur\ufb01ng on the ocean. They are wearing \nblack wet suits and have their boards on their \nheads. The sky in the background is a light \nblue color with white clouds.\nFigure 1: Given an image and human instruction as the input, we introduce GPT4-Assisted Visual\nInstruction Evaluation (GAVIE) to assess the output from current LMMs, such as MiniGPT4 and\nmPLUG-Owl. BLUE represents LMMs can not accurately follow human instructions while RED\nmeans they suffer from the hallucination problem. After finetuning current LMMs on our proposed\nLRV-Instruction dataset, we can generate a more robust answer.\nlead to harmful consequences, especially when users without adequate domain knowledge over-rely\non these increasingly convincing language models. In the context of LMM hallucinations, the model\ncan generate descriptions with conflicting information to the given image. For instance, as shown in\nFig. 1 (highlighted in red), existing LMMs [44; 26; 8; 11] tend to describe nonexistent objects such\nas a \"dog\" engaging in a nonexisting activity like \"playing with a ball\". Additionally, the model may\ngenerate long image descriptions without following human instructions (highlighted in blue).\nWhat are the likely causes of these hallucinations? As current LMMs are built on strong LLMs, they\nmay over-rely on language priors and generate words more likely to go together with the instruction\ntext regardless of the image content. What\u2019s more, LMMs, such as MiniGPT4 [44] and LLaVA\n[26], employ synthetic instruction data for training, which are generally long and involve nonexistent\nobjects, activities, or relationships in the image.\nWhy can\u2019t LMMs accurately follow human instructions? We conjecture it is due to the lack of\ndiversity in their training data. For example, MiniGPT4 [44] is only instruction tuning with four\ninstruction templates designed for image captioning tasks. Though MMGPT [11] and InstructBLIP\n[8] combine several datasets as the instruction tuning data, their instructions and answers are still\nbased on a few templates.\nTo address these challenges, we present LRV-Instruction, a large and diverse visual instruction\nbenchmark. Our benchmark consists of 400k visual instructions generated by GPT4, taking inspiration\nfrom the success of recent GPT models in text-annotation tasks [27]. Unlike previous studies that\nfocused on limited tasks and pre-defined templates created by human experts [44; 8; 11], LRV-\nInstruction covers 16 vision-language tasks with open-ended instructions and answers, as shown in\nFig. 2 and Fig. 4. As observed by [19], current LMMs tend to answer \"Yes\" for any instructions\npresented to the model, even when the proper answer should be \"No\". Our investigation reveals\nthat most LMMs are finetuned on unbalanced datasets containing only positive instructions (Tab. 1).\nTo enable LMMs to respond to human instructions more faithfully, we design LRV-Instruction to\ninclude both negative and positive instructions for robust instruction tuning. Our negative instructions\nare generated at three semantic levels (Fig. 2): (i) Nonexistent Object Manipulation, (ii) Existent\nObject Manipulation and (iii) Knowledge Manipulation in two different formats, Declarative and\nInterrogative. To improve the robustness and flexibility of the evaluation on visual instruction tuning,\nwe propose GPT4-Assisted Visual Instruction Evaluation (GAVIE) to assess the LMM output in\ntwo different aspects: Relevancy to evaluate the instruction-following performance and Accuracy\nto measure the visual hallucination in the LMM output. GAVIE does not require human-annotated\ngroundtruth answers [32] and can be easily adapted to different formats instead of specific designs\n2\n(Pos) Multi-Choice VQA\nQ: Choose the correct statement about the weather \nconditions in the image: (a) Cloudy and rainy, (b) Clear \nblue sky, (c)Foggy and misty, (d) Snowy and cold\nA : (b) Clear blue sky\n(Pos) Object Attribute Detection\nQ: Describe the girl's hair color and whether she is wearing \nany accessory on her wrist.\nA: The girl\u2019s color is black and the she is wearing a bracelet \non her wrist.\n(Pos) Visual Entailment\nQ: Verify if this statement is correct: \"There is a car parking \nin the image.\"\nA:  True\n(Neg) Nonexistent Object  Manipulation\nQ: Can you spot a hot air balloon above the tall tree?\nA: There is no hot air balloon mentioned in the image.\n(Neg) Knowledge  Manipulation\nQ: Was the youth employment rate in Ukraine gradually \ndecreasing from 2010 to 2015?\nA: No.The youth employment rate in Morocco gradually \nincreasing  from 2010 to 2015.\n(Pos) Object Detection\nQ: What objects are on toddler\u2019s feet?\nA: White socks\n(Pos) OCR\nQ: What is the number written on the lead \nsnowboarder?\nA: The number is 88\n(Pos) VCR\nQ: What appears to be the purpose of the green \nstreet sign?\nA: Giving directions or providing street name.\n(Neg) Existent Object Manipulation\nQ: Is the woman in the blue pants and pink shirt \nsitting on the \ufb02oor?\nA: There is no hot air balloon mentioned in the \nimage.\n(Neg) Knowledge Manipulation\nQ: Did  Merkel arrive to the Los Angeles Get Out \nThe Vote Rally?\nA:No, Hillary Clinton arrived to the Los Angeles Get \nOut The Vote Rally in the image..\nFigure 2: Examples of positive and negative instances in our LRV-Instruction dataset. RED means\ninconsistent elements in the negative instructions. More examples are in the Appendix.\nin [19]. From our experiments, we show that GAVIE is not only stable but also aligns with human\nevaluation.\nWe empirically evaluate five publicly available LMMs [44; 26; 8; 11; 42] on our benchmark and found\nthat existing LMMs seriously hallucinate when prompted with our negative instructions, especially\nwith Existent Object Manipulation and Knowledge Manipulation instructions. We further verify\nthe effectiveness of our LRV-Instruction by finetuning MiniGPT4 [44] and mPLUG-Owl [42] on\nthis more balanced data. Our instruct-tuned LMMs suffer much less from hallucination and achieve\nstate-of-the-art performance compared to the original MiniGPT4, LLaVA [26], InstructBLP [8],\nmPLUG-Owl [42] and MMGPT [11] on both our evaluation set and public benchmarks [19; 12; 9].\nWe also observe that Existent Object Manipulation and Knowledge Manipulation instructions are\nmore challenging than Nonexistent Object Manipulation instructions for LMMs. Furthermore, a\nrobust model performance requires a balanced ratio between positive and negative instances. To sum\nup, our contributions are three-fold:\n\u2022 We build LRV-Instruction, a large and diverse dataset containing 400k visual instructions, with 16\nvision and language tasks and negative instructions in different semantic levels and styles.\n\u2022 We propose GAVIE, a novel approach to evaluate visual instruction tuning without requiring\ngroundtruth answers and pre-designed instruction formats.\n\u2022 We conduct comprehensive experiments to investigate the hallucination of current LMMs. The\nempirical study validates the effectiveness of LRV-Instruction for robust visual instruction tuning.\n2\nRelated Works\nEarly explorations [39; 17; 18; 35] of vision and language pre-trained models tend to use Bert-based\n[28; 13] models as the language decoder. Inspired by the recent success of large language models\n[37; 10; 43; 20; 22] and datasets [21; 6; 14; 24; 33; 34; 23], many studies [1; 16; 15] have been focused\non improving vision-language pre-trained models by integrating powerful LLMs with in-context or\nfew-shot learning capability. More recently, some visual instruction-tuned LMMs [44; 26; 11; 8] have\nemerged, showing excellent generalization performance in unseen VL tasks. Specifically, LLaVA\n[26] projects the output of a visual encoder as input to LLaMA [37] and trains both the alignment\nnetwork and the LLM on synthetic data. MiniGPT4 [44] is built on BLIP-2 but uses Vicuna [7] as\nthe language decoder. It only finetunes the cross-modal alignment network on longer image captions\nfrom ChatGPT. The research approaches [11; 8] are instruction-tuned on a collection of VL datasets,\nbut InstructBLIP [8] uses BLIP2 [16] as the backbone while [11] is initialized from Flamingo [1].\nmPLUG-owl [42] finetunes LLaMA [37] model using both text instruction data and vision-language\ninstruction data from LLaVA [26]. In comparison, we propose a large and diverse visual instruction\n3\ndataset with 16 vision and language tasks and negative instructions in different semantic levels and\nstyles. This can help improve the robustness of current LMMs.\nAlthough LMMs are powerful in solving VL tasks, they also suffer from the hallucination inherited\nfrom LLM. Popular image captioning metrics like CIDEr [38] SPICE [2] do not appropriately penalize\nhallucination. CHAIR, [32], is unstable and needs complex human-crafted parsing rules for exact\nmatching. Alternatively, [19] converts the hallucination into a binary classification problem. However,\nit requires the input questions to follow specific templates, such as \"Is there a/an <object> in the\nimage?\". In comparison, our proposed GAVIE can evaluate model hallucination in an open-ended\nmanner without needing human-annotated groundtruth answers.\n3\nLRV-Instruction\nAnnotating large-scale visual instruction data can be challenging and time-consuming [40]. It involves\nexpertly written detailed instructions and specific labels for different tasks. Inspired by the success\nof GPT4 in text-annotation tasks [10], we leverage GPT4, instead of human workers, to build LRV-\nInstruction. LRV-Instruction is designed to cover a variety of VL tasks, with open-ended positive and\nnegative instructions (Fig. 2) in different linguistic styles.\nPositive Visual Instruction Generation. Inspired by [40], we use the in-context few-shot learning\nability of GPT4 to generate instruction data for various VL tasks automatically. We filter the output\ntasks manually and select 16 tasks (Tab. 3a) with text answers. In contrast with [26] using a few\nscene captions to represent an image as input to the text-only GPT4, we take advantage of the Visual\nGenome dataset [14], which has detailed visual information like image size, bounding boxes, and\ndense captions. Specifically, each image typically has 21 object regions and their corresponding\ncaptions. We leverage GPT4 to create the instruction-following data with the image size, bounding\nboxes, and dense captions as the \"visual\" input as if it can \"see\" the image. An example is shown in\nFig. 3. For each image, we randomly select 10 tasks. To enrich the instructions, we ask GPT4 to\ngenerate instances in both declarative and interrogative formats. The limitation of [26; 44] is that\nsynthetic visual instructions are generally longer and may involve unexpected descriptive information\ninconsistent with the image. Therefore, we explicitly instruct GPT4 with \"The answers should be less\nthan 30 words\" to reduce the chance of generating extra unrelated information in the training data.\nTo improve the diversity of images, we collect chart images from [36], which has human-annotated\ncaptions describing the construction and patterns of charts. We also select news images from [24]\nwith many named entities in the captions. We ask GPT4 to generate question-answers pairs with\ncaptions as visual input. The last two images in Fig. 2 are examples. More examples and the general\nprompt we use are shown in the Appendix (Fig. 5, 32).\nNegative Visual Instruction Generation. As shown in [19], current LMMs tend to answer \u201cYes\u201d by\nfollowing any instruction presented to the model rather than predicting a faithful answer. To teach\nLMMs [44; 26; 11; 8] to answer questions in instructions faithfully, we introduce three categories of\nnegative instructions based on Visual Genome dataset: (1) Neg1: \"Nonexistent Object Manipulation\"\nby introducing nonexistent objects, activities, attributes and interactions to the \"visual\" input as\ndescribed above. (2) Neg2: \"Existent Object Manipulation\" by manipulating existent objects with\ninconsistent attributes (Fig. 2). (3) Neg3: \"Knowledge Manipulation\" by manipulating knowledge in\ninstructions (Fig. 2). As for the detailed prompt of Neg1, we leverage the same format of the \"visual\"\ninput as shown in Fig. 3. Additionally, we provide the following instructions to GPT4:\n\"Come up with 6 misleading instructions with nonexistent elements (nonexistent objects, nonexistent\nactivities, nonexistent attributes, nonexistent interactions) in the images with different language\nstyles. The instructions should contain interrogative and declarative sentences. Please also ex-\nplain the reason.\"\nWe replace the underlined text with \"existing objects but wrong attributes\" for the prompt of Neg2.\nAs for the Neg3: knowledge manipulation, we use GPT4 to manipulate the knowledge in the captions,\nincluding named entities, events or keywords. After that, GPT4 is instructed to generate questions\nand answers indicating correct knowledge. More examples are shown in the Appendix (Fig. 6, 32).\nQuality Control. We first remove instances with answers longer than 30 words. We remove\nthe instances mentioning unneeded content like \"bounding box description\", \"given caption\", and\n\"existing descriptions\". Additionally, GPT4 will output the task name for each instruction. However,\nwe found GPT4 sometimes assigns inaccurate task names for the instructions. As a result, we exclude\n4\nOurs\nMiniGPT4\nLLaVA\nInstructBLIP\nMMGPT\nmPLUG-Owl\nHard Negative Instructions?\n\u2714\n\u2718\n\u2718\n\u2718\n\u2718\n\u2718\nSelf Generated Instruction?\n\u2714\n\u2718\n\u2714\n\u2718\n\u2718\n\u2718\nAddress Hallucination?\n\u2714\n\u2718\n\u2718\n\u2718\n\u2718\n\u2718\nNOT Template Instruction?\n\u2714\n\u2718\n\u2714\n\u2718\n\u2718\n\u2714\n# of Self-Generated Instances\n400k\n3k\n150k\n\u2718\n\u2718\n\u2718\n# of VL Tasks\n16\n1\n3\n11\n5\n3\nTable 1: A comparison of LRV-Instruction with datasets used by current LMMs.\nPrompt:\nGive an image with following information: bounding box, positions that are the object left-top corner coordinates(X, Y), \nobject sizes(Width, Height). Highly overlapping bounding boxes may refer to the same object.\nbounding box:\nelephant heard on rocks X: 73 Y: 80 Width: 418 Height: 418\nwoman wearing long dress X: 176 Y: 298 Width: 35 Height: 83\ngroup of green chairs X: 153 Y: 326 Width: 95 Height: 126\nan orange bucket on the ground X: 91 Y: 341 Width: 38 Height: 36\na group of white umbrellas X: 99 Y: 82 Width: 112 Height: 28\na man in an orange shirt X: 204 Y: 265 Width: 31 Height: 47\na woman wearing a yellow dress X: 169 Y: 298 Width: 47 Height: 76\n. . .\nTask: image captioning, Image Sentiment Analysis,  Image Quality Assessment, Object Interaction Analysis, Object \nAttribute Detection, Muli-choice VQA \u2026\nCome up with 20 diverse instructions for all the tasks above with different language styles and accurate answers. The \ninstructions should contain interrogative sentence and declarative sentences. The answers should be less than 30 words. \nEach task should have less than 3 instructions. \nGPT4 OUTPUT Example:\nInstruction:\nCraft a brief narrative about the baby elephant and adult elephant.\nAnswer:\nA baby elephant is depicted behind an adult elephant, possibly seeking protection.\nFigure 3: One example to illustrate the prompt we use to generate the visual instruction data by GPT4.\nWe use the bounding box coordinates and dense captions to represent image content.\nthe task name in our release data. Furthermore, we remove the instructions asking about facial\nexpressions. This is because the Visual Genome dataset doesn\u2019t include facial expression attributes\nin the ground truth dense captions. To examine the quality of our dataset, we randomly sample 500\ninstances and ask ten expert annotators to determine whether the output answers from GPT4 are\ncorrect or not, with regard to the instruction and the image content. We found 91% of the instructions\nare appropriate for the image inputs. Furthermore, 85% of outputs are acceptable responses to the\ninstructions. Even though some responses may contain errors, most generations conform to the\ncorrect structure, serving as applicable visual instruction tuning guidelines. We created a total of over\n400k visual instructions after filtering.\nEvaluation Set. After the processing above, we randomly select 1000 instances as our evaluation set.\nFurthermore, we manually check the quality of all instances and see whether the instruction describes\na valid task. If it\u2019s not, we edit the instruction to make it clearer for LMMs. For example, we edit the\ninstruction \u2018Observe the beautiful rainbow-colored sign that says \u2019Le Louvre\u2019. You won\u2019t miss it!\u2019 to\n\"Are you able to observe the beautiful rainbow-colored sign that says \u2019Le Louvre\u2019 in the image?\"\n3.1\nData Statistics\nTab. 1 shows a comparison of LRV-Instruction and other datasets used by current LMMs. LRV-\nInstruction covers much more VL tasks than existing visual instruction tuning datasets. Instead\nof only using positive instructions, LRV-Instruction also includes negative instructions at different\nsemantic levels. In addition, employing the GPT4-assisted generation, LRV-Instruction has more\nopen-ended instructions instead of following a few templates. From Fig. 4 (b), we observe that\ninstructions with non-existing objects generated by GPT4 are diverse and physically plausible in the\nimage, including \u201cbirds in the sky\u201d or replacing \u2018elephant\u2019 with \u2018zebra\u2019. Fig. 10 in the appendix\n5\nTasks of LRV-Instruction\nImage Captioning, Object Detection, Image Sentiment Analysis, Image \nQuality Assessment, Object Interaction Analysis, Image Anomaly \nDetection, Referential Expression Grounding, OCR, VCR, Object Attribute \nDetection, Muli-choice VQA, Semantic Segmentation, Dense Captioning, \nVisual Entailment, Styled Image Caption, Activity Recognition, Chart \nUnderstanding\n(b)  Statistic of Non-existent objects generated by GPT4\n                      (a)  16 VL tasks in our dataset.\nStatistic\n# of Instructions                                       400k\n# of Images                                               35k\n# of Interrogative Instruction                     250k\n# of Declarative Instruction                        150k\n# of Negative Instructions                          180k\nAve # of Instruction per Image                   17.7\nAve Answer Length                                    15.2\nAve Instruction Length                               12.1\n               (c)  Statistic of LRV-Instruction\n           (d)  Distribution of different Manipulation\nFigure 4: Comprehensive Statistic of LRV-Instruction. In (d), BLUE means existent object manipula-\ntion. PINK means nonexistent object manipulation. GREEN means knowledge manipulation.\nshows the diverse distribution of knowledge manipulation, including event, number, date, persons,\nplace, and others.\n4\nVisual Instruction Tuning\nWe constructed two current LMMs: MiniGPT4 [44] and mPLUG-Owl [42] as the backbones for\nvisual instruction tuning. MiniGPT4 consists of the Vision transformer [25] backbone as the image\nencoder, Vicuna [7] as the text decoder and a pre-trained Q-Former to connect them. Vicuna is built\nupon LLaMA [37] with stronger following ability. Following [44], the Q-Former is designed to\nextract visual features from the frozen image encoder. Before feeding into the frozen Vicuna as\nthe visual prompt, we use a learnable linear projection layer to narrow the gap between extracted\nvisual features with Vicuna embeddings. mPLUG-Owl comprises a pre-trained visual encoder, a\nvisual abstractor, and Vicuna [7] as the text decoder. The visual encoder is responsible for extracting\nvisual features from the input images, and the visual abstractor distills these features using a set of\nlearnable tokens. The resulting visual features are concatenated with the word embeddings of the\ninput sentence and fed into Vicuna to generate the response. We freeze the visual abstractor and\nvisual encoder. Instead, we adopt the low-rank adaptation [? ] to train the text decoder.\n5\nGPT4-Assisted Visual Instruction Evaluation\nCHAIR [32] was introduced to evaluate object hallucination in image captioning tasks. However,\nit usually demands complex human-crafted rules. Alternatively, [19; 9] formulate the evaluation of\nhallucination as a binary classification task that prompts LMM to output \"Yes\" or \"No\". However,\nit is hard to evaluate the LMM output in an open-ended manner. In addition, both methods highly\ndepend on human-annotated groundtruth answers.\nTo this end, we introduce GPT4-Assisted Visual Instruction Evaluation (GAVIE) as a more flexible\nand robust approach to evaluate object-level hallucination. The general prompt we use is shown in\nthe Appendix. GPT4 takes the dense captions with bounding box coordinates as the image content\nand compares human instructions and model response. Then we ask GPT4 to work as a smart teacher\nand score (0-10) students\u2019 answers based on two criteria. (1) Accuracy: whether the response is\naccurate concerning the image content. (2) Relevancy: whether the response directly follows the\ninstruction. We use GPT4-32k-0314 in the experiments. Fig. 1 successfully points out that \"dog, ball\"\nis inconsistent with the image, and the response from the MiniGPT4 did not address the instruction.\nUnlike previous evaluation methods [19; 32], GAVIE does not require human-annotated groundtruth\nanswers and can freely adapt to diverse instruction formats. As for the knowledge level hallucination\n6\nBackbone\nPerception\nCognition\nOriginal MiniGPT4\n616.41\n232.71\nFinetuned MiniGPT4\n895.96\n296.43\nOriginal mPLUG-Owl\n967.34\n276.07\nFinetuned mPLUG-Owl\n1298.78\n328.21\nBackbone\nAcc(Pos)\nAcc(Neg)\nOriginal MiniGPT4\n0.53\n0.54\nFinetuned MiniGPT4\n0.58\n0.68\nOriginal mPLUG-Owl\n0.62\n0.55\nFinetuned mPLUG-Owl\n0.69\n0.78\nTable 2: Zero-shot multimodal evaluation on MME [9] of MiniGPT4-7B, mPLUG-Owl-7B between\noriginal models and LRV-Instruction-finetuned models. The left chart shows perception and cognition\nscores. The right chart shows the accuracy on the positive set and the negative set.\nModel\nAcc\nF1\nmPLUG-Owl-7B\n0.52\n0.68\nLLaVA-13B\n0.50\n0.66\nMiniGPT4-13B\n0.73\n0.71\nInstructBLIP-13B\n0.86\n0.87\nOurs-7B\n0.86\n0.88\n(a) Random Set.\nModel\nAcc\nF1\nmPLUG-Owl-7B\n0.57\n0.66\nLLaVA-13B\n0.50\n0.66\nMiniGPT4-13B\n0.67\n0.67\nInstructBLIP-13B\n0.71\n0.76\nOurs-7B\n0.73\n0.79\n(b) Popular Set.\nModel\nAcc\nF1\nmPLUG-Owl-7B\n0.60\n0.64\nLLaVA-13B\n0.50\n0.66\nMiniGPT4-13B\n0.62\n0.63\nInstructBLIP-13B\n0.63\n0.72\nOurs-7B\n0.65\n0.73\n(c) Adversarial Set.\nTable 3: Zero-shot object hallucination evaluation on POPE [19]. Objects not existing in the image\nare sampled with three different strategies. Random: random sampling, Popular: top-k most frequent\nobjects in MS-COCO, Adversial: objects are first ranked based on co-occurring frequencies, then\ntop-k frequent ones are sampled. Ours-7B means Finetuned mPLUG-Owl-7B.\nor images that are not from the Visual Genome dataset, we use the groundtruth answers as a reference\nand compare them with predictions (Fig. 7 in the appendix).\n6\nExperiment\n6.1\nImplementation Setup\nBaselines. We evaluate the zero-shot performance of 5 recently released LMMs: (1) MiniGPT4; (2)\nLLaVA; (3) InstructBLIP; (4) Multimodal-GPT (MMGPT); (5) mPLUG-Owl. All models above\nhave been tuned on their collected visual instruction data.\nTraining Details. As for MiniGPT4, we initialize from its checkpoint of the first pretraining stage.\nThen we instruct-tune the model on LRV-Instruction with the linear projection layer as the only\nlearnable module. As for mPLUG-Owl, we train the text encoder by LoRA training. Additionally,\nwe only replace the LLaVA dataset in their finetuning data with LRV-Instruction to make a fair\ncomparison with the original Mplug-Owl. We utilize MiniGPT4-7B and mPLUG-Owl-7B since we\ndon\u2019t have the computing resources to finetune the 13B models. We trained our models on NVIDIA\nQuadro RTX 8000. As for the hyper-parameters, please refer to [44; 42].\nEvaluation Benchmarks. Apart from our proposed evaluation set, we evaluate LMMs on three\npublic benchmarks. MME [9] is a human-annotated benchmark, measuring perception and cognition\nabilities on 14 subtasks. POPE [19] is a recently released dataset to evaluate object hallucination.\nGQA dataset [12] is a public visual question-answer dataset with open-ended questions.\n6.2\nMain Results\nHow do LMMs perform on public datasets? We compare our model against the baseline models on\nPOPE in Tab.3. The results show that current LMMs may not work well with open-ended negative\ninstructions. In contrast, the highest scores of our model demonstrate that LRV-Instruction exhibits\nrobustness to visual hallucination, matching or surpassing the performance of 13B counterparts.\nFrom Tab.2, we found both finetuned LMMs on LRV-Instruction outperform original ones in the\nzero-shot evaluations. Additionally, Finetuned-Mplug-Owl exceeds Finetuned-MiniGPT4 because\nMplug-Owl can do the LoRA training to improve the language ability. We also calculate the accuracy\non positive and negative samples of MME in the right chart of Tab.2. The improvement in the positive\nsamples is because LRV-Instruction has more diverse tasks than mPLUG-Owl datasets and MiniGPT4\ndatasets. The improvement in the negative samples demonstrates the value of LRV-Instruction dataset\nto equip the model with the ability to say \u2018no\u2019 and provide correct answers. The completed results\n7\nOurs MiniGPT4 LLaVA InstructBLIP MMGPT mPLUG-Owl\nGAVIE-ACCURACY (0-10)\n6.58\n4.14\n4.36\n5.93\n0.91\n4.84\nGAVIE-RELEVANCY (0-10)\n8.46\n5.81\n6.11\n7.34\n1.79\n6.35\nHuman Expert1 (1-4)\n3.48\n2.61\n2.87\n3.00\n1.90\n2.90\nHuman Expert2 (1-4)\n3.58\n2.23\n2.07\n2.48\n1.05\n2.27\nHuman Expert3 (1-4)\n3.33\n2.58\n2.89\n2.94\n1.38\n2.91\nTable 4: Comparison results on our evaluation set evaluated by GAVIE. Ours means Finetuned\nmPLUG-Owl-7B. All the LMMs are 7B versions to make a fair comparison.\nModel\nInstructBLIP-13B LLaVA-13B MiniGPT4-13B mPLUG-Owl-7B Ours-7B Ours-7B-Psu\nAccuracy\n0.62\n0.47\n0.42\n0.41\n0.64\n0.60\nTable 5: Zero-shot evaluation on GQA. Ours-7B means Finetuned mPLUG-Owl-7B. Ours-7B-Psu\nmeans we finetune mPLUG-Owl on pseudo instruction data by [41].\non shown in Tab. 11.12. We further explore the LMMs\u2019 performance in the common scenario of\nvisual question-answering (VQA). As shown in Tab. 5, the results suggest that our method (Finetuned\nmPLUG-Owl) achieves on-par performance with InstructBLIP in a generic VQA setting.\nHow do LMMs perform on LRV-Instruction? We show the evaluation results on our dataset in\nTab. 4. Among the baselines, InstructBLIP achieves better results than other LMM baselines because\nits visual instructions are collected from a wide variety of publicly available datasets. LLaVA [26]\nutilizes the GPT-assisted approach to generate visual instructions, but its performance is much worse.\nThis is probably because its synthetic answers from GPT4 are generally longer and may involve\nirrelevant information. As a comparison, our model outperforms the existing LMM baselines by a\nlarge margin, benefiting from the rich composition of our dataset and better prompt design.\n6.3\nDetailed Analysis\nDoes GPT4-Assisted Visual Instruction Evaluation align with Human Evaluation? We select\nthree human experts specializing in the field of NLP to evaluate the predictions from LMMs with\nfour options for the scores (1) Very Poor, (2) Poor, (3) Good, (4) Excellent. To evaluate the results\nquantitatively, we assign different scores for the options: Very Poor=1, Poor=2, Good=3, Excellent=4.\nMore implementation details are shown in the appendix. From Tab. 4, all experts agree that the\noutput from our model is the best, followed by InstructBLIP in second place, and MMGPT performs\nthe worst. The observation aligns with the GAVIE evaluation results.\nIs GPT4-Assisted Evaluation Stable? We execute GAVIE 5 times on each instruction and evaluate\nthe predictions from different LMMs. We leverage Standard Deviation (STD) to measure the stability\nof GAVIE. From Tab. 7 (left), we observe that STD ranges from 0.65 to 2.46. The ACCURACY and\nRELEVANCY scores of an instance from GPT4 may vary between different times, but they always\nbelong to the same grade level. According to completed results from Tab. 9, RELEVANCY has four\ngrade levels: (1) The response is completely relevant (9-10), (2) The response is mostly relevant (6-8),\n(3) The response is partly relevant (3-5), (4) The response is seldom relevant (0-2). ACCURACY has\nfour grade levels: (1) The response is completely accurate (9-10), (2) The response has minor errors\n(6-8), (3) The response is partly accurate (3-5), (4) The response is mostly or completely wrong (0-2).\nHow do LMMs perform at the different semantic levels of hallucination? As shown in Tab 6, all\nbaselines perform better on Neg1 (Nonexistent Object Manipulation) than Neg2 (Existent Object\nManipulation) and Neg3 (Knowledge Manipulation). From the visual perspective, existent object\nmanipulations with wrong attributes in Neg2 are more challenging than adding nonexistent objects\nfrom images to instructions in Neg1. For example, in Fig. 2, it may be straightforward to find that\nthe \"hot air balloon\" does not appear in the image. However, \"woman\" does exist in the second\nexample of Fig. 2 while she is not in the blue pants and pink shirts, which requires a fine-grained\nunderstanding of the visual content. Therefore, a more powerful vision encoder is needed for future\nLMMs. Knowledge manipulation is challenging because current LMMs are finetuned on general\nimages without specific knowledge. In contrast, our model greatly improves at all semantic levels,\nwhich benefits from our diverse instruction tuning data.\n8\nCategories\nMetric\nOurs MiniGPT4 LLaVA InstructBLIP MMGPT mPLUG-Owl\nNeg1\nACCURACY(GPT4)\n8.90\n3.72\n2.09\n5.50\n1.13\n4.20\nNeg2\nACCURACY(GPT4)\n6.50\n2.57\n1.42\n2.18\n0.96\n2.46\nNeg3\nACCURACY(GPT4)\n6.25\n2.30\n1.56\n2.38\n0.94\n2.57\nNeg1\nRELEVANCY(GPT4)\n8.96\n5.94\n4.83\n7.22\n2.24\n5.35\nNeg2\nRELEVANCY(GPT4)\n8.46\n2.53\n1.82\n2.73\n1.19\n3.16\nNeg3\nRELEVANCY(GPT4)\n8.21\n2.40\n1.78\n2.39\n0.98\n2.87\nTable 6: Completed evaluation results on Neg1: Nonexistent Object Manipulation, Neg2: Existent\nObject Manipulation and Neg3: Knowledge Manipulation by GAVIE.\nMetric\nAccuracy-STD\nAccuracy-Mean\nOurs\n2.42\n6.60\nMiniGPT4\n2.46\n3.76\nInstructBLIP\n2.42\n5.29\nmPLUG-Owl\n1.96\n0.87\nLLaVA\n2.37\n3.80\nMMGPT\n0.65\n4.84\nRatio\nAccpos\nAccneg\nAll Pos\n0.97\n0.05\nPos:Neg=2:1\n0.95\n0.50\nPos:Neg=1:1\n0.92\n0.85\nPos:Neg=1:2\n0.87\n0.86\nAll Neg\n0.10\n0.98\nTable 7: (left): Evaluation of the stability of GAVIE. STD means standard deviation. Completed\nresults are shown in Tab. 9. (right): Results of different composition ratios in instruction tuning.\nHow do LMMs perform at the different composition ratios in training data? In Tab. 7 (right), we\ninvestigate how LRV-Instruction addresses hallucination issues with different ratios of positive and\nnegative samples in the training set. Inspired by [19], we instruct the model to produce \u201cYes\u201d or\n\u201cNo\u201d and use classification accuracy on our evaluation set. Accpos is the accuracy on the positive\ninstruction set, while Accneg is the accuracy on the negative instruction set. From Tab. 7 (right),\nwe found that Accneg increases with more negative samples, which verifies our hypothesis that the\nhallucination problem of current LMMs is due to the lack of negative instructions. Besides, with a\nbalanced ratio (pos:neg=1:1), the model performs the best in both positive and negative sets.\nUse Pseudo Dense Captions instead of GT from Visual Genome to Generate Instructions. To\ndemonstrate the scalability of our dataset, we use pseudo-dense captions generated by GRiT [41]\nto replace the GT captions in the Visual Genome dataset. We remove the images, whose detected\nobjects by GRiT are less than 15 to ensure GPT4 has enough visual information when generating\nvisual instructions. From Tab. 5, we found finetuning on pseudo captions can also improve the\nperformance compared to the original mPLUG-Owl. This demonstrates that our visual instruction\ngeneration method can be further scaled up without groundtruth dense captions.\n7\nConclusion\nIn this work, we constructed LRV-Instruction, a large and diverse dataset containing 400k visual\ninstructions, covering 16 vision and language tasks with both positive and negative instructions in\ndifferent semantic levels and styles. With LRV-Instruction, we comprehensively investigated the\nhallucination of existing LMMs and empirically validated its effectiveness in a more robust visual\ninstruction tuning. In addition, we propose GAVIE, a novel approach to evaluate visual instruction\ntuning without requiring human-labeled groundtruth answers and can be easily adapted to different\ninstruction formats. We hope our work can help address the unexpected hallucination issues of LMMs.\nFuture directions include replacing the vision encoders in current LMMs with more powerful visual\nmodels to match the capabilities of multimodal GPT4 and investigation of other biases of LMMs to\ndevelop more robust models.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems,\n35:23716\u201323736, 2022.\n9\n[2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic\npropositional image caption evaluation. In Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages\n382\u2013398. Springer, 2016.\n[3] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023.\n[4] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Love-\nnia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation\nof chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023,\n2023.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality, 2023.\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\n[9] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu,\nWei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for\nmultimodal large language models. arXiv preprint arXiv:2306.13394, 2023.\n[10] Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for\ntext-annotation tasks. arXiv preprint arXiv:2303.15056, 2023.\n[11] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,\nWenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for\ndialogue with humans. arXiv preprint arXiv:2305.04790, 2023.\n[12] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\nreasoning and compositional question answering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 6700\u20136709, 2019.\n[13] MV Koroteev. Bert: a review of applications in natural language processing and understanding.\narXiv preprint arXiv:2103.11943, 2021.\n[14] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations. International journal of\ncomputer vision, 123:32\u201373, 2017.\n[15] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023.\n[17] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-\nimage pre-training for unified vision-language understanding and generation. In International\nConference on Machine Learning, pages 12888\u201312900. PMLR, 2022.\n10\n[18] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A\nsimple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557,\n2019.\n[19] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating\nobject hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.\n[20] Zongxia Li, Paiheng Xu, Fuxiao Liu, and Hyemi Song. Towards understanding in-context\nlearning with contrastive demonstrations and saliency maps. arXiv preprint arXiv:2307.05052,\n2023.\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n[22] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning\nlarge multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.\n[23] Fuxiao Liu, Hao Tan, and Chris Tensmeyer. Documentclip: Linking figures and main body text\nin reflowed documents. arXiv preprint arXiv:2306.06306, 2023.\n[24] Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. Visual news: Benchmark and\nchallenges in news image captioning. arXiv preprint arXiv:2010.03743, 2020.\n[25] Fuxiao Liu, Yaser Yacoob, and Abhinav Shrivastava. Covid-vts: Fact extraction and verification\non short video platforms. In Proceedings of the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, pages 178\u2013188, 2023.\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[27] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval:\nNlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634,\n2023.\n[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[29] OpenAI. Gpt-4 technical report. 2023.\n[30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[32] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object\nhallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018.\n[33] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 2556\u20132565, 2018.\n[34] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit:\nWikipedia-based image text dataset for multimodal multilingual machine learning. In Pro-\nceedings of the 44th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, pages 2443\u20132449, 2021.\n11\n[35] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\njoint model for video and language representation learning. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 7464\u20137473, 2019.\n[36] Benny J Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: A benchmark for semantically\nrich chart captioning. arXiv preprint arXiv:2307.05356, 2023.\n[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[38] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4566\u20134575, 2015.\n[39] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu,\nCe Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language.\narXiv preprint arXiv:2205.14100, 2022.\n[40] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-\ntions. arXiv preprint arXiv:2212.10560, 2022.\n[41] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan\nWang. Grit: A generative region-to-text transformer for object understanding. arXiv preprint\narXiv:2212.00280, 2022.\n[42] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large\nlanguage models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n[43] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv\npreprint arXiv:2303.18223, 2023.\n[44] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n12\nA\nAppendix\nA.1\nGAVIE Evaluation\nWe show two full examples of the text prompt for GAVIE in (i) Fig. 21, 22, 23 and (ii) Fig. 24, 25,\n26. We first leverage the bounding boxes and dense captions as the \"visual\" input. We provide the\nhuman instructions and responses from different models in Fig. 22 and Fig. 25. Furthermore, we ask\nGPT4 to pretend as a smart teacher and score (0-10) the answers according to the image content and\ninstructions. There are two criteria. (1) Accuracy: whether the response is accurate concerning the\nimage content. (2) Relevancy: whether the response directly follows the instruction. After that, GPT4\nis required to generate a score and reason. Fig. 23 and Fig. 26 show the full evaluation output from\nGAVIE.\nA.1.1\nGPT4-Assisted Visual Instruction Evaluation (GAVIE) vs. Human Evaluation\nThis section provides insights into the GAVIE via human evaluation. Here, we randomly select 40\nimage-instruction instances from the evaluation set. The human assessment is carried out by three\nexperts specializing in NLP. The questionnaire consists of 40 questions randomly shuffled for each\nexpert. The questionnaire takes about 20 minutes to complete on average. Each question includes an\ninstruction, an image, and responses from 4 different LMMs. We provide instructions for experts as\nfollows:\n\"As for each question, there are an instruction, an image, and several answers. Suppose you are a\nsmart teacher, please score the answers according to the two criteria. (1) Accuracy: whether the\nresponse is accurate concerning the image content. (2) Relevancy: whether the response directly\nfollows the instruction without unrelated answers. There are four options for the scores (1) Very Poor,\n(2) Poor, (3) Good, (4) Excellent.\"\nEvaluator\nOurs MiniGPT4 LLaVA InstructBLIP MMGPT mPLUG-Owl\nExpert1(1-4)\n3.48\n2.61\n2.87\n3.00\n1.90\n2.90\nExpert2(1-4)\n3.58\n2.23\n2.07\n2.48\n1.05\n2.27\nExpert3(1-4)\n3.33\n2.58\n2.89\n2.94\n1.38\n2.91\nGAVIE-Accuracy (0-10)\n6.58\n4.14\n4.36\n5.93\n0.91\n4.84\nGAVIE-Relevancy (0-10) 8.46\n5.81\n6.11\n7.34\n1.79\n6.35\nTable 8: GAVIE vs. Human Evaluation. GAVIE scores roughly align with the expert ratings. Numbers\nhighlighted with red, orange, black, green, blue, and magenta indicate rank 1 to 6.\nTo evaluate the results quantitatively, we assign different scores for the options: Very Poor=1, Poor=2,\nGood=3, Excellent=4. From Tab. 8, all experts agree that the output from our model is the best,\nfollowed by InstructBLIP in second place, and MMGPT performs the worst. The observation is\nsimilar to that of GAVIE evaluation results. Although the ranking orders of MiniGPT4 and LLaVA\nfrom experts are not always the same as that of GAVIE, the scores assigned to them are fairly close.\nOne possible reason is that the answers from MiniGPT4 and LLaVA tend to be longer, making them\nmore challenging for humans to evaluate.\nA.1.2\nStability of GPT4-Assisted Visual Instruction Evaluation (GAVIE)\nThis section investigates the stability of GAVIE. Precisely, we execute GAVIE 5 times on the model\npredictions. We leverage two metrics to measure the stability of GAVIE on each instance: Mean and\nStandard Deviation (STD). The average scores of the evaluation set are shown in the following table.\nFrom the perspective of the Mean, the ranking order of ACCURACY and RELEVANCY is the same\nas Tab. 8. As for the Standard Deviation in Tab. 9, it ranges from 0.65 to 2.46. From our observation,\nthe ACCURACY and RELEVANCY scores of an instance may vary between different times, but they\nbelong to the same grade level. Specifically, RELEVANCY has four grade levels: (1) The response\nis completely relevant (9-10), (2) The response is mostly relevant (6-8), (3) The response is partly\nrelevant (3-5), (4) The response is seldom relevant (0-2). ACCURACY has four grade levels: (1) The\nresponse is completely accurate (9-10), (2) The response has minor errors (6-8), (3) The response is\npartly accurate (3-5), (4) The response is mostly or completely wrong (0-2).\n13\nMetric\nOurs MiniGPT4 InstructBLIP MMGPT mPLUG-Owl LLaVA\nACCURACY(GPT4)-Mean\n6.60\n3.76\n5.29\n0.87\n4.84\n3.80\nRELEVANCY(GPT4)-Mean 8.37\n5.35\n6.83\n1.71\n6.35\n5.65\nACCURACY(GPT4)-STD\n2.42\n2.46\n2.42\n0.65\n1.96\n2.37\nRELEVANCY(GPT4)-STD\n1.30\n1.99\n1.88\n0.81\n1.48\n2.18\nTable 9: Evaluation of the stability of GAVIE. We run GAVIE 5 times on the randomly selected\ninstances from the evaluation set. Mean and Standard Deviation(STD) are calculated to measure the\nstability. The metric scores of ACCURACY(GPT4) and RELEVANCY(GPT4) are from 0 to 10.\nA.2\nMore Experiments\nA.2.1\nDo LMMs perform better on Positive or Negative Instructions?\nOur evaluation set consists of positive and negative instances. We divide it into two sets and analyze\nthe model performance on each. As shown in Fig. 8, baseline models, including MiniGPT4, LLaVa,\nand InstructBLIP, perform better on positive instances than negative ones, as the training data adopted\nby these models do not contain negative instructions. MMGPT performance poorly on both sets due\nto many repetitive phrases in the response. In addition, we found that the degradation of LLaVA\nis the most severe. We hypothesize that the synthetic answers for instruction tuning in LLaVA are\ngenerally longer and involve more unrelated information. In contrast, our model performs the best in\nboth sets. InstructBLIP performs with higher scores than other LMMs because of the effectiveness of\nits instruction-aware visual encoder to extract image information.\nA.2.2\nDo LMMs perform better on different formats and lengths of instructions?\nFrom Tab 10, LMMs perform with higher scores on interrogative instructions than declarative, but\nthe difference is relatively small. Even though recent visual instruction tuning datasets lack diverse\ndeclarative instructions, the LMMs built on LLM are powerful enough to understand and follow the\ndeclarative instructions. From Fig. 9, current LMMs achieve better results in short instructions than\nlong ones since longer instructions contain more information, making it more challenging.\nA.3\nPrompt Design\nA.3.1\nPositive Instance Generation based on Visual Genome Dataset\nWe show two full examples of our input prompts in (i) Fig. 11, 12, 13 and (ii) Fig. 14, 15, 16. In\nFig. 11 and Fig. 14, we first present the images for the two examples, but they are not included in\nthe text prompt for GPT4. As for the text input, we leverage the groundtruth bounding boxes and\ndense captions to represent the visual content as if GPT4 can see the image. After that, we randomly\nselect 10 tasks from the 16 seeds and ask GPT4 to generate 20 instances for these tasks. Additionally,\nthere can be more than one caption describing the same object with different attributes, such as\n\"woman wearing a long dress\" and \"woman wearing a yellow dress\" in Fig. 11. Although we present\nthe bounding box coordinates of each caption to GPT4, it can be easily confused, treating them as\ntwo instances, one in a long dress and the other in a yellow dress. To mitigate this issue, we add\n\"highly overlapping bounding boxes may refer to the same object\" into the prompt to help GPT4\nunderstand the \"visual\" input better. To enrich the instructions, we ask GPT4 to generate instances\nin both declarative and interrogative formats. We also explicitly instruct GPT4 with \"The answers\nshould be less than 30 words\" as a requirement to reduce the chance of generating extra unrelated\ninformation in the training data. In order to make the output of GPT4 in a good format, we also ask\nGPT4 to generate an instruction, an answer, and a task name in order at the end of the prompt (Fig.\n11 and Fig. 14). The full output of instructions and answers are shown in Fig. 12, 13 and Fig. 15, 16.\nWe also present more positive instances with the output from different LMMs in Fig. 29, 30, 31.\nA.3.2\nPositive Instance Generation based on Chart Images\nWe collect chart images from [36], which has human-annotated captions describing the construction\nand patterns of charts. We instruct GPT-4 to generate question-answers pairs with captions as visual\ninput. The detailed prompt is shown in Fig. 5. We also present more positive instances with the\noutput from different LMMs in Fig. 32.\n14\nPrompt:\nHere is the description of a chart \"Here is a area chart is labeled Cumulative number of patients \ndiagnosed with coronavirus (COVID-19) in Japan as of December 4, 2020, by place of infection. On the \nx-axis, Month is measured with a categorical scale starting with April and ending with October. There is a \nlinear scale with a minimum of 0 and a maximum of 150,000 along the y-axis, labeled Patients within \nJapan \"\nPlease generate 3 different questions and answers pairs about title, x-axis, y-axis, data range or data \npattern of the chart. The answers should come from the descriptions above. Each Answer must be less \nthan 20 words. The output format should be as follows:\nquestion1=> \nanswer1 =>\nquestion2=> \nanswer2 =>\nquestion3=> \nanswer3=>\nGPT4 OUTPUT Example:\nquestion1=> What does the area chart represent?\nanswer1 => Cumulative COVID-19 cases in Japan by place of infection from April to October 2020.\nquestion2=> What does the x-axis represent?\nanswer2 => Months from April to October 2020\nquestion3=> When did the greatest increase in COVID-19 cases in Japan occur?\nanswer3=> Between November and December 2020.\nFigure 5: An example prompt for text-only GPT4 we use to generate instruction and answers for\nchart images. The sentence in BLUE is the captions of the chart.\nA.3.3\nNegative Instance Generation - Nonexistent/Existent Object Manipulation\nWe show two full examples of our input prompts in (i) Fig. 17, 18 and (ii) Fig. 19, 20. In Fig.\n17 and Fig. 19, we present the images to help readers understand dense captions better but they\nare not included in the text prompt for GPT4. We leverage the bounding boxes and dense captions\nas the \"visual\" input. As for Nonexistent object Manipulation in 17, we ask GPT4 to generate\n6 instructions with nonexistent elements (nonexistent objects, nonexistent activities, nonexistent\nattributes, nonexistent interactions). As for Existent object Manipulation in 19, we ask GPT4 to\ngenerate 6 instructions of existing objects with wrong attributes. At the end of the text prompt, we\nask GPT4 to generate an instruction and a reason to explain why the instruction is inconsistent with\nthe image in order. The reason is regarded as the answer for the instruction in our training data. Fig.\n18 and Fig. 20 show the full output from GPT4. We also present more negative instances with the\noutput from different LMMs in Fig. 27, 28.\nA.3.4\nNegative Instance Generation - Knowledge Manipulation\nAs for the Neg3: knowledge manipulation, we use GPT4 to manipulate the knowledge in the captions,\nincluding named entities and events.\nPrompt:\nPlease change the knowledge including keywords, name entities or event elements in the description \n\u201cCumulative COVID-19 cases in Japan by place of infection from April to October 2020\u201d\nOutput format should be as follows:\nanswer=>\nGPT4 OUTPUT Example:\n\u201cCumulative influenza cases in France by region of infection from March to October 2020.\u201d\nFigure 6: An example prompt for text-only GPT4 we use to generate negative instruction. The next\nstep is to transfer the ouput into an interrogative sentence whose answer is \"yes\" or \"no\".\n15\nAs shown in Fig. 6, GPT4 manipulates the \"Japan\", \"COVID-19\" and \"April\" in the original captions.\nAfter that, we instruct GPT4 to transfer the output sentence into an interrogative sentence whose\nanswer is \"yes\" or \"no\". Finally, we combine \"No.\" and the original answer as the final answer:\nQuestion: Did the image show the cumulative influenza cases in France by region of infection from\nMarch to October 2020? Answer: No. Cumulative COVID-19 cases in Japan by place of infection\nfrom April to October 2020\".\nA.3.5\nPrompt Design for Evaluating Knowledge Hallucination\nAs for the knowledge level hallucination, we will use the groundtruth answers as a reference and\ncompare them with predictions of models. A prompt example for GPT4 is shown in Fig. 7:\nPrompt:\nImagine you are an intelligent teacher. Thoroughly read both the reference answer and the prediction \nanswer to ensure a clear understanding of the information provided. Assess the accuracy of the \npredictions, and note that a prediction answer will be considered incorrect if it contradicts the reference \nanswer.\nQuestion: What does the volume of U.S. imports of trade goods in 2009 experienced , increase, \ndecrease or steady?\nReference answer: The volume of U.S. imports of trade goods in 2009 experienced a significant \ndecrease, with a drop between -0.2 and -0.3.\nPrediction answer: The volume of U.S. imports of trade goods experienced a decrease\nGPT4 OUTPUT Example:\nThe prediction answer states that \"The volume of U.S. imports of trade goods experienced a \ndecrease\". This matches with the reference answer which says \"The volume of U.S. imports of trade \ngoods in 2009 experienced a signi\ufb01cant decrease\". Therefore, the prediction answer is correct.\nFigure 7: An example prompt for text-only GPT4 we use to evaluate knowledge manipulation\ninstruction. The sentences in BLUE are the questions, reference answers, and predictions of models.\n.\nCategories\nMetric\nOurs\nMiniGPT4\nLLaVA\nInstructBLIP\nMMGPT\nInterrogative\nACCURACY(GPT4)\n6.61\n4.14\n4.60\n5.95\n1.01\nInterrogative\nRELEVANCY(GPT4)\n8.46\n6.20\n5.88\n7.67\n2.00\nDeclarative\nACCURACY(GPT4)\n6.50\n3.98\n3.82\n5.47\n0.90\nDeclarative\nRELEVANCY(GPT4)\n8.21\n5.39\n5.84\n6.64\n1.62\nTable 10: Evaluation results on Interrogative Instructions and Declarative Instructions by GAVIE. The\nmetric scores of ACCURACY(GPT4) and RELEVANCY(GPT4) are in a scale of 0 to 10.\nA.4\nMore Dataset Statistic\nI summarized the popular words in the knowledge manipulation generated by GPT4 in Fig. 10 and\nfound they mainly include six categories: event, number, date, persons, place, and others. Some\nexamples are shown below.\nCanada, increase, decrease, lowest, 2009, United States, 2016, employment, unemployment, higher,\n2013, 2017, 2015, drop, minimum, worst, consistent, kingdom, x-axis, y-axis, under, Italy, pie, bar...\n16\n(a) Accuracy Performance.\n(b) Relevancy Performance.\nFigure 8: Evaluation results on positive and negative instructions by GAVIE.\n(a) Accuracy Performance.\n(b) Relevancy Performance.\nFigure 9: Evaluation results on different instruction lengths by GAVIE.\nPerception\nExistence Count Position Color Posters Celebrity Scene Landmark Artwork OCR\nOriginal MiniGPT4\n68.33\n55.00 43.33\n75.00 41.84\n54.41\n71.75\n54.00\n60.50 57.50\nFinetuned MiniGPT4\n115.0\n88.33 68.33\n96.67 71.42\n72.35\n122.00\n104.34\n77.50 80.00\nOriginal mPLUG-Owl\n120.00\n50.00 50.00\n55.00 136.05 100.29 135.50\n159.25\n96.25 65.00\nFinetuned mPLUG-Owl 165.00 111.67 86.67 165.00 139.04 112.65 147.98\n160.53\n101.25 110.0\nTable 11: Completed experiments of Perception on MME [9] benchmark.\nCognition\nCommonsense Reasoning Numerical Calculation Text Translation Code Reasoning\nOriginal MiniGPT4\n59.29\n45.00\n0.00\n40.00\nFinetuned MiniGPT4\n76.42\n55.00\n77.50\n67.50\nOriginal mPLUG-Owl\n78.57\n60.00\n80.00\n57.50\nFinetuned mPLUG-Owl\n100.71\n70.00\n85.00\n72.50\nTable 12: Completed experiments of Cognition on MME [9] benchmark.\n17\nFigure 10: Distribution of Knowledge Manipulations. The knowledge mainly includes six categories:\nevent, number, date, persons, place, and others.\n18\nImage:\nInput Prompt:\nGive  an image with following information: bounding box, positions that are the object left-top corner coordinates(X, Y), object sizes(Width, Height). \nHighly overlapping bounding boxes may refer to the same object.\nbounding box:\nelephant heard on rocks X: 73 Y: 80 Width: 418 Height: 418\nwoman wearing straw hat X: 179 Y: 285 Width: 23 Height: 43\nwoman wearing long dress X: 176 Y: 298 Width: 35 Height: 83\ngroup of green chairs X: 153 Y: 326 Width: 95 Height: 126\norange bucket on sidewalk X: 80 Y: 334 Width: 50 Height: 60\nman wearing white shirt X: 204 Y: 439 Width: 51 Height: 52\nbaby elephant behind adult elephant X: 244 Y: 235 Width: 119 Height: 155\ngroup of umbrellas on deck X: 82 Y: 72 Width: 136 Height: 83\nperson wearing yellow shirt X: 202 Y: 270 Width: 35 Height: 46\nwater is dark green X: 252 Y: 87 Width: 231 Height: 99\na baby elephant X: 297 Y: 317 Width: 60 Height: 69\nan orange bucket on the ground X: 91 Y: 341 Width: 38 Height: 36\na group of white umbrellas X: 99 Y: 82 Width: 112 Height: 28\na group of green chairs X: 149 Y: 364 Width: 94 Height: 86\na man in an orange shirt X: 204 Y: 265 Width: 31 Height: 47\na blue tin awning X: 65 Y: 190 Width: 88 Height: 95\na woman wearing a yellow dress X: 169 Y: 298 Width: 47 Height: 76\na woman wearing a tan hat X: 173 Y: 288 Width: 38 Height: 79\na man sitting down X: 200 Y: 425 Width: 65 Height: 72\na man wearing a white shirt X: 196 Y: 422 Width: 80 Height: 72\na elephant coming out of the water X: 384 Y: 219 Width: 88 Height: 88\na man sitting in a chair X: 200 Y: 428 Width: 79 Height: 67\na orange basket X: 68 Y: 329 Width: 77 Height: 69\nblue chairs on a deck X: 150 Y: 325 Width: 98 Height: 142\nelephants walking on rocks X: 152 Y: 161 Width: 261 Height: 239\na baby elephant X: 280 Y: 295 Width: 98 Height: 105\nwhite umbrellas outside a building X: 91 Y: 66 Width: 161 Height: 53\na white tiled staircase X: 47 Y: 367 Width: 109 Height: 126\nsomeone taking a photo X: 12 Y: 3 Width: 75 Height: 73\npeople standing on a deck X: 104 Y: 166 Width: 153 Height: 165\nplastic blue chairs  X: 146 Y: 318 Width: 93 Height: 129\na herd of asian elephants  X: 83 Y: 110 Width: 409 Height: 308\nthe elephant is stepping out of the river  X: 388 Y: 218 Width: 86 Height: 114\na small elephant  X: 302 Y: 309 Width: 71 Height: 95\nthis man is dressed in white  X: 208 Y: 416 Width: 49 Height: 82\nthis man is dressed in white  X: 208 Y: 416 Width: 49 Height: 82\nshe is wearing a sun hat and a dress  X: 175 Y: 287 Width: 64 Height: 112\nthe people are gathered  X: 88 Y: 165 Width: 163 Height: 157\nthey are watching the elephants  X: 87 Y: 167 Width: 165 Height: 179\nthis person is recording the elephants  X: 18 Y: 6 Width: 74 Height: 113\na rapid flowing river  X: 349 Y: 73 Width: 143 Height: 243\na herd of a elephants moving through a town X: 78 Y: 108 Width: 386 Height: 274\nthe trunk of an elephant X: 395 Y: 261 Width: 15 Height: 21\na blue awning of a building X: 66 Y: 187 Width: 91 Height: 104\nblue chairs on a patio X: 152 Y: 324 Width: 76 Height: 120\nwhite umbrellas on a patio  X: 104 Y: 84 Width: 113 Height: 30\na trunk of an elephant X: 335 Y: 254 Width: 14 Height: 27\nthe tail of an elephant X: 308 Y: 233 Width: 16 Height: 32\nan orange bucket of on a patio X: 95 Y: 348 Width: 30 Height: 33\nthe leaves of a tree X: 200 Y: 17 Width: 41 Height: 33\nthe leaves of a tree X: 361 Y: 36 Width: 43 Height: 23\nTask: image captioning, Image Sentiment Analysis,  Activity Recognition, Object Interaction Analysis, Object Attribute Detection, Muli-choice VQA, \nVisual Entailment, Visual Commonsense reasoning, Styled Image Caption, Referential Expression Grounding.\nCome up with 20 diverse instructions for all the tasks above with different language styles and accurate answers. The instructions should contain \ninterrogative sentences and declarative sentences. The answers should be less than 30 words. Each task should have less than 3 instructions.\nOutput format:\nInstruction: \ninstruction example\nAnswer: \nanswer example\nTask: \ntask sample\nPositive Instance Generation\nFigure 11: The first example for generating positive instruction and answers (Part1). The image is\nnot included in the text prompt.\n19\nOutput:\nInstruction:\nCraft a brief narrative about the baby elephant and adult elephant.\nAnswer: \nA baby elephant is depicted behind an adult elephant, possibly seeking protection.\nTask: \nImage Captioning\nInstruction:\nWrite a concise description of the entire scene in this image.\nAnswer:\nA herd of elephants, including baby elephants, is seen near a river with people watching and taking photos. There are also green chairs, white \numbrellas, and blue awnings nearby.\nTask:\nImage Captioning\nInstruction:\nWhat is the overall sentiment conveyed by this image?\nAnswer: \nThe sentiment of the image is predominantly peaceful and joyful, showcasing the harmonious interaction of wildlife with human presence.\nTask:\nImage Sentiment Analysis\nInstruction:\nHow would you interpret the sentiment of the scene involving the man in white shirt?\nAnswer: \nThe man in the white shirt exudes a feeling of calmness and interest as he observes the scene.\nTask:\nImage Sentiment Analysis\nInstruction:\nExplain how the elephants and the humans interact in this image.\nAnswer: \nThe humans are observing and photographing the elephants, while the elephants seem to be moving freely and peacefully around them.\nTask:\nObject Interaction Analysis\nInstruction:\nAnalyze the interaction between the objects in the foreground and the background.\nAnswer: \nThe foreground, featuring green chairs and people, provides a contrast to the background where the elephants are moving.\nTask:\nObject Interaction Analysis\nInstruction:\nWhich type of animal is featured prominently in the image? Choices: A) Lions B) Elephants C) Giraffes D) Zebras\nAnswer: \nB) Elephants\nTask:\nMulti-choice VQA\nInstruction:\nWhat is the primary color of the umbrellas present in the image? Choices: A) Blue B) Green C) White D) Red\nAnswer: \nC) White\nTask:\nMulti-choice VQA\nInstruction: \nBased on the image, does it entail that the people are afraid of the elephants?\nAnswer: \nNo, the image does not entail that the people are afraid of the elephants.\nTask:\nVisual Entailment\nInstruction: \nBased on the image, does it entail that the location is in a city center or on the outskirts?\nAnswer: \nThe image entails that the location is more likely on the outskirts, not in a city center. The presence of a herd of elephants suggests a location \ncloser to natural habitats.\nTask:\nVisual Entailment\nPositive Instance Generation\nFigure 12: The first example for generating positive instruction and answers (Part2).\n20\nOutput:\nInstruction:\nIdentify and describe the attributes of the bucket in the image.\nAnswer: \nThe bucket is orange, possibly made of plastic, and appears to be on a sidewalk or patio.\nTask:\nObject Attribute Detection\nInstruction:\nAssess the color of the baby elephant. Is it gray or brown?\nAnswer: \nWithout visual data, we cannot accurately determine the color of the baby elephant.\nTask:\nObject Attribute Detection\nInstruction:\nBased on the image, what is the probable reason for the congregation of people?\nAnswer: \nThe probable reason for the congregation of people is to observe the elephants passing through.\nTask:\nVisual Commonsense Reasoning\nInstruction: \nWhy might the baby elephant be close to the adult elephant?\nAnswer: \nThe baby elephant is likely close to the adult elephant for protection and guidance as they move through the area.\nTask:\nVisual Commonsense Reasoning\nInstruction: \nLocate \"the man dressed in white\".\nAnswer: \nThe man dressed in white is located at the coordinates (208, 416), with a width of 49 and a height of 82.\nTask:\nReferential Expression Grounding\nInstruction: \nIdentify the location of \"the woman wearing a straw hat\".\nAnswer: \nThe woman wearing a straw hat is located at X: 179 Y: 285 with Width: 23 and Height: 43.\nTask:\nReferential Expression Grounding\nInstruction: \nProvide a poetic caption for the image.\nAnswer: \nBeneath a sky kissed by the sun, a parade of ancient giants meanders, while observers stand still in hushed admiration.\nTask:\nStyled Image Caption\nInstruction: \nCaption the image in a romantic style.\nAnswer: \nIn a scene gently touched by nature's grandeur, silent stories of love and companionship play out as the elephants pass by.\nTask:\nStyled Image Caption\nInstruction:\nIdentify the primary activity happening in the scene described by the image.\nAnswer:\nThe primary activity in the image is observing and photographing a herd of elephants moving through a town.\nTask:\nActivity Recognition\nInstruction:\nBy analyzing the given information, what could you infer about the main event in the image?\nAnswer:\nThe main event in the image is a herd of elephants moving through a town while being observed and photographed by people.\nTask:\nActivity Recognition\nPositive Instance Generation\nFigure 13: The first example for generating positive instruction and answers (Part3).\n21\nImage:\nInput Prompt:\nGive an image with following information: bounding box, positions that are the object left-top corner coordinates(X, Y), object \nsizes(Width, Height). Highly overlapping bounding boxes may refer to the same object.\nbounding box:\neyebrow on the man X: 79 Y: 77 Width: 36 Height: 13\nforehead on the man X: 73 Y: 52 Width: 63 Height: 26\ntwo people playing game X: 2 Y: 1 Width: 330 Height: 497\nhair on the man X: 52 Y: 11 Width: 78 Height: 24\nwriting on mans shirt X: 0 Y: 366 Width: 91 Height: 47\nlogo on mans shirt X: 64 Y: 327 Width: 36 Height: 57\nwii controller in right hand X: 144 Y: 351 Width: 107 Height: 76\na pair of glasses on the man X: 16 Y: 91 Width: 149 Height: 32\nthe head of a man playing a game X: 0 Y: 7 Width: 153 Height: 203\nupper teeth of the young man X: 113 Y: 154 Width: 26 Height: 6\nblurry left hand of the man X: 192 Y: 310 Width: 80 Height: 78\na blue tshirt on the man X: 0 Y: 171 Width: 174 Height: 254\ncords of the wii game X: 234 Y: 387 Width: 52 Height: 111\na wide set of blinds X: 49 Y: 1 Width: 279 Height: 270\na young man on the sofa X: 197 Y: 190 Width: 134 Height: 278\nwriting and an insignia on a shirt X: 8 Y: 325 Width: 88 Height: 85\na boy wearing glasses X: 34 Y: 37 Width: 132 Height: 118\na boy playign a game X: 49 Y: 32 Width: 137 Height: 133\na white wii remote X: 138 Y: 341 Width: 160 Height: 74\na boy earing a shirt X: 24 Y: 212 Width: 164 Height: 269\nglasses on a mans face X: 34 Y: 86 Width: 131 Height: 44\nnintendo wii remote control X: 144 Y: 350 Width: 70 Height: 43\nnintendo wii gaming nunchuk X: 198 Y: 310 Width: 48 Height: 47\nthe man is asian X: 11 Y: 20 Width: 247 Height: 430\nthe man has a controller X: 129 Y: 332 Width: 145 Height: 115\nTask: Image Captioning, Image Sentiment Analysis,  Activity Recognition, Object Interaction Analysis, Object Attribute Detection, \nMuli-choice VQA, Visual Entailment, Visual Commonsense Reasoning, Styled Image Caption, Referential Expression Grounding.\nCome up with 20 diverse instructions for all the tasks above with different language styles and accurate answers. The instructions \nshould contain interrogative sentences and declarative sentences. The answers should be less than 30 words. Each task should \nhave less than 3 instructions.\nOutput format:\nInstruction: \ninstruction example\nAnswer: \nanswer example\nTask: \ntask sample\nPositive Instance Generation\nFigure 14: The second example for generating positive instruction and answers (Part1). The image is\nnot included in the text prompt.\n22\nOutput:\nInstruction:\nIdentify the primary activity taking place in the described image.\nAnswer:\nThe primary activity is two people playing a game.\nTask:\nActivity Recognition\nWhat is the sentiment expressed in the image based on the given details?\nAnswer:\nThe sentiment is joyful and engaged, as depicted by a man playing a game.\nTask:\nImage Sentiment Analysis\nProvide a concise caption for the given image.\nAnswer:\nAn Asian man joyfully playing a Wii game on the sofa, wearing a blue shirt with an insignia.\nTask:\nImage Captioning\nInstruction:\nSpecify the emotion depicted in the image based on the objects\nAnswer:\nThe emotion depicted is happiness, inferred from the activity and facial features of the man.\nTask:\nImage Sentiment Analysis\nInstruction:\nBased on the details given, state which object is interacting with the man.\nAnswer:\nThe man is interacting with the Wii controller.\nTask:\nObject Interaction Analysis\nInstruction:\nIdentify the unique attribute associated with the man in the given image.\nAnswer:\nThe unique attribute is that the man is Asian and wearing glasses.\nTask:\nObject Attribute Detection\nInstruction:\nFrom the listed objects, which one is closest to the man's face?\nAnswer:\nThe glasses are closest to the man's face.\nTask:\nReferential Expression Grounding\nInstruction:\nIs the statement \"The man is playing a video game\" true or false based on the image details?\nAnswer:\nTrue\nTask:\nVisual Entailment\nInstruction:\nBased on the information, is it reasonable to conclude that the man is indoors?\nAnswer:\nYes, it's reasonable, due to the presence of a sofa and a set of blinds.\nTask:\nVisual Commonsense Reasoning\nInstruction:\nWrite a caption for the image with a humorous style.\nAnswer:\n\"Multitasking man: mastering Wii, sporting stylish glasses, and showing off that shirt insignia all at once!\"\nTask:\nStyled Image Caption\nPositive Instance Generation\nFigure 15: The second example for generating positive instruction and answers (Part2).\n23\nOutput:\nInstruction:\nDetermine the color of the man's shirt in the image.\nAnswer:\nThe man's shirt is blue.\nTask:\nObject Attribute Detection\nInstruction:\nBased on the details, can we infer that the game being played is a Nintendo Wii game?\nAnswer:\nYes, we can infer that the game being played is a Nintendo Wii game due to the presence of a Wii controller and nunchuk.\nTask:\nVisual Commonsense Reasoning\nInstruction:\nDescribe the location of the man in relation to the set of blinds.\nAnswer:\nThe man is positioned in front of the set of blinds.\nTask:\nReferential Expression Grounding\nInstruction:\nGiven the data, could the statement \"The boy is playing a game with a PS4 controller\" be true?\nAnswer:\nNo, it's false because the controller specified is a Wii controller, not a PS4 controller.\nTask:\nVisual Entailment\nIdentify any identifiable attributes of the man in the given image.\nAnswer:\nThe man is young, Asian, wears glasses, has a shirt with a logo and writing, and is using a Wii controller.\nTask:\nObject Attribute Detection\nInstruction:\nIdentify the object the man is holding in his right hand based on the image. A) PS4 controller B) Wii controller C) Mobile phone D) Book\nAnswer:\nB) Wii controller\nTask:\nMuli-choice VQA\nInstruction:\nDetermine the color of the man's shirt from the image description. A) Red B) Black C) Blue D) Green\nAnswer:\nC) Blue\nTask:\nMuli-choice VQA\nInstruction:\nGiven the data, state the activity that the man and the boy are participating in.\nAnswer:\nThe man and the boy are participating in gaming.\nTask:\nActivity Recognition\nInstruction:\nUsing an old-fashioned style of speech, describe the image portrayed..\nAnswer:\nBehold, a youthful gent of Asian descent, donned in spectacles and a blue tunic, merrily engrossed in a game of the Nintendo Wii.\nTask:\nStyled Image Caption\nInstruction:\nBased on the image, create a vivid description of the image.\nAnswer:\nAn engaged Asian man with glasses and a logo-clad blue shirt, situated on a sofa, immerses himself in an exhilarating Wii game.\nTask:\nImage Captioning\nPositive Instance Generation\nFigure 16: The second example for generating positive instruction and answers (Part3).\n24\nImage:\nInput Prompt:\nGive an image with following information: bounding box, positions that are the object left-top corner \ncoordinates(X, Y), object sizes(Width, Height). Highly overlapping bounding boxes may refer to the same object.\nbounding box:\nthere is light blue sky in the distance X: 366 Y: 15 Width: 17 Height: 42\nthere is a white surfboard here X: 201 Y: 113 Width: 43 Height: 100\nthere are black swim trunks here X: 276 Y: 146 Width: 34 Height: 67\nthis man has long hair here X: 337 Y: 112 Width: 20 Height: 37\nthere is some wild water here X: 58 Y: 63 Width: 51 Height: 96\njackson mingus took this photo X: 173 Y: 31 Width: 165 Height: 174\nthis man has very hairy legs X: 261 Y: 137 Width: 15 Height: 26\nthere is a wonderful amount of coloring X: 176 Y: 53 Width: 122 Height: 167\nthis photo will soon be published in a magazine X: 161 Y: 27 Width: 64 Height: 120\nthis photo is a really artistic example X: 179 Y: 34 Width: 113 Height: 162\nblack and white surf board X: 190 Y: 85 Width: 106 Height: 150\nblack cotton swim trunks X: 268 Y: 149 Width: 49 Height: 59\nwater splashed in air X: 51 Y: 65 Width: 174 Height: 137\nblack decal on surf board X: 221 Y: 153 Width: 57 Height: 75\nman standing on surf board X: 248 Y: 92 Width: 120 Height: 135\nman surfing in ocean X: 188 Y: 77 Width: horizon where ocean meets the sky X: 250 Y: 42 Width: 190 Height: 54\ncalm blue ocean water X: 412 Y: 77 Width: 73 Height: 41\nwhite surfboard mostly out of the water X: 194 Y: 70 Width: 95 Height: 165\nocean wave around surfboard X: 341 Y: 178 Width: 152 Height: 50\nwave breaking on shore X: 2 Y: 177 Width: 494 Height: 57\nwhite foam in water X: 4 Y: 203 Width: 184 Height: 32\nblack piece on surf board X: 190 Y: 96 Width: 22 Height: 19\nman riding a surf board X: 189 Y: 82 Width: 180 Height: 147\nman on surfboard X: 197 Y: 57 Width: 206 Height: 163\nwhite ocean spray from surfer X: 62 Y: 64 Width: 122 Height: 166\nthree blue fins on surfboard X: 185 Y: 88 Width: 51 Height: 60\nman with arms up to balance X: 212 Y: 69 Width: 162 Height: 157\nphotograph taken in the ocean X: 62 Y: 12 Width: 430 Height: 218\ndesign on bottom of surfboard X: 210 Y: 149 Width: 62 Height: 67\nlling off surfboard X: 26 Y: 12 Width: 470 Height: 223\nCome up with 6 misleading instructions with nonexistent elements (nonexistent objects, nonexistent activities, \nnonexistent attributes, nonexistent interactions) in the images with different language styles. The instructions \nshould contain interrogative and declarative sentences. Please also explain the reason.\nOutput format:\nInstruction: \ninstruction example\nReason:\nReason example\n Negative Instance Generation (Nonexistent Element Manipulation)\nFigure 17: The example for generating negative instruction and answers by nonexistent object\nmanipulation (Part1). The image is not included in the text prompt.\n25\nOutput:\nInstruction:\nCould you please zoom in on the red bird sitting on the surfboard?\nReason:\nThere is no mention of any bird, let alone a red one, in the provided bounding box \ninformation, which means this element is nonexistent in the image.\nInstruction:\nFind the green umbrella on the beach and note its position.\nReason:\nThe provided bounding boxes do not include any information about a beach or a green \numbrella. This makes the object nonexistent within the context of the image.\nInstruction:\nIdentify the interaction between the surfer and the shark in the water.\nReason:\nAlthough there's a man and water in the image, there's no indication of a shark. Therefore, \nthere's no interaction between a surfer and a shark in this image, making it a nonexistent \ninteraction.\nInstruction:\nCan you locate the cluster of palm trees in the background?\nReason:\nThe bounding boxes only describe a light blue sky, surfboard, swim trunks, man, water, and \nwave - there's no mention of any palm trees. Therefore, palm trees are a nonexistent element \nin this image.\nInstruction:\nLook for the sunset reflecting off the ocean waves.\nReason:\nThere is no information given that suggests a sunset scene in the image. The sky is \ndescribed as light blue, which suggests a day scene, making a sunset a nonexistent attribute.\nInstruction:\nHighlight the second surfer trying to overtake the first one.\nReason:\nThe image only contains bounding box data for a single man, who can be assumed to be a \nsurfer based on other objects (like the surfboard). The second surfer is therefore a \nnonexistent object.\nNegative Instance Generation (Nonexistent Element Manipulation)\nFigure 18: The example for generating negative instruction and answers by nonexistent object\nmanipulation (Part2).\n26\nImage:\nInput Prompt:\nGive an image with following information: bounding box, positions that are the object left-top corner \ncoordinates(X, Y), object sizes(Width, Height). Highly overlapping bounding boxes may refer to the same object.\nbounding box:\nthere is light blue sky in the distance X: 366 Y: 15 Width: 17 Height: 42\nthere is a white surfboard here X: 201 Y: 113 Width: 43 Height: 100\nthere are black swim trunks here X: 276 Y: 146 Width: 34 Height: 67\nthis man has long hair here X: 337 Y: 112 Width: 20 Height: 37\nthere is some wild water here X: 58 Y: 63 Width: 51 Height: 96\njackson mingus took this photo X: 173 Y: 31 Width: 165 Height: 174\nthis man has very hairy legs X: 261 Y: 137 Width: 15 Height: 26\nthere is a wonderful amount of coloring X: 176 Y: 53 Width: 122 Height: 167\nthis photo will soon be published in a magazine X: 161 Y: 27 Width: 64 Height: 120\nthis photo is a really artistic example X: 179 Y: 34 Width: 113 Height: 162\nblack and white surf board X: 190 Y: 85 Width: 106 Height: 150\nblack cotton swim trunks X: 268 Y: 149 Width: 49 Height: 59\nwater splashed in air X: 51 Y: 65 Width: 174 Height: 137\nblack decal on surf board X: 221 Y: 153 Width: 57 Height: 75\nman standing on surf board X: 248 Y: 92 Width: 120 Height: 135\nman surfing in ocean X: 188 Y: 77 Width: horizon where ocean meets the sky X: 250 Y: 42 Width: 190 Height: 54\ncalm blue ocean water X: 412 Y: 77 Width: 73 Height: 41\nwhite surfboard mostly out of the water X: 194 Y: 70 Width: 95 Height: 165\nocean wave around surfboard X: 341 Y: 178 Width: 152 Height: 50\nwave breaking on shore X: 2 Y: 177 Width: 494 Height: 57\nwhite foam in water X: 4 Y: 203 Width: 184 Height: 32\nblack piece on surf board X: 190 Y: 96 Width: 22 Height: 19\nman riding a surf board X: 189 Y: 82 Width: 180 Height: 147\nman on surfboard X: 197 Y: 57 Width: 206 Height: 163\nwhite ocean spray from surfer X: 62 Y: 64 Width: 122 Height: 166\nthree blue fins on surfboard X: 185 Y: 88 Width: 51 Height: 60\nman with arms up to balance X: 212 Y: 69 Width: 162 Height: 157\nphotograph taken in the ocean X: 62 Y: 12 Width: 430 Height: 218\ndesign on bottom of surfboard X: 210 Y: 149 Width: 62 Height: 67\nman falling off surfboard X: 26 Y: 12 Width: 470 Height: 223\nCome up with 6 misleading instructions with existing objects with wrong attributes  in the images with different \nlanguage styles. The instructions should contain interrogative and declarative sentences. Please also explain the \nreason.\nOutput format:\nInstruction: \ninstruction example\nReason:\nReason example\nNegative Instance Generation (Existent Element Manipulation)\nFigure 19: The example for generating negative instruction and answers by existent object manipula-\ntion (Part1). The image is not included in the text prompt.\n27\nOutput\nInstruction:\nDid you spot the surfer's pink board riding the wave?\nReason:\nThe surfboard in the image is described as white, not pink. This misleading instruction \nchanges the color attribute of the surfboard.\nInstruction:\nLook closely, isn't that a calm lake in the distance?\nReason:\nThe water body in the image is described as a part of an ocean, not a calm lake. This \nmisleading instruction alters the context and setting of the photo.\nInstruction:\nThe man in the picture is sporting a buzz cut, isn't he?\nReason:\nThe man in the image is described as having long hair, not a buzz cut. This misleading \ninstruction misrepresents the attribute related to the man's hairstyle.\nInstruction:\nThat's a serene morning sky in the background, don't you think?\nReason:\nThe sky in the image is described as a light blue sky, which does not necessarily mean \nit's morning. This misleading instruction gives an incorrect time context.\nInstruction:\nIsn't that surfboard fully immersed in the water?\nReason:\nThe surfboard is described as being mostly out of the water, not fully immersed. This \nmisleading instruction changes the positioning of the surfboard relative to the water.\nInstruction:\nThe man on the surfboard is performing a complex trick, isn't he?\nReason:\nThe man on the surfboard is described as having his arms up to balance, which does \nnot imply he is performing a complex trick. This misleading instruction misrepresents \nthe man's action on the surfboard.\nNegative Instance Generation (Existent Element Manipulation)\nFigure 20: The first example for generating negative instruction and answers by existent object\nmanipulation (Part2).\n28\nImage:\nInput Prompt:\nGive  an image with following information: bounding box, positions that are the object left-top corner coordinates(X, Y), \nobject sizes(Width, Height). Highly overlapping bounding boxes may refer to the same object.\nbounding box:\nfood is in a tray X:20 Y:55 Width:470 Height:470\nthe tray is white X:18 Y:56 Width:471 Height:471\nsome pieces of chicken X:85 Y:149 Width:142 Height:142\npile of white rice X:218 Y:112 Width:196 Height:196\nthe carrot is orange X:177 Y:116 Width:67 Height:67\na piece of broccoli X:83 Y:130 Width:52 Height:52\nthe spoon is white X:0 Y:7 Width:69 Height:69\nspoon and napkin in plastic wrapper X:0 Y:0 Width:135 Height:135\ntable is beige colored X:0 Y:17 Width:498 Height:498\nsauce on the tray X:382 Y:241 Width:72 Height:72\na plastic spoon in a wrapper X:1 Y:4 Width:70 Height:70\na beige tray  X:0 Y:17 Width:499 Height:499\na serving of white rice  X:220 Y:115 Width:194 Height:194\nbeef cubes with brown sauce  X:86 Y:151 Width:140 Height:140\nbrown sauce on the side of a white container  X:363 Y:228 Width:102 Height:102\na baby carrot  X:173 Y:115 Width:70 Height:70\nbots of cauliflower between two carrots  X:138 Y:95 Width:76 Height:76\na bit of broccoli X:82 Y:127 Width:51 Height:51\nrice beef and veggie in a plastic container X:83 Y:96 Width:332 Height:332\na white plastic container  X:19 Y:57 Width:472 Height:472\ncircle of rice in a plate X:260 Y:119 Width:135 Height:135\ncut up pieces of meat X:93 Y:173 Width:45 Height:45\nsmall part of broccoli next to meat X:88 Y:130 Width:54 Height:54\nsmall part of cut up carrot X:98 Y:98 Width:73 Height:73\nmeat sauce on the side of plate X:428 Y:228 Width:35 Height:35\ncut up cauliflower in the corner X:170 Y:98 Width:49 Height:49\nsmall part of plastic spoon in the corner X:1 Y:2 Width:75 Height:75\ntan folding table holding food X:243 Y:14 Width:182 Height:182\nsmall piece of napkin wrapped in plastic X:67 Y:8 Width:84 Height:84\nsilver part of table attached to a table X:396 Y:1 Width:72 Height:72\nportion of cooked white rice X:213 Y:114 Width:200 Height:200\nkalua pork X:88 Y:150 Width:140 Height:140\nmixed cooked vegetables X:85 Y:95 Width:162 Height:162\nto go container with meat rick and vegetables X:21 Y:56 Width:469 Height:469\nmeal in bottom half of to go container X:21 Y:55 Width:469 Height:469\ncooked carrot piece X:177 Y:117 Width:70 Height:70\ncooked cauliflower piece X:164 Y:95 Width:54 Height:54\npiece of cooked chopped broccoli X:84 Y:126 Width:49 Height:49\ncooked meat with skin on X:82 Y:153 Width:149 Height:149\nplastic to go spoon and napkin in plastic wrap X:3 Y:1 Width:132 Height:132\nserving of white rice  X:230 Y:131 Width:187 Height:187\nmixed veggies in container  X:99 Y:104 Width:145 Height:145\nchicken with sauce in container  X:89 Y:162 Width:132 Height:132\nwhite container with food  X:69 Y:65 Width:428 Height:428\nplastic spoon in wrap  X:3 Y:3 Width:98 Height:98\nbeige tray with food on it  X:1 Y:28 Width:487 Height:487\nsauce inside white container  X:57 Y:252 Width:67 Height:67\nbaby carrot beside rice  X:167 Y:117 Width:78 Height:78\ncauliflower beside baby carrot  X:145 Y:105 Width:67 Height:67\ncrispy chicken with glaze  X:91 Y:187 Width:130 Height:130\nGPT4-Assisted Visual Instruction Evaluation\nFigure 21: The first example for the GPT4-Assisted Visual Instruction Evaluation (Part1). The image\nis not included in the text prompt.\n29\nHere is the instruction for the image:\nDon't forget to notice the red wine glass beside the white container.\nAnswer1: The instruction is misleading because there are no mentions of a red wine glass \nin the image. The declarative sentence in this instruction suggests that the reader should \npay attention to a non-existent object.\nAnswer2: Yes, I see the red wine glass and the white container.</s>\nAnswer3: Based on the image, the meal consists of a combination of meat and vegetables, \nincluding rice, broccoli, and carrots, served on a white plate on a dining table. What can I \ndo for you today?\nAnswer4: there is a red wine glass beside the white container\nAnswer5: Don't forget to notice the red wine glass beside the white container. It's a nice \ntouch. This is a great idea. I'll have to try it. Thanks for sharing! I love this idea! I'm \ndefinitely going to try it. Thanks for sharing! I love this idea! I'm definitely going to try it. \nThanks for sharing! I love this idea! I'm definitely going to try it. Thanks for sharing! I love \nthis idea! I'm definitely going to try it. Thanks for sharing! I love this.\nSuppose you are a smart teacher, after looking at the image information above, please \nscore the above answers(0-10) according to the following criteria:\n1: whether the response directly follows the instruction\n2: whether the response is accurate concerning the image content.\nOutput format:\nrelevancy: \nscore of answer1: \nreason: \nscore of answer2: \nreason: \nscore of answer3: \nreason: \nscore of answer4: \nreason: \nscore of answer5: \nreason: \naccuracy: \nscore of answer1: \nreason: \nscore of answer2: \nreason: \nscore of answer3: \nreason: \nscore of answer4: \nreason: \nscore of answer5: \nreason: \nGPT4-Assisted Visual Instruction Evaluation\nFigure 22: The first example for the GPT4-Assisted Visual Instruction Evaluation (Part2).\n30\nOutput:\nrelevancy:\nscore of answer1: 10\nreason: Answer 1 directly responds to the instruction by stating that the red wine glass isn't \npresent in the image.\nscore of answer2: 6\nreason: Even though the answer refers to the red wine glass, it's not relevant because there's no \nmention of a red wine glass in the image details.\nscore of answer3: 2\nreason: Answer 3 does not directly respond to the instruction. It describes the food and other \nobjects in the image, but doesn't mention the red wine glass at all.\nscore of answer4: 6\nreason: Answer 4 directly states there is a red wine glass beside the white container, even though \nthe image information doesn't support this.\nscore of answer5: 1\nreason: Answer 5 doesn't address the instruction and instead offers multiple vague, irrelevant \ncompliments about the idea.\naccuracy:\nscore of answer1: 10\nreason: Answer 1 accurately refers to the fact that there's no mention of a red wine glass in the \nimage details.\nscore of answer2: 1\nreason: Answer 2 inaccurately claims that there is a red wine glass, even though there's no \nmention of one in the image details.\nscore of answer3: 8\nreason: Even though it doesn't mention the wine glass, Answer 3 provides a detailed and accurate \ndescription of the image based on the information given.\nscore of answer4: 1\nreason: Answer 4 inaccurately claims that there is a red wine glass beside the white container, \ncontradicting the image details.\nscore of answer5: 1\nreason: Answer 5 neither addresses the instruction nor accurately reflects the image content. It \nmentions a red wine glass but there's no information about it in the image details. Furthermore, its \nrepeated statements seem nonsensical and irrelevant to the task.\nGPT4-Assisted Visual Instruction Evaluation\nFigure 23: The first example for the GPT4-Assisted Visual Instruction Evaluation (Part3).\n31\nImage:\nInput Prompt:\nGive  an image with following information: bounding box, positions that are the object left-top corner coordinates(X, Y), \nobject sizes(Width, Height). Highly overlapping bounding boxes may refer to the same object.\nbounding box:\nfood is in a tray X:20 Y:55 Width:470 Height:470\nthe tray is white X:18 Y:56 Width:471 Height:471\nsome pieces of chicken X:85 Y:149 Width:142 Height:142\npile of white rice X:218 Y:112 Width:196 Height:196\nthe carrot is orange X:177 Y:116 Width:67 Height:67\na piece of broccoli X:83 Y:130 Width:52 Height:52\nthe spoon is white X:0 Y:7 Width:69 Height:69\nspoon and napkin in plastic wrapper X:0 Y:0 Width:135 Height:135\ntable is beige colored X:0 Y:17 Width:498 Height:498\nsauce on the tray X:382 Y:241 Width:72 Height:72\na plastic spoon in a wrapper X:1 Y:4 Width:70 Height:70\na beige tray  X:0 Y:17 Width:499 Height:499\na serving of white rice  X:220 Y:115 Width:194 Height:194\nbeef cubes with brown sauce  X:86 Y:151 Width:140 Height:140\nbrown sauce on the side of a white container  X:363 Y:228 Width:102 Height:102\na baby carrot  X:173 Y:115 Width:70 Height:70\nbots of cauliflower between two carrots  X:138 Y:95 Width:76 Height:76\na bit of broccoli X:82 Y:127 Width:51 Height:51\nrice beef and veggie in a plastic container X:83 Y:96 Width:332 Height:332\na white plastic container  X:19 Y:57 Width:472 Height:472\ncircle of rice in a plate X:260 Y:119 Width:135 Height:135\ncut up pieces of meat X:93 Y:173 Width:45 Height:45\nsmall part of broccoli next to meat X:88 Y:130 Width:54 Height:54\nsmall part of cut up carrot X:98 Y:98 Width:73 Height:73\nmeat sauce on the side of plate X:428 Y:228 Width:35 Height:35\ncut up cauliflower in the corner X:170 Y:98 Width:49 Height:49\nsmall part of plastic spoon in the corner X:1 Y:2 Width:75 Height:75\ntan folding table holding food X:243 Y:14 Width:182 Height:182\nsmall piece of napkin wrapped in plastic X:67 Y:8 Width:84 Height:84\nsilver part of table attached to a table X:396 Y:1 Width:72 Height:72\nportion of cooked white rice X:213 Y:114 Width:200 Height:200\nkalua pork X:88 Y:150 Width:140 Height:140\nmixed cooked vegetables X:85 Y:95 Width:162 Height:162\nto go container with meat rick and vegetables X:21 Y:56 Width:469 Height:469\nmeal in bottom half of to go container X:21 Y:55 Width:469 Height:469\ncooked carrot piece X:177 Y:117 Width:70 Height:70\ncooked cauliflower piece X:164 Y:95 Width:54 Height:54\npiece of cooked chopped broccoli X:84 Y:126 Width:49 Height:49\ncooked meat with skin on X:82 Y:153 Width:149 Height:149\nplastic to go spoon and napkin in plastic wrap X:3 Y:1 Width:132 Height:132\nserving of white rice  X:230 Y:131 Width:187 Height:187\nmixed veggies in container  X:99 Y:104 Width:145 Height:145\nchicken with sauce in container  X:89 Y:162 Width:132 Height:132\nwhite container with food  X:69 Y:65 Width:428 Height:428\nplastic spoon in wrap  X:3 Y:3 Width:98 Height:98\nbeige tray with food on it  X:1 Y:28 Width:487 Height:487\nsauce inside white container  X:57 Y:252 Width:67 Height:67\nbaby carrot beside rice  X:167 Y:117 Width:78 Height:78\ncauliflower beside baby carrot  X:145 Y:105 Width:67 Height:67\ncrispy chicken with glaze  X:91 Y:187 Width:130 Height:130\nGPT4-Assisted Visual Instruction Evaluation\nFigure 24: The second example for the GPT4-Assisted Visual Instruction Evaluation (Part1). The\nimage is not included in the text prompt.\n32\nHere is the question for the image:\nWith respect to the image, is the statement \"The napkin is wrapped in the plastic spoon\" true or \nfalse?\nAnswer1: True\nAnswer2: True\nAnswer3: In the image, the statement is false. The napkin is not wrapped in the plastic spoon. \nInstead, there is a spoon resting on the tray beside the foam container, which is filled with rice, meat, \nand vegetables.\nAnswer4: \n10000000000000000000000000000000000000000000000000000000000000000000000000000000\n00000000000000000000000000000000000000000000000000000000000000000000000000000000\n00000000000000000000000000000000000000000000000000000000000000000000000000000000\n00000000000000\nAnswer5: With respect to the image, is the statement \"The napkin is wrapped in the plastic spoon\" \ntrue or false? With respect to the image, is the statement \"The plastic spoon is wrapped in the \nnapkin\" true or false? With respect to the image, is the statement \"The napkin is wrapped in the \nplastic spoon\" true or false? With respect to the image, is the statement \"The plastic spoon is \nwrapped in the napkin\" true or false? With respect to the image, is the statement \"The napkin is \nwrapped in the plastic spoon\"\nSuppose you are a smart teacher, after looking at the image information above, please score the \nabove answers(0-10) according to the following criteria:\n1: whether the response directly follows the instruction\n2: whether the response is accurate concerning the image content.\nOutput format:\nrelevancy: \nscore of answer1: \nreason: \nscore of answer2: \nreason: \nscore of answer3: \nreason: \nscore of answer4: \nreason: \nscore of answer5: \nreason: \naccuracy: \nscore of answer1: \nreason: \nscore of answer2: \nreason: \nscore of answer3: \nreason: \nscore of answer4: \nreason: \nscore of answer5: \nreason: \nGPT4-Assisted Visual Instruction Evaluation\nFigure 25: The second example for the GPT4-Assisted Visual Instruction Evaluation (Part2).\n33\nOutput:\nrelevancy:\nscore of answer1: 10\nreason: Answer1 gives a straight answer to the question as either true or false.\nscore of answer2: 10\nreason: Answer2 directly responds to the question by stating if it's true or false.\nscore of answer3: 10\nreason: While Answer3 provides more details than necessary, it does answer the question \ndirectly and completely.\nscore of answer4: 1\nreason: Answer4 does not address the question at all. It appears to be a series of numbers \nwithout any clear relation to the question.\nscore of answer5: 1\nreason: Answer5 does not directly answer the question. It repeats the question multiple \ntimes without providing an answer.\nAccuracy:\nscore of answer1: 1\nreason: Answer1 is not accurate. The image information doesn't state that the napkin is \nwrapped in the spoon.\nscore of answer2: 1\nreason: Answer2 is incorrect. The image information doesn't state that the napkin is \nwrapped in the spoon.\nscore of answer3: 10\nreason: Answer3 is accurate. The image information supports the statement made in the \nanswer that the spoon and the napkin are both in plastic, not that the napkin is wrapped in \nthe spoon.\nscore of answer4: 1\nreason: The response is not relevant to the question and does not refer to the image \ncontent. It is not an accurate answer.\nscore of answer5: 1\nreason: This response fails to provide an accurate answer, instead repeating the question \nmultiple times without providing any answer.\nGPT4-Assisted Visual Instruction Evaluation\nFigure 26: The second example for the GPT4-Assisted Visual Instruction Evaluation (Part3).\n34\nMiniGPT4:  The water bottle is located near the yellow tennis ball on the ground. Can you \nlocate the water bottle placed near the yellow tennis ball on the ground?\nLLaVA:  Unfortunately, as an AI visual assistant, I am not capable of moving or locating \nobjects in a speci\ufb01c environment. However, given the image description, the water bottle \nis near the yellow tennis ball on the ground, and the boy is walking up to it, so you should \nbe able to see the bottle soon enough.\nInstructBLIP:  Yes, the water bottle is located near the yellow tennis ball on the ground.\nMMGPT: Yes, there is a water bottle placed near the yellow tennis ball on the ground.\nOurs:  There is no object resembling a water bottle mentioned in the image. This question \nintroduces a new object that doesn't exist in the image.\nInstruction: Can you locate the water bottle placed near the yellow tennis ball on the \nground? \n(a) All LMMs except ours produce inconsistent descriptions with the images. MiniGPT4 also has repetitive\nsentences.\nMiniGPT4:  There is a painting of a sunset hanging above the stove in the kitchen. The \nkitchen has a blue and white color scheme, and there are some grey tiles on the \ufb02oor. \nThere is a white oven on one wall, a fridge on another wall, and a microwave oven on \nthe third wall. The kitchen also has a small dining table with two chairs, and there is a \nsmall window above the table.\nLLaVA: The painting of a sunset hanging above the stove creates a visually appealing \nambiance in the kitchen. It adds warmth and a sense of tranquility to the space. The \nkitchen appears to be empty, with a stove, a sink, and some cabinets. Perhaps you could \nalso consider adding a few decorative elements, such as plants or colorful kitchen \ntowels, to further enhance the space and make it more inviting.\nInstructBLIP: There is a painting of a sunset hanging above the stove in the kitchen, \nwhich adds a touch of warmth and beauty to the space. The painting might be a \ndecorative element that complements the overall design of the kitchen, making it more \nvisually appealing and inviting.\nMMGPT: What is the color of the sunset in the painting?\nOurs:  This instruction misleads the viewer into believing that there is a painting in the \nkitchen when there is no mention of a painting in the provided image.. \nInstruction: You might notice a painting of sunset hanging over stove in the kitchen.\n(b) MiniGPT4, LLaVA, and InstructBLIP respond with long descriptive answers about the \"painting\", which\ndoes not exist in the image. Multimodal-GPT produces a question about the \"painting,\" but it does not address\nthe instruction.\nMiniGPT4:  The image shows a group of sheep grazing on the side of a mountain. The \nmountain is covered in rocks and there are clouds in the sky. The sheep appear to be \ngrazing on a \ufb01eld of green grass. The clouds in the sky are quite ominous and it appears to \nbe a stormy day.\nLLaVA: The pink \ufb02owers blooming at the bottom left corner of the image are likely a \nseasonal variety, as they are not seen growing year-round. Their appearance in the image \nsuggests that the photo was taken during a speci\ufb01c season when these \ufb02owers are in \nbloom.\nInstructBLIP: The pink \ufb02owers are blooming year-round.\nMMGPT: Identify  the pink \ufb02owers blooming at the bottom left corner of the image. \nAre they a seasonal variety or do they grow year-round? If they are seasonal, when \ndo they bloom? If they are year-round, what is the name of the plant. The pink \n\ufb02owers blooming at the bottom left corner of the image are a seasonal variety, as they are \nin bloom during the spring season.\nOurs: This instruction is misleading because there are no pink \ufb02owers n the image. The \ngiven image primarily focuses on outdoor objects such as sheep, mountains, and rocks.\nInstruction: Identify the pink \ufb02owers blooming at the bottom left corner of the image. Are \nthey a seasonal variety or do they grow year-round?.\n(c) MiniGPT4, LLaVA, and Multimodal-GPT respond with a long descriptive answer about the \"pink flowers\",\nwhich do not exist in the image. Although InstructBLIP\u2019s answer is brief, it\u2019s still false.\nFigure 27: Model predictions on negative instruction examples of Neg1:\"Nonexistent object Manipu-\nlation\". RED represents the nonexistent elements in the images.\n35\nMiniGPT4: The cat is actually on top of the curtain, which is not white but has different \ncolors, such as grey, brown and white.  \nLLaVA: Yes, the white curtain on top of the grey cat actually has a butter\ufb02y pattern.\nInstructBLIP:  Yes, the loose strands from the yarn blanket form a butter\ufb02y pattern. \nMMGPT: Yes, the white curtain on top of the grey cat is actually a colorful curtain with \nbutter\ufb02y patterns.\nOurs: This instruction is misleading because the white curtain is mentioned to be on the \ntop of the window, but it is not described as a colorful curtain with butter\ufb02y patterns.\nInstruction: Is the white curtain on top of the grey cat actually a colorful curtain with \nbutter\ufb02y patterns?\n(a) All LMMs except ours produce inconsistent descriptions with the images. Our model can point out that the\nloose strands do not form a butterfly pattern.\nMiniGPT4: Yes. The windows on the brick building are red-framed. \nLLaVA: Yes, the windows on the brick building have red frames\nInstructBLIP:  The windows on the building are white-framed.\nMMGPT: yes\nOurs: The windows on the building are white-framed.\nInstruction: Are the windows on the brick building red-framed?\n(b) All LMMs except ours produce false attributes of windows in the images. Our model can point out that the\nwindows are white instead of red.\nMiniGPT4: Yes, the woman has a black backpack on her shoulder. \nLLaVA: Yes, the woman with green hair has a black backpack on her shoulder.\nInstructBLIP:Yes, the woman with green hair has a black backpack on her \nshoulder. \nMMGPT: Yes, the woman with green hair has a black backpack on her shoulder.\nOurs: There is no mention of a woman with green hair and a black backpack in \nthe image. This instruction is misleading because it presents a false attribute (the \nbackpack) that is not present in the image.\nInstruction:  Does the woman with green hair have a black backpack on her shoulder?\n(c) All LMMs except ours produce inconsistent descriptions with the images. Our model can point out that the\nwoman with green hair doesn\u2019t have a black backpack on her shoulder.\nFigure 28: Model predictions on negative instruction examples of Neg2:\"Existent object Manipula-\ntion\". RED represents the wrong attributes of existent objects in the images.\n36\nMiniGPT4: Two teddy bears are sitting on the toddler's feet.\nLLaVA: The toddler has a teddy bear and a shoe on their feet.\nInstructBLIP: 2 objects are on the toddler's feet: socks and shoes.\nMMGPT: The toddler\u2019s feet are covered with a blanket, and there is a \nteddy bear on top of the blanket\u3002\nOurs: White socks\nInstruction:  What objects are on the toddler\u2019s feet?\n(a) Object Detection Task. MiniGPT4, LLaVa, and MMGPT can\u2019t locate the objects correctly.\nMiniGPT4: (d) Snowy and cold\nLLaVA: The correct statement about the weather conditions in the image \nis (a) Cloudy and rainy. The image features a red stop sign at the corner of \na street, and the sky is \ufb01lled with clouds, suggesting a rainy day. The \npresence of rain in the image, as opposed to the other options, provides \nmore speci\ufb01c information about the weather conditions\nInstructBLIP: \u201c\u201d.\nMMGPT: The correct statement about the weather conditions in the \nimage is \u00a9 Foggy and misty.\nOurs: (b) Clear blue sky\nInstruction:  Choose the correct statement about the weather conditions in the image: (a) \nCloudy and rainy, (b) Clear blue sky, (c) Foggy and misty, (d) Snowy and cold\n(b) Multi-choice VQA Task. InstructBLIP can\u2019t follow the instruction well by generating anything. LLaVA\u2019s\nanswer seems reasonable, but it incorrectly detects the clouds in the image.\nMiniGPT4: The room is tiled with black and white tiles.\nLLaVA: The tiled area in the room is the kitchen.\nInstructBLIP: The part of the room that is tiled is the \ufb02oor.\nMMGPT: The answer is kitchen.\nOurs: The \ufb02oor of the room is tiled.\nInstruction: From the image, tell me what part of the room is tiled.\n(c) Referential Expression Grounding Task. Other LMMs can\u2019t directly address the instruction by giving a\ngeneral answer, \"kitchen,\" while our model can clearly predict the \"floor\".\nFigure 29: Positive instruction demos with different tasks and predictions from different models.\n37\nMiniGPT4: There are some onions, carrots, mushrooms, and parsley in \nthe image.\nLLaVA: In the image, there are two pieces of broccoli located on a cutting \nboard. Additionally, there is a group of mushrooms and carrots on the \ntable. These vegetables appear fresh and ready to be used as ingredients \nfor a meal. However, there are no unusual elements in the scene, as it \nshows a typical assortment of cooking ingredients.\nInstructBLIP: The image features a wooden cutting board topped with a \nvariety of vegetables, including mushrooms, carrots, and parsley. These \ningredients are arranged in a pile on the cutting board, ready to be \nchopped or sliced for use in a recipe or meal. \nMMGPT: The image features a bowl of fresh vegetables, including \ncarrots, broccoli, and celery. There is also a knife placed on top of the \nbowl.\nOurs:  There are no unusual elements in the image.\nInstruction: Detect any unusual elements in the image.\n(a) Image Anomaly Detection Task. Our model and LLaVA point out there is a normal image without unusual\nelements, while MiniGPT4, InstructBLIP, and MMGPT describe the image instead of addressing the instruction.\nMiniGPT4: False\nLLaVA: The coffee table is wooden.\nInstructBLIP: The coffee table looks wooden. True.\nMMGPT: True\nOurs:  True.\nInstruction: The coffee table looks wooden. True or False?\n(b) Visual Entailment Task.\nMiniGPT4: The interaction between the red chair and the book is \nvisual interest.\nLLaVA: In the image, the red chair is positioned next to the bed, and a \nbook is placed on the chair. This arrangement suggests that someone \nmight have been reading in the bedroom or preparing to read before \nleaving the room. The presence of the book on the chair indicates an \ninteraction between the chair and the book, as they are in close \nproximity to each other, creating a cozy and comfortable atmosphere \nfor reading or relaxing in the bedroom.\nInstructBLIP: The red chair is placed near the white book.\nMMGPT: The interaction between the red chair and the book is that \nthe chair is placed next to the book, creating a cozy reading area.\nOurs:  The book is laying on the red chair.\nInstruction: Identify the interaction between the red chair and the book.\n(c) Object Interaction Analysis Task. All LMMs except ours describe the wrong location of the book. LLaVA\ngenerates long text with unrelated information to address the instruction.\nFigure 30: Positive instruction demos with different tasks and predictions from different models.\n38\nMiniGPT4: The horse's bridle is black and is located on a red truck..\nLLaVA: The horse has a white bridle, and its harness appears to be thrown \nonto the trailer.\nInstructBLIP: The horse's bridle is brown. \nMMGPT: The horse's bridle is red and is in a ready-to-wear state.\nOurs:  The bridle is pink and attaches on the horse's head.\nInstruction: What is the color and state of the horse's bridle?\n(a) Dense Description Task. All LMMs except ours incorrectly recognize the color of the bridle. InstructBLIP\nalso fails to answer the \"state of the horse\u2019s bridle\" in the instruction.\nMiniGPT4: The goats are grazing on the hillside, surrounded by rocks and \nmountains. The sky is a brilliant blue, and the goats are content in their natural \nenvironment.\nLLaVA: In the image, a group of four goats is seen grazing on grass and plants \non top of a rocky hillside. They are scattered around the area, with some of \nthem eating grass close to the rocks, while others are grazing further away. The \ngoats are surrounded by a mountainous landscape, which provides a scenic \nbackdrop for this peaceful moment.\nInstructBLIP: A herd of goats is grazing on a rocky hillside, surrounded by \nmountains in the background. The goats appear to be eating grass and other \nvegetation growing on the rocky terrain. The mountainous landscape provides \na scenic backdrop for the animals, adding to the natural beauty of the scene. \nMMGPT: The caption should be between 5-10 words. A herd of goats grazing \nin the mountains.\nOurs:  Flock of goats feasting on grass on a hilltop with large rocky mountains \nand green shrubs in the background, with a blue sky and cloudy weather.\nInstruction: Create a caption for the image using a sentence that describes the goats and their \nsurroundings.\n(b) Image Caption Task. LLaVA and InstructBLP fail to follow the instruction for generating one caption.\nInstead, they generate long text with several sentences.\nMiniGPT4: The person closest to the frisbee is throwing it\nLLaVA: The person closest to the frisbee is preparing to catch it, with their arms \noutstretched.\nInstructBLIP: The person closest to the frisbee is attempting to catch the \nfrisbee.  \nMMGPT: The person closest to the frisbee is throwing the frisbee.\nOurs:  The person is throwing the frisbee.\nInstruction: What action is the person closest to the frisbee performing?\n(c) Activity Recognition Task.\nFigure 31: Positive instruction demos with different tasks and predictions from different models.\n39\nQuestion: Did net sales increase in 2012 to 2013 and \nbegin to drop again?\nAnswer: No.Net sales dropped in 2012 to 2013 and \nbegan to rise again.\nQuestion: Did China contribute more than 25,000 jobs to the direct \nemployment of travel and tourism in the Asia Paci\ufb01c region in 2019, \nwhile India contributed less than 25,000 jobs?\nAnswer: No.The direct contribution of travel and tourism to \nemployment in the Asia Paci\ufb01c region in 2019 was carried by China \nand India who both contributed higher than 25,000 jobs each.\nQuestion: Did the number of police stop and \nsearches peak during 2009/08?\nAnswer: No.The number of police stop and searches \npeaked during 2008/09.\nQuestion: Is the title of the bar plot \"Least  valuable \nFrance brands in 2020\"?\nAnswer: No. the title of the bar plot \"Most  valuable \nItaly  brands in 2020\"?.\nQuestion: please summary the statistics and relations \nof the chart. \nAnswer: I can see that 2005 until 2019 the volunteer \nwork has almost his the 0.20 mark. In the years 2011 \nuntil 2012 is the lowest time in the years studied. \nThere isn\u2019t much variation between all of the years.\nQuestion: What is the prediction for Pakistan's \nnational debt in relation to GDP from 2020 onwards?\nAnswer:  From 2020 onwards, the national debt of \nPakistan (in relation to GDP) is predicted to decline.\n(a) More examples of chart instruction data and knowledge manipulation examples.\n(b) Comparison between current LMMs when the input is Knowledge Manipulation instruction.\nFigure 32: Examples of Knowledge Manipulations.\n40\n"
  },
  {
    "title": "Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window",
    "link": "https://arxiv.org/pdf/2306.13776.pdf",
    "upvote": "4",
    "text": "Swin-Free: Achieving Better Cross-Window Attention and Efficiency with\nSize-varying Window\nJinkyu Koo, John Yang, Le An, Gwenaelle Cunha Sergio, and Su Inn Park\nNVIDIA, 2788 San Tomas Expy, Santa Clara, CA 95051\n{jinkyuk, johnyang, lean, gcunhasergio, joshp}@nvidia.com\nAbstract\nTransformer models have shown great potential in\ncomputer vision, following their success in language\ntasks. Swin Transformer is one of them that outperforms\nconvolution-based architectures in terms of accuracy, while\nimproving efficiency when compared to Vision Transformer\n(ViT) and its variants, which have quadratic complexity with\nrespect to the input size. Swin Transformer features shifting\nwindows that allows cross-window connection while lim-\niting self-attention computation to non-overlapping local\nwindows. However, shifting windows introduces memory\ncopy operations, which account for a significant portion of\nits runtime. To mitigate this issue, we propose Swin-Free in\nwhich we apply size-varying windows across stages, instead\nof shifting windows, to achieve cross-connection among lo-\ncal windows. With this simple design change, Swin-Free\nruns faster than the Swin Transformer at inference with bet-\nter accuracy. Furthermore, we also propose a few of Swin-\nFree variants that are faster than their Swin Transformer\ncounterparts.\n1. Introduction\nUntil recently, convolutional neural network (CNN) had\nbeen leading the remarkable innovations in computer vi-\nsion tasks which had otherwise been considered too diffi-\ncult in the past, such as autonomous driving [2\u20137]. How-\never, the leading role of CNNs is recently being transferred\nto Transformer-based networks [1, 8, 9]. The Transformer\nmodel was first proposed for natural language processing\n(NLP) tasks, such as text classification and machine trans-\nlation, and it has demonstrated great success [10\u201312]. Such\na breakthrough in the language domain has sparked great in-\nterest in the computer vision community and recently lead\nto promising results on various tasks such as image classifi-\ncation [1,8] and semantic segmentation [13].\nThe key component in Transformer architecture is the\nself-attention module, which learns the relevance of one el-\nement to the other elements of a sequence. Unlike recur-\nrent networks, such as LSTM [14], that can only attend\nto context within a limited scope, the self-attention mech-\nanism explicitly models the interactions among all entities\nof a sequence. This allows Transformers to learn global\ncontext at once, resulting in their success in many applica-\ntions [8,12,15]. A drawback is, however, that computation\ncomplexity of the self-attention increases quadratically with\nrespect to the length of an input sequence. This can be a crit-\nical problem especially in computer vision tasks, since the\nsequence length, often determined by the image resolution,\ncan be intractably large.\nSwin Transformer [1] mitigates the quadratic complexity\nissue by partitioning an image into non-overlapping win-\ndows and computing self-attention within the local win-\ndows. To bridge the non-overlapping windows, Swin Trans-\nformer features shifting the window partition between con-\nsecutive self-attention layers, providing cross-connections\namong local windows. While this design choice leads to\nimproved efficiency and accuracy, the operations for shift-\ning windows incur data movement in memory. In fact, as\nshown in Table 1, shifting windows account for about 8.7%\nof the total runtime for a Swin Transformer model, when\ninference is performed with NVIDIA TensorRT [16].\nTo mitigate this shortcoming of Swin Transformer, we\npropose Swin-Free, which does not shift local windows in\norder to reduce data movement. Instead, to achieve cross-\nconnection among non-overlapping windows, Swin-Free\nvaries the size of windows across different stages (see Table\n2). For example, Swin-Free may double the window size\nat a stage in order to model cross-attention among smaller\nlocal windows of the previous stage.\nExperimental results show that Swin-Free featuring the\nsize-varying windows reduces the model runtime signifi-\ncantly as compared to Swin Transformer, mainly thanks\nto avoiding shifting windows and being able to leverage\nfaster matrix multiplication with larger inputs. Note that\non modern GPUs, efficient implementations of math op-\nerations such as convolution with large kernels are widely\navailable. In Swin-Free, a larger portion of its runtime is\n1\narXiv:2306.13776v1  [cs.CV]  23 Jun 2023\n(a) Conventional window-shifting block structures of Swin transformers [1]\n(b) Our proposed block structures with varying window sizes\nFigure 1. Comparison in functional blocks between Swin and Swin-Free. Note that in Swin-Free, shifting windows is removed and the\nsize of the local window varies across stages.\nTable 1. Operation profile of a Swin Transformer model (Swin-B)\non NVIDIA RTX 3080 GPU.\nOperation\nPercentage (%) in runtime\nTensorRT\n(FP16)\nPyTorch\n(FP32)\nShifting windows\n8.74\n4.39\nLayerNorm\n10.11\n9.63\nGELU\n13.46\n3.15\nspent on computation rather than memory copy, indicating\na better GPU utilization. At the same time, Swin-Free im-\nproves the classification accuracy as well, implying that the\nsize-varying windows can provide better modeling power\nthan shifting windows with a constant window size.\nWe also propose several variants of Swin-Free that pri-\noritize latency over accuracy. In other words, with on par\naccuracy, a variant of Swin-Free is designed to be faster\nthan its Swin Transformer counterpart. In addition, we fur-\nther simplify Swin-Free with more efficient layers such as\nBatchNorm and ReLU, instead of more commonly used but\nexpensive LayerNorm and GELU layers, which also ac-\ncount for significant part of the runtime (see Table 1). With\nthose design elements, we were able to improve the latency\nby 19% compared to Swin-B. In addition, we also show that\nby utilizing the improved modeling power of Swin-Free, we\ncan further reduce the depth of our model. For example, a\nvariant of Swin-Free is faster than Swin by about 33% with-\nout loss of accuracy (see Table 6).\n2. Related Work\nConvolutional Neural Network (CNN): Over the past\ndecade, CNNs have been the de facto standard in computer\nvision, and keep improving accuracy with innovations in ar-\nchitecture design [2\u20135]. In parallel, a lot of efforts have also\nbeen made to reduce the complexity of CNN models for ef-\nficiency. Such directions include model compression, quan-\ntization, and low cost operations such as depth-wise con-\nvolution [6, 7]. Although CNNs are still dominant in com-\nputer vision tasks, many recent works have demonstrated\nthat Transformer-based models outperform the state-of-the-\nart CNN-based models [1, 8, 9]. Arguably, we are about to\nsee a paradigm shift in computer vision from CNN to Trans-\n2\nformer.\nTransformer Architectures: Introduced in a pioneer\nwork [17] for machine translation tasks, Transformers have\nbecome the state-of-the-art models for NLP tasks, replacing\nmost of the LSTM-based sequence-to-sequence approaches\n[10\u201312,18,19]. As opposed to recurrent networks that pro-\ncess short-term context recursively, Transformer architec-\ntures are based on the attention mechanism, which explic-\nitly models the relative importance among all elements of a\nsequence, thereby learning sequence-wide relationships. In\nother words, Transformers process a sequence as a whole\nand recursion is totally avoided.\nTransformer in vision: With minimal vision-specific\nmodifications, ViT [8] applies the attention mechanism to\nimage classification tasks. As the counterpart of input token\nembeddings, ViT divides the images into patch embedding\nsequences and feeds them into a standard Transformer. ViT\noutperforms CNNs in image classifications, but it has been\noften reported to be difficult to train compared to CNNs.\nSince the computational complexity of the attention oper-\nation is quadratically proportional to the input size, ViT\nhas challenges to take high-resolution images in as inputs.\nOther Transformer-based vision models such as DETR [20]\nand SETR [21] also hold such a quadratic complexity issue.\n3. Preliminary: Swin Transformer\nSwin Transformer [1] leverages a multi-stage hierarchi-\ncal architecture, where the input image is first divided into\nsmall-sized patches and feature maps are gradually merged\nwith neighboring patches along the stages. With these hi-\nerarchical representations, Swin Transformer can easily be\napplied to dense prediction tasks such as object detection\nand segmentation. Swin Transformer achieves a linear com-\nputational complexity by computing self-attention within\nnon-overlapping local windows. To capture interactions be-\ntween local windows, the shifted window scheme that al-\nternates between two window configurations in consecutive\nTransformer blocks is employed.\nShifting windows plays a critical role in achieving Swin\nTransformer\u2019s claimed accuracy, but also introduces a lot of\nmemory movements. As shown in Table 1, the shifting win-\ndow operations in Swin-B (one of Swin Transformer vari-\nants) account for 8.7% of the total runtime with NVIDIA\nTensorRT (FP16 precision) and 4.4% with PyTorch (FP32\nprecision). This suggests that there is room for latency im-\nprovement if memory movements can be minimized.\nIn addition, LayerNorm and GELU used in Swin Tran-\nformer are also responsible for a significant portion of the\nruntime as shown in Table 1. Taking a look at those two op-\nerations in ONNX representation [22] in Figure 2, a cascade\nof math operations can be identified to fulfill those two lay-\ners. Previous study has suggested that by strategically us-\ning BatchNorm and ReLU layers, the accuracy of a Trans-\n(a) LayerNorm\n(b) GELU\nFigure 2. Examples of ONNX representations of LayerNorm and\nGELU.\nformer model will not be degraded much [23]. In this paper,\nwe attempt to improve on top of Swin Transformer for both\naccuracy and runtime, and propose Swin-Free, which will\nbe explained in the following section.\n4. Method\n4.1. Overview of Swin-Free\nOur baseline architecture shown in Figure 3 is similar to\nSwin Transformer [1], except that it does not use the shifted\nwindows. The input image is first patchified. Each stage\napplies a number of Swin-style Transformer blocks for the\npatches, where the self-attention computation is done within\neach of non-overlapping local windows.\nHere, the local\nwindow operates on an M \u00d7 M patch. Like in Swin Trans-\nformer, the number of patches are reduced by half at each\nstage by the patch merging layer. The only difference from\nSwin Transformer is that we do not shift the local windows.\nInstead, we choose to vary the size of the local window (i.e.,\nM) at each stage, which will be explained in more detail in\nSection 4.2.\nThe difference between Swin and Swin-Free for input\nsize 224 \u00d7 224 is summarized in Table 2. Note that in stage\n2 and 3, Swin-Free uses a larger window size than Swin,\nand therefore the number of non-overlapping windows in\n3\nFigure 3. Overall architecture of Swin-Free.\nTable 2. Comparison between Swin and Swin-Free for the input\nsize 224\u00d7224. Here, P means the number of patches at the be-\nginning of a stage. The values of M and N denote the size of a\nlocal window and the number of non-overlapping windows in a\nstage, respectively. Note that Swin Transformer applies shifting\nwindows in every other Transformer block, while Swin-Free does\nnot shift windows.\nStage\nSwin\nSwin-Free\n1\nP = 56 \u00d7 56\nM = 7\nN = 64\nP = 56 \u00d7 56\nM = 7\nN = 64\n2\nP = 28 \u00d7 28\nM = 7\nN = 16\nP = 28 \u00d7 28\nM = 14\nN = 4\n3\nP = 14 \u00d7 14\nM = 7\nN = 4\nP = 14 \u00d7 14\nM = 14\nN = 1\n4\nP = 7 \u00d7 7\nM = 7\nN = 1\nP = 7 \u00d7 7\nM = 7\nN = 1\nSwin-Free is smaller at those stages than in Swin. Figure 1\nalso shows how Swin-Free is different from Swin in detail\nat the block level. In Swin-Free, shifting windows and its\nreverse operation used in Swin Transformer are removed,\nand the size of the window changes with each stage.\n4.2. Size-Varying Windows\nShifting the local windows in Swin Transformer is an ef-\nfective way to achieve cross-connection among windows,\nbut it requires moving data in memory. This is typically\nmore costly than math computation on GPUs, and can there-\nfore negatively impact the model efficiency.\nIn fact, as\nshown in Table 1, shifting windows takes a considerable\nportion of the total runtime.\nTo avoid using the shifted windows, we enable cross-\nconnection between non-overlapping windows by changing\nthe size of the local windows at each stage. Recall that M is\nthe size of the local window. As Table 2 shows, in our im-\nplementations for the input size 224\u00d7224, we vary the value\nof M as M = 7, 14, 14, 7 for the four stages. From this\nsetup, we consider the cross-connection among four neigh-\nboring 7\u00d77 local windows at stages 2 and 3, i.e., a 14\u00d714\nlocal window in the current stage effectively includes four\nof 7\u00d77 local windows in the previous stage.\nThe above changes may increase GPU computation load\nof a single local window due to the enlarged window size in\nthe attention block. However, note in Table 2 that the num-\nber of non-overlapping local windows (i.e., N) in stages 2\nand 3 of Swin-Free becomes one fourth of that in Swin. In\nother words, in the matrix multiplication of Swin-Free, the\nmatrices\u2019 size is larger, but the number of matrices to be\nprocessed is smaller. We have observed that processing a\n14\u00d714 local window does not increase the latency as com-\npared to processing four of 7\u00d77 local windows on GPU, but\nrather decreased the latency, thanks to their massive paral-\nlel computing capability. We will discuss this point in more\ndetail in Section 5.\n4.3. Further Optimization\nReplacement of LayerNorm and GELU: As shown in\nFigure 2, LayerNorm and GELU are composed of multiple\nmath layers, which require more computation as compared\nto the commonly used BatchNorm and ReLU layers. In Ta-\nble 1, it is observed that LayerNorm and GELU account for\nabout 24% of the total runtime of a Swin Transformer model\nwhen running with TensorRT. Thus, when the latency is also\ncritical in an application, we replace them with BatchNorm\nand ReLU without significant accuracy degradation [23]. It\ncan be seen in Section 5 that such modification allows Swin-\nFree to run even faster while still surpassing Swin Trans-\nformer in terms of accuracy.\nDepth reduction: Another way to prioritize latency is\nto reduce the depth of a model. Specifically, we consider\nreducing the number of Transformer blocks at stage 3. For\nexample, compared to Swin-B, where stage 3 consists of 18\nTransformer blocks, we may consider using 14 blocks only.\nWe will see in Section 5 that this variant of Swin-Free can\nstill achieve better accuracy than Swin Transformer with\nsignificant improvement in latency.\n4\n5. Experiments\nOur focus of experiments is to compare Swin-Free with\nSwin Transformer in terms of both latency and accuracy\nin classification tasks. All latency results are measured us-\ning NVIDIA RTX 3080, PyTorch 1.13, and TensorRT 8.5.3\nwith CUDA 11.8. Evaluations are done with the ImageNet\ndataset [24] with 1K classes and input shape 224\u00d7224. We\nconsider the same variant models as in Swin Transformer,\nshown in Table 3a. Note that we do not consider the Large\n(L) variant with embedding dimension 192 used in Swin,\nsince it requires what is called the fall11 version of the 22K-\nclass dataset that is no longer available. Like Swin-B, we\nadd a post-fix to a model name to indicate its variant (e.g.,\nSwin-Free-B). Additionally, we also consider other variants\nresulting from the modification in Table 3b, mentioned in\nSection 4.3. These additional optimizations enhance the la-\ntency of a model, possibly at the cost of reduced accuracy.\nThe abbreviated symbols of these variants (i.e., BR or DRx)\nare also added as a post-fix to a model name.\n5.1. Shifted windows of Swin\nBefore going into the evaluation of Swin-Free, we first\nwant to understand the importance of the shifted window\nin each stage of Swin-B. Table 4 shows the top-1 accuracy\nof Swin-B depending on which stage has shifting windows\nenabled or disabled. Note that Case 1 uses the shifted win-\ndows for all stages, and thus it is exactly the same as Swin-\nB.1 We can first see from Case 8 that without the shifted\nwindows, it is even difficult to successfully complete train-\ning, and thus the shifted windows is indeed critical in Swin.\nWe can also see from Cases 4 to 7 that stage 3 is a critical\nstage to use the shifted windows. This is, to some extent, not\nsurprising, since stage 3 is a dominant portion of Swin-B.\nHowever, we can also see from Cases 1 to 3 that selectively\nusing the shifted windows over each stage marginally helps\nin increasing accuracy. Thus, it is important to apply them\nto all stages of Swin-B.\n5.2. Windows size of Swin-Free\nIn this section, we show which window size configura-\ntions are better suited for each stage of Swin-Free. To en-\nsure fair comparison with Swin, we assume that the input\nsize is 224\u00d7224 and the smallest windows size is 7. For\nthis reason, there are only two options for the window size\nat stages 1 to 3, which are 7 and 14, whereas stage 4 should\nalways have 7 as the window size. With that in mind, Table\n5 shows the latency and accuracy for all possible configu-\nrations that we can have from Swin-B with no shifted win-\ndows. It is worth mentioning that Case 1, with configuration\n1Even though we used the same training configuration, our Swin-B\u2019s\ntop-1 accuracy, trained from scratch, is 83.4%, which is slightly lower than\n83.5% reported in [1].\n\u20187, 7, 7, 7\u2019, is the same as Swin-B without shifted windows,\nwhich is the same as Case 8 of Table 4.\nWe can first notice from Cases 2 to 4 in Table 5 that the\nmost effective stage to use 14 as the window size is stage\n3. Increasing the window size to 14 at stage 3 leads to the\nbest latency and accuracy compared to using the window\nsize of 14 at stage 1 or 2. This would again come from the\nfact that the stage 3 is the dominant part of Swin-B in terms\nof depth. Using a 14\u00d714 local window at stage 3, we take\ncross-connection into account among four neighboring 7\u00d77\nlocal windows at stage 2. Note that using the larger window\nsize means that we need to handle larger-kernel matrix mul-\ntiplications, but the number of such matrix multiplications\n(i.e., the number of non-overlapping windows) gets smaller\n(refer to Table 2). Comparing latency results between Cases\n1 and 2, this rather helps reducing the latency. We may\nclaim the same improvement in latency at stage 1 or 2 by\nusing a window size of 14, but considering that those stages\nare of only depth two, we could not observe meaningful\nspeed-up there. See that Cases 3 and 4 get the same latency\nas Case 1 up to the first decimal point.\nIn Cases 5 to 7, we use the 14\u00d714 local window at two\nstages at the same time. We see that not using the 14\u00d714\nlocal window at stage 3 degrades both accuracy and latency,\nemphasizing the importance of stage 3 once again. We can\nalso see from Cases 5 and 6 that using the 14\u00d714 local\nwindow at stage 1 or 2 in addition to stage 3 meaningfully\nimproves latency over Case 2, resulting in them being the\nfastest variants.\nLooking at Case 8, using a window size of 14 at stages 1\nto 3 does not further improve the latency over Case 5 or 6.\nThe accuracy rather slightly decreases. The reason may be\nthat the modeling of cross-window connection is less effec-\ntive at early stages. From this study, we chose the configu-\nration of Swin-Free as Case 5 (as shown in Table 2), which\nwas one of the best ones in both accuracy and latency.\n5.3. Comparison between Swin and Swin-Free\nTable 6 lists all variants of Swin-Free and some of Swin\nfamily that we trained from scratch. First, from Cases 1\nand 6, we can compare Swin-Free with Swin for the Base\n(B) variant. Although Swin-Free-B has more FLOPs and\nparameters than Swin-B, we can see that Swin-Free-B is\nfaster than Swin-B at inference using either PyTorch (12.6\nms vs. 14.3 ms) or TensorRT (2.0ms vs. 2.1ms). From the\nstudy in Table 5, we understand this happens because Swin-\nFree-B has a smaller number of non-overlapping windows\nat stages 2 and 3, although each window is larger in Swin-\nFree-B.\nWe can also note that Swin-Free-B achieves better ac-\ncuracy than Swin-B. This implies that even without us-\ning the shifted windows, changing the size of the local\nwindow at certain stages can well model cross-connection\n5\nTable 3. Model variants: (a) We consider variants by changing hyper-parameters of a given architecture. (b) We apply architectural\nmodification to a given model. The abbreviated symbol of each variant is added to a model name as a postfix.\n(a) Variants by hyper-parameters.\nVariant\nEmbedding dimension per patch\n# of blocks at a stage (depth)\nTiny (T)\n96\n{2,2,6,2}\nSmall (S)\n96\n{2,2,18,2}\nBase (B)\n128\n{2,2,18,2}\n(b) Variants by modification.\nVariant\nModification\nBatchNorm/ReLU (BR)\nReplace LayerNorm with BatchNorm and GELU with ReLU.\nDepth reduction to x (DRx)\nReduce the number of Transformer blocks at stage 3 to x.\nTable 4. Turning on/off shifting windows in Swin-B at each stage:\n1 means \u2018on\u2019. For example, \u20181, 1, 1, 1\u2019 implies that all stages\nuse the shifted windows, meaning exactly Swin-B. The symbol \u2018-\u2019\nmeans that training could not finish successfully (i.e., diverged).\nCase\nOn/off on cyclic shift\nTop-1 accuracy (%)\n1\n1, 1, 1, 1\n83.4\n2\n0, 1, 1, 1\n82.3\n3\n0, 0, 1, 1\n82.3\n4\n0, 0, 0, 1\n-\n5\n0, 0, 1, 0\n82.2\n6\n0, 1, 0, 0\n-\n7\n1, 0, 0, 0\n-\n8\n0, 0, 0, 0\n-\namong neighboring windows. Consistently, Swin-Free-T\nand Swin-Free-S in Cases 5 and 6 also achieve better ac-\ncuracy than Swin\u2019s corresponding variants (not shown here;\nRefer to [1]).\nWe also observed that for an input size of 224\u00d7224,\nSwinV2-B [9] gets the same accuracy as Swin-Free-B,\nbut its latency is significantly slower. Thus, for latency-\ncritical applications, Swin-Free would be a better choice\nthan SwinV2.\n5.4. BatchNorm/ReLU (BR) variants\nReplacing LayerNorm and GELU in Swin-Free with\nBatchNorm and ReLU, respectively, we get the variants in\nCases 7 to 9 in Table 6. We first notice that the accuracy\ndegradation that occurs with these replacements is trivial.\nNamely, only Swin-Free-B-BR has slightly lower accuracy\nthan Swin-Free-B, while others hold the same accuracy as\ntheir corresponding models. In regards to latency, BR vari-\nants achieve meaningful speed gain in TensorRT, although\nnot in Pytorch. Nonetheless, considering that TensorRT is\na de facto standard for deploying a deep learning model,\nBR variants would be good alternatives in case of latency-\ncritical applications. It is also worth noting from Case 2\nthat simply applying BR modification to the original Swin-\nB does not yield similar accuracy or latency as compared to\nSwin-Free-B-BR.\n5.5. Depth reduction (DRx) variants\nCases 10 to 13 in Table 6 show the DRx variants of Swin-\nFree-B. Not to mention, D10, D12, D14, and D16 variants\nof Swin-Free-B reduce FLOPs and the number of parame-\nters, thereby improving the latency from Swin-Free-B. See\nthat in Case 11, Swin-Free-B-DR12 has even lower FLOPs\nthan Swin-B and its TensorRT runtime is reduced from 2.0\nms to 1.5 ms when compared to Swin-Free-B. In regards to\naccuracy, we can see that it stays the same as Swin-Free-B.\nThis implies that with our size-varying window, we may not\nneed such deep depth of Swin at stage 3.\nFrom Cases 14 to 16, we can also see that the combi-\nnation of BR and DRx can still result in superior accuracy\ncompared to Swin-B, while improving latency further. For\nexample, Swin-Free-B-BR-DR14 has an accuracy of 83.7%\nand latency of 1.4 ms, compared to 83.4% and 2.1 ms from\nSwin-B. Note in Cases 1 and 14 that by sacrificing a little\nbit of accuracy (from 83.4% to 83.3%), Swin-Free-B-BR-\nDR12 can achieve significant reduction in latency (from 2.1\nms to 1.3 ms, which is about 38% reduction from Swin-B).\nThese kinds of Swin-Free variants could be attractive alter-\nnatives for Swin in situations where latency is more impor-\ntant than accuracy.\n6. Conclusion\nThis paper presents Swin-Free, which attempts to im-\nprove latency over Swin Transformer by reducing memory\ntraffic incurred by shifted window scheme. Instead, Swin-\nFree varies the size of windows over stages, which mimics\nthe mechanism of the shifted windows. This simple tech-\nnique is shown to offer reduced latency and better accuracy\ncompared to its Swin counterpart. We also show that fur-\n6\nTable 5. Latency and accuracy according to the variation in window size at each stage of Swin-B without using cyclic shift. For example,\n\u20187, 7, 14, 7\u2019 means that stage 3 uses 14 as the window size, while stages 1, 2, and 4 use 7. The symbol \u2018-\u2019 means that training could not\nfinish successfully (i.e., diverged).\nCase\nWindow size at a stage\nTop-1 accuracy (%)\nLatency in PyTorch (FP32) (ms)\n1\n7, 7, 7, 7\n-\n13.7\n2\n7, 7, 14, 7\n83.8\n12.7\n3\n7, 14, 7, 7\n81.1\n13.7\n4\n14, 7, 7, 7\n81.2\n13.7\n5\n7, 14, 14, 7\n83.8\n12.6\n6\n14, 7, 14, 7\n83.8\n12.6\n7\n14, 14, 7, 7\n81.2\n13.8\n8\n14, 14, 14, 7\n83.7\n12.6\nTable 6. Models trained with ImageNet-1K from scratch. FLOP and parameter counts are measured by [25]. SwinV2 did not work with\nthis tool so we mark it with \u2018-\u2019 here.\nCase\nModel\nFLOPs\n# of parameters\nTop-1 accuracy (%)\nLatency (ms)\nTensorRT (FP16)\nPyTorch (FP32)\n1\nSwin-B\n15.9G\n88.7M\n83.4\n2.1\n14.3\n2\nSwin-B-BR\n15.6G\n88.7M\n83.2\n1.8\n15.3\n3\nSwinV2-B\n-\n-\n83.8\n3.5\n21.5\n4\nSwin-Free-B\n16.8G\n99.4M\n83.8\n2.0\n12.6\n5\nSwin-Free-T\n5.0G\n31.6M\n82.1\n0.9\n6.7\n6\nSwin-Free-S\n9.7G\n58.3M\n83.6\n1.7\n12.6\n7\nSwin-Free-T-BR\n4.8G\n31.6M\n82.1\n0.8\n7.0\n8\nSwin-Free-S-BR\n9.5G\n58.3M\n83.6\n1.4\n13.2\n9\nSwin-Free-B-BR\n16.4G\n99.4M\n83.7\n1.7\n13.2\n10\nSwin-Free-B-DR10\n11.3G\n69.3M\n83.5\n1.4\n9.3\n11\nSwin-Free-B-DR12\n12.7G\n76.8M\n83.8\n1.5\n9.7\n12\nSwin-Free-B-DR14\n14.0G\n84.4M\n83.8\n1.7\n10.7\n13\nSwin-Free-B-DR16\n15.4G\n91.9M\n83.8\n1.9\n11.6\n14\nSwin-Free-B-BR-DR12\n12.4G\n76.9M\n83.3\n1.3\n10.1\n15\nSwin-Free-B-BR-DR14\n13.7G\n84.4M\n83.7\n1.4\n11.2\n16\nSwin-Free-B-BR-DR16\n15.1G\n91.9M\n83.8\n1.6\n12.2\nther speedup can be achieved by using simpler operations\nand shallower blocks without accuracy loss. Therefore, the\nproposed model is particularly suitable for deployment in\nproduction with improved efficiency.\nIn future work, we plan on applying Swin-Free to other\nvision tasks such as object detection and semantic segmen-\ntation with larger input resolution.\nMore optimizations,\nsuch as dynamic window size across different stages, will\nalso be investigated to further improve GPU utilization for\ninference.\nReferences\n[1] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 10012\u201310022,\nOctober 2021. 1, 2, 3, 5, 6\n[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\nberger, editors, Advances in Neural Information Processing\nSystems, volume 25. Curran Associates, Inc., 2012. 1, 2\n[3] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. CoRR,\nabs/1409.1556, 2014. 1, 2\n[4] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions. CoRR, abs/1409.4842, 2014. 1, 2\n7\n[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. CoRR,\nabs/1512.03385, 2015. 1, 2\n[6] Song Han, Huizi Mao, and William J. Dally. Deep com-\npression: Compressing deep neural network with pruning,\ntrained quantization and huffman coding. In Yoshua Ben-\ngio and Yann LeCun, editors, 4th International Conference\non Learning Representations, ICLR 2016, San Juan, Puerto\nRico, May 2-4, 2016, Conference Track Proceedings, 2016.\n1, 2\n[7] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. CoRR,\nabs/1704.04861, 2017. 1, 2\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 1, 2, 3\n[9] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu\nWei, and Baining Guo. Swin transformer V2: scaling up\ncapacity and resolution. CoRR, abs/2111.09883, 2021. 1, 2,\n6\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. CoRR, abs/1810.04805,\n2018. 1, 3\n[11] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 1, 3\n[12] Tom B. Brown et al. Language models are few-shot learners.\nCoRR, abs/2005.14165, 2020. 1, 3\n[13] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M. Alvarez, and Ping Luo.\nSegFormer: Simple and\nefficient design for semantic segmentation with transformers.\nIn Advances in Neural Information Processing Systems 34\npre-proceedings (NeurIPS), 2021. 1\n[14] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term\nmemory. Neural Computation, 9(8):1735\u20131780, 1997. 1\n[15] Aakanksha Chowdhery et al. Palm: Scaling language mod-\neling with pathways, 2022. 1\n[16] NVIDIA TensorRT. https://developer.nvidia.com/tensorrt. 1\n[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998\u20136008, 2017. 3\n[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer.\nCoRR, abs/1910.10683,\n2019. 3\n[19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell,\nQuoc V. Le, and Ruslan Salakhutdinov.\nTransformer-xl:\nAttentive language models beyond a fixed-length context.\nCoRR, abs/1901.02860, 2019. 3\n[20] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers.\nCoRR,\nabs/2005.12872, 2020. 3\n[21] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers. CoRR, abs/2012.15840, 2020. 3\n[22] Junjie Bai, Fang Lu, Ke Zhang, et al. ONNX: Open neu-\nral network exchange. https://github.com/onnx/\nonnx, 2019. 3\n[23] John Yang, Le An, Anurag Dixit, Jinkyu Koo, and Su Inn\nPark. Depth estimation with simplified transformer, 2022. 3,\n4\n[24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255, 2009. 5\n[25] ThanatosShinji.\nonnx-tool.\nhttps://github.com/\nThanatosShinji/onnx-tool, 2023. 7\n8\n"
  },
  {
    "title": "Restart Sampling for Improving Generative Processes",
    "link": "https://arxiv.org/pdf/2306.14878.pdf",
    "upvote": "4",
    "text": "Restart Sampling for Improving Generative Processes\nYilun Xu\u2217\nMIT\nylxu@mit.edu\nMingyang Deng\u2217\nMIT\ndengm@mit.edu\nXiang Cheng\u2217\nMIT\nchengx@mit.edu\nYonglong Tian\nGoogle Research\nyonglong@google.com\nZiming Liu\nMIT\nzmliu@mit.edu\nTommi Jaakkola\nMIT\ntommi@csail.mit.edu\nAbstract\nGenerative processes that involve solving differential equations, such as diffusion\nmodels, frequently necessitate balancing speed and quality. ODE-based samplers\nare fast but plateau in performance while SDE-based samplers deliver higher sample\nquality at the cost of increased sampling time. We attribute this difference to sam-\npling errors: ODE-samplers involve smaller discretization errors while stochasticity\nin SDE contracts accumulated errors. Based on these findings, we propose a novel\nsampling algorithm called Restart in order to better balance discretization errors and\ncontraction. The sampling method alternates between adding substantial noise in ad-\nditional forward steps and strictly following a backward ODE. Empirically, Restart\nsampler surpasses previous SDE and ODE samplers in both speed and accuracy.\nRestart not only outperforms the previous best SDE results, but also accelerates the\nsampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet 64\u00d764. In addition,\nit attains significantly better sample quality than ODE samplers within comparable\nsampling times. Moreover, Restart better balances text-image alignment/visual\nquality versus diversity than previous samplers in the large-scale text-to-image\nStable Diffusion model pre-trained on LAION 512\u00d7512. Code is available at\nhttps://github.com/Newbeeer/diffusion_restart_sampling\n1\nIntroduction\nDeep generative models based on differential equations, such as diffusion models and Poission\nflow generative models, have emerged as powerful tools for modeling high-dimensional data, from\nimage synthesis [23, 9, 13, 27, 28] to biological data [10, 26]. These models use iterative backward\nprocesses that gradually transform a simple distribution (e.g., Gaussian in diffusion models) into a\ncomplex data distribution by solving a differential equations. The associated vector fields (or drifts)\ndriving the evolution of the differential equations are predicted by neural networks. The resulting\nsample quality can be often improved by enhanced simulation techniques but at the cost of longer\nsampling times.\nPrior samplers for simulating these backward processes can be categorized into two groups: ODE-\nsamplers whose evolution beyond the initial randomization is deterministic, and SDE-samplers\nwhere the generation trajectories are stochastic. Several works [23, 12, 13] show that these samplers\ndemonstrate their advantages in different regimes, as depicted in Fig. 1(b). ODE solvers [22, 16, 13]\nresult in smaller discretization errors, allowing for decent sample quality even with larger step sizes\n(i.e., fewer number of function evaluations (NFE)). However, their generation quality plateaus rapidly.\nIn contrast, SDE achieves better quality in the large NFE regime, albeit at the expense of increased\nsampling time. To better understand these differences, we theoretically analyze SDE performance: the\n\u2217Equal Contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.14878v2  [cs.LG]  1 Nov 2023\nstochasticity in SDE contracts accumulated error, which consists of both the discretization error along\nthe trajectories as well as the approximation error of the learned neural network relative to the ground\ntruth drift (e.g., score function in diffusion model [23]). The approximation error dominates when\nNFE is large (small discretization steps), explaining the SDE advantage in this regime. Intuitively,\nthe stochastic nature of SDE helps \"forget\" accumulated errors from previous time steps.\nInspired by these findings, we propose a novel sampling algorithm called Restart, which combines\nthe advantages of ODE and SDE. As illustrated in Fig. 1(a), the Restart sampling algorithm involves\nK repetitions of two subroutines in a pre-defined time interval: a Restart forward process that adds\na substantial amount of noise, akin to \"restarting\" the original backward process, and a Restart\nbackward process that runs the backward ODE. The Restart algorithm separates the stochasticity\nfrom the drifts, and the amount of added noise in the Restart forward process is significantly larger\nthan the small single-step noise interleaving with drifts in previous SDEs such as [23, 13], thus\namplifying the contraction effect on accumulated errors. By repeating the forward-backward cycle\nK times, the contraction effect introduced in each Restart iteration is further strengthened. The\ndeterministic backward processes allow Restart to reduce discretization errors, thereby enabling step\nsizes comparable to ODE. To maximize the contraction effects in practice, we typically position the\nRestart interval towards the end of the simulation, where the accumulated error is larger. Additionally,\nwe apply multiple Restart intervals to further reduce the initial errors in more challenging tasks.\nExperimentally, Restart consistently surpasses previous ODE and SDE solvers in both quality and\nspeed over a range of NFEs, datasets, and pre-trained models. Specifically, Restart accelerates\nthe previous best-performing SDEs by 10\u00d7 fewer steps for the same FID score on CIFAR-10\nusing VP [23] (2\u00d7 fewer steps on ImageNet 64 \u00d7 64 with EDM [13]), and outperforms fast ODE\nsolvers (e.g., DPM-solver [16]) even in the small NFE regime. When integrated into previous state-\nof-the-art pre-trained models, Restart further improves performance, achieving FID scores of 1.88 on\nunconditional CIFAR-10 with PFGM++ [28], and 1.36 on class-conditional ImageNet 64 \u00d7 64 with\nEDM. To the best of our knowledge, these are the best FID scores obtained on commonly used UNet\narchitectures for diffusion models without additional training. We also apply Restart to the practical\napplication of text-to-image Stable Diffusion model [19] pre-trained on LAION 512 \u00d7 512. Restart\nmore effectively balances text-image alignment/visual quality (measured by CLIP/Aesthetic scores)\nand diversity (measured by FID score) with a varying classifier-free guidance strength, compared to\nprevious samplers.\nOur contributions can be summarized as follows: (1) We investigate ODE and SDE solvers and\ntheoretically demonstrate the contraction effect of stochasticity via an upper bound on the Wasserstein\ndistance between generated and data distributions (Sec 3); (2) We introduce the Restart sampling,\nwhich better harnesses the contraction effect of stochasticity while allowing for fast sampling. The\nsampler results in a smaller Wasserstein upper bound (Sec 4); (3) Our experiments are consistent with\nthe theoretical bounds and highlight Restart\u2019s superior performance compared to previous samplers\non standard benchmarks in terms of both quality and speed. Additionally, Restart improves the\ntrade-off between key metrics on the Stable Diffusion model (Sec 5).\n2\nBackground on Generative Models with Differential Equations\nMany recent successful generative models have their origin in physical processes, including diffusion\nmodels [9, 23, 13] and Poisson flow generative models [27, 28]. These models involve a forward\nprocess that transforms the data distribution into a chosen smooth distribution, and a backward\nprocess that iteratively reverses the forward process. For instance, in diffusion models, the forward\nprocess is the diffusion process with no learned parameters:\ndx =\np\n2 \u02d9\u03c3(t)\u03c3(t)dWt,\nwhere \u03c3(t) is a predefined noise schedule increasing with t, and Wt \u2208 Rd is the standard Wiener\nprocess. For simplicity, we omit an additional scaling function for other variants of diffusion models\nas in EDM [13]. Under this notation, the marginal distribution at time t is the convolution of data\ndistribution p0 = pdata and a Gaussian kernel, i.e., pt = p0 \u2217 N(0, \u03c32(t)Id\u00d7d). The prior distribution\nis set to N(0, \u03c32(T)Id\u00d7d) since pT is approximately Gaussian with a sufficiently large T. Sampling\nof diffusion models is done via a reverse-time SDE [1] or a marginally-equivalent ODE [23]:\n(SDE)\ndx = \u22122 \u02d9\u03c3(t)\u03c3(t)\u2207x log pt(x)dt +\np\n2 \u02d9\u03c3(t)\u03c3(t)dWt\n(1)\n(ODE)\ndx = \u2212 \u02d9\u03c3(t)\u03c3(t)\u2207x log pt(x)dt\n(2)\n2\n(a)\n(b)\nFigure 1: (a) Illustration of the implementation of drift and noise terms in ODE, SDE, and Restart. (b)\nSample quality versus number of function evaluations (NFE) for different approaches. ODE (Green)\nprovides fast speeds but attains only mediocre quality, even with a large NFE. SDE (Yellow) obtains\ngood sample quality but necessitates substantial sampling time. In contrast to ODE and SDE, which\nhave their own winning regions, Restart (Red) achieves the best quality across all NFEs.\nwhere \u2207x log pt(x) in the drift term is the score of intermediate distribution at time t. W.l.o.g we set\n\u03c3(t) = t in the remaining text, as in [13]. Both processes progressively recover p0 from the prior\ndistribution pT while sharing the same time-dependent distribution pt. In practice, we train a neural\nnetwork s\u03b8(x, t) to estimate the score field \u2207x log pt(x) by minimizing the denoising score-matching\nloss [25]. We then substitute the score \u2207x log pt(x) with s\u03b8(x, t) in the drift term of above backward\nSDE (Eq. (1))/ODE (Eq. (2)) for sampling.\nRecent work inspired by electrostatics has not only challenged but also integrated diffusion models,\nnotably PFGM/PFGM++, enhances performance in both image and antibody generation [27, 28, 10].\nThey interpret data as electric charges in an augmented space, and the generative processes involve\nthe simulations of differential equations defined by electric field lines. Similar to diffusion models,\nPFGMs train a neural network to approximate the electric field in the augmented space.\n3\nExplaining SDE and ODE performance regimes\nTo sample from the aforementioned generative models, a prevalent approach employs general-purpose\nnumerical solvers to simulate the corresponding differential equations. This includes Euler and Heun\u2019s\n2nd method [2] for ODEs (e.g., Eq. (2)), and Euler-Maruyama for SDEs (e.g., Eq. (1)). Sampling\nalgorithms typically balance two critical metrics: (1) the quality and diversity of generated samples,\noften assessed via the Fr\u00e9chet Inception Distance (FID) between generated distribution and data\ndistribution [7] (lower is better), and (2) the sampling time, measured by the number of function\nevaluations (NFE). Generally, as the NFE decreases, the FID score tends to deteriorate across all\nsamplers. This is attributed to the increased discretization error caused by using a larger step size in\nnumerical solvers.\nHowever, as illustrated in Fig. 1(b) and observed in previous works on diffusion models [23, 22, 13],\nthe typical pattern of the quality vs time curves behaves differently between the two groups of\nsamplers, ODE and SDE. When employing standard numerical solvers, ODE samplers attain a decent\nquality with limited NFEs, whereas SDE samplers struggle in the same small NFE regime. However,\nthe performance of ODE samplers quickly reaches a plateau and fails to improve with an increase in\nNFE, whereas SDE samplers can achieve noticeably better sample quality in the high NFE regime.\nThis dilemma raises an intriguing question: Why do ODE samplers outperform SDE samplers in the\nsmall NFE regime, yet fall short in the large NFE regime?\nThe first part of the question is relatively straightforward to address: given the same order of numerical\nsolvers, simulation of ODE has significantly smaller discretization error compared to the SDE. For\nexample, the first-order Euler method for ODE results in a local error of O(\u03b42), whereas the first-order\nEuler-Maruyama method for SDEs yeilds a local error of O(\u03b4\n3\n2 ) (see e.g., Theorem 1 of [4]), where\n\u03b4 denotes the step size. As O(\u03b4\n3\n2 ) \u226b O(\u03b42), ODE simulations exhibit lower sampling errors than\nSDEs, likely causing the better sample quality with larger step sizes in the small NFE regime.\n3\nIn the large NFE regime the step size \u03b4 shrinks and discretization errors become less significant\nfor both ODEs and SDEs. In this regime it is the approximation error \u2014 error arising from an\ninaccurate estimation of the ground-truth vector field by the neural network s\u03b8 \u2014 starts to dominate\nthe sampling error. We denote the discretized ODE and SDE using the learned field s\u03b8 as ODE\u03b8 and\nSDE\u03b8, respectively. In the following theorem, we evaluate the total errors from simulating ODE\u03b8\nand SDE\u03b8 within the time interval [tmin, tmax] \u2282 [0, T]. This is done via an upper bound on the\nWasserstein-1 distance between the generated and data distributions at time tmin. We characterize the\naccumulated initial sampling errors up until tmax by total variation distances. Below we show that the\ninherent stochasticity of SDEs aids in contracting these initial errors at the cost of larger additional\nsampling error in [tmin, tmax]. Consequently, SDE results in a smaller upper bound as the step size \u03b4\nnears 0 (pertaining to the high NFE regime).\nTheorem 1 (Informal). Let tmax be the initial noise level and pt denote the true distribution at noise\nlevel t. Let pODE\u03b8\nt\n, pSDE\u03b8\nt\ndenote the distributions of simulating ODE\u03b8, SDE\u03b8 respectively. Assume\nthat \u2200t \u2208 [tmin, tmax], \u2225xt\u2225 < B/2 for any xt in the support of pt, pODE\u03b8\nt\nor pSDE\u03b8\nt\n. Then\nW1(pODE\u03b8\ntmin , ptmin) \u2264 B \u00b7 TV\n\u0010\npODE\u03b8\ntmax , ptmax\n\u0011\n+ O(\u03b4 + \u03f5approx) \u00b7 (tmax \u2212 tmin)\nW1(pSDE\u03b8\ntmin , ptmin)\n|\n{z\n}\ntotal error\n\u2264\n\u0010\n1 \u2212 \u03bbe\u2212U\u0011\nB \u00b7 TV (pSDE\u03b8\ntmax , ptmax)\n|\n{z\n}\nupper bound on contracted error\n+ O(\np\n\u03b4tmax + \u03f5approx) (tmax \u2212 tmin)\n|\n{z\n}\nupper bound on additional sampling error\nIn the above, U = BL1/tmin + L2\n1t2\nmax/t2\nmin, \u03bb < 1 is a contraction factor, L1 and \u03f5approx are\nuniform bounds on \u2225ts\u03b8(xt, t)\u2225 and the approximation error \u2225t\u2207x log pt(x) \u2212 ts\u03b8(x, t)\u2225 for all xt, t,\nrespectively. O() hides polynomial dependency on various Lipschitz constants and dimension.\nWe defer the formal version and proof of Theorem 1 to Appendix A.1. As shown in the theorem,\nthe upper bound on the total error can be decomposed into upper bounds on the contracted error\nand additional sampling error. TV (pODE\u03b8\ntmax , ptmax) and TV (pSDE\u03b8\ntmax , ptmax) correspond to the initial errors\naccumulated from both approximation and discretization errors during the simulation of the backward\nprocess, up until time tmax. In the context of SDE, this accumulated error undergoes contraction by a\nfactor of 1 \u2212 \u03bbe\u2212BL1/tmin\u2212L2\n1t2\nmax/t2\nmin within [tmin, tmax], due to the effect of adding noise. Essentially,\nthe minor additive Gaussian noise in each step can drive the generated distribution and the true\ndistribution towards each other, thereby neutralizing a portion of the initial accumulated error.\nThe other term related to additional sampling error includes the accumulation of discretization and\napproximation errors in [tmin, tmax]. Despite the fact that SDE incurs a higher discretization error than\nODE (O(\n\u221a\n\u03b4) versus O(\u03b4)), the contraction effect on the initial error is the dominant factor impacting\nthe upper bound in the large NFE regime where \u03b4 is small. Consequently, the upper bound for SDE is\nsignificantly lower. This provides insight into why SDE outperforms ODE in the large NFE regime,\nwhere the influence of discretization errors diminishes and the contraction effect dominates. In\nlight of the distinct advantages of SDE and ODE, it is natural to ask whether we can combine their\nstrengths. Specifically, can we devise a sampling algorithm that maintains a comparable level of\ndiscretization error as ODE, while also benefiting from, or even amplifying, the contraction effects\ninduced by the stochasticity of SDE? In the next section, we introduce a novel algorithm, termed\nRestart, designed to achieve these two goals simultaneously.\n4\nHarnessing stochasticity with Restart\nIn this section, we present the Restart sampling algorithm, which incorporates stochasticity during\nsampling while enabling fast generation. We introduce the algorithm in Sec 4.1, followed by a\ntheoretical analysis in Sec 4.2. Our analysis shows that Restart achieves a better Wasserstein upper\nbound compared to those of SDE and ODE in Theorem 1 due to greater contraction effects.\n4.1\nMethod\nIn the Restart algorithm, simulation performs a few repeated back-and-forth steps within a pre-defined\ntime interval [tmin, tmax] \u2282 [0, T], as depicted in Figure 1(a). This interval is embedded into the\nsimulation of the original backward ODE referred to as the main backward process, which runs from\nT to 0. In addition, we refer to the backward process within the Restart interval [tmin, tmax] as the\nRestart backward process, to distinguish it from the main backward process.\n4\nStarting with samples at time tmin, which are generated by following the main backward process,\nthe Restart algorithm adds a large noise to transit the samples from tmin to tmax with the help of\nthe forward process. The forward process does not require any evaluation of the neural network\ns\u03b8(x, t), as it is generally defined by an analytical perturbation kernel capable of transporting\ndistributions from tmin to tmax. For instance, in the case of diffusion models, the perturbation kernel is\nN(0, (\u03c3(tmax)2 \u2212\u03c3(tmin)2)Id\u00d7d). The added noise in this step induces a more significant contraction\ncompared to the small, interleaved noise in SDE. The step acts as if partially restarting the main\nbackward process by increasing the time. Following this step, Restart simulates the backward ODE\nfrom tmax back to tmin using the neural network predictions as in regular ODE. We repeat these\nforward-backward steps within [tmin, tmax] interval K times in order to further derive the benefit from\ncontraction. Specifically, the forward and backward processes in the ith iteration (i \u2208 {0, . . . , K \u22121})\nproceed as follows:\n(Restart forward process)\nxi+1\ntmax = xi\ntmin + \u03b5tmin\u2192tmax\n(3)\n(Restart backward process)\nxi+1\ntmin = ODE\u03b8(xi+1\ntmax , tmax \u2192 tmin)\n(4)\nwhere the initial x0\ntmin is obtained by simulating the ODE until tmin: x0\ntmin = ODE\u03b8(xT , T \u2192 tmin),\nand the noise \u03b5tmin\u2192tmax is sampled from the corresponding perturbation kernel from tmin to tmax.\nThe Restart algorithm not only adds substantial noise in the Restart forward process (Eq. (3)), but\nalso separates the stochasticity from the ODE, leading to a greater contraction effect, which we will\ndemonstrate theoretically in the next subsection. For example, we set [tmin, tmax] = [0.05, 0.3] for\nthe VP model [13] on CIFAR-10. Repetitive use of the forward noise effectively mitigates errors\naccumulated from the preceding simulation up until tmax. Furthermore, the Restart algorithm does\nnot suffer from large discretization errors as it is mainly built from following the ODE in the Restart\nbackward process (Eq. (4)). The effect is that the Restart algorithm is able to reduce the total sampling\nerrors even in the small NFE regime. Detailed pseudocode for the Restart sampling process can be\nfound in Algorithm 2, Appendix B.2.\n4.2\nAnalysis\nWe provide a theoretical analysis of the Restart algorithm under the same setting as Theorem 1.\nIn particular, we prove the following theorem, which shows that Restart achieves a much smaller\ncontracted error in the Wasserstein upper bound than SDE (Theorem 1), thanks to the separation of\nthe noise from the drift, as well as the large added noise in the Restart forward process (Eq. (3)). The\nrepetition of the Restart cycle K times further leads to a enhanced reduction in the initial accumulated\nerror. We denote the intermediate distribution in the ith Restart iteration, following the discretized\ntrajectories and the learned field s\u03b8, as pRestart\u03b8(i)\nt\u2208[tmin,tmax].\nTheorem 2 (Informal). Under the same setting of Theorem 1, assume K \u2264\nC\nL2(tmax\u2212tmin) for some\nuniversal constant C. Then\nW1(pRestart\u03b8(K)\ntmin\n, ptmin)\n|\n{z\n}\ntotal error\n\u2264 B \u00b7 (1 \u2212 \u03bb)K TV (pRestart\u03b8(0)\ntmax\n, ptmax)\n|\n{z\n}\nupper bound on contracted error\n+ (K + 1) \u00b7 O (\u03b4 + \u03f5approx) (tmax \u2212 tmin)\n|\n{z\n}\nupper bound on additional sampling error\nwhere \u03bb < 1 is the same contraction factor as Theorem 1. O() hides polynomial dependency on\nvarious Lipschitz constants, dimension.\nProof sketch. To bound the total error, we introduce an auxiliary process qRestart\u03b8(i)\nt\u2208[tmin,tmax], which initiates\nfrom true distribution ptmax and performs the Restart iterations. This process differs from pRestart\u03b8(i)\nt\u2208[tmin,tmax]\nonly in its initial distribution at tmax (ptmax versus pRestart\u03b8(0)\ntmax\n). We bound the total error by the following\ntriangular inequality:\nW1(pRestart\u03b8(K)\ntmin\n, ptmin)\n|\n{z\n}\ntotal error\n\u2264 W1(pRestart\u03b8(K)\ntmin\n, qRestart\u03b8(K)\ntmin\n)\n|\n{z\n}\ncontracted error\n+ W1(qRestart\u03b8(K)\ntmin\n, ptmin)\n|\n{z\n}\nadditional sampling error\nTo bound the contracted error, we construct a careful coupling process between two individual\ntrajectories sampled from pRestart\u03b8(i)\ntmin\nand qRestart\u03b8(i)\ntmin\n, i = 0, . . . , K \u2212 1. Before these two trajectories\nconverge, the Gaussian noise added in each Restart iteration is chosen to maximize the probability of\n5\nthe two trajectories mapping to an identical point, thereby maximizing the mixing rate in TV. After\nconverging, the two processes evolve under the same Gaussian noise, and will stay converged as their\ndrifts are the same. Lastly, we convert the TV bound to W1 bound by multiplying B. The bound on\nthe additional sampling error echoes the ODE analysis in Theorem 1: since the noise-injection and\nODE-simulation stages are separate, we do not incur the higher discretization error of SDE.\nWe defer the formal version and proof of Theorem 2 to Appendix A.1. The first term in RHS bounds\nthe contraction on the initial error at time tmax and the second term reflects the additional sampling\nerror of ODE accumulated across repeated Restart iterations. Comparing the Wasserstein upper\nbound of SDE and ODE in Theorem 1, we make the following three observations: (1) Each Restart\niteration has a smaller contraction factor 1 \u2212 \u03bb compared to the one in SDE, since Restart separates\nthe large additive noise (Eq. (3)) from the ODE (Eq. (4)). (2) Restart backward process (Eq. (4))\nhas the same order of discretization error O(\u03b4) as the ODE, compared to O(\n\u221a\n\u03b4) in SDE. Hence, the\nRestart allows for small NFE due to ODE-level discretization error. (3) The contracted error further\ndiminishes exponentially with the number of repetitions K though the additional error increases\nlinearly with K. It suggests that there is a sweet spot of K that strikes a balance between reducing\nthe initial error and increasing additional sampling error. Ideally, one should pick a larger K when\nthe initial error at time tmax greatly outweigh the incurred error in the repetitive backward process\nfrom tmax to tmin. We provide empirical evidences in Sec 5.2.\nWhile Theorem 1 and Theorem 2 compare the upper bounds on errors of different methods, we\nprovide empirical validation in Section 5.1 by directly calculating these errors, showing that the\nRestart algorithm indeed yields a smaller total error due to its superior contraction effects. The\nmain goal of Theorem 1 and Theorem 2 is to study how the already accumulated error changes\nusing different samplers, and to understand their ability to self-correct the error by stochasticity. In\nessence, these theorems differentiate samplers based on their performance post-error accumulation.\nFor example, by tracking the change of accumulated error, Theorem 1 shed light on the distinct\n\"winning regions\" of ODE and SDE: ODE samplers have smaller discretization error and hence\nexcel at the small NFE regime. In contrast, SDE performs better in large NFE regime where the\ndiscretization error is negligible and its capacity to contract accumulated errors comes to the fore.\n4.3\nPractical considerations\nThe Restart algorithm offers several degrees of freedom, including the time interval [tmin, tmax] and\nthe number of restart iterations K. Here we provide a general recipe of parameter selection for\npractitioners, taking into account factors such as the complexity of the generative modeling tasks and\nthe capacity of the network. Additionally, we discuss a stratified, multi-level Restart approach that\nfurther aids in reducing simulation errors along the whole trajectories for more challenging tasks.\nWhere to Restart? Theorem 2 shows that the Restart algorithm effectively reduces the accumulated\nerror at time tmax by a contraction factor in the Wasserstein upper bound. These theoretical findings\ninspire us to position the Restart interval [tmin, tmax] towards the end of the main backward process,\nwhere the accumulated error is more substantial. In addition, our empirical observations suggest that a\nlarger time interval tmax\u2212tmin is more beneficial for weaker/smaller architectures or more challenging\ndatasets. Even though a larger time interval increases the additional sampling error, the benefits of\nthe contraction significantly outweighs the downside, consistent with our theoretical predictions. We\nleave the development of principled approaches for optimal time interval selection for future works.\nMulti-level Restart For challenging tasks that yield significant approximation errors, the backward\ntrajectories may diverge substantially from the ground truth even at early stage. To prevent the ODE\nsimulation from quickly deviating from the true trajectory, we propose implementing multiple Restart\nintervals in the backward process, alongside the interval placed towards the end. Empirically, we\nobserve that a 1-level Restart is sufficient for CIFAR-10, while for more challenging datasets such as\nImageNet [5], a multi-level Restart results in enhanced performance [5].\n5\nExperiments\nIn Sec 5.1, we first empirically verify the theoretical analysis relating to the Wasserstein upper\nbounds. We then evaluate the performance of different sampling algorithms on standard image\ngeneration benchmarks, including CIFAR-10 [14] and ImageNet 64 \u00d7 64 [5] in Sec 5.2. Lastly,\n6\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAdditional Sampling error\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\nContracted error\nODE\nSDE\nRestart\n(a)\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAdditional Sampling error\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTotal Error\nODE\nSDE\nRestart\n(b)\n20\n40\n80\n160\n320\nNFE\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\nTotal Error\nODE\nSDE\nRestart\n(c)\nFigure 2: Additional sampling error versus (a) contracted error, where the Pareto frontier is plotted\nand (b) total error, where the scatter plot is provided. (c) Pareto frontier of NFE versus total error.\nwe employ Restart on text-to-image generation, using Stable Diffusion model [19] pre-trained on\nLAION-5B [21] with resolution 512 \u00d7 512, in Sec 5.3.\n5.1\nAdditional sampling error versus contracted error\nOur proposed Restart sampling algorithm demonstrates a higher contraction effect and smaller\naddition sampling error compared to SDE, according to Theorem 1 and Theorem 2. Although our\ntheoretical analysis compares the upper bounds of the total, contracted and additional sampling errors,\nwe further verify their relative values through a synthetic experiment.\nSetup We construct a 20-dimensional dataset with 2000 points sampled from a Gaussian mixture,\nand train a four-layer MLP to approximate the score field \u2207x log pt. We implement the ODE, SDE,\nand Restart methods within a predefined time range of [tmin, tmax] = [1.0, 1.5], where the process\noutside this range is conducted via the first-order ODE. To compute various error types, we define\nthe distributions generated by three methods as outlined in the proof of Theorem 2 and directly\ngauge the errors at end of simulation t = 0 instead of t = tmin: (1) the generated distribution as\npSampler\n0\n, where Sampler \u2208 {ODE\u03b8, SDE\u03b8, Restart\u03b8(K)}; (2) an auxiliary distribution qSampler\n0\ninitiating\nfrom true distribution ptmax at time tmax. The only difference between pSampler\n0\nand qSampler\n0\nis their initial\ndistribution at tmax (pODE\u03b8\ntmax versus ptmax); and (3) the true data distribution p0. In line with Theorem 2,\nwe use Wasserstein-1 distance W1(pSampler\n0\n, qSampler\n0\n) / W1(qSampler\n0\n, p0) to measure the contracted error\n/ additional sampling error, respectively. Ultimately, the total error corresponds to W1(pSampler\n0\n, p0).\nDetailed information about dataset, metric and model can be found in the Appendix C.5.\nResults In our experiment, we adjust the parameters for all three processes and calculate the total,\ncontracted, and additional sampling errors across all parameter settings. Figure 2(a) depicts the Pareto\nfrontier of additional sampling error versus contracted error. We can see that Restart consistently\nachieves lower contracted error for a given level of additional sampling error, compared to both the\nODE and SDE methods, as predicted by theory. In Figure 2(b), we observe that the Restart method\nobtains a smaller total error within the additional sampling error range of [0.8, 0.85]. During this\nrange, Restart also displays a strictly reduced contracted error, as illustrated in Figure 2(a). This\naligns with our theoretical analysis, suggesting that the Restart method offers a smaller total error due\nto its enhanced contraction effects. From Figure 2(c), Restart also strikes an better balance between\nefficiency and quality, as it achieves a lower total error at a given NFE.\n5.2\nExperiments on standard benchmarks\nTo evaluate the sample quality and inference speed, we report the FID score [7] (lower is better) on 50K\nsamplers and the number of function evaluations (NFE). We borrow the pretrained VP/EDM/PFGM++\nmodels on CIFAR-10 or ImageNet 64 \u00d7 64 from [13, 28]. We also use the EDM discretization\nscheme [13] (see Appendix B.1 for details) during sampling.\nFor the proposed Restart sampler, the hyperparameters include the number of steps in the main/Restart\nbackward processes, the number of Restart iteration K, as well as the time interval [tmin, tmax]. We\npick the tmin and tmax from the list of time steps in EDM discretization scheme with a number of steps\n18. For example, for CIFAR-10 (VP) with NFE=75, we choose tmin=0.06, tmax=0.30, K=10, where\n0.30/0.06 is the 12th/14th time step in the EDM scheme. We also adopt EDM scheme for the Restart\nbackward process in [tmin, tmax]. In addition, we apply the multi-level Restart strategy (Sec 4.3) to\n7\n32\n64\n128\n256\n512\n1024\nNFE\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\nFID score (50K)\nODE\nGonna Go Fast\nImproved SDE\nRestart\n(a) CIFAR-10, VP\n32\n64\n128\n256\n512\n1024\nNFE\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nFID score (50K)\n128\n256\n512\n1024\n1.4\n1.6\n1.8\nODE\nImproved SDE\nRestart\n(b) ImageNet 64\u00d764, EDM\nFigure 3: FID versus NFE on (a) unconditional generation on CIFAR-10 with VP; (b) class-\nconditional generation on ImageNet with EDM.\n15\n20\n25\n30\n35\nNFE\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\nFID score (50K)\nDPM Solver\nRestart\nFigure 4: CIFAR-10, VP, in the low NFE regime. Restart consistently outperforms the DPM-solver\nwith an NFE ranging from 16 to 36.\nmitigate the error at early time steps for the more challenging ImageNet 64 \u00d7 64. We provide the\ndetailed Restart configurations in Appendix C.2.\nFor SDE, we compare with the previously best-performing stochastic samplers proposed by [13]\n(Improved SDE). We use their optimal hyperparameters for each dataset. We also report the FID\nscores of the adaptive SDE [12] (Gonna Go Fast) on CIFAR-10 (VP). Since the vanilla reverse-\ndiffusion SDE [23] has a significantly higher FID score, we omit its results from the main charts and\ndefer them to Appendix D. For ODE samplers, we compare with the Heun\u2019s 2nd order method [2]\n(Heun), which arguably provides an excellent trade-off between discretization errors and NFE [13].\nTo ensure a fair comparison, we use Heun\u2019s method as the sampler in the main/Restart backward\nprocesses in Restart.\nWe report the FID score versus NFE in Figure 3(a) and Table 1 on CIFAR-10, and Figure 3(b)\non ImageNet 64 \u00d7 64 with EDM. Our main findings are: (1) Restart outperforms other SDE or\nODE samplers in balancing quality and speed, across datasets and models. As demonstrated in\nthe figures, Restart achieves a 10-fold / 2-fold acceleration compared to previous best SDE results\non CIFAR-10 (VP) / ImageNet 64 \u00d7 64 (EDM) at the same FID score. In comparison to ODE\nsampler (Heun), Restart obtains a better FID score, with the gap increasing significantly with NFE.\n(2) For stronger models such as EDM and PFGM++, Restart further improve over the ODE baseline\non CIFAR-10. In contrast, the Improved SDE negatively impacts performance of EDM, as also\nobserved in [13]. It suggests that Restart incorporates stochasticity more effectively. (3) Restart\nestablishes new state-of-the-art FID scores for UNet architectures without additional training. In\nparticular, Restart achieves FID scores of 1.36 on class-cond. ImageNet 64 \u00d7 64 with EDM, and 1.88\non uncond. CIFAR-10 with PFGM++.\nTo further validate that Restart can be applied in low NFE regime, we show that one can employ\nfaster ODE solvers such as the DPM-solver-3 [16] to further accelerate Restart. Fig. 4 shows that\nthe Restart consistently outperforms the DPM-solver with an NFE ranging from 16 to 36. This\ndemonstrates Restart\u2019s capability to excel over ODE samplers, even in the small NFE regime. It\nalso suggests that Restart can consistently improve other ODE samplers, not limited to the DDIM,\nHeun. Surprisingly, when paired with the DPM-solver, Restart achieves an FID score of 2.11 on\nVP setting when NFE is 30, which is significantly lower than any previous numbers (even lower\n8\nthan the SDE sampler with an NFE greater than 1000 in [23]), and make VP model on par\nwith the performance with more advanced models (such as EDM). We include detailed Restart\nconfiguration in Table 3 in Appendix C.2.\nTable\n1:\nUncond.\nCIFAR-10 with EDM\nand PFGM++\nNFE FID\nEDM-VP [13]\nODE (Heun)\n63\n1.97\n35\n1.97\nImproved SDE\n63\n2.27\n35\n2.45\nRestart\n43\n1.90\nPFGM++ [28]\nODE (Heun)\n63\n1.91\n35\n1.91\nRestart\n43\n1.88\n0\n20\n40\n60\nNumber of Restart iterations K\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\nFID score (50K)\nEDM (Restart)\nEDM (ODE)\nVP (Restart)\nVP (ODE)\nFigure 5:\nFID score with a\nvarying number of Restart itera-\ntions K.\nTheorem 4 shows that each Restart iteration\nreduces the contracted errors while increasing\nthe additional sampling errors in the backward\nprocess. In Fig. 5, we explore the choice of\nthe number of Restart iterations K on CIFAR-\n10. We find that FID score initially improves\nand later worsens with increasing iterations K,\nwith a smaller turning point for stronger EDM\nmodel. This supports the theoretical analysis\nthat sampling errors will eventually outweigh\nthe contraction benefits as K increases, and\nEDM only permits fewer Restart iterations\ndue to smaller accumulated errors. It also\nsuggests that, as a rule of thumb, we should\napply greater Restart strength (e.g., larger K)\nfor weaker or smaller architectures and vice versa.\n5.3\nExperiments on large-scale text-to-image model\n0.290\n0.295\n0.300\n0.305\n0.310\n0.315\n0.320\nCLIP score (ViT-g/14)\n14\n16\n18\n20\n22\nFID score (5K)\nDDIM (Steps=50)\nDDIM (Steps=100)\nDDPM (Steps=100)\nDDPM (Steps=200)\nRestart (Steps=66)\n(a) FID versus CLIP score\n5.15\n5.20\n5.25\n5.30\n5.35\n5.40\nAesthetic score\n14\n16\n18\n20\n22\nFID score (5K)\nDDIM (Steps=50)\nDDIM (Steps=100)\nDDPM (Steps=100)\nDDPM (Steps=200)\nRestart (Steps=66)\n(b) FID versus Aesthetic score\nFigure 6: FID score versus (a) CLIP ViT-g/14 score and (b) Aesthetic score for text-to-image\ngeneration at 512\u00d7512 resolution, using Stable Diffusion v1.5 with a varying classifier-free guidance\nweight w = 2, 3, 5, 8.\nWe further apply Restart to the text-to-image Stable Diffusion v1.5 2 pre-trained on LAION-5B [21] at\na resolution of 512\u00d7512. We employ the commonly used classifier-free guidance [8, 20] for sampling,\nwherein each sampling step entails two function evaluations \u2013 the conditional and unconditional\npredictions. Following [18, 20], we use the COCO [15] validation set for evaluation. We assess\ntext-image alignment using the CLIP score [6] with the open-sourced ViT-g/14 [11], and measure\ndiversity via the FID score. We also evaluate visual quality through the Aesthetic score, as rated\nby the LAION-Aesthetics Predictor V2 [24]. Following [17], we compute all evaluation metrics\nusing 5K captions randomly sampled from the validation set and plot the trade-off curves between\nCLIP/Aesthetic scores and FID score, with the classifier-free guidance weight w in {2, 3, 5, 8}.\nWe compare with commonly used ODE sampler DDIM [22] and the stochastic sampler DDPM [9].\nFor Restart, we adopt the DDIM solver with 30 steps in the main backward process, and Heun in the\nRestart backward process, as we empirically find that Heun performs better than DDIM in the Restart.\nIn addition, we select different sets of the hyperparameters for each guidance weight. For instance,\nwhen w = 8, we use [tmin, tmax]=[0.1, 2], K=2 and 10 steps in Restart backward process. We defer\nthe detailed Restart configuration to Appendix C.2, and the results of Heun to Appendix D.1.\nAs illustrated in Fig. 6(a) and Fig. 6(b), Restart achieves better FID scores in most cases, given the\nsame CLIP/Aesthetic scores, using only 132 function evaluations (i.e., 66 sampling steps). Remark-\nably, Restart achieves substantially lower FID scores than other samplers when CLIP/Aesthetic scores\n2https://huggingface.co/runwayml/stable-diffusion-v1-5\n9\n(a) Restart (Steps=66)\n(b) DDIM (Steps=100)\n(c) DDPM (Steps=100)\nFigure 7: Visualization of generated images with classifier-free guidance weight w = 8, using four\ntext prompts (\u201cA photo of an astronaut riding a horse on mars.\", \"A raccoon playing table tennis\",\n\"Intricate origami of a fox in a snowy forest\" and \"A transparent sculpture of a duck made out of\nglass\") and the same random seeds.\nare high (i.e., with larger w values). Conversely, Restart generally obtains a better text-image align-\nment/visual quality given the same FID. We also observe that DDPM generally obtains comparable\nperformance with Restart in FID score when CLIP/Aesthetic scores are low, with Restart being more\ntime-efficient. These findings suggest that Restart balances diversity (FID score) against text-image\nalignment (CLIP score) or visual quality (Aesthetic score) more effectively than previous samplers.\nIn Fig. 7, we visualize the images generated by Restart, DDIM and DDPM with w = 8. Compared\nto DDIM, the Restart generates images with superior details (e.g., the rendition of duck legs by\nDDIM is less accurate) and visual quality. Compared to DDPM, Restart yields more photo-realistic\nimages (e.g., the astronaut). We provide extended of text-to-image generated samples in Appendix E.\n6\nConclusion and future direction\nIn this paper, we introduce the Restart sampling for generative processes involving differential\nequations, such as diffusion models and PFGMs. By interweaving a forward process that adds a\nsignificant amount of noise with a corresponding backward ODE, Restart harnesses and even enhances\nthe individual advantages of both ODE and SDE. Theoretically, Restart provides greater contraction\neffects of stochasticity while maintaining ODE-level discretization error. Empirically, Restart achieves\na superior balance between quality and time, and improves the text-image alignment/visual quality\nand diversity trade-off in the text-to-image Stable Diffusion models.\nA current limitation of the Restart algorithm is the absence of a principled way for hyperparameters\nselection, including the number of iterations K and the time interval [tmin, tmax]. At present, we\nadjust these parameters based on the heuristic that weaker/smaller models, or more challenging tasks,\nnecessitate a stronger Restart strength. In the future direction, we anticipate developing a more\nprincipled approach to automating the selection of optimal hyperparameters for Restart based on the\nerror analysis of models, in order to fully unleash the potential of the Restart framework.\nAcknowledgements\nYX and TJ acknowledge support from MIT-DSTA Singapore collaboration, from NSF Expeditions\ngrant (award 1918839) \"Understanding the World Through Code\", and from MIT-IBM Grand\nChallenge project. Xiang Cheng acknowledges support from NSF CCF-2112665 (TILOS AI Research\nInstitute).\n10\nReferences\n[1] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their\nApplications, 12(3):313\u2013326, 1982.\n[2] Uri M. Ascher and Linda R. Petzold. Computer methods for ordinary differential equations and\ndifferential-algebraic equations. In SIAM, 1998.\n[3] Andrei N Borodin and Paavo Salminen. Handbook of Brownian motion-facts and formulae.\nSpringer Science & Business Media, 2015.\n[4] Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte\ncarlo with inaccurate gradient. Stochastic Processes and their Applications, 129(12):5278\u20135311,\n2019.\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248\u2013255, 2009.\n[6] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Joseph Le Bras, and Yejin Choi. Clipscore:\nA reference-free evaluation metric for image captioning. In Conference on Empirical Methods\nin Natural Language Processing, 2021.\n[7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS,\n2017.\n[8] Jonathan Ho. Classifier-free diffusion guidance. ArXiv, abs/2207.12598, 2022.\n[9] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. ArXiv,\nabs/2006.11239, 2020.\n[10] Chutian Huang, Zijing Liu, Shengyuan Bai, Linwei Zhang, Chencheng Xu, Zhe Wang, Yang\nXiang, and Yuanpeng Xiong. Pf-abgen: A reliable and efficient antibody generator via poisson\nflow. Machine Learning for Drug Discovery Workshop, International Conference on Learning\nRepresentations, 2023.\n[11] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan\nTaori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi,\nAli Farhadi, and Ludwig Schmidt. Openclip. Zenodo, 2021.\n[12] Alexia Jolicoeur-Martineau, Ke Li, Remi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas.\nGotta go fast when generating data with score-based models. ArXiv, abs/2105.14080, 2021.\n[13] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of\ndiffusion-based generative models. ArXiv, abs/2206.00364, 2022.\n[14] Alex Krizhevsky. Learning multiple layers of features from tiny images. Citeseer, 2009.\n[15] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In\nEuropean Conference on Computer Vision, 2014.\n[16] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022.\n[17] Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. ArXiv, abs/2210.03142, 2022.\n[18] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. In International Conference on Machine Learning, 2021.\n11\n[19] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 10674\u201310685, 2021.\n[20] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo\nLopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic\ntext-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487,\n2022.\n[21] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick\nSchramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk,\nand Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text\nmodels. ArXiv, abs/2210.08402, 2022.\n[22] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ArXiv,\nabs/2010.02502, 2020.\n[23] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Er-\nmon, and Ben Poole. Score-based generative modeling through stochastic differential equations.\nArXiv, abs/2011.13456, 2020.\n[24] LAION-AI\nTeam.\nLaion-aesthetics\npredictor\nv2.\nhttps://github.com/\nchristophschuhmann/improved-aesthetic-predictor, 2022.\n[25] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural\nComputation, 23:1661\u20131674, 2011.\n[26] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E.\nEisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M.\nWicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham\nVenkatesh, Isaac Sappington, Susana V\u00e1zquez Torres, Anna Lauko, Valentin De Bortoli, Emile\nMathieu, Regina Barzilay, T. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker.\nBroadly applicable and accurate protein design by integrating structure prediction networks and\ndiffusion generative models. bioRxiv, 2022.\n[27] Yilun Xu, Ziming Liu, Max Tegmark, and T. Jaakkola. Poisson flow generative models. ArXiv,\nabs/2209.11178, 2022.\n[28] Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, and T. Jaakkola.\nPfgm++: Unlocking the potential of physics-inspired generative models. ArXiv, abs/2302.04265,\n2023.\n12\nAppendix\nA\nProofs of Main Theoretical Results\nIn this section, we provide proofs of our main results. We define below some crucial notations\nwhich we will use throughout. We use ODE(. . . ) to denote the backwards ODE under exact score\n\u2207 log pt(x). More specifically, given any x \u2208 Rd and s > r > 0, let xt denote the solution to the\nfollowing ODE:\ndxt = \u2212t\u2207 log pt(xt)dt.\n(5)\nODE(x, s \u2192 r) is defined as \"the value of xr when initialized at xs = x\". It will also be useful to\nconsider a \"time-discretized ODE with drift ts\u03b8(x, t)\": let \u03b4 denote the discretization step size and let\nk denote any integer. Let \u03b4 denote a step size, let xt denote the solution to\ndxt = \u2212ts\u03b8(xk\u03b4, k\u03b4)dt,\n(6)\nwhere for any t, k is the unique integer such that t \u2208 ((k \u2212 1)\u03b4, k\u03b4]. We verify that the dynamics of\nEq. (6) is equivalent to the following discrete-time dynamics for t = k\u03b4, k \u2208 Z:\nx(k\u22121)\u03b4 = xk\u03b4 \u2212 1\n2\n\u0010\n((k \u2212 1)\u03b4)2 \u2212 (k\u03b4)2\u0011\ns\u03b8(xk\u03b4, k\u03b4).\nWe similarly denote the value of xr when initialized at xs = x as ODE\u03b8(x, s \u2192 r). Analogously, we\nlet SDE(x, s \u2192 r) and SDE\u03b8(x, s \u2192 r) denote solutions to\ndyt = \u22122t\u2207 log pt(yt)dt +\n\u221a\n2tdBt\ndyt = \u22122ts\u03b8(yt, t)dt +\n\u221a\n2tdBt\nrespectively. Finally, we will define the Restart\u03b8 process as follows:\n(Restart\u03b8 forward process)\nxi+1\ntmax = xi\ntmin + \u03b5i\ntmin\u2192tmax\n(Restart\u03b8 backward process)\nxi+1\ntmin = ODE\u03b8(xi+1\ntmax , tmax \u2192 tmin),\n(7)\nwhere \u03b5i\ntmin\u2192tmax \u223c N\n\u00000,\n\u0000t2\nmax \u2212 t2\nmin\n\u0001\nI\n\u0001\n. We use Restart\u03b8(x, K) to denote xK\ntmin in the above\nprocesses, initialized at x0\ntmin = x. In various theorems, we will refer to a function Q(r) : R+ \u2192\n[0, 1/2), defined as the Gaussian tail probability Q(r) = Pr(a \u2265 r) for a \u223c N(0, 1).\nA.1\nMain Result\nTheorem 3. [Formal version of Theorem 1] Let tmax be the initial noise level. Let the initial random\nvariables xtmax = ytmax, and\nxtmin = ODE\u03b8(xtmax, tmax \u2192 tmin)\nytmin = SDE\u03b8(ytmax, tmax \u2192 tmin),\nLet pt denote the true population distribution at noise level t. Let pODE\u03b8\nt\n, pSDE\u03b8\nt\ndenote the distributions\nfor xt, yt respectively. Assume that for all x, y, s, t, s\u03b8(x, t) satisfies \u2225ts\u03b8(x, t) \u2212 ts\u03b8(x, s)\u2225 \u2264\nL0|s \u2212 t|, \u2225ts\u03b8(x, t)\u2225 \u2264 L1, \u2225ts\u03b8(x, t) \u2212 ts\u03b8(y, t)\u2225 \u2264 L2 \u2225x \u2212 y\u2225, and the approximation error\n\u2225ts\u03b8(x, t) \u2212 t\u2207 log pt(x)\u2225 \u2264 \u03f5approx. Assume in addition that \u2200t \u2208 [tmin, tmax], \u2225xt\u2225 < B/2 for any\nxt in the support of pt, pODE\u03b8\nt\nor pSDE\u03b8\nt\n, and K \u2264\nC\nL2(tmax\u2212tmin) for some universal constant C. Then\nW1(pODE\u03b8\ntmin\n, ptmin) \u2264 B \u00b7 TV\n\u0010\npODE\u03b8\ntmax , ptmax\n\u0011\n+ eL2(tmax\u2212tmin) \u00b7 (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin)\n(8)\nW1(pSDE\u03b8\ntmin , ptmin) \u2264 B \u00b7\n\u0010\n1 \u2212 \u03bbe\u2212BL1/tmin\u2212L2\n1t2\nmax/t2\nmin\n\u0011\nTV (pSDE\u03b8\ntmax , ptmax)\n+ e2L2(tmax\u2212tmin) \u0010\n\u03f5approx + \u03b4L0 + L2\n\u0010\n\u03b4L1 +\np\n2\u03b4dtmax\n\u0011\u0011\n(tmax \u2212 tmin) (9)\nwhere \u03bb := 2Q\n\u0012\nB\n2\u221a\nt2\nmax\u2212t2\nmin\n\u0013\n.\n13\nProof. Let us define xtmax \u223c ptmax, and let xtmin = ODE(xtmax, tmax \u2192 tmin). We verify that xtmin\nhas density ptmin. Let us also define \u02c6xtmin = ODE\u03b8(xtmax, tmax \u2192 tmin). We would like to bound\nthe Wasserstein distance between \u00afxtmin and xtmin (i.e., pODE\u03b8\ntmin\nand ptmin), by the following triangular\ninequality:\nW1(\u00afxtmin, xtmin) \u2264 W1(\u00afxtmin, \u02c6xtmin) + W1(\u02c6xtmin, xtmin)\n(10)\nBy Lemma 2, we know that\n\u2225\u02c6xtmin \u2212 xtmin\u2225 \u2264 e(tmax\u2212tmin)L2 (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin) ,\nwhere we use the fact that \u2225\u02c6xtmax \u2212 xtmax\u2225 = 0. Thus we immediately have\nW1(\u02c6xtmin, xtmin) \u2264 e(tmax\u2212tmin)L2 (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin)\n(11)\nOn the other hand,\nW1(\u02c6xtmin, xtmin) \u2264B \u00b7 TV (\u02c6xtmin, xtmin)\n\u2264B \u00b7 TV (\u02c6xtmax, xtmax)\n(12)\nwhere the last equality is due to the data-processing inequality. Combining Eq. (11) , Eq. (12) and the\ntriangular inequality Eq. (10), we arrive at the upper bound for ODE (Eq. (8)). The upper bound for\nSDE (Eq. (9)) shares a similar proof approach. First, let ytmax \u223c ptmax. Let \u02c6ytmin = SDE\u03b8(ytmax, tmax \u2192\ntmin). By Lemma 5,\nTV\n\u0000\u02c6ytmin, ytmin\n\u0001\n\u2264\n \n1 \u2212 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!\n\u00b7 e\u2212BL1/tmin\u2212L2\n1t2\nmax/t2\nmin\n!\n\u00b7 TV\n\u0000\u02c6ytmax, ytmax\n\u0001\nOn the other hand, by Lemma 4,\nE [\u2225\u02c6ytmin \u2212 ytmin\u2225] \u2264e2L2(tmax\u2212tmin) \u0010\n\u03f5approx + \u03b4L0 + L2\n\u0010\n\u03b4L1 +\np\n2\u03b4dtmax\n\u0011\u0011\n(tmax \u2212 tmin) .\nThe SDE triangular upper bound on W1(\u00afytmin, ytmin) follows by multiplying the first inequality by B (to\nbound W1(\u00afytmin, \u02c6ytmin)) and then adding the second inequality (to bound W1(ytmin, \u02c6ytmin)). Notice\nthat by definition, TV\n\u0000\u02c6ytmax, ytmax\n\u0001\n= TV\n\u0000ytmax, ytmax\n\u0001\n. Finally, because of the assumption that\nK \u2264\nC\nL2(tmax\u2212tmin) for some universal constant, we summarize the second term in the Eq. (8) and\nEq. (9) into the big O in the informal version Theorem 1.\nTheorem 4. [Formal version of Theorem 2] Consider the same setting as Theorem 3. Let pRestart\u03b8,i\ntmin\ndenote the distributions after ith Restart iteration, i.e., the distribution of xi\ntmin = Restart\u03b8(x0\ntmin, i).\nGiven initial x0\ntmax \u223c pRestart,0\ntmax\n, let x0\ntmin = ODE\u03b8(x0\ntmax, tmax \u2192 tmin). Then\nW1(pRestart\u03b8,K\ntmin\n, ptmin) \u2264 B \u00b7 (1 \u2212 \u03bb)K TV (pRestart,0\ntmax\n, ptmax)\n|\n{z\n}\nupper bound on contracted error\n+ e(K+1)L2(tmax\u2212tmin)(K + 1) (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin)\n|\n{z\n}\nupper bound on additional sampling error\n(13)\nwhere \u03bb = 2Q\n\u0012\nB\n2\u221a\nt2\nmax\u2212t2\nmin\n\u0013\n.\nProof. Let x0\ntmax \u223c ptmax. Let xK\ntmin = Restart(x0\ntmin, K). We verify that xK\ntmin has density ptmin. Let us\nalso define \u02c6x0\ntmin = ODE\u03b8(x0\ntmax, tmax \u2192 tmin) and \u02c6xK\ntmin = Restart\u03b8(\u02c6x0\ntmin, K).\nBy Lemma 1,\nTV\n\u0000xK\ntmin, \u02c6xK\ntmin\n\u0001\n\u2264\n \n1 \u2212 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!!K\nTV\n\u0000x0\ntmin, \u02c6x0\ntmin\n\u0001\n\u2264\n \n1 \u2212 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!!K\nTV\n\u0000x0\ntmax, \u02c6x0\ntmax\n\u0001\n=\n \n1 \u2212 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!!K\nTV\n\u0000x0\ntmax, x0\ntmax\n\u0001\n14\nThe second inequality holds by data processing inequality. The above can be used to bound the\n1-Wasserstein distance as follows:\nW1\n\u0000xK\ntmin, \u02c6xK\ntmin\n\u0001\n\u2264 B \u00b7 TV\n\u0000xK\ntmin, \u02c6xK\ntmin\n\u0001\n\u2264\n \n1 \u2212 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!!K\nTV\n\u0000x0\ntmax, x0\ntmax\n\u0001\n(14)\nOn the other hand, using Lemma 3,\nW1\n\u0000xK\ntmin, \u02c6xK\ntmin\n\u0001\n\u2264\n\r\rxK\ntmin \u2212 \u02c6xK\ntmin\n\r\r\n\u2264e(K+1)L2(tmax\u2212tmin)(K + 1) (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin)\n(15)\nWe arrive at the result by combining the two bounds above (Eq. (14), Eq. (15)) with the following\ntriangular inequality,\nW1(\u00afxK\ntmin, xK\ntmin) \u2264 W1(\u00afxK\ntmin, \u02c6xK\ntmin) + W1(\u02c6xK\ntmin, xK\ntmin)\nA.2\nMixing under Restart with exact ODE\nLemma 1. Consider the same setup as Theorem 4. Consider the Restart\u03b8 process defined in\nequation 7. Let\nxi\ntmin = Restart\u03b8(x0\ntmin, i)\nyi\ntmin = Restart\u03b8(y0\ntmin, i).\nLet pRestart\u03b8(i)\nt\nand qRestart\u03b8(i)\nt\ndenote the densities of xi\nt and yi\nt respectively. Then\nTV\n\u0010\npRestart\u03b8(K)\ntmin\n, qRestart\u03b8(K)\ntmin\n\u0011\n\u2264 (1 \u2212 \u03bb)K TV\n\u0010\npRestart\u03b8(0)\ntmin\n, qRestart\u03b8(0)\ntmin\n\u0011\n,\nwhere \u03bb = 2Q\n\u0012\nB\n2\u221a\nt2max\u2212t2\nmin\n\u0013\n.\nProof. Conditioned on xi\ntmin, yi\ntmin, let xi+1\ntmax\n= xi\ntmin +\np\nt2max \u2212 t2\nmin\u03bex\ni and yi+1\ntmax\n= yi\ntmin +\np\nt2max \u2212 t2\nmin\u03bey\ni . We now define a coupling between xi+1\ntmin and yi+1\ntmin by specifying the joint dis-\ntribution over \u03bex\ni and \u03bey\ni .\nIf xi\ntmin = yi\ntmin, let \u03bex\ni = \u03bey\ni , so that xi+1\ntmin = yi+1\ntmin . On the other hand, if xi\ntmin \u0338= yi\ntmin, let xi+1\ntmax and yi+1\ntmax\nbe coupled as described in the proof of Lemma 7, with x\u2032 = xi+1\ntmax , y\u2032 = yi+1\ntmax , \u03c3 =\np\nt2max \u2212 t2\nmin.\nUnder this coupling, we verify that,\nE\n\u0002\n1\n\b\nxi+1\ntmin \u0338= yi+1\ntmin\n\t\u0003\n\u2264E\n\u0002\n1\n\b\nxi+1\ntmax \u0338= yi+1\ntmax\n\t\u0003\n\u2264E\n\" \n1 \u2212 2Q\n \r\rxi\ntmin \u2212 yi\ntmin\n\r\r\n2\np\nt2max \u2212 t2\nmin\n!!\n1\n\b\nxi\ntmin \u0338= yi\ntmin\n\t\n#\n\u2264\n \n1 \u2212 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!!\nE\n\u0002\n1\n\b\nxi\ntmin \u0338= yi\ntmin\n\t\u0003\n.\nApplying the above recursively,\nE\n\u0002\n1\n\b\nxK\ntmin \u0338= yK\ntmin\n\t\u0003\n\u2264\n \n1 \u2212 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!!K\nE\n\u0002\n1\n\b\nx0\ntmin \u0338= y0\ntmin\n\t\u0003\n.\nThe conclusion follows by noticing that TV\n\u0010\npRestart\u03b8(K)\ntmin\n, qRestart\u03b8(K)\ntmin\n\u0011\n\u2264 Pr\n\u0000xK\ntmin \u0338= yK\ntmin\n\u0001\n=\nE\n\u0002\n1\n\b\nxK\ntmin \u0338= yK\ntmin\n\t\u0003\n,\nand by selecting the initial coupling so that Pr\n\u0000x0\ntmin \u0338= y0\ntmin\n\u0001\n=\nTV\n\u0010\npRestart\u03b8(0)\ntmin\n, qRestart\u03b8(0)\ntmin\n\u0011\n.\n15\nA.3\nW1 discretization bound\nLemma 2 (Discretization bound for ODE). Let xtmin = ODE (xtmax, tmax \u2192 tmin) and let xtmin =\nODE\u03b8 (xtmax, tmax \u2192 tmin). Assume that for all x, y, s, t, s\u03b8(x, t) satisfies \u2225ts\u03b8(x, t) \u2212 ts\u03b8(x, s)\u2225 \u2264\nL0|s \u2212 t|, \u2225ts\u03b8(x, t)\u2225 \u2264 L1 and \u2225ts\u03b8(x, t) \u2212 ts\u03b8(y, t)\u2225 \u2264 L2 \u2225x \u2212 y\u2225. Then\n\u2225xtmin \u2212 xtmin\u2225 \u2264 e(tmax\u2212tmin)L2 (\u2225xtmax \u2212 xtmax\u2225 + (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin))\nProof. Consider some fixed arbitrary k, and recall that \u03b4 is the step size. Recall that by definition of\nODE and ODE\u03b8, for t \u2208 ((k \u2212 1)\u03b4, k\u03b4],\ndxt = \u2212t\u2207 log pt(xt)dt\ndxt = \u2212ts\u03b8(xk\u03b4, k\u03b4)dt.\nFor t \u2208 [tmin, tmax], let us define a time-reversed process x\u2190\nt\n:= x\u2212t. Let v(x, t) := \u2207 log p\u2212t(x).\nThen for t \u2208 [\u2212tmax, \u2212tmin]\ndx\u2190\nt\n= tv(x\u2190\nt , t)ds.\nSimilarly, define x\u2190\nt\n:= x\u2212t and v(x, t) := s\u03b8 (x, \u2212t). It follows that\ndx\u2190\nt\n= tv(x\u2190\nk\u03b4, k\u03b4)ds,\nwhere k is the unique (negative) integer satisfying t \u2208 [k\u03b4, (k + 1)\u03b4). Following these definitions,\nd\ndt \u2225x\u2190\nt \u2212 x\u2190\nt \u2225\n\u2264 \u2225tv(x\u2190\nt , t) \u2212 tv(x\u2190\nt , t)\u2225\n+ \u2225tv(x\u2190\nt , t) \u2212 tv(x\u2190\nt , t)\u2225\n+ \u2225tv(x\u2190\nt , t) \u2212 tv(x\u2190\nt , k\u03b4)\u2225\n+ \u2225tv(x\u2190\nt , k\u03b4) \u2212 tv(x\u2190\nk\u03b4, k\u03b4)\u2225\n\u2264\u03f5approx + L2 \u2225x\u2190\nt \u2212 x\u2190\nt \u2225 + \u03b4L0 + L2 \u2225x\u2190\nt \u2212 x\u2190\nk\u03b4\u2225\n\u2264\u03f5approx + L2 \u2225x\u2190\nt \u2212 x\u2190\nt \u2225 + \u03b4L0 + \u03b4L2L1.\nApplying Gronwall\u2019s Lemma over the interval t \u2208 [\u2212tmax, \u2212tmin],\n\u2225xtmin \u2212 xtmin\u2225\n=\n\r\rx\u2190\n\u2212tmin \u2212 x\u2190\n\u2212tmin\n\r\r\n\u2264eL2(tmax\u2212tmin) \u0000\r\rx\u2190\n\u2212tmax \u2212 x\u2190\n\u2212tmax\n\r\r + (\u03f5approx + \u03b4L0 + \u03b4L2L1) (tmax \u2212 tmin)\n\u0001\n=eL2(tmax\u2212tmin) (\u2225xtmax \u2212 xtmax\u2225 + (\u03f5approx + \u03b4L0 + \u03b4L2L1) (tmax \u2212 tmin)) .\nLemma 3. Given initial x0\ntmax, let x0\ntmin\n=\nODE\n\u0000x0\ntmax, tmax \u2192 tmin\n\u0001\n, and let \u02c6x0\ntmin\n=\nODE\u03b8\n\u0000x0\ntmax, tmax \u2192 tmin\n\u0001\n. We further denote the variables after K Restart iterations as xK\ntmin =\nRestart(x0\ntmin, K) and \u02c6xK\ntmin = Restart\u03b8(\u02c6x0\ntmin, K), with true field and learned field respectively. Then\nthere exists a coupling between xK\ntmin and \u02c6xK\ntmin such that\n\r\rxK\ntmin \u2212 \u02c6xK\ntmin\n\r\r \u2264 e(K+1)L2(tmax\u2212tmin)(K + 1) (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin) .\nProof. We will couple xi\ntmin and \u02c6xi\ntmin by using the same noise \u03b5i\ntmin\u2192tmax in the Restart forward process\nfor i = 0 . . . K \u2212 1 (see Eq. (7)). For any i, let us also define yi,j\ntmin := Restart\u03b8\n\u0000xi\ntmin, j \u2212 i\n\u0001\n, and this\nprocess uses the same noise \u03b5i\ntmin\u2192tmax as previous ones. From this definition, yK,K\ntmin\n= xK\ntmin. We can\nthus bound\n\r\rxK\ntmin, \u02c6xK\ntmin\n\r\r \u2264\n\r\r\ry0,K\ntmin \u2212 \u02c6xK\ntmin\n\r\r\r +\nK\u22121\nX\ni=0\n\r\r\ryi,K\ntmin \u2212 yi+1,K\ntmin\n\r\r\r\n(16)\n16\nUsing the assumption that ts\u03b8(\u00b7, t) is L2 Lipschitz,\n\r\r\ry0,i+1\ntmin\n\u2212 \u02c6xi+1\ntmin\n\r\r\r\n=\n\r\r\rODE\u03b8(y0,i\ntmax, tmax \u2192 tmin) \u2212 ODE\u03b8(\u02c6xi\ntmax, tmax \u2192 tmin)\n\r\r\r\n\u2264eL2(tmax\u2212tmin) \r\r\ry0,i\ntmax \u2212 \u02c6xi\ntmax\n\r\r\r\n=eL2(tmax\u2212tmin) \r\r\ry0,i\ntmin \u2212 \u02c6xi\ntmin\n\r\r\r ,\nwhere the last equality is because we add the same additive Gaussian noise \u03b5i\ntmin\u2192tmax to y0,i\ntmin and \u02c6xi\ntmin\nin the Restart forward process. Applying the above recursively, we get\n\r\r\ry0,K\ntmin \u2212 \u02c6xK\ntmin\n\r\r\r \u2264eKL2(tmax\u2212tmin) \r\r\ry0,0\ntmin \u2212 \u02c6x0\ntmin\n\r\r\r\n\u2264eKL2(tmax\u2212tmin) \r\rx0\ntmin \u2212 \u02c6x0\ntmin\n\r\r\n\u2264e(K+1)L2(tmax\u2212tmin) (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin) ,\n(17)\nwhere the last line follows by Lemma 2 when setting xtmax = \u00afxtmax.\nWe will now bound\n\r\r\ryi,K\ntmin \u2212 yi+1,K\ntmin\n\r\r\r for some i \u2264 K. It follows from definition that\nyi,i+1\ntmin\n= ODE\u03b8\n\u0000xi\ntmax, tmax \u2192 tmin\n\u0001\nyi+1,i+1\ntmin\n= xi+1\ntmin = ODE\n\u0000xi\ntmax, tmax \u2192 tmin\n\u0001\n.\nBy Lemma 2,\n\r\r\ryi,i+1\ntmin\n\u2212 yi+1,i+1\ntmin\n\r\r\r \u2264 eL2(tmax\u2212tmin) (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin)\nFor the remaining steps from i + 2 . . . K, both yi,\u00b7 and yi+1,\u00b7 evolve with ODE\u03b8 in each step. Again\nusing the assumption that ts\u03b8(\u00b7, t) is L2 Lipschitz,\n\r\r\ryi,K\ntmin \u2212 yi+1,K\ntmin\n\r\r\r \u2264 e(K\u2212i)L2(tmax\u2212tmin) (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin)\nSumming the above for i = 0...K \u2212 1, and combining with Eq. (16) and Eq. (17) gives\n\r\rxK\ntmin \u2212 \u02c6xK\ntmin\n\r\r \u2264 e(K+1)L2(tmax\u2212tmin)(K + 1) (\u03b4(L2L1 + L0) + \u03f5approx) (tmax \u2212 tmin) .\nLemma 4. Consider the same setup as Theorem 3. Let xtmin = SDE (xtmax, tmax \u2192 tmin) and let\nxtmin = SDE (xtmax, tmax \u2192 tmin). Then there exists a coupling between xt and xt such that\nE [\u2225xtmin \u2212 xtmin\u2225] \u2264 e2L2(tmax\u2212tmin)E [\u2225xtmax \u2212 xtmax\u2225]\n+ e2L2(tmax\u2212tmin) \u0010\n\u03f5approx + \u03b4L0 + L2\n\u0010\n\u03b4L1 +\np\n2\u03b4dtmax\n\u0011\u0011\n(tmax \u2212 tmin)\nProof. Consider some fixed arbitrary k, and recall that \u03b4 is the stepsize. By definition of SDE and\nSDE\u03b8, for t \u2208 ((k \u2212 1)\u03b4, k\u03b4],\ndxt = \u22122t\u2207 log pt(xt)dt +\n\u221a\n2tdBt\ndxt = \u22122ts\u03b8(xk\u03b4, k\u03b4)dt +\n\u221a\n2tdBt.\nLet us define a coupling between xt and xt by identifying their respective Brownian motions. It\nwill be convenient to define the time-reversed processes x\u2190\nt\n:= x\u2212t, and x\u2190\nt\n:= x\u2212t, along with\nv(x, t) := \u2207 log p\u2212t(x) and v(x, t) := s\u03b8(x, \u2212t). Then there exists a Brownian motion B\u2190\nt , such\nthat for t \u2208 [\u2212tmax, \u2212tmin],\ndx\u2190\nt\n= \u22122tv(x\u2190\nt , t)dt +\n\u221a\n\u22122tdB\u2190\nt\ndx\u2190\nt\n= \u22122tv(x\u2190\nk\u03b4, k\u03b4)dt +\n\u221a\n\u22122tdB\u2190\nt\n\u21d2\nd(x\u2190\nt \u2212 x\u2190\nt ) = \u22122t (v(x\u2190\nt , t) \u2212 v(x\u2190\nk\u03b4, k\u03b4)) dt,\n17\nwhere k is the unique negative integer such that t \u2208 [k\u03b4, (k + 1)\u03b4). Thus\nd\ndtE [\u2225x\u2190\nt \u2212 x\u2190\nt \u2225]\n\u22642 (E [\u2225tv(x\u2190\nt , t) \u2212 tv(x\u2190\nt , t)\u2225] + E [\u2225tv(x\u2190\nt , t) \u2212 tv(x\u2190\nt , t)\u2225])\n+ 2 (E [\u2225tv(x\u2190\nt , t) \u2212 tv(x\u2190\nt , k\u03b4)\u2225] + E [\u2225tv(x\u2190\nt , k\u03b4) \u2212 tv(x\u2190\nk\u03b4, k\u03b4)\u2225])\n\u22642 (\u03f5approx + L2E [\u2225x\u2190\nt \u2212 x\u2190\nt \u2225] + \u03b4L0 + L2E [\u2225x\u2190\nt \u2212 x\u2190\nk\u03b4\u2225])\n\u22642\n\u0010\n\u03f5approx + L2E [\u2225x\u2190\nt \u2212 x\u2190\nt \u2225] + \u03b4L0 + L2\n\u0010\n\u03b4L1 +\np\n2\u03b4dtmax\n\u0011\u0011\n.\nBy Gronwall\u2019s Lemma,\nE [\u2225xtmin \u2212 xtmin\u2225]\n=E\n\u0002\r\rx\u2190\n\u2212tmin \u2212 x\u2190\n\u2212tmin\n\r\r\u0003\n\u2264e2L2(tmax\u2212tmin) \u0010\nE\n\u0002\r\rx\u2190\n\u2212tmax \u2212 x\u2190\n\u2212tmax\n\r\r\u0003\n+\n\u0010\n\u03f5approx + \u03b4L0 + L2\n\u0010\n\u03b4L1 +\np\n2\u03b4dtmax\n\u0011\u0011\n(tmax \u2212 tmin)\n\u0011\n=e2L2(tmax\u2212tmin) \u0010\nE [\u2225xtmax \u2212 xtmax\u2225] +\n\u0010\n\u03f5approx + \u03b4L0 + L2\n\u0010\n\u03b4L1 +\np\n2\u03b4dtmax\n\u0011\u0011\n(tmax \u2212 tmin)\n\u0011\nA.4\nMixing Bounds\nLemma 5. Consider the same setup as Theorem 3. Assume that \u03b4 \u2264 tmin. Let\nxtmin = SDE\u03b8 (xtmax, tmax \u2192 tmin)\nytmin = SDE\u03b8 (ytmax, tmax \u2192 tmin) .\nThen there exists a coupling between xs and ys such that\nTV (xtmin, ytmin) \u2264\n \n1 \u2212 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!\n\u00b7 e\u2212BL1/tmin\u2212L2\n1t2\nmax/t2\nmin\n!\nTV (xtmax, ytmax)\nProof. We will construct a coupling between xt and yt. First, let (xtmax, ytmax) be sampled from the\noptimal TV coupling, i.e., Pr(xtmax \u0338= ytmax) = TV (xtmax, ytmax). Recall that by definition of SDE\u03b8,\nfor t \u2208 ((k \u2212 1)\u03b4, k\u03b4],\ndxt = \u22122ts\u03b8(xk\u03b4, k\u03b4)dt +\n\u221a\n2tdBt.\nLet us define a time-rescaled version of xt: xt := xt2. We verify that\ndxt = \u2212s\u03b8(x(k\u03b4)2, k\u03b4)dt + dBt,\nwhere k is the unique integer satisfying t \u2208 [((k \u2212 1)\u03b4)2, k2\u03b42). Next, we define the time-reversed\nprocess x\u2190\nt\n:= x\u2212t, and let v(x, t) := s\u03b8(x, \u2212t). We verify that there exists a Brownian motion Bx\nt\nsuch that, for t \u2208 [\u2212t2\nmax, \u2212t2\nmin],\ndx\u2190\nt\n= vx\nt dt + dBx\nt ,\nwhere vx\nt = s\u03b8(x\u2190\n\u2212(k\u03b4)2, \u2212k\u03b4), where k is the unique positive integer satisfying \u2212t \u2208 (((k \u2212\n1)\u03b4)2, (k\u03b4)2]. Let dy\u2190\nt\n= vy\nt dt + dBy\nt , be defined analogously. For any positive integer k and for\nany t \u2208 [\u2212(k\u03b4)2, \u2212((k \u2212 1)\u03b4)2), let us define\nzt = x\u2190\n\u2212k2\u03b42 \u2212 y\u2190\n\u2212k2\u03b42 + (2k \u2212 1)\u03b42 \u0010\nvx\n\u2212(k\u03b4)2 \u2212 vy\n\u2212(k\u03b4)2\n\u0011\n+\n\u0010\nBx\nt \u2212 Bx\n\u2212(k\u03b4)2\n\u0011\n\u2212\n\u0010\nBy\nt \u2212 By\n\u2212(k\u03b4)2\n\u0011\n.\nLet \u03b3t :=\nzt\n\u2225zt\u2225. We will now define a coupling between dBx\nt and dBy\nt as\ndBy\nt =\n\u0000I \u2212 21 {t \u2264 \u03c4}\u03b3t\u03b3T\nt\n\u0001\ndBx\nt ,\n18\nwhere 1 {} denotes the indicator function, i.e. 1 {t \u2264 \u03c4} = 1 if t \u2264 \u03c4, and \u03c4 is a stopping time given\nby the first hitting time of zt = 0. Let rt := \u2225zt\u2225. Consider some t \u2208\n\u0000\u2212i2\u03b42, \u2212(i \u2212 1)2\u03b42\u0001\n, and\nLet j := tmax\n\u03b4\n(assume w.l.o.g that this is an integer), then\nrt \u2212 r\u2212t2max \u2264\nj\nX\nk=i\n(2k \u2212 1)\u03b42 \r\r\r(vx\n\u2212(k\u03b4)2 \u2212 vy\n\u2212(k\u03b4)2)\n\r\r\r +\nZ t\n\u2212t2max\n1 {t \u2264 \u03c4}2dB1\ns\n\u2264\nj\nX\nk=i\n\u0000k2 \u2212 (k \u2212 1)2\u0001\n\u03b422L1/ (tmin) +\nZ t\n\u2212t2max\n1 {t \u2264 \u03c4}2dB1\nt\n=\nZ \u2212(i\u22121)\u03b42\n\u2212t2max\n2L1\ntmin\nds +\nZ t\n\u2212t2max\n1 {t \u2264 \u03c4}2dB1\ns,\nwhere dB1\ns = \u27e8\u03b3t, dBx\ns \u2212 dBy\ns \u27e9 is a 1-dimensional Brownian motion. We also verify that\nr\u2212t2max =\n\r\rz\u2212t2max\n\r\r\n=\n\r\r\rx\u2190\n\u2212t2max \u2212 y\u2190\n\u2212t2max + (2j \u2212 1)\u03b42 \u0010\nvx\n\u2212t2max \u2212 vy\n\u2212t2max\n\u0011\n+\n\u0010\nBx\nt \u2212 Bx\n\u2212t2max\n\u0011\n\u2212\n\u0010\nBy\nt \u2212 By\n\u2212t2max\n\u0011\r\r\r\n\u2264\n\r\r\rx\u2190\n\u2212t2max + (2j \u2212 1)\u03b42vx\n\u2212t2max +\n\u0010\nBx\n\u2212(j\u22121)2\u03b42 \u2212 Bx\n\u2212t2max\n\u0011\r\r\r\n+\n\r\r\ry\u2190\n\u2212t2max + (2j \u2212 1)\u03b42vy\n\u2212t2max +\n\u0010\nBx\n\u2212(j\u22121)2\u03b42 \u2212 Bx\nt + By\nt \u2212 By\n\u2212t2max\n\u0011\r\r\r \u2264 B\nwhere the third relation is by adding and subtracting Bx\n\u2212(j\u22121)2\u03b42 \u2212 Bx\nt and using triangle inequality.\nThe fourth relation is by noticing that x\u2190\n\u2212t2max + (2j \u2212 1)\u03b42vx\n\u2212t2max +\n\u0010\nBx\n\u2212(j\u22121)2\u03b42 \u2212 Bx\n\u2212t2max\n\u0011\n=\nx\u2190\n\u2212(j\u22121)2\u03b42 and that y\u2190\n\u2212t2max +(2j \u22121)\u03b42vy\n\u2212t2max +\n\u0010\nBx\n\u2212(j\u22121)2\u03b42 \u2212 Bx\nt + By\nt \u2212 By\n\u2212t2max\n\u0011 d= y\u2190\n\u2212(j\u22121)2\u03b42,\nand then using our assumption in the theorem statement that all processes are supported on a ball of\nradius B/2.\nWe now define a process st defined by dst = 2L1/tmindt + 2dB1\nt , initialized at s\u2212t2max = B \u2265 r\u2212t2max.\nWe can verify that, up to time \u03c4, rt \u2264 st with probability 1. Let \u03c4 \u2032 denote the first-hitting time of st\nto 0, then \u03c4 \u2264 \u03c4 \u2032 with probability 1. Thus\nPr(\u03c4 \u2264 \u2212t2\nmin) \u2265Pr(\u03c4 \u2032 \u2264 \u2212t2\nmin) \u2265 2Q\n \nB\n2\np\nt2max \u2212 t2\nmin\n!\n\u00b7 e\u2212BL1/tmin\u2212L2\n1t2\nmax/t2\nmin\nwhere we apply Lemma 6. The proof follows by noticing that, if \u03c4 \u2264 \u2212t2\nmin, then xtmin = ytmin. This\nis because if \u03c4 \u2208 [\u2212k2\u03b42, \u2212(k \u2212 1)2\u03b42], then x\u2190\n\u2212(k\u22121)2\u03b42 = y\u2190\n\u2212(k\u22121)2\u03b42, and thus x\u2190\nt\n= y\u2190\nt for all\nt \u2265 \u2212(k \u2212 1)2\u03b42, in particular, at t = \u2212t2\nmin.\nLemma 6. Consider the stochastic process\ndrt = dB1\nt + cdt.\nAssume that r0 \u2264 B/2. Let \u03c4 denote the hitting time for rt = 0. Then for any T \u2208 R+,\nPr(\u03c4 \u2264 T) \u2265 2Q\n\u0012 B\n2\n\u221a\nT\n\u0013\n\u00b7 e\u2212ac\u2212 c2T\n2 ,\nwhere Q is the tail probability of a standard Gaussian defined in Definition 1.\nProof. We will use he following facts in our proof:\n1. For x \u223c N(0, \u03c32), Pr(x > r) = 1\n2\n\u0010\n1 \u2212 erf\n\u0010\nr\n\u221a\n2\u03c3\n\u0011\u0011\n= 1\n2erfc\n\u0010\nr\n\u221a\n2\u03c3\n\u0011\n.\n2.\nR T\n0\na exp\n\u0010\n\u2212 a2\n2t\n\u0011\n\u221a\n2\u03c0t3\ndt = erfc\n\u0010\na\n\u221a\n2T\n\u0011\n= 2Pr (N(0, T) > a) = 2Q\n\u0010\na\n\u221a\nT\n\u0011\nby definition of Q.\n19\nLet drt = dB1\nt + cdt, with r0 = a. The density of the hitting time \u03c4 is given by\np(\u03c4 = t) = f(a, c, t) =\na exp\n\u0010\n\u2212 (a+ct)2\n2t\n\u0011\n\u221a\n2\u03c0t3\n.\n(18)\n(see e.g. [3]). From item 2 above,\nZ T\n0\nf(a, 0, t)dt = 2Q\n\u0012 a\n\u221a\nT\n\u0013\n.\nIn the case of a general c \u0338= 0, we can bound (a+ct)2\n2t\n= a2\n2t + ac + c2t\n2 . Consequently,\nf(a, c, t) \u2265 f(a, 0, t) \u00b7 e\u2212ac\u2212 c2t\n2 .\nTherefore,\nPr(\u03c4 \u2264 T) =\nZ T\n0\nf(a, c, t)dt \u2265\nZ T\n0\nf(a, 0, t)dte\u2212c = 2Q\n\u0012 B\n2\n\u221a\nT\n\u0013\n\u00b7 e\u2212ac\u2212 c2T\n2 .\nA.5\nTV Overlap\nDefinition 1. Let x be sampled from standard normal distribution N(0, 1). We define the Gaussian\ntail probability Q(a) := Pr(x \u2265 a).\nLemma 7. We verify that for any two random vectors \u03bex \u223c N(0, \u03c32I) and \u03bey \u223c N(0, \u03c32I), each\nbelonging to Rd, the total variation distance between x\u2032 = x + \u03bex and y\u2032 = y + \u03bey is given by\nTV (x\u2032, y\u2032) = 1 \u2212 2Q (r) \u2264 1 \u2212\n2r\nr2 + 1\n1\n\u221a\n2\u03c0 e\u2212r2/2,\nwhere r = \u2225x\u2212y\u2225\n2\u03c3\n, and Q(r) = Pr(\u03be \u2265 r), when \u03be \u223c N(0, 1).\nProof. Let \u03b3 :=\nx\u2212y\n\u2225x\u2212y\u2225. We decompose x\u2032, y\u2032 into the subspace/orthogonal space defined by \u03b3:\nx\u2032 = x\u22a5 + \u03be\u22a5\nx + x\u2225 + \u03be\u2225\nx\ny\u2032 = y\u22a5 + \u03be\u22a5\ny + y\u2225 + \u03be\u2225\ny\nwhere we define\nx\u2225 := \u03b3\u03b3T x\nx\u22a5 := x \u2212 x\u2225\ny\u2225 := \u03b3\u03b3T y\ny\u22a5 := y \u2212 y\u2225\n\u03be\u2225\nx := \u03b3\u03b3T \u03bex\n\u03be\u22a5\nx := \u03bex \u2212 \u03be\u2225\nx\n\u03be\u2225\ny := \u03b3\u03b3T \u03bey\n\u03be\u22a5\ny := \u03bey \u2212 \u03be\u2225\ny\nWe verify the independence \u03be\u22a5\nx \u22a5\u22a5 \u03be\u2225\nx and \u03be\u22a5\ny \u22a5\u22a5 \u03be\u2225\ny as they are orthogonal decompositions of the\nstandard Gaussian. We will define a coupling between x\u2032 and y\u2032 by setting \u03be\u22a5\nx = \u03be\u22a5\ny . Under this\ncoupling, we verify that\n\u0000x\u22a5 + \u03be\u22a5\nx\n\u0001\n\u2212\n\u0000y\u22a5 + \u03be\u22a5\ny\n\u0001\n= x \u2212 y \u2212 \u03b3\u03b3T (x \u2212 y) = 0\nTherefore, x\u2032 = y\u2032 if and only if x\u2225 + \u03be\u2225\nx = y\u2225 + \u03be\u2225\ny. Next, we draw (a, b) from the optimal coupling\nbetween N(0, 1) and N( \u2225x\u2212y\u2225\n\u03c3\n, 1). We verify that x\u2225 + \u03be\u2225\nx and y\u2225 + \u03be\u2225\ny both lie in the span of\n\u03b3. Thus it suffices to compare\nD\n\u03b3, x\u2225 + \u03be\u2225\nx\nE\nand\nD\n\u03b3, y\u2225 + \u03be\u2225\ny\nE\n. We verify that\nD\n\u03b3, x\u2225 + \u03be\u2225\nx\nE\n=\n20\n\n\u03b3, y\u2225\u000b\n+\n\n\u03b3, x\u2225 \u2212 y\u2225\u000b\n+\nD\n\u03b3, \u03be\u2225\nx\nE\n\u223c N(\n\n\u03b3, y\u2225\u000b\n+\u2225x \u2212 y\u2225 , \u03c32)\nd=\n\n\u03b3, y\u2225\u000b\n+\u03c3b. We similarly verify\nthat\nD\n\u03b3, y\u2225 + \u03be\u2225\ny\nE\n=\n\n\u03b3, y\u2225\u000b\n+\nD\n\u03b3, \u03be\u2225\ny\nE\n\u223c N(\n\n\u03b3, y\u2225\u000b\n, \u03c32)\nd=\n\n\u03b3, y\u2225\u000b\n+ \u03c3a.\nThus TV (x\u2032, y\u2032) = TV (\u03c3a, \u03c3b) = 1 \u2212 2Q\n\u0010\n\u2225x\u2212y\u2225\n2\u03c3\n\u0011\n. The last inequality follows from\nPr(N(0, 1) \u2265 r) \u2265\nr\nr2 + 1\n1\n\u221a\n2\u03c0 e\u2212r2/2\nB\nMore on Restart Algorithm\nB.1\nEDM Discretization Scheme\n[13] proposes a discretization scheme for ODE given the starting tmax and end time tmin. Denote the\nnumber of steps as N, then the EDM discretization scheme is:\nti<N =\n\u0012\nt\n1\n\u03c1\nmax +\ni\nN \u2212 1(t\n1\n\u03c1\nmin \u2212 t\n1\n\u03c1\nmax)\n\u0013\u03c1\nwith t0 = tmax and tN\u22121 = tmin. \u03c1 is a hyperparameter that determines the extent to which steps near\ntmin are shortened. We adopt the value \u03c1 = 7 suggested by [13] in all of our experiments. We apply\nthe EDM scheme to creates a time discretization in each Restart interval [tmax, tmin] in the Restart\nbackward process, as well as the main backward process between [0, T] (by additionally setting\ntmin = 0.002 and tN = 0 as in [13]). It is important to note that tmin should be included within the\nlist of time steps in the main backward process to seamlessly incorporate the Restart interval into the\nmain backward process. We summarize the scheme as a function in Algorithm 1.\nAlgorithm 1 EDM_Scheme(tmin, tmax, N, \u03c1 = 7)\n1: return\n\u001a\n(t\n1\n\u03c1\nmax +\ni\nN\u22121(t\n1\n\u03c1\nmin \u2212 t\n1\n\u03c1\nmax))\u03c1\n\u001bN\u22121\ni=0\nB.2\nRestart Algorithm\nWe present the pseudocode for the Restart algorithm in Algorithm 2. In this pseudocode, we describe\na more general case that applies l-level Restarting strategy. For each Restart segment, the include\nthe number of steps in the Restart backward process NRestart, the Restart interval [tmin, tmax] and the\nnumber of Restart iteration K. We further denote the number of steps in the main backward process\nas Nmain. We use the EDM discretization scheme (Algorithm 1) to construct time steps for the main\nbackward process (t0 = T, tNmain = 0) as well as the Restart backward process, when given the\nstarting/end time and the number of steps.\nAlthough Heun\u2019s 2nd order method [2] (Algorithm 3) is the default ODE solver in the pseudocode, it\ncan be substituted with other ODE solvers, such as Euler\u2019s method or the DPM solver [16].\nThe provided pseudocode in Algorithm 2 is tailored specifically for diffusion models [13]. To\nadapt Restart for other generative models like PFGM++ [28], we only need to modify the Gaussian\nperturbation kernel in the Restart forward process (line 10 in Algorithm 2) to the one used in\nPFGM++.\nC\nExperimental Details\nIn this section, we discuss the configurations for different samplers in details. All the experiments are\nconducted on eight NVIDIA A100 GPUs.\n21\nAlgorithm 2 Restart sampling\n1: Input: Score network s\u03b8, time steps in main backward process ti\u2208{0,Nmain}, Restart parameters\n{(NRestart,j, Kj, tmin,j, tmax,j)}l\nj=1\n2: Round tmin,j\u2208{1,l} to its nearest neighbor in ti\u2208{0,Nmain}\n3: Sample x0 \u223c N(0, T 2I)\n4: for i = 0 . . . Nmain \u2212 1 do\n\u25b7 Main backward process\n5:\nxti+1 = OneStep_Heun(s\u03b8, ti, ti+1)\n\u25b7 Running single step ODE\n6:\nif \u2203j \u2208 {1, . . . , l}, ti+1 = tmin,j then\n7:\ntmin = tmin,j, tmax = tmax,j, K = Kj, NRestart = NRestart,j\n8:\nx0\ntmin = xti+1\n9:\nfor k = 0 . . . K \u2212 1 do\n\u25b7 Restart for K iterations\n10:\n\u03b5tmin\u2192tmax \u223c N(0, (t2\nmax \u2212 t2\nmin)I)\n11:\nxk+1\ntmax = xk\ntmin + \u03b5tmin\u2192tmax\n\u25b7 Restart forward process\n12:\n{\u00aftm}NRestart\u22121\nm=0\n= EDM_Scheme(tmin, tmax, NRestart)\n13:\nfor m = 0 . . . NRestart \u2212 1 do\n\u25b7 Restart backward process\n14:\nxk+1\n\u00aftm+1 = OneStep_Heun(s\u03b8, \u00aftm, \u00aftm+1)\n15:\nend for\n16:\nend for\n17:\nend if\n18: end for\n19: return xtNmain\nAlgorithm 3 OneStep_Heun(s\u03b8, xti, ti, ti+1)\n1: di = tis\u03b8(xti, ti)\n2: xti+1 = xti \u2212 (ti+1 \u2212 ti)di\n3: if ti+1 \u0338= 0 then\n4:\nd\u2032\ni = ti+1s\u03b8(xti+1, ti+1)\n5:\nxti+1 = xti \u2212 (ti+1 \u2212 ti)( 1\n2di + 1\n2d\u2032\ni)\n6: end if\n7: return xti+1\nC.1\nConfigurations for Baselines\nWe select Vanilla SDE [23], Improved SDE [13], Gonna Go Fast [12] as SDE baselines and\nthe Heun\u2019s 2nd order method [2] (Alg 3) as ODE baseline on standard benchmarks CIFAR-10 and\nImageNet 64\u00d764. We choose DDIM [22], Heun\u2019s 2nd order method, and DDPM [9] for comparison\non Stable Diffusion model.\nVanilla SDE denotes the reverse-time SDE sampler in [23]. For Improved SDE, we use the recom-\nmended dataset-specific hyperparameters (e.g., Smax, Smin, Schurn) in Table 5 of the EDM paper [13].\nThey obtained these hyperparameters by grid search. Gonna Go Fast [12] applied an adaptive step\nsize technique based on Vanilla SDE and we directly report the FID scores listed in [12] for Gonna\nGo Fast on CIFAR-10 (VP). For fair comparison, we use the EDM discretization scheme [13] for\nVanilla SDE, Improved SDE, Heun as well as Restart.\nWe borrow the hyperparameters such as discretization scheme or initial noise scale on Stable Diffusion\nmodels in the diffuser 3 code repository. We directly use the DDIM and DDPM samplers implemented\nin the repo. We apply the same set of hyperparameters to Heun and Restart.\nC.2\nConfigurations for Restart\nWe report the configurations for Restart for different models and NFE on standard benchmarks\nCIFAR-10 and ImageNet 64 \u00d7 64. The hyperparameters of Restart include the number of steps\nin the main backward process Nmain, the number of steps in the Restart backward process NRestart,\nthe Restart interval [tmin, tmax] and the number of Restart iteration K. In Table 3 (CIFAR-10, VP)\n3https://github.com/huggingface/diffusers\n22\nwe provide the quintuplet (Nmain, NRestart, tmin, tmax, K) for each experiment. Since we apply the\nmulti-level Restart strategy for ImageNet 64 \u00d7 64, we provide Nmain as well as a list of quadruple\n{(NRestart,i, Ki, tmin,i, tmax,i)}l\ni=1 (l is the number of Restart interval depending on experiments) in\nTable 5. In order to integrate the Restart time interval to the main backward process, we round tmin,i\nto its nearest neighbor in the time steps of main backward process, as shown in line 2 of Algorithm 2.\nWe apply Heun method for both main/backward process. The formula for NFE calculation is\nNFE =\n2 \u00b7 Nmain \u2212 1\n|\n{z\n}\nmain backward process\n+ Pl\ni=1\nKi\n|{z}\nnumber of repetitions\n\u00b7\n(2 \u00b7 (NRestart,i \u2212 1))\n|\n{z\n}\nper iteration in ith Restart interval\nin this case. Inspired by\n[13], we inflate the additive noise in the Restart forward process by multiplying Snoise = 1.003 on\nImageNet 64 \u00d7 64, to counteract the over-denoising tendency of neural networks. We also observe\nthat setting \u03b3 = 0.05 in Algorithm 2 of EDM [13] would sligtly boost the Restart performance on\nImageNet 64 \u00d7 64 when t \u2208 [0.01, 1].\nWe further include the configurations for Restart on Stable Diffusion models in Table 10, with a\nvarying guidance weight w. Similar to ImageNet 64 \u00d7 64, we use multi-level Restart with a fixed\nnumber of steps Nmain = 30 in the main backward process. We utilize the Euler method for the\nmain backward process and the Heun method for the Restart backward process, as our empirical\nobservations indicate that the Heun method doesn\u2019t yield significant improvements over the Euler\nmethod, yet necessitates double the steps. The number of steps equals to Nmain + Pl\ni=1 Ki \u00b7 (2 \u00b7\n(NRestart,i \u2212 1)) in this case. We set the total number of steps to 66, including main backward process\nand Restart backward process.\nGiven the prohibitively large search space for each Restart quadruple, a comprehensive enumeration\nof all possibilities is impractical due to computational limitations. Instead, we adjust the configuration\nmanually, guided by the heuristic that weaker/smaller models or more challenging tasks necessitate\na stronger Restart strength (e.g., larger K, wider Restart interval, etc). On average, we select the\nbest configuration from 5 sets for each experiment; these few trials have empirically outperformed\nprevious SDE/ODE samplers. We believe that developing a systematic approach for determining\nRestart configurations could be of significant value in the future.\nC.3\nPre-trained Models\nFor CIFAR-10 dataset, we use the pre-trained VP and EDM models from the EDM repository 4, and\nPFGM++ (D = 2048) model from the PFGM++ repository 5. For ImageNet 64 \u00d7 64, we borrow the\npre-trained EDM model from EDM repository as well.\nC.4\nClassifier-free Guidance\nWe follow the convention in [20], where each step in classifier-free guidance is as follows:\n\u02dcs\u03b8(x, c, t) = ws\u03b8(x, c, t) + (1 \u2212 w)s\u03b8(x, t)\nwhere c is the conditions, and s\u03b8(x, c, t)/s\u03b8(x, t) is the conditional/unconditional models, sharing\nparameters. Increasing w would strength the effect of guidance, usually leading to a better text-image\nalignment [20].\nC.5\nMore on the Synthetic Experiment\nC.5.1\nDiscrete Dataset\nWe generate the underlying discrete dataset S with |S| = 2000 as follows. Firstly, we sample 2000\npoints, denoted as S1, from a mixture of two Gaussians in R4. Next, we project these points onto R20.\nTo ensure a variance of 1 on each dimension, we scale the coordinates accordingly. This setup aims\nto simulate data points that primarily reside on a lower-dimensional manifold with multiple modes.\nThe specific details are as follows: S1 \u223c 0.3N(a, s2I) + 0.7(\u2212a, s2I), where a = (3, 3, 3, 3) \u2282 R4\nand s = 1. Then, we randomly select a projection matrix P \u2208 R20\u00d74, where each entry is drawn\nfrom N(0, 1), and compute S2 = PS1. Finally, we scale each coordinate by a constant factor to\nensure a variance of 1.\n4https://github.com/NVlabs/edm\n5https://github.com/Newbeeer/pfgmpp\n23\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAdditional Sampling error\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nContracted error\nODE\nVanilla SDE\nImproved SDE\nRestart\n(a)\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAdditional Sampling error\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTotal Error\nODE\nImproved SDE\nVanilla SDE\nRestart\n(b)\n20\n40\n80\n160\n320\nNFE\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\nTotal Error\nODE\nVanilla SDE\nImproved SDE\nRestart\n(c)\nFigure 8: Comparison of additional sampling error versus (a) contracted error (plotting the Pareto\nfrontier) and (b) total error (using a scatter plot). (c) Pareto frontier of NFE versus total error.\nC.5.2\nModel Architecture\nWe employ a common MLP architecture with a latent size of 64 to learn the score function. The\ntraining method is adapted from [13], which includes the preconditioning technique and denoising\nscore-matching objective [25].\nC.5.3\nVarying Hyperparameters\nTo achieve the best trade-off between contracted error and additional sampling error, and optimize\nthe NFE versus FID (Fr\u00e9chet Inception Distance) performance, we explore various hyperparameters.\n[13] shows that the Vanilla SDE can be endowed with additional flexibility by varying the coefficient\n\u03b2(t) (Eq.(6) in [13]). Hence, regarding SDE, we consider NFE values from {20, 40, 80, 160, 320},\nand multiply the original \u03b2(t) = \u02d9\u03c3(t)/\u03c3(t) [13] with values from {0, 0.25, 0.5, 1, 1.5, 2, 4, 8}. It\nis important to note that larger NFE values do not lead to further performance improvements. For\nrestarts, we tried the following two settings: first we set the number of steps in Restart backward\nprocess to 40 and vary the number of Restart iterations K in the range {0, 5, 10, 15, 20, 25, 30, 35}.\nWe also conduct a grid search with the number of Restart iterations K ranging from 5 to 25 and the\nnumber of steps in Restart backward process varying from 2 to 7. For ODE, we experiment with the\nnumber of steps set to {20, 40, 80, 160, 320, 640}.\nAdditionally, we conduct an experiment for Improved SDE in EDM. We try different values of Schurn\nin the range of {0, 1, 2, 4, 8, 16, 32, 48, 64}. We also perform a grid search where the number of steps\nranged from 20 to 320 and Schurn takes values of [0.2 \u00d7 steps, 0.5 \u00d7 steps, 20, 60]. The plot combines\nthe results from SDE and is displayed in Figure 8.\nTo mitigate the impact of randomness, we collect the data by averaging the results from five runs with\nthe same hyperparameters. To compute the Wasserstein distance between two discrete distributions,\nwe use minimum weight matching.\nC.5.4\nPlotting the Pareto frontier\nWe generate the Pareto frontier plots as follows. For the additional sampling error versus contracted\nerror plot, we first sort all the data points based on their additional sampling error and then connect\nthe data points that represent prefix minimums of the contracted error. Similarly, for the NFE versus\nFID plot, we sort the data points based on their NFE values and connect the points where the FID is a\nprefix minimum.\nD\nExtra Experimental Results\nD.1\nNumerical Results\nIn this section, we provide the corresponding numerical reuslts of Fig. 3(a) and Fig. 3(b), in Ta-\nble 2, 3 (CIFAR-10 VP, EDM, PFGM++) and Table 4, 5 (ImageNet 64 \u00d7 64 EDM), respectively. We\nalso include the performance of Vanilla SDE in those tables. For the evaluation, we compute the\nFr\u00e9chet distance between 50000 generated samples and the pre-computed statistics of CIFAR-10 and\nImageNet 64 \u00d7 64. We follow the evaluation protocol in EDM [13] that calculates each FID scores\nthree times with different seeds and report the minimum.\n24\nWe also provide the numerical results on the Stable Diffusion model [19], with a classifier guidance\nweight w = 2, 3, 5, 8 in Table 6, 7, 8, 9. As in [17], we report the zero-shot FID score on 5K random\nprompts sampled from the COCO validation set. We evaluate CLIP score [6] with the open-sourced\nViT-g/14 [11], Aesthetic score by the more recent LAION-Aesthetics Predictor V2 6. We average the\nCLIP and Aesthetic scores over 5K generated samples. The number of function evaluations is two\ntimes the sampling steps in Stable Diffusion model, since each sampling step involves the evaluation\nof the conditional and unconditional model.\nTable 2: CIFAR-10 sample quality (FID score) and number of function evaluations (NFE) on VP [23]\nfor baselines\nNFE\nFID\nODE (Heun) [13]\n1023\n2.90\n511\n2.90\n255\n2.90\n127\n2.90\n63\n2.89\n35\n2.97\nVanilla SDE [23]\n1024\n2.79\n512\n4.01\n256\n4.79\n128\n12.57\nGonna Go Fast [12]\n1000\n2.55\n329\n2.70\n274\n2.74\n179\n2.59\n147\n2.95\n49\n72.29\nImproved SDE [13]\n1023\n2.35\n511\n2.37\n255\n2.40\n127\n2.58\n63\n2.88\n35\n3.45\n6https://github.com/christophschuhmann/improved-aesthetic-predictor\n25\nTable 3: CIFAR-10 sample quality (FID score), number of function evaluations (NFE) and Restart\nconfigurations on VP [23], VP with DPM-Solver-3 [16], EDM [13] and PFGM++ [28]\nMethod\nNFE\nFID\nConfiguration\n(Nmain, NRestart,i, Ki, tmin,i, tmax,i)\nVP\n519\n2.11\n(20, 9, 30, 0.06, 0.20)\n115\n2.21\n(18, 3, 20, 0.06, 0.30)\n75\n2.27\n(18, 3, 10, 0.06, 0.30)\n55\n2.45\n(18, 3, 5, 0.06, 0.30)\n43\n2.70\n(18, 3, 2, 0.06, 0.30)\nVP w/ DPM-Solver-3\n27\n2.11\n(8, 3, 1, 0.06, 0.3)\n24\n2.15\n(7, 3, 1, 0.06, 1)\n21\n2.28\n(6, 3, 1, 0.06, 1)\n18\n2.40\n(5, 3, 1, 0.06, 1)\nEDM\n43\n1.90\n(18, 3, 2, 0.14, 0.30)\nPFGM++\n43\n1.88\n(18, 3, 2, 0.14, 0.30)\nTable 4: ImageNet 64 \u00d7 64 sample quality (FID score) and number of function evaluations (NFE) on\nEDM [13] for baselines\nNFE\nFID (50k)\nODE (Heun) [13]\n1023\n2.24\n511\n2.24\n255\n2.24\n127\n2.25\n63\n2.30\n35\n2.46\nVanilla SDE [23]\n1024\n1.89\n512\n3.38\n256\n11.91\n128\n59.71\nImproved SDE [13]\n1023\n1.40\n511\n1.45\n255\n1.50\n127\n1.75\n63\n2.24\n35\n2.97\n26\nTable 5: ImageNet 64 \u00d7 64 sample quality (FID score), number of function evaluations (NFE) and\nRestart configurations on EDM [13]\nNFE\nFID (50k)\nConfiguration\nNmain, {(NRestart,i, Ki, tmin,i, tmax,i)}l\ni=1\n623\n1.36\n36, {(10, 3, 19.35, 40.79),(10, 3, 1.09, 1.92),\n(7, 6, 0.59, 1.09), (7, 6, 0.30, 0.59),\n(7, 25, 0.06, 0.30)}\n535\n1.39\n36, {(6, 1, 19.35, 40.79),(6, 1, 1.09, 1.92),\n(7, 6, 0.59, 1.09), (7, 6, 0.30, 0.59),\n(7, 25, 0.06, 0.30)}\n385\n1.41\n36, {(3, 1, 19.35, 40.79),(6, 1, 1.09, 1.92),\n(6, 5, 0.59, 1.09), (6, 5, 0.30, 0.59),\n(6, 20, 0.06, 0.30)}\n203\n1.46\n36, {(4, 1, 19.35, 40.79),(4, 1, 1.09, 1.92),\n(4, 5, 0.59, 1.09), (4, 5, 0.30, 0.59),\n(6, 6, 0.06, 0.30)}\n165\n1.51\n18, {(3, 1, 19.35, 40.79),(4, 1, 1.09, 1.92),\n(4, 5, 0.59, 1.09), (4, 5, 0.30, 0.59),\n(4, 10, 0.06, 0.30)}\n99\n1.71\n18, {(3, 1, 19.35, 40.79),(4, 1, 1.09, 1.92),\n(4, 4, 0.59, 1.09), (4, 1, 0.30, 0.59),\n(4, 4, 0.06, 0.30)}\n67\n1.95\n18, {(5, 1, 19.35, 40.79),(5, 1, 1.09, 1.92),\n(5, 1, 0.59, 1.09), (5, 1, 0.06, 0.30)}\n39\n2.38\n14, {(3, 1, 19.35, 40.79),\n(3, 1, 1.09, 1.92), (3, 1, 0.06, 0.30)}\nTable 6: Numerical results on Stable Diffusion v1.5 with a classifier-free guidance weight w = 2\nSteps\nFID (5k) \u2193\nCLIP score \u2191\nAesthetic score \u2191\nDDIM [22]\n50\n16.08\n0.2905\n5.13\n100\n15.35\n0.2920\n5.15\nHeun\n51\n18.80\n0.2865\n5.14\n101\n18.21\n0.2871\n5.15\nDDPM [9]\n100\n13.53\n0.3012\n5.20\n200\n13.22\n0.2999\n5.19\nRestart\n66\n13.16\n0.2987\n5.19\nTable 7: Numerical results on Stable Diffusion v1.5 with a classifier-free guidance weight w = 3\nSteps\nFID (5k) \u2193\nCLIP score \u2191\nAesthetic score \u2191\nDDIM [22]\n50\n14.28\n0.3056\n5.22\n100\n14.30\n0.3056\n5.22\nHeun\n51\n15.63\n0.3022\n5.20\n101\n15.40\n0.3026\n5.21\nDDPM [9]\n100\n15.72\n0.3129\n5.28\n200\n15.13\n0.3131\n5.28\nRestart\n66\n14.48\n0.3079\n5.25\n27\nTable 8: Numerical results on Stable Diffusion v1.5 with a classifier-free guidance weight w = 5\nSteps\nFID (5k) \u2193\nCLIP score \u2191\nAesthetic score \u2191\nDDIM [22]\n50\n16.60\n0.3154\n5.31\n100\n16.80\n0.3157\n5.31\nHeun\n51\n16.26\n0.3135\n5.28\n101\n16.38\n0.3136\n5.29\nDDPM [9]\n100\n19.62\n0.3197\n5.36\n200\n18.88\n0.3200\n5.35\nRestart\n66\n16.21\n0.3179\n5.33\nTable 9: Numerical results on Stable Diffusion v1.5 with a classifier-free guidance weight w = 8\nSteps\nFID (5k) \u2193\nCLIP score \u2191\nAesthetic score \u2191\nDDIM [22]\n50\n19.83\n0.3206\n5.37\n100\n19.82\n0.3200\n5.37\nHeun\n51\n18.44\n0.3186\n5.35\n101\n18.72\n0.3185\n5.36\nDDPM [9]\n100\n22.58\n0.3223\n5.39\n200\n21.67\n0.3212\n5.38\nRestart\n47\n18.40\n0.3228\n5.41\nTable 10: Restart (Steps=66) configurations on Stable Diffusion v1.5\nw\nConfiguration\nNmain, {(NRestart,i, Ki, tmin,i, tmax,i)}l\ni=1\n2\n30, {(5, 2, 1, 9), (5, 2, 5, 10)}\n3\n30, {(10, 2, 0.1, 3)}\n5\n30, {(10, 2 0.1, 2)}\n8\n30, {(10, 2, 0.1, 2)}\n0.285\n0.290\n0.295\n0.300\n0.305\n0.310\n0.315\n0.320\nCLIP score (ViT-g/14)\n14\n16\n18\n20\n22\nFID score (5K)\nRestart (Steps=66)\nDDIM (Steps=50)\nDDIM (Steps=100)\nHeun (Steps=51)\nHeun (Steps=101)\nDDPM (Steps=100)\nDDPM (Steps=200)\n(a) FID versus CLIP score\n5.15\n5.20\n5.25\n5.30\n5.35\n5.40\nAesthetic score\n14\n16\n18\n20\n22\nFID score (5K)\nRestart (Steps=66)\nDDIM (Steps=50)\nDDIM (Steps=100)\nHeun (Steps=51)\nHeun (Steps=101)\nDDPM (Steps=100)\nDDPM (Steps=200)\n(b) FID versus Aesthetic score\nFigure 9: FID score versus (a) CLIP ViT-g/14 score and (b) Aesthetic score for text-to-image\ngeneration at 512 \u00d7 512 resolution, using Stable Diffusion v1.5 with varying classifier-free guidance\nweight w = 2, 3, 5, 8.\n28\n4\n2\n0\n2\nlog(tmin)\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\nFID score (50K)\nEDM (Restart)\nEDM (ODE)\nVP (Restart)\nVP (ODE)\n(a)\n(b)\nFigure 10: (a): Adjusting tmin in Restart on VP/EDM; (b): Adjusting the Restart interval length when\ntmin = 0.06.\nD.2\nSensitivity Analysis of Hyper-parameters\nWe also investigate the impact of varying tmin when tmax = tmin + 0.3, and the length the restart\ninterval when tmin = 0.06. Fig. 10(a) reveals that FID scores achieve a minimum at a tmin close to 0\non VP, indicating higher accumulated errors at the end of sampling and poor neural estimations at\nsmall t. Note that the Restart interval 0.3 is about twice the length of the one in Table 1 and Restart\ndoes not outperform the ODE baseline on EDM. This suggests that, as a rule of thumb, we should\napply greater Restart strength (e.g., larger K, tmax \u2212 tmin) for weaker or smaller architectures and\nvice versa.\nIn theory, a longer interval enhances contraction but may add more additional sampling errors. Again,\nthe balance between these factors results in a V-shaped trend in our plots (Fig. 10(b)). In practice,\nselecting tmax close to the dataset\u2019s radius usually ensures effective mixing when tmin is small.\nE\nExtended Generated Images\nIn this section, we provide extended generated images by Restart, DDIM, Heun and DDPM on\ntext-to-image Stable Diffusion v1.5 model [19]. We showcase the samples of four sets of text prompts\nin Fig. 11, Fig. 12, Fig. 13, Fig. 14, with a classifier-guidance weight w = 8.\nF\nHeun\u2019s method is DPM-Solver-2 (with r2 = 1)\nThe first order ODE in DPM-Solver [16] (DPM-Solver-1) is in the form of:\n\u02c6xti\u22121 =\n\u03b1ti\n\u03b1ti\u22121\n\u02c6xti\u22121 \u2212 (\u02c6\u03c3ti\u22121\n\u03b1ti\n\u03b1ti\u22121\n\u2212 \u02c6\u03c3ti)\u02c6\u03c3ti\u2207x log p\u02c6\u03c3ti (\u02c6xti)\n(19)\nThe first order ODE in EDM is in the form of\nxti\u22121 = xti \u2212 (\u03c3ti\u22121 \u2212 \u03c3ti)\u03c3ti\u2207x log p\u03c3ti (xti)\n(20)\nWhen xt = \u02c6xt\n\u03b1t , \u02c6\u03c3t = \u03c3t\u03b1t, we can rewrite the DPM-Solver-1 (Eq. (19)) as:\nxti\u22121 = xti \u2212 (\u03c3ti\u22121 \u2212 \u03c3ti)\u02c6\u03c3ti\u2207x log p\u02c6\u03c3ti (\u02c6xti)\n= xti \u2212 (\u03c3ti\u22121 \u2212 \u03c3ti)\u02c6\u03c3ti\u2207x log p\u03c3ti (xti) 1\n\u03b1ti\n(change-of-variable)\n= xti \u2212 (\u03c3ti\u22121 \u2212 \u03c3ti)\u03c3ti\u2207x log p\u03c3ti (xti)\n29\n(a) Restart (Steps=66)\n(b) DDIM (Steps=100)\n(c) Heun (Steps=101)\n(d) DDPM (Steps=100)\nFigure 11: Generated images with text prompt=\"A photo of an astronaut riding a horse on mars\" and\nw = 8.\n30\n(a) Restart (Steps=66)\n(b) DDIM (Steps=100)\n(c) Heun (Steps=101)\n(d) DDPM (Steps=100)\nFigure 12: Generated images with text prompt=\"A raccoon playing table tennis\" and w = 8.\nwhere the expression is exact the same as the ODE in EDM [13]. It indicates that the sampling\ntrajectory in DPM-Solver-1 is equivalent to the one in EDM, up to a time-dependent scaling (\u03b1t).\nAs limt\u21920 \u03b1t = 1, the two solvers will leads to the same final points when using the same time\ndiscretization. Note that the DPM-Solver-1 is also equivalent to DDIM (c.f. Section 4.1 in [16]), as\nalso used in this paper.\nWith that, we can further verify that the Heun\u2019s method used in this paper corresponds to the\nDPM-Solver-2 when setting r1 = 1.\nG\nBroader Impact\nThe field of deep generative models incorporating differential equations is rapidly evolving and holds\nsignificant potential to shape our society. Nowadays, a multitude of photo-realistic images generated\nby text-to-image Stable Diffusion models populate the internet. Our work introduces Restart, a novel\nsampling algorithm that outperforms previous samplers for diffusion models and PFGM++. With\napplications extending across diverse areas, the Restart sampling algorithm is especially suitable\nfor generation tasks demanding high quality and rapid speed. Yet, it is crucial to recognize that\n31\n(a) Restart (Steps=66)\n(b) DDIM (Steps=100)\n(c) Heun (Steps=101)\n(d) DDPM (Steps=100)\nFigure 13: Generated images with text prompt=\"Intricate origami of a fox in a snowy forest\" and\nw = 8.\n32\n(a) Restart (Steps=66)\n(b) DDIM (Steps=100)\n(c) Heun (Steps=101)\n(d) DDPM (Steps=100)\nFigure 14: Generated images with text prompt=\"A transparent sculpture of a duck made out of glass\"\nand w = 8.\n33\nthe utilization of such algorithms can yield both positive and negative repercussions, contingent on\ntheir specific applications. On the one hand, Restart sampling can facilitate the generation of highly\nrealistic images and audio samples, potentially advancing sectors such as entertainment, advertising,\nand education. On the other hand, it could also be misused in deepfake technology, potentially leading\nto social scams and misinformation. In light of these potential risks, further research is required to\ndevelop robustness guarantees for generative models, ensuring their use aligns with ethical guidelines\nand societal interests.\n34\n"
  },
  {
    "title": "ViNT: A Foundation Model for Visual Navigation",
    "link": "https://arxiv.org/pdf/2306.14846.pdf",
    "upvote": "4",
    "text": "ViNT: A Foundation Model for Visual Navigation\nDhruv Shah\u2020, Ajay Sridhar\u2020, Nitish Dashora\u2020,\nKyle Stachowicz, Kevin Black, Noriaki Hirose, Sergey Levine\nUC Berkeley\nAbstract: General-purpose pre-trained models (\u201cfoundation models\u201d) have en-\nabled practitioners to produce generalizable solutions for individual machine\nlearning problems with datasets that are significantly smaller than those required\nfor learning from scratch. Such models are typically trained on large and diverse\ndatasets with weak supervision, consuming much more training data than is avail-\nable for any individual downstream application. In this paper, we describe the\nVisual Navigation Transformer (ViNT), a foundation model that aims to bring\nthe success of general-purpose pre-trained models to vision-based robotic navi-\ngation. ViNT is trained with a general goal-reaching objective that can be used\nwith any navigation dataset, and employs a flexible Transformer-based architec-\nture to learn navigational affordances and enable efficient adaptation to a variety\nof downstream navigational tasks. ViNT is trained on a number of existing naviga-\ntion datasets, comprising hundreds of hours of robotic navigation from a variety of\ndifferent robotic platforms, and exhibits positive transfer, outperforming special-\nist models trained on narrower datasets. ViNT can be augmented with diffusion-\nbased goal proposals to explore novel environments, and can solve kilometer-scale\nnavigation problems when equipped with long-range heuristics. ViNT can also be\nadapted to novel task specifications with a technique inspired by prompt-tuning,\nwhere the goal encoder is replaced by an encoding of another task modality (e.g.,\nGPS waypoints or turn-by-turn directions) embedded into the same space of goal\ntokens. This flexibility and ability to accommodate a variety of downstream prob-\nlem domains establish ViNT as an effective foundation model for mobile robotics.\nTraining Data\nZero-Shot Deployment\nViNT Foundation Model\nAdapt to Downstream Tasks\nKilometer-Scale Exploration\nRoute-Guided Navigation\nCoverage Mapping\nFigure 1: Overview of the ViNT foundation model. ViNT generalizes zero-shot across environments and\nrobot embodiments, and can be directly applied to tasks including exploration and navigation around humans.\nViNT can also be fine-tuned with a small amount of data to expand its capabilities to new tasks.\n1\nIntroduction\nRecently, machine learning methods have achieved broad success in natural language processing [1],\nvisual perception [2\u20134], and other domains [5, 6] by leveraging Internet-scale data to train general-\npurpose \u201cfoundation\u201d models that can be adapted to new tasks by zero-shot transfer, prompt-tuning,\nor fine-tuning on target data [7\u201310]. Although this paradigm has been successful in many domains,\nit is difficult to apply in robotics due to the sheer diversity of environments, platforms, and applica-\ntions. In this paper we ask the question: what is required of a foundation model for mobile robotics?\n\u2020 Lead Authors. Videos, code, and models: general-navigation-models.github.io.\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2306.14846v2  [cs.RO]  24 Oct 2023\nIn this paper, we define a robot foundation model as a pre-trained model that can be (i) deployed\nzero-shot in novel, useful settings (e.g., different sensors, robot embodiments, environments etc.),\nand (ii) adapted to a downstream task of choice (e.g., different objectives, goal specification types,\nbehaviors etc.). We specifically consider the problem of visual navigation, where the robot must\nnavigate its environment solely using egocentric visual observations. A general pre-trained robot\nnavigation model should enable a wide range of navigation applications, readily allow fine-tuning\nto downstream tasks, and generalize to a broad range of environments and robotic platforms. Such\na model should provide a broadly capable navigation policy on top of which applications to specific\ndomains can be constructed, giving a base level of generalization and capabilities to new robotic\nplatforms in zero shot that can be further improved after fine-tuning with a small amount of data.\nTo this end, we propose the Visual Navigation Transformer, or ViNT: a cross-embodiment foun-\ndation model for visual navigation with strong zero-shot generalization. We train ViNT to reach\ngoals specified by camera images, providing a very general pre-training objective that can be ap-\nplied to almost any mobile robot dataset. We propose a novel exploration algorithm for the visual\nnavigation paradigm using a diffusion model to propose short-horizon goals, and demonstrate that it\nenables ViNT to navigate in novel environments. ViNT can control new robots in zero-shot, explore\npreviously unseen environments, perform indoor mapping, and navigate kilometer-scale outdoor\nenvironments without interventions. Furthermore, we show that ViNT can be fine-tuned on a small\namount of data to achieve high performance with new task specification modalities \u2013 such as GPS\nwaypoints or high-level routing commands \u2013 allowing ViNT to serve as a foundation for a wide va-\nriety of navigation applications. Lastly, we qualitatively analyze some emergent behaviors exhibited\nby ViNT, such as implicit preferences and navigation around dynamic pedestrians.\nWe hope that ViNT represents a step towards such general-purpose robot foundation models that can\nbe deployed on a wide range of robots, and on a wide range of tasks, and serve as a foundation for\ndiverse mobile robotic applications. Model weights for ViNT as well as training and deployment\ncode will be released on our project page: general-navigation-models.github.io.\n2\nRelated Work\nLearning from large, diverse robotic datasets has been studied for various robotic applications where\ndata sharing across similar robots provides a larger training set for more generalizable models [11\u2013\n13]. However, for applications in mobile robotics, with varying dynamics and camera configurations\n(e.g., focal length, field of view, and extrinsics), current approaches tend to rely on learning either\nfrom small real-world datasets, which are only representative of a single robotic platform, or from\nsimulation, with paired robot and environment models to transfer learned policies [14\u201316]. Instead,\nour paper follows the paradigm of learning navigation behavior from data collected across multiple\ndifferent real-world robotic systems [17\u201319], while focusing on training a foundation model that can\nbe adapted for a variety of downstream tasks in zero shot or with small amounts of data.\nOur goal is to train an effective visual navigation policy that can solve a range of downstream tasks,\nsuch as navigating to GPS goals [20], goal images [21], and skill-conditioned driving [22].Following\na large body of research in visual navigation, we use a combination of topological graphs for main-\ntaining a spatial representation of the environment and learned policies for low-level control [23\u2013\n28], and use learned heuristics to guide the robot in novel environments [15, 29]. But unlike these\nworks, our goal is to train a single generalist model rather than specialist solutions to each of these\nproblems, showing how a single high-capacity model can be adapted for diverse tasks.\nThe closest related works to ViNT are RT-1, I2O, and GNM [15, 19, 30], which study broad gen-\neralization across environments and embodiments for robots deployed in real-world settings. While\nRT-1 demonstrates impressive performance in following diverse instructions, our focus is on adapt-\ning a single model across many robots to solve different tasks, by fine-tuning with small amounts of\ndata. I2O and related efforts [15, 16] show impressive transfer from simulation to real-world envi-\nronments, but we emphasize that our aim is orthogonal to the specific choice of algorithm: we focus\non learning a capable navigation policy that can be efficiently adapted to solve different downstream\ntasks. GNM [19] demonstrates policy learning from heterogeneous RGB datasets, but focuses on the\n2\nEf\ufb01cientNet-B0\nEf\ufb01cientNet-B0\nSelf-Attention\nSelf-Attention\nx6\nObservations \nGoal + Observation\nEarly Fusion\nTemporal Distance\nNormalized Actions\nTransformer\n4 Layers, 4 Heads\n7 Tokens\nx512-D\nPositional Encoding\n5M Parameters\n21M Parameters\n512-D\n512-D\n1-D, Continuous\n3x5-D, Continuous\nPast 5 timesteps\n85x64x3 RGB\n5M Parameters\nActions\nFigure 2: ViNT Model Architecture. ViNT uses two EfficientNet encoders \u03c8, \u03d5 to generate input tokens to a\nTransformer decoder. The resulting sequence is concatenated and passed through a fully-connected network to\npredict (temporal) distance to the goal as well as a sequence of H = 5 future actions.\nsingular task of reaching image goals in the zero-shot setting. Instead, ViNT trains a single generalist\npolicy with an emphasis on adaptation to new embodiments and tasks in downstream applications,\nthough it can also be used zero-shot to great effect (Sec. 6.1).\n3\nThe ViNT Model\nOur model is trained for image-goal navigation, providing general navigational capabilities that can\nthen either be utilized directly, or serve as a pre-trained foundation for downstream fine-tuning with\nother task specifications. In the image-goal navigation task, the robot is tasked with navigating to\na subgoal specified by an image observation s (i.e., the robot\u2019s observation at the goal). Unlike\nalternative mechanisms for goal specification such as PointGoal [31], GPS navigation, or semantic\nobjectives [32], a model can be trained for image-goal navigation with minimal assumptions, uti-\nlizing any data that contains videos and actions, without requirements on ground-truth localization,\nsemantic labels, or other metadata. This makes it practical to train on a large and diverse dataset\nsourced from many different robots, facilitating broad generalization.\nViNT takes as input current and past visual observations ot\u2212P :t and a subgoal image os, and predicts\n(i) the number of time steps needed to reach the subgoal (the dynamical distance), and (ii) a sequence\nwith length H of future actions leading towards the subgoal. Our 31M-parameter model, ViNT, is\nbuilt on the Transformer architecture [33] and is optimized for: (i) fast and efficient inference on\nresource-constrained robots, and (ii) the ability to prompt and fine-tune for downstream tasks. We\ninitialize all networks from scratch and train them end-to-end with the training objective in Eqn. 1.\nThe model architecture is summarized in Figure 2, and described in detail in Appendix A.\nTokenization: The ViNT architecture (Fig. 2) first tokenizes its inputs into an embedding of size\ndmodel = 512. ViNT independently tokenizes current and P = 5 past visual observations by encod-\ning them with an EfficientNet-B0 [34] model, which takes 85 \u00d7 64 \u00d7 3 images as input and outputs\na flattened feature vector \u03c8(oi) from the final convolutional layer [30].\nGoal fusion: We found that na\u00a8\u0131vely extracting features from the goal image \u03d5(os) using an Effi-\ncientNet encoder \u03d5 led to poor performance, often ignoring the goal entirely (see Appendix A). We\nhypothesize that effective features for image-based goal-reaching tasks are often relative, encoding\nthe difference between the current observation and the goal rather than an absolute representation of\nthe goal itself. Hence, we use a separate goal fusion encoder \u03d5(ot, os) to jointly encode the current\nand goal observations. We stack the two images along their channel dimensions, pass them through\na second EfficientNet-B0 encoder, and flatten to obtain the goal token.\nTransformer: The P + 2 observation and goal tokens are combined with a positional encoding and\nfed into a Transformer backbone f. We use a decoder-only Transformer with nL = 4 multi-headed\nattention blocks, each with nH = 4 heads and dFF = 2048 hidden units.\n3\nViNT\nGoal\nFigure 3: Long-horizon navigation in unseen environments with ViNT. We use physical search with a\ntopological graph-based planner to explore the environment. An image-to-image diffusion model proposes\ndiverse exploration targets which are spatially grounded using ViNT (yellow), and scored using a goal-directed\nheuristic h. Subgoals are added to the topological graph M and executed using the ViNT policy.\nTraining objective: During training, we first sample a minibatch of trajectories \u03c4 from the dataset\nD. We then choose P consecutive observations to form the temporal context ot:t\u2212P and randomly\nselect a future observation os := ot+d, with d sampled uniformly from [lmin, lmax], to serve as the\nsubgoal [35]. The corresponding H future actions \u02c6a := at:t+H and the distance d are used as labels\nand trained with a maximum likelihood objective:\nLViNT(\u03d5, \u03c8, f) = E\u03c4EtEd [log p(\u02c6a|f(\u03c8(o)t:t\u2212P , \u03d5(ot, os)) + \u03bb log p(d|f(\u03c8(o)t:t\u2212P , \u03d5(ot, os))]\n(1)\nwhere \u03d5, \u03c8, f are as defined above, and \u03bb balances the two losses.\nEmbodiment-agnostic action space: To effectively train a single model across robots of varying\nsizes, speeds, and dynamics, we follow Shah et al. [19] and choose an embodiment-agnostic action\nspace for ViNT. To abstract away low-level control, ViNT uses relative waypoints as its action space\n\u02c6a; to account for the large variance in speeds and sizes of the robots, we normalize these waypoints\nby scaling them according to the robot\u2019s top speed. During deployment, a robot-specifc controller is\nused to un-normalize and track these waypoints using low-level control.\nTraining data: We train ViNT using a large-scale dataset of heterogeneous navigation trajectories\nfrom a diverse set of environments and robotic platforms with varying dynamics, camera parame-\nters, and behaviors. The training dataset contains over 100 hours of real-world trajectories sourced\nentirely from existing datasets, spanning 8 distinct robotic platforms with varying speeds and dy-\nnamics. For more details about the dataset, see Appendix C.\nDeployment: ViNT can run on any robot equipped with an onboard camera and a low-level velocity\ntracking controller. Given a subgoal image s at time t, we run the model at 4Hz, and use a PD\ncontroller to track the predicted waypoints \u02c6a in a receding-horizon fashion.\n4\nLong-Horizon Navigation with ViNT\nWhile the goal-conditioned policy learned by ViNT captures a general understanding of navigational\naffordances and obstacles, it has limited applicability on its own. Many practical tasks are either not\ndefined by goal images, or require a much longer horizon than what ViNT directly supports. We\napply ViNT to several downstream applications by combining it with an episodic memory formed\nby a topological graph, which provides short-horizon subgoals for reaching faraway locations. In\npreviously unseen environments, we can further augment this graph-based planner with exploratory\nsubgoal proposals, which can drive ViNT to explore a new environment and discover a path to\nthe goal. We consider multiple such proposal mechanisms and find that maximum performance is\nattained by an image diffusion model that samples diverse future subgoal candidates conditioned on\nthe current observation.\nThese subgoals are scored with a goal-directed heuristic to identify the best subgoal that makes\nprogress towards the goal using a process akin to physical A\u2217 search [29]. Past observations in the\nenvironment and unexplored frontiers are stored as nodes in a topological graph, with their connec-\ntivity determined by the distances predicted by ViNT. During exploration, we build this topological\ngraph on the fly as the robot explores the environment. During later deployments it may be used\n4\nfor discovering shortcuts to arbitrary goals in the environment. We first describe the high-level al-\ngorithm that plans on top of subgoal candidates, and then discuss the process for obtaining these\nsubgoal candidates.\n4.1\nHigh-Level Planning and Exploration\nLet\u2019s assume that we have access to subgoal candidates osi \u2208 S available to ViNT for planning. We\nincorporate these subgoal candidates into an exploration framework for goal-directed exploration in\nnovel environments, where the user provides a high-level goal G, which may be arbitrarily far away.\nWe largely follow prior work [29], but swap out the learned models with ViNT and the diffusion\nmodel. We summarize the system here, and provide a more complete discussion in Appendix B.3.\nWe construct a topological graph M online to act as episodic memory, with each node as an in-\ndividual subgoal observation and edges representing paths between two subgoals, added when the\npath is taken by the robot, or the model predicts a subgoal to be reachable from another node. We\nframe goal-directed exploration as a search problem, where the robot incrementally builds M while\nsearching for the goal. To guide search towards the goal, the robot uses a goal-directed heuristic\nh(ot, osi, G, M, C) to score subgoal candidates according to their likelihood of reaching the goal,\ngiven additional context C \u2014 for example, a floor plan or satellite imagery [15, 29]. This heuristic\nmay be geometric (e.g., Euclidean distance) or learned (see Appendix B.3).\nDuring deployment in a new environment, the robot uses the diffusion model to generate subgoal\ncandidates S from ot, spatially grounds them using ViNT, and scores them using the goal-directed\nheuristic h(.). The robot then selects the best subgoal os\u2217 according to this heuristic using an A\u2217-like\nplanner, adds it to M, and drives towards it using ViNT (Figure 3). During subsequent deployments\nin the same environment, the robot can use M to discover shortcuts to arbitrary goals in the environ-\nment. Please see Appendix B.3 for more details about the planner and heuristics. In our experiments,\nwe consider two candidate search heuristics: a geometric heuristic based on positions of the robot\nand the goal, and a learned heuristic based on additional context in the form of a satellite image.\n4.2\nSubgoal Generation with Diffusion\nThe physical search algorithm presented above relies on the ability to propose subgoal candidates\nS that are both diverse and reachable from the current observation of the robot ot. This amounts to\nsampling from a high-dimensional, multimodal distribution of RGB images.1\nTo do so, we train a conditional generative model g(osi|ot) on the ViNT training data. Specifically,\nwe apply an image-to-image diffusion model [38, 39], a generative model class that is well-suited for\nproducing diverse samples over high-dimensional spaces such as RGB images. We train the model\nusing randomly-sampled future observations from trajectories in the ViNT dataset (Appendix B.2),\nand sample K subgoal candidates S = {s1, . . . , sK} from the model at inference time.\nHowever, these subgoal generations are not spatially grounded: they do not include an actionable re-\nlationship to ot. We ground these candidates by using ViNT to compute temporal distances d(si, ot)\nand action rollouts a(si, ot), yielding a set of grounded subgoals as in Fig. 6. While the samples\ngenerated by the diffusion model do not necessarily match any real observation (see Figure 3), they\npreserve sufficient relative features from ot to be plausible, and we find that ViNT generalizes well\nto generated subgoals. We further study the behavior of this diffusion model in Section 6.5.\n1Prior work has also studied sampling subgoals in a latent space [36], which may require simpler density\nmodels to learn the latent distribution. However, directly implementing this idea in our framework leads to\noptimization challenges and poor performance (see Section 6.1). Learning a sampling distribution directly in\nthe latent space [37] is an exciting future direction, but orthogonal to the contributions of this work.\n5\nCoverage\n(384m)\nCoverage\n(142m)\nPosition-Guided \n(100m)\nSatellite-Guided \n(1.24km)\nFigure 5: ViNT accomplishes long-horizon navigation with a variety of objectives in indoor and outdoor\nenvironments; example trajectories between start (orange) and goal (green) visualized here. Goal-reaching\nbehavior can be achieved with a goal-directed heuristic (optionally guided by satellite imagery), while removing\nthis heuristic allows for undirected exploration to maximally cover a workspace.\n5\nViNT: A Foundation Model For Downstream Tasks\nBeyond its core functionality as an image goal-conditioned model, we show that the strong naviga-\ntional priors learned by ViNT can be adapted to a variety of downstream tasks, beyond navigating\nto image goals, by fine-tuning part or all of the model in novel environments or with new modalities\nof data.\nFigure 4: Adapting ViNT\nto different goals using a\nnew tunable goal token.\nFull model fine-tuning: While ViNT demonstrates strong zero-shot\ngeneralization to new environments and robots, we can further improve\non-task performance by fine-tuning the entire model with the same ob-\njective but using on-task data. This allows ViNT to quickly learn new\nskills, forming a continually improving model. ViNT can master new\nenvironments and embodiments with as little as 1 hour of navigation\ndata, transferring the capabilities of the original model to a new setting\nwithout retraining from scratch.\nAdapting to new modalities: While specifying image goals gives a gen-\neral pre-training objective, ViNT can easily be adapted to other common\nforms of goal-specification by learning a \u201csoft prompt\u201d mapping from\nthe desired goal modality to the ViNT goal token [10]. We build on the\nTransformer architecture\u2019s ability to attend to multimodal inputs pro-\njected into a shared token space [40, 41]. Given a subgoal \u03c3 in a new\nmodality (such as 2D coordinates or high-level routing directions [22]),\nwe train a small neural network \u02dc\u03d5 that maps the subgoal to this shared\ntoken space as shown in Figure 4, and replace \u03d5(ot, os). We fine-tune\nViNT with on-task data DF using the modified objective:\nLadapt = E\u03c4\u2208DF EtEd\nh\nlog p(\u02c6a|f(\u03c8(o)t:t\u2212P , \u02dc\u03d5(\u03c3)) + \u03bb log p(d|f(\u03c8(o)t:t\u2212P , \u02dc\u03d5(\u03c3)))\ni\n(2)\nThis allows adaptation to new tasks with minimal data, while still leveraging the performance and\ngeneralization of ViNT. Appendix B.4 includes additional details.\n6\nReal-world Evaluation\nWe deployed our ViNT foundation model on five distinct robotic platforms, including a drone, a\nquadruped, and two other novel robots which are not present in the training data. We designed our\nexperiments to answer the following questions:\nQ1. Can ViNT efficiently explore previously unseen environments and incorporate heuristics?\nQ2. Does ViNT generalize to novel robots, environments, and obstacles?\nQ3. Can ViNT be fine-tuned to improve performance in out-of-distribution environments?\nQ4. Can the ViNT policy be adapted to handle new task specification and modalities?\nPlease see Appendix D for more details on platforms used in the training data and in evaluation. We\ndid not perform any careful system identification or homogenization of sensors across the robots\n6\nIndoor\nOutdoor\nMethod\nSuccess\nSuccess\nEnd-to-End BC\n0.72\n0.44\nEnd-to-End GCG [42]\n\u2014\n0.61\nRECON\n0.19\n0.23\nViNT-R (Random Subgoals)\n0.81\n\u2014\nViNT\n0.94\n1.00\nViNT\nBC\nRECON\nViNT-R\nTable 1: ViNT paired with our physical search algorithm consistently outperforms baselines for the task of\nundirected goal-reaching in indoor and outdoor environments (left). By effectively planning over diffusion\nsubgoal proposals, ViNT is able to find an efficient path to the goal. Other baselines struggle to explore large\nindoor environments, shown by trajectories overlaid on an indoor floor plan (right).\nFigure 6: Visualizing ViNT exploration rollouts in challenging indoor environments using the Vizbot (top)\nand LoCoBot (bottom) robotic platforms. Future action samples \u02c6a obtained by spatially grounding the subgoal\ncandidates are shown in yellow, with the best actions corresponding to the best candidate marked in blue.\nand datasets; all datasets are used as obtained from their original sources, and every robot platform\nhas its own low-level controller and onboard stack.\n6.1\nNavigation Performance\nTowards understanding Q1, we deploy our full graph-based navigation pipeline (Section 4.1) in a\nvariety of challenging indoor and outdoor environments, previously unseen in the training data. We\nevaluate the performance of ViNT on two challenging tasks: (i) coverage exploration, where the\nobjective is maximally explore an environment in search of a goal whose location is unknown, and\n(ii) guided exploration, where the objective is to reach a goal using contextual information such as\nGPS coordinates or a satellite image (see Figure 5 for task examples). We compare ViNT to a variety\nof baselines, including end-to-end policies trained with imitation or RL [15, 42], a prior graph-based\napproach using a VIB for exploration [36], and an ablation of ViNT that samples random images\nfrom the training set to use as goals rather than generating subgoals with a diffusion model. See\nAppendix E.1 for details about the experimental setup.\nFor the coverage exploration task, the agent is placed in an unknown environment and tasked with\nexploring the environment maximally in search of the goal without any additional cues. Table 1\nsummarizes the success rates for this task in indoor and outdoor environments. We find that, while\nend-to-end baselines avoid collisions with their surroundings, they fail to explore new areas and\noften get stuck in small areas of the environment. Graph-based methods avoid this trap by explicitly\nIndoor: Position\nOutdoor: GPS\nOutdoor: Satellite\nMethod\nSuccess\nDistance\nSuccess\nSPL\nDistance\nSuccess\nSPL\nDistance\nViKiNG [29]\n0.60\n56m\n0.64\n0.42\n720m\n0.77\n0.68\n780m\nViNT\n0.90\n91m\n0.95\n0.84\n1270m\n1.00\n0.94\n1040m\nTable 2: ViNT can effectively utilize goal-directed heuristics, such as 2D goal positions and satellite images,\nto explore novel kilometer-scale environments successfully and without interventions.\n7\n(a)\n(b)\n(c)\n(d)\n(e)\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 7: Satellite-guided physical search with ViNT. We visualize a 765m rollout of ViNT with a satellite\nimage-based heuristic from start (orange) to goal (green). The future action samples \u02c6a obtained by spatially\ngrounding the subgoal candidates for five instances in the trajectory are shown in yellow. An A\u2217-like planner\nuses the heuristic to pick the best subgoal (corresponding \u02c6a marked in blue), guiding the robot to the goal.\nreasoning about coverage with the search objective, leading to a high success rate for ViNT. Qualita-\ntive analysis (Table 1-right) shows that planning over the diverse subgoals proposed using diffusion\nleads to more efficient paths, whereas other baselines take winding paths while exploring. Figure 5\nillustrates the egocentric rollouts of the coverage exploration task in challenging indoor environ-\nments. ViNT-R performs respectably despite the lack of valid subgoal proposals. See Section 6.5\nfor a discussion on this observation.\nThis observation extends to the position-guided navigation task (Table 2), where the robots are\ntasked with reaching a 2D goal position in a previously unseen environment. The robots have ac-\ncess to onboard wheel odometry (indoors), GPS coordinates (outdoors), or passive satellite imagery\n(outdoors), to track their position and use as a goal-directed heuristic. Compared to a baseline of the\nprevious state of the art [29], we find that the various sub-goal predictions from the diffusion model\npaired with the graph-based scoring scheme lead to a higher success rate and a greater distance\ntraveled without collisions. ViNT is also more effective at avoiding collisions in crowded indoor\nspaces, and more efficient at reaching goals in outdoor spaces (captured by the SPL metric), owing\nto the implicit affordances and preferences learned by the large-scale pre-training (see further anal-\nysis in Section 6.5. ViNT also requires fewer interventions, observed by the larger distance before\nobserved collisions. Figure 7 illustrates a rollout of physical search in an outdoor environment with\nViNT using satellite image as context (also see Figure 5).\n6.2\nZero-Shot Generalization: a Single Policy to Drive Any Robot\nTowards answering Q2, we deploy the same pre-trained ViNT policy on four distinct robotic plat-\nforms without any fine-tuning for the task of undirected exploration. We report the maximum dis-\nplacement of the robot (in meters) from its starting position, without interventions, as a proxy for\nreaching arbitrarily distant goals in complex environments in Table 3. Most notably, ViNT success-\nfully generalizes zero-shot to control a Go 1 quadruped, which does not appear during training.\nModel\nLoCoBot Go 1 Vizbot Jackal\nSingle-Robot\n40\n12\n40\n184\nGNM [19]\n60\n8\n20\n427\nViNT\n120\n45\n110\n438\nTable 3:\nIn coverage tasks, ViNT drives dif-\nferent robots for 100s of meters (reported maxi-\nmum displacement without intervention), beating\nlower-capacity models (GNM) and specialist mod-\nels trained on a single robot dataset.\nWe compare ViNT trained across all the com-\nbined datasets and robots to the best single-robot\nbaseline \u2014 a model trained using data only from\nthe target environment \u2014 as well as the GNM\nmodel [19] trained on all datasets. We observe that\npolicies trained across robot embodiments can not\nonly match, but also outperform, single-robot mod-\nels across all the embodiments we studied.\nWe\nalso find that the larger capacity of ViNT leads to\nimproved generalization compared to the smaller\nGNM model, especially on robots that do not ap-\npear in the training dataset (e.g., Go 1). Crucially, we also find that ViNT demonstrates positive\ntransfer for in-domain robots (Vizbot), greatly outperforming a specialist model trained on only the\ntarget robot and setting, an emergent phenomenon not present in smaller models. This indicates that\nthe model generalizes between tasks to improve performance, a key property of a foundation model.\n8\nImages\nPositions\nRouting\nMethod\nSuccess\nIn Lane\nSuccess\nSuccess\nScratch\n0.45\n0.74\n0.79\n0.43\nImageNet\n0.22\n0.71\n0.59\n0.45\nSimCLR [7]\n0.21\n0.63\n0.70\n0.64\nVC-1 [44]\n0.19\n0.65\n0.49\n0.38\nGNM [19]\n0.49\n0.66\n0.45\n0.49\nViNT\n0.82\n0.82\n0.89\n0.72\nTable 3: Left: ViNT can be fine-tuned end-to-end (Images) or adapted to downstream tasks (Positions and\nRouting), and outperforms training from scratch and other pre-training methods. Right: ViNT can transfer\nnavigational affordances to novel tasks (40% success without fine-tuning), and efficiently masters the task\n(80% success) with less than 1 hour of fine-tuning data. ViNT fine-tuning (green) outperforms a single-domain\nmodel trained with 5\u00d7 data (orange).\n6.3\nBroader Generalization via Fine-Tuning\nTo answer Q3, we consider the problem of fine-tuning ViNT in the low data regime. In this setting,\nthe entire ViNT model is fine-tuned end-to-end with a reduced learning rate of 1\u00d710\u22124 over nep = 5\nepochs (Section 5). We assume access to a small amount of on-task data (at most 5 hours, with\nsuccessful results in 1-2 hours of data), and study the the efficiency of learning from subsets of this\ndata using ViNT. We study fine-tuning for the task of autonomous driving in the CARLA simulator\nfor two reasons: (i) the simulated CARLA environment is perceptually distinct from the real-world\ndata used to train ViNT (Fig. 8), and (ii) the on-road driving task requires very specific semantic\nbehavior, i.e., driving in a lane and making smooth turns, that is not present in our real-world training\ndata. We show that ViNT can be fine-tuned on a small amount of data (under 1 hour) to achieve\nstrong performance by effectively leveraging the navigational priors encoded in the model.\nFigure 8: The CARLA test environment\n(top), and a bird\u2019s eye view showing high-\nlevel routing commands for the routing task.\nWe compare the ViNT backbone to several alternatives,\nincluding visual representations trained with supervised\nlearning [2], unsupervised objectives [7, 43, 44], and an\nembodiment-agnostic navigation policy [19]. We use the\nsame fine-tuning data and procedure for all models (see\nSection 5); please see Appendix E.3 for more details.\nTable 3 summarizes our findings. We report fractional\nprogress towards the goal as \u201csuccess\u201d, and the fraction\nof trajectories where the agent drives within the driv-\ning lane as \u201cin lane\u201d.\nWhile pre-trained visual repre-\nsentations significantly improve task performance over\na policy trained entirely from scratch, we observe that\nthe learned policies suffer from frequent collisions and\npoor performance. GNM [19] outperforms these base-\nlines due to its strong navigation prior, but the lower-\ncapacity model is unable to generalize fully to the task.\nViNT, on the other hand, is able to achieve strong perfor-\nmance, achieving substantially higher success rate than\nthe next best baseline. Sweeping over fine-tuning dataset\nsize (Table 3-right) shows that ViNT achieves strong per-\nformance with as little as 1 hour of fine-tuning data, demonstrating its ability to generalize to new\nenvironments with very little data.\n6.4\nAdapting ViNT to Downstream Tasks\nTo evaluate Q4, we investigate whether ViNT can serve as a foundation model for a broader range\nof downstream tasks by considering goal modalities beyond subgoal images (see Section 6.4). We\n9\nconsider the same CARLA driving task but with two different high-level planners: (i) a position-\nbased planner that commands a sequence of GPS waypoints, and (ii) a routing planner with similar\nfunctionality to Google Maps that commands high-level navigation directions (left/right/straight) to\nthe policy [22]. We compare the pre-trained navigational priors learned by ViNT to the baselines\ndiscussed earlier, corresponding to pre-trained visual representations and policies, each adapted to\nthe downstream task using the same on-task data (see Appendix E.3 for more details).\nTable 3 summarizes our results for the two tasks. We again find that general pre-trained visual rep-\nresentations, such as ImageNet or VC-1, are not sufficient to extract navigational affordances for\nchallenging downstream tasks, suggesting that effective generalization requires more than general\nvisual representations [44, 45]. We also find that unlike fine-tuning, GNM [19] struggles with the\nadaptation task, suggesting that the architecture and increased capacity of ViNT are essential for\nbroad generalization and adaptation. ViNT achieves strong performance on both tasks, demonstrat-\ning its ability to serve as a foundation model for downstream tasks.\n6.5\nEmergent Behaviors\nOne of the most exciting aspects of large-scale machine learning is the potential for emergent be-\nhavior that arises from the training of large models on diverse datasets. Despite the simple self-\nsupervised training objective used by ViNT (Equation 1), it shows a number of emergent behaviors,\nwhich we describe qualitatively in this section and present as examples on the project page and\nsupplemental videos: general-navigation-models.github.io.\nFigure 9: Samples from the diffusion model may be invalid subgoals,\nbut ViNT is robust to such proposals.\nImplicit\nnavigation\naffor-\ndances: Ideally, we would like\na robot foundation model to\nexhibit some desirable \u201cdefault\u201d\nbehavior,\nwhile\nproviding\na\nmechanism\nfor\ndownstream\napplications to adapt this be-\nhavior as needed. We find that\nViNT has this property vis-a-vis\ncollision-avoidance. One piece\nof evidence is its behavior when provided with random subgoals from locations that are not\nreachable by the robot, studied quantatively via the ViNT-R baseline in Table 1. In this case, despite\nthe subgoals being invalid and out-of-distribution (ViNT was only trained to reach subgoals), ViNT\nsucceeds at exploring the environment and reaches the goal 80% of the time, outperforming all\nbaselines. This suggests that ViNT takes collision-free actions when provided with meaningless\ngoals (i.e. the above \u201cdefault\u201d), while still attempting to follow reachable subgoals.\nFigure 10: ViNT exhibits an implicit preference for fol-\nlowing paved roads (left) and hallways (right).\nIndeed, although the \u201cfull\u201d version of our\nmethod augmented with the diffusion model\nperforms better, the subgoals generated by this\nmodel are often of low quality with many ar-\ntifacts, and sometimes do not match any real\nreachable state (Figure 9).\nNonetheless, be-\ncause of this \u201cdefault\u201d behavior, ViNT is able\nto successfully leverage the valid subgoals,\nwhile ignoring the bad ones, and demonstrate\ncollision-free navigation in previously unseen\nenvironments.\nImplicit navigation preferences: Yet another interesting property exhibited by ViNT is its implicit\npreference to follow paved roads (outdoors), and drive smoothly in the middle of hallways (indoors),\nas demonstrated in Figure 10 and in the supplemental video. This is particularly interesting since\na large chunk of the pre-training dataset contains suboptimal, weavy trajectories, and suggests that\n10\nFigure 11: Robustness to dynamic pedestrians. ViNT can successfully navigate around a crowd of dynamic\npedestrians and reach the goal behind them, despite its simple self-supervised training objective.\nViNT can learn \u201cgood\u201d default behavior from the diverse training behaviors. This preference helps\nViNT efficiently explore previously unseen environments, where other baselines tend to explore the\nenvironment haphazardly (see Table 1 (right)).\nRobustness to dynamic pedestrians: While ViNT is trained only on offline data with a simple, self-\nsupervised training objective, we find that its collision avoidance capabilities generalize to dynamic\nobstacles and pedestrians. Figure 11 exhibits an instance where the robot is tasked with navigating\nto a goal behind two pedestrians. ViNT selects actions that avoid the pedestrians and recovers to the\noriginal path, successfully reaching the goal.\n7\nDiscussion\nWe presented ViNT, a robot foundation model that is trained for a generic image-goal navigation\ntask on diverse data from many different robots, and can then support a variety of different navigation\nfunctionalities. ViNT can be deployed for long-horizon navigation in combination with a topological\ngraph planning method, explore new environments with goals proposed by a diffusion model, be\nfine-tuned to new domains (such as autonomous driving), and be adapted to new task specification\nmethods, such as GPS coordinates or turn-by-turn routing commands. Our results show that ViNT\ncan successfully generalize across robots and environments, outperforms prior navigational models,\ncan be efficiently fine-tuned to new domains and tasks, and shows promising emergent behaviors\nsuch as navigating through dynamic pedestrians.\nLimitations and Future Work\nAs with many large-scale models, ViNT carries a heavier computational burden at inference time,\nwhich can present a challenge for power-constrained platforms such as quadcopters. While our de-\nsign aims to enable efficient inference, our Transformer-based model is still significantly costlier to\nrun at deployment time than simpler feedforward convolutional networks. Additionally, although\nViNT generalizes effectively across robots in our experiments, it assumes a degree of structural\nsimilarity. For example, it cannot control the altitude of a quadcopter or handle other changes in\nthe action representation, nor accommodate new sensors such as LIDAR. Training on a variety of\nmodalities and action spaces in future work could unlock this capability. More broadly, while ViNT\nillustrates the promise of a general-purpose and broadly reusable navigational foundation model, we\nbelieve that the most exciting developments for general-purpose cross-robot models are still ahead:\nas larger and larger multi-robot datasets are assembled, perhaps we will see even broader generaliza-\ntion and more flexible specification with increasingly powerful and general-purpose robotic models.\nWe hope that ViNT represents a step in this direction.\n11\nAcknowledgments\nThis research was partly supported by ARL DCIST CRA W911NF-17-2-0181, ARO W911NF-21-\n1-0097, IIS-2150826, and compute from Google TPU Research Cloud, NVIDIA, NSF CloudBank,\nand the Berkeley Research Computing program. The authors would like to thank Yunhao Cao,\nSeung-Hyun Kwon, Hrish Leen, Laura Smith, Medini Tolpadi, and Christopher Yu, for help with\nsetting up experiments; Ted Xiao, for help with reproducing RT-1; Chethan Bhateja, Devendra Singh\nChaplot, Kuan Fang, Adrien Gaidon, Dibya Ghosh, Saurabh Gupta, Haresh Karnan, Vijay Kumar,\nHrish Leen, Fangchen Liu, Arjun Majumdar, Carlos Nieto, Aravind Rajeswaran, Ethan Stump, Jie\nTan, Joanne Truong, and Tingnan Zhang, for useful discussions and feedback during various stages\nof this research. The authors would also like to thank Byron Boots, Gregory Kahn, Haresh Karnan,\nXiangyun Meng, and Xuesu Xiao, for their help in aggregating the dataset used for training ViNT.\nAuthor Contributions\nDS: Designed and trained the ViNT architecture, investigated goal-fusion challenges, conceptual-\nized prompt-tuning for adaptation, integrated diffusion-based sampling and scoring with ViNT, gath-\nered heterogeneous datasets, implemented baselines, conducted all outdoor experiments on Jackal,\nwrote key sections of the paper, led and coordinated the project.\nAS: Implemented graph-based search, investigated goal-fusion challenges, implemented and trained\ndifferent goal-fusion architectures, integrated diffusion-based sampling and scoring with ViNT, in-\nvestigated emergent behaviors, processed datasets, implemented baselines, conducted indoor exper-\niments on LoCoBot and Go 1.\nND: Conceptualized and implemented prompt-tuning for adaptation, wrote data-collection and la-\nbeling pipeline for CARLA, implemented baselines, conducted all fine-tuning and adaptation exper-\niments in CARLA.\nKS: Investigated emergent behaviors, supported indoor experiments, wrote and edited key sections\nof the paper.\nKB: Conceptualized diffusion-based sampling and scoring, implemented and trained the diffusion\nmodel, helped write related sections of the paper.\nNH: Advised on various aspects of the project, conducted indoor experiments on Vizbot.\nSL: Advised, set direction, wrote and edited key sections of the paper.\nReferences\n[1] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding\nby generative pre-training. 2018. 1\n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierar-\nchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR),\n2009. 1, 9\n[3] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics\ndataset. In Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n[4] K. Grauman et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2022. 1\n[5] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1\n[6] M. Chen et al.\nEvaluating large language models trained on code.\narXiv preprint\narXiv:2107.03374, 2021. 1\n12\n[7] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\nof visual representations. International Conference on Machine Learning (ICML), 2020. 1, 9,\n24\n[8] D. Shah, B. Osi\u00b4nski, B. Ichter, and S. Levine. Robotic navigation with large pre-trained models\nof language, vision, and action. In Conference on Robot Learning (CoRL, 2022.\n[9] X. Liu, Y. Li, C. Liang, and X. Li. The power of scale for parameter-efficient prompt tuning.\nIn Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n[10] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt\ntuning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n1, 6\n[11] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network\npolicies for multi-task and multi-robot transfer. In 2017 International Conference on Robotics\nand Automation (ICRA), 2017. 2\n[12] S. Dasari, F. Ebert, et al. Robonet: Large-scale multi-robot learning. In Conference on Robot\nLearning (CoRL), 2020.\n[13] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell. Bdd100k:\nA diverse driving dataset for heterogeneous multitask learning. In Conference on Computer\nVision and Pattern Recognition (CVPR), June 2020. 2, 22\n[14] P. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee. Sim-\nto-real transfer for vision-and-language navigation. arXiv preprint arXiv:2011.03807, 2020.\n2\n[15] J. Truong, A. Zitkovich, S. Chernova, D. Batra, T. Zhang, J. Tan, and W. Yu. Indoorsim-to-\noutdoorreal: Learning to navigate outdoors without any outdoor experience. arXiv preprint\narXiv:2305.01098, 2023. 2, 5, 7\n[16] A. Kadian, J. Truong, A. Gokaslan, A. Clegg, E. Wijmans, S. Lee, M. Savva, S. Chernova, and\nD. Batra. Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Perfor-\nmance? IEEE Robotics and Automation Letters, 2020. 2\n[17] A. Loquercio, A. I. Maqueda, C. R. del Blanco, and D. Scaramuzza. Dronet: Learning to fly\nby driving. IEEE Robotics and Automation Letters, 2018. 2\n[18] N. Hirose, D. Shah, A. Sridhar, and S. Levine. ExAug: Robot-Conditioned Navigation Poli-\ncies via Geometric Experience Augmentation. In International Conference on Robotics and\nAutomation (ICRA), 2023.\n[19] D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine. GNM: A General Navigation Model\nto Drive Any Robot. In International Conference on Robotics and Automation (ICRA), 2023.\n2, 4, 8, 9, 10, 23, 25\n[20] Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Y. Zhao, E. Wijmans, B. Jain,\nJ. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra. Habitat: A Platform for Embod-\nied AI Research. In International Conference on Computer Vision (ICCV), 2019. 2\n[21] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi. Target-driven\nvisual navigation in indoor scenes using deep reinforcement learning. In International Confer-\nence on Robotics and Automation (ICRA), 2017. 2\n[22] F. Codevilla, M. M\u00a8uller, A. L\u00b4opez, V. Koltun, and A. Dosovitskiy. End-to-End Driving Via\nConditional Imitation Learning.\nIn International Conference on Robotics and Automation\n(ICRA), 2018. 2, 6, 10, 20, 24\n13\n[23] N. Savinov, A. Dosovitskiy, and V. Koltun. Semi-Parametric Topological Memory for Naviga-\ntion. In International Conference on Learning Representations (ICLR), 2018. 2\n[24] J. Bruce, N. Sunderhauf, P. Mirowski, R. Hadsell, and M. Milford. Learning deployable nav-\nigation policies at kilometer scale from a single traversal. In A. Billard, A. Dragan, J. Peters,\nand J. Morimoto, editors, 2nd Conference on Robot Learning, 2018.\n[25] A. Faust, K. Oslund, O. Ramirez, A. Francis, L. Tapia, M. Fiser, and J. Davidson. Prm-rl:\nLong-range robotic navigation tasks by combining reinforcement learning and sampling-based\nplanning. In 2018 International Conference on Robotics and Automation (ICRA), 2018.\n[26] X. Meng, N. Ratliff, Y. Xiang, and D. Fox. Scaling Local Control to Large-Scale Topological\nNavigation. In International Conference on Robotics and Automation (ICRA), 2020.\n[27] D. Shah, B. Eysenbach, G. Kahn, N. Rhinehart, and S. Levine.\nViNG: Learning Open-\nWorld Navigation with Visual Goals. In International Conference on Robotics and Automation\n(ICRA), 2021.\n[28] N. Hirose, F. Xia, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, A. Sadeghian, and S. Savarese. Deep Visual MPC-Policy\nLearning for Navigation. IEEE Robotics and Automation Letters, 2019. 2, 22\n[29] D. Shah and S. Levine. ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic\nHints. In Robotics: Science and Systems (RSS), 2022. 2, 4, 5, 7, 8, 19, 20, 22, 23\n[30] A. Brohan et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint\narXiv:2212.06817, 2023. 2, 3, 17, 18\n[31] P. Anderson et al.\nOn Evaluation of Embodied Navigation Agents.\narXiv preprint\narXiv:1807.06757, 2018. 3\n[32] S. Wani, S. Patel, U. Jain, A. Chang, and M. Savva. Multion: Benchmarking semantic map\nmemory using multi-object navigation. In Neural Information Processing Systems (NeurIPS),\n2020. 3\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u.\nKaiser, and I. Polosukhin.\nAttention is all you need.\nIn I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\n2017.\nURL https://proceedings.neurips.cc/paper_files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. 3\n[34] M. Tan and Q. Le. EfficientNet: Rethinking model scaling for convolutional neural networks.\nIn K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Con-\nference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,\npages 6105\u20136114. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/\ntan19a.html. 3, 17\n[35] K. Hartikainen, X. Geng, T. Haarnoja, and S. Levine. Dynamical Distance Learning for Semi-\nSupervised and Unsupervised Skill Discovery. In International Conference on Learning Rep-\nresentations (ICLR), 2020. 4\n[36] D. Shah, B. Eysenbach, N. Rhinehart, and S. Levine. Rapid exploration for open-world nav-\nigation with latent goal models. In Conference on Robot Learning (CoRL), 2021. 5, 7, 22,\n23\n[37] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 10684\u201310695, June 2022. 5\n14\n[38] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Neural Information\nProcessing Systems, 2020. 5, 18\n[39] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet, and M. Norouzi. Palette:\nImage-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages\n1\u201310, 2022. 5, 18\n[40] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke,\nK. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence. Palm-e: An\nembodied multimodal language model. In arXiv preprint arXiv:2303.03378, 2023. 6\n[41] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu,\nand L. Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint\narXiv:2210.03094, 2023. 6\n[42] G. Kahn, A. Villaflor, B. Ding, P. Abbeel, and S. Levine. Self-Supervised Deep RL with Gen-\neralized Computation Graphs for Robot Navigation. In International Conference on Robotics\nand Automation (ICRA), 2018. 7, 22, 23\n[43] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell.\nReal-world robot\nlearning with masked visual pre-training. In Conference on Robot Learning (CoRL), 2022. 9,\n25\n[44] A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges,\nP. Abbeel, J. Malik, D. Batra, Y. Lin, O. Maksymets, A. Rajeswaran, and F. Meier. Where\nare we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint\narXiv:2303.18240, 2023. 9, 10, 24, 25\n[45] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of\npre-trained vision models for control. arXiv preprint arXiv:2203.03580, 2022. 10\n[46] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations (ICLR), 2019. 19\n[47] J. Song, C. Meng, and S. Ermon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 19\n[48] P. von Platen, S. Patil, A. Lozhkov, P. Cuenca, N. Lambert, K. Rasul, M. Davaadorj, and\nT. Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/\ndiffusers, 2022. 18\n[49] D. Kingma, T. Salimans, B. Poole, and J. Ho. Variational diffusion models. Neural Information\nProcessing Systems (NeurIPS), 2021. 18\n[50] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022. 19\n[51] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-\ntijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Neural Information Processing Systems (NeurIPS), 2022.\n19\n[52] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.\narXiv preprint arXiv:1807.03748, 2018. 20\n[53] H. Karnan et al. Socially CompliAnt Navigation Dataset (SCAND): A Large-Scale Dataset Of\nDemonstrations For Social Navigation. IEEE Robotics and Automation Letters, 2022. 22\n15\n[54] A. Shaban, X. Meng, J. Lee, B. Boots, and D. Fox. Semantic terrain classification for off-road\nautonomous driving. In Conference on Robot Learning (CoRL), 2022. 22\n[55] S. Triest et al. TartanDrive: A Large-Scale Dataset for Learning Off-Road Dynamics Models.\nIn International Conference on Robotics and Automation (ICRA), 2022. 22\n[56] A. Agha et al. Nebula: Quest for robotic autonomy in challenging environments; team costar\nat the darpa subterranean challenge. arXiv preprint arXiv:2103.11470, 2021. 22\n[57] N. Hirose, D. Shah, A. Sridhar, and S. Levine. Sacson: Scalable autonomous control for social\nnavigation. arXiv preprint arXiv:2306.01874, 2023. 22\n[58] T. Niwa, S. Taguchi, and N. Hirose. Spatio-temporal graph localization networks for image-\nbased navigation. In International Conference on Intelligent Robots and Systems (IROS), 2022.\n22\n[59] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00b4ar, and R. Girshick. Masked autoencoders are scalable\nvision learners. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 25\n16\nAppendix\nA\nViNT Model Architecture\nTable 4 shows the ViNT model architecture in detail. We use all 18 layers of the EfficientNet-B0\nconvolutional encoder [34], initialized from scratch, to tokenize the observation and subgoal images\ninto 512-dimensional embeddings each. We utilize an observation encoder to tokenize the past\nand current observation images and a joint observation and goal encoder to tokenize the subgoal\nimage fused with the current observation image channel-wise. For tokenizing the joint observation\nand subgoal token, images ot and os are concatenated along their channel dimension, yielding a\n6 \u00d7 85 \u00d7 64 tensor per training data point.\nLayer\nInput [Dimensions]\nOutput [Dimensions]\nLayer Details\n1\not, os [64, 85, 3]\nIg\nt [64, 85, 6]\nConcatenate observations and goal\n2\nIs\nt [64, 85, 6]\nEs\nt [1, 1000]\nGoal EfficientNet encoder\n3\not:t\u2212P [P+1, 64, 85, 3]\nEt:t\u2212P [P+1, 1000]\nContext EfficientNet encoder\n4\nEs\nt [1, 1000]\nEs\u2032\nt [1, 512]\nGoal embedding compression\n5\nEt:t\u2212P [P+1, 1000]\nE\n\u2032\nt:t\u2212P [P+1, 512]\nContext embedding compression\n6\nE\n\u2032\nt:t\u2212P [P+1, 512], Es\u2032\nt [1, 512]\nS [P+2, 512]\nConcatenate\n7\nS [P+2, 512]\n\u02dcS [1, 32]\nFeed into Transformer f\n8\n\u02dcS [1, 32]\nd\nPredict temporal distance d\n9\n\u02dcS [1, 32]\n\u02c6a, [1, T, 4]\nPredict future actions \u02c6a\nTable 4: Architectural Details of ViNT The inputs to the model are RGB images ot:t\u2212P \u2208 [0, 1]P \u00d73\u00d785\u00d764\nand os \u2208 [0, 1]3\u00d785\u00d764, representing the current, past, and goal images. We seek to predict a H future actions\n\u02c6a and the temporal distance d.\nA.1\nGoal-Conditioning Architectures\nWe considered different mechanisms for conditioning ViNT with features from the subgoal image,\nas illustrated in Figure 12.\n1. Late Fusion: Extract the observation and goal features independently and fuse them in\nthe multi-head attention layers. To achieve this effect, we avoid any channel-wise con-\ncatenation between any of the observation and goal images before inputting them into the\nmodel.\n2. Early Fusion: Jointly extract observation (context) and goal features and fuse the observa-\ntion and goal features before we tokenize them. We achieve this by concatenating the goal\nimage with every observation image along the channel dimension. We remove the goal\ntoken in this setup since information about the goal is already in every observation token.\n3. FiLM (RT-1): Following the FiLM+EfficientNet encoder (Brohan et al. [30]), encode each\nobservation image separately. For conditioning on visual goals, we replace the \u201cUniversal\nSentence Encoder\u201d with an EfficientNet encoder. We remove the goal token in this setup\nsince information about the goal is already in every observation token.\nOur observations are summarized in Table 5. While FiLM works well for language, we found that\ntraining was unstable for image-based navigation tasks. Instead, we directly encode each observa-\ntion independently and pass them to a Transformer. Ideally, the goal would be encoded separately\nand then combined with the observations in the Transformer layers, allowing the entire goal encoder\nto later be swapped out for different goal modalities. Unfortunately, we found that this approach\n(which we term \u201clate fusion\u201d, as the goal and observations are not fused until after encoding them)\nperforms poorly: in image-based navigation, it is the relative features between the observation and\n17\nFigure 12: Different goal-conditioning architectures considered for ViNT.\nMethod\nPerformance\nAdaptation\nLate Fusion\n\u2717\n\u2713\nEarly Fusion\n\u2713\n\u2717\nFiLM (RT-1) [30]\n\u2717\n\u2713\nViNT\n\u2713\n\u2713\nTable 5: Comparing merits (\u2713) and demerits (\u2717) of different goal-conditioning architectures. While \u201cEarly\nFusion\u201d works the best for the core navigation task, it does not support downstream adaptation (Section 5).\n\u201cLate Fusion\u201d is ideal for adaptation, but does not perform well for our tasks. Our goal fusion architecture is\nable to closely match the performance of early fusion, while also supporting adaptation.\ngoal images that are important, rather than absolute goal features. An \u201cearly fusion\u201d architecture\nwould fuse the goal image with all the past and current observation images immediately, which\nallows for learning joint features between the goal image and current state. However, this architec-\nture is inflexible as the observation encoder would have to be learned entirely from scratch when\nadapting to a new goal modality. ViNT avoids this issue by using two distinct types of encoders:\nan observation-only encoder used to tokenize each observation image, and a joint observation and\ngoal encoder that should extract relative goal features. This latter encoder can be replaced to allow\nalternative goal specifications in downstream tasks, as described in Appendix B.4. Specifically, we\nadapt to new tasks by learning the final token conditioned on the new task goal information in place\nof the joint observation/goal encoder.\nB\nImplementation Details\nB.1\nTraining ViNT\nSee Table 6 for a detailed list of hyperparameters for training the ViNT foundation model.2\nB.2\nSubgoal Diffusion\nFor generating subgoals, we use an image-to-image diffusion model. It takes an image ot as input\nand produces samples from g(osi | ot), where osi are candidate subgoal images reachable from ot.\nTo produce training pairs for the diffusion model, we first select ot uniformly at random from the\ntraining data and then select osi to fall between 5 and 20 timesteps in the future from ot.\nFollowing Saharia et al. [39], we implement image conditioning as simple channel-wise concatena-\ntion to the U-Net input. We use the Flax U-Net implementation from the diffusers library [48] with\ntextual cross-attention removed since we do not condition on text inputs.\nWe use the continuous-time diffusion formulation from Kingma et al. [49] with a fixed linear noise\nschedule rather than a learned one. Also unlike Kingma et al. [49], we use the unweighted training\nobjective, called Lsimple in Ho et al. [38, Equation 14] and Kingma et al. [49, Appendix K]. We\n2We used a variety of workstations equipped with different GPU configurations over the course of this\nresearch, including 2\u00d74090, 3\u00d7Titan Xp, 4\u00d7P100, 8\u00d71080Ti, 8\u00d7V100, and 8\u00d7A100. With the model archi-\ntecture fixed, the batch size and training time varies significantly across these devices, and the entry in Table 6\nis representative of our most common training configuration.\n18\nHyperparameter\nValue\nViNT Model\n# Parameters\n31M\nRGB Resolution\n85 \u00d7 64\nEncoder\nEfficientNet-B0\nToken Dimension\n512\nAttn. hidden dim.\n2048\n# Attention Layers nL\n4\n# Attention Heads nH\n4\nTemporal Context P\n5\nPrediction Horizon H\n5\nMLP layers\n(256, 128, 64, 32)\nViNT Training\n# Epochs nep\n30\nBatch Size\n300 2\nLearning Rate\n5 \u00d7 10\u22124\nOptimizer\nAdamW [46]\nWarmup Epochs\n4\nLR Schedule\nCosine\nScheduler Period\n10\nCompute Resources\n8\u00d7V100 2\nTraining Time\n30 hours 2\nFine-tuning LR\n1 \u00d7 10\u22124\nDiffusion Model\n# Parameters\n318M\nResolution\n128\u00d7128\n# Up/Down Blocks\n4\nAttn. Resolutions\n32, 16, 8\nLayers per Block\n2\nAttn. Head Dim\n8\nChannels\n(128, 128, 256, 512, 640)\nDiffusion Type\ncontinuous time\nNoise Schedule\nlinear\nHyperparameter\nValue\nDiffusion Training\nDropout\n0.1\nBatch Size\n128\nOptimizer\nAdamW\nWarmup Steps\n1000\nLearning Rate\n1e-4\nLR Schedule\nCosine\nAdam \u03b21\n0.95\nAdam \u03b22\n0.999\nAdam \u03f5\n1e-8\nWeight Decay\n0.001\nEMA Inv. Gamma\n1.0\nEMA Power\n0.75\nEMA Max Decay\n0.9999\nCFG Mask Proportion\n0.2\nTrain Steps\n250,000\nTraining Time\n30 hours\nCompute Resources\nv4-8 TPU board\nDiffusion Sampling\nSampler\nDDIM [47]\nDDIM \u03b7\n0.0\nSampling Steps\n200\nGuidance Weight\n1.0\nOther\nMaximum distance\n20\nDistance tradeoff \u03bb\n0.01\nTable 6: Hyperparameters for training ViNT and the diffusion model.\nemploy classifier-free guidance [50] and find that it helps produce subgoals with better visual fidelity,\nwhich is consistent with prior work [51].\nB.3\nLong-Horizon Physical Search via Topological Graphs\nAs in Shah and Levine [29], we implement physical search similarly to a standard A\u2217 algorithm, by\nkeeping track of an open set \u2126 of possible unvisited subgoals (generated by our diffusion model)\nand following Alg. 1.\nNodes are visited according to a costing function f(s) that depends on the distance from the current\nstate ot to the parent node s\u2212 (measured along the graph), the predicted distance from s\u2212 to s, and\na heuristic function h (similar to that of A\u2217) providing long-horizon navigation hints:\nf(s) = dM(ot, s\u2212) + dpred(s\u2212, s) + h(s, G, C)\nIn general, the heuristic can be any function providing a notion of distance between a subgoal s and\nthe long-horizon goal G, optionally with some context C. For our experiments, we considered three\nheuristics to demonstrate the flexibility of our approach:\n\u2022 Coverage exploration: We have no long-horizon guidance for coverage exploration, and\nthus, use h(s) = 0.\n\u2022 Position-guided: For long-horizon GPS goals (outdoors) and 2D position goals (indoors),\nwe use Euclidean distance h(s) = \u2225s \u2212 G\u2225.\n19\nFigure 13: Subgoal diffusion model U-Net architecture. Each ResNet consists of 2 residual blocks. Downsam-\npling and upsampling is done with strided convolutions.\nAlgorithm 1: Long-Horizon Navigation via Topological Graph\n1: while goal G not reached do\n2:\ns \u2190 minf(\u2126);\n3:\nP \u2190 ShortestPath(M, ot, s\u2212)\n4:\nfor (s, s\u2032) in P do\n5:\nViNT.GoToGoal(s\u2032);\n6:\nend for\n7:\nViNT.GoToGoal(s)\n8:\not \u2190 Observe();\n9:\nAddNode(M, ot, parent: s\u2212);\n10:\nSample si \u223c g(si|ot);\n11:\nAdd(\u2126, si);\n12: end while\n\u2022 Satellite-guided: In the context-guided experiments, we train a learned heuristic function\nthat uses the satellite image as an input to learn a a heuristic for \u201cgood\u201d subgoals. We train\na convolutional neural network on the overhead image to predict the probability that the\nsubgoal s is included on a trajectory from ot to G, trained using a contrastive objective [52].\nAdditional information can be found in Shah and Levine [29].\nB.4\nFine-tuning ViNT\nIn all CARLA fine-tuning experiments, on-task data was collected using a rule-based oracle agent,\nwith start and end locations sampled randomly up to 900 meters apart. We collect 181 training\ntrajectories (roughly 4 hours) in CARLA\u2019s Town 01 environment, and a further 52 trajectories (1\nhour) in the held-out Town 02 environment. Inspired by Codevilla et al. [22], we further augment\nthis dataset by allowing the rule-based agent to correct its position and re-center to the lane after a\nperturbation.\nImage Fine-tuning:\n\u2022 Architecture: We utilize the exact same architecture as ViNT with no changes.\n\u2022 Training: For fine-tuning the image-goal directed model, we utilize the same training\nprocess for ViNT with a learning rate of 0.0001, AdamW optimizer, but no warmup or\ncosine scheduler. We do not mix any prior data for fine-tuned training.\nGPS-Adaptation:\n20\nFigure 14: Adaptation architectures for ViNT. Left: GPS-adaptation architecture. The local coordinates of the\ngoal are concatenated to the fixed latent z. Right: command-adaptation architecture, using latent zi selected by\ncommand label index i.\n\u2022 Architecture: To adapt to GPS-style goals, we cut off the goal encoder block from ViNT.\nWe then learn a fixed tensor of size 3000 and concatenate it to the GPS-command goal in\nego-centric coordinates. We then pass this into a 2-layer MLP which outputs the prediction\nof the final token for the transformer. The architecture is shown in Figure 14.\n\u2022 Training: During training, instead of randomly sampling future images to serve as goals,\nwe sample goals from future odometry information. Once we have a future goal coordinate\nfor self-supervision, we convert to local coordinates and pass into our architecture, fine-\ntuning with the same objective as ViNT. We use a cosine scheduler with a learning rate\nwarmup to 0.0001 for 4 epochs. We also sample goal points from between 1.25s and 1.75s\nrather than from 0.5s to 2.5s.\nCommand-Adaptation:\n\u2022 Architecture: For discrete command goals, we adopt a similar approach for GPS-style\ngoals. We learn a fixed tensor for each discrete command and use the command index to\nselect the corresponding latent to pass into a 2-layer MLP for predicting the final token. In\nthis way, we learn a dictionary of latents, each corresponding to a distinct command. This\narchitecture is illustrated in Figure 14.\n\u2022 Training: For our experiments, we use \u201cleft\u201d, \u201cright\u201d, and \u201cstraight\u201d as our discrete com-\nmands. We assume training data is not labelled with the discrete command, so we label\ndataset trajectories with the corresponding commands retroactively by sampling a future\nposition (as in GPS-Adaptation) and then selecting a command based on its lateral devia-\ntion. For our experiments we bin samples with lateral coordinate greater than 0.05 as \u201cleft\u201d\nor \u201cright\u201d and label the remaining samples as \u201dstraight\u201d. We again use a cosine scheduler\nwith a learning rate warmup to 0.0001 for 4 epochs.\nC\nTraining Dataset\nThe ViNT training dataset contains over 100 hours of real-world navigation trajectories, sourced\nentirely from existing datasets. The dataset consists of a combination of tele-operated and au-\ntonomous navigation behaviors collected across 8 distinct robotic platforms, including 4 commer-\ncially available platforms (TurtleBot, Clearpath Jackal, Warthog and Spot) and several custom plat-\nforms (Yamaha Viking ATV, RC Car, passenger automobiles). The trajectories contain widely vary-\ning robot dynamics and top speeds, ranging between 0.2 and 10m/s, operating in a diverse set of\nenvironments (e.g., office buildings, hallways, suburban, off-road trails, university campuses, etc.).\nAll data is either publicly available, or collected by other researchers for past projects; no additional\ntraining data was collected specifically for training ViNT.\n21\nDataset\nPlatform\nSpeed\nTotal Hrs.\nHrs. Used\nEnvironment\n1\nGoStanford [28]\nTurtleBot2\n0.5m/s\n17h\n14h\noffice\n2\nRECON [36]\nJackal\n1m/s\n25h\n25h\noff-road\n3\nCoryHall [42]\nRC Car\n1.2m/s\n2h\n2h\nhallways\n4\nBerkeley [29]\nJackal\n2m/s\n4h\n4h\nsuburban\n5\nSCAND-S [53]\nSpot\n1.5m/s\n8h\n4h\nsidewalks\n6\nSCAND-J [53]\nJackal\n2m/s\n1h\n1h\nsidewalks\n7\nSeattle [54]\nWarthog\n5m/s\n1h\n1h\noff-road\n8\nTartanDrive [55]\nATV\n10m/s\n7h\n5h\noff-road\n9\nNeBula [56]\nATV\n10m/s\n10h\n10h\noff-road\n10\nSACSoN [57]\nTurtleBot2\n0.5m/s\n75h\n10h\noffice\n11\nBDD [13]\nCar(s)\n20m/s\n10h\n4h\non-road\nTotal\n160h\n80h\nTable 7: The ViNT training dataset contains over 150 hours of navigation data in challenging indoor, outdoor,\nand off-road environments across 8 different robots of varying sizes, speeds, and capabilities.\nRemember to mention: total size, number of robots, conversion to number of frames and so on.\nD\nRobotic Platforms for Evaluating ViNT\nVizbot: A custom-built robot platform inspired by the design of Niwa et al. [58], based on a\nRoomba. It is equipped with an off-the-shelf PCB-mounted fisheye camera.\nUnitree Go 1: A commercially available quadruped robot equipped with the original forward-facing\ncamera. There is no training data from a Go 1 in the training dataset. Athough SCAND includes\ndata collected on a Boston Dynamics Spot, which is also a quadrupedal robot, the two platforms\npractically have very different characteristics.\nClearpath Jackal UGV: A commercially available off-road platform equipped with an off-the-\nshelf PCB-mounted fisheye camera. This system resembles the data collection platform used for the\nRECON, Berkeley, and SCAND-J datasets, but has a different camera and mounting height.\nLoCoBot: A popular open-source platform based on a Kobuki equipped with an off-the-shelf PCB-\nmounted fisheye camera. This robot is not present in the training dataset, although GS was collected\non a similar TurtleBot2 with a different spherical camera at a lower height.\nE\nEvaluation Setup and Details\nE.1\nNavigation Performance\nE.1.1\nIndoor Experiments\nFor setting up the indoor coverage exploration experiments, we use the LoCoBot and Vizbot robotic\nplatforms. We choose a random starting point and goal in an enclosed environment, and keep these\nlocations consistent across all baselines we test. For the coverage exploration task, we ensure that\nthe environments are \u201cenclosed\u201d and block any glass walls and stairwells, which are beyond the\ncapabilities of the robots. Experiments are terminated when either (i) the robot is unable to reach\nthe goal within a pre-specified time limit of 10 minutes, or (ii) the robot becomes physically stuck\n(e.g., collides and is unable to recover).\nFor setting up the indoor guidance exploration experiments on the LoCoBot, we mark the start and\ngoal locations in a large office building and note their 2D positions. The goal location is conveyed\nto the robot as the context, and is available to the search algorithm. The system uses the robot\u2019s\nonboard wheel odometry to track position.\n22\nE.1.2\nOutdoor Experiments\nFor the coverage exploration experiments, we follow the setup of Shah et al. [36] and use the\nClearpath Jackal UGV. We choose a random start and goal location in confined outdoor environ-\nments and obtain a goal image observation for the robot to seek. Experiments are terminated either\nwhen (i) the robot is unable to reach the goal within a pre-specified time limit of 20 minutes, or (ii)\nthe robot collides with an obstacle in the environment.\nFor the guided exploration experiments, we closely follow the setup of Shah and Levine [29]. For\nthe GPS guided experiments, the robot has access to the GPS location of the goal, in addition to a\ngoal image. For the satellite-guided experments, the robot further has access to an overhead satellite\nimage centered at its current location and a learned heuristic funtion h.\nE.1.3\nBaselines\nFor experiments presented in Section 6.1, we evaluate 4 baselines against our method.\n1. End-to-End BC: A modified ViNT model with no goal token, trained end-to-end for the\ntask of only predicting future actions. This represents a typical undirected BC baseline with\nsimilar model capacity as the other baselines.\n2. End-to-End GCG: A model-based algorithm that uses a predictive model to plan a se-\nquence of actions that reach the goal without causing collisions [42]. Since this model\nrequires collision labels for training, it is only trained on a subset of the training data (RE-\nCON, CoryHall, Berkeley) that has these labels; hence, this baseline is only evaluated\noutdoors.\n3. RECON: A variant of the physical search algorithm RECON [36], which uses a latent goal\nmodel to represent reachable goals and plans over sampled subgoals to explore the environ-\nment in a similar manner to ours. This baseline uses a variational information bottleneck\nto sample latent subgoals, rather than a diffusion model sampling subgoal images.\n4. ViNT-R: An ablation of our method that uses subgoals randomly sampled from the training\ndata, instead of samples from a conditional diffusion model, as subgoal candidates.\nE.2\nMulti-robot Generalization Experiments\nThe setup for the multi-robot generalization experiment is same as the coverage exploration experi-\nments. The only differences are the baselines we evaluate.\nE.2.1\nBaselines\nFor experiments presented in Section 6.2, we test three baseline low-level policies on each robot.\nEach baseline uses the graph-based exploration scheme described in Section 4.1. We use the follow-\ning baselines:\n1. Single-Robot: We train a single-dataset policy model (ViNT architecture) and diffusion\nmodel on the two largest datasets (RECON for outdoor, and SACSoN for indoor), and\nevaluate them on each of our robots to identify the best single-dataset model for each robot.\nNote that we do not have comparable magnitudes of training data of visual locomotion on\nthe Go 1.\n2. GNM: We use the pre-trained model checkpoint from the authors of GNM [19] coupled\nwith our diffusion model (since GNM is not compatible with the exploration task) to eval-\nuate each robot.\n3. ViNT: We use our pre-trained ViNT policy and image diffusion model (no fine-tuning) to\nevaluate each robot.\n23\nE.3\nFine-tuning and Adaptation\nThis section describes the setup and implementation details for ViNT fine-tuning and adaptation\nexperiments in the CARLA autonomous driving simulator, as presented in Sections 6.3 and 6.4.\nE.3.1\nCARLA Data Collection\nWe collect expert trajectories with an oracle rule-based self-driving agent and gather odometry and\nRGB information across the trajectory at 4 Hz. These trajectories have random spawn points and\nrandom destination points up to to 900 meters in length. We collect 52 trajectories in the CARLA\nTown 02 for held-out testing, and collect 181 trajectories in Town 01 for training. This makes\nfor a dataset size of 5 hours for the autopilot control data. Inspired by [22], we also collect short\ntrajectories of the agent correcting back onto the right lane after drifting off course in Town 01\nand Town 02 for training and testing, respectively. This data is 4 hours long, and we add it to the\nautopilot data for training.\nE.3.2\nFine-tuning Experiments\nTo test the fine-tuning system which trains ViNT with the same goal specification but in new do-\nmains, we utilize the collected test trajectories as sequences of goal images to follow. Each Town 02\ntest trajectory creates a graph in which every node is a timestamped odometry point corresponding\nto an image. To evaluate a model on a test trajectory, we spawn it at the same start point and localize\nit on the trajectory\u2019s map. We then query the image for the goal node which corresponds to node\n1.5s after the current node. This goal image is sent to ViNT along with the 4Hz image context to\ncompute a short-range trajectory. This is tracked by a simple PID controller. The average progress\ntowards the goal before collision is collected and reported across all trials. Table 3 summarizes the\nresults of these experiments with multiple baselines and data sizes.\nE.3.3\nAdaptation Experiments\nTo test the new tasks, we adopt a similar evaluation setup to the fine-tuning experiments, but rely on\nthe odometry position for the selected goal node rather than the image. For positional-adaptation,\nwe move the goal coordinates into a local frame and send it to ViNT. For routing-adaptation, we de-\ntermine the difference in lateral coordinates between the current node and the goal node. We choose\nthe current node as reference to ensure an open-loop experiment and to allow for pre-computation\nof the command signals to be sent. We then apply the same binning strategy during training using a\n0.05 normalized distance as the boundary between \u201cleft\u201d, \u201cright\u201d, and \u201cstraight\u201d. The control system\ndownstream of this is identical to image fine-tuning and the experiment terminates when at the goal\nor when colliding. The progress towards the goal before collision is collected and averaged across\nall trials in Table 3.\nE.3.4\nBaselines\nWe have the following baselines for the CARLA experiments:\n1. Scratch: ViNT trained from scratch on the CARLA on-task dataset.\n2. Pre-trained Visual Representations\n(a) ImageNet: ViNT initialized with the EfficientNet-B0 weights pre-trained on Ima-\ngeNet, other parameters initialized from scratch, and fine-tuned with the CARLA on-\ntask dataset.\n(b) SimCLR: ViNT initialized with the EfficientNet-B0 weights pre-trained with Sim-\nCLR [7] on the training data described in Section C, other parameters initialized from\nscratch, and fine-tuned with the CARLA on-task dataset.\n(c) VC-1: ViNT initialized with a pre-trained ViT-B model checkpoint from the authors\nof VC-1 [44] and frozen, other parameters initialized from scratch, and fine-tuned\n24\nwith the CARLA on-task dataset. The VC-1 encoder is pre-trained on a combina-\ntion of Ego4D, manipulation, navigation, and ImageNet images using Masked Auto-\nEncoding [43, 59].\n3. GNM: The pre-trained embodiment-agnostic model checkpoint from the authors of\nGNM [19], fine-tuned with the CARLA on-task dataset. Note that GNM has 8.7M trainable\nparameters, compared to ViNT\u2019s 31M.\nMethod\nImages\nPositions\nVC-1 [44]\n0.19\n0.49\nViNT-FE\n0.32\n0.78\nViNT\n0.82\n0.89\nTable 8: Evaluation of ViNT fine-tuning with and with-\nout a frozen encoder, as compared to a general-purpose\nvisual encoder. Even when frozen, ViNT\u2019s navigation-\nrelevant features appear to transfer more readily to out-\nof-distribution inputs than general-purpose features.\nWe note that the VC-1 baseline\u2019s weak per-\nformance in Section 6.4 may be explained by\nthe fact that it is frozen, while all other visual\nencoders were free to fine-tune. This is rep-\nresentative of typical downstream usage [44].\nDespite training on multiple, diverse datasets,\nthe visual representation\u2019s general-purpose fea-\ntures are not optimized for the navigation task,\nhampering zero-shot transfer to out-of-domain\ntasks. To provide a fair comparison of the qual-\nity of pre-trained visual features, we compare\nthis performance to ViNT-FE (a pre-trained\nViNT model that has it\u2019s visual encoder frozen). ViNT-FE has an equal number of trainable pa-\nrameters to the VC-1 baseline, and frozen visual representations (see Table 8).\n25\n"
  },
  {
    "title": "Zero-shot spatial layout conditioning for text-to-image diffusion models",
    "link": "https://arxiv.org/pdf/2306.13754.pdf",
    "upvote": "4",
    "text": "Zero-shot spatial layout conditioning for text-to-image diffusion models\nGuillaume Couairon\u2217\nMeta AI, Sorbonne Universit\u00b4e\nMarl`ene Careil\u2217\nMeta AI\nLTCI, T\u00b4el\u00b4ecom Paris, IP Paris\nMatthieu Cord\nSorbonne Universit\u00b4e, Valeo.ai\nSt\u00b4ephane Lathuili`ere\nLTCI, T\u00b4el\u00b4ecom Paris, IP Paris\nJakob Verbeek\nMeta AI\nAbstract\nLarge-scale text-to-image diffusion models have signif-\nicantly improved the state of the art in generative image\nmodeling and allow for an intuitive and powerful user inter-\nface to drive the image generation process. Expressing spa-\ntial constraints, e.g. to position specific objects in particular\nlocations, is cumbersome using text; and current text-based\nimage generation models are not able to accurately follow\nsuch instructions. In this paper we consider image gener-\nation from text associated with segments on the image can-\nvas, which combines an intuitive natural language interface\nwith precise spatial control over the generated content. We\npropose ZestGuide, a \u201czero-shot\u201d segmentation guidance\napproach that can be plugged into pre-trained text-to-image\ndiffusion models, and does not require any additional train-\ning. It leverages implicit segmentation maps that can be ex-\ntracted from cross-attention layers, and uses them to align\nthe generation with input masks. Our experimental results\ncombine high image quality with accurate alignment of gen-\nerated content with input segmentations, and improve over\nprior work both quantitatively and qualitatively, including\nmethods that require training on images with corresponding\nsegmentations. Compared to Paint with Words, the previous\nstate-of-the art in image generation with zero-shot segmen-\ntation conditioning, we improve by 5 to 10 mIoU points on\nthe COCO dataset with similar FID scores.\n1. Introduction\nThe ability of diffusion models to generate high-quality\nimages has garnered widespread attention from the research\ncommunity as well as the general public. Text-to-image\nmodels, in particular, have demonstrated astonishing capa-\n*These authors contributed equally to this work.\nA realistic photograph of a piece\nof cake with a glass and a lemon\nin a natural landscape\nSegmentation\nConsistency\nGuidance\nDiffusion\nModel\nZero-shot\nSegmentation\nFigure 1. In ZestGuide the image generation is guided by the gra-\ndient of a loss computed between the input segmentation and a\nsegmentation recovered from attention in a text-to-image diffusion\nmodel. The approach does not require any additional training of\nthe pretrained text-to-image diffusion model to solve this task.\nbilities when trained on vast web-scale datasets [15, 34, 36,\n38]. This has led to the development of numerous image\nediting tools that facilitate content creation and aid creative\nmedia design [16, 24, 37]. Textual description is an intuitive\nand powerful manner to condition image generation. With\na simple text prompt, even non-expert users can accurately\ndescribe their desired image and easily obtain correspond-\ning results. A single text prompt can effectively convey in-\nformation about the objects in the scene, their interactions,\nand the overall style of the image. Despite their versatility,\ntext prompts may not be the optimal choice for achieving\nfine-grained spatial control. Accurately describing the pose,\nposition, and shape of each object in a complex scene with\nwords can be a cumbersome task. Moreover, recent works\nhave shown the limitation of diffusion models to follow spa-\ntial guidance expressed in natural language [1, 28].\nOn the contrary, semantic image synthesis is a condi-\ntional image generation task that allows for detailed spatial\ncontrol, by providing a semantic map to indicate the desired\nclass label for each pixel. Both adversarial [29, 39] and\ndiffusion-based [44, 45] approaches have been explored to\narXiv:2306.13754v1  [cs.CV]  23 Jun 2023\n\u201cA cat wearing\n\u201c A dog looking\n\u201cAstronauts on the\na dress.\u201d\nat the sunrise\nstreet with rainbow\nbehind the fuji.\u201d\nin outer space \u201d\nFigure 2. ZestGuide generates images conditioned on segmenta-\ntion maps with corresponding free-form textual descriptions.\ngenerate high-quality and diverse images. However, these\napproaches rely heavily on large datasets with tens to hun-\ndreds of thousands of images annotated with pixel-precise\nlabel maps, which are expensive to acquire and inherently\nlimited in the number of class labels.\nAddressing this issue, Balaji et al. [2] showed that se-\nmantic image synthesis can be achieved using a pretrained\ntext-to-image diffusion model in a zero-shot manner. Their\ntraining-free approach modifies the attention maps in the\ncross-attention layers of the diffusion model, allowing both\nspatial control and natural language conditioning. Users can\ninput a text prompt along with a segmentation map that indi-\ncates the spatial location corresponding to parts of the cap-\ntion. Despite their remarkable quality, the generated images\ntend to only roughly align with the input segmentation map.\nTo overcome this limitation, we propose a novel ap-\nproach called ZestGuide, short for ZEro-shot SegmenTation\nGUIDancE, which empowers a pretrained text-to-image\ndiffusion model to enable image generation conditioned on\nsegmentation maps with corresponding free-form textual\ndescriptions, see examples presented in Fig. 2. ZestGuide\nis designed to produce images which more accurately ad-\nhere to the conditioning semantic map. Our zero-shot ap-\nproach builds upon classifier-guidance techniques that al-\nlow for conditional generation from a pretrained uncondi-\ntional diffusion model [12]. These techniques utilize an ex-\nternal classifier to steer the iterative denoising process of\ndiffusion models toward the generation of an image cor-\nresponding to the condition. While these approaches have\nbeen successfully applied to various forms of conditioning,\nsuch as class labels [12] and semantic maps [3], they still\nrely on pretrained recognition models. In the case of seman-\ntic image synthesis, this means that an image-segmentation\nnetwork must be trained, which (i) violates our zero-shot\nobjective, and (ii) allows each segment only to be condi-\ntioned on a single class label. To circumvent the need for\nan external classifier, our approach takes advantage of the\nspatial information embedded in the cross-attention layers\nof the diffusion model to achieve zero-shot image segmen-\ntation. Guidance is then achieved by comparing a segmen-\ntation extracted from the attention layers with the condition-\ning map, eliminating the need for an external segmentation\nnetwork. In particular, ZestGuide computes a loss between\nthe inferred segmentation and the input segmentation, and\nuses the gradient of this loss to guide the noise estimation\nprocess, allowing conditioning on free-form text rather than\njust class labels. Our approach does not require any training\nor fine-tuning on top of the text-to-image model.\nWe conduct extensive experiments and compare our\nZestGuide to various approaches introduced in the recent\nliterature. Our results demonstrate state-of-the-art perfor-\nmance, improving both quantitatively and qualitatively over\nprior approaches. Compared to Paint with Words, the previ-\nous state-of-the art in image generation with zero-shot seg-\nmentation conditioning, we improve by 5 to 10 mIoU points\non the COCO dataset with similar FID scores.\nIn summary, our contributions are the following:\n\u2022 We introduce ZestGuide, a zero-shot method for im-\nage generation from segments with text, designed to\nachieve high accuracy with respect to the condition-\ning map. We employ the attention maps of the cross-\nattention layer to perform zero-shot segmentation al-\nlowing classifier-guidance without the use of an exter-\nnal classifier.\n\u2022 We obtain excellent experimental results, improving\nover existing both zero-shot and training-based ap-\nproaches both quantitatively and qualitatively.\n2. Related work\nSpatially conditioned generative image models. Follow-\ning seminal works on image-to-image translation [19], spa-\ntially constrained image generation has been extensively\nstudied. In particular, the task of semantic image synthesis\nconsists in generating images conditioned on masks where\neach pixel is annotated with a class label. Until recently,\nGAN-based approaches were prominent with methods such\nas SPADE [29], and OASIS [39]. Alternatively, autoregres-\nsive transformer models over discrete VQ-VAE [27] repre-\nsentations to synthesize images from text and semantic seg-\nmentation maps have been considered [13, 15, 35], as well\nas non-autoregressive models with faster sampling [6, 20].\nDiffusion models have recently emerged as a very pow-\nerful class of generative image models, and have also\nbeen explored for semantic image synthesis. For example,\nPITI [44] finetunes GLIDE [26], a large pretrained text-\nto-image generative model, by replacing its text encoder\nwith an encoder of semantic segmentation maps. SDM [45]\ntrains a diffusion model using SPADE blocks to condition\nthe denoising U-Net on the input segmentation.\nThe iterative nature of the decoding process in diffu-\nsion models, allows so called \u201cguidance\u201d techniques to\nstrengthen the input conditioning during the decoding pro-\ncess. For example, classifier guidance [12] has been used\nfor class-conditional image generation by applying a pre-\ntrained classifier on the partially decoded image, and using\nthe gradient of the classifier to guide the generation process\nto output an image of the desired class. It has since been\nextended to take as input other constraints such as for the\ntasks of inpainting, colorization, and super-resolution [37].\nFor semantic image synthesis, the gradient of a pretrained\nsemantic segmentation network can be used as guidance [3].\nThis approach, however, suffers from two drawbacks. First,\nonly the classes recognized by the segmentation model can\nbe used to constrain the image generation, although this can\nto some extent be alleviated using an open-vocabulary seg-\nmentation model like CLIPSeg [22]. The second drawback\nis that this approach requires a full forwards-backwards\npass through the external segmentation network in order to\nobtain the gradient at each step of the diffusion process,\nwhich requires additional memory and compute on top of\nthe diffusion model itself.\nWhile there is a vast literature on semantic image syn-\nthesis, it is more limited when it comes to the more gen-\neral task of synthesizing images conditioned on masks with\nfree-form textual descriptions. SpaText [1] finetunes a large\npretrained text-to-image diffusion model with an additional\ninput of segments annotated with free-form texts. This rep-\nresentation is extracted from a pretrained multi-modal CLIP\nencoder [32]: using visual embeddings during training, and\nswapping to textual embeddings during inference.\nGLI-\nGEN [21] adds trainable layers on top of a pretrained dif-\nfusion models to extend conditioning from text to bound-\ning boxes and pose. These layers take the form of addi-\ntional attention layers that incorporate the local information.\nT2I [25] and ControlNet [46] propose to extend a pretrained\nand frozen diffusion model with small adapters for task-\nspecific spatial control using pose, sketches, or segmenta-\ntion maps. All these methods require to be trained on a\nlarge dataset with segmentation annotations, which is com-\nputationally costly and requires specialized training data.\nTrain-free adaptation of text-to-image diffusion models.\nSeveral recent studies [7, 14, 16, 30] found that the position-\ning content in generated images from large text-to-image\ndiffusion models correlates with the cross-attention maps,\nwhich diffusion models use to condition the denoising pro-\ncess on the conditioning text. This correlation can be lever-\naged to adapt text-to-image diffusion at inference time for\nvarious downstream applications. For example, [7, 14] aim\nto achieve better image composition and attribute binding.\nFeng et al. [14] design a pipeline to associate attributes to\nobjects and incorporate this linguistic structure by modify-\ning values in cross-attention maps. Chefer et al. [7] guide\nthe generation process with gradients from a loss aiming at\nstrengthening attention maps activations of ignored objects.\nZero-shot\nimage\nediting\nwas\nexplored\nin\nseveral\nworks [11, 16, 24, 30].\nSDEdit [24] consists in adding\nnoise to an input image, and denoising it to project it to\nthe manifold of natural images.\nIt is mostly applied on\ntransforming sketches into natural images. Different from\nSDEdit, in which there is no constraint on which part of\nthe image to modify, DiffEdit [11] proposes a method to\nautomatically find masks corresponding to where images\nshould be edited for a given prompt modification. Prompt-\nto-Prompt [16] and pix2pix-zero [30] act on cross-attention\nlayers by manipulating attention layers and imposing a\nstruture-preserving loss on the attention maps, respectively.\nCloser to our work, eDiff-I [2] proposes a procedure to\nsynthesize images from segmentation maps with local free-\nform texts. They do so by rescaling attention maps at lo-\ncations specified by the input semantic masks, similarly\nto [23] for controlling the position of objects. MultiDif-\nfusion [4] fuses multiple generation processes constrained\nby shared parameters from a pretrained diffusion model\nby solving an optimization problem, and applying it to\npanorama generation and spatial image guidance. In [3] a\npretrained segmentation net guides image generation to re-\nspect a segmentation map during the denoising process.\n3. Method\nIn this section, we provide a concise introduction of dif-\nfusion models in Sec. 3.1 before presenting our novel ap-\nproach, ZestGuide, which extends pretrained text-to-image\ndiffusion models to enable conditional generation of images\nbased on segmentation maps and associated text without\nrequiring additional training, as described in Sec. 3.2. In\nFig. 3 we provide an overview of ZestGuide.\n3.1. Preliminaries\nDiffusion models.\nDiffusion models [18] approximate a\ndata distribution by gradually denoising a random variable\ndrawn from a unit Gaussian prior. The denoising function is\ntrained to invert a diffusion process, which maps sample x0\nfrom the data distribution to the prior by sequentially adding\na small Gaussian noise for a large number of timesteps T.\nIn practice, a noise estimator neural network \u03f5\u03b8(xt, t) is\ntrained to denoise inputs xt = \u221a\u03b1tx0 + \u221a1 \u2212 \u03b1t\u03f5, which\nare data points x0 corrupted with Gaussian noise \u03f5 where\n\u03b1t controls the level of noise, from \u03b10 = 1 (no noise) to\n\u03b1T \u2243 0 (pure noise). Given the trained noise estimator,\nsamples from the model can be drawn by sampling Gaus-\nsian noise xT \u223c N(0, I), and iteratively applying the de-\nnoising Diffusion Implicit Models (DDIM) equation [41].\nQ\nKV\nQ\nKV\nQ\nKV\nQ\nKV +\nLoop\nGuidance\nZero-shot Segmentation\nDiffusion\nA grey cat lying on a desk\nInputs\nOutput\nBackprop\nFigure 3.\nZestGuide extracts segmentation maps from text-\nattention layers in pretrained diffusion models, and uses them to\nalign the generation with input masks via gradient-based guidance.\nRather than applying diffusion models directly in pixel\nspace, it is more efficient to apply them in the latent space\nof a learned autoencoder [36].\nText-conditional generation can be achieved by provid-\ning an encoding \u03c1(y) of the text y as additional input to the\nnoise estimator \u03f5\u03b8(xt, t, \u03c1(y)) during training. The noise es-\ntimator \u03f5\u03b8 is commonly implemented using the U-Net archi-\ntecture, and the text encoding takes the form of a sequence\nof token embeddings obtained using a transformer model.\nThis sequence is usually processed with cross-attention lay-\ners in the U-Net, where keys and values are estimated from\nthe text embedding.\nClassifier guidance. Classifier guidance is a technique for\nconditional sampling of diffusion models [40, 42]. Given\na label c of an image x0, samples from the posterior dis-\ntribution p(x0|c) can be obtained by sampling each transi-\ntion in the generative process according to p(xt|xt+1, c) \u221d\np(xt|xt+1)p(c|xt) instead of p(xt|xt+1).\nDhariwal and\nNichol [12] show that DDIM sampling can be extended to\nsample the posterior distribution, with the following modi-\nfication for the noise estimator \u03f5\u03b8:\n\u02dc\u03f5\u03b8(xt, t, \u03c1(y)) = \u03f5\u03b8(xt, t, \u03c1(y))\n\u2212\n\u221a\n1 \u2212 \u03b1t\u2207xtp(c|xt).\n(1)\nClassifier guidance can be straightforwardly adapted to gen-\nerate images conditioned on semantic segmentation maps\nby replacing the classifier by a segmentation network which\noutputs a label distribution for each pixel in the input image.\nHowever this approach suffers from several weaknesses: (i)\nGenerated\nU-Net input\nLion attn.\nBook attn.\nLion\nBook\nFigure 4. Top, from left to right: image generated from the prompt\n\u201cA lion reading a book.\u201d, the noisy input to the U-Net at t = 20,\ncross-attention averaged over different heads and U-Net layers for\n\u201cLion\u201d and \u201cBook\u201d. Bottom: individual attention heads.\nit requires to train an external segmentation model; (ii) se-\nmantic synthesis is bounded to the set of classes modeled\nby the segmentation model; (iii) it is computationally ex-\npensive since it implies back-propagation through both the\nlatent space decoder and the segmentation network at ev-\nery denoising step. To address these issues, we propose to\nemploy the cross-attention maps computed in the denois-\ning model \u03f5\u03b8 of text-to-image diffusion models to achieve\nzero-shot segmentation. This has two major advantages:\nfirst, there is no need to decode the image at each denois-\ning step; second, our zero-shot segmentation process is ex-\ntremely lightweight, so the additional computational cost\nalmost entirely comes from backpropagation through the U-\nNet, which is a relatively low-cost method for incorporating\nclassifier guidance.\n3.2. Zero-shot segmentation with attention\nTo condition the image generation, we consider a text\nprompt of length N denoted as T = {T1, . . . , TN}, and\na set of K binary segmentation maps S = {S1, . . . , SK}.\nEach segment Si is associated with a subset Ti \u2282 T .\nAttention map extraction.\nWe leverage cross-attention\nlayers of the diffusion U-Net to segment the image as it\nis generated. The attention maps are computed indepen-\ndently for every layer and head in the U-Net. For layer l, the\nqueries Ql are computed from local image features using a\nlinear projection layer. Similarly, the keys Kl are computed\nfrom the word descriptors T with another layer-specific lin-\near projection. The cross-attention from image features to\ntext tokens, is computed as\nAl = Softmax\n\u0012QlKT\nl\n\u221a\nd\n\u0013\n,\n(2)\nwhere the query/key dimension d is used to normalize the\nsoftmax energies [43]. Let An\nl = Al[n] denote the attention\nof image features w.r.t. specific text token Tn \u2208 T in layer\nl of the U-Net. To simplify notation, we use l to index over\nboth the layers of the U-Net as well as the different attention\nheads in each layer. In practice, we find that the attention\nmaps provide meaningful localisation information, but only\nwhen they are averaged across different attention heads and\nfeature layers. See Fig. 4 for an illustration.\nSince the attention maps have varying resolutions de-\npending on the layer, we upsample them to the highest res-\nolution. Then, for each segment we compute an attention\nmap Si by averaging attention maps across layers and text\ntokens associated with the segment:\n\u02c6Si = 1\nL\nL\nX\nl=1\nN\nX\nj=1\nJTj \u2208 TiK Al\nj,\n(3)\nwhere J\u00b7K is the Iverson bracket notation which is one if the\nargument is true and zero otherwise.\nSpatial self-guidance. We compare the averaged attention\nmaps to the input segmentation using a sum of binary cross-\nentropy losses computed separately for each segment:\nLZest =\nK\nX\ni=1\n\u0012\nLBCE(\u02c6Si, Si) + LBCE(\n\u02c6Si\n\u2225\u02c6Si\u2225\u221e\n, Si)\n\u0013\n.\n(4)\nIn the second loss term, we normalized the attention maps\n\u02c6Si independently for each object. This choice is motivated\nby two observations. Firstly, we found that averaging soft-\nmax outputs across heads, as described in Eq. (3), gener-\nally results in low maximum values in \u02c6Si. By normalizing\nthe attention maps, we make them more comparable with\nthe conditioning S. Secondly, we observed that estimated\nmasks can have different maximum values across different\nsegments resulting in varying impacts on the overall loss.\nNormalization helps to balance the impact of each object.\nHowever, relying solely on the normalized term is insuffi-\ncient, as the normalization process cancels out the gradient\ncorresponding to the maximum values.\nWe then use DDIM sampling with classifier guidance\nbased on the gradient of this loss. We use Eq. (1) to com-\npute the modified noise estimator at each denoising step.\nInterestingly, since xt\u22121 is computed from \u02dc\u03f5\u03b8(xt), this con-\nditional DDIM sampling corresponds to an alternation of\nregular DDIM updates and gradient descent updates on xt\nof the loss L, with a fixed learning rate \u03b7 multiplied by a\nfunction \u03bb(t) monotonically decreasing from one to zero\nthroughout the generative process. In this formulation, the\ngradient descent update writes:\n\u02dcxt\u22121 = xt\u22121 \u2212 \u03b7 \u00b7 \u03bb(t)\n\u2207xtLZest\n\u2225\u2207xtLZest\u2225\u221e\n.\n(5)\nNote that differently from Eq. (1), the gradient is normal-\nized to make updates more uniform in strength across im-\nages and denoising steps. We note that the learning rate\n\u03b7 can be set freely, which, as noted by [12], corresponds\nto using a renormalized classifier distribution in classifier\nguidance. As in [2], we define a hyperparameter \u03c4 as the\nfraction of steps during which classifier guidance is applied.\nPreliminary experiments suggested that classifier guidance\nis only useful in the first 50% of DDIM steps, and we set\n\u03c4 = 0.5 as our default value, see Sec. 4.3 for more details.\n4. Experiments\nWe present our experimental setup in Sec. 4.1, followed\nby our main results in Sec. 4.2 and ablations in Sec. 4.3.\n4.1. Experimental setup\nEvaluation protocol. We use the COCO-Stuff validation\nsplit, which contains 5k images annotated with fine-grained\npixel-level segmentation masks across 171 classes, and five\ncaptions describing each image [5]. We adopt three differ-\nent setups to evaluate our approach and to compare to base-\nlines. In all three settings, the generative diffusion model is\nconditioned on one of the five captions corresponding to the\nsegmentation map, but they differ in the segmentation maps\nused for spatial conditioning.\nThe first evaluation setting, Eval-all, conditions im-\nage generation on complete segmentation maps across all\nclasses, similar to the evaluation setup in OASIS [39] and\nSDM [45]. In the Eval-filtered setting, segmentation maps\nare modified by removing all segments occupying less than\n5% of the image, which is more representative of real-world\nscenarios where users may not provide segmentation masks\nfor very small objects. Finally, in Eval-few we retain be-\ntween one and three segments, each covering at least 5% of\nthe image, similar to the setups in [1, 4]. It is the most re-\nalistic setting, as users may be interested in drawing only a\nfew objects, and therefore the focus of our evaluation. Re-\ngarding the construction of the text prompts, we follow [1]\nand concatenate the annotated prompt of COCO with the\nlist of class names corresponding to the input segments.\nEvaluation metrics.\nWe use the two standard metrics\nto evaluate semantic image synthesis, see e.g. [29, 39].\nFr\u00b4echet Inception Distance (FID) [17] captures both image\nquality and diversity. We compute FID with InceptionV3\nand generate 5k images. The reference set is the original\nCOCO validation set, and we use code from [31].\nThe\nmean Intersection over Union (mIoU) metric measures to\nwhat extent the generated images respect the spatial condi-\ntioning. We additionally compute a CLIP score that mea-\nsures alignment between captions and generated images.\nAll methods, including ours, generate images at resolution\n512\u00d7512, except OASIS and SDM, for which we use avail-\nable pretrained checkpoints synthesizing images at resolu-\ntion 256 \u00d7 256, which we upsample to 512 \u00d7 512.\nBaselines. We compare to baselines that are either trained\nMethod\nFree-form Zero-\nEval-all\nEval-filtered\nEval-few\nmask texts\nshot\n\u2193FID \u2191mIoU \u2191CLIP \u2193FID \u2191mIoU \u2191CLIP \u2193FID \u2191mIoU \u2191CLIP\nOASIS [39]\n\u2717\n\u2717\n15.0\n52.1\n\u2014\n18.2\n53.7\n\u2014\n46.8\n41.4\n\u2014\nSDM [45]\n\u2717\n\u2717\n17.2\n49.3\n\u2014\n28.6\n41.7\n\u2014\n65.3\n29.3\n\u2014\nSD w/ T2I-Adapter [25]\n\u2717\n\u2717\n17.2\n33.3\n31.5\n17.8\n35.1\n31.3\n19.2\n31.6\n30.6\nLDM w/ External Classifier\n\u2717\n\u2717\n24.1\n14.2\n30.6\n23.2\n17.1\n30.2\n23.7\n20.5\n30.1\nSD w/ SpaText [1]\n\u2713\n\u2717\n19.8\n16.8\n30.0\n18.9\n19.2\n30.1\n16.2\n23.8\n30.2\nSD w/ PwW [2]\n\u2713\n\u2713\n36.2\n21.2\n29.4\n35.0\n23.5\n29.5\n25.8\n23.8\n29.6\nLDM w/ MultiDiffusion[4]\n\u2713\n\u2713\n59.9\n15.8\n23.9\n46.7\n18.6\n25.8\n21.1\n19.6\n29.0\nLDM w/ PwW\n\u2713\n\u2713\n22.9\n27.9\n31.5\n23.4\n31.8\n31.4\n20.3\n36.3\n31.2\nLDM w/ ZestGuide (ours)\n\u2713\n\u2713\n22.8\n33.1\n31.9\n23.1\n43.3\n31.3\n21.0\n46.9\n30.3\nTable 1. Comparison of ZestGuide to other methods in our three evaluation settings. OASIS and SDM are trained from scratch on COCO,\nother methods are based on pre-trained text-to-image models: StableDiffusion (SD) or our latent diffusion model (LDM). Methods that do\nnot allow for free-form text description of segments are listed in the upper part of the table. Best scores in each part of the table are marked\nin bold. For OASIS and SDM the CLIP score is omitted as it is not meaningful for methods that don\u2019t condition on text prompts.\nfrom scratch, finetuned or training-free.\nThe adversarial\nOASIS model [39] and diffusion-based SDM model [45]\nare both trained from scratch and conditioned on segmen-\ntation maps with classes of COCO-Stuff dataset. For SDM\nwe use T = 50 diffusion decoding steps. T2I-Adapter [25]\nand SpaText [1] both fine-tune pre-trained text-to-image\ndiffusion models for spatially-conditioned image genera-\ntion by incorporating additional trainable layers in the dif-\nfusion pipeline.\nSimilar to Universal Guidance [3], we\nimplemented a method in which we use classifier guid-\nance based on the external pretrained segmentation network\nDeepLabV2 [9] to guide the generation process to respect\na semantic map. We also compare ZestGuide to other zero-\nsot methods that adapt a pre-trained text-to-image diffusion\nmodel during inference. MultiDiffusion [4] decomposes the\ndenoising procedure into several diffusion processes, where\neach one focuses on one segment of the image and fuses all\nthese different predictions at each denoising iteration. In [2]\na conditioning pipeline called \u201cpaint-with-words\u201d (PwW) is\nproposed, which manually modifies the values of attention\nmaps. For a fair comparison, we evaluate these zero-shot\nmethods on the same diffusion model used to implement\nour method. Note that SpaText, MultiDiffusion, PwW, and\nour method can be locally conditioned on free-form text,\nunlike Universal Guidance, OASIS, SDM and T2I-Adapter\nwhich can only condition on COCO-Stuff classes.\nText-to-image model. Due to concerns regarding the train-\ning data of Stable Diffusion [36] (such as copyright in-\nfringements and consent), we refrain from experimenting\nwith this model and instead use a large diffusion model\n(2.2B parameters) trained on a proprietary dataset of 330M\nimage-text pairs. We refer to this model as LDM. Similar\nto [36] the model is trained on the latent space of an autoen-\ncoder, and we use an architecture for the diffusion model\nbased on GLIDE [26], with a T5 text encoder [33]. With\nan FID score of 19.1 on the COCO-stuff dataset, our LDM\nmodel achieves image quality similar to that of Stable Dif-\nfusion, whose FID score was 19.0, while using an order of\nmagnitude less training data.\nImplementation details. For all experiments that use our\nLDM diffusion model, we use 50 steps of DDIM sampling\nwith classifier-free guidance strength set to 3. For Zest-\nGuide results, unless otherwise specified, we use classifier\nguidance in combination with the PwW algorithm. We re-\nview this design choice in Sec. 4.3.\n4.2. Main results\nWe present our evaluation results in Tab. 1. Compared to\nother methods that allow free-text annotation of segments\n(bottom part of the table), our approach leads to marked im-\nprovements in mIoU in all settings. For example improving\nby more than 10 points (36.3 to 46.9) over the closest com-\npetitor PwW, in the most realistic Eval-few setting. Note\nthat we even improve over SpaText, which finetunes Stable\nDiffusion specifically for this task. In terms of CLIP score,\nour approach yields similar or better results across all set-\ntings. Our approach obtains the best FID values among the\nmethods based on our LDM text-to-image model. SpaText\nobtains the best overall FID values, which we attribute to the\nfact that it is finetuned on a dataset very similar to COCO,\nunlike the vanilla Stable Diffusion or our LDM.\nIn the top part of the table we report results for methods\nthat do not allow to condition segments on free-form text,\nand all require training on images with semantic segmenta-\ntion maps. We find they perform well in the Eval-all set-\nting for which they are trained, and also in the similar Eval-\nfiltered setting, but deteriorate in the Eval-few setting where\nonly a few segments are provided as input. In the Eval-few\nsetting, our ZestGuide approach surpasses all methods in\nthe top part of the table in terms of mIoU. Compared to\nLDM w/ External Classfier, which is based on the same dif-\nfusion model as ZestGuide but does not allow to condition\nsegments on free text, we improve across all metrics and\nsettings, while being much faster at inference: LDM w/ Ex-\n\u201cTwo zebra standing\n\u201cFive oranges\n\u201cThere is a dog\n\u201cA person over a box\u201d\n\u201cA train traveling\n\u201cThere is a woman\nnext to each other\nwith a red apple\nholding a Frisbee\njumping a horse\nthrough rural country-\nabout to ski\nin a dry grass field.\u201d\nand a green apple.\u201d\nin its mouth.\u201d\nover a box\u2019.\u2019\nside lined with trees.\u201d\ndown a hill.\u201d\nplant\napple\nsky\nhorse\nbush\nperson\nstraw\norange\nsand\nfence\ngrass\nsnow\ndog\ntree\ntrain\ntree\nExt. Classifier\nMultiDiffusion\nPwW\nZestGuide (ours)\nFigure 5. Qualitative comparison of ZestGuide to other methods based on LDM, conditioning on COCO captions and up to three segments.\nternalClassifier takes 1 min. for one image while ZestGuide\ntakes around 15 secs.\nWe provide qualitative results for the methods based\non LDM in Fig. 5 when conditioning on up to three seg-\nments, corresponding to the Eval-few setting. Our Zest-\nGuide clearly leads to superior aligment between the con-\nditioning masks and the generated content.\n4.3. Ablations\nIn this section we focus on evaluation settings Eval-\nfiltered and Eval-few, which better reflect practical use\ncases. To reduce compute, metrics are computed with a sub-\nset of 2k images from the COCO val set.\nAblation on hyperparameters \u03c4 and \u03b7. Our approach has\ntwo hyperparamters that control the strength of the spatial\nguidance: the learning rate \u03b7 and the percentage of denois-\ning steps \u03c4 until which classifier guidance is applied. Vary-\ning these hyperparameters strikes different trade-offs be-\ntween mIoU (better with stronger guidance) and FID (better\nwith less guidance and thus less perturbation of the diffu-\nsion model). In Fig. 6 we show generations for a few values\nof these parameters. We can see that, given the right learn-\ning rate, applying gradient updates for as few as the first\n25% denoising steps can suffice to enforce the layout con-\nditioning. This is confirmed by quantitative results in the\nEval-few setting presented in the supplementary material.\nFor \u03b7 = 1, setting \u03c4 = 0.5 strikes a good trade-off with an\n\u201cOn the moon\u201d\n\u201c A horse\u201d\n\u201cAn astronaut\u201d\nFigure 6. ZestGuide outputs when varying the two main hyperpa-\nrameters \u03b7 (learning rate) and \u03c4 (percentage of steps using classi-\nfier guidance). Our default configuration is \u03b7 =1, \u03c4 =0.5.\nmIoU of 43.3 and FID of 31.5. Setting \u03c4 = 1 marginally\nimproves mIoU by 1.3 points, while worsening FID by 3.2\npoints, while setting \u03c4 = 0.1 worsens mIoU by 9.1 points\nfor a gain of 1 point in FID. Setting \u03c4 = 0.5 requires ad-\nditional compute for just the first half of denoising steps,\nmaking our method in practice only roughly 50% more ex-\npensive than regular DDIM sampling.\nGuidance losses and synergy with PwW. In Fig. 7 we ex-\nplore the FID-mIoU trade-off in the Eval-filtered setting, for\nPwW and variations of our approach using different losses\nand with/out including PwW. The combined loss refers to\nour full loss in Eq. (4), while the BCE loss ignores the sec-\nond normalized loss. For PwW, the FID-mIoU trade-off\nis controlled by the constant W that is added to the atten-\ntion values to reinforce the association of image regions and\ntheir corresponding text. For ZestGuide, we vary \u03b7 to obtain\ndifferent trade-offs, with \u03c4 = 0.5. We observe that all ver-\nsions of our approach provide better mIoU-FID trade-offs\nthan PwW alone. Interestingly, using the combined loss\nand PwW separately hardly improve the mIoU-FID trade-\noff w.r.t. only using the BCE loss, but their combination\ngives a much better trade-off (Combined Loss + pWW).\nThis is possibly due to the loss with normalized maps help-\ning to produce more uniform segmentation masks, which\nhelps PwW to provide more consistent updates.\nIn the remainder of the ablations, we consider the sim-\nplest version of ZestGuide with the LBCE loss and without\nPwW, to better isolate the effect of gradient guiding.\nAttention map averaging. As mentioned in Sec. 3.2, we\nfound that averaging the attention maps across all heads\nof the different cross-attention layers is important to obtain\ngood spatial localization. We review this choice in Tab. 2.\nWhen we compute our loss on each head separately, we can\nsee a big drop in mIoU scores (-11 points). This reflects\nour observation that each attention head focuses on different\n30\n32\n34\n36\n38\n40\n42\n44\n46\nFID\n32\n34\n36\n38\n40\n42\n44\nMIoU\nLBCE\nLBCE + PwW\nLZest\nLZest + PwW (ours)\nPwW\nunconstrained\ngeneration\nFigure 7. Trade-off in Eval-filtered setting between FID (lower is\nbetter) and mIoU (higher is better) of PwW and ZestGuide us-\ning different losses. In dotted green is shown the FID for uncon-\nstrained text-to-image generation. Using LZest in combination with\nPwW (our default setting) gives the best trade-off.\nComponents\n\u2193FID\n\u2191mIoU\n\u2191CLIP\nLoss for each attention head\n33.6\n32.1\n29.9\nLoss for each layer\n31.6\n42.7\n30.5\nLoss for global average (ours)\n31.5\n43.3\n30.4\nTable 2. Evaluation of ZestGuide on Eval-few setting, with dif-\nferent averaging schemes for computing the loss. Averaging all\nattention heads before applying the loss gives best results.\nparts of each object. By computing a loss on the averaged\nmaps, a global pattern is enforced while still maintaining\nflexibility for each attention head. This effect is much less\nvisible when we average attention maps per layer, and ap-\nply the loss per layer: in this case mIoU deteriorates by 1.6\npoints, while FID improves by 0.9 points.\nGradient normalization. Unlike standard classifier guid-\nance, ZestGuide uses normalized gradient to harmonize gra-\ndient descent updates in Eq. (5). We find that while Zest-\nGuide also works without normalizing gradient, adding it\ngives a boost of 2 mIoU points for comparable FID scores.\nQualitatively, it helped for some cases where the gradient\nnorm was too high at the beginning of generation process,\nwhich occasionally resulted in low-quality samples.\nAdditional ablations are provided in the supplementary.\n5. Conclusion\nIn this paper, we have presented ZestGuide, a zero-shot\nmethod which enables precise spatial control over the gen-\nerated content by conditioning on segmentation masks an-\nnotated with free-form textual descriptions. Our approach\nleverages implicit segmentation maps extracted from text-\nattention in pre-trained text-to-image diffusion models to\nalign the generation with input masks. Experimental re-\nsults demonstrate that our approach achieves high-quality\nimage generation while accurately aligning the generated\ncontent with input segmentations. Our quantitative evalua-\ntion shows that ZestGuide is even competitive with methods\ntrained on large image-segmentation datasets. Despite this\nsuccess, there remains a limitation shared by many exist-\ning approaches. Specifically, the current approach, like oth-\ners, tends to overlook small objects in the input condition-\ning maps. Further work is required to address this problem\nwhich may be related to the low resolution of the attention\nmaps in the diffusion model.\nAcknowledgments.\nWe would like to thank Oron Ashual,\nUriel Singer, Adam Polyak and Shelly Sheynin for prepar-\ning the data and training and sharing the text-to-image\nmodel on which the work in this paper is based.\nReferences\n[1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,\nYaniv Taigman,\nDevi Parikh,\nDani Lischinski,\nOhad\nFried, and Xi Yin.\nSpaText:\nSpatio-textual representa-\ntion for controllable image generation.\narXiv preprint,\narXiv:2211.14305, 2022. 1, 3, 5, 6\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu\nLiu. eDiff-I: Text-to-image diffusion models with ensemble\nof expert denoisers. arXiv preprint, arXiv:2211.01324, 2022.\n2, 3, 5, 6\n[3] Arpit\nBansal,\nHong-Min\nChu,\nAvi\nSchwarzschild,\nSoumyadip Sengupta,\nMicah Goldblum,\nJonas Geip-\ning, and Tom Goldstein. Universal guidance for diffusion\nmodels. arXiv preprint, arXiv:2302.07121, 2023. 2, 3, 6\n[4] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultiDiffusion: Fusing diffusion paths for controlled image\ngeneration. arXiv preprint, arXiv:2302.08113, 2023. 3, 5, 6\n[5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-\nStuff: Thing and stuff classes in context. In CVPR, 2018.\n5\n[6] Huiwen Chang,\nHan Zhang,\nLu Jiang,\nCe Liu,\nand\nWilliam T. Freeman. MaskGIT: Masked generative image\ntransformer. In CVPR, 2022. 2\n[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. arXiv\npreprint, arXiv:2301.13826, 2023. 3\n[8] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\nA. Yuille. Semantic image segmentation with deep convolu-\ntional nets and fully connected CRFs. In ICLR, 2015. 11\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. DeepLab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected CRFs. PAMI, 40(4):834\u2013848,\n2017. 6\n[10] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\ndense predictions. In ICLR, 2023. 11\n[11] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and\nMatthieu Cord. DiffEdit: Diffusion-based semantic image\nediting with mask generation. In ICLR, 2023. 3\n[12] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\nGANs on image synthesis. In NeurIPS, 2021. 2, 3, 4, 5\n[13] Patrick Esser, Robin Rombach, and B. Ommer.\nTaming\ntransformers for high-resolution image synthesis. In CVPR,\n2021. 2\n[14] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun\nAkula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang,\nand William Yang Wang. Training-free structured diffusion\nguidance for compositional text-to-image synthesis. arXiv\npreprint, arXiv:2212.05032, 2022. 3\n[15] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors. In ECCV,\n2022. 1, 2\n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint,\narXiv:2208.01626, 2022. 1, 3\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local Nash equi-\nlibrium. In NeurIPS, 2017. 5\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 3\n[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In CVPR, 2017. 2\n[20] Jos\u00b4e Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Im-\nproved masked image generation with token-critic. In ECCV,\n2022. 2\n[21] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu,\nJianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae\nLee. GLIGEN: Open-set grounded text-to-image generation.\narXiv preprint, arXiv:2301.07093, 2023. 3\n[22] Timo L\u00a8uddecke and Alexander S. Ecker. Image segmenta-\ntion using text and image prompts. In CVPR, 2022. 3\n[23] Wan-Duo Kurt Ma, JP Lewis, W Bastiaan Kleijn, and\nThomas Leung. Directed diffusion: Direct control of ob-\nject placement through attention guidance. arXiv preprint\narXiv:2302.13153, 2023. 3\n[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In ICLR, 2022. 1, 3\n[25] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint, arXiv:2302.08453, 2023.\n3, 6\n[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In ICML,\n2022. 2, 6\n[27] A. van den Oord, O. Vinyals, and K. Kavukcuoglu. Neural\ndiscrete representation learning. In NeurIPS, 2017. 2\n[28] Arantxa Casanova Paga, Marlene Careil, Adriana Romero\nSoriano, Christopher J. Pal, Jakob Verbeek, and Michal\nDrozdzal. Controllable image generation via collage repre-\nsentations. ICLR submission, 2022. 1\n[29] T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu. Semantic\nimage synthesis with spatially-adaptive normalization.\nIn\nCVPR, 2019. 1, 2, 5\n[30] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. arXiv preprint, arXiv:2302.03027, 2023. 3\n[31] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\naliased resizing and surprising subtleties in GAN evaluation.\nIn CVPR, 2022. 5\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n3\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. JMLR, 21, 2022. 6\n[34] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen.\nHierarchical text-conditionalimage generation with CLIP la-\ntents. arXiv preprint, arXiv:2204.06125, 2022. 1\n[35] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gen-\nerating diverse high-fidelity images with VQ-VAE-2.\nIn\nNeurIPS, 2019. 2\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 1, 4, 6\n[37] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi. Palette: Image-to-image diffusion models. In ACM\nSIGGRAPH, 2022. 1, 3\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding.\nIn\nNeurIPS, 2022. 1\n[39] Edgar Sch\u00a8onfeld, Vadim Sushko, Dan Zhang, Juergen Gall,\nBernt Schiele, and Anna Khoreva. You only need adversarial\nsupervision for semantic image synthesis. In ICLR, 2021. 1,\n2, 5, 6\n[40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 4\n[41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2020. 3\n[42] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICLR, 2021. 4\n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you\nneed. In NeurIPS, 2017. 4\n[44] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong\nChen, Qifeng Chen, and Fang Wen.\nPretraining is all\nyou need for image-to-image translation.\narXiv preprint,\narXiv:2205.12952, 2022. 1, 2\n[45] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong\nChen, Dong Chen, Lu Yuan, and Houqiang Li.\nSeman-\ntic image synthesis via diffusion models.\narXiv preprint,\narXiv:2207.00050, 2022. 1, 2, 5, 6\n[46] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models. arXiv preprint,\narXiv:2302.05543, 2023. 3\nA. Societal impact\nOur work advances the capabilities of generative image\nmodels, contributing to the democratization of creative de-\nsign by offering tools for non-expert users. Generative im-\nage models, however, also pose risks, including using these\ntools to generate harmful content or deep-fakes, or models\ngenerating images similar to the training data which may\ncontain personal data. These concerns have led us to steer\naway from using large-scale open-source generative image\nmodels trained on datasets scraped from the web, for which\nthe licensing of the content is not always clear and which\nmay contain harmful content. Instead, we trained models\non a large in-house curated dataset which mitigates these\nconcerns to a large extent.\nB. Implementation details\nImplementation details. For all experiments that use our\nLDM diffusion model, we use 50 steps of DDIM sam-\npling with classifier-free guidance strength set to 3. Stable\nDiffusion-based competing methods, like PwW, also use 50\nsteps of DDIM sampling, but with a classifier-free guidance\nof 7.5.\nComputation of metrics.\nTo compute the mIoU met-\nric we use ViT-Adapter[10] as segmentation model rather\nthan the commonly used DeepLabV2 [8], as the former im-\nproves over the latter by 18.6 points of mIoU (from 35.6\nto 54.2) on COCO-Stuff.\nScores for methods based on\nStable Diffusion are taken from https://cdancette.\nfr/diffusion-models/.\nC. Additional ablation experiments\nFor these additional ablation experiments, we use the\nEval-few setting as presented in the paper, where 1 \u2264 K \u2264\n3 spatial masks are used for conditioning.\nAttention layers used. We first validate which layers are\nuseful for computing our classifier guidance loss in Table 3.\nWe find that whatever the set of cross-attention layers used\nfor computing loss, the mIoU and FID scores are very com-\npetitive. In accordance with preliminary observations, it is\nslightly better to skip attention maps at resolution 8 when\ncomputing our loss.\nLayers used\n\u2193FID\n\u2191mIoU\n\u2191CLIP\nAll layers\n33.74\n40.17\n30.19\nOnly decoder layers\n33.81\n40.02\n30.05\nOnly encoder layers\n30.98\n38.24\n30.67\nOnly res32 layers\n29.35\n39.49\n30.75\nOnly res16 layers\n33.59\n40.27\n30.23\nres16 and res32 layers (ours)\n31.53\n43.34\n30.44\nTable 3. Ablation on cross-attention layers used for estimating seg-\nmentation maps.\nNormalization\n\u2193FID\n\u2191mIoU\n\u2191CLIP\nNo normalization\n30.77\n38.99\n30.70\nL2 norm\n28.57\n36.39\n31.27\nL1 norm\n28.85\n39.74\n31.04\nL\u221e norm (ours)\n31.53\n43.34\n30.44\nTable 4. Impact of gradient normalization scheme on performance.\nGradient normalization. We validate the impact of nor-\nmalizing gradient when applying classifier guidance with\nour LZest loss. Results are in Table 4.\nImpact of parameter \u03c4. In our method, classifier guidance\nis only used in a fraction \u03c4 of denoising steps, after which\nit is disabled. Table 5 demonstrates that after our default\nvalue \u03c4 = 0.5, mIoU gains are marginal, while the FID\nscores are worse. Conversely, using only 10% or 25% of\ndenoising steps for classifier guidance already gives very\ngood mIoU/FID scores, better than PwW for \u03c4 = 0.25. As\nillustrated in Sec. D, this is because estimated segmentation\nmaps converge very early in the generation process.\nComponents\n\u2193FID\n\u2191mIoU\n\u2191CLIP\n\u03c4 = 0.1\n30.54\n34.25\n31.18\n\u03c4 = 0.25\n30.36\n40.75\n30.77\n\u03c4 = 0.5\n31.53\n43.34\n30.44\n\u03c4 = 1\n34.75\n44.58\n29.99\nTable 5. Ablation on parameter \u03c4, with fixed learning rate \u03b7 = 1\nin the Eval-few setting.\nTokens used as attention keys. Our estimated segmenta-\ntion masks are computed with an attention mechanism over\na set of keys computed from the text prompt embeddings. In\nthis experiment, we analyze whether the attention over the\nfull text-prompt is necessary, or whether we could simply\nuse classification scores over the set of classes correspond-\ning to the segments. We encode each class text separately\nwith the text encoder, followed by average pooling to get a\nsingle embedding per class. Computing our loss with these\nembeddings as attention keys results in a probability distri-\nbution over the segmentation classes. We find that the FID\nscores are worse (+ 3 pts FID), but the mIoU scores are very\nclose (43.36 vs. 43.34). We conclude that our loss function\nprimarily serves to align spatial image features with the rel-\nevant textual feature at each spatial location, and that the\npatterns that we observe in attention maps are a manifesta-\ntion of this alignment.\nD. Additional visualizations\nEvolution of attention maps across timesteps. We show\nin Fig. 8 and Fig. 9 average attention maps on the different\nobjects present in the input segmentation during the first 12\ndenoising steps with and without our guidance scheme. We\n\u201cA big burly grizzly\nbear is shown with grass\nin the background.\u201d\ngrass\nW/o Guidance\nW/ Guidance\nbear\nW/o Guidance\nBear\nGrass\nW/ Guidance\nBear\nGrass\nFigure 8. Visualization of first 12 denoising steps out of 50 steps. Same seed for w/ and w/o guidance.\ncondition on the same Gaussian noise seed in both cases.\nWe notice that attention maps quickly converges to the cor-\nrect input conditioning mask when we apply ZestGuide and\nthat the attention masks are already close to ground truth\nmasks only after 12 denoising iteration steps out of 50.\nAdditional visualizations on COCO. In Figure 10, we\nshow additional qualitative samples generated with COCO\nmasks comparing ZestGuide to the different zero-shot\nmethods.\nVisualizations on hand-drawn masks.\nIn Fig. 11, we\nshow generations conditioned on coarse hand-drawn masks,\na setting which is closer to real-world applications, similar\nto Fig. 2 in the main paper. In this case the generated ob-\njects do not exactly match the shape of conditioning masks:\nthe flexibility of ZestGuide helps to generate realistic im-\nages even in the case of unrealistic segmentation masks, see\ne.g. the cow and mouse examples.\n\u201cFive oranges\nwith a red apple\nand a green apple.\u201d\napple\nW/o Guidance\nW/ Guidance\norange\nW/o Guidance\nOrange\nApple\nW/ Guidance\nOrange\nApple\nFigure 9. Visualization of first 12 denoising steps out of 50 steps. Same seed for w/ and w/o guidance.\n\u201c6 open umbrellas\n\u201cCat sitting up\n\u201cThere are two brown\n\u201cA close-up of an\n\u201cA broken suitcase\n\u201cA group of zebras\nof various colors\nwith a fake tie\nbears that are playing\norange on the side\nis on the side\nwalking away\nhanging on a line\u201d\naround it\u2019s neck.\u201d\ntogether in the water.\u201d\nof the road.\u201d\nof the road.\u201d\nfrom trees.\u201d\numbrella\ncat\nbear\norange\nplant\ntree\nhouse\nfurniture\nriver\nclouds\nsuitcase\nzebra\nsky\nwall\ntree\nwall\ndirt\nExt. Classifier\nMultiDiffusion\nPwW\nZestGuide (ours)\nFigure 10. Qualitative comparison of ZestGuide to other methods based on LDM, conditioning on COCO captions and up to three segments.\n\u201cA car and a tree,\n\u201c A mirror, sink\n\u201cPlate with cookies\n\u201cA brown cow in\n\u201cA mouse wearing\nat the beach.\u201d\nand flowers\nand cup of coffee,\na field, cloudy sky,\na hat in the desert.\u201d\nin a bathroom.\u201d\nfancy tablecloth \u201d\nred full moon\u201d\nFigure 11. ZestGuide generations on coarse hand-drawn masks.\n"
  },
  {
    "title": "RVT: Robotic View Transformer for 3D Object Manipulation",
    "link": "https://arxiv.org/pdf/2306.14896.pdf",
    "upvote": "2",
    "text": "RVT: Robotic View Transformer for 3D Object\nManipulation\nAnkit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, Dieter Fox\nNVIDIA\nAbstract: For 3D object manipulation, methods that build an explicit 3D rep-\nresentation perform better than those relying only on camera images. But using\nexplicit 3D representations like voxels comes at large computing cost, adversely\naffecting scalability. In this work, we propose RVT, a multi-view transformer for\n3D manipulation that is both scalable and accurate. Some key features of RVT are\nan attention mechanism to aggregate information across views and re-rendering\nof the camera input from virtual views around the robot workspace. In simula-\ntions, we find that a single RVT model works well across 18 RLBench tasks with\n249 task variations, achieving 26% higher relative success than the existing state-\nof-the-art method (PerAct). It also trains 36X faster than PerAct for achieving the\nsame performance and achieves 2.3X the inference speed of PerAct. Further, RVT\ncan perform a variety of manipulation tasks in the real world with just a few (\u223c10)\ndemonstrations per task. Visual results, code, and trained model are provided at:\nhttps://robotic-view-transformer.github.io/.\nKeywords: 3D Manipulation, Multi-View, Transformer\n1\nIntroduction\nA fundamental goal of robot learning is to build systems that can solve various manipulation tasks in\nunconstrained 3D settings. A popular class of learning methods directly processes image(s) viewed\nfrom single or multiple cameras. These view-based methods have achieved impressive success on a\nvariety of pick-and-place and object rearrangement tasks [1, 2, 3, 4]. However, their success on tasks\nthat require 3D reasoning has been limited. As shown by James et al. [5] and Shridhar et al. [6],\nview-based methods struggle at 3D manipulation tasks on RLBench [7] with less than 2% success.\n0 1 2 3 4 5 6 7 8 9 10111213141516\nTraining Time (in Days)\n0\n10\n20\n30\n40\n50\n60\nAvg. Success\n36X Faster\n1.26X\nRVT\nPerAct\nFigure 1: RVT scales and performs better\nthan PerAct on RLBench, achieving on-\npar performance in 36X less time (same\nhardware), and 1.26X peak performance.\nTo address this, methods have been proposed that rea-\nson with explicit 3D representations of the scene. C2F-\nARM [5] represents the scene with multi-resolution\nvoxels and achieves strong performance on difficult\nRLBench tasks. PerAct [6] improves upon C2F-ARM\nin behavior cloning by using perceiver transformer [8]\nto process voxels.\nHowever, creating and reasoning\nover voxels comes at a higher computing cost com-\npared to reasoning over images, since the number of\nvoxels scales cubicly with the resolution as opposed\nto squarely for image pixels. This makes voxel-based\nmethods less scalable compared to their view-based\ncounterparts. In fact, training PerAct on 18 RLBench\ntasks takes 16 days using 8 V100 GPUs (3072 GPU\nhours). This hinders fast development and prototyp-\ning. Moreover, such computing requirements become even more prohibitive when scaling to larger\ndatasets with more tasks and diversity.\narXiv:2306.14896v1  [cs.RO]  26 Jun 2023\nHence, a key question is \u2013 can we build a manipulation network that not only performs well but\nalso inherits the scalability of view-based methods? To this end, we propose RVT (Robotic View\nTransformer) that significantly outperforms the SOTA voxel-based method both in terms of success\nrate and training time, as shown in Fig. 1. With the same hardware, RVT achieves the peak per-\nformance of PerAct in 36X less time, decreasing the training time from 14 days to just 10 hours.\nApart from being much faster to train, RVT also achieves a 26% higher success rate than PerAct,\naveraged over 18 tasks (249 task variations) on RLBench. RVT outperforms PerAct on 88.9% of\ntasks on RLBench while achieving 2.3X the inference speed (11.6 vs 4.9 fps). Further, we find\nthat RVT also works well in the real world, where with only 51 demonstrations, a single RVT model\ncan learn to perform a variety of manipulation tasks (5 tasks, 13 variations) like opening a drawer,\nplacing objects on a shelf, pressing hand sanitizer, and stacking objects (see Fig. 4).\nAt its core, RVT is a view-based method that leverages the transformer architecture. It jointly attends\nover multiple views of the scene and aggregates information across the views. It then produces\nview-wise heatmaps and features that are used to predict robot end-effector pose. We extensively\nexplore the design of the multi-view architecture and report several useful findings. For example,\nwe observe a better performance when enforcing the transformer to first attend over patches within\nthe same image before concatenating the patches for joint attention.\nAnother key innovation is that, unlike prior view-based methods, we decouple the camera images\nfrom the images fed to the transformer, by re-rendering the images from virtual views. This allows\nus to control the rendering process and leads to several benefits. For example, we can re-render from\nviewpoints that are useful for the task (e.g., directly above the table) while not being restricted by\nreal-world physical constraints. Also, since the multi-view input to RVT is obtained via re-rendering,\nwe can use RVT even with a single sensor camera \u2013 as done in our real-world experiments.\nTo summarize, our contributions are threefold: first, we propose RVT, a multi-view transformer for\n3D object manipulation that is accurate and scalable; second, we investigate various design choices\nfor the multi-view transformer that lead to better object manipulation performance; and finally, we\npresent an empirical study for multi-task object manipulation in simulation and the real world.\n2\nRelated Work\nVision-based Object Manipulation. The learning of robotic control policy has been tradition-\nally studied with low-dimensional state observations [9, 10, 11, 12, 13]. Recently, vision-based\npolicies [14, 15, 16, 17, 18, 19, 20, 21] have gained increasing attention since the high-dimensional\nvisual sensory input provides more generalizable observation representation across tasks and is more\naccessible in real-world perception systems. Various forms of visual input have been explored. Prior\nwork has directly encoded the RGB images into a low-dimensional latent space and relied on model-\nbased [22, 23] or model-free [24, 25] reinforcement learning (RL) to train policies to operate in this\nspace. More recently, RT-1 [26] infers the robot\u2019s actions from a history of images by leveraging\ntransformer architectures [27]. Our proposed RVT also uses a transformer to predict actions, how-\never, unlike RT-1, we additionally leverage depth to construct a multi-view scene representation.\nThe use of depth input has also been extensively studied. Methods like CLIPort [3] and IFOR [1]\ndirectly process the RGB-D images for object manipulation, and hence are limited to simple pick-\nand-place tasks in 2D top-down settings. To overcome this issue, explicit 3D representations such\nas point clouds have been utilized. C2F-ARM [5] and PerAct [6] voxelize the point clouds and use\na 3D convolutional network as the backbone for control inference. However, high-precision tasks\ntypically require high resolution of voxelization, resulting in high memory consumption and slow\ntraining. Our approach falls into this category but addresses the scalability issue by transforming the\npoint cloud into a set of RGB-D images from multiple views. We show that this significantly im-\nproves memory footprint and training efficiency, and leads to higher performance when compared to\ndirectly working with RGB(-D) or point cloud input (see Table. 1). Another relevant work is MIRA\n[28], which also uses novel view images to represent the 3D scene for action inference. MIRA\nachieves this by implicitly constructing a neural radiance field representation (NeRF) of the scene\n2\na. Sensor Data\nb. Point Cloud and Virtual Cameras\nc. Virtual Images\ne. Prediction in 3D\nd. Predicted Heatmaps\nMulti-View Transformer\nFigure 2: Overview of RVT. Given RGB-D from sensor(s), we first construct a point cloud of the\nscene. The point cloud is then used to produce virtual images around the robot workspace. The\nvirtual images are fed to a multi-view transformer model to predict view-specific features, which are\nthen combined to predict action in 3D.\nfrom a set of RGB images and then generating novel view images from the optimized NeRF model.\nHowever, the requirement of optimizing a scene NeRF model slows down the inference speed at test\ntime and relies on RGB images from a dense set of views as input. In contrast, our approach can\nachieve significantly faster inference speed and can work with even a single-view RGB image.\nMulti-Task Learning in Robotics. Learning a single model for many different tasks has been\nof particular interest to the robotics community recently. A large volume of work achieves the\nmulti-task generalization by using a generalizable task or action representation such as object point\ncloud [18, 19], semantic segmentation and optical flow [1], and object-centric representation [29,\n30]. However, the limited expressiveness of such representations constrains them to only generalize\nwithin a task category. Task parameterization [31, 32] and discrete task-dependent output layer [33,\n34] approaches are investigated with reinforcement learning to learn policies for tasks in different\ncategories. With the recent breakthrough in large language models, multi-task robot learning has\nbeen approached by using natural language to specify a broad range of tasks and learning the policy\nfrom large pre-collected datasets [35, 26, 36, 2, 37, 38, 39, 40, 41]. We are inspired by this success\nbut propose to learn language-conditioned multi-task policies with a small demonstration dataset.\nTransformers for Object Manipulation. The success of transformers in vision and NLP has led\nits way into robot learning [42, 43, 44, 17]. Especially in object manipulation, transformer-based\nmodels with an attention mechanism can be utilized to extract features from sensory inputs to im-\nprove policy learning [45, 46, 47, 48, 49]. Unlike most prior work, we do not use large datasets for\ntraining. RVT efficiently learns from a small set of demonstrations, handle multiple views as visual\ninputs, and fuses information from language goals to tackle multiple manipulation tasks.\nMulti-View Networks in Computer Vision. Multi-view representations have been explored in\nvarious vision problems. For point cloud recognition, SimpleView [50] showed how a simple view-\nbased method outperforms sophisticated point-based methods. Follow-up works like MVTN [51]\nand Voint cloud [52] have further improved upon SimpleView\u2019s architecture. Multi-view representa-\ntions have also been used for other problems like 3D visual grounding [53] and view synthesis [54].\nUnlike them, we focus on the problem of predicting robot actions for object manipulation.\n3\nMethod\nOur goal is to learn a single model that can complete a wide range of manipulation tasks. The\ninput consists of (1) a language description of the task, (2) the current visual state (from RGB-D\ncamera(s)), and (3) the current gripper state (open or closed). The model should predict an action,\nspecified by a target end-effector pose and gripper state at the next key-frame. The key-frames\nrepresent important or bottleneck steps of the gripper during the task execution [55], such as a pre-\npick, grasp, or place pose. Given a target end effector pose, we assume a low-level motion planner\n3\nand controller that can move the end effector to the target pose. To train the model, we assume a\ndataset D = {D1, D2, \u00b7 \u00b7 \u00b7 , Dn} of n expert demonstrations covering various tasks is given. Each\ndemonstration Di = ({oi\n1...mi}, {ai\n1...mi}, li) is a successful roll-out of length mi, where li is the\nlanguage description of the task, {oi\n1, oi\n2, ..., oi\nmi} is a sequence of the observations from RGB-D\ncamera(s) with gripper state, and {ai\n1, ai\n2, ..., ai\nmi} is the sequence of corresponding robot actions.\nThis demonstration dataset can be used to train models with behavior cloning.\nOur proposed method (RVT) is a transformer model [27] that processes images re-rendered around\nthe robot workspace, produces an output for each view, and then back-projects into 3D to predict\ngripper pose actions, as shown in Fig. 2.\nRendering. The first step is the re-rendering of camera input. Given the RGB-D image(s) captured\nby one or multiple sensor cameras, we first reconstruct a point cloud of the scene. The point cloud is\nthen re-rendered from a set of virtual viewpoints anchored in the space centered at the robot\u2019s base\n(see Fig. 2 and Fig. 3). Specifically, for each view, we render three image maps with a total of 7\nchannels: (1) RGB (3 channels), (2) depth (1 channel), and (3) (x, y, z) coordinates of the points in\nthe world frame (3 channels). The (x, y, z) coordinates help establish the correspondence of pixels\nacross views, i.e., if pixels from different views share the same (x, y, z), they correspond to the same\npoint in 3D. We use PyTorch3D [56] for rendering. We empirically verify various design choices in\nour rendering pipeline (see Tab. 2 (left)).\nThe re-rendering process decouples the input images to the ones fed to the transformer. This offers\nseveral benefits such as: the ability to re-render at arbitrary and useful locations (e.g., directly above\nthe table) while not being constrained by real-world camera placements; multi-view reasoning even\nwith a single sensor camera; allowing the use of orthographic images instead of generally provided\nperspective ones; facilitating 3D point-cloud augmentations and enabling additional channels like\npoint correspondence which are not natively presented in the sensor images. We empirically find\nthat these contribute to achieving high performance with view-based networks (see Sec. 4.1).\nJoint Transformer. The re-rendered images, the language description of the task, and the gripper\nstate (open or close) are processed by a joint transformer model (see Fig. 5 in the appendix). For\nlanguage, we use pretrained CLIP [57] embeddings (ResNet-50 variant), which provide one token\nfor each word. For the virtual images, we break each of them into 20 \u00d7 20 patches and pass through\na multi-layer perceptron (MLP) to produce image tokens, similar to ViT [58]. For the gripper state,\nsimilar to PerAct [6], we pass it through an MLP and concatenate it to the image tokens. We also add\npositional embeddings to all the image and language tokens to preserve the positional information.\nOverall, RVT has eight self-attention layers. In the first four layers, an image token is only allowed\nto attend to other tokens from the same image. This biases the network to process individual images\nfirst before sharing information across images. We concatenate all the image tokens along with the\nlanguage tokens afterward. In the last four layers, we allow the attention layers to propagate and\naccumulate information across different images and text. Finally, the image tokens are rearranged\nback to the original spatial configuration, resulting in the feature channels of each image.\nAction Prediction. The model outputs an 8-dimensional action, including the 6-DoF target end\neffector pose (3-DoF for translation and 3-DoF for rotation), 1-DoF gripper state (open or close),\nand a binary indicator for whether to allow collision for the low-level motion planner (see [6] for\ndetails). For translation, we first predict a heatmap for each view from the per-image features from\nthe joint transformer (as shown in Fig. 5 in the appendix). The heatmaps across different views are\nthen back-projected to predict scores for a discretized set of 3D points that densely cover the robot\nworkspace. Finally, the end effector translation is determined by the 3D point with the highest score.\nNote that this multi-view heatmap representation for translation prediction extends prior approaches\nin the 2D top-down view setting [4]. Hence, RVT inherits the benefit of superior sample efficiency\nby representing the visual input and action in the same spatial structure [4].\nFor end effector rotation, we follow PerAct to use the Euler angles representation, where each angle\nis discretized into bins of 5\u25e6 resolution. The gripper state and the motion planner collision indicator\nare represented as binary variables. To predict the rotations, gripper state, and collision indicator,\n4\nAvg.\nAvg.\nTrain time Inf. Speed\nClose\nDrag\nInsert\nMeat off\nOpen\nPlace\nPlace\nModels\nSuccess \u2191\nRank \u2193\n(in days) \u2193 (in fps) \u2191\nJar\nStick\nPeg\nGrill\nDrawer\nCups\nWine\nImage-BC (CNN) [2, 6]\n1.3\n3.7\n-\n-\n0\n0\n0\n0\n4\n0\n0\nImage-BC (ViT) [2, 6]\n1.3\n3.8\n-\n-\n0\n0\n0\n0\n0\n0\n0\nC2F-ARM-BC [5, 6]\n20.1\n3.1\n-\n-\n24\n24\n4\n20\n20\n0\n8\nPerAct [6]\n49.4\n1.9\n16.0\n4.9\n55.2 \u00b1 4.7 89.6 \u00b1 4.1\n5.6 \u00b1 4.1 70.4 \u00b1 2.0 88.0 \u00b1 5.7 2.4 \u00b1 3.2 44.8 \u00b1 7.8\nRVT (ours)\n62.9\n1.1\n1.0\n11.6\n52.0 \u00b1 2.5 99.2 \u00b1 1.6 11.2 \u00b1 3.0 88.0 \u00b1 2.5 71.2 \u00b1 6.9 4.0 \u00b1 2.5 91.0 \u00b1 5.2\nPush\nPut in\nPut in\nPut in\nScrew\nSlide\nSort\nStack\nStack\nSweep to\nTurn\nModels\nButtons\nCupboard\nDrawer\nSafe\nBulb\nBlock\nShape\nBlocks\nCups\nDustpan\nTap\nImage-BC (CNN) [2, 6]\n0\n0\n8\n4\n0\n0\n0\n0\n0\n0\n8\nImage-BC (ViT) [2, 6]\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\nC2F-ARM-BC [5, 6]\n72\n0\n4\n12\n8\n16\n8\n0\n0\n0\n68\nPerAct [6]\n92.8 \u00b1 3.0 28.0 \u00b1 4.4\n51.2 \u00b1 4.7\n84.0 \u00b1 3.6 17.6 \u00b1 2.0 74.0 \u00b1 13.0 16.8 \u00b1 4.7 26.4 \u00b1 3.2\n2.4 \u00b1 2.0 52.0 \u00b1 0.0 88.0 \u00b1 4.4\nRVT (ours)\n100.0 \u00b1 0.0 49.6 \u00b1 3.2\n88.0 \u00b1 5.7\n91.2 \u00b1 3.0 48.0 \u00b1 5.7 81.6 \u00b1 5.4 36.0 \u00b1 2.5 28.8 \u00b1 3.9 26.4 \u00b1 8.2 72.0 \u00b1 0.0 93.6 \u00b1 4.1\nTable 1: Multi-Task Performance on RLBench. RVT outperforms state-of-the-art methods while\nbeing faster to train and execute. RVT has the best success rate and rank when averaged across all\ntasks. Performance for Image-BC (CNN), Image-BC (ViT) and C2F-ARM-BC are as reported by\nShridhar et al. in [6]. We re-evalaute PerAct using the released final model and estimate mean and\nvariance. RVT is 2.3X faster on execution speed than PerAct and outpeforms it on 16/18 tasks. The\ntraining time and inference speed of PerAct and RVT are measured on the same GPU model.\nwe use global features (G). The global features are a concatenation of (1) the sum of image features\nalong the spatial dimensions, weighted by the predicted translation heatmap; and (2) the max-pooled\nimage features along the spatial dimension. Specifically, let fi be the image feature and hi be\nthe predicted translation heatmap for the ith image. Then the global feature G is given by G =\n[\u03d5(f1 \u2299 h1); \u00b7 \u00b7 \u00b7 ; \u03d5(fK \u2299 hK); \u03c8(f1); \u00b7 \u00b7 \u00b7 ; \u03c8(fK)], where K is the number of images, \u2299 denotes\nelement-wise multiplication, and \u03d5 and \u03c8 denote the sum and max-pooling over the height and\nwidth dimensions. The weighted sum operation provides higher weights to image locations near the\npredicted end effector position.\nLoss Function. We train RVT using a mixture of losses. For heatmaps, we use the cross-entropy\nloss for each image. The ground truth is obtained by a truncated Gaussian distribution around the\n2D projection of the ground-truth 3D location. For rotation, we use the cross-entropy loss for each\nof the Euler angles. We use binary classification loss for the gripper state and collision indicator.\n4\nExperiments\n4.1\nSimulation Experiments\nSimulation Setup. We follow the simulation setup in PerAct [6], where CoppelaSim [59] is applied\nto simulate various RLBench [7] tasks. A Franka Panda robot with a parallel gripper is controlled\nto complete the tasks. We test on the same 18 tasks as PerAct, including picking and placing, tool\nuse, drawer opening, and high-accuracy peg insertions (see the appendix for a detailed specification\nof each task). Each task includes several variations specified by the associated language description.\nSuch a wide range of tasks and intra-task variations requires the model to not just specialize in one\nspecific skill but rather learn different skill categories. The visual observations are captured from\nfour noiseless RGB-D cameras positioned at the front, left shoulder, right shoulder, and wrist with a\nresolution of 128\u00d7128. To achieve the target gripper pose, we generate joint space actions by using\nthe same sampling-based motion planner [60, 61] as in [5, 6].\nBaselines. We compare against the following three baselines: (1) Image-BC [2] is an image-to-\naction behavior cloning agent that predicts action based on the image observations from the sensor\ncamera views. We compare with two variants with CNN and ViT vision encoders respectively. (2)\nC2F-ARM-BC [5] is a behavior cloning agent that converts the RGB-D images into multi-resolution\nvoxels and predicts the next key-frame action using a coarse-to-fine scheme. (3) PerAct [6] is the\nstate-of-the-art multi-task behavior cloning agent that encodes the RGB-D images into voxel grid\npatches and predicts discretized next key-frame action using the perceiver [8] transformer.\nTraining and Evaluation Details. Just like the baselines, we use the RLBench training dataset with\n100 expert demonstrations per task (1800 demonstrations over all tasks). Similar to PerAct, we apply\n5\na. Cube\nc. Rotated Cube 15\u00b0\nd. RLBench Real \nb. Cube - 3 Views\ne. Perspective Proj.\nf. Orthographic Proj.\nFigure 3: We evaluate RVT with various camera locations for re-rendering (a-d) and find that loca-\ntions in (a) perform best. We also test various projection options (e-f) for rendering images and find\nthat RVT works better with orthographic images.\ntranslation and rotation data augmentations. For translation, we randomly perturb the point clouds in\nthe range [\u00b1 0.125m, \u00b1 0.125m, \u00b1 0.125m]. For rotation, we randomly rotate the point cloud around\nthe z-axis (vertical) in the range of \u00b1 45\u25e6. We train RVT for 100k steps, using the LAMB [62]\noptimizer as PerAct. We use a batch size of 24 and an initial learning rate of 2.4 \u00d7 10\u22124. We use\ncosine learning rate decay with warm-start for 2K steps.\nFor Image-BC and C2F-ARM-BC, we adopt the evaluation results from [6] since their trained mod-\nels have not been released. These results overestimate the performance of Image-BC and C2F-ARM-\nBC, as they select the best model for each of the 18 tasks independently based on the performance\non validation sets. Hence, the reported performance does not reflect a single multi-task model. Nev-\nertheless, these baselines still underperform both PerAct and RVT (see Tab. 1). For PerAct, we\nevaluate the final model released by Shridhar et al. [6]. We test our models (including the models\nin the ablation study, Tab. 2 (left)) and PerAct on the same 25 variations for each task. Due to the\nrandomness of the sampling-based motion planner, we run each model five times on the same 25\nvariations for each task and report the average success rate and standard deviation in Tab. 1.\nTo fairly compare the training efficiency against PerAct, we train both PerAct and our model with\nthe same GPU type (NVIDIA Tesla V100) and number of GPUs (8), as reported by Shridhar et al.\n[6]. We report the total training time for both models in Tab. 1 (\u201cTraining time\u201d). We also evaluate\nthe inference speed of PerAct and RVT models by running the prediction inferences for the same\ninput data on the same GPU (NVIDIA RTX 3090).\nMulti-Task Performance. Tab. 1 compares the performance between RVT and the baselines. We\nfind that PerAct and RVT perform significantly better than the rest. Overall, RVT outperforms\nall baselines with the best rank and success rate when averaged across all tasks. It outperforms\nprior state-of-the-art methods, C2F-ARM, by 42 percentage points (213% relative improvement);\nand PerAct by 13 percentage points (26% relative improvement). RVT outperforms PerAct on\n88.9% (16/18) of the tasks. More remarkably, RVT trains 36X faster than PerAct for achieving the\nsame performance (see Fig. 1). We also observe that at inference time, RVT is 2.3X faster than\nPerAct. These results demonstrate that RVT is both more accurate and scalable when compared to\nexisting state-of-the-art voxel-based methods. More visualizations of the task setups and the model\nperformance are also provided.1\nAblation Study. We conduct ablation experiments to analyze different design choices of RVT: (a)\nthe resolution of the rendered images (\u201cIm. Res.\u201d column in Tab. 2 (left)); (b) whether to include\nthe correspondence information across rendered images (\u201cView Corr.\u201d); (c) whether to include the\ndepth channel (\u201cDep. Ch.\u201d); (d) whether to separately process the tokens of each image before\njointly processing all tokens (\u201cSep. Proc.\u201d); (e) the projection type for rendering\u2014perspective or\northographic (\u201cProj. Type\u201d); (f) whether to use rotation augmentation (\u201cRot. Aug.\u201d); (g) the number\nof views and camera locations for re-rendering (\u201c# of View\u201d and \u201cCam. Loc.\u201d); and (h) the benefit\nof using re-rendered images versus using real sensor camera images (\u201cReal\u201d for \u201cCam. Loc.\u201d).\nTab. 2 (left) summarizes the ablation experiment results. The same table along with the mean and\nstandard deviation for each task can be found in the appendix Tab. 4. Below we discuss the findings:\n(a) As expected, virtual images rendered at higher resolution help as RVT with virtual image reso-\nlution 220 outperforms the one with 100.\n6\nIm.\nView\nDep.\nSep.\nProj.\nRot.\nCam\n# of\nAvg.\nRes.\nCorr.\nCh.\nProc.\nType\nAug.\nLoc.\nView\nSucc.\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n62.9\n100\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n51.1\n220\n\u2717\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n59.7\n220\n\u2713\n\u2717\n\u2713\nOrth.\n\u2713\nCube\n5\n60.3\n220\n\u2713\n\u2713\n\u2717\nOrth.\n\u2713\nCube\n5\n58.4\n220\n\u2713\n\u2713\n\u2713\nPers.\n\u2713\nCube\n5\n40.2\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2717\nCube\n5\n60.4\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n3\n60.2\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nFront\n1\n35.8\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nRot. 15\n5\n59.9\n220\n\u2713\n\u2713\n\u2713\nPers.\n\u2717\nReal\n4\n10.4\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2717\nReal\n4\n22.9\n# of\n# of\n# of\nSucc.\nSucc.\nTask\nvari.\ntrain\ntest\n(+ mark.)\n(- mark.)\nStack\n3\n14\n10\n100%\n100%\nblocks\nPress\nsanitizer\n1\n7\n10\n80%\n80%\nPut marker\n4\n12\n10\n0%\n\u2013\nin mug/bowl\nPut object\nin drawer\n3\n10\n10\n50%\n100%\nPut object\n2\n8\n10\n50%\n50%\nin shelf\nAll tasks\n13\n51\n50\n56%\n82.5%\nTable 2: Left: Ablations on RLBench. A larger res., adding view correspondence, adding depth\nchannel, separating initial attention layers, orthographic projection, using rotation aug., and re-\nrendered views around cube improve the performance. Right: Results of the real-world experi-\nments. A single RVT model can perform well on most tasks with only a few demonstrations.\n(b) Adding correspondence information for points across different views helps (see Sec. 3). This\nis likely because the network need not learn to solve the correspondence problem and can predict\nmore consistent heatmaps across views. Note that the view correspondence channel is not present in\nsensor images but is rendered along with RGB(D) images in RVT.\n(c) Adding the depth channel along with RGB channels helps, likely because it aids 3D reasoning.\n(d) Independently processing the tokens from a single image, before merging all the image tokens,\nhelps. It is likely because this design expects the network to extract meaningful features for each\nimage before reasoning over them jointly.\n(e) Rendering images with orthographic projection performs better than rendering with perspective\nprojection, for both the cube and real camera locations. We hypothesize that it is because ortho-\ngraphic projection preserves the shape and size of an object regardless of its distance from the\ncamera (see Fig. 3 (e-f)). It also highlights the advantage of re-rendering, as real sensors generally\nrender with perspective projections.\n(f) As expected, using 3D rotation augmentation in the point cloud before rendering helps. To take\nadvantage of 3D augmentations, the re-rendering process is necessary.\n(g) The model with 5 views around a cube (Fig. 3 (a)) performs the best followed by the one with\n3 views (front, top, left) around a cube (Fig. 3 (b)). The single view model, where we predict the\nthird coordinate as an offset like TransporterNet [4], performs substantially worse, calling for the\nneed for multiple views for 3D manipulation. It also highlights the advantage of re-rendering as with\nre-rendering we can leverage multiple views even with a single sensor camera. We also empirically\nfind that rotating the location of the cameras by 15\u25e6 (see Fig. 3) with respect to the table (and robot)\ndecreases performance. This could be likely because views aligned with the table and robot might\nbe easier to reason with (e.g., overhead top view, aligned front view).\n(h) RVT performs better with re-rendered images as compared to using sensor camera images (Tab. 2\n(left), second last row). The sensor camera images are rendered with perspective projection (physical\nrendering process) and are not straightforward to apply 3D augmentations (e.g., rotation) without\nre-rendering. Also, the location of sensor cameras may be sub-optimal for 3D reasoning, e.g., the\nviews are not axially aligned with the table or robot (see Fig. 3 (d)). All these factors contribute to\nRVT performing better with re-rendered images than with sensor camera images.\nNotably, one might consider rearranging the sensor cameras to match the re-rendering views in order\nto bypass re-rendering. However, this will void the gains from using orthographic projections, 3D\naugmentation, and adding correspondences. This also strictly requires a multi-camera setup (Fig. 3\n(a)), which is more costly and less portable in the real world than using one sensor camera. Finally,\nwe have briefly explored view selection and found an option that works well across tasks. Further\noptimization of views, including the sensor and re-rendered ones, is an interesting future direction.\n7\nPut orange bottle in top drawer  \nPut yellow cube in the bottom shelf \nFigure 4: Examples of RVT in the real world. A single RVT model can perform multiple tasks (5\ntasks, 13 variations) in the real world with just \u223c10 demonstrations per task.\n4.2\nReal-World\nWe study the performance of RVT on real visual sensory data by training and testing the model on a\nreal-world setup. See the attached videos1 for more details about the setup and model performance.\nReal World Setup. We experiment on a table-top setup using a statically mounted Franka Panda\narm. The scene is perceived via an Azure Kinect (RGB-D) camera statically mounted in a third-\nperson view. We calibrate the robot-camera extrinsics and transform the perceived point clouds\nto the robot base frame before passing into RVT. Given a target gripper pose from RVT, we use\nFrankaPy [63] to move the robot to the target with trajectory generation and feedback control.\nTasks. We adopt a total of 5 tasks similar to the ones in PerAct [6] (see Tab. 2 (right)): stack\nblocks, press sanitizer, put marker in mug/bowl, put object in drawer, put object in shelf. Each task\ncan be instantiated with different variations defined by the language description. For example, for\nstack blocks, some variations could be \u201cput yellow block on blue block\u201d and \u201cput blue block on red\nblock\u201d. Given a task and variation, we sample a scene by placing the task-related objects and a set\nof distractor objects on the table in a random configuration.\nData Collection. We first collect a dataset for training RVT through human demonstration. Given\na sampled task and scene configuration, we ask the human demonstrator to specify a sequence of\ngripper target poses by kinesthetically moving the robot arm around. Once we have the target pose\nsequence, we reset the robot to the start pose, and then control it to sequentially move to each target\npose following the specified order. We simultaneously record the RGB-D stream from the camera\nduring the robot\u2019s motion to the targets. This provides us with a dataset of RGB-D frames paired\nwith target pose annotations. In total, we collected 51 demonstration sequences over all 5 tasks.\nResults. We train on real-world data for 10K steps, with the same optimizer, batch size, and learning\nrate schedule as the simulation data. We report the results in Tab. 2 (right). Overall, RVT achieves\nhigh success rates for the stack block task (100%) and the press sanitizer task (80%). Even on\nlonger horizon tasks such as putting objects in drawers and shelves (e.g., the robot has to first open\nthe drawer/shelf and then pick up the object), our model achieves 50% success rates (see Fig. 4).\nWe found RVT struggled with marker-related tasks, which is likely due to sparse and noisily sensed\npoint clouds. We further divide the results into two sets: \u201c+ markers\u201d (full set) and \u201c- markers\u201d. Our\nmodel overall achieves an 82.5% success rate on non-marker tasks. The marker issue can potentially\nbe addressed by attaching the camera to the gripper to capture point clouds at higher quality. Another\npossibility is to use zoom-in views similar to C2F-ARM [5].\n5\nConclusions and Limitations\nWe proposed RVT, a multi-view transformer model for 3D object manipulation. We found that RVT\noutperforms prior state-of-the-art models like PerAct and C2F-ARM on a variety of 3D manipula-\ntion tasks, while being more scalable and faster. We also found that RVT can work on real-world\nmanipulation tasks with only a few demonstrations.\nAlthough we found RVT to achieve state-of-the-art results, we identify some limitations that present\nexciting directions for future research. We briefly explore various view options and found an option\nthat works well across tasks, but it would be exciting if view specification can be optimized or\n1Videos are provided at https://robotic-view-transformer.github.io/.\n8\nlearned from data. Further, when compared to prior view-based methods, RVT (as well as explicit\nvoxel-based methods like PerAct and C2F-ARM), requires the calibration of extrinsics from the\ncamera to the robot base. It would be exciting to explore extensions that remove this constraint.\nReferences\n[1] A. Goyal, A. Mousavian, C. Paxton, Y.-W. Chao, B. Okorn, J. Deng, and D. Fox. Ifor: Iterative\nflow minimization for robotic object rearrangement. In arXiv:2202.00732, 2022.\n[2] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. BC-z:\nZero-shot task generalization with robotic imitation learning. In 5th Annual Conference on\nRobot Learning, 2021. URL https://openreview.net/forum?id=8kbp23tSGYv.\n[3] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipu-\nlation. In Proceedings of the 5th Conference on Robot Learning (CoRL), 2021.\n[4] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin,\nD. Duong, V. Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic\nmanipulation. In Conference on Robot Learning, pages 726\u2013747. PMLR, 2021.\n[5] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient learning\nfor visual robotic manipulation via discretisation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13739\u201313748, 2022.\n[6] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-Actor: A multi-task transformer for robotic\nmanipulation. In CoRL, 2022.\n[7] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark &\nlearning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020.\n[8] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General\nperception with iterative attention. In International conference on machine learning, pages\n4651\u20134664. PMLR, 2021.\n[9] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki,\nA. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation.\nThe International Journal of Robotics Research, 39(1):3\u201320, 2020.\n[10] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter. Learn-\ning agile and dynamic motor skills for legged robots. Science Robotics, 4(26):eaau5872, 2019.\n[11] A. Kumar, Z. Fu, D. Pathak, and J. Malik. Rma: Rapid motor adaptation for legged robots.\narXiv preprint arXiv:2107.04034, 2021.\n[12] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[13] J. Xu, T. Du, M. Foshey, B. Li, B. Zhu, A. Schulz, and W. Matusik. Learning to fly: compu-\ntational controller design for hybrid uavs with reinforcement learning. ACM Transactions on\nGraphics (TOG), 38(4):1\u201312, 2019.\n[14] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Ma-\nlik, R. Mottaghi, M. Savva, et al. On evaluation of embodied navigation agents. arXiv preprint\narXiv:1807.06757, 2018.\n[15] A. Goyal and J. Deng. Packit: A virtual environment for geometric planning. In International\nConference on Machine Learning, pages 3700\u20133710. PMLR, 2020.\n9\n[16] D. Shah, B. Eysenbach, G. Kahn, N. Rhinehart, and S. Levine. Ving: Learning open-world\nnavigation with visual goals. In 2021 IEEE International Conference on Robotics and Automa-\ntion (ICRA), pages 13215\u201313222. IEEE, 2021.\n[17] R. Yang, M. Zhang, N. Hansen, H. Xu, and X. Wang. Learning vision-guided quadrupedal lo-\ncomotion end-to-end with cross-modal transformers. arXiv preprint arXiv:2107.03996, 2021.\n[18] W. Huang, I. Mordatch, P. Abbeel, and D. Pathak. Generalization in dexterous manipulation\nvia geometry-aware multi-task learning. arXiv preprint arXiv:2111.03062, 2021.\n[19] T. Chen, J. Xu, and P. Agrawal. A system for general in-hand object re-orientation. Conference\non Robot Learning, 2021.\n[20] W. Yu, D. Jain, A. Escontrela, A. Iscen, P. Xu, E. Coumans, S. Ha, J. Tan, and T. Zhang. Visual-\nlocomotion: Learning to walk on complex terrains with vision. In 5th Annual Conference on\nRobot Learning, 2021. URL https://openreview.net/forum?id=NDYbXf-DvwZ.\n[21] D. Shah, A. Bhorkar, H. Leen, I. Kostrikov, N. Rhinehart, and S. Levine. Offline reinforcement\nlearning for visual navigation. In 6th Annual Conference on Robot Learning, 2022. URL\nhttps://openreview.net/forum?id=uhIfIEIiWm_.\n[22] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent\nimagination. arXiv preprint arXiv:1912.01603, 2019.\n[23] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally\nlinear latent dynamics model for control from raw images. Advances in neural information\nprocessing systems, 28, 2015.\n[24] D. Yarats, I. Kostrikov, and R. Fergus. Image augmentation is all you need: Regularizing deep\nreinforcement learning from pixels. In International Conference on Learning Representations,\n2021. URL https://openreview.net/forum?id=GY6-6sTvGaf.\n[25] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Improved\ndata-augmented reinforcement learning. In International Conference on Learning Representa-\ntions, 2022. URL https://openreview.net/forum?id=_SJ-_yyes8.\n[26] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi, R. Ju-\nlian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath,\nI. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao,\nM. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran,\nV. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-\n1: Robotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817,\n2022.\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\n[28] Y.-C. Lin, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov, A. R. Garcia, and\nP. Isola. Mira: Mental imagery for robotic affordances. In K. Liu, D. Kulic, and J. Ichnowski,\neditors, Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of\nMachine Learning Research, pages 1916\u20131927. PMLR, 14\u201318 Dec 2023.\n[29] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. The International\nJournal of Robotics Research, 34(4-5):705\u2013724, 2015.\n[30] A. Saxena, J. Driemeyer, J. Kearns, and A. Ng. Robotic grasping of novel objects. Advances\nin neural information processing systems, 19, 2006.\n10\n[31] M. P. Deisenroth, P. Englert, J. Peters, and D. Fox. Multi-task policy search for robotics. In\n2014 IEEE international conference on robotics and automation (ICRA), pages 3876\u20133881.\nIEEE, 2014.\n[32] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and\nK. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv\npreprint arXiv:2104.08212, 2021.\n[33] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network\npolicies for multi-task and multi-robot transfer. In 2017 IEEE international conference on\nrobotics and automation (ICRA), pages 2169\u20132176. IEEE, 2017.\n[34] Y. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pas-\ncanu. Distral: Robust multitask reinforcement learning. Advances in neural information pro-\ncessing systems, 30, 2017.\n[35] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakr-\nishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J.\nRuano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee,\nS. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes,\nP. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu,\nM. Yan, and A. Zeng. Do as i can and not as i say: Grounding language in robotic affordances.\nIn arXiv preprint arXiv:2204.01691, 2022.\n[36] A. Goyal, K. Yang, D. Yang, and J. Deng. Rel3d: A minimally contrastive benchmark for\ngrounding spatial relations in 3d. Advances in Neural Information Processing Systems, 33:\n10514\u201310525, 2020.\n[37] C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data.\narXiv preprint arXiv:2005.07648, 2020.\n[38] O. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imitation\nlearning over unstructured data. IEEE Robotics and Automation Letters, 7(4):11205\u201311212,\n2022.\n[39] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn, et al. Learning language-conditioned robot\nbehavior from offline data and crowd-sourced annotation. In Conference on Robot Learning,\npages 1303\u20131315. PMLR, 2022.\n[40] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez,\nY. Sulsky, J. Kay, J. T. Springenberg, et al.\nA generalist agent.\narXiv preprint\narXiv:2205.06175, 2022.\n[41] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and\nA. Garg. Progprompt: Generating situated robot task plans using large language models. ICRA,\n2022.\n[42] D. S. Chaplot, D. Pathak, and J. Malik. Differentiable spatial planning using transformers. In\nInternational Conference on Machine Learning, pages 1484\u20131495. PMLR, 2021.\n[43] H. M. Clever, A. Handa, H. Mazhar, K. Parker, O. Shapira, Q. Wan, Y. Narang, I. Akinola,\nM. Cakmak, and D. Fox. Assistive tele-op: Leveraging transformers to collect robotic task\ndemonstrations. arXiv preprint arXiv:2112.05129, 2021.\n[44] J. J. Johnson, L. Li, A. H. Qureshi, and M. C. Yip. Motion planning transformers: One model\nto plan them all. arXiv preprint arXiv:2106.02791, 2021.\n[45] T. Cachet, J. Perez, and S. Kim. Transformer-based meta-imitation learning for robotic manip-\nulation. In Neural Information Processing Systems, Workshop on Robot Learning, 2020.\n11\n[46] H. Kim, Y. Ohmura, and Y. Kuniyoshi. Transformer-based deep imitation learning for dual-\narm robot manipulation. In 2021 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 8965\u20138972. IEEE, 2021.\n[47] S. Dasari and A. Gupta. Transformers for one-shot visual imitation. In Conference on Robot\nLearning, pages 2071\u20132084. PMLR, 2021.\n[48] R. Jangir, N. Hansen, S. Ghosal, M. Jain, and X. Wang. Look closer: Bridging egocentric and\nthird-person views with transformers for robotic manipulation. IEEE Robotics and Automation\nLetters, 7(2):3046\u20133053, 2022.\n[49] W. Liu, C. Paxton, T. Hermans, and D. Fox.\nStructformer: Learning spatial structure for\nlanguage-guided semantic rearrangement of novel objects. In 2022 International Conference\non Robotics and Automation (ICRA), pages 6322\u20136329. IEEE, 2022.\n[50] A. Goyal, H. Law, B. Liu, A. Newell, and J. Deng. Revisiting point cloud shape classification\nwith a simple and effective baseline. In International Conference on Machine Learning, pages\n3809\u20133820. PMLR, 2021.\n[51] A. Hamdi, S. Giancola, and B. Ghanem. Mvtn: Multi-view transformation network for 3d\nshape recognition. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 1\u201311, 2021.\n[52] A. Hamdi, S. Giancola, and B. Ghanem. Voint cloud: Multi-view point cloud representation\nfor 3d understanding. arXiv preprint arXiv:2111.15363, 2021.\n[53] S. Huang, Y. Chen, J. Jia, and L. Wang. Multi-view transformer for 3d visual grounding. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n15524\u201315533, 2022.\n[54] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthesis. Communications of the ACM,\n65(1):99\u2013106, 2021.\n[55] E. Johns. Coarse-to-fine imitation learning: Robot manipulation from a single demonstration.\nIn 2021 IEEE international conference on robotics and automation (ICRA), pages 4613\u20134619.\nIEEE, 2021.\n[56] N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson, and G. Gkioxari. Ac-\ncelerating 3d deep learning with pytorch3d. arXiv:2007.08501, 2020.\n[57] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n[58] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-\nhghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transform-\ners for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[59] E. Rohmer, S. P. Singh, and M. Freese.\nV-rep: A versatile and scalable robot simulation\nframework. In 2013 IEEE/RSJ international conference on intelligent robots and systems,\npages 1321\u20131326. IEEE, 2013.\n[60] G. S\u00b4anchez and J.-C. Latombe. A single-query bi-directional probabilistic roadmap planner\nwith lazy collision checking. In Int. Symp. Robotics Research, pages 403\u2013417, 2001.\n[61] I. A. Sucan, M. Moll, and L. E. Kavraki. The open motion planning library. IEEE Robotics &\nAutomation Magazine, 19(4):72\u201382, 2012.\n12\n[62] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer,\nand C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv\npreprint arXiv:1904.00962, 2019.\n[63] K. Zhang, M. Sharma, J. Liang, and O. Kroemer. A modular robotic arm control stack for\nresearch: Franka-Interface and FrankaPy. arXiv preprint arXiv:2011.02398, 2020.\n13\n6\nAppendix\n6.1\nRLBench Tasks\nWe provide a brief summary of the RLBench tasks in Tab. 3. There are 18 tasks with 249 variations.\nFor more detailed description of each task, please refer to PerAct [6], Appendix A.\nTask\nLanguage Template\n# of Variations\nopen drawer\n\u201copen the\ndrawer\u201d\n3\nslide block\n\u201cslide the\nblock to target\u201d\n4\nsweep to dustpan\n\u201csweep dirt to the\ndustpan\u201d\n2\nmeat off grill\n\u201ctake the\noff the grill\u201d\n2\nturn tap\n\u201cturn\ntap\u201d\n2\nput in drawer\n\u201cput the item in the\ndrawer\u201d\n3\nclose jar\n\u201cclose the\njar\u201d\n20\ndrag stick\n\u201cuse the stick to drag the cube onto the\ntarget\u201d\n20\nstack blocks\n\u201cstack\nblocks\u201d\n60\nscrew bulb\n\u201cscrew in the\nlight bulb\u201d\n20\nput in safe\n\u201cput the money away in the safe on the\nshelf\u201d\n3\nplace wine\n\u201cstack the wine bottle to the\nof the rack\u201d\n3\nput in cupboard\n\u201cput the\nin the cupboard\u201d\n9\nsort shape\n\u201cput the\nin the shape sorter\u201d\n5\npush buttons\n\u201cpush the\nbutton, [then the\nbutton]\u201d\n50\ninsert peg\n\u201cput the\npeg in the spoke\u201d\n20\nstack cups\n\u201cstack the other cups on top of the\ncup\u201d\n20\nplace cups\n\u201cplace\ncups on the cup holder\u201d\n3\nTable 3: Tasks in RLBench We evaluate on 18 RLBench tasks which are same as those used\nin PerAct [6]. For more details, check see PerAct [6], Appendix A. For videos, visit https:\n//corlrvt.github.io/\n14\n6.2\nRVT Overview\nInsert peg in the  \nblue spoke\nVirtual Image 1\nVirtual Image 2\nVirtual Image 5\nPatchify\nProjection\nAttention X 4\nAttention X 4\nRotation\n0\n1\n0\n1\nGripper \nCollision\nCLIP\nMLP\nFigure 5: Overview of the transformer used in RVT. The input to the transformer is a language\ndescription of the task and virtual images of the scene point cloud. The text is converted into token\nembeddings using the pretrained CLIP [57] model, while the virtual images are converted into token\nembeddings via patchify and projection operations. For each virtual image, tokens belonging to the\nsame image are processed via four attention layers. Finally, the processed image tokens as well as\nthe language tokens are jointly processed using four attention layers. The 3D action is inferred using\nthe resulting image tokens.\n15\n6.3\nAblations\nWe report the ablations mentioned in Tab. 2, along with the mean and standard deviations for each\ntask Tab. 4.\nIm.\nView\nDep.\nBi-\nProj.\nRot.\nCam\n# of\nAvg.\nClose\nDrag\nInsert\nMeat off\nOpen\nPlace\nRes.\nCorr.\nCh.\nLev.\nType\nAug.\nLoc.\nView\nSucc.\nJar\nStick\nPeg\nGrill\nDrawer\nCups\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n62.9\n52 \u00b1 2.5\n99.2 \u00b1 1.6\n11.2 \u00b1 3\n88 \u00b1 2.5\n71.2 \u00b1 6.9\n4 \u00b1 2.5\n100\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n51.1\n60 \u00b1 0\n83 \u00b1 1.7\n4 \u00b1 2.8\n91 \u00b1 3.3\n67 \u00b1 5.2\n1 \u00b1 1.7\n220\n\u2717\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n59.7\n44 \u00b1 0\n100 \u00b1 0\n17 \u00b1 4.4\n90 \u00b1 6\n71 \u00b1 9.1\n7 \u00b1 5.9\n220\n\u2713\n\u2717\n\u2713\nOrth.\n\u2713\nCube\n5\n60.3\n37 \u00b1 3.3\n96 \u00b1 0\n11 \u00b1 3.3\n97 \u00b1 1.7\n57 \u00b1 8.2\n3 \u00b1 3.3\n220\n\u2713\n\u2713\n\u2717\nOrth.\n\u2713\nCube\n5\n58.4\n32 \u00b1 7.5\n96 \u00b1 0\n11 \u00b1 3.3\n90 \u00b1 2\n68 \u00b1 2.8\n2 \u00b1 2\n220\n\u2713\n\u2713\n\u2713\nPers.\n\u2713\nCube\n5\n40.2\n20 \u00b1 2.5\n90.4 \u00b1 2\n4 \u00b1 0\n84.8 \u00b1 4.7\n13.6 \u00b1 4.8\n2.4 \u00b1 2\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2717\nCube\n5\n60.4\n52 \u00b1 0\n92 \u00b1 0\n12.8 \u00b1 1.6\n97.6 \u00b1 4.8\n85.6 \u00b1 5.4\n0 \u00b1 0\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n3\n60.2\n44.8 \u00b1 1.6\n75.2 \u00b1 4.7\n15 \u00b1 3.3\n89.6 \u00b1 4.1\n68.8 \u00b1 9.3\n3.2 \u00b1 1.6\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nFront\n1\n35.8\n36 \u00b1 4.9\n87 \u00b1 1.7\n2 \u00b1 2\n90 \u00b1 6\n58 \u00b1 6.6\n0 \u00b1 0\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nRot. 15\n5\n59.9\n48.8 \u00b1 1.6\n99.2 \u00b1 1.6\n12 \u00b1 4.4\n80 \u00b1 2.5\n71.2 \u00b1 9.3\n0 \u00b1 0\n220\n\u2713\n\u2713\n\u2713\nPers.\n\u2717\nReal\n4\n10.4\n14.4 \u00b1 6.5\n14.4 \u00b1 5.4\n0 \u00b1 0\n0 \u00b1 0\n22.4 \u00b1 5.4\n0 \u00b1 0\n220\n\u2713\n\u2713\n\u2713\nOrtho.\n\u2717\nReal\n4\n22.9\n43.2 \u00b1 4.7\n54.4 \u00b1 3.2\n0 \u00b1 0\n0 \u00b1 0\n15.2 \u00b1 5.3\n0.8 \u00b1 1.6\nIm.\nView\nDep.\nBi-\nProj.\nRot.\nCam\n# of\nAvg.\nPlace\nPush\nPut in\nPut in\nPut in\nScrew\nRes.\nCorr.\nCh.\nLev.\nType\nAug.\nLoc.\nView\nSucc.\nWine\nButtons\nCupboard\nDrawer\nSafe\nBulb\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n62.9\n91 \u00b1 5.2\n100 \u00b1 0\n49.6 \u00b1 3.2\n88 \u00b1 5.7\n91.2 \u00b1 3\n48 \u00b1 5.7\n100\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n51.1\n38 \u00b1 8.7\n100 \u00b1 0\n49 \u00b1 4.4\n86 \u00b1 2\n77 \u00b1 1.7\n22 \u00b1 4.5\n220\n\u2717\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n59.7\n96 \u00b1 2.8\n99 \u00b1 1.7\n48 \u00b1 6.9\n50 \u00b1 6\n79 \u00b1 5.9\n36 \u00b1 0\n220\n\u2713\n\u2717\n\u2713\nOrth.\n\u2713\nCube\n5\n60.3\n71 \u00b1 1.7\n99 \u00b1 1.7\n56 \u00b1 0\n92 \u00b1 4.9\n77 \u00b1 3.3\n39 \u00b1 4.4\n220\n\u2713\n\u2713\n\u2717\nOrth.\n\u2713\nCube\n5\n58.4\n65 \u00b1 5.2\n100 \u00b1 0\n54 \u00b1 2\n94 \u00b1 4.5\n78 \u00b1 3.5\n48 \u00b1 6.3\n220\n\u2713\n\u2713\n\u2713\nPers.\n\u2713\nCube\n5\n40.2\n28 \u00b1 5.7\n91.2 \u00b1 1.6\n26.4 \u00b1 2\n64.8 \u00b1 3\n51.2 \u00b1 3.9\n20 \u00b1 4.4\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2717\nCube\n5\n60.4\n84 \u00b1 3.6\n96 \u00b1 2.5\n40 \u00b1 2.5\n88 \u00b1 7.2\n90.4 \u00b1 4.1\n48 \u00b1 8.4\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n3\n60.2\n84.8 \u00b1 8.9\n97.6 \u00b1 2\n40.8 \u00b1 4.7\n94.4 \u00b1 4.1\n82.4 \u00b1 7.8\n43.2 \u00b1 3.9\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nFront\n1\n35.8\n82 \u00b1 4.5\n46 \u00b1 2\n14 \u00b1 4.5\n29 \u00b1 7.1\n57 \u00b1 5.9\n6 \u00b1 2\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nRot. 15\n5\n59.9\n74.4 \u00b1 5.4\n99.2 \u00b1 1.6\n46.4 \u00b1 4.1\n81.6 \u00b1 2\n80.8 \u00b1 4.7\n45.6 \u00b1 4.8\n220\n\u2713\n\u2713\n\u2713\nPers.\n\u2717\nReal\n4\n10.4\n11.2 \u00b1 3.9\n26.4 \u00b1 4.1\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\n220\n\u2713\n\u2713\n\u2713\nOrtho.\n\u2717\nReal\n4\n22.9\n67.2 \u00b1 5.9\n76 \u00b1 5.7\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\nIm.\nView\nDep.\nBi-\nProj.\nRot.\nCam\n# of\nAvg.\nSlide\nSort\nStack\nStack\nSweep to\nTurn\nRes.\nCorr.\nCh.\nLev.\nType\nAug.\nLoc.\nView\nSucc.\nBlock\nShape\nBlocks\nCups\nDustpan\nTap\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n62.9\n81.6 \u00b1 5.4\n36 \u00b1 2.5\n28.8 \u00b1 3.9\n26.4 \u00b1 8.2\n72 \u00b1 0\n93.6 \u00b1 4.1\n100\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n51.1\n93 \u00b1 3.3\n18 \u00b1 2\n17 \u00b1 5.2\n1 \u00b1 1.7\n36 \u00b1 0\n76 \u00b1 2.8\n220\n\u2717\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n5\n59.7\n83 \u00b1 1.7\n41 \u00b1 4.4\n26.7 \u00b1 5\n20 \u00b1 4.9\n72 \u00b1 0\n95 \u00b1 4.4\n220\n\u2713\n\u2717\n\u2713\nOrth.\n\u2713\nCube\n5\n60.3\n72 \u00b1 4\n37 \u00b1 5.2\n23 \u00b1 3.3\n33 \u00b1 5.9\n92 \u00b1 0\n95 \u00b1 4.4\n220\n\u2713\n\u2713\n\u2717\nOrth.\n\u2713\nCube\n5\n58.4\n66 \u00b1 6\n31 \u00b1 6.6\n25 \u00b1 3.3\n29 \u00b1 5.2\n72 \u00b1 0\n91 \u00b1 3.3\n220\n\u2713\n\u2713\n\u2713\nPers.\n\u2713\nCube\n5\n40.2\n88 \u00b1 4.4\n19.2 \u00b1 4.7\n22.4 \u00b1 9\n1.6 \u00b1 2\n16 \u00b1 0\n80.8 \u00b1 3\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2717\nCube\n5\n60.4\n72.8 \u00b1 1.6\n25.6 \u00b1 2\n18.4 \u00b1 6\n8.8 \u00b1 5.3\n84 \u00b1 0\n92 \u00b1 2.5\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nCube\n3\n60.2\n95.2 \u00b1 1.6\n37.6 \u00b1 4.1\n29.6 \u00b1 3.2\n8.8 \u00b1 4.7\n80 \u00b1 0\n92.8 \u00b1 3\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nFront\n1\n35.8\n42 \u00b1 2\n2 \u00b1 2\n0 \u00b1 0\n0 \u00b1 0\n0 \u00b1 0\n93 \u00b1 5.2\n220\n\u2713\n\u2713\n\u2713\nOrth.\n\u2713\nRot. 15\n5\n59.9\n83 \u00b1 1.7\n30.4 \u00b1 5.4\n46.4 \u00b1 9.3\n20.8 \u00b1 4.7\n64 \u00b1 0\n94.4 \u00b1 3.2\n220\n\u2713\n\u2713\n\u2713\nPers.\n\u2717\nReal\n4\n10.4\n37.6 \u00b1 10.6\n2.4 \u00b1 3.2\n0.8 \u00b1 1.6\n0 \u00b1 0\n0 \u00b1 0\n56.8 \u00b1 6.9\n220\n\u2713\n\u2713\n\u2713\nOrtho.\n\u2717\nReal\n4\n22.9\n72.8 \u00b1 3\n7.2 \u00b1 1.6\n11.2 \u00b1 4.7\n0 \u00b1 0\n12 \u00b1 0\n53 \u00b1 5.2\nTable 4: Ablations results for RVT on RLBench with metrics for each task.\n16\n"
  },
  {
    "title": "SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models",
    "link": "https://arxiv.org/pdf/2306.14066.pdf",
    "upvote": "1",
    "text": "SEEDS: Emulation of Weather Forecast Ensembles\nwith Diffusion Models\nLizao Li\nGoogle Research\nlizaoli@google.com\nRobert Carver\u2217\nGoogle Research\ncarver@google.com\nIgnacio Lopez-Gomez\u2217\nGoogle Research\nilopezgp@google.com\nFei Sha\u2020\nGoogle Research\nfsha@google.com\nJohn Anderson\nGoogle Research\njanders@google.com\nAbstract\nUncertainty quantification is crucial to decision-making. A prominent example\nis probabilistic forecasting in numerical weather prediction. The dominant ap-\nproach to representing uncertainty in weather forecasting is to generate an ensem-\nble of forecasts. This is done by running many physics-based simulations under\ndifferent conditions, which is a computationally costly process. We propose to\namortize the computational cost by emulating these forecasts with deep genera-\ntive diffusion models learned from historical data. The learned models are highly\nscalable with respect to high-performance computing accelerators and can sample\nhundreds to tens of thousands of realistic weather forecasts at low cost. When\ndesigned to emulate operational ensemble forecasts, the generated ones are sim-\nilar to physics-based ensembles in important statistical properties and predictive\nskill. When designed to correct biases present in the operational forecasting sys-\ntem, the generated ensembles show improved probabilistic forecast metrics. They\nare more reliable and forecast probabilities of extreme weather events more accu-\nrately. While this work demonstrates the utility of the methodology by focusing\non weather forecasting, the generative artificial intelligence methodology can be\nextended for uncertainty quantification in climate modeling, where we believe the\ngeneration of very large ensembles of climate projections will play an increasingly\nimportant role in climate risk assessment.\n1\nIntroduction\nWeather is inherently uncertain, making the assessment of forecast uncertainty a vital component of\noperational weather forecasting [2, 51]. Given a numerical weather prediction (NWP) model, the\nstandard way to quantify this uncertainty is to stochastically perturb the model\u2019s initial conditions\nand its representation of small-scale physical processes to create an ensemble of weather trajecto-\nries [34]. These trajectories are then regarded as Monte Carlo samples of the underlying probability\ndistribution of weather states.\nGiven the computational cost of generating each ensemble member, weather forecasting centers\ncan only afford to generate 10 to 50 members for each forecast cycle [12, 27, 50]. This limitation\nis particularly problematic for users concerned with the likelihood of high-impact extreme or rare\nweather events, which typically requires much larger ensembles to assess [14, 32, 33]. For instance,\none would need a 10,000-member calibrated ensemble to forecast events with 1% probability of\n\u2217Contributed equally\n\u2020to whom correspondence should be addressed\narXiv:2306.14066v3  [cs.LG]  8 Oct 2023\noccurrence with a relative error less than 10%. Large ensembles are even more necessary for fore-\ncasting compound extreme events [4, 27]. Besides relying on increases in available computational\npower to generate larger ensembles in the future, it is imperative to explore more efficient approaches\nfor generating ensemble forecasts.\nIn this context, recent advances in generative artificial intelligence (GAI) offer a potential path to-\nwards massive reductions in the cost of ensemble forecasting. GAI models extract statistical pri-\nors from datasets, and enable conditional and unconditional sampling from the learned probability\ndistributions. Through this mechanism, GAI techniques reduce the cost of ensemble forecast gen-\neration: once learning is complete, the sampling process is far more computationally efficient than\ntime-stepping a physics-based NWP model.\nIn this work, we propose a technique that is based on probabilistic diffusion models, which have\nrecently revolutionized GAI use cases such as image and video generation [10, 24, 36]. Our Scal-\nable Ensemble Envelope Diffusion Sampler (SEEDS) can generate an arbitrarily large ensemble\nconditioned on as few as one or two forecasts from an operational NWP system. We compare the\ngenerated ensembles to ground-truth ensembles from the operational systems, and to ERA5 reanal-\nysis [21]. The generated ensembles not only yield weather-like forecasts but also match or exceed\nphysics-based ensembles in skill metrics such as the rank histogram, the root-mean-squared error\n(RMSE) and continuous ranked probability score (CRPS). In particular, the generated ensembles as-\nsign more accurate likelihoods to the tail of the distribution, such as \u00b12\u03c3 and \u00b13\u03c3 weather events.\nMost importantly, the computational cost of the model is negligible; it has a throughput of 256 en-\nsemble members (at 2\u25e6 resolution) per 3 minutes on Google Cloud TPUv3-32 instances and can\neasily scale to higher throughput by deploying more accelerators. We apply our methodology to\nuncertainty quantification in weather forecasting due to the wealth of data available and the ability\nto validate models on reanalysis. Nevertheless, the same approach could be used to augment climate\nprojection ensembles.\nPrevious work leveraging artificial intelligence to augment and post-process ensemble or determin-\nistic forecasts has focused on improving the aggregate output statistics of the prediction system.\nConvolutional neural networks have been used to learn a global measure of forecast uncertainty\ngiven a single deterministic forecast, trained using as labels either the error of previous forecasts\nor the spread of an ensemble system [40]. This approach has been generalized to predict the en-\nsemble spread at each location of the input deterministic forecast; over both small regions using\nfully connected networks [38], or over the entire globe using conditional generative adversarial\nnetworks [5] based on the PIX2PIX architecture [25]. Deep learning has also proved effective in\ncalibrating limited-size ensembles. For instance, self-attentive transformers can be used to calibrate\nthe ensemble spread [13]. More related to our work, deep learning models have been successfully\nused to correct the probabilistic forecasts of ensemble prediction systems such that their final skill\nexceeds that of pure physics-based ensembles with at least double the number of members [17].\nOur work differs from all previous studies in that our probabilistic generative model outputs high-\ndimensional weather-like samples from the target forecast distribution, akin to generative precipita-\ntion downscaling models [20]. Thus, our approach offers added value beyond improved estimates\nof the ensemble mean and spread: the drawn samples can be used to characterize spatial patterns\nassociated with weather extremes [41], or as input to targeted weather applications that depend on\nvariable and spatial correlations [32].\n2\nMethod\nWe start by framing the learning tasks. We then outline the data and neural network learning algo-\nrithm we use. Details, including background, data processing and preparation, and learning archi-\ntectures and procedures, are presented in Supplementary Information A and B.\n2.1\nSetup\nTo address the computational challenge of generating large weather forecast ensembles, we consider\ntwo learning tasks: generative ensemble emulation and generative post-processing. In both tasks,\nwe are given as inputs a few examples sampled from a probability distribution p(v), where v stands\nfor the atmospheric state variables. In our case, these examples represent physics-based weather\n2\n286\n288\n290\n292\n294\nTemperature (K)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nProbability\nMixture\nERA5-Ensemble\nGEFS-Full\nFigure 1: Illustration of the target distributions of generative ensemble emulation (GEFS-FULL) and\npost-processing (Mixture). Shown are the histograms (bars: frequencies with 12 shared bins, curves:\nGaussian kernel density estimators fit to the bars), i.e., the empirical distributions of the surface\ntemperature near Mountain View, CA on 2021/07/04 in the GEFS and ERA5 ensembles. The goal\ncommon to both tasks is to generate additional ensemble members to capture the statistics of the\ndesired distribution conditioned on a few GEFS samples. Note the small \u201cbump\u201d at the temperature\nof 287K in the mixture distribution.\nforecasts. We seek to generate additional samples that either approximate the same distribution, or\na related desired distribution. The central theme of statistical modeling for both tasks is to construct\na computationally fast and scalable sampler for the target distributions.\nGenerative ensemble emulation leverages K input samples to conditionally generate N > K sam-\nples such that they approximate the original distribution p(v) from which the input samples are\ndrawn. Its main purpose is to augment the ensemble size inexpensively without the need to compute\nand issue more than K physics-based forecasts.\nIn generative post-processing, the sampler generates N > K samples such that they approximate a\nmixture distribution where p(v) is just one of the components. We consider the case where the target\ndistribution is \u03b1p(v) + (1 \u2212 \u03b1)p\u2032(v), with \u03b1 \u2208 [0, 1) being the mixture weight and p\u2032(v) a different\ndistribution. The generative post-processing task aims not only to augment the ensemble size, but\nalso to bias the new samples towards p\u2032(v), which we take to be a distribution that more closely\nresembles actual weather. The underlying goal is to generate ensembles that are less biased than\nthose provided by the physics-based model, while still quantifying the forecast uncertainty captured\nby p(v). We emphasize that while this task has the flavor and also achieves the effect of debiasing\nto some degree, we focus on generating samples instead of minimizing the difference between their\nmean and a given reanalysis or observations. In both the emulation and post-processing tasks, the\nsmaller the value of K is, the greater the computational savings.\nFigure 1 illustrates the concepts behind these two tasks. There, p(v) is the distribution of the surface\ntemperature near Mountain View, CA on 2021/07/04 as predicted by the GEFS 13-day forecast\nensemble [50], and p\u2032(v) the corresponding ERA5 reanalysis ensemble [21]. While the GEFS\nensemble has 31 members, our goal is to use K \u226a 31 GEFS ensemble members to steer our\nsamplers to generate additional forecast members that are consistent with either GEFS\u2019s statistics or\nthe mixture distribution\u2019s statistics. Inspired by terminology from natural language understanding\nand computer vision, we refer to those K input examples from p(v) as \u201cseeds\u201d. The desirability to\nhave a small K is in spirit similar to few-shot learning setups in those works.\nWe stress that the primary goal of both tasks is to improve the computational efficiency of ensemble\nweather forecasting, not to replace physics-based models. The generated samples should be not\nonly consistent with the underlying distribution of atmospheric states (each sample is \u201cweather-\nlike\u201d), but also validated by standard forecast verification metrics. In this work, we examine the\ngenerated ensembles by comparing them to other physics-based ensembles using the rank histogram,\nthe anomaly correlation coefficient (ACC), RMSE, CRPS, and rare event classification metrics, as\ndefined in C.\n3\nTable 1: Data Used for Training and Evaluation\nName\nDate Range\nEnsemble size\nCitation\nERA5-HRES\n1959/01/01 \u2013 2022/12/31\n1\n[21]\nERA5-Ensemble\n1959/01/01 \u2013 2021/12/31\n10\n[21]\nGEFS\n2020/09/23 \u2013 2022/12/31\n31\n[50]\nGEFS-Reforecast\n2000/01/01 - 2019/12/31\n5\n[18]\n2.2\nData for Learning and Evaluation\nWe target the GEFS (version 12) ensemble forecasting system for the generative ensemble emula-\ntion task [50]. We use 20 years of GEFS 5-member reforecasts [18], denoted hereafter as GEFS-RF5,\nto learn p(v). Generative post-processing attempts to remove systematic biases of the original fore-\ncasting system from the learned emulator. To this end, we take the ERA5 10-member Reanalysis\nEnsemble [21], denoted as ERA5-10, to represent p\u2032(v) in the target mixture distribution. We also\nuse ERA5 HRES reanalysis as a proxy for real historical observations when evaluating the skill of\nour generated ensemble predictions.\nAll data are derived from the publicly available sources listed in Table 1. Table 2 lists the atmo-\nspheric state variables that are considered by our models. They are extracted and spatially regridded\nto the same cubed sphere mesh of size 6\u00d748\u00d748 (\u2248 2\u25e6 resolution) using inverse distance weighting\nwith 4 neighbors [37]. We only retain the 00h-UTC time snapshots of the fields in Table 2 for each\nday.\nThe climatology is computed from the ERA5 HRES dataset, using the reference period 1990-2020.\nThe daily climatological mean and standard deviation are obtained by smoothing these two time\nseries with a 15-day centered window over the year with periodic boundary conditions. The mean\nand standard deviation for February 29th is the average of those for February 28th and March 1st.\nOur models take as inputs and produce as outputs the standardized climatological anomalies of\nvariables in Table 2, defined as the standardized anomalies using the aforementioned climatological\nmean and standard deviation for the day of year and location, which facilitates learning [8, 29, 35].\nThe outputs are converted back to raw values for evaluation.\nFor each unique pair of forecast lead time and number of seeds K, we train a diffusion model for\nthe generative ensemble emulation task. For each unique triplet of lead time, K and mixture weight\n\u03b1, we train a model for the generative post-processing task. We provide results for lead times of\n{1, 4, 7, 10, 13, 16} days, K = 2 seeds, and generated ensembles with N = 512 members. For\nthe post-processing task, we consider the mixing ratio \u03b1 = 0.5. The sensitivity to K, N, and \u03b1 is\nexplored in C.\nWe evaluate our models against the operational GEFS 31-member ensemble [50] (GEFS-FULL) and\nthe ERA5 HRES reanalysis. Note that we can do so because the GEFS-FULL and GEFS-RF5 datasets\nare considered to have similar distributions \u2014 the reforecasts are reruns of the operational GEFS\nmodel using historical initial conditions [18]. We use the 20 years from 2000 to 2019 for training,\nyear 2020 and 2021 for validation, and year 2022 for evaluation. In particular, to accommodate\nthe longest lead time of 16 days, we evaluate using the forecasts initialized from 2022/01/01 to\n2022/12/15 (349 days in total) and the ERA5 HRES data aligned with the corresponding days.\n2.3\nLearning Method and Architecture\nThe use of probabilistic diffusion models to parameterize the target distributions, conditioned on a\nfew \u201cseeds\u201d, is at the core of our statistical modeling algorithm for both tasks.\nProbabilistic diffusion models are generative models of data. The generative process follows a\nMarkov chain. It starts with a random draw from an initial noise distribution \u2013 often an isotropic\nmultivariate Gaussian. Then it iteratively transforms and denoises the sample until it resembles a\nrandom draw from the data distribution [23]. The iteration steps advance the diffusion time, which\nis independent from the real-world time. The denoising operation relies on the instantiation of a\ndiffusion-time-dependent score function, which is the Jacobian of the log-likelihood of the data at a\n4\nTable 2: List of Atmospheric State Variables That Are Modeled\nQuantity\nProcessed Units\nMean sea level pressure\nPa\nTemperature at 2 meters\nK\nEastward wind speed at 850hPa\nm/s\nNorthward wind speed at 850hPa\nm/s\nGeopotential at 500hPa\nm2/s2\nTemperature at 850hPa\nK\nTotal column water vapour\nkg/m2\nSpecific humidity at 500 hPa\nkg/kg\ngiven diffusion time [43]. Score functions often take the form of deep learning architectures whose\nparameters are learned from training data.\nTypically, the score is a function of the noisy sample and the diffusion time. In this case, the resulting\ndata distribution is a model of the unconditional distribution of the training data. When additional\ninputs are passed to the score function, such as K seeding forecasts in our setting, the sampler\nconstructs the distribution conditioned on these inputs.\nIn this work, our choice of the score function is inspired by the Vision Transformer (ViT), which has\nbeen successfully applied to a range of computer vision tasks [11]. It is intuitive to view atmospheric\ndata as a temporal sequence of snapshots, which are in turn viewed as \u201cimages\u201d. Each snapshot is\nformed by \u201cpixels\u201d covering the globe with \u201ccolor\u201d channels. In this case, the channels correspond\nto the collection of atmospheric variables at different vertical levels. These can easily exceed in\nnumber the very few color channels of a typical image, e.g. 3 in the case of an RGB image. Due to\nthis, we use a variant of ViT via axial attention [22], so that the model remains moderate in size and\ncan be trained efficiently.\nIrrespective of the lead times and the number of seeds, all the models share the same architecture\nand have about 114M trainable parameters. They are trained with a batch size of 128 for 200K steps.\nThe training of each model takes slightly less than 18 hours on a 2 \u00d7 2 \u00d7 4 TPUv4 cluster. Inference\n(namely, ensemble generation) runs at batch size 512 on a 4\u00d78 TPUv3 cluster at less than 3 minutes\nper batch. It is thus very efficient and easily scalable to generate thousands of members.\n3\nResults\nUsing the SEEDS methodology, we have developed two generative models. The SEEDS-GEE model\nlearns to emulate the distribution of the U.S. operational ensemble NWP system, the Global Ensem-\nble Forecast System (GEFS) Version 12 [50]. The SEEDS-GPP model learns to emulate a blended\ndistribution that combines the GEFS ensemble with historical data from the ERA5 reanalysis of the\nEuropean Centre for Medium-Range Weather Forecasts (ECMWF), aiming to correct underlying\nbiases in the operational GEFS system (i.e., post-processing).\nSEEDS-GEE is trained using 20 years of GEFS 5-member retrospective forecasts [18], and SEEDS-\nGPP additionally learns from ECMWF\u2019s ERA5 10-member Reanalysis Ensemble over the same\nperiod [21]. Once learned, both models take as inputs a few randomly selected member forecasts\nfrom the operational GEFS ensemble, which has 31 members. We refer to the selected members\nas the seeding forecasts. These seeds provide the physical basis used by the generative models to\nconditionally sample additional plausible weather states. Both SEEDS-GEE and SEEDS-GPP can be\nused to generate ensembles with a significantly larger number of forecasts than operational physics-\nbased systems, easily reaching hundreds to tens of thousands of members.\nFigure 2 compares samples from the GEFS operational system, the ERA5 reanalysis, and the gener-\native emulator SEEDS-GEE. We also assess the quality of the generated ensembles in terms of mul-\ntiple important characteristics of useful ensemble prediction systems. First, we analyze whether the\nforecasts in the generative ensembles display spatial coherence, multivariate correlation structures,\nand wavenumber spectra consistent with actual weather states. Second, we compare the pointwise\n5\n0\n10\n20\n30\n40\n50\n60\n70\nTotal column vertically-integrated water vapour (kg/m2)\nFigure 2: Maps of total column vertically-integrated water vapor (kg/m2) for 2022/07/14, as cap-\ntured by (top left) the ERA5 reanalysis, (top right and middle row) 5 members of the GEFS-FULL\nforecast issued with a 7-day lead time, and (bottom) 3 samples from SEEDS-GEE. The top 2 GEFS\nforecasts were used to seed the SEEDS-GEE sampler.\npredictive skill of the generative ensembles and the full operational physics-based GEFS ensemble,\nmeasured against the ERA5 high resolution (HRES) reanalysis [21].\nWe report results on a subset of field variables: the mean sea level pressure, the temperature 2 m\nabove the surface, and the zonal wind speed at pressure level 850 hPa. Results for all modeled\nfields, listed in Table 2, are presented in the Supplementary Information (SI). We use GEFS-FULL\nto refer to the full 31-member GEFS ensemble, and GEFS-2 to an ensemble made of 2 randomly\nselected seeding forecasts. Unless noted, our generated ensembles have 512 members.\n3.1\nGenerated Weather Forecasts Are Plausible Weather Maps\nEnsemble forecasting systems are most useful when individual weather forecasts resemble real\nweather maps [30]. This is because for many applications, such as ship routing, energy forecasting,\nor compound extreme event forecasting, capturing cross-field and spatial correlations is fundamental\n[32, 41, 49].\nTo investigate this aspect of weather forecasts, we compare the covariance structure of the generated\nsamples to those from the ERA5 Reanalysis and GEFS through a stamp map over Europe for a date\nduring the 2022 European heatwave in Figure 3 [48]. The global atmospheric context of a few of\nthese samples is shown in Figure 2 for reference. We also present in Figure 3 weather samples\nobtained from a Gaussian model that predicts the univariate mean and standard deviation of each\natmospheric field at each location, such as the data-driven model proposed in [5]. This Gaussian\nmodel is meant to characterize the output of pointwise post-processing [5, 38, 39], which ignore\ncorrelations and treat each grid point as an independent random variable.\nSEEDS-GEE captures well both the spatial covariance and the correlation between midtropospheric\ngeopotential and mean sea level pressure, since it directly models the joint distribution of the atmo-\nspheric state. The generative samples display a geopotential trough west of Portugal with spatial\nstructure similar to that found in samples from GEFS-FULL or the reanalysis. They also depict re-\n6\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\nGeopotential at 500hPa height (m)\nFigure 3: Visualization of spatial coherence in forecasted weather charts for 2022/07/14, with a\n7-day lead time. The contours are for mean sea level pressure (dashed lines mark isobars below\n1010 hPa) while the heatmap depicts the geopotential height at the 500 hPa pressure level. Row 1:\nERA5 reanalysis, then 2 forecast members from GEFS-FULL used as seeds to our model. Row 2\u20133:\nOther forecast members from GEFS-FULL. Row 4\u20135: 8 samples drawn from SEEDS-GEE. Row 6:\nSamples from a pointwise Gaussian model parameterized by the GEFS-FULL ensemble mean and\nvariance.\n7\n2\n4\n6\nlog2(wave number)\n37.5\n40.0\n42.5\n45.0\n47.5\nlog2(energy)\nMean sea level pressure\n2\n4\n6\nlog2(wave number)\n22.5\n25.0\n27.5\n30.0\n32.5\nlog2(energy)\nTemperature at 2 meters\n2\n4\n6\nlog2(wave number)\n22.5\n25.0\n27.5\n30.0\n32.5\nlog2(energy)\nEastward wind speed at 850hPa\nSEEDS-GEE\nGEFS-Full\nERA5\nFigure 4: The energy spectra of several global atmospheric variables for January of 2022 from the\nERA5 reanalysis (thick black), members of the GEFS-FULL 7-day forecast (orange), and samples\nfrom SEEDS-GEE (green). The forecasts for each day are re-gridded to a latitude-longitude rectan-\ngular grid of the same angular resolution prior to computing the spectra. The computed spectra are\naveraged over the entire month. Each ensemble member is plotted separately.\nalistic correlations between geopotential and sea level pressure anomalies. Although the Gaussian\nmodel predicts the marginal univariate distributions adequately, it fails to capture cross-field or spa-\ntial correlations. This hinders the assessment of the effects that these anomalies may have on hot air\nintrusions from North Africa, which can exacerbate heatwaves over Europe [44].\nFigure 4 contrasts the energy spectra of SEEDS-GEE forecasts with that of ERA5 and GEFS-FULL.\nThe large overlap between samples from both forecast systems and the reanalysis demonstrates that\nthe two ensembles have similar spatial structure. Small systematic differences can be observed in\nsome variables like the zonal wind in the low troposphere, but for most variables the differences\nbetween SEEDS-GEE and GEFS-FULL are similar to the differences between the operational system\nand the ERA5 reanalysis.\nIn addition to examining the coherence of regional structures and the global spectra of the generative\nsamples, we also examine the multivariate correlation structure of generative samples locally. Fig-\nure 5 depicts the joint distributions of temperature at 2 meters and total column water vapour at the\ngrid point near Lisbon during the extreme heat event on 2022/07/14. We used the 7-day forecasts\nmade on 2022/07/07. For each plot, we generate 16,384-member ensembles. The observed weather\nevent from ERA5 is denoted by the star. The operational ensemble, denoted by the squares (also\nused as the seeding forecasts) and triangles (the rest of the GEFS ensemble), fails to predict the\nintensity of the extreme temperature event. This highlights that the observed event was so unlikely 7\ndays prior that none of the 31 forecast members from GEFS-FULL attained near-surface temperatures\nas warm as those observed. In contrast, the generated ensembles are able to extrapolate from the two\nseeding forecasts, providing an envelope of possible weather states with much better coverage of the\nevent. This allows quantifying the probability of the event taking place (see Figure 8 and C). Specif-\nically, our highly scalable generative approach enables the creation of very large ensembles that can\ncapture the likelihood of very rare events that would be characterized with a null probability by\nlimited-size ensembles. Moreover, we observe that the distributions of the generated ensembles do\nnot depend critically on the (positioning of the) seeding forecasts. This suggests that the generative\napproach is plausibly learning the intrinsic dynamical structure, i.e., the attractor of the atmosphere,\nin order to expand the envelopes of the phase of the dynamical systems to include extreme events\nthat deviate strongly from the seeds.\n3.2\nForecast Reliability and Predictive Skills\nAn important characteristic of ensemble forecast systems is their ability to adequately capture the\nfull distribution of plausible weather states. This characteristic is known as forecast calibration\nor reliability [47]. Forecast reliability can be characterized for a given lead time in terms of the\nrank histogram [1, 45]. Deviations from flatness of this histogram indicate systematic differences\nbetween the ensemble forecast distribution and the true weather distribution. Rank histograms for 7-\nday forecasts from GEFS-FULL, GEFS-2, SEEDS-GEE, and SEEDS-GPP over California and Nevada\nare shown in Figure 6. The GEFS ensembles display systematic negative biases in mean sea level\npressure and near-surface temperature over the region, as well as an underestimation of near-surface\n8\nERA5\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nERA5\nGEFS-2\nGEFS-Full\nSEEDS-GPP\nFigure 5: Generated ensembles provide better statistical coverage of the extreme heat event over\nPortugal. Each plot displays 16,384 generated forecasts from our method, extrapolating from the two\nseeding forecasts randomly taken from the operational forecasts. Contour curves of iso-probability\nare also shown. The first row is from SEEDS-GEE and the second from SEEDS-GPP. SEEDS-GPP\ncharacterizes the event best. Most notably, in the two rightmost plots of the bottom row, SEEDS-GPP\nis able to generate well-dispersed forecast envelopes that cover the extreme event, despite the two\nseeding ones deviating substantially from the observed event.\ntemperature uncertainty. Our model ensembles are more reliable than GEFS-2 and GEFS-FULL, due\nin part to the larger number of members that can be effortlessly generated. SEEDS-GPP shows the\nhighest reliability of all, validating generative post-processing as a useful debiasing methodology.\nIn particular, Figure 6 shows how SEEDS-GPP substantially reduces the ensemble under-dispersion\nfor 2-meter temperature forecasts.\nThe reliability information contained in the rank histogram can be further summarized in terms of\nits bulk deviation from flatness, which we measure using the unreliability metric \u03b4 introduced by\nCandille and Talagrand [7]. Higher values of \u03b4 indicate higher deviations from flatness, or a lower\nreliability of the forecasts. Figure 6 confirms that the generated ensembles are on a global average\nmore reliable than GEFS-FULL for all lead times. In addition, the refined calibration of SEEDS-GPP\nis more noticeable in the first forecast week.\nThe predictive skill of the generated ensembles is measured in terms of the root-mean-squared-\nerror (RMSE) and the anomaly correlation coefficient (ACC) of the ensemble mean, as well as the\ncontinuous ranked probability score (CRPS), treating the ERA5 HRES reanalsyis as the reference\nground-truth. These metrics are computed and averaged over the grid points every forecast day in\nthe test set and then aggregate over the test days. C details how these metrics are defined.\nFigure 7 reports these metrics for 3 atmospheric fields: the mean sea level pressure, the temperature\n2 meters above the ground, and the eastward wind speed at 850hPa. Both SEEDS-GEE or SEEDS-\nGPP perform significantly better than the seeding GEFS-2 ensemble across all metrics. The emulator\nSEEDS-GEE shows similar but slightly lower skill than GEFS-FULL across all metrics and variables.\nOur generative post-processing SEEDS-GPP is noticeably better than the physics-based GEFS-FULL\nat predicting near-surface temperature, roughly matching its skill for the other two fields. Intuitively,\n9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nQuantile\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\n0.20\nDifference to uniform distribution\nMean sea level pressure\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nQuantile\n0.05\n0.00\n0.05\n0.10\nDifference to uniform distribution\nTemperature at 2 meters\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nQuantile\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\nDifference to uniform distribution\nEastward wind speed at 850hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\n1\n4\n7\n10\n13\n16\nLead time (days)\n0\n5\n10\n15\n20\n25\nUnreliability\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0\n10\n20\n30\n40\n50\n60\n70\nUnreliability\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n4\n6\n8\n10\n12\nUnreliability\nEastward wind speed at 850hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 6: Top: Rank histograms from 7-day forecasts for grid points in the region bounded by\nparallels 34N and 42N, and meridians 124W and 114W, for the year 2022. This region roughly\nencompasses California and Nevada, USA. To compare the histograms of ensembles of different\nsize, the x axis is normalized to quantiles instead of ranks, and the y axis shows the difference to the\nuniform distribution. A perfectly calibrated ensemble forecast should have a flat curve at 0. Bottom:\nUnreliability parameter \u03b4 [7] as a function of lead time, computed for the same year and averaged\nglobally.\nthe potential benefits of statistical blending with a corrective data source are determined by the\nvariable-dependent biases of the emulated forecast model. In this case, the GEFS model is known\nto have a cold bias near the surface [50].\nA particularly challenging but important task of ensemble forecasts is being able to forecast extreme\nevents and assign meaningful likelihoods to them [32]. Figure 8 compares the skill of the same\n4 ensembles in predicting events deviating at least \u00b12\u03c3 from the mean climatology. We measure\nbinary classification skill by computing the Brier score of occurrence using ERA5 HRES as the\nbinary reference, and assigning a probability of occurrence to the ensemble forecasts equal to the\nfraction of occurrences within the ensemble.\nWe observe that SEEDS-GEE is comparable in skill to the full ensemble GEFS-FULL and far ex-\nceeds the skill of the seeding forecast ensemble GEFS-2. In the forecast of 2-meter temperature,\nSEEDS-GPP performs noticeably better than the other ensembles. For other variables, despite the\nless apparent advantage, SEEDS-GPP remains the best extreme forecast system for most lead times\nand variables. This highlights the relevance of our generative approach for forecasting tasks focused\non extremes.\n3.3\nHallucination or In-filling?\nOne frequently cited issue of generative AI technology is its tendency to \u201challucinate information\u201d.\nWe conclude this section by exploring the nature of the distribution information that the genera-\ntive ensembles are able to represent, beyond what is present in the two seeding forecasts from the\nGEFS full ensemble. As shown previously, the generated ensembles outperform the seeding forecast\nensembles in all metrics and often match or improve over the physics-based full ensemble.\nFigure 9 measures the correlation of the generative ensembles (SEEDS-GEE and SEEDS-GPP), the\nseeding ensemble GEFS-2, and the GEFS model climatology, with respect to the GEFS-FULL en-\nsemble forecasts. While comparing full joint distributions remains infeasible, we compute how well\nthe spread of each ensemble forecast correlates with that of the full physics-based ensemble GEFS-\nFULL. The plots show that at long-lead times (\u2265 10 days), all ensembles but GEFS-2 converge to\n10\n1\n4\n7\n10\n13\n16\nLead time (days)\n100\n200\n300\n400\n500\n600\n700\n800\n900\nRMSE (Pa)\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n1.5\n2.0\n2.5\n3.0\nRMSE (K)\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n3\n4\n5\n6\n7\nRMSE (m/s)\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n20\n40\n60\n80\n100\nACC (%)\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n10\n20\n30\n40\n50\n60\n70\n80\n90\nACC (%)\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n20\n40\n60\n80\nACC (%)\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n100\n200\n300\n400\n500\nCRPS (Pa)\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nCRPS (K)\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nCRPS (m/s)\nEastward wind speed at 850hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 7: Metrics of point-wise skill (RMSE, ACC and CRPS) of the generative and physics-based\nensemble forecasts, measured against the ERA5 HRES reanalysis as ground-truth. Shown are results\nfor mean sea level pressure (left), 2-meter temperature (center), and zonal velocity at 850 hPa (right).\nA detailed description of these metrics is included in C.\nhigh correlations (\u2265 95%) with GEFS-FULL. This is also true for the model climatology. However, in\nthe medium range (more than 4 days but less than 10 days ahead), the generative ensembles display a\nhigher correlation with the GEFS-FULL than both the model climatology and GEFS-2. This suggests\nthat the generative models are indeed able to generate information about forecast uncertainty beyond\nthe two seeding forecasts. In addition, the fact that generative ensembles can capture a higher cor-\nrelation with GEFS-FULL than the model climatology in the short and medium range shows that the\ndiffusion models are learning to emulate dynamically-relevant features beyond model biases; they\nhave resolution beyond climatology.\nThus, we put forward a reasonable hypothesis that the generated ensembles in-fill probability den-\nsity gaps in the small seeding ensembles. They also extend the (tails of the) envelopes of the full\nensembles such that extreme events are well represented in the envelops.\n4\nRelated Work\nPrevious work leveraging artificial intelligence to augment and post-process ensemble forecasts has\nfocused on improving the aggregate output statistics of the prediction system. Scher and Messori\ntrained a convolutional neural network to quantify forecast uncertainty given a single deterministic\nforecast [40]. They learned a global measure of uncertainty in a supervised setting, using as la-\nbels either the error of previous forecasts or the spread of an ensemble system. Brecht and Bihlo\ngeneralized this approach by predicting the ensemble spread at each forecast location, given a de-\nterministic forecast [5]. For this task, they used a conditional generative adversarial network based\non the PIX2PIX architecture [25]. Gr\u00a8onquist et al. trained a deep learning system to post-process a\n5-member ensemble forecast, resulting in a lower CRPS than a 10-member ensemble from the same\n11\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nBrier Score +2Sigma\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.03\n0.04\n0.05\n0.06\nBrier Score +2Sigma\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\nBrier Score +2Sigma\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nBrier Score -2Sigma\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nBrier Score -2Sigma\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nBrier Score -2Sigma\nEastward wind speed at 850hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 8: Binary classification skill of the different ensembles regarding extreme events (\u00b12\u03c3 from\nclimatology) in mean slea level pressure, 2-m temperature, and zonal velocity at 850 hPa, using\nERA5 HRES as the ground-truth. Skill is measured in terms of the cross-entropy; lower values are\nindicative of higher skill. First row: Brier score for +2\u03c3. Second row: Brier score for \u22122\u03c3.\n1\n4\n7\n10\n13\n16\nLead time (days)\n60\n65\n70\n75\n80\n85\n90\n95\nCorrelation (%)\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n55\n60\n65\n70\n75\n80\n85\n90\n95\nCorrelation (%)\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n50\n60\n70\n80\n90\nCorrelation (%)\nEastward wind speed at 850hPa\nGEFS-2\nGEFS-Model-Climatology\nSEEDS-GEE\nSEEDS-GPP\nFigure 9: Comparing the ensembles and the model climatology to GEFS-FULL in terms of how the\nensemble spreads are correlated with those from GEFS-FULL. The plots show that in medium-range\nbetween 4 to 10 days, the model has leveraged the two seeding forecasts to generate different, yet\ninformative, ensemble members to represent uncertainty.\noperational system [17]. Sacco et al. extended this work to build a system capable of predicting the\nensemble mean and spread over a limited domain [38].\nOur work differs from that of [5], [17], and [38] in that our probabilistic generative model outputs\nactual samples from the target forecast distribution. Thus, our approach offers added value beyond\nthe ensemble mean and spread: the drawn samples can be used to characterize spatial patterns\nassociated with weather extremes [41], or as input to targeted weather applications that depend on\nvariable and spatial correlations [32].\n5\nDiscussion\nThe Scalable Ensemble Envelope Diffusion Sampler (SEEDS) proposed in this work leverages the\npower of generative artificial intelligence to produce ensemble forecasts comparable to those from\nthe operational GEFS system at accelerated pace \u2013 the results reported in this paper need only 2\nseeding forecasts from the operational system, which generates 31 forecasts in its current version\n[50]. This leads to a hybrid forecasting system where a few weather trajectories computed with\na physics-based model are used to seed a diffusion model that can generate additional forecasts\nmuch more efficiently. This methodology provides an alternative to the current operational weather\n12\nforecasting paradigm, where the computational resources saved by the statistical emulator could be\nallocated to increasing the resolution of the physics-based model [31], or issuing forecasts more\nfrequently.\nSEEDS is trained on historical retrospective forecasts (i.e., reforecasts) issued with the operational\nphysics-based model, which are already required for post-processing in the current paradigm [19].\nOur framework is also flexible enough to enable direct generation of debiased ensembles when the\ngenerative post-processing task is considered during training; the only additional requirement is\naccess to historical reanalysis for the reforecast period.\nFor future work, we will conduct case studies of high-impact weather events to further evaluate\nSEEDS\u2019 performance, and consider specific ensemble forecast applications such as tropical and\nextratropical cyclone tracking [15, 28]. We will also explore more deeply the statistical modeling\nmechanisms that such models employ to extract information from weather data and in-fill the en-\nsemble forecast distribution. It is our belief that our application of generative AI to weather forecast\nemulation represents just one way of many that will accelerate progress in operational NWP in com-\ning years. Additionally, we hope the established utility of generative AI technology for weather\nforecast emulation and post-processing will spur its application in research areas such as climate\nrisk assessment, where generating a large number of ensembles of climate projections is crucial to\naccurately quantifying the uncertainty about future climate [9].\nAcknowledgments\nOur colleagues at Google Research have provided invaluable advice.\nAmong them, we thank\nStephan Rasp, Stephan Hoyer, and Tapio Schneider for their inputs and useful discussion on the\nmanuscript. We thank Carla Bromberg and Tyler Russell for technical program management, as\nwell as Alex Merose for data coordination and support. We also thank Cenk Gazen, Shreya Agrawal\nand Jason Hickey for discussions with them in the early stage of this work.\nReferences\n[1] Jeffrey L Anderson. A method for producing and evaluating probabilistic forecasts from en-\nsemble model integrations. J. Clim., 9:1518\u20131530, 1996. URL http://www.jstor.org/\nstable/26201352.\n[2] Peter Bauer, Alan Thorpe, and Gilbert Brunet. The quiet revolution of numerical weather\nprediction. Nature, 525:47\u201355, 2015. doi: 10.1038/nature14956. URL https://doi.org/\n10.1038/nature14956.\n[3] Riccardo Benedetti. Scoring rules for forecast verification. Mon. Weather Rev., 138(1):203\u2013\n211, January 2010.\n[4] Emanuele Bevacqua, Laura Suarez-Gutierrez, Agla\u00b4e J\u00b4ez\u00b4equel, Flavio Lehner, Mathieu Vrac,\nPascal Yiou, and Jakob Zscheischler. Advancing research on compound weather and climate\nevents via large ensemble model simulations. Nat. Commun., 14:2145, 2023. doi: 10.1038/\ns41467-023-37847-5. URL https://doi.org/10.1038/s41467-023-37847-5.\n[5] R\u00a8udiger Brecht and Alex Bihlo. Computing the ensemble spread from deterministic weather\npredictions using conditional generative adversarial networks.\nGeophys. Res. Lett., 50:\ne2022GL101452, 1 2023. URL https://doi.org/10.1029/2022GL101452.\n[6] Glenn W Brier. Verification of forecasts expressed in terms of probability. Mon. Weather\nRev., 78:1\u20133, 1950. doi: https://doi.org/10.1175/1520-0493(1950)078\u27e80001:VOFEIT\u27e92.0.CO;\n2.\nURL https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_\n1950_078_0001_vofeit_2_0_co_2.xml.\n[7] G Candille and O Talagrand. Evaluation of probabilistic prediction systems for a scalar vari-\nable. Q. J. R. Meteorol. Soc., 131(609):2131\u20132150, July 2005.\n[8] Markus Dabernig, Georg J Mayr, Jakob W Messner, and Achim Zeileis. Spatial ensemble\npost-processing with standardized anomalies. Q. J. R. Meteorol. Soc., 143:909\u2013916, 1 2017.\ndoi: https://doi.org/10.1002/qj.2975. URL https://doi.org/10.1002/qj.2975.\n13\n[9] C Deser, F Lehner, K B Rodgers, T Ault, T L Delworth, P N DiNezio, A Fiore, C Frankignoul,\nJ C Fyfe, D E Horton, J E Kay, R Knutti, N S Lovenduski, J Marotzke, K A McKinnon,\nS Minobe, J Randerson, J A Screen, I R Simpson, and M Ting. Insights from earth system\nmodel initial-condition large ensembles and future prospects. Nature Climate Change, 10:\n277\u2013286, 2020. ISSN 1758-6798. doi: 10.1038/s41558-020-0731-2. URL https://doi.\norg/10.1038/s41558-020-0731-2.\n[10] Prafulla Dhariwal and Alexander Nichol.\nDiffusion models beat GANs on image syn-\nthesis.\nIn Advances in Neural Information Processing Systems, volume 34, pages 8780\u2013\n8794, 2021.\nURL https://proceedings.neurips.cc/paper_files/paper/2021/\nfile/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=YicbFdNTTy.\n[12] ECMWF.\nIFS Documentation CY47R3 - Part V Ensemble prediction system.\nNumber 5.\nECMWF, 09/2021 2021. doi: 10.21957/zw5j5zdz5. URL https://www.ecmwf.int/node/\n20199.\n[13] Tobias Sebastian Finn. Self-Attentive ensemble transformer: Representing ensemble interac-\ntions in neural networks for earth system models. June 2021.\n[14] E M Fischer, U Beyerle, L Bloin-Wibe, C Gessner, V Humphrey, F Lehner, A G Pen-\ndergrass, S Sippel, J Zeder, and R Knutti.\nStorylines for unprecedented heatwaves based\non ensemble boosting.\nNature Communications, 14:4643, 2023.\nISSN 2041-1723.\ndoi:\n10.1038/s41467-023-40112-4. URL https://doi.org/10.1038/s41467-023-40112-4.\n[15] Lizzie S R Froude, Lennart Bengtsson, and Kevin I Hodges. The prediction of extratropical\nstorm tracks by the ECMWF and NCEP ensemble prediction systems. Mon. Weather Rev., 135\n(7):2545\u20132567, July 2007.\n[16] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estima-\ntion. J. Am. Stat. Assoc., 102(477):359\u2013378, March 2007.\n[17] Peter Gr\u00a8onquist, Chengyuan Yao, Tal Ben-Nun, Nikoli Dryden, Peter Dueben, Shigang Li, and\nTorsten Hoefler. Deep learning for post-processing ensemble weather forecasts. Philos. Trans.\nR. Soc. A, 379(2194):20200092, April 2021.\n[18] Hong Guan, Yuejian Zhu, Eric Sinsky, Bing Fu, Wei Li, Xiaqiong Zhou, Xianwu Xue,\nDingchen Hou, Jiayi Peng, M M Nageswararao, Vijay Tallapragada, Thomas M Hamill, Jef-\nfrey S Whitaker, Gary Bates, Philip Pegion, Sherrie Frederick, Matthew Rosencrans, and Arun\nKumar. GEFSv12 reforecast dataset for supporting subseasonal and hydrometeorological ap-\nplications. Mon. Weather Rev., 150(3):647\u2013665, March 2022.\n[19] Renate Hagedorn, Thomas M Hamill, and Jeffrey S Whitaker. Probabilistic forecast calibration\nusing ECMWF and GFS ensemble reforecasts. part I: Two-meter temperatures. Mon. Weather\nRev., 136:2608\u20132619, 2008. doi: https://doi.org/10.1175/2007MWR2410.1. URL https:\n//journals.ametsoc.org/view/journals/mwre/136/7/2007mwr2410.1.xml.\n[20] Lucy Harris, Andrew T T McRae, Matthew Chantry, Peter D Dueben, and Tim N Palmer. A\ngenerative deep learning approach to stochastic downscaling of precipitation forecasts. J. Adv.\nModel. Earth Syst., 14(10):e2022MS003120, April 2022.\n[21] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr\u00b4as Hor\u00b4anyi, Joaqu\u00b4\u0131n Mu\u02dcnoz-\nSabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, Adrian Simmons,\nCornel Soci, Saleh Abdalla, Xavier Abellan, Gianpaolo Balsamo, Peter Bechtold, Gionata Bi-\navati, Jean Bidlot, Massimo Bonavita, Giovanna De Chiara, Per Dahlgren, Dick Dee, Michail\nDiamantakis, Rossana Dragani, Johannes Flemming, Richard Forbes, Manuel Fuentes, Alan\nGeer, Leo Haimberger, Sean Healy, Robin J Hogan, El\u00b4\u0131as H\u00b4olm, Marta Janiskov\u00b4a, Sarah Kee-\nley, Patrick Laloyaux, Philippe Lopez, Cristina Lupu, Gabor Radnoti, Patricia de Rosnay, Iryna\n14\nRozum, Freja Vamborg, Sebastien Villaume, and Jean-No\u00a8el Th\u00b4epaut. The ERA5 global reanal-\nysis. Q. J. R. Meteorol. Soc., 146(730):1999\u20132049, 2020.\n[22] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in\nmultidimensional transformers, 2019. https://arxiv.org/abs/1912.12180.\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran As-\nsociates, Inc., 2020.\nURL https://proceedings.neurips.cc/paper_files/paper/\n2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.\n[24] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video:\nHigh definition video generation with diffusion models. arXiv preprint arXiv:2210.02303,\n2022.\n[25] P Isola, J Zhu, T Zhou, and A A Efros.\nImage-to-image translation with conditional ad-\nversarial networks.\nIn 2017 IEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 5967\u20135976, 2017. doi: 10.1109/CVPR.2017.632. URL http://doi.\nieeecomputersociety.org/10.1109/CVPR.2017.632.\n[26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of\ndiffusion-based generative models. In Proc. NeurIPS, 2022.\n[27] Martin Leutbecher. Ensemble size: How suboptimal is less than infinity? Q. J. R. Meteorol.\nSoc., 145:107\u2013128, 9 2019. doi: https://doi.org/10.1002/qj.3387. URL https://doi.org/\n10.1002/qj.3387. https://doi.org/10.1002/qj.3387.\n[28] Jonathan Lin, Kerry Emanuel, and Jonathan L Vigh. Forecasts of hurricanes using Large-\nEnsemble outputs. Weather Forecast., 35(5):1713\u20131731, October 2020.\n[29] Ignacio Lopez-Gomez, Amy McGovern, Shreya Agrawal, and Jason Hickey. Global extreme\nheat forecasting using neural weather models. Artif. Intell. Earth Syst., 2:e220035, 2023. doi:\nhttps://doi.org/10.1175/AIES-D-22-0035.1.\n[30] E N Lorenz. Atmospheric predictability experiments with a large numerical model. Tellus A,\n34(6):505\u2013513, 1982.\n[31] Juhui Ma, Yuejian Zhu, Richard Wobus, and Panxing Wang.\nAn effective configuration\nof ensemble size and horizontal resolution for the NCEP GEFS.\nAdv. Atmos. Sci., 29:\n782\u2013794, 2012.\ndoi: 10.1007/s00376-012-1249-y.\nURL https://doi.org/10.1007/\ns00376-012-1249-y.\n[32] Tim Palmer. The economic value of ensemble forecasts as a tool for risk assessment: From\ndays to decades. Q. J. R. Meteorol. Soc., 128:747\u2013774, 4 2002. doi: https://doi.org/10.1256/\n0035900021643593. URL https://doi.org/10.1256/0035900021643593.\n[33] Tim Palmer.\nThe primacy of doubt: Evolution of numerical weather prediction from de-\nterminism to probability.\nJ. Adv. Model. Earth Syst., 9:730\u2013734, 6 2017.\ndoi: https:\n//doi.org/10.1002/2017MS000999. URL https://doi.org/10.1002/2017MS000999.\n[34] Tim Palmer. The ECMWF ensemble prediction system: Looking back (more than) 25 years\nand projecting forward 25 years. Q. J. R. Meteorol. Soc., 145:12\u201324, 9 2019. doi: https:\n//doi.org/10.1002/qj.3383. URL https://doi.org/10.1002/qj.3383.\n[35] Weihong Qian, Jun Du, and Yang Ai.\nA review: Anomaly-based versus full-field-based\nweather analysis and forecasting. Bull. Am. Meteorol. Soc., 102:E849\u2013E870, 2021. doi: https:\n//doi.org/10.1175/BAMS-D-19-0297.1.\nURL https://journals.ametsoc.org/view/\njournals/bams/102/4/BAMS-D-19-0297.1.xml.\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2022.\nURL https:\n//github.com/CompVis/latent-diffusionhttps://arxiv.org/abs/2112.10752.\n15\n[37] C Ronchi, R Iacono, and P S Paolucci. The \u201ccubed sphere\u201d: A new method for the solu-\ntion of partial differential equations in spherical geometry. J. Comput. Phys., 124:93\u2013114,\n1996. doi: https://doi.org/10.1006/jcph.1996.0047. URL https://www.sciencedirect.\ncom/science/article/pii/S0021999196900479.\n[38] Maximiliano A Sacco, Juan J Ruiz, Manuel Pulido, and Pierre Tandeo. Evaluation of machine\nlearning techniques for forecast uncertainty quantification. Q. J. R. Meteorol. Soc., 148(749):\n3470\u20133490, October 2022.\n[39] Sebastian Scher and Gabriele Messori. Predicting weather forecast uncertainty with machine\nlearning. Q. J. R. Meteorol. Soc., 144:2830\u20132841, 10 2018. doi: https://doi.org/10.1002/qj.\n3410. URL https://doi.org/10.1002/qj.3410. https://doi.org/10.1002/qj.3410.\n[40] Sebastian Scher and Gabriele Messori. Predicting weather forecast uncertainty with machine\nlearning. Q. J. R. Meteorol. Soc., 144(717):2830\u20132841, 2018.\n[41] Sebastian Scher, Stephen Jewson, and Gabriele Messori. Robust worst-case scenarios from\nensemble forecasts. Weather Forecast., 36:1357\u20131373, 2021. doi: https://doi.org/10.1175/\nWAF-D-20-0219.1. URL https://journals.ametsoc.org/view/journals/wefo/36/\n4/WAF-D-20-0219.1.xml.\n[42] Julia Slingo and Tim Palmer. Uncertainty in weather and climate prediction. Philos. Trans.\nR. Soc. A, 369:4751\u20134767, 12 2011. doi: 10.1098/rsta.2011.0161. URL https://doi.org/\n10.1098/rsta.2011.0161. doi: 10.1098/rsta.2011.0161.\n[43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. In\nInternational Conference on Learning Representations, 2020.\n[44] A S\u00b4anchez-Ben\u00b4\u0131tez, R Garc\u00b4\u0131a-Herrera, D Barriopedro, P M Sousa, and R M Trigo. June 2017:\nThe earliest european summer mega-heatwave of reanalysis period. Geophys. Res. Lett., 45:\n1955\u20131962, 2 2018. doi: https://doi.org/10.1002/2018GL077253. URL https://doi.org/\n10.1002/2018GL077253.\n[45] O. Talagrand, R. Vautard, and B Strauss. Evaluation of probabilistic prediction systems. In\nWorkshop on Predictability, 20-22 October 1997, pages 1\u201326, Shinfield Park, Reading, 1997.\nECMWF.\n[46] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Ragha-\nvan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng.\nFourier fea-\ntures let networks learn high frequency functions in low dimensional domains.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 7537\u20137547. Curran As-\nsociates, Inc., 2020.\nURL https://proceedings.neurips.cc/paper_files/paper/\n2020/file/55053683268957697aa39fba6f231c68-Paper.pdf.\n[47] Daniel S Wilks. Statistical Methods in the Atmospheric Sciences. Elsevier, 4th edition, 2019.\ndoi: https://doi.org/10.1016/B978-0-12-815823-4.00009-2.\n[48] Alexandra Witze. Extreme heatwaves: Surprising lessons from the record warmth. Nature,\n608:464\u2013465, 2022.\n[49] R. P. Worsnop, M. Scheuerer, T. M. Hamill, and J. K. Lundquist. Generating wind power\nscenarios for probabilistic ramp event prediction using multivariate statistical post-processing.\nWind Energy Sci., 3(1):371\u2013393, 2018. doi: 10.5194/wes-3-371-2018. URL https://wes.\ncopernicus.org/articles/3/371/2018/.\n[50] Xiaqiong Zhou, Yuejian Zhu, Dingchen Hou, Bing Fu, Wei Li, Hong Guan, Eric Sinsky, Wal-\nter Kolczynski, Xianwu Xue, Yan Luo, Jiayi Peng, Bo Yang, Vijay Tallapragada, and Philip\nPegion. The development of the NCEP global ensemble forecast system version 12. Weather\nForecast., 37(6):1069\u20131084, June 2022.\n16\n[51] Yuejian Zhu, Zoltan Toth, Richard Wobus, David Richardson, and Kenneth Mylne.\nThe\neconomic value of ensemble-based weather forecasts.\nBull. Am. Meteorol. Soc., 83:\n73\u201384, 2002.\ndoi:\nhttps://doi.org/10.1175/1520-0477(2002)083\u27e80073:TEVOEB\u27e92.3.CO;\n2.\nURL https://journals.ametsoc.org/view/journals/bams/83/1/1520-0477_\n2002_083_0073_tevoeb_2_3_co_2.xml.\n17\nA\nProbabilistic Diffusion Models\nWe give a brief introduction to probabilistic diffusion models. Diffusion models are powerful meth-\nods for learning distributions from data. They have recently become one of the most popular ap-\nproaches for image generation and video synthesis [24]. For a detailed description, see [43].\nConsider a multivariate random variable V with an underlying distribution pdata(v). Intuitively,\ndiffusion-based generative models iteratively transform samples from an initial noise distribution\npT into samples from the target data distribution pdata through a denoising operator. By convention,\nT is set to 1 as it is just nominal and does not correspond to the real time in the physical world3.\nAdditionally, as it will be clear from the explanation below, this initial distribution is a multivariate\nGaussian distribution.\nNoise is removed such that the samples follow a family of diffusion-time-dependent marginal distri-\nbutions p\u03c4(v\u03c4; \u03c3\u03c4) for decreasing diffusion times \u03c4 and noise levels \u03c3\u03c4 [23]. The distributions are\ngiven by a forward blurring process that is described by the following stochastic differential equation\n(SDE) [26, 43]\ndV\u03c4 = f(V\u03c4, \u03c4)d\u03c4 + g(V\u03c4, \u03c4)dW\u03c4,\n(1)\nwith drift f, diffusion coefficient g, and the standard Wiener process W\u03c4. Following [26], we set\nf(V\u03c4, \u03c4) = f(\u03c4)V\u03c4 := \u02d9s\u03c4\ns\u03c4\nV\u03c4,\nand\ng(V\u03c4, \u03c4) = g(\u03c4) := s\u03c4\np\n2 \u02d9\u03c3\u03c4\u03c3\u03c4,\n(2)\nwhere the overhead dot denotes the time derivative. Solving the SDE in (1) forward in time with\nan initial condition v0 leads to the Gaussian perturbation kernel p\u03c4(V\u03c4|v0) = N(s\u03c4v0, s2\n\u03c4\u03c32\n\u03c4I).\nIntegrating the kernel over the data distribution p0(v0) = pdata, we obtain the marginal distribution\np\u03c4(v\u03c4) at any \u03c4. As such, one may prescribe the profiles of s\u03c4 and \u03c3\u03c4 so that p0 = pdata (with\ns0 = 1, \u03c30 = 0), and more importantly\npT (VT ) \u2248 N(0, s2\nT \u03c32\nT I),\n(3)\ni.e., the distribution at the (nominal) terminal time T\n= 1 becomes indistinguishable from an\nisotropic, zero-mean Gaussian. To sample from pdata, we utilize the fact that the reverse-time SDE\ndV\u03c4 =\n\u0002\nf(\u03c4)V\u03c4 \u2212 g(\u03c4)2\u2207V\u03c4 log p\u03c4(V\u03c4)\n\u0003\nd\u03c4 + g(\u03c4)dW\u03c4\n(4)\nhas the same marginals as eq. (1). Thus, by solving (4) backwards using (3) as the initial condition,\nwe obtain samples from pdata at \u03c4 = 0.\nThere are several approaches to learn the score function \u2207V\u03c4 log p\u03c4(V\u03c4). The key step is to param-\neterize it with a neural network s\u03b8(v\u03c4, \u03c3\u03c4) such that\ns\u03b8(v\u03c4, \u03c3\u03c4) \u2248 \u2207v\u03c4 log p\u03c4(v\u03c4|v0) = \u2212s\u03c4v0 \u2212 v\u03c4\n\u03c32\u03c4\n= z\u03c4/\u03c3\u03c4\n(5)\nfor any v \u223c p\u03c4(V\u03c4|v0) and \u03c3\u03c4. In other words, the normalized score network \u03c3\u03c4s\u03b8(v\u03c4, \u03c3\u03c4) iden-\ntifies the noise z\u03c4 injected into the data v0. The parameters \u03b8 of the score network are trained to\nreduce the L2 difference between them. Once this score network is learned, it is used to denoise\na noised version of the desired data v0. We refer to this process as learning the normalized score\nfunction. In this work, we use the hyperparameters f(\u03c4) = 0 and g(\u03c4) = 100\u03c4.\nB\nMethod Details\nB.1\nProblem Setup and Main Idea\nHere we provide a detailed description of the two generative tasks considered. Formally, let p(v)\ndenote an unknown distribution of the atmospheric state V . In the task of generative ensemble\nemulation, we are given a few examples or seeds sampled from p(v) and our task is to generate\nmore samples from the same distribution.\n3For notation clarity, we will be using \u03c4 and its capitalized version T to denote diffusion times, which are\ndifferent from the physical times.\n18\nGiven K samples EK = (v1, v2, \u00b7 \u00b7 \u00b7 , vK) from p(v), we construct an easy-to-sample conditional\ndistribution\n\u02c6p(v) = p(v; EK),\n(6)\nwhich approximates p(v). The conditional distribution \u02c6p(v) needs to have two desiderata: it ap-\nproximates well p(v), and it is much less costly to sample than p(v). Note that this problem extends\nthe typical density estimation problem, where there is no conditioning, i.e., K = 0.\nIn the task of generative post-processing, we construct an efficient sampler to approximate a mix-\nture distribution. As another example, let p\u2032(v) be the reanalysis distribution of the atmospheric state\ncorresponding to the forecast p(v). We construct a conditional distribution p(v; EK) to approximate\nthe following mixture of distributions:\npMIX(v) = \u03b1p(v) + (1 \u2212 \u03b1)p\u2032(v).\n(7)\nIn practice, the information about p\u2032(v) is given to the learning algorithm in the form of a represen-\ntative K\u2032-member ensemble E\u2032K\u2032. The mixture weight \u03b1 is then given by\n\u03b1 =\nM \u2212 K\nM \u2212 K + K\u2032 .\n(8)\nwhere M > K is the number of samples we have drawn from p(v). In the following, we describe\nhow to apply the technique of generative modeling introduced in A to these two tasks. We first\nmention how data is organized.\nB.2\nData for Training and Evaluation\nEach ensemble Etl is identified by the forecast initialization day t and the lead time l. Here, t \u2208\nT TRAIN indexes all the days in the training set and l \u2208 L, where L = {1, 3, 6, 10, 13, 16} days is\nthe set of lead times we consider. We group ensembles of the same lead time l as a training dataset\nDTRAIN\nl\n= {E1l, E2l, \u00b7 \u00b7 \u00b7 , ET TRAINl}. In generative ensemble emulation, DTRAIN\nl\ncontains samples from\nGEFS-RF5 exclusively, since the target distribution is GEFS-FULL. In generative post-processing,\nDTRAIN\nl\ncontains samples from both GEFS-RF5 and ERA5-10, to captue the target distribution eq. (7).\nNote that the K seeds can be included in the generated ensembles for evaluation. However, in\nthis work, we decide to adopt a stricter evaluation protocol where the seeds are excluded from the\ngenerated ensembles; including them would mildly improve the quality of the generated ensembles,\nsince the number of generated forecasts is far greater than K.\nAs a shorthand, we use vt to denote all members in the ensemble forecast made at time t, and vtm\nto denote the mth forecast at the same time.\nB.2.1\nGenerative Ensemble Emulation\nIn the ensemble emulation task we use K samples to construct a sampler of the conditional dis-\ntribution \u02c6p(v). To this end, for each lead time l and value of K, we learn a distinct conditional\ndistribution model MlK. We choose K = 1, 2, 3, 4; and use DTRAIN\nl\nto learn MlK.\nTo train the conditional generation model MlK, we use as input data K randomly chosen members\nfrom each ensemble forecast vt in DTRAIN\nl\n, and another member vtm0 from the remaining (M \u2212 K)\nmembers as our target. This procedure augments the data in DTRAIN\nl\nby CK\nM-fold, where CK\nM is the\nnumber of K-combinations of the M members.\nThe learning goal is then to optimize the score network eq. (5), so that the resulting sampler gives\nrise to:\nplK(vtm0|vtm1, vtm2, \u00b7 \u00b7 \u00b7 , vtmK),\n(9)\nwhere m1, m2, \u00b7 \u00b7 \u00b7 , mK, and m0 index the chosen ensemble members. Specifically, the score net-\nwork is given by\ns\u03b8(vtm,\u03c4, vtm1, vtm2, \u00b7 \u00b7 \u00b7 , vtmK, ct, \u03c3\u03c4)\n(10)\nwhere vtm,\u03c4 is a perturbed version of vtm0. We include the climatological mean ct as an input to\nthe score function eq. (10), since the atmospheric state is non-stationary with respect to the time of\nthe year.\n19\n0\n1\n...\n6(C/P)2-1\nTransformer \nstack in \nthe l \ndimension\nSlice \nembedding\nhsqld\nPatch embedding\nPatch embedding\nPatch embedding\nPatch embedding\nPatch Position \nembeddings\n+\nhslice\nFigure 10: Schematic of the spatial attention mechanism. Patches of cubed sphere grid points are\nmapped to patch embeddings, and a transformer stack transforms their combination with patch po-\nsition embeddings in the patch (l) dimension.\nB.2.2\nGenerative Post-Processing\nFor the task of generative post-processing, the setup is similar except that we seek to approximate the\nmixture distribution of p(v) and p\u2032(v) defined in eq. (7). The training dataset DTRAIN\nl\nthus contains\nensembles from two different distributions, vt and v\u2032\nt.\nIn this case, we train conditional distribution models MlKK\u2032 for each triplet (l, K, K\u2032) using the\nsame input as the conditional emulator MlK, i.e., K out of M members of each ensemble vt.\nHowever, the target sample is now drawn randomly from either p(v) or p\u2032(v). Consistent with\neq. (7), we draw a sample from the remaining (M \u2212 K) members of ensemble vt with probability\n(M \u2212 K)/(M \u2212 K + K\u2032), or a member from v\u2032\nt with probability K\u2032/(M \u2212 K + K\u2032). Denoting\nthe final selection as xt0, the score function becomes\ns\u03b8(xt0,\u03c4, vtm1, vtm2, \u00b7 \u00b7 \u00b7 , vtmK, ct, \u03c3\u03c4)\n(11)\nwhere xt0,\u03c4 is a perturbed version of xt0.\nIn this task, K\u2032 is a hyperparameter that controls the mixture distribution being approximated. We\nchoose K\u2032 from 2, 3 or 4.\nB.3\nModel Architecture\nWe employ an axial variant of the vision Transformer (ViT) [11] to model the normalized score\nfunctions \u03c3\u03c4s\u03b8(\u00b7, \u00b7) in eqns. (10) and (11), adapted to the particular characteristics of atmospheric\nmodeling.\nEach atmospheric state snapshot v is represented as a tensor vqp, where q indexes the physical\nquantities and p the locations on the cubed sphere. In particular, the altitude levels of the same\nphysical quantity are also indexed by q. The overall model operates on sequences of atmospheric\nstate snapshots vsqp where s indexes the sequence position. In the context of computer vision, this\nis similar to a video model, where p indexes the position in the image, q the \u201cRGB\u201d channels, and s\nthe time.\nThe number of channels, which we use to model physical fields at different atmospheric levels, can\nbe q \u223c O(100) in our setting. For this reason, we resort to axial attention, applied to p, q, and s\nseparately [22]. Each sequence dimension has its own learned position embedding instead of the\nusual fixed positional encodings.\nB.3.1\nSpatial Embedding\nWe call a piece of data assigned to each grid point of the cubed sphere as a slice. Regarding spatial\nattention, the 6 \u00d7 C \u00d7 C values of a slice associated with the cubed sphere grid points indexed by\np are partitioned into 6(C/P)2 square patches of size P \u00d7 P. We employ C = 48 in our work,\ncorresponding roughly to 2\u25e6 resolution. Each patch is then flattened and linearly embedded into D\ndimensions with learned weights, leading to a tensor hsqld, where l = 0, . . . 6(C/P)2 \u2212 1 indexes\nthe patches and d the in-patch dimension. Due to the nontrivial neighborhood structure of the cubed\nsphere, a shared learned positional encoding epos\nld is used for the l dimension. This is then fed into\na transformer of TL layers operating with l as the sequence dimension and d as the embedding\ndimension. The output is the the slice embedding corresponding to each patch:\nhslice\nsqld = TL(hsqld + epos\nld ).\n(12)\n20\nintegral\nintegral\nsurface\n850hPa\n850hPa\n2m\n850hPa\n500hPa\nTransformer \nstack in \nthe q \ndirection\nSnapshot \nembedding\nhsqld\nSlice embedding\nSlice embedding\nSlice embedding\nSlice embedding\nSlice embedding\nSlice embedding\nSlice embedding\nSlice embedding\nLevel \nembeddings\nField \nembeddings\n+\ntcwv\nmsl\nz\ntp\nu\nv\n+\nt\nt\n      \n  hsnapshot\nFigure 11: Transformer stacked in the q dimension\n\u0394T=7\n\u0394T=7\n\u0394T=7\n\u0394T=0\nDenoise\nClimatology\nTransformer \nstack in \nthe s \ndimension\nOutput h1qld\nUnused\nSnapshot embedding\nSnapshot embedding\nSnapshot embedding\nTime \nembeddings\nType \nembeddings\n+\n+\nDiffusion time token\nGEFS\nGEFS\nSnapshot embedding\nUnused\nhout\nFigure 12: Transformer stacked in the s dimension\nFor a schematic illustration, see Figure 10.\nB.3.2\nAtmospheric Field Embedding\nThe physical quantities have discrete semantic meanings, so we encode the positions in the q di-\nmension using a sum of two shared learned embeddings efield\nqd\nand elevel\nqd . We treat field labels like\n\u201cmean sea-level pressure\u201d and \u201ceastward wind speed\u201d, and generalized levels like \u201csurface\u201d, \u201c2m\u201d,\n\u201c500hPa\u201d, or \u201cintegratal\u201d as strings. We then encode them with a categorical embedding. This is\nthen fed into a transformer of TF layers operating with q as the sequence dimension and d as the\nembedding dimension. The output is the snapshot embedding\nhsnapshot\nsqld\n= TF (efield\nqd + elevel\nqd + hslice\nsqld).\n(13)\nFor the schematic illustration, see Figure 11.\nB.3.3\nSequence Embedding\nThe neural network operates on a sequence of snapshots. We tag each snapshot by adding two\nembeddings: a learned categorical embedding etype\nsd with values like \u201cDenoise\u201d, \u201cGEFS\u201d, \u201cClimatol-\nogy\u201d for the type of the snapshot and a random Fourier embedding [46] etime\nsd for the relative physical\ntime. In particular, the label \u201cDenoise\u201d denotes a snapshot for the score function input v. For ex-\namples, GEFS ensemble members are labeled as type \u201cGEFS\u201d and the same relative physical time\nembedding, so to the sequence transformer, they are exchangeable as intended.\nThe score function of a diffusion model requires the diffusion time \u03c4 as an additional input, which\nwe model by prepending to the sequence the token e\u03c4\nd, derived from a random Fourier embedding\nof \u03c4. The sequence and its embeddings are then fed into a transformer of TS layers operating with s\nas the sequence dimension and d as the embedding dimension. The output embedding is\nhout\nsqld = TS(e\u03c4\nd \u2295s (etype\nsd + etime\nsd + hsnapshot\nsqld\n)),\n(14)\nwhere \u2295s means concatenation in the s dimension. This is illustrated in Figure 12.\nTo obtain the score function s\u03b8(v, \u03c4), we take the output token hout\n1qld matching the position of the\ndenoise input and project it with learned weights and reshape back to a tensor vout\nqp as the final output.\n21\nB.3.4\nHyperparameters\nThe most salient hyperparameters of this architecture are the patch size P, the model embedding\ndimension D, and the number of layers LL, LF , LS of transformer stacks TL, TF , TS. In our work,\nwe use patch size P = 12 (number of patches 96), embedding dimension D = 768, and transformer\nlayers (TL, TF , TS) = (6, 4, 6). The hidden layers in the feed-forward network have dimension 4D\nas usual. The model has 113,777,296 trainable parameters. These parameters are learned by training\nover randomly sampled days from the 20-year GEFS reforecast dataset [18], using a batch size 128\nfor 200,000 steps.\nC\nEvaluation Metrics and More Results\nSimilar to how we organize the training data (cf. B.2), we partition all evaluation data according to\nthe lead time l, and generate one evaluation dataset DEVAL\nl\nfor each training dataset DTRAIN\nl\n. In the\nmain text, we report results for ensembles generated by models Ml2 and Ml2K\u2032, conditioned on\nK = 2 ensemble forecasts.\nC.1\nEvaluation Metrics\nWe use v (or w) to denote ensemble forecasts, which are indexed by t (time), m (ensemble member\nID), q (the atmospheric variable of interest), and p (the geolocation). We use Mv to denote the\nnumber of members in the ensemble, v to denote the ensemble mean forecast, and sv to denote the\nspread:\nv =\n1\nMv\nMv\nX\nm=1\nvm,\n(15)\nsv =\ns\n1\nMv \u2212 1\nX\nm\n(vm \u2212 v)2.\n(16)\nWe omit other indices (t, q and p) to avoid excessive subscripts, assuming the operations are applied\nelement-wise to all of them.\nC.1.1\nRMSE\nThe (spatial) root-mean-square-error (RMSE) between two (ensemble) means v and w is defined as\nRMSEt(v, u) =\nv\nu\nu\nt 1\nP\nP\nX\np=1\n(vtp \u2212 wtp)2,\n(17)\nwhere p indexes all the geospatial locations. Again, this is computed element-wise with respect to\nevery variable q. We summarize the RMSE along the temporal axis by computing the sample mean\nand variance,\nRMSE(v, u) = 1\nT\nT\nX\nt=1\nRMSEt(v, u),\n(18)\ns2\nRMSE(v,u) =\n1\nT \u2212 1\nT\nX\nt=1\n(RMSEt(v, u) \u2212 RMSE(v, u))2 .\n(19)\nAssuming the errors at different time are i.i.d., the sample variance can be used to estimate the\nstandard error of the mean estimate eq. (18).\nC.1.2\nCorrelation Coefficients and ACC\nThe spatial correlation between two (ensemble) means v and w is defined as\nCORRt(v, u) =\nP\np(vtp \u2212 vtp)(wtp \u2212 wtp)\nqP\np(vtp \u2212 vtp)2\nqP\np(wtp \u2212 wtp)2 ,\n(20)\n22\nwhere \u00b7tp refers to averages in space.\nThe centered anomaly correlation coefficient (ACC) is defined as the spatial correlation between the\nclimatological anomalies [47],\nACCt(v, u) =\nP\np(v\u2032\ntp \u2212 v\u2032\ntp)(w\u2032\ntp \u2212 w\u2032\ntp)\nqP\np(v\u2032\ntp \u2212 v\u2032\ntp)2\nqP\np(w\u2032\ntp \u2212 w\u2032\ntp)2\n,\n(21)\nwhere the anomalies are defined as the raw values minus the corresponding climatological mean ctp,\nv\u2032\ntp = vtp \u2212 ctp,\nw\u2032\ntp = wtp \u2212 ctp.\n(22)\nBoth estimates eq. (20) and (21) are then averaged in time over the evaluation set.\nC.1.3\nCRPS\nThe continuous ranked probability score (CRPS) [16] between an ensemble and ERA5 is given by\nCRPS(v, ERA5) =\n1\nMv\nMv\nX\nm=1\n|vm \u2212 ERA5| \u2212\n1\n2M 2v\nMv\nX\nm=1\nMv\nX\nm\u2032=1\n|vm \u2212 vm\u2032|.\n(23)\nThis is computed for all time and geo-spatial locations, and then averaged.\nWe use the traditional CRPS instead of the ensemble-adjusted CRPS\u2217 [27]. The latter examines\nproperties of ensembles in the theoretical asymptotic limit where their sizes are infinite. Since our\napproach aims at augmenting the physics-based ensembles, the definition eq. (23) is more opera-\ntionally relevant; even perfect emulation would obviously not improve the CRPS\u2217 score. Indeed,\nour model SEEDS-GEE can be seen as a practical way to bridge the skill gap between a small ensem-\nble and the asymptotic limit.\nC.1.4\nRank Histogram and Reliability Metric\nWe assess the reliability of the ensemble forecasts in terms of their rank histograms [1]. An ideal\nensemble forecast should have a flat rank histogram, so deviations from a flat rank histogram indicate\nunreliability.\nWe aggregate rank histograms for all ntesting dates in the evaluation set of days to obtain the average\nrank histogram {si}Mv\ni=0, where si is the number of times the label has rank i and Mv is the ensemble\nsize. Following Candille and Talagrand [7], the unreliability \u2206 of the ensemble is defined as the\nsquared distance of this histogram to a flat histogram\n\u2206 =\nMv\nX\ni=0\n\u0012\nsi \u2212 ntesting\n1 + Mv\n\u00132\n.\n(24)\nThis metric has the expectation \u22060 = ntesting\nMv\n1+Mv for a perfectly reliable forecast. The unreliability\nmetric \u03b4 is defined as\n\u03b4 = \u2206\n\u22060\n,\n(25)\nwhich has the advantage that it can be used to compare ensembles of different sizes. Once \u03b4 is\ncomputed for each location, a global \u03b4 value is obtained by averaging over all locations.\nC.1.5\nBrier Score for Extreme Event Classification\nTo measure how well the ensembles can predict extreme events, we first apply a binarization criterion\nb : R \u2192 {0, 1} (such as \u201cif T2M is 2\u03c3 away from its mean, issue an extreme heat warning.\u201d), and\nverify against the classification of the same event by the ERA5 HRES reanalysis,\nbERA5\ntqp\n= b(ERA5tqp),\n(26)\nby converting the ensemble forecast into a probabilistic (Bernoulli) prediction\nbv\ntqp =\n1\nMv\nMv\nX\nm=1\nb(vtmqp).\n(27)\n23\nWe can evaluate how well the probabilistic predictions align with the discrete labels by evaluating\nthe Brier score [6]\nBRIERb(v, ERA5)tq = 1\nP\nP\nX\np=1\n(bERA5\ntqp \u2212 bv\ntqp)2.\n(28)\nAlternatively, the probabilistic predictions can be evaluated by their cross-entropy, also known as\nthe logarithmic loss [3]\nLOGLOSSb(v, ERA5)tq = \u2212 1\nP\nP\nX\np=1\nbv\ntqp ln bERA5\ntqp + (1 \u2212 bv\ntqp) ln(1 \u2212 bERA5\ntqp ).\n(29)\nIt is possible for all ensemble members to predict the wrong result (especially for small ensembles),\nleading to the LOGLOSSb = \u2212\u221e due to ln(0). In practice, a small number \u03f5 = 10\u22127 is added to\nboth logarithms since no forecast is truly impossible [3]. The value of the metric is thus affected by\nthe choice of \u03f5, a known inconvenience of this otherwise strictly proper metric.\nC.1.6\nGEFS Model Climatological Spread\nThe baseline GEFS-Climatology spread in Figure 9 is computed from the training dataset, which\nis the 20-year GEFS reforecast [18]. The climatological spread is computed for each lead time\nindependently. For a fixed lead time, the pointwise spread of the 5-member ensemble is computed\nfor each day. Then the model climatological spread for that lead time is defined as the day-of-year\naverage over the 20 years.\nNote that we slightly abuse standard notation, since this spread accounts not only for the internal\nvariability of the model, but also for forecast uncertainty [42].\nC.2\nDetailed and Additional Results\nUnless noted, the results reported here are obtained from N = 512 generated ensembles from our\nmodels, with K = 2 seeding forecasts and K\u2032 = 3 for SEEDS-GPP.\nC.2.1\nRMSE, ACC, CRPS, Reliability for All Fields\nWe report values for RMSE (Figure 13), ACC (Figure 14), CRPS (Figure 15), and \u03b4 (Figure 16)\nfor all 8 modeled fields in Table 2. The results are consistent with those reported in the main\ntext: SEEDS ensembles attain similar or better skill than the physics-based ensembles, with SEEDS-\nGPP performing better than SEEDS-GEE for variables that are biased in GEFS-FULL. In addition,\nSEEDS-GPP is in general more reliable than GEFS-FULL and SEEDS-GEE, particularly within the\nfirst forecast week.\nC.2.2\nBrier score of all fields at various thresholds\nFrom Figure 17 to Figure 22, we report the Brier score at different thresholds: \u00b13\u03c3, \u00b12\u03c3, and \u00b1 1\u03c3,\nrespectively. For all thresholds, SEEDS-GPP provides the most accurate extreme forecast ensem-\nbles among all others, for most lead times and variables. Moreover, the generative ensembles from\nSEEDS-GPP and SEEDS-GEE perform particularly well for longer lead times and more extreme pre-\ndiction tasks. For the most extreme thresholds considered (\u00b13\u03c3), the generative ensembles in gen-\neral outperform GEFS-FULL.\nC.3\nEffect of N, K and K\u2032\nC.3.1\nGenerative ensemble emulation with varying N for 7-day lead time with K = 2\nFigure 23 shows the effect of N, the size of the generated ensemble, on the skill of SEEDS-GEE\nforecasts. (Some metrics such as Brier scores for 2\u03c3 are omitted as they convey similar information.)\nFor most metrics, increasing N offers diminishing marginal gains beyond N = 256. Metrics which\nare sensitive to the sampling coverage, such as the unreliability \u03b4 and the Brier score at \u22122\u03c3, still\nimprove with larger N > 256.\n24\nC.3.2\nGenerative ensemble emulation with varying K for 7-day lead time\nFigure 24 studies the effect of K, the number of seeds, on the skill of SEEDS-GEE forecasts. We\nfind that the ensemble forecast skill of SEEDS-GEE is comparable to physics-based full ensembles,\nwhen conditioned on K \u2265 2 seeds from GEFS. Note that SEEDS-GEE is a drastically better forecast\nsystem than physics-based ensembles formed by the conditioning members (GEFS-2).\nFor all values of K, the affordability of large ensemble generation warranted by SEEDS leads to\nvery significant improvements in extreme event classification skill with respect to the conditioning\nensemble.\nIt is also shown that the generative emulation approach is also feasible when K = 1. However,\nthe skill of the generative ensemble is closer to physics-based ensembles with just a few members\n(GEFS-2) than to GEFS-FULL. These studies suggest that K = 2 could be a good compromise\nbetween generative ensemble performance and computational savings.\nC.3.3\nGenerative post-processing with varying K\u2032 for 7-day lead time with K = 2\nFigure 25 studies the effect of K\u2032, which contains the mixture weight, on the skill of SEEDS-GPP\nforecasts. Since the training data has ensemble size 5, the number of available GEFS training labels\nis 3. We see no forecast skill gains blending in beyond 4 reanalyses members (around 50% mixing\nratio). In the main text, we report results with K\u2032 = 3.\nC.3.4\nCase Study: Visualization of Generated Ensembles\nEnsemble forecasts are samples drawn from multivariate high-dimensional distributions. Making\ncomparisons among these distributions using a limited number of samples is a challenging statistical\ntask. In this case study, we study several ensembles by reducing the dimensions and focusing on\ntwo fields at a single spatial location.\nFigures 26 and 27 visualize the joint distributions of temperature at 2 meters and total column water\nvapour at the grid point near Lisbon during the extreme heat event on 2022/07/14. We used the 7-day\nforecasts made on 2022/07/07. For each plot, we generate 16,384-member ensembles, represented\nby the orange dots using different seeds from the 31-member GEFS ensemble. The density level sets\nare computed by fitting a kernel density estimator to the ensemble forecasts. The observed weather\nevent is denoted by the black star. The operational ensembles, denoted by blue rectangles (seeds)\nand orange triangles, do not cover the extreme event well, while the generated ensembles extrapolate\nfrom the two seeding forecasts, providing a better coverage of the event.\nIn Figure 26, we compare SEEDS-GEE and SEEDS-GPP generated samples conditioned on 6 differ-\nent randomly sampled two seeding forecasts from the 31-member GEFS ensemble. The plots are\narranged in 2 pairs of rows (4 rows in total) such that all the odd rows are from SEEDS-GEE and\neven rows are from SEEDS-GPP and that on each paired rows, the seeds for SEEDS-GEE above and\nSEEDS-GPP below are exactly the same. We see that given the same seeds, the SEEDS-GPP distribu-\ntion is correctly more biased towards warmer temperatures while maintaining the distribution shape\nand spread compared to SEEDS-GEE.\nIn Figure 27 we compare SEEDS-GEE based on 2 seeds with SEEDS-GEE based on 4 seeds. We first\ngenerate samples conditioned on 4 randomly sampled seeding forecasts from the 31-member GEFS\nensemble. Then we generate samples conditioned on 2 randomly selected seeding forecasts from\nthe 4 chosen in the first round. The plots show 6 such pairs of samples, arranged in 2 pairs of rows\n(4 rows in total) such that all odd rows use 2 seeds and even rows use 4 seeds and that for each pair\nof rows, the 4 seeds below always contain the 2 seeds above. We observe that the distribution of the\ngenerated samples conditioned on 4 seeds are more similar to each other, which verifies the intuition\nthat more seeds lead to more robustness to the sampling of the seeds. More seeds also lead to better\ncoverage of the extreme heat event by the generated envelopes.\n25\n1\n4\n7\n10\n13\n16\nLead time (days)\n100\n200\n300\n400\n500\n600\n700\n800\n900\nRMSE (Pa)\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n1.5\n2.0\n2.5\n3.0\nRMSE (K)\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n3\n4\n5\n6\n7\nRMSE (m/s)\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n3\n4\n5\n6\n7\nRMSE (m/s)\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n200\n400\n600\n800\nRMSE (m2/s2)\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nRMSE (K)\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n3\n4\n5\n6\n7\nRMSE (kg/m2)\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.0004\n0.0005\n0.0006\n0.0007\n0.0008\n0.0009\n0.0010\nRMSE (kg/kg)\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 13: RMSE of the ensemble means with ERA5 HRES as the \u201cground-truth\u201d labels.\n26\n1\n4\n7\n10\n13\n16\nLead time (days)\n20\n40\n60\n80\n100\nACC (%)\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n10\n20\n30\n40\n50\n60\n70\n80\n90\nACC (%)\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n20\n40\n60\n80\nACC (%)\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n20\n40\n60\n80\nACC (%)\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n20\n40\n60\n80\n100\nACC (%)\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n20\n40\n60\n80\nACC (%)\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n20\n30\n40\n50\n60\n70\n80\n90\nACC (%)\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n10\n20\n30\n40\n50\n60\n70\nACC (%)\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 14: ACC of the ensemble means with ERA5 HRES as \u201cground-truth\u201d labels.\n27\n1\n4\n7\n10\n13\n16\nLead time (days)\n100\n200\n300\n400\n500\nCRPS (Pa)\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nCRPS (K)\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nCRPS (m/s)\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nCRPS (m/s)\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n100\n200\n300\n400\n500\nCRPS (m2/s2)\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\nCRPS (K)\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nCRPS (kg/m2)\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.0002\n0.0003\n0.0004\n0.0005\n0.0006\nCRPS (kg/kg)\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 15: CRPS with ERA5 HRES as the \u201cground-truth\u201d labels.\n28\n1\n4\n7\n10\n13\n16\nLead time (days)\n0\n5\n10\n15\n20\n25\nUnreliability\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0\n10\n20\n30\n40\n50\n60\n70\nUnreliability\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n4\n6\n8\n10\n12\nUnreliability\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n4\n6\n8\n10\nUnreliability\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0\n20\n40\n60\n80\n100\nUnreliability\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0\n5\n10\n15\n20\n25\n30\n35\nUnreliability\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n4\n6\n8\n10\n12\nUnreliability\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n2\n4\n6\n8\n10\nUnreliability\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 16: Unreliability \u03b4 with ERA5 as the label.\n29\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nBrier Score -3Sigma\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\nBrier Score -3Sigma\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.002\n0.003\n0.004\n0.005\n0.006\nBrier Score -3Sigma\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nBrier Score -3Sigma\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nBrier Score -3Sigma\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.0020\n0.0025\n0.0030\n0.0035\n0.0040\n0.0045\n0.0050\nBrier Score -3Sigma\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.0010\n0.0015\n0.0020\n0.0025\n0.0030\n0.0035\nBrier Score -3Sigma\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.00020\n0.00025\n0.00030\n0.00035\n0.00040\n0.00045\n0.00050\nBrier Score -3Sigma\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 17: Brier score for \u22123\u03c3 with ERA5 HRES as the \u201cground-truth\u201d label.\n30\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nBrier Score -2Sigma\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nBrier Score -2Sigma\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nBrier Score -2Sigma\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\nBrier Score -2Sigma\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nBrier Score -2Sigma\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\nBrier Score -2Sigma\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\nBrier Score -2Sigma\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.0020\n0.0025\n0.0030\n0.0035\n0.0040\n0.0045\n0.0050\nBrier Score -2Sigma\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 18: Brier score for \u22122\u03c3 with ERA5 HRES as the \u201cground-truth\u201d label.\n31\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nBrier Score -1Sigma\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nBrier Score -1Sigma\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nBrier Score -1Sigma\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nBrier Score -1Sigma\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nBrier Score -1Sigma\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nBrier Score -1Sigma\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nBrier Score -1Sigma\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\nBrier Score -1Sigma\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 19: Brier score for \u22121\u03c3 with ERA5 HRES as the \u201cground-truth\u201d label.\n32\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nBrier Score +1Sigma\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nBrier Score +1Sigma\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nBrier Score +1Sigma\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nBrier Score +1Sigma\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nBrier Score +1Sigma\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\nBrier Score +1Sigma\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nBrier Score +1Sigma\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\nBrier Score +1Sigma\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 20: Brier score for +1\u03c3 with ERA5 HRES as the \u201cground-truth\u201d label.\n33\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nBrier Score +2Sigma\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.03\n0.04\n0.05\n0.06\nBrier Score +2Sigma\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\nBrier Score +2Sigma\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\nBrier Score +2Sigma\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\nBrier Score +2Sigma\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055\nBrier Score +2Sigma\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.02\n0.03\n0.04\n0.05\n0.06\nBrier Score +2Sigma\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nBrier Score +2Sigma\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 21: Brier score for +2\u03c3 with ERA5 HRES as the \u201cground-truth\u201d label.\n34\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.0010\n0.0015\n0.0020\n0.0025\n0.0030\n0.0035\nBrier Score +3Sigma\nMean sea level pressure\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nBrier Score +3Sigma\nTemperature at 2 meters\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\n0.009\nBrier Score +3Sigma\nEastward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\nBrier Score +3Sigma\nNorthward wind speed at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.00025\n0.00050\n0.00075\n0.00100\n0.00125\n0.00150\n0.00175\n0.00200\nBrier Score +3Sigma\nGeopotential at 500hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.004\n0.006\n0.008\n0.010\n0.012\nBrier Score +3Sigma\nTemperature at 850hPa\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\nBrier Score +3Sigma\nTotal column vertically-integrated water vapour\n1\n4\n7\n10\n13\n16\nLead time (days)\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\n0.0225\n0.0250\nBrier Score +3Sigma\nSpecific humidity at 500hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nSEEDS-GPP\nFigure 22: Brier score for +3\u03c3 with ERA5 HRES as the \u201cground-truth\u201d label.\n35\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n500\n520\n540\n560\n580\nRMSE (Pa)\nMean sea level pressure\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n2.15\n2.20\n2.25\n2.30\n2.35\n2.40\n2.45\nRMSE (K)\nTemperature at 2 meters\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n4.6\n4.8\n5.0\n5.2\n5.4\nRMSE (m/s)\nEastward wind speed at 850hPa\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n64\n66\n68\n70\n72\nACC (%)\nMean sea level pressure\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n53\n54\n55\n56\n57\n58\n59\n60\nACC (%)\nTemperature at 2 meters\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n52\n54\n56\n58\n60\n62\n64\nACC (%)\nEastward wind speed at 850hPa\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n220\n240\n260\n280\n300\n320\nCRPS (Pa)\nMean sea level pressure\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\nCRPS (K)\nTemperature at 2 meters\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n2.4\n2.6\n2.8\n3.0\n3.2\nCRPS (m/s)\nEastward wind speed at 850hPa\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nUnreliability\nMean sea level pressure\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n5\n10\n15\n20\n25\n30\n35\nUnreliability\nTemperature at 2 meters\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n1\n2\n3\n4\n5\n6\nUnreliability\nEastward wind speed at 850hPa\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n0.025\n0.030\n0.035\n0.040\n0.045\nBrier Score -2Sigma\nMean sea level pressure\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n0.0225\n0.0250\n0.0275\n0.0300\n0.0325\n0.0350\n0.0375\n0.0400\nBrier Score -2Sigma\nTemperature at 2 meters\n32\n64\n128\n256\n512\n1024\nNumber of generated samples\n0.024\n0.026\n0.028\n0.030\n0.032\n0.034\n0.036\nBrier Score -2Sigma\nEastward wind speed at 850hPa\nGEFS-2\nGEFS-Full\nSEEDS-GEE\nFigure 23: The effect of N on various metrics. Top down: RMSE, ACC, CRPS, unreliability, Brier\nscore for \u22122\u03c3 events.\n36\n1\n2\n3\n4\nNumber of conditioning members\n500\n525\n550\n575\n600\n625\n650\n675\nRMSE (Pa)\nMean sea level pressure\n1\n2\n3\n4\nNumber of conditioning members\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\nRMSE (K)\nTemperature at 2 meters\n1\n2\n3\n4\nNumber of conditioning members\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\n6.00\n6.25\nRMSE (m/s)\nEastward wind speed at 850hPa\n1\n2\n3\n4\nNumber of conditioning members\n56\n58\n60\n62\n64\n66\n68\n70\n72\nACC (%)\nMean sea level pressure\n1\n2\n3\n4\nNumber of conditioning members\n48\n50\n52\n54\n56\n58\n60\nACC (%)\nTemperature at 2 meters\n1\n2\n3\n4\nNumber of conditioning members\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\nACC (%)\nEastward wind speed at 850hPa\n1\n2\n3\n4\nNumber of conditioning members\n225\n250\n275\n300\n325\n350\n375\n400\n425\nCRPS (Pa)\nMean sea level pressure\n1\n2\n3\n4\nNumber of conditioning members\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\nCRPS (K)\nTemperature at 2 meters\n1\n2\n3\n4\nNumber of conditioning members\n2.5\n3.0\n3.5\n4.0\n4.5\nCRPS (m/s)\nEastward wind speed at 850hPa\n1\n2\n3\n4\nNumber of conditioning members\n5\n10\n15\n20\n25\nUnreliability\nMean sea level pressure\n1\n2\n3\n4\nNumber of conditioning members\n5\n10\n15\n20\n25\n30\n35\n40\nUnreliability\nTemperature at 2 meters\n1\n2\n3\n4\nNumber of conditioning members\n1\n2\n3\n4\n5\n6\n7\nUnreliability\nEastward wind speed at 850hPa\n1\n2\n3\n4\nNumber of conditioning members\n0.03\n0.04\n0.05\n0.06\nBrier Score -2Sigma\nMean sea level pressure\n1\n2\n3\n4\nNumber of conditioning members\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055\nBrier Score -2Sigma\nTemperature at 2 meters\n1\n2\n3\n4\nNumber of conditioning members\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\nBrier Score -2Sigma\nEastward wind speed at 850hPa\nGEFS-Full\nGEFS-K\nSEEDS-GEE\nFigure 24: The effect of K on various metrics. Top down: RMSE, ACC, CRPS, unreliability, Brier\nscore for \u22122\u03c3 event.\n37\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n500\n520\n540\n560\n580\nRMSE (Pa)\nMean sea level pressure\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n2.0\n2.1\n2.2\n2.3\n2.4\nRMSE (K)\nTemperature at 2 meters\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n4.6\n4.8\n5.0\n5.2\n5.4\nRMSE (m/s)\nEastward wind speed at 850hPa\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n64\n66\n68\n70\n72\nACC (%)\nMean sea level pressure\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n52\n54\n56\n58\n60\n62\nACC (%)\nTemperature at 2 meters\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n52\n54\n56\n58\n60\n62\n64\nACC (%)\nEastward wind speed at 850hPa\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n220\n240\n260\n280\n300\n320\nCRPS (Pa)\nMean sea level pressure\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n0.9\n1.0\n1.1\n1.2\n1.3\nCRPS (K)\nTemperature at 2 meters\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n2.4\n2.6\n2.8\n3.0\n3.2\nCRPS (m/s)\nEastward wind speed at 850hPa\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nUnreliability\nMean sea level pressure\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n0\n5\n10\n15\n20\n25\n30\n35\nUnreliability\nTemperature at 2 meters\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n1\n2\n3\n4\n5\n6\nUnreliability\nEastward wind speed at 850hPa\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n0.025\n0.030\n0.035\n0.040\n0.045\nBrier Score -2Sigma\nMean sea level pressure\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nBrier Score -2Sigma\nTemperature at 2 meters\n0\n40\n57\n66\n72\n76\nPercentage of reanalysis in training data\n0.024\n0.026\n0.028\n0.030\n0.032\n0.034\n0.036\nBrier Score -2Sigma\nEastward wind speed at 850hPa\nGEFS-2\nGEFS-Full\nSEEDS-GPP\nFigure 25: The effect of K\u2032 on various metrics. Top down: RMSE, ACC, CRPS, unreliability, Brier\nscore for \u22122\u03c3 event.\n38\nSEEDS-GEE\nSEEDS-GPP\nSEEDS-GEE\nSEEDS-GPP\nERA5\nGEFS-2\nGEFS-Full\nSEEDS\nFigure 26: Temperature at 2 meters and total column water vapour of the grid point near Lisbon on\n2022/07/14. SEEDS-GEE (green) and SEEDS-GPP (red) with N = 16384 samples (dots and kernel\ndensity level sets) were conditioned on various GEFS subsamples of size K = 2 (square).\n39\nConditioned on 2\nConditioned on 4\nConditioned on 2\nConditioned on 4\nERA5\nGEFS-2 or GEFS-4\nGEFS-Full\nSEEDS-GEE\nFigure 27: Temperature at 2 meters and total column water vapour of the grid point near Lisbon\non 2022/07/14. SEEDS-GEE with N = 16384 samples (dots and kernel density level sets) were\nconditioned on various GEFS subsamples of size K = 4 (square).\n40\n"
  }
]