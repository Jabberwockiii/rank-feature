[
  {
    "title": "The Chosen One: Consistent Characters in Text-to-Image Diffusion Models",
    "link": "https://arxiv.org/pdf/2311.10093.pdf",
    "upvote": "54",
    "text": "The Chosen One: Consistent Characters in Text-to-Image Diffusion Models\nOmri Avrahami1,2\nAmir Hertz1\nYael Vinker1,3\nMoab Arar1,3\nShlomi Fruchter1 Ohad Fried4 Daniel Cohen-Or1,3 Dani Lischinski1,2\n1Google Research\n2The Hebrew University of Jerusalem\n3Tel Aviv University\n4Reichman University\n\u201cA photo of a 50 \nyears old man \nwith curly hair.\u201d\n\u201cin the park\u201d\n\u201dreading a book\u201d\n\u201cat the beach\u201d\n\u201cA rendering of a \ncute albino \nporcupine, cozy \nindoor lighting.\"\n\u201cA portrait of a \nman with a \nmustache and a \nhat, fauvism.\u201d\n\u201cholding an avocado\u201d\nFigure 1. The Chosen One: Given a text prompt describing a character, our method distills a representation that enables consistent\ndepiction of the same character in novel contexts.\nAbstract\nRecent advances in text-to-image generation models\nhave unlocked vast potential for visual creativity. However,\nthese models struggle with generation of consistent charac-\nters, a crucial aspect for numerous real-world applications\nsuch as story visualization, game development asset design,\nadvertising, and more. Current methods typically rely on\nmultiple pre-existing images of the target character or in-\nvolve labor-intensive manual processes. In this work, we\npropose a fully automated solution for consistent charac-\nter generation, with the sole input being a text prompt. We\nintroduce an iterative procedure that, at each stage, identi-\nfies a coherent set of images sharing a similar identity and\nextracts a more consistent identity from this set. Our quan-\ntitative analysis demonstrates that our method strikes a bet-\nter balance between prompt alignment and identity consis-\ntency compared to the baseline methods, and these findings\nare reinforced by a user study. To conclude, we showcase\nseveral practical applications of our approach.\n1. Introduction\nThe ability to maintain consistency of generated visual con-\ntent across various contexts, as shown in Figure 1, plays\na central role in numerous creative endeavors. These in-\nclude illustrating a book, crafting a brand, creating comics,\ndeveloping presentations, designing webpages, and more.\nSuch consistency serves as the foundation for establishing\nProject page is available at: https://omriavrahami.com/the-chosen-one\nOmri, Yael, Moab, Daniel and Dani performed this work while work-\ning at Google.\n1\narXiv:2311.10093v2  [cs.CV]  27 Nov 2023\n\u201cA plasticine of a cute baby cat with big eyes.\u201d\nStandard\nOurs\nFigure 2. Identity consistency. Given the prompt \u201ca Plasticine of\na cute baby cat with big eyes\u201d, a standard text-to-image diffusion\nmodel produces different cats (all corresponding to the input text),\nwhereas our method produces the same cat.\nbrand identity, facilitating storytelling, enhancing commu-\nnication, and nurturing emotional engagement.\nDespite the increasingly impressive abilities of text-to-\nimage generative models, these models struggle with such\nconsistent generation, a shortcoming that we aim to rectify\nin this work. Specifically, we introduce the task of consis-\ntent character generation, where given an input text prompt\ndescribing a character, we derive a representation that en-\nables generating consistent depictions of the same character\nin novel contexts. Although we refer to characters through-\nout this paper, our work is in fact applicable to visual sub-\njects in general.\nConsider, for example, an illustrator working on a Plas-\nticine cat character. As demonstrated in Figure 2, provid-\ning a state-of-the-art text-to-image model with a prompt\ndescribing the character, results in a variety of outcomes,\nwhich may lack consistency (top row). In contrast, in this\nwork we show how to distill a consistent representation of\nthe cat (2nd row), which can then be used to depict the same\ncharacter in a multitude of different contexts.\nThe widespread popularity of text-to-image generative\nmodels [57, 63, 69, 72], combined with the need for con-\nsistent character generation, has already spawned a vari-\nety of ad hoc solutions. These include, for example, us-\ning celebrity names in prompts [64] for creating consistent\nhumans, or using image variations [63] and filtering them\nmanually by similarity [65]. In contrast to these ad hoc,\nmanually intensive solutions, we propose a fully-automatic\nprincipled approach to consistent character generation.\nThe academic works most closely related to our setting\nare ones dealing with personalization [20, 70] and story\ngeneration [24, 36, 62]. Some of these methods derive a rep-\nresentation for a given character from several user-provided\nimages [20, 24, 70]. Others cannot generalize to novel char-\nacters that are not in the training data [62], or rely on textual\ninversion of an existing depiction of a human face [36].\nIn this work, we argue that in many applications the goal\nis to generate some consistent character, rather than visually\nmatching a specific appearance. Thus, we address a new\nsetting, where we aim to automatically distill a consistent\nrepresentation of a character that is only required to comply\nwith a single natural language description. Our method does\nnot require any images of the target character as input; thus,\nit enables creating a novel consistent character that does not\nnecessarily resemble any existing visual depiction.\nOur fully-automated solution to the task of consistent\ncharacter generation is based on the assumption that a suffi-\nciently large set of generated images, for a certain prompt,\nwill contain groups of images with shared characteristics.\nGiven such a cluster, one can extract a representation that\ncaptures the \u201ccommon ground\u201d among its images. Repeat-\ning the process with this representation, we can increase\nthe consistency among the generated images, while still re-\nmaining faithful to the original input prompt.\nWe start by generating a gallery of images based on the\nprovided text prompt, and embed them in a Euclidean space\nusing a pre-trained feature extractor. Next, we cluster these\nembeddings, and choose the most cohesive cluster to serve\nas the input for a personalization method that attempts to ex-\ntract a consistent identity. We then use the resulting model\nto generate the next gallery of images, which should exhibit\nmore consistency, while still depicting the input prompt.\nThis process is repeated iteratively until convergence.\nWe evaluate our method quantitatively and qualitatively\nagainst several baselines, as well as conducting a user study.\nFinally, we present several applications of our method.\nIn summary, our contributions are: (1) we formalize the\ntask of consistent character generation, (2) propose a novel\nsolution to this task, and (3) we evaluate our method quan-\ntitatively and qualitatively, in addition to a user study, to\ndemonstrate its effectiveness.\n2. Related Work\nText-to-image generation.\nText conditioned image gen-\nerative models (T2I) [63, 69, 95] show unprecedented capa-\nbilities of generating high quality images from mere natural\nlanguage text descriptions. They are quickly becoming a\nfundamental tool for any creative vision task. In particular,\ntext-to-image diffusion models [9, 30, 52, 77\u201379] are em-\nployed for guided image synthesis [8, 15, 18, 22, 27, 51, 87,\n97] and image editing tasks [5, 7, 10, 13, 28, 38, 47, 49, 55,\n75, 84]. Using image editing methods, one can edit an im-\nage of a given character, and change its pose, etc., however,\nthese methods cannot ensure consistency of the character in\nnovel contexts, as our problem dictates.\nIn addition, diffusion models were used in other tasks\n[56, 96], such as: video editing [23, 44, 45, 50, 59, 92], 3D\nsynthesis [19, 31, 48, 58], editing [11, 25, 74, 98] and tex-\n2\nturing [67], typography generation [35], motion generation\n[60, 81], and solving inverse problems [32].\nText-to-image personalization.\nText-conditioned mod-\nels cannot generate an image of a specific object or char-\nacter. To overcome this limitation, a line of works utilizes\nseveral images of the same instance to encapsulate new pri-\nors in the generative model. Existing solutions range from\noptimization of text tokens [20, 85, 88] to fine-tuning the\nparameters of the entire model [6, 70], where in the mid-\ndle, recent works suggest fine-tuning a small subset of pa-\nrameters [1, 17, 26, 33, 41, 71, 82]. Models trained in this\nmanner can generate consistent images of the same subject.\nHowever, they typically require a collection of images de-\npicting the subject, which naturally narrows their ability to\ngenerate any imaginary character. Moreover, when training\non a single input image [6], these methods tend to overfit\nand produce similar images with minimal diversity during\ninference.\nUnlike previous works, our method does not require an\ninput image; instead, it can generate consistent and diverse\nimages of the same character based only on a text descrip-\ntion. Additional works are aimed to bypass the personal-\nization training by introducing a dedicated personalization\nencoder [3, 16, 21, 37, 42, 76, 90, 93]. Given an image and\na prompt, these works can produce images with a character\nsimilar to the input. However, as shown in Section 4.1, they\nlack consistency when generating multiple images from the\nsame input. Concurrently, ConceptLab [66] is able to gen-\nerate new members of a broad category (e.g., a new pet);\nin contrast, we seek a consistent instance of a character de-\nscribed by the input text prompt.\nStory visualization.\nConsistent character generation is\nwell studied in the field of story visualization. Early GAN\nworks [43, 80] employ a story discriminator for the image-\ntext alignment. Recent works, such as StoryDALL-E [46]\nand Make-A-Story [62] utilize pre-trained T2I models for\nthe image generation, while an adapter model is trained\nto embed story captions and previous images into the T2I\nmodel. However, those methods cannot generalize to novel\ncharacters, as they are trained over specific datasets. More\nclosely related, Jeong et al. [36] generate consistent story-\nbooks by combining textual inversion with a face-swapping\nmechanism; therefore, their work relies on images of exist-\ning human-like characters. TaleCrafter [24] presents a com-\nprehensive pipeline for storybook visualization. However,\ntheir consistent character module is based on an existing\npersonalization method that requires fine-tuning on several\nimages of the same character.\nManual methods.\nOther attempts for achieving consis-\ntent character generation using a generative model rely on\nAlgorithm 1 Consistent Character Generation\nInput: Text-to-image diffusion model M, parameterized\nby \u0398 = (\u03b8, \u03c4), where \u03b8 are the LoRA weights and \u03c4 is\na set of custom text embeddings, target prompt p, feature\nextractor F.\nHyper-parameters: number of generated images per\nstep N, minimum cluster size dmin-c, target cluster size\ndsize-c, convergence criterion dconv, maximum number of\niterations diter\nOutput: a consistent representation \u0398(p)\nrepeat\nS = S\nN F(M\u0398(p))\nC = K-MEANS++(S, k = \u230aN/dsize-c\u230b)\nC = {c \u2208 C|dmin-c < |c|} {filter small clusters}\nccohesive = argmin\nc\u2208C\n1\n|c|\nP\ne\u2208c \u2225e \u2212 ccen\u22252\n\u0398 = argmin\n(\u03b8,\u03c4)\nLrec over ccohesive\nuntil dconv \u2265\n1\n|S|2\nP\ns1,s2\u2208S \u2225s1 \u2212 s2\u22252\nreturn \u0398\nad hoc and manually-intensive tricks such as using text to-\nkens of a celebrity, or a combination of celebrities [64] in\norder to create a consistent human; however, the gener-\nated characters resemble the original celebrities, and this\napproach does not generalize to other character types (e.g.,\nanimals). Users have also proposed to ensure consistency\nby manually crafting very long and elaborate text prompts\n[65], or by using image variations [63] and filtering them\nmanually by similarity [65]. Other users suggested gener-\nating a full design sheet of a character, then manually filter\nthe best results and use them for further generation [94].\nAll these methods are manual, labor-intensive, and ad hoc\nfor specific domains (e.g., humans). In contrast, our method\nis fully automated and domain-agnostic.\n3. Method\nAs stated earlier, our goal in this work is to enable gener-\nation of consistent images of a character (or another kind of\nvisual subject) based on a textual description. We achieve\nthis by iteratively customizing a pre-trained text-to-image\nmodel, using sets of images generated by the model itself as\ntraining data. Intuitively, we refine the representation of the\ntarget character by repeatedly funneling the model\u2019s output\ninto a consistent identity. Once the process has converged,\nthe resulting model can be used to generate consistent im-\nages of the target character in novel contexts. In this section,\nwe describe our method in detail.\nFormally, we are given a text-to-image model M\u0398, pa-\nrameterized by \u0398, and a text prompt p that describes a tar-\nget character. The parameters \u0398 consist of a set of model\nweights \u03b8 and a set of custom text embeddings \u03c4. We seek\n3\n\"Luna is a forest sprite \nwith green skin and \nleaves for hair.\u201d \nCluster\nIdentity\nExtract\nCoh\nes\nive\nRe\np\ne\na\nt\nM\u0398\nF\nS\nC\nM\u0398\nFigure 3. Method overview. Given an input text prompt, we start by generating numerous images using the text-to-image model M\u0398,\nwhich are embedded into a semantic feature space using the feature extractor F. Next, these embeddings are clustered and the most\ncohesive group is chosen, since it contains images with shared characteristics. The \u201ccommon ground\u201d among the images in this set is used\nto refine the representation \u0398 to better capture and fit the target. These steps are iterated until convergence to a consistent identity.\na representation \u0398(p), s.t., the parameterized model M\u0398(p)\nis able to generate consistent images of the character de-\nscribed by p in novel contexts.\nOur approach, described in Algorithm 1 and depicted in\nFigure 3, is based on the premise that a sufficiently large\nset of images generated by M for the same text prompt, but\nwith different seeds, will reflect the non-uniform density of\nthe manifold of generated images. Specifically, we expect\nto find some groups of images with shared characteristics.\nThe \u201ccommon ground\u201d among the images in one of these\ngroups can be used to refine the representation \u0398(p) so as to\nbetter capture and fit the target. We therefore propose to it-\neratively cluster the generated images, and use the most co-\nhesive cluster to refine \u0398(p). This process is repeated, with\nthe refined representation \u0398(p), until convergence. Below,\nwe describe the clustering and the representation refinement\ncomponents of our method in detail.\n3.1. Identity Clustering\nWe start each iteration by using M\u0398, parameterized with the\ncurrent representation \u0398, to generate a collection of N im-\nages, each corresponding to a different random seed. Each\nimage is embedded in a high-dimensional semantic embed-\nding space, using a feature extractor F, to form a set of\nembeddings S = S\nN F(M\u0398(p)). In our experiments, we\nuse DINOv2 [54] as the feature extractor F.\nNext, we use the K-MEANS++ [4] algorithm to cluster\nthe embeddings of the generated images according to cosine\nsimilarity in the embedding space. We filter the resulting\ncollection of clusters C by removing all clusters whose size\nis below a pre-defined threshold dmin-c, as it was shown [6]\nthat personalization algorithms are prone to overfitting on\nsmall datasets. Among the remaining clusters, we choose\nthe most cohesive one to serve as the input for the identity\nextraction stage (see Figure 4). We define the cohesion of a\ncluster c as the average distance between the members of c\nFigure 4. Embedding visualization. Given generated images for\nthe text prompt \u201ca sticker of a ginger cat\u201d, we project the set S of\ntheir high-dimensional embeddings into 2D using t-SNE [29] and\nindicate different K-MEANS++ [4] clusters using different colors.\nRepresentative images are shown for three of the clusters. It may\nbe seen that images in each cluster share the same characteristics:\nblack cluster \u2014 full body cats, red cluster \u2014 cat heads, brown\ncluster \u2014 images with multiple cats. According to our cohesion\nmeasure (1), the black cluster is the most cohesive, and therefore,\nchosen for identity extraction (or refinement).\nand its centroid ccen:\ncohesion(c) = 1\n|c|\nX\ne\u2208c\n\u2225e \u2212 ccen\u22252.\n(1)\nIn Figure 4 we show a visualization of the DINOv2 em-\nbedding space, where the high-dimensional embeddings S\nare projected into 2D using t-SNE [29] and colored accord-\ning to their K-MEANS++ [4] clusters. Some of the embed-\ndings are clustered together more tightly than others, and\nthe black cluster is chosen as the most cohesive one.\n4\n3.2. Identity Extraction\nDepending on the diversity of the image set generated in\nthe current iteration, the most cohesive cluster ccohesive may\nstill exhibit an inconsistent identity, as can be seen in Fig-\nure 3. The representation \u0398 is therefore not yet ready for\nconsistent generation, and we further refine it by training on\nthe images in ccohesive to extract a more consistent identity.\nThis refinement is performed using text-to-image personal-\nization methods [20, 70], which aim to extract a character\nfrom a given set of several images that already depict a con-\nsistent identity. While we apply them to a set of images\nwhich are not completely consistent, the fact that these im-\nages are chosen based on their semantic similarity to each\nother, enables these methods to nevertheless distill a com-\nmon identity from them.\nWe base our solution on a pre-trained Stable Diffusion\nXL (SDXL) [57] model, which utilizes two text encoders:\nCLIP [61] and OpenCLIP [34]. We perform textual inver-\nsion [20] to add a new pair of textual tokens \u03c4, one for each\nof the two text encoders. However, we found that this pa-\nrameter space is not expressive enough, as demonstrated in\nSection 4.3, hence we also update the model weights \u03b8 via a\nlow-rank adaptation (LoRA) [33, 71] of the self- and cross-\nattention layers of the model.\nWe use the standard denoising loss:\nLrec = Ex\u223cccohesive,z\u223cE(x),\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u0398(p)(zt, t)\u22252\n2\ni\n,\n(2)\nwhere ccohesive is the chosen cluster, E(x) is the VAE en-\ncoder of SDXL, \u03f5 is the sample\u2019s noise and t is the time\nstep, zt is the latent z noised to time step t. We optimize\nLrec over \u0398 = (\u03b8, \u03c4), the union of the LoRA weights and\nthe newly-added textual tokens.\n3.3. Convergence\nAs explained earlier (Algorithm 1 and Figure 3), the above\nprocess is performed iteratively. Note that the representa-\ntion \u0398 extracted in each iteration is the one used to generate\nthe set of N images for the next iteration. The generated\nimages are thus funneled into a consistent identity.\nRather than using a fixed number of iterations, we apply\na convergence criterion that enables early stopping. After\neach iteration, we calculate the average pairwise Euclidean\ndistance between all N embeddings of the newly-generated\nimages, and stop when this distance is smaller than a pre-\ndefined threshold dconv.\nFinally, it should be noticed that our method is non-\ndeterministic, i.e., when running our method multiple times,\non the same input prompt p, different consistent characters\nwill be generated. This is aligned with the one-to-many na-\nture of our task. For more details and examples, please refer\nto the supplementary material.\n4. Experiments\nIn Section 4.1 we compare our method against several\nbaselines, both qualitatively and quantitatively.\nNext, in\nSection 4.2 we describe the user study we conducted and\npresent its results. The results of an ablation study are re-\nported in Section 4.3. Finally, in Section 4.4 we demon-\nstrate several applications of our method.\n4.1. Qualitative and Quantitative Comparison\nWe compared our method against the most related person-\nalization techniques [20, 42, 71, 89, 93]. In each exper-\niment, each of these techniques is used to extract a char-\nacter from a single image, generated by SDXL [57] from\nan input prompt p. The same prompt p is also provided\nas input to our method. Textual Inversion (TI) [20] opti-\nmizes a textual token using several images of the same con-\ncept, and we converted it to support SDXL by learning two\ntext tokens, one for each of its text encoders, as we did in\nour method. In addition, we used LoRA DreamBooth [71]\n(LoRA DB), which we found less prone to overfitting than\nstandard DB. Furthermore, we compared against all avail-\nable image encoder techniques that encode a single image\ninto the textual space of the diffusion model for later gener-\nation in novel contexts: BLIP-Diffusion [42], ELITE [89],\nand IP-adapter [93]. For all the baselines, we used the same\nprompt p to generate a single image, and used it to extract\nthe identity via optimization (TI and LoRA DB) or encod-\ning (ELITE, BLIP-diffusion and IP-adapter).\nIn Figure 5 we qualitatively compare our method against\nthe above baselines. While TI [20], BLIP-diffusion [42]\nand IP-adapter [93] are able to follow the specified prompt,\nthey fail to produce a consistent character. LoRA DB [71]\nsucceeds in consistent generation, but it does not always re-\nspond to the prompt. Furthermore, the resulting character\nis generated in the same fixed pose. ELITE [90] struggles\nwith prompt following and the generated characters tend to\nbe deformed. In comparison, our method is able to follow\nthe prompt and maintain consistency, while generating ap-\npealing characters in different poses and viewing angles.\nIn order to automatically evaluate our method and the\nbaselines quantitatively, we instructed ChatGPT [53] to\ngenerate prompts for characters of different types (e.g., ani-\nmals, creatures, objects, etc.) in different styles (e.g., stick-\ners, animations, photorealistic images, etc.). Each of these\nprompts was then used to extract a consistent character by\nour method and by each of the baselines. Next, we gen-\nerated these characters in a predefined collection of novel\ncontexts. For a visual comparison, please refer to the sup-\nplementary material.\nWe employ two standard evaluation metrics: prompt\nsimilarity and identity consistency, which are commonly\nused in the personalization literature [6, 20, 70]. Prompt\nsimilarity measures the correspondence between the gener-\n5\nTI [20]\nLoRA DB [71]\nELITE [90]\nBLIP-diff [42]\nIP-Adapter [93]\nOurs\n\u201cindoors\u201d\n\u201cin the park\u201d\n\u201ca photo of a white fluffy toy\u201d\n\u201cwearing a red\nhat in the street\u201d\n\u201cjumping near\nthe river\u201d\n\u201ca 3D animation of a happy pig\u201d\n\u201cnear the\nGolden Gate\nBridge\u201d\n\u201cin the snow\u201d\n\u201ca rendering of a fox, full body\u201d\nFigure 5. Qualitative comparison. We compare our method against several baselines: TI [20], BLIP-diffusion [42] and IP-adapter [93]\nare able to follow the target prompts, but do not preserve a consistent identity. LoRA DB [71] is able to maintain consistency, but it does not\nalways follow the prompt. Furthermore, the character is generated in the same fixed pose. ELITE [90] struggles with prompt following and\nalso tends to generate deformed characters. On the other hand, our method is able to follow the prompt and maintain consistent identities,\nwhile generating the characters in different poses and viewing angles.\nated images and the input text prompt. We use the standard\nCLIP [61] similarity, i.e., the normalized cosine similarity\nbetween the CLIP image embedding of the generated im-\nages and the CLIP text embedding of the source prompts.\nFor measuring identity consistency, we calculate the sim-\nilarity between the CLIP image embeddings of generated\n6\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.7\n0.75\n0.8\n0.85\n0.9\nTI\nLoRA DB\nELITE\nBLIP-diffusion\nIP-adapter\nOurs single iter.\nOurs w/o LoRA\nOurs w reinit.\nOurs w/o clustering\nOurs\nAutomatic prompt similarity (\u2192)\nAutomatic identity consistency (\u2192)\n2.9\n3\n3.1\n3.2\n3.3\n3.4\n2.8\n3\n3.2\n3.4\n3.6\n3.8\nTI\nLoRA DB\nELITE\nBLIP\nIP-Adapter\nOurs\nUser prompt similarity ranking (\u2192)\nUser identity consistency (\u2192)\nFigure 6. Quantitative Comparison and User Study. (Left) We compared our method quantitatively with various baselines in terms of\nidentity consistency and prompt similarity, as explained in Section 4.1. LoRA DB and ELITE maintain high identity consistency, while\nsacrificing prompt similarity. TI and BLIP-diffusion achieve high prompt similarity but low identity consistency. Our method and IP-\nadapter both lie on the Pareto front, but the better identity consistency of our method is perceptually significant, as demonstrated in the\nuser study. We also ablated some components of our method: removing the clustering stage, reducing the optimizable representation,\nre-initializing the representation in each iteration and performing only a single iteration. All of the ablated cases resulted in a significant\ndegradation of consistency. (Right) The user study rankings also demonstrate that our method lies on the Pareto front, balancing between\nidentity consistency and prompt similarity.\nimages of the same concept across different contexts.\nAs can be seen in Figure 6 (left), there is an inher-\nent trade-off between prompt similarity and identity con-\nsistency: LoRA DB and ELITE exhibit high identity con-\nsistency, while sacrificing prompt similarity. TI and BLIP-\ndiffusion achieve high prompt similarity but low identity\nconsistency.\nOur method and IP-adapter both lie on the\nPareto front. However, our method achieves better iden-\ntity consistency than IP-adapter, which is significant from\nthe user\u2019s perspective, as supported by our user study.\n4.2. User Study\nWe conducted a user study to evaluate our method, using the\nAmazon Mechanical Turk (AMT) platform [2]. We used\nthe same generated prompts and samples that were used in\nSection 4.1 and asked the evaluators to rate the prompt sim-\nilarity and identity consistency of each result on a Likert\nscale of 1\u20135. For ranking the prompt similarity, the eval-\nuators were presented with the target text prompt and the\nresult of our method and the baselines on the same page,\nand were asked to rate each of the images. For identity con-\nsistency, for each of the generated concepts, we compared\nour method and the baselines by randomly choosing pairs\nof generated images with different target prompts, and the\nevaluators were asked to rate on a scale of 1\u20135 whether the\nimages contain the same main character. Again, all the pairs\nof the same character for the different baselines were shown\non the same page.\nAs can be seen in Figure 6 (right), our method again\nexhibits a good balance between identity consistency and\nprompt similarity, with a wider gap separating it from the\nbaselines. For more details and statistical significance anal-\nysis, read the supplementary material.\n4.3. Ablation Study\nWe conducted an ablation study for the following cases: (1)\nWithout clustering \u2014 we omit the clustering step described\nin Section 3.1, and instead simply generate 5 images ac-\ncording to the input prompt. (2) Without LoRA \u2014 we re-\nduce the optimizable representation \u0398 in the identity ex-\ntraction stage, as described in Section 3.2, to consist of only\nthe newly-added text tokens without the additional LoRA\nweights. (3) With re-initialization \u2014 instead of using the\nlatest representation \u0398 in each of the optimization itera-\ntions, as described in Section 3.3, we re-initialize it in each\niteration. (4) Single iteration \u2014 rather than iterating until\nconvergence (Section 3.3), we stop after a single iteration.\nAs can be seen in Figure 6 (left), all of the above key\ncomponents are crucial for achieving a consistent identity\nin the final result: (1) removing the clustering harms the\nidentity extraction stage because the training set is too di-\nverse, (2) reducing the representation causes underfitting,\nas the model does not have enough parameters to properly\ncapture the identity, (3) re-initializing the representation in\neach iteration, or (4) performing a single iteration, does not\nallow the model to converge into a single identity.\nFor a visual comparison of the ablation study, as well as\ncomparison of alternative feature extractors (DINOv1 [14]\nand CLIP [61]), please refer to the supplementary material.\n4.4. Applications\nAs demonstrated in Figure 7, our method can be used for\nvarious down-stream tasks, such as (a) Illustrating a story by\nbreaking it into a different scenes and using the same con-\n7\n\u201cThis is a story about Jasper, a cute mink with a brown jacket and red pants.\nJasper started his day by jogging on the beach, and afterwards, he enjoyed a\ncoffee meetup with a friend in the heart of New York City. As the day drew\nto a close, he settled into his cozy apartment to review a paper.\u201d\n(a) Story\nillustration\nScene 1\nScene 2\nScene 3\nScene 4\n\u201ca Plasticine of a cute baby cat with big eyes\u201d\n(b) Local\nimage editing\nImage + mask\n\u201csitting\u201d\n\u201c jumping\u201d\n\u201cwearing\nsunglasses\u201d\n\u201ca photo of a ginger woman with long hair\u201d\n(c) Additional\npose control\nInput Pose 1\nResult 1\nInput Pose 2\nResult 2\nFigure 7. Applications. Our method can be used for various ap-\nplications: (a) Illustrating a full story with the same consistent\ncharacter. (b) Local text-driven image editing via integration with\nBlended Latent Diffusion [5, 7]. (c) Generating a consistent char-\nacter with an additional pose control via integration with Control-\nNet [97].\nsistent character for all of them. (b) Local text-driven image\nediting by integrating Blended Latent Diffusion [5, 7] \u2014 a\nconsistent character can be injected into a specified location\nof a provided background image, in a novel pose specified\nby a text prompt. (c) Generating a consistent character with\nan additional pose control using ControlNet [97]. For more\ndetails, please refer to the supplementary material.\n5. Limitations and Conclusions\nWe found our method to suffer from the following limita-\ntions: (a) Inconsistent identity \u2014 in some cases, our method\nis not able to converge to a fully consistent identity (with-\nout overfitting). As demonstrated in Figure 8(a), when try-\ning to generate a portrait of a robot, our method generated\nrobots with slightly different colors and shapes (e.g., differ-\nent arms). This may result from a prompt that is too gen-\neral, for which identity clustering (Section 3.1) is not able to\nfind a sufficiently cohesive set. (b) Inconsistent supporting\ncharacters/elements\u2014 although our method is able to find\na consistent identity for the character described by the in-\nput prompt, the identities of other characters, related to the\ninput character (e.g., their pet), might be inconsistent. For\nexample, in Figure 8(b) the input prompt p to our method\ndescribed only the girl, and when asked to generate the girl\n(a) Inconsistent\nidentity\n\u201ca portrait of a round robot with glasses ...\u201d\n(b) Inconsistent\nsupporting elements\n\u201ca hyper-realistic digital painting of a happy girl, brown eyes...\u201d\n+ \u201cwith her cat\u201d\n(c) Spurious\nattributes\n\u201ca sticker of a ginger cat\u201d\nFigure 8. Limitations. Our method suffers from the following\nlimitations: (a) in some cases, our method is not able to converge\nto a fully consistent identity \u2014 notice slight color and arm shape\nchanges. (b) Our method is not able to associate a consistent iden-\ntity to a supporting character that may appear with the main ex-\ntracted character, for example our method generates different cats\nfor the same girl. (c) Our method sometimes adds spurious at-\ntributes to the character, that were not present in the text prompt.\nFor example, it learns to associate green leaves with the cat sticker.\nwith her cat, different cats were generated. In addition, our\nframework does not support finding multiple concepts con-\ncurrently [6]. (c) Spurious attributes \u2014 we found that in\nsome cases, our method binds additional attributes, which\nare not part of the input text prompt, with the final iden-\ntity of the character. For example, in Figure 8(c), the input\ntext prompt was \u201ca sticker of a ginger cat\u201d, however, our\nmethod added green leaves to the generated sticker, even\nthough it was not asked to do so.\nThis stems from the\nstochastic nature of the text-to-image model \u2014 the model\nadded these leaves in some of the stickers generated during\nthe identity clustering stage (Section 3.1), and the stickers\ncontaining the leaves happened to form the most cohesive\nset ccohesive. (d) Significant computational cost \u2014 each iter-\nation of our method involves generating a large number of\nimages, and learning the identity of the most cohesive clus-\nter. It takes about 20 minutes to converge into a consistent\nidentity. Reducing the computational costs is an appealing\ndirection for further research.\nIn conclusion, in this paper we offered the first fully-\nautomated solution to the problem of consistent character\ngeneration. We hope that our work will pave the way for\nfuture advancements, as we believe this technology of con-\nsistent character generation may have a disruptive effect on\nnumerous sectors, including education, storytelling, enter-\ntainment, fashion, brand design, advertising, and more.\n8\nAcknowledgments. We thank Yael Pitch, Matan Cohen,\nNeal Wadhwa and Yaron Brodsky for their valuable help\nand feedback.\nA. Additional Experiments\nBelow, we provide additional experiments that were omit-\nted from the main paper. In Appendix A.1 we provide addi-\ntional comparisons and results of our method, and demon-\nstrate its non-deterministic nature in Appendix A.2. In Ap-\npendix A.3 we compare our method against two na\u00a8\u0131ve base-\nlines. Appendix A.4 presents the results of our method us-\ning different feature extractors. Lastly, in Appendix A.6 we\nprovide results that reduce the concerns of dataset memo-\nrization by our method.\nA.1. Additional Comparisons and Results\nIn Figure 9 we provide a qualitative comparison on the au-\ntomatically generated prompts, and in Figure 10 we provide\nan additional qualitative comparison.\nConcurrently to our work, the DALL\u00b7E 3 model [12] was\ncommercially released as part of the paid ChatGPT Plus\n[53] subscription, enabling generating images in a conver-\nsational setting. We tried, using a conversation, to create a\nconsistent character of a Plasticine cat, as demonstrated in\nFigure 11. As can be seen, the generated characters share\nonly some of the characteristics (e.g., big eyes) but not all\nof them (e.g., colors, textures and shapes).\nIn Figure 12 we provide a qualitative comparison of the\nablated cases. In addition, as demonstrated in Figure 13, our\napproach is applicable to consistent generation of a wide\nrange of subjects, without the requirement for them to nec-\nessarily depict human characters or creatures. Figure 14\nshows additional results of our method, demonstrating a va-\nriety of character styles. Lastly, in Figure 15 we demon-\nstrate the ability of creating a fully consistent \u201clife story\u201d of\na character using our method.\nA.2. Non-determinism of Our Method\nIn Figures 16 and 17 we demonstrate the non-deterministic\nnature of our method.\nUsing the same text prompt, we\nrun our method multiple times with different initial seeds,\nthereby generating a different set of images for the identity\nclustering stage (Section 3.1). Consequently, the most co-\nhesive cluster ccohesive is different in each run, yielding dif-\nferent consistent identities. This behavior of our method is\naligned with the one-to-many nature of our task \u2014 a single\ntext prompt may correspond to many identities.\nA.3. Na\u00a8\u0131ve Baselines\nAs explained in Section 4.1, we compared our method\nagainst a version of TI [20] and LoRA DB [71] that were\ntrained on a single image (with a single identity). Instead,\nwe could generate a small set of five images for the given\nprompt (that are not guaranteed to be of the same identity),\nand use this small dataset for TI and LoRA DB baselines,\nreferred to as TI multi and LoRA DB multi, respectively. As\ncan be seen in Figures 18 and 19, these baselines fail to\nachieve satisfactory identity consistency.\nA.4. Additional Feature Extractors\nInstead of using DINOv2 [54] features for the identity clus-\ntering stage (Section 3.1), we also experimented with two\nalternative feature extractors: DINOv1 [14] and CLIP [61]\nimage encoder. We quantitatively evaluate our method with\neach of these feature extractors in terms of identity consis-\ntency and prompt similarity, as explained in Section 4.1. As\ncan be seen in Figure 20, DINOv1 produces higher identity\nconsistency, while sacrificing prompt similarity, whereas\nCLIP achieves higher prompt similarity at the expense of\nidentity consistency. Qualitatively, as demonstrated in Fig-\nure 21, we found the DINOv1 extractor to perform similarly\nto DINOv2, whereas CLIP produces results with a slightly\nlower identity consistency.\nA.5. Additional Clustering Visualization\nIn Figure 22 we provide a visualization of the clustering al-\ngorithm described in Section 3.1. As can be seen, given the\ninput text prompt \u201ca purple astronaut, digital art, smooth,\nsharp focus, vector art\u201d, in the first iteration (top three\nrows), our algorithm divides the generated image set into\nthree clusters: (1) focusing on the astronaut\u2019s head, (2) an\nastronaut with no face, and (3) a full body astronaut. In the\nsecond iteration (bottom three rows), all the clusters share\nthe same identity, that was extracted in the first iteration,\nas described in Section 3.2, and our algorithm divides them\ninto clusters by their pose.\nA.6. Dataset Non-Memorization\nOur method is able to produce consistent characters, which\nraises the question of whether these characters already ex-\nist in the training data of the generative model. We em-\nployed SDXL [57] as our text-to-image model, whose train-\ning dataset is, unfortunately, undisclosed in the paper [57].\nConsequently, we relied on the most likely overlapping\ndataset, LAION-5B [73], which was also utilized by Stable\nDiffusion V2.\nTo probe for dataset memorization, we found the top 5\nnearest neighbors in the dataset in terms of CLIP [61] im-\nage similarity, for a few representative characters from our\npaper, using an open-source solution [68]. As demonstrated\nin Figure 23, our method does not simply memorize images\nfrom the LAION-5B dataset.\n9\nTI [20]\nLoRA DB [71]\nELITE [90]\nBLIP-diff [42]\nIP-Adapter [93]\nOurs\n\u201cdrinking\na beer\u201d\n\u201cwith a city in\nthe background\u201d\n\u201ca 2D animation of captivating Arctic fox with fluffy fur, bright eyes, and\nnimble movements, bringing the magic of the icy wilderness to animated life\u201d\n\u201ceating\na burger\u201d\n\u201cwearing a\nblue hat\u201d\n\u201ca watercolor portrayal of a joyful child, radiating innocence and wonder with\nrosy cheeks and a genuine, wide-eyed smile\u201d\n\u201cnear the Statue\nof Liberty\u201d\n\u201cas a\npolice officer\u201d\n\u201ca 3D animation of a playful kitten, with bright eyes and a\nmischievous expression, embodying youthful curiosity and joy\u201d\nFigure 9. Qualitative comparison to baselines on the automatically generated prompts. We compared our method against several\nbaselines: TI [20], BLIP-diffusion [42] and IP-adapter [93] are able to correspond to the target prompt but fail to produce consistent\nresults. LoRA DB [71] is able to achieve consistency, but it does not always follow to the prompt, in addition, the generate character is\nbeing generated in the same fixed pose. ELITE [90] struggles with following the prompt and also tends to generate deformed characters.\nOur method is able to follow the prompt, and generate consistent characters in different poses and viewing angles.\n10\nTI [20]\nLoRA DB [71]\nELITE [90]\nBLIP-diff [42]\nIP-Adapter [93]\nOurs\n\u201cin the\ndesert\u201d\n\u201ctaking a\npicture with\nhis phone\u201d\n\u201can oil painting of a man with a mustache and a hat\u201d\n\u201cworking on\nhis laptop\u201d\n\u201ceating\na burger\u201d\n\u201ca Plasticine of a cute baby cat with big eyes\u201d\n\u201ccelebrating in\na party\u201d\n\u201cin the forest\u201d\n\u201ca rendering of a cute turtle, cozy lighting ...\u201d\nFigure 10. Additional qualitative comparisons to baselines. We compared our method against several baselines: TI [20], BLIP-diffusion\n[42] and IP-adapter [93] are able to correspond to the target prompt but fail to produce consistent results. LoRA DB [71] is able to achieve\nconsistency, but it does not always follow to the prompt, in addition, the generate character is being generated in the same fixed pose.\nELITE [90] struggles with following the prompt and also tends to generate deformed characters. On the other hand, our method is able to\nfollow the prompt, and generate consistent characters in different poses and viewing angles.\nA.7. Stable Diffusion 2 Results\nWe experimented with a version of our method that uses\nthe Stable Diffusion 2 [69] model. The implementation is\nthe same as explained in Appendix B.1, with the following\nchanges: (1) The set of custom text embeddings \u03c4 in the\ncharacter representation \u0398 (as explained in Section 2 in the\n11\n\u201cholding an\n\u201cin the park\u201d\n\u201creading a book\u201d\n\u201cat the beach\u201d\navocado\u201d\nDALL\u00b7E 3 [12]\nOurs\nFigure 11. DALL\u00b7E 3 comparison. We attempted to create a\nconsistent character using the commercial ChatGPT Plus system,\nfor the given prompt \u201ca Plasticine of a cute baby cat with big\neyes\u201d. As can be seen, the DALL\u00b7E 3 generated characters share\nonly some of the characteristics (e.g., big eyes) but not all of them\n(e.g., colors, textures and shapes).\nmain paper ), contains only one text embedding. (2) We\nused a higher learning rate of 5e-4. The rest of the imple-\nmentation details are the same. More specifically, we used\nStable Diffusion v2.1 implementation from Diffusers [86]\nlibrary.\nAs can be seen in Figure 24, when using the Stable Diffu-\nsion 2 backbone, our method can extract a consistent char-\nacter, however, as expected, the results are of a lower quality\nthan when using the SDXL [57] backbone that we use in\nthe rest of this paper.\nB. Implementation Details\nIn this section, we provide the implementation details that\nwere omitted from the main paper. In Appendix B.1 we pro-\nvide the implementation details of our method and the base-\nlines. Then, in Appendix B.2 we provide the implementa-\ntion details of the automatic metrics that we used to evaluate\nour method against the baselines. In Appendix B.3 we pro-\nvide the implementation details and the statistical analysis\nfor the user study we conducted. Lastly, in Appendix B.4\nwe provide the implementation details for the applications\nwe presented.\nB.1. Method Implementation Details\nWe based our method, and all the baselines (except\nELITE [90] and BLIP-diffusion [42]) on Stable Diffusion\nXL (SDXL) [57], which is the state-of-the-art open source\ntext-to-image model, at the writing of this paper.\nWe\nused the official ELITE implementation, that uses Stable\nDiffusion V1.4, and the official implementation of BLIP-\ndiffusion, that uses Stable Diffusion V1.5. We could not\nreplace these two baselines to SDXL backbone, as the en-\ncoders were trained on these specific models. As for the rest\nof the baselines, we used the same SDXL architecture and\nweights.\nFor our method, we generated a set of N = 128 images\nat each iteration, which we found to be sufficient, empiri-\ncally. We utilized the Adam optimizer [39] with learning\nrate of 3e-5, \u03b21 = 0.9, \u03b22 = 0.99 and weight decay of\n1e-2. In each identity extraction iteration of our method,\nwe used 500 steps. We also found empirically that we can\nset the convergence criterion dconv adaptively to be 80% of\nthe average pairwise Euclidean distance between all N ini-\ntial image embeddings of the first iteration. In most cases,\nour method converges in 1\u20132 iterations, which takes about\n13\u201326 minutes on A100 NVIDIA GPU when using bfloat16\nmixed precision.\nList of the third-party packages that we used:\n\u2022 Official\nSDXL\n[57]\nimplementation\nby\nHugging-\nFace Diffusers [86] at https://github.com/\nhuggingface/diffusers\n\u2022 Official SDXL LoRA DB implementation by Hugging-\nFace Diffusers [86] at https://github.com/\nhuggingface/diffusers.\n\u2022 Official ELITE [90] implementation at https://\ngithub.com/csyxwei/ELITE\n\u2022 Official BLIP-diffusion [42] implementation at https:\n/ / github . com / salesforce / LAVIS / tree /\nmain/projects/blip-diffusion\n\u2022 Official IP-adapter [93] implementation at https://\ngithub.com/tencent-ailab/IP-Adapter\n\u2022 DINOv2 [54] ViT-g/14, DINOv1 [14] ViT-B/16 and\nCLIP [61] ViT-L/14 implementation by HuggingFace\nTransformers [91] at https : / / github . com /\nhuggingface/transformers\nB.2. Automatic Metrics Implementation Details\nIn order to automatically evaluate our method and the base-\nlines quantitatively, we instructed ChatGPT [53] to gener-\nate prompts for characters of different types (e.g., animals,\ncreatures, objects, etc.) in different styles (e.g., stickers, an-\nimations, photorealistic images, etc.). These prompts were\nthen used to generate a set of consistent characters by our\nmethod and by each of the baselines. Next, these prompts\nwere used to generate these characters in a predefined col-\nlection of novel contexts from the following list:\n\u2022 \u201ca photo of [v] at the beach\u201d\n\u2022 \u201ca photo of [v] in the jungle\u201d\n\u2022 \u201ca photo of [v] in the snow\u201d\n\u2022 \u201ca photo of [v] in the street\u201d\n\u2022 \u201ca photo of [v] with a city in the background\u201d\n\u2022 \u201ca photo of [v] with a mountain in the background\u201d\n\u2022 \u201ca photo of [v] with the Eiffel Tower in the background\u201d\n\u2022 \u201ca photo of [v] near the Statue of Liberty\u201d\n\u2022 \u201ca photo of [v] near the Sydney Opera House\u201d\n\u2022 \u201ca photo of [v] floating on top of water\u201d\n\u2022 \u201ca photo of [v] eating a burger\u201d\n12\nTable 1. Users\u2019 rankings means and variances. The means and\nvariances of the rankings that are reported in the user study.\nMethod\nPrompt similarity (\u2191)\nIdentity consistency (\u2191)\nTI [20]\n3.31 \u00b1 1.43\n3.17 \u00b1 1.17\nLoRA DB [71]\n3.03 \u00b1 1.43\n3.67 \u00b1 1.20\nELITE [90]\n2.87 \u00b1 1.46\n3.20 \u00b1 1.21\nBLIP-Diffusion [42]\n3.35 \u00b1 1.41\n2.76 \u00b1 1.31\nIP-Adapter [93]\n3.25 \u00b1 1.42\n2.99 \u00b1 1.28\nOurs\n3.30 \u00b1 1.36\n3.48 \u00b1 1.20\nTable 2. Statistical analysis. We use Tukey\u2019s honestly significant\ndifference procedure [83] to test whether the differences between\nmean scores in our user study are statistically significant.\nMethod 1\nMethod 2\nPrompt similarity\nIdentities similarity\np-value\np-value\nTI [20]\nOurs\np < 0.001\np < 1e\u221210\nLoRA DB [71]\nOurs\np < 1e\u221213\n1e\u22124\nELITE [90]\nOurs\np < 1e\u221213\np < 1e\u22127\nBLIP-Diffusion [42]\nOurs\np < 0.01\np < 1e\u221213\nIP-Adapter [93]\nOurs\np < 1e\u22125\np < 1e\u221213\n\u2022 \u201ca photo of [v] drinking a beer\u201d\n\u2022 \u201ca photo of [v] wearing a blue hat\u201d\n\u2022 \u201ca photo of [v] wearing sunglasses\u201d\n\u2022 \u201ca photo of [v] playing with a ball\u201d\n\u2022 \u201ca photo of [v] as a police officer\u201d\nwhere [v] is the newly-added token that represents the con-\nsistent character.\nB.3. User Study Details\nAs explained in Section 4.2, we conducted a user study to\nevaluate our method, using the Amazon Mechanical Turk\n(AMT) platform [2]. We used the same generated prompts\nand samples that were used in Section 4.1, and asked the\nevaluators to rate the prompt similarity and identity consis-\ntency of each result on a Likert scale of 1\u20135. For ranking\nthe prompt similarity, the evaluators were instructed the fol-\nlowing: \u201cFor each of the following images, please rank on\na scale of 1 to 5 its correspondence to this text description:\n{PROMPT}. The character in the image can be anything\n(e.g., a person, an animal, a toy etc.\u201d where {PROMPT}\nis the target text prompt (in which we replaced the special\ntoken with the word \u201ccharacter\u201d). All the baselines, as well\nas our method, were presented in the same page, and the\nevaluators were asked to rate each one of the results using\na slider from 1 (\u201cDo not match at all\u201d) to 5 (\u201cMatch per-\nfectly\u201d). Next, to assess identity consistency, we took for\neach one of the characters two generated images that corre-\nspond to different target text prompts, put them next to each\nother, and instructed the evaluators the following: \u201cFor each\nof the following image pairs, please rank on a scale of 1 to 5\nif they contain the same character (1 means that they contain\ntotally different characters and 5 means that they contain\nexactly the same character). The images can have different\nbackgrounds\u201d. We put all the compared images on the same\npage, and the evaluators were asked to rate each one of the\npairs using a slider from 1 (\u201cTotally different characters\u201d)\nto 5 (\u201cExactly the same character\u201d).\nWe collected three ratings per question, resulting in\n1104 ratings per task (prompt similarity and identity con-\nsistency). The time allotted per task was one hour, to allow\nthe raters to properly evaluate the results without time pres-\nsure. The means and variances of the user study responses\nare reported in Table 1.\nIn addition, we conducted a statistical analysis of our\nuser study by validating that the difference between all the\nconditions is statistically significant using Kruskal-Wallis\n[40] test (p < 1e\u221228 for the text similarity test and p <\n1e\u221276 for the identity consistency text). Lastly, we used\nTukey\u2019s honestly significant difference procedure [83] to\nshow that the comparison of our method against all the base-\nlines is statistically significant, as detailed in Table 2.\nB.4. Applications Implementation Details\nIn Section 4.4, we presented three downstream applications\nof our method.\nStory illustration.\nGiven a long story, e.g., \u201cThis is a\nstory about Jasper, a cute mink with a brown jacket and red\npants. Jasper started his day by jogging on the beach, and\nafterwards, he enjoyed a coffee meetup with a friend in the\nheart of New York City. As the day drew to a close, he settled\ninto his cozy apartment to review a paper\u201d, one can create\na consistent character from the main character description\n(\u201ca cute mink with a brown jacket and red pants\u201d), then\nthey can generate the various scenes by simply rephrasing\nthe sentence:\n1. \u201c[v] jogging on the beach\u201d\n2. \u201c[v] drinking coffee with his friend in the heart of New\nYork City\u201d\n3. \u201c[v] reviewing a paper in his cozy apartment\u201d\nLocal image editing.\nOur method can be simply inte-\ngrated with Blended Latent Diffusion [5, 7] for editing im-\nages locally: given a text prompt, we start by running our\nmethod to extract a consistent identity, then, given an input\nimage and mask, we can plant the character in the image\nwithin the mask boundaries. In addition, we can provide a\nlocal text description for the character.\nAdditional pose control.\nOur method can be integrated\nwith ControlNet [97]: given a text prompt, we first apply\nour method to extract a consistent identity \u0398 = (\u03b8, \u03c4),\nwhere \u03b8 are the LoRA weights and \u03c4 is a set of custom text\nembeddings. Then, we can take an off-the-shelf pre-trained\n13\nControlNet model, plug-in our representation \u0398, and use it\nto generate the consistent character in different poses given\nby the user.\nC. Societal Impact\nWe believe that the emergence of technology that facilitates\nthe effortless creation of consistent characters holds excit-\ning promise in a variety of creative and practical applica-\ntions. It can empower storytellers and content creators to\nbring their narratives to life with vivid and unique charac-\nters, enhancing the immersive quality of their work. In ad-\ndition, it may offer accessibility to those who may not pos-\nsess traditional artistic skills, democratizing character de-\nsign in the creative industry. Furthermore, it can reduce the\ncost of advertising, and open up new opportunities for small\nand underprivileged entrepreneurs, enabling them to reach a\nwider audience and compete in the market more effectively.\nOn the other hand, as any other generative AI technol-\nogy, it can be misused by creating false and misleading vi-\nsual content for deceptive purposes. Creating fake charac-\nters or personas can be used for online scams, disinforma-\ntion campaigns, etc., making it challenging to discern gen-\nuine information from fabricated content. Such technolo-\ngies underscore the vital importance of developing gener-\nated content detection systems, making it a compelling re-\nsearch direction to address.\n14\nOurs single iter.\nOurs w/o clust.\nOurs w/o LoRA\nOurs w reinit.\nOurs\n\u201cdrinking a beer\u201d\n\u201cwith a city in\nthe background\u201d\n\u201ca 2D animation of captivating Arctic fox with fluffy fur, bright\nand nimble movements, bringing the magic of the icy wilderness to animated life\u201d\n\u201ceating a burger\u201d\n\u201cwearing a\nblue hat\u201d\n\u201ca watercolor portrayal of a joyful child, radiating innocence\nand wonder with rosy cheeks and a genuine, wide-eyed smile\u201d\n\u201cnear the\nStatue of Liberty\u201d\n\u201cas a police\nofficer\u201d\n\u201ca 3D animation of a playful kitten, with bright eyes and\na mischievous expression, embodying youthful curiosity and joy\u201d\nFigure 12. Qualitative comparison of ablations. We ablated the following components of our method: using a single iteration, removing\nthe clustering stage, removing the LoRA trainable parameters, using the same initial representation at every iteration. As can be seen, all\nthese ablated cases struggle with preserving the character\u2019s consistency.\n15\n\u201cin the desert\u201d\n\u201cin Times Square\u201d\n\u201cnear a lake\u201d\n\u201cnear the Eiffel Tower\u201d \u201cnear the Taj Mahal\u201d\n\u201ca photo of a bottle of water\u201d\n\u201ca photo of a blue car\u201d\n\u201ca photo of a purple bag\u201d\n\u201ca photo of a green bowl\u201d\nFigure 13. Consistent generation of non-character objects. Our approach is applicable to a wide range of objects, without the require-\nment for them to depict human characters or creatures.\n16\n\u201cholding an\n\u201cin the park\u201d\n\u201creading a book\u201d\n\u201cat the beach\u201d\navocado\u201d\n\u201ca portrait of a woman with a large hat in a scenic environment, fauvism\u201d\n\u201ca 3D animation of a happy pig\u201d\n\u201ca sticker of a ginger cat\u201d\n\u201ca purple astronaut, digital art, smooth, sharp focus, vector art\u201d\nFigure 14. Additional results. Our method is able to consistently generate different types and styles of characters, e.g., paintings,\nanimations, stickers and vector art.\n17\n\u201cas a baby\u201d\n\u201cas a small child\u201d\n\u201cas a teenager\u201d\n\u201cwith his\n\u201cbefore the prom\u201d\nfirst girlfriend\u201d\n\u201cas a soldier\u201d\n\u201cin the\n\u201csitting in a lecture\u201d\n\u201cplaying football\u201d\n\u201cdrinking a beer\u201d\ncollege campus\u201d\n\u201cstudying in\n\u201chappy with his\n\u201cgiving a talk\n\u201cgraduating from\n\u201ca profile picture\u201d\nhis room\u201d\naccepted paper\u201d\nin a conference\u201d\ncollege\u201d\n\u201cworking in a\n\u201cin his wedding\u201d\n\u201cwith his\n\u201cas a 50\n\u201cas a 70\ncoffee shop\u201d\nsmall child\u201d\nyears old man\u201d\nyears old man\u201d\n\u201ca watercolor\n\u201ca pencil sketch\u201d\n\u201ca rendered avatar\u201d\n\u201ca 2D animation\u201d\n\u201ca graffiti\u201d\npainting\u201d\nFigure 15. Life story. Given a text prompt describing a fictional character, \u201ca photo of a man with short black hair\u201d, we can generate a\nconsistent life story for that character, demonstrating the applicability of our method for story generation.\n18\n\u201cholding an\n\u201cin the park\u201d\n\u201creading a book\u201d\n\u201cat the beach\u201d\navocado\u201d\nFigure 16. Non-determinism. By running our method multiple times, given the same prompt \u201ca photo of a 50 years old man with curly\nhair\u201d, but using different initial seeds, we obtain different consistent characters corresponding to the text prompt.\n19\n\u201cholding an\n\u201cin the park\u201d\n\u201creading a book\u201d\n\u201cat the beach\u201d\navocado\u201d\nFigure 17. Non-determinism. By running our method multiple times, given the same prompt \u201ca Plasticine of a cute baby cat with big\neyes\u201d, but using different initial seeds, we obtain different consistent characters corresponding to the text prompt.\n20\nTI multi\nLoRA DB multi\nOurs\n\u201cdrinking a beer\u201d\n\u201cwith a city in\nthe background\u201d\n\u201ca 2D animation of captivating Arctic fox with fluffy fur, bright eyes\nand nimble movements, bringing the magic of the icy wilderness\nto animated life\u201d\n\u201ceating a burger\u201d\n\u201cwearing a\nblue hat\u201d\n\u201ca watercolor portrayal of a joyful child, radiating innocence and\nwonder with rosy cheeks and a genuine, wide-eyed smile\u201d\n\u201cnear the Statue\nof Liberty\u201d\n\u201cas a police\nofficer\u201d\n\u201ca 3D animation of a playful kitten, with bright eyes and\na mischievous expression, embodying youthful curiosity and joy\u201d\nFigure 18. Qualitative comparison to na\u00a8\u0131ve baselines. We tested two additional na\u00a8\u0131ve baselines against our method: TI [20] and LoRA\nDB [71] that were trained on a small dataset of 5 images generated from the same prompt. The baselines are referred to as TI multi (left\ncolumn) and LoRA DB multi (middle column). As can be seen, both of these baselines fail to extract a consistent identity.\n21\n0.17\n0.18\n0.19\n0.2\n0.75\n0.8\n0.85\nTI multi\nLoRA DB multi\nOurs\nAutomatic prompt similarity (\u2192)\nAutomatic identity consistency (\u2192)\nFigure 19. Comparison to na\u00a8\u0131ve baselines. We tested two ad-\nditional na\u00a8\u0131ve baselines against our method: TI [20] and LoRA\nDB [71] that were trained on a small dataset of 5 images gener-\nated from the same prompt. The baselines are referred to as TI\nmulti and LoRA DB multi. Our automatic testing procedure, de-\nscribed in Section 4.1, measures identity consistency and prompt\nsimilarity. As can be seen, both of these baselines fail to achieve\nhigh identity consistency.\n0.16\n0.17\n0.17\n0.18\n0.82\n0.84\n0.86\nOurs w CLIP\nOurs w DINOv1\nOurs w DINOv2\nAutomatic prompt similarity (\u2192)\nAutomatic identity consistency (\u2192)\nFigure 20. Comparison of feature extractors. We tested two ad-\nditional feature extractors in our method: DINOv1 [14] and CLIP\n[61]. Our automatic testing procedure, described in Section 4.1,\nmeasures identity consistency and prompt similarity. As can be\nseen, DINOv1 produces higher identity consistency by sacrificing\nprompt similarity, while CLIP results in higher prompt similarity\nat the expense of lower identity consistency. In practice, however,\nthe DINOv1 results are similar to those obtained with DINOv2\nfeatures in terms of prompt adherence (see Figure 21).\n22\nOurs with CLIP\nOurs with DINOv1\nOurs\n\u201cdrinking a beer\u201d\n\u201cwith a city in\nthe background\u201d\n\u201ca 2D animation of captivating Arctic fox with fluffy fur, bright eyes\nand nimble movements, bringing the magic of the icy wilderness\nto animated life\u201d\n\u201ceating a burger\u201d\n\u201cwearing a\nblue hat\u201d\n\u201ca watercolor portrayal of a joyful child, radiating innocence and\nwonder with rosy cheeks and a genuine, wide-eyed smile\u201d\n\u201cnear the Statue\nof Liberty\u201d\n\u201cas a police\nofficer\u201d\n\u201ca 3D animation of a playful kitten, with bright eyes and a\nmischievous expression, embodying youthful curiosity and joy\u201d\nFigure 21. Comparison of feature extractors. We experimented with two additional feature extractors in our method: DINOv1 [14] and\nCLIP [61]. As can be seen, DINOv1 results are qualitatively similar to DINOv2, whereas CLIP produces results with a slightly lower\nidentity consistency.\n23\nCluster 1\nCluster 2\nCluster 3\nCluster 1\nCluster 2\nCluster 3\nFigure 22. Clustering visualization. We visualize the clustering of images generated with the prompt \u201ca purple astronaut, digital art,\nsmooth, sharp focus, vector art\u201d. In the initial iteration (top three rows), our algorithm divides the generated images into three clusters: (1)\nemphasizing the astronaut\u2019s head, (2) an astronaut without a face, and (3) a full-body astronaut. Cluster 1 (top row) is the most cohesive\ncluster, and it is chosen for the identity extraction phase. In the subsequent iteration (bottom three rows), all images adopt the same\nextracted identity, and the clusters mainly differ from each other in the pose of the character.\n24\nGenerated\ncharacter\nTop 5 nearest neighbors\nFigure 23. Dataset non-memorization. We found the top 5 nearest neighbors in the LAION-5B dataset [73], in terms of CLIP [61] image\nsimilarity, for a few representative characters from our paper, using an open-source solution [68]. As can be seen, our method does not\nsimply memorize images from the LAION-5B dataset.\n25\n\u201cholding an\n\u201cin the park\u201d\n\u201creading a book\u201d\n\u201cat the beach\u201d\navocado\u201d\n\u201ca photo of a 50 years old man with curly hair\u201d\n\u201ca photo of a woman with long ginger hair\u201d\n\u201ca portrait of a man with a mustache and a hat, fauvism\u201d\n\u201ca rendering of a cute albino porcupine, cozy indoor lighting\u201d\nFigure 24. Our method using Stable Diffusion v2.1 backbone. We experimented with a version of our method that uses the Stable\nDiffusion v2.1 [69] model. As can be seen, our method can extract a consistent character, however, as expected, the results are of a lower\nquality than when using the SDXL [57] backbone that we use in the rest of this paper.\n26\nReferences\n[1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel\nCohen-Or. A neural space-time representation for text-to-\nimage personalization. ArXiv, abs/2305.15391, 2023. 3\n[2] Amazon.\nAmazon mechanical turk.\nhttps://www.\nmturk.com/, 2023. 7, 13\n[3] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel\nCohen-Or, Ariel Shamir, and Amit H Bermano. Domain-\nagnostic tuning-encoder for fast personalization of text-to-\nimage models. arXiv preprint arXiv:2307.06925, 2023. 3\n[4] David Arthur and Sergei Vassilvitskii. k-means++: the ad-\nvantages of careful seeding. In ACM-SIAM Symposium on\nDiscrete Algorithms, 2007. 4\n[5] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 18208\u201318218, 2022.\n2, 8, 13\n[6] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski. Break-a-scene: Extracting multiple\nconcepts from a single image. ArXiv, abs/2305.16311, 2023.\n3, 4, 5, 8\n[7] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM Trans. Graph., 42(4), 2023. 2, 8, 13\n[8] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,\nYaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,\nand Xi Yin. Spatext: Spatio-textual representation for con-\ntrollable image generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 18370\u201318380, 2023. 2\n[9] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\naming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and\nMing-Yu Liu. ediff-i: Text-to-image diffusion models with\nan ensemble of expert denoisers.\nArXiv, abs/2211.01324,\n2022. 2\n[10] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In European conference on computer vi-\nsion, pages 707\u2013723. Springer, 2022. 2\n[11] Sagie Benaim, Frederik Warburg, Peter Ebert Christensen,\nand Serge J. Belongie. Volumetric disentanglement for 3d\nscene manipulation. ArXiv, abs/2206.02776, 2022. 2\n[12] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, et al. Improving image generation with better\ncaptions. 2023. 9, 12\n[13] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. MasaCtrl: tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 22560\u201322570,\n2023. 2\n[14] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e Jegou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\n2021 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 9630\u20139640, 2021. 7, 9, 12, 22, 23\n[15] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTransactions on Graphics (TOG), 42:1 \u2013 10, 2023. 2\n[16] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W. Cohen.\nSubject-\ndriven text-to-image generation via apprenticeship learning.\nArXiv, abs/2304.00186, 2023. 3\n[17] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao. Anydoor: Zero-shot object-level im-\nage customization. ArXiv, abs/2307.09481, 2023. 3\n[18] Guillaume Couairon,\nMarlene Careil,\nMatthieu Cord,\nSt\u00b4ephane Lathuili`ere, and Jakob Verbeek.\nZero-shot spa-\ntial layout conditioning for text-to-image diffusion models.\nArXiv, abs/2306.13754, 2023. 2\n[19] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel.\nScenescape: Text-driven consistent scene generation. ArXiv,\nabs/2302.01133, 2023. 2\n[20] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or.\nAn image is worth one word: Personalizing text-to-image\ngeneration using textual inversion. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022. 2, 3,\n5, 6, 9, 10, 11, 13, 21, 22\n[21] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Encoder-based domain\ntuning for fast personalization of text-to-image models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201313, 2023. 3\n[22] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin\nHuang. Expressive text-to-image generation with rich text.\nArXiv, abs/2304.06720, 2023. 2\n[23] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 2\n[24] Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia,\nHaoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang,\nYing Shan, and Yujiu Yang. TaleCrafter: interactive story vi-\nsualization with multiple characters. ArXiv, abs/2305.18247,\n2023. 2, 3\n[25] Ori Gordon, Omri Avrahami, and Dani Lischinski. Blended-\nnerf: Zero-shot object generation and blending in existing\nneural radiance fields. ArXiv, abs/2306.12760, 2023. 2\n[26] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris N. Metaxas, and Feng Yang.\nSvdiff:\nCom-\npact parameter space for diffusion fine-tuning.\nArXiv,\nabs/2303.11305, 2023. 3\n[27] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 2\n[28] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta de-\nnoising score. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 2328\u20132337, 2023. 2\n[29] Geoffrey E. Hinton and Sam T. Roweis. Stochastic neighbor\nembedding. In NIPS, 2002. 4\n27\n[30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Proc. NeurIPS, 2020. 2\n[31] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin John-\nson, and Matthias Nie\u00dfner.\nText2room: Extracting tex-\ntured 3d meshes from 2d text-to-image models.\nArXiv,\nabs/2303.11989, 2023. 2\n[32] Eliahu Horwitz and Yedid Hoshen. Conffusion: Confidence\nintervals for diffusion models. ArXiv, abs/2211.09795, 2022.\n3\n[33] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\nShean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-\nrank adaptation of large language models. In International\nConference on Learning Representations, 2021. 3, 5\n[34] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nCLIP, 2021. 5\n[35] Shira Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel\nCohen-Or, and Ariel Shamir. Word-as-image for semantic\ntypography. ACM Transactions on Graphics (TOG), 42:1 \u2013\n11, 2023. 3\n[36] Hyeonho Jeong, Gihyun Kwon, and Jong-Chul Ye. Zero-\nshot generation of coherent storybook from plain text story\nusing diffusion models. ArXiv, abs/2302.03900, 2023. 2, 3\n[37] Xuhui Jia, Yang Zhao, Kelvin C. K. Chan, Yandong Li,\nHan-Ying Zhang, Boqing Gong, Tingbo Hou, H. Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models. ArXiv,\nabs/2304.02642, 2023. 3\n[38] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 2\n[39] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. CoRR, abs/1412.6980, 2014. 12\n[40] William H. Kruskal and Wilson Allen Wallis. Use of ranks\nin one-criterion variance analysis. Journal of the American\nStatistical Association, 47:583\u2013621, 1952. 13\n[41] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 3\n[42] Dongxu Li, Junnan Li, and Steven C. H. Hoi.\nBLIP-\nDiffusion:\nPre-trained subject representation for con-\ntrollable text-to-image generation and editing.\nArXiv,\nabs/2305.14720, 2023. 3, 5, 6, 10, 11, 12, 13\n[43] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng,\nYuexin Wu, Lawrence Carin, David Carlson, and Jianfeng\nGao. Storygan: A sequential conditional gan for story visu-\nalization. CVPR, 2019. 3\n[44] Shaoteng Liu, Yuecheng Zhang, Wenbo Li, Zhe Lin, and Ji-\naya Jia. Video-p2p: Video editing with cross-attention con-\ntrol. ArXiv, abs/2303.04761, 2023. 2\n[45] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 2\n[46] Adyasha Maharana, Darryl Hannan, and Mohit Bansal.\nStorydall-e: Adapting pretrained text-to-image transformers\nfor story continuation. In European Conference on Computer\nVision, pages 70\u201387. Springer, 2022. 3\n[47] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions, 2021. 2\n[48] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 2\n[49] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 2\n[50] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha,\nY. Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen.\nDreamix: Video diffusion models are general video editors.\nArXiv, abs/2302.01329, 2023. 2\n[51] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\nZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-\nadapter:\nLearning adapters to dig out more controllable\nability for text-to-image diffusion models.\narXiv preprint\narXiv:2302.08453, 2023. 2\n[52] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. In Interna-\ntional Conference on Machine Learning, 2021. 2\n[53] OpenAI. ChatGPT. https://chat.openai.com/,\n2022. Accessed: 2023-10-15. 5, 9, 12\n[54] Maxime Oquab,\nTimoth\u00b4ee Darcet,\nTh\u00b4eo Moutakanni,\nHuy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernan-\ndez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\nMahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ\nHowes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra,\nMichael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao\nXu, Herv\u00b4e J\u00b4egou, Julien Mairal, Patrick Labatut, Armand\nJoulin, and Piotr Bojanowski. DINOv2: Learning robust vi-\nsual features without supervision. ArXiv, abs/2304.07193,\n2023. 4, 9, 12\n[55] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-\nElor, and Daniel Cohen-Or. Localizing object-level shape\nvariations with text-to-image diffusion models.\nArXiv,\nabs/2303.11306, 2023. 2\n[56] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman,\nJonathan T. Barron, Amit H. Bermano, Eric Ryan Chan, Tali\nDekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen\nLiu, Lingjie Liu, Ben Mildenhall, Matthias Nie\u00dfner, Bjorn\nOmmer, Christian Theobalt, Peter Wonka, and Gordon Wet-\n28\nzstein. State of the art on diffusion models for visual com-\nputing. ArXiv, abs/2310.07204, 2023. 2\n[57] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann,\nTim Dockhorn, Jonas Muller, Joe Penna, and Robin Rom-\nbach. SDXL: Improving latent diffusion models for high-\nresolution image synthesis. ArXiv, abs/2307.01952, 2023. 2,\n5, 9, 12, 26\n[58] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2\n[59] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 2\n[60] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar,\nAmit H. Bermano, and Daniel Cohen-Or. Single motion dif-\nfusion. ArXiv, abs/2302.05905, 2023. 3\n[61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In International\nConference on Machine Learning, 2021. 5, 6, 7, 9, 12, 22,\n23, 25\n[62] Tanzila Rahman, Hsin-Ying Lee, Jian Ren, S. Tulyakov,\nShweta Mahajan, and Leonid Sigal.\nMake-a-story: Vi-\nsual memory conditioned consistent story generation. 2023\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2493\u20132502, 2022. 2, 3\n[63] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with CLIP latents. arXiv preprint arXiv:2204.06125,\n2022. 2, 3\n[64] reddit.com. How to create consistent character faces without\ntraining (info in the comments) : Stablediffusion. https:\n/ / www . reddit . com / r / StableDiffusion /\ncomments / 12djxvz / how _ to _ create _\nconsistent _ character _ faces _ without/,\n2023. 2, 3\n[65] reddit.com.\n8\nways\nto\ngenerate\nconsistent\nchar-\nacters\n(for\ncomics,\nstoryboards,\nbooks\netc)\n:\nSta-\nblediffusion.\nhttps : / / www . reddit . com / r /\nStableDiffusion/comments/10yxz3m/8_ways_\nto_generate_consistent_characters_for/,\n2023. 2, 3\n[66] Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel\nCohen-Or. Conceptlab: Creative generation using diffusion\nprior constraints. arXiv preprint arXiv:2308.02669, 2023. 3\n[67] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or.\nTexture: Text-guided texturing of\n3d shapes. ACM SIGGRAPH 2023 Conference Proceedings,\n2023. 3\n[68] Romain Beaumont.\nClip retrival.\nhttps://github.\ncom/rom1504/clip-retrieval, 2023. 9, 25\n[69] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj\u00a8orn Ommer. High-resolution image synthesis\nwith latent diffusion models. 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n10674\u201310685, 2021. 2, 11, 26\n[70] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 3, 5\n[71] Simo Ryu.\nLow-rank adaptation for fast text-to-image\ndiffusion\nfine-tuning.\nhttps : / / github . com /\ncloneofsimo/lora, 2022. 3, 5, 6, 9, 10, 11, 13, 21,\n22\n[72] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[73] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. ArXiv, abs/2210.08402,\n2022. 9, 25\n[74] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar\nAverbuch-Elor. Vox-e: Text-guided voxel editing of 3d ob-\njects. ArXiv, abs/2303.12048, 2023. 2\n[75] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer,\nOran Gafni, Eliya Nachmani, and Yaniv Taigman.\nknn-\ndiffusion: Image generation via large-scale retrieval. In The\nEleventh International Conference on Learning Representa-\ntions, 2022. 2\n[76] Jing Shi, Wei Xiong, Zhe L. Lin, and Hyun Joon Jung. In-\nstantbooth: Personalized text-to-image generation without\ntest-time finetuning. ArXiv, abs/2304.03411, 2023. 3\n[77] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n2\n[78] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2020.\n[79] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in Neural\nInformation Processing Systems, 32, 2019. 2\n[80] G\u00b4abor Sz\u02dducs and Modafar Al-Shouha.\nModular storygan\nwith background and theme awareness for story visualiza-\ntion.\nIn International Conference on Pattern Recognition\nand Artificial Intelligence, pages 275\u2013286. Springer, 2022.\n3\n[81] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H. Bermano. Human motion\ndiffusion model. ArXiv, abs/2209.14916, 2022. 3\n29\n[82] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.\nKey-locked rank one editing for text-to-image personaliza-\ntion. ACM SIGGRAPH 2023 Conference Proceedings, 2023.\n3\n[83] John W. Tukey. Comparing individual means in the analysis\nof variance. Biometrics, 5 2:99\u2013114, 1949. 13\n[84] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 2\n[85] Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel\nShamir. Concept decomposition for visual exploration and\ninspiration. ArXiv, abs/2305.18203, 2023. 3\n[86] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion\nmodels.\nhttps://github.com/huggingface/\ndiffusers, 2022. 12\n[87] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or.\nSketch-guided text-to-image diffusion models.\narXiv\npreprint arXiv:2211.13752, 2022. 2\n[88] Andrey Voynov, Q. Chu, Daniel Cohen-Or, and Kfir Aber-\nman.\nP+: Extended textual conditioning in text-to-image\ngeneration. ArXiv, abs/2303.09522, 2023. 3\n[89] Yuxiang Wei. Official implementation of ELITE. https:\n//github.com/csyxwei/ELITE, 2023.\nAccessed:\n2023-05-01. 5\n[90] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. ELITE: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. ArXiv, abs/2302.13848, 2023. 3, 5, 6, 10, 11, 12,\n13\n[91] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R\u00b4emi Louf, Morgan Funtowicz, Joe Davison, Sam\nShleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien\nPlu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush.\nTrans-\nformers: State-of-the-art natural language processing.\nIn\nProceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations,\npages 38\u201345, Online, 2020. Association for Computational\nLinguistics. 12\n[92] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. ArXiv, abs/2306.07954, 2023. 2\n[93] Hu Ye, Jun Zhang, Siyi Liu, Xiao Han, and Wei Yang. IP-\nAdapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. ArXiv, abs/2308.06721, 2023. 3, 5,\n6, 10, 11, 12, 13\n[94] youtube.com. How to create consistent characters in mid-\njourney.\nhttps://www.youtube.com/watch?v=\nZ7_ta3RHijQ, 2023. 3\n[95] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022. 2\n[96] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang,\nand In-So Kweon. Text-to-image diffusion models in gen-\nerative ai: A survey. ArXiv, abs/2303.07909, 2023. 2\n[97] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 3836\u20133847, 2023. 2, 8, 13\n[98] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields. ArXiv, abs/2306.13455, 2023. 2\n30\n"
  },
  {
    "title": "UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs",
    "link": "https://arxiv.org/pdf/2311.09257.pdf",
    "upvote": "43",
    "text": "UFOGen: You Forward Once Large Scale Text-to-Image Generation via\nDiffusion GANs\nYanwu Xu*[1,2]\u2020, Yang Zhao[1]\u2020, Zhisheng Xiao[1]\u2020, Tingbo Hou[1]\n1 Google\n{yanwuxu,yzhaoeric,zsxiao,tingbo}@google.com\n2 Department of Electrical Computer Engineering, Boston University\nyanwuxu@bu.edu\nAbstract\nText-to-image diffusion models have demonstrated re-\nmarkable capabilities in transforming text prompts into co-\nherent images, yet the computational cost of the multi-step\ninference remains a persistent challenge. To address this\nissue, we present UFOGen, a novel generative model de-\nsigned for ultra-fast, one-step text-to-image generation. In\ncontrast to conventional approaches that focus on improv-\ning samplers or employing distillation techniques for diffu-\nsion models, UFOGen adopts a hybrid methodology, inte-\ngrating diffusion models with a GAN objective. Leveraging\na newly introduced diffusion-GAN objective and initializa-\ntion with pre-trained diffusion models, UFOGen excels in\nefficiently generating high-quality images conditioned on\ntextual descriptions in a single step.\nBeyond traditional\ntext-to-image generation, UFOGen showcases versatility in\napplications. Notably, UFOGen stands among the pioneer-\ning models enabling one-step text-to-image generation and\ndiverse downstream tasks, presenting a significant advance-\nment in the landscape of efficient generative models.\n1. Introduction\nDiffusion models [16, 54, 56] has recently emerged as a\npowerful class of generative models, demonstrating un-\nprecedented results in many generative modeling tasks [6,\n18, 27, 47, 49, 61]. In particular, they have shown the re-\nmarkable ability to synthesize high-quality images condi-\ntioned on texts [1, 41, 45, 47, 49, 64]. Beyond the text-\nto-image synthesis tasks, large-scale text-to-image models\nserve as foundational building blocks for various down-\nstream applications, including personalized generation [8,\n11, 28, 48], controlled generation [40, 68] and image edit-\n*Work done as a student researcher of Google, \u2020 indicates equal con-\ntribution.\ning [5, 13, 65].\nYet, despite their impressive generative\nquality and wide-ranging utility, diffusion models have a\nnotable limitation: they rely on iterative denoising to gen-\nerate final samples, which leads to slow generation speeds.\nThe slow inference and the consequential computational de-\nmands of large-scale diffusion models pose significant im-\npediments to their deployment.\nIn the seminal work by Song et al. [56], it was revealed\nthat sampling from a diffusion model is equivalent to solv-\ning the probability flow ordinary differential equation (PF-\nODE) associated with the diffusion process. Presently, the\nmajority of research aimed at enhancing the sampling effi-\nciency of diffusion models centers on the ODE formulation.\nOne line of work seeks to advance numerical solvers for the\nPF-ODE, with the intention of enabling the solution of the\nODE with greater discretization size, ultimately leading to\nfewer requisite sampling steps [2, 35, 36, 55]. However, the\ninherent trade-off between step size and accuracy still ex-\nists. Given the highly non-linear and complicated trajectory\nof the PF-ODE, it would be extremely difficult to reduce\nthe number of required sampling steps to a minimal level.\nEven the most advanced solvers [35, 36] can generate im-\nages within 10 to 20 sampling steps, and further reduction\nleads to a noticeable drop in image quality. An alternative\napproach seeks to distill the PF-ODE trajectory from a pre-\ntrained diffusion model. For instance, progressive distilla-\ntion [29, 39, 50] tries to condense multiple discretization\nsteps of the PF-ODE solver into a single step by explicitly\naligning with the solver\u2019s output. Similarly, consistency dis-\ntillation [37, 57] works on learning consistency mappings\nthat preserve point consistency along the ODE trajectory.\nThese methods have demonstrated the potential to signifi-\ncantly reduce the number of sampling steps. However, due\nto the intrinsic complexity of the ODE trajectory, they still\nstruggle in the extremely small step regime, especially for\nlarge-scale text-to-image diffusion models.\nThe pursuit of developing ultra-fast large-scale diffusion\narXiv:2311.09257v5  [cs.CV]  7 Dec 2023\nFigure 1. Images generated by our UFOGen Model with 1 sampling step. The model is trained by fine-tuning Stable Diffusion 1.5 with\nour introduced techniques.\nmodels that requires just one or two sampling steps, remains\na challenging open problem. We assert that to achieve this\nambitious objective, fundamental adjustments are necessary\nin the formulation of diffusion models, as the current ODE-\nbased approach seems intrinsically constrained for very few\nsteps sampling, as elucidated earlier. In this work, we intro-\nduce a novel one-step text-to-image generative model, rep-\nresenting a fusion of GAN and diffusion model elements.\nOur inspiration stems from previous work that successfully\nincorporated GANs into the framework of diffusion mod-\nels [59, 60, 63, 69], which have demonstrated the capacity\nto generate images in as few as four steps when trained on\nsmall-scale datasets. These models diverge from the tradi-\ntional ODE formulation by leveraging adversarial loss for\nlearning the denoising distribution, rather than relying on\nKL minimization. Section 3 offers a comprehensive review\nof existing diffusion-GAN hybrid models.\nDespite the promising outcomes of earlier diffusion\nGAN hybrid models, achieving one-step sampling and ex-\ntending their utility to text-to-image generation remains a\nnon-trivial challenge. In this research, we introduce inno-\nvative techniques to enhance diffusion GAN models, re-\nsulting in an ultra-fast text-to-image model capable of pro-\nducing high-quality images in a single sampling step. In\nlight of this achievement, we have named our model UFO-\nGen, an acronym denoting \u201cYou Forward Once\u201d Generative\nmodel. A detailed exposition of UFOGen is presented in\nSection 4. Our UFOGen model excels at generating high-\nquality images in just one inference step. Notably, when\ninitialized with a pre-trained Stable Diffusion model [47],\nour method efficiently transforms Stable Diffusion into a\none-step inference model while largely preserving the qual-\nity of generated content. See Figure 1 for a showcase of\ntext-conditioned images generated by UFOGen. To the best\nof our knowledge, our model stands among the pioneers\nto achieve a reduction in the number of required sampling\nsteps for text-to-image diffusion models to just one.\nOur work presents several significant contributions:\n1. We introduce UFOGen, a powerful generative model ca-\npable of producing high-quality images conditioned on\ntext descriptions in a single inference step.\n2. We present an efficient and simplified training process,\nenabling the fine-tuning of pre-existing large-scale diffu-\nsion models, like Stable Diffusion, to operate as one-step\ngenerative models.\n3. Our model\u2019s versatility extends to applications such as\nimage-to-image and controllable generation, thereby un-\nlocking the potential for one-step inference across vari-\nous generative scenarios.\n2. Related Works\nText-to-image Diffusion Models Denoising diffusion\nmodels [16, 54, 56] are trained to reconstruct data from\ncorrupted inputs.\nThe simplicity of the training objec-\ntive makes denoising diffusion models well-suited for scal-\ning up generative models.\nResearchers have made nu-\nmerous efforts to train diffusion models on large datasets\ncontaining image-text pairs [53] for the text-to-image gen-\neration task [1, 41, 45, 47, 49, 64].\nAmong these, la-\ntent diffusion models, such as the popular Stable Diffu-\nsion model [42, 47], have gained substantial attention in the\nresearch community due to their simplicity and efficiency\ncompared to pixel-space counterparts.\nAccelerating Diffusion Models The notable issue of slow\ngeneration speed has motivated considerable efforts to-\nwards enhancing the sampling efficiency of diffusion mod-\nels. These endeavors can be categorized into two primary\napproaches. The first focuses on the development of im-\nproved numerical solvers\n[2, 24, 35, 36, 55].\nThe sec-\nond approach explores the concept of knowledge distilla-\ntion [15], aiming at condensing the sampling trajectory of\na numerical solver into fewer steps [3, 29, 37, 39, 50, 57].\nHowever, both of these approaches come with significant\nlimitations, and thus far, they have not demonstrated the\nability to substantially reduce the sampling steps required\nfor text-to-image diffusion models to a truly minimal level.\nText-to-image GANs As our model has GAN [12] as one\nof its component, we provide a brief overview of previ-\nous attempts of training GANs for text-to-image genera-\ntion.\nEarly GAN-based text-to-image models were pri-\nmarily confined to small-scale datasets [46, 58, 62, 67].\nLater, with the evolution of more sophisticated GAN ar-\nchitectures [22, 23, 51], GANs trained on large datasets\nhave shown promising results in the domain of text-to-\nimage generation [20, 52, 70]. Comparatively, our model\nhas several distinct advantages. Firstly, to overcome the\nwell-known issues of training instability and mode collapse,\ntext-to-image GANs have to incorporate multiple auxiliary\nlosses and complex regularization techniques, which makes\ntraining and parameter tuning extremely intricate.\nThis\ncomplexity is particularly exemplified by GigaGAN [20],\ncurrently regarded as the most powerful GAN-based mod-\nels. In contrast, our model offers a streamlined and robust\ntraining process, thanks to the diffusion component. Sec-\nondly, our model\u2019s design allows us to seamlessly harness\npre-trained diffusion models for initialization, significantly\nenhancing the efficiency of the training process. Lastly, our\nmodel exhibits greater flexibility when it comes to down-\nstream applications (see Section 5.3), an area in which\nGAN-based models have not explored.\nRecent Progress on Few-step Text-to-image Generation\nWhile developing our model, we noticed some concurrent\nwork on few-step text-to-image generation. Latent Consis-\ntency Model [37] extends the idea of consistency distilla-\ntion [57] to Stable Diffusion, leading to 4-step sampling\nwith reasonable quality. However, further reducing the sam-\npling step results in significant quality drop. InstaFlow [33]\nachieves text-to-image generation in a single sampling step.\nSimilar to our model, InstaFlow tackles the slow sampling\nissue of diffusion models by introducing improvements to\nthe model itself. Notably, they extend Rectified Flow mod-\nels [31, 32] to create a more direct trajectory in the diffusion\nprocess. In direct comparison to InstaFlow, our model out-\nperforms in terms of both quantitative metrics and visual\nquality. Moreover, our approach presents the added benefits\nof a streamlined training pipeline and improved training ef-\nficiency. InstaFlow requires multiple stages of fine-tuning,\nfollowed by a subsequent distillation stage. In contrast, our\nmodel only need one single fine-tuning stage with a mini-\nmal number of training iterations.\n3. Background\nDiffusion Models Diffusion models [16, 54] is a fam-\nily of generative models that progressively inject Gaus-\nsian noises into the data, and then generate samples\nfrom noise via a reverse denoising process.\nDiffu-\nsion models define a forward process that corrupts data\nx0\n\u223c\nq(x0) in T steps with variance schedule \u03b2t:\nq(xt|xt\u22121) := N(xt; \u221a1 \u2212 \u03b2txt\u22121, \u03b2tI).\nThe parame-\nterized reversed diffusion process aims to gradually re-\ncover cleaner data from noisy observations: p\u03b8(xt\u22121|xt) :=\nN(xt\u22121; \u00b5\u03b8(xt, t), \u03c32\nt I).\nThe model p\u03b8(xt\u22121|xt) is parameterized as a Gaussian\ndistribution, because when the denoising step size from t\nto t \u2212 1 is sufficiently small, the true denoising distribution\nq(xt\u22121|xt) is a Gaussian [9]. To train the model, one can\nminimize the negative ELBO objective [16, 25]:\nL = Et,q(x0)q(xt|x0)KL(q(xt\u22121|xt, x0)||p\u03b8(xt\u22121|xt)),\n(1)\nwhere q(xt\u22121|xt, x0) is Gaussian posterior distribution de-\nrived in [16].\nDiffusion-GAN Hybrids The idea of combining diffusion\nmodels and GANs is first explored in [60].\nThe main\nmotivation is that, when the denoising step size is large,\nthe true denoising distribution q(xt\u22121|xt) is no longer a\nGaussian. Therefore, instead of minimizing KL divergence\nwith a parameterized Gaussian distribution, they param-\neterized p\u03b8(x\u2032\nt\u22121|xt) as a conditional GAN to minimize\nthe adversarial divergence between model p\u03b8(x\u2032\nt\u22121|xt) and\nq(xt\u22121|xt):\nmin\n\u03b8\nEq(xt)\nh\nDadv(q(xt\u22121|xt)||p\u03b8(x\u2032\nt\u22121|xt))\ni\n.\n(2)\nThe objective of Denoising Diffusion GAN (DDGAN) in\n[60] can be expressed as:\nmin\n\u03b8\nmax\nD\u03d5 Eq(xt)\nh\nEq(xt\u22121|xt)[\u2212 log(D\u03d5(xt\u22121, xt, t))]\n+ Ep\u03b8(x\u2032\nt\u22121|xt)[\u2212 log(1 \u2212 D\u03d5(x\u2032\nt\u22121, xt, t))]\ni\n,\n(3)\nwhere D\u03d5\nis the conditional discriminator network,\nand\nthe\nexpectation\nover\nthe\nunknown\ndistribution\nq(xt\u22121|xt)\ncan\nbe\napproximated\nby\nsampling\nfrom\nq(x0)q(xt\u22121|x0)q(xt|xt\u22121).\nThe flexibility of a GAN-\nbased denoising distribution surpasses that of a Gaussian\nparameterization, enabling more aggressive denoising step\nsizes. Consequently, DDGAN successfully achieves a re-\nduction in the required sampling steps to just four.\nNonetheless, the utilization of a purely adversarial ob-\njective in DDGAN introduces training instability, as docu-\nmented by the findings in [63]. In response to this chal-\nlenge, the authors in [63] advocated matching the joint dis-\ntribution q(xt\u22121, xt) and p\u03b8(xt\u22121, xt), as opposed to the\nconditional distribution as outlined in Equation 2. [63] fur-\nther demonstrated that the joint distribution matching can be\ndisassembled into two components: matching marginal dis-\ntributions using adversarial divergence and matching condi-\ntional distributions using KL divergence:\nmin\n\u03b8 Eq(xt)\nh\nDadv(q(xt\u22121)||p\u03b8(xt\u22121))\n+ \u03bbKLKL(p\u03b8(xt|xt\u22121)||q(xt|xt\u22121))\ni\n.\n(4)\nThe objective of adversarial divergence minimization is\nsimilar to Equation 3 except that the discriminator does not\ntake xt as part of its input. The KL divergence minimiza-\ntion translates into a straightforward reconstruction objec-\ntive, facilitated by the Gaussian nature of the diffusion pro-\ncess (see Appendix A.1 for a derivation). This introduction\nof a reconstruction objective plays a pivotal role in enhanc-\ning the stability of the training dynamics. As observed in\n[63], which introduced Semi-Implicit Denoising Diffusion\nModels (SIDDMs), this approach led to markedly improved\nresults, especially on more intricate datasets.\n4. Methods\nIn this section, we present a comprehensive overview of\nthe enhancements we have made in our diffusion-GAN hy-\nbrid models, ultimately giving rise to the UFOGen model.\nThese improvements are primarily focused on two critical\ndomains: 1) enabling one step sampling, as detailed in Sec-\ntion 4.1, and 2) scaling-up for text-to-image generation, as\ndiscussed in Section 4.2.\n4.1. Enabling One-step Sampling for UFOGen\nDiffusion-GAN hybrid models are tailored for training with\na large denoising step size. However, attempting to train\nthese models with just a single denoising step (i.e., xT \u22121 =\nx0) effectively reduces the training to that of a conventional\nGAN. Consequently, prior diffusion-GAN models were un-\nable to achieve one-step sampling. In light of this chal-\nlenge, we conducted an in-depth examination of the SIDDM\n[63] formulation and implemented specific modifications in\nthe generator parameterization and the reconstruction term\nwithin the objective. These adaptations enabled UFOGen\nto perform one-step sampling, while retaining training with\nseveral denoising steps.\nParameterization of the Generator In diffusion-GAN\nmodels, the generator should produce a sample of xt\u22121.\nHowever, instead of directly outputting xt\u22121, the generator\nof DDGAN and SIDDM is parameterized by p\u03b8(xt\u22121|xt) =\nq(xt\u22121|xt, x0 = G\u03b8(xt, t)). In other words, first x0 is pre-\ndicted using the denoising generator G\u03b8(xt, t), and then,\nxt\u22121 is sampled using the Gaussian posterior distribution\nq(xt\u22121|xt, x0) derived in [16, 60]. Note that this parame-\nterization is mainly for practical purposes, as discussed in\n[60], and alternative parameterization would not break the\nmodel formulation.\nWe propose another plausible parameterization for the\ngenerator: p\u03b8(xt\u22121) = q(xt\u22121|x0 = G\u03b8(xt, t)). The gen-\nerator still predicts x0, but we sample xt\u22121 from the forward\ndiffusion process q(xt\u22121|x0) instead of the posterior. As we\nwill show later, this design allows distribution matching at\nx0, paving the path to one-step sampling.\nImproved Reconstruction Loss at x0\nWe argue that\nwith the new generator parameterization, the objective of\nSIDDM in Equation 4 indirectly matches the distribution at\nx0. To see this, we analyze the adversarial objective and KL\nobjective in Equation 4 separately. The first term minimizes\nadversarial divergence Dadv\n\u0000q(xt\u22121)||p\u03b8(x\u2032\nt\u22121)\n\u0001\n, where\nq(xt\u22121) and p\u03b8(x\u2032\nt\u22121) can both be seen as the corruption\nof a distribution at x0 by the same Gaussian kernel. Specif-\nically, since q(xt\u22121) = Eq(x0)[q(xt\u22121|x0)], given a sample\nx0 \u223c q(x0), we have q(xt) = N(xt\u22121; \u221a\u00af\u03b1t\u22121x0, (1 \u2212\n\u00af\u03b1t\u22121)I), according to the forward diffusion formulation\n[16]. Similarly, p\u03b8(x\u2032\nt\u22121) has the same form except that x0\nis produced by the generator. As a result, adversarial distri-\nbution matching on q(xt\u22121) and p\u03b8(x\u2032\nt\u22121) will also encour-\nage the matching between q(x0) and p\u03b8(x\u2032\n0), which is the\ndistribution over x0 produced by the generator. A formal\nexplanation will be presented in Appendix A.2.1.\nThe second term in the objective minimizes the KL di-\nvergence between p\u03b8(xt|x\u2032\nt\u22121) and q(xt|xt\u22121), which, as\nderived in Appendix A.1, can be simplified to the following\nreconstruction term:\nEq(xt)\nh(1 \u2212 \u03b2t)||x\u2032\nt\u22121 \u2212 xt\u22121||2\n2\u03b2t\ni\n.\n(5)\nBased on above analysis on x\u2032\nt\u22121 and xt\u22121, it is easy to\nsee that minimizing this reconstruction loss will essentially\nUFOGEN (4 steps inference)\nSIDDMs (4 steps inference)\nUFOGEN (1 step inference)\nSIDDMs (1 step inference)\nFigure 2. Results of training with UFOGen loss versus the orig-\ninal loss of SIDDM on 25-Gaussian toy data. With the modified\nobjective, UFO enables one-step sampling.\nmatches x0 and x\u2032\n0 as well (a straightforward derivation is\nprovided in Appendix A.2.2).\nPer our analysis, both terms in the SIDDM objective in\nEquation 4 implicitly matches the distribution at x0, which\nsuggests that one-step sampling is possible. However, em-\npirically we observe that one-step sampling from SIDDM\ndoes not work well even on 2-D toy dataset (See Figure 2).\nWe conjecture that this is due to the variance introduced\nin the additive Gaussian noise when sampling xt\u22121 with\nx0. To reduce the variance, we propose to replace the re-\nconstruction term in Equation 5 with the reconstruction at\nclean sample ||x0 \u2212 x\u2032\n0||2, so that the matching at x0 be-\ncomes explicit. We observe that with this change, we can\nobtain samples in one step, as shown in Figure 2.\nTraining and Sampling of UFOGen To put things to-\ngether, we present the complete training objective and strat-\negy for the UFOGen model. UFOGen is trained with the\nfollowing objective:\nmin\n\u03b8\nmax\nD\u03d5 Eq(x0)q(xt\u22121|x0),p\u03b8(x\u2032\n0)p\u03b8(x\u2032\nt\u22121|x\u2032\n0)\nh\n[log(D\u03d5(xt\u22121, t))] + [log(1 \u2212 D\u03d5(x\u2032\nt\u22121, t))]\n+ \u03bbKL\u03b3t \u2225x0 \u2212 x\u2032\n0\u22252 i\n,\n(6)\nwhere \u03b3t is a time-dependent coefficient. The objective con-\nsists of an adversarial loss to match noisy samples at time\nstep t \u2212 1, and a reconstruction loss at time step 0. Note\nthat the reconstruction term is essentially the training ob-\njective of diffusion models [16, 56], and therefore the train-\ning of UFOGen model can also be interpreted as training a\ndiffusion model with adversarial refinement. The training\nscheme of UFOGen is presented in Algorithm 1.\nDespite the straightforward nature of the modifications\nto the training objective, these enhancements have yielded\nimpressive outcomes, particularly evident in the context of\none-step sampling, where we simply sample xT \u223c N(0, I)\nand produce sample x\u2032\n0 = G\u03b8(xT ).\n4.2. Leverage Pre-trained Diffusion Models\nOur objective is developing an ultra-fast text-to-image\nmodel.\nHowever, the transition from an effective UFO-\nGen recipe to web-scale data presents considerable chal-\nT\nF\nT\nT\nF\nT\nT\nF\nF\nT\nF\nT\nT\nF\nF\nT\nF\nF\nT\nT\nF\nF\nT\nT\nF\nDenoiser\nGenerator\nDiscriminator\nReconstruction \nLoss\nReconstruction \nLoss\nForward Diffusion\nForward Diffusion\nInitialize\nInitialize\nDiffusion \nPretraining\nUFOGen\nFinetuning\nAdversarial Loss\nFigure 3. Illustration of the training strategy for UFOGen model.\nlenges. Training diffusion-GAN hybrid models for text-to-\nimage generation encounters several intricacies. Notably,\nthe discriminator must make judgments based on both tex-\nture and semantics, which govern text-image alignment.\nThis challenge is particularly pronounced during the ini-\ntial stage of training. Moreover, the cost of training text-\nto-image models can be extremely high, particularly in the\ncase of GAN-based models, where the discriminator intro-\nduces additional parameters.\nPurely GAN-based text-to-\nimage models [20, 52] confront similar complexities, result-\ning in highly intricate and expensive training.\nTo surmount the challenges of scaling-up diffusion-GAN\nhybrid models, we propose the utilization of pre-trained\ntext-to-image diffusion models, notably the Stable Diffu-\nsion model [47]. Specifically, our UFOGen model is de-\nsigned to employ a consistent UNet structure for both its\ngenerator and discriminator. This design enables seamless\ninitialization with the pre-trained Stable Diffusion model.\nWe posit that the internal features within the Stable Dif-\nfusion model contain rich information of the intricate in-\nterplay between textual and visual data. This initialization\nstrategy significantly streamlines the training of UFOGen.\nUpon initializing UFOGen\u2019s generator and discriminator\nwith the Stable Diffusion model, we observe stable training\ndynamics and remarkably fast convergence. The complete\ntraining strategy of UFOGen is illustrated in Figure 3.\n5. Experiments\nIn this section, we evaluate our proposed UFOGen model\nfor the text-to-image synthesis problem. In Section 5.1, we\nstart with briefly introducing our experimental setup, fol-\nlowed by comprehensive evaluations of UFOGen model on\nthe text-to-image task, both quantitatively and qualitatively.\nWe conduct ablation studies in Section 5.2, highlighting\nthe effectiveness of our modifications introduced in Section\n4. In Section 5.3, we present qualitative results for down-\nstream applications of UFOGen.\nAlgorithm 1 UFOGen Training\nRequire: Generator G\u03b8, discriminator D\u03d5, loss coefficient \u03bbKL\n1: repeat\n2:\nSample x0 \u223c q(x0), t \u2212 1 \u223c Uniform(0, ..., T \u2212 1).\n3:\nSample xt\u22121 \u223c q(xt\u22121|x0), xt \u223c q(xt|xt\u22121)\n4:\nSample x\u2032\nt\u22121 \u223c q(xt\u22121|x\u2032\n0), where x\u2032\n0 = G\u03b8(xt, t)\n5:\nUpdate D\u03d5 with gradient\n\u2207\u03d5 (\u2212 log (D\u03d5(xt\u22121, t \u2212 1)) \u2212 log (1 \u2212 D\u03d5(x\u2032\nt\u22121, t \u2212 1)))\n6:\nUpdate G\u03b8 with gradient\n\u2207\u03b8\n\u0000\u2212 log(D\u03d5(x\u2032\nt\u22121, t \u2212 1) + \u03bbKL\u03b3t||x0 \u2212 x\u2032\n0||2\n2\n\u0001\n7: until converged\n5.1. Text-to-image Generation\nConfiguration for Training and Evaluation For experi-\nments on text-to-image generation, we follow the scheme\nproposed in Section 4.2 to initialize both the generator\nand discriminator with the pre-trained Stable Diffusion 1.51\nmodel [47]. We train our model on the LAION-Aesthetics-\n6+ subset of LAION-5B [53]. More training details are pro-\nvided in Appendix A.3. For evaluation, we adopt the com-\nmon practice that uses zero-shot FID [14] on MS-COCO\n[30], and CLIP score with ViT-g/14 backbone [43].\nMain Results To kick-start our evaluation, we perform a\ncomparative analysis in Table 1, bench-marking UFOGen\nagainst other few-step sampling models that share the same\nStable Diffusion backbone. Our baselines include Progres-\nsive Distillation [39] and its variant [29], which are previ-\nously the state-of-the-art for few-step sampling of SD, as\nwell as the concurrent work of InstaFlow [33]. Latent Con-\nsistency Model (LCM) [37] is excluded, as the metric is\nnot provided in their paper. Analysis of the results pre-\nsented in Table 1 reveals the superior performance of our\nsingle-step UFOGen when compared to Progressive Distil-\nlation across one, two, or four sampling steps, as well as the\nCFG-Aware distillation [29] in eight steps. Furthermore,\nour method demonstrates advantages in terms of both FID\nand CLIP scores over the single-step competitor, InstaFlow-\n0.9B, which share the same network structure of SD with\nus. Impressively, our approach remains highly competitive\neven when compared to InstaFlow-1.7B with stacked UNet\nstructures, which effectively doubles the parameter count.\nThe results depicted in Table 1 may suggest that In-\nstaFlow remains a strong contender in one-step generation\nalongside UFOGen. However, we argue that relying solely\non the MS-COCO zero-shot FID score for evaluating vi-\nsual quality might not be the most reliable metric, a con-\ncern highlighted in prior research such as [26, 42] and dis-\ncussed by [4]. Consequently, we believe that qualitative as-\n1https : / / huggingface . co / runwayml / stable -\ndiffusion-v1-5\nMethod\n#Steps\nTime (s)\nFID-5k\nCLIP\nDPM Solver [35]\n25\n0.88\n20.1\n0.318\n8\n0.34\n31.7\n0.320\nProgressive Distillation [39]\n1\n0.09\n37.2\n0.275\n2\n0.13\n26.0\n0.297\n4\n0.21\n26.4\n0.300\nCFG-Aware Distillation [29]\n8\n0.34\n24.2\n0.30\nInstaFlow-0.9B\n1\n0.09\n23.4\n0.304\nInstaFlow-1.7B\n1\n0.12\n22.4\n0.309\nUFOGen\n1\n0.09\n22.5\n0.311\nTable 1. Comparison of FID on MSCOCO-2017 5k and CLIP\nscore. All models are based on SD. Numbers of progressive distil-\nlation and InstaFlow are cited from [33].\nsessments can provide more comprehensive insights. We\npresent qualitative comparisons involving InstaFlow and\nLCM2 in Table 2.\nThe comparisons allow for a clear-\ncut conclusion: UFOGen\u2019s one-step image generation sur-\npasses InstaFlow by a substantial margin in terms of image\nquality. Notably, UFOGen also demonstrates significant ad-\nvantages when contrasted with the 2-step LCM, as showed\nby the evident blurriness present in LCM\u2019s samples. Fur-\nthermore, even when compared to the samples generated by\nthe 4-step LCM, our generated images exhibit distinct char-\nacteristics, including sharper textures and finer details. We\ndo not present results of single-step LCM, as we observe\nthat it fail to generate any textures (see Appendix A.5.1).\nAdditional examples of the comparison are provided in Ap-\npendix A.5.2, where we display multiple images generated\nby each model for different prompts. We provide additional\nqualitative samples of UFOGen in Appendix A.6.\nFor completeness, we extend our comparison to encom-\npass a diverse array of text-to-image generative models in\nTable 3. While the results in Table 3 are not directly compa-\nrable due to substantial variations in model architecture, pa-\nrameter count, and training data, it is noteworthy that UFO-\nGen is a competitive contender among the contemporary\nlandscape of text-to-image models, offering the advantage\nof remarkable speed over auto-regressive or diffusion mod-\nels, thanks to its inherent one-step generation capability.\nBased on both quantitative and qualitative assessments,\nwe assert that UFOGen stands as a powerful text-to-image\ngenerative model, capable of producing sharp and visually\nappealing images that align well with the provided text con-\nditioning, all in a single step. Our evaluation underscores\nits capacity to produce superior sample quality when con-\ntrasted with competing diffusion-based methods designed\nfor a few-step generation process.\n2InstaFlow (https://huggingface.co/spaces/XCLiu/\nInstaFlow) and LCM (https://huggingface.co/spaces/\nSimianLuo/Latent_Consistency_Model)\nSD (50 steps)\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nCute small corgi sitting in a movie theater eating popcorn, unreal engine.\nA Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style.\nA dog is reading a thick book.\nThree cats having dinner at a table at new years eve, cinematic shot, 8k.\nAn astronaut riding a pig, highly realistic dslr photo, cinematic shot.\nTable 2. Qualitative comparisons of UFOGen against competing methods and SD baseline. Zoom-in for better viewing.\n5.2. Ablation Studies\nAblation studies have been conducted to offer deeper in-\nsights into the effectiveness of our training strategies. As\noutlined in Table 4, we compare the training of diffusion-\nGAN hybrid models using the SIDDM objective [63]\nagainst the proposed UFOGen objective in Section 4.1. The\nresults validate our assertions, demonstrating that the mod-\nifications in the UFOGen objective facilitate one-step sam-\npling.\nWe additionally provide qualitative samples, and\nan supplementary ablation study on the denoising step size\nduring training in Appendix A.4.\n5.3. Applications\nA promising aspect of text-to-image diffusion models is\ntheir versatility as foundational components for various ap-\nplications, whether fine-tuned or utilized as is. In this sec-\ntion, we showcase UFOGen\u2019s ability to extend beyond text-\nto-image generation, while benefiting from its unique ad-\nvantage of single-step generation. Specifically, we explore\ntwo applications of UFOGen: image-to-image [38] genera-\ntion and controllable generation [40, 68].\nMethod\nType\nTime (s)\n# Param.\nFID-30k\nDALLE [44]\nAR\n-\n12B\n27.5\nParti-20B [66]\nAR\n-\n20B\n7.23\nMake-A-Scene [10]\nAR\n25.0\n-\n11.84\nGLIDE [41]\nDiff\n15.0\n5B\n12.24\nDALLE 2 [45]\nDiff\n-\n5.5B\n10.39\nImagen [17]\nDiff\n9.1\n3B\n7.27\neDiff-I [1]\nDiff\n32.0\n9B\n6.95\nSD [47]\nDiff\n2.9\n0.9B\n9.62\nLAFITE [70]\nGAN\n0.02\n75M\n26.94\nStyleGAN-T [52]\nGAN\n0.10\n1B\n13.90\nGigaGAN [21]\nGAN\n0.13\n1B\n9.09\nMuse-3B [7]\n-\n1.3\n3B\n7.88\nInstaFlow [33]\n-\n0.09\n0.9B\n13.10\nUFOGen (Ours)\n-\n0.09\n0.9B\n12.78\nTable 3. Comparison of FID on MSCOCO 2014 with 30k images.\nNumbers of other models are cited from [33]. Inference time mea-\nsurement follows the setting of [19].\nMethod\n#Steps\nFID-5k\nCLIP\nSIDDM [63]\n4\n21.7\n0.306\n1\n28.0\n0.289\nUFOGen\n4\n22.1\n0.307\n1\n22.5\n0.311\nTable 4. Ablation study comparing the SIDDM objective with our\nUFOGen objective, incorporating the introduced modifications de-\ntailed in Section 4.1.\nInput\nOil painting of mountain\nand lake.\nChinese landscape paint-\ning.\nInput\nTree with autumn leaves.\nA winter scene.\nTable 5.\nResults of single-step image-to-image generation by\nUFOGen. Zoom in to view the details.\nTable 5 showcases UFOGen\u2019s image-to-image genera-\ntion outcomes.\nFollowing SDEdit [38], we introduce a\nsuitable amount of noise to the input data, and let UFO-\nGen to execute single-step generation based on the given\nprompt. Our observations affirm that UFOGen adeptly pro-\nduces samples that adhere to the specified conditions of both\nthe prompt and the input image.\nTo facilitate controllable generation, we conduct fine-\nCanny edge\nA cute black and white\ndog, sitting on the beach.\nA cute dog, sitting on the\ngrass, watercolor paint-\ning.\nDepth map\na red sport car on snow-\nfield.\nVintage photo of a rusty car.\nTable 6. Results of controllable generation by UFOGen.\ntuning of UFOGen by incorporating an additional adapter\nnetwork, akin to the approach outlined in [40]. This adapter\nnetwork takes control signals as input to guide the genera-\ntion process. In our exploration, we employ two types of\ncontrol signals: depth maps and canny edges. The results\nare presented in Table 6. Post fine-tuning, UFOGen exhibits\nthe ability to generate high-quality samples that align with\nboth the provided prompt and control signal.\nOur results highlight UFOGen can work on diverse gen-\neration tasks in a single step, a distinctive feature that, to\nthe best of our knowledge, sets our model apart. Unlike\nGAN-based text-to-image models [20, 52], which lack the\nability to handle zero-shot image-to-image generation tasks\nas they do not generate samples through denoising, UFO-\nGen excels in this context. Moreover, our model succeeds\nin controllable generation, a domain that earlier GAN-based\nmodels have not explored due to the complexities of fine-\ntuning and adding supplementary modules to the StyleGAN\narchitecture. Consequently, the flexibility of our model in\naddressing various downstream tasks positions it uniquely\namong one-step text-to-image models. Additional results\nof the applications are provided in Appendix A.7.\n6. Conclusions\nIn this paper, we present UFOGen, a groundbreaking ad-\nvancement in text-to-image synthesis that effectively ad-\ndresses the enduring challenge of inference efficiency. Our\ninnovative hybrid approach, combining diffusion models\nwith a GAN objective, propels UFOGen to achieve ultra-\nfast, one-step generation of high-quality images condi-\ntioned on textual descriptions. The comprehensive evalu-\nations consistently affirm UFOGen\u2019s superiority over exist-\ning accelerated diffusion-based methods. Its distinct capa-\nbility for one-step text-to-image synthesis and proficiency\nin downstream tasks underscore its versatility and mark it\nas a standout in the field. As a pioneer in enabling ultra-fast\ntext-to-image synthesis, UFOGen paves the way for a trans-\nformative shift in the generative models landscape. The po-\ntential impact of UFOGen extends beyond academic dis-\ncourse, promising to revolutionize the practical landscape\nof rapid and high-quality image generation.\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 1, 3, 8\n[2] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-\ndpm:\nan analytic estimate of the optimal reverse vari-\nance in diffusion probabilistic models.\narXiv preprint\narXiv:2201.06503, 2022. 1, 3\n[3] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap,\nShuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Tal-\nbot, and Eric Gu.\nTract:\nDenoising diffusion models\nwith transitive closure time-distillation.\narXiv preprint\narXiv:2303.04248, 2023. 3\n[4] Eyal Betzalel, Coby Penso, Aviv Navon, and Ethan Fe-\ntaya. A study on the evaluation of generative models. arXiv\npreprint arXiv:2206.10935, 2022. 6\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n1\n[6] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun\nHao, Serge Belongie, Noah Snavely, and Bharath Hariharan.\nLearning gradient fields for shape generation. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part III 16, pages\n364\u2013381. Springer, 2020. 1\n[7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 8\n[8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 1\n[9] William Feller. Retracted chapter: On the theory of stochas-\ntic processes, with particular reference to applications. In\nSelected Papers I, pages 769\u2013798. Springer, 2015. 3\n[10] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors. In Eu-\nropean Conference on Computer Vision, pages 89\u2013106.\nSpringer, 2022. 8\n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An\nimage is worth one word: Personalizing text-to-image gener-\nation using textual inversion. In The Eleventh International\nConference on Learning Representations, 2023. 1\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 3\n[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 1\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015. 3\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1, 3, 4, 5, 14\n[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 8\n[18] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash\nPanangaden, and Aaron C Courville. Riemannian diffusion\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:2750\u20132761, 2022. 1\n[19] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park. Scaling up\ngans for text-to-image synthesis, 2023. 8\n[20] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park. Scaling\nup gans for text-to-image synthesis. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 3, 5, 8\n[21] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park.\nScal-\ning up gans for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10124\u201310134, 2023. 8\n[22] Animesh Karnewar and Oliver Wang. Msg-gan: Multi-scale\ngradients for generative adversarial networks. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 7799\u20137808, 2020. 3\n[23] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 3\n[24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:26565\u201326577, 2022. 3\n[25] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. Advances in neural infor-\nmation processing systems, 34:21696\u201321707, 2021. 3\n[26] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023. 6\n[27] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. Diffwave: A versatile diffusion model for\naudio synthesis. In International Conference on Learning\nRepresentations, 2021. 1\n[28] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 1\n[29] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,\nYun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-\nfusion: Text-to-image diffusion model on mobile devices\nwithin two seconds. arXiv preprint arXiv:2306.00980, 2023.\n1, 3, 6\n[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 6\n[31] Qiang Liu. Rectified flow: A marginal preserving approach\nto optimal transport. arXiv preprint arXiv:2209.14577, 2022.\n3\n[32] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast:\nLearning to generate and transfer data with rectified flow. In\nThe Eleventh International Conference on Learning Repre-\nsentations, 2022. 3\n[33] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and\nQiang Liu. Instaflow: One step is enough for high-quality\ndiffusion-based text-to-image generation.\narXiv preprint\narXiv:2309.06380, 2023. 3, 6, 8\n[34] Ilya Loshchilov and Frank Hutter.\nDecoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2018. 15\n[35] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. Advances\nin Neural Information Processing Systems, 35:5775\u20135787,\n2022. 1, 3, 6\n[36] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 1, 3\n[37] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao.\nLatent consistency models:\nSynthesizing high-\nresolution images with few-step inference. arXiv preprint\narXiv:2310.04378, 2023. 1, 3, 6, 16, 18\n[38] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions, 2022. 7, 8\n[39] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 14297\u201314306, 2023. 1, 3, 6\n[40] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 1,\n7, 8\n[41] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning,\npages 16784\u201316804. PMLR, 2022. 1, 3, 8\n[42] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 3, 6\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 6\n[44] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n8\n[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1, 3, 8\n[46] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-\ngeswaran, Bernt Schiele, and Honglak Lee. Generative ad-\nversarial text to image synthesis. In International conference\non machine learning, pages 1060\u20131069. PMLR, 2016. 3\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2, 3, 5, 6, 8\n[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 1\n[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-\nmans, Jonathan Ho, David J Fleet, and Mohammad Norouzi.\nPhotorealistic text-to-image diffusion models with deep lan-\nguage understanding.\nIn Advances in Neural Information\nProcessing Systems, pages 36479\u201336494. Curran Associates,\nInc., 2022. 1, 3\n[50] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In International Confer-\nence on Learning Representations, 2022. 1, 3\n[51] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-\nxl: Scaling stylegan to large diverse datasets. In ACM SIG-\nGRAPH 2022 conference proceedings, pages 1\u201310, 2022. 3\n[52] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger,\nand Timo Aila. Stylegan-t: Unlocking the power of gans\nfor fast large-scale text-to-image synthesis. arXiv preprint\narXiv:2301.09515, 2023. 3, 5, 8\n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 3,\n6\n[54] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n1, 3\n[55] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 1, 3\n[56] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, 2021. 1, 3, 5\n[57] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. 2023. 1, 3, 16\n[58] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun\nBao, and Changsheng Xu.\nDf-gan: A simple and effec-\ntive baseline for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16515\u201316525, 2022. 3\n[59] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu\nChen, and Mingyuan Zhou. Diffusion-gan: Training gans\nwith diffusion. In The Eleventh International Conference on\nLearning Representations, 2023. 2\n[60] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling\nthe generative learning trilemma with denoising diffusion\nGANs. In International Conference on Learning Represen-\ntations, 2022. 2, 3, 4, 14, 15\n[61] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Er-\nmon, and Jian Tang. Geodiff: A geometric diffusion model\nfor molecular conformation generation.\nIn International\nConference on Learning Representations, 2022. 1\n[62] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1316\u2013\n1324, 2018. 3\n[63] Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias\nGrundmann, Tingbo Hou, et al. Semi-implicit denoising dif-\nfusion models (siddms). arXiv preprint arXiv:2306.12511,\n2023. 2, 4, 7, 8, 13, 15\n[64] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuo-\nfan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image\ngeneration via large mixture of diffusion paths.\narXiv\npreprint arXiv:2305.18295, 2023. 1, 3\n[65] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18381\u201318391,\n2023. 1\n[66] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. Transac-\ntions on Machine Learning Research, 2022. 8\n[67] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In Proceedings of the IEEE\ninternational conference on computer vision, pages 5907\u2013\n5915, 2017. 3\n[68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 1, 7\n[69] Huangjie Zheng,\nPengcheng He,\nWeizhu Chen,\nand\nMingyuan Zhou.\nTruncated diffusion probabilistic mod-\nels and diffusion-based adversarial auto-encoders.\nIn The\nEleventh International Conference on Learning Representa-\ntions, 2023. 2\n[70] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,\nChris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and\nTong Sun. Towards language-free training for text-to-image\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 17907\u2013\n17917, 2022. 3, 8\nA. Appendices\nA.1. Deriving the KL objective in Equation 4\nIn this section, we provide a derivation of obtaining a reconstruction objective from the KL term in Equation 4:\nKL(p\u03b8(xt|x\u2032\nt\u22121)||q(xt|xt\u22121)).\n(7)\nNote that q(xt|xt\u22121) = N(\u221a1 \u2212 \u03b2txt\u22121, \u03b2tI) is a Gaussian distribution defined by the forward diffusion. For p\u03b8(xt|x\u2032\nt\u22121),\nalthough the distribution on p\u03b8(x\u2032\nt\u22121) is quite complicated (because this depends on the generator model), given a specific\nx\u2032\nt\u22121, it follows the same distribution of forward diffusion: p\u03b8(xt|x\u2032\nt\u22121) = N(\u221a1 \u2212 \u03b2tx\u2032\nt\u22121, \u03b2tI). Therefore, Equation 7\nis the KL divergence between two Gaussian distributions, which we can computed in closed form. For two multivariate\nGaussian distributions with means \u00b51, \u00b52 and covariance \u03a31, \u03a32, the KL divergence can be expressed as\n1\n2\n\u0014\nlog |\u03a32|\n|\u03a31| \u2212 d + tr\n\b\n\u03a3\u22121\n2 \u03a31\n\t\n+ (\u00b52 \u2212 \u00b51)T \u03a3\u22121\n2\n(\u00b52 \u2212 \u00b51)\n\u0015\n.\nWe can easily plug-in the means and variances for q(xt|xt\u22121) and p\u03b8(x\u2032\nt|x\u2032\nt\u22121) into the expression. Note that \u03a31 = \u03a32 = \u03b2tI,\nso the expression can be simplified to\n(1 \u2212 \u03b2t)||x\u2032\nt\u22121 \u2212 xt\u22121||2\n2\u03b2t\n+ C,\n(8)\nwhere C is a constant. Therefore, with the outer expectation over q(xt) in Equation 4, minimizing the KL objective is\nequivalent to minimizing a weighted reconstruction loss between x\u2032\nt\u22121 and xt\u22121, where xt\u22121 is obtained by sampling x0 \u223c\nq(x0) and xt\u22121 \u223c q(xt\u22121|x0); x\u2032\nt\u22121 is obtained from generating an x\u2032\n0 from the generator followed by sampling x\u2032\nt\u22121 \u223c\nq(x\u2032\nt\u22121|x\u2032\n0).\nNote that in [63], the authors did not leverage the Gaussian distribution\u2019s KL-divergence property. Instead, they decom-\nposed the KL-divergence into an entropy component and a cross-entropy component, subsequently simplifying each aspect\nby empirically estimating the expectation. This simplification effectively converges to the same objective as expressed in\nEquation 8, albeit with an appended term associated with entropy. The authors of [63] introduced an auxiliary parametric\ndistribution for entropy estimation, which led to an adversarial training objective. Nevertheless, our analysis suggests that\nthis additional term is dispensable, and we have not encountered any practical challenges when omitting it.\nA.2. Analysis of the distribution matching at x0\nIn this section, we offer a detailed explanation of why training the model with the objective presented in Equation 4 effectively\nresults in matching x0 and x\u2032\n0. The rationale is intuitive: xt\u22121 and x\u2032\nt\u22121 are both derived from their respective base images,\nx0 and x\u2032\n0, through independent Gaussian noise corruptions. As a result, when we enforce the alignment of distributions\nbetween xt\u22121 and x\u2032\nt\u22121, this implicitly encourages a matching of the distributions between x0 and x\u2032\n0 as well. To provide a\nmore rigorous and formal analysis, we proceed as follows.\nA.2.1\nAdversarial term\nWe provide an explanation of why the adversarial objective in Equation 4 corresponds to matching the distributions q(x0)\nand p\u03b8(x\u2032\n0). Firstly, note that since q(xt\u22121) = Eq(x0) [q(xt\u22121|x0)], where q(xt\u22121|x0) is the Gaussian distribution defined by\nthe forward diffusion process. Therefore, q(xt\u22121) can be expressed as a convolution between q(x0) and a Gaussian kernel:\nq(xt\u22121) = q(x0) \u2217 k(x),\nk(x) = N(0, (1 \u2212 \u00af\u03b1t\u22121) I).\n(9)\nSimilarly, p(\u03b8)(xt\u22121) = p\u03b8(x0) \u2217 k(x), where p\u03b8(x0) is the implicit distribution defined by the generator G\u03b8.\nIn the following lemma, we show that for a probability divergence D, if p(x) and q(x) are convoluted with the same kernel\nk(x), then minimizing D on the distributions after the convolution is equivalent to matching the original distributions p(x)\nand q(x).\nLemma 1 Let Y = X + K, if K is absolutely continuous with density k(x) > 0, x \u2208 R. And a divergence D(Q||P) is\na measure of the difference between distribution Q and P, where D(Q||P) \u2265 0 and D(Q||P) = 0 \u21d0\u21d2 Q = P. Then\nD(q(y)||p(y)) = 0 \u21d0\u21d2 q(x) = p(x).\nProof: The probability density of the summation between two variables is the convolution between their probability\ndensities. Thus, we have:\nD(q(y)||p(y)) = D(q(x) \u2217 k(x)||p(x) \u2217 k(x)),\nD(q(x) \u2217 k(x)||p(x) \u2217 k(x)) = 0 a.e.,\n\u21d0\u21d2 q(x) \u2217 k(x) = p(x) \u2217 k(x),\n\u21d0\u21d2 F(q(x) \u2217 k(x)) = F(p(x) \u2217 k(x)),\n\u21d0\u21d2 F(q(x))F(k(x)) = F(p(x))F(k(x)),\n\u21d0\u21d2 q(x) = p(x)\na.e.,\nwhere F denotes the Fourier Transform, and we utilize the invertibility of the Fourier Transform for the above derivation.\nThus, from Lemma 1, we can get q(x0) = p\u03b8(x0) almost everywhere when JSD(q(xt\u22121)||p\u03b8(xt\u22121)) = 0. Notably,\nwhile training with the adversarial objective on xt\u22121 inherently aligns the distributions of q(x0) and ptheta(x\u2032\n0),it is crucial to\nacknowledge that we cannot directly employ GAN training on x0. This is because the additive Gaussian noise, which serves\nto smooth the distributions, rendering GAN training more stable. Indeed, training GANs on smooth distributions is one of\nthe essential components of all diffusion-GAN hybrid models, as highlighted in [60].\nA.2.2\nKL term\nHere we show that minimizing the reconstruction loss in Equation 8 over the expectation of q(xt) as in Equation 4 is equiv-\nalent to minimizing the reconstruction loss between x0 and x\u2032\n0. According to the sampling scheme of xt\u22121 and x\u2032\nt\u22121, we\nhave\nEq(xt\u22121),p\u03b8(x\u2032\nt\u22121)\n\u0014(1 \u2212 \u03b2t)||x\u2032\nt\u22121 \u2212 xt\u22121||2\n2\u03b2t\n\u0015\n= Eq(x0)q(xt\u22121|x0),p\u03b8(x\u2032\n0)p\u03b8(x\u2032\nt\u22121|x\u2032\n0)\n\u0014(1 \u2212 \u03b2t)||x\u2032\nt\u22121 \u2212 xt\u22121||2\n2\u03b2t\n\u0015\n.\n(10)\nSince the forward diffusion q(xt\u22121|x0) has the Gaussian form [16]\nq(xt\u22121|x0) = N (\u221a\u00af\u03b1t\u22121x0, (1 \u2212 \u00af\u03b1t\u22121) I)\n(11)\nand similar form holds for p\u03b8(x\u2032\nt\u22121|x\u2032\n0), we can rewrite the expectation in Equation 10 over the distribution of simple Gaussian\ndistribution p(\u03f5) = N (\u03f5; 0, I):\nEq(x0)q(xt\u22121|x0),p\u03b8(x\u2032\n0)p\u03b8(x\u2032\nt\u22121|x\u2032\n0)\n\u0014(1 \u2212 \u03b2t)||x\u2032\nt\u22121 \u2212 xt\u22121||2\n2\u03b2t\n\u0015\n= Eq(x0),p\u03b8(x\u2032\n0),p(\u03f5)\n\u0014(1 \u2212 \u03b2t)||x\u2032\nt\u22121 \u2212 xt\u22121||2\n2\u03b2t\n\u0015\n,\n(12)\nwhere x\u2032\nt\u22121 = \u221a\u00af\u03b1t\u22121x\u2032\n0 + (1 \u2212 \u00af\u03b1t\u22121) \u03f5\u2032 and xt\u22121 = \u221a\u00af\u03b1t\u22121x0 + (1 \u2212 \u00af\u03b1t\u22121) \u03f5 are obtained by i.i.d. samples \u03f5\u2032, \u03f5 from p(\u03f5).\nPlug in the expressions to Equation 12, we obtain\nEq(x0),p\u03b8(x\u2032\n0),p(\u03f5)\n\u0014(1 \u2212 \u03b2t)||x\u2032\nt\u22121 \u2212 xt\u22121||2\n2\u03b2t\n\u0015\n= Eq(x0),p\u03b8(x\u2032\n0),p(\u03f5)\n\u0014(1 \u2212 \u03b2t)||\u221a\u00af\u03b1t\u22121(x\u2032\n0 \u2212 x0) + (1 \u2212 \u00af\u03b1t\u22121) (\u03f5\u2032 \u2212 \u03f5)||2\n2\u03b2t\n\u0015\n= Eq(x0),p\u03b8(x\u2032\n0)\n\u0014(1 \u2212 \u03b2t)\u00af\u03b1t\u22121||x\u2032\n0 \u2212 x0||2\n2\u03b2t\n\u0015\n+ C,\nwhere C is a constant independent of the model. Therefore, we claim the equivalence of the reconstruction objective and the\nmatching between x0 and x\u2032\n0.\nHowever, it\u2019s essential to emphasize that the matching between x0 and x\u2032\n0 is performed with an expectation over Gaussian\nnoises. In practical terms, this approach can introduce significant variance during the sampling of xt\u22121 and x\u2032\nt\u22121. This\nvariance, in turn, may result in a less robust learning signal when it comes to aligning the distributions at clean data.As\ndetailed in Section 4.1, we propose a refinement to address this issue. Specifically, we advocate for the direct enforcement\nof reconstruction between x0 and x\u2032\n0. This modification introduces explicit distribution matching at the level of clean data,\nenhancing the model\u2019s robustness and effectiveness.\nA.3. Experimental Details\nFor all the experiments, we initialize the parameters of both the generator and discriminator with the pre-trained Stable\nDiffusion 1.5 checkpoint. In consequence, we follow SD 1.5 to use the same VAE for image encoding/decoding and the\nfrozen text encoder of CLIP ViT-L/14 for text conditioning. Note that both the generator and discriminator operates on latent\nspace. In other words, the generator generates the latent variables and the discriminator distinguishes the fake and true (noisy)\nlatent variables.\nImportant Hyper-parameters\nOne important hyper-parameter is the denoising step size during training, which is the gap\nbetween t \u2212 1 and t. Note that in Section 4.1, we mentioned that the model is trained with multiple denoising steps, while\nit enables one-step inference. Throughout the experiments, we train the models using denoising step size 250, given the\n1000-step discrete time scheduler of SD. Specifically, during training, we sample t randomly from 1 to 1000, and the time\nstep for t \u2212 1 is max(0, t \u2212 250). We conduct ablation studies on this hyper-parameter in Section A.4.\nAnother important hyper-parameter is \u03bbKL, the weighting coefficient for reconstruction term in the objective in Equation\n6. We set \u03bbKL = 1.0 throughout the experiments. We found the results insensitive to slight variations of this coefficient.\nCommon Hyper-parameters\nWe train our models on the LAION Aesthetic 6+ dataset. For the generator, we use AdamW\noptimizer [34] with \u03b21 = 0.9 and \u03b22 = 0.999; for the discriminator, we use AdamW optimizer with \u03b21 = 0.0 and \u03b22 =\n0.999. We adopt learning rate warm-up in the first 1000 steps, with peak learning rate 1e \u2212 4 for both the discriminator and\nthe generator. For training the generator, we apply gradient norm clipping with value 1.0 for generator only. We use batch\nsize 1024. For the generator, we apply EMA with coefficient 0.999. We observe quick convergence, typically in < 50k steps.\nA.4. Additional Results of Ablation Studies\nIn this section, we provide additional results for ablation studies, which are briefly covered in the main text due to the\nconstraints of space. In Appendix A.4.1, we provide qualitative results corresponds to the ablation study conducted in\nSection 5.2. In Appendix A.4.2, we conduct an additional ablation experiment on the denoising step size during training.\nA.4.1\nQualitative Results for Table 4\nWe provide qualitative examples to contrast between the single-step sample generated by SIDDM [63] and our proposed\nUFOGen. Results are shown in Table 7 and 8. We observe that when sampling from SIDDM in only one-step, the samples\nare blurry and over-smoothed, while UFOGen can produce sharp samples in single step. The observation strongly supports\nthe effectiveness of our introduced modifications to the training objective.\nA.4.2\nAblation on Denoising Step-size\nOne important hyper-parameter of training UFOGen is the denoising step size, which is the gap between t and t \u2212 1 during\ntraining. Note that although UFOGen can produce samples in one step, the training requires a meaningful denoising step size\nto compute the adversarial loss on noisy observations. Our model is based on Stable Diffusion, which adopts a discrete time\nscheduler with 1000 steps. Previous diffusion GAN hybrid models [60, 63] divides the denoising process into 2 to 4 steps.\nWe explore denoising step size 125, 250, 500 and 1000, which corresponds to divide the denoising process to 8, 4, 2, and 1\nsteps. Note that during training, we sample t uniformly in [1, 1000), and when the sampled t is smaller than the denoising\nstep size, we set t \u2212 1 to be 0. In other words, a denoising step size 1000 corresponds to always setting t \u2212 1 = 0 and hence\nthe adversarial loss is computed on clean data x0.\nQuantitative results of the ablation study is presented in Table 9. We observe that a denoising step size 1000 fails,\nsuggesting that training with the adversarial loss on noisy data is critical for stabilizing the diffusion-GAN training. This\nobservation was made on earlier work [60, 63] as well. We also observe that denoising step size 250 is the sweet spot, which\nis also aligned with the empirical observations of [60, 63]. We conjecture that the reason for the performance degrade when\nreducing the denoising step size is that the discriminator does not have enough capacity to discriminate on many distinct\nnoise levels.\nSIDDM (1 step)\nUFOen (1 step)\nCute small corgi sitting in a movie theater eating popcorn, unreal engine.\nA Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style.\nTable 7. Qualitative results for the ablation study that compares one-step samples from SIDDM and UFOGen.\nA.5. Additional Results for Qualitative Comparisons\nA.5.1\nFailure of Single-step LCM\nConsistency models try to learn the consistency mapping that maps every point on the PF-ODE trajectory to its boundary\nvalue, i.e., x0 [57], and therefore ideally consistency models should generate samples in one single step. However, in\npractice, due to the complexity of the ODE trajectory, one-step generation for consistency models is not feasible, and some\niterative refinements are necessary. Notably, Latent consistency models (LCM) [37] distilled the Stable Diffusion model\ninto a consistency model, and we observe that single-step sampling fail to generate reasonable textures. We demonstrate the\nsingle-step samples from LCM in figure A.5.1. Due to LCM\u2019s ineffectiveness of single-step sampling, we only qualitatively\ncompare our model to 2-step and 4-step LCM.\nSIDDM (1 step)\nUFOen (1 step)\nAn astronaut riding a pig, highly realistic dslr photo, cinematic shot.\nThree cats having dinner at a table at new years eve, cinematic shot, 8k.\nTable 8. Qualitative results for the ablation study that compares one-step samples from SIDDM and UFOGen.\nDenoising Step-size FID-5k CLIP\n1000\n32.92 0.288\n500\n23.2\n0.314\n250\n22.5\n0.311\n125\n24.7\n0.305\nTable 9. Ablation study comparing the denoising step size during training. For all training denoising step sizes, we generate the samples in\none step.\nFigure 4. Single-step samples from LCM [37] with prompt \u201cPhoto of an astronaut riding a horse\u201d.\nA.5.2\nExtended Results of Table 2\nIn consideration of space constraints in the main text, our initial qualitative comparison of UFOGen with competing methods\nfor few-step generation in Table 2 employs a single image per prompt. It is essential to note that this approach introduces\nsome variability due to the inherent randomness in image generation. To provide a more comprehensive and objective\nevaluation, we extend our comparison in this section by presenting four images generated by each method for every prompt.\nThis expanded set of prompts includes those featured in Table 2, along with additional prompts. The results of this in-\ndepth comparison are illustrated across Table 10 to 17, consistently highlighting UFOGen\u2019s advantageous performance in\ngenerating sharp and visually appealing images within an ultra-low number of steps when compared to competing methods.\nConcurrent to our paper submission, the authors of LCM [37] released updated LCM models trained with more resources.\nThe models are claimed to be stronger than the initially released LCM model, which is used in our qualitative evaluation. For\nfairness in the comparison, we obtain some qualitative samples of the updated LCM model that shares the SD 1.5 backbone\nwith us3, and show them in Table 18 and 19. We observe that while the new LCM model generates better samples than initial\nLCM model does, our single-step UFOGen is still highly competitive against 4-step LCM and significantly better than 2-step\nLCM.\nA.6. Additional Qualitative Samples from UFOGen\nIn this section, we present supplementary samples generated by UFOGen models, showcasing the diversity of results in Table\n20, 21 and 22. Through an examination of these additional samples, we deduce that UFOGen exhibits the ability to generate\nhigh-quality and diverse outputs that align coherently with prompts spanning various styles (such as painting, photo-realistic,\nanime) and contents (including objects, landscapes, animals, humans, etc.). Notably, our model demonstrates a promising\ncapability to produce visually compelling images with remarkable quality within just a single sampling step.\nIn Table 23, we present some failure cases of UFOGen. We observe that UFOGen suffers from missing objects, attribute\nleakage and counting, which are common issues of SD based models, as discussed in [? ? ].\nA.7. Additional Results of UFOGen\u2019s Applications\nIn this section, we provide extended results of UFOGen\u2019s applications, including the image-to-image generation in Figure 5\nand controllable generation in Figure 6.\n3https://huggingface.co/latent-consistency/lcm-lora-sdv1-5\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nTable 10. Prompt: Cute small corgi sitting in a movie theater eating popcorn, unreal engine.\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nTable 11. Prompt: A Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style.\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nTable 12. Prompt: A dog is reading a thick book.\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nTable 13. Prompt: Three cats having dinner at a table at new years eve, cinematic shot, 8k.\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nTable 14. Prompt: An astronaut riding a pig, highly realistic dslr photo, cinematic shot.\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nTable 15. Prompt: A cute black cat inside of a pumpkin.\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nTable 16. Prompt: A traditional tea house in a tranquil garden with blooming cherry blossom trees.\nInstaFlow (1 step)\nLCM (2 steps)\nLCM (4 steps)\nUFOGen (1 step)\nTable 17. Prompt: Hyperrealistic photo of a fox astronaut, perfect face, artstation.\nUpdated LCM (2 steps)\nUpdated LCM (4 steps)\nCute small corgi sitting in a movie theater eating popcorn, unreal engine.\nA Pikachu with an angry expression and red eyes, with lightning around it, hyper realistic style.\nTable 18. Qualitative results of updated LCM model.\nUpdated LCM (2 steps)\nUpdated LCM (4 steps)\nAn astronaut riding a pig, highly realistic dslr photo, cinematic shot.\nThree cats having dinner at a table at new years eve, cinematic shot, 8k.\nTable 19. Qualitative results of updated LCM model.\nTemple ruins in forest, stairs, mist, concept art.\nCute toy tiger made of suede, geometric accurate, intricate details, cinematic.\nCute girl, crop-top, blond hair, animation key art feminine mid shot.\nChinese painting of grapes.\nSunset in a valley with trees and mountains.\nTable 20. Additional qualitative results of UFOGen. Zoom-in for better viewing.\nDog graduation at university.\nAn oil painting of a tall ship sailing through a field of wheat at sunset.\nAn high-resolution photo of a orange Porsche under sunshine.\nPortrait photo of a Asian old warrior chief, tribal panther make up, blue on red.\nA close-up photo of a intricate beautiful natural landscape of mountains and waterfalls.\nTable 21. Additional qualitative results of UFOGen. Zoom-in for better viewing.\nLarge plate of delicious fried chicken, with a side of dipping sauce, realistic advertising photo, 4k.\nAn aerial view of a forest, with a giant tree in the center, realistic render, 4k.\nPhoto of a bowl filled with plums on a wooden table, volumetric lighting.\nPainting of island and cliff overseeing a vast ocean.\nPhoto of a modern glass house in the jungle, small stream flowing, mist, atmospheric.\nTable 22. Additional qualitative results of UFOGen. Zoom-in for better viewing.\nA green apple and a red banana.\nA red bird and a green banana.\nFour dogs on the street.\nTable 23. Failure cases of UFOGen.\nCute small corgi sitting in a movie theater eating popcorn, unreal engine.\nCute small cat sitting in a movie theater eating popcorn, unreal engine.\nA tiger is running.\nA cat is running.\nFigure 5. Extended results of image-to-image generation from UFOGen. For each group, we edit the images by adding noise and sightly\nmodifying the prompt.\nFigure 6. Extended results of controllable generation from UFOGen. For each group of canny edge and depth map images, we use same\nprompts per column.\n"
  },
  {
    "title": "Contrastive Chain-of-Thought Prompting",
    "link": "https://arxiv.org/pdf/2311.09277.pdf",
    "upvote": "29",
    "text": "Contrastive Chain-of-Thought Prompting\nYew Ken Chia\u2217 1,\nDeCLaRe\nGuizhen Chen\u2217 1, 2\nLuu Anh Tuan2\nSoujanya Poria\nDeCLaRe\nLidong Bing\u2020 1\n1DAMO Academy, Alibaba Group, Singapore\nDeCLaReSingapore University of Technology and Design\n2Nanyang Technological University, Singapore\n{yewken_chia, sporia}@sutd.edu.sg\n{guizhen001, anhtuan.luu}@ntu.edu.sg\n{yewken.chia, guizhen.chen, l.bing}@alibaba-inc.com\nAbstract\nDespite the success of chain of thought in en-\nhancing language model reasoning, the under-\nlying process remains less well understood. Al-\nthough logically sound reasoning appears in-\nherently crucial for chain of thought, prior stud-\nies surprisingly reveal minimal impact when\nusing invalid demonstrations instead. Further-\nmore, the conventional chain of thought does\nnot inform language models on what mistakes\nto avoid, which potentially leads to more er-\nrors.\nHence, inspired by how humans can\nlearn from both positive and negative exam-\nples, we propose contrastive chain of thought\nto enhance language model reasoning. Com-\npared to the conventional chain of thought, our\napproach provides both valid and invalid rea-\nsoning demonstrations, to guide the model to\nreason step-by-step while reducing reasoning\nmistakes. To improve generalization, we in-\ntroduce an automatic method to construct con-\ntrastive demonstrations. Our experiments on\nreasoning benchmarks demonstrate that con-\ntrastive chain of thought can serve as a general\nenhancement of chain-of-thought prompting.1\n1\nIntroduction\nWith the trend of large language models (LLMs),\nmassively scaling the model size has enabled\ngreater generalization (Brown et al., 2020) and\nthe emergent ability to perform new tasks when\ngiven suitable prompts (Wei et al., 2022a). How-\never, solely increasing the model size cannot solve\ncomplex reasoning tasks (Rae et al., 2022). To this\nend, chain-of-thought prompting was proposed to\nunlock the reasoning ability of LLMs by generating\nintermediate reasoning steps (Wei et al., 2022b). In\n\u2217Equal contribution. Yew Ken and Guizhen are students\nunder the Joint PhD Program between Alibaba and their cor-\nresponding university.\n\u2020Corresponding author.\n1Our\ncode\nimplementation\nwill\nbe\nreleased\nat\nhttps://github.com/DAMO-NLP-SG/contrastive-cot\nExplanation: The dentist drills 4 teeth, so there are 30 - \n4 = 26 teeth left. The dentist caps 7 more teeth than he \ndrills, so he caps 4 + 7 = 11 teeth. Therefore, the \ndentist fixes a total of 4 + 11 = 15 teeth. To find the \npercentage of teeth the dentist fixes, we divide the \nnumber of teeth fixed by the total number of teeth and \nmultiply by 100: 15/30 x 100 = 50%\nQuestion : James writes a 3-page letter to 2 different \nfriends twice a week. How many pages does he write \na year?\nExplanation: He writes each friend 3*2=6 pages a \nweek. So he writes 6*2=12 pages every week. That \nmeans he writes 12*52=624 pages a year.\nWrong Explanation: He writes each friend 12*52=624 \npages a week. So he writes 3*2=6 pages every week. \nThat means he writes 6*2=12 pages a year.\nQuestion: James has 30 teeth. His dentist drills 4 of \nthem and caps 7 more teeth than he drills. What \npercentage of James' teeth does the dentist fix?\nModel Input\nModel Output\nFigure 1: Example of contrastive chain-of-thought\nwhich leverages both positive and negative demonstra-\ntions to enhance language model reasoning.\npractice, most methods based on chain of thought\nleverage in-context learning (Brown et al., 2020)by\nprompting the model with demonstrations of the in-\nput, chain-of-thought, and output (Chu et al., 2023).\nHowever, despite its success, we lack a thor-\nough understanding of the chain of thought (Cooper\net al., 2021). For example, it was shown that even\ndemonstrations with invalid reasoning can lead to\nsimilar performance compared to valid demonstra-\ntions (Wang et al., 2023)2. Hence, it is not clear\nhow language models learn to reason effectively\nbased on the chain-of-thought demonstrations. On\nthe other hand, mistakes in the intermediate steps\ncan compound and derail the reasoning process\n2Note that while chain-of-thought can be performed in\na zero-shot fashion with prompts, we focus on the few-shot\nsetting, as it was originally proposed in Wei et al. (2022b).\narXiv:2311.09277v1  [cs.CL]  15 Nov 2023\n(Ling et al., 2023). Any potential error in the rea-\nsoning process not only affects the accuracy of the\nfinal result but also undermines the trustworthiness\nof the language model (Turpin et al., 2023). Thus,\nit is also important to reduce mistakes in intermedi-\nate reasoning steps.\nTo address the challenges of chain of thought,\nwe are inspired by how humans can learn from pos-\nitive as well as negative examples. For instance,\nwhen solving a complex task where the intermedi-\nate steps are not well-defined, it is useful to learn\nthe correct steps from positive demonstrations, as\nwell as avoiding faults in negative demonstrations.\nHence, we propose contrastive chain of thought,\nwhich provides both positive and negative demon-\nstrations to enhance the reasoning of language mod-\nels. Naturally, this raises the question of how to\ndesign effective negative demonstrations, as well\nas whether they can be generalized to diverse tasks.\nThrough our analysis of multiple invalid reasoning\ntypes, we design a simple and effective method\nthat can automatically generate contrastive demon-\nstrations from existing valid reasoning chains. Fur-\nthermore, as contrastive chain-of-thought is task-\nagnostic and compatible with methods such as self-\nconsistency (Wang et al., 2022), we believe that\nit can serve as a general enhancement of chain of\nthought.\nTo measure the effectiveness of contrastive chain\nof thought, we present evaluations on a wide range\nof reasoning benchmarks, and find significant ben-\nefits. Notably, compared to conventional chain of\nthought, we observe improvements of 9.8 and 16.0\npoints for GSM-8K (Cobbe et al., 2021) and Bam-\nboogle (Press et al., 2023) respectively when using\nGPT-3.5-Turbo3, a widely used LLM. Further anal-\nysis of the reasoning chains generated from our\nmethod also shows significant reduction in errors.\nIn summary, our main contributions include: (1)\nWe analyse various invalid reasoning types and\nfind that combining positive and negative demon-\nstrations generally boost the effectiveness of chain-\nof-thought. (2) Based on the analysis above, we\npropose contrastive chain of thought to enhance lan-\nguage model reasoning. To improve generalization,\nwe also propose an automatic method to construct\ncontrastive demonstrations. (3) Evaluations on mul-\ntiple reasoning benchmarks demonstrate significant\nimprovements compared to conventional chain of\nthought.\n3https://platform.openai.com/docs/models\n2\nPreliminary Study: Effect of Different\nTypes of Contrastive Demonstrations\nWhile chain of thought (CoT) prompting has en-\nhanced the reasoning of large language models, it\nremains less well understood. For instance, while\nsound reasoning seems intuitively important to ef-\nfective chain of thought, previous work has shown\nthat there is little effect when using invalid demon-\nstrations. On the other hand, previous works in\ncontrastive learning (Khosla et al., 2020) and align-\nment (Ouyang et al., 2022) have demonstrated how\nlanguage models can learn more effectively from\nboth valid and invalid examples. Hence, we con-\nduct a preliminary study with the following re-\nsearch question: Can invalid reasoning demon-\nstrations be instead used to enhance chain of\nthought? Specifically, we aim to study the effect\nof providing chain-of-thought demonstrations in a\n\u201ccontrastive\u201d manner, i.e., demonstrations contain-\ning both valid and invalid rationales.\n2.1\nComponents of Chain of Thought\nCompared to standard prompting with in-context\ndemonstrations (Brown et al., 2020), chain-of-\nthought (CoT) prompting (Wei et al., 2022b) in-\ncludes a rationale for each demonstration example.\nEach rationale consists of a series of intermedi-\nate reasoning steps, guiding the language model to\nsolve tasks in a step-by-step manner. Following the\nformulation of (Wang et al., 2023), we identify two\ndistinct components of each CoT rationale:\n\u2022 Bridging objects are the symbolic items that\nthe model traverses in order to reach the final\nsolution. For example, the objects could be\nnumbers and equations in arithmetic tasks, or\nthe names of entities in factual tasks.\n\u2022 Language templates are the textual hints that\nguide the language model to derive and con-\ntextualize the correct bridging objects during\nthe reasoning process.\n2.2\nWhat is Invalid Chain of Thought?\nGiven the distinct components of chain of thought,\nwe are now able to systematically identify the as-\npects which lead to invalid rationales. Concretely\nthere are two main aspects which are applicable to\nboth the language and object components:\n\u2022 Coherence refers to the correct ordering of\nsteps in a rationale, and is necessary for suc-\ncessful chain of thought. Specifically, as chain\nPrompting\nMethod\nArithmetic Reasoning Example\nQuestion: Leah had 32 chocolates and her sister had 42. \nIf they ate 35, how many pieces do they have left in total?\nFactual Reasoning Example\nQuestion: Who is the grandchild of Dambar Shah?\nStandard\nAnswer: 39\nAnswer: Rudra Shah\nChain-of-Thought \n(CoT)\nStep-by-step answer: Originally, Leah had 32 chocolates \nand her sister had 42. So in total they had 32 + 42 = 74. \nAfter eating 35, they had 74 - 35 = 39 pieces left in total. \nThe answer is 39.\nStep-by-step answer: Dambar Shah (? - 1645) was the \nfather of Krishna Shah. Rudra Shah was the child of \nKrishna Shah (? - 1661). So the final answer (the name \nof the grandchild) is: Rudra Shah.\nCoT: Invalid \nReasoning\nStep-by-step answer: Originally, Leah had 32 chocolates \nand her sister had 42. So her sister had 42 - 32 = 10 \nchocolates more than Leah has. After eating 35, since 10 \n+ 35 = 45, they had 45 - 6 = 39 pieces left in total. The \nanswer is 39. \nStep-by-step answer: Dambar Shah (? - 1645) was the \nking of the Gorkha Kingdom. The Gorkha Kingdom was \nestablished by Prince Dravya Shah. Dravya Shah has a \nchild named Rudra Shah. So the final answer (the name \nof the grandchild) is: Rudra Shah.\nCoT: Incoherent \nObjects\nStep-by-step answer: Originally, Leah had 32 + 42 = 74 \nchocolates and her sister had 32. So in total they had 74 - \n35 = 39. After eating 35, they had 42 pieces left in total. \nThe answer is 39.\nStep-by-step answer: Krishna Shah was the father of \nRudra Shah. Dambar Shah (? - 1645) was the child of \nKrishna Shah (? - 1661). So the final answer (the name \nof the grandchild) is: Rudra Shah.\nCoT: Incoherent \nLanguage\nStep-by-step answer: After eating 32, they had 42 pieces \nleft in total. Originally, Leah had 32 + 42 = 74 chocolates \nand her sister had 35. So in total they had 74 - 35 = 39. \nThe answer is 39.\nStep-by-step answer: Dambar Shah (? - 1645) was the \nchild of Krishna Shah. Rudra Shah was the father of \nKrishna Shah (? - 1661). So the final answer (the name \nof the grandchild) is: Rudra Shah.\nCoT: Irrelevant \nObjects\nStep-by-step answer: Originally, Leah had 19 chocolates \nand her sister had 31. So in total they had 19 + 31 = 50. \nAfter eating 29, they had 50 - 29 = 21 pieces left in total. \nThe answer is 21.\nStep-by-step answer: Metis Amando was the father of \nDavid Amando. Randall Amando was the child of David \nAmando. So the final answer (the name of the \ngrandchild) is: Randall Amando.\nCoT: Irrelevant \nLanguage\nStep-by-step answer: Patricia needs to donate 32 inches, \nand wants her hair to be 42 inches long after the donation. \nHer hair is 35 inches long currently. Her hair needs to be \n32 + 42 = 74 inches long when she cuts it. So she needs \nto grow 74 - 35 = 39 more inches. The answer is 39.\nStep-by-step answer: The husband of Dambar Shah (? - \n1645) is Krishna Shah. Krishna Shah (? - 1661) has a \nbrother called Rudra Shah. So the final answer (the \nname of the brother-in-law) is: Rudra Shah.\nLanguage Component\nObject Component\nInvalid Component (Reasoning / Language / Object)\nFigure 2: Categorization of invalid chain-of-thought examples, following Wang et al. (2023).\nof thought is a sequential reasoning process,\nit is not possible for later steps to be pre-\nconditions of earlier steps.\n\u2022 Relevance refers to whether the rationale\ncontains corresponding information from the\nquestion. For instance, if the question men-\ntions a person named Leah eating chocolates,\nit would be irrelevant to discuss a different\nperson cutting their hair.\nIn addition, following Wang et al. (2023), we\ninclude invalid reasoning as a category of invalid\nchain of thought, which is neither incoherent nor\nirrelevant, but contains logical mistakes. Hence,\nwe aim to study the five main categories of invalid\nchain-of-thought, as shown in Figure 2.\n2.3\nExperimental Setup\nTo conduct the experiments for the preliminary\nstudy, we leverage the GSM8K (Cobbe et al., 2021)\nand Bamboogle (Press et al., 2023) datasets for\narithmetic and factual reasoning respectively. We\nuse the OpenAI Chat Completions API4 which is\none of the most popular and well-performing lan-\nguage models with reasonable cost. Specifically,\nwe use the GPT-3.5-Turbo (0301) version. To study\nthe effect of contrastive demonstrations under vari-\nous settings, we evaluate the five main invalid cat-\negories as shown in Figure 2. Note that we use\n4-shot prompting for each dataset, and the chain-of-\nthought demonstrations are manually constructed\nby previous works (Wei et al., 2022b; Wang et al.,\n2023). To standardize the prompting process, we\nuse a simplified chain-of-thought prompt format,\nas shown in Figure 1.\n2.4\nPreliminary Results\nBased on the preliminary results in Table 1, we\nobserve significant gains across all invalid ratio-\nnale categories compared to conventional chain-\nof-thought. Notably, leveraging chain of thought\nwith contrastive demonstrations containing incoher-\nent objects yields the highest average performance\n4https://platform.openai.com/docs/api-reference\nAnswer: 37.5%\nExplanation: The dentist fixes a total of \n4 + 7 = 11 teeth. To find the \npercentage, we divide the number of \nteeth fixed by the total number of teeth \nand multiply by 100: 11/30 x 100 = \n36.67%\nExplanation: The dentist drills 4 teeth, so there \nare 30 - 4 = 26 teeth left. The dentist caps 7 \nmore teeth than he drills, so he caps 4 + 7 = 11 \nteeth. Therefore, the dentist fixes a total of 4 + 11 \n= 15 teeth. To find the percentage of teeth the \ndentist fixes, we divide the number of teeth fixed \nby the total number of teeth and multiply by 100: \n15/30 x 100 = 50%\nQuestion : James writes a 3-page letter \nto 2 different friends twice a week. How \nmany pages does he write a year?\nExplanation: He writes each friend \n3*2=6 pages a week So he writes \n6*2=12 pages every week. That means \nhe writes 12*52=624 pages a year.\nQuestion: James has 30 teeth. His \ndentist drills 4 of them and caps 7 more \nteeth than he drills. What percentage of \nJames' teeth does the dentist fix?\nQuestion : James writes a 3-page letter to 2 \ndifferent friends twice a week. How many pages \ndoes he write a year?\nExplanation: He writes each friend 3*2=6 pages \na week. So he writes 6*2=12 pages every week. \nThat means he writes 12*52=624 pages a year.\nWrong Explanation: He writes each friend \n12*52=624 pages a week. So he writes 3*2=6 \npages every week. That means he writes 6*2=12 \npages a year.\nQuestion: James has 30 teeth. His dentist drills 4 \nof them and caps 7 more teeth than he drills. \nWhat percentage of James' teeth does the \ndentist fix?\nQuestion: James writes a \n3-page letter to 2 different \nfriends twice a week. How \nmany pages does he write \na year?\nAnswer: 624\nQuestion: James has 30 \nteeth. His dentist drills 4 of \nthem and caps 7 more \nteeth than he drills. What \npercentage of James' teeth \ndoes the dentist fix?\nChain-of-Thought (CoT)\nContrastive Chain-of-Thought\nStandard Prompting\nModel Input\nModel Input\nModel Input\nModel Output\nModel Output\nModel Output\nFigure 3: Overview of contrastive chain-of-thought (right), with comparison to common prompting methods.\nPrompting Method\nGSM8K\nBamboogle\nAvg.\nStandard\n27.4\n11.2\n19.3\nChain-of-Thought\n69.2\n40.8\n55.0\nw/ Invalid Reasoning\n76.0\n45.6\n60.8\nw/ Incoherent Objects\n79.6\n53.6\n66.6\nw/ Incoherent Language\n78.8\n52.8\n65.8\nw/ Irrelevant Objects\n79.8\n48.8\n64.3\nw/ Irrelevant Language\n80.2\n49.6\n64.9\nTable 1: Preliminary results on the effect of contrastive\ndemonstrations for chain of thought.\non GSM8K and Bamboogle. This suggests that\nlanguage models are better able to learning step-\nby-step reasoning when provided with both valid\nand invalid rationales. Hence, we believe that con-\ntrastive demonstrations have the potential to greatly\nenhance language model reasoning ability.\n3\nContrastive Chain of Thought\nChain-of-thought (CoT) prompting, as evidenced\nby prior research, has indeed elevated the reasoning\ncapabilities of large language models (Wei et al.,\n2022b). However, a comprehensive understand-\ning of this phenomenon is still lacking. Although\nlogically sound reasoning appears to be inherently\ncrucial for chain of thought, prior studies surpris-\ningly reveal minimal impact when employing in-\nvalid demonstrations. To this end, based on our\npreliminary study in Section 2, we found that pro-\nviding both valid and invalid reasoning demonstra-\ntions in a \u201ccontrastive\u201d manner greatly improves\nreasoning performance. However, this approach\nmay not generalize well to new tasks, as it requires\nmanual construction of the invalid rationales.\nThus, we propose a general prompting method\nknown as contrastive chain of thought, which\nincludes automatic construction of contrastive\ndemonstrations. Figure 3 presents an overview of\nour approach. Specifically, the language model is\nprovided with the question, ground truth answer ex-\nplanation and incorrect answer explanation. Com-\npared to standard prompting, our method enables\nmodels to perform more complex reasoning by de-\ncomposing problems into intermediate steps. Com-\npared to conventional chain-of-thought prompting,\nour method contrasts the valid and invalid answer\nexplanations, guiding the model to generate more\naccurate reasoning chains.\nConcretely, given a small set of n in-context\ndemonstration examples D = {E1, . . . , E|n|}, and\na query Q, the goal of the model is to generate a\nsuitable answer A. For standard prompting, the\ndemonstration examples consist of just the ques-\ntion and answer, i.e., Ej = (Qj, Aj). On the other\nhand, chain-of-thought is a more advanced prompt-\ning method that guides the model with intermediate\nPrompting Method\nArithmetic Reasoning\nFactual QA\nGSM8K\nAQuA\nGSM-Hard\nSVAMP\nASDIV\nBamboogle\nStrategyQA\nStandard\n27.4\n29.5\n11.2\n69.3\n75.8\n12.0\n59.4\nCoT\n69.2\n53.5\n33.8\n67.2\n70.8\n40.8\n55.8\nContrastive CoT\n79.0 (+9.8)\n57.5 (+3.9)\n44.2 (+10.4)\n81.6 (+14.4)\n84.4 (+13.6)\n56.8 (+16.0)\n66.2 (+10.4)\nStandard-SC\n28.0\n29.9\n11.0\n69.0\n76.0\n11.2\n59.6\nCoT-SC\n71.0\n55.9\n34.0\n71.6\n74.0\n40.8\n57.0\nContrastive CoT-SC\n86.2 (+15.2)\n71.7 (+15.7)\n50.0 (+16.0)\n85.2 (+13.6)\n89.6 (+15.6)\n58.4 (+17.6)\n69.6 (+12.6)\nTable 2: Main evaluation results for contrastive chain-of-thought on several reasoning tasks.\nDataset\nType\n|Train|\n|Test|\nGSM8K\nArithmetic Reasoning\n4\n500\nAQuA\nArithmetic Reasoning\n4\n254\nGSM-Hard\nArithmetic Reasoning\n4\n500\nSVAMP\nArithmetic Reasoning\n4\n500\nASDIV\nArithmetic Reasoning\n4\n500\nBamboogle\nFactual QA\n4\n125\nStrategyQA\nFactual QA\n4\n500\nTable 3: Details of datasets used.\nreasoning steps T. As shown in the figure above,\nthe reasoning steps T typically consist of multi-\nple sentences where each sentence describes one\nreasoning step. Hence, chain-of-thought prompt-\ning examples consist of the question, reasoning\nsteps, and final answer, i.e., Ej = (Qj, Tj, Aj).\nHowever, the model does not know what faults\nto avoid in conventional chain-of-thought, which\ncould lead to increased mistakes and error prop-\nagation. Hence, our contrastive chain of thought\nmethod provides both the correct and incorrect rea-\nsoning steps in the demonstration examples, i.e.,\nEj = (Qj, Tj,+, Aj,+, Tj,\u2212, Aj,\u2212).\nTo obtain the correct reasoning steps T+ for the\ndemonstration examples, we use the annotated ex-\namples from the previous chain-of-thought works.\nFor the incorrect reasoning steps T\u2212, we automati-\ncally construct it from the correct reasoning steps\nT+, based on the \"Incoherent Objects\" category in\nSection 2. Concretely, we use an existing entity\nrecognition model5 to extract the object spans such\nas numbers, equations, or persons from a given\nchain-of-thought rationale. Consequently, we ran-\ndomly shuffle the position of the objects within the\nrationale, thus constructing a rationale with inco-\nherent bridging objects. Note that when testing\nwith a new question, only the question and demon-\nstration examples are provided to the model, and\nthe model must generate its own reasoning steps\n5https://spacy.io/models/en#en_core_web_trf\nbefore producing the final answer.\n4\nExperiments\n4.1\nExperimental Setup\nWe focus our study on two main types of reasoning\ntasks: arithmetic reasoning and factual question\nanswering (QA). For arithmetic reasoning, we con-\nduct experiments on a range of datasets including\nGSM8K (Cobbe et al., 2021), AQuA (Ling et al.,\n2017), GSM-Hard (Gao et al., 2023), SVAMP (Pa-\ntel et al., 2021), and ASDIV (Miao et al., 2020).\nFor factual QA, we include two datasets: Bam-\nboogle (Press et al., 2023) and StrategyQA (Geva\net al., 2021). To maintain a reasonable computing\nbudget, we limit each dataset to a maximum of\n500 test samples through random sampling. For\ndatasets that contain less than 500 test samples, we\ninstead use all available test samples. The datasets\u2019\ndetails are included in Table 3. Regarding model\nand prompting details, we use the same experimen-\ntal setup as for our preliminary study in Section\n2.\n4.2\nMain Results\nTo assess the effectiveness of our method, we eval-\nuate on several reasoning tasks and report the main\nresults in Table 2. Our main findings are as follows:\nContrastive CoT demonstrates consistent im-\nprovements\nover\nconventional\nCoT.\nCon-\ntrastive CoT consistently outperforms conventional\nCoT across the datasets in both arithmetic and fac-\ntual reasoning categories. Notably, we observe\nsubstantial gains of more than 10 points on GSM-\nHard, SVAMP, ASDIV, Bamboogle and Strate-\ngyQA. Thus, the consistent and significant perfor-\nmance improvements demonstrate the general ef-\nfectiveness of our proposed method. As contrastive\nchain of thought can be automatically constructed\nfrom existing rationales, the annotation cost is the\nsame as conventional chain of thought. Hence, it\ncan be viewed as a general enhancement of chain\nof thought.\nContrastive CoT is more effective when ap-\nplied with self-consistency.\nAs self-consistency\n(Wang et al., 2022) is a popular decoding strategy\nto boost the chain-of-thought performance of large\nlanguage models, we are interested to see if con-\ntrastive chain of thought can benefit similarly from\nself-consistency. In general, we observe that self-\nconsistency further enhances the performance of\ncontrastive CoT. This enhancement is particularly\nevident in the case of the AQuA dataset. While con-\ntrastive CoT alone results in a modest performance\nimprovement of 4.0%, applying self-consistency\namplifies this gain significantly, achieving an addi-\ntional improvement of 14.2%.\n5\nRelated Work\nLarge Language Models\nRecent developments\nin large language models have shown that mas-\nsively scaling the size and training data of models\ncan greatly improve generalization (Kaplan et al.,\n2020). Notably, large language models have been\nshown to generalize to new tasks when given suit-\nable prompts and demonstrations (Brown et al.,\n2020). This has brought about a new paradigm of\nleveraging language models for tasks without the\nneed for additional training (Liu et al., 2023). How-\never, simply scaling language models has not been\nsufficient to attain good performance on challeng-\ning tasks such as arithmetic reasoning and factual\nquestion answering (Wei et al., 2022b). Hence, in\nthis work, we focus on enhancing the reasoning\nability of large language models through prompts.\nChain of Thought\nChain-of-thought prompting\nwas introduced by Wei et al. (2022b) to enhance\nlanguage model reasoning by generating interme-\ndiate steps. Notably, this has inspired numerous\nworks that build upon this direction of step-by-\nstep reasoning. For instance, automatic chain-of-\nthought (Zhang et al., 2023) was proposed to ad-\ndress the challenges in manually annotating chain-\nof-thought demonstrations. On the other hand, it\nwas shown that specific prompts such as \u201cLet\u2019s\nthink step-by-step\u201d can enable language models\nto perform chain-of-thought in a zero-shot man-\nner, without any demonstrations (Kojima et al.,\n2022). In addition, challenging problems can be de-\ncomposed into multiple sub-problems (Zhou et al.,\n2023), or even into code programs that can be au-\ntomatically executed (Gao et al., 2023). Despite\nthe progress in chain-of-thought on multiple fronts,\nwe still lack a rigorous understanding of the under-\nlying mechanism (Turpin et al., 2023; Feng et al.,\n2023). In this work, inspired by the findings of pre-\nvious works regarding invalid demonstrations, we\npropose contrastive chain-of-thought to enhance\nlanguage model reasoning. As contrastive chain-\nof-thought leverages both valid and invalid reason-\ning demonstrations, we believe this may encour-\nage other researchers to fundamentally rethink the\nchain-of-thought process.\nLearning from Negative Examples\nWhile\nchain-of-thought prompting typically involves only\nvalid demonstrations, it is not clear whether in-\nvalid demonstrations can also benefit the reason-\ning process (Wang et al., 2023).\nOn the other\nhand, learning from negative or invalid samples\nis not new. For instance, contrastive learning is\na well-established deep learning approach that en-\ncourages models to distinguish between \u201cpositive\u201d\nand \u201cnegative\u201d samples, thus learning better rep-\nresentations (Khosla et al., 2020). Similarly, rein-\nforcement learning from human feedback (RLHF)\ntrains a reward model based on positive and neg-\native samples of human preference data (Ouyang\net al., 2022; Christiano et al., 2017). Hence, in-\nspired by the previous approaches, we propose con-\ntrastive chain-of-thought, a general enhancement\nof chain-of-thought prompting, by enabling mod-\nels to learn from both valid and invalid reasoning\ndemonstrations.\n6\nConclusions\nIn this work, we have explored the effect of leverag-\ning invalid reasoning demonstrations for enhancing\nchain of thought. Through our preliminary study\non different invalid chain-of-thought categories, we\nfound that providing both valid and invalid demon-\nstrations in a contrastive manner greatly improves\nreasoning ability in language models. To overcome\nthe challenge of manually annotating invalid ratio-\nnales, we propose contrastive chain of thought, a\ngeneral prompting method which can automatically\nconstruct contrastive demonstrations from existing\nrationales. Through experiments on several reason-\ning tasks, we find contrastive chain of thought to be\na general enhancement of chain-of-thought prompt-\ning. Further investigation into alternative forms of\nchain-of-thought prompting will hopefully inspire\nfuture advancements in language-based reasoning.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nZheng Chu, Jingchang Chen, Qianglong Chen, Weijiang\nYu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,\nBing Qin, and Ting Liu. 2023. A survey of chain of\nthought reasoning: Advances, frontiers and future.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. CoRR, abs/2110.14168.\nNathan Cooper, Carlos Bernal-C\u00e1rdenas, Oscar Cha-\nparro, Kevin Moran, and Denys Poshyvanyk. 2021.\nIt takes two to tango: Combining visual and textual\ninformation for detecting duplicate video-based bug\nreports. CoRR, abs/2101.09194.\nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye,\nDi He, and Liwei Wang. 2023. Towards revealing\nthe mystery behind chain of thought: A theoretical\nperspective. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. PAL: Program-aided language\nmodels. In Proceedings of the 40th International\nConference on Machine Learning, volume 202 of\nProceedings of Machine Learning Research, pages\n10764\u201310799. PMLR.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346\u2013\n361.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna,\nYonglong\nTian,\nPhillip\nIsola,\nAaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n18661\u201318673. Curran Associates, Inc.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158\u2013167, Vancouver,\nCanada. Association for Computational Linguistics.\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang,\nMingu Lee, Roland Memisevic, and Hao Su. 2023.\nDeductive verification of chain-of-thought reasoning.\nIn Thirty-seventh Conference on Neural Information\nProcessing Systems.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9).\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nEnglish math word problem solvers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 975\u2013984, Online.\nAssociation for Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d\u2019Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R. Bowman. 2023. Language models don\u2019t\nalways say what they think: Unfaithful explanations\nin chain-of-thought prompting. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2023.\nTowards understanding chain-of-thought prompting:\nAn empirical study of what matters. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2717\u20132739, Toronto, Canada. Association for\nComputational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Huai hsin Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. ArXiv, abs/2203.11171.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\n2022a. Emergent abilities of large language models.\nTrans. Mach. Learn. Res., 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023. Automatic chain of thought prompting\nin large language models. In The Eleventh Interna-\ntional Conference on Learning Representations.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H.\nChi. 2023. Least-to-most prompting enables com-\nplex reasoning in large language models. In The\nEleventh International Conference on Learning Rep-\nresentations.\n"
  },
  {
    "title": "Adaptive Shells for Efficient Neural Radiance Field Rendering",
    "link": "https://arxiv.org/pdf/2311.10091.pdf",
    "upvote": "16",
    "text": "Adaptive Shells for Efficient Neural Radiance Field Rendering\nZIAN WANG\u2217, NVIDIA, University of Toronto, Vector Institute, Canada\nTIANCHANG SHEN\u2217, NVIDIA, University of Toronto, Vector Institute, Canada\nMERLIN NIMIER-DAVID\u2217, NVIDIA, Switzerland\nNICHOLAS SHARP, NVIDIA, USA\nJUN GAO, NVIDIA, University of Toronto, Vector Institute, Canada\nALEXANDER KELLER, NVIDIA, Germany\nSANJA FIDLER, NVIDIA, University of Toronto, Vector Institute, Canada\nTHOMAS M\u00dcLLER, NVIDIA, Switzerland\nZAN GOJCIC, NVIDIA, Switzerland\nkernel\nsize\nsamples\n8\n24\n0\n1\noutdoor\nsimulation\ntabletop\nsynthetic\nanimation\nrendered\n201fps/4.97ms @ 1080p\nbounding\nshell\nFig. 1. This work presents an approach for efficiently rendering neural radiance fields by restricting volumetric rendering to a narrow band around the object.\nLeft: We first fit a dense neural volume using a new spatially-varying kernel that automatically adapts to be large in volumetric regions such as hair or grass,\nand small in sharp-surface regions such as skin or furniture. We then extract an explicit bounding mesh of the region to be rendered whose width is determined\nby the kernel, and render at real-time rates. Right: the proposed method is general and effective across a wide range of data and well-suited for downstream\napplications such as simulation and animation. The face model of the Khady synthetic human shown left is courtesy of texturing.xyz.\nNeural radiance fields achieve unprecedented quality for novel view synthe-\nsis, but their volumetric formulation remains expensive, requiring a huge\nnumber of samples to render high-resolution images. Volumetric encodings\nare essential to represent fuzzy geometry such as foliage and hair, and they\nare well-suited for stochastic optimization. Yet, many scenes ultimately con-\nsist largely of solid surfaces which can be accurately rendered by a single\nsample per pixel. Based on this insight, we propose a neural radiance for-\nmulation that smoothly transitions between volumetric- and surface-based\nrendering, greatly accelerating rendering speed and even improving visual\nfidelity. Our method constructs an explicit mesh envelope which spatially\n\u2217Authors contributed equally.\nAuthors\u2019 addresses: Zian Wang, NVIDIA, University of Toronto, Vector Institute,\nToronto, Canada, zianw@nvidia.com; Tianchang Shen, NVIDIA, University of Toronto,\nVector Institute, Toronto, Canada, frshen@nvidia.com; Merlin Nimier-David, NVIDIA,\nZ\u00fcrich, Switzerland, mnimierdavid@nvidia.com; Nicholas Sharp, NVIDIA, Seattle,\nUSA, nsharp@nvidia.com; Jun Gao, NVIDIA, University of Toronto, Vector Insti-\ntute, Toronto, Canada, jung@nvidia.com; Alexander Keller, NVIDIA, Berlin, Germany,\nakeller@nvidia.com; Sanja Fidler, NVIDIA, University of Toronto, Vector Institute,\nToronto, Canada, sfidler@nvidia.com; Thomas M\u00fcller, NVIDIA, Z\u00fcrich, Switzerland,\ntmueller@nvidia.com; Zan Gojcic, NVIDIA, Z\u00fcrich, Switzerland, zgojcic@nvidia.com.\nbounds a neural volumetric representation. In solid regions, the envelope\nnearly converges to a surface and can often be rendered with a single sam-\nple. To this end, we generalize the NeuS [Wang et al. 2021] formulation\nwith a learned spatially-varying kernel size which encodes the spread of\nthe density, fitting a wide kernel to volume-like regions and a tight ker-\nnel to surface-like regions. We then extract an explicit mesh of a narrow\nband around the surface, with width determined by the kernel size, and\nfine-tune the radiance field within this band. At inference time, we cast rays\nagainst the mesh and evaluate the radiance field only within the enclosed\nregion, greatly reducing the number of samples required. Experiments show\nthat our approach enables efficient rendering at very high fidelity. We also\ndemonstrate that the extracted envelope enables downstream applications\nsuch as animation and simulation.\nCCS Concepts: \u2022 Computing methodologies \u2192 Rendering; Shape rep-\nresentations; Reconstruction.\nAdditional Key Words and Phrases: Neural Radiance Fields, Fast Rendering,\nLevel Set Methods, Novel View Synthesis\narXiv:2311.10091v1  [cs.CV]  16 Nov 2023\n2\n\u2022\nWang, Shen, Nimier-David, et al.\n1\nINTRODUCTION\nNeural radiance fields, which we will refer to as NeRFs, have recently\nemerged as a powerful 3D representation enabling photorealistic\nnovel-view synthesis and reconstruction. Unlike traditional explicit\nmethods for novel-view synthesis, NeRFs forego high-quality mesh\nreconstruction and explicit surface geometry in favor of neural net-\nworks, which encode the volumetric density and appearance of a\nscene as a function of 3D spatial coordinates and viewing direction.\nHowever, the high visual fidelity of NeRFs comes at a great compu-\ntational cost, as the volume rendering formulation requires a large\nnumber of samples along each ray and ultimately prevents real-\ntime synthesis of high-resolution novel views. In tandem, explicit\nreconstruction and novel-view synthesis have continued to make\ngreat progress by leveraging advances in inverse rendering and\ndata-driven priors, but a fidelity gap remains. The goal of this work\nis to close this gap by developing a neural volumetric formulation\nthat leverages explicit geometry to accelerate performance, without\nsacrificing quality.\nMuch recent and concurrent work has likewise sought to improve\nthe efficiency of NeRF representations and volume rendering. An\nimportant step towards this goal was the evolution from a global\nlarge multi-layer perceptron (MLP) representation [Mildenhall et al.\n2020] to local sparse feature fields combined with shallow MLP\ndecoders [M\u00fcller et al. 2022; Sara Fridovich-Keil and Alex Yu et al.\n2022]. This resulted in several orders-of-magnitude speed-ups. Com-\nplementary research to improve the efficiency of NeRFs proposed\nreplacing the neural networks by simpler functions such as spheri-\ncal harmonics, or baking the volumetric representation onto proxy\ngeometry that accelerates rendering [Chen et al. 2023; Yariv et al.\n2023]. The latter formulation enables especially large speedups and\nfacilitates real-time rendering even on commodity devices [Chen\net al. 2023]. Yet, doing so compromises the quality as the scene\ncontent is projected onto proxy geometry.\nIn this work, we instead aim to make NeRF rendering more effi-\ncient while maintaining or even improving the perceptual quality.\nTo this end, we propose a narrow-band rendering formulation that\nenables efficient novel-view synthesis, while enjoying the desir-\nable properties of the volumetric representation (Figure 1 left). Our\nmethod is inspired by the insight that different regions of the scene\nbenefit from different styles of rendering. Indeed, fuzzy surfaces\nwith intricate geometry and complex transparency patterns ben-\nefit greatly from exhaustive volume rendering, while conversely,\nopaque smooth surfaces can be well\u2014or potentially even better\u2014\nrepresented by a single sample where the ray intersects the surface.\nThis observation allows us to better distribute the computational\ncost across the rays by assigning as many samples as needed to\nfaithfully represent the ground-truth appearance.\nWith the introduction of auxiliary acceleration data structures\nthat promote empty space skipping [M\u00fcller et al. 2022], NeRFs can\nalready render images with a varying number of samples per ray.\nStill, there remain many challenges that prevent the current formu-\nlations from efficiently adapting to the local complexity of the scene\n(Figure 2). First, the memory footprint of grid-based acceleration\nstructures scales poorly with resolution. Second, the smooth induc-\ntive bias of MLPs hinders learning a sharp impulse or step function\noccupied\nvoxels\nvoxel space-skipping\nnarrow-band rendering (ours)\nray\nsamples\nextracted\nband\nexplicit\nmesh\nsingle\nsample\nFig. 2. One state-of-the-art approach to accelerate volumetric rendering is\nto skip empty voxels, however this still requires multiple samples within\noccupied voxels (left). Our approach extracts a narrow band mesh, for which\na single sample at the midpoint is a very good approximation of the surface\n(right).\nfor volume density, and even if such an impulse was learned it\nwould be difficult to sample it efficiently. Finally, due to the lack of\nconstraints, the implicit volume density field fails to accurately rep-\nresent the underlying surfaces [Wang et al. 2021], which often limits\ntheir application in downstream tasks that rely on mesh extraction.\nTo remedy the last point, [Wang et al. 2021, 2022a; Yariv et al.\n2021] propose to optimize a signed distance function (SDF) along\nwith a kernel size encoding the spread of the density, rather than\noptimizing density directly. While this is effective for improving\nsurface representations, the use of a global kernel size contradicts\nthe observation that different regions of the scene demand adaptive\ntreatment.\nTo address the above challenges, we propose a new volumetric\nneural radiance field representation. In particular: i) We generalize\nthe NeuS [Wang et al. 2021] formulation with a spatially-varying\nkernel width that converges to a wide kernel for fuzzy surfaces,\nwhile collapsing to an impulse function for solid opaque surfaces\nwithout additional supervision. This improvement alone results in\nan increased rendering quality across all scenes in our experiments.\nii) We use the learned spatially-varying kernel width to extract a\nmesh envelope of a narrow band around the surface. The width of\nthe extracted envelope adapts itself to the complexity of the scene\nand serves as an efficient auxiliary acceleration data structure. iii)\nAt inference time, we cast rays against the envelope in order to skip\nempty space and sample the radiance field only in regions which\ncontribute significantly to the rendering. In surface-like regions,\nthe narrow band enables rendering from a single sample, while\nprogressing to a wider kernel and local volumetric rendering for\nfuzzy surfaces.\nThe experiments of Section 4 validate the effectiveness of our\nformulation across several data sets. In addition, the applications of\nSection 5 demonstrate the benefits of our representation.\n2\nRELATED WORK\nSynthesizing novel views from a set of images is a longstanding\nproblem in the fields of computer vision and graphics. The classi-\ncal approaches to novel-view synthesis can be roughly categorized\nbased on the coverage density of the input images. In particular,\nAdaptive Shells for Efficient Neural Radiance Field Rendering\n\u2022\n3\ninitial fiting\nfine tuning\nreal-time rendering\ntarget images\nneural\nimplicits\n+\nhash grid\nx\ny\nz\ncolor\nSDF\nkernel size\nnormal\nshell extraction\nenables eficent\nsampling\nerosion flow &\ndilation flow\nexplicit\nbounding mesh\nlarge        volume\nsmall        surface\nFig. 3. Overview of the proposed approach. We demonstrate high-fidelity, efficient neural implicit scene reconstruction by efficiently-sampling volumetric\nrendering inside of an explicit thin shell, which is automatically fit from visual objectives.\nlight field interpolation methods [Davis et al. 2012; Gortler et al.\n1996; Levoy and Hanrahan 1996] assume that the input views are\nsampled densely and close to the target view. When the input views\nare sparse, classical methods usually follow a two-stage approach:\nIn the first stage, they construct a proxy geometry from the images\nusing a combination of a multi-view stereo pipeline [Sch\u00f6nberger\nand Frahm 2016; Sch\u00f6nberger et al. 2016] and point cloud recon-\nstruction methods [Kazhdan et al. 2006; Kazhdan and Hoppe 2013].\nIn the second stage, the input images are then unprojected onto the\ngeometry either directly in terms of RGB colors [Buehler et al. 2001;\nDebevec et al. 1996; Waechter et al. 2014; Wood et al. 2000] or, more\nrecently, latent features [Riegler and Koltun 2020, 2021]. Other lines\nof research have developed specialized methods for certain classes\nof objects, such as faces (e.g. Bi et al. [2021])\u2014although we show\nresults on synthetic human and animal data (Figure 1), the approach\npresented here is entirely general.\nNeural Radiance Fields (NeRFs). NeRF [Mildenhall et al. 2020] have\nrevolutionized the prevailing paradigm of novel-view synthesis,\nby using a neural network to represent the scenes as a volumet-\nric (radiance) field that may be queried at any location to return\nthe view-dependent radiance and volume density. Mildenhall et al.\n[2020] synthesize novel views by querying the radiance field along\nthe image rays and accumulating the appearance using volume\nrendering. The photorealistic quality of NeRF has inspired a large\nbody of follow-up work. NeRF++ [Zhang et al. 2020] analyzed the\ndifficulties of NeRF to represent unbounded scenes and proposed a\nbackground formulation based on the inverted sphere representa-\ntion. MipNeRF [Barron et al. 2021] addressed the aliasing effects with\nan integrated positional encoding. This work was later extended to\nunbounded scenes [Barron et al. 2022] by contracting the volume\nand using an additional proposal network. [Deng et al. 2022] and\n[Niemeyer et al. 2022] tackled the challenging setting with sparse in-\nput views and proposed to regularize the volumetric representation\nusing depth supervision or smoothness constraints and data priors\nbased on normalizing flows, respectively. NeRF-W [Martin-Brualla\net al. 2021] has shown how NeRF can be extended to unstructured\ncollections of images captured in-the-wild, by using per-frame learn-\nable latent codes to compensate for appearance differences and a\ntransient embedding to remove dynamic objects. Alternative repre-\nsentations to neural fields include point clouds [Kopanas et al. 2021;\nR\u00fcckert et al. 2021], spheres [Lassner and Zollh\u00f6fer 2021], and 3D\nGaussians [Kerbl et al. 2023].\nImplicit surface representation. The NeRF formulation has two\nmain shortcomings when it comes to modeling surfaces: i) Besides\na lack of regularization of the density field, ii) surface extraction has\nto be performed at an arbitrary level-set of the density field. In com-\nbination, these lead to noisy and low-fidelity surface reconstruction.\nHowever, with small changes in the formulation, implicit represen-\ntations combined with volume rendering [Oechsle et al. 2021; Wang\net al. 2021, 2022b; Yariv et al. 2021, 2020; Zhang et al. 2021] still\nappear as a promising alternative to classical surface reconstruction\napproaches from image data [Sch\u00f6nberger et al. 2016]. For exam-\nple, instead of directly optimizing the density field, [Wang et al.\n2021; Yariv et al. 2021] proposed to decompose it into an SDF and a\nglobal kernel size that defines the spread of the density. This allows\nfor extracting accurate surfaces from the zero-level set of the SDF,\nwhich can also be regularized using the Eikonal constraint. Similar\nto NeRFs, implicit surface representations were also combined with\nlocal feature fields and auxiliary acceleration data structures [Li et al.\n2023; Rosu and Behnke 2023; Tang et al. 2023; Wang et al. 2022a;\nZhao et al. 2022] with the goal of improved efficiency and represen-\ntation capacity. While our method is built on the NeuS [Wang et al.\n2021] formulation, our main goal is not to improve the accuracy\nof the extracted surface. Instead, we utilize the SDF to extract a\nnarrow shell that allows us to adapt the representation to the local\ncomplexity of the scene and in turn to accelerate rendering.\nAccelerating neural volume rendering. One of the main limitations\nof NeRFs is the computational complexity of neural volume ren-\ndering which slows down both training and inference. Recently,\nvarious different directions to accelerate NeRFs have been explored.\nFor example, replacing a global MLP with a (sparse) local feature\nfield combined with a shallow MLP [Chen et al. 2022; Liu et al. 2020;\nM\u00fcller et al. 2022; Sun et al. 2022; Yu et al. 2021] or the spherical\nharmonics embedding [Chen et al. 2022; Karnewar et al. 2022; Sara\nFridovich-Keil and Alex Yu et al. 2022], partitioning the volume\ninto a large number of local (shallow) MLPs [Rebain et al. 2020;\nReiser et al. 2021], or using efficient sampling strategies [Hu et al.\n2022; Kurz et al. 2022; Lin et al. 2022; Neff et al. 2021], or image-\nspace convolutions [Cao et al. 2022; Wan et al. 2023]. However, even\nthe most optimized volumetric representations [M\u00fcller et al. 2022]\n4\n\u2022\nWang, Shen, Nimier-David, et al.\nare still much slower than pure surface-based approaches such as\nNvDiffRec [Munkberg et al. 2022].\nTo further increase the efficiency of the inference phase, volu-\nmetric representations can be baked onto a proxy surface geome-\ntry [Chen et al. 2023; Wan et al. 2023; Yariv et al. 2023] that can be\nefficiently rendered using high-performance rasterization pipelines.\nAn alternative \"baking\" strategy is to precompute the outputs of\nthe neural network and store them on a (sparse) discrete grid that\nacts as a lookup during inference [Hedman et al. 2021; Reiser et al.\n2023]. In this work, we investigate an alternate approach to speeding\nup the (volumetric) rendering, by adapting the number of samples\nrequired to render each pixel to the underlying local complexity\nof the scene. Note that our formulation is complementary to the\n\"baking\" approaches and we consider the combination of both an\ninteresting avenue for future research.\n3\nMETHOD\nOur method (see Figure 3) builds on NeRF [Mildenhall et al. 2020]\nand NeuS [Wang et al. 2021]. Specifically, we generalize NeuS [Wang\net al. 2021] with a new spatially-varying kernel (Section 3.2), which\nimproves the quality and guides the extraction of a narrow-band\nshell (Section 3.3). Then, the neural representation is fine-tuned\n(Section 3.5) within the shell that significantly accelerates rendering\n(Section 3.4).\n3.1\nPreliminaries\nNeRF [Mildenhall et al. 2020] represents a scene as a volumetric\nradiance field that maps a 3D point x \u2208 R3 and a viewing direction\nd \u2208 R3 to the volume density \ud835\udf0e and the emitted view-dependent\ncolor c \u2208 R3. This volumetric field is represented by a neural net-\nwork NN\ud835\udf03 (\u00b7) with parameters \ud835\udf03, such that (c, \ud835\udf0e) = NN\ud835\udf03 (x, d). The\nscene can then be rendered along a ray r = o + \ud835\udf0fd with origin\no \u2208 R3 and direction d \u2208 R3 from \ud835\udf0f\ud835\udc5b to \ud835\udf0f\ud835\udc53 via standard volumetric\nrendering\nc(r) =\n\u222b \ud835\udf0f\ud835\udc53\n\ud835\udf0f\ud835\udc5b\nexp\nh \u222b \ud835\udf0f\n\ud835\udf0f\ud835\udc5b\n\u2212\ud835\udf0e(r(\ud835\udc67))\ud835\udc51\ud835\udc67\ni\n\ud835\udf0e(r(\ud835\udf0f))c(r(\ud835\udf0f), d)\ud835\udc51\ud835\udf0f ,\n(1)\nwhich is approximated by numerical integration\nc(r) =\n\ud835\udc41\ud835\udc5f\n\u2211\ufe01\n\ud835\udc56=1\nexp\nh\n\u2212\n\ud835\udc56\u22121\n\u2211\ufe01\n\ud835\udc57=1\n\ud835\udf0e\ud835\udc57\ud835\udeff\ud835\udc57\ni\n(1 \u2212 exp(\u2212\ud835\udf0e\ud835\udc56\ud835\udeff\ud835\udc56))c(r, d)\ud835\udc56 ,\n(2)\nwhere \ud835\udc41\ud835\udc5f denotes the number of samples along the ray r and \ud835\udeff\ud835\udc56 is\nthe distance between two adjacent samples.\nTo improve geometric surface quality in NeRF-like scene recon-\nstructions, NeuS [Wang et al. 2021] and VolSDF [Yariv et al. 2021]\npropose to replace the learned density \ud835\udf0e by a learned signed distance\nfield \ud835\udc53 , and then transform \ud835\udc53 to \ud835\udf0e for rendering via a sigmoid-shaped\nmap. The formulation of NeuS optimizes an SDF (c, \ud835\udc53 ) = NN\ud835\udf03 (x, d)\nalong with a global kernel size \ud835\udc60 that controls the sharpness of the\nimplied density. To evaluate volume rendering (Equation 2) the SDF\nvalue \ud835\udc53 at x is transformed to a density \ud835\udf0e by\n\ud835\udf0e = max\n \n\u2212\n\ud835\udc51\u03a6\ud835\udc60\n\ud835\udc51\ud835\udf0f (\ud835\udc53 )\n\u03a6\ud835\udc60 (\ud835\udc53 ) , 0\n!\n,\n\u03a6\ud835\udc60 (\ud835\udc53 ) = (1 + exp(\u2212\ud835\udc53 /\ud835\udc60))\u22121,\n(3)\ndilation\nerosion\nextracted\nshell\nlevelset\nFig. 4. After fitting an initial SDF and spatially varying kernel, we apply level\nset flows to extract an adaptive shell via dilation and erosion. For illustrative\npurposes, the adaptive shell is enlarged; in practice it very tightly encloses\nsharp surfaces.\n\u22120.02\n\u22120.01\n0.00\n0.01\n0.02\nf\nd\u03c6s/d\u03c4\ns =\n1\n100\ns =\n1\n300\ns =\n1\n1000\nwhere \ud835\udc53 is implicitly \ud835\udc53 (r(\ud835\udf0f))\nalong a ray. Intuitively, a small\n\ud835\udc60 results in a wide kernel with a\nfuzzy density, while in the limit\nlim\ud835\udc60\u21920 \ud835\udc51\u03a6\ud835\udc60/\ud835\udc51\ud835\udf0f approximates a\nsharp impulse function (see in-\nset). This SDF-based formulation\nallows for the use of an Eikonal\nregularizer during training, which encourages the learned \ud835\udc53 to be\nan actual distance function, resulting in a more accurate surface\nreconstruction. The relevant losses are discussed in Section 3.5.\n3.2\nSpatially-Varying Kernel Size\nThe NeuS SDF formulation is highly effective, yet, it relies on one\nglobal kernel size. In combination with the Eikonal regularization\nthis implies a constant spread of the volume density across the whole\nscene. However, a one-size-fits-all approach does not adapt well to\nscenes that contain a mixture of \u201csharp\u201d surfaces (e.g. furniture or\ncars) and \u201cfuzzy\u201d volumetric regions (e.g. hair or grass).\nOur first contribution is to augment the NeuS formulation with\na spatially-varying, locally learned kernel size \ud835\udc60 as an additional\nneural output that is dependent on the input 3D position x. The\nextended network becomes (c, \ud835\udc53 ,\ud835\udc60) = NN\ud835\udf03 (x, d) (see the imple-\nmentation details in Section 4.1). During training, we additionally\ninclude a regularizer that promotes the smoothness of the kernel\nsize field (Section 3.5). This neural field can still be fit from only\ncolor image supervision, and the resulting spatially-varying ker-\nnel size automatically adapts to the sharpness of the scene content\n(Figure 7). This enhanced representation is independently valuable,\nimproving reconstruction quality in difficult scenes, but importantly\nit will serve to guide our explicit shell extraction in Section 3.3,\nwhich greatly accelerates rendering.\n3.3\nExtracting an Explicit Shell\nThe adaptive shell delimits the region of space which contributes\nsignificantly to the rendered appearance, and is represented by\ntwo explicit triangle meshes. When \ud835\udc60 is large the shell is thick,\ncorresponding to volumetric scene content, and when \ud835\udc60 is small\nthe shell is thin, corresponding to surfaces. After the implicit fields\nAdaptive Shells for Efficient Neural Radiance Field Rendering\n\u2022\n5\n\ud835\udc60 and \ud835\udc53 have been fit as described in Section 3.2, we extract this\nadaptive shell once as a post-process.\nIn Equation 3 the magnitude of the quantity \ud835\udc53 /\ud835\udc60 in the sigmoid\nexponent determines the rendering contribution along a ray (see\nthe inset figure in Section 3.1). It is tempting to simply extract\na band where |\ud835\udc53 /\ud835\udc60| < \ud835\udf02 for some \ud835\udf02 as the region that makes a\nsignificant contribution to the rendering. However, the learned\nfunctions quickly become noisy away from the \ud835\udc53 = 0 level set, and\ncannot be sufficiently regularized without destroying fine details.\nOur solution is to separately extract an inner boundary as an erosion\nof the \ud835\udc53 = 0 level set, and an outer boundary as its dilation (Figure 4),\nboth implemented via a regularized, constrained level set evolution\ntailored to the task.\nIn detail, we first sample the fields \ud835\udc53 and \ud835\udc60 at the vertices of a\nregular grid. We then apply a level set evolution to \ud835\udc53 , producing\na new eroded field SDF\u2212, and extract the SDF\u2212 = 0 level set as\nthe inner shell boundary via marching cubes. A separate, similar\nevolution yields the dilated field SDF+, and the SDF+ = 0 level set\nforms the outer shell boundary. We define both level sets separately:\nthe dilated outer surface should be smooth to avoid visible boundary\nartifacts, while the eroded inner surface needs not be smooth, but\nmust only exclude regions which certainly do not contribute to the\nrendered appearance.\nRecall that a basic level set evolution of a field \ud835\udc4e is given by\n\ud835\udf15\ud835\udc4e/\ud835\udf15\ud835\udc61 = \u2212 |\u2207\ud835\udc4e| \ud835\udc63, where \ud835\udc63 is the desired scalar outward-normal\nvelocity of the level set. Our constrained, regularized flow on \ud835\udc53 is\nthen\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc61 = \u2212 |\u2207\ud835\udc53 |\n\u0012\n\ud835\udc63(\ud835\udc530,\ud835\udc60) + \ud835\udf06curv\u2207 \u00b7 \u2207\ud835\udc53\n|\u2207\ud835\udc53 |\n\u0013\n\ud835\udf14(\ud835\udc53 ),\n(4)\nwhere \ud835\udc530 here denotes the initial learned\nSDF, the divergence term is a curvature\nsmoothness regularizer with weight \ud835\udf06curv.\nThe soft falloff \ud835\udf14 (see inset) limits the flow\nto a window around the level set:\n\ud835\udf14(\ud835\udc53 ) = 1\n2\n\u00001 + cos(\ud835\udf0b clamp(\ud835\udc53 /\ud835\udf01, \u22121., 1.)\u0001,\n(5)\nwith window width \ud835\udf01. To dilate the level set, the velocity is chosen\nto fill all regions with density \ud835\udf0e > \ud835\udf0emin for a ray incoming in the\nnormal direction\n\ud835\udc63dilate(\ud835\udc530,\ud835\udc60) =\n(\n\ud835\udefd\ud835\udc51\ud835\udf0e(\ud835\udc530,\ud835\udc60)\n\ud835\udf0e(\ud835\udc530,\ud835\udc60) > \ud835\udf0emin\n0\n\ud835\udf0e(\ud835\udc530,\ud835\udc60) \u2264 \ud835\udf0emin\n,\n(6)\nwith \ud835\udefd\ud835\udc51 as a scaling coefficient. We use \ud835\udf01 = 0.1, and \ud835\udf06curv = 0.01. To\nerode the level set, the velocity is inversely-proportional to density,\nso that the shell expands inward quickly for low density regions\nand slowly for high density regions\n\ud835\udc63erode(\ud835\udc530,\ud835\udc60) = min \u0000\ud835\udc63max, \ud835\udefd\ud835\udc52\n1\n\ud835\udf0e(\ud835\udc530,\ud835\udc60)\n\u0001,\n(7)\nwhere here we use \ud835\udf01 = 0.05, and \ud835\udf06curv = 0. These velocities lead to\na short-distance flow, and thus a narrow shell where \ud835\udc60 is small and\nthe content is surface-like. They lead to a long-distance flow and\nhence a wide shell where \ud835\udc60 is large and the content is volume-like.\nWe compute the dilated field SDF+ and eroded field SDF\u2212 re-\nspectively by forward-Euler integrating this flow on the grid for\ndilated\nSDF\ntarget scene\neroded\nSDF\nrender\nshell\nlearned\nkernel size\nlearned\nSDF\nFig. 5. Given kernel size \ud835\udc60 and SDF \ud835\udc53 learned from Section 3.2 (top), we\napply a erosion and dilation flows to \ud835\udc53 (bottom middle and left) to extract a\nnarrow shell in which we efficiently render (bottom right). Here, we visualize\neach quantity on a 2D slice through a scene. For clarity, we show the fields\nonly nearby the adaptive shell that is ultimately rendered.\n50 steps of integration, computing derivatives via spatial finite dif-\nferences. We do not find numerical redistancing to be necessary.\nFinally, we clamp the results SDF\u2212 \u2190 max(\ud835\udc530, SDF\u2212) and SDF+ \u2190\nmin(\ud835\udc530, SDF+), to ensure that the eroded field only shrinks the level\nset, and the dilated flow only grows the level set. The SDF+ = 0\nand SDF\u2212 = 0 level sets are extracted via marching cubes as the\nouter and inner shell boundary meshes M+ and M\u2212, respectively.\nFigure 5 visualizes the resulting fields. Further details are provided\nin Procedure 1 and 2 of the Appendix.\n3.4\nNarrow-Band Rendering\nThe extracted adaptive shell serves as an auxiliary acceleration\ndata structure to guide the sampling of points along a ray (Equa-\ntion 2), enabling us to efficiently skip empty space and sample points\nonly where necessary for high perceptual quality. For each ray, we\nuse hardware-accelerated ray tracing to efficiently enumerate the\nordered intervals defined by the intersection of the ray and the\ngrazing,\nmult. samples\nnarrow hit,\n1 sample\n1 interval,\nmult. samples\nmiss,\n0 samples\nin general,\nmult. intervals, mult. samples\nadaptive shell. Within each inter-\nval we query equally-spaced sam-\nples. Our renderer does not re-\nquire any dynamic adaptive sam-\npling or sample-dependent termina-\ntion criteria, which facilitates high-\nperformance parallel evaluation.\nIn detail, we first build ray tracing\nacceleration data structures for both\nthe outer mesh M+ and inner mesh\nM\u2212 We then cast each ray against the\nmeshes, yielding a series of intersec-\ntions where the ray enters or exits\nthe mesh, partitioning the ray into\nzero or more intervals contained in\nthe shell (see inset). For each interval\n6\n\u2022\nWang, Shen, Nimier-David, et al.\nMobileNerf\nours\nours, # samples\nground truth\nFig. 6. Pure surface-based representations struggle to represent fuzzy sur-\nfaces such as the tail. On the other hand, our method adapts the narrow\nshell to the local complexity of the scene, using a single sample\nfor the\nsharp skin surface and up to 16 samples\nfor the tail.\nwith width \ud835\udc64, a target inter-sample spacing \ud835\udeff\ud835\udc60, and a single-sample\nthreshold \ud835\udc64s, we compute the number of samples as ceil(max(\ud835\udc64 \u2212\n\ud835\udc64s, 0)/\ud835\udeff\ud835\udc60) + 1. We cap the maximum number of samples to \ud835\udc41max,\nand equidistantly sample the interval. Note that if the interval has\n\ud835\udc64 < \ud835\udc64s, a single sample is taken at the center of the interval. When\nan interval ends because the ray hits the inner mesh M\u2212, we do not\nprocess any subsequent samples, as this represents the interior of a\nsolid object. Otherwise, we process intervals until the ray exits the\nscene or we hit a maximum cap, accumulating the contributions as\nin Equation 2.\nNote that this procedure can be implemented by first generating\nall samples within all intervals, and then performing a single batched\nMLP inference pass, which improves throughput. For surfaces, our\nnarrow-band sampling often amounts to just a hardware-accelerated\nray tracing, followed by a single network evaluation, while for\nfuzzy regions it densely samples only where necessary\u2014in either\ncase, performance is greatly accelerated (Table 1). More algorithmic\ndetails are included in Procedure 3 of the Appendix.\n3.5\nLosses and Training\nWe optimize the parameters of our representation in two stages. In\nthe first stage, we use the fully volumetric formulation described in\nSections 3.1 and 3.2 and minimize the following objective\nL = Lc + \ud835\udf06\ud835\udc52L\ud835\udc52 + \ud835\udf06\ud835\udc60L\ud835\udc60 + \ud835\udf06nLn\n(8)\nwith the weights \ud835\udf06c = 1, \ud835\udf06\ud835\udc52 = 0.1, \ud835\udf06n = 0.1, and \ud835\udf06\ud835\udc60 = 0.01 for all\nexperiments. Here Lc is the standard pixel-wise color loss against\ncalibrated ground-truth images\nLc =\n1\n|R|\n\u2211\ufe01\nr\u2208R\n|c(r) \u2212 cgt(r)|\n(9)\nand L\ud835\udc52 is the Eikonal regularizer as in [Wang et al. 2021]\nL\ud835\udc52 =\n1\n|X|\n\u2211\ufe01\nx\u2208X\n(||\u2207\ud835\udc53 (x)||2 \u2212 1)2 ,\n(10)\nwhere R and X denote the set of rays and samples along the rays,\nrespectively. \u2207\ud835\udc53 (x) can be obtained either analytically [Wang et al.\n2021, 2022a; Yariv et al. 2021] or through finite differences [Li et al.\nours\nNeuS\nkernel size\nrendering\nsamples\nFig. 7. The original NeuS [Wang et al. 2021] formulation uses a single global\nkernel size \ud835\udc60. On complex scenes with varying content, the global \ud835\udc60 value\nconverges to an average which is too small for volumetric parts\n, and too\nlarge for sharp surfaces\n. Instead, our locally varying kernel size adapts to\nthe scene, in-turn allowing us to reduce the number of samples to a single\nsample\nfor sharp surfaces and up to 32 samples\nfor the fern (top right).\nNeuS uses a constant 384 samples per pixel (bottom right).\n2023; Wang et al. 2023]. We use the latter approach. The loss L\ud835\udc60\nregularizes the spatially varying kernel size introduced in our for-\nmulation for smoothness\nL\ud835\udc60 = 1\nX\n\u2211\ufe01\nx\u2208X\n|| log\n\u0002\n\ud835\udc60(x)\n\u0003\n\u2212 log\n\u0002\n\ud835\udc60(x + N (0,\ud835\udf002))\n\u0003\n||2,\n(11)\nwhere N (0,\ud835\udf002) denotes samples from the normal distribution with\nstandard deviation \ud835\udf00. Lastly, we incorporate the loss\nLn =\n1\n|X|\n\f\f\f\fn(x) \u2212\n\u2207\ud835\udc53 (x)\n||\u2207\ud835\udc53 (x)||2\n\f\f\f\f2,\n(12)\ninternal to our network architecture (Section 4.1). Like NeuS, we\nwill leverage geometric normals as an input to a shading subnet-\nwork, but we find that predicting these normals internally improves\ninference performance vs. gradient evaluation. Ln serves to train\nthese predicted normals to remain approximately faithful to ones\nobtained through the finite differences of the underlying SDF field\n\u2207\ud835\udc53 (x) [Li et al. 2023; Wang et al. 2023].\nAfter the implicit field has been fit, we extract the adaptive shell\nas in Section 3.3. While initial training requires dense sampling\nalong rays, our explicit shell now allows narrow-band rendering\nto concentrate samples only in significant regions. We therefore\nfine-tune the representation within the narrow band, now with\nonly Lc\u2014it is no longer necessary to encourage a geometrically-\nnice representation, as we have already extracted the shell and\nrestricted the sampling to a small band around the scene content.\nDisabling regularization enables the network to devote its whole\ncapacity to fit the visual appearance, which leads to improved visual\nfidelity (Table 4). In Procedure 4 of the Appendix, we also present\nthe training pipeline with algorithm details.\n4\nEXPERIMENTS\nIn this section, we provide low-level details of our implementation\nand evaluate our method in terms of rendering quality and efficiency\nAdaptive Shells for Efficient Neural Radiance Field Rendering\n\u2022\n7\non four data sets that range from synthetic object-level Shelly, NeRF-\nSynthetic [Mildenhall et al. 2020] and tabletop DTU [Jensen et al.\n2014] data, to real-world large outbound scenes MipNeRF360 [Bar-\nron et al. 2022]. For comparisons, we treat Instant NGP [M\u00fcller et al.\n2022] as our volumetric baseline, due to its balance between high\nfidelity and efficiency. In addition, we compare to prior methods that\nwere optimized either for fidelity [Barron et al. 2021, 2022; Wang\net al. 2021; Yuan et al. 2022] or rendering efficiency [Chen et al. 2023;\nGuo et al. 2023; Yariv et al. 2023].\nWhen running NeRF [Mildenhall et al. 2020] and Mip-NeRF [Bar-\nron et al. 2021] on DTU and Shelly, we use the implementation from\nNerfstudio [Tancik et al. 2023]. For other methods and experiment\nsettings, we use their official implementations.\n4.1\nArchitecture Details\nFollowing the state of the art in neural volumetric rendering, we rep-\nresent our neural field as a combination of a feature field and a small\n(decoder) neural network. Specifically, we use a multi-resolution\nhash encoding [M\u00fcller et al. 2022] \u03a8(\u00b7) with 14 levels, where each\nlevel is represented by a hash-table with 222 two-dimensional fea-\ntures. The voxel grid resolution of our feature field grows from\n163 \u2192 40963 for Shelly and NeRFSynthetic, and from 163 \u2192 81923\nfor the other data sets. The features at each level are obtained\nthrough tri-linear interpolation before being concatenated to form\nthe feature embedding \u03a8(\u00b7) \u2208 R28. This is further concatenated\nwith the sample coordinates and input to the geometry network\n(\ud835\udc53 , 1/\ud835\udc60, fgeo, n) = NNgeo\n\ud835\udf03\n([\u03a8(x), x]) which is an MLP with a single\nhidden layer and 31 \u2192 64 \u2192 31 dimensions. Apart from the SDF\nvalue \ud835\udc53 and kernel size \ud835\udc60, NNgeo\n\ud835\udf03\nalso outputs a geometry latent fea-\nture fgeo \u2208 R26 and the normal vector n \u2208 R3 which are combined\nwith x and the encoded view direction d as input to the radiance\nnetwork c = NNrad\n\ud835\udf03\n([\ud835\udefe(d), fgeo, n, x]) that predicts the emitted color.\nHere, NNrad\n\ud835\udf03\nis an MLP with two hidden layers and dimensions\n48 \u2192 64 \u2192 64 \u2192 3. To reduce the computational cost, we directly\npredict the normal vector n with an MLP rather than computing it\nas the gradient of the underlying SDF field as done in NeuS [Wang\net al. 2021]. Finally, to encode the input direction d, we use the\nspherical harmonic basis up to degree 4, such that \ud835\udefe(d) \u2208 R16. The\ndimensions of all layers in both networks and the feature field were\nselected for high throughput on modern GPU devices.\n4.2\nImplementation\nThe training stage of our method is implemented in PyTorch [Paszke\net al. 2017], while the inference stage is implemented in Dr.Jit [Jakob\net al. 2022]. To achieve real-time inference rates, we rely on the auto-\nmatic kernel fusion performed by Dr.Jit as well as GPU-accelerated\nray-mesh intersection provided by OptiX [Parker et al. 2010]. While\nthe inference pass is implemented with high-level Python code, the\nasynchronous execution of large fused kernels hides virtually all\nof the interpreter\u2019s overhead. Combined with the algorithmic im-\nprovements described above, we achieve frame rates from 40 fps\n(25 ms/frame) on complex outdoor scenes to 300 fps (3.33 ms/frame)\non object-level scenes, at 1080p resolution on a single RTX 4090\nGPU. A performance comparison to Instant NGP [M\u00fcller et al. 2022]\non four data sets is given in Table 1. Note that in this work, we\nfocused on inference performance only, and have not yet applied\nthese performance optimizations to the training procedure. Detailed\npseudo-code is given in Procedures 1, 2, 3 and 4 of the Appendix.\n4.3\nEvaluation Metrics\nIn order to evaluate the rendering quality, we report the commonly\nused peak signal-to-noise ratio (PSNR), learned perceptual image\npatch similarity (LPIPS), and structural similarity (SSIM) metrics. Un-\nfortunately, evaluating the efficiency of the methods is less straight-\nforward as the complexity of the method is often intertwined with\nthe selected hardware and low-level implementation details. Indeed,\nreporting only the number of frames-per-second (FPS) or the time\nneeded to render a single frame may paint an incomplete picture. We\ntherefore additionally report the number of samples along the ray\nthat are required to render each pixel. While the number of samples\nalong the ray also does not tell the whole story as the per-sample\nevaluation can have different computational complexity, combin-\ning all metrics provides a good assessment of the computational\ncomplexity of the individual methods.\n4.4\nShelly Data Set\nThe NeRFSynthetic data set that was introduced in [Mildenhall et al.\n2020] is still one of the most widely used data sets to evaluate\nnovel-view synthesis methods. Yet, it mainly consists of objects\nwith sharp surfaces that can be well-represented by surface render-\ning methods [Munkberg et al. 2022], and thus does not represent\nthe challenge of general scene reconstruction. This motivated us to\nintroduce a new synthetic data set, which we name Shelly. It covers\na wider variety of appearance including fuzzy surfaces such as hair,\nfur, and foliage. Shelly contains six object-level scenes: Khady, Pug,\nKitty, Horse, Fernvase and Woolly. For each scene, we have ren-\ndered 128 training and 32 test views from random camera positions\ndistributed on a sphere with a fixed radius. We are grateful to the\noriginal artists of these models: jhon maycon, Pierre-Louis Baril,\nabdoubouam, ckat609, the BlenderKit team, and textures.xyz.\nTable 2 shows quantitative results, while the novel views are\nqualitatively compared in Figure 8. Our method significantly out-\nperforms prior methods across all quality metrics achieving more\nthan 2dB higher PSNR than Instant NGP. Figure 8 demonstrates\nthat surface-based rendering methods (MobileNerf) struggle to rep-\nresent fuzzy surfaces. On the other hand, our method aligns its\nrepresentation to the complexity of the scene. For example, Figure 6\nshows that our method represents the skin of the horse as a sharp\nsurface, while using a wider kernel for its tail, which benefits from\nvolumetric rendering.\n4.5\nDTU Data Set\nWe consider 15 tabletop scenes from the DTU data set [Jensen\net al. 2014]. These scenes were captured by a robot-held monocular\nRGB camera, and are commonly used to evaluate implicit surface\nrepresentations. We follow prior work [Wang et al. 2021; Yariv et al.\n2021] and task the methods to represent the full scene, but evaluate\nthe performance only within the provided object masks.\nTable 2 depicts that our method outperforms all baselines across\nall evaluation metrics. Qualitative results are provided in Figure 9.\n8\n\u2022\nWang, Shen, Nimier-David, et al.\nGround Truth\nInstantNGP\nMobileNerf\nOurs\nGround Truth\nInstantNGP\nMobileNerf\nOurs\nFig. 8. A gallery of results on the test-views of our Shelly data set.\nAdaptive Shells for Efficient Neural Radiance Field Rendering\n\u2022\n9\nTable 1. Performance comparisons on all four data sets, measured at 1080p without GUI overhead using an RTX 4090 GPU. Our adaptive sample placement and\nmesh-based empty-space skipping technique allows us to outperform Instant NGP without compromising visual fidelity. Note that Instant NGP\u2019s performance\non the DTU data set was hindered by a large number of background samples, and is therefore not necessarily indicative of a real use case: the user may specify a\ntighter scene bounding box to focus the samples on the main scene contents.\nOurs\nInstant NGP [M\u00fcller et al. 2022]\nSample count \u2193 ms/frame \u2193 FPS \u2191 Sample count \u2193 ms/frame \u2193 FPS \u2191\nShelly\n2.07\n3.81\n262.69\n2.89\n11.74\n85.16\nDTU\n5.11\n6.37\n157.00\n56.10\n123.31\n8.10\nNeRFSynthetic\n1.98\n3.56\n280.68\n3.19\n14.20\n70.40\nMipNeRF360\n17.05\n27.64\n36.18\n45.62\n93.57\n10.69\nTable 2. Quantitative results on Shelly data set, DTU data set and NeRFSynthetic data set. We report PSNR, LPIPS and SSIM. Our method achieves better\nresults across all metrics on Shelly and DTU and comparable results on NeRFSynthetic. Real-time denotes methods that achieve >30FPS at 1080p. On Shelly\nand DTU, we run NeRF and Mip-NeRF with Nerfstudio [Tancik et al. 2023], and use official implementation for other methods. Baselines of NeRFSynthetic are\nfrom the original papers. Detailed results for each object/scene are provided in the Supplement.\nShelly\nDTU\nNeRFSynthetic\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\noffline\nNeRF [Mildenhall et al. 2020]\n31.27\n0.893\n0.157\n28.51\n0.894\n0.183\n31.01\n0.947\n0.081\nNeuS [Wang et al. 2021]\n29.98\n0.893\n0.158\n28.92\n0.913\n0.168\n/\n/\n/\nMip-NeRF [Barron et al. 2021]\n32.59\n0.899\n0.148\n28.90\n0.900\n0.179\n33.09\n0.961\n0.043\nOurs (full ray)\n34.26\n0.932\n0.104\n33.51\n0.901\n0.081\n32.51\n0.962\n0.048\nreal-time\nI-NGP [M\u00fcller et al. 2022]\n33.22\n0.922\n0.125\n31.37\n0.932\n0.139\n33.18\n/\n/\nMobileNeRF [Chen et al. 2023]\n31.62\n0.911\n0.129\n/\n/\n/\n30.90\n0.947\n0.062\nVMesh [Guo et al. 2023]\n/\n/\n/\n/\n/\n/\n30.70\n0.947\n0.060\nOurs\n36.02\n0.954\n0.079\n33.37\n0.964\n0.077\n31.84\n0.957\n0.056\nDifferent from the Shelly data set, the performance of Ours on the\nDTU data set is slightly lower than that of Ours (full ray) in terms of\nPSNR. We hypothesize that this is due to the distribution of the cam-\nera poses that observe the scene only from a single direction. This\nhinders constraining the neural field and hence also the adaptive\nshell extraction. The same reason also contributes to a significant\nincrease in the sample count for Instant NGP (see Table 1).\n4.6\nNeRFSynthetic Data Set\nThe NeRFSynthetic data set introduced in [Mildenhall et al. 2020] con-\ntains 8 synthetic objects rendered in Blender and is widely adopted\nto evaluate the quality of novel view synthesis methods.\nAs shown in Table 2, our method can achieve comparable quality\nto the state of the art methods Mip-NeRF [Barron et al. 2021] and\nI-NGP [M\u00fcller et al. 2022], but with a much faster runtime perfor-\nmance (Table 1). Our method also achieves better image quality\ncompared to recent works optimized for rendering efficiency [Chen\net al. 2023; Guo et al. 2023]1.\n4.7\nMipNeRF360 Data Set\nThe MipNeRF-360 data set [Barron et al. 2022] is a challenging\nreal-world data set that contains large indoor and outdoor scenes\ncaptured from 360\u25e6 camera views2. The scenes feature a complex\n1Note that VMesh [Guo et al. 2023] also optimizes for disk storage, which is an orthog-\nonal direction to this paper.\n2In our evaluation, we exclude the two scenes with license issues: Flowers and Treehill.\ncentral object accompanied by a highly detailed background. To\nbetter represent the background, we follow [Yariv et al. 2023] and\nextend our method with the scene contraction proposed in [Barron\net al. 2022] (more details are provided in the supplemental docu-\nment).\nTable 3 provides the quantitative results and the qualitative com-\nparison is depicted in Figure 10. Our method achieves comparable\nperformance to other interactive methods. Directly compared to\nI-NGP, our proposed narrow-band formulation can reduce the num-\nber of samples by a factor of three, resulting in fivetimes higher\naverage frame rates at comparable rendering quality. We note that\non this data set, performance and quality depend significantly on\nthe background, which our approach is not specialized to handle.\n4.8\nPerformance Evaluation\nWe compare the performance of our method to the most efficient\nvolumetric baseline, Instant NGP [M\u00fcller et al. 2022], in Table 1. To\nensure a fair comparison, we render the same test views for both\nmethods at 1080p resolution and remove the GUI overhead. The\ncomparison was run on a single RTX 4090 GPU. Our narrow-band\nrendering formulation can efficiently reduce the number of samples\nalong the ray (up to 10 times) which results in significantly reduced\ninference time per frame. On the challenging outbound 360 scenes,\nour method already runs at real-time rates. Yet, additional speed-\nups could be achieved by further studying the interaction of our\nadaptive sample placement with the spatial remapping employed in\nthese scenes.\n10\n\u2022\nWang, Shen, Nimier-David, et al.\nGround Truth\nInstantNGP\nMipNerf\nOurs\nGround Truth\nInstantNGP\nMipNerf\nOurs\nFig. 9. A gallery of results on the DTU data set.\nTable 3. Quantitative results on the MipNeRF360 data set. We report the\nPSNR, LPIPS and SSIM results for each object and compare them to baselines.\nOur method achieves a performance comparable to the baselines while being\nsignificantly faster during inference (see Table 1). In our comparison, we\nexclude the two scenes with license issues: Flowers, Treehill.\nOutdoor scenes\nIndoor scenes\nPSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193\noffline\nNeRF [Mildenhall et al. 2020]\n22.20\n0.485\n0.501\n26.84\n0.790\n0.370\nMip-NeRF [Barron et al. 2021]\n22.02\n0.505\n0.484\n26.98\n0.798\n0.361\nMip-NeRF 360 [Barron et al. 2022] 25.92\n0.747\n0.244\n31.72\n0.917\n0.179\nOurs (full ray)\n24.30\n0.703\n0.316\n29.04\n0.900\n0.239\ninteractive\nI-NGP [M\u00fcller et al. 2022]\n23.90\n0.648\n0.369\n29.47\n0.877\n0.273\nMobileNeRF [Chen et al. 2023]\n22.90\n0.524\n0.463\n25.74\n0.757\n0.453\nBakedSDF [Yariv et al. 2023]\n23.40\n0.577\n0.351\n27.20\n0.845\n0.300\nOurs\n23.17\n0.606\n0.389\n29.19\n0.872\n0.285\n4.9\nAblation Study\nWe ablate our design choices on the Shelly data set in Table 4. In\nline with our motivation in Section 3.2, the spatially-varying kernel\nsize provides the required flexibility to adapt to the local complex-\nity of the scene which results in improvement across all metrics.\nUsing a fixed SDF threshold to extract the band requires seeking a\ncompromise between an adaptive shell that is too narrow to repre-\nsent fuzzy surfaces (threshold 0.02) or an increased sample count\n(threshold 0.05). Instead, our formulation can automatically adapt to\nTable 4. Ablating our method on the Shelly data set. SV Kernel denotes the\nspatially varying kernel as introduced in Section 3.2. Band, fixed denotes\nthe shell is not adaptive but extracted for a given SDF threshold.\nModel\nPSNR\n\u2191\nLPIPS \u2193\nSSIM \u2191\nSample \u2193\nOurs (full ray, w/o SV kernel)\n32.99\n0.115\n0.921\n384\nOurs (full ray)\n34.26\n0.104\n0.932\n384\nOurs (band, fixed \u00b10.05)\n33.83\n0.110\n0.928\n4.51\nOurs (band, fixed \u00b10.02)\n31.14\n0.136\n0.913\n2.29\nOurs (keep regularization)\n34.22\n0.085\n0.948\n1.74\nOurs\n36.02\n0.079\n0.954\n1.74\nthe local complexity of the scene leading to higher quality metrics\nand lower sample count. As described in Section 3.5, we disable the\nregularization terms after shell extraction to devote more capacity\nto fit the visual appearance. Comparing Ours (keep regularization)\nwith Ours, this leads to improved visual fidelity.\nIn Figure 11, we ablate our method and study how image quality\nand runtime change with different sample counts. We vary the sam-\nple step size \ud835\udeff\ud835\udc60 in narrow-band rendering (Section 3.4) to produce\nvarying sample counts, and keep other hyperparameters unchanged.\nThe PSNR is sensitive to sample counts when the samples are in-\nsufficient (0.25\u00d7-1\u00d7), and the image quality starts to saturate as the\nsample counts go higher (1\u00d7-4\u00d7). In most scenes, the runtime perfor-\nmance is approximately linear w.r.t. the sample count. For simpler\nAdaptive Shells for Efficient Neural Radiance Field Rendering\n\u2022\n11\nGround Truth\nInstantNGP\nMobileNerf\nBakedSDF\nOurs\nGround Truth\nOurs (full view)\nGround Truth\nOurs (full view)\nGround Truth\nOurs (full view)\nGround Truth\nInstantNGP\nMobileNerf\nBakedSDF\nOurs\nFig. 10. A gallery of results on the test-views of the MipNerf360 data set.\nscenes such as Kitten and Fernvase, smaller sample counts (0.25\u00d7-\n1\u00d7) do not further reduce the runtime due to a mixture of fixed\noverheads (e.g. Python interpreter and Dr.Jit tracing) and under-\nutilization of the GPU.\n5\nAPPLICATIONS\nOur method directly constructs an explicit outer shell mesh M+\nwhich by definition contains all regions of space that contribute to\nthe rendered appearance. This property has great utility for use in\ndownstream applications.\n12\n\u2022\nWang, Shen, Nimier-David, et al.\n0.25\u00d7\n0.5\u00d7\n1\u00d7\n2\u00d7\n4\u00d7\nRelative sample count\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n+0.5\nPSNR change (dB)\nfernvase\nkhady\nkitten\nbicycle\ngarden\nroom\n0.25\u00d7\n0.5\u00d7\n1\u00d7\n2\u00d7\n4\u00d7\nRelative sample count\n0.25\u00d7\n0.5\u00d7\n1\u00d7\n2\u00d7\n4\u00d7\nRelative runtime\nfernvase\nkhady\nkitten\nbicycle\ngarden\nroom\nFig. 11. Ablating the effect of sample count on image quality and runtime\nperformance. We vary the sample count, and plot the PSNR change (left)\nand relative runtime performance (right) compared to the default hyper-\nparameters denoted as \u201c1\u00d7 sample count\u201d. We experiment with six scenes\nfrom the Shelly (fernvase, khady, kitten) and MipNeRF360 (bicycle, garden,\nroom) data sets.\nSo far our scenes have represented entirely static content, yet,\nthe world is full of motion. Cage-based deformation methods have\nshown promise for enabling dynamic, non-rigid motion in NeRF and\nother volumetric representations [Garbin et al. 2022; Joshi et al. 2007;\nLee et al. 2018; Xu and Harada 2022; Yuan et al. 2022]. The basic idea\nis to construct a coarse tetrahedral cage around a neural volume,\ndeform the cage, and use it to render the deformed appearance\nof the underlying volume. Our approach perfectly supports such\ntechniques, as the outer shell mesh M+ guides the construction of a\ncage which will surely contain the content.\nWe first dilate and tetrahedralize the outer mesh M+ with Fast-\nTetWild [Hu et al. 2020] to produce a tetrahedral mesh that encap-\nsulates the scene. This mesh acts as a proxy for performing physics\nsimulations, animations, editing, and other operations. To render\nour representation after deforming the tetrahedral cage, any defor-\nmation is transferred to M+ and M\u2212 via barycentric interpolation,\nusing precomputed barycentric coordinates generated as a prepro-\ncess. Ray directions are likewise transformed via finite differences.\nAfter the transformation, we proceed with rendering as usual in the\nreference space of our representation, as described in Section 3.4.\nNote that even in the presence of deformations, the rendering pro-\ncess still benefits from our efficient adaptive shell representation,\nand is able to efficiently sample the underlying neural volume.\nWe show two examples of applying physical simulation and an-\nimation to the reconstructed objects in Figure 12; please see the\nsupplemental video for dynamic motion. In the animation example,\nwe manually drive the motion of plants in a vase according to an an-\nalytical wind-like spatial function. Other animation schemes, such\nas blend shapes or character rigs could potentially be substituted to\ndrive the motion. In the physical simulation example, we simulate\nthe reconstructed asset via finite-element elastic simulation on the\ncage tetrahedra including collision penalties [Jatavallabhula et al.\n2021].\n6\nDISCUSSION\nRecent work has developed schemes to accelerate and improve the\nquality of NeRF-like scene representations. Section 4 provides com-\nparisons to selected, particularly relevant methods. Note that due\nto the high research activity in the field, it is impossible to compare\nFig. 12. Our representation is well-suited for animation (top) and physical\nsimulation (bottom). The visual quality is preserved under deformation: the\noriginal shape is shown in leftmost column, with deformations in the middle\nand rightmost column. For details, please zoom into the fuzzy regions (e.g.\nfur, leaves), and refer to the supplemental video.\nto all techniques and for many approaches implementations are not\navailable. Hence, we offer additional comments on some related\nwork:\n\u2022 MobileNeRF [Chen et al. 2023], BakedSDF [Yariv et al. 2023],\nNeRFMeshing [Rakotosaona et al. 2023], and nerf2mesh [Tang\net al. 2023] post-process NeRF-like models and extract meshes\nto accelerate inference, similar to this work. However, these ap-\nproaches constrain appearance to surfaces, sacrificing quality.\nOur method instead retains a full volumetric representation and\nnearly full-NeRF quality, at the cost of moderately more expensive\ninference (though still real-time on modern hardware).\n\u2022 DuplexRF [Wan et al. 2023] also extracts an explicit shell from\nthe underlying neural field and uses it to accelerate rendering,\nalthough it does so with a very different neural representation,\nprioritizing performance. Their shell is directly extracted from\ntwo thresholds of the radiance field, which requires the careful\nselection of the thresholds and results in a noisy shell that is not\nadapted to the local complexity of the scene in contrast to our\napproach.\n\u2022 VMesh [Guo et al. 2023] builds upon the similar insight that dif-\nferent parts of the scene require different treatment. However,\ntheir formulation assumes an additional voxel-grid data structure\nto mark the volumetric areas that contribute to the final render-\ning. This approach suffers from poor complexity scaling as with\nthe auxiliary acceleration data structure of [M\u00fcller et al. 2022].\nInstead, our method uses an explicit, adaptive shell to delimit the\nareas that contribute to the rendering. Apart from lower com-\nplexity, our formulation seamlessly enables further applications\nas discussed in Section 5.\n7\nCONCLUSION AND FUTURE WORK\nIn this work we focus on efficiently rendering NeRFs. Our first\nstage of training (Section 3.2) is largely similar to that of [Li et al.\n2023], and likely can be accelerated by algorithmic advancements\nAdaptive Shells for Efficient Neural Radiance Field Rendering\n\u2022\n13\nand low-level tuning similar to our inference pipeline [Wang et al.\n2022a].\nAlthough our method offers large speedups for high-fidelity neu-\nral rendering and runs at real-time rates on modern hardware (Ta-\nble 1, it is still significantly more expensive than approaches such as\nMeRF [Reiser et al. 2023] that precompute the neural field outputs\nand bake them onto a discrete grid representation. Our formulation\nis complimentary to that of MeRF [Reiser et al. 2023] and we hypoth-\nesize that combining both approaches will lead to further speedups,\npotentially reaching the performance\u2014at high quality\u2014of the meth-\nods that bake the volumetric representation to explicit meshes and\ncan run in real-time even on commodity hardware (e.g. [Chen et al.\n2023]).\nOur method does not guarantee to capture thin structures\u2014if the\nextracted adaptive shell omits a geometric region, it can never be\nrecovered during fine-tuning and will always be absent from the\nreconstruction. Artifacts of this form are visible in some MipNeRF360\nscenes. Future work will explore an iterative procedure, in which\nwe alternately tune our reconstruction and adapt the shell to ensure\nthat no significant geometry is missed. Other artifacts occasionally\npresent in our reconstructions include spurious floating geometry\nand poorly-resolved backgrounds; both are common challenges in\nneural reconstructions and our approach may borrow solutions\nfrom other work across the field (e.g. [Niemeyer et al. 2022]).\nMore broadly, there is great potential in combining recent neural\nrepresentations with high-performance techniques honed for real-\ntime performance in computer graphics. Here, we have shown how\nray tracing and adaptive shells can be used to greatly improve\nperformance.\nACKNOWLEDGMENTS\nThe authors are grateful to Alex Evans for the insightful brainstorm-\ning sessions and his explorations in the early stages of this project.\nWe also appreciate the feedback received from Jacob Munkberg,\nJon Hasselgren, Chen-Hsuan Lin, and Wenzheng Chen during the\nproject. Finally, we would like to thank Lior Yariv for providing the\nresults of BakedSDF.\nREFERENCES\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-\nBrualla, and Pratul P. Srinivasan. 2021. Mip-NeRF: A Multiscale Representation for\nAnti-Aliasing Neural Radiance Fields. ICCV (2021).\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.\n2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. CVPR (2022).\nSai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon, Shih-En Wei, Kevyn Mcphail,\nRavi Ramamoorthi, Yaser Sheikh, and Jason Saragih. 2021. Deep relightable appear-\nance models for animatable faces. ACM Transactions on Graphics (TOG) 40, 4 (2021),\n1\u201315.\nChris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen.\n2001. Unstructured Lumigraph Rendering. In Proceedings of the 28th Annual Confer-\nence on Computer Graphics and Interactive Techniques (SIGGRAPH \u201901). Association\nfor Computing Machinery, New York, NY, USA, 425\u2013432.\nJunli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, Denys\nMakoviichuk, Sergey Tulyakov, and Jian Ren. 2022. Real-Time Neural Light Field\non Mobile Devices. arXiv preprint arXiv:2212.08057 (2022).\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022. TensoRF:\nTensorial Radiance Fields. In European Conference on Computer Vision (ECCV).\nZhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. 2023.\nMobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural\nField Rendering on Mobile Architectures. In The Conference on Computer Vision and\nPattern Recognition (CVPR).\nAbe Davis, Marc Levoy, and Fredo Durand. 2012. Unstructured Light Fields. Comput.\nGraph. Forum 31, 2pt1 (2012), 305\u2013314.\nPaul E. Debevec, Camillo J. Taylor, and Jitendra Malik. 1996. Modeling and Rendering\nArchitecture from Photographs: A Hybrid Geometry- and Image-Based Approach.\nIn Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive\nTechniques (SIGGRAPH \u201996). Association for Computing Machinery, 11\u201320.\nKangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. 2022. Depth-supervised\nNeRF: Fewer Views and Faster Training for Free. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR).\nStephan J Garbin, Marek Kowalski, Virginia Estellers, Stanislaw Szymanowicz, Shideh\nRezaeifar, Jingjing Shen, Matthew Johnson, and Julien Valentin. 2022. VolTeMorph:\nRealtime, Controllable and Generalisable Animation of Volumetric Representations.\narXiv preprint arXiv:2208.00949 (2022).\nSteven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. 1996. The\nLumigraph. In Proceedings of the 23rd Annual Conference on Computer Graphics\nand Interactive Techniques (SIGGRAPH \u201996). Association for Computing Machinery,\n43\u201354.\nYuan-Chen Guo, Yan-Pei Cao, Chen Wang, Yu He, Ying Shan, Xiaohu Qie, and Song-\nHai Zhang. 2023. VMesh: Hybrid Volume-Mesh Representation for Efficient View\nSynthesis. arXiv preprint arXiv:2303.16184 (2023).\nPeter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul\nDebevec. 2021. Baking Neural Radiance Fields for Real-Time View Synthesis. ICCV\n(2021).\nTao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia. 2022. EfficientNeRF Efficient\nNeural Radiance Fields. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR). 12902\u201312911.\nYixin Hu, Teseo Schneider, Bolun Wang, Denis Zorin, and Daniele Panozzo. 2020. Fast\ntetrahedral meshing in the wild. ACM Transactions on Graphics (TOG) 39, 4 (2020),\n117\u20131.\nWenzel Jakob, S\u00e9bastien Speierer, Nicolas Roussel, and Delio Vicini. 2022. DR. JIT: a\njust-in-time compiler for differentiable rendering. ACM Transactions on Graphics\n(TOG) 41, 4 (2022), 1\u201319.\nKrishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti, Linda\nPetrini, Martin Weiss, Breandan Considine, J\u00e9r\u00f4me Parent-L\u00e9vesque, Kevin Xie,\nKenny Erleben, et al. 2021. gradsim: Differentiable simulation for system identifica-\ntion and visuomotor control. arXiv preprint arXiv:2104.02646 (2021).\nRasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola, and Henrik Aan\u00e6s. 2014.\nLarge scale multi-view stereopsis evaluation. In 2014 IEEE Conference on Computer\nVision and Pattern Recognition. IEEE, 406\u2013413.\nPushkar Joshi, Mark Meyer, Tony DeRose, Brian Green, and Tom Sanocki. 2007. Har-\nmonic Coordinates for Character Articulation. ACM Trans. Graph. 26, 3 (jul 2007),\n71\u2013es. https://doi.org/10.1145/1276377.1276466\nAnimesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. 2022. ReLU Fields:\nThe Little Non-Linearity That Could. In ACM SIGGRAPH 2022 Conference Proceedings\n(SIGGRAPH \u201922). Association for Computing Machinery, New York, NY, USA, Article\n27, 9 pages. https://doi.org/10.1145/3528233.3530707\nMichael M. Kazhdan, Matthew Bolitho, and Hugues Hoppe. 2006. Poisson Surface\nReconstruction. In Proceedings of the Fourth Eurographics Symposium on Geometry\nProcessing (SGP \u201906, Vol. 256). Eurographics Association, 61\u201370.\nMichael M. Kazhdan and Hugues Hoppe. 2013. Screened poisson surface reconstruction.\nACM Trans. Graph. 32, 3 (2013), 29:1\u201329:13.\nBernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 2023.\n3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on\nGraphics 42, 4 (July 2023). https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/\nGeorgios Kopanas, Julien Philip, Thomas Leimk\u00fchler, and George Drettakis. 2021.\nPoint-Based Neural Rendering with Per-View Optimization. Computer Graphics\nForum (Proceedings of the Eurographics Symposium on Rendering) 40, 4 (June 2021).\nhttp://www-sop.inria.fr/reves/Basilic/2021/KPLD21\nAndreas Kurz, Thomas Neff, Zhaoyang Lv, Michael Zollh\u00f6fer, and Markus Steinberger.\n2022. AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance\nFields. In European Conference on Computer Vision (ECCV).\nChristoph Lassner and Michael Zollh\u00f6fer. 2021. Pulsar: Efficient Sphere-based Neural\nRendering. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) (2021), 1440\u20131449.\nMinjae Lee, David Hyde, Michael Bao, and Ronald Fedkiw. 2018. A skinned tetra-\nhedral mesh for hair animation and hair-water interaction. IEEE transactions on\nvisualization and computer graphics 25, 3 (2018), 1449\u20131459.\nMarc Levoy and Pat Hanrahan. 1996. Light Field Rendering. In Proceedings of the 23rd\nAnnual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH\n\u201996). Association for Computing Machinery, 31\u201342.\nMax Zhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H. Taylor, Mathias Unberath,\nMing-Yu Liu, and Chen-Hsuan Lin. 2023. Neuralangelo: High-Fidelity Neural Surface\nReconstruction. In Conference on Computer Vision and Pattern Recognition (CVPR).\nHaotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei\nZhou. 2022. Efficient Neural Radiance Fields with Learned Depth-Guided Sampling.\nIn SIGGRAPH Asia Conference Proceedings.\n14\n\u2022\nWang, Shen, Nimier-David, et al.\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020.\nNeural Sparse Voxel Fields. NeurIPS (2020).\nRicardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey\nDosovitskiy, and Daniel Duckworth. 2021. NeRF in the Wild: Neural Radiance Fields\nfor Unconstrained Photo Collections. In CVPR.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields\nfor View Synthesis. In ECCV.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nNeural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans.\nGraph. 41, 4, Article 102 (July 2022), 15 pages.\nhttps://doi.org/10.1145/3528223.\n3530127\nJacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex\nEvans, Thomas M\u00fcller, and Sanja Fidler. 2022. Extracting Triangular 3D Models,\nMaterials, and Lighting From Images. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). 8280\u20138290.\nThomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller,\nChakravarty R. Alla Chaitanya, Anton S. Kaplanyan, and Markus Steinberger. 2021.\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using\nDepth Oracle Networks. Computer Graphics Forum 40, 4 (2021).\nMichael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas\nGeiger, and Noha Radwan. 2022. RegNeRF: Regularizing Neural Radiance Fields\nfor View Synthesis from Sparse Inputs. In Proc. IEEE Conf. on Computer Vision and\nPattern Recognition (CVPR).\nMichael Oechsle, Songyou Peng, and Andreas Geiger. 2021. UNISURF: Unifying Neural\nImplicit Surfaces and Radiance Fields for Multi-View Reconstruction. In International\nConference on Computer Vision (ICCV).\nSteven G. Parker, James Bigler, Andreas Dietrich, Heiko Friedrich, Jared Hoberock,\nDavid Luebke, David McAllister, Morgan McGuire, Keith Morley, Austin Robison,\nand Martin Stich. 2010. OptiX: A General Purpose Ray Tracing Engine. ACM Trans.\nGraph. 29, 4, Article 66 (jul 2010), 13 pages. https://doi.org/10.1145/1778765.1778803\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Auto-\nmatic differentiation in PyTorch. NeurIPS Workshop on Autodiff (2017).\nMarie-Julie Rakotosaona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer,\nAbhijit Kundu, and Federico Tombari. 2023. NeRFMeshing: Distilling Neural Radi-\nance Fields into Geometrically-Accurate 3D Meshes. arXiv preprint arXiv:2303.09431\n(2023).\nDaniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasac-\nchi. 2020. DeRF: Decomposed Radiance Fields. 2021 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) (2020), 14148\u201314156.\nChristian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. KiloNeRF: Speed-\ning up Neural Radiance Fields with Thousands of Tiny MLPs. In International\nConference on Computer Vision (ICCV).\nChristian Reiser, Richard Szeliski, Dor Verbin, Pratul P Srinivasan, Ben Mildenhall,\nAndreas Geiger, Jonathan T Barron, and Peter Hedman. 2023. Merf: Memory-efficient\nradiance fields for real-time view synthesis in unbounded scenes. arXiv preprint\narXiv:2302.12249 (2023).\nGernot Riegler and Vladlen Koltun. 2020. Free View Synthesis. In European Conference\non Computer Vision.\nGernot Riegler and Vladlen Koltun. 2021. Stable View Synthesis. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition.\nRadu Alexandru Rosu and Sven Behnke. 2023. PermutoSDF: Fast Multi-View Reconstruc-\ntion with Implicit Surfaces using Permutohedral Lattices. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), Vol. 2.\nDarius R\u00fcckert, Linus Franke, and Marc Stamminger. 2021. Adop: Approximate differ-\nentiable one-pixel point rendering. arXiv preprint arXiv:2110.06635 (2021).\nSara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and\nAngjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In\nCVPR.\nJohannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. 2016. Structure-from-Motion\nRevisited. In Conference on Computer Vision and Pattern Recognition (CVPR).\nJohannes Lutz Sch\u00f6nberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm.\n2016. Pixelwise View Selection for Unstructured Multi-View Stereo. In European\nConference on Computer Vision (ECCV).\nCheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct Voxel Grid Optimization:\nSuper-fast Convergence for Radiance Fields Reconstruction. In CVPR.\nMatthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance\nWang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David\nMcAllister, and Angjoo Kanazawa. 2023. Nerfstudio: A Modular Framework for\nNeural Radiance Field Development. In ACM SIGGRAPH 2023 Conference Proceedings\n(SIGGRAPH \u201923).\nJiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang,\nand Gang Zeng. 2023. Delicate textured mesh recovery from nerf via adaptive\nsurface refinement. arXiv preprint arXiv:2303.02091 (2023).\nMichael Waechter, Nils Moehrle, and Michael Goesele. 2014. Let There Be Color! Large-\nScale Texturing of 3D Reconstructions. In Computer Vision \u2013 ECCV 2014 (Heidelberg)\n(Lecture Notes in Computer Science, Vol. 8693), David Fleet, Tomas Pajdla, Bernt\nSchiele, and Tinne Tuytelaars (Eds.). Springer, 836\u2013850. https://doi.org/10.1007/978-\n3-319-10602-1_54\nZiyu Wan, Christian Richardt, Alja\u017e Bo\u017ei\u010d, Chao Li, Vijay Rengarajan, Seonghyeon Nam,\nXiaoyu Xiang, Tuotuo Li, Bo Zhu, Rakesh Ranjan, et al. 2023. Learning Neural Duplex\nRadiance Fields for Real-Time View Synthesis. arXiv preprint arXiv:2304.10537\n(2023).\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping\nWang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\nMulti-view Reconstruction. NeurIPS (2021).\nYiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and\nLingjie Liu. 2022a. NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view\nReconstruction.\nYiqun Wang, Ivan Skorokhodov, and Peter Wonka. 2022b. HF-NeuS: Improved Surface\nReconstruction Using High-Frequency Details. arXiv preprint arXiv:2206.07850\n(2022).\nZian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren,\nZan Gojcic, Wenzheng Chen, and Sanja Fidler. 2023. Neural Fields meet Explicit\nGeometric Representations for Inverse Rendering of Urban Scenes. In The IEEE\nConference on Computer Vision and Pattern Recognition (CVPR).\nDaniel N. Wood, Daniel I. Azuma, Ken Aldinger, Brian Curless, Tom Duchamp, David H.\nSalesin, and Werner Stuetzle. 2000. Surface Light Fields for 3D Photography. In\nProceedings of the 27th Annual Conference on Computer Graphics and Interactive\nTechniques (SIGGRAPH \u201900). ACM Press/Addison-Wesley Publishing Co., 287\u2013296.\nTianhan Xu and Tatsuya Harada. 2022. Deforming radiance fields with cages. In\nComputer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part XXXIII. Springer, 159\u2013175.\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of\nneural implicit surfaces. In Thirty-Fifth Conference on Neural Information Processing\nSystems.\nLior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard\nSzeliski, Jonathan T Barron, and Ben Mildenhall. 2023. BakedSDF: Meshing Neural\nSDFs for Real-Time View Synthesis. arXiv preprint arXiv:2302.14859 (2023).\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and\nYaron Lipman. 2020. Multiview Neural Surface Reconstruction by Disentangling\nGeometry and Appearance. Advances in Neural Information Processing Systems 33\n(2020).\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021.\nPlenOctrees for Real-time Rendering of Neural Radiance Fields. In ICCV.\nYu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, and Lin Gao. 2022.\nNeRF-editing: geometry editing of neural radiance fields. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 18353\u201318364.\nKai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. 2021. PhySG:\nInverse Rendering with Spherical Gaussians for Physics-based Material Editing and\nRelighting. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR).\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. NeRF++: Analyzing\nand Improving Neural Radiance Fields. arXiv:2010.07492 (2020).\nFuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao Wang, Haizhao Dai, Yuhui\nZhong, Yingliang Zhang, Minye Wu, Lan Xu, and Jingyi Yu. 2022. Human Perfor-\nmance Modeling and Rendering via Neural Animated Mesh. ACM Trans. Graph. 41,\n6, Article 235 (2022), 17 pages.\nAPPENDIX\nWe provide additional algorithmic details and pseudocode. In the\nfirst phase of training, our method adaptively extracts an explicit\nmesh envelope which spatially bounds the neural volumetric rep-\nresentation: level set evolution and shell extraction are shown in\nProcedures 1 and 2. In the second phase of training, as well as infer-\nence, we leverage the extracted shells to sample query points only\nwhere they are needed. We cast rays against the shell meshes and\ncompute query locations in the narrow band between the outer and\ninner shell. This is detailed in Procedure 3. Note that one ray may\nintersect with multiple narrow bands, however it always terminates\nwhen encountering an inner shell. Finally, we include the overall\ntraining pipeline in Procedure 4.\nAdaptive Shells for Efficient Neural Radiance Field Rendering\n\u2022\n15\nProcedure 1 LevelSetEvolution(\ud835\udc53 , \ud835\udc63evolve,\ud835\udc47, \u0394\ud835\udc61,\ud835\udf01, \ud835\udf06curv)\nInput: level set field \ud835\udc53 \u2208 R\ud835\udc4b \u00d7\ud835\udc4c \u00d7\ud835\udc4d , velocity \ud835\udc63evolve \u2208 R\ud835\udc4b \u00d7\ud835\udc4c \u00d7\ud835\udc4d , evo-\nlution steps\ud835\udc47, timestep \u0394\ud835\udc61, soft falloff threshold \ud835\udf01, curvature\nregularization weight \ud835\udf06curv\nOutput: Evolved level set field \ud835\udc53\ud835\udc47\n1: \ud835\udc530 \u2190 \ud835\udc53\n\u22b2Initialize level set field\n2: for \ud835\udc56 \u2190 0 to \ud835\udc47 \u2212 1 do\n3:\nh \ud835\udf15\ud835\udc53\ud835\udc56\n\ud835\udf15\ud835\udc61\ni\nmot \u2190 \u2212 |\u2207\ud835\udc53\ud835\udc56 | \ud835\udc63evolve\n\u22b2Motion term\n4:\nh \ud835\udf15\ud835\udc53\ud835\udc56\n\ud835\udf15\ud835\udc61\ni\ncurv \u2190 \u2212\ud835\udf06curv |\u2207\ud835\udc53\ud835\udc56 | \u0000\u2207 \u00b7 \u2207\ud835\udc53\n|\u2207\ud835\udc53 |\n\u0001\n\u22b2Curvature term\n5:\n\ud835\udf14\ud835\udc56 \u2190 1\n2\n\u00001 + cos(\ud835\udf0b clamp(\ud835\udc53\ud835\udc56/\ud835\udf01, \u22121., 1.)\u0001 \u22b2Soft falloff (Eq. 5)\n6:\n\ud835\udf15\ud835\udc53\ud835\udc56\n\ud835\udf15\ud835\udc61 \u2190 \ud835\udf14\ud835\udc56\n\u0000h \ud835\udf15\ud835\udc53\ud835\udc56\n\ud835\udf15\ud835\udc61\ni\nmot +\nh \ud835\udf15\ud835\udc53\ud835\udc56\n\ud835\udf15\ud835\udc61\ni\ncurv\n\u0001\n7:\n\ud835\udc53\ud835\udc56+1 = \ud835\udc53\ud835\udc56 + \u0394\ud835\udc61 \ud835\udf15\ud835\udc53\ud835\udc56\n\ud835\udf15\ud835\udc61\n\u22b2Update level set field\n8: Return \ud835\udc53\ud835\udc47\nProcedure 2 ShellExtraction(\ud835\udc53 ,\ud835\udc60,\ud835\udf0f\ud835\udc51, \ud835\udefd\ud835\udc51, \ud835\udf0emin, \ud835\udefd\ud835\udc52, \ud835\udc63max)\nInput: signed distance field \ud835\udc53 \u2208 R\ud835\udc4b \u00d7\ud835\udc4c \u00d7\ud835\udc4d , spatially-varying kernel\nsize \ud835\udc60 \u2208 R\ud835\udc4b \u00d7\ud835\udc4c \u00d7\ud835\udc4d , grid size \ud835\udf0f\ud835\udc51, dilation hyperparameters\n\ud835\udefd\ud835\udc51, \ud835\udf0emin, erosion hyperparameters \ud835\udefd\ud835\udc52, \ud835\udc63max\nOutput: outer mesh \ud835\udc40+ and inner mesh \ud835\udc40\u2212\n1: \ud835\udefc \u2190 Sigmoid\u0000(\ud835\udc53 \u2212\ud835\udf0f\ud835\udc51/2)/\ud835\udc60\u0001\n\u2212Sigmoid\u0000(\ud835\udc53 +\ud835\udf0f\ud835\udc51/2)/\ud835\udc60\u0001\nSigmoid\u0000(\ud835\udc53 \u2212\ud835\udf0f\ud835\udc51/2)/\ud835\udc60\u0001\n\u22b2Eq. 3, as in NeuS\n2:\n3: \u22b2Level set dilation for outer mesh\n4: \ud835\udc63dilate \u2190 Where(\ud835\udefc > \ud835\udf0emin, \ud835\udefd\ud835\udc51\ud835\udefc, 0)\n\u22b2Eq. 6\n5: \ud835\udc53dilate \u2190 LevelSetEvolution(\ud835\udc53 , \ud835\udc63dilate,\ud835\udc47 = 50, \u0394\ud835\udc61 = 0.1,\n6:\n\ud835\udf01 = 0.1, \ud835\udf06curv = 0.01)\n7: \ud835\udc53dilate \u2190 min(\ud835\udc530, \ud835\udc53dilate)\n\u22b2Clip SDF for a strict dilation\n8: \ud835\udc40+ \u2190 MarchingCubes(\ud835\udc53dilate)\n9:\n10: \u22b2Level set erosion for inner mesh\n11: \ud835\udc63erode \u2190 min(\ud835\udc63max, \ud835\udefd\ud835\udc52/\ud835\udefc)\n\u22b2Eq. 7\n12: \ud835\udc53erode \u2190 LevelSetEvolution(\ud835\udc53 , \ud835\udc63erode,\ud835\udc47 = 50, \u0394\ud835\udc61 = 0.1,\n13:\n\ud835\udf01 = 0.05, \ud835\udf06curv = 0)\n14: \ud835\udc53erode \u2190 max(\ud835\udc530, \ud835\udc53erode)\n\u22b2Clip SDF for a strict erosion\n15: \ud835\udc40\u2212 \u2190 MarchingCubes(\ud835\udc53erode)\n16:\n17: Return \ud835\udc40+, \ud835\udc40\u2212\nProcedure 3 NarrowBandSampling(\ud835\udc40+, \ud835\udc40\u2212,\ud835\udc5f,\ud835\udc5c,\ud835\udc64\ud835\udc60,\ud835\udeff\ud835\udc60,\n.\n\ud835\udc41\ud835\udc5a\ud835\udc4e\ud835\udc65,\ud835\udc51\ud835\udc5d\ud835\udc5a\ud835\udc4e\ud835\udc65)\nInput: outer mesh \ud835\udc40+ and inner mesh \ud835\udc40\u2212, ray origin o \u2208 R3 and di-\nrection r \u2208 R3, target inter-sample spacing \ud835\udeff\ud835\udc60, single-sample\nthreshold \ud835\udc64\ud835\udc60, maximum number of samples per interval\n\ud835\udc41\ud835\udc5a\ud835\udc4e\ud835\udc65, and a maximum cap for depth peeling \ud835\udc51\ud835\udc5d\ud835\udc5a\ud835\udc4e\ud835\udc65\nOutput: a list of distances \ud835\udf0f\ud835\udc60 to sampled points along the ray\n1: \ud835\udf0f\ud835\udc60 \u2190 empty list, \ud835\udc5b\u210e\ud835\udc56\ud835\udc61\ud835\udc60 \u2190 0\n\u22b2Initialize Steps\n2: \u22b2Find hits of the inner mesh\n3: \ud835\udc5f\ud835\udc4e\ud835\udc66\ud835\udc3b\ud835\udc56\ud835\udc61\ud835\udc60\ud835\udc40\u2212 \u2190 CastRay(\ud835\udc40\u2212, o, r)\n4: if HasNext(\ud835\udc5f\ud835\udc4e\ud835\udc66\ud835\udc3b\ud835\udc56\ud835\udc61\ud835\udc60\ud835\udc40\u2212) then\n5:\n(\ud835\udc51\ud835\udc40\u2212, \ud835\udc53 \ud835\udc59\ud835\udc4e\ud835\udc54) \u2190 GetNextHit(\ud835\udc5f\ud835\udc4e\ud835\udc66\ud835\udc3b\ud835\udc56\ud835\udc61\ud835\udc60\ud835\udc40\u2212)\n6: else\n7:\n\ud835\udc51\ud835\udc40\u2212 \u2190 \u221e\n8: \u22b2Loop through the hits of the outer mesh\n9: \ud835\udc5f\ud835\udc4e\ud835\udc66\ud835\udc3b\ud835\udc56\ud835\udc61\ud835\udc60\ud835\udc40+ \u2190 CastRay(\ud835\udc40+, o, r)\n10: while HasNext(\ud835\udc5f\ud835\udc4e\ud835\udc66\ud835\udc3b\ud835\udc56\ud835\udc61\ud835\udc60\ud835\udc40+) AND \ud835\udc5b\u210e\ud835\udc56\ud835\udc61\ud835\udc60 < \ud835\udc51\ud835\udc5d\ud835\udc5a\ud835\udc4e\ud835\udc65 do\n11:\n(\u210e\ud835\udc56\ud835\udc61\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52, \ud835\udc53 \ud835\udc59\ud835\udc4e\ud835\udc54) \u2190 GetNextHit(\ud835\udc5f\ud835\udc4e\ud835\udc66\ud835\udc3b\ud835\udc56\ud835\udc61\ud835\udc60\ud835\udc40+)\n12:\n\ud835\udc5b\u210e\ud835\udc56\ud835\udc61\ud835\udc60 \u2190 \ud835\udc5b\u210e\ud835\udc56\ud835\udc61\ud835\udc60 + 1\n13:\nif \ud835\udc53 \ud835\udc59\ud835\udc4e\ud835\udc54 = ENTERING then\n\u22b2Ray enters the mesh band\n14:\n\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \u2190 \u210e\ud835\udc56\ud835\udc61\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\n15:\nelse if \ud835\udc53 \ud835\udc59\ud835\udc4e\ud835\udc54 = EXITING then\n\u22b2Ray exits the mesh band\n16:\n\u22b2Compute samples between enter and exit\n17:\n\ud835\udc51\ud835\udc52\ud835\udc65\ud835\udc56\ud835\udc61 \u2190 Min(\u210e\ud835\udc56\ud835\udc61\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52, \ud835\udc51\ud835\udc40\u2212)\n18:\n\ud835\udc64 \u2190 \ud835\udc51\ud835\udc52\ud835\udc65\ud835\udc56\ud835\udc61 \u2212 \ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n19:\n\ud835\udc41 \u2190 Min(Ceil(Max(\ud835\udc64 \u2212 \ud835\udc64\ud835\udc60, 0)/\ud835\udeff\ud835\udc60)+1,\ud835\udc41\ud835\udc5a\ud835\udc4e\ud835\udc65)\n20:\n\ud835\udf0f\ud835\udc60 \u2190 \ud835\udf0f\ud835\udc60 + Linspace(\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f,\ud835\udc51\ud835\udc52\ud835\udc65\ud835\udc56\ud835\udc61, \ud835\udc41 + 2)[1 : \u22121]\n21:\nif \u210e\ud835\udc56\ud835\udc61\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 > \ud835\udc51\ud835\udc40\u2212 then\n22:\nbreak\n\u22b2Terminate if beyond the inner mesh\n23: end while\n24: Return \ud835\udf0f\ud835\udc60\nProcedure 4 Training Pipeline\nInput: rays R, ground-truth pixel colors C, training iterations for\nthe first stage \ud835\udc411 and second stage \ud835\udc412, network NN\ud835\udf03\nOutput: optimized network parameters \ud835\udf03, shell \ud835\udc40+, \ud835\udc40\u2212\n\u22b2First stage training with full ray volume rendering\n1: for \ud835\udc56 \u2190 0 to \ud835\udc411 \u2212 1 do\n2:\nSample data r\ud835\udc56 \u2208 R, c\ud835\udc56 \u2208 C\n3:\n\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61 \u2190 VolumeRendering(NN\ud835\udf03, r\ud835\udc56)\n4:\nL \u2190 Loss(\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61, c\ud835\udc56)\n\u22b2Compute loss with Eq. 8\n5:\nUpdate network: \ud835\udf03 \u2190 \ud835\udf03 \u2212 \ud835\udf02 \ud835\udf15L\n\ud835\udf15\ud835\udf03\n6: \u22b2Extract adaptive shells\n7: \ud835\udc40+, \ud835\udc40\u2212 \u2190 ShellExtraction(NN\ud835\udf03)\n\u22b2Section 3.3\n8: \u22b2Second stage training with narrow-band rendering\n9: for \ud835\udc56 \u2190 0 to \ud835\udc412 \u2212 1 do\n10:\nSample data r\ud835\udc56 \u2208 R, c\ud835\udc56 \u2208 C\n11:\n\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61 \u2190 NarrowBandRendering(NN\ud835\udf03, r\ud835\udc56, \ud835\udc40+, \ud835\udc40\u2212)\n12:\nLc \u2190 ColorLoss(\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc5d\ud835\udc62\ud835\udc61, c\ud835\udc56)\n\u22b2Compute loss with Eq. 9\n13:\nUpdate network: \ud835\udf03 \u2190 \ud835\udf03 \u2212 \ud835\udf02 \ud835\udf15Lc\n\ud835\udf15\ud835\udf03\n14: Return \ud835\udf03, \ud835\udc40+, \ud835\udc40\u2212\nAdaptive Shells for Efficient Neural Radiance Field Rendering\nSupplementary Material\nIn this supplementary material, we provide additional results (Sec-\ntion 1) and implementation details (Section 2).\n1\nADDITIONAL RESULTS\nWe provide additional qualitative results on MipNeRF360 data set\n(Figure 1) and DTU data set (Figure ??, ??), as well as per-scene\nquantitative numbers for all data sets (Tables 1\u201311).\n2\nIMPLEMENTATION DETAILS\nLevel set evolution. To perform the level set evolution, we extract\nthe initial SDF values on a 5123 grid and separately dilate and erode\nthe zero level set for 50 iterations with the timestep as 0.1. For\ndilation, we use \ud835\udefd\ud835\udc51 = 1 and the density threshold \ud835\udf0emin is set to 0.01\nfor all data sets. These values were determined empirically such that\nthe dilated level set sufficiently covers thin structures. We set the\nerosion hyperparameters to \ud835\udefd\ud835\udc52 = 0.001, \ud835\udc63max = 100. The evolution\nprocess takes approximately 2 seconds which is negligible compared\nto the other steps of our training pipeline.\nNarrow-band rendering. When sampling query points within the\nshells, we set the maximum number of samples per interval as\n\ud835\udc41\ud835\udc5a\ud835\udc4e\ud835\udc65 = 16, and the maximum cap for depth peeling \ud835\udc51\ud835\udc5d\ud835\udc5a\ud835\udc4e\ud835\udc65 = 20.\nFor Shelly, DTU and MipNeRF360 data sets, we use single-sample\nthreshold \ud835\udc64\ud835\udc60 = 0.02 and inter-sample spacing \ud835\udeff\ud835\udc60 = 0.01. For NeRF-\nSynthetic data set, we use single-sample threshold \ud835\udc64\ud835\udc60 = 0.005 and\ninter-sample spacing \ud835\udeff\ud835\udc60 = 0.0025.\nRepresenting the background. For DTU data set, we combine the\nmain volume representation with a spherical background placed at\ninfinity (only dependent on the ray direction). Similar to our main\nnetwork, the spherical background is represented using a combina-\ntion of a hash encoding (4 levels, 2D features per level) and a small\nMLP such that c = NNbckg\n\ud835\udf03\n([\u03a8(d)]). All the rays that completely\nmiss the extracted shell obtain the color from the background, which\nrequires a single sample evaluation.\nWhen training on MipNeRF360 data set, we follow prior works\n[Yariv et al. 2023] and extend our method with the scene contraction\nproposed in [Barron et al. 2022]. Specifically, we map the scene\noutside the unit sphere into a sphere with radius 2 using the scene\ncontraction function\ncontract(x) =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\nx,\n||x|| \u2264 1,\n(2 \u2212\n1\n||x|| ) x\n||x||,\n||x|| > 1.\n(1)\nTraining details. During training, we linearly warm up the learn-\ning rate to 1 \u00d7 10\u22122 in the first 5k iterations, and then exponentially\ndecay it to 1\u00d710\u22124 at the end of training. In all experiments, we use\nthe AdamW optimizer with weight decay as 1e-2. For Shelly, DTU\nand NeRFSynthetic data sets, we train for a total of 300k iterations,\nwhere the first 200k iterations are used for the first stage (full-ray\nAuthor\u2019s address:\nformulation) and the remaining 100k for the second stage (finetun-\ning within the narrow band). We use the batch-size of 4096 rays.\nFor each scene, the training takes 8h on a single A100 GPU. For\nlarger MipNeRF360 scenes, we increase the full-ray training to 500k\niterations. We adopt the progressive training scheme [Li et al. 2023;\nWang et al. 2022], where we initially enable the 8 coarsest levels\nof features (with the remaining feature channels set to zero), we\nthen add one level every 5k iterations until reaching the maximum\nnumber of levels which equals 14.\nREFERENCES\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-\nBrualla, and Pratul P. Srinivasan. 2021. Mip-NeRF: A Multiscale Representation for\nAnti-Aliasing Neural Radiance Fields. ICCV (2021).\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.\n2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. CVPR (2022).\nZhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. 2023.\nMobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural\nField Rendering on Mobile Architectures. In The Conference on Computer Vision and\nPattern Recognition (CVPR).\nMax Zhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H. Taylor, Mathias Unberath,\nMing-Yu Liu, and Chen-Hsuan Lin. 2023. Neuralangelo: High-Fidelity Neural Surface\nReconstruction. In Conference on Computer Vision and Pattern Recognition (CVPR).\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields\nfor View Synthesis. In ECCV.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nNeural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans.\nGraph. 41, 4, Article 102 (July 2022), 15 pages.\nhttps://doi.org/10.1145/3528223.\n3530127\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping\nWang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\nMulti-view Reconstruction. NeurIPS (2021).\nYiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and\nLingjie Liu. 2022. NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view\nReconstruction.\nLior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard\nSzeliski, Jonathan T Barron, and Ben Mildenhall. 2023. BakedSDF: Meshing Neural\nSDFs for Real-Time View Synthesis. arXiv preprint arXiv:2302.14859 (2023).\narXiv:2311.10091v1  [cs.CV]  16 Nov 2023\n1:2\n\u2022\nTable 1. Per-scene quantitative PSNR comparison on Shelly data set.\nPSNR \u2191\nNeRF\n[Mildenhall et al. 2020]\nMipNeRF\n[Barron et al. 2021]\nNeuS\n[Wang et al. 2021]\nI-NGP\n[M\u00fcller et al. 2022]\nMobileNeRF\n[Chen et al. 2023]\nOurs\n(full ray)\nOurs\nFernvase\n31.77\n32.54\n29.26\n33.48\n31.38\n33.93\n36.47\nPug\n31.34\n32.26\n31.38\n32.70\n31.50\n33.60\n35.83\nWoolly\n28.33\n29.18\n27.90\n31.13\n31.61\n31.61\n34.19\nHorse\n34.02\n37.12\n30.92\n37.67\n36.48\n39.40\n40.57\nKhady\n29.01\n29.88\n28.29\n29.21\n26.84\n31.09\n31.22\nKitten\n33.18\n34.54\n32.16\n35.13\n31.93\n35.94\n37.82\nAverage\n31.28\n32.59\n29.98\n33.22\n31.62\n34.26\n36.02\nTable 2. Per-scene quantitative LPIPS comparison on Shelly data set.\nLPIPS \u2193\nNeRF\n[Mildenhall et al. 2020]\nMipNeRF\n[Barron et al. 2021]\nNeuS\n[Wang et al. 2021]\nI-NGP\n[M\u00fcller et al. 2022]\nMobileNeRF\n[Chen et al. 2023]\nOurs\n(full ray)\nOurs\nFernvase\n0.093\n0.088\n0.094\n0.068\n0.074\n0.065\n0.046\nPug\n0.198\n0.190\n0.209\n0.156\n0.167\n0.132\n0.093\nWoolly\n0.241\n0.217\n0.232\n0.181\n0.163\n0.139\n0.089\nHorse\n0.071\n0.064\n0.067\n0.049\n0.057\n0.036\n0.029\nKhady\n0.246\n0.239\n0.251\n0.215\n0.218\n0.185\n0.160\nKitten\n0.094\n0.092\n0.097\n0.079\n0.094\n0.066\n0.056\nAverage\n0.157\n0.148\n0.158\n0.125\n0.129\n0.104\n0.079\nTable 3. Per-scene quantitative SSIM comparison on Shelly dataset.\nSSIM \u2191\nNeRF\n[Mildenhall et al. 2020]\nMipNeRF\n[Barron et al. 2021]\nNeuS\n[Wang et al. 2021]\nI-NGP\n[M\u00fcller et al. 2022]\nMobileNeRF\n[Chen et al. 2023]\nOurs\n(full ray)\nOurs\nFernvase\n0.937\n0.940\n0.932\n0.955\n0.944\n0.964\n0.976\nPug\n0.863\n0.868\n0.865\n0.896\n0.885\n0.910\n0.947\nWoolly\n0.805\n0.822\n0.803\n0.876\n0.891\n0.896\n0.950\nHorse\n0.975\n0.980\n0.973\n0.985\n0.980\n0.988\n0.992\nKhady\n0.831\n0.835\n0.833\n0.852\n0.823\n0.862\n0.881\nKitten\n0.949\n0.954\n0.949\n0.967\n0.942\n0.969\n0.979\nAverage\n0.893\n0.899\n0.893\n0.922\n0.911\n0.932\n0.954\nTable 4. Per-scene sample count of our method on Shelly data set.\nFernvase\nPug\nWoolly\nHorse\nKhady\nKitten\nAverage\nOurs\n1.514\n1.921\n2.040\n0.425\n3.292\n1.228\n1.737\nTable 5. Per-scene quantitative results on DTU data set. We report the PSNR, LPIPS and SSIM results for each scene and compare them with baselines.\nI-NGP [M\u00fcller et al. 2022]\nNeus [Wang et al. 2021]\nNerf [Mildenhall et al. 2020]\nMipNeRF [Barron et al. 2021]\nOurs (full ray)\nOurs\nScene\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\n24\n29.85\n0.237\n0.871\n26.22\n0.324\n0.787\n24.87\n0.364\n0.751\n25.60\n0.359\n0.763\n32.42\n0.110\n0.931\n30.91\n0.102\n0.934\n37\n25.05\n0.169\n0.869\n23.63\n0.191\n0.835\n21.31\n0.225\n0.792\n22.97\n0.212\n0.815\n27.46\n0.095\n0.932\n27.38\n0.096\n0.938\n40\n28.80\n0.291\n0.829\n26.38\n0.344\n0.755\n24.98\n0.363\n0.715\n25.90\n0.343\n0.743\n31.16\n0.161\n0.913\n32.10\n0.150\n0.931\n55\n27.80\n0.171\n0.923\n25.56\n0.171\n0.893\n23.61\n0.202\n0.846\n24.20\n0.192\n0.865\n32.09\n0.067\n0.974\n32.35\n0.065\n0.975\n63\n33.06\n0.075\n0.961\n30.51\n0.115\n0.954\n29.94\n0.117\n0.945\n30.41\n0.114\n0.947\n34.94\n0.049\n0.974\n34.10\n0.044\n0.975\n65\n33.75\n0.091\n0.962\n30.71\n0.108\n0.962\n29.86\n0.118\n0.948\n30.38\n0.114\n0.951\n34.90\n0.069\n0.069\n35.08\n0.066\n0.974\n69\n31.21\n0.149\n0.946\n27.97\n0.184\n0.938\n28.15\n0.217\n0.913\n28.75\n0.213\n0.916\n32.15\n0.104\n0.961\n31.79\n0.102\n0.960\n83\n35.28\n0.055\n0.977\n33.57\n0.081\n0.974\n33.13\n0.077\n0.971\n33.29\n0.077\n0.971\n36.75\n0.040\n0.983\n37.12\n0.036\n0.985\n97\n28.50\n0.140\n0.937\n26.93\n0.144\n0.936\n26.58\n0.168\n0.918\n26.70\n0.172\n0.918\n29.99\n0.086\n0.959\n29.77\n0.093\n0.956\n105\n34.08\n0.113\n0.948\n31.39\n0.164\n0.932\n31.37\n0.160\n0.927\n31.35\n0.160\n0.926\n35.50\n0.067\n0.967\n35.91\n0.061\n0.970\n106\n33.31\n0.135\n0.947\n28.80\n0.161\n0.934\n30.63\n0.177\n0.921\n30.92\n0.174\n0.923\n36.29\n0.073\n0.973\n35.81\n0.072\n0.972\n110\n29.89\n0.118\n0.951\n28.14\n0.145\n0.946\n28.84\n0.140\n0.943\n28.55\n0.140\n0.942\n32.81\n0.073\n0.971\n33.18\n0.071\n0.972\n114\n29.41\n0.163\n0.925\n28.13\n0.178\n0.921\n28.28\n0.183\n0.907\n28.33\n0.183\n0.908\n31.08\n0.102\n0.955\n31.07\n0.094\n0.956\n118\n35.23\n0.105\n0.964\n31.60\n0.128\n0.961\n33.42\n0.130\n0.952\n33.23\n0.129\n0.951\n37.67\n0.066\n0.978\n36.71\n0.062\n0.979\n122\n35.42\n0.074\n0.972\n34.36\n0.090\n0.971\n32.66\n0.102\n0.959\n32.97\n0.098\n0.961\n37.50\n0.047\n0.983\n37.21\n0.044\n0.983\nMean\n31.38\n0.139\n0.932\n28.93\n0.168\n0.913\n28.51\n0.183\n0.894\n28.90\n0.179\n0.900\n33.51\n0.081\n0.901\n33.37\n0.077\n0.964\nAdaptive Shells for Efficient Neural Radiance Field Rendering\nSupplementary Material\n\u2022\n1:3\nFig. 1. Qualitative visualization of geometry and kernel size on MipNeRF360 data set. The kernel size is re-scaled to between 0 and 1. Our method automatically\nconverges to a large kernel size for fuzzy regions such as grass and a small kernel size for sharp surfaces.\nTable 6. Per-scene sample count of our method on DTU data set.\nScene\n24\n37\n40\n55\n63\n65\n69\n83\n97\n105\n106\n110\n114\n118\n122\nAverage\nOurs\n3.989\n5.716\n4.085\n5.254\n3.570\n4.385\n6.844\n4.608\n5.882\n5.750\n4.015\n6.492\n4.454\n3.117\n2.910\n4.738\nTable 7. Per-scene results of our method on NeRFSynthetic data set.\nOurs (full ray)\nMic\nFicus\nChair\nHotdog\nMaterials\nDrums\nShip\nLego\nAverage\nPSNR\n34.46\n34.67\n35.14\n36.17\n28.47\n25.32\n30.27\n35.60\n32.51\nLPIPS\n0.012\n0.024\n0.020\n0.029\n0.076\n0.081\n0.121\n0.021\n0.048\nSSIM\n0.989\n0.985\n0.986\n0.982\n0.941\n0.939\n0.896\n0.981\n0.962\nOurs\nMic\nFicus\nChair\nHotdog\nMaterials\nDrums\nShip\nLego\nAverage\nPSNR\n33.91\n33.63\n34.94\n36.21\n27.82\n25.19\n29.54\n33.49\n31.84\nLPIPS\n0.015\n0.033\n0.023\n0.035\n0.086\n0.086\n0.141\n0.031\n0.056\nSSIM\n0.988\n0.981\n0.985\n0.981\n0.935\n0.937\n0.877\n0.973\n0.957\nSample count\n1.200\n3.097\n2.113\n3.728\n4.235\n3.042\n6.799\n4.035\n3.531\nTable 8. Per-scene quantitative PSNR comparison on MipNeRF360 data set.\nNeRF\n[Mildenhall et al. 2020]\nMip-NeRF\n[Barron et al. 2021]\nMip-NeRF 360\n[Barron et al. 2022]\nI-NGP\n[M\u00fcller et al. 2022]\nMobileNeRF\n[Chen et al. 2023]\nBakedSDF\n[Yariv et al. 2023]\nOurs\n(full ray)\nOurs\nOutdoor\nBicycle\n21.76\n21.69\n24.37\n23.67\n21.70\n22.08\n24.07\n22.19\nGarden\n23.11\n23.16\n26.98\n24.60\n23.04\n24.53\n25.73\n25.35\nStump\n21.73\n21.21\n26.40\n23.43\n23.96\n23.59\n23.10\n21.96\nAverage\n22.20\n22.02\n25.92\n23.90\n22.90\n23.40\n24.30\n23.17\nIndoor\nRoom\n28.56\n28.73\n31.63\n30.16\n28.76\n28.63\n29.61\n30.63\nCounter\n25.67\n25.59\n29.55\n26.03\n24.74\n25.63\n26.26\n25.24\nKitchen\n26.31\n26.47\n32.23\n29.86\n26.28\n26.88\n30.10\n28.43\nBonsai\n26.81\n27.13\n33.46\n31.82\n23.20\n27.67\n30.19\n32.47\nAverage\n26.84\n26.98\n31.72\n29.47\n25.74\n27.20\n29.04\n29.19\n1:4\n\u2022\nTable 9. Per-scene quantitative LPIPS comparison on MipNeRF360 data set.\nNeRF\n[Mildenhall et al. 2020]\nMip-NeRF\n[Barron et al. 2021]\nMip-NeRF 360\n[Barron et al. 2022]\nI-NGP\n[M\u00fcller et al. 2022]\nMobileNeRF\n[Chen et al. 2023]\nBakedSDF\n[Yariv et al. 2023]\nOurs\n(full ray)\nOurs\nOutdoor\nBicycle\n0.536\n0.541\n0.301\n0.417\n0.513\n0.394\n0.326\n0.438\nGarden\n0.415\n0.422\n0.170\n0.248\n0.396\n0.243\n0.200\n0.247\nStump\n0.551\n0.490\n0.261\n0.441\n0.480\n0.415\n0.421\n0.482\nAverage\n0.501\n0.484\n0.244\n0.369\n0.463\n0.351\n0.316\n0.389\nIndoor\nRoom\n0.353\n0.346\n0.211\n0.300\n0.423\n0.310\n0.295\n0.300\nCounter\n0.394\n0.390\n0.201\n0.341\n0.476\n0.340\n0.276\n0.395\nKitchen\n0.335\n0.336\n0.127\n0.224\n0.393\n0.267\n0.168\n0.219\nBonsai\n0.398\n0.370\n0.176\n0.227\n0.522\n0.281\n0.217\n0.225\nAverage\n0.370\n0.361\n0.179\n0.273\n0.453\n0.300\n0.239\n0.285\nTable 10. Per-scene quantitative SSIM comparison on MipNeRF360 data set.\nNeRF\n[Mildenhall et al. 2020]\nMip-NeRF\n[Barron et al. 2021]\nMip-NeRF 360\n[Barron et al. 2022]\nI-NGP\n[M\u00fcller et al. 2022]\nMobileNeRF\n[Chen et al. 2023]\nBakedSDF\n[Yariv et al. 2023]\nOurs\n(full ray)\nOurs\nOutdoor\nBicycle\n0.455\n0.454\n0.685\n0.624\n0.426\n0.394\n0.689\n0.544\nGarden\n0.546\n0.543\n0.813\n0.708\n0.589\n0.740\n0.814\n0.757\nStump\n0.453\n0.517\n0.744\n0.613\n0.557\n0.597\n0.607\n0.516\nAverage\n0.485\n0.505\n0.747\n0.648\n0.524\n0.577\n0.703\n0.606\nIndoor\nRoom\n0.843\n0.851\n0.913\n0.886\n0.836\n0.872\n0.892\n0.895\nCounter\n0.775\n0.779\n0.894\n0.826\n0.724\n0.809\n0.872\n0.795\nKitchen\n0.749\n0.745\n0.920\n0.869\n0.751\n0.825\n0.904\n0.866\nBonsai\n0.792\n0.818\n0.941\n0.925\n0.716\n0.875\n0.933\n0.933\nAverage\n0.790\n0.798\n0.917\n0.877\n0.757\n0.845\n0.900\n0.872\nTable 11. Per-scene sample count of our method on MipNeRF360 data set.\nBicycle\nGarden\nStump\nRoom\nCounter\nKitchen\nBonsai\nAverage\nOurs\n16.33\n14.05\n22.81\n17.14\n22.24\n14.49\n14.80\n17.41\n"
  },
  {
    "title": "Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying",
    "link": "https://arxiv.org/pdf/2311.09578.pdf",
    "upvote": "10",
    "text": "Tied-LoRA : Enhancing parameter efficiency of LoRA with Weight Tying\nAdithya Renduchintala\nTugrul Konuk\nNVIDIA\n{adithyare,tkonuk,okuchaiev}@nvidia.com\nOleksii Kuchaiev\nAbstract\nWe propose Tied-LoRA , a simple paradigm uti-\nlizes weight tying and selective training to fur-\nther increase parameter efficiency of the Low-\nrank adaptation (LoRA) method. Our investiga-\ntions include all feasible combinations parame-\nter training/freezing in conjunction with weight\ntying to identify the optimal balance between\nperformance and the number of trainable pa-\nrameters. Through experiments covering a vari-\nety of tasks and two base language models, we\nprovide analysis revealing trade-offs between\nefficiency and performance. Our experiments\nuncovered a particular Tied-LoRA configura-\ntion that stands out by demonstrating compa-\nrable performance across several tasks while\nemploying only 13 % percent of parameters\nutilized by the standard LoRA method.\n1\nIntroduction\nLarge language models (LLMs) are increasingly\nemployed as the fundamental component in a wide\narray of applications that require proficiency in Nat-\nural Language Processing (NLP) tasks. A key con-\ntributor to this widespread adoption is the capacity\nto fine-tune pretrained LLMs for downstream tasks\nin a parameter-efficient manner. This fine-tuning\nprocedure enables the development of specialized\nlanguage models that excel on particular domains\nand tasks. Despite smaller training data sizes com-\npared to pretraining, finetuning still demands con-\nsiderable computational resources, particularly for\nlarge models containing billions of parameters. Fur-\nthermore, it is often desirable to maintain several\ndifferent customization for each combination of\nuser \u00d7 task of the service, many of which are of-\nten served simultaneously. As the number of users\nand tasks per user grow, so do the costs associated\nwith customization. Thus, making efficient use of\ncustomizable parameters paramount.\nLow-rank Adaptation (LoRA) (Hu et al., 2021)\nhas emerged as a popular parameter-efficient fine-\nW \u2208 Rd\u00d73d\nA \u2208 Rd\u00d7r\n\u00ae\nv \u2208 R3d\u00d71\nB \u2208 Rr\u00d73d\n\u00ae\nx\nz\nu \u2208 Rr\u00d71\nFigure 1: Schematic of our Tied-Lora paradigm, the\nmain low-rank matrices A and B are tied across (indi-\ncated by the \u00ae symbol) all the layers of the base lan-\nguage model. We use the gradient shading to indicate\nthat these parameters can either be trained or frozen.\ntuning (PEFT) method because of its straightfor-\nward implementation and the ability to merge\nLoRA weights into the base model. However, de-\nspite its advantages, LoRA training can still be\nexpensive, especially as the base models become\nincreasingly larger. While prior work has attempted\nto make LoRA more parameter efficient, they con-\ncentrated on appropriate low-rank selection. How-\never, we introduce a novel approach, Instead of\ncontrolling the number of parameters by the rank,\nwe employ simple weight tying coupled with selec-\ntive training. By integrating these two core ideas,\nwe propose a range of Tied-LoRA configurations\nand study the performance of each configuration\non five diverse customization tasks.\nOur contributions are threefold:\n1. We propose a range of Tied-LoRA configura-\ntions that use simple weight tying in LoRA\nalong with selective training to boost the pa-\nrameter efficiency of LoRA.\n2. We study this spectrum of possible Tied-\nLoRA configurations on diverse tasks that re-\narXiv:2311.09578v1  [cs.CL]  16 Nov 2023\nsemble real-world customization problems.\n3. Based on the results of our study, we propose\nthe specific vB\u00aeuA\u00ae configuration as the\nbest option for maintaining performance while\nreducing parameters by 87%.\n2\nRelated Work\nParameter-efficient fine-tuning (PEFT).\nRe-\ncent work on PEFT of pretrained language models\nhas shown competitive capabilities, often match-\ning full fine-tuning performance for task-specific\nmodel customization while utilizing significantly\nfewer trainable parameters (Houlsby et al., 2019;\nLin et al., 2020; Pfeiffer et al., 2021; R\u00fcckl\u00e9 et al.,\n2021; Liu et al., 2022).\nLow-Rank adaptation (LoRA).\nOne of the most\npopular PEFT techniques is LoRA, introduced by\nHu et al. (2021). LoRA employs low-rank matrix\napproximations of full weights\u2019 gradient-descent\n(GD) update to significantly reduce the number\nof trainable parameters. Importantly, LoRA can\nincorporate the low-rank updates into the frozen\nbase weights after the fine-tuning process, avoiding\nany inference speed penalties or model architecture\nchanges. In summary, LoRA paves the way for effi-\ncient fine-tuning for task-specific customization of\nlarge models with minimal computational overhead\nand no changes to the model\u2019s architecture.\nExtensions to LoRA.\nSince its arrival, there have\nbeen several efforts to improve the LoRA method.\nThese methods mostly concentrated around reduc-\ning the trainable parameters and memory footprint\nwhile increasing the performance of the method on\ndownstream tasks. AdaLoRA (Zhang et al., 2023)\nintroduces dynamic rank adjustment for the low-\nrank matrices during the fine-tuning process. The\nfundamental premise of this extension is to opti-\nmally distribute the parameter budget over model\nlayers. Chavan et al. (2023) combined the adapter\ntuning with LoRA to derive a generalized frame-\nwork that utilized both methods for increased flexi-\nbility and capability across a wide variety of tasks\nand datasets. Kopiczko et al. (2023) proposes the\nVeRA method the freezes randomly initialized pro-\njection matrices and introduces trainable scaling\nvectors that vary across layers. This method shows\nsimilar performance to the vBuA(LoRA) method\nwhile dramatically reducing the number of train-\nable parameters. Our work draws significant inspi-\nration from the principles of the VeRA method.\nTangential to the efforts that aim to reduce train-\nable parameters, QLoRA (Dettmers et al., 2023),\nsignificantly reduces the memory usage of LoRA\nusing a 4-bit or 8-bit quantized base language\nmodel during training. The method provides al-\ngorithms and custom kernels to backpropagate gra-\ndients through the frozen, quantized base model to\nupdate low-rank matrices during training, resulting\nin considerable reduction in memory usage. Com-\nbining quantization and reduction in the number of\ntrainable parameters is a direction of future work.\nWeight tying.\nWeight tying (Press and Wolf,\n2017) is a common approach that reduces the num-\nber of parameters by using the same set of weights\nfor both the input word embedding layer and the\noutput word embedding layer (sometimes referred\nto as the language model head). In this study, we\napply weight tying to the low-rank weight matrices\nused in LoRA, and share them across the layers of\nthe base language model. This simple procedure\nleads to efficient training methods where the num-\nber of trainable parameters are either unaffected by,\nor only increases marginally with the number of\nhidden layers. As models get deeper this approach\nnaturally provides greater parameter reduction over\noriginal LoRA method.\n3\nMethod\nIn this section, we introduce tied vBuA(LoRA) ,\na generalized paradigm for parameter-efficient fine-\ntuning of large language models through low-rank\nweight-update approximations. Our framework of-\nfers a range of training strategies through a series\nof design choices over selective parameter train-\ning and weight tying, including some of the ex-\nisting PEFT methodologies available in the litera-\nture. Specifically, we use weight tying alongside\npairs of projection matrices and scaling vectors that\ncan be selectively either trained or frozen. As the\nlow-rank computation path does not introduce any\nnon-linearity, all Tied-LoRA configurations can be\nmerged into the base model weights to preventing\nadditional latency during inference. Table 1 pro-\nvides an overview of the scenarios we study.\n3.1\nFormulation\nThe overall structure of the tied LoRA framework\ncan be seen in Figure 1. Note that the original\nLoRA (Hu et al., 2021) uses a dedicated pair of low-\nrank projections for each of the Q, K, V matrices.\nHowever, in our formulation, W is a d \u00d7 3d matrix\nthat jointly projects Q, K, and V attention matrices,\nwhere d is the hidden size of the base language\nmodel. Therefore, our down projection A is a d \u00d7\nr shaped matrix and up projection matrix B has\nshape r \u00d7 3d, where r is the low-rank bottleneck\ndimension. Essentially, the down projection A is\nshared by Q, K, and V , leading to fewer trainable\nparameters (4dr) than the original LoRA (6dr).\nFor a linear layer with a frozen pretrained weight\nmatrix W, we define the layer output as\nz = Wx + \u2206Wx \u2248 Wx + \u03b1\nr \u039bvB\u039buAx,\n(1)\nwhere \u2206W is the full-rank update matrix, \u03b1 is a\nscaling factor, A and B are low-rank projection ma-\ntrices, and \u039bu and \u039bv are diagonal matrices with\ndiagonal elements given by u and v, respectively.\nHerein, \u039bvB\u039buAx is the low-rank approximation\nto the parameter update matrix \u2206W. Unlike the\noriginal LoRA, where \u03b1 is a hyper-parameter that\ncan be manually set, we simply set \u03b1 = r, effec-\ntively removing its scaling effect.\nEquation 1 is a generalized formulation for meth-\nods that utilize low-rank approximations to esti-\nmate parameter updates. Particular settings of pa-\nrameter updates and weight tying reduces this equa-\ntion to some of the existing formulations in the lit-\nerature. Setting and freezing \u039bu = \u039bv = I and\nuntying A and B results in LoRA:\nz = Wx + BAx.\n(2)\nSimilarly, randomly initializing A and B matri-\nces and tying them across all layer leads the the\nVeRA formulation (Kopiczko et al., 2023):\nz = Wx + \u039bvB\u039buAx,\n(3)\n3.2\nWeight Tying\nThe third column of Table 1 presents representa-\ntions for number of trainable parameters each Tied-\nLora configuration requires. As is apparent from\nthe table, weight tying is a critical ingredient of\nour proposed approach which drastically reduces\nthe number of trainable parameters. For exam-\nple, vBuA(LoRA) training using the 7B LLaMA-\n2 (Touvron et al., 2023) language model with a typ-\nical low rank setting of 8 requires \u223c 4.2M trainable\nparameters. By merely introducing weight tying\nacross the 32 layers of this model reduces the train-\nable parameters to \u223c 131K, which is a 96.875%\nreduction. In comparison, the Vera method results\nin a reduction of 90.6%.\nMethod\nParameters\nInitialization\nvBuA(LoRA)\n4Ldr\nA \u223c N, B = 0, u, v = 1\nvB\u00aeuA\u00ae\n4dr\nA \u223c N, B = 0, u, v = 1\nvB\u00aeuA\u00ae\n4dr + L(r + 3d)\nA, B \u223c N, u = 1, v = 0\nvB\u00aeuA\u00ae\n(L + 3d)r\nA, B \u223c N, v, u = 1\nvB\u00aeuA\u00ae\n3dr\nA, B \u223c N, u, v = 1\nvB\u00aeuA\u00ae\ndr + L(r + 3d)\nA, B \u223c N, u = 1, v = 0\nvB\u00aeuA\u00ae\ndr\nA, B \u223c N, u, v = 1\nvB\u00aeuA\u00ae(Vera)\nL(r + 3d)\nA, B \u223c N, u = 1, v = 0\nTable 1: Tied-LoRA configurations included in our\nstudy. The first column shows acronyms used to identify\neach Tied-LoRA configuration (i.e., method). Symbols\nwith subscript \u00ae indicate that it is shared across all layers\nand the color blue indicates that the parameter is frozen.\nFormulas for the number of trainable parameters in each\nconfiguration as a function of number of layers L, hid-\nden size d, and low-rank r are also provided.\n3.3\nSelective Training\nThrough the flexible framework that equation 1 of-\nfers, we are given the opportunity to investigate a\nrange training configurations. By selectively updat-\ning the components A, B, u, and v during the train-\ning process, we can generate a variety of method-\nological variations. These variations not only ex-\nhibit differences in parameter count, but they also\ndemonstrate distinct capabilities across a variety\nof tasks. This exploration allows us to investigate\nthe intriguing regime of extremely low-parameter\nand low-rank PEFT models. This is a key step to-\nwards the customization of models, enabling them\nto excel at specific tasks while maintaining a mini-\nmal parameter count. Our ultimate goal here is to\nharness the power of this methodology to create\nhighly efficient, task-specific models that achieve\nhigh performance with reduced complexity.\n4\nExperiments\nWe now turn to evaluating the different configu-\nrations possible within our Tied-LoRA paradigm.\nWhile vBuA(LoRA) and PEFT methods can be\nused to train models for general instruction follow-\ning (Sun et al., 2023; Lermen et al., 2023; Sun\net al., 2023), we focus our evaluations in a \u201ctask\ncustomization\u201d perspective, where each model is\ntrained on a specific task and is evaluated on a test\nset from the same task.\n4.1\nTasks & Datasets\nTo evaluate the performance of each Tied-\nLoRA configuration across diverse data settings,\nwe utilized the following types of tasks:\nBase\nModel\nMethod\nAvg. Score over tasks @ Rank r\n2\n8\n32\n128\n2B\nvBuA(LoRA)\n50.29 (100) \n51.41 (100) \n51.16 (100) \n51.22 (100) \nvB\u00aeuA\u00ae\n50.25 (41.7) \n50.77 (13.6) \n50.73 (6.5) \n50.95 (4.8) \nvB\u00aeuA\u00ae\n47.80 (4.2)\n49.59 (4.2)\n49.86 (4.2)\n44.24 (4.2)\nvB\u00aeuA\u00ae\n47.35 (3.1)\n48.32 (3.1)\n46.68 (3.1)\n31.94 (3.1)\nvB\u00aeuA\u00ae\n46.55 (3.1)\n47.52 (3.1)\n46.86 (3.1)\n31.41 (3.1)\nvB\u00aeuA\u00ae\n49.99 (38.6) \n49.77 (10.4) \n48.19 (3.4)\n46.00 (1.6) \nvB\u00aeuA\u00ae\n49.79 (1.0)\n49.51 (1.0)\n48.26 (1.0) \n45.98 (1.0)\nvB\u00aeuA\u00ae(Vera)\n49.92 (37.5)\n49.23 (9.4)\n47.99 (2.4)\n45.60 (0.6)\n7B\nvBuA(LoRA)\n58.05 (100) \n58.64 (100) \n57.98 (100) \n58.43 (100) \nvB\u00aeuA\u00ae\n57.23 (40.6) \n57.80 (12.5) \n57.93 (5.5) \n57.86 (3.7) \nvB\u00aeuA\u00ae\n55.59 (3.1)\n57.53 (3.1) \n57.47 (3.1) \n55.53 (3.1)\nvB\u00aeuA\u00ae\n54.79 (2.3)\n56.48 (2.3)\n54.90 (2.3)\n29.00 (3.3)\nvB\u00aeuA\u00ae\n54.93 (2.3)\n55.85 (2.3)\n55.09 (2.3)\n28.02 (2.3)\nvB\u00aeuA\u00ae\n57.02 (38.3) \n56.79 (10.2)\n56.60 (3.1)\n55.66 (1.4) \nvB\u00aeuA\u00ae\n57.01 (0.8)\n56.63 (0.8)\n56.45 (0.8)\n55.52 (0.8)\nvB\u00aeuA\u00ae(Vera)\n56.75 (37.5)\n56.44 (9.4)\n56.65 (2.3)\n55.38 (0.6)\nTable 2: Average scores across all tasks at each low rank value for the Tied-LoRA methods in comparison with LoRA.\nThe fraction of trainable parameters used by each method compared to vBuA(LoRA) is show in brackets. We\nindicate the first, second and third highest scoring method for each low-rank setting with  ,  and  , respectively.\nExtractive QA\nis a common task where the\nmodel is expected to \u201cread\u201d some relevant text (the\ncontext) and answer questions. The answers are\nusually exact sub-strings from the provided context.\nWe use SQuADv1 dataset (Rajpurkar et al., 2016)\nin our experiments. Since the official test split of\nthis dataset does not contain ground-truth answers,\nwe use the validation set as our test set. We create\na validation set comprising of a random sample of\n4800 examples extracted from the training set.\nSummarization\nis a central problem in NLP\nand several variations of summarization datasets\nhave been proposed. We employ the DialogSum\ndataset (Chen et al., 2021) to study our models\u2019 per-\nformance on this task. DialogSum includes sum-\nmaries of real-word conversations on a diverse set\nof topics and scenarios. This dataset was an attrac-\ntive option as the length of the conversations and\nsummarizes are within the context lengths (4096\ntokens) of the base language models.\nCommonsense Natural Language Inference\n(NLI)\nis a task designed to probe the ability\nof language models to apply \u201ccommonsense rea-\nsoning\u201d to choose a possible ending for a given\nsituation described in natural language.\nThese\ntasks are typically trivial for humans but language\nmodels can still struggle. We use the HellaSwag\ndataset (Zellers et al., 2019) to study the perfor-\nmance of our proposed models on this type of task.\nAs HellaSwag contains multiple-choice questions,\nit can be viewed as a classification problem.\nTranslation\nMachine translation is a natural lan-\nguage generation task which is widely used in re-\nsearch and industry. Translation is inherently mul-\ntilingual and thus offers a challenging domain to\nstudy our Tied-LoRA paradigm. There are sev-\neral large scale translation datasets but we focus\non a moderately sized IWSLT 2017 German-to-\nEnglish translation dataset (Cettolo et al., 2017).\nThe dataset contains translation of spoken language\ninto various other natural languages. With over\n206k training examples this is the largest dataset\nthat we study.\nMathematical Reasoning\nis a challenging do-\nmain where large language models still lag behind\nhuman performance. Using PEFT methods on such\ntasks further amplifies these challenges as there are\nvery few trainable parameters. In our experiments,\nwe use the GSM8K benchmark (Cobbe et al., 2021)\nwhich contains 8.5K high-quality, grade-school\nlevel math word problems. Each example in the\nGSM8K benchmark contains a question and an\nanswer. The answers are provided with natural lan-\nguage solutions which contain explanations of each\nstep used to obtain the final answer. The final nu-\nmerical answer is demarcated from the rest of the\nnatural language solution. We evaluate our models\nby comparing these final numerical answers.\n21\n23\n25\n27\n78\n80\n82\n84\nAcc.\n(a) squad,2B\n21\n23\n25\n27\n84\n86\n88\nAcc.\n(b) squad,7B\n21\n23\n25\n27\n36\n37\n38\nRougeL\n(c) dialogsum,2B\n21\n23\n25\n27\n38\n39\n40\n41\nRougeL\n(d) dialogsum,7B\n21\n23\n25\n27\n60\n70\n80\nAcc.\n(e) hellaswag,2B\n21\n23\n25\n27\n84\n86\n88\n90\n92\nAcc.\n(f) hellaswag,7B\n21\n23\n25\n27\n38\n39\n40\nBLEU\n(g) iwslt2017,2B\n21\n23\n25\n27\n39\n40\n41\nBLEU\n(h) iwslt2017,7B\n21\n23\n25\n27\n4\n6\n8\n10\n12\nAcc.\n(i) gsm8k,2B\n21\n23\n25\n27\n25\n30\nAcc.\n(j) gsm8k,7B\nvBuA(LoRA)\nvB\u00aeuA\u00ae(Vera)\nvB\u00aeuA\u00ae\nvB\u00aeuA\u00ae\nvB\u00aeuA\u00ae\nvB\u00aeuA\u00ae\nvB\u00aeuA\u00ae\nvB\u00aeuA\u00ae\nFigure 2: Plots showing the performance of the tied-Lora configurations along with the baseline vBuA(LoRA) for\n5 diverse tasks at 4 different values for low-rank dimension setting. Note that we let the plot for vB\u00aeuA\u00ae and\nvB\u00aeuA\u00ae go out of bounds to show details for the other curves.\n4.2\nBase Language Models\nAlthough PEFT enables the base language model\nto perform new tasks, the final performance heavily\ndepends on the inherent abilities learned during pre-\ntraining. This necessitates investigating the perfor-\nmance of Tied-LoRA on multiple base models with\ndifferent inherent capabilities. Therefore, we use a\nrelatively small two billion parameter, GPT-2B-001\nmodel1 released by NVIDIA and the moderately\nlarge 7B LLaMA 2 model (Touvron et al., 2023)\nreleased by Meta.\nIn addition to the size differences, these models\nalso differ in the amount of pretraining data used.\nThe GPT-2B-001 model was trained on 1.1 trillion\ntokens of text from publicly available multilingual\ntext spaning 53 languages. The LLaMA2 7B model\nwas trained on 2 trillion tokens of predominately\nEnglish text. Both models are auto-regressive lan-\nguage models with a context size of 4096 tokens.\n4.3\nImplementation Details\nWe use the open-source NeMo Framework to im-\nplement all the algorithms presented in this paper.\nOur implementation is publicly available through\nthe NeMo GitHub repository.2 All training routines\nwere run for 2k max steps, but training was termi-\nnated sooner using early stopping with a patience\nof 10 to prevent overfitting. We trained all configu-\nrations using AdamW optimizer (Loshchilov and\nHutter, 2017) with a weight decay of 0.01 and a co-\nsine learning rate schedule with 50 warm-up steps.\nFor each Tied-Lora method we tried two learning\nrates, a high rate of 1e \u2212 4 and a low learning rate\nof 1e\u22125. While the \u201ctypical\u201d range of the low-rank\ndimension r is 4 \u2212 16 we find that some complex\ntasks benefit from higher r so we trained all our\nmodels with a wide range of r \u2208 {2, 8, 32, 128}.\nEach task was trained with a global batch size of\n256 and a validation check interval of 30 steps. The\nonly exception was the IWSLT translation dataset\nfor which we set global batch size and validation\ncheck interval of 1024 and 60 respectively. No\nextensive hyper-parameter search was conducted.\nDuring inference, we used greedy-decoding to\ngenerate the models\u2019 predictions with a limit of\n500 tokens.\n1https://huggingface.co/nvidia/GPT-2B-001\n2https://github.com/NVIDIA/NeMo/tree/\nadithyare/vera\n5\nResults\nTable 2 shows average scores attained by each\nTied-Lora configuration over the 5 tasks, per\nlow-rank value.\nWe can immediately see that\nvBuA(LoRA)\nis the best performing model\nfor both the 2B and 7B base language models.\nThis is hardly surprising as vBuA(LoRA) is\nthe most expensive method which does not use\ntied weights.\nWith this in mind we see that\nvB\u00aeuA\u00ae is a consistently the next best perform-\ning method with average scores comparable to\nvBuA(LoRA) , demonstrating the efficacy of\nweight tying. vB\u00aeuA\u00ae however does not per-\nform as well suggesting that the scaling vectors u\nand v provide an additional boost in performance\nespecially as the rank r is increased to 128 (at the\ncost of more trainable parameters).\nNext\nbest\nTied-Lora\nconfiguration\nis\nvB\u00aeuA\u00ae\nwhich obtains third place for 6\nout of the 8 settings shown in Table 2.\nNote\nthat vB\u00aeuA\u00ae beats other Tied-LoRA meth-\nods which use more parameters.\nInterestingly,\nvB\u00aeuA\u00ae(Vera) which uses fewer parameters than\nvB\u00aeuA\u00ae and vB\u00aeuA\u00ae has better performance.\nvB\u00aeuA\u00ae and vB\u00aeuA\u00ae does the worst in most\ncases, especially when r is increased.\nFigure 2 shows the performance for each\ntask individually.\nWe see that for tasks like\nHellaSwag and SQuAD Tied-LoRA methods\n(vB\u00aeuA\u00ae and vB\u00aeuA\u00ae specifically) are virtu-\nally the same as vBuA(LoRA) over the entire\nrange of ranks, fewer parameters. vB\u00aeuA\u00ae for\nexample, only uses 4.2% and 3.1% of parameters\nthat vBuA(LoRA) uses for the 2B and 7B models,\nrespectively.\nOn the flip side tasks like GSM8K seem\nto benefit from the additional parameters pro-\nvided by vBuA(LoRA) . A similar gap between\nvBuA(LoRA) and Tied-LoRA methods can be\nseen for the translation task as well especially in\nthe 2B model. We hypothesize that tasks in which\nthe base language model already performs well can\nbe easily enhanced by Tied-Lora, while tasks that\nare not \u201cnatutal\u201d to the base model (like mathemat-\nical reasoning) requires more parameters.\nAgain, we can see that in Tied-LoRA methods\nthe addition of untied parameters u and v are most\nhelpful as the r is increased. This suggests that the\nuntied parameters act as a per layer \u201cadjustment\u201d\nin the Tied-LoRA paradigm. We also see that it is\nbest to either train both A and B or just freeze B\nand train A (with untied weights u and v when ap-\nplicable). Lastly, we see that in the specific cases of\nvB\u00aeuA\u00ae and vB\u00aeuA\u00ae there is extreme instabil-\nity when r is increased. This pattern is consistent\nacross all the tasks we studied.\n6\nConclusion & Future Work\nWe have presented our Tied-Lora paradigm of ex-\ntending the parameter efficiency of Lora by us-\ning simple technique of weight-tying and selec-\ntive training of low-rank matrices. Our study sug-\ngests that for several tasks vB\u00aeuA\u00ae configura-\ntion can perform as well as Lora (over a range of\nlow-rank dimensions) with just 13% of the param-\neters of Lora when r is within the typical setting\nof 8. Increasing to larger r result is more aggres-\nsive reduction of trainable parameters compared to\nvBuA(LoRA) . This is especially true for tasks\nwhich the base language model already has some\nabilities, such as commonsense NLI, extractive\nQA and summarization. Given that the baseline\nabilities of LLMs are consistently improving with\neach iteration of LLMs, we hope that our best Tied-\nLoRA configuration can be used as a replacement\nfor LoRA for more tasks in the future.\nReferences\nMauro Cettolo, Marcello Federico, Luisa Bentivogli,\nJan Niehues, Sebastian St\u00fcker, Katsuhito Sudoh,\nKoichiro Yoshino, and Christian Federmann. 2017.\nOverview of the IWSLT 2017 evaluation campaign.\nIn Proceedings of the 14th International Conference\non Spoken Language Translation, pages 2\u201314, Tokyo,\nJapan. International Workshop on Spoken Language\nTranslation.\nArnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing,\nand Zhiqiang Shen. 2023.\nOne-for-all: General-\nized lora for parameter-efficient fine-tuning. arXiv\npreprint arXiv:2306.07967.\nYulong Chen, Yang Liu, Liang Chen, and Yue Zhang.\n2021. DialogSum: A real-life scenario dialogue sum-\nmarization dataset. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 5062\u20135074, Online. Association for Computa-\ntional Linguistics.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790\u20132799. PMLR.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nDawid Jan Kopiczko,\nTijmen Blankevoort,\nand\nYuki Markus Asano. 2023.\nVera:\nVector-\nbased random matrix adaptation.\narXiv preprint\narXiv:2310.11454.\nSimon Lermen, Charlie Rogers-Smith, and Jeffrey\nLadish. 2023. Lora fine-tuning efficiently undoes\nsafety training in llama 2-chat 70b. arXiv preprint\narXiv:2310.20624.\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2020. Exploring versatile generative language model\nvia parameter-efficient transfer learning. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 441\u2013459, Online. Association\nfor Computational Linguistics.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin A Raf-\nfel. 2022. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Ad-\nvances in Neural Information Processing Systems,\n35:1950\u20131965.\nIlya Loshchilov and Frank Hutter. 2017.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9,\nKyunghyun Cho,\nand Iryna Gurevych. 2021.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n487\u2013503, Online. Association for Computational Lin-\nguistics.\nOfir Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 157\u2013163, Valencia, Spain.\nAssociation for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nAndreas R\u00fcckl\u00e9, Gregor Geigle, Max Glockner, Tilman\nBeck, Jonas Pfeiffer, Nils Reimers, and Iryna\nGurevych. 2021. AdapterDrop: On the efficiency\nof adapters in transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7930\u20137946, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nXianghui Sun, Yunjie Ji, Baochang Ma, and Xian-\ngang Li. 2023. A comparative study between full-\nparameter and lora-based fine-tuning on chinese in-\nstruction data for instruction following large language\nmodel. arXiv preprint arXiv:2304.08109.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791\u20134800, Florence,\nItaly. Association for Computational Linguistics.\nQingru Zhang, Minshuo Chen, Alexander Bukharin,\nPengcheng He, Yu Cheng, Weizhu Chen, and\nTuo Zhao. 2023.\nAdaptive budget allocation for\nparameter-efficient fine-tuning.\narXiv preprint\narXiv:2303.10512.\n"
  },
  {
    "title": "ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks",
    "link": "https://arxiv.org/pdf/2311.09835.pdf",
    "upvote": "7",
    "text": "ML-BENCH: LARGE LANGUAGE MODELS LEVERAGE\nOPEN-SOURCE LIBRARIES FOR MACHINE LEARNING\nTASKS\nYuliang Liu\u2217\u2663\nXiangru Tang\u2217\u2660\nZefan Cai\u2217\u2661\nJunjie Lu\nYichi Zhang\nYanjun Shao\nZexuan Deng\nHelan Hu\nZengxian Yang\nKaikai An\nRuijun Huang\nShuzheng Si\nSheng Chen\nHaozhe Zhao\nZhengliang Li\nLiang Chen\nYiming Zong\nYan Wang\nTianyu Liu\nZhiwei Jiang\nBaobao Chang\nYujia Qin\nWangchunshu Zhou\nYilun Zhao\u2660\nArman Cohan\u2660\nMark Gerstein\u2660\n\u2660 Yale University\n\u2663 Nanjing University\n\u2661 Peking University\nxiangru.tang@yale.edu\nABSTRACT\nLarge language models have shown promising performance in code generation\nbenchmarks. However, a considerable divide exists between these benchmark\nachievements and their practical applicability, primarily attributed to real-world\nprogramming\u2019s reliance on pre-existing libraries. Instead of evaluating LLMs to\ncode from scratch, this work aims to propose a new evaluation setup where LLMs\nuse open-source libraries to finish machine learning tasks. Therefore, we propose\nML-BENCH, an expansive benchmark developed to assess the effectiveness of\nLLMs in leveraging existing functions in open-source libraries. Consisting of\n10,040 samples spanning 130 tasks over 14 notable machine learning GitHub\nrepositories. In this setting, given a specific machine learning task instruction and\nthe accompanying README in a codebase, an LLM is tasked to generate code to\naccomplish the task. This necessitates the comprehension of long and language-\ncode interleaved documents, as well as the understanding of complex cross-file code\nstructures, introducing new challenges. Notably, while GPT-4 exhibits remarkable\nimprovement over other LLMs, it manages to accomplish only 39.73% of the tasks,\nleaving a huge space for improvement. We address these challenges by proposing\nML-AGENT, designed to effectively navigate the codebase, locate documentation,\nretrieve code, and generate executable code. Empirical results demonstrate that\nML-AGENT, built upon GPT-4, results in further improvements. Code, data, and\nmodels are available at https://ml-bench.github.io/.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in function-level code\ngeneration, producing sophisticated code snippets with remarkable performance (Li et al., 2022;\nAustin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Bui et al., 2023; Tang et al., 2023a;\nFried et al., 2022; Chen et al., 2023c) in existing code generation benchmarks These models have\nbeen progressively used as powerful language agents, equipped to undertake various programming-\nrelated tasks (Liu et al., 2023b; Zhou et al., 2023; Shinn et al., 2023; Xie et al., 2023). Despite\nthese significant strides, a notable gap remains between the competencies exhibited by these models\nunder controlled experimental conditions and the dynamic requirements of real-world programming\nscenarios (Jimenez et al., 2023).\nExisting code generation benchmarks often evaluate LLM\u2019s ability to write code from scratch.\nHowever, practices in programming seldom demand the creation of all code elements from scratch.\n\u2217YL, XT, and ZC contribute equally, while XT leads this project. The remaining contribute to data annotation.\narXiv:2311.09835v1  [cs.CL]  16 Nov 2023\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nFigure 1: ML-BENCH collects data from 14 machine learning GitHub repositories through a three-\nstep pipeline: Extracting readme information, automatically generates instructions, and forms stan-\ndardized outputs. Furthermore, we introduce the ML-AGENT to accomplish proposed task, and then\nevaluated it on ML-BENCH against the currently highly robust language model.\nReal-world programming often involves employing pre-existing and open-source libraries to ac-\ncomplish tasks effectively and efficiently (Zan et al., 2023). These evolved libraries offer robust,\nbattle-tested solutions to a broad range of problems. Therefore, measuring the efficacy of code LLMs\nshould not merely be restricted to function generation (Shrivastava et al., 2023; Liu et al., 2023a;\nZhang et al., 2023). It should also encompass their proficiency in executing programming files from\nopen-source libraries with precise parameter use.\nThis brings to light a more challenging, but more practical target for code LLMs - generating code\nthat can successfully execute programming files and utilize resources with parameters accurately from\nopen-source libraries. This target often demands not just code generation, but also comprehension\nof natural language documentation appended to the code, such as README files. README files\ntypically provide a walkthrough of the open-source library, offering detailed narratives on different\ntask examples that the library can handle, including guidance on parameter selection. In reality, the\nparameters stipulated by the user may not always align with those enumerated in the README\nexamples, mandating the LLM to accurately adapt the parameter values based on the user\u2019s specific\nrequirements. Hence, an LLM with the ability to effectively interpret, leverage the information within\nREADME files, and precisely customize parameters in code based on README examples, would\nsignificantly advance machine learning automation.\nWe introduce ML-BENCH, a comprehensive and realistic benchmark dataset designed to evaluate\nLLMs and their capabilities in understanding user instructions, navigating GitHub repositories, and\nwriting executable code. ML-BENCH provides high-quality instructions along with corresponding\nexecutable ground truth code that meets the requirements specified in the instructions. ML-BENCH\nconsists of 9444 samples spanning 130 tasks over 14 notable machine learning GitHub repositories.\nIn the experiments, we adopt metrics such as Pass@k and Parameter Hit Precision and delve deeper\ninto the capabilities of GPT-3.5-16k, GPT-4-32k, Claude 2, and CodeLlama in ML-BENCH setups.\nML-BENCH proposes new challenges to LLMs. Empirically, we find that GPT models and Claude 2\nperformed significantly better than CodeLlama. A key insight from our study is the crucial need for\nLLMs to comprehend long-text documentation, not just generate code. Based on our error analysis,\nour primary technical endeavor in this work is the proposition of the ML-AGENT, an autonomous\nlanguage agent developed to fill the gaps highlighted earlier. These agents are empowered to\nunderstand natural language and instructions, to produce requisite code effectively, and to solve\ncomplex tasks.\n2\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nOur findings and contributions can be summarized as:\n\u2022 We propose a novel task that requires LLMs to comprehend long-context documents, navi-\ngate codebases, understand instructions, and generate executable code. We propose ML-\nBENCH, a benchmark to evaluate the effectiveness of LLMs in utilizing existing libraries\nand performing machine learning tasks.\n\u2022 We carefully provide various settings to accommodate different LLMs (i.e., closed-source\nLLMs, open-source LLMs, and agents) Additionally, we carefully design comprehensive\nand fair evaluation metrics to quantify the performance of LLMs.\n\u2022 We conduct comprehensive evaluations across settings and popular LLMs. Experiments\nshow that GPT-4 performs remarkable improvement over other LLMs, but still only manages\nto accomplish 39.73% of the tasks. Other popular LLms suffer from hallucinations and\nperform poorly. Based on error analysis, we further propose ML-AGENT to address the\nchallenging tasks.\n2\nDATASET AND BENCHMARK\nIn this section, we provide detailed information about the task formulation, selected repositories and\ndata annotation pipeline of the benchmark. In Section 2.1, we provide a clear definition of the task.\nIn Section 2.2, we explain how we choose the selected 14 GitHub repositories. In Section 2.3, we\ndescribe the annotation pipeline of dataset construction. Finally, in Section 2.4, we present some\nstatistics regarding the dataset.\n2.1\nTASK FORMULATION\nIn our benchmark, we focus on a scenario where, given a GitHub repository F, the language model\nhas access to all files f \u2208 F within the repository. Upon receiving a user instruction i with a set\nof specific parameters argi \u2208 A, the ML-AGENT is required to write code c that invokes models\nor functions from the GitHub repository. We emphasize that the generated code c must align with\nthe user\u2019s instruction i, particularly in terms of all specified parameters argi \u2208 A, and must be\nexecutable. This approach ensures that the model\u2019s output meets the user\u2019s needs and adheres to\npractical application standards.\n2.2\nREPOSITRIES SELECTION\nWe collected all papers and their corresponding GitHub links from the PapersWithCode website 1.\nSubsequently, we organized these papers based on their categorical information (i.e., tasks, models,\ndatasets, modalities) From each category, we selected the top repositories based on the ranking\nof stars. Further, through manual filtering, we identified the most representative and feature-rich\nrepositories from this subset, finally making our large group of 14 repositories. More details about\nthe selected repositories can be found in Sec. 2.4 and Appendix A.\n2.3\nANNOTATION PIPELINE\nAbout data annotation, relying solely on human efforts can be a time-intensive and labour-intensive\nprocess. To address this challenge, we have developed a comprehensive workflow with the help\nof LLMs to facilitate the dataset annotation process. Our annotators consist of undergraduates and\ngraduate students with proficient programming skills. Each GitHub repository is assigned to a\ndesignated annotator. 2\n1https://paperswithcode.com/\n2The minimum hourly wage in the United States is near $7.5, which can be found at https://www.wo\nrker.gov/. On average, annotating an example takes 5 hours. Consequently, the cost per annotator amounts\nto $37.5.\n3\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nFigure 2: The data construction of ML-BENCH, encompassing 14 prominent GitHub repositories\nand aggregating a total of 10,040 samples\n.\nREADME Page Selection\nWhen users utilize a GitHub repository to accomplish tasks, the\nREADME file is often the most valuable resource. The README effectively demonstrates the\nusage of various codes, functions and classes contained within the repository. Therefore, we instruct\nthese annotators to carefully review the contents of the repository first. Their primary task includes\nidentifying all README files within the repository. This encompasses not only the main README\nbut also those located in various sub-directories, which describe the contents of their respective\nfolders. Although more than half of GitHub repositories contain only a single README file, there\nare instances where a repository may include numerous README files. Each of these files typically\ndocuments only a fraction of the repository\u2019s functionality. In average, each GitHub covers 15\nREADME pages, while GNN covers 154 README pages. 3\nTask Mining\nAfter identifying all the README files contained within a GitHub repository,\nour annotators are tasked with pinpointing each task covered by every README, along with the\ncode scripts capable of accomplishing these tasks. The anotators are instructed to select the most\nrepresentative and practical tasks inside each GitHub. In average, each GitHub repository covers 9\ntasks.\nParameter Extraction\nIn addition to specifying the task, users may also need to define impor-\ntant parameters, as the setting of these parameters is crucial to the effectiveness of finishing the\ntask. Therefore, upon identifying the tasks and corresponding code within the README files, the\nannotators are further tasked with locating key parameters in the code (such as batch size, epochs,\nmodel architecture, and dataset). Annotators all have experience in machine learning or deep learning\ndevelopment, they are guided to select the most important parameters and corresponding candidate\nvalues that they will search in machine learning development. We aim for these parameters to be\nsufficiently representative, encapsulating essential parameters that engineers are likely to search and\ntest during practical experiments.\nMulti-Parameter Combination\nOnce the code and its pertinent parameters for each task were\nidentified, we instructed the annotators to iteratively and systematically combine candidate values for\n3https://github.com/pyg-team/pytorch_geometric\n4\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nArgument combination 1\nwiki_cs, 128, , 5e-3\n\u2026\u2026\nArgument combination N\namazon_photos, 256, 5e-1\nThis DGL example implements the GNN experiment proposed in the \npaper Large-Scale Representation Learning on Graphs via Bootstrapping. For the \noriginal implementation, see here(URL). \nRequirements\nThe codebase is implemented in Python 3.8. For version requirement of \npackages, see below.\n\u2022\ndgl 0.8.3\n\u2022\nnumpy 1.21.2 \u2026\nDataset\nDataset summary:\nStep1: ReadMe Page Selection\nStep3: Argument Extraction\nDataset: wiki_cs, amazon_photos, amazon_computers, coauthor_cs ...\nGraph Encoder Layer: 64, 128, 256, 512, 1024 \u2026\nLearning Rate: 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1 \u2026\nStep4: Multi-Argument Combination\nStep5: Diverse Instruction Construction\nStep6: Ground Truth Code Construction\nIn Selected Argument:\nHuman Instruction: I have a dataset entitled wiki_cs that I'm eager to \nutilize for training purposes. Specifically, I'm aiming to employ this \ndataset to train a cutting-edge BGRL Model. To achieve optimal \nperformance, I intend to utilize graph encoder layer 128, alongside a \nlearning rate (lr) of 0.005. Could you kindly assist me in crafting the \nnecessary code to accomplish this task?\u201d\npython main.py \\\n--dataset wiki_cs \\\n--graph_encoder_layer 128 \\\n--drop_edge_p 0.3 \\\n--feat_mask_p 0.25 \\\n--lr 5e-3\nDataset\nTask\nNodes\nEdges\nFeatures\nWikiCS\nTransductive\n11,701\n216,123\n300\nAmazon Computers\nTransductive\n13,752\n245,861\n767\nAmazon Photos\nTransductive\n7,650\n119,081\n745\nCoauthor CS\nTransductive\n18,333\n81,894\n6,805\nStep2: Task Mining\nSelected Argument :\nwiki_cs,\n128\n5e-3\nREADME path: dgl/examples/pytorch/bgrl/README.md\nInductive Task\npython main.py \\\n--dataset coauthor_cs \\\n--graph_encoder_layer 1024 \\\n--drop_edge_p 0.3 \\\n--feat_mask_p 0.25 \\\n--lr 5e-2\nFigure 3: The construction pipeline of the ML-BENCH. Step1: Find the readme file in the repository\nthat implements a specific task. Step2: Essential task-related information is discerned. Step3: Extract\nparameters from task mining. Step4: Leveraging the details extracted from the readme file, multiple\ncombinations of parameters are formulated. Step5: Creating the instructions for each parameter\ncombinations. Step6: Substitutes the code or script within the readme, serving as the ground truth\ncode for ML-BENCH.\nselected parameters. This combinatorial approach resulted in numerous combinations of parameters\nthat are relevant to each task. For every unique combination of parameters, we employed templates to\nconstruct code scripts and instructions that satisfy the requirements of these parameter combinations.\nDiverse Instruction Construction\nFor each set of parameters obtained during the Multi-Parameter\nCombination process, we utilized ChatGPT to generate instructions specifically tailored to the task\nand these parameters. We meticulously design templates to ensure that output instructions contain\nthe given parameter requirements and exhibit a strong diversity by providing specific parameter\nindications and various diverse examples.\nGround Truth Code Construction\nBased on the parameter combinations obtained during the\nMulti-Parameter Combination process, we further produce a code template based on the code for\ntarget task. Then we use the code template to construct ground truth code.\nQuality Test\nWe have conducted rigorous quality checks on the dataset. Specifically, for the ground\ntruth output, we perform strict checks for executability and meeting parameter requirements. Any\ndata that did not meet the requirements were discarded. Regarding the instructions, we conducted\nchecks to ensure that all parameters were explicitly specified within the instruction. Any instructions\nthat did not meet the requirements were manually corrected.\n2.4\nOVERALL STATISTICS\nReferring to the ML-BENCH data construction process detailed in Section 2.3, we carefully curated\na GitHub repository tailored to prominent Machine Learning domains, serving as the cornerstone\nof our ML-BENCH dataset. This extensive dataset is partitioned into separate training and testing\nsubsets, designed for distinct purposes in training and evaluation. Table 1 delineates the specific data\nstatistics for each subset. Further elaboration on the detailed data metrics for each repository utilized\ncan be found in Appendix 10.\n5\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nRaw README\nOracle Segment\nBM25\nTrain\nAverage token length\n8009.42\n151.32\n492.90\nMax token length\n20701\n851\n5540\nOOD Train\nAverage token length\n4118.90\n186.04\n585.80\nMax token length\n8363\n367\n1339\nTest\nAverage token length\n8331.47\n1275.22\n1102.49\nMax token length\n20655\n5420\n3192\nTable 1: The average token length and maximum token length of README files across three settings:\nTrain, Out-of-Distribution (OOD) Train, and Test settings within the ML-BENCH.\nThe comprehensive statistics of ML-BENCH are itemized in Table 2, which culled from our selected\nGitHub repositories, offering statistical insights into the diverse range of instructions we meticulously\ndeveloped.\n3\nEXPERIMENTS\nIn this section, we predent details about the experiment. In Sec. 3.1, we describe the experimental\nsetup of ML-BENCH, along with the associated evaluation metrics. In Sec. 3.2, we introduce the\nmodel used in the experiments. In Sec. 3.3, we present how we formulate the dataset into input for\nLLms. In Sec. 3.4, we present the details of evaluation setting and metrics. In Sec. 3.6 , we present\nresults of popular models in ML-BENCH.\n3.1\nEXPERIMENTAL SETUP\nOur input includes both human instructions for the task and README files. We have three exper-\nimental setups, each differing in terms of how the content of README files are presented. The\npurpose of controlling the input content\u2019s form is to replicate real-world situations. With manual\nannotation, we identify the relevant segments including code and texts inside the README files\nthat should be utilized to finish the task, which we term as \"Oracle Segments\". This segment should\nprovide all supporting evidence required to produce the ground truth code. In addition, we offer two\nsimulated scenarios: one provides the complete README file, while the other uses a BM25 retriever\nto extract task-related segments from the README file.\nHere are the detailed descriptions of the three settings:\nTask\nNumber\n- GNN\n608\n- Text\n1475\n- Molecular\n649\n- Image-GAN\n1189\n- Text and image\n3646\n- Video\n75\n- Time-series\n1478\n- Attention Usage\n127\n- Medical\n805\nAverage token length per instruction\n70.4\nMax token length in instruction\n216\nInstruction edit distance among the same task\n238.4\nInstruction edit distance across tasks\n300.1\nTable 2: The statistics of ML-BENCH.\n6\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nOracle Segment: Our annotators keep a record of the corresponding segments for the task in the\nREADME file when writing the ground-truth code. Typically, this process involves initially locating\nan example code for a task within the README file and subsequently altering its parameters to yield\na new task and its respective ground-truth code. An Oracle Segment usually consists of a task-related\ncode example coupled with its natural language explanation in the README file.\nRaw README: In this setting, we present the entirety of the README content to the model without\nany modifications. If the length of the README exceeds the model\u2019s maximum context limit, the\npart of the text that falls outside this window at the end will be truncated.\nBM25 Retrieval: We employ a BM25 retriever to assist the model in locating relevant information\nwithin the README. The BM25 retriever deploys the NLTK sentence tokenizer 4, and we have set a\nuniform retrieval span of 10 sentences. The average length of an Oracle Segment is 9.5 sentences,\nsuggesting that the majority of relevant information can be found within this range. Also, README\nfiles have an average sentence length of 106, so limiting the retrieval window to 10 sentences helps\nmanage the overall data volume by focusing on the most important sections.\n3.2\nMODEL\nThe maximum length of the README files is 14.9k tokens (based on the gpt-3.5-turbo token\ngenerator) based on tiktoken tool5. To ensure that we did not exceed this limit, we utilized GPT-3.5-\n16k, GPT-4-32k, and Claude 2 models.\nTo examine the performance of open-source models, we chose CodeLlama6 (CodeLlama-7b-Instruct)\nfor its exceptional ability in code comprehension. We fine-tuned CodeLlama on our dataset to enhance\nits performance in our specific use cases.\n3.3\nINPUT FORMAT\nFor GPT-3.5, GPT-4, and Claude 2, the input includes the system prompt, human instruction, and the\nchosen README content.\nTo facilitate a user-friendly testing framework, we specifically chose 10 repositories that are easy\nto use as the base bench repositories. We sampled 25-30 examples each from annotated data in 10\nrepositories to serve as our test set. We refer to these repositories as the base bench repositories.\nFor fine-tuning CodeLlama, we additionally prepared the training set. To validate the effectiveness of\nfine-tuning under two scenarios, we define two setups: the ID (In-Distribution) setting and the OOD\n(Out-of-Distribution) setting.\nGiven that we have a total of 14 repositories, 10 of which have already been sampled for the test set,\nwe use the left 4 repositories without overlaps with test set.\nFor the ID (In-Distribution) setting and OOD (Out-of-Distribution) setting: we sample from the base\nbench repositories (excluding examples from the test set) for the training set. Meanwhile, repositories\nnot chosen for the base bench repositories were sampled for the training set in the OOD setup.\n3.4\nEVALUATION METRICS\nFor evaluation, the output code needs to not only be executable but also aligned with the parameter\nrequirements stated in the instruction, we employ two evaluation metrics to assess the output generated\nby the models: Pass@K and Parameter Hit Precision.\nPass@K refers to the probability that the correctly executed code is generated. Parameter Hit\nPrecision(PHP) focuses on how good the model is able to identify and output the code taking care of\nthe correct parameters specified in the instruction. During our data annotation process, we instruct\n4https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html\n5https://github.com/openai/tiktoken\n6Due to hardware limitations, CodeLlama\u2019s input is truncated to the first 8k tokens for training and inference.\n7\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\n[readme content]:\n DGL is framework agnostic, meaning if a \n deep graph model is a component of an \n end-to-end application, the rest of the \n logics can be implemented in any major \n frameworks, such as PyTorch, Apache \n MXNet or TensorFlow....\n[instruction]:\n I am eager to utilize the Citeseer \n dataset as the training data to empower \n the ARMA Model with the learning rate \n set to a commendably small value of \n 0.0001. Additionally, I'd like to \n incorporate 5 stacks into this model. \n Your assistance in formulating the \n necessary code to accomplish this task \n would be of tremendous help.\n[System Prompt]:\n You are given [readme content], you need \n to carefully see [readme content] and \n choose wirte code or script to implement \n my [instruction].\n Please output code or script directly, \n use markdown to output code without \n explanation.\nModel Input  \nEvaluation Result\n \"python citation.py --dataset Citeseer --lr 0.01 --stacks 5\",\n \n \"python main.py --dataset citeseer --lr 0.0001 --stacks 5\",\n \n \"dgl-go --model=arma --dataset=citeseer --lr=0.0001 --stacks=5\",\n \n \"import dgl\\ndataset = dgl.data.CiteseerGraphDataset()\\ng = \n dataset[0]\\nfrom dgllife.model import DAGNNPredictor ...\", \n \"python citation.py --dataset Citeseer --lr 0.0001 --num-stacks 5\"\nGenerated Result\n python citation.py --dataset Citeseer --lr 0.0001 --num-stacks 5 \nGold Output\nFAILED      python citation.py --dataset Citeseer --lr 0.01 --stacks 5                 wrong argument\n \nFAILED      python citation.py --dataset citeseer --lr 0.0001 --stacks 5              no argument error\nFAILED      dgl-go --model=arma --dataset=citeseer --lr=0.0001 --stacks=5     no file error\nFAILED      import dgl\\ndataset = dgl.data.CiteseerGraphDataset()\\n ...           wrong execution\nPASSED    python citation.py --dataset Citeseer --lr 0.0001 --num-stacks 5     pass\nFigure 4: The illustrative set of input-output pairs, gold output, and execution results, accom-\npanied by precision metrics: Pass@1=0, Pass@2=0, and Pass@5=1. Various colors within the\ninstructions signify different parameters.\nannotators to record explicit parameter information contained within each input\u2019s human instruction.\nThese parameters are required to remain correct and consistent in the generated code.\nWe attempted the above settings to assess the repository using capability and report the results in\nTable 3 and Table 4.\n3.5\nEVALUATION ENVIRONMENT\nThe necessary models and input samples were downloaded and placed in their respective locations in\nthe evaluation environment. All execution environments are publicly released.\n3.6\nMAIN RESULTS\nTable 3 shows the results for Pass@K and Table 4 shows the results for PHP. We can observe that:\n1. In the Raw README setting, GPT-4 consistently demonstrates superior performance in\nachieving optimal results in the test set. Claude 2 only outperforms GPT-3.5 in the Pass@1\n6\n* Means evaluated on test set in ML-BENCH.\nTable 3: Pass@K (%) results for experiment in Section3. Pass@1, Pass@2, and Pass@5 scores are\nreported for models on the Raw README, BM25 Retrieval\nModels\nOracle Segment\nRaw README\nBM25 Retrieval\nPass@1\nPass@2\nPass@5\nPass@1\nPass@2\nPass@5\nPass@1\nPass@2\nPass@5\nGPT-4 *\n39.73\n47.94\n61.64\n28.77\n32.25\n50.69\n20.55\n21.92\n27.40\nGPT-3.5 *\n35.61\n41.10\n54.79\n19.18\n30.16\n42.47\n13.70\n21.92\n32.88\nClaude 2 *\n30.13\n41.10\n43.84\n24.66\n31.51\n37.99\n10.96\n15.07\n17.84\nGPT-3.5\n35.27\n51.03\n11.22\n17.81\n28.08\n40.76\n25.34\n19.86\n31.84\nClaude 2\n23.29\n32.53\n40.07\n22.97\n27.39\n31.51\n11.31\n13.70\n16.44\nCodeLlama\n8.78\n12.60\n27.86\n2.67\n6.87\n12.98\n4.20\n7.63\n16.79\nCodeLlama-ID\n16.78\n25.68\n35.27\n/\n/\n/\n3.77\n8.56\n14.38\nCodeLlama-OOD\n16.78\n21.57\n27.74\n/\n/\n/\n2.74\n4.11\n6.51\n8\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nTable 4: Parameter Hit Precision (%). The meaning of Parameter is to use parameter as the basic\nunit of statistics, and Generation is to use output as the basic unit of statistics.\nModels\nOracle Segment\nRaw README\nBM25 Retrieval\nParameter\nGeneration\nParameter\nGeneration\nParameter\nGeneration\nGPT-4 *\n93.03\n72.82\n95.47\n81.53\n90.24\n67.60\nGPT-3.5\n92.75\n71.71\n91.75\n68.63\n87.58\n56.85\nClaude 2\n89.76\n59.29\n91.11\n62.19\n87.67\n55.03\nCodeLlama-ID\n81.87\n52.22\n/\n/\n84.68\n53.67\nCodeLlama-OOD\n72.71\n41.70\n/\n/\n68.18\n30.83\nTable 5: The BLEU-1 scores between Oracle\nSegments and the BM25 Retrieval content.\nID-train\nOOD-train\nML-BENCH\nBLEU score\n0.0112\n0.0087\n0.0080\nTable 6: Cutoff date of training data for GPTs\nand Claude 2\nGPT-3.5\nGPT-4\nClaude 2\nCodeLlama\nDatetime\n2021.09\n2021.09\nEarly 2023\n2023.07\nscore in the full set. Additionally, the PHP scores favor GPTs too, further highlighting their\nsuperior performance.\n2. When presented with an Oracle Segment, the GPT models consistently demonstrate superior\nperformance compared to Claude 2. And the experimental results were all significantly\nimproved. Providing concise and precise initial code is advantageous for generating model\ncode.\n3. Predefined single-segment searches provide limited help for the task and can even have\nunintended consequences. Based on our observations, very few of the BM25-retrieved\nsegments fully covered the important segment. This can be attributed to both the lengthy\nOracle Segment and retrieval errors. We show the BLEU scores between Oracle Segments\nand BM25 searched content in Table 5\n4. CodeLlama\u2019s experimental results consistently exhibit lower values compared to those\nclosed-source models. Further discussion on CodeLlama\u2019s results is provided in the subse-\nquent sections.\n4\nANALYSIS\nIn this section, we analyze the performance results conducted on the above models, followed by our\ndiscussion about the difficulties and challenges of this task. We first talk about the data leakage in our\nexperiments in Sec. 4.1. In Sec. 4.2, we analyze the performance on each repository and errors for\nGPTs and Claude 2. In Sec. 4.3, we present the performance of one trainable open-source model on\nML-BENCH.\n4.1\nDATA LEAKAGE\nDuring our experiments, we discovered data leakage concerns due to the huge time span and widely-\nknown repositories selected. One interesting observation we made while experimenting with the\nBERT library using Claude 2 was that it performed remarkably well (80%) in all settings except\nfor the Oracle Segment setting (3.33%). When we used the Raw README setting, Claude 2 was\nunable to generate the correct Bash script as it relied on memorization instead of following human\ninstructions. However, in the other two settings where it was provided with the exact code segment,\nClaude 2 deviated from its memorization approach and generated an inaccurate Bash script based on\nthe Oracle Segment, resulting in suboptimal outcomes.\nSimilar phenomena are observed in the DGL library, where models also tended to deviate from\nexplicit instructions, failing to adhere strictly to the \"follow the README (Oracle Segment)\" system\nprompt when generating Bash scripts. Notably, this issue occurs less frequently in GPT-4. We\n9\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nTable 7: This table shows the Pass@5 scores of GPT-3.5 and Claude 2 on Oracle Segment, Raw\nREADME, and BM25 Retrieval settings in different GitHub repositories.\nRepos\nGPT 3.5\nClaude 2\nOracle Segment\nRaw README\nBM25 Retrieval\nOracle Segment\nRaw README\nBM25 Retrieval\nDGL\n64.00\n4.00\n12.00\n24.00\n4.00\n4.00\nBERT\n20.00\n10.00\n6.67\n3.33\n80.00\n13.33\nLavis\n66.67\n80.00\n56.67\n50.00\n3.33\n40.00\nIf\n70.00\n66.67\n53.33\n40.00\n6.67\n36.67\nvid2vid\n56.67\n23.33\n30.00\n60.00\n3.33\n0.00\nESM\n65.52\n48.28\n62.07\n68.97\n82.76\n20.69\nOpenCLIP\n33.33\n50.00\n40.00\n3.33\n60.00\n3.33\nTSL\n36.67\n26.67\n0.00\n70.00\n6.67\n13.33\nEAP\n67.86\n67.86\n31.14\n7.14\n35.71\n0.00\nPy-GAN\n33.33\n26.67\n16.67\n70.00\n30.00\n30.00\nTotal\n51.03\n40.76\n31.84\n40.07\n35.51\n16.44\nhave systematically classified these occurrences, wherein code generation from memory results in\ninaccuracies, as instances of hallucination errors.\nData leakage induces models to deviate from the expectation of generating code or scripts of the\nsame type as the README provided. Even when explicitly instructed in the PROMPT to adhere to\nthe provided information, models tend to overlook this directive. This occurrence carries significant\nimplications for both model performance and the potential for error classification.\nWe show the updating status for all repositories in Appendix A. And the cutoff date of training data\nfor GPTs and Claude 2 in Table 6.\n4.2\nCLOSED-SOURCE MODEL RESULTS ANALYSIS\n4.2.1\nREPOSITORIES ANALYSIS\nWe reported Pass@5 scores of GPT 3.5 and Claude 2 for all the ten repositories in ML-BENCH in\nTable 7, respectively. After that, we discussed these results about the characteristics of the different\nrepositories.\n1. In a majority of instances, the Oracle Segment exhibits optimal performance, followed by the\nRaw README, with the BM25 Retrieval consistently ranking as the least effective. Such\nas results on BERT, If, and TSL for GPT-3.5, and vid2vid for Claude 2. The improvement\nbetween the Raw README setting and the oracle setting is particularly notable including\nPy-GAN, characterized by an extensive README with a higher proportion of textual\ninformation compared to code details.\n2. When information is scattered across different sections of a README, the Oracle setting\nperforms worse than the Raw README setting. The Oracle Segment struggles to accurately\ncapture information that is distributed across multiple parts. For instance, when details about\nthe model and code are not presented together, the Oracle Segment may face challenges. A\ngood example of this is OpenCLIP\n3. We conduct a log analysis to understand the substantial disparity in results between Claude\n2 and GPT-3.5 across the two repositories: Lavis and If in the Raw README setting. In\nLavis, the model generates Python code based on the README, and Claude 2 occasionally\nmakes errors, such as misspelling \"from lavis.datasets.builders import load_dataset\" as \"from\nlavis.datasets import load_dataset\" in the documentation. These hallucination errors are\nprevalent in Claude 2 generated code, indicating a potential deviation from strict adherence\nto the provided documentation. If faces more issues with undefined variables in the Python\ncode. This might be attributed to the scattered code within this repository.\n4.2.2\nERROR ANALYSIS\nThe identified errors in this task fall into four categories: hallucination errors, lack of knowledge\nor information, knowledge manipulation, and syntax errors. We hope that analyzing these types of\n10\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nerrors in the context of the task could guide future development efforts for projects utilizing GitHub\nor similar platforms.\nHallucination errors: These errors include instances when the model has misinterpreted the user\u2019s\nintention, misplaced code script (like creating Python code when it was supposed to generate Bash\nscript), or generated random and irrelevant code.\nLack of knowledge or information: Under the current design, the background information is\nconsidered slightly weak. Possible types of errors are as follows:\n1. Code Inner Information. The model sometimes lacks sufficient information necessary to\nsatisfy the user\u2019s requirements. For instance, this deficiency might involve missing parameter\nnames (\u2013lr and learning rate), or unavailable parameter options (it only accepts \u2019Citeseer\u2019\nwhen the input given was \u2019citeseer\u2019).\n2. Model Domain Knowledge. The model sometimes lacks domain-specific knowledge required\nto appropriately handle certain instances. For example, in BERT, the model generated \u2013\ncase=True and \u2013model=uncased simultaneously, which can\u2019t be used together.\n3. Grammar knowledge This happens when the model doesn\u2019t correctly identify and handle\ncertain command line symbols. For instance, not recognizing that the $ symbol could affect\nexecution.\n4. Local data information For certain commands, the model did not supply enough crucial\nparameter information, leading to the inability to find paths and successful execution. This\nerror, while less common, was particularly notable in the OpenCLIP library. If the model\ncould read local data and find corresponding solutions, its performance could be enhanced.\nKnowledge manipulation: Take BERT, for instance, where the model needed to integrate\nDIR=/model/ and \u2013model_dir = $DIR to form \u2013model_dir =/model (as the user claimed). There\nwere also cases where it couldn\u2019t merge \"/model_name in /data\" into a proper path format like\n/data/model_name. Similar incidents were observed with OpenCLIP.\nSyntax errors: These errors cover instances of incorrect code generation of syntax errors instead of\nhallucination.\nUpon reviewing logs for Raw README settings for GPT-3.5 and Claude-2, we systematically\ncategorized the identified error terms in Figure 5. The figure reveals that:\n1. Hallucination errors constitute the largest portion in both models, followed by instances\nwhere models generate incorrect code, namely Syntax errors. Knowledge manipulation\nrepresents the smallest share, partially due to the limited dataset content that can capture\nthis type of error.\n2. Claude 2 exhibits a relatively higher prevalence of hallucination errors, as discussed earlier,\nstemming from its inclination to generate code based on its internal knowledge rather than\nstrictly adhering to the provided README files. In addition to the hallucination errors,\nmore errors in both models occurred in writing the wrong code itself, which suggests that\nthe model\u2019s foundational ability to modify the code according to the documentation leaves\nshould be desired.\n3. It should be noted that, owing to limitations in manpower, multiple errors may have occurred\nsimultaneously. However, the calculations presented here only highlight the most significant\nones, as identified through error logs.\n4.3\nOPEN-SOURCE MODEL RESULT ANALYSIS\n4.3.1\nRAW CODELLAMA ANALYSIS\nBy observing the result in Table 3 and Table 4, our experience with the standard versions of CodeL-\nlama has shown that while it has a good understanding of general code, it struggles to generate Bash\nscript or Python code that is specific to our needs. Even though presented with abundant relevant\ninformation and clear instructions, the generated code still suffers from hallucinations. For instance,\n11\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\n \n0\n10\n20\n30\n40\n50\n60\n70\nHallucination\nerrors\nLack of knowledge\nor information\nKnowledge\nmanipulation\nSyntax errors\nError Types %\nGPT-3.5\nClaude 2\nFigure 5: Error analysis by human annotation. We analyze 240 errors for GPT 3.5 and 220 errors for\nClaude 2\nwhen provided with clear instructions and an \"Oracle\" Bash script directly, CodeLlama still attempts\nto generate Python code based on its understanding.\n4.3.2\nFINE-TUNED RESULT ANALYSIS\nWe analyze the outcomes with a focus on Oracle Segment and limitations with BM25 Retrieval and\nRaw README settings here.\nOracle Segment Enhancement: During our experiments, we discovered that the only way to improve\nperformance in fine-tuned CodeLlama was by utilizing Oracle Segments. These segments consist\nof code snippets that provide a rich context for the model to learn from, whether in the ID or OOD\nsetting. The fine-tuning process enabled CodeLlama to develop a nuanced understanding of these\nOracle Segments, thereby enhancing its ability to extract and interpret relevant information efficiently.\nBM25 Limitations: One of the primary limitations of the BM25 algorithm is its inconsistency in\nincluding relevant segments in search results. Predefined single-segment searches often offer limited\nassistance and may even have unintended side effects. These segments are less informative and result\nin poor performance for training. As a result, we have observed a large number of CodeLlama\u2019s\nattempts to generate random code due to insufficient information.\nRaw README Limitations: We didn\u2019t fine-tune CodeLlama under Raw README setting. A\nconsiderable proportion of READMEs in the test set (45.3%) exceeded the token limit of our\nmaximum inference context length (8k), necessitating truncation for inference feasibility. This\ntruncation potentially led to extreme loss of critical information, impacting the quality of generating\nresults. We show the token number for each repository in Appendix B.\n5\nML-AGENT\n5.1\nGAPS BETWEEN MODEL AND HUMAN BEING\nThe performance gap, which is clearly evident when comparing the README contents of a repository\nin Raw README and BM25 Retrieval settings, as well as the Oracle Segment model in the Oracle\nSegment setting, underscores the inherent challenge that a singular model faces when attempting to\nnavigate a real-world GitHub repository without human intervention in a single iterative step. This\nhighlights the complexity of effectively managing such a task and emphasizes the need for further\nresearch and development to optimize the process.\nUpon this analysis, we have identified the following primary discrepancies between model-generated\nresults and actual human utilization of GitHub:\n12\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nScript\nCode\npython xxx.py\n--lr =...\n--batch_size=...\n--dataset=...\nimport ...\nfrom ... import ...\n......\n......\n \nHuman\nInstruction\nCan you assist me\nin writing the code\nor script to ....\nML-AGENT\nStep 1\nProcess\nTool Server\nStep 2\nStep 3\nStep 4\nHuman\ninstruction\nExtract\nkeywords\nKeywords\nKeywords\nRetrieve\n GitHub\nrepository Urls\nTop5 Urls\nRanking Urls\nRanked Urls list\nRanked repo Urls\nlist\nGet repo name and\nrepo structure\nChoose a repo\nRetrieve all\nREADME\nCan fullfill user\ntask?\nReturn\nranked README list\nRanked\nREADME list\nChoose a README\nCan write code or\nscript?\nReturn file path\nExtract\ncode parameters\nCode\nparameters\nREADME\nREADME\nfile\nRepo\nstructure\nTop5 Urls\nRepo names\nRepo\ndescriptions\nHuman\ninstruction\nCan write code or\nscript?\nLLMs\nRetrieval_Repo\nGenerete_Index\nExtract_Keywords\nYes\nNo\nNo\nFile\nNo\nRepo\nname\nRepo\nstructure\nAll\nREADME\nMain\nREADME\nYes\nYes\nAll  readme  are  read\nCode or\nScript\nIf Code\nIf Script\nFigure 6: Structure of ML-AGENT. ML-AGENT follows a four-step process upon receiving human\ninstructions: Step1: Extracting keywords from the instructions and conducting searches on GitHub\nto identify the five most relevant repositories. Step2: Sorting the retrieved GitHub data. Step3:\nEvaluating each retrieved repository to determine its capability to fulfill the task. Step4: Write script\nor code necessary to implement the task.\n- Active Retrieval: In the course of machine learning tasks, human users typically prefer to\nactively retrieve task-specific tools from the Web or directly from GitHub, as opposed to\ncreating code solely from existing knowledge.\n- Discrepancies in Data Processing: GitHub users usually perform initial analysis and\nmodification of their data. However, models often encounter difficulties, particularly when\nlocal data formats diverge from the predefined acceptable formats of the GitHub model.\nThis problem becomes evident in the benchmark, where models struggle to independently\nretrieve relevant details, such as the path and type of pending local data, which subsequently\nleads to the occurrence of errors due to a lack of knowledge or information in the code.\n- Insufficient Domain Knowledge: The model\u2019s lack of domain knowledge is evident in two\nprimary areas. First, the occurrence of syntax errors, highlighting the model\u2019s difficulty in\nconforming to correct programming standards. Second, in the absence of detailed usage\ninstructions, the model resorts to its interpretation of domain knowledge to infer the potential\nuse of a library, which could lead to inaccuracies.\n- Limited Context Windows and Content Comprehension: Constrained by its window\nlength, the model\u2019s ability to navigate through files and understand their content is limited,\nhindering its capability to assist in the composition of code or Bash scripts. Unlike human\nusers, models lack the ability to dynamically explore and interpret file information, limiting\ntheir effectiveness in creating contextually accurate Python code or Bash scripts.\nConsequently, we have designed ML-AGENT, with the explicit intention of mitigating the identified\ndiscrepancies between LLM and human beings, while also being designed to confront and overcome\nthe outlined challenges inherent in this task.\n5.2\nDESIGN FOR ML-AGENT\nIn the initial stages of designing ML-AGENT, our primary objective was to emulate human thought\nprocesses when navigating a real GitHub repository. Consequently, we aimed to equip ML-AGENT\nwith the capacity to independently retrieve relevant GitHub repositories and rank them in response\nto a query. Following the categorization of these repositories, ML-AGENT is designed to select the\n13\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nsequence in which it reads the README files and any code files present in the repository, mirroring\nthe human reading process. Furthermore, it is intended to evaluate whether the provided information\ncan fulfil the task that the user wishes to accomplish. To this end, we have structured the agent design\ninto four distinct phases, encompassing the processes of retrieval, sorting, planning and generation.\nMoreover, to homogenize the results returned by all LLM-related modules within the agent, we\nincorporate the function calling capabilities of GPT-3.5 and GPT-4.\nStep 1: Retrieval\nIn this phase, ML-AGENT extracts keywords from the user\u2019s instructions, including the intended task,\ndesired model, and any other relevant factors. These extracted keywords are then used to search for\nrelated repositories on GitHub, storing the meta information of the top five retrieved repositories.\nStep 2: Sorting\nNext, ML-AGENT utilizes the stored meta information to rank the five selected repositories in\ndescending order based on their relevance to the user\u2019s requirements. The agent then memorizes the\nsorted list of these repositories.\nStep 3: Planning\nFollowing this, the agent reviews the GitHub repositories according to the sorted list and makes\ndecisions. For each repository, ML-AGENT initially extracts the directory structure for reference.\nSimultaneously, the agent scans all the README files in the repository and memorizes them. The\nagent then collates the repository\u2019s name, directory structure, main README file, and paths to all\nREADME files, using this information to make a preliminary judgment. If the repository is considered\nsuitable, all README files will be sorted, and the list will be memorized before proceeding to Step\n4. If the repository fails to meet the user\u2019s instructions, the agent refines its selection.\nStep 4: Gereration\nFinally, the agent decides on the next steps based on the contents of the README, the directory\nstructure of the repository, and the user\u2019s instructions. These next steps may include directly\ngenerating the corresponding code or script, deciding to open files to extract code or parameter\nsnippets, or rejecting the current README, forming a loop. The loop can only be exited by either\nwriting Python code or Bash script, or by returning to Step 3.\n5.3\nAGENT RESULTS ANALYSIS\nThe results for ML-AGENT are displayed in Table 8. The data indicates that ML-AGENT\u2019s perfor-\nmance is marginally behind the GPT-4 model in the Oracle Segment setting, demonstrating optimal\nperformance among the four repositories: DGL, If, ESM, and OpenCLIP.\nTable 8: This table shows the Pass@5 scores of ML-AGENT, GPT 4 and Claude 2 on Oracle\nSegment, Raw README and BM25 Retrieval settings in different GitHub repositories on quarter\nset.\nRepos\nML-AGENT\nGPT 4\nClaude 2\nOracle Segment\nRaw README\nBM25 Retrieval\nOracle Segment\nRaw README\nBM25 Retrieval\nDGL\n66.67\n66.67\n16.67\n0.00\n33.33\n0.00\n0.00\nBERT\n42.86\n42.86\n57.14\n28.57\n14.28\n100.00\n14.29\nLavis\n50.00\n50.00\n75.00\n50.00\n50.00\n12.50\n37.50\nIf\n100.00\n100.00\n100.00\n42.86\n28.57\n14.29\n42.86\nvid2vid\n12.50\n25.00\n50.00\n25.00\n62.50\n12.50\n0.00\nESM\n87.50\n75.00\n37.50\n62.50\n37.50\n87.50\n12.50\nOpenCLIP\n57.14\n57.14\n42.86\n42.86\n14.29\n57.14\n14.29\nTSL\n57.14\n42.86\n42.86\n0.00\n85.71\n14.29\n14.29\nEAP\n57.14\n100.00\n71.42\n0.00\n14.29\n42.86\n0.00\nPy-GAN\n0.00\n62.50\n12.50\n12.50\n87.50\n25.00\n37.50\nTotal\n52.05\n61.64\n49.32\n27.40\n43.84\n42.47\n24.66\nTaking DGL and ESM as examples, in DGL, the expected Bash script for the model to generate is\nlocated in a README file within folders. To accurately generate this code, the user must first locate\nthe correct README folder. ML-AGENT successfully identifies the file and produces the desired\n14\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nresult with a high degree of accuracy. In the ESM repository, we noticed that the agent scanned the\nJupyter files in folders and constructed executable code based on the contents of these Jupyter files.\nNevertheless, there are repositories where ML-AGENT\u2019s performance is sub-optimal, such as Py-\nGAN and Vid2Vid. In the case of Py-GAN, our agent examined the Python files referenced by\nREADME, as opposed to directly writing Bash script files with adequate information. For Vid2Vid,\nthe agent\u2019s \"Generation PHP score\" was only 0.5185, significantly lower than the average score of\n0.8114 for all repositories. This poor performance can be attributed to incorrect parameter generation\nand erroneous Bash script.\nIn light of these observations, we believe that ML-AGENT effectively mirrors human behavior\nin selecting and reading files. However, it does exhibit some weaknesses, such as its inability to\naccurately select files or halt the reading of new files when the requirements have been met.\n6\nRELATED WORK\n6.1\nAGENT\nThe rapid advancement of usages (Yao et al. (2023); Li et al. (2023); Wang et al. (2023c); Cai et al.\n(2023)) for large language models (Brown et al. (2020);Shao et al. (2022); Zhao et al. (2023a)) has\npropelled the evolution of intelligent agents within the realm of artificial intelligence (Zhao et al.\n(2023b); Xi et al. (2023)). The development of agents based on LLMs encompasses two primary\nfacets: general agents and task-specific agents.\nGeneral agents. General agents refer to agents that are not specifically assigned a predefined task\nduring construction. This category of agents demonstrates the capability to autonomously undertake\na diverse range of complex tasks through mechanisms such as planning, reaction, memorization,\nreasoning, and the establishment of communication protocols (Yao et al. (2023), Wang et al. (2023b)),\nlike AutoGPT (Gravitas (2023)), XAgent (Team (2023)), AgentVerse (Chen et al. (2023b)), HOLMES\n(Chen et al. (2023a)) and MetaGPT (Hong et al. (2023)).\nTask-specific agents. Task-specific agents pertain to agents meticulously engineered for the explicit\npurpose of undertaking a specific task. Illustrative instances encompass MindAgent (Gong et al.\n(2023)), designed to augment collaborations between humans and NPCs in games, and Voyager\n(Wang et al. (2023a)), tailored for playing Minecraft.\n6.2\nCODE GENERATION\nCode generation has been a subject of longstanding inquiry within the field of NLP, and it has\ndemonstrated activity in the formulation of methodologies and the establishment of benchmarks\n(Chen et al. (2021), Christopoulou et al. (2022), Orlanski et al. (2023), Wang et al. (2023d), Cassano\net al. (2022); Tang et al. (2023b); Tang et al. (2023a)). There are two avenues for code generation:\none involves advancing the capability for straightforward code generation, while the other leverages\ncode generation for facilitating tool usage.\nStraightforward Code generation. The objective of straightforward code generation is to produce\ncode snippets that fulfill user requirements or contribute to code completion (Feng et al. (2020), Li\net al. (2022)), encompassing the development of foundational models for code generation (Zheng\net al. (2023), AI (2023)).\nTool-using. Tool-using code generation empowers the model\u2019s ability of tool calling, enabling it to\nacquire the proficiency to invoke tools when engaging with the user, like ToolLLM (Qin et al. (2023)),\nGorilla (Patil et al. (2023)) and HuggingGPT Shen et al. (2023).\n7\nCONCLUSION\nIn this paper, we introduced ML-Bench, a comprehensive benchmark designed to evaluate the\neffectiveness of utilizing existing functions in available packages for machine learning tasks. Our\nbenchmark serves as a critical tool for assessing the efficiency and adaptability of various methods in\n15\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nreal-world scenarios. Furthermore, we proposed ML-Agent, an innovative agent capable of reading\nfiles and writing scripts to effectively fulfill user needs. This agent represents a significant step\nforward in automating and streamlining machine learning workflows, thereby reducing the barrier to\nentry for users with varying levels of expertise. Our extensive evaluations, conducted across different\nsettings and models within ML-Bench, demonstrate the robustness and versatility of ML-Agent.\nThese evaluations not only validate the effectiveness of our approach but also provide valuable\ninsights into the potential areas for future research and development in this domain. We believe that\nML-Bench and ML-Agent together make a substantial contribution to the field of machine learning,\noffering researchers and practitioners new tools to advance the state of the art in automated machine\nlearning processes.\n8\nLIMITATION\nOur study, while comprehensive within its scope, is subject to certain limitations that stem primarily\nfrom linguistic and data source constraints.\nLinguistic Limitation - English as a Working Language\nWe exclusively focused on English\nfor our analyses and model development. This choice, while pragmatic due to English\u2019s prevalence\nin scientific literature and technical documentation, inherently limits the generalizability of our\nfindings. English, as a language, possesses unique syntactic and semantic structures that may not\nbe representative of other languages. Consequently, the applicability of our results to non-English\ncontexts is uncertain. This linguistic limitation also restricts the diversity of perspectives and cultural\nnuances that non-English documents could offer. Such perspectives are particularly crucial in fields\nlike Natural Language Processing (NLP) and Machine Learning (ML), where cultural and contextual\nunderstanding significantly influences model performance and bias.\nData Source Limitation - Reliance on GitHub Repositories in English\nOur reliance on GitHub\nrepositories with documents exclusively in English introduces a selection bias. GitHub, while a rich\nsource of open-source projects and documentation, may not comprehensively represent the broader\nlandscape of software development practices and trends globally. This choice potentially overlooks\nsignificant contributions and insights from non-English-speaking communities. These communities\noften host their work on other platforms or use other languages for documentation, leading to a\nskewed understanding of global software development practices. Furthermore, this limitation might\nimpact the development of tools and models that are tailored for a more diverse set of programming\nenvironments and community needs. The underrepresentation of non-English repositories could\nlead to models that are less effective or inclusive when applied in multilingual or culturally diverse\ncontexts.\nMethodological Limitation - Relying on Pre-built Machine Learning Packages\nIn our method-\nology, we opted to utilize existing machine learning packages instead of developing algorithms from\nscratch. While this approach allowed us to leverage well-established, tested, and optimized tools, it\nalso introduces certain constraints. Dependence on pre-built packages means our work is confined to\nthe capabilities and limitations of these tools. This reliance could limit our ability to fully explore\nnovel or unconventional approaches that might be possible with custom-built algorithms. Moreover,\nthis choice potentially impacts the reproducibility and customization of our findings. Researchers\nwho seek to build upon our work may encounter similar constraints imposed by the pre-built packages\nwe utilized. These limitations have the potential to hinder innovation and adaptation in different\ncontexts or for specific use cases.\nScope Limitation - Tasks Limited to README File Descriptions\nOur research methodology\nstrictly adheres to tasks and applications as described in the README files of the GitHub repositories.\nThis approach ensures clarity and focus but also restricts the scope of our study to pre-defined\ntasks. By adhering strictly to the specified tasks, our study may overlook potential applications\nor challenges that are not explicitly documented in the README files. This limitation can result\nin a narrower understanding of the tools and models we examined, as it fails to explore their full\npotential and applicability. The reliance on README descriptions also assumes that these documents\n16\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\ncomprehensively and accurately reflect all relevant aspects of the repositories, which may not always\nbe the case. Important tasks or nuances might be undocumented or underrepresented in these files.\n9\nETHICS STATEMENT\nIn conducting our research, we have carefully considered the ethical implications of our work,\nparticularly in the realms of data annotation and related activities. Our methodologies and processes\nhave been meticulously designed to ensure that they are free from ethical concerns. We affirm that\nour research practices, including the handling of data, have been conducted with the utmost integrity\nand in compliance with ethical standards.\nThroughout the study, we have not engaged in any practices that could raise ethical issues in data\nannotation or other aspects of our work. Our approach has been guided by principles that prioritize\nrespect for data integrity, transparency in our methods, and adherence to established ethical guidelines.\nWe have taken necessary measures to ensure that our research does not inadvertently contribute to or\nperpetuate any form of bias or ethical misconduct.\nOur team remains committed to upholding the highest standards of ethical research, ensuring that our\nwork contributes positively to the field without compromising ethical values or responsibilities.\nREFERENCES\nDeepSeek AI. DeepSeek Coder, 2023. URL https://github.com/deepseek-ai/deep\nseek-coder.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nNghi D. Q. Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, and Steven C. H.\nHoi.\nCodeTF: One-stop transformer library for state-of-the-art code LLM.\narXiv preprint\narXiv:2306.00029, 2023.\nZefan Cai, Baobao Chang, and Wenjuan Han. Human-in-the-loop through chain-of-thought. arXiv\npreprint arXiv:2306.07932, 2023.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda. MultiPL-E: A scalable and extensible approach to\nbenchmarking neural code generation. arXiv preprint arXiv:2208.08227, 2022.\nLiang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu\nLiu, and Baobao Chang. Towards end-to-end embodied decision making via multi-modal large\nlanguage model: Explorations with GPT4-Vision and beyond. arXiv preprint arXiv:2310.02071,\n2023a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,\nYaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong\nSun, and Jie Zhou. AgentVerse: Facilitating multi-agent collaboration and exploring emergent\nbehaviors. arXiv preprint arXiv:2308.10848, 2023b.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128, 2023c.\n17\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nFenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li,\nQi Zhang, Meng Xiao, Bo Shen, Lin Li, et al. PanGu-Coder: Program synthesis with function-level\nlanguage modeling. arXiv preprint arXiv:2207.11280, 2022.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural\nlanguages. arXiv preprint arXiv:2002.08155, 2020.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen-tau Yih, Luke Zettlemoyer, and Mike Lewis. InCoder: A generative model for code infilling\nand synthesis. arXiv preprint arXiv:2204.05999, 2022.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng,\nSong-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. MindAgent: Emergent\ngaming interaction. arXiv preprint arXiv:2309.09971, 2023.\nSignificant Gravitas. AutoGPT, 2023. URL https://github.com/Significant-Gravi\ntas/AutoGPT.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring coding\nchallenge competence with APPS. In Thirty-fifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2021.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. MetaGPT: Meta programming for a multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352, 2023.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik\nNarasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint\narXiv:2310.06770, 2023.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\nCAMEL: Communicative agents for \"mind\" exploration of large language model society. arXiv\npreprint arXiv:2303.17760, 2023.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien\nde Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven\nGowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level\ncode generation with AlphaCode. Science, 378(6624):1092\u20131097, 2022.\nTianyang Liu, Canwen Xu, and Julian McAuley. RepoBench: Benchmarking repository-level code\nauto-completion systems. arXiv preprint arXiv:2306.03091, 2023a.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, et al. AgentBench: Evaluating LLMs as agents. arXiv preprint\narXiv:2308.03688, 2023b.\nGabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud, Jacob\nAustin, Rishabh Singh, and Michele Catasta. Measuring the impact of programming language\ndistribution. In Proceedings of the 40th International Conference on Machine Learning, 2023.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\nconnected with massive APIs. arXiv preprint arXiv:2305.15334, 2023.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,\nDahai Li, Zhiyuan Liu, and Maosong Sun. ToolLLM: Facilitating large language models to master\n16000+ real-world APIs. arXiv preprint arXiv:2307.16789, 2023.\n18\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nNan Shao, Zefan Cai, Chonghua Liao, Yanan Zheng, Zhilin Yang, et al.\nCompositional task\nrepresentations for large language models. In The Eleventh International Conference on Learning\nRepresentations, 2022.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugging-\nGPT: Solving AI tasks with chatGPT and its friends in hugging face. In Thirty-seventh Conference\non Neural Information Processing Systems, 2023.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion:\nlanguage agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023.\nDisha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. Repo-\nFusion: Training code models to understand your repository. arXiv preprint arXiv:2306.10998,\n2023.\nXiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, and Mark B. Gerstein. BioCoder:\nA benchmark for bioinformatics code generation with contextual pragmatic knowledge. arXiv\npreprint arXiv:2308.16458, 2023a.\nXiangru Tang, Yiming Zong, Yilun Zhao, Arman Cohan, and Mark Gerstein. Struc-Bench: Are\nlarge language models really good at generating complex structured data?\narXiv preprint\narXiv:2309.08963, 2023b.\nXAgent Team. XAgent: An autonomous agent for complex task solving, 2023.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. In\nNeurIPS 2023 Foundation Models for Decision Making Workshop, 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large\nlanguage model-based autonomous agents. arXiv preprint arXiv:2308.11432, 2023b.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,\n2023c.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, and Steven C. H. Hoi.\nCodeT5+: Open code large language models for code understanding and generation. arXiv preprint\narXiv:2305.07922, 2023d.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\nA survey. arXiv preprint arXiv:2309.07864, 2023.\nTianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning\nZhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong,\nand Tao Yu. OpenAgents: An open platform for language agents in the wild. arXiv preprint\narXiv:2310.10634, 2023.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. In The Eleventh International\nConference on Learning Representations, 2023.\nDaoguang Zan, Bei Chen, Yongshun Gong, Junzhi Cao, Fengji Zhang, Bingchao Wu, Bei Guan,\nYilong Yin, and Yongji Wang. Private-library-oriented code generation with large language models.\narXiv preprint arXiv:2307.15370, 2023.\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu\nChen. RepoCoder: Repository-level code completion through iterative retrieval and generation.\narXiv preprint arXiv:2303.12570, 2023.\n19\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nHaozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng\nWang, Wenjuan Han, and Baobao Chang. MMICL: Empowering vision-language model with\nmulti-modal in-context learning. arXiv preprint arXiv:2309.07915, 2023a.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv\npreprint arXiv:2303.18223, 2023b.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,\nAndi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for\ncode generation with multilingual evaluations on HumanEval-X. In Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining, pp. 5673\u20135684, New York, NY,\nUSA, 2023.\nWangchunshu Zhou, Yuchen Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing\nChen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun\nChen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous\nlanguage agents. arXiv preprint arXiv:2309.07870, 2023.\n20\nML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks\nA\nDETAILS OF SELECTED GITHUB REPOSITRIES\nDomain\nGitHub\nStar\nURL\n#README\nLast Updated\nText\nBERT\n35693\nhttps://github.com/google-research/bert\n1\n2020.03.11\nTensor2Tensor\n14280\nhttps://github.com/tensorflow/tensor2tensor\n9\n2023.04.01\nText Classification\n7665\nhttps://github.com/brightmart/text_classification\n1\n2022.09.21\nGNN\nGNN\n12429\nhttps://github.com/dmlc/dgl\n154\n2023.11.16\nmolecular\nESM\n2462\nhttps://github.com/facebookresearch/esm\n8\n2023.06.27\nImage-GAN\nPyTorch-GAN\n14947\nhttps://github.com/eriklindernoren/PyTorch-GAN\n1\n2021.01.07\nMulti-Modality\nLAVIS\n7300\nhttps://github.com/salesforce/lavis\n8\n2023.09.25\nIF\n7237\nhttps://github.com/deep-floyd/if\n1\n2023.06.03\nOPEN CLIP\n6856\nhttps://github.com/mlfoundations/open_clip\n1\n2023.11.01\nStable\n31506\nhttps://github.com/Stability-AI/stablediffusion\n1\n2023.03.25\nVideo\nVid2Vid\n8393\nhttps://github.com/NVIDIA/vid2vid\n2\n2019.07.04\nTime-Series\nTime-Series-Library\n2670\nhttps://github.com/thuml/Time-Series-Library\n1\n2023.11.10\nAttention Usage\nExternal-Attention-pytorch\n9949\nhttps://github.com/xmu-xiaoma666/External-Attention-pytorch\n1\n2023.10.25\nMedicine\nMedicalZooPytorch\n1516\nhttps://github.com/black0017/MedicalZooPytorch\n21\n2022.02.07\nTable 9: Detailed information about the selected GitHub repositries.\nB\nEXPERIMENTAL SETUP DETAILS\nRepos\nCodeLlama\nGPT 3.5\nRaw README\nBM25 retrieval\nOracle segment\nRaw README\nBM25 retrieval\nOracle segment\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nvid2vid\n4468\n4669\n408\n1615\n556\n556\n3569\n3807\n338\n1240\n416\n416\nIf\n3972\n4202\n1390\n1065\n3023\n3023\n3042\n3320\n1119\n851\n2367\n2367\nDGL\n7401\n9668\n312\n2598\n179\n138\n5924\n7194\n275\n1924\n143\n110\nPy-GAN\n10619\n10644\n532\n515\n314\n314\n9067\n9092\n433\n424\n268\n268\nESM\n11955\n12705\n585\n1419\n177\n173\n9218\n10101\n486\n1168\n139\n136\nBERT\n13373\n14309\n401\n985\n372\n375\n10558\n11323\n335\n783\n287\n290\nOpenCLIP\n8005\n8403\n415\n1517\n5420\n5420\n6340\n6813\n350\n1202\n4397\n4397\nLavis\n4447\n4779\n471\n1735\n1984\n1984\n3377\n3699\n372\n1347\n1547\n1547\nTSL\n1891\n2008\n382\n1136\n345\n345\n1460\n1598\n315\n918\n276\n276\nEAP\n20701\n20655\n1155\n473\n105\n118\n14591\n14908\n857\n410\n69\n80\nSD\n4203\n/\n471\n/\n234\n/\n3341\n/\n393\n/\n183\n/\nMedZooPy\n2393\n/\n1101\n/\n133\n/\n1868\n/\n879\n/\n99\n/\nTCL\n8363\n/\n304\n/\n116\n/\n7066\n/\n278\n/\n96\n/\nT2T\n4566\n/\n476\n/\n192\n/\n3673\n/\n399\n/\n153\n/\nTable 10: Detailed length of tokens for each repository.\nC\nCONTRIBUTION\n\u2022 Xiangru Tang, Zefan Cai, Yuliang Liu, Junjie Lu, Yichi Zhang, Yanjun Shao, and Kaikai An\nwrote the paper\n\u2022 Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Zengxian Yang, Kaikai\nAn, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Zhengliang Li, Liang Chen,\nYiming Zong, Yan Wang, and Tianyu Liu participated in the data annotation process.\n\u2022 Yuliang Liu, Xiangru Tang, and Zefan Cai lead this project.\n\u2022 Yilun Zhao, Yujia Qin, Wangchunshu Zhou, Tianyu Liu, Zhiwei Jiang, and Baobao Chang\nparticipated in the discussion.\n\u2022 Arman Cohan and Mark Gerstein advised this project.\n21\n"
  },
  {
    "title": "JaxMARL: Multi-Agent RL Environments in JAX",
    "link": "https://arxiv.org/pdf/2311.10090.pdf",
    "upvote": "6",
    "text": "JaxMARL: Multi-Agent RL Environments and Algorithms in JAX\nAlexander Rutherford1\u2217\u2020, Benjamin Ellis1\u2217\u2020, Matteo Gallici2\u2217\u2020, Jonathan Cook1\u2217, Andrei Lupu1\u2217, Gar\u00f0ar Ingvarsson3\u2217,\nTimon Willi1\u2217, Akbir Khan3, Christian Schroeder de Witt1, Alexandra Souly3, Saptarashmi Bandyopadhyay4,\nMikayel Samvelyan3, Minqi Jiang3, Robert Tjarko Lange5, Shimon Whiteson1, Bruno Lacerda1, Nick Hawes1,\nTim Rockt\u00e4schel3, Chris Lu1\u2217\u2020, Jakob Nicolaus Foerster1\n1University of Oxford 2Universitat Polit\u00e8cnica de Catalunya 3UCL 4University of Maryland 5Technical University Berlin\nABSTRACT\nBenchmarks play an important role in the development of machine\nlearning algorithms. For example, research in reinforcement learn-\ning (RL) has been heavily influenced by available environments\nand benchmarks. However, RL environments are traditionally run\non the CPU, limiting their scalability with typical academic com-\npute. Recent advancements in JAX have enabled the wider use of\nhardware acceleration to overcome these computational hurdles,\nenabling massively parallel RL training pipelines and environments.\nThis is particularly useful for multi-agent reinforcement learning\n(MARL) research. First of all, multiple agents must be considered at\neach environment step, adding computational burden, and secondly,\nthe sample complexity is increased due to non-stationarity, decen-\ntralised partial observability, or other MARL challenges. In this\npaper, we present JaxMARL, the first open-source code base that\ncombines ease-of-use with GPU enabled efficiency, and supports a\nlarge number of commonly used MARL environments as well as\npopular baseline algorithms. When considering wall clock time, our\nexperiments show that per-run our JAX-based training pipeline is\nup to 12500x faster than existing approaches. This enables efficient\nand thorough evaluations, with the potential to alleviate the evalu-\nation crisis of the field. We also introduce and benchmark SMAX, a\nvectorised, simplified version of the popular StarCraft Multi-Agent\nChallenge, which removes the need to run the StarCraft II game\nengine. This not only enables GPU acceleration, but also provides\na more flexible MARL environment, unlocking the potential for\nself-play, meta-learning, and other future applications in MARL.\nWe provide code at https://github.com/flairox/jaxmarl.\nKEYWORDS\nMulti-Agent Reinforcement Learning, JAX, Benchmarks\n1\nINTRODUCTION\nBenchmarks play a pivotal role in the development of new single\nand multi-agent reinforcement learning (MARL) algorithms by\ndefining problems, enabling comparisons, and focusing efforts. For\nexample, in recent years, Go and Chess drove the development of\nMuZero [50] while decentralised StarCraft Micromanagement [17]\nand later the StarCraft Multi-Agent Challenge [SMAC, 49] resulted\nin the development of algorithms such as QMIX [46], a popular\nMARL technique.\nData transfer between the CPU (where the environment is sim-\nulated) and the GPU (where the agents are evaluated) is a crucial\nbottleneck for simulation speed. Simulation speed in turn is vital\n\u2217Core Contributor\n\u2020Equal Contribution. Corresponding Author: christopher.lu@exeter.ox.ac.uk. Full\nauthorship contribution statements appear in the end of the document (Section 8).\nFigure 1: JaxMARL\u2019s philosophy. JaxMARL combines a wide\nrange of environments with ease of use and evaluation speed.\nfor progress in reinforcement learning (RL) because RL algorithms\noften require a large number of environment interactions. This\nproblem is even worse in MARL, where non-stationarity and de-\ncentralised partial observability greatly worsen the sample com-\nplexity [4]. Hardware acceleration and parallelisation are crucial to\nalleviating this, but current acceleration and parallelisation methods\nare typically not implemented in Python, reducing their accessibil-\nity for most machine learning researchers [52, 61]. For example, the\nextremely efficient Hanabi library [21] from Meta-AI research is\nimplemented in C++ and has seen relatively little adoption by the\ncommunity. However, recent advances in JAX [7] have opened up\nnew possibilities for using Python code directly with hardware ac-\ncelerators, enabling the wider use of massively parallel RL training\npipelines and environments.\nThe JAX [7] library provides composable function transforma-\ntions, allowing for automatic vectorisation, device parallelisation,\nautomatic differentiation and just-in-time (JIT) compilation with\nXLA [48], for device-agnostic optimisation. Using JAX, both the\nenvironment rollouts and model training can happen on a hard-\nware accelerator (such as a GPU or TPU), removing the cost of data\ntransfer between devices and allowing for significant parallelisation.\nRecently, PureJaxRL [33, 36] has demonstrated the power of this\nend-to-end JAX-based approach; running both the environment\nand the model training on a GPU yields a 4000x speedup over a\n\u201ctraditional\u201d pipeline with a GPU-trained policy but a CPU-based\nenvironment.\nThese accelerations could substantially advance RL and MARL re-\nsearch by quickening the testing and iteration of ideas. Furthermore,\nthey lower computational hurdles for in-depth MARL research, en-\nabling researchers to utilise billions of frames and extract more\nperformance from single GPUs.\nAlongside the current computational issues faced by MARL re-\nsearchers, recent work also highlights issues with the evaluation\narXiv:2311.10090v4  [cs.LG]  19 Dec 2023\n(a) MPE\n(b) Overcooked\n(c) Multi-Agent Brax\n(d) STORM\n(e) Hanabi\n(f) Switch Riddle\n(g) Coin Game\n(h) SMAX\nFigure 2: JaxMARL environments. We provide vectorised implementations of a wide range of environments from different\nMARL settings.\nstandards and use of benchmarks in the MARL community. In par-\nticular, MARL papers typically only test on a few domains. Of the 75\nrecent MARL papers analysed by [20], 50% used only one evaluation\nenvironment and a further 30% used only two. While SMAC and\nMPE [32], the two most used environments, have various tasks or\nmaps, the lack of a standard set raises the risk of biased comparisons\nand incorrect conclusions. This leads to environment overfitting\nand unclear progress markers.\nInstead, novel MARL methods should be tested on a wide range\nof domains to accurately evaluate their limits and enable better\ncomparisons. The likely issue preventing this is the lack of a unified\ncodebase and the computational burden of further evaluation.\nThis paper presents JaxMARL, a Python library that for the\nfirst time brings together JAX implementations of eight common\nMARL environments under one API. We additionally provide JAX\nimplementations for four state-of-the-art algorithms, allowing for\nend-to-end JAX-based training pipelines in a similar fashion to\nPureJaxRL. As outlined in Figure 1, we present a library with end-\nto-end hardware-accelerated training, simple Python implemen-\ntations, and a broad range of MARL environments. By alleviating\ncomputational constraints, JaxMARL allows rapid evaluation of\nnovel methods across a broad set of domains, and hence has the\npotential to be a powerful tool to address MARL\u2019s evaluation crisis.\nSpecifically, we find that JaxMARL achieves over 12500x speedup\ncompared to \u201cconventional\u201d aproaches.\nWe also create SMAX, a JAX-based simplification of the cen-\ntralised training with decentralised execution (CTDE) benchmarks\nSMAC [49] and SMACv2 [14]. SMAX features simplified dynamics,\ngreater flexibility and a more sophisticated but fully-decentralised\nheuristic AI, while retaining the high-dimensional observation\nspace, complex unit type interactions and procedural scenario gen-\neration that lend SMAC and SMACv2 much of their difficulty.\nAs shown in Figure 2, in addition to SMAX, our library includes\nthe most popular environments from several MARL settings. For\ncentralised training with decentralised execution (CTDE), we in-\nclude the Multi-Agent Particle Environments (MPE) [32], and Multi-\nAgent Brax (MABrax). Meanwhile, for zero-shot coordination (ZSC)\nand ad-hoc teamplay, we include Hanabi and Overcooked. Lastly,\nfrom the general-sum literature, we include the CoinGame and\nSpatial-Temporal Representations of Matrix Games (STORM), a\nrepresentation of matrix games as grid-world scenarios with tempo-\nrally extended actions. JaxMARL provides the first JAX implemen-\ntation of these environments and unifies them in a single codebase.\nWe additionally provide JAX implementations of Independent\nPPO (IPPO) [13, 51], QMIX, VDN [55] and Independent \ud835\udc44-Learning\n(IQL) [40], four of the most common MARL algorithms, allowing\nnew techniques to be easily benchmarked against existing practices.\nWe will extend this list before the camera-ready copy, e.g. with the\npopular MAPPO [63] algorithm.\n2\nBACKGROUND\n2.1\nHardware Accelerated Environments\nJAX enables the use of Python code with any hardware accelerator,\nallowing researchers to write hardware-accelerated code easily.\nWithin the RL community, writing environment code in JAX has\ngained recent popularity. This brings two chief advantages: firstly,\nenvironments written in JAX can be very easily parallelised by\nusing JAX\u2019s vmap operation, which vectorises a function across an\ninput dimension, and secondly writing the environment in JAX\nallows the agent and environment to be co-located on the GPU,\nwhich eliminates the time taken to copy between CPU and GPU\nmemory. Combined, these two factors bring significant increases\nin training speed, with PureJaxRL [33] achieving a 4000x speedup\nover traditional training in single-agent settings.\n2.2\nSMAC\nStarCraft is a popular environment for testing RL algorithms. It typ-\nically features features a centralised controller issuing commands\nto balance micromanagement, the low-level control of individual\nunits, and macromanagement, the high level plans for economy and\nresource management.\nSMAC [49], instead, focuses on decentralised unit micromanage-\nment across a range of scenarios divided into three broad categories:\nsymmetric, where each side has the same units, asymmetric, where\nthe enemy team has more units, and micro-trick, which are scenarios\ndesigned specifically to feature a particular StarCraft micromanage-\nment strategy. SMACv2 [14] demonstrates that open-loop policies\ncan be effective on SMAC and adds additional randomly generated\nscenarios to rectify SMAC\u2019s lack of stochasticity. However, both\nof these environments rely on running the full game of StarCraft\nII, which severely increases their CPU and memory requirements.\nSMAClite [38] attempts to alleviate this computational burden by\nrecreating the SMAC environment primarily in NumPy, with some\ncore components written in C++. While this is much more light-\nweight than SMAC, it cannot be run on a GPU and therefore cannot\nbe parallelised effectively with typical academic hardware, which\ncommonly has very few CPU cores compared to industry clusters.\n3\nJAXMARL\nWe present JaxMARL, a library containing simple and accessible\nJAX implementations of popular MARL environments and algo-\nrithms. JAX enables significant acceleration and parallelisation over\nexisting implementations. To the best of our knowledge, JaxMARLis\nthe first open source library that provides JAX-based implementa-\ntions of a wide range of MARL environments and baselines.\n3.1\nAPI\nThe interface of JaxMARL is inspired by PettingZoo [58] and Gym-\nnax. We designed it to be a simple and easy-to-use interface for\na wide-range of MARL problems. An example of instantiating an\nenvironment from JaxMARL\u2019s registry and executing one transition\nis presented in Figure 3. As JAX\u2019s JIT compilation requires pure\nfunctions, our step method has two additional inputs compared to\nPettingZoo\u2019s. The state object stores the environment\u2019s internal\nstate and is updated with each call to step, before being passed to\n1\nimport jax\n2\nfrom jaxmarl import make\n3\n4\nkey = jax.random.PRNGKey (0)\n5\nkey , key_reset , key_act , key_step = jax.random.split(key , 4)\n6\n7\n# Initialise and reset the environment.\n8\nenv = make('MPE_simple_world_comm_v3 ')\n9\nobs , state = env.reset(key_reset)\n10\n11\n# Sample random actions.\n12\nkey_act = jax.random.split(key_act , env.num_agents)\n13\nactions = {agent: env.action_space(agent). sample(key_act[i]) \\\n14\nfor i, agent in enumerate(env.agents )}\n15\n16\n# Perform the step transition.\n17\nobs , state , reward , done , infos = env.step(key_step , state , actions)\nFigure 3: An example of JaxMARL\u2019s API, which is flexible\nand easy-to-use.\nsubsequent calls. Meanwhile, key_step is a pseudo-random key,\nconsumed by JAX functions that require stochasticity. This key is\nseparated from the internal state for clarity.\nSimilar to PettingZoo, the remaining inputs and outputs are dic-\ntionaries keyed by agent names, allowing for differing action and ob-\nservation spaces. However, as JAX\u2019s JIT compilation requires arrays\nto have static shapes, the total number of agents in an environment\ncannot vary during an episode. Thus, we do not use PettingZoo\u2019s\nagent iterator. Instead, the maximum number of agents is set upon\nenvironment instantiation and any agents that terminate before the\nend of an episode pass dummy actions thereafter. As asynchronous\ntermination is possible, we signal the end of an episode using a\nspecial \"__all__\" key within done. The same dummy action ap-\nproach is taken for environments where agents act asynchronously\n(e.g. turn-based games).\nTo ensure clarity and reproducibility, we keep strict registra-\ntion of environments with suffixed version numbers, for example\n\u201cMPE Simple Spread V3\u201d. Whenever JaxMARL environments corre-\nspond to existing CPU-based implementations, the version numbers\nmatch.\n3.2\nEnvironments\nJaxMARL contains a diverse range of environments, all imple-\nmented in JAX. We also introduce SMAX, a SMAC-like environment\nimplemented entirely in JAX. In this section we introduce these\nenvironments and provide details on their implementations.\nSMAX. The StarCraft Multi-Agent Challenge (SMAC) is a popu-\nlar benchmark but has a number of shortcomings. First, as noted\nand addressed in prior work [14], it is not sufficiently stochastic to\nrequire complex closed-loop policies. Additionally, SMAC relies on\nStarCraft II as a simulator. While this allows SMAC to use the wide\nrange of units, objects and terrain available in StarCraft II, running\nan entire instance of StarCraft II is slow [38] and memory intensive.\nStarCraft II runs on the CPU and therefore SMAC\u2019s parallelisation\nis severely limited with typical academic compute.\nUsing the StarCraft II game engine constrains environment de-\nsign. For example, StarCraft II groups units into three races and\ndoes not allow units of different races on the same team, limiting\nthe variety of scenarios that can be generated. Secondly, SMAC\ndoes not support a competitive self-play setting without significant\nTable 1: SMAX scenarios. The first section corresponds to SMAC scenarios, while the second corresponds to SMACv2.\nScenario\nAlly Units\nEnemy Units\nStart Positions\n2s3z\n2 stalkers and 3 zealots\n2 stalkers and 3 zealots\nFixed\n3s5z\n3 stalkers and 5 zealots\n3 stalkers and 5 zealots\nFixed\n5m_vs_6m\n5 marines\n6 marines\nFixed\n10m_vs_11m\n10 marines\n11 marines\nFixed\n27m_vs_30m\n27 marines\n30 marines\nFixed\n3s5z_vs_3s6z\n3 stalkers and 5 zealots\n3 stalkers and 6 zealots\nFixed\n3s_vs_5z\n3 stalkers\n5 zealots\nFixed\n6h_vs_8z\n6 hydralisks\n8 zealots\nFixed\nsmacv2_5_units\n5 uniformly randomly chosen\n5 uniformly randomly chosen\nSMACv2-style\nsmacv2_10_units\n10 uniformly randomly chosen\n10 uniformly randomly chosen\nSMACv2-style\nsmacv2_20_units\n20 uniformly randomly chosen\n20 uniformly randomly chosen\nSMACv2-style\nengineering work. The purpose of SMAX is to address these limi-\ntations. It provides access to a SMAC-like, hardware-accelerated,\ncustomisable environment that supports self-play and custom unit\ntypes.\nUnits in SMAX are modelled as circles in a two-dimensional\ncontinuous space. SMAX makes a number of additional simplifica-\ntions to the dynamics of StarCraft II, details of which are given in\nAppendix A.1.\nSMAX also features a different, and more sophisticated, heuristic\nAI. The heuristic in SMAC simply moves to a fixed location [38], at-\ntacking any enemies it encounters along the way, and the heuristic\nin SMACv2 globally pursues the nearest agent. Thus the SMAC AI\noften does not aggressively pursue enemies that run away, and can-\nnot generalise to the SMACv2 start positions, whereas the SMACv2\nheuristic AI conditions on global information and is exploitable\nbecause of its tendency to flip-flop between two similarly close\nenemies. SMAC\u2019s heuristic AI must be coded in the map editor,\nwhich does not provide a simple coding interface.\nIn contrast, SMAX features a decentralised heuristic AI that can\neffectively find enemies without requiring the global information\nof the SMACv2 heuristic. This guarantees that in principle a 50%\nwin rate is always achievable by copying the decentralised heuristic\npolicy exactly. This means any win-rate below 50% represents a\nconcrete failure to learn.\nSMAX scenarios incorporate both a number of the original sce-\nnarios from SMAC and scenarios similar to those found in SMACv2.\nThe latter sample units uniformly across all SMAX unit types\n(stalker, zealot, hydralisk, zergling, marine, marauder) and ensure\nfairness by having identical team composition for the enemy and\nally teams. We provide more details on SMAX in Appendix A.1.\nOvercooked. Inspired by the popular videogame of the same\nname, Overcooked is commonly used for assessing fully cooperative\nand fully observable Human-AI task performance. The aim is to\nquickly prepare and deliver soup, which involves putting three\nonions in a pot, cooking the soup, and serving it into bowls. Two\nagents, or cooks, must coordinate to effectively divide the tasks to\nmaximise their common reward signal. Our implementation mimics\nthe original from Overcooked-AI [9], including all five original\nlayouts and a simple method for creating additional ones. For a\ndiscussion on the limitations of the Overcooked-AI environment,\nsee [30].\nHanabi. Hanabi is a fully cooperative partially observable mul-\ntiplayer card game, where players can observe other players\u2019 cards\nbut not their own. To win, the team must play a series of cards in a\nspecific order while sharing only a limited amount of information\nbetween players. As reasoning about the beliefs and intentions\nof other agents is central to performance, it is a common bench-\nmark for ZSC and ad-hoc teamplay research. Our implementation\nis inspired by the Hanabi Learning Environment [2] and includes\ncustom configurations for varying game settings, such as the num-\nber of colours/ranks, number of players, and number of hint tokens.\nCompared to the Hanabi Learning Environment, which is written\nin C++ and split over dozens of files, our implementation is a sin-\ngle easy-to-read Python file, which simplifies interfacing with the\nlibrary and running experiments.\nMulti-Agent Particle Environments (MPE). The multi-agent par-\nticle environments feature a 2D world with simple physics where\nparticle agents can move, communicate, and interact with fixed\nlandmarks. Each specific environment varies the format of the world\nand the agents\u2019 abilities, creating a diverse set of tasks that include\nboth competitive and cooperative settings. We implement all the\nMPE scenarios featured in the PettingZoo library and the transi-\ntions of our implementation map exactly to theirs. We additionally\ninclude a fully cooperative predator-prey variant of simple tag, pre-\nsented in [44]. The code is structured to allow for straightforward\nextensions, enabling further tasks to be added.\nMulti-Agent Brax (MABrax). MABrax is a derivative of Multi-\nAgent MuJoCo [44], an extension of the MuJoCo Gym environ-\nment [59] that is commonly used for benchmarking continuous\nmulti-agent robotic control. Our implementation utilises Brax[18]\nas the underlying physics engine and includes five of Multi-Agent\nMuJoCo\u2019s multi-agent factorisation tasks, where each agent controls\na subset of the joints and only observes the local state. The included\ntasks, illustrated in Figure 2, are: ant_4x2, halfcheetah_6x1, hopper_3x1,\nhumanoid_9|8, and walker2d_2x3. The task descriptions mirror\nthose from Gymnasium-Robotics [12].\nCoin Game. Coin Game is a two-player grid-world environment\nwhich emulates social dilemmas such as the iterated prisoner\u2019s\ndilemma [53]. Used as a benchmark for the general-sum setting, it\nTable 2: Benchmark results for JAX-based MARL environments (steps-per-second) when taking random actions. All environ-\nments are significantly faster than existing CPU implementations.\nEnvironment\nOriginal, 1 Env\nJax, 1 Env\nJax, 100 Envs\nJax, 10k Envs\nMaximum Speedup\nMPE Simple Spread\n8.34 \u00d7 104\n5.48 \u00d7 103\n5.24 \u00d7 105\n3.99 \u00d7 107\n4.78 \u00d7 102\nMPE Simple Reference\n1.46 \u00d7 105\n5.24 \u00d7 103\n4.85 \u00d7 105\n3.35 \u00d7 107\n2.29 \u00d7 102\nSwitch Riddle\n2.69 \u00d7 104\n6.24 \u00d7 103\n7.92 \u00d7 105\n6.68 \u00d7 107\n2.48 \u00d7 103\nHanabi\n2.10 \u00d7 103\n1.36 \u00d7 103\n1.05 \u00d7 105\n5.02 \u00d7 106\n2.39 \u00d7 103\nOvercooked\n1.91 \u00d7 103\n3.59 \u00d7 103\n3.04 \u00d7 105\n1.69 \u00d7 107\n8.85 \u00d7 103\nMABrax Ant 4x2\n1.77 \u00d7 103\n2.70 \u00d7 102\n1.81 \u00d7 104\n7.62 \u00d7 105\n4.31 \u00d7 102\nStarcraft 2s3z\n8.31 \u00d7 101\n5.37 \u00d7 102\n4.53 \u00d7 104\n2.71 \u00d7 106\n3.26 \u00d7 104\nStarcraft 27m vs 30m\n2.73 \u00d7 101\n1.45 \u00d7 102\n1.12 \u00d7 104\n1.90 \u00d7 105\n6.96 \u00d7 103\nSTORM\n\u2013\n2.48 \u00d7 103\n1.75 \u00d7 105\n1.46 \u00d7 107\n\u2013\nCoin Game\n1.97 \u00d7 104\n4.67 \u00d7 103\n4.06 \u00d7 105\n4.03 \u00d7 107\n2.05 \u00d7 103\nexpands on simpler social dilemmas by adding a high-dimensional\nstate. Two players, \u2018red\u2019 and \u2018blue\u2019 move in a grid world and are\neach awarded 1 point for collecting any coin. However, \u2018red\u2019 loses\n2 points if \u2018blue\u2019 collects a red coin and vice versa. Thus, if both\nagents ignore colour when collecting coins their expected reward\nis 0. Further details are provided in Appendix A.2.\nSpatial-Temporal Representations of Matrix Games (STORM). In-\nspired by the \u201cin the Matrix\u201d games in Melting Pot 2.0 [1], the\nSTORM [26] environment expands on matrix games by represent-\ning them as grid-world scenarios. Agents collect resources which\ndefine their strategy during interactions and are rewarded based\non a pre-specified payoff matrix. This allows for the embedding of\nfully cooperative, competitive or general-sum games, such as the\nprisoner\u2019s dilemma [53]. Thus, STORM can be used for studying\nparadigms such as opponent shaping, where agents act with the in-\ntent to change other agents\u2019 learning dynamics, which has been em-\npirically shown to lead to more prosocial outcomes [16, 26, 35, 65].\nCompared to the Coin Game or matrix games, the grid-world setting\npresents a variety of new challenges such as partial observability,\nmulti-step agent interactions, temporally-extended actions, and\nlonger time horizons. Unlike the \u201cin the Matrix\u201d games from Melt-\ning Pot, STORM features stochasticity, increasing the difficulty [14].\nA further environment specification is provided in Appendix A.3.\nSwitch Riddle. Originally used to illustrate the Differentiable\nInter-Agent Learning algorithm [15], Switch Riddle is a simple\ncooperative communication environment that we include as a de-\nbugging tool. \ud835\udc5b prisoners held by a warden can secure their release\nby collectively ensuring that each has passed through a room with\na light bulb and a switch. Each day, a prisoner is chosen at random\nto enter this room. They have three choices: do nothing, signal\nto the next prisoner by toggling the light, or inform the warden\nthey think all prisoners have been in the room. The game ends\nwhen a prisoner informs the warden or the maximum time steps\nare reached. The rewards are +1 if the prisoner informs the warden,\nand all prisoners have been in the room, -1 if the prisoner informs\nthe warden before all prisoners have taken their turn, and 0 oth-\nerwise, including when the maximum time steps are reached. We\nbenchmark using the implementation from [64].\n3.3\nAlgorithms\nIn this section, we present our re-implementation of four well\nknown MARL baseline algorithms using JAX. The primary objec-\ntive of these baselines is to provide a structured framework for\ndeveloping MARL algorithms leveraging the advantages of the Jax-\nMARL environments. All of the training pipelines are fully compati-\nble with JAX\u2019s JIT and VMAP functions, resulting in a significant ac-\nceleration of both the training and metric evaluation processes. This\nenables parallelisation of training across various seeds and hyper-\nparameters on a single machine in parallel. We follow the CleanRL\nphilosophy of providing clear, single-file implementations [24].\nIPPO. Our Independent PPO (IPPO) [13, 51] implementation is\nbased on PureJaxRL [33], with parameter sharing across homoge-\nneous agents. We provide both feed-forward and RNN versions.\n\ud835\udc44-learning Methods. Our \ud835\udc44-Learning baselines, including Inde-\npendent \ud835\udc44-Learning (IQL) [57], Value Decomposition Networks\n(VDN) [56], and QMIX [47], have been implemented in accordance\nwith the PyMARL codebase [47] to ensure consistency with pub-\nlished results and enable direct comparisons with PyTorch. Our\nbaselines natively support aggregating trajectories from batched\nenvironments, simplifying parallelisation. This approach is more\nconvenient than managing environments on distinct threads and\nsubsequently aggregating results, as done in PyMARL. We provide\na brief overview of the implemented baselines in the Appendix.\n4\nRESULTS\nIn our results, we aim to demonstrate the speed and correctness of\nour environments and algorithms.In several cases, minor changes to\nthe environments mean that our environments do not exactly match\nthe originals on a step-by-step level. We therefore demonstrate the\ncorrectness in different ways for each environment and discuss each\nseparately. By combining this evidence, we demonstrate that our\nlibrary provides overall correct and far quicker baselines on a wide\nrange of sufficiently correct and easily-modifiable environments.\n4.1\nEnvironment Speed\nWe measure the performance of our environments in steps per\nsecond when using random actions and compare to the original\nenvironments in Table 2 and Figure 4. All results were collected on\n100\n101\n102\n103\n104\nNumber of parallel environments\n103\n104\n105\n106\nSteps per Second\nSteps per second for Hanabi\nJaxMARL\nOriginal\n(a) Hanabi\n100\n101\n102\n103\n104\nNumber of parallel environments\n103\n104\n105\n106\nSteps per Second\nSteps per second for MABrax Ant 4x2\nJaxMARL\nOriginal\n(b) MABrax Ant\n100\n101\n102\n103\n104\nNumber of parallel environments\n104\n105\n106\n107\nSteps per Second\nSteps per second for Overcooked\nJaxMARL\nOriginal\n(c) Overcooked\n100\n101\n102\n103\n104\nNumber of parallel environments\n102\n103\n104\n105\n106\nSteps per Second\nSteps per second for SMAX 2s3z\nJaxMARL\nOriginal\n(d) Starcraft 2s3z\nFigure 4: Speedup of four JaxMARL environments compared to singled-threaded CPU-based implementations.\n10\n1\n100\n101\n102\n103\nSeconds (Log Scale)\n115\n110\n105\n100\n95\n90\n85\n80\nTraining Return\nMPE Simple Spread V3\nJAX IPPO\nMARLLIB IPPO\n(a) MPE Simple Spread Returns\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimestep\n1e6\n115\n110\n105\n100\n95\n90\n85\n80\nTraining Return\nMPE Simple Spread V3\nJAX IPPO\nMARLLIB IPPO\n(b) MPE Simple Spread Returns\n101\n102\n103\nSeconds (Log Scale)\n0\n200\n400\n600\n800\n1000\nNumber of Independent Training Runs\nTotal Vectorised Training Time for MPE Simple Spread V3\nSingle MARLLIB IPPO\nJaxMARL IPPO\n(c) MPE Training Speed\n103\n104\n105\nSeconds (Log Scale)\n0\n100\n200\n300\n400\n500\nNumber of Independent Training Runs\nTotal Vectorised Training Time for SMAX 2s3z\nSingle PyMARL SMAC 2s3z MAPPO\nJaxMARL SMAX 2s3z IPPO\n(d) SMAX Training Speed\nFigure 5: IPPO Speed and Performance in JaxMARL compared to MARLLIB and PyMARL in SMAX and MPE. Return results were averaged\nacross 3 seeds. Performance results show 1 seed collected on the hardware described in Section 4.1.\na single NVIDIA A100 GPU and AMD EPYC 7763 64-core proces-\nsor. Environments were rolled out for 1000 sequential steps. Many\nenvironments have comparable performance to JaxMARL when\ncomparing single environments, but the ease of parallelisation with\nJax allows for more efficient scaling compared to CPU-based envi-\nronments. For example, MPE Simple Spread\u2019s JAX implementation\nis \u02dc20x slower than the original when comparing a single environ-\nment, but even when only running 100 environments in parallel,\nthe JAX environment is already over 6x faster. When considering\n10000 environments, the JAX versions are much faster, achieving\nspeedups of up to 8500x over the single-threaded environment (in\nthe case of Overcooked). Running this many environments in par-\nallel using CPU environments would require a large CPU cluster\nand sophisticated communication mechanisms. This engineering\nis typically beyond the resources of academic labs, and therefore\nJaxMARL can unlock new research directions for such institutions.\n4.2\nAlgorithm Speed\nWe investigate the speed of our IPPO implementation in Figure 5.\nBy vectorising over agents, it is possible to train a vast number\nof agents in a fraction of the time it takes to train a single agent\nwithout hardware acceleration. For MPE, it is possible to train 1024\nteams in 198.4 seconds, which is less than 0.2 seconds per teams\nof agents. A single run of MARLLIB\u2019s IPPO implementation on\nthe same hardware takes around 2435.7 seconds on average. This\nrepresents a speedup of over 12500x.\nOur JAX-based\ud835\udc44-learning algorithms also offer significant speed\nadvantages. In Figure 6a, training a single IQL, VDN, or QMIX pol-\nicy in MPE takes \u223c 130 seconds while using PyMarl takes over\nan hour. Training 1024 QMIX learners in a batch requires 1670\nseconds, which translates to 1.6 seconds per learner, indicating a\n2700x speedup. This speedup is not as large as for IPPO because\n\ud835\udc44-learning baselines are typically trained with fewer parallel envi-\nronments. In our experiments, we used 8 parallel environments for\n\ud835\udc44-learning compared to the 25 or 64 used for PPO. This difference\nis due to \ud835\udc44-learners benefiting more from a buffer with trajectories\ncollected by different policies, resulting in a more frequent pol-\nicy update, rather than collecting many trajectories with the same\npolicy in parallel.\nFor SMAX, we compare our vectorised IPPO baseline to the\nMAPPO implementation provided in [54]. MAPPO utilises an RNN\nand IPPO uses a feed forward network. This was run on a machine\nwith a 64-core CPU and NVIDIA 2080Ti GPU. Additionally, as dis-\ncussed in Section 3.2, SMAC and SMAX are different environments.\nThese caveats aside, the differences in performance are so striking\nthat we believe this clearly demonstrates the advantages of our ap-\nproach. We trained 512 SMAX teams on 2s3z in under 33 minutes,\nwhereas a single training run of PyTorch IPPO implementation\ntakes 44 hours on average. This is roughly a 40000x speedup.\n101\n102\n103\nSeconds (Log Scale)\n120\n100\n80\n60\n40\nTest Return\nMPE Simple Spread V3\nJaxMARL IQL\nJaxMARL QMIX\nJaxMARL VDN\nPyMARL IQL\nPyMARL QMIX\nPyMARL VDN\n(a) Simple Spread Training Time\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTimesteps\n1e6\n180\n160\n140\n120\n100\n80\n60\n40\nTest Returns\nTest returns in MPE Simple Spread V3\nJaxMARL IQL\nJaxMARL QMIX\nJaxMARL VDN\nPyMARL IQL\nPyMARL QMIX\nPyMARL VDN\n(b) Simple Spread Returns\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTimesteps\n1e6\n300\n250\n200\n150\n100\n50\n0\nTest Returns\nTest returns in MPE Simple Speaker Listener V4\nJaxMARL IQL\nJaxMARL QMIX\nJaxMARL VDN\nPyMARL IQL\nPyMARL QMIX\nPyMARL VDN\n(c) Speaker-Listener Returns\n102\n103\n104\nSeconds (Log Scale)\n0\n200\n400\n600\n800\n1000\nNumber of Independent Training Runs\nTotal Vectorised Training Time for MPE Simple Spread V3\nSingle Pymarl QMIX\nJaxMARL QMIX\n(d) QMIX Training Speed\nFigure 6: Performance and speed of JaxMARL \ud835\udc44-Learning baselines compared to PyMARL on MPE. Our implementations\nmatch PyMARL\u2019s returns, while being over 2000x faster to train\n4.3\nAlgorithm Correctness\nWe verify the correctness of our algorithm implementations by\ncomparing to baselines from other libraries on the MPE Simple\nSpread and Simple Speaker Listener environments. For IPPO we\nreport the mean return across 3 seeds in Figure 5b. Results were\ncollected on the same hardware as listed in Section 4.1. Our IPPO\nimplementation obtains the same performance as MARLLIB and\nruns 250x quicker, taking only ten seconds to train.\nFor the \ud835\udc44-learning algorithms, we verify the correctness by com-\nparing with PyMARL implementations of the same algorithms on\nthe MPE Simple Spread and Simple Speaker Listener environments.\nIQL, VDN and QMIX all obtain the same or better results than their\nPyMARL counterparts. The returns are from greedy policies and\naveraged across 8 runs. The hyperparameters used are from the\nPyMARL library.\n4.4\nEnvironment Correctness\nMPE. Our MPE environment corresponds exactly to the Pet-\ntingZoo implementation. We validate this for each environment\nusing a uniform-random policy on 1000 rollouts, ensuring all ob-\nservations and rewards are within a tolerance of 1 \u00d7 10\u22124 at each\ntransition. This tolerance accounts for non-determinism due to\nrunning floating point computation on the GPU. The correspon-\ndence is also shown through the performance of IPPO in Figure 5b\nand the \ud835\udc44-learning algorithms in Figures 6b and 6c respectively, as\nthe performance of these algorithms is inline with existing base-\nlines [63]. We additionally report training performance for IQL on\nthe remaining MPE environments in Appendix C.2.\nOvercooked. The transition dynamics of our Overcooked imple-\nmentation match those of the Overcooked-AI implementation. We\ndemonstrate this by training an IPPO policy on our implementation\nand evaluating the policy on both our Overcooked implementation\nand the original at regular intervals. Results are illustrated in Fig-\nure 7a and performance is similar, demonstrating their equivalence.\nSMAX. SMAX and SMAC are different environments. How-\never, we demonstrate some similarity between them by comparing\nour IPPO and MAPPO implementations against MAPPO results\non SMAC, using the implementation from [54]. We show this in\nFigure 8. SMAX and SMAC have different opponent policies and\ndynamics, which makes this comparison more qualitative than pre-\ncise. We describe the differences between the two in more depth in\nin the supplementary material. However, despite these differences,\nthe environments seem similarly difficult, with some environments\nbeing more difficult in SMAC, and some more difficult in SMAX.\nThis is shown in Figure 8 and in the supplementary material.\nMABrax. As Brax differs subtly from MuJoCo, MABrax does not\ncorrespond to MAMuJoCo but the learning dynamics are qualita-\ntively similar. To demonstrate this, we report mean training return\nacross 10 seeds for IPPO on ant_4x2 in Figure 7b, and our results\nare in line with the performance of TRPO reported in [28]. We\nreport the performance of IPPO on HalfCheetah and Walker in\nAppendix C.1, the results are also in line with TRPO.\n0\n1\n2\n3\n4\nTimesteps\n1e6\n0\n50\n100\n150\n200\n250\nTest Return\nIPPO Overcooked Performance\nOvercooked-AI\nJaxMARL\n(a) Overcooked\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTimesteps\n1e7\n1000\n0\n1000\n2000\n3000\n4000\n5000\nTraining Return\nMABrax ant_4x2\nIPPO\n(b) MABrax Ant\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e10\n5\n10\n15\n20\nTest Returns\nTest returns in Hanabi\nIPPO\n(c) 2 Player Hanabi\nFigure 7: JaxMARL IPPO baseline results. These results cor-\nrespond to similar baselines and therefore demonstrate the\ncorrectness of our implementations.\n0\n10M\nStep\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nMean Train Win Rate\n6h_vs_8z\n0\n10M\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n3s5z\n0\n10M\nStep\n0.0\n0.2\n0.4\n0.6\n27m_vs_30m\nSMAX IPPO\nSMAX MAPPO\nSMAC MAPPO\nFigure 8: SMAX IPPO and MAPPO baselines compared to\nMAPPO in SMAC.\nTable 3: Recommended Minimal Environment Evaluations for different research settings\nSetting\nRecommended Environments\nCTDE\nSMAX (all scenarios), Hanabi (2-5 players), Overcooked\nZero-shot Coordination\nHanabi (2 players), Overcooked (5 basic scenarios)\nGeneral-Sum\nSTORM (iterated prisoner\u2019s dilemma), STORM (matching pennies)\nCooperative Continuous Actions\nMABrax\nHanabi. Our implementation does not correspond exactly to\nthe Hanabi Learning Environment as we use a subtly different\nobservation space, with the reasoning given in Appendix A.4. To\ndemonstrate qualitative similarity, we train IPPO on Hanabi in\nself-play with 2 players, with the mean test return across 3 seeds\nreported in Figure 7c.\nSTORM, Coin Game & Switch Riddle. STORM differs from Melt-\ning Pot 2.0 significantly, making direct comparisons challenging,\nwith differences discussed in Appendix A.3. Furthermore, STORM\nand Coin Game are general-sum games, so the environment returns\nof IPPO in self-play would not be a good indicator of performance.\nSwitch Riddle is a simple diagnostic environment \u2013 we do not use\nit for thorough evaluations.\n5\nEVALUATION RECOMMENDATIONS\nPrevious work [20] has found significant differences in the evalu-\nation protocols between MARL research works. We identify four\nmain research areas that would benefit from our library: coopera-\ntive centralised training with decentralised execution (CTDE) [15],\nzero-shot coordination [22], general-sum games, and cooperative\ncontinuous action methods.\nTo aid comparisons between methods, we recommend standard\nminimal sets of evaluation environments for each of these settings\nin Table 3. It\u2019s important to note that these are minimal and we\nencourage as broad an evaluation as possible. For example, in the\nzero-shot coordination setting, all methods should be able to evalu-\nate on Hanabi and Overcooked. However, it may also be possible to\nevaluate such methods on the SMACv2 settings of SMAX. Similarly,\nSMAX could be used to evaluate two-player zero-sum methods by\ntraining in self-play. For some settings, such as continuous action\nenvironments and general-sum games, there is only one difficult\nenvironment. We encourage further development of JAX-based en-\nvironments in these settings to improve the quality of evaluation.\n6\nRELATED WORK\nSeveral open-source libraries exist for both MARL algorithms and\nenvironments. The popular library PyMARL [49] provides PyTorch\nimplementations of QMIX, VDN and IQL and integrates easily with\nSMAC. E-PyMARL [43] extends this by adding the actor-critic al-\ngorithms MADDPG [32], MAA2C [39], IA2C [39], and MAPPO,\nand supports the SMAC, Gym [8], Robot Warehouse [10], Level-\nBased Foraging [10], and MPE environments. Recently released\nMARLLib [23] is instead based on the open-source RL library RL-\nLib [31] and combines a wide range of competitive, cooperative and\nmixed environments with a broad set of baseline algorithms. Mean-\nwhile, MALib [66] focuses on population-based MARL across a wide\nrange of environments. However, none of these frameworks feature\nhardware-accelerated environments and thus lack the associated\nperformance benefits.\nThere has also been a recent proliferation of hardware-accelerated\nand JAX-based RL environments. Isaac gym [37] provides a GPU-\naccelerated simulator for a range of robotics platforms and CuLE [11]\nis a CUDA reimplementation of the Atari Learning Environment [3].\nBoth of these environments are GPU-specific and cannot be ex-\ntended to other hardware accelerators. Madrona [52] is an exten-\nsible game-engine written in C++ that allows for GPU accelera-\ntion and parallelisation across environments. However, it requires\nenvironment code to be written in C++, limiting its accessibility.\nVMAS [5] provides a vectorized 2D physics engine written in Py-\nTorch and a set of challenging multi-robot scenarios, including those\nfrom the MPE environment. For RL environments implemented in\nJAX, Jumanji [6] features mostly single-agent environments with a\nstrong focus on combinatorial problems. The authors also provide\nan actor-critic baseline in addition to random actions. PGX [27]\nincludes several board-game environments written in JAX. Gym-\nnax [29] provides JAX implementations of the BSuite [42], classic\ncontinuous control, MinAtar [62] and other assorted environments.\nGymnax\u2019s sister-library, gymnax-baselines, provides PPO and ES\nbaselines. Further extensions to Gymnax [34] also include POPGym\nenvironments [41]. Brax [18] reimplements the MuJoCo simula-\ntor in JAX and also provides a PPO implementation as a baseline.\nJax-LOB [19] implements a vectorized limit order book as an RL\nenvironment that runs on the accelerator. Perhaps the most similar\nto our work is Mava [45], which provides a MAPPO baseline, as\nwell as integration with the Robot Warehouse environment. How-\never, none of these libraries combine a range of JAX-based MARL\nenvironments with both value-based and actor-critic baselines.\nBroadly, no other work provides implementations of a wide\nrange of hardware-accelerated MARL environments, while also\nimplementing value-based and actor-critic baselines. Secondly, no\nother JAX simplification of SMAC exists. All other versions are\neither tied to the StarCraft II simulator or not hardware accelerated.\n7\nCONCLUSION\nHardware acceleration offers important opportunities for MARL\nresearch by lowering computational barriers, increasing the speed\nat which ideas can be iterated, and allowing for more thorough\nevaluation. We present JaxMARL, an open-source library of popu-\nlar MARL environments and baseline algorithms implemented in\nJAX. We combine ease of use with hardware accelerator enabled\nefficiency to give significant speed-ups compared to traditional\nCPU-based implementations. Furthermore, by bringing together a\nwide range of MARL environments under one codebase, we have the\npotential to help alleviate issues with MARL\u2019s evaluation standards.\nWe hope that JaxMARL will help advance MARL by improving the\nability of academic labs to conduct research with thorough, fast,\nand effective evaluations.\n8\nAUTHOR CONTRIBUTIONS\nThis project is a large-scale effort spanning many labs and contrib-\nutors.\nAR\u2217\u2020 led the design of the JaxMARL API and interface the imple-\nmentation of IPPO and MPE environments. BE\u2217\u2020 led the design and\nimplementation of the SMAX environments and IPPO evaluations.\nAR and BE also led the writing of this manuscript. MG\u2217\u2020 led the im-\nplementation of the off-policy MARL algorithms, their evaluations.,\nand the implementation of the Switch Riddle environment.\nJC\u2217 led the implementation of the Hanabi environment and heav-\nily assisted with benchmarking and verifying its performance. AL\u2217\nled the implementation of the Overcooked environments. GI\u2217 led\nthe implementation of the Multi-Agent Brax environments. TW\u2217\nled the implementation of the STORM environments. AK and AS\nworked on the STORM environments. CSW led the implementation\nof the Predator-Prey environment.\nCSW, SB, MS, MJ, and RL provided invaluable discussions for\nproject planning and implementations across the project. SB helped\ninitiate the project plan. MS worked on the Multi-Agent Brax envi-\nronments. MJ worked on the Overcooked and Hanabi environments.\nRL assisted with the design of the API and testing infrastructure.\nSW, BL, NH, and TR provided invaluable feedback on the project,\nmanuscript, and results.\nCL\u2217\u2020 initiated the project and led the organizational and planning\nefforts, speed-based benchmarking, and Coin Game implementa-\ntion.\nJF is the primary advisor for the project.\nREFERENCES\n[1] John P Agapiou, Alexander Sasha Vezhnevets, Edgar A Du\u00e9\u00f1ez-Guzm\u00e1n, Jayd\nMatyas, Yiran Mao, Peter Sunehag, Raphael K\u00f6ster, Udari Madhushani, Kavya\nKopparapu, Ramona Comanescu, et al. 2022. Melting Pot 2.0. arXiv preprint\narXiv:2211.13746 (2022).\n[2] Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Fran-\ncis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward\nHughes, et al. 2020. The hanabi challenge: A new frontier for ai research. Artificial\nIntelligence 280 (2020), 103216.\n[3] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. 2013. The Arcade Learning\nEnvironment: An Evaluation Platform for General Agents. Journal of Artificial\nIntelligence Research 47 (jun 2013), 253\u2013279.\n[4] Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein.\n2002. The complexity of decentralized control of Markov decision processes.\nMathematics of operations research 27, 4 (2002), 819\u2013840.\n[5] Matteo Bettini, Ryan Kortvelesy, Jan Blumenkamp, and Amanda Prorok. 2022.\nVMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning. The\n16th International Symposium on Distributed Autonomous Robotic Systems (2022).\n[6] Cl\u00e9ment Bonnet, Daniel Luo, Donal Byrne, Shikha Surana, Vincent Coyette,\nPaul Duckworth, Laurence I. Midgley, Tristan Kalloniatis, Sasha Abramowitz,\nCemlyn N. Waters, Andries P. Smit, Nathan Grinsztajn, Ulrich A. Mbou Sob,\nOmayma Mahjoub, Elshadai Tegegn, Mohamed A. Mimouni, Raphael Boige, Ruan\nde Kock, Daniel Furelos-Blanco, Victor Le, Arnu Pretorius, and Alexandre Laterre.\n2023. Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments\nin JAX. arXiv:2306.09884 [cs.LG] https://arxiv.org/abs/2306.09884\n[7] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris\nLeary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye\nWanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of\nPython+NumPy programs. http://github.com/google/jax\n[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John\nSchulman, Jie Tang, and Wojciech Zaremba. 2016.\nOpenAI Gym.\narXiv:arXiv:1606.01540\n[9] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel,\nand Anca Dragan. 2019. On the utility of learning about humans for human-ai\ncoordination. Advances in neural information processing systems 32 (2019).\n[10] Filippos Christianos, Lukas Sch\u00e4fer, and Stefano V Albrecht. 2020. Shared Ex-\nperience Actor-Critic for Multi-Agent Reinforcement Learning. In Advances in\nNeural Information Processing Systems (NeurIPS).\n[11] Steven Dalton and iuri frosio. 2020.\nAccelerating Reinforcement Learning\nthrough GPU Atari Emulation. In Advances in Neural Information Processing\nSystems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.),\nVol. 33. Curran Associates, Inc., 19773\u201319782. https://proceedings.neurips.cc/\npaper/2020/file/e4d78a6b4d93e1d79241f7b282fa3413-Paper.pdf\n[12] Rodrigo de Lazcano, Kallinteris Andreas, Jun Jet Tai, Seungjae Ryan Lee, and\nJordan Terry. 2023. Gymnasium Robotics. http://github.com/Farama-Foundation/\nGymnasium-Robotics\n[13] Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviy-\nchuk, Philip H. S. Torr, Mingfei Sun, and Shimon Whiteson. 2020.\nIs In-\ndependent Learning All You Need in the StarCraft Multi-Agent Challenge?\nhttps://doi.org/10.48550/arXiv.2011.09533 arXiv:2011.09533 [cs].\n[14] Benjamin Ellis, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Ma-\nhajan, Jakob N Foerster, and Shimon Whiteson. 2022. SMACv2: An improved\nbenchmark for cooperative multi-agent reinforcement learning. arXiv preprint\narXiv:2212.07489 (2022).\n[15] Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon\nWhiteson. 2016. Learning to Communicate with Deep Multi-Agent Reinforce-\nment Learning. In Advances in Neural Information Processing Systems, D. Lee,\nM. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Eds.), Vol. 29. Curran\nAssociates, Inc.\nhttps://proceedings.neurips.cc/paper_files/paper/2016/file/\nc7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf\n[16] Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter\nAbbeel, and Igor Mordatch. 2018. Learning with Opponent-Learning Awareness.\nIn Proceedings of the 17th International Conference on Autonomous Agents and\nMultiAgent Systems. 122\u2013130.\n[17] Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras,\nPhilip HS Torr, Pushmeet Kohli, and Shimon Whiteson. 2017. Stabilising ex-\nperience replay for deep multi-agent reinforcement learning. In International\nconference on machine learning. PMLR, 1146\u20131155.\n[18] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and\nOlivier Bachem. 2021. Brax - A Differentiable Physics Engine for Large Scale Rigid\nBody Simulation. http://github.com/google/brax\n[19] Sascha Frey, Kang Li, Peer Nagy, Silvia Sapora, Chris Lu, Stefan Zohren, Jakob\nFoerster, and Anisoara Calinescu. 2023. JAX-LOB: A GPU-Accelerated limit\norder book simulator to unlock large scale reinforcement learning for trading.\narXiv preprint arXiv:2308.13289 (2023).\n[20] Rihab Gorsane, Omayma Mahjoub, Ruan de Kock, Roland Dubb, Siddarth Singh,\nand Arnu Pretorius. 2022. Towards a Standardised Performance Evaluation\nProtocol for Cooperative MARL. arXiv preprint arXiv:2209.10485 (2022).\n[21] Hengyuan Hu and Jakob N Foerster. 2020. Simplified Action Decoder for Deep\nMulti-Agent Reinforcement Learning. In International Conference on Learning\nRepresentations. https://openreview.net/forum?id=B1xm3RVtwB\n[22] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. \u201cother-\nplay\u201d for zero-shot coordination. In International Conference on Machine Learning.\nPMLR, 4399\u20134410.\n[23] Siyi Hu, Yifan Zhong, Minquan Gao, Weixun Wang, Hao Dong, Zhihui Li, Xiao-\ndan Liang, Xiaojun Chang, and Yaodong Yang. 2022. MARLlib: Extending RLlib\nfor Multi-agent Reinforcement Learning. arXiv preprint arXiv:2210.13708 (2022).\n[24] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam\nChakraborty, Kinal Mehta, and Jo\u00e3o G.M. Ara\u00fajo. 2022. CleanRL: High-quality\nSingle-file Implementations of Deep Reinforcement Learning Algorithms. Journal\nof Machine Learning Research 23, 274 (2022), 1\u201318. http://jmlr.org/papers/v23/21-\n1342.html\n[25] Roberto Ierusalimschy. 2006. Programming in lua. Roberto Ierusalimschy.\n[26] Akbir Khan, Newton Kwan, Timon Willi, Chris Lu, Andrea Tacchetti, and\nJakob Nicolaus Foerster. 2022. Context and History Aware Other-Shaping. (2022).\n[27] Sotetsu Koyamada, Shinri Okano, Soichiro Nishimori, Yu Murata, Keigo Habara,\nHaruka Kita, and Shin Ishii. 2023. Pgx: Hardware-accelerated Parallel Game\nSimulators for Reinforcement Learning. arXiv preprint arXiv:2303.17503 (2023).\n[28] Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun\nWang, and Yaodong Yang. 2021. Trust region policy optimisation in multi-agent\nreinforcement learning. arXiv preprint arXiv:2109.11251 (2021).\n[29] Robert Tjarko Lange. 2022. gymnax: A JAX-based Reinforcement Learning Envi-\nronment Library. http://github.com/RobertTLange/gymnax\n[30] Niklas Lauffer, Ameesh Shah, Micah Carroll, Michael D Dennis, and Stuart Russell.\n2023. Who Needs to Know? Minimal Knowledge for Optimal Coordination. In\nInternational Conference on Machine Learning. PMLR, 18599\u201318613.\n[31] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Gold-\nberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. 2018. RLlib: Abstractions\nfor distributed reinforcement learning. In International conference on machine\nlearning. PMLR, 3053\u20133062.\n[32] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. 2017.\nMulti-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.\nNeural Information Processing Systems (NIPS) (2017).\n[33] Chris Lu, Jakub Kuba, Alistair Letcher, Luke Metz, Christian Schroeder de Witt,\nand Jakob Foerster. 2022. Discovered policy optimisation. Advances in Neural\nInformation Processing Systems 35 (2022), 16455\u201316468.\n[34] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satin-\nder Singh, and Feryal Behbahani. 2023. Structured state space models for in-\ncontext reinforcement learning. arXiv preprint arXiv:2303.03982 (2023).\n[35] Christopher Lu, Timon Willi, Christian A Schroeder De Witt, and Jakob Foerster.\n2022. Model-Free Opponent Shaping. In International Conference on Machine\nLearning. PMLR, 14398\u201314411.\n[36] Chris Lu, Timon Willi, Alistair Letcher, and Jakob Nicolaus Foerster. 2023. Ad-\nversarial cheap talk. In International Conference on Machine Learning. PMLR,\n22917\u201322941.\n[37] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier\nStorey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur\nHanda, and Gavriel State. 2021. Isaac Gym: High Performance GPU-Based\nPhysics Simulation For Robot Learning.\n[38] Adam Michalski, Filippos Christianos, and Stefano V Albrecht. 2023. SMAClite:\nA Lightweight Environment for Multi-Agent Reinforcement Learning. arXiv\npreprint arXiv:2305.05566 (2023).\n[39] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-\nothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-\nchronous methods for deep reinforcement learning. In International conference\non machine learning. PMLR, 1928\u20131937.\n[40] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. 2015. Human-level control through deep reinforcement learning.\nnature 518, 7540 (2015), 529\u2013533.\n[41] Steven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda\nProrok. 2023. POPGym: Benchmarking Partially Observable Reinforcement\nLearning. In The Eleventh International Conference on Learning Representations.\nhttps://openreview.net/forum?id=chDrutUTs0K\n[42] Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre\nSaraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al.\n2019. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568\n(2019).\n[43] Georgios Papoudakis, Filippos Christianos, Lukas Sch\u00e4fer, and Stefano V. Al-\nbrecht. 2021. Benchmarking Multi-Agent Deep Reinforcement Learning Algo-\nrithms in Cooperative Tasks. In Proceedings of the Neural Information Processing\nSystems Track on Datasets and Benchmarks (NeurIPS). http://arxiv.org/abs/2006.\n07869\n[44] Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kami-\nenny, Philip Torr, Wendelin B\u00f6hmer, and Shimon Whiteson. 2021. Facmac: Fac-\ntored multi-agent centralised policy gradients. Advances in Neural Information\nProcessing Systems 34 (2021), 12208\u201312221.\n[45] Arnu Pretorius, Kale ab Tessera, Andries P. Smit, Kevin Eloff, Claude Formanek,\nSt John Grimbly, Siphelele Danisa, Lawrence Francis, Jonathan Shock, Herman\nKamper, Willie Brink, Herman Engelbrecht, Alexandre Laterre, and Karim Beguir.\n2021. Mava: A Research Framework for Distributed Multi-Agent Reinforcement\nLearning. arXiv preprint arXiv:2107.01460 (2021).\nhttps://arxiv.org/pdf/2107.\n01460.pdf\n[46] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Far-\nquhar, Jakob Foerster, and Shimon Whiteson. 2020. Monotonic value function\nfactorisation for deep multi-agent reinforcement learning. The Journal of Machine\nLearning Research 21, 1 (2020), 7234\u20137284.\n[47] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob\nFoerster, and Shimon Whiteson. 2018. Qmix: Monotonic value function factori-\nsation for deep multi-agent reinforcement learning. In International conference\non machine learning. PMLR, 4295\u20134304.\n[48] Amit Sabne. 2020. XLA : Compiling Machine Learning for Peak Performance.\n[49] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Far-\nquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob\nFoerster, and Shimon Whiteson. 2019. The starcraft multi-agent challenge. arXiv\npreprint arXiv:1902.04043 (2019).\n[50] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,\nLaurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,\nThore Graepel, et al. 2020. Mastering atari, go, chess and shogi by planning with\na learned model. Nature 588, 7839 (2020), 604\u2013609.\n[51] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\n(2017).\n[52] Brennan Shacklett, Luc Guy Rosenzweig, Zhiqiang Xie, Bidipta Sarkar, Andrew\nSzot, Erik Wijmans, Vladlen Koltun, Dhruv Batra, and Kayvon Fatahalian. 2023.\nAn Extensible, Data-Oriented Architecture for High-Performance, Many-World\nSimulation. ACM Trans. Graph. 42, 4 (2023).\n[53] Glenn H Snyder. 1971. \" Prisoner\u2019s Dilemma\" and\" Chicken\" Models in Interna-\ntional Politics. International Studies Quarterly 15, 1 (1971), 66\u2013103.\n[54] Mingfei Sun, Sam Devlin, Jacob Beck, Katja Hofmann, and Shimon Whiteson.\n2023. Trust region bounds for decentralized ppo under non-stationarity. In\nProceedings of the 2023 International Conference on Autonomous Agents and Mul-\ntiagent Systems. 5\u201313.\n[55] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vini-\ncius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl\nTuyls, et al. 2017. Value-decomposition networks for cooperative multi-agent\nlearning. arXiv preprint arXiv:1706.05296 (2017).\n[56] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vini-\ncius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl\nTuyls, et al. 2018. Value-Decomposition Networks For Cooperative Multi-Agent\nLearning Based On Team Reward. In Proceedings of the 17th International Con-\nference on Autonomous Agents and MultiAgent Systems, (AAMAS 2018), Vol. 3.\n2085\u20132087.\n[57] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus,\nJuhan Aru, Jaan Aru, and Raul Vicente. 2017. Multiagent cooperation and\ncompetition with deep reinforcement learning. PloS one 12, 4 (2017), e0172395.\n[58] J Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari,\nRyan Sullivan, Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo\nPerez-Vicente, et al. 2021. Pettingzoo: Gym for multi-agent reinforcement learn-\ning. Advances in Neural Information Processing Systems 34 (2021), 15032\u201315043.\n[59] Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012. MuJoCo: A physics engine\nfor model-based control. In 2012 IEEE/RSJ International Conference on Intelligent\nRobots and Systems. IEEE, 5026\u20135033. https://doi.org/10.1109/IROS.2012.6386109\n[60] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement\nlearning with double q-learning. In Proceedings of the AAAI conference on artificial\nintelligence, Vol. 30.\n[61] Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Vik-\ntor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, Zhong-\nwen Xu, and Shuicheng Yan. 2022.\nEnvPool: A Highly Parallel Rein-\nforcement Learning Environment Execution Engine. In Advances in Neu-\nral Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc.,\n22409\u201322421.\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/\n8caaf08e49ddbad6694fae067442ee21-Paper-Datasets_and_Benchmarks.pdf\n[62] Kenny Young and Tian Tian. 2019. MinAtar: An Atari-Inspired Testbed for\nThorough and Reproducible Reinforcement Learning Experiments. arXiv preprint\narXiv:1903.03176 (2019).\n[63] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen,\nand Yi Wu. 2022. The surprising effectiveness of ppo in cooperative multi-agent\ngames. Advances in Neural Information Processing Systems 35 (2022), 24611\u2013\n24624.\n[64] Qizhen Zhang, Chris Lu, Animesh Garg, and Jakob Foerster. 2022. Centralized\nModel and Exploration Policy for Multi-Agent RL. In Proceedings of the 21st\nInternational Conference on Autonomous Agents and Multiagent Systems. 1500\u2013\n1508.\n[65] Stephen Zhao, Chris Lu, Roger B Grosse, and Jakob Foerster. 2022. Proximal\nLearning With Opponent-Learning Awareness. Advances in Neural Information\nProcessing Systems 35 (2022), 26324\u201326336.\n[66] Ming Zhou, Ziyu Wan, Hanjing Wang, Muning Wen, Runzhe Wu, Ying Wen,\nYaodong Yang, Yong Yu, Jun Wang, and Weinan Zhang. 2023. MALib: A Parallel\nFramework for Population-based Multi-agent Reinforcement Learning. Journal\nof Machine Learning Research 24, 150 (2023), 1\u201312. http://jmlr.org/papers/v24/22-\n0169.html\nA\nFURTHER DETAILS ON ENVIRONMENTS\nA.1\nSMAX\nObservations in SMAX are structured similarly to SMAC. Each\nagent observes the health, previous action, position, weapon cooldown\nand unit type of all allies and enemies in its sight range. Like\nSMACv2[14], we use the sight and attack ranges as prescribed\nby StarCraft II rather than the fixed values used in SMAC.\nSMAX and SMAC have different returns. SMAC\u2019s reward func-\ntion, like SMAX\u2019s, is split into two parts: one part for depleting\nenemy health, and another for winning the episode. However,\nin SMAC, the part which rewards depleting enemy health scales\nwith the number of agents. This is most clearly demonstrated in\n27m_vs_30m, where a random policy gets a return of around 10 out\nof a maximum of 20 because almost all the reward is for depleting\nenemy health or killing agents, rather than winning the episode.\nIn SMAX, however, 50% of the total return is always for depleting\nenemy health, and 50% for winning. Unlike StarCraft II, where all\nactions happen in a randomised order in the game loop, some ac-\ntions in SMAX are simultaneous, meaning draws are possible. In\nthis case both teams get 0 reward.\nLike SMAC, each environment step in SMAX consists of eight\nindividual time ticks. SMAX uses a discrete action space, consisting\nof movement in the four cardinal directions, a stop action, and a\nshoot action per enemy.\nSMAX makes three notable simplifications of the StarCraft II\ndynamics to reduce complexity. First, zerg units do not regenerate\nhealth. This health regeneration is slow at 0.38 health per second,\nand so likely has little impact on the game. Protoss units also do\nnot have shields. Shields only recharge after 10 seconds out of\ncombat, and therefore are unlikely to recharge during a single\nmicromanagement task. Protoss units have additional health to\ncompensate for their lost shields. Finally, the available unit types\nare reduced compared to SMAC. SMAX has no medivac, colossus or\nbaneling units. Each of these unit types has special mechanics that\nwere left out for the sake of simplicity. For the SMACv2 scenarios,\nthe start positions are generated as in SMACv2, with the small\ndifference that the \u2018surrounded\u2019 start positions now treat allies\nand enemies identically, rather than always spawning allies in the\nmiddle of the map. This symmetry guarantees that a 50% win rate\nis always achievable.\nCollisions are handled by moving agents to their desired location\nfirst and then pushing them out from one another.\nA.2\nCoin Game\nTwo agents, \u2018red\u2019 and \u2018blue\u2019, move in a wrap-around grid and collect\nred and blue coloured coins. When an agent collects any coin,\nthe agent receives a reward of 1. However, when \u2018red\u2019 collects\na blue coin, \u2018blue\u2019 receives a reward of \u22122 and vice versa. Once\na coin is collected, a new coin of the same colour appears at a\nrandom location within the grid. If a coin is collected by both agents\nsimultaneously, the coin is duplicated and both agents collect it.\nEpisodes are of a set length.\nA.3\nSpatial-Temporal Representations of Matrix\nGames (STORM)\nThis environment features directional agents within an 8x8 grid-\nworld with a restricted field of view. Agents cannot move backwards\nor share the same location. Collisions are resolved by either giving\npriority to the stationary agent or randomly if both are moving.\nAgents collect two unique resources: cooperate and defect coins.\nOnce an agent picks up any coin, the agent\u2019s colour shifts, indicat-\ning its readiness to interact. The agents can then release an interact\nbeam directly ahead; when this beam intersects with another ready\nagent, both are rewarded based on the specific matrix game payoff\nmatrix. The agents\u2019 coin collections determine their strategies. For\ninstance, if an agent has 1 cooperate coin and 3 defect coins, there\u2019s\na 25% likelihood of the agent choosing to cooperate. After an inter-\naction, the two agents involved are frozen for five steps, revealing\ntheir coin collections to surrounding agents. After five steps, they\nrespawn in a new location, with their coin count set back to zero.\nOnce an episode concludes, the coin placements are shuffled. This\ngrid-based approach to matrix games can be adapted for n-player\nversions. While STORM is inspired by MeltingPot 2.0, there are\nnoteworthy differences:\n\u2022 Meltingpot uses pixel-based observations while we allow\nfor direct grid access.\n\u2022 Meltingpot\u2019s grid size is typically 23x15, while ours is 8x8.\n\u2022 Meltingpot features walls within its layout, ours does not.\n\u2022 Our environment introduces stochasticity by shuffling the\ncoin placements, which remain static in Meltingpot.\n\u2022 Our agents begin with an empty coin inventory, making it\neasier for them to adopt pure cooperate or defect tactics,\nunlike in Meltingpot where they start with one of each coin.\n\u2022 MeltingPot is implemented in Lua [25] where as ours is a\nvectorized implementation in Jax.\nWe deem the coin shuffling especially crucial because even large\nenvironments representing POMDPs, such as SMAC, can be solved\nwithout the need for memory if they lack sufficient randomness [14].\nA.4\nHanabi\nThere are a few details that differ between our Hanabi implemen-\ntation and the original Hanabi Learning Environment (HLE). The\nmost notable of these is how we choose to represent card knowledge\ninformation in the agents\u2019 observation. In the HLE, card knowledge\nis observed as a colour/rank if there has been an explicit hint about\na given card. As a separate feature, implicit card knowledge is rep-\nresented as possible colours/ranks if there has not been an explicit\nhint that indicates a given card is not that colour/rank. We, on the\nother hand, combine implicit and explicit card knowledge, by only\nmaintaining a representation of implicit card knowledge, which\nreduces to explicit card knowledge in the event an explicit hint is\ngiven about a card. This is because all possible colours/ranks are\nrepresented as 1s, whilst all ruled out colours/ranks are represented\nas 0s. By giving an explicit hint, all but one colour/rank are ruled\nout, leaving a one-hot encoding of the explicit card knowledge. We\nimplement card knowledge this way, because knowledge updates\nare implemented via tensor calculus using JAX Numpy arrays of\nfixed shape and data type.\nB\nVALUE-BASED MARL METHODS AND\nIMPLEMENTATION DETAILS\nKey features of our framework include parameter sharing, a re-\ncurrent neural network (RNN) for agents, an epsilon-greedy ex-\nploration strategy with linear decay, a uniform experience replay\nbuffer, and the incorporation of Double Deep \ud835\udc44-Learning (DDQN)\n[60] techniques to enhance training stability.\nUnlike PyMARL, we use the Adam optimizer as the default op-\ntimization algorithm. Below is an introduction to common value-\nbased MARL methods.\nIQL (Independent \ud835\udc44-Learners) is a straightforward adaptation\nof Deep \ud835\udc44-Learning to multi-agent scenarios. It features multiple\n\ud835\udc44-Learner agents that operate independently, optimizing their indi-\nvidual returns. This approach follows a decentralized learning and\ndecentralized execution pipeline.\nVDN (Value Decomposition Networks) extends \ud835\udc44-Learning to\nmulti-agent scenarios with a centralized-learning-decentralized-\nexecution framework. Individual agents approximate their own\naction\u2019s \ud835\udc44-Value, which is then summed during training to compute\na jointed \ud835\udc44\ud835\udc61\ud835\udc5c\ud835\udc61 for the global state-action pair. Back-propagation of\nthe global DDQN loss in respect to a global team reward optimizes\nthe factorization of the jointed \ud835\udc44-Value.\nQMIX improves upon VDN by relaxing the full factorization\nrequirement. It ensures that a global \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 operation on the total\n\ud835\udc44-Value (\ud835\udc44\ud835\udc61\ud835\udc5c\ud835\udc61) is equivalent to individual \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 operations on\neach agent\u2019s \ud835\udc44-Value. This is achieved using a feed-forward neural\nnetwork as the mixing network, which combines agent network\noutputs to produce \ud835\udc44\ud835\udc61\ud835\udc5c\ud835\udc61 values. The global DDQN loss is com-\nputed using a single shared reward function and is back-propagated\nthrough the mixer network to the agents\u2019 parameters. Hypernet-\nworks generate the mixing network\u2019s weights and biases, ensuring\nnon-negativity using an absolute activation function. These hy-\npernetworks are two-layered multi-layer perceptrons with ReLU\nnon-linearity.\nC\nTRAINING RESULTS\nC.1\nMABrax\nThe performance of IPPO on HalfCheeta and Walker is reported in\nFigure 9, with hyperparameters reported in Table 4.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTimesteps\n1e7\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nTraining Return\nMABrax halfcheetah_6x1\nIPPO\n(a) HalfCheetah\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTimesteps\n1e7\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTraining Return\nMABrax walker2d_2x3\nIPPO\n(b) Walker\nFigure 9: Performance of IPPO on MABrax Tasks\nC.2\nMPE\nPerformance of \ud835\udc44-Learning baselines in all the MPE scenarios are\nreported in Figure 10. The upper row represents cooperative sce-\nnarios, with results for all our \ud835\udc44-learning baselines reported. The\nbottom row refers to competitive scenarios, and results for IQL are\ndivided by agent types. Hyperparameters are given in Table 9\nC.3\nSMAX\nThe performance of IPPO in SMAX versus MAPPO in SMAC is\nshown in Figure 11 while the performance of our \ud835\udc44-learning base-\nlines is reported in Figure 12. We do not report them together\nbecause their hyperparameters were tuned over a different num-\nber of timesteps. Hyperparameters for IPPO and the \ud835\udc44-learning\nmethods are given in Tables 6 and 10 respectively.\n0\n2M\nStep\n121\n82\n44\n6\nMean Return\nMpe Simple V3\nQMIX\nVDN\nIQL\n0\n2M\nStep\n163\n119\n76\n32\nMpe Simple Spread V3\nQMIX\nVDN\nIQL\n0\n2M\nStep\n218\n152\n85\n18\nMpe Simple Speaker Listener V4\nQMIX\nVDN\nIQL\n0\n2M\nStep\n81\n65\n49\n34\nMpe Simple Reference V3\nQMIX\nVDN\nIQL\n0\n2M\nStep\n47\n29\n12\n5\nMean Return\nMpe Simple Push V3\nADVERSARY\nAGENT\n0\n2M\nStep\n47\n22\n2\n26\nMpe Simple Adversary V3\nADVERSARY\nAGENTS\n0\n2M\nStep\n86\n29\n27\n84\nMpe Simple Tag V3\nAGENT\nADVERSARIES\n0\n2M\nStep\n254\n152\n50\n51\nMpe Simple World Comm V3\nLEADADVERSARY\nADVERSARIES\nAGENTS\nFigure 10: \ud835\udc44-Learning Baselines in all MPE scenarios. Where no algorithm names are given, the results represent IQL.\n0\n10M\nStep\n0.0\n0.2\n0.4\n0.6\nMean Train Win Rate\n27m_vs_30m\n0\n10M\nStep\n0.0\n0.2\n0.4\n5m_vs_6m\n0\n10M\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n2s3z\n0\n10M\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n3s5z\n0\n10M\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Train Win Rate\n10m_vs_11m\n0\n10M\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n3s_vs_5z\n0\n10M\nStep\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n6h_vs_8z\n0\n10M\nStep\n0.0\n0.1\n0.2\n0.3\n0.4\n3s5z_vs_3s6z\nSMAX IPPO\nSMAX MAPPO\nSMAC MAPPO\nFigure 11: IPPO and MAPPO in SMAX versus MAPPO in SMAC for all SMAC maps.\n0\n20M\nStep\n0\n5\n10\n15\n20\nTest Returns\n2s3z\nQMIX\nVDN\nIQL\n0\n20M\nStep\n0\n5\n10\n15\n20\n3s5z_vs_3s6z\n0\n20M\nStep\n0\n5\n10\n15\n20\n3s5z\n0\n20M\nStep\n0\n5\n10\n15\n20\n5m_vs_6m\n0\n20M\nStep\n0\n5\n10\n15\n20\nTest Returns\n27m_vs_30m\n0\n20M\nStep\n0\n5\n10\n15\n20\n10m_vs_11m\n0\n20M\nStep\n0\n5\n10\n15\n20\n3s_vs_5z\n0\n20M\nStep\n0\n5\n10\n15\n20\n3s5z_vs_3s6z\n0\n20M\nStep\n0\n5\n10\n15\n20\nTest Returns\nsmacv2_10_units\n0\n20M\nStep\n0\n5\n10\n15\n20\nsmacv2_5_units\n0\n20M\nStep\n0\n5\n10\n15\n20\nsmacv2_20_units\nFigure 12: Performance of \ud835\udc44-Learning Baselines for all SMAX scenarios\nD\nHYPERPARAMETERS\nValue\nAnt\nHalfCheetah\nWalker\nVF_COEF\n4.5\n0.14\n1.9\nENT_COEF\n2 \u00d7 10\u22126\n4.5 \u00d7 10\u22123\n1 \u00d7 10\u22123\nLR\n1 \u00d7 10\u22123\n6 \u00d7 10\u22124\n7 \u00d7 10\u22123\nNUM_ENVS\n64\n\u2013\n\u2013\nNUM_STEPS\n300\n\u2013\n\u2013\nTOTAL_TIMESTEPS\n1 \u00d7 108\n\u2013\n\u2013\nNUM_MINIBATCHES\n4\n\u2013\n\u2013\nGAMMA\n0.99\n\u2013\n\u2013\nGAE_LAMBDA\n1.0\n\u2013\n\u2013\nCLIP_EPS\n0.2\n\u2013\n\u2013\nMAX_GRAD_NORM\n0.5\n\u2013\n\u2013\nACTIVATION\ntanh\n\u2013\n\u2013\nANNEAL_LR\nTrue\n\u2013\n\u2013\nTable 4: MABrax Hyperparameters, where \u2013 indicates repeated parameters\nHyperparameter\nValue\nLR\n0.0005\nNUM_ENVS\n25\nNUM_STEPS\n128\nTOTAL_TIMESTEPS\n1 \u00d7 106\nUPDATE_EPOCHS\n5\nNUM_MINIBATCHES\n2\nGAMMA\n0.99\nGAE_LAMBDA\n1.0\nCLIP_EPS\n0.3\nENT_COEF\n0.01\nVF_COEF\n1.0\nMAX_GRAD_NORM\n0.5\nACTIVATION\ntanh\nANNEAL_LR\nTrue\nTable 5: Hyperparameters for MPE IPPO\nHyperparameter\nValue\nLR\n0.004\nNUM_ENVS\n64\nNUM_STEPS\n128\nTOTAL_TIMESTEPS\n1 \u00d7 107\nUPDATE_EPOCHS\n2\nNUM_MINIBATCHES\n2\nGAMMA\n0.99\nGAE_LAMBDA\n0.95\nCLIP_EPS\n0.2\nSCALE_CLIP_EPS\nFalse\nENT_COEF\n0.0\nVF_COEF\n0.5\nMAX_GRAD_NORM\n0.5\nACTIVATION\nrelu\nTable 6: Hyperparameters for SMAX IPPO\nHyperparameter\nValue\nLR\n5 \u00d7 10\u22124\nNUM_ENVS\n1024\nNUM_STEPS\n128\nTOTAL_TIMESTEPS\n1 \u00d7 1010\nUPDATE_EPOCHS\n4\nNUM_MINIBATCHES\n4\nGAMMA\n0.99\nGAE_LAMBDA\n0.95\nCLIP_EPS\n0.2\nENT_COEF\n0.01\nVF_COEF\n0.5\nMAX_GRAD_NORM\n0.5\nACTIVATION\nrelu\nANNEAL_LR\nTrue\nNUM_FC_LAYERS\n2\nLAYER_WIDTH\n512\nTable 7: Hyperparameters for Hanabi IPPO\nHyperparameter\nValue\nLR\n2.5 \u00d7 10\u22124\nNUM_ENVS\n16\nNUM_STEPS\n128\nTOTAL_TIMESTEPS\n5 \u00d7 106\nUPDATE_EPOCHS\n4\nNUM_MINIBATCHES\n4\nGAMMA\n0.99\nGAE_LAMBDA\n0.95\nCLIP_EPS\n0.2\nENT_COEF\n0.01\nVF_COEF\n0.5\nMAX_GRAD_NORM\n0.5\nACTIVATION\ntanh\nANNEAL_LR\nTrue\nNUM_EVALS\n16\nTable 8: Hyperparameters for Overcooked IPPO\nHyperparameter\nValue\nNUM_ENVS\n8\nNUM_STEPS\n25\nBUFFER_SIZE\n5000\nBUFFER_BATCH_SIZE\n32\nTOTAL_TIMESTEPS\n2 \u00d7 106\nAGENT_HIDDEN_DIM\n64\nAGENT_INIT_SCALE\n2.0\nEPSILON_START\n1.0\nEPSILON_FINISH\n0.05\nEPSILON_ANNEAL_TIME\n100000\nMIXER_EMBEDDING_DIM*\n32\nMIXER_HYPERNET_HIDDEN_DIM*\n64\nMIXER_INIT_SCALE*\n0.00001\nMAX_GRAD_NORM\n25\nTARGET_UPDATE_INTERVAL\n200\nLR\n0.005\nEPS_ADAM\n0.001\nWEIGHT_DECAY_ADAM\n0.00001\nGAMMA\n0.9\nNUM_TEST_EPISODES\n32\nTEST_INTERVAL\n50000\nTable 9: Hyperparameters for MPE \ud835\udc44-Learning Algorithms\n(* Parameters specific to QMix.)\nHyperparameter\nValue\nNUM_ENVS\n8\nNUM_STEPS\n100\nBUFFER_SIZE\n3000\nBUFFER_BATCH_SIZE\n32\nTOTAL_TIMESTEPS\n2 \u00d7 107\nAGENT_HIDDEN_DIM\n256\nAGENT_INIT_SCALE\n1.0\nEPSILON_START\n1.0\nEPSILON_FINISH\n0.05\nEPSILON_ANNEAL_TIME\n100000\nMIXER_EMBEDDING_DIM*\n64\nMIXER_HYPERNET_HIDDEN_DIM*\n256\nMIXER_INIT_SCALE*\n0.001\nMAX_GRAD_NORM\n10\nTARGET_UPDATE_INTERVAL\n200\nLR\n0.001\nEPS_ADAM\n0.00001\nWEIGHT_DECAY_ADAM\n1 \u00d7 10\u22126\nGAMMA\n0.99\nNUM_TEST_EPISODES\n32\nTEST_INTERVAL\n1 \u00d7 105\nTable 10: Hyperparameters for SMAX \ud835\udc44-Learning Algorithms\n(* Parameters specific to QMix.)\n"
  },
  {
    "title": "Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives",
    "link": "https://arxiv.org/pdf/2311.09227.pdf",
    "upvote": "5",
    "text": "Open-Sourcing Highly Capable Foundation Models:\nAn evaluation of risks, benefits, and alternative\nmethods for pursuing open-source objectives\nElizabeth Seger1,2,\u2217 Noemi Dreksler1\nRichard Moulange1,3\nEmily Dardaman4\nJonas Schuett1\nK. Wei1,5\nChristoph Winter6,7,8\nMackenzie Arnold8\nSe\u00e1n \u00d3\nh\u00c9igeartaigh2\nAnton Korinek1,9,10\nMarkus Anderljung1\nBen Bucknall11\nAlan Chan12,13\nEoghan Stafford1\nLeonie Koessler1\nAviv Ovadya14\nBen Garfinkel1\nEmma Bluemke1\nMichael Aird15\nPatrick Levermore15\nJulian Hazell1,16\nAbhishek Gupta3,17\n1Centre for the Governance of AI\n2AI: Futures and Responsibility Programme, University of\nCambridge\n3MRC Biostatistics Unit, University of Cambridge\n4BCG Henderson Institute\n5Harvard Law School\n6Harvard University\n7Instituto Tecnol\u00f3gico Aut\u00f3nomo de M\u00e9xico\n8Legal Priorities Project\n9University of Virginia\n10Brookings Institution\n11Independent\n12Mila\n13University of Montreal\n14Thoughtful Technology Project\n15Institute for AI Policy\n& Strategy\n16Oxford Internet Institute, University of Oxford\n17Montreal AI Ethics Institute\nAcknowledgements: We thank the following people for feedback and comments: An-\ndrew Trask, Ben Cottier, Herbie Bradley, Irene Solaiman, Norman Johnson, Peter Cihon,\nShahar Avin, Stella Biderman, Toby Shevlane.\nGiven the size of the group, inclusion as an author does not entail endorsement of all claims in\nthe paper, nor does inclusion entail an endorsement on the part of any individual\u2019s organization.\nAbstract\nRecent decisions by leading AI labs to either open-source their models or to restrict\naccess to their models has sparked debate about whether, and how, increasingly capable AI\nmodels should be shared. Open-sourcing in AI typically refers to making model architecture\nand weights freely and publicly accessible for anyone to modify, study, build on, and\nuse. This offers advantages such as enabling external oversight, accelerating progress, and\ndecentralizing control over AI development and use. However, it also presents a growing\npotential for misuse and unintended consequences. This paper offers an examination of the\nrisks and benefits of open-sourcing highly capable foundation models. While open-sourcing\nhas historically provided substantial net benefits for most software and AI development\nprocesses, we argue that for some highly capable foundation models likely to be developed in\nthe near future, open-sourcing may pose sufficiently extreme risks to outweigh the benefits.\nIn such a case, highly capable foundation models should not be open-sourced, at least\nnot initially. Alternative strategies, including non-open-source model sharing options, are\nexplored. The paper concludes with recommendations for developers, standard-setting\nbodies, and governments for establishing safe and responsible model sharing practices and\npreserving open-source benefits where safe.\n\u2217Corresponding author: elizabeth.seger@governance.ai\nPlease cite as Seger, Dreksler, Moulange, Dardaman, Schuett, Wei, et al, \u2018Open-Sourcing Highly Capable\nFoundation Models: An Evaluation of Risks, Benefits, and Alternative Methods for Pursuing Open-Source\nObjectives\u2019, Centre for the Governance of AI, 2023.\narXiv:2311.09227v1  [cs.CY]  29 Sep 2023\nExecutive Summary\nRecent decisions by AI developers to open-source foundation models have sparked debate over\nthe prudence of open-sourcing increasingly capable AI systems. Open-sourcing in AI typically\ninvolves making model architecture and weights freely and publicly accessible for anyone to modify,\nstudy, build on, and use. On the one hand, this offers clear advantages including enabling external\noversight, accelerating progress, and decentralizing AI control. On the other hand, it presents notable\nrisks, such as allowing malicious actors to use AI models for harmful purposes without oversight and\nto disable model safeguards designed to prevent misuse.\nThis paper attempts to clarify open-source terminology and to offer a thorough analysis of risks and\nbenefits from open-sourcing AI. While open-sourcing has, to date, provided substantial net benefits\nfor most software and AI development processes, we argue that for some highly capable models\nlikely to emerge in the near future, the risks of open sourcing may outweigh the benefits.\nThere are three main factors underpinning this concern:\n1. Highly capable models have the potential for extreme risks. Of primary concern is diffusion\nof dangerous AI capabilities that could pose extreme risks\u2014risk of significant physical harm or\ndisruption to key societal functions. Malicious actors might apply highly capable systems, for\ninstance, to help build new biological and chemical weapons, or to mount cyberattacks against\ncritical infrastructures and institutions. We also consider other risks such as models helping\nmalicious actors disseminate targeted misinformation at scale or to enact coercive population\nsurveillance.\nArguably, current AI capabilities do not yet surpass a critical threshold of capability for the most\nextreme risks. However, we are already seeing nascent dangerous capabilities emerge, and this\ntrend is likely to continue as models become increasingly capable and it becomes easier and\nrequires less expertise and compute resources for users to deploy and fine-tune these models.\n(Section 3)\n2. Open-sourcing is helpful in addressing some risks, but could\u2014overall\u2014exacerbate the\nextreme risks that highly capable AI models may pose. For traditional software, open-sourcing\nfacilitates defensive activities to guard against misuse more so than it facilitates offensive misuse\nby malicious actors. However, the offense-defense balance is likely to skew more towards offense\nfor increasingly capable foundation models for a variety of reasons including: (i) Open-sourcing\nallows malicious actors to disable safeguards against misuse and to possibly introduce new\ndangerous capabilities via fine-tuning. (ii) Open-sourcing greatly increases attacker knowledge\nof possible exploits beyond what they would have been able to easily discover otherwise. (iii)\nResearching safety vulnerabilities is comparatively time consuming and resource intensive, and\nfixes are often neither straightforward nor easily implemented. (iv) It is more difficult to ensure\nimprovements are implemented downstream, and flaws and safety issues are likely to perpetuate\nfurther due to the general use nature of the foundation models. (Section 3)\n3. There are alternative, less risky methods for pursuing open-source goals. There are a variety\nof strategies that might be employed to work towards the same goals as open-sourcing for highly\ncapable foundation models but with less risk, albeit with their own shortcomings. These alternative\nmethods include more structured model access options catered to specific research, auditing, and\ndownstream development needs, as well as proactive efforts to organize secure collaborations,\nand to encourage and enable wider involvement in AI development, evaluation, and governance\nprocesses. (Section 4)\nIn light of these potential risks, limitations, and alternatives, we offer the following recommenda-\ntions for developers, standards setting bodies, and governments. These recommendations are to help\nestablish safe and responsible model sharing practices and to preserve open-source benefits where\nsafe. They also summarize the paper\u2019s main takeaways. (Section 5)\n1. Developers and governments should recognize that some highly capable models will be too\nrisky to open-source, at least initially. These models may become safe to open-source in the\nfuture as societal resilience to AI risk increases and improved safety mechanisms are developed.\n2\n2. Decisions about open-sourcing highly capable foundation models should be informed by\nrigorous risk assessments. In addition to evaluating models for dangerous capabilities and\nimmediate misuse applications, risk assessments must consider how a model might be fine-tuned\nor otherwise amended to facilitate misuse.\n3. Developers should consider alternatives to open-source release that capture some of the\nsame distributive, democratic, and societal benefits, without creating as much risk. Some\npromising alternatives include gradual or \u201cstaged\u201d model release, structured model access for\nresearchers and auditors, and democratic oversight of AI development and governance decisions.\n4. Developers, standards setting bodies, and open-source communities should engage in col-\nlaborative and multi-stakeholder efforts to define fine-grained standards for when model\ncomponents should be released. These standards should be based on an understanding of the\nrisks posed by releasing different combinations of model components.\n5. Governments should exercise oversight of open-source AI models and enforce safety mea-\nsures when stakes are sufficiently high. AI developers may not voluntarily adopt risk assessment\nand model sharing standards. Governments will need to enforce such measures through options\nsuch as liability law and regulation, licensing requirements, fines, or penalties. They will also\nneed to build the capacity to enforce such oversight mechanisms effectively. Immediate work is\nneeded to evaluate the costs, consequences, and legal feasibility of various policy interventions\nand enforcement mechanisms we list.\n3\nContents\nExecutive Summary\n2\n1\nIntroduction\n5\n2\nWhat Do We Mean by \u201cOpen-Source Highly Capable Foundation Models\u201d?\n6\n2.1\nWhat are Highly Capable Foundation Models? . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nOpen-Source AI: Definition and Disanalogy . . . . . . . . . . . . . . . . . . . . .\n8\n3\nRisks of Open-Sourcing Foundation Models\n10\n3.1\nMalicious Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.1.1\nVarieties of Malicious Use . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.1.2\nEase of Malicious Use . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.1.3\nOffense-Defense Balance . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.2\nRisks from the Proliferation of Unresolved Model Flaws\n. . . . . . . . . . . . . .\n16\n4\nBenefits of Open-Sourcing Foundation Models and Alternative Methods for Achieving\nThem\n17\n4.1\nExternal Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4.1.1\nThe Argument for Open-Source . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.1.2\nEvaluating the Benefit for Foundation Models\n. . . . . . . . . . . . . . .\n18\n4.1.3\nOther Ways to Enable External Evaluation . . . . . . . . . . . . . . . . . .\n19\n4.2\nAccelerate (beneficial) AI Progress\n. . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.2.1\nThe Argument for Open-Source . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.2.2\nEvaluating the Benefit for Foundation Models . . . . . . . . . . . . . . . .\n22\n4.2.3\nOther Ways to Drive (Beneficial) Progress\n. . . . . . . . . . . . . . . . .\n25\n4.3\nDistribute Control Over AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.3.1\nThe Argument for Open-Source . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.3.2\nEvaluating the Benefit for Foundation Models . . . . . . . . . . . . . . . .\n27\n4.3.3\nOther Ways to Reduce Corporate or Autocratic Control . . . . . . . . . . .\n29\n5\nRecommendations\n30\n6\nConclusion\n34\nReferences\n35\nA AI Model Component Guide\n48\n4\n1\nIntroduction\nAs AI developers build increasingly capable models, they face a dilemma about whether and how\nthey should share their models. One foundational decision they must make is whether to open-source\ntheir models\u2014that is, make their models freely and publicly accessible for anyone to use, study,\nmodify, and share.1\nSoftware development communities have traditionally enjoyed strong norms for sharing and open-\nsource publication. Accordingly, for many AI researchers and developers open-sourcing is a deeply\nheld professional and personal value. However, this value can sit in tension with others, like growing\na profitable organization may contradict protecting consumers from harm [1]. Debate continues about\nthe risks, benefits, and tradeoffs of open-source model release.\nRecently, some large AI labs have decided that open-sourcing foundation models involves unaccept-\nable trade-offs and have chosen to restrict model access out of competitive concerns and worries\nabout model misuse. These labs are either keeping their models completely private (e.g., DeepMind\u2019s\nChinchilla [2]) or employing a structured access approach to model sharing (e.g., OpenAI\u2019s GPT-4\n[3] and Anthropic\u2019s Claude 2 [4] via their APIs [5], which enable the enforcement of user restrictions\nand implementation of controls such as safety filters in order to manage harms.\nThere has been Pushback against this trend to restrict model access and calls to reinforce traditional\nsoftware development community norms for sharing and openness is common.. The concerns are that\nmodel access restriction stifles innovation, disallows external oversight, hinders the distribution of AI\nbenefits, and concentrates control over AI\u2019s future to a small number of major AI labs [6, 7]. Labs\nsuch as Hugging Face, Allen Institute for AI, EleutherAI, RedPajama, LAION, Together.xyz, Mosaic,\nand Stability AI have recently chosen to open-source large models. Meta has been a particularly\nvocal open-source proponent with its release of I-JEPA [8], an efficient and visual transformer in\nJune 2023, followed closely by Llama 2 [9\u201311], in July 2023.\nThere are many considerable benefits of open-source software (OSS) development. For thirty years,\nOSS has proliferated alongside, and often inside, of commercial software, encouraging cooperation,\npromoting software adoption via lowered costs, reducing monopolistic control by major software\ncompanies, fostering rapid innovation, growing talent, and improving software quality through\ncommunity review [12\u201314]. The academic tradition in which many machine learning researchers are\ntrained also enjoys strong norms of open research publication. It is only natural that many machine\nlearning developers and researchers follow suit, creating groups and organizations like Hugging\nFace, Stability AI, RedPajama, and EleutherAI in order to build and release increasingly capable AI\nmodels.\nHowever, we will explain that there is a disanalogy between OSS and open-source AI, and that\nwe should not expect these same benefits to seamlessly translate from OSS to cutting-edge AI\ndevelopment efforts. While it is natural that an OSS lens has been used to motivate the open-sourcing\nof AI systems, continuing to do so could come with significant downsides. The rapid increase in\ncapabilities that we have observed, and likely will continue to see, mean that open-sourcing AI\nsystems come with higher risks of misuse, accidents, and dangerous structural effects than traditional\nsoftware [15].\nIn comparative terms, open-sourcing a model will tend to present greater risks than releasing it\nusing a structured access approach whereby model access is mediated, for example, through an API\n[16]. First, once a model is open-sourced, any safeguards put in place by an AI lab to prevent its\nmisuse can be circumvented (see Section 3.1). No methods currently exist to reliably prevent this.\nSecond, once a model is open-sourced, those with sufficient expertise and computing resources can,\nwithout oversight, \"fine-tune\" it to introduce and enhance capabilities that can be misused. These\ntwo possibilities mean that any threshold of safe behavior observed and evaluated under closed or\nrestricted contexts cannot necessarily be assumed to hold once the model is made publicly available.2\n1We use the term open-source without precise requirements on license permissions, but more generally to\nmean making a model publicly and freely available. See section 2 for further discussion on open-source meaning\nand terminology.\n2Since it is difficult to verify the safety of any model and ensure that you have observed the true range of\npossible behaviors, this also holds true for models that are not open-sourced. However, the fact models can be\n5\nFurthermore, open-source AI model release is irreversible; there is no \u201cundo\u201d function if significant\nharms materialize. If a model has a flaw\u2014some exploit that elicits undesirable capabilities\u2014or\ngrave misuse potential, there is nothing to stop users from continuing to use the model once released.\nSimilarly, if developers release patches or updated model versions to remedy flaws, there is no way to\nensure users will implement the patches or operate the most up-to-date version. For malicious users\nwho seek to exploit model vulnerabilities that allow for harmful applications, they are incentivized\nnot to adopt any safety improvements.\nUltimately, as AI labs push the boundaries of foundation model development, the risks of open-\nsourcing will grow as models become increasingly capable. The risks from such capability improve-\nments could become sufficiently severe that the benefits of open-sourcing outweigh the costs. We\ntherefore recommend that decisions to open-source highly capable foundation models should be made\nonly after careful deliberation that considers (i) the range of misuse risks the open-source model may\npresent and (ii) the potential for open-source benefits to be provided through alternative means. We\nexpect that in the future some highly capable foundation models should not be open-sourced.\nWe begin by defining highly capable foundation models (section 2) and the risks presented by open-\nsourcing them (Section 3). The harms are significant and plausibly, in certain cases, justify foundation\nmodel access restrictions. We then turn to three key arguments for open-source model sharing and\nexplore alternative mechanisms for achieving the desired end with significantly less risk (Section 4).\nFinally, we present recommendations for AI developers and policymakers in light of our discussion\n(Section 5).\n2\nWhat Do We Mean by \u201cOpen-Source Highly Capable Foundation Models\u201d?\n2.1\nWhat are Highly Capable Foundation Models?\nFoundation models.\nFoundation models, sometimes referred to as general-purpose AI models, are\nmachine learning models like GPT-4 that demonstrate a base of general capabilities that allow them\nto be adapted to perform a wide range of downstream tasks [17, 18]. These capabilities can include\nnatural language conversation, behavior prediction, image analysis, and media generation3, which\ncan be used to develop or be directly integrated into other AI systems, products, and models.4\nWhen modalities are combined, multimodal foundation models can integrate and respond to numerous\ndata types (e.g., text, audio, images, etc.). For instance, Stable Diffusion [27] and DALL\u00b7E 2 [28]\ncombine natural language processing capabilities with image generation capabilities to translate\nnatural language prompts into image outputs. GPT-4 is also multimodal, though that functionality is\nnot made widely available [29],5 and Meta\u2019s open-source ImageBind project aims to link up numerous\nstreams of data including audio, text, visual data, movement and temperature readings to produce\nimmersive, multi-sensory experiences [31].\nFoundation models can be used positively in healthcare [32], for data analysis [21], customer\nsupport [22], immersive gaming [33], or personalized tutoring [24]. But they can also be misused and\ndeployed by bad actors, for example, to generate child sexual abuse material [34], create fake real-time\nfurther fine-tuned, adapted, and integrated with other systems upon release means that the true range of possible\nbehaviors can shift in unpredictable ways untestable at the pre-release stage.\n3Today, many of the most discussed foundation models are generative AI systems that are variants of large\nlanguage models (LLMs) like GPT-4 (the model which forms the base of the conversational ChatGPT interface).\nLLMs are machine learning models with complex architectures that generate plausible text or visual content in\nresponse to user prompts (that are often text-based). To do so, they are first trained on vast amounts of text, where\nthey learn to predict the next token (or word). Additional training then steers the LLM towards providing outputs\nthat humans rate highly\u2014this makes it more likely that the LLM will provide helpful, non-toxic responses.\n4We are already seeing current-generation foundation models, like GPT-4, being integrated into clinical\ndiagnoses in healthcare [19], visual web accessibility tooling [20], qualitative data analysis [21], video game\ncharacter development [22], customer assistance and support [23], foreign language education [24], financial\nfraud detection [25], legal tools [26], and many other industries. As their capabilities increase, future generations\nof foundation models will continue to be deployed across industry and government, integrating them into many\ndownstream applications across a wide-range of sectors, including safety-critical applications.\n5Multimodal functionality is now available to some Microsoft Enterprise customers via BingChat [30].\n6\ninterviews or recorded histories for influential politicians [35], or to conduct highly-effective targeted\nscams convincing victims that they are calling with trusted friends and family [36, 37]. Other current\nand ongoing harms posed by foundation models include, but are not limited to, bias, discrimination,\nrepresentational harms, hate speech and online abuse, and privacy-invading information hazards [17,\n38\u201340].\nFoundation models have also been associated with upstream harms including poor labor conditions in\nthe supply chain and for those hired to label data [41, 42] as well as putting strain on the environment\nthrough high energy and resource usage during training, deployment, and the production of the\nrequired hardware [43\u201345].\n\u201cHighly capable\u201d foundation models.\nWe define highly capable foundation models as foundation\nmodels that exhibit high performance across a broad domain of cognitive tasks, often performing the\ntasks as well as, or better than, a human.6\nResearchers are working to develop suitable benchmarks to track the increase in such general-\npurpose capabilities by measuring performance of such models holistically (e.g., in regards to\nlanguage, reasoning, and robustness [46] and across a spectrum of specific areas of knowledge, from\nprofessional medicine and jurisprudence to electrical engineering and formal logic [47].\nExtreme risks and harms.\nIn this paper we are particularly concerned with the possibility proba-\nbility that highly capable models may come to exhibit dangerous capabilities causing extreme risks\nand harms such as significant physical harm or disruption to key societal functions.7\nDangerous capabilities that highly capable foundation models could possess include making it easier\nfor non-experts to access known biological weapons or aid in the creation of new ones [50], or\ngiving unprecedented offensive cyberattack capabilities to malicious actors [51, 52]. Being able\nto produce highly persuasive personalized disinformation at scale, effectively produce propaganda\nand influence campaigns, or act deceptively towards humans, could also present extreme risks [53].\nSelf-proliferation abilities, such as evading post-deployment monitoring systems, gaining financial\nand computing resources without user or developer consent, or a model exfiltrating its own trained\nweights, are more speculative but might also facilitate extreme risks [49, 54]. This is particularly the\ncase if models are embedded within critical infrastructure. The magnitude of these risks requires\nthat model developers more carefully and systematically weigh risks against benefits when making\nopen-sourcing decisions for highly capable foundation models than for present-day foundation\nmodels.\nPerhaps in the future we will use AI models to guard against the risks and harms presented by the\nmisuse of, and accidents caused by, other AI models, allowing us to safely deploy AI models with\nincreasingly powerful capabilities. However, such solutions are currently technically under-developed,\nand there are substantial challenges to effectively deploying defensive solutions for AI at a societal\nlevel and at scale [55]. We therefore focus on forthcoming models that may take us into a zone of\nhigh risk against which we do not yet have sufficient social or technological resilience.\nIn section 3 we discuss many risks that foundation models at the frontier of today\u2019s capabilities\ncurrently present. Arguably, these capabilities do not yet surpass a critical threshold of capability for\n6We intentionally speak about \u201chighly-capable models\u201d instead of \u201cfrontier models\u201d. The \u201cfrontier\u201d refers to\nthe cutting-edge of AI development [18], however the frontier of cutting-edge AI moves forward as AI research\nprogresses. This means that some highly capable systems of concern\u2014those capable of exhibiting dangerous\ncapabilities with the potential to cause significant physical and societal-scale harm\u2014will sit behind the frontier\nof AI capability. Even if these models are behind the frontier, we should still exercise caution in deciding to\nrelease such models, all else being equal.\n7Shevlane et al. [48] operationalise such extreme risks and harms in terms of the scale of the impact they\ncould have\u2014e.g., killing tens of thousands of people or causing hundreds of billions of dollars of economic or\nenvironmental damage\u2014or the level of disruption this would cause to society and the political order.\nIn their recently released Responsible Scaling Policy [49], Anthropic distinguishes between four AI Safety Levels\n(ASL\u2019s). Like the Anthropic document, this paper is primarily focused on the likely near future development of\nASL-3 models which are those that show \u201clow level autonomous capabilities\u201d or for which \u201caccess to the model\nwould substantially increase the risk of catastrophic misuse, either by proliferating capabilities, lowering costs,\nor enabling new methods of attack as compared to non-LLM baseline of risk.\u201d\n7\nthe most extreme risks. However, we are seeing some dangerous capabilities emerge, and this trend\nis likely to continue as models become increasingly capable and as it becomes easier and requires\nless expertise and compute resources for users to deploy and fine-tune these models.8 Recently, after\nextensive testing of their large language model, Claude, by biosecurity experts, Anthropic reported\nthat \u201cunmitigated LLMs could accelerate a bad actor\u2019s efforts to misuse biology relative to solely\nhaving internet access, and enable them to accomplish tasks they could not without an LLM.\u201d They\nnote that these effects, while \u201clikely small today\u201d, are on the near-term horizon and could materialize\n\u201cin the next two to three years, rather than five or more\u201d [56].\nOur general recommendation is that it is prudent to assume that the next generation of foundation\nmodels could exhibit a sufficiently high level of general-purpose capability to actualize specific\nextreme risks. Developers and policymakers should therefore implement measures now to guide\nresponsible model research decisions in anticipation of more highly capable models.\nThese recommendations are driven by the fast pace of AI progress, the immense challenge of verifying\nthe safety of AI systems, and our ongoing struggle to effectively prevent harms from even current-day\nsystems on a technical and social level. It is difficult to predict when more extreme risks may arise.\nThe level of risk that a model presents is intimately tied to model capability, and it is hard to know\nwhen a critical line of capability has been or will likely be passed to pose extreme risks. In the\npast, model capabilities often have arisen unexpectedly or have been discovered only after model\ndeployment [57].\nAI models do not need to be general-purpose to pose a risk.\nFinally, it is worth noting that\nhigh-risk AI models do not necessarily need to be general-purpose in nature like foundation models,\nnor must they be at the frontier of current capabilities to pose the risks described above. For example,\nUrbina et al. [58] demonstrated that standard, narrow AI tools used within the pharmaceutical industry\ncan be repurposed to assist with the design of chemical weapons. There are also more pressing\nconcerns that AI systems might soon present extreme biological risks [59]. So while outside the remit\nof this paper, care should similarly be taken in the open-sourcing of narrow AI models that could, for\nexample, be used to aid in chemical or biological weapons development.\n2.2\nOpen-Source AI: Definition and Disanalogy\n\u201cOpen-source\u201d is a term borrowed from open-source software (OSS). In the context of open-source\nsoftware, \u201copen-source\u201d was defined in 1998 as a \u201csocial contract\u201d (and later a certification) describing\nsoftware designed to be publicly accessible\u2014meaning anyone can view, use, modify, and distribute\nthe source-code\u2014and that is released under an open-source license. An open-source license must\nmeet ten core criteria, including free source code access, permission for derived works, and no\ndiscrimination against which fields or groups may use the software [60, 61].\nWith the release of AI models like LLaMA, LLaMA2, Dolly, StableLM the term \u201copen-source\u201d has\nbecome disjointed from open-source license requirements [62]. Some developers use \u201copen-source\u201d\nmerely to mean that their model is available for download, while the license may still disallow certain\nuse cases and distribution. For example, while Meta refers to LLaMA-2 as an open-source model, the\nLLaMA-2 license caveat is that the model cannot be used commercially by downstream developers\nwith over 700 million monthly users, and the outputs cannot be used to train other large language\nmodels. Strictly speaking, LLaMA2 is therefore not open-source according to the traditional OSS\n8According to Anthropic\u2019s Responsible Scaling Policy [49], current cutting-edge foundation model capabilities\nare at AI Safety Level 2 (ASL-2). Anthropic defines ASL-2 models as those \u201cthat do not yet pose a risk of\ncatastrophe, but do exhibit early signs of the necessary capabilities required for catastrophic harms. For example,\nASL-2 models may (in absence of safeguards) (a) provide information related to catastrophic misuse, but not\nin a way that significantly elevates risk compared to existing sources of knowledge such as search engines, or\n(b) provide information about catastrophic misuse cases that cannot be easily found in another way, but is\ninconsistent or unreliable enough to not yet present a significantly elevated risk of actual harm.\u201d Given current\nindications from ASL-2 models, it is prudent to expect that ALS-3 models (see footnote 8) will begin to emerge\nin the near future, and developers and policymakers should prepare accordingly.\n8\ndefinition [63], and the marketing of it as such has been criticized as false and misleading by the\nOpen Source Initiative [63].9\nHowever, in this paper we set licensing considerations aside, as we are concerned with the\nrisks and benefits of public model accessibility.\nFrom an AI risk perspective, even where more\nrestrictive licenses such as RAIL (Responsible AI License) include clauses that restrict certain use\ncases [66], license breaches are difficult to track and enforce when models are feely and publicly\navailable for download [67]. License breach will also not be of great concern for malicious actors\nintending to cause significant harm. Accordingly, and in line with increasing common parlance, we\nuse the term open-source only to refer to models that are publicly accessible at no cost.10\nLicensing aside, the open-source software concept\u2014referring only to \u201cfree and publicly down-\nloadable source code\u201d\u2014does not translate directly to AI due to differences in how AI systems\nare built [62, 68].\nFor AI systems, \u201csource code\u201d can refer to either or both of the inference code\nand the training code which can be shared independently. AI systems also have additional system\ncomponents beyond source code, such as model weights and training data, all of which can be shared\nor kept private independent of the source code and of each other.\nExperts disagree on precisely which model components need to be shared for an AI model to be\nconsidered open-source. Rather, the term is being used to encapsulate a variety of system access\noptions ranging on a spectrum from what Irene Solaiman [69] calls non-gated downloadable to\nfully open models. For fully open models, training and inference code, weights, and all other\nmodel components and available documentation are made public (e.g., GPT-J [70]). For non-gated\ndownloadable models, key model components are publicly available for download while others are\nwithheld. The available components generally include some combination of training code (minimally\nmodel architecture), model weights, and training data.11\nTable 1 presents a useful reference list of standard model components and definitions. See Appendix A\nfor a more detailed breakdown.\nTable 1: Useful definitions of commonly-shared AI model components\nTerm\nDefinition\nModel\narchitecture\nThe code that specifies the structure and design of an AI model, including the types of\nlayers, the connections between them, and any additional components or features that\nneed to be incorporated. It also specifies the types of inputs and outputs to the model,\nhow input data are processed, and how learning happens in the model.\nModel\nweights\nThe variables or numerical values used to specify how the input (e.g., text describing\nan image) is transformed into the output (e.g., the image itself). These are iteratively\nupdated during model training to improve the model\u2019s performance on the tasks for\nwhich it is trained.\nInference\ncode\nThe code that, given the model weights and architecture, implements the trained\nmodel. In other words, it runs the AI model and allows it to perform tasks (like writing,\nclassifying images and playing games).\nTraining\ncode\nThe code that defines the model architecture and implements the algorithms used to\noptimize the model weights during training. The training algorithms iteratively update\nthe model weights to improve the AI model\u2019s performance on the training tasks.\n9Indeed, there are likely economic, strategic, and reputational benefits for a company to \u2018open-source\u2019 a\nmodel in this way [64]. Open-source innovation building on publicly available architectures can easily be\nreincorporated into the model developer\u2019s downstream products. \u201cOpenness\u201d also has a reputationally positive\nconnotation. \u201cOpenwashing\u201d is a term that describes companies who spin an appearance of open-source and\nopen-licensing for marketing purposes, while continuing proprietary practices [65].\n11For gated downloadable models, in contrast, privileged download access is granted only to specific actors.\n9\nThe more model components that are publicly released, the easier it is for other actors to\nreproduce, modify, and use the model.\nFor example, access to model architecture and trained\nweights (e.g., StabilityAI\u2019s Stable Diffusion [71]), when combined with inference code, is sufficient\nfor anyone to use a pre-trained model to perform tasks. Inference code can be easily written by\ndownstream developers or even generated by large language models such as ChatGPT. It also does\nnot need to match the original inference code used by the model developer to run the model. Access\nto model weights also allows downstream developers to fine-tune and optimize model performance\nfor specific tasks and applications.\nReleasing other useful parts of the training code makes it much easier for other actors to reproduce\nand use the trained model. For instance, providing the optimal hyperparameters would make a\npre-trained OS AI model more capable (and possibly dangerous), and releasing the code used to\nclean, label and load the training data would reduce the burden on actors trying to reproduce model\nweights.\nSometimes, an AI developer will release the training and inference code for a model, but not the\ntrained model weights (e.g., Meta\u2019s LLaMA [72] before the weights were leaked).12 In such cases,\nactors with sufficient computing resources and data access could train the model and, with some\ninference code, run it.13 However, at the moment, few actors (realistically, only large technology\ncompanies, state-level actors, or well-funded start-ups) have the computing resources available to\ntrain highly capable foundation models that represent the frontier of model performance.14\nTherefore, in this paper, when we refer to open-source foundation models, we mean models for\nwhich at least model architecture and trained weights are publicly available unless otherwise\nspecified.\nBox 1 describes the need for further work defining open-source gradients beyond the definition we\ngive here; releasing different (combinations of) model components in addition to trained weights and\ntraining code enables different downstream activities.\n3\nRisks of Open-Sourcing Foundation Models\nDue to their vast application space and pace of development, foundation models have potential for\nbroad and significant benefit and harm. Accordingly, open-sourcing these models poses some\nsubstantial risks which we present in two categories: malicious use (3.1) and proliferation of\nunresolved flaws (3.2).\nThese harms are intensified by the fact that once a decision has been made to open-source, there is\nno \u201cundo\u201d function. A published model cannot be rolled back if major safety issues emerge or if\nmalicious actors find an AI tool to be particularly useful for scamming, hacking, deceptive influence,\nor acts of terror. Methods exist that allow even partially open-sourced models (e.g., code with some\nor no other model components) to be replicated and shared in full [79].\n12Furthermore, we should expect model weight leaks to be frequent. Weights are contained in relatively small\nfiles (usually less than 256 GB) that can be easily and untraceably shared. Meta, for instance, chose to restrict\naccess to the weights of its large language model LLaMa to researchers on a case-by-case basis, but a week\nlater the weights were leaked and are now available publicly on the internet [31]. If weights for a trainable\nopen-source model are leaked, the public functionally has access to a pre-trained open-source model.\n13Note that if the model weights were not made publicly available, external actors who trained a trainable OS\nmodel may discover a set of model weights distinct from those discovered by the original developer who released\nthe model. Using a different set of weights, however, does not preclude a model from performing equally well as\n(or perhaps even better than) a model using the original weights.\n14Training frontier foundation models costs $10\u2013100 million in compute costs and is projected to increase\nto $1\u201310 billion in coming years [73]. However, the cost to train a model that matches the performance of a\nprevious state-of-the-art system has fallen rapidly. For instance, training GPT-3, the most powerful foundation\nmodel available in June 2020, was estimated to cost at least $4.6 million [74], but by September 2022 an\nequivalently powerful model was theoretically available for $450,000 [75]. This is due to both advances in AI\nchip technology and the discovery of more efficient AI algorithms [76\u201378].\n10\nBox 1: Further research is needed to define open-source gradients\nGradient of System Access\nThe idea that models are either released open-source or maintained closed-source presents\na false dichotomy; there are a variety of model release options ranging from fully closed to\nfully open model [68, 80, 81].\n\u201cConsiderations and Systems Along the Gradient of System Access\u201d\n[figure reproduced from Solaiman [69]]\nWhat is generally referred to as \u201copen-source\u201d model release spans the two system access\ncategories on the far right of Irene Solaiman\u2019s [69] gradient: Downloadable (specifically\nnon-gated downloadable\u2014meaning that anyone is free to download the available components)\nand Fully Open.\nGradient of Open-Source Access\nFor fully-open models, source code, weights, training data, and all other model components\nand available documentation are made public. However, in the non-gated downloadable\ncategory\u2014in which some components are publicly downloadable (usually including weights\nand architecture) while others are withheld\u2014there is room for further specification. Im-\nportantly, the precise benefits and risks of open-sourcing are determined by the specific\ncombinations of model components and documentation that are made publicly available.\nPrecise Definitions for Precise Standards\nNear-term investment in a project is needed to investigate and articulate what activities are\nmade possible by access to different (combinations of) model components. This information\nwill be key to constructing effective and fine-grained model release standards that are not\noverly burdensome, and to ensure open-source values are protected and benefits enjoyed\nwhere safe.\nWe make a start in Appendix A, though it is a much larger and more involved project than we\ncan do justice here, and it is a project on which members of open-source communities should\nbe centrally involved. The Open Source Initiative recently launched one such initiative to\ndefine what machine learning systems will be characterized as open-source [82].\n11\n3.1\nMalicious Use\nOpen-source publication increases foundation models\u2019 vulnerability to misuse. Given access to the\nmodel\u2019s weights and architecture, any actor with the requisite technical background15 can write their\nown inference code\u2014or modify available inference code\u2014to run the model without safety filters.\nThey can also fine-tune the model to enhance the model\u2019s dangerous capabilities or introduce new\nones.\nThere are several ways in which open-source publication can facilitate misuse:\nFirstly, open-sourcing a model allows actors to run the model using new or modified inference code\nthat lacks any content safety filters included in the original code. Stable Diffusion\u2019s safety filter, for\nexample, can be removed by deleting a single line of inference code.16 This is possible because such\nfilters are implemented post-hoc, appending additional processes to the model\u2019s inference code, rather\nthan fundamentally changing the behavior of the model itself. With content safety filters removed,\nthere is nothing to prevent users from presenting the models with unsafe requests or to prevent the\nmodel from yielding unsafe outputs.\nSecondly, the ability to fine-tune an open-source model without restrictions enables the modification\nof models specifically for malicious purposes. Fine-tuning that occurs through an API can be\nmonitored; for example, the API owner can inspect the contents of the fine-tuning data set. Without\nsuch monitoring, fine-tuning could involve the reintroduction of potentially dangerous capabilities\nthat were initially removed by developers pre-release through their own fine-tuning. Fine-tuning\ncan also lead models to become even more dangerous than they were before safety measures were\napplied. However, increasing a model\u2019s dangerous capabilities by fine-tuning would be more difficult\nthan removing certain kinds of post-hoc safeguards like filters; fine-tuning requires the curation of\na dataset to promote those dangerous capabilities, as well as requiring the necessary compute and\ntechnical expertise to successfully fine-tune the model.\nThirdly, access to model weights can aid adversarial actors in effectively jailbreaking system safe-\nguards (including for copies of the system that have not been modified). Traditional jailbreaks use\nclever prompt engineering to override safety controls in order to elicit dangerous behavior from a\nmodel (e.g., getting a large language model (LLMs) to provide instructions for building a bomb by\nasking it to write a movie script in which one character describes how to build a bomb). Creative\nprompting only requires model query access. However, researchers recently discovered a method of\nadversarial attack in which the network weights of open-source LLMs aided researchers in optimizing\nthe automatic and unlimited production of \u201cadversarial suffixes\u201d, sequences of characters that, when\nappended to a query, will reliably cause the model to obey commands even if it produces harmful\ncontent [84]. Notably, this method, which was developed using open-source models Vicuna-7B\nand Meta\u2019s LLaMA-2, is transferable; it also works against other LLMs such as GPT-4 (OpenAI),\nBard (Google), and Claude (Anthropic), indicating that open-sourcing one model can expose the\nvulnerabilities of others.\nThe above methods have the potential of reducing, if not entirely nullifying, the measures taken\nby developers to limit the misuse potential of their models. These measures would be much more\ndifficult to bypass in cases where the model weights and training code are not openly released, and\nwhere user interaction with the model is facilitated through an API. Fine-tuning, in particular, can\nalso lead models to be more dangerous than they might have been originally.\n15Knowledge equivalent to that from a graduate-level machine learning course would be sufficient to perform\nfine-tuning, but additional experience in training models would likely be useful in addressing the myriad of\nissues that sometimes come up, like divergence and memory issues. Depending on the malicious use case, it\nmay be more or less difficult to source the required data set.\n16This observation comes from personal correspondence with several technical researchers. We do not provide\nfurther details on specific technical flaws since we believe it would be irresponsible to do so. Please see Rando\net al. [83] on red-teaming the Stable Diffusion safety filter for related information.\n12\n3.1.1\nVarieties of Malicious Use17\nPotential epistemic, social and political consequences of foundation model misuse include the\nfollowing [85, 86].\n\u2022 Influence operations. There is a wealth of existing research theorizing AI\u2019s utility in automating, or\notherwise scaling, political or ideological influence campaigns through the production and targeted\ndissemination of false or misleading information [17, 86\u201388]. There is concern about multimodal\nfoundation models being used to create interactive deepfakes of politicians or constructing and\ncatering detailed and seemingly verifiable false histories [35]. A recent experiment demonstrated\nthe potential for AI-based influence operations when the LLM-based system, CounterCloud, was\ndeployed to autonomously identify political articles, to generate and publish counter-narratives,\nand then to direct internet traffic by writing tweets and building fake journalist profiles to create a\nveneer of authenticity [89].\nConcerns about AI being used to manipulate public views, undermine trust, drive polarization, or\notherwise shape community epistemics have led some scholars to speculate that \u2018whoever controls\nlanguage models controls politics\u2019 [90].\n\u2022 Surveillance and population control. AI advances the means of states to monitor and control their\npopulations through immersive data collection, such as facial and voice recognition [91], the nascent\npractice of affect recognition [92], and predictive policing [93]. AI also allows automating and\nthus ever more cheaply analyzing unprecedented amounts of data [48]. Authoritarian governments\nmay be most likely to make use of AI to monitor and control their populations or to suppress\nsubpopulations [94, 95], but and? other types of governments are employing AI enabled surveillance\ncapabilities as well. Nascent AI surveillance technologies are spreading globally and in countries\nwith political systems ranging from closed autocracies to advanced democracies [96, 97].\n\u2022 Scamming and spear phishing. Malicious actors can use AI to fraudulently pose as a trusted\nindividual for the purpose of theft or extraction of sensitive information [98]. For example, large\nlanguage models have been shown to be proficient in generating convincing spear phishing emails,\ntargeted at specific individuals, at negligible cost [99].\nEvidence from online forums also indicates that malicious AI tools and the use of \u201cjailbreaks\u201d to\nproduce sensitive information and harmful content are proliferating amongst cyber criminals [100].\nHigh profile scams using generative AI have also been observed, with one report detailing how\n$35million was stolen from a Japanese firm by scammers who used AI voice cloning tools to pose\nas a company executive to employees [37].\n\u2022 Cyber attacks. Foundation models have applications for both cybersecurity and cyber warfare\n[52, 101]. Early demonstrations show that LLMs\u2019 current coding abilities can already find direct\napplication in the development of malware and the design of cyber attacks [102]. With improved\naccessibility and system capability, the pace of customized malware production may increase as\ncould the variability of the malware generated. This poses a threat to the production of viable\ndefense mechanisms. Especially in the near term, there is some evidence that AI generated malware\ncan evade current detection systems designed for less variable, human-written programs [103\u2013105].\nUltimately, information gained from cyberattacks might be used to steal identities, or to gather\npersonal information used to mount more sophisticated and targeted influence operations and\nspear phishing attacks. Cyberattacks could also be used to target government agencies or critical\ninfrastructure such as electrical grids [106], financial infrastructures, and weapons controls.\n\u2022 Biological and chemical weapons development. Finally, current foundation models have shown\nnascent capabilities in aiding and automating scientific research, especially when augmented with\nexternal specialized tools and databases [107, 108]. Foundation models may therefore reduce the\nhuman expertise required to carry-out dual-use scientific research, such as gain-of-function research\nin virology, or the synthesis of dangerous chemical compounds or biological pathogens [50, 109].\nFor example, pre-release model evaluation of GPT-4 showed that the model could re-engineer\n17To be clear, open-sourcing is not to blame for the malicious use of AI. Foundation models are a dual use\ntechnology, and where the technology is built by malicious actors or where effective safety restrictions are\nnot in-place for models accessible via API, misuse can occur. Open-sourcing risks the diffusion of potentially\ndangerous capabilities to malicious actors and lowers barriers against misuse.\n13\nknown harmful biochemical compounds [110], and red-teaming on Anthropic\u2019s Claude 2 identified\nsignificant potential for biosecurity risks [56, 111].\nSpecialized AI tools used within these domains can also be easily modified for the purpose of\ndesigning potent novel toxins [58]. Integrating narrow tools with a foundation model could increase\nrisk further: During pre-deployment evaluation of GPT-4, a red-teamer was able to use the language\nmodel to generate the chemical formula for a novel, unpatented molecule and order it to the red-\nteamer\u2019s house [110]. Law-makers in the United States are beginning to take this biosecurity threat\nseriously, with bipartisan legislation\u2014the Artificial Intelligence and Biosecurity Risk Assessment\nAct\u2014being proposed that would monitor and study the potential threats of generative and open-\nsource AI models being used \u201cintentionally or unintentionally to develop novel pathogens, viruses,\nbioweapons, or chemical weapons\u201d [112].\n3.1.2\nEase of Malicious Use\nOne factor that potentially mitigates the misuse of open-source foundation models is that the pool of\nactors with the requisite talent and compute resources to download, run and, when necessary, modify\nhighly capable models effectively is relatively small. Nevertheless, there are still several reasons to\nbe concerned.\nFirst, there is an increasing number of individuals who have the skills to train, use, and fine-tune AI\nmodels as illustrated by growing computer science PhD enrollment as well as ballooning attendance\nat AI conferences [113]. This is supplemented by an increasing number of tutorials and guides\navailable online to use and fine-tune AI systems.\nSecond, running a pre-trained AI model at a small scale requires only a small amount of compute\u2014\nfar less compute than training does. We estimate the largest Llama 2 model (Llama-2-70B) costs\nbetween $1.7 million and $3.4 million to train,18 while the inference costs for Llama-2-70B are\nestimated to be between 0.2 and 6 cents per 750-word prompt [116] and $4 per hour of GPU time.19\nWhile the compute requirement becomes large when running models at a very large scale (that is,\nperforming many inferences),20 large-scale runs may not be required for impactful misuses of a\nmodel. It is conceivable that only a few inferences may be needed in certain domains for models\nto be dangerous (e.g., a malicious actor may only need to find one critical vulnerability to disrupt\ncritical infrastructure).\nThird, while the overall cost of training frontier models is increasing [73],21 algorithmic progress\nfocuses heavily on reducing demands on compute resource, both for training22 and for fine-tuning\n[118]. This, combined with the decreasing cost of compute (measured in FLOP/s per $)[119], means\nthat while initial model development and training may remain prohibitively expensive for many\nactors, we should not expect compute accessibility to always act as a strong limiting factor for\n18Meta reported using 1,720,320 A100 GPU-hours to train Llama-2-70B [114]. A single consumer A100\nGPU can be rented privately for $1.99/hour (e.g. from RunPod [115]. Our range assumes that Meta\u2019s cost was\nbetween $1 and $2 per hour.\n19Since the Llama-2-70B model is about 129GB, it requires 2 80GB A100 GPUs to store, each of which can\nbe rented for about $2/hour (e.g. from RunPod [115]).\n20Both training and inference processes are typically more economical when run on centralized high-\nperformance computing (HPC) systems optimized for AI workloads housed within data centers. While a\nsingle training run demands more compute than a single inference, the majority of compute for AI systems is not\nbeing used for training runs. As with most infrastructure, the operating costs will eventually be larger than the\nupfront cost. As the final product of AI systems, inferences are triggered by a multitude of daily actions, ranging\nfrom chatbot interactions and Google searches to commands to virtual personal assistants like Siri or Alexa.\nConsider image generation: the cumulative compute used for generating images via a generative AI model has\nnow likely surpassed the initial training compute for the most popular generative systems by orders of magnitude.\nThe key difference between development and deployment lies in timeframe and independence. In inference, the\ncomputational resources can be distributed across multiple copies of the trained model across multiple compute\ninfrastructures over a longer time duration. Whereas, in training, the computational resources are required over a\nsmaller time frame within one closed system, usually one compute cluster.\n21See Footnote 9.\n22For example, Meta\u2019s recently released I-JEPA (Image Joint Embedding Predictive Architecture) offers a\nnon-generative approach for self-supervised learning that does not rely on hand-crafted data-augmentations, and\nrequires significantly fewer GPU hours to train for a better performing model [8, 117].\n14\nfine-tuning existing open-source foundation models. Targeted fine-tuning of a pre-trained model to\ncreate dangerous models would remain much less expensive than building a model from scratch.\n3.1.3\nOffense-Defense Balance\nAnother argument against the threat of malicious use posed by open-sourcing is that while open-\nsourcing may increase model vulnerability to exploitation by malicious actors, it does more to help\ndevelopers identify those vulnerabilities before malicious actors do and to support development of\ntools to guard against model exploitation and harms [120]. In other words, in the offense-defense\nbalance\u2014a term referring to the \u201crelative ease of carrying out and defending against attacks\u201d [121,\n122]\u2014it has been argued that open-sourcing favors defense.\nThis is often true in the context of software development; open-sourcing software and disclosing\nsoftware vulnerabilities often facilitate defensive activities more than they empower malicious actors\nto offensively identify and exploit system vulnerabilities. However, the same might not be safely\nassumed for open-source AI, especially for larger and more highly capable models [55]. Shevlane\nand Dafoe [55] explain that when a given publication (e.g., publication of software, AI models, or of\nresearch in biology or nuclear physic etc.) is potentially helpful for both people seeking to misuse a\ntechnology and those seeking to prevent misuse, whether offensive or defensive activities are favored\ndepends on several factors:\n\u2022 Counterfactual possession. How likely would a would-be attacker or defender be able to acquire\nthe relevant knowledge without publication? If counterfactual possession by the attacker or defender\nis probable, then the impact of publication on their respective offensive and defensive activities is\nless.\n\u2022 Absorption and application capacity. A publication only benefits attackers and defenders to\nthe extent that they can absorb and apply the knowledge toward their desired ends. This depends\non how much knowledge is disclosed, how the knowledge is presented, and the attentiveness and\ncomprehension of the recipients.\n\u2022 Resources for solution finding. For defenders, given publication, how many additional actors will\nhelp develop defenses? Impact of publication is greater if many people are likely to contribute to\ndefensive applications.\n\u2022 Availability of effective solutions. Are vulnerability patches easy to implement, or will developing\nsolutions be a more complicated and time intensive endeavor? The positive effects of publication\ndecrease the more difficult vulnerabilities are to address.\n\u2022 Difficulty/cost of propagating solutions. Even where defensive solutions exist, if they are difficult\nto propagate then the impact is less.\nFor software development, the offense-defense balance of open-source publication often comes out in\nfavor of defense. Software vulnerabilities are easy to find, so counterfactual possession by attackers\nis likely, and software patches are relatively easy to make, usually fully resolve the vulnerability, and\nare easily rolled out through automatic updates.\nHowever, in the context of AI research, Shevlane and Dafoe offer the tentative conclusion that as\nAI models grow in capability and complexity, open-source publication will likely skew the balance\ntowards offense. As discussed at the start of this section, attacker knowledge of vulnerabilities and\ntheir ability to exploit those vulnerabilities is greatly increased by open-source publication. For some\nvulnerabilities, researching solutions is time consuming and resource intensive (See Section 4.2).\nSolutions developed also tend not to be perfect fixes. This is for a variety of reasons: (i) given our\ncurrent lack of understanding of how advanced AI systems work internally, it may be difficult to\nidentify the source of risk or failure; (ii) certain risks, such as bias and discrimination, may be learned\nfrom the training data, and it could be impossible to \u201cremove\u201d all bias from training data [123]; (iii)\nreducing misuse of AI systems may require changes to social systems beyond changes to technical\nones [55]; (iv) the structure of AI systems introduces new sources of failure specific to AI that are\nresistant to quick fixes (e.g., the stochastic nature of large language models may make it difficult\nto eliminate all negative outputs, and the inability to distinguish prompt injections from \u201cregular\u201d\ninputs may make it difficult to defend against such attacks) [124]. Finally, it is difficult to ensure\n15\nimprovements to open-source models are implemented by downstream users and developers which\ncan result in widespread proliferation of unresolved model flaws. We address this topic in Section 3.2.\nThe conclusion that the offense-defense balance skews towards offense when open-sourcing AI\nremains tentative because the offense-defense balance is influenced by a myriad of factors making\nit difficult to reliably predict outcomes. The balance will vary with each model, application space,\nand combination of released model components. In addition, we may develop measures in the future\nthat build our defensive capabilities. Nonetheless, the general notion holds; open-sourcing AI leans\ntowards offense more so than open-sourcing software. AI developers should therefore think critically\nabout the potential for, and potential protections against, misuse before every model release decision.\n3.2\nRisks from the Proliferation of Unresolved Model Flaws\nExcitement about foundation models stems from the large number of potential downstream capability\nmodifications and applications. These can include applications involving malicious intent and misuse,\nbut more frequently will involve well-intentioned commercial, scientific, and personal applications\nof foundation models. If they have the necessary resources and model access (via open-source or\nsufficient API access), downstream individuals, AI labs, and other industry and government actors\ncan:\n1. Employ foundation models to new tasks that were not previously subject to risk assessments due\nto the general capabilities of these models.\n2. Fine-tune or otherwise alter open-sourced foundation models to enable specialized or additional\n(narrow and general) capabilities.\n3. Combine foundation models with other AI models, tools, and services, such as the internet or\nother APIs, to create a system of AI models which can have new narrow and general capabilities.23\nFor example, AutoGPT is an open-source app that integrates with GPT-3.5 and GPT-4. While\nGPT-3.5 and GPT-4 can respond one prompt at a time, AutoGPT handles follow-ups to an initial\nprompt. This allows users to ask AutoGPT autonomously to complete higher-level goals that\nrequire iteratively responding to and generating new prompts [125, 126].\nIn all three cases, the risks, flaws, system vulnerabilities, and unresolved safety issues of\nthe initial foundation model propagate downstream.\nFor instance, biased and discriminatory\nbehavior, vulnerabilities to prompt injection [127] and adversarial attacks [84], autonomous self-\nproliferation abilities [54], or other dangerous capabilities could quickly proliferate if not caught and\nfixed before being integrated into downstream products and applications.\nThe fact that the models can be applied to new contexts (1), but also adapted (2 and 3) to unlock new\nnarrow and general capabilities, also means that further, difficult to predict risks and harms could\nemerge. Consequently, it is not certain that the safeguards put in place by the foundation model\ndeveloper will continue to be effective if downstream developers fine-tune, alter, and combine AI\nmodels. This means that not only will existing model flaws proliferate, but previously fixed flaws and\nnew flaws may also arise.\nIf (1), (2), and (3) are enabled via structured API access (e.g., OpenAI\u2019s davinci-002 and GPT-3.5\ncan be fine-tuned via API [128], then developer monitoring of API use may go some way towards\nmitigating the proliferation harms described above. There is no such recourse, however, if a model is\nmade open-source. Once a model is open-sourced, there are no take-backs if harms ensue.\nWhen risks and vulnerabilities are proliferated there is no way of ensuring that when a fix is rolled out\n(assuming a fix is possible - see end of 3.1) that it is adopted or integrated effectively by downstream\nAI developers and users. Even in the context of traditional open-source software, software flaws\nare proliferated [129] as downstream developers and users more often than not fail to implement\n23For example, ChemCrow is a large language model that integrates 17 expert-designed computational\nchemistry tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The\ndevelopers note that ChemCrow aids expert chemists and lowers barriers for non-experts which can foster\nscientific advancement but could also pose significant risk of misuse [108]. Also see Boiko, MacKnight, &\nGomes [107] on combining large language models.\n16\npatches and version updates, even where the open-source license requires they do so [130]. Very often\nconsumers are unaware that their systems are running on out-of-date software or that vulnerability\npatches are available. Other times an updated software version will not integrate well with other\nsoftware packages and existing infrastructure. We should expect the same challenges to undermine\nthe maintenance of open-source foundation models, though a given foundation model will likely be\napplied to a much wider range of applications than a piece of software.\nThere are also different incentives influencing decisions to implement updates for traditional software\nthan for foundation models. For traditional software, patches and version updates improve system\nperformance and functionality and resolve vulnerabilities that could cause harm to the user. It is to the\nuser\u2019s benefit to implement software updates when feasible. In comparison, for increasingly capable\nfoundation models, safety patches and updates often aim to reduce system functionality, disallowing\ncertain activities that were possible with previous versions. If downstream developers and users wish\nto retain those functionalities (e.g., to be able to produce nude art with an image generator), they are\nincentivized not to update versions and, in some cases, not to disclose the existence of potential risks\nand system vulnerabilities.\nDue to the potential of proliferating risks and model flaws from highly capable foundation mod-\nels, developers need to consider model release decisions carefully. Developers of highly capable\nfoundation models must be cognizant of the potential downstream harms of their models (harms\nwhich they would be powerless to backtrack) and carefully consider alternative methods by which\nopen-source benefits might be pursued but at significantly less risk [131]. We discuss alternatives\nfurther in Section 4. Clear legislation is also needed to hold developers and controllers of AI systems\nliable for the impacts of their systems.\n4\nBenefits of Open-Sourcing Foundation Models and Alternative Methods for\nAchieving Them\nIn this section we analyze three key benefits of open-source software: facilitating external evaluation\n(4.1), accelerating beneficial progress (4.2), and distributing control over technological development\nand benefits (4.3). For each, we first present the benefit, then evaluate the benefit in the context of\nhighly capable foundation models, and finally consider other strategies that might contribute to the\nsame goals. A summary table is provided at the start of each subsection.\n4.1\nExternal Model Evaluation\nTable 2: Section summary: Open-sourcing as a mechanism for enabling external model evaluation\nThe argument\nfor open-source\nAI\nOpen-sourcing enables independent model evaluations of projects by wider com-\nmunities of developers. Tapping into the wider AI community helps to catch bugs,\nbiases, and safety issues that may otherwise go unnoticed, ultimately leading to\nbetter performing and safer AI products.\nEvaluation of\nbenefit\n\u2022 Open-sourcing is most useful for evaluating complex safety issues and less\nuseful for identifying discrete bugs. There may also be suitable alternatives to\nopen-sourcing that achieve these same benefits with fewer risks.\nAlternative\nmethods\n\u2022 Grant privileged model access to trusted (independently selected) third-party\nauditors via gated-download or research API.\n\u2022 Establish a community of (independently selected) red-team professionals to\nstress-test models pre-release.\n\u2022 Explore social impacts and safety issues through incremental, staged release of\nmodels.\n\u2022 Employ safety bounties to incentivize wide public involvement in reporting new\nbehaviors and safety issues.\n17\n4.1.1\nThe Argument for Open-Source\nA clear benefit for open-source software development is that open-sourcing facilitates independent\nevaluations of projects by wider communities of developers and many more people than a single\ndeveloper would be able to employ internally to check for bugs and safety issues. This means a\nmore diverse pool of expertise can be tapped, with a low barrier to entry for individuals to contribute,\nwhose skill to identify and solve problems is enhanced by increased access to relevant materials.\nSo in the case of highly capable foundation models, it is reasonable to expect that open-sourcing\nwould leverage the same talent multiplier as with OSS. Tapping into the wider AI community would\nenable audit and analysis of foundation models and any model components (e.g., training data,\nweights, documentation) by interested parties helping to catch bugs, biases, and safety issues that\nmay otherwise go unnoticed. Such external oversight would help hold AI developers to account for\nthe quality and consequences of their products at a team and an industry level, and ultimately lead to\nbetter performing and safer AI products.\n4.1.2\nEvaluating the Benefit for Foundation Models\nIn this section we consider the benefits of open-sourcing for enabling external model evaluations\naccording to two classes of model issues: (1) discrete bugs, and (2) complex safety challenges.\nDiscrete Bugs.\nDiscrete bugs such as interface glitches, data exposures and authentication issues\nare self-contained flaws that are relatively simple to fix. Once discovered, discrete bugs can be easy\nand relatively low cost for model developers to fix in-house. But bug spotting certainly benefits\nfrom additional eyes, and there are alternative methods to open-sourcing that attempt to facilitate\nmore widespread participation in model review. For example, AI developers can set up community\nreporting systems as they are encountered and even incentivize engagement via bug bounties like\nthat employed by OpenAI [132]. That being said, conscious steps need to be taken to ensure that the\nbenefits of open-sourcing can be replicated: active efforts need to be made to engage the attention\nof a diverse set of experts and it remains difficult to mimic open-sourcing here in all respects. For\nexample, not having full access to materials will impede individuals in their ability to find bugs.\nA further advantage of open-sourcing is that it allows downstream developers to patch such issues on\ntheir own and to pass those patches back to the developer for integration into future model versions.\nComplex Safety Challenges.\nIncreasingly capable foundation models are bringing with them an\narray of new behaviors and safety challenges that arise unpredictably and are not well-understood\nby developers [133]. For example, emergent abilities are unexpected and unintended features or\nbehaviors that arise in AI models as they become more advanced. These abilities are not observed in\nsmaller precursors and are not explicitly programmed by developers [57]. \u201cCapability overhang\u201d is\na concept that further describes how these emergent abilities can be latent within a system only to\nemerge unexpectedly when elicited, for example, by clever prompt engineering or integration with\nother software. Sometimes new capabilities continue to be elicited many months after model release\n[80].\nDrawing input from a large pool of contributors will be instrumental to exploring this evolving space\nof unknown unknowns; what do new safety issues look like and, if not immediately evident, how\nare they triggered? Furthermore, because some model behaviors will only emerge with downstream\nmodification of model weights, model evaluators will need to be able to experiment with model\nfine-tuning to test a variety of possible model versions.\nOpen-sourcing provides the necessary access to model weights and parameters for attempting to\nelicit new behaviors from models for safety evaluation (although it simultaneously allows malicious\nattempts to elicit new dangerous behaviors and avenues of misuse). For models that are not open-\nsourced, fine-tuning might also be facilitated via APIs that allow users to manipulate model weights\nand parameters (e.g. OpenAI\u2019s davinci-002 and GPT-3.5 [128]. However, some APIs may introduce\nadditional limitations on fine-tuning. For example, API controllers could attempt to limit the format or\ncontent of data used to fine-tune a model, limit access to weights and parameters (e.g. provide access\nto weights and parameters of base-line models but not to fine-tuned model versions), or limit the\n18\namount of fine-tuning that can be done. These limitations might be in place to protect the developer\u2019s\ncommercial interests or to reduce risk of misuse.\nSafety research, such as alignment and interpretability research which aim to understand and resolve\ncomplex safety issues, also require varying degrees of model access. We will discuss the benefits of\nopen-sourcing for promoting safety research in section 4.2.\n4.1.3\nOther Ways to Enable External Evaluation\nThere are some alternatives to open-sourcing that can facilitate the identification and evaluation of\nbugs and safety issues, with less risk than open model release.\nStaged-Release Impact Testing.\nAI developers can conduct staged-release impact testing to gather\nobservational data about how a model is likely to be (mis)used and modified if open-sourced. Staged-\nreleased impact testing is a process by which incrementally larger versions of a model are released\nbehind API [134, 135]. Each stage of release allows time to observe how the model is used, to\nstudy its social impacts, and to implement any patches or new safety measures before the next, more\npowerful version is released (if it is deemed safe to do so).\nIf many safety measures need to be implemented between stages to mitigate harms, this is a solid\nindication that open-sourcing will lead to malicious use because, once open-sourced, those measures\ncould be easily circumvented.\nConducting staged release impact testing allows AI developers to be more comfortable with open-\nsourcing their models, assuming no other significant issues emerge in model evaluation and risk\nassessment process. However, this can come at a cost to the developer by allowing competitors to\ncapture market share in the meantime if such processes are not implemented for the industry at large\nthrough regulation. In addition, any benefits from the model are also delayed from reaching the\nrelevant communities that could benefit from them.\nExternal Audits & Red-teaming.\nIn addition to staged-release impact testing, developers can\ngrant privileged model access to trusted third-party auditors. These are external actors (government\ndepartments, private expert organizations, or some combination thereof) tasked with evaluating the\nsafety and security of foundation models prior to model release or assessing and verifying the model\nevaluation measures employed by AI labs.\nThough they are in early stages of development, external auditing has been proposed as a key\ninstitutional mechanism for facilitating trustworthy AI development [136\u2013139]. One early example is\nthe Alignment Research Center\u2019s (ARC) pre-release evaluation of GPT-4 for dangerous capabilities\n[140].\nThe ARC evaluations largely involved red-teaming GPT-4. Red-teaming is an evaluation method\nthat stress-tests models to discover how and where safety concerns arise. The aim is to identify\npotentially dangerous model properties (e.g., manipulative or power-seeking behavior), security flaws\n(e.g., jailbreaks), and possible misuse applications. Stress-testing requires that red-teams are able\nto prompt models to elicit new and dangerous behavior which can be facilitated with model query\naccess\u2014that is, being able prompt models and receive outputs without open-source access to model\ncode and weights.\nWhere model weight access is needed to experiment with fine-turning, access might be granted to\nidentified individuals or research groups via gated download or API. For gated download developers\nmake models (minimally weights and training code) available for specific actors to download and\nrun on their own hardware.24 The risk with gated download is that model leaks could result in\nthe dissemination of potentially dangerous models. Download recipients would need to be vetted\ncarefully. Another option is for developers to provide fine-tuning access via API. However, as\nmentioned above, some developers may choose to implement limitations on fine-tuning in order to\nprevent misuse or model reproduction. For this reason, Bucknall et al. [141] recommend the design\n24For further discussion on gradients of model release, including gated and non-gated downloadable models,\nsee Box 1 and [69]\n19\nand implementation of \u2018research APIs\u2019 whereby more flexible fine-tuning permissions are granted to\ntrusted researchers, red-teams, and auditors depending on their access needs.\nRed-teams such as those employed by OpenAI [29, 142, 143] and Anthropic [56, 144] are increasingly\ncommon, though best practices are still being developed. Model evaluation is a nascent field. This\nmakes it difficult to evaluate the skill of potential auditors. Moving forward, standards will need\nto be developed and implemented to ensure the quality and consistency of third-party audits [145]\nas numerous governments and private actors move to occupy a growing AI assurance sector [146].\nMechanisms will also be needed to ensure developers provide sufficient model access to auditors and\nrespond to audit findings. For instance, audit reports should be published publicly or shared with a\ngovernment overseer while regulatory requirements ensure labs respond and disclose their efforts\nand results. Governments should consider establishing mandatory auditing regimes for large and\npotentially dangerous foundation models to minimize the risk of model developers only granting\naccess to favored auditors, who might be less likely to expose failure modes that are potentially\nembarrassing or inconvenient for the developer.\nMuch work is needed developing new model evaluation techniques and establishing best practice.\nSome evaluation processes may benefit from leveraging foundation model capabilities [147] as well\nas input from wider AI developer communities. Decisions about how and by whom models are\naudited are currently entirely at the discretion of individual developers. Without standardized risk\nassessment procedures a lab could choose an \u201ceasy\u201d or \u201cfriendly\u201d auditor.\nAnother possibility Brundage et al. [136] suggests, is to extend red-teams to elicit input from a wider\ncommunity of \u2018red-team professionals\u2019. Such a community would be composed of members from\nthe wider AI community as well as security professionals, and representatives from high-risk domains\nto which foundation models might be put to use. This would help distribute red-teaming costs for\nlabs less-inclined to form internal red-teams, and the community of red-team professionals would\nbenefit from greater insight to common attack vectors and useful red-teaming strategies shared within\nthe community. But again, risks arise by allowing AI developers to choose red-teamers on their own,\nincluding capture of the safety evaluation process and a potential narrowing of focus and values\nby not ensuring an optimally diverse and comprehensive set of experts. Further best practices and\nregulatory mechanisms need to be put in place to make sure red-teaming can provide effective safety\nevaluations of AI models.\nBug Bounties and Safety Bounties.\nSafety bounty programs have been proposed as another method\nof tapping into a wider global community to help identify and surface new safety and alignment issues\nin large foundation models [148]. Bounty \u201chunters\u201d are not pre-vetted as with selected red-teams.\nAnalogous to bug bounty programs commonly used in cybersecurity, safety bounty programs would\noffer financial and reputational rewards to members of the public who discover and responsibly report\nnew safety failures, such as novel jailbreaks, or capabilities beyond those found in internal tests. As\nwith red-teaming, bounty \u201chunters\u201d can do this by interacting with systems behind an API. However,\nit is as yet unclear to what extent this impedes the ability of external testers to surface and probe\nsafety issues.\nAn early safety bounty trial by OpenAI for ChatGPT incentivized over 1500 submissions, with limited\npublicization and $20,000 of API prizes in total [149]. While OpenAI noted that the submissions\nseemed to yield few new discoveries beyond the safety issues that internal red-teams had already\nnoticed, the exercise produced insight into the most common routes of attack and lessons for future\npublic engagement [148].\nSafety bounty programs can also be leveraged to identify promising talent. Bounty hunters who\nsubmit multiple helpful tips could be contacted and employed to perform more extensive system\ntesting, and be granted deeper levels of system access after appropriate vetting. In cybersecurity,\nsome bug bounty hunters earn payouts totaling over $1 million for their work, and go on to work for\nlarge firms [150, 151].\n20\n4.2\nAccelerate (beneficial) AI Progress\nTable 3: Section summary: Open-sourcing as a mechanism for accelerating AI progress\nThe argument\nfor open-source\nAI\nOpen-sourcing allows more people to contribute to AI development processes\nand enables large-scale collaborative efforts. The idea is that more expertise,\nmore diverse perspectives, and simply more human creativity and hours put into\nAI development will drive innovation in new and useful downstream integra-\ntions, advance AI safety research, and help push forward the boundaries of AI\ncapability.\nEvaluation\nof Benefit\nIntegration Progress\n\u2022 Open-sourcing is most helpful for integration progress. Model access al-\nlows more people to tinker, innovate, and optimize for integration with new\ndownstream applications.\nCapability Progress\n\u2022 Open-sourcing is less beneficial for capability progress than for integration\nprogress.\n\u2022 The benefit is limited by bottlenecks in the talent, compute, and data resources\nneeded for contributing to cutting-edge AI capability research.\nSafety Progress\n\u2022 Academic safety research is often curtailed by insufficient access to highly\ncapable models.\n\u2022 The benefit of open-source might be reduced by insufficient computation\ninfrastructure outside of leading AI labs for running highly capable models.\nAlternative\nmethods for\ndriving AI\nprogress\nIntegration Progress\n\u2022 Use plugins for exploration of new applications.\n\u2022 Provide gated access [i.e. full access restricted to identified third parties]\ncoupled with Know-Your-Customer Requirements.\nCapability Progress / Safety Progress\n\u2022 Provide privileged model access to identified AI research groups, possibly via\nstructured access research APIs.\n\u2022 Seek and organize collaborations with trusted parties and provide gated down-\nload access to collaborators.\n\u2022 Establish a multistakeholder governing body to mediate research access to\nprotect against favoritism and to facilitate independent academic research.\n\u2022 Build incentive structures like large rewards programs for major scientific dis-\ncoveries (e.g., protein folding) or pro-social advances (e.g. health and equity\napplications) using AI and for AI safety breakthroughs (e.g., interpretability).\n\u2022 Commit a certain percentage of profits or research hours towards AI safety\nprojects.\n4.2.1\nThe Argument for Open-Source\nAnother argument for open-sourcing AI is that doing so helps to accelerate progress that pushes the\nboundaries of AI capability, advances AI safety research, and drives innovation of new downstream\napplications and integrations. The idea is that open-sourcing allows more people to contribute\nto AI development processes. It allows downstream developers to optimize and perfect existing\nmodels instead of having to start from scratch for each new application, and it enables large-scale\ncollaborative efforts. Furthermore, progress created by the wider AI community will benefit from\n21\nmore diverse perspectives and insights, which will ultimately help develop AI aligned to unique\ncommunity needs and cultural preferences. In addition, open-source efforts may be more likely\nto focus on pro-social applications of AI, and be less influenced by the financial and commercial\nincentives than industry AI developers.\nThese are benefits widely enjoyed by open-source software communities. Linus Torvalds\u2019 open-source\nrelease of the Linux kernel, in particular, showed how taking advantage of community-wide co-\ncreation allows OSS tools to be developed and released quickly, maintained cheaply, and customized\nfor individual needs without compromising quality. For cloud computing especially, these benefits\nallowed the Linux operating system to directly compete with Windows and MacOS, commercial\nsystems backed by significantly more resources such as specialized knowledge, corporate information-\nsharing infrastructure, performance accountability mechanisms, and marketing and legal support\n[152, 153].\nIt follows that we might expect AI progress to benefit similarly.\n4.2.2\nEvaluating the Benefit for Foundation Models\nIn this section we evaluate the influence of open-source model sharing on driving beneficial foundation\nmodel progress. To focus the conversation we differentiate between three kinds of AI progress: (1)\nintegration progress, (2) safety progress, and (3) capability progress.\n(1) Integration progress.\nIntegration progress is about the discovery of new applications and\nintegrations for foundation models to serve a greater variety of needs\u2014i.e. how a model can be\napplied to new tasks and integrated with other applications. For example, ChatGPT embedded with\nDuolingo has made for an effective language tutoring and practice tool [24].\nOf the three forms of progress, integration progress benefits most from open-source. Open-sourcing\nmodels and model components gives more people access to tinker and innovate. But perhaps more\nimportantly, passing on a model with all life-cycle documentation to downstream developers enables\nthose developers to optimize the model\u2019s performance by fine-tuning its training and to infinitely\ntest and evaluate the model when integrated into the final product \u2014 as Alex Engler writes, there is\n\u201csimply too much at stake for downstream developers to use AI systems they do not fully understand\u201d\n[154].\nIndeed, recent breakthroughs in fine-tuning\u2014specifically Low Rank Adaptation (LoRA)[155]\u2014were\ndriven by open-source communities out of necessity for reducing costs and compute requirements. It\nis a process by which the performance of smaller models can be significantly improved by optimizing\nmodel weights using the outputs of more high-capable models as training data.25\n(2) Safety Progress.\nSafety progress refers to advances made in AI safety research. AI Safety\nresearch works to improve AI safety by identifying causes of unintended and harmful behavior,\naligning AI behavior with human values, improving model interpretability and robustness, and\notherwise developing tools to ensure AI systems work safely and reliably [157, 158].\nCurrent safety research is often limited by insufficient access to large, cutting-edge models and\nrelevant information such as their architecture and training processes [141]. Open-sourcing does\nalleviate these restrictions but is not necessary for all safety research.\nDifferent areas of safety research require different kinds of model access. For example, evaluation\nand benchmarking research aims to develop and test methods to assess the capabilities and safety of\nAI systems. Often the ability to sample from a model via an API will be sufficient for this research,\nas current approaches are based on observing a model\u2019s output in response to a given prompt.\nIn contrast, research areas such as alignment and interpretability require more comprehensive access.\nAlignment research, which aims to help AI systems better reflect user preferences and values, typically\nrequires researchers to be able to modify a model through fine-tuning, including through the use\n25We classify fine-tuning as a form of integration progress instead of capability progress because the impressive\nperformance of fine-turned models bootstraps on the capabilities of existing models. Pushing the frontier of AI\ncapability still requires significant talent and compute at a scale only found in large, well-resourced labs [156].\n22\nof reinforcement learning. Like model sampling, fine-tuning might also be facilitated through an\nAPI (e.g. [128]. However, some experts express concern that current interfaces often do not provide\nenough information about underlying models for them to draw meaningful conclusions from their\nresearch.26 Interpretability research further requires that researchers can directly modify model\ninternals such as learned parameters and activation patterns. Full (or nearly full) model access is\nneeded for interpretability research. That said, current interpretability research is not limited by access\nto large models because interpretability techniques are not mature enough to be \u201ccomputationally\ndoable\u201d in the largest models. In other words, we have a way to go before open-sourcing our most\ncapable models is a significant benefit to interpretability research.\nEven where comprehensive model access is crucial to a research agenda, other factors can reduce the\nbenefits of open-sourcing highly capable models. For example, safety researchers external to private\nlabs sometimes lack sufficient computational infrastructure to run highly capable foundation models\n[141]. Yet, some research agendas, such as those studying emergent capabilities, require access to the\nlargest models at the bleeding edge of development; smaller models that can be run on local hardware\ndo not reliably exhibit the emergent capabilities under investigation, even when fine-tuned on the\noutputs of larger models.\n(3) Capability progress.\nFinally, capability progress describes advancement in frontier AI research\ntoward developing more powerful and capable systems (i.e. working towards AGI).\nThe extent to which open-source contributions can drive progress on AI frontier capabilities may be\nlimited by access to compute and data resources, as well as the distribution of talent.\nFew AI actors have the requisite financial, compute, high-quality data and talent resources to operate\nat the cutting edge of AI research and development. Training new foundation models costs $10-100\nmillion in compute costs and is projected to increase to $1-10 billion in coming years [73]; the stock\nof high-quality data used to train large language models (such as books) currently freely available\non the internet may be depleted in a few years, requiring potentially costly new sources of data,\ninnovations in data efficiency [160], or expensive human feedback data; and AI talent is most heavily\nconcentrated in high-paying positions at leading AI labs, primarily based in the United States, while\nsmaller labs struggle to fill positions [161].\nOpen-sourcing large pre-trained models does allow less-well-resourced actors such as academic labs\nand open-source developers to study and innovate on these existing models. These communities\ncan make technical and conceptual innovation and refinements within open-source environments\nthat generate knowledge that can be incorporated to advance the AI capability frontier. If a high\nvariance of research and development strategies are employed by open-source communities, their\ncontributions may be particularly valuable for advancing state of the art AI.\nFurthermore, open-source model sharing also facilitates talent development. More people being able\nto interact with pre-trained cutting edge-models may, over time, lead to a larger and more diverse AI\ntalent pool for government regulators, AI labs, universities, and auditing institutions to draw from.\nOn a longer time scale this could have a positive effect on capability progress (and safety progress)\nby increasing the talent pool.\nRealistically, however, the advancements that push the capability frontier will nearly exclusively\ntake place at frontier labs in leading nations. In these locations, in-house expertise can draw upon\nopen-source innovations and top talent to run giant training runs using huge compute, data, and\nengineering resources not available to the open source community. (See Section 4.3 for discussion on\ndistributing AI development away from big tech.)\nFurthermore, the desirability of accelerating capability progress is presently hotly debated.\nThis is due to concerns over risks as well as benefits of more advanced models, in addition to the\ngovernance challenge of preparing appropriate regulation and oversight for such a rapidly advancing\n26For example, when attempting to evaluate the effect of instruct fine-tuning across multiple models, Wei et\nal. [159] write: \u201cWe do not compare InstructGPT against GPT-3 models in this experiment because we cannot\ndetermine if the only difference between these model families is instruction tuning (e.g., we do not even know if\nthe base models are the same).\u201d Bucknall et al. [141] discuss this and other examples from literature and expert\ninterviews that elucidate the limitations many APIs pose for researchers.\n23\ntechnology [162\u2013165]. Accordingly, \u201cAccelerating AI capability progress\u201d, to the extent that open-\nsourcing does drive capability progress, should only be considered an open-source benefit if the effect\nof open-sourcing is to drive beneficial progress disproportionately to increasing risk and severity of\nharm.\nToward beneficial AI progress, one benefit of open-sourcing is that it puts AI tools in the hands of\nsafety researchers, e.g., in academia, who would otherwise not have access to the cutting edge models.\nWe expand on this point shortly under \u201cSafety Progress\u201d. Open-sourcing also increases opportunity\nfor external scrutiny.\nHowever, open-sourcing frontier models might also drive progress in undesirable directions. One\nexample of this is the potential effect of open-source model sharing on the offense-defense balance;\nopen-sourcing may empower malicious actors to offensively identify and exploit system vulnerabil-\nities to a greater extent than it facilitates defensive activities to protect against malicious use (See\nSection 3.1 for further details).\nBox 2: Strategies for driving safety progress alongside model sharing\nAlongside alternative model sharing strategies, there are also other activities that can be\nemployed to help safety progress. These are not alternatives to model sharing, but are\nworthwhile considerations if accelerating safety progress is the desired outcome.\nLarge rewards programs\nProgress might be accelerated in crucial AI safety domains by building new incentive struc-\ntures, for instance, large rewards programs on the scale of millions or billions of dollars to\nreward major AI safety breakthroughs (e.g., in model interpretability). The goal is to make\nsafety progress, like capability progress, a financially lucrative endeavor.\nCommitting profits to safety research\nSafety progress could also be prioritized by orchestrating agreements between frontier AI\nlabs to commit a certain percentage of profits or research hours towards AI safety projects.\nThis would reduce incentives for labs to cut corners on safety research and help remedy the\nlarge mismatch in resources currently dedicated to capability progress versus safety progress\nby major labs.\nInternational institutions and collaborations for AI Safety\nFinally, in the long term we may benefit greatly from establishing international institutions and\ncollaboration to promote AI safety [166]. For instance, there is budding interest in establishing\nglobal collaboration on advancing AI safety research akin to CERN or ITER27[166, 167].\nSuch a project could funnel significant resources towards AI safety research, enable open and\nsecure sharing of insights between leading nations, and reduce the burden of cost (financial\nand opportunity costs) associated with dedicating significant resources to AI safety research.\nThere is a risk that collaborative AI safety research would facilitate the diffusion of dual use\ntechnologies and disincentivize leading labs from conducting their own safety research. It is\ntherefore imperative that any such project be coupled with efforts to involve safety researchers\nfrom leading labs (e.g., by offering dual appointment or advisory positions) and to implement\ncareful membership restrictions and information security measures [166].\n27The International Thermonuclear Experimental Reactor (ITER) is an international nuclear fusion research\nand engineering megaproject aimed at creating energy through nuclear fusion. https://www.iter.org/\n24\n4.2.3\nOther Ways to Drive (Beneficial) Progress\nThere are a variety of methods that might be employed to help pursue open-source objectives. These\nmethods do not necessarily cover all losses from not open-sourcing, but they do not suffer the same\nrisks as open-sourcing and can be used in combination.\nToward integration progress, for example, new integrations and applications can be explored and\nimplemented through the development of \u201cplugins\u201d allowing a model to integrate with other services\n[168]. The plugin could be submitted to the developer or a third-party auditor before publication.\nThis option provides a mechanism for new integrations and applications to be reviewed and approved\nbefore being shipped while still tapping into public creativity and representation of interests and\nneeds.\nIn so far as model access allows downstream developers to more thoroughly understand and test\nthe performance and safety of their integrations, labs could also provide identified downstream\ndevelopers with privileged access to requested model components via gated download. One policy\nrecommendation is that labs are held to a \u201cknow-your-customer\u201d requirement whereby labs must\nvet and keep a record of potential model recipients (e.g., proposed use, past activities, funding source,\netc.) [53, 169]. Additionally, technical safety measures such as applying a unique fingerprint to each\ncopy of the model\u2019s weights should be applied when feasible [170].\nAs discussed above, the benefit of open-sourcing for safety progress and capability progress is\ndampened by limited talent and compute resources external to major labs. There are, however, other\nmeans of driving both forward.\nAs mentioned in 4.1.3, developers might provide privileged model access to AI safety research\ngroups, possibly via structured access research APIs. While not yet fully realized, there is hope that\nsuitably comprehensive researcher access to closed models can also be provided through structured\naccess approaches [16], such as specialized researcher API access [141]. Such solutions could be\nused in addition to existing social and legal mechanisms for ensuring information security, such as\nresearcher NDAs, thereby potentially providing more comprehensive security guarantees than either\napproach could in isolation.\nFor the purpose of propelling capability progress, labs could also actively seek collaborations with\ntrusted parties and provide gated download access to collaborators. This is similar, for example,\nto how OpenAI partnered with research institutions during the staged release of GPT-2, providing\naccess to models for carrying out research into biases and methods for detecting GPT-2-generated\ntext [134]. As before, any time gated download access is provided, it should be backed by know-\nyour-customer investigation and documentation requirements, and any applicable technical safety\nmeasures. Selectively providing model weights to only those researchers whose work requires them\nwould also help reduce the risk of leaks.\nThere is a challenge, however, regarding the decision as to which actors are provided privileged\nmodel access (gated downloadable or via research API) to conduct external evaluation and research or\nfor collaborations. Where labs are inundated with an unmanageable number of requests for research\naccess, favoritism and in-group model evaluations may emerge out of necessity. Labs are also likely\nto prioritize external collaborators who they believe will support their market interests. One possible\nsolution could be to establish a multistakeholder governance body or system for mediating\nresearcher access to highly capable foundation models. For example, within the UK, we might\nimagine the recently established Frontier AI Taskforce taking on such a role.\nSuch a body could also determine the degree of access provided to external researchers (if through\nresearch API). This is important for preventing \u201cindependence by permissions\u201d whereby academic\ncollaborators are able to conduct high-quality independent research, but research directions are\nultimately determined by the access permissions given by the developer [171]. For cutting-edge\nmodels especially, researchers may not know which access permissions they need to request, and\nthe incentives are not clear for developers to reveal everything they know (or suspect) about their\nproprietary models.\n25\n4.3\nDistribute Control Over AI\nTable 4: Section summary: Open-sourcing as a mechanism for distributing control over AI\nThe argument for\nopen-source AI\nOpen-sourcing foundation models will help distribute influence over the future of AI\naway from major labs by empowering smaller groups and independent developers.\nThe idea is that open-sourcing \u201cdemocratizes AI\u201d, giving more people influence over\nhow AI is developed, optimized, and used, and promotes the representation of more\ndiverse interests and needs in the direction of the field.\nEvaluation\nof Benefit\n\u2022 Open-sourcing helps distribute control over downstream integration progress to\nopen-source communities.\n\u2022 The effect of open-source on distributing influence over capability and safety\nprogress is reduced by concentration of compute, data, and talent resources needed\nto influence frontier AI capability progress in large, well-resourced labs.\n\u2022 Open-sourcing large and highly capable models can also help amplify the original\ndeveloper\u2019s influence over AI ecosystems; downstream innovations building on\nopen-sourced models are easily integrated back into the developers\u2019 products, and\nthe open-source communities become go-to hiring pools already familiar with the\ncompany\u2019s tools and models.\n\u2022 Open-sourcing is a tool that can aid the democratization of AI. But AI democra-\ntization is a multifaceted and proactive project to distribute influence over highly\ncapable AI systems\u2014how they are used, distributed, developed, and regulated\u2014to\nwider communities of stakeholders and impacted populations. Open-sourcing alone\ncannot fulfill the goal of AI democratization.\nAlternative\nmethods for\ndistributing\ncontrol over AI\n\u2022 Implement participatory or representative deliberative processes to democratically\ninform high-impact decisions about AI development, use, and governance, includ-\ning decisions about model access.\n\u2022 Institutionalize democratic structures (e.g., via democratically selected boards or\nby requiring the use of such deliberative processes for all decisions on particular\ntopics) within large labs to dissipate control away from unilateral decision-makers.\n\u2022 Support appropriate regulatory intervention to developer behaviors and to guard\nagainst regulatory capture.\n4.3.1\nThe Argument for Open-Source\nA commonly cited argument for open-sourcing foundation models is that doing so will help distribute\ninfluence over the future of AI away from major labs and to the wider AI community [172, 173].\nThere are very good reasons for wanting to distribute influence over AI. There are economic implica-\ntions; if open-sourcing foundation models enables downstream developers to independently innovate\nand capitalize on a lucrative technology, this could help to ensure that the huge value AI promises to\nproduce does not accrue only to a handful of tech giants.\nThere are also social and political implications; major AI labs are unelected entities that primarily\nserve their own and shareholder interests. The idea is that distributing influence over AI development\nprocesses prevents private labs from exercising too much control over numerous aspects of public\nlife that emerging AI capabilities promise to transform. As Emad Mostaque explains Stability AI\u2019s\ndecision to open-source Stable Diffusion, \u201cWe trust people, and we trust the community, as opposed\nto having a centralized, unelected entity controlling the most powerful technology in the world\u201d [174].\nOverall, the idea is that open-sourcing \u201cdemocratizes AI\u201d, giving more people influence over how AI\nis developed and used, and promoting the representation of more diverse interests and needs in the\ndirection of the field.\n26\n4.3.2\nEvaluating the Benefit for Foundation Models\nHistorically, open-source software development has had a noteworthy influence-distributing effect.\nFor instance, the open-source Linux kernel now underpins numerous operatings systems (e.g., Ubuntu,\nFedora, Debian) that offer competitive and highly-utilized alternatives to Windows and MacOS. We\ncaution, however, that this effect should not be expected to translate perfectly to the context of\nopen-source foundation models.\nAI democratization is a multifaceted project. Open-sourcing certainly contributes to AI democra-\ntization, though for some aspects of AI democratization the effect is marginal. All aspects of AI\ndemocratization benefit from investment in other proactive activities aimed at distributing influence\nover AI and AI impacts. We briefly review four aspects of AI democratization originally outlined in\n[175] and comment on the extent to which open-source model sharing contributes to each.\n(1) Democratization of AI development\nThe democratization of AI development is about helping a wider range of people contribute to\nAI design and development processes. Of the four forms of AI democratization, open-sourcing\npromotes the democratization of development most, and most directly. Open-sourcing places models\nin the hands of large communities of open-source developers who can continue to examine and\nmodify the model. Open-sourcing also supports self-learning and education among open-source\ndevelopers, allowing them to keep up with advances in model design and safety research and to\ncontinue participating in AI development as techniques evolve.\nThere are, however, some ways in which the effect of open-source on the democratization of AI\ndevelopment is limited.\nFirst, especially with respect to highly-capable models, open-source development activities may\nbe increasingly limited by resource accessibility. Participating at the cutting-edge of AI research\nand development requires significant financial, compute, talent, and high-quality data resources,\nand few actors outside of major labs and government actors have these requisite resources (See\nSection 4.2). As Widder et al. [64] write, \u201ceven maximalist varieties of \u2018open\u2019 AI don\u2019t democratize\nor extend access to the resources needed to build AI from scratch\u2014during which highly significant\n\u2018editorial\u2019 decisions are made.\u201d Accordingly, toward the goals of facilitating wider and more diverse\nparticipation in driving AI development, the benefit of open-sourcing is limited.\nSecond, open-sourcing can help leading AI developers to further entrench their control over AI\necosystems and value production [13, 176]. While a near term, first-order effect is that downstream\ndevelopers gain influence over model application and integration progress, a longer-term, second-\norder effect of open-sourcing large foundation models is to feed back value and influence to the\noriginal developer. Open-sourcing grants wider AI communities access to a technology that they can\nfine-tune and customize to a variety of new applications. However, these downstream innovations\nwhich build on top of the original open-sourced model architecture, are then easily integrated back\ninto the original developer\u2019s own products and ecosystems. Open-source communities also become\ngo-to hiring pools already familiar with the company\u2019s tools and models.\nThird, the wider AI community, including open-source communities, are relatively homogenous in\nterms of economic, cultural, gender, and geographic grouping [161, 177]. Open-source communities\nare often better than tech companies at building diverse and inclusive spaces, and they put significant\neffort into engaging with the broader world.28 However, something is still lost conflating the\ndistribution of power to open-source communities and the distribution of power to communities\ngenerally. There is a risk that by missing this nuance we exaggerate the benefits of open-sourcing\nalone and underplay the need for other mechanisms for promoting the democratization of AI. In\naddition to model sharing, democratizing AI development requires the provision of educational and\nupskilling opportunities and technical support infrastructure (e.g., high bandwidth network access\n28For example, the open-source AI research organization EleutherAI [178] and the open-source collective\nBigScience [179] have teams spanning four or more continents and have projects focusing on increasing access\nto NLP technologies for people who speak non-dominant languages. Similarly, Cohere is running a program to\ncollect fine-tuning data in hundreds of languages [180], and LAION is the only organization, at time of writing,\nto be training massively multilingual CLIP models [181, 182].\n27\nand cloud compute services) to encourage and enable wider and more diverse participation in AI\ndevelopment processes.\n(2) Democratization of AI use\nThe democratization of AI use is about enabling a wide range of people to use and benefit from AI\napplications. Open-sourcing allows downstream developers to tailor models to serve diverse needs.\nFor most people, using an AI system also requires the provision of intuitive interfaces to facilitate\nhuman-AI interaction without extensive training or technical knowhow. Open-source communities\ncan help develop these interfaces.\nHowever, one thing to consider is that benefiting from the use of an AI system does not always require\nthat everyone be able to use the AI system. Especially for highly-capable and potentially high-risk\nsystems, a designated user could employ the system for the benefit of the community. For example, a\ndrug discovery system which could be maliciously used to discover new toxins, could be used in a\ncontrolled, limited-access setting while resulting pharmaceuticals are \u201cdemocratized\u201d in the sense\nthat they are made accessible to anyone in need.\n(3) Democratization of AI profits\nThe democratization of AI profits is about facilitating the broad and equitable distribution of value\naccrued to organizations that build and control advanced AI capabilities. Subgoals of profit democra-\ntization include: smoothing economic transition in case of massive growth of the AI industry, easing\nfinancial burden of job loss to automation, preventing a widening economic divide between AI leading\nand lagging nations, and acknowledging through compensation the human labor and creativity that\ngoes into producing and catering the data upon which highly lucrative AI capabilities are built.\nOpen-sourcing helps democratize profits in two ways. First, by open-sourcing their models, rather\nthan charging for access, companies will tend to capture less of the wealth produced by these models;\nusers can employ the models to generate profits (e.g. through increased productivity) without having\nto pay some portion back to the developer. Second, open-sourcing helps democratize profits insofar\nas it allows a more widespread array of downstream developers to iterate upon AI models and place\ncompetitive pressure on large labs; open-sourcing can make it more difficult for large labs to build\nprofitable downstream applications of their models, since they will need to compete with open-source\ndeveloper communities that are building competing applications.\nHowever, the effect of open-source on distributing profits from highly-capable AI will likely be\nlimited in a couple respects. First, open-source community participation in the development of\ncutting-edge models will be curbed by inadequate access to necessary compute and financial resources\n(Section 4.2), thus limiting the competitive pressure open-source developers can put on well-resourced\nlarge labs. Second, as discussed earlier in this section, open-sourcing frontier systems can also be\nfinancially advantageous to large companies in the long run as they can use downstream developers\nas a free labor source, easily feeding their best contributions and insights back into the company\u2019s\nown products.\nAdditional proactive measures are needed to help pursue the goals of profit democratization. These\nmight include implementation of a profit redistribution scheme such as taxation and redistribution by\nthe state [183, 184], lab commitments to a windfall clause whereby developers obligate themselves to\ndonate windfall profits (measured as \u201ca substantial fraction of the world\u2019s total economic output\u201d)\nfor redistribution [185], and mechanisms for compensating content creators for the data on which\ngenerative AI models are trained, for instance, through the creation of licensed data sets [186, 187].\n(4) Democratization of AI governance\nFinally, the democratization of AI governance is about distributing influence over decisions about AI\nto a wider community of stakeholders and impacted populations. AI governance decisions involve\nbalancing AI related risks and benefits to determine how and by whom AI is used, distributed,\ndeveloped, and regulated.\nOf the four forms of AI democratization, open-sourcing has the least impact on distributing influence\nover AI governance decisions. Open-sourcing distributes influence over AI governance decisions away\n28\nfrom major labs insofar as it enables wider AI research and development communities to participate\nin, and therefore direct, AI development processes. However, open-sourcing does little to gain\ninfluence over AI governance decisions for the public more broadly. In this respect, democratizing\nAI governance involves applying democratic processes directly to high-impact decisions made by AI\ndevelopers, subjugating labs to regulation by democratic governments, or some combination thereof.\nWe expand on these possibilities shortly in 4.3.3.\nOverall, open-sourcing AI should not be conflated with democratizing AI\n. Open-sourcing is\nbut one option for sharing models and model components; model sharing is but one mechanism for\ndemocratizing AI development; and the democratization of AI development is but one dimension of\ndistributing influence and control over the future of AI. Indeed, the decision to open-source is itself a\nconsequential decision over which influence can and likely should be distributed away from private\nlabs.\n4.3.3\nOther Ways to Reduce Corporate or Autocratic Control\nA comprehensive approach will be needed to counteract the centralisation of power in AI companies\nas AI systems become more capable and therefore confer more political and economic power. This\nsection presents options for distributing influence over AI via the democratization of AI governance.\nIt is not an exhaustive list, but it illustrates that there are a wide variety of methods that can be used\nto decentralize power and to better facilitate representation of diverse stakeholder interests and needs\nin decisions about how and by whom AI is developed, used, distributed, and regulated.\nPublic participation and deliberation.\nAI labs and policymakers could institute participatory\nand deliberative democratic processes to guide decision-making about complex issues in AI [175].\nFor example, participatory platforms such as Pol.is [188] might be used to solicit and synthesize\npublic input into complex normative decisions about AI at low cost. Alternatively, representative\ndeliberations, such as citizens assemblies, can convene representative microcosms of impacted\npopulations (or even global populations) selected by sortition (i.e. stratified sampling) to tackle AI\ngovernance questions [189, 190].\nSuch efforts by large tech companies are not unprecedented. Meta, for example, has quietly run a set\nof national and transnational pilots [191, 192] to navigate their \u2018complex normative challenges\u2019 and\nhave since scaled up to a near-global deliberative process [193]. Twitter had also planned to pilot\nsuch processes before its acquisition [194], and OpenAI recently has launched a \u201cdemocratic inputs\nto AI\u201d grant program to experiment with setting up democratic processes for deciding what rules AI\nsystems should follow within legal bounds [195].\nInstitutional structure.\nInstead of, or in addition to, directly eliciting public input to inform key\ndecisions, another option is for AI labs to introduce organizational structures that are more democratic\nin nature. These structures would help maintain transparency of internal practices and to dissipate\ncontrol away from unilateral decision-makers in such a way that better reflects stakeholder interests.\nRelevant stakeholders importantly include public communities whose lives are impacted by emerging\nAI capabilities.\nFor example, AI labs can incorporate as Public Benefit Corporations (PBC).29 A PBC is a for-profit\ncorporation intended to produce public benefits and to operate in a responsible and sustainable manner.\nIncorporating as a PBC does not necessitate public involvement, but it does give a corporation clearer\nlegal standing to make decisions about institutional structure that aim to maximize public benefit,\neven if that might conflict with maximizing shareholder interests.\nFor more direct public control, a golden share (a nominal share which is able to outvote all other\nshares) could be held by a perpetual purpose trust (a non-charitable trust established for the benefit of\na purpose) governed by a committee that is a representative sample of the public selected by sortition\nor elected by stakeholders.\n29There is increased momentum toward this now, as two leading AI organizations, Anthropic and Inflection\nAI, are both PBC\u2019s.\n29\nAlternatively, AI labs could implement democratically selected oversight boards. Such a board\nmight, for instance, be composed of representatives from the public selected by sortition or, perhaps a\nmore palatable option, a sortition body is used to \u201celect\u201d board members from among a nominated\nlist. \u201cNominators\u201d could be members of government (e.g., state governors), and perhaps two to three\nboard members are committed to \u2018voting\u2019 on issues as determined by a democratic process (e.g.,\npublic polling or citizen assembly, whichever is appropriate to the situation).\nRegulation by democratic governments.\nFinally, of course, labs can encourage government\nregulation that restricts their behavior and capacity for independent decision-making where the\npotential for significant societal impact is high. For example, governments could require authorization\nfor large foundation model release and institute multistakeholder committees to mediate research\naccess to highly capable models. Regulatory interventions should be developed in response to\ndeliberative processes involving developers, open source communities, academia, and civil society to\nreflect diverse stakeholder interests and to guard against regulatory capture by AI industry. In this\nway appropriate government regulation can help systematically reduce unilateral control over AI by\nleading private labs.\n5\nRecommendations\nWe conclude this paper with five high-level recommendations for AI developers, standard setting\nbodies, and governments for working towards safe and responsible model sharing decisions. These\nrecommendations are necessarily incomplete and preliminary because best practices for open-sourcing\nhighly capable models will be highly context-dependent and require input from numerous parties.\nWe look forward to further development of these recommendations in future work.\nTable 5 summarizes the recommendations. Each recommendation is explained in more detail below.\nTable 5: Recommendations for working towards responsible model-sharing\n1. Developers and governments should recognise that some highly capable models will be\ntoo risky to open-source, at least initially. These models may become safe to open-source\nin the future as societal resilience to AI risk increases and improved safety mechanisms are\ndeveloped.\n2. Decisions about open-sourcing highly capable foundation models should be informed\nby rigorous risk assessments. In addition to evaluating models for dangerous capabilities\nand immediate misuse applications, risk assessments must consider how a model might be\nfine-tuned or otherwise amended to facilitate misuse.\n3. Developers should consider alternatives to open-source release that capture some of the\nsame [distributive, democratic, and societal] benefits, without creating as much risk. Some\npromising alternatives include gradual or \u201cstaged\u201d model release, model access for researchers\nand auditors, and democratic oversight of AI development and governance decisions.\n4. Developers, standards setting bodies, and open-source communities should engage in\ncollaborative and multi-stakeholder efforts to define fine-grained standards for when\nmodel components should be released. These standards should be based on an understanding\nof the risks posed by releasing (different combinations of) model components.\n5. Governments should exercise oversight of open source AI models and enforce safety\nmeasures when stakes are sufficiently high. AI developers may not voluntarily adopt risk\nassessment and model sharing standards. Governments will need to enforce such measures\nthrough options such as liability law and regulation (e.g. via licensing requirements, fines,\nor penalties). Governments will also need to build the capacity to enforce such oversight\nmechanisms effectively.\n30\n1. Developers and governments should recognise that some highly capable models will be too\ndangerous to open-source, at least initially.\nIf models are determined to pose significant threats, and those risks are determined to outweigh the\npotential benefits of open-sourcing, then those models should not be open-sourced. Such models may\ninclude those that can materially assist development of biological and chemical weapons [50, 109],\nenable successful cyberattacks against critical national infrastructure [52], or facilitate highly-effective\nmanipulation and persuasion [88].30\nThis is not to say that a given highly capable model should never be open-sourced. Expected model\nimpacts are likely to change with increasing societal resilience and development of new defensive\ntechniques. However, model developers should consider a default policy of pursuing release through\nalternative methods rather than open-source if they find that a model poses significant threats, and\nthat the benefits of open-sourcing do not outweigh the risks of doing so.\n2. Decisions about open-sourcing highly capable foundation models should be informed by\nrigorous risk assessments.\nIn the past, the benefits of open-sourcing seem to have clearly outweighed the risks. However, we\nare not confident that this will continue to be the case in the future for highly capable foundation\nmodels (Section 3). It is therefore important to carefully assess potential risks and benefits before\nopen-sourcing the model, especially since the decision to open-source a model is irreversible. The\nneed to conduct risk assessments prior to model release seems to be generally accepted [53, 196,\n197].\nThe National Institute of Standards and Technology (NIST) provides guidance for how to conduct\nsuch an assessment [198] which might be applied to inform open-sourcing decisions. Some scholars\nhave suggested ways in which the NIST AI Risk Management Framework could be adapted to\ngeneral-purpose AI systems [199] and catastrophic risks [200]. In the future, we think that developers\nof highly capable foundation models will need to combine qualitative and quantitative approaches.\nThey may need to conduct deterministic safety assessment as well as probabilistic risk assessments,\nas is common in the nuclear industry [201].\nSince risks associated with certain model capabilities are particularly concerning, risk assessments\nshould be informed by evaluations of dangerous model capabilities [48]. Both internal [29, 202]\nand external model evaluations should be conducted. External assessments can take many different\nshapes, such as model evaluations [54, 140], model audits [138, 203, 204], red-teaming [144, 147], or\nresearcher access via API [141].\nDevelopers intending to open-source a model that is likely to be highly capable should conduct more\ninvolved risk assessments than they would have otherwise. Firstly, the risk assessment should be more\nthorough to have the decision be as well-informed as possible, given the irreversibility of decisions to\nopen-source. Methods such as additional red teaming, internal testing, and staged release approaches\nshould be pursued.\nSecondly, risk assessments ahead of open-sourcing decisions need to assess how the model can be\namended to facilitate misuse. The risk assessment must consider the ease with which safeguards\ncan be removed and \u201cuncensored\u201d versions of the model can be distributed. Often, safeguards will\nbe so easy to remove that it is better to avoid the model having the worrying capability altogether\n(Section 3). For example, while Stable Diffusion 1.0 had a safety filter, it was easy to disable [83]. In\nfuture releases, Stability AI therefore opted to remove inappropriate content from the training data\ninstead [205].\nRisk assessments should also consider the extent to which risks can be exacerbated by malicious\nactors fine-tuning or otherwise amending the model to elicit or develop more dangerous capabilities\n(Section 3). It is difficult to anticipate how the model is going to be fine-tuned. It is therefore crucial\nthat red-teamers have fine-tuning access to the model ahead of release.\n30Note that we do not claim that existing models are already too risky. We also do not make any predictions\nabout how risky the next generation of models will be. Our claim is that developers need to assess the risks and\nbe willing to not open-source a model if the risks outweigh the benefits.\n31\nThirdly, risk assessments should consider factors external to the model. The social impacts of a\nmodel (e.g., on democratic processes) are difficult to forecast and necessitate consideration of how\nthe model will interact with other tools and outside institutions, cultures and material conditions [39].\nFinally, for red-teaming, model evaluations and other external safety assessments to be effective, AI\ndevelopers need to elicit participation from a diverse and comprehensive set of experts. Only by\nharnessing a varied set of viewpoints and expertise can we ensure a broad spectrum of potential risks\nare adequately identified and evaluated.\n3. Developers should consider alternatives to open-source release as possibilities for working\ntowards distributive, democratic, and societal advancement goals with less risk.\nBefore open-sourcing a highly capable foundation model, developers should first clarify goals\u2014\nreflecting on why specifically they want to open-source a model\u2014and then consider alternatives that\nmay reach those goals at lower risk.\nWith respect to alternative model-sharing strategies, some options may offer some of the same benefits\nas open-sourcing, but unlike open-sourcing, still allow developers to adjust their deployment strategy\nafter release. The idea that models are either fully open or fully closed is a false dichotomy. As\ndiscussed in Section 4, there are numerous options for gated, API, or hosted access in between which\nallow for varying degrees of model probing and researchability [69, 141], and there are proposed\nframeworks to help navigate these options [81, 131].\nDevelopers could also deploy the model in stages (staged-release) and gather observational data\nabout how a model is likely to be (mis)used and modified if open-sourced (Section 4.1.3). Finally,\ndevelopers could employ proactive efforts to pursue desired benefits, such as by implementing\ndemocratic processes to distribute influence over development and release decisions (Section 4.3.3).\n4. A collaborative and multi-stakeholder effort is needed to define fine-grained standards for\nwhen model components should be released.\nStandard-setting organizations or industry bodies should develop model-sharing standards that provide\nguidance relating to decisions about whether, and if so how, to open-source highly capable foundation\nmodels. Such a standard would contribute to more consistent industry practices and could be an\nimportant step towards regulation. There are a wide range of model-sharing options, even within the\ncurrently ill-defined category of \u201copen-source\u201d systems (see Box 1).\nModel-sharing standards should both support safe model distribution and protect open-source prac-\ntices and benefits. To achieve both, these standards must be fine-grained and built on a well-researched\nunderstanding of the extent to which access to different (combinations of) model components enable\nunrestricted model use, reproduction, and modification.\nWe make a start at breaking down and defining the numerous model components that can be indepen-\ndently shared in Appendix A. It is, however, a much larger project than we can do justice to here, and\nit is a project on which members of open-source communities should be centrally involved. A clear\nunderstanding of activities enabled by access to various model components can then be used to inform\nmodel-sharing standards that are well-tailored to their purpose, that are not overly burdensome, that\nprevent distribution of dangerous capabilities, and that do not unnecessarily undermine open-source\nbenefits.\nTechnical experts, open-source communities, policymakers, and civil society all need to be involved\nin this process. There are several actors who could develop such standards. Although standard-setting\norganizations like NIST [198] and ISO/IEC [206] have published standards for AI, they do not\nseem to have engaged with questions around open-sourcing foundation models specifically. The\nPartnership on AI (PAI) has a working group on foundation models [207] and they have published\nsimilar guidelines for publishing research in the past [208]. The Open Source Initiative recently\nstarted a working group to define what makes an AI system \u201copen source\u201d [82]. Another body that\ncould contribute industry expertise is the recently-announced Frontier Model Forum [209], however\ncurrent participants have generally not open-sourced their most advanced foundation models.\n32\n5. Government should exercise oversight and enforcement where stakes are sufficiently high.\nAI developers may not voluntarily adopt the risk assessment and model sharing standards described\nabove, and government involvement will likely be needed. Without such involvement, developers\nmay not be sufficiently incentivised to voluntarily conduct thorough risk assessments ahead of\nmodel release, to appropriately act on those results, to provide sufficient external access to their\nmodels, or put in place appropriate safeguards. For instance, AI developers may preferentially\nchoose \u201cfriendly\u201d external assessors who share similar concerns around certain types of risk, or\nwhose financial incentives undermine their ability to provide an independent assessment.\nTo mitigate such potentialities, governments should increase oversight capacity and set up mechanisms\nfor enforcing rigorous risk assessments and responsible model release in sufficiently high-stakes\ncontexts. Governments need to ensure that oversight is rigorous and independent, supported by a\ndiverse and comprehensive set of independent advisors, and investigates a wide range of AI risks.\nSimilarly, enforcement mechanisms need to guard against the risk of regulatory capture.\nThere are multiple options governments could consider in terms of enforcement, such as:\nLiability.\nDevelopers could be held liable for harms caused by their models that could have been\nreasonably foreseen31 or avoided through an exercise of due care.32 While courts will ultimately have\nto decide liability on a case-by-case basis, there are strong incentives for developers to demonstrate\ndue care, by, for example, conducting thorough risk assessments and model evaluations, implementing\nadequate precautionary measures, refraining from or reducing high-risk activity,33 and maintaining\ntheir ability to limit harms that occur post-release. Existing tort law already covers unjustifiably risky\nacts and omissions, via negligence for failing to exercise due care (including to prevent foreseeable\ncriminal conduct by a third parties34), products liability for defective designs, and strict liability for\nabnormally dangerous activities.35 A critical task will be to clarify the application of these doctrines\nto open-sourcing highly capable foundation models [214]. Where the application of existing liability\nregimes fails to address significant risks, new statutory duties and liability laws may need to be\ndeveloped.\nRegulation.\nGovernments could legally require developers of highly capable foundation models to\nconduct pre-deployment risk assessments, report potentially dangerous capabilities discovered during\nmodel evaluation, and provide model access pre-deployment to government auditors. Regulations may\nalso specify under which conditions models may be open-sourced [53]. They could also encourage\nor mandate that significant model deployments are preceded by notifications to relevant parts of\ngovernment [215]. Such requirements could be enforced by administrative enforcement measures,\nboth before model deployments (e.g., via a licensing regime) as well as after (e.g., via fines and\npenalties) [53].\nIt is worth noting that liability and regulation each have their strengths and weaknesses. While\nliability is generally less onerous and more flexible, enforcing liability rules might be difficult (e.g.,\nbecause of causation and attribution problems, especially when a malicious actor intervenes) and it is\nnot possible to enforce liability rules ahead of model deployments. Regulation is the only way to\nenforce compliance before a model is open-sourced. However, regulation typically leads to higher\ncompliance costs and there are risks of regulatory capture. In general, liability should be seen as a\n31See [210] \u00a7 4 (Duty) and \u00a7 6 (Tortious Conduct) (1965), and \u00a7 901 on the general principle of liability\n(1979); See [211] on products liability.\n32See [212] on the legal concept of negligence.\n33See [213, p. 61]\n34See [210] \u00a7\u00a7 302A-B (1965); Restatement (Third) of Torts: General Principles \u00a7 17 (Discussion Draft April\n5, 1999) (\"The conduct of a defendant can lack reasonable care insofar as it can foreseeably combine with or\nbring about the improper conduct of . . . a third party.\"); see, e.g., Hamilton v. Accu-Tek, 62 F. Supp. 2d 802,\n825 (E.D.N.Y. 1999), 222 F. 3d 36 (2d Cir. 2000), 95 N.Y.2d 878 (N.Y. 2000) (Holding that gun manufacturers\nhad a duty \u201cto take reasonable steps available . . . to reduce the possibility that [their products would] fall into\nthe hands of those likely to misuse them\u201d and thus could be held legally responsible under New York negligence\nlaw for criminal shootings resulting from failures to \u201cminimize the risk\u201d through their distribution and marketing\nchoices).\n35See [210] \u00a7 520 (1977).\n33\ncomplement to, rather than a substitute for, regulation [53]. Since the right mix of policies will be\nhighly context-specific, we do not make any further recommendations.\nPolicy interventions on open-sourcing are delicate because of the obvious benefits of open-sourcing\nand because for-profit companies might use safety concerns as an excuse to gain a competitive\nadvantage. These concerns should be taken seriously, and further research is needed to understand\nthe risks, benefits, and legal feasibility of different policy options. However, policy interventions\nstill seem necessary because open-sourcing highly capable foundation models might essentially\ndemocratize the ability to cause significant harm and because the decision to open-source a model is\nirreversible [216]. We think the current debate around the issue [217] is healthy and necessary to\nstrike the right balance between open-source risks and benefits. In this paper, we have advocated for\na risk-based approach that could be summarized as \u201cmake open-source decisions with care\u201d.\n6\nConclusion\nOpen-sourcing offers clear advantages including enabling external oversight, accelerating progress,\nand decentralizing control over a potentially transformative technology. To date, open-source practice\nhas provided substantial net benefits for most software and AI development processes, distributing\ninfluence over the direction of technological innovation and facilitating the development of products\nwell-tailored to diverse user needs.\nHowever, as AI research progresses and capabilities improve, open-sourcing also presents a growing\npotential for misuse and unintended consequences. Open-sourcing increases the risk of proliferation\nof model flaws downstream. With access to model weights and code, malicious actors can also more\neasily bypass safety measures and modify models or fine-tune models to display dangerous capabili-\nties. Some of the most worrying potentialities involve the use of highly capable foundation models to\nbuild new biological and chemical weapons, to mount cyberattacks against critical infrastructures and\ninstitutions, and to execute highly-effective political influence operations.\nFor some highly capable foundation models these risks may come to outweigh open-source benefits.\nIn such cases, developers and regulators should acknowledge that the model should not be open-\nsourced, at least initially. These models may become safe to open-source in the future as societal\nresilience to AI risk increases and improved safety mechanisms are developed.\nModel release decisions should therefore be responsive to comprehensive risk assessments and a\nfine-grained understanding of what activities are enabled by freely sharing different combinations\nof model components. These decisions should also take into account how alternative model sharing\noptions (e.g. staged release, gated access, and research API) might further some of the same goals as\nopen-sourcing. Alternative proactive measures to organize secure collaborations, and to encourage\nand enable wider involvement in AI development, evaluation, and governance processes might also\nbe employed. Open-sourcing is but one option for sharing models, and model sharing is but one\nmechanism for facilitating wider community contributions to AI evaluation, development, and control.\nOverall, openness, transparency, accessibility, and wider community input are key to facilitating\na future for beneficial AI. The goal of this paper is therefore not to argue that foundation model\ndevelopment should be kept behind closed doors. Model sharing, including open-sourcing, remains\na valuable practice in most cases. Rather, we submit that decisions to open-source increasingly\ncapable models must be considered with great care. Comprehensive risk assessments and careful\nconsideration of alternative methods for pursuing open-source objectives are minimum first steps.\n34\nReferences\n[1]\nThe Collective Intelligence Project. Introducing the Collective Intelligence Project Solving\nthe Transformative Technology Trilemma through Governance R&D. 2023. URL: https:\n//cip.org/whitepaper (visited on September 23, 2023).\n[2]\nJ. Hoffmann et al. Training Compute-Optimal Large Language Models, March 29, 2022. DOI:\n10.48550/arXiv.2203.15556. arXiv: 2203.15556 [cs].\n[3]\nOpenAI. GPT-4 is OpenAI\u2019s most advanced system, producing safer and more useful re-\nsponses. URL: https://openai.com/gpt-4 (visited on September 23, 2023).\n[4]\nAnthropic. Claude 2. Anthropic. July 11, 2023. URL: https://www.anthropic.com/\nindex/claude-2 (visited on September 24, 2023).\n[5]\nG. Brockman, A. Eleti, E. Georges, J. Jang, L. Kilpatrick, R. Lim, L. Miller, and M. Pokrass.\nIntroducing ChatGPT and Whisper APIs. March 1, 2023. URL: https://openai.com/\nblog/introducing-chatgpt-and-whisper-apis (visited on September 24, 2023).\n[6]\nS. Goldman. Hugging Face, GitHub and more unite to defend open source in EU AI legislation.\nVentureBeat. July 26, 2023. URL: https://venturebeat.com/ai/hugging- face-\ngithub-and-more-unite-to-defend-open-source-in-eu-ai-legislation/\n(visited on September 24, 2023).\n[7]\nCreative Commons, Eleuther.ai, GitHub, Hugging Face, LAION, and Open Future. Support-\ning Open Source and Open Science in the EU AI Act, 2023. URL: https://huggingface.\nco/blog/assets/eu_ai_act_oss/supporting_OS_in_the_AIAct.pdf.\n[8]\nM. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas.\nSelf-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,\nApril 13, 2023. DOI: 10.48550/arXiv.2301.08243. arXiv: 2301.08243 [cs, eess].\n[9]\nMeta AI. Introducing Llama 2: The next generation of our open source large language model.\nMeta AI. 2023. URL: https://ai.meta.com/llama-project (visited on September 24,\n2023).\n[10]\nS. Inskeep and O. Hampton. Meta leans on \u2019wisdom of crowds\u2019 in AI model release, July 19,\n2023. URL: https://www.npr.org/2023/07/19/1188543421/metas-nick-clegg-\non- the- companys- decision- to- offer- ai- tech- as- open- source- softwa\n(visited on September 24, 2023).\n[11]\nD. Milmo. Nick Clegg defends release of open-source AI model by Meta. The Guardian.\nTechnology, July 19, 2023. URL: https://www.theguardian.com/technology/2023/\njul/19/nick-clegg-defends-release-open-source-ai-model-meta-facebook.\n[12]\nM. Langenkamp and D. N. Yue. How Open Source Machine Learning Software Shapes AI.\nIn Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. AIES \u201922:\nAAAI/ACM Conference on AI, Ethics, and Society, pages 385\u2013395, Oxford United Kingdom.\nACM, July 26, 2022. ISBN: 978-1-4503-9247-1. DOI: 10.1145/3514094.3534167. (Visited\non September 24, 2023).\n[13]\nA. Engler. How Open-Source Software Shapes AI Policy. AI Governance Report, Brookings,\nAugust 10, 2021. URL: https://www.brookings.edu/articles/how-open-source-\nsoftware-shapes-ai-policy/ (visited on September 24, 2023).\n[14]\nA. Engler. The EU\u2019s attempt to regulate open-source AI is counterproductive. Brookings.\nAugust 24, 2022. URL: https://www.brookings.edu/articles/the-eus-attempt-\nto-regulate-open-source-ai-is-counterproductive/ (visited on September 24,\n2023).\n[15]\nR. Zwetsloot and A. Dafoe. Thinking About Risks From AI: Accidents, Misuse and Struc-\nture. Default. February 11, 2019. URL: https://www.lawfaremedia.org/article/\nthinking-about-risks-ai-accidents-misuse-and-structure (visited on Septem-\nber 24, 2023).\n[16]\nT. Shevlane. Structured access: an emerging paradigm for safe AI deployment, April 11,\n2022. DOI: 10.48550/arXiv.2201.05159. arXiv: 2201.05159 [cs].\n[17]\nR. Bommasani et al. On the Opportunities and Risks of Foundation Models, July 12, 2022.\nDOI: 10.48550/arXiv.2108.07258. arXiv: 2108.07258 [cs].\n35\n[18]\nE. Jones. Explainer: What Is a Foundation Model?, Ada Lovelace Institute, July 17, 2023.\nURL: https://www.adalovelaceinstitute.org/resource/foundation-models-\nexplainer/ (visited on September 24, 2023).\n[19]\nY.-F. Shea, C. M. Y. Lee, W. C. T. Ip, D. W. A. Luk, and S. S. W. Wong. Use of GPT-4 to\nAnalyze Medical Records of Patients With Extensive Investigations and Delayed Diagnosis.\nJAMA Network Open, 6(8):e2325000, August 14, 2023. ISSN: 2574-3805. DOI: 10.1001/\njamanetworkopen.2023.25000.\n[20]\nOpenAI. Be My Eyes: Be My Eyes uses GPT-4 to transform visual accessibility. March 14,\n2023. URL: https://openai.com/customer- stories/be- my- eyes (visited on\nSeptember 24, 2023).\n[21]\nOpenAI. Viable: Viable uses GPT-4 to analyze qualitative data at a revolutionary scale with\nunparalleled accuracy. July 7, 2023. URL: https://openai.com/customer-stories/\nviable (visited on September 24, 2023).\n[22]\nOpenAI. Inworld AI: Using GPT-3 to create the next generation of AI-powered characters.\nJanuary 1, 2023. URL: https://openai.com/customer-stories/inworld-ai (visited\non September 24, 2023).\n[23]\nY. Altmann. GPT-4 Chatbot for Customer Service | The New ChatGPT Beta Chatbot in\nTest. OMQ Blog. March 27, 2023. URL: https://omq.ai/blog/gpt-4-chatbot-in-\ncustomer-service-beta-chatbot/ (visited on September 24, 2023).\n[24]\nB. Marr. The Amazing Ways Duolingo Is Using AI And GPT-4. Forbes. April 28, 2023. URL:\nhttps://www.forbes.com/sites/bernardmarr/2023/04/28/the-amazing-ways-\nduolingo-is-using-ai-and-gpt-4/ (visited on September 24, 2023).\n[25]\nOpenAI. Stripe: Stripe leverages GPT-4 to streamline user experience and combat fraud.\nMarch 14, 2023. URL: https://openai.com/customer-stories/stripe (visited on\nSeptember 24, 2023).\n[26]\nHarvey.ai. Harvey: Unprecedented legal AI. URL: https://www.harvey.ai/ (visited on\nSeptember 24, 2023).\n[27]\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image\nSynthesis with Latent Diffusion Models, April 13, 2022. DOI: 10.48550/arXiv.2112.\n10752. arXiv: 2112.10752 [cs].\n[28]\nA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical Text-Conditional\nImage Generation with CLIP Latents, April 12, 2022. DOI: 10.48550/arXiv.2204.06125.\narXiv: 2204.06125 [cs].\n[29]\nOpenAI. GPT-4 Technical Report, March 27, 2023. DOI: 10.48550/arXiv.2303.08774.\narXiv: 2303.08774 [cs].\n[30]\nY. Mehdi and J. Spataro. Furthering our AI ambitions \u2013 Announcing Bing Chat Enterprise\nand Microsoft 365 Copilot pricing. Official Microsoft Blog. July 18, 2023. URL: https:\n//blogs.microsoft.com/blog/2023/07/18/furthering-our-ai-ambitions-\nannouncing- bing- chat- enterprise- and- microsoft- 365- copilot- pricing/\n(visited on September 24, 2023).\n[31]\nJ. Vincent. Meta\u2019s powerful AI language model has leaked online \u2014 what happens now?\n- The Verge. The Verge. March 8, 2023. URL: https://www.theverge.com/2023/3/\n8/23629362/meta-ai-language-model-llama-leak-online-misuse (visited on\nSeptember 24, 2023).\n[32]\nJ. Fries, E. Steinberg, S. Fleming, M. Wornow, Y. Xu, K. Morse, D. Dash, and N. Shah. How\nFoundation Models Can Advance AI in Healthcare. Stanford HAI. December 15, 2022. URL:\nhttps://hai.stanford.edu/news/how-foundation-models-can-advance-ai-\nhealthcare (visited on September 24, 2023).\n[33]\nB. Marr. Digital Twins, Generative AI, And The Metaverse. Forbes. May 23, 2023. URL:\nhttps://www.forbes.com/sites/bernardmarr/2023/05/23/digital-twins-\ngenerative-ai-and-the-metaverse/ (visited on September 24, 2023).\n[34]\nD. Milmo. Paedophiles using open source AI to create child sexual abuse content, says\nwatchdog. The Guardian. Society, September 13, 2023. URL: https://www.theguardian.\ncom/society/2023/sep/12/paedophiles-using-open-source-ai-to-create-\nchild-sexual-abuse-content-says-watchdog.\n36\n[35]\nE. Horvitz. On the Horizon: Interactive and Compositional Deepfakes. In ICMI \u201922: Pro-\nceedings of the 2022 International Conference on Multimodal Interaction, pages 653\u2013661,\nBengaluru India. ACM, November 7, 2022. ISBN: 978-1-4503-9390-4. DOI: 10.1145/\n3536221.3558175. (Visited on September 24, 2023).\n[36]\nP. Verma. They thought loved ones were calling for help. It was an AI scam. Washington\nPost, March 10, 2023. URL: https://www.washingtonpost.com/technology/2023/\n03/05/ai-voice-scam/.\n[37]\nT. Brewster. Fraudsters Cloned Company Director\u2019s Voice In $35 Million Heist, Police Find.\nForbes. October 14, 2021. URL: https://www.forbes.com/sites/thomasbrewster/\n2021 / 10 / 14 / huge - bank - fraud - uses - deep - fake - voice - tech - to - steal -\nmillions/ (visited on September 24, 2023).\n[38]\nL. Weidinger et al. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference\non Fairness, Accountability, and Transparency. FAccT \u201922: 2022 ACM Conference on\nFairness, Accountability, and Transparency, pages 214\u2013229, Seoul Republic of Korea. ACM,\nJune 21, 2022. ISBN: 978-1-4503-9352-2. DOI: 10.1145/3531146.3533088. (Visited on\nSeptember 24, 2023).\n[39]\nI. Solaiman et al. Evaluating the Social Impact of Generative AI Systems in Systems and\nSociety, June 12, 2023. DOI: 10.48550/arXiv.2306.05949. arXiv: 2306.05949 [cs].\n[40]\nR. Shelby et al. Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for\nHarm Reduction, July 18, 2023. DOI: 10.48550/arXiv.2210.05791. arXiv: 2210.05791\n[cs].\n[41]\nK. Crawford. Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence.\nYale University Press, New Haven London, 2021. 327 pages. ISBN: 978-0-300-26463-0.\n[42]\nM. L. Gray and S. Suri. Ghost Work: How to Stop Silicon Valley from Building a New Global\nUnderclass. Houghton Mifflin Harcourt, Boston, 2019. 1 page. ISBN: 978-1-328-56628-7.\n[43]\nP. Li, J. Yang, M. A. Islam, and S. Ren. Making AI Less \"Thirsty\": Uncovering and Addressing\nthe Secret Water Footprint of AI Models, April 6, 2023. DOI: 10.48550/arXiv.2304.\n03271. arXiv: 2304.03271 [cs].\n[44]\nE. Strubell, A. Ganesh, and A. McCallum. Energy and Policy Considerations for Deep\nLearning in NLP, June 5, 2019. DOI: 10.48550/arXiv.1906.02243. arXiv: 1906.02243\n[cs].\n[45]\nD. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier,\nand J. Dean. Carbon Emissions and Large Neural Network Training, April 23, 2021. DOI:\n10.48550/arXiv.2104.10350. arXiv: 2104.10350 [cs].\n[46]\nP. Liang et al. Holistic Evaluation of Language Models, November 16, 2022. DOI: 10.48550/\narXiv.2211.09110. arXiv: 2211.09110 [cs].\n[47]\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nMassive Multitask Language Understanding, January 12, 2021. DOI: 10.48550/arXiv.\n2009.03300. arXiv: 2009.03300 [cs].\n[48]\nT. Shevlane et al. Model evaluation for extreme risks, May 24, 2023. DOI: 10.48550/arXiv.\n2305.15324. arXiv: 2305.15324 [cs].\n[49]\nAnthropic. Anthropic\u2019s Responsible Scaling Policy, Version 1.0, Anthropic, September 19,\n2023. URL: https : / / www . anthropic . com / index / anthropics - responsible -\nscaling-policy (visited on September 24, 2023).\n[50]\nJ. B. Sandbrink. Artificial intelligence and biological misuse: Differentiating risks of language\nmodels and biological design tools, August 12, 2023. DOI: 10.48550/arXiv.2306.13952.\narXiv: 2306.13952 [cs].\n[51]\nY. Mirsky et al. The Threat of Offensive AI to Organizations, June 29, 2021. DOI: 10.48550/\narXiv.2106.15764. arXiv: 2106.15764 [cs].\n[52]\nCenter for Security and Emerging Technology and B. Buchanan. A National Security Re-\nsearch Agenda for Cybersecurity and Artificial Intelligence, Center for Security and Emerging\nTechnology, May 2020. DOI: 10.51593/2020CA001. (Visited on September 24, 2023).\n[53]\nM. Anderljung et al. Frontier AI Regulation: Managing Emerging Risks to Public Safety,\nSeptember 4, 2023. DOI: 10.48550/arXiv.2307.03718. arXiv: 2307.03718 [cs].\n37\n[54]\nM. Kinniment et al. Evaluating Language-Model Agents on Realistic Autonomous Tasks,\nAlignment Research Center, July 2023. URL: https : / / evals . alignment . org /\nEvaluating_LMAs_Realistic_Tasks.pdf.\n[55]\nT. Shevlane and A. Dafoe. The Offense-Defense Balance of Scientific Knowledge: Does\nPublishing AI Research Reduce Misuse?, January 9, 2020. DOI: 10.48550/arXiv.2001.\n00463. arXiv: 2001.00463 [cs].\n[56]\nAnthropic. Frontier Threats Red Teaming for AI Safety. Anthropic. July 26, 2023. URL:\nhttps://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-\nsafety (visited on September 24, 2023).\n[57]\nJ. Wei et al. Emergent Abilities of Large Language Models, October 26, 2022. DOI: 10.\n48550/arXiv.2206.07682. arXiv: 2206.07682 [cs].\n[58]\nF. Urbina, F. Lentzos, C. Invernizzi, and S. Ekins. Dual use of artificial-intelligence-powered\ndrug discovery. Nature Machine Intelligence, 4(3):189\u2013191, March 7, 2022. ISSN: 2522-5839.\nDOI: 10.1038/s42256-022-00465-9.\n[59]\nHELENA. Biosecurity in the Age of AI. 2023. URL: https://www.helenabiosecurity.\norg (visited on September 24, 2023).\n[60]\nC. DiBona, S. Ockman, and M. Stone, editors. Open Sources: Voices from the Open Source\nRevolution. O\u2019Reilly, Beijing ; Sebastopol, CA, 1st ed edition, 1999. 272 pages. ISBN:\n978-1-56592-582-3.\n[61]\nGithub. Licenses. URL: https://choosealicense.com/licenses/ (visited on Septem-\nber 24, 2023).\n[62]\nA. Fanelli. LLaMA2 isn\u2019t \"Open Source\"\u2014and why it doesn\u2019t matter. Alessio Fanelli\u2019s blog.\nJuly 19, 2023. URL: https://www.alessiofanelli.com/blog/llama2-isnt-open-\nsource (visited on September 24, 2023).\n[63]\nS. Maffulli. Meta\u2019s LLaMa 2 license is not Open Source. Voices of Open Source. July 20,\n2023. URL: https://blog.opensource.org/metas-llama-2-license-is-not-\nopen-source/ (visited on September 24, 2023).\n[64]\nD. Gray Widder, S. West, and M. Whittaker. Open (For Business): Big Tech, Concentrated\nPower, and the Political Economy of Open AI. SSRN Electronic Journal, 2023. ISSN: 1556-\n5068. DOI: 10.2139/ssrn.4543807.\n[65]\nK. Finley. How to Spot Openwashing. ReadWrite. February 3, 2011. URL: https://\nreadwrite.com/how_to_spot_openwashing/ (visited on September 24, 2023).\n[66]\nResponsible AI Licenses. Responsible AI Licenses. URL: https://www.licenses.ai\n(visited on September 24, 2023).\n[67]\nD. G. Widder, D. Nafus, L. Dabbish, and J. Herbsleb. Limits and Possibilities for \u201cEthical AI\u201d\nin Open Source: A Study of Deepfakes. In 2022 ACM Conference on Fairness, Accountability,\nand Transparency. FAccT \u201922: 2022 ACM Conference on Fairness, Accountability, and\nTransparency, pages 2035\u20132046, Seoul Republic of Korea. ACM, June 21, 2022. ISBN:\n978-1-4503-9352-2. DOI: 10.1145/3531146.3533779. (Visited on September 24, 2023).\n[68]\nSijbrandij. AI weights are not open \"source\". June 27, 2023.\nURL: https : / /\nopencoreventures.com/blog/2023-06-27-ai-weights-are-not-open-source/\n(visited on September 24, 2023).\n[69]\nI. Solaiman. The Gradient of Generative AI Release: Methods and Considerations, February 5,\n2023. DOI: 10.48550/arXiv.2302.04844. arXiv: 2302.04844 [cs].\n[70]\nB. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language\nModel. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n[71]\nStability AI. Stable Diffusion Public Release. stability.ai. URL: https://stability.ai/\nblog/stable-diffusion-public-release (visited on September 24, 2023).\n[72]\nMeta AI. Introducing LLaMA: A foundational, 65-billion-parameter language model. Febru-\nary 24, 2023. URL: https://ai.meta.com/blog/large-language-model-llama-\nmeta-ai/ (visited on September 24, 2023).\n[73]\nB. Cottier. Trends in the dollar training cost of machine learning systems. EPOCH. January 31,\n2023. URL: https://epochai.org/blog/trends-in-the-dollar-training-cost-\nof-machine-learning-systems (visited on September 24, 2023).\n38\n[74]\nC. Li. OpenAI\u2019s GPT-3 Language Model: A Technical Overview. Lambda. June 3, 2020.\nURL: https://lambdalabs.com/blog/demystifying-gpt-3 (visited on September 24,\n2023).\n[75]\nA. Venigalla and L. Linden. Mosaic LLMs (Part 2): GPT-3 quality for < $500k. Mosaic ML.\nSeptember 29, 2022. URL: https://www.mosaicml.com/blog/gpt-3-quality-for-\n500k (visited on September 24, 2023).\n[76]\nJ. Sevilla, L. Heim, A. Ho, T. Besiroglu, M. Hobbhahn, and P. Villalobos. Compute Trends\nAcross Three Eras of Machine Learning, March 9, 2022. DOI: 10.48550/arXiv.2202.\n05924. arXiv: 2202.05924 [cs].\n[77]\nE. Erdil and T. Besiroglu. Algorithmic progress in computer vision, August 24, 2023. DOI:\n10.48550/arXiv.2212.05153. arXiv: 2212.05153 [cs].\n[78]\nC.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee,\nand T. Pfister. Distilling Step-by-Step! Outperforming Larger Language Models with Less\nTraining Data and Smaller Model Sizes, July 5, 2023. DOI: 10.48550/arXiv.2305.02301.\narXiv: 2305.02301 [cs].\n[79]\nS. Goldman. RedPajama replicates LLaMA dataset to build open source, state-of-the-art\nLLMs. VentureBeat. April 18, 2023. URL: https://venturebeat.com/ai/redpajama-\nreplicates-llama-to-build-open-source-state-of-the-art-llms/ (visited on\nSeptember 25, 2023).\n[80]\nG. Sastry. Beyond \u201cRelease\u201d vs. \u201cNot Release\u201d. Center for Research on Foundation Models.\n2021. URL: https://crfm.stanford.edu/commentary/2021/10/18/sastry.html\n(visited on September 24, 2023).\n[81]\nP. Liang, R. Bommasani, K. A. Creel, and R. Reich. The time is now to develop community\nnorms for the release of foundation models. Center for Research on Foundation Models. 2022.\nURL: https://crfm.stanford.edu/2022/05/17/community-norms.html.\n[82]\nS. Maffulli. Towards a definition of \"Open Artificial Intelligence\": First meeting recap. Voices\nof Open Source. July 13, 2023. URL: https://blog.opensource.org/towards-a-\ndefinition-of-open-artificial-intelligence-first-meeting-recap/ (visited\non September 25, 2023).\n[83]\nJ. Rando, D. Paleka, D. Lindner, L. Heim, and F. Tram\u00e8r. Red-Teaming the Stable Diffusion\nSafety Filter, November 10, 2022. DOI: 10.48550/arXiv.2210.04610. arXiv: 2210.\n04610 [cs].\n[84]\nA. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and Transferable Adversarial\nAttacks on Aligned Language Models, July 27, 2023. DOI: 10.48550/arXiv.2307.15043.\narXiv: 2307.15043 [cs].\n[85]\nM. Anderljung and J. Hazell. Protecting Society from AI Misuse: When are Restrictions\non Capabilities Warranted?, March 29, 2023. DOI: 10.48550/arXiv.2303.09377. arXiv:\n2303.09377 [cs].\n[86]\nM. Brundage et al. The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and\nMitigation, February 20, 2018. DOI: 10.48550/arXiv.1802.07228. arXiv: 1802.07228\n[cs].\n[87]\nL. Weidinger et al. Ethical and social risks of harm from Language Models, December 8,\n2021. DOI: 10.48550/arXiv.2112.04359. arXiv: 2112.04359 [cs].\n[88]\nJ. A. Goldstein, G. Sastry, M. Musser, R. DiResta, M. Gentzel, and K. Sedova. Generative\nLanguage Models and Automated Influence Operations: Emerging Threats and Potential\nMitigations, January 10, 2023. DOI: 10.48550/arXiv.2301.04246. arXiv: 2301.04246\n[cs].\n[89]\nM. J. Banias. Inside CounterCloud: A Fully Autonomous AI Disinformation System. The\nDebrief. August 16, 2023. URL: https : / / thedebrief . org / countercloud - ai -\ndisinformation/ (visited on September 25, 2023).\n[90]\nH. Bajohr. Whoever Controls Language Models Controls Politics. April 8, 2023. URL:\nhttps://hannesbajohr.de/en/2023/04/08/whoever- controls- language-\nmodels-controls-politics/ (visited on September 25, 2023).\n39\n[91]\nD. Almeida, K. Shmarko, and E. Lomas. The ethics of facial recognition technologies,\nsurveillance, and accountability in an age of artificial intelligence: a comparative analysis of\nUS, EU, and UK regulatory frameworks. AI and Ethics, 2(3):377\u2013387, August 2022. ISSN:\n2730-5953, 2730-5961. DOI: 10.1007/s43681-021-00077-w.\n[92]\nA. Kaklauskas, A. Abraham, I. Ubarte, R. Kliukas, V. Luksaite, A. Binkyte-Veliene, I.\nVetloviene, and L. Kaklauskiene. A Review of AI Cloud and Edge Sensors, Methods, and\nApplications for the Recognition of Emotional, Affective and Physiological States. Sensors,\n22(20):7824, October 14, 2022. ISSN: 1424-8220. DOI: 10.3390/s22207824.\n[93]\nA. Ferguson. Policing predictive policing. Washington University Law Review, 94(5):1109\u2013\n1189, January 2017.\n[94]\nX. Xu. To Repress or to Co-opt? Authoritarian Control in the Age of Digital Surveillance.\nAmerican Journal of Political Science, 65(2):309\u2013325, April 2021. ISSN: 0092-5853, 1540-\n5907. DOI: 10.1111/ajps.12514.\n[95]\nA. Kendall-Taylor, E. Frantz, and J. Wright. The Digital Dictators. Foreign Affairs, 99(2),\nFebruary 6, 2020. ISSN: 0015-7120. URL: https : / / www . foreignaffairs . com /\narticles/china/2020-02-06/digital-dictators.\n[96]\nK. Crawford et al. AI Now 2019 Report, AI Now Institute, New York, 2019. URL: https:\n//ainowinstitute.org/publication/ai-now-2019-report-2.\n[97]\nS. Feldstein. The Global Expansion of AI Surveillance. Working Paper, Carnegie Endowment\nfor International Peace, 2019. URL: https://carnegieendowment.org/2019/09/17/\nglobal-expansion-of-ai-surveillance-pub-79847.\n[98]\nA. Gupta. The evolution of fraud: Ethical implications in the age of large-scale data breaches\nand widespread artificial intelligence solutions deployment. International Telecommunication\nUnion Journal, 1, February 2, 2018. URL: http://handle.itu.int/11.1002/pub/\n812a022b-en.\n[99]\nJ. Hazell. Large Language Models Can Be Used To Effectively Scale Spear Phishing Cam-\npaigns, May 12, 2023. DOI: 10.48550/arXiv.2305.06972. arXiv: 2305.06972 [cs].\n[100]\nD. Kelley. WormGPT - The Generative AI Tool Cybercriminals Are Using to Launch BEC\nAttacks. SlashNext. July 13, 2023. URL: https://slashnext.com/blog/wormgpt-the-\ngenerative-ai-tool-cybercriminals-are-using-to-launch-business-email-\ncompromise-attacks/ (visited on September 25, 2023).\n[101]\nE. Horvitz. Artificial Intelligence and Cybersecurity: Rising Challenges and Promising\nDirections. In Hearing on Artificial Intelligence Applications to Operations in Cyberspace,\n117th Congress, May 3, 2022. URL: https://aka.ms/AAhee56.\n[102]\nE. Shimony and O. Tsarfati. Chatting Our Way Into Creating a Polymorphic Malware.\nCyberArk. January 17, 23. URL: https://www.cyberark.com/resources/threat-\nresearch-blog/chatting-our-way-into-creating-a-polymorphic-malware\n(visited on September 25, 2023).\n[103]\nL. Fritsch, A. Jaber, and A. Yazidi. An Overview of Artificial Intelligence Used in Malware.\nIn E. Zouganeli, A. Yazidi, G. Mello, and P. Lind, editors, Nordic Artificial Intelligence\nResearch and Development. Volume 1650, pages 41\u201351. Springer International Publishing,\nCham, 2022. DOI: 10.1007/978-3-031-17030-0_4. (Visited on September 25, 2023).\n[104]\nM. P. Stoecklin, J. Jang, and D. Kirat. DeepLocker: How AI Can Power a Stealthy\nNew Breed of Malware. Security Intelligence. August 8, 2018. URL: https : / /\nsecurityintelligence.com/deeplocker-how-ai-can-power-a-stealthy-new-\nbreed-of-malware/ (visited on September 25, 2023).\n[105]\nJ. Li, L. Zhou, H. Li, L. Yan, and H. Zhu. Dynamic Traffic Feature Camouflaging via\nGenerative Adversarial Networks. In 2019 IEEE Conference on Communications and Network\nSecurity (CNS). 2019 IEEE Conference on Communications and Network Security (CNS),\npages 268\u2013276, Washington DC, DC, USA. IEEE, June 2019. ISBN: 978-1-5386-7117-7.\nDOI: 10.1109/CNS.2019.8802772. (Visited on September 25, 2023).\n40\n[106]\nL. A. Garcia, F. Brasser, M. H. Cintuglu, A.-R. Sadeghi, O. Mohammed, and S. A. Zonouz.\nHey, My Malware Knows Physics! Attacking PLCs with Physical Model Aware Rootkit.\nIn Proceedings 2017 Network and Distributed System Security Symposium. Network and\nDistributed System Security Symposium, San Diego, CA. Internet Society, 2017. ISBN:\n978-1-891562-46-4. DOI: 10.14722/ndss.2017.23313. (Visited on September 25, 2023).\n[107]\nD. A. Boiko, R. MacKnight, and G. Gomes. Emergent autonomous scientific research\ncapabilities of large language models, April 11, 2023. DOI: 10.48550/arXiv.2304.05332.\narXiv: 2304.05332 [physics].\n[108]\nA. M. Bran, S. Cox, A. D. White, and P. Schwaller. ChemCrow: Augmenting large-language\nmodels with chemistry tools, June 21, 2023. DOI: 10.48550/arXiv.2304.05376. arXiv:\n2304.05376 [physics, stat].\n[109]\nE. H. Soice, R. Rocha, K. Cordova, M. Specter, and K. M. Esvelt. Can large language models\ndemocratize access to dual-use biotechnology?, June 6, 2023. DOI: 10.48550/arXiv.2306.\n03809. arXiv: 2306.03809 [cs].\n[110]\nOpenAI. GPT-4 System Card. March 23, 2023. URL: https://cdn.openai.com/papers/\ngpt-4-system-card.pdf.\n[111]\nD. V. Gerrit. AI leaders warn Congress that AI could be used to create bioweapons. Washing-\nton Post, July 25, 2023. URL: https://www.washingtonpost.com/technology/2023/\n07/25/ai-bengio-anthropic-senate-hearing/.\n[112]\nE. J. Markey [D-MA]. Text - S.2399 - 118th Congress (2023-2024): Artificial Intelligence\nand Biosecurity Risk Assessment Act, July 19, 2023. URL: https://www.congress.gov/\nbill/118th-congress/senate-bill/2399/text (visited on September 25, 2023).\n[113]\nN. Maslej et al. Chapter 5: Education. In The AI Index 2023 Annual Report. Institute\nfor Human-Centered AI, Stanford University, Stanford, CA, April 2023. URL: https :\n/ / aiindex . stanford . edu / wp - content / uploads / 2023 / 04 / HAI _ AI - Index -\nReport-2023_CHAPTER_5.pdf.\n[114]\nH. Touvron et al. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 19, 2023.\nDOI: 10.48550/arXiv.2307.09288. arXiv: 2307.09288 [cs].\n[115]\nRunPod. GPU Instance Pricing. 2023. URL: https://www.runpod.io/gpu-instance/\npricing (visited on September 25, 2023).\n[116]\nAman. Why GPT-3.5 is (mostly) cheaper than Llama 2. Cursor. July 20, 2023. URL: https:\n//www.cursor.so/blog/llama-inference (visited on September 25, 2023).\n[117]\nM. AI. I-JEPA: The first AI model based on Yann LeCun\u2019s vision for more human-like AI.\nMeta AI. June 13, 2023. URL: https://ai.meta.com/blog/yann-lecun-ai-model-\ni-jepa/ (visited on September 25, 2023).\n[118]\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA:\nLow-Rank Adaptation of Large Language Models, October 16, 2021. DOI: 10.48550/\narXiv.2106.09685. arXiv: 2106.09685 [cs].\n[119]\nM. Hobbhahn. Trends in GPU price-performance. EPOCH. June 27, 2022. URL: https:\n//epochai.org/blog/trends-in-gpu-price-performance (visited on September 25,\n2023).\n[120]\nR. Zellers. Why We Released Grover. The Gradient. July 15, 2019. URL: https : / /\nthegradient.pub/why-we-released-grover/ (visited on September 25, 2023).\n[121]\nR. Jervis. Cooperation under the Security Dilemma. World Politics, 30(2):167\u2013214, January\n1978. DOI: 10.2307/2009958.\n[122]\nB. Garfinkel and A. Dafoe. How does the offense-defense balance scale? Journal of Strategic\nStudies, 42(6):736\u2013763, September 19, 2019. DOI: 10.1080/01402390.2019.1631810.\n[123]\nE. Ferrara. Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language\nModels, April 18, 2023. DOI: 10.48550/arXiv.2304.03738. arXiv: 2304.03738 [cs].\n41\n[124]\nM. Kassab, J. DeFranco, and P. Laplante. Investigating Bugs in AI-Infused Systems: Analysis\nand Proposed Taxonomy. In 2022 IEEE International Symposium on Software Reliability En-\ngineering Workshops (ISSREW). 2022 IEEE International Symposium on Software Reliability\nEngineering Workshops (ISSREW), pages 365\u2013370, Charlotte, NC, USA. IEEE, October\n2022. ISBN: 978-1-66547-679-9. DOI: 10.1109/ISSREW55968.2022.00094. (Visited on\nSeptember 25, 2023).\n[125]\nK. Wiggers. What is Auto-GPT and why does it matter? | TechCrunch. TechCrunch. April 22,\n2023. URL: https://techcrunch.com/2023/04/22/what-is-auto-gpt-and-why-\ndoes-it-matter/?guccounter=1 (visited on September 25, 2023).\n[126]\nAuto-GPT. Home. The Official Auto-GPT Website. 2023. URL: https://news.agpt.co/\n(visited on September 25, 2023).\n[127]\nE. Bagdasaryan, T.-Y. Hsieh, B. Nassi, and V. Shmatikov. (Ab)using Images and Sounds for\nIndirect Instruction Injection in Multi-Modal LLMs, July 24, 2023. DOI: 10.48550/arXiv.\n2307.10490. arXiv: 2307.10490 [cs].\n[128]\nOpenAI. Welcome to the OpenAI platform. URL: https://platform.openai.com (visited\non September 25, 2023).\n[129]\nS. E. Ponta, H. Plate, and A. Sabetta. Detection, assessment and mitigation of vulnerabilities\nin open source dependencies. Empirical Software Engineering, 25(5):3175\u20133215, September\n2020. ISSN: 1382-3256, 1573-7616. DOI: 10.1007/s10664-020-09830-x.\n[130]\nSynopsys Editorial Team. 2023 OSSRA: A deep dive into open source trends. Synopsys.\nFebruary 21, 2023. URL: https://www.synopsys.com/blogs/software-security/\nopen-source-trends-ossra-report.html (visited on September 25, 2023).\n[131]\nJ. Whittlestone and A. Ovadya. The tension between openness and prudence in AI research,\nJanuary 13, 2020. DOI: 10.48550/arXiv.1910.01170. arXiv: 1910.01170 [cs].\n[132]\nBugcrowd. OpenAI. URL: https://bugcrowd.com/openai (visited on September 25,\n2023).\n[133]\nS. R. Bowman. Eight Things to Know about Large Language Models, April 2, 2023. DOI:\n10.48550/arXiv.2304.00612. arXiv: 2304.00612 [cs].\n[134]\nI. Solaiman et al. Release Strategies and the Social Impacts of Language Models, Novem-\nber 12, 2019. DOI: 10.48550/arXiv.1908.09203. arXiv: 1908.09203 [cs].\n[135]\nT. Shevlane. The Artefacts of Intelligence: Governing Scientists\u2019 Contribution to AI Prolif-\neration. PhD thesis, University of Oxford, April 22, 2022. 278 pages. URL: https://cdn.\ngovernance.ai/Shevlane,_Artefacts_of_Intelligence.pdf.\n[136]\nM. Brundage et al. Toward Trustworthy AI Development: Mechanisms for Supporting\nVerifiable Claims, April 20, 2020. DOI: 10.48550/arXiv.2004.07213. arXiv: 2004.\n07213 [cs].\n[137]\nI. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud,\nD. Theron, and P. Barnes. Closing the AI Accountability Gap: Defining an End-to-End\nFramework for Internal Algorithmic Auditing, January 3, 2020. DOI: 10.48550/arXiv.\n2001.00973. arXiv: 2001.00973 [cs].\n[138]\nJ. M\u00f6kander, J. Schuett, H. R. Kirk, and L. Floridi. Auditing large language models: a\nthree-layered approach. AI and Ethics, May 30, 2023. ISSN: 2730-5953, 2730-5961. DOI:\n10.1007/s43681-023-00289-2.\n[139]\nH. Khlaaf, P. Mishkin, J. Achiam, G. Krueger, and M. Brundage. A Hazard Analysis Frame-\nwork for Code Synthesis Large Language Models, July 25, 2022. DOI: 10.48550/arXiv.\n2207.14157. arXiv: 2207.14157 [cs].\n[140]\nARC Evals. Update on ARC\u2019s recent eval efforts: more information about arc\u2019s evaluations of\ngpt-4 and claude. March 17, 2023. URL: https://evals.alignment.org/blog/2023-\n03-18-update-on-recent-evals/ (visited on September 25, 2023).\n[141]\nB. Bucknall, R. Trager, and T. Shevlane. Structured Access for Third-Party Safety Research\non Frontier AI Models Investigating researchers\u2019 model access requirements. Working Paper.\nForthcoming.\n42\n[142]\nOpenAI. DALL\u00b7E 2 Preview - Risks and Limitations. GitHub. 2022. URL: https : / /\ngithub.com/openai/dalle-2-preview/blob/main/system-card.md (visited on\nSeptember 25, 2023).\n[143]\nM. Murgia. OpenAI\u2019s red team: the experts hired to \u2018break\u2019 ChatGPT. Financial Times,\nApril 14, 2023.\n[144]\nD. Ganguli et al. Red Teaming Language Models to Reduce Harms: Methods, Scaling\nBehaviors, and Lessons Learned, November 22, 2022. DOI: 10.48550/arXiv.2209.07858.\narXiv: 2209.07858 [cs].\n[145]\nS. Costanza-Chock, I. D. Raji, and J. Buolamwini. Who Audits the Auditors? Recommen-\ndations from a field scan of the algorithmic auditing ecosystem. In 2022 ACM Conference\non Fairness, Accountability, and Transparency. FAccT \u201922: 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, pages 1571\u20131583, Seoul Republic of Korea. ACM,\nJune 21, 2022. ISBN: 978-1-4503-9352-2. DOI: 10.1145/3531146.3533213. (Visited on\nSeptember 25, 2023).\n[146]\nCentre for Data Ethics and Innovation. The Roadmap to an Effective AI Assurance Ecosystem.\nIndependent report, Centre for Data Ethics and Innovation, December 8, 2021. URL: https:\n//www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-\nassurance-ecosystem (visited on September 25, 2023).\n[147]\nE. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and\nG. Irving. Red Teaming Language Models with Language Models, February 7, 2022. DOI:\n10.48550/arXiv.2202.03286. arXiv: 2202.03286 [cs].\n[148]\nP. Levermore. AI Safety Bounties, Rethink Priorities, August 10, 2023. URL: https://\nrethinkpriorities.org/publications/ai-safety-bounties (visited on Septem-\nber 25, 2023).\n[149]\nOpenAI. ChatGPT Feedback Contest: Official Rules, 2022. URL: https://cdn.openai.\ncom/chatgpt/ChatGPT_Feedback_Contest_Rules.pdf.\n[150]\nhackerone. Hacker-Powered Security Report. 2022. URL: https://www.hackerone.com/\nresources/i/1487910-2022-hacker-powered-security-report-q4fy23/3?.\n[151]\nM. Zhao, J. Grossklags, and P. Liu. An Empirical Study of Web Vulnerability Discovery\nEcosystems. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Commu-\nnications Security. CCS\u201915: The 22nd ACM Conference on Computer and Communications\nSecurity, pages 1105\u20131117, Denver Colorado USA. ACM, October 12, 2015. ISBN: 978-1-\n4503-3832-5. DOI: 10.1145/2810103.2813704. (Visited on September 25, 2023).\n[152]\nE. Dardaman and A. Gupta. When openness fails: Towards a more robust governance frame-\nwork for generative AI. In Proceedings of the Sixth AAIA/ACM Conference on Artificial\nIntelligence, Ethics, and Society. Montreal, Ontario, Canada, 2023.\n[153]\nTeam Nuggets. Why Linux runs 90 percent of the public cloud workload. CBT Nuggets.\nAugust 10, 2018. URL: https://www.cbtnuggets.com/blog/certifications/open-\nsource/why-linux-runs-90-percent-of-the-public-cloud-workload (visited\non September 25, 2023).\n[154]\nA. Engler. To Regulate General Purpose AI, Make the Model Move. Tech Policy Press.\nNovember 10, 2022. URL: https://techpolicy.press/to- regulate- general-\npurpose-ai-make-the-model-move/ (visited on September 25, 2023).\n[155]\nT. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efficient Finetuning of\nQuantized LLMs, May 23, 2023. DOI: 10.48550/arXiv.2305.14314. arXiv: 2305.14314\n[cs].\n[156]\nA. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine, and D. Song. The\nFalse Promise of Imitating Proprietary LLMs, May 25, 2023. DOI: 10.48550/arXiv.2305.\n15717. arXiv: 2305.15717 [cs].\n[157]\nCenter for Security and Emerging Technology, T. Rudner, and H. Toner. Key Concepts in\nAI Safety: An Overview, Center for Security and Emerging Technology, March 2021. DOI:\n10.51593/20190040. (Visited on September 25, 2023).\n[158]\nD. Hendrycks, N. Carlini, J. Schulman, and J. Steinhardt. Unsolved Problems in ML Safety,\nJune 16, 2022. DOI: 10.48550/arXiv.2109.13916. arXiv: 2109.13916 [cs].\n43\n[159]\nJ. Wei et al. Larger language models do in-context learning differently, March 8, 2023. DOI:\n10.48550/arXiv.2303.03846. arXiv: 2303.03846 [cs].\n[160]\nP. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and A. Ho. Will we run out of\ndata? An analysis of the limits of scaling datasets in Machine Learning, October 25, 2022.\nDOI: 10.48550/arXiv.2211.04325. arXiv: 2211.04325 [cs].\n[161]\nMacroPolo. The Global AI Talent Tracker. MacroPolo. 2023. URL: https://macropolo.\norg/digital-projects/the-global-ai-talent-tracker/ (visited on September 25,\n2023).\n[162]\nLAION.ai. Petition for keeping up the progress tempo on AI research while securing its\ntransparency and safety. LAION. March 29, 2023. URL: https://laion.ai/blog/\npetition (visited on September 25, 2023).\n[163]\nD. Jeffries. Let\u2019s Speed Up AI. Future History. February 4, 2023. URL: https : / /\ndanieljeffries.substack.com/p/lets-speed-up-ai (visited on September 25,\n2023).\n[164]\nK. Grace. Let\u2019s think about slowing down AI. LESSWRONG. December 22, 2022. URL:\nhttps://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-\nslowing-down-ai (visited on September 25, 2023).\n[165]\nFuture of Life Institute. Pause Giant AI Experiments: An Open Letter. March 22, 2023.\nURL: https://futureoflife.org/open-letter/pause-giant-ai-experiments/\n(visited on September 25, 2023).\n[166]\nL. Ho et al. International Institutions for Advanced AI, July 11, 2023. DOI: 10.48550/\narXiv.2307.04699. arXiv: 2307.04699 [cs].\n[167]\nG. Marcus and A. Reuel. The world needs an international agency for artificial intelli-\ngence, say two AI experts. The Economist, April 18, 2023. ISSN: 0013-0613. URL: https:\n//www.economist.com/by- invitation/2023/04/18/the- world- needs- an-\ninternational-agency-for-artificial-intelligence-say-two-ai-experts.\n[168]\nOpenAI. Chat Plugins. URL: https : / / platform . openai . com / docs / plugins /\nintroduction (visited on September 25, 2023).\n[169]\nJ. Schuett, N. Dreksler, M. Anderljung, D. McCaffary, L. Heim, E. Bluemke, and B. Garfinkel.\nTowards best practices in AGI safety and governance: A survey of expert opinion, May 11,\n2023. DOI: 10.48550/arXiv.2305.07153. arXiv: 2305.07153 [cs].\n[170]\nN. Yu, V. Skripniuk, D. Chen, L. Davis, and M. Fritz. Responsible Disclosure of Generative\nModels Using Scalable Fingerprinting, March 17, 2022. DOI: 10.48550/arXiv.2012.\n08726. arXiv: 2012.08726 [cs].\n[171]\nM. W. Wagner. Independence by permission. Science, 381(6656):388\u2013391, July 28, 2023.\nISSN: 0036-8075, 1095-9203. DOI: 10.1126/science.adi2430.\n[172]\nJ. Howard. AI Safety and the Age of Dislightenment: Model licensing & surveillance will\nlikely be counterproductive by concentrating power in unsustainable ways. fast.ai. July 10,\n2023. URL: https://www.fast.ai/posts/2023-11-07-dislightenment.html\n(visited on September 26, 2023).\n[173]\nLAION.ai. A Call to Protect Open-Source AI in Europe. LAION. April 28, 2023. URL:\nhttps://laion.ai/notes/letter-to-the-eu-parliament (visited on September 26,\n2023).\n[174]\nScale Virtual Events. Emad Mostaque (Stability AI): Democratizing AI, Stable Diffusion &\nGenerative Models. October 23, 2022. URL: https://exchange.scale.com/public/\nvideos/emad-mostaque-stability-ai-stable-diffusion-open-source (visited\non September 26, 2023).\n[175]\nE. Seger, A. Ovadya, D. Siddarth, B. Garfinkel, and A. Dafoe. Democratising AI: Multiple\nMeanings, Goals, and Methods. In Proceedings of the 2023 AAAI/ACM Conference on AI,\nEthics, and Society. AIES \u201923: AAAI/ACM Conference on AI, Ethics, and Society, pages 715\u2013\n722, Montr\u00e9al QC Canada. ACM, August 8, 2023. DOI: 10.1145/3600211.3604693.\n(Visited on September 26, 2023).\n44\n[176]\nD. Patel and A. Ahmad. Google \"We Have No Moat, And Neither Does OpenAI\": Leaked\nInternal Google Document Claims Open Source AI Will Outcompete Google and OpenAI.\nSemiAnalysis. May 4, 2023. URL: https://www.semianalysis.com/p/google-we-\nhave-no-moat-and-neither (visited on September 26, 2023).\n[177]\nN. Maslej et al. Chapter 7: Diversity. In The AI Index 2023 Annual Report. Institute for Human-\nCentered AI, Stanford University, Stanford, CA, April 2023. URL: https://aiindex.\nstanford.edu/wp-content/uploads/2023/04/HAI_AI- Index- Report- 2023_\nCHAPTER_7.pdf.\n[178]\nEleutherAI. EleutherAI is a non-profit AI research lab that focuses on interpretability and\nalignment of large models. 2023. URL: https://www.eleuther.ai/about (visited on\nSeptember 26, 2023).\n[179]\nBigScience. A one-year long research workshop on large multilingual models and datasets.\nURL: https://bigscience.huggingface.co/ (visited on September 26, 2023).\n[180]\nA. Kayid and N. Reimers. Bonjour. AJ.kQ\u00d3. Guten tag. Hola. Cohere\u2019s Multilingual Text\nUnderstanding Model is Now Available. Cohere. December 12, 2022. URL: https://txt.\ncohere.com/multilingual/ (visited on September 26, 2023).\n[181]\nR. Beaumont. Large Scale Openclip: L/14, H/14 and G/14 trained on LAION-2B. LAION.\nSeptember 15, 2022. URL: https://laion.ai/blog/large- openclip (visited on\nSeptember 26, 2023).\n[182]\nG. Ilharco et al. OpenCLIP, version 0.1, Zenodo, July 28, 2021. DOI: 10.5281/ZENODO.\n5143773. (Visited on September 26, 2023).\n[183]\nS. Altman. Moore\u2019s Law for Everything. March 16, 2021. URL: https : / / moores .\nsamaltman.com/ (visited on September 26, 2023).\n[184]\nK. Miller. Radical Proposal: Universal Basic Income to Offset Job Losses Due to Automation.\nStanford HAI. October 20, 2021. URL: https://hai.stanford.edu/news/radical-\nproposal - universal - basic - income - offset - job - losses - due - automation\n(visited on September 26, 2023).\n[185]\nC. O\u2019Keefe, P. Cihon, B. Garfinkel, C. Flynn, J. Leung, and A. Dafoe. The Windfall Clause:\nDistributing the Benefits of AI, Centre for the Governance of AI Research Report. Future of\nHumanity Institute, University of Oxford, 2020. URL: https://www.fhi.ox.ac.uk/wp-\ncontent/uploads/Windfall-Clause-Report.pdf.\n[186]\nBigCode. Datasets. BigCode. November 16, 2020. URL: https://www.bigcode-project.\norg/docs/about/the-stack/ (visited on September 26, 2023).\n[187]\nJ. Vincent. The scary truth about AI copyright is nobody knows what will happen next. The\nVerge. November 15, 2022. URL: https://www.theverge.com/23444685/generative-\nai-copyright-infringement-legal-fair-use-training-data (visited on Septem-\nber 26, 2023).\n[188]\nPolis. Input Crowd, Output Meaning. 2023. URL: https://pol.is/home (visited on\nSeptember 26, 2023).\n[189]\nP. Coy. Can A.I. and Democracy Fix Each Other? The New York Times. April 5, 2023. URL:\nhttps://www.nytimes.com/2023/04/05/opinion/artificial-intelligence-\ndemocracy-chatgpt.html (visited on September 26, 2023).\n[190]\nThe Collective Intelligence Project. Alignment Assemblies. The Collective Intelligence\nProject. 2023. URL: https://cip.org/alignmentassemblies (visited on September 26,\n2023).\n[191]\nE. Costa. Deliberative democracy in action: A closer look at our recent pilot with Meta. The\nBehavioural Insights Team. November 7, 2022. URL: https://www.bi.team/blogs/\ndeliberative-democracy-in-action/ (visited on September 26, 2023).\n[192]\nA. Ovadya. Meta Ran a Giant Experiment in Governance. Now It\u2019s Turning to AI. WIRED.\nJuly 10, 2023. URL: https : / / www . wired . com / story / meta - ran - a - giant -\nexperiment-in-governance-now-its-turning-to-ai/ (visited on September 26,\n2023).\n45\n[193]\nB. Harris. Improving People\u2019s Experiences Through Community Forums. Meta. Novem-\nber 16, 2022. URL: https://about.fb.com/news/2022/11/improving-peoples-\nexperiences-through-community-forums/ (visited on September 26, 2023).\n[194]\nA. Ovadya. \u2018Platform Democracy\u2019\u2014a very different way to govern big tech: Facebook is\ntrying ~ it. Twitter, Google, OpenAI, and other companies should too. Reimagining Tech-\nnology. July 10, 2023. URL: https://reimagine.aviv.me/p/platform-democracy-\na-different-way-to-govern (visited on September 26, 2023).\n[195]\nW. Zaremba, A. Dhar, L. Ahmad, T. Eloundou, S. Shibani Santurkar, S. Agarwal, and J. Leung.\nDemocratic inputs to AI. May 25, 2023. URL: https://openai.com/blog/democratic-\ninputs-to-ai (visited on September 26, 2023).\n[196]\nT. W. House. FACT SHEET: Biden-Harris Administration Secures Voluntary Commit-\nments from Leading Artificial Intelligence Companies to Manage the Risks Posed by AI.\nThe White House. July 21, 2023. URL: https://www.whitehouse.gov/briefing-\nroom / statements - releases / 2023 / 07 / 21 / fact - sheet - biden - harris -\nadministration-secures-voluntary-commitments-from-leading-artificial-\nintelligence - companies - to - manage - the - risks - posed - by - ai/ (visited on\nSeptember 26, 2023).\n[197]\nJ. Schuett. Risk Management in the Artificial Intelligence Act. European Journal of Risk\nRegulation:1\u201319, February 8, 2023. ISSN: 1867-299X, 2190-8249. DOI: 10.1017/err.\n2023.1.\n[198]\nE. Tabassi. AI Risk Management Framework: AI RMF (1.0). error: NIST AI 100-1, National\nInstitute of Standards and Technology, Gaithersburg, MD, 2023, error: NIST AI 100\u20131. DOI:\n10.6028/NIST.AI.100-1. (Visited on September 26, 2023).\n[199]\nCenter for Long-Term Cybersecurity. UC Berkeley AI Risk-Management Standards Pro-\nfile for General-Purpose AI Systems (GPAIS) and Foundation Models. CLTC. August 29,\n2023. URL: https://cltc.berkeley.edu/seeking- input- and- feedback- ai-\nrisk-management-standards-profile-for-increasingly-multi-purpose-or-\ngeneral-purpose-ai/ (visited on September 26, 2023).\n[200]\nA. M. Barrett, D. Hendrycks, J. Newman, and B. Nonnecke. Actionable Guidance for High-\nConsequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks,\nFebruary 23, 2023. DOI: 10.48550/arXiv.2206.08966. arXiv: 2206.08966 [cs].\n[201]\nI. A. E. Agency. Applications of Probabilistic Safety Assessment (PSA) for Nuclear Power\nPlants. TECDOC 1200, International Atomic Energy Agency, Vienna, 2001. URL: https:\n//www-pub.iaea.org/mtcd/publications/pdf/te_1200_prn.pdf.\n[202]\nAnthropic. Model Card and Evaluations for Claude Models, 2023. URL: https://www-\nfiles.anthropic.com/production/images/Model-Card-Claude-2.pdf.\n[203]\nI. D. Raji and J. Buolamwini. Actionable Auditing: Investigating the Impact of Publicly\nNaming Biased Performance Results of Commercial AI Products. In Proceedings of the 2019\nAAAI/ACM Conference on AI, Ethics, and Society. AIES \u201919: AAAI/ACM Conference on\nAI, Ethics, and Society, pages 429\u2013435, Honolulu HI USA. ACM, January 27, 2019. ISBN:\n978-1-4503-6324-2. DOI: 10.1145/3306618.3314244. (Visited on September 26, 2023).\n[204]\nI. D. Raji, P. Xu, C. Honigsberg, and D. Ho. Outsider Oversight: Designing a Third Party\nAudit Ecosystem for AI Governance. In Proceedings of the 2022 AAAI/ACM Conference\non AI, Ethics, and Society. AIES \u201922: AAAI/ACM Conference on AI, Ethics, and Society,\npages 557\u2013571, Oxford United Kingdom. ACM, July 26, 2022. ISBN: 978-1-4503-9247-1.\nDOI: 10.1145/3514094.3534181. (Visited on September 26, 2023).\n[205]\nStability AI. Stable Diffusion 2.0 Release. November 24, 2022. URL: https://stability.\nai/blog/stable-diffusion-v2-release (visited on September 26, 2023).\n[206]\nISO. ISO/IEC 23894:2023. February 2023. URL: https://www.iso.org/standard/\n77304.html (visited on September 26, 2023).\n[207]\nPartnership on AI Staff. PAI Is Collaboratively Developing Shared Protocols for Large-Scale\nAI Model Safety. Partnership on AI. April 6, 2023. URL: https://partnershiponai.\norg/pai- is- collaboratively- developing- shared- protocols- for- large-\nscale-ai-model-safety/ (visited on September 26, 2023).\n46\n[208]\nP. on AI Staff. Managing the Risks of AI Research: Six Recommendations for Responsible\nPublication, May 6, 2021. URL: https://partnershiponai.org/paper/responsible-\npublication-recommendations/ (visited on September 26, 2023).\n[209]\nMicrosoft. Microsoft, Anthropic, Google, and OpenAI launch Frontier Model Forum. Mi-\ncrosoft On the Issues. July 26, 2023. URL: https://blogs.microsoft.com/on-the-\nissues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-\nmodel-forum/ (visited on September 26, 2023).\n[210]\nAmerican Law Institute. Restatement of the Law (Second) Torts. The American Law Institute,\nPhiladelphia, PA, 1965. URL: https://www.ali.org/publications/show/torts/.\n[211]\nAmerican Law Institute. Restatement of the Law (Third) Torts: Products Liability. The Amer-\nican Law Institute, Philadelphia, PA, 1998. URL: https://www.ali.org/publications/\nshow/torts-third/.\n[212]\nJ. C. P. Goldberg and B. C. Zipursky. The Restatement (Third) and the Place of Duty\nin Negligence Law. Vanderbilt Law Review, 54(3):657, April 1, 2001. URL: https://\nscholarship.law.vanderbilt.edu/vlr/vol54/iss3/2.\n[213]\nW. M. Landes and R. A. Posner. The Economic Structure of Tort Law: Harvard University\nPress, Cambridge, MA, May 20, 1987. 329 pages. ISBN: 978-0-674-86403-0.\n[214]\nP. Hacker. The European AI liability directives \u2013 Critique of a half-hearted approach and\nlessons for the future. Computer Law & Security Review, 51:105871, November 2023. ISSN:\n02673649. DOI: 10.1016/j.clsr.2023.105871.\n[215]\nN. Mulani and J. Whittlestone. Proposing a Foundation Model Information-Sharing Regime\nfor the UK | GovAI Blog. June 16, 2023. URL: https://www.governance.ai/post/\nproposing-a-foundation-model-information-sharing-regime-for-the-uk\n(visited on September 26, 2023).\n[216]\nM. Anderljung and P. Scharre. How to Prevent an AI Catastrophe. Foreign Affairs, Au-\ngust 14, 2023. URL: https://www.foreignaffairs.com/world/how-prevent-ai-\ncatastrophe-artificial-intelligence.\n[217]\nW. Henshall. The Heated Debate Over Who Should Control Access to AI. Time. August 25,\n2023. URL: https://time.com/6308604/meta-ai-access-open-source/ (visited on\nSeptember 26, 2023).\n47\nA\nAI Model Component Guide\nTable 6: AI Model Component Guide\nComponent\nSubcomponent\nDefinition\nWhat does access to this component\nallow actors to do?\nModel weights\nThe variables or numerical values used to\nspecify how the input (e.g., text describing\nan image) is transformed into the output\n(e.g., the image itself)\n[See trained weights]\nTrained weights\nThe final values of model weights after\nthey have been updated during training\nAlone, nothing; but when combined with\nthe model architecture, any actor can run\nor fine-tune the optimized model with very\nlow computing costs\nModel weight\nsnapshots\nThe record of the different weight values\nas they were updated during training\nCombined with model architecture, actors\ncould run or fine-tune partially-optimized\nsystems\nHyperparameters\nThe variables used to define other parts\nof the model, such as model architecture\n(e.g., the number of layers in the model)\nand training process (e.g., the strength of\nregularization in the loss function)\n[See optimized hyperparameters]\nOptimized\nhyperparameters\nThe hyperparameter values chosen through\nthe hyperparameter optimization process\nthat optimize the efficiency of the training\nprocess and increase the model\u2019s perfor-\nmance on the training task(s)\nImmediately\ntrain\nmodel\nmore\neffi-\nciently by skipping the computationally-\nexpensive hyperparameter search; this en-\nables actors to train higher-performance\nmodels for a fixed computing cost\nMethods for\nhyperparameter\noptimization\nThe techniques used to optimize the hy-\nperparameter for model performance (e.g.,\ngrid search, random search, Bayesian opti-\nmization); also known as hyperparameter\ntuning\nLeverage known techniques to efficiently\nfind the best model configurations\nData processing\ncode\nThe code used to obtain raw training data\nand convert it into the form necessary for\nmodel training\nReproduce the full data pipeline that sup-\nplies training data to the model\nData cleaning\nThe code used to transform the training\ndata into a form more amenable for model\ntraining (e.g., normalization, removing in-\nvalid data, etc.)\nTransform new data into the structure ex-\npected by the model and ensure data com-\npatibility\nSynthetic data\ncreation\nThe code used to generate additional, artifi-\ncial data that is similar to the original train-\ning data; synthetic data is useful because\ntraining on more data sometimes improves\nmodel performance\nGenerate additional training data with sim-\nilar statistical properties as the original\nData loading\nThe code used to transform the cleaned\ntraining data into the correct structure / for-\nmat to be input directly into the model (e.g.\ntransforming data into tensors for training\non high-performance chips)\nFeed new data into the model seamlessly\nto enable training\nTraining code\nThe code that defines the model architec-\nture and implements the algorithms used to\noptimize the model weights during training\nRebuild the model architecture from\nscratch and train it end-to-end with the\nsame code\nContinued on next page\n48\nTable 6 \u2013 continued from previous page\nComponent\nSubcomponent\nDefinition\nWhat does access to this component\nallow actors to do?\nModel\narchitecture\nThe code specifying the structure and de-\nsign of an AI model, including the types\nof layers, the connections between them,\nand any additional components or features\nthat need to be incorporated; it also speci-\nfies the types of inputs and outputs to the\nmodel, how input data are processed, and\nhow learning happens in the model\nAlone, understand better how to train simi-\nlar models; with trained weights, any actor\ncan run or fine-tune the model\nLoss function /\nreward function\nThe code that defines the loss function:\na mathematical formula that measures\nmodel\u2019s performance on the training task\n(e.g. MSE loss); the loss function is crit-\nical because minimizing it during train-\ning guides the optimization of the model\nweights\nBetter understand how to train similar\nmodels\nSaving and\nloading models\nThe code that handles saving the trained\nmodel parameters or weights to disk or\nother storage mediums, allowing the pa-\nrameters to be loaded and reused for infer-\nence or further fine-tuning\nUnderstand better how to distribute trained\nmodels\nTraining loop\nThe training loop code iterates over the\ntraining data; within each iteration, it feeds\nsome input data to the model, computes\nthe loss, and updates the model\u2019s weights\nusing the chosen optimization algorithm\nRun full end-to-end training from raw data\nto final model (given training data and\nmodel architecture)\nHyperparameter\noptimization\ncode\nThe code used to optimize the hyperpa-\nrameters to improve performance, imple-\nmenting the methods for hyperparameter\noptimization (see above)\nDiscover optimal hyperparameters effi-\nciently and create more capable models\nfaster\nRelated models\nSome AI systems rely on multiple models,\neither during the training/fine-tuning pro-\ncess or during inference; for instance, after\ninitial training, many foundation models\nare fine-tuned via a related Reinforcement\nLearning from Human Feedback (RLHF)\nmodel and, more directly, Meta\u2019s CICERO\ncombines a language processing model\nwith a strategic reasoning model\nRelated models cannot be easily used on\ntheir own, but would help actors under-\nstand how to integrate different types of\nAI model into a single system\nGuidelines for\nhuman\nevaluators in\nRLHF\nThe instructions specifying what kind of\nfeedback human evaluators should provide\non the outputs from the foundation model;\nthis feedback is then used in the RLHF\ntraining process\nUnderstand how to efficiently obtain high-\nquality training data from human labelers\nInference code\n(prediction or\ndeployment code)\nThe code that, given the model weights\nand architecture, implements the trained\nmodel; in other words, it runs the AI model\nand allows it to perform tasks (like writing,\nclassifying images and playing games)\nGenerate model outputs and use the model\ndirectly, understand how to efficiently run\nthe model and how to integrate it into pro-\nduction systems\nSafety code\nAdditional code is often included within\nthe inference code to prevent malicious or\nharmful use of the model (e.g., prevent-\ning users from generating pornographic\nimages)\nUnderstand how developers tried to pre-\nvent misuse of the model\nContinued on next page\n49\nTable 6 \u2013 continued from previous page\nComponent\nSubcomponent\nDefinition\nWhat does access to this component\nallow actors to do?\nTraining strategies\nSpecific techniques used to train the model\n(e.g., how long to train the model for);\nthese are specified in the training code but\nalso communicated at a high-level in asso-\nciated papers and model cards\nUnderstand which techniques boost train-\ning efficiency and thus model performance\nfor a fixed computing cost\nTraining data\nThe data used to train and test the model\n(for instance, pictures for an image recog-\nnition model or internet webpages for a\nlarge language model)\nUnderstand features of the data used to\ntrain the model and, given model architec-\nture and training code, train the model\nData labels\nSometimes, training data are labeled (e.g.,\na label for a picture could be a caption or\ndescription of the image); labels enable\nevaluation during training about how well\nthe machine learning model is predicting\nthe label, but they are not always necessary\ndepending on the model being trained\nUnderstand how labeling takes place (and\nwhether it is outsourced to a third-party,\nfor example), train or retrain models (de-\npending on the model)\nTesting data\nTo fairly evaluate how well a model per-\nforms, its predictions are often evaluated\non a new set of testing data that was never\nused during training; this can be a portion\nof the original training data that is \"held-\nout\" and excluded from training, or a new\ndataset\nSame as training data (but to a lesser extent\nsince there tends to be more training than\ntesting data), evaluate performance when\ntraining or retraining models\nEvaluation\nMetrics\nMeasures against which to assess the per-\nformance of the model during training;\nthese metrics may vary depending on the\nspecific task; commonly-used metrics in-\nclude accuracy, precision, recall, or per-\nplexity\nUnderstand how the model capabilities\nwere assessed, evaluate performance when\ntraining or retraining models\nTacit knowledge\nAdditional information known only to cer-\ntain researchers and engineers within AI\nlabs that is often very helpful (and some-\ntimes necessary) to train advanced AI mod-\nels; for example, Phuong & Hutter (2022)\nsummarizes some tacit knowledge relating\nto the Transformer architecture\nTrain more advanced models more effi-\nciently\nSoftware stack\nA set of software or code libraries that en-\nables the training of an AI model; this in-\ncludes machine learning frameworks such\nas PyTorch, TensorFlow and Jax, as well\nas compilers and optimized libraries like\nCUDA, cuDNN and Triton that enable\ntraining on advanced GPUs\nKnowing the version of certain software\ntools would save time when building train-\ning pipelines\n50\n"
  }
]